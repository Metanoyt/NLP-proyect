{
  "pr_id": 169001,
  "title": "Add ViewOps Replacement Test and Fix Device Metadata Propagation in Inductor",
  "description": "This PR introduces a new test (`test_cond_with_replace_view_ops`) validating ReplaceViewOpsWithViewCopyOpsPass under the AOT-Inductor path. Additionally, it fixes incomplete propagation of device metadata (`device_types`, `device_idxs`, `device_type`) inside subgraph lowering in `torch._inductor.graph`, ensuring correct compilation behavior for GPU/Triton backends.",
  "files_modified": [
    {
      "file_path": "test/inductor/test_aot_inductor.py",
      "language": "python",
      "diff": "diff --git a/test/inductor/test_aot_inductor.py b/test/inductor/test_aot_inductor.py\nindex fd962c8bea70..8e7a62ae0954 100644\n--- a/test/inductor/test_aot_inductor.py\n+++ b/test/inductor/test_aot_inductor.py\n@@ -21,6 +21,7 @@\n from torch._dynamo.device_interface import get_interface_for_device\n from torch._dynamo.testing import rand_strided, same\n from torch._dynamo.utils import counters\n+from torch._export.passes import ReplaceViewOpsWithViewCopyOpsPass\n from torch._inductor import config\n from torch._inductor.codecache import WritableTempFile\n from torch._inductor.cpp_builder import normalize_path_separator\n@@ -2229,6 +2230,39 @@ def test_cond_with_reinterpret_view_inputs_outputs(self):\n             dynamic_shapes=dynamic_shapes,\n         )\n\n+    @requires_gpu\n+    def test_cond_with_replace_view_ops(self):\n+        if self.device != GPU_TYPE:\n+            raise unittest.SkipTest(\"requires GPU\")\n+\n+        class CondModelWithViewAndLinear(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.linear = torch.nn.Linear(4, 4)\n+\n+            def forward(self, cache, x):\n+                def true_fn(cache, x):\n+                    return cache + 1.0\n+\n+                def false_fn(cache, x):\n+                    return self.linear(x).view(1, 2, 4, 4)\n+\n+                cache_is_initialized = (cache != 0).any()\n+                return torch.cond(cache_is_initialized, false_fn, false_fn, [cache, x])\n+\n+        example_input = (\n+            torch.zeros(1, 2, 4, 4, dtype=torch.float32, device=self.device),\n+            torch.randn(8, 4, dtype=torch.float32, device=self.device),\n+        )\n+        model = CondModelWithViewAndLinear().to(device=self.device)\n+        exported_program = torch.export.export(model, example_input)\n+        program = exported_program.run_decompositions()\n+        gm = ReplaceViewOpsWithViewCopyOpsPass()(program.graph_module).graph_module\n+        with config.patch(\n+            {\"max_autotune\": True, \"max_autotune_gemm_backends\": \"TRITON\"}\n+        ):\n+            _ = torch._inductor.aot_compile(gm, example_input)\n"
    },
    {
      "file_path": "torch/_inductor/graph.py",
      "language": "python",
      "diff": "diff --git a/torch/_inductor/graph.py b/torch/_inductor/graph.py\nindex 517d6c3e39d1..a16e09f3ca5c 100644\n--- a/torch/_inductor/graph.py\n+++ b/torch/_inductor/graph.py\n@@ -2369,6 +2369,9 @@ def codegen_subgraph(self, parent_graph: GraphLowering) -> None:\n             self.wrapper_code = parent_graph.wrapper_code\n             self.device_ops = parent_graph.device_ops\n             self.cpp_wrapper = parent_graph.cpp_wrapper\n+            self.device_types = parent_graph.device_types\n+            self.device_idxs = parent_graph.device_idxs\n+            self.device_type = parent_graph.device_type\n\n             self._update_scheduler()\n             self.scheduler.codegen()"
    }
  ]
}
