Owner s oncall quantization ruff noqa F copy unittest typing Any torch torch ao quantization observer HistogramObserver MinMaxObserver PlaceholderObserver torch ao quantization quantize_pt e convert_pt e prepare_pt e torch ao quantization quantizer QuantizationAnnotation QuantizationSpec Quantizer SharedQuantizationSpec torch ao quantization quantizer xnnpack_quantizer get_symmetric_quantization_config torch ao quantization quantizer xnnpack_quantizer_utils OP_TO_ANNOTATOR QuantizationConfig torch export export torch testing _internal common_quantization QuantizationTestCase torch testing _internal common_utils IS_WINDOWS raise_on_run_directly TestHelperModules Conv dWithObsSharingOps torch nn Module __init__ - None super __init__ conv = torch nn Conv d hardtanh = torch nn Hardtanh adaptive_avg_pool d = torch nn AdaptiveAvgPool d linear = torch nn Linear forward x x = conv x x = adaptive_avg_pool d x x = hardtanh x x = x view - x = linear x x Conv dWithSharedDQ torch nn Module __init__ - None super __init__ conv = torch nn Conv d conv = torch nn Conv d linear = torch nn Linear forward x x = conv x z = x view - w = linear z y = conv x add_output = x + y extra_output = x w add_output extra_output ModuleForDifferentQconfig torch nn Module __init__ - None super __init__ conv = torch nn Conv d conv = torch nn Conv d adaptive_avg_pool d = torch nn AdaptiveAvgPool d forward x x = conv x w = adaptive_avg_pool d x y = conv x add_output = x + y extra_output = x + w add_output extra_output _DEQUANTIZE_OPS = torch ops quantized_decomposed dequantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor tensor torch ops quantized_decomposed dequantize_per_channel default unittest skipIf IS_WINDOWS Windows yet supported torch compile TestDuplicateDQPass QuantizationTestCase _test_duplicate_dq model example_inputs quantizer m_eager = model eval program capture m = copy deepcopy m_eager m = export m example_inputs strict=True module m = prepare_pt e m quantizer Calibrate m example_inputs m = convert_pt e m pt _quant_output = m example_inputs n m graph nodes annotation = n meta get quantization_annotation None annotation None arg n args isinstance arg torch fx Node arg target _DEQUANTIZE_OPS assertEqual len arg users keys test_no_need_for_duplicate_dq Model under test conv d - avgpool - hardtanh - linear Check quantization tags conv d avgpool linear correctly set BackendAQuantizer Quantizer annotate gm torch fx GraphModule - torch fx GraphModule backend_string = BackendA quantization_config = get_symmetric_quantization_config is_per_channel=True OP_TO_ANNOTATOR linear gm quantization_config OP_TO_ANNOTATOR conv gm quantization_config OP_TO_ANNOTATOR adaptive_avg_pool d gm quantization_config validate model torch fx GraphModule - None pass example_inputs = torch randn _test_duplicate_dq TestHelperModules Conv dWithObsSharingOps example_inputs BackendAQuantizer test_simple_duplicate_dq Model under test conv d - conv d - add &#124; &#124; --------- &#124; ----- view_copy -- linear &#124; ----- mul There should three dq nodes because output first conv d fed next conv d add view_copy + linear All three quantized Thus DQ node duplicated those three uses BackendAQuantizer Quantizer annotate gm torch fx GraphModule - torch fx GraphModule backend_string = BackendA quantization_config = get_symmetric_quantization_config is_per_channel=True OP_TO_ANNOTATOR linear gm quantization_config OP_TO_ANNOTATOR conv gm quantization_config OP_TO_ANNOTATOR add gm quantization_config validate model torch fx GraphModule - None pass example_inputs = torch randn _test_duplicate_dq TestHelperModules Conv dWithSharedDQ example_inputs BackendAQuantizer test_no_add_quant_duplicate_dq Model under test conv d - conv d - add &#124; &#124; --------- &#124; ----- view_copy -- linear &#124; ----- mul There should three dq nodes because output first conv d fed next conv d view_copy + linear Both quantized However skip connection add mul quantized Thus DQ node duplicated those two uses BackendAQuantizer Quantizer annotate gm torch fx GraphModule - torch fx GraphModule backend_string = BackendA quantization_config = get_symmetric_quantization_config is_per_channel=True OP_TO_ANNOTATOR linear gm quantization_config OP_TO_ANNOTATOR conv gm quantization_config validate model torch fx GraphModule - None pass example_inputs = torch randn _test_duplicate_dq TestHelperModules Conv dWithSharedDQ example_inputs BackendAQuantizer test_avgpool_use_different_qconfig Model under test conv d - conv d - add &#124; &#124; --------- &#124; ----- adaptive_avgpool d different qconfig &#124; ----- add output conv d - dq - conv d - add &#124; &#124; ------- dq ----- &#124; - dq - q - dq ----- adaptive_avgpool d different qconfig &#124; - dq ----- add _get_uint _quantization_config act_observer_or_fake_quant_ctr = HistogramObserver type ignore assignment act_quantization_spec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine observer_or_fake_quant_ctr=act_observer_or_fake_quant_ctr with_args eps= - weight_observer_or_fake_quant_ctr _ObserverOrFakeQuantizeConstructor = noqa F MinMaxObserver extra_args dict str Any = eps - weight_quantization_spec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine ch_axis= is_dynamic=False observer_or_fake_quant_ctr=weight_observer_or_fake_quant_ctr with_args extra_args bias_observer_or_fake_quant_ctr _ObserverOrFakeQuantizeConstructor = noqa F PlaceholderObserver bias_quantization_spec = QuantizationSpec dtype=torch float observer_or_fake_quant_ctr=bias_observer_or_fake_quant_ctr quantization_config = QuantizationConfig act_quantization_spec act_quantization_spec weight_quantization_spec bias_quantization_spec quantization_config BackendAQuantizer Quantizer annotate gm torch fx GraphModule - torch fx GraphModule backend_string = BackendA quantization_config = get_symmetric_quantization_config is_per_channel=True avgpool_qconfig = _get_uint _quantization_config OP_TO_ANNOTATOR conv gm quantization_config OP_TO_ANNOTATOR add gm quantization_config n gm graph nodes n op == call_function n target == torch ops aten mean dim qspec = avgpool_qconfig input_activation input_act = n args output_qspec = SharedQuantizationSpec input_act n n meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act qspec output_qspec=output_qspec _annotated=True validate model torch fx GraphModule - None pass example_inputs = torch randn _test_duplicate_dq TestHelperModules ModuleForDifferentQconfig example_inputs BackendAQuantizer __name__ == __main__ raise_on_run_directly test test_quantization py