usr bin env python A Python script logging system-level utilization usage json format Data collected CPU memory GPU memory utilization GPU utilization available Usage - To run script default data collect time setting use following command python monitor py - To run script local machine debug mode customized data collect time use following command python monitor py -- debug -- log-interval -- data-collect-interval - To log data file use following command python monitor py usage_log txt - To gracefully exit script local machine press ctrl+c kill process using kill pid __future__ annotations os sys adding sys path makes monitor script able path tools stats utilization_stats_lib sys path insert os path join os path dirname __file__ argparse copy dataclasses os signal threading time collections defaultdict typing Any psutil type ignore tools stats utilization_stats_lib getDataModelVersion getTsNow GpuUsage RecordData UtilizationMetadata UtilizationRecord UtilizationStats _HAS_PYNVML = False _HAS_AMDSMI = False _job_name = os environ get JOB_NAME _job_id = os environ get JOB_ID _workflow_run_id = os environ get WORKFLOW_RUN_ID _workflow_name = os environ get WORKFLOW_NAME dataclasses dataclass UsageData Dataclass storing usage data This data will logged usage_log file cpu_percent float memory_percent float processes list dict str Any gpu_list list GpuData dataclasses dataclass GpuData Dataclass storing gpu data This data will logged usage_log file uuid str utilization float mem_utilization float allocated_mem float allocated_mem_value float total_mem_value float try pynvml type ignore _HAS_PYNVML = True except ModuleNotFoundError pass try amdsmi type ignore _HAS_AMDSMI = True except ModuleNotFoundError pass parse_args - argparse Namespace Parse command line arguments Returns argparse Namespace Parsed arguments parser = argparse ArgumentParser description= System-level Usage Logger debug mode used local gracefully exit script when ctrl+c pressed print out json output pretty format parser add_argument -- debug action= store_true help= Enable debug mode parser add_argument -- log-interval type=float default= help= set time interval logging utilization data default seconds parser add_argument -- data-collect-interval type=float default= help= set time interval collect data default second should longer than log_interval args = parser parse_args args SharedResource thread-safe utils shared resources used both worker processor main processor during UsageLogger It collects usage data errors worker processor output aggregated data errors main processor logging __init__ is_debug_mode bool = False - None _data_list list UsageData = _data_errors list str = _data_logs list str = _lock = threading Lock get_and_reset - tuple list UsageData list str list str get deepcopy list usageData list string errors copy_data = copy_errors = copy_logs = _lock copy_data = copy deepcopy _data_list copy_errors = copy deepcopy _data_errors copy_logs = copy deepcopy _data_logs _data_list clear _data_errors clear _data_logs clear copy_data copy_errors copy_logs add_data data UsageData - None _lock _data_list append data add_error error Exception - None _lock _data_errors append str error add_log log str - None _lock print here log _data_logs append log UsageLogger Collect display usage data including CPU memory GPU memory utilization GPU utilization By default data collected every seconds log aggregated result every seconds __init__ log_interval float = data_collect_interval float = is_debug_mode bool = False pynvml_enabled bool = False amdsmi_enabled bool = False - None log_interval Time interval seconds collecting usage data default seconds is_debug_mode Useful you re testing local machine want see output pretty format more information _log_interval = log_interval _data_collect_interval = data_collect_interval _metadata = UtilizationMetadata level= metadata usage_collect_interval=self _data_collect_interval data_model_version=getDataModelVersion job_id=_job_id job_name=_job_name workflow_id=_workflow_run_id workflow_name=_workflow_name start_at=getTsNow _has_pynvml = pynvml_enabled _has_amdsmi = amdsmi_enabled _gpu_handles list Any = _gpu_lib_detected str = _num_of_cpus = _debug_mode = is_debug_mode _initial_gpu_handler shared_resource = SharedResource exit_event = threading Event _collect_data - None Collects data every data_collect_interval seconds while exit_event is_set try collect cpu memory gpu metrics memory = psutil virtual_memory percent cpu_percent = psutil cpu_percent processes = _get_process_info gpu_list = _collect_gpu_data data = UsageData cpu_percent=cpu_percent memory_percent=memory processes=processes gpu_list=gpu_list _debug_mode print f collecting data data shared_resource add_data data except Exception e _debug_mode print f error detected str e shared_resource add_error e finally time sleep _data_collect_interval _generate_stats data_list list float - UtilizationStats Generate stats data list len data_list == UtilizationStats total = sum data_list avg = total len data_list maxi = max data_list UtilizationStats avg=round avg max=round maxi raw=data_list _output_data - None output data _metadata start_at = getTsNow log_json _metadata to_json while exit_event is_set collecting_start_time = time time stats = UtilizationRecord level= record timestamp=getTsNow try data_list error_list log_list = shared_resource get_and_reset _debug_mode print f collected data len data_list errors found len error_list logs len log_list records clears found errors errors = list set error_list has errors data list None bug may exist monitor code log errors data_list len errors raise ValueError f no data collected detected errors during interval errors logs log_list data_list pass since no data collected continue cpu_stats = _generate_stats data cpu_percent data data_list memory_stats = _generate_stats data memory_percent data data_list find all cmds during interval cmds = process cmd data data_list process data processes stats cmd_names = list cmds record = RecordData record cpu = cpu_stats record memory = memory_stats collect gpu metrics _has_pynvml _has_amdsmi gpu_list = _calculate_gpu_utilization data_list record gpu_usage = gpu_list stats data = record stats logs = log_list except Exception e stats = UtilizationRecord level= record timestamp=getTsNow error=str e finally collecting_end_time = time time time_diff = collecting_end_time - collecting_start_time verify there data stats level stats log_duration = f time_diff f ms log_json stats to_json time sleep _log_interval shut down gpu connections when exiting _shutdown_gpu_connections _calculate_gpu_utilization data_list list UsageData - list GpuUsage Calculates GPU utilization calculate_gpu = gpu_mem_utilization = defaultdict list gpu_utilization = defaultdict list gpu_allocated_mem = defaultdict list gpu_allocated_mem_values = defaultdict list gpu_total_mem_values = defaultdict float data data_list gpu data gpu_list gpu_mem_utilization gpu uuid append gpu mem_utilization gpu_utilization gpu uuid append gpu utilization gpu_allocated_mem gpu uuid append gpu allocated_mem gpu_allocated_mem_values gpu uuid append gpu allocated_mem_value gpu_total_mem_values gpu uuid = gpu total_mem_value gpu_uuid gpu_utilization keys gpu_util_stats = _generate_stats gpu_utilization gpu_uuid gpu_mem_util_stats = _generate_stats gpu_mem_utilization gpu_uuid gpu_allocated_mem_stats = _generate_stats gpu_allocated_mem gpu_uuid gpu_allocated_mem_value_stats = _generate_stats gpu_allocated_mem_values gpu_uuid calculate_gpu append GpuUsage uuid=gpu_uuid util_percent=gpu_util_stats mem_util_percent=gpu_mem_util_stats allocated_mem_percent=gpu_allocated_mem_stats allocated_mem_value=gpu_allocated_mem_value_stats total_mem_value=gpu_total_mem_values gpu_uuid calculate_gpu start - None collect_thread = threading Thread target=self _collect_data collect_thread start _output_data collect_thread join stop args Any - None Exits program gracefully shuts down logging loop exit_event set log_json stats Any - None Logs stats json format stdout print stats _collect_gpu_data - list GpuData gpu_data_list = _has_pynvml Iterate over available GPUs gpu_handle _gpu_handles see https docs nvidia com deploy nvml-api group__nvmlDeviceQueries html gpu_utilization = pynvml nvmlDeviceGetUtilizationRates gpu_handle gpu_uuid = pynvml nvmlDeviceGetUUID gpu_handle gpu_memory_info = pynvml nvmlDeviceGetMemoryInfo gpu_handle mem_utilization = gpu_utilization memory allocate_mem_MB = gpu_memory_info used total_mem_MB = gpu_memory_info total allocate_mem_percent = allocate_mem_MB total_mem_MB gpu_data_list append GpuData uuid=gpu_uuid utilization=gpu_utilization gpu mem_utilization=mem_utilization allocated_mem=allocate_mem_percent allocated_mem_value=allocate_mem_MB total_mem_value=total_mem_MB _has_amdsmi Iterate over available GPUs handle _gpu_handles see https rocm docs amd com projects amdsmi en docs- py-interface_readme_link html engine_usage = amdsmi amdsmi_get_gpu_activity handle gpu_uuid = amdsmi amdsmi_get_gpu_device_uuid handle gpu_utilization = engine_usage gfx_activity gpu_mem_utilization = gpu_utilization umc_activity mem_info = amdsmi amdsmi_get_gpu_memory_usage handle allocate_mem_MB = mem_info vram_usage total_mem_MB = mem_info vram_total allocate_mem_percent = allocate_mem_MB total_mem_MB gpu_data_list append GpuData uuid=gpu_uuid utilization=gpu_utilization mem_utilization=gpu_mem_utilization allocated_mem=allocate_mem_percent allocated_mem_value=allocate_mem_MB total_mem_value=total_mem_MB gpu_data_list _initial_gpu_handler - None Initializes GPU handlers gpus available updates log summary info try _has_pynvml _gpu_lib_detected = pynvml Todo investigate we can use device uuid instead index there chance gpu index can change when gpu rebooted _gpu_handles = pynvml nvmlDeviceGetHandleByIndex i i range pynvml nvmlDeviceGetCount _has_amdsmi _gpu_lib_detected = amdsmi _gpu_handles = amdsmi amdsmi_get_processor_handles _num_of_cpus = psutil cpu_count logical=True update summary info _metadata gpu_count = len _gpu_handles _metadata cpu_count = _num_of_cpus _has_pynvml _has_amdsmi len _gpu_handles == _metadata gpu_type = _metadata gpu_type = _gpu_lib_detected except Exception e _metadata error = str e _shutdown_gpu_connections - None _has_amdsmi try amdsmi amdsmi_shut_down except amdsmi AmdSmiException pass _has_pynvml try pynvml nvmlShutdown except pynvml NVMLError pass _pynvml_get_per_process_gpu_info handle Any - list dict str Any processes = pynvml nvmlDeviceGetComputeRunningProcesses handle per_process_info = p processes mem = p usedGpuMemory pid = p pid info = pid pid gpu_memory mem try proc = psutil Process pid cmdline = proc cmdline info update cmd join cmdline except Exception pass finally per_process_info append info per_process_info _rocm_get_per_process_gpu_info handle Any - list dict str Any processes = amdsmi amdsmi_get_gpu_process_list handle per_process_info = p processes try proc_info = amdsmi amdsmi_get_gpu_process_info handle p except AttributeError https github com ROCm amdsmi commit c c caedbd ba e fdffa b d e BC-breaking change removes amdsmi_get_gpu_process_info API amdsmi proc_info = p info = pid proc_info pid gpu_memory proc_info memory_usage vram_mem try proc = psutil Process proc_info pid cmdline = proc cmdline info update cmd join cmdline except Exception pass finally per_process_info append info per_process_info _get_process_info - list dict str Any get_processes_running_python_tests - list Any python_test_processes = process psutil process_iter try cmd = join process cmdline processName = process name pid = process pid is_python = python processName python cmd is_pytest = pytest cmd is_python is_pytest python_test_processes append pid pid cmd cmd except Exception pass python_test_processes processes = get_processes_running_python_tests processes main - None Main function program initialize gpu management libraries pynvml_enabled = False amdsmi_enabled = False _HAS_PYNVML try pynvml nvmlInit pynvml_enabled = True except pynvml NVMLError pass _HAS_AMDSMI try amdsmi amdsmi_init amdsmi_enabled = True except amdsmi AmdSmiException pass args = parse_args usagelogger = UsageLogger log_interval=args log_interval data_collect_interval=args data_collect_interval is_debug_mode=args debug pynvml_enabled=pynvml_enabled amdsmi_enabled=amdsmi_enabled gracefully exit script when pid killed signal signal signal SIGTERM usagelogger stop gracefully exit script when keyboard ctrl+c pressed signal signal signal SIGINT usagelogger stop start logging usagelogger start __name__ == __main__ main