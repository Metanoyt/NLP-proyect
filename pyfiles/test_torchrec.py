Owner s module dynamo sys unittest torch torch _dynamo config torch _dynamo test_case torch nn torch _dynamo test_case TestCase torch _dynamo testing CompileCounter torch testing _internal common_utils NoTest try torchrec datasets random RandomRecDataset torchrec sparse jagged_tensor JaggedTensor KeyedJaggedTensor HAS_TORCHREC = True except ImportError HAS_TORCHREC = False torch _dynamo config patch force_unspec_int_unbacked_size_like_on_torchrec_kjt=True BucketizeMod torch nn Module __init__ feature_boundaries dict str list float super __init__ bucket_w = torch nn ParameterDict boundaries_dict = key boundaries feature_boundaries items bucket_w key = torch nn Parameter torch empty len boundaries + fill_ requires_grad=True buf = torch tensor boundaries requires_grad=False register_buffer f key _boundaries buf persistent=False boundaries_dict key = buf forward features KeyedJaggedTensor - KeyedJaggedTensor weights_list = key boundaries boundaries_dict items jt = features key bucketized = torch bucketize jt weights boundaries doesn t super matter I guess hashed = torch ops fb index_hash bucketized seed= modulo=len boundaries hashed = bucketized weights = torch gather bucket_w key dim= index=hashed weights_list append weights KeyedJaggedTensor keys=features keys values=features values weights=torch cat weights_list lengths=features lengths offsets=features offsets stride=features stride length_per_key=features length_per_key HAS_TORCHREC print torchrec available skipping tests file=sys stderr TestCase = NoTest noqa F unittest skipIf HAS_TORCHREC these tests require torchrec TorchRecTests TestCase test_pooled tables = nn EmbeddingBag b nn EmbeddingBag b nn EmbeddingBag b embedding_groups = b b b b counter = CompileCounter torch compile backend=counter fullgraph=True dynamic=True f id_list_features KeyedJaggedTensor id_list_jt_dict dict str JaggedTensor = id_list_features to_dict pooled_embeddings = TODO run feature processor emb_module feature_names tables features_dict = id_list_jt_dict feature_name feature_names f = features_dict feature_name pooled_embeddings feature_name = emb_module f values f offsets pooled_embeddings_by_group = group_name group_embedding_names embedding_groups items group_embeddings = pooled_embeddings name name group_embedding_names pooled_embeddings_by_group group_name = torch cat group_embeddings dim= pooled_embeddings_by_group dataset = RandomRecDataset keys= b b b batch_size= hash_size= ids_per_feature= num_dense= di = iter dataset unsync should work d = next di sparse_features unsync d = next di sparse_features unsync d = next di sparse_features unsync r = f d r = f d r = f d assertEqual counter frame_count counter frame_count = sync should work too d = next di sparse_features sync d = next di sparse_features sync d = next di sparse_features sync r = f d r = f d r = f d assertEqual counter frame_count export only works unsync gm = torch _dynamo export f next di sparse_features unsync graph_module gm print_readable assertEqual gm d r assertEqual gm d r assertEqual gm d r test_bucketize mod = BucketizeMod f features = KeyedJaggedTensor from_lengths_sync keys= f values=torch tensor lengths=torch tensor weights=torch tensor unsync f x This trick populate computed cache instruct ShapeEnv they re all sizey x to_dict mod x torch _dynamo export f aten_graph=True features graph_module print_readable unittest expectedFailure test_simple jag_tensor = KeyedJaggedTensor values=torch tensor keys= index_ index_ lengths=torch tensor sync ordinarily would trigger one specialization assertEqual jag_tensor length_per_key counter = CompileCounter torch compile backend=counter fullgraph=True f jag_tensor The indexing here requires more symbolic reasoning doesn t work right now jag_tensor index_ values sum f jag_tensor assertEqual counter frame_count jag_tensor = KeyedJaggedTensor values=torch tensor keys= index_ index_ lengths=torch tensor sync f jag_tensor assertEqual counter frame_count __name__ == __main__ torch _dynamo test_case run_tests run_tests