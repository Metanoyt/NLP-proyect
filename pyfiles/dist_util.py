argparse functools importlib os torch torch distributed dist torch nn nn torch _dynamo testing reduce_to_scalar_loss torch distributed algorithms _checkpoint checkpoint_wrapper apply_activation_checkpointing checkpoint_wrapper CheckpointImpl torch distributed fsdp FullyShardedDataParallel FSDP torch distributed fsdp wrap ModuleWrapPolicy try torchbench setup_torchbench_cwd except ImportError torchbench setup_torchbench_cwd setup rank world_size os environ MASTER_ADDR = os getenv MASTER_ADDR localhost os environ MASTER_PORT = os getenv MASTER_PORT os environ RANK = os getenv RANK os environ WORLD_SIZE = os getenv WORLD_SIZE dist init_process_group nccl cleanup dist destroy_process_group CustomLinear torch nn Module __init__ b super __init__ weight = nn Parameter torch randn b forward x torch mm x weight MyModule torch nn Module __init__ b super __init__ net = nn Sequential nn Linear b nn ReLU forward x net x ToyModel nn Module __init__ super __init__ net = nn Sequential nn Linear nn ReLU + nn Linear nn ReLU + MyModule + MyModule + MyModule + MyModule + MyModule + MyModule + MyModule + MyModule + MyModule + nn Linear forward x net x model_iter_fn model example_inputs collect_outputs=False outputs = model example_inputs loss = reduce_to_scalar_loss outputs loss backward collect_outputs outputs get_model args args torchbench_model setup_torchbench_cwd module = importlib import_module f torchbenchmark models args torchbench_model benchmark_cls = getattr module Model None bm = benchmark_cls test= train device=args device batch_size=args batch_size model inputs = bm get_module args toy_model model = ToyModel inputs = torch randn raise argparse ArgumentError args torchbench_model message= Must specify model model inputs fsdp_checkpointing_base model blocks apply activation checkpointing model returns None model updated directly non_reentrant_wrapper = functools partial checkpoint_wrapper offload_to_cpu=False checkpoint_impl=CheckpointImpl NO_REENTRANT check_fn submodule isinstance submodule blocks apply_activation_checkpointing model checkpoint_wrapper_fn=non_reentrant_wrapper check_fn=check_fn MODEL_FSDP_WRAP = toy_model MyModule apply_fsdp args model use_checkpointing=False use_wrap_policy=True wrap_policy = None blocks = MODEL_FSDP_WRAP toy_model model __class__ ToyModel args torchbench_model use_wrap_policy wrap_policy = ModuleWrapPolicy blocks model = FSDP model auto_wrap_policy=wrap_policy use_orig_params=True use_checkpointing fsdp_checkpointing_base model blocks model