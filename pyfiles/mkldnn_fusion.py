mypy allow-untyped-defs functools operator functools reduce typing Any Callable torch torch _dynamo utils counters torch fx experimental symbolic_shapes has_free_symbols torch utils _ordered_set OrderedSet ir lowering lowerings L pattern_matcher Arg CallFunction filter_nodes get_arg_value KeywordArg MULTIPLE utils is_mkldnn_bf _supported is_mkldnn_fp _supported SUPPORTED_MKLDNN_DEVICES virtualized ops V freezing_patterns register_freezing_graph_pattern post_grad register_lowering_pattern quantization _register_int _woq_concat_linear_pattern _register_quantization_lowerings _register_quantization_weight_pack_pass _register_woq_lowerings torch _C _has_mkldnn aten = torch ops aten mkldnn = torch ops mkldnn prims = torch ops prims _conv_args = Arg _ range _linear_args = Arg _ range _conv_transpose_args = Arg _ range MkldnnDeviceOpBase get_linear_transpose_weight weight_node raise NotImplementedError pack_conv_weight graph is_transposed weight constant_args input_size raise NotImplementedError pack_linear_weight graph is_lp_weight transpose_weight_node batch_size raise NotImplementedError pack_linear graph is_lp_weight batch_size input packed_weight_node bias raise NotImplementedError CpuMkldnnDeviceOp MkldnnDeviceOpBase get_linear_transpose_weight weight_node packed_weight_node = weight_node assert packed_weight_node target == mkldnn _reorder_linear_weight transpose_weight_node = packed_weight_node args assert transpose_weight_node target aten permute default transpose_weight_node pack_conv_weight graph is_transposed weight constant_args input_size packed_weight_op = mkldnn _reorder_convolution_weight is_transposed packed_weight_op = mkldnn _reorder_convolution_transpose_weight mkldnn_reorder_conv_weight padding stride dilation groups input_size packed_weight_inputs = weight + tuple constant_args + input_size graph create_node call_function packed_weight_op args=packed_weight_inputs pack_linear_weight graph is_lp_weight transpose_weight_node batch_size For bfloat dynamic shape path using input size hint pack weight better performance packed_weight_inputs = transpose_weight_node batch_size node shape_env size_hint batch_size node expr has_free_symbols batch_size batch_size MKL packed matrix can t copied different address because internal implementation depends alignment internally-stored metadata In aot mode we need firstly save packed weight when loading will different address which doesn t work Disable MKL prepack linear AOT mode Disable MKL prepack linear when batch_size has free symbols packed_weight_op = mkldnn _reorder_linear_weight is_lp_weight mkldnn _is_mkldnn_acl_supported V aot_compilation has_free_symbols batch_size torch ops mkl _mkl_reorder_linear_weight graph create_node call_function packed_weight_op args=packed_weight_inputs pack_linear graph is_lp_weight batch_size input packed_weight_node bias packed_linear_inputs tuple Any = input packed_weight_node transpose_weight_node = packed_weight_node args is_lp_weight mkldnn _is_mkldnn_acl_supported V aot_compilation has_free_symbols batch_size packed_linear_inputs += bias none packed_linear_op Callable Any = mkldnn _linear_pointwise default packed_linear_inputs += transpose_weight_node bias batch_size packed_linear_op = torch ops mkl _mkl_linear graph create_node call_function packed_linear_op packed_linear_inputs XpuMkldnnDeviceOp MkldnnDeviceOpBase pack_conv_weight graph is_transposed weight constant_args input_size assert is_transposed mkldnn _convolution_transpose_pointwise currently implemented XPU device weight _get_mkldnn_device_op device_type str - MkldnnDeviceOpBase Returns MKLDNN device operation based current device type device_type == cpu CpuMkldnnDeviceOp device_type == xpu XpuMkldnnDeviceOp raise RuntimeError f MKLDNN supported device_type device _is_valid_grouped_gemm_fusion computation_nodes Here we check More than GEMM nodes has been found All GEMM nodes share same activation All GEMM nodes have same weight size different wgt node computation_op = mkldnn _linear_pointwise default act = computation_nodes args wgt = computation_nodes args wgt_size = wgt meta get val size type ignore union-attr len computation_nodes = all node target == computation_op node args == act node args meta get val size == wgt_size node args = wgt gemm_idx == gemm_idx node enumerate computation_nodes grouped_gemm_pass graph torch fx Graph Group GEMM has multi output nodes which complicated define Pattern Use below way connect pattern lowering TODO Use MultiOutputPattern current limitation pattern requires fixed number output nodes Extend support Group GEMM pattern matcher computation_op = mkldnn _linear_pointwise default mkldnn_lowerings grouped_gemm_lowering node graph find_nodes op= call_function target=computation_op node _erased isinstance node meta get val torch Tensor node meta val device type == cpu act = node args users = list act users _is_valid_grouped_gemm_fusion users graph inserting_before node grouped_gemm_node = graph create_node call_function grouped_gemm_lowering act user args user users user args user users grouped_gemm_node meta val = user meta val user users graph inserting_after grouped_gemm_node gemm_idx user enumerate users assert user target == computation_op get_item = graph create_node call_function operator getitem grouped_gemm_node gemm_idx user replace_all_uses_with get_item graph erase_node user _conv_call users= CallFunction mkldnn _convolution_pointwise default _conv_args _users=users _linear_call users= CallFunction mkldnn _linear_pointwise default _linear_args _users=users _conv_transpose_call users= CallFunction mkldnn _convolution_transpose_pointwise default _conv_transpose_args _users=users _to_float input_call users= CallFunction prims convert_element_type default input_call KeywordArg to_float _users=users _to_bf input_call CallFunction prims convert_element_type default input_call KeywordArg to_bf _users= _to_fp input_call CallFunction prims convert_element_type default input_call KeywordArg to_fp _users= _unary_fusion_pattern unary_fusion call_fn users lowp_dtype only insert to_dtype lowp_dtype True computation_call = _to_float call_fn users=users lowp_dtype call_fn users=users out = unary_fusion computation_call lowp_dtype == torch bfloat _to_bf out lowp_dtype == torch float _to_fp out out _gelu_fusion_ computation_call CallFunction aten mul CallFunction aten mul computation_call CallFunction aten add CallFunction aten erf CallFunction aten mul computation_call _gelu_fusion_ computation_call CallFunction aten mul CallFunction aten mul computation_call CallFunction aten add CallFunction aten tanh CallFunction aten mul CallFunction aten add computation_call CallFunction aten mul CallFunction aten mul CallFunction aten mul computation_call computation_call computation_call _hardswish_fusion computation_call CallFunction aten div CallFunction aten mul computation_call CallFunction aten clamp_max CallFunction aten clamp_min CallFunction aten add computation_call _silu_fusion computation_call CallFunction aten mul computation_call CallFunction aten sigmoid computation_call _hardsigmoid_fusion computation_call CallFunction aten div CallFunction aten clamp_max CallFunction aten clamp_min CallFunction aten add computation_call _leaky_relu_fusion computation_call CallFunction aten where CallFunction aten gt computation_call computation_call CallFunction aten mul computation_call KeywordArg negative_slope _hardtanh_fusion computation_call CallFunction aten clamp_max CallFunction aten clamp_min computation_call KeywordArg min_value KeywordArg max_value _combined_fusion computation_call elementwise_op CallFunction elementwise_op computation_call binary_op other computation_op _binary_fusion_v computation_call binary_fn CallFunction binary_fn KeywordArg other computation_call binary_op computation_op other _binary_fusion_v computation_call binary_fn CallFunction binary_fn computation_call KeywordArg other _is_single_computation_op computation_op lowp_dtype=None fn match computation_nodes = filter_nodes match nodes computation_op lowp_dtype output_node_meta = match output_node meta get val output_node_meta dtype = lowp_dtype False len computation_nodes False any n args - = none n computation_nodes False True fn _is_valid_computation_unary_fusion computation_op lowp_dtype=None fn match matched = _is_single_computation_op computation_op lowp_dtype match computation_node = filter_nodes match nodes computation_op lowp_dtype conversion_dtype_nodes = filter_nodes match nodes prims convert_element_type default len conversion_dtype_nodes = False fusion pattern always form computation_op + to_float + unary_op + to_bfloat computation_node == conversion_dtype_nodes args to_float = conversion_dtype_nodes args to_lp = conversion_dtype_nodes args to_float = conversion_dtype_nodes args to_lp = conversion_dtype_nodes args matched = matched to_float == torch float to_lp == lowp_dtype matched fn _register_unary_fusion_lowering pattern unary_attr computation_op lowp_dtype=None register_lowering_pattern pattern extra_check=_is_valid_computation_unary_fusion computation_op lowp_dtype fn match args kwargs computation_args = list args - + unary_attr op_name unary_attr scalars_attr unary_attr algorithm_attr counters inductor mkldnn_unary_fusion_matcher_count += counters inductor mkldnn_unary_fusion_matcher_nodes += len match nodes L computation_op computation_args fn _register_leaky_relu_fusion_lowering pattern computation_op lowp_dtype=None register_lowering_pattern pattern extra_check=_is_single_computation_op computation_op lowp_dtype fn match args kwargs negative_slope = kwargs get negative_slope isinstance negative_slope ir TensorBox matched = False inp Number matched = True lowp_dtype dtype = kwargs get to_float dtype = kwargs get to_bf lowp_dtype == torch bfloat kwargs get to_fp matched = matched dtype == torch float dtype == lowp_dtype computation_args = list args counters inductor mkldnn_unary_fusion_matcher_count += counters inductor mkldnn_unary_fusion_matcher_nodes += len match nodes matched computation_args = computation_args - + leaky_relu negative_slope L computation_op computation_args computation_args += none out = L computation_op computation_args lowp_dtype out = L prims convert_element_type default out dtype=torch float out = L aten where L aten gt out out L aten mul out negative_slope lowp_dtype out = L prims convert_element_type default out dtype=dtype type ignore possibly-undefined out fn _register_hardtanh_fusion_lowering pattern computation_op lowp_dtype=None register_lowering_pattern pattern extra_check=_is_single_computation_op computation_op lowp_dtype fn match args kwargs min_value = kwargs get min_value max_value = kwargs get max_value isinstance min_value ir TensorBox isinstance max_value ir TensorBox matched = False inp Number assert max_value None matched = min_value = max_value lowp_dtype dtype = kwargs get to_float dtype = kwargs get to_bf lowp_dtype == torch bfloat kwargs get to_fp matched = matched dtype == torch float dtype == lowp_dtype computation_args = list args counters inductor mkldnn_unary_fusion_matcher_count += counters inductor mkldnn_unary_fusion_matcher_nodes += len match nodes matched computation_args = computation_args - + hardtanh min_value max_value L computation_op computation_args out = L computation_op computation_args lowp_dtype out = L prims convert_element_type default out dtype=torch float out = L aten clamp_max L aten clamp_min out min_value max_value lowp_dtype out = L prims convert_element_type default out dtype=dtype type ignore possibly-undefined out fn _binary_attr = aten add add ops add add aten sub sub ops sub sub _is_valid_binary match computation_op binary_op binary_nodes = filter_nodes match nodes binary_op len binary_nodes False get_meta_value argument torch fx node Argument Only torch fx Node expected have meta isinstance argument torch fx Node argument meta get val None None any isinstance get_meta_value n args torch Tensor isinstance get_meta_value n args torch Tensor n binary_nodes False check alpha one any get_arg_value n kwarg_name= alpha = get_arg_value n kwarg_name= alpha None n binary_nodes False _check_input_sizes n computation_op Check tensor shape other node same can broadcasted tensor shape computation node computation_node = n args n args match kwargs other n args assert computation_node target == computation_op computation_node_size = get_meta_value computation_node size computation_op mkldnn _linear_pointwise default broadcast_sizes = len computation_node_size = broadcast_sizes = torch Size _ range len computation_node_size - + computation_node_size - assert len computation_node_size broadcast_sizes = torch Size computation_node_size computation_node_size + _ range len computation_node_size - torch Size computation_node_size + _ range len computation_node_size - torch Size _ range len computation_node_size get_meta_value match kwargs other size computation_node_size + broadcast_sizes any _check_input_sizes n computation_op get_meta_value n args device = get_meta_value n args device get_meta_value n args dtype = get_meta_value n args dtype n binary_nodes False check args args same any n args == n args n binary_nodes False True _is_valid_computation_binary computation_op binary_op other_index=None fn match _is_single_computation_op computation_op match False _is_valid_binary match computation_op binary_op False True fn _get_remaining_users extra_input_node compute_node Think about pattern ReLU \ Conv \ Conv \ Add Although extra input node ReLU has more than users Conv Add The Conv ancestor node current compute node Conv This indicates buffer ReLU has completed all its usage So we can safely make changes now doing Conv - Add inplace fusion Take above case example extra_input_node ReLU compute_node Conv _get_remaining_users will users extra_input_node which ancestor node compute_node _is_ancestor_node _current_node _ancestor_node Check whether _ancestor_node ancestor node _current_node _node_list = _current_node _visited_nodes = OrderedSet torch fx Node while len _node_list = _current_node = _node_list pop _current_node _visited_nodes _visited_nodes add _current_node _current_node == _ancestor_node True isinstance _current_node torch fx Node _current_node op placeholder output get_attr input _current_node all_input_nodes _node_list append input noqa PERF False user user list extra_input_node users _is_ancestor_node compute_node user _is_valid_computation_binary_inplace computation_op binary_op other_index fn match _is_valid_computation_binary computation_op binary_op match False binary_nodes = filter_nodes match nodes binary_op _get_compute_node _binary_node _other_index assert len _binary_node all_input_nodes == Binary node should have input nodes _compute_index = _other_index == _binary_node args _compute_index _other_input_not_inplaceable _binary_node _other_index _compute_node = _get_compute_node _binary_node _other_index len _get_remaining_users _binary_node args _other_index _compute_node _binary_node args _other_index == _compute_node args any _other_input_not_inplaceable n other_index n binary_nodes False any pyrefly ignore missing-attribute n args other_index op placeholder output n binary_nodes False True fn _register_binary_unary_fusion_lowering pattern computation_op binary_op fusion_op unary_attr=None register_lowering_pattern pattern extra_check=_is_valid_computation_binary computation_op binary_op fn match args kwargs other = kwargs get other assert isinstance other ir TensorBox binary_attr = _binary_attr binary_op args_list = list args computation_args = args_list other + args_list - + binary_attr len args_list unary_attr None computation_args += unary_attr op_name unary_attr scalars_attr unary_attr algorithm_attr computation_args += None None counters inductor mkldnn_conv_binary_unary_fusion_matcher_count += counters inductor mkldnn_conv_binary_unary_fusion_matcher_nodes += len match nodes L fusion_op computation_args fn _can_be_inplace _other isinstance _other data ir BaseView len _other get_inputs_that_alias_output _register_binary_unary_maybe_inplace_fusion_lowering pattern computation_op binary_op inplace_fusion_op outplace_fusion_op unary_attr=None other_index=None register_lowering_pattern pattern extra_check=_is_valid_computation_binary_inplace computation_op binary_op other_index fn match args kwargs other = kwargs get other assert isinstance other ir TensorBox binary_attr = _binary_attr binary_op args_list = list args computation_args = args_list other + args_list - + binary_attr len args_list unary_attr None computation_args += unary_attr op_name unary_attr scalars_attr unary_attr algorithm_attr computation_args += None None counters inductor mkldnn_conv_binary_unary_fusion_matcher_count += counters inductor mkldnn_conv_binary_unary_fusion_matcher_nodes += len match nodes Make sure other alias mutation fx side doesn t has such info other realize _can_be_inplace other other data shape = list match nodes meta val size L outplace_fusion_op computation_args L inplace_fusion_op computation_args fn computation_ops = mkldnn _convolution_pointwise default mkldnn _linear_pointwise default mkldnn _convolution_transpose_pointwise default UnaryAttr __init__ op_name str scalars_attr=None algorithm_attr=None - None op_name = op_name scalars_attr = scalars_attr scalars_attr algorithm_attr = algorithm_attr algorithm_attr _register_unary_fusion computation_call_fns = _conv_call _linear_call _conv_transpose_call _unary_fusion_patterns lowp_dtype replacement_unary_fusion_patterns = UnaryAttr gelu algorithm_attr= tanh _unary_fusion_pattern _gelu_fusion_ call_fn lowp_dtype call_fn computation_call_fns UnaryAttr gelu algorithm_attr= none _unary_fusion_pattern _gelu_fusion_ call_fn lowp_dtype call_fn computation_call_fns UnaryAttr hardswish _unary_fusion_pattern _hardswish_fusion call_fn lowp_dtype call_fn computation_call_fns UnaryAttr hardsigmoid _unary_fusion_pattern _hardsigmoid_fusion call_fn lowp_dtype call_fn computation_call_fns UnaryAttr swish _unary_fusion_pattern _silu_fusion call_fn lowp_dtype call_fn computation_call_fns lowp_dtype call_user = call_fn users= call_fn computation_call_fns replacement_unary_fusion_patterns update UnaryAttr relu _combined_fusion u aten relu u call_user UnaryAttr sigmoid _combined_fusion u aten sigmoid u call_user UnaryAttr tanh _combined_fusion u aten tanh u call_user replacement_unary_fusion_patterns lowp_dtype torch bfloat torch float None replace_patterns = _unary_fusion_patterns lowp_dtype unary_attr patterns replace_patterns items _register_unary_fusion_lowering patterns unary_attr computation_ops lowp_dtype _register_unary_fusion_lowering patterns unary_attr computation_ops lowp_dtype _register_unary_fusion_lowering patterns unary_attr computation_ops lowp_dtype _leaky_relu_patterns = _unary_fusion_pattern _leaky_relu_fusion call_fn lowp_dtype call_fn computation_call_fns pattern computation_op zip _leaky_relu_patterns computation_ops _register_leaky_relu_fusion_lowering pattern computation_op lowp_dtype hardtanh_patterns = _unary_fusion_pattern _hardtanh_fusion call_fn lowp_dtype call_fn computation_call_fns pattern computation_op zip hardtanh_patterns computation_ops _register_hardtanh_fusion_lowering pattern computation_op lowp_dtype _register_inplace_fusion binary_ops = aten add ops add inplace_fusion_op = mkldnn _convolution_pointwise_ binary outplace_fusion_op = mkldnn _convolution_pointwise binary conv_call = _conv_call users= conv_op = computation_ops binary_op binary_ops binary_v = _binary_fusion_v conv_call binary_op binary_unary_v = _combined_fusion binary_v aten relu _register_binary_unary_maybe_inplace_fusion_lowering binary_unary_v conv_op binary_op inplace_fusion_op outplace_fusion_op other_index= unary_attr=UnaryAttr relu _register_binary_unary_maybe_inplace_fusion_lowering binary_v conv_op binary_op inplace_fusion_op outplace_fusion_op other_index= binary_v = _binary_fusion_v conv_call binary_op binary_unary_v = _combined_fusion binary_v aten relu _register_binary_unary_maybe_inplace_fusion_lowering binary_unary_v conv_op binary_op inplace_fusion_op outplace_fusion_op other_index= unary_attr=UnaryAttr relu _register_binary_unary_maybe_inplace_fusion_lowering binary_v conv_op binary_op inplace_fusion_op outplace_fusion_op other_index= _register_binary_fusion binary_ops = aten add ops add aten sub ops sub fusion_ops = mkldnn _convolution_pointwise binary mkldnn _linear_pointwise binary _computation_user_ = _conv_call users= _linear_call users= computation_call computation_op fusion_op zip _computation_user_ computation_ops - fusion_ops binary_op binary_ops pattern = _binary_fusion_v computation_call binary_op _register_binary_unary_fusion_lowering pattern computation_op binary_op fusion_op binary_op aten add ops add pattern = _binary_fusion_v computation_call binary_op _register_binary_unary_fusion_lowering pattern computation_op binary_op fusion_op _register_binary_unary_fusion binary_ops = aten add ops add aten sub ops sub fusion_ops = mkldnn _convolution_pointwise binary _computation_user_ = _conv_call users= computation_call computation_op fusion_op zip _computation_user_ computation_ops - fusion_ops binary_op binary_ops pattern_v = _combined_fusion _binary_fusion_v computation_call binary_op aten relu _register_binary_unary_fusion_lowering pattern_v computation_op binary_op fusion_op unary_attr=UnaryAttr relu binary_op aten add ops add pattern_v = _combined_fusion _binary_fusion_v computation_call binary_op aten relu _register_binary_unary_fusion_lowering pattern_v computation_op binary_op fusion_op unary_attr=UnaryAttr relu _recover_linear convert reshape+linear+reshape single linear applying fusion path concat_linear pass_number= - mkldnn_linear_pack pass_number= - _recover_linear pass_number= register_freezing_graph_pattern CallFunction aten reshape default CallFunction mkldnn _linear_pointwise default CallFunction aten reshape default Arg KeywordArg reshape_ _users=MULTIPLE Arg Arg Arg Arg Arg KeywordArg reshape_ pass_number= reshape_linear_reshape_pattern match args kwargs get_val val val isinstance val int val meta get val reshape_ = kwargs get reshape_ reshape_ = kwargs get reshape_ assert isinstance reshape_ list assert isinstance reshape_ list assert len reshape_ == graph = match graph reshape_ _node = match output_node linear_input_node = reshape_ _node args args args check linear s input s shape - == reshape_ - check product reshape_ - == reshape_ can_remove_reshape = linear_input_node meta get val shape - == torch Size get_val val val reshape_ - can_remove_reshape = can_remove_reshape reduce operator mul get_val val val reshape_ - == get_val reshape_ can_remove_reshape repl = graph call_function mkldnn _linear_pointwise default args repl meta update reshape_ _node meta reshape_ _node replace_all_uses_with repl old_linear_node = reshape_ _node args reshape_ _node = old_linear_node args graph erase_node reshape_ _node graph erase_node old_linear_node len reshape_ _node users == graph erase_node reshape_ _node counters inductor mkldnn_reshape_linear_reshape_matcher_count += counters inductor mkldnn_reshape_linear_reshape_matcher_nodes += len match nodes is_linear_add_bias match add_node = match output_node linear_node = add_node args device_type = add_node meta get val device type mkldnn_device_op = _get_mkldnn_device_op device_type transpose_weight_node = mkldnn_device_op get_linear_transpose_weight linear_node args weight_meta = transpose_weight_node args meta get val bias_node = add_node args isinstance bias_node int we only folding bias constant False bias_meta = add_node args meta get val weight_meta None bias_meta None False bias_meta dtype = weight_meta dtype False linear_node args None bias_meta dim == bias_meta size == weight_meta size convert linear+bias single linear applying fusion path register_freezing_graph_pattern CallFunction aten add Tensor CallFunction mkldnn _linear_pointwise default _linear_args Arg pass_number= extra_check=is_linear_add_bias linear_bias_pattern match args graph = match graph add_node = match output_node linear_node = add_node args new_args = list linear_node args new_args = add_node args repl = graph call_function mkldnn _linear_pointwise default tuple new_args repl meta update add_node meta add_node replace_all_uses_with repl match erase_nodes counters inductor mkldnn_linear_bias_matcher_count += counters inductor mkldnn_linear_bias_matcher_nodes += len match nodes _is_packable_mkldnn_rnn_layer match lstm_node = match output_node POS_WEIGHTS = POS_INPUTS = POS_ARGS = POS_WEIGHTS + POS_INPUTS Weights should Constant any lstm_node args POS_WEIGHT op = get_attr POS_WEIGHT POS_WEIGHTS False Meta info weights inputs should available any lstm_node args POS_ARG meta get val None POS_ARG POS_ARGS False Check device any lstm_node args POS_ARG meta get val device type = cpu POS_ARG POS_ARGS False Check dtype any lstm_node args POS_ARG meta get val dtype == torch bfloat is_mkldnn_bf _supported cpu POS_ARG POS_ARGS False any lstm_node args POS_ARG meta get val dtype == torch float is_mkldnn_fp _supported cpu POS_ARG POS_ARGS False True _is_packable_convolution match Check node supported MKLDNN convolution conv_node = match output_node device_type = conv_node meta get val device type The operator mkldnn _convolution_transpose_pointwise currently implemented XPU device match kwargs is_transposed device_type == xpu False input_meta_value = conv_node args meta get val weight_meta_value = conv_node args meta get val input_meta_value None weight_meta_value None False input_size = input_meta_value shape conv_node args op = get_attr False meta_value input_meta_value weight_meta_value meta_value None meta_value device type SUPPORTED_MKLDNN_DEVICES meta_value dim = meta_value dim = False input_meta_value dtype == torch bfloat weight_meta_value dtype == torch bfloat is_mkldnn_bf _supported device_type False input_meta_value dtype == torch float weight_meta_value dtype == torch float is_mkldnn_fp _supported device_type False is_transposed = conv_node args - is_transposed TODO Support dynamic shape case MKLDNN conv transpose has_free_symbols input_size False groups = conv_node args - in_channels = weight_meta_value size doesn t support group_depthwise_conv_transpose groups groups == in_channels False Port aten src ATen native Convolution cpp is_output_padding_big output_paddings = conv_node args - strides = conv_node args any output_padding = stride output_padding stride zip output_paddings strides False True _is_packable_linear match Check node supported MKLDNN linear is_const_or_cat_by_const weight weight op == get_attr True weight target = aten cat default False all arg op == get_attr arg weight args linear_node = match output_node mkldnn linear only supports beta= alpha= linear_node target aten addmm default alpha = linear_node kwargs get alpha beta = linear_node kwargs get beta beta = beta = alpha = False weight_idx aten mm aten addmm weight_idx = linear_node target aten addmm default is_const_or_cat_by_const linear_node args weight_idx False input_meta_value = linear_node args weight_idx - meta get val weight_meta_value = linear_node args weight_idx meta get val input_meta_value None weight_meta_value None False input_meta_value dtype == torch float weight_meta_value dtype == torch float False is_lp_weight = weight_meta_value dtype torch bfloat torch float reduced_f _matmul_enabled = torch backends mkldnn matmul fp _precision type ignore attr-defined bf tf use_reduced_f _for_fp _weight = reduced_f _matmul_enabled weight_meta_value dtype == torch float compute_with_lp = is_lp_weight use_reduced_f _for_fp _weight x fp mkl should enabled aarch use mkldnn op fp well acl enabled compute_with_lp mkldnn _is_mkldnn_acl_supported torch _C has_mkl False meta_value input_meta_value weight_meta_value meta_value None meta_value device type = cpu meta_value dim = False weight_idx == bias_meta_value = linear_node args meta get val bias_meta_value None meta_value device type = cpu bias_meta_value dim = bias_meta_value size = weight_meta_value size False device_type = input_meta_value device type input_meta_value dtype == torch bfloat weight_meta_value dtype == torch bfloat is_mkldnn_bf _supported device_type False input_meta_value dtype == torch float weight_meta_value dtype == torch float is_mkldnn_fp _supported device_type False True _aten_conv_args = Arg Arg Arg Arg Arg Arg KeywordArg is_transposed Arg Arg _aten_mkldnn_rnn_layer_args = Arg input Arg weight Arg weight Arg weight Arg weight Arg hx_ Arg cx_ KeywordArg reverse reverse Arg batch_sizes Arg mode Arg hidden_size Arg num_layers Arg has_biases Arg bidirectional Arg batch_first Arg train _register_weight_pack_pass register_freezing_graph_pattern CallFunction aten convolution default _aten_conv_args extra_check=_is_packable_convolution convolution match args kwargs is_transposed = kwargs get is_transposed assert isinstance is_transposed bool graph = match graph conv_node = match output_node device_type = conv_node args meta get val device type mkldnn_device_op = _get_mkldnn_device_op device_type input_size = conv_node args meta get val shape graph inserting_before conv_node constant_args = args args args args - packed_conv_op = mkldnn _convolution_pointwise default is_transposed constant_args insert args - output_padding packed_conv_op = mkldnn _convolution_transpose_pointwise default has_free_symbols input_size packed_weight_node = mkldnn_device_op pack_conv_weight graph is_transposed args constant_args input_size assert is_transposed For dynamic shape case we need pack weight runtime packed_weight_node = args packed_conv_inputs = args packed_weight_node args + tuple constant_args + none packed_conv_node = graph create_node call_function packed_conv_op tuple packed_conv_inputs conv_node replace_all_uses_with packed_conv_node packed_conv_node meta update conv_node meta graph erase_node conv_node counters inductor mkldnn_conv_weight_pack_matcher_count += counters inductor mkldnn_conv_weight_pack_matcher_nodes += len match nodes register_freezing_graph_pattern CallFunction aten mkldnn_rnn_layer default _aten_mkldnn_rnn_layer_args extra_check=_is_packable_mkldnn_rnn_layer mkldnn_rnn_layer match args kwargs get_item graph node index graph call_function operator getitem node index graph = match graph lstm_node = match output_node weight weight = args reverse = kwargs get reverse packed_lstm_op = aten mkldnn_rnn_layer default hidden_size = args has_biases = args batch_first = args graph inserting_before lstm_node packed_weight_op = mkldnn _reorder_mkldnn_rnn_layer_weight default packed_weight_inputs = weight weight hidden_size reverse has_biases batch_first packed_weight_node = graph create_node call_function packed_weight_op packed_weight_inputs name packed_weight_items = get_item graph packed_weight_node i i range pack_lstm_inputs = args packed_weight_items args args args args reverse args packed_lstm_node = graph create_node call_function packed_lstm_op args=pack_lstm_inputs lstm_node replace_all_uses_with packed_lstm_node packed_lstm_node meta update lstm_node meta graph erase_node lstm_node counters inductor mkldnn_rnn_weight_pack_matcher_count += counters inductor mkldnn_rnn_weight_pack_matcher_nodes += len match nodes register_freezing_graph_pattern CallFunction aten addmm default Arg Arg Arg beta=KeywordArg beta alpha=KeywordArg alpha extra_check=_is_packable_linear pass_number= register_freezing_graph_pattern CallFunction aten mm default Arg Arg extra_check=_is_packable_linear pass_number= linear match args kwargs graph = match graph linear_node = match output_node input = args linear_node target aten mm default args bias = None linear_node target aten mm default linear_node target aten addmm default linear_node kwargs get beta == args weight = args linear_node target aten mm default args device_type = input meta get val device type mkldnn_device_op = _get_mkldnn_device_op device_type graph inserting_before linear_node transpose_weight_node = graph create_node call_function aten permute default weight weight_dtype = weight meta get val dtype is_lp_weight = weight_dtype torch bfloat torch float reduced_f _matmul_enabled = torch backends mkldnn matmul fp _precision bf tf type ignore attr-defined use_reduced_f _for_fp _weight = reduced_f _matmul_enabled weight_dtype == torch float compute_with_lp = is_lp_weight use_reduced_f _for_fp _weight batch_size = input meta get val shape packed_weight_node = mkldnn_device_op pack_linear_weight graph compute_with_lp transpose_weight_node batch_size packed_linear_node = mkldnn_device_op pack_linear graph compute_with_lp batch_size input packed_weight_node bias linear_node replace_all_uses_with packed_linear_node packed_linear_node meta update linear_node meta graph erase_node linear_node counters inductor mkldnn_linear_weight_pack_matcher_count += counters inductor mkldnn_linear_weight_pack_matcher_nodes += len match nodes _eliminate_duplicate_packed_nodes gm Combine packed weight nodes same inputs reduce memory usage example Model nn Module __init__ - None super __init__ linear = nn Linear bias=True forward x linear linear x above s packed weight nodes duplicate two linear calls have same input size torch backends mkldnn enabled torch backends mkldnn is_available gm packed_weight_ops = torch _C _nn mkldnn_reorder_conv d_weight torch _C _nn mkldnn_reorder_conv d_weight mkldnn _reorder_convolution_transpose_weight mkldnn _reorder_linear_weight mkldnn _reorder_mkldnn_rnn_layer_weight torch _C has_mkl packed_weight_ops append torch ops mkl _mkl_reorder_linear_weight node gm graph nodes node target packed_weight_ops len node args users user_node list node args users keys user_node target == node target user_node = node user_node args == node args user_node replace_all_uses_with node gm graph erase_node user_node functools cache _mkldnn_fusion_init TODO aarch enable op fusion acl once supports fused operators Disabling now Otherwise even matmul innerproduct can accelerated acl torch backends mkldnn enabled torch backends mkldnn is_available torch ops mkldnn _is_mkldnn_acl_supported _register_unary_fusion _register_inplace_fusion _register_binary_unary_fusion _register_binary_fusion _register_quantization_lowerings _register_woq_lowerings functools cache _mkldnn_weight_pack_init torch backends mkldnn enabled torch backends mkldnn is_available _register_weight_pack_pass _recover_linear _register_quantization_weight_pack_pass _register_int _woq_concat_linear_pattern