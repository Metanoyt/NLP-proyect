Owner s oncall distributed contextlib unittest copy deepcopy functools partial torch torch nn nn torch distributed algorithms _checkpoint checkpoint_wrapper apply_activation_checkpointing checkpoint_wrapper CheckpointImpl CheckpointWrapper offload_wrapper OffloadWrapper torch distributed fsdp wrap ModuleWrapPolicy torch testing _internal common_fsdp get_devtype torch testing _internal common_utils run_tests TestCase torch utils checkpoint checkpoint _SAVED_PREFIX = _saved_ GRAD_FN_NEXT_FUNCTIONS = next_functions device_type = torch device get_devtype CheckpointWrapperTest TestCase test_load_activation_checkpointed_module lin = nn Linear bias=False lin = checkpoint_wrapper lin checkpoint_fn=checkpoint checkpoint kwargs use_reentrant=True preserve_rng_state=False state_dict = deepcopy lin state_dict Load into non-checkpoint wrapped linear module lin_new = nn Linear bias=False lin_new load_state_dict state_dict p p zip lin parameters lin_new parameters assertEqual p p assertTrue torch allclose p p Load non-checkpoint wrapped module into checkpoint wrapped one Make params different p lin_new parameters torch no_grad p add_ state_dict = deepcopy lin_new state_dict Verify checkpoint wrapped linear can load unwrapped linear lin load_state_dict state_dict p p zip lin parameters lin_new parameters assertEqual p p test_checkpoint_wrapper_kwarg_support MyModel nn Module __init__ - None super __init__ lin = nn Linear forward b c=None d=None kwargs lin lin b lin c lin d wrapper partial checkpoint_wrapper checkpoint_impl=CheckpointImpl REENTRANT partial checkpoint_wrapper checkpoint_impl=CheckpointImpl NO_REENTRANT offload_wrapper subTest wrapper=wrapper model = wrapper MyModel wrapper == offload_wrapper assertTrue isinstance model OffloadWrapper assertTrue isinstance model CheckpointWrapper Verify kwargs can passed inp = torch ones requires_grad=True out = model inp inp c=inp d=inp e=inp f=inp assertTrue isinstance out tuple assertEqual len out Without kwargs should have equivalent gradient requirements out_no_kwarg = model inp inp inp inp t t zip out_no_kwarg out assertEqual t t assertEqual t requires_grad t requires_grad Test model enforces kwarg inputs ModelEnforceKwarg nn Module __init__ - None super __init__ lin = nn Linear forward a=None b=None lin lin b model = checkpoint_wrapper ModelEnforceKwarg checkpoint_impl=CheckpointImpl REENTRANT inp = torch ones requires_grad=True out = model a=inp b=inp assertEqual len out test_checkpoint_wrapper_args_kwargs Tests checkpoint_wrapper can pass down args kwargs configure torch utils checkpoint count = contextlib contextmanager ctx_manager nonlocal count count += yield get_ctx_mgrs ctx_manager ctx_manager kwargs test torch_utils_checkpoint = torch utils checkpoint checkpoint m = checkpoint_wrapper torch nn Linear checkpoint_fn=torch_utils_checkpoint use_reentrant=False context_fn=get_ctx_mgrs m torch randn sum backward assertEqual count unittest skip test_checkpoint_wrapper_parity Tests using checkpoint_wrapper functional torch utils checkpoint same reentrant config results same maximum memory usage i e they equivalent memory usage wise Model nn Module __init__ n int use_cp bool use_wrapper bool = False use_reentrant bool = True super __init__ layers = nn ModuleList n = n use_cp = use_cp use_wrapper = use_wrapper use_reentrant = use_reentrant wrp = partial checkpoint_wrapper checkpoint_impl= CheckpointImpl REENTRANT use_reentrant CheckpointImpl NO_REENTRANT _ range n l = nn Sequential nn Linear nn Linear nn Linear use_checkpoint_wrapper = use_wrapper use_checkpoint_wrapper l = wrp l layers append l forward x i range n use_wrapper use_cp x = layers i x x = checkpoint layers i x use_reentrant=self use_reentrant x test use_checkpointing use_wrapper use_reentrant = Model use_checkpointing use_wrapper=use_wrapper use_reentrant=use_reentrant device_type type x = torch randn requires_grad=True device_type type torch get_device_module device_type type reset_peak_memory_stats loss = x sum loss backward torch get_device_module device_type type max_memory_allocated functional_no_reentrant = test use_checkpointing=True use_wrapper=False use_reentrant=False wrapper_no_reentrant = test use_checkpointing=False use_wrapper=True use_reentrant=False assertEqual functional_no_reentrant wrapper_no_reentrant functional_reentrant = test use_checkpointing=True use_wrapper=False use_reentrant=True wrapper_reentrant = test use_checkpointing=False use_wrapper=True use_reentrant=True assertEqual functional_reentrant wrapper_reentrant test_forward_missing_attributes lin = nn Linear m = nn Sequential lin lin wrapped = CheckpointWrapper m Test indexing forwarded assertEqual wrapped lin Test missing attributes forwarded m _foo = bar assertEqual wrapped _foo bar test_apply_activation_checkpointing Ensures ` apply_activation_checkpointing ` can used swap modules their checkpoint-wrapped counterparts given model LinearWithBatchNorm nn Module __init__ - None super __init__ lin = nn Linear bn = nn BatchNorm d nested_linear = nn Sequential nn Linear forward x bn nested_linear lin x MyModel nn Module __init__ - None super __init__ seq = nn Sequential LinearWithBatchNorm LinearWithBatchNorm LinearWithBatchNorm forward x seq x check_fn l isinstance l nn Linear n_linear = None i wrapper enumerate partial checkpoint_wrapper checkpoint_impl=CheckpointImpl REENTRANT partial checkpoint_wrapper checkpoint_impl=CheckpointImpl NO_REENTRANT offload_wrapper model = MyModel n_linear None n_linear = sum isinstance x nn Linear x model modules subTest wrapper=wrapper i = apply_activation_checkpointing model checkpoint_wrapper_fn=wrapper check_fn=check_fn apply_activation_checkpointing model checkpoint_wrapper_fn=wrapper auto_wrap_policy=ModuleWrapPolicy nn Linear n_linear_wrapped = sum isinstance x nn Linear x model modules n_checkpointed = sum isinstance x CheckpointWrapper OffloadWrapper x model modules assertEqual n_checkpointed n_linear_wrapped assertEqual n_linear n_linear_wrapped j range assertTrue isinstance model seq j lin CheckpointWrapper OffloadWrapper assertTrue isinstance model seq j nested_linear CheckpointWrapper OffloadWrapper inp = torch randn requires_grad=True _ range Kwarg input loss = model x=inp sum assertTrue loss requires_grad loss backward ensure checkpointed part model has gradients j range weight_lin = model seq j lin _checkpoint_wrapped_module weight bias_lin = model seq j lin _checkpoint_wrapped_module bias weight_nested_lin = model seq j nested_linear _checkpoint_wrapped_module weight bias_nested_lin = model seq j nested_linear _checkpoint_wrapped_module bias param weight_lin bias_lin weight_nested_lin bias_nested_lin assertTrue param requires_grad assertFalse param grad None test_fqn lin = nn Linear bias=False lin = checkpoint_wrapper lin state_dict = lin state_dict fqn _ lin named_parameters assertTrue fqn state_dict msg=f fqn state_dict test_checkpoint_wrapper_cpu_offload model = nn Sequential nn Linear nn Linear nn Linear device_type type Patch saved_tensor_hooks make unpack keep tensor CPU testing otherwise tensor access during DFS will cause orig unpack run transferring tensor back GPU patched_init saved_tensor_hook_obj pack_hook _ saved_tensor_hook_obj pack_hook = pack_hook testing_cpu_offload_unpack_hook packed _ tensor = packed tensor saved_tensor_hook_obj unpack_hook = testing_cpu_offload_unpack_hook orig_init = torch autograd graph saved_tensors_hooks __init__ torch autograd graph saved_tensors_hooks __init__ = patched_init model = offload_wrapper model inp = torch randn device=device_type type loss = model inp sum All autograd saved tensors should offloaded CPU offload_verified = False dfs grad_fn e dir grad_fn e startswith _SAVED_PREFIX continue saved = getattr grad_fn e isinstance saved torch Tensor assertEqual torch device cpu saved device nonlocal offload_verified offload_verified = True hasattr grad_fn GRAD_FN_NEXT_FUNCTIONS next_grad_fn _ grad_fn next_functions dfs next_grad_fn dfs loss grad_fn assertTrue offload_verified torch autograd graph saved_tensors_hooks __init__ = orig_init __name__ == __main__ run_tests