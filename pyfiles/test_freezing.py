Owner s oncall jit ruff noqa F io unittest itertools product typing Any torch torch nn nn torch nn functional F torch jit _recursive wrap_cpp_module torch testing FileCheck torch testing _internal common_cuda TEST_CUDA TEST_CUDNN torch testing _internal common_quantization skipIfNoFBGEMM torch testing _internal common_quantized override_quantized_engine torch testing _internal common_utils raise_on_run_directly set_default_dtype skipCUDAMemoryLeakCheckIf skipIfTorchDynamo TEST_WITH_ROCM torch testing _internal jit_utils JitTestCase torch utils mkldnn mkldnn_utils try torchvision HAS_TORCHVISION = True except ImportError HAS_TORCHVISION = False skipIfNoTorchVision = unittest skipIf HAS_TORCHVISION no torchvision TEST_ROCM = torch cuda is_available torch version hip None removeExceptions graph n graph findAllNodes prim RaiseException n destroy TestFreezing JitTestCase test_freeze_module M nn Module __init__ - None super __init__ = folded b = folded c = hello folded c = hi\xa folded d = folded e = folded f = hello world folded f = Over \u e \u e g = torch tensor requires_grad=True folded h = layer torch tensor requires_grad=True h = layer\xb torch tensor requires_grad=True t = torch tensor requires_grad=True folded ts = torch tensor requires_grad=True torch tensor requires_grad=True folded tt = torch tensor requires_grad=True None forward x str + str b + c + c + str d + str e + str f + str f + str g + str h + str h + str t + str ts + str tt m = torch jit script M m eval input = torch randn output_s = m forward input m _c = torch _C _freeze_module m _c buffer = io BytesIO torch jit save m _c buffer buffer seek m = torch jit load buffer Check frozen module looks below module m attributes tt = assertFalse m _c hasattr assertFalse m _c hasattr b assertFalse m _c hasattr c assertFalse m _c hasattr c assertFalse m _c hasattr d assertFalse m _c hasattr e assertFalse m _c hasattr f assertFalse m _c hasattr f assertFalse m _c hasattr g assertFalse m _c hasattr h assertFalse m _c hasattr h assertFalse m _c hasattr t assertFalse m _c hasattr ts assertFalse m _c hasattr tt output_f = m forward input assertEqual output_s output_f test_freeze_module_with_submodule SubModule nn Module __init__ - None super __init__ = b = forward x + b SubModule nn Module __init__ - None super __init__ = b = forward x b = + b TestModule nn Module __init__ - None super __init__ sub = SubModule sub = SubModule = b = forward x b = sub x + + b + sub x m = torch jit script TestModule m eval input = torch randn output_s = m forward input mf = torch jit freeze m Check frozen module looks below module m attributes sub = b = submodule module m attributes sub = b = mf = mf _c assertFalse mf hasattr sub assertFalse mf hasattr assertTrue mf hasattr b assertTrue mf hasattr sub assertTrue mf sub hasattr b verify b preserved sub assertFalse mf sub hasattr verify removed sub output_f = mf forward input assertEqual output_s output_f test_freeze_module_with_fork SubModule nn Module __init__ - None super __init__ = torch ones b = torch ones forward x b + x TestModule nn Module __init__ - None super __init__ sub = SubModule forward x fut = torch jit _fork sub forward x y_hat = sub x y = torch jit _wait fut y_hat + y m = torch jit script TestModule m eval input = torch randn output_s = m forward input mf = torch _C _freeze_module m _c Check frozen module looks below module m attributes submodule assertFalse mf hasattr assertFalse mf hasattr b output_f = mf forward input assertEqual output_s output_f test_freeze_module_with_nested_fork SubModule nn Module __init__ - None super __init__ = torch ones b = torch ones forward x b + x SubModule nn Module __init__ - None super __init__ sub = SubModule c = torch ones forward x fut = torch jit _fork sub forward x y_hat = sub x y = torch jit _wait fut y_hat + y + c TestModule nn Module __init__ - None super __init__ sub = SubModule d = forward x fut = torch jit _fork sub forward x y_hat = sub x y = torch jit _wait fut d = y_hat y + d m = torch jit script TestModule m eval input = torch randn output_s = m forward input mf = torch _C _freeze_module m _c Check frozen module looks below module m attributes submodule assertFalse mf hasattr assertFalse mf hasattr b assertFalse mf hasattr c assertTrue mf hasattr d output_f = mf forward input assertEqual output_s output_f test_freeze_module_with_fork torch jit script foo x x TestModule nn Module __init__ - None super __init__ = torch ones b = torch ones forward x fut = torch jit _fork foo y_hat = foo b y = torch jit _wait fut y_hat + y m = torch jit script TestModule m eval input = torch randn output_s = m forward input mf = torch _C _freeze_module m _c Check frozen module looks below module m attributes = b = submodule TODO Although there no mutation alias analysis conservatively assumes there mutation because attributes passed fork subgraph both b preserved assertTrue mf hasattr assertFalse mf hasattr b output_f = mf forward input assertEqual output_s output_f test_freeze_module_with_fork_calling_module_method torch jit script foo x y x y TestModule nn Module __init__ - None super __init__ = torch ones b = torch ones torch jit export foo x x torch jit export bar x x b forward x fut = torch jit _fork foo b y_hat = bar y = torch jit _wait fut y_hat + y m = torch jit script TestModule m eval input = torch randn output_s = m forward input mf = torch _C _freeze_module m _c Check frozen module looks below module m attributes b = TODO Although there no mutation alias analysis conservatively assumes there mutation because attributes passed fork subgraph b preserved assertFalse mf hasattr assertTrue mf hasattr b output_f = mf forward input assertEqual output_s output_f test_freeze_module_with_sharedclasstype SubModule nn Module __init__ - None super __init__ = torch tensor b = torch tensor forward x + b torch jit export modify_a x += b torch jit export modify_b x b += SubModule nn Module __init__ - None super __init__ sub = SubModule b = torch tensor forward x y = sub modify_b x y + b TestModule nn Module __init__ - None super __init__ sub = SubModule sub sub sub shared same type sub = SubModule = torch tensor forward x z = sub modify_a x sub x + z + m = torch jit script TestModule m eval input = torch randn output_s = m forward input mf = torch _C _freeze_module m _c Checking Frozen module looks below module mf attributes sub = sub = submodules module sub attributes = b = module sub attributes sub = submodule module sub attributes = b = assertTrue mf hasattr sub assertTrue mf sub hasattr assertTrue mf sub hasattr b assertFalse mf hasattr assertTrue mf hasattr sub assertTrue mf sub hasattr sub assertFalse mf sub hasattr b assertTrue mf sub sub hasattr assertTrue mf sub sub hasattr b output_f = mf forward input assertEqual output_s output_f test_freeze_module_with_nestedaliasing SubModule nn Module __init__ - None super __init__ = torch tensor b = torch tensor forward x + b torch jit export modify_a x = b torch jit export modify_b x b = Sub = SubModule SubModule nn Module __init__ - None super __init__ sub = Sub aliasing forward x sub TestModule nn Module __init__ - None super __init__ sub = Sub aliasing sub = SubModule forward x z = sub modify_a x sub x + z m = torch jit script TestModule m eval mf = torch _C _freeze_module m _c assertTrue mf hasattr sub assertTrue mf sub hasattr assertFalse mf sub hasattr b assertTrue mf hasattr sub assertTrue mf sub hasattr sub assertTrue mf sub sub hasattr Freezing detects sub sub sub alias assertFalse mf sub sub hasattr b input = torch randn output_s = m forward input output_f = mf forward input assertEqual output_s output_f FIXME JIT honoring aliasing Sub module copied As result Eager Script modules produce different output test_freeze_module_with_nestedaliasingscalar SubModule nn Module __init__ - None super __init__ = b = forward x + b torch jit export modify_a x = b torch jit export modify_b x b = Sub = SubModule SubModule nn Module __init__ - None super __init__ sub = Sub aliasing forward x sub TestModule nn Module __init__ - None super __init__ sub = Sub aliasing sub = SubModule forward x z = sub modify_a x sub x + z m = TestModule ms = torch jit script m ms eval mf = torch _C _freeze_module ms _c assertTrue mf hasattr sub assertTrue mf sub hasattr assertFalse mf sub hasattr b sub fully folded because sub sub sub alias Scripting bug assertFalse mf hasattr sub input = torch randn output = m forward input output_s = ms forward input output_f = mf forward input Should equal assertNotEqual output output_s assertEqual output_s output_f test_freeze_module_with_preserve_sub_module SubModule nn Module __init__ - None super __init__ = torch tensor b = forward x TestModule nn Module __init__ - None super __init__ sub = SubModule aliasing sub = SubModule forward x sub x + sub x m = TestModule ms = torch jit script m ms eval mf = torch _C _freeze_module ms _c sub Test sub preserved entirely sub completely folded assertTrue mf hasattr sub assertTrue mf sub hasattr assertTrue mf sub hasattr b assertFalse mf hasattr sub input = torch randn output_s = ms forward input output_f = mf forward input assertEqual output_s output_f test_freeze_module_with_preserve_sub_module_and_mutation SubModule nn Module __init__ - None super __init__ = torch tensor b = forward x = TestModule nn Module __init__ - None super __init__ sub = SubModule aliasing sub = SubModule forward x sub x + sub x m = TestModule ms = torch jit script m ms eval mf = torch _C _freeze_module ms _c sub Test both sub sub preserved b preserved even used To fulfill user request preserve sub assertTrue mf hasattr sub assertTrue mf sub hasattr assertTrue mf sub hasattr b assertTrue mf hasattr sub assertTrue mf sub hasattr assertTrue mf sub hasattr b input = torch randn output_s = ms forward input output_f = mf forward input assertEqual output_s output_f test_freeze_module_with_helperfunction SubModule nn Module __init__ - None super __init__ = b = forward x + b TestModule nn Module __init__ - None super __init__ sub = SubModule = b = forward x b = _forward x + + b _forward x sub x m = torch jit script TestModule m eval input = torch randn mf = torch _C _freeze_module m _c assertFalse mf hasattr sub assertFalse mf hasattr assertTrue mf hasattr b assertRaisesRegex AttributeError TestModule does have field name _forward mf _forward x noqa F test_freeze_module_with_inplace_mutable FreezeMe torch jit ScriptModule __init__ - None super __init__ = torch jit script_method forward x i range append i m = FreezeMe m eval m_f = torch _C _freeze_module m _c assertTrue m_f hasattr m forward torch tensor out = m_f forward torch tensor expected = assertEqual out expected Mutable attributes test_freeze_module_with_mutable_list FreezeMe nn Module __init__ - None super __init__ = forward x m = FreezeMe m eval m append m_s = torch jit script m v = m_s v append m_s = v m_s eval m_f = torch _C _freeze_module m_s _c Post-freezing mutating m_s does affect m_f m_f has its own copy v = m_s v append m_s = v assertFalse m_f hasattr out = m_f forward torch tensor expected = assertEqual out expected test_freeze_module_with_mutable_dict FreezeMe nn Module __init__ - None super __init__ = layer forward x torch jit export modify_a x layer = layer + m = FreezeMe m eval m layer = m_s = torch jit script m t = torch tensor m_s modify_a t m_s eval m_f = torch _C _freeze_module m_s _c m layer += m_s modify_a t assertFalse m_f hasattr out = m_f forward t expected = layer layer assertEqual out expected test_freeze_module_with_mutable_tensor FreezeMe nn Module __init__ - None super __init__ = torch tensor forward x m = FreezeMe m_s = torch jit script m m_s += m_s eval m_f = torch _C _freeze_module m_s _c Post-freezing tensor attribute mutations affect m_f FIXME deep copy all folded attributes so m_f has full ownership m_s += assertFalse m_f hasattr out = m_f forward torch tensor expected = assertEqual out expected test_freeze_module_with_tuple FreezeMe nn Module __init__ - None super __init__ = torch tensor hi forward x x == = sum m = FreezeMe m_s = torch jit script m m_s eval inp = torch tensor expected = m_s forward inp m_s = m_f = torch _C _freeze_module m_s _c assertFalse m_f hasattr out = m_f forward inp assertEqual out expected test_freeze_module_with_tensor FreezeMe nn Module __init__ - None super __init__ = torch tensor forward x x = view x += sum m = FreezeMe m_s = torch jit script m m_s eval inp = torch tensor expected = m_s forward inp m_f = torch _C _freeze_module m_s _c assertTrue m_f hasattr m_f -= out = m_f forward inp assertEqual out expected test_freeze_module_with_list FreezeMe nn Module __init__ - None super __init__ = torch tensor forward x += sum m = FreezeMe m_s = torch jit script m m_s eval inp = torch tensor expected = m_s forward inp m_s -= m_f = torch _C _freeze_module m_s _c assertFalse m_f hasattr out = m_f forward inp assertEqual out expected test_freeze_module_with_aliased_tensor_attr FreezeMe nn Module __init__ - None super __init__ = torch tensor b = view forward x b += sum m = FreezeMe m_s = torch jit script m m_s eval m_f = torch _C _freeze_module m_s _c assertTrue m_f hasattr inp = torch tensor out = m_f forward inp expected = torch tensor + + + + + assertEqual out expected test_freeze_module_with_aliased_tensor_attr FreezeMe nn Module __init__ - None super __init__ = torch tensor b = layer view torch tensor c = view torch tensor d = view forward x d += sum m = FreezeMe m_s = torch jit script m m_s eval inp = torch tensor expected = m_s forward inp assertRaisesRegex RuntimeError module contains attributes values overlaps m_f = torch _C _freeze_module m_s _c test_freeze_module_with_aliased_tensor_attr FreezeMe nn Module __init__ - None super __init__ = torch tensor b = torch tensor forward x += b sum m = FreezeMe m_s = torch jit script m m_s eval inp = torch tensor expected = m_s forward inp m_f = torch _C _freeze_module m_s _c assertTrue m_f hasattr assertTrue m_f hasattr b out = m_f forward inp expected += account += assertEqual out expected test_freeze_module_with_aliased_tensor_attr FreezeMe nn Module __init__ - None super __init__ = torch tensor b = torch tensor forward x b += sum m = FreezeMe m_s = torch jit script m m_s eval inp = torch tensor expected = m_s forward inp m_s -= assertRaisesRegex RuntimeError module contains attributes values overlaps m_f = torch _C _freeze_module m_s _c test_freeze_module_with_overlapping_attrs = torch tensor FreezeMe nn Module __init__ - None super __init__ b = view torch tensor c = view forward x b += c sum m = FreezeMe m_s = torch jit script m m_s eval inp = torch tensor expected = m_s forward inp -= assertRaisesRegex RuntimeError module contains attributes values overlaps m_f = torch _C _freeze_module m_s _c test_freeze_module_with_aliased_attr FreezeMe nn Module __init__ - None super __init__ = b = c = forward x b += str + str c m = FreezeMe m_s = torch jit script m m_s eval m_f = torch _C _freeze_module m_s _c FIXME It should assertTrue Currently scripting making copy setting b see assertFalse m_f hasattr assertFalse m_f hasattr c inp = torch tensor out = m_f forward inp expected = m_s forward inp assertEqual out expected Check attribute preserved Alias analysis detects has output writers In example mutated However we do track which sub values composite ivalue mutated test_freeze_module_with_aliased_attr FreezeMe nn Module __init__ - None super __init__ = b = forward x v = b = v v = b v append str v + str v m = FreezeMe m_s = torch jit script m m_s eval m_f = torch _C _freeze_module m_s _c assertTrue m_f hasattr inp = torch tensor out = m_f forward inp expected = m forward inp assertEqual out expected test_freeze_module_with_aliased_attr FreezeMe nn Module __init__ - None super __init__ = b = forward x v = v = v v = v v append str m = FreezeMe m_s = torch jit script m m_s eval m_f = torch _C _freeze_module m_s _c assertTrue m_f hasattr inp = torch tensor out = m_f forward inp expected = m forward inp assertEqual out expected test_freeze_module_return_self FreezeMe nn Module __init__ - None super __init__ = torch tensor forward x m = FreezeMe m_s = torch jit script m m_s eval assertRaisesRegex RuntimeError attempted freeze module itself m_f = torch _C _freeze_module m_s _c test_freeze_module_inlining torch jit script noqa B Obj noqa B __init__ x int y int x = x y = y Mod nn Module __init__ - None super __init__ obj = Obj forward i int print obj i mod = torch jit freeze torch jit script Mod eval obj = mod graph findNode prim Constant assertTrue torch _C _jit_object_is_non_holding obj buffer = io BytesIO torch jit save mod buffer buffer seek loaded = torch jit load buffer obj = mod graph findNode prim Constant assertTrue torch _C _jit_object_is_non_holding obj test_freeze_module_return_sub_module FreezeMe nn Module __init__ - None super __init__ conv = nn Conv d forward x conv m = FreezeMe m_s = torch jit script m m_s eval m_f = torch _C _freeze_module m_s _c assertTrue m_f hasattr conv test_freeze_module_no_forward FreezeMe nn Module __init__ - None super __init__ lin = nn Linear torch jit export foo x lin x m = FreezeMe m_s = torch jit script m m_s eval m_f = torch _C _freeze_module m_s _c preservedAttrs= foo input = torch ones assertEqual m_s foo input m_f foo input test_freeze_no_forward FreezeMe nn Module __init__ - None super __init__ lin = nn Linear torch jit export foo x lin x m = FreezeMe m_s = torch jit script m m_s eval m_f = torch jit freeze m_s preserved_attrs= foo input = torch ones assertEqual m_s foo input m_f foo input test_freeze_module_in_training_mode Net nn Module __init__ - None super __init__ conv = nn Conv d conv = nn Conv d dropout = nn Dropout d dropout = nn Dropout d fc = nn Linear fc = nn Linear forward x x = conv x x = nn functional relu x x = conv x x = nn functional max_pool d x x = dropout x x = torch flatten x x = fc x x = nn functional relu x x = dropout x x = fc x output = nn functional log_softmax x dim= output model = torch jit script Net model train mTrain_freezed = torch _C _freeze_module model _c verify mTrain_freezed looks exactly module attributes conv = conv = dropout = dropout = fc = fc = submodules module conv attributes weight = bias = module conv attributes weight = bias = module dropout attributes training = module dropout attributes training = module fc attributes weight = bias = module fc attributes weight = bias = assertFalse mTrain_freezed hasattr training assertTrue mTrain_freezed hasattr conv assertFalse mTrain_freezed conv hasattr training assertTrue mTrain_freezed conv hasattr weight assertTrue mTrain_freezed conv hasattr bias assertTrue mTrain_freezed hasattr conv assertFalse mTrain_freezed conv hasattr training assertTrue mTrain_freezed conv hasattr weight assertTrue mTrain_freezed conv hasattr bias assertTrue mTrain_freezed hasattr dropout assertTrue mTrain_freezed dropout hasattr training assertTrue mTrain_freezed hasattr dropout assertTrue mTrain_freezed dropout hasattr training assertTrue mTrain_freezed hasattr fc assertTrue mTrain_freezed fc hasattr weight assertTrue mTrain_freezed fc hasattr bias assertTrue mTrain_freezed hasattr fc assertTrue mTrain_freezed fc hasattr weight assertTrue mTrain_freezed fc hasattr bias model eval mEval_freezed = torch _C _freeze_module model _c assertFalse mEval_freezed hasattr conv assertFalse mEval_freezed hasattr conv assertFalse mEval_freezed hasattr dropout assertFalse mEval_freezed hasattr training assertFalse mEval_freezed hasattr fc assertFalse mEval_freezed hasattr dropout assertFalse mEval_freezed hasattr fc assertRaisesRegex AttributeError does have field name state_dict print mEval_freezed state_dict buffer = io BytesIO torch jit save mEval_freezed buffer buffer seek m = torch jit load buffer FileCheck check_not GetAttr name= run m _c _get_method forward graph m = torch _C _freeze_module model _c preserveParameters=True assertTrue m hasattr conv assertTrue m hasattr conv assertFalse m hasattr dropout assertFalse m hasattr training assertTrue m hasattr fc assertFalse m hasattr dropout assertTrue m hasattr fc test_freeze_module_detach_gradient mod = nn Conv d assertTrue mod weight requires_grad smod = torch jit script mod smod eval fmod = torch _C _freeze_module smod _c assertTrue mod weight requires_grad assertTrue smod weight requires_grad assertFalse fmod hasattr weight inp = torch ones out = fmod forward inp FIXME frozen module mutated outside original module torch no_grad smod weight += out = fmod forward inp out = smod inp assertNotEqual out out assertEqual out out test_freeze_module_with_user_preserved_attr Module nn Module __init__ - None super __init__ = torch tensor b = torch tensor forward x + b m = torch jit script Module m eval fm = torch _C _freeze_module m _c Attribute preserved assertTrue fm hasattr assertFalse fm hasattr b test_freeze_module_with_user_preserved_method Module nn Module __init__ - None super __init__ = torch tensor b = torch tensor forward x + b torch jit export modify_a x += b torch jit export modify_b x b += m = torch jit script Module m eval fm = torch _C _freeze_module m _c modify_a Both attribute method modify_a preserved assertTrue fm hasattr assertFalse fm hasattr b input = torch randn expected = m forward input out = fm forward input assertEqual out expected test_freeze_module_with_user_preserved_method Module nn Module __init__ - None super __init__ = torch tensor b = torch tensor forward x b += + b torch jit export modify_a x += b + m = torch jit script Module m eval fm = torch _C _freeze_module m _c modify_a FileCheck check prim GetAttr name= run fm forward graph FileCheck check prim GetAttr name= b run fm modify_a graph test_freeze_module_with_user_preserved_attribute_on_submodule SubModule nn Module __init__ - None super __init__ = b = forward + b Module nn Module __init__ - None super __init__ sub = SubModule sub = SubModule forward sub + sub m = torch jit script Module m eval m = torch jit freeze m preserved_attrs= sub sub fm = m _c assertTrue fm hasattr sub assertTrue fm sub hasattr assertFalse fm sub hasattr b assertTrue fm hasattr sub assertTrue fm sub hasattr assertFalse fm sub hasattr b assertEqual m m sub += assertEqual m test_freeze_module_with_user_preserved_attribute_on_unused_submodule SubModule nn Module __init__ - None super __init__ = b = forward + b torch jit export method_a Module nn Module __init__ - None super __init__ sub = SubModule forward m = torch jit script Module m eval fm = torch jit freeze m preserved_attrs= sub sub method_a _c assertTrue fm hasattr sub assertTrue fm sub hasattr assertFalse fm sub hasattr b assertTrue fm sub _has_method method_a test_freeze_module_with_user_preserved_method_on_submodule SubModule nn Module forward x method_a x + method_b x method_a x x x method_b x x + x Module nn Module __init__ - None super __init__ sub = SubModule forward x sub x m = torch jit script Module m eval fm = torch jit freeze m preserved_attrs= sub method_a _c assertTrue fm hasattr sub assertTrue fm sub _has_method method_a assertFalse fm sub _has_method method_b skipIfNoFBGEMM test_module_with_shared_type_instances Child nn Module __init__ - None super __init__ conv = nn Conv d dtype=torch float forward x x = conv x x Parent nn Module __init__ - None super __init__ quant = torch ao quantization QuantStub conv = nn Conv d dtype=torch float child = Child child = Child dequant = torch ao quantization DeQuantStub forward x x = quant x x = conv x x = child x x = child x x = dequant x x _static_quant model qModel = torch ao quantization QuantWrapper model qModel qconfig = torch ao quantization default_qconfig torch ao quantization prepare qModel inplace=True qModel torch rand dtype=torch float torch ao quantization convert qModel inplace=True model override_quantized_engine fbgemm data = torch randn dtype=torch float m = Parent torch float m = _static_quant m m = torch jit script m m eval torch _C _jit_pass_inline m graph m_frozen = wrap_cpp_module torch _C _freeze_module m _c Earlier bug resulted _packed_params set false FileCheck check_not _packed_params = False run m_frozen _c dump_to_str True True False m_res = m data It used segfault while running frozen module m_frozen_res = m_frozen data assertEqual m_res m_frozen_res test_module_getattr_indirection torch jit script ValHolder __init__ val int val int = val Mod nn Module __init__ - None super __init__ mod = ValHolder mod = ValHolder forward cond bool cond mod = mod mod = mod mod val mod = Mod mod eval frozen_mod = torch jit freeze torch jit script mod mod_eager = Mod assertEqual mod_eager True frozen_mod True assertEqual mod_eager False frozen_mod False test_freeze_module_with_non_static_module_container_index Test Modules containing non-static ModuleDict ModuleList indexing cannot frozen torch jit interface ModuleInterface torch nn Module forward inp Any - Any pass ImplementsInterface torch nn Module forward inp Any - Any isinstance inp torch Tensor torch max inp dim= inp ModWithDict torch nn Module __init__ - None super __init__ d = torch nn ModuleDict module ImplementsInterface forward x torch Tensor key str - Any value ModuleInterface = d key value forward x m = torch jit script ModWithDict m eval assertRaisesRegex RuntimeError Freezing modules containing prim ModuleContainerIndex supported mf = torch _C _freeze_module m _c ModWithList torch nn Module __init__ - None super __init__ l = torch nn ModuleList ImplementsInterface forward x torch Tensor idx int - Any value ModuleInterface = l idx value forward x m = torch jit script ModWithList m eval assertRaisesRegex RuntimeError Freezing modules containing prim ModuleContainerIndex supported mf = torch _C _freeze_module m _c test_freeze_with_interface_mutable torch jit interface ModuleInterface torch nn Module forward inp torch Tensor - torch Tensor pass ImplementsInterface torch nn Module __init__ - None super __init__ sum = torch zeros forward inp torch Tensor - torch Tensor sum += inp relu sum WrapperModule torch nn Module impl ModuleInterface __init__ - None super __init__ impl = ImplementsInterface forward x torch Tensor - torch Tensor impl forward x m = torch jit script WrapperModule m eval m_frozen = torch jit freeze m x = torch rand m_frozen x assertEqual m_frozen impl sum x relu test_freeze_with_swapping_interfaces torch jit interface ModuleInterface torch nn Module forward inp torch Tensor - torch Tensor pass Implementation torch nn Module forward inp torch Tensor - torch Tensor inp relu Implementation torch nn Module forward inp torch Tensor - torch Tensor inp sin WrapperModule torch nn Module impl ModuleInterface __init__ - None super __init__ option = Implementation option = Implementation impl = option idx = forward x torch Tensor - torch Tensor idx += idx == impl = option impl = option impl x m = torch jit script WrapperModule m eval assertRaisesRegex RuntimeError Freezing does support SetAttr interface type m_frozen = torch jit freeze m test_freeze_recursive_interfaces torch jit interface InnerInterface torch nn Module forward inp torch Tensor - torch Tensor pass torch jit interface OuterInterface torch nn Module forward inp torch Tensor - torch Tensor pass InnerImpl torch nn Module __init__ - None super __init__ x = torch ones forward inp inp cos x OuterImpl torch nn Module inner_impl InnerInterface __init__ - None super __init__ inner_impl = InnerImpl forward inp inp relu + inner_impl inp sin WrapperModule torch nn Module outer_impl OuterInterface __init__ - None super __init__ outer_impl = OuterImpl forward inp outer_impl inp + inp m = WrapperModule x = torch rand expected = m x m_s = torch jit script m m_s eval m_s = torch jit freeze m_s actual = m_s x assertEqual expected actual test_freeze_recursive_interfaces_with_reassignment torch jit interface InnerInterface torch nn Module forward inp torch Tensor - torch Tensor pass torch jit interface OuterInterface torch nn Module forward inp torch Tensor - torch Tensor pass InnerImpl torch nn Module __init__ - None super __init__ x = torch ones forward inp inp cos x InnerImpl torch nn Module __init__ - None super __init__ x = torch ones forward inp inp sin x OuterImpl torch nn Module inner_impl InnerInterface __init__ - None super __init__ inner_impl = InnerImpl impl = InnerImpl impl = InnerImpl idx = forward inp idx += idx == inner_impl = impl inner_impl = impl inp relu + inner_impl inp sin WrapperModule torch nn Module outer_impl OuterInterface __init__ - None super __init__ outer_impl = OuterImpl forward inp outer_impl inp + inp m = WrapperModule m_s = torch jit script m m_s eval assertRaisesRegex RuntimeError Freezing does support SetAttr interface type m_s = torch jit freeze m_s test_freeze_interface_swapping_two_methods torch jit interface MyInterface torch nn Module forward inp torch Tensor - torch Tensor pass Impl torch nn Module forward inp inp cos Impl torch nn Module forward inp inp sin WrapperModule torch nn Module interface_impl MyInterface __init__ - None super __init__ interface_impl = Impl impl = Impl impl = Impl idx = forward x interface_impl x torch jit export other_method x idx += idx == interface_impl = impl interface_impl = impl interface_impl x WrapperModule torch nn Module interface_impl MyInterface __init__ - None super __init__ interface_impl = Impl impl = Impl impl = Impl idx = forward x idx += idx == interface_impl = impl interface_impl = impl interface_impl x torch jit export other_method x interface_impl x m = torch jit script WrapperModule m = torch jit script WrapperModule m eval m eval assertRaisesRegex RuntimeError Freezing does support SetAttr interface type torch jit freeze m preserved_attrs= other_method assertRaisesRegex RuntimeError Freezing does support SetAttr interface type torch jit freeze m preserved_attrs= other_method test_freeze_recursive_interfaces_same_name torch jit interface InnerInterface torch nn Module forward inp torch Tensor - torch Tensor pass torch jit interface OuterInterface torch nn Module forward inp torch Tensor - torch Tensor pass InnerImpl torch nn Module __init__ - None super __init__ x = torch ones forward inp inp cos x OuterImpl torch nn Module impl InnerInterface __init__ - None super __init__ impl = InnerImpl x = torch ones forward inp other_method inp other_method inp inp relu + impl inp sin + x WrapperModule torch nn Module impl OuterInterface __init__ - None super __init__ impl = OuterImpl forward inp impl inp + inp m = WrapperModule x = torch rand expected = m x m_s = torch jit script m m_s eval m_s = torch jit freeze m_s actual = m_s x assertEqual expected actual test_freeze_non_interface_module_swap InnerModule torch nn Module __init__ x super __init__ x = x forward inp torch Tensor - torch Tensor inp relu + x WrapperModule torch nn Module __init__ - None super __init__ option = InnerModule torch rand option = InnerModule torch rand impl = option idx = forward x torch Tensor - torch Tensor idx += idx == impl = option impl = option impl x unfrozen = WrapperModule m = torch jit script unfrozen m eval m_frozen = torch jit freeze m x = torch rand expected = unfrozen x actual = m_frozen x assertEqual expected actual unittest expectedFailure test_freeze_interface_within_object I don t think there s any way create plain python object contains torch nn Module inside just case I m sure freezing would handle case correctly so marking xfail so ever _does_ start working someone will need investigate make sure handled correctly MyIface torch nn Module forward inp torch Tensor - torch Tensor pass MyImpl torch nn Module forward inp torch Tensor - torch Tensor inp sin MyObject impl MyIface run x impl x WrapperModule torch nn Module impl MyObject __init__ - None super __init__ impl = MyObject impl impl = MyImpl forward x torch Tensor - torch Tensor impl x unfrozen = WrapperModule m = torch jit script unfrozen m eval m_frozen = torch jit freeze m x = torch rand expected = unfrozen x actual = m_frozen x expectEqual expected actual test_freeze_non_module_class_getattr BoxCoder __init__ bbox_xform_clip type float - None bbox_xform_clip = bbox_xform_clip decode input input bbox_xform_clip MyModule torch nn Module __annotations__ = box_coder BoxCoder __init__ - None super __init__ box_coder = BoxCoder forward input box_coder decode input model = MyModule model eval script_model = torch jit freeze torch jit script model inp = torch randn output_eager = model inp assertEqual model inp script_model inp FileCheck check_not GetAttr run script_model graph test_freeze_module_with_tupleoutput_submodule SubModule nn Module forward x x + x + TestModule nn Module __init__ - None super __init__ sub = SubModule forward x y y = sub x y + y m = torch jit script TestModule m = m eval mf = torch jit freeze m inp = torch randn expected = m forward inp output = mf forward inp Check prim TupleConstruct prim TupleUnpack Don t exist frozen graph FileCheck check_not prim TupleConstruct run mf graph FileCheck check_not prim TupleUnpack run mf graph assertEqual output expected test_freeze_module_with_call_method Mod nn Module __init__ val super __init__ param = nn Parameter val forward x method will change during freezing x + param torch jit export make_prediction x y = x + x forward y param = torch rand x = torch rand unscripted_mod = Mod param mod = torch jit script unscripted_mod mod eval mod = torch jit freeze mod preserved_attrs= make_prediction assertEqual mod forward x unscripted_mod forward x atol= e- rtol= e- skipIfTorchDynamo somehow causing hanging during python shutdown TestFrozenOptimizations JitTestCase setUp super setUp default_dtype = torch get_default_dtype torch set_default_dtype torch double tearDown torch set_default_dtype default_dtype super tearDown test_conv_bn_folding conv_bias = True False module_pairs = nn Conv d nn BatchNorm d nn Conv d nn BatchNorm d nn Conv d nn BatchNorm d use_tracing = True False bn_running_stats = True False use_bias modules tracing track_stats product conv_bias module_pairs use_tracing bn_running_stats ConvBN torch nn Module __init__ in_channels out_channels kwargs super __init__ conv = modules in_channels out_channels bias=use_bias kwargs bn = modules out_channels eps= track_running_stats=track_stats forward x x = conv x bn x mod_eager = ConvBN kernel_size= stride= eval inps = modules nn Conv d inps append inps - modules nn Conv d inps append inps - inps append inps - inp = torch rand inps tracing scripted_mod = torch jit trace mod_eager inp scripted_mod = torch jit script mod_eager run_pass inline scripted_mod graph run_pass peephole scripted_mod graph run_pass constant_propagation scripted_mod graph FileCheck check conv check batch run scripted_mod graph successfully no-ops non-const inputs run_pass fold_frozen_conv_bn scripted_mod graph FileCheck check conv check aten batch_norm run scripted_mod graph scripted_mod = torch jit freeze scripted_mod run_pass fold_frozen_conv_bn scripted_mod graph track_stats FileCheck check conv check_not aten batch_norm run scripted_mod graph FileCheck check conv check aten batch_norm run scripted_mod graph assertEqual mod_eager inp scripted_mod inp assertEqual mod_eager inp scripted_mod inp test_conv_bn_folding_not_forward ConvBN torch nn Module __init__ in_channels out_channels kwargs super __init__ conv = torch nn Conv d in_channels out_channels bias=True kwargs bn = torch nn BatchNorm d out_channels eps= amt = forward x x = conv x bn x torch jit export make_prediction x forward x + amt mod_eager = ConvBN kernel_size= stride= eval scripted_mod = torch jit script mod_eager torch _C _jit_pass_inline scripted_mod make_prediction graph FileCheck check conv check aten batch_norm run scripted_mod make_prediction graph _jit_pass_optimize_frozen_graph should called non-method attributes e g amt scripted_mod = torch jit freeze scripted_mod preserved_attrs= make_prediction amt FileCheck check conv check_not aten batch_norm run scripted_mod make_prediction graph During freezing creates tensors constants attached frozen graph which then kept alive compilation unit which causes leak skipCUDAMemoryLeakCheckIf True unittest skipIf TEST_CUDA Optimization currently only run GPU test_conv_bn_folding_autocast_scenario_cuda CUDA conv takes input tensors which must all same dtype which can cause issues folding produces inputs different dtypes ConvBN torch nn Module __init__ in_channels out_channels kwargs super __init__ conv = torch nn Conv d in_channels out_channels bias=False dtype=torch half kwargs bn = torch nn BatchNorm d out_channels eps= dtype=torch float forward x bn conv x mod_eager = ConvBN kernel_size= stride= cuda eval scripted_mod = torch jit script mod_eager scripted_mod = torch jit freeze scripted_mod FileCheck check conv check_not aten batch_norm run scripted_mod graph conv_node = scripted_mod graph findNode aten conv d True assertTrue conv_node None bias_input = conv_node namedInput bias assertTrue bias_input None assertTrue bias_input type dtype == torch half x = torch rand dtype=torch half cuda assertEqual mod_eager x scripted_mod x atol= e- rtol= e- assertEqual mod_eager x scripted_mod x atol= e- rtol= e- test_conv_add_folding torch no_grad test_conv_fusion use_bias module tracing op scalar add_tensor expect_success ConvOp torch nn Module __constants__ = use_scalar __init__ in_channels out_channels tensor=None kwargs super __init__ conv = module in_channels out_channels bias=use_bias kwargs conv = module in_channels out_channels bias=use_bias kwargs use_scalar = scalar tensor_size = _ range conv weight ndim tensor_size = conv weight size tensor = add_tensor add_tensor None torch rand tensor_size op = op forward x x = conv x use_scalar op x op x tensor mod_eager = ConvOp kernel_size= stride= eval inps = module nn Conv d inps append inps - module nn Conv d inps append inps - inps append inps - inp = torch rand inps tracing scripted_mod = torch jit trace mod_eager inp scripted_mod = torch jit script mod_eager run_pass inline scripted_mod graph op_str = aten + op __name__ FileCheck check conv check op_str run scripted_mod graph successively no-ops non-const inputs run_pass fold_frozen_conv_mul_or_div scripted_mod graph run_pass fold_frozen_conv_add_or_sub scripted_mod graph FileCheck check conv check op_str run scripted_mod graph scripted_mod = torch jit freeze scripted_mod run_pass fold_frozen_conv_mul_or_div scripted_mod graph run_pass fold_frozen_conv_add_or_sub scripted_mod graph expect_success FileCheck check conv check_not op_str run scripted_mod graph FileCheck check conv check op_str run scripted_mod graph assertEqual mod_eager inp scripted_mod inp assertEqual mod_eager inp scripted_mod inp conv_bias = True False modules = nn Conv d nn Conv d nn Conv d use_tracing = False True use_scalar = False True ops = torch add torch sub torch mul torch div use_bias module tracing pytorch_op scalar product conv_bias modules use_tracing ops use_scalar test_conv_fusion use_bias module tracing pytorch_op scalar add_tensor=None expect_success=True use_bias pytorch_op product conv_bias ops broadcasting add test_conv_fusion use_bias nn Conv d False pytorch_op False add_tensor=torch rand expect_success=False broadcasting add test_conv_fusion use_bias nn Conv d False pytorch_op False add_tensor=torch rand expect_success=True add different dtype test_conv_fusion use_bias nn Conv d False pytorch_op False add_tensor=torch tensor torch int expect_success=True test_conv_mul_add_bn Conv_Mul_Add_Bn nn Module __init__ in_channels out_channels kwargs super __init__ conv = nn Conv d in_channels out_channels kwargs bn = nn BatchNorm d out_channels eps= tensor = torch tensor tensor = torch tensor forward x bn torch add torch mul conv x tensor tensor input = torch randn model = Conv_Mul_Add_Bn kernel_size= stride= eval torch no_grad result = model input traced_model = torch jit trace model input eval traced_model = torch jit freeze traced_model tresult = traced_model input assertEqual result tresult FileCheck check conv check_not aten batch_norm run traced_model graph FileCheck check conv check_not aten add run traced_model graph test_linear_bn_folding module_pairs = nn Linear nn BatchNorm d nn Linear nn BatchNorm d nn Linear nn BatchNorm d use_tracing = True False bn_running_stats = True False modules tracing track_stats product module_pairs use_tracing bn_running_stats LinearBN torch nn Module __init__ in_features out_features super __init__ linear = modules in_features out_features bn = modules out_features eps= track_running_stats=track_stats forward x x = linear x bn x mod_eager = LinearBN eval inps = modules nn BatchNorm d inps append inps - inps append inps - modules nn BatchNorm d inps append inps - inps append inps - inps append inps - inp = torch rand inps tracing scripted_mod = torch jit trace mod_eager inp scripted_mod = torch jit script mod_eager run_pass inline scripted_mod graph run_pass peephole scripted_mod graph run_pass constant_propagation scripted_mod graph FileCheck check linear check batch run scripted_mod graph successfully no-ops non-const inputs run_pass fold_frozen_linear_bn scripted_mod graph FileCheck check linear check aten batch_norm run scripted_mod graph scripted_mod = torch jit freeze scripted_mod run_pass fold_frozen_linear_bn scripted_mod graph track_stats FileCheck check linear check_not aten batch_norm run scripted_mod graph FileCheck check linear check aten batch_norm run scripted_mod graph assertEqual mod_eager inp scripted_mod inp assertEqual mod_eager inp scripted_mod inp test_bn_not_broadcast_with_linear module_pairs = nn Linear nn BatchNorm d nn Linear nn BatchNorm d nn Linear nn BatchNorm d use_tracing = True False linear_in = linear_out bn_in case linear_out bn_in case linear_out bn_in case linear_out = bn_in linear_out = dims = modules tracing dim product module_pairs use_tracing dims linear_out bn_in = dim dim linear = modules linear_in linear_out bn = modules bn_in mod_eager = nn Sequential linear bn eval N C = bn_in input_shape = N C modules nn BatchNorm d H = linear_in input_shape append H modules nn BatchNorm d H W = linear_in input_shape append H input_shape append W modules nn BatchNorm d D H W = linear_in input_shape append D input_shape append H input_shape append W inp = torch rand input_shape tracing scripted_mod = torch jit trace mod_eager inp scripted_mod = torch jit script mod_eager run_pass inline scripted_mod graph run_pass peephole scripted_mod graph run_pass constant_propagation scripted_mod graph FileCheck check linear check batch run scripted_mod graph run_pass fold_frozen_linear_bn scripted_mod graph FileCheck check linear check aten batch_norm run scripted_mod graph frozen_mod = torch jit freeze scripted_mod run_pass fold_frozen_linear_bn frozen_mod graph successfully skipped folding FileCheck check linear check aten batch_norm run frozen_mod graph assertEqual mod_eager inp frozen_mod inp assertEqual mod_eager inp frozen_mod inp successfully failed folding assertRaisesRegex AssertionError To fuse linear out_features == bn num_features bn num_features == nn utils fusion fuse_linear_bn_eval linear bn skipCUDAMemoryLeakCheckIf True unittest skipIf TEST_CUDA Optimization currently only run GPU test_linear_bn_folding_autocast_scenario_cuda module_pairs = nn Linear nn BatchNorm d nn Linear nn BatchNorm d nn Linear nn BatchNorm d use_tracing = True False bn_running_stats = True False modules tracing track_stats product module_pairs use_tracing bn_running_stats LinearBN torch nn Module __init__ in_features out_features super __init__ linear = modules in_features out_features bias=False dtype=torch half bn = modules out_features eps= dtype=torch float forward x x = linear x bn x mod_eager = LinearBN cuda eval inps = modules nn BatchNorm d inps append inps - inps append inps - modules nn BatchNorm d inps append inps - inps append inps - inps append inps - x = torch rand inps dtype=torch half cuda tracing scripted_mod = torch jit trace mod_eager x scripted_mod = torch jit script mod_eager scripted_mod = torch jit freeze scripted_mod FileCheck check linear check_not aten batch_norm run scripted_mod graph lin_node = scripted_mod graph findNode aten linear True assertTrue lin_node None weight_input = lin_node namedInput weight bias_input = lin_node namedInput bias assertTrue bias_input None assertTrue weight_input type dtype == torch half assertTrue bias_input type dtype == torch half assertEqual mod_eager x scripted_mod x atol= e- rtol= e- assertEqual mod_eager x scripted_mod x atol= e- rtol= e- unittest skipIf TEST_CUDA Optimization currently only run GPU test_linear_concat out_dimms = w _dim w _dim out_dimms ModMultLinear nn Module __init__ w _dim w _dim super __init__ w = nn Parameter torch rand w _dim b = nn Parameter torch rand w _dim w = nn Parameter torch rand w _dim b = nn Parameter torch rand w _dim forward in_tensor res = torch _C _nn linear in_tensor w b res = torch _C _nn linear in_tensor w b res res mod_eager = ModMultLinear w _dim w _dim eval test_val = torch rand check_linear_optimizations mod_eager test_val unittest skipIf TEST_CUDA Optimization currently only run GPU test_linear_concat_complex Testing interleaving multiple optimizations does cause errors gets optimized expected ModMultLinear nn Module __init__ - None super __init__ w _dim = w _dim = w = nn Parameter torch rand w _dim b = nn Parameter torch rand w _dim w = nn Parameter torch rand w _dim b = nn Parameter torch rand w _dim forward in_tensor res = torch _C _nn linear in_tensor w b res = torch _C _nn linear res w b res = torch _C _nn linear in_tensor w b res = torch _C _nn linear res w b res res res mod_eager = ModMultLinear eval test_val = torch rand check_linear_optimizations mod_eager test_val unittest skipIf TEST_CUDA Optimization currently only run GPU test_linear_concat_different_input There should no change graph due optimization pass due two input tensors being different Freezing requires graph module ModMultLinear nn Module __init__ w _dim w _dim super __init__ w = nn Parameter torch rand w _dim b = nn Parameter torch rand w _dim w = nn Parameter torch rand w _dim b = nn Parameter torch rand w _dim forward in_tensor in_tensor res = torch _C _nn linear in_tensor w b res = torch _C _nn linear in_tensor w b res res mod_eager = ModMultLinear eval test_val = torch rand test_val = torch rand check_linear_optimizations mod_eager test_val test_val unittest skipIf TEST_CUDA Optimization currently only run GPU test_linear_multiple_blocks ModMultLinear nn Module __init__ w _dim w _dim super __init__ w = nn Parameter torch rand w _dim b = nn Parameter torch rand w _dim w = nn Parameter torch rand w _dim b = nn Parameter torch rand w _dim forward in_tensor in_tensor cond bool res = torch _C _nn linear in_tensor w b cond res = torch _C _nn linear in_tensor w b res = torch _C _nn linear in_tensor w b raise AssertionError res = torch _C _nn linear in_tensor w b res res res res mod_eager = ModMultLinear eval test_val = torch rand test_val = torch rand check_linear_optimizations mod_eager test_val test_val True check_linear_optimizations eager_mod orig_linears new_linears test_vals is_cuda False True is_cuda mod_to_device = eager_mod cuda test_vals_to_device = t cuda isinstance t torch Tensor t t test_vals mod_to_device = eager_mod test_vals_to_device = test_vals script_mod = torch jit script mod_to_device op_graph = script_mod graph FileCheck check_count aten linear orig_linears exactly=True run op_graph successively no-ops non-const inputs run_pass concat_frozen_linear op_graph FileCheck check_count aten linear orig_linears exactly=True run op_graph script_mod = torch jit freeze script_mod op_graph = script_mod graph run_pass concat_frozen_linear op_graph is_cuda FileCheck check_count aten linear new_linears exactly=True run op_graph FileCheck check_count aten linear orig_linears exactly=True run op_graph assertEqual mod_to_device test_vals_to_device script_mod test_vals_to_device test_optimize_freeze_module in_channels out_channels = conv = torch nn Conv d in_channels out_channels kernel_size= stride= bias=True bn = torch nn BatchNorm d out_channels eps= mod = torch nn Sequential conv bn set optimize False here default freezing runs run_frozen_optimizations frozen_mod = torch jit freeze torch jit script mod eval optimize_numerics=False inspect frozen mod FileCheck check batch_norm run frozen_mod graph torch jit run_frozen_optimizations frozen_mod FileCheck check_not batch_norm run frozen_mod graph run_frozen_optimizations should run frozen_mod = torch jit freeze torch jit script mod eval FileCheck check_not batch_norm run frozen_mod graph test_freeze_remove_dropout Net nn Module __init__ - None super __init__ dropout = nn Dropout forward x dropout x mod = torch jit script Net inspect mod torch _C _jit_pass_inline mod graph FileCheck check aten dropout run mod graph frozen_mod = torch jit freeze mod eval FileCheck check_not aten dropout run frozen_mod graph input = torch randn output_s = mod forward input output_f = frozen_mod forward input assertEqual output_s output_f test_freeze_remove_feature_dropout Net nn Module __init__ - None super __init__ dropout = nn Dropout d forward x dropout x mod = torch jit script Net eval inspect mod torch _C _jit_pass_inline mod graph FileCheck check aten feature_dropout run mod graph frozen_mod = torch jit freeze mod FileCheck check_not aten feature_dropout run frozen_mod graph input = torch randn output_s = mod forward input output_f = frozen_mod forward input assertEqual output_s output_f unittest skipIf torch backends mkldnn is_available MKL-DNN build disabled test_freeze_mkdlnn conv = torch nn Conv d kernel_size= stride= eval float convmkl = mkldnn_utils to_mkldnn conv out = torch jit freeze torch jit script convmkl eval inp = torch rand float assertEqual out inp to_mkldnn to_dense conv inp unittest skipIf torch backends mkldnn is_available MKL-DNN build disabled test_conv_to_mkldnn set_default_dtype torch float module trace product nn Conv d nn Conv d False True mod = module kernel_size= stride= eval inps = module nn Conv d inps append inps - module nn Conv d inps append inps - inps append inps - inp = torch rand inps trace scripted_mod = torch jit script mod scripted_mod = torch jit trace mod inp run_pass inline scripted_mod graph FileCheck check conv run scripted_mod graph successfully no-ops non-const inputs run_pass convert_frozen_ops_to_mkldnn scripted_mod graph FileCheck check_not to_mkldnn run scripted_mod graph scripted_mod = torch jit freeze scripted_mod run_pass convert_frozen_ops_to_mkldnn scripted_mod graph FileCheck check to_mkldnn check prim mkldnn_convolution check to_dense run scripted_mod graph assertEqual mod inp scripted_mod inp assertEqual mod inp scripted_mod inp test_linear_transpose ModLinear torch nn Module __init__ - None super __init__ bias = torch nn Parameter torch rand weight = torch nn Parameter torch rand forward x torch _C _nn linear x weight bias mod_eager = ModLinear eval test_val = torch rand check_linear_optimizations_ mod_eager transpose_frozen_linear test_val test_linear_non_constant_weight ModLinear torch nn Module __init__ - None super __init__ bias = torch nn Parameter torch rand forward x weight torch _C _nn linear x weight bias mod_eager = ModLinear eval test_val = torch rand test_weight = torch rand check_linear_optimizations_ mod_eager transpose_frozen_linear test_val test_weight check_linear_optimizations_ eager_mod orig_linears new_linears opt_pass test_vals TODO merge check_linear_optimizations once both diffs land mod_to_device = eager_mod test_vals_to_device = test_vals script_mod = torch jit script mod_to_device op_graph = script_mod graph FileCheck check_count aten linear orig_linears exactly=True run op_graph successively no-ops non-const inputs run_pass opt_pass op_graph FileCheck check_count aten linear orig_linears exactly=True run op_graph script_mod = torch jit freeze script_mod op_graph = script_mod graph run_pass opt_pass op_graph FileCheck check_count aten linear new_linears exactly=True run op_graph assertEqual mod_to_device test_vals_to_device script_mod test_vals_to_device staticmethod conv Generic composable conv testing purposes nn Conv d unittest skipIf torch backends mkldnn is_available MKL-DNN build disabled test_collapse_adjacent_conversions set_default_dtype torch float mod = nn Sequential conv conv eval scripted_mod = torch jit script mod scripted_mod = torch jit freeze scripted_mod run_pass convert_frozen_ops_to_mkldnn scripted_mod graph FileCheck check to_mkldnn check prim mkldnn_convolution check prim mkldnn_convolution check to_dense run scripted_mod graph FileCheck check_count to_mkldnn exactly=True run scripted_mod graph inp = torch rand assertEqual scripted_mod inp mod inp assertEqual scripted_mod inp mod inp unittest skipIf torch backends mkldnn is_available MKL-DNN build disabled test_mkldnn_fuser_broadcasting Add nn Module __init__ tensor super __init__ tensor = tensor forward x x + tensor set_default_dtype torch float add_inp mod = nn Sequential conv Add torch rand add_inp eval scripted_mod = torch jit script mod scripted_mod = torch jit freeze scripted_mod run_pass convert_frozen_ops_to_mkldnn scripted_mod graph FileCheck check prim BroadcastMKLDNNTensors run scripted_mod graph inp = torch rand assertEqual scripted_mod inp mod inp assertEqual scripted_mod inp mod inp good measure check broadcasting does work without op so we can remove op ever gets supported assertRaisesRegex RuntimeError torch rand to_mkldnn + torch rand add_inp to_mkldnn unittest skipIf torch backends mkldnn is_available MKL-DNN build disabled test_mkldnn_inplace_removal AddMul nn Module __init__ tensor super __init__ tensor = tensor forward x x add_ tensor div_ tensor - set_default_dtype torch float mod = nn Sequential conv AddMul torch rand eval scripted_mod = torch jit script mod scripted_mod = torch jit freeze scripted_mod run_pass convert_frozen_ops_to_mkldnn scripted_mod graph add gets uninplaced reinplaced FileCheck check aten to_mkldnn check aten add_ check aten div_ run scripted_mod graph inp = torch rand assertEqual scripted_mod inp mod inp assertEqual scripted_mod inp mod inp unittest skipIf torch backends mkldnn is_available MKL-DNN build disabled skipIfNoTorchVision test_maxpool_mkldnn set_default_dtype torch float model = torchvision models resnet sub_model = torch nn Sequential model conv model bn model relu model maxpool mod = torch jit freeze torch jit script sub_model eval N C H W = inp = torch randn N C H W run_pass convert_frozen_ops_to_mkldnn mod graph FileCheck check max_pool check to_dense run mod graph FileCheck check_count to_dense exactly=True run mod graph assertEqual mod inp sub_model inp unittest skipIf torch backends mkldnn is_available Testing no mkldnn test_conv_to_mkldnn_no_mkldnn test no error when mkldnn available set_default_dtype torch float mod = torch jit script nn Conv d kernel_size= stride= eval frozen = torch jit freeze mod run_pass convert_frozen_ops_to_mkldnn frozen graph inp = torch rand assertEqual frozen inp mod inp unittest skipIf TEST_CUDNN TEST_WITH_ROCM requires CUDNN test_freeze_conv_relu_fusion set_default_dtype torch float conv_bias = True False conv_ops = nn Conv d nn Conv d use_add_z = True False use_tracing = True False use_bias conv add_z tracing product conv_bias conv_ops use_add_z use_tracing Net nn Module __init__ in_channels out_channels kwargs super __init__ conv = conv in_channels out_channels bias=use_bias kwargs relu = nn ReLU inplace=True add_z = add_z forward x z = conv x out = conv x add_z out += z out = relu out out mod_eager = Net kernel_size= stride= eval cuda inps = conv nn Conv d inps append inps - inp = torch rand inps cuda tracing scripted_mod = torch jit trace mod_eager inp scripted_mod = torch jit script mod_eager frozen_mod = torch jit optimize_for_inference scripted_mod TEST_WITH_ROCM add_z FileCheck check aten miopen_convolution_add_relu run frozen_mod graph FileCheck check aten miopen_convolution_relu run frozen_mod graph add_z FileCheck check aten cudnn_convolution_add_relu run frozen_mod graph FileCheck check aten cudnn_convolution_relu run frozen_mod graph assertEqual mod_eager inp frozen_mod inp unittest skipIf TEST_CUDNN TEST_WITH_ROCM requires CUDNN test_freeze_conv_relu_fusion_not_forward set_default_dtype torch float Net nn Module __init__ in_channels out_channels kwargs super __init__ conv = nn Conv d in_channels out_channels bias=None kwargs relu = nn ReLU inplace=True forward x z = conv x out = conv x out = relu out out torch jit export make_prediction x forward x mod_eager = Net kernel_size= stride= eval cuda inps = inp = torch rand inps cuda scripted_mod = torch jit script mod_eager frozen_mod = torch jit freeze scripted_mod preserved_attrs= make_prediction optimized_mod = torch jit optimize_for_inference frozen_mod other_methods= make_prediction TEST_WITH_ROCM FileCheck check aten miopen_convolution_relu run optimized_mod make_prediction graph FileCheck check aten cudnn_convolution_relu run optimized_mod make_prediction graph assertEqual mod_eager make_prediction inp optimized_mod make_prediction inp unittest skipIf torch backends mkldnn is_available MKL-DNN build disabled test_numel_less_than_size_with_padding set_default_dtype torch float MyModule nn Module __init__ - None super __init__ conv = nn Conv d kernel_size= stride= padding= dilation= forward i x = conv i o = torch max x i o = torch clip x - o o i = torch zeros dtype=torch float mod = MyModule out = mod i exported = torch jit trace mod i exported = torch jit optimize_for_inference exported eout = exported i assertTrue all torch allclose x y x y zip out eout unittest skipIf torch backends mkldnn is_available MKL-DNN build disabled test_incompatible_perf_formats set_default_dtype torch float Mod nn Module __init__ - None super __init__ conv = torch nn Conv d max_pool = torch nn MaxPool d forward x = conv x b = max_pool + b model = Mod model eval mod = torch jit freeze torch jit script model N C H W = inp = torch randn N C H W run_pass convert_frozen_ops_to_mkldnn mod graph assertEqual model inp mod inp unittest skipIf torch backends mkldnn is_available MKL-DNN build disabled test_pool d_batchnorm set_default_dtype torch float pooling_layers = torch nn AdaptiveAvgPool d torch nn AdaptiveMaxPool d tuples torch nn MaxPool d torch nn AvgPool d torch nn BatchNorm d eval pl pooling_layers sub_model = torch nn Sequential torch nn Conv d torch nn ReLU pl torch nn Hardswish sub_model eval mod = torch jit freeze torch jit script sub_model N C H W = inp = torch randn N C H W these two passes needed remove size check BatchNorm d removeExceptions mod graph run_pass dce mod graph run_pass convert_frozen_ops_to_mkldnn mod graph FileCheck check aten to_dense check_next run mod graph assertEqual sub_model inp mod inp unittest skipIf torch backends mkldnn is_available MKL-DNN build disabled test_pool d_batchnorm set_default_dtype torch float pooling_layers = torch nn MaxPool d torch nn AdaptiveAvgPool d no ideep bindings torch nn AdaptiveMaxPool d tuples torch nn AvgPool d torch nn BatchNorm d eval pl pooling_layers sub_model = torch nn Sequential torch nn Conv d torch nn ReLU pl torch nn Hardswish sub_model eval mod = torch jit freeze torch jit script sub_model N C H W D = inp = torch randn N C D H W these two passes needed remove size check BatchNorm d removeExceptions mod graph run_pass dce mod graph run_pass convert_frozen_ops_to_mkldnn mod graph FileCheck check aten to_dense check_next run mod graph assertEqual sub_model inp mod inp unittest skipIf torch backends mkldnn is_available MKL-DNN build disabled skipIfNoTorchVision test_conv_hardswish set_default_dtype torch float Clamp torch nn Module __init__ min_val max_val kwargs super __init__ min_val = min_val max_val = max_val forward x torch clamp x min_val max_val N C H W = activations = torch nn Hardswish torch nn Hardsigmoid torch nn ReLU torch nn Tanh torch nn Hardtanh torch nn Hardtanh torch nn Hardtanh - - torch nn GELU Clamp - - Clamp Clamp Clamp - model = torchvision models resnet activation activations sub_model = torch nn Sequential model conv activation sub_model eval mod = torch jit freeze torch jit script sub_model inp = torch randn N C H W run_pass convert_frozen_ops_to_mkldnn mod graph FileCheck check_count aten to_dense exactly=True run mod graph assertEqual sub_model inp mod inp unittest skipIf torch backends mkldnn is_available MKL-DNN build disabled test_hardswish_hardsigmoid set_default_dtype torch float op_map = prim MKLDNNHardSwish F hardswish prim MKLDNNHardSigmoid F hardsigmoid input_sizes = mkldnn_opname aten_op op_map items size input_sizes inplace True False inplace_str = _ inplace inplace_tgt = inplace graph_str = f graph input Tensor None = prim Constant Tensor = aten to_mkldnn input Tensor = mkldnn_opname inplace_str inplace_tgt g = torch _C parse_ir graph_str m = createFunctionFromGraph g x = torch rand size ` inplace=False ` intentional otherwise we modify input we aren t testing aten impls anyways assertEqual aten_op x inplace=False m x to_dense unittest skipIf torch backends mkldnn is_available MKL-DNN build disabled test_scalar_mul set_default_dtype torch float Mod nn Module __init__ - None super __init__ mod = nn Conv d padding= forward x = mod x + mod = Mod eval scripted = torch jit freeze torch jit script mod optimized = torch jit optimize_for_inference scripted inp = torch rand cant inplaced first use can second FileCheck check ScalarMul check ScalarMul_ run optimized graph assertEqual optimized inp mod inp test_remove_detach Mod nn Module forward x y = x detach y y mod = Mod eval frozen_mod = torch jit freeze torch jit script mod inp = torch randn FileCheck check_not aten detach run frozen_mod graph assertEqual frozen_mod inp mod inp test_remove_detach_not_applied Mod nn Module forward x y = x detach x y mod = Mod eval frozen_mod = torch jit freeze torch jit script mod inp = torch randn FileCheck check aten detach run frozen_mod graph assertEqual frozen_mod inp mod inp skipIfTorchDynamo somehow causing hanging during python shutdown unittest skipIf torch backends mkldnn is_available MKL-DNN build disabled TestMKLDNNReinplacing JitTestCase setUp super setUp default_dtype = torch get_default_dtype torch set_default_dtype torch float tearDown super tearDown torch set_default_dtype default_dtype getConv nn Conv d kernel_size= stride= eval getInput torch rand freezeAndConvert mod mod = torch jit freeze torch jit script mod eval run_pass convert_frozen_ops_to_mkldnn mod graph mod checkResults mod mod inp = getInput assertEqual mod inp mod inp test_successful simple conv-relu mod_eager = nn Sequential getConv nn Hardswish nn ReLU mod = freezeAndConvert mod_eager FileCheck check mkldnn_convolution check_next prim MKLDNNHardSwish_ check_next aten relu_ run mod graph checkResults mod_eager mod test_merge_liveness Mod nn Module __init__ tensor super __init__ tensor = tensor forward x mul can inplaced since x dead after use temporary = x tensor temporary livespan node add can inplaced temporary + temporary temporary mod_eager = nn Sequential getConv Mod torch rand mod = freezeAndConvert mod_eager FileCheck check aten mul_ check_not aten add_ run mod graph checkResults mod_eager mod test_always_alive_values Mod nn Module __init__ tensor super __init__ tensor = tensor forward x x can t inplaced because its value check inplacing pass doesnt try inplace tensor because its always alive x tensor x mod_eager = nn Sequential getConv Mod torch rand mod = freezeAndConvert mod_eager FileCheck check_not aten mul_ run mod graph checkResults mod_eager mod conv = getConv Mod nn Module __init__ - None super __init__ tensor = torch rand conv = conv forward x shapes dont add up just testing particular pattern conv_output = conv x conv_output conv torch add x x mod = freezeAndConvert Mod x input graph so should inplaced torch add x x call FileCheck check_not aten add_ run mod graph test_switch_inputs_to_inplace Mod nn Module __init__ tensor super __init__ tensor = tensor forward x tensor cannot inplaced however x can bc add commutative we can reverse inputs add_ tensor + x mod_eager = nn Sequential getConv Mod torch rand mod = freezeAndConvert mod_eager FileCheck check aten add_ run mod graph checkResults mod_eager mod __name__ == __main__ raise_on_run_directly test test_jit py