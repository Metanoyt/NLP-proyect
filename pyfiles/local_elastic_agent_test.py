usr bin env python Owner s oncall r p Copyright c Facebook Inc its affiliates All rights reserved This source code licensed under BSD-style license found LICENSE file root directory source tree json multiprocessing mp os shutil signal socket tempfile time unittest uuid collections abc Callable dataclasses dataclass typing Optional unittest mock unittest mock Mock patch torch torch distributed dist torch distributed elastic rendezvous registry rdzv_registry torch distributed rpc rpc torch distributed elastic agent server api RunResult WorkerSpec WorkerState torch distributed elastic agent server local_elastic_agent LocalElasticAgent TORCHELASTIC_HEALTH_CHECK_PORT TORCHELASTIC_TIMER_FILE torch distributed elastic multiprocessing DefaultLogsSpecs Std torch distributed elastic multiprocessing errors ChildFailedError record torch distributed elastic rendezvous RendezvousParameters torch distributed elastic rendezvous etcd_server EtcdServer torch distributed rpc backend_registry BackendType torch testing _internal common_utils skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN init_rpc name backend rank = int os environ RANK world_size = int os environ WORLD_SIZE rpc init_rpc name=name backend=backend rank=rank world_size=world_size rpc_master msg init_rpc master BackendType TENSORPIPE ret = rpc rpc_sync to= worker func=_echo args= msg rpc shutdown f ret worker rpc_worker init_rpc worker BackendType TENSORPIPE rpc shutdown _happy_function _sad_function raise RuntimeError sad because i throw dummy_compute - torch Tensor returns predefined size random Tensor torch rand dummy_compute_simulate_rank_failure - torch Tensor fails rank once other cases returns predefined size random Tensor os environ RANK == os environ TORCHELASTIC_RESTART_COUNT == os kill os getpid torch rand _fatal_signal_function expected_error_index int sig int rank = int os environ RANK rank == expected_error_index os kill os getpid sig _check_master_port_addr_override expected_master_addr str expected_master_port int actual_master_addr = os environ MASTER_ADDR actual_master_port = int os environ MASTER_PORT expected_master_addr = actual_master_addr expected_master_port = actual_master_port raise RuntimeError f Expected addr expected_master_addr expected_master_port got addr actual_master_addr actual_master_port _bipolar_function rank = int os environ RANK rank == _happy_function _sad_function _bipolar_sleep_function sleep_sec rank = int os environ RANK rank == _sleep sleep_sec _sad_function _dist_sum wait= rank = int os environ RANK world_size = int os environ WORLD_SIZE dist init_process_group backend= gloo t = torch tensor rank time sleep wait dist all_reduce t op=dist reduce_op SUM expected_sum = sum range world_size actual = t item expected_sum = actual raise RuntimeError f Expected rank sum expected_sum got actual _sleep sleep_sec - int time sleep sleep_sec int os environ RANK dataclass RankInfo rank int role_rank int group_rank int role_world_size int world_size int _get_role_info - RankInfo rank = int os environ RANK role_rank = int os environ ROLE_RANK group_rank = int os environ GROUP_RANK role_world_size = int os environ ROLE_WORLD_SIZE world_size = int os environ WORLD_SIZE RankInfo rank role_rank group_rank role_world_size world_size _echo msg msg _check_env_function just check these env vars exist os environ will naturally throw variable does exist env_vars = RANK LOCAL_RANK ROLE_RANK ROLE_NAME GROUP_RANK LOCAL_WORLD_SIZE ROLE_WORLD_SIZE WORLD_SIZE GROUP_WORLD_SIZE MASTER_ADDR MASTER_PORT TORCHELASTIC_RESTART_COUNT TORCHELASTIC_MAX_RESTARTS TORCHELASTIC_RUN_ID TORCHELASTIC_USE_AGENT_STORE TORCH_NCCL_ASYNC_ERROR_HANDLING var env_vars _ = os environ var _check_env_value key str expected str checks env var ` ` key ` ` matches ` ` value ` ` function intended used entrypoint elastic run key os environ raise RuntimeError f Environment variable key found os environ actual = os getenv key expected = actual raise RuntimeError f os environ key = actual f does equal expected value expected _check_local_watchdog_setup key str should_exist bool should_exist key os environ raise RuntimeError f Environment variable key found os environ should_exist key os environ raise RuntimeError f Environment variable key found os environ acquire_available_port Uses sockets acquire available port os use Note To reduce race condition where another process grabs port after function returns available port we should aim use port quickly possible addrs = socket getaddrinfo host= localhost port=None family=socket AF_UNSPEC type=socket SOCK_STREAM addr addrs family type proto _ _ = addr try s = socket socket family type proto s bind localhost s listen port = s getsockname s close port except OSError e s close print f Socket creation attempt failed e raise RuntimeError Failed create socket dataclass Conf Holds arguments launch agent e g simulates agent run node entrypoint Callable local_world_size int args tuple = role str = default redirects Std = Std NONE tee Std = Std NONE LocalElasticAgentTest unittest TestCase classmethod setUpClass cls start standalone single process etcd server use all tests cls _etcd_server = EtcdServer cls _etcd_server start classmethod tearDownClass cls stop standalone etcd server cls _etcd_server stop setUp _test_dir = tempfile mkdtemp prefix=self __class__ __name__ _run_id = str uuid uuid split - tearDown shutil rmtree _test_dir log_dir - str tempfile mkdtemp prefix= torchelastic_ dir=self _test_dir get_worker_spec node_config Conf min_nodes= max_nodes= max_restarts= monitor_interval= master_addr_override Optional str = None master_port_override Optional int = None is_host=True rdzv_params = RendezvousParameters backend=self _backend endpoint=self _endpoint run_id=self _run_id min_nodes=min_nodes max_nodes=max_nodes is_host=is_host rdzv_handler = rdzv_registry get_rendezvous_handler rdzv_params WorkerSpec role=node_config role local_world_size=node_config local_world_size entrypoint=node_config entrypoint args=node_config args rdzv_handler=rdzv_handler max_restarts=max_restarts monitor_interval=monitor_interval master_addr=master_addr_override master_port=master_port_override get_agent spec WorkerSpec node_config Conf start_method str = spawn exit_barrier_timeout= log_line_prefix_template Optional str = None - LocalElasticAgent LocalElasticAgent spec start_method=start_method exit_barrier_timeout=exit_barrier_timeout logs_specs=DefaultLogsSpecs log_dir=self log_dir redirects=node_config redirects tee=node_config tee log_line_prefix_template=log_line_prefix_template pyre-fixme Pyre able infer type decorator ` torch distributed elastic multiprocessing errors record ` record run_agent conf Conf agent_results Optional mp Queue = None role agent_result min_nodes= max_nodes= start_method str = spawn max_restarts int = exit_barrier_timeout= master_addr_override Optional str = None master_port_override Optional int = None is_host=True monitor_interval= log_line_prefix_template Optional str = None - Optional RunResult Runs single agent This method can called either separate process main test process When calling method separate process make sure pass ` ` agent_results ` ` multiprocessing Queue so agent s run results can returned If ` ` agent_results ` ` omitted then run result returned method spec = get_worker_spec node_config=conf min_nodes=min_nodes max_nodes=max_nodes max_restarts=max_restarts master_addr_override=master_addr_override master_port_override=master_port_override is_host=is_host monitor_interval=monitor_interval agent = get_agent spec=spec node_config=conf start_method=start_method exit_barrier_timeout=exit_barrier_timeout log_line_prefix_template=log_line_prefix_template result = agent run spec rdzv_handler shutdown agent_results agent_results put conf role result result is_failed raise ChildFailedError spec get_entrypoint_name result failures agent_results result run_job node_configs list Conf exit_barrier_timeout int = log_line_prefix_template Optional str = None - dict str list RunResult Simulates running distributed job running multiple agents one each process Agent run main process test coverage ease debugging nnodes = len node_configs each element queue holds tuple role RunResult each agent agent_results = mp Queue run first agent first config main process test coverage + ease debugging important we loop reverse order b c running fn main process blocks procs = node_idx reversed range len node_configs conf = node_configs node_idx run_agent_args = conf conf agent_results agent_results min_nodes nnodes max_nodes nnodes start_method spawn max_restarts exit_barrier_timeout exit_barrier_timeout is_host node_idx == log_line_prefix_template log_line_prefix_template p = mp Process target=self run_agent kwargs=run_agent_args procs append p p start p procs p join results dict str list RunResult = while agent_results empty role run_result = agent_results get results setdefault role append run_result results run_test_with_backend backend str test_to_run Callable Sets backend determines endpoint before running given test Note This method must invoked run any test functions spawn agent This because function sets backend endpoint parameters _backend = backend _backend == etcd-v _backend == etcd _endpoint = _etcd_server get_endpoint default c d backend _endpoint = f localhost acquire_available_port test_to_run dummy_compute res = run_agent Conf entrypoint=dummy_compute local_world_size= assertFalse res is_failed return_value res return_values values assertIsInstance return_value torch Tensor assertEqual return_value shape skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_dummy_compute_c d run_test_with_backend backend= c d test_to_run=self dummy_compute skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_dummy_compute_etcd run_test_with_backend backend= etcd test_to_run=self dummy_compute skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_dummy_compute_etcd_v run_test_with_backend backend= etcd-v test_to_run=self dummy_compute run_happy_function res = run_agent Conf entrypoint=_happy_function local_world_size= assertFalse res is_failed assertIsNone res return_values assertIsNone res return_values skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_happy_function_c d run_test_with_backend backend= c d test_to_run=self run_happy_function skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_happy_function_etcd run_test_with_backend backend= etcd test_to_run=self run_happy_function skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_happy_function_etcd_v run_test_with_backend backend= etcd-v test_to_run=self run_happy_function check_master_addr_port_override master_addr = test_host master_port = res = run_agent Conf entrypoint=_check_master_port_addr_override args= master_addr master_port local_world_size= master_addr_override=master_addr master_port_override=master_port assertFalse res is_failed assertIsNone res return_values skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_check_master_addr_port_override_etcd run_test_with_backend backend= etcd test_to_run=self check_master_addr_port_override skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_check_master_addr_port_override_etcd_v run_test_with_backend backend= etcd-v test_to_run=self check_master_addr_port_override run_check_env_function just checks all env vars we need set user script actually set res = run_agent Conf entrypoint=_check_env_function local_world_size= assertFalse res is_failed run_check_nccl_async_error_handling_env make sure TORCH_NCCL_ASYNC_ERROR_HANDLING set os environ honored patch dict os environ TORCH_NCCL_ASYNC_ERROR_HANDLING res = run_agent Conf entrypoint=_check_env_value local_world_size= args= TORCH_NCCL_ASYNC_ERROR_HANDLING assertFalse res is_failed run_check_nccl_async_error_handling_env_default present env var should default res = run_agent Conf entrypoint=_check_env_value local_world_size= args= TORCH_NCCL_ASYNC_ERROR_HANDLING assertFalse res is_failed run_agent_local_watchdog_setup_enabled Set env watchdog watchdog_env_name = TORCHELASTIC_TIMER_FILE watchdog_file_path = tmp watchdog_timer_ + str uuid uuid os environ watchdog_env_name = watchdog_file_path Run agent node_conf = Conf entrypoint=_check_local_watchdog_setup local_world_size= args= TORCHELASTIC_TIMER_FILE True spec = get_worker_spec node_conf max_restarts= agent = get_agent spec node_config=node_conf res = agent run assertFalse res is_failed run_agent_local_watchdog_setup_disabled Do set env watchdog watchdog_env_name = TORCHELASTIC_TIMER_FILE watchdog_env_name os environ del os environ watchdog_env_name Run agent node_conf = Conf entrypoint=_check_local_watchdog_setup local_world_size= args= TORCHELASTIC_TIMER_FILE False spec = get_worker_spec node_conf max_restarts= agent = get_agent spec node_config=node_conf res = agent run assertFalse res is_failed skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_agent_local_watchdog_setup_enabled_etcd run_test_with_backend backend= etcd test_to_run=self run_agent_local_watchdog_setup_enabled skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_agent_local_watchdog_setup_enabled_c d run_test_with_backend backend= c d test_to_run=self run_agent_local_watchdog_setup_enabled skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_agent_local_watchdog_setup_disabled_etcd run_test_with_backend backend= etcd test_to_run=self run_agent_local_watchdog_setup_disabled skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_agent_local_watchdog_setup_disabled_c d run_test_with_backend backend= c d test_to_run=self run_agent_local_watchdog_setup_disabled run_agent_healthcheck_setup_enabled Set env healthcheck healthcheck_port_env_name = TORCHELASTIC_HEALTH_CHECK_PORT os environ healthcheck_port_env_name = Run agent node_conf = Conf entrypoint=_check_local_watchdog_setup local_world_size= args= TORCHELASTIC_HEALTH_CHECK_PORT True spec = get_worker_spec node_conf max_restarts= agent = get_agent spec node_config=node_conf res = agent run assertFalse res is_failed run_agent_healthcheck_setup_disabled Do set env healthcheck healthcheck_port_env_name = TORCHELASTIC_HEALTH_CHECK_PORT healthcheck_port_env_name os environ del os environ healthcheck_port_env_name Run agent node_conf = Conf entrypoint=_check_local_watchdog_setup local_world_size= args= TORCHELASTIC_HEALTH_CHECK_PORT False spec = get_worker_spec node_conf max_restarts= agent = get_agent spec node_config=node_conf res = agent run assertFalse res is_failed skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_agent_healthcheck_setup_enabled_etcd run_test_with_backend backend= etcd test_to_run=self run_agent_healthcheck_setup_enabled skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_agent_healthcheck_setup_enabled_c d run_test_with_backend backend= c d test_to_run=self run_agent_healthcheck_setup_enabled skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_agent_healthcheck_setup_disabled_etcd run_test_with_backend backend= etcd test_to_run=self run_agent_healthcheck_setup_disabled skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_agent_healthcheck_setup_disabled_c d run_test_with_backend backend= c d test_to_run=self run_agent_healthcheck_setup_disabled skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_check_env_function_etcd run_test_with_backend backend= etcd test_to_run=self run_check_env_function skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_check_nccl_async_error_handling_env_c d run_test_with_backend backend= c d test_to_run=self run_check_nccl_async_error_handling_env skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_check_nccl_async_error_handling_env_default_c d run_test_with_backend backend= c d test_to_run=self run_check_nccl_async_error_handling_env_default run_function_with_return_value res = run_agent Conf entrypoint=_echo args= foo local_world_size= assertFalse res is_failed assertEqual foo res return_values assertEqual foo res return_values skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_function_with_return_value_c d run_test_with_backend backend= c d test_to_run=self run_function_with_return_value skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_function_with_return_value_etcd run_test_with_backend backend= etcd test_to_run=self run_function_with_return_value skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_function_with_return_value_etcd_v run_test_with_backend backend= etcd-v test_to_run=self run_function_with_return_value simple_dist_sum res = run_agent Conf entrypoint=_dist_sum local_world_size= assertFalse res is_failed _dist_sum internally checks sum computed valid skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_simple_dist_sum_c d run_test_with_backend backend= c d test_to_run=self simple_dist_sum skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_simple_dist_sum_etcd run_test_with_backend backend= etcd test_to_run=self simple_dist_sum skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_simple_dist_sum_etcd_v run_test_with_backend backend= etcd-v test_to_run=self simple_dist_sum run_distributed_sum_homogeneous log_line_prefix_template Optional str = None node_configs = Conf role= sum entrypoint=_dist_sum local_world_size= tee=Std ALL Conf role= sum entrypoint=_dist_sum local_world_size= tee=Std ALL When process method spawn coverage collector hangs due getting stuck _dist_sum waiting TCPStore workers join cluster TODO aivanou t come up proper fix res = run_job node_configs log_line_prefix_template=log_line_prefix_template assertEqual len res sum ranks = set run_results res sum assertFalse run_results is_failed ranks update run_results return_values keys assertSetEqual set range + ranks unittest skipIf TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN test incompatible dev dbg asan tsan test_run_distributed_sum_homogeneous_c d run_test_with_backend backend= c d test_to_run=self run_distributed_sum_homogeneous test_run_with_custom_log_lines log_line_prefix_template = $ role_name -$ local_rank $ rank run_test_with_backend backend= c d test_to_run=lambda run_distributed_sum_homogeneous log_line_prefix_template unittest skipIf TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN test incompatible dev dbg asan tsan test_run_distributed_sum_homogeneous_etcd run_test_with_backend backend= etcd test_to_run=self run_distributed_sum_homogeneous unittest skipIf TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN test incompatible dev dbg asan tsan test_run_distributed_sum_homogeneous_etcd_v run_test_with_backend backend= etcd-v test_to_run=self run_distributed_sum_homogeneous run_distributed_sum_heterogeneous sums all ranks agents each running workers respectively sum should equal + + + + + = sum asserted inside _dist_sum node_configs = Conf role= sum entrypoint=_dist_sum local_world_size= Conf role= sum entrypoint=_dist_sum local_world_size= Conf role= sum entrypoint=_dist_sum local_world_size= When process method spawn coverage collector hangs due getting stuck _dist_sum waiting TCPStore workers join cluster TODO aivanou t come up proper fix res = run_job node_configs assertEqual len res sum ranks = set run_results res sum assertFalse run_results is_failed ranks update run_results return_values keys assertSetEqual set range + + ranks skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_distributed_sum_heterogeneous_c d run_test_with_backend backend= c d test_to_run=self run_distributed_sum_heterogeneous skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_distributed_sum_heterogeneous_etcd run_test_with_backend backend= etcd test_to_run=self run_distributed_sum_heterogeneous skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_distributed_sum_heterogeneous_etcd_v run_test_with_backend backend= etcd-v test_to_run=self run_distributed_sum_heterogeneous run_sad_function checks error propagation logic replyfile = os path join _test_dir error json mock patch dict os environ TORCHELASTIC_ERROR_FILE replyfile assertRaises ChildFailedError cm run_agent Conf entrypoint=_sad_function local_world_size= rank failure = cm exception get_first_failure failure_data = failure error_file_data message open replyfile fp data = json load fp message ran two both failed first failure either rank assertTrue rank assertTrue failure local_rank assertEqual failure exitcode assertEqual data message failure_data message assertEqual int data extraInfo timestamp failure timestamp skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_sad_function_c d run_test_with_backend backend= c d test_to_run=self run_sad_function skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_sad_function_etcd run_test_with_backend backend= etcd test_to_run=self run_sad_function skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_sad_function_etcd_v run_test_with_backend backend= etcd-v test_to_run=self run_sad_function run_bipolar_function checks agent failure handling logic node_conf = Conf entrypoint=_bipolar_function local_world_size= spec = get_worker_spec node_conf max_restarts= agent = get_agent spec node_config=node_conf run_result = agent run assertTrue run_result is_failed assertEqual agent _remaining_restarts assertEqual WorkerState FAILED agent get_worker_group state assertTrue agent _total_execution_time skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_bipolar_function_c d run_test_with_backend backend= c d test_to_run=self run_bipolar_function skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_bipolar_function_etcd run_test_with_backend backend= etcd test_to_run=self run_bipolar_function skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_run_bipolar_function_etcd_v run_test_with_backend backend= etcd-v test_to_run=self run_bipolar_function correct_rank_assignment_heterogeneous node_configs = Conf role= master entrypoint=_get_role_info local_world_size= Conf role= trainer entrypoint=_get_role_info local_world_size= Conf role= trainer entrypoint=_get_role_info local_world_size= Conf role= trainer entrypoint=_get_role_info local_world_size= Conf role= trainer entrypoint=_get_role_info local_world_size= Conf role= ps entrypoint=_get_role_info local_world_size= Conf role= ps entrypoint=_get_role_info local_world_size= results = run_job node_configs print f heterogeneous job result results assertEqual len results master assertEqual len results trainer assertEqual len results ps assert_rank_consistency results expected_role_world_sizes= master trainer + + + ps + unittest skipIf TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN test incompatible dev dbg asan tsan test_correct_rank_assignment_heterogeneous_etcd run_test_with_backend backend= etcd test_to_run=self correct_rank_assignment_heterogeneous unittest skipIf TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN test incompatible dev dbg asan tsan test_correct_rank_assignment_heterogeneous_etcd_v run_test_with_backend backend= etcd-v test_to_run=self correct_rank_assignment_heterogeneous correct_rank_assignment_homogeneous node_configs = Conf role= master entrypoint=_get_role_info local_world_size= Conf role= trainer entrypoint=_get_role_info local_world_size= Conf role= trainer entrypoint=_get_role_info local_world_size= Conf role= trainer entrypoint=_get_role_info local_world_size= Conf role= trainer entrypoint=_get_role_info local_world_size= Conf role= ps entrypoint=_get_role_info local_world_size= Conf role= ps entrypoint=_get_role_info local_world_size= results = run_job node_configs print f homogeneous job result results assertEqual len results master assertEqual len results trainer assertEqual len results ps assert_rank_consistency results expected_role_world_sizes= master trainer ps unittest skipIf TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN test incompatible dev dbg asan tsan test_correct_rank_assignment_homogeneous_etcd run_test_with_backend backend= etcd test_to_run=self correct_rank_assignment_homogeneous unittest skipIf TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN test incompatible dev dbg asan tsan test_correct_rank_assignment_homogeneous_etcd_v run_test_with_backend backend= etcd-v test_to_run=self correct_rank_assignment_homogeneous assert_rank_consistency run_results dict str list RunResult expected_role_world_sizes dict str int Asserts ranks consecutive w r t role_rank If local world sizes role_rank_ - ranks role_rank_ - ranks etc global_ranks list int = role - role_rank role_ranks dict str list int = group rank - rank role_rank grouped_ranks dict int list tuple int int = global world size == sum all role world sizes expected_world_size = sum expected_role_world_sizes values role results run_results items result results res = result return_values role_info res values rank = role_info rank role_rank = role_info role_rank group_rank = role_info group_rank role_world_size = role_info role_world_size world_size = role_info world_size assertEqual expected_world_size world_size assertEqual expected_role_world_sizes role role_world_size grouped_ranks setdefault group_rank append rank role_rank role_ranks setdefault role append role_rank global_ranks append rank global_ranks = sorted global_ranks assertEqual list range expected_world_size global_ranks role expected_role_world_size expected_role_world_sizes items assertEqual list range expected_role_world_size sorted role_ranks role Make sure each agent assigns consecutive ranks workers The first argument global_rank second argument role_rank ranks_lst grouped_ranks values assert_ranks_sequential ranks_lst assert_ranks_sequential ranks_lst assert_ranks_sequential ranks_pairs rank_idx ranks = sorted rank_pair rank_idx rank_pair ranks_pairs start_rank end_rank = ranks ranks - assertEqual list range start_rank end_rank + ranks double_agent_fault_tolerance start ` ` nnodes ` ` agents kill restart odd ones validate fault-tolerance works nnodes = wait = node_conf = Conf entrypoint=_dist_sum args= wait local_world_size= agent_results = mp Queue agent_args = conf node_conf agent_results agent_results min_nodes nnodes max_nodes nnodes max_restarts procs = _ range nnodes p = mp Process target=self run_agent kwargs=agent_args procs append p p start restart odd agents i range nnodes i = procs i kill p = mp Process target=self run_agent kwargs=agent_args procs i = p p start i range nnodes p = procs i p join assertEqual p exitcode unittest skipIf TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN test incompatible dev dbg asan tsan test_double_agent_fault_tolerance_etcd run_test_with_backend backend= etcd test_to_run=self double_agent_fault_tolerance unittest skipIf TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN test incompatible dev dbg asan tsan test_double_agent_fault_tolerance_etcd_v run_test_with_backend backend= etcd-v test_to_run=self double_agent_fault_tolerance no_exit_barrier_on_failure start ` ` nnodes ` ` agents kill restart odd ones validate fault-tolerance works nnodes = wait = node_conf = Conf entrypoint=_bipolar_sleep_function args= wait local_world_size= agent_results = mp Queue monitor_interval_s = agent_args = conf node_conf agent_results agent_results min_nodes nnodes max_nodes nnodes max_restarts exit_barrier_timeout monitor_interval monitor_interval_s procs = _ range nnodes p = mp Process target=self run_agent kwargs=agent_args procs append p p start wait all processes finish should take exit barrier time exit_interval_between_agents = i range nnodes p = procs i p join assertNotEqual p exitcode exit_interval_between_agents = time monotonic - exit_interval_between_agents Validate processes finish close each other Using slightly higher timeout than monitor_interval make less flaky assertGreater monitor_interval_s exit_interval_between_agents Agents cleaned up until monitor_interval unittest skipIf TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN test incompatible dev dbg asan tsan test_no_exit_barrier_on_failure run_test_with_backend backend= c d test_to_run=self no_exit_barrier_on_failure double_agent_elastic start ` ` nnodes ` ` agents kill odd ones do restart validate elasticity scale-down works scale-up covered fault_tolerance test min_nodes = max_nodes = wait = node_conf = Conf entrypoint=_dist_sum args= wait local_world_size= agent_results = mp Queue agent_args = conf node_conf agent_results agent_results min_nodes min_nodes max_nodes max_nodes max_restarts procs = _ range max_nodes p = mp Process target=self run_agent kwargs=agent_args procs append p p start kill odd agents i range max_nodes i = procs i kill i range max_nodes p = procs i p join i == assertEqual p exitcode assertEqual -signal SIGKILL p exitcode unittest skipIf TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN test incompatible dev dbg asan tsan test_double_agent_elastic_c d run_test_with_backend backend= c d test_to_run=self double_agent_elastic unittest skipIf TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN test incompatible dev dbg asan tsan test_double_agent_elastic_etcd run_test_with_backend backend= etcd test_to_run=self double_agent_elastic unittest skipIf TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN test incompatible dev dbg asan tsan test_double_agent_elastic_etcd_v run_test_with_backend backend= etcd-v test_to_run=self double_agent_elastic torch_rpc Simple torch rpc example torchelastic Creates two agents simulate two node job each agent runs single worker worker calls rpc_sync worker msg = hello world node_configs = Conf role= master entrypoint=rpc_master args= msg local_world_size= tee=Std ALL Conf role= worker entrypoint=rpc_worker args= local_world_size= tee=Std ALL results = run_job node_configs master_retvals = results master return_values there only one master global rank stable so compare master value collection assertEqual f msg worker list master_retvals values unittest skipIf TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN test incompatible dev dbg asan tsan test_torch_rpc_c d run_test_with_backend backend= c d test_to_run=self torch_rpc unittest skipIf TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN test incompatible dev dbg asan tsan test_torch_rpc_etcd run_test_with_backend backend= etcd test_to_run=self torch_rpc unittest skipIf TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN test incompatible dev dbg asan tsan test_torch_rpc_etcd_v run_test_with_backend backend= etcd-v test_to_run=self torch_rpc workers_drift_success two agents one worker each finishes within ` ` sec ` ` seconds each other exit barrier timeout set ` ` sec ` ` sec = node_configs = Conf role= zzz entrypoint=_sleep args= sec local_world_size= Conf role= zzz entrypoint=_sleep args= sec local_world_size= results = run_job node_configs exit_barrier_timeout= sec i range run_results = results zzz i assertFalse run_results is_failed rank output run_results return_values items _sleep returns its own rank assertEqual rank output unittest skipIf TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN test incompatible dev dbg asan tsan test_workers_drift_success_etcd run_test_with_backend backend= etcd test_to_run=self workers_drift_success unittest skipIf TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN test incompatible dev dbg asan tsan test_workers_drift_success_etcd_v run_test_with_backend backend= etcd-v test_to_run=self workers_drift_success workers_drift_fail two agents one worker each finishes within ` ` x sec ` ` seconds each other exit barrier timeout set Exit barriers should NOT fail job sec = node_configs = Conf role= zzz entrypoint=_sleep args= sec local_world_size= Conf role= zzz entrypoint=_sleep args= sec local_world_size= results = run_job node_configs exit_barrier_timeout= i range run_results = results zzz i assertFalse run_results is_failed rank output run_results return_values items _sleep returns its own rank assertEqual rank output unittest skipIf TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN test incompatible dev dbg asan tsan test_workers_drift_fail_etcd run_test_with_backend backend= etcd test_to_run=self workers_drift_fail unittest skipIf TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN test incompatible dev dbg asan tsan test_workers_drift_fail_etcd_v run_test_with_backend backend= etcd-v test_to_run=self workers_drift_fail patch torch distributed elastic utils store barrier barrier_failed barrier_mock Failure during barrier should NOT fail job barrier_mock side_effect = RuntimeError test error res = run_agent Conf entrypoint=_happy_function local_world_size= assertFalse res is_failed barrier_mock assert_called_once skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_barrier_failed_c d run_test_with_backend backend= c d test_to_run=self barrier_failed skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_barrier_failed_etcd run_test_with_backend backend= etcd test_to_run=self barrier_failed skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_barrier_failed_etcd_v run_test_with_backend backend= etcd-v test_to_run=self barrier_failed patch torch distributed elastic agent server local_elastic_agent start_processes shutdown_called start_processes_mock pcontext_mock = Mock pcontext_mock pids return_value = start_processes_mock return_value = pcontext_mock node_conf = Conf entrypoint=_happy_function local_world_size= spec = get_worker_spec node_conf max_restarts= agent = get_agent spec node_config=node_conf patch object agent _monitor_workers monitor_mock monitor_mock return_value = RunResult state=WorkerState SUCCEEDED return_values= agent run worker pcontext_mock close assert_called_once skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_shutdown_called_c d run_test_with_backend backend= c d test_to_run=self shutdown_called skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_shutdown_called_etcd run_test_with_backend backend= etcd test_to_run=self shutdown_called skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_shutdown_called_etcd_v run_test_with_backend backend= etcd-v test_to_run=self shutdown_called fail_rank_one_once res = run_agent Conf entrypoint=dummy_compute_simulate_rank_failure local_world_size= max_restarts= assertFalse res is_failed return_value res return_values values assertIsInstance return_value torch Tensor assertEqual return_value shape skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN test incompatible dev dbg asan test_rank_restart_after_failure run_test_with_backend backend= c d test_to_run=self fail_rank_one_once __name__ == __main__ raise RuntimeError This test currently used should enabled discover_tests py required