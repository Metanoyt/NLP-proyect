Owner s module dynamo dataclasses unittest mock torch torch _dynamo test_case torch _dynamo testing torch _dynamo testing same torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_utils TestCase try transformers modeling_outputs transformers configuration_utils PretrainedConfig transformers file_utils ModelOutput transformers modeling_outputs BaseModelOutput BaseModelOutputWithPastAndCrossAttentions BaseModelOutputWithPoolingAndCrossAttentions CausalLMOutputWithPast except ImportError modeling_outputs = None maybe_skip fn modeling_outputs None unittest skip requires HuggingFace fn fn TestHFPretrained torch _dynamo test_case TestCase maybe_skip test_pretrained fn tmp hasattr tmp somekey = + tmp return_dict + torch ones tmp max_length x = torch randn tmp = PretrainedConfig return_dict=True max_length= ref = fn x tmp opt_fn = torch compile fn backend= eager fullgraph=True res = opt_fn x tmp assertTrue same ref res maybe_skip test_pretrained_non_const_attr fn tmp tmp pruned_heads + - x = torch randn tmp = PretrainedConfig ref = fn x tmp opt_fn = torch compile backend= eager fullgraph=True fn res = opt_fn x tmp assertTrue same ref res TestModelOutput torch _dynamo test_case TestCase maybe_skip test_mo_create fn b tmp = BaseModelOutput + attentions=b + tmp torch _dynamo testing standard_test fn=fn nargs= expected_ops= maybe_skip test_mo_assign fn b tmp = BaseModelOutput last_hidden_state=b + tmp hidden_states = + tmp attentions = + b + tmp args = torch randn torch randn obj = fn args cnts = torch _dynamo testing CompileCounter opt_fn = torch _dynamo optimize_assert cnts fn obj = opt_fn args assertTrue same obj last_hidden_state obj last_hidden_state assertTrue same obj hidden_states obj hidden_states assertTrue same obj attentions obj attentions assertEqual cnts frame_count assertEqual cnts op_count _common fn op_count args = BaseModelOutput last_hidden_state=torch randn attentions=torch randn obj = fn args cnts = torch _dynamo testing CompileCounter opt_fn = torch _dynamo optimize_assert cnts fn obj = opt_fn args assertTrue same obj obj assertEqual cnts frame_count assertEqual cnts op_count op_count maybe_skip test_mo_getattr fn obj BaseModelOutput x = obj last_hidden_state obj hidden_states None x += obj hidden_states obj attentions None x += obj attentions x _common fn maybe_skip test_mo_getattr_missing fn obj BaseModelOutput getattr obj asdf None None obj asdf += obj attentions + _common fn maybe_skip test_mo_getitem fn obj BaseModelOutput x = obj last_hidden_state hidden_stats obj x += obj hidden_states attentions obj x += obj attentions x _common fn maybe_skip test_mo_tuple fn obj BaseModelOutput b = obj to_tuple + b _common fn maybe_skip test_mo_index fn obj BaseModelOutput obj + obj _common fn maybe_skip test_mo_init dataclasses dataclass MyDataClass ModelOutput torch Tensor b torch Tensor = None c torch Tensor = None d torch Tensor = None e torch Tensor = None fn obj class_fields = dataclasses fields obj assert len class_fields assert all field default None field class_fields other_fields_are_none = all getattr obj field name None field class_fields assert other_fields_are_none total = getattr obj class_fields name field class_fields v = getattr obj field name v None total += v total tensors = torch randn torch randn torch randn obj = MyDataClass tensors correct = fn obj obj = MyDataClass tensors cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts assertTrue same opt_fn obj correct assertEqual cnts frame_count assertEqual cnts op_count maybe_skip test_mo_init ModelOutput subclass runs different __post_init__ codepath dataclasses dataclass MyDataClass ModelOutput x torch FloatTensor = None fn x obj = MyDataClass x=x obj inp = torch randn opt_fn = torch compile fn backend= eager fullgraph=True assertEqual fn inp x opt_fn inp x maybe_skip test_mo_init_with_disable Can result non-function method super slot wrapper __setattr__ object objects graph breaks although may first Minimal repro https github com pytorch pytorch issues dataclasses dataclass MyDataClass ModelOutput x torch FloatTensor = None torch _dynamo disable recursive=False fn x MyDataClass x=x inp = torch randn opt_fn = torch compile fn backend= eager assertEqual fn inp x opt_fn inp x maybe_skip test_mo_newkey obj = BaseModelOutput fn obj obj wwww + inp = torch randn obj wwww = inp opt_fn = torch compile fn backend= eager fullgraph=True assertEqual fn obj opt_fn obj maybe_skip test_mo_from_outside fn obj obj attentions + obj = BaseModelOutput attentions=torch randn opt_fn = torch compile fn backend= eager fullgraph=True assertEqual fn obj opt_fn obj maybe_skip test_mo_reconstruct_bytecode fn inp BaseModelOutput attentions=inp + inp = torch randn opt_fn = torch compile fn backend= eager assertEqual fn inp attentions opt_fn inp attentions maybe_skip test_none Model torch nn Module forward x x = x + CausalLMOutputWithPast loss=None logits=x model = Model opt_model = torch compile model backend= eager fullgraph=True x = torch randn assertTrue same model x opt_model x maybe_skip test_reconstruction Model torch nn Module forward x x = x + CausalLMOutputWithPast loss=x logits=None model = Model x = torch randn eo = torch _dynamo export Model aten_graph=True x assertTrue same model x eo graph_module x TestModelOutputBert TestCase maybe_skip test_HF_bert_model_output device BertPooler torch nn Module __init__ - None super __init__ dense = torch nn Linear device activation = torch nn Tanh forward hidden_states torch Tensor - torch Tensor We pool model simply taking hidden state corresponding first token first_token_tensor = hidden_states pooled_output = dense first_token_tensor pooled_output = activation pooled_output pooled_output BertEncoder torch nn Module __init__ - None super __init__ forward hidden_states torch Tensor - BaseModelOutputWithPastAndCrossAttentions BaseModelOutputWithPastAndCrossAttentions last_hidden_state=hidden_states past_key_values=None hidden_states=None attentions=None cross_attentions=None BertModel torch nn Module __init__ - None super __init__ encoder = BertEncoder pooler = BertPooler forward sequence_output torch Tensor - BaseModelOutputWithPoolingAndCrossAttentions encoder_outputs = encoder sequence_output test __getitem__ to_tuple sequence_output = encoder_outputs pooled_output = pooler sequence_output pooler None None test CustomDictVariable create result = BaseModelOutputWithPoolingAndCrossAttentions last_hidden_state=sequence_output pooler_output=pooled_output past_key_values=encoder_outputs past_key_values hidden_states=encoder_outputs hidden_states attentions=encoder_outputs attentions cross_attentions=encoder_outputs cross_attentions test __setattr__ result pooler_output = pooled_output test __setitem__ result pooler_output = pooled_output result sequence_output = torch rand device model = BertModel orig_result = model sequence_output compiled_model = torch compile model backend= eager compiled_result = compiled_model sequence_output assertTrue torch allclose orig_result last_hidden_state compiled_result last_hidden_state assertTrue torch allclose orig_result pooler_output compiled_result pooler_output devices = cpu cuda xpu hpu instantiate_device_type_tests TestModelOutputBert globals only_for=devices allow_xpu=True __name__ == __main__ torch _dynamo test_case run_tests run_tests