mypy allow-untyped-defs typing Any cast Optional Union torch torch Tensor optimizer _capturable_doc _default_to_fused_or_foreach _differentiable_doc _disable_dynamo_if_unsupported _foreach_doc _get_capturable_supported_devices _get_scalar_dtype _maximize_doc _params_doc _to_scalar _use_grad_for_differentiable _view_as_real Optimizer ParamsT __all__ = Adadelta adadelta Adadelta Optimizer __init__ params ParamsT lr Union float Tensor = rho float = eps float = e- weight_decay float = foreach Optional bool = None capturable bool = False maximize bool = False differentiable bool = False isinstance lr Tensor lr numel = raise ValueError Tensor lr must -element = lr raise ValueError f Invalid learning rate lr = rho = raise ValueError f Invalid rho value rho = eps raise ValueError f Invalid epsilon value eps = weight_decay raise ValueError f Invalid weight_decay value weight_decay defaults = lr lr rho rho eps eps weight_decay weight_decay maximize maximize capturable capturable foreach foreach differentiable differentiable super __init__ params defaults __setstate__ state super __setstate__ state group param_groups group setdefault foreach None group setdefault maximize False group setdefault differentiable False group setdefault capturable False p group params p_state = state get p len p_state = torch is_tensor p_state step step_val = float p_state step p_state step = torch tensor step_val dtype=_get_scalar_dtype device=p device group capturable torch tensor step_val dtype=_get_scalar_dtype _init_group group dict str Any params_with_grad list Tensor grads list Tensor square_avgs list Tensor acc_deltas list Tensor state_steps list Tensor has_complex = False p Tensor p group params p grad None continue has_complex &#124; = torch is_complex p params_with_grad append p p grad is_sparse raise RuntimeError Adadelta does support sparse gradients grads append p grad state = state p Lazy state initialization len state == state step = torch zeros dtype=_get_scalar_dtype device=p device group capturable torch zeros dtype=_get_scalar_dtype state square_avg = torch zeros_like p memory_format=torch preserve_format state acc_delta = torch zeros_like p memory_format=torch preserve_format square_avgs append state square_avg acc_deltas append state acc_delta state_steps append state step has_complex _use_grad_for_differentiable step closure=None Perform single optimization step Args closure Callable optional A closure reevaluates model returns loss _cuda_graph_capture_health_check loss = None closure None torch enable_grad loss = closure group param_groups params_with_grad list Tensor = grads list Tensor = square_avgs list Tensor = acc_deltas list Tensor = state_steps list Tensor = lr rho eps weight_decay foreach maximize differentiable capturable = group lr group rho group eps group weight_decay group foreach group maximize group differentiable group capturable has_complex = _init_group group params_with_grad grads square_avgs acc_deltas state_steps adadelta params_with_grad grads square_avgs acc_deltas state_steps lr=lr rho=rho eps=eps weight_decay=weight_decay foreach=foreach maximize=maximize differentiable=differentiable capturable=capturable has_complex=has_complex loss Adadelta __doc__ = r Implements Adadelta algorithm math \begin aligned \rule mm pt \\ \textbf input \gamma \text lr \ \theta_ \text params \ f \theta \text objective \ \rho \text decay \ \lambda \text weight decay \\ \textbf initialize v_ \leftarrow \ \text square avg \ u_ \leftarrow \ \text accumulate variables \\ - ex \rule mm pt \\ \textbf \ t= \ \textbf \ \ldots \ \textbf do \\ \hspace mm g_t \leftarrow \nabla_ \theta f_t \theta_ t- \\ \hspace mm \ \lambda \neq \\ \hspace mm g_t \leftarrow g_t + \lambda \theta_ t- \\ \hspace mm v_t \leftarrow v_ t- \rho + g^ _t - \rho \\ \hspace mm \Delta x_t \leftarrow \frac \sqrt u_ t- + \epsilon \sqrt v_t + \epsilon g_t \hspace mm \\ \hspace mm u_t \leftarrow u_ t- \rho + \Delta x^ _t - \rho \\ \hspace mm \theta_t \leftarrow \theta_ t- - \gamma \Delta x_t \\ \rule mm pt \\ - ex \bf \ \theta_t \\ - ex \rule mm pt \\ - ex \end aligned For further details regarding algorithm we refer ` ADADELTA An Adaptive Learning Rate Method ` _ + rf Args _params_doc lr float Tensor optional coefficient scale delta before applied parameters default rho float optional coefficient used computing running average squared gradients default A higher value ` rho ` will result slower average which can helpful preventing oscillations learning process eps float optional term added denominator improve numerical stability default e- weight_decay float optional weight decay L penalty default _foreach_doc _capturable_doc _maximize_doc _differentiable_doc _ADADELTA\ An Adaptive Learning Rate Method https arxiv org abs _single_tensor_adadelta params list Tensor grads list Tensor square_avgs list Tensor acc_deltas list Tensor state_steps list Tensor lr float rho float eps float weight_decay float maximize bool differentiable bool capturable bool has_complex bool If compiling compiler will handle cudagraph checks see note torch compile x capturable torch compiler is_compiling capturable capturable_supported_devices = _get_capturable_supported_devices supports_xla=False all p device type == step device type p device type capturable_supported_devices p step zip params state_steps strict=True raise AssertionError f If capturable=True params state_steps must supported devices capturable_supported_devices torch jit is_scripting lr = _to_scalar lr param grad square_avg acc_delta step zip params grads square_avgs acc_deltas state_steps strict=True step += grad = grad maximize -grad weight_decay = grad = grad add param alpha=weight_decay torch is_complex param square_avg = torch view_as_real square_avg acc_delta = torch view_as_real acc_delta grad = torch view_as_real grad square_avg mul_ rho addcmul_ grad grad value= - rho std = square_avg add eps sqrt_ delta = acc_delta add eps sqrt_ differentiable delta = delta clone delta div_ std mul_ grad acc_delta mul_ rho addcmul_ delta delta value= - rho torch is_complex param delta = torch view_as_complex delta param add_ delta alpha=-lr _multi_tensor_adadelta params list Tensor grads list Tensor square_avgs list Tensor acc_deltas list Tensor state_steps list Tensor lr float rho float eps float weight_decay float maximize bool differentiable bool capturable bool has_complex bool differentiable raise AssertionError _foreach ops don t support autograd If compiling compiler will handle cudagraph checks see note torch compile x capturable torch compiler is_compiling capturable capturable_supported_devices = _get_capturable_supported_devices supports_xla=False all p device type == step device type p device type capturable_supported_devices p step zip params state_steps strict=True raise AssertionError f If capturable=True params state_steps must supported devices capturable_supported_devices len params == lr = _to_scalar lr grouped_tensors = Optimizer _group_tensors_by_device_and_dtype params grads square_avgs acc_deltas state_steps type ignore list-item device_params_ device_grads_ device_square_avgs_ device_acc_deltas_ device_state_steps_ _ grouped_tensors values device_params = cast list Tensor device_params_ device_grads = cast list Tensor device_grads_ device_square_avgs = cast list Tensor device_square_avgs_ device_acc_deltas = cast list Tensor device_acc_deltas_ device_state_steps = cast list Tensor device_state_steps_ has_complex _view_as_real device_params device_grads device_square_avgs device_acc_deltas Update steps If steps CPU foreach will fall back slow path which for-loop calling t add over over will then wrapped into Tensor over over again which slower than we just wrapped once now The alpha required assure we go right overload torch compiler is_compiling device_state_steps is_cpu torch _foreach_add_ device_state_steps torch tensor device= cpu alpha= torch _foreach_add_ device_state_steps maximize device_grads = torch _foreach_neg device_grads type ignore assignment weight_decay = Reuse intermediate memory device_grads already allocated maximize maximize torch _foreach_add_ device_grads device_params alpha=weight_decay device_grads = torch _foreach_add type ignore assignment device_grads device_params alpha=weight_decay torch _foreach_mul_ device_square_avgs rho torch _foreach_addcmul_ device_square_avgs device_grads device_grads value= - rho std = torch _foreach_add device_square_avgs eps torch _foreach_sqrt_ std deltas = torch _foreach_add device_acc_deltas eps torch _foreach_sqrt_ deltas torch _foreach_div_ deltas std torch _foreach_mul_ deltas device_grads torch _foreach_mul_ device_acc_deltas rho torch _foreach_addcmul_ device_acc_deltas deltas deltas value= - rho If LR tensor branch will internally call item which will cause silent incorrectness we capturing capturable isinstance lr torch Tensor torch _foreach_mul_ deltas -lr torch _foreach_add_ device_params deltas torch _foreach_add_ device_params deltas alpha=-lr _disable_dynamo_if_unsupported single_tensor_fn=_single_tensor_adadelta adadelta params list Tensor grads list Tensor square_avgs list Tensor acc_deltas list Tensor state_steps list Tensor kwonly args defaults supported functions compiled torchscript issue setting kwarg now functional API compiled torch distributed optim capturable bool = False foreach Optional bool = None differentiable bool = False has_complex bool = False lr float rho float eps float weight_decay float maximize bool r Functional API performs Adadelta algorithm computation See ` ~torch optim Adadelta ` details check slow during compilation so we skip s strictly needed we can add check back dynamo torch compiler is_compiling all isinstance t torch Tensor t state_steps raise RuntimeError API has changed ` state_steps ` argument must contain list singleton tensors We still respect when user inputs False foreach foreach None _ foreach = _default_to_fused_or_foreach params differentiable use_fused=False foreach torch jit is_scripting raise RuntimeError torch jit script supported foreach optimizers foreach torch jit is_scripting func = _multi_tensor_adadelta func = _single_tensor_adadelta func params grads square_avgs acc_deltas state_steps lr=lr rho=rho eps=eps weight_decay=weight_decay maximize=maximize differentiable=differentiable capturable=capturable has_complex=has_complex