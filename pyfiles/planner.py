abc io operator dataclasses dataclass enum auto Enum functools reduce typing Any Optional Union torch torch distributed checkpoint metadata ChunkStorageMetadata Metadata MetadataIndex STATE_DICT_TYPE StorageMeta TensorProperties __all__ = WriteItemType LoadItemType BytesIOWriteData TensorWriteData WriteItem ReadItem SavePlan LoadPlan SavePlanner LoadPlanner WriteItemType Enum TENSOR = auto SHARD = auto BYTE_IO = auto LoadItemType Enum TENSOR = auto BYTE_IO = auto dataclass frozen=True BytesIOWriteData nbytes int dataclass frozen=True TensorWriteData chunk ChunkStorageMetadata properties TensorProperties size torch Size dataclass frozen=True WriteItem Dataclass which holds information about what needs written storage index MetadataIndex type WriteItemType Size bytesIO data written bytes_io_data Optional BytesIOWriteData = None Value present s tensor write tensor_data Optional TensorWriteData = None tensor_storage_size - Optional int Calculates storage size underlying tensor None tensor write Returns Optional int storage size bytes underlying tensor any tensor_data None None numels = reduce operator mul tensor_data size dtype_size = torch _utils _element_size tensor_data properties dtype numels dtype_size dataclass frozen=True ReadItem Read Item type LoadItemType Index into state_dict dest_index MetadataIndex Offsets into destination tensor dest_offsets torch Size Index into checkpoint storage_index MetadataIndex Offset into checkpoint data storage_offsets torch Size Size hypercube copy lengths torch Size dataclass frozen=True SavePlan items list WriteItem storage_data Any = None planner_data Any = None This used indicate ranks should use cached plans write data instead usable bool = True dataclass LoadPlan items list ReadItem storage_data Any = None planner_data Any = None SavePlanner abc ABC Abstract defining protocol used save_state_dict plan save process SavePlanners stateful objects can used customize whole save process SavePlanner acts access proxy state_dict so any transformation done will visible whole process A planner subclass can expect following sequence calls during save_state_dict set_up_planner - called all ranks Signals start checkpoint save create_local_plan - called all ranks Process state_dict produces ` SavePlan ` will sent global planning create_global_plan - called coordinator rank only Takes SavePlan all ranks make any global decision finish_plan - called all ranks This gives each rank chance adjust global planning decisions resolve_data - called multiple times each rank Lookups value ` state_dict ` storage layer write Users recommended extend DefaultSavePlanner instead interface directly most changes can expressed changes single method There usual patterns extension Rewriting state_dict This simplest way extend save process doesn t requite understanding intrincacies how SavePlan works xdoctest +SKIP undefined vars RenamePlanner DefaultSavePlanner set_up_planner state_dict STATE_DICT_TYPE storage_meta Optional StorageMeta is_coordinator bool - None prefix all keys ` foo_ ` ` super set_up_planner foo_ + k v k v state_dict items storage_meta is_coordinator Modifying local plan lookup tandem This useful when fine control how data persisted xdoctest +SKIP undefined vars FP Planner DefaultSavePlanner create_local_plan plan = super create_local_plan p plan p tensor_data None p tensor_data properties dtype = torch float plan resolve_data write_item item = super resolve_data write_item item write_item type == WriteItemType BYTE_IO item torch float Using global planning step make central decisions can t made individually each rank xdoctest +SKIP undefined vars itertools zip_longest dataclasses replace DDPLoadBalancingPlanner DefaultSavePlanner This uses default local plan behavior having all non-sharded writes rank This sample doesn t handle ShardedTensors create_global_plan all_plans iters = iter all_plans items len all_plans items_per_rank = item item items item None items zip zip_longest iters strict=True all_plans = replace plan items=items plan items zip all_plans items_per_rank strict=True super create_global_plan all_plans Finally some planners need save additional metadata checkpoint accomplished having each rank contribute their data items local plan global planner aggregate them xdoctest +SKIP undefined vars SaveExtraDataPlanner DefaultSavePlanner create_local_plan - SavePlan plan = super create_local_plan replace plan planner_data= per-rank-data create_global_plan all_plans List SavePlan - Tuple List SavePlan Metadata global_plan metadata = super create_global_plan all_plans merged_data = p planner_data p global_plan metadata = replace metadata planner_data=merged_data global_plan metadata Save plan current rank computed ` create_local_plan ` API Cached local rank _cached_save_plan dict str SavePlan = Final save plan current rank This created merging plan created ` create_local_plan ` API result ` create_global_plan ` given rank This final plan computed ` finish_plan ` API gets sent ` write_data ` Cached local rank _cached_final_save_plan dict str SavePlan = Collection all local plans all ranks This input ` create_global_plan ` API Cached coordinator rank _cached_all_plans dict str list SavePlan = Global checkpoint plan computed ` create_global_plan ` API Cached coordinator rank _cached_global_plan dict str list SavePlan = Metadata global checkpoint plan computed ` create_global_plan ` API Cached coordinator rank _cached_metadata dict str Metadata = abc abstractmethod set_up_planner state_dict STATE_DICT_TYPE storage_meta Optional StorageMeta = None is_coordinator bool = False - None Initialize planner save ` ` state_dict ` ` Implementations should save those values they won t provided lated save process This called all ranks abc abstractmethod create_local_plan - SavePlan Compute save plan current rank This will aggregated passed create_global_plan Planner specific data can passed through SavePlan planner_data This called all ranks abc abstractmethod create_global_plan all_plans list SavePlan - tuple list SavePlan Metadata Compute global checkpoint plan local plan each rank This called coordinator rank only abc abstractmethod finish_plan new_plan SavePlan - SavePlan Merge plan created ` create_local_plan ` result ` create_global_plan ` This called all ranks abc abstractmethod resolve_data write_item WriteItem - Union torch Tensor io BytesIO Transform prepare ` ` write_item ` ` ` ` state_dict ` ` storage ensuring idempotency thread-safety Lookup object associated ` ` write_item ` ` ` ` state_dict ` ` apply any transformation such serialization prior storage layer consuming Called each rank multiple times least once per WriteItem final SavePlan This method should idempotent thread-save StorageWriter implementations free call frequently they need Any transformation allocates memory should lazily done when his method called order reduce peak memory required checkpointing When returning tensors they can any device format they can views too It s storage layer responsibility figure out how save them LoadPlanner Abstract defining protocol used load_state_dict plan load process LoadPlanner stateful objects can used customize whole load process LoadPlanner acts access proxy state_dict so any transformation done will visible whole process A planner subclass can expect following sequence calls during load_state_dict set_up_planner - called all ranks Signals start loading checkpoint create_local_plan - called all ranks Process state_dict produces ` LoadPlan ` will sent global planning create_global_plan - called coordinator rank only Takes LoadPlan all ranks make any global decision load_bytes - called multiple times each rank This called once per non-tensor value state_dict resolve_tensor commit_tensor - called multiple times each rank They called pair each Tensor value state_dict Users recommended extend DefaultLoadPlanner instead interface directly most changes can expressed changes single method There two usual patterns extension Rewriting state_dict This simplest way extend load process doesn t requite understanding intrincacies how LoadPlan works We need keep reference original state_dict load happens place so we need able perform place xdoctest +SKIP undefined vars RenamePlanner DefaultLoadPlanner set_up_planner state_dict STATE_DICT_TYPE metadata Metadata is_coordinator bool - None original_state_dict = state_dict state_dict = foo_ + k v k v state_dict items flatten_sharded_tensors state_dict = _flatten_sharded_tensors state_dict flatten_state_dict state_dict mappings = flatten_state_dict state_dict state_dict = state_dict metadata = metadata is_coordinator = is_coordinator load_bytes read_item value Remove foo_ prefix original_state_dict read_item dest_index fqn = torch load value weights_only=False Modifying resolve_tensor commit_tensor handle load time transformation xdoctest +SKIP undefined vars MetaModelMaterialize DefaultSavePlanner resolve_tensor read_item tensor = super resolve_tensor read_item torch empty_like tensor device= cpu commit_tensor read_item tensor state_dict read_item dest_index fqn = tensor abc abstractmethod set_up_planner state_dict STATE_DICT_TYPE metadata Optional Metadata = None is_coordinator bool = False - None Initialize instance load data into ` ` state_dict ` ` N B This called every rank abc abstractmethod create_local_plan - LoadPlan Create LoadPlan based state_dict metadata provided set_up_planner N B This called every rank abc abstractmethod create_global_plan global_plan list LoadPlan - list LoadPlan Compute global load plan plans each rank N B This called coordinator rank only abc abstractmethod finish_plan central_plan LoadPlan - LoadPlan Accept plan coordinator final LoadPlan abc abstractmethod load_bytes read_item ReadItem value io BytesIO - None Load item described ` ` read_item ` ` ` ` value ` ` This method expected modify in-place underlying state_dict The contents ` ` value ` ` defined SavePlanner used produce checkpoint being loaded resolve_bytes read_item ReadItem - io BytesIO Return BytesIO used StorageReader load ` read_item ` The BytesIO should alias one underlying state_dict StorageReader will replace its contents raise NotImplementedError LoadPlanner resolve_bytes implemented abc abstractmethod resolve_tensor read_item ReadItem - torch Tensor Return tensor described ` ` read_item ` ` used StorageReader load ` read_item ` The tensor should alias one underlying state_dict StorageReader will replace its contents If any reason s possible planner can use ` ` commit_tensor ` ` method copy data back one state_dict abc abstractmethod commit_tensor read_item ReadItem tensor torch Tensor - None Call once StorageReader finished loading data into ` ` tensor ` ` The provided tensor same one returned call ` ` resolve_tensor ` ` This method only needed LoadPlanner needs post process ` ` tensor ` ` prior copying back one state_dict The contents tensor will follow its device synchronization model