mypy allow-untyped-defs typing Optional torch torch optim _functional F torch Tensor torch distributed optim _deprecation_warning _scripted_functional_optimizer_deprecation_warning __all__ list str = Define TorchScript compatible Functional Adam Optimizer where we use these optimizer functional way Instead using ` param grad ` when updating parameters we explicitly allow distributed optimizer pass gradients ` step ` function In way we could separate gradients parameters allow multithreaded trainer update parameters without data traces accumulating same grad NOTE This should only used distributed optimizer internals meant expose user torch jit script _FunctionalAdam __init__ params list Tensor lr float = e- betas tuple float float = eps float = e- weight_decay float = amsgrad bool = False maximize bool = False foreach bool = False fused bool = False _allow_empty_param_list bool = False _scripted_functional_optimizer_deprecation_warning stacklevel= = lr raise ValueError f Invalid learning rate lr = eps raise ValueError f Invalid epsilon value eps = betas raise ValueError f Invalid beta parameter index betas = betas raise ValueError f Invalid beta parameter index betas = weight_decay raise ValueError f Invalid weight_decay value weight_decay defaults = lr lr eps eps beta betas beta betas weight_decay weight_decay amsgrad = amsgrad maximize = maximize foreach = foreach fused = fused state = torch jit annotate dict torch Tensor dict str torch Tensor len params == _allow_empty_param_list raise ValueError optimizer got empty parameter list NOTE we only have one param_group don t allow user add additional param group s common use case param_group = params params step_param param Tensor grad Optional Tensor Similar step operates single parameter optionally gradient tensor params_with_grad = grads = exp_avgs = exp_avg_sqs = max_exp_avg_sqs = state_steps list Tensor = has_complex = torch is_complex param grad None params_with_grad append param grads append grad param state state param = state = state param state step = torch tensor state exp_avg = torch zeros_like param memory_format=torch preserve_format state exp_avg_sq = torch zeros_like param memory_format=torch preserve_format amsgrad state max_exp_avg_sq = torch zeros_like param memory_format=torch preserve_format state = state param exp_avgs append state exp_avg exp_avg_sqs append state exp_avg_sq amsgrad max_exp_avg_sqs append state max_exp_avg_sq state_steps append state step torch no_grad F adam params_with_grad grads exp_avgs exp_avg_sqs max_exp_avg_sqs state_steps amsgrad=self amsgrad has_complex=has_complex maximize=self maximize beta =self defaults beta beta =self defaults beta lr=self defaults lr weight_decay=self defaults weight_decay eps=self defaults eps foreach=self foreach fused=self fused grad_scale=None found_inf=None step gradients list Optional Tensor params = param_group params params_with_grad = grads = exp_avgs = exp_avg_sqs = max_exp_avg_sqs = state_steps list Tensor = has_complex = False len params = len gradients raise ValueError gradients passed does equal size parameters + f Params length len params + f Gradients length len gradients param gradient zip param_group params gradients gradient None has_complex &#124; = torch is_complex param params_with_grad append param grads append gradient Lazy state initialization param state state param = state = state param state step = torch tensor Exponential moving average gradient values state exp_avg = torch zeros_like param memory_format=torch preserve_format Exponential moving average squared gradient values state exp_avg_sq = torch zeros_like param memory_format=torch preserve_format amsgrad Maintains max all exp moving avg sq grad values state max_exp_avg_sq = torch zeros_like param memory_format=torch preserve_format state = state param exp_avgs append state exp_avg exp_avg_sqs append state exp_avg_sq amsgrad max_exp_avg_sqs append state max_exp_avg_sq state_steps append state step torch no_grad F adam params_with_grad grads exp_avgs exp_avg_sqs max_exp_avg_sqs state_steps amsgrad=self amsgrad has_complex=has_complex maximize=self maximize beta =self defaults beta beta =self defaults beta lr=self defaults lr weight_decay=self defaults weight_decay eps=self defaults eps foreach=self foreach fused=self fused grad_scale=None found_inf=None