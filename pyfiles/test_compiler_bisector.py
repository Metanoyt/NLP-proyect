Owner s module dynamo contextlib contextmanager importlib import_module torch torch _prims_common utils torch _dynamo utils preserve_rng_state torch _inductor config torch _inductor compiler_bisector CompilerBisector torch _inductor test_case TestCase torch library _scoped_library Library torch testing _internal triton_utils requires_cuda_and_triton aten = torch ops aten f = torch float i = torch int i = torch int requires_cuda_and_triton TestCompilerBisector TestCase test_ns = _test_bisector tearDown hasattr torch ops test_ns delattr torch ops test_ns hasattr lib del lib m del lib get_op name getattr getattr torch ops test_ns name default get_lib lib = Library test_ns FRAGMENT noqa TOR lib = lib lib test_bad_decomp import_module torch _inductor compile_fx bad_exp_decomp rate= generator=None assert generator None torch _check utils is_complex_dtype dtype utils is_integer_dtype dtype utils is_boolean_dtype dtype lambda f Exponential distribution continuous probability distribution \ dtype must floating point you specified dtype torch _check rate lambda f exponential_ expects lambda found lambda= rate torch rand_like float nan contextmanager patch_exp_decomp torch _inductor compile_fx select_decomp_table old_decomp get_decomp out = old_decomp out = out copy out aten exponential default = bad_exp_decomp out torch _inductor compile_fx select_decomp_table = get_decomp try yield finally torch _inductor compile_fx select_decomp_table = old_decomp vq x x + exponential_ test_fn torch _dynamo reset patch_exp_decomp vq_compiled = torch compile vq x = torch randn cuda torch _dynamo utils preserve_rng_state vq x out_compiled = vq_compiled x out_compiled isnan any out = CompilerBisector do_bisect test_fn assertEqual out backend aot_eager_decomp_partition assertEqual out subsystem decomposition assertEqual out bisect_number assertTrue aten exponential out debug_info test_pre_grad operator torch _inductor config similar setup test_joint_graph see below pass_fn graph torch fx Graph nodes = graph find_nodes op= call_function target=operator add assert len nodes == args = list nodes args args = nodes args = tuple args config pre_grad_custom_pass = pass_fn foo x x + test_fn torch _dynamo reset inp = torch rand out = foo inp out_c = torch compile foo inp torch allclose out out_c out = CompilerBisector do_bisect test_fn assertEqual out backend inductor assertEqual out subsystem pre_grad_passes assertEqual out bisect_number assertTrue pre_grad_custom_pass out debug_info test_joint_graph torch _inductor config pass_fn graph torch fx Graph nodes = graph find_nodes op= call_function target=torch ops aten add Tensor assert len nodes == args = list nodes args args = nodes args = tuple args config joint_custom_post_pass = pass_fn foo x x + test_fn torch _dynamo reset inp = torch rand device= cuda out = foo inp out_c = torch compile foo inp torch allclose out out_c out = CompilerBisector do_bisect test_fn assertEqual out backend inductor assertEqual out subsystem joint_graph_passes assertEqual out bisect_number assertTrue joint_custom_post_pass out debug_info test_rng foo torch rand device= cuda + test_fn torch _dynamo reset preserve_rng_state out = foo preserve_rng_state out_c = torch compile foo torch allclose out out_c out = CompilerBisector do_bisect test_fn assertEqual out backend inductor assertEqual out subsystem inductor_fallback_random assertTrue inductor_fallback_random out debug_info test_crossref _scoped_library test_ns FRAGMENT lib lib define foo Tensor x - Tensor op = get_op foo Foo torch autograd Function staticmethod forward ctx x Emulate AutoDispatchBelowADInplaceOrView which bound into python torch _C _AutoDispatchBelowAutograd torch _C _ExcludeDispatchKeyGuard torch _C DispatchKeySet torch _C DispatchKey ADInplaceOrView op x staticmethod backward ctx gx gx foo_impl x x view_as x clone foo_meta x x view_as x lib impl foo Foo apply Autograd lib impl foo foo_impl CPU lib impl foo foo_meta Meta x = torch tensor requires_grad=True test_fn torch _dynamo reset try torch testing assert_close torch compile op x op x except Exception False True out = CompilerBisector do_bisect test_fn assertEqual out backend aot_eager_decomp_partition_crossref test_emulate_precision_casts test_fn torch _dynamo reset calculate_scale inp amax = torch abs torch max inp scale = torch clamp amax min= e- scale = scale torch float scale dtype = torch bfloat torch manual_seed inp = torch randn dtype=dtype device= cuda eager_scale = calculate_scale inp compile_scale = torch compile calculate_scale inp torch equal eager_scale compile_scale out = CompilerBisector do_bisect test_fn assertEqual out backend inductor assertEqual out subsystem inductor_emulate_precision_casts test_bad_lowering test_fn torch _dynamo reset config patch triton inject_relu_bug_TESTING_ONLY accuracy my_func x x - - relu inp = torch rand device= cuda torch allclose torch compile my_func inp my_func inp out = CompilerBisector do_bisect test_fn assertEqual out backend inductor assertEqual out subsystem lowerings assertEqual out bisect_number assertTrue relu out debug_info test_eager_backend should indicate problem first backend test_fn False out = CompilerBisector do_bisect test_fn assertEqual out backend eager assertEqual out subsystem None config patch test_configs bisect_pre_grad_graph True test_configs bisect_keep_custom_backend_for_inductor True test_bisect_pre_grad_graph f x i range x = x + x relu MyBackend __call__ gm example_inputs node_idx = node_to_graph_id node nonlocal node_idx out = node_idx node_idx += out split_gm = torch fx passes split_module split_module gm None node_to_graph_id keep_original_order=True name submod split_gm named_modules submod_ name test case simple enough using original example_inputs works sub moule submod forward = torch _inductor standalone_compile submod example_inputs dynamic_shapes= from_example_inputs options= split_gm test_fn torch _dynamo reset x = torch randn device= cuda config patch triton inject_relu_bug_TESTING_ONLY accuracy opt_f = torch compile f backend=MyBackend torch allclose opt_f x f x out = CompilerBisector do_bisect test_fn assertEqual out backend inductor assertEqual out subsystem pre_grad_graph assertEqual out bisect_number __name__ == __main__ torch _dynamo test_case run_tests run_tests