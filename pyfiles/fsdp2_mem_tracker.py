collections abc Callable copy deepcopy enum auto Enum functools partial wraps typing Any NamedTuple Optional TypeVar Union typing_extensions ParamSpec TypeVarTuple Unpack torch torch distributed _tools fake_collectives torch nn optim torch _guards active_fake_mode torch distributed _tools mem_tracker _RefType _State MemTracker torch distributed fsdp FSDPModule torch distributed fsdp _fully_shard _fsdp_param_group FSDPParamGroup torch utils _python_dispatch TorchDispatchMode torch utils _pytree tree_map_only torch utils weak WeakIdKeyDictionary weakref _TOTAL_KEY = Total __all__ = FSDPMemTracker _P = ParamSpec _P _R = TypeVar _R _Ts = TypeVarTuple _Ts c d = torch ops c d _FSDPRefType _RefType Enumerates categories memory usage FSDP modules including parameters gradients activations optimizer states Attributes SHARDED_PARAM str Memory usage sharded parameters UNSHARDED_PARAM str Memory usage unsharded parameters SHARDED_GRAD str Memory usage sharded gradients corresponding sharded parameters UNSHARDED_GRAD str Memory usage unsharded gradients corresponding unsharded parameters ACT str Memory usage activations tensors forward AC recomputation TEMP str Memory usage temporary tensors during backward pass including gradients activations ALL_GATHER str Memory usage all_gather output tensor REDUCE_SCATTER str Memory usage reduce_scatter input tensor OPT str Memory usage tensors storing optimizer states INP str Memory usage input tensors SHARDED_PARAM = Sharded Param UNSHARDED_PARAM = Unsharded Param BUFFER = Buffer SHARDED_GRAD = Sharded Grad UNSHARDED_GRAD = Unsharded Grad ACT = Activation TEMP = Temp ALL_GATHER = All Gather REDUCE_SCATTER = Reduce Scatter OPT = OptState INP = Inputs _SavedFSDPMethods NamedTuple pre_backward Callable post_backward Callable _FSDPModState _State Enumerates states FSDP modules during forward backward passes BEF_PRE_FW = Before Pre-Forward AFT_PRE_FW = After Pre-Forward BEF_POST_FW = Before Post-Forward AFT_POST_FW = After Post-Forward BEF_PRE_BW = Before Pre-Backward AFT_PRE_BW = After Pre-Backward BEF_POST_BW = Before Post-Backward AFT_POST_BW = After Post-Backward PRE_FW_AC = Pre-Forward AC POST_FW_AC = Post-Forward AC PEAK_FW = Peak Forward PEAK_BW = Peak Backward _FSDPModMemStats A store memory statistics FSDP module Args mod_fqn str The fully qualified name FSDP module Attributes snapshots Dict _FSDPModState Dict torch device Dict str int A dictionary memory snapshots module different states defined ` ` _FSDPModState ` ` Each key device each value another dictionary keys memory reference types defined ` ` _FSDPRefType ` ` values memory consumed bytes __init__ mod_fqn str - None mod_fqn = mod_fqn local_peak dict torch device int = snapshots dict _FSDPModState list dict torch device dict str int = _FSDPState Enum PRE_FW = auto FW = auto POST_FW = auto PRE_BW = auto BW = auto POST_BW = auto FSDPMemTracker MemTracker A ` ` TorchDispatchMode ` ` based context manager extends ` ` torch distributed _tools mem_tracker MemTracker ` ` track categorize peak memory module-wise memory usage FSDP modules It tracks peak memory usage across all devices all FSDP modules module tree categorizes tensor memory usage defined ` ` _FSDPRefType ` ` Further captures memory ` snapshots ` different stages module execution defined ` ` _FSDPModState ` ` Attributes memory_tracking A weakref key dictionary store memory statistics each module Each key reference module each value ` ` _FSDPModMemStats ` ` object stores memory statistics module Args mod torch nn Module The root FSDP module tracked optm torch optim Optimizer optional The optimizer tracked Note Please refer ` ` torch distributed _tools mem_tracker MemTracker ` ` learn about limitations Example usage code-block python module = optimizer = inp = fmt = FSDPMemTracker module optimizer fmt track_inputs inp fmt optimizer zero_grad loss = module inp print After Forward fmt display_snapshot current loss backward optimizer step fmt display_snapshot peak fmt display_modulewise_snapshots depth= units= MB __init__ mod torch nn Module optm Optional torch optim Optimizer = None - None super __init__ assert isinstance mod FSDPModule FSDPMemTracker only supports FSDP modules _root_mod = mod _optm = optm _fsdp_mod_to_saved_methods WeakIdKeyDictionary = WeakIdKeyDictionary _fsdp_state _FSDPState = _FSDPState PRE_FW _ref_class type _RefType = _FSDPRefType _instrument_fsdp_sharded_params_grads fsdp_param_group FSDPParamGroup - None Track sharded params grads after initialization fsdp_param fsdp_param_group fsdp_params _update_and_maybe_create_winfos fsdp_param sharded_param _FSDPRefType SHARDED_PARAM sharded_grad = fsdp_param sharded_param grad sharded_grad None _update_and_maybe_create_winfos sharded_grad _FSDPRefType SHARDED_GRAD _fsdp_state_pre_forward fsdp_mod FSDPModule orig_fsdp_state_pre_fw Callable _P tuple tuple Unpack _Ts dict str Any - Callable _P tuple tuple Unpack _Ts dict str Any We capture memory snapshots before after ` ` FSDPState _pre_forward ` ` attribute ` unsharded ` params ` all_gather ` buffers There three cases Case If module ` ` memory_tracking ` ` dictionary create new ` ` _FSDPModMemStats ` ` instance module add ` ` memory_tracking ` ` dictionary Case If module already ` ` memory_tracking ` ` dictionary we backward means we AC region We check top most module AC region If we store weak reference set flag ` ` _in_ac ` ` True Case If module already ` ` memory_tracking ` ` dictionary we forward means module called second time If root module means we next iteration we error out If root module means s submodule being used multiple times same iteration which we allow track For Case we also initialize ` ` local_peak ` ` ` ` PEAK_FW ` ` snapshot module For Case we only capture snapshot after ` ` FSDPState _pre_forward ` ` runs because no-op wraps orig_fsdp_state_pre_fw inner args _P args kwargs _P kwargs - tuple tuple Unpack _Ts dict str Any _fsdp_state = _FSDPState PRE_FW mod_fqn = _mod_tracker get_known_fqn fsdp_mod assert mod_fqn None fsdp_mod memory_tracking mod_stat = _FSDPModMemStats mod_fqn memory_tracking fsdp_mod = mod_stat snapshot = get_tracker_snapshot mod_stat local_peak = dev dev_snap _TOTAL_KEY dev dev_snap snapshot items mod_stat snapshots setdefault _FSDPModState PEAK_FW append snapshot mod_stat snapshots setdefault _FSDPModState BEF_PRE_FW append deepcopy snapshot _mod_tracker is_bw parents = _mod_tracker parents - mod_fqn len parents == Global parents raise NotImplementedError FSDPMemTracker does support memory tracking multiple iterative calls Either use ` ` reset_mod_stats ` ` clear module memory stats previous iteration file github issue you need feature pyrefly ignore bad-assignment args kwargs = orig_fsdp_state_pre_fw args kwargs fsdp_state = fsdp_mod _get_fsdp_state fsdp_param_group = fsdp_state _fsdp_param_group fsdp_param fsdp_param_group fsdp_params _update_and_maybe_create_winfos fsdp_param unsharded_param _FSDPRefType UNSHARDED_PARAM mod_stat = memory_tracking fsdp_mod _mod_tracker is_bw state = _FSDPModState PRE_FW_AC _ac_mod None _ac_mod = weakref ref fsdp_mod _in_ac = True state = _FSDPModState AFT_PRE_FW mod_stat snapshots setdefault state append get_tracker_snapshot _fsdp_state = _FSDPState FW args kwargs inner _fsdp_state_post_forward fsdp_mod FSDPModule orig_fsdp_state_post_fw Callable _P _R - Callable _P _R We capture memory snapshots before after ` ` FSDPState _post_forward ` ` capture resharded state ` ` reshard_after_forward ` ` ` ` False ` ` There two cases Case This called backward which means we AC region If top most module AC region we set flag ` ` _in_ac ` ` False Case This called forward wraps orig_fsdp_state_post_fw inner args _P args kwargs _P kwargs - _R mod_stat = memory_tracking fsdp_mod _mod_tracker is_bw state = _FSDPModState POST_FW_AC _ac_mod None _ac_mod fsdp_mod _ac_mod = None _in_ac = False state = _FSDPModState BEF_POST_FW mod_stat snapshots setdefault state append get_tracker_snapshot _fsdp_state = _FSDPState POST_FW output = orig_fsdp_state_post_fw args kwargs _mod_tracker is_bw mod_stat snapshots setdefault _FSDPModState AFT_POST_FW append get_tracker_snapshot output inner _fsdp_param_group_pre_backward fsdp_mod FSDPModule orig_fsdp_param_group_pre_backward Callable _P Any - Callable _P None We capture memory snapshots before after ` ` FSDPParamGroup pre_backward ` ` capture pre-fetching unsharding params We also initialize ` ` local_peak ` ` ` ` PEAK_BW ` ` snapshot module wraps orig_fsdp_param_group_pre_backward inner args _P args kwargs _P kwargs - None _fsdp_state = _FSDPState PRE_BW mod_stat = memory_tracking fsdp_mod snapshot = get_tracker_snapshot mod_stat local_peak = dev dev_snap _TOTAL_KEY dev dev_snap snapshot items mod_stat snapshots setdefault _FSDPModState PEAK_BW append snapshot mod_stat snapshots setdefault _FSDPModState BEF_PRE_BW append deepcopy snapshot orig_fsdp_param_group_pre_backward args kwargs mod_stat snapshots setdefault _FSDPModState AFT_PRE_BW append get_tracker_snapshot _fsdp_state = _FSDPState BW inner _fsdp_param_group_post_backward fsdp_mod FSDPModule orig_fsdp_param_group_post_backward Callable _P Any - Callable _P None We capture memory snapshots before after ` ` FSDPParamGroup post_backward ` ` track attribute ` unsharded ` grads before post backward then ` sharded ` grads ` reduce_scatter ` buffers after post backward wraps orig_fsdp_param_group_post_backward inner args _P args kwargs _P kwargs - None fsdp_state = fsdp_mod _get_fsdp_state fsdp_param_group = fsdp_state _fsdp_param_group fsdp_param fsdp_param_group fsdp_params unsharded_grad = fsdp_param _unsharded_param grad unsharded_grad None _update_and_maybe_create_winfos unsharded_grad _FSDPRefType UNSHARDED_GRAD update_existing=True mod_stat = memory_tracking fsdp_mod mod_stat snapshots setdefault _FSDPModState BEF_POST_BW append get_tracker_snapshot _fsdp_state = _FSDPState POST_BW orig_fsdp_param_group_post_backward args kwargs fsdp_param_group = fsdp_state _fsdp_param_group fsdp_param fsdp_param_group fsdp_params sharded_grad = fsdp_param sharded_param grad sharded_grad None _update_and_maybe_create_winfos sharded_grad _FSDPRefType SHARDED_GRAD mod_stat snapshots setdefault _FSDPModState AFT_POST_BW append get_tracker_snapshot inner _instrument_fsdp_module - None We uninstall existing ` FSDPState _pre_forward ` ` FSDPState _post_forward ` hooks install our own hooks wrap them We choose over monkey-patching ` FSDPParamGroup pre_forward ` ` FSDPParamGroup post_forward ` because during AC these won t called TODO sanketpurandare This will need modified after PR https github com pytorch pytorch pull lands For backward we monkey-patch ` FSDPParamGroup pre_backward ` ` FSDPParamGroup post_backward ` pyrefly ignore missing-attribute module _root_mod modules isinstance module FSDPModule fsdp_state = module _get_fsdp_state fsdp_param_group = fsdp_state _fsdp_param_group _instrument_fsdp_sharded_params_grads fsdp_param_group fsdp_state _pre_forward_hook_handle remove fsdp_state _post_forward_hook_handle remove fsdp_state _pre_forward_hook_handle = pyrefly ignore missing-attribute module register_forward_pre_hook _fsdp_state_pre_forward module fsdp_state _pre_forward prepend=True with_kwargs=True pyrefly ignore missing-attribute fsdp_state _post_forward_hook_handle = module register_forward_hook _fsdp_state_post_forward module fsdp_state _post_forward prepend=False always_call=True _fsdp_mod_to_saved_methods module = _SavedFSDPMethods fsdp_param_group pre_backward fsdp_param_group post_backward fsdp_param_group pre_backward = _fsdp_param_group_pre_backward type ignore assignment module fsdp_param_group pre_backward fsdp_param_group post_backward = type ignore assignment _fsdp_param_group_post_backward module fsdp_param_group post_backward pyrefly ignore missing-attribute buffer _root_mod buffers _update_and_maybe_create_winfos buffer _FSDPRefType BUFFER _instrument_optimizer - None Register hook optimizer step track optimizer states The pre-hook set flag ` ` _in_opt ` ` True The post-hook unsets flag also tracks any optimizer states created during optimizer step _optm None _track_optimizer_states _FSDPRefType OPT _optm _opt_step_pre_hook optimizer optim Optimizer args Any kwargs Any - None _in_opt = True _opt_step_post_hook optimizer optim Optimizer args Any kwargs Any - None _track_optimizer_states _FSDPRefType OPT optimizer _in_opt = False _optimizer_hook_handles = _optm register_step_pre_hook _opt_step_pre_hook _optm register_step_post_hook _opt_step_post_hook _register_module_and_optimizer_hooks - None _instrument_fsdp_module _instrument_optimizer _deregister_module_and_optimizer_hooks - None fsdp_mod saved_methods _fsdp_mod_to_saved_methods items fsdp_state = fsdp_mod _get_fsdp_state fsdp_state _pre_forward_hook_handle remove fsdp_state _post_forward_hook_handle remove fsdp_state _pre_forward_hook_handle = fsdp_mod register_forward_pre_hook fsdp_state _pre_forward prepend=True with_kwargs=True fsdp_state _post_forward_hook_handle = fsdp_mod register_forward_hook fsdp_state _post_forward prepend=False fsdp_param_group = fsdp_state _fsdp_param_group fsdp_param_group pre_backward = saved_methods pre_backward fsdp_param_group post_backward = saved_methods post_backward _fsdp_mod_to_saved_methods clear _optimizer_hook_handles None handle _optimizer_hook_handles handle remove _optimizer_hook_handles = None track_inputs inputs tuple Any - None This used track input tensors model annotate them ` ` Inputs ` ` Args inputs Tuple Any A tuple containing input data This can include tensors well other data types Only tensors will tracked _track_inputs t torch Tensor - None _update_and_maybe_create_winfos t _FSDPRefType INP tree_map_only torch Tensor _track_inputs inputs track_external external Union nn Module optim Optimizer torch Tensor - None This no-op ` ` FSDPMemTracker ` ` __enter__ - FSDPMemTracker _depth == _register_module_and_optimizer_hooks _track_resize _track_dtensor_dispatch _peak_mem_snap = get_tracker_snapshot _peak_mem = dev dev_snap _TOTAL_KEY dev dev_snap _peak_mem_snap items _mod_tracker __enter__ TorchDispatchMode __enter__ _depth += __exit__ args Any - None _depth -= _depth == _deregister_module_and_optimizer_hooks _restore_resize _restore_dtensor_dispatch _mod_tracker __exit__ args TorchDispatchMode __exit__ args __torch_dispatch__ func types args= kwargs=None type ignore no-untyped-def func torch ops _c d_functional wait_tensor default active_fake_mode N B This hacky way override Meta IMPL wait_tensor The original impl returns new tensor which does happen eager mode when wait_tensor called pyrefly ignore unsupported-operation res = args res = func args kwargs If we tracking optimizer state we use optimizer reference type If we backward region AC region we use backward reference type Else we use forward reference type _in_opt reftype = _FSDPRefType OPT _mod_tracker is_bw _in_ac reftype = _FSDPRefType TEMP reftype = _FSDPRefType ACT func c d _allgather_base_ default _fsdp_state _FSDPState PRE_FW _FSDPState PRE_BW pyrefly ignore unsupported-operation output_tensor = args _update_and_maybe_create_winfos output_tensor _FSDPRefType ALL_GATHER update_existing=True func c d _reduce_scatter_base_ default _fsdp_state == _FSDPState POST_BW pyrefly ignore unsupported-operation input_tensor = args _update_and_maybe_create_winfos input_tensor _FSDPRefType REDUCE_SCATTER update_existing=True tree_map_only torch Tensor partial _track reftype res peak_state = _FSDPModState PEAK_BW _mod_tracker is_bw _FSDPModState PEAK_FW _update_peak_stats peak_state res