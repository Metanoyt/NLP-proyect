mypy allow-untyped-defs builtins functools warnings collections abc Callable typing Any Optional Union torch torch fx embedding_override input torch empty input shape weight shape - device= meta nn_layernorm_override input input torch_relu_override x x torch_nn_relu_override x x functional_relu_override x inplace=False assert inplace dont support inplace functional relu metatensor analysis x torch_where_override condition x y torch where returns broadcasted tensor condition x y so hack using addition condition device= meta + x device= meta + y device= meta torch_abs_override input out=None assert out None Dont support in-place abs MetaTensor analysis input manual_meta_overrides dict Callable Callable = torch nn Embedding embedding_override torch nn LayerNorm nn_layernorm_override torch relu torch_relu_override torch nn functional relu functional_relu_override torch nn ReLU torch_nn_relu_override torch where torch_where_override torch abs torch_abs_override gen_constructor_wrapper target functools wraps target wrapper args kwargs proxy = None check_has_proxy v isinstance v torch fx Proxy nonlocal proxy proxy = v torch fx node map_aggregate args check_has_proxy torch fx node map_aggregate kwargs check_has_proxy proxy None proxy tracer create_proxy call_function target args kwargs target args kwargs wrapper target MetaProxy torch fx Proxy install_tensor_meta tensor_meta _tensor_meta = tensor_meta size dim=None hasattr _tensor_meta _tensor_meta None _tensor_meta size dim dim tracer create_proxy call_method size dim dim dim hasattr _tensor_meta _tensor_meta None _tensor_meta dim tracer create_proxy call_method dim property shape hasattr _tensor_meta _tensor_meta None _tensor_meta shape tracer create_proxy call_function builtins getattr shape property dtype hasattr _tensor_meta _tensor_meta None _tensor_meta dtype tracer create_proxy call_function builtins getattr dtype property device Hack so we can track when devices used During meta-tensor propagation replace these values constant meta MetaDeviceAttribute device __getattr__ k k == _tensor_meta __getattribute__ k note added graph yet method call we peephole optimize method invocation MetaAttribute k MetaAttribute MetaProxy __init__ root attr str root = root attr = attr tracer = root tracer _node = None property node type ignore override node attributes added lazily since most will just method calls which do rely getitem call _node None _node = tracer create_proxy call_function getattr root attr node _node __call__ args kwargs tracer create_proxy call_method attr root + args kwargs MetaDeviceAttribute MetaAttribute pass proxys_to_metas v isinstance v MetaDeviceAttribute meta isinstance v torch fx Proxy assert isinstance v MetaProxy f Expected MetaProxy got type v assert hasattr v _tensor_meta MetaProxy does have associated meta v _tensor_meta v MetaTracer torch fx Tracer allow_insert_stateless_mods bool = True _TORCH_METHODS_TO_PATCH = arange zeros ones full_like eye create_proxy kind target args kwargs name=None type_expr=None proxy_factory_fn=None rv = super create_proxy kind target args kwargs name type_expr pyrefly ignore bad-argument-type proxy_factory_fn kind == placeholder target meta_args rv install_tensor_meta meta_args target rv target orig_fns NOTE tensor constructors PyTorch define ` device ` argument kwargs-only That why works If you add methods _TORCH_METHODS_TO_PATCH do define ` device ` kwarg-only will break you will likely see issues where we cannot infer size output device kwargs kwargs device = meta try args_metas = torch fx node map_aggregate args proxys_to_metas kwargs_metas = torch fx node map_aggregate kwargs proxys_to_metas kind == call_function meta_target = manual_meta_overrides get target target pyrefly ignore not-callable meta_out = meta_target args_metas kwargs_metas kind == call_method meta_target = getattr args_metas target type ignore index meta_out = meta_target args_metas kwargs_metas type ignore index kind == call_module assert hasattr orig_forward _disable_module_getattr = True try mod = root get_submodule target mod_type = type mod mod_type manual_meta_overrides meta_out = manual_meta_overrides mod_type mod args_metas kwargs_metas type ignore misc arg-type meta_out = orig_forward args_metas kwargs_metas finally _disable_module_getattr = False kind == get_attr _disable_module_getattr = True try attr_itr = root atoms = target split atom atoms attr_itr = getattr attr_itr atom assert isinstance attr_itr torch Tensor meta_out = attr_itr device= meta finally _disable_module_getattr = False rv TODO assert isinstance rv torch fx Proxy Dont support composite output yet rv install_tensor_meta meta_out except Exception e warnings warn f Could compute metadata kind target target e rv getattr attr attr_val parameter_proxy_cache getattr _disable_module_getattr False attr_val super getattr attr attr_val parameter_proxy_cache call_module m forward args kwargs orig_forward = forward super call_module m forward args kwargs _insert_module_as_submodule mod torch nn Module - str Helper method which tries insert module declared submodule idx = mod_name = mod __class__ __name__ lower path = f mod_name _ idx while hasattr root path path = f mod_name _ idx idx += root add_module path mod path path_of_module mod torch nn Module - str try super path_of_module mod except NameError allow_insert_stateless_mods len list mod parameters == len list mod buffers == path = _insert_module_as_submodule mod prev_module = path path raise proxy node MetaProxy node trace root meta_args dict str torch Tensor concrete_args=None type ignore override assert isinstance meta_args dict meta_args = meta_args patched_torch_methods = target gen_constructor_wrapper getattr torch target target _TORCH_METHODS_TO_PATCH orig_fns = set name wrapper orig patched_torch_methods items setattr torch name wrapper orig_fns add orig try graph = super trace root concrete_args graph _tracer_extras = meta_args meta_args graph finally name _ orig patched_torch_methods items setattr torch name orig symbolic_trace root Union torch nn Module Callable Any meta_args Optional dict str torch Tensor = None concrete_args Optional dict str Any = None - torch fx GraphModule tracer = MetaTracer graph = tracer trace root meta_args concrete_args type ignore arg-type name = root __class__ __name__ isinstance root torch nn Module root __name__ gm = torch fx GraphModule tracer root graph name gm