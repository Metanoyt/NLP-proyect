Owner s module inductor logging os unittest try test_aot_inductor_utils AOTIRunnerUtil except ImportError test_aot_inductor_utils AOTIRunnerUtil torch torch _inductor config torch _inductor test_case run_tests TestCase torch _inductor utils try_import_ck_lib torch testing _internal common_cuda tf _off torch testing _internal common_utils instantiate_parametrized_tests parametrize torch testing _internal inductor_utils _quantize_rowwise _quantize_tensorwise HAS_CPU HAS_CUDA_AND_TRITON HAS_CUDA_AND_TRITON torch cuda memory _set_allocator_settings expandable_segments False log = logging getLogger __name__ patch env tests needed _test_env = instantiate_parametrized_tests TestCKBackend TestCase setUp The new inductor cache refresh mechanism introduced https github com pytorch pytorch pull interacts badly persistent subprocesses during autotuning So we need disable automatic cache refresh before calling setUp parent old_disable_fresh_cache_envvar = os environ get INDUCTOR_TEST_DISABLE_FRESH_CACHE torch random manual_seed ck_dir _ _ _ = try_import_ck_lib ck_dir raise unittest SkipTest Composable Kernel library installed try os environ INDUCTOR_TEST_DISABLE_FRESH_CACHE = super setUp finally os environ INDUCTOR_TEST_DISABLE_FRESH_CACHE = old_disable_fresh_cache_envvar unittest skipIf torch version hip ROCM only unittest mock patch dict os environ _test_env parametrize max_autotune_gemm_backends CK CKTILE ATen Triton CK parametrize autotune_in_subproc True False parametrize use_aoti True False test_max_autotune_precompile_matmul max_autotune_gemm_backends autotune_in_subproc use_aoti Make sure autotuning mm doesn t crash mm b b tensor_options = device cuda dtype torch bfloat = torch randn tensor_options b = torch randn tensor_options assert rocm dir config config patch max_autotune True autotune_in_subproc autotune_in_subproc max_autotune_gemm_backends max_autotune_gemm_backends compile_threads rocm ck_max_profiling_configs rocm ck_tile_max_profiling_configs rocm ck_dir ck_dir tf _off use_aoti Y_compiled = AOTIRunnerUtil run model=mm example_inputs= b torch compile dynamic=False compiled_mm x w mm x w Y_compiled = compiled_mm b Y = mm a=a b=b torch testing assert_close Y_compiled Y unittest skipIf torch version hip ROCM only unittest mock patch dict os environ _test_env parametrize max_autotune_gemm_backends CK parametrize autotune_in_subproc True test_max_autotune_precompile_matmul_dynamic max_autotune_gemm_backends autotune_in_subproc Test matmul dynamic shapes tensor_options = device cuda dtype torch bfloat = torch randn tensor_options b = torch randn tensor_options torch _dynamo mark_dynamic assert rocm dir config config patch max_autotune True autotune_in_subproc autotune_in_subproc max_autotune_gemm_backends max_autotune_gemm_backends compile_threads rocm ck_max_profiling_configs rocm ck_tile_max_profiling_configs rocm ck_dir ck_dir tf _off torch compile dynamic=True compiled_mm b b Y_compiled = compiled_mm b Y = b torch testing assert_close Y_compiled Y = torch randn tensor_options Y _compiled = compiled_mm b Y = b torch testing assert_close Y _compiled Y unittest skipIf torch version hip ROCM only unittest mock patch dict os environ _test_env parametrize max_autotune_gemm_backends CK ATen Triton CK test_max_autotune_precompile_preselected max_autotune_gemm_backends End end test picking preselected ck instances mm b b tensor_options = device cuda dtype torch float = torch randn tensor_options b = torch randn tensor_options transpose assert rocm dir config config patch max_autotune True autotune_in_subproc True max_autotune_gemm_backends max_autotune_gemm_backends compile_threads rocm ck_dir ck_dir rocm use_preselected_instances True tf _off Y_compiled = torch compile mm dynamic=False b Y = mm b torch testing assert_close Y_compiled Y unittest skipIf torch version hip ROCM only unittest mock patch dict os environ _test_env parametrize max_autotune_gemm_backends Aten CK test_max_autotune_precompile_non_contiguous max_autotune_gemm_backends Make sure matmul non-contiguous inputs can fallback tensor_options = device cuda dtype torch float = torch empty_strided tensor_options b = torch empty_strided tensor_options assert rocm dir config config patch max_autotune True autotune_in_subproc True max_autotune_gemm_backends max_autotune_gemm_backends compile_threads rocm ck_dir ck_dir rocm ck_max_profiling_configs rocm ck_tile_max_profiling_configs tf _off torch compile dynamic=False mm b b Y_compiled = mm b Y_eager = b torch testing assert_close Y_compiled Y_eager equal_nan=True unittest skipIf torch version hip ROCM only unittest mock patch dict os environ _test_env parametrize max_autotune_gemm_backends CK ATen Triton CK parametrize x_shape test_max_autotune_addmm max_autotune_gemm_backends x_shape m k n = alpha beta = tensor_options = device cuda dtype torch float x = torch ones x_shape tensor_options = torch randn m k tensor_options b = torch randn k n tensor_options assert rocm dir config config patch max_autotune True autotune_in_subproc True max_autotune_gemm_backends max_autotune_gemm_backends compile_threads rocm ck_dir ck_dir rocm ck_max_profiling_configs tf _off torch compile dynamic=False addmm x b alpha beta torch addmm x b alpha=alpha beta=beta Y_compiled = addmm x b alpha beta Y_eager = torch addmm x b alpha=alpha beta=beta torch testing assert_close Y_compiled Y_eager unittest skip FIXME tenpercent kernel compilation errors gfx unittest skipIf torch version hip ROCM only unittest mock patch dict os environ _test_env parametrize max_autotune_gemm_backends CK ATen Triton CK parametrize quantize_type tensorwise rowwise parametrize has_bias True False test_max_autotune_scaled_mm max_autotune_gemm_backends quantize_type has_bias use_fast_accum = False runtime_arch = torch cuda get_device_properties gcnArchName gfx runtime_arch gfx runtime_arch skipTest f Unsupported arch runtime_arch output dtype dtype = torch bfloat tensor_options = device cuda dtype dtype M = N = K = x = torch randn M K tensor_options w = torch randn N K tensor_options bias = None has_bias bias = torch randn N tensor_options dtype_float = torch float _e m fnuz gfx runtime_arch torch float _e m fn f_quantize = _quantize_tensorwise quantize_type == tensorwise _quantize_rowwise quantize weight prior inference w_fp w_inverse_scale = f_quantize w dtype_float w_t_fp = w_fp t w_inverse_scale_t = w_inverse_scale t quantize input x x_fp x_inverse_scale = f_quantize x dtype_float assert rocm dir config linear x_fp x_inverse_scale w_t_fp w_inverse_scale bias y = torch _scaled_mm x_fp w_t_fp x_inverse_scale w_inverse_scale bias out_dtype=dtype use_fast_accum=use_fast_accum y y_eager = linear x_fp x_inverse_scale w_t_fp w_inverse_scale_t bias config patch max_autotune True max_autotune_gemm_backends max_autotune_gemm_backends compile_threads rocm ck_max_profiling_configs rocm ck_dir ck_dir linear_compiled = torch compile linear backend= inductor mode= max-autotune y_compiled = linear_compiled x_fp x_inverse_scale w_t_fp w_inverse_scale_t bias assertEqual y_eager dtype dtype assertEqual y_compiled dtype dtype torch testing assert_close y_eager y_compiled rtol= e- atol= unittest skipIf torch version hip ROCM only unittest mock patch dict os environ _test_env PYTORCH_MIOPEN_SUGGEST_NHWC parametrize max_autotune_conv_backends CK ATEN CK TRITON test_max_autotune_conv d max_autotune_conv_backends tensor_options = device cuda dtype torch float x = torch randn tensor_options w = torch randn tensor_options x_cl = x memory_format=torch channels_last w_cl = w memory_format=torch channels_last assert rocm dir config config patch max_autotune True autotune_in_subproc False max_autotune_conv_backends max_autotune_conv_backends compile_threads rocm ck_dir ck_dir rocm ck_max_profiling_configs tf _off torch compile dynamic=False conv d x w torch conv d x w Y_eager = torch conv d x_cl w_cl Y_compiled = conv d x_cl w_cl torch testing assert_close Y_compiled Y_eager atol= e- rtol= e- unittest skipIf torch version hip ROCM only unittest mock patch dict os environ _test_env parametrize max_autotune_gemm_backends CK ATen Triton CK test_max_autotune_precompile_bmm max_autotune_gemm_backends Test gemm-max-autotune torch bmm CK backend bmm b torch bmm b tensor_options = device cuda dtype torch bfloat = torch randn tensor_options b = torch randn tensor_options transpose assert rocm dir config config patch max_autotune True max_autotune_gemm_backends max_autotune_gemm_backends compile_threads rocm ck_max_profiling_configs rocm ck_dir ck_dir tf _off torch compile dynamic=False compiled_bmm x w bmm x w Y_compiled = compiled_bmm b Y_eager = bmm a=a b=b torch testing assert_close Y_compiled Y_eager __name__ == __main__ torch _inductor utils is_big_gpu Set env make work CI HAS_CUDA_AND_TRITON HAS_CPU is_big_gpu run_tests