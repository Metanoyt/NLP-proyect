mypy ignore-errors functools wraps partial itertools product chain islice itertools functools copy operator random unittest math enum torch numpy np numpy typing npt torch inf nan typing Any Union collections abc Sequence torch testing make_tensor torch testing _internal common_dtype _dispatch_dtypes floating_types floating_types_and complex_types floating_and_complex_types floating_and_complex_types_and all_types_and_complex_and all_types_and all_types_and_complex integral_types_and empty_types complex_types_and integral_types custom_types all_types_complex_float _and float _types torch testing _internal common_device_type \ onlyCPU onlyCUDA onlyNativeDeviceTypes disablecuDNN skipCUDAIfNoMagma skipCUDAIfNoMagmaAndNoCusolver skipCUDAIfNoCusolver skipCPUIfNoLapack skipCPUIfNoFFT skipCUDAIf precisionOverride skipCPUIfNoMklSparse toleranceOverride tol skipXPU torch testing _internal common_cuda PLATFORM_SUPPORTS_FLASH_ATTENTION PLATFORM_SUPPORTS_MEM_EFF_ATTENTION SM OrLater SM OrLater SM OrLater with_tf _off TEST_CUDNN _get_torch_cuda_version _get_torch_rocm_version torch testing _internal common_utils make_fullrank_matrices_with_distinct_singular_values TEST_WITH_ROCM IS_FBCODE IS_WINDOWS IS_MACOS IS_S X TEST_SCIPY torch_to_numpy_dtype_dict numpy_to_torch_dtype TEST_WITH_ASAN GRADCHECK_NONDET_TOL slowTest TEST_WITH_SLOW TEST_WITH_TORCHINDUCTOR MACOS_VERSION torch testing _utils wrapper_set_seed torch _refs refs noqa F torch _refs nn functional torch _refs special torch _refs linalg torch _prims prims noqa F torch utils _pytree pytree torch _vendor packaging version torch testing _internal opinfo core noqa F L M S XS _NOTHING _getattr_qual DecorateInfo SampleInput ErrorInput AliasInfo NumericsFilter OpInfo _generate_reduction_inputs _generate_reduction_kwargs sample_inputs_reduction ReductionOpInfo reference_inputs_elementwise_binary make_error_inputs_elementwise_binary generate_elementwise_binary_tensors generate_elementwise_binary_arbitrarily_strided_tensors generate_elementwise_binary_small_value_tensors generate_elementwise_binary_large_value_tensors generate_elementwise_binary_extremal_value_tensors generate_elementwise_binary_broadcasting_tensors generate_elementwise_binary_with_scalar_samples generate_elementwise_binary_with_scalar_and_type_promotion_samples generate_elementwise_binary_noncontiguous_tensors sample_inputs_elementwise_binary BinaryUfuncInfo sample_inputs_elementwise_unary generate_elementwise_unary_tensors generate_elementwise_unary_small_value_tensors generate_elementwise_unary_large_value_tensors generate_elementwise_unary_extremal_value_tensors reference_inputs_elementwise_unary UnaryUfuncInfo sample_inputs_spectral_ops SpectralFuncType SpectralFuncInfo ShapeFuncInfo sample_inputs_foreach ForeachFuncInfo gradcheck_wrapper_hermitian_input gradcheck_wrapper_ctc_loss gradcheck_wrapper_triangular_input gradcheck_wrapper_triangular_input_real_positive_diagonal gradcheck_wrapper_masked_operation gradcheck_wrapper_masked_pointwise_operation clone_sample torch testing _internal opinfo refs NOQA F _find_referenced_opinfo _inherit_constructor_args PythonRefInfo ReductionPythonRefInfo ElementwiseUnaryPythonRefInfo ElementwiseBinaryPythonRefInfo torch testing _internal opinfo utils np_unary_ufunc_integer_promotion_wrapper reference_reduction_numpy prod_numpy torch testing _internal opinfo torch testing _internal opinfo definitions linalg sample_inputs_linalg_cholesky sample_inputs_linalg_cholesky_inverse sample_inputs_cross sample_inputs_linalg_qr_geqrf sample_inputs_linalg_invertible sample_inputs_lu_solve sample_inputs_legacy_solve sample_inputs_svd sample_inputs_linalg_det_logdet_slogdet sample_inputs_linalg_lu sample_inputs_diagonal_diag_embed error_inputs_diagonal_diag_embed torch testing _internal opinfo definitions special sample_inputs_i _i sample_inputs_polygamma reference_polygamma torch testing _internal opinfo definitions _masked sample_inputs_softmax_variant torch testing _internal opinfo definitions sparse error_inputs_sparse_like_fns sample_inputs_sparse_like_fns error_inputs_sparse_mul sample_inputs_sparse_mul error_inputs_sparse_reduction_sum sample_inputs_sparse_reduction_sum TEST_SCIPY scipy stats scipy spatial scipy special test tensor close integer close_to_int x eps= x is_complex y = torch abs torch view_as_complex torch frac torch view_as_real x y = torch abs torch frac x y eps &#124; y - eps sample_inputs_slice op_info device dtype requires_grad kwargs make_input = partial make_tensor device=device dtype=dtype low=None high=None requires_grad=requires_grad yield SampleInput make_input yield SampleInput make_input dim= start= end=- yield SampleInput make_input dim= start= end=- step= yield SampleInput make_input dim= start=- end=- step= sample_inputs_tensor_split op_info device dtype requires_grad kwargs make_input = partial make_tensor device=device dtype=dtype low=None high=None requires_grad=requires_grad args_cases = Cases tensor indices torch tensor torch tensor torch tensor torch tensor Cases list indices - Cases integer section - args args_cases yield SampleInput make_input S S S args=args sample_inputs_hsplit op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device low=None high=None requires_grad=requires_grad yield SampleInput make_arg yield SampleInput make_arg S S S sample_inputs_vsplit op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device low=None high=None requires_grad=requires_grad yield SampleInput make_arg S yield SampleInput make_arg S S S sample_inputs_dsplit op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device low=None high=None requires_grad=requires_grad yield SampleInput make_arg S S S yield SampleInput make_arg S S error_inputs_hsplit op_info device kwargs make_arg = partial make_tensor dtype=torch float device=device err_msg = torch hsplit requires tensor least dimension got tensor dimensions yield ErrorInput SampleInput make_arg error_regex=err_msg err_msg = f torch hsplit attempted split along dimension f size dimension S f divisible split_size yield ErrorInput SampleInput make_arg S S S error_regex=err_msg Incorrect type indices_or_section argument err_msg = received invalid combination arguments yield ErrorInput SampleInput make_arg S S S abc error_type=TypeError error_regex=err_msg error_inputs_vsplit op_info device kwargs make_arg = partial make_tensor dtype=torch float device=device err_msg = torch vsplit requires tensor least dimension got tensor dimensions yield ErrorInput SampleInput make_arg S error_regex=err_msg err_msg = f torch vsplit attempted split along dimension f size dimension S f divisible split_size yield ErrorInput SampleInput make_arg S S S error_regex=err_msg Incorrect type indices_or_section argument err_msg = received invalid combination arguments yield ErrorInput SampleInput make_arg S S S abc error_type=TypeError error_regex=err_msg error_inputs_dsplit op_info device kwargs make_arg = partial make_tensor dtype=torch float device=device err_msg = torch dsplit requires tensor least dimension got tensor dimensions yield ErrorInput SampleInput make_arg S error_regex=err_msg err_msg = f torch dsplit attempted split along dimension f size dimension S f divisible split_size yield ErrorInput SampleInput make_arg S S S error_regex=err_msg sample_inputs_as_strided op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad input shape output shape output stride output storage offset test_cases = input_shape output_shape stride storage_offset test_cases input_t = make_arg input_shape kwargs = dict storage_offset=storage_offset yield SampleInput input_t args= output_shape stride kwargs=kwargs sample_inputs_as_strided_partial_views op_info device dtype requires_grad kwargs make_arg base = make_tensor device=device dtype=dtype base requires_grad_ requires_grad as_strided offset partial views yield SampleInput make_arg yield SampleInput make_arg storage_offset= yield SampleInput make_arg storage_offset= sample_inputs_as_strided_scatter op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad input shape output shape output stride output storage offset test_cases = Scatter larger dimensions Scatter larger dimensions strides inverted input_shape output_shape stride storage_offset test_cases input_t = make_arg input_shape input_src = make_arg output_shape yield SampleInput input_t input_src output_shape stride storage_offset=storage_offset error_inputs_as_strided_scatter op_info device kwargs make_arg = partial make_tensor device=device dtype=torch float requires_grad=False Create small tensor try scatter out bounds input_t = make_arg input_src = make_arg yield ErrorInput SampleInput input_t input_src storage_offset= error_regex= itemsize requiring storage size out bounds storage size sample_inputs_combinations op_info device dtype requires_grad kwargs inputs = rvals = products = product inputs rvals False True input_data r with_replacement products input_t = torch tensor input_data device=device dtype=dtype requires_grad=requires_grad yield SampleInput input_t r=r with_replacement=with_replacement sample_inputs_cartesian_prod op_info device dtype requires_grad kwargs make_arg = partial torch tensor device=device dtype=dtype requires_grad=requires_grad constructs -D tensors varying number elements = make_arg b = make_arg c = make_arg sample only tensor yield SampleInput sample tensors yield SampleInput b sample tensors yield SampleInput b c sample_inputs_cosine_similarity op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Ordered input_shape dict dim eps cases tuple tuple dict = type ignore assignment S S dim S dim - S dim eps dim S S M dim S S input_shape kwargs cases yield SampleInput make_arg input_shape args= make_arg input_shape kwargs=kwargs Test Broadcasting yield SampleInput make_arg args= make_arg kwargs= dim - yield SampleInput make_arg args= make_arg kwargs= dim - yield SampleInput make_arg args= make_arg kwargs= dim - sample_inputs_item op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=False cases = shape cases yield SampleInput make_arg shape error_inputs_item op device kwargs make_arg = partial make_tensor dtype=torch float device=device requires_grad=False cases = M S S S S M L shape cases yield ErrorInput SampleInput make_arg shape error_type=RuntimeError error_regex= elements cannot converted Scalar sample_inputs_batch_norm op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_arg_without_requires_grad = partial make_tensor device=device dtype=dtype requires_grad=False Ordered input shape kwargs training momentum eps cases tuple tuple int dict = type ignore assignment S S S training True momentum eps training False momentum - training True momentum training True training False training True momentum - eps training False momentum - eps input_shape kwargs cases args running mean running var weight bias should necessarily shape channels channels = input_shape len input_shape weight = make_arg channels channels None bias = make_arg channels channels None running_mean = make_arg_without_requires_grad channels low= running_var = make_arg_without_requires_grad channels low= yield SampleInput make_arg input_shape args= running_mean running_var weight bias kwargs=kwargs Checking permutations weights biases ` None ` is_training = True False False training is_training yield SampleInput make_arg input_shape args= running_mean running_var make_arg channels make_arg channels kwargs= training training Test case no optional kwargs running_mean running_var required evaluation mode training False training mode yield SampleInput make_arg args= None None None None kwargs= training True sample_inputs_softmax_backward_data op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad cases = S S S S M S - input_dtypes = dtype dtype == torch float device == cuda input_dtypes += torch float shape dim input_dtype product cases input_dtypes input = make_arg shape output = torch nn functional softmax input dim=dim dtype=input_dtype yield SampleInput make_arg shape output dim input_dtype sample_inputs_native_batch_norm op_info device dtype requires_grad kwargs samples = sample_inputs_batch_norm op_info device dtype requires_grad kwargs sample samples torch native_batch_norm does support numel tensors IndexError Dimension out range expected range - got sample input numel == continue args = sample args training = sample kwargs get training True momentum = sample kwargs get momentum eps = sample kwargs get eps e- yield SampleInput sample input args= args args args args training momentum eps sample_inputs__native_batch_norm_legit op_info device dtype requires_grad kwargs samples = sample_inputs_batch_norm op_info device dtype requires_grad kwargs sample samples torch native_batch_norm does support numel tensors IndexError Dimension out range expected range - got sample input numel == continue args = sample args training = sample kwargs get training True momentum = sample kwargs get momentum eps = sample kwargs get eps e- args None args None yield SampleInput sample input args= args args args args training momentum eps yield SampleInput sample input args= args args training momentum eps sample_inputs__batch_norm_with_update op_info device dtype requires_grad kwargs samples = sample_inputs_batch_norm op_info device dtype requires_grad kwargs sample samples torch native_batch_norm does support numel tensors IndexError Dimension out range expected range - got sample input numel == continue args = sample args momentum = sample kwargs get momentum eps = sample kwargs get eps e- any args i None i range continue yield SampleInput sample input args= args args args args momentum eps sample_inputs_nn_activation_relu op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad cases = S S S S M S shape cases yield SampleInput make_arg shape sample_inputs_prelu op_info device dtype requires_grad kwargs op_kwargs = op_info sample_kwargs device dtype None yield sample_inputs_elementwise_unary op_info device dtype requires_grad op_kwargs=op_kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad cases = S S S S M S shape cases weight - weight_tensor = torch tensor weight device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_arg shape args= weight_tensor channel_size = shape len shape = yield SampleInput make_arg shape args= make_arg channel_size weight_tensor = torch tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_arg S S kwargs=dict weight=weight_tensor yield SampleInput make_arg S S kwargs=dict weight=make_arg S reference_inputs_prelu op device dtype requires_grad kwargs yield sample_inputs_prelu op device dtype requires_grad kwargs yield reference_inputs_elementwise_unary op device dtype requires_grad kwargs sample_kwargs_prelu_scalar_weight device dtype input weight = torch rand device=device dtype=dtype NumPy does support bfloat so we default float only NumPy case dtype == torch bfloat weight_cpu = weight dtype=torch float device= cpu weight_cpu = weight cpu np_weight = weight_cpu numpy weight weight weight np_weight error_inputs_prelu op device Weight has numel = ndim zero-dim tensor inp = make_tensor device=device dtype=torch float weight = make_tensor device=device dtype=torch float yield ErrorInput SampleInput inp kwargs= weight weight error_regex= Not allow zero-dim input tensor Weight has numel = numel does match channel size inp = make_tensor device=device dtype=torch float weight = make_tensor device=device dtype=torch float yield ErrorInput SampleInput inp kwargs= weight weight error_regex= Mismatch parameter numbers input channel size Weight neither scalar nor -D tensor inp = make_tensor device=device dtype=torch float weight = make_tensor device=device dtype=torch float yield ErrorInput SampleInput inp kwargs= weight weight error_regex= prelu Expected ` weight ` scalar D tensor got ndim = src index tensors must have same dimensions sample_inputs_norm op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ord = inf tested inputs_norm_inf fails some tests cases = S S S S S S _ S S S S S S - neg_ S S - neg_ S S - neg_ _ S S - neg_ _ cases_nonzero_input = S S S _ _default S S S _ _dim S S S - _ _neg_dim S S S True keepdim_ _ _dim S S S - True keepdim_ _ _neg_dim cases_posdim = S S - neg_ _dim S S - neg_ _dim S S _dim S S _dim S S _dim S S _dim S S S _dim S S S _dim S S S True keepdim_ _dim S S S True keepdim_ _dim _dim_scalar _dim_scalar True keepdim_ _dim_scalar True keepdim_ _dim_scalar cases_negdim = shape args + -args + args name replace _dim _neg_dim shape args name cases_posdim shape args name itertools chain cases cases_posdim cases_negdim yield SampleInput make_arg shape args=args name=name shape args name cases_nonzero_input yield SampleInput make_arg shape exclude_zero=True args=args name=name sample_inputs_norm_fro op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad cases = S S default S S fro fro_default S S fro fro shape args name cases yield SampleInput make_arg shape args=args name=name sample_inputs_norm_nuc op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad cases = S S nuc nuc S S S nuc nuc_batched shape args name cases yield SampleInput make_arg shape args=args name=name sample_inputs_norm_inf op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad cases = S S -inf -inf S S inf inf S S inf inf_ _dim S S inf - inf_ _neg_dim shape args name cases yield SampleInput make_arg shape args=args name=name sample_inputs_equal op device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad shapes = S S S S M S S S S S shape_lhs shape_rhs shapes lhs = make_arg shape_lhs rhs = make_arg shape_rhs broadcasts_input = shape_lhs = torch broadcast_shapes shape_lhs shape_rhs yield SampleInput lhs args= rhs broadcasts_input=broadcasts_input shape_lhs == shape_rhs yield SampleInput lhs args= lhs clone detach_ sample_inputs_jiterator op device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad shapes = S S S M S S M S M S S M S S M S M S M S M S M S num_inputs = kwargs get num_inputs sample_kwargs = kwargs get sample_kwargs shape_lhs shape_rhs shapes lhs = make_arg shape_lhs args = make_arg shape_rhs _ range num_inputs - broadcasts_input = shape_lhs = torch broadcast_shapes shape_lhs shape_rhs yield SampleInput lhs args=tuple args kwargs=sample_kwargs broadcasts_input=broadcasts_input sample_inputs_broadcast_shapes op device dtype requires_grad kwargs shapes = S S S S S M S S M S M S S M S S M S M S M S M S M S shape shapes inp arg = shape yield SampleInput inp args=tuple arg sample_inputs_add_sub op device dtype requires_grad kwargs yield sample_inputs_elementwise_binary op device dtype requires_grad kwargs Adds alpha kwarg cases make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad lhs = make_arg S S op lhs_make_tensor_kwargs rhs = make_arg S S op rhs_make_tensor_kwargs dtype torch bool yield SampleInput lhs args= rhs kwargs= alpha yield SampleInput lhs args= rhs kwargs= alpha True neg_alpha = - dtype is_floating_point dtype is_complex - lhs = make_arg S S op lhs_make_tensor_kwargs rhs = make_arg S S op rhs_make_tensor_kwargs dtype torch bool yield SampleInput lhs args= rhs kwargs= alpha neg_alpha yield SampleInput lhs args= rhs kwargs= alpha False error_inputs_arange op device kwargs yield ErrorInput SampleInput args= error_type=RuntimeError error_regex= step must nonzero yield ErrorInput SampleInput args= - error_type=RuntimeError error_regex= upper bound lower bound inconsistent step sign yield ErrorInput SampleInput args= - error_type=RuntimeError error_regex= upper bound lower bound inconsistent step sign yield ErrorInput SampleInput args= error_type=RuntimeError error_regex= upper bound lower bound inconsistent step sign yield ErrorInput SampleInput args= float inf error_type=RuntimeError error_regex= unsupported range yield ErrorInput SampleInput float -inf args= error_type=RuntimeError error_regex= unsupported range sample_inputs_arange op device dtype requires_grad kwargs int_samples = positive direction - negative direction - - start == end - divides evenly - - bool False True True default step None default start None None to_float start end step start = start + start None None end = end + step = float step step None None start end step float_samples = includes endpoint - - e- - + e- - - to_float start end step start end step int_samples large_samples = None samples = int_samples + float_samples dtype torch int torch uint samples += large_samples start end step samples start None assert step None Pass end positional arg yield SampleInput end kwargs= dtype dtype device device Similar calling torch arange end= yield SampleInput kwargs= end end dtype dtype device device step None yield SampleInput start args= end kwargs= dtype dtype device device yield SampleInput start args= end step kwargs= dtype dtype device device yield SampleInput yield SampleInput args= sample_inputs_randn op device dtype requires_grad kwargs shapes = M S S shape shapes yield SampleInput input=shape kwargs=dict dtype=dtype device=device requires_grad=requires_grad sample_inputs_normal op device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=False samples = S S S S S - shape mean std samples yield SampleInput make_arg shape args= mean std error_inputs_normal op device kwargs t = torch zeros device=device invalid_std = - yield ErrorInput SampleInput t args= invalid_std error_type=RuntimeError error_regex=fr normal expects std = found std invalid_std sample_inputs_cauchy op device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=False samples = M S S S S S - shape median gamma samples yield SampleInput make_arg shape args= median gamma error_inputs_cauchy op device kwargs t = torch zeros device=device invalid_scale = yield ErrorInput SampleInput t args= invalid_scale error_type=RuntimeError error_regex=fr cauchy_ expects sigma found sigma= invalid_scale sample_inputs_exponential op device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=False samples = M S S S S S shape rate samples yield SampleInput make_arg shape args= rate error_inputs_exponential op device kwargs t = torch zeros device=device invalid_rate = yield ErrorInput SampleInput t args= invalid_rate error_type=RuntimeError error_regex=fr exponential_ expects lambda found lambda= invalid_rate sample_inputs_geometric op device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=False samples = M S S S S S shape rate samples yield SampleInput make_arg shape args= rate error_inputs_geometric op device kwargs t = torch zeros device=device neg_prob = - yield ErrorInput SampleInput t args= neg_prob error_type=RuntimeError error_regex=fr geometric_ expects p \ \ got p= neg_prob sample_inputs_log_normal op device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=False samples = M S S S S S shape mean std samples yield SampleInput make_arg shape args= mean std error_inputs_log_normal op device kwargs t = torch zeros device=device invalid_std = yield ErrorInput SampleInput t args= invalid_std error_type=RuntimeError error_regex=fr log_normal_ expects std found std= invalid_std sample_inputs_uniform op device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=False samples = M - S S S S S shape hi lo samples yield SampleInput make_arg shape args= hi lo sample_inputs_ones_zeros op device dtype requires_grad kwargs bit messy we want args tuples so we pass size tuple we have tuple containing tuple sizes = M S S size sizes yield SampleInput size kwargs= dtype dtype device device sample_inputs_full op device dtype requires_grad kwargs get_val dtype make_tensor dtype=dtype device= cpu item sizes = M S S fill_values = get_val dtype get_val torch int size fill_value product sizes fill_values yield SampleInput size fill_value dtype=dtype device=device error_inputs_uniform op device kwargs t = torch zeros device=device yield ErrorInput SampleInput t args= - error_type=RuntimeError error_regex=r uniform_ expects \ to\ range found from= to=- error_inputs_linspace op device kwargs yield ErrorInput SampleInput args= - error_type=RuntimeError error_regex= number steps must non-negative yield ErrorInput SampleInput args= error_type=TypeError error_regex= received invalid combination arguments - got \\ int int float yield ErrorInput SampleInput torch tensor device=device args= torch tensor device=device error_type=RuntimeError error_regex= only supports -dimensional start end tensors sample_inputs_linspace op device dtype requires_grad kwargs ends = - starts = - nsteps = Extra case replicate off-by-one issue CUDA cases = list product starts ends nsteps + start end nstep cases dtype == torch uint end start continue yield SampleInput start args= end nstep kwargs= dtype dtype device device yield SampleInput args= sample_inputs_linspace_tensor_overload op device dtype requires_grad kwargs ends = - starts = - nsteps = is_start_end_tensors = True True True False False True make_arg = partial torch tensor device=device requires_grad=False Extra case replicate off-by-one issue CUDA cases = list product starts ends nsteps is_start_end_tensors + True True start end nstep is_start_tensor is_end_tensor cases dtype == torch uint end start continue tensor_options = dtype dtype device device is_start_tensor start = make_arg start dtype=torch float isinstance start float torch int is_end_tensor end = make_arg end dtype=torch float isinstance end float torch int yield SampleInput start args= end nstep kwargs=tensor_options yield SampleInput args= sample_inputs_logspace op device dtype requires_grad kwargs ends = - starts = - nsteps = bases = dtype torch int torch uint None start end nstep base product starts ends nsteps bases dtype == torch uint end start continue nstep == isinstance start float dtype is_complex dtype is_floating_point https github com pytorch pytorch issues continue base None yield SampleInput start args= end nstep kwargs= dtype dtype device device yield SampleInput start args= end nstep base kwargs= dtype dtype device device yield SampleInput args= sample_inputs_logspace_tensor_overload op device dtype requires_grad kwargs ends = - starts = - nsteps = bases = dtype torch int torch uint None is_start_end_tensors = True True True False False True make_arg = partial torch tensor device=device start end nstep base is_start_tensor is_end_tensor product starts ends nsteps bases is_start_end_tensors dtype == torch uint end start continue nstep == isinstance start float dtype is_complex dtype is_floating_point https github com pytorch pytorch issues continue tensor_options = dtype dtype device device is_start_tensor start = make_arg start dtype=torch float isinstance start float torch int is_end_tensor end = make_arg end dtype=torch float isinstance end float torch int base None yield SampleInput start args= end nstep kwargs=tensor_options yield SampleInput start args= end nstep base kwargs=tensor_options yield SampleInput args= sample_inputs_isclose op device dtype requires_grad kwargs yield sample_inputs_elementwise_binary op device dtype requires_grad kwargs Creates additional inputs test rtol atol equal_nan params rtols = e- atols = e- equal_nans = False True products = product rtols atols equal_nans make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad rtol atol equal_nan products lhs = make_arg S S op lhs_make_tensor_kwargs rhs = make_arg S S op rhs_make_tensor_kwargs yield SampleInput lhs args= rhs kwargs=dict rtol=rtol atol=atol equal_nan=equal_nan error_inputs_isclose op device kwargs make_float_arg = partial make_tensor device=device dtype=torch float requires_grad=False yield ErrorInput SampleInput make_float_arg args= make_float_arg kwargs= rtol - error_type=RuntimeError error_regex= rtol must greater than equal zero yield ErrorInput SampleInput make_float_arg args= make_float_arg kwargs= atol - error_type=RuntimeError error_regex= atol must greater than equal zero sample_inputs_t op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_arg yield SampleInput make_arg yield SampleInput make_arg sample_inputs_mm op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_arg_conj size make_arg size conj requires_grad_ requires_grad first_shape second_shape = S M M S yield SampleInput make_arg first_shape args= make_arg second_shape dtype is_complex yield SampleInput make_arg first_shape args= make_arg_conj second_shape Matmul empty matrices yield SampleInput make_arg S args= make_arg S M yield SampleInput make_arg S args= make_arg M sample_inputs_addmm op_info device dtype requires_grad kwargs alpha_val = kwargs get alpha + j dtype is_complex dtype is_floating_point beta_val = kwargs get beta + j dtype is_complex dtype is_floating_point tests_list = False False tests_with_lhs_broadcasting = True True test_cases = tests_list + tests_with_lhs_broadcasting type ignore operator kwargs = dict alpha=alpha_val beta=beta_val make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad shape_a shape_b shape_c broadcasts_input test_cases yield SampleInput make_arg shape_a make_arg shape_b make_arg shape_c kwargs with_metadata broadcasts_input=broadcasts_input dtype is_complex shape = yield SampleInput make_arg shape make_arg shape requires_grad=False mH requires_grad_ requires_grad make_arg shape kwargs yield SampleInput make_arg shape make_arg shape make_arg shape requires_grad=False mH requires_grad_ requires_grad kwargs addmm empty matrices dtype is_floating_point yield SampleInput make_arg S M make_arg S make_arg M kwargs empty matmul broadcastable input yield SampleInput make_arg M make_arg S make_arg M kwargs with_metadata broadcasts_input=True sample_inputs_sparse_sampled_addmm op_info device dtype requires_grad kwargs alpha = + j dtype is_complex beta = + j dtype is_complex make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad sparse sampled_addmm performs alpha A B sparse_ones_like C + beta C m n k itertools product repeat= yield SampleInput torch eye m n device=device dtype=dtype to_sparse_csr requires_grad_ requires_grad make_arg m k make_arg k n alpha=alpha beta=beta sample_inputs_sparse_mm_reduce op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad reductions = sum mean amax amin m k reduce product reductions yield SampleInput torch eye m m device=device dtype=dtype to_sparse_csr requires_grad_ requires_grad make_arg m k reduce sample_inputs_mv device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device low=None high=None requires_grad=requires_grad yield SampleInput make_arg S M make_arg M sample_inputs_bmm device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device low=None high=None requires_grad=requires_grad yield SampleInput make_arg M S M make_arg M M S sample_inputs_dot_vdot device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_arg_conj size make_arg size conj requires_grad_ requires_grad yield SampleInput make_arg S make_arg S dtype is_complex dot vdot conj input conj arg_tensor conj input arg_tensor tested test_conj_view which tests operations only conjugated input tensor -- conjugated arg tensors yield SampleInput make_arg S make_arg_conj S error_inputs_dot_vdot op_info device is_ref=False kwargs make_input = partial make_tensor device=device dtype=torch float yield ErrorInput SampleInput make_input args= make_input dtype=torch float error_regex= dot expected both vectors have same dtype yield ErrorInput SampleInput make_input args= make_input error_regex= D tensors expected yield ErrorInput SampleInput make_input args= make_input error_regex= inconsistent tensor size device = cpu is_ref yield ErrorInput SampleInput make_input args= make_input device= cpu error_regex= Expected all tensors same device sample_inputs_addmv op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad test_cases = S S M M False S S M M False test_cases_with_broadcast = S M M True S M M True S M M True S M M True cases = test_cases + test_cases_with_broadcast addmv performs beta M + alpha mat vec size mat vec beta alpha broadcasts_input cases yield SampleInput make_arg size args= make_arg mat make_arg vec kwargs=dict beta=beta alpha=alpha broadcasts_input=broadcasts_input sample_inputs_addbmm op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad input_shape batch _shape batch _shape beta_val alpha_val is_broadcasting test_cases = S M S S S S S M False S S S S S M True S M S S S S S M False S S S S S M True S S S S S M True S S S S S M True input_shape batch _shape batch _shape beta alpha is_broadcasting test_cases dtype is_complex beta_complex alpha_complex = beta + j alpha + j yield SampleInput make_arg input_shape args= make_arg batch _shape make_arg batch _shape kwargs=dict beta=beta_complex alpha=alpha_complex broadcasts_input=is_broadcasting yield SampleInput make_arg input_shape args= make_arg batch _shape make_arg batch _shape kwargs=dict beta=beta alpha=alpha broadcasts_input=is_broadcasting sample_inputs_addcmul_addcdiv op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad test_cases = S S S S S S False S S S S False S S S True False S S True S S S True input_args broadcasts_input test_cases addcdiv should accept inputs zero value Currently throws ZeroDivisionError when denominator zero TODO exclude_zeros can removed after https github com pytorch pytorch issues fixed args = tuple make_arg arg exclude_zero=True isinstance arg tuple arg arg input_args yield SampleInput args with_metadata broadcasts_input=broadcasts_input addcdiv should accept inputs zero value Currently throws ZeroDivisionError when denominator zero TODO exclude_zeros can removed after https github com pytorch pytorch issues fixed args = tuple make_arg arg exclude_zero=True isinstance arg tuple arg arg input_args yield SampleInput args value= dtype is_floating_point dtype is_complex with_metadata broadcasts_input=broadcasts_input reference_inputs_addcmul_addcdiv op_info device dtype requires_grad kwargs yield sample_inputs_addcmul_addcdiv op_info device dtype requires_grad kwargs type promotion cases supported_dtypes = op_info supported_dtypes device make_arg = partial make_tensor device=device requires_grad=requires_grad types = torch float torch complex torch bfloat torch float values = None True False - - + j type type value product types values type supported_dtypes type supported_dtypes continue RuntimeError value cannot converted without overflow type value complex type torch complex continue arg = make_arg dtype=dtype arg = make_arg dtype=type arg = make_arg dtype=type TypeError addcdiv argument value must Number NoneType value None yield SampleInput arg args= arg arg kwargs=dict value=value yield SampleInput arg args= arg arg sample_inputs_baddbmm op_info device dtype requires_grad kwargs test_cases = S S M S S S S S M False S S S S S M True S S M S S S S S M False S S S S S M True S S S S S M True S S S S S M True make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad low=None high=None input_shape batch _shape batch _shape alpha beta broadcasts_input test_cases yield SampleInput make_arg input_shape make_arg batch _shape make_arg batch _shape beta=beta alpha=alpha with_metadata broadcasts_input=broadcasts_input dtype is_complex yield SampleInput make_arg input_shape make_arg batch _shape make_arg batch _shape beta=beta + j alpha=alpha + j with_metadata broadcasts_input=broadcasts_input dtype is_complex shapes = S S S S M S S S M args = tuple make_arg s s shapes yield SampleInput args transpose_ - args transpose - conj requires_grad_ requires_grad args transpose - conj requires_grad_ requires_grad beta=beta + j alpha=alpha + j TODO add reduction kwargs sample_inputs_multilabel_soft_margin_loss op_info device dtype requires_grad kwargs _make_tensor = partial make_tensor device=device dtype=dtype requires_grad=requires_grad shapes = S S S shape shapes Produce one weight one without yield SampleInput _make_tensor shape args= _make_tensor shape requires_grad=False kwargs= yield SampleInput _make_tensor shape args= _make_tensor shape requires_grad=False kwargs= weight _make_tensor shape requires_grad=False sample_inputs_addr op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad low=None high=None yield SampleInput make_arg S M make_arg S make_arg M yield SampleInput make_arg make_arg S make_arg M with_metadata broadcasts_input=True dtype is_complex alpha beta = + j + j dtype is_floating_point alpha beta = alpha beta = yield SampleInput make_arg S M make_arg S make_arg M beta=beta alpha=alpha yield SampleInput make_arg make_arg S make_arg M beta=beta alpha=alpha with_metadata broadcasts_input=True These samples fail gradcheck dtype is_floating_point requires_grad tensor_options = dict device=device dtype=dtype requires_grad=requires_grad yield SampleInput torch tensor math nan tensor_options torch tensor tensor_options torch tensor tensor_options beta= alpha= with_metadata broadcasts_input=True yield SampleInput torch tensor tensor_options torch tensor math nan tensor_options torch tensor math nan tensor_options beta= alpha= with_metadata broadcasts_input=True sample_inputs_zero_ op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad cases = S S S S shape cases yield SampleInput make_arg shape sample_inputs_multi_margin_loss op_info device dtype requires_grad kwargs _make_tensor = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_target = partial _make_tensor dtype=torch long requires_grad=False make_weight = partial _make_tensor requires_grad=False inputs = make_target low= high= S make_target low= high=S p S make_target low= high=S p S M make_target S low= high=M margin S M make_target S low= high=M margin - M S make_target M low= high=S weight None M S make_target M low= high=S weight make_weight S low=- high= M S make_target M low= high=S reduction none M S make_target M low= high=S reduction mean M S make_target M low= high=S reduction sum input_shape target kwargs inputs yield SampleInput _make_tensor input_shape args= target kwargs=kwargs reference_inputs_multi_margin_loss op_info device dtype requires_grad kwargs yield sample_inputs_multi_margin_loss op_info device dtype requires_grad kwargs _make_tensor = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_target = partial _make_tensor dtype=torch long requires_grad=False make_weight = partial _make_tensor requires_grad=False inputs = make_target low= high= S make_target low= high=S S make_target low= high=S M S make_target M low= high=S ps = margins = - weights = False True reductions = None none mean sum input_shape target p margin weight reduction product inputs ps margins weights reductions input = _make_tensor input_shape weight_shape = input size - input ndim weight = make_weight weight_shape low=- high= weight None kwargs = p p margin margin weight weight reduction None kwargs reduction = reduction yield SampleInput input args= target kwargs=kwargs error_inputs_multi_margin_loss op device kwargs make_input = partial make_tensor device=device dtype=torch float invalid reduction yield ErrorInput SampleInput make_input args= make_input kwargs= reduction abc error_type=ValueError error_regex= abc valid value reduction invalid input yield ErrorInput SampleInput make_input args= make_input kwargs= error_type=RuntimeError error_regex=r Expected non-empty vector matrix optional -dim batch size got \ \ yield ErrorInput SampleInput make_input args= make_input kwargs= error_type=RuntimeError error_regex=r Expected non-empty vector matrix optional -dim batch size got \ \ invalid target yield ErrorInput SampleInput make_input args= make_input kwargs= error_type=RuntimeError error_regex=r inconsistent target size expected got \ \ invalid target dtype yield ErrorInput SampleInput make_input args= make_input kwargs= error_type=RuntimeError error_regex= expected scalar type Long found Float invalid weight yield ErrorInput SampleInput make_input args= make_input kwargs= weight make_input error_type=ValueError error_regex= weight must one-dimensional yield ErrorInput SampleInput make_input args= make_input kwargs= weight make_input error_type=ValueError error_regex= weight must one-dimensional yield ErrorInput SampleInput make_input args= make_input kwargs= weight make_input error_type=RuntimeError error_regex=r inconsistent weight size expected got \ \ invalid p yield ErrorInput SampleInput make_input args= make_input kwargs= p error_type=ValueError error_regex= only p == p == supported sample_inputs_logsumexp device dtype requires_grad kwargs inputs = True S S True S S False S S - False S S False Test large inputs check numerical stability lows = None e e dtype torch float torch float torch complex torch complex None low lows high = low low None None shape dim keepdim inputs t = make_tensor shape dtype=dtype device=device low=low high=high requires_grad=requires_grad yield SampleInput t dim keepdim reference_inputs_logsumexp op device dtype requires_grad kwargs yield sample_inputs_logsumexp op device dtype requires_grad kwargs https github com pytorch pytorch issues t = torch tensor dtype=dtype device=device requires_grad=requires_grad yield SampleInput t False t = torch tensor dtype=dtype device=device requires_grad=requires_grad yield SampleInput t False tests masking https github com pytorch pytorch pull #pullrequestreview- t = torch tensor float inf yield SampleInput t True sample_inputs_like_fns device dtype requires_grad kwargs inputs = S S S S dtype dtype device device Hard-code some dtypes devices We want test cases where dtype device different input s dtype device S dtype torch double device = mps torch float S device cpu S dtype torch double device cpu torch cuda is_available inputs append S device cuda shape kwargs inputs t = make_tensor shape dtype=dtype device=device low=None high=None requires_grad=requires_grad yield SampleInput t kwargs reference_inputs_like_fns op device dtype requires_grad kwargs yield sample_inputs_like_fns op device dtype requires_grad kwargs shape cases = make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad shape cases yield SampleInput make_arg shape yield SampleInput make_arg shape transpose - yield SampleInput make_arg shape noncontiguous=True yield SampleInput make_arg shape noncontiguous=True transpose - sample_inputs_multilabel_margin_loss op_info device dtype requires_grad kwargs _make_tensor = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_target = partial _make_tensor dtype=torch long requires_grad=False inputs = make_target low= high= S make_target S low= high=S M S make_target M S low= high=S M S make_target M S low= high=S reduction none M S make_target M S low= high=S reduction mean M S make_target M S low= high=S reduction sum shape target kwargs inputs yield SampleInput _make_tensor shape args= target kwargs=kwargs reference_inputs_multilabel_margin_loss op_info device dtype requires_grad kwargs yield sample_inputs_multilabel_margin_loss op_info device dtype requires_grad kwargs _make_tensor = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_target = partial _make_tensor dtype=torch long requires_grad=False make_target_tensor = partial torch tensor device=device dtype=torch long requires_grad=False inputs = random tests including - target labels make_target low=- high= S make_target S low=- high=S M S make_target M S low=- high=S repeated target labels - labels after first - ignored make_target_tensor - make_target_tensor - - make_target_tensor - - - - reductions = None none mean sum shape target reduction product inputs reductions kwargs = reduction None kwargs reduction = reduction yield SampleInput _make_tensor shape args= target kwargs=kwargs error_inputs_multilabel_margin_loss op device kwargs make_input = partial make_tensor device=device dtype=torch float invalid reduction yield ErrorInput SampleInput make_input args= make_input kwargs= reduction abc error_type=ValueError error_regex= abc valid value reduction invalid input yield ErrorInput SampleInput make_input args= make_input kwargs= error_type=RuntimeError error_regex=r Expected non-empty vector matrix optional -dim batch size got \ \ yield ErrorInput SampleInput make_input args= make_input kwargs= error_type=RuntimeError error_regex=r Expected non-empty vector matrix optional -dim batch size got \ \ invalid target yield ErrorInput SampleInput make_input args= make_input kwargs= error_type=RuntimeError error_regex=r inconsistent target size \ \ input size \ \ yield ErrorInput SampleInput make_input args= make_input kwargs= error_type=RuntimeError error_regex=r inconsistent target size \ \ input size \ \ get_independent_tensor tensor tensor clone requires_grad_ tensor requires_grad sample_inputs_randint device dtype requires_grad kwargs low = high = sample sample_inputs_like_fns device dtype requires_grad kwargs sample kwargs setdefault device device With high yield SampleInput high sample input shape sample args sample kwargs With low high yield SampleInput low high sample input shape sample args sample kwargs sample_inputs_randint_like device dtype requires_grad kwargs low = high = sample sample_inputs_like_fns device dtype requires_grad kwargs With high yield SampleInput sample input high sample args sample kwargs With low high yield SampleInput get_independent_tensor sample input low high sample args sample kwargs sample_inputs_margin_ranking_loss op_info device dtype requires_grad kwargs _make_tensor = partial make_tensor device=device dtype=dtype requires_grad=requires_grad shapes = S S S S S S margins = reductions = sum mean none shape shapes margin reduction product margins reductions kwargs = margin margin reduction reduction yield SampleInput _make_tensor shape args= _make_tensor shape requires_grad=False _make_tensor shape requires_grad=False kwargs=kwargs reference_inputs_margin_ranking_loss op device dtype requires_grad kwargs yield sample_inputs_margin_ranking_loss op device dtype requires_grad kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad reduction sum mean none dtype is_floating_point only supports ints floats NaN propagation inp = make_input inp = float nan inp = make_input inp = float nan target = make_input inp = float nan yield SampleInput inp args= inp target kwargs= reduction reduction Inf handling inp = make_input inp = float inf inp = make_input inp = float inf target = make_input inp = float inf yield SampleInput inp args= inp target kwargs= reduction reduction Broadcasting inp = make_input inp = make_input target = make_input yield SampleInput inp args= inp target kwargs= reduction reduction error_inputs_margin_ranking_loss op device kwargs make_input = partial make_tensor device=device dtype=torch float invalid reduction value yield ErrorInput SampleInput make_input args= make_input make_input kwargs= reduction abc error_type=ValueError error_regex= valid value invalid input shapes yield ErrorInput SampleInput make_input args= make_input make_input error_regex= margin_ranking_loss All input tensors should sample_inputs_new_fns device dtype requires_grad is_strided=False kwargs input_shape output_shape strides kwargs lengths output_shape strides must equal inputs = S S S S dtype dtype device device Hard-code some dtypes devices We want test cases where dtype device different input s dtype device S S dtype torch double device = mps torch float S S L M device cpu S L M S dtype torch double device cpu torch cuda is_available inputs append S device cuda input_shape output_shape strides kwargs inputs t = make_tensor input_shape dtype=dtype device=device low=None high=None requires_grad=requires_grad is_strided yield SampleInput t output_shape strides kwargs yield SampleInput t output_shape kwargs sample_inputs_empty_strided op device dtype requires_grad=False kwargs inputs = dtype dtype device device S dtype dtype device device S S dtype dtype device device S S S dtype dtype device device shape strides kwargs inputs yield SampleInput shape strides requires_grad=requires_grad kwargs sample_inputs_empty op device dtype requires_grad kwargs shape cases = case cases yield SampleInput case device=device dtype=dtype requires_grad=requires_grad sample_inputs_empty_permuted op device dtype requires_grad kwargs shape cases = case cases layout itertools permutations range len case yield SampleInput case layout device=device dtype=dtype requires_grad=requires_grad error_inputs_empty_permuted op_info device kwargs yield ErrorInput SampleInput args= error_type=RuntimeError error_regex= Number dimensions size does match length physical_layout yield ErrorInput SampleInput args= error_type=RuntimeError error_regex= Dimension out range yield ErrorInput SampleInput args= error_type=RuntimeError error_regex= Duplicate dim allowed sample_inputs_scalar_tensor op device dtype requires_grad kwargs Not including scalar tensor vals because meta tests start failing due lack meta support _local_scalar_dense torch tensor device=device vals = - item vals yield SampleInput item device=device dtype=dtype requires_grad=requires_grad sample_inputs_eye op device dtype requires_grad kwargs only ints = allowed both arguments unless m omitted sizes = None L M S n m product sizes sizes n None continue TODO no layout _kwargs = device device dtype dtype requires_grad requires_grad m None yield SampleInput n args= kwargs=_kwargs yield SampleInput n args= m kwargs=_kwargs error_inputs_eye op_info device kwargs TODO no layout _kwargs = device device dtype torch float yield ErrorInput SampleInput - args= kwargs=_kwargs error_regex= n must greater equal got - yield ErrorInput SampleInput - args= kwargs=_kwargs error_regex= n must greater equal got - yield ErrorInput SampleInput args= - kwargs=_kwargs error_regex= m must greater equal got - sample_inputs_new_full device dtype requires_grad kwargs get_val dtype make_tensor dtype=dtype device= cpu item sample sample_inputs_new_fns device dtype requires_grad kwargs The scalar we passing new_full must same dtype one resulting tensor use_dtype = sample kwargs get dtype dtype yield SampleInput sample input sample args get_val use_dtype sample kwargs sample_inputs_full_like device dtype requires_grad kwargs get_val dtype make_tensor dtype=dtype device= cpu item double_dtype = torch double device = mps torch float inputs = get_val dtype S S get_val dtype S get_val dtype S get_val dtype dtype dtype device device Hard-code some dtypes devices We want test cases where dtype device different input s dtype device S get_val double_dtype dtype double_dtype S get_val dtype device cpu S get_val double_dtype dtype double_dtype device cpu torch cuda is_available inputs append S get_val dtype device cuda torch mps is_available dtype torch float torch complex torch uint torch uint inputs append S get_val dtype device mps dtype is_signed For unsigned dtypes negative values converted inputs append S -get_val dtype shape fill_value kwargs inputs t = make_tensor shape dtype=dtype device=device low=None high=None requires_grad=requires_grad yield SampleInput t fill_value kwargs sample_inputs_multinomial device dtype requires_grad kwargs cases = dict replacement=False dict replacement=True dict replacement=True dict replacement=False shape num_samples kwargs cases t = make_tensor shape dtype=dtype device=device low= high=None requires_grad=requires_grad yield SampleInput t num_samples kwargs sample_inputs_normal_common device dtype requires_grad cases kwargs get_value_or_make_tensor value_or_shape isinstance value_or_shape list make_tensor value_or_shape dtype=dtype device=device low= high=None requires_grad=requires_grad value_or_shape value_or_mean_shape value_or_std_shape kwargs cases mean = get_value_or_make_tensor value_or_mean_shape std = get_value_or_make_tensor value_or_std_shape yield SampleInput mean std kwargs sample_inputs_normal_tensor_first device dtype requires_grad kwargs value_or_size value_or_size kwargs cases = broadcasting sample_inputs_normal_common device dtype requires_grad cases kwargs sample_inputs_normal_tensor_second device dtype requires_grad kwargs yield SampleInput dtype=dtype device=device yield SampleInput dtype=dtype layout=torch strided device=device yield SampleInput make_tensor dtype=dtype device=device low= high=None requires_grad=requires_grad sample_inputs_bernoulli device dtype requires_grad kwargs shapes = shape shapes t = make_tensor shape dtype=dtype device=device low= high= requires_grad=requires_grad yield SampleInput t error_inputs_bernoulli op_info device kwargs more than one element written-to tensor refers single memory location x = torch rand device=device expand err_msg = unsupported operation yield ErrorInput SampleInput torch rand_like x kwargs= out x error_regex=err_msg sample_inputs_logcumsumexp device dtype requires_grad kwargs inputs = S S S S S S large_number True False shape dim inputs t = make_tensor shape dtype=dtype device=device low=None high=None requires_grad=requires_grad large_number t dim t = yield SampleInput t dim sample_inputs_trace device dtype requires_grad kwargs yield SampleInput make_tensor S S dtype=dtype device=device low=None high=None requires_grad=requires_grad error_inputs_trace op device yield ErrorInput SampleInput make_tensor dtype=torch float device=device error_regex= expected matrix sample_inputs_renorm device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad cases = S S S S S S - S S S S S S float inf shape args cases yield SampleInput make_arg shape args=args sample_inputs_transpose_swapdims device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad cases = - - - - M M S S S shape args cases yield SampleInput make_arg shape args=args _numpy_ref_transpose dim dim ndim = np swapaxes dim dim sample_inputs_adjoint device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad shapes = M M S S S S M S M S M S SampleInput make_arg shape shape shapes sample_inputs_T device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad shapes = M M M L SampleInput make_arg shape shape shapes error_inputs_T device has_ndims_error=False make_arg = partial make_tensor device=device dtype=torch float Deprecated behavior regular PyTorch throws error primTorch https github com pytorch pytorch issues has_ndims_error ndims == yield ErrorInput SampleInput make_arg M error_regex= r The use ` x\ T ` tensors dimension other than r reverse their shape supported\ ndims yield ErrorInput SampleInput make_arg M S L error_regex= r The use ` x\ T ` tensors dimension other than r reverse their shape supported\ sample_inputs_singular_matrix_factors op_info device dtype requires_grad=False This function produces two tensors shape m k n k k = min m n Their matrix product could used generate tensor shape m n rank k make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad batches = size = batch m n product batches size size k = = make_arg batch m k b = make_arg batch n k yield b sample_inputs_svd_lowrank op_info device dtype requires_grad=False kwargs Function s well defined outputs complex inputs fn usv U S V = usv U V mH S b sample_inputs_singular_matrix_factors op_info device dtype requires_grad batch m k = shape n = b shape - NOTE since svd_lowrank relies non rank-revealing SVD inherits problem unstable behavior repeated singular values including zeros Since we want avoid repeated zeros singular values we can only use k q This issues could resolved using rank-revealing SVD which does include zero singular values yield SampleInput b q=k M=None with_metadata output_process_fn_grad=fn b sample_inputs_singular_matrix_factors op_info device dtype requires_grad batch m k = shape n = b shape - M = make_tensor batch m n dtype=dtype device=device requires_grad=requires_grad yield SampleInput b q=k M=M with_metadata output_process_fn_grad=fn chunk_iter iterable size = iter iterable while True chunk = tuple islice size chunk break yield chunk sample_inputs_pca_lowrank op_info device dtype requires_grad=False kwargs we reuse samples svd_lowrank which come group two kwarg M = None kwarg M = some tensor samples = sample_inputs_svd_lowrank op_info device dtype requires_grad kwargs s s chunk_iter samples del s kwargs M del s kwargs M s kwargs center = False s kwargs center = True yield s yield s np_sinc_with_fp _as_fp x Wraps numpy s sinc function so fp values promoted fp before sinc invoked Context numpy s sinc returns NaN when evaluated fp x dtype == np float np sinc x astype np float np sinc x sample_inputs_broadcast_to op_info device dtype requires_grad kwargs test_cases = S S S S S S S S S S S S S S S S S S SampleInput make_tensor size dtype=dtype device=device low=None high=None requires_grad=requires_grad shape size shape test_cases sample_inputs_broadcast_tensors op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad test_cases tuple tuple = shape other_shapes test_cases yield SampleInput make_arg shape args=tuple make_arg s s other_shapes reference_inputs_broadcast_tensors op device dtype requires_grad kwargs yield sample_inputs_broadcast_tensors op device dtype requires_grad kwargs m = partial make_tensor dtype=dtype device=device requires_grad=requires_grad n = partial make_tensor dtype=dtype device=device requires_grad=requires_grad noncontiguous=True cases = b c d cases yield SampleInput m args= m b m c m d yield SampleInput n args= n b n c n d sample_inputs_block_diag op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad test_cases tuple tuple = S S S S S S S S shape other_shapes test_cases yield SampleInput make_arg shape args=tuple make_arg s s other_shapes We also want test mixed complex-non-complex inputs block_diag dtype == torch complex dtype == torch complex non_complex_dtype = torch float dtype == torch complex torch float make_arg_non_complex = partial make_tensor dtype=non_complex_dtype device=device requires_grad=requires_grad yield SampleInput make_arg_non_complex shape args=tuple make_arg s s other_shapes sample_inputs_cdist op_info device dtype requires_grad kwargs small_S = test_cases = S S S S + S S S S S S S S S S S Using S here would make one test take s small_S small_S small_S + small_S small_S small_S + small_S small_S small_S small_S small_S small_S small_S small_S make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad cm use_mm_for_euclid_dist donot_use_mm_for_euclid_dist FIXME add override JIT revert back since s accepted eager p float inf t _size t _size test_cases The args should never non-contiguous supported backward yield SampleInput make_arg t _size make_arg t _size p cm _fill_np value = copy fill value _fill_sample_kwargs device dtype input dtype torch bool value = True value = value value value value sample_inputs_comparison_ops op device dtype requires_grad kwargs yield sample_inputs_elementwise_binary op device dtype requires_grad kwargs Adds sample input where both tensors have same values make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad lhs = make_arg S S yield SampleInput lhs args= lhs clone sample_inputs_stack op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad shape x number tensors cases = shape num_tensors cases tensors = make_arg shape _ range num_tensors dim range - len shape - yield SampleInput tensors args= dim sample_inputs_chunk_cat op_info device dtype requires_grad kwargs If input tensors have different ndims dim should non-negative less than ndims every input tensors If all input tensors have same ndims we support both negative non-negative dim For wrapped_dim all tensors should have same size wrapped_dim- dimensions No requirements wrapped_dim -th dimension Expect positive num_chunks Expect non-empty input tensor list each input tensor should have least element Non-contiguous input tensors allowed make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad same_ndim_cases = torch Size torch Size - torch Size torch Size - torch Size torch Size torch Size torch Size torch Size sizes dim num_chunks same_ndim_cases tensors = make_arg size size sizes yield SampleInput tensors args= dim num_chunks different_ndim_case = torch Size torch Size torch Size torch Size torch Size max_dim num_chunks = dim range max_dim tensors = size different_ndim_case tensors append make_arg size yield SampleInput tensors args= dim num_chunks non-contiguous dim range max_dim tensors = size different_ndim_case make last dims column-major i e non-contiguous t = make_arg size transpose - - contiguous transpose - - tensors append t yield SampleInput tensors args= dim num_chunks error_inputs_chunk_cat op_info device kwargs make_arg = partial make_tensor device=device dtype=torch float input tensors have different ndims dim negative sizes dim num_chunks = torch Size torch Size - tensors = make_arg size size sizes yield ErrorInput SampleInput tensors args= dim num_chunks error_regex= _chunk_cat expects non-negative dim when input tensors have different ndims input tensors have different ndims dim = ndim some input tensors sizes dim num_chunks = torch Size torch Size tensors = make_arg size size sizes yield ErrorInput SampleInput tensors args= dim num_chunks error_regex= _chunk_cat expects dim ndim all input tensors some tensors have different sizes dim- dimensions sizes dim num_chunks = torch Size torch Size tensors = make_arg size size sizes yield ErrorInput SampleInput tensors args= dim num_chunks error_regex= _chunk_cat expects same sizes dim- dimensions all tensors negative num_chunks sizes dim num_chunks = torch Size torch Size - tensors = make_arg size size sizes yield ErrorInput SampleInput tensors args= dim num_chunks error_regex= _chunk_cat expects positive num_chunks zero num_chunks sizes dim num_chunks = torch Size torch Size tensors = make_arg size size sizes yield ErrorInput SampleInput tensors args= dim num_chunks error_regex= _chunk_cat expects positive num_chunks empty input tensor list dim num_chunks = yield ErrorInput SampleInput args= dim num_chunks error_regex= _chunk_cat expects non-empty input tensor list empty input tensor elements sizes dim num_chunks = torch Size torch Size tensors = make_arg size size sizes yield ErrorInput SampleInput tensors args= dim num_chunks error_regex= _chunk_cat expects non-empty tensor sample_inputs_cat_concat op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad cases tuple tuple tuple dict = type ignore assignment S S S S dim - S S S S dim M S S S dim different shapes dim - dim empty tensor S S dim empty tensor unempty dim= special case legacy_cat_wrap_dim S S S dim dim passed fallback default input_shape input_shape kwargs cases yield SampleInput make_arg input_shape make_arg input_shape kwargs=kwargs coat_lite_mini yield SampleInput make_arg memory_format=torch channels_last args= error_inputs_cat op_info device kwargs make_arg = partial make_tensor device=device dtype=torch float error inputs more than one element written-to tensor refer single memory location yield ErrorInput SampleInput make_arg S S make_arg S S kwargs= out make_arg S expand S S error_regex= unsupported operation error inputs empty tensors yield ErrorInput SampleInput kwargs= dim error_regex= non-empty list Tensors error_type=ValueError error inputs different sizes yield ErrorInput SampleInput make_arg S S L L make_arg S L - L kwargs= dim error_regex= Sizes tensors must match except dimension yield ErrorInput SampleInput make_arg S L - L make_arg S S L L kwargs= dim error_regex= Sizes tensors must match except dimension error inputs different dimensions yield ErrorInput SampleInput make_arg S - make_arg S L - L kwargs= dim error_regex= Tensors must have same number dimensions yield ErrorInput SampleInput make_arg S L - L make_arg S - kwargs= dim error_regex= Tensors must have same number dimensions error inputs same memory locations x = torch zeros device=device y = torch randn device=device err_msg = written-to tensor refer single memory location yield ErrorInput SampleInput x y kwargs= dim out x error_regex=err_msg yield ErrorInput SampleInput x y kwargs= dim out y error_regex=err_msg z = torch zeros device=device yield ErrorInput SampleInput y z kwargs= out z error_regex=err_msg error inputs different devices torch device device type == cuda x_cuda = make_tensor device=device dtype=torch float y_cpu = make_tensor device= cpu dtype=torch float yield ErrorInput SampleInput x_cuda y_cpu error_regex= Expected all tensors same device error inputs different input sizes more than tensors yield ErrorInput SampleInput make_arg L make_arg L make_arg L error_regex= Tensors must have same number dimensions yield ErrorInput SampleInput make_arg S M make_arg S make_arg S M kwargs= dim error_regex= Sizes tensors must match error inputs None input yield ErrorInput SampleInput make_arg S None error_type=TypeError error_regex= got None error inputs zero-dimensional tensors yield ErrorInput SampleInput make_arg make_arg error_regex= zero-dimensional cannot concatenated error inputs different dtype out tensors d = make_tensor device=device dtype=torch double device startswith mps torch float x = make_tensor device=device dtype=torch float yield ErrorInput SampleInput x kwargs= out d error_type=TypeError error_regex= invalid combination arguments reference_inputs_cat op device dtype requires_grad kwargs yield sample_inputs_cat_concat op device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Noncontiguous type promoting tensors = make_arg b = make_arg noncontiguous=True dtype=torch double c = make_arg dtype=torch float permute yield SampleInput b c kwargs= dim Special D tensor dim length case = make_arg b = make_arg yield SampleInput b yield SampleInput _elementwise_type_promo_np args type_promotion_kind _maybe_torch x isinstance x np ndarray torch from_numpy x x flattened = pytree arg_tree_leaves args transformed = tuple _maybe_torch flattened result_dtype _ = prims utils elementwise_dtypes transformed type_promotion_kind=type_promotion_kind torch_to_numpy_dtype_dict result_dtype _cat_np input_seq dim= inputs = tuple input_seq ndim == size == len inputs == np_dtype = _elementwise_type_promo_np input_seq type_promotion_kind=prims utils ELEMENTWISE_TYPE_PROMOTION_KIND NO_OPMATH np empty dtype=np_dtype np concatenate inputs axis=dim _floor_divide_np b dtype = _elementwise_type_promo_np b type_promotion_kind=prims utils ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT isinstance np ndarray = astype dtype isinstance b np ndarray b = b astype dtype np floor_divide b sample_inputs_hstack_dstack_vstack op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad tensor_shapes = First Tensor being -D special case hstack S S S S S S S S S s s s tensor_shapes tensors = make_arg s make_arg s make_arg s yield SampleInput tensors error_inputs_hstack_dstack_vstack op device make_arg = partial make_tensor dtype=torch int device=device requires_grad=False tensor_shapes = S S S S S S s s s tensor_shapes tensors = make_arg s make_arg s make_arg s Different dimension tensor yield ErrorInput SampleInput tensors error_regex= Tensors must have same number dimensions empty tensor list yield ErrorInput SampleInput error_regex= expects non-empty TensorList sample_inputs_unbind op_info device dtype requires_grad kwargs Note we don t do any tests where we unbind along -length dims because case unbind returns empty tuple breaks some assumptions some backward tests test_ops py shape_dims = S S S S S S S - S S S S S shape dim shape_dims yield SampleInput make_tensor shape dtype=dtype device=device requires_grad=requires_grad args= dim error_inputs_unbind op_info device make_arg = partial make_tensor dtype=torch int device=device requires_grad=False yield ErrorInput SampleInput make_arg args= error_type=IndexError error_regex= Dimension specified tensor has no dimensions yield ErrorInput SampleInput make_arg args= error_type=IndexError error_regex= Dimension out range reference_unbind t dim A numpy implementation torch unbind tuple s squeeze dim s np split t t shape dim dim sample_inputs_gather op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad low=None high=None yield SampleInput make_arg M S gather_variable S S M True device=device yield SampleInput make_arg M S gather_variable S S M True device=device torch int yield SampleInput make_arg M S gather_variable M S S True device=device Empty index tensor case see https github com pytorch pytorch pull yield SampleInput make_arg S torch tensor dtype=torch uint device=device yield SampleInput make_arg S torch tensor dtype=torch uint device=device D tensor case yield SampleInput make_arg torch tensor dtype=torch int device=device yield SampleInput make_arg torch tensor dtype=torch int device=device _fill_indices idx dim dim_size elems_per_row m n o i range dim == m j range dim == n k range dim == o ii = i j k ii dim = slice idx size dim + idx tuple ii = torch randperm dim_size elems_per_row error_inputs_gather op_info device kwargs src src = torch tensor device=device dtype=torch float idx idx = torch tensor device=device dtype=torch long Index should smaller than except dimension bad_src = make_tensor device=device dtype=torch float yield ErrorInput SampleInput bad_src args= idx error_regex= Size does match dimension TODO FIXME out dtype must match src dtype Creates new src idx since SampleInputs can t share tensors src = torch tensor device=device dtype=torch float idx = torch tensor device=device dtype=torch long out = torch empty device=device dtype=torch float yield ErrorInput SampleInput src args= idx kwargs= out out error_regex= Expected out tensor have dtype src index tensors must have same dimensions idx too few dimensions src = torch tensor device=device dtype=torch float idx = torch tensor device=device dtype=torch long yield ErrorInput SampleInput src args= idx error_regex= Index tensor must have same number dimensions src too few dimensions src = torch tensor device=device dtype=torch float idx = torch tensor device=device dtype=torch long yield ErrorInput SampleInput src args= idx error_regex= Index tensor must have same number dimensions index out bounds NOTE ErrorInput guarded because bounds checking does occur CUDA devices torch device device type == cpu src = torch tensor device=device dtype=torch float idx = torch tensor device=device dtype=torch long yield ErrorInput SampleInput src args= idx error_regex= index out bounds dimension x = torch rand device=device expand src = torch rand device=device ind = torch tensor device=device dtype=torch int yield ErrorInput SampleInput src args= ind kwargs=dict out=x error_type=RuntimeError error_regex= unsupported operation yield ErrorInput SampleInput src args= ind kwargs=dict out=src error_type=RuntimeError error_regex= unsupported operation yield ErrorInput SampleInput ind clone args= ind kwargs=dict out=ind error_type=RuntimeError error_regex= unsupported operation error_inputs_take op_info device kwargs x = torch rand device=device expand src = torch rand device=device ind = torch tensor device=device dtype=torch int yield ErrorInput SampleInput src args= ind kwargs=dict out=x error_type=RuntimeError error_regex= unsupported operation yield ErrorInput SampleInput src args= ind kwargs=dict out=src error_type=RuntimeError error_regex= unsupported operation yield ErrorInput SampleInput ind clone args= ind kwargs=dict out=ind - error_type=RuntimeError error_regex= unsupported operation Error inputs scatter error_inputs_scatter_and_scatter_add op_info device kwargs Error when dtype = src dtype src scalar src = make_tensor device=device dtype=torch float idx = torch tensor device=device dtype=torch long dst = torch zeros device=device dtype=torch double yield ErrorInput SampleInput dst args= idx src error_regex= Expected dtype equal src dtype Index destination must have same number dimensions src = make_tensor device=device dtype=torch float idx = torch tensor device=device dtype=torch long dst = torch zeros device=device dtype=torch float yield ErrorInput SampleInput dst args= idx src error_regex= Index tensor must have same number dimensions tensor Index src must have same number dimensions when src scalar src = make_tensor device=device dtype=torch float idx = torch tensor device=device dtype=torch long dst = torch zeros device=device dtype=torch float yield ErrorInput SampleInput dst args= idx src error_regex= Index tensor must have same number dimensions src tensor Index out bounds NOTE ErrorInput guarded because bounds checking does occur CUDA devices torch device device type == cpu src = make_tensor device=device dtype=torch float idx = torch tensor device=device dtype=torch long dst = torch zeros device=device dtype=torch float yield ErrorInput SampleInput dst args= idx src error_regex= index out bounds dimension size error_inputs_renorm op_info device kwargs zero_d = torch randn device=device yield ErrorInput SampleInput zero_d args= error_type=RuntimeError error_regex= needs least dimensions got dimensions error_inputs_ormqr op_info device kwargs zero_d = torch randn device=device yield ErrorInput SampleInput zero_d args= zero_d zero_d error_type=RuntimeError error_regex= input must have least dimensions https github com pytorch pytorch issues tensor_ = torch full device=device tensor_ = torch full device=device tensor_ = torch full device=device bool_ = True bool_ = True yield ErrorInput SampleInput tensor_ args= tensor_ tensor_ bool_ bool_ error_type=RuntimeError error_regex=r tau shape\ - \ must equal min\ other shape\ - \ input shape\ - \ \ error_inputs_diag op_info device kwargs zero_d = torch randn device=device yield ErrorInput SampleInput zero_d args= error_type=RuntimeError error_regex= D D zero_d = torch randn device=device yield ErrorInput SampleInput zero_d args= error_type=RuntimeError error_regex= D D error_inputs_embedding op_info device kwargs indices = torch rand device=device long weights = torch tensor device=device torch tensor device=device reshape weight weights yield ErrorInput SampleInput weight args= indices error_type=RuntimeError error_regex= weight must -D error_inputs_t op_info device kwargs yield ErrorInput SampleInput torch randn device=device error_regex= expects tensor = error_inputs_multinomial op_info device kwargs x = torch empty dtype=torch double device=device yield ErrorInput SampleInput x args= error_regex= prob_dist must dim x = torch empty dtype=torch long device=device yield ErrorInput SampleInput x args= error_regex= multinomial only supports floating-point dtypes input x = torch empty dtype=torch double device=device y = torch empty dtype=torch double device=device yield ErrorInput SampleInput x args= kwargs=dict out=y error_regex= multinomial expects Long tensor out x = torch empty dtype=torch double device=device yield ErrorInput SampleInput x args= error_regex= cannot sample n_sample = samples x = torch empty dtype=torch double device=device yield ErrorInput SampleInput x args= - error_regex= cannot sample n_sample = samples x = torch empty dtype=torch double device=device yield ErrorInput SampleInput x args= False error_regex= cannot sample n_sample prob_dist x = torch empty dtype=torch double device=device yield ErrorInput SampleInput x args= error_regex= number categories cannot exceed inputs = - inf -inf nan err_msg = probability tensor contains either ` inf ` ` nan ` element err_msg = invalid multinomial distribution rep_arg = False True torch device device type == cpu False torch device device type == cpu rep rep_arg kwargs = num_samples replacement rep shape inputs error case when input tensor contains ` inf ` ` nan ` negative element yield ErrorInput SampleInput torch tensor shape kwargs=kwargs error_regex=err_msg rep False err_msg error case invalid multinomial distribution sum probabilities = -D input x = torch zeros device=device yield ErrorInput SampleInput x kwargs=kwargs error_regex=err_msg error case invalid multinomial distribution sum probabilities = -D input x = torch zeros device=device yield ErrorInput SampleInput x kwargs=kwargs error_regex=err_msg error case invalid multinomial distribution x = yield ErrorInput SampleInput x kwargs=kwargs error_regex=err_msg error_inputs_gradient op_info device kwargs dtype torch long torch float torch complex t = torch tensor device=device dtype=dtype dim = spacing = yield ErrorInput SampleInput t kwargs=dict spacing=spacing dim=dim edge_order= error_type=RuntimeError error_regex= torch gradient expected spacing unspecified scalar yield ErrorInput SampleInput t kwargs=dict edge_order= error_type=RuntimeError error_regex= torch gradient only supports edge_order= edge_order= dim = spacing = yield ErrorInput SampleInput t kwargs=dict spacing=spacing dim=dim edge_order= error_type=RuntimeError error_regex= dim appears multiple times list dims dim = coordinates = torch tensor device= cpu torch tensor device= meta yield ErrorInput SampleInput t kwargs=dict spacing=coordinates dim=dim edge_order= error_type=RuntimeError error_regex= torch gradient expected each tensor same device yield ErrorInput SampleInput t kwargs=dict dim= error_type=IndexError error_regex= t = torch tensor yield ErrorInput SampleInput t kwargs=dict edge_order= error_type=RuntimeError error_regex= torch gradient expected each dimension size least t = torch tensor yield ErrorInput SampleInput t kwargs=dict edge_order= error_type=RuntimeError error_regex= torch gradient expected each dimension size least sample_inputs_rrelu op_info device dtype requires_grad kwargs yield sample_inputs_elementwise_unary op_info device dtype requires_grad op_kwargs=dict lower= upper= training=True make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_arg S yield SampleInput make_arg S training=False error_inputs_rrelu op_info device kwargs input = make_tensor S S device=device dtype=torch float yield ErrorInput SampleInput input kwargs= lower upper error_regex= Lower bound should less than equal upper bound error_inputs_masked_select op_info device kwargs x = torch rand device=device expand y = torch rand device=device mask = torch tensor True False True True False False device=device yield ErrorInput SampleInput y args= mask kwargs=dict out=x error_type=RuntimeError error_regex= unsupported operation yield ErrorInput SampleInput y args= mask kwargs=dict out=y error_type=RuntimeError error_regex= unsupported operation yield ErrorInput SampleInput mask clone args= mask kwargs=dict out=mask error_type=RuntimeError error_regex= unsupported operation error_inputs_median op_info device kwargs x = torch tensor nan nan device=device device == cuda yield ErrorInput SampleInput x kwargs=dict dim= - error_type=RuntimeError error_regex= CUDA Tensors cannot have more than dimensions error_inputs_index_select op_info device kwargs x = torch rand device=device expand y = torch rand device=device ind = torch tensor dtype=torch int device=device yield ErrorInput SampleInput y args= ind kwargs=dict out=x error_type=RuntimeError error_regex= unsupported operation error_inputs_index_add op_info device kwargs result = torch tensor source = torch tensor yield ErrorInput SampleInput result args= torch tensor source error_type=RuntimeError error_regex=r source tensor shape must match tensor shape r excluding specified dimension Got shape = \ \ source shape = \ \ error_inputs_logcumsumexp op_info device kwargs dim = srcs = torch randn device=device torch randn device=device src srcs yield ErrorInput SampleInput src args= dim error_type=IndexError error_regex= Dimension out range sample_inputs_take_along_dim op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad low=None high=None yield SampleInput make_arg S S gather_variable S S S True device=device ` indices ` broadcast yield SampleInput make_arg S S gather_variable S S True device=device ` ` broadcast yield SampleInput make_arg S gather_variable S S S True device=device without ` dim ` arg yield SampleInput make_arg S S gather_variable S S S True device=device error_inputs_aminmax_amax_amin op_info device is_ref=False kwargs Error Inputs zero-dim tensors when dim arg provided shape = S S err_msg_amax_amin = reduction err_msg_aminmax = cannot compute aminmax over empty dimension operation has no identity op_info name amax amin _refs amax _refs amin yield ErrorInput SampleInput torch rand shape device=device error_regex=err_msg_amax_amin op_info name == aminmax yield ErrorInput SampleInput torch rand shape device=device error_regex=err_msg_aminmax Error Inputs tensors more than dimension sizes = err_msg = only tensors up dims supported yield ErrorInput SampleInput torch randn sizes device=device kwargs= dim - error_regex=err_msg yield ErrorInput SampleInput torch randn sizes device=device kwargs= dim error_regex=err_msg Error Inputs repeated dim op_info name amax amin _refs amax _refs amin dims = - err_msg = list dims x = torch randn S S S S device=device dim dims yield ErrorInput SampleInput x kwargs= dim dim error_regex=err_msg Error Input illegal dtype input = torch randn L L dtype=torch float device=device max_values = torch empty L dtype=torch float device=device min_values = torch empty L dtype=torch double device=device illegal_values = torch empty L dtype=torch int device=device Unlike regular PyTorch amax amin refs don t require input out dtypes match exactly https github com pytorch pytorch pull #pullrequestreview- is_ref err_msg_amax_amin = Attempting cast torch float out tensor dtype torch int can t cast because safe err_msg_amax_amin = Expected dtype input out match got Float input s dtype Int out s dtype err_msg_aminmax = Expected out tensor have dtype float got double instead op_info name amax amin _refs amax _refs amin yield ErrorInput SampleInput input kwargs= dim out illegal_values error_regex=err_msg_amax_amin op_info name == aminmax yield ErrorInput SampleInput input kwargs= dim out max_values min_values error_regex=err_msg_aminmax Error Inputs functions raise error specified zero d dimension reduction dim err_msg = reduction FIXME eager ref impl throw different types errors error_type = IndexError refs op_info name RuntimeError yield ErrorInput SampleInput torch rand shape device=device kwargs= dim error_type=error_type error_regex=err_msg sample_inputs_aminmax op_info device dtype requires_grad kwargs test_cases tuple tuple dict = type ignore assignment S S S S S S dim S S S dim keepdim True dim dim keepdim True S S dim shape kwargs test_cases yield SampleInput make_tensor shape dtype=dtype device=device requires_grad=requires_grad kwargs error_inputs_diff op_info device kwargs t = torch rand device=device n = - yield ErrorInput SampleInput t args= n kwargs=kwargs error_type=RuntimeError error_regex=f order must non-negative got n sample_inputs_diff op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad test_cases = None None S None None S None None S None None S S None None S S None None S S S S S S None S XS XS XS None None XS XS XS None None XS XS XS XS XS XS XS XS XS XS XS XS XS XS XS XS XS XS XS XS XS XS XS size dim size_prepend size_append test_cases prepend_size = size_prepend None size_prepend dim append_size = size_append None size_append dim dim_size = size dim + prepend_size + append_size n range dim_size input_tensor = make_arg size prepend = make_arg size_prepend size_prepend None append = make_arg size_append size_append None yield SampleInput input_tensor n dim prepend append add some samples n dim_size yield SampleInput make_arg XS XS XS S + yield SampleInput make_arg XS XS XS S + make_arg XS XS XS make_arg XS XS XS sample_inputs_histogram op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad sizes = S S S S S S S S S S size bin_ct weighted density product sizes range False True False True input_tensor = make_arg size weight_tensor = make_arg size weighted None yield SampleInput input_tensor bin_ct weight=weight_tensor density=density bins_tensor = make_arg bin_ct + sorted_bins _bins_indices = torch sort bins_tensor yield SampleInput input_tensor sorted_bins weight=weight_tensor density=density sample_inputs_histogramdd op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad sizes = S S S S S S S S S bin_ct_patterns = size bin_ct_pattern weighted density product sizes bin_ct_patterns False True False True input_tensor = make_arg size bin_ct = bin_ct_pattern size - weight_tensor = make_arg size - weighted None yield SampleInput input_tensor bin_ct weight=weight_tensor density=density bins_tensor = make_arg ct + ct bin_ct yield SampleInput input_tensor bins_tensor weight=weight_tensor density=density error_inputs_histogramdd opinfo device kwargs invalid_bins = make_arg = partial make_tensor dtype=torch float device=device requires_grad=False msg = histogramdd The size bins must equal innermost dimension input yield ErrorInput SampleInput make_arg invalid_bins error_regex=msg sample_inputs_histc op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad sizes = S S S S S S S S S S size min max product sizes - construct sample input omitting bins arg yield SampleInput make_arg size min=min max=max construct sample inputs few different bins values bins yield SampleInput make_arg size bins=bins min=min max=max sample_inputs_bincount op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad size weighted product S M False True input_tensor = torch randint size size dtype=dtype device=device weight_tensor = make_arg size weighted None max_val = int input_tensor max item minlength max_val max_val max_val yield SampleInput input_tensor weights=weight_tensor minlength=minlength sample_inputs_bucketize op_info device dtype requires_grad reference_inputs_mode=False kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad sizes = S S S S S S S S S S S S S S S S reference_inputs_mode sizes += input_shape nb out_int right product sizes False True False True input_tensor = make_arg input_shape boundaries = make_arg nb msort yield SampleInput input_tensor boundaries out_int =out_int right=right reference_inputs_bucketize = partial sample_inputs_bucketize reference_inputs_mode=True error_inputs_bucketize opinfo device kwargs make_arg = partial make_tensor dtype=torch float device=device requires_grad=False yield ErrorInput SampleInput make_arg S S S make_arg S S error_regex= boundaries tensor must dimension sample_inputs_searchsorted op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad unsorted tensor size input sizes is_scalar sizes = False M M M M False False M M M M False False M M M M M M False L True size input_sizes is_scalar noncontiguous out_int right product sizes False True False True False True unsorted_tensor = make_arg size noncontiguous=noncontiguous input_size input_sizes input = make_arg input_size noncontiguous=noncontiguous is_scalar input = input item np prod size == boundary_tensor = unsorted_tensor sorter = make_tensor size dtype=torch int device=device noncontiguous=noncontiguous boundary_tensor sorter = torch sort unsorted_tensor side = right right left yield SampleInput boundary_tensor input out_int =out_int right=right yield SampleInput boundary_tensor input out_int =out_int side=side yield SampleInput unsorted_tensor input out_int =out_int right=right sorter=sorter yield SampleInput unsorted_tensor input out_int =out_int side=side sorter=sorter sample_inputs_gradient op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad low=None high=None test_cases_float = S None None S None S S None None S S None S S size spacing dim edge_order test_cases_float t = make_arg size yield SampleInput t dim=dim spacing=spacing edge_order=edge_order test_cases_tensor = - size coordinates dim edge_order test_cases_tensor t = make_arg size coordinates_tensor_list = coords coordinates ` coords ` will always contain floating point values Python does support implicit conversion integer using ` __int__ ` TODO can simplified after https github com pytorch pytorch issues fixed = torch tensor coords device=device coordinates_tensor_list append dtype yield SampleInput t dim=dim spacing=coordinates_tensor_list edge_order=edge_order sample_inputs_getitem op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad test_args = slice slice slice None slice None slice None slice None slice None slice None slice None slice None Ellipsis torch LongTensor index_variable S device=device mask_not_all_zeros S args test_args yield SampleInput make_arg S S S args=args yield SampleInput make_arg S S S S args= slice None slice None sample_inputs_index_put op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad accumulate False True Test indices arg yield SampleInput make_arg S S As defined docs accumulate false duplicate indices supported index_variable accumulate S device=device make_arg accumulate S accumulate=accumulate Test mask arg mask = torch zeros S dtype=torch bool accumulate mask_not_all_zeros S yield SampleInput make_arg S S mask make_arg S accumulate=accumulate sample_inputs_sort op_info device dtype requires_grad kwargs small_ d_unique res = torch randperm S S S dtype=torch int device=device view S S S res = res dtype requires_grad_ requires_grad res large_ d_unique res = torch randperm L L L dtype=torch int device=device res = res dtype requires_grad_ requires_grad res Test case large tensor yield SampleInput large_ d_unique Test cases small d tensors Imitates legacy tests test test_torch py dims = range - flag = True False dim descending stable product dims flag flag default schema without stable sort dtype == torch bool torch device device type == cuda bool cuda requires stable sort stable results least index yield SampleInput small_ d_unique dim descending schema stable sort no CUDA support yet torch device device type == cpu yield SampleInput small_ d_unique dim=dim descending=descending stable=stable Test cases scalar tensor tensor_opt = dict dtype=dtype device=device requires_grad=requires_grad yield SampleInput torch tensor tensor_opt yield SampleInput torch tensor tensor_opt yield SampleInput torch tensor tensor_opt True Test cases empty tensor yield SampleInput torch tensor tensor_opt yield SampleInput torch tensor tensor_opt yield SampleInput torch tensor tensor_opt True Test cases stable sort yield SampleInput small_ d_unique stable=True yield SampleInput small_ d_unique dim= stable=True yield SampleInput small_ d_unique dim= descending=True stable=True sample_inputs_threshold op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad sizes = S S S S S S x_size sizes threshold values args must numbers yield SampleInput make_arg x_size make_arg item make_arg item sample_inputs_unique op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad sizes = S S S S S S S S S S shape sorted return_inverse return_counts dim \ product sizes False True False True False True None - - torch unique cannot called input tensor has zero dimension which isn t selected dim shape shape index dim continue skip invalid dim args dim None dim -len shape dim = len shape continue kwargs = dict sorted=sorted return_inverse=return_inverse return_counts=return_counts dim=dim construct test case only one distinct value input_t = torch zeros shape dtype=dtype device=device requires_grad=requires_grad yield SampleInput input_t kwargs construct test case mixed s s input_t = make_arg shape dtype=torch bool requires_grad=False \ dtype requires_grad_ requires_grad yield SampleInput input_t kwargs construct test case many different values yield SampleInput make_arg shape kwargs sample_inputs_unique_consecutive args kwargs sample_input sample_inputs_unique args kwargs sample_input kwargs sorted sample_input kwargs pop sorted yield sample_input sample_inputs_adaptive_avg_pool d op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Ordered input shape output size cases = input_shape output_size cases Batched yield SampleInput make_arg input_shape args= output_size Unbatched yield SampleInput make_arg input_shape args= output_size error_inputs_adaptive_avg_pool d opinfo device kwargs make_arg = partial make_tensor device=device dtype=torch float error inputs empty output yield ErrorInput SampleInput make_arg output_size= error_regex= output_size should contain one int error inputs output_size lesser than yield ErrorInput SampleInput make_arg output_size= - error_regex= elements output_size must greater than equal sample_inputs_adaptive_avg_pool d op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Ordered input shape output size cases = None None None None input_shape output_size cases Batched yield SampleInput make_arg input_shape args= output_size Unbatched yield SampleInput make_arg input_shape args= output_size error_inputs_adaptive_avg_pool d opinfo device kwargs make_arg = partial make_tensor device=device dtype=torch float error inputs incorrect input dimension yield ErrorInput SampleInput make_arg output_size= error_type=ValueError error_regex= Input dimension should least error inputs empty output yield ErrorInput SampleInput make_arg output_size= error_regex= output_size must error inputs output_size lesser than yield ErrorInput SampleInput make_arg output_size= - error_regex= elements output_size must greater than equal sample_inputs_adaptive_avg_pool d op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Ordered input shape output size cases = None None None None None None input_shape output_size cases Batched yield SampleInput make_arg input_shape args= output_size Unbatched yield SampleInput make_arg input_shape args= output_size error_inputs_adaptive_avg_pool d opinfo device kwargs make_arg = partial make_tensor device=device dtype=torch float error inputs incorrect input dimension yield ErrorInput SampleInput make_arg output_size= error_type=ValueError error_regex= Input dimension should least error inputs empty output yield ErrorInput SampleInput make_arg output_size= error_regex= output_size must error inputs output_size lesser than yield ErrorInput SampleInput make_arg output_size= - error_regex= elements output_size must greater than equal sample_inputs_adaptive_max_pool d op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Ordered input shape output size cases = batch size doesn t work cannot reshape tensor elements into shape - shapes return_idx product cases True False Batched yield SampleInput make_arg shapes args= shapes return_idx Unbatched yield SampleInput make_arg shapes args= shapes return_idx error_inputs_adaptive_max_pool d opinfo device kwargs make_arg = partial make_tensor device=device dtype=torch float error inputs empty output yield ErrorInput SampleInput make_arg output_size= error_regex= output_size should contain one int error inputs output_size lesser than yield ErrorInput SampleInput make_arg output_size= - error_regex= Trying create tensor negative dimension sample_inputs_adaptive_max_pool d op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Ordered input shape output size cases = batch size doesn t work cannot reshape tensor elements into shape - None None None None shapes return_idx product cases True False Batched yield SampleInput make_arg shapes args= shapes return_idx Unbatched yield SampleInput make_arg shapes args= shapes return_idx error_inputs_adaptive_max_pool d opinfo device kwargs make_arg = partial make_tensor device=device dtype=torch float error inputs incorrect input dimension yield ErrorInput SampleInput make_arg output_size= error_type=ValueError error_regex= Input dimension should least error inputs empty output yield ErrorInput SampleInput make_arg output_size= error_regex= internal error error inputs output_size lesser than yield ErrorInput SampleInput make_arg output_size= - error_regex= Trying create tensor negative dimension sample_inputs_adaptive_max_pool d op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Ordered input shape output size cases = batch size doesn t work cannot reshape tensor elements into shape - None None None None None None shapes return_idx product cases True False Batched yield SampleInput make_arg shapes args= shapes return_idx Unbatched yield SampleInput make_arg shapes args= shapes return_idx error_inputs_adaptive_max_pool d opinfo device kwargs make_arg = partial make_tensor device=device dtype=torch float error inputs incorrect input dimension yield ErrorInput SampleInput make_arg output_size= error_type=ValueError error_regex= Input dimension should least error inputs empty output yield ErrorInput SampleInput make_arg output_size= error_regex= internal error error inputs output_size lesser than yield ErrorInput SampleInput make_arg output_size= - error_regex= Trying create tensor negative dimension _TestParamsMaxPoolBase __init__ - None kwargs = kernel_size stride None ceil_mode True False padding dilation return_indices True False shapes = None batch channels signal _gen_shape shape product shapes shape None indicates missing batch dimension shape None shape = shape yield shape torch contiguous_format only d N C H W rank tensors support channels_last memory format len shapes == len shape == yield shape torch channels_last _gen_kwargs keys = kwargs keys values product kwargs values yield dict zip keys values strict=True gen_input_params yield product _gen_shape _gen_kwargs _TestParamsMaxPool d _TestParamsMaxPoolBase __init__ - None super __init__ kwargs kernel_size += kwargs stride += kwargs padding += kwargs dilation += _TestParamsMaxPool d _TestParamsMaxPoolBase __init__ - None super __init__ kwargs kernel_size += kwargs stride += kwargs padding += kwargs dilation += shapes append _TestParamsMaxPool d _TestParamsMaxPoolBase __init__ - None super __init__ kwargs kernel_size += kwargs stride += kwargs dilation += shapes append shapes append sample_inputs_max_pool op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=False params_generator_type_dict = nn functional max_pool d _TestParamsMaxPool d nn functional max_pool d _TestParamsMaxPool d nn functional max_pool d _TestParamsMaxPool d max_pool d_with_indices_backward _TestParamsMaxPool d params_generator = params_generator_type_dict op_info name shape memory_format kwargs params_generator gen_input_params arg = make_arg shape memory_format=memory_format requires_grad_ requires_grad yield SampleInput arg kwargs=kwargs max_pool d_backward args kernel_size= stride= padding= dilation= ceil_mode=False kwargs out indices = torch nn functional max_pool d_with_indices args kernel_size=kernel_size stride=stride padding=padding dilation=dilation ceil_mode=ceil_mode return_indices=True grad_out = torch ones_like out stride None stride = kernel_size out_b = torch ops aten max_pool d_with_indices_backward default grad_out args kernel_size stride padding dilation ceil_mode indices out_b error_inputs_max_pool d op_info device kwargs Toggle requires_grad because ` max_pool d ` has different path based whether ` requires_grad ` set requires_grad True False make_arg = partial make_tensor device=device dtype=torch float requires_grad=requires_grad error inputs when pad negative x = make_arg yield ErrorInput SampleInput x kwargs= kernel_size stride padding - return_indices True error_regex= pad must non-negative error inputs when pad kernel_size yield ErrorInput SampleInput x kwargs= kernel_size stride padding return_indices True error_regex= pad should most half effective kernel size error inputs when pad kernel_size - dilation + when dilation default yield ErrorInput SampleInput x kwargs= kernel_size dilation stride padding return_indices True error_regex= pad should most half effective kernel size error inputs input tensor error_msg = r Expected D D \ batch mode\ tensor optional dim batch size input yield ErrorInput SampleInput make_arg requires_grad=requires_grad kwargs= kernel_size error_regex=error_msg error inputs empty input yield ErrorInput SampleInput torch tensor device=device requires_grad=requires_grad kwargs= kernel_size error_regex=error_msg error unbatched input sized non-batch dims yield ErrorInput SampleInput make_arg requires_grad=requires_grad kwargs= kernel_size error_regex=error_msg error batched input sized non-batch dims yield ErrorInput SampleInput make_arg requires_grad=requires_grad kwargs= kernel_size error_regex=error_msg error inputs empty input stride= error_msg = stride must greater than zero got yield ErrorInput SampleInput make_arg kwargs= kernel_size stride error_regex=error_msg error inputs empty input dilation= error_msg = dilation must greater than zero got yield ErrorInput SampleInput make_arg kwargs= kernel_size stride padding dilation error_regex=error_msg error inputs invalid output size error_msg = Invalid computed output size - yield ErrorInput SampleInput make_arg kwargs= kernel_size stride padding dilation error_regex=error_msg error inputs when kernel_size= error_msg = kernel_size must greater than zero yield ErrorInput SampleInput x kwargs= kernel_size error_regex=error_msg error inputs strides error_msg = stride must greater than zero yield ErrorInput SampleInput x kwargs= kernel_size stride error_regex=error_msg error_inputs_max_pool d op_info device kwargs make_arg = partial make_tensor device=device dtype=torch float requires_grad=False error inputs when pad negative x = make_arg yield ErrorInput SampleInput x kwargs= kernel_size stride padding - return_indices True error_regex= pad must non-negative -dimensional kernel yield ErrorInput SampleInput x kwargs= kernel_size stride padding - return_indices True error_regex= pad must non-negative error inputs when pad kernel_size kernel_size int yield ErrorInput SampleInput x kwargs= kernel_size stride padding return_indices True error_regex= pad should most half effective kernel size error inputs when pad kernel_size kernel_size tuple yield ErrorInput SampleInput x kwargs= kernel_size stride padding return_indices True error_regex= pad should most half effective kernel size error unbatched input sized non-batch dims err_msg = r Expected D D \ batch mode\ tensor optional dim batch size input yield ErrorInput SampleInput make_arg kwargs= kernel_size error_regex=err_msg error batched input sized non-batch dims yield ErrorInput SampleInput make_arg kwargs= kernel_size error_regex=err_msg error_inputs_max_pool d op_info device kwargs make_arg = partial make_tensor device=device dtype=torch float requires_grad=False error inputs when pad negative x = make_arg yield ErrorInput SampleInput x kwargs= kernel_size stride padding - return_indices True error_regex= pad must non-negative -dimensional kernel yield ErrorInput SampleInput x kwargs= kernel_size stride padding - return_indices True error_regex= pad must non-negative error inputs when pad kernel_size kernel_size int yield ErrorInput SampleInput x kwargs= kernel_size stride padding return_indices True error_regex= pad should most half effective kernel size error inputs when pad kernel_size kernel_size tuple yield ErrorInput SampleInput x kwargs= kernel_size stride padding return_indices True error_regex= pad should most half effective kernel size error unbatched input sized non-batch dims err_msg = r Expected input\ s non-batch dimensions have positive length yield ErrorInput SampleInput make_arg kwargs= kernel_size error_regex=err_msg error batched inputs sized non-batch dims yield ErrorInput SampleInput make_arg kwargs= kernel_size error_regex=err_msg sample_inputs_normalize device dtype requires_grad kwargs make_arg = partial make_tensor low=- high= device=device dtype=dtype requires_grad=requires_grad cases tuple tuple int dict = type ignore assignment p dim p dim p dim p - dim p dim - p dim eps e- input_shape kwargs cases yield SampleInput make_arg input_shape kwargs=kwargs complex_conv fn input_size weight grad_output stride padding dilation groups conv W x b = conv Wr xr br - conv Wi xi + i conv Wi xr bi + conv Wr xi = conv Wr xr br b = conv Wi xi c = conv Wr + Wi xr + xi br + bi conv W x b = - b + i c - - b grad_output_ = torch view_as_real grad_output grad_output_r = grad_output_ grad_output_i = grad_output_ weight_ = torch view_as_real weight weight_r = weight_ weight_i = weight_ = fn input_size weight_r grad_output_r stride padding dilation groups b = fn input_size weight_i grad_output_i stride padding dilation groups c = fn input_size weight_r + weight_i grad_output_r + grad_output_i stride padding dilation groups - b + j c - - b conv_transpose_ref input weight bias stride= padding= output_padding= dilation= groups= fn=None Derivative ` conv ` ` conv_transpose ` To verify correctness ` conv_transpose ` we rely ` torch nn grad ` implementation which tested test_nn py floating dtypes assert fn None grad_fn_map = torch nn functional conv_transpose d torch nn grad conv d_input torch nn functional conv_transpose d torch nn grad conv d_input torch nn functional conv_transpose d torch nn grad conv d_input batched_dim_map = torch nn functional conv_transpose d torch nn functional conv_transpose d torch nn functional conv_transpose d Input ` ref ` ndarray input weight = torch from_numpy input torch from_numpy weight is_batched = len input shape == batched_dim_map fn is_batched input = input unsqueeze bias None bias = torch from_numpy bias unsqueeze_dims = input ndim - _ range unsqueeze_dims bias = bias unsqueeze grad_output = input Get input shape grad_fn conv_transpose_output = fn grad_output meta weight meta None stride=stride padding=padding output_padding=output_padding groups=groups dilation=dilation input_size = conv_transpose_output shape grad_fn = grad_fn_map fn weight dtype is_complex out = complex_conv grad_fn input_size weight grad_output stride padding dilation groups Floating out = grad_fn input_size weight grad_output stride padding dilation groups bias None out = out + bias out squeeze is_batched out sample_inputs_conv_transpose d op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Ordered shapes input weight bias dict values stride padding output_padding groups dilation cases tuple tuple int tuple int tuple int dict = type ignore assignment stride padding output_padding groups stride padding output_padding groups dilation stride padding output_padding groups dilation None stride padding output_padding groups None input_shape weight bias kwargs cases Batched yield SampleInput make_arg input_shape args= make_arg weight make_arg bias bias None bias kwargs=kwargs Unbatched yield SampleInput make_arg input_shape args= make_arg weight make_arg bias bias None bias kwargs=kwargs sample_inputs_conv_transpose d op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Ordered shapes input weight bias dict values stride padding output_padding groups dilation cases tuple tuple int tuple int tuple int dict = type ignore assignment stride padding output_padding groups stride padding output_padding groups dilation stride padding output_padding groups dilation None stride padding output_padding groups None groups None input_shape weight bias kwargs cases Batched yield SampleInput make_arg input_shape args= make_arg weight make_arg bias bias None bias kwargs=kwargs Unbatched yield SampleInput make_arg input_shape args= make_arg weight make_arg bias bias None bias kwargs=kwargs sample_inputs_conv_transpose d op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Ordered shapes input weight bias dict values stride padding output_padding groups dilation cases tuple tuple int tuple int tuple int dict = type ignore assignment stride padding output_padding groups stride padding output_padding groups dilation stride padding output_padding groups dilation None stride padding output_padding groups None input_shape weight bias kwargs cases Batched yield SampleInput make_arg input_shape args= make_arg weight make_arg bias bias None bias kwargs=kwargs Unbatched yield SampleInput make_arg input_shape args= make_arg weight make_arg bias bias None bias kwargs=kwargs sample_inputs_conv d op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Ordered shapes input weight bias dict values stride padding dilation groups cases tuple = stride padding groups stride padding groups dilation None stride padding valid stride padding same groups dilation With defaults None input_shape weight bias kwargs cases Batched yield SampleInput make_arg input_shape args= make_arg weight make_arg bias bias None bias kwargs=kwargs Unbatched yield SampleInput make_arg input_shape args= make_arg weight make_arg bias bias None bias kwargs=kwargs error_inputs_conv d opinfo device kwargs make_arg = partial make_tensor device=device dtype=torch float make_int_arg = partial make_tensor device=device dtype=torch int make_complex_arg = partial make_tensor device=device dtype=torch complex error inputs different dtypes input tensor bias yield ErrorInput SampleInput make_int_arg args= make_int_arg make_arg error_regex= should same error inputs different dtypes input tensor bias yield ErrorInput SampleInput make_arg args= make_arg make_complex_arg error_regex= should same error inputs negative strides yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= stride - error_regex= non-positive stride supported error inputs negative padding yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= padding - error_regex= negative padding supported error inputs negative dilation yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= dilation - error_regex= dilation should greater than zero FIXME https github com pytorch pytorch issues error inputs bias shape equal output channels yield ErrorInput SampleInput make_arg args= make_arg make_arg error_regex= expected bias -dimensional elements error inputs input ndim = weight ndim yield ErrorInput SampleInput make_arg args= make_arg make_arg error_regex= weight should have least three dimensions error inputs weight less than number groups yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= padding same groups error_regex= expected weight least dimension error inputs weight less than number groups yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= groups error_regex= expected weight least dimension error inputs invalid groups yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= padding same groups - error_regex= non-positive groups supported error inputs invalid groups yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= padding same groups error_regex= non-positive groups supported error_inputs_conv d opinfo device kwargs make_arg = partial make_tensor device=device dtype=torch float make_int_arg = partial make_tensor device=device dtype=torch int make_complex_arg = partial make_tensor device=device dtype=torch complex error inputs different dtypes input tensor bias yield ErrorInput SampleInput make_int_arg args= make_int_arg make_arg error_regex= should same error inputs different dtypes input tensor bias yield ErrorInput SampleInput make_arg args= make_arg make_complex_arg error_regex= should same error inputs negative strides yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= stride - error_regex= non-positive stride supported error inputs negative padding yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= padding - error_regex= negative padding supported error inputs negative dilation yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= dilation - error_regex= dilation should greater than zero FIXME https github com pytorch pytorch issues error inputs bias shape equal output channels yield ErrorInput SampleInput make_arg args= make_arg make_arg error_regex= expected bias -dimensional elements error inputs input ndim = weight ndim yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= padding same error_regex= Expected -dimensional input -dimensional weight error inputs weight less than number groups yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= groups error_regex= expected weight least dimension error inputs groups weight less than number groups yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= padding same groups error_regex= expected weight least dimension error inputs invalid groups yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= padding same groups - error_regex= non-positive groups supported error inputs invalid groups yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= padding same groups error_regex= non-positive groups supported sample_inputs_conv d op_info device dtype requires_grad jit_fail_sample=False kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Ordered shapes input weight bias dict values stride padding groups dilation cases tuple = stride padding groups stride padding groups dilation stride padding groups dilation stride padding groups dilation None stride padding groups stride padding valid stride padding same dilation Below group related samples common_nn py groups groups None groups groups stride groups padding groups dilation groups With defaults None input_shape weight bias kwargs cases Batched yield SampleInput make_arg input_shape args= make_arg weight make_arg bias bias None bias kwargs=kwargs Unbatched yield SampleInput make_arg input_shape args= make_arg weight make_arg bias bias None bias kwargs=kwargs sample_inputs_conv d opinfo device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Ordered shapes input weight bias dict values stride padding dilation groups cases tuple = padding same stride dilation None padding valid None padding same None padding same dilation None padding same dilation None padding valid groups stride dilation groups input_shape weight bias kwargs cases Batched yield SampleInput make_arg input_shape args= make_arg weight make_arg bias bias None bias kwargs=kwargs Unbatched yield SampleInput make_arg input_shape args= make_arg weight make_arg bias bias None bias kwargs=kwargs error_inputs_conv d opinfo device kwargs make_arg = partial make_tensor device=device dtype=torch float make_int_arg = partial make_tensor device=device dtype=torch int make_complex_arg = partial make_tensor device=device dtype=torch complex error inputs different dtypes input tensor bias yield ErrorInput SampleInput make_int_arg args= make_int_arg make_arg error_regex= should same error inputs different dtypes input tensor bias yield ErrorInput SampleInput make_arg args= make_arg make_complex_arg error_regex= should same error inputs negative strides yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= stride - error_regex= non-positive stride supported error inputs negative padding yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= padding - error_regex= negative padding supported error inputs negative dilation yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= dilation - error_regex= dilation should greater than zero FIXME https github com pytorch pytorch issues error inputs bias shape equal output channels yield ErrorInput SampleInput make_arg args= make_arg make_arg error_regex= expected bias -dimensional elements error inputs input ndim = weight ndim yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= padding same error_regex= Expected -dimensional input -dimensional weight error inputs weight less than number groups yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= groups error_regex= expected weight least dimension error inputs weight less than number groups yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= padding same groups error_regex= expected weight least dimension error inputs invalid groups yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= padding same groups error_regex= non-positive groups supported error inputs padding= same supported strided convolutions yield ErrorInput SampleInput make_arg args= make_arg make_arg kwargs= stride padding same groups error_regex= padding= same supported strided convolutions sample_inputs_group_norm opinfo device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Ordered input shape num groups kwargs eps cases tuple tuple int int float = type ignore assignment eps eps - eps e- eps e- S S S eps num_channels inferred input shape dimension input_shape num_groups kwargs cases Shape weight bias should same num_channels channels = input_shape len input_shape weight_tensor = make_arg channels bias_tensor = make_arg channels Checking permutations weights biases ` None ` weights = weight_tensor None biases = bias_tensor None weight bias itertools product weights biases kwargs = weight weight bias bias kwargs yield SampleInput make_arg input_shape num_groups kwargs Without any optional args yield SampleInput make_arg args= reference_inputs_group_norm op_info device dtype requires_grad kwargs yield sample_inputs_group_norm op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Ordered input shape num groups kwargs eps cases tuple tuple int int float = type ignore assignment eps e- equivalent InstanceNorm GroupNorm C num_groups=C == InstanceNorm num_features=C eps e- equivalent LayerNorm GroupNorm C num_groups= affine=False == LayerNorm normalized_shape= C H W elementwise_affine=False eps e- num_channels inferred input shape dimension input_shape num_groups kwargs cases Shape weight bias should same num_channels channels = input_shape len input_shape input_tensor = make_arg input_shape weight_tensor = make_arg channels bias_tensor = make_arg channels Checking permutations weights biases ` None ` weights = weight_tensor None biases = bias_tensor None weight bias itertools product weights biases kwargs = weight weight bias bias kwargs yield SampleInput input_tensor num_groups kwargs sample_inputs_instance_norm opinfo device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_arg_without_requires_grad = partial make_tensor device=device dtype=dtype requires_grad=False Ordered input shape kwargs momentum eps cases tuple tuple int dict = type ignore assignment S S S momentum eps S S S momentum eps use_input_stats True momentum - momentum momentum - eps momentum - eps input_shape kwargs cases args running mean running var weight bias should necessarily shape channels channels = input_shape weight = make_arg channels bias = make_arg channels running_mean = make_arg_without_requires_grad channels low= running_var = make_arg_without_requires_grad channels low= new_kwargs = running_mean running_mean running_var running_var weight weight bias bias kwargs yield SampleInput make_arg input_shape args= kwargs=new_kwargs Checking permutations weights biases ` None ` instance_norm assumes there s bias there s weight weights = channels None biases = None None weight_channels bias_channels zip weights biases strict=True running_mean = make_arg_without_requires_grad channels low= running_var = make_arg_without_requires_grad channels low= yield SampleInput make_arg input_shape args= kwargs= running_mean running_mean running_var running_var weight make_arg weight_channels weight_channels None None bias make_arg bias_channels bias_channels None None Test case no optional kwargs yield SampleInput make_arg kwargs= sample_inputs_safe_softmax opinfo device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=False make_bool_mask shape torch randint shape device=device dtype=torch bool mask_two_rows rows cols mask_two_rows = torch ones rows cols dtype=torch bool device=device mask_two_rows rows - = False mask_two_rows rows - = False mask_two_rows convert_to_float_mask mask torch Tensor - torch Tensor torch where ~mask float -inf with_requires_grad tensor tensor requires_grad_ requires_grad generate_input_from_mask mask_shape dim mask = make_bool_mask mask_shape input_tensor = make_arg mask_shape masked_input = input_tensor + convert_to_float_mask mask SampleInput with_requires_grad masked_input kwargs= dim dim samples = Basic D tensor mask generate_input_from_mask dim= D tensor mask testing different dim generate_input_from_mask dim= D tensor testing different dim generate_input_from_mask dim= Edge case D tensor generate_input_from_mask dim= Edge case tensor one dimension size generate_input_from_mask dim= Testing all elements masked SampleInput with_requires_grad make_arg + convert_to_float_mask torch zeros dtype=torch bool device=device kwargs= dim Testing no elements masked SampleInput with_requires_grad make_arg + convert_to_float_mask torch ones dtype=torch bool device=device kwargs= dim Testing two rows masked SampleInput with_requires_grad make_arg + convert_to_float_mask mask_two_rows kwargs= dim yield samples sample_inputs_layer_norm opinfo device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Ordered input shape normalized_shape kwarg dict eps cases tuple tuple int tuple int dict = type ignore assignment eps eps - input_shape normalized_shape kwargs cases Shape weight bias should same normalized_shape weight = make_arg normalized_shape bias = make_arg normalized_shape yield SampleInput make_arg input_shape args= normalized_shape weight bias kwargs=kwargs Without any optional args yield SampleInput make_arg args= TODO krshrimali once to_numpy method SampleInput modified take None inputs enable these inputs see https github com pytorch pytorch pull #discussion_r With weight ` None ` bias yield SampleInput make_arg args= make_arg None With ` None ` weight bias tests failing see link above yield SampleInput make_arg args= None make_arg sample_inputs_native_layer_norm opinfo device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Ordered input shape normalized_shape eps cases tuple tuple int tuple int float = type ignore assignment - e- e- e- input_shape normalized_shape eps cases Shape weight bias should same normalized_shape weight = make_arg normalized_shape bias = make_arg normalized_shape yield SampleInput make_arg input_shape args= normalized_shape weight bias eps yield SampleInput make_arg input_shape args= normalized_shape None bias eps yield SampleInput make_arg input_shape args= normalized_shape weight None eps yield SampleInput make_arg input_shape args= normalized_shape None None eps sample_inputs_rms_norm opinfo device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad high= Ordered input shape normalized_shape kwarg dict eps cases tuple tuple int tuple int dict = type ignore assignment eps eps - input_shape normalized_shape kwargs cases Shape weight bias should same normalized_shape weight = make_arg normalized_shape yield SampleInput make_arg input_shape args= normalized_shape weight kwargs=kwargs Without any optional args yield SampleInput make_arg args= error_inputs_group_norm opinfo device kwargs make_arg = partial make_tensor device=device dtype=torch float requires_grad=False check input has minimum number dimensions err_msg = Expected least dimensions input tensor received s = SampleInput make_arg args= yield ErrorInput s error_regex=err_msg check channels dimension compatible number groups err_msg = Expected number channels input divisible num_groups got input shape s = SampleInput make_arg args= yield ErrorInput s error_regex=err_msg error_inputs_native_layer_norm opinfo device kwargs make_arg = partial make_tensor device=device dtype=torch float requires_grad=False input_shape = err_msg = Expected normalized_shape least -dimensional s = SampleInput make_arg input_shape args= None None e- yield ErrorInput s error_regex=err_msg normalized_shape = weight = make_arg err_msg = Expected weight same shape normalized_shape s = SampleInput make_arg input_shape args= normalized_shape weight None e- yield ErrorInput s error_regex=err_msg bias = make_arg err_msg = Expected bias same shape normalized_shape s = SampleInput make_arg input_shape args= normalized_shape None bias e- yield ErrorInput s error_regex=err_msg err_msg = Given normalized_shape= s = SampleInput make_arg args= None None e- yield ErrorInput s error_regex=err_msg error_inputs_rms_norm opinfo device kwargs make_arg = partial make_tensor device=device dtype=torch float requires_grad=False input_shape = err_msg = Expected normalized_shape least -dimensional s = SampleInput make_arg input_shape args= None e- yield ErrorInput s error_regex=err_msg normalized_shape = weight = make_arg err_msg = Expected weight same shape normalized_shape s = SampleInput make_arg input_shape args= normalized_shape weight e- yield ErrorInput s error_regex=err_msg err_msg = Given normalized_shape= s = SampleInput make_arg args= None e- yield ErrorInput s error_regex=err_msg sample_inputs_local_response_norm opinfo device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Ordered input shape size kwarg dict alpha beta k cases tuple tuple int tuple int dict = type ignore assignment alpha e- beta k beta k alpha e- k alpha e- beta alpha e- beta k alpha e- beta k alpha e- beta k alpha e- beta k input_shape size kwargs cases yield SampleInput make_arg input_shape args= size kwargs=kwargs sample_inputs_hardswish device dtype requires_grad kwargs N = make sure we testing - - range default - - so maybe unnecessary make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad low=- high= SampleInput make_arg N N _ range N sample_inputs_linear device dtype requires_grad kwargs features_options = batch_options list list int = no batch create_tensor = partial make_tensor device=device dtype=dtype requires_grad=requires_grad low=- high= has_bias in_feat out_feat batch_shape \ itertools product True False features_options batch_options input_tensor = create_tensor batch_shape + in_feat weight = create_tensor out_feat in_feat has_bias yield SampleInput input_tensor weight continue bias = create_tensor out_feat yield SampleInput input_tensor weight bias D tensor used crash MPS see https github com pytorch pytorch issues yield SampleInput create_tensor create_tensor yield SampleInput create_tensor create_tensor create_tensor sample_inputs_bilinear device dtype requires_grad kwargs features_options = batch_options list list int = no batch create_tensor = partial make_tensor device=device dtype=dtype requires_grad=requires_grad low=- high= has_bias in_feat in_feat out_feat batch_shape \ itertools product True False features_options batch_options input_tensor = create_tensor batch_shape + in_feat input_tensor = create_tensor batch_shape + in_feat weight = create_tensor out_feat in_feat in_feat has_bias yield SampleInput input_tensor input_tensor weight continue bias = create_tensor out_feat yield SampleInput input_tensor input_tensor weight bias sample_inputs_glu device dtype requires_grad kwargs features_options = batch_options list list int = no batch create_tensor = partial make_tensor device=device dtype=dtype requires_grad=requires_grad low=- high= features batch_shape itertools product features_options batch_options ndim = len features + len batch_shape dim range ndim input_tensor = create_tensor batch_shape + features dim_size = input_tensor size dim dim_size dim_size == yield SampleInput input_tensor dim sample_inputs_interpolate mode device dtype requires_grad kwargs N C = D = S = L = align_corners_options tuple Any = None mode linear bilinear bicubic trilinear align_corners_options = True False None ranks_for_mode = nearest nearest-exact linear bilinear bicubic trilinear area shape size rank with_batch_channel=True with_batch_channel tuple N C + size rank tuple size rank uneven_shape size rank with_batch_channel=True rc = list shape size rank with_batch_channel rc - += rank rc - -= tuple rc mode bilinear bicubic dtype == torch uint make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad we pick more realistic upper bound instead default uint dtype high= dtype == torch uint None provide few samples more close typical image processing usage rank = memory_format torch contiguous_format torch channels_last yield SampleInput make_arg shape rank memory_format=memory_format shape rank False scale_factor=None mode=mode align_corners=False make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad align_corners align_corners_options rank ranks_for_mode mode yield SampleInput make_arg shape D rank shape S rank False scale_factor=None mode=mode align_corners=align_corners yield SampleInput make_arg shape D rank shape L rank False scale_factor=None mode=mode align_corners=align_corners rank dtype is_floating_point yield SampleInput make_arg uneven_shape D rank uneven_shape S rank False scale_factor=None mode=mode align_corners=align_corners yield SampleInput make_arg uneven_shape D rank uneven_shape L rank False scale_factor=None mode=mode align_corners=align_corners recompute_scale_factor False True scale_factor yield SampleInput make_arg shape D rank size=None scale_factor=scale_factor mode=mode align_corners=align_corners recompute_scale_factor=recompute_scale_factor reference_inputs_interpolate mode device dtype requires_grad kwargs yield sample_inputs_interpolate mode device dtype requires_grad kwargs mode bilinear bicubic make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad we pick more realistic upper bound instead default uint dtype high= dtype == torch uint None provide few samples more typical image processing usage memory_format torch contiguous_format torch channels_last aa True False yield SampleInput make_arg memory_format=memory_format scale_factor=None mode=mode align_corners=False antialias=aa sample_inputs_upsample mode device dtype requires_grad kwargs N C = D = S = L = ranks_for_mode = nearest bilinear shape size rank with_batch_channel=True with_batch_channel torch Size N C + size rank torch Size size rank make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad rank ranks_for_mode mode yield SampleInput make_arg shape D rank size=shape S rank False yield SampleInput make_arg shape D rank size=shape L rank False yield SampleInput make_arg shape D rank scale_factor= yield SampleInput make_arg shape D rank scale_factor= reference_inputs_upsample mode device dtype requires_grad kwargs yield sample_inputs_upsample mode device dtype requires_grad kwargs mode == bilinear make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad we pick more realistic upper bound instead default uint dtype high= dtype == torch uint None provide single sample more typical image processing usage memory_format torch contiguous_format torch channels_last yield SampleInput make_arg memory_format=memory_format sample_inputs_upsample_aa mode device dtype requires_grad kwargs N = C = H = W = S = L = input_tensor = make_tensor torch Size N C H W device=device dtype=dtype requires_grad=requires_grad yield SampleInput input_tensor output_size=torch Size S S align_corners=False scale_factors=None yield SampleInput input_tensor output_size=torch Size L L align_corners=False scale_factors=None yield SampleInput input_tensor output_size=None align_corners=False scale_factors= yield SampleInput input_tensor output_size=None align_corners=True scale_factors= yield SampleInput input_tensor output_size=torch Size S S align_corners=False scales_h=None scales_w=None yield SampleInput input_tensor output_size=torch Size S S align_corners=False scales_h= scales_w= yield SampleInput input_tensor output_size=torch Size S S align_corners=True scales_h= scales_w= sample_inputs_gelu device dtype requires_grad kwargs N = _ range N approximate none tanh yield SampleInput make_tensor N N device=device dtype=dtype requires_grad=requires_grad low=- high= approximate=approximate error_inputs_gelu op device kwargs Tests gelu errors out when passed approximation we don t know yield ErrorInput SampleInput make_tensor dtype=torch float device=device kwargs= approximate asdf error_regex= approximate argument must either sample_inputs_max_min_reduction_with_dim op_info device dtype requires_grad kwargs args_for_reduction_with_dim = S S S S S S True True SampleInput make_tensor input_tensor dtype=dtype device=device low=None high=None requires_grad=requires_grad args input_tensor args args_for_reduction_with_dim sample_inputs_max_min_reduction_no_dim op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad low=None high=None yield SampleInput make_arg S S S yield SampleInput make_arg _generate_nan_reduction_inputs device dtype requires_grad kwargs yield _generate_reduction_inputs device dtype requires_grad NaN only exists floating point numbers dtype is_complex dtype is_floating_point yield torch tensor torch nan - device=device dtype=dtype requires_grad=requires_grad yield torch tensor torch nan device=device dtype=dtype requires_grad=requires_grad sample_inputs_nan_reduction supports_multiple_dims Generates sample inputs reduction ops contain input tensor dim keepdim kwargs If reduction op needs test additional args kwargs then create separate sample_inputs function fn op_info device dtype requires_grad kwargs t _generate_nan_reduction_inputs device dtype requires_grad Add case without dim keepdim kwargs yield SampleInput t clone requires_grad_ requires_grad kwargs _generate_reduction_kwargs t ndim supports_multiple_dims yield SampleInput t clone requires_grad_ requires_grad kwargs fn sample_inputs_reduction_quantile op_info device dtype requires_grad kwargs test_quantiles = make_tensor dtype=dtype device=device low= high= requires_grad=requires_grad test_interpolations = linear midpoint quantiles test_quantiles t _generate_reduction_inputs device dtype requires_grad Add case without dim keepdim kwargs input = t clone requires_grad_ requires_grad yield SampleInput input quantiles kwargs _generate_reduction_kwargs t ndim supports_multiple_dims=False Interpolation kwarg now only supported when providing both dim keepdim kwargs setdefault dim kwargs setdefault keepdim False interpolation test_interpolations kwargs interpolation = interpolation input = t clone requires_grad_ requires_grad yield SampleInput input quantiles kwargs sample_inputs_reduction_count_nonzero args kwargs Sample inputs count_nonzero count_nonzero does support keepdim yet sample sample_inputs_reduction args kwargs sample kwargs pop keepdim None yield sample sample_inputs_leaky_relu op_info device dtype requires_grad kwargs N = make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad SampleInput make_arg N N _ range N sample_inputs_fractional_max_pool d op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Order input_shape kernel_size cases = input_shape kernel_size cases return_indices False True test case passing single output size yield SampleInput make_arg input_shape kernel_size output_size= return_indices=return_indices test case passing tuple output size yield SampleInput make_arg input_shape kernel_size output_size= return_indices=return_indices test case passing output ratio yield SampleInput make_arg input_shape kernel_size output_ratio= return_indices=return_indices yield SampleInput make_arg output_ratio= return_indices=True _random_samples=make_tensor device=device dtype=dtype requires_grad=False sample_inputs_fractional_max_pool d op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Order input_shape kernel_size cases = input_shape kernel_size cases return_indices False True test case passing single output size yield SampleInput make_arg input_shape kernel_size output_size= return_indices=return_indices test case passing tuple output size yield SampleInput make_arg input_shape kernel_size output_size= return_indices=return_indices test case passing output ratio yield SampleInput make_arg input_shape kernel_size output_ratio= return_indices=return_indices sample_inputs_avgpool d op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Order input_shape kernel_size stride padding ceil_mode count_include_pad divisor_override cases = True False True False True True True False False True - True True None input_shape kernel_size stride padding ceil_mode count_include_pad divisor_override cases yield SampleInput make_arg input_shape args= kernel_size stride padding ceil_mode count_include_pad divisor_override Case just input_shape kernel_size yield SampleInput make_arg args= sample_inputs_avgpool d op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Order input_shape kernel_size kwargs cases list tuple tuple int Union int tuple int dict = dict stride= padding= ceil_mode=True count_include_pad=False dict stride= padding= ceil_mode=True count_include_pad=True dict stride= padding= ceil_mode=False count_include_pad=True dict stride= padding= ceil_mode=False count_include_pad=True dict stride= padding= ceil_mode=False dict stride= padding= ceil_mode=True dict stride= ceil_mode=False dict stride= ceil_mode=True input_shape kernel_size kwargs cases yield SampleInput make_arg input_shape args= kernel_size kwargs=kwargs sample_inputs_avgpool d op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Order input_shape kernel_size stride padding ceil_mode count_include_pad divisor_override cases list tuple tuple int Union int tuple int dict = dict stride= padding= ceil_mode=True count_include_pad=False divisor_override= dict stride= padding= ceil_mode=True count_include_pad=True divisor_override= dict stride= padding= ceil_mode=False dict stride= padding= ceil_mode=False count_include_pad=False divisor_override= dict stride= padding= ceil_mode=False count_include_pad=True divisor_override=- dict stride= padding= ceil_mode=True count_include_pad=True divisor_override=None dict stride= padding= ceil_mode=False count_include_pad=True divisor_override=None input_shape kernel_size kwargs cases yield SampleInput make_arg input_shape args= kernel_size kwargs=kwargs error_inputs_avg_pool d op_info device kwargs error inputs when pad negative x = torch rand dtype=torch float yield ErrorInput SampleInput x kwargs= kernel_size stride padding - error_regex= pad must non-negative error inputs when pad kernel_size yield ErrorInput SampleInput x kwargs= kernel_size stride padding error_regex= pad should most half effective kernel size error_inputs_avg_pool d op_info device kwargs error inputs when pad negative x = torch rand dtype=torch float yield ErrorInput SampleInput x kwargs= kernel_size stride padding - error_regex= pad must non-negative -dimensional kernel yield ErrorInput SampleInput x kwargs= kernel_size stride padding - error_regex= pad must non-negative error inputs when pad kernel_size yield ErrorInput SampleInput x kwargs= kernel_size stride padding error_regex= pad should most half effective kernel size -dimensional kernel yield ErrorInput SampleInput x kwargs= kernel_size stride padding error_regex= pad should most half effective kernel size error inputs zero divisor x = torch zeros yield ErrorInput SampleInput x kwargs= kernel_size divisor_override error_regex= divisor must zero error_inputs_avg_pool d op_info device kwargs error inputs when pad negative x = torch rand dtype=torch float yield ErrorInput SampleInput x kwargs= kernel_size stride padding - error_regex= pad must non-negative -dimensional kernel yield ErrorInput SampleInput x kwargs= kernel_size stride padding - error_regex= pad must non-negative error inputs when pad kernel_size yield ErrorInput SampleInput x kwargs= kernel_size stride padding error_regex= pad should most half effective kernel size -dimensional kernel yield ErrorInput SampleInput x kwargs= kernel_size stride padding error_regex= pad should most half effective kernel size error inputs zero divisor x = torch zeros yield ErrorInput SampleInput x kwargs= kernel_size divisor_override error_regex= divisor must zero error inputs invalid input dimension x = torch rand dtype=torch float yield ErrorInput SampleInput x kwargs= kernel_size stride padding error_regex= non-empty D D sample_inputs_to op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad test_multiple_devices_to_cuda would fail we use different device than given devices = device torch device device type == cpu devices = torch device cpu torch device cuda torch cuda is_available devices memory_formats = torch preserve_format torch channels_last TODO can t switch ` device ` overload use positional arguments https github com pytorch pytorch issues device overload device nb cp mem_f product devices True False True False memory_formats kwargs = memory_format mem_f yield SampleInput make_arg S S S S args= device torch float nb cp kwargs=kwargs dtype overload nb cp mem_f product True False True False memory_formats kwargs = memory_format mem_f yield SampleInput make_arg S S S S args= torch float nb cp kwargs=kwargs other overload device nb cp mem_f product devices True False True False memory_formats kwargs = memory_format mem_f other = make_arg S S S S dtype=torch float device=device yield SampleInput make_arg S S S S args= other nb cp kwargs=kwargs sample_inputs_topk op_info device dtype requires_grad kwargs get_tensor_input size make_tensor size dtype=dtype device=device requires_grad=requires_grad yield SampleInput get_tensor_input S M S yield SampleInput get_tensor_input S M S yield SampleInput get_tensor_input S M S - yield SampleInput get_tensor_input S M S True yield SampleInput get_tensor_input S M S - True yield SampleInput get_tensor_input S M S True True yield SampleInput get_tensor_input S M S - True True yield SampleInput get_tensor_input yield SampleInput get_tensor_input yield SampleInput get_tensor_input - yield SampleInput get_tensor_input True yield SampleInput get_tensor_input - True yield SampleInput get_tensor_input True True yield SampleInput get_tensor_input - True True sample_inputs_outer op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_arg S make_arg M sample_inputs_dist op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad sizes = S S S S S S S S ps = size_x size_y p product sizes sizes ps yield SampleInput make_arg size_x args= make_arg size_y p Missing test nondeterminism operation https github com pytorch pytorch issues sample_inputs_index op_info device dtype requires_grad reference=False kwargs target index_add dim idx source alpha= add = index_add op_info name target index_copy dim idx source copy = index_copy op_info name target index_fill dim idx value fill = index_fill op_info name Extended reference inputs We generate exercise atomic adds writing several times one location reference make_arg = partial torch ones device=device dtype=dtype requires_grad=requires_grad make_idx = partial torch zeros device=device dtype=torch int make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad idx They need different copy add deterministic copy add make_idx = partial torch randperm device=device dtype=torch int make_idx n make_tensor n device=device dtype=torch int low= high=n shapes = S S extra parameter add add dtype == torch bool alphas = True False alphas = - alphas = None fill A weird number catch errors The former one tests ` index_fill int_Scalar ` latter one tests ` index_fill int_Tensor ` values = make_arg item make_arg values = None shape alpha value product shapes alphas values t = make_arg shape args = dim We handle scalar case dim = - t ndim == args append dim idx = make_idx t shape dim t ndim = args append idx source copy add args append make_arg shape fill args append value args = tuple args kwargs = alpha None alpha alpha yield SampleInput t args=args kwargs=kwargs sample_inputs_index_reduce op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_idx n m make_tensor n device=device dtype=torch int low= high=m shapes = S S S M S S S S M S include_selfs = True False reduce = op_info variant_test_name assert reduce prod mean amin amax shape include_self product shapes include_selfs self_shape src_shape = shape dim We handle scalar case dim = len self_shape = idx = make_idx src_shape dim len src_shape = self_shape dim len self_shape = args = dim idx make_arg src_shape reduce yield SampleInput make_arg self_shape args=args kwargs= include_self include_self Sample inputs test edge cases backward requires_grad reduce == prod Check gradients propagated correctly prod when zeros src reduced This sample tests gradients following cases zero reduced source b zeros reduced src c no zeros reduced d zeros reduced both src tested test test_autograd py test_scatter_index_reduce_prod_gradgrad_error case supported gradgrad input = torch tensor dtype=dtype device=device requires_grad=requires_grad src = torch tensor dtype=dtype device=device requires_grad=requires_grad idx = torch tensor dtype=torch long device=device yield SampleInput input args= idx src reduce kwargs= include_self True sample_inputs__unsafe_masked_index op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_idx n m dim d view_shape = dim view_shape d = n make_tensor n device=device dtype=torch int low= high=m view view_shape cases = S S S M S S M S S S S S M fill_value = make_tensor dtype=dtype device= cpu item c cases self_shape high idx_size = c dim = len self_shape indices = make_idx idx_size high dim d d range dim masks = torch logical_and idx = idx self_shape i i idx enumerate indices idx None mask = functools reduce torch logical_and masks yield SampleInput make_arg self_shape mask indices fill_value masks = torch logical_and idx = idx self_shape i - i idx enumerate indices idx None mask = functools reduce torch logical_and masks yield SampleInput make_arg self_shape mask indices fill_value sample_inputs__unsafe_masked_index_put_accumulate op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_idx n m dim d view_shape = dim view_shape d = n make_tensor n device=device dtype=torch int low= high=m view view_shape cases = S S S M M S S M S S + S S S S M M - M + c cases self_shape high idx_sizes = c dim = len self_shape indices = make_idx idx_sizes d high dim d d range dim masks = torch logical_and idx = idx self_shape i i idx enumerate indices idx None mask = functools reduce torch logical_and masks values = make_arg idx_sizes yield SampleInput make_arg self_shape mask indices values masks = torch logical_and idx = idx self_shape i - i idx enumerate indices idx None mask = functools reduce torch logical_and masks yield SampleInput make_arg self_shape mask indices values sample_inputs_mode op_info device dtype requires_grad kwargs args = S S S S S S S S S True True Non-fused mode kernel CUDA make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad low=None high=None SampleInput make_arg input_tensor args input_tensor args args Missing test nondeterminism operation https github com pytorch pytorch issues sample_inputs_put op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad make_idx = partial make_tensor low= dtype=torch int device=device requires_grad=False S = Generic inputs idx = torch randperm S S device=device dtype=torch int S idx_list = idx -idx - idx acc product idx_list True False yield SampleInput input=make_arg S S args= idx clone make_arg S acc Scalar cases scalar_sizes = tgt_gen = make_arg size size scalar_sizes idx_gen = make_idx size high= size scalar_sizes src_gen = make_arg size size scalar_sizes tgt idx src acc product tgt_gen idx_gen src_gen True False yield SampleInput input=tgt clone requires_grad_ requires_grad args= idx clone src clone requires_grad_ requires_grad acc Empty cases tgt_sizes = tgt_gen = make_arg size size tgt_sizes idx = make_idx high= src = make_arg tgt acc product tgt_gen True False yield SampleInput input=tgt clone requires_grad_ requires_grad args= idx clone src clone requires_grad_ requires_grad acc sample_inputs_take op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad make_idx = partial make_tensor low= dtype=torch int device=device requires_grad=False S = Generic inputs take S elements out S S index = make_idx S high= S S idx index -index - yield SampleInput input=make_arg S S args= idx Scalar cases scalar_sizes = src_gen = make_arg size size scalar_sizes idx_gen = make_idx size high= size scalar_sizes src idx product src_gen idx_gen yield SampleInput input=src clone requires_grad_ requires_grad args= idx clone Empty cases src_sizes = src_gen = make_arg size size src_sizes idx = make_idx high= src src_gen yield SampleInput input=src clone requires_grad_ requires_grad args= idx clone sample_movedim_moveaxis op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device low=None high=None requires_grad=requires_grad yield SampleInput make_arg yield SampleInput make_arg - - - - - - - reference_movedim_moveaxis op_info device dtype requires_grad kwargs yield sample_movedim_moveaxis op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad shape source destination args = empty inputs int inputs negative - swap bounds - - non-sequential negative - - idempotence negative - - reverse sequential positive reverse non-sequential - - - - - reverse sequential negative - - - - - - shape source destination args yield SampleInput make_arg shape args= source destination error_movedim_moveaxis op_info device kwargs make_arg = partial make_tensor device=device dtype=torch float source length destination length yield ErrorInput SampleInput make_arg args= - - error_regex= r movedim Invalid source destination dims source r \ \ - \ dims\ should contain same number r dims destination \ \ - \ dims\ source length destination length yield ErrorInput SampleInput make_arg args= - error_regex= r movedim Invalid source destination dims source r \ \ - \ dims\ should contain same number r dims destination \ \ \ dims\ repeated source dim negative indices yield ErrorInput SampleInput make_arg args= - error_regex=r movedim repeated dim ` source ` \ \ - \ \ repeated destination dim negative indices yield ErrorInput SampleInput make_arg args= - error_regex=r movedim repeated dim ` destination ` \ \ - \ \ repeated dim both negative indices yield ErrorInput SampleInput make_arg args= - - error_regex=r movedim repeated dim ` source ` \ \ - \ \ out bounds source inputs negative indices yield ErrorInput SampleInput make_arg args= - error_regex=r Dimension out range \ expected range \ - \ got - \ error_type=IndexError out bounds destination inputs negative indices yield ErrorInput SampleInput make_arg args= - error_regex=r Dimension out range \ expected range \ - \ got - \ error_type=IndexError out bounds source input int yield ErrorInput SampleInput make_arg args= - error_regex=r Dimension out range \ expected range \ - \ got - \ error_type=IndexError out bounds destination input int yield ErrorInput SampleInput make_arg args= - error_regex=r Dimension out range \ expected range \ - \ got - \ error_type=IndexError sample_repeat_tile op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad rep_dims = shapes = requires_grad Tests variant_consistency_jit grad gradgrad slower Use smaller bags ` rep_dims ` ` shapes ` case rep_dims = type ignore assignment shapes = type ignore assignment is_repeat_op = op_info name repeat _refs repeat rep_dim shape product rep_dims shapes ` torch repeat ` errors ` len rep_dims t dim ` so we filter such combinations is_repeat_op len rep_dim len shape continue yield SampleInput make_arg shape rep_dim sample_inputs_narrow_narrow_copy op_info device dtype requires_grad is_narrow kwargs shapes_and_args = S S S S S S - S S S S S S - S S S shape dim start length shapes_and_args tensor = make_tensor shape dtype=dtype device=device low=None high=None requires_grad=requires_grad yield SampleInput tensor dim start length narrow also accepts start argument being Tensor is_narrow yield SampleInput tensor dim torch tensor start length reference_inputs_narrow_narrow_copy op_info device dtype requires_grad is_narrow kwargs yield sample_inputs_narrow_narrow_copy op_info device dtype requires_grad is_narrow=is_narrow kwargs shapes_and_args = -dim M elems left M - - elems right M elems left M - elems right M - M M elems left M -M M M elems right -dim M S dim elems left S M - - dim elems right L S dim elems left L S - dim elems left M L M dim M elems left M L - -L L dim L elems right -dim L M S dim elems left M S L - - dim elems right S L M M dim M elems left L S M - -M M dim M elems right S L M dim elems left S L M dim elem left M S M - - dim elems right shape dim start length shapes_and_args tensor = make_tensor shape dtype=dtype device=device low=None high=None requires_grad=requires_grad yield SampleInput tensor dim start length narrow also accepts start argument being Tensor is_narrow yield SampleInput tensor dim torch tensor start length error_inputs_narrow_narrow_copy op_info device is_narrow is_ref make_arg = partial make_tensor device=device dtype=torch float -dim yield ErrorInput SampleInput make_arg error_type=RuntimeError error_regex=r narrow\ \ cannot applied -dim tensor\ out bounds dim is_narrow is_ref torch device device type == cpu narrow_copy_dense_cpu_out yield ErrorInput SampleInput make_arg M S L error_type=RuntimeError error_regex=r Expected dim static_cast int _t \ self_sizes size\ \ \ true got false\ yield ErrorInput SampleInput make_arg M S L error_type=IndexError error_regex=r Dimension out range \ expected range \ - \ got \ out bounds dim negative yield ErrorInput SampleInput make_arg L S M - error_type=IndexError error_regex=r Dimension out range \ expected range \ - \ got - \ out bounds start yield ErrorInput SampleInput make_arg L M S M + error_type=IndexError error_regex=r start out range \ expected range \ - \ got \ out bounds start negative yield ErrorInput SampleInput make_arg L M S -M - error_type=IndexError error_regex=r start out range \ expected range \ - \ got - \ out bounds length yield ErrorInput SampleInput make_arg S L M M + error_type=RuntimeError error_regex=r start \ \ \+ length \ \ exceeds dimension size \ \ \ out bounds length negative is_narrow is_ref torch device device type == cpu narrow_copy_dense_cpu_out yield ErrorInput SampleInput make_arg M - error_type=RuntimeError error_regex=r start \ \ \+ length \ - \ exceeds dimension size \ \ \ yield ErrorInput SampleInput make_arg M - error_type=RuntimeError error_regex=r narrow\ \ length must non-negative\ Test Tensor overload added XLA Start must -dim integral Tensor narrow_copy doesn t have overload https github com pytorch pytorch issues is_narrow -dim integral Tensor yield ErrorInput SampleInput make_arg L M S make_arg S dtype=torch int error_type=RuntimeError error_regex=r start must -dim integral Tensor\ -dim bool Tensor bools allowed yield ErrorInput SampleInput make_arg L M S - make_arg dtype=torch bool error_type=RuntimeError error_regex=r start must -dim integral Tensor\ sample_trapezoid op_info device dtype requires_grad kwargs y_shape_x_shape_and_kwargs = dim None When trapezoid called empty input does produce output requires_grad See Issue dim - None dx None dx make_arg = partial make_tensor dtype=dtype device=device low=None high=None requires_grad=requires_grad y_shape x_shape kwarg y_shape_x_shape_and_kwargs y_tensor = make_arg y_shape x_shape None x_tensor = make_arg x_shape yield SampleInput y_tensor x_tensor kwarg yield SampleInput y_tensor kwarg sample_cumulative_trapezoid op_info device dtype requires_grad kwargs y_shape_x_shape_and_kwargs = dim None When cumulative_trapezoid called empty input does produce output requires_grad See Issue dim - None dx None dx make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad low=None high=None y_shape x_shape kwarg y_shape_x_shape_and_kwargs y_tensor = make_arg y_shape x_shape None x_tensor = make_arg x_shape yield SampleInput y_tensor x_tensor kwarg yield SampleInput y_tensor kwarg sample_unsqueeze op_info device dtype requires_grad kwargs shapes_and_axes = - - - - shape axis shapes_and_axes tensor = make_tensor shape dtype=dtype device=device low=None high=None requires_grad=requires_grad yield SampleInput tensor axis sample_inputs_nn_unfold op_info device dtype requires_grad kwargs shapes = kernel_sizes = dilations = paddings = strides = cases = product shapes kernel_sizes dilations paddings strides make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad shape kernel_size dilation padding stride cases tensor = make_arg shape yield SampleInput tensor kernel_size dilation padding stride With default args yield SampleInput make_arg sample_inputs_squeeze op_info device dtype requires_grad kwargs shapes_and_args = S S S S S S - S S S S - shape args shapes_and_args tensor = make_tensor shape dtype=dtype device=device low=None high=None requires_grad=requires_grad yield SampleInput tensor args=args sample_inputs_squeeze_multiple op_info device dtype requires_grad kwargs shapes_and_args = S S S S - S S S S shape dims shapes_and_args tensor = make_tensor shape dtype=dtype device=device low=None high=None requires_grad=requires_grad yield SampleInput tensor dims _squeeze_ref x axis=None NumPy doesn t allow squeezing scalars x ndim == x isinstance axis Sequence Numpy doesn t allow specifying non-singular dimensions axis = tuple axis x shape == isinstance axis int x shape axis = x np squeeze x axis sample_inputs_nn_pad op_info device dtype requires_grad mode kwargs assert mode constant reflect replicate circular mode reflect replicate cases tuple = ignore - - mode == constant cases = - - - - mode == circular dtype == torch bool test_dtypes fails ASAN case ab runtime error load value which valid value type bool Reference https github com pytorch pytorch pull #issuecomment- Reference Issue https github com pytorch pytorch issues cases = cases = - - make_inp = partial make_tensor device=device dtype=dtype requires_grad=requires_grad mode == constant Default args yield SampleInput make_inp args= mode reflect replicate circular shape pad cases yield SampleInput make_inp shape args= pad mode mode == constant pad_value shape pad cases yield SampleInput make_inp shape args= pad mode pad_value sample_inputs_nn_pad_replicate_negative op_info device dtype requires_grad kwargs cases tuple = - - - - - - - - - - - - - make_inp = partial make_tensor device=device dtype=dtype requires_grad=requires_grad shape pad cases yield SampleInput make_inp shape args= pad replicate sample_inputs_constant_pad_nd op_info device dtype args kwargs Inherit sample inputs nn pad transform them fit constant_pad_nd s interface nn_samples = sample_inputs_nn_pad op_info device dtype args mode= constant kwargs NOTE primTorch more strict about type fill value argument So we must cast correct dtype torch _prims_common dtype_to_type scalar_type = dtype_to_type dtype drop_mode_argument input pad mode=None value=None value None SampleInput input args= pad SampleInput input args= pad scalar_type value sample nn_samples yield drop_mode_argument sample input sample args sample kwargs sample_inputs_repeat_interleave op_info device dtype requires_grad kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_input repeats= yield SampleInput make_input repeats= yield SampleInput make_input repeats= dim= yield SampleInput make_input repeats=torch arange device=device dim= yield SampleInput make_input repeats=torch arange device=device dim= output_size= sample_inputs_stft op_info device dtype requires_grad kwargs mt shape kwargs make_tensor shape device=device dtype=dtype requires_grad=requires_grad kwargs yield SampleInput mt n_fft= return_complex=True yield SampleInput mt n_fft= return_complex=False dtype is_complex yield SampleInput mt n_fft= center False True yield SampleInput mt n_fft= center=center return_complex=True yield SampleInput mt n_fft= hop_length= center=center return_complex=True window = mt low= high= yield SampleInput mt kwargs=dict n_fft= window=window return_complex=True center=center yield SampleInput mt kwargs=dict n_fft= window=window return_complex=True center=center dtype is_complex yield SampleInput mt n_fft= window=window onesided=False return_complex=True sample_inputs_istft op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad mt shape kwargs real_shape = shape dtype is_complex shape + make_arg real_shape kwargs yield SampleInput mt kwargs=dict n_fft= yield SampleInput mt kwargs=dict n_fft= onesided=False yield SampleInput mt kwargs=dict n_fft= onesided=True center False True yield SampleInput mt kwargs=dict n_fft= center=center yield SampleInput mt kwargs=dict n_fft= hop_length= center=center window = make_arg low= high= yield SampleInput mt kwargs=dict n_fft= window=window center=center return_complex=dtype is_complex yield SampleInput mt kwargs=dict n_fft= window=window win_length= center=center return_complex=True real_window = window dtype is_complex window real yield SampleInput mt kwargs=dict n_fft= window=real_window center=center sample_inputs_ormqr op_info device dtype requires_grad kwargs create helper function wrapping ` make_tensor ` make_input = partial make_tensor dtype=dtype device=device low=- high= batches = ns = tf = True False batch m n left transpose product batches product ns ns tf tf input = make_input batch m n reflectors tau = torch geqrf input reflectors requires_grad_ requires_grad tau requires_grad_ requires_grad other_matrix_shape = m n left n m other = make_input batch other_matrix_shape requires_grad=requires_grad yield SampleInput reflectors tau other left=left transpose=transpose sample_inputs_cholesky_solve op_info device dtype requires_grad=False kwargs cholesky_inverse_samples = sample_inputs_linalg_cholesky_inverse op_info device dtype requires_grad=False sample cholesky_inverse_samples psd_matrix = sample input sample input = make_tensor psd_matrix shape dtype=dtype device=device requires_grad=requires_grad low=None high=None sample args = psd_matrix requires_grad_ requires_grad yield sample sample_inputs_lu op_info device dtype requires_grad=False kwargs make_arg = partial make_fullrank_matrices_with_distinct_singular_values dtype=dtype device=device requires_grad=requires_grad needed once OpInfo tests support Iterables batch_shapes = batch_shape get_infos size_delta product batch_shapes True False - - + + shape = batch_shape + S + size_delta S input = make_arg shape yield SampleInput input args= True get_infos sample_inputs_lu_unpack op_info device dtype requires_grad=False kwargs out_fn output output output lu_sample sample_inputs_linalg_lu op_info device dtype requires_grad kwargs lu_data pivots = torch linalg lu_factor lu_sample input lu_data requires_grad_ requires_grad yield SampleInput lu_data pivots with_metadata output_process_fn_grad=out_fn sample_inputs_roll op_info device dtype requires_grad=False kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad args = - - arg args yield SampleInput make_arg args=arg yield SampleInput make_arg S S S args=arg Scalar tensor yield SampleInput make_arg args= error_inputs_roll op_info device kwargs make_arg = partial make_tensor device=device dtype=torch float err_msg = ` shifts ` required s = SampleInput make_arg S yield ErrorInput s error_regex=err_msg err_msg = shifts dimensions must align s = SampleInput make_arg S S yield ErrorInput s error_regex=err_msg err_msg = out range s = SampleInput make_arg S yield ErrorInput s error_regex=err_msg error_type=IndexError err_msg = Dimension specified s = SampleInput make_arg yield ErrorInput s error_regex=err_msg error_type=IndexError sample_inputs_rot op_info device dtype requires_grad=False kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad args = itertools product range - - yield SampleInput make_arg S S S arg args yield SampleInput make_arg S S S args=arg error_inputs_rot op_info device kwargs make_arg = partial make_tensor device=device dtype=torch float err_msg = expected total rotation dims s = SampleInput make_arg S S dims= yield ErrorInput s error_regex=err_msg err_msg = expected total dims = s = SampleInput make_arg S yield ErrorInput s error_regex=err_msg err_msg = expected rotation dims different s = SampleInput make_arg S S dims= yield ErrorInput s error_regex=err_msg sample_inputs_std_var op_info device dtype requires_grad kwargs tensor_nd = partial make_tensor S S S device=device dtype=dtype requires_grad=requires_grad tensor_ d = partial make_tensor S device=device dtype=dtype requires_grad=requires_grad yield SampleInput tensor_nd yield SampleInput tensor_nd dim= yield SampleInput tensor_nd dim= unbiased=True keepdim=True yield SampleInput tensor_ d dim= unbiased=True keepdim=True yield SampleInput tensor_ d dim= unbiased=False keepdim=False yield SampleInput tensor_nd dim= correction= yield SampleInput tensor_nd dim= correction=S yield SampleInput tensor_nd dim=None correction= keepdim=True yield SampleInput tensor_nd dim=None correction=None yield SampleInput tensor_nd correction= keepdim=True yield SampleInput make_tensor device=device dtype=dtype requires_grad=requires_grad dim=- sample_inputs_std_var_unbiased op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Test var_mean Tensor bool unbiased=True - Tensor Tensor yield SampleInput make_arg S S True yield SampleInput make_arg S False _generate_correlation_inputs device dtype requires_grad kwargs shapes = shape shapes yield make_tensor shape dtype=dtype device=device requires_grad=requires_grad sample_inputs_corrcoef op_info device dtype requires_grad kwargs SampleInput t t _generate_correlation_inputs device dtype requires_grad sample_inputs_copysign op_info device dtype requires_grad kwargs yield sample_inputs_elementwise_binary op_info device dtype requires_grad kwargs dtype is_floating_point yield SampleInput make_tensor dtype=dtype device=device requires_grad=requires_grad - sample_inputs_cov op_info device dtype requires_grad kwargs t _generate_correlation_inputs device dtype requires_grad yield SampleInput t num_observations = t numel t ndimension t size fweights = make_tensor num_observations dtype=torch int device=device low= high= aweights = make_tensor num_observations dtype=torch float device=device low= high= requires_grad=requires_grad correction fw aw product range num_observations None fweights None aweights yield SampleInput t clone requires_grad_ requires_grad correction=correction fweights=fw aweights=aw error_inputs_cov op_info device kwargs = torch rand S device=device yield ErrorInput SampleInput torch rand S S S device=device error_regex= expected input have two fewer dimensions yield ErrorInput SampleInput fweights=torch rand S S device=device error_regex= expected fweights have one fewer dimensions yield ErrorInput SampleInput aweights=torch rand S S device=device error_regex= expected aweights have one fewer dimensions yield ErrorInput SampleInput fweights=torch rand S device=device error_regex= expected fweights have integral dtype yield ErrorInput SampleInput aweights=torch tensor device=device error_regex= expected aweights have floating point dtype yield ErrorInput SampleInput fweights=torch tensor device=device error_regex= expected fweights have same numel yield ErrorInput SampleInput aweights=torch rand device=device error_regex= expected aweights have same numel yield ErrorInput SampleInput fweights=torch tensor - - - - - device=device error_regex= fweights cannot negative yield ErrorInput SampleInput aweights=torch tensor - - - - - device=device error_regex= aweights cannot negative sample_inputs_permute op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad cases = - - shape args cases yield SampleInput make_arg shape args= args reference_inputs_permute op device dtype requires_grad kwargs yield sample_inputs_permute op device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad cases = Adds tricky permutations permutations noncontiguity shape permutation cases p itertools permutations permutation = make_arg shape permute p yield SampleInput args= permutation = make_arg shape noncontiguous=True permute p yield SampleInput args= permutation error_inputs_softshrink op device kwargs yield ErrorInput SampleInput make_tensor dtype=torch float device=device kwargs= lambd - error_regex= lambda must greater equal found - sample_inputs_softshrink op_info device dtype requires_grad=False kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad The additional sample check additional values lambd beyond default value what already checked sample_inputs_elementwise_unary lbda yield SampleInput make_arg S S kwargs= lambd lbda yield sample_inputs_elementwise_unary op_info device dtype requires_grad sample_inputs_hardshrink op_info device dtype requires_grad=False kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad The additional sample check additional values lambd beyond default value what already checked sample_inputs_elementwise_unary Note unlike softshrink lambd allowed negative hardshrink lbda - yield SampleInput make_arg S S kwargs= lambd lbda yield sample_inputs_elementwise_unary op_info device dtype requires_grad sample_inputs_hardtanh op_info device dtype requires_grad=False kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad The additional sample check additional values min_val max_val beyond default value what already checked sample_inputs_elementwise_unary max_val min_val - yield SampleInput make_arg S S kwargs= min_val min_val max_val max_val yield sample_inputs_elementwise_unary op_info device dtype requires_grad error_inputs_hardtanh op_info device kwargs Tests hardtanh errors out when passed min_val max_val yield ErrorInput SampleInput make_tensor dtype=torch float device=device kwargs= min_val max_val - error_type=ValueError error_regex= min_val cannot greater than max_val sample_inputs_einsum op_info device dtype requires_grad=False kwargs c t t clone requires_grad_ requires_grad make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad x = make_arg y = make_arg A = make_arg B = make_arg C = make_arg D = make_arg E = make_arg H = make_arg I = make_arg Vector operations yield SampleInput c x i- sum yield SampleInput c x c y i j- ij outer Matrix operations yield SampleInput c A ij- i col sum yield SampleInput c A c B ij kj- ik matmul yield SampleInput c A c E ij Ab- ijAb matrix outer product Tensor operations yield SampleInput c C c D aij ajk- aik batch matmul yield SampleInput c D c E aij jk- aik tensor matrix contraction yield SampleInput c C c B ijk ik- j non contiguous Test diagonals yield SampleInput c I iji- j non-contiguous trace Test ellipsis yield SampleInput c H i - yield SampleInput c C c x ik j - ij sample_inputs_flip op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad sizes = S M S S M all_dims = - size dims product sizes all_dims yield SampleInput make_arg size kwargs= dims dims sample_inputs_fliplr_flipud op_info device dtype requires_grad kwargs shapes = S M S S M make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad SampleInput make_arg shape low=None high=None shape shapes error_inputs_fliplr op device kwargs yield ErrorInput SampleInput make_tensor dtype=torch float device=device error_regex= Input must = -d error_inputs_flipud op device kwargs yield ErrorInput SampleInput make_tensor dtype=torch float device=device error_regex= Input must = -d sample_inputs_clamp op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device low=None high=None requires_grad=requires_grad make_integral_arg = partial make_tensor dtype=torch int device=device low=None high=None requires_grad=False shape = S M S yield SampleInput make_arg shape args= make_arg shape make_arg shape yield SampleInput make_arg shape args= make_arg shape make_arg shape yield SampleInput make_arg shape args= make_arg S S yield SampleInput make_arg shape args= None make_arg shape yield SampleInput make_arg shape args= make_arg shape None test type promotion yield SampleInput make_arg shape args= make_integral_arg shape None yield SampleInput make_arg shape args= make_arg shape make_integral_arg shape reference_inputs_elementwise_ternary op device dtype requires_grad sample_inputs_func supports_scalars=False kwargs yield sample_inputs_func op device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_scalar_tensor = partial make_tensor device= cpu dtype=dtype requires_grad=requires_grad supported_dtypes = op supported_dtypes device broadcasting oncontiguous cases cases = b c cases yield SampleInput make_arg args= make_arg b make_arg c yield SampleInput make_arg noncontiguous=True args= make_arg b transpose - make_arg c noncontiguous=True transpose - scalar cases supports_scalars cases = make_scalar_tensor make_scalar_tensor torch complex supported_dtypes cases extend complex b c cases yield SampleInput make_arg args= b c type promotion cases int x float torch float supported_dtypes torch long supported_dtypes = make_arg dtype=torch long b = make_arg dtype=torch float c = make_arg cases = b c c b b c cases yield SampleInput args= b c NaN propagation dtype is_floating_point dtype is_complex nan = float nan dtype is_floating_point complex float nan float nan = make_arg = nan = nan b = make_arg b = nan b = nan c = make_arg c = nan yield SampleInput args= b c _clamp_min_numpy min=None np maximum min _clamp_max_numpy max=None np minimum max _clamp_numpy min=None max=None min None np minimum max max None np maximum min np minimum max np maximum min sample_inputs_cumprod op_info device dtype requires_grad kwargs make_arg shape shrink values interval - + better precision gradgradcheck make_tensor shape dtype=dtype device=device low=- high=+ requires_grad=requires_grad prod_zeros dim_select assert len dim_select == result = make_arg S result narrow dim_select narrow dim_select zero_ result narrow dim_select narrow dim_select zero_ result narrow dim_select narrow dim_select zero_ result dim range yield SampleInput make_arg S S S args= dim Scalar tensors empty tensor size yield SampleInput make_arg size args= yield SampleInput prod_zeros args= yield SampleInput prod_zeros args= yield SampleInput prod_zeros args= test dtype kwarg yield SampleInput prod_zeros args= kwargs= dtype dtype sample_inputs_view_as_complex op_info device dtype requires_grad kwargs yield SampleInput make_tensor S dtype=dtype device=device requires_grad=requires_grad sample_inputs_view_as_real op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad sizes = S S SampleInput make_arg size size sizes error_inputs_complex op_info device is_ref=False kwargs make_arg = partial make_tensor dtype=torch float device=device other_dtype = torch float device startswith mps torch float other_dtype_name = Half device startswith mps Double is_ref error_float = Expected both inputs Half Float Double tensors got torch float torch int error_dtype = Expected object scalar type torch float got scalar type torch float second argument error_out = Expected out tensor have dtype torch complex got torch complex instead error_float = Expected both inputs Half Float Double tensors got Float Int error_dtype = f Expected object scalar type Float got scalar type other_dtype_name second argument error_out = f Expected object scalar type Complex other_dtype_name got scalar type ComplexFloat argument out yield ErrorInput SampleInput make_arg M S make_arg M S dtype=torch int error_type=RuntimeError error_regex=error_float yield ErrorInput SampleInput make_arg M S make_arg M S dtype=other_dtype error_type=RuntimeError error_regex=error_dtype yield ErrorInput SampleInput make_arg M S dtype=other_dtype make_arg M S dtype=other_dtype out=make_arg M S dtype=torch complex error_type=RuntimeError error_regex=error_out sample_inputs_logaddexp op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad shape = S S yield SampleInput make_arg shape make_arg shape sample_inputs_prod op_info device dtype requires_grad kwargs make_arg shape shrink values interval - + better precision gradgradcheck make_tensor shape dtype=dtype device=device low=- high=+ requires_grad=requires_grad prod_single_zero result = make_arg S result = result sample sample_inputs_cumprod op_info device dtype requires_grad only Tensor ignore other inputs yield SampleInput sample input clone requires_grad_ requires_grad yield sample Generates samples keepdim = True sample sample_inputs_cumprod op_info device dtype requires_grad sample kwargs keepdim = True yield sample yield SampleInput prod_single_zero yield SampleInput make_arg args= yield SampleInput make_arg args= kwargs= keepdim True yield SampleInput make_arg args= yield SampleInput make_arg args= kwargs= keepdim True yield SampleInput torch tensor dtype=dtype device=device requires_grad=requires_grad test zero scalar tensor zero = make_arg zero zero_ yield SampleInput zero clone requires_grad_ requires_grad yield SampleInput zero clone requires_grad_ requires_grad args= yield SampleInput zero clone requires_grad_ requires_grad args= kwargs= keepdim True error_inputs_neg op_info device kwargs si = SampleInput torch tensor False True device=device msg = Negation ` \\- ` operator bool tensor supported If you trying invert mask use ` \\~ ` ` logical_not\\ \\ ` operator instead yield ErrorInput si error_regex=msg sample_inputs_diag op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad low=None high=None yield SampleInput make_arg M tensors = make_arg M M make_arg make_arg args = - tensor arg product tensors args yield SampleInput tensor clone requires_grad_ requires_grad arg reference_inputs_diagonal_diag_embed op_info device dtype requires_grad kwargs yield sample_inputs_diagonal_diag_embed op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad shapes d = shapes d = L M shapes d = L M S kwargs d = kwargs d = dim dim allowed dict dim = dim = negative dims allowed dict dim =- dim =- one dim negative other nonnegative allowed dict dim =- dim = out bounds offset should empty tensor diagonal offset diagonal diag_embed dict offset= kwargs d = kwargs d + make sure we can use non-sequential dims dict offset=- dim = dim = samples d = product shapes d kwargs d samples d = product shapes d kwargs d samples d = product shapes d kwargs d shape kwargs chain samples d samples d samples d diagonal op_info name these error inputs diagonal shape continue yield SampleInput input=make_arg shape kwargs=kwargs sample_inputs_diagonal_scatter op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad Shapes D Tensors shapes_ d = M M Shapes D Tensors shapes_ d = M M M args_ d = - args_ d = - input_shape arg chain product shapes_ d args_ d product shapes_ d args_ d input_ = make_arg input_shape We can programmatically figure out right shape src It should same size input diagonal other_args isinstance arg tuple arg_tuple = arg arg_tuple = arg src_shape = input_ diagonal arg_tuple size src = make_arg src_shape yield SampleInput input_ args= src arg_tuple sample_inputs_to_sparse op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_arg S S with_metadata output_process_fn_grad=lambda x x to_dense yield SampleInput make_arg S S with_metadata output_process_fn_grad=lambda x x to_dense sample_inputs_cross_entropy op_info device dtype requires_grad kwargs batch_size num_classes = shape = reductions = mean sum none input_shape_and_kwargs list tuple tuple int dict str Any = shape shape shape shape shape dict reduction=reduction reduction reductions shape dict weight=make_tensor num_classes device=device dtype=dtype reduction=reduction reduction reductions shape dict ignore_index= input_shape kwargs probabilities_target itertools product input_shape_and_kwargs False True input = make_tensor input_shape device=device dtype=dtype requires_grad=requires_grad probabilities_target ignore_index supported probabilities target ignore_index kwargs continue target = make_tensor input_shape low= high= device=device dtype=dtype requires_grad=requires_grad target = make_tensor batch_size input_shape low= high=num_classes device=device dtype=torch long ignore_index kwargs torch all target == kwargs ignore_index make sure least one item target ignored target = random sample sorted set range num_classes - kwargs ignore_index yield SampleInput input target kwargs sample_inputs_logit op_info device dtype requires_grad kwargs low high = op_info domain Note Operator very sensitive points near start end domain leads NaN float domain_eps e- dtype is_floating_point dtype is_complex domain_eps = op_info _domain_eps dtype = torch float e- low = low + domain_eps high = high - domain_eps make_arg = partial make_tensor dtype=dtype device=device low=low high=high requires_grad=requires_grad yield SampleInput make_arg S S S yield SampleInput make_arg S S S yield SampleInput make_arg yield SampleInput make_arg sample_inputs_isin op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad isin has two paths based size elements test_elements elements numel pow test_elements numel yield SampleInput make_arg L args= make_arg S yield SampleInput make_arg S args= make_arg L sample_inputs_masked_scatter op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_arg S S args= torch randn S S device=device make_arg S S yield SampleInput make_arg S S args= torch randn S device=device make_arg S S yield SampleInput make_arg S S args= bernoulli_scalar device make_arg S S yield SampleInput make_arg S args= torch randn S S device=device make_arg S S broadcasts_input=True error_inputs_masked_scatter op_info device kwargs make_arg = partial make_tensor device=device dtype=torch float mask_dtype torch float torch uint yield ErrorInput SampleInput make_arg args= torch ones device=device dtype=mask_dtype make_arg error_regex=r masked_scatter_ only supports boolean masks sample_inputs_masked_fill op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_arg S S args= torch randn S S device=device yield SampleInput make_arg S S args= torch randn S S device=device make_arg yield SampleInput make_arg S S args= torch randn S device=device yield SampleInput make_arg args= torch randn device=device yield SampleInput make_arg args= torch randn device=device make_arg yield SampleInput make_arg S S args= torch randn device=device yield SampleInput make_arg S args= torch randn S S device=device make_arg broadcasts_input=True yield SampleInput make_arg S args= torch randn S S device=device broadcasts_input=True torch device device type == cuda ` ` ` mask ` CUDA ` value ` CPU scalar tensor yield SampleInput make_arg S S args= torch randn S S device=device make_tensor device= cpu dtype=dtype error_inputs_masked_fill op_info device kwargs make_arg = partial make_tensor device=device dtype=torch float requires_grad=False ` value ` -D tensor yield ErrorInput SampleInput make_arg args= make_arg make_arg error_regex= only supports -dimensional value tensor got tensor dimension downcasting complex value scalar overload yield ErrorInput SampleInput make_arg args= make_arg j error_regex=r value cannot converted type without overflow downcasting complex value tensor overload yield ErrorInput SampleInput torch ones dtype=torch long device=device args= make_arg torch tensor j device=device error_regex=r value cannot converted type without overflow torch device device type == cuda ` ` ` mask ` CPU ` value ` CUDA scalar tensor yield ErrorInput SampleInput torch randn S S device= cpu args= torch randn S S device= cpu torch randn device= cuda error_regex=r same device sample_inputs_masked_select op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad low=None high=None yield SampleInput make_arg M M torch randn M M device=device yield SampleInput make_arg M M torch randn M device=device yield SampleInput make_arg M torch randn M M device=device yield SampleInput make_arg M M torch randn M M device=device yield SampleInput make_arg torch tensor device=device dtype=torch bool yield SampleInput make_arg M M torch tensor device=device dtype=torch bool yield SampleInput make_arg torch randn M M device=device sample_inputs_matrix_exp op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_arg S S yield SampleInput make_arg S S S sample_inputs_matmul op_info device dtype requires_grad is_rmatmul=False kwargs make_arg = partial make_tensor dtype=dtype device=device low=None high=None requires_grad=requires_grad test_cases = L L S M M M M S S M M S S M S S M M S S M M S S S S M S M S S M S M S S S S M M S S M S S S M M M M S S M S S S S S S lhs_shape rhs_shape test_cases lhs = make_arg lhs_shape rhs = make_arg rhs_shape is_rmatmul yield SampleInput lhs rhs yield SampleInput rhs lhs sample_inputs_meshgrid op_info OpInfo device torch device dtype torch dtype requires_grad bool variant str kwargs - list SampleInput variant == variadic make_inputs tensors list torch Tensor - tuple Union torch Tensor list torch Tensor tuple torch Tensor tensors variant == list make_inputs tensors list torch Tensor - tuple Union torch Tensor list torch Tensor tuple torch Tensor tensors raise ValueError Unsupported variant must one variadic list f Got variant SCALAR = torch Size VECTOR = torch Size test_cases list list torch Size = SCALAR VECTOR VECTOR SCALAR VECTOR SCALAR VECTOR VECTOR SCALAR VECTOR SCALAR shapes indexing itertools product test_cases xy ij args = make_inputs make_tensor shape dtype=dtype device=device requires_grad=requires_grad shape shapes yield SampleInput args indexing=indexing sample_inputs_mvlgamma op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad tensor_shapes = S S ns = Since accepted lower bound input mvlgamma depends ` p ` argument following function computes lower bound which we pass ` make_tensor ` compute_min_val p p - shape n product tensor_shapes ns min_val = compute_min_val n dtype is_floating_point Round-up minimum value integral dtypes min_val += min_val += torch finfo dtype eps yield SampleInput make_arg shape low=min_val args= n Since ` mvlgamma ` has multiple entries there multiple common skips additional entries Following function helper end skips_mvlgamma skip_redundant=False skips = outside domain values hard error mvlgamma op DecorateInfo unittest skip Skipped TestUnaryUfuncs test_float_domains DecorateInfo unittest expectedFailure TestUnaryUfuncs test_reference_numerics_extremal DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch float torch int DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_small dtypes= torch int skip_redundant Redundant tests skips = skips + type ignore assignment DecorateInfo unittest skip Skipped TestFwdGradients DecorateInfo unittest skip Skipped TestBwdGradients DecorateInfo unittest skip Skipped TestJit DecorateInfo unittest skip Skipped TestCommon skips To test reference numerics against multiple values argument ` p ` we make multiple OpInfo entries each entry corresponding different value p We run op tests test_ops py only ` p= ` avoid redundancy testing make_mvlgamma_opinfo variant_test_name domain skips sample_kwargs UnaryUfuncInfo mvlgamma ref=reference_mvlgamma TEST_SCIPY None aliases= special multigammaln variant_test_name=variant_test_name domain=domain decorators= precisionOverride torch float e- dtypes=all_types_and torch half torch bfloat dtypesIfCUDA=all_types_and torch float torch bfloat sample_inputs_func=sample_inputs_mvlgamma supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True skips=skips sample_kwargs=sample_kwargs sample_inputs_cumulative_ops op_info device dtype requires_grad supports_dtype_kwargs=True kwargs _make_tensor_helper shape low=None high=None make_tensor shape dtype=dtype device=device low=low high=high requires_grad=requires_grad yield SampleInput _make_tensor_helper S S S yield SampleInput _make_tensor_helper S S S yield SampleInput _make_tensor_helper supports_dtype_kwargs NOTE ` dtype ` same input then inplace variants fail ` provided dtype must match dtype tensor cumsum ` yield SampleInput _make_tensor_helper S S S dtype=dtype sample_inputs_unfold op_info device dtype requires_grad kwargs test_cases = S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S M M M S S S shape arguments test_cases yield SampleInput make_tensor shape dtype=dtype device=device low=None high=None requires_grad=requires_grad arguments sample_inputs_split op_info device dtype requires_grad list_args=False kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad list_args cases = S S S torch Size int S S - int S int S S S S torch Size int S S - int S int S S S S torch Size int S S - int S int S - cases = type ignore assignment S S S S S S S shape args cases yield SampleInput make_arg shape args=args sample_inputs_split_with_sizes op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad cases = S S S torch Size int S S - int S int S S S S torch Size int S S - int S S S S torch Size int S S - int S int S S S S torch Size int S S - int S int S - shape args cases yield SampleInput make_arg shape args=args sample_inputs_msort op_info device dtype requires_grad kwargs apply_grad t dtype floating_types_and torch float torch bfloat t requires_grad_ requires_grad large_ d_unique dtype device res = torch randperm L L L dtype=torch int device=device res = res dtype apply_grad res res Test case large tensor yield SampleInput large_ d_unique dtype device yield SampleInput make_tensor S M S dtype=dtype device=device low=None high=None requires_grad=requires_grad sample_inputs_lerp op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad no broadcast yield SampleInput make_arg S S make_arg S S broadcast rhs yield SampleInput make_arg S S make_arg S scalar tensor yield SampleInput make_arg make_arg broadcast rhs scalar-tensor yield SampleInput make_arg S S make_arg broadcast rhs weight tensor yield SampleInput make_arg S S make_arg S make_arg S S broadcast rhs weight tensor yield SampleInput make_arg S S make_arg S make_arg S broadcast lhs yield SampleInput make_arg S make_arg S S with_metadata broadcasts_input=True scalar broadcast_lhs yield SampleInput make_arg make_arg S S with_metadata broadcasts_input=True broadcast all yield SampleInput make_arg S make_arg S S with_metadata broadcasts_input=True tensor broadcast all yield SampleInput make_arg S make_arg S S make_arg S with_metadata broadcasts_input=True no broadcast weight tensor yield SampleInput make_arg S S make_arg S S make_arg S S broadcast lhs weight tensor yield SampleInput make_arg S make_arg S S make_arg S S with_metadata broadcasts_input=True broadcast lhs weight tensor yield SampleInput make_arg S make_arg S S S make_arg S S with_metadata broadcasts_input=True broadcast lhs weight tensor variant yield SampleInput make_arg S S make_arg S S S make_arg S with_metadata broadcasts_input=True dtype is_complex no broadcast yield SampleInput make_arg S S make_arg S S j yield SampleInput make_arg S S make_arg S S + j broadcast rhs yield SampleInput make_arg S S make_arg S j yield SampleInput make_arg S S make_arg S S + j scalar tensor yield SampleInput make_arg make_arg j yield SampleInput make_arg make_arg + j broadcast rhs scalar-tensor yield SampleInput make_arg S S make_arg j yield SampleInput make_arg S S make_arg + j sample_inputs_tensordot device dtype requires_grad kwargs cases = first_shape second_shape dims cases yield SampleInput make_tensor first_shape dtype=dtype device=device requires_grad=requires_grad low=- high=+ make_tensor second_shape dtype=dtype device=device requires_grad=requires_grad low=- high=+ dims=dims sample_inputs_kron op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad low=None high=None test_cases = S S M L input_shape other_shape test_cases input = make_arg input_shape other = make_arg other_shape yield SampleInput input other sample_inputs_inner device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad yield SampleInput make_arg S make_arg S yield SampleInput make_arg make_arg S S sample_inputs_scatter op_info device dtype requires_grad kwargs _tensor shape dtype=dtype low=None high=None make_tensor shape dtype=dtype device=device low=low high=high requires_grad=requires_grad _gather shape index_dim max_indices gather_variable shape index_dim max_indices device=device zero = torch tensor dtype=torch long device=device test_cases = _tensor M S _gather S S M _tensor S S _tensor M S _gather S S M torch int _tensor S S _tensor M S _gather S S S _tensor S S _tensor M S - _gather S S S _tensor S S _tensor M S _gather M S M _tensor M S _tensor M S _gather M S S _tensor M S _tensor M S - _gather M S S _tensor M S _tensor zero detach clone _tensor _tensor zero detach clone tensor args test_cases yield SampleInput tensor args requires_grad yield SampleInput tensor detach clone args reduce= add dtype is_floating_point yield SampleInput tensor detach clone args reduce= multiply sample_inputs_scatter_add op_info device dtype requires_grad kwargs _tensor shape dtype=dtype low=None high=None make_tensor shape dtype=dtype device=device low=low high=high requires_grad=requires_grad _gather shape index_dim max_indices gather_variable shape index_dim max_indices device=device zero = torch tensor dtype=torch long device=device yield SampleInput _tensor M S _gather S S M _tensor S S yield SampleInput _tensor M S _gather S S S _tensor S S yield SampleInput _tensor M S - _gather S S S _tensor S S yield SampleInput _tensor M S _gather M S M _tensor M S yield SampleInput _tensor M S _gather M S S _tensor M S yield SampleInput _tensor M S - _gather M S S _tensor M S yield SampleInput _tensor zero detach clone _tensor sample_inputs_scatter_reduce op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad gather = partial gather_variable device=device zero = torch tensor dtype=torch long device=device test_cases = M S gather S S M S S M S gather S S S S S M S - gather S S S S S M S gather M S M M S M S gather M S S M S M S - gather M S S M S zero detach clone reduce = op_info variant_test_name inp_shape dim index src_shape include_self product test_cases False True False yield SampleInput make_arg inp_shape args= dim index make_arg src_shape reduce kwargs= include_self include_self Sample inputs test edge cases backward Check gradients propagated correctly prod when zeros src reduced requires_grad reduce == prod This sample tests gradients following cases zero reduced src b zeros reduced src c no zeros reduced d zeros reduced both src tested test test_autograd py test_scatter_index_reduce_prod_gradgrad_error case supported gradgrad input = torch tensor dtype=dtype device=device requires_grad=requires_grad src = torch tensor dtype=dtype device=device requires_grad=requires_grad idx = torch tensor dtype=torch long device=device yield SampleInput input args= idx src reduce kwargs= include_self True sample_inputs_segment_reduce op_info device dtype requires_grad mode= lengths kwargs _tensor shape dtype=dtype low=None high=None make_tensor shape dtype=dtype device=device low=low high=high requires_grad=requires_grad test_cases = inp_shape dim lengths unsafe S False S True S False S S False test when lengths do sum dim size M S S True test higher dimensions S S _ range S False S S False S S S _ range S False S S S False reductions = max mean min sum prod args reduce initial product test_cases reductions inp_shape dim lengths unsafe = args lengths_t = torch tensor lengths dtype=torch long device=device sample_input_kwargs = axis dim unsafe unsafe initial initial mode == lengths sample_input_kwargs lengths = lengths_t mode == offsets zeros_shape = list lengths_t shape zeros_shape dim = offsets_t = torch cat lengths_t new_zeros zeros_shape lengths_t dim cumsum_ dim sample_input_kwargs offsets = offsets_t raise RuntimeError f mode most one offsets lengths got mode yield SampleInput _tensor inp_shape args= reduce kwargs=sample_input_kwargs sample_inputs_ravel op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device low=None high=None requires_grad=requires_grad yield SampleInput make_arg S S S yield SampleInput make_arg yield SampleInput make_arg S S S noncontiguous=True sample_inputs_unravel_index op_info device dtype requires_grad kwargs yield SampleInput torch tensor device=device dtype=dtype yield SampleInput torch tensor device=device dtype=dtype yield SampleInput torch tensor device=device dtype=dtype yield SampleInput torch tensor device=device dtype=dtype max_val = dtype itemsize - dtype is_signed - yield SampleInput torch tensor max_val - device=device dtype=dtype max_val yield SampleInput torch tensor device=device dtype=dtype yield SampleInput torch tensor min max_val device=device dtype=dtype yield SampleInput torch tensor device=device dtype=dtype yield SampleInput torch tensor device=device dtype=dtype yield SampleInput torch tensor device=device dtype=dtype yield SampleInput torch tensor device=device dtype=dtype = np array b = np array _ i i = np intersect d b assume_unique=True return_indices=True yield SampleInput torch tensor i device=device dtype=dtype shape yield SampleInput torch tensor i device=device dtype=dtype b shape = np array b = np array _ i i = np intersect d b return_indices=True yield SampleInput torch tensor i device=device dtype=dtype shape yield SampleInput torch tensor i device=device dtype=dtype b shape sample_inputs_tril_triu op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad cases = M M M M M S M S - M M S M S S M S S S shape args cases yield SampleInput make_arg shape args=args error_inputs_tril_triu opinfo device kwargs make_arg = partial make_tensor device=device dtype=torch float error inputs input ndim = yield ErrorInput SampleInput make_arg error_regex= input tensor must have least dimensions sample_inputs_trilu_indices op_info device dtype requires_grad kwargs row col offset args_list = - Large test cases below deliberately commented out speed up CI tests avoid OOM error When modifying implementations tril_indices triu_indices please enable these tests make sure they pass - args args_list yield SampleInput args args=args kwargs= dtype dtype device device sample_inputs_clone_contiguous op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad yield SampleInput make_arg S M S yield SampleInput make_arg reference_inputs_clone_contiguous op device dtype requires_grad kwargs NOTE default memory format clone torch preserve_format contiguous s torch contiguous_format This exploits default test torch preserve_format clone without causing error when testing contiguous yield sample_inputs_clone_contiguous op device dtype requires_grad kwargs shapes = make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad shape shapes yield SampleInput make_arg shape yield SampleInput make_arg shape transpose - yield SampleInput make_arg shape noncontiguous=True yield SampleInput make_arg shape noncontiguous=True transpose - yield SampleInput make_arg shape kwargs= memory_format torch contiguous_format yield SampleInput make_arg shape transpose - kwargs= memory_format torch contiguous_format yield SampleInput make_arg shape noncontiguous=True kwargs= memory_format torch contiguous_format yield SampleInput make_arg shape noncontiguous=True transpose - kwargs= memory_format torch contiguous_format shape strides offset strided_cases = shape strides offset strided_cases yield SampleInput make_arg as_strided shape strides offset yield SampleInput make_arg as_strided shape strides offset kwargs= memory_format torch contiguous_format channels last D yield SampleInput make_arg kwargs= memory_format torch channels_last = make_arg permute yield SampleInput kwargs= memory_format torch channels_last channels last D yield SampleInput make_arg kwargs= memory_format torch channels_last_ d = make_arg permute yield SampleInput kwargs= memory_format torch channels_last_ d sample_inputs_sum_to_size op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad list tuples shape shape defining shapes input output tensors sample_shapes = S S S S S S S S S S S S S S S input_shape output_shape sample_shapes yield SampleInput make_arg input_shape args= output_shape output_shape == continue yield SampleInput make_arg input_shape args= list output_shape yield SampleInput make_arg input_shape args= output_shape error_inputs_sum_to_size op_info device kwargs shape = M S M err_msg = expandable size si = SampleInput make_tensor shape device=device dtype=torch float args= M M yield ErrorInput si error_regex=err_msg shape = M + S S M err_msg = expandable size si = SampleInput make_tensor shape device=device dtype=torch float args= M + yield ErrorInput si error_regex=err_msg sample_inputs_resize_ops op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device cases = S S S S S S shape args_or_shape cases Update ` args ` based operator op_info name == resize_ resize_ takes shape tuple ints args = args_or_shape op_info name == resize_as_ resize_as_ takes another tensor args = make_arg shape requires_grad=False type ignore assignment raise ValueError sample_inputs_resize_ops being used incorrect operator yield SampleInput make_arg shape requires_grad=requires_grad args=args sample_inputs_view_reshape op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad cases = b is_tensor_supported S S S S S S True S S S S S S True S S S S - S False neg index S S S S - False neg index S S True False empty True b is_tensor_supported cases skip unsupported cases kwargs get tensor_arg is_tensor_supported continue convert tensor kwargs get tensor_arg b = make_arg b requires_grad=False yield SampleInput make_arg args= b reference_inputs_view_reshape op device dtype requires_grad kwargs yield sample_inputs_view_reshape op device dtype requires_grad kwargs cases = b is_tensor_supported True True True True True True True True True False empty True True False empty True False empty irreversible_cases = - False neg index empty - False neg index make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad b is_tensor_supported cases skip unsupported cases kwargs get tensor_arg is_tensor_supported continue kwargs get tensor_arg convert tensor yield SampleInput make_arg args= make_arg b requires_grad=False yield SampleInput make_arg b args= make_arg requires_grad=False yield SampleInput make_arg args= b yield SampleInput make_arg b args= b is_tensor_supported irreversible_cases skip unsupported cases kwargs get tensor_arg is_tensor_supported continue convert tensor kwargs get tensor_arg b = make_arg b requires_grad=False yield SampleInput make_arg args= b error_inputs_view_reshape op device kwargs cases = b is_tensor_supported Reshape different numel False empty False empty True True No valid inference - False neg index Two inferred shapes - - False neg index - False neg index - False neg index make_arg = partial make_tensor dtype=torch float device=device requires_grad=False b is_tensor_supported cases skip unsupported cases kwargs get tensor_arg is_tensor_supported continue b == - - error_regex = only one dimension can inferred == error_regex = r cannot reshape tensor elements into shape r \ - \ because unspecified dimension size r - can any value ambiguous avoid having issues regex shape = join map str b size = type int functools reduce operator mul error_regex = rf shape \ shape \ invalid input size size convert tensor kwargs get tensor_arg b = make_arg b requires_grad=False yield ErrorInput SampleInput make_arg args= b error_type=Exception error_regex=error_regex sample_inputs_atleast d d d op_info device dtype requires_grad kwargs shapes = S S S S S S S S S S make_tensor_partial = partial make_tensor dtype=dtype device=device requires_grad=requires_grad shape shapes yield SampleInput make_tensor_partial shape yield SampleInput make_tensor_partial shape shape shapes sample_inputs_column_stack op_info device dtype requires_grad kwargs cases tuple tuple tuple = type ignore assignment S S S S S make_tensor_partial = partial make_tensor dtype=dtype device=device requires_grad=requires_grad shape shape cases yield SampleInput make_tensor_partial shape make_tensor_partial shape sample_inputs_flatten op_info device dtype requires_grad kwargs shapes = S S S S S S make_tensor_partial = partial make_tensor dtype=dtype device=device requires_grad=requires_grad shape shapes yield SampleInput make_tensor_partial shape len shape yield SampleInput make_tensor_partial shape start_dim= end_dim=- reference_inputs_flatten op device dtype requires_grad kwargs yield sample_inputs_flatten op device dtype requires_grad kwargs shape x start_dim x end_dim cases = - - - - - make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad shape start end cases yield SampleInput make_arg shape args= start end yield SampleInput make_arg shape noncontiguous=True transpose - args= start end yield SampleInput make_arg shape transpose - args= start end sample_inputs_unflatten op_info device dtype requires_grad kwargs in_shape dim sizes args = - - - - - - make_tensor_partial = partial make_tensor dtype=dtype device=device requires_grad=requires_grad in_shape dim sizes args yield SampleInput make_tensor_partial in_shape args= dim sizes sample_inputs_select op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad cases = S S S S S S - S S S - - S S S - S S - S shape args cases yield SampleInput make_arg shape args=args sample_inputs_select_scatter op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad cases = S S S S S S S S S S - S S S S S - - S S S S S - S input_shape src_shape args cases input_ = make_arg input_shape src = make_arg src_shape yield SampleInput input_ args= src args sample_inputs_slice_scatter op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad cases = L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L L input_shape src_shape args cases input_ = make_arg input_shape src = make_arg src_shape yield SampleInput input_ args= src args sample_inputs_expand op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad cases = S S S S S S S S S S S - S - S S - S S S S S S S S S S S case cases shape args = case yield SampleInput make_arg shape args= args sample_inputs_conversion op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad shapes = memory_format_options = None torch contiguous_format shape memory_format itertools product shapes memory_format_options yield SampleInput make_arg shape kwargs= memory_format memory_format memory_format yield SampleInput make_arg kwargs= memory_format torch channels_last sample_inputs_byte op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device low= high= requires_grad=requires_grad shapes = memory_format_options = None torch contiguous_format shape memory_format itertools product shapes memory_format_options yield SampleInput make_arg shape kwargs= memory_format memory_format memory_format yield SampleInput make_arg kwargs= memory_format torch channels_last sample_inputs_expand_as op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device cases = S S S S shape shape_other cases yield SampleInput make_arg shape requires_grad=requires_grad args= make_arg shape_other requires_grad=False sample_inputs_where op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad make_bool_mask shape Make sure least one element nonzero except empty tensor mask_t = make_tensor shape dtype=torch bool device=device requires_grad=False mask_t numel == mask_t mask_t numel == mask_t fill_ True mask_t mask_t sum == random_index shape tuple random randrange max_idx max_idx shape mask_t random_index mask_t shape = True mask_t mask_t cases = M M M M M M False M M M M M M True False M M M M True M M True True shape mask_shape other_shape broadcasts_input cases yield SampleInput make_arg shape args= make_bool_mask mask_shape make_arg other_shape broadcasts_input=broadcasts_input TODO add reference inputs where condition signature reference_inputs_where op device dtype requires_grad kwargs yield sample_inputs_where op device dtype requires_grad kwargs make_cond = partial make_tensor dtype=torch bool device=device requires_grad=requires_grad make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad noncontiguous c = make_cond noncontiguous=True = make_arg noncontiguous=True b = make_arg transpose - NOTE OpInfo where takes samples form cond b yield SampleInput args= c b MPS does support float which causes issues following tests torch device device type == mps type promoting FIXME rec shouldn t other_dtype used two lines below other_dtype = torch double dtype torch double torch long noqa F c = make_cond noncontiguous=True = make_arg dtype=torch long b = make_arg yield SampleInput args= c b two python scalars c = make_cond noncontiguous=True = make_arg item b = make_arg item yield SampleInput args= c b NaN propagation dtype is_floating_point dtype is_complex dtype is_floating_point nan = float nan dtype is_complex nan = complex float nan float nan c = make_cond = make_arg noncontiguous=True = nan b = make_arg b = nan yield SampleInput args= c b Python scalars type promotion scalar j False yield SampleInput scalar args= c b yield SampleInput args= c scalar error_inputs_where op_info device kwargs shape = S err_msg = Expected all tensors same device devices product cpu device repeat= len set devices == si = SampleInput make_tensor shape device=devices dtype=torch float args= make_tensor shape dtype=torch bool device=devices make_tensor shape device=devices dtype=torch float yield ErrorInput si error_regex=err_msg sample_inputs_nonzero op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad sizes = S S S S S S S S S S inputs = shape sizes construct input without any non-zero elements zeros = torch zeros shape dtype=dtype device=device requires_grad=requires_grad inputs append zeros construct input mixed zero non-zero elements mixed = make_arg shape requires_grad_ False mask_t = make_tensor shape dtype=torch bool device=device requires_grad=False mixed mask_t = inputs append mixed input_t as_tuple product inputs False True yield SampleInput input_t clone requires_grad_ requires_grad kwargs=dict as_tuple=as_tuple sample_inputs_nonzero_static op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad sizes = S S S S S S S S S S inputs = shape sizes construct input without any non-zero elements zeros = torch zeros shape dtype=dtype device=device requires_grad=requires_grad inputs append zeros construct input mixed zero non-zero elements mixed = make_arg shape requires_grad_ False mask_t = make_tensor shape dtype=torch bool device=device requires_grad=False mixed mask_t = inputs append mixed nonzero_sizes = XS S M input_t nonzero_size product inputs nonzero_sizes yield SampleInput input_t clone requires_grad_ requires_grad kwargs=dict size=nonzero_size sample_inputs_chunk op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad cases = S S S S S S S S S S S - case cases shape args = case yield SampleInput make_arg shape args=args reference_inputs_chunk op device dtype requires_grad kwargs yield sample_inputs_chunk op device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad shape x chunks x dim cases = - - - - shape chunks dim cases yield SampleInput make_arg shape args= chunks dim sample_inputs_kthvalue op_info device dtype requires_grad kwargs _tensor shape dtype=dtype low=None high=None make_tensor shape dtype=dtype device=device low=low high=high requires_grad=requires_grad test_cases = S S S S S S S S S - S S S True S S S - True S S True True yield SampleInput _tensor tensor args tensor args test_cases error_inputs_kthvalue op_info device kwargs tests overlapping output fails t = make_tensor dtype=torch float device=device indices = torch empty device=device dtype=torch long yield ErrorInput SampleInput t out= t indices error_regex= unsupported operation k_out_of_range_err = selected number k out range dimension yield ErrorInput SampleInput torch randn device=device error_regex=k_out_of_range_err yield ErrorInput SampleInput torch randn device=device error_regex=k_out_of_range_err yield ErrorInput SampleInput torch tensor device=device error_regex=k_out_of_range_err sample_inputs_dropout op_info device dtype requires_grad train=None valid_input_dim=None kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad valid_input_dim cases = S i i valid_input_dim cases = S S S p_vals = This handle special case feature_alpha_dropout which has different supported dtypes depending ` train ` parameter training_vals = train train None True False case p training product cases p_vals training_vals yield SampleInput make_arg case p=p training=training yield SampleInput make_arg case sample_inputs_dropout_backward op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_mask = partial make_tensor device=device dtype=torch bool requires_grad=False cases = S S S S S scale_vals = case scale product cases scale_vals yield SampleInput make_arg case make_mask case scale sample_inputs_embedding_bag op_info device dtype requires_grad kwargs make_input shape make_tensor shape device=device dtype=dtype requires_grad=requires_grad make_long_input shape low high noncontiguous=False make_tensor shape device=device dtype=torch long low=low high=high noncontiguous=noncontiguous make_per_sample_weight flag idx tensor float double weights None indicate all weights should taken flag make_input idx shape None offsets = torch tensor device=device dtype=torch long generate_per_sample_weight True False mode sum mean max per_sample_weights only supported mode= sum got mode= generate_per_sample_weight mode mean max continue -D index tensor idx = make_long_input S low= high=M per_sample_weights = make_per_sample_weight generate_per_sample_weight idx yield SampleInput make_input M S args= idx kwargs= offsets offsets mode mode per_sample_weights per_sample_weights idx = make_long_input S low= high=M noncontiguous=True per_sample_weights = make_per_sample_weight generate_per_sample_weight idx yield SampleInput make_input M S args= idx kwargs= offsets offsets mode mode per_sample_weights per_sample_weights bag zero length idx = make_long_input S low= high=M noncontiguous=True per_sample_weights = make_per_sample_weight generate_per_sample_weight idx yield SampleInput make_input M S args= idx kwargs= offsets torch tensor device=device dtype=torch long mode mode per_sample_weights per_sample_weights -D index tensor idx = make_long_input S S low= high=M per_sample_weights = make_per_sample_weight generate_per_sample_weight idx yield SampleInput make_input M S args= idx kwargs= mode mode per_sample_weights per_sample_weights idx = make_long_input S S low= high=M noncontiguous=True per_sample_weights = make_per_sample_weight generate_per_sample_weight idx yield SampleInput make_input M S args= idx kwargs= mode mode per_sample_weights per_sample_weights The gradient vector ` padding_idx ` updated Negative padding_idx idx = make_long_input low= high=S idx = idx = per_sample_weights = make_per_sample_weight generate_per_sample_weight idx yield SampleInput make_input S S args= idx kwargs= padding_idx - offsets offsets mode mode per_sample_weights per_sample_weights idx = make_long_input low= high=S Positive padding_idx idx = idx = per_sample_weights = make_per_sample_weight generate_per_sample_weight idx yield SampleInput make_input S S args= idx kwargs= padding_idx mode mode per_sample_weights per_sample_weights idx = make_long_input low= high=S weights = make_input S S offsets_ = torch tensor device=device dtype=torch long per_sample_weights = make_per_sample_weight generate_per_sample_weight idx yield SampleInput weights args= idx kwargs= mode mode offsets offsets_ include_last_offset True requires_grad Following inputs different gradient numerical gradient This expected relevant tests present ` test_nn py ` Due inplace renorming weight numerical gradient doesn t match analytical gradient idx = make_long_input low= high=S weights = make_input S S per_sample_weights = make_per_sample_weight generate_per_sample_weight idx yield SampleInput weights args= idx kwargs= max_norm mode mode per_sample_weights per_sample_weights idx = make_long_input low= high=S weights = make_input S S per_sample_weights = make_per_sample_weight generate_per_sample_weight idx yield SampleInput weights args= idx kwargs= max_norm norm_type mode mode offsets offsets per_sample_weights per_sample_weights mode = max Scale gradient based inverse frequency particular index Note smax mode does support sparse weights idx = make_long_input low= high=S idx = idx = weights = make_input S S per_sample_weights = make_per_sample_weight generate_per_sample_weight idx yield SampleInput weights args= idx kwargs= scale_grad_by_freq True mode mode per_sample_weights per_sample_weights gradcheck implemented sparse tensors Note max mode does support sparse weights idx = make_long_input low= high=S weights = make_input S S per_sample_weights = make_per_sample_weight generate_per_sample_weight idx yield SampleInput weights args= idx kwargs= sparse True offsets offsets mode mode per_sample_weights per_sample_weights idx = make_long_input low= high=S idx = freq more than idx = freq more than idx = padding_idx weights = make_input S S per_sample_weights = make_per_sample_weight generate_per_sample_weight idx yield SampleInput weights args= idx kwargs= sparse True scale_grad_by_freq True padding_idx max_norm offsets offsets mode mode per_sample_weights per_sample_weights sample_inputs_embedding op_info device dtype requires_grad kwargs make_input shape make_tensor shape device=device dtype=dtype requires_grad=requires_grad make_long_input shape low high make_tensor shape device=device dtype=torch long low=low high=high -D index tensor idx = make_long_input low= high=M yield SampleInput make_input M S args= idx -D index tensor idx = make_long_input S low= high=M yield SampleInput make_input M S args= idx -D index tensor idx = make_long_input S S low= high=M yield SampleInput make_input M S args= idx requires_grad Following inputs different gradient numerical gradient This expected relevant tests present ` test_nn py ` The gradient vector ` padding_idx ` updated idx = make_long_input low= high=S idx = idx = yield SampleInput make_input S S args= idx kwargs= padding_idx idx = make_long_input low= high=S idx = idx = yield SampleInput make_input S S args= idx kwargs= padding_idx - Due inplace renorming weight numerical gradient doesn t match analytical gradient idx = make_long_input low= high=S weights = make_input S S yield SampleInput weights args= idx kwargs= max_norm idx = make_long_input low= high=S weights = make_input S S yield SampleInput weights args= idx kwargs= max_norm norm_type Scale gradient based inverse frequency particular index idx = make_long_input low= high=S idx = idx = weights = make_input S S yield SampleInput weights args= idx kwargs= scale_grad_by_freq True gradcheck implemented sparse tensors idx = make_long_input low= high=S weights = make_input S S yield SampleInput weights args= idx kwargs= sparse True idx = make_long_input low= high=S idx = freq more than idx = freq more than idx = padding_idx weights = make_input S S yield SampleInput weights args= idx kwargs= sparse True scale_grad_by_freq True padding_idx max_norm sample_inputs_one_hot op_info device dtype requires_grad kwargs make_input shape low high make_tensor shape device=device dtype=dtype low=low high=high requires_grad=requires_grad shapes = S L M S num_classess = - SampleInput make_input shape low= high= num_classes == - num_classes kwargs=dict num_classes=num_classes shape num_classes itertools product shapes num_classess sample_inputs_loss op_info device dtype requires_grad kwargs rhs_requires_grad = kwargs get rhs_requires_grad requires_grad _make_tensor = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Although most losses also support reduce size_average combination instead reduce former deprecated since thus tested shapes_and_kwargs = None S dict reduction= mean S dict reduction= sum S dict reduction= none S S None S S S None shape kwargs shapes_and_kwargs yield SampleInput _make_tensor shape args= _make_tensor shape requires_grad=rhs_requires_grad kwargs=kwargs sample_inputs_grid_sample op_info device dtype requires_grad kwargs We get better tests we change range values something like - because grid second tensor argument useful range - way you get better combination out-of-range in-range test cases _make_tensor = partial make_tensor device=device dtype=dtype requires_grad=requires_grad low=- high= batch_size = num_channels = modes = bilinear nearest align_cornerss = False True padding_modes = zeros border reflection dim modes_ = modes bicubic dim == modes mode padding_mode align_corners itertools product modes_ padding_modes align_cornerss yield SampleInput _make_tensor batch_size num_channels S dim _make_tensor batch_size S dim dim mode=mode padding_mode=padding_mode align_corners=align_corners reference_inputs_grid_sample op_info device dtype requires_grad kwargs batch_size = num_channels = height = width = modes = bilinear nearest bicubic align_cornerss = False True padding_modes = zeros border reflection Create affine transformation matrix = torch deg rad torch tensor ca sa = torch cos torch sin rotation angles s s = scales theta = torch tensor ca s sa -sa ca s dtype=dtype device=device theta = theta expand batch_size contiguous x = torch arange batch_size num_channels height width device=device x = x reshape batch_size num_channels height width torch uint x = x dtype=dtype x requires_grad_ requires_grad mode padding_mode align_corners itertools product modes padding_modes align_cornerss grid = torch nn functional affine_grid theta size= batch_size num_channels height width align_corners=align_corners yield SampleInput x grid mode padding_mode align_corners sample_inputs_grid_sampler_ d op_info device dtype requires_grad kwargs We get better tests we change range values something like - because grid second tensor argument useful range - way you get better combination out-of-range in-range test cases _make_tensor = partial make_tensor device=device dtype=dtype requires_grad=requires_grad low=- high= batch_size = num_channels = modes = align_cornerss = False True padding_modes = mode padding_mode align_corners itertools product modes padding_modes align_cornerss yield SampleInput _make_tensor batch_size num_channels S L _make_tensor batch_size M + M mode padding_mode align_corners sample_inputs_grid_sampler_ d op_info device dtype requires_grad kwargs _make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad low=- high= Test both out-of-range in-range grid values _make_grid = partial make_tensor device=device dtype=dtype requires_grad=requires_grad low=- high= modes = padding_modes = align_cornerss = False True shape_pairs = input_shape grid_shape S L L M + M + M L L + L + L + L + L M + M + M M M + M + M + M + M L + L + L + L M + M + M + M + L L + L + L + params_prod = itertools product modes padding_modes align_cornerss shape_pairs mode padding_mode align_corners input_shape grid_shape params_prod yield SampleInput _make_input input_shape _make_grid grid_shape mode padding_mode align_corners sample_inputs_cosine_embedding_loss op_info device dtype requires_grad kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_target shape shape = len shape == shape t = torch randint shape device=device dtype=torch long Label - t = t - target = t dtype=dtype detach_ requires_grad_ requires_grad target shapes = S S S reductions = none mean sum s r product shapes reductions yield SampleInput make_input s args= make_input s make_target s kwargs=dict reduction=r margin=random uniform - sample_inputs_ctc_loss op_info device dtype requires_grad kwargs input_length = batch = num_char = target_length = make_log_probs s t = make_tensor s device=device dtype=dtype log_probs = t log_softmax device=device dtype=dtype detach requires_grad_ requires_grad=requires_grad log_probs reductions = none mean sum zero_inf = True False lengths_type = list torch Tensor r z lt product reductions zero_inf lengths_type log_probs = make_log_probs input_length batch num_char targets = torch randint num_char batch target_length dtype=torch long device=device input_lengths = torch full batch input_length dtype=torch long device=device target_lengths = torch randint target_length batch dtype=torch long device=device Dont generate int types reduction = Mean since results non composite compliant calls ctc_loss IntList since tensor needs created target lengths Creating such tensor requires use pointers copy data int - torch Tensor e g via std copy Similarly symbolic real tracing fx will also work lt list r none sum input_lengths = input_lengths tolist target_lengths = target_lengths tolist yield SampleInput log_probs args= targets input_lengths target_lengths kwargs=dict reduction=r zero_infinity=z sample_inputs_nll_loss op_info device dtype requires_grad kwargs shape = num_classes = shape make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad FIXME Derivative wrt weight implemented make_weight = partial make_tensor num_classes device=device dtype=dtype requires_grad=False make_target shape zeros=False s = shape shape len shape zeros torch zeros s device=device dtype=torch long make_tensor s low= high=shape len shape shape device=device dtype=torch long gen_shape_kwargs Batched non-batched d shapes = shape num_classes shape + reductions = none mean sum reduction s product reductions shapes yield make_input s make_target s dict reduction=reduction yield make_input s make_target s dict weight=make_weight reduction=reduction yield make_input s make_target s dict weight=make_weight low= reduction=reduction dtype is_floating_point dtype is_complex yield make_input s make_target s dict weight=make_weight high= reduction=reduction t = make_target s ignore = num_classes If mean nll returns NaN so s differentiable those points t eq ignore all reduction == mean t fill_ yield make_input s t dict ignore_index=num_classes reduction=reduction yield make_input s t dict ignore_index=num_classes reduction=reduction weight=make_weight Test ignoring all targets If mean nll returns NaN so s differentiable those points reduction = mean yield make_input s make_target s zeros=True dict ignore_index= reduction=reduction input target kwargs gen_shape_kwargs yield SampleInput input args= target kwargs=kwargs target = torch tensor - device=device dtype=torch long yield SampleInput make_input shape args= target kwargs= ignore_index - sample_inputs_binary_cross_entropy_with_logits op_info device dtype requires_grad kwargs make = partial make_tensor device=device dtype=dtype make_prob = partial make low= high= reductions = mean sum none make_weight_shape_kwargs kwargs = shape S S S S kwargs extend S S dict reduction=reduction weight=make shape reduction reductions kwargs shapes_and_kwargs = shape None shape S S S S S S S S dict reduction=reduction reduction reductions make_weight_shape_kwargs S S dict reduction=reduction pos_weight=make S low= reduction reductions S S dict reduction=reduction weight=make S S pos_weight=make S low= reduction reductions shape kwargs shapes_and_kwargs yield SampleInput make shape requires_grad=requires_grad args= make_prob shape requires_grad=requires_grad kwargs=kwargs sample_inputs_argwhere op_info device dtype requires_grad kwargs yield SampleInput torch tensor dtype=dtype device=device requires_grad=requires_grad mask = torch tensor dtype=torch bool device=device t = make_tensor S S dtype=dtype device=device requires_grad=requires_grad t mask = yield SampleInput t t = make_tensor S S dtype=dtype device=device requires_grad=requires_grad noncontiguous=True t mask = yield SampleInput t t = make_tensor S dtype=dtype device=device requires_grad=requires_grad yield SampleInput t yield SampleInput torch zeros S dtype=dtype device=device requires_grad=requires_grad yield SampleInput make_tensor dtype=dtype device=device requires_grad=requires_grad _generate_sample_shape_reduction shapes = S S S S S S reductions = none mean sum yield product shapes reductions sample_inputs_gaussian_nll_loss op_info device dtype requires_grad kwargs _make_tensor = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Set low slightly above so gradcheck doesn t accidentally dip below make_var = partial make_tensor low= device=device dtype=dtype requires_grad=requires_grad gen_shape shape yield shape Broadcast yield shape - yield shape - gen_shape_kwargs s r _generate_sample_shape_reduction t_s v_s product gen_shape s gen_shape s yield _make_tensor s _make_tensor t_s make_var v_s dict reduction=r yield _make_tensor s _make_tensor t_s make_var v_s dict full=True reduction=r yield _make_tensor s _make_tensor t_s make_var v_s dict eps=random uniform e- e- reduction=r yield _make_tensor s _make_tensor t_s make_var v_s dict full=True eps=random uniform e- e- reduction=r input target var kwargs gen_shape_kwargs yield SampleInput input args= target var kwargs=kwargs error_inputs_gaussian_nll_loss op_info device kwargs _make = partial make_tensor device=device dtype=torch float invalid reduction value yield ErrorInput SampleInput _make _make _make low= reduction= abc error_type=ValueError error_regex= abc valid var incorrect shape yield ErrorInput SampleInput _make _make _make low= error_type=ValueError error_regex= var incorrect size target incorrect shape yield ErrorInput SampleInput _make _make _make low= error_type=RuntimeError error_regex= r The size tensor \ \ must match size tensor b \ \ r non-singleton dimension _generate_sample_inputs_nn_loss op_info device dtype requires_grad kwargs _make_tensor = partial make_tensor device=device dtype=dtype requires_grad=requires_grad s r _generate_sample_shape_reduction yield _make_tensor s _make_tensor s dict reduction=r sample_inputs_hinge_embedding_loss op_info device dtype requires_grad kwargs input target d _generate_sample_inputs_nn_loss op_info device dtype requires_grad kwargs target should contain either - per docs mask = torch rand_like target target mask = target ~mask = - d margin = random uniform - yield SampleInput input args= target kwargs=d scalar input target _make_tensor = partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput _make_tensor args= _make_tensor error_inputs_hinge_embedding_loss op device kwargs make_input = partial make_tensor device=device dtype=torch float invalid reduction value yield ErrorInput SampleInput make_input args= make_input kwargs= reduction abc error_type=ValueError error_regex= valid value reference_inputs_hinge_embedding_loss op device dtype requires_grad kwargs yield sample_inputs_hinge_embedding_loss op device dtype requires_grad kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad reduction sum mean none dtype is_floating_point only supports ints floats NaN propagation inp = make_input inp = float nan target = make_input target should contain either - per docs mask = torch rand_like target target mask = - target ~mask = yield SampleInput inp args= target kwargs= reduction reduction Inf Handling inp = make_input inp = float inf target = make_input mask = torch rand_like target target mask = - target ~mask = yield SampleInput inp args= target kwargs= reduction reduction Broadcasting inp = make_input target = make_input mask = torch rand_like target target mask = - target ~mask = yield SampleInput inp args= target kwargs= reduction reduction sample_inputs_huber_loss op_info device dtype requires_grad kwargs input target d _generate_sample_inputs_nn_loss op_info device dtype requires_grad kwargs d delta = random uniform e- yield SampleInput input args= target kwargs=d error_inputs_huber_loss op device kwargs make_input = partial make_tensor device=device dtype=torch float invalid reduction value err = valid value reduction yield ErrorInput SampleInput make_input args= make_input kwargs= reduction abc error_type=ValueError error_regex=err delta = delta - err = huber_loss does support non-positive values delta yield ErrorInput SampleInput make_input args= make_input kwargs= delta delta error_type=RuntimeError error_regex=err sample_inputs_poisson_nll_loss op_info device dtype requires_grad kwargs _make_tensor = partial make_tensor device=device dtype=dtype requires_grad=requires_grad gen_shape_kwargs s r _generate_sample_shape_reduction li True False f True False i = _make_tensor s i = _make_tensor s For Poisson NLL Loss target assumed Poisson Distribution which always has positive samples t = _make_tensor s low= t = _make_tensor s low= li i abs_ i abs_ t abs_ t abs_ yield i t dict log_input=li full=f reduction=r yield i t dict log_input=li full=f eps=random uniform e- e- reduction=r input target kwargs gen_shape_kwargs yield SampleInput input args= target kwargs=kwargs test INT_TO_FLOAT promotion dtype is_complex d torch bool torch int yield SampleInput _make_tensor dtype=dtype args= _make_tensor dtype=d yield SampleInput _make_tensor dtype=d args= _make_tensor dtype=dtype error_inputs_poisson_nll_loss op_info device kwargs make = partial make_tensor device=device dtype=torch float invalid reduction value yield ErrorInput SampleInput make args= make kwargs= reduction abc error_type=ValueError error_regex= abc valid value reduction invalid input shapes yield ErrorInput SampleInput make args= make error_regex= r Attempting broadcast dimension length &#124; r The size tensor \ \ must match r size tensor b \ \ non-singleton r dimension error_inputs_soft_margin_loss op_info device kwargs make = partial make_tensor device=device dtype=torch float invalid reduction value yield ErrorInput SampleInput make args= make kwargs= reduction abc error_type=ValueError error_regex= abc valid value reduction invalid input shapes yield ErrorInput SampleInput make args= make error_regex= r Attempting broadcast dimension length &#124; r The size tensor \ \ must match r size tensor b \ \ non-singleton r dimension sample_inputs_triplet_margin_loss op_info device dtype requires_grad with_distance=False kwargs make = partial make_tensor S M device=device dtype=dtype requires_grad=requires_grad kwargss = dict margin=margin margin e- dict swap=True dict reduction=reduction reduction mean sum none kwargs kwargss input = make args = make make with_distance kwargs distance_function = torch nn PairwiseDistance yield SampleInput input args=args kwargs=kwargs error_inputs_triplet_margin_loss op_info device kwargs make_input = partial make_tensor device=device dtype=torch float samples = input args kwargs error_type error_regex invalid reduction make_input make_input make_input dict reduction= abc ValueError abc valid value reduction invalid margin make_input make_input make_input dict margin=- ValueError margin must greater than got - shape mismatch make_input make_input make_input RuntimeError r Attempting broadcast dimension length &#124; r The size tensor \ \ must match size tensor b \ \ r non-singleton dimension make_input make_input make_input RuntimeError r Attempting broadcast dimension length &#124; r The size tensor \ \ must match size tensor b \ \ r non-singleton dimension make_input make_input make_input RuntimeError r Attempting broadcast dimension length &#124; r The size tensor \ \ must match size tensor b \ \ r non-singleton dimension different dimensions make_input make_input make_input RuntimeError r The anchor positive negative tensors expected have r same number dimensions got anchor D positive D r negative D inputs make_input make_input make_input RuntimeError r The anchor positive negative tensors expected have r same number dimensions got anchor D positive D r negative D inputs make_input make_input make_input RuntimeError r The anchor positive negative tensors expected have r same number dimensions got anchor D positive D r negative D inputs input args kwargs error_type error_regex samples yield ErrorInput SampleInput input args=args kwargs=kwargs error_type=error_type error_regex=error_regex sample_inputs_scaled_mm op_info device dtype requires_grad kwargs make_mat_e m = partial make_tensor device=device dtype=torch float _e m fn requires_grad=requires_grad make_mat_e m = partial make_tensor device=device dtype=torch float _e m requires_grad=requires_grad make_scale = partial make_tensor device=device dtype=torch float requires_grad=False M N K = samples = two e m mat = make_mat_e m M K mat = make_mat_e m K N t contiguous t scale = make_scale scale = make_scale samples append SampleInput mat mat scale scale mat e m mat e m mat = make_mat_e m M K mat = make_mat_e m K N t contiguous t scale = make_scale scale = make_scale samples append SampleInput mat mat scale scale mat e m mat e m mat = make_mat_e m M K mat = make_mat_e m K N t contiguous t scale = make_scale scale = make_scale samples append SampleInput mat mat scale scale yield samples sample_inputs_scaled_dot_product_attention op_info device dtype requires_grad kwargs make = partial make_tensor device=device dtype=dtype requires_grad=requires_grad batch seq_q seq_kv num_heads head_dim = dim_ _q_shape = batch seq_q head_dim dim_ _kv_shape = batch seq_kv head_dim dim_ _q_shape = batch num_heads seq_q head_dim dim_ _kv_shape = batch num_heads seq_kv head_dim broadcast_tuple = num_heads seq_q head_dim batch num_heads seq_kv head_dim qkv_shapes = dim_ _q_shape dim_ _kv_shape dim_ _q_shape dim_ _kv_shape broadcast_tuple samples = gqa_options = True False causal_options = True False qkv_shape is_causal dropout_p _enable_gqa product qkv_shapes causal_options gqa_options shape_q shape_kv = qkv_shape samples append SampleInput make shape_q make shape_kv make shape_kv is_causal=is_causal dropout_p=dropout_p Add non standard shapes FIXME rec should diff_v_head_dim appended samples diff_v_head_dim = SampleInput noqa F make batch num_heads seq_q head_dim make batch num_heads seq_kv head_dim make batch num_heads seq_kv head_dim + is_causal=is_causal dropout_p=dropout_p Add attn_mask samples append SampleInput make batch num_heads seq_q head_dim make batch num_heads seq_kv head_dim make batch num_heads seq_kv head_dim attn_mask=make seq_q seq_kv is_causal=False dropout_p= yield samples sample_inputs_efficient_attention_forward op_info device dtype requires_grad kwargs make = partial make_tensor device=device dtype=dtype requires_grad=requires_grad batch num_heads head_dim = seq_q = seq_kv = dim_ _q_shape = batch num_heads seq_q head_dim dim_ _kv_shape = batch num_heads seq_kv head_dim qkv_shapes = dim_ _q_shape dim_ _kv_shape samples = mask_types = UpperLeft LowerRight scales = None qkv_shape _is_causal dropout_p mask_type scale product qkv_shapes True False mask_types scales shape_q shape_kv = qkv_shape samples append SampleInput make shape_q transpose make shape_kv transpose make shape_kv transpose bias=None cu_seqlens_q=None cu_seqlens_k=None max_seqlen_q=None max_seqlen_k=None dropout_p=dropout_p custom_mask_type=mask_type compute_log_sumexp=requires_grad scale=scale seqlen_k=None Add non standard shapes FIXME rec should diff_v_head_dim appended samples diff_v_head_dim = SampleInput noqa F make batch seq_q num_heads head_dim make batch seq_kv num_heads head_dim make batch seq_kv num_heads head_dim + bias=None cu_seqlens_q=None cu_seqlens_k=None max_seqlen_q=None max_seqlen_k=None dropout_p=dropout_p custom_mask_type= No Mask compute_log_sumexp=requires_grad scale=None seqlen_k=None Add attn_mask samples append SampleInput make batch seq_q num_heads head_dim make batch seq_kv num_heads head_dim make batch seq_kv num_heads head_dim bias=make batch num_heads seq_q seq_kv cu_seqlens_q=None cu_seqlens_k=None max_seqlen_q=None max_seqlen_k=None dropout_p=dropout_p custom_mask_type= No Mask compute_log_sumexp=requires_grad scale=None seqlen_k=None jagged query keys offsets cu_seqlens_k = torch arange - + dtype=torch int device=device cu_seqlens_k - = cu_seqlens_k = samples append SampleInput make view - unsqueeze make view - unsqueeze make view - unsqueeze bias=None cu_seqlens_q=torch arange + dtype=torch int device=device cu_seqlens_k=cu_seqlens_k max_seqlen_q= max_seqlen_k= dropout_p= custom_mask_type= No Mask compute_log_sumexp=requires_grad scale=None seqlen_k=None yield samples sample_inputs_flash_attention_forward op_info device dtype requires_grad kwargs make = partial make_tensor device=device dtype=dtype requires_grad=requires_grad batch num_heads head_dim = seq_q = seq_kv = dim_ _q_shape = batch num_heads seq_q head_dim dim_ _kv_shape = batch num_heads seq_kv head_dim qkv_shapes = dim_ _q_shape dim_ _kv_shape samples = scales = None qkv_shape is_causal dropout_p scale product qkv_shapes True False scales shape_q shape_kv = qkv_shape samples append SampleInput make shape_q transpose make shape_kv transpose make shape_kv transpose cum_seq_q=None cum_seq_k=None max_q=seq_q max_k=seq_kv dropout_p=dropout_p is_causal=is_causal return_debug_mask=False scale=scale yield samples sample_inputs_pairwise_distance op_info device dtype requires_grad kwargs make = partial make_tensor device=device dtype=dtype requires_grad=requires_grad shape = batched_shape = shape shapes_and_kwargs = shape None batched_shape None shape dict keepdim=True batched_shape dict keepdim=True shape dict p= shape dict p=- shape dict eps= SampleInput make shape args= make shape kwargs=kwargs shape kwargs shapes_and_kwargs sample_inputs_pixel_shuffle op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_arg upscale_factor=upscale_factor upscale_factor yield SampleInput make_arg shape upscale_factor= shape sample_inputs_pixel_unshuffle op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_arg downscale_factor=downscale_factor downscale_factor yield SampleInput make_arg shape downscale_factor= shape sample_inputs_channel_shuffle op_info device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad shapes_groups = yield SampleInput make_arg shape args= groups shape groups shapes_groups sample_inputs_binary_cross_entropy op_info device dtype requires_grad logits=False kwargs make = partial make_tensor device=device dtype=dtype Lower bounds must greater than eps defined gradcheck py gradgradcheck - eps otherwise perturbation calculation causes Tensor value become negative triggering device-side hardware assertion make_prob = partial make low= e- high= reductions = mean sum none shapes_and_kwargs = shape None shape S S S S S S S S dict reduction=reduction reduction reductions S S dict reduction=reduction weight=make S S reduction reductions logits shapes_and_kwargs extend S S dict reduction=reduction pos_weight=make S low= reduction reductions shape kwargs shapes_and_kwargs yield SampleInput make logits make_prob shape requires_grad=requires_grad args= make_prob shape requires_grad=requires_grad kwargs=kwargs sample_inputs_allclose op_info device dtype requires_grad kwargs sample_shapes = S S S S atols = e- e- rtols = e- s rtol atol product sample_shapes rtols atols close sample t = make_tensor s device=device dtype=dtype requires_grad=requires_grad close = t + atol detach requires_grad_ requires_grad yield SampleInput t close rtol=rtol atol=atol random sample = make_tensor s device=device dtype=dtype requires_grad=requires_grad b = make_tensor s device=device dtype=dtype requires_grad=requires_grad yield SampleInput b rtol=rtol atol=atol sample_inputs_l _loss op_info device dtype requires_grad kwargs yield sample_inputs_loss op_info device dtype requires_grad kwargs test COMPLEX_TO_FLOAT promotion dtype is_complex make = partial make_tensor device=device requires_grad=requires_grad yield SampleInput make dtype=dtype args= make dtype=torch double yield SampleInput make dtype=torch double args= make dtype=dtype error_inputs_l _loss op_info device kwargs make = partial make_tensor device=device dtype=torch float invalid reduction value yield ErrorInput SampleInput make args= make kwargs= reduction abc error_type=ValueError error_regex= abc valid value reduction invalid input shapes yield ErrorInput SampleInput make args= make error_regex= r Attempting broadcast dimension length &#124; r The size tensor \ \ must match r size tensor b \ \ non-singleton r dimension sample_inputs_smooth_l _loss op_info device dtype requires_grad kwargs yield sample_inputs_loss op_info device dtype requires_grad kwargs make = partial make_tensor S S device=device dtype=dtype requires_grad=requires_grad This test case always triggers smooth condition since absolute difference input target smaller than beta yield SampleInput make low= high= args= make low=- high= kwargs=dict beta= yield SampleInput make args= make kwargs=dict beta= sample_inputs_kl_div op_info device dtype requires_grad kwargs kl_div works inputs aka pdf probability measure Then log = -inf so log space make_arg = partial make_tensor low= device=device dtype=dtype requires_grad=requires_grad make_log shape out = torch nn functional log_softmax make_arg shape - out requires_grad_ requires_grad out make_prob shape out = torch nn functional softmax make_arg shape - out requires_grad_ requires_grad out shapes = reductions = none mean batchmean sum shape reduction log_target product shapes reductions True False input = make_log shape target = make_log shape log_target make_prob shape yield SampleInput input args= target kwargs=dict reduction=reduction log_target=log_target sample_inputs_pdist op_info device dtype requires_grad kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_input n m n m itertools product S repeat= yield SampleInput make_input S S kwargs=dict p=p p float inf reference_pdist input p= pdist = scipy spatial distance pdist p == output = pdist input hamming input shape p == float inf output = pdist input lambda x y np abs x - y max output = pdist input minkowski p=p output astype input dtype sample_inputs_diagflat op_info device dtype requires_grad kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_input yield SampleInput make_input yield SampleInput make_input yield SampleInput make_input offset= yield SampleInput make_input offset=- sample_inputs_max_unpool op_info device dtype requires_grad kwargs unpool_name_to_pool_method_dict = nn functional max_unpool d torch nn functional max_pool d nn functional max_unpool d torch nn functional max_pool d nn functional max_unpool d torch nn functional max_pool d unpool_name_to_dim = nn functional max_unpool d nn functional max_unpool d nn functional max_unpool d unpool_to_pool_name_dict = k f nn functional v __name__ k v unpool_name_to_pool_method_dict items pool_dim = unpool_name_to_dim op_info name pool_method = unpool_name_to_pool_method_dict op_info name pool_op_info = copy copy op_info pool_op_info name = unpool_to_pool_name_dict op_info name sample sample_inputs_max_pool pool_op_info device dtype requires_grad kwargs shapes C do work now see https github com pytorch pytorch issues TODO remove once issue resolved sample input dim = pool_dim + continue No dilation max_unpool see https github com pytorch pytorch issues sample kwargs dilation = continue Can t unpool without indices sample kwargs return_indices pool indices = pool_method sample input sample kwargs arg has leaf arg = pool detach requires_grad_ requires_grad sample_kwargs = kernel_size sample kwargs kernel_size stride sample kwargs stride padding sample kwargs padding output_size could None we specify explicitly compensate information lose pool due floor ceil operation used compute shapes output_size sample input size yield SampleInput arg args= indices kwargs=sample_kwargs sample_inputs_max_unpool_grad op_info device dtype requires_grad kwargs sample sample_inputs_max_unpool op_info device dtype requires_grad kwargs indices = sample args The samples max_unpool generated max_pool It could single element max_pool s input mapped several locations its output This situation leads failed gradchecks because finite difference algorithm perturbs elements output one one classes equivalences determined whether two elements output coming same location input simply put they have same corresponding index So there two ways resolve issue Extract perturbation one element apply all elements same equivalence Make sure equivalence classes all singletons i e index tensor has comprised only unique indices Here we go solution easiest all indices unique numel == indices numel yield sample sample_inputs_multi_head_attention_forward opinfo device dtype requires_grad kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad requires_grad backward tests would take too long complete causing job timeout bsz = is_batcheds = True use_separate_proj_weights = False emb_sizes = src_lens = XS tgt_lens = XS heads = dropouts = mask_types = d bsz = is_batcheds = False True use_separate_proj_weights = False True emb_sizes = src_lens = XS tgt_lens = XS S heads = dropouts = mask_types = None d d is_batched use_separate_proj_weight mask_type emb_size src_len tgt_len num_heads dropout_p itertools product is_batcheds use_separate_proj_weights mask_types emb_sizes src_lens tgt_lens heads dropouts attn_mask = None mask_type == d attn_mask = make_input src_len tgt_len mask_type == d attn_mask = make_input bsz is_batched num_heads src_len tgt_len is_batched q = make_input src_len bsz emb_size k = make_input tgt_len bsz emb_size v = make_input tgt_len bsz emb_size q = make_input src_len emb_size k = make_input tgt_len emb_size v = make_input tgt_len emb_size use_separate_proj_weight in_proj_weight = None q_proj_weight = make_input emb_size emb_size k_proj_weight = make_input emb_size emb_size v_proj_weight = make_input emb_size emb_size in_proj_weight = make_input emb_size emb_size q_proj_weight = None k_proj_weight = None v_proj_weight = None bias_k = make_input emb_size bias_v = make_input emb_size in_proj_bias = make_input emb_size out_proj_weight = make_input emb_size emb_size out_proj_bias = make_input emb_size sample_args = k v emb_size num_heads in_proj_weight in_proj_bias bias_k bias_v False dropout_p out_proj_weight out_proj_bias sample_kwargs = q_proj_weight q_proj_weight k_proj_weight k_proj_weight v_proj_weight v_proj_weight attn_mask attn_mask training dropout_p use_separate_proj_weight use_separate_proj_weight yield SampleInput q args=sample_args kwargs=sample_kwargs Includes some values such N N won t multiple which should ensure we test vectorized non-vectorized kernel code paths NUM_SIZE _TENSORS = foreach_num_tensors = TEST_WITH_SLOW _foreach_inputs_default_kwargs = noncontiguous False same_size False low None high None ForeachRightmostArgType enum Enum TensorList = enum auto ScalarList = enum auto Scalar = enum auto Tensor = enum auto ForeachSampleInput SampleInput For TensorList op Scalar Tensor we compute reference converting into TensorList op ScalarList TensorList then converting into multiple Tensor op Scalar Tensor ref_args contains args converted TensorList op ScalarList TensorList ref_args Any disable_fastpath bool __init__ args disable_fastpath=False ref_args=None kwargs super __init__ args kwargs ref_args = ref_args args disable_fastpath = disable_fastpath foreach_inputs_sample_func __init__ arity int rightmost_supports_scalar bool rightmost_supports_scalarlist bool rightmost_supports_tensor bool = False - None arity = arity _set_rightmost_arg_types rightmost_supports_scalar rightmost_supports_scalarlist rightmost_supports_tensor _intersperse_empty = True False _set_rightmost_arg_types rightmost_supports_scalar bool rightmost_supports_scalarlist bool rightmost_supports_tensor bool - None _rightmost_arg_types = ForeachRightmostArgType TensorList arity rightmost_supports_scalar _rightmost_arg_types append ForeachRightmostArgType Scalar rightmost_supports_scalarlist _rightmost_arg_types append ForeachRightmostArgType ScalarList rightmost_supports_tensor _rightmost_arg_types append ForeachRightmostArgType Tensor _sample_rightmost_arg opinfo rightmost_arg_type device dtype num_tensors allow_higher_dtype_scalars _foreach_inputs_kwargs rightmost_arg_type == ForeachRightmostArgType TensorList sample_inputs_foreach None device dtype num_tensors _foreach_inputs_kwargs rightmost_arg_type == ForeachRightmostArgType Tensor make_tensor device=device dtype=dtype noncontiguous=_foreach_inputs_kwargs noncontiguous requires_grad=_foreach_inputs_kwargs get requires_grad False should_use_simpler_scalars = opinfo name == _foreach_pow dtype torch float torch bfloat sample_float s = random random should_use_simpler_scalars s - s high = should_use_simpler_scalars rightmost_arg_type == ForeachRightmostArgType ScalarList scalarlist_list = scalarlist_list append random randint high + _ range num_tensors allow_higher_dtype_scalars dtype is_floating_point scalarlist_list append sample_float _ range num_tensors allow_higher_dtype_scalars dtype is_complex scalarlist_list append complex sample_float sample_float _ range num_tensors scalarlist_list append + j + _ range num_tensors - scalarlist_list append True + j + _ range num_tensors - scalarlist_list rightmost_arg_type == ForeachRightmostArgType Scalar scalars = scalars append random randint high + allow_higher_dtype_scalars dtype is_floating_point scalars append sample_float allow_higher_dtype_scalars dtype is_complex scalars append complex sample_float sample_float scalars append True scalars raise AssertionError f Invalid rightmost_arg_type rightmost_arg_type _should_disable_fastpath opinfo rightmost_arg rightmost_arg_type dtype arity == foreach_abs opinfo name dtype complex_types True unary opinfo ref torch abs torch neg False opinfo ref_inplace == torch Tensor zero_ False dtype integral_types_and torch bool arity rightmost_arg_type == ForeachRightmostArgType Tensor None foreach_pow opinfo name dtype integral_types_and torch bool True any foreach_name opinfo name foreach_name foreach_clamp_max foreach_clamp_min foreach_maximum foreach_minimum dtype integral_types_and torch bool True rightmost_arg_type == ForeachRightmostArgType TensorList disable_fastpath = foreach_div opinfo name dtype integral_types_and torch bool foreach_add opinfo name dtype == torch bool disable_fastpath = True disable_fastpath rightmost_arg_type == ForeachRightmostArgType Scalar disable_fastpath = foreach_div opinfo name dtype integral_types_and torch bool isinstance rightmost_arg bool disable_fastpath &#124; = dtype == torch bool opinfo ref torch add torch mul disable_fastpath = False isinstance rightmost_arg int disable_fastpath &#124; = dtype == torch bool isinstance rightmost_arg float disable_fastpath &#124; = dtype integral_types_and torch bool isinstance rightmost_arg complex disable_fastpath &#124; = dtype complex_types raise AssertionError f Invalid scalar type rightmost_arg_type - rightmost_arg disable_fastpath rightmost_arg_type == ForeachRightmostArgType ScalarList disable_fastpath = opinfo ref == torch div dtype integral_types_and torch bool elmt_t = type rightmost_arg has_same_type = all isinstance v elmt_t v rightmost_arg has_same_type dtype complex_types isinstance rightmost_arg bool foreach_add opinfo name foreach_mul opinfo name dtype == torch bool disable_fastpath = False isinstance rightmost_arg int disable_fastpath &#124; = dtype == torch bool isinstance rightmost_arg float disable_fastpath &#124; = dtype integral_types_and torch bool isinstance rightmost_arg complex disable_fastpath &#124; = dtype complex_types raise AssertionError f Invalid scalarlist rightmost_arg disable_fastpath raise AssertionError f Invalid rightmost_arg_type rightmost_arg_type _sample_kwargs opinfo rightmost_arg rightmost_arg_type dtype kwargs = rightmost_arg_type == ForeachRightmostArgType TensorList opinfo supports_alpha_param dtype integral_types_and torch bool kwargs alpha = dtype is_complex kwargs alpha = complex kwargs alpha = arity kwargs disable_fastpath = _should_disable_fastpath opinfo rightmost_arg rightmost_arg_type dtype kwargs sample_zero_size_tensor_inputs opinfo device dtype requires_grad kwargs assert num_input_tensors kwargs _foreach_inputs_kwargs = k kwargs pop k v k v _foreach_inputs_default_kwargs items _foreach_inputs_kwargs requires_grad = requires_grad allow_higher_dtype_scalars = kwargs pop allow_higher_dtype_scalars False _rightmost_arg_type _rightmost_arg_types zero_size_foreach_inputs_kwargs = copy deepcopy _foreach_inputs_kwargs zero_size_foreach_inputs_kwargs zero_size = True input = sample_inputs_foreach None device dtype NUM_SIZE _TENSORS zero_size_foreach_inputs_kwargs arity args = sample_inputs_foreach None device dtype NUM_SIZE _TENSORS zero_size_foreach_inputs_kwargs _ range arity - args append _sample_rightmost_arg opinfo ForeachRightmostArgType TensorList device dtype NUM_SIZE _TENSORS allow_higher_dtype_scalars=allow_higher_dtype_scalars zero_size_foreach_inputs_kwargs kwargs = _sample_kwargs opinfo args - ForeachRightmostArgType TensorList dtype args = kwargs = opinfo ref torch abs torch neg kwargs disable_fastpath = False kwargs disable_fastpath = dtype integral_types_and torch bool yield ForeachSampleInput input args kwargs __call__ opinfo device dtype requires_grad kwargs num_input_tensors_specified = num_input_tensors kwargs num_input_tensors = kwargs pop num_input_tensors num_input_tensors_specified foreach_num_tensors assert isinstance num_input_tensors list _foreach_inputs_kwargs = k kwargs pop k v k v _foreach_inputs_default_kwargs items _foreach_inputs_kwargs requires_grad = requires_grad _foreach_inputs_kwargs zero_size = False allow_higher_dtype_scalars = kwargs pop allow_higher_dtype_scalars False add empty tensor interspersion test fully fixing num_tensors rightmost_arg_type intersperse_empty_tensors itertools product num_input_tensors _rightmost_arg_types _intersperse_empty intersperse_empty_tensors num_tensors = max num_input_tensors str device == cpu generate interspersed empty tensors only N non-cpu device lessen redundancy continue _foreach_inputs_kwargs intersperse_empty_tensors = intersperse_empty_tensors input = sample_inputs_foreach None device dtype num_tensors _foreach_inputs_kwargs args = arity args = sample_inputs_foreach None device dtype num_tensors _foreach_inputs_kwargs _ range arity - rightmost_arg_list = _sample_rightmost_arg opinfo rightmost_arg_type device dtype num_tensors allow_higher_dtype_scalars _foreach_inputs_kwargs rightmost_arg rightmost_arg_list args append rightmost_arg kwargs = _sample_kwargs opinfo rightmost_arg rightmost_arg_type dtype ref_args = args rightmost_arg_type ForeachRightmostArgType Scalar ForeachRightmostArgType Tensor ref_args = args - + args - _ range num_tensors sample = ForeachSampleInput input args ref_args=ref_args kwargs yield sample args pop yield ForeachSampleInput input args disable_fastpath=self _should_disable_fastpath opinfo None None dtype foreach_max_sample_func foreach_inputs_sample_func __init__ arity int rightmost_supports_scalar bool rightmost_supports_scalarlist bool rightmost_supports_tensor bool = False - None super __init__ arity rightmost_supports_scalar rightmost_supports_scalarlist rightmost_supports_tensor _intersperse_empty = False sample_zero_size_tensor_inputs opinfo device dtype requires_grad kwargs _should_disable_fastpath opinfo rightmost_arg rightmost_arg_type dtype False foreach_norm_sample_func foreach_inputs_sample_func sample_zero_size_tensor_inputs opinfo device dtype requires_grad kwargs assert num_input_tensors kwargs _foreach_inputs_kwargs = k kwargs pop k v k v _foreach_inputs_default_kwargs items _foreach_inputs_kwargs requires_grad = requires_grad ord - - float inf float -inf input = sample_inputs_foreach None device dtype NUM_SIZE _TENSORS zero_size=True _foreach_inputs_kwargs disable_fastpath = True ord float inf dtype floating_types_and torch half torch bfloat disable_fastpath = False yield ForeachSampleInput input ord=ord disable_fastpath=disable_fastpath __call__ opinfo device dtype requires_grad kwargs num_input_tensors = kwargs pop num_input_tensors foreach_num_tensors assert isinstance num_input_tensors list _foreach_inputs_kwargs = k kwargs pop k v k v _foreach_inputs_default_kwargs items _foreach_inputs_kwargs requires_grad = requires_grad _allow_higher_dtype_scalars = kwargs pop allow_higher_dtype_scalars False num_tensors ord out_dtype intersperse_empty_tensors product num_input_tensors - - float inf float -inf None + torch complex dtype complex_types torch float True False inf norm negative norms empty tensors supported our reference func vector norm linalg vector_norm cannot compute inf norm empty tensor because operation does have identity ord float inf float -inf ord intersperse_empty_tensors continue _foreach_inputs_kwargs intersperse_empty_tensors = intersperse_empty_tensors input = sample_inputs_foreach None device dtype num_tensors zero_size=False _foreach_inputs_kwargs disable_fastpath = True ord float inf dtype floating_types_and torch half torch bfloat disable_fastpath = False yield ForeachSampleInput input ord=ord disable_fastpath=disable_fastpath dtype=out_dtype Also test nan propagation single tensor skip autograd testing requires_grad nan_inputs = float nan float nan float nan float nan float nan float nan float nan - float nan float nan - float nan float nan - input nan_inputs x = torch tensor input device=device disable_fastpath = True ord float inf dtype floating_types_and torch half torch bfloat disable_fastpath = False yield ForeachSampleInput x ord=ord disable_fastpath=disable_fastpath foreach_pointwise_sample_func foreach_inputs_sample_func __init__ arity int = rightmost_supports_scalar bool = False rightmost_supports_scalarlist bool = False super __init__ arity rightmost_supports_scalar rightmost_supports_scalarlist _should_disable_fastpath opinfo rightmost_arg rightmost_arg_type dtype dtype integral_types_and torch bool opinfo ref == torch addcmul sample_zero_size_tensor_inputs opinfo device dtype requires_grad kwargs assert num_input_tensors kwargs _foreach_inputs_kwargs = k kwargs pop k v k v _foreach_inputs_default_kwargs items _foreach_inputs_kwargs requires_grad = requires_grad zero_size tensor input = sample_inputs_foreach None device dtype NUM_SIZE _TENSORS zero_size=True _foreach_inputs_kwargs args = sample_inputs_foreach None device dtype NUM_SIZE _TENSORS zero_size=True _foreach_inputs_kwargs _ range kwargs pop scalars None kwargs update _sample_kwargs opinfo args - ForeachRightmostArgType TensorList dtype yield ForeachSampleInput input args kwargs __call__ opinfo device dtype requires_grad kwargs num_input_tensors_specified = num_input_tensors kwargs num_input_tensors = kwargs pop num_input_tensors num_input_tensors_specified foreach_num_tensors assert isinstance num_input_tensors list _foreach_inputs_kwargs = k kwargs pop k v k v _foreach_inputs_default_kwargs items _foreach_inputs_kwargs requires_grad = requires_grad allow_higher_dtype_scalars = kwargs pop allow_higher_dtype_scalars False num_tensors rightmost_arg_type intersperse_empty_tensors itertools product num_input_tensors _rightmost_arg_types True False _foreach_inputs_kwargs intersperse_empty_tensors = intersperse_empty_tensors input = sample_inputs_foreach None device dtype num_tensors zero_size=False _foreach_inputs_kwargs args = sample_inputs_foreach None device dtype num_tensors zero_size=False _foreach_inputs_kwargs _ range - int rightmost_arg_type == ForeachRightmostArgType TensorList rightmost_arg_list = _sample_rightmost_arg opinfo rightmost_arg_type device dtype num_tensors zero_size=False allow_higher_dtype_scalars=False intersperse_empty_tensors allow_higher_dtype_scalars _foreach_inputs_kwargs rightmost_arg rightmost_arg_list kwargs = rightmost_arg_type == ForeachRightmostArgType TensorList args append rightmost_arg rightmost_arg_type ForeachRightmostArgType Tensor ForeachRightmostArgType ScalarList kwargs scalars = rightmost_arg kwargs value = rightmost_arg kwargs update _sample_kwargs opinfo rightmost_arg rightmost_arg_type dtype assert len args == f len args = sample = ForeachSampleInput input args kwargs yield sample rightmost_arg_type == ForeachRightmostArgType TensorList args pop foreach_unary_op_db list OpInfo = ForeachFuncInfo exp sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float backward_requires_result=True supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True supports_sparse=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool ForeachFuncInfo acos sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool ForeachFuncInfo asin sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool ForeachFuncInfo atan sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool ForeachFuncInfo cos sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool ForeachFuncInfo cosh sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool ForeachFuncInfo log sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool ForeachFuncInfo log sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool ForeachFuncInfo log sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool ForeachFuncInfo tan sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat backward_requires_result=True supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= due https github com pytorch pytorch pull enabling jiterator complex DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestForeach test_parity device_type= cuda ForeachFuncInfo tanh sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat backward_requires_result=True supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestForeach test_parity device_type= cuda ForeachFuncInfo sin sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool ForeachFuncInfo sinh sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool ForeachFuncInfo neg sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_outplace dtypes= torch bool DecorateInfo unittest expectedFailure TestForeach test_unary_op_tensors_on_different_devices device_type= cuda dtypes= torch bool ForeachFuncInfo sqrt sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True backward_requires_result=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool ForeachFuncInfo rsqrt sample_inputs_func=foreach_inputs_sample_func False False supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True backward_requires_result=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool ForeachFuncInfo ceil sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat torch int torch int supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestForeach test_autodiff device_type= cuda dtypes= torch complex ForeachFuncInfo erf sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool + complex_types DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace dtypes=complex_types DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool + complex_types DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace dtypes=complex_types DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool + complex_types DecorateInfo unittest expectedFailure TestMeta test_meta_outplace dtypes=complex_types DecorateInfo unittest expectedFailure TestForeach test_autodiff device_type= cuda dtypes= torch complex ForeachFuncInfo erfc sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool + complex_types DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace dtypes=complex_types DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool + complex_types DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace dtypes=complex_types DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool + complex_types DecorateInfo unittest expectedFailure TestMeta test_meta_outplace dtypes=complex_types DecorateInfo unittest expectedFailure TestForeach test_autodiff device_type= cuda dtypes= torch complex ForeachFuncInfo expm sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True backward_requires_result=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool ForeachFuncInfo floor sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestForeach test_autodiff device_type= cuda dtypes= torch complex ForeachFuncInfo log p sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool ForeachFuncInfo round sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestForeach test_autodiff device_type= cuda dtypes= torch complex ForeachFuncInfo frac sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool + complex_types DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace dtypes=integral_types_and torch bool + complex_types DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool + complex_types DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace dtypes=integral_types_and torch bool + complex_types DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool + complex_types DecorateInfo unittest expectedFailure TestMeta test_meta_outplace dtypes=integral_types_and torch bool + complex_types DecorateInfo unittest expectedFailure TestForeach test_autodiff device_type= cuda dtypes= torch complex ForeachFuncInfo reciprocal sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True backward_requires_result=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool ForeachFuncInfo sigmoid sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat torch float supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True backward_requires_result=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool ForeachFuncInfo trunc sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat torch int torch int supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestForeach test_autodiff device_type= cuda dtypes= torch complex ForeachFuncInfo abs sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True supports_fwgrad_bwgrad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=complex_types DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=complex_types DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace device_type= cpu dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace device_type= cpu dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace device_type= cpu dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace device_type= cpu dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_outplace device_type= cpu dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace device_type= cpu dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=complex_types ForeachFuncInfo zero sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True supports_out=False ForeachFuncInfo sign sample_inputs_func=foreach_inputs_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=complex_types DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace dtypes=complex_types DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=complex_types DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace dtypes=complex_types DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=complex_types DecorateInfo unittest expectedFailure TestMeta test_meta_outplace dtypes=complex_types DecorateInfo unittest expectedFailure TestForeach test_autodiff device_type= cuda dtypes= torch complex ForeachFuncInfo lgamma sample_inputs_func=foreach_inputs_sample_func False False supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest skip In-place lgamma supported integral tensors TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest skip In-place lgamma supported integral tensors TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest skip In-place lgamma supported integral tensors TestMeta test_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=complex_types + integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace dtypes=complex_types DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=complex_types + integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace dtypes=complex_types DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=complex_types + integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_outplace dtypes=complex_types DecorateInfo unittest expectedFailure TestForeach test_autodiff device_type= cuda dtypes= torch complex foreach_binary_op_db list OpInfo = ForeachFuncInfo add sample_inputs_func=foreach_inputs_sample_func True True True dtypesIfHpu=custom_types torch float torch bfloat torch float torch int supports_alpha_param=True supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= These tests fail aten _local_scalar_dense being implemented DecorateInfo unittest expectedFailure TestMeta test_meta_outplace DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=all_types_and_complex_and torch bool torch bfloat torch float Samples have complex types inplace only works dtype complex DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace_all_strides dtypes=integral_types + complex_types_and torch bool torch bfloat torch float torch float ForeachFuncInfo sub sample_inputs_func=foreach_inputs_sample_func True True dtypesIfHpu=custom_types torch float torch bfloat supports_alpha_param=True supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace DecorateInfo unittest expectedFailure TestMeta test_meta_inplace DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace DecorateInfo unittest expectedFailure TestMeta test_meta_outplace DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace_all_strides DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace_all_strides DecorateInfo unittest skip consistently fails internally causes other tests appear flaky TestForeach test_parity dtypes= torch complex active_if=lambda kwargs IS_FBCODE kwargs noncontiguous ForeachFuncInfo mul sample_inputs_func=foreach_inputs_sample_func True True True dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= Samples have complex types inplace only works dtype complex DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace_all_strides dtypes= torch bool DecorateInfo unittest skip consistently fails internally causes other tests appear flaky TestForeach test_parity dtypes= torch complex active_if=lambda kwargs IS_FBCODE kwargs noncontiguous ForeachFuncInfo div sample_inputs_func=foreach_inputs_sample_func True True True dtypesIfHpu=custom_types torch float torch bfloat torch float torch int torch int supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= Samples have complex types inplace only works dtype complex DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace_all_strides dtypes=integral_types_and torch bool ForeachFuncInfo clamp_min sample_inputs_func=foreach_inputs_sample_func True True dtypesIfHpu=custom_types torch float torch bfloat torch float torch int torch int torch int torch bool supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace_all_strides dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace_all_strides dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestForeach test_autodiff device_type= cuda dtypes= torch complex DecorateInfo unittest expectedFailure TestForeach test_binary_op_scalar_with_overlapping_tensors dtypes=complex_types ForeachFuncInfo clamp_max sample_inputs_func=foreach_inputs_sample_func True True dtypesIfHpu=custom_types torch float torch bfloat torch float torch int torch int torch int torch bool supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace_all_strides dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace_all_strides dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestForeach test_autodiff device_type= cuda dtypes= torch complex DecorateInfo unittest expectedFailure TestForeach test_binary_op_scalar_with_overlapping_tensors dtypes=complex_types note crcrpar forward ad implemented ForeachFuncInfo minimum sample_inputs_func=foreach_inputs_sample_func True True dtypesIfHpu=custom_types torch float torch bfloat torch int supports_autograd=True supports_inplace_autograd=False supports_forward_ad=False decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace_all_strides dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace_all_strides dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestForeach test_autodiff device_type= cuda dtypes= torch complex DecorateInfo unittest expectedFailure TestForeach test_binary_op_scalar_with_overlapping_tensors dtypes=complex_types note crcrpar forward ad implemented ForeachFuncInfo maximum sample_inputs_func=foreach_inputs_sample_func True True dtypesIfHpu=custom_types torch float torch bfloat torch int supports_autograd=True supports_forward_ad=False supports_inplace_autograd=False decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_outplace dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace_all_strides dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace_all_strides dtypes=complex_types_and torch bool DecorateInfo unittest expectedFailure TestForeach test_autodiff device_type= cuda dtypes= torch complex DecorateInfo unittest expectedFailure TestForeach test_binary_op_scalar_with_overlapping_tensors dtypes=complex_types ForeachFuncInfo pow supports_alpha_param=False supports_scalar_self_arg=True sample_inputs_func=foreach_inputs_sample_func True True dtypesIfHpu=custom_types torch float torch bfloat torch float torch int torch int torch bool supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_outplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace dtypes= torch bool DecorateInfo unittest skip flaky TestForeach test_parity device_type= cpu dtypes= torch complex DecorateInfo unittest skip failed starting ROCm TestForeach test_parity device_type= cuda dtypes= torch complex active_if=TEST_WITH_ROCM DecorateInfo unittest expectedFailure TestForeach test_binary_op_with_scalar_self_support device_type= cuda dtypes= torch bool active_if=lambda kwargs kwargs is_fastpath backward_requires_result=True ForeachFuncInfo copy sample_inputs_func=foreach_inputs_sample_func False False supports_out=False supports_forward_ad=False supports_autograd=False supports_inplace_autograd=False foreach_pointwise_op_db list ForeachFuncInfo = ForeachFuncInfo addcmul sample_inputs_func=foreach_pointwise_sample_func True True dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_outplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace_all_strides dtypes= torch bool Samples have complex types inplace only works dtype complex DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace_all_strides dtypes=integral_types + complex_types_and torch bool ForeachFuncInfo addcdiv sample_inputs_func=foreach_pointwise_sample_func True True dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= Samples have complex types inplace only works dtype complex DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace_all_strides dtypes=integral_types + complex_types_and torch bool fails div_cpu implemented ComplexHalf DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_outplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace_all_strides dtypes=integral_types + complex_types_and torch bool foreach_reduce_op_db list ForeachFuncInfo = ForeachFuncInfo max sample_inputs_func=foreach_max_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat torch int supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= no complex support ordering ops like max DecorateInfo unittest expectedFailure TestForeach test_autodiff dtypes= torch complex torch complex DecorateInfo unittest expectedFailure TestForeach test_foreach_reduce_large_input dtypes= torch complex torch complex DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace dtypes= torch complex torch complex DecorateInfo unittest expectedFailure TestMeta test_meta_outplace dtypes= torch complex torch complex DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace dtypes= torch complex torch complex ForeachFuncInfo norm sample_inputs_func=foreach_norm_sample_func False False dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace DecorateInfo unittest expectedFailure TestMeta test_meta_outplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace_all_strides DecorateInfo unittest expectedFailure TestForeach test_foreach_reduce_large_input device_type= cuda dtypes=integral_types_and torch bool foreach_other_op_db list ForeachFuncInfo = ForeachFuncInfo lerp sample_inputs_func=foreach_inputs_sample_func True True dtypesIfHpu=custom_types torch float torch bfloat supports_autograd=True supports_inplace_autograd=True supports_forward_ad=True decorators= DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_meta_outplace dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace_all_strides dtypes=integral_types_and torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace_all_strides dtypes=integral_types_and torch bool reference_sign x x dtype == np bool_ ` np sign ` doesn t support ` bool ` np sign True ufunc sign did contain loop signature matching types dtype bool - dtype bool np sign x dtype=np uint astype np bool_ np sign x reference_sgn x NumPy doesn t have equivalent ` torch sgn ` when dtype complex For complex inputs ` np sign ` returns sign x real + j x real = sign x imag + j while ` torch sgn ` returns abs input == input abs input x dtype np complex np complex reference_sign x out = x np abs x out ndim == Handle x == case x == Can t assign np complex object So make new one np array complex dtype=x dtype out Handle x == case mask = x == out mask = complex out reference_sigmoid x scipy special expit supported input types x dtype np complex np complex + np exp -x scipy special expit x reference_logsigmoid x np where x x - np log p np exp x -np log p np exp -x reference_hardsigmoid x intermediate = x + y = np clip intermediate None np where y y astype x dtype reference_lgamma x scipy special gammaln returns ` -inf ` when input ` -inf ` While Pytorch C C++ all ` inf ` when input ` -inf ` Reference https en cppreference com w cpp numeric math lgamma https en cppreference com w c numeric math lgamma To handle above discrepancy we replace -inf inf so values originally -inf map inf expected x dtype kind == f x = np where x == float -inf np array float inf dtype=x dtype x out = scipy special gammaln x x dtype == np float ` scipy special gammaln ` returns output float when input float while ` torch lgamma ` preserves ` float ` But due smaller range float Pytorch version outputs ` inf ` while SciPy returns finite values out = out astype np float out reference_mvlgamma x d x dtype == np float scipy special multigammaln x d astype np float scipy special multigammaln x d reference_softplus input beta= threshold= non_linear = input beta = threshold output = input copy output non_linear = np log + np exp beta input non_linear beta output reference_gelu X approximate= none _gelu_ref X X stats norm cdf X _tanh_gelu_ref X M_SQRT_ _PI = math sqrt math pi Z = M_SQRT_ _PI X + np power X X + np tanh Z approximate == tanh _tanh_gelu_ref X _gelu_ref X reference_one_hot npt NDArray num_classes int = - - npt NDArray num_classes == - num_classes = int np amax + idcs = reshape - + np arange size dtype=np int num_classes one_hot = np zeros size num_classes dtype=a dtype np put one_hot idcs one_hot reshape shape - reference_mse_loss input target reduction= mean se = input - target reduction == mean np mean se reduction == sum np sum se reduction == none se reference_layer_norm inp npt NDArray normalized_shape tuple int weight=None bias=None eps= e- reference_native_layer_norm inp normalized_shape weight bias eps reference_native_layer_norm inp npt NDArray normalized_shape tuple int weight bias eps feature_size = np prod normalized_shape inp_view = inp reshape - feature_size type ignore call-overload mean = inp_view mean axis=- keepdims=True var = inp_view var axis=- ddof= keepdims=True Y = inp_view - mean np sqrt var + eps weight None bias None Y = Y + bias reshape - weight None bias None Y = Y weight reshape - weight None bias None Y = Y weight reshape - + bias reshape - axis = inp ndim - len normalized_shape stat_shape = inp shape axis + len normalized_shape Y reshape inp shape mean reshape stat_shape np sqrt var + eps reshape stat_shape reference_rms_norm inp npt NDArray normalized_shape tuple int weight=None eps=None eps None eps = torch finfo numpy_to_torch_dtype inp dtype eps feature_size = np prod normalized_shape inp_view = inp reshape - feature_size type ignore call-overload rms = np sqrt inp_view mean axis=- keepdims=True + eps Y = inp_view rms weight None Y = Y weight reshape - Y reshape inp shape reference_group_norm inp npt NDArray num_groups int weight=None bias=None eps= e- inp_view = inp np prod inp shape = inp_view = inp reshape inp shape num_groups - mean = inp_view mean axis=- keepdims=True var = inp_view var axis=- ddof= keepdims=True Y = inp_view - mean np sqrt var + eps Y = Y reshape inp shape weight None weight vector length equal channel len Y shape weight = np expand_dims weight + idx + idx range inp ndim - Y = Y weight bias None bias vector length equal channel len Y shape bias = np expand_dims bias + idx + idx range inp ndim - Y = Y + bias Y using custom reference function since numpy only has string side arg instead right side doesn t have out_int arg Additionally numpy doesn t support searchsorted ND arrays so splits those into stacked D cases reference_searchsorted sorted_sequence boundary out_int =False right=False side= left sorter=None side = right right side == right left len sorted_sequence shape == ret = np searchsorted sorted_sequence boundary side=side sorter=sorter ret astype np int out_int ret sorted_sequence shape == sorter None sorter = sorter flatten ret = np searchsorted sorted_sequence flatten boundary flatten side=side sorter=sorter ret = ret astype np int out_int ret ret reshape boundary shape numpy searchsorted only supports D inputs so we split up ND inputs orig_shape = boundary shape num_splits = np prod sorted_sequence shape - splits = range num_splits sorted_sequence boundary = sorted_sequence reshape num_splits - boundary reshape num_splits - sorter None sorter = sorter reshape num_splits - split_sequence = sorted_sequence i i splits split_boundary = boundary i i splits split_sorter = sorter i sorter None None i splits split_ret = np searchsorted s_seq b side=side sorter=s_sort s_seq b s_sort zip split_sequence split_boundary split_sorter strict=True split_ret = i astype np int i split_ret out_int split_ret np stack split_ret reshape orig_shape reference_hash_tensor tensor dim= keepdim=False mode= assert mode == Only mode= xor_sum supported right now dtype = tensor dtype dtype kind == f tensor = tensor astype np float view np uint tensor = tensor astype np uint dim == result = np bitwise_xor reduce tensor flatten keepdims=keepdim isinstance dim list dim = tuple dim result = np bitwise_xor reduce tensor axis=dim keepdims=keepdim result loss_reference_reduction_wrapper fn wrapper input target size_average=None reduce=None reduction= mean other_kwargs size_average None reduce None raise RuntimeError The keyword arguments size_average reduce deprecated supported wrapper output = fn input target other_kwargs reduction == mean np mean output reduction == sum np sum output reduction == none output wrapper loss_reference_reduction_wrapper reference_smooth_l _loss input target beta= diff = input - target abs_diff = np abs diff above_threshold = abs_diff = beta loss = np empty_like input loss above_threshold = abs_diff above_threshold - beta loss ~above_threshold = diff ~above_threshold beta loss reference_std_var f Forwards unbiased correction kwargs NumPy s equivalent ddof g = reference_reduction_numpy f wraps g wrapper x npt NDArray args kwargs assert unbiased kwargs correction kwargs unbiased kwargs kwargs ddof = int kwargs pop unbiased correction kwargs kwargs ddof = kwargs pop correction g x args kwargs wrapper generate_std_var_kwargs t torch Tensor kwargs Generates unbiased correction kwargs std var operators yield unbiased True yield unbiased False Currently calling std correction only enabled when both dim keepdim provided dim kwargs keepdim kwargs yield correction yield correction numel = torch tensor t shape kwargs get dim prod yield correction numel error_inputs_mean op_info device is_ref=False kwargs is_ref err_msg = r mean\ \ could infer output dtype r Input dtype must either floating point complex dtype r Got torch int err_msg = r mean\ \ could infer output dtype r Input dtype must either floating point complex dtype r Got Long yield ErrorInput SampleInput make_tensor dtype=torch int device=device error_regex=err_msg is_ref err_msg = r mean\ \ could infer output dtype r Optional dtype must either floating point complex dtype r Got torch int err_msg = r mean\ \ could infer output dtype r Optional dtype must either floating point complex dtype r Got Long yield ErrorInput SampleInput make_tensor dtype=torch float device=device dtype=torch int error_regex=err_msg numpy implementation torch flatten unfortunately there s no np flatten we figure out desired shape call np reshape reference_flatten input start_dim= end_dim=- in_shape = input shape in_rank = len in_shape d start_dim end_dim in_rank == d - -in_rank = d in_rank raise IndexError f Dimension out range expected range -in_rank in_rank - got d end_dim = end_dim end_dim = in_rank + end_dim start_dim = start_dim start_dim = in_rank + start_dim in_rank == end_dim = start_dim end_dim start_dim raise RuntimeError flatten has invalid args start_dim cannot come after end_dim flatten_bit_dim = functools reduce operator mul in_shape start_dim end_dim + out_shape = in_shape start_dim + flatten_bit_dim + in_shape end_dim + np reshape input out_shape sample_inputs_alias_copy op_info device dtype requires_grad kwargs yield SampleInput make_tensor S dtype=dtype device=device requires_grad=requires_grad yield SampleInput make_tensor dtype=dtype device=device requires_grad=requires_grad Operator database sorted alphabetically op_db list OpInfo = UnaryUfuncInfo abs aliases= absolute ref=np abs dtypes=all_types_and_complex_and torch half torch bfloat torch chalf dtypesIfCUDA=all_types_and_complex_and torch bool torch half torch bfloat torch chalf dtypesIfHpu=custom_types torch float torch bfloat skips= DecorateInfo unittest skip In-place abs supported complex tensors TestBwdGradients test_inplace_grad dtypes= torch cdouble DecorateInfo unittest skip In-place abs supported complex tensors TestBwdGradients test_inplace_gradgrad dtypes= torch cdouble DecorateInfo unittest skip In-place abs supported complex tensors TestFwdGradients test_inplace_forward_mode_AD dtypes= torch cdouble DecorateInfo unittest skip In-place abs supported complex tensors TestSparseUnaryUfuncs test_inplace dtypes= torch cdouble torch cfloat torch chalf Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_small dtypes= torch int active_if=TEST_WITH_ASAN TODO Fix test_out_arg_all_dtypes torch empty_like expected_output where expected_output=op input We can break logic loop over all possible types OK https github com pytorch pytorch blob master test test_unary_ufuncs py#L -L DecorateInfo unittest skip Skipped TestUnaryUfuncs test_out_arg_all_dtypes dtypes= torch cfloat torch cdouble DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes= torch cdouble torch cfloat torch chalf DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes= torch cdouble torch cfloat torch chalf DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes= torch cdouble torch cfloat torch chalf DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace_all_strides dtypes= torch cdouble torch cfloat torch chalf supports_fwgrad_bwgrad=True assert_autodiffed=True supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True supports_forward_ad=True NOTE CPU complex acos produces incorrect outputs https github com pytorch pytorch issues UnaryUfuncInfo acos aliases= arccos ref=np arccos domain= - dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and_complex_and torch chalf torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True decorators= precisionOverride torch float e- torch bfloat e- torch complex e- skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_normal device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS Failing wrong imaginary sign least some Windows jobs DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_small device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS Failing wrong imaginary sign least some Windows jobs DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestBwdGradients test_fn_grad dtypes= torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestBwdGradients test_method_grad dtypes= torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestBwdGradients test_inplace_grad dtypes= torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestFwdGradients test_forward_mode_AD dtypes= torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestFwdGradients test_inplace_forward_mode_AD dtypes= torch cdouble active_if=IS_WINDOWS NOTE derivative inplace acosh implemented UnaryUfuncInfo acosh aliases= arccosh ref=np arccosh domain= None dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and_complex_and torch chalf torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat decorators= precisionOverride torch bfloat e- supports_inplace_autograd=False supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_normal device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS Failing wrong imaginary sign least some Windows jobs DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_small device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS acosh defined x real reference_numerics_filter=NumericsFilter condition=lambda x x x is_complex torch zeros_like x dtype=torch bool safe_val= BinaryUfuncInfo add NumPy has no builtin reference alpha kwarg easy enough emulate ref=lambda input other alpha= np add input other alpha == np add input np multiply alpha other dtypes=all_types_and_complex_and torch bool torch bfloat torch float torch chalf dtypesIfHpu=custom_types torch float torch bfloat torch int assert_autodiffed=True sample_inputs_func=sample_inputs_add_sub supports_fwgrad_bwgrad=True supports_forward_ad=True supports_two_python_scalars=True decorators= DecorateInfo toleranceOverride torch chalf tol atol= e- rtol= TestBinaryUfuncs test_reference_numerics skips= boolean alpha handled properly DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness dtypes= torch bool DecorateInfo unittest skip Skipped TestCommon test_numpy_refs dtypes= torch complex DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_extremal_values dtypes= torch complex torch complex OpInfo item op=lambda inp args kwargs wrapper_set_seed torch Tensor item inp args kwargs ref=np ndarray item method_variant=None dtypes=all_types_and_complex_and torch bfloat torch float torch chalf torch bool dtypesIfHpu=custom_types torch float supports_out=False supports_autograd=False error_inputs_func=error_inputs_item sample_inputs_func=sample_inputs_item skips= Error testing item function variant DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float torch complex FX failed normalize op - add op op_skip list DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError Composite compliance check failed above error DecorateInfo unittest expectedFailure TestCompositeCompliance test_operator Booleans mismatch AssertionError False true DecorateInfo unittest expectedFailure TestFakeTensor test_fake_autocast Booleans mismatch AssertionError False true DecorateInfo unittest expectedFailure TestFakeTensor test_fake OpInfo arange dtypes=all_types_and torch bfloat torch float dtypesIfHpu=custom_types torch float torch bfloat torch int torch int supports_out=True supports_autograd=False is_factory_function=True error_inputs_func=error_inputs_arange sample_inputs_func=sample_inputs_arange skips= https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive Tests assume input tensor sequence tensors DecorateInfo unittest expectedFailure TestCommon test_variant_consistency_eager DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view Lazy tensor failures DecorateInfo unittest expectedFailure TestLazyOpInfo test_dispatched_to_lazy DecorateInfo unittest skip Skipped TestLazyOpInfo test_correctness DecorateInfo unittest skip Skipped TestLazyOpInfo test_correctness_with_reusing_ir Exception raised analyzeImpl torch csrc jit ir alias_analysis cpp We don t have op aten arange isn t special case Argument types bool bool bool int int Device boo DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness Captured graph does contain aten arange succeeds complex g graph Long strides= requires_grad= device=cpu = prim Constant value= DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float UserWarning triggered Resized non-empty tensor did warn about DecorateInfo unittest expectedFailure TestCommon test_out_warning OpInfo cauchy op=lambda inp args kwargs wrapper_set_seed torch Tensor cauchy_ inp args kwargs inplace_variant=torch Tensor cauchy_ dtypes=floating_types_and torch float torch bfloat supports_out=False supports_autograd=False allow_cow_input_materialize_forward= sample_inputs_func=sample_inputs_cauchy error_inputs_func=error_inputs_cauchy skips= Tests assume input tensor has meaningful effect output tensor DecorateInfo unittest expectedFailure TestCommon test_variant_consistency_eager DecorateInfo unittest expectedFailure TestMathBits test_neg_view AssertionError JIT Test does execute any logic DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit AssertionError Tensor-likes close DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu FX failed normalize op - add op op_skip list DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive vmap calling random operator supported DecorateInfo unittest skip Test expects tensor input TestVmapOperatorsOpInfo test_vmap_exhaustive DecorateInfo unittest skip Test expects tensor input TestVmapOperatorsOpInfo test_op_has_batch_rule DecorateInfo unittest skip make_traced doesn t set seed properly TestCommon test_python_ref_executor DecorateInfo unittest expectedFailure TestDecomp test_quick OpInfo exponential op=lambda inp args kwargs wrapper_set_seed torch Tensor exponential_ inp args kwargs inplace_variant=torch Tensor exponential_ dtypes=floating_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat supports_out=False supports_autograd=False allow_cow_input_materialize_forward= sample_inputs_func=sample_inputs_exponential error_inputs_func=error_inputs_exponential skips= Tests assume input tensor has meaningful effect output tensor DecorateInfo unittest expectedFailure TestCommon test_variant_consistency_eager DecorateInfo unittest expectedFailure TestMathBits test_neg_view AssertionError JIT Test does execute any logic DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit AssertionError Tensor-likes close DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu FX failed normalize op - add op op_skip list DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive vmap calling random operator supported DecorateInfo unittest expectedFailure TestVmapOperatorsOpInfo test_vmap_exhaustive DecorateInfo unittest expectedFailure TestVmapOperatorsOpInfo test_op_has_batch_rule DecorateInfo unittest expectedFailure TestDecomp test_quick DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu OpInfo geometric op=lambda inp args kwargs wrapper_set_seed torch Tensor geometric_ inp args kwargs inplace_variant=torch Tensor geometric_ dtypes=floating_types_and torch float torch bfloat torch int torch int torch int torch int torch uint dtypesIfHpu=custom_types torch float torch bfloat supports_out=False supports_autograd=False allow_cow_input_materialize_forward= sample_inputs_func=sample_inputs_geometric error_inputs_func=error_inputs_geometric skips= Tests assume input tensor has meaningful effect output tensor DecorateInfo unittest expectedFailure TestCommon test_variant_consistency_eager DecorateInfo unittest expectedFailure TestMathBits test_neg_view AssertionError JIT Test does execute any logic DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit AssertionError Tensor-likes close DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu FX failed normalize op - add op op_skip list DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive vmap calling random operator supported DecorateInfo unittest skip Test expects tensor input TestVmapOperatorsOpInfo test_vmap_exhaustive DecorateInfo unittest skip Test expects tensor input TestVmapOperatorsOpInfo test_op_has_batch_rule DecorateInfo unittest expectedFailure TestDecomp test_quick OpInfo log_normal op=lambda inp args kwargs wrapper_set_seed torch Tensor log_normal_ inp args kwargs inplace_variant=torch Tensor log_normal_ dtypes=floating_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat supports_out=False supports_autograd=False allow_cow_input_materialize_forward= sample_inputs_func=sample_inputs_log_normal error_inputs_func=error_inputs_log_normal skips= Tests assume input tensor has meaningful effect output tensor DecorateInfo unittest expectedFailure TestCommon test_variant_consistency_eager DecorateInfo unittest expectedFailure TestMathBits test_neg_view AssertionError JIT Test does execute any logic DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit AssertionError Tensor-likes close DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu FX failed normalize op - add op op_skip list DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive vmap calling random operator supported DecorateInfo unittest skip Test expects tensor input TestVmapOperatorsOpInfo test_vmap_exhaustive DecorateInfo unittest skip Test expects tensor input TestVmapOperatorsOpInfo test_op_has_batch_rule DecorateInfo unittest expectedFailure TestDecomp test_quick OpInfo normal variant_test_name= in_place op=lambda inp args kwargs wrapper_set_seed torch Tensor normal_ inp args kwargs inplace_variant=torch Tensor normal_ dtypes=floating_and_complex_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat supports_out=False supports_autograd=False allow_cow_input_materialize_forward= sample_inputs_func=sample_inputs_normal error_inputs_func=error_inputs_normal skips= Tests assume input tensor sequence tensors DecorateInfo unittest skip Test expects tensor input TestCommon test_noncontiguous_samples Tests assume input tensor has meaningful effect output tensor DecorateInfo unittest expectedFailure TestCommon test_variant_consistency_eager DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view DecorateInfo unittest expectedFailure TestDecomp test_quick AssertionError JIT Test does execute any logic DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit AssertionError Tensor-likes close DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu FX failed normalize op - add op op_skip list DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive vmap calling random operator supported DecorateInfo unittest skip Test expects tensor input TestVmapOperatorsOpInfo test_vmap_exhaustive DecorateInfo unittest skip Test expects tensor input TestVmapOperatorsOpInfo test_op_has_batch_rule OpInfo uniform op=lambda inp args kwargs wrapper_set_seed torch Tensor uniform_ inp args kwargs method_variant=None inplace_variant=torch Tensor uniform_ dtypes=floating_and_complex_types_and torch bfloat torch float dtypesIfHpu=custom_types torch float torch bfloat supports_out=False supports_autograd=False is_factory_function=False allow_cow_input_materialize_forward= sample_inputs_func=sample_inputs_uniform error_inputs_func=error_inputs_uniform skips= FX failed normalize op - add op op_skip list DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive Tests assume input tensor has meaningful effect output tensor DecorateInfo unittest expectedFailure TestCommon test_variant_consistency_eager DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view AssertionError JIT Test does execute any logic DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit aten uniform decomposed DecorateInfo unittest expectedFailure TestDecomp test_quick DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu BinaryUfuncInfo clamp_max ref=_clamp_max_numpy dtypes=all_types_and torch bool torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int torch int supports_forward_ad=True supports_rhs_python_scalar=False supports_fwgrad_bwgrad=True rhs_make_tensor_kwargs=dict exclude_zero=False skips= RuntimeError max_elementwise_cuda implemented ComplexFloat DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion device_type= cuda dispatch lazy test failed DecorateInfo unittest expectedFailure TestLazyOpInfo test_dispatched_to_lazy test error disabled since rhs non-tensor python scalar supported DecorateInfo unittest expectedFailure TestCommon test_errors BinaryUfuncInfo clamp_min ref=_clamp_min_numpy dtypes=all_types_and torch bool torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int torch int supports_forward_ad=True supports_rhs_python_scalar=False supports_fwgrad_bwgrad=True rhs_make_tensor_kwargs=dict exclude_zero=False skips= RuntimeError min_elementwise_cuda implemented ComplexFloat DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion device_type= cuda dispatch lazy test failed DecorateInfo unittest expectedFailure TestLazyOpInfo test_dispatched_to_lazy test error disabled since rhs non-tensor python scalar supported DecorateInfo unittest expectedFailure TestCommon test_errors BinaryUfuncInfo mul aliases= multiply dtypes=all_types_and_complex_and torch chalf torch float torch bfloat torch bool dtypesIfHpu=custom_types torch float torch bfloat assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_two_python_scalars=True error_inputs_sparse_func=error_inputs_sparse_mul sample_inputs_sparse_coo_func=partial sample_inputs_sparse_mul layout=torch sparse_coo sample_inputs_sparse_csr_func=partial sample_inputs_sparse_mul layout=torch sparse_csr sample_inputs_sparse_csc_func=partial sample_inputs_sparse_mul layout=torch sparse_csc sample_inputs_sparse_bsr_func=partial sample_inputs_sparse_mul layout=torch sparse_bsr sample_inputs_sparse_bsc_func=partial sample_inputs_sparse_mul layout=torch sparse_bsc BinaryUfuncInfo sub NumPy has no builtin reference alpha kwarg easy enough emulate ref=lambda input other alpha= np subtract input np multiply alpha other aliases= subtract dtypes=all_types_and_complex_and torch bfloat torch float torch chalf dtypesIfHpu=custom_types torch float torch bfloat assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_add_sub supports_two_python_scalars=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= torch bfloat tol atol= e- rtol= e- torch complex tol atol= e- rtol= e- TestBinaryUfuncs test_reference_numerics DecorateInfo toleranceOverride torch chalf tol atol= e- rtol= TestCommon test_complex_half_reference_testing device_type= cpu DecorateInfo toleranceOverride torch chalf tol atol= e- rtol= TestDecomp test_comprehensive device_type= cpu DecorateInfo toleranceOverride torch chalf tol atol= e- rtol= TestDecomp test_quick device_type= cpu skips= DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics dtypes= torch uint DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_small_values dtypes= torch uint OpInfo addmm This addmm OpInfo when alpha beta both equal alpha=beta= tested following opinfo because special case will trigger addmm being decomposed jit pass dtypes=all_types_and_complex_and torch float torch bfloat dtypesIfROCM=floating_and_complex_types_and torch float torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True gradcheck_nondet_tol=GRADCHECK_NONDET_TOL sample_inputs_func=sample_inputs_addmm skips= Issue conj torch dispatch see https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestSchemaCheckModeOpInfo test_schema_correctness dtypes= torch complex torch complex DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestConsistency test_output_grad_match device_type= mps OpInfo addmm When alpha=beta= compile-time constants JIT will decompose addmm into mm add variant_test_name= decomposed dtypes=all_types_and_complex_and torch float torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True gradcheck_nondet_tol=GRADCHECK_NONDET_TOL autodiff_nonfusible_nodes= aten add aten mm sample_inputs_func=partial sample_inputs_addmm alpha= beta= skips= Issue conj torch dispatch see https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestSchemaCheckModeOpInfo test_schema_correctness dtypes= torch complex torch complex https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestNNCOpInfo test_nnc_correctness device_type= cpu dtypes= torch float OpInfo addmv dtypes=all_types_and_complex_and torch bfloat torch float dtypesIfCUDA=floating_types_and torch float torch complex torch complex torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True decorators= DecorateInfo toleranceOverride torch half tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cpu DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestConsistency test_output_match device_type= mps DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestConsistency test_output_grad_match device_type= mps sample_inputs_func=sample_inputs_addmv OpInfo addbmm ref=lambda M batch batch beta= alpha= np add np multiply np asarray beta dtype=M dtype M np multiply np asarray alpha dtype=batch dtype np sum np matmul batch batch axis= dtypes=all_types_and_complex_and torch bfloat torch float dtypesIfCUDA=floating_and_complex_types_and torch float torch bfloat SM OrLater TEST_WITH_ROCM dtypesIfHpu=custom_types torch float torch bfloat Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch complex tol atol= e- rtol= e- TestCommon test_numpy_refs MPS has slightly worse precision Is acceptable DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch complex tol atol= e- rtol= e- TestCommon test_numpy_ref_mps DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch bfloat tol atol= e- rtol= e- TestConsistency test_output_match DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestCommon test_out DecorateInfo toleranceOverride torch half tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cpu skips= NVIDIA only assures bfloat supported bmm SM = DecorateInfo unittest skip Skipped TestCommon test_dtypes device_type= cuda active_if=not SM OrLater addbmm does correctly warn when resizing out= inputs DecorateInfo unittest expectedFailure TestCommon test_out_warning https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager sample_inputs_func=sample_inputs_addbmm OpInfo baddbmm dtypes=all_types_and_complex_and torch bfloat torch float dtypesIfCUDA=floating_types_and torch float torch complex torch complex torch bfloat backward_dtypesIfCUDA=floating_types_and torch float torch bfloat SM OrLater TEST_WITH_ROCM torch complex torch complex Runs very slowly slow gradcheck - alternatively reduce input sizes dtypesIfHpu=custom_types torch float torch bfloat gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True decorators= DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestCommon test_variant_consistency_eager device_type= cuda Higher differences starting Zen Alder Lake DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestDecomp test_quick device_type= cpu DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestMathBits test_conj_view device_type= cuda sample_inputs_func=sample_inputs_baddbmm skips= Issue conj torch dispatch see https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestSchemaCheckModeOpInfo test_schema_correctness dtypes= torch complex torch complex OpInfo dot dtypes=all_types_and_complex_and torch float torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat assert_autodiffed=True sample_inputs_func=sample_inputs_dot_vdot error_inputs_func=error_inputs_dot_vdot supports_forward_ad=True supports_fwgrad_bwgrad=True skips= Issue conj torch dispatch see https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestSchemaCheckModeOpInfo test_schema_correctness dtypes= torch complex torch complex OpInfo vdot dtypes=all_types_and_complex_and torch float torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat sample_inputs_func=sample_inputs_dot_vdot error_inputs_func=error_inputs_dot_vdot supports_forward_ad=True supports_fwgrad_bwgrad=True skips= Issue conj torch dispatch see https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestSchemaCheckModeOpInfo test_schema_correctness dtypes= torch complex torch complex OpInfo bmm dtypes=all_types_and_complex_and torch float torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch float torch bfloat SM OrLater TEST_WITH_ROCM dtypesIfHpu=custom_types torch float torch bfloat assert_autodiffed=True assert_jit_shape_analysis=True supports_forward_ad=True supports_fwgrad_bwgrad=True skips= NVIDIA only assures bfloat supported bmm SM = DecorateInfo unittest skip Skipped TestCommon test_dtypes device_type= cuda active_if=not SM OrLater DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestCommon test_out Fast math MacOS- DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestConsistency test_output_match active_if=lambda _ MACOS_VERSION device_type= mps dtypes= torch float sample_inputs_func=sample_inputs_bmm OpInfo mv dtypes=all_types_and_complex_and torch float torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_mv OpInfo addr dtypes=all_types_and_complex_and torch bool torch bfloat torch float Reference https github com pytorch pytorch issues supports_forward_ad=True supports_fwgrad_bwgrad=True skips= Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager dtypes=all_types_and_complex_and torch bool torch bfloat torch float sample_inputs_func=sample_inputs_addr gradcheck_nondet_tol=GRADCHECK_NONDET_TOL OpInfo addcmul dtypes=all_types_and_complex_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True skips= TODO update sample inputs for_inplace_variant kwarg support test DecorateInfo unittest expectedFailure TestCommon test_variant_consistency_eager sample_inputs_func=sample_inputs_addcmul_addcdiv reference_inputs_func=partial reference_inputs_elementwise_ternary sample_inputs_func=reference_inputs_addcmul_addcdiv OpInfo addcdiv dtypes=floating_and_complex_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True skips= TODO update sample inputs for_inplace_variant kwarg support test DecorateInfo unittest expectedFailure TestCommon test_variant_consistency_eager sample_inputs_func=sample_inputs_addcmul_addcdiv reference_inputs_func=partial reference_inputs_elementwise_ternary sample_inputs_func=reference_inputs_addcmul_addcdiv UnaryUfuncInfo asin aliases= arcsin ref=np arcsin domain= - supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and_complex_and torch chalf torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat assert_autodiffed=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestUnaryUfuncs device_type= cuda DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cuda DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu precisionOverride torch bfloat e- skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped sparse backward supported TestSparseUnaryUfuncs test_sparse_fn_grad NOTE derivative inplace asinh implemented UnaryUfuncInfo asinh aliases= arcsinh ref=np arcsinh dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and_complex_and torch chalf torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat decorators= precisionOverride torch bfloat e- supports_inplace_autograd=False supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True promotes_int_to_float=True skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_small device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_normal device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped sparse backward supported TestSparseUnaryUfuncs test_sparse_fn_grad UnaryUfuncInfo atan aliases= arctan ref=np arctan dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and_complex_and torch chalf torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True promotes_int_to_float=True decorators= precisionOverride torch bfloat e- skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch cfloat torch cdouble active_if= IS_MACOS IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch cfloat torch cdouble active_if= IS_MACOS IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_small dtypes= torch cfloat torch cdouble active_if= IS_MACOS IS_WINDOWS DecorateInfo unittest skip Skipped sparse backward supported TestSparseUnaryUfuncs test_sparse_fn_grad BinaryUfuncInfo atan aliases= arctan dtypes=all_types_and torch bool torch bfloat torch half dtypesIfHpu=custom_types torch float torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True supports_rhs_python_scalar=False skips= Incorrectly attempts use scalar second argument DecorateInfo unittest expectedFailure TestJit test_jit_alias_remapping UnaryUfuncInfo atanh aliases= arctanh ref=np arctanh domain= - dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and_complex_and torch chalf torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat decorators= precisionOverride torch bfloat e- DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cuda DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestConsistency test_output_grad_match device_type= mps supports_inplace_autograd=False supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True promotes_int_to_float=True skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_small device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda dtypes= torch cfloat torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cuda dtypes= torch cfloat active_if=IS_WINDOWS DecorateInfo unittest skip Skipped sparse backward supported TestSparseUnaryUfuncs test_sparse_fn_grad OpInfo allclose dtypes=floating_and_complex_types_and torch float torch bfloat ref=np allclose supports_autograd=False supports_forward_ad=False sample_inputs_func=sample_inputs_allclose skips= DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit DecorateInfo unittest skip Skipped TestNNCOpInfo test_nnc_correctness DecorateInfo unittest skip Skipped TestCudaFuserOpInfo supports_out=False OpInfo broadcast_to ref=np broadcast_to dtypes=all_types_and_complex_and torch bool torch float torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False sample_inputs_func=sample_inputs_broadcast_to OpInfo broadcast_shapes op=torch broadcast_shapes ref=np broadcast_shapes np lib NumpyVersion np __version__ = None dtypes=_dispatch_dtypes torch float supports_out=False supports_gradgrad=False assert_autodiffed=False supports_autograd=False supports_scripting=False sample_inputs_func=sample_inputs_broadcast_shapes skips= https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive skip dtype tests since broadcast_shape device dependent having dtypes limited torch float would cause test_dtypes report unexpected success DecorateInfo unittest skip Skipped TestCommon test_dtypes skip these tests since we have non tensor input DecorateInfo unittest skip Skipped TestCommon test_noncontiguous_samples DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit OpInfo broadcast_tensors ref=np broadcast_arrays dtypes=all_types_and_complex_and torch bool torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat sample_inputs_func=sample_inputs_broadcast_tensors reference_inputs_func=reference_inputs_broadcast_tensors supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False skips= https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive JIT does support variadic tensors RuntimeError input- type - kind == TypeKind OptionalType INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float OpInfo block_diag dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True Default batching rule core doesn t work ops TensorList args check_batched_forward_grad=False skips= https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive JIT does support variadic tensors RuntimeError input- type - kind == TypeKind OptionalType INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float sample_inputs_func=sample_inputs_block_diag UnaryUfuncInfo bitwise_not ref=np bitwise_not dtypes=integral_types_and torch bool dtypesIfHpu=custom_types torch bool operator_variant=operator invert supports_autograd=False BinaryUfuncInfo bitwise_left_shift op=torch bitwise_left_shift dtypes=integral_types dtypesIfCUDA=integral_types dtypesIfHpu=custom_types torch int torch int torch bool operator_variant=operator lshift inplace_operator_variant=operator ilshift supports_autograd=False supports_one_python_scalar=True rhs_make_tensor_kwargs=dict low= skips= DecorateInfo unittest skip Skipped TestBinaryUfuncs test_type_promotion https github com pytorch pytorch issues DecorateInfo unittest skip Some inputs produce undefined outputs TestCommon test_compare_cpu BinaryUfuncInfo bitwise_right_shift op=torch bitwise_right_shift dtypes=integral_types dtypesIfCUDA=integral_types dtypesIfHpu=custom_types torch int torch int torch bool operator_variant=operator rshift inplace_operator_variant=operator irshift supports_autograd=False supports_one_python_scalar=True rhs_make_tensor_kwargs=dict low= skips= DecorateInfo unittest skip Skipped TestBinaryUfuncs test_type_promotion https github com pytorch pytorch issues DecorateInfo unittest skip Some inputs produce undefined outputs TestCommon test_compare_cpu OpInfo combinations op=torch combinations dtypes=all_types_and_complex_and torch bool torch float torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False supports_out=False sample_inputs_func=sample_inputs_combinations OpInfo cartesian_prod op=torch cartesian_prod dtypes=all_types_and_complex_and torch bool torch float torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False sample_inputs_func=sample_inputs_cartesian_prod skips= DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError input- type - kind == TypeKind OptionalType INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float OpInfo cdist dtypes=floating_types supports_out=False supports_gradgrad=False assert_autodiffed=False sample_inputs_func=sample_inputs_cdist UnaryUfuncInfo ceil ref=np ceil dtypes=all_types_and torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int torch int supports_forward_ad=True supports_fwgrad_bwgrad=True skips= DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness dtypes=tuple t t integral_types t = torch uint supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True assert_autodiffed=True OpInfo cholesky dtypes=floating_and_complex_types sample_inputs_func=sample_inputs_linalg_cholesky gradcheck_wrapper=gradcheck_wrapper_hermitian_input decorators= skipCUDAIfNoMagma skipCPUIfNoLapack OpInfo cholesky_inverse dtypes=floating_and_complex_types backward_dtypes=floating_and_complex_types https github com pytorch pytorch issues gradcheck_fast_mode=True supports_fwgrad_bwgrad=True supports_forward_ad=True check_batched_gradgrad=True sample_inputs_func=sample_inputs_linalg_cholesky_inverse gradcheck_wrapper=gradcheck_wrapper_triangular_input_real_positive_diagonal decorators= skipCUDAIfNoMagma skipCPUIfNoLapack DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestCommon device_type= cpu DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestEagerFusionOpInfo device_type= cpu skips= Strides same Original strides strides now DecorateInfo unittest expectedFailure TestCommon test_out OpInfo cholesky_solve op=torch cholesky_solve dtypes=floating_and_complex_types sample_inputs_func=sample_inputs_cholesky_solve check_batched_gradgrad=False supports_forward_ad=True supports_fwgrad_bwgrad=True gradcheck_wrapper=lambda args kwargs gradcheck_wrapper_triangular_input args idx= kwargs decorators= skipCUDAIfNoMagma skipCPUIfNoLapack OpInfo chunk dtypes=all_types_and_complex_and torch bool torch bfloat torch float torch chalf dtypesIfHpu=custom_types torch float torch bfloat torch int sample_inputs_func=sample_inputs_chunk reference_inputs_func=reference_inputs_chunk supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False OpInfo unsafe_chunk dtypes=all_types_and_complex_and torch bool torch bfloat torch float torch chalf sample_inputs_func=sample_inputs_chunk check_batched_forward_grad=False reference_inputs_func=reference_inputs_chunk supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False OpInfo clone ref=np copy dtypes=all_types_and_complex_and torch bool torch bfloat torch float torch chalf dtypesIfHpu=custom_types torch float torch bfloat torch int torch int torch bool sample_inputs_func=sample_inputs_clone_contiguous reference_inputs_func=reference_inputs_clone_contiguous supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False skips= TypeError _copy_dispatcher got unexpected keyword argument memory_format NumPy reference needs extended memory_format DecorateInfo unittest expectedFailure TestCommon test_numpy_ref DecorateInfo unittest expectedFailure TestCommon test_numpy_ref_mps OpInfo contiguous op=lambda x args kwargs x contiguous args kwargs dtypes=all_types_and_complex_and torch bool torch bfloat torch float torch chalf sample_inputs_func=sample_inputs_clone_contiguous reference_inputs_func=reference_inputs_clone_contiguous supports_forward_ad=True supports_fwgrad_bwgrad=True autodiff_fusible_nodes= aten contiguous assert_jit_shape_analysis=True supports_out=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive OpInfo sum_to_size op=lambda x args kwargs x sum_to_size args kwargs dtypes=all_types_and_complex_and torch bool torch float torch bfloat sample_inputs_func=sample_inputs_sum_to_size error_inputs_func=error_inputs_sum_to_size supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False skips= lambda impl DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float OpInfo clamp aliases= clip ref=_clamp_numpy dtypes=all_types_and torch bfloat torch half dtypesIfHpu=custom_types torch float torch bfloat torch int torch int torch bool sample_inputs_func=sample_inputs_clamp reference_inputs_func=partial reference_inputs_elementwise_ternary sample_inputs_func=sample_inputs_clamp assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True skips= NNC appear handle boolean clamp DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness dtypes= torch bool MPS does support float while numpy does internal computations float See https github com pytorch pytorch blob c cf fde bdbe f ffb d c b c test test_ops py#L -L DecorateInfo unittest expectedFailure TestCommon test_numpy_ref_mps UnaryUfuncInfo positive ref=np positive dtypes=all_types_and_complex_and torch half torch bfloat torch chalf supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True UnaryUfuncInfo conj ref=np conj dtypes=all_types_and_complex_and torch bool torch bfloat torch half torch chalf dtypesIfHpu=custom_types torch float torch int supports_sparse=True supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False supports_out=False UnaryUfuncInfo conj_physical decomp_aten_name= _conj_physical ref=np conj dtypes=all_types_and_complex_and torch bool torch bfloat torch half torch chalf supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True skips= RuntimeError inputSet outputSet INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes= torch float DecorateInfo unittest skip Skipped conj_physical_ implemented sparse TestSparseUnaryUfuncs test_inplace OpInfo resolve_conj dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat sample_inputs_func=sample_inputs_view_as_real supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False OpInfo resolve_neg dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf dtypesIfHpu=custom_types torch float torch bfloat sample_inputs_func=sample_inputs_view_as_real supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False OpInfo view_as_real dtypes=complex_types supports_forward_ad=True supports_out=False supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_view_as_real test_conjugated_samples=False OpInfo view_as_complex dtypes=floating_types_and torch half supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True test_neg_view=False sample_inputs_func=sample_inputs_view_as_complex skips= RuntimeError Tensor must have last dimension stride DecorateInfo unittest expectedFailure TestCommon test_noncontiguous_samples RuntimeError eq_cpu implemented ComplexHalf DecorateInfo unittest skip Skipped TestNNCOpInfo test_nnc_correctness dtypes= torch half RuntimeError view size compatible input tensor s size stride DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace_all_strides BinaryUfuncInfo complex dtypes=floating_types_and torch half supports_forward_ad=True supports_fwgrad_bwgrad=True supports_rhs_python_scalar=False error_inputs_func=error_inputs_complex skips= Tests don t account complex s type promotion semantics DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion DecorateInfo unittest skip Skipped TestCommon test_out device_type= mps DecorateInfo unittest expectedFailure TestMeta test_binary_ufuncs_mixed_dtype BinaryUfuncInfo copysign sample_inputs_func=sample_inputs_copysign dtypes=all_types_and torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat promotes_int_to_float=True https github com pytorch pytorch issues gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True OpInfo corrcoef dtypes=all_types_and_complex_and torch half torch bfloat sample_inputs_func=sample_inputs_corrcoef supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False skips= Issue conj torch dispatch see https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestSchemaCheckModeOpInfo test_schema_correctness dtypes= torch complex torch complex supports_out=False UnaryUfuncInfo cos ref=np cos dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and_complex_and torch chalf torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat assert_autodiffed=True handles_large_floats=False supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True decorators= precisionOverride torch bfloat e- skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch cfloat torch cdouble device_type= cpu active_if=IS_WINDOWS This fails CUDA passes ROCm DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch cdouble device_type= cuda DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch cfloat torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble active_if=IS_MACOS AssertionError Tensor-likes close Greatest absolute difference nan index up e- allowed Greatest relative difference nan index up allowed DecorateInfo unittest expectedFailure TestUnaryUfuncs test_reference_numerics_large device_type= cuda dtypes= torch chalf active_if=IS_WINDOWS UnaryUfuncInfo cosh ref=np_unary_ufunc_integer_promotion_wrapper np cosh dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and_complex_and torch chalf torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True skips= Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cpu dtypes= torch int DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch cfloat torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch cfloat torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble active_if=IS_MACOS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cpu dtypes= torch cfloat torch cdouble active_if=IS_MACOS AssertionError Tensor-likes close Greatest absolute difference nan index up e- allowed Greatest relative difference nan index up allowed DecorateInfo unittest expectedFailure TestUnaryUfuncs test_reference_numerics_large device_type= cuda dtypes= torch chalf active_if=IS_WINDOWS OpInfo cov dtypes=all_types_and_complex_and torch half torch bfloat sample_inputs_func=sample_inputs_cov error_inputs_func=error_inputs_cov supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True skips= Issue conj torch dispatch see https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestSchemaCheckModeOpInfo test_schema_correctness dtypes= torch complex torch complex Float did match double DecorateInfo unittest expectedFailure TestBwdGradients test_fn_grad Jacobian mismatch DecorateInfo unittest expectedFailure TestBwdGradients test_fn_gradgrad DecorateInfo unittest expectedFailure TestFwdGradients test_forward_mode_AD DecorateInfo unittest skip Barely fails TestFwdGradients test_fn_fwgrad_bwgrad JIT test working tensor kwargs https github com pytorch pytorch issues RuntimeError undefined value tensor File string line the_method i torch cov i correction= fweights=None aweights=tensor dtype=torch float requires_grad=True noqa B ~~~~~~ --- HERE DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cpu DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestConsistency test_output_grad_match device_type= mps OpInfo cross dtypes=all_types_and_complex_and torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int sample_inputs_func=sample_inputs_cross supports_fwgrad_bwgrad=True supports_out=True supports_forward_ad=True OpInfo cumsum dtypes=all_types_and_complex_and torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int supports_forward_ad=True supports_fwgrad_bwgrad=True skips= cumsum does handle correctly out= dtypes DecorateInfo unittest expectedFailure TestCommon test_out sample_inputs_func=sample_inputs_cumulative_ops OpInfo cumprod dtypes=all_types_and_complex_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True skips= cumprod does handle correctly out= dtypes DecorateInfo unittest expectedFailure TestCommon test_out gradgradcheck fails fast_mode=True sample_inputs_func=sample_inputs_cumprod gradcheck_fast_mode=False OpInfo cummax dtypes=all_types_and torch bool torch half torch bfloat sample_inputs_func=partial sample_inputs_cumulative_ops supports_dtype_kwargs=False supports_forward_ad=True supports_fwgrad_bwgrad=True skips= gradcheck_nondet_tol=GRADCHECK_NONDET_TOL OpInfo cummin dtypes=all_types_and torch bool torch half torch bfloat sample_inputs_func=partial sample_inputs_cumulative_ops supports_dtype_kwargs=False supports_forward_ad=True supports_fwgrad_bwgrad=True skips= gradcheck_nondet_tol=GRADCHECK_NONDET_TOL UnaryUfuncInfo deg rad ref=np radians decorators= precisionOverride torch bfloat e- torch float e- dtypes=all_types_and torch bool torch half torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True promotes_int_to_float=True OpInfo diff op=torch diff np diff has np _NoValue default values prepend append compare_with_reference breaks prepend append set None when converting numpy ref=lambda input n= dim=- prepend=np _NoValue append=np _NoValue np diff input n dim np _NoValue prepend None prepend np _NoValue append None append dtypes=all_types_and_complex_and torch bool torch float torch bfloat Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_diff error_inputs_func=error_inputs_diff See https github com pytorch pytorch pull check_batched_forward_grad=False skips= BinaryUfuncInfo div aliases= divide variant_test_name= no_rounding_mode dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and_complex_and torch bool torch half torch bfloat torch chalf dtypesIfHpu=custom_types torch float torch bfloat torch int torch int Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_forward_ad=True promotes_int_to_float=True supports_fwgrad_bwgrad=True supports_two_python_scalars=True assert_autodiffed=True rhs_make_tensor_kwargs=dict exclude_zero=True BinaryUfuncInfo div aliases= divide variant_test_name= trunc_rounding dtypes=all_types_and torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int torch int sample_kwargs=lambda device dtype input rounding_mode trunc rounding_mode trunc https github com pytorch pytorch issues gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_two_python_scalars=True assert_autodiffed=True rhs_make_tensor_kwargs=dict exclude_zero=True decorators= See https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion skips= RuntimeError MALFORMED INPUT Unhandled node kind computeValue aten div DecorateInfo unittest expectedFailure TestNNCOpInfo test_working FIXME torch autograd gradcheck GradcheckError Jacobian mismatch output respect input numerical tensor - dtype=torch float analytical tensor dtype=torch float DecorateInfo unittest skip Skipped TestBwdGradients test_fn_grad device_type= cpu dtypes= torch float BinaryUfuncInfo div aliases= divide variant_test_name= floor_rounding dtypes=all_types_and torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int torch int sample_kwargs=lambda device dtype input rounding_mode floor rounding_mode floor https github com pytorch pytorch issues gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_two_python_scalars=True assert_autodiffed=True rhs_make_tensor_kwargs=dict exclude_zero=True decorators= See https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion skips= RuntimeError MALFORMED INPUT Unhandled node kind computeValue aten div DecorateInfo unittest expectedFailure TestNNCOpInfo test_working FIXME torch autograd gradcheck GradcheckError Jacobian mismatch output respect input numerical tensor - dtype=torch float analytical tensor dtype=torch float DecorateInfo unittest skip Skipped TestBwdGradients test_fn_grad dtypes= torch float device_type= cpu DecorateInfo unittest skip Broken MacOS TestConsistency test_output_match device_type= mps dtypes= torch float active_if=lambda _ MACOS_VERSION BinaryUfuncInfo true_divide dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_forward_ad=True promotes_int_to_float=True supports_fwgrad_bwgrad=True supports_two_python_scalars=True rhs_make_tensor_kwargs=dict exclude_zero=True OpInfo equal dtypes=all_types_and_complex_and torch bool torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int torch int torch bool ref=lambda input other input == other all sample_inputs_func=sample_inputs_equal supports_autograd=False supports_tracing=False skips= UnaryUfuncInfo exp ref=np_unary_ufunc_integer_promotion_wrapper np exp dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and_complex_and torch bool torch half torch bfloat torch chalf dtypesIfHpu=custom_types torch float torch bfloat skips= Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble active_if=IS_WINDOWS assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True OpInfo expand op=lambda shape expand shape dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int sample_inputs_func=sample_inputs_expand supports_forward_ad=True supports_fwgrad_bwgrad=True assert_jit_shape_analysis=True supports_out=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive OpInfo expand_as op=lambda other expand_as other dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int torch int torch bool supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_expand_as supports_out=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive OpInfo expand_copy dtypes=all_types_and_complex_and torch bool torch half torch bfloat sample_inputs_func=sample_inputs_expand supports_forward_ad=True supports_fwgrad_bwgrad=True assert_jit_shape_analysis=True supports_out=True skips= DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float OpInfo diag ref=np diag dtypes=all_types_and_complex_and torch bool torch bfloat torch float dtypesIfCUDA=all_types_and_complex_and torch chalf torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True check_batched_forward_grad=False sample_inputs_func=sample_inputs_diag error_inputs_func=error_inputs_diag OpInfo diag_embed dtypes=all_types_and_complex_and torch bool torch bfloat torch float torch chalf supports_out=False Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_diagonal_diag_embed reference_inputs_func=reference_inputs_diagonal_diag_embed error_inputs_func=error_inputs_diagonal_diag_embed OpInfo diagonal aten_backward_name= diagonal_backward dtypes=all_types_and_complex_and torch bool torch bfloat torch float torch chalf supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_diagonal_diag_embed reference_inputs_func=reference_inputs_diagonal_diag_embed error_inputs_func=error_inputs_diagonal_diag_embed OpInfo diagonal_copy dtypes=all_types_and_complex_and torch bool torch bfloat torch float torch chalf supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_diagonal_diag_embed reference_inputs_func=reference_inputs_diagonal_diag_embed error_inputs_func=error_inputs_diagonal_diag_embed OpInfo diagonal_scatter dtypes=all_types_and_complex_and torch bool torch bfloat torch float supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_diagonal_scatter OpInfo alias_copy dtypes=all_types_and_complex_and torch bool torch bfloat torch float torch chalf sample_inputs_func=sample_inputs_alias_copy supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=True BinaryUfuncInfo eq ref=np equal dtypes=all_types_and_complex_and torch bool torch bfloat torch float torch chalf dtypesIfHpu=custom_types torch float torch bfloat torch int torch int torch bool always_returns_bool=True supports_autograd=False sample_inputs_func=sample_inputs_comparison_ops skips= BinaryUfuncInfo fmax op=torch fmax dtypes=all_types_and torch float torch bfloat torch bool dtypesIfHpu=custom_types torch float torch bfloat torch int torch int torch bool supports_forward_ad=True supports_fwgrad_bwgrad=True supports_rhs_python_scalar=False skips= RuntimeError max_elementwise_cuda implemented ComplexFloat DecorateInfo unittest skip Skipped TestBinaryUfuncs test_type_promotion BinaryUfuncInfo fmin op=torch fmin dtypes=all_types_and torch float torch bfloat torch bool dtypesIfHpu=custom_types torch float torch bfloat torch int torch int torch bool supports_forward_ad=True supports_fwgrad_bwgrad=True supports_rhs_python_scalar=False skips= RuntimeError min_elementwise_cuda implemented ComplexFloat DecorateInfo unittest skip Skipped TestBinaryUfuncs test_type_promotion BinaryUfuncInfo fmod ref=np fmod dtypes=all_types_and torch float torch bfloat dtypesIfCUDA=all_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int https github com pytorch pytorch issues gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True assert_autodiffed=None rhs_make_tensor_kwargs= exclude_zero True decorators= DecorateInfo unittest skip Skipped TestBinaryUfuncs test_contig_vs_every_other dtypes= torch bfloat DecorateInfo unittest skip Skipped TestBinaryUfuncs test_non_contig dtypes= torch bfloat DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics dtypes= torch bfloat DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_small_values dtypes= torch uint FIXME torch autograd gradcheck GradcheckError Jacobian mismatch output respect input numerical tensor dtype=torch float analytical tensor - dtype=torch float DecorateInfo unittest skip Skipped TestBwdGradients test_fn_grad dtypes= torch float device_type= cpu BinaryUfuncInfo remainder ref=np remainder dtypes=all_types_and torch float torch bfloat dtypesIfCUDA=all_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int torch int torch bool https github com pytorch pytorch issues gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True assert_autodiffed=None operator_variant=operator mod inplace_operator_variant=operator imod supports_one_python_scalar=True rhs_make_tensor_kwargs= exclude_zero True decorators= DecorateInfo unittest skip Skipped TestBinaryUfuncs test_contig_vs_every_other dtypes= torch bfloat DecorateInfo unittest skip Skipped TestBinaryUfuncs test_non_contig dtypes= torch bfloat DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics dtypes= torch bfloat DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_small_values dtypes= torch uint DecorateInfo unittest skip Skipped TestNNCOpInfo test_nnc_correctness dtypes= torch bfloat Fails XLA False true Tensors failed compare equal Attempted compare equality tensors different dtypes DecorateInfo unittest skip Skipped TestOpInfo device_type= xla dtypes= torch long FIXME torch autograd gradcheck GradcheckError Jacobian mismatch output respect input numerical tensor dtype=torch float analytical tensor - dtype=torch float DecorateInfo unittest skip Skipped TestBwdGradients test_fn_grad device_type= cpu dtypes= torch float DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cuda DecorateInfo unittest skip Broken MacOS TestConsistency test_output_match device_type= mps dtypes= torch float active_if=lambda _ MACOS_VERSION UnaryUfuncInfo frac ref=lambda x np modf x dtypes=floating_types_and torch bfloat torch float dtypesIfCUDA=floating_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch bfloat torch float torch float torch float DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness dtypes= torch bfloat torch float torch float OpInfo stft decorators= skipCPUIfNoFFT DecorateInfo unittest skip Skipped stft does match native function TestJit test_variant_consistency_jit dtypes=floating_and_complex_types sample_inputs_func=sample_inputs_stft Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True check_batched_forward_grad=False check_batched_grad=False check_batched_gradgrad=False supports_out=False gradcheck_nondet_tol=GRADCHECK_NONDET_TOL OpInfo istft dtypes=complex_types sample_inputs_func=sample_inputs_istft Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True check_batched_forward_grad=False check_batched_grad=False check_batched_gradgrad=False supports_out=False decorators= DecorateInfo unittest skip Skipped istft does match native function TestJit test_variant_consistency_jit skips= skipCPUIfNoFFT gradcheck fails ROCm gh- grad computed improperly probably weights tensor DecorateInfo unittest expectedFailure TestBwdGradients test_fn_grad Pre-existing condition calls item needs fixed DecorateInfo unittest expectedFailure TestCompositeCompliance test_backward UnaryUfuncInfo floor ref=np floor dtypes=all_types_and torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int torch int supports_forward_ad=True supports_fwgrad_bwgrad=True skips= DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness dtypes=tuple t t integral_types t = torch uint supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True assert_autodiffed=True OpInfo flip op=torch flip dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int torch int torch bool sample_inputs_func=sample_inputs_flip supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False OpInfo fliplr op=torch fliplr dtypes=all_types_and_complex_and torch bool torch half torch bfloat sample_inputs_func=sample_inputs_fliplr_flipud error_inputs_func=error_inputs_fliplr supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False OpInfo flipud op=torch flipud dtypes=all_types_and_complex_and torch bool torch half torch bfloat sample_inputs_func=sample_inputs_fliplr_flipud error_inputs_func=error_inputs_flipud supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False OpInfo sparse sampled_addmm dtypes=floating_and_complex_types supports_autograd=True sample_inputs_func=sample_inputs_sparse_sampled_addmm decorators= skipCUDAIf _get_torch_cuda_version = _get_torch_rocm_version = cusparseSDDMM added skipCPUIfNoMklSparse skipXPU skips= NotImplementedError Tensors type SparseCsrTensorImpl do have is_contiguous DecorateInfo unittest skip Skipped TestCommon test_noncontiguous_samples RuntimeError Sparse CSR tensors do have strides DecorateInfo unittest skip Skipped TestCommon test_out DecorateInfo unittest skip Skipped TestTags test_tags RuntimeError sampled_addmm Expected result have sparse csr layout got Strided DecorateInfo unittest skip Skipped TestCommon test_out_warning RuntimeError Sparse CSR tensors do have strides DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager RuntimeError Sparse CSR tensors do have strides DecorateInfo unittest skip Skipped TestCompositeCompliance test_operator RuntimeError Sparse CSR tensors do have strides DecorateInfo unittest skip Skipped TestCompositeCompliance test_backward RuntimeError Sparse CSR tensors do have strides DecorateInfo unittest skip Skipped TestMathBits test_conj_view RuntimeError Sparse CSR tensors do have strides DecorateInfo unittest skip Skipped TestMathBits test_neg_conj_view RuntimeError Sparse CSR tensors do have strides DecorateInfo unittest skip Skipped TestMathBits test_neg_view RuntimeError Sparse CSR tensors do have strides DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit RuntimeError unsupported memory format option Preserve DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit RuntimeError sparse_mask does support automatic differentiation outputs complex dtype RuntimeError Sparse CSR tensors do have strides DecorateInfo unittest skip Skipped TestFwdGradients test_fn_fwgrad_bwgrad ValueError Sparse output supported gradcheck yet Please call to_dense masked_grad= DecorateInfo unittest skip Skipped TestBwdGradients test_fn_grad RuntimeError sparse_mask does support automatic differentiation outputs complex dtype RuntimeError Sparse CSR tensors do have is_contiguous DecorateInfo unittest skip Skipped TestBwdGradients test_fn_gradgrad ValueError Sparse output supported gradcheck yet Please call to_dense masked_grad= DecorateInfo unittest skip Skipped TestFwdGradients test_forward_mode_AD NotImplementedError Could run aten sparse_sampled_addmm arguments SparseCsrMeta backend DecorateInfo unittest skip Skipped TestMeta test_dispatch_meta_outplace DecorateInfo unittest skip Skipped TestMeta test_dispatch_symbolic_meta_outplace DecorateInfo unittest skip Skipped TestMeta test_meta_outplace DecorateInfo unittest skip Skipped TestMeta test_dispatch_symbolic_meta_outplace_all_strides DecorateInfo unittest skip Skipped TestFakeTensor test_fake_crossref_backward_no_amp OpInfo sparse mm dtypes=floating_types_and torch bfloat torch float variant_test_name= reduce supports_autograd=True supports_out=False supports_gradgrad=False supports_forward_ad=False sample_inputs_func=sample_inputs_sparse_mm_reduce decorators= onlyCPU skips= NotImplementedError Tensors type SparseCsrTensorImpl do have is_contiguous DecorateInfo unittest skip Skipped TestCommon test_noncontiguous_samples RuntimeError Sparse CSR tensors do have strides DecorateInfo unittest skip Skipped TestTags test_tags RuntimeError Sparse CSR tensors do have strides DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager RuntimeError Sparse CSR tensors do have strides DecorateInfo unittest skip Skipped TestCompositeCompliance test_operator RuntimeError Sparse CSR tensors do have strides DecorateInfo unittest skip Skipped TestCompositeCompliance test_backward RuntimeError Sparse CSR tensors do have strides DecorateInfo unittest skip Skipped TestMathBits test_conj_view RuntimeError Sparse CSR tensors do have strides DecorateInfo unittest skip Skipped TestMathBits test_neg_conj_view RuntimeError Sparse CSR tensors do have strides DecorateInfo unittest skip Skipped TestMathBits test_neg_view RuntimeError Sparse CSR tensors do have strides DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit RuntimeError unsupported memory format option Preserve DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit ValueError Sparse output supported gradcheck yet Please call to_dense masked_grad= DecorateInfo unittest skip Skipped TestFwdGradients test_fn_fwgrad_bwgrad RuntimeError Sparse CSR tensors do have is_contiguou DecorateInfo unittest skip Skipped TestBwdGradients test_fn_grad ValueError Sparse output supported gradcheck yet Please call to_dense masked_grad= DecorateInfo unittest skip Skipped TestBwdGradients test_fn_gradgrad RuntimeError Sparse CSR tensors do have strides DecorateInfo unittest skip Skipped TestFwdGradients test_forward_mode_AD ValueError Sparse output supported gradcheck yet Please call to_dense masked_grad= DecorateInfo unittest skip Skipped TestBwdGradients test_fn_fail_gradgrad NotImplementedError Could run aten _sparse_mm_reduce_impl arguments SparseCsrMeta backend DecorateInfo unittest skip Skipped TestMeta test_dispatch_meta_outplace DecorateInfo unittest skip Skipped TestMeta test_dispatch_symbolic_meta_outplace DecorateInfo unittest skip Skipped TestMeta test_meta_outplace UnaryUfuncInfo i ref=np_unary_ufunc_integer_promotion_wrapper scipy special i TEST_SCIPY None aliases= special i decorators= precisionOverride torch bfloat e- torch float e- dtypes=all_types_and torch bool torch half torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True sample_inputs_func=sample_inputs_i _i skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch int BinaryUfuncInfo floor_divide ref=_floor_divide_np dtypes=all_types_and torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int torch int torch bool supports_autograd=False rhs_make_tensor_kwargs=dict exclude_zero=True supports_two_python_scalars=True skips= AssertionError Results original model exported imported version model differed DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit bfloat floor_divide compared float reference works inconsistently DecorateInfo unittest skip Skipped TestBinaryUfuncs dtypes= torch bfloat int floor divide has different results - - vs NumPy DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_small_values dtypes= torch int The following tests fails some jobs DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_extremal_values dtypes= torch float DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestBinaryUfuncs test_reference_numerics UnaryUfuncInfo frexp op=torch frexp ref=np frexp dtypes=floating_types_and torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat skip testing torch frexp supported ROCm platform yet decorators= supports_forward_ad=True supports_fwgrad_bwgrad=True skips= skips below tests torch frexp returns tuple-like mantissa exponent outputs while these tests currently requires output single tensor DecorateInfo unittest skip Skipped TestUnaryUfuncs test_batch_vs_slicing DecorateInfo unittest skip Skipped TestUnaryUfuncs test_contig_vs_every_other DecorateInfo unittest skip Skipped TestUnaryUfuncs test_contig_vs_transposed DecorateInfo unittest skip Skipped TestUnaryUfuncs test_non_contig_expand DecorateInfo unittest skip Skipped TestUnaryUfuncs test_variant_consistency DecorateInfo unittest skip Skipped TestUnaryUfuncs test_out_arg_all_dtypes skips test_reference_numerics due error Windows CI The np frexp returns exponent np intc dtype Windows platform np intc does have correspond torch dtype DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_small active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal active_if=IS_WINDOWS UnaryUfuncInfo log p ref=np log p aliases= special log p domain= - None dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat decorators= precisionOverride torch bfloat e- supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True assert_autodiffed=True promotes_int_to_float=True BinaryUfuncInfo ge ref=np greater_equal aliases= greater_equal dtypes=all_types_and torch bool torch bfloat torch float dtypesIfHpu=custom_types torch float torch bfloat torch int always_returns_bool=True supports_autograd=False skips= OpInfo geqrf dtypes=floating_and_complex_types sample_inputs_func=sample_inputs_linalg_qr_geqrf decorators= skipCUDAIfNoMagmaAndNoCusolver skipCPUIfNoLapack supports_autograd=False skips= FIXME geqrf can t forward complex inputs require grad DecorateInfo unittest expectedFailure TestCommon test_dtypes Strides same DecorateInfo unittest expectedFailure TestCommon test_out BinaryUfuncInfo gt ref=np greater aliases= greater dtypes=all_types_and torch bool torch bfloat torch float dtypesIfHpu=custom_types torch float torch bfloat torch int torch int always_returns_bool=True supports_autograd=False skips= UnaryUfuncInfo imag ref=np imag dtypes=complex_types_and torch chalf supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch issues RuntimeError view_as_real doesn t work unresolved conjugated tensors check_batched_forward_grad=False skips= Skip since real imag don t have out variants DecorateInfo unittest expectedFailure TestUnaryUfuncs test_out_arg_all_dtypes OpInfo gradient dtypes=floating_and_complex_types_and torch int torch int torch int torch int torch bfloat torch half supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive following tests give runtime error undefined value tensor see discussion https github com pytorch pytorch issues RuntimeError Arguments call valid DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes= torch float torch complex noqa B DecorateInfo unittest skip Skipped TestNNCOpInfo test_nnc_correctness DecorateInfo unittest skip Skipped TestCudaFuserOpInfo supports_inplace_autograd=False sample_inputs_func=sample_inputs_gradient error_inputs_func=error_inputs_gradient OpInfo isin dtypes=all_types_and torch bfloat torch half supports_autograd=False sample_inputs_func=sample_inputs_isin OpInfo kthvalue dtypes=all_types_and torch bfloat torch float dtypesIfHpu=custom_types torch float torch bfloat torch int supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_kthvalue error_inputs_func=error_inputs_kthvalue BinaryUfuncInfo le ref=np less_equal aliases= less_equal dtypes=all_types_and torch bool torch bfloat torch float always_returns_bool=True supports_autograd=False skips= OpInfo linspace dtypes=all_types_and_complex_and torch bfloat torch float dtypesIfHpu=custom_types torch float torch bfloat torch int is_factory_function=True supports_out=True supports_autograd=False error_inputs_func=error_inputs_linspace sample_inputs_func=sample_inputs_linspace skips= FX failed normalize op - add op op_skip list DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive Tests assume input tensor sequence tensors DecorateInfo unittest expectedFailure TestCommon test_variant_consistency_eager DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view Same failure arange cannot find linspace captured graph DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float UserWarning triggered Resized non-empty tensor did warn about DecorateInfo unittest expectedFailure TestCommon test_out_warning UserWarning CUDA caching allocator reports memory leak verified driver API __main__ TestJitCUDA test_variant_consistency_jit_logspace_cuda_complex Caching allocator allocated memory now reported device CUDA driver allocated memory now DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes= torch cfloat device_type= cuda OpInfo linspace dtypes=all_types_and_complex_and torch bfloat torch float dtypesIfHpu=custom_types torch float torch bfloat torch int is_factory_function=True supports_out=True supports_autograd=False error_inputs_func=error_inputs_linspace sample_inputs_func=sample_inputs_linspace_tensor_overload variant_test_name= tensor_overload skips= FX failed normalize op - add op op_skip list DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive TypeError int object subscriptable DecorateInfo unittest expectedFailure TestCommon test_variant_consistency_eager DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view Same failure arange cannot find linspace captured graph DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float UserWarning triggered Resized non-empty tensor did warn about DecorateInfo unittest expectedFailure TestCommon test_out_warning UserWarning CUDA caching allocator reports memory leak verified driver API __main__ TestJitCUDA test_variant_consistency_jit_logspace_cuda_complex Caching allocator allocated memory now reported device CUDA driver allocated memory now DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes= torch cfloat device_type= cuda OpInfo logspace dtypes=all_types_and_complex_and torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat is_factory_function=True supports_out=True supports_autograd=False error_inputs_func=error_inputs_linspace sample_inputs_func=sample_inputs_logspace skips= FX failed normalize op - add op op_skip list DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive Tests assume input tensor sequence tensors DecorateInfo unittest expectedFailure TestCommon test_variant_consistency_eager DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view Same failure arange cannot find linspace captured graph DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float UserWarning triggered Resized non-empty tensor did warn about DecorateInfo unittest expectedFailure TestCommon test_out_warning Off-by-one issue when casting floats ints DecorateInfo unittest expectedFailure TestDecomp test_quick dtypes= torch int torch int torch int device_type= cuda DecorateInfo unittest expectedFailure TestDecomp test_comprehensive dtypes= torch int torch int torch int device_type= cuda UserWarning CUDA caching allocator reports memory leak verified driver API __main__ TestJitCUDA test_variant_consistency_jit_logspace_cuda_complex Caching allocator allocated memory now reported device CUDA driver allocated memory now DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes= torch cfloat device_type= cuda OpInfo logspace dtypes=all_types_and_complex_and torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat is_factory_function=True supports_out=True supports_autograd=False error_inputs_func=error_inputs_linspace sample_inputs_func=sample_inputs_logspace_tensor_overload variant_test_name= tensor_overload skips= FX failed normalize op - add op op_skip list DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive TypeError int object subscriptable DecorateInfo unittest expectedFailure TestCommon test_variant_consistency_eager DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view Same failure arange cannot find linspace captured graph DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float UserWarning triggered Resized non-empty tensor did warn about DecorateInfo unittest expectedFailure TestCommon test_out_warning Off-by-one issue when casting floats ints DecorateInfo unittest expectedFailure TestDecomp test_quick dtypes= torch int torch int torch int device_type= cuda DecorateInfo unittest expectedFailure TestDecomp test_comprehensive dtypes= torch int torch int torch int device_type= cuda UserWarning CUDA caching allocator reports memory leak verified driver API __main__ TestJitCUDA test_variant_consistency_jit_logspace_cuda_complex Caching allocator allocated memory now reported device CUDA driver allocated memory now DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes= torch cfloat device_type= cuda UnaryUfuncInfo log ref=np log domain= None dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and_complex_and torch bool torch half torch bfloat torch chalf backward_dtypesIfCUDA=floating_and_complex_types_and torch half torch bfloat torch chalf dtypesIfHpu=custom_types torch float torch bfloat assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True decorators= precisionOverride torch bfloat e- skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble active_if=IS_WINDOWS log z - -inf &#124; z &#124; - reference_numerics_filter=NumericsFilter condition=lambda x torch abs x safe_val= UnaryUfuncInfo log ref=np log domain= None decorators= precisionOverride torch bfloat e- dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble active_if=IS_WINDOWS log z - -inf &#124; z &#124; - reference_numerics_filter=NumericsFilter condition=lambda x torch abs x safe_val= UnaryUfuncInfo log ref=np log domain= None dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True decorators= precisionOverride torch bfloat e- skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch cfloat torch cdouble log z - -inf &#124; z &#124; - reference_numerics_filter=NumericsFilter condition=lambda x torch abs x safe_val= BinaryUfuncInfo ldexp dtypes=all_types_and_complex_and torch bool torch half torch bfloat Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_inplace_autograd=False promotes_int_to_float=True supports_out=True supports_rhs_python_scalar=False skips= RuntimeError mul functions out= arguments don t support automatic differentiation one arguments requires grad https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestCommon test_variant_consistency_eager DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view decorators= DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestCommon device_type= cpu BinaryUfuncInfo logaddexp dtypes=floating_and_complex_types_and torch bfloat torch float dtypesIfCUDA=floating_types_and torch bfloat torch float dtypesIfHpu=custom_types torch float torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True supports_rhs_python_scalar=False skips= TODO FIXME RuntimeError implemented ComplexFloat DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion device_type= cuda OpInfo logaddexp dtypes=floating_types_and torch bfloat torch half dtypesIfHpu=custom_types torch float torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_logaddexp UnaryUfuncInfo logical_not ref=np logical_not decorators= precisionOverride torch bfloat e- torch float e- dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int torch bool supports_autograd=False skips= The function variant always returns BoolTensor while inplace variant preserves input dtype t = torch randn torch logical_not t tensor False False False torch logical_not t dtype torch bool t logical_not_ dtype torch float DecorateInfo unittest skip Skipped TestUnaryUfuncs test_variant_consistency dtypes=all_types_and_complex_and torch half torch bfloat DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager dtypes=all_types_and_complex_and torch half torch bfloat BinaryUfuncInfo lt ref=np less aliases= less dtypes=all_types_and torch bool torch bfloat torch float dtypesIfHpu=custom_types torch float torch bfloat torch int torch int always_returns_bool=True supports_autograd=False skips= OpInfo lu_unpack op=torch lu_unpack dtypes=floating_and_complex_types Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True skips= skipCPUIfNoLapack sample_inputs_func=sample_inputs_lu_unpack OpInfo lu op=torch lu dtypes=floating_and_complex_types Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True https github com pytorch pytorch issues check_batched_forward_grad=False sample_inputs_func=sample_inputs_lu decorators= skipCUDAIfNoMagmaAndNoCusolver skipCPUIfNoLapack skips= we skip jit tests because ` lu ` torch function RuntimeError Tensor inferred object has no attribute method lu File string line the_method i i lu True True ~~~~~ --- HERE DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit RuntimeError raised Expected RuntimeError when calling input device=cpu out device=cuda DecorateInfo unittest expectedFailure TestCommon test_out UserWarning triggered Resized non-empty tensor did warn about DecorateInfo unittest expectedFailure TestCommon test_out_warning OpInfo lu_solve op=torch lu_solve dtypes=floating_and_complex_types supports_forward_ad=True See https github com pytorch pytorch issues check_batched_forward_grad=False supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_lu_solve skips= DecorateInfo unittest skip Skipped TestCommon test_out device_type= mps dtypes= torch float DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager device_type= mps dtypes= torch float DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit device_type= mps dtypes= torch float DecorateInfo unittest skip Tests different backward paths TestCommon test_floating_inputs_are_differentiable decorators= skipCPUIfNoLapack skipCUDAIfNoMagmaAndNoCusolver OpInfo masked_fill dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf dtypesIfHpu=custom_types torch float torch bfloat torch int torch bool torch int sample_inputs_func=sample_inputs_masked_fill error_inputs_func=error_inputs_masked_fill supports_forward_ad=True supports_fwgrad_bwgrad=True check_batched_forward_grad=False supports_out=False OpInfo masked_scatter dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int torch bool torch int sample_inputs_func=sample_inputs_masked_scatter error_inputs_func=error_inputs_masked_scatter supports_forward_ad=True supports_fwgrad_bwgrad=True https github com pytorch pytorch issues check_batched_forward_grad=False supports_out=False skips= Compiler issue ROCm Regression started ROCm DecorateInfo unittest skip Skipped TestCommon test_non_standard_bool_values dtypes= torch bool active_if=TEST_WITH_ROCM OpInfo masked_select dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_masked_select error_inputs_func=error_inputs_masked_select skips= Compiler issue ROCm Might need skip until ROCm DecorateInfo unittest skip Skipped TestCommon test_non_standard_bool_values dtypes= torch bool active_if=TEST_WITH_ROCM OpInfo matrix_exp dtypes=floating_and_complex_types_and torch float torch bfloat aliases= linalg matrix_exp sample_inputs_func=sample_inputs_matrix_exp Needs construct nx n matrix copy_ ing into check_batched_grad=False check_batched_gradgrad=False supports_forward_ad=True supports_fwgrad_bwgrad=True https github com pytorch pytorch issues check_batched_forward_grad=False skips= mexp does support bf fp DecorateInfo unittest skip Skipped TestInductorOpInfo test_comprehensive dtypes= torch half device_type= cpu supports_out=False OpInfo matmul aliases= linalg matmul dtypes=all_types_and_complex_and torch float torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch float torch bfloat SM OrLater TEST_WITH_ROCM dtypesIfHpu=custom_types torch float torch bfloat assert_autodiffed=True assert_jit_shape_analysis=True Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True check_batched_forward_grad=False sample_inputs_func=partial sample_inputs_matmul is_rmatmul=False decorators= NVIDIA only assures bfloat supported bmm SM = DecorateInfo unittest skip Skipped TestCommon test_dtypes device_type= cuda active_if=not SM OrLater ROCm intermittently fails test standard atol rtol DecorateInfo toleranceOverride torch float tol atol= e- rtol= TestCommon test_noncontiguous_samples device_type= cuda active_if=TEST_WITH_ROCM DecorateInfo toleranceOverride torch float tol atol= e- rtol= TestCommon test_out device_type= cuda active_if=TEST_WITH_ROCM mv sample shapes S S M M M has some variance backward CPU DecorateInfo toleranceOverride torch float tol atol= rtol= e- TestCommon test_noncontiguous_samples device_type= cpu DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch complex tol atol= e- rtol= e- TestDecomp test_comprehensive device_type= cuda skips= Strides same DecorateInfo unittest expectedFailure TestCommon test_out https github com pytorch pytorch issues DecorateInfo unittest skip TestCommon test_noncontiguous_samples device_type= cpu dtypes= torch long AssertionError False true Tensors failed compare equal DecorateInfo unittest skip Skipped TestOpInfo device_type= xla dtypes= torch long https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestNNCOpInfo test_nnc_correctness device_type= cpu dtypes= torch long OpInfo max variant_test_name= reduction_with_dim dtypes=all_types_and torch float torch bfloat torch bool dtypesIfHpu=custom_types torch float torch bfloat torch int sample_inputs_func=sample_inputs_max_min_reduction_with_dim supports_fwgrad_bwgrad=True skips= supports_forward_ad=True OpInfo max variant_test_name= reduction_no_dim dtypes=all_types_and torch float torch bfloat torch bool dtypesIfHpu=custom_types torch float torch bfloat torch int supports_out=True supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_max_min_reduction_no_dim skips= OpInfo median dtypes=all_types_and torch bfloat torch float dtypesIfHpu=custom_types torch float torch bfloat torch int TODO some signatures median do support out supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True error_inputs_func=error_inputs_median sample_inputs_func=partial sample_inputs_reduction supports_multiple_dims=False OpInfo nanmedian dtypes=all_types_and torch bfloat torch float TODO some signatures nanmedian do support out supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=partial sample_inputs_reduction supports_multiple_dims=False OpInfo var_mean dtypes=floating_and_complex_types_and torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat sample_inputs_func=sample_inputs_std_var TODO some signatures var_mean do support out supports_out=False supports_forward_ad=True check_batched_forward_grad=False supports_fwgrad_bwgrad=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestDecomp test_comprehensive device_type= cuda DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cuda OpInfo var_mean variant_test_name= unbiased dtypes=floating_and_complex_types_and torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat sample_inputs_func=sample_inputs_std_var_unbiased TODO some signatures var_mean do support out supports_out=False supports_forward_ad=True check_batched_forward_grad=False supports_fwgrad_bwgrad=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestDecomp test_comprehensive device_type= cuda DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cuda OpInfo std_mean dtypes=floating_and_complex_types_and torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat sample_inputs_func=sample_inputs_std_var TODO some signatures std_mean do support out supports_out=False supports_forward_ad=True check_batched_forward_grad=False supports_fwgrad_bwgrad=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestDecomp test_comprehensive device_type= cuda OpInfo std_mean variant_test_name= unbiased dtypes=floating_and_complex_types_and torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat sample_inputs_func=sample_inputs_std_var_unbiased TODO some signatures var_mean do support out supports_out=False supports_forward_ad=True check_batched_forward_grad=False supports_fwgrad_bwgrad=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch float tol atol= e- rtol= e- TestDecomp test_comprehensive device_type= cuda DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cuda OpInfo meshgrid variant_test_name= variadic_tensors ref=np meshgrid dtypes=all_types_and_complex_and torch bfloat torch bool torch float dtypesIfHpu=custom_types torch float torch bfloat sample_inputs_func=partial sample_inputs_meshgrid variant= variadic skips= JIT does support variadic tensors RuntimeError input- type - kind == TypeKind OptionalType INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit meshgrid defined torch functional take variadic list tensors Variadic parameters compatible normalize operator tests DecorateInfo unittest skip Skipped TestNormalizeOperators test_normalize_operator_exhaustive Skip operator schema test because functional operator DecorateInfo unittest skip Skipped TestOperatorSignatures test_get_torch_func_signature_exhaustive supports_out=False supports_fwgrad_bwgrad=True supports_forward_ad=True See https github com pytorch pytorch pull check_batched_forward_grad=False OpInfo meshgrid variant_test_name= list_of_tensors Unlike variant above we do use np meshgrid ref since does officially support list numpy arrays dtypes=all_types_and_complex_and torch bfloat torch bool torch float dtypesIfHpu=custom_types torch float torch bfloat sample_inputs_func=partial sample_inputs_meshgrid variant= list skips= meshgrid defined torch functional take variadic list tensors Variadic parameters compatible normalize operator tests DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive assert_autodiffed=True supports_out=False autodiff_nonfusible_nodes= supports_fwgrad_bwgrad=True supports_forward_ad=True See https github com pytorch pytorch pull check_batched_forward_grad=False OpInfo min variant_test_name= reduction_with_dim dtypes=all_types_and torch float torch bfloat torch bool dtypesIfHpu=custom_types torch float torch bfloat torch int sample_inputs_func=sample_inputs_max_min_reduction_with_dim supports_fwgrad_bwgrad=True supports_forward_ad=True skips= OpInfo min variant_test_name= reduction_no_dim dtypes=all_types_and torch float torch bfloat torch bool supports_out=True supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_max_min_reduction_no_dim skips= OpInfo quantile dtypes=floating_types sample_inputs_func=sample_inputs_reduction_quantile supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch issues Relies copy_ broadcast forward AD path calls broadcast_to which does have batching rule core check_batched_forward_grad=False OpInfo nanquantile dtypes=floating_types sample_inputs_func=sample_inputs_reduction_quantile supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch issues Relies copy_ broadcast forward AD path calls broadcast_to which does have batching rule core check_batched_forward_grad=False BinaryUfuncInfo max aliases= maximum variant_test_name= binary dtypes=all_types_and torch float torch bfloat torch bool dtypesIfHpu=custom_types torch float torch bfloat torch int supports_forward_ad=True supports_fwgrad_bwgrad=True assert_autodiffed=True ref=np maximum supports_rhs_python_scalar=False skips= Incorrectly attempts use scalar second argument DecorateInfo unittest expectedFailure TestJit test_jit_alias_remapping TODO FIXME RuntimeError max_elementwise_cuda implemented ComplexFloat DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion device_type= cuda BinaryUfuncInfo maximum dtypes=all_types_and torch float torch bfloat torch bool dtypesIfHpu=custom_types torch float torch bfloat torch int supports_forward_ad=True supports_fwgrad_bwgrad=True ref=np maximum supports_rhs_python_scalar=False skips= TODO FIXME RuntimeError max_elementwise_cuda implemented ComplexFloat DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion device_type= cuda BinaryUfuncInfo min aliases= minimum variant_test_name= binary dtypes=all_types_and torch float torch bfloat torch bool dtypesIfHpu=custom_types torch float torch bfloat torch int supports_forward_ad=True supports_fwgrad_bwgrad=True assert_autodiffed=True ref=np minimum supports_rhs_python_scalar=False skips= Incorrectly attempts use scalar second argument DecorateInfo unittest expectedFailure TestJit test_jit_alias_remapping TODO FIXME RuntimeError min_elementwise_cuda implemented ComplexFloat DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion device_type= cuda BinaryUfuncInfo minimum dtypes=all_types_and torch float torch bfloat torch bool dtypesIfHpu=custom_types torch float torch bfloat torch int supports_forward_ad=True supports_fwgrad_bwgrad=True ref=np minimum supports_rhs_python_scalar=False skips= TODO FIXME RuntimeError min_elementwise_cuda implemented ComplexFloat DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion device_type= cuda BinaryUfuncInfo logical_and ref=np logical_and dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int torch int torch bool supports_autograd=False always_returns_bool=True supports_rhs_python_scalar=False BinaryUfuncInfo logical_or ref=np logical_or dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int torch bool supports_autograd=False always_returns_bool=True supports_rhs_python_scalar=False BinaryUfuncInfo logical_xor ref=np logical_xor dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int torch bool supports_autograd=False always_returns_bool=True supports_rhs_python_scalar=False skips= BinaryUfuncInfo bitwise_and ref=np bitwise_and dtypes=integral_types_and torch bool dtypesIfHpu=custom_types torch bool operator_variant=operator and_ inplace_operator_variant=operator iand supports_autograd=False supports_one_python_scalar=True skips= RuntimeError bitwise_and_cuda implemented Half DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion device_type= cuda BinaryUfuncInfo bitwise_or ref=np bitwise_or dtypes=integral_types_and torch bool dtypesIfHpu=custom_types torch bool operator_variant=operator or_ inplace_operator_variant=operator ior supports_autograd=False supports_one_python_scalar=True skips= TODO FIXME RuntimeError bitwise_or_cuda implemented Half DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion device_type= cuda BinaryUfuncInfo bitwise_xor ref=np bitwise_xor dtypes=integral_types_and torch bool dtypesIfHpu=custom_types torch bool operator_variant=operator xor inplace_operator_variant=operator ixor supports_autograd=False supports_one_python_scalar=True skips= TODO FIXME RuntimeError bitwise_xor_cuda implemented Half DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion device_type= cuda BinaryUfuncInfo heaviside ref=lambda b necessary because np heaviside incorrectly returns float when passed args dtype int np int np heaviside b dtype == np int b dtype == np int np heaviside b dtypes=all_types_and torch bool torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int supports_autograd=False supports_rhs_python_scalar=False skips= RuntimeError heaviside yet implemented tensors different dtypes DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion DecorateInfo unittest expectedFailure TestMeta test_binary_ufuncs_mixed_dtype PyTorch s heaviside does appear propagate NaNs DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_extremal_values BinaryUfuncInfo lcm ref=np lcm dtypes=integral_types_and supports_autograd=False supports_rhs_python_scalar=False BinaryUfuncInfo gcd ref=np gcd dtypes=integral_types_and supports_autograd=False supports_rhs_python_scalar=False skips= DecorateInfo unittest expectedFailure TestBinaryUfuncs test_reference_numerics_small_values dtypes= torch int BinaryUfuncInfo isclose ref=np isclose dtypes=all_types_and_complex_and torch bool torch float torch bfloat sample_inputs_func=sample_inputs_isclose error_inputs_func=error_inputs_isclose supports_autograd=False supports_out=False supports_rhs_python_scalar=False skips= DecorateInfo unittest expectedFailure TestCommon test_numpy_refs dtypes= torch complex RuntimeError Short did match Int DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion DecorateInfo unittest expectedFailure TestMeta test_binary_ufuncs_mixed_dtype DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_extremal_values ` softmax ` supports different dtypes based whether ` dtype ` argument passed Hence two OpInfo entries one dtype other without https github com pytorch pytorch issues OpInfo softmax aliases= special softmax nn functional softmax aten_name= softmax aten_backward_name= _softmax_backward_data dtypes=floating_types_and torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat sample_inputs_func=sample_inputs_softmax_variant assert_jit_shape_analysis=True assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=True OpInfo softmax aliases= special softmax nn functional softmax variant_test_name= with_dtype aten_name= softmax dtypes=all_types_and_complex_and torch bool torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat sample_inputs_func=partial sample_inputs_softmax_variant with_dtype=True assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=True OpInfo _softmax_backward_data op=torch ops aten _softmax_backward_data aten_name= _softmax_backward_data dtypes=floating_types_and torch bfloat torch float sample_inputs_func=sample_inputs_softmax_backward_data assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False skips= DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float ` softmin ` supports different dtypes based whether ` dtype ` argument passed Hence two OpInfo entries one dtype other without https github com pytorch pytorch issues OpInfo nn functional softmin aten_name= softmin dtypes=floating_types_and torch half torch bfloat sample_inputs_func=sample_inputs_softmax_variant assert_jit_shape_analysis=False assert_autodiffed=False supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False OpInfo nn functional softmin variant_test_name= with_dtype aten_name= softmin dtypes=all_types_and_complex_and torch float torch bfloat sample_inputs_func=partial sample_inputs_softmax_variant with_dtype=True assert_autodiffed=False supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False OpInfo nn functional cross_entropy dtypes=floating_types_and torch float torch bfloat sample_inputs_func=sample_inputs_cross_entropy supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestJit test_variant_consistency_jit device_type= cpu skips= AssertionError False true Scalars failed compare equal = test_ops TestJitCUDA test_variant_consistency_jit_nn_functional_cross_entropy_cuda_float leaked bytes CUDA memory device DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit device_type= cuda DecorateInfo unittest skip FP corss_entropy cases have been enabled MPS yet dtypes= torch half device_type= mps OpInfo nn functional normalize dtypes=floating_and_complex_types_and torch half torch bfloat sample_inputs_func=sample_inputs_normalize supports_forward_ad=True supports_fwgrad_bwgrad=True OpInfo aminmax ref=lambda x dim=None keepdim=False np amin x axis=dim keepdims=keepdim np amax x axis=dim keepdims=keepdim dtypes=all_types_and torch bool torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch int torch int decorators= onlyNativeDeviceTypes supports_autograd=False sample_inputs_func=sample_inputs_aminmax error_inputs_func=error_inputs_aminmax_amax_amin OpInfo as_strided dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf dtypesIfHpu=custom_types torch float torch bfloat torch int torch int torch bool supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True vmap does support inplace views check_inplace_batched_forward_grad=False sample_inputs_func=sample_inputs_as_strided skips= Note This xfail fine -- s inherent how as_strided works DecorateInfo unittest expectedFailure TestCommon test_noncontiguous_samples AssertionError False true Scalars failed compare equal DecorateInfo unittest skip Errors when storage_offset included TestCommon test_variant_consistency_eager Not close DecorateInfo unittest skip Errors when storage_offset included TestCommon test_complex_half_reference_testing Not close DecorateInfo unittest skip Errors when storage_offset included TestMathBits test_conj_view DecorateInfo unittest skip Errors when storage_offset included TestMathBits test_neg_view DecorateInfo unittest skip Numerous errors TestFwdGradients DecorateInfo unittest skip Numerous errors TestBwdGradients OpInfo as_strided variant_test_name= partial_views dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf dtypesIfHpu=custom_types torch float torch bfloat torch int torch int torch bool supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True vmap does support inplace views check_inplace_batched_forward_grad=False sample_inputs_func=sample_inputs_as_strided_partial_views skips= Note This xfail fine -- s inherent how as_strided works DecorateInfo unittest expectedFailure TestCommon test_noncontiguous_samples These fail because test changes input s in-memory layout DecorateInfo unittest expectedFailure TestCommon test_complex_half_reference_testing DecorateInfo unittest expectedFailure TestCommon test_variant_consistency_eager DecorateInfo unittest expectedFailure TestCommon test_compare_cpu DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest expectedFailure TestFwdGradients test_fn_fwgrad_bwgrad dtypes= torch complex torch complex DecorateInfo unittest expectedFailure TestFwdGradients test_forward_mode_AD DecorateInfo unittest expectedFailure TestFwdGradients test_inplace_forward_mode_AD DecorateInfo unittest expectedFailure TestBwdGradients test_inplace_grad DecorateInfo unittest expectedFailure TestBwdGradients test_inplace_gradgrad DecorateInfo unittest expectedFailure TestProxyTensorOpInfo test_make_fx_symbolic_exhaustive_inplace DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness Fail also flaky DecorateInfo unittest skip Test changes memory layout TestMathBits DecorateInfo unittest skip Modifies input strides storage_offset TestCommon test_non_standard_bool_values RuntimeError setStorage sizes strides storage offset itemsize requiring storage size out bounds storage size DecorateInfo unittest expectedFailure TestMeta test_meta_inplace DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace_all_strides OpInfo as_strided_copy dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf supports_out=True supports_forward_ad=True supports_fwgrad_bwgrad=True vmap does support inplace views check_inplace_batched_forward_grad=False sample_inputs_func=sample_inputs_as_strided skips= Note This xfail fine -- s inherent how as_strided works DecorateInfo unittest expectedFailure TestCommon test_noncontiguous_samples AssertionError False true Scalars failed compare equal DecorateInfo unittest skip Errors when storage_offset included TestCommon test_variant_consistency_eager Not close DecorateInfo unittest skip Errors when storage_offset included TestCommon test_complex_half_reference_testing Not close DecorateInfo unittest skip Errors when storage_offset included TestMathBits test_conj_view DecorateInfo unittest skip Errors when storage_offset included TestMathBits test_neg_view DecorateInfo unittest skip Numerous errors TestFwdGradients DecorateInfo unittest skip Numerous errors TestBwdGradients DecorateInfo unittest expectedFailure TestDTensorOps test_dtensor_op_db OpInfo as_strided_scatter dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True vmap does support inplace views check_inplace_batched_forward_grad=False sample_inputs_func=sample_inputs_as_strided_scatter error_inputs_func=error_inputs_as_strided_scatter skips= DecorateInfo unittest skip Works int fails everything TestCommon test_noncontiguous_samples noqa B DecorateInfo unittest skip Fails most cases passes LAZY some reason TestCommon test_variant_consistency_eager noqa B DecorateInfo unittest skip Fails cuda TestCommon test_complex_half_reference_testing active_if=not TEST_WITH_ROCM DecorateInfo unittest expectedFailure TestBwdGradients test_fn_grad DecorateInfo unittest expectedFailure TestFwdGradients test_forward_mode_AD DecorateInfo unittest skip Passes complex float only TestFwdGradients test_fn_fwgrad_bwgrad AssertionError Tensor-likes close new_empty_strided default DecorateInfo unittest skip Expected new_empty_strided comparable TestDecomp test_comprehensive OpInfo native_layer_norm aten_name= native_layer_norm ref=reference_native_layer_norm dtypes=floating_types_and torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat supports_out=False assert_jit_shape_analysis=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_native_layer_norm error_inputs_func=error_inputs_native_layer_norm skips= IndexError tuple index out range DecorateInfo unittest skip Skipped TestFwdGradients test_forward_mode_AD Tests fail when weight=None bias defined https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestBwdGradients test_fn_gradgrad JIT test also tries compute double backward which fails DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest skip Unsupported MPS now TestCommon test_numpy_ref_mps DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestDecomp test_comprehensive device_type= cpu OpInfo native_batch_norm aten_name= native_batch_norm dtypes=floating_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True assert_jit_shape_analysis=True allow_cow_input_materialize_forward= allow_cow_input_materialize_backward= sample_inputs_func=sample_inputs_native_batch_norm skips= NotImplementedError Could run aten native_batch_norm out arguments CPU backend DecorateInfo unittest expectedFailure TestCommon test_out_warning device_type= cpu RuntimeError out_invstd dim == out_invstd is_contiguous out_invstd sizes DecorateInfo unittest expectedFailure TestCommon test_out device_type= cuda Problem _get_numerical_jacobian IndexError tuple index out range DecorateInfo unittest skip Skipped TestFwdGradients test_forward_mode_AD RuntimeError deepEquals input iValue deepCopiedInput INTERNAL ASSERT FAILED DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestCommon test_compare_cpu AssertionError Booleans mismatch True False DecorateInfo unittest skip Skipped TestFakeTensor test_fake_autocast DecorateInfo unittest skip Skipped TestFakeTensor test_fake DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestCompositeCompliance test_forward_ad OpInfo _native_batch_norm_legit aten_name= _native_batch_norm_legit dtypes=floating_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True assert_jit_shape_analysis=True allow_cow_input_materialize_forward= allow_cow_input_materialize_backward= sample_inputs_func=sample_inputs__native_batch_norm_legit skips= NotImplementedError Could run aten native_batch_norm out arguments CPU backend DecorateInfo unittest expectedFailure TestCommon test_out_warning device_type= cpu RuntimeError out_invstd dim == out_invstd is_contiguous out_invstd sizes DecorateInfo unittest expectedFailure TestCommon test_out device_type= cuda Problem _get_numerical_jacobian IndexError tuple index out range DecorateInfo unittest skip Skipped TestFwdGradients test_forward_mode_AD RuntimeError deepEquals input iValue deepCopiedInput INTERNAL ASSERT FAILED DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestCommon test_compare_cpu DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestCompositeCompliance test_forward_ad OpInfo _batch_norm_with_update op=torch ops aten _batch_norm_with_update aten_name= _batch_norm_with_update dtypes=floating_types_and torch float torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True assert_jit_shape_analysis=True allow_cow_input_materialize_forward= allow_cow_input_materialize_backward= sample_inputs_func=sample_inputs__batch_norm_with_update skips= NotImplementedError Could run aten native_batch_norm out arguments CPU backend DecorateInfo unittest expectedFailure TestCommon test_out_warning device_type= cpu RuntimeError out_invstd dim == out_invstd is_contiguous out_invstd sizes DecorateInfo unittest expectedFailure TestCommon test_out device_type= cuda Problem _get_numerical_jacobian IndexError tuple index out range DecorateInfo unittest skip Skipped TestFwdGradients test_forward_mode_AD RuntimeError deepEquals input iValue deepCopiedInput INTERNAL ASSERT FAILED DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestCompositeCompliance test_forward_ad _batch_norm_with_update expects contiguous inputs cudnn miopen DecorateInfo unittest expectedFailure TestCommon test_noncontiguous_samples device_type= cuda DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace_all_strides device_type= cuda _batch_norm_with_update does have python bindings DecorateInfo unittest skip Skipped TestNormalizeOperators test_normalize_operator_exhaustive aten out variants do accept out= kwarg only python out variants DecorateInfo unittest expectedFailure TestCommon test_out DecorateInfo unittest expectedFailure TestCommon test_out_warning OpInfo nn functional cosine_similarity aten_name= cosine_similarity dtypes=floating_types_and torch half torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cuda sample_inputs_func=sample_inputs_cosine_similarity OpInfo nn functional adaptive_avg_pool d dtypes=floating_types_and torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch float supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True gradcheck_nondet_tol=GRADCHECK_NONDET_TOL error_inputs_func=error_inputs_adaptive_avg_pool d sample_inputs_func=sample_inputs_adaptive_avg_pool d OpInfo nn functional adaptive_avg_pool d dtypes=floating_types_and torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch float decorators= RuntimeError adaptive_avg_pool d Tensor input int output_size - Tensor Expected value type List int argument output_size instead found type Tuple NoneType int File string line the_method i torch nn functional adaptive_avg_pool d i None ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ --- HERE DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True gradcheck_nondet_tol=GRADCHECK_NONDET_TOL error_inputs_func=error_inputs_adaptive_avg_pool d sample_inputs_func=sample_inputs_adaptive_avg_pool d OpInfo nn functional adaptive_avg_pool d dtypes=floating_types_and torch half torch bfloat dtypesIfCUDA=floating_types_and torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch float decorators= RuntimeError adaptive_avg_pool d Tensor input int output_size - Tensor Expected value type List int argument output_size instead found type Tuple NoneType NoneType NoneType File string line the_method i torch nn functional adaptive_avg_pool d i None None None ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ --- HERE DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True gradcheck_nondet_tol=GRADCHECK_NONDET_TOL error_inputs_func=error_inputs_adaptive_avg_pool d sample_inputs_func=sample_inputs_adaptive_avg_pool d OpInfo nn functional adaptive_max_pool d dtypes=floating_types_and torch half torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True got Batching rule implemented aten flatten using_ints check_batched_forward_grad=False gradcheck_nondet_tol=GRADCHECK_NONDET_TOL error_inputs_func=error_inputs_adaptive_max_pool d sample_inputs_func=sample_inputs_adaptive_max_pool d OpInfo nn functional adaptive_max_pool d dtypes=floating_types_and torch half torch bfloat decorators= RuntimeError adaptive_max_pool d Tensor input int output_size - Tensor Expected value type List int argument output_size instead found type Tuple NoneType int File string line the_method i torch nn functional adaptive_max_pool d i None ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ --- HERE DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True got Batching rule implemented aten flatten using_ints check_batched_forward_grad=False gradcheck_nondet_tol=GRADCHECK_NONDET_TOL error_inputs_func=error_inputs_adaptive_max_pool d sample_inputs_func=sample_inputs_adaptive_max_pool d OpInfo nn functional adaptive_max_pool d dtypes=floating_types_and torch bfloat torch half decorators= RuntimeError adaptive_max_pool d Tensor input int output_size - Tensor Expected value type List int argument output_size instead found type Tuple NoneType NoneType NoneType File string line the_method i torch nn functional adaptive_max_pool d i None None None ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ --- HERE DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True got Batching rule implemented aten flatten using_ints check_batched_forward_grad=False gradcheck_nondet_tol=GRADCHECK_NONDET_TOL error_inputs_func=error_inputs_adaptive_max_pool d sample_inputs_func=sample_inputs_adaptive_max_pool d OpInfo nn functional avg_pool d aten_name= avg_pool d supports_autograd=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True dtypes=floating_types_and torch int torch float torch bfloat dtypesIfCUDA=floating_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch float gradcheck_nondet_tol=GRADCHECK_NONDET_TOL error_inputs_func=error_inputs_avg_pool d sample_inputs_func=sample_inputs_avgpool d OpInfo nn functional avg_pool d aten_name= avg_pool d supports_autograd=True supports_forward_ad=True supports_fwgrad_bwgrad=True dtypes=floating_types_and torch int dtypesIfCUDA=floating_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch float gradcheck_nondet_tol=GRADCHECK_NONDET_TOL error_inputs_func=error_inputs_avg_pool d sample_inputs_func=sample_inputs_avgpool d skips= AssertionError Tensor-likes close DecorateInfo unittest expectedFailure TestCommon test_out device_type= cpu OpInfo nn functional binary_cross_entropy_with_logits aten_name= binary_cross_entropy_with_logits supports_autograd=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False dtypes=floating_types_and torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch float gradcheck_nondet_tol=GRADCHECK_NONDET_TOL sample_inputs_func=sample_inputs_binary_cross_entropy_with_logits skips= DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes= torch float DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestConsistency test_output_match device_type= mps UnaryUfuncInfo nn functional relu aten_name= relu ref=lambda np where = supports_autograd=True supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True dtypes=all_types_and torch half torch bfloat dtypesIfHpu=custom_types torch float torch bfloat torch float sample_inputs_func=sample_inputs_nn_activation_relu supports_out=False supports_fwgrad_bwgrad=True supports_forward_ad=True OpInfo nn functional conv_transpose d ` ref ` function backward corresponding ` conv d ` ref=partial conv_transpose_ref fn=torch nn functional conv_transpose d aten_name= conv_transpose d aliases= conv_transpose d dtypes=floating_and_complex_types_and torch int torch float torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch float torch chalf torch bfloat sample_inputs_func=sample_inputs_conv_transpose d supports_forward_ad=True supports_fwgrad_bwgrad=True assert_jit_shape_analysis=True gradcheck_nondet_tol=GRADCHECK_NONDET_TOL decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestCommon test_variant_consistency_eager device_type= cuda DecorateInfo toleranceOverride torch chalf tol atol= e- rtol= e- TestCommon test_complex_half_reference_testing DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestCommon test_numpy_ref_mps DecorateInfo toleranceOverride torch half tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cpu skips= Reason Skip https github com pytorch pytorch pull #issuecomment- DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes= torch complex RuntimeError UNSUPPORTED DTYPE complex DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness dtypes= torch complex torch complex RuntimeError lhs isAliasOf rhs INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float RuntimeError slow_conv d_cpu_grad_input implemented Long DecorateInfo unittest expectedFailure TestCommon test_numpy_ref dtypes= torch int supports_out=False OpInfo nn functional conv_transpose d aten_name= conv_transpose d aliases= conv_transpose d ` ref ` function backward corresponding ` conv d ` ref=partial conv_transpose_ref fn=torch nn functional conv_transpose d dtypes=floating_and_complex_types_and torch int torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch float torch chalf torch bfloat sample_inputs_func=sample_inputs_conv_transpose d Runs very slowly slow-gradcheck complex gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True assert_jit_shape_analysis=True gradcheck_nondet_tol=GRADCHECK_NONDET_TOL decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestCommon test_variant_consistency_eager device_type= cuda DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestCommon test_noncontiguous_samples device_type= cuda DecorateInfo toleranceOverride torch chalf tol atol= e- rtol= e- TestCommon test_complex_half_reference_testing DecorateInfo toleranceOverride torch half tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cpu skips= RuntimeError lhs isAliasOf rhs INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit RuntimeError UNSUPPORTED DTYPE complex DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness dtypes= torch complex torch complex RuntimeError slow_conv d_cpu_grad_input implemented Long DecorateInfo unittest expectedFailure TestCommon test_numpy_ref dtypes= torch int Reference https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestCommon test_numpy_ref dtypes= torch double torch cdouble DecorateInfo unittest skip Unsupported MPS now TestCommon test_numpy_ref_mps AssertionError None mismatch torch complex None DecorateInfo unittest expectedFailure TestDtypeCustomRules test_custom_rules dtypes= torch complex torch complex supports_out=False OpInfo nn functional conv_transpose d aten_name= conv_transpose d aliases= conv_transpose d ` ref ` function backward corresponding ` conv d ` ref=partial conv_transpose_ref fn=torch nn functional conv_transpose d dtypes=floating_and_complex_types_and torch int torch float torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch float torch chalf torch bfloat sample_inputs_func=sample_inputs_conv_transpose d supports_forward_ad=True supports_fwgrad_bwgrad=True assert_jit_shape_analysis=True Runs very slowly slow-gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True gradcheck_nondet_tol=GRADCHECK_NONDET_TOL decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cuda DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch complex tol atol= e- rtol= e- TestCommon test_variant_consistency_eager device_type= cuda DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestCompositeCompliance test_operator device_type= cuda DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch complex tol atol= e- rtol= e- TestCommon test_noncontiguous_samples device_type= cuda DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestCompositeCompliance test_forward_ad device_type= cuda active_if=TEST_CUDNN DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestMathBits test_conj_view device_type= cuda DecorateInfo toleranceOverride torch chalf tol atol= e- rtol= e- TestCommon test_complex_half_reference_testing DecorateInfo toleranceOverride torch half tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cpu skips= RuntimeError lhs isAliasOf rhs INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit RuntimeError slow_conv d_cpu_grad_input implemented Long DecorateInfo unittest expectedFailure TestCommon test_numpy_ref dtypes= torch int Reference https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestCommon test_numpy_ref dtypes= torch double torch cdouble DecorateInfo unittest skip Unsupported MPS now TestCommon test_numpy_ref_mps RuntimeError UNSUPPORTED DTYPE complex DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness dtypes= torch complex torch complex DecorateInfo unittest skip Skipped ROCm TestCommon test_complex_half_reference_testing dtypes= torch complex active_if=TEST_WITH_ROCM supports_out=False OpInfo nn functional conv d aliases= conv d aten_name= conv d dtypes=floating_and_complex_types_and torch int torch float torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch float torch chalf torch bfloat dtypesIfHpu=custom_types torch float torch bfloat sample_inputs_func=sample_inputs_conv d error_inputs_func=error_inputs_conv d supports_forward_ad=True supports_fwgrad_bwgrad=True assert_jit_shape_analysis=True gradcheck_nondet_tol=GRADCHECK_NONDET_TOL decorators= DecorateInfo toleranceOverride torch chalf tol atol= e- rtol= e- TestCommon test_complex_half_reference_testing DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cuda skips= RuntimeError lhs isAliasOf rhs INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit Ref https github com pytorch pytorch issues AssertionError None mismatch torch complex None DecorateInfo unittest expectedFailure TestDtypeCustomRules test_custom_rules dtypes= torch complex torch complex Ref https github com pytorch pytorch issues RuntimeError UNSUPPORTED DTYPE complex DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness dtypes= torch complex torch complex supports_expanded_weight=True supports_out=False OpInfo nn functional conv d aliases= conv d aten_name= conv d dtypes=floating_and_complex_types_and torch int torch float torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch float torch chalf torch bfloat dtypesIfHpu=custom_types torch float torch bfloat sample_inputs_func=partial sample_inputs_conv d error_inputs_func=error_inputs_conv d gradcheck_nondet_tol=GRADCHECK_NONDET_TOL Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True assert_jit_shape_analysis=True decorators= DecorateInfo toleranceOverride torch chalf tol atol= e- rtol= e- TestCommon test_complex_half_reference_testing DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive skips= RuntimeError lhs isAliasOf rhs INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest skip Works some configs TestJit test_variant_consistency_jit Ref https github com pytorch pytorch issues AssertionError None mismatch torch complex None DecorateInfo unittest expectedFailure TestDtypeCustomRules test_custom_rules dtypes= torch complex torch complex RuntimeError UNSUPPORTED DTYPE complex DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness dtypes= torch complex torch complex supports_expanded_weight=True supports_out=False OpInfo nn functional conv d aliases= conv d aten_name= conv d dtypes=floating_and_complex_types_and torch int torch bfloat torch float dtypesIfCUDA=floating_and_complex_types_and torch float torch chalf torch bfloat dtypesIfHpu=custom_types torch float torch bfloat sample_inputs_func=sample_inputs_conv d error_inputs_func=error_inputs_conv d gradcheck_nondet_tol=GRADCHECK_NONDET_TOL gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True decorators= DecorateInfo toleranceOverride torch chalf tol atol= e- rtol= e- TestCommon test_complex_half_reference_testing TF DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch complex tol atol= e- rtol= e- TestCommon test_noncontiguous_samples DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestCommon test_variant_consistency_eager DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestMathBits test_conj_view DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestOperators test_vjpvmap DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive skips= RuntimeError lhs isAliasOf rhs INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit RuntimeError UNSUPPORTED DTYPE complex DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness dtypes= torch complex torch complex AssertionError Tensor-likes close break slow tests DecorateInfo unittest skip Skipped TestCommon test_compare_cpu supports_expanded_weight=True supports_out=False OpInfo nn functional group_norm aten_name= group_norm aliases= group_norm ref=reference_group_norm dtypes=floating_types_and torch float torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True error_inputs_func=error_inputs_group_norm decorators= RuntimeError Cannot insert Tensor requires grad constant Consider making parameter input detaching gradient DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestDecomp test_comprehensive device_type= cpu sample_inputs_func=sample_inputs_group_norm reference_inputs_func=reference_inputs_group_norm supports_expanded_weight=True OpInfo nn functional instance_norm no ref because instance_norm will often have numerical instability large numbers nan dtypes=floating_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True allow_cow_input_materialize_forward= running_mean running_var decorators= RuntimeError Cannot insert Tensor requires grad constant Consider making parameter input detaching gradient DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float sample_inputs_func=sample_inputs_instance_norm supports_expanded_weight=True OpInfo nn functional layer_norm aten_name= layer_norm aten_backward_name= layer_norm_backward aliases= layer_norm ref=reference_layer_norm dtypes=floating_types_and torch half torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True assert_jit_shape_analysis=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestCommon test_numpy_refs DecorateInfo unittest skip Bug MPS backend TestCommon test_numpy_ref_mps sample_inputs_func=sample_inputs_layer_norm supports_expanded_weight=True OpInfo nn functional rms_norm aten_name= rms_norm aliases= rms_norm ref=reference_rms_norm dtypes=floating_and_complex_types_and torch half torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_rms_norm error_inputs_func=error_inputs_rms_norm OpInfo nn functional local_response_norm dtypes=floating_types_and torch int torch float torch bfloat dtypesIfCUDA=floating_types_and torch float torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True decorators= RuntimeError falseINTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float sample_inputs_func=sample_inputs_local_response_norm OpInfo constant_pad_nd supports_forward_ad=True supports_fwgrad_bwgrad=True dtypes=all_types_and_complex_and torch bool torch bfloat torch half sample_inputs_func=sample_inputs_constant_pad_nd supports_out=False skips= bool can t passed Scalar arguments JIT tracer because BoolType subtype ScalarType DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness dtypes= torch bool OpInfo nn functional pad variant_test_name= constant aten_name= constant_pad_nd Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True dtypes=all_types_and_complex_and torch bool torch bfloat torch half sample_inputs_func=partial sample_inputs_nn_pad mode= constant supports_out=False OpInfo nn functional pad variant_test_name= reflect supports_forward_ad=True supports_fwgrad_bwgrad=True dtypes=all_types_and_complex_and torch bfloat torch half sample_inputs_func=partial sample_inputs_nn_pad mode= reflect skips= Doesn t have corresponding aten operator RuntimeError falseINTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes= torch float gradcheck_nondet_tol=GRADCHECK_NONDET_TOL supports_out=False OpInfo nn functional pad variant_test_name= replicate supports_forward_ad=True supports_fwgrad_bwgrad=True dtypes=all_types_and_complex_and torch half torch bfloat sample_inputs_func=partial sample_inputs_nn_pad mode= replicate skips= Doesn t have corresponding aten operator RuntimeError falseINTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes= torch float gradcheck_nondet_tol=GRADCHECK_NONDET_TOL supports_out=False OpInfo nn functional pad variant_test_name= replicate_negative supports_forward_ad=True supports_fwgrad_bwgrad=True dtypes=all_types_and_complex_and torch half torch bfloat sample_inputs_func=sample_inputs_nn_pad_replicate_negative skips= Doesn t have corresponding aten operator RuntimeError falseINTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes= torch float Some negative padding cases cause segfault MPS DecorateInfo unittest skip Not fully supported MPS TestConsistency gradcheck_nondet_tol=GRADCHECK_NONDET_TOL supports_out=False OpInfo nn functional pad variant_test_name= circular dtypes=all_types_and_complex_and torch bool torch bfloat torch half sample_inputs_func=partial sample_inputs_nn_pad mode= circular supports_forward_ad=True supports_fwgrad_bwgrad=True check_batched_grad=False https github com pytorch pytorch issues check_batched_forward_grad=False skips= Doesn t have corresponding aten operator RuntimeError falseINTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes= torch float Difference type larger decomposition new_empty_strided default than original output DecorateInfo unittest skip Expected new_empty_strided comparable TestDecomp test_comprehensive supports_out=False OpInfo nn functional hardswish aten_name= hardswish aten_backward_name= hardswish_backward supports_autograd=True assert_autodiffed=True sample_inputs_func=sample_inputs_hardswish dtypes=floating_types_and torch bfloat torch half supports_gradgrad=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False autodiff_nonfusible_nodes= aten hardswish OpInfo nn functional unfold aten_name= im col dtypes=floating_and_complex_types_and torch half torch bfloat torch bool dtypesIfCUDA=floating_and_complex_types_and torch half torch bfloat torch bool sample_inputs_func=sample_inputs_nn_unfold Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False skips= NOTE failure may reproduce consistently different systems false INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp DecorateInfo unittest skip Internal assert failed TestJit test_variant_consistency_jit Compiler issue ROCm Regression started ROCm DecorateInfo unittest skip Skipped TestCommon test_non_standard_bool_values dtypes= torch bool active_if=TEST_WITH_ROCM OpInfo nn functional interpolate aten_name= interpolate variant_test_name= nearest supports_autograd=True supports_fwgrad_bwgrad=True supports_forward_ad=True dtypes=floating_types_and torch uint torch half torch bfloat sample_inputs_func=partial sample_inputs_interpolate nearest skips= RuntimeError false INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit supports_out=False OpInfo nn functional interpolate aten_name= interpolate variant_test_name= nearest-exact supports_autograd=True supports_fwgrad_bwgrad=True supports_forward_ad=True dtypes=floating_types_and torch half torch bfloat torch uint sample_inputs_func=partial sample_inputs_interpolate nearest-exact skips= RuntimeError false INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit RuntimeError aten _upsample_nearest_exact d hit vmap fallback which currently disabled DecorateInfo unittest expectedFailure TestOperators test_vmapjvpall_has_batch_rule DecorateInfo unittest expectedFailure TestOperators test_vmapvjp_has_batch_rule DecorateInfo unittest expectedFailure TestVmapOperatorsOpInfo test_op_has_batch_rule supports_out=False OpInfo nn functional interpolate aten_name= interpolate variant_test_name= linear supports_autograd=True supports_fwgrad_bwgrad=True supports_forward_ad=True dtypes=floating_types_and torch half torch bfloat sample_inputs_func=partial sample_inputs_interpolate linear skips= RuntimeError false INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit supports_out=False OpInfo nn functional interpolate aten_name= interpolate variant_test_name= bilinear supports_fwgrad_bwgrad=True supports_autograd=True supports_forward_ad=True dtypes=floating_types_and torch uint torch half torch bfloat dtypesIfCUDA=floating_types_and torch half torch bfloat gradcheck_nondet_tol=GRADCHECK_NONDET_TOL sample_inputs_func=partial sample_inputs_interpolate bilinear reference_inputs_func=partial reference_inputs_interpolate bilinear skips= RuntimeError false INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit supports_out=False OpInfo nn functional interpolate aten_name= interpolate variant_test_name= bicubic supports_autograd=True supports_forward_ad=True supports_fwgrad_bwgrad=True dtypes=floating_types_and torch uint torch half torch bfloat dtypesIfCUDA=floating_types_and torch half torch bfloat sample_inputs_func=partial sample_inputs_interpolate bicubic reference_inputs_func=partial reference_inputs_interpolate bicubic gradcheck_nondet_tol=GRADCHECK_NONDET_TOL skips= RuntimeError false INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit supports_out=False OpInfo nn functional interpolate aten_name= interpolate variant_test_name= trilinear supports_autograd=True supports_forward_ad=True supports_fwgrad_bwgrad=True dtypes=floating_types_and torch half torch bfloat gradcheck_nondet_tol=GRADCHECK_NONDET_TOL sample_inputs_func=partial sample_inputs_interpolate trilinear skips= RuntimeError false INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit supports_out=False OpInfo nn functional interpolate aten_name= interpolate variant_test_name= area supports_autograd=True supports_forward_ad=True supports_fwgrad_bwgrad=True dtypes=floating_types_and torch half torch bfloat dtypesIfCUDA=floating_types_and torch half torch bfloat sample_inputs_func=partial sample_inputs_interpolate area gradcheck_nondet_tol=GRADCHECK_NONDET_TOL skips= RuntimeError false INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit supports_out=False OpInfo nn functional upsample_bilinear supports_autograd=True supports_forward_ad=True supports_fwgrad_bwgrad=True dtypes=floating_types_and torch uint torch half torch bfloat dtypesIfCUDA=floating_types_and torch half torch bfloat gradcheck_nondet_tol=GRADCHECK_NONDET_TOL sample_inputs_func=partial sample_inputs_upsample bilinear reference_inputs_func=partial reference_inputs_upsample bilinear skips= RuntimeError false INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit supports_out=False OpInfo _upsample_bilinear d_aa op=torch ops aten _upsample_bilinear d_aa aten_name= _upsample_bilinear d_aa supports_autograd=True supports_forward_ad=True supports_fwgrad_bwgrad=True dtypes=floating_types_and torch uint dtypesIfCUDA=floating_types_and torch half torch bfloat gradcheck_nondet_tol=GRADCHECK_NONDET_TOL sample_inputs_func=partial sample_inputs_upsample_aa bilinear supports_out=False skips= DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest expectedFailure TestDTensorOps test_dtensor_op_db DecorateInfo unittest expectedFailure TestInductorOpInfo test_comprehensive DecorateInfo unittest expectedFailure TestMathBits test_neg_view OpInfo nn functional soft_margin_loss dtypes=floating_types_and torch half torch bfloat supports_out=False supports_forward_ad=True doesn t support grad target sample_inputs_func=partial sample_inputs_loss rhs_requires_grad=False error_inputs_func=error_inputs_soft_margin_loss OpInfo nn functional upsample_nearest supports_autograd=True supports_forward_ad=True supports_fwgrad_bwgrad=True dtypes=floating_types_and torch uint torch half torch bfloat gradcheck_nondet_tol=GRADCHECK_NONDET_TOL sample_inputs_func=partial sample_inputs_upsample nearest skips= RuntimeError false INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit supports_out=False OpInfo nn functional margin_ranking_loss dtypes=all_types_and torch half torch bfloat supports_out=False sample_inputs_func=sample_inputs_margin_ranking_loss error_inputs_func=error_inputs_margin_ranking_loss reference_inputs_func=reference_inputs_margin_ranking_loss supports_forward_ad=True supports_fwgrad_bwgrad=True OpInfo nn functional multi_margin_loss dtypes=floating_types dtypesIfCUDA=floating_types_and torch bfloat torch float supports_out=False supports_gradgrad=False sample_inputs_func=sample_inputs_multi_margin_loss reference_inputs_func=reference_inputs_multi_margin_loss error_inputs_func=error_inputs_multi_margin_loss decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestJit test_variant_consistency_jit OpInfo nn functional multilabel_margin_loss dtypes=floating_types dtypesIfCUDA=floating_types_and torch bfloat torch float supports_out=False supports_gradgrad=False sample_inputs_func=sample_inputs_multilabel_margin_loss reference_inputs_func=reference_inputs_multilabel_margin_loss error_inputs_func=error_inputs_multilabel_margin_loss OpInfo nn functional leaky_relu aliases=None aten_name= leaky_relu aten_backward_name= leaky_relu_backward sample_inputs_func=sample_inputs_leaky_relu dtypes=floating_types_and torch bfloat torch float inplace_variant=lambda x negative_slope= torch nn functional leaky_relu x negative_slope inplace=True supports_autograd=True assert_autodiffed=True supports_gradgrad=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True autodiff_nonfusible_nodes= aten leaky_relu OpInfo nn functional multilabel_soft_margin_loss supports_out=False dtypes=floating_types_and torch half torch bfloat sample_inputs_func=sample_inputs_multilabel_soft_margin_loss supports_forward_ad=True supports_fwgrad_bwgrad=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestJit test_variant_consistency_jit DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cuda skips= AssertionError False true Scalars failed compare equal = __main__ TestJitCUDA test_variant_consistency_jit_nn_functional_multilabel_soft_margin_loss_cuda_float leaked bytes CUDA memory device DecorateInfo Skip instead expectedFailure because fails locally me passes CI unittest skip Skipped TestJit test_variant_consistency_jit device_type= cuda OpInfo nn functional avg_pool d aten_name= avg_pool d supports_autograd=True supports_forward_ad=True supports_fwgrad_bwgrad=True dtypes=floating_types_and torch int torch float torch bfloat dtypesIfCUDA=floating_types_and torch float torch bfloat error_inputs_func=error_inputs_avg_pool d sample_inputs_func=sample_inputs_avgpool d skips= DecorateInfo unittest expectedFailure TestCommon test_out device_type= cuda OpInfo nn functional fractional_max_pool d supports_autograd=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True op=lambda input args kwargs wrapper_set_seed torch nn functional fractional_max_pool d input args kwargs vmap does support random operations check_batched_forward_grad=False dtypes=floating_types_and torch bfloat torch float test_neg_view=False sample_inputs_func=sample_inputs_fractional_max_pool d decorators= FIXME AssertionError False true Tensors failed compare equal DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError input- type - kind == TypeKind OptionalType INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit skips= DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu OpInfo nn functional fractional_max_pool d supports_autograd=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True op=lambda input args kwargs wrapper_set_seed torch nn functional fractional_max_pool d input args kwargs vmap does support random operations check_batched_forward_grad=False dtypes=floating_types_and torch bfloat torch float test_neg_view=False gradcheck_nondet_tol=GRADCHECK_NONDET_TOL sample_inputs_func=sample_inputs_fractional_max_pool d decorators= FIXME both derivatives implemented incorrectly https github com pytorch pytorch issues FIXME AssertionError False true Tensors failed compare equal DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError input- type - kind == TypeKind OptionalType INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit skips= DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu OpInfo nn functional max_pool d aten_name= max_pool d supports_autograd=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True got Batching rule implemented aten flatten using_ints check_batched_forward_grad=False TODO add shape checks assert_jit_shape_analysis=False dtypes=floating_types_and torch bfloat torch float dtypesIfCUDA=floating_types_and torch float torch bfloat skips= Pre-existing condition Needs fixed DecorateInfo unittest skip Works some configs TestNNCOpInfo test_nnc_correctness dtypes= torch bfloat RuntimeError The tensor has non-zero number elements its data allocated yet Caffe uses lazy allocation so you will need call mutable_data raw_mutable_data actually allocate memory DecorateInfo unittest skip Skipped TestTags test_tags error_inputs_func=error_inputs_max_pool d sample_inputs_func=sample_inputs_max_pool OpInfo nn functional max_pool d aten_name= max_pool d Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True Vmap happy non-contiguous channels_last inputs check_batched_gradgrad=False supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True got Batching rule implemented aten flatten using_ints check_batched_forward_grad=False assert_jit_shape_analysis=True dtypes=all_types_and torch float torch bfloat dtypesIfCUDA=floating_types_and torch float torch bfloat error_inputs_func=error_inputs_max_pool d sample_inputs_func=sample_inputs_max_pool OpInfo max_pool d_with_indices_backward op=max_pool d_backward We ve defined custom op so there s no corresponding aten op aten_name=None method_variant=None inplace_variant=None operator_variant=None inplace_operator_variant=None check_batched_gradgrad=False supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True check_batched_forward_grad=False assert_jit_shape_analysis=False dtypes=floating_types_and torch bfloat torch float sample_inputs_func=sample_inputs_max_pool skips= We ve defined custom op here we don t handle case where we receive out kwarg DecorateInfo unittest skip Skipped TestCommon test_out DecorateInfo unittest expectedFailure TestCommon test_out_warning FX failed normalize op - add op op_skip list DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive object has no attribute max_pool d_with_indices_backward It s available torch -- so expected DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit OpInfo nn functional max_pool d aten_name= max_pool d Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True got Batching rule implemented aten flatten using_ints check_batched_forward_grad=False TODO add shape checks assert_jit_shape_analysis=False dtypes=all_types_and torch bfloat torch float dtypesIfCUDA=floating_types_and torch float torch bfloat TODO investigate nondeterminism gradcheck_nondet_tol=GRADCHECK_NONDET_TOL error_inputs_func=error_inputs_max_pool d sample_inputs_func=sample_inputs_max_pool OpInfo nn functional max_unpool d aten_name= max_unpool d supports_autograd=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False assert_jit_shape_analysis=False dtypes=floating_types_and torch float torch bfloat sample_inputs_func=sample_inputs_max_unpool skips= Gradients tested ` variant_test_name=grad ` below We skip tests here because there non-determinism backward gather when there writes into same memory location there several indices pointing same memory gradcheck oblivious about cannot perturb them all once see sample_inputs_max_unpool_grad find out more DecorateInfo unittest skip Skipped TestBwdGradients test_fn_grad DecorateInfo unittest skip Skipped TestBwdGradients test_fn_gradgrad DecorateInfo unittest skip Skipped TestFwdGradients test_forward_mode_AD active_if= IS_MACOS DecorateInfo unittest skip Skipped TestCompositeCompliance test_forward_ad device_type= cpu DecorateInfo unittest skip Skipped TestDecomp test_quick_core_backward OpInfo nn functional max_unpool d variant_test_name= grad aten_name= max_unpool d supports_autograd=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False assert_jit_shape_analysis=False dtypes=floating_types_and torch float torch bfloat sample_inputs_func=sample_inputs_max_unpool_grad OpInfo nn functional max_unpool d aten_name= max_unpool d supports_autograd=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False assert_jit_shape_analysis=False dtypes=floating_types_and torch float torch bfloat sample_inputs_func=sample_inputs_max_unpool skips= Gradients tested ` variant_test_name=grad ` below We skip tests here because there non-determinism backward gather when there writes into same memory location there several indices pointing same memory gradcheck oblivious about cannot perturb them all once see sample_inputs_max_unpool_grad find out more DecorateInfo unittest skip Skipped TestFwdGradients test_forward_mode_AD active_if= IS_MACOS DecorateInfo unittest skip Skipped TestBwdGradients test_fn_gradgrad DecorateInfo unittest skip Skipped TestBwdGradients test_fn_grad DecorateInfo unittest skip Skipped TestCompositeCompliance test_forward_ad DecorateInfo unittest skip Skipped TestDecomp test_quick_core_backward OpInfo nn functional max_unpool d variant_test_name= grad aten_name= max_unpool d Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True Vmap happy non-contiguous channels_last inputs check_batched_grad=False supports_out=False assert_jit_shape_analysis=False dtypes=floating_types_and torch float torch bfloat sample_inputs_func=sample_inputs_max_unpool_grad OpInfo nn functional max_unpool d aten_name= max_unpool d Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False assert_jit_shape_analysis=False dtypes=floating_types_and torch float torch bfloat sample_inputs_func=sample_inputs_max_unpool skips= Gradients tested ` variant_test_name=grad ` below We skip tests here because there non-determinism backward gather when there writes into same memory location there several indices pointing same memory gradcheck oblivious about cannot perturb them all once see sample_inputs_max_unpool_grad find out more DecorateInfo unittest skip Skipped TestFwdGradients test_forward_mode_AD active_if= IS_MACOS DecorateInfo unittest skip Skipped TestBwdGradients test_fn_gradgrad DecorateInfo unittest skip Skipped TestBwdGradients test_fn_grad DecorateInfo unittest skip Skipped TestCompositeCompliance test_forward_ad DecorateInfo unittest skip Skipped TestDecomp test_quick_core_backward OpInfo nn functional max_unpool d variant_test_name= grad aten_name= max_unpool d supports_autograd=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False assert_jit_shape_analysis=False dtypes=floating_types_and torch float torch bfloat sample_inputs_func=sample_inputs_max_unpool_grad OpInfo nn functional linear aten_name= linear supports_autograd=True supports_gradgrad=True sample_inputs_func=sample_inputs_linear dtypes=all_types_and_complex_and torch float torch bfloat dtypesIfROCM=floating_and_complex_types_and torch float torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch float torch bfloat backward_dtypesIfCUDA=floating_and_complex_types_and torch float torch bfloat linear calls mm under hood which nondeterministic CUDA https pytorch org docs stable generated torch use_deterministic_algorithms html#torch use_deterministic_algorithms gradcheck_nondet_tol=GRADCHECK_NONDET_TOL supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch issues check_batched_forward_grad=False supports_expanded_weight=True decorators= Strides same DecorateInfo unittest expectedFailure TestCommon test_out OpInfo nn functional bilinear aten_name= bilinear supports_autograd=True sample_inputs_func=sample_inputs_bilinear dtypes=all_types_and torch float torch bfloat dtypesIfCUDA=floating_types_and torch float torch bfloat SM OrLater TEST_WITH_ROCM decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cpu skips= NVIDIA only assures bfloat supported bmm SM = DecorateInfo unittest skip Skipped TestCommon test_dtypes device_type= cuda active_if=not SM OrLater DecorateInfo unittest skip Skipped TestNNCOpInfo test_nnc_correctness dtypes= torch bfloat Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False OpInfo nn functional glu aten_name= glu Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True sample_inputs_func=sample_inputs_glu dtypes=floating_types_and torch bfloat torch float supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False UnaryUfuncInfo nn functional elu aten_backward_name= elu_backward ref=lambda x alpha= inplace=False np maximum x + np minimum alpha np exp x - dtypes=floating_types_and torch bfloat torch float supports_forward_ad=True supports_fwgrad_bwgrad=True supports_autograd=True assert_autodiffed=False supports_gradgrad=True supports_out=False sample_kwargs=lambda device dtype input alpha alpha inplace_variant=lambda x alpha= torch nn functional elu x alpha inplace=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch bfloat tol atol= e- rtol= e- TestUnaryUfuncs device_type= cuda Marked Unary function because has some rather odd broadcasting semantics its second argument UnaryUfuncInfo nn functional prelu aten_backward_name= _prelu_kernel_backward ref=lambda x weight np maximum x + np minimum x weight x ndim == weight reshape weight size i == i range x ndim dtypes=floating_types_and torch bfloat torch float supports_forward_ad=True supports_fwgrad_bwgrad=True supports_autograd=True assert_autodiffed=False supports_gradgrad=True supports_out=False test_reference_numerics only tests case when weight tensor scalar sample_kwargs=sample_kwargs_prelu_scalar_weight error_inputs_func=error_inputs_prelu sample_inputs_func=sample_inputs_prelu reference_inputs_func=reference_inputs_prelu decorators= RuntimeError Cannot insert Tensor requires grad constant Consider making parameter input detaching gradient https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit UnaryUfuncInfo nn functional celu ref=lambda x alpha= inplace=False np maximum x + np minimum alpha np exp x alpha - dtypes=floating_types_and torch bfloat torch float supports_forward_ad=True supports_fwgrad_bwgrad=True supports_autograd=True assert_autodiffed=False supports_gradgrad=True supports_out=False sample_kwargs=lambda device dtype input alpha alpha inplace_variant=lambda x alpha= torch nn functional celu x alpha inplace=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch bfloat tol atol= e- rtol= e- TestUnaryUfuncs device_type= cuda UnaryUfuncInfo nn functional rrelu aten_backward_name= rrelu_with_noise_backward op=lambda input args kwargs wrapper_set_seed torch nn functional rrelu input args kwargs inplace_variant=lambda input args kwargs wrapper_set_seed torch nn functional rrelu input args inplace=True kwargs dtypes=floating_types_and torch bfloat dtypesIfCUDA=floating_types_and torch float torch bfloat gradcheck_wrapper=wrapper_set_seed supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False sample_kwargs=lambda device dtype input dict lower= upper= training=True dict lower= upper= training=True sample_inputs_func=sample_inputs_rrelu error_inputs_func=error_inputs_rrelu decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch bfloat tol atol= e- rtol= e- TestUnaryUfuncs device_type= cuda skips= lambda impl DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive lambda impl DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit In-place operations do play well forward AD https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestFwdGradients test_inplace_forward_mode_AD The noise vector s generated these tests same elementwise DecorateInfo unittest skip Different noise TestUnaryUfuncs test_batch_vs_slicing DecorateInfo unittest skip Different noise TestUnaryUfuncs test_contig_vs_every_other DecorateInfo unittest skip Different noise TestUnaryUfuncs test_non_contig_expand DecorateInfo unittest skip Different noise TestUnaryUfuncs test_contig_vs_transposed DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu skip_correctness_check_compile_vs_eager=True UnaryUfuncInfo nn functional selu ref=lambda x inplace=False np maximum x + np minimum np exp x - dtypes=floating_types_and torch bfloat torch float supports_forward_ad=True depends elu supports_fwgrad_bwgrad=True supports_autograd=True assert_autodiffed=False supports_gradgrad=True supports_out=False inplace_variant=lambda x torch nn functional selu x inplace=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch bfloat tol atol= e- rtol= e- TestUnaryUfuncs device_type= cuda OpInfo torch _scaled_mm sample_inputs_func=sample_inputs_scaled_mm dtypes=float _types dtypesIfCUDA=empty_types + torch float _e m fn supports_out=True supports_forward_ad=False supports_autograd=False decorators= skipXPU skipCUDAIf SM OrLater TEST_WITH_ROCM Requires CUDA SM = skips= Sample inputs isn t really parametrized dtype DecorateInfo unittest skip Skipped TestCommon test_dtypes add_stub implemented Float _e m fn ufunc_add_CUDA implemented Float _e m fn https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestCommon test_out mul_cuda implemented float _e m fn mul_cpu_reduced_float implemented Float _e m fn https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestSchemaCheckModeOpInfo test_schema_correctness aten _scaled_mm hit vmap fallback which currently disabled DecorateInfo unittest skip Skipped TestVmapOperatorsOpInfo test_op_has_batch_rule DecorateInfo unittest skip Skipped TestVmapOperatorsOpInfo test_vmap_exhaustive DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness dtypes= torch float _e m fn torch float _e m fnuz torch float _e m torch float _e m fnuz OpInfo torch ops aten _safe_softmax default dtypes=all_types_and torch half torch bfloat torch bool sample_inputs_func=sample_inputs_safe_softmax assert_jit_shape_analysis=True assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False supports_cow_input_no_materialize_backward=False decorators= skips= DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit OpInfo nn functional scaled_dot_product_attention op=lambda args kwargs wrapper_set_seed torch nn functional scaled_dot_product_attention args kwargs sample_inputs_func=sample_inputs_scaled_dot_product_attention dtypes=floating_types_and torch float torch bfloat supports_out=False supports_forward_ad=False supports_fwgrad_bwgrad=True check_batched_forward_grad=False decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestCommon skips= When attn mask composite tensor fails backward returning none DecorateInfo unittest skip Skipped TestCompositeCompliance test_backward device_type= cuda This only failing Linux Bionic Cuda DecorateInfo unittest skip Skipped TestCommon test_dtypes device_type= cuda active_if=_get_torch_cuda_version = DecorateInfo unittest skip Skipped TestCommon test_noncontiguous_samples dtypes= torch float AssertionError JIT Test does execute any logic DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit Forward works dtype=float which math path DecorateInfo unittest skip Skipped TestFwdGradients test_forward_mode_AD Not implemented Forward AD DecorateInfo unittest skip Skipped TestFwdGradients test_fn_fwgrad_bwgrad device_type= cpu Not implemented backward derivative DecorateInfo unittest skip Skipped TestBwdGradients test_fn_gradgrad device_type= cpu CPU CUDA have inconsistencies intermediate outputs DecorateInfo unittest skip Skipped TestMeta test_dispatch_meta_outplace device_type= cpu DecorateInfo unittest skip Skipped TestMeta test_dispatch_symbolic_meta_outplace device_type= cpu When changing input Tensor CompositeCompliantTensor input requires_grad changes true false DecorateInfo unittest skip Skipped TestCompositeCompliance test_backward device_type= cpu OpInfo implemented lambda DecorateInfo unittest skip Skipped TestNormalizeOperators test_normalize_operator_exhaustive TODO Need understand what testing why doesn t work DecorateInfo unittest skip Skipped TestDecomp test_comprehensive DecorateInfo unittest skip output non-deterministic when dropout_p TestCommon test_compare_cpu TODO skip now since we can t skip runtime arch support DecorateInfo unittest skip This TestInductorOpInfo test_comprehensive skip sm DecorateInfo unittest skip Skipped TestSchemaCheckModeOpInfo test_schema_correctness device_type= cuda dtypes= torch bfloat active_if=not SM OrLater FIXME DecorateInfo unittest skip test_cow_input does work efficient attention ROCM TestCompositeCompliance test_cow_input device_type= cuda dtypes= torch bfloat torch float torch float active_if=TEST_WITH_ROCM PLATFORM_SUPPORTS_MEM_EFF_ATTENTION OpInfo torch ops aten _flash_attention_forward sample_inputs_func=sample_inputs_flash_attention_forward dtypes=empty_types dtypesIfCUDA=custom_types torch float SM OrLater custom_types torch float torch bfloat supports_out=False supports_autograd=True supports_fwgrad_bwgrad=False supports_forward_ad=False check_batched_forward_grad=False decorators= skipCUDAIf PLATFORM_SUPPORTS_FLASH_ATTENTION This platform doesn t support Flash Attention skips= Checking scalar value philox seed offset DecorateInfo unittest expectedFailure TestCompositeCompliance test_operator device_type= cuda DecorateInfo unittest expectedFailure TestCommon test_noncontiguous_samples device_type= cuda DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit device_type= cuda None Mismatch Tensor DecorateInfo unittest expectedFailure TestCompositeCompliance test_backward device_type= cuda OpInfo torch ops aten _efficient_attention_forward sample_inputs_func=sample_inputs_efficient_attention_forward dtypes=empty_types dtypesIfCUDA=custom_types torch float torch float SM OrLater custom_types torch float torch float torch bfloat supports_out=False supports_autograd=True supports_fwgrad_bwgrad=False supports_forward_ad=False check_batched_forward_grad=False TODO Skip because produces CUDA illegal memory access some reason skip_cow_input_backward=True FIXME mask_type == LowerRight decorators= skipCUDAIf PLATFORM_SUPPORTS_MEM_EFF_ATTENTION This platform doesn t support efficient attention skipCUDAIf TEST_WITH_ROCM Efficient attention ROCM doesn t support custom_mask_type== skipXPU skips= Checking scaler value philox seed offset DecorateInfo unittest expectedFailure TestCompositeCompliance test_operator device_type= cuda DecorateInfo unittest expectedFailure TestCommon test_noncontiguous_samples device_type= cuda DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit device_type= cuda None Mismatch Tensor DecorateInfo unittest expectedFailure TestCompositeCompliance test_backward device_type= cuda UnaryUfuncInfo nn functional silu aten_backward_name= silu_backward ref=lambda x inplace=False x + np exp -x dtypes=floating_types_and torch bfloat torch float supports_forward_ad=True supports_autograd=True supports_fwgrad_bwgrad=True assert_autodiffed=True supports_out=False inplace_variant=lambda x torch nn functional silu x inplace=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch bfloat tol atol= e- rtol= e- TestUnaryUfuncs device_type= cuda skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_normal dtypes= torch cfloat device_type= cpu autodiff_nonfusible_nodes= aten silu TODO combine nn functional silu OpInfo when complex autodiff silu supported when forward bug fixed Note silu errors when given inputs require grad doesn t support grad their dtype This why dtypes list above passes test_dtypes because s getting lucky failing forward because test_dtypes sets requires_grad True THIS IS A BUG UnaryUfuncInfo nn functional silu variant_test_name= complex ref=lambda x inplace=False x + np exp -x dtypes=complex_types dtypesIfCUDA=complex_types supports_forward_ad=False supports_autograd=False assert_autodiffed=False supports_out=False inplace_variant=lambda x torch nn functional silu x inplace=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch bfloat tol atol= e- rtol= e- TestUnaryUfuncs device_type= cuda skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_normal dtypes= torch cfloat FIXME intentionally misreports dtypes DecorateInfo unittest expectedFailure TestCommon test_dtypes FIXME numpy reference diverges Comparing nan+nanj - + j DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch complex torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_small dtypes= torch complex DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch complex UnaryUfuncInfo nn functional hardsigmoid aten_backward_name= hardsigmoid_backward ref=reference_hardsigmoid dtypes=floating_types_and torch bfloat torch float supports_autograd=True assert_autodiffed=False supports_gradgrad=False supports_forward_ad=True supports_out=False inplace_variant=partial torch nn functional hardsigmoid inplace=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= TestUnaryUfuncs device_type= cuda skips= still want test first derivative works though second derivative isn t supported DecorateInfo unittest expectedFailure TestBwdGradients test_inplace_gradgrad UnaryUfuncInfo nn functional logsigmoid aten_name= log_sigmoid aten_backward_name= log_sigmoid_backward ref=reference_logsigmoid dtypes=floating_types_and torch half torch bfloat supports_autograd=True assert_autodiffed=False supports_forward_ad=True supports_fwgrad_bwgrad=True supports_gradgrad=True autodiff_nonfusible_nodes= aten log_sigmoid decorators= DecorateInfo precisionOverride torch float e- torch bfloat e- TestUnaryUfuncs test_reference_numerics_small DecorateInfo precisionOverride torch float e- torch bfloat e- TestUnaryUfuncs test_reference_numerics_large DecorateInfo precisionOverride torch float e- torch bfloat e- TestUnaryUfuncs test_reference_numerics_extremal skips= Resized non-empty tensor did warn about DecorateInfo unittest expectedFailure TestCommon test_out_warning device_type= cpu UnaryUfuncInfo nn functional mish aten_backward_name= mish_backward ref=lambda x x np tanh reference_softplus x dtypes=floating_types_and torch bfloat torch float supports_forward_ad=True supports_fwgrad_bwgrad=True supports_autograd=True assert_autodiffed=False supports_gradgrad=True supports_out=False inplace_variant=partial torch nn functional mish inplace=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestUnaryUfuncs UnaryUfuncInfo nn functional softsign ref=lambda x x np abs x + dtypes=all_types_and_complex_and torch float torch bfloat dtypesIfCUDA=all_types_and_complex_and torch float torch bfloat torch bool supports_forward_ad=True supports_fwgrad_bwgrad=True supports_autograd=True assert_autodiffed=False supports_gradgrad=True supports_out=False decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestUnaryUfuncs skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_small dtypes= torch int torch int UnaryUfuncInfo nn functional tanhshrink ref=lambda x x - np tanh x dtypes=all_types_and_complex_and torch half torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True supports_autograd=True assert_autodiffed=False supports_gradgrad=True supports_out=False decorators= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_normal device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo toleranceOverride torch bfloat tol atol= e- rtol= e- TestUnaryUfuncs DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- torch bfloat tol atol= e- rtol= e- TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda skips= each case pytorch will produce nan while numpy will DecorateInfo unittest skip Fails some jobs works others TestUnaryUfuncs test_reference_numerics_large dtypes= torch complex torch complex active_if= IS_MACOS DecorateInfo unittest skip Fails some jobs works others TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch complex torch complex device_type= cpu active_if= IS_MACOS IS_WINDOWS tan j pi odd_number nan which also make tanhshrink nan reference_numerics_filter=NumericsFilter condition=lambda x close_to_int x math pi j x is_complex x new_tensor False dtype=torch bool safe_val= UnaryUfuncInfo nn functional threshold ref=lambda x threshold value np where x = threshold value x astype x dtype dtypes=all_types_and torch half torch bfloat inplace_variant=lambda x threshold value torch nn functional threshold x threshold value inplace=True supports_forward_ad=True supports_fwgrad_bwgrad=True assert_autodiffed=False supports_gradgrad=True supports_out=False sample_kwargs=lambda device dtype input threshold float fromhex x ap- value - threshold float fromhex x ap- value - TODO whc should need sample_inputs_func without kwargs aren t being hooked up properly sample_inputs_func=sample_inputs_threshold OpInfo nn functional triplet_margin_loss sample_inputs_func=sample_inputs_triplet_margin_loss error_inputs_func=error_inputs_triplet_margin_loss dtypes=all_types_and_complex_and torch half torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True OpInfo nn functional triplet_margin_with_distance_loss sample_inputs_func=partial sample_inputs_triplet_margin_loss with_distance=True error_inputs_func=error_inputs_triplet_margin_loss dtypes=all_types_and_complex_and torch half torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True skips= This test cannot handle callable passed ` distance_function ` If we would use ` distance_function=None ` test would pass fine DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive BinaryUfuncInfo nextafter dtypes=floating_types_and torch bfloat torch half supports_autograd=False supports_rhs_python_scalar=False OpInfo op=lambda x args kwargs x args kwargs dtypes=all_types_and_complex_and torch bfloat torch float torch bool supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False sample_inputs_func=sample_inputs_to skips= RuntimeError undefined value cpu DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit device_type= cpu NotImplementedError Cannot copy out meta tensor no data DecorateInfo unittest skip Skipped TestMeta test_meta_outplace https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestProxyTensorOpInfo test_make_fx_symbolic_exhaustive DecorateInfo unittest skip Skipped TestNormalizeOperators test_normalize_operator_exhaustive OpInfo topk dtypes=all_types_and torch bfloat torch float supports_forward_ad=True supports_fwgrad_bwgrad=True assert_jit_shape_analysis=True sample_inputs_func=sample_inputs_topk Multiple variants batch_norm test without cuDNN disabled See https github com pytorch pytorch pull #discussion_r more details OpInfo nn functional batch_norm aten_name= batch_norm dtypes=floating_types_and torch float torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True assert_jit_shape_analysis=True allow_cow_input_materialize_forward= allow_cow_input_materialize_backward= sample_inputs_func=sample_inputs_batch_norm skips= see https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness DecorateInfo unittest skip Skipped TestNNCOpInfo test_nnc_correctness device_type= cpu dtypes= torch bfloat torch float DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestCompositeCompliance test_forward_ad device_type= cpu This variant tests batch_norm cuDNN disabled only CUDA devices OpInfo nn functional batch_norm variant_test_name= without_cudnn aten_name= batch_norm dtypes=empty_types dtypesIfCUDA=floating_types_and torch float torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True allow_cow_input_materialize_forward= allow_cow_input_materialize_backward= decorators= onlyCUDA disablecuDNN skips= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestJit test_variant_consistency_jit sample_inputs_func=sample_inputs_batch_norm OpInfo nn functional binary_cross_entropy aten_backward_name= binary_cross_entropy_backward sample_inputs_func=sample_inputs_binary_cross_entropy dtypes=floating_types_and torch float torch bfloat dtypesIfCUDA=floating_types_and torch float torch bfloat supports_out=False gradcheck_fast_mode=False supports_autograd=True supports_forward_ad=True supports_fwgrad_bwgrad=True decorators= RuntimeError expected int position got Tensor DecorateInfo unittest skip Skipped TestCudaFuserOpInfo RuntimeError expected int position got Tensor DecorateInfo unittest skip Skipped TestNNCOpInfo test_nnc_correctness Fails unknown reason https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestCompositeCompliance test_cow_input device_type= cuda DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestJit test_variant_consistency_jit RuntimeError output shape doesn t match broadcast shape DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace_all_strides skips= RuntimeError expected int position got Tensor DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit We have add OpInfo entry ` igamma ` ` igammac ` First standard entry second run gradcheck tests second argument BinaryUfuncInfo igamma dtypes=floating_types_and torch bfloat torch float aliases= torch special gammainc dtypesIfCUDA=floating_types TODO FIXME supports_rhs_python_scalar=False supports_autograd=False skips= FIXME incorrectly tries pass rhs scalar DecorateInfo unittest expectedFailure TestJit test_jit_alias_remapping TODO FIXME ideally implemented grad both inputs BinaryUfuncInfo igamma variant_test_name= grad_other Since autograd formula implemented only other gradcheck test verifies formula input SampleInput we permute arguments op=lambda other kwargs torch igamma other kwargs inplace_variant=None method_variant=None supports_rhs_python_scalar=False rhs_make_tensor_kwargs=dict requires_grad=False dtypes=floating_types_and torch bfloat torch float backward_dtypesIfCPU=floating_types_and torch bfloat dtypesIfCUDA=floating_types backward_dtypesIfCUDA=floating_types supports_inplace_autograd=False skips= Derivative wrt first tensor implemented DecorateInfo unittest expectedFailure TestCommon test_floating_inputs_are_differentiable test does work passing lambda op AssertionError False true Tensors failed compare equal DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit test fails we permute arguments function variant inplace method DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager TypeError igamma argument input position must Tensor float DecorateInfo unittest skip Skipped TestBinaryUfuncs BinaryUfuncInfo igammac dtypes=floating_types_and torch bfloat torch float aliases= torch special gammaincc dtypesIfCUDA=floating_types supports_autograd=False supports_rhs_python_scalar=False skips= FIXME incorrectly tries pass rhs scalar DecorateInfo unittest expectedFailure TestJit test_jit_alias_remapping TODO FIXME ideally implementing grad both inputs BinaryUfuncInfo igammac variant_test_name= grad_other Since autograd formula implemented only other gradcheck test verifies formula input SampleInput we permute arguments op=lambda other kwargs torch igammac other kwargs inplace_variant=None method_variant=None supports_rhs_python_scalar=False rhs_make_tensor_kwargs=dict requires_grad=False dtypes=floating_types_and torch bfloat torch float backward_dtypesIfCPU=floating_types_and torch bfloat dtypesIfCUDA=floating_types backward_dtypesIfCUDA=floating_types supports_inplace_autograd=False decorators= Derivative wrt first tensor implemented DecorateInfo unittest expectedFailure TestCommon test_floating_inputs_are_differentiable skips= test does work passing lambda op AssertionError False true Tensors failed compare equal DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit test fails we permute arguments function variant inplace method DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager TypeError igammac argument input position must Tensor float DecorateInfo unittest skip Skipped TestBinaryUfuncs UnaryUfuncInfo nn functional softshrink aten_name= softshrink aten_backward_name= softshrink_backward dtypes=floating_types_and torch bfloat torch float supports_forward_ad=True supports_fwgrad_bwgrad=True assert_autodiffed=False sample_inputs_func=sample_inputs_softshrink error_inputs_func=error_inputs_softshrink UnaryUfuncInfo nn functional hardshrink aten_name= hardshrink aten_backward_name= hardshrink_backward dtypes=floating_types_and torch bfloat torch float assert_autodiffed=True sample_inputs_func=sample_inputs_hardshrink supports_forward_ad=True supports_fwgrad_bwgrad=True autodiff_nonfusible_nodes= aten hardshrink UnaryUfuncInfo nn functional hardtanh aten_name= hardtanh aten_backward_name= hardtanh_backward dtypes=floating_types_and torch int torch int torch int torch int torch half torch bfloat backward_dtypes=all_types_and torch half torch bfloat backward_dtypesIfCUDA=floating_types_and torch float torch bfloat assert_autodiffed=True sample_inputs_func=sample_inputs_hardtanh error_inputs_func=error_inputs_hardtanh supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True autodiff_nonfusible_nodes= aten hardtanh OpInfo nn functional gelu aten_name= gelu aten_backward_name= gelu_backward ref=reference_gelu TEST_SCIPY None error_inputs_func=error_inputs_gelu supports_autograd=True assert_autodiffed=True sample_inputs_func=sample_inputs_gelu dtypes=floating_types_and torch bfloat torch half supports_gradgrad=True supports_forward_ad=True supports_fwgrad_bwgrad=True autodiff_nonfusible_nodes= aten gelu skips= AssertionError Tensor-likes close May replicate CI DecorateInfo unittest skip Skipped TestCommon test_out DecorateInfo unittest skip Unsupported MPS now TestCommon test_numpy_ref_mps UnaryUfuncInfo nn functional relu aten_name= relu dtypes=all_types_and torch half torch bfloat backward_dtypes=floating_types_and torch half torch bfloat assert_autodiffed=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True autodiff_nonfusible_nodes= aten relu OpInfo mm dtypes=all_types_and_complex_and torch float torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch float torch bfloat assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_mm skips= Issue conj torch dispatch see https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestSchemaCheckModeOpInfo test_schema_correctness dtypes= torch complex torch complex Fast math MacOS- DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestConsistency test_output_match active_if=lambda _ MACOS_VERSION device_type= mps dtypes= torch float OpInfo mode op=torch mode dtypes=all_types_and torch float torch bfloat torch bool supports_forward_ad=True supports_fwgrad_bwgrad=True skips= Resized non-empty tensor did warn about DecorateInfo unittest expectedFailure TestCommon test_out_warning FIXME Expected got Absolute difference up allowed Relative difference up allowed DecorateInfo unittest skip Skipped TestCommon test_compare_cpu dtypes= torch float device_type= cuda sample_inputs_func=sample_inputs_mode make_mvlgamma_opinfo variant_test_name= mvlgamma_p_ domain= None skips=skips_mvlgamma sample_kwargs=lambda device dtype input p d make_mvlgamma_opinfo variant_test_name= mvlgamma_p_ domain= None skips=skips_mvlgamma sample_kwargs=lambda device dtype input p d make_mvlgamma_opinfo variant_test_name= mvlgamma_p_ domain= None skips=skips_mvlgamma sample_kwargs=lambda device dtype input p d BinaryUfuncInfo ne ref=np not_equal aliases= not_equal dtypes=all_types_and_complex_and torch bool torch bfloat torch float always_returns_bool=True supports_autograd=False skips= OpInfo narrow dtypes=all_types_and_complex_and torch bool torch bfloat torch float torch chalf supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=partial sample_inputs_narrow_narrow_copy is_narrow=True reference_inputs_func=partial reference_inputs_narrow_narrow_copy is_narrow=True error_inputs_func=partial error_inputs_narrow_narrow_copy is_narrow=True is_ref=False skips= Use item DecorateInfo unittest expectedFailure TestCompositeCompliance test_operator DecorateInfo unittest expectedFailure TestCompositeCompliance test_backward DecorateInfo unittest expectedFailure TestCompositeCompliance test_forward_ad DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_view OpInfo narrow_copy dtypes=all_types_and_complex_and torch bool torch bfloat torch float torch chalf supports_out=True supports_forward_ad=False supports_fwgrad_bwgrad=False supports_autograd=False https github com pytorch pytorch issues sample_inputs_func=partial sample_inputs_narrow_narrow_copy is_narrow=False reference_inputs_func=partial reference_inputs_narrow_narrow_copy is_narrow=False error_inputs_func=partial error_inputs_narrow_narrow_copy is_narrow=False is_ref=False skips= https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestCommon test_out DecorateInfo unittest expectedFailure TestCommon test_out_warning Could run aten narrow_copy out arguments CUDA backend DecorateInfo unittest expectedFailure TestMeta test_meta_outplace device_type= cuda DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace device_type= cuda DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace device_type= cuda DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace_all_strides OpInfo view_copy dtypes=all_types_and_complex_and torch bool torch bfloat torch float ref=lambda x newshape np reshape x newshape copy supports_out=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_autograd=True sample_inputs_func=sample_inputs_view_reshape error_inputs_func=error_inputs_view_reshape skips= RuntimeError view size compatible input tensor s size stride least one dimension spans across two contiguous subspaces Use reshape instead DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace_all_strides UnaryUfuncInfo neg aliases= negative ref=np negative dtypes=all_types_and_complex_and torch half torch bfloat torch chalf error_inputs_func=error_inputs_neg supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True assert_autodiffed=True OpInfo dist op=torch dist dtypes=floating_and_complex_types_and torch half torch bfloat Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_out=False supports_forward_ad=True torch autograd gradcheck GradcheckError While computing batched gradients got Could allocate memory change Tensor SizesAndStrides check_batched_forward_grad=False supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_dist OpInfo outer op=torch outer aliases= ger dtypes=all_types_and_complex_and torch bool torch float torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False sample_inputs_func=sample_inputs_outer OpInfo ormqr op=torch ormqr dtypes=floating_and_complex_types https github com pytorch pytorch issues gradcheck_fast_mode=True supports_forward_ad=False supports_fwgrad_bwgrad=False sample_inputs_func=sample_inputs_ormqr error_inputs_func=error_inputs_ormqr decorators= skipCUDAIfNoCusolver skipCPUIfNoLapack skips= Strides same DecorateInfo unittest expectedFailure TestCommon test_out OpInfo permute ref=np transpose dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf supports_out=False assert_autodiffed=True autodiff_fusible_nodes= aliases inputs shouldn t fused autodiff_nonfusible_nodes= aliases inputs shouldn t fused assert_jit_shape_analysis=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_varargs=True sample_inputs_func=sample_inputs_permute reference_inputs_func=reference_inputs_permute OpInfo permute_copy dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf supports_out=True assert_autodiffed=True assert_jit_shape_analysis=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_varargs=False torch permute also varargs sample_inputs_func=sample_inputs_permute reference_inputs_func=reference_inputs_permute skips= DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float BinaryUfuncInfo pow dtypes=all_types_and_complex_and torch half torch bfloat dtypesIfCUDA=all_types_and_complex_and torch half torch bfloat torch chalf ref=np power Due AVX currently being fully supported Float log_vml_cpu can t enabled Float causing test fail pow s autograd Float thus currently unsupported CPU backward_dtypes=floating_and_complex_types_and torch half torch bfloat backward_dtypesIfCUDA=floating_and_complex_types_and torch bfloat torch half torch chalf https github com pytorch pytorch issues gradcheck_fast_mode=True supports_inplace_autograd=False supports_forward_ad=True supports_fwgrad_bwgrad=True assert_autodiffed=True supports_one_python_scalar=True Integer types do support negative exponentes rhs_make_tensor_kwargs=dict low= Raising negative real numbers fractional powers supported lhs_make_tensor_kwargs=dict low= decorators= DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestBinaryUfuncs test_reference_numerics DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- torch complex tol atol= e- rtol= e- TestBinaryUfuncs test_scalar_support skips= Skipping integers because they being raised negative powers causing error DecorateInfo unittest expectedFailure TestBinaryUfuncs test_reference_numerics_small_values dtypes= torch int torch int torch int torch int DecorateInfo unittest expectedFailure TestBinaryUfuncs test_reference_numerics_large_values dtypes= torch int torch int torch int FIXME Complex values error Greatest absolute difference nan index Ref https github com pytorch pytorch issues For ` chalf ` reference computation ` numpy ` computed ` cfloat ` Output ` chalf ` saturates ` inf ` quicker than reference due its small range which leads failure test DecorateInfo unittest skip Skipped TestDecomp test_quick dtypes= torch complex active_if=TEST_WITH_ROCM FIXME Mismatched elements Greatest absolute difference nan index up e- allowed Greatest relative difference nan index up allowed DecorateInfo unittest skip Skipped TestDecomp test_comprehensive dtypes= torch complex DecorateInfo unittest skip Skipped TestCommon test_complex_half_reference_testing dtypes= torch complex active_if=TEST_WITH_ROCM DecorateInfo unittest skip Skipped TestBinaryUfuncs test_batch_vs_slicing dtypes= torch complex DecorateInfo unittest skip Skipped TestBinaryUfuncs test_non_contig dtypes= torch complex DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics dtypes= torch complex DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_small_values dtypes= torch complex torch complex torch complex DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_large_values dtypes= torch complex torch complex torch complex DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_extremal_values dtypes= torch complex torch complex torch complex BinaryUfuncInfo float_power ref=np float_power dtypes=all_types_and_complex_and torch half torch bfloat torch bool promotes_int_to_float=True https github com pytorch pytorch issues gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_one_python_scalar=True Integer types do support negative exponentes rhs_make_tensor_kwargs=dict low= Raising negative real numbers fractional powers supported lhs_make_tensor_kwargs=dict low= decorators= DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- torch complex tol atol= e- rtol= e- TestBinaryUfuncs test_scalar_support skips= FIXME AssertionError Object comparison failed torch float = torch float DecorateInfo unittest skip Skipped TestBinaryUfuncs test_type_promotion - e+ outside range representable values type float DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit Complex values error Greatest absolute difference nan index DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_small_values dtypes= torch complex torch complex DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_large_values dtypes= torch complex torch complex DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_extremal_values dtypes= torch complex torch complex Inplace always promotes double thus other floating dtypes supported DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes= torch bfloat torch float torch float OpInfo qr op=torch qr dtypes=floating_and_complex_types sample_inputs_func=sample_inputs_linalg_qr_geqrf supports_forward_ad=True supports_fwgrad_bwgrad=True In-place ops check_batched_gradgrad=False decorators= skipCUDAIfNoCusolver skipCPUIfNoLapack UnaryUfuncInfo rad deg ref=np degrees decorators= precisionOverride torch bfloat e- torch float e- dtypes=all_types_and torch bool torch half torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True promotes_int_to_float=True UnaryUfuncInfo real ref=np real dtypes=all_types_and_complex_and torch bool torch bfloat torch half torch chalf supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch issues check_batched_forward_grad=False skips= Skip since real imag don t have out variants DecorateInfo unittest expectedFailure TestUnaryUfuncs test_out_arg_all_dtypes OpInfo roll ref=np roll dtypes=all_types_and_complex_and torch bool torch bfloat torch half torch chalf error_inputs_func=error_inputs_roll supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_roll decorators= onlyNativeDeviceTypes OpInfo rot dtypes=all_types_and_complex_and torch bool torch bfloat torch half error_inputs_func=error_inputs_rot Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_rot To test reference numerics against multiple values argument ` decimals ` we make multiple OpInfo entries each entry corresponding different value decimals UnaryUfuncInfo round ref=np round aliases= special round dtypes=all_types_and torch half torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True skips= DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness dtypes=tuple t t integral_types t = torch uint DecorateInfo unittest skip Skipped TestNNCOpInfo test_nnc_correctness dtypes= torch bfloat supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True assert_autodiffed=True UnaryUfuncInfo round ref=np round variant_test_name= decimals_ aliases= special round dtypes=floating_types_and torch half torch bfloat sample_kwargs=lambda device dtype input decimals decimals sample_inputs_func=partial sample_inputs_elementwise_unary op_kwargs= decimals supports_forward_ad=True supports_fwgrad_bwgrad=True assert_autodiffed=False supports_sparse_csr=False UnaryUfuncInfo round ref=np round variant_test_name= decimals_ aliases= special round dtypes=floating_types_and torch bfloat dtypesIfCUDA=floating_types_and torch half torch bfloat sample_kwargs=lambda device dtype input decimals decimals sample_inputs_func=partial sample_inputs_elementwise_unary op_kwargs= decimals skips= test_ops already tested overload ` decimals_ ` opinfo entry DecorateInfo unittest skip Skipped TestCommon DecorateInfo unittest skip Skipped TestFwdGradients DecorateInfo unittest skip Skipped TestBwdGradients DecorateInfo unittest skip Skipped TestJit DecorateInfo unittest skip Skipped TestMathBits DecorateInfo toleranceOverride torch bfloat tol atol= e- rtol= TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda DecorateInfo toleranceOverride torch bfloat tol atol= e- rtol= TestUnaryUfuncs test_reference_numerics_normal device_type= cuda supports_forward_ad=True supports_fwgrad_bwgrad=True assert_autodiffed=False supports_sparse_csr=False UnaryUfuncInfo round ref=np round variant_test_name= decimals_neg_ aliases= special round dtypes=floating_types_and torch bfloat dtypesIfCUDA=floating_types_and torch half torch bfloat sample_kwargs=lambda device dtype input decimals - decimals - sample_inputs_func=partial sample_inputs_elementwise_unary op_kwargs= decimals - skips= test_ops already tested overload ` decimals_ ` opinfo entry DecorateInfo unittest skip Skipped TestCommon DecorateInfo unittest skip Skipped TestFwdGradients DecorateInfo unittest skip Skipped TestBwdGradients DecorateInfo unittest skip Skipped TestJit DecorateInfo unittest skip Skipped TestMathBits supports_forward_ad=True supports_fwgrad_bwgrad=True assert_autodiffed=False supports_sparse_csr=False UnaryUfuncInfo sin ref=np sin dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and_complex_and torch chalf torch bool torch half torch bfloat assert_autodiffed=True handles_large_floats=False supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True skips= Fails CUDA passes ROCm DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch cdouble device_type= cuda DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch cfloat torch cdouble device_type= cpu active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch cfloat torch cdouble device_type= cpu active_if=IS_WINDOWS DecorateInfo unittest skip Skipped sparse backward supported TestSparseUnaryUfuncs test_sparse_fn_grad DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestConsistency test_output_grad_match device_type= mps decorators= precisionOverride torch bfloat e- UnaryUfuncInfo sinc ref=np_sinc_with_fp _as_fp aliases= special sinc dtypes=all_types_and_complex_and torch bool torch half torch bfloat handles_large_floats=False supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True UnaryUfuncInfo sinh ref=np_unary_ufunc_integer_promotion_wrapper np sinh dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and_complex_and torch chalf torch bool torch half torch bfloat assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True promotes_int_to_float=True decorators= precisionOverride torch float e- skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble active_if= IS_MACOS IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cpu dtypes= torch cfloat torch cdouble active_if= IS_MACOS IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch cdouble Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cpu dtypes= torch int DecorateInfo unittest skip Skipped sparse backward supported TestSparseUnaryUfuncs test_sparse_fn_grad UnaryUfuncInfo sign ref=reference_sign dtypes=all_types_and torch bool torch bfloat torch half dtypesIfCUDA=all_types_and torch bool torch bfloat torch half supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True skips= Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch bfloat torch float torch float torch float UnaryUfuncInfo sgn ref=reference_sgn dtypes=all_types_and_complex_and torch bool torch bfloat torch half torch chalf backward_dtypes=floating_and_complex_types_and torch bfloat torch half backward_dtypesIfCUDA=floating_and_complex_types_and torch bfloat torch half torch chalf supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True skips= Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch bfloat torch float torch float torch float DecorateInfo unittest skip Skipped sparse backward supported TestSparseUnaryUfuncs test_sparse_fn_grad OpInfo split dtypes=all_types_and_complex_and torch bfloat torch half torch bool torch chalf sample_inputs_func=partial sample_inputs_split list_args=False supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False autodiff_fusible_nodes= aliases inputs shouldn t fused autodiff_nonfusible_nodes= aliases inputs shouldn t fused assert_autodiffed=True OpInfo split Cannot declare aten_name because test_variant_consistency_jit_split_list_args_cpu_float decomp_aten_name= split_with_sizes variant_test_name= list_args dtypes=all_types_and_complex_and torch bfloat torch half torch bool sample_inputs_func=partial sample_inputs_split list_args=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False ` unsafe_split ` supports only ` int ` split_size argument OpInfo unsafe_split dtypes=all_types_and_complex_and torch bfloat torch half torch bool torch chalf sample_inputs_func=partial sample_inputs_split list_args=False supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False autodiff_fusible_nodes= aliases inputs shouldn t fused autodiff_nonfusible_nodes= aliases inputs shouldn t fused assert_autodiffed=True check_batched_forward_grad=False OpInfo split_with_sizes dtypes=all_types_and_complex_and torch bfloat torch half torch bool torch chalf sample_inputs_func=sample_inputs_split_with_sizes autodiff_fusible_nodes= aliases inputs shouldn t fused autodiff_nonfusible_nodes= aliases inputs shouldn t fused supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True assert_autodiffed=True OpInfo split_with_sizes_copy dtypes=all_types_and_complex_and torch bfloat torch half torch bool torch chalf sample_inputs_func=sample_inputs_split_with_sizes supports_out=True supports_forward_ad=True supports_fwgrad_bwgrad=True skips= No error raised DecorateInfo unittest expectedFailure TestCommon test_out_requires_grad_error BinaryUfuncInfo __radd__ op=torch Tensor __radd__ dtypes=all_types_and_complex_and torch bfloat torch half torch bool supports_out=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True autodiff_nonfusible_nodes= aten add BinaryUfuncInfo __rdiv__ op=torch Tensor __rdiv__ dtypes=all_types_and_complex_and torch bfloat torch half torch bool promotes_int_to_float=True lhs_make_tensor_kwargs= exclude_zero True Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_out=False skips= https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit supports_forward_ad=True supports_fwgrad_bwgrad=True assert_autodiffed=True autodiff_nonfusible_nodes= aten mul aten reciprocal BinaryUfuncInfo __rmul__ op=torch Tensor __rmul__ dtypes=all_types_and_complex_and torch bfloat torch half torch bool supports_out=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True autodiff_nonfusible_nodes= aten mul BinaryUfuncInfo __rand__ op=torch Tensor __rand__ dtypes=integral_types_and torch bool supports_out=False supports_autograd=False supports_forward_ad=True skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive BinaryUfuncInfo __ror__ op=torch Tensor __ror__ dtypes=integral_types_and torch bool supports_out=False supports_autograd=False supports_forward_ad=True skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive BinaryUfuncInfo __rxor__ op=torch Tensor __rxor__ dtypes=integral_types_and torch bool supports_out=False supports_autograd=False supports_forward_ad=True skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive OpInfo __rmatmul__ op=torch Tensor __rmatmul__ dtypes=all_types_and_complex_and torch bfloat torch float dtypesIfCUDA=floating_and_complex_types_and torch float torch bfloat SM OrLater TEST_WITH_ROCM assert_autodiffed=True sample_inputs_func=partial sample_inputs_matmul is_rmatmul=True Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True check_batched_forward_grad=False decorators= NVIDIA only assures bfloat supported bmm SM = DecorateInfo unittest skip Skipped TestCommon test_dtypes device_type= cuda active_if=not SM OrLater DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestMathBits test_conj_view DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestCommon test_noncontiguous_samples DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestDecomp test_comprehensive device_type= cuda active_if=TEST_WITH_ROCM skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit https github com pytorch pytorch issues DecorateInfo unittest skip TestCommon test_noncontiguous_samples device_type= cpu dtypes= torch long Fails XLA AssertionError False true Tensors failed compare equal DecorateInfo unittest skip Skipped TestOpInfo device_type= xla dtypes= torch long https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestNNCOpInfo test_nnc_correctness device_type= cpu dtypes= torch long BinaryUfuncInfo __rmod__ op=torch Tensor __rmod__ dtypes=floating_types_and torch bfloat torch half dtypesIfCUDA=all_types_and torch bfloat torch half https github com pytorch pytorch issues gradcheck_fast_mode=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True supports_one_python_scalar=True skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit Support autograd after torch remainder Tensor Tensor supports autograd second argument https github com pytorch pytorch pull files#r supports_autograd=False assert_autodiffed=True autodiff_nonfusible_nodes= aten remainder BinaryUfuncInfo __rpow__ op=torch Tensor __rpow__ dtypes=all_types_and_complex_and torch bfloat torch half Reference https github com pytorch pytorch issues log _vml_cpu implemented Half backward_dtypes=all_types_and_complex_and torch bfloat torch half supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True supports_one_python_scalar=True skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit TODO FIXME tolerance too high DecorateInfo unittest skip Skipped TestFwdGradients DecorateInfo unittest skip Skipped TestBwdGradients assert_autodiffed=True autodiff_nonfusible_nodes= aten pow BinaryUfuncInfo __rsub__ op=torch Tensor __rsub__ dtypes=all_types_and_complex_and torch bfloat torch half supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False supports_one_python_scalar=True skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit assert_autodiffed=True autodiff_nonfusible_nodes= aten rsub BinaryUfuncInfo rsub dtypes=all_types_and_complex_and torch bfloat torch half supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False supports_inplace_autograd=False assert_autodiffed=None sample_inputs_func=sample_inputs_add_sub OpInfo select aten_backward_name= select_backward dtypes=all_types_and_complex_and torch bfloat torch half torch bool torch chalf sample_inputs_func=sample_inputs_select assert_jit_shape_analysis=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False OpInfo select_scatter dtypes=all_types_and torch bfloat torch half torch bool sample_inputs_func=sample_inputs_select_scatter supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False OpInfo slice op=torch ops aten slice Tensor dtypes=all_types_and_complex_and torch bfloat torch half torch bool torch chalf sample_inputs_func=sample_inputs_slice gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_scripting=False supports_inplace_autograd=False supports_out=False OpInfo slice_scatter dtypes=all_types_and torch bfloat torch half torch bool sample_inputs_func=sample_inputs_slice_scatter https github com pytorch pytorch issues gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=True UnaryUfuncInfo signbit ref=np signbit dtypes=all_types_and torch bool torch bfloat torch half supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True supports_autograd=False UnaryUfuncInfo tan ref=np tan dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and_complex_and torch chalf torch bool torch half torch bfloat decorators= DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True promotes_int_to_float=True skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble active_if= IS_MACOS IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cpu dtypes= torch cfloat torch cdouble active_if= IS_MACOS IS_WINDOWS DecorateInfo unittest skip Skipped sparse backward supported TestSparseUnaryUfuncs test_sparse_fn_grad FIXME Mismatched elements Greatest absolute difference inf index up e- allowed Greatest relative difference nan index up allowed DecorateInfo unittest skip Skipped TestInductorOpInfo test_comprehensive dtypes= torch float device_type= cuda DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestConsistency test_output_match device_type= mps DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestConsistency test_output_grad_match device_type= mps tan pi odd_number nan reference_numerics_filter=NumericsFilter condition=lambda x close_to_int x math pi safe_val=math pi UnaryUfuncInfo tanh ref=np tanh aten_backward_name= tanh_backward aliases= nn functional tanh decorators= precisionOverride torch bfloat e- DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and_complex_and torch chalf torch bool torch half torch bfloat assert_autodiffed=True assert_jit_shape_analysis=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True promotes_int_to_float=True skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble active_if= IS_MACOS IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cpu dtypes= torch cfloat torch cdouble active_if= IS_MACOS IS_WINDOWS DecorateInfo unittest skip Skipped sparse backward supported TestSparseUnaryUfuncs test_sparse_fn_grad DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestConsistency test_output_match device_type= mps tan j pi odd_number nan reference_numerics_filter=NumericsFilter condition=lambda x close_to_int x math pi j x is_complex x new_tensor False dtype=torch bool safe_val= OpInfo tensor_split ref=np array_split dtypes=all_types_and_complex_and torch bool torch bfloat torch float dtypesIfCUDA=all_types_and_complex_and torch bool torch bfloat torch float supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True skips= Pre-existing condition Needs fixed DecorateInfo unittest expectedFailure TestCompositeCompliance test_operator DecorateInfo unittest expectedFailure TestCompositeCompliance test_backward DecorateInfo unittest expectedFailure TestCompositeCompliance test_forward_ad sample_inputs_func=sample_inputs_tensor_split OpInfo hsplit dtypes=all_types_and_complex_and torch complex torch bool torch bfloat torch float supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False sample_inputs_func=sample_inputs_hsplit error_inputs_func=error_inputs_hsplit OpInfo vsplit dtypes=all_types_and_complex_and torch complex torch bool torch bfloat torch float supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False sample_inputs_func=sample_inputs_vsplit error_inputs_func=error_inputs_vsplit OpInfo dsplit dtypes=all_types_and_complex_and torch complex torch bool torch bfloat torch float supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False sample_inputs_func=sample_inputs_dsplit error_inputs_func=error_inputs_dsplit OpInfo triangular_solve op=torch triangular_solve dtypes=floating_and_complex_types sample_inputs_func=sample_inputs_legacy_solve check_batched_gradgrad=False supports_forward_ad=True supports_fwgrad_bwgrad=True gradcheck_wrapper=lambda args kwargs gradcheck_wrapper_triangular_input args idx= kwargs decorators= skipCUDAIfNoMagma skipCPUIfNoLapack DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestConsistency test_output_match device_type= cpu skips= AssertionError Scalars equal DecorateInfo unittest expectedFailure TestCommon test_out Gradcheck fails DecorateInfo unittest expectedFailure TestFwdGradients test_fn_fwgrad_bwgrad dtypes=floating_and_complex_types DecorateInfo unittest skip Skipped TestCommon test_out device_type= mps dtypes= torch float DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager device_type= mps dtypes= torch float DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit device_type= mps dtypes= torch float UnaryUfuncInfo trunc aliases= fix ref=np trunc dtypes=all_types_and torch half torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse=True skips= DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness dtypes=tuple t t integral_types t = torch uint supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True assert_autodiffed=True UnaryUfuncInfo exp aliases= special exp ref=np_unary_ufunc_integer_promotion_wrapper np exp dtypes=all_types_and_complex_and torch bool torch half torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch cdouble Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble active_if=IS_WINDOWS UnaryUfuncInfo expm aliases= special expm ref=np_unary_ufunc_integer_promotion_wrapper np expm dtypes=all_types_and_complex_and torch bool torch half torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True promotes_int_to_float=True assert_autodiffed=True skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cuda dtypes= torch complex DecorateInfo unittest skip Skipped sparse backward supported TestSparseUnaryUfuncs test_sparse_fn_grad UnaryUfuncInfo nan_to_num ref=np nan_to_num dtypes=all_types_and torch half torch bool torch bfloat dtypesIfCUDA=all_types_and torch half torch bool torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse=True skips= DecorateInfo unittest skip Skipped sparse backward supported TestSparseUnaryUfuncs test_sparse_fn_grad Passing numpy_kwargs via sample_kwargs numpy does comparison BFloat float since currently doesn t support BFloat Ref https github com pytorch pytorch issues #issuecomment- sample_kwargs=lambda device dtype input posinf torch finfo torch bfloat max neginf torch finfo torch bfloat min dtype torch bfloat UnaryUfuncInfo reciprocal ref=np_unary_ufunc_integer_promotion_wrapper np reciprocal dtypes=all_types_and_complex_and torch bool torch half torch bfloat assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True skips= Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch cfloat torch cdouble UnaryUfuncInfo rsqrt ref=lambda x np reciprocal np sqrt x domain= None dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and_complex_and torch chalf torch bool torch half torch bfloat decorators= precisionOverride torch half e- assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch cfloat torch cdouble AssertionError Tensor-likes close Greatest absolute difference nan index up allowed Greatest relative difference nan index up allowed DecorateInfo unittest expectedFailure TestUnaryUfuncs test_reference_numerics_large dtypes= torch chalf UnaryUfuncInfo sqrt ref=np sqrt supports_sparse=True domain= None dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and_complex_and torch chalf torch bool torch half torch bfloat assert_autodiffed=True supports_forward_ad=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True supports_fwgrad_bwgrad=True promotes_int_to_float=True decorators= precisionOverride torch bfloat e- DecorateInfo toleranceOverride torch chalf tol atol= e- rtol= TestUnaryUfuncs test_reference_numerics_large skips= Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cpu dtypes= torch cfloat torch cdouble active_if=IS_MACOS DecorateInfo unittest skip Skipped sparse backward supported TestSparseUnaryUfuncs test_sparse_fn_grad DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestConsistency test_output_match device_type= mps UnaryUfuncInfo square ref=np square dtypes=all_types_and_complex_and torch bool torch float torch bfloat decorators= precisionOverride torch complex e- torch bfloat e- supports_forward_ad=True supports_fwgrad_bwgrad=True skips= Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch cfloat torch cdouble t = torch tensor complex - float inf np square t numpy -inf-infj t square tensor -inf-infj t cuda square tensor inf+nanj device= cuda DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda dtypes= torch cfloat torch cdouble DecorateInfo unittest expectedFailure TestMeta test_meta_inplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_inplace dtypes= torch bool DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_inplace dtypes= torch bool OpInfo lerp dtypes=floating_and_complex_types_and torch bfloat torch half dtypesIfCUDA=floating_and_complex_types_and torch chalf torch half torch bfloat sample_inputs_func=sample_inputs_lerp supports_forward_ad=True supports_fwgrad_bwgrad=True assert_autodiffed=True UnaryUfuncInfo angle ref=np angle dtypes=all_types_and_complex_and torch bool torch bfloat torch float dtypesIfCUDA=all_types_and_complex_and torch chalf torch bool decorators= precisionOverride torch float e- torch bfloat e- backward_dtypes=floating_and_complex_types_and torch bfloat torch float backward_dtypesIfCUDA=floating_and_complex_types_and torch chalf supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True supports_complex_to_float=True skips= Ref https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestUnaryUfuncs test_reference_numerics_small dtypes= torch bfloat torch float torch float torch float UnaryUfuncInfo isfinite ref=np isfinite dtypes=all_types_and_complex_and torch bool torch bfloat torch float torch chalf supports_out=False supports_autograd=False UnaryUfuncInfo isinf ref=np isinf dtypes=all_types_and_complex_and torch bool torch bfloat torch float torch chalf supports_out=False supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True supports_autograd=False UnaryUfuncInfo isposinf ref=np isposinf dtypes=all_types_and torch bool torch bfloat torch float supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True supports_autograd=False UnaryUfuncInfo isneginf ref=np isneginf dtypes=all_types_and torch bool torch bfloat torch float supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True supports_autograd=False UnaryUfuncInfo isreal ref=np isreal dtypes=all_types_and_complex_and torch bool torch bfloat torch float torch chalf supports_out=False supports_autograd=False UnaryUfuncInfo isnan ref=np isnan dtypes=all_types_and_complex_and torch bool torch bfloat torch float supports_out=False supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True supports_autograd=False OpInfo einsum we need lambda because SampleInput expects tensor input first argument TODO heitorschueroff update SampleInput handle such cases op=lambda tensors equation torch einsum equation tensors dtypes=all_types_and_complex_and torch half torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch half torch bfloat backward_dtypesIfCUDA=floating_and_complex_types_and torch half torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True check_batched_forward_grad=False See https github com pytorch pytorch issues sample_inputs_func=sample_inputs_einsum skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive test does work passing lambda op there s test ` test_einsum ` ` test_jit py ` handle case AssertionError JIT Test does execute any logic DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit OpInfo svd op=torch svd dtypes=floating_and_complex_types sample_inputs_func=sample_inputs_svd Runs very slowly slow-gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True check_batched_forward_grad=False We re using allclose which does have batching rule check_batched_grad=False check_batched_gradgrad=False decorators= skipCUDAIfNoMagmaAndNoCusolver skipCPUIfNoLapack with_tf _off skips= Issue conj torch dispatch see https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestSchemaCheckModeOpInfo test_schema_correctness dtypes= torch complex torch complex DecorateInfo unittest skip Skipped TestCommon test_out device_type= mps dtypes= torch float DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager device_type= mps dtypes= torch float DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit device_type= mps dtypes= torch float OpInfo svd_lowrank op=lambda args kwargs wrapper_set_seed lambda b kwargs torch svd_lowrank b mT kwargs args kwargs dtypes=floating_and_complex_types Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_out=False Due use randomness check_batched_grad=False check_batched_gradgrad=False check_batched_forward_grad=False supports_fwgrad_bwgrad=True supports_forward_ad=True sample_inputs_func=sample_inputs_svd_lowrank decorators= skipCUDAIfNoCusolver skipCPUIfNoLapack with_tf _off DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch complex tol atol= e- rtol= e- TestCommon test_noncontiguous_samples FIXME This should following toleranceOverride does seem do anything DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestFwdGradients test_fn_fwgrad_bwgrad DecorateInfo unittest skip See comment above TestFwdGradients test_fn_fwgrad_bwgrad dtypes= torch complex skips= test does work passing lambda op DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu Issue conj torch dispatch see https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestSchemaCheckModeOpInfo test_schema_correctness dtypes= torch complex torch complex DecorateInfo slowTest TestCompositeCompliance test_forward_ad OpInfo pca_lowrank op=lambda args kwargs wrapper_set_seed lambda b kwargs torch pca_lowrank b mT kwargs args kwargs dtypes=floating_and_complex_types Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_out=False check_batched_forward_grad=False check_batched_grad=False check_batched_gradgrad=False supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_pca_lowrank decorators= skipCUDAIfNoCusolver skipCPUIfNoLapack with_tf _off DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch complex tol atol= e- rtol= e- TestCommon test_noncontiguous_samples DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestOperators test_grad FIXME This should following toleranceOverride does seem do anything DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestFwdGradients test_fn_fwgrad_bwgrad DecorateInfo unittest skip See comment above TestFwdGradients test_fn_fwgrad_bwgrad dtypes= torch complex DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cuda skips= test does work passing lambda op DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit Issue conj torch dispatch see https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestSchemaCheckModeOpInfo test_schema_correctness dtypes= torch complex torch complex DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu BinaryUfuncInfo polar dtypes=floating_types function undefined abs values supports_forward_ad=True lhs_make_tensor_kwargs=dict low= supports_rhs_python_scalar=False skips= RuntimeError Expected object scalar type Float got scalar type Double second argument DecorateInfo unittest skip Skipped TestBinaryUfuncs test_type_promotion DecorateInfo unittest expectedFailure TestMeta test_binary_ufuncs_mixed_dtype GradcheckError Jacobian computed forward mode mismatch output respect input Numerical tensor dtype=torch float Analytical tensor - dtype=torch float grad_fn= CopySlices DecorateInfo unittest expectedFailure TestFwdGradients test_fn_fwgrad_bwgrad TODO kshitij Refactor similar ` mvlgamma ` entries To test reference numerics against multiple values argument ` n ` we make multiple OpInfo entries each entry corresponding different value n currently We run op tests test_ops py only ` n= ` avoid redundancy testing UnaryUfuncInfo polygamma op=lambda x n kwargs torch polygamma n x kwargs variant_test_name= polygamma_n_ ref=reference_polygamma TEST_SCIPY None dtypes=all_types_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and torch bool torch half torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True sample_inputs_func=sample_inputs_polygamma skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive sample_kwargs=lambda device dtype input n n polygamma functions have multiple singularities x having non-positive integer value reference_numerics_filter=NumericsFilter condition=lambda x x x - x round abs e- safe_val= UnaryUfuncInfo polygamma op=lambda x n kwargs torch polygamma n x kwargs variant_test_name=f polygamma_n_ n_ ref=reference_polygamma TEST_SCIPY None dtypes=all_types_and torch bool torch bfloat dtypesIfCUDA=all_types_and torch bool torch half torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True sample_inputs_func=sample_inputs_polygamma decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestUnaryUfuncs DecorateInfo toleranceOverride torch bfloat tol atol= e rtol= e- torch float tol atol= e- rtol= e- TestUnaryUfuncs test_reference_numerics_normal active_if=IS_WINDOWS skips= Redundant tests DecorateInfo unittest skip Skipped TestFwdGradients DecorateInfo unittest skip Skipped TestBwdGradients DecorateInfo unittest skip Skipped TestJit DecorateInfo unittest skip Skipped TestNormalizeOperators DecorateInfo unittest skip Skipped TestCommon Mismatch https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large sample_kwargs=lambda device dtype input n n_ n n_ polygamma functions have multiple singularities x having non-positive integer value reference_numerics_filter=NumericsFilter condition=lambda x x x - x round abs e- safe_val= n_ OpInfo ravel ref=np ravel dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False sample_inputs_func=sample_inputs_ravel OpInfo unravel_index ref=np unravel_index dtypes=integral_types_and supports_out=False supports_autograd=False sample_inputs_func=sample_inputs_unravel_index OpInfo reshape dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf sample_inputs_func=sample_inputs_view_reshape reference_inputs_func=reference_inputs_view_reshape error_inputs_func=error_inputs_view_reshape supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True OpInfo reshape_as op=lambda x other x reshape_as other dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf sample_inputs_func=partial sample_inputs_view_reshape tensor_arg=True reference_inputs_func=partial reference_inputs_view_reshape tensor_arg=True error_inputs_func=partial error_inputs_view_reshape tensor_arg=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive OpInfo view op=lambda x shape x view shape dtypes=all_types_and_complex_and torch complex torch bool torch float torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True assert_jit_shape_analysis=True sample_inputs_func=sample_inputs_view_reshape reference_inputs_func=reference_inputs_view_reshape error_inputs_func=error_inputs_view_reshape skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError view size compatible input tensor s size stride least one dimension spans across two contiguous subspaces Use reshape instead DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace_all_strides OpInfo view_as op=lambda x other x view_as other dtypes=all_types_and_complex_and torch complex torch bool torch float torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=partial sample_inputs_view_reshape tensor_arg=True reference_inputs_func=partial reference_inputs_view_reshape tensor_arg=True error_inputs_func=partial error_inputs_view_reshape tensor_arg=True skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError view size compatible input tensor s size stride DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace_all_strides OpInfo atleast_ d dtypes=all_types_and_complex_and torch complex torch bool torch float torch bfloat Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False sample_inputs_func=sample_inputs_atleast d d d skips= JIT does support variadic tensors RuntimeError input- type - kind == TypeKind OptionalType INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float OpInfo atleast_ d dtypes=all_types_and_complex_and torch complex torch bool torch float torch bfloat Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float sample_inputs_func=sample_inputs_atleast d d d OpInfo atleast_ d dtypes=all_types_and_complex_and torch complex torch bool torch float torch bfloat Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float sample_inputs_func=sample_inputs_atleast d d d OpInfo flatten dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf ref=reference_flatten supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False sample_inputs_func=sample_inputs_flatten reference_inputs_func=reference_inputs_flatten OpInfo unflatten op=torch unflatten dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_unflatten OpInfo column_stack dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False sample_inputs_func=sample_inputs_column_stack OpInfo pinverse op=torch pinverse dtypes=floating_and_complex_types check_batched_grad=False check_batched_gradgrad=False supports_forward_ad=True supports_fwgrad_bwgrad=True gradcheck_nondet_tol=GRADCHECK_NONDET_TOL supports_out=False sample_inputs_func=sample_inputs_linalg_invertible decorators= skipCUDAIfNoMagmaAndNoCusolver skipCPUIfNoLapack skips= DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager device_type= mps dtypes= torch float DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit device_type= mps dtypes= torch float OpInfo gather dtypes=all_types_and_complex_and torch bool torch float torch bfloat dtypesIfCUDA=all_types_and_complex_and torch bool torch float torch bfloat sample_inputs_func=sample_inputs_gather gradcheck_nondet_tol=GRADCHECK_NONDET_TOL supports_forward_ad=True supports_fwgrad_bwgrad=True error_inputs_func=error_inputs_gather OpInfo index_fill dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch complex inplace_variant=torch Tensor index_fill_ supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True https github com pytorch pytorch issues check_batched_forward_grad=False skips= RuntimeError Mismatch aten _unique default Shapes torch Size torch Size equal DecorateInfo unittest expectedFailure TestFakeTensor test_fake_crossref_backward_no_amp RuntimeError Mismatch aten _unique default Shapes torch Size torch Size equal DecorateInfo unittest expectedFailure TestFakeTensor test_fake_crossref_backward_amp sample_inputs_func=sample_inputs_index reference_inputs_func=partial sample_inputs_index reference=True OpInfo index_copy dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch complex supports_out=True supports_forward_ad=True supports_fwgrad_bwgrad=True https github com pytorch pytorch issues check_batched_forward_grad=False sample_inputs_func=sample_inputs_index reference_inputs_func=partial sample_inputs_index reference=True gradcheck_nondet_tol=GRADCHECK_NONDET_TOL OpInfo index_select dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf backward_dtypesIfCUDA=floating_and_complex_types_and torch float torch bfloat torch chalf sample_inputs_func=sample_inputs_index reference_inputs_func=partial sample_inputs_index reference=True error_inputs_func=error_inputs_index_select supports_forward_ad=True supports_fwgrad_bwgrad=True assert_jit_shape_analysis=True gradcheck_nondet_tol=GRADCHECK_NONDET_TOL OpInfo index_add dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf inplace_variant=torch Tensor index_add_ supports_out=True supports_forward_ad=True supports_fwgrad_bwgrad=True https github com pytorch pytorch issues check_batched_forward_grad=False sample_inputs_func=sample_inputs_index reference_inputs_func=partial sample_inputs_index reference=True error_inputs_func=error_inputs_index_add skips= boolean alpha handled properly DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness dtypes= torch bool gradcheck_nondet_tol=GRADCHECK_NONDET_TOL OpInfo index_reduce variant_test_name=reduction_type dtypes=all_types_and torch float torch bfloat skips= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive supports_out=True sample_inputs_func=sample_inputs_index_reduce reduction_type mean prod amin amax OpInfo _unsafe_masked_index dtypes=all_types_and_complex_and torch float torch bfloat torch bool supports_out=False supports_inplace_autograd=False supports_scripting=False supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs__unsafe_masked_index skips= DecorateInfo unittest skip Skipped TestNNCOpInfo test_nnc_correctness DecorateInfo slowTest TestDecomp test_quick_core_backward dtypes= torch float active_if=IS_WINDOWS OpInfo _unsafe_masked_index_put_accumulate dtypes=all_types_and_complex_and torch float torch bfloat torch bool supports_out=False supports_inplace_autograd=False supports_scripting=False supports_forward_ad=True supports_fwgrad_bwgrad=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cpu sample_inputs_func=sample_inputs__unsafe_masked_index_put_accumulate skips= DecorateInfo slowTest TestDecomp test_quick_core_backward dtypes= torch float active_if=IS_WINDOWS OpInfo __getitem__ dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True supports_inplace_autograd=False supports_scripting=False op=torch Tensor __getitem__ skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive AssertionError False true Scalars failed compare equal = DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit device_type= cuda sample_inputs_func=sample_inputs_getitem OpInfo index_put dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf supports_out=False supports_inplace_autograd=True supports_forward_ad=True supports_fwgrad_bwgrad=True https github com pytorch pytorch issues check_batched_forward_grad=False test_neg_view=False sample_inputs_func=sample_inputs_index_put skips= DecorateInfo unittest skip Skipped TestBwdGradients test_fn_grad dtypes= torch float device_type= cuda active_if= TEST_WITH_ROCM TEST_WITH_TORCHINDUCTOR OpInfo sort dtypes=all_types_and torch bool torch float torch bfloat dtypesIfCUDA=all_types_and torch bool torch float torch bfloat sample_inputs_func=sample_inputs_sort supports_forward_ad=True supports_fwgrad_bwgrad=True skips= DecorateInfo unittest expectedFailure TestCommon test_non_standard_bool_values dtypes= torch bool device_type= cuda active_if=not TEST_WITH_ROCM OpInfo unique dtypes=all_types_and torch bool torch float torch bfloat torch uint torch uint torch uint sample_inputs_func=sample_inputs_unique supports_out=False supports_autograd=False skips= lambda impl DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest skip Output order undefined when sorted=False TestCommon test_compare_cpu OpInfo unique_consecutive dtypes=all_types_and torch bool torch float torch bfloat sample_inputs_func=sample_inputs_unique_consecutive supports_out=False supports_autograd=False skips= lambda impl DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit OpInfo put dtypes=all_types_and_complex_and torch bool torch float torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True check_batched_forward_grad=False check_batched_gradgrad=False vmap complains sizes sample_inputs_func=sample_inputs_put OpInfo take dtypes=all_types_and_complex_and torch bool torch float torch bfloat check_batched_grad=False vmap complains sizes supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_take error_inputs_func=error_inputs_take OpInfo scatter dtypes=all_types_and_complex_and torch bool torch half torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_scatter error_inputs_func=error_inputs_scatter_and_scatter_add skips= Compiler issue ROCm Regression started ROCm DecorateInfo unittest skip Skipped TestCommon test_non_standard_bool_values dtypes= torch bool active_if=TEST_WITH_ROCM UnaryUfuncInfo bfloat op=lambda x args kwargs x bfloat args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_out=False sample_inputs_func=sample_inputs_conversion skips= autograd tests don t handle operators change dtype DecorateInfo unittest expectedFailure TestFwdGradients DecorateInfo unittest expectedFailure TestBwdGradients DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attribute lookup defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest skip Skipped TestNNCOpInfo test_nnc_correctness UnaryUfuncInfo bool op=lambda x args kwargs x bool args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_out=False sample_inputs_func=sample_inputs_conversion supports_autograd=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attributis defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit UnaryUfuncInfo byte op=lambda x args kwargs x byte args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat supports_out=False sample_inputs_func=sample_inputs_byte The autograd test runner cannot handle functions change dtype supports_autograd=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attribute lookup defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest skip Overflow when downcasting signed type undefined TestCommon test_compare_cpu UnaryUfuncInfo char op=lambda x args kwargs x char args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_out=False sample_inputs_func=sample_inputs_conversion The autograd test runner cannot handle functions change dtype supports_autograd=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attribute lookup defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest skip Overflow when downcasting signed type undefined TestCommon test_compare_cpu UnaryUfuncInfo double op=lambda x args kwargs x double args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_out=False sample_inputs_func=sample_inputs_conversion supports_forward_ad=True supports_fwgrad_bwgrad=True skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attribute lookup defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit UnaryUfuncInfo float op=lambda x args kwargs x float args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_out=False sample_inputs_func=sample_inputs_conversion skips= autograd tests don t handle operators change dtype DecorateInfo unittest expectedFailure TestFwdGradients DecorateInfo unittest expectedFailure TestBwdGradients DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attribute lookup defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit UnaryUfuncInfo half op=lambda x args kwargs x half args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat supports_out=False sample_inputs_func=sample_inputs_conversion supports_autograd=True skips= autograd tests don t handle operators change dtype DecorateInfo unittest expectedFailure TestFwdGradients DecorateInfo unittest expectedFailure TestBwdGradients DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attribute lookup defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit UnaryUfuncInfo int op=lambda x args kwargs x int args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat supports_out=False sample_inputs_func=sample_inputs_conversion supports_autograd=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attribute lookup defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest skip Overflow when downcasting signed type undefined TestCommon test_compare_cpu UnaryUfuncInfo long op=lambda x args kwargs x long args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_out=False sample_inputs_func=sample_inputs_conversion supports_autograd=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attribute lookup defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest skip Overflow when downcasting signed type undefined TestCommon test_compare_cpu UnaryUfuncInfo short op=lambda x args kwargs x short args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat supports_out=False sample_inputs_func=sample_inputs_conversion supports_autograd=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attribute lookup defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest skip Overflow when downcasting signed type undefined TestCommon test_compare_cpu UnaryUfuncInfo cdouble op=torch Tensor cdouble dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_out=False sample_inputs_func=sample_inputs_conversion supports_forward_ad=True supports_fwgrad_bwgrad=True skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attribute lookup defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest skip Skipped TestNNCOpInfo test_nnc_correctness UnaryUfuncInfo cfloat op=torch Tensor cfloat dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_out=False sample_inputs_func=sample_inputs_conversion skips= autograd tests don t handle operators change dtype DecorateInfo unittest expectedFailure TestFwdGradients DecorateInfo unittest expectedFailure TestBwdGradients DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attribute lookup defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest skip Skipped TestNNCOpInfo test_nnc_correctness UnaryUfuncInfo chalf op=lambda x args kwargs x chalf args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_out=False sample_inputs_func=sample_inputs_conversion skips= autograd tests don t handle operators change dtype DecorateInfo unittest expectedFailure TestFwdGradients DecorateInfo unittest expectedFailure TestBwdGradients use lambda doesn t work test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError sum_cpu implemented ComplexHalf DecorateInfo unittest expectedFailure TestCommon test_variant_consistency_eager device_type= cpu TypeError int object iterable DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit RuntimeError sum_cpu implemented ComplexHalf DecorateInfo unittest expectedFailure TestMathBits test_conj_view device_type= cpu RuntimeError sum_cpu implemented ComplexHalf DecorateInfo unittest expectedFailure TestMathBits test_neg_view device_type= cpu RuntimeError sum_cpu implemented ComplexHalf RuntimeError neg_conj_cuda implemented ComplexHalf DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view OpInfo empty_like dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_out=False sample_inputs_func=sample_inputs_like_fns reference_inputs_func=reference_inputs_like_fns supports_autograd=False skips= Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestNormalizeOperators test_normalize_operator_exhaustive Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestCommon test_noncontiguous_samples Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestMathBits test_conj_view Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestMathBits test_neg_view Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestMathBits test_neg_conj_view Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestNNCOpInfo test_nnc_correctness Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestCudaFuserOpInfo Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestCommon test_complex_half_reference_testing Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestCommon test_non_standard_bool_values DecorateInfo unittest skip Expected empty_like comparable TestCompositeCompliance test_operator DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu OpInfo zeros_like dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_out=False sample_inputs_func=sample_inputs_like_fns supports_autograd=False error_inputs_sparse_func=error_inputs_sparse_like_fns sample_inputs_sparse_coo_func=partial sample_inputs_sparse_like_fns layout=torch sparse_coo sample_inputs_sparse_csr_func=partial sample_inputs_sparse_like_fns layout=torch sparse_csr sample_inputs_sparse_csc_func=partial sample_inputs_sparse_like_fns layout=torch sparse_csc sample_inputs_sparse_bsr_func=partial sample_inputs_sparse_like_fns layout=torch sparse_bsr sample_inputs_sparse_bsc_func=partial sample_inputs_sparse_like_fns layout=torch sparse_bsc skips= OpInfo ones_like dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_out=False sample_inputs_func=sample_inputs_like_fns supports_autograd=False skips= OpInfo randn dtypes=floating_and_complex_types_and torch half torch bfloat torch complex op=lambda args kwargs wrapper_set_seed torch randn args kwargs supports_out=True sample_inputs_func=sample_inputs_randn supports_autograd=False skips= Tests assume input tensor sequence tensors DecorateInfo unittest skip Test expects tensor input TestCommon test_noncontiguous_samples DecorateInfo unittest skip Test expects tensor input TestVmapOperatorsOpInfo test_vmap_exhaustive DecorateInfo unittest skip Test expects tensor input TestVmapOperatorsOpInfo test_op_has_batch_rule CPU randn generates different values based strides out tensor DecorateInfo unittest expectedFailure TestCommon test_out device_type= cpu randn fails warn when resizing its out tensor DecorateInfo unittest expectedFailure TestCommon test_out_warning FX failed normalize op - add op op_skip list DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive Tests assume input tensor has meaningful effect output tensor DecorateInfo unittest expectedFailure TestCommon test_variant_consistency_eager DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view AssertionError JIT Test does execute any logic DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest expectedFailure TestDecomp test_quick OpInfo randn_like dtypes=floating_and_complex_types_and torch half torch bfloat torch complex op=lambda inp args kwargs wrapper_set_seed torch randn_like inp args kwargs supports_out=False sample_inputs_func=sample_inputs_like_fns supports_autograd=False error_inputs_sparse_func=error_inputs_sparse_like_fns sample_inputs_sparse_coo_func=partial sample_inputs_sparse_like_fns layout=torch sparse_coo sample_inputs_sparse_csr_func=partial sample_inputs_sparse_like_fns layout=torch sparse_csr sample_inputs_sparse_csc_func=partial sample_inputs_sparse_like_fns layout=torch sparse_csc sample_inputs_sparse_bsr_func=partial sample_inputs_sparse_like_fns layout=torch sparse_bsr sample_inputs_sparse_bsc_func=partial sample_inputs_sparse_like_fns layout=torch sparse_bsc skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive AssertionError JIT Test does execute any logic DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest skip Expected randn_like comparable between dtypes TestCommon test_complex_half_reference_testing DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu OpInfo rand_like dtypes=floating_types_and torch half torch bfloat torch complex torch complex torch complex op=lambda inp args kwargs wrapper_set_seed torch randn_like inp args kwargs supports_out=False sample_inputs_func=sample_inputs_like_fns supports_autograd=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive AssertionError JIT Test does execute any logic DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest skip Expected randn_like comparable between dtypes TestCommon test_complex_half_reference_testing DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu OpInfo randint dtypes=all_types_and torch half torch bfloat op=lambda args kwargs wrapper_set_seed torch randint args kwargs supports_out=False sample_inputs_func=sample_inputs_randint supports_autograd=False skips= Tests assume input tensor sequence tensors DecorateInfo unittest skip Test expects tensor input TestCommon test_noncontiguous_samples DecorateInfo unittest skip Test expects tensor input TestVmapOperatorsOpInfo test_vmap_exhaustive DecorateInfo unittest skip Test expects tensor input TestVmapOperatorsOpInfo test_op_has_batch_rule CPU randint generates different values based strides out tensor DecorateInfo unittest expectedFailure TestCommon test_out randint fails warn when resizing its out tensor DecorateInfo unittest expectedFailure TestCommon test_out_warning FX failed normalize op - add op op_skip list DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive Tests assume input tensor has meaningful effect output tensor DecorateInfo unittest expectedFailure TestCommon test_variant_consistency_eager DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view AssertionError JIT Test does execute any logic DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit Might need skip until ROCm DecorateInfo unittest skip Skipped TestCommon test_multiple_devices dtypes= torch float torch int active_if=TEST_WITH_ROCM OpInfo randint_like dtypes=all_types_and torch half torch bfloat op=lambda inp args kwargs wrapper_set_seed torch randint_like inp args kwargs supports_out=False sample_inputs_func=sample_inputs_randint_like supports_autograd=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive AssertionError JIT Test does execute any logic DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu OpInfo full_like dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch uint torch uint supports_out=False sample_inputs_func=sample_inputs_full_like supports_autograd=False OpInfo new_zeros op=lambda x args kwargs x new_zeros args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_out=False sample_inputs_func=sample_inputs_new_fns skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive supports_autograd=False OpInfo new_ones op=lambda x args kwargs x new_ones args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_out=False sample_inputs_func=sample_inputs_new_fns skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive supports_autograd=False OpInfo ones op=torch ones supports_autograd=False supports_varargs=True is_factory_function=True dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_out=True sample_inputs_func=sample_inputs_ones_zeros skips= Tests assume input tensor sequence tensors DecorateInfo unittest expectedFailure TestCommon test_variant_consistency_eager DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view Same failure arange cannot find linspace captured graph DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit UserWarning triggered Resized non-empty tensor did warn about DecorateInfo unittest expectedFailure TestCommon test_out_warning OpInfo zeros op=torch zeros supports_autograd=False is_factory_function=True dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_out=True sample_inputs_func=sample_inputs_ones_zeros skips= Tests assume input tensor sequence tensors DecorateInfo unittest expectedFailure TestCommon test_variant_consistency_eager DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view Same failure arange cannot find linspace captured graph DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit UserWarning triggered Resized non-empty tensor did warn about DecorateInfo unittest expectedFailure TestCommon test_out_warning OpInfo full op=torch full supports_autograd=False is_factory_function=True dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_out=True sample_inputs_func=sample_inputs_full skips= Tests assume input tensor sequence tensors DecorateInfo unittest expectedFailure TestCommon test_variant_consistency_eager DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view Same failure arange cannot find linspace captured graph DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit UserWarning triggered Resized non-empty tensor did warn about DecorateInfo unittest expectedFailure TestCommon test_out_warning RuntimeError UNSUPPORTED DTYPE bool DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness dtypes= torch bool OpInfo new_empty op=lambda x args kwargs x new_empty args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_out=False sample_inputs_func=sample_inputs_new_fns skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestCommon test_noncontiguous_samples Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestMathBits test_conj_view Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestMathBits test_neg_view Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestMathBits test_neg_conj_view Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestNNCOpInfo test_nnc_correctness Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestCudaFuserOpInfo Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestCommon test_non_standard_bool_values DecorateInfo unittest skip Expected new_empty comparable TestCompositeCompliance test_operator DecorateInfo unittest skip Expected new_empty comparable TestCommon test_complex_half_reference_testing DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu supports_autograd=False OpInfo new_empty_strided op=lambda x args kwargs x new_empty_strided args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_out=False sample_inputs_func=partial sample_inputs_new_fns is_strided=True supports_autograd=False skips= FX failed normalize op DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive Lazy tensor failures DecorateInfo unittest skip Skipped TestLazyOpInfo test_correctness DecorateInfo unittest skip Skipped TestLazyOpInfo test_correctness_with_reusing_ir Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Expected new_empty_strided comparable TestCommon test_variant_consistency_eager DecorateInfo unittest skip Expected new_empty_strided comparable TestCommon test_noncontiguous_samples DecorateInfo unittest skip Expected new_empty_strided comparable TestMathBits test_conj_view DecorateInfo unittest skip Expected new_empty_strided comparable TestMathBits test_neg_view DecorateInfo unittest skip Expected new_empty_strided comparable TestMathBits test_neg_conj_view DecorateInfo unittest skip Expected new_empty_strided comparable TestCommon test_non_standard_bool_values DecorateInfo unittest skip Expected new_empty_strided comparable TestCommon test_complex_half_reference_testing DecorateInfo unittest skip Expected new_empty_strided comparable TestCompositeCompliance test_operator DecorateInfo unittest skip Expected new_empty_strided comparable TestDecomp test_comprehensive DecorateInfo unittest skip Expected new_empty_strided comparable TestDecomp test_quick DecorateInfo unittest skip Expected new_empty_strided comparable TestJit test_variant_consistency_jit DecorateInfo unittest skip Expected new_empty_strided comparable TestProxyTensorOpInfo test_make_fx_exhaustive DecorateInfo unittest skip Expected new_empty_strided comparable TestProxyTensorOpInfo test_make_fx_fake_exhaustive DecorateInfo unittest skip Expected new_empty_strided comparable TestProxyTensorOpInfo test_make_fx_symbolic_exhaustive DecorateInfo unittest skip Expected new_empty_strided comparable TestNNCOpInfo test_nnc_correctness DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu OpInfo empty_strided op=lambda inp args kwargs wrapper_set_seed torch empty_strided inp args kwargs dtypes=all_types_and_complex_and torch bfloat torch bool torch half supports_out=False supports_autograd=False sample_inputs_func=sample_inputs_empty_strided skips= FX failed normalize op - add op op_skip list DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive AssertionError JIT Test does execute any logic DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestCommon test_noncontiguous_samples DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager DecorateInfo unittest skip Skipped TestCommon test_non_standard_bool_values DecorateInfo unittest skip Skipped TestMathBits test_neg_conj_view DecorateInfo unittest skip Skipped TestMathBits test_neg_view DecorateInfo unittest skip Skipped TestMathBits test_conj_view DecorateInfo unittest skip Skipped TestCommon test_compare_cpu DecorateInfo unittest skip Expected empty comparable TestCompositeCompliance test_operator Lazy tensor failures DecorateInfo unittest skip Expected empty comparable TestLazyOpInfo RuntimeError unsupported operation more than one element written-to tensor refers single memory location Please clone tensor before performing operation DecorateInfo unittest expectedFailure TestMeta test_dispatch_meta_outplace DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace_all_strides OpInfo empty dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf sample_inputs_func=sample_inputs_empty supports_autograd=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestCommon test_noncontiguous_samples Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestMathBits test_conj_view Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestMathBits test_neg_view Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestMathBits test_neg_conj_view Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestNNCOpInfo test_nnc_correctness Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestCudaFuserOpInfo Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestCommon test_non_standard_bool_values DecorateInfo unittest skip Expected empty comparable TestCompositeCompliance test_operator requires_grad doesn t exist jit schema DecorateInfo unittest expectedFailure TestOperatorSignatures test_get_torch_func_signature_exhaustive DecorateInfo unittest skip Expected empty comparable TestCommon test_out DecorateInfo unittest skip Expected empty comparable TestCommon test_out_warning DecorateInfo unittest skip Expected empty comparable TestLazyOpInfo DecorateInfo unittest skip Expected empty comparable TestCommon test_complex_half_reference_testing DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu OpInfo eye dtypes=all_types_complex_float _and torch bool torch half torch bfloat sample_inputs_func=sample_inputs_eye error_inputs_func=error_inputs_eye supports_out=True supports_autograd=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive TODO same https github com pytorch pytorch issues also see arange new_full fails match any schemas despite working interpreter DecorateInfo unittest expectedFailure TestOperatorSignatures test_get_torch_func_signature_exhaustive fails match any schemas despite working interpreter DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit skip these tests since we have non tensor input DecorateInfo unittest skip Skipped TestCommon test_noncontiguous_samples DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager DecorateInfo unittest skip Skipped TestMathBits test_conj_view DecorateInfo unittest skip Skipped TestMathBits test_neg_conj_view DecorateInfo unittest skip Skipped TestMathBits test_neg_view UserWarning triggered Resized non-empty tensor did warn about DecorateInfo unittest expectedFailure TestCommon test_out_warning mul_cpu_reduced_float implemented Float _e m fn DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness dtypes= torch float _e m fn torch float _e m fnuz torch float _e m torch float _e m fnuz OpInfo empty_permuted dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf sample_inputs_func=sample_inputs_empty_permuted error_inputs_func=error_inputs_empty_permuted supports_out=False supports_autograd=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestCommon test_noncontiguous_samples Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestMathBits test_conj_view Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestMathBits test_neg_view Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestMathBits test_neg_conj_view Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestNNCOpInfo test_nnc_correctness Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestCudaFuserOpInfo Empty tensor data garbage so s hard make comparisons DecorateInfo unittest skip Skipped TestCommon test_non_standard_bool_values DecorateInfo unittest skip Expected empty_permuted comparable TestCompositeCompliance test_operator requires_grad doesn t exist jit schema DecorateInfo unittest expectedFailure TestOperatorSignatures test_get_torch_func_signature_exhaustive DecorateInfo unittest skip Expected empty_permuted comparable TestCommon test_out DecorateInfo unittest skip Expected empty_permuted comparable TestCommon test_out_warning DecorateInfo unittest skip Expected empty_permuted comparable TestLazyOpInfo DecorateInfo unittest skip Expected empty_permuted comparable TestCommon test_complex_half_reference_testing DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu OpInfo scalar_tensor dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf sample_inputs_func=sample_inputs_scalar_tensor supports_autograd=False supports_out=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive fails match any schemas despite working interpreter DecorateInfo unittest expectedFailure TestOperatorSignatures test_get_torch_func_signature_exhaustive fails match any schemas despite working interpreter DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit skip these tests since we have non tensor input DecorateInfo unittest skip Skipped TestCommon test_noncontiguous_samples DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager DecorateInfo unittest skip Skipped TestMathBits test_conj_view DecorateInfo unittest skip Skipped TestMathBits test_neg_conj_view DecorateInfo unittest skip Skipped TestMathBits test_neg_view OpInfo new_full op=lambda x args kwargs x new_full args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_out=False sample_inputs_func=sample_inputs_new_full skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive supports_autograd=False OpInfo multinomial op=lambda inp args kwargs wrapper_set_seed torch multinomial inp args kwargs method_variant=lambda inp args kwargs wrapper_set_seed torch Tensor multinomial inp args kwargs dtypes=floating_types_and torch bfloat torch half supports_out=True sample_inputs_func=sample_inputs_multinomial error_inputs_func=error_inputs_multinomial skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive Strides same This may reproducible CI DecorateInfo unittest skip Skipped TestCommon test_out AssertionError JIT Test does execute any logic DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit UserWarning triggered Resized non-empty tensor did warn about DecorateInfo unittest expectedFailure TestCommon test_out_warning DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu supports_autograd=False OpInfo normal op=lambda inp args kwargs wrapper_set_seed torch normal inp args kwargs The inplace variant Tensor normal_ different torch normal inplace_variant=None dtypes=floating_types_and torch bfloat torch half dtypesIfCUDA=floating_types_and torch bfloat torch half supports_out=True sample_inputs_func=sample_inputs_normal_tensor_first skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive Tensor-likes close DecorateInfo unittest expectedFailure TestCommon test_out AssertionError JIT Test does execute any logic DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit UserWarning triggered Resized non-empty tensor did warn about DecorateInfo unittest expectedFailure TestCommon test_out_warning Computed gradient incorrect -- would exfail gradgrad somehow passes DecorateInfo unittest skip Gradients incorrect TestFwdGradients DecorateInfo unittest skip Gradients incorrect TestBwdGradients DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu RuntimeError Difference dtype larger decomposition DecorateInfo unittest skip Skipped TestDecomp test_comprehensive DecorateInfo unittest skip Skipped TestDecomp test_quick The inplace variant Tensor normal_ different torch normal inplace variant Tensor normal_ decomposed using randn_like DecorateInfo unittest skip Skipped TestMeta test_dispatch_symbolic_meta_outplace_all_strides OpInfo normal This has its own variant b c OpInfos assume first arg Tensor here variant_test_name= number_mean op=lambda std mean args kwargs wrapper_set_seed torch normal mean std args kwargs The inplace variant Tensor normal_ different torch normal inplace_variant=None dtypes=floating_types_and torch bfloat torch half dtypesIfCUDA=floating_types_and torch bfloat torch half supports_out=True sample_inputs_func=sample_inputs_normal_tensor_second skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive AssertionError JIT Test does execute any logic DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest skip Skipped TestCommon test_noncontiguous_samples DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager DecorateInfo unittest skip Skipped TestCommon test_out DecorateInfo unittest skip Skipped TestCommon test_out_warning DecorateInfo unittest skip Skipped TestCompositeCompliance test_backward DecorateInfo unittest skip Skipped TestMathBits test_neg_view DecorateInfo unittest skip Skipped TestFwdGradients DecorateInfo unittest skip Skipped TestBwdGradients DecorateInfo unittest skip Skipped TestCommon test_compare_cpu DecorateInfo unittest skip Skipped TestEagerFusionOpInfo DecorateInfo unittest skip Skipped TestOperators AssertionError DecorateInfo unittest skip Skipped TestDecomp test_comprehensive AssertionError DecorateInfo unittest skip Skipped TestDecomp test_quick AssertionError CUDA variant DecorateInfo unittest skip Skipped TestFakeTensor device_type= cuda DecorateInfo unittest skip Skipped TestDeviceUtils test_device_mode_ops OpInfo bernoulli op=lambda inp args kwargs wrapper_set_seed torch bernoulli inp args kwargs The inplace variant Tensor bernoulli_ different torch bernoulli inplace_variant=None method_variant=lambda inp args kwargs wrapper_set_seed torch Tensor bernoulli inp args kwargs dtypes=floating_types_and torch bfloat torch half supports_out=True supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_bernoulli error_inputs_func=error_inputs_bernoulli skips= vmap We do yet support calling random operations inside vmap DecorateInfo unittest expectedFailure TestFwdGradients test_forward_mode_AD DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive AssertionError JIT Test does execute any logic DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit Expected RuntimeError when doing unsafe cast result dtype torch float into out= dtype torch lon DecorateInfo unittest expectedFailure TestCommon test_out UserWarning triggered Resized non-empty tensor did warn about DecorateInfo unittest expectedFailure TestCommon test_out_warning DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu OpInfo scatter_add dtypes=all_types_and_complex_and torch bool torch half torch bfloat inplace_variant=torch Tensor scatter_add_ sample_inputs_func=sample_inputs_scatter_add error_inputs_func=error_inputs_scatter_and_scatter_add supports_forward_ad=True supports_fwgrad_bwgrad=True skips= Compiler issue ROCm Regression started ROCm DecorateInfo unittest skip Skipped TestCommon test_non_standard_bool_values dtypes= torch bool active_if=TEST_WITH_ROCM OpInfo stack dtypes=all_types_and_complex_and torch complex torch bool torch float torch bfloat sample_inputs_func=sample_inputs_stack assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True skips= https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_view OpInfo _chunk_cat dtypes=all_types_and_complex_and torch complex torch bool torch float torch bfloat sample_inputs_func=sample_inputs_chunk_cat error_inputs_func=error_inputs_chunk_cat supports_autograd=False supports_out=True OpInfo hstack dtypes=all_types_and_complex_and torch complex torch bool torch float torch bfloat sample_inputs_func=sample_inputs_hstack_dstack_vstack error_inputs_func=error_inputs_hstack_dstack_vstack supports_forward_ad=True supports_fwgrad_bwgrad=True BinaryUfuncInfo hypot dtypes=floating_types_and torch bfloat torch half dtypesIfCUDA=floating_types_and torch half torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True supports_rhs_python_scalar=False OpInfo histogram dtypes=floating_types dtypesIfCUDA=_dispatch_dtypes histogram only implemented CPU sample_inputs_func=sample_inputs_histogram supports_autograd=False skips= JIT tests don t work Tensor keyword arguments https github com pytorch pytorch issues RuntimeError undefined value tensor File string line the_method i torch histogram i weight=tensor - dtype=torch float density=False ~~~~~~ --- HERE DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit Not Implemented XLA DecorateInfo unittest skip Skipped TestOpInfo device_type= xla OpInfo histogramdd dtypes=floating_types dtypesIfCUDA=_dispatch_dtypes histogramdd only implemented CPU sample_inputs_func=sample_inputs_histogramdd error_inputs_func=error_inputs_histogramdd supports_autograd=False skips= Not implemented CUDA DecorateInfo unittest expectedFailure TestCommon test_errors device_type= cuda JIT tests don t work Tensor keyword arguments https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit OpInfo histc dtypes=floating_types_and torch bfloat torch float dtypesIfCUDA=floating_types_and torch int torch uint torch int torch int torch int sample_inputs_func=sample_inputs_histc supports_out=True supports_autograd=False skips= CUDA histc returns float tensor does correctly warn when passed integral out tensor AssertionError RuntimeError raised Expected RuntimeError when doing unsafe cast result dtype torch float into out= dtype torch long DecorateInfo unittest expectedFailure TestCommon test_out device_type= cuda OpInfo bincount dtypes=integral_types_and sample_inputs_func=sample_inputs_bincount supports_out=False supports_autograd=False skips= JIT tests don t work Tensor keyword arguments https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit OpInfo bucketize dtypes=all_types_and torch float torch bfloat dtypesIfCUDA=all_types_and torch bfloat torch float sample_inputs_func=sample_inputs_bucketize reference_inputs_func=reference_inputs_bucketize error_inputs_func=error_inputs_bucketize supports_autograd=False skips= JIT tests don t work Tensor keyword arguments DecorateInfo unittest skip Expected failure TestJit test_variant_consistency_jit OpInfo searchsorted dtypes=all_types_and torch bfloat torch float dtypesIfCUDA=all_types_and torch bfloat torch float sample_inputs_func=sample_inputs_searchsorted supports_autograd=False ref=reference_searchsorted skips= JIT tests don t work Tensor keyword arguments https github com pytorch pytorch issues DecorateInfo unittest skip Expected failure TestJit test_variant_consistency_jit OpInfo cat ref=_cat_np aliases= concat concatenate dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch complex sample_inputs_func=sample_inputs_cat_concat reference_inputs_func=reference_inputs_cat error_inputs_func=error_inputs_cat https github com pytorch pytorch issues gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch issues check_batched_forward_grad=False assert_autodiffed=True skips= https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestCommon test_numpy_ref_mps RuntimeError Arguments call valid Expected value type List Tensor argument tensors instead found type Tensor inferred DecorateInfo unittest expectedFailure TestJit test_jit_alias_remapping see https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestNNCOpInfo test_nnc_correctness see https github com pytorch pytorch issues RuntimeError The size tensor must match size tensor b non-singleton dimension DecorateInfo unittest expectedFailure TestBwdGradients test_fn_gradgrad OpInfo unbind dtypes=all_types_and_complex_and torch complex torch bool torch float torch bfloat ref=reference_unbind sample_inputs_func=sample_inputs_unbind error_inputs_func=error_inputs_unbind supports_forward_ad=True supports_fwgrad_bwgrad=True supports_gradgrad=True supports_out=False OpInfo unbind_copy dtypes=all_types_and_complex_and torch complex torch bool torch float torch bfloat ref=reference_unbind sample_inputs_func=sample_inputs_unbind error_inputs_func=error_inputs_unbind supports_forward_ad=True supports_fwgrad_bwgrad=True supports_gradgrad=True supports_out=True check_batched_grad=False OpInfo vstack aliases= row_stack dtypes=all_types_and_complex_and torch complex torch bool torch float torch bfloat sample_inputs_func=sample_inputs_hstack_dstack_vstack error_inputs_func=error_inputs_hstack_dstack_vstack supports_forward_ad=True supports_fwgrad_bwgrad=True skips= RuntimeError _fn Expected value type Tensor inferred argument t instead found type tuple DecorateInfo unittest expectedFailure TestJit test_jit_alias_remapping OpInfo dstack dtypes=all_types_and_complex_and torch complex torch bool torch float torch bfloat sample_inputs_func=sample_inputs_hstack_dstack_vstack error_inputs_func=error_inputs_hstack_dstack_vstack supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False OpInfo unfold op=lambda x args x unfold args dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf backward_dtypes=floating_and_complex_types_and torch float torch bfloat Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True check_batched_gradgrad=False See https github com pytorch pytorch issues check_batched_forward_grad=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive Skip operator schema test because functional operator DecorateInfo unittest expectedFailure TestOperatorSignatures test_get_torch_func_signature_exhaustive sample_inputs_func=sample_inputs_unfold OpInfo unfold_copy dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf backward_dtypes=floating_and_complex_types_and torch float torch bfloat Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_out=True supports_forward_ad=True supports_fwgrad_bwgrad=True check_batched_gradgrad=False See https github com pytorch pytorch issues check_batched_forward_grad=False sample_inputs_func=sample_inputs_unfold OpInfo msort dtypes=all_types_and torch bool torch float torch bfloat dtypesIfCUDA=all_types_and torch bool torch float torch bfloat check_batched_gradgrad=False supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_msort OpInfo movedim aliases= moveaxis dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False sample_inputs_func=sample_movedim_moveaxis reference_inputs_func=reference_movedim_moveaxis error_inputs_func=error_movedim_moveaxis OpInfo renorm dtypes=floating_and_complex_types_and torch float torch bfloat sample_inputs_func=sample_inputs_renorm error_inputs_func=error_inputs_renorm supports_forward_ad=True supports_fwgrad_bwgrad=True skips= RuntimeError Difference float larger decomposition linalg_vector_norm default than original output Original max diff e- Decomp max diff e- DecorateInfo unittest skip Inconsistent accuracy TestDecomp test_comprehensive device_type= cpu dtypes= torch float DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestConsistency test_output_match device_type= mps ShapeFuncInfo repeat op=lambda x dims x repeat dims ref=np tile dtypes=all_types_and_complex_and torch bool torch float torch bfloat https github com pytorch pytorch issues gradcheck_fast_mode=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_repeat_tile skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive OpInfo squeeze ref=_squeeze_ref dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf supports_out=False assert_autodiffed=True autodiff_fusible_nodes= aliases inputs shouldn t fused autodiff_nonfusible_nodes= aliases inputs shouldn t fused assert_jit_shape_analysis=True supports_forward_ad=True supports_fwgrad_bwgrad=True vmap does support inplace views check_inplace_batched_forward_grad=False https github com pytorch pytorch issues check_batched_forward_grad=False sample_inputs_func=sample_inputs_squeeze OpInfo squeeze ref=_squeeze_ref variant_test_name= multiple dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf supports_out=False assert_autodiffed=True autodiff_fusible_nodes= aliases inputs shouldn t fused autodiff_nonfusible_nodes= aliases inputs shouldn t fused supports_forward_ad=True supports_fwgrad_bwgrad=True vmap does support inplace views check_inplace_batched_forward_grad=False https github com pytorch pytorch issues check_batched_forward_grad=False sample_inputs_func=sample_inputs_squeeze_multiple OpInfo squeeze_copy ref=_squeeze_ref dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf supports_out=True assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True vmap does support inplace views check_inplace_batched_forward_grad=False https github com pytorch pytorch issues check_batched_forward_grad=False sample_inputs_func=sample_inputs_squeeze skips= DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float UnaryUfuncInfo fill ref=_fill_np method_variant=None sample_kwargs=_fill_sample_kwargs sample_inputs_func=partial sample_inputs_elementwise_unary op_kwargs= value True supports_forward_ad=True supports_fwgrad_bwgrad=True https github com pytorch pytorch issues check_batched_forward_grad=False dtypes=all_types_and_complex_and torch complex torch bool torch float torch bfloat supports_out=False skips= JIT has issue when op passed lambda AssertionError JIT Test does execute any logic DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest skip No fill_ op TestCudaFuserOpInfo DecorateInfo unittest skip No fill_ op TestNNCOpInfo OpInfo resize_ op=lambda x shape x clone resize_ shape method_variant=None inplace_variant=torch Tensor resize_ test fails because resize_ doesn t work imag views expected test https github com pytorch pytorch issues test_neg_view=False dtypes=all_types_and_complex_and torch bool torch float torch bfloat supports_out=False supports_autograd=False skips= Cannot resize variables require grad DecorateInfo unittest expectedFailure TestCommon test_dtypes DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest skip Allowed exception TestCompositeCompliance test_operator sample_inputs_func=sample_inputs_resize_ops OpInfo resize_as_ op=lambda x other torch resize_as_ x clone other method_variant=None inplace_variant=torch Tensor resize_as_ dtypes=all_types_and_complex_and torch bool torch float torch bfloat supports_out=False supports_autograd=False skips= Cannot resize variables require grad DecorateInfo unittest expectedFailure TestCommon test_dtypes DecorateInfo unittest skip Allowed exemption TestCompositeCompliance test_operator sample_inputs_func=sample_inputs_resize_ops OpInfo take_along_dim dtypes=all_types_and_complex_and torch bool torch float torch bfloat dtypesIfCUDA=all_types_and_complex_and torch bool torch float torch bfloat supports_inplace_autograd=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False sample_inputs_func=sample_inputs_take_along_dim gradcheck_nondet_tol=GRADCHECK_NONDET_TOL decorators= RuntimeError view size compatible input tensor s size stride DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace_all_strides ShapeFuncInfo tile ref=np tile dtypes=all_types_and_complex_and torch bool torch float torch bfloat https github com pytorch pytorch issues gradcheck_fast_mode=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_repeat_tile OpInfo trapz TODO future trapz should made proper alias trapezoid dtypes=all_types_and_complex_and torch float torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False decorators= DecorateInfo toleranceOverride torch half tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cuda sample_inputs_func=sample_trapezoid OpInfo trapezoid dtypes=all_types_and_complex_and torch float torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False decorators= DecorateInfo toleranceOverride torch half tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cuda sample_inputs_func=sample_trapezoid OpInfo cumulative_trapezoid dtypes=all_types_and_complex_and torch bfloat torch float supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False supports_out=False decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive sample_inputs_func=sample_cumulative_trapezoid OpInfo unsqueeze dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False vmap does support inplace views check_inplace_batched_forward_grad=False assert_jit_shape_analysis=True assert_autodiffed=True autodiff_fusible_nodes= aliases inputs shouldn t fused autodiff_nonfusible_nodes= aliases inputs shouldn t fused sample_inputs_func=sample_unsqueeze OpInfo unsqueeze_copy dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf supports_out=True supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False vmap does support inplace views check_inplace_batched_forward_grad=False assert_jit_shape_analysis=True assert_autodiffed=True autodiff_fusible_nodes= aliases inputs shouldn t fused autodiff_nonfusible_nodes= aliases inputs shouldn t fused sample_inputs_func=sample_unsqueeze skips= DecorateInfo unittest expectedFailure TestDTensorOps test_dtensor_op_db DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float BinaryUfuncInfo xlogy aliases= special xlogy dtypes=all_types_and torch bool torch half torch bfloat promotes_int_to_float=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_one_python_scalar=True We don t test gradient will NaN ll break rhs_make_tensor_kwargs=dict low= OpInfo zero_ op=lambda x torch zero_ x clone method_variant=None inplace_variant=torch Tensor zero_ dtypes=all_types_and_complex_and torch bool torch float torch bfloat https github com pytorch pytorch issues gradcheck_fast_mode=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True supports_gradgrad=True skips= DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit sample_inputs_func=sample_inputs_zero_ OpInfo logsumexp aliases= special logsumexp dtypes=all_types_and_complex_and torch bool torch half torch bfloat assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True gradcheck_fast_mode=False sample_inputs_func=sample_inputs_logsumexp reference_inputs_func=reference_inputs_logsumexp OpInfo trace dtypes=all_types_and_complex dtypesIfCUDA=all_types_and_complex_and torch chalf torch bool torch half torch bfloat error_inputs_func=error_inputs_trace supports_inplace_autograd=False supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_trace OpInfo transpose ref=_numpy_ref_transpose aliases= swapdims swapaxes assert_jit_shape_analysis=True dtypes=all_types_and_complex_and torch bool torch bfloat torch half torch chalf supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True vmap does support inplace views check_inplace_batched_forward_grad=False sample_inputs_func=sample_inputs_transpose_swapdims OpInfo transpose_copy assert_jit_shape_analysis=True dtypes=all_types_and_complex_and torch bool torch bfloat torch half torch chalf supports_out=True supports_forward_ad=True supports_fwgrad_bwgrad=True vmap does support inplace views check_inplace_batched_forward_grad=False sample_inputs_func=sample_inputs_transpose_swapdims skips= DecorateInfo unittest expectedFailure TestDTensorOps test_dtensor_op_db DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float OpInfo T op=lambda x x T dtypes=all_types_and_complex_and torch bool torch bfloat torch half torch chalf supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True skips= lambda impl DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit sample_inputs_func=sample_inputs_T error_inputs_func=error_inputs_T OpInfo H op=lambda x x H dtypes=all_types_and_complex_and torch bool torch bfloat torch half torch chalf supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False skips= lambda impl DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit sample_inputs_func=sample_inputs_T OpInfo mT op=lambda x x mT dtypes=all_types_and_complex_and torch bool torch bfloat torch half torch chalf Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True skips= lambda impl DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit sample_inputs_func=sample_inputs_adjoint OpInfo mH op=lambda x x mH aliases= adjoint dtypes=all_types_and_complex_and torch bool torch bfloat torch half torch chalf Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False skips= lambda impl DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit sample_inputs_func=sample_inputs_adjoint OpInfo tril dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_forward_ad=True supports_fwgrad_bwgrad=True error_inputs_func=error_inputs_tril_triu sample_inputs_func=sample_inputs_tril_triu skips= Compiler issue ROCm Regression started ROCm DecorateInfo unittest skip Skipped TestCommon test_non_standard_bool_values dtypes= torch bool active_if=TEST_WITH_ROCM OpInfo triu dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf supports_forward_ad=True supports_fwgrad_bwgrad=True error_inputs_func=error_inputs_tril_triu sample_inputs_func=sample_inputs_tril_triu skips= Compiler issue ROCm Regression started ROCm DecorateInfo unittest skip Skipped TestCommon test_non_standard_bool_values dtypes= torch bool active_if=TEST_WITH_ROCM OpInfo triu_indices dtypes=_dispatch_dtypes torch int torch int sample_inputs_func=sample_inputs_trilu_indices ref=lambda h w ofs= dtype=torch long device= cpu np array np triu_indices h ofs w dtype=dtype supports_out=False supports_autograd=False skips= skip these tests since we have non tensor input DecorateInfo unittest skip Skipped TestCommon test_noncontiguous_samples DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit DecorateInfo unittest skip Skipped TestMathBits test_neg_view OpInfo tril_indices dtypes=_dispatch_dtypes torch int torch int sample_inputs_func=sample_inputs_trilu_indices ref=lambda h w ofs= dtype=torch long device= cpu np array np tril_indices h ofs w dtype=dtype supports_out=False supports_autograd=False skips= skip these tests since we have non tensor input DecorateInfo unittest skip Skipped TestCommon test_noncontiguous_samples DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit DecorateInfo unittest skip Skipped TestMathBits test_neg_view OpInfo kron dtypes=all_types_and_complex_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and_complex_and torch bool torch half torch bfloat Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_inplace_autograd=False supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_kron decorators= RuntimeError view size compatible input tensor s size stride DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace_all_strides OpInfo inner dtypes=all_types_and_complex_and torch float torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch float torch bfloat dtypesIfROCM=floating_and_complex_types_and torch half torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False sample_inputs_func=sample_inputs_inner OpInfo tensordot dtypes=all_types_and_complex_and torch float torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch float torch bfloat dtypesIfROCM=floating_and_complex_types_and torch half torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False sample_inputs_func=sample_inputs_tensordot skips= Skip operator schema test because functional operator Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestOperatorSignatures test_get_torch_func_signature_exhaustive OpInfo to_sparse op=lambda x args x to_sparse args sample_inputs_func=sample_inputs_to_sparse dtypes=all_types_and_complex_and torch bool torch float torch bfloat backward_dtypes=floating_types backward_dtypesIfCUDA=floating_types_and torch float torch bfloat supports_out=False supports_sparse_csr=True supports_sparse_csc=True check_batched_grad=False check_batched_gradgrad=False skips= NotImplementedError Could run aten normal_ arguments SparseCPU backend DecorateInfo unittest skip TestCommon test_noncontiguous_samples TODO FIXME complex inputs requiring grad error forward DecorateInfo unittest skip Skipped TestCommon test_dtypes lambda impl DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive Allowed exception sparse tensors don t have strides DecorateInfo unittest skip Allowed exception TestCompositeCompliance test_operator DecorateInfo unittest skip Allowed exception TestCompositeCompliance test_backward DecorateInfo unittest skip Allowed exception TestTags test_tags TODO implement csr to_sparse sample_dim where sampled_dim DecorateInfo unittest skip csr to_sparse implemented Skipped TestSparseCSR test_sparse_csr_consistency Compiler issue ROCm Might need skip until ROCm DecorateInfo unittest skip Skipped TestCommon test_non_standard_bool_values dtypes= torch bool active_if=TEST_WITH_ROCM OpInfo logcumsumexp dtypes=floating_and_complex_types_and torch bfloat torch half backward_dtypes=floating_and_complex_types_and torch bfloat torch half supports_forward_ad=True supports_fwgrad_bwgrad=True skips= AssertionError UserWarning triggered Resized non-empty tensor did warn about DecorateInfo unittest expectedFailure TestCommon test_out_warning device_type= cuda RuntimeError max_values_cpu implemented ComplexDouble Falling back non-numerically stabilized exp causing nan results DecorateInfo unittest expectedFailure TestFwdGradients test_forward_mode_AD dtypes= torch complex DecorateInfo unittest expectedFailure TestFwdGradients test_fn_fwgrad_bwgrad dtypes= torch complex DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cuda sample_inputs_func=sample_inputs_logcumsumexp error_inputs_func=error_inputs_logcumsumexp UnaryUfuncInfo sigmoid aliases= special expit nn functional sigmoid aten_backward_name= sigmoid_backward ref=reference_sigmoid TEST_SCIPY None decorators= precisionOverride torch float e- torch complex e- torch bfloat e- skips= Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch complex torch cdouble device_type= cuda DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch chalf torch complex torch cdouble device_type= cuda dtypes=all_types_and_complex_and torch bool torch float torch bfloat dtypesIfCUDA=all_types_and_complex_and torch complex torch bool torch half torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True assert_autodiffed=True sigmoid z = + exp -z z = j pi odd_number denominator zero reference_numerics_filter=NumericsFilter condition=lambda x close_to_int x math pi j x is_complex x new_tensor False dtype=torch bool safe_val= UnaryUfuncInfo digamma ref=scipy special digamma TEST_SCIPY None aliases= special psi special digamma decorators= precisionOverride torch float e- dtypes=all_types_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and torch bool torch half torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True UnaryUfuncInfo erf ref=scipy special erf TEST_SCIPY None aliases= special erf decorators= precisionOverride torch float e- torch bfloat e- skips= DecorateInfo unittest skip Skipped sparse backward supported TestSparseUnaryUfuncs test_sparse_fn_grad dtypes=all_types_and torch bool torch half torch bfloat assert_autodiffed=True assert_jit_shape_analysis=True supports_sparse=True supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True UnaryUfuncInfo erfc ref=scipy special erfc TEST_SCIPY None aliases= special erfc decorators= precisionOverride torch float e- torch bfloat e- dtypes=all_types_and torch bool torch half torch bfloat assert_autodiffed=True supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True UnaryUfuncInfo erfinv ref=scipy special erfinv TEST_SCIPY None aliases= special erfinv decorators= precisionOverride torch float e- torch bfloat e- torch float e- dtypes=all_types_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and torch bool torch half torch bfloat supports_sparse_csr=True supports_sparse_csc=True supports_sparse_bsr=True supports_sparse_bsc=True supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True domain= - skips= Reference https github com pytorch pytorch pull #issuecomment- DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal active_if=TEST_SCIPY version parse scipy __version__ version parse DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large active_if=TEST_SCIPY version parse scipy __version__ version parse DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_small active_if=TEST_SCIPY version parse scipy __version__ version parse OpInfo nn functional smooth_l _loss ref=reference_smooth_l _loss sample_inputs_func=sample_inputs_smooth_l _loss dtypes=floating_types_and torch float torch bfloat backward_dtypes=floating_types_and torch float torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True skips= RuntimeError input- type - kind == TypeKind OptionalTypeINTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit OpInfo nn functional l _loss ref=loss_reference_reduction_wrapper lambda input target np abs input - target sample_inputs_func=sample_inputs_l _loss error_inputs_func=error_inputs_l _loss dtypes=floating_and_complex_types_and torch float torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True skips= RuntimeError input- type - kind == TypeKind OptionalTypeINTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float UnaryUfuncInfo lgamma ref=reference_lgamma TEST_SCIPY None aliases= special gammaln decorators= precisionOverride torch float e- dtypes=all_types_and torch bool torch half torch bfloat dtypesIfCUDA=all_types_and torch bool torch half torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True skips= Reference https github com pytorch pytorch pull #issuecomment- DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch float torch float active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch float torch float active_if=IS_WINDOWS lgamma have multiple singularities x = reference_numerics_filter=NumericsFilter condition=lambda x x safe_val= OpInfo logdet dtypes=floating_and_complex_types supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_linalg_det_logdet_slogdet decorators= skipCUDAIfNoMagma skipCPUIfNoLapack ` log_softmax ` supports different dtypes based whether ` dtype ` argument passed Hence two OpInfo entries one dtype other without OpInfo log_softmax aliases= special log_softmax nn functional log_softmax supports_out=True aten_backward_name= _log_softmax_backward_data dtypes=floating_types_and torch float torch bfloat sample_inputs_func=sample_inputs_softmax_variant supports_forward_ad=True supports_fwgrad_bwgrad=True assert_autodiffed=True OpInfo log_softmax variant_test_name= with_dtype aliases= special log_softmax nn functional log_softmax supports_out=True dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf sample_inputs_func=partial sample_inputs_softmax_variant with_dtype=True supports_forward_ad=True supports_fwgrad_bwgrad=True assert_autodiffed=True UnaryUfuncInfo logit aten_backward_name= logit_backward ref=scipy special logit TEST_SCIPY None domain= aliases= special logit supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True decorators= precisionOverride torch bfloat e- torch float e- dtypes=all_types_and torch bool torch half torch bfloat sample_inputs_func=sample_inputs_logit OpInfo where Currently only ` input ` tested gradcheck If we pass ` condition ` first none input which supports autograd will tested Hence following lambda op=lambda condition other kwargs torch where condition other kwargs ref=lambda condition other np where condition other sample_inputs_func=sample_inputs_where reference_inputs_func=reference_inputs_where error_inputs_func=error_inputs_where supports_forward_ad=True supports_fwgrad_bwgrad=True decorators= DecorateInfo onlyCUDA TestCommon test_errors skips= lambda impl DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf OpInfo nonzero dtypes=all_types_and_complex_and torch bool torch bfloat torch float torch chalf sample_inputs_func=sample_inputs_nonzero supports_autograd=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive nonzero argument out must Tensor tuple DecorateInfo unittest expectedFailure TestCommon test_out https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit nonzero raising warning when out resized DecorateInfo unittest expectedFailure TestCommon test_out_warning Can t find schemas operator some reason DecorateInfo unittest expectedFailure TestOperatorSignatures test_get_torch_func_signature_exhaustive Compiler issue ROCm Might need skip until ROCm DecorateInfo unittest skip Skipped TestCommon test_non_standard_bool_values dtypes= torch bool active_if=TEST_WITH_ROCM OpInfo nonzero_static dtypes=all_types_and_complex_and torch bool torch bfloat torch float torch chalf sample_inputs_func=sample_inputs_nonzero_static supports_out=False supports_autograd=False decorators= onlyCPU skips= DecorateInfo unittest expectedFailure TestCommon test_out DecorateInfo unittest expectedFailure TestCommon test_out_warning DecorateInfo unittest expectedFailure TestDTensorOps test_dtensor_op_db DecorateInfo unittest expectedFailure TestInductorOpInfo test_comprehensive DecorateInfo unittest expectedFailure TestVmapOperatorsOpInfo test_op_has_batch_rule DecorateInfo unittest skip Skipped TestCommon test_non_standard_bool_values dtypes= torch bool active_if=TEST_WITH_ROCM Following tests jiterator s python interface Jiterator can used author elementwise CUDA kernel jiterator _create_jit_fn returns callable behaves like regular pytorch op See create_jit_fn jiterator py more information UnaryUfuncInfo jiterator_unary op=torch cuda jiterator _create_jit_fn template typename T T unary T x x x + x ref=lambda x x x + x dtypes=all_types_and_complex_and torch bfloat torch float torch bool supports_out=False supports_autograd=False jiterator ops doesn t have backward defined decorators= onlyCUDA DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestUnaryUfuncs test_reference_numerics_extremal DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestUnaryUfuncs test_reference_numerics_hard DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestUnaryUfuncs test_reference_numerics_normal DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestUnaryUfuncs test_reference_numerics_small skips= Jiterator ops doesn t support neg conj view DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view Jiterator ops doesn t support CompositeCompliantTensor Following test should expectedFailure s causing cascading failures CUDA thus skipped DecorateInfo unittest skip skip TestCompositeCompliance test_operator Skip reference_numerics tests bool type defined function doesn t work bool DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch bool DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_hard dtypes= torch bool DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_normal dtypes= torch bool ROCm generates -inf+infj instead nan+infj complex some results DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch complex active_if=TEST_WITH_ROCM Newer numpy generates -inf+infj instead nan+infj complex some results DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch complex device_type= cuda Expected failure torch jiterator_unary valid op DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit Skip Nvfuser DecorateInfo unittest skip Skipped TestCudaFuserOpInfo BinaryUfuncInfo jiterator_binary op=torch cuda jiterator _create_jit_fn template typename T T binary T x T y T alpha x + alpha y alpha= ref=lambda input other alpha= np add input other alpha == np add input np multiply alpha other dtypes=all_types_and_complex_and torch bfloat torch float torch bool sample_inputs_func=partial sample_inputs_jiterator num_inputs= alpha=- supports_out=False supports_autograd=False jiterator ops doesn t have backward defined supports_rhs_python_scalar=False decorators= onlyCUDA skips= Jiterator ops doesn t support neg conj view DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view Jiterator ops doesn t support CompositeCompliantTensor Following test should expectedFailure s causing cascading failures CUDA thus skipped DecorateInfo unittest skip skip TestCompositeCompliance test_operator Expected failure torch jiterator_binary valid op DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit Skip Nvfuser DecorateInfo unittest skip Skipped TestCudaFuserOpInfo OpInfo jiterator_ inputs_with_extra_args op=torch cuda jiterator _create_jit_fn template typename T T binary T i T i T i T i T alpha T beta alpha i + beta i + i + i alpha= beta= ref=lambda i i i i alpha= beta= alpha i + beta i + i + i dtypes=all_types_and_complex_and torch bfloat torch float torch bool sample_inputs_func=partial sample_inputs_jiterator num_inputs= alpha= beta=- supports_out=False supports_autograd=False jiterator ops doesn t have backward defined decorators= onlyCUDA skips= Jiterator ops doesn t support neg conj view DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view Jiterator ops doesn t support CompositeCompliantTensor Following test should expectedFailure s causing cascading failures CUDA thus skipped DecorateInfo unittest skip skip TestCompositeCompliance test_operator Expected failure torch jiterator_ inputs_with_extra_args valid op DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit Skip Nvfuser DecorateInfo unittest skip Skipped TestCudaFuserOpInfo BinaryUfuncInfo jiterator_binary_return_by_ref op=torch cuda jiterator _create_multi_output_jit_fn template typename T void binary_return_by_ref T i T i T out out = i + i num_outputs= ref=operator add dtypes=all_types_and_complex_and torch bfloat torch float torch bool sample_inputs_func=partial sample_inputs_jiterator num_inputs= alpha=- supports_out=False supports_autograd=False jiterator ops doesn t have backward defined supports_rhs_python_scalar=False decorators= onlyCUDA skips= Jiterator ops doesn t support neg conj view DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view Jiterator ops doesn t support CompositeCompliantTensor Following test should expectedFailure s causing cascading failures CUDA thus skipped DecorateInfo unittest skip skip TestCompositeCompliance test_operator Expected failure torch jiterator_ inputs_with_extra_args valid op DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit Skip Nvfuser DecorateInfo unittest skip Skipped TestCudaFuserOpInfo OpInfo jiterator_ inputs_ outputs op=torch cuda jiterator _create_multi_output_jit_fn template typename T void binary_ outputs T i T i T out T out out = i + i out = i - i num_outputs= ref=lambda i i alpha= i + i i - i dtypes=all_types_and_complex_and torch bfloat torch float torch bool sample_inputs_func=partial sample_inputs_jiterator num_inputs= supports_out=False supports_autograd=False jiterator ops doesn t have backward defined decorators= onlyCUDA skips= Jiterator ops doesn t support neg conj view DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view Jiterator ops doesn t support CompositeCompliantTensor Following test should expectedFailure s causing cascading failures CUDA thus skipped DecorateInfo unittest skip skip TestCompositeCompliance test_operator Expected failure torch jiterator_ inputs_with_extra_args valid op DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit Skip Nvfuser DecorateInfo unittest skip Skipped TestCudaFuserOpInfo ` torch norm ` has multiple code paths depending value ` p ` These paths have different dtype support Also JIT supports most variants all them So we split OpInfo entries ` norm ` based code-paths JIT support OpInfo norm sample_inputs_func=sample_inputs_norm dtypes=floating_and_complex_types_and torch float torch bfloat torch chalf dtypesIfCUDA=floating_and_complex_types_and torch float torch bfloat TODO Benchmark again new implementation Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True check_batched_forward_grad=False supports_forward_ad=True supports_fwgrad_bwgrad=True skips= Dispatches Python vector_norm Not sure how make test happy Happens pass complex Also mystery DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float OpInfo norm variant_test_name= nuc sample_inputs_func=sample_inputs_norm_nuc decorators= skipCUDAIfNoMagmaAndNoCusolver skipCPUIfNoLapack check_batched_gradgrad=False torch autograd gradcheck GradcheckError While computing batched gradients got Could allocate memory change Tensor SizesAndStrides check_batched_forward_grad=False supports_forward_ad=True supports_fwgrad_bwgrad=True dtypes=floating_and_complex_types dtypesIfCUDA=floating_and_complex_types skips= Dispatches Python matrix_norm Not sure how make test happy DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch complex torch float OpInfo norm variant_test_name= fro sample_inputs_func=sample_inputs_norm_fro dtypes=floating_and_complex_types_and torch bfloat torch float dtypesIfCUDA=floating_and_complex_types_and torch float torch bfloat supports_forward_ad=True torch autograd gradcheck GradcheckError While computing batched gradients got Could allocate memory change Tensor SizesAndStrides check_batched_forward_grad=False supports_fwgrad_bwgrad=True skips= MPS has some mild accuracy issues float We divide tolerances DecorateInfo toleranceOverride torch float tol atol= e- rtol= TestConsistency test_output_match Issue conj torch dispatch see https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestSchemaCheckModeOpInfo test_schema_correctness dtypes= torch complex torch complex Dispatches Python vector_norm Not sure how make test happy DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch complex torch float OpInfo norm variant_test_name= inf sample_inputs_func=sample_inputs_norm_inf dtypes=floating_and_complex_types_and torch float torch bfloat torch chalf dtypesIfCUDA=floating_and_complex_types_and torch float torch bfloat supports_forward_ad=True check_batched_forward_grad=False supports_fwgrad_bwgrad=True fast gradcheck produces NaNs gradcheck_fast_mode=False skips= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cuda Dispatches Python vector_norm Not sure how make test happy Happens pass complex Also mystery DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float OpInfo t sample_inputs_func=sample_inputs_t supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False vmap does support inplace views check_inplace_batched_forward_grad=False autodiff_fusible_nodes= aliases inputs shouldn t fused autodiff_nonfusible_nodes= aliases inputs shouldn t fused dtypes=all_types_and_complex_and torch bool torch float torch bfloat assert_autodiffed=True error_inputs_func=error_inputs_t OpInfo t_copy sample_inputs_func=sample_inputs_t supports_out=True supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False vmap does support inplace views check_inplace_batched_forward_grad=False autodiff_fusible_nodes= aliases inputs shouldn t fused autodiff_nonfusible_nodes= aliases inputs shouldn t fused dtypes=all_types_and_complex_and torch bool torch float torch bfloat assert_autodiffed=True error_inputs_func=error_inputs_t OpInfo nn functional dropout op=lambda input args kwargs wrapper_set_seed torch nn functional dropout input args kwargs dtypes=floating_types_and torch float torch bfloat skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive Probably because we have used lambda op here AssertionError JIT Test does execute any logic DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit inplace variant dispatches dropout kernel while CUDA op dispatches _fused_dropout few more conditions hence different values skip here DecorateInfo unittest skip Skipped TestMathBits test_neg_view device_type= cuda DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu supports_forward_ad=True supports_fwgrad_bwgrad=True https github com pytorch pytorch issues check_batched_forward_grad=False supports_out=False sample_inputs_func=sample_inputs_dropout inplace_variant=lambda input args kwargs wrapper_set_seed torch nn functional dropout input args kwargs inplace=True OpInfo native_dropout_backward op=torch ops aten native_dropout_backward default aten_name= native_dropout_backward dtypes=all_types_and torch float torch bfloat torch bool dtypesIfCUDA=floating_types_and torch float torch bfloat supports_out=False sample_inputs_func=sample_inputs_dropout_backward skips= DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit Lazy tensor failures DecorateInfo unittest skip Skipped TestLazyOpInfo test_dispatched_to_lazy These tests fail only when built ASAN DecorateInfo unittest skip Fails ASAN TestLazyOpInfo test_correctness active_if=TEST_WITH_ASAN DecorateInfo unittest skip Fails ASAN TestLazyOpInfo test_correctness_with_reusing_ir active_if=TEST_WITH_ASAN OpInfo nn functional dropout d op=lambda input args kwargs wrapper_set_seed torch nn functional dropout d input args kwargs dtypes=floating_types_and torch float torch bfloat skips= lambda impl DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False check_batched_forward_grad=False As per docs valid input dims sample_inputs_func=partial sample_inputs_dropout valid_input_dim= inplace_variant=lambda input args kwargs wrapper_set_seed torch nn functional dropout d input args kwargs inplace=True OpInfo nn functional dropout d op=lambda input args kwargs wrapper_set_seed torch nn functional dropout d input args kwargs dtypes=floating_types_and torch float torch bfloat skips= lambda impl DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False check_batched_forward_grad=False As per docs valid input dims sample_inputs_func=partial sample_inputs_dropout valid_input_dim= inplace_variant=lambda input args kwargs wrapper_set_seed torch nn functional dropout d input args kwargs inplace=True OpInfo nn functional alpha_dropout op=lambda input args kwargs wrapper_set_seed torch nn functional alpha_dropout input args kwargs dtypes=floating_types_and torch float torch bfloat gradcheck_wrapper=wrapper_set_seed supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False sample_inputs_func=sample_inputs_dropout check_batched_forward_grad=False inplace_variant=lambda input args kwargs wrapper_set_seed torch nn functional alpha_dropout input args kwargs inplace=True skips= lambda impl DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive AssertionError Tensor-likes close Fails cuda DecorateInfo unittest expectedFailure TestCommon test_compare_cpu device_type= cuda AssertionError Tensor-likes close DecorateInfo unittest expectedFailure TestCommon test_compare_cpu device_type= xpu DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit In training mode feature_alpha_dropout currently doesn t support inputs complex dtype unlike when ` train=False ` supports complex inputs hence OpInfos cover all cases OpInfo nn functional feature_alpha_dropout op=lambda input args kwargs wrapper_set_seed torch nn functional feature_alpha_dropout input args kwargs variant_test_name= with_train dtypes=floating_types_and torch float torch bfloat skips= lambda impl DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit torch autograd gradcheck GradcheckError While computing batched gradients got vmap We do yet support calling random operations inside vmap Please perform random operations outside vmap workaround DecorateInfo unittest expectedFailure TestFwdGradients test_forward_mode_AD DecorateInfo unittest expectedFailure TestFwdGradients test_inplace_forward_mode_AD DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False As per docs valid input dims sample_inputs_func=partial sample_inputs_dropout train=True valid_input_dim= inplace_variant=lambda input args kwargs wrapper_set_seed torch nn functional feature_alpha_dropout input args kwargs inplace=True OpInfo nn functional feature_alpha_dropout op=lambda input args kwargs wrapper_set_seed torch nn functional feature_alpha_dropout input args kwargs variant_test_name= without_train dtypes=all_types_and_complex_and torch bool torch float torch bfloat skips= lambda impl DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit gradcheck_wrapper=wrapper_set_seed supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False sample_inputs_func=partial sample_inputs_dropout train=False inplace_variant=lambda input args kwargs wrapper_set_seed torch nn functional feature_alpha_dropout input args kwargs inplace=True OpInfo nn functional one_hot ref=reference_one_hot supports_out=False dtypes=_dispatch_dtypes torch int sample_inputs_func=sample_inputs_one_hot OpInfo nn functional embedding aten_backward_name= embedding_dense_backward We use lambda reshuffle positional arguments This because currently only ` input ` field SampleInput tested gradient tests op=lambda weight idx kwargs torch nn functional embedding idx weight kwargs dtypes=floating_types_and torch bfloat torch float sample_inputs_func=sample_inputs_embedding allow_cow_input_materialize_forward= error_inputs_func=error_inputs_embedding supports_forward_ad=True supports_fwgrad_bwgrad=True skips= lambda impl DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive Fails CI https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestCommon test_compare_cpu Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestMathBits test_neg_view device_type= cuda Not problem embedding does weird stuff its input renormalizes DecorateInfo unittest skip Allowed exemption TestCompositeCompliance test_operator Fails due non-determinism see issue TODO Investigate why more granular skips test don t work CI DecorateInfo unittest skip Skipped TestExpandedWeightFunctional test_expanded_weight_forward supports_expanded_weight=True supports_out=False OpInfo nn functional embedding_bag We use lambda reshuffle positional arguments This because currently only ` input ` field SampleInput tested gradient tests op=lambda weight idx kwargs torch nn functional embedding_bag idx weight kwargs dtypes=floating_types_and torch bfloat torch float dtypesIfCUDA=floating_types_and torch bfloat torch float backward supported mode ` max ` dtype ` bfloat ` backward_dtypesIfCUDA=floating_types_and torch float sample_inputs_func=sample_inputs_embedding_bag skips= lambda impl DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive Not problem embedding_bag does weird stuff its input renormalizes DecorateInfo unittest skip Allowed exemption TestCompositeCompliance test_operator gradcheck_nondet_tol=GRADCHECK_NONDET_TOL supports_out=False supports_gradgrad=False allow_cow_input_materialize_forward= OpInfo nn functional multi_head_attention_forward op=lambda input args kwargs wrapper_set_seed torch nn functional multi_head_attention_forward input args kwargs dtypes=floating_types_and torch bfloat torch float sample_inputs_func=sample_inputs_multi_head_attention_forward skips= Tensor-likes close DecorateInfo unittest skip Skipped TestCommon test_noncontiguous_samples dtypes= torch float DecorateInfo toleranceOverride torch float tol atol= e- rtol= TestDecomp test_comprehensive TODO skip now since we can t skip runtime arch support taken scaled_dot_product_attention DecorateInfo unittest skip Skipped TestInductorOpInfo test_comprehensive randomness DecorateInfo unittest skip Skipped TestFwdGradients test_forward_mode_AD DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu lambda impl AssertionError JIT Test does execute any logic DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive tests running very slowly break slow tests so we skip them instead using ` slowTest ` DecorateInfo unittest skip Skipped TestCompositeCompliance test_forward_ad DecorateInfo unittest skip Skipped TestCompositeCompliance test_operator DecorateInfo unittest skip Skipped - baddbmm decomp does have enough precision -bit float TestDecomp test_comprehensive dtypes= torch bfloat torch float DecorateInfo unittest skip Skipped - baddbmm decomp does have enough precision -bit float TestDecomp test_quick dtypes= torch bfloat torch float supports_out=False supports_gradgrad=True supports_forward_ad=True supports_fwgrad_bwgrad=True Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True UnaryUfuncInfo nn functional softplus aten_backward_name= softplus_backward ref=reference_softplus sample_kwargs=lambda device dtype input beta threshold beta threshold sample_inputs_func=partial sample_inputs_elementwise_unary op_kwargs= beta threshold supports_forward_ad=True supports_fwgrad_bwgrad=True dtypes=floating_types_and torch bfloat torch float decorators= DecorateInfo toleranceOverride torch half tol atol= e- rtol= e- torch bfloat tol atol= e- rtol= e- TestUnaryUfuncs OpInfo nn functional mse_loss aten_backward_name= mse_loss_backward ref=loss_reference_reduction_wrapper lambda input target input - target sample_inputs_func=sample_inputs_loss supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True dtypes=floating_types_and torch float torch bfloat skips= RuntimeError input- type - kind == TypeKind OptionalType INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit dtypes= torch float OpInfo nn functional grid_sample dtypes=floating_types_and torch float torch bfloat supports_out=False sample_inputs_func=sample_inputs_grid_sample reference_inputs_func=reference_inputs_grid_sample supports_gradgrad=False gradcheck_nondet_tol= e- TODO delete OpInfo once we add meta support grid_sampler_ d OpInfo grid_sampler_ d dtypes=floating_types_and torch float torch bfloat supports_out=False sample_inputs_func=sample_inputs_grid_sampler_ d supports_gradgrad=False gradcheck_nondet_tol= e- skips= DecorateInfo slowTest TestDecomp test_comprehensive dtypes= torch float torch float active_if=IS_WINDOWS TODO Remove grid_sampler_ d tests once ` nn functional grid_sample ` has MPS support all cases OpInfo grid_sampler_ d dtypes=floating_types_and torch float torch bfloat supports_out=False sample_inputs_func=sample_inputs_grid_sampler_ d supports_gradgrad=False gradcheck_nondet_tol= e- skips= NOTE Only run MPS DecorateInfo unittest skip Skipped device_type= cpu DecorateInfo unittest skip Skipped device_type= cuda DecorateInfo unittest skip Skipped device_type= xpu DecorateInfo unittest skip Skipped device_type= meta OpInfo argwhere ref=np argwhere dtypes=all_types_and_complex_and torch bool torch float torch bfloat supports_out=False supports_autograd=False sample_inputs_func=sample_inputs_argwhere skips= Compiler issue ROCm Might need skip until ROCm DecorateInfo unittest skip Skipped TestCommon test_non_standard_bool_values dtypes= torch bool active_if=TEST_WITH_ROCM ReductionOpInfo all identity=True supports_autograd=False result_dtype=torch bool dtypes=all_types_and_complex_and torch bool torch float torch bfloat ref=reference_reduction_numpy np all skips= FIXME uint input returns uint instead bool DecorateInfo unittest expectedFailure TestReductions test_result_dtype dtypes= torch uint ReductionOpInfo any identity=False supports_autograd=False result_dtype=torch bool dtypes=all_types_and_complex_and torch bool torch float torch bfloat ref=reference_reduction_numpy np any skips= FIXME uint input returns uint instead bool DecorateInfo unittest expectedFailure TestReductions test_result_dtype dtypes= torch uint ReductionOpInfo amax nan_policy= propagate supports_forward_ad=True check_batched_forward_grad=False supports_fwgrad_bwgrad=True dtypes=all_types_and torch float torch bfloat torch bool ref=reference_reduction_numpy np amax skips= FIXME reduces all dimensions when dim= DecorateInfo unittest expectedFailure TestReductions test_dim_empty DecorateInfo unittest expectedFailure TestReductions test_dim_empty_keepdim error_inputs_func=error_inputs_aminmax_amax_amin ReductionOpInfo amin nan_policy= propagate supports_forward_ad=True check_batched_forward_grad=False supports_fwgrad_bwgrad=True dtypes=all_types_and torch float torch bfloat torch bool ref=reference_reduction_numpy np amin skips= FIXME reduces all dimensions when dim= DecorateInfo unittest expectedFailure TestReductions test_dim_empty DecorateInfo unittest expectedFailure TestReductions test_dim_empty_keepdim error_inputs_func=error_inputs_aminmax_amax_amin ReductionOpInfo argmax supports_multiple_dims=False supports_autograd=False assert_jit_shape_analysis=True result_dtype=torch int dtypes=all_types_and torch float torch bfloat ref=reference_reduction_numpy np argmax supports_keepdims=False ReductionOpInfo argmin supports_multiple_dims=False supports_autograd=False result_dtype=torch int dtypes=all_types_and torch float torch bfloat ref=reference_reduction_numpy np argmin supports_keepdims=False ReductionOpInfo count_nonzero identity= supports_out=False supports_autograd=False result_dtype=torch int dtypes=all_types_and_complex_and torch bool torch float torch bfloat sample_inputs_func=sample_inputs_reduction_count_nonzero ref=reference_reduction_numpy np count_nonzero skips= FIXME count_nonzero does accept keepdim kwarg DecorateInfo unittest skip Skipped TestReductions test_dim_default_keepdim DecorateInfo unittest skip Skipped TestReductions test_dim_none_keepdim DecorateInfo unittest skip Skipped TestReductions test_dim_single_keepdim DecorateInfo unittest skip Skipped TestReductions test_dim_empty_keepdim DecorateInfo unittest skip Skipped TestReductions test_dim_multi_keepdim DecorateInfo unittest skip Skipped TestReductions test_dim_multi_unsorted_keepdim DecorateInfo unittest skip Skipped TestReductions test_dim_offbounds_keepdim FIXME dim= reduces all dimensions DecorateInfo unittest skip Skipped TestReductions test_dim_empty ReductionOpInfo mean nan_policy= propagate supports_forward_ad=True supports_fwgrad_bwgrad=True FIXME mean needs dim parameter when using out overload Adding generate_args_kwargs does work since these also get passed onto reference implementations supports_out=True assert_autodiffed=True assert_jit_shape_analysis=True promotes_int_to_float=True dtypes=floating_and_complex_types_and torch float torch bfloat ref=reference_reduction_numpy np mean error_inputs_func=error_inputs_mean skips= AssertionError RuntimeError raised Expected RuntimeError when doing unsafe cast result dtype torch float into out= dtype torch long DecorateInfo unittest skip Skipped TestCommon test_out device_type= cuda dtypes= torch float FIXME mean does support passing keepdim without passing dim DecorateInfo unittest skip Skipped TestReductions test_dim_default_keepdim FIXME mean reduces all dimensions when dim= DecorateInfo unittest skip Skipped TestReductions test_dim_empty DecorateInfo unittest skip Skipped TestReductions test_dim_empty_keepdim FIXME improve precision DecorateInfo unittest skip Skipped TestReductions test_ref_small_input dtypes= torch float DecorateInfo unittest skip Skipped TestReductions test_ref_extremal_values device_type= cuda dtypes= torch complex ReductionOpInfo nanmean nan_policy= omit assert_autodiffed=True promotes_int_to_float=True supports_forward_ad=True check_batched_forward_grad=False supports_fwgrad_bwgrad=True dtypes=floating_types_and torch float torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch float torch bfloat torch chalf sample_inputs_func=sample_inputs_nan_reduction supports_multiple_dims=True ref=reference_reduction_numpy np nanmean skips= AssertionError False true Failure testing nodes autodifferentiation DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit FIXME prod reduces all dimensions when dim= DecorateInfo unittest skip Skipped TestReductions test_dim_empty DecorateInfo unittest skip Skipped TestReductions test_dim_empty_keepdim FIXME improve precision DecorateInfo unittest skip Skipped TestReductions test_ref_small_input dtypes= torch float DecorateInfo unittest skip Skipped TestReductions test_ref_duplicate_values device_type= cuda dtypes= torch float DecorateInfo unittest skip Skipped TestReductions test_ref_extremal_values device_type= cuda dtypes= torch complex DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestConsistency test_output_match device_type= mps ReductionOpInfo std nan_policy= propagate supports_out=True complex_to_real=True supports_forward_ad=True supports_fwgrad_bwgrad=True assert_autodiffed=True promotes_int_to_float=True check_batched_forward_grad=False dtypes=floating_and_complex_types_and torch half torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch half torch bfloat sample_inputs_func=sample_inputs_std_var ref=reference_std_var np std generate_args_kwargs=generate_std_var_kwargs skips= FIXME cannot specify keepdim without dim DecorateInfo unittest skip Skipped TestReductions test_dim_default_keepdim FIXME dim= reduces all dimensions DecorateInfo unittest skip Skipped TestReductions test_dim_empty DecorateInfo unittest skip Skipped TestReductions test_dim_empty_keepdim FIXME improve precision DecorateInfo unittest skip Skipped TestReductions test_ref_small_input dtypes= torch float DecorateInfo unittest skip Skipped TestReductions test_ref_duplicate_values dtypes= torch float ReductionOpInfo std variant_test_name= unbiased nan_policy= propagate supports_out=False complex_to_real=True supports_forward_ad=True supports_fwgrad_bwgrad=True assert_autodiffed=True promotes_int_to_float=True check_batched_forward_grad=False dtypes=floating_and_complex_types_and torch half torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch half torch bfloat sample_inputs_func=sample_inputs_std_var_unbiased skips= FIXME dim= reduces all dimensions DecorateInfo unittest skip Skipped TestReductions test_dim_empty DecorateInfo unittest skip Skipped TestReductions test_dim_empty_keepdim ReductionOpInfo var nan_policy= propagate supports_out=True assert_autodiffed=True promotes_int_to_float=True complex_to_real=True supports_forward_ad=True supports_fwgrad_bwgrad=True check_batched_forward_grad=False dtypes=floating_and_complex_types_and torch half torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch half torch bfloat sample_inputs_func=sample_inputs_std_var ref=reference_std_var np var generate_args_kwargs=generate_std_var_kwargs skips= FIXME cannot specify keepdim without dim DecorateInfo unittest skip Skipped TestReductions test_dim_default_keepdim FIXME dim= reduces all dimensions DecorateInfo unittest skip Skipped TestReductions test_dim_empty DecorateInfo unittest skip Skipped TestReductions test_dim_empty_keepdim FIXME improve precision DecorateInfo unittest skip Skipped TestReductions test_ref_small_input DecorateInfo unittest skip Skipped TestReductions test_ref_duplicate_values NumPy giving NaN DecorateInfo unittest skip Skipped TestReductions test_ref_large_input ReductionOpInfo var variant_test_name= unbiased nan_policy= propagate supports_out=False complex_to_real=True supports_forward_ad=True supports_fwgrad_bwgrad=True assert_autodiffed=True promotes_int_to_float=True check_batched_forward_grad=False dtypes=floating_and_complex_types_and torch half torch bfloat dtypesIfCUDA=floating_and_complex_types_and torch half torch bfloat sample_inputs_func=sample_inputs_std_var_unbiased skips= FIXME dim= reduces all dimensions DecorateInfo unittest skip Skipped TestReductions test_dim_empty DecorateInfo unittest skip Skipped TestReductions test_dim_empty_keepdim ReductionOpInfo prod identity= nan_policy= propagate supports_multiple_dims=False https github com pytorch pytorch issues gradcheck_fast_mode=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_int =True gradcheck_nondet_tol=GRADCHECK_NONDET_TOL dtypes=all_types_and_complex_and torch bool torch bfloat torch float dtypesIfCUDA=all_types_and_complex_and torch bool torch float torch bfloat torch chalf sample_inputs_func=sample_inputs_prod ref=prod_numpy skips= FIXME prod does support passing keepdim without passing dim DecorateInfo unittest skip Skipped TestReductions test_dim_default_keepdim FIXME prod reduces all dimensions when dim= DecorateInfo unittest skip Skipped TestReductions test_dim_empty DecorateInfo unittest skip Skipped TestReductions test_dim_empty_keepdim FIXME prod does support passing None dim DecorateInfo unittest skip Skipped TestReductions test_dim_none DecorateInfo unittest skip Skipped TestReductions test_dim_none_keepdim DecorateInfo unittest skip Skipped TestReductions test_ref_small_input dtypes= torch float torch complex DecorateInfo unittest skip Skipped TestReductions test_ref_duplicate_values dtypes= torch uint torch float torch complex FIXME ValueError The data MaskedTensor Tensor b do match DecorateInfo unittest skip Skipped TestOperators test_reduction_all dtypes= torch float ReductionOpInfo sum identity= nan_policy= propagate supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_int =True dtypes=all_types_and_complex_and torch bool torch float torch bfloat dtypesIfCUDA=all_types_and_complex_and torch bool torch float torch bfloat torch chalf ref=reference_reduction_numpy np sum error_inputs_sparse_func=error_inputs_sparse_reduction_sum sample_inputs_sparse_coo_func=partial sample_inputs_sparse_reduction_sum layout=torch sparse_coo sample_inputs_sparse_csr_func=partial sample_inputs_sparse_reduction_sum layout=torch sparse_csr sample_inputs_sparse_csc_func=partial sample_inputs_sparse_reduction_sum layout=torch sparse_csc sample_inputs_sparse_bsr_func=partial sample_inputs_sparse_reduction_sum layout=torch sparse_bsr sample_inputs_sparse_bsc_func=partial sample_inputs_sparse_reduction_sum layout=torch sparse_bsc skips= FIXME sum does support passing keepdim without passing dim DecorateInfo unittest skip Skipped TestReductions test_dim_default_keepdim FIXME sum reduces all dimensions when dim= DecorateInfo unittest skip Skipped TestReductions test_dim_empty DecorateInfo unittest skip Skipped TestReductions test_dim_empty_keepdim FIXME improve precision DecorateInfo unittest skip Skipped TestReductions test_ref_small_input dtypes= torch float DecorateInfo unittest skip Skipped TestReductions test_ref_duplicate_values dtypes= torch float DecorateInfo unittest skip Skipped TestOperators test_reduction_all dtypes= torch float ReductionOpInfo nansum identity= nan_policy= omit supports_out=True promotes_int_to_int =True supports_forward_ad=True check_batched_forward_grad=False supports_fwgrad_bwgrad=True dtypes=all_types_and torch bool torch float torch bfloat dtypesIfCUDA=all_types_and_complex_and torch bool torch float torch bfloat torch chalf sample_inputs_func=sample_inputs_nan_reduction supports_multiple_dims=True ref=reference_reduction_numpy np nansum skips= please report bug PyTorch DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit FIXME nansum reduces all dimensions when dim= DecorateInfo unittest expectedFailure TestReductions test_dim_empty DecorateInfo unittest expectedFailure TestReductions test_dim_empty_keepdim FIXME flaky test so skipped instead xfailed possibly bad low precision reference numpy DecorateInfo unittest skip Skipped TestReductions test_ref_small_input dtypes= torch float DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestConsistency test_output_match device_type= mps ReductionOpInfo hash_tensor result_dtype=torch uint supports_autograd=False dtypes=all_types_and torch bool torch float torch bfloat dtypesIfCUDA=all_types_and torch bool torch float torch bfloat ref=reference_hash_tensor skips= hash_tensor reduces all dimensions when dim= do sum prod etc DecorateInfo unittest expectedFailure TestReductions test_dim_empty DecorateInfo unittest expectedFailure TestReductions test_dim_empty_keepdim aten hash_tensor hit vmap fallback which currently disabled DecorateInfo unittest skip Skipped TestVmapOperatorsOpInfo test_op_has_batch_rule DecorateInfo unittest skip Skipped TestVmapOperatorsOpInfo test_vmap_exhaustive NYI DecorateInfo unittest expectedFailure TestInductorOpInfo test_comprehensive Sharding strategy NYI DecorateInfo unittest expectedFailure TestDTensorOps test_dtensor_op_db OpInfo nn functional ctc_loss dtypes=floating_types supports_out=False sample_inputs_func=sample_inputs_ctc_loss gradcheck_wrapper see https github com pytorch pytorch issues gradcheck_wrapper=gradcheck_wrapper_ctc_loss skips= RuntimeError derivative aten _ctc_loss_backward implemented DecorateInfo unittest expectedFailure TestBwdGradients test_fn_gradgrad dtypes= torch float RuntimeError derivative aten _ctc_loss_backward implemented DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes= torch float Ref https github com pytorch pytorch issues DecorateInfo unittest skip Fails ASAN TestProxyTensorOpInfo test_make_fx_fake_exhaustive active_if=TEST_WITH_ASAN OpInfo nn functional cosine_embedding_loss dtypes=all_types_and torch half torch bfloat torch bool supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cuda sample_inputs_func=sample_inputs_cosine_embedding_loss OpInfo nn functional nll_loss dtypes=floating_types_and torch float torch bfloat supports_out=False sample_inputs_func=sample_inputs_nll_loss supports_forward_ad=True supports_fwgrad_bwgrad=True assert_jit_shape_analysis=True skips= RuntimeError undefined value tensor File string line the_method i i torch nn functional nll_loss i i weight=tensor dtype=torch float ~~~~~~ --- HERE DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes= torch float Fails unknown reason https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestCompositeCompliance test_cow_input device_type= cuda DecorateInfo unittest skip FP nll_loss cases have been enabled MPS yet dtypes= torch half device_type= mps OpInfo nn functional gaussian_nll_loss dtypes=floating_types_and torch half torch bfloat Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_gaussian_nll_loss error_inputs_func=error_inputs_gaussian_nll_loss skips= Pre-existing condition calls item needs fixed DecorateInfo unittest expectedFailure TestCompositeCompliance test_backward DecorateInfo unittest expectedFailure TestCompositeCompliance test_forward_ad Pre-existing condition calls item needs fixed DecorateInfo unittest expectedFailure TestCompositeCompliance test_operator JIT does support variadic tensors RuntimeError input- type - kind == TypeKind OptionalType INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes= torch float DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestConsistency test_output_match device_type= mps DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestConsistency test_output_grad_match device_type= mps OpInfo nn functional hinge_embedding_loss dtypes=floating_types_and torch half torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_hinge_embedding_loss error_inputs_func=error_inputs_hinge_embedding_loss reference_inputs_func=reference_inputs_hinge_embedding_loss OpInfo nn functional huber_loss aten_backward_name= huber_loss_backward dtypes=floating_types_and torch float torch bfloat supports_out=False supports_forward_ad=True sample_inputs_func=sample_inputs_huber_loss error_inputs_func=error_inputs_huber_loss skips= JIT does support variadic tensors RuntimeError input- type - kind == TypeKind OptionalType INTERNAL ASSERT FAILED torch csrc jit passes utils check_alias_annotation cpp please report bug PyTorch DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes= torch float OpInfo nn functional pdist ref=reference_pdist sample_inputs_func=sample_inputs_pdist dtypes=floating_types supports_out=False supports_gradgrad=False skips= DecorateInfo unittest skip Unsupported MPS now TestCommon test_numpy_ref_mps OpInfo nn functional poisson_nll_loss dtypes=all_types_and torch half torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_poisson_nll_loss error_inputs_func=error_inputs_poisson_nll_loss OpInfo argsort dtypes=all_types_and torch bool torch float torch bfloat dtypesIfCUDA=all_types_and torch bool torch float torch bfloat sample_inputs_func=sample_inputs_sort supports_out=False supports_autograd=False skips= DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes= torch float DecorateInfo unittest expectedFailure TestCommon test_non_standard_bool_values dtypes= torch bool device_type= cuda active_if=not TEST_WITH_ROCM OpInfo repeat_interleave dtypes=all_types_and_complex_and torch bool torch float torch bfloat torch chalf backward_dtypesIfCUDA=floating_and_complex_types_and torch float torch bfloat torch chalf sample_inputs_func=sample_inputs_repeat_interleave supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False skips= DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes= torch float torch complex OpInfo nn functional pairwise_distance ref=lambda b p= eps= e- keepdim=False np sum np abs - b + eps p axis=- keepdims=keepdim p sample_inputs_func=sample_inputs_pairwise_distance dtypes=all_types_and_complex_and torch float torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True skips= DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes= torch float torch complex OpInfo nn functional pixel_shuffle sample_inputs_func=sample_inputs_pixel_shuffle dtypes=all_types_and_complex_and torch bool torch float torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True skips= DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes= torch float torch complex OpInfo nn functional pixel_unshuffle sample_inputs_func=sample_inputs_pixel_unshuffle dtypes=all_types_and_complex_and torch bool torch float torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True skips= DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit dtypes= torch float torch complex OpInfo nn functional channel_shuffle sample_inputs_func=sample_inputs_channel_shuffle dtypes=all_types_and_complex_and torch bool torch float torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True allow_cow_input_materialize_forward= allow_cow_input_materialize_backward= output grad skips= Skip due NotImplementedError MPS device DecorateInfo unittest expectedFailure TestConsistency DecorateInfo unittest expectedFailure TestDTensorOps test_dtensor_op_db DecorateInfo unittest expectedFailure TestMeta test_dispatch_symbolic_meta_outplace_all_strides OpInfo nn functional kl_div sample_inputs_func=sample_inputs_kl_div dtypes=floating_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True OpInfo diagflat ref=lambda input offset= np diagflat input k=offset sample_inputs_func=sample_inputs_diagflat dtypes=all_types_and_complex_and torch bool torch bfloat torch float dtypesIfCUDA=all_types_and_complex_and torch bool torch float torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False OpInfo scatter_reduce variant_test_name= sum inplace_variant=torch Tensor scatter_reduce_ complex added dtypes complex gradients properly handled scatter_reduce hasn t been added whitelist gen_variable_type yet dtypes=all_types_and torch float torch bfloat torch bool supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_scatter_reduce skips= Compiler issue ROCm Regression started ROCm DecorateInfo unittest skip Skipped TestCommon test_non_standard_bool_values dtypes= torch bool active_if=TEST_WITH_ROCM OpInfo scatter_reduce variant_test_name= prod complex added dtypes complex gradients properly handled scatter_reduce hasn t been added whitelist gen_variable_type yet dtypes=all_types_and torch float torch bfloat torch bool dtypesIfCUDA=all_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat sample_inputs_func=sample_inputs_scatter_reduce skips= Not implemented DecorateInfo unittest expectedFailure TestFwdGradients test_forward_mode_AD DecorateInfo unittest expectedFailure TestFwdGradients test_inplace_forward_mode_AD DecorateInfo unittest expectedFailure TestFwdGradients test_fn_fwgrad_bwgrad OpInfo scatter_reduce variant_test_name= mean complex added dtypes complex gradients properly handled scatter_reduce hasn t been added whitelist gen_variable_type yet dtypes=all_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat dtypesIfCUDA=all_types_and torch float torch bfloat supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_scatter_reduce OpInfo scatter_reduce variant_test_name= amin dtypes=all_types_and torch float torch bfloat torch bool dtypesIfCUDA=all_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat supports_forward_ad=True check_batched_forward_grad=False supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_scatter_reduce OpInfo scatter_reduce variant_test_name= amax dtypes=all_types_and torch float torch bfloat torch bool dtypesIfCUDA=all_types_and torch float torch bfloat dtypesIfHpu=custom_types torch float torch bfloat supports_forward_ad=True check_batched_forward_grad=False supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_scatter_reduce OpInfo _segment_reduce aten_name= segment_reduce variant_test_name= lengths dtypes=floating_types_and torch float torch bfloat supports_out=False RuntimeError derivative aten _segment_reduce_backward implemented supports_gradgrad=False sample_inputs_func=sample_inputs_segment_reduce skips= FIXME CUDA driver API confirmed leak __main__ TestJitCUDA test_variant_consistency_jit_segment_reduce_cuda_float DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit device_type= cuda OpInfo _segment_reduce aten_name= segment_reduce variant_test_name= offsets dtypes=floating_types_and torch float torch bfloat supports_out=False RuntimeError derivative aten _segment_reduce_backward implemented supports_gradgrad=False sample_inputs_func=partial sample_inputs_segment_reduce mode= offsets skips= FIXME CUDA driver API confirmed leak __main__ TestJitCUDA test_variant_consistency_jit_segment_reduce_cuda_float DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit device_type= cuda op_db += opinfo definitions op_db Separate registry experimental Python Reference OpInfos python_ref_db = Elementwise Unary OpInfos ElementwiseUnaryPythonRefInfo _refs abs torch_opinfo_name= abs skips= Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_small dtypes= torch int active_if=TEST_WITH_ASAN ElementwiseUnaryPythonRefInfo _refs acos torch_opinfo_name= acos skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_normal device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS Failing wrong imaginary sign least some Windows jobs DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_small device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS Failing wrong imaginary sign least some Windows jobs DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS ElementwiseUnaryPythonRefInfo _refs acosh torch_opinfo_name= acosh skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_normal device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS Failing wrong imaginary sign least some Windows jobs DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_small device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS ElementwiseUnaryPythonRefInfo _refs asin torch_opinfo_name= asin decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestUnaryUfuncs device_type= cuda DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu precisionOverride torch bfloat e- skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS ElementwiseUnaryPythonRefInfo _refs asinh torch_opinfo_name= asinh decorators= precisionOverride torch bfloat e- skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_small device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_normal device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cuda dtypes= torch cdouble active_if=IS_WINDOWS PythonRefInfo _refs lerp torch_opinfo_name= lerp PythonRefInfo _refs ones torch_opinfo_name= ones skips= Tests assume input tensor sequence tensors DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view PythonRefInfo _refs zeros torch_opinfo_name= zeros skips= Tests assume input tensor sequence tensors DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view PythonRefInfo _refs cauchy torch_opinfo_name= cauchy decorators= TODO RuntimeError no _refs support torch rand_like DecorateInfo unittest skip TODO RuntimeError no _refs support torch rand_like TestCommon test_python_ref AssertionError Tensor-likes close DecorateInfo unittest skip Expected cauchy comparable TestCommon test_out DecorateInfo unittest skip Expected cauchy comparable TestCommon test_out_warning DecorateInfo unittest expectedFailure TestCommon test_python_ref_executor DecorateInfo unittest skip Expected cauchy comparable TestCommon test_python_ref_torch_fallback DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu DecorateInfo unittest expectedFailure TestMathBits test_neg_view PythonRefInfo _refs exponential torch_opinfo_name= exponential supports_out=True decorators= dtypes do support check_uniform_bounds rand_like DecorateInfo unittest expectedFailure TestCommon test_python_ref_meta dtypes= torch int torch uint torch int torch int torch int DecorateInfo unittest skip Skipped TestCommon test_dtypes TODO RuntimeError no _refs support torch rand_like DecorateInfo unittest skip TODO RuntimeError no _refs support torch rand_like TestCommon test_python_ref AssertionError Tensor-likes close DecorateInfo unittest skip Expected exponential comparable TestCommon test_out DecorateInfo unittest skip Expected exponential comparable TestCommon test_out_warning DecorateInfo unittest expectedFailure TestCommon test_python_ref_executor DecorateInfo unittest skip Expected exponential comparable TestCommon test_python_ref_torch_fallback DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu DecorateInfo unittest expectedFailure TestMathBits test_neg_view PythonRefInfo _refs geometric torch_opinfo_name= geometric supports_out=True decorators= dtypes do support check_uniform_bounds rand_like DecorateInfo unittest skip Skipped TestCommon test_dtypes DecorateInfo unittest expectedFailure TestCommon test_python_ref_meta dtypes= torch int torch uint torch int torch int torch int DecorateInfo unittest expectedFailure TestCommon test_python_ref_torch_fallback dtypes= torch int torch uint torch int torch int torch int TODO RuntimeError no _refs support torch rand_like DecorateInfo unittest skip TODO RuntimeError no _refs support torch rand_like TestCommon test_python_ref DecorateInfo unittest skip Expected geometric comparable TestCommon test_python_ref_executor device_type= cuda AssertionError Tensor-likes close DecorateInfo unittest skip Expected geometric comparable TestCommon test_out DecorateInfo unittest skip Expected geometric comparable TestCommon test_out_warning DecorateInfo unittest skip Expected geometric comparable TestCommon test_python_ref_torch_fallback DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu DecorateInfo unittest expectedFailure TestMathBits test_neg_view PythonRefInfo _refs log_normal torch_opinfo_name= log_normal supports_out=True decorators= TODO RuntimeError no _refs support torch rand_like DecorateInfo unittest skip TODO RuntimeError no _refs support torch rand_like TestCommon test_python_ref DecorateInfo unittest skip Expected log_normal comparable TestCommon test_python_ref_executor device_type= cuda AssertionError Tensor-likes close DecorateInfo unittest skip Expected log_normal comparable TestCommon test_out DecorateInfo unittest skip Expected log_normal comparable TestCommon test_out_warning DecorateInfo unittest skip Expected log_normal comparable TestCommon test_python_ref_torch_fallback DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu DecorateInfo unittest expectedFailure TestMathBits test_neg_view PythonRefInfo _refs normal torch_opinfo_name= normal supports_out=True decorators= TODO RuntimeError no _refs support torch rand_like DecorateInfo unittest skip TODO RuntimeError no _refs support torch rand_like TestCommon test_python_ref AssertionError Tensor-likes close DecorateInfo unittest skip Expected normal comparable TestCommon test_out DecorateInfo unittest skip Expected normal comparable TestCommon test_out_warning DecorateInfo unittest skip Expected normal comparable TestCommon test_python_ref_torch_fallback DecorateInfo unittest skip Expected normal comparable TestDecomp test_comprehensive DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu DecorateInfo unittest skip make_traced doesn t set seed properly TestCommon test_python_ref_executor DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view PythonRefInfo _refs normal torch_opinfo_name= normal torch_opinfo_variant_name= number_mean supports_out=True decorators= TODO RuntimeError no _refs support torch rand_like DecorateInfo unittest skip TODO RuntimeError no _refs support torch rand_like TestCommon test_python_ref AssertionError Tensor-likes close DecorateInfo unittest skip Expected normal comparable TestCommon test_out DecorateInfo unittest skip Expected normal comparable TestCommon test_out_warning DecorateInfo unittest skip Expected normal comparable TestCommon test_python_ref_torch_fallback DecorateInfo unittest skip Expected normal comparable TestDecomp test_comprehensive DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu DecorateInfo unittest skip make_traced doesn t set seed properly TestCommon test_python_ref_executor DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view PythonRefInfo _refs normal_ op=torch Tensor normal_ torch_opinfo_name= normal torch_opinfo_variant_name= in_place supports_out=False decorators= TODO RuntimeError no _refs support torch rand_like DecorateInfo unittest skip TODO RuntimeError no _refs support torch rand_like TestCommon test_python_ref AssertionError Tensor-likes close DecorateInfo unittest skip Expected normal comparable TestCommon test_out DecorateInfo unittest skip Expected normal comparable TestCommon test_out_warning DecorateInfo unittest skip Expected normal comparable TestCommon test_python_ref_torch_fallback DecorateInfo unittest skip Expected normal comparable TestDecomp test_comprehensive DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu DecorateInfo unittest skip make_traced doesn t set seed properly TestCommon test_python_ref_executor DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view PythonRefInfo _refs arange torch_opinfo_name= arange skips= Tests assume input tensor sequence tensors DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view PythonRefInfo _refs linspace torch_opinfo_name= linspace skips= Tests assume input tensor sequence tensors DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view cpu implementation wrong some integral types https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestCommon test_python_ref_torch_fallback dtypes= torch int torch uint torch int torch int torch int device_type= cpu DecorateInfo unittest expectedFailure TestCommon test_python_ref dtypes= torch int torch uint torch int torch int torch int device_type= cpu cuda implementation off-by-one some inputs due precision issues https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestCommon test_python_ref_torch_fallback dtypes= torch uint torch int torch int torch int torch int device_type= cuda DecorateInfo unittest expectedFailure TestCommon test_python_ref dtypes= torch uint torch int torch int torch int torch int device_type= cuda DecorateInfo unittest expectedFailure TestCommon test_python_ref_executor dtypes= torch uint torch int torch int torch int torch int device_type= cuda PythonRefInfo _refs linspace torch_opinfo_name= linspace torch_opinfo_variant_name= tensor_overload skips= TypeError int object subscriptable DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view cpu implementation wrong some integral types https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestCommon test_python_ref_torch_fallback dtypes= torch int torch uint torch int torch int torch int device_type= cpu DecorateInfo unittest expectedFailure TestCommon test_python_ref dtypes= torch int torch uint torch int torch int torch int device_type= cpu cuda implementation off-by-one some inputs due precision issues https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestCommon test_python_ref_torch_fallback dtypes= torch uint torch int torch int torch int torch int device_type= cuda DecorateInfo unittest expectedFailure TestCommon test_python_ref dtypes= torch uint torch int torch int torch int torch int device_type= cuda TODO torch ops aten copy _refs DecorateInfo unittest expectedFailure TestCommon test_python_ref dtypes= torch float torch float torch float torch complex torch complex torch bfloat device_type= cuda DecorateInfo unittest expectedFailure TestCommon test_python_ref dtypes= torch float torch float torch float torch complex torch complex torch bfloat device_type= cpu DecorateInfo unittest expectedFailure TestCommon test_python_ref_executor dtypes= torch uint torch int torch int torch int torch int device_type= cuda PythonRefInfo _refs logspace torch_opinfo_name= logspace skips= Tests assume input tensor sequence tensors DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view DecorateInfo unittest expectedFailure TestMathBits test_neg_conj_view Off-by-one issue when casting floats ints DecorateInfo unittest expectedFailure TestCommon test_python_ref_torch_fallback dtypes= torch int torch int torch int device_type= cuda DecorateInfo unittest expectedFailure TestCommon test_python_ref dtypes= torch int torch int torch int device_type= cuda DecorateInfo unittest expectedFailure TestCommon test_python_ref_executor dtypes= torch int torch int torch int device_type= cuda PythonRefInfo _refs logspace torch_opinfo_name= logspace torch_opinfo_variant_name= tensor_overload skips= TypeError int object subscriptable DecorateInfo unittest expectedFailure TestMathBits test_neg_view DecorateInfo unittest expectedFailure TestMathBits test_conj_view Off-by-one issue when casting floats ints DecorateInfo unittest expectedFailure TestCommon test_python_ref_torch_fallback dtypes= torch int torch int torch int device_type= cuda DecorateInfo unittest expectedFailure TestCommon test_python_ref dtypes= torch int torch int torch int device_type= cuda DecorateInfo unittest expectedFailure TestCommon test_python_ref_executor dtypes= torch int torch int torch int device_type= cuda TODO copy doesn t have prim refs DecorateInfo unittest expectedFailure TestCommon test_python_ref dtypes= torch float torch float torch float torch complex torch complex torch bfloat torch int torch uint device_type= cuda DecorateInfo unittest expectedFailure TestCommon test_python_ref dtypes= torch float torch float torch float torch complex torch complex torch bfloat torch int torch int torch int torch int torch uint device_type= cpu PythonRefInfo _refs meshgrid torch_opinfo_name= meshgrid torch_opinfo_variant_name= variadic_tensors PythonRefInfo _refs take_along_dim torch_opinfo_name= take_along_dim skips= DecorateInfo unittest expectedFailure TestCommon test_python_ref PythonRefInfo _refs torch_opinfo_name= PythonRefInfo _refs triu torch_opinfo_name= triu PythonRefInfo _refs tril torch_opinfo_name= tril PythonRefInfo _refs triu_indices torch_opinfo_name= triu_indices implementation uses torch stack violates view consistency validate_view_consistency=False skips= skip these tests since we have non tensor input DecorateInfo unittest skip Skipped TestCommon test_noncontiguous_samples DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit DecorateInfo unittest skip Skipped TestMathBits test_neg_view PythonRefInfo _refs tril_indices torch_opinfo_name= tril_indices implementation uses torch stack violates view consistency validate_view_consistency=False skips= skip these tests since we have non tensor input DecorateInfo unittest skip Skipped TestCommon test_noncontiguous_samples DecorateInfo unittest skip Skipped TestCommon test_variant_consistency_eager DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit DecorateInfo unittest skip Skipped TestMathBits test_neg_view PythonRefInfo _refs meshgrid torch_opinfo_name= meshgrid torch_opinfo_variant_name= list_of_tensors PythonRefInfo _refs movedim aliases= moveaxis torch_opinfo_name= movedim PythonRefInfo _refs bucketize torch_opinfo_name= bucketize skips= RuntimeError It appears you re trying get value out tracing tensor aten _local_scalar_dense default - erroring out triggered mid_val = boundaries mid DecorateInfo unittest expectedFailure TestCommon test_python_ref_executor PythonRefInfo _refs equal torch_opinfo_name= equal skips= RuntimeError Cannot cast FakeTensor number DecorateInfo unittest expectedFailure TestCommon test_python_ref_meta ElementwiseUnaryPythonRefInfo _refs atan torch_opinfo_name= atan decorators= precisionOverride torch bfloat e- skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_small device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda dtypes= torch cfloat torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cuda dtypes= torch cfloat torch cdouble active_if=IS_WINDOWS ElementwiseUnaryPythonRefInfo _refs atanh torch_opinfo_name= atanh decorators= precisionOverride torch bfloat e- skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_small device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda dtypes= torch cfloat torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cuda dtypes= torch cfloat active_if=IS_WINDOWS ElementwiseUnaryPythonRefInfo _refs bitwise_not torch_opinfo_name= bitwise_not ElementwiseUnaryPythonRefInfo _refs ceil torch_opinfo_name= ceil Fails int https github com pytorch pytorch issues PythonRefInfo _refs item torch_opinfo_name= item skips= RuntimeError Cannot cast FakeTensor FakeTensor device= meta size= cpu number DecorateInfo unittest expectedFailure TestCommon test_python_ref_meta ValueError Can t convert tensor elements number DecorateInfo unittest expectedFailure TestCommon test_python_ref_errors ElementwiseUnaryPythonRefInfo _refs conj_physical torch_opinfo_name= conj_physical ElementwiseUnaryPythonRefInfo _refs cos torch_opinfo_name= cos decorators= precisionOverride torch bfloat e- skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch cfloat torch cdouble device_type= cpu active_if=IS_WINDOWS This fails CUDA passes ROCm DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch cdouble device_type= cuda DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch cfloat torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble active_if=IS_MACOS AssertionError Tensor-likes close Greatest absolute difference nan index up e- allowed Greatest relative difference nan index up allowed DecorateInfo unittest expectedFailure TestUnaryUfuncs test_reference_numerics_large device_type= cuda dtypes= torch chalf active_if=IS_WINDOWS ElementwiseUnaryPythonRefInfo _refs cosh torch_opinfo_name= cosh skips= Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cpu dtypes= torch int DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch cfloat torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch cfloat torch cdouble active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble active_if=IS_MACOS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cpu dtypes= torch cfloat torch cdouble active_if=IS_MACOS AssertionError Tensor-likes close Greatest absolute difference nan index up e- allowed Greatest relative difference nan index up allowed DecorateInfo unittest expectedFailure TestUnaryUfuncs test_reference_numerics_large device_type= cuda dtypes= torch chalf active_if=IS_WINDOWS ElementwiseUnaryPythonRefInfo _refs digamma torch_opinfo_name= digamma ElementwiseUnaryPythonRefInfo _refs erf torch_opinfo_name= erf ElementwiseUnaryPythonRefInfo _refs erfinv torch_opinfo_name= erfinv decorators= precisionOverride torch float e- torch bfloat e- torch float e- skips= Reference https github com pytorch pytorch pull #issuecomment- DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal active_if=TEST_SCIPY version parse scipy __version__ version parse DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large active_if=TEST_SCIPY version parse scipy __version__ version parse DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_small active_if=TEST_SCIPY version parse scipy __version__ version parse ElementwiseUnaryPythonRefInfo _refs erfc torch_opinfo_name= erfc ElementwiseUnaryPythonRefInfo _refs exp torch_opinfo_name= exp skips= Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble active_if=IS_WINDOWS ElementwiseUnaryPythonRefInfo _refs expm torch_opinfo_name= expm ElementwiseUnaryPythonRefInfo _refs exp torch_opinfo_name= exp skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch cdouble Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble active_if=IS_WINDOWS ElementwiseUnaryPythonRefInfo _refs fill torch_opinfo_name= fill supports_out=True ElementwiseUnaryPythonRefInfo _refs floor torch_opinfo_name= floor Fails int https github com pytorch pytorch issues ElementwiseUnaryPythonRefInfo _refs frexp torch_opinfo_name= frexp Skipped due numerical failures Windows CI This also skipped frexp earlier file skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal active_if=IS_WINDOWS ElementwiseUnaryPythonRefInfo _refs frac torch_opinfo_name= frac skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch bfloat torch float torch float torch float ElementwiseUnaryPythonRefInfo _refs imag torch_opinfo_name= imag ElementwiseUnaryPythonRefInfo _refs isfinite torch_opinfo_name= isfinite supports_out=True ElementwiseUnaryPythonRefInfo _refs isinf torch_opinfo_name= isinf supports_out=True ElementwiseUnaryPythonRefInfo _refs isposinf torch_opinfo_name= isposinf supports_out=True ElementwiseUnaryPythonRefInfo _refs isneginf torch_opinfo_name= isneginf supports_out=True ElementwiseUnaryPythonRefInfo _refs isnan torch_opinfo_name= isnan supports_out=True ElementwiseUnaryPythonRefInfo _refs isreal torch_opinfo_name= isreal supports_out=True ElementwiseUnaryPythonRefInfo _refs i torch_opinfo_name= i decorators= precisionOverride torch bfloat e- torch float e- skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch int ElementwiseUnaryPythonRefInfo _refs lgamma torch_opinfo_name= lgamma decorators= precisionOverride torch float e- skips= Reference https github com pytorch pytorch pull #issuecomment- DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch float torch float active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch float torch float active_if=IS_WINDOWS ElementwiseUnaryPythonRefInfo _refs special multigammaln torch_opinfo_name= mvlgamma torch_opinfo_variant_name= mvlgamma_p_ skips=skips_mvlgamma decorators= DecorateInfo torch testing _internal common_utils markDynamoStrictTest TestUnaryUfuncs test_reference_numerics_large DecorateInfo torch testing _internal common_utils xfailIfTorchDynamo TestUnaryUfuncs test_reference_numerics_large ElementwiseUnaryPythonRefInfo _refs special multigammaln torch_opinfo_name= mvlgamma torch_opinfo_variant_name= mvlgamma_p_ skips=skips_mvlgamma ElementwiseUnaryPythonRefInfo _refs special multigammaln torch_opinfo_name= mvlgamma torch_opinfo_variant_name= mvlgamma_p_ skips=skips_mvlgamma ElementwiseUnaryPythonRefInfo _refs log torch_opinfo_name= log decorators= precisionOverride torch bfloat e- skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble active_if=IS_WINDOWS ElementwiseUnaryPythonRefInfo _refs log p torch_opinfo_name= log p ElementwiseUnaryPythonRefInfo _refs log torch_opinfo_name= log decorators= precisionOverride torch bfloat e- skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble active_if=IS_WINDOWS ElementwiseUnaryPythonRefInfo _refs log torch_opinfo_name= log decorators= precisionOverride torch bfloat e- skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch cfloat torch cdouble PythonRefInfo _refs logsumexp torch_opinfo_name= logsumexp When keepdim=False logsumexp function uses squeeze operation yet exposed nvFuser s Python API PythonRefInfo _refs log_softmax torch_opinfo_name= log_softmax torch_opinfo_variant_name= with_dtype ElementwiseUnaryPythonRefInfo _refs nan_to_num torch_opinfo_name= nan_to_num ElementwiseUnaryPythonRefInfo _refs neg torch_opinfo_name= neg ElementwiseUnaryPythonRefInfo _refs positive torch_opinfo_name= positive ElementwiseUnaryPythonRefInfo _refs real torch_opinfo_name= real ElementwiseUnaryPythonRefInfo _refs reciprocal torch_opinfo_name= reciprocal skips= Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch cfloat torch cdouble ElementwiseUnaryPythonRefInfo _refs round torch_opinfo_name= round Fails int https github com pytorch pytorch issues skips= DecorateInfo toleranceOverride torch bfloat tol atol= e- rtol= TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda DecorateInfo toleranceOverride torch bfloat tol atol= e- rtol= TestUnaryUfuncs test_reference_numerics_normal device_type= cuda ElementwiseUnaryPythonRefInfo _refs rsqrt torch_opinfo_name= rsqrt decorators= precisionOverride torch half e- skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch cfloat torch cdouble AssertionError Tensor-likes close Greatest absolute difference nan index up allowed Greatest relative difference nan index up allowed DecorateInfo unittest expectedFailure TestUnaryUfuncs test_reference_numerics_large dtypes= torch chalf ElementwiseUnaryPythonRefInfo _refs sigmoid torch_opinfo_name= sigmoid aliases= _refs special expit Reference https github com pytorch pytorch issues handles_complex_extremal_values=False handles_large_floats=False decorators= precisionOverride torch float e- torch complex e- torch bfloat e- skips= Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch complex torch cdouble device_type= cuda DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch chalf torch complex torch cdouble device_type= cuda ElementwiseUnaryPythonRefInfo _refs sign torch_opinfo_name= sign skips= Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch bfloat torch float torch float torch float ElementwiseUnaryPythonRefInfo _refs sgn torch_opinfo_name= sgn This issue vectorised abs CPU handles_complex_extremal_values=False handles_large_floats=False skips= Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch bfloat torch float torch float torch float ElementwiseUnaryPythonRefInfo _refs signbit torch_opinfo_name= signbit ElementwiseUnaryPythonRefInfo _refs sin torch_opinfo_name= sin decorators= precisionOverride torch bfloat e- skips= Fails CUDA passes ROCm DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch cdouble device_type= cuda DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch cfloat torch cdouble device_type= cpu active_if=IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch cfloat torch cdouble device_type= cpu active_if=IS_WINDOWS ElementwiseUnaryPythonRefInfo _refs sinc torch_opinfo_name= sinc decorators= precisionOverride torch bfloat e- torch float e- skips= Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_small dtypes= torch cfloat ElementwiseUnaryPythonRefInfo _refs sinh torch_opinfo_name= sinh decorators= precisionOverride torch float e- skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble active_if= IS_MACOS IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cpu dtypes= torch cfloat torch cdouble active_if= IS_MACOS IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch cdouble Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cpu dtypes= torch int PythonRefInfo _refs softmax torch_opinfo_name= softmax torch_opinfo_variant_name= with_dtype ElementwiseUnaryPythonRefInfo _refs sqrt torch_opinfo_name= sqrt decorators= precisionOverride torch bfloat e- DecorateInfo toleranceOverride torch chalf tol atol= e- rtol= TestUnaryUfuncs test_reference_numerics_large skips= Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cpu dtypes= torch cfloat torch cdouble active_if=IS_MACOS Reference https github com pytorch pytorch pull #issuecomment- DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch bfloat ElementwiseUnaryPythonRefInfo _refs square torch_opinfo_name= square decorators= precisionOverride torch complex e- torch bfloat e- skips= AssertionError Reference result farther e- precise computation DecorateInfo unittest skip Skipped TestCommon test_python_ref_executor dtypes= torch complex Reference https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large dtypes= torch cfloat torch cdouble DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda dtypes= torch cfloat torch cdouble ElementwiseUnaryPythonRefInfo _refs tan torch_opinfo_name= tan decorators= DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble active_if= IS_MACOS IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cpu dtypes= torch cfloat torch cdouble active_if= IS_MACOS IS_WINDOWS ElementwiseUnaryPythonRefInfo _refs tanh torch_opinfo_name= tanh decorators= DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda skips= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_extremal device_type= cpu dtypes= torch cfloat torch cdouble active_if= IS_MACOS IS_WINDOWS DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_large device_type= cpu dtypes= torch cfloat torch cdouble active_if= IS_MACOS IS_WINDOWS ElementwiseUnaryPythonRefInfo _refs trunc torch_opinfo_name= trunc Fails int https github com pytorch pytorch issues PythonRefInfo _refs special log_softmax torch_opinfo_name= log_softmax alias torch_opinfo_variant_name= with_dtype supports_out=False PythonRefInfo _refs special softmax torch_opinfo_name= softmax alias torch_opinfo_variant_name= with_dtype supports_out=False Elementwise Unary Special OpInfos ElementwiseUnaryPythonRefInfo _refs special logit torch_opinfo_name= logit Elementwise Unary nn functional OpInfos PythonRefInfo _refs nn functional alpha_dropout torch_opinfo_name= nn functional alpha_dropout decorators= DecorateInfo unittest skip Expected dropout comparable TestCommon test_python_ref AssertionError Tensor-likes close DecorateInfo unittest skip Expected dropout comparable TestCommon test_python_ref_torch_fallback DecorateInfo unittest skip Expected dropout comparable TestCommon test_python_ref_executor device_type= cuda AssertionError Tensor-likes close DecorateInfo unittest skip Expected dropout comparable TestMathBits test_neg_view AssertionError Tensor-likes close DecorateInfo unittest skip Expected dropout comparable TestCommon test_compare_cpu ElementwiseUnaryPythonRefInfo _refs nn functional celu torch_opinfo_name= nn functional celu supports_out=True PythonRefInfo _refs nn functional channel_shuffle torch_opinfo_name= nn functional channel_shuffle supports_out=True ElementwiseUnaryPythonRefInfo _refs nn functional threshold torch_opinfo_name= nn functional threshold supports_out=True PythonRefInfo _refs nn functional dropout torch_opinfo_name= nn functional dropout decorators= DecorateInfo unittest skip Expected dropout comparable TestCommon test_python_ref DecorateInfo unittest skip Expected dropout comparable TestCommon test_python_ref_torch_fallback DecorateInfo unittest skip Expected dropout comparable TestCommon test_out DecorateInfo unittest skip Expected dropout comparable TestCommon test_out_warning DecorateInfo unittest skip Expected dropout comparable TestMathBits test_conj_view DecorateInfo unittest skip Expected dropout comparable TestMathBits test_neg_conj_view DecorateInfo unittest skip Expected dropout comparable TestMathBits test_neg_view dropout comparable DecorateInfo unittest expectedFailure TestCommon test_python_ref_executor DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu ElementwiseUnaryPythonRefInfo _refs nn functional elu torch_opinfo_name= nn functional elu supports_out=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch bfloat tol atol= e- rtol= e- TestUnaryUfuncs device_type= cuda ElementwiseUnaryPythonRefInfo _refs nn functional hardtanh torch_opinfo_name= nn functional hardtanh supports_out=True PythonRefInfo TODO Port UnaryOpInfo _refs nn functional gelu torch_opinfo_name= nn functional gelu PythonRefInfo _refs nn functional layer_norm torch_opinfo_name= nn functional layer_norm skips= Reference result farther e- precise computation than torch result e- DecorateInfo unittest skip Skipped TestCommon test_python_ref dtypes= torch float device_type= cpu PythonRefInfo _refs nn functional glu torch_opinfo_name= nn functional glu supports_out=True PythonRefInfo _refs nn functional pairwise_distance torch_opinfo_name= nn functional pairwise_distance supports_out=True PythonRefInfo _refs nn functional pdist torch_opinfo_name= nn functional pdist supports_out=True skips= RunTimeError no _refs support torch Tensor index_select DecorateInfo unittest expectedFailure TestCommon test_python_ref Reference result farther e- precise computation than torch result e- DecorateInfo unittest expectedFailure TestCommon test_python_ref_torch_fallback dtypes= torch float device_type= cpu PythonRefInfo _refs nn functional leaky_relu torch_opinfo_name= nn functional leaky_relu supports_out=True PythonRefInfo _refs nn functional log_softmax torch_opinfo_name= log_softmax alias torch_opinfo_variant_name= with_dtype supports_out=False PythonRefInfo _refs nn functional pixel_shuffle torch_opinfo_name= nn functional pixel_shuffle PythonRefInfo _refs nn functional pixel_unshuffle torch_opinfo_name= nn functional pixel_unshuffle PythonRefInfo _refs nn functional poisson_nll_loss torch_opinfo_name= nn functional poisson_nll_loss ElementwiseUnaryPythonRefInfo _refs nn functional prelu torch_opinfo_name= nn functional prelu ElementwiseUnaryPythonRefInfo _refs nn functional relu torch_opinfo_name= nn functional relu supports_out=True ElementwiseUnaryPythonRefInfo _refs nn functional relu torch_opinfo_name= nn functional relu supports_out=True ElementwiseUnaryPythonRefInfo _refs nn functional mish torch_opinfo_name= nn functional mish supports_out=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestUnaryUfuncs ElementwiseUnaryPythonRefInfo _refs nn functional selu torch_opinfo_name= nn functional selu supports_out=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch bfloat tol atol= e- rtol= e- TestUnaryUfuncs device_type= cuda PythonRefInfo _refs nn functional softmax torch_opinfo_name= softmax alias torch_opinfo_variant_name= with_dtype supports_out=False PythonRefInfo _refs nn functional softmin torch_opinfo_name= nn functional softmin torch_opinfo_variant_name= with_dtype supports_out=False ElementwiseUnaryPythonRefInfo _refs nn functional softplus torch_opinfo_name= nn functional softplus PythonRefInfo _refs nn functional l _loss torch_opinfo_name= nn functional l _loss PythonRefInfo _refs nn functional margin_ranking_loss torch_opinfo_name= nn functional margin_ranking_loss PythonRefInfo _refs nn functional mse_loss torch_opinfo_name= nn functional mse_loss PythonRefInfo _refs nn functional smooth_l _loss torch_opinfo_name= nn functional smooth_l _loss PythonRefInfo _refs nn functional hinge_embedding_loss torch_opinfo_name= nn functional hinge_embedding_loss PythonRefInfo _refs nn functional nll_loss torch_opinfo_name= nn functional nll_loss The corresponding PyTorch op doesn t support out But ref registered decomp ATen has out variant supports_out=True For simpler indexing we flatten target indices then reshape result tensor This creates inconsistent view state reference impl validate_view_consistency=False skips= RuntimeError It appears you re trying get value out tracing tensor - erroring out DecorateInfo unittest expectedFailure TestCommon test_python_ref_executor device_type= cuda PythonRefInfo _refs nn functional huber_loss torch_opinfo_name= nn functional huber_loss The corresponding PyTorch op doesn t support out But ref registered decomp ATen has out variant supports_out=True ElementwiseUnaryPythonRefInfo _refs nn functional tanhshrink torch_opinfo_name= nn functional tanhshrink decorators= DecorateInfo unittest skip Skipped TestUnaryUfuncs test_reference_numerics_normal device_type= cpu dtypes= torch cfloat torch cdouble DecorateInfo toleranceOverride torch bfloat tol atol= e- rtol= e- torch complex tol atol= e- rtol= e- TestUnaryUfuncs test_reference_numerics_extremal device_type= cuda skips= each case pytorch will produce nan while numpy will DecorateInfo unittest skip Fails some jobs works others TestUnaryUfuncs test_reference_numerics_large dtypes= torch complex torch complex active_if= IS_MACOS DecorateInfo unittest skip Fails some jobs works others TestUnaryUfuncs test_reference_numerics_extremal dtypes= torch complex torch complex device_type= cpu active_if= IS_MACOS IS_WINDOWS ElementwiseUnaryPythonRefInfo _refs nn functional hardshrink torch_opinfo_name= nn functional hardshrink ElementwiseUnaryPythonRefInfo _refs nn functional softshrink torch_opinfo_name= nn functional softshrink Elementwise Binary Reference OpInfos ElementwiseBinaryPythonRefInfo _refs add torch_opinfo_name= add https github com pytorch pytorch issues supports_two_python_scalars=True supports_one_python_scalar=True decorators= DecorateInfo toleranceOverride torch chalf tol atol= e- rtol= TestBinaryUfuncs test_reference_numerics skips= DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_extremal_values dtypes= torch complex torch complex ElementwiseBinaryPythonRefInfo _refs atan torch_opinfo_name= atan ElementwiseBinaryPythonRefInfo _refs bitwise_and torch_opinfo_name= bitwise_and ElementwiseBinaryPythonRefInfo _refs bitwise_left_shift torch_opinfo_name= bitwise_left_shift skips= https github com pytorch pytorch issues DecorateInfo unittest skip Some inputs produce undefined outputs TestCommon test_compare_cpu ElementwiseBinaryPythonRefInfo _refs bitwise_right_shift torch_opinfo_name= bitwise_right_shift skips= https github com pytorch pytorch issues DecorateInfo unittest skip Skipped some inputs produce undefined outputs TestCommon test_compare_cpu ElementwiseBinaryPythonRefInfo _refs bitwise_or torch_opinfo_name= bitwise_or ElementwiseBinaryPythonRefInfo _refs bitwise_xor torch_opinfo_name= bitwise_xor ElementwiseBinaryPythonRefInfo _refs copysign torch_opinfo_name= copysign skips= RuntimeError Expected divisor b same device cuda dividend found cpu DecorateInfo unittest skip Skipped TestBinaryUfuncs test_type_promotion FIXME output meta disagrees real impl DecorateInfo unittest expectedFailure TestMeta test_binary_ufuncs_mixed_dtype ElementwiseBinaryPythonRefInfo _refs div torch_opinfo_name= div torch_opinfo_variant_name= no_rounding_mode https github com pytorch pytorch issues supports_two_python_scalars=True supports_one_python_scalar=True skips= NotImplementedError argument type complex DecorateInfo unittest skip Skipped TestCommon test_python_ref_executor dtypes= torch complex torch complex torch complex Reference result farther precise computation than torch result nan DecorateInfo unittest expectedFailure TestCommon test_python_ref dtypes= torch complex device_type= cuda Reference result farther precise computation than torch result nan DecorateInfo unittest expectedFailure TestCommon test_python_ref_torch_fallback dtypes= torch complex device_type= cuda ElementwiseBinaryPythonRefInfo _refs div torch_opinfo_name= div torch_opinfo_variant_name= trunc_rounding https github com pytorch pytorch issues supports_two_python_scalars=True supports_one_python_scalar=True decorators= See https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion ElementwiseBinaryPythonRefInfo _refs div torch_opinfo_name= div torch_opinfo_variant_name= floor_rounding https github com pytorch pytorch issues supports_two_python_scalars=True supports_one_python_scalar=True decorators= See https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion Reference result farther nan precise computation than torch result inf DecorateInfo unittest expectedFailure TestCommon test_python_ref dtypes= torch bfloat device_type= cpu active_if=not IS_S X ElementwiseBinaryPythonRefInfo _refs eq torch_opinfo_name= eq ElementwiseBinaryPythonRefInfo _refs float_power torch_opinfo_name= float_power skips= Test doesn t account float - double type promotion DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion Complex values error Greatest absolute difference nan index DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_small_values dtypes= torch complex torch complex DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_large_values dtypes= torch complex torch complex DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_extremal_values dtypes= torch complex torch complex ElementwiseBinaryPythonRefInfo _refs logaddexp torch_opinfo_name= logaddexp skips= failure due mismatch edge cases which boils down what torch exp inf + infj should DecorateInfo unittest expectedFailure TestCommon test_python_ref device_type= cpu dtypes= torch complex torch complex DecorateInfo unittest expectedFailure TestCommon test_python_ref_torch_fallback device_type= cpu dtypes= torch complex torch complex PythonRefInfo _refs logaddexp torch_opinfo_name= logaddexp ElementwiseBinaryPythonRefInfo _refs floor_divide torch_opinfo_name= floor_divide rhs_make_tensor_kwargs=dict exclude_zero=True https github com pytorch pytorch issues supports_two_python_scalars=True supports_one_python_scalar=True bfloat floor_divide compared float reference works inconsistently skips= DecorateInfo unittest skip Skipped TestCommon test_python_ref dtypes= torch bfloat DecorateInfo unittest skip Skipped TestCommon test_python_ref_torch_fallback dtypes= torch bfloat bfloat floor_divide compared float reference works inconsistently DecorateInfo unittest skip Skipped TestBinaryUfuncs dtypes= torch bfloat int floor divide has different results - - vs NumPy DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_small_values dtypes= torch int The following tests fails some jobs DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_extremal_values dtypes= torch float DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestBinaryUfuncs test_reference_numerics FIXME output meta disagrees real impl DecorateInfo unittest expectedFailure TestMeta test_binary_ufuncs_mixed_dtype ElementwiseBinaryPythonRefInfo _refs fmax torch_opinfo_name= fmax supports_rhs_python_scalar=False ElementwiseBinaryPythonRefInfo _refs fmin torch_opinfo_name= fmin supports_rhs_python_scalar=False ElementwiseBinaryPythonRefInfo _refs fmod torch_opinfo_name= fmod rhs_make_tensor_kwargs= exclude_zero True supports_rhs_python_scalar=True skips= DecorateInfo unittest skip Skipped TestCommon test_python_ref dtypes= torch bfloat device_type= cpu DecorateInfo unittest skip Skipped TestCommon test_python_ref_torch_fallback dtypes= torch bfloat device_type= cpu DecorateInfo unittest skip Skipped TestBinaryUfuncs test_contig_vs_every_other dtypes= torch bfloat DecorateInfo unittest skip Skipped TestBinaryUfuncs test_non_contig dtypes= torch bfloat DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics dtypes= torch bfloat DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_small_values dtypes= torch uint ElementwiseBinaryPythonRefInfo _refs gcd torch_opinfo_name= gcd skips= DecorateInfo unittest expectedFailure TestBinaryUfuncs test_reference_numerics_small_values dtypes= torch int ElementwiseBinaryPythonRefInfo _refs ge torch_opinfo_name= ge ElementwiseBinaryPythonRefInfo _refs gt torch_opinfo_name= gt ElementwiseBinaryPythonRefInfo _refs heaviside torch_opinfo_name= heaviside supports_rhs_python_scalar=False skips= PyTorch s heaviside does appear propagate NaNs DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_extremal_values ElementwiseBinaryPythonRefInfo _refs hypot torch_opinfo_name= hypot supports_rhs_python_scalar=False ElementwiseBinaryPythonRefInfo _refs igamma torch_opinfo_name= igamma ElementwiseBinaryPythonRefInfo _refs igammac torch_opinfo_name= igammac ElementwiseBinaryPythonRefInfo _refs isclose torch_opinfo_name= isclose skips= Intentional xfail -- isclose does type promote DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion DecorateInfo unittest expectedFailure TestMeta test_binary_ufuncs_mixed_dtype DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_extremal_values ElementwiseBinaryPythonRefInfo _refs lcm torch_opinfo_name= lcm ElementwiseBinaryPythonRefInfo _refs le torch_opinfo_name= le ElementwiseBinaryPythonRefInfo _refs logical_and torch_opinfo_name= logical_and ElementwiseUnaryPythonRefInfo _refs logical_not torch_opinfo_name= logical_not ElementwiseBinaryPythonRefInfo _refs logical_or torch_opinfo_name= logical_or ElementwiseBinaryPythonRefInfo _refs logical_xor torch_opinfo_name= logical_xor ElementwiseBinaryPythonRefInfo _refs lt torch_opinfo_name= lt ElementwiseBinaryPythonRefInfo _refs maximum torch_opinfo_name= maximum skips= DecorateInfo unittest expectedFailure TestCommon test_python_ref_errors ElementwiseBinaryPythonRefInfo _refs minimum torch_opinfo_name= minimum skips= DecorateInfo unittest expectedFailure TestCommon test_python_ref_errors ElementwiseBinaryPythonRefInfo _refs mul torch_opinfo_name= mul https github com pytorch pytorch issues supports_two_python_scalars=True supports_one_python_scalar=True skips= Reference result farther precise computation than torch result nan DecorateInfo unittest expectedFailure TestCommon test_python_ref_executor dtypes= torch complex Reference result farther precise computation than torch result nan DecorateInfo unittest expectedFailure TestCommon test_python_ref dtypes= torch complex device_type= cuda Reference result farther precise computation than torch result nan DecorateInfo unittest expectedFailure TestCommon test_python_ref_torch_fallback dtypes= torch complex device_type= cuda ElementwiseBinaryPythonRefInfo _refs ne torch_opinfo_name= ne ElementwiseBinaryPythonRefInfo _refs nextafter torch_opinfo_name= nextafter ElementwiseBinaryPythonRefInfo _refs pow torch_opinfo_name= pow decorators= DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- TestBinaryUfuncs test_reference_numerics DecorateInfo toleranceOverride torch complex tol atol= e- rtol= e- torch complex tol atol= e- rtol= e- TestBinaryUfuncs test_scalar_support skips= Reference result farther inf precise computation than torch result nan DecorateInfo unittest expectedFailure TestCommon test_python_ref_executor dtypes= torch complex Reference result farther inf precise computation than torch result nan DecorateInfo unittest expectedFailure TestCommon test_python_ref dtypes= torch complex device_type= cuda Reference result farther inf precise computation than torch result nan DecorateInfo unittest expectedFailure TestCommon test_python_ref_torch_fallback dtypes= torch complex device_type= cuda Skipping integers because they being raised negative powers causing error DecorateInfo unittest expectedFailure TestBinaryUfuncs test_reference_numerics_small_values dtypes= torch int torch int torch int torch int DecorateInfo unittest expectedFailure TestBinaryUfuncs test_reference_numerics_large_values dtypes= torch int torch int torch int DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics dtypes= torch complex DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_small_values dtypes= torch complex torch complex torch complex DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_large_values dtypes= torch complex torch complex torch complex DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_extremal_values dtypes= torch complex torch complex torch complex ElementwiseBinaryPythonRefInfo _refs remainder torch_opinfo_name= remainder skips= DecorateInfo unittest skip Skipped TestCommon test_python_ref dtypes= torch bfloat device_type= cpu DecorateInfo unittest skip Skipped TestCommon test_python_ref_torch_fallback dtypes= torch bfloat device_type= cpu DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics dtypes= torch bfloat DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_small_values dtypes= torch uint ElementwiseBinaryPythonRefInfo _refs rsub torch_opinfo_name= rsub https github com pytorch pytorch issues skips= Reference result farther nan precise computation than torch result nan DecorateInfo unittest expectedFailure TestCommon test_python_ref dtypes= torch chalf device_type= cpu Reference result farther nan precise computation than torch result nan DecorateInfo unittest expectedFailure TestCommon test_python_ref_torch_fallback dtypes= torch chalf device_type= cpu ElementwiseBinaryPythonRefInfo _refs sub torch_opinfo_name= sub https github com pytorch pytorch issues supports_two_python_scalars=True supports_one_python_scalar=True decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= torch bfloat tol atol= e- rtol= e- torch complex tol atol= e- rtol= e- TestBinaryUfuncs test_reference_numerics DecorateInfo toleranceOverride torch chalf tol atol= e- rtol= TestCommon test_complex_half_reference_testing device_type= cpu DecorateInfo toleranceOverride torch chalf tol atol= e- rtol= TestDecomp test_comprehensive device_type= cpu DecorateInfo toleranceOverride torch chalf tol atol= e- rtol= TestDecomp test_quick device_type= cpu skips= DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics dtypes= torch uint DecorateInfo unittest skip Skipped TestBinaryUfuncs test_reference_numerics_small_values dtypes= torch uint ElementwiseBinaryPythonRefInfo _refs true_divide torch_opinfo_name= true_divide https github com pytorch pytorch issues supports_two_python_scalars=True supports_one_python_scalar=True skips= Reference result farther precise computation than torch result nan DecorateInfo unittest expectedFailure TestCommon test_python_ref_executor dtypes= torch complex Reference result farther precise computation than torch result nan DecorateInfo unittest expectedFailure TestCommon test_python_ref dtypes= torch complex device_type= cuda Reference result farther precise computation than torch result nan DecorateInfo unittest expectedFailure TestCommon test_python_ref_torch_fallback dtypes= torch complex device_type= cuda Elementwise Ternary Reference OpInfos PythonRefInfo _refs addcdiv torch_opinfo_name= addcdiv PythonRefInfo _refs addcmul torch_opinfo_name= addcmul skips= Reference result farther e- precise computation than torch result e- FIXME enable dtype-based tolerances test_ops py TestCommon _ref_test_helper DecorateInfo unittest skip Skipped TestCommon test_python_ref dtypes= torch float device_type= cpu DecorateInfo unittest skip Skipped TestCommon test_python_ref_torch_fallback dtypes= torch float device_type= cpu ElementwiseBinaryPythonRefInfo _refs clamp_min torch_opinfo_name= clamp_min skips= test error disabled since rhs non-tensor python scalar supported DecorateInfo unittest expectedFailure TestCommon test_python_ref_errors ElementwiseBinaryPythonRefInfo _refs clamp_max torch_opinfo_name= clamp_max skips= test error disabled since rhs non-tensor python scalar supported DecorateInfo unittest expectedFailure TestCommon test_python_ref_errors PythonRefInfo _refs clamp torch_opinfo_name= clamp PythonRefInfo _refs nn functional triplet_margin_loss torch_opinfo_name= nn functional triplet_margin_loss supports_out=False TODO Uses minimum clamp skips= AssertionError Tensor-likes close Greatest absolute difference e- index up e- allowed Greatest relative difference e- index up e- allowed DecorateInfo unittest skip Skipped TestCommon test_python_ref dtypes= torch uint device_type= cpu ElementwiseBinaryPythonRefInfo _refs xlogy torch_opinfo_name= xlogy supports_one_python_scalar=True Elementwise Binary Special OpInfos ElementwiseBinaryPythonRefInfo _refs special xlog py torch_opinfo_name= special xlog py supports_one_python_scalar=True Data Conversion Data Movement Opinfos ElementwiseUnaryPythonRefInfo _refs _conversions bfloat torch_opinfo_name= bfloat TODO If already has correct dtype device then returned ignoring memory_format https github com pytorch pytorch issues validate_view_consistency=False ElementwiseUnaryPythonRefInfo _refs _conversions bool torch_opinfo_name= bool TODO If already has correct dtype device then returned ignoring memory_format https github com pytorch pytorch issues validate_view_consistency=False ElementwiseUnaryPythonRefInfo _refs _conversions byte torch_opinfo_name= byte TODO If already has correct dtype device then returned ignoring memory_format https github com pytorch pytorch issues validate_view_consistency=False skips= DecorateInfo unittest skip Overflow when downcasting signed type undefined TestCommon test_compare_cpu ElementwiseUnaryPythonRefInfo _refs _conversions char torch_opinfo_name= char TODO If already has correct dtype device then returned ignoring memory_format https github com pytorch pytorch issues validate_view_consistency=False skips= DecorateInfo unittest skip Overflow when downcasting signed type undefined TestCommon test_compare_cpu ElementwiseBinaryPythonRefInfo _refs _conversions complex torch_opinfo_name= complex error_inputs_func=partial error_inputs_complex is_ref=True skips= Tests don t account complex s type promotion semantics DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion DecorateInfo unittest expectedFailure TestMeta test_binary_ufuncs_mixed_dtype ElementwiseBinaryPythonRefInfo _refs _conversions polar torch_opinfo_name= polar skips= Tests don t account complex s type promotion semantics DecorateInfo unittest expectedFailure TestBinaryUfuncs test_type_promotion DecorateInfo unittest expectedFailure TestMeta test_binary_ufuncs_mixed_dtype ElementwiseUnaryPythonRefInfo _refs _conversions double torch_opinfo_name= double TODO If already has correct dtype device then returned ignoring memory_format https github com pytorch pytorch issues validate_view_consistency=False ElementwiseUnaryPythonRefInfo _refs _conversions float torch_opinfo_name= float TODO If already has correct dtype device then returned ignoring memory_format https github com pytorch pytorch issues validate_view_consistency=False ElementwiseUnaryPythonRefInfo _refs _conversions half torch_opinfo_name= half TODO If already has correct dtype device then returned ignoring memory_format https github com pytorch pytorch issues validate_view_consistency=False ElementwiseUnaryPythonRefInfo _refs _conversions int torch_opinfo_name= int TODO If already has correct dtype device then returned ignoring memory_format https github com pytorch pytorch issues validate_view_consistency=False skips= DecorateInfo unittest skip Overflow when downcasting signed type undefined TestCommon test_compare_cpu ElementwiseUnaryPythonRefInfo _refs _conversions long torch_opinfo_name= long TODO If already has correct dtype device then returned ignoring memory_format https github com pytorch pytorch issues validate_view_consistency=False skips= DecorateInfo unittest skip Overflow when downcasting signed type undefined TestCommon test_compare_cpu ElementwiseUnaryPythonRefInfo _refs _conversions short torch_opinfo_name= short TODO If already has correct dtype device then returned ignoring memory_format https github com pytorch pytorch issues validate_view_consistency=False skips= DecorateInfo unittest skip Overflow when downcasting signed type undefined TestCommon test_compare_cpu ElementwiseUnaryPythonRefInfo _refs _conversions chalf torch_opinfo_name= chalf TODO If already has correct dtype device then returned ignoring memory_format https github com pytorch pytorch issues validate_view_consistency=False ElementwiseUnaryPythonRefInfo _refs _conversions cfloat torch_opinfo_name= cfloat TODO If already has correct dtype device then returned ignoring memory_format https github com pytorch pytorch issues validate_view_consistency=False ElementwiseUnaryPythonRefInfo _refs _conversions cdouble torch_opinfo_name= cdouble TODO If already has correct dtype device then returned ignoring memory_format https github com pytorch pytorch issues validate_view_consistency=False PythonRefInfo _refs clone torch_opinfo_name= clone View Shape OpInfos PythonRefInfo _refs alias_copy torch_opinfo_name= alias_copy supports_out=True PythonRefInfo _refs atleast_ d torch_opinfo_name= atleast_ d validate_view_consistency=False PythonRefInfo _refs atleast_ d torch_opinfo_name= atleast_ d validate_view_consistency=False PythonRefInfo _refs atleast_ d torch_opinfo_name= atleast_ d validate_view_consistency=False PythonRefInfo _refs as_strided torch_opinfo_name= as_strided FIXME doesn t support chalf dtypes=all_types_and_complex_and torch bool torch float torch bfloat skips= cloned_mutable_input is_same returned_output INTERNAL ASSERT FAILED DecorateInfo unittest skip Errors when storage_offset included TestMathBits test_neg_view DecorateInfo unittest skip Errors when storage_offset included TestMathBits test_conj_view DecorateInfo unittest skip Errors when storage_offset included TestMathBits test_neg_conj_view PythonRefInfo _refs as_strided_copy torch_opinfo_name= as_strided_copy supports_out=True FIXME doesn t support chalf dtypes=all_types_and_complex_and torch bool torch float torch bfloat skips= cloned_mutable_input is_same returned_output INTERNAL ASSERT FAILED DecorateInfo unittest skip Errors when storage_offset included TestMathBits test_neg_view DecorateInfo unittest skip Errors when storage_offset included TestMathBits test_conj_view DecorateInfo unittest skip Errors when storage_offset included TestMathBits test_neg_conj_view The view function decompose into does have ref DecorateInfo unittest expectedFailure TestCommon test_python_ref PythonRefInfo _refs as_strided torch_opinfo_name= as_strided torch_opinfo_variant_name= partial_views FIXME doesn t support chalf dtypes=all_types_and_complex_and torch bool torch float torch bfloat skips= cloned_mutable_input is_same returned_output INTERNAL ASSERT FAILED DecorateInfo unittest skip Errors when storage_offset included TestMathBits test_neg_view DecorateInfo unittest skip Errors when storage_offset included TestMathBits test_conj_view DecorateInfo unittest skip Errors when storage_offset included TestMathBits test_neg_conj_view DecorateInfo unittest expectedFailure TestCommon test_compare_cpu PythonRefInfo _refs as_strided_scatter torch_opinfo_name= as_strided_scatter returns view intermediate tensor as_strided validate_view_consistency=False PythonRefInfo _refs block_diag torch_opinfo_name= block_diag PythonRefInfo _refs broadcast_shapes torch_opinfo_name= broadcast_shapes PythonRefInfo _refs broadcast_tensors torch_opinfo_name= broadcast_tensors PythonRefInfo _refs broadcast_to torch_opinfo_name= broadcast_to PythonRefInfo _refs cat torch_opinfo_name= cat skips= FIXME AssertionError RuntimeError raised DecorateInfo unittest expectedFailure TestCommon test_python_ref_errors PythonRefInfo _refs chunk torch_opinfo_name= chunk PythonRefInfo _refs column_stack torch_opinfo_name= column_stack ElementwiseUnaryPythonRefInfo _refs conj torch_opinfo_name= conj PythonRefInfo _refs constant_pad_nd torch_opinfo_name= constant_pad_nd PythonRefInfo _refs contiguous torch_opinfo_name= contiguous ElementwiseUnaryPythonRefInfo _refs deg rad torch_opinfo_name= deg rad decorators= precisionOverride torch bfloat e- torch float e- PythonRefInfo _refs dsplit torch_opinfo_name= dsplit PythonRefInfo _refs diag torch_opinfo_name= diag PythonRefInfo _refs diagonal torch_opinfo_name= diagonal PythonRefInfo _refs diagonal_copy torch_opinfo_name= diagonal_copy supports_out=True PythonRefInfo _refs diagonal_scatter torch_opinfo_name= diagonal_scatter supports_out=True returns view intermediate tensor as_strided validate_view_consistency=False PythonRefInfo _refs diag_embed torch_opinfo_name= diag_embed supports_out=True PythonRefInfo _refs dstack torch_opinfo_name= dstack skips= DecorateInfo unittest expectedFailure TestCommon test_python_ref_errors PythonRefInfo _refs expand torch_opinfo_name= expand PythonRefInfo _refs expand_as torch_opinfo_name= expand_as PythonRefInfo _refs expand_copy torch_opinfo_name= expand_copy supports_out=True PythonRefInfo _refs flatten torch_opinfo_name= flatten PythonRefInfo _refs flip torch_opinfo_name= flip PythonRefInfo _refs fliplr torch_opinfo_name= fliplr PythonRefInfo _refs flipud torch_opinfo_name= flipud PythonRefInfo _refs hstack torch_opinfo_name= hstack skips= https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestCommon test_python_ref_errors PythonRefInfo _refs narrow torch_opinfo_name= narrow error_inputs_func=partial error_inputs_narrow_narrow_copy is_narrow=True is_ref=True PythonRefInfo _refs narrow_copy torch_opinfo_name= narrow_copy supports_out=True error_inputs_func=partial error_inputs_narrow_narrow_copy is_narrow=False is_ref=True skips= The view function decompose into does have ref DecorateInfo unittest expectedFailure TestCommon test_python_ref PythonRefInfo _refs nn functional group_norm torch_opinfo_name= nn functional group_norm validate_view_consistency=False PythonRefInfo _refs native_layer_norm torch_opinfo_name= native_layer_norm skips= DecorateInfo unittest skip Skipped TestCommon test_python_ref device_type= cpu dtypes= torch float DecorateInfo unittest skip Skipped TestCommon test_python_ref_torch_fallback device_type= cpu dtypes= torch float PythonRefInfo _refs permute torch_opinfo_name= permute PythonRefInfo _refs permute_copy torch_opinfo_name= permute_copy supports_out=True ElementwiseUnaryPythonRefInfo _refs rad deg torch_opinfo_name= rad deg decorators= precisionOverride torch bfloat e- torch float e- PythonRefInfo _refs ravel torch_opinfo_name= ravel PythonRefInfo _refs renorm torch_opinfo_name= renorm PythonRefInfo _refs repeat torch_opinfo_name= repeat validate_view_consistency=False PythonRefInfo _refs reshape torch_opinfo_name= reshape PythonRefInfo _refs reshape_as torch_opinfo_name= reshape_as PythonRefInfo _refs roll torch_opinfo_name= roll validate_view_consistency=False PythonRefInfo _refs rot torch_opinfo_name= rot validate_view_consistency=False PythonRefInfo _refs select_scatter torch_opinfo_name= select_scatter PythonRefInfo _refs stack torch_opinfo_name= stack validate_view_consistency=False PythonRefInfo _refs squeeze torch_opinfo_name= squeeze PythonRefInfo _refs squeeze_copy torch_opinfo_name= squeeze_copy supports_out=True PythonRefInfo _refs squeeze torch_opinfo_name= squeeze torch_opinfo_variant_name= multiple PythonRefInfo _refs tensor_split torch_opinfo_name= tensor_split skips= RuntimeError no _refs support torch Tensor tolist DecorateInfo unittest expectedFailure TestCommon test_python_ref PythonRefInfo _refs hsplit torch_opinfo_name= hsplit PythonRefInfo _refs vsplit torch_opinfo_name= vsplit PythonRefInfo _refs dot torch_opinfo_name= dot error_inputs_func=partial error_inputs_dot_vdot is_ref=True conj does set _is_view correctly ATen validate_view_consistency=False skips= RuntimeError no _refs support torch Tensor is_conj DecorateInfo unittest expectedFailure TestCommon test_python_ref dtypes= torch complex torch complex PythonRefInfo _refs vdot torch_opinfo_name= vdot error_inputs_func=partial error_inputs_dot_vdot is_ref=True conj does set _is_view correctly ATen validate_view_consistency=False skips= RuntimeError no _refs support torch Tensor is_conj DecorateInfo unittest expectedFailure TestCommon test_python_ref dtypes= torch complex torch complex PythonRefInfo _refs transpose torch_opinfo_name= transpose PythonRefInfo _refs transpose_copy torch_opinfo_name= transpose_copy supports_out=True PythonRefInfo _refs t torch_opinfo_name= t PythonRefInfo _refs t_copy torch_opinfo_name= t_copy supports_out=True PythonRefInfo _refs T torch_opinfo_name= T error_inputs_func=partial error_inputs_T has_ndims_error=True PythonRefInfo _refs unbind_copy torch_opinfo_name= unbind_copy PythonRefInfo _refs unfold torch_opinfo_name= unfold PythonRefInfo _refs unfold_copy torch_opinfo_name= unfold_copy supports_out=True PythonRefInfo _refs unsqueeze torch_opinfo_name= unsqueeze PythonRefInfo _refs unsqueeze_copy torch_opinfo_name= unsqueeze_copy supports_out=True PythonRefInfo _refs view torch_opinfo_name= view PythonRefInfo _refs view_as torch_opinfo_name= view_as PythonRefInfo _refs view_copy torch_opinfo_name= view_copy supports_out=True PythonRefInfo _refs vstack torch_opinfo_name= vstack skips= https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestCommon test_python_ref_errors PythonRefInfo _refs unflatten torch_opinfo_name= unflatten PythonRefInfo _refs unbind torch_opinfo_name= unbind Reduction Reference OpInfos ReductionPythonRefInfo _refs all torch_opinfo_name= all skips= FIXME uint input returns uint instead bool DecorateInfo unittest expectedFailure TestReductions test_result_dtype dtypes= torch uint ReductionPythonRefInfo _refs amax torch_opinfo_name= amax error_inputs_func=partial error_inputs_aminmax_amax_amin is_ref=True skips= FIXME reduces all dimensions when dim= DecorateInfo unittest expectedFailure TestReductions test_dim_empty DecorateInfo unittest expectedFailure TestReductions test_dim_empty_keepdim ReductionPythonRefInfo _refs amin torch_opinfo_name= amin error_inputs_func=partial error_inputs_aminmax_amax_amin is_ref=True skips= FIXME reduces all dimensions when dim= DecorateInfo unittest expectedFailure TestReductions test_dim_empty DecorateInfo unittest expectedFailure TestReductions test_dim_empty_keepdim ReductionPythonRefInfo _refs any torch_opinfo_name= any skips= FIXME uint input returns uint instead bool DecorateInfo unittest expectedFailure TestReductions test_result_dtype dtypes= torch uint ReductionPythonRefInfo _refs count_nonzero torch_opinfo_name= count_nonzero skips= FIXME count_nonzero does accept keepdim kwarg DecorateInfo unittest skip Skipped TestReductions test_dim_default_keepdim DecorateInfo unittest skip Skipped TestReductions test_dim_none_keepdim DecorateInfo unittest skip Skipped TestReductions test_dim_single_keepdim DecorateInfo unittest skip Skipped TestReductions test_dim_empty_keepdim DecorateInfo unittest skip Skipped TestReductions test_dim_multi_keepdim DecorateInfo unittest skip Skipped TestReductions test_dim_multi_unsorted_keepdim FIXME dim= reduces all dimensions DecorateInfo unittest skip Skipped TestReductions test_dim_empty ReductionPythonRefInfo _refs mean torch_opinfo_name= mean supports_out=True error_inputs_func=partial error_inputs_mean is_ref=True skips= FIXME reduces all dimensions when dim= DecorateInfo unittest expectedFailure TestReductions test_dim_empty DecorateInfo unittest expectedFailure TestReductions test_dim_empty_keepdim ReductionPythonRefInfo _refs std torch_opinfo_name= std supports_out=True skips= FIXME reduces all dimensions when dim= DecorateInfo unittest expectedFailure TestReductions test_dim_empty DecorateInfo unittest expectedFailure TestReductions test_dim_empty_keepdim FIXME improve precision DecorateInfo unittest skip Skipped TestReductions test_ref_small_input dtypes= torch float DecorateInfo unittest skip Skipped TestReductions test_ref_duplicate_values dtypes= torch float std_mean var_mean ReductionInfos PythonRefInfo _refs std_mean torch_opinfo_name= std_mean ReductionPythonRefInfo _refs sum torch_opinfo_name= sum supports_out=True skips= FIXME doesn t test out behavior properly operator DecorateInfo unittest expectedFailure TestCommon test_out FIXME mean reduces all dimensions when dim= DecorateInfo unittest skip Skipped TestReductions test_dim_empty DecorateInfo unittest skip Skipped TestReductions test_dim_empty_keepdim FIXME improve precision DecorateInfo unittest skip Skipped TestReductions test_ref_small_input dtypes= torch float DecorateInfo unittest skip Skipped TestReductions test_ref_duplicate_values dtypes= torch float DecorateInfo unittest skip Skipped TestOperators test_reduction_all dtypes= torch float PythonRefInfo _refs cumsum torch_opinfo_name= cumsum supports_out=True skips= doesn t test out behavior properly operator DecorateInfo unittest expectedFailure TestCommon test_out PythonRefInfo _refs cumprod torch_opinfo_name= cumprod supports_out=True skips= doesn t test out behavior properly operator DecorateInfo unittest expectedFailure TestCommon test_out PythonRefInfo _refs sum_to_size torch_opinfo_name= sum_to_size validate_view_consistency=False ReductionPythonRefInfo _refs prod torch_opinfo_name= prod supports_out=True supports_multiple_dims=True skips= FIXME doesn t test out behavior properly operator DecorateInfo unittest expectedFailure TestCommon test_out FIXME reduces all dimensions when dim= DecorateInfo unittest expectedFailure TestReductions test_dim_empty DecorateInfo unittest expectedFailure TestReductions test_dim_empty_keepdim FIXME improve precision DecorateInfo unittest skip Skipped TestReductions test_ref_small_input dtypes= torch float torch complex ReductionPythonRefInfo _refs var torch_opinfo_name= var supports_out=True skips= FIXME reduces all dimensions when dim= DecorateInfo unittest expectedFailure TestReductions test_dim_empty DecorateInfo unittest expectedFailure TestReductions test_dim_empty_keepdim FIXME improve precision DecorateInfo unittest skip Skipped TestReductions test_ref_small_input PythonRefInfo _refs var_mean torch_opinfo_name= var_mean validate_view_consistency=False Linear Algebra Operators PythonRefInfo _refs addr torch_opinfo_name= addr decorators= DecorateInfo unittest expectedFailure TestCommon test_python_ref PythonRefInfo _refs trace torch_opinfo_name= trace PythonRefInfo _refs norm torch_opinfo_name= norm supports_out=True Uses vector_norm inside vector_norm affected https github com pytorch pytorch issues validate_view_consistency=False Tensor Creation Reference OpInfos PythonRefInfo _refs empty torch_opinfo_name= empty skips= DecorateInfo unittest skip Expected empty comparable TestCommon test_python_ref DecorateInfo unittest skip Expected empty comparable TestCommon test_python_ref_torch_fallback DecorateInfo unittest skip Expected empty comparable TestCommon test_out DecorateInfo unittest skip Expected empty comparable TestCommon test_out_warning DecorateInfo unittest skip Expected empty comparable TestMathBits test_conj_view DecorateInfo unittest skip Expected empty comparable TestMathBits test_neg_conj_view DecorateInfo unittest skip Expected empty comparable TestMathBits test_neg_view FIXME shouldn t check empty results DecorateInfo unittest skip Can t check result empty TestCommon test_python_ref_executor DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu PythonRefInfo _refs empty_like torch_opinfo_name= empty_like skips= DecorateInfo unittest skip Expected empty comparable TestCommon test_python_ref DecorateInfo unittest skip Expected empty comparable TestCommon test_python_ref_torch_fallback DecorateInfo unittest skip Expected empty comparable TestCommon test_out DecorateInfo unittest skip Expected empty comparable TestCommon test_out_warning DecorateInfo unittest skip Expected empty comparable TestMathBits test_conj_view DecorateInfo unittest skip Expected empty comparable TestMathBits test_neg_conj_view DecorateInfo unittest skip Expected empty comparable TestMathBits test_neg_view FIXME should compare results empty_like DecorateInfo unittest skip Can t check result empty_like TestCommon test_python_ref_executor DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu PythonRefInfo _refs randn torch_opinfo_name= randn op=lambda args kwargs wrapper_set_seed refs randn args kwargs skips= see https github com pytorch pytorch issues DecorateInfo unittest skip make_traced doesn t set seed properly TestCommon test_python_ref_executor These tests expect input tensor sequence tensors DecorateInfo unittest skip Test expects tensor input TestCommon test_noncontiguous_samples DecorateInfo unittest skip Test expects tensor input TestMathBits test_neg_view DecorateInfo unittest skip Test expects tensor input TestMathBits test_conj_view DecorateInfo unittest skip Test expects tensor input TestMathBits test_neg_conj_view PythonRefInfo _refs eye torch_opinfo_name= eye skips= skip these tests since we have non tensor input DecorateInfo unittest skip Skipped TestMathBits test_conj_view DecorateInfo unittest skip Skipped TestMathBits test_neg_conj_view DecorateInfo unittest skip Skipped TestMathBits test_neg_view PythonRefInfo _refs new_empty torch_opinfo_name= new_empty skips= DecorateInfo unittest skip Expected empty comparable TestCommon test_python_ref DecorateInfo unittest skip Expected empty comparable TestCommon test_python_ref_torch_fallback DecorateInfo unittest skip Expected empty comparable TestCommon test_out DecorateInfo unittest skip Expected empty comparable TestCommon test_out_warning DecorateInfo unittest skip Expected empty comparable TestMathBits test_conj_view DecorateInfo unittest skip Expected empty comparable TestMathBits test_neg_conj_view DecorateInfo unittest skip Expected empty comparable TestMathBits test_neg_view FIXME should compare results empty_like DecorateInfo unittest skip Can t check result new_empty TestCommon test_python_ref_executor DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu PythonRefInfo _refs new_empty_strided torch_opinfo_name= new_empty_strided skips= DecorateInfo unittest skip Expected empty_strided comparable TestCommon test_python_ref DecorateInfo unittest skip Expected empty_strided comparable TestCommon test_python_ref_torch_fallback DecorateInfo unittest skip Expected empty_strided comparable TestMathBits test_conj_view DecorateInfo unittest skip Expected empty_strided comparable TestMathBits test_neg_conj_view DecorateInfo unittest skip Expected empty_strided comparable TestMathBits test_neg_view DecorateInfo unittest skip Expected empty_strided comparable TestCommon test_python_ref_executor DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu PythonRefInfo _refs empty_strided torch_opinfo_name= empty_strided skips= DecorateInfo unittest skip Expected empty_strided comparable TestCommon test_python_ref DecorateInfo unittest skip Expected empty_strided comparable TestCommon test_python_ref_torch_fallback DecorateInfo unittest skip Expected empty_strided comparable TestMathBits test_conj_view DecorateInfo unittest skip Expected empty_strided comparable TestMathBits test_neg_conj_view DecorateInfo unittest skip Expected empty_strided comparable TestMathBits test_neg_view DecorateInfo unittest skip Expected empty_strided comparable TestCommon test_python_ref_executor DecorateInfo unittest skip output non-deterministic TestCommon test_compare_cpu PythonRefInfo _refs new_full torch_opinfo_name= new_full PythonRefInfo _refs new_ones torch_opinfo_name= new_ones PythonRefInfo _refs new_zeros torch_opinfo_name= new_zeros Conditional Reference OpInfos PythonRefInfo _refs masked_fill torch_opinfo_name= masked_fill skips= DecorateInfo unittest expectedFailure TestCommon test_python_ref_errors PythonRefInfo _refs where torch_opinfo_name= where op=lambda condition other refs where condition other supports_out=False skips= DecorateInfo unittest expectedFailure TestCommon test_python_ref_errors device_type= cuda PythonRefInfo _refs index_select torch_opinfo_name= index_select empty_strided skips= no _refs support Tensor __setitem__ DecorateInfo unittest expectedFailure TestCommon test_python_ref Sample out= stride zero This _out operation checks input has no inner overlap DecorateInfo unittest expectedFailure TestCommon test_python_ref_errors PythonRefInfo _refs index_copy torch_opinfo_name= index_copy empty_strided skips= no _refs support Tensor __setitem__ DecorateInfo unittest expectedFailure TestCommon test_python_ref PythonRefInfo _refs index_add torch_opinfo_name= index_add empty_strided skips= no _refs support Tensor __setitem__ DecorateInfo unittest expectedFailure TestCommon test_python_ref DecorateInfo unittest expectedFailure TestCommon test_python_ref_errors PythonRefInfo _refs index_fill torch_opinfo_name= index_fill empty_strided skips= no _refs support Tensor __setitem__ DecorateInfo unittest expectedFailure TestCommon test_python_ref Test-related functions PythonRefInfo _refs allclose torch_opinfo_name= allclose Misc functions PythonRefInfo _refs stft torch_opinfo_name= stft skips= RuntimeError no _refs support aten pad DecorateInfo unittest expectedFailure TestCommon test_python_ref PythonRefInfo _refs istft torch_opinfo_name= istft skips= RuntimeError no _refs support aten unfold_backward DecorateInfo unittest expectedFailure TestCommon test_python_ref DecorateInfo unittest skip Expected unfold_backward got unexpected keyword argument input_sizes TestCommon test_python_ref_executor dtypes= torch complex torch complex PythonRefInfo _refs view_as_complex torch_opinfo_name= view_as_complex PythonRefInfo _refs split_with_sizes torch_opinfo_name= split_with_sizes python_ref_db += opinfo definitions python_ref_db Common operator groupings ops_and_refs = op_db + python_ref_db unary_ufuncs = op op ops_and_refs isinstance op UnaryUfuncInfo binary_ufuncs = op op ops_and_refs isinstance op BinaryUfuncInfo binary_ufuncs_and_refs = tuple op op ops_and_refs isinstance op BinaryUfuncInfo spectral_funcs = op op ops_and_refs isinstance op SpectralFuncInfo sparse_unary_ufuncs = op op op_db isinstance op UnaryUfuncInfo op supports_sparse sparse_csr_unary_ufuncs = op op op_db isinstance op UnaryUfuncInfo op supports_sparse_csr sparse_reduction_ops = op op op_db isinstance op ReductionOpInfo op supports_sparse shape_funcs = op op ops_and_refs isinstance op ShapeFuncInfo reduction_ops = op op ops_and_refs isinstance op ReductionOpInfo reference_filtered_ops = op op reduction_ops op ref None reference_masked_ops = op op reference_filtered_ops op name startswith masked sparse_masked_reduction_ops = op op sparse_reduction_ops op name startswith masked index_variable shape max_indices device=torch device cpu isinstance shape tuple shape = shape torch testing make_tensor shape dtype=torch long device=device low= high=max_indices gather_variable shape index_dim max_indices duplicate=False device=torch device cpu assert len shape == assert index_dim batch_dim = - index_dim index = torch zeros shape dtype=torch long device=device i range shape index_dim index select index_dim i copy_ torch randperm max_indices device=device shape batch_dim duplicate index select batch_dim copy_ index select batch_dim index bernoulli_scalar torch tensor dtype=torch bool bernoulli_ mask_not_all_zeros shape assert len shape while True result = torch randn shape gt result sum result Copied functorch xfail op_name variant_name= device_type=None dtypes=None op_name variant_name device_type dtypes True skip op_name variant_name= device_type=None dtypes=None op_name variant_name device_type dtypes False skipOps test_case_name base_test_name to_skip all_opinfos = op_db xfail to_skip op_name variant_name device_type dtypes expected_failure = xfail matching_opinfos = o o all_opinfos o name == op_name o variant_test_name == variant_name assert len matching_opinfos = f Couldn t find OpInfo xfail op matching_opinfos decorators = list op decorators expected_failure decorator = DecorateInfo unittest expectedFailure test_case_name base_test_name device_type=device_type dtypes=dtypes decorators append decorator decorator = DecorateInfo unittest skip Skipped test_case_name base_test_name device_type=device_type dtypes=dtypes decorators append decorator op decorators = tuple decorators This decorator doesn t modify fn any way wrapped fn fn wrapped