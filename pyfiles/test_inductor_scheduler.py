Owner s module inductor unittest skipIf unittest mock Mock torch torch _inductor metrics metrics torch utils flop_counter torch _dynamo utils counters torch _inductor dependencies Dep ReadWrites torch _inductor scheduler BaseSchedulerNode Scheduler torch _inductor utils fresh_inductor_cache torch testing _internal common_cuda SM OrLater torch testing _internal common_device_type dtypes instantiate_device_type_tests skipCUDAIf torch testing _internal common_utils parametrize run_tests TestCase torch testing _internal inductor_utils IS_BIG_GPU torch utils _ordered_set OrderedSet FlopCounterMode args kwargs torch utils flop_counter FlopCounterMode args kwargs display=False get_total_flops mode sum v _ v mode flop_counts Global items random_tensor size dtype kwargs dtype torch half torch bfloat torch float torch double torch randn size dtype=dtype kwargs dtype torch uint torch int torch short torch int torch long torch randint size dtype=dtype kwargs raise ValueError Unsupported data type cT device dtype T shape requires_grad=False random_tensor shape requires_grad=requires_grad device=device dtype=dtype T inductor_metrics_log = torch _logging getArtifactLogger __name__ inductor_metrics _test_cases device dtype T = cT device dtype composite x y z tmp = torch mm x + y torch mm tmp z composite_relu x y tmp = torch mm x y torch relu tmp test_cases = torch mm T T torch add T T composite T T T composite_relu T T test_cases TestScheduler TestCase dtypes torch float torch float skipCUDAIf SM OrLater GPU capability SM test_disable_get_estimated_runtime_logging device dtype device == cpu tc = _test_cases device dtype turn off logging inductor metrics so they don t get logged torch _logging set_logs inductor_metrics=False metrics reset op example_inputs kwargs tc comp = torch compile op torch _dynamo reset fresh_inductor_cache comp example_inputs kwargs assertEqual metrics num_bytes_accessed assertEqual any m m metrics node_runtimes False assertEqual any m m metrics nodes_num_elem False metrics reset torch _logging set_logs dtypes torch float torch float skipCUDAIf SM OrLater GPU capability SM parametrize options max_autotune True max_autotune_gemm_backends TRITON max_autotune True max_autotune_gemm_backends TRITON ATEN torch _inductor config patch force_disable_caches True skipIf IS_BIG_GPU we can t use Triton only backend max autotune test_flop_counter_op device dtype options device == cpu tc = _test_cases device dtype torch _logging set_logs inductor_metrics=True op example_inputs kwargs tc comp = torch compile op options=options next two lines required otherwise flops will cached previous runs function torch _dynamo reset fresh_inductor_cache actually run set counters comp example_inputs kwargs FlopCounterMode mode comp example_inputs kwargs reference_flops = get_total_flops mode assertEqual reference_flops counters inductor flop_count msg=f op = op reference flops = reference_flops = counters counters inductor flop_count op = torch add assertNotEqual reference_flops msg=f op = op flops counters inductor flop_count = torch _logging set_logs test_fusion_prevent_too_many_reads_and_writes_prevents_fusion Test fusion prevented when unique I O buffers exceed threshold Setup Create nodes many unique I O buffers node reads A B C writes D node reads D E F writes G D becomes internal node reads node s write After fusion unique I O = A B C E F G = buffers scheduler = Mock spec=Scheduler scheduler can_buffer_be_removed_through_fusion = Mock return_value=False node = _create_mock_node name= node reads= A B C writes= D node = _create_mock_node name= node reads= D E F writes= G Execute Check threshold should prevent fusion since result = Scheduler fusion_prevent_too_many_reads_and_writes scheduler node node threshold= Assert Fusion should prevented unique buffers threshold assertTrue result test_fusion_prevent_too_many_reads_and_writes_allows_fusion Test fusion allowed when intermediate buffers removed Setup Create nodes where node reads node s output node reads A B writes C node reads C D writes E C becomes internal node reads node s write After fusion unique I O = A B D E = buffers scheduler = Mock spec=Scheduler scheduler can_buffer_be_removed_through_fusion = Mock return_value=False node = _create_mock_node name= node reads= A B writes= C node = _create_mock_node name= node reads= C D writes= E Execute Check threshold should allow fusion since = result = Scheduler fusion_prevent_too_many_reads_and_writes scheduler node node threshold= Assert Fusion should allowed unique buffers = threshold assertFalse result _create_mock_node name str reads list str writes list str - Mock Helper method create mock scheduler node specified reads writes node = Mock spec=BaseSchedulerNode node get_name = Mock return_value=name node get_nodes = Mock return_value= node Create mock Dep objects reads writes read_deps = OrderedSet read_name reads dep = Mock spec=Dep dep name = read_name read_deps add dep write_deps = OrderedSet write_name writes dep = Mock spec=Dep dep name = write_name write_deps add dep Create mock ReadWrites object read_writes = Mock spec=ReadWrites read_writes reads = read_deps read_writes writes = write_deps node read_writes = read_writes node instantiate_device_type_tests TestScheduler globals allow_xpu=True __name__ == __main__ run_tests