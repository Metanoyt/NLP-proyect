mypy allow-untyped-defs disable-error-code= attr-defined valid-type copy logging math random collections namedtuple typing Optional sympy torch torch _inductor config torch _inductor codegen cpp_utils DTYPE_TO_CPP torch _inductor codegen rocm ck_template CKTemplate torch _inductor codegen rocm compile_command rocm_compile_command torch _inductor codegen rocm rocm_kernel ROCmTemplateKernel torch _inductor ir Buffer Layout torch _inductor runtime runtime_utils next_power_of_ utils IndentedBuffer is_dynamic try_import_ck_lib _ gen_ops_library gen_ops_preselected CKGemmOperation = try_import_ck_lib log = logging getLogger __name__ lightweight collection information about single op InductorROCmOp = namedtuple InductorROCmOp op kBatch padding_lookup = M GemmSpecialization MPadding True GemmSpecialization MNPadding True GemmSpecialization MKPadding True GemmSpecialization MNKPadding True N GemmSpecialization NPadding True GemmSpecialization MNPadding True GemmSpecialization NKPadding True GemmSpecialization MNKPadding True K GemmSpecialization KPadding True GemmSpecialization MKPadding True GemmSpecialization NKPadding True GemmSpecialization MNKPadding True is_static_int number isinstance number int sympy Integer torch_layout_to_ck_layout torch_layout torch_layout stride - == Row torch_layout stride - == Col None CKGemmTemplate CKTemplate JINJA template rendering CK Universal GEMMs gemm_template = r version_comment headers globals instance_definition extern C PT_EXPORT kernel_definition auto gemm = instance_type auto invoker = gemm MakeInvoker is_batched auto argument = gemm MakeArgument reinterpret_cast const a_element_dtype X reinterpret_cast const b_element_dtype W std array const void ds_size ds_names reinterpret_cast c_element_dtype Y M N K B LDA LDB std array ck index_t ds_size ds_strides LDC M K batch_stride_A N K batch_stride_B std array ck index_t ds_size ds_batch_strides M N batch_stride_C a_elementwise_op b_elementwise_op epilogue c_elementwise_op auto argument = gemm MakeArgument reinterpret_cast const a_element_dtype X reinterpret_cast const b_element_dtype W std array const void ds_size ds_names reinterpret_cast c_element_dtype Y M N K LDA LDB std array ck index_t ds_size ds_strides LDC kBatch kBatch a_elementwise_op b_elementwise_op epilogue c_elementwise_op endif gemm IsSupportedArgument argument we do our best statically avoid case ` filter_op ` std cerr invalid argument gemm instance gemm GetTypeString std endl argument Print - workspace_size workspace_size = gemm GetWorkSpaceSize argument run kernel #ifdef GENERATE_CK_STANDALONE_RUNNER const auto stream_config = StreamConfig stream time kernel log level n_cold_iter n_hot_iter flush_l _cache rotate_count #else const auto stream_config = StreamConfig stream time kernel false log level #endif const float elapsed_time = invoker Run argument stream_config #ifdef GENERATE_CK_STANDALONE_RUNNER std cout elapsed time elapsed_time ms std endl #else void elapsed_time #endif kernel definition extern C standalone_runner_template = r #ifdef GENERATE_CK_STANDALONE_RUNNER standalone runner generated CK GEMM kernel inline_utils extern C int run_main int argc char argv is_batched const int _t B = B endif const int _t M = M const int _t N = N const int _t K = K const int _t LDA = LDA const int _t LDB = LDB const int _t LDC = LDC const int _t LDD = LDD const int _t kBatch = kBatch using AElementType = a_ck_dtype using BElementType = b_ck_dtype using CElementType = c_ck_dtype has_bias using BiasElementType = bias_ck_dtype endif has_scale using ScaleAElementType = scale_a_ck_dtype using ScaleBElementType = scale_b_ck_dtype endif using AArgType = a_torch_dtype using BArgType = b_torch_dtype using CArgType = c_torch_dtype has_bias using BiasArgType = bias_torch_dtype endif has_scale using ScaleAArgType = scale_a_torch_dtype using ScaleBArgType = scale_b_torch_dtype endif using ALayout = a_layout using BLayout = b_layout using CLayout = c_layout has_bias using BiasLayout = bias_layout endif is_batched using strides_t = std array int _t auto get_strides = int _t batch_stride int _t leading_dimension auto layout constexpr - strides_t constexpr std is_same_v decltype layout Row batch_stride leading_dimension batch_stride leading_dimension auto a_size = strides_t B M K auto a_stride = get_strides M K LDA ALayout auto b_size = strides_t B N K auto b_stride = get_strides N K LDB BLayout auto c_size = strides_t B M N auto c_stride = get_strides M N LDC CLayout using strides_t = std array int _t auto get_strides = int _t leading_dimension auto layout constexpr - strides_t constexpr std is_same_v decltype layout Row leading_dimension leading_dimension auto a_size = strides_t M K auto a_stride = get_strides LDA ALayout auto b_size = strides_t N K auto b_stride = get_strides LDB BLayout auto c_size = strides_t M N auto c_stride = get_strides LDC CLayout endif Tensor AElementType a_m_k HostTensorDescriptor a_size a_stride Tensor BElementType b_k_n HostTensorDescriptor b_size b_stride has_bias Tensor BiasElementType d_m_n HostTensorDescriptor c_size get_strides LDD BiasLayout endif has_scale NB these hardcoded Tensor ScaleAElementType s_a_m_n HostTensorDescriptor strides_t M N get_strides Row Tensor ScaleAElementType s_b_m_n HostTensorDescriptor strides_t M N get_strides Col endif Tensor CElementType c_m_n_host HostTensorDescriptor c_size c_stride Tensor CElementType c_m_n_device HostTensorDescriptor c_size c_stride a_m_k GenerateTensorValue GeneratorTensor_ AElementType b_k_n GenerateTensorValue GeneratorTensor_ BElementType has_bias d_m_n GenerateTensorValue GeneratorTensor_ BiasElementType endif has_scale s_a_m_n GenerateTensorValue GeneratorTensor_ ScaleAElementType s_b_m_n GenerateTensorValue GeneratorTensor_ ScaleBElementType endif DeviceMem a_m_k_device_buf sizeof AElementType a_m_k mDesc GetElementSpaceSize DeviceMem b_k_n_device_buf sizeof BElementType b_k_n mDesc GetElementSpaceSize has_bias DeviceMem d_m_n_device_buf sizeof BiasElementType d_m_n mDesc GetElementSpaceSize endif has_scale DeviceMem s_a_m_n_device_buf sizeof ScaleAElementType s_a_m_n mDesc GetElementSpaceSize DeviceMem s_b_m_n_device_buf sizeof ScaleBElementType s_b_m_n mDesc GetElementSpaceSize endif DeviceMem c_m_n_device_buf sizeof CElementType c_m_n_device mDesc GetElementSpaceSize a_m_k_device_buf ToDevice a_m_k mData data b_k_n_device_buf ToDevice b_k_n mData data has_bias d_m_n_device_buf ToDevice d_m_n mData data endif has_scale s_a_m_n_device_buf ToDevice s_a_m_n mData data s_b_m_n_device_buf ToDevice s_b_m_n mData data endif kernel_name static_cast const AArgType a_m_k_device_buf GetDeviceBuffer static_cast const BArgType b_k_n_device_buf GetDeviceBuffer has_scale static_cast const ScaleAArgType s_a_m_n_device_buf GetDeviceBuffer static_cast const ScaleBArgType s_b_m_n_device_buf GetDeviceBuffer endif has_bias static_cast const BiasArgType d_m_n_device_buf GetDeviceBuffer endif static_cast CArgType c_m_n_device_buf GetDeviceBuffer is_batched B endif M N K LDA LDB LDC LDD nullptr workspace_size nullptr workspace nullptr stream hip_check_error hipDeviceSynchronize run_main extern C int main int argc char argv run_main argc argv compile compile_cmd #endif GENERATE_CK_STANDALONE_RUNNER __init__ input_nodes list Buffer layout Layout alpha float beta float input_reorder Optional list int = None - None is_batched = len layout size == name = ck_batched_gemm_template is_batched ck_gemm_template super __init__ name=name input_nodes=input_nodes layout=layout input_reorder=input_reorder alpha = alpha beta = beta is_batched = is_batched header - IndentedBuffer res = super header is_batched res splice CK GEMM header s #include ck tensor_operation gpu device impl device_batched_gemm_multiple_d_xdl_cshuffle_v hpp res splice CK GEMM header s #include ck tensor_operation gpu device impl device_gemm_multiple_d_xdl_cshuffle_v hpp res globals - IndentedBuffer res = super globals res splice CK GEMM globals using Row = ck tensor_layout gemm RowMajor using Col = ck tensor_layout gemm ColumnMajor using BlockGemmPipelineScheduler = ck BlockGemmPipelineScheduler using GemmSpecialization = ck tensor_operation device GemmSpecialization using BlockGemmPipelineVersion = ck BlockGemmPipelineVersion struct MultiplyMultiplyAdd template typename E typename C typename D typename D typename D __host__ __device__ constexpr void operator E e const C c const D d const D d const D d const e = ck type_convert E ck type_convert float c ck type_convert float d ck type_convert float d + ck type_convert float d res inline_utils res = IndentedBuffer res splice #include host_tensor cpp #include device_memory cpp res _has_padding dimension gemm_specialization Get relevant padding map given dimension dimension_padding = padding_lookup get dimension Check specialization dimension s padding map dimension_padding get gemm_specialization False filter_op op_info InductorROCmOp Determines whether given op definition suitable current input output operation template implements Filter based inputs dtype layout statically inferred size Returns None op suitable otherwise returns op used op kBatch = op_info op op_info kBatch metas = T get_layout T input_nodes output_node X_meta = metas W_meta = metas Y_meta = metas - disable instance dtypes don t match op a_element_dtype = _TORCH_DTYPE_TO_CK X_meta dtype None op b_element_dtype = _TORCH_DTYPE_TO_CK W_meta dtype None op c_element_dtype = _TORCH_DTYPE_TO_CK Y_meta dtype None disable instance layouts don t match op a_layout = torch_layout_to_ck_layout X_meta None op b_layout = torch_layout_to_ck_layout W_meta None op c_layout = torch_layout_to_ck_layout Y_meta None try avoid launching instance invalid problem size see GridwiseGemm_xdl_cshuffle_v CheckValidity M = X_meta size - K = X_meta size - N = W_meta size - is_static_int M _has_padding M op gemm_specialization M op m_per_block = None is_static_int N _has_padding N op gemm_specialization N op n_per_block = None is_static_int K _has_padding K op gemm_specialization K op k_per_block = None K_t = kBatch op k_per_block K K_t = None need another kBatch check here lcm = abs op a_k op b_k math gcd op a_k op b_k K_t = kBatch lcm k_read_pad_splited = math ceil K K_t lcm k_read_pad_splited kBatch - = K None a_contig_size = K op a_layout == Row M op a_layout == Col None is_static_int a_contig_size a_contig_size op a_block_transfer_src_scalar_per_vector = None b_contig_size = N op b_layout == Row K op b_layout == Col None is_static_int b_contig_size b_contig_size op b_block_transfer_src_scalar_per_vector = None c_contig_size = N op c_layout == Row M op c_layout == Col None c_shuffle_block_transfer_scalar_per_vector_n_per_block = op c_shuffle_block_transfer_scalar_per_vector_n_per_block isinstance op c_shuffle_block_transfer_scalar_per_vector_n_per_block tuple op c_shuffle_block_transfer_scalar_per_vector_n_per_block is_static_int c_contig_size c_contig_size c_shuffle_block_transfer_scalar_per_vector_n_per_block = None _check_num_k_loops op kBatch None TBD disable instances invalid number pipeline prefetch stages It will avoid compiling small percentage unrunnable instances which fail gemm argument check op _check_num_k_loops op kBatch Additional splitK scenario check metas = T get_layout T input_nodes X_meta = metas W_meta = metas K = X_meta size - kBatch op block_gemm_pipeline_version = BlockGemmPipelineVersion v try prefetch_stages = _prefetch_stages op torch empty dtype=X_meta dtype element_size torch empty dtype=W_meta dtype element_size torch cuda get_device_properties X_meta device warp_size except Exception e log debug noqa G Failed prefetch_stages s exception s op name e conservative here disable op False K_t = op k_per_block kBatch ak = K + K_t - K_t op k_per_block op a_k num_k_loop = ak op k_per_block op a_k num_k_loop = prefetch_stages log debug Op s compatible due invalid number pipeline prefetch stages Parameters kBatch= s block_gemm_pipeline_version= s prefetch_stages= s num_k_loop= s op name kBatch op block_gemm_pipeline_version prefetch_stages num_k_loop False True small helper figure out prefetch stages AMD _prefetch_stages op a_dtype_size b_dtype_size warp_size int = version_str = op block_gemm_pipeline_version split - try version = int version_str Assuming format always vX except ValueError e raise ValueError f Invalid version string version_str e version raise ValueError f unknown prefetch stages op block_gemm_pipeline_version Define mapping versions stages version_to_stages = Get stages given version stages = version_to_stages get version stages None This means we re stage requires computation See github com ROCm composable_kernel blob d include ck tensor_operation gpu block blockwise_gemm_pipeline_xdlops_v hpp#L noqa B wgp_per_cu = max warp_size op block_size full_mem_band_prefetch_stages = math ceil wgp_per_cu op m_per_block a_dtype_size + op n_per_block b_dtype_size op k_per_block stages = min max full_mem_band_prefetch_stages stages emit_ck_instance op CKGemmOperation The Jinja template generating C++ type alias definition Universal GEMM instance struct_name = DeviceBatchedGemmMultiD_Xdl_CShuffle_V is_batched DeviceGemmMultiD_Xdl_CShuffle_V template_definition = r Gemm operator operation_name using Operation_ operation_name = ck tensor_operation device struct_name template_params The Jinja template generating C++ type alias usage Universal GEMM instance template_type = r Operation_ operation_name template_params = field_name field_value op dict_items isinstance field_value tuple tuple_elements = join map str iter field_value ds field_name element type layout bias arg = f field_name Tuple tuple_elements tile shape arg = f field_name S tuple_elements pyrefly ignore bad-argument-type template_params append arg field_value None pyrefly ignore bad-argument-type template_params append f field_name field_value operation_name = op name replace replace replace _template_from_string template_definition render operation_name=operation_name template_params= \n + join template_params struct_name=struct_name _template_from_string template_type render operation_name=operation_name render type ignore override kernel ROCmTemplateKernel op CKGemmOperation kwargs - str The primary entry point code rendering process used template epilogue_nodes = kwargs get epilogue_nodes assert epilogue_nodes None == len epilogue_nodes template_buffer_node = kwargs get template_buffer_node template_buffer_node None output_node = template_buffer_node input nodes X W matmul X W Bias addmm X W inv_scale_x inv_scale_w scaled_mm X W inv_scale_x inv_scale_w Bias scaled_mm bias X W = input_nodes input_nodes Y = output_node Bias = input_nodes == len input_nodes input_nodes == len input_nodes None has_bias = Bias None has_scale = len input_nodes op = copy deepcopy op This parameter converted into tuple because change DeviceGemm_Xdl_CShuffleV DeviceGemmMultiD_Xdl_CShuffle_V The first tuple element corresponds matmul result isinstance op c_shuffle_block_transfer_scalar_per_vector_n_per_block tuple op c_shuffle_block_transfer_scalar_per_vector_n_per_block = op c_shuffle_block_transfer_scalar_per_vector_n_per_block has_scale scale_x = input_nodes scale_w = input_nodes == scale_x get_numel == scale_w get_numel tensorwise scale both X W has_bias op c_elementwise_op = ScaleAdd op c_elementwise_op = Scale rowwise scale both X W has_bias op c_elementwise_op = MultiplyMultiplyAdd op c_elementwise_op = MultiplyMultiply op c_shuffle_dtype = F op ds_layouts = torch_layout_to_ck_layout scale_x get_layout torch_layout_to_ck_layout scale_w get_layout op ds_element_dtypes = _TORCH_DTYPE_TO_CK scale_x get_layout dtype _TORCH_DTYPE_TO_CK scale_w get_layout dtype op c_shuffle_block_transfer_scalar_per_vector_n_per_block += scale_x = None scale_w = None bias_dtype = Bias None bias_layout = torch_layout_to_ck_layout Bias get_layout bias_dtype = _TORCH_DTYPE_TO_CK Bias get_layout dtype op ds_layouts += bias_layout op ds_element_dtypes += bias_dtype has_scale op c_elementwise_op = Bilinear c_shuffle_dtype also used adding bias matmul result before converting down result dtype op c_shuffle_dtype = op acc_dtype parameter needs set accordingly bias stride correct accumulation bias_layout == Row bias has N shape bias_shuffle_block_transfer_scalar_per_vector_n_per_block = op c_shuffle_block_transfer_scalar_per_vector_n_per_block bias_layout == Col bias has M shape bias_shuffle_block_transfer_scalar_per_vector_n_per_block = raise AssertionError Bias layout neither row-major nor column-major second tuple element corresponds bias op c_shuffle_block_transfer_scalar_per_vector_n_per_block += bias_shuffle_block_transfer_scalar_per_vector_n_per_block instance_definition instance_type = emit_ck_instance op version_comment = rf Generated code CK inductor backend See type __module__ type __qualname__ Template instance op torch __version__= torch version git_version= getattr torch version git_version None epilogue = None op c_elementwise_op == Bilinear scale_w None epilogue = f Bilinear alpha beta op c_elementwise_op == Scale epilogue = Scale inv_scale_w inv_scale_x inv_scale_w inv_scale_x f op c_elementwise_op == ScaleAdd epilogue = ScaleAdd inv_scale_w inv_scale_x inv_scale_w inv_scale_x f op c_elementwise_op == MultiplyMultiply epilogue = MultiplyMultiply op c_elementwise_op == MultiplyMultiplyAdd epilogue = MultiplyMultiplyAdd op c_elementwise_op == PassThrough epilogue = PassThrough assert epilogue None CK GEMM epilogue set size_arg_strs = M N K LDA LDB LDC LDD is_batched size_arg_strs insert B res = _template_from_string gemm_template render inline_utils=self inline_utils headers=self header getvalue globals=self globals getvalue instance_definition=instance_definition kernel_definition=kernel def_kernel inputs= X W scale_x scale_w Bias type ignore list-item outputs= Y names_str= X W inv_scale_x inv_scale_w Bias Y input_reorder=self input_reorder size_args= f int _t arg arg size_arg_strs instance_type=instance_type a_element_dtype=op a_element_dtype b_element_dtype=op b_element_dtype c_element_dtype=op c_element_dtype bias_element_dtype=bias_dtype alpha=self alpha beta=self beta a_elementwise_op= PassThrough b_elementwise_op= PassThrough epilogue=epilogue has_bias=has_bias ds_size= op c_elementwise_op Bilinear ScaleAdd op c_elementwise_op == MultiplyMultiply op c_elementwise_op == MultiplyMultiplyAdd ds_names= join Bias op c_elementwise_op Bilinear ScaleAdd inv_scale_x inv_scale_w op c_elementwise_op == MultiplyMultiply inv_scale_x inv_scale_w Bias op c_elementwise_op == MultiplyMultiplyAdd ds_strides= join LDD op c_elementwise_op Bilinear ScaleAdd op c_elementwise_op == MultiplyMultiply LDD op c_elementwise_op == MultiplyMultiplyAdd version_comment=version_comment is_batched=self is_batched ds_batch_strides= join FIXME when supporting baddbmm config rocm generate_test_runner is_static_problem = all is_static_int arg arg size_args NOTE size_arg_strs defined above size_arg_vals = size_args is_static_problem f std stoi argv k k _ enumerate size_args size_args = dict zip size_arg_strs size_arg_vals strict=True runtime_args = dict zip name get_runtime_arg_info get_runtime_arg_values runner_code = _template_from_string standalone_runner_template render inline_utils=self inline_utils getvalue kernel_name=kernel kernel_name has_bias=has_bias has_scale=has_scale is_batched=self is_batched a_ck_dtype=op a_element_dtype b_ck_dtype=op b_element_dtype c_ck_dtype=op c_element_dtype bias_ck_dtype=op ds_element_dtypes has_bias scale_a_ck_dtype=op ds_element_dtypes has_scale == len op ds_element_dtypes BF scale_b_ck_dtype=op ds_element_dtypes has_scale == len op ds_element_dtypes BF a_torch_dtype=DTYPE_TO_CPP X get_layout dtype b_torch_dtype=DTYPE_TO_CPP W get_layout dtype c_torch_dtype=DTYPE_TO_CPP Y get_layout dtype bias_torch_dtype=DTYPE_TO_CPP Bias get_layout dtype Bias None scale_a_torch_dtype=DTYPE_TO_CPP scale_x get_layout dtype scale_x None scale_b_torch_dtype=DTYPE_TO_CPP scale_w get_layout dtype scale_w None a_layout=torch_layout_to_ck_layout X get_layout b_layout=torch_layout_to_ck_layout W get_layout c_layout=torch_layout_to_ck_layout Y get_layout bias_layout=torch_layout_to_ck_layout Bias get_layout Bias None compile_cmd=rocm_compile_command source_file_name executable_name exe size_args runtime_args res += runner_code res _is_rcr_f X_meta W_meta Y_meta = T get_layout T input_nodes output_node X_dtype W_dtype Y_dtype = _TORCH_DTYPE_TO_CK m dtype m X_meta W_meta Y_meta X_layout W_layout Y_layout = torch_layout_to_ck_layout m m X_meta W_meta Y_meta X_dtype == F W_dtype == F Y_dtype == F X_layout == Row W_layout == Col Y_layout == Row helper calculate potentially optimal kBatch es problem _get_kBatch op we only set higher kBatch K larger M N hand-tuned heuristic start metas = T get_layout T input_nodes X_meta = metas W_meta = metas M = X_meta size - K = X_meta size - N = W_meta size - is_dynamic input_nodes K max M N config rocm split_k_threshold user telling us which kBatches sweep just use those config rocm kBatch_sweep None config rocm kBatch_sweep Calculate number blocks needed each dimension total_k_blocks = math ceil K op k_per_block we want calculate how many blocks we need fit per CU cus = torch cuda get_device_properties X_meta device multi_processor_count again manual heuristics much larger kBatch significantly worse initial testing kBatch = min max next_power_of_ total_k_blocks cus kBatch gen_ops - list InductorROCmOp Creates list ` CKGemmOperation ` instances match GEMM operation template represents The instances guaranteed have correct layout dtype dimension padding GEMM input arguments An instance may invalidate GEMM configuration runtime Such instances will assigned +inf runtime autotune process try ck inductor batched_universal_gemm gen_instances type ignore gen_ops_library gen_batched_gemm_ops_library ck inductor universal_gemm gen_instances type ignore gen_ops_library gen_gemm_ops_library gen_ops_preselected gen_gemm_ops_preselected except ImportError generator = None is_batched generator = gen_batched_gemm_ops_library generator = gen_gemm_ops_library config rocm use_preselected_instances _is_rcr_f generator = gen_gemm_ops_preselected assert generator None rops = generator ops = o rops kBatches = _get_kBatch o kBatch kBatches pyrefly ignore bad-argument-type ops append InductorROCmOp op=o kBatch=kBatch filtered_instances = list filter lambda op filter_op op ops NB when using fixed list order most likely we will pick subset instances which very similar each other Randomizing choice seems solve random seed - chosen_instances = random sample filtered_instances min len filtered_instances config rocm ck_max_profiling_configs config rocm ck_max_profiling_configs filtered_instances log debug generated d ck instances after filter s len chosen_instances chosen_instances chosen_instances staticmethod add_ck_gemm_choices choices layout input_nodes alpha= beta= input_reorder=None Add Composable Kernel Universal GEMM instance choices auto-tuning list template = CKGemmTemplate input_nodes layout alpha=alpha beta=beta input_reorder=input_reorder ops = template gen_ops op ops template maybe_append_choice choices op=op op kBatch=op kBatch size_args X = input_nodes W = input_nodes Bias = input_nodes len input_nodes == input_nodes len input_nodes == None Y = output_node M = X get_size - K = X get_size - N = W get_size - LDA = X get_stride - X get_stride - == - LDB = W get_stride - W get_stride - == - LDC = Y get_stride - Y get_stride - == - LDD = Bias None len Bias get_size == Bias get_stride - Bias get_stride - == - is_batched B = X get_size B M N K LDA LDB LDC LDD M N K LDA LDB LDC LDD