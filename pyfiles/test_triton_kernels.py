Owner s module inductor ruff noqa F flake noqa E Skip do assign lambda expression use functools logging os torch torch _dynamo testing torch _inductor test_case torch utils _pytree pytree torch _dynamo config dynamo_config torch _higher_order_ops triton_kernel_wrap generate_ttir triton_kernel_wrapper_functional triton_kernel_wrapper_mutation torch _inductor config inductor_config metrics torch _inductor pattern_matcher CallFunctionVarArgs PatternMatcherPass register_graph_pattern torch _inductor utils run_and_get_code triton_version_uses_attrs_dict torch _library capture_triton torch testing FileCheck torch testing _internal common_utils torch testing _internal common_utils parametrize skipIfRocm skipIfWindows skipIfXpu torch testing _internal inductor_utils GPU_TYPE HAS_CUDA_AND_TRITON HAS_GPU HAS_XPU_AND_TRITON torch testing _internal logging_utils log_settings logs_to_string Defines all kernels tests torch testing _internal triton_utils noqa F torch utils _triton has_triton_experimental_host_tma has_triton_package has_triton_tensor_descriptor_host_tma HAS_GPU triton triton language tl HAS_CUDA_AND_TRITON try triton language extra libdevice manual fast_dividef fast_dividef my_fast_dividef except ImportError triton language extra cuda libdevice manual fast_dividef fast_dividef my_fast_dividef HAS_XPU_AND_TRITON triton language extra intel libdevice manual fast_dividef fast_dividef my_fast_dividef _triton_get_ast_equal_to_str params try triton backends compiler AttrsDescriptor noqa F f tt equal_to params except ImportError f equal_to_ = params Define shared triton constants here CONSTANT_C tl constexpr = tl constexpr STRING_CONSTANT_C tl constexpr = tl constexpr CONSTANT_C BOOL_CONSTANT_C tl constexpr = tl constexpr True FLOAT_CONSTANT_C = tl constexpr intentionally un-annotated hasattr triton constexpr_function triton constexpr_function log n len bin n - KernelTests torch _inductor test_case TestCase _kernel_launched_in_code kernel_name str code str - bool inductor_config cpp_wrapper f launchKernel kernel_name code f kernel_name run code requires_gpu test_triton_kernel_with_kernel_param triton jit pass_kernel kernel pass torch compile backend= eager f x grid = x numel pass_kernel grid kernel=x t = torch rand device=GPU_TYPE f t No need assert anything goal make sure dynamo does crash requires_gpu test_triton_kernel_higher_order_func torch _higher_order_ops triton_kernel_wrap kernel_side_table add_kernel_id = kernel_side_table add_kernel add_kernel t = torch rand device=GPU_TYPE t = torch rand device=GPU_TYPE torch_add = t + t Test higher order function mutation output = torch zeros_like t n_elements = output numel constant_args_idx = kernel_side_table add_constant_args n_elements n_elements BLOCK_SIZE grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE triton_kernel_wrapper_mutation kernel_idx=add_kernel_id constant_args_idx=constant_args_idx grid= grid tma_descriptor_metadata= kwargs= in_ptr t in_ptr t out_ptr output assertEqual output torch_add Make sure modified assertNotEqual output torch zeros_like t Test higher order function without mutation output = torch zeros_like t out_dict = triton_kernel_wrapper_functional kernel_idx=add_kernel_id constant_args_idx=constant_args_idx grid= grid tma_descriptor_metadata= kwargs= in_ptr t in_ptr t out_ptr output tensors_to_clone= in_ptr in_ptr out_ptr assertEqual out_dict out_ptr torch_add Make sure NOT modified assertEqual output torch zeros_like t requires_gpu test_triton_kernel_functionalize functorch make_fx torch _higher_order_ops triton_kernel_wrap kernel_side_table torch _subclasses functional_tensor CppFunctionalizeAPI FunctionalTensorMode PythonFunctionalizeAPI kernel_side_table reset_table f x output out = triton_kernel_wrapper_functional kernel_idx=kernel_side_table add_kernel mul _kernel constant_args_idx=kernel_side_table add_constant_args n_elements output numel BLOCK_SIZE grid= x numel tma_descriptor_metadata= kwargs= in_ptr x out_ptr output tensors_to_clone= in_ptr out_ptr out out_ptr t = torch rand device=GPU_TYPE t = torch rand device=GPU_TYPE FunctionalTensorMode gm = make_fx PythonFunctionalizeAPI functionalize f t t Make sure t modified assertNotEqual gm t t t gm = make_fx CppFunctionalizeAPI functionalize f t t Make sure t modified assertNotEqual gm t t t gm = make_fx torch func functionalize f t t Make sure t modified assertNotEqual gm t t t gm = make_fx f tracing_mode= fake t t assertExpectedInline gm code strip \ forward x_ output_ triton_kernel_wrapper_functional_proxy = torch ops higher_order triton_kernel_wrapper_functional kernel_idx = constant_args_idx = grid = tma_descriptor_metadata = kwargs = in_ptr x_ out_ptr output_ tensors_to_clone = in_ptr out_ptr x_ = output_ = None getitem = triton_kernel_wrapper_functional_proxy in_ptr getitem = None getitem_ = triton_kernel_wrapper_functional_proxy out_ptr triton_kernel_wrapper_functional_proxy = None getitem_ requires_gpu test_triton_kernel_mutation_type torch _higher_order_ops triton_kernel_wrap kernel_side_table torch _subclasses fake_tensor FakeTensorMode torch _subclasses functional_tensor FunctionalTensor FunctionalTensorMode prep x = torch ones device=GPU_TYPE requires_grad=True FunctionalTensorMode x_func = FunctionalTensor to_functional x assertTrue torch _is_functional_tensor x_func elem x_func normal mutation only FakeTensorMode x_func = prep FunctionalTensorMode x_func mul_ assertFalse torch _functionalize_are_all_mutations_hidden_from_autograd x_func elem triton kernel mutation only FakeTensorMode x_func = prep FunctionalTensorMode triton_kernel_wrapper_mutation kernel_idx=kernel_side_table add_kernel mul _inplace_kernel constant_args_idx=kernel_side_table add_constant_args n_elements x_func numel BLOCK_SIZE grid= x_func numel tma_descriptor_metadata= kwargs= ptr x_func assertTrue torch _functionalize_are_all_mutations_hidden_from_autograd x_func elem normal mutation + triton kernel mutation FakeTensorMode x_func = prep FunctionalTensorMode x_func mul_ triton_kernel_wrapper_mutation kernel_idx=kernel_side_table add_kernel mul _inplace_kernel constant_args_idx=kernel_side_table add_constant_args n_elements x_func numel BLOCK_SIZE grid= x_func numel tma_descriptor_metadata= kwargs= ptr x_func assertFalse torch _functionalize_are_all_mutations_hidden_from_autograd x_func elem requires_gpu common_utils parametrize dynamic False True common_utils parametrize backend eager aot_eager inductor test_triton_kernel_with_views dynamic backend call_triton_take_view x torch Tensor output = torch zeros_like x n_elements = output numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE mul _kernel grid x output n_elements BLOCK_SIZE= output call_triton_return_view x torch Tensor output = torch zeros_like x n_elements = output numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE mul _kernel grid x output n_elements BLOCK_SIZE= output view t = torch rand device=GPU_TYPE t_view = t view compiled_func = torch compile call_triton_take_view backend=backend fullgraph=True dynamic=dynamic assertEqual t_view compiled_func t_view assertEqual t compiled_func t_view view compiled_func = torch compile call_triton_return_view backend=backend fullgraph=True dynamic=dynamic assertEqual t_view compiled_func t view assertEqual t compiled_func t requires_gpu test_no_nan_kernels triton jit add_one_kernel in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask output = x + tl store out_ptr + offsets output mask=mask add_one x out n_elements = x numel add_one_kernel n_elements x out n_elements BLOCK_SIZE= AddOne torch autograd Function staticmethod forward ctx x out = torch empty_like x add_one x out ctx save_for_backward out out staticmethod backward ctx grad saved = ctx saved_tensors out = torch empty_like grad add_one saved out out torch compile f x AddOne apply x log_stream ctx = logs_to_string torch _inductor codecache output_code x = torch randn requires_grad=True device=GPU_TYPE ctx y = f x output_code = \n join log_stream getvalue strip split \n strip assertTrue len output_code msg= output code empty inductor_config cpp_wrapper assertEqual output_code count std numeric_limits double quiet_NaN assertEqual output_code count float nan assertEqual output_code count float nan requires_gpu common_utils parametrize grad_fn torch no_grad torch enable_grad common_utils parametrize backend eager aot_eager inductor test_triton_kernel_with_grad_option grad_fn backend call_triton x torch Tensor grad_fn output = torch zeros_like x n_elements = output numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE mul _kernel grid x output n_elements BLOCK_SIZE= output t = torch rand device=GPU_TYPE compiled_func = torch compile call_triton backend=backend fullgraph=True assertEqual t compiled_func t requires_gpu common_utils parametrize backend eager aot_eager inductor test_triton_kernel_inner_triton_function backend f x torch Tensor triton jit pow _kernel in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask output = x x tl store out_ptr + offsets output mask=mask output = torch zeros_like x n_elements = output numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE pow _kernel grid x output n_elements BLOCK_SIZE= output t = torch rand device=GPU_TYPE compiled_func = torch compile f backend=backend fullgraph=True TODO oulgen NYI - Support assertEqual t t compiled_func t requires_gpu common_utils parametrize grad False True common_utils parametrize dynamic False True inductor_config patch implicit_fallbacks False test_triton_kernel_no_clones grad dynamic torch _inductor utils run_and_get_code call_triton x torch Tensor y torch Tensor output torch Tensor n_elements = output numel tmp = torch add x grid = x numel add_kernel run x y output n_elements warmup=False grid=grid BLOCK_SIZE= output tmp t = torch rand device=GPU_TYPE requires_grad=grad t = torch rand device=GPU_TYPE requires_grad=grad o = torch zeros_like t requires_grad=grad torch_add = call_triton t t o metrics reset o = torch zeros_like t requires_grad=grad test code = run_and_get_code torch compile call_triton dynamic=dynamic t t o grad assertEqual metrics generated_kernel_count assertEqual torch_add test These two asserts optimal since requires original aten metadata so there might false negatives assertNotIn aoti_torch_copy_ inductor_config cpp_wrapper aten copy code assertNotIn aoti_torch_clone inductor_config cpp_wrapper aten clone code The following checks there only tensor output compiled graph dynamic grad inductor_config cpp_wrapper assertIn output_handles = code assertIn output_handles = code assertIn buf s code assertIn output_handles = inductor_config cpp_wrapper buf code requires_gpu test_triton_kernel_caching torch _inductor utils run_and_get_code add_in_loop x torch Tensor y torch Tensor output = torch zeros_like x n_elements = output numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE add_kernel_autotuned grid x y output n_elements output call_triton_add x torch Tensor y torch Tensor _ range x = add_in_loop x y x t = torch ones device=GPU_TYPE t = torch ones device=GPU_TYPE test code = run_and_get_code torch compile call_triton_add t t assertEqual test torch ones device=GPU_TYPE assertTrue add_kernel_autotuned_ run code requires_gpu test_triton_kernel_caching_duplicate torch _inductor utils run_and_get_code C triton jit pass_kernel in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask tl store out_ptr + offsets x mask=mask D triton jit pass_kernel in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask tl store out_ptr + offsets x mask=mask call_triton x torch Tensor output = torch zeros_like x output = torch zeros_like x n_elements = output numel grid = n_elements C pass_kernel grid x output n_elements BLOCK_SIZE= D pass_kernel grid x output n_elements BLOCK_SIZE= output + output t = torch ones device=GPU_TYPE test code = run_and_get_code torch compile call_triton t Make sure we emitted two kernels here assertTrue _kernel_launched_in_code pass_kernel_ code assertTrue _kernel_launched_in_code pass_kernel_ code requires_gpu test_triton_kernel_various_args triton autotune configs= triton Config BLOCK_SIZE key= triton jit pass_kernel out_ptr n_elements dummy_None dummy_empty dummy_float BLOCK_SIZE tl constexpr RANDOM_SIZE tl constexpr pass torch compile call_triton output n_elements = output numel grid = n_elements pass_kernel grid output n_elements None torch empty_like output RANDOM_SIZE= output output = torch randn device=GPU_TYPE Make sure does crash call_triton output requires_gpu test_triton_kernel_dependancies call_triton x torch Tensor y torch Tensor output = torch zeros_like x n_elements = output numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE add_kernel_autotuned grid x y output n_elements output = torch zeros_like output add_kernel_autotuned grid output y output n_elements output = torch add output output t = torch rand device=GPU_TYPE t = torch rand device=GPU_TYPE torch_result = call_triton t t compiled_result = torch compile call_triton t t assertEqual torch_result compiled_result requires_gpu test_triton_kernel_reinplace_inplaceable_pass call_triton x torch Tensor y torch Tensor output = torch zeros_like x n_elements = output numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE add_kernel_autotuned grid x y output n_elements add_kernel_autotuned grid output x output n_elements output t = torch rand device=GPU_TYPE t = torch rand device=GPU_TYPE torch_result = call_triton t t compiled_result = torch compile call_triton t t assertEqual torch_result compiled_result requires_gpu common_utils parametrize grad False True test_triton_kernel_multi_kernel grad triton jit mul _and_add_and_zero_negatives_kernel in_ptr in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr ACTIVATION tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements indirection_kernel in_ptr in_ptr n_elements BLOCK_SIZE=BLOCK_SIZE ACTIVATION= mul _inplace_kernel indirection_kernel in_ptr in_ptr n_elements BLOCK_SIZE=BLOCK_SIZE ACTIVATION= mul _inplace_kernel x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = x + y ACTIVATION == zero_negs output = zero_negs output tl store out_ptr + offsets output mask=mask torch compile call_triton x torch Tensor y torch Tensor xi torch Tensor yi torch Tensor output torch Tensor outputi torch Tensor n_elements = output numel grid = x numel mul _and_add_and_zero_negatives_kernel grid x y output n_elements BLOCK_SIZE= ACTIVATION= zero_negs mul _and_add_and_zero_negatives_kernel grid xi yi outputi n_elements BLOCK_SIZE= ACTIVATION=None output outputi t = torch tensor - - device=GPU_TYPE requires_grad=grad t = torch tensor - - device=GPU_TYPE requires_grad=grad float_result = t + t float_result = float_result where float_result = t i = torch randint - device=GPU_TYPE t i = torch randint - device=GPU_TYPE o = torch zeros_like t requires_grad=grad oi = torch zeros_like t i int_result = t i + t i result resulti = call_triton t t t i t i o oi assertEqual float_result result assertEqual int_result resulti requires_gpu skipIfXpu test_triton_kernel_constants triton jit mulC_kernel in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr CONSTANT_NAME tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask CONSTANT_NAME == STRING_CONSTANT_C output = CONSTANT_C x BOOL_CONSTANT_C output = CONSTANT_C tl store out_ptr + offsets output mask=mask call_triton x torch Tensor output = torch zeros_like x n_elements = output numel grid = x numel mulC_kernel grid x output n_elements BLOCK_SIZE= CONSTANT_NAME= CONSTANT_C output Triton kernels capture global constants their parse time value runtime value global CONSTANT_C prev_c = CONSTANT_C If behavior triton kernels change test will fail CONSTANT_C = tl constexpr assert CONSTANT_C = prev_c t = torch randn device=GPU_TYPE torch_result = call_triton t compiled_result = torch compile call_triton t assertEqual torch_result compiled_result reset back CONSTANT_C = prev_c requires_gpu common_utils parametrize grad False True common_utils parametrize dynamic False True common_utils parametrize backend eager aot_eager inductor common_utils parametrize grid_type test_triton_kernel_autotune grad dynamic backend grid_type call_triton x torch Tensor y torch Tensor output torch Tensor n_elements = output numel grid_fn meta triton cdiv n_elements meta BLOCK_SIZE grid_type == grid = n_elements grid_type == grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE grid_type == grid = grid_fn add_kernel_autotuned grid x y output n_elements output t = torch rand device=GPU_TYPE requires_grad=grad t = torch rand device=GPU_TYPE requires_grad=grad output = torch zeros_like t requires_grad=grad torch_add = call_triton t t output compiled_func = torch compile call_triton backend=backend fullgraph=True dynamic=dynamic output = torch zeros_like t requires_grad=grad assertEqual compiled_func t t output torch_add requires_gpu common_utils parametrize backend eager aot_eager inductor inductor_config patch unsafe_ignore_unsupported_triton_autotune_args True test_triton_kernel_autotune_with_unsupported_args backend call_triton x torch Tensor y torch Tensor output = torch zeros_like x n_elements = output numel add_kernel_autotuned_with_unsupported_args n_elements x y output n_elements output t = torch rand device=GPU_TYPE t = torch rand device=GPU_TYPE torch_add = call_triton t t compiled_func = torch compile call_triton backend=backend fullgraph=True compiled_add = compiled_func t t assertEqual compiled_add torch_add requires_gpu common_utils parametrize grad False True common_utils parametrize dynamic False True common_utils parametrize backend eager aot_eager inductor common_utils parametrize grid_type common_utils parametrize tdlp test_triton_kernel_ d_autotune grad dynamic backend grid_type tdlp os os environ TORCHINDUCTOR_DUMP_LAUNCH_PARAMS = tdlp call_triton x torch Tensor y torch Tensor output torch Tensor x_elements = output size y_elements = output size grid_fn meta triton cdiv x_elements meta BLOCK_SIZE_X triton cdiv y_elements meta BLOCK_SIZE_Y grid_type == grid = x_elements y_elements grid_type == grid = lambda meta triton cdiv x_elements meta BLOCK_SIZE_X triton cdiv y_elements meta BLOCK_SIZE_Y grid_type == grid = grid_fn add_kernel_ d_autotuned grid x y output x_elements y_elements output t = torch rand device=GPU_TYPE requires_grad=grad t = torch rand device=GPU_TYPE requires_grad=grad output = torch zeros_like t requires_grad=grad torch_result = call_triton t t output compiled_func = torch compile call_triton backend=backend fullgraph=True dynamic=dynamic output = torch zeros_like t requires_grad=grad assertEqual compiled_func t t output torch_result requires_gpu common_utils parametrize dynamic False True test_triton_kernel_tracing dynamic call_triton_add x torch Tensor y torch Tensor grid_type int num= positional=False autotuned=False output = torch empty_like x n_elements = output numel grid_fn meta triton cdiv num meta BLOCK_SIZE grid_type == grid = x numel grid_type == grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE grid_type == grid = grid_fn grid = x numel autotuned capture_triton add_kernel_autotuned grid x y output n_elements positional capture_triton add_kernel grid x y output n_elements capture_triton add_kernel grid x y output n_elements BLOCK_SIZE= output t = torch rand device=GPU_TYPE requires_grad=True t = torch rand device=GPU_TYPE requires_grad=True t = torch rand device=GPU_TYPE requires_grad=True t = torch rand device=GPU_TYPE requires_grad=True torch_add = t + t tests = functools partial call_triton_add grid_type= functools partial call_triton_add grid_type= functools partial call_triton_add grid_type= num= positional=True functools partial call_triton_add grid_type= num= functools partial call_triton_add grid_type= functools partial call_triton_add grid_type= autotuned=True functools partial call_triton_add grid_type= num= autotuned=True functools partial call_triton_add grid_type= num= autotuned=True functools partial call_triton_add grid_type= autotuned=True functorch make_fx tracing_mode = symbolic dynamic fake test tests gm = make_fx test tracing_mode=tracing_mode t t result = test t t assertEqual result torch_add requires_gpu common_utils parametrize grad False True common_utils parametrize dynamic False True common_utils parametrize backend eager aot_eager inductor inductor_config patch implicit_fallbacks False test_triton_kernel_native grad dynamic backend call_triton_add x torch Tensor y torch Tensor output torch Tensor grid_type int num= positional=False n_elements = output numel grid_fn meta triton cdiv num meta BLOCK_SIZE grid_type == grid = x numel grid_type == grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE grid = grid_fn positional add_kernel grid x y output n_elements add_kernel grid x y output n_elements BLOCK_SIZE= output t = torch rand device=GPU_TYPE requires_grad=grad t = torch rand device=GPU_TYPE requires_grad=grad o = torch zeros_like t requires_grad=grad torch_add = t + t No Dynamo -- Make sure triton kernel works assertEqual call_triton_add t t o torch_add No Dynamo -- Make sure triton kernel works positional BLOCK_SIZE o = torch zeros_like t requires_grad=grad assertEqual call_triton_add t t o True torch_add With Dynamo compiled_func = torch compile call_triton_add backend=backend fullgraph=True dynamic=dynamic With simple kernel o = torch zeros_like t requires_grad=grad assertEqual compiled_func t t o torch_add With lambda kernel o = torch zeros_like t requires_grad=grad assertEqual compiled_func t t o torch_add With lambda kernel positional BLOCK_SIZE o = torch zeros_like t requires_grad=grad assertEqual compiled_func t t o True torch_add With user defined function kernel o = torch zeros_like t requires_grad=grad assertEqual compiled_func t t o torch_add requires_gpu test_triton_kernel_mutation_not_mark_dirty torch compile f x n_elements = x numel add_kernel n_elements x x x n_elements x x = torch randn device=GPU_TYPE requires_grad=True x_cloned = x clone out = x_cloned sin f x_cloned out sum backward requires_gpu inductor_config patch allow_buffer_reuse True test_triton_kernel_inputs_buffer_reuse _mul x y = torch empty_like x mul _kernel in_ptr =x out_ptr=y n_elements=x numel BLOCK_SIZE= y torch compile f x _ range The output one kernel input next kernel some point we should reuse buffers allocate new ones x = _mul x x + x = torch randn device=GPU_TYPE dtype=torch float eager_out = f x compiled_out code = run_and_get_code torch compile f x assertEqual compiled_out eager_out Check we re allocating minimal buffers code_string = aoti_torch_empty_strided inductor_config cpp_wrapper f empty_strided_ GPU_TYPE torch float num_bufs_allocated = code count code_string assertEqual num_bufs_allocated Check we re reusing buffers allocating num_bufs_reused = code count reuse inductor_config cpp_wrapper reuse assertEqual num_bufs_reused requires_gpu test_triton_kernel_matmul_tracking triton jit ones_kernel x_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl store x_ptr + offsets x mask=mask torch compile f x out = torch zeros_like x ones_kernel out BLOCK_SIZE= torch mm out x + x = torch randn device=GPU_TYPE torch_out = f x python_out = torch mm torch ones device=GPU_TYPE x + assertEqual torch_out python_out requires_gpu test_triton_kernel_strided_input f inp left has strides left right = torch split inp dim= out = torch empty_like left X_BLOCK_SIZE Y_BLOCK_SIZE = grid = left size X_BLOCK_SIZE left size Y_BLOCK_SIZE double_strided_kernel grid in_ptr=left out_ptr=out in_y_stride=left stride out_y_stride=out stride X_BLOCK_SIZE=X_BLOCK_SIZE Y_BLOCK_SIZE=Y_BLOCK_SIZE out inp = torch randn device=GPU_TYPE eager_out = f inp compiled_out = torch compile f inp assertEqual compiled_out eager_out inductor_config patch triton_kernel_default_layout_constraint= needs_fixed_stride_order requires_gpu test_layout_constraint_needs_fixed_stride_order Construct custom op whose output strides torch library custom_op mylib weird_op_with_lowering mutates_args= weird_op_with_lowering x torch Tensor - torch Tensor torch empty_strided dtype=x dtype device=x device weird_op_with_lowering register_fake _ x torch empty_strided dtype=x dtype device=x device The lowering custom op produces output strides torch _inductor lowering empty_strided register_lowering register_lowering torch ops mylib weird_op_with_lowering _ x empty_strided x shape dtype=x dtype device=torch device GPU_TYPE Triton kernel has different behavior depending input strides triton jit kernel in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements output = offsets tl store out_ptr + offsets output mask=mask arange_out x out n_elements = x numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE kernel grid x out n_elements BLOCK_SIZE= f x y = weird_op_with_lowering x Inductor lowering will decide y better having strides This different strides tracing time Under needs_fixed_stride_order config inductor will coerce y have strides before passing arange_out If doesn t then result will different eager mode arange_out x y x + y x = torch randn device=GPU_TYPE eager_out = f x compiled_inductor_f = torch compile f backend= inductor fullgraph=True compiled_inductor_out = compiled_inductor_f x assertEqual compiled_inductor_out eager_out requires_gpu test_triton_kernel_strided_input_nonzero_offset f inp right has strides storage offset left right = torch split inp dim= out = torch empty_like right X_BLOCK_SIZE Y_BLOCK_SIZE = grid = right size X_BLOCK_SIZE right size Y_BLOCK_SIZE double_strided_kernel grid in_ptr=right out_ptr=out in_y_stride=right stride out_y_stride=out stride X_BLOCK_SIZE=X_BLOCK_SIZE Y_BLOCK_SIZE=Y_BLOCK_SIZE out inp = torch randn device=GPU_TYPE eager_out = f inp compiled_out = torch compile f inp assertEqual compiled_out eager_out requires_gpu test_triton_kernel_slice_and_view_input f inp left has strides left = inp left = left view out = torch empty_like left X_BLOCK_SIZE Y_BLOCK_SIZE = grid = left size left size X_BLOCK_SIZE left size Y_BLOCK_SIZE double_strided_kernel grid in_ptr=left out_ptr=out in_y_stride=left stride out_y_stride=out stride X_BLOCK_SIZE=X_BLOCK_SIZE Y_BLOCK_SIZE=Y_BLOCK_SIZE out + left inp = torch randn device=GPU_TYPE eager_out = f inp compiled_out = torch compile f inp assertEqual compiled_out eager_out requires_gpu test_triton_kernel_fallback f x y out = torch zeros_like x out = torch zeros_like x torch mm ExternKernelOut add_kernel x torch mm x y out torch sort creates fallback kernel hence MultiOutput add_kernel x torch sort y values out out out x = torch randn device=GPU_TYPE y = torch randn device=GPU_TYPE eager_out = f x y compiled_out = torch compile f x y assertEqual compiled_out eager_out requires_gpu test_triton_kernel_out_of_order triton jit add_kernel in_ptr in_ptr BLOCK_SIZE tl constexpr out_ptr n_elements pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = x + y tl store out_ptr + offsets output mask=mask f x y out = torch zeros_like x n_elements = x numel add_kernel n_elements x y out n_elements out x = torch randn device=GPU_TYPE y = torch randn device=GPU_TYPE eager_out = f x y compiled_out = torch compile f x y assertEqual compiled_out eager_out requires_gpu dynamo_config patch capture_dynamic_output_shape_ops=True dynamo_config patch capture_scalar_outputs=True common_utils parametrize backend eager aot_eager inductor test_triton_kernel_unbacked_shape_tensor backend triton jit square in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask output = x x tl store out_ptr + offsets output mask=mask f x x = x x n_elements = x numel output = torch zeros_like x grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE square grid x output n_elements BLOCK_SIZE= output x = torch randn device=GPU_TYPE eager_out = f x compiled_out = torch compile f fullgraph=True backend=backend x assertEqual compiled_out eager_out requires_gpu common_utils parametrize dump_launch_params common_utils parametrize dynamic False True test_triton_kernel_equal_to_ _arg dynamic dump_launch_params os environ TORCHINDUCTOR_DUMP_LAUNCH_PARAMS = dump_launch_params triton jit add_kernel_half_n_elements in_ptr in_ptr out_ptr half_n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets half_n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = x + y tl store out_ptr + offsets output mask=mask f x y out = torch empty_like x half_n_elements = x numel add_kernel_half_n_elements half_n_elements x y out half_n_elements BLOCK_SIZE= out x = torch randn device=GPU_TYPE y = torch randn device=GPU_TYPE eager_out = f x y compiled_out sources = run_and_get_code torch compile f dynamic=dynamic x y triton_version_uses_attrs_dict assertFalse equal_to sources dynamic when half_n_elements passed Triton kernel dynamic equal_to_ specialization can t enforced also equal_to_ specialization doesn t occur appear signature newer versions triton i e ones where triton_version_uses_attrs_dict == True assertTrue _triton_get_ast_equal_to_str sources assertTrue _triton_get_ast_equal_to_str sources assertEqual compiled_out eager_out requires_gpu common_utils parametrize dynamic False True test_triton_kernel_equal_to_ _float_arg dynamic f x y out = torch empty_like x n_elements = x numel scaling_factor = n_elements add_kernel_with_scaling n_elements x y out n_elements scaling_factor BLOCK_SIZE= out x = torch randn device=GPU_TYPE y = torch randn device=GPU_TYPE eager_out = f x y compiled_out sources = run_and_get_code torch compile f dynamic=dynamic x y float both literal symbolic should added equal_to_ triton_version_uses_attrs_dict assertTrue _triton_get_ast_equal_to_str sources assertEqual compiled_out eager_out requires_gpu test_triton_kernel_with_imported_symbol triton jit add_kernel_with_imported_symbol in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask output = fast_dividef x tl store out_ptr + offsets output mask=mask f x out = torch empty_like x n_elements = x numel add_kernel_with_imported_symbol n_elements x out n_elements BLOCK_SIZE= out x = torch randn device=GPU_TYPE eager_out = f x compiled_out = torch compile f x assertEqual compiled_out eager_out unittest skipIf HAS_GPU hasattr triton constexpr_function newer triton version required test_triton_kernel_with_constexpr_function triton jit kernel x_ptr output_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= We use D launch grid so axis block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load x_ptr + offsets mask=mask FIRST_DIM tl constexpr = x shape output = x + log FIRST_DIM tl store output_ptr + offsets output mask=mask f x out = torch zeros_like x n_elements = x numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE kernel grid x out n_elements BLOCK_SIZE= out x = torch randn device=GPU_TYPE eager_out = f x compiled_out triton_code = run_and_get_code torch compile f fullgraph=True x assertIn triton constexpr_function triton_code assertEqual compiled_out eager_out requires_gpu test_triton_kernel_with_imported_symbol_with_custom_name triton jit add_kernel_with_imported_symbol in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask output = my_fast_dividef x tl store out_ptr + offsets output mask=mask f x out = torch empty_like x n_elements = x numel add_kernel_with_imported_symbol n_elements x out n_elements BLOCK_SIZE= out x = torch randn device=GPU_TYPE eager_out = f x compiled_out = torch compile f x assertEqual compiled_out eager_out requires_gpu common_utils parametrize size common_utils parametrize dynamic False True test_triton_kernel_different_shapes size dynamic torch _inductor utils run_and_get_code f x y xx yy n_elements = x numel output_ = torch zeros_like x grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE add_kernel grid x y output_ n_elements BLOCK_SIZE= n_elements = xx numel output_ = torch zeros_like xx grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE add_kernel grid xx yy output_ n_elements BLOCK_SIZE= output_ output_ x = torch rand size device=GPU_TYPE y = torch rand size device=GPU_TYPE xx = torch rand size size device=GPU_TYPE yy = torch rand size size device=GPU_TYPE args = x y xx yy eager_out = f args compiled_out code = run_and_get_code torch compile f fullgraph=True dynamic=dynamic backend= inductor args size == dynamic Produce kernels due divisibility assertTrue _kernel_launched_in_code add_kernel_ code assertTrue _kernel_launched_in_code add_kernel_ code size == dynamic Only one kernel assertTrue _kernel_launched_in_code add_kernel_ code assertFalse _kernel_launched_in_code add_kernel_ code assertEqual compiled_out eager_out requires_gpu common_utils parametrize dynamic False True common_utils parametrize backend eager aot_eager inductor test_triton_kernel_triton_dtype dynamic backend triton jit add_kernel_with_dtype in_ptr in_ptr out_ptr dtype tl constexpr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask dtype y = tl load in_ptr + offsets mask=mask dtype output = x + y tl store out_ptr + offsets output mask=mask f x y dtype_torch dtype_triton output = torch zeros_like x dtype=dtype_torch n_elements = output numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE add_kernel_with_dtype grid x y output dtype_triton n_elements BLOCK_SIZE= output x = torch randn device=GPU_TYPE y = torch randn device=GPU_TYPE args_list = x y torch float tl float torch cuda is_bf _supported including_emulation=False args_list append x y torch bfloat tl bfloat args args_list eager_out = f args compiled_out = torch compile f fullgraph=True backend=backend dynamic=dynamic args assertEqual compiled_out eager_out requires_gpu common_utils parametrize backend eager aot_eager inductor test_triton_kernel_special_kwargs_with_autotune backend triton autotune configs= triton Config BLOCK_SIZE triton Config BLOCK_SIZE key= n_elements triton jit add_kernel in_ptr in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = x + y tl store out_ptr + offsets output mask=mask torch compile fullgraph=True backend=backend f x y output = torch zeros_like x n_elements = output numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE add_kernel grid x y output n_elements num_warps= num_stages= output x = torch randn device=GPU_TYPE f x x requires_gpu common_utils parametrize backend eager aot_eager inductor test_triton_kernel_empty_autotune_config_dict backend triton autotune configs= triton Config num_stages= triton Config num_stages= key= n_elements triton jit add_kernel in_ptr in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = x + y tl store out_ptr + offsets output mask=mask torch compile fullgraph=True backend=backend f x y output = torch zeros_like x n_elements = output numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE add_kernel grid x y output n_elements BLOCK_SIZE= output x = torch randn device=GPU_TYPE f x x requires_gpu common_utils parametrize autotune False True common_utils parametrize backend eager aot_eager inductor test_triton_kernel_special_params autotune backend triton jit special_params_kernel in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr num_warps tl constexpr num_stages tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask output = x num_stages + num_warps tl store out_ptr + offsets output mask=mask NUM_WARPS = NUM_STAGES = autotune special_params_kernel = triton autotune configs= triton Config BLOCK_SIZE num_stages=NUM_STAGES num_warps=NUM_WARPS triton Config BLOCK_SIZE num_stages=NUM_STAGES num_warps=NUM_WARPS key= n_elements special_params_kernel kwargs = kwargs = BLOCK_SIZE num_stages NUM_STAGES num_warps NUM_WARPS f x output = torch zeros_like x n_elements = output numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE special_params_kernel grid x output n_elements kwargs output x = torch randn device=GPU_TYPE eager_out = f x compiled_out = torch compile f fullgraph=True backend=backend x expected_out = x NUM_STAGES + NUM_WARPS assertEqual eager_out expected_out assertEqual compiled_out expected_out requires_gpu common_utils parametrize dynamic False True common_utils parametrize tma_version new old test_on_device_tma dynamic tma_version tma_version == new has_triton_tensor_descriptor_host_tma skipTest requires triton tools tensor_descriptor TMA support tma_version == old has_triton_experimental_host_tma skipTest requires triton tools experimental_descriptor TMA support kernel = add_kernel_on_device_tma_new_api tma_version == new add_kernel_on_device_tma_old_api f b BLOCK_SIZE = out = torch zeros_like m n = out size Allocate workspace on-device TMA descriptors Need bytes per descriptor descriptors total tma_version == old workspace = torch zeros dtype=torch uint device=a device workspace = None grid = lambda meta triton cdiv m meta BLOCK_SIZE triton cdiv n meta BLOCK_SIZE kernel grid b out m n workspace BLOCK_SIZE=BLOCK_SIZE out = torch randn device=GPU_TYPE b = torch randn device=GPU_TYPE expected_out = + b triton set_allocator lambda size align stream torch empty size dtype=torch int device=GPU_TYPE eager_out = f b compiled_out = torch compile f fullgraph=True dynamic=dynamic b assertEqual eager_out expected_out assertEqual compiled_out expected_out requires_gpu common_utils parametrize dynamic False True common_utils parametrize backend eager aot_eager inductor test_triton_kernel_multiple_outputs dynamic backend triton jit add_kernel in_ptr in_ptr out_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = x + y tl store out_ptr + offsets output mask=mask tl store out_ptr + offsets output + mask=mask torch compile fullgraph=True backend=backend dynamic=dynamic f x y z output = torch empty_like x output = torch empty_like x n_elements = output numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE add_kernel grid x y output output n_elements BLOCK_SIZE= The z intentional we re testing training output output z x = torch randn requires_grad=True device=GPU_TYPE y = torch randn requires_grad=True device=GPU_TYPE z = torch randn requires_grad=True device=GPU_TYPE out out out = f x y z assertEqual out x + y assertEqual out x + y + assertEqual out z requires_gpu common_utils parametrize dynamic False True common_utils parametrize tma_version new old test_tma_capture_and_functionalize dynamic tma_version tma_version == new has_triton_tensor_descriptor_host_tma skipTest requires triton tools tensor_descriptor TMA support tma_version == old has_triton_experimental_host_tma skipTest requires triton tools experimental_descriptor TMA support torch _higher_order_ops triton_kernel_wrap kernel_side_table kernel_side_table reset_table kernel = add_kernel_with_tma_ d_new_api tma_version == new add_kernel_with_tma_ d_old_api f b BLOCK_SIZE = out = torch zeros_like n_elements = out numel desc_a desc_b desc_out = create_tensor_descriptor_shim t BLOCK_SIZE new_api= tma_version == new t b out grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE kernel grid desc_a desc_b desc_out BLOCK_SIZE=BLOCK_SIZE out = torch randn device=GPU_TYPE b = torch randn device=GPU_TYPE backend = torch _dynamo testing AotEagerAndRecordGraphs _ = f b torch compile f fullgraph=True backend=backend dynamic=dynamic b dynamic tma_version == new assertExpectedInline backend fw_graphs code strip \ forward arg _ arg _ arg _ zeros_like = torch ops aten zeros_like default arg _ pin_memory = False add_ = arg _ + arg _ = None sub_ = add_ - add_ = None floordiv = sub_ sub_ = None triton_kernel_wrapper_functional_proxy = torch ops higher_order triton_kernel_wrapper_functional kernel_idx = constant_args_idx = grid = floordiv tma_descriptor_metadata = in_desc_ptr stable in_desc_ptr stable out_desc_ptr stable kwargs = in_desc_ptr arg _ in_desc_ptr arg _ out_desc_ptr zeros_like tensors_to_clone = out_desc_ptr floordiv = arg _ = arg _ = zeros_like = None getitem = triton_kernel_wrapper_functional_proxy out_desc_ptr triton_kernel_wrapper_functional_proxy = None getitem tma_version == old assertExpectedInline backend fw_graphs code strip \ forward arg _ arg _ arg _ zeros_like = torch ops aten zeros_like default arg _ pin_memory = False add_ = arg _ + sub_ = add_ - add_ = None floordiv = sub_ sub_ = None triton_kernel_wrapper_functional_proxy = torch ops higher_order triton_kernel_wrapper_functional kernel_idx = constant_args_idx = grid = floordiv tma_descriptor_metadata = in_desc_ptr experimental arg _ in_desc_ptr experimental arg _ out_desc_ptr experimental arg _ kwargs = in_desc_ptr arg _ in_desc_ptr arg _ out_desc_ptr zeros_like tensors_to_clone = out_desc_ptr floordiv = arg _ = arg _ = arg _ = zeros_like = None getitem = triton_kernel_wrapper_functional_proxy out_desc_ptr triton_kernel_wrapper_functional_proxy = None getitem tma_version == new assertExpectedInline backend fw_graphs code strip \ forward arg _ arg _ zeros_like = torch ops aten zeros_like default arg _ pin_memory = False triton_kernel_wrapper_functional_proxy = torch ops higher_order triton_kernel_wrapper_functional kernel_idx = constant_args_idx = grid = tma_descriptor_metadata = in_desc_ptr stable in_desc_ptr stable out_desc_ptr stable kwargs = in_desc_ptr arg _ in_desc_ptr arg _ out_desc_ptr zeros_like tensors_to_clone = out_desc_ptr arg _ = arg _ = zeros_like = None getitem = triton_kernel_wrapper_functional_proxy out_desc_ptr triton_kernel_wrapper_functional_proxy = None getitem tma_version == old assertExpectedInline backend fw_graphs code strip \ forward arg _ arg _ zeros_like = torch ops aten zeros_like default arg _ pin_memory = False triton_kernel_wrapper_functional_proxy = torch ops higher_order triton_kernel_wrapper_functional kernel_idx = constant_args_idx = grid = tma_descriptor_metadata = in_desc_ptr experimental in_desc_ptr experimental out_desc_ptr experimental kwargs = in_desc_ptr arg _ in_desc_ptr arg _ out_desc_ptr zeros_like tensors_to_clone = out_desc_ptr arg _ = arg _ = zeros_like = None getitem = triton_kernel_wrapper_functional_proxy out_desc_ptr triton_kernel_wrapper_functional_proxy = None getitem requires_gpu common_utils parametrize after_data_ptr False True common_utils parametrize after_create_desc False True common_utils parametrize tma_version new old test_tma_graph_breaks after_data_ptr after_create_desc tma_version tma_version == new has_triton_tensor_descriptor_host_tma skipTest requires triton tools tensor_descriptor TMA support tma_version == old has_triton_experimental_host_tma skipTest requires triton tools experimental_descriptor TMA support kernel = add_kernel_with_tma_ d_new_api tma_version == new add_kernel_with_tma_ d_old_api f b BLOCK_SIZE = out = torch zeros_like n_elements = out numel after_data_ptr torch _dynamo graph_break descs = create_tensor_descriptor_shim t BLOCK_SIZE new_api= tma_version == new t b out after_create_desc torch _dynamo graph_break grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE kernel grid descs BLOCK_SIZE=BLOCK_SIZE out = torch randn device=GPU_TYPE b = torch randn device=GPU_TYPE expected_out = + b eager_out = f b compiled_out = torch compile f fullgraph=False backend= eager dynamic=False b assertEqual eager_out expected_out assertEqual compiled_out expected_out requires_gpu common_utils parametrize dynamic False True common_utils parametrize backend eager aot_eager inductor common_utils parametrize tma_version new old test_tma_descriptor_ d dynamic backend tma_version tma_version == new has_triton_tensor_descriptor_host_tma skipTest requires triton tools tensor_descriptor TMA support tma_version == old has_triton_experimental_host_tma skipTest requires triton tools experimental_descriptor TMA support kernel = add_kernel_with_tma_ d_new_api tma_version == new add_kernel_with_tma_ d_old_api f b BLOCK_SIZE = out = torch zeros_like n_elements = out numel desc_a desc_b desc_out = create_tensor_descriptor_shim t BLOCK_SIZE new_api= tma_version == new t b out grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE kernel grid desc_a desc_b desc_out BLOCK_SIZE=BLOCK_SIZE out = torch randn device=GPU_TYPE b = torch randn device=GPU_TYPE expected_out = + b eager_out = f b compiled_out = torch compile f fullgraph=True backend=backend dynamic=dynamic b assertEqual eager_out expected_out assertEqual compiled_out expected_out requires_gpu common_utils parametrize tma_version new old test_tma_descriptor_dedup tma_version tma_version == new has_triton_tensor_descriptor_host_tma skipTest requires triton tools tensor_descriptor TMA support tma_version == old has_triton_experimental_host_tma skipTest requires triton tools experimental_descriptor TMA support kernel = add_kernel_with_tma_ d_new_api tma_version == new add_kernel_with_tma_ d_old_api f BLOCK_SIZE = out = torch zeros_like n_elements = out numel desc_a desc_out = create_tensor_descriptor_shim t BLOCK_SIZE new_api= tma_version == new t out grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE kernel grid desc_a desc_a desc_out BLOCK_SIZE=BLOCK_SIZE out = torch randn device=GPU_TYPE expected_out = + eager_out = f compiled_out code = run_and_get_code torch compile f fullgraph=True backend= inductor dynamic=True assertEqual eager_out expected_out assertEqual compiled_out expected_out calls one two inputs dedupped one output tma_version == new assertEqual code count TensorDescriptor from_tensor assertEqual code count create_ d_tma_descriptor requires_gpu common_utils parametrize dynamic False True common_utils parametrize backend eager aot_eager common_utils parametrize tma_version new old test_tma_descriptor_ d dynamic backend tma_version tma_version == new has_triton_tensor_descriptor_host_tma skipTest requires triton tools tensor_descriptor TMA support tma_version == old has_triton_experimental_host_tma skipTest requires triton tools experimental_descriptor TMA support kernel = add_kernel_with_tma_ d_new_api tma_version == new add_kernel_with_tma_ d_old_api f b BLOCK_SIZE_X = BLOCK_SIZE_Y = out = torch zeros_like x_size y_size = out size desc_a desc_b desc_out = create_tensor_descriptor_shim t BLOCK_SIZE_X BLOCK_SIZE_Y new_api= tma_version == new t b out grid = lambda meta triton cdiv x_size meta BLOCK_SIZE_X triton cdiv y_size meta BLOCK_SIZE_Y kernel grid desc_a desc_b desc_out BLOCK_SIZE_X=BLOCK_SIZE_X BLOCK_SIZE_Y=BLOCK_SIZE_Y out = torch randn device=GPU_TYPE b = torch randn device=GPU_TYPE expected_out = + b eager_out = f b compiled_out = torch compile f fullgraph=True backend=backend dynamic=dynamic b assertEqual eager_out expected_out assertEqual compiled_out expected_out requires_gpu common_utils parametrize backend eager aot_eager inductor test_triton_kernel_num_ctas backend triton jit kernel X torch compile fullgraph=True backend=backend f x kernel x num_ctas= kernel run x num_ctas= grid= warmup=False x msg = Passing num_ctas directly Triton kernel supported Please use Config triton autotune instead assertRaisesRegex torch _dynamo exc Unsupported msg x = torch randn device=GPU_TYPE f x requires_gpu common_utils parametrize backend eager aot_eager inductor test_triton_kernel_special_kwargs_without_autotune backend triton jit add_kernel in_ptr in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = x + y tl store out_ptr + offsets output mask=mask torch compile fullgraph=True backend=backend f x y output = torch zeros_like x n_elements = output numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE add_kernel grid x y output n_elements BLOCK_SIZE= num_warps= num_stages= output x = torch randn device=GPU_TYPE f x x requires_gpu common_utils parametrize backend eager aot_eager inductor common_utils parametrize autotune_at_compile_time True False test_triton_kernel_restore_value backend autotune_at_compile_time autotune_at_compile_time backend = inductor raise unittest SkipTest compile-time autotuning only exists inductor triton autotune configs= triton Config BLOCK_SIZE num_stages= num_warps= triton Config BLOCK_SIZE num_stages= num_warps= key= restore_value= in_ptr triton jit increment_kernel in_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask output = x + tl store in_ptr + offsets output mask=mask torch compile fullgraph=True backend=backend f x n_elements = x numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE increment_kernel grid x n_elements=n_elements x x = torch rand device=GPU_TYPE prev = x clone inductor_config patch triton autotune_at_compile_time autotune_at_compile_time f x make sure x restored after autotuning torch testing assert_close x prev + requires_gpu parametrize dtype torch float torch float torch float test_triton_kernel_float _constant dtype f x x x shape x = torch ones device=GPU_TYPE dtype=dtype eager_out = f x compiled_out = torch compile f dynamic=True x assertEqual compiled_out eager_out TODO enable test case XPU requires_cuda_and_triton parametrize cfg normal cpp_wrapper test_triton_kernel_dtype_view cfg https github com pytorch pytorch issues cfg == normal config_kwargs = cpp_wrapper False cfg == cpp_wrapper config_kwargs = cpp_wrapper True inductor_config patch config_kwargs triton jit _triton_kernel out_ptr numel BLOCK_SIZE tl constexpr pid = tl program_id offsets = BLOCK_SIZE pid + tl arange BLOCK_SIZE mask = offsets numel ones = tl full BLOCK_SIZE tl float tl store out_ptr + offsets ones mask fn x buf = torch empty x shape device=x device dtype=torch float buf view should view sharing same storage buf bfloat_buf = buf view dtype=torch bfloat BLOCK_SIZE = numel = buf numel grid = triton cdiv numel BLOCK_SIZE _triton_kernel grid bfloat_buf numel BLOCK_SIZE buf bfloat_buf fn_c = torch compile fn x = torch randn device=GPU_TYPE out_c = fn_c x out_e = fn x expect view actual view sharing same data original buffer verify first true eager output assertEqual out_e data_ptr out_e data_ptr also compiled output assertEqual out_c data_ptr out_c data_ptr assertEqual out_e out_c assertEqual out_e out_c TODO enable test case XPU requires_gpu test_i _input The i seed input needs marked i i triton jit triton_add_noise_ x_ptr y_ptr seed numel BLOCK_SIZE tl constexpr pid = tl program_id offsets = pid BLOCK_SIZE + tl arange BLOCK_SIZE x = tl load x_ptr + offsets mask= offsets numel rnd = tl rand seed offsets res = x + rnd tl store y_ptr + offsets res mask= offsets numel add_noise x seed y = torch empty_like x numel = x numel BLOCK_SIZE = grid meta triton cdiv numel meta BLOCK_SIZE triton_add_noise_ grid x y seed numel BLOCK_SIZE y fn x x = x x seed = torch randint low= high= size= dtype=torch int item add_noise x seed inp = torch rand device=GPU_TYPE torch _dynamo mark_dynamic inp fn_c = torch compile fn fullgraph=True dynamo_config patch capture_scalar_outputs=True res = fn_c inp assertTrue res res = all item requires_gpu parametrize wrapped False True parametrize autotune False True test_constexpr_dynamic_shapes wrapped autotune https github com pytorch pytorch issues triton jit triton_ x_ptr y_ptr NUMEL tl constexpr IS_ODD tl constexpr BLOCK_SIZE tl constexpr pid = tl program_id offsets = BLOCK_SIZE pid + tl arange BLOCK_SIZE mask = offsets NUMEL data = tl load x_ptr + offsets mask result = data data IS_ODD result = result + tl store y_ptr + offsets result mask autotune triton_ = triton autotune triton Config kwargs= BLOCK_SIZE triton Config kwargs= BLOCK_SIZE key= triton_ triton_kernel_impl x torch Tensor - torch Tensor y = torch empty_like x numel = x numel args = x y numel numel == autotune args append BLOCK_SIZE grid meta triton cdiv numel meta BLOCK_SIZE wrapped capture_triton triton_ grid args triton_ grid args y wrapped triton_kernel = torch library triton_op constexpr_test square triton_kernel_impl mutates_args= triton_kernel = triton_kernel_impl fn x triton_kernel x fn_c = torch compile fn dynamic=True x = torch randn + device=GPU_TYPE res = fn_c x assertEqual x x res x = torch randn + device=GPU_TYPE res = fn_c x assertEqual x x res requires_gpu test_triton_kernel_none_args https github com pytorch pytorch issues triton autotune configs= triton Config BLOCK_SIZE num_stages= num_warps= triton Config BLOCK_SIZE num_stages= num_warps= key= n_elements triton jit sin_kernel in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements in_ptr None x = tl load in_ptr + offsets mask=mask x = output = tl sin x tl store out_ptr + offsets output mask=mask sin_triton x out n_elements = out numel sin_kernel n_elements x out n_elements x = torch randn device=GPU_TYPE out = torch empty_like x out_compiled = torch empty_like x sin_triton_compiled = torch compile fullgraph=True sin_triton sin_triton x out sin_triton_compiled x out_compiled assertEqual out out_compiled sin_triton None out sin_triton_compiled None out_compiled assertEqual out out_compiled requires_gpu test_triton_kernel_global_constexpr triton jit triton_ in_ptr out_ptr BLOCK_SIZE tl constexpr pid = tl program_id offsets = pid BLOCK_SIZE + tl arange BLOCK_SIZE x = tl load in_ptr + offsets output = x + FLOAT_CONSTANT_C tl store out_ptr + offsets output fn x y = torch empty_like x BLOCK_SIZE = grid = triton cdiv x numel BLOCK_SIZE triton_ grid x y BLOCK_SIZE y make sure FLOAT_CONSTANT_C NOT annotated assertFalse FLOAT_CONSTANT_C globals get __annotations__ sanity check STRING_CONSTANT_C _should_ annotated assertTrue STRING_CONSTANT_C globals get __annotations__ x = torch randn device=GPU_TYPE expected = x + actual = torch compile fn x assertEqual expected actual requires_gpu unittest skipIf triton_version_uses_attrs_dict Test only valid new triton versions where attrs represented raw dict test_triton_attrs_dict_equal_ _None_format triton jit triton_ in_ptr out_ptr numel add_amount BLOCK_SIZE tl constexpr offsets = tl arange BLOCK_SIZE x = tl load in_ptr + offsets mask= offsets numel output = x x add_amount None output = output + add_amount tl store out_ptr + offsets output mask= offsets numel fn x y = torch empty_like x BLOCK_SIZE = grid = triton_ grid x y x numel None BLOCK_SIZE y x = torch full device=GPU_TYPE expected = fn x fn_c = torch compile fn res code = run_and_get_code fn_c x assertEqual expected res FileCheck check triton_meta= check constants check numel run code FileCheck check triton_meta= check constants check add_amount None run code FileCheck check triton_meta= check constants check BLOCK_SIZE run code FileCheck check triton_meta= check signature check numel constexpr run code FileCheck check triton_meta= check signature check add_amount constexpr run code FileCheck check triton_meta= check signature check BLOCK_SIZE constexpr run code requires_gpu inductor_config patch triton autotune_at_compile_time True parametrize quotes single double test_kernel_with_docstring quotes kernel = kernel_with_docstring_single_quotes quotes == single kernel_with_docstring_double_quotes https github com pytorch pytorch issues fn sz x = torch empty sz device=GPU_TYPE BLOCK_SIZE = grid = triton cdiv sz BLOCK_SIZE kernel grid x sz BLOCK_SIZE x actual = fn expected = torch compile fn fullgraph=True assertEqual actual expected requires_gpu skipIfRocm skipIfXpu inductor_config patch triton autotune_at_compile_time True parametrize quotes single double test_kernel_inline_asm quotes kernel = kernel_inline_asm_single_quotes quotes == single kernel_inline_asm_double_quotes https github com pytorch pytorch issues fn inp sz = inp size x = torch empty sz device=GPU_TYPE BLOCK_SIZE = grid = triton cdiv sz BLOCK_SIZE kernel grid inp x sz BLOCK_SIZE x inp = torch randn device=GPU_TYPE actual = fn inp expected = torch compile fn fullgraph=True inp assertEqual actual expected requires_gpu inductor_config patch emulate_precision_casts True test_triton_kernel_emulate_precision_unaffected triton jit triton_ in_ptr out_ptr numel add_amount BLOCK_SIZE tl constexpr offsets = tl arange BLOCK_SIZE x = tl load in_ptr + offsets mask= offsets numel output = x x add_amount None output = output + add_amount tl store out_ptr + offsets output mask= offsets numel fn x y = torch empty_like x BLOCK_SIZE = grid = triton_ grid x y x numel None BLOCK_SIZE y t = torch rand device=GPU_TYPE fn = torch compile fn _ code = run_and_get_code fn t assertTrue enable_fp_fusion code requires_gpu inductor_config patch emulate_precision_casts True inductor_config patch max_autotune_gemm_backends TRITON test_triton_kernel_emulate_precision_mm_kernels_do_not_change torch _inductor utils run_and_get_code torch compile mode= max-autotune fn b b t = torch rand device=GPU_TYPE t = torch rand device=GPU_TYPE try _ code = run_and_get_code fn t t assertTrue enable_fp_fusion code except Exception e NoValidChoicesError str e raise unittest SkipTest where inductor has no triton mm kernels available test meaningless e raise make_mutation_test fn requires_gpu test_fn torch _higher_order_ops triton_kernel_wrap identify_mutated_tensors kernel inputs tma_descriptor_metadata outputs = fn assertListEqual identify_mutated_tensors kernel inputs tma_descriptor_metadata outputs test_fn Triton codegen suffers scoping issues Define helpers here HAS_GPU triton jit helper_id p p triton jit helper_add_and_out x y out_ptr x + y out_ptr MutationTests torch _inductor test_case TestCase Tests injected below make_mutation_test test_out_of_order_kernel triton jit add_kernel_out_of_order in_ptr n_elements in_ptr out_ptr BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = x + y tl store out_ptr + offsets output mask=mask t = torch randn add_kernel_out_of_order in_ptr t n_elements in_ptr t out_ptr t BLOCK_SIZE out_ptr make_mutation_test test_out_of_order_kernel_call triton jit add_kernel_out_of_order_fn in_ptr n_elements in_ptr out_ptr BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements add_kernel_out_of_order_fn in_ptr in_ptr n_elements out_ptr BLOCK_SIZE=BLOCK_SIZE t = torch randn add_kernel_out_of_order_fn in_ptr t n_elements in_ptr t out_ptr t BLOCK_SIZE out_ptr make_mutation_test test_reduce_sum triton jit reduce_sum_kernel a_ptr c_ptr stride_am stride_an offs_am = tl arange offs_an = tl arange a_ptrs = a_ptr + offs_am None stride_am + offs_an None stride_an = tl load a_ptrs m = tl sum axis= tl store c_ptr + tl arange m t = torch randn kernel = reduce_sum_kernel kwargs = a_ptr t c_ptr t stride_am stride_an TODO aakhundov tt reduce now supported only new MLIR-based Triton analysis pass old TTIR string parsing-based one remove gating use c_ptr ` expected ` after new Triton pin lands both OSS internally ttir_module _ = generate_ttir kernel kwargs tma_descriptor_metadata= hasattr ttir_module walk MLIR-based Triton analysis pass expected = c_ptr TTIR string parsing-based Triton analysis pass expected = a_ptr c_ptr kernel kwargs expected make_mutation_test test_argmax triton jit argmax_kernel a_ptr c_ptr stride_am stride_an offs_am = tl arange offs_an = tl arange a_ptrs = a_ptr + offs_am None stride_am + offs_an None stride_an = tl load a_ptrs m = tl argmax axis= tl store c_ptr + tl arange m t = torch randn kernel = argmax_kernel kwargs = a_ptr t c_ptr t stride_am stride_an TODO aakhundov tt reduce now supported only new MLIR-based Triton analysis pass old TTIR string parsing-based one remove gating use c_ptr ` expected ` after new Triton pin lands both OSS internally ttir_module _ = generate_ttir kernel kwargs tma_descriptor_metadata= hasattr ttir_module walk MLIR-based Triton analysis pass expected = c_ptr TTIR string parsing-based Triton analysis pass expected = a_ptr c_ptr kernel kwargs expected requires_gpu test_triton_kernel_inference_mode f x y out n_elements = x numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE add_kernel grid x y out n_elements BLOCK_SIZE= torch inference_mode x = torch ones device=GPU_TYPE y = torch ones device=GPU_TYPE out_ref = torch zeros_like x out_test = torch zeros_like x f x y out_ref torch compile f x y out_test assertEqual out_ref out_test make_mutation_test test_cumsum triton jit cumsum_kernel in_ptr out_ptr XBLOCK tl constexpr RBLOCK tl constexpr rindex = tl arange RBLOCK None xindex = tl arange XBLOCK None data = tl load in_ptr + rindex scan = tl cumsum data expected_max = tl sum data tl device_assert scan = expected_max tl store out_ptr + xindex RBLOCK + rindex scan t = torch randn kernel = cumsum_kernel kwargs = in_ptr t out_ptr t XBLOCK RBLOCK TODO aakhundov tt scan now supported only new MLIR-based Triton analysis pass old TTIR string parsing-based one remove gating use out_ptr ` expected ` after new Triton pin lands both OSS internally ttir_module _ = generate_ttir kernel kwargs tma_descriptor_metadata= hasattr ttir_module walk MLIR-based Triton analysis pass expected = out_ptr TTIR string parsing-based Triton analysis pass expected = in_ptr out_ptr kernel kwargs expected make_mutation_test test_fn_call_one_return triton jit add_kernel_with_fn_call in_ptr in_ptr n_elements out_ptr BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = x + y out = helper_id out_ptr tl store out + offsets output mask=mask t = torch randn add_kernel_with_fn_call in_ptr t in_ptr t n_elements out_ptr t BLOCK_SIZE out_ptr make_mutation_test test_fn_call_multi_return triton jit add_kernel_with_fn_call in_ptr in_ptr n_elements out_ptr BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output out = helper_add_and_out x y out_ptr tl store out + offsets output mask=mask t = torch randn add_kernel_with_fn_call in_ptr t in_ptr t n_elements out_ptr t BLOCK_SIZE out_ptr make_mutation_test test_nested_cond_op_kernel triton jit nested_cond_op_kernel in_ptr in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask tl program_id == tl program_id == output = x + y tl store out_ptr + offsets output mask=mask pass t = torch randn nested_cond_op_kernel in_ptr t in_ptr t out_ptr t n_elements BLOCK_SIZE out_ptr make_mutation_test test_add_for_loop triton jit add_ _times_kernel in_ptr in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = tl zeros n_elements dtype=tl float _ range output += x + y tl store out_ptr + offsets output mask=mask t = torch randn add_ _times_kernel in_ptr t in_ptr t out_ptr t n_elements BLOCK_SIZE out_ptr make_mutation_test test_add_for_loop triton jit add_ _time_kernel in_ptr in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask i range BLOCK_SIZE i = tl multiple_of i output = x + y tl store out_ptr + offsets output mask=mask t = torch randn add_ _time_kernel in_ptr t in_ptr t out_ptr t n_elements BLOCK_SIZE out_ptr make_mutation_test test_add_nested_for_loop triton jit add_ _times_kernel in_ptr in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = tl zeros n_elements dtype=tl float _ range _ range output += x + y tl store out_ptr + offsets output mask=mask t = torch randn add_ _times_kernel in_ptr t in_ptr t out_ptr t n_elements BLOCK_SIZE out_ptr make_mutation_test test_add_nested_for_loop_multi_return triton jit add_ _times_kernel in_ptr in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = tl zeros n_elements dtype=tl float output = tl zeros n_elements dtype=tl float _ range _ range output += y output += x output = output + output tl store out_ptr + offsets output mask=mask t = torch randn add_ _times_kernel in_ptr t in_ptr t out_ptr t n_elements BLOCK_SIZE out_ptr make_mutation_test test_labels triton jit kernel_with_label in_ptr in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= pid block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = x + y tl store out_ptr + offsets output mask=mask t = torch randn kernel_with_label in_ptr t in_ptr t out_ptr t n_elements BLOCK_SIZE out_ptr make_mutation_test test_for_loop_arg triton jit fwd_kernel X_ptr W _ptr b _ptr O_ptr M tl constexpr C tl constexpr C tl constexpr BLOCK_SIZE_M tl constexpr BLOCK_SIZE_C tl constexpr Get program ids pid_m = tl program_id Compute offsets offs_c = tl arange C offs_m = pid_m BLOCK_SIZE_M + tl arange BLOCK_SIZE_M Load input data x_block_ptr = X_ptr + offs_m None C + offs_c None x = tl load x_block_ptr Compute gating c range tl cdiv C BLOCK_SIZE_C Compute block pointers offs_c = c BLOCK_SIZE_C + tl arange BLOCK_SIZE_C o_block_ptr = O_ptr + offs_m None C + offs_c None w _block_ptr = W _ptr + offs_c None C + offs_c None b _block_ptr = b _ptr + offs_c Compute output w = tl load w _block_ptr b = tl load b _block_ptr o = tl dot x w allow_tf =False o += b None Store output tl store o_block_ptr o t = torch randn fwd_kernel X_ptr t W _ptr t b _ptr t O_ptr t M C C BLOCK_SIZE_M BLOCK_SIZE_C O_ptr make_mutation_test test_for_loop_arg_ triton jit fwd_kernel x_ptr o_ptr M N stride_m stride_n BLOCK_B tl constexpr BLOCK_M tl constexpr BLOCK_N tl constexpr Get program ids pid_m = tl program_id X_block_ptr = tl make_block_ptr base=x_ptr shape= M N strides= stride_m stride_n offsets= block_shape= BLOCK_M BLOCK_N order= O_block_ptr = tl make_block_ptr base=o_ptr shape= M N strides= stride_m stride_n offsets= block_shape= BLOCK_M BLOCK_N order= _ range BLOCK_B x = tl load X_block_ptr tl store O_block_ptr x X_block_ptr = tl advance X_block_ptr BLOCK_M O_block_ptr = tl advance O_block_ptr BLOCK_M t = torch randn o = torch empty_like t B M N = t shape fwd_kernel x_ptr t o_ptr o M M N N stride_m N stride_n BLOCK_B B BLOCK_M M BLOCK_N N o_ptr make_mutation_test test_while_loop triton jit fwd_kernel x_ptr o_ptr M N stride_m stride_n BLOCK_B tl constexpr BLOCK_M tl constexpr BLOCK_N tl constexpr Get program ids pid_m = tl program_id X_block_ptr = tl make_block_ptr base=x_ptr shape= M N strides= stride_m stride_n offsets= block_shape= BLOCK_M BLOCK_N order= O_block_ptr = tl make_block_ptr base=o_ptr shape= M N strides= stride_m stride_n offsets= block_shape= BLOCK_M BLOCK_N order= i = while i BLOCK_B x = tl load X_block_ptr tl store O_block_ptr x X_block_ptr = tl advance X_block_ptr BLOCK_M O_block_ptr = tl advance O_block_ptr BLOCK_M i += t = torch randn o = torch empty_like t B M N = t shape fwd_kernel x_ptr t o_ptr o M M N N stride_m N stride_n BLOCK_B B BLOCK_M M BLOCK_N N o_ptr make_mutation_test test_branch_with_multiple_yield_args triton jit branch_with_multiple_yield_args in_ptr in_ptr out_ptr conditional_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements conditional = tl load conditional_ptr conditional = in_ptr + = in_ptr + out = out_ptr + = in_ptr = in_ptr out = out_ptr x = tl load + offsets mask=mask y = tl load + offsets mask=mask tl store out + offsets x + y mask=mask x = torch randn y = torch randn out = torch zeros conditional = torch tensor True branch_with_multiple_yield_args in_ptr x in_ptr y out_ptr out conditional_ptr conditional n_elements BLOCK_SIZE out_ptr test_get_tma_stores torch _higher_order_ops triton_kernel_wrap get_tma_stores Intermediate Op Param functions = helper Intermediate idx= Op tt reinterpret_tensor_descriptor None Param idx= Intermediate idx= main Intermediate idx=- Op tt call helper Param idx= Param idx= Intermediate idx=- assertEqual get_tma_stores functions helper set assertEqual get_tma_stores functions main set functions helper Intermediate idx=- = Op tt experimental_descriptor_store None Intermediate idx= Param idx= Intermediate idx=- get_tma_stores reset assertEqual get_tma_stores functions helper Param idx= Intermediate idx= assertEqual get_tma_stores functions main Param idx= unittest skipIf has_triton_experimental_host_tma requires experimental TMA descriptor API make_mutation_test test_add_kernel_on_device_tma_old_api = torch randn b = torch randn c = torch empty workspace = torch empty dtype=torch int add_kernel_on_device_tma_old_api a_ptr b_ptr b c_ptr c m n workspace workspace BLOCK_SIZE c_ptr workspace unittest skipIf has_triton_tensor_descriptor_host_tma requires TensorDescriptor API Triton make_mutation_test test_add_kernel_on_device_tma_new_api = torch randn b = torch randn c = torch empty workspace = torch empty dtype=torch int Not used new API kept consistency add_kernel_on_device_tma_new_api a_ptr b_ptr b c_ptr c m n workspace workspace BLOCK_SIZE c_ptr HAS_GPU t = torch randn tt = torch randn tests = add_kernel in_ptr t in_ptr t out_ptr t n_elements BLOCK_SIZE out_ptr add_kernel_ d_autotuned in_ptr t in_ptr t out_ptr t x_elements y_elements out_ptr indirection_kernel in_ptr t out_ptr t n_elements BLOCK_SIZE ACTIVATION mul _inplace_kernel in_ptr out_ptr indirection_kernel in_ptr t out_ptr t n_elements BLOCK_SIZE ACTIVATION add_kernel out_ptr mul _inplace_kernel ptr t n_elements BLOCK_SIZE ptr inline_asm_kernel_is_pure_true X t Y t Z t n BLOCK Z inline_asm_kernel_is_pure_false X t Y t Z t n BLOCK X Y Z add_kernel_with_block_ptr x_ptr t y_ptr t output_ptr t n_elements BLOCK_SIZE output_ptr kernel_with_block_ptr_ d x_ptr tt output_ptr tt n_elements BLOCK_SIZE output_ptr add_kernel_with_import in_ptr t in_ptr t out_ptr t n_elements BLOCK_SIZE out_ptr atomic_add_kernel in_ptr t in_ptr t out_ptr t n_elements BLOCK_SIZE out_ptr add_ _times_kernel in_ptr t in_ptr t out_ptr t n_elements BLOCK_SIZE out_ptr cond_op_kernel in_ptr t in_ptr t out_ptr t n_elements BLOCK_SIZE out_ptr kernel inputs tma_descriptor_metadata outputs tests fn = make_mutation_test Add default arguments avoid Python lambda capture pitfall This forces capture lambda creation lambda kernel=kernel inputs=inputs tma_descriptor_metadata=tma_descriptor_metadata outputs=outputs kernel inputs tma_descriptor_metadata outputs name = f test_mutations_ kernel fn __name__ Poor way make test names unique while name MutationTests __dict__ name += setattr MutationTests name fn CustomOpTests torch _inductor test_case TestCase Tests custom ops wrapping triton kernels requires_gpu common_utils parametrize autotuned False True common_utils parametrize dynamic False True test_add_kernel autotuned dynamic torch _inductor utils run_and_get_code libname = my_cool_namespace opname = my_triton_operator torch library triton_op f libname opname mutates_args= add x torch Tensor y torch Tensor - torch Tensor output = torch empty_like x n_elements = output numel grid meta triton cdiv n_elements meta BLOCK_SIZE autotuned capture_triton add_kernel_autotuned grid x y output n_elements capture_triton add_kernel grid x y output n_elements output f x y add x y x = torch randn device=GPU_TYPE y = torch randn device=GPU_TYPE out = f x y expected = x + y assertEqual out expected out_compiled codes = run_and_get_code torch compile f dynamic=dynamic x y assertEqual out_compiled expected assertEqual len codes Check we decomposed operator away code = \n join codes assertNotIn libname code assertNotIn opname code requires_gpu test_subclass libname = my_cool_namespace opname = my_triton_operator torch library triton_op f libname opname mutates_args= add x torch Tensor y torch Tensor - torch Tensor output = torch empty_like x n_elements = output numel grid meta triton cdiv n_elements meta BLOCK_SIZE capture_triton add_kernel grid x y output n_elements output f x y add x y x = torch randn device=GPU_TYPE y = torch randn device=GPU_TYPE x = torch randn device=GPU_TYPE y = torch randn device=GPU_TYPE torch testing _internal two_tensor TwoTensor x = TwoTensor x x y = TwoTensor y y out = torch compile f fullgraph=True x y expected = f x y assertEqual out expected assertEqual out b expected b requires_gpu dynamo_config patch recompile_limit test_triton_dynamic_grid_no_recompile libname = my_cool_namespace opname = my_triton_operator torch library triton_op f libname opname mutates_args= add x torch Tensor y torch Tensor - torch Tensor output = torch empty_like x n_elements = output numel capture_triton add_kernel n_elements x y output n_elements output torch compile fullgraph=True dynamic=True f x add x x f torch randn device=GPU_TYPE f torch randn device=GPU_TYPE unittest skipIf has_triton_package requires triton test_capture_triton_meta triton triton language tl triton jit add_kernel in_ptr in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = x + y tl store out_ptr + offsets output mask=mask torch library triton_op mylib add mutates_args= add x torch Tensor y torch Tensor - torch Tensor output = torch empty_like x n_elements = output numel grid meta triton cdiv n_elements meta BLOCK_SIZE capture_triton add_kernel grid x y output n_elements output f x y add x y x = torch randn device= meta y = torch randn device= meta out = f x y expected = torch empty_like x assertEqual out expected requires_gpu test_wrap_triton_disabled_in_triton_op triton manual triton language tl manual triton jit add_kernel in_ptr in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = x + y tl store out_ptr + offsets output mask=mask add_kernel_decorated = torch library wrap_triton add_kernel status = torch library triton_op mylib add mutates_args= add x torch Tensor y torch Tensor - torch Tensor torch _higher_order_ops triton_kernel_wrap status append torch _library triton is_wrap_triton_enabled capture_triton should kernel directly disabled result = torch library wrap_triton add_kernel assertIs result add_kernel Smoke test check capture_triton disabled still does something output = torch empty_like x output = torch empty_like x n_elements = output numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE add_kernel_decorated grid x y output n_elements BLOCK_SIZE= add_kernel_decorated run x y output n_elements BLOCK_SIZE= grid=grid warmup=False output + output x = torch randn device=GPU_TYPE y = torch randn device=GPU_TYPE z = add x y assertEqual status - False assertEqual z x + y requires_gpu common_utils parametrize variant triton_kernel custom_op mutable_custom_op test_preserves_strides variant triton triton language tl triton jit add_kernel in_ptr in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = x + y tl store out_ptr + offsets output mask=mask x = torch randn device=GPU_TYPE other = torch randn device=GPU_TYPE add_triton y z grid = z numel out = torch empty_like z memory_format=torch contiguous_format add_kernel grid y z out z numel BLOCK_SIZE= out _CustomPass PatternMatcherPass __init__ - None super __init__ __call__ g torch fx Graph apply g g = _CustomPass called = False register_graph_pattern CallFunctionVarArgs torch ops aten permute pass_dict=g _ match args kwargs flat_args spec = pytree tree_flatten args kwargs decomp flat_args args kwargs = pytree tree_unflatten flat_args spec torch ops mylib force_channels_last torch ops aten permute args kwargs nonlocal called called = True match replace_by_example decomp flat_args torch _inductor config torch library _scoped_library mylib FRAGMENT lib lib define force_channels_last Tensor x - Tensor tags= torch _C Tag flexible_layout impl x x clone memory_format=torch channels_last lib impl force_channels_last impl CompositeExplicitAutograd lib define add_op Tensor x Tensor y - Tensor impl x y add_triton x y meta x y torch empty_like y memory_format=torch contiguous_format lib impl add_op impl CompositeExplicitAutograd lib impl add_op meta Meta lib define add_out_op Tensor x Tensor y Tensor out - impl_out x y out grid = y numel add_kernel grid x y out y numel BLOCK_SIZE= lib impl add_out_op impl_out CompositeExplicitAutograd lib impl add_out_op lambda x y out None Meta f x other y = x transpose contiguous transpose z = y sin transpose variant == triton_kernel add_triton y z variant == custom_op torch ops mylib add_op default y z variant == mutable_custom_op out = torch empty_like y memory_format=torch contiguous_format torch ops mylib add_out_op y z out out raise AssertionError should hit config patch post_grad_custom_post_pass=g f_compile = torch compile f fullgraph=True assertEqual f x other f_compile x other assertTrue called requires_gpu common_utils parametrize dynamic False True common_utils parametrize autotune False True test_capture_triton_special_kwargs dynamic autotune triton jit add_kernel in_ptr in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = x + y tl store out_ptr + offsets output mask=mask autotune add_kernel = triton autotune configs= triton Config BLOCK_SIZE triton Config BLOCK_SIZE key= n_elements add_kernel f x y output = torch zeros_like x n_elements = output numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE autotune kwargs = kwargs = BLOCK_SIZE capture_triton add_kernel grid x y output n_elements num_warps= num_stages= kwargs output x = torch randn device=GPU_TYPE tracing_mode = symbolic dynamic fake result = f x x assertEqual result x + x functorch make_fx gm = make_fx f tracing_mode=tracing_mode x x assertEqual gm x x x + x skipIfWindows msg= AOTI Cpp_Wrapper have enabled Windows requires_gpu inductor_config patch cpp_wrapper True inductor_config patch triton autotune_at_compile_time True test_autotune_unbacked triton triton language tl get_op_configs triton Config BLOCK_M BLOCK_N BLOCK_K GROUP_M num_stages= num_warps= triton Config BLOCK_M BLOCK_N BLOCK_K GROUP_M num_stages= num_warps= triton autotune configs=get_op_configs key= N K triton jit op_zeros x_ptr w_ptr z_ptr M N K stride_xm stride_xk stride_wk stride_wn stride_zm stride_zn BLOCK_M tl constexpr BLOCK_N tl constexpr BLOCK_K tl constexpr GROUP_M tl constexpr ALLOW_TF tl constexpr pid = tl program_id axis= num_pid_m = tl cdiv M BLOCK_M num_pid_n = tl cdiv N BLOCK_N num_pid_in_group = GROUP_M num_pid_n group_id = pid num_pid_in_group first_pid_m = group_id GROUP_M group_size_m = min num_pid_m - first_pid_m GROUP_M pid_m = first_pid_m + pid group_size_m pid_n = pid num_pid_in_group group_size_m offs_m = tl arange BLOCK_M offs_n = tl arange BLOCK_N mask_m = pid_m BLOCK_M + offs_m None M mask_n = pid_n BLOCK_N + offs_n None N z_mask = mask_m mask_n z = z_ptr += pid_m tl int BLOCK_M stride_zm z_ptr += pid_n tl int BLOCK_N stride_zn z_ptrs = z_ptr + stride_zm offs_m None + stride_zn offs_n None tl store z_ptrs z mask=z_mask torch compile foo x w M K = x shape KB N = w shape assert K == KB f incompatible dimensions K KB z = torch empty M N device=x device dtype=x dtype grid META triton cdiv M META BLOCK_M triton cdiv N META BLOCK_N op_zeros grid x w z M N K x stride x stride w stride w stride z stride z stride ALLOW_TF =torch backends cuda matmul allow_tf z M K N = x = torch randn M K device=GPU_TYPE w = torch randn K N device=GPU_TYPE torch _dynamo decorators mark_unbacked x log_settings +output_code assertLogs logger= torch _inductor level=logging DEBUG log foo x w output = \n join record getMessage record log records correct grid example values updated per block size FileCheck check Compile-time auto-tuning block check PrecomputedGrid check + _launcher_s check + _launcher_s run output Triton adds required flags Autotuner object test PR https github com triton-lang triton pull requires_gpu test_autotune_no_pre_or_post_hook_user_defined triton runtime autotuner Autotuner init_to_zero name lambda nargs nargs name zero_ triton autotune configs= triton Config BLOCK_SIZE num_warps= num_stages= pre_hook=init_to_zero output_ptr pre_hook=init_to_zero output_ptr post_hook=init_to_zero output_ptr key= n_elements triton jit add_kernel x_ptr y_ptr output_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load x_ptr + offsets mask=mask y = tl load y_ptr + offsets mask=mask output = x + y tl atomic_add output_ptr + offsets output mask=mask add x torch Tensor y torch Tensor - torch Tensor output = torch ones x shape device=x device dtype=x dtype n_elements = output numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE add_kernel grid x y output n_elements output x = torch ones device=GPU_TYPE dtype=torch float y = torch ones device=GPU_TYPE dtype=torch float should always pass assert add x y mean == Problem add kernel assert user_defined_ flags properly set kernel before compilation assertEqual isinstance add_kernel Autotuner True hasattr add_kernel user_defined_pre_hook hasattr add_kernel user_defined_post_hook raise unittest SkipTest test requires Triton version = Autotuner user_defined hooks assertEqual add_kernel user_defined_pre_hook True assertEqual add_kernel user_defined_post_hook True should cause exception since pre_hook allowed msg = pre_hook post_hook supported triton Autotune triton Config assertRaisesRegex torch _dynamo exc Unsupported msg add_compiled = torch compile add mode= reduce-overhead fullgraph=True add_compiled x y mean requires_gpu common_utils parametrize backend eager aot_eager inductor common_utils parametrize autotune_at_compile_time True False test_triton_kernel_reset_to_zero backend autotune_at_compile_time autotune_at_compile_time backend = inductor raise unittest SkipTest compile-time autotuning only exists inductor triton autotune configs= triton Config BLOCK_SIZE num_stages= num_warps= triton Config BLOCK_SIZE num_stages= num_warps= triton Config BLOCK_SIZE num_stages= num_warps= key= reset_to_zero= increment_ptr triton jit increment_kernel in_ptr increment_ptr reset zero every time n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements in_ptr_vals = tl load in_ptr + offsets mask=mask increment_val = tl load increment_ptr + offsets mask=mask increment_val should always zero tl store in_ptr + offsets in_ptr_vals + increment_val mask=mask torch compile fullgraph=True backend=backend f x increment n_elements = x numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE increment_kernel grid x increment n_elements=n_elements x x = torch rand device=GPU_TYPE y = torch clone x increment = torch rand device=GPU_TYPE during autotuning x should change value inductor_config patch triton autotune_at_compile_time autotune_at_compile_time we will add rand single time x f x increment assertEqual y + increment x requires_gpu common_utils parametrize backend eager aot_eager inductor test_triton_single_autotune backend triton autotune configs= triton Config BLOCK_SIZE key= n_elements Currently autotuning decorator will never run We only support having single autotuning decorator each Triton kernel triton autotune configs= triton Config BLOCK_SIZE key= n_elements triton jit add_kernel x_ptr y_ptr output_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load x_ptr + offsets mask=mask y = tl load y_ptr + offsets mask=mask output = x + y tl store output_ptr + offsets output mask=mask add x torch Tensor y torch Tensor - torch Tensor output = torch ones x shape device=x device dtype=x dtype n_elements = output numel grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE add_kernel grid x y output n_elements output x = torch ones device=GPU_TYPE dtype=torch float y = torch ones device=GPU_TYPE dtype=torch float should cause exception since pre_hook allowed msg = Passing multiple triton autotune decorators supported Please use single triton autotune decorator instead assertRaisesRegex torch _dynamo exc Unsupported msg add_compiled = torch compile add mode= reduce-overhead fullgraph=True backend=backend add_compiled x y mean requires_gpu common_utils parametrize non_strict True False common_utils parametrize backend eager aot_eager inductor common_utils parametrize with_perf_model True False test_triton_kernel_prune_configs_by backend with_perf_model non_strict non-strict mode libname = my_cool_namespace opname = my_triton_operator records = early_config_prune configs named_args kwargs we need save records returned config records run_early_config_prune = True N kwargs kwargs N == records capture_kwargs = True named args dst src add_float dst named_args src named_args len named_args == records capture_named_args = True configs perf_model args kwargs records run_perf_model = True kwargs BLOCK_SIZE - with_perf_model prune_configs_by = perf_model perf_model top_k prune_configs_by = early_config_prune early_config_prune triton autotune configs= triton Config kwargs= BLOCK_SIZE triton Config kwargs= BLOCK_SIZE key= N prune_configs_by=prune_configs_by triton jit prune_by_kernel dst src add_float N BLOCK_SIZE tl constexpr offsets = tl program_id BLOCK_SIZE + tl arange BLOCK_SIZE x = tl load src + offsets mask=offsets N we only modify dst our perf_model applied BLOCK_SIZE selected BLOCK_SIZE == x = x + add_float tl store dst + offsets x mask=offsets N f dst torch Tensor src torch Tensor add_float float N int - None grid = lambda META triton cdiv N META BLOCK_SIZE non_strict torch library wrap_triton prune_by_kernel grid dst src add_float N=N prune_by_kernel grid dst src add_float N=N non_strict decorator = torch library triton_op f libname opname mutates_args= dst f we can just pass function f dynamo decorator = f compiled_f = torch compile decorator backend=backend N = src = torch randn N device=GPU_TYPE dst = torch empty N device=GPU_TYPE compiled_f dst src N with_perf_model when applying perf_model kwargs BLOCK_SIZE - largest config BLOCK_SIZE== selected assertEqual len records assertEqual src + dst without perf_model BLOCK_SIZE== result dst modified remains equal src assertEqual src dst assertEqual len records assertTrue records run_early_config_prune assertTrue records capture_kwargs assertTrue records capture_named_args requires_gpu common_utils parametrize backend eager aot_eager inductor common_utils parametrize with_perf_model True False test_triton_kernel_prune_configs_by_recompile backend with_perf_model We want recompile anyone changes configs autotuner object In short example following sequence events happens foo = torch compile bar call foo autotuner configs = new configs list call foo A recompile event should occur which we check Dynamo counters This tests we installing guards input objects properly We don t modify records here because we testing whether recompiles occur guards installed If we modified non-local records dict here would trigger recompile events early_config_prune configs named_args kwargs configs perf_model args kwargs kwargs BLOCK_SIZE - with_perf_model prune_configs_by = perf_model perf_model top_k prune_configs_by = early_config_prune early_config_prune triton autotune configs= triton Config kwargs= BLOCK_SIZE triton Config kwargs= BLOCK_SIZE key= N prune_configs_by=prune_configs_by triton jit prune_by_kernel dst src add_float N BLOCK_SIZE tl constexpr offsets = tl program_id BLOCK_SIZE + tl arange BLOCK_SIZE x = tl load src + offsets mask=offsets N Let s make sure we always select block size based our perf_model BLOCK_SIZE == x = x + add_float tl store dst + offsets x mask=offsets N torch _dynamo reset counter = torch _dynamo testing CompileCounterWithBackend backend=backend torch compile fullgraph=True backend=counter f dst src add_float N grid = lambda META triton cdiv N META BLOCK_SIZE prune_by_kernel grid dst src add_float N=N N = src = torch randn N device=GPU_TYPE dst = torch empty N device=GPU_TYPE first compilation prunes configs f dst src N assertEqual counter op_count f dst src N should trigger recompilation because we modified test touch records dict we do test_triton_kernel_prune_configs_by If we kept would trigger recompile here assertEqual counter op_count Modify autotuner object prune_by_kernel configs = triton Config kwargs= BLOCK_SIZE Calling kernel after modifying autotuner should trigger recompile f dst src N assertEqual counter op_count there should no recompile here f dst src N assertEqual counter op_count see https github com triton-lang triton blob ea f bdecb e e c af python test unit language test_decorator py#L requires_gpu common_utils parametrize non_strict True False common_utils parametrize backend eager aot_eager inductor common_utils parametrize autotune_at_compile_time True False test_triton_kernel_heuristic backend autotune_at_compile_time non_strict non-strict mode libname = my_cool_namespace opname = my_triton_operator triton autotune configs= triton Config kwargs= BLOCK_SIZE key= N we should able modify existing keys kwargs triton heuristics BLOCK_SIZE lambda nargs nargs BLOCK_SIZE test kwargs triton heuristics EVEN_N lambda nargs nargs N + triton heuristics EVEN_N lambda nargs nargs EVEN_N test args There differences here OSS Triton because we run these functions Dynamo We don t have access data_ptr TensorVariables triton heuristics NDIM_src lambda nargs nargs src None test heuristics applied correct order triton heuristics EVEN_N lambda nargs nargs EVEN_N - triton jit heuristics_kernel dst src N BLOCK_SIZE tl constexpr EVEN_N tl constexpr NDIM_src tl constexpr tl store dst EVEN_N + BLOCK_SIZE tl store dst + NDIM_src grid = lambda META triton cdiv N META BLOCK_SIZE f dst torch Tensor src torch Tensor N int - None grid = lambda META triton cdiv N META BLOCK_SIZE non_strict torch library wrap_triton heuristics_kernel grid dst src N=N heuristics_kernel grid dst src N=N non_strict decorator = torch library triton_op f libname opname mutates_args= dst f we can just pass function f dynamo decorator = f compiled_f = torch compile decorator backend=backend N = src = torch empty N device=GPU_TYPE dst = torch zeros N device=GPU_TYPE inductor_config patch triton autotune_at_compile_time autotune_at_compile_time compiled_f dst src N=N now let s run without torch compile compare triton_src = torch empty N device=GPU_TYPE triton_dst = torch zeros N device=GPU_TYPE heuristics_kernel grid triton_dst triton_src N=N triton_dst item + - + BLOCK_SIZE = + = test we apply heuristics correct order assertEqual triton_dst item assertEqual triton_dst item Results should match assertEqual dst item triton_dst item assertEqual dst item triton_dst item triton heuristics cannot non-constant values check exception non_strict triton autotune configs= triton Config kwargs= BLOCK_SIZE key= N torch randint will produce non-constant value triton heuristics EVEN_N lambda nargs torch randint triton jit heuristics_kernel dst src N BLOCK_SIZE tl constexpr EVEN_N tl constexpr tl store dst N grid = lambda META triton cdiv N META BLOCK_SIZE f dst torch Tensor src torch Tensor N int - None grid = lambda META triton cdiv N META BLOCK_SIZE heuristics_kernel grid dst src N=N compiled_f = torch compile f backend=backend fullgraph=True N = src = torch empty N device=GPU_TYPE dst = torch zeros N device=GPU_TYPE msg = triton heuristics must constant values because configs can only contain constant values assertRaisesRegex torch _dynamo exc Unsupported msg compiled_f dst src N=N common_utils instantiate_parametrized_tests KernelTests common_utils instantiate_parametrized_tests CustomOpTests __name__ == __main__ torch _inductor test_case run_tests run_tests