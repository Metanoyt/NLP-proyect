Owner s module inductor copy functools io os shutil subprocess sys tempfile unittest zipfile collections abc Callable pathlib Path parameterized parameterized_class torch torch _inductor config torch _inductor codecache get_kernel_bin_format WritableTempFile torch _inductor package load_package package_aoti torch _inductor test_case TestCase torch _inductor utils fresh_cache torch export Dim torch export experimental _ExportPackage torch export pt _archive _package AOTICompiledModel load_pt load_weights_to_pt _contents torch testing _internal common_cuda _get_torch_cuda_version torch testing _internal common_utils IS_FBCODE skipIfRocm skipIfXpu torch testing _internal inductor_utils GPU_TYPE HAS_GPU skipif predicate Callable str bool bool reason str decorator func functools wraps func wrapper args kwargs predicate device package_cpp_only skipTest reason func args kwargs wrapper decorator compile model args kwargs=None dynamic_shapes=None package_path=None inductor_configs=None - AOTICompiledModel ep = torch export export model args kwargs dynamic_shapes=dynamic_shapes strict=False package_path = torch _inductor aoti_compile_and_package ep package_path=package_path inductor_configs=inductor_configs type ignore arg-type loaded = load_package package_path loaded unittest skipIf sys platform == darwin No CUDA MacOS parameterized_class device cpu package_cpp_only False + FIXME AssertionError AOTInductor compiled library does exist device cpu package_cpp_only True IS_FBCODE + device GPU_TYPE package_cpp_only False device GPU_TYPE package_cpp_only True sys platform = darwin class_name_func=lambda cls _ params f cls __name__ Cpp params package_cpp_only _ params device TestAOTInductorPackage TestCase check_model TestCase model example_inputs inductor_configs=None dynamic_shapes=None atol=None rtol=None - AOTICompiledModel torch no_grad torch manual_seed model = model device ref_model = copy deepcopy model ref_inputs = copy deepcopy example_inputs expected = ref_model ref_inputs inductor_configs = inductor_configs inductor_configs aot_inductor package_cpp_only = package_cpp_only torch manual_seed WritableTempFile suffix= pt f compiled_model = compile model example_inputs dynamic_shapes=dynamic_shapes inductor_configs=inductor_configs package_path=f name actual = compiled_model example_inputs assertEqual actual expected atol=atol rtol=rtol compiled_model check_package_cpp_only TestCase - None Check cmake make available Skip package_cpp_only=False tests package_cpp_only raise unittest SkipTest Only meant test cpp package shutil which cmake None raise unittest SkipTest cmake available shutil which make None raise unittest SkipTest make available cmake_compile_and_run base_dir custom_env = os environ copy custom_env CMAKE_PREFIX_PATH = join str Path torch __file__ parent + os environ get CMAKE_PREFIX_PATH split build_path = Path base_dir build build_path mkdir subprocess run cmake cwd=build_path env=custom_env check=True subprocess run make cwd=build_path check=True result = subprocess run build main cwd=base_dir check=True capture_output=True text=True result cmake_compile model example_inputs options tmp_dir Exports model compiles using AOTInductor extracts generated files tmp_dir builds C++ code using CMake Make Returns - build_path Path Path CMake build directory containing compiled binary - tmp_path Path Path extracted model source directory ep = torch export export model example_inputs package_path = torch _inductor aoti_compile_and_package ep inductor_configs=options zipfile ZipFile package_path r zip_ref filenames = zip_ref namelist prefix = filenames split zip_ref extractall tmp_dir tmp_path = Path tmp_dir prefix data aotinductor model assertTrue tmp_path exists Create build directory run cmake build_path = tmp_path build assertTrue build_path exists build_path mkdir custom_env = os environ copy custom_env CMAKE_PREFIX_PATH = join str Path torch __file__ parent + os environ get CMAKE_PREFIX_PATH split subprocess run cmake cwd=build_path env=custom_env check=True subprocess run make cwd=build_path check=True build_path tmp_path test_add Model torch nn Module forward x y x + y example_inputs = torch randn device=self device torch randn device=self device check_model Model example_inputs test_remove_intermediate_files For CUDA generated cpp files contain absolute path generated cubin files With package artifact cubin path should overridden run time so removing those intermediate files test verify Model torch nn Module forward x y x + y example_inputs = torch randn device=self device torch randn device=self device model = Model torch no_grad torch manual_seed model = model device ref_model = copy deepcopy model ref_inputs = copy deepcopy example_inputs expected = ref_model ref_inputs torch manual_seed WritableTempFile suffix= pt f ep = torch export export model example_inputs strict=True fresh_cache cubin files removed when exiting context package_path = torch _inductor aoti_compile_and_package ep package_path=f name type ignore arg-type loaded = torch _inductor aoti_load_package package_path actual = loaded example_inputs assertEqual actual expected test_linear Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y x + linear y example_inputs = torch randn device=self device torch randn device=self device check_model Model example_inputs unittest skipIf IS_FBCODE cmake won t work fbcode unittest skipIf _get_torch_cuda_version Test only supported CUDA + skipIfXpu build system may different test_compile_after_package check_package_cpp_only Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y x + linear y torch no_grad example_inputs = torch randn device=self device torch randn device=self device model = Model device=self device expected = model example_inputs options = aot_inductor package_cpp_only package_cpp_only Require kernels compiled into o files aot_inductor embed_kernel_binary True tempfile TemporaryDirectory tmp_dir build_path tmp_path = cmake_compile model example_inputs options tmp_dir device == GPU_TYPE kernel_bin = get_kernel_bin_format device assertTrue list tmp_path glob f kernel_bin Check cubin o files exist use unique kernel names assertTrue list tmp_path glob f triton_ kernel_bin o Check so file build successfully so_path = build_path libaoti_model so assertTrue so_path exists optimized = torch _export aot_load str so_path device actual = optimized example_inputs assertTrue torch allclose actual expected unittest skipIf _get_torch_cuda_version Test only supported CUDA + unittest skipIf IS_FBCODE cmake won t work fbcode skipIfRocm doesn t support multi-arch binary skipIfXpu doesn t support multi-arch binary test_compile_after_package_multi_arch device = GPU_TYPE raise unittest SkipTest Only meant test GPU_TYPE check_package_cpp_only Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y x + linear y torch no_grad example_inputs = torch randn device=self device torch randn device=self device model = Model device=self device expected = model example_inputs options = aot_inductor package_cpp_only package_cpp_only Expect kernel embedded final binary We will make default behavior standalone mode aot_inductor emit_multi_arch_kernel True aot_inductor embed_kernel_binary True tempfile TemporaryDirectory tmp_dir build_path _ = cmake_compile model example_inputs options tmp_dir Check so file build successfully so_path = build_path libaoti_model so assertTrue so_path exists optimized = torch _export aot_load str so_path device actual = optimized example_inputs assertTrue torch allclose actual expected unittest skipIf _get_torch_cuda_version Test only supported CUDA + unittest skipIf IS_FBCODE cmake won t work fbcode skipIfXpu build system may different torch _inductor config patch test_configs use_libtorch True test_compile_after_package_static compile_standalone will set package_cpp_only=True check_package_cpp_only Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y x + linear y torch no_grad example_inputs = torch randn device=self device torch randn device=self device model = Model device=self device Test compilation when no name passed options = aot_inductor_mode compile_standalone True tempfile TemporaryDirectory tmp_dir build_path _ = cmake_compile model example_inputs options tmp_dir Check file build successfully a_path = build_path libaoti_model assertTrue a_path exists Test compilation when model name passed options = aot_inductor_mode compile_standalone True aot_inductor model_name_for_generated_files linear tempfile TemporaryDirectory tmp_dir build_path _ = cmake_compile model example_inputs options tmp_dir Check file build successfully a_path = build_path liblinear assertTrue a_path exists test invalid model name options = aot_inductor_mode compile_standalone True aot_inductor model_name_for_generated_files linear linear assertRaisesRegex Exception Invalid AOTI model name cmake_compile model example_inputs options unittest skipIf IS_FBCODE cmake won t work fbcode skipIfXpu build system may different torch _inductor config patch test_configs use_libtorch True test_compile_standalone_cos compile_standalone will set package_cpp_only=True check_package_cpp_only Model torch nn Module __init__ - None super __init__ forward x torch cos x torch no_grad example_inputs = torch randn device=self device model = Model device=self device Test compilation when model name passed options = aot_inductor_mode compile_standalone True aot_inductor model_name_for_generated_files cos tempfile TemporaryDirectory tmp_dir build_path _ = cmake_compile model example_inputs options tmp_dir Check file build successfully a_path = build_path libcos assertTrue a_path exists unittest skipIf _get_torch_cuda_version Test only supported CUDA + unittest skipIf IS_FBCODE cmake won t work fbcode skipIfRocm doesn t support multi-arch binary skipIfXpu doesn t support multi-arch binary torch _inductor config patch test_configs use_libtorch True test_compile_with_exporter check_package_cpp_only Model torch nn Module forward x y x + y Model torch nn Module forward x y x - y default args kwargs None example_inputs = torch ones device torch ones device package = _ExportPackage m = Model m = Model exporter = package _exporter Plus m _define_overload default default exporter = package _exporter Minus m _define_overload default default exporter example_inputs exporter example_inputs package_example_inputs True False tempfile TemporaryDirectory tmp_dir package _compiled_and_package tmp_dir + package pt True package_example_inputs Test compiling generated files result = cmake_compile_and_run tmp_dir package_example_inputs device == GPU_TYPE assertEqual result stdout output_tensor \n \n \n \n CUDAFloatType \noutput_tensor \n \n \n \n CUDAFloatType \n assertEqual result stdout output_tensor \n \n \n \n CPUFloatType \noutput_tensor \n \n \n \n CPUFloatType \n unittest skipIf _get_torch_cuda_version Test only supported CUDA + unittest skipIf IS_FBCODE cmake won t work fbcode skipIfRocm doesn t support multi-arch binary skipIfXpu doesn t support multi-arch binary torch _inductor config patch test_configs use_libtorch True test_compile_with_exporter_weights check_package_cpp_only Model torch nn Module __init__ super __init__ fc = torch nn Linear forward x x = fc x x default args kwargs None example_inputs = torch ones device package = _ExportPackage m = Model device exporter = package _exporter Model m _define_overload default default exporter example_inputs expected_res = m example_inputs package_example_inputs = True tempfile TemporaryDirectory tmp_dir package _compiled_and_package tmp_dir + package pt True package_example_inputs Test compiling generated files cmake_compile_and_run tmp_dir tensor_model = torch load tmp_dir + output_tensor pt weights_only=False true_res = next iter tensor_model parameters assertEqual expected_res true_res test_metadata Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y x + linear y example_inputs = torch randn device=self device torch randn device=self device metadata = dummy moo torch no_grad torch manual_seed model = Model device=self device ref_model = copy deepcopy model ref_inputs = copy deepcopy example_inputs expected = ref_model ref_inputs inductor_configs = aot_inductor package_cpp_only package_cpp_only aot_inductor metadata metadata WritableTempFile suffix= pt f ep = torch export export model example_inputs strict=False package_path = torch _inductor aoti_compile_and_package ep package_path=f name inductor_configs=inductor_configs type ignore arg-type We can load metadata w o loading actual package loaded_metadata = torch _C _aoti AOTIModelPackageLoader load_metadata_from_package package_path model assertEqual loaded_metadata get dummy moo device = loaded_metadata AOTI_DEVICE_KEY current_device_info = torch _inductor codecache get_device_information device k v current_device_info items assertTrue k loaded_metadata assertEqual v loaded_metadata k compiled_model = torch _inductor aoti_load_package package_path actual = compiled_model example_inputs assertEqual actual expected loaded_metadata = compiled_model get_metadata type ignore attr-defined assertEqual loaded_metadata get dummy moo test_bool_input Specialize whichever branch example input b Model torch nn Module forward x b b x x x + x example_inputs = torch randn device=self device True check_model Model example_inputs test_multiple_methods options = aot_inductor package True aot_inductor package_cpp_only package_cpp_only Model torch nn Module __init__ - None super __init__ forward b torch cat b dim= dim _a = Dim dim _a min= max= dim _b = Dim dim _b min= max= dynamic_shapes = dim _a b dim _b example_inputs = torch randn device=self device torch randn device=self device ep = torch export export Model example_inputs dynamic_shapes=dynamic_shapes strict=True aoti_files = torch _inductor aot_compile ep module example_inputs options=options Model torch nn Module __init__ device super __init__ device = device forward x t = torch tensor x size - device=self device dtype=torch float t = torch sqrt t x t example_inputs = torch randn device=self device ep = torch export export Model device example_inputs strict=True aoti_files = torch _inductor aot_compile ep module example_inputs options=options WritableTempFile suffix= pt f package_path = package_aoti f name model aoti_files model aoti_files loaded = load_package package_path model loaded = load_package package_path model assertEqual loaded example_inputs ep module example_inputs assertEqual loaded example_inputs ep module example_inputs unittest skipIf HAS_GPU requires gpu test_duplicate_calls options = aot_inductor package True device = GPU_TYPE Model torch nn Module __init__ - None super __init__ forward b torch cat b dim= dim _a = Dim dim _a min= max= dim _b = Dim dim _b min= max= dynamic_shapes = dim _a b dim _b example_inputs = torch randn device=device torch randn device=device check_model Model example_inputs ep = torch export export Model example_inputs dynamic_shapes=dynamic_shapes strict=True aoti_files = torch _inductor aot_compile ep module example_inputs options=options device = cpu example_inputs = torch randn device=device torch randn device=device ep = torch export export Model example_inputs dynamic_shapes=dynamic_shapes strict=True aoti_files = torch _inductor aot_compile ep module example_inputs options=options WritableTempFile suffix= pt f package_path = package_aoti f name model aoti_files model aoti_files loaded = load_package package_path model loaded = load_package package_path model assertTrue torch allclose loaded example_inputs ep module example_inputs assertTrue torch allclose loaded example_inputs ep module example_inputs test_specified_output_dir Model torch nn Module __init__ - None super __init__ forward b torch cat b dim= example_inputs = torch randn device=self device torch randn device=self device ep = torch export export Model example_inputs strict=True aoti_files = torch _inductor aot_compile ep module example_inputs options= aot_inductor output_path tmp_output_ aot_inductor package True aot_inductor package_cpp_only package_cpp_only WritableTempFile suffix= pt f package_path = package_aoti f name model aoti_files loaded = load_package package_path model assertTrue torch allclose loaded example_inputs ep module example_inputs test_save_buffer Model torch nn Module __init__ - None super __init__ forward b torch cat b dim= example_inputs = torch randn device=self device torch randn device=self device ep = torch export export Model example_inputs strict=True buffer = io BytesIO buffer = torch _inductor aoti_compile_and_package ep package_path=buffer type ignore arg-type _ range loaded = load_package buffer assertTrue torch allclose loaded example_inputs ep module example_inputs skipif lambda device package_cpp_only package_cpp_only No support cpp only test_package_without_weight Model torch nn Module __init__ n k device super __init__ linear = torch nn Linear k n device=device forward linear M N K = model = Model N K device example_inputs = torch randn M K device=self device inductor_configs = always_keep_tensor_constants True aot_inductor package_constants_in_so False compiled = compile model example_inputs inductor_configs=inductor_configs assertEqual set compiled get_constant_fqns set model state_dict keys compiled load_constants model state_dict check_full_update=True test_inputs = torch randn M K device=self device expected = model test_inputs output = compiled test_inputs assertEqual expected output skipif lambda device package_cpp_only package_cpp_only No support cpp only test_package_user_managed_weight Model torch nn Module __init__ n k device super __init__ linear = torch nn Linear k n device=device forward linear M N K = model = Model N K device example_inputs = torch randn M K device=self device inductor_configs = always_keep_tensor_constants True aot_inductor package_constants_in_so False compiled = compile model example_inputs inductor_configs=inductor_configs assertEqual set compiled get_constant_fqns set model state_dict keys compiled load_constants model state_dict check_full_update=True user_managed=False test_inputs = torch randn M K device=self device expected = model test_inputs output = compiled test_inputs assertEqual expected output Let s try modify weight in-place result shouldn t change model linear weight data = new_output = compiled test_inputs assertEqual new_output output Recreate new model we will test against user_managed=True new_compiled = compile model example_inputs inductor_configs=inductor_configs new_compiled load_constants model state_dict check_full_update=True user_managed=True expected = model test_inputs new_output = new_compiled test_inputs assertEqual expected new_output Try modify weight in-place result should change model linear weight data = expected = model test_inputs new_output = new_compiled test_inputs assertEqual new_output expected test_deepcopy_compiled_model Model torch nn Module forward x y x + y example_inputs = torch randn device=self device torch randn device=self device model = Model compiled = compile model example_inputs copmiled_copy = copy deepcopy compiled expected = model example_inputs output = compiled example_inputs output_copy = copmiled_copy example_inputs assertEqual expected output assertEqual expected output_copy skipif lambda device package_cpp_only package_cpp_only No support cpp only test_update_weights Model torch nn Module __init__ n k device super __init__ linear = torch nn Linear k n device=device forward linear M N K = model = Model N K device example_inputs = torch randn M K device=self device compiled = check_model model example_inputs new_state_dict = linear weight torch randn N K device=self device linear bias torch randn N device=self device model load_state_dict new_state_dict compiled load_constants model state_dict check_full_update=True test_inputs = torch randn M K device=self device expected = model test_inputs output = compiled test_inputs assertEqual expected output skipif lambda device package_cpp_only package_cpp_only No support cpp only test_package_shared_weights options = aot_inductor package True aot_inductor package_cpp_only package_cpp_only always_keep_tensor_constants True aot_inductor package_constants_in_so False aot_inductor package_constants_on_disk_format pickle_weights Bar torch nn Module __init__ p p super __init__ p = p register_buffer p p forward p += p += p p Bar torch nn Module __init__ p p super __init__ p = p register_buffer p p forward p += p += p p x = torch randn y = torch randn buffer = torch nn Buffer x clone buffer = torch nn Buffer y clone bar = Bar buffer buffer bar = Bar buffer buffer ep = torch export export bar ep = torch export export bar aoti_files = torch _inductor aot_compile ep module options=options aoti_files = torch _inductor aot_compile ep module options=options WritableTempFile suffix= pt f package_path = package_aoti f name model aoti_files model aoti_files pt _contents = load_pt package_path load_weights_from_disk=True loaded = pt _contents aoti_runners model loaded = pt _contents aoti_runners model note loading like below doesn t work because new weights will loaded each load_package call loaded = load_package package_path model loaded = load_package package_path model result_ _p result_ _p = loaded assertEqual result_ _p x + assertEqual result_ _p y + result_ _p result_ _p = loaded result already incremented run above assertEqual result_ _p x + assertEqual result_ _p y + note returned result will change though p changed assertEqual result_ _p y + test shared weights user managed gm = ep module gm = ep module load_weights_to_pt _contents pt _contents model gm state_dict model gm state_dict result_ _p result_ _p = loaded assertEqual result_ _p x + assertEqual result_ _p y + assertEqual gm p x + assertEqual gm p y + skipif lambda device package_cpp_only package_cpp_only No support cpp only test_package_weights_on_disk_nested_module options = aot_inductor package True aot_inductor package_cpp_only package_cpp_only always_keep_tensor_constants True aot_inductor package_constants_in_so False aot_inductor package_constants_on_disk_format pickle_weights linear weight s node name linear_weight This unit test tests we package right weight name ` liear weight ` ` linear_weight ` Bar torch nn Module __init__ super __init__ linear = torch nn Linear forward x linear x x = torch randn device bar = Bar device ep = torch export export bar x package_path = torch _inductor aoti_compile_and_package ep inductor_configs=options pt _contents = load_pt package_path load_weights_from_disk=True loaded = pt _contents aoti_runners model assertEqual loaded x bar x test_loading_wrong_model Model torch nn Module forward x x + example_inputs = torch randn device=self device model = Model ep = torch export export model example_inputs package_path = torch _inductor aoti_compile_and_package ep assertRaisesRegex RuntimeError Failed find generated cpp file so file model forward zip archive load_package package_path model_name= forward __name__ == __main__ torch _inductor test_case run_tests HAS_GPU sys platform == darwin run_tests needs= filelock