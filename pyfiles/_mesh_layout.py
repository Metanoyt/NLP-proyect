Definition CuTe inspired Layouts DeviceMesh internal bookkeeping functions manipulate them math collections abc Iterator dataclasses dataclass itertools product torch torch distributed _pycute as_tuple coalesce complement composition flatten IntTuple is_int is_tuple Layout match_structure dataclass frozen=True init=True _MeshLayout Layout Utility representing integer layout borrowing ideas CuTe Layout Algebra See https docs nvidia com cutlass media docs cpp cute _layout_algebra html more details Each layout represented list sizes strides We use way mechanical bookkeeping integers such ranks SPMD mesh transformation top Lots methods layout like coalesce composition complement etc borrowed pycute https github com NVIDIA cutlass blob dd d ee bfa d e b c c python pycute layout py#L L Note CuTe-inspired layout because CuTe uses co-lexicographic way linearization while PyTorch using lexicographic So even though CuTe documentation can still referenced implementation will different PyCute s pyrefly ignore bad-override shape IntTuple pyrefly ignore bad-override stride IntTuple __post_init__ - None is_tuple shape is_int shape raise TypeError f shape must tuple int got type shape is_tuple stride is_int stride raise TypeError f stride must tuple int got type stride match_structure shape stride raise ValueError f sizes shape strides stride don t match property sizes - IntTuple shape property strides - IntTuple stride property sizes_and_strides - Iterator tuple int int zip flatten shape flatten stride property top_level_sizes - tuple int tuple i numel i range len numel - int math prod flatten shape operator get-i like tuples __getitem__ i int - _MeshLayout i -len i = len raise IndexError f Dim i out range layout len dimensions f Expected dim range -len len - layout = super __getitem__ i _MeshLayout layout shape layout stride nest - _MeshLayout _MeshLayout shape stride coalesce - _MeshLayout A layout represented sizes strides e g Two consecutive dimensions can merged into one their strides contiguous multiplicative i e inner stride inner size equals next stride we perform kind merge inside coalesce Example simple - inner dimension has stride= size= - outer dimension stride = inner_stride inner_size = → coalesced = acts like flat D array length Example non-coalescible - inner dimension stride= size= → = - outer dimension stride= mismatch ≠ → cannot merge result stays layout = coalesce _MeshLayout layout shape layout stride composition layout _MeshLayout - _MeshLayout By-dimension composition allows one layout select filter through another layout Think function composition ∘ layout input = layout input between two layouts This function wrapper pycute s composition Mental model about how understand composition logic - The LEFT layout defines output space - what indices possible - The RIGHT layout layout parameter acts selector - which specific indices pick - The composition only generates indices left layout could originally produce right layout determines which indices picked - The stride composition layout will smaller than stride right layout because when picking indices composition will least follow right layout s stride move forward Example = sizes= strides= layout = sizes= stride= o layout = Returns Layout being composed result = composition layout _MeshLayout result shape result stride complement world_size int - _MeshLayout Compute complement layout relative given world_size A complement layout fills missing factor so repeat layout complement world_size will get complete world_size We use ⊗ denote repeat operation Example = size= stride= world_size = Then complete needed factor = = complement = Together they form ⊗ = which has world_size = = required In distributed terms complement often used derive other rank grouping when splitting processes into D meshes For visualized explanation see https x com ezyang status layout = complement world_size _MeshLayout layout shape layout stride splice start int end int layout _MeshLayout - _MeshLayout sizes = list as_tuple sizes strides = list as_tuple strides sizes start end = list as_tuple layout sizes strides start end = list as_tuple layout strides _MeshLayout tuple sizes tuple strides all_ranks_from_zero - list int This function computes all ranks specified layout staring zero How works we enumerates every possible coordinate like nested for-loop If sizes = we get following coordinates For each coordinate we compute linear rank index all_ranks_from_zero = sum coord i strides i i range ndim Example A sizes = rows cols strides = row-major layout coords = - + = - + = - + = - + = - + = - + = result = Example B sizes = strides = non-standard strided layout coords = - + = - + = - + = - + = - + = - + = result = sum c s c s zip coord flatten strides coord product range s s flatten sizes global_ranks world_size int - list list int Build global ranks specified layout via two-level ranks composition The nested list forms Cartesian product all ranks one layout offset regarding filling up world_size layout The final global ranks addition these two The result list lists one sublist per layout This rank list will used build communicator underlying layout given ` world_size ` Example world_size = size = stride = ranks = offsets = result = + + + + → + + + + → + + + + → + + + + → offset + rank rank all_ranks_from_zero offset complement world_size all_ranks_from_zero check_non_overlap - bool Check layout has any overlap between ranks generates If there overlap we False otherwise True The layout supposed injective i e aside indice indices each dim layout must non-overlapping Example - Valid no overlap Layout sizes= strides= - Dim stride= span= = covers indices - Dim stride= span= = covers indices → No overlap since Example - Invalid overlap Layout sizes= strides= - Dim stride= span= = covers indices - Dim stride= span= = covers indices → Overlap stride= span= so indices duplicated Example - Invalid overlap Layout sizes= strides= - Dim stride= span= covers indices - Dim stride= span= covers indices → Overlap stride same two dims so indices duplicated Returns bool True no overlap False overlap detected ranks = all_ranks_from_zero len ranks == len set ranks remap_to_tensor rank_map torch Tensor - torch Tensor Leverage layout index mesh tensor re-maps indexes after layout transformation actual device ranks With method cute layout serves backend indices bookkeeping mesh tensor when comes flatten unflatten slicing operations The actual mesh tensor still represents actual device assignment ranks We need function specify device allocation create backend mesh Although any transform mesh tensors can treated view subset mesh tensor we do need use actual view sub-tensor DeviceMesh its backend creation The shape ` rank_map ` must D contiguous Examples Case - Consecutive ranks full world original_mesh_tensor = x mesh ranks - world_size = layout = Layout Return Case - Non-consecutive ranks original_mesh_tensor = custom rank assignment world_size = layout = Layout Return Args rank_map The concrete mesh tensor actual device ranks Returns torch Tensor A tensor representing actual device allocation rank_map assert rank_map ndim == assert rank_map is_contiguous assert rank_map numel = cosize complement_layout = complement rank_map numel rank_map as_strided flatten complement_layout sizes + flatten sizes flatten complement_layout strides + flatten strides reshape - top_level_sizes