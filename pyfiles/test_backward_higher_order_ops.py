Owner s module dynamo flake noqa B functools itertools unittest mock torch torch _dynamo test_case torch _dynamo testing torch _dynamo utils torch _inductor inductor torch _dynamo compiled_autograd torch _dynamo _trace_wrapped_higher_order_op trace_wrapped torch _dynamo testing normalize_gm torch fx experimental proxy_tensor make_fx _multiply x x x _multiply_invoke grad trace_wrapped grad fn=_multiply BackwardHigherOrderOpTests torch _dynamo test_case TestCase test_invoke_in_eager x = torch tensor requires_grad=True y = torch tensor requires_grad=True fn x y x register_hook _multiply_invoke x y out = fn x y grad_out = torch tensor out backward grad_out assertEqual x grad y grad_out test_invoke_in_pt backend eager aot_eager inductor torch _dynamo reset x = torch tensor requires_grad=True y = torch tensor requires_grad=True fn x y x register_hook _multiply_invoke x y fn = torch compile fn backend=backend out = fn x y grad_out = torch tensor out backward grad_out assertEqual x grad grad_out y test_invoke_make_fx_forward_contrived x = torch tensor requires_grad=True out = make_fx _multiply_invoke x assertEqual out x torch tensor actual = normalize_gm out print_readable False assertExpectedInline actual \ _multiply_invoke torch nn Module forward grad_ f trace_wrapped f = torch__dynamo__trace_wrapped_higher_order_op_self_invoke grad_ grad_ = None trace_wrapped test_invoke_make_bw x = torch tensor requires_grad=True fwd x z = x x z + z res = fwd x res backward torch tensor out = make_fx _multiply_invoke x grad assertEqual out x grad torch tensor actual = normalize_gm out print_readable False assertExpectedInline actual \ _multiply_invoke torch nn Module forward grad_ f trace_wrapped f = torch__dynamo__trace_wrapped_higher_order_op_self_invoke grad_ grad_ = None trace_wrapped mock patch torch _functorch aot_autograd AOT_COUNTER new_callable=itertools count test_invoke_in_pt _compiled_autograd _ graph = None compiler_fn gm inner_compiler gm_ example_inputs_ nonlocal graph assertEqual graph None graph = gm_ inductor compile gm_ example_inputs_ torch compile gm backend=inner_compiler fullgraph=True dynamic=True backend eager aot_eager inductor torch _dynamo reset x = torch tensor requires_grad=True y = torch tensor requires_grad=True fn x y x register_hook _multiply_invoke x + y fn = torch compile fn backend=backend out = fn x y grad_out = torch tensor compiled_autograd _enable compiler_fn out backward grad_out actual = normalize_gm graph print_readable False assertEqual x grad grad_out grad_out backend == aot_eager assertExpectedInline actual \ GraphModule torch nn Module forward L_inputs_ list s Sym s L_sizes_ _ f s l_inputs_ = L_inputs_ l_sizes_ _ = L_sizes_ _ getitem f s = l_inputs_ getitem_ f s = l_inputs_ getitem_ f s = l_inputs_ l_inputs_ = None size Sym s = l_sizes_ _ size l_sizes_ _ = None validate_outputs = torch__dynamo_compiled_autograd_ops_validate_outputs getitem None None device type= cpu None size False getitem = size = None getitem_ f s = validate_outputs validate_outputs = None call_aot_bwd_prologue = torch__dynamo_compiled_autograd_call_aot_bwd_prologue getitem_ getitem_ = None aot _tangents_ f s = call_aot_bwd_prologue call_aot_bwd_prologue = None accumulate_grad = torch__dynamo_compiled_autograd_ops_AccumulateGrad aot _tangents_ getitem_ None False getitem_ = None getitem_ f s = accumulate_grad accumulate_grad = None result f s = aot _tangents_ aot _tangents_ aot _tangents_ = None accumulate_grad_ = torch__dynamo_compiled_autograd_ops_AccumulateGrad result getitem_ None False result = getitem_ = None getitem_ f s = accumulate_grad_ accumulate_grad_ = None getitem_ getitem_ backend == inductor assertExpectedInline actual \ GraphModule torch nn Module forward L_inputs_ list s Sym s L_sizes_ _ f s l_inputs_ = L_inputs_ l_sizes_ _ = L_sizes_ _ getitem f s = l_inputs_ getitem_ f s = l_inputs_ getitem_ f s = l_inputs_ l_inputs_ = None size Sym s = l_sizes_ _ size l_sizes_ _ = None validate_outputs = torch__dynamo_compiled_autograd_ops_validate_outputs getitem None None device type= cpu None size False getitem = size = None getitem_ f s = validate_outputs validate_outputs = None call_aot_bwd_prologue = torch__dynamo_compiled_autograd_call_aot_bwd_prologue getitem_ getitem_ = None aot _tangents_ f s = call_aot_bwd_prologue call_aot_bwd_prologue = None accumulate_grad = torch__dynamo_compiled_autograd_ops_AccumulateGrad aot _tangents_ getitem_ None False getitem_ = None getitem_ f s = accumulate_grad accumulate_grad = None result f s = aot _tangents_ aot _tangents_ aot _tangents_ = None accumulate_grad_ = torch__dynamo_compiled_autograd_ops_AccumulateGrad result getitem_ None False result = getitem_ = None getitem_ f s = accumulate_grad_ accumulate_grad_ = None getitem_ getitem_ graph = None mock patch torch _functorch aot_autograd AOT_COUNTER new_callable=itertools count test_invoke_in_pt _compiled_autograd_side_effect _ _side_effect_stateful_fn x obj obj counter = obj counter + _multiply x _side_effectful_invoke grad fn trace_wrapped grad fn=fn graph = None compiler_fn gm inner_compiler gm_ example_inputs_ nonlocal graph assertEqual graph None graph = gm_ inductor compile gm_ example_inputs_ torch compile gm backend=inner_compiler fullgraph=True dynamic=True backend inductor torch _dynamo reset x = torch tensor requires_grad=True y = torch tensor requires_grad=True MyObj __init__ - None counter = obj = MyObj inner_fn = functools partial _side_effect_stateful_fn obj=obj hook_fn = functools partial _side_effectful_invoke fn=inner_fn x register_hook hook_fn fn x y x + y fn = torch compile fn backend=backend fullgraph=True out = fn x y grad_out = torch tensor compiled_autograd _enable compiler_fn out backward grad_out actual = normalize_gm graph print_readable False assertEqual obj counter assertEqual x grad grad_out + grad_out backend aot_eager inductor assertExpectedInline actual \ GraphModule torch nn Module forward L_inputs_ list s Sym s L_sizes_ _ f s L_hooks_ _keywords_fn_keywords_obj_counter Sym s l_inputs_ = L_inputs_ l_sizes_ _ = L_sizes_ _ l_hooks_ _keywords_fn_keywords_obj_counter = L_hooks_ _keywords_fn_keywords_obj_counter getitem f s = l_inputs_ getitem_ f s = l_inputs_ getitem_ f s = l_inputs_ l_inputs_ = None size Sym s = l_sizes_ _ size l_sizes_ _ = None validate_outputs = torch__dynamo_compiled_autograd_ops_validate_outputs getitem None None device type= cpu None size False getitem = size = None getitem_ f s = validate_outputs validate_outputs = None call_aot_bwd_prologue = torch__dynamo_compiled_autograd_call_aot_bwd_prologue getitem_ getitem_ = None aot _tangents_ f s = call_aot_bwd_prologue call_aot_bwd_prologue = None accumulate_grad = torch__dynamo_compiled_autograd_ops_AccumulateGrad aot _tangents_ getitem_ None False getitem_ = None getitem_ f s = accumulate_grad accumulate_grad = None add Sym s + = l_hooks_ _keywords_fn_keywords_obj_counter + l_hooks_ _keywords_fn_keywords_obj_counter = None result f s = aot _tangents_ aot _tangents_ aot _tangents_ = None accumulate_grad_ = torch__dynamo_compiled_autograd_ops_AccumulateGrad result getitem_ None False result = getitem_ = None getitem_ f s = accumulate_grad_ accumulate_grad_ = None getitem_ getitem_ add out = fn x y out backward grad_out assertEqual obj counter out = fn x y out backward grad_out assertEqual obj counter graph = None test_invoke_in_pt _compiled_autograd_graph_breaks _graph_breaking_fn x print Boo _multiply x _graph_break_invoke grad trace_wrapped grad fn=_graph_breaking_fn compiler_fn gm torch compile gm backend= inductor fullgraph=True dynamic=True backend eager aot_eager inductor torch _dynamo reset x = torch tensor requires_grad=True y = torch tensor requires_grad=True fn x y x register_hook _graph_break_invoke x + y fn = torch compile fn backend=backend fullgraph=True out = fn x y grad_out = torch tensor assertRaisesRegex torch _dynamo exc Unsupported print compiled_autograd _enable compiler_fn out backward grad_out __name__ == __main__ torch _dynamo test_case run_tests run_tests