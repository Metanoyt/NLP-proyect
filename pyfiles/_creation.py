This module contains tensor creation utilities collections abc functools math warnings typing cast Optional Union torch _INTEGRAL_TYPES = torch uint torch int torch int torch int torch int torch uint torch uint torch uint _FLOATING_TYPES = torch float torch bfloat torch float torch float _FLOATING_ BIT_TYPES = torch float _e m fn torch float _e m torch float _e m fnuz torch float _e m fnuz _COMPLEX_TYPES = torch complex torch complex torch complex _BOOLEAN_OR_INTEGRAL_TYPES = torch bool _INTEGRAL_TYPES _FLOATING_OR_COMPLEX_TYPES = _FLOATING_TYPES _COMPLEX_TYPES _uniform_random_ t torch Tensor low float high float - torch Tensor uniform_ requires to-from = std numeric_limits scalar_t max Work around scaling range before after PRNG high - low = torch finfo t dtype max t uniform_ low high mul_ t uniform_ low high make_tensor shape Union int torch Size list int tuple int dtype torch dtype device Union str torch device low Optional float = None high Optional float = None requires_grad bool = False noncontiguous bool = False exclude_zero bool = False memory_format Optional torch memory_format = None - torch Tensor r Creates tensor given attr ` shape ` attr ` device ` attr ` dtype ` filled values uniformly drawn ` ` low high ` ` If attr ` low ` attr ` high ` specified outside range attr ` dtype ` s representable finite values then they clamped lowest highest representable finite value respectively If ` ` None ` ` then following table describes default values attr ` low ` attr ` high ` which depend attr ` dtype ` + --------------------------- + ------------ + ---------- + &#124; ` ` dtype ` ` &#124; ` ` low ` ` &#124; ` ` high ` ` &#124; +===========================+============+==========+ &#124; boolean type &#124; ` ` ` ` &#124; ` ` ` ` &#124; + --------------------------- + ------------ + ---------- + &#124; unsigned integral type &#124; ` ` ` ` &#124; ` ` ` ` &#124; + --------------------------- + ------------ + ---------- + &#124; signed integral types &#124; ` ` - ` ` &#124; ` ` ` ` &#124; + --------------------------- + ------------ + ---------- + &#124; floating types &#124; ` ` - ` ` &#124; ` ` ` ` &#124; + --------------------------- + ------------ + ---------- + &#124; complex types &#124; ` ` - ` ` &#124; ` ` ` ` &#124; + --------------------------- + ------------ + ---------- + Args shape Tuple int Single integer sequence integers defining shape output tensor dtype ` torch dtype ` The data type returned tensor device Union str torch device The device returned tensor low Optional Number Sets lower limit inclusive given range If number provided clamped least representable finite value given dtype When ` ` None ` ` default value determined based attr ` dtype ` see table above Default ` ` None ` ` high Optional Number Sets upper limit exclusive given range If number provided clamped greatest representable finite value given dtype When ` ` None ` ` default value determined based attr ` dtype ` see table above Default ` ` None ` ` deprecated Passing ` ` low==high ` ` func ` ~torch testing make_tensor ` floating complex types deprecated since will removed Use func ` torch full ` instead requires_grad Optional bool If autograd should record operations returned tensor Default ` ` False ` ` noncontiguous Optional bool If ` True ` returned tensor will noncontiguous This argument ignored constructed tensor has fewer than two elements Mutually exclusive ` ` memory_format ` ` exclude_zero Optional bool If ` ` True ` ` then zeros replaced dtype s small positive value depending attr ` dtype ` For bool integer types zero replaced one For floating point types replaced dtype s smallest positive normal number tiny value attr ` dtype ` s func ` ~torch finfo ` object complex types replaced complex number whose real imaginary parts both smallest positive normal number representable complex type Default ` ` False ` ` memory_format Optional torch memory_format The memory format returned tensor Mutually exclusive ` ` noncontiguous ` ` Raises ValueError If ` ` requires_grad=True ` ` passed integral ` dtype ` ValueError If ` ` low = high ` ` ValueError If either attr ` low ` attr ` high ` ` ` nan ` ` ValueError If both attr ` noncontiguous ` attr ` memory_format ` passed TypeError If attr ` dtype ` isn t supported function Examples xdoctest +SKIP xdoctest +REQUIRES env TORCH_DOCTEST_CUDA torch testing make_tensor Creates float tensor values - make_tensor device= cpu dtype=torch float low=- high= xdoctest +SKIP tensor - Creates bool tensor CUDA make_tensor device= cuda dtype=torch bool tensor False False False True device= cuda modify_low_high low Optional float high Optional float lowest_inclusive float highest_exclusive float default_low float default_high float - tuple float float Modifies raises ValueError when appropriate low high values given user input_low input_high required clamp float l float h float - float min max l h low = low low None default_low high = high high None default_high any isinstance value float math isnan value value low high raise ValueError f ` low ` ` high ` cannot NaN got low= high= low == high dtype _FLOATING_OR_COMPLEX_TYPES warnings warn Passing ` low==high ` ` torch testing make_tensor ` floating complex types deprecated since will removed Use ` torch full ` instead FutureWarning stacklevel= low = high raise ValueError f ` low ` must less than ` high ` got low = high high lowest_inclusive low = highest_exclusive raise ValueError f The value interval specified ` low ` ` high ` low high f dtype only supports lowest_inclusive highest_exclusive low = clamp low lowest_inclusive highest_exclusive high = clamp high lowest_inclusive highest_exclusive dtype _BOOLEAN_OR_INTEGRAL_TYPES ` low ` ceiled avoid creating values smaller than ` low ` thus outside specified interval Following same reasoning ` high ` should floored However higher bound ` torch randint ` exclusive thus we need ceil here well math ceil low math ceil high low high len shape == isinstance shape collections abc Sequence shape = shape type ignore assignment shape = cast tuple int tuple shape noncontiguous memory_format None raise ValueError f The parameters ` noncontiguous ` ` memory_format ` mutually exclusive f got noncontiguous= memory_format= requires_grad dtype _BOOLEAN_OR_INTEGRAL_TYPES raise ValueError f ` requires_grad=True ` supported boolean integral dtypes got dtype= noncontiguous = noncontiguous functools reduce lambda x y x y shape noncontiguous Double size shape last dimension so we have non-identical values when we make non-contiguous operation shape = cast tuple int shape - shape - dtype torch bool low high = cast tuple int int modify_low_high low high lowest_inclusive= highest_exclusive= default_low= default_high= result = torch randint low high shape device=device dtype=dtype dtype _BOOLEAN_OR_INTEGRAL_TYPES low high = cast tuple int int modify_low_high low high lowest_inclusive=torch iinfo dtype min highest_exclusive=torch iinfo dtype max In theory ` highest_exclusive ` should always maximum value + However ` torch randint ` internally converts bounds int would overflow In other words ` torch randint ` cannot sample - i e maximum value ` torch int ` we need account here + dtype torch int This incorrect ` torch uint ` since we clamp ` lowest ` i e ` torch uint ` _after_ we use default value we don t need special case here default_low=- default_high= result = torch randint low high shape device=device dtype=dtype dtype _FLOATING_OR_COMPLEX_TYPES low high = modify_low_high low high lowest_inclusive=torch finfo dtype min highest_exclusive=torch finfo dtype max default_low=- default_high= result = torch empty shape device=device dtype=dtype _uniform_random_ torch view_as_real result dtype _COMPLEX_TYPES result low high dtype _FLOATING_ BIT_TYPES low high = modify_low_high low high lowest_inclusive=torch finfo dtype min highest_exclusive=torch finfo dtype max default_low=- default_high= result = torch empty shape device=device dtype=torch float _uniform_random_ result low high result = result dtype raise TypeError f The requested dtype dtype supported torch testing make_tensor To request support file issue https github com pytorch pytorch issues noncontiguous Offset also catch offsetting issues result = result memory_format None result = result clone memory_format=memory_format exclude_zero result result == = dtype _BOOLEAN_OR_INTEGRAL_TYPES torch finfo dtype tiny dtype _FLOATING_OR_COMPLEX_TYPES result requires_grad = requires_grad result