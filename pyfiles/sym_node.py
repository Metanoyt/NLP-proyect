mypy allow-untyped-defs __future__ annotations This file does three things - Contains definition SymNode - Installs all magic methods into SymBool SymFloat SymFloat time - Does depend sympy time As file imported within torch __init__ py we do want depend SymPy avoid having load SymPy time doing so very slow builtins functools inspect itertools logging math operator sys functools lru_cache update_wrapper typing Optional TYPE_CHECKING Union torch torch _logging structured structured NB The sym_ functions used via getattr must imported here torch noqa F sym_float sym_ite sym_max sym_min sym_not SymBool SymFloat SymInt torch _logging dtrace_structured TYPE_CHECKING torch fx experimental symbolic_shapes ShapeEnv log = logging getLogger __name__ sym_node_log = torch _logging getArtifactLogger __name__ sym_node __all__ = SymNode method_to_operator magic_methods DynamicInt torch types py_sym_types SymTypes _to_symtype t t bool SymBool t int SymInt t float SymFloat t TODO An incomplete list Set variables equal when we do equality Specialize when we do subtraction SymNode This type erased SymInt SymFloat which we use do actual operations End users don t touch Magic methods NOT defined object Note optimized_summation indicates SymNode Add expression form + b + c + d etc where all terms unique symbols This allows us do some optimizations common patterns see _optimized_add The unfortunate reason we have here because sympy sets __slots__ = add expression so we cannot add attribute directly sympy expression Furthermore we cannot use weak dictionary key either So instead we attach attribute here SymNode _optimized_summation bool = False __init__ expr shape_env pytype hint Optional Union int float bool constant=None fx_node=None optimized_summation=False _expr = expr shape_env = shape_env pytype = pytype _optimized_summation = optimized_summation What s difference between hint constant - A constant known invariant across invocations model will always value We only really know when we encounter honest-to-goodness literal when wrapping into SymNode we set constant Most time constant None - A hint particular value particular run we tracing may vary next time around It s useful keep around we need concrete value SymNode we will hint guard expression produced giving same hint next time around The hint guaranteed set either you have unbacked SymNode there won t any hint result some tensor-dependent computation we don t know what actually because we haven t actually run tensor computation If _hint None we will query maybe_evaluate_static compute_hint=True hopes we ve learned enough about unbacked symints discharge hint otherwise you re likely just error out A previous version system had some optimizations only recompute when possible we had learned enough about unbacked symint hint now possible we added more potential refinements unbacked symints got harder keep sync so we ve deleted now compute_hint torch fx experimental symbolic_shapes has_free_unbacked_symbols This occasionally gets exercised e g convert_shape_to_symint It s just nicety so you don t HAVE have correct hint hand when making SymNode Don t attempt compute unbacked can quite expensive has_free_unbacked_symbols expr None hint = shape_env _maybe_evaluate_static expr compute_hint=True hint None hint = pytype hint isinstance hint SymTypes hint hint hint None assert type hint pytype type hint _to_symtype pytype Cannot create SymNode type f pytype incompatible hint type type hint shape_env shape_env _translation_validation_enabled This technically TV assert expensive so let s only do when we re already doing expensive things computed_hint = compute_hint assert hint == computed_hint f hint = computed_hint expr hint = compute_hint _hint = hint constant Optional Union int float bool = constant Record FX node current node we doing translation validation They will used building input assertions translation validation problem tx_validation_en = shape_env shape_env _translation_validation_enabled fx_node = tx_validation_en fx_node with_shape_env shape_env ShapeEnv - SymNode SymNode _expr shape_env pytype _hint constant fx_node _value_eq other SymNode - bool Purposely don t include shape_env eq _expr == other _expr pytype == other pytype _hint == other _hint constant == other constant fx_node == other fx_node _value_hash - int Purposely don t include shape_env hash hash _expr pytype _hint constant fx_node property expr shape_env replace _expr property hint _hint has_hint _hint None require_hint fallback=None torch fx experimental symbolic_shapes free_unbacked_symbols _hint None fallback None Say we have some expr like u + s The hint will None since expr contains least unbacked We will - replace every backed free symbol its corresponding hint - replace every unbacked free symbol fallback - regenerate expression those symbol replacements Note really complete either since right now logic does take into account any value ranges unbacked symints we may need beef up some point unbacked_symbols = free_unbacked_symbols expr replacements = s s unbacked_symbols shape_env var_to_val s s expr free_symbols expr xreplace replacements NB we expect raise shape_env size_hint expr _hint maybe_as_int expr is_number int expr None NB This does conversions sure good maybe_as_float sympy isinstance expr sympy Float float expr None maybe_as_bool sympy expr sympy true True expr sympy false False None is_int pytype int is_float pytype float is_bool pytype bool is_nested_int Unbacked SymInts cannot nested int today _hint None isinstance _hint SymInt _hint node is_nested_int wrap_int num assert type num int sympy SymNode sympy Integer num shape_env int num constant=num fx_node=num wrap_float num assert type num float sympy SymNode sympy Float num shape_env float num constant=num fx_node=num wrap_bool num assert type num bool sympy SymNode sympy true num sympy false shape_env bool num constant=num fx_node=num clone str f expr __str__ str __repr__ rep = f SymNode _expr shape_env= shape_env pytype= pytype _hint None rep append f hint= _hint constant None rep append f constant= constant fx_node None rep append f fx_node= fx_node join rep + _graph_repr - builtins str Representation used GraphModule create pythonic version graph str These methods call metaprogrammed methods they re hand written here so we get good stack traces abs - SymNode _abs type ignore attr-defined pos - SymNode _pos type ignore attr-defined round ndigits=None - SymNode _round ndigits type ignore attr-defined trunc - SymNode _trunc type ignore attr-defined add other - SymNode _add other type ignore attr-defined sub other - SymNode _sub other type ignore attr-defined mul other - SymNode _mul other type ignore attr-defined mod other - SymNode _mod other type ignore attr-defined float_pow other - SymNode _float_pow other type ignore attr-defined pow_by_natural other - SymNode _pow_by_natural other type ignore attr-defined and_ other - SymNode _and_ other type ignore attr-defined or_ other - SymNode _or_ other type ignore attr-defined float_truediv other - SymNode _float_truediv other type ignore attr-defined int_truediv other - SymNode _int_truediv other type ignore attr-defined int_floordiv other - SymNode _int_floordiv other type ignore attr-defined lshift other - SymNode _lshift other type ignore attr-defined rshift other - SymNode _rshift other type ignore attr-defined sym_not - SymNode noqa F _sym_not type ignore attr-defined eq other - SymNode _eq other type ignore attr-defined ne other - SymNode _ne other type ignore attr-defined gt other - SymNode _gt other type ignore attr-defined lt other - SymNode _lt other type ignore attr-defined le other - SymNode _le other type ignore attr-defined ge other - SymNode _ge other type ignore attr-defined floor - SymNode _floor type ignore attr-defined is_integer - SymNode _is_integer type ignore attr-defined sym_float - SymNode noqa F _sym_float type ignore attr-defined sym_int - SymNode _sym_int type ignore attr-defined ceil - SymNode _ceil type ignore attr-defined neg - SymNode _neg type ignore attr-defined sym_min other - SymNode noqa F _sym_min other type ignore attr-defined sym_max other - SymNode noqa F _sym_max other type ignore attr-defined sym_ite then_val else_val - SymNode _sym_ite then_val else_val type ignore attr-defined is_contiguous sizes strides - SymNode _is_contiguous sizes strides type ignore attr-defined is_channels_last_contiguous_ d sizes strides - SymNode _is_channels_last_contiguous_ d sizes strides type ignore attr-defined is_channels_last_contiguous_ d sizes strides - SymNode _is_channels_last_contiguous_ d sizes strides type ignore attr-defined is_channels_last_strides_ d sizes strides - SymNode _is_channels_last_strides_ d sizes strides type ignore attr-defined is_channels_last_strides_ d sizes strides - SymNode _is_channels_last_strides_ d sizes strides type ignore attr-defined is_non_overlapping_and_dense_indicator sizes strides - SymNode _is_non_overlapping_and_dense_indicator sizes strides type ignore attr-defined Make C++ happy sym_or other or_ other sym_and other and_ other Integer bitwise ops bitwise_and other _bitwise_and other type ignore attr-defined bitwise_or other _bitwise_or other type ignore attr-defined There no int_truediv available C++ truediv other float_truediv other floordiv other - SymNode int_floordiv other We didn t bind integer pow C++ pow other float_pow other is_non_overlapping_and_dense sizes strides is_non_overlapping_and_dense_indicator sizes strides eq to_node type ignore attr-defined int_ guard_int NB uses Python backtrace This one currently done hand we add other variadic functions consider factoring out metaprogrammed too Note some load bearing logic directly torch sym_sum sym_sum args - SymNode sympy Inner impl torch fx experimental proxy_tensor get_proxy_mode handle_sym_dispatch get_proxy_mode to_node handle_sym_dispatch torch sym_sum tuple wrap_node args exprs = expr args out = sympy Add exprs size_hints = out_hint = None args hint None break size_hints append hint out_hint = sum size_hints fx_node _ = shape_env _create_fx_call_function torch sym_sum tuple fx_node args NB Only integers SymNode out shape_env int out_hint fx_node=fx_node evaluate size_oblivious=False shape_env evaluate_sym_node size_oblivious You can manually trigger guard function guard_int file line TODO use file line some useful diagnostic why guard occurred r = evaluate try int r except Exception log warning Failed convert int s r raise guard_float file line TODO use file line some useful diagnostic why guard occurred r = evaluate try float r except Exception log warning Failed convert float s r raise guard_bool file line TODO use file line some useful diagnostic why guard occurred r = evaluate try bool r except Exception log warning Failed convert bool s r raise expect_true file line torch fx experimental symbolic_shapes free_unbacked_symbols has_hint free_unbacked_symbols expr shape_env prefer_deferred_runtime_asserts_over_guards OK generate guards guard_bool file line Generate deferred runtime assert might actually end up doing regular guard we can TODO file line here very important because assert has been deferred so you can t backtrace easily shape_env guard_or_defer_runtime_assert expr f file line fx_node=self fx_node statically_known_true file line torch fx experimental symbolic_shapes statically_known_true assert is_bool statically_known_true SymBool guard_size_oblivious file line Like guard_bool we encounter unbacked symbols those symbols size-like we will treat them = purposes analysis This CHANGES runtime semantics all size-oblivious sites have been audited ensure runtime semantics don t change material way Acceptable runtime semantic changes e g squeeze no longer dropping unbacked one size tensor reporting non-contiguous even s contiguous would have been reported contiguous due being empty TODO use file line some useful diagnostic why guard occurred r = evaluate size_oblivious=True try bool r except Exception log warning Failed convert bool s r raise guard_or_false file line torch fx experimental symbolic_shapes guard_or_false assert is_bool guard_or_false SymBool guard_or_true file line torch fx experimental symbolic_shapes guard_or_true assert is_bool guard_or_true SymBool bool_ guard_bool is_symbolic True nested_int None is_constant False _DynamicScalar __new__ cls args cls _DynamicScalar raise TypeError _DynamicScalar abstract base use DynamicInt super __new__ cls args DynamicInt _DynamicScalar int User API marking dynamic integers ` torch compile ` Intended compatible both compile eager mode Example usage fn = torch compile f x = DynamicInt fn x compiles x dynamic integer input returns f __new__ cls val assert isinstance val int obj = super __new__ cls int val obj __repr__ f DynamicInt real __floordiv__ other casting int without these overrides DynamicInt real other __rfloordiv__ other DynamicInt other real TODO probably needs sizes-strides eval functions METHOD_TO_OPERATOR = pos operator pos abs operator abs add operator add operator and_ bitwise_and operator and_ ceil math ceil eq operator eq floor math floor trunc math trunc int_floordiv operator floordiv ge operator ge gt operator gt is_integer lambda x x is_integer le operator le lshift operator lshift lt operator lt mod operator mod mul operator mul ne operator ne neg operator neg operator or_ bitwise_or operator or_ float_pow operator pow pow_by_natural operator pow round builtins round rshift operator rshift sub operator sub sym_float sym_float sym_ite sym_ite sym_max sym_max sym_min sym_min sym_not sym_not float_truediv operator truediv int_truediv operator truediv unary_magic_methods = abs sym_float sym_int ceil floor neg sym_not pos trunc Adding math ops sqrt cos sin _get_sym_node_fn name fn getattr f _sym_ name fn math_op_names = sqrt cos cosh sin sinh tan tanh asin acos atan log name math_op_names sym_name = f sym_ name priv_sym_name = f _ sym_name setattr SymNode sym_name _get_sym_node_fn name METHOD_TO_OPERATOR sym_name = getattr torch priv_sym_name unary_magic_methods add sym_name __all__ append sym_name Unary methods magic methods unary_nonmagic_methods = is_integer unary_methods = unary_magic_methods &#124; unary_nonmagic_methods Most methods only registered SymInt SymFloat Some methods only registered SymBool only_bool_magic_methods = sym_not sym_ite Methods implicitly convert SymBool into SymInt bool_becomes_int_magic_methods = add sub mul Methods also SymBool addition SymInt SymFloat also_bool_magic_methods = eq bool_magic_methods = only_bool_magic_methods &#124; also_bool_magic_methods Methods only float only_float_magic_methods = is_integer round sym_int sym_log magic_methods_on_operator_with_trailing_underscore = remap necessary because op name can have bitwise boolean implementation bitwise_ops = bitwise_and bitwise_or always_float_magic_methods = int_truediv float_truediv sym_float float_pow name math_op_names sym_name = f sym_ name always_float_magic_methods add sym_name always_int_magic_methods = ceil floor trunc pow_by_natural always_bool_magic_methods = eq ne gt lt le ge sym_not is_non_overlapping_and_dense is_integer Methods have ` __foo__ ` well ` __rfoo__ ` _sympy_float_truediv b torch utils _sympy functions FloatTrueDiv FloatTrueDiv b _sympy_int_truediv b torch utils _sympy functions IntTrueDiv IntTrueDiv b _sympy_floordiv b torch utils _sympy functions FloorDiv FloorDiv b _sympy_mod b torch utils _sympy functions Mod PythonMod is_nonnegative b is_nonnegative Mod b PythonMod b _sympy_pow_by_natural b torch utils _sympy functions PowByNatural PowByNatural b _sympy_float_pow b torch utils _sympy functions FloatPow FloatPow b _sympy_and b sympy sympy And b _sympy_or b sympy sympy Or b _sympy_lshift b torch utils _sympy functions LShift LShift b _sympy_rshift b torch utils _sympy functions RShift RShift b _binary_search_insert_arg ordered_args new_arg If new_arg found ordered_args None returned new ordered_args new_arg inserted len ordered_args == new_arg sympy core basic _args_sortkey sort_key Basic Fast path when new_arg ordered_args - sort_key ordered_args - sort_key new_arg ordered_args + new_arg Fast path when new_arg ordered_args sort_key ordered_args sort_key new_arg new_arg + ordered_args low high = len ordered_args - while low = high mid = low + high compare_result = Basic compare ordered_args mid new_arg compare_result == None compare_result low = mid + high = mid - ordered_args insert low new_arg ordered_args _optimized_add lhs rhs lhs_is_optimized_summation=False rhs_is_optimized_summation=False Custom optimization Add used optimize incremental binary summations certain properties The idea when we know expression summation unique symbols all we need know correct order symbols no other optimizations needed We pass evaluate=false correct order args save following Avoid running other optimizations when Add constructed Manually figure out order args new expression log n comparisons instead nLog n comparing terms expensive shows profiles The function returns tuple boolean indicates whether output summation unique symbols result sympy expression sympy sympy core basic _args_sortkey sortkey make_optimized ordered_args assert ordered_args None result = sympy Add ordered_args evaluate=False True result torch utils _sympy functions _is_symbols_binary_summation lhs_is_optimized_summation &#124; = _is_symbols_binary_summation lhs rhs_is_optimized_summation &#124; = _is_symbols_binary_summation rhs lhs_is_optimized_summation rhs_is_optimized_summation +a + +a = +a +a +a sortkey lhs _args - sortkey rhs _args make_optimized lhs _args + rhs _args +a + +a = +a +a +a sortkey lhs _args sortkey rhs _args - make_optimized rhs _args + lhs _args +a + +a = +a +a +a len lhs _args = len rhs _args = new_args = list lhs _args rhs _args new_args = _binary_search_insert_arg new_args new_args None break None means element already exists new_args None make_optimized new_args +a + = +a +a lhs_is_optimized_summation rhs is_symbol new_args = _binary_search_insert_arg list lhs _args rhs None means element already exists new_args None make_optimized new_args + +a = +a +a rhs_is_optimized_summation lhs is_symbol new_args = _binary_search_insert_arg list rhs _args lhs None means element already exists new_args None make_optimized new_args result = sympy Add lhs rhs _is_symbols_binary_summation result result _bitwise_and b torch utils _sympy functions BitwiseFn_bitwise_and BitwiseFn_bitwise_and b _bitwise_or b torch utils _sympy functions BitwiseFn_bitwise_or BitwiseFn_bitwise_or b reflectable_magic_methods = add _optimized_add sub operator sub mul operator mul mod _sympy_mod pow_by_natural _sympy_pow_by_natural float_pow _sympy_float_pow _sympy_and bitwise_and _bitwise_and _sympy_or bitwise_or _bitwise_or float_truediv _sympy_float_truediv int_truediv _sympy_int_truediv int_floordiv _sympy_floordiv lshift _sympy_lshift rshift _sympy_rshift _floor_ceil_helper fn sympy isinstance sympy Mul aa = args len aa == isinstance aa sympy Float aa is_integer coef = sympy Integer aa aa == coef structural equality test coef aa isinstance sympy Float == sympy Integer isinstance sympy Integer sympy Integer fn _sympy_floor torch utils _sympy functions FloorToInt FloorToInt NB Python trunc semantics which returns int Do NOT use represent torch trunc which float float _sympy_trunc torch utils _sympy functions TruncToInt TruncToInt _sympy_ceil torch utils _sympy functions CeilToInt CeilToInt _sympy_eq b sympy sympy Eq b _sympy_ne b sympy sympy Ne b _sympy_gt b sympy sympy Gt b _sympy_lt b sympy sympy Lt b _sympy_le b sympy sympy Le b _sympy_ge b sympy sympy Ge b _sympy_min b torch utils _sympy functions Min Min b _sympy_max b torch utils _sympy functions Max Max b _sympy_ite t f sympy sympy Piecewise t f True current_module = sys modules __name__ _get_sym_math_fn name fn torch utils _sympy functions getattr torch utils _sympy functions f OpaqueUnaryFn_ name fn name math_op_names priv_sympy_name = f _sympy_ name fn = _get_sym_math_fn name fn __qualname__ = fn __name__ = priv_sympy_name setattr current_module priv_sympy_name fn del fn name priv_sympy_name type ignore possibly-undefined _sympy_abs sympy sympy Abs _sympy_round number ndigits=None torch utils _sympy functions RoundDecimal RoundToInt ndigits None RoundToInt number RoundDecimal number ndigits _sympy_sym_float torch utils _sympy functions ToFloat NB Cannot use here because which incorrectly reports integer ToFloat _sympy_is_integer sympy torch utils _sympy functions ToFloat sympy Eq ToFloat sympy floor magic_methods = reflectable_magic_methods sym_not operator invert pos operator pos eq _sympy_eq ne _sympy_ne gt _sympy_gt lt _sympy_lt le _sympy_le ge _sympy_ge floor _sympy_floor trunc _sympy_trunc sym_float _sympy_sym_float ceil _sympy_ceil neg operator neg sym_min _sympy_min sym_max _sympy_max sym_ite _sympy_ite abs _sympy_abs round _sympy_round is_integer _sympy_is_integer name math_op_names sym_name = f sym_ name magic_methods sym_name = getattr current_module f _sympy_ name del name sym_name math_op_names current_module type ignore possibly-undefined sympy_is_contiguous sizes strides dim = len sizes sympy_is_contiguous_generic sizes strides list range dim - - - sympy_is_contiguous_generic sizes strides dim_order sympy dim = len sizes len dim_order = dim sympy false is_contiguous = sympy true z = sympy S One Contiguous strides make sense dim size d dim_order is_contiguous = sympy Eq sizes d sympy S One &#124; sympy Eq strides d z z = sizes d OR any size zero d range dim is_contiguous &#124; = sympy Eq sizes d sympy S Zero is_contiguous NB There TODO C++ allow omitting batch dim If happens you will need refactor sympy_is_channels_last_contiguous_ d sizes strides sympy_is_contiguous_generic sizes strides sympy_is_channels_last_contiguous_ d sizes strides sympy_is_contiguous_generic sizes strides sympy_is_channels_last_strides_generic sizes strides dim_order sympy torch utils _sympy functions Max dim = len sizes dim = len dim_order sympy false m = sympy S Zero r = sympy true special case trivial C dimension default NCHW r = sympy Ne strides d dim_order r = sympy Ne sizes d strides d = m Fallback NCHW default layout ambiguous cases This flaw implicit memory_format strides N tensor identical strides size dimension Two cases could lead us here N contiguous Tensor N b N W contiguous Tensor sliced W-dimension N W W W W d == r = sympy Ne m strides This necessary distinguish memory_format N H H channels_last stride H H contiguous stride permutation C W C H HC H H transpose H C HC H H shouldn t identified channels_last m = strides d Max sizes d r sympy_is_channels_last_strides_ d sizes strides sympy_is_channels_last_strides_generic sizes strides sympy_is_channels_last_strides_ d sizes strides sympy_is_channels_last_strides_generic sizes strides _sympy_is_non_overlapping_and_dense_indicator sizes strides torch utils _sympy functions IsNonOverlappingAndDenseIndicator IsNonOverlappingAndDenseIndicator sizes strides sizes_strides_methods = TODO These could also done indicators maybe better reasoning do way is_contiguous sympy_is_contiguous is_channels_last_contiguous_ d sympy_is_channels_last_contiguous_ d is_channels_last_contiguous_ d sympy_is_channels_last_contiguous_ d is_channels_last_strides_ d sympy_is_channels_last_strides_ d is_channels_last_strides_ d sympy_is_channels_last_strides_ d is_non_overlapping_and_dense_indicator _sympy_is_non_overlapping_and_dense_indicator to_node num isinstance num SymTypes num node type num bool wrap_bool num type num int wrap_int num type num float wrap_float num NotImplemented important so Python tries other magic method NotImplemented wrap_node x TODO let C++ also take advantage isinstance x SymNode x constant None x constant x is_int SymInt x x is_float SymFloat x x is_bool SymBool x raise AssertionError f unrecognized type x method_to_operator method METHOD_TO_OPERATOR method _make_node_magic method func func = lru_cache func method magic_methods_on_operator_with_trailing_underscore method_attr = f method _ method_attr = method uninteresting_files - set str torch mods = torch _dynamo eval_frame torch _dynamo utils torch fx experimental sym_node torch torch _dynamo guards inspect getfile m m mods &#124; torch _dynamo guards uninteresting_files &#124; string capture_provenance fn functools wraps fn wrapper other=None other None result = fn result = fn other torch _logging _internal GET_DTRACE_STRUCTURED other None arguments = other arguments = get_id sym_node - Optional int We don t want ID input constant sympy sym_node constant None None id sym_node == id result None isinstance sym_node expr sympy Integer sympy Float None sym_node expr sympy true sympy false None id sym_node dtrace_structured expression_created metadata_fn=lambda method method result str result result_id id result arguments str arguments argument_ids get_id i i arguments get_id i None user_stack structured get_user_stack stack structured get_framework_stack result wrapper capture_provenance binary_magic_impl other torch fx experimental proxy_tensor get_proxy_mode handle_sym_dispatch op = method_to_operator method out_hint = None hint None other hint None out_hint = op hint other hint get_proxy_mode to_node handle_sym_dispatch op wrap_node wrap_node other assert isinstance other SymNode optimized_summation = False try method == mod torch utils _sympy functions Mod PythonMod Special handling mod requires access value ranges shape_env = shape_env expr is_nonnegative shape_env bound_sympy expr lower = other expr is_nonnegative shape_env bound_sympy other expr lower = out = Mod expr other expr out = PythonMod expr other expr method == add see Note optimized_summation optimized_summation out = func expr other expr _optimized_summation other _optimized_summation TODO consider constant prop here out = func expr other expr except Exception log warning failed eval s s s method expr other expr raise sym_node_log debug s s s - s method expr other expr out pytype type This strictly correct In Python b may complex when b float - Same sympy sqrt - This returns float while both arguments ints - Also max min do type promote To avoid having data-dependent control flow here we just set type float one args float In case type mismatch we assume will detected during evaluation method always_float_magic_methods pytype = float method always_bool_magic_methods pytype = bool pytype float other pytype float pytype = float pytype = pytype pytype None out_hint None isinstance out_hint SymTypes out_hint = pytype out_hint Create FX node corresponds operation being applied node fx_node _ = shape_env _create_fx_call_function op fx_node other fx_node result = SymNode out shape_env pytype out_hint type ignore arg-type fx_node=fx_node optimized_summation=optimized_summation see Note optimized_summation result capture_provenance unary_magic_impl torch fx experimental proxy_tensor get_proxy_mode handle_sym_dispatch op = method_to_operator method get_proxy_mode to_node handle_sym_dispatch op wrap_node TODO consider constant prop here expr = expr method == floor method == ceiling expr = shape_env _simplify_floor_div expr try out = func expr except Exception log warning failed eval s s method expr raise sym_node_log debug s s - s func expr out out_hint = None hint None out_hint = op hint pytype type method always_int_magic_methods pytype = int method always_bool_magic_methods pytype = bool method always_float_magic_methods pytype = float pytype = pytype fx_node _ = shape_env _create_fx_call_function op fx_node SymNode out shape_env pytype out_hint fx_node=fx_node method unary_methods setattr SymNode f _ method_attr unary_magic_impl method == sym_ite sym_ite_impl pred_node then_node else_node torch fx experimental proxy_tensor get_proxy_mode handle_sym_dispatch out_hint = then_node hint pred_node hint else_node hint get_proxy_mode to_node pred_node handle_sym_dispatch sym_ite wrap_node pred_node wrap_node then_node wrap_node else_node try out = func pred_node expr then_node expr else_node expr except Exception log warning failed eval s s s s method pred_node expr then_node expr else_node expr raise fx_node _ = pred_node shape_env _create_fx_call_function sym_ite pred_node fx_node then_node fx_node else_node fx_node SymNode out pred_node shape_env then_node pytype out_hint fx_node=fx_node setattr SymNode f _ method_attr sym_ite_impl method == round round_impl ndigits=None torch fx experimental proxy_tensor get_proxy_mode handle_sym_dispatch op = builtins round get_proxy_mode to_node handle_sym_dispatch op wrap_node ndigits expr = expr try out = func expr ndigits except Exception log warning failed eval s s ndigits= s method expr ndigits raise ndigits None pytype = int pytype = pytype out_hint = None hint None out_hint = op hint ndigits Internally None used sentinel indicate something node FX graph At same time there no way wrap plain None into FX node Thus there no way pass None here without triggering some asserts check whether we mixing FX nodes untracked arguments The hack down below works because all round function down line all take ndigits=None default their signature TODO Remove args construction below different sentinel used FX ezyang May LOL args = fx_node ndigits None args append ndigits fx_node _ = shape_env _create_fx_call_function op tuple args SymNode out shape_env pytype out_hint fx_node=fx_node setattr SymNode f _ method_attr round_impl setattr SymNode f _ method_attr binary_magic_impl _make_node_sizes_strides method func NB don t LRU cache lots arguments sizes_strides_impl sizes strides torch fx experimental proxy_tensor get_proxy_mode handle_sym_dispatch op = getattr sys modules __name__ method get_proxy_mode to_node handle_sym_dispatch op wrap_node s s sizes wrap_node s s strides size_exprs = s expr s sizes stride_exprs = s expr s strides try out = func size_exprs stride_exprs except Exception log warning failed eval s s s method size_exprs stride_exprs raise bool never expandable size_hints = out_hint = None s sizes s hint None break size_hints append s hint stride_hints = s strides s hint None break stride_hints append s hint out_hint = op size_hints stride_hints NB This indicator function actual bool pytype type method endswith _indicator pytype = int pytype = bool SymNode out shape_env pytype out_hint setattr SymNode f _ method sizes_strides_impl TODO This technically hotpath ideal end state guards will resolve higher level so you never spend time code sizes_strides_user sizes strides sympy torch fx experimental symbolic_shapes eval_is_non_overlapping_and_dense itertools chain sizes strides isinstance SymInt wrap_node getattr node method to_node node b b sizes to_node node b b strides method == is_non_overlapping_and_dense_indicator eval_is_non_overlapping_and_dense sizes strides TODO awful implementation bool func sympy sympify sizes sympy sympify strides Skip is_non_overlapping_and_dense_indicator hasattr sys modules __name__ method setattr sys modules __name__ method sizes_strides_user method func magic_methods items _make_node_magic method func method func sizes_strides_methods items _make_node_sizes_strides method func _make_user_magic method user_type User magic takes care wrapping other operand into node so our internal logic can assume everything nodes method magic_methods_on_operator_with_trailing_underscore method_attr = f sym_ method method_attr = method get_constant x Union SymInt int SymFloat float SymBool bool isinstance x int float bool x isinstance x SymInt x node guard_int isinstance x SymBool x node guard_bool raise AssertionError expect called constant SymBools is_constant x isinstance x int float bool True isinstance x SymInt SymFloat SymBool x node is_constant False Promotion rules binary operations NB we preserve PYTHON semantics - args same type do nothing - one arg float promote other arg float - nb applies floordiv even though output integral s still float - pow funny business - both ints - trigger guard exponent = - non-negative output int - otherwise output float - otherwise promote other arg float - nb complex impossible handle correctly lol negative base integral float need diverge semantics just always complex Neener neener pretend problem doesn t exist - equality pain Python does fancy thing where unpacks mantissa float then compares against int Which means able tell = rather than LHS promoted float which case would have truncated RHS subsequently been equal We ll model exactly having special mixed type equality operations Unfortunately we need do all comparison operations maybe I ll only implement compare - sym_ite mumble mumble really shouldn t allow mixed whatever method bool_becomes_int_magic_methods promote x Implements True+True= which works python sympy isinstance x SymBool SymInt x node wrap_int int x x promote x x promote other TODO Remove eq other relations list CPython has fancy implementations these get much precision possible instead just promoting float praying so we need handle them specially too Also note int_truediv doesn t go through path both arguments int so there isn t any promotion method add sub mul mod float_pow float_truediv int_floordiv sym_min sym_max TODO remove these eq ne gt lt le ge other f_self = isinstance float torch SymFloat f_other = isinstance other float torch SymFloat f_self f_other f_self = torch sym_float f_other other = torch sym_float other other Before after performing operation check any operands constant If so extract out constant values first If ` ` itself constant then redispatch calling back into operator Sometimes means operations involving SymBool plain bools Alternatively we could also rewrap into constant Symbool i e implementing wrap_bool ConstantSymNodeImpl we re doing today no particular reason unary_magic_impl = promote is_constant method_to_operator method get_constant wrap_node getattr node method_attr binary_magic_impl other isinstance other int float bool SymInt SymFloat SymBool NotImplemented sym_node_log debug MAGIC s s s method other = promote other = promote other other = promote other is_constant method_to_operator method get_constant other is_constant other other = get_constant other other_node = to_node node other other_node NotImplemented NotImplemented ret = wrap_node getattr node method_attr other_node get_constant ret is_constant ret ret rbinary_magic_impl other isinstance other int float bool SymInt SymFloat SymBool NotImplemented = promote other = promote other other = promote other is_constant method_to_operator method other get_constant is_constant other other = get_constant other other_node = to_node node other other_node NotImplemented NotImplemented ret = wrap_node getattr other_node method_attr node get_constant ret is_constant ret ret setattrs user_type attr symnode_impl Registers SymNode magic method SymInt Float Bool optionally registers corresponding wrapped method DynamicInt SymInt Float Bool setattr user_type attr symnode_impl DynamicInt impl dynamic_int_impl args args = x real isinstance x DynamicInt x x args out = getattr int attr args isinstance out int isinstance out bool DynamicInt out out user_type SymInt setattr DynamicInt attr dynamic_int_impl method unary_magic_methods setattrs user_type f __ method __ unary_magic_impl method unary_nonmagic_methods orig = getattr user_type method setattrs user_type method update_wrapper unary_magic_impl orig method == sym_ite sym_ite_magic_impl pred then_val else_val pred_node = pred node then_node = to_node pred_node then_val else_node = to_node pred_node else_val then_node NotImplemented else_node NotImplemented NotImplemented assert isinstance then_node SymNode isinstance else_node SymNode then_node pytype == else_node pytype ret = wrap_node getattr pred node method_attr then_node else_node get_constant ret ret node is_constant ret setattrs user_type f __ method __ sym_ite_magic_impl method == round round_magic_impl ndigits=None is_constant builtins round get_constant ndigits wrap_node getattr node method ndigits setattrs user_type f __ method __ round_magic_impl method_name = method method bitwise_ops method_name = bitwise_ops method setattrs user_type f __ method_name __ binary_magic_impl method reflectable_magic_methods setattrs user_type f __r method_name __ rbinary_magic_impl method magic_methods keys type ignore assignment method only_bool_magic_methods _make_user_magic method SymBool continue method only_float_magic_methods _make_user_magic method SymFloat continue method also_bool_magic_methods method bool_becomes_int_magic_methods _make_user_magic method SymBool _make_user_magic method SymInt method bitwise_ops _make_user_magic method SymFloat del method del func