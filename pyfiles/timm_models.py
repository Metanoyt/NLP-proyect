usr bin env python importlib logging os re subprocess sys warnings try common BenchmarkRunner download_retry_decorator load_yaml_file main except ImportError common BenchmarkRunner download_retry_decorator load_yaml_file main torch torch _dynamo testing collect_results reduce_to_scalar_loss torch _dynamo utils clone_inputs Enable FX graph caching TORCHINDUCTOR_FX_GRAPH_CACHE os environ torch _inductor config fx_graph_cache = True pip_install package subprocess check_call sys executable -m pip install package try importlib import_module timm except ModuleNotFoundError print Installing PyTorch Image Models pip_install git+https github com rwightman pytorch-image-models finally timm __version__ timmversion timm data resolve_data_config timm models create_model TIMM_MODELS = Run only selected group models leave empty run everything TORCHBENCH_ONLY_MODELS = m strip m os getenv TORCHBENCH_ONLY_MODELS split m strip filename = os path join os path dirname __file__ timm_models_list txt open filename fh lines = fh readlines lines = line rstrip line lines line lines model_name batch_size = line split TORCHBENCH_ONLY_MODELS model_name TORCHBENCH_ONLY_MODELS continue TIMM_MODELS model_name = int batch_size TODO - Figure out reason cold start memory spike BATCH_SIZE_DIVISORS = beit_base_patch _ deit_base_distilled_patch _ gluon_xception mobilevit_s swin_base_patch _window _ REQUIRE_HIGHER_TOLERANCE = inception_v mobilenetv _large_ REQUIRE_HIGHER_TOLERANCE_AMP = REQUIRE_EVEN_HIGHER_TOLERANCE = deit_base_distilled_patch _ vit_base_patch _siglip_ These models need higher tolerance MaxAutotune mode REQUIRE_EVEN_HIGHER_TOLERANCE_MAX_AUTOTUNE = REQUIRE_HIGHER_TOLERANCE_FOR_FREEZING = adv_inception_v SCALED_COMPUTE_LOSS = mobilevit_s FORCE_AMP_FOR_FP _BF _MODELS = SKIP_ACCURACY_CHECK_AS_EAGER_NON_DETERMINISTIC_MODELS = REQUIRE_LARGER_MULTIPLIER_FOR_SMALLER_TENSOR = inception_v mobilenetv _large_ refresh_model_names glob timm models list_models read_models_from_docs models = set TODO - set path pytorch-image-models repo fn glob glob pytorch-image-models docs models md open fn f while True line = f readline line break line startswith model = timm create_model continue model = line split print model models add model models get_family_name name known_families = darknet densenet dla dpn ecaresnet halo regnet efficientnet deit mobilevit mnasnet convnext resnet resnest resnext selecsls vgg xception known_family known_families known_family name known_family name startswith gluon_ gluon_ + name split _ name split _ populate_family models family = model_name models family_name = get_family_name model_name family_name family family family_name = family family_name append model_name family docs_models = read_models_from_docs all_models = list_models pretrained=True exclude_filters= k all_models_family = populate_family all_models docs_models_family = populate_family docs_models key docs_models_family del all_models_family key chosen_models = set chosen_models update value value docs_models_family values chosen_models update value key value all_models_family items filename = timm_models_list txt os path exists benchmarks filename = benchmarks + filename open filename w fw model_name sorted chosen_models fw write model_name + \n TimmRunner BenchmarkRunner __init__ super __init__ suite_name = timm_models property _config load_yaml_file timm_models yaml property _skip _config skip property skip_models_for_cpu _skip device cpu property skip_models_for_cpu_aarch _skip device cpu_aarch property skip_models _skip all property force_amp_for_fp _bf _models FORCE_AMP_FOR_FP _BF _MODELS property force_fp _for_bf _models set property get_output_amp_train_process_func property skip_accuracy_check_as_eager_non_deterministic args accuracy args training SKIP_ACCURACY_CHECK_AS_EAGER_NON_DETERMINISTIC_MODELS set property guard_on_nn_module_models property inline_inbuilt_nn_modules_models download_retry_decorator _download_model model_name model = create_model model_name in_chans= scriptable=False num_classes=None drop_rate= drop_path_rate=None drop_block_rate=None pretrained=True model load_model device model_name batch_size=None extra_args=None args enable_activation_checkpointing raise NotImplementedError Activation checkpointing implemented Timm models is_training = args training use_eval_mode = args use_eval_mode channels_last = _args channels_last model = _download_model model_name model None raise RuntimeError f Failed load model model_name model device=device memory_format=torch channels_last channels_last None data_config = resolve_data_config vars _args timmversion = _args model=model use_test_size=not is_training input_size = data_config input_size recorded_batch_size = TIMM_MODELS model_name model_name BATCH_SIZE_DIVISORS recorded_batch_size = max int recorded_batch_size BATCH_SIZE_DIVISORS model_name batch_size = batch_size recorded_batch_size torch manual_seed input_tensor = torch randint size= batch_size + input_size device=device dtype=torch float mean = torch mean input_tensor std_dev = torch std input_tensor example_inputs = input_tensor - mean std_dev channels_last example_inputs = example_inputs contiguous memory_format=torch channels_last example_inputs = example_inputs loss = torch nn CrossEntropyLoss device model_name SCALED_COMPUTE_LOSS compute_loss = scaled_compute_loss is_training use_eval_mode model train model eval validate_model model example_inputs device model_name model example_inputs batch_size iter_model_names args model_name list_models pretrained=True exclude_filters= k model_names = sorted TIMM_MODELS keys start end = get_benchmark_indices len model_names index model_name enumerate model_names index start index = end continue re search &#124; join args filter model_name re IGNORECASE re search &#124; join args exclude model_name re IGNORECASE model_name args exclude_exact model_name skip_models continue yield model_name pick_grad name is_training is_training torch enable_grad torch no_grad use_larger_multiplier_for_smaller_tensor name name REQUIRE_LARGER_MULTIPLIER_FOR_SMALLER_TENSOR get_tolerance_and_cosine_flag is_training current_device name cosine = args cosine tolerance = e- args freezing name REQUIRE_HIGHER_TOLERANCE_FOR_FREEZING conv-batchnorm fusion used under freezing may cause relatively large numerical difference We need larger tolerance Check https github com pytorch pytorch issues context tolerance = e- is_training torch _inductor config inductor_config name == beit_base_patch _ tolerance = e- name REQUIRE_EVEN_HIGHER_TOLERANCE inductor_config max_autotune name REQUIRE_EVEN_HIGHER_TOLERANCE_MAX_AUTOTUNE tolerance = e- name REQUIRE_HIGHER_TOLERANCE args amp name REQUIRE_HIGHER_TOLERANCE_AMP tolerance = e- tolerance = e- tolerance cosine compute_loss pred High loss values make gradient checking harder small changes accumulation order upsets accuracy checks reduce_to_scalar_loss pred scaled_compute_loss pred Loss values need zoom out further reduce_to_scalar_loss pred forward_pass mod inputs collect_outputs=True autocast autocast_arg mod inputs forward_and_backward_pass mod inputs collect_outputs=True cloned_inputs = clone_inputs inputs optimizer_zero_grad mod autocast autocast_arg pred = mod cloned_inputs isinstance pred tuple pred = pred loss = compute_loss pred grad_scaler scale loss backward optimizer_step collect_outputs collect_results mod None loss cloned_inputs None timm_main logging basicConfig level=logging WARNING warnings filterwarnings ignore main TimmRunner __name__ == __main__ timm_main