__future__ annotations abc abstractmethod copy copy typing Any TYPE_CHECKING tools testing test_run TestRun TYPE_CHECKING collections abc Iterable Iterator TestPrioritizations Describes results whether heuristics consider test relevant All different ranks tests disjoint meaning test can only one category they only declared initialization time A list can empty heuristic doesn t consider any tests category Important Lists tests must always returned deterministic order otherwise breaks test sharding logic _original_tests frozenset str _test_scores dict TestRun float __init__ tests_being_ranked Iterable str The tests being prioritized scores dict TestRun float - None _original_tests = frozenset tests_being_ranked _test_scores = TestRun test test _original_tests test score scores items set_test_score test score validate validate - None Union all TestRuns contain include exclude pairs all_tests = _test_scores keys files = test all_tests test test_file files files test test_file = copy test assert files test test_file test is_empty f Test run ` test ` overlaps ` files test test_file ` files test test_file &#124; = test test files values assert test is_full_file f All includes should have been excluded elsewhere vice versa Test run ` test ` violates noqa B Ensure set tests TestPrioritizations identical set tests passed assert _original_tests == set files keys The set tests TestPrioritizations must identical set tests passed _traverse_scores - Iterator tuple float TestRun Sort score then alphabetically test name test score sorted _test_scores items key=lambda x -x str x yield score test set_test_score test_run TestRun new_score float - None test_run test_file _original_tests We don t need test relevant_test_runs list TestRun = tr tr _test_scores keys tr test_run tr = test_run Set score all tests covered test_run same score _test_scores test_run = new_score Set score all tests covered test_run original score relevant_test_run relevant_test_runs old_score = _test_scores relevant_test_run del _test_scores relevant_test_run not_to_be_updated = relevant_test_run - test_run not_to_be_updated is_empty _test_scores not_to_be_updated = old_score validate add_test_score test_run TestRun score_to_add float - None test_run test_file _original_tests relevant_test_runs list TestRun = tr tr _test_scores keys tr test_run relevant_test_run relevant_test_runs old_score = _test_scores relevant_test_run del _test_scores relevant_test_run intersection = relevant_test_run test_run intersection is_empty _test_scores intersection = old_score + score_to_add not_to_be_updated = relevant_test_run - test_run not_to_be_updated is_empty _test_scores not_to_be_updated = old_score validate get_all_tests - list TestRun Returns all tests TestPrioritizations x x _traverse_scores get_top_per_tests n int - tuple list TestRun list TestRun Divides list tests into two based top n scores The first list top second rest tests = x x _traverse_scores index = n len tests + tests index tests index get_info_str verbose bool = True - str info = score test _traverse_scores verbose score == continue info += f test score \n info rstrip print_info - None print get_info_str get_priority_info_for_test test_run TestRun - dict str Any Given failing test returns information about s prioritization we want emit our metrics idx score test enumerate _traverse_scores Different heuristics may result given test file being split into different test runs so look overlapping tests find match test test_run position idx score score raise AssertionError f Test run test_run found get_test_stats test TestRun - dict str Any test_name test test_file test_filters test get_pytest_filter get_priority_info_for_test test max_score max score score _ _traverse_scores min_score min score score _ _traverse_scores all_scores str test score test score _test_scores items to_json - dict str Any Returns JSON dict describes TestPrioritizations object json_dict = _test_scores test to_json score test score _test_scores items score = _original_tests list _original_tests json_dict staticmethod from_json json_dict dict str Any - TestPrioritizations Returns TestPrioritizations object JSON dict test_prioritizations = TestPrioritizations tests_being_ranked=json_dict _original_tests scores= TestRun from_json testrun_json score testrun_json score json_dict _test_scores test_prioritizations amend_tests tests list str - None Removes tests given list TestPrioritizations Adds tests list TestPrioritizations valid_scores = test score test score _test_scores items test test_file tests _test_scores = valid_scores test tests test _original_tests _test_scores TestRun test = _original_tests = frozenset tests validate AggregatedHeuristics Aggregates results across all heuristics It saves individual results each heuristic exposes aggregated view _heuristic_results dict HeuristicInterface TestPrioritizations Key Heuristic s name Dicts will preserve order insertion which important sharding _all_tests frozenset str __init__ all_tests list str - None _all_tests = frozenset all_tests _heuristic_results = validate validate - None heuristic heuristic_results _heuristic_results items heuristic_results validate assert heuristic_results _original_tests == _all_tests f Tests heuristic name same tests AggregatedHeuristics add_heuristic_results heuristic HeuristicInterface heuristic_results TestPrioritizations - None heuristic _heuristic_results raise ValueError f We already have heuristics heuristic name _heuristic_results heuristic = heuristic_results validate get_aggregated_priorities include_trial bool = False - TestPrioritizations Returns aggregated priorities across all heuristics valid_heuristics = heuristic heuristic_results heuristic heuristic_results _heuristic_results items heuristic trial_mode include_trial new_tp = TestPrioritizations _all_tests heuristic_results valid_heuristics values score testrun heuristic_results _traverse_scores new_tp add_test_score testrun score new_tp validate new_tp get_test_stats test TestRun - dict str Any Returns aggregated statistics given test stats dict str Any = test_name test test_file test_filters test get_pytest_filter Get metrics about heuristics used heuristics = heuristic heuristic_results _heuristic_results items metrics = heuristic_results get_priority_info_for_test test metrics heuristic_name = heuristic name metrics trial_mode = heuristic trial_mode heuristics append metrics stats heuristics = heuristics stats aggregated = get_aggregated_priorities get_priority_info_for_test test stats aggregated_trial = get_aggregated_priorities include_trial=True get_priority_info_for_test test stats to_json - dict str Any Returns JSON dict describes AggregatedHeuristics object json_dict dict str Any = heuristic heuristic_results _heuristic_results items json_dict heuristic name = heuristic_results to_json json_dict HeuristicInterface Interface all heuristics description str When trial mode set True heuristic s predictions will used reorder tests It s results will however emitted metrics trial_mode bool abstractmethod __init__ kwargs Any - None trial_mode = kwargs get trial_mode False type ignore assignment property name - str __class__ __name__ __str__ - str name abstractmethod get_prediction_confidence tests list str - TestPrioritizations Returns float ranking ranging - where negative means skip positive means run means no idea magnitude = how confident heuristic Used AggregatedHeuristicsRankings