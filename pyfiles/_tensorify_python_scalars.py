__future__ annotations logging os typing Any Union sympy Integer Number Symbol sympy logic boolalg BooleanAtom torch torch fx fx torch _dynamo exc TensorifyScalarRestartAnalysis torch _dynamo symbolic_convert TensorifyState torch _dynamo utils get_metrics_context torch _prims_common get_computation_dtype torch _subclasses fake_tensor noqa TCH torch _subclasses fake_tensor FakeTensor torch _utils_internal justknobs_check torch fx _utils lazy_format_graph_code torch fx experimental symbolic_shapes noqa TCH guard_scalar has_free_symbols ShapeEnv torch fx graph_module GraphModule noqa TCH TODO refactor torch fx passes runtime_assert _get_sym_val torch fx proxy MetaProxy torch utils _sympy interp _run_sympy_handler sympy_interp torch utils _sympy reference TensorReferenceAnalysis torch utils _sympy symbol symbol_is_type SymT __all__ list str = log = logging getLogger __name__ graph_code_log = torch _logging getArtifactLogger __name__ graph_code_verbose The general shape transformation look Tensor operations take backed SymFloat argument then redo them tensor compute ints tensors inputs For example add Tensor Scalar can translated into add Tensor Tensor Because Dynamo has already arranged floats Tensor inputs graph typical float compute you can entirely translate Python float operations into Tensor operations only Tensor inputs This pass also responsible doing CSE fly we do since you don t want keep recomputing same quantity over over again s used multiple times This pass runs JOINT graph produced AOT Autograd prior partitioning The primary goal pass eliminate floats replacing TensorScalar operations TensorTensor operations then Dead Code Elimination DCE item calls which effectively removes floats This needs happen before partitioning because influences partitioning decisions specifically ensuring we don t need save floats across partitions Additionally there separate pass changes which device computations occur That pass must run after one still before partitioning HISTORY NOTE Originally I wanted formulate pass pushing item calls down transforming float compute into int compute we went If you manage eliminate all float compute ends up being equivalent there critical difference when some floats cannot eliminated when we call item them what should s SymFloat Ideally would same backed SymFloat we had before But without symbolic expression propagation tensor quantities repropagating would instead give you unbacked SymFloat Maybe good idea implement symbolic propagation d scalar tensors I decided go something simpler start The boring stuff What operators can I Tensor-ify Anything Scalar argument How do I Tensor-ify SymFloat sympy expression Sympy - Op Handler - Tensor TODO make sure runs before CPU- CUDA pass cudagraph friendliness SUPPORTED_OPS = torch ops aten mul Tensor torch ops aten mul Tensor torch ops aten add Tensor torch ops aten add Tensor torch ops aten sub Tensor torch ops aten sub Tensor torch ops aten div Tensor torch ops aten div Tensor torch ops aten gt Scalar torch ops aten gt Tensor torch ops aten lt Scalar torch ops aten lt Tensor torch ops aten ge Scalar torch ops aten ge Tensor torch ops aten le Scalar torch ops aten le Tensor torch ops aten eq Scalar torch ops aten eq Tensor torch ops aten ne Scalar torch ops aten ne Tensor torch fx _compatibility compatibility is_backward_compatible=False tensorify_python_scalars gm GraphModule shape_env ShapeEnv fake_mode fake_tensor FakeTensorMode - None Converts Python scalar operations into Tensor operations within graph This pass looks Tensor operations involve SymFloat arguments transforms them into equivalent operations use only Tensor inputs Args gm The FX graph module representing computation graph shape_env The shape environment responsible symbolic shape tracking propagation during graph transformations Returns None sympy knob = True env = os getenv TENSORIFY_PYTHON_SCALARS None env FALSE knob = False knob = justknobs_check pytorch compiler tensorify_python_scalars knob None graph = gm graph tracer = fx proxy GraphAppendingTracer graph expr_to_sym_proxy dict sympy Expr MetaProxy = expr_to_tensor_proxy dict sympy Expr MetaProxy = tensorified_symbols set sympy Symbol = set should_restart = False first_non_placeholder = None placeholders = set node graph nodes node op = placeholder first_non_placeholder = node break placeholders add node Analysis = TensorReferenceAnalysis _sympy_interp expr sympy Expr - MetaProxy sympy_interp hash consing special handling generating constants correctly hash cons isinstance expr Symbol expr expr_to_tensor_proxy This guaranteed populated invariant established insert_deferred_runtime_asserts expr_to_tensor_proxy expr = torch ops aten scalar_tensor default expr_to_sym_proxy expr cache constants why isinstance expr Integer Number BooleanAtom dtype = None c Union bool int float isinstance expr BooleanAtom dtype = torch bool c = bool expr isinstance expr sympy Integer dtype = torch int c = int expr isinstance expr sympy Number dtype = torch float c = float expr node = graph call_function torch ops aten scalar_tensor default pyrefly ignore unbound-name c dtype dtype fake_mode pyrefly ignore unbound-name node meta val = torch ops aten scalar_tensor default c dtype=dtype expr_to_tensor_proxy expr = MetaProxy node tracer=tracer fake_mode=fake_mode expr expr_to_tensor_proxy expr_to_tensor_proxy expr don t cache isinstance expr Symbol sympy_interp Analysis expr_to_tensor_proxy expr type ignore arg-type hash cons arguments run expr handler expr_to_tensor_proxy expr = _run_sympy_handler Analysis _sympy_interp arg arg expr args type ignore arg-type expr expr_to_tensor_proxy expr failed_tensorify_ops set str = set nodes = list graph nodes i node enumerate nodes - graph inserting_before nodes i + node placeholders first_non_placeholder Look tensor item calls placeholders node None node op == call_function node target torch ops aten _local_scalar_dense default dtype = node args meta val dtype dtype is_floating_point continue assert isinstance node args fx Node node args s = node meta val node expr expr_to_tensor_proxy s = MetaProxy node args tracer=tracer fake_mode=fake_mode Upcast float tensor torch float avoid precision problem expr_to_tensor_proxy s = torch ops prims convert_element_type default expr_to_tensor_proxy s torch float expr_to_sym_proxy s = MetaProxy node tracer=tracer fake_mode=fake_mode pyrefly ignore bad-argument-type sym_expr = _get_sym_val node None sym_expr expr_to_sym_proxy isinstance sym_expr sympy Number sympy logic boolalg BooleanAtom expr_to_sym_proxy sym_expr = MetaProxy pyrefly ignore bad-argument-type node tracer=tracer fake_mode=fake_mode Specialize all dimensions contain symfloats Here s example test requires PYTORCH_OPINFO_SAMPLE_INPUT_INDEX= python test inductor test_torchinductor_opinfo py TestInductorOpInfoCUDA test_comprehensive_nn_functional_interpolate_bicubic_cuda_float noqa B pyrefly ignore missing-attribute val = node meta get val isinstance val FakeTensor dim val shape isinstance dim torch SymInt s dim node expr free_symbols name = str s symbol_is_type s SymT FLOAT TensorifyState should_specialize name In principle we could support float input used do size compute The problem we don t actually want tensorify compute case which means we need codegen support all symfloats TensorifyState specialize name should_restart = True Look functions convert pyrefly ignore missing-attribute node op == call_function pyrefly ignore missing-attribute replacement_op = SUPPORTED_OPS get node target args list Any = transform = False pyrefly ignore missing-attribute compute_dtype = get_computation_dtype node meta val dtype pyrefly ignore missing-attribute node args isinstance fx Node val meta isinstance zf = meta val torch SymFloat transform = True try proxy = _sympy_interp zf node expr except NotImplementedError transform = False break We use _expr instead expr b c we want symbol replacement tensorified_symbols add meta val node _expr The upcasting irrelevant when compute dtype bool This happens cases where we tensorifying comparison operator such torch ops aten gt Tensor compute_dtype = torch bool proxy node meta val dtype = compute_dtype proxy = torch ops prims convert_element_type default proxy compute_dtype args append proxy isinstance fx Node args append MetaProxy tracer=tracer fake_mode=fake_mode args append transform replacement_proxy = replacement_op args pyrefly ignore missing-attribute compute_dtype = node meta val dtype replacement_proxy = torch ops prims convert_element_type default replacement_proxy node meta val dtype pyrefly ignore missing-attribute node replace_all_uses_with replacement_proxy node pyrefly ignore bad-argument-type graph erase_node node metrics_context = get_metrics_context metrics_context in_progress metrics_context set tensorify_float_success True overwrite=True pyrefly ignore missing-attribute node args isinstance fx Node val meta isinstance zf = meta val torch SymFloat pyrefly ignore missing-attribute failed_tensorify_ops update str node target pyrefly ignore missing-attribute log info Failed tensorify s str node target Now do one more pass specializes all symfloats we didn t manage tensorify away node reversed graph nodes node op == output node op == placeholder continue graph inserting_before node len node users == node is_impure graph erase_node node continue isinstance val = node meta get val torch SymFloat torch SymInt torch SymBool has_free_symbols val node expr all symbol_is_type s SymT FLOAT s val node expr free_symbols If all symbols backed symfloats we can just specialize whole node get more precise guards eg zf = item zf = zf op zf It s better guard zf == than zf == node replace_all_uses_with guard_scalar val graph erase_node node Sometimes time we get tensorify there have already been specializations eg python_arg_parser h In these cases placeholder nodes no longer have reference their original symfloat thus we need deduce specializations have happened via shape_env replacements NB there s important invariant here symfloats keep consistent names across restarts k v shape_env var_to_val items symbol_is_type k SymT FLOAT isinstance v sympy core numbers Float name = str k TensorifyState should_specialize name k tensorified_symbols TensorifyState specialize name should_restart = True should_restart Sledgehammer time Restart dynamo analysis keeping track which input sources no longer needed should specialized Restarting analysis necessary because we need instruct Dynamo NOT make these inputs metrics_context = get_metrics_context metrics_context in_progress metrics_context set tensorify_float_failure failed_tensorify_ops overwrite=True metrics_context set tensorify_float_success True overwrite=True raise TensorifyScalarRestartAnalysis graph_code_log debug s lazy_format_graph_code tensorify_python_scalars gm colored=True