Owner s oncall quantization ruff noqa F torch math typing Union torch ao quantization FakeQuantize MovingAverageMinMaxObserver default_observer default_fixed_qparams_range_ _fake_quant torch ao quantization _learnable_fake_quantize _LearnableFakeQuantize torch testing _internal common_quantized _fake_quantize_per_channel_affine_reference _fake_quantize_per_channel_affine_grad_reference to_tensor torch nn nn Standard library io itertools unittest numpy np Testing utils hypothesis given settings hypothesis strategies st torch testing _internal hypothesis_utils hu hu assert_deadline_disabled torch testing _internal common_cuda TEST_CUDA TEST_WITH_ROCM torch testing _internal common_utils TestCase skipIfTorchDynamo Reference method fake quantize Note because scale zero_point left float actual kernel mimics how fake_quant works float _fake_quantize_per_tensor_affine_reference X scale zero_point quant_min quant_max dtype = X dtype res = torch clamp torch round X torch float scale + zero_point quant_min quant_max - zero_point scale res dtype Reference method gradient fake quantize operator Note because scale zero_point left float actual kernel mimics how fake_quant works float _fake_quantize_per_tensor_affine_grad_reference dY X scale zero_point quant_min quant_max dtype = X dtype Xq = torch round X torch float scale + zero_point mask = Xq = quant_min Xq = quant_max res = torch zeros_like dY res mask = dY mask res dtype Reference method gradients fake quantize operator _fake_quantize_learnable_per_tensor_affine_grad_reference dY X scale zero_point quant_min quant_max device dtype r This method references following literatures back propagation scale zero point - https arxiv org pdf pdf - https arxiv org pdf pdf dtype torch bfloat dY = dY dtype=torch float X = X dtype=torch float scale = scale dtype=torch float zero_point = zero_point dtype=torch float zero_point_rounded = int zero_point + clamp quant_min quant_max item Xq = torch round X scale + zero_point_rounded indicate_small_scale = Xq quant_min float device indicate_big_scale = Xq quant_max float device indicate_middle_scale = torch ones indicate_small_scale shape device - \ indicate_small_scale - indicate_big_scale indicate_saturate_zp = Xq quant_min float + Xq quant_max float device indicate_unsaturate_zp = torch ones indicate_saturate_zp shape device - indicate_saturate_zp Xq = Xq clamp quant_min quant_max Xfq = Xq - zero_point_rounded scale grad_small_scale = quant_min - zero_point_rounded grad_big_scale = quant_max - zero_point_rounded grad_middle_scale = Xfq - X scale device grad_saturate_zp = -scale device grad_unsaturate_zp = grad_scale = indicate_small_scale grad_small_scale + \ indicate_big_scale grad_big_scale + \ indicate_middle_scale grad_middle_scale grad_zp = indicate_saturate_zp grad_saturate_zp + \ indicate_unsaturate_zp grad_unsaturate_zp grad_X = _fake_quantize_per_tensor_affine_grad_reference dY X scale zero_point quant_min quant_max device grad_scale = grad_scale dY sum unsqueeze dim= grad_zp = grad_zp dY sum unsqueeze dim= dtype torch bfloat grad_X = grad_X torch bfloat grad_scale = grad_scale torch bfloat grad_zp = grad_zp torch bfloat grad_X grad_scale grad_zp Reference method quantization _quantize_per_tensor x scale zero_point quant_min quant_max x scale + zero_point round clamp quant_min quant_max Reference method per channel gradients learnable fake quantize operator _fake_quantize_learnable_per_channel_affine_grad_reference dY X per_channel_scale per_channel_zero_point axis quant_min quant_max device dtype r This method references following literatures back propagation scale zero point - https arxiv org pdf pdf - https arxiv org pdf pdf dtype torch bfloat dY = dY dtype=torch float X = X dtype=torch float per_channel_scale = per_channel_scale dtype=torch float per_channel_zero_point = per_channel_zero_point dtype=torch float per_channel_zero_point = per_channel_zero_point detach + clamp quant_min quant_max type torch int grad_X = _fake_quantize_per_channel_affine_grad_reference dY X per_channel_scale per_channel_zero_point axis quant_min quant_max device per_channel_scale = per_channel_scale detach type torch float grad_scale = torch zeros per_channel_scale size device grad_zero_point = torch zeros per_channel_zero_point size device X_flattened = torch unbind X dim=axis dY_flattened = torch unbind dY dim=axis i X_i enumerate torch unbind X dim=axis scale_i = per_channel_scale i zero_point_i = per_channel_zero_point i X_i = X_flattened i dY_i = dY_flattened i Xq_i = X_i scale_i + zero_point_i round Xfq_i = Xq_i - zero_point_i scale_i indicate_small_scale_i = Xq_i quant_min float device indicate_big_scale_i = Xq_i quant_max float device indicate_middle_scale_i = torch ones indicate_small_scale_i shape device - \ indicate_small_scale_i - indicate_big_scale_i indicate_saturate_zp_i = Xq_i quant_min float + Xq_i quant_max float device indicate_unsaturate_zp_i = torch ones indicate_saturate_zp_i shape device - \ indicate_saturate_zp_i Xq_i = Xq_i clamp quant_min quant_max Xfq_i = Xq_i - zero_point_i scale_i grad_small_scale_i = quant_min - zero_point_i grad_big_scale_i = quant_max - zero_point_i grad_middle_scale_i = Xfq_i - X_i scale_i device grad_saturate_zp_i = -scale_i device grad_unsaturate_zp_i = grad_scale_i = indicate_small_scale_i grad_small_scale_i + \ indicate_middle_scale_i grad_middle_scale_i + \ indicate_big_scale_i grad_big_scale_i grad_zp_i = indicate_saturate_zp_i grad_saturate_zp_i + \ indicate_unsaturate_zp_i grad_unsaturate_zp_i grad_scale_i = grad_scale_i dY_i sum unsqueeze dim= grad_zp_i = grad_zp_i dY_i sum unsqueeze dim= grad_scale i = grad_scale_i grad_zero_point i = grad_zp_i dtype torch bfloat we downcast before returning gradients mimic autograd s downcasting dtype torch bfloat grad_X = grad_X torch bfloat grad_scale = grad_scale torch bfloat grad_zero_point = grad_zero_point torch bfloat grad_X grad_scale grad_zero_point _get_tensor_min_max X torch Tensor running_min Union float torch Tensor = float inf running_max Union float torch Tensor = float -inf averaging_const float = dtype torch dtype = torch float - tuple float float min_val_tensor = X min dtype=dtype max_val_tensor = X max dtype=dtype averaging_const_tensor = torch tensor averaging_const dtype=dtype item isinstance running_min torch Tensor running_min = torch tensor running_min dtype=dtype isinstance running_max torch Tensor running_max = torch tensor running_max dtype=dtype torch isinf running_min min_val_tensor = running_min + averaging_const_tensor min_val_tensor - running_min torch isinf running_max max_val_tensor = running_max + averaging_const_tensor max_val_tensor - running_max min_val_tensor item max_val_tensor item _get_per_row_min_max x torch Tensor min_vals torch Tensor max_vals torch Tensor axis int = averaging_const float = - tuple torch Tensor torch Tensor x_dim = x size new_axis_list = i i range len x_dim noqa C new_axis_list axis = new_axis_list = axis y = x permute new_axis_list y = torch flatten y start_dim= min_vals max_vals = torch aminmax y dim= math isinf min_vals math isinf max_vals min_vals max_vals = torch aminmax y dim= min_vals_cur max_vals_cur = torch aminmax y dim= min_vals = min_vals + averaging_const min_vals_cur - min_vals max_vals = max_vals + averaging_const max_vals_cur - max_vals min_vals max_vals _get_scale_zp min_val float max_val float dtype torch dtype reduce_range bool = False preserve_sparsity bool = False - tuple float int Calculate quantization parameters scale zero_point based min max element tensor dtype == torch qint reduce_range qmin qmax = - qmin qmax = - reduce_range qmin qmax = qmin qmax = min_val max_val preserve_sparsity symmetric_qmin = int - qmax - qmin + symmetric_qmax = int qmax - qmin max_scale = max abs min_val symmetric_qmin abs max_val symmetric_qmax min_val = max_scale symmetric_qmin max_val = max_scale symmetric_qmax min_val = min min_val max_val = max max_val scale = max_val - min_val qmax - qmin scale == math isinf scale scale = zero_point = zero_point_from_min = qmin - min_val float scale zero_point_from_max = qmax - max_val float scale zero_point_from_min_error = abs qmin - abs min_val float scale zero_point_from_max_error = abs qmax - abs max_val float scale zero_point_from_min_error zero_point_from_max_error initial_zero_point = zero_point_from_min initial_zero_point = zero_point_from_max min_val max_val preserve_sparsity initial_zero_point = qmin + qmax + nudged_zero_point = initial_zero_point qmin nudged_zero_point = qmin initial_zero_point qmax nudged_zero_point = qmax nudged_zero_point = int round initial_zero_point scale int nudged_zero_point NP_RANDOM_SEED = tolerance = e- TestFakeQuantizeOps TestCase given device=st sampled_from cpu cuda torch cuda is_available cpu X=hu tensor shapes=hu array_shapes qparams=hu qparams dtypes=torch quint test_forward_per_tensor device X r Tests forward path FakeQuantizePerTensorAffine op np random seed NP_RANDOM_SEED X scale zero_point torch_type = X quant_min = torch iinfo torch_type min quant_max = torch iinfo torch_type max X = to_tensor X device Y = _fake_quantize_per_tensor_affine_reference X cpu scale zero_point quant_min quant_max Y_prime = torch fake_quantize_per_tensor_affine X scale zero_point quant_min quant_max np testing assert_allclose Y Y_prime cpu rtol=tolerance atol=tolerance given device=st sampled_from cpu cuda torch cuda is_available cpu X=hu tensor shapes=hu array_shapes qparams=hu qparams dtypes=torch quint unittest skip temporarily disable test test_backward_per_tensor device X r Tests backward method np random seed NP_RANDOM_SEED X scale zero_point torch_type = X quant_min = torch iinfo torch_type min quant_max = torch iinfo torch_type max X = to_tensor X device X requires_grad_ Y = _fake_quantize_per_tensor_affine_reference X cpu scale zero_point quant_min quant_max Y_prime = torch fake_quantize_per_tensor_affine X scale zero_point quant_min quant_max dout = torch rand_like X dtype=torch float device dX = _fake_quantize_per_tensor_affine_grad_reference dout X scale zero_point quant_min quant_max Y_prime backward dout np testing assert_allclose dX cpu X grad cpu detach numpy rtol=tolerance atol=tolerance test_forward_backward_per_tensor_with_amp net = nn Sequential nn Conv d net qconfig = torch ao quantization get_default_qat_qconfig fbgemm net_prep = torch ao quantization prepare_qat net torch cuda amp autocast x = torch randn out = net_prep x sum out backward assertTrue net_prep weight grad None test_forward_per_tensor_half_precision_numerics scale = zero = maxi = mini = _ range X = torch randn torch float Y = torch fake_quantize_per_tensor_affine X scale zero mini maxi Y r = _fake_quantize_per_tensor_affine_reference X scale zero mini maxi assertEqual Y Y r rtol=tolerance atol=tolerance force overflow X = torch tensor + torch float Y = torch fake_quantize_per_tensor_affine X scale zero mini maxi Y r = _fake_quantize_per_tensor_affine_reference X scale zero mini maxi assertEqual Y Y r rtol=tolerance atol=tolerance scale = force underflow X = torch tensor - torch float Y = torch fake_quantize_per_tensor_affine X scale zero mini maxi Y r = _fake_quantize_per_tensor_affine_reference X scale zero mini maxi assertEqual Y Y r rtol=tolerance atol=tolerance _test_forward_per_tensor_cachemask_impl device float_types = torch float torch float torch float torch bfloat torch_types = torch qint torch quint Xs = torch randn device=device torch randn device=device tensor_qparam = True False float_type torch_type X tensor_qparams itertools product float_types torch_types Xs tensor_qparam pick scale + zp so some values get clipped X = X float_type obs = torch ao quantization MinMaxObserver torch_type obs device obs X scale zero_point = obs calculate_qparams quant_min quant_max = obs quant_min obs quant_max tensor_qparam scale zero_point = float scale int zero_point Y_test = torch fake_quantize_per_tensor_affine X scale zero_point quant_min quant_max Y_ref = _fake_quantize_per_tensor_affine_reference X scale zero_point quant_min quant_max device assertEqual Y_test Y_ref rtol=tolerance atol=tolerance assertTrue Y_test dtype == float_type test_forward_per_tensor_cachemask_cpu device = torch device cpu _test_forward_per_tensor_cachemask_impl device unittest skipIf TEST_CUDA No gpu available test_forward_per_tensor_cachemask_cuda device = torch device cuda _test_forward_per_tensor_cachemask_impl device _test_backward_per_tensor_cachemask_impl device float_types = torch float torch float torch float torch_types = torch qint torch quint tensor_qparams = True False float_type torch_type tensor_qparam itertools product float_types torch_types tensor_qparams X = torch randn device float_type X requires_grad_ pick scale + zp so some values get clipped obs = torch ao quantization MinMaxObserver torch_type obs device obs X scale zero_point = obs calculate_qparams tensor_qparam scale zero_point = float scale int zero_point quant_min quant_max = obs quant_min obs quant_max forward pass Y_test = torch fake_quantize_per_tensor_affine X scale zero_point quant_min quant_max Y_ref = _fake_quantize_per_tensor_affine_reference X scale zero_point quant_min quant_max device assertEqual Y_test Y_ref rtol=tolerance atol=tolerance backward pass dout = torch rand_like X dtype=torch float device dX = _fake_quantize_per_tensor_affine_grad_reference dout X scale zero_point quant_min quant_max Y_test backward dout assertEqual dX X grad assertTrue X grad dtype == float_type test_backward_per_tensor_cachemask_cpu device = torch device cpu _test_backward_per_tensor_cachemask_impl device unittest skipIf TEST_CUDA No gpu available test_backward_per_tensor_cachemask_cuda device = torch device cuda _test_backward_per_tensor_cachemask_impl device _test_learnable_forward_per_tensor X device scale_base zero_point_base X_base = torch tensor X device n_bits quant_min quant_max = n_bits - X = X_base clone float scale_base = scale_base device float zero_point_base = zero_point_base dtype=torch int device=device scale = scale_base clone zero_point = zero_point_base clamp quant_min quant_max Y = _fake_quantize_per_tensor_affine_reference X scale zero_point quant_min quant_max device grad_factor Y_prime = torch _fake_quantize_learnable_per_tensor_affine X scale zero_point quant_min quant_max grad_factor device assertTrue torch allclose Y Y_prime rtol=tolerance atol=tolerance Expected kernel forward function have results match reference forward function given X=hu tensor shapes=hu array_shapes elements=hu floats - e e allow_nan=False allow_infinity=False qparams=hu qparams dtypes=torch quint unittest skip broken without changes any relevant code we need remove hypothesis testing CI test_learnable_forward_per_tensor_cpu X X _ _ _ = X scale_base = torch normal mean= std= size= clamp e- zero_point_base = torch normal mean= std= size= _test_learnable_forward_per_tensor X cpu scale_base zero_point_base given X=hu tensor shapes=hu array_shapes elements=hu floats - e e allow_nan=False allow_infinity=False qparams=hu qparams dtypes=torch quint unittest skipIf TEST_CUDA No gpu available test_learnable_forward_per_tensor_cuda X X _ _ _ = X scale_base = torch normal mean= std= size= clamp e- zero_point_base = torch normal mean= std= size= _test_learnable_forward_per_tensor X cuda scale_base zero_point_base _test_learnable_backward_per_tensor X device scale_base zero_point_base dtype=torch float r Tests backward method additional backprop support scale zero point X_base = torch tensor X device n_bits quant_min quant_max = n_bits - X = X_base clone device X requires_grad_ scale_base = scale_base device zero_point_base = zero_point_base device scale = scale_base clone scale requires_grad_ zero_point = zero_point_base clone clamp quant_min quant_max zero_point requires_grad_ grad_factor Y_prime = torch _fake_quantize_learnable_per_tensor_affine X scale zero_point quant_min quant_max grad_factor device dout = torch rand_like X dtype=torch float device dX dScale dZeroPoint = _fake_quantize_learnable_per_tensor_affine_grad_reference dout X scale zero_point quant_min quant_max device dtype Y_prime backward dout expected_dX = dX device detach actual_dX = X grad device detach expected_dScale = dScale device detach actual_dScale = scale grad device detach expected_dZeroPoint = dZeroPoint device detach actual_dZeroPoint = zero_point grad device detach assertTrue torch allclose expected_dX actual_dX rtol=tolerance atol=tolerance Expected dX match X grad assertTrue torch allclose expected_dScale grad_factor actual_dScale rtol=tolerance atol=tolerance Expected dScale match scale grad assertTrue torch allclose expected_dZeroPoint grad_factor actual_dZeroPoint rtol=tolerance atol=tolerance Expected dZeroPoint match zero_point grad X grad data zero_ scale grad data zero_ zero_point grad data zero_ given X=hu tensor shapes=hu array_shapes elements=hu floats - e e allow_nan=False allow_infinity=False qparams=hu qparams dtypes=torch quint test_learnable_backward_per_tensor_cpu X torch random manual_seed NP_RANDOM_SEED X _ _ _ = X scale_base = torch normal mean= std= size= clamp e- zero_point_base = torch normal mean= std= size= _test_learnable_backward_per_tensor X cpu scale_base zero_point_base unittest skipIf TEST_CUDA No gpu available test_learnable_backward_per_tensor_cuda setting seed avoid increasing tolerance due cases where difference Python vs CPP downcasting causes tensor mismatches e g vs before downcasting vs after downcasting Python vs CPP op torch random manual_seed x_shape = dtype torch bfloat torch float X_base = torch randn x_shape dtype=dtype device= cuda scale_base = torch normal mean= std= size= clamp e- dtype=dtype zero_point_base = torch normal mean= std= size= dtype=dtype _test_learnable_backward_per_tensor X_base cuda scale_base zero_point_base dtype given device=st sampled_from cpu cuda torch cuda is_available cpu X=hu tensor shapes=hu array_shapes qparams=hu qparams dtypes= torch quint test_fq_module_per_tensor device X np random seed NP_RANDOM_SEED X scale zero_point torch_type = X quant_min = torch iinfo torch_type min quant_max = torch iinfo torch_type max X = to_tensor X device X requires_grad_ fq_module = torch ao quantization default_fake_quant device Y_prime = fq_module X assert fq_module scale None assert fq_module zero_point None Y = _fake_quantize_per_tensor_affine_reference X fq_module scale fq_module zero_point quant_min quant_max np testing assert_allclose Y cpu detach numpy Y_prime cpu detach numpy rtol=tolerance atol=tolerance Test backward dout = torch rand_like X dtype=torch float device=device Y_prime backward dout dX = _fake_quantize_per_tensor_affine_grad_reference dout X fq_module scale fq_module zero_point quant_min quant_max np testing assert_allclose dX cpu numpy X grad cpu detach numpy rtol=tolerance atol=tolerance given device=st sampled_from cpu cuda torch cuda is_available cpu X=hu tensor shapes=hu array_shapes qparams=hu qparams dtypes=torch quint test_fixed_qparams_fq_module device X X scale zero_point torch_type = X X = to_tensor X device fq_module = default_fixed_qparams_range_ _fake_quant fq_module device fixed_scale = fq_module scale clone fixed_zero_point = fq_module zero_point clone run fq module make sure quantization parameters does change torch ao quantization enable_observer fq_module fq_module X assertEqual fixed_scale fq_module scale assertEqual fixed_zero_point fq_module zero_point test_fq_serializable_per_tensor observer = default_observer quant_min = quant_max = FakeQuantizeClass FakeQuantize _LearnableFakeQuantize fq_module = FakeQuantizeClass observer quant_min quant_max X = torch tensor - - - dtype=torch float y_ref = fq_module X state_dict = fq_module state_dict assertEqual state_dict scale assertEqual state_dict zero_point b = io BytesIO torch save state_dict b weights_only True False b seek loaded_dict = torch load b weights_only=weights_only loaded_fq_module = FakeQuantizeClass observer quant_min quant_max loaded_fq_module load_state_dict loaded_dict key state_dict assertEqual state_dict key loaded_fq_module state_dict key assertEqual loaded_fq_module calculate_qparams fq_module calculate_qparams test_fake_quant_control fq_module torch ao quantization default_fake_quant _LearnableFakeQuantize with_args observer=MovingAverageMinMaxObserver quant_min= quant_max= dtype=torch quint qscheme=torch per_tensor_affine reduce_range=True torch manual_seed X = torch rand dtype=torch float Output fake quant identical input Y = fq_module X assertNotEqual Y X type fq_module _LearnableFakeQuantize fq_module toggle_fake_quant False torch ao quantization disable_fake_quant fq_module X = torch rand dtype=torch float Y = fq_module X Fake quant disabled output identical input assertEqual Y X Explicit copy point time because FakeQuant keeps internal state mutable buffers scale = fq_module scale detach clone zero_point = fq_module zero_point detach clone type fq_module _LearnableFakeQuantize fq_module toggle_observer_update False fq_module toggle_fake_quant True torch ao quantization disable_observer fq_module torch ao quantization enable_fake_quant fq_module X = torch rand dtype=torch float - Y = fq_module X assertNotEqual Y X Observer disabled scale zero-point do change assertEqual fq_module scale scale assertEqual fq_module zero_point zero_point type fq_module _LearnableFakeQuantize fq_module toggle_observer_update True torch ao quantization enable_observer fq_module Y = fq_module X assertNotEqual Y X Observer enabled scale zero-point different assertNotEqual fq_module scale scale assertNotEqual fq_module zero_point zero_point test_fake_quant_preserves_qparam_shapes_for_activations Model nn Module __init__ - None super __init__ linear = nn Linear forward x x = linear x x m = Model m qconfig = torch ao quantization get_default_qat_qconfig fbgemm torch ao quantization prepare_qat m inplace=True scale_shape_before = m linear activation_post_process scale shape zero_point_shape_before = m linear activation_post_process zero_point shape x = torch rand m x scale_shape_after = m linear activation_post_process scale shape zero_point_shape_after = m linear activation_post_process zero_point shape assertEqual scale_shape_before scale_shape_after msg= FakeQuant scale shape must stay consistent assertEqual zero_point_shape_before zero_point_shape_after msg= FakeQuant zero_point shape must stay consistent fake_quant_scriptable observer = default_observer quant_min = quant_max = FakeQuantizeClass FakeQuantize _LearnableFakeQuantize fq_module = FakeQuantizeClass observer quant_min quant_max scripted_module = torch jit script fq_module X = torch tensor - - - dtype=torch float fq_module X scripted_module X assertEqual fq_module calculate_qparams scripted_module calculate_qparams buf = io BytesIO torch jit save scripted_module buf buf seek loaded_module = torch jit load buf assertEqual fq_module calculate_qparams loaded_module calculate_qparams given device=st sampled_from cpu cuda torch cuda is_available cpu X=hu per_channel_tensor shapes=hu array_shapes qparams=hu qparams dtypes=torch quint test_forward_per_channel device X r Tests forward path FakeQuantizePerTensorAffine op np random seed NP_RANDOM_SEED X scale zero_point axis torch_type = X quant_min = torch iinfo torch_type min quant_max = torch iinfo torch_type max X = to_tensor X device scale = to_tensor scale device zero_point = torch tensor zero_point dtype=torch int device=device Y = _fake_quantize_per_channel_affine_reference X cpu scale cpu zero_point cpu axis quant_min quant_max Y_prime = torch fake_quantize_per_channel_affine X scale zero_point axis quant_min quant_max np testing assert_allclose Y Y_prime cpu rtol=tolerance atol=tolerance _test_forward_per_channel_cachemask_impl device torch_types = torch qint torch quint float_types = torch float torch float torch float torch bfloat zero_point_types = torch int torch float torch float torch_type float_type zero_point_type itertools product torch_types float_types zero_point_types X = torch randn dtype=float_type device pick scale + zp so some values get clipped axis = obs = torch ao quantization PerChannelMinMaxObserver axis torch_type device obs X scale zero_point = obs calculate_qparams TODO future PR fix wrong dtype obs calculate_qparams remove cast zero_point = zero_point zero_point_type quant_min quant_max = obs quant_min obs quant_max Y = _fake_quantize_per_channel_affine_reference X cpu scale cpu zero_point cpu axis quant_min quant_max Y_prime = torch fake_quantize_per_channel_affine X scale zero_point axis quant_min quant_max torch testing assert_close Y Y_prime cpu rtol=tolerance atol=tolerance assertTrue Y dtype == float_type test_forward_per_channel_cachemask_cpu _test_forward_per_channel_cachemask_impl cpu unittest skipIf TEST_CUDA No gpu available test_forward_per_channel_cachemask_cuda _test_forward_per_channel_cachemask_impl cuda test_forward_per_channel_half_precision_numerics scale = torch randn abs zero = torch randn dtype=torch int axis = mini = maxi = _ range X = torch randn torch float Y = torch fake_quantize_per_channel_affine X scale zero axis mini maxi Y r = _fake_quantize_per_channel_affine_reference X scale zero axis mini maxi assertEqual Y Y r rtol=tolerance atol=tolerance force overflow X = torch randn torch float X = + Y = torch fake_quantize_per_channel_affine X scale zero axis mini maxi Y r = _fake_quantize_per_channel_affine_reference X scale zero axis mini maxi assertEqual Y Y r rtol=tolerance atol=tolerance scale = torch zeros + force underflow X = torch randn torch float X = - Y = torch fake_quantize_per_channel_affine X scale zero axis mini maxi Y r = _fake_quantize_per_channel_affine_reference X scale zero axis mini maxi assertEqual Y Y r rtol=tolerance atol=tolerance given X=hu per_channel_tensor shapes=hu array_shapes qparams=hu qparams dtypes=torch quint test_fake_quant_per_channel_qparam_range X X scale zero_point axis torch_type = X quant_min = torch iinfo torch_type min quant_max = torch iinfo torch_type max device cpu cuda torch cuda is_available cpu X = to_tensor X device scale = to_tensor scale device Ensure zero_point quant_min zero_point = torch full zero_point shape - - quant_min dtype=torch int device=device For non-float zero_point fakequant requires zero_point between quant_min quant_max assertRaisesRegex RuntimeError ` zero_point ` must between ` quant_min ` ` quant_max ` Y = torch fake_quantize_per_channel_affine X scale zero_point axis quant_min quant_max For float zero_point fakequant can outside quant_min quant_max zero_point_dtype torch float torch float zero_point = zero_point dtype=zero_point_dtype Y = torch fake_quantize_per_channel_affine X scale zero_point axis quant_min quant_max Y_ref = _fake_quantize_per_channel_affine_reference X cpu scale cpu zero_point cpu axis quant_min quant_max np testing assert_allclose Y cpu numpy Y_ref cpu numpy rtol=tolerance atol=tolerance _test_learnable_forward_per_channel X_base device scale_base zero_point_base axis r Tests forward path learnable FakeQuantizePerTensorAffine op n_bits quant_min quant_max = n_bits - scale_base = scale_base device zero_point_base = zero_point_base device X_curr = X_base clone scale_curr = scale_base clone zero_point_curr = zero_point_base clone Y = _fake_quantize_per_channel_affine_reference X_curr scale_curr zero_point_curr round clamp quant_min quant_max axis quant_min quant_max device grad_factor Y_prime = torch _fake_quantize_learnable_per_channel_affine X_curr scale_curr zero_point_curr axis quant_min quant_max grad_factor device assertTrue torch allclose Y Y_prime rtol=tolerance atol=tolerance Expected kernel forward function have results match reference forward function given X=hu per_channel_tensor shapes=hu array_shapes qparams=hu qparams dtypes=torch quint test_learnable_forward_per_channel_cpu X torch random manual_seed NP_RANDOM_SEED X _ _ axis _ = X X_base = torch tensor X cpu channel_size = X_base size axis scale_base = torch normal mean= std= size= channel_size clamp e- zero_point_base = torch normal mean= std= size= channel_size _test_learnable_forward_per_channel X_base cpu scale_base zero_point_base axis unittest skipIf TEST_CUDA No gpu available test_learnable_forward_per_channel_cuda torch random manual_seed NP_RANDOM_SEED shape = axis = dtype torch float torch bfloat X_base = torch randn shape device= cuda dtype channel_size = X_base size axis scale_base = torch normal mean= std= size= channel_size clamp e- dtype zero_point_base = torch normal mean= std= size= channel_size dtype _test_learnable_forward_per_channel X_base cuda scale_base zero_point_base axis given device=st sampled_from cpu cuda torch cuda is_available cpu X=hu per_channel_tensor shapes=hu array_shapes qparams=hu qparams dtypes=torch quint unittest skip broken without changes any relevant code we need remove hypothesis testing CI test_backward_per_channel device X r Tests backward method np random seed NP_RANDOM_SEED X scale zero_point axis torch_type = X quant_min = torch iinfo torch_type min quant_max = torch iinfo torch_type max zero_point_types = torch int torch float torch float zero_point_type zero_point_types X = to_tensor X device scale = to_tensor scale device zero_point = to_tensor zero_point device dtype=zero_point_type X requires_grad_ Y_prime = torch fake_quantize_per_channel_affine X scale zero_point axis quant_min quant_max dout = torch rand_like X dtype=torch float device dX = _fake_quantize_per_channel_affine_grad_reference dout X scale zero_point axis quant_min quant_max Y_prime backward dout np testing assert_allclose dX cpu detach numpy X grad cpu detach numpy rtol=tolerance atol=tolerance _test_backward_per_channel_cachemask_impl device torch_types = torch qint torch quint float_types = torch float torch float torch float zero_point_types = torch int torch float torch float torch_type float_type zero_point_type itertools product torch_types float_types zero_point_types X = torch randn dtype=float_type device pick scale + zp so some values get clipped axis = obs = torch ao quantization PerChannelMinMaxObserver axis torch_type device obs X scale zero_point = obs calculate_qparams TODO future PR fix wrong dtype obs calculate_qparams remove cast zero_point = zero_point zero_point_type quant_min quant_max = obs quant_min obs quant_max X requires_grad_ Y_prime = torch fake_quantize_per_channel_affine X scale zero_point axis quant_min quant_max dout = torch rand_like X dtype=float_type device dX = _fake_quantize_per_channel_affine_grad_reference dout X scale zero_point axis quant_min quant_max Y_prime backward dout np testing assert_allclose dX cpu detach numpy X grad cpu detach numpy rtol=tolerance atol=tolerance assert X grad dtype == float_type test_backward_per_channel_cachemask_cpu _test_backward_per_channel_cachemask_impl cpu unittest skipIf TEST_CUDA No gpu available test_backward_per_channel_cachemask_cuda _test_backward_per_channel_cachemask_impl cuda _test_learnable_backward_per_channel X_base device scale_base zero_point_base axis dtype=torch float r Tests backward path learnable FakeQuantizePerTensorAffine op n_bits quant_min quant_max = n_bits - scale_base = scale_base device zero_point_base = zero_point_base device=device X_curr = X_base clone X_curr requires_grad_ scale_curr = scale_base clone scale_curr requires_grad_ zero_point_curr = zero_point_base clone zero_point_curr requires_grad_ grad_factor Y_prime = torch _fake_quantize_learnable_per_channel_affine X_curr scale_curr zero_point_curr axis quant_min quant_max grad_factor device dout = torch rand X_curr shape dtype=torch float device dX dScale dZeroPoint = _fake_quantize_learnable_per_channel_affine_grad_reference dout X_curr scale_curr zero_point_curr axis quant_min quant_max device dtype Y_prime backward dout dX_expected = dX device detach dX_actual = X_curr device grad detach dScale_expected = dScale device detach dScale_actual = scale_curr device grad detach dZeroPoint_expected = dZeroPoint device detach dZeroPoint_actual = zero_point_curr device grad detach increasing tolerance bf due differences python s x torch bfloat cpp s x kBFloat example - gets downcast - after applying grad_factor python CPP - gets downcast - tolerance = e- dtype torch bfloat e- assertTrue torch allclose dX_expected dX_actual rtol=tolerance atol=tolerance f Expected dX= dX_expected match X grad= dX_actual X= X_curr s= scale_curr z= zero_point_curr dout= dout n_bits= n_bits noqa B assertTrue torch allclose dScale_expected grad_factor dScale_actual rtol=tolerance atol=tolerance f Expected dScale= dScale_expected grad_factor match scale grad= dScale_actual X= X_curr s= scale_curr z= zero_point_curr dout= dout n_bits= n_bits noqa B assertTrue torch allclose dZeroPoint_expected grad_factor dZeroPoint_actual rtol=tolerance atol=tolerance f Expected dZeroPoint= dZeroPoint_expected grad_factor match zero_point grad= dZeroPoint_actual X= X_curr s= scale_curr z= zero_point_curr dout= dout n_bits= n_bits noqa B X_curr grad data zero_ scale_curr grad data zero_ zero_point_curr grad data zero_ given X=hu per_channel_tensor shapes=hu array_shapes qparams=hu qparams dtypes=torch quint unittest skip broken without changes any relevant code we need remove hypothesis testing CI test_learnable_backward_per_channel_cpu X torch random manual_seed NP_RANDOM_SEED X _ _ axis _ = X X_base = torch tensor X cpu channel_size = X_base size axis scale_base = torch normal mean= std= size= channel_size clamp e- zero_point_base = torch normal mean= std= size= channel_size _test_learnable_backward_per_channel X_base cpu scale_base zero_point_base axis unittest skipIf TEST_CUDA No gpu available test_learnable_backward_per_channel_cuda torch random manual_seed NP_RANDOM_SEED x_shape = scale_shape = zero_point_shape = axis = dtype torch bfloat torch float X_base = torch randn x_shape dtype=dtype device= cuda scale_base = torch randn scale_shape dtype=dtype device= cuda zero_point_base = torch randint zero_point_shape device= cuda dtype=dtype _test_learnable_backward_per_channel X_base cuda scale_base zero_point_base axis dtype test_numerical_consistency_per_tensor _test_numerical_consistency per_tensor test_numerical_consistency_per_channel _test_numerical_consistency per_channel _test_numerical_consistency test_type r Comparing numerical consistency between quantize dequantize op fake quantize op across devices dtypes torch random manual_seed NP_RANDOM_SEED torch_types = torch qint torch quint float_types = torch float torch float torch float test_type == per_channel zero_types = torch int torch float torch float zero_types = torch int devices = torch device cpu torch device cuda torch cuda is_available torch device cpu axis = _ range torch_type float_type device zero_type itertools product torch_types float_types devices zero_types X = torch randn device=device float_type scales = torch randn device=device abs scale = scales mean float item zeros = torch randn device=device abs dtype=zero_type zero = zeros max view item quant_min = torch iinfo torch_type min quant_max = torch iinfo torch_type max test_was_run = False test_type == per_tensor test_was_run = True Y = torch dequantize torch quantize_per_tensor X cpu torch float scale zero torch_type device float_type Y_prime = torch fake_quantize_per_tensor_affine X scale zero quant_min quant_max assertEqual Y Y_prime Difference found between dequant+quant_per_tensor fake_quantize_per_tensor test_type == per_channel test_was_run = True Y = torch dequantize torch quantize_per_channel X cpu torch float scales cpu zeros cpu axis torch_type device float_type Y_prime = torch fake_quantize_per_channel_affine X scales zeros axis quant_min quant_max assertEqual Y Y_prime Difference found between dequant+quant_per_channel fake_quantize_per_channel assertTrue test_was_run skipIfTorchDynamo Not suitable test TorchDynamo test_fake_quantize_per_channel_affine_scale_dtypes Ensure error message more helpful dtype_list = torch float torch float torch bfloat torch half scale_dtype dtype_list input = torch randn scale = torch Tensor scale_dtype zero_point = torch tensor dtype=torch int axis = quant_min = quant_max = scale_dtype = torch float assertRaises RuntimeError torch fake_quantize_per_channel_affine input scale zero_point axis quant_min quant_max torch fake_quantize_per_channel_affine input scale zero_point axis quant_min quant_max skipIfTorchDynamo Not suitable test TorchDynamo unittest skipIf TEST_WITH_ROCM Not suitable test ROCM given dtype=st sampled_from torch float torch float torch half torch bfloat device=st sampled_from cpu cuda torch cuda is_available cpu test_fake_quantize_per_tensor_affine_inf dtype device - None https github com pytorch pytorch issues input_tensor = torch tensor torch inf dtype=dtype device scale = zero_point = quant_min = quant_max = result = torch fake_quantize_per_tensor_affine input_tensor scale zero_point quant_min quant_max ref_result = min quant_max max quant_min torch round input_tensor scale + zero_point - zero_point scale ref_result = torch Tensor ref_result dtype device assertEqual result ref_result TestFusedObsFakeQuant TestCase given device=st sampled_from cpu cuda torch cuda is_available cpu sampled_dtype=st sampled_from bf fp fp symmetric_quant=st booleans use_bool=st booleans settings deadline=None test_fused_obs_fake_quant_moving_avg device sampled_dtype symmetric_quant use_bool - None Tests case where we call fused_obs_fake_quant op multiple times update running_min max activation tensors device == cpu sampled_dtype = fp dtype = bf torch bfloat fp torch half fp torch float sampled_dtype in_running_min_ref = out_running_min_ref = torch tensor float inf dtype=dtype in_running_min_op = torch tensor float inf dtype=dtype device=device in_running_max_ref = out_running_max_ref = torch tensor float -inf dtype=dtype in_running_max_op = torch tensor float -inf dtype=dtype device=device avg_const = scale = torch tensor device=device zero_point = torch tensor dtype=torch int device=device observer_on = fake_quant_on = False use_bool pt_op = torch fused_moving_avg_obs_fake_quant enable observer after iterations fake_quant after iterations i range i observer_on = True use_bool i fake_quant_on = True use_bool x = torch randn dtype=dtype device=device out = pt_op x torch tensor observer_on device=device torch tensor fake_quant_on device=device in_running_min_op in_running_max_op scale zero_point avg_const False symmetric_quant observer_on in_running_min_ref in_running_max_ref = _get_tensor_min_max x running_min=in_running_min_ref running_max=in_running_max_ref averaging_const= dtype=dtype fake_quant_on x_scale x_zero_point = _get_scale_zp in_running_min_ref in_running_max_ref torch quint preserve_sparsity=symmetric_quant x_in = _fake_quantize_per_tensor_affine_reference x x_scale x_zero_point assertEqual scale x_scale assertEqual zero_point x_zero_point x_in = x assertEqual in_running_min_ref in_running_min_op assertEqual in_running_max_ref in_running_max_op torch testing assert_close out x_in Test empty input works x = torch empty dtype=dtype device=device out = pt_op x torch tensor device=device torch tensor device=device in_running_min_op in_running_max_op scale zero_point avg_const False symmetric_quant output_shape = assertEqual out shape output_shape given device=st sampled_from cpu cuda torch cuda is_available cpu symmetric_quant=st booleans use_bool=st booleans settings deadline=None test_fused_obs_fake_quant_moving_avg_per_channel device symmetric_quant use_bool - None Tests case where we call fused_obs_fake_quant op multiple times update running_min max activation tensors m = sizes = size sizes in_running_min_ref = torch empty m device=device fill_ float inf in_running_min_op = torch empty m device=device fill_ float inf in_running_max_ref = torch empty m device=device fill_ float -inf in_running_max_op = torch empty m device=device fill_ float -inf avg_const = scale = torch empty m device=device fill_ zero_point = torch empty m dtype=torch int device=device fill_ observer_on = fake_quant_on = False use_bool pt_op = torch fused_moving_avg_obs_fake_quant enable observer after iterations fake_quant after iterations i range i observer_on = True use_bool i fake_quant_on = True use_bool x = torch randn size device=device out = pt_op x torch tensor observer_on device=device torch tensor fake_quant_on device=device in_running_min_op in_running_max_op scale zero_point avg_const True per_channel_enabled symmetric_quant observer_on in_running_min_ref in_running_max_ref = _get_per_row_min_max x in_running_min_ref in_running_max_ref fake_quant_on x_scale = torch empty m device=device x_zero_point = torch empty m dtype=torch int device=device i range x_scale numel x_scale i x_zero_point i = _get_scale_zp in_running_min_ref i item in_running_max_ref i item torch quint preserve_sparsity=symmetric_quant x_in = _fake_quantize_per_channel_affine_reference x x_scale x_zero_point assertEqual scale x_scale assertEqual zero_point x_zero_point x_in = x assertEqual in_running_min_ref in_running_min_op assertEqual in_running_max_ref in_running_max_op torch testing assert_close out x_in given device=st sampled_from cpu cuda torch cuda is_available cpu settings deadline=None test_fused_obs_fake_quant_backward_op device - None n = m = k = input_shape = m n output_shape = m n x = torch randn input_shape device=device requires_grad=True avg_const = scale = torch tensor device=device zero_point = torch tensor dtype=torch int device=device x_min x_max = _get_tensor_min_max x x_scale x_zero_point = _get_scale_zp x_min x_max torch quint x_scale = torch tensor x_scale device=device x_zero_point = torch tensor x_zero_point dtype=torch int device=device x_fake_quant = torch fake_quantize_per_tensor_affine x x_scale x_zero_point pt_op = torch fused_moving_avg_obs_fake_quant out = pt_op x torch tensor device=device torch tensor device=device torch tensor x_min device=device torch tensor x_max device=device scale zero_point avg_const False verify output matches torch testing assert_close out x_fake_quant verify gradient matches expectation fake_quant op dout = torch rand_like x dtype=torch float device out backward dout dX = _fake_quantize_per_tensor_affine_grad_reference dout x x_scale x_zero_point assertEqual dX x grad assertTrue x grad dtype == torch float given device=st sampled_from cpu cuda torch cuda is_available cpu settings deadline=None test_fused_backward_op_fake_quant_off device - None n = m = input_shape = m n output_shape = m n x = torch randn input_shape device=device requires_grad=True avg_const = scale = torch tensor device=device zero_point = torch tensor dtype=torch int device=device x_min x_max = _get_tensor_min_max x x_scale x_zero_point = _get_scale_zp x_min x_max torch quint pt_op = torch fused_moving_avg_obs_fake_quant out = pt_op x torch tensor device=device torch tensor device=device torch tensor x_min device=device torch tensor x_max device=device scale zero_point avg_const False verify output matches torch testing assert_close out x verify gradient matches expectation fake_quant op dout = torch rand_like x dtype=torch float device out backward dout dX = _fake_quantize_per_tensor_affine_grad_reference dout x x_scale x_zero_point assertEqual dX x grad assertTrue x grad dtype == torch float __name__ == __main__ raise RuntimeError This test file meant run directly use \n\n \tpython test test_quantization py TESTNAME\n\n instead