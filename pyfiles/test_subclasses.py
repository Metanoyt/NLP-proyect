Owner s module dynamo functools itertools unittest functools partial torch torch _dynamo test_case torch _dynamo testing torch _functorch config torch utils _pytree pytree torch utils checkpoint torch _dynamo backends common aot_autograd torch _dynamo testing CompileCounterWithBackend normalize_gm torch _functorch _aot_autograd utils make_boxed_compiler torch _functorch compilers min_cut_rematerialization_partition torch _higher_order_ops wrap wrap torch fx experimental symbolic_shapes DimDynamic ShapeEnv StatelessSymbolicContext torch nested _internal nested_tensor jagged_from_list jagged_from_tensor_and_lengths nested_view_from_values_offsets torch testing _internal common_utils instantiate_parametrized_tests NestedTensorTestCase parametrize subtest torch testing _internal triton_utils requires_cuda_and_triton torch testing _internal two_tensor TwoTensor torch utils _python_dispatch return_and_correct_aliasing nontraceable_subclass c torch _dynamo config patch nontraceable_tensor_subclasses c _check_recompiles fn inputs inputs expected_recompiles actual_recompiles = _recompiles_for_inputs fn inputs inputs assertEqual actual_recompiles expected_recompiles get_jagged_tensor nested_size offsets requires_grad=True Makes jagged tensor N constituent tensors size specified S S S D D = nested_size out = s nested_size out append torch randn s D requires_grad=requires_grad dtype=torch float jagged_from_list out offsets get_view_test_cases Test all cases both NT base dense base Subclass - Subclass Dense - Subclass NB Don t close over loop variables they will get copied into closure NB These functions so we don t generate tensors during test collection time mk_basic base_is_nt There three cases consider here based logic meta_utils py basic case view leaf has same requires grad its basic case x _ = get_jagged_tensor None requires_grad=True x = x clone base_is_nt x assert x is_leaf x unsqueeze - mk_leaf base_is_nt requires_grad_ requires_grad_ x _ = get_jagged_tensor None requires_grad=requires_grad_ x = x clone base_is_nt x torch no_grad x_view = x unsqueeze - The issue doesn t quite work x_view requires_grad_ requires_grad_ x_view mk_obscure base_is_nt x _ = get_jagged_tensor None requires_grad=False x = x clone base_is_nt x intermediate leaf view torch no_grad x_view = x unsqueeze - x_view requires_grad_ True x_view_view = x_view unsqueeze - x_view_view base_is_nt False True prefix = f base_is_nt_ base_is_nt yield partial mk_basic base_is_nt f prefix _basic leaf view case view has leaf w requires_grad True requires_grad False base w requires_grad True requires_grad False requires_grad_ requires_grad_ itertools product True False repeat= yield partial mk_leaf base_is_nt requires_grad_ requires_grad_ f prefix _leaf_ requires_grad_ _ requires_grad_ obscure case view leaf implies requires_grad True base w requires_grad False yield partial mk_obscure base_is_nt f prefix _obscure Subclass - Dense yield lambda get_jagged_tensor None requires_grad=True clone subclass_dense Dense - Subclass - Dense - Subclass mk_dense_subclass_dense_subclass values = torch randn offsets = torch tensor nested_view_from_values_offsets nested_view_from_values_offsets values offsets values offsets yield mk_dense_subclass_dense_subclass dense_subclass_dense_subclass mk_subclass_dense_subclass_dense x = get_jagged_tensor None requires_grad=True clone offsets = x offsets detach clone nested_view_from_values_offsets x values offsets values yield mk_subclass_dense_subclass_dense subclass_dense_subclass_dense VIEW_TEST_CASES = k v v k get_view_test_cases compile_full_eager = torch compile backend= eager fullgraph=True BaseTorchFunction torch Tensor classmethod __torch_function__ cls func types args= kwargs=None super __torch_function__ func types args kwargs MockSubclass torch Tensor classmethod __torch_function__ cls func types args= kwargs=None super __torch_function__ func types args kwargs AttrSubclass torch Tensor x int = size int = classmethod __torch_function__ cls func types args= kwargs=None super __torch_function__ func types args kwargs DummyNDim torch Tensor classmethod __torch_function__ cls func types args= kwargs=None func == torch Tensor ndim __get__ super __torch_function__ func types args kwargs WrapperSubclass __init__ tensor tensor = tensor classmethod __torch_function__ cls func types args= kwargs=None kwargs None kwargs = args = pytree tree_map_only WrapperSubclass lambda x x tensor args kwargs = pytree tree_map_only WrapperSubclass lambda x x tensor kwargs func args kwargs SigmoidToExpSubclass torch Tensor classmethod __torch_function__ cls func types args= kwargs=None func == torch Tensor sigmoid super __torch_function__ torch Tensor exp types args kwargs super __torch_function__ func types args kwargs Wrapper subclass two inner tensors data scale data has same shape outer scale has single dim size ScaledTensor torch Tensor __new__ cls data torch Tensor scale torch Tensor constant int = torch Tensor _make_wrapper_subclass cls data size strides=data stride storage_offset=data storage_offset dtype=data dtype layout=data layout requires_grad=data requires_grad device=data device __init__ data torch Tensor scale torch Tensor constant int = _data = data _scale = scale _constant = constant __tensor_flatten__ ctx = _constant _constant _data _scale ctx staticmethod __tensor_unflatten__ inner_tensors metadata outer_size outer_stride assert len inner_tensors == ScaledTensor inner_tensors _data inner_tensors _scale constant=metadata _constant classmethod __torch_dispatch__ cls func types args kwargs=None scaled_tensor = args out = func scaled_tensor _data args kwargs ScaledTensor out scaled_tensor _scale constant=scaled_tensor _constant __repr__ f _data __repr__ \n _scale __repr__ OptionalScaledTensor torch Tensor __new__ cls data scale constant int = torch Tensor _make_wrapper_subclass cls data size strides=data stride storage_offset=data storage_offset dtype=data dtype layout=data layout requires_grad=data requires_grad device=data device __init__ data torch Tensor scale constant int = _data = data _scale = scale _constant = constant __tensor_flatten__ ctx = _constant _constant _scale None _data _scale ctx _data ctx staticmethod __tensor_unflatten__ inner_tensors metadata outer_size outer_stride OptionalScaledTensor inner_tensors _data inner_tensors get _scale None constant=metadata _constant classmethod __torch_dispatch__ cls func types args kwargs=None scaled_tensor = args out = func scaled_tensor _data args kwargs scaled_tensor _scale None out = out scaled_tensor _scale OptionalScaledTensor out scaled_tensor _scale constant=scaled_tensor _constant __repr__ f OptionalScaledTensor _data __repr__ \n _scale __repr__ CtxSubclassTensor torch Tensor Class used verify guarding subclass metadata staticmethod __new__ cls constant shape = shape kwargs = kwargs strides = stride kwargs storage_offset = storage_offset kwargs device = device kwargs layout = layout kwargs requires_grad = requires_grad kwargs dtype = dtype out = torch Tensor _make_wrapper_subclass cls shape kwargs out __init__ constant = constant = constant __repr__ a_repr = repr f CtxSubclassTensor a_repr __tensor_flatten__ constant staticmethod __tensor_unflatten__ inner_tensors meta sizes strides constant = meta = inner_tensors CtxSubclassTensor constant classmethod __torch_dispatch__ cls func types args kwargs kwargs None kwargs = biggest_constant = max x constant x pytree tree_flatten args isinstance x CtxSubclassTensor args_a = pytree tree_map lambda x x isinstance x CtxSubclassTensor x args kwargs_a = pytree tree_map lambda x x isinstance x CtxSubclassTensor x kwargs out_a = func args_a kwargs_a out = pytree tree_map lambda x CtxSubclassTensor x biggest_constant isinstance x torch Tensor x out_a func == torch ops aten mul Tensor out = out + out constant return_and_correct_aliasing func args kwargs out func sin EagerRecordGraphAndInputs __init__ - None graphs = example_inputs = __call__ gm torch fx GraphModule example_inputs graphs append gm example_inputs append example_inputs gm GLOBAL_TEST_SUBCLASSES = MockSubclass DummyNDim SigmoidToExpSubclass BaseTorchFunction Returns True function recompiles between inputs inputs specified dynamic setting _recompiles_for_inputs fn inputs inputs dynamic=True compile_count = counter gm example_inputs compile_count += gm compiled_f = torch compile fn fullgraph=True backend=counter dynamic=dynamic compiled_f inputs compiled_f inputs compile_count SubclassTests torch _dynamo test_case TestCase classmethod tearDownClass cls cls _exit_stack close _check_recompiles fn inputs inputs expected_recompiles _check_recompiles fn inputs inputs expected_recompiles test_no_call_to_new BadNewTorchFunction torch Tensor __new__ cls args kwargs raise RuntimeError Oops classmethod __torch_function__ cls func types args= kwargs=None kwargs None kwargs = super __torch_function__ func types args kwargs torch compile backend= eager fullgraph=True fn x torch add x input = torch ones as_subclass BadNewTorchFunction res = fn input assertIsInstance res BadNewTorchFunction test_no_torch_function_recompiles NJT __repr__ f NJT shape= shape __init__ values offsets _values = values _offsets = offsets sin torch sin classmethod __torch_function__ cls func types args= kwargs=None kwargs None kwargs = func == torch sin = args NJT func _values _offsets raise AssertionError should get here values = torch randn requires_grad=True values = torch randn requires_grad=True offsets = torch tensor njt = NJT values offsets njt = NJT values offsets torch compile backend= eager fullgraph=True f x torch sin x unittest mock patch torch _dynamo config error_on_recompile True f njt f njt test_base_torch_function_tracing fn x torch add x input = torch ones as_subclass BaseTorchFunction out = fn input out_opt = compile_full_eager fn input assertIsInstance out BaseTorchFunction assertEqual out out_opt test_torch_function_state_graph_break torch compile backend= eager fn x torch _C DisableTorchFunctionSubclass torch _dynamo graph_break torch _C _is_torch_function_enabled torch add x input = torch ones res _ = fn input assertFalse res test_disable_all_torch_function torch compile backend= eager fn x torch _C DisableTorchFunction torch _dynamo graph_break torch _C _is_torch_function_enabled torch _C _is_torch_function_all_disabled torch add x input = torch ones res res _ = fn input assertFalse res assertTrue res test_disable_all_torch_function_restore_values torch compile backend= eager fn x torch _C DisableTorchFunction x = torch _C _is_torch_function_all_disabled x torch _C _is_torch_function_all_disabled torch add x input = torch ones res res _ = fn input assertTrue res assertFalse res test_disable_all_torch_function_restore_values_graph_break torch compile backend= eager fn x torch _C DisableTorchFunction torch _dynamo graph_break x = torch _C _is_torch_function_all_disabled x torch _C _is_torch_function_all_disabled torch add x input = torch ones res res _ = fn input assertTrue res assertFalse res test_torch_function_state_nested torch compile backend= eager fn x torch _C DisableTorchFunctionSubclass torch _C DisableTorchFunctionSubclass x = x + Should reset outer state disabled after exiting ctx manager torch _C _is_torch_function_enabled torch add x input = torch ones res _ = fn input assertFalse res test_torch_function_state_tracing torch compile backend= eager fullgraph=True fn x torch _C DisableTorchFunctionSubclass torch add x input = torch ones fn input test_torch_function_state_guards cnt = torch _dynamo testing CompileCounter torch compile backend=cnt fullgraph=True fn x torch add x input = torch ones torch _C DisableTorchFunctionSubclass fn input fn input assertEqual cnt frame_count test_return_subclass torch compile backend= eager fullgraph=True fn x MockSubclass torch add x input = torch ones res = fn input assertIsInstance res MockSubclass test_return_as_subclass torch compile backend= eager fullgraph=True fn x torch add x as_subclass MockSubclass input = torch ones res = fn input assertIsInstance res MockSubclass test_return_local_subclass LocalSubclass torch Tensor classmethod __torch_function__ cls func types args= kwargs=None super __torch_function__ func types args kwargs torch compile backend= eager fullgraph=True fn x LocalSubclass torch add x input = torch ones res = fn input assertIsInstance res LocalSubclass test_torch_function_list_args HANDLED_FUNCTIONS = MyClass __init__ foo foo = foo classmethod __torch_function__ cls func types args= kwargs=None kwargs None kwargs = func HANDLED_FUNCTIONS all noqa C noqa C issubclass t torch Tensor MyClass t types NotImplemented HANDLED_FUNCTIONS func args kwargs _stack input dim= out=None MyClass sum x foo x input HANDLED_FUNCTIONS torch stack = _stack torch compile backend= eager fullgraph=True fn v v torch stack v v ret = fn MyClass MyClass assertEqual ret foo parametrize comparison subtest isinstance isinstance subtest lambda instance type_ type instance type_ equality subtest lambda instance type_ type instance type_ identity parametrize input_type subtest torch Tensor tensor subtest DummyNDim subclass test_type_check comparison input_type fn x comparison x DummyNDim torch ones torch zeros input = torch ones as_subclass input_type exp_res = fn input act_res = torch compile backend= eager fullgraph=True fn input assertEqual exp_res act_res test_torch_function_call_on_method x = torch ones y = torch ones z = torch ones wrapped = x as_subclass SigmoidToExpSubclass wrapped = y as_subclass SigmoidToExpSubclass fn w w exp fn_opt = compile_full_eager fn res_exp = fn wrapped res_act = fn_opt wrapped res_exp = z exp assertEqual res_exp res_act assertEqual res_exp res_exp test_torch_function_call_on_method_arg LocalSubclass torch Tensor classmethod __torch_function__ cls func types args= kwargs=None func == torch _C TensorBase add_ func = torch _C TensorBase sub_ kwargs None kwargs = super __torch_function__ func types args kwargs sigmoid None x = torch ones y = torch ones z = torch ones wrapped = y as_subclass LocalSubclass wrapped = z as_subclass LocalSubclass fn w add_ w fn_opt = torch compile fn res_exp = fn x wrapped res_act = fn_opt y wrapped assertEqual res_exp res_act test_user_overridden_method_unsupported LocalSubclass torch Tensor classmethod __torch_function__ cls func types args= kwargs=None super __torch_function__ func types args kwargs sigmoid None fn x x sigmoid x = torch ones as_subclass LocalSubclass fn_opt = compile_full_eager fn res_exp = fn x res_act = fn_opt x assertEqual res_exp res_act test_user_overridden_attr_unsupported LocalSubclass torch Tensor classmethod __torch_function__ cls func types args= kwargs=None kwargs None kwargs = super __torch_function__ func types args kwargs ndim = torch compile backend= eager fullgraph=True fn x x ndim msg = ` torch compile ` only support tracing certain types overridden tensor subclass attributes assertRaisesRegex torch _dynamo exc Unsupported msg x = torch ones as_subclass LocalSubclass fn x test_user_overridden_property_unsupported LocalSubclass torch Tensor __init__ args kwargs - None _ndim = classmethod __torch_function__ cls func types args= kwargs=None super __torch_function__ func types args kwargs property ndim _ndim ndim setter ndim value _ndim = value fn x x + x ndim x = LocalSubclass torch ones fn_opt = compile_full_eager fn res_exp = fn x res_act = fn_opt x assertEqual res_exp res_act test_overridden_method_guarding LocalSubclass torch Tensor classmethod __torch_function__ cls func types args= kwargs=None kwargs None kwargs = super __torch_function__ func types args kwargs torch compile backend= eager fn x x sigmoid torch _dynamo config patch error_on_recompile=True x = torch ones as_subclass LocalSubclass fn x fn x x = torch ones as_subclass LocalSubclass fn x assertRaisesRegex TypeError bool object callable LocalSubclass sigmoid = False fn x test_torch_function_call_on_attr x = torch ones wrapped = x as_subclass DummyNDim fn w w ndim + torch ones fn_opt = compile_full_eager fn res_exp = fn wrapped res_act = fn_opt wrapped assertEqual res_exp res_act assertEqual res_exp torch ones + test_torch_function_wrapper_class x = torch ones wrapped = WrapperSubclass x fn w torch add w fn_opt = compile_full_eager fn res_exp = fn wrapped res_act = fn_opt wrapped assertEqual res_exp res_act test_no_torch_function_on_size_bytecode TestTensor torch Tensor classmethod __torch_function__ cls func types args= kwargs=None kwargs None kwargs = torch _C DisableTorchFunctionSubclass out = func args kwargs func == torch clone out out fn x torch clone x inp = torch ones x = inp as_subclass TestTensor torch _dynamo mark_dynamic x compiled_fn = torch compile fn fullgraph=True out = compiled_fn x assertEqual out torch ones test_torch_function_wrapper_class_with_kwargs x = torch ones wrapped = WrapperSubclass x fn w torch add w alpha= fn_opt = compile_full_eager fn res_exp = fn wrapped res_act = fn_opt wrapped assertEqual res_exp res_act test_tensor_subclass_with_non_classmethod_torch_function MySubclass torch Tensor __torch_function__ func types args kwargs=None kwargs None kwargs = torch _C DisableTorchFunctionSubclass func args kwargs fn x x + fn_opt = compile_full_eager fn x = torch randn as_subclass MySubclass res_exp = fn x res_act = fn_opt x assertEqual res_exp res_act test_tensor_subclass_custom_attr AttrSubclass torch Tensor x int = classmethod __torch_function__ cls func types args= kwargs=None kwargs None kwargs = super __torch_function__ func types args kwargs torch compile backend= eager fullgraph=True fn x x x + torch ones input = torch ones as_subclass AttrSubclass fn_opt = compile_full_eager fn res_exp = fn input res_act = fn_opt input assertEqual res_exp res_act test_make_subclass Make sure ` torch Tensor _make_subclass ` traceable Dynamo models its aliasing relationships correctly MySubclass torch Tensor pass fn x Downcast then upcast y = torch Tensor _make_subclass MySubclass x z = torch Tensor _make_subclass torch Tensor x Now ` x y z ` should have same underlying data x += y += z += res = x y + z res x = torch randn x = x clone fn_opt = compile_full_eager fn res_exp = fn x res_act = fn_opt x assertEqual res_exp res_act assertEqual x x test_subclass_override_shape_and_to This slight variabtion https github com huggingface diffusers blob fbf b cc fd ad bff aafe f src diffusers quantizers gguf utils py#L -L MySubclass torch Tensor args kwargs new = super args kwargs new tensor_shape = getattr tensor_shape new data shape new property shape hasattr tensor_shape tensor_shape = size tensor_shape fn x x_shape = x shape y = x cpu x + y + x_shape x tensor_shape y tensor_shape x = torch nn Parameter torch randn as_subclass MySubclass x = torch nn Parameter x clone as_subclass MySubclass fn_opt = compile_full_eager fn res_exp = fn x res_act = fn_opt x assertEqual res_exp res_act assertEqual x x assertEqual x tensor_shape x tensor_shape test_subclass_dont_invoke_torch_function_on_overridden_method We shouldn t fire ` __torch_function__ ` overridden tensor methods MySubclass torch Tensor device len device classmethod __torch_function__ cls func types args= kwargs=None func torch Tensor torch _dynamo graph_break super __torch_function__ func types args kwargs fn x x cpu x = torch nn Parameter torch randn as_subclass MySubclass fn_opt = compile_full_eager fn res_exp = fn x res_act = fn_opt x assertEqual res_exp res_act test_subclass_dont_invoke_torch_function_on_overridden_attr types MethodWrapperType We shouldn t fire ` __torch_function__ ` overridden tensor attrs MySubclass torch Tensor ndim classmethod __torch_function__ cls func types args= kwargs=None type func MethodWrapperType func __name__ == ndim torch _dynamo graph_break super __torch_function__ func types args kwargs fn x x + x ndim x = torch nn Parameter torch randn as_subclass MySubclass fn_opt = compile_full_eager fn res_exp = fn x res_act = fn_opt x assertEqual res_exp res_act test_parameter_subclass_with_old_torch_function MySubclass torch nn Parameter pass fn x x = x t x = x T x + fn_opt = compile_full_eager fn x = torch randn as_subclass MySubclass res_exp = fn x res_act = fn_opt x assertEqual res_exp res_act test_subclass_with_disabled_torch_function MySubclass torch Tensor __torch_function__ = torch _C _disabled_torch_function_impl fn x x = x t x = x T x + fn_opt = compile_full_eager fn x = torch randn as_subclass MySubclass res_exp = fn x res_act = fn_opt x assertEqual res_exp res_act test_parameter_subclass_custom_torch_func_and_dynamic_attr This slight variation https github com huggingface diffusers blob fbf b cc fd ad bff aafe f src diffusers quantizers gguf utils py#L -L which basically uses tensor subclass attach quantization metadata onto tensors preserve them across torch ops use metadata dequantize tensor convert regular tensor The test meant make sure Dynamo won t graph break over GGUFParameter torch nn Parameter __new__ cls data requires_grad=False quant_type=None data = data data None torch empty = torch Tensor _make_subclass cls data requires_grad __init__ args quant_type=None kwargs quant_type = quant_type as_tensor torch Tensor _make_subclass torch Tensor requires_grad classmethod __torch_function__ cls func types args= kwargs=None kwargs None kwargs = result = super __torch_function__ func types args kwargs quant_type = None arg args isinstance arg list isinstance arg GGUFParameter quant_type = arg quant_type break isinstance arg GGUFParameter quant_type = arg quant_type break isinstance result torch Tensor cls result quant_type=quant_type Handle tuples lists isinstance result tuple list Preserve original type tuple list wrapped = cls x quant_type=quant_type isinstance x torch Tensor x x result type result wrapped result f x tmp = x tmp = tmp + tmp quant_type tmp = tmp as_tensor tmp opt_f = torch compile f backend= eager fullgraph=True x = GGUFParameter torch ones quant_type= res = f x ref = opt_f x assertEqual res ref test_newly_constructed_tensor_subclass_attr_mutation Make sure attribute mutation newly constructed tensor subclass object constructor call handled both during Dynamo tracing codegen-ed visible outside ` torch compile ` MySubclass torch Tensor pass f x = MySubclass torch ones x bar = x x x bar opt_f = compile_full_eager f res = f ref = opt_f assertEqual res ref assertEqual res bar ref bar test_as_subclass_attr_mutation Make sure attribute mutation newly constructed tensor subclass object as_subclass call handled both during Dynamo tracing codegen-ed visible outside ` torch compile ` MySubclass torch Tensor pass f x = torch ones as_subclass MySubclass x bar = x x x bar opt_f = compile_full_eager f res = f ref = opt_f assertEqual res ref assertEqual res bar ref bar test_tensor_subclass_attr_codegen_tos This repros very subtle interaction between ` TensorWithTFOverrideVariable ` attribute mutation codegen ` PyCodegen top_of_stack ` It uncovered ` test_tensor_subclass_deepcopy ` MySubclass torch Tensor __new__ cls elem args kwargs r = torch Tensor _make_subclass cls torch ones r elem = elem r f t MySubclass t elem clone opt_f = compile_full_eager f t = MySubclass torch ones res = f t ref = opt_f t assertEqual res ref assertEqual res elem ref elem assertEqual type res type ref test_nontraceable_tensor_subclass This will error Dynamo tries wrap tensor variable because involves calling certain methods inspect tensor property which will blow up overridden ` __torch_function__ ` MySubclass torch Tensor classmethod __torch_function__ cls func types args= kwargs=None raise RuntimeError one shall pass f t t foo + torch ones opt_f = torch compile f backend= eager fullgraph=False t = MySubclass torch ones t foo = Make sure ` nontraceable_tensor_subclasses ` config prevents Dynamo wrapping ` t ` nontraceable_subclass MySubclass res = f t ref = opt_f t assertEqual res ref test_compile_with_fake_tensor_dynamic_dim x = torch randn f x torch sin x test_dynamic_dim f x dim_dynamic exp_frame_count exp_op_count torch _dynamo reset cnt = torch _dynamo testing CompileCounter opt_f = torch compile f backend=cnt fullgraph=True x = torch rand_like x f x f torch randn shape_env = ShapeEnv torch _subclasses fake_tensor FakeTensorMode shape_env=shape_env fake_mode x_fake = fake_mode from_tensor x symbolic_context=StatelessSymbolicContext dynamic_sizes= dim_dynamic i range x dim x _fake = fake_mode from_tensor x symbolic_context=StatelessSymbolicContext dynamic_sizes= dim_dynamic i range x dim opt_f x_fake opt_f x _fake assertEqual cnt frame_count exp_frame_count assertEqual cnt op_count exp_op_count test_dynamic_dim f x DimDynamic DYNAMIC test_dynamic_dim f x DimDynamic DUCK test_dynamic_dim f x DimDynamic STATIC test_compile_with_fake_tensor_automatic_dynamic f x torch sin x test_automatic_dynamic f inps dim_dynamic exp_frame_count exp_op_count torch _dynamo reset cnt = torch _dynamo testing CompileCounter opt_f = torch compile f backend=cnt fullgraph=True shape_env = ShapeEnv torch _subclasses fake_tensor FakeTensorMode shape_env=shape_env fake_mode inp inps fake_inp = fake_mode from_tensor inp symbolic_context=StatelessSymbolicContext dim_dynamic i range x dim opt_f fake_inp assertEqual cnt frame_count exp_frame_count assertEqual cnt op_count exp_op_count x = torch randn y = torch randn z = torch randn = torch randn b = torch randn When inputs DimDynamic DYNAMIC DUCK inputs opt_f will tensors SymInt sizes Dynamo will treat input dynamic automatically will only compile once dim_dynamic DimDynamic DYNAMIC DimDynamic DUCK test_automatic_dynamic f x y z dim_dynamic test_automatic_dynamic f x z dim_dynamic test_automatic_dynamic f x b z dim_dynamic dim_dynamic DimDynamic STATIC Recompile once first dim become Dynamic test_automatic_dynamic f x y z dim_dynamic Recompile times first dim become Dynamic second dim becomes Dynamic test_automatic_dynamic f x z dim_dynamic Recompile times first dim become Dynamic second dim becomes Dynamic test_automatic_dynamic f x b z dim_dynamic test_compile_with_functionalization x = torch randn x_clone = x clone x_clone = x clone backend = EagerRecordGraphAndInputs cnt = torch _dynamo testing CompileCounterWithBackend backend torch compile backend=cnt fullgraph=True f x x add_ + torch nn functional relu_ x f_out = f x assertEqual cnt frame_count assertEqual cnt op_count assertEqual len backend graphs assertEqual len backend example_inputs actual = normalize_gm backend graphs print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ add_ f = l_x_ add_ relu_ f = torch relu_ l_x_ l_x_ = None add f = add_ + relu_ add_ = relu_ = None add ff = torch func functionalize f ff_out = ff x_clone assertEqual cnt frame_count assertEqual cnt op_count assertEqual len backend graphs assertEqual len backend example_inputs actual = normalize_gm backend graphs print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ add_ f = l_x_ add_ relu_ f = torch relu_ l_x_ l_x_ = None add f = add_ + relu_ add_ = relu_ = None add assertTrue torch _is_functional_tensor backend example_inputs Cannot reuse version AOTAutograd since uses python functional tensors to_fun x x_functional = torch _to_functional_tensor x torch _mirror_autograd_meta_to x x_functional x_functional aot_f_wrapper func functools wraps func wrapper args kwargs torch _enable_functionalization reapply_views=False try func_args = pytree tree_map to_fun args func_kwargs = pytree tree_map to_fun kwargs func func_args func_kwargs finally torch _disable_functionalization wrapper aot_ff = aot_f_wrapper f aot_ff_out = aot_ff x_clone assertEqual cnt frame_count assertEqual cnt op_count assertEqual len backend graphs assertEqual len backend example_inputs actual = normalize_gm backend graphs print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ add_ f = l_x_ add_ relu_ f = torch relu_ l_x_ l_x_ = None add f = add_ + relu_ add_ = relu_ = None add assertTrue torch _is_functional_tensor backend example_inputs assertEqual f_out ff_out assertEqual f_out aot_ff_out try torch _enable_functionalization reapply_views=False xf = pytree tree_map to_fun x x_view = xf t assertRaisesRegex RuntimeError Cannot safely fakify view f x_view finally torch _disable_functionalization test_compile_higher_order_with_functionalization backend = EagerRecordGraphAndInputs cnt = torch _dynamo testing CompileCounterWithBackend backend torch compile backend=cnt fullgraph=True f x wrap lambda x x add_ x check_count_and_graph exp_frame_count exp_op_count exp_n_graph exp_graph assertEqual cnt frame_count exp_frame_count assertEqual cnt op_count exp_op_count assertEqual len backend graphs exp_n_graph actual = normalize_gm backend graphs exp_n_graph - print_readable print_output=False assertExpectedInline actual exp_graph skip= t = torch randn t_clone = t clone t_clone = t clone f t check_count_and_graph \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ l_x_ wrap_body_ = l_x_ = None getitem f = wrap wrap = None getitem wrap_body_ torch nn Module forward l_x_ f add_ f = l_x_ add_ l_x_ = None add_ ff = torch func functionalize f ff_out = ff t_clone noqa F frame count op count incremented due re-compilation check_count_and_graph \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ l_x_ wrap_body_ = l_x_ = None getitem f = wrap wrap = None getitem wrap_body_ torch nn Module forward l_x_ f add_ f = l_x_ add_ l_x_ = None add_ try x = torch _to_functional_tensor t_clone torch _mirror_autograd_meta_to t_clone x torch _enable_functionalization reapply_views=False aot_f_out = f x noqa F finally torch _disable_functionalization frame count op count incremented due re-compilation check_count_and_graph \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ l_x_ wrap_body_ = l_x_ = None getitem f = wrap wrap = None getitem wrap_body_ torch nn Module forward l_x_ f add_ f = l_x_ add_ l_x_ = None add_ test_has_torch_function MyTensor classmethod __torch_function__ cls func types args= kwargs=None kwargs None kwargs = func torch max torch tensor func args kwargs LocalSubclass torch Tensor classmethod __torch_function__ cls func types args= kwargs=None kwargs None kwargs = func args kwargs fn x torch overrides has_torch_function_unary x torch overrides has_torch_function_variadic x test_class MyTensor LocalSubclass x = test_class ref = fn x ref = fn opt_fn = torch compile fn backend= eager res = opt_fn x res = opt_fn assertEqual ref res assertEqual ref res test_wrapper_subclass_guards_on_inner_tensor Holds inner tensor has distinct shape outer wrapper tensor Also adds additional guards inner tensor s sizes When first input op has x shape we insert extra add node DoubleSizeMaybeAddGeThreeTensor torch Tensor staticmethod __new__ cls inner Double outer-most dimension outer_shape = inner shape + inner shape torch Tensor _make_wrapper_subclass TODO right now _make_wrapper_subclass s dynamic shape interaction great Calling overload has kwargs causes us go down first overload path which will always specialize sizes We should probably eventually fix so first overload can just handle dynamic shapes cls outer_shape inner stride None None inner dtype inner layout inner device False inner requires_grad __init__ inner inner_elem = inner __tensor_flatten__ inner_elem None staticmethod __tensor_unflatten__ inner_tensors _ outer_size outer_stride DoubleSizeMaybeAddGeThreeTensor inner_tensors inner_elem __repr__ f DoubleSizeMayberAddGeThreeTensor repr inner_elem classmethod __torch_dispatch__ cls func types args= kwargs=None kwargs None kwargs = args_inner = torch utils _pytree tree_map_only DoubleSizeMaybeAddGeThreeTensor lambda x x inner_elem args out_inner = func args_inner kwargs Add guards inner tensor s sizes args_inner shape out_inner += DoubleSizeMaybeAddGeThreeTensor out_inner curr_var_to_val = None curr_var_to_sources = None guards = None backend gm args context = torch _guards TracingContext get Grab info sources guards shapeenv nonlocal curr_var_to_val nonlocal curr_var_to_sources nonlocal guards guards = str g expr g context fake_mode shape_env guards curr_var_to_val = str k v k v context fake_mode shape_env var_to_val items curr_var_to_sources = str k v name k v context fake_mode shape_env var_to_sources items gm torch compile backend=backend fn x x shape torch mul x x torch div x x inp = torch ones x = DoubleSizeMaybeAddGeThreeTensor inp torch _dynamo mark_dynamic x res = fn x noqa F During fakeifying we end up allocating separate symint outer inner tensor test s unused expected_var_to_val = s s expected_var_to_sources = s L x inner_elem size s L x size assertEqual curr_var_to_val expected_var_to_val assertEqual curr_var_to_sources expected_var_to_sources assertExpectedInline \n join guards \ Eq s s s s test_wrapper_subclass_with_same_sized_inner_tensor shouldn t recompile different sizes when dynamic=True sub = ScaledTensor torch randn torch randn sub = ScaledTensor torch randn torch randn assertFalse _recompiles_for_inputs func sub sub dynamic=True should recompile different data size when dynamic=False sub = ScaledTensor torch randn torch randn sub = ScaledTensor torch randn torch randn assertTrue _recompiles_for_inputs func sub sub dynamic=False avoid recompile using manual mark_dynamic different data size sub = ScaledTensor torch randn torch randn NB mark_dynamic outer tensor should translate inner tensors same size torch _dynamo mark_dynamic sub torch _dynamo mark_dynamic sub sub = ScaledTensor torch randn torch randn assertFalse _recompiles_for_inputs func sub sub dynamic=False test_wrapper_subclass_with_differently_sized_inner_tensor should recompile different scale size when dynamic=False sub = ScaledTensor torch randn torch randn sub = ScaledTensor torch randn torch randn assertTrue _recompiles_for_inputs func sub sub dynamic=False still recompiles using manual mark_dynamic outer different scale size sub = ScaledTensor torch randn torch randn NB mark_dynamic outer tensor doesn t translate inner tensors different size torch _dynamo mark_dynamic sub torch _dynamo mark_dynamic sub sub = ScaledTensor torch randn torch randn assertTrue _recompiles_for_inputs func sub sub dynamic=False test_recompiles_with_optional_inner_tensor f x x + sub does have optional tensor specified while sub does sub = OptionalScaledTensor torch randn None sub = OptionalScaledTensor torch randn torch randn sanity check don t recompile same input assertFalse _recompiles_for_inputs f sub sub dynamic=True assertFalse _recompiles_for_inputs f sub sub dynamic=True these should recompile optional tensor changes between specified unspecified assertTrue _recompiles_for_inputs f sub sub dynamic=True assertTrue _recompiles_for_inputs f sub sub dynamic=True f_compiled = torch compile f backend= aot_eager assertEqual f sub _data f_compiled sub _data assertEqual f sub _data f_compiled sub _data test_torch_dispatch_subclass_guard_recompile x = torch ones x_two = TwoTensor x clone x clone fn w torch add w fn_opt = torch compile backend= eager fn ref = fn x_two res = fn_opt x_two assertEqual ref res ensure no recompilation same input type unittest mock patch torch _dynamo config error_on_recompile True fn_opt TwoTensor x + x + recompile ref = fn x res = fn_opt x assertEqual ref res test_tensor_subclass_ctx_guards x = CtxSubclassTensor torch ones x = CtxSubclassTensor torch ones x = CtxSubclassTensor torch ones _check_recompiles lambda x x x x x False _check_recompiles lambda x x x x x True test_tensor_subclass_ctx_recursive_guards x = torch ones x = CtxSubclassTensor x clone x = CtxSubclassTensor x clone tt = TwoTensor x clone x tt = TwoTensor x clone x _check_recompiles lambda x x x tt tt True test_tensor_subclass_ctx_custom_guards_override CtxSubclassTensorCustomGuardFn CtxSubclassTensor classmethod __metadata_guard__ cls orig_data other orig_data = other x = CtxSubclassTensorCustomGuardFn torch ones x = CtxSubclassTensorCustomGuardFn torch ones x = CtxSubclassTensorCustomGuardFn torch ones _check_recompiles lambda x x x x x False _check_recompiles lambda x x x x x True test_tensor_subclass_ctx_custom_guards_error_arg_num torch _dynamo exc CtxSubclassTensorCustomGuardFn CtxSubclassTensor classmethod __metadata_guard__ cls y Shouldn t reach here False x = CtxSubclassTensorCustomGuardFn torch ones assertRaisesRegex torch _dynamo exc InternalTorchDynamoError Tensor subclass method __metadata_guard__ must take exactly two subclass metadata arguments lambda torch compile lambda x x x x test_tensor_subclass_ctx_custom_guards_error_not_classmethod torch _dynamo exc CtxSubclassTensorCustomGuardFn CtxSubclassTensor __metadata_guard__ x y False x = CtxSubclassTensorCustomGuardFn torch ones assertRaisesRegex torch _dynamo exc InternalTorchDynamoError Tensor subclass method __metadata_guard__ must classmethod lambda torch compile lambda x x x x test_subclass_constructor_proxying dataclasses collections namedtuple typing Any dataclasses dataclass frozen=True SubclassTensorArgs original_shape torch Size device torch device inner_meta Any SubclassTensorArgs = namedtuple SubclassTensorArgs original_shape device inner_meta SubclassTensor torch Tensor staticmethod __new__ cls meta shape = shape kwargs = kwargs strides = stride kwargs storage_offset = storage_offset kwargs device = device kwargs layout = layout kwargs requires_grad = requires_grad kwargs dtype = dtype out = torch Tensor _make_wrapper_subclass cls shape kwargs out __init__ meta = meta = meta __repr__ a_repr = repr f SubclassTensor a_repr __tensor_flatten__ meta staticmethod __tensor_unflatten__ inner_tensors meta _ __ = inner_tensors SubclassTensor meta classmethod __torch_dispatch__ cls func types args kwargs kwargs None kwargs = args_a = pytree tree_map lambda x x isinstance x SubclassTensor x args kwargs_a = pytree tree_map lambda x x isinstance x SubclassTensor x kwargs out_a = func args_a kwargs_a out = pytree tree_map lambda x SubclassTensor x SubclassTensorArgs x shape x device None isinstance x torch Tensor x out_a return_and_correct_aliasing func args kwargs out torch compile fullgraph=True f x meta = SubclassTensorArgs x shape x device SubclassTensorArgs x shape x device None out = SubclassTensor x meta out out x = torch randn f x torch compile fullgraph=True f x meta = SubclassTensorArgs x shape x device SubclassTensorArgs x shape x device None out = SubclassTensor x meta out out x = torch randn f x test_torch_function_subclass_survives_into_aot_autograd If you have tensor subclass relies dispatch into same op without unwrapping calling torch _C DisableTorchFunctionSubclass torch function-ness will survive into AOTAutograd Today NestedTensor actually relies behavior Because torch function logic runs during AOTAutograd test tests there no logic below relies torch function gets unexpectedly disabled after we redispatch subclass s torch function SubTensor torch Tensor staticmethod __new__ cls t torch Tensor _make_wrapper_subclass cls t shape t stride t storage_offset torch contiguous_format t dtype torch strided t device False t requires_grad sizes False False None __init__ t super __init__ _t = t __tensor_flatten__ _t staticmethod __tensor_unflatten__ inner_tensors ctx outer_size outer_stride t = inner_tensors _t SubTensor t __repr__ f SubTensor _t classmethod __torch_function__ cls func types args= kwargs=None kwargs None kwargs = torch _C DisableTorchFunctionSubclass func args kwargs classmethod __torch_dispatch__ cls func types args= kwargs=None kwargs = kwargs None kwargs new_args = pytree tree_map_only SubTensor lambda s s _t args output = func new_args kwargs output = pytree tree_map_only torch Tensor lambda t SubTensor t output output torch compile dynamic=True f x x unflatten - s = SubTensor torch randn f s Guard validation upsets guard https github com pytorch pytorch issues unittest expectedFailure test_recompile_with_symbool_inputs f pred bool pred torch ones torch ones test_recompilation f x sizes exp_graphs exp_frame_count exp_shape_env_guards torch _dynamo reset shape_env = ShapeEnv backend = torch _dynamo testing EagerAndRecordGraphs cnt = torch _dynamo testing CompileCounterWithBackend backend f_cond = torch compile f backend=cnt fullgraph=True torch _subclasses fake_tensor FakeTensorMode shape_env=shape_env fake_mode fake_inp = fake_mode from_tensor x symbolic_context=StatelessSymbolicContext dynamic_sizes= DimDynamic DYNAMIC i range x dim i size enumerate sizes pred = fake_inp size == size f_cond pred actual = normalize_gm backend graphs exp_frame_count i - print_readable print_output=False actual_guard_str = str guard expr guard shape_env guards assertExpectedInline actual exp_graphs i assertEqual cnt frame_count exp_frame_count i assertEqual actual_guard_str exp_shape_env_guards i true_graph = \ GraphModule torch nn Module forward ones f = torch ones ones false_graph = \ GraphModule torch nn Module forward ones f = torch ones ones test_recompilation f torch randn exp_graphs= true_graph true_graph false_graph false_graph exp_frame_count= exp_shape_env_guards= s specialized guarded outer shape_env when dynamo checks guards Eq Piecewise Eq s True Eq Piecewise Eq s True Ne Piecewise Eq s True Eq Piecewise Eq s True Ne Piecewise Eq s True Ne Piecewise Eq s True test_recompilation f torch randn exp_graphs= false_graph false_graph true_graph true_graph exp_frame_count= exp_shape_env_guards= s specialized guarded outer shape_env when dynamo checks guards Ne Piecewise Eq s True Ne Piecewise Eq s True Eq Piecewise Eq s True Ne Piecewise Eq s True Eq Piecewise Eq s True Eq Piecewise Eq s True test_wrapper_subclass_dynamo_attribute_access_on_intermediate f x_subclass tmp_subclass = torch add x torch mul tmp_subclass _scale tmp_subclass _constant x = ScaledTensor torch randn torch randn constant= out_ref = f x out_test = torch compile f backend= aot_eager fullgraph=True x assertEqual out_ref out_test test_support_bases abc torch fx _symbolic_trace Meta abc ABCMeta torch fx _symbolic_trace ProxyableClassMeta __new__ cls name bases dct x = super __new__ cls name bases dct x attr = x Multistreamable abc ABC noqa B pass Foo Multistreamable metaclass=Meta pass torch compile backend= eager fullgraph=True f x typ = type Foo typ __bases__ typ __bases__ assertEqual f torch randn Multistreamable torch compile backend= eager fullgraph=True g x typ = type Foo typ __base__ typ __base__ assertEqual g torch randn Multistreamable parametrize dynamic False True test_subclass_views dynamic _get_views t returns view Tensor expects_raises_false Note any closed-over SymInts will symbolicized during fake-ification yield t narrow dim=- start= length= False yield t split - False yield t split_with_sizes - False yield t unsqueeze - expand False yield t select - False https github com pytorch pytorch issues yield t dynamic yield t view - False f x x compiled_f = torch compile f backend= aot_eager fullgraph=True dynamic=dynamic Take view subclass pass input t = TwoTensor torch randn torch randn view expects_raises _get_views t torch _dynamo reset out_ref = f view expects_raises assertRaises AssertionError out_test = compiled_f view out_test = compiled_f view assertEqual out_ref out_test torch _dynamo config patch inline_inbuilt_nn_modules True parametrize dynamic True False test_mark_static_with_subclass_desugaring dynamic collections abc Callable typing Any Optional torch _dynamo decorators mark_static_address torch _inductor compile_fx compile_fx torch _inductor cudagraph_utils BoxedDeviceIndex torch _inductor utils BoxedBool x_inner = torch ones x = TwoTensor x_inner x_inner mark_static_address x guard=False inner_compile gm torch fx GraphModule example_inputs list torch Tensor cudagraphs Optional BoxedBool = None static_input_idxs Optional list int = None is_backward bool = False graph_id Optional int = None cpp_wrapper bool = False aot_mode bool = False is_inference bool = False boxed_forward_device_index Optional BoxedDeviceIndex = None layout_opt Optional bool = None extern_node_serializer Optional Callable list Any Any = None dynamic assertEqual static_input_idxs assertEqual static_input_idxs gm compiler = functools partial compile_fx inner_compile=inner_compile torch compile backend=compiler dynamic=dynamic fn t t t t + t + t + fn torch ones x torch ones copied common_utils py NestedTensorTestCase assertEqualIgnoringNestedInts b unbinding NJTs allows us compare them essentially equal without caring about exact nested int comparison _unbind_njts x isinstance x torch Tensor x is_nested x layout == torch jagged x unbind x assertEqual pytree tree_map _unbind_njts pytree tree_map _unbind_njts b _compile_check fn inps dynamic=True fullgraph=True call_backward=False call_backward_fn t t is_nested torch nested _internal nested_tensor buffer_from_jagged t = buffer_from_jagged t t sum backward retain_graph=True torch manual_seed fw_compiler = EagerRecordGraphAndInputs bw_compiler = EagerRecordGraphAndInputs compiler_fn = aot_autograd fw_compiler=make_boxed_compiler fw_compiler bw_compiler=make_boxed_compiler bw_compiler partition_fn=min_cut_rematerialization_partition keep_inference_input_mutations=True c = torch compile backend=compiler_fn dynamic=dynamic fullgraph=fullgraph fn inp inps expected = fn inp reset seed randn generate same tensor torch manual_seed got = c inp assertEqualIgnoringNestedInts expected got call_backward re = pytree tree_map_only lambda x isinstance x torch Tensor x requires_grad call_backward_fn expected rg = pytree tree_map_only lambda x isinstance x torch Tensor x requires_grad call_backward_fn got assertEqualIgnoringNestedInts re rg call_backward fw_compiler graphs bw_compiler graphs fw_compiler graphs None test_tensor_subclass_TwoTensor_simple f tt tt tt size = torch ones requires_grad=True b = detach clone requires_grad_ True tt = TwoTensor b fw bw = _compile_check f tt dynamic=True call_backward=True assertExpectedInline normalize_gm fw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ Sym s PlainAOTInput idx= primals_ Sym s PlainAOTInput idx= primals_ f s s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= primals_ f s s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= b primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassStrideAOTInput base=PlainAOTInput idx= idx= mul f s s = torch ops aten mul Tensor primals_ primals_ primals_ = None mul_ f s s = torch ops aten mul Tensor primals_ primals_ primals_ = None mul SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= mul_ SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= b primals_ SubclassSizeAOTOutput base=PlainAOTOutput idx= idx= primals_ SubclassSizeAOTOutput base=PlainAOTOutput idx= idx= primals_ SubclassStrideAOTOutput base=PlainAOTOutput idx= idx= primals_ SavedForBackwardsAOTOutput idx= primals_ SavedForBackwardsAOTOutput idx= primals_ SavedForBackwardsAOTOutput idx= noqa B assertExpectedInline normalize_gm bw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ Sym s PlainAOTInput idx= primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassStrideAOTInput base=PlainAOTInput idx= idx= tangents_ f s s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= tangents_ f s s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= b mul_ f s s = torch ops aten mul Tensor tangents_ primals_ tangents_ = None mul_ f s s = torch ops aten mul Tensor tangents_ primals_ tangents_ = primals_ = None None None None None mul_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= mul_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= b primals_ SubclassSizeAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= primals_ SubclassSizeAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= primals_ SubclassStrideAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= noqa B test_tensor_subclass_TwoTensor_clone_view f tt y = tt clone y view y shape y shape = torch ones requires_grad=True b = clone tt = TwoTensor b fw bw = _compile_check f tt dynamic=True call_backward=True assertExpectedInline normalize_gm fw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ Sym s PlainAOTInput idx= primals_ Sym s PlainAOTInput idx= primals_ f s s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= primals_ f s s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= b primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassStrideAOTInput base=PlainAOTInput idx= idx= clone f s s = torch ops aten clone default primals_ primals_ = None clone_ f s s = torch ops aten clone default primals_ primals_ = None view f s s = torch ops aten view default clone primals_ primals_ clone = None view_ f s s = torch ops aten view default clone_ primals_ primals_ clone_ = primals_ = None view SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= view_ SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= b primals_ SubclassSizeAOTOutput base=PlainAOTOutput idx= idx= primals_ SubclassSizeAOTOutput base=PlainAOTOutput idx= idx= primals_ SubclassStrideAOTOutput base=PlainAOTOutput idx= idx= primals_ SavedForBackwardsAOTOutput idx= primals_ SavedForBackwardsAOTOutput idx= noqa B assertExpectedInline normalize_gm bw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassStrideAOTInput base=PlainAOTInput idx= idx= tangents_ f s s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= tangents_ f s s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= b view_ f s s = torch ops aten view default tangents_ primals_ primals_ tangents_ = None view_ f s s = torch ops aten view default tangents_ primals_ primals_ tangents_ = None None None None None view_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= view_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= b primals_ SubclassSizeAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= primals_ SubclassSizeAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= primals_ SubclassStrideAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= noqa B test_tensor_subclass_TwoTensor_mul f tt b s s = size s s = b size tt size tt s s s s = torch ones requires_grad=True b = clone tt = TwoTensor b fw bw = _compile_check f tt b dynamic=True call_backward=True assertExpectedInline normalize_gm fw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ Sym s PlainAOTInput idx= primals_ Sym s PlainAOTInput idx= primals_ f s s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= primals_ f s s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= b primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassStrideAOTInput base=PlainAOTInput idx= idx= mul f s s = torch ops aten mul Tensor primals_ primals_ primals_ = None mul_ f s s = torch ops aten mul Tensor primals_ primals_ primals_ = None mul_ f s s = torch ops aten mul Tensor mul primals_ mul = None mul_ f s s = torch ops aten mul Tensor mul_ primals_ mul_ = None mul_ f s s = torch ops aten mul Tensor mul_ primals_ mul_ = None mul_ f s s = torch ops aten mul Tensor mul_ primals_ mul_ = None mul_ f s s = torch ops aten mul Tensor mul_ primals_ mul_ = None mul_ f s s = torch ops aten mul Tensor mul_ primals_ mul_ = None mul_ SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= mul_ SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= b primals_ SubclassSizeAOTOutput base=PlainAOTOutput idx= idx= primals_ SubclassSizeAOTOutput base=PlainAOTOutput idx= idx= primals_ SubclassStrideAOTOutput base=PlainAOTOutput idx= idx= primals_ SavedForBackwardsAOTOutput idx= primals_ SavedForBackwardsAOTOutput idx= primals_ SavedForBackwardsAOTOutput idx= primals_ SavedForBackwardsAOTOutput idx= noqa B assertExpectedInline normalize_gm bw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ Sym s PlainAOTInput idx= primals_ Sym s PlainAOTInput idx= primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassStrideAOTInput base=PlainAOTInput idx= idx= tangents_ f s s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= tangents_ f s s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= b mul_ f s s = torch ops aten mul Tensor tangents_ primals_ tangents_ = None mul_ f s s = torch ops aten mul Tensor tangents_ primals_ tangents_ = None mul_ f s s = torch ops aten mul Tensor mul_ primals_ mul_ = None mul_ f s s = torch ops aten mul Tensor mul_ primals_ mul_ = None mul_ f s s = torch ops aten mul Tensor mul_ primals_ mul_ = None mul_ f s s = torch ops aten mul Tensor mul_ primals_ mul_ = primals_ = None mul_ f s s = torch ops aten mul Tensor mul_ primals_ mul_ = None mul_ f s s = torch ops aten mul Tensor mul_ primals_ mul_ = primals_ = None None None None None mul_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= mul_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= b primals_ SubclassSizeAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= primals_ SubclassSizeAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= primals_ SubclassStrideAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= noqa B test_tensor_subclass_TwoTensor_view f tt y = tt clone y view y shape y shape = torch ones requires_grad=True b = clone tt = TwoTensor b fw bw = _compile_check f tt dynamic=True call_backward=True assertExpectedInline normalize_gm fw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ Sym s PlainAOTInput idx= primals_ Sym s PlainAOTInput idx= primals_ f s s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= primals_ f s s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= b primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassStrideAOTInput base=PlainAOTInput idx= idx= clone f s s = torch ops aten clone default primals_ primals_ = None clone_ f s s = torch ops aten clone default primals_ primals_ = None view f s s = torch ops aten view default clone primals_ primals_ clone = None view_ f s s = torch ops aten view default clone_ primals_ primals_ clone_ = primals_ = primals_ = None view SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= view_ SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= b primals_ SubclassSizeAOTOutput base=PlainAOTOutput idx= idx= primals_ SubclassSizeAOTOutput base=PlainAOTOutput idx= idx= primals_ SubclassStrideAOTOutput base=PlainAOTOutput idx= idx= primals_ SavedForBackwardsAOTOutput idx= primals_ SavedForBackwardsAOTOutput idx= noqa B assertExpectedInline normalize_gm bw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassStrideAOTInput base=PlainAOTInput idx= idx= tangents_ f s s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= tangents_ f s s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= b view_ f s s = torch ops aten view default tangents_ primals_ primals_ tangents_ = None view_ f s s = torch ops aten view default tangents_ primals_ primals_ tangents_ = None None None None None view_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= view_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= b primals_ SubclassSizeAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= primals_ SubclassSizeAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= primals_ SubclassStrideAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= noqa B test_tensor_subclass_TwoTensor_view_mul f tt y = tt clone y view y shape y shape = torch ones requires_grad=True b = clone tt = TwoTensor b fw bw = _compile_check f tt dynamic=True call_backward=True assertExpectedInline normalize_gm fw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ Sym s PlainAOTInput idx= primals_ Sym s PlainAOTInput idx= primals_ f s s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= primals_ f s s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= b primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassStrideAOTInput base=PlainAOTInput idx= idx= clone f s s = torch ops aten clone default primals_ primals_ = None clone_ f s s = torch ops aten clone default primals_ primals_ = None mul_ Sym s s = primals_ primals_ primals_ = primals_ = None view f s s = torch ops aten view default clone mul_ clone = None view_ f s s = torch ops aten view default clone_ mul_ clone_ = None view SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= view_ SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= b mul_ SubclassSizeAOTOutput base=PlainAOTOutput idx= idx= primals_ SavedForBackwardsAOTOutput idx= primals_ SavedForBackwardsAOTOutput idx= noqa B assertExpectedInline normalize_gm bw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassStrideAOTInput base=PlainAOTInput idx= idx= tangents_ f s s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= tangents_ f s s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= b view_ f s s = torch ops aten view default tangents_ primals_ primals_ tangents_ = None view_ f s s = torch ops aten view default tangents_ primals_ primals_ tangents_ = None None None None None view_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= view_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= b primals_ SubclassSizeAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= primals_ SubclassSizeAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= primals_ SubclassStrideAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= noqa B test_tensor_subclass_TwoTensor_return_tensor_and_subclass f tt y = tt clone y y view y shape y shape = torch ones requires_grad=True b = clone tt = TwoTensor b fw bw = _compile_check f tt dynamic=True call_backward=True assertExpectedInline normalize_gm fw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ Sym s PlainAOTInput idx= primals_ Sym s PlainAOTInput idx= primals_ f s s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= primals_ f s s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= b primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassStrideAOTInput base=PlainAOTInput idx= idx= clone f s s = torch ops aten clone default primals_ primals_ = None clone_ f s s = torch ops aten clone default primals_ primals_ = None mul_ Sym s s = primals_ primals_ primals_ = primals_ = None view f s s = torch ops aten view default clone mul_ view_ f s s = torch ops aten view default clone_ mul_ clone_ = None clone PlainAOTOutput idx= view SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= view_ SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= b mul_ SubclassSizeAOTOutput base=PlainAOTOutput idx= idx= primals_ SavedForBackwardsAOTOutput idx= primals_ SavedForBackwardsAOTOutput idx= noqa B assertExpectedInline normalize_gm bw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassStrideAOTInput base=PlainAOTInput idx= idx= tangents_ f s s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= tangents_ f s s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= b view_ f s s = torch ops aten view default tangents_ primals_ primals_ tangents_ = None view_ f s s = torch ops aten view default tangents_ primals_ primals_ tangents_ = None None None None None view_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= view_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= b primals_ SubclassSizeAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= primals_ SubclassSizeAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= primals_ SubclassStrideAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= noqa B unittest expectedFailure test_tensor_subclass_TwoTensor_return_multiple f tt y = tt clone z = tt clone y y view y shape y shape y b z view - = torch ones requires_grad=True b = clone tt = TwoTensor b fw bw = _compile_check f tt dynamic=True call_backward=True assertExpectedInline normalize_gm fw print_readable print_output=False \ GraphModule torch nn Module forward primals_ f primals_ f primals_ Sym primals_ Sym primals_ Sym primals_ Sym clone f = torch ops aten clone default primals_ primals_ = None clone_ f = torch ops aten clone default primals_ primals_ = None mul Sym = primals_ primals_ view f = torch ops aten view default clone mul view_ f = torch ops aten view default clone_ mul clone_ = None clone view view_ mul primals_ primals_ noqa B assertExpectedInline normalize_gm bw print_readable print_output=False \ GraphModule torch nn Module forward primals_ Sym primals_ Sym tangents_ f tangents_ f view_ f = torch ops aten view default tangents_ primals_ primals_ tangents_ = None view_ f = torch ops aten view default tangents_ primals_ primals_ tangents_ = primals_ = primals_ = None view_ view_ None None noqa B test_tensor_subclass_TwoTensor_automatic_dynamic_shapes f tt y = tt clone y y view - y b = torch ones requires_grad=True b = clone tt = TwoTensor b = torch ones requires_grad=True b = clone tt = TwoTensor b fw bw = _compile_check f tt tt dynamic=None call_backward=True assertExpectedInline normalize_gm fw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ f SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= primals_ f SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= b clone f = torch ops aten clone default primals_ primals_ = None clone_ f = torch ops aten clone default primals_ primals_ = None view f = torch ops aten view default clone - view_ f = torch ops aten view default clone_ - clone PlainAOTOutput idx= view SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= view_ SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= b clone_ PlainAOTOutput idx= noqa B assertExpectedInline normalize_gm fw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ Sym s PlainAOTInput idx= primals_ f s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= primals_ f s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= b primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassStrideAOTInput base=PlainAOTInput idx= idx= clone f s = torch ops aten clone default primals_ primals_ = None clone_ f s = torch ops aten clone default primals_ primals_ = None view f s = torch ops aten view default clone - sym_size_int_ Sym s = torch ops aten sym_size int view view_ f s = torch ops aten view default clone_ - clone PlainAOTOutput idx= view SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= view_ SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= b sym_size_int_ SubclassSizeAOTOutput base=PlainAOTOutput idx= idx= clone_ PlainAOTOutput idx= primals_ SavedForBackwardsAOTOutput idx= noqa B assertExpectedInline normalize_gm bw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward tangents_ f SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= tangents_ f SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= b view_ f = torch ops aten view default tangents_ tangents_ = None view_ f = torch ops aten view default tangents_ tangents_ = None view_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= view_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= b noqa B assertExpectedInline normalize_gm bw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ Sym s SubclassStrideAOTInput base=PlainAOTInput idx= idx= tangents_ f s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= tangents_ f s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= b view_ f s = torch ops aten view default tangents_ primals_ tangents_ = None view_ f s = torch ops aten view default tangents_ primals_ tangents_ = None None None view_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= view_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= b primals_ SubclassSizeAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= primals_ SubclassStrideAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= noqa B test_tensor_subclass_TwoTensor_mark_dynamic_shapes f tt y = tt clone y y view - y b = torch ones requires_grad=True b = clone tt = TwoTensor b torch _dynamo mark_dynamic tt fw bw = _compile_check f tt dynamic=None call_backward=True assertExpectedInline normalize_gm fw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ Sym s PlainAOTInput idx= primals_ f s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= primals_ f s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= b primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassStrideAOTInput base=PlainAOTInput idx= idx= clone f s = torch ops aten clone default primals_ primals_ = None clone_ f s = torch ops aten clone default primals_ primals_ = None view f s = torch ops aten view default clone - sym_size_int_ Sym s = torch ops aten sym_size int view view_ f s = torch ops aten view default clone_ - clone PlainAOTOutput idx= view SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= view_ SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= b sym_size_int_ SubclassSizeAOTOutput base=PlainAOTOutput idx= idx= clone_ PlainAOTOutput idx= primals_ SavedForBackwardsAOTOutput idx= noqa B assertExpectedInline normalize_gm bw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ Sym s SubclassStrideAOTInput base=PlainAOTInput idx= idx= tangents_ f s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= tangents_ f s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= b view_ f s = torch ops aten view default tangents_ primals_ tangents_ = None view_ f s = torch ops aten view default tangents_ primals_ tangents_ = None None None view_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= view_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= b primals_ SubclassSizeAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= primals_ SubclassStrideAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= noqa B test_tensor_subclass_TwoTensor_different_shape f tt y = tt clone y view = torch ones requires_grad=True b = clone tt = TwoTensor b fw bw = _compile_check f tt dynamic=True call_backward=True assertExpectedInline normalize_gm fw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ f SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= primals_ f SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= b clone f = torch ops aten clone default primals_ primals_ = None clone_ f = torch ops aten clone default primals_ primals_ = None view f = torch ops aten view default clone clone = None view_ f = torch ops aten view default clone_ clone_ = None view SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= view_ SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= b noqa B assertExpectedInline normalize_gm bw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward tangents_ f SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= tangents_ f SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= b view_ f = torch ops aten view default tangents_ tangents_ = None view_ f = torch ops aten view default tangents_ tangents_ = None view_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= view_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= b noqa B test_tensor_subclass_TwoTensor_return_shape torch compile backend= aot_eager dynamic=True fn x x clone view x shape x shape = torch ones b = clone tt = TwoTensor b out = fn tt assertEqual tt view out assertEqual out shape test_tensor_subclass_TwoTensor_nested torch compile backend= aot_eager dynamic=True f x i y out = x sin + i sin + y sin val = x shape i shape y shape out val i = torch randn requires_grad=True x = TwoTensor i i clone y = TwoTensor x clone x clone out = f x i y x_test = x detach clone requires_grad_ True i_test = i detach clone requires_grad_ True y_test = y detach clone requires_grad_ True out_test = f x_test i_test y_test torch allclose out out_test out sum backward out_test sum backward torch allclose x grad x_test grad torch allclose i grad i_test grad torch allclose y grad y_test grad test_subclass_TwoTensor_TwoTensor_TwoTensor torch compile backend= aot_eager dynamic=True f x x sin data = torch randn s = TwoTensor data data clone y = TwoTensor s s clone z = TwoTensor s y out = f z assertEqual out z sin test_subclass_TwoTensor_nested_diff_sizes TT TwoTensor staticmethod __new__ cls b outer_size=None outer_stride=None outer_size None outer_size = size outer_stride None outer_stride = stride assert device == b device layout == b layout requires_grad == b requires_grad dtype == b dtype shape = outer_size kwargs = kwargs strides = outer_stride kwargs storage_offset = storage_offset kwargs device = device kwargs layout = layout kwargs requires_grad = requires_grad kwargs dtype = dtype out = torch Tensor _make_wrapper_subclass cls shape kwargs out staticmethod __tensor_unflatten__ inner_tensors meta outer_size outer_stride assert meta None b = inner_tensors inner_tensors b type torch Tensor assert outer_size None assert outer_stride None TT b outer_size outer_stride torch compile dynamic=True f x y tmp = x sin tmp = y sin tmp sum tmp sum x = TT TT torch randn torch randn TT torch randn torch randn y = TT torch randn TT torch randn torch randn out = f x y assertEqual out x sin sum y sin sum test_njt_subclass_simple f nt y = nt clone y y size nt _ = get_jagged_tensor None True fw bw = _compile_check f nt dynamic=True call_backward=True assertExpectedInline normalize_gm fw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ Sym s PlainAOTInput idx= primals_ Sym s PlainAOTInput idx= primals_ Sym s PlainAOTInput idx= primals_ f s s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= _values primals_ i s + SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= _offsets primals_ f s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= _min_seqlen_tensor primals_ f s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= _max_seqlen_tensor primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassStrideAOTInput base=PlainAOTInput idx= idx= clone f s s = torch ops aten clone default primals_ primals_ = None mul f s s = torch ops aten mul Tensor clone primals_ clone = None mul SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= _values primals_ SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= _offsets primals_ SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= _min_seqlen_tensor primals_ SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= _max_seqlen_tensor primals_ SubclassSizeAOTOutput base=PlainAOTOutput idx= idx= primals_ SubclassSizeAOTOutput base=PlainAOTOutput idx= idx= primals_ SubclassStrideAOTOutput base=PlainAOTOutput idx= idx= primals_ SavedForBackwardsAOTOutput idx= primals_ SavedForBackwardsAOTOutput idx= primals_ SavedForBackwardsAOTOutput idx= noqa B assertExpectedInline normalize_gm bw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ Sym s PlainAOTInput idx= primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassStrideAOTInput base=PlainAOTInput idx= idx= tangents_ f s s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= _values tangents_ i s + SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= _offsets tangents_ f s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= _min_seqlen_tensor tangents_ f s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= _max_seqlen_tensor mul_ f s s = torch ops aten mul Tensor tangents_ primals_ tangents_ = primals_ = None None None None None None None mul_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= _values tangents_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= _offsets tangents_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= _min_seqlen_tensor tangents_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= _max_seqlen_tensor primals_ SubclassSizeAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= primals_ SubclassSizeAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= primals_ SubclassStrideAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= noqa B test_njt_subclass_from_cat create existing NJT f nt y = nt clone z = torch cat y y dim=- z nt _ = get_jagged_tensor None True fw bw = _compile_check f nt dynamic=True call_backward=True assertExpectedInline normalize_gm fw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ Sym s PlainAOTInput idx= primals_ Sym s PlainAOTInput idx= primals_ Sym s PlainAOTInput idx= primals_ f s s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= _values primals_ i s + SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= _offsets primals_ f s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= _min_seqlen_tensor primals_ f s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= _max_seqlen_tensor primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassStrideAOTInput base=PlainAOTInput idx= idx= clone f s s = torch ops aten clone default primals_ primals_ = None cat f s s = torch ops aten cat default clone clone clone = None add_ Sym s = primals_ + primals_ cat SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= _values primals_ SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= _offsets primals_ SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= _min_seqlen_tensor primals_ SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= _max_seqlen_tensor primals_ SubclassSizeAOTOutput base=PlainAOTOutput idx= idx= add_ SubclassSizeAOTOutput base=PlainAOTOutput idx= idx= add_ SubclassStrideAOTOutput base=PlainAOTOutput idx= idx= primals_ SavedForBackwardsAOTOutput idx= primals_ SavedForBackwardsAOTOutput idx= add_ SavedForBackwardsAOTOutput idx= noqa B assertExpectedInline normalize_gm bw print_readable print_output=False expanded_def=True \ GraphModule torch nn Module forward primals_ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= primals_ Sym s SubclassStrideAOTInput base=PlainAOTInput idx= idx= add_ Sym s tangents_ f s s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= _values tangents_ i s + SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= _offsets tangents_ f s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= _min_seqlen_tensor tangents_ f s SubclassGetAttrAOTInput base=TangentAOTInput output=PlainAOTOutput idx= attr= _max_seqlen_tensor slice_ f s s = torch ops aten slice Tensor tangents_ primals_ slice_ f s s = torch ops aten slice Tensor tangents_ primals_ add_ tangents_ = add_ = None add_ f s s = torch ops aten add Tensor slice_ slice_ slice_ = slice_ = None None None None None None None add_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= _values tangents_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= _offsets tangents_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= _min_seqlen_tensor tangents_ SubclassGetAttrAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= attr= _max_seqlen_tensor primals_ SubclassSizeAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= primals_ SubclassSizeAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= primals_ SubclassStrideAOTOutput base=GradAOTOutput grad_of=PlainAOTInput idx= idx= noqa B test_njt_subclass_from_buffer create NJT buffer f nt nested_size = offsets = None nt _ = get_jagged_tensor nested_size offsets requires_grad=False nt = torch cat nt nt dim=- nt sin nt size nested_size = offsets = None nt _ = get_jagged_tensor nested_size offsets requires_grad=False fw _ = _compile_check f nt dynamic=True call_backward=False we cannot set requires_grad=True inside compile region assertExpectedInline normalize_gm fw print_readable print_output=False expanded_def=True \ lambda torch nn Module forward arg _ Sym s PlainAOTInput idx= arg _ Sym s PlainAOTInput idx= arg _ Sym s PlainAOTInput idx= arg _ f s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= _values arg _ i s + SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= _offsets arg _ f s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= _min_seqlen_tensor arg _ f s SubclassGetAttrAOTInput base=PlainAOTInput idx= attr= _max_seqlen_tensor arg _ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= arg _ Sym s SubclassSizeAOTInput base=PlainAOTInput idx= idx= arg _ Sym s SubclassStrideAOTInput base=PlainAOTInput idx= idx= randn f = torch ops aten randn default dtype = torch float device = device type= cpu pin_memory = False randn_ f = torch ops aten randn default dtype = torch float device = device type= cpu pin_memory = False randn_ f = torch ops aten randn default dtype = torch float device = device type= cpu pin_memory = False cat f = torch ops aten cat default randn randn_ randn_ randn = randn_ = randn_ = None zeros i = torch ops aten zeros default dtype = torch int device = device type= cpu pin_memory = False _tensor_constant i = _tensor_constant lift_fresh_copy i = torch ops aten lift_fresh_copy default _tensor_constant _tensor_constant = None cumsum i = torch ops aten cumsum default lift_fresh_copy lift_fresh_copy = None cat_ i = torch ops aten cat default zeros cumsum zeros = cumsum = None zeros_ f = torch ops aten zeros default device = device type= cpu pin_memory = False zeros_ f = torch ops aten zeros default device = device type= cpu pin_memory = False cat_ f s + = torch ops aten cat default cat arg _ cat = arg _ = None sin f s + = torch ops aten sin default cat_ mul f s + = torch ops aten mul Tensor sin sin = None sym_size_int Sym s + = torch ops aten sym_size int cat_ cat_ = None sym_stride_int Sym s + = torch ops aten sym_stride int mul mul SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= _values cat_ SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= _offsets zeros_ SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= _min_seqlen_tensor zeros_ SubclassGetAttrAOTOutput base=PlainAOTOutput idx= attr= _max_seqlen_tensor sym_size_int SubclassSizeAOTOutput base=PlainAOTOutput idx= idx= sym_stride_int SubclassStrideAOTOutput base=PlainAOTOutput idx= idx= noqa B instantiate_parametrized_tests SubclassTests TestNestedTensor torch _dynamo test_case TestCase NestedTensorTestCase _get_jagged_tensor nested_size offsets requires_grad=True get_jagged_tensor nested_size offsets requires_grad _get_nc_jagged_tensor inner_dim starts lengths requires_grad=True Makes jagged tensor N constituent tensors size specified S S S D max_dim = starts + lengths max values_tensor = torch randn starts shape max_dim item inner_dim requires_grad=requires_grad dtype=torch float jagged_from_tensor_and_lengths values_tensor starts lengths _check_recompiles fn inputs inputs expected_recompiles _check_recompiles fn inputs inputs expected_recompiles test_unary_does_not_recompile nt _ = _get_jagged_tensor None nt _ = _get_jagged_tensor None _check_recompiles lambda nt nt sin nt nt False test_binary_does_not_recompile binary nt nt nt shape == nt shape nt + nt nt sin NB If we have shape e g j duck sizing will give us s s s This causes recompile later when realizes batch last dim should always equal To avoid we use j here nt offsets = _get_jagged_tensor None nt _ = _get_jagged_tensor offsets nt offsets = _get_jagged_tensor None nt _ = _get_jagged_tensor offsets _check_recompiles binary nt nt nt nt False test_binary_recompiles binary nt nt nt shape == nt shape nt + nt nt sin Binary recompiles because singleton ints no longer match nt offsets = _get_jagged_tensor None nt _ = _get_jagged_tensor offsets nt _ = _get_jagged_tensor None _check_recompiles binary nt nt nt nt True _validate_compile fn arg_fn _gen_grad_outputs out_val isinstance out_val list tuple tuple torch ones_like c c out_val torch ones_like out_val branch_nested_state torch nested _internal nested_tensor _tensor_symint_registry Validate compilation does modify eager state registry_before = list _tensor_symint_registry items count_before = torch nested _internal nested_tensor _tensor_id_counter guards_exported = guards_failed = append_guard_export guards g guards g code_list None guards_exported append g code_list append_guard_fail guards guards_failed extend guards compiled = torch _dynamo optimize nopython=True backend= aot_eager guard_export_fn=append_guard_export guard_fail_fn=append_guard_fail fn registry_after = list _tensor_symint_registry items count_after = torch nested _internal nested_tensor _tensor_id_counter assertEqual registry_before registry_after assertEqual count_before count_after args = arg_fn compile_out = compiled args compile_grads = g_args = arg arg args arg requires_grad len g_args compile_grad_outputs = _gen_grad_outputs compile_out compile_grads = torch autograd grad compile_out inputs=g_args grad_outputs=compile_grad_outputs branch_nested_state args = arg_fn ref_out = fn args ref_grads = g_args = arg arg args arg requires_grad len g_args ref_grad_outputs = _gen_grad_outputs ref_out ref_grads = torch autograd grad ref_out inputs=g_args grad_outputs=ref_grad_outputs Validate correctness forward isinstance compile_out list tuple TODO Fix assertEqual support NJTs so isn t necessary assertEqual len compile_out len ref_out c r zip compile_out ref_out assertEqualIgnoringNestedInts c r assertEqualIgnoringNestedInts compile_out ref_out Validate correctness backward compile_grad ref_grad zip compile_grads ref_grads assertEqualIgnoringNestedInts compile_grad ref_grad guards_exported guards_failed test_in_graph_is_nested_call f nt nt is_nested nt + nt + cnt = CompileCounterWithBackend aot_eager compiled_f = torch compile f backend=cnt fullgraph=True nt offsets = _get_jagged_tensor None output = compiled_f nt output backward torch ones_like output assertEqual cnt frame_count assertEqual len cnt graphs graph = cnt graphs norm_graph = normalize_gm graph print_readable print_output=False expect -no- is_nested calls within graph assertExpectedInline norm_graph \ GraphModule torch nn Module forward s Sym s L_nt_ NestedTensor f s l_nt_ = L_nt_ add NestedTensor f s = l_nt_ + l_nt_ = None add noqa B Note What kind guards involved nested tensor compilation Until we implement UnionFind dynamic shapes guards involved we rely only dynamo s tensor aliasing guards This possible because dynamo able generate tensor aliasing guards only outer tensor also inner tensor The case where dynamic shapes guards would eventually come into play when my inputs two non-aliased tensors declared equal using trust me assert equal API Note Compiling nested tensor global state Today there two pieces global eager state NJTs deals - tensor_id_counter global counter assigns unique ids tensors - tensor_symint_registry maps tensor nested int - used eager only we should get rid because necessary cache nested int eager - during tracing we DO need cache nested int we do so FakeTensor Ideally we would like satisfy following - The eager state mutated during tracing - Running compiled function should mutate eager state same way running eager function would The global counter should incremented b The registry updated same way Today we can satisfy cannot satisfy b Today satisfied because we maintain separate counter during tracing cache nested int FakeTensor instead relying tensor_symint_registry cannot completely satisfied because we trace away side-effectful operations which we can fix wrapping side-effectful operations custom op threading through effect tokens The current plan do UnionFind impl Interestingly despite state mutated way somewhat close what we want e g I construct nested tensor using offsets compiled region AOTAutograd runtime wrapper must rewrap inner- inner graph outputs back into subclass This triggers eager logic run updating counter registry Notably however compile differs two ways eager The order which offsets assigned ids different registry would set order offsets returned which necessarily same order they constructed If NestedTensor returned then AOTAutograd wrapping logic will triggered I claim correctness affected these differences today e g there never case where two distinct offsets silently share same id clearly problem should only problem nested int returned its own without corresponding NJT being returned This problem current implementation because returning only shape supported Note Creating symbolic nested int We must create symbolic nested int when we construct nested tensor tensor There two main cases The offsets has NOT been used construct NJT - Create new plain nested int current val fake nt id counter - Increment fake nt id counter - Create new symint plain nested int hint The offsets HAS been used construct NJT - Create new symint plain nested int hint More details case - During fakification offsets we check eager registry tensor HAS been used construct NJT we create symint existing nested int hint cache FakeTensor Always use ephemeral source We create new symint ALWAYS ephemeral source whether case even though we could ve had proper source case Using proper source would enable few more edge cases since we plan handle things more holistically future anyway we don t bother doing so today Using ephemeral source has some consequences But we happy - We do silently miss recompiles e g we guard when necessary We know true because dynamo guards alone already sufficient - We producing errors cases we care about The main case we care about when we guard two shapes equal In case replacements logic would simplify away ephemeral symbol there no error produced The unsupported case when we guard two shapes equal which we will try fail generate guard Case in-graph construction where offsets passed inputs test_in_graph_construction_from_input The offsets passed input fn values offsets torch nested nested_tensor_from_jagged values offsets values = torch randn requires_grad=True offsets = torch tensor dtype=torch int _validate_compile fn arg_fn=lambda values offsets Do specialize offsets unittest mock patch torch _dynamo config error_on_recompile True different_offsets = torch tensor dtype=torch int _validate_compile fn arg_fn=lambda values different_offsets test_in_graph_construction_from_input_ Construct two NJTs both passed inputs fn values offsets offsets nt = torch nested nested_tensor_from_jagged values offsets nt = torch nested nested_tensor_from_jagged values offsets nt nt values = torch randn requires_grad=True offsets = torch tensor dtype=torch int offsets = torch tensor dtype=torch int Offsets different guards_exported guards_failed = _validate_compile fn arg_fn=lambda values offsets offsets assertEqual len guards_failed assertNotIn L offsets L offsets guards_exported TODO Offsets same new_guards_exported _ = _validate_compile fn arg_fn=lambda values offsets offsets assertTrue any Duplicate tensors found g g guards_failed assertIn L offsets L offsets new_guards_exported unittest mock patch torch _dynamo config error_on_recompile True offsets = offsets clone _validate_compile fn arg_fn=lambda values offsets offsets Do binary op fn values offsets offsets nt = torch nested nested_tensor_from_jagged values offsets nt = torch nested nested_tensor_from_jagged values offsets nt nt _validate_compile fn arg_fn=lambda values offsets offsets test_in_graph_construction_from_input_ The offsets taken NJT input fn nt other_values nt = torch nested nested_tensor_from_jagged other_values nt offsets nt + nt values = torch randn requires_grad=True other_values = torch randn requires_grad=True offsets = torch tensor dtype=torch int arg_fn values=values other_values=other_values offsets=offsets nt = torch nested nested_tensor_from_jagged values offsets nt other_values _validate_compile fn arg_fn=arg_fn Do specialize offsets unittest mock patch torch _dynamo config error_on_recompile True different_offsets = offsets clone arg_fn values=values other_values=other_values offsets=different_offsets nt = torch nested nested_tensor_from_jagged values different_offsets nt other_values _validate_compile fn arg_fn=arg_fn test_in_graph_construction_from_input_ Construct lengths instead offsets fn values lengths nt = torch nested nested_tensor_from_jagged values lengths=lengths nt sin values = torch randn requires_grad=True lengths = torch tensor _validate_compile fn arg_fn=lambda values lengths test_in_graph_construction_from_input_ Construct symbolic int fn values offsets max_seqlen t = torch nested nested_tensor_from_jagged values offsets max_seqlen=max_seqlen torch nested nested_tensor_from_jagged values t offsets max_seqlen=t _maybe_max_seqlen opt_fn = torch compile fn fullgraph=True dynamic=True values = torch randn offsets = torch tensor max_seqlen = ref = fn values offsets max_seqlen res = opt_fn values offsets max_seqlen assertEqualIgnoringNestedInts ref res Case in-graph construction where offsets graph intermediates test_in_graph_construction_from_intermediate offsets intermediate computed lengths fn values lengths offsets = torch cat lengths new_zeros lengths cumsum nt = torch nested nested_tensor_from_jagged values offsets nt = torch nested nested_tensor_from_jagged values offsets nt nt sin values = torch randn requires_grad=True lengths = torch tensor _validate_compile fn arg_fn=lambda values lengths Do specialize lengths unittest mock patch torch _dynamo config error_on_recompile True different_lengths = lengths clone _validate_compile fn arg_fn=lambda values different_lengths test_in_graph_construction_from_intermediate_ fn values offsets torch nested nested_tensor_from_jagged values offsets clone values = torch randn requires_grad=True offsets = torch tensor dtype=torch int _validate_compile fn arg_fn=lambda values offsets test_in_graph_construction_from_intermediate_ Note due CSE clone necessarily called twice fn values offsets nt = torch nested nested_tensor_from_jagged values offsets clone nt = torch nested nested_tensor_from_jagged values offsets clone nt nt values = torch randn requires_grad=True offsets = torch tensor dtype=torch int _validate_compile fn arg_fn=lambda values offsets test_in_graph_construction_from_intermediate_ Shared intermediate should same case fn values offsets = torch tensor dtype=torch int nt = torch nested nested_tensor_from_jagged values offsets values = torch ones_like values nt = torch nested nested_tensor_from_jagged values offsets nt nt values = torch randn requires_grad_ True _validate_compile fn arg_fn=lambda values AssertionError s could ephemeral intermediate_offsets_or_lengths unittest expectedFailure test_in_graph_construction_from_intermediate_ non-shared intermediate fn values offsets = torch tensor dtype=torch int nt = torch nested nested_tensor_from_jagged values offsets values = torch ones_like values nt = torch nested nested_tensor_from_jagged values offsets clone nt shape = nt shape nt nt values = torch randn requires_grad_ True _validate_compile fn arg_fn=lambda values Case in-graph construction where offsets both direct graph inputs passed part NJT s offsets test_in_graph_construction_mixed fn nt values offsets nt = torch nested nested_tensor_from_jagged values offsets nt nt values = torch randn requires_grad=True offsets = torch tensor dtype=torch int arg_fn values=values offsets=offsets nt = torch nested nested_tensor_from_jagged values offsets nt values offsets _validate_compile fn arg_fn See Note Creating symbolic nested int AssertionError s could ephemeral intermediate_offsets_or_lengths unittest expectedFailure test_in_graph_construction_mixed_ fn nt values offsets nt Intermediate offsets has ephemeral source intermediate_nt = torch nested nested_tensor_from_jagged values offsets clone This creates dynamic shapes neq guard nt shape = intermediate_nt shape We should always go here nt = nt nt values = torch randn requires_grad=True offsets = torch tensor dtype=torch int offsets = torch tensor dtype=torch int arg_fn values=values offsets=offsets offsets =offsets Values shared shouldn t matter nt = torch nested nested_tensor_from_jagged values offsets nt = torch nested nested_tensor_from_jagged values offsets nt values offsets nt _validate_compile fn arg_fn test_in_graph_construction_mixed_ More involved mixed case fn nt values offsets nt = torch nested nested_tensor_from_jagged values offsets nt = torch nested nested_tensor_from_jagged values offsets nt + nt + nt values = torch randn requires_grad=True offsets = torch tensor dtype=torch int arg_fn values=values offsets=offsets nt = torch nested nested_tensor_from_jagged values offsets nt values offsets _validate_compile fn arg_fn test_return_shape nt _ = _get_jagged_tensor None fn nt nt shape compiled = torch compile fn fullgraph=True backend= aot_eager compiled nt test_inference_tensor torch inference_mode nt _ = _get_jagged_tensor None fn n n torch compile fn backend= eager nt TODO cannot parametrize test device some reason _test_autograd backend = torch randn requires_grad=True dtype=torch float b = torch randn requires_grad=True dtype=torch float c = torch randn requires_grad=True dtype=torch float nt = torch nested as_nested_tensor b c layout=torch jagged TODO Switch public API when exists nt _ = jagged_from_list b c nt offsets fn nt nt nt + nt sin cos compiled_f = torch compile fn fullgraph=True backend=backend dynamic=True out = compiled_f nt nt out_buffer = out values ga gb gc = torch autograd grad out_buffer sum b c out_ref = fn nt nt out_buffer_ref = out_ref values ga_ref gb_ref gc_ref = torch autograd grad out_buffer_ref sum b c assertTrue torch allclose ga ga_ref assertTrue torch allclose gb gb_ref assertTrue torch allclose gc gc_ref test_basic_autograd _test_autograd aot_eager requires_cuda_and_triton test_basic_autograd_inductor _test_autograd inductor test_subclass_with_mutation_in_graph In graph we have in-graph mutation i e mutation allowed remain graph Normally allowed s allowed graph handles subclasses all Whether mutation allowed allowed graph alters number outputs forward graph Previously bug handling meant sometimes expected number actual number outputs joint graph did match causing assertion failures fn x y z = x sin y sin_ z cos y cos fn_c = torch compile fn backend= inductor values = torch rand i requires_grad=True i range values_copy = x detach clone requires_grad_ True x values nt offsets = jagged_from_list values None nt_copy offsets = jagged_from_list values_copy offsets y = torch rand y_copy = y clone ret = fn_c nt y ref = fn nt_copy y_copy assertEqual ret values ref values ret values sum backward ref values sum backward ref_v res_v zip values_copy values assertEqual ref_v grad res_v grad torch _dynamo config patch capture_scalar_outputs True test_unbind NB If we have shape e g j duck sizing will give us s s s This causes recompile later when realizes batch last dim should always equal To avoid we use j here nt _ = _get_jagged_tensor None nt _ = _get_jagged_tensor None nt _ = _get_jagged_tensor None fn x x unbind compiled_f = torch compile fn fullgraph=True backend= eager dynamic=True out = compiled_f nt out_ref = fn nt correctness assertEqual len out len out_ref x x_ref zip out out_ref assertTrue torch allclose x x_ref We specialize length offsets e g we recompile length offsets different we don t recompile length offsets same even size constituent tensors different _check_recompiles fn nt nt False _check_recompiles fn nt nt True test_inline_nested_tensor_from_jagged nt _ = _get_jagged_tensor None fn x torch nested nested_tensor_from_jagged x values x offsets torch compile fn fullgraph=True backend= aot_eager nt The test here nn Parameters secretly subclasses have metaclass overrides __isinstance__ dynamo needs respect when inlines statement test_param_subclass_isinstance_input x_inner = torch randn requires_grad=True x = torch nn Parameter TwoTensor x_inner x_inner m = torch nn Linear m weight = x fn isinstance m weight torch nn Parameter m weight + m weight + out_ref = fn out_test = torch compile fn backend= aot_eager assertEqual out_ref out_test _input_view_test nt_view_name nt_view = VIEW_TEST_CASES nt_view_name fn x x sin out_ref = fn nt_view torch _dynamo reset compile_fn = torch compile fn fullgraph=True backend= aot_eager dynamic=True out = compile_fn nt_view Check metadata values correct assertTrue out size == out_ref size assertTrue out stride == out_ref stride out is_nested assertTrue torch allclose out values out_ref values assertTrue torch allclose out out_ref Check no upper lower bound guards incurred backend gm args context = torch _guards TracingContext get guards = str g expr g context fake_mode shape_env guards varies based type view guard_str = \n join guards nt_view_name == base_is_nt_False_basic assertExpectedInline guard_str \ Eq s - s Eq s s Eq s - s Eq s s nt_view_name == base_is_nt_False_leaf_False_False assertExpectedInline guard_str \ Eq s - s Eq s - s Eq s s nt_view_name == base_is_nt_False_leaf_False_True assertExpectedInline guard_str \ Eq s - s Eq s s Eq s - s Eq s s nt_view_name == base_is_nt_False_leaf_True_False assertExpectedInline guard_str \ Eq s - s Eq s s Eq s - s Eq s s nt_view_name == base_is_nt_False_leaf_True_True assertExpectedInline guard_str \ Eq s - s Eq s s Eq s - s Eq s s nt_view_name == base_is_nt_False_obscure assertExpectedInline guard_str \ Eq s - s Eq s s Eq s - s Eq s s nt_view_name == base_is_nt_True_basic assertExpectedInline guard_str \ Eq s - s Eq s s nt_view_name == base_is_nt_True_leaf_False_False assertExpectedInline guard_str Eq s - s nt_view_name == base_is_nt_True_leaf_False_True assertExpectedInline guard_str \ Eq s - s Eq s s nt_view_name == base_is_nt_True_leaf_True_False assertExpectedInline guard_str \ Eq s - s Eq s s nt_view_name == base_is_nt_True_leaf_True_True assertExpectedInline guard_str \ Eq s - s Eq s s nt_view_name == base_is_nt_True_obscure assertExpectedInline guard_str \ Eq s - s Eq s s nt_view_name == dense_subclass_dense_subclass assertExpectedInline guard_str \ Eq s - s Eq s - s Eq s s nt_view_name == subclass_dense assertExpectedInline guard_str \ Eq s - s Eq s s raise NotImplementedError gm torch _dynamo reset compile_fn = torch compile fn fullgraph=True backend=backend dynamic=True out = compile_fn nt_view parametrize nt_view_name k k VIEW_TEST_CASES keys k = subclass_dense_subclass_dense test_inputs_to_compiled_fn_are_views nt_view_name _input_view_test nt_view_name test_subclass_gives_static_shapes_when_dynamic_false check_graph gm args first_node_example_val = next iter gm graph nodes meta example_value We compiled dynamic=False expect no SymInt sizes our placeholders assertTrue all isinstance x int x first_node_example_val shape gm torch compile backend=check_graph dynamic=False f x x + x_inner = torch ones x = TwoTensor x_inner x_inner x_view = x view out = f x_view noqa F NJT - Dense - NJT - Dense view During view replay Dense - NJT part will construct intermediate symbolically-sized NJT immediately deconstructed final dense view To construct intermediate properly we need associated nested int symbolic This view expected fail compilation until symbolic nested ints cached onto fake offsets solve problem unittest expectedFailure test_subclass_dense_subclass_dense_view _input_view_test subclass_dense_subclass_dense instantiate_parametrized_tests TestNestedTensor __name__ == __main__ torch _dynamo test_case run_tests run_tests