mypy allow-untyped-defs copy glob importlib importlib abc os re shlex shutil setuptools subprocess sys sysconfig types collections pathlib Path errno logging logger = logging getLogger __name__ torch torch _appdirs file_baton FileBaton _cpp_extension_versioner ExtensionVersioner typing Optional Union typing_extensions deprecated torch torch_version TorchVersion Version setuptools command build_ext build_ext IS_WINDOWS = sys platform == win IS_MACOS = sys platform startswith darwin IS_LINUX = sys platform startswith linux LIB_EXT = pyd IS_WINDOWS so EXEC_EXT = exe IS_WINDOWS CLIB_PREFIX = IS_WINDOWS lib CLIB_EXT = dll IS_WINDOWS so SHARED_FLAG = DLL IS_WINDOWS -shared _HERE = os path abspath __file__ _TORCH_PATH = os path dirname os path dirname _HERE TORCH_LIB_PATH = os path join _TORCH_PATH lib SUBPROCESS_DECODE_ARGS = oem IS_WINDOWS MINIMUM_GCC_VERSION = MINIMUM_MSVC_VERSION = VersionRange = tuple tuple int tuple int VersionMap = dict str VersionRange The following values taken following GitHub gist summarizes minimum valid major versions g++ clang++ each supported CUDA version https gist github com ax l Or include crt host_config h CUDA SDK The second value exclusive upper bound i e min = version max CUDA_GCC_VERSIONS VersionMap = MINIMUM_GCC_VERSION MINIMUM_GCC_VERSION MINIMUM_GCC_VERSION MINIMUM_GCC_VERSION MINIMUM_CLANG_VERSION = CUDA_CLANG_VERSIONS VersionMap = MINIMUM_CLANG_VERSION MINIMUM_CLANG_VERSION MINIMUM_CLANG_VERSION MINIMUM_CLANG_VERSION MINIMUM_CLANG_VERSION MINIMUM_CLANG_VERSION MINIMUM_CLANG_VERSION __all__ = get_default_build_root check_compiler_ok_for_platform get_compiler_abi_compatibility_and_version BuildExtension CppExtension CUDAExtension SyclExtension include_paths library_paths load load_inline is_ninja_available verify_ninja_availability remove_extension_h_precompiler_headers get_cxx_compiler check_compiler_is_gcc Taken directly python stdlib See https github com pytorch pytorch issues _nt_quote_args args Optional list str - list str Quote command-line arguments DOS Windows conventions Just wraps every argument which contains blanks double quotes returns new argument list Cover None-type args f arg arg arg arg args _find_cuda_home - Optional str Find CUDA install path Guess cuda_home = os environ get CUDA_HOME os environ get CUDA_PATH cuda_home None Guess nvcc_path = shutil which nvcc nvcc_path None cuda_home = os path dirname os path dirname nvcc_path Guess IS_WINDOWS cuda_homes = glob glob C Program Files NVIDIA GPU Computing Toolkit CUDA v len cuda_homes == cuda_home = cuda_home = cuda_homes cuda_home = usr local cuda os path exists cuda_home cuda_home = None cuda_home torch cuda is_available logger warning No CUDA runtime found using CUDA_HOME= s cuda_home cuda_home _find_rocm_home - Optional str Find ROCm install path Guess rocm_home = os environ get ROCM_HOME os environ get ROCM_PATH rocm_home None Guess hipcc_path = shutil which hipcc hipcc_path None rocm_home = os path dirname os path dirname os path realpath hipcc_path can either ROCM_HOME hip bin hipcc ROCM_HOME bin hipcc os path basename rocm_home == hip rocm_home = os path dirname rocm_home Guess fallback_path = opt rocm os path exists fallback_path rocm_home = fallback_path rocm_home torch version hip None logger warning No ROCm runtime found using ROCM_HOME= s rocm_home rocm_home _find_sycl_home - Optional str sycl_home = None icpx_path = shutil which icpx Guess source code build developer user we ll have icpx PATH which will tell us SYCL_HOME location icpx_path None sycl_home = os path dirname os path dirname os path realpath icpx_path Guess users install Pytorch XPU support sycl runtime inside intel-sycl-rt which automatically installed via pip dependency try files = importlib metadata files intel-sycl-rt f files f name == libsycl so sycl_home = os path dirname Path f locate parent resolve break except importlib metadata PackageNotFoundError logger warning Trying find SYCL_HOME intel-sycl-rt package installed sycl_home _join_rocm_home paths - str Join paths ROCM_HOME raises error ROCM_HOME set This basically lazy way raising error missing $ ROCM_HOME only once we need get any ROCm-specific path ROCM_HOME None raise OSError ROCM_HOME environment variable set Please set your ROCm install root os path join ROCM_HOME paths _join_sycl_home paths - str Join paths SYCL_HOME raises error SYCL_HOME found This basically lazy way raising error missing SYCL_HOME only once we need get any SYCL-specific path SYCL_HOME None raise OSError SYCL runtime dected Please setup pytorch prerequisites Intel GPU following instruction https github com pytorch pytorch tab=readme-ov-file#intel-gpu-support install intel-sycl-rt via pip os path join SYCL_HOME paths ABI_INCOMPATIBILITY_WARNING = WARNING Your compiler s may ABI-incompatible PyTorch Please use compiler ABI-compatible GCC above See https gcc gnu org onlinedocs libstdc++ manual abi html See https gist github com goldsborough d f e ffc ff de c d instructions how install GCC higher WARNING WRONG_COMPILER_WARNING = WARNING Your compiler s compatible compiler Pytorch built platform which s s Please use s compile your extension Alternatively you may compile PyTorch source using s then you can also use s compile your extension See https github com pytorch pytorch blob master CONTRIBUTING md help compiling PyTorch source WARNING CUDA_MISMATCH_MESSAGE = The detected CUDA version s mismatches version used compile PyTorch s Please make sure use same CUDA versions CUDA_MISMATCH_WARN = The detected CUDA version s has minor version mismatch version used compile PyTorch s Most likely shouldn t problem CUDA_NOT_FOUND_MESSAGE = CUDA found system please set CUDA_HOME CUDA_PATH environment variable add NVCC your system PATH The extension compilation will fail ROCM_HOME = _find_rocm_home torch cuda _is_compiled torch version hip None HIP_HOME = _join_rocm_home hip ROCM_HOME None IS_HIP_EXTENSION = bool ROCM_HOME None torch version hip None ROCM_VERSION = None torch version hip None ROCM_VERSION = tuple int v v torch version hip split CUDA_HOME = _find_cuda_home torch cuda _is_compiled torch version cuda None CUDNN_HOME = os environ get CUDNN_HOME os environ get CUDNN_PATH SYCL_HOME = _find_sycl_home torch xpu _is_compiled None WINDOWS_CUDA_HOME = os environ get WINDOWS_CUDA_HOME used AOTI cross-compilation PyTorch releases have version pattern major minor patch whereas when PyTorch built source we append git commit hash which gives below pattern BUILT_FROM_SOURCE_VERSION_PATTERN = re compile r \d+\ \d+\ \d+\w+\+\w+ COMMON_MSVC_FLAGS = MD wd wd wd wd wd wd wd wd wd wd EHsc MSVC_IGNORE_CUDAFE_WARNINGS = base_class_has_different_dll_interface field_without_dll_interface dll_interface_conflict_none_assumed dll_interface_conflict_dllexport_assumed COMMON_NVCC_FLAGS = -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT _CONVERSIONS__ -D__CUDA_NO_HALF _OPERATORS__ -- expt-relaxed-constexpr COMMON_HIP_FLAGS = -D__HIP_PLATFORM_AMD__= -DUSE_ROCM= -DHIPBLAS_V IS_WINDOWS COMMON_HIP_FLAGS append -fPIC COMMON_HIPCC_FLAGS = -DCUDA_HAS_FP = -D__HIP_NO_HALF_OPERATORS__= -D__HIP_NO_HALF_CONVERSIONS__= -DHIP_ENABLE_WARP_SYNC_BUILTINS= IS_WINDOWS Compatibility flags similar those set cmake Dependencies cmake COMMON_HIPCC_FLAGS append -fms-extensions Suppress warnings about dllexport COMMON_HIPCC_FLAGS append -Wno-ignored-attributes _get_icpx_version - str icpx = icx IS_WINDOWS icpx compiler_info = subprocess check_output icpx -- version match = re search r \d+ \ \d+ \ \d+ compiler_info decode strip version = match None list match groups version = list map int version len version = raise AssertionError Failed parse DPC++ compiler version Aligning version format what torch version xpu returns f version version version _get_sycl_arch_list TORCH_XPU_ARCH_LIST os environ os environ get TORCH_XPU_ARCH_LIST arch_list = torch xpu get_arch_list Dropping dg archs since they lack hardware support fp require special consideration user If needed these platforms can requested thru TORCH_XPU_ARCH_LIST environment variable arch_list = x x arch_list x startswith dg join arch_list If arch list returned _get_sycl_arch_list empty then sycl kernels will compiled default spir target avoid device specific compilations entirely Further kernels will JIT compiled runtime _append_sycl_targets_if_missing cflags any flag startswith -fsycl-targets= flag cflags do nothing user has manually specified sycl targets _get_sycl_arch_list = AOT spir _gen + JIT spir cflags append -fsycl-targets=spir _gen spir JIT spir cflags append -fsycl-targets=spir _get_sycl_device_flags cflags We need last occurrence -fsycl-targets will one taking effect So searching reversed list flags = f f reversed cflags f startswith -fsycl-targets= flags raise AssertionError bug -fsycl-targets should have been amended cflags arch_list = _get_sycl_arch_list arch_list = flags += f -Xs -device arch_list flags _COMMON_SYCL_FLAGS = -fsycl _SYCL_DLINK_FLAGS = _COMMON_SYCL_FLAGS -fsycl-link -- offload-compress JIT_EXTENSION_VERSIONER = ExtensionVersioner PLAT_TO_VCVARS = win x win-amd x _amd min_supported_cpython = x A Python hexcode get_cxx_compiler IS_WINDOWS compiler = os environ get CXX cl compiler = os environ get CXX c++ compiler _is_binary_build - bool BUILT_FROM_SOURCE_VERSION_PATTERN match torch version __version__ _accepted_compilers_for_platform - list str gnu-c++ gnu-cc conda gcc compilers clang++ clang IS_MACOS g++ gcc gnu-c++ gnu-cc clang++ clang _maybe_write filename new_content r Equivalent writing content into file will touch file already had right content avoid triggering recompile os path exists filename open filename f content = f read content == new_content The file already contains right thing open filename w source_file source_file write new_content get_default_build_root - str Return path root folder under which extensions will built For each extension module built there will one folder underneath folder returned function For example ` ` p ` ` path returned function ` ` ext ` ` name extension build folder extension will ` ` p ext ` ` This directory user-specific so multiple users same machine won t meet permission issues os path realpath torch _appdirs user_cache_dir appname= torch_extensions check_compiler_ok_for_platform compiler str - bool Verify compiler expected one current platform Args compiler str The compiler executable check Returns True compiler gcc g++ Linux clang clang++ macOS always True Windows IS_WINDOWS True compiler_path = shutil which compiler compiler_path None False Use os path realpath resolve any symlinks particular c++ e g g++ compiler_path = os path realpath compiler_path Check compiler name any name compiler_path name _accepted_compilers_for_platform True If compiler wrapper used try infer actual compiler invoking -v flag env = os environ copy env LC_ALL = C Don t localize output try version_string = subprocess check_output compiler -v stderr=subprocess STDOUT env=env decode SUBPROCESS_DECODE_ARGS except subprocess CalledProcessError If -v fails try -- version version_string = subprocess check_output compiler -- version stderr=subprocess STDOUT env=env decode SUBPROCESS_DECODE_ARGS IS_LINUX Check gcc g++ sccache wrapper pattern = re compile ^COLLECT_GCC= $ re MULTILINE results = re findall pattern version_string len results = Clang also supported compiler Linux Though Ubuntu s sometimes called Ubuntu clang version clang version version_string compiler_path = os path realpath results strip On RHEL CentOS c++ gcc compiler wrapper os path basename compiler_path == c++ gcc version version_string True any name compiler_path name _accepted_compilers_for_platform IS_MACOS Check clang clang++ version_string startswith Apple clang False get_compiler_abi_compatibility_and_version compiler - tuple bool TorchVersion Determine given compiler ABI-compatible PyTorch alongside its version Args compiler str The compiler executable name check e g ` ` g++ ` ` Must executable shell process Returns A tuple contains boolean defines compiler likely ABI-incompatible PyTorch followed ` TorchVersion ` string contains compiler version separated dots _is_binary_build True TorchVersion os environ get TORCH_DONT_CHECK_COMPILER_ABI ON YES TRUE Y True TorchVersion First check compiler one expected ones particular platform check_compiler_ok_for_platform compiler logger warning WRONG_COMPILER_WARNING compiler _accepted_compilers_for_platform sys platform _accepted_compilers_for_platform False TorchVersion IS_MACOS There no particular minimum version we need clang so we re good here True TorchVersion try IS_LINUX minimum_required_version = MINIMUM_GCC_VERSION compiler_info = subprocess check_output compiler -dumpfullversion -dumpversion minimum_required_version = MINIMUM_MSVC_VERSION compiler_info = subprocess check_output compiler stderr=subprocess STDOUT match = re search r \d+ \ \d+ \ \d+ compiler_info decode SUBPROCESS_DECODE_ARGS strip version = match None list match groups except Exception _ error _ = sys exc_info logger warning Error checking compiler version s s compiler error False TorchVersion convert alphanumeric string numeric string amdclang++ returns str like git others numeric_version = re sub r \D v v version tuple map int numeric_version = minimum_required_version True TorchVersion join numeric_version compiler = f compiler join numeric_version logger warning ABI_INCOMPATIBILITY_WARNING compiler False TorchVersion join numeric_version _check_cuda_version compiler_name str compiler_version TorchVersion - None CUDA_HOME raise RuntimeError CUDA_NOT_FOUND_MESSAGE nvcc = os path join CUDA_HOME bin nvcc exe IS_WINDOWS nvcc os path exists nvcc raise FileNotFoundError f nvcc found nvcc Ensure CUDA path CUDA_HOME correct cuda_version_str = subprocess check_output nvcc -- version strip decode SUBPROCESS_DECODE_ARGS cuda_version = re search r release \d+ \d+ cuda_version_str cuda_version None cuda_str_version = cuda_version group cuda_ver = Version cuda_str_version torch version cuda None torch_cuda_version = Version torch version cuda cuda_ver = torch_cuda_version major minor attributes only available setuptools = getattr cuda_ver major None None raise ValueError setuptools = required cuda_ver major = torch_cuda_version major raise RuntimeError CUDA_MISMATCH_MESSAGE cuda_str_version torch version cuda logger warning CUDA_MISMATCH_WARN cuda_str_version torch version cuda sys platform startswith linux os environ get TORCH_DONT_CHECK_COMPILER_ABI ON YES TRUE Y _is_binary_build cuda_compiler_bounds VersionMap = CUDA_CLANG_VERSIONS compiler_name startswith clang CUDA_GCC_VERSIONS cuda_str_version cuda_compiler_bounds logger warning There no s version bounds defined CUDA version s compiler_name cuda_str_version min_compiler_version max_excl_compiler_version = cuda_compiler_bounds cuda_str_version Special case which has lower compiler bounds than V cuda_version_str cuda_compiler_bounds == CUDA_GCC_VERSIONS max_excl_compiler_version = min_compiler_version_str = join map str min_compiler_version max_excl_compiler_version_str = join map str max_excl_compiler_version version_bound_str = f = min_compiler_version_str max_excl_compiler_version_str compiler_version TorchVersion min_compiler_version_str raise RuntimeError f The current installed version compiler_name compiler_version less f than minimum required version CUDA cuda_str_version min_compiler_version_str f Please make sure use adequate version compiler_name version_bound_str compiler_version = TorchVersion max_excl_compiler_version_str raise RuntimeError f The current installed version compiler_name compiler_version greater f than maximum required version CUDA cuda_str_version f Please make sure use adequate version compiler_name version_bound_str Specify Visual Studio C runtime library hipcc _set_hipcc_runtime_lib is_standalone debug is_standalone debug COMMON_HIP_FLAGS append -fms-runtime-lib=static_dbg COMMON_HIP_FLAGS append -fms-runtime-lib=static debug COMMON_HIP_FLAGS append -fms-runtime-lib=dll_dbg COMMON_HIP_FLAGS append -fms-runtime-lib=dll _append_sycl_std_if_no_std_present cflags any flag startswith -sycl-std= flag cflags cflags append -sycl-std= _wrap_sycl_host_flags cflags host_cxx = get_cxx_compiler host_cflags = f -fsycl-host-compiler= host_cxx shlex quote f -fsycl-host-compiler-options= cflags host_cflags BuildExtension build_ext A custom mod ` setuptools ` build extension This ` setuptools build_ext ` subclass takes care passing minimum required compiler flags e g ` ` -std=c++ ` ` well mixed C++ CUDA SYCL compilation support CUDA SYCL files general When using ` BuildExtension ` allowed supply dictionary ` ` extra_compile_args ` ` rather than usual list maps languages compilers only expected values ` ` cxx ` ` ` ` nvcc ` ` ` ` sycl ` ` list additional compiler flags supply compiler This makes possible supply different flags C++ CUDA SYCL compiler during mixed compilation ` ` use_ninja ` ` bool If ` ` use_ninja ` ` ` ` True ` ` default then we attempt build using Ninja backend Ninja greatly speeds up compilation compared standard ` ` setuptools build_ext ` ` Fallbacks standard distutils backend Ninja available note By default Ninja backend uses #CPUS + workers build extension This may use up too many resources some systems One can control number workers setting ` MAX_JOBS ` environment variable non-negative number classmethod with_options cls options Return subclass alternative constructor extends any original keyword arguments original constructor given options cls_with_options cls type ignore misc valid-type __init__ args kwargs kwargs update options super __init__ args kwargs cls_with_options __init__ args kwargs - None super __init__ args kwargs no_python_abi_suffix = kwargs get no_python_abi_suffix False use_ninja = kwargs get use_ninja True use_ninja Test we can use ninja Fallback otherwise msg = Attempted use ninja BuildExtension backend s Falling back using slow distutils backend is_ninja_available logger warning msg we could find ninja use_ninja = False finalize_options - None super finalize_options use_ninja force = True build_extensions - None compiler_name compiler_version = _check_abi cuda_ext = False sycl_ext = False extension_iter = iter extensions extension = next extension_iter None while cuda_ext sycl_ext extension source extension sources _ ext = os path splitext source ext == cu cuda_ext = True ext == sycl sycl_ext = True This check accounts case when cuda sycl sources mixed same extension We can stop checking sources both found there no more sources cuda_ext sycl_ext break extension = next extension_iter None sycl_ext use_ninja raise AssertionError ninja required build sycl extensions cuda_ext IS_HIP_EXTENSION _check_cuda_version compiler_name compiler_version extension extensions Ensure least empty list flags cxx nvcc sycl when extra_compile_args dict Otherwise default torch flags do get passed Necessary when only one cxx nvcc sycl passed extra_compile_args CUDAExtension SyclExtension i e CUDAExtension extra_compile_args= cxx CUDAExtension extra_compile_args= nvcc isinstance extension extra_compile_args dict ext cxx nvcc sycl ext extension extra_compile_args extension extra_compile_args ext = _add_compile_flag extension -DTORCH_API_INCLUDE_EXTENSION_H IS_HIP_EXTENSION _hipify_compile_flags extension extension py_limited_api compile any extension has passed py_limited_api Extension constructor Py_LIMITED_API flag set our min supported CPython version See https docs python org c-api stable html#c Py_LIMITED_API _add_compile_flag extension f -DPy_LIMITED_API= min_supported_cpython _define_torch_extension_name extension nvcc_dlink extension extra_compile_args use_ninja raise AssertionError f With dlink=True ninja required build cuda extension extension name Register cu cuh hip mm sycl valid source extensions NOTE At moment sycl standard extension SYCL supported compiler Here we introduce torch level convention SYCL sources should have sycl file extension compiler src_extensions += cu cuh hip sycl torch backends mps is_built compiler src_extensions += mm Save original _compile method later compiler compiler_type == msvc compiler _cpp_extensions += cu cuh original_compile = compiler compile original_spawn = compiler spawn original_compile = compiler _compile append_std _if_no_std_present cflags - None NVCC does allow multiple -std passed so we avoid overriding option user explicitly passed cpp_format_prefix = compiler compiler_type == msvc - = cpp_flag_prefix = cpp_format_prefix format std cpp_flag = cpp_flag_prefix + c++ any flag startswith cpp_flag_prefix flag cflags cflags append cpp_flag unix_cuda_flags cflags cflags = COMMON_NVCC_FLAGS + -- compiler-options -fPIC + cflags + _get_cuda_arch_flags cflags NVCC does allow multiple -ccbin -- compiler-bindir passed so we avoid overriding option user explicitly passed _ccbin = os getenv CC _ccbin None any flag startswith -ccbin -- compiler-bindir flag cflags cflags extend -ccbin _ccbin cflags convert_to_absolute_paths_inplace paths Helper function See Note Absolute include_dirs paths None i range len paths os path isabs paths i paths i = os path abspath paths i unix_wrap_single_compile obj src ext cc_args extra_postargs pp_opts - None Copy before we make any modifications cflags = copy deepcopy extra_postargs try original_compiler = compiler compiler_so _is_cuda_file src nvcc = _join_rocm_home bin hipcc IS_HIP_EXTENSION _join_cuda_home bin nvcc compiler set_executable compiler_so nvcc isinstance cflags dict cflags = cflags nvcc IS_HIP_EXTENSION cflags = COMMON_HIPCC_FLAGS + cflags + _get_rocm_arch_flags cflags cflags = unix_cuda_flags cflags isinstance cflags dict cflags = cflags cxx IS_HIP_EXTENSION cflags = COMMON_HIP_FLAGS + cflags append_std _if_no_std_present cflags original_compile obj src ext cc_args cflags pp_opts finally Put original compiler back place compiler set_executable compiler_so original_compiler unix_wrap_ninja_compile sources output_dir=None macros=None include_dirs=None debug= extra_preargs=None extra_postargs=None depends=None r Compiles sources outputting ninja file running NB I copied some lines compiler which instance distutils UnixCCompiler See following link https github com python cpython blob f f d ad b b dbd e cc Lib distutils ccompiler py#L -L codespell ignore This can fragile lot other repos also do see https github com search q=_setup_compile type=Code so probably OK we ll also get CI signal when we update our python version which when distutils can upgraded Use absolute path output_dir so object file paths ` objects ` get generated absolute paths pyrefly ignore no-matching-overload output_dir = os path abspath output_dir See Note Absolute include_dirs convert_to_absolute_paths_inplace compiler include_dirs _ objects extra_postargs pp_opts _ = \ compiler _setup_compile output_dir macros include_dirs sources depends extra_postargs common_cflags = compiler _get_cc_args pp_opts debug extra_preargs extra_cc_cflags = compiler compiler_so with_cuda = any map _is_cuda_file sources with_sycl = any map _is_sycl_file sources extra_postargs can either - dict mapping cxx nvcc sycl extra flags - list extra flags isinstance extra_postargs dict post_cflags = extra_postargs cxx post_cflags = list extra_postargs IS_HIP_EXTENSION post_cflags = COMMON_HIP_FLAGS + post_cflags append_std _if_no_std_present post_cflags cuda_post_cflags = None cuda_cflags = None with_cuda cuda_cflags = common_cflags isinstance extra_postargs dict cuda_post_cflags = extra_postargs nvcc cuda_post_cflags = list extra_postargs IS_HIP_EXTENSION cuda_post_cflags = cuda_post_cflags + _get_rocm_arch_flags cuda_post_cflags cuda_post_cflags = COMMON_HIP_FLAGS + COMMON_HIPCC_FLAGS + cuda_post_cflags cuda_post_cflags = unix_cuda_flags cuda_post_cflags append_std _if_no_std_present cuda_post_cflags cuda_cflags = shlex quote f f cuda_cflags cuda_post_cflags = shlex quote f f cuda_post_cflags isinstance extra_postargs dict nvcc_dlink extra_postargs cuda_dlink_post_cflags = unix_cuda_flags extra_postargs nvcc_dlink cuda_dlink_post_cflags = shlex quote f f cuda_dlink_post_cflags cuda_dlink_post_cflags = None sycl_post_cflags = None sycl_cflags = None sycl_dlink_post_cflags = None with_sycl sycl_cflags = extra_cc_cflags + common_cflags + _COMMON_SYCL_FLAGS isinstance extra_postargs dict sycl_post_cflags = extra_postargs sycl sycl_post_cflags = list extra_postargs _append_sycl_targets_if_missing sycl_post_cflags append_std _if_no_std_present sycl_cflags _append_sycl_std_if_no_std_present sycl_cflags host_cflags = extra_cc_cflags + common_cflags + post_cflags append_std _if_no_std_present host_cflags escaping quoted arguments pass them thru SYCL compiler icpx_version = _get_icpx_version int icpx_version = host_cflags = item replace \\ item host_cflags host_cflags = item replace \\\\ item host_cflags host_cflags = join host_cflags Note order shlex quote sycl_flags first _wrap_sycl_host_flags second Reason sycl host flags quoted space containing strings passed SYCL compiler sycl_cflags = shlex quote f f sycl_cflags sycl_cflags += _wrap_sycl_host_flags host_cflags sycl_dlink_post_cflags = _SYCL_DLINK_FLAGS copy sycl_dlink_post_cflags += _get_sycl_device_flags sycl_post_cflags sycl_post_cflags = shlex quote f f sycl_post_cflags _write_ninja_file_and_compile_objects sources=sources objects=objects cflags= shlex quote f f extra_cc_cflags + common_cflags post_cflags= shlex quote f f post_cflags cuda_cflags=cuda_cflags cuda_post_cflags=cuda_post_cflags cuda_dlink_post_cflags=cuda_dlink_post_cflags sycl_cflags=sycl_cflags sycl_post_cflags=sycl_post_cflags sycl_dlink_post_cflags=sycl_dlink_post_cflags build_directory=output_dir verbose=True with_cuda=with_cuda with_sycl=with_sycl Return all object filenames just ones we just built objects win_cuda_flags cflags COMMON_NVCC_FLAGS + cflags + _get_cuda_arch_flags cflags win_hip_flags cflags COMMON_HIPCC_FLAGS + COMMON_HIP_FLAGS + cflags + _get_rocm_arch_flags cflags win_wrap_single_compile sources output_dir=None macros=None include_dirs=None debug= extra_preargs=None extra_postargs=None depends=None cflags = copy deepcopy extra_postargs extra_postargs = None spawn cmd Using regex match src obj include files src_regex = re compile T p &#124; c src_list = m group m src_regex match elem elem cmd m obj_regex = re compile Fo codespell ignore obj_list = m group m obj_regex match elem elem cmd m include_regex = re compile r \- &#124; \ I include_list = m group m include_regex match elem elem cmd m len src_list = len obj_list = src = src_list obj = obj_list _is_cuda_file src IS_HIP_EXTENSION nvcc = _get_hipcc_path nvcc = _join_cuda_home bin nvcc isinstance cflags dict cflags = cflags nvcc isinstance cflags list cflags = cflags cflags = IS_HIP_EXTENSION cflags = win_hip_flags cflags cflags = win_cuda_flags cflags + -std=c++ -- use-local-env ignore_warning MSVC_IGNORE_CUDAFE_WARNINGS cflags = -Xcudafe -- diag_suppress= + ignore_warning + cflags flag COMMON_MSVC_FLAGS cflags = -Xcompiler flag + cflags cmd = nvcc -c src -o obj + include_list + cflags isinstance cflags dict cflags = COMMON_MSVC_FLAGS + cflags cxx append_std _if_no_std_present cflags cmd += cflags isinstance cflags list cflags = COMMON_MSVC_FLAGS + cflags append_std _if_no_std_present cflags cmd += cflags original_spawn cmd try compiler spawn = spawn original_compile sources output_dir macros include_dirs debug extra_preargs extra_postargs depends finally compiler spawn = original_spawn win_wrap_ninja_compile sources output_dir=None macros=None include_dirs=None debug= extra_preargs=None extra_postargs=None depends=None is_standalone=False compiler initialized compiler initialize pyrefly ignore no-matching-overload output_dir = os path abspath output_dir Note Absolute include_dirs Convert relative path compiler include_dirs absolute path any For ninja build build location local instead build happens script-created build folder Thus relative paths lose their correctness To consistent jit extension we allow user enter relative include_dirs setuptools setup we convert relative path absolute path here convert_to_absolute_paths_inplace compiler include_dirs _ objects extra_postargs pp_opts _ = \ compiler _setup_compile output_dir macros include_dirs sources depends extra_postargs Replace space \ when using hipcc hipcc passes includes clang without s so clang sees space include paths new argument IS_HIP_EXTENSION pp_opts = -I format s replace \\ s startswith -I s s pp_opts common_cflags = extra_preargs cflags = debug cflags extend compiler compile_options_debug cflags extend compiler compile_options cflags = cflags + common_cflags + pp_opts + COMMON_MSVC_FLAGS IS_HIP_EXTENSION _set_hipcc_runtime_lib is_standalone debug common_cflags extend COMMON_HIP_FLAGS common_cflags extend COMMON_MSVC_FLAGS with_cuda = any map _is_cuda_file sources extra_postargs can either - dict mapping cxx nvcc extra flags - list extra flags isinstance extra_postargs dict post_cflags = extra_postargs cxx post_cflags = list extra_postargs IS_HIP_EXTENSION post_cflags = COMMON_HIP_FLAGS + post_cflags append_std _if_no_std_present post_cflags cuda_post_cflags = None cuda_cflags = None with_cuda cuda_cflags = -std=c++ common_cflag common_cflags cuda_cflags append -Xcompiler cuda_cflags append common_cflag IS_HIP_EXTENSION cuda_cflags append -- use-local-env ignore_warning MSVC_IGNORE_CUDAFE_WARNINGS cuda_cflags append -Xcudafe cuda_cflags append -- diag_suppress= + ignore_warning cuda_cflags extend pp_opts isinstance extra_postargs dict cuda_post_cflags = extra_postargs nvcc cuda_post_cflags = list extra_postargs IS_HIP_EXTENSION cuda_post_cflags = win_hip_flags cuda_post_cflags cuda_post_cflags = win_cuda_flags cuda_post_cflags cflags = _nt_quote_args cflags post_cflags = _nt_quote_args post_cflags with_cuda cuda_cflags = _nt_quote_args cuda_cflags cuda_post_cflags = _nt_quote_args cuda_post_cflags isinstance extra_postargs dict nvcc_dlink extra_postargs cuda_dlink_post_cflags = win_cuda_flags extra_postargs nvcc_dlink cuda_dlink_post_cflags = None _write_ninja_file_and_compile_objects sources=sources objects=objects cflags=cflags post_cflags=post_cflags cuda_cflags=cuda_cflags cuda_post_cflags=cuda_post_cflags cuda_dlink_post_cflags=cuda_dlink_post_cflags sycl_cflags=None sycl_post_cflags=None sycl_dlink_post_cflags=None build_directory=output_dir verbose=True with_cuda=with_cuda with_sycl=False Return all object filenames just ones we just built objects Monkey-patch _compile compile method https github com python cpython blob dc ee f b f d e d e Lib distutils ccompiler py#L codespell ignore compiler compiler_type == msvc use_ninja compiler compile = win_wrap_ninja_compile compiler compile = win_wrap_single_compile use_ninja compiler compile = unix_wrap_ninja_compile compiler _compile = unix_wrap_single_compile build_ext build_extensions get_ext_filename ext_name Get original shared library name For Python name will suffixed SOABI so where SOABI will something like cpython- m-x _ -linux-gnu ext_filename = super get_ext_filename ext_name If ` no_python_abi_suffix ` ` True ` we omit Python ABI component This makes building shared libraries setuptools aren t Python modules nicer no_python_abi_suffix The parts will e g my_extension cpython- m-x _ -linux-gnu so ext_filename_parts = ext_filename split Omit second last element without_abi = ext_filename_parts - + ext_filename_parts - ext_filename = join without_abi ext_filename _check_abi - tuple str TorchVersion On some platforms like Windows compiler_cxx available hasattr compiler compiler_cxx compiler = compiler compiler_cxx compiler = get_cxx_compiler _ version = get_compiler_abi_compatibility_and_version compiler Warn user VC env activated ` DISTUILS_USE_SDK ` set IS_WINDOWS VSCMD_ARG_TGT_ARCH os environ DISTUTILS_USE_SDK os environ msg = It seems VC environment activated DISTUTILS_USE_SDK set This may lead multiple activations VC env Please set ` DISTUTILS_USE_SDK= ` try again raise UserWarning msg compiler version _add_compile_flag extension flag extension extra_compile_args = copy deepcopy extension extra_compile_args isinstance extension extra_compile_args dict args extension extra_compile_args values args append flag extension extra_compile_args append flag Simple hipify replace first occurrence CUDA HIP flags starting - containing CUDA exclude -I flags _hipify_compile_flags extension isinstance extension extra_compile_args dict nvcc extension extra_compile_args modified_flags = flag extension extra_compile_args nvcc flag startswith - CUDA flag flag startswith -I check split flag into flag value parts = flag split = len parts == flag_part value_part = parts replace fist instance CUDA HIP only flag flag value modified_flag_part = flag_part replace CUDA HIP modified_flag = f modified_flag_part = value_part replace fist instance CUDA HIP flag modified_flag = flag replace CUDA HIP modified_flags append modified_flag logger info Modified flag s - s flag modified_flag modified_flags append flag extension extra_compile_args nvcc = modified_flags _define_torch_extension_name extension pybind doesn t support dots names so order support extensions packages like torch _C we take last part string library name names = extension name split name = names - define = f -DTORCH_EXTENSION_NAME= name _add_compile_flag extension define CppExtension name sources args kwargs Create ` setuptools Extension ` C++ Convenience method creates ` setuptools Extension ` bare minimum often sufficient arguments build C++ extension All arguments forwarded ` setuptools Extension ` constructor Full list arguments can found https setuptools pypa io en latest userguide ext_modules html#extension-api-reference warning The PyTorch python API provided libtorch_python cannot built flag ` ` py_limited_api=True ` ` When flag passed user s responsibility their library use APIs libtorch_python particular pytorch python bindings only use APIs libtorch aten objects operators dispatcher For example give access custom ops python library should register ops through dispatcher Contrary CPython setuptools who does define -DPy_LIMITED_API compile flag when py_limited_api specified option bdist_wheel command ` ` setup ` ` PyTorch does We will specify -DPy_LIMITED_API=min_supported_cpython best enforce consistency safety sanity order encourage best practices To target different version set min_supported_cpython hexcode CPython version choice Example xdoctest +SKIP xdoctest +REQUIRES env TORCH_DOCTEST_CPP_EXT setuptools setup torch utils cpp_extension BuildExtension CppExtension setup name= extension ext_modules= CppExtension name= extension sources= extension cpp extra_compile_args= -g extra_link_args= -Wl -- no-as-needed -lm cmdclass= build_ext BuildExtension include_dirs = kwargs get include_dirs include_dirs += include_paths kwargs include_dirs = include_dirs library_dirs = kwargs get library_dirs library_dirs += library_paths kwargs library_dirs = library_dirs libraries = kwargs get libraries libraries append c libraries append torch libraries append torch_cpu kwargs get py_limited_api False torch_python uses more than python limited api libraries append torch_python IS_WINDOWS libraries append sleef kwargs libraries = libraries kwargs language = c++ setuptools Extension name sources args kwargs CUDAExtension name sources args kwargs Create ` setuptools Extension ` CUDA C++ Convenience method creates ` setuptools Extension ` bare minimum often sufficient arguments build CUDA C++ extension This includes CUDA include path library path runtime library All arguments forwarded ` setuptools Extension ` constructor Full list arguments can found https setuptools pypa io en latest userguide ext_modules html#extension-api-reference warning The PyTorch python API provided libtorch_python cannot built flag ` ` py_limited_api=True ` ` When flag passed user s responsibility their library use APIs libtorch_python particular pytorch python bindings only use APIs libtorch aten objects operators dispatcher For example give access custom ops python library should register ops through dispatcher Contrary CPython setuptools who does define -DPy_LIMITED_API compile flag when py_limited_api specified option bdist_wheel command ` ` setup ` ` PyTorch does We will specify -DPy_LIMITED_API=min_supported_cpython best enforce consistency safety sanity order encourage best practices To target different version set min_supported_cpython hexcode CPython version choice Example xdoctest +SKIP xdoctest +REQUIRES env TORCH_DOCTEST_CPP_EXT setuptools setup torch utils cpp_extension BuildExtension CUDAExtension setup name= cuda_extension ext_modules= CUDAExtension name= cuda_extension sources= extension cpp extension_kernel cu extra_compile_args= cxx -g nvcc -O extra_link_args= -Wl -- no-as-needed -lcuda cmdclass= build_ext BuildExtension Compute capabilities By default extension will compiled run all archs cards visible during building process extension plus PTX If down road new card installed extension may need recompiled If visible card has compute capability CC s newer than newest version which your nvcc can build fully-compiled binaries PyTorch will make nvcc fall back building kernels newest version PTX your nvcc does support see below details PTX You can override default behavior using ` TORCH_CUDA_ARCH_LIST ` explicitly specify which CCs you want extension support ` ` TORCH_CUDA_ARCH_LIST= python build_my_extension py ` ` ` ` TORCH_CUDA_ARCH_LIST= +PTX python build_my_extension py ` ` The +PTX option causes extension kernel binaries include PTX instructions specified CC PTX intermediate representation allows kernels runtime-compile any CC = specified CC example +PTX generates PTX can runtime-compile any GPU CC = This improves your binary s forward compatibility However relying older PTX provide forward compat runtime-compiling newer CCs can modestly reduce performance those newer CCs If you know exact CC s GPUs you want target you re always better off specifying them individually For example you want your extension run +PTX would work functionally because includes PTX can runtime-compile would better Note while s possible include all supported archs more archs get included slower building process will will build separate kernel image each arch Note CUDA- nvcc will hit internal compiler error while parsing torch extension h Windows To workaround issue move python binding logic pure C++ file Example use #include ATen ATen h Tensor SigmoidAlphaBlendForwardCuda Instead #include torch extension h torch Tensor SigmoidAlphaBlendForwardCuda Currently open issue nvcc bug https github com pytorch pytorch issues Complete workaround code example https github com facebookresearch pytorch d commit cb ac f f ffe af c d f d Relocatable device code linking If you want reference device symbols across compilation units across object files object files need built ` relocatable device code ` -rdc=true -dc An exception rule dynamic parallelism nested kernel launches which used lot anymore ` Relocatable device code ` less optimized so needs used only object files need Using ` -dlto ` Device Link Time Optimization device code compilation step ` dlink ` step helps reduce protentional perf degradation ` -rdc ` Note needs used both steps useful If you have ` rdc ` objects you need have extra ` -dlink ` device linking step before CPU symbol linking step There also case where ` -dlink ` used without ` -rdc ` when extension linked against static lib containing rdc-compiled objects like NVSHMEM library https developer nvidia com nvshmem Note Ninja required build CUDA Extension RDC linking Example xdoctest +SKIP xdoctest +REQUIRES env TORCH_DOCTEST_CPP_EXT CUDAExtension name= cuda_extension sources= extension cpp extension_kernel cu dlink=True dlink_libraries= dlink_lib extra_compile_args= cxx -g nvcc -O -rdc=true library_dirs = kwargs get library_dirs library_dirs += library_paths device_type= cuda kwargs library_dirs = library_dirs libraries = kwargs get libraries libraries append c libraries append torch libraries append torch_cpu kwargs get py_limited_api False torch_python uses more than python limited api libraries append torch_python IS_HIP_EXTENSION libraries append amdhip libraries append c _hip libraries append torch_hip libraries append cudart libraries append c _cuda libraries append torch_cuda kwargs libraries = libraries include_dirs = kwargs get include_dirs IS_HIP_EXTENSION hipify hipify_python build_dir = os getcwd hipify_result = hipify_python hipify project_directory=build_dir output_directory=build_dir header_include_dirs=include_dirs includes= os path join build_dir limit scope build_dir only extra_files= os path abspath s s sources show_detailed=True is_pytorch_extension=True hipify_extra_files_only=True don t hipify everything includes path hipified_sources = set source sources s_abs = os path abspath source hipified_s_abs = hipify_result s_abs hipified_path s_abs hipify_result hipify_result s_abs hipified_path None s_abs setup arguments must always -separated paths relative setup py directory never absolute paths hipified_sources add os path relpath hipified_s_abs build_dir sources = list hipified_sources include_dirs += include_paths device_type= cuda kwargs include_dirs = include_dirs kwargs language = c++ dlink_libraries = kwargs get dlink_libraries dlink = kwargs get dlink False dlink_libraries dlink extra_compile_args = kwargs get extra_compile_args extra_compile_args_dlink = extra_compile_args get nvcc_dlink extra_compile_args_dlink += -dlink extra_compile_args_dlink += f -L x x library_dirs extra_compile_args_dlink += f -l x x dlink_libraries torch version cuda None TorchVersion torch version cuda = extra_compile_args_dlink += -dlto Device Link Time Optimization started cuda extra_compile_args nvcc_dlink = extra_compile_args_dlink kwargs extra_compile_args = extra_compile_args setuptools Extension name sources args kwargs SyclExtension name sources args kwargs r Creates ` setuptools Extension ` SYCL C++ Convenience method creates ` setuptools Extension ` bare minimum often sufficient arguments build SYCL C++ extension All arguments forwarded ` setuptools Extension ` constructor warning The PyTorch python API provided libtorch_python cannot built flag ` ` py_limited_api=True ` ` When flag passed user s responsibility their library use APIs libtorch_python particular pytorch python bindings only use APIs libtorch aten objects operators dispatcher For example give access custom ops python library should register ops through dispatcher Contrary CPython setuptools who does define -DPy_LIMITED_API compile flag when py_limited_api specified option bdist_wheel command ` ` setup ` ` PyTorch does We will specify -DPy_LIMITED_API=min_supported_cpython best enforce consistency safety sanity order encourage best practices To target different version set min_supported_cpython hexcode CPython version choice Example xdoctest +SKIP xdoctest +REQUIRES env TORCH_DOCTEST_CPP_EXT torch utils cpp_extension BuildExtension SyclExtension setup name= xpu_extension ext_modules= SyclExtension name= xpu_extension sources= extension cpp extension_kernel cpp extra_compile_args= cxx -g -std=c++ -fPIC cmdclass= build_ext BuildExtension By default extension will compiled run all archs cards visible during building process extension If down road new card installed extension may need recompiled You can override default behavior using ` TORCH_XPU_ARCH_LIST ` explicitly specify which device architectures you want extension support ` ` TORCH_XPU_ARCH_LIST= pvc xe-lpg python build_my_extension py ` ` Note while s possible include all supported archs more archs get included slower building process will will build separate kernel image each arch Note Ninja required build SyclExtension library_dirs = kwargs get library_dirs library_dirs += library_paths kwargs library_dirs = library_dirs libraries = kwargs get libraries libraries append c libraries append c _xpu libraries append torch libraries append torch_cpu kwargs get py_limited_api False torch_python uses more than python limited api libraries append torch_python libraries append torch_xpu kwargs libraries = libraries include_dirs = kwargs get include_dirs include_dirs += include_paths kwargs include_dirs = include_dirs kwargs language = c++ setuptools Extension name sources args kwargs include_paths device_type str = cpu torch_include_dirs=True - list str Get include paths required build C++ CUDA SYCL extension Args device_type Defaults cpu Returns A list include path strings paths = lib_include = os path join _TORCH_PATH include torch_include_dirs paths extend lib_include Remove once torch torch h officially no longer supported C++ extensions os path join lib_include torch csrc api include device_type == cuda IS_HIP_EXTENSION paths append os path join lib_include THH paths append _join_rocm_home include device_type == cuda cuda_home_include = _join_cuda_home include we have Debian Ubuntu packages cuda we get usr cuda home gcc doesn t like having usr include passed explicitly cuda_home_include = usr include paths append cuda_home_include Support CUDA_INC_PATH env variable supported CMake files cuda_inc_path = os environ get CUDA_INC_PATH None \ cuda_inc_path = usr include paths append cuda_inc_path CUDNN_HOME None paths append os path join CUDNN_HOME include device_type == xpu paths append _join_sycl_home include paths append _join_sycl_home include sycl paths library_paths device_type str = cpu torch_include_dirs bool = True cross_target_platform Optional str = None - list str Get library paths required build C++ CUDA extension Args device_type Defaults cpu Returns A list library path strings paths = torch_include_dirs We need link against libtorch so paths extend TORCH_LIB_PATH device_type == cuda IS_HIP_EXTENSION lib_dir = lib paths append _join_rocm_home lib_dir HIP_HOME None paths append os path join HIP_HOME lib device_type == cuda cross_target_platform == windows lib_dir = os path join lib x WINDOWS_CUDA_HOME None raise RuntimeError Need set WINDOWS_CUDA_HOME windows cross-compilation paths append os path join WINDOWS_CUDA_HOME lib_dir IS_WINDOWS lib_dir = os path join lib x lib_dir = lib os path exists _join_cuda_home lib_dir os path exists _join_cuda_home lib -bit CUDA may installed lib see e g gh- Note s also possible both don t exist see _find_cuda_home - case we stay lib lib_dir = lib paths append _join_cuda_home lib_dir CUDNN_HOME None paths append os path join CUDNN_HOME lib_dir device_type == xpu IS_WINDOWS lib_dir = os path join lib x lib_dir = lib os path exists _join_sycl_home lib_dir os path exists _join_sycl_home lib lib_dir = lib paths append _join_sycl_home lib_dir paths load name sources Union str list str extra_cflags=None extra_cuda_cflags=None extra_sycl_cflags=None extra_ldflags=None extra_include_paths=None build_directory=None verbose=False with_cuda Optional bool = None with_sycl Optional bool = None is_python_module=True is_standalone=False keep_intermediates=True Load PyTorch C++ extension just-in-time JIT To load extension Ninja build file emitted which used compile given sources into dynamic library This library subsequently loaded into current Python process module returned function ready use By default directory which build file emitted resulting library compiled ` ` tmp torch_extensions name ` ` where ` ` tmp ` ` temporary folder current platform ` ` name ` ` name extension This location can overridden two ways First ` ` TORCH_EXTENSIONS_DIR ` ` environment variable set replaces ` ` tmp torch_extensions ` ` all extensions will compiled into subfolders directory Second ` ` build_directory ` ` argument function supplied overrides entire path i e library will compiled into folder directly To compile sources default system compiler ` ` c++ ` ` used which can overridden setting ` ` CXX ` ` environment variable To pass additional arguments compilation process ` ` extra_cflags ` ` ` ` extra_ldflags ` ` can provided For example compile your extension optimizations pass ` ` extra_cflags= -O ` ` You can also use ` ` extra_cflags ` ` pass further include directories CUDA support mixed compilation provided Simply pass CUDA source files ` ` cu ` ` ` ` cuh ` ` along other sources Such files will detected compiled nvcc rather than C++ compiler This includes passing CUDA lib directory library directory linking ` ` cudart ` ` You can pass additional flags nvcc via ` ` extra_cuda_cflags ` ` just like ` ` extra_cflags ` ` C++ Various heuristics finding CUDA install directory used which usually work fine If setting ` ` CUDA_HOME ` ` environment variable safest option SYCL support mixed compilation provided Simply pass SYCL source files ` ` sycl ` ` along other sources Such files will detected compiled SYCL compiler such Intel DPC++ Compiler rather than C++ compiler You can pass additional flags SYCL compiler via ` ` extra_sycl_cflags ` ` just like ` ` extra_cflags ` ` C++ SYCL compiler expected found via system PATH environment variable Args name The name extension build This MUST same name pybind module sources A list relative absolute paths C++ source files extra_cflags optional list compiler flags forward build extra_cuda_cflags optional list compiler flags forward nvcc when building CUDA sources extra_sycl_cflags optional list compiler flags forward SYCL compiler when building SYCL sources extra_ldflags optional list linker flags forward build extra_include_paths optional list include directories forward build build_directory optional path use build workspace verbose If ` ` True ` ` turns verbose logging load steps with_cuda Determines whether CUDA headers libraries added build If set ` ` None ` ` default value automatically determined based existence ` ` cu ` ` ` ` cuh ` ` ` ` sources ` ` Set ` True ` ` force CUDA headers libraries included with_sycl Determines whether SYCL headers libraries added build If set ` ` None ` ` default value automatically determined based existence ` ` sycl ` ` ` ` sources ` ` Set ` True ` ` force SYCL headers libraries included is_python_module If ` ` True ` ` default imports produced shared library Python module If ` ` False ` ` behavior depends ` ` is_standalone ` ` is_standalone If ` ` False ` ` default loads constructed extension into process plain dynamic library If ` ` True ` ` build standalone executable Returns If ` ` is_python_module ` ` ` ` True ` ` Returns loaded PyTorch extension Python module If ` ` is_python_module ` ` ` ` False ` ` ` ` is_standalone ` ` ` ` False ` ` Returns nothing The shared library loaded into process side effect If ` ` is_standalone ` ` ` ` True ` ` Return path executable On Windows TORCH_LIB_PATH added PATH environment variable side effect Example xdoctest +SKIP torch utils cpp_extension load module = load name= extension sources= extension cpp extension_kernel cu extra_cflags= -O verbose=True _jit_compile name sources isinstance sources str sources extra_cflags extra_cuda_cflags extra_sycl_cflags extra_ldflags extra_include_paths build_directory _get_build_directory name verbose verbose with_cuda with_sycl is_python_module is_standalone keep_intermediates=keep_intermediates deprecated PyBind ABI handling internal PyBind will removed after PyTorch _get_pybind _abi_build_flags - list str check_compiler_is_gcc compiler IS_LINUX False env = os environ copy env LC_ALL = C Don t localize output try version_string = subprocess check_output compiler -v stderr=subprocess STDOUT env=env decode SUBPROCESS_DECODE_ARGS except Exception try version_string = subprocess check_output compiler -- version stderr=subprocess STDOUT env=env decode SUBPROCESS_DECODE_ARGS except Exception False Check gcc g++ sccache wrapper pattern = re compile ^COLLECT_GCC= $ re MULTILINE results = re findall pattern version_string len results = False compiler_path = os path realpath results strip On RHEL CentOS c++ gcc compiler wrapper os path basename compiler_path == c++ gcc version version_string True False _check_and_build_extension_h_precompiler_headers extra_cflags extra_include_paths is_standalone=False r Precompiled Headers PCH can pre-build same headers reduce build time pytorch load_inline modules GCC official manual https gcc gnu org onlinedocs gcc- gcc Precompiled-Headers html PCH only works when built pch file header h gch build target have same build parameters So We need add signature file record PCH file parameters If build parameters signature changed should rebuild PCH file Note Windows MacOS have different PCH mechanism We only support Linux currently It only works GCC G++ IS_LINUX compiler = get_cxx_compiler b_is_gcc = check_compiler_is_gcc compiler b_is_gcc False head_file = os path join _TORCH_PATH include torch extension h head_file_pch = os path join _TORCH_PATH include torch extension h gch head_file_signature = os path join _TORCH_PATH include torch extension h sign listToString s initialize empty string string = s None string traverse string element s string += element + string string format_precompiler_header_cmd compiler head_file head_file_pch common_cflags torch_include_dirs extra_cflags extra_include_paths re sub r \n + f compiler -x c++-header head_file -o head_file_pch torch_include_dirs extra_include_paths extra_cflags common_cflags strip command_to_signature cmd signature = cmd replace _ signature check_pch_signature_in_file file_path signature b_exist = os path isfile file_path b_exist False False open file_path file read all content file content = file read check string present file signature == content _create_if_not_exist path_dir os path exists path_dir try Path path_dir mkdir parents=True exist_ok=True except OSError exc Guard against race condition exc errno = errno EEXIST raise RuntimeError f Fail create path path_dir exc write_pch_signature_to_file file_path pch_sign _create_if_not_exist os path dirname file_path open file_path w f f write pch_sign f close build_precompile_header pch_cmd try subprocess check_output pch_cmd shell=True stderr=subprocess STDOUT except subprocess CalledProcessError e raise RuntimeError f Compile PreCompile Header fail command pch_cmd e extra_cflags_str = listToString extra_cflags extra_include_paths_str = join f -I include include extra_include_paths extra_include_paths lib_include = os path join _TORCH_PATH include torch_include_dirs = f -I lib_include Python h -I format sysconfig get_path include torch all h -I format os path join lib_include torch csrc api include torch_include_dirs_str = listToString torch_include_dirs common_cflags = is_standalone common_cflags += -DTORCH_API_INCLUDE_EXTENSION_H common_cflags += -std=c++ -fPIC common_cflags_str = listToString common_cflags pch_cmd = format_precompiler_header_cmd compiler head_file head_file_pch common_cflags_str torch_include_dirs_str extra_cflags_str extra_include_paths_str pch_sign = command_to_signature pch_cmd os path isfile head_file_pch True build_precompile_header pch_cmd write_pch_signature_to_file head_file_signature pch_sign b_same_sign = check_pch_signature_in_file head_file_signature pch_sign b_same_sign False build_precompile_header pch_cmd write_pch_signature_to_file head_file_signature pch_sign remove_extension_h_precompiler_headers _remove_if_file_exists path_file os path exists path_file os remove path_file head_file_pch = os path join _TORCH_PATH include torch extension h gch head_file_signature = os path join _TORCH_PATH include torch extension h sign _remove_if_file_exists head_file_pch _remove_if_file_exists head_file_signature load_inline name cpp_sources cuda_sources=None sycl_sources=None functions=None extra_cflags=None extra_cuda_cflags=None extra_sycl_cflags=None extra_ldflags=None extra_include_paths=None build_directory=None verbose=False with_cuda=None with_sycl=None is_python_module=True with_pytorch_error_handling=True keep_intermediates=True use_pch=False no_implicit_headers=False r Load PyTorch C++ extension just-in-time JIT string sources This function behaves exactly like func ` load ` takes its sources strings rather than filenames These strings stored files build directory after which behavior func ` load_inline ` identical func ` load ` See ` tests https github com pytorch pytorch blob master test test_cpp_extensions_jit py ` _ good examples using function Sources may omit two required parts typical non-inline C++ extension necessary header includes well pybind binding code More precisely strings passed ` ` cpp_sources ` ` first concatenated into single ` ` cpp ` ` file This file then prepended ` ` #include torch extension h ` ` Furthermore ` ` functions ` ` argument supplied bindings will automatically generated each function specified ` ` functions ` ` can either list function names dictionary mapping function names docstrings If list given name each function used its docstring The sources ` ` cuda_sources ` ` concatenated into separate ` ` cu ` ` file prepended ` ` torch types h ` ` ` ` cuda h ` ` ` ` cuda_runtime h ` ` includes The ` ` cpp ` ` ` ` cu ` ` files compiled separately ultimately linked into single library Note no bindings generated functions ` ` cuda_sources ` ` per se To bind CUDA kernel you must create C++ function calls either declare define C++ function one ` ` cpp_sources ` ` include its name ` ` functions ` ` The sources ` ` sycl_sources ` ` concatenated into separate ` ` sycl ` ` file prepended ` ` torch types h ` ` ` ` sycl sycl hpp ` ` includes The ` ` cpp ` ` ` ` sycl ` ` files compiled separately ultimately linked into single library Note no bindings generated functions ` ` sycl_sources ` ` per se To bind SYCL kernel you must create C++ function calls either declare define C++ function one ` ` cpp_sources ` ` include its name ` ` functions ` ` See func ` load ` description arguments omitted below Args cpp_sources A string list strings containing C++ source code cuda_sources A string list strings containing CUDA source code sycl_sources A string list strings containing SYCL source code functions A list function names which generate function bindings If dictionary given should map function names docstrings which otherwise just function names with_cuda Determines whether CUDA headers libraries added build If set ` ` None ` ` default value automatically determined based whether ` ` cuda_sources ` ` provided Set ` ` True ` ` force CUDA headers libraries included with_sycl Determines whether SYCL headers libraries added build If set ` ` None ` ` default value automatically determined based whether ` ` sycl_sources ` ` provided Set ` ` True ` ` force SYCL headers libraries included with_pytorch_error_handling Determines whether pytorch error warning macros handled pytorch instead pybind To do each function ` ` foo ` ` called via intermediary ` ` _safe_foo ` ` function This redirection might cause issues obscure cases cpp This flag should set ` ` False ` ` when redirect causes issues no_implicit_headers If ` ` True ` ` skips automatically adding headers most notably ` ` #include torch extension h ` ` ` ` #include torch types h ` ` lines Use option improve cold start times when you already include necessary headers your source code Default ` ` False ` ` Example xdoctest +REQUIRES env TORCH_DOCTEST_CPP_EXT torch utils cpp_extension load_inline source = Tensor sin_add Tensor x Tensor y x sin + y sin module = load_inline name= inline_extension cpp_sources= source functions= sin_add note Since load_inline will just-in-time compile source code please ensure you have right toolchains installed runtime For example when loading C++ make sure C++ compiler available If you re loading CUDA extension you will need additionally install corresponding CUDA toolkit nvcc any other dependencies your code has Compiling toolchains included when you install torch must additionally installed During compiling default Ninja backend uses #CPUS + workers build extension This may use up too many resources some systems One can control number workers setting ` MAX_JOBS ` environment variable non-negative number build_directory = build_directory _get_build_directory name verbose isinstance cpp_sources str cpp_sources = cpp_sources cuda_sources = cuda_sources isinstance cuda_sources str cuda_sources = cuda_sources sycl_sources = sycl_sources isinstance sycl_sources str sycl_sources = sycl_sources no_implicit_headers cpp_sources insert #include torch extension h use_pch True Using PreCompile Header torch extension h reduce compile time _check_and_build_extension_h_precompiler_headers extra_cflags extra_include_paths remove_extension_h_precompiler_headers If ` functions ` supplied we create pybind bindings user Here ` functions ` becomes after some processing map function names function docstrings functions None module_def = module_def append PYBIND _MODULE TORCH_EXTENSION_NAME m isinstance functions str functions = functions isinstance functions list Make function docstring same function name functions = f f f functions isinstance functions dict raise ValueError f Expected functions list dict type functions function_name docstring functions items with_pytorch_error_handling module_def append f m function_name torch wrap_pybind_function function_name docstring module_def append f m function_name function_name docstring module_def append cpp_sources += module_def cpp_source_path = os path join build_directory main cpp _maybe_write cpp_source_path \n join cpp_sources sources = cpp_source_path cuda_sources no_implicit_headers cuda_sources insert #include torch types h cuda_sources insert #include cuda h cuda_sources insert #include cuda_runtime h cuda_source_path = os path join build_directory cuda cu _maybe_write cuda_source_path \n join cuda_sources sources append cuda_source_path sycl_sources no_implicit_headers sycl_sources insert #include torch types h sycl_sources insert #include sycl sycl hpp sycl_source_path = os path join build_directory sycl sycl _maybe_write sycl_source_path \n join sycl_sources sources append sycl_source_path _jit_compile name sources extra_cflags extra_cuda_cflags extra_sycl_cflags extra_ldflags extra_include_paths build_directory verbose with_cuda with_sycl is_python_module is_standalone=False keep_intermediates=keep_intermediates _jit_compile name sources extra_cflags extra_cuda_cflags extra_sycl_cflags extra_ldflags extra_include_paths build_directory str verbose bool with_cuda Optional bool with_sycl Optional bool is_python_module is_standalone keep_intermediates=True - Union types ModuleType str is_python_module is_standalone raise ValueError ` is_python_module ` ` is_standalone ` mutually exclusive with_cuda None with_cuda = any map _is_cuda_file sources with_cudnn = any cudnn f f extra_ldflags with_sycl None with_sycl = any map _is_sycl_file sources old_version = JIT_EXTENSION_VERSIONER get_version name version = JIT_EXTENSION_VERSIONER bump_version_if_changed name sources build_arguments= extra_cflags extra_cuda_cflags extra_ldflags extra_include_paths build_directory=build_directory with_cuda=with_cuda with_sycl=with_sycl is_python_module=is_python_module is_standalone=is_standalone version version = old_version verbose logger info The input conditions extension module s have changed name logger info Bumping version s re-building s_v s version name version name = f name _v version baton = FileBaton os path join build_directory lock baton try_acquire try version = old_version hipify hipify_python hipify hipify_python GeneratedFileCleaner GeneratedFileCleaner keep_intermediates=keep_intermediates clean_ctx IS_HIP_EXTENSION with_cuda with_cudnn hipify_result = hipify_python hipify project_directory=build_directory output_directory=build_directory header_include_dirs= extra_include_paths extra_include_paths None extra_files= os path abspath s s sources ignores= _join_rocm_home os path join _TORCH_PATH no need hipify ROCm PyTorch headers show_detailed=verbose show_progress=verbose is_pytorch_extension=True clean_ctx=clean_ctx hipified_sources = set source sources s_abs = os path abspath source hipified_sources add hipify_result s_abs hipified_path s_abs hipify_result s_abs sources = list hipified_sources _write_ninja_file_and_build_library name=name sources=sources extra_cflags=extra_cflags extra_cuda_cflags=extra_cuda_cflags extra_sycl_cflags=extra_sycl_cflags extra_ldflags=extra_ldflags extra_include_paths=extra_include_paths build_directory=build_directory verbose=verbose with_cuda=with_cuda with_sycl=with_sycl is_standalone=is_standalone verbose logger debug No modifications detected re-loaded extension module s skipping build step name finally baton release baton wait verbose logger info Loading extension module s name is_standalone _get_exec_path name build_directory _import_module_from_library name build_directory is_python_module _get_hipcc_path IS_WINDOWS mypy thinks ROCM_VERSION None will never None here hipcc_exe = hipcc exe ROCM_VERSION = hipcc bat type ignore operator _join_rocm_home bin hipcc_exe _join_rocm_home bin hipcc _write_ninja_file_and_compile_objects sources list str objects cflags post_cflags cuda_cflags cuda_post_cflags cuda_dlink_post_cflags sycl_cflags sycl_post_cflags sycl_dlink_post_cflags build_directory str verbose bool with_cuda Optional bool with_sycl Optional bool - None verify_ninja_availability compiler = get_cxx_compiler get_compiler_abi_compatibility_and_version compiler with_cuda None with_cuda = any map _is_cuda_file sources with_sycl None with_sycl = any map _is_sycl_file sources build_file_path = os path join build_directory build ninja verbose logger debug Emitting ninja build file s build_file_path Create build_directory does exist os path exists build_directory verbose logger debug Creating directory s build_directory This like mkdir -p i e will also create parent directories os makedirs build_directory exist_ok=True _write_ninja_file path=build_file_path cflags=cflags post_cflags=post_cflags cuda_cflags=cuda_cflags cuda_post_cflags=cuda_post_cflags cuda_dlink_post_cflags=cuda_dlink_post_cflags sycl_cflags=sycl_cflags sycl_post_cflags=sycl_post_cflags sycl_dlink_post_cflags=sycl_dlink_post_cflags sources=sources objects=objects ldflags=None library_target=None with_cuda=with_cuda with_sycl=with_sycl verbose logger info Compiling objects _run_ninja_build build_directory verbose It would better we could tell users name extension failed build there isn t good way get here error_prefix= Error compiling objects extension _write_ninja_file_and_build_library name sources list str extra_cflags extra_cuda_cflags extra_sycl_cflags extra_ldflags extra_include_paths build_directory str verbose bool with_cuda Optional bool with_sycl Optional bool is_standalone bool = False - None verify_ninja_availability compiler = get_cxx_compiler get_compiler_abi_compatibility_and_version compiler with_cuda None with_cuda = any map _is_cuda_file sources with_sycl None with_sycl = any map _is_sycl_file sources extra_ldflags = _prepare_ldflags extra_ldflags with_cuda verbose is_standalone build_file_path = os path join build_directory build ninja verbose logger debug Emitting ninja build file s build_file_path Create build_directory does exist os path exists build_directory verbose logger debug Creating directory s build_directory This like mkdir -p i e will also create parent directories os makedirs build_directory exist_ok=True NOTE Emitting new ninja build file does cause re-compilation sources did change so s ok re-emit s fast _write_ninja_file_to_build_library path=build_file_path name=name sources=sources extra_cflags=extra_cflags extra_cuda_cflags=extra_cuda_cflags extra_sycl_cflags=extra_sycl_cflags extra_ldflags=extra_ldflags extra_include_paths=extra_include_paths with_cuda=with_cuda with_sycl=with_sycl is_standalone=is_standalone verbose logger info Building extension module s name _run_ninja_build build_directory verbose error_prefix=f Error building extension name is_ninja_available Return ` ` True ` ` ` ninja https ninja-build org ` _ build system available system ` ` False ` ` otherwise try subprocess check_output ninja -- version except Exception False True verify_ninja_availability Raise ` ` RuntimeError ` ` ` ninja https ninja-build org ` _ build system available system does nothing otherwise is_ninja_available raise RuntimeError Ninja required load C++ extensions pip install ninja get _prepare_ldflags extra_ldflags with_cuda verbose is_standalone IS_WINDOWS python_lib_path = os path join sys base_exec_prefix libs extra_ldflags append c lib with_cuda extra_ldflags append c _hip lib IS_HIP_EXTENSION c _cuda lib extra_ldflags append torch_cpu lib with_cuda extra_ldflags append torch_hip lib IS_HIP_EXTENSION torch_cuda lib INCLUDE used ensure torch_cuda linked against project relies Related issue https github com pytorch pytorch issues extra_ldflags append -INCLUDE warp_size cuda YAHXZ extra_ldflags append torch lib extra_ldflags append f LIBPATH TORCH_LIB_PATH is_standalone extra_ldflags append torch_python lib extra_ldflags append f LIBPATH python_lib_path extra_ldflags append f -L TORCH_LIB_PATH extra_ldflags append -lc with_cuda extra_ldflags append -lc _hip IS_HIP_EXTENSION -lc _cuda extra_ldflags append -ltorch_cpu with_cuda extra_ldflags append -ltorch_hip IS_HIP_EXTENSION -ltorch_cuda extra_ldflags append -ltorch is_standalone extra_ldflags append -ltorch_python is_standalone extra_ldflags append f -Wl -rpath TORCH_LIB_PATH with_cuda verbose logger info Detected CUDA files patching ldflags IS_WINDOWS IS_HIP_EXTENSION extra_ldflags append f LIBPATH _join_cuda_home lib x extra_ldflags append cudart lib CUDNN_HOME None extra_ldflags append f LIBPATH os path join CUDNN_HOME lib x IS_HIP_EXTENSION extra_lib_dir = lib os path exists _join_cuda_home extra_lib_dir os path exists _join_cuda_home lib -bit CUDA may installed lib Note s also possible both don t exist see _find_cuda_home - case we stay lib extra_lib_dir = lib extra_ldflags append f -L _join_cuda_home extra_lib_dir extra_ldflags append -lcudart CUDNN_HOME None extra_ldflags append f -L os path join CUDNN_HOME lib IS_HIP_EXTENSION IS_WINDOWS extra_ldflags append f LIBPATH _join_rocm_home lib extra_ldflags append amdhip lib extra_ldflags append f -L _join_rocm_home lib extra_ldflags append -lamdhip extra_ldflags _get_cuda_arch_flags cflags Optional list str = None - list str Determine CUDA arch flags use For arch say added compile flag will ` ` -gencode=arch=compute_ code=sm_ ` ` For added +PTX additional ` ` -gencode=arch=compute_xx code=compute_xx ` ` added See select_compute_arch cmake corresponding named supported arches when building CMake If cflags given there may already user-provided arch flags ` extra_compile_args ` cflags None flag cflags TORCH_EXTENSION_NAME flag continue arch flag Note keep combined names arch +arch above single names otherwise string replacement may do right thing named_arches = collections OrderedDict Kepler+Tesla Kepler +PTX Maxwell+Tegra Maxwell +PTX Pascal +PTX Volta+Tegra Volta +PTX Turing +PTX Ampere+Tegra Ampere +PTX Ada +PTX Hopper +PTX Blackwell+Tegra Blackwell +PTX supported_arches = valid_arch_strings = supported_arches + s + +PTX s supported_arches The default sm_ CUDA x x First check env var same used main setup py Can one more architectures e g +PTX See cmake Modules_CUDA_fix upstream FindCUDA select_compute_arch cmake _arch_list = os environ get TORCH_CUDA_ARCH_LIST None If given set native determine what s best GPU CUDA version can found _arch_list _arch_list == native arch_list = assumption extension should run any currently visible cards which could different types - therefore all archs visible cards should included i range torch cuda device_count capability = torch cuda get_device_capability i supported_sm = int join re findall r \d+ arch split _ arch torch cuda get_arch_list sm_ arch max_supported_sm = max sm sm sm supported_sm Capability device may higher than what s supported user s NVCC causing compilation error User s NVCC expected match one used build pytorch so we use maximum supported capability pytorch clamp capability capability = min max_supported_sm capability arch = f capability capability arch arch_list arch_list append arch arch_list = sorted arch_list arch_list - += +PTX _arch_list Only log rank distributed settings avoid spam torch distributed is_available torch distributed is_initialized torch distributed get_rank == arch_list_str = join arch_list logger debug TORCH_CUDA_ARCH_LIST set using TORCH_CUDA_ARCH_LIST= s visible GPU architectures Set os environ TORCH_CUDA_ARCH_LIST override arch_list_str Deal lists separated only deal after _arch_list = _arch_list replace Expand named arches named_arch archival named_arches items _arch_list = _arch_list replace named_arch archival arch_list = _arch_list split flags = arch arch_list arch valid_arch_strings raise ValueError f Unknown CUDA arch arch GPU supported Handle both single double-digit architecture versions version = arch split + Remove +PTX present major minor = version split num = f major minor flags append f -gencode=arch=compute_ num code=sm_ num arch endswith +PTX flags append f -gencode=arch=compute_ num code=compute_ num sorted set flags _get_rocm_arch_flags cflags Optional list str = None - list str If cflags given there may already user-provided arch flags ` extra_compile_args ` If user also specified -fgpu-rdc -fno-gpu-rdc we assume they know what they re doing Otherwise we force -fno-gpu-rdc default has_gpu_rdc_flag = False cflags None has_custom_flags = False flag cflags amdgpu-target flag offload-arch flag has_custom_flags = True gpu-rdc flag has_gpu_rdc_flag = True has_custom_flags has_gpu_rdc_flag -fno-gpu-rdc Use same defaults used building PyTorch Allow env var override just like during initial cmake build _archs = os environ get PYTORCH_ROCM_ARCH None _archs archFlags = torch _C _cuda_getArchFlags archFlags archs = archFlags split archs = archs = _archs replace split flags = f -- offload-arch= arch arch archs flags += has_gpu_rdc_flag -fno-gpu-rdc flags _get_build_directory name str verbose bool - str Get build directory extension Args name The name extension verbose Whether print verbose information Returns The path build directory root_extensions_directory = os environ get TORCH_EXTENSIONS_DIR root_extensions_directory None root_extensions_directory = get_default_build_root cu_str = cpu torch version cuda None f cu torch version cuda replace python_version = f py sys version_info major sys version_info minor getattr sys abiflags build_folder = f python_version _ cu_str root_extensions_directory = os path join root_extensions_directory build_folder verbose logger info Using s PyTorch extensions root root_extensions_directory build_directory = os path join root_extensions_directory name os path exists build_directory verbose logger debug Creating extension directory s build_directory This like mkdir -p i e will also create parent directories os makedirs build_directory exist_ok=True build_directory _get_num_workers verbose bool - Optional int max_jobs = os environ get MAX_JOBS max_jobs None max_jobs isdigit verbose logger debug Using envvar MAX_JOBS s number workers max_jobs int max_jobs verbose logger info Allowing ninja set default number workers overridable setting environment variable MAX_JOBS=N None _get_vc_env vc_arch str - dict str str try setuptools distutils type ignore attr-defined pyrefly ignore missing-attribute distutils _msvccompiler _get_vc_env vc_arch except AttributeError try setuptools _distutils _msvccompiler _msvccompiler _get_vc_env vc_arch type ignore attr-defined except AttributeError setuptools _distutils compilers C msvc msvc _get_vc_env vc_arch type ignore attr-defined _run_ninja_build build_directory str verbose bool error_prefix str - None command = ninja -v num_workers = _get_num_workers verbose num_workers None command extend -j str num_workers env = os environ copy Try activate vc env users IS_WINDOWS VSCMD_ARG_TGT_ARCH env setuptools distutils type ignore attr-defined plat_name = distutils util get_platform plat_spec = PLAT_TO_VCVARS plat_name vc_env = k upper v k v _get_vc_env plat_spec items k v env items uk = k upper uk vc_env vc_env uk = v env = vc_env try sys stdout flush sys stderr flush Warning don t pass stdout=None subprocess run get output subprocess run assumes sys __stdout__ has been modified attempts write default However when we call _run_ninja_build ahead-of-time cpp extensions following happens If stdout encoding utf- setuptools detaches __stdout__ https github com pypa setuptools blob e fafabe b bbc bb setuptools dist py#L probably shouldn t do subprocess run POSIX no stdout override relies __stdout__ being detached https github com python cpython blob c e c c b f db b Lib subprocess py#L To work around we pass fileno directly hope valid stdout_fileno = subprocess run command shell=IS_WINDOWS IS_HIP_EXTENSION stdout=stdout_fileno verbose subprocess PIPE stderr=subprocess STDOUT cwd=build_directory check=True env=env except subprocess CalledProcessError e Python compatible way getting error object _ error _ = sys exc_info error output contains stdout stderr build attempt message = error_prefix ` error ` CalledProcessError which has ` output ` attribute mypy thinks s Optional BaseException doesn t narrow hasattr error output error output type ignore union-attr message += f error output decode SUBPROCESS_DECODE_ARGS type ignore union-attr raise RuntimeError message e _get_exec_path module_name path IS_WINDOWS TORCH_LIB_PATH os getenv PATH split torch_lib_in_path = any os path exists p os path samefile p TORCH_LIB_PATH p os getenv PATH split torch_lib_in_path os environ PATH = f TORCH_LIB_PATH os getenv PATH os path join path f module_name EXEC_EXT _import_module_from_library module_name path is_python_module filepath = os path join path f module_name LIB_EXT is_python_module https stackoverflow com questions how-to-import-a-module-given-the-full-path spec = importlib util spec_from_file_location module_name filepath spec None raise AssertionError f Failed create spec module module_name filepath module = importlib util module_from_spec spec isinstance spec loader importlib abc Loader raise AssertionError spec loader valid importlib Loader spec loader exec_module module module torch ops load_library filepath filepath _write_ninja_file_to_build_library path name sources extra_cflags extra_cuda_cflags extra_sycl_cflags extra_ldflags extra_include_paths with_cuda with_sycl is_standalone - None extra_cflags = flag strip flag extra_cflags extra_cuda_cflags = flag strip flag extra_cuda_cflags extra_sycl_cflags = flag strip flag extra_sycl_cflags extra_ldflags = flag strip flag extra_ldflags extra_include_paths = flag strip flag extra_include_paths Turn into absolute paths so we can emit them into ninja build file wherever user_includes = os path abspath file file extra_include_paths include_paths gives us location torch extension h TODO generalize with_cuda specific device type with_cuda system_includes = include_paths cuda system_includes = include_paths cpu sysconfig get_path include gives us location Python h Explicitly specify posix_prefix scheme non-Windows platforms workaround error some MacOS installations where default ` get_path ` points non-existing ` Library Python M m include ` folder python_include_path = sysconfig get_path include scheme= nt IS_WINDOWS posix_prefix python_include_path None system_includes append python_include_path common_cflags = is_standalone common_cflags append f -DTORCH_EXTENSION_NAME= name common_cflags append -DTORCH_API_INCLUDE_EXTENSION_H Windows does understand ` -isystem ` quotes flags later IS_WINDOWS common_cflags += f -I include include user_includes + system_includes common_cflags += f -I shlex quote include include user_includes common_cflags += f -isystem shlex quote include include system_includes IS_WINDOWS COMMON_HIP_FLAGS extend -fms-runtime-lib=dll cflags = common_cflags + std c++ + extra_cflags cflags += COMMON_MSVC_FLAGS + COMMON_HIP_FLAGS IS_HIP_EXTENSION cflags = _nt_quote_args cflags cflags = common_cflags + -fPIC -std=c++ + extra_cflags with_cuda IS_HIP_EXTENSION cuda_flags = -DWITH_HIP + common_cflags + extra_cflags + COMMON_HIP_FLAGS + COMMON_HIPCC_FLAGS cuda_flags = cuda_flags + -std=c++ cuda_flags += _get_rocm_arch_flags cuda_flags cuda_flags += extra_cuda_cflags IS_WINDOWS cuda_flags = _nt_quote_args cuda_flags with_cuda cuda_flags = common_cflags + COMMON_NVCC_FLAGS + _get_cuda_arch_flags extra_cuda_cflags IS_WINDOWS flag COMMON_MSVC_FLAGS cuda_flags = -Xcompiler flag + cuda_flags ignore_warning MSVC_IGNORE_CUDAFE_WARNINGS cuda_flags = -Xcudafe -- diag_suppress= + ignore_warning + cuda_flags cuda_flags = cuda_flags + -std=c++ cuda_flags = _nt_quote_args cuda_flags cuda_flags += _nt_quote_args extra_cuda_cflags cuda_flags += -- compiler-options -fPIC cuda_flags += extra_cuda_cflags any flag startswith -std= flag cuda_flags cuda_flags append -std=c++ cc_env = os getenv CC cc_env None cuda_flags = -ccbin cc_env + cuda_flags cuda_flags = None with_sycl sycl_cflags = cflags + _COMMON_SYCL_FLAGS sycl_cflags += extra_sycl_cflags _append_sycl_targets_if_missing sycl_cflags _append_sycl_std_if_no_std_present sycl_cflags host_cflags = cflags escaping quoted arguments pass them thru SYCL compiler icpx_version = _get_icpx_version int icpx_version host_cflags = item replace \\ \\\\ item host_cflags host_cflags = join host_cflags sycl_cflags += _wrap_sycl_host_flags host_cflags sycl_dlink_post_cflags = _SYCL_DLINK_FLAGS copy sycl_dlink_post_cflags += _get_sycl_device_flags sycl_cflags sycl_cflags = None sycl_dlink_post_cflags = None object_file_path source_file str - str path file cpp - file file_name = os path splitext os path basename source_file _is_cuda_file source_file with_cuda Use different object filename case C++ CUDA file have same filename different extension cpp vs cu target = f file_name cuda o _is_sycl_file source_file with_sycl target = f file_name sycl o target = f file_name o target objects = object_file_path src src sources ldflags = is_standalone SHARED_FLAG + extra_ldflags The darwin linker needs explicit consent ignore unresolved symbols IS_MACOS ldflags append -undefined dynamic_lookup IS_WINDOWS ldflags = _nt_quote_args ldflags ext = EXEC_EXT is_standalone LIB_EXT library_target = f name ext _write_ninja_file path=path cflags=cflags post_cflags=None cuda_cflags=cuda_flags cuda_post_cflags=None cuda_dlink_post_cflags=None sycl_cflags=sycl_cflags sycl_post_cflags= sycl_dlink_post_cflags=sycl_dlink_post_cflags sources=sources objects=objects ldflags=ldflags library_target=library_target with_cuda=with_cuda with_sycl=with_sycl _write_ninja_file path cflags post_cflags cuda_cflags cuda_post_cflags cuda_dlink_post_cflags sycl_cflags sycl_post_cflags sycl_dlink_post_cflags sources objects ldflags library_target with_cuda with_sycl - None r Write ninja file does desired compiling linking ` path ` Where write file ` cflags ` list flags pass $ cxx Can None ` post_cflags ` list flags append $ cxx invocation Can None ` cuda_cflags ` list flags pass $ nvcc Can None ` cuda_post_cflags ` list flags append $ nvcc invocation Can None ` cuda_dlink_post_cflags ` list flags append $ nvcc device code link invocation Can None ` sycl_cflags ` list flags pass SYCL compiler Can None ` sycl_post_cflags ` list flags append SYCL compiler invocation Can None ` sycl_dlink_post_cflags ` list flags append SYCL compiler device code link invocation Can None e ` sources ` list paths source files ` objects ` list desired paths objects one per source ` ldflags ` list flags pass linker Can None ` library_target ` Name output library Can None case we do no linking ` with_cuda ` If we should compiling CUDA sanitize_flags flags flags None flag strip flag flags cflags = sanitize_flags cflags post_cflags = sanitize_flags post_cflags cuda_cflags = sanitize_flags cuda_cflags cuda_post_cflags = sanitize_flags cuda_post_cflags cuda_dlink_post_cflags = sanitize_flags cuda_dlink_post_cflags sycl_cflags = sanitize_flags sycl_cflags sycl_post_cflags = sanitize_flags sycl_post_cflags sycl_dlink_post_cflags = sanitize_flags sycl_dlink_post_cflags ldflags = sanitize_flags ldflags Sanity checks len sources = len objects raise AssertionError sources objects lists must same length len sources == raise AssertionError At least one source required build library compiler = get_cxx_compiler Version required ` deps ` directive config = ninja_required_version = config append f cxx = compiler with_cuda cuda_dlink_post_cflags PYTORCH_NVCC os environ nvcc = os getenv PYTORCH_NVCC user can set nvcc compiler ccache using environment variable here IS_HIP_EXTENSION nvcc = _get_hipcc_path nvcc = _join_cuda_home bin nvcc config append f nvcc = nvcc with_sycl sycl_dlink_post_cflags sycl = icx IS_WINDOWS icpx config append f sycl = sycl IS_HIP_EXTENSION post_cflags = COMMON_HIP_FLAGS + post_cflags flags = f cflags = join cflags flags append f post_cflags = join post_cflags with_cuda flags append f cuda_cflags = join cuda_cflags flags append f cuda_post_cflags = join cuda_post_cflags flags append f cuda_dlink_post_cflags = join cuda_dlink_post_cflags with_sycl flags append f sycl_cflags = join sycl_cflags flags append f sycl_post_cflags = join sycl_post_cflags flags append f sycl_dlink_post_cflags = join sycl_dlink_post_cflags flags append f ldflags = join ldflags Turn into absolute paths so we can emit them into ninja build file wherever sources = os path abspath file file sources See https ninja-build org build ninja html reference compile_rule = rule compile IS_WINDOWS compiler_name = $ cxx IS_HIP_EXTENSION cl compile_rule append f command = compiler_name showIncludes $ cflags -c $ Fo$ out $ post_cflags codespell ignore IS_HIP_EXTENSION compile_rule append deps = msvc compile_rule append command = $ cxx -MMD -MF $ out d $ cflags -c $ -o $ out $ post_cflags compile_rule append depfile = $ out d compile_rule append deps = gcc with_cuda cuda_compile_rule = rule cuda_compile nvcc_gendeps = -- generate-dependencies-with-compile supported ROCm Nvcc flag ` -- generate-dependencies-with-compile ` supported sccache which may increase build time torch version cuda None os getenv TORCH_EXTENSION_SKIP_NVCC_GEN_DEPENDENCIES = cuda_compile_rule append depfile = $ out d cuda_compile_rule append deps = gcc Note non-system deps nvcc only supported Linux so use -- generate-dependencies-with-compile make work Windows too nvcc_gendeps = -- generate-dependencies-with-compile -- dependency-output $ out d cuda_compile_rule append f command = $ nvcc nvcc_gendeps $ cuda_cflags -c $ -o $ out $ cuda_post_cflags with_sycl sycl_compile_rule = rule sycl_compile SYCL compiler does recognize sycl extension automatically so we pass -x c++ explicitly notifying compiler file format sycl_compile_rule append command = $ sycl $ sycl_cflags -c -x c++ $ -o $ out $ sycl_post_cflags Emit one build rule per source enable incremental build build = source_file object_file zip sources objects strict=True is_cuda_source = _is_cuda_file source_file with_cuda is_sycl_source = _is_sycl_file source_file with_sycl is_cuda_source rule = cuda_compile is_sycl_source rule = sycl_compile rule = compile IS_WINDOWS source_file = source_file replace $ object_file = object_file replace $ source_file = source_file replace $ object_file = object_file replace $ build append f build object_file rule source_file cuda_dlink_post_cflags cuda_devlink_out = os path join os path dirname objects dlink o cuda_devlink_rule = rule cuda_devlink cuda_devlink_rule append command = $ nvcc $ -o $ out $ cuda_dlink_post_cflags cuda_devlink = f build cuda_devlink_out cuda_devlink join objects objects += cuda_devlink_out cuda_devlink_rule cuda_devlink = sycl_dlink_post_cflags sycl_devlink_out = os path join os path dirname objects sycl_dlink o sycl_devlink_rule = rule sycl_devlink sycl_devlink_rule append command = $ sycl $ -o $ out $ sycl_dlink_post_cflags sycl_devlink = f build sycl_devlink_out sycl_devlink join objects objects += sycl_devlink_out sycl_devlink_rule sycl_devlink = library_target None link_rule = rule link IS_WINDOWS cl_paths = subprocess check_output where cl decode SUBPROCESS_DECODE_ARGS split \r\n len cl_paths = cl_path = os path dirname cl_paths replace $ raise RuntimeError MSVC required load C++ extensions link_rule append f command = cl_path link exe $ nologo $ ldflags out $ out link_rule append command = $ cxx $ $ ldflags -o $ out link = f build library_target link join objects default = f default library_target link_rule link default = Blocks should separated newlines visual benefit blocks = config flags compile_rule with_cuda blocks append cuda_compile_rule type ignore possibly-undefined with_sycl blocks append sycl_compile_rule type ignore possibly-undefined blocks += cuda_devlink_rule sycl_devlink_rule link_rule build cuda_devlink sycl_devlink link default content = \n\n join \n join b b blocks Ninja requires new lines end ninja file content += \n _maybe_write path content _join_cuda_home paths - str Join paths CUDA_HOME raises error CUDA_HOME set This basically lazy way raising error missing $ CUDA_HOME only once we need get any CUDA-specific path CUDA_HOME None raise OSError CUDA_HOME environment variable set Please set your CUDA install root os path join CUDA_HOME paths _is_cuda_file path str - bool valid_ext = cu cuh IS_HIP_EXTENSION valid_ext append hip os path splitext path valid_ext _is_sycl_file path str - bool valid_ext = sycl os path splitext path valid_ext