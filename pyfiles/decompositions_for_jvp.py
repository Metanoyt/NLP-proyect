mypy allow-untyped-decorators mypy allow-untyped-defs inspect collections abc Callable typing Optional torch torch _decomp torch Tensor torch _prims_common wrappers _maybe_remove_out_wrapper decomposition_table = torch _decomp decomposition_table decomposition_table_for_jvp dict torch _ops OperatorBase Callable = register_decomposition = torch _decomp register_decomposition aten = torch ops aten NOTE forward-mode AD decompositions mechanism The mechanism VariableType IF any inputs have forward grad AND there no forward AD formula implemented AND functions actually differentiable run decomposition See run_jit_decomposition_with_args_for_jvp We currently use python decompositions we torchscript Note we would building backward graph decomposed level too OK because we would ve errored out otherwise anyway TODO The mechanism we using register decompositions doesn t seem exclusively used jvp So open question here whether torch csrc jit runtime decomposition_registry cpp being used other things If case we may go down decomposition path unexpectedly possibly produce unintelligible error vs erroring out earlier printing forward AD formula implemented The solution may have explicitly white list control when enable decomposition maybe_register_decomposition op decorator f try register_decomposition op f except Exception f decorator Functions where we need special decomposition jvp there s another version should used more generally ex jvp we need recompute mean variance backwards normalization function Without jvp should use saved value decomposition_table_for_jvp = register_decomposition_for_jvp fn register_decomposition fn registry=decomposition_table_for_jvp _register_jit_decomposition_for_jvp decomp use_python=False decomp decomposition_table_for_jvp decomposition_table_used = decomposition_table_for_jvp decomp decomposition_table decomposition_table_used = decomposition_table raise RuntimeError f could find decomposition decomp decomp_fn = decomposition_table_used decomp ` out_wrapper ` extends decompositions signature ` out ` parameter However jit will use unwrapped function s signature instead so we need unwrap here prevent error decomp_fn = _maybe_remove_out_wrapper decomp_fn use_python decomp_fn = torch jit ignore decomp_fn sig = inspect signature decomp_fn Create string wrapping function signature example output wrapped_decomp x torch Tensor y int z int decomp_fn x y z Thanks copilot get_function_def sig param_def = f param_str param_str sig parameters values param_use = f param_str param_str sig parameters keys f wrapped_decomp join param_def \n decomp_fn join param_use \n f_str = get_function_def sig graph = torch jit CompilationUnit f_str wrapped_decomp graph graph = torch jit script decomp_fn graph torch jit _register_decomposition decomp graph The only decompositions here temporary hacks purposes jvp TODO do these also belong here maybe_register_decomposition aten trace default trace Tensor - Tensor torch sum torch diag maybe_register_decomposition aten log_sigmoid_forward default log_sigmoid_forward Tensor - tuple Tensor Tensor min = torch minimum new_zeros z = torch exp -torch abs is_cuda is_xpu buffer = new_zeros buffer = z min - torch log p z buffer recompute_mean_var input Tensor rstd Tensor inner_dim_indices list int keepdim bool most norm decompositions will same core version except here We recompute mean variance so they track gradients through input mean = torch mean input dim=inner_dim_indices keepdim=keepdim var = torch var input dim=inner_dim_indices unbiased=False keepdim=keepdim eps = torch pow rstd - var makes me so sad inside eps = eps detach rstd = torch sqrt var + eps mean rstd register_decomposition_for_jvp aten native_layer_norm_backward native_layer_norm_backward grad_out Tensor input Tensor normalized_shape list int mean Tensor rstd Tensor weight Optional Tensor bias Optional Tensor output_mask list bool - tuple Optional Tensor Optional Tensor Optional Tensor input_shape = input shape input_ndim = input dim axis = input_ndim - len normalized_shape inner_dims = input_shape axis outer_dims = input_shape axis inner_dim_indices = list range axis input_ndim outer_dim_indices = list range axis N = i inner_dims N = i M = i outer_dims M = i M = N = input new_zeros input_shape input new_zeros input_shape axis input new_zeros input_shape axis mean_ rstd_ = recompute_mean_var input rstd inner_dim_indices keepdim=True x_hat = input - mean_ rstd_ weight None grad_x_hat = grad_out weight grad_x_hat = grad_out = grad_x_hat N b = torch sum grad_x_hat inner_dim_indices True c = torch mul grad_x_hat x_hat c = torch sum c inner_dim_indices True c = torch mul x_hat c inner = - b - c output_mask d_input Optional Tensor = rstd_ N inner d_input = torch zeros_like input should None doesn t work vjp output_mask weight None len outer_dim_indices d_weight Optional Tensor = torch sum grad_out x_hat outer_dim_indices False d_weight = grad_out x_hat weight None d_weight = torch zeros_like weight should None doesn t work vjp d_weight = torch zeros should None doesn t work vjp output_mask bias None len outer_dim_indices d_bias Optional Tensor = torch sum grad_out outer_dim_indices False d_bias = grad_out clone bias None d_bias = torch zeros_like bias should None doesn t work vjp d_bias = torch zeros should None doesn t work vjp d_input d_weight d_bias prod x list int r = i x r = i r register_decomposition_for_jvp aten native_batch_norm_backward native_batch_norm_backward grad_out Tensor input Tensor weight Optional Tensor running_mean Optional Tensor running_var Optional Tensor save_mean Optional Tensor save_invstd Optional Tensor train bool eps float output_mask list bool - tuple Tensor Optional Tensor Optional Tensor input_shape = input shape input_rank = input dim assert input_rank = rank input must least axis = num_features = prod input_shape input_shape axis type ignore arg-type mean = save_mean invstd = save_invstd train assert save_mean None save_invstd None when train=True save_mean save_invstd required reduciton_dims = + list range input dim assert invstd None typing mean invstd = recompute_mean_var input invstd reduciton_dims keepdim=False assert running_mean None running_var None mean = running_mean invstd = torch rsqrt running_var + eps assert invstd None mean None broadcast_mask = input_rank broadcast_mask axis = input_shape axis reduction_axes list int = i range input_rank i = axis reduction_axes append i mean = torch reshape mean broadcast_mask norm = num_features grad_output_sum = torch sum grad_out reduction_axes dot_p = torch sum grad_out input - mean reduction_axes grad_mean = torch reshape grad_output_sum norm broadcast_mask proj_scale = torch reshape torch mul dot_p norm invstd invstd broadcast_mask weight None grad_scale = torch reshape invstd broadcast_mask grad_scale = torch reshape invstd weight broadcast_mask train proj = input - mean proj_scale grad_input = grad_out - proj - grad_mean grad_scale grad_input = grad_out grad_scale output_mask grad_weight = dot_p invstd weight None grad_weight = torch zeros_like weight should None doesn t work vjp grad_weight = torch zeros should None doesn t work vjp output_mask grad_bias = grad_output_sum grad_bias = torch zeros_like grad_output_sum should None doesn t work vjp grad_input grad_weight grad_bias register_decomposition_for_jvp aten batch_norm_backward batch_norm_backward grad_out Tensor input Tensor weight Tensor running_mean Optional Tensor running_var Optional Tensor save_mean Optional Tensor save_var Optional Tensor update bool eps float output_mask list bool reserve Tensor - tuple Tensor Optional Tensor Optional Tensor native_batch_norm_backward grad_out input weight running_mean running_var save_mean save_var update eps output_mask _register_jit_decomposition_for_jvp torch ops aten trace default use_python=True _register_jit_decomposition_for_jvp torch ops aten nll_loss_backward default _register_jit_decomposition_for_jvp torch ops aten nll_loss d_backward default _register_jit_decomposition_for_jvp torch ops aten _log_softmax_backward_data default _register_jit_decomposition_for_jvp torch ops aten _softmax_backward_data default _register_jit_decomposition_for_jvp torch ops aten log_sigmoid_forward default _register_jit_decomposition_for_jvp torch ops aten native_layer_norm_backward default _register_jit_decomposition_for_jvp torch ops aten native_batch_norm_backward default _register_jit_decomposition_for_jvp torch ops aten cudnn_batch_norm_backward default _register_jit_decomposition_for_jvp torch ops aten batch_norm_backward default _register_jit_decomposition_for_jvp torch ops aten miopen_batch_norm_backward default