torch nn functional F torch Tensor torch nn common_types _size_any_t module Module __all__ = Fold Unfold Fold Module r Combines array sliding local blocks into large containing tensor Consider batched attr ` input ` tensor containing sliding local blocks e g patches images shape math ` N C \times \prod \text kernel\_size L ` where math ` N ` batch dimension math ` C \times \prod \text kernel\_size ` number values within block block has math ` \prod \text kernel\_size ` spatial locations each containing math ` C ` -channeled vector math ` L ` total number blocks This exactly same specification output shape ` ~torch nn Unfold ` This operation combines these local blocks into large attr ` output ` tensor shape math ` N C \text output\_size \text output\_size \dots ` summing overlapping values Similar ` ~torch nn Unfold ` arguments must satisfy math L = \prod_d \left\lfloor\frac \text output\_size d + \times \text padding d - \text dilation d \times \text kernel\_size d - - \text stride d + \right\rfloor where math ` d ` over all spatial dimensions attr ` output_size ` describes spatial shape large containing tensor sliding local blocks It useful resolve ambiguity when multiple input shapes map same number sliding blocks e g ` ` stride ` ` The attr ` padding ` attr ` stride ` attr ` dilation ` arguments specify how sliding blocks retrieved attr ` stride ` controls stride sliding blocks attr ` padding ` controls amount implicit zero-paddings both sides attr ` padding ` number points each dimension before reshaping attr ` dilation ` controls spacing between kernel points also known \u e trous algorithm It harder describe ` link ` _ has nice visualization what attr ` dilation ` does r Args output_size int tuple shape spatial dimensions output i e ` ` output sizes ` ` kernel_size int tuple size sliding blocks dilation int tuple optional parameter controls stride elements within neighborhood Default padding int tuple optional implicit zero padding added both sides input Default stride int tuple stride sliding blocks input spatial dimensions Default If attr ` output_size ` attr ` kernel_size ` attr ` dilation ` attr ` padding ` attr ` stride ` int tuple length then their values will replicated across all spatial dimensions For case two output spatial dimensions operation sometimes called ` ` col im ` ` note ` ~torch nn Fold ` calculates each combined value resulting large tensor summing all values all containing blocks ` ~torch nn Unfold ` extracts values local blocks copying large tensor So blocks overlap they inverses each other In general folding unfolding operations related follows Consider ` ~torch nn Fold ` ` ~torch nn Unfold ` instances created same parameters fold_params = dict kernel_size= dilation= padding= stride= fold = nn Fold output_size= fold_params unfold = nn Unfold fold_params Then any supported ` ` input ` ` tensor following equality holds fold unfold input == divisor input where ` ` divisor ` ` tensor depends only shape dtype ` ` input ` ` xdoctest +SKIP input_ones = torch ones input shape dtype=input dtype divisor = fold unfold input_ones When ` ` divisor ` ` tensor contains no zero elements then ` ` fold ` ` ` ` unfold ` ` operations inverses each other up constant divisor warning Currently only unbatched D batched D image-like output tensors supported Shape - Input math ` N C \times \prod \text kernel\_size L ` math ` C \times \prod \text kernel\_size L ` - Output math ` N C \text output\_size \text output\_size \dots ` math ` C \text output\_size \text output\_size \dots ` described above Examples fold = nn Fold output_size= kernel_size= input = torch randn output = fold input output size torch Size _link https github com vdumoulin conv_arithmetic blob master README md __constants__ = output_size kernel_size dilation padding stride output_size _size_any_t kernel_size _size_any_t dilation _size_any_t padding _size_any_t stride _size_any_t __init__ output_size _size_any_t kernel_size _size_any_t dilation _size_any_t = padding _size_any_t = stride _size_any_t = - None super __init__ output_size = output_size kernel_size = kernel_size dilation = dilation padding = padding stride = stride forward input Tensor - Tensor Runs forward pass F fold input output_size kernel_size dilation padding stride extra_repr - str Return extra representation module output_size= output_size kernel_size= kernel_size dilation= dilation padding= padding stride= stride format __dict__ Unfold Module r Extracts sliding local blocks batched input tensor Consider batched attr ` input ` tensor shape math ` N C ` where math ` N ` batch dimension math ` C ` channel dimension math ` ` represent arbitrary spatial dimensions This operation flattens each sliding attr ` kernel_size ` -sized block within spatial dimensions attr ` input ` into column i e last dimension -D attr ` output ` tensor shape math ` N C \times \prod \text kernel\_size L ` where math ` C \times \prod \text kernel\_size ` total number values within each block block has math ` \prod \text kernel\_size ` spatial locations each containing math ` C ` -channeled vector math ` L ` total number such blocks math L = \prod_d \left\lfloor\frac \text spatial\_size d + \times \text padding d - \text dilation d \times \text kernel\_size d - - \text stride d + \right\rfloor where math ` \text spatial\_size ` formed spatial dimensions attr ` input ` math ` ` above math ` d ` over all spatial dimensions Therefore indexing attr ` output ` last dimension column dimension gives all values within certain block The attr ` padding ` attr ` stride ` attr ` dilation ` arguments specify how sliding blocks retrieved attr ` stride ` controls stride sliding blocks attr ` padding ` controls amount implicit zero-paddings both sides attr ` padding ` number points each dimension before reshaping attr ` dilation ` controls spacing between kernel points also known \u e trous algorithm It harder describe ` link ` _ has nice visualization what attr ` dilation ` does r Args kernel_size int tuple size sliding blocks dilation int tuple optional parameter controls stride elements within neighborhood Default padding int tuple optional implicit zero padding added both sides input Default stride int tuple optional stride sliding blocks input spatial dimensions Default If attr ` kernel_size ` attr ` dilation ` attr ` padding ` attr ` stride ` int tuple length their values will replicated across all spatial dimensions For case two input spatial dimensions operation sometimes called ` ` im col ` ` note ` ~torch nn Fold ` calculates each combined value resulting large tensor summing all values all containing blocks ` ~torch nn Unfold ` extracts values local blocks copying large tensor So blocks overlap they inverses each other In general folding unfolding operations related follows Consider ` ~torch nn Fold ` ` ~torch nn Unfold ` instances created same parameters fold_params = dict kernel_size= dilation= padding= stride= fold = nn Fold output_size= fold_params unfold = nn Unfold fold_params Then any supported ` ` input ` ` tensor following equality holds fold unfold input == divisor input where ` ` divisor ` ` tensor depends only shape dtype ` ` input ` ` xdoctest +SKIP input_ones = torch ones input shape dtype=input dtype divisor = fold unfold input_ones When ` ` divisor ` ` tensor contains no zero elements then ` ` fold ` ` ` ` unfold ` ` operations inverses each other up constant divisor warning Currently only -D input tensors batched image-like tensors supported Shape - Input math ` N C ` - Output math ` N C \times \prod \text kernel\_size L ` described above Examples unfold = nn Unfold kernel_size= input = torch randn output = unfold input each patch contains values x = vectors each channels blocks x kernels total x input output size torch Size xdoctest +IGNORE_WANT Convolution equivalent Unfold + Matrix Multiplication + Fold view output shape inp = torch randn w = torch randn inp_unf = torch nn functional unfold inp out_unf = inp_unf transpose matmul w view w size - t transpose out = torch nn functional fold out_unf equivalently avoiding copy out = out_unf view torch nn functional conv d inp w - out abs max tensor e- _link https github com vdumoulin conv_arithmetic blob master README md __constants__ = kernel_size dilation padding stride kernel_size _size_any_t dilation _size_any_t padding _size_any_t stride _size_any_t __init__ kernel_size _size_any_t dilation _size_any_t = padding _size_any_t = stride _size_any_t = - None super __init__ kernel_size = kernel_size dilation = dilation padding = padding stride = stride forward input Tensor - Tensor Runs forward pass F unfold input kernel_size dilation padding stride extra_repr - str Return extra representation module kernel_size= kernel_size dilation= dilation padding= padding stride= stride format __dict__