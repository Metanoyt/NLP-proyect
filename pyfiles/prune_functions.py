mypy allow-untyped-defs Collection conversion functions linear conv d structured pruning Also contains utilities bias propagation collections abc Callable typing cast Optional torch torch nn Tensor torch nn utils parametrize torch nn utils parametrize ParametrizationList parametrization BiasHook FakeStructuredSparsity BIAS PROPAGATION _remove_bias_handles module nn Module - None hasattr module _forward_hooks bias_hooks list int = key hook module _forward_hooks items isinstance hook BiasHook bias_hooks append key key bias_hooks del module _forward_hooks key _get_adjusted_next_layer_bias next_layer nn Module pruned_biases Tensor mask Tensor - nn Parameter r Returns new adjusted bias second supported module parametrize is_parametrized next_layer need access original weight parametrization_dict = cast nn ModuleDict next_layer parametrizations weight_parameterizations = cast ParametrizationList parametrization_dict weight next_weight = weight_parameterizations original next_weight = cast Tensor next_layer weight scaling_weight = next_weight ~mask isinstance next_layer nn Conv d checking Conv d Propagating first layer pruned biases calculating new second layer bias involves more steps since Conv d scaling weight has extra dimensions so adding bias involves broadcasting logically each channel k range oC scaled_biases = sum first_bias pruned_idx next_weight k pruned_idx T new_next_bias k = old_next_bias k + scaled_biases scaling_product = torch matmul pruned_biases reshape - torch transpose scaling_weight sum_range = list range len scaling_product shape all first dimension scaled_biases = torch sum scaling_product sum_range isinstance next_layer nn Linear Linear scaled_biases = torch matmul pruned_biases torch transpose scaling_weight recall b _new = b w T + b raise NotImplementedError f Type type next_layer supported yet parametrize is_parametrized next_layer getattr next_layer _bias None None next_layer parametrized has original bias _bias adjusted_bias = nn Parameter scaled_biases + next_layer _bias type ignore operator parametrize is_parametrized next_layer next_layer bias None next_layer parametrized has bias adjusted_bias = nn Parameter scaled_biases + next_layer bias type ignore operator next_layer has no bias adjusted_bias = nn Parameter scaled_biases adjusted_bias _prune_module_bias module nn Module mask Tensor - None r Applies mask given modules bias prune bias along weights discard pruned indices bias original_bias = cast Tensor getattr module _bias module bias original_bias None module bias = nn Parameter original_bias mask remove _bias parameter hasattr module _bias delattr module _bias _propagate_module_bias module nn Module mask Tensor - Optional Tensor r In case we need propagate biases function will biases we need set current module bias module bias None module bias = nn Parameter cast Tensor module bias mask getattr module _bias None None pyrefly ignore bad-assignment module bias = nn Parameter cast Tensor module _bias mask get pruned biases propagate subsequent layer getattr module _bias None None pruned_biases = cast Tensor module _bias ~mask pruned_biases = None hasattr module _bias delattr module _bias pruned_biases LINEAR _prune_linear_helper linear nn Linear - Tensor expects linear parameterized linear module parametrization_dict = cast nn ModuleDict linear parametrizations weight_parameterizations = cast ParametrizationList parametrization_dict weight p weight_parameterizations isinstance p FakeStructuredSparsity mask = cast Tensor p mask torch no_grad parametrize remove_parametrizations linear weight leave_parametrized=True linear weight = nn Parameter linear weight mask type ignore possibly-undefined linear out_features = linear weight shape _remove_bias_handles linear pyrefly ignore unbound-name mask prune_linear linear nn Linear - None mask = _prune_linear_helper linear getattr linear prune_bias False _prune_module_bias linear mask prune_linear_linear linear nn Linear linear nn Linear - None prune_linear_activation_linear linear None linear prune_linear_activation_linear linear nn Linear activation Optional Callable Tensor Tensor linear nn Linear mask = _prune_linear_helper linear getattr linear prune_bias False _prune_module_bias linear mask pruned_biases = _propagate_module_bias linear mask pruned_biases None activation pruned_biases = activation pruned_biases linear bias = _get_adjusted_next_layer_bias linear pruned_biases mask torch no_grad parametrize is_parametrized linear parametrization_dict = cast nn ModuleDict linear parametrizations weight_parameterizations = cast ParametrizationList parametrization_dict weight weight_parameterizations original = nn Parameter weight_parameterizations original mask linear in_features = weight_parameterizations original shape linear weight = nn Parameter linear weight mask linear in_features = linear weight shape CONV D _prune_conv d_helper conv d nn Conv d - Tensor parametrization_dict = cast nn ModuleDict conv d parametrizations weight_parameterizations = cast ParametrizationList parametrization_dict weight p weight_parameterizations isinstance p FakeStructuredSparsity mask = cast Tensor p mask torch no_grad parametrize remove_parametrizations conv d weight leave_parametrized=True conv d weight = nn Parameter conv d weight mask type ignore possibly-undefined conv d out_channels = conv d weight shape _remove_bias_handles conv d pyrefly ignore unbound-name mask prune_conv d_padded conv d_ nn Conv d - None parametrization_dict = cast nn ModuleDict conv d_ parametrizations weight_parameterizations = cast ParametrizationList parametrization_dict weight p weight_parameterizations isinstance p FakeStructuredSparsity mask = cast Tensor p mask torch no_grad parametrize remove_parametrizations conv d_ weight leave_parametrized=True getattr conv d_ _bias None None conv d_ bias None conv d_ has original bias bias propagated previous layer new_bias = torch zeros conv d_ bias shape new_bias mask = conv d_ bias mask type ignore possibly-undefined adjusted bias keep conv d_ pyrefly ignore unbound-name new_bias ~mask = cast Tensor conv d_ _bias ~mask pruned biases kept instead propagated conv d_ bias = nn Parameter new_bias conv d_ has only original bias conv d_ bias = nn Parameter cast Tensor conv d_ _bias no original bias only propagated bias conv d_ bias None conv d_ has bias propagated previous layer conv d_ bias data ~mask = type ignore possibly-undefined hasattr conv d_ _bias delattr conv d_ _bias prune_conv d conv d nn Conv d - None mask = _prune_conv d_helper conv d getattr conv d prune_bias False _prune_module_bias conv d mask prune_conv d_conv d conv d_ nn Conv d conv d_ nn Conv d - None prune_conv d_activation_conv d conv d_ None conv d_ prune_conv d_activation_conv d conv d_ nn Conv d activation Optional Callable Tensor Tensor conv d_ nn Conv d r Fusion Pattern conv d - some activation module function - conv d layers parametrization_dict = cast nn ModuleDict conv d_ parametrizations weight_parameterizations = cast ParametrizationList parametrization_dict weight p weight_parameterizations isinstance p FakeStructuredSparsity mask = cast Tensor p mask prune_bias = getattr conv d_ prune_bias False hasattr conv d_ padding cast tuple int conv d_ padding conv d_ bias None getattr conv d_ _bias None None prune_conv d_padded conv d_ mask = _prune_conv d_helper conv d_ prune_bias _prune_module_bias conv d_ mask pruned_biases = _propagate_module_bias conv d_ mask pruned_biases None activation pruned_biases = activation pruned_biases conv d_ bias = _get_adjusted_next_layer_bias conv d_ pruned_biases mask hasattr conv d_ padding cast tuple int conv d_ padding conv d_ bias None torch no_grad parametrize is_parametrized conv d_ parametrization_dict = cast nn ModuleDict conv d_ parametrizations weight_parameterizations = cast ParametrizationList parametrization_dict weight weight_parameterizations original = nn Parameter weight_parameterizations original mask conv d_ in_channels = weight_parameterizations original shape conv d_ weight = nn Parameter conv d_ weight mask conv d_ in_channels = conv d_ weight shape prune_conv d_pool_activation_conv d c nn Conv d pool nn Module activation Optional Callable Tensor Tensor c nn Conv d - None prune_conv d_activation_conv d c activation c prune_conv d_activation_pool_conv d c nn Conv d activation Optional Callable Tensor Tensor pool nn Module c nn Conv d - None prune_conv d_activation_conv d c activation c prune_conv d_pool_flatten_linear conv d nn Conv d pool nn Module flatten Optional Callable Tensor Tensor linear nn Linear - None mask = _prune_conv d_helper conv d We map pruned indices Conv d output flattened indices Linear following Flatten layer we determine flattening scale h w readjust ` first_pruned_indices ` each idx maps range idx h w idx+ h w ` first_valid_indices ` ` pruned_biases ` repeat each bias h w parametrize is_parametrized linear parametrization_dict = cast nn ModuleDict linear parametrizations weight_parameterizations = cast ParametrizationList parametrization_dict weight linear_ic = weight_parameterizations original shape linear_ic = linear weight shape conv d_oc = len mask linear_ic conv d_oc = raise AssertionError f Flattening dimensions conv d_oc linear_ic supported flatten_scale = linear_ic conv d_oc flattened_mask = torch tensor val flatten_scale val mask dtype=torch bool device=mask device flatten getattr conv d prune_bias False _prune_module_bias conv d mask pruned_biases = cast Tensor _propagate_module_bias conv d mask flattened_pruned_biases = torch tensor bias flatten_scale bias pruned_biases device=mask device flatten linear bias = _get_adjusted_next_layer_bias linear flattened_pruned_biases flattened_mask torch no_grad parametrize is_parametrized linear parametrization_dict = cast nn ModuleDict linear parametrizations weight_parameterizations = cast ParametrizationList parametrization_dict weight weight_parameterizations original = nn Parameter weight_parameterizations original flattened_mask linear in_features = weight_parameterizations original shape linear weight = nn Parameter linear weight flattened_mask linear in_features = linear weight shape prune_lstm_output_linear lstm nn LSTM getitem Callable linear nn Linear - None prune_lstm_output_layernorm_linear lstm getitem None linear prune_lstm_output_layernorm_linear lstm nn LSTM getitem Callable layernorm Optional nn LayerNorm linear nn Linear - None i range lstm num_layers parametrize is_parametrized lstm f weight_ih_l i parametrization_dict = cast nn ModuleDict lstm parametrizations weight_parameterizations = cast ParametrizationList parametrization_dict f weight_ih_l i mask = weight_parameterizations mask torch no_grad parametrize remove_parametrizations lstm f weight_ih_l i leave_parametrized=True setattr lstm f weight_ih_l i nn Parameter getattr lstm f weight_ih_l i mask setattr lstm f bias_ih_l i nn Parameter getattr lstm f bias_ih_l i mask parametrize is_parametrized lstm f weight_hh_l i parametrization_dict = cast nn ModuleDict lstm parametrizations weight_parameterizations = cast ParametrizationList parametrization_dict f weight_hh_l i mask = weight_parameterizations mask torch no_grad parametrize remove_parametrizations lstm f weight_hh_l i leave_parametrized=True splitting out hidden-hidden masks W_hi W_hf W_hg W_ho = torch split getattr lstm f weight_hh_l i lstm hidden_size M_hi M_hf M_hg M_ho = torch split mask lstm hidden_size type ignore arg-type resize each individual weight separately W_hi = W_hi M_hi M_hi W_hf = W_hf M_hf M_hf W_hg = W_hg M_hg M_hg W_ho = W_ho M_ho M_ho concat use new weight new_weight = torch cat W_hi W_hf W_hg W_ho setattr lstm f weight_hh_l i nn Parameter new_weight setattr lstm f bias_hh_l i nn Parameter getattr lstm f bias_hh_l i mask If final layer then we need prune linear layer columns i + == lstm num_layers lstm hidden_size = int M_hi sum torch no_grad parametrize is_parametrized linear parametrization_dict = cast nn ModuleDict linear parametrizations weight_parameterizations = cast ParametrizationList parametrization_dict weight weight_parameterizations original = nn Parameter weight_parameterizations original M_ho linear in_features = weight_parameterizations original shape linear weight = nn Parameter linear weight M_ho linear in_features = linear weight shape layernorm module prune weight bias layernorm None layernorm normalized_shape = linear in_features layernorm weight = nn Parameter layernorm weight M_ho layernorm bias = nn Parameter layernorm bias M_ho otherwise need prune columns input next LSTM layer torch no_grad parametrize is_parametrized lstm f weight_ih_l i + parametrization_dict = cast nn ModuleDict lstm parametrizations weight_parameterizations = cast ParametrizationList getattr parametrization_dict f weight_ih_l i + weight_parameterizations original = nn Parameter weight_parameterizations original M_ho next_layer_weight = getattr lstm f weight_ih_l i + setattr lstm f weight_ih_l i + nn Parameter next_layer_weight M_ho