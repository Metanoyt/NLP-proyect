Owner s module fx unittest sympy torch torch fx GraphModule symbolic_trace torch fx annotate annotate torch fx experimental graph_gradual_typechecker broadcast_types GraphTypeChecker Refine torch fx experimental refinement_types Equality torch fx experimental rewriter RewritingTracer torch fx experimental unify_refinements infer_symbolic_types torch fx passes shape_prop ShapeProp torch fx tensor_type Dyn is_consistent is_more_precise TensorType torch testing _internal common_utils raise_on_run_directly TestCase try torchvision models resnet HAS_TORCHVISION = True except ImportError HAS_TORCHVISION = False skipIfNoTorchVision = unittest skipIf HAS_TORCHVISION no torchvision conv x in_planes out_planes stride= groups= dilation= x convolution padding torch nn Conv d in_planes out_planes kernel_size= stride=stride padding=dilation groups=groups bias=False dilation=dilation AnnotationsTest TestCase test_annotations Test type annotations forward function The annotation should appear n graph where n corresponding node resulting graph M torch nn Module forward x TensorType Dyn y Dyn z TensorType Dyn Dyn torch add x y + z module = M symbolic_traced torch fx GraphModule = symbolic_trace module expected_ph_types = TensorType Dyn Dyn TensorType Dyn Dyn expected_iter = iter expected_ph_types n symbolic_traced graph nodes n op == placeholder assert n type == next expected_iter test_annotate M torch nn Module forward x y = annotate x TensorType Dyn torch add x y module = M symbolic_traced torch fx GraphModule = symbolic_trace module n symbolic_traced graph nodes n op == placeholder assert n type == TensorType Dyn test_consistency Test consistency relation assertTrue is_consistent TensorType TensorType Dyn assertTrue is_consistent int Dyn assertTrue is_consistent int int assertFalse is_consistent TensorType TensorType assertFalse is_consistent TensorType int test_precision Test consistency relation assertTrue is_more_precise TensorType TensorType Dyn assertTrue is_more_precise int Dyn assertTrue is_more_precise int int assertFalse is_more_precise TensorType TensorType assertFalse is_more_precise TensorType int test_broadcasting t = TensorType t = TensorType t = TensorType t = TensorType t = TensorType todo switch all code use list instead tuple t = TensorType assert broadcast_types t t == TensorType TensorType assert broadcast_types t t == t t assert broadcast_types t t == t t test_broadcasting t = TensorType t = TensorType assert broadcast_types t t == TensorType TensorType test_broadcasting t = TensorType Dyn t = TensorType assert broadcast_types t t == TensorType Dyn TensorType TypeCheckerTest TestCase test_type_check_add_with_broadcast M torch nn Module forward x TensorType Dyn y TensorType torch add x y module = M symbolic_traced torch fx GraphModule = symbolic_trace module tc = GraphTypeChecker symbolic_traced tc type_check expected_ph_types = TensorType Dyn TensorType TensorType Dyn TensorType Dyn expected_iter = iter expected_ph_types n symbolic_traced graph nodes n op == call_function assert n meta broadcast assert n type == next expected_iter test_type_check_add_with_scalar M torch nn Module forward x int y TensorType torch add x y module = M symbolic_traced torch fx GraphModule = symbolic_trace module tc = GraphTypeChecker symbolic_traced tc type_check expected_ph_types = int TensorType TensorType TensorType expected_iter = iter expected_ph_types n symbolic_traced graph nodes assert n type == next expected_iter test_type_check_add_false M torch nn Module forward x TensorType Dyn y TensorType torch add x y module = M symbolic_traced torch fx GraphModule = symbolic_trace module tc = GraphTypeChecker symbolic_traced assertRaises TypeError tc type_check test_type_check_add_true M torch nn Module forward x TensorType Dyn y TensorType torch add x y module = M symbolic_traced torch fx GraphModule = symbolic_trace module tc = GraphTypeChecker symbolic_traced assertTrue tc type_check expected_ph_types = TensorType Dyn TensorType expected_iter = iter expected_ph_types n symbolic_traced graph nodes n op == placeholder assert n type == next expected_iter n op == output assert n type == TensorType Dyn test_type_check_reshape_true M torch nn Module forward x TensorType torch reshape x module = M symbolic_traced torch fx GraphModule = symbolic_trace module tc = GraphTypeChecker symbolic_traced assertTrue tc type_check n symbolic_traced graph nodes n op == placeholder assert n type == TensorType n op == call_function assert n type == TensorType n op == output assert n type == TensorType test_type_check_reshape_false M torch nn Module forward x TensorType torch reshape x module = M symbolic_traced torch fx GraphModule = symbolic_trace module tc = GraphTypeChecker symbolic_traced assertRaises TypeError tc type_check test_type_check_reshape_dyn_false M torch nn Module forward x TensorType torch reshape x - module = M symbolic_traced torch fx GraphModule = symbolic_trace module tc = GraphTypeChecker symbolic_traced assertRaises TypeError tc type_check test_type_check_reshape_dyn_true M torch nn Module forward x TensorType torch reshape x - module = M symbolic_traced torch fx GraphModule = symbolic_trace module tc = GraphTypeChecker symbolic_traced assertTrue tc type_check test_type_check_reshape_dyn_true_param_false M torch nn Module forward x TensorType Dyn torch reshape x - module = M symbolic_traced torch fx GraphModule = symbolic_trace module tc = GraphTypeChecker symbolic_traced assertRaises TypeError tc type_check test_type_check_transpose_true M torch nn Module forward x TensorType torch transpose x module = M symbolic_traced torch fx GraphModule = symbolic_trace module tc = GraphTypeChecker symbolic_traced assertTrue tc type_check n symbolic_traced graph nodes n op == call_function assert n type == TensorType n op == output assert n type == TensorType n op == x assert n placeholder == TensorType test_type_check_transpose_False M torch nn Module forward x TensorType torch transpose x module = M symbolic_traced torch fx GraphModule = symbolic_trace module tc = GraphTypeChecker symbolic_traced assertRaises TypeError tc type_check test_type_check_batch_norm_ D BasicBlock torch nn Module __init__ inplanes planes super __init__ norm_layer = torch nn BatchNorm d bn = norm_layer planes forward x TensorType identity = x out TensorType Dyn = bn x out += identity out B = BasicBlock ast_rewriter = RewritingTracer graph = ast_rewriter trace B traced = GraphModule ast_rewriter root graph gm tc = GraphTypeChecker traced tc type_check n graph nodes n op == placeholder assert n type == TensorType n op == output assert n type == TensorType n op == call_module assert n type == TensorType n op == call_function assert n type == TensorType test_type_check_batch_norm_ D_false BasicBlock torch nn Module __init__ inplanes planes super __init__ norm_layer = torch nn BatchNorm d bn = norm_layer planes forward x TensorType identity = x out TensorType Dyn = bn x out += identity out B = BasicBlock ast_rewriter = RewritingTracer graph = ast_rewriter trace B traced = GraphModule ast_rewriter root graph gm tc = GraphTypeChecker traced assertRaises TypeError tc type_check test_type_check_batch_norm_ D_broadcast BasicBlock torch nn Module __init__ inplanes planes super __init__ norm_layer = torch nn BatchNorm d bn = norm_layer planes forward x Dyn identity = x out TensorType Dyn = bn x out += identity out B = BasicBlock ast_rewriter = RewritingTracer graph = ast_rewriter trace B traced = GraphModule ast_rewriter root graph gm tc = GraphTypeChecker traced tc type_check n graph nodes n op == placeholder assert n type == TensorType Dyn Dyn Dyn Dyn n op == call_function assert n type == TensorType Dyn Dyn Dyn Dyn n op == output assert n type == TensorType Dyn Dyn Dyn Dyn n op == call_module assert n type == TensorType Dyn B = BasicBlock ast_rewriter = RewritingTracer graph = ast_rewriter trace B traced = GraphModule ast_rewriter root graph gm tc = GraphTypeChecker traced assertRaises TypeError tc type_check test_type_check_conv D BasicBlock torch nn Module __init__ inplanes planes stride= super __init__ norm_layer = torch nn BatchNorm d conv = conv x inplanes planes stride bn = norm_layer planes forward x Dyn identity = x out TensorType Dyn = conv x out += identity out B = BasicBlock ast_rewriter = RewritingTracer graph = ast_rewriter trace B traced = GraphModule ast_rewriter root graph gm tc = GraphTypeChecker traced tc type_check n graph nodes n op == placeholder assert n type == TensorType Dyn Dyn Dyn Dyn n op == call_function assert n type == TensorType Dyn Dyn Dyn Dyn n op == output assert n type == TensorType Dyn Dyn Dyn Dyn n op == call_module assert n type == TensorType Dyn test_type_check_conv D_ BasicBlock torch nn Module __init__ inplanes planes stride= super __init__ norm_layer = torch nn BatchNorm d conv = conv x inplanes planes stride bn = norm_layer planes forward x TensorType identity = x out = conv x out += identity out B = BasicBlock b = B forward torch rand ast_rewriter = RewritingTracer graph = ast_rewriter trace B traced = GraphModule ast_rewriter root graph gm tc = GraphTypeChecker traced tc type_check t = TensorType n graph nodes n op == placeholder assert n type == t n op == call_function assert n type == t n op == output assert torch Size n type __args__ == b shape n op == call_module assert n type == t B = BasicBlock ast_rewriter = RewritingTracer graph = ast_rewriter trace B traced = GraphModule ast_rewriter root graph gm tc = GraphTypeChecker traced assertRaises TypeError tc type_check test_type_check_conv D_ _fully_static annotation_list = Dyn Dyn Dyn Dyn input_list = intermediate_types = Dyn Dyn Dyn Dyn Dyn Dyn Dyn in_planes_list = stride_list = out_planes_list = groups_list = dilation_list = padding_list = kernel_size_list = output_types = Dyn Dyn Dyn Dyn i range annotation = annotation_list i input = input_list i in_planes = in_planes_list i stride = stride_list i out_planes = out_planes_list i groups = groups_list i dilation = dilation_list i padding = padding_list i kernel_size = kernel_size_list i intermediate_type = intermediate_types i BasicBlock torch nn Module __init__ in_planes out_planes kernel_size stride padding groups dilation super __init__ conv = torch nn Conv d in_channels=in_planes out_channels=out_planes kernel_size=kernel_size stride=stride padding=padding groups=groups bias=False dilation=dilation forward x out = conv x out B = BasicBlock in_planes out_planes kernel_size stride padding groups dilation ast_rewriter = RewritingTracer graph = ast_rewriter trace B traced = GraphModule ast_rewriter root graph gm annotate our argument n graph nodes n op == placeholder n type = TensorType annotation b = B forward torch rand input tc = GraphTypeChecker traced tc type_check n graph nodes n op == output assert is_consistent n type TensorType b size test intermediate annotations BasicBlock torch nn Module __init__ in_planes out_planes kernel_size stride padding groups dilation super __init__ conv = torch nn Conv d in_channels=in_planes out_channels=out_planes kernel_size=kernel_size stride=stride padding=padding groups=groups bias=False dilation=dilation forward x out = conv x out B = BasicBlock in_planes out_planes kernel_size stride padding groups dilation ast_rewriter = RewritingTracer graph = ast_rewriter trace B traced = GraphModule ast_rewriter root graph gm populate our intermediate notes n traced graph nodes n op == call_module n type = TensorType intermediate_type tc = GraphTypeChecker traced tc type_check n traced graph nodes n op == output assert n type == TensorType output_types i assert is_consistent n type TensorType b size test_typecheck_basicblock BasicBlock torch nn Module expansion = __init__ inplanes planes stride= downsample=None groups= base_width= dilation= super __init__ norm_layer = torch nn BatchNorm d groups = base_width = raise ValueError BasicBlock only supports groups= base_width= dilation raise NotImplementedError Dilation supported BasicBlock Both conv downsample layers downsample input when stride = conv = conv x inplanes planes stride bn = norm_layer planes relu = torch nn ReLU inplace=True conv = conv x planes planes bn = norm_layer planes downsample = downsample stride = stride forward x TensorType identity = x out = conv x out = bn out out = relu out out = conv out out = bn out downsample None identity = downsample x out += identity out = relu out out B = BasicBlock ast_rewriter = RewritingTracer graph = ast_rewriter trace B traced = GraphModule ast_rewriter root graph gm tc = GraphTypeChecker traced tc type_check n traced graph nodes n target == output assert isinstance n type TensorType assert torch Size n type __args__ == B forward torch rand size test_type_check_conv D_maxpool d_flatten BasicBlock torch nn Module __init__ - None super __init__ conv = torch nn Conv d pool = torch nn MaxPool d conv = torch nn Conv d fc = torch nn Linear pool = torch nn AdaptiveAvgPool d forward x TensorType out = conv x out = pool out out = conv out out = pool out out = fc out out = pool out out = torch flatten out out B = BasicBlock ast_rewriter = RewritingTracer graph = ast_rewriter trace B traced = GraphModule ast_rewriter root graph gm tc = GraphTypeChecker traced tc type_check expected_ph_types = TensorType TensorType TensorType TensorType TensorType TensorType TensorType TensorType TensorType expected_iter = iter expected_ph_types traced graph eliminate_dead_code n traced graph nodes assert n type == next expected_iter test_type_check_flatten M torch nn Module forward x TensorType Dyn torch flatten x module = M symbolic_traced torch fx GraphModule = symbolic_trace module tc = GraphTypeChecker symbolic_traced tc type_check n symbolic_traced graph nodes n op == output assert n type == TensorType Dyn test_type_check_flatten_ M torch nn Module forward x TensorType Dyn Dyn torch flatten x module = M symbolic_traced torch fx GraphModule = symbolic_trace module tc = GraphTypeChecker symbolic_traced tc type_check n symbolic_traced graph nodes n op == output assert n type == TensorType Dyn Dyn test_type_check_flatten M torch nn Module forward x TensorType torch flatten x start_dim= end_dim= module = M symbolic_traced torch fx GraphModule = symbolic_trace module tc = GraphTypeChecker symbolic_traced tc type_check n symbolic_traced graph nodes n op == output assert n type == TensorType r = Refine symbolic_traced r refine c = r constraints assert c == Equality test_type_typechecl_maxpool d_ dinput BasicBlock torch nn Module __init__ - None super __init__ pool = torch nn MaxPool d forward x TensorType out = pool x out B = BasicBlock ast_rewriter = RewritingTracer graph = ast_rewriter trace B traced = GraphModule ast_rewriter root graph gm tc = GraphTypeChecker traced tc type_check n traced graph nodes n target == output assert n type == TensorType test_type_maxpool d_fully_static annotation_list = Dyn Dyn Dyn Dyn Dyn Dyn input_list = intermediate_types = Dyn Dyn Dyn Dyn Dyn Dyn Dyn stride_list = dilation_list = padding_list = kernel_size_list = output_types = Dyn Dyn i range annotation = annotation_list i input = input_list i stride = stride_list i dilation = dilation_list i padding = padding_list i kernel_size = kernel_size_list i intermediate_type = intermediate_types i BasicBlock torch nn Module __init__ kernel_size stride padding dilation super __init__ pool = torch nn MaxPool d kernel_size stride=stride padding=padding dilation=dilation return_indices=False ceil_mode=False forward x out = pool x out B = BasicBlock kernel_size stride padding dilation ast_rewriter = RewritingTracer graph = ast_rewriter trace B traced = GraphModule ast_rewriter root graph gm annotate our argument n graph nodes n op == placeholder n type = TensorType annotation b = B forward torch rand input tc = GraphTypeChecker traced tc type_check n graph nodes n op == output assert is_consistent n type TensorType b size test intermediate annotations BasicBlock torch nn Module __init__ kernel_size stride padding dilation super __init__ pool = torch nn MaxPool d kernel_size stride=stride padding=padding dilation=dilation return_indices=False ceil_mode=False forward x out = pool x out B = BasicBlock kernel_size stride padding dilation ast_rewriter = RewritingTracer graph = ast_rewriter trace B traced = GraphModule ast_rewriter root graph gm annotate our argument n graph nodes n op == placeholder n type = TensorType annotation populate our intermediate notes n traced graph nodes n op == call_module n type = TensorType intermediate_type tc = GraphTypeChecker traced tc type_check n traced graph nodes n op == output assert n type == TensorType output_types i assert is_consistent n type TensorType b size test_flatten_fully_static annotation_list = Dyn TensorType TensorType TensorType Dyn TensorType Dyn Dyn Dyn input_list = intermediate_list = noqa F Dyn start_dim = end_dim = - i range annotation = annotation_list i input = input_list i intermediate_type = intermediate_list i BasicBlock torch nn Module __init__ start end super __init__ start = start end = end forward x out = torch flatten x start end out B = BasicBlock start_dim i end_dim i ast_rewriter = RewritingTracer graph = ast_rewriter trace B traced = GraphModule ast_rewriter root graph gm annotate our argument n graph nodes n op == placeholder n type = annotation b = B forward torch rand input tc = GraphTypeChecker traced tc type_check n graph nodes n op == output assert is_consistent n type TensorType b size skipIfNoTorchVision test_resnet gm_run = symbolic_trace resnet sample_input = torch randn run our nodes ShapeProp gm_run propagate sample_input gm_static = symbolic_trace resnet n gm_static graph nodes n type = None g = GraphTypeChecker gm_static g type_check gm_static graph eliminate_dead_code gm_run graph eliminate_dead_code here we checking consistency fully dynamic nodes n n zip gm_static graph nodes gm_run graph nodes assert is_consistent n type TensorType n meta tensor_meta shape here we give same input runtime gm_static_with_types = symbolic_trace resnet we initialize our placeholder n gm_static_with_types graph nodes n op == placeholder n type = TensorType g = GraphTypeChecker gm_static_with_types g type_check n n zip gm_static_with_types graph nodes gm_run graph nodes assert n type == TensorType n meta tensor_meta shape apply shape inference graph check batch size equal across all layers infer_symbolic_types gm_static batch_sizes = set gm_static graph eliminate_dead_code n gm_static graph nodes assert isinstance n type TensorType batch_sizes add n type __args__ assert len batch_sizes == test_type_check_batch_norm_symbolic BasicBlock torch nn Module __init__ inplanes planes super __init__ norm_layer = torch nn BatchNorm d bn = norm_layer planes forward x Dyn identity = x out TensorType Dyn = bn x out += identity out B = BasicBlock ast_rewriter = RewritingTracer graph = ast_rewriter trace B traced = GraphModule ast_rewriter root graph gm tc = GraphTypeChecker traced tc type_check infer_symbolic_types traced my_types = iter TensorType sympy symbols ~ TensorType sympy symbols ~ TensorType sympy symbols ~ TensorType sympy symbols ~ n graph nodes assert n type == next my_types test_symbolic_add_with_broadcast M torch nn Module forward x TensorType Dyn y TensorType torch add x y module = M symbolic_traced torch fx GraphModule = symbolic_trace module tc = GraphTypeChecker symbolic_traced tc type_check infer_symbolic_types symbolic_traced r = Refine symbolic_traced r refine assert r constraints == Equality Equality Equality note there no equality constraint between dyn because dyn could infer_symbolic_types symbolic_traced expected_ph_types = TensorType sympy symbols ~ TensorType TensorType sympy symbols ~ TensorType sympy symbols ~ expected_iter = iter expected_ph_types n symbolic_traced graph nodes assert n type == next expected_iter test_symbolic_add_with_broadcast_ M torch nn Module forward x TensorType y TensorType Dyn torch add x y module = M symbolic_traced torch fx GraphModule = symbolic_trace module tc = GraphTypeChecker symbolic_traced tc type_check infer_symbolic_types symbolic_traced r = Refine symbolic_traced r refine expected_ph_types = TensorType TensorType sympy symbols ~ TensorType sympy symbols ~ TensorType sympy symbols ~ expected_iter = iter expected_ph_types n symbolic_traced graph nodes assert n type == next expected_iter test_type_check_conv D_types BasicBlock torch nn Module __init__ inplanes planes stride= super __init__ norm_layer = torch nn BatchNorm d conv = conv x inplanes planes stride bn = norm_layer planes forward x Dyn identity = x out TensorType Dyn = conv x out += identity out B = BasicBlock ast_rewriter = RewritingTracer graph = ast_rewriter trace B traced = GraphModule ast_rewriter root graph gm tc = GraphTypeChecker traced tc type_check infer_symbolic_types traced n traced graph nodes n op == call_module assert isinstance n type __args__ sympy floor assert isinstance n type __args__ sympy floor test_type_check_symbolic_inferenceconv D_maxpool d_flatten BasicBlock torch nn Module __init__ - None super __init__ conv = torch nn Conv d pool = torch nn MaxPool d conv = torch nn Conv d fc = torch nn Linear pool = torch nn AdaptiveAvgPool d forward x TensorType Dyn Dyn out = conv x out = pool out out = conv out out = pool out out = fc out out = pool out out = torch flatten out out B = BasicBlock ast_rewriter = RewritingTracer noqa F traced = symbolic_trace B tc = GraphTypeChecker traced tc type_check infer_symbolic_types traced n traced graph nodes n target == conv assert n type == TensorType sympy floor sympy symbols ~ - sympy floor sympy symbols ~ - n target == conv assert n type == TensorType sympy floor sympy symbols ~ - sympy floor sympy symbols ~ - __name__ == __main__ raise_on_run_directly test test_fx py