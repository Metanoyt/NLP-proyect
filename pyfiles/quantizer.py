mypy allow-untyped-defs abc ABC abstractmethod collections abc Callable dataclasses dataclass field typing Annotated Optional Union torch torch Tensor torch ao quantization ObserverOrFakeQuantize torch ao quantization qconfig _ObserverOrFakeQuantizeConstructor torch fx Node __all__ = Quantizer QuantizationSpecBase QuantizationSpec FixedQParamsQuantizationSpec EdgeOrNode SharedQuantizationSpec DerivedQuantizationSpec QuantizationAnnotation QuantizationSpecBase ABC noqa B Base different types quantization specs allows users specify how quantize Tensor input output Node model dataclass eq=True frozen=True QuantizationSpec QuantizationSpecBase Quantization spec common operators allows user specify how quantize Tensor includes dtype quant_min quant_max etc dtype torch dtype observer fake_quantize constructor such MinMaxObserver PerChannelHistogramObserver etc we can attach some custom args them e g MinMaxObserver with_args eps=eps observer_or_fake_quant_ctr _ObserverOrFakeQuantizeConstructor quant_min Optional int = None quant_max Optional int = None qscheme Optional torch qscheme = None ch_axis Optional int = None is_dynamic bool = False __post_init__ TODO add init quant_min quant_max quant_min must less than quant_max quant_min None quant_max None quant_min quant_max raise ValueError f quant_min quant_min must = quant_max quant_max ch_axis must less than number channels no way check here Just check ch_axis None ch_axis raise ValueError Ch_axis dataclass eq=True frozen=True FixedQParamsQuantizationSpec QuantizationSpecBase dtype torch dtype scale float zero_point int quant_min Optional int = None quant_max Optional int = None qscheme Optional torch qscheme = None is_dynamic bool = False The way we refer other points quantization graph will either input edge output value input edge connection between input node node consuming input so s Tuple Node Node output value fx Node EdgeOrNode = Annotated Union tuple Node Node Node None EdgeOrNode __module__ = torch ao quantization quantizer quantizer dataclass eq=True frozen=True SharedQuantizationSpec QuantizationSpecBase Quantization spec Tensors whose quantization parameters shared other Tensors edge node share observer fake quant instances edge_or_node EdgeOrNode dataclass eq=True frozen=True DerivedQuantizationSpec QuantizationSpecBase Quantization spec Tensors whose quantization parameters derived other Tensors derived_from list EdgeOrNode derive_qparams_fn Callable list ObserverOrFakeQuantize tuple Tensor Tensor dtype torch dtype quant_min Optional int = None quant_max Optional int = None qscheme Optional torch qscheme = None ch_axis Optional int = None is_dynamic bool = False dataclass QuantizationAnnotation How input argument output should quantized expressed QuantizationSpec corresponds how Tensor operator Graph observed PTQ fake quantized QAT map torch fx Node type QuantizationSpecBase input_qspec_map dict Node Optional QuantizationSpecBase = field default_factory=dict How output node quantized expressed QuantizationSpec TODO change value QuantizationSpec separate PR output_qspec Optional QuantizationSpecBase = None For Node node edge node node since they observing same Tensor we may want implicitly share observers flag allows people turn off behavior output node allow_implicit_sharing bool = True whether node annotated _annotated bool = False Quantizer ABC transform_for_annotation model torch fx GraphModule - torch fx GraphModule Allows user defined transforms run before annotating graph This allows quantizer allow quantizing part model otherwise quantizable For example quantizer can decompose compound operator like scaled dot product attention into bmm softmax quantizer knows how quantize bmm softmax sdpa b transform scalars tensor allow quantizing scalares Note optional method model annotate nodes graph observer fake quant constructors convey desired way quantization abstractmethod annotate model torch fx GraphModule - torch fx GraphModule pass validate annotated graph supported backend abstractmethod validate model torch fx GraphModule - None pass prepare_obs_or_fq_callback model torch fx GraphModule edge_or_node_to_obs_or_fq dict EdgeOrNode ObserverOrFakeQuantize - None A callback will called after observers fake quants created each sharing group before they inserted into graph The callback can used make final quantization adjustments such enforcing specific scale zero point model input output Args ` model ` graph module being prepared ` edge_or_node_to_obs_or_fq ` dictionary mapping each annotated edge node corresponding observer fake quant object Note multiple edges nodes can map same observer fake quant instance they annotated SharedQuantizationSpec This dictionary can modified callback