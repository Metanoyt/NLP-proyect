Owner s oncall distributed unittest collections deque OrderedDict contextlib ContextDecorator contextmanager nullcontext copy deepcopy functools partial torch torch nn nn torch distributed _composable checkpoint torch testing _internal common_cuda TEST_CUDA torch testing _internal common_utils run_tests TEST_XPU TestCase torch utils checkpoint CheckpointError device_type = acc type acc = torch accelerator current_accelerator cpu MemoryDelta ContextDecorator __init__ device torch device device torch device = device active_memory_enter int = active_memory_exit int = __enter__ active_memory_enter = torch accelerator memory_stats active_bytes all current device type == cuda device type == xpu __exit__ exc active_memory_exit = torch accelerator memory_stats active_bytes all current device type == cuda device type == xpu delta - int active_memory_exit - active_memory_enter ToyModel nn Module __init__ - None super __init__ l = nn Linear seq = nn Sequential nn ReLU nn Linear nn ReLU forward x seq l x RandomModel nn Module __init__ - None super __init__ p = nn Parameter torch randn forward x y = torch matmul p torch randn device=self p device torch matmul x y MultiOutputModel nn Module __init__ device torch device super __init__ w = nn Parameter torch randn device=device w = nn Parameter torch randn device=device forward x torch Tensor - tuple torch Tensor torch Tensor z = x w z = nn functional relu z z = z w z sin z cos MultiInputModel nn Module __init__ device torch device super __init__ w = nn Parameter torch randn device=device forward xs tuple torch Tensor torch Tensor - torch Tensor assert len xs == f Expects args got len xs x y = xs z = x + y z = z w nn functional relu z TestCheckpoint TestCase _get_graph_size out torch Tensor - int q = deque out grad_fn num_functions = while len q fn = q pop num_functions += next_fn _ fn next_functions next_fn q append next_fn num_functions _test_tensor_only net nn Module x torch Tensor - None x = x clone x = x clone x requires_grad = True x requires_grad = True net = net net = deepcopy net no checkpoint MemoryDelta x device mem loss = net x sum loss backward checkpoint checkpoint net seq MemoryDelta x device mem loss = net x sum loss backward x is_cuda x is_xpu assertTrue mem delta mem delta p p zip net parameters net parameters assertEqual p grad p grad test_tensor_only_cpu x = torch randn net = ToyModel _test_tensor_only net x unittest skipIf TEST_CUDA TEST_XPU no cuda xpu test_tensor_only_gpu x = torch randn device=f device_type net = ToyModel f device_type _test_tensor_only net x test_random_cpu x = torch randn requires_grad=True x = x clone net = RandomModel net = deepcopy net cpu_rng_state = torch get_rng_state net x sum backward torch set_rng_state cpu_rng_state checkpoint net x sum backward p p zip net parameters net parameters assertEqual p grad p grad test_multi_args Tests checkpoint modules multiple output args hence multiple backward function input args device = torch device cpu net = nn Sequential MultiOutputModel device MultiInputModel device MultiOutputModel device MultiInputModel device net = deepcopy net checkpoint net checkpoint net x = torch randn requires_grad=True x = x clone net x sum backward net x sum backward p p zip net parameters net parameters assertEqual p grad p grad test_clears_state_on_error_in_forward MyModel torch nn Module __init__ raise_in_recomp super __init__ fwd_count = raise_in_recomp = raise_in_recomp = torch nn Linear forward x raise_in_recomp fwd_count == raise RuntimeError foo raise_in_recomp raise first forward raise RuntimeError foo fwd_count += x m = MyModel raise_in_recomp=True m_seq = torch nn Sequential OrderedDict m m checkpoint m_seq m inp = torch randn out = m_seq inp sum Should raise forward recomputation assertRaisesRegex RuntimeError foo out backward Check _ac_generator cleared out assertEqual None checkpoint state m _ac_generator m = MyModel raise_in_recomp=False checkpoint m inp = torch randn Should raise first forward assertRaises RuntimeError m inp assertEqual None checkpoint state m _ac_generator test_checkpoint_kwargs MyModel torch nn Module __init__ raise_exp bool change_shape_in_recomp bool super __init__ fwd_count = raise_exp = raise_exp change_shape_in_recomp = change_shape_in_recomp = torch nn Linear forward x raise_exp fwd_count == raise RuntimeError foo raise_exp fwd_count == raise RuntimeError bar change_shape_in_recomp fwd_count == x relu_ random_tensor = torch randn x = x + random_tensor fwd_count += x m = MyModel True False m m m m = deepcopy m _ range composable checkpoint does support use_reentrant=True assertRaisesRegex NotImplementedError use_reentrant=True supported composable checkpoint Please use torch utils checkpoint checkpoint instead checkpoint m use_reentrant=True check giving unsupported kwarg assertRaisesRegex ValueError Unexpected keyword arguments foo checkpoint m foo= bar handled_fwd_exp = False handled_recomp_exp = False contextmanager fwd_ctx mod MyModel try mod raise_exp = False yield finally nonlocal handled_fwd_exp handled_fwd_exp = True mod raise_exp = True contextmanager recomp_ctx mod MyModel try mod raise_exp = False yield finally nonlocal handled_recomp_exp handled_recomp_exp = True mod raise_exp = True Test different context functions x = torch randn requires_grad=True checkpoint m context_fn=lambda partial fwd_ctx m partial recomp_ctx m m x clone sum backward assertEqual handled_fwd_exp handled_recomp_exp True True checkpoint m context_fn=lambda nullcontext partial recomp_ctx m assertRaisesRegex RuntimeError foo m x clone handled_fwd_exp = False Reset flag checkpoint m context_fn=lambda partial fwd_ctx m nullcontext assertRaisesRegex RuntimeError bar m x clone sum backward assertEqual handled_fwd_exp True Test determinism check failure m = MyModel False True m = deepcopy m Determinism check should throw error autograd should throw RuntimeError checkpoint m determinism_check= none assertRaises RuntimeError m x clone sum backward Determinism check should throw CheckpointError checkpoint m determinism_check= default assertRaises CheckpointError m x clone sum backward Test preserving random state m = MyModel False False m m = deepcopy m _ range checkpoint m preserve_rng_state=False checkpoint m preserve_rng_state=True mi m m m torch manual_seed loss = mi x clone sum torch manual_seed loss backward check m m have least one different grad assertNotEqual p grad p m parameters p grad p m parameters check m m have identical grads p p zip m parameters m parameters assertEqual p grad p grad __name__ == __main__ run_tests