Owner s oncall quantization copy torch torch ao ns _numeric_suite ns torch nn nn torch ao quantization default_qconfig QuantWrapper torch ao quantization _correct_bias _supported_modules _supported_modules_quantized bias_correction get_module get_param parent_child_names torch testing _internal common_quantization QuantizationTestCase skipIfNoFBGEMM torch testing _internal common_utils raise_on_run_directly TestBiasCorrectionEager QuantizationTestCase compute_sqnr x y Ps = torch norm x Pn = torch norm x - y torch log Ps Pn correct_artificial_bias_quantize float_model img_data Adding artificial bias testing bias persists after bias correction This test case changes bias quantized submodule artificial_model = copy deepcopy float_model artificial_model qconfig = default_qconfig torch ao quantization prepare artificial_model inplace=True data img_data artificial_model data torch ao quantization convert artificial_model inplace=True manually changing bias name submodule artificial_model named_modules type submodule _supported_modules x = get_param submodule bias weight = get_param submodule weight x None submodule set_weight_bias weight x data bias_correction float_model artificial_model img_data target_modules=_supported_modules_quantized Trims off shadow module name submodule artificial_model named_modules isinstance submodule ns Shadow parent_name child_name = parent_child_names name parent = get_module artificial_model parent_name parent _modules child_name = submodule orig_module name artificial_submodule artificial_model named_modules type artificial_submodule _supported_modules_quantized submodule = get_module float_model name float_bias = get_param submodule bias artificial_bias = get_param artificial_submodule bias assertTrue compute_sqnr float_bias artificial_bias Correcting quantized bias produced too much noise sqnr score too low skipIfNoFBGEMM test_linear_chain LinearChain nn Module __init__ - None super __init__ linear = nn Linear linear = nn Linear linear = nn Linear forward x x = linear x x = linear x x = linear x x float_model = QuantWrapper LinearChain img_data = torch rand dtype=torch float torch randint dtype=torch long _ range correct_artificial_bias_quantize float_model img_data skipIfNoFBGEMM test_conv_chain ConvChain nn Module __init__ - None super __init__ conv d = nn Conv d conv d = nn Conv d conv d = nn Conv d forward x x = conv d x x = conv d x x = conv d x x float_model = QuantWrapper ConvChain img_data = torch rand dtype=torch float torch randint dtype=torch long _ range correct_artificial_bias_quantize float_model img_data __name__ == __main__ raise_on_run_directly test test_quantization py