usr bin env python Owner s oncall mobile ctypes os unittest torch torch backends _nnapi prepare convert_model_to_nnapi torch testing _internal common_quantized supported_qengines torch testing _internal common_utils run_tests TestCase qpt t scale zero_point dtype=torch quint t = torch tensor t torch quantize_per_tensor t scale zero_point dtype nhwc t t = t clone contiguous memory_format=torch channels_last t nnapi_nhwc = True t unittest skipUnless qnnpack supported_qengines This Pytorch Build has been built does support QNNPACK TestNNAPI TestCase setUp Avoid saturation fbgemm torch backends quantized engine = qnnpack libneuralnetworks_path = os environ get LIBNEURALNETWORKS_PATH libneuralnetworks_path ctypes cdll LoadLibrary libneuralnetworks_path print Will attempt run NNAPI models can_run_nnapi = True can_run_nnapi = False Created easy override subclasses eg TestNnapiBackend call_lowering_to_nnapi traced_module args convert_model_to_nnapi traced_module args Created subclasses set can_run_nnapi eg TestNnapiBackend set_can_run_nnapi can_run can_run_nnapi = can_run check module arg_or_args trace_args=None convert_args=None atol_rtol=None limit=None expected_memory_format=None torch no_grad isinstance arg_or_args torch Tensor args = arg_or_args args = arg_or_args module eval traced = torch jit trace module trace_args args nnapi_module = call_lowering_to_nnapi traced convert_args args can_run_nnapi Only test model converted successfully eager_output = module args nnapi_output = nnapi_module args kwargs = atol_rtol None kwargs atol = atol_rtol kwargs rtol = atol_rtol assertEqual eager_output nnapi_output kwargs limit None mismatches = eager_output int_repr torch int - nnapi_output int_repr torch int mismatches count_nonzero limit Too many mismatches Re-run check no tolerance get nice message assertEqual eager_output nnapi_output atol= rtol= expected_memory_format assertTrue nnapi_output is_contiguous memory_format=expected_memory_format float_and_quant_and_nhwc inp_float scale zero_point torch manual_seed inp_quant = qpt inp_float float inp_float float-nhwc nhwc inp_float quant inp_quant quant-nhwc nhwc inp_quant test_prelu arg = torch tensor - - unsqueeze - unsqueeze - single_a = torch nn PReLU check single_a arg multi_a = torch nn PReLU torch no_grad multi_a weight copy_ torch tensor check multi_a nhwc arg Test flexible size check multi_a arg trace_args= torch zeros convert_args= nhwc torch zeros test_quantize check torch ao nn quantized Quantize torch quint nhwc torch tensor test_dequantize check torch ao nn quantized DeQuantize nhwc qpt test_unsqueeze UnsqueezeModule torch nn Module __init__ dim super __init__ dim = dim forward arg arg unsqueeze dim check UnsqueezeModule - torch randn check UnsqueezeModule - torch randn check UnsqueezeModule torch randn check UnsqueezeModule torch randn check UnsqueezeModule torch randn test_reshape ReshapeModule torch nn Module __init__ shape super __init__ shape = shape forward arg arg reshape shape check ReshapeModule torch randn check ReshapeModule - nhwc torch randn assertRaisesRegex Exception target size check ReshapeModule nhwc torch randn test_flatten mod torch nn Flatten torch nn Flatten start_dim= end_dim= torch nn Flatten start_dim= end_dim= torch nn Flatten start_dim= end_dim=- torch nn Flatten start_dim= end_dim= check mod torch randn flex inputs check torch nn Flatten torch randn convert_args= torch zeros channels last check torch nn Flatten nhwc torch randn check torch nn Flatten nhwc torch randn Exceptions assertRaisesRegex Exception supported NHWC check torch nn Flatten nhwc torch randn assertRaisesRegex Exception Flattening flexible dims supported yet check torch nn Flatten torch randn assertRaisesRegex Exception Only dim check torch nn Flatten start_dim= end_dim=- torch randn test_slice SliceModule torch nn Module __init__ start stop step super __init__ start = start stop = stop step = step forward t t start stop step SliceModule torch nn Module forward t t check SliceModule torch randn check SliceModule torch randn flex inputs check SliceModule torch randn convert_args= torch zeros assertRaisesRegex Exception slice flexible shape check SliceModule torch randn convert_args= torch zeros test_cat CatModule torch nn Module __init__ dim super __init__ dim = dim forward t t torch cat t t dim check CatModule torch randn torch randn check CatModule torch randn torch randn check CatModule nhwc torch randn nhwc torch randn check CatModule torch randn torch randn convert_args= torch zeros torch zeros test_pointwise_unary op relu sigmoid subTest op UnaryModule torch nn Module forward arg op == relu torch nn functional relu arg op == sigmoid torch sigmoid arg raise Exception Bad op noqa TRY check UnaryModule torch tensor - check UnaryModule qpt torch tensor - test_pointwise_binary op add sub mul div subTest op BinaryModule torch nn Module forward lhs rhs op == add lhs + rhs op == sub lhs - rhs op == mul lhs rhs op == div lhs rhs raise Exception Bad op noqa TRY check BinaryModule torch tensor torch tensor check BinaryModule torch tensor torch tensor assertRaisesRegex Exception Non-equal-rank broadcast check BinaryModule torch tensor torch tensor test_pointwise_binary_const const = torch randn ArgPlusConst torch nn Module forward arg arg + const ConstPlusArg torch nn Module forward arg const + arg arg_contig = torch randn arg_nhwc = nhwc torch randn mod_class ArgPlusConst ConstPlusArg use_nhwc False True subTest mod_class=mod_class __name__ use_nhwc=use_nhwc arg = arg_nhwc use_nhwc arg_contig memory_format = torch channels_last use_nhwc torch contiguous_format check mod_class arg expected_memory_format=memory_format test_hardtanh inp = torch tensor - - check torch nn Hardtanh inp check torch nn Hardtanh inp assertRaisesRegex Exception hardtanh args check torch nn Hardtanh inp test_softmax inp = torch tensor - - check torch nn Softmax inp check torch nn Softmax dim= inp Test flexible size check torch nn Softmax inp convert_args= torch zeros test_to ToCPU torch nn Module __init__ - None super __init__ prelu = torch nn PReLU forward x y = x cpu add prelu since input operand can t output prelu y arg = torch randn check ToCPU arg Test flexible size check ToCPU arg convert_args= torch zeros test_detach DetachModule torch nn Module forward x y = x detach torch nn functional relu y check DetachModule torch randn check DetachModule torch randn convert_args= torch zeros test_log_softmax inp = torch randn check torch nn LogSoftmax inp check torch nn LogSoftmax inp test_mean MeanModule torch nn Module __init__ dim keep=False super __init__ dim = dim keep = keep forward t torch mean t dim=self dim keepdim=self keep check MeanModule torch randn check MeanModule torch randn check MeanModule torch randn check MeanModule nhwc torch randn check MeanModule - - nhwc torch randn check MeanModule - - keep=True nhwc torch randn test_max_pool d name inp float_and_quant_and_nhwc torch randn subTest name check torch nn MaxPool d inp check torch nn MaxPool d inp check torch nn MaxPool d inp test_avg_pool d name inp float_and_quant_and_nhwc torch randn subTest name atol_rtol = None limit = None convert_dims = convert_arg = torch zeros convert_dims model torch nn AvgPool d torch nn AvgPool d torch nn AvgPool d quant name atol_rtol = limit = model inp numel convert_arg = qpt torch zeros convert_dims nhwc name convert_arg = nhwc convert_arg check model inp atol_rtol=atol_rtol limit=limit check model inp convert_args= convert_arg atol_rtol=atol_rtol limit=limit test_adaptive_avg_pool d name inp float_and_quant_and_nhwc torch randn subTest name check torch nn AdaptiveAvgPool d inp assertRaisesRegex Exception output size check torch nn AdaptiveAvgPool d inp test_upsample_nearest d convert_args = dict float_and_quant_and_nhwc torch randn name inp float_and_quant_and_nhwc torch randn subTest name check torch nn UpsamplingNearest d size= inp check torch nn UpsamplingNearest d size= inp check torch nn UpsamplingNearest d size= inp check torch nn UpsamplingNearest d scale_factor= inp check torch nn UpsamplingNearest d scale_factor= inp check torch nn UpsamplingNearest d scale_factor= inp check torch nn UpsamplingNearest d size= inp convert_args= convert_args name check torch nn UpsamplingNearest d scale_factor= inp convert_args= convert_args name test_linear torch manual_seed check torch nn Linear torch randn check torch nn Linear torch randn convert_args= torch zeros test_conv d cases = in_ch out_ch kernel stride padding groups bias input_dim name x noqa E E x nobias noqa E E x p noqa E E x s noqa E E x noqa E E x dw noqa E E x noqa E E kind float float-nhwc quant quant-nhwc case cases in_ch out_ch kernel stride padding groups bias input_dim name = case subTest f kind - name inp = torch randn input_dim model = torch nn Conv d in_ch out_ch kernel stride padding groups=groups bias=bool bias output_size = model inp numel atol_rtol = None limit = None convert_dims = in_ch convert_arg = torch zeros convert_dims quant kind model = torch nn Sequential model model eval model qconfig = torch ao quantization get_default_qconfig qnnpack model = torch ao quantization prepare model model inp model = torch ao quantization convert model inp = qpt inp I ve seen numerical differences between QNNPACK NNAPI never more than quantum never more than ~ output test atol_rtol = limit = output_size convert_arg = qpt torch zeros convert_dims nhwc kind inp = nhwc inp convert_arg = nhwc convert_arg check model inp atol_rtol=atol_rtol limit=limit check model inp convert_args= convert_arg atol_rtol=atol_rtol limit=limit test_conv d_transpose torch manual_seed in_ch out_ch kernel = input_dim = convert_dims = input_dim + kind float float-nhwc quant quant-nhwc subTest kind inp = torch randn input_dim model = torch nn ConvTranspose d in_ch out_ch kernel output_size = model inp numel atol_rtol = limit = None convert_arg = torch zeros convert_dims quant kind model = torch ao nn quantized ConvTranspose d in_ch out_ch kernel model qconfig = torch ao quantization get_default_qconfig qnnpack inp = qpt inp I ve seen numerical differences between QNNPACK NNAPI never more than quantum never more than ~ output test atol_rtol = limit = output_size convert_arg = qpt convert_arg nhwc kind inp = nhwc inp convert_arg = nhwc convert_arg check model inp atol_rtol=atol_rtol limit=limit check model inp convert_args= convert_arg atol_rtol=atol_rtol limit=limit test_qadd func = torch ao nn quantized QFunctional func scale = func zero_point = AddMod torch nn Module forward lhs rhs func add lhs rhs AddReluMod torch nn Module forward lhs rhs func add_relu lhs rhs MulMod torch nn Module forward lhs rhs func mul lhs rhs name mod add AddMod add_relu AddReluMod mul MulMod subTest name check mod qpt qpt check mod qpt qpt convert_args= qpt qpt torch zeros check mod qpt qpt convert_args= qpt torch zeros qpt check mod qpt qpt convert_args= qpt torch zeros qpt torch zeros NOTE NNAPI qadd supports broadcast PT does test_qlinear torch manual_seed weight = qpt torch randn torch qint bias = torch randn mod = torch ao nn quantized Linear mod set_weight_bias weight bias inp = qpt torch randn torch quint check mod inp test_seblock_mul MulModel torch nn Module forward lhs rhs lhs rhs check MulModel nhwc torch randn torch randn test_multi_output MultiModel torch nn Module forward lhs rhs - tuple torch Tensor torch Tensor the_sum = lhs + rhs the_diff = lhs - rhs the_sum the_diff check MultiModel torch tensor torch tensor __name__ == __main__ run_tests