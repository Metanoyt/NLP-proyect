mypy allow-untyped-defs logging math dataclasses dataclass functools lru_cache typing Optional torch torch distributed _functional_collectives funcol torch distributed tensor _dtensor_spec dtensor_spec torch _C _distributed_c d _resolve_process_group torch _logging warning_once torch distributed _local_tensor local_tensor_mode maybe_run_for_local_tensor torch distributed device_mesh _mesh_resources DeviceMesh torch distributed distributed_c d _get_group_size_by_name broadcast get_group_rank get_rank ProcessGroup scatter Work logger = logging getLogger __name__ torch library register_fake _dtensor shard_dim_alltoall _shard_dim_alltoall_meta input gather_dim shard_dim group_name group_size = _get_group_size_by_name group_name stacked_list = torch empty_like input _ range group_size group = _resolve_process_group group_name group_rank = get_group_rank group get_rank torch cat stacked_list dim=gather_dim chunk group_size dim=shard_dim group_rank contiguous shard_dim_alltoall input gather_dim shard_dim mesh mesh_dim mesh device_type == cpu local_tensor_mode None Gloo does support alltoall so falling back allgather + chunk warning_once logger CPU process group does support alltoall yet falling back allgather + chunk out = funcol all_gather_tensor input gather_dim mesh mesh_dim isinstance out funcol AsyncCollectiveTensor stick same behavior alltoall case remove once we enable alltoall async out = out wait out = torch chunk out mesh size mesh_dim dim=shard_dim mesh get_local_rank mesh_dim out contiguous group_name = funcol _resolve_group_name mesh mesh_dim TODO enable async op shard_dim_alltoall torch ops _dtensor shard_dim_alltoall input gather_dim shard_dim group_name mesh_scatter output torch Tensor scatter_list list torch Tensor mesh DeviceMesh mesh_dim int = async_op bool = False group_src int = - Optional Work scatter list tensors device mesh dimension We default use first rank mesh dimension source truth i e d mesh we scatter mesh_dim = we will scatter tensor list rank rank tensor list rank rank Args output torch Tensor tensor receive scattered list scatter_list List torch Tensor tensor list scattered mesh_dim int optional indicate which mesh dimension we want scatter we default choose first rank mesh dimension source truth Keyword args group_src int optional group rank source data logical global tensor specific mesh dimension By default we use ` ` group_rank= ` ` each DeviceMesh dimension source data preserve single-device semantic If passing ` ` None ` ` explicitly method simply uses its local data no communication Returns A ` Work ` object TODO Ideally we should use meta tensor way register meta kernel collective op so would avoid communication Need remove check below once done output is_meta None dim_group = mesh get_group mesh_dim assert isinstance dim_group ProcessGroup group_src == get_rank dim_group fut = scatter output scatter_list=scatter_list group=dim_group async_op=async_op group_src=group_src fut = scatter output scatter_list=None group=dim_group async_op=async_op group_src=group_src fut mesh_broadcast tensor torch Tensor mesh DeviceMesh mesh_dim int = async_op bool = False group_src int = - Optional Work broadcast tensor device mesh dimension We default use first rank mesh dimension source truth i e d mesh we broadcast mesh_dim = we will broadcast tensor rank rank tensor rank rank Args tensor torch Tensor tensor broadcast mesh_dim int optional indicate which mesh dimension we want scatter we default choose first rank mesh dimension source truth Keyword args group_src int optional group rank source data logical global tensor specific mesh dimension By default we use ` ` group_rank= ` ` each DeviceMesh dimension source data preserve single-device semantic If passing ` ` None ` ` explicitly method simply uses its local data no communication Returns A ` Work ` object TODO Ideally we should use meta tensor way register meta kernel collective op so would avoid communication Need remove check below once done tensor is_meta None dim_group = mesh get_group mesh_dim assert isinstance dim_group ProcessGroup broadcast tensor group=dim_group async_op=async_op group_src=group_src maybe_run_for_local_tensor pad_tensor tensor torch Tensor pad_dim int pad_size int - torch Tensor pad_size == tensor pad = tensor ndim - pad_dim pad - = pad_size torch nn functional pad tensor pad maybe_run_for_local_tensor unpad_tensor tensor torch Tensor pad_dim int pad_size int - torch Tensor pad_size == tensor tensor narrow pad_dim start= length=tensor size pad_dim - pad_size fill_empty_tensor_to_shards shards list torch Tensor shard_dim int num_empty_tensors int - list torch Tensor num_empty_tensors == shards tensor_size = list shards size tensor_size shard_dim = tensor = shards new_zeros tensor_size shards extend tensor _ range num_empty_tensors shards check_tensor_meta local_tensor check_shape_stride=False - Optional dtensor_spec TensorMeta local_metadata = dtype local_tensor dtype requires_grad local_tensor requires_grad check_shape_stride local_metadata update shape local_tensor shape stride local_tensor stride gathered_metadata = None _ range torch distributed get_world_size torch distributed all_gather_object gathered_metadata local_metadata Check metadata consistent across ranks all meta == local_metadata meta gathered_metadata raise ValueError Inconsistent tensor metadata including shape stride across ranks None spec_to_bytes spec dtensor_spec DTensorSpec - int assert spec tensor_meta None spec should have tensor meta defined spec tensor_meta dtype itemsize math prod spec shape dataclass MeshTopoInfo Mesh information collective cost estimation mesh DeviceMesh mesh_dim_devices list int mesh_dim_bandwidth list float mesh_dim_latency list float staticmethod lru_cache None build_from_mesh mesh DeviceMesh - MeshTopoInfo Generate mesh topology info intra-host inter-host communication pattern Note we made bunch assumptions simplicity we assume mesh homogeneous s gpu nccl model we assume gpu arch Ampere Hopper we assume collectives all ring base algo now num_devices_per_host = _mesh_resources num_devices_per_host mesh device_type base bw number intra-node GB s base_bw = mesh_dim_bandwidth = base_bw mesh ndim latency terms us intra-node nv-link mesh_dim_latency = mesh ndim mesh_dim_devices = mesh ndim total_num_devices = mesh_dim reversed range mesh ndim num_devices = mesh size mesh_dim mesh_dim_devices mesh_dim = num_devices total_num_devices = num_devices total_num_devices num_devices_per_host magic number inter-host communication bandwidth latency factor This number assumes latest GPU arch i e Ampere Hopper TODO see we need tweak offer way user specify bandwidths latency mesh_dim_bandwidth mesh_dim = set ethernet latency inter-host mesh_dim_latency mesh_dim = MeshTopoInfo mesh mesh_dim_devices mesh_dim_bandwidth mesh_dim_latency allgather_cost bytes_gb float mesh_topo MeshTopoInfo mesh_dim int - float num_devices_on_mesh_dim = mesh_topo mesh_dim_devices mesh_dim mesh_dim_bandwidth = mesh_topo mesh_dim_bandwidth mesh_dim num_hops = num_devices_on_mesh_dim - base latency + comm latency latency = + num_hops mesh_topo mesh_dim_latency mesh_dim us bw = bytes_gb num_hops num_devices_on_mesh_dim mesh_dim_bandwidth s latency + bw e rescale us allreduce_cost bytes_gb float mesh_topo MeshTopoInfo mesh_dim int - float num_devices_on_mesh_dim = mesh_topo mesh_dim_devices mesh_dim mesh_dim_bandwidth = mesh_topo mesh_dim_bandwidth mesh_dim allreduce have almost x comm bytes compare allgather reduce_scatter num_hops = num_devices_on_mesh_dim - latency = + num_hops mesh_topo mesh_dim_latency mesh_dim bw = bytes_gb num_hops num_devices_on_mesh_dim mesh_dim_bandwidth latency + bw e reduce_scatter_cost bytes_gb float mesh_topo MeshTopoInfo mesh_dim int - float num_devices_on_mesh_dim = mesh_topo mesh_dim_devices mesh_dim mesh_dim_bandwidth = mesh_topo mesh_dim_bandwidth mesh_dim num_hops = num_devices_on_mesh_dim - base latency + comm latency latency = + num_hops mesh_topo mesh_dim_latency mesh_dim bw = bytes_gb num_hops num_devices_on_mesh_dim mesh_dim_bandwidth latency + bw e redistribute_cost current_spec dtensor_spec DTensorSpec target_spec dtensor_spec DTensorSpec - float This function returns cost redistribute current target DTensorSpec NOTE Only consider communication cost here since computation costs redistribute quite trivial i e we only need narrow simple division Only consider redistribute cost same mesh cross mesh communication cost quite needed operator strategy estimation selection current_spec mesh = target_spec mesh make infinite cost meshes same TODO see we want support once there s cross mesh communication float inf current_spec is_replicated short-cut comm cost current spec already full replication mesh_topo = MeshTopoInfo build_from_mesh current_spec mesh cost = comm_bytes_gb = spec_to_bytes current_spec current_spec num_shards Transformation considered redistribute cost allgather alltoall allreduce reduce_scatter i current target enumerate zip current_spec placements target_spec placements current == target continue num_devices_on_mesh_dim = mesh_topo mesh_dim_devices i current is_shard target is_replicate allgather gives larger comm bytes comm_bytes_gb = num_devices_on_mesh_dim add up allgather comm cost cost += allgather_cost comm_bytes_gb mesh_topo i current is_shard target is_shard should alltoall comm since we haven t implement yet add penalty favor allgather instead cost += allgather_cost comm_bytes_gb mesh_topo i + current is_partial target is_replicate add up allreduce comm cost cost += allreduce_cost comm_bytes_gb mesh_topo i current is_partial target is_shard add up reduce_scatter comm cost cost += reduce_scatter_cost comm_bytes_gb mesh_topo i after reduce_scatter comm bytes further collectives halved comm_bytes_gb = num_devices_on_mesh_dim current is_shard target is_partial ban shard - partial does make sense perform redistribute float inf cost