This module provides TVM backend integration TorchDynamo Apache TVM deep learning compiler framework can optimize execute models various hardware backends This module enables - Compilation PyTorch models TVM s computation graphs - Multiple scheduling options - Default scheduler - Auto-scheduler automatic optimization - Meta-schedule evolutionary search-based tuning - Hardware-specific optimizations - CUDA GPU support - CPU support LLVM targeting architecture-specific tuning - Automatic detection CPU capabilities AVX AVX - Tensor conversion utilities between PyTorch TVM formats - Configurable optimization levels tuning trials The backend can used torch compile model = torch compile model backend= tvm functools importlib logging os sys tempfile collections abc Callable types MappingProxyType typing Any Optional torch torch fx common device_from_inputs fake_tensor_unsupported registry register_backend log = logging getLogger __name__ register_backend fake_tensor_unsupported type ignore arg-type tvm gm fx GraphModule example_inputs list torch Tensor options Optional MappingProxyType str Any = None - Callable Any options None options = MappingProxyType scheduler None trials opt_level assert options None tvm type ignore tvm relay type ignore tvm contrib graph_executor type ignore jit_mod = torch jit trace gm example_inputs device = device_from_inputs example_inputs shape_list = f inp_ idx i shape idx i enumerate example_inputs example_outputs = gm example_inputs len example_outputs == log warning Explicitly fall back eager due zero output gm forward mod params = relay frontend from_pytorch jit_mod shape_list device type == cuda dev = tvm cuda device index target = tvm target cuda dev = tvm cpu target = tvm target Target llvm_target scheduler = options get scheduler None scheduler None scheduler = os environ get TVM_SCHEDULER None trials = options get trials opt_level = options get opt_level scheduler == auto_scheduler pyrefly ignore import-error tvm auto_scheduler log_file = tempfile NamedTemporaryFile pyrefly ignore bad-argument-type os path exists log_file tasks task_weights = auto_scheduler extract_tasks mod main params target len tasks = tuner = auto_scheduler TaskScheduler tasks task_weights pyrefly ignore bad-argument-type os path exists log_file assert trials tune_option = auto_scheduler TuningOptions num_measure_trials=trials measure_callbacks= auto_scheduler RecordToFile log_file early_stopping= try tuner tune tune_option except Exception pyrefly ignore bad-argument-type os path exists log_file pyrefly ignore bad-argument-type os unlink log_file raise auto_scheduler ApplyHistoryBest log_file tvm transform PassContext opt_level=opt_level config= relay backend use_auto_scheduler True lib = relay build mod target=target params=params scheduler == meta_schedule pyrefly ignore import-error tvm meta_schedule ms tempfile TemporaryDirectory work_dir device type = cuda meta_schedule needs num-cores specified here we use maximum core count target = tvm target Target f llvm_target -- num-cores ms utils cpu_count logical=False TODO shingjan This could replaced tvm contrib torch optimize_torch once USE_PT_TVMDSOOP updated turned default TVM assert trials database = ms relay_integration tune_relay mod=mod target=target work_dir=work_dir max_trials_global=trials num_trials_per_iter= params=params strategy= evolutionary opt_level=opt_level lib = ms relay_integration compile_relay database=database mod=mod target=target params=params opt_level=opt_level scheduler == default scheduler no autotuning tvm transform PassContext opt_level=opt_level lib = relay build mod target=target params=params raise NotImplementedError This tuning option invalid implemented torchdynamo s TVM-related backend There three available options default auto_scheduler meta_schedule m = graph_executor GraphModule lib default dev to_torch_tensor nd_tensor tvm nd array - torch Tensor A helper function transfer NDArray torch tensor nd_tensor dtype == bool DLPack does support boolean so can t handled torch utils dlpack from_pack Workaround going through numpy although brings additional data copy overhead torch from_numpy nd_tensor numpy torch utils dlpack from_dlpack nd_tensor to_dlpack to_tvm_tensor torch_tensor torch Tensor - tvm nd array A helper function transfer torch tensor NDArray torch_tensor dtype == torch bool same reason above fallback numpy conversion which could introduce data copy overhead tvm nd array torch_tensor cpu numpy tvm nd from_dlpack torch_tensor exec_tvm i_args torch Tensor - list torch Tensor args = contiguous i_args shape_info _ = m get_input_info active_inputs = name name _ shape_info items idx arg enumerate args arg dim = arg requires_grad arg = arg detach inp_name = f inp_ idx inp_name active_inputs log warning input s skipped found tvm s runtime library inp_name continue m set_input inp_name to_tvm_tensor arg m run to_torch_tensor m get_output i i range m get_num_outputs exec_tvm tvm_meta_schedule = functools partial tvm scheduler= meta_schedule tvm_auto_scheduler = functools partial tvm scheduler= auto_scheduler has_tvm - bool try importlib import_module tvm True except ImportError False functools cache llvm_target - str sys platform == linux cpuinfo = open proc cpuinfo read avx cpuinfo llvm -mcpu=skylake-avx avx cpuinfo llvm -mcpu=core-avx llvm