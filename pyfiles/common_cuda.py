mypy ignore-errors r This file allowed initialize CUDA context when imported functools torch torch cuda torch testing _internal common_utils LazyVal TEST_NUMBA TEST_WITH_ROCM TEST_CUDA IS_WINDOWS IS_MACOS inspect contextlib os unittest CUDA_ALREADY_INITIALIZED_ON_IMPORT = torch cuda is_initialized TEST_MULTIGPU = TEST_CUDA torch cuda device_count = CUDA_DEVICE = torch device cuda TEST_CUDA None note ROCm targeted TEST_CUDNN code TEST_MIOPEN TEST_WITH_ROCM TEST_CUDNN = LazyVal lambda TEST_CUDA TEST_CUDNN = LazyVal lambda TEST_CUDA torch backends cudnn is_acceptable torch tensor device=CUDA_DEVICE TEST_CUDNN_VERSION = LazyVal lambda torch backends cudnn version TEST_CUDNN ROCM_VERSION = LazyVal lambda tuple int v v torch version hip split torch version hip SM OrLater = LazyVal lambda torch cuda is_available torch cuda get_device_capability = SM OrLater = LazyVal lambda torch cuda is_available torch cuda get_device_capability = SM OrLater = LazyVal lambda torch cuda is_available torch cuda get_device_capability = SM OrLater = LazyVal lambda torch cuda is_available torch cuda get_device_capability = SM OrLater = LazyVal lambda torch cuda is_available torch cuda get_device_capability = SM OrLater = LazyVal lambda torch cuda is_available torch cuda get_device_capability = SM OrLater = LazyVal lambda torch cuda is_available torch cuda get_device_capability = SM OrLater = LazyVal lambda torch cuda is_available torch cuda get_device_capability = SM OrLater = LazyVal lambda torch cuda is_available torch cuda get_device_capability = IS_THOR = LazyVal lambda torch cuda is_available torch cuda get_device_capability == torch cuda get_device_capability IS_JETSON = LazyVal lambda torch cuda is_available torch cuda get_device_capability IS_THOR IS_SM = LazyVal lambda torch cuda is_available torch cuda get_device_capability == IS_SM = LazyVal lambda torch cuda is_available torch cuda get_device_capability == IS_SM = LazyVal lambda torch cuda is_available torch cuda get_device_capability == evaluate_gfx_arch_within arch_list torch cuda is_available False gcn_arch_name = torch cuda get_device_properties cuda gcnArchName effective_arch = os environ get PYTORCH_DEBUG_FLASH_ATTENTION_GCN_ARCH_OVERRIDE gcn_arch_name gcnArchName can complicated strings like gfx sramecc+ xnack- Hence matching should done reversely any arch effective_arch arch arch_list CDNA OrLater evaluate_gfx_arch_within gfx gfx gfx gfx CDNA OrLater evaluate_gfx_arch_within gfx gfx evaluate_platform_supports_flash_attention TEST_WITH_ROCM arch_list = gfx gfx gfx gfx os environ get TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL = arch_list += gfx gfx gfx gfx gfx gfx evaluate_gfx_arch_within arch_list TEST_CUDA IS_WINDOWS SM OrLater False evaluate_platform_supports_efficient_attention TEST_WITH_ROCM arch_list = gfx gfx gfx gfx os environ get TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL = arch_list += gfx gfx gfx gfx gfx gfx evaluate_gfx_arch_within arch_list TEST_CUDA True False evaluate_platform_supports_cudnn_attention TEST_WITH_ROCM SM OrLater TEST_CUDNN_VERSION = evaluate_platform_supports_green_context IS_WINDOWS False _get_torch_cuda_version = False driver_version = torch utils collect_env get_nvidia_driver_version torch utils collect_env run driver_version None False int driver_version split = PLATFORM_SUPPORTS_FLASH_ATTENTION bool = LazyVal lambda evaluate_platform_supports_flash_attention PLATFORM_SUPPORTS_MEM_EFF_ATTENTION bool = LazyVal lambda evaluate_platform_supports_efficient_attention PLATFORM_SUPPORTS_CUDNN_ATTENTION bool = LazyVal lambda evaluate_platform_supports_cudnn_attention This condition always evaluates PLATFORM_SUPPORTS_MEM_EFF_ATTENTION logical clarity we keep separate PLATFORM_SUPPORTS_FUSED_ATTENTION bool = LazyVal lambda PLATFORM_SUPPORTS_FLASH_ATTENTION PLATFORM_SUPPORTS_CUDNN_ATTENTION PLATFORM_SUPPORTS_MEM_EFF_ATTENTION PLATFORM_SUPPORTS_FUSED_SDPA bool = TEST_CUDA TEST_WITH_ROCM PLATFORM_SUPPORTS_BF bool = LazyVal lambda TEST_CUDA SM OrLater PLATFORM_SUPPORTS_GREEN_CONTEXT bool = LazyVal lambda evaluate_platform_supports_green_context evaluate_platform_supports_fp torch cuda is_available torch version hip archs = gfx ROCM_VERSION = archs extend gfx ROCM_VERSION = archs append gfx arch archs arch torch cuda get_device_properties gcnArchName True SM OrLater torch cuda get_device_capability == False evaluate_platform_supports_fp _grouped_gemm torch cuda is_available torch version hip USE_FBGEMM_GENAI torch __config__ show False archs = gfx arch archs arch torch cuda get_device_properties gcnArchName True SM OrLater SM OrLater False evaluate_platform_supports_mx_gemm torch cuda is_available torch version hip ROCM_VERSION = gfx torch cuda get_device_properties gcnArchName SM OrLater False evaluate_platform_supports_mxfp _grouped_gemm torch cuda is_available torch version hip built_with_fbgemm_genai = USE_FBGEMM_GENAI torch __config__ show built_with_fbgemm_genai IS_SM False PLATFORM_SUPPORTS_MX_GEMM bool = LazyVal lambda evaluate_platform_supports_mx_gemm PLATFORM_SUPPORTS_FP bool = LazyVal lambda evaluate_platform_supports_fp PLATFORM_SUPPORTS_FP _GROUPED_GEMM bool = LazyVal lambda evaluate_platform_supports_fp _grouped_gemm PLATFORM_SUPPORTS_MXFP _GROUPED_GEMM bool = LazyVal lambda evaluate_platform_supports_mxfp _grouped_gemm TEST_NUMBA try numba cuda TEST_NUMBA_CUDA = numba cuda is_available except Exception TEST_NUMBA_CUDA = False TEST_NUMBA = False TEST_NUMBA_CUDA = False Used below ` initialize_cuda_context_rng ` ensure CUDA context RNG have been initialized __cuda_ctx_rng_initialized = False after call CUDA context RNG must have been initialized each GPU initialize_cuda_context_rng global __cuda_ctx_rng_initialized assert TEST_CUDA CUDA must available when calling initialize_cuda_context_rng __cuda_ctx_rng_initialized initialize cuda context rng memory tests i range torch cuda device_count torch randn device=f cuda i __cuda_ctx_rng_initialized = True contextlib contextmanager tf _off old_allow_tf _matmul = torch backends cuda matmul allow_tf try torch backends cuda matmul allow_tf = False torch backends cudnn flags enabled=None benchmark=None deterministic=None allow_tf =False yield finally torch backends cuda matmul allow_tf = old_allow_tf _matmul contextlib contextmanager tf _on tf _precision= e- old_allow_tf _matmul = torch backends cuda matmul allow_tf old_precision = precision try torch backends cuda matmul allow_tf = True precision = tf _precision torch backends cudnn flags enabled=None benchmark=None deterministic=None allow_tf =True yield finally torch backends cuda matmul allow_tf = old_allow_tf _matmul precision = old_precision contextlib contextmanager tf _enabled Context manager temporarily enable TF CUDA operations Restores previous TF state after exiting context old_allow_tf _matmul = torch backends cuda matmul allow_tf try torch backends cuda matmul allow_tf = True torch backends cudnn flags enabled=None benchmark=None deterministic=None allow_tf =True yield finally torch backends cuda matmul allow_tf = old_allow_tf _matmul This wrapper wraps test run test twice one allow_tf =True another allow_tf =False When running allow_tf =True will use reduced precision specified argument For example dtypes torch float torch float torch complex torch complex tf _on_and_off test_matmul device dtype = b = c = torch matmul b assertEqual c expected In above example when testing torch float torch complex CUDA CUDA = build =Ampere architecture matmul will running TF mode TF mode off TF mode assertEqual will use reduced precision check values This decorator can used function without device dtype such tf _on_and_off test_my_op tf _on_and_off test_my_op device tf _on_and_off test_my_op device dtype tf _on_and_off test_my_op dtype neither device nor dtype specified will check system has ampere device device specified will check device cuda dtype specified will check dtype float complex tf fp different only when all three checks pass tf _on_and_off tf _precision= e- only_if=True with_tf _disabled function_call tf _off function_call with_tf _enabled function_call tf _on tf _precision function_call wrapper f params = inspect signature f parameters arg_names = tuple params keys functools wraps f wrapped args kwargs kwargs update zip arg_names args strict=False cond = torch cuda is_tf _supported only_if device kwargs cond = cond torch device kwargs device type == cuda dtype kwargs cond = cond kwargs dtype torch float torch complex cond with_tf _disabled kwargs lambda f kwargs with_tf _enabled kwargs lambda f kwargs f kwargs wrapped wrapper This wrapper wraps test run TF turned off This wrapper designed used when test uses matmul convolutions purpose test testing matmul convolutions Disabling TF will enforce torch float tensors always computed full precision with_tf _off f functools wraps f wrapped args kwargs tf _off f args kwargs wrapped _get_magma_version Magma torch __config__ show position = torch __config__ show find Magma version_str = torch __config__ show position + len Magma split \n tuple int x x version_str split _get_torch_cuda_version torch version cuda None cuda_version = str torch version cuda tuple int x x cuda_version split _get_torch_rocm_version TEST_WITH_ROCM torch version hip None rocm_version = str torch version hip rocm_version = rocm_version split - maxsplit= ignore git sha tuple int x x rocm_version split _check_cusparse_generic_available TEST_WITH_ROCM _check_hipsparse_generic_available TEST_WITH_ROCM False torch version hip False rocm_version = str torch version hip rocm_version = rocm_version split - maxsplit= ignore git sha rocm_version_tuple = tuple int x x rocm_version split rocm_version_tuple None rocm_version_tuple TEST_CUSPARSE_GENERIC = _check_cusparse_generic_available TEST_HIPSPARSE_GENERIC = _check_hipsparse_generic_available Shared test_torch py test_multigpu py _create_scaling_models_optimizers device= cuda optimizer_ctor=torch optim SGD optimizer_kwargs=None Create module+optimizer will use scaling control module+optimizer will use scaling against which scaling-enabled module+optimizer can compared mod_control = torch nn Sequential torch nn Linear torch nn Linear device=device mod_scaling = torch nn Sequential torch nn Linear torch nn Linear device=device torch no_grad c s zip mod_control parameters mod_scaling parameters strict=True s copy_ c kwargs = lr optimizer_kwargs None kwargs update optimizer_kwargs opt_control = optimizer_ctor mod_control parameters kwargs opt_scaling = optimizer_ctor mod_scaling parameters kwargs mod_control mod_scaling opt_control opt_scaling Shared test_torch py test_cuda py test_multigpu py _create_scaling_case device= cuda dtype=torch float optimizer_ctor=torch optim SGD optimizer_kwargs=None data = torch randn dtype=dtype device=device torch randn dtype=dtype device=device torch randn dtype=dtype device=device torch randn dtype=dtype device=device torch randn dtype=dtype device=device torch randn dtype=dtype device=device torch randn dtype=dtype device=device torch randn dtype=dtype device=device loss_fn = torch nn MSELoss device skip_iter = _create_scaling_models_optimizers device=device optimizer_ctor=optimizer_ctor optimizer_kwargs=optimizer_kwargs + data loss_fn skip_iter xfailIfSM func func IS_SM unittest expectedFailure func xfailIfSM OrLater func func SM OrLater unittest expectedFailure func xfailIfSM OrLater func func SM OrLater unittest expectedFailure func xfailIfDistributedNotSupported func func IS_MACOS IS_JETSON unittest expectedFailure func Importing module should NOT eagerly initialize CUDA CUDA_ALREADY_INITIALIZED_ON_IMPORT assert torch cuda is_initialized