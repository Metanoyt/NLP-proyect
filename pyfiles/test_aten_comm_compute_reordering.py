flake noqa B Owner s module inductor unittest unittest mock patch torch torch _dynamo torch _dynamo logging torch _dynamo test_case some reason importing functional collectives after dynamo breaks collectives handling torch distributed _functional_collectives _functional_collectives torch _C FileCheck torch _dynamo utils counters same torch _inductor utils run_and_get_code run_and_get_triton_code torch testing _internal common_distributed _dynamo_dist_per_rank_init at_least_x_gpu DynamoDistributedMultiProcTestCase requires_accelerator_dist_backend aten = torch ops aten functools torch testing _internal common_fsdp get_devtype torch testing _internal common_utils skipIfRocm torch testing _internal inductor_utils HAS_GPU estimate_aten_runtime fx_node compute_multiplier= tests assume matmul can hide single collective c str fx_node target fx_node target == aten mm default compute_multiplier None device_type = str get_devtype apply_reordering_and_get_graph graph out_li - None gm = graph owning_module torch _inductor config aten_distributed_optimizations dist_opts torch _inductor fx_passes overlap_scheduling schedule_overlap_bucketing Read config values only pass non-None values use function defaults kwargs dict str object = config_keys = collective_bucketing max_compute_pre_fetch custom_runtime_estimation insert_overlap_deps key config_keys val = getattr dist_opts key None kwargs key = val schedule_overlap_bucketing gm kwargs gm graph lint out_li append str gm graph run_and_get_aten_graph fn inputs li = apply = functools partial apply_reordering_and_get_graph out_li=li torch _inductor config patch post_grad_custom_post_pass=apply out = fn inputs out li get_patches aten_distributed_optimizations custom_runtime_estimation estimate_aten_runtime reorder_for_locality False triton native_matmul False reorder_for_compute_comm_overlap_passes compile_threads force_disable_caches True Messes up existing test strings aten_distributed_optimizations insert_overlap_deps False interferes testing custom estimation test_configs assume_bucketing_reduces_latency False requires_accelerator_dist_backend TODO somehow inductor bg compile threads causing hangs exit distributed work dtor unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch TestComputeCommReorderingMultiProc DynamoDistributedMultiProcTestCase Run correctness checks multi-proc runner mark minimum GPUs run under Note these tests fork test distributed test_compute_comm_reordering py setUp super setUp torch _dynamo reset torch _dynamo utils counters clear get_world_trs tag ranks list range world_size group_size world_size property world_size - int hack no matter whether we have gpus just run works around issue skipif workers unpredictable #s gpu unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch get_patches test_sink_waits func ar = _functional_collectives all_reduce sum b = torch matmul torch matmul ar b _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu inputs = torch ones dtype=torch float device=device_type + rank out aten_graph_str = run_and_get_aten_graph torch compile func inputs Verify wait_tensor sinked below st matmul above nd matmul FileCheck check all_reduce default check aten mm default check wait_tensor default check aten mm default run aten_graph_str correct = func inputs assertTrue same out correct assertEqual counters inductor overlap_scheduling_exposed torch _inductor config patch get_patches test_raise_comms func b = torch matmul c = torch relu b d = torch matmul c c e = _functional_collectives all_reduce b + sum torch matmul d e _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu inputs = torch ones dtype=torch float device=device_type + rank compiled = torch compile func out aten_graph_str = run_and_get_aten_graph torch compile func inputs Verify all_reduce_ has been raised above nd matmul below st matmul Note all_reduce_ directly writes output buffer st matmul which input first relu Therefore all_reduce_ should scheduled after first relu FileCheck check aten mm check all_reduce default check aten mm check wait_tensor default check aten mm run aten_graph_str out = compiled inputs correct = func inputs assertTrue same out correct assertEqual counters inductor overlap_scheduling_exposed torch _inductor config patch get_patches test_sink_waits_raise_comms func tag ranks group_size b = torch matmul c = torch relu b d = torch matmul c c e = _functional_collectives all_reduce b sum f = torch relu d g = torch matmul f f torch mm e g _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu inputs = torch ones dtype=torch float device=device_type + rank kwargs = get_world_trs func = functools partial func kwargs compiled = torch compile func out aten_graph_str = run_and_get_aten_graph compiled inputs Things verify - The all_reduce_ its prologue should raised above nd matmul below st matmul - The wait_tensor should sinked below rd matmul above th matmul assertExpectedInline aten_graph_str \ graph arg _ num_users= = placeholder target=arg _ mm num_users= = call_function target=torch ops aten mm default args = arg _ arg _ kwargs = relu num_users= = call_function target=torch ops aten relu default args = mm kwargs = all_reduce num_users= = call_function target=torch ops _c d_functional all_reduce default args = mm sum kwargs = mm_ num_users= = call_function target=torch ops aten mm default args = relu relu kwargs = relu_ num_users= = call_function target=torch ops aten relu default args = mm_ kwargs = mm_ num_users= = call_function target=torch ops aten mm default args = relu_ relu_ kwargs = wait_tensor num_users= = call_function target=torch ops _c d_functional wait_tensor default args = all_reduce kwargs = mm_ num_users= = call_function target=torch ops aten mm default args = wait_tensor mm_ kwargs = mm_ Note triggered all_reduce_ bug correct = func inputs get_world_trs assertTrue same out correct assertEqual counters inductor overlap_scheduling_exposed torch _inductor config patch get_patches test_reorder_compute_for_overlap_mul func tag ranks group_size ar = _functional_collectives all_reduce sum ranks tag g = torch matmul c = torch relu d = torch matmul c c f = d c ar fr = _functional_collectives all_reduce f sum ranks tag e = torch matmul d + ar + fr g e _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu inputs = torch ones dtype=torch float device=device_type + rank func_c = functools partial func get_world_trs compiled = torch compile func_c out_c aten_graph_str = run_and_get_aten_graph compiled inputs Note because we have given collectives mms equal estimation we overlap each collective single mm Same schedule test_reorder_compute_for_overlap_custom_runtime_estimation although there exposed collective FileCheck check all_reduce default check aten mm check aten mm check wait_tensor default check aten mul check all_reduce default check wait_tensor default check aten mm run aten_graph_str correct = func inputs get_world_trs assertEqual counters inductor overlap_scheduling_exposed assertTrue same out_c correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skipIfRocm TODO somehow inductor bg compile threads causing hangs exit distributed work dtor patch object torch _inductor config compile_threads unittest skipIf True Logic yet implemented torch _inductor config patch get_patches test_grouped_scheduler_node func tag ranks group_size add = + div = add ar = _functional_collectives all_reduce div sum ranks tag Normally we would fuse ` add = + ` ` div = add ` ` mul = ` together into single fused op here unit test we intentionally put ` add ` ` div ` ` ar ` computation into GroupedSchedulerNode which prevents them being fused any other ops mul = mm = torch matmul mul ar mm _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu inputs = torch ones dtype=torch float device=device_type + rank compiled = torch compile func code = run_and_get_triton_code compiled inputs get_world_trs Expectations ` add = + ` ` div = add ` still fused which means fusion still happens among nodes within GroupedSchedulerNode ` mul = ` fused ` add ` ` div ` because latter two within GroupedSchedulerNode thus prevented being fused any outside ops FileCheck check triton_poi_fused_add_all_reduce_div_ check _c d_functional all_reduce_ check triton_poi_fused_mul_ run code out = compiled inputs get_world_trs correct = func inputs get_world_trs assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch get_patches test_inductor_default_comms_ordering pg_info = get_world_trs tag = pg_info tag ranks = pg_info ranks group_size = pg_info group_size g = torch ones device=device_type g = torch ones device=device_type g = torch ones device=device_type torch compile fn g g g handle = torch ops c d_functional all_reduce g avg tag ranks group_size handle = torch ops c d_functional all_reduce g avg tag ranks group_size handle = torch ops c d_functional all_reduce g avg tag ranks group_size wait them different order grad = torch ops _c d_functional wait_tensor default handle grad = torch ops _c d_functional wait_tensor default handle grad = torch ops _c d_functional wait_tensor default handle grad grad grad _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=True all_reduces remain order note isnt actually invariant pass currently we should keep collectives stable without reordering opportunities _ code = run_and_get_aten_graph fn g g g FileCheck check all_reduce check_same arg _ check all_reduce check_same arg _ check all_reduce check_same arg _ run code assertEqual counters inductor overlap_scheduling_exposed these have no overlap opportunities assertEqual counters inductor overlap_scheduling_bad_exposed unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_overlap_scheduling_via_config Test overlap scheduling enabled via config post_grad pass func ar = _functional_collectives all_reduce sum b = torch matmul torch matmul ar b patches = get_patches aten_distributed_optimizations enable_overlap_scheduling True _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu inputs = torch ones dtype=torch float device=device_type + rank torch _inductor config patch patches compiled_func = torch compile func out code = run_and_get_code compiled_func inputs Verify wait_tensor sinked below matmul FileCheck check all_reduce check mm check wait_tensor check mm run code correct = func inputs assertTrue same out correct assertEqual counters inductor overlap_scheduling_exposed get_bucket_patches compute_multiplier= estimate_aten_runtime_part = functools partial estimate_aten_runtime compute_multiplier=compute_multiplier aten_distributed_optimizations custom_runtime_estimation estimate_aten_runtime_part aten_distributed_optimizations collective_bucketing True reorder_for_locality False triton native_matmul False reorder_for_compute_comm_overlap_passes compile_threads force_disable_caches True messes up test strings aten_distributed_optimizations insert_overlap_deps False interferes testing custom estimation test_configs assume_bucketing_reduces_latency False TestComputeCommReorderingBucketing TestComputeCommReorderingMultiProc unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch get_bucket_patches test_basic_all_gather_bucketing Test independent all_gather operations get bucketed together func b c ranks Three independent all_gathers should bucketed ag = _functional_collectives all_gather_tensor ranks + ag = _functional_collectives all_gather_tensor b ranks + ag = _functional_collectives all_gather_tensor c ranks + ag + ag + ag _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu inputs_a = torch ones dtype=torch float device=device_type + rank inputs_b = torch ones dtype=torch float device=device_type inputs_c = torch ones dtype=torch float device=device_type ranks = list range world_size func_c = functools partial func ranks=ranks compiled = torch compile func_c out aten_graph_str = run_and_get_aten_graph compiled inputs_a inputs_b inputs_c Should see single bucketed all_gather FileCheck check_count torch ops _c d_functional all_gather_into_tensor exactly=True run aten_graph_str correct = func inputs_a inputs_b inputs_c ranks=ranks assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch get_bucket_patches test_reduce_scatter_bucketing Test bucketing reduce_scatter operations func b c rs = _functional_collectives reduce_scatter_tensor sum rs = _functional_collectives reduce_scatter_tensor b sum rs = _functional_collectives reduce_scatter_tensor c sum torch cat rs rs rs _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu inputs_a = torch ones dtype=torch float device=device_type inputs_b = torch ones dtype=torch float device=device_type inputs_c = torch ones dtype=torch float device=device_type out aten_graph_str = run_and_get_aten_graph torch compile func inputs_a inputs_b inputs_c Should bucket reduce_scatter ops FileCheck check_count torch ops _c d_functional reduce_scatter_tensor exactly=True run aten_graph_str TODO debug - ci fails correct = func inputs_a inputs_b inputs_c assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch get_bucket_patches test_no_bucketing_with_dependent_hiding_nodes Test collectives dependent hiding nodes don t get bucketed func b ranks ag could hidden mm ag = _functional_collectives all_gather_tensor ranks mm = torch matmul ag can hidden mm mm depends ag s result ag start mm = torch matmul ag b ag end ag = _functional_collectives all_gather_tensor b ranks ag sum ag sum mm mm _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu inputs_a = torch ones dtype=torch float device=device_type inputs_b = torch ones dtype=torch float device=device_type ranks = list range world_size func_c = functools partial func ranks=ranks compiled = torch compile func_c out aten_graph_str = run_and_get_aten_graph compiled inputs_a inputs_b mm depends ag so mm hide ag we can t bucket ag ag because would create dependency issue even though we could bucket them FileCheck check_count torch ops _c d_functional all_gather_into_tensor exactly=True run aten_graph_str correct = func inputs_a inputs_b ranks=ranks assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch get_bucket_patches test_no_bucketing_when_collective_depends_on_hiding_node Test collectives don t get bucketed when one depends another s hiding node func ranks ag hidden mm ag = _functional_collectives all_gather_tensor ranks mm = torch matmul ag depends mm which hides ag b = mm ag = _functional_collectives all_gather_tensor b ranks ag sum ag sum mm _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu inputs = torch ones dtype=torch float device=device_type ranks = list range world_size func_c = functools partial func ranks=ranks compiled = torch compile func_c out aten_graph_str = run_and_get_aten_graph compiled inputs ag depends mm ag s hiding node so they can t bucketed FileCheck check_count _c d_functional all_gather_into_tensor exactly=True run aten_graph_str correct = func inputs ranks=ranks assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch get_bucket_patches test_bucketing_wait_sink Test independent all-gathers split bucketed func b c d ranks All all-gathers independent - COULD bucketed together ag = _functional_collectives all_gather_tensor ranks ag = _functional_collectives all_gather_tensor b ranks ag = _functional_collectives all_gather_tensor c ranks ag = _functional_collectives all_gather_tensor d ranks First compute - can hide ag ag e = mm = torch matmul e e T Second compute - can hide ag ag f = b mm = torch matmul f f T Use all collective results result = ag sum + ag sum + ag sum + ag sum + mm sum + mm sum result _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu = torch ones dtype=torch float device=device_type b = torch ones dtype=torch float device=device_type c = torch ones dtype=torch float device=device_type d = torch ones dtype=torch float device=device_type ranks = list range world_size func_c = functools partial func ranks=ranks compiled = torch compile func_c out aten_graph_str = run_and_get_aten_graph compiled b c d The all gathers can bucketed their waits should sunk below mms FileCheck check_count _c d_functional all_gather_into_tensor exactly=True check_count ops aten mm exactly=True check _c d_functional wait_tensor run aten_graph_str correct = func b c d ranks=ranks assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch get_bucket_patches test_bucketing_split_for_overlap_blocking_no_deps Test independent all-gathers split into + buckets better overlap compute func b c d ranks All all-gathers independent - COULD bucketed together ag = _functional_collectives all_gather_tensor ranks ag = _functional_collectives all_gather_tensor b ranks ag = _functional_collectives all_gather_tensor c ranks ag = _functional_collectives all_gather_tensor d ranks First compute - can hide ag ag e = Use avoid fusion mm = torch matmul e e T Force ag ag complete before mm ag ag can still deferred Use first x elements match mm s shape intermediate = ag + ag Second compute - depends ag ag through intermediate can hide ag ag mm = torch matmul mm + intermediate c Use all results result = ag sum + ag sum + ag sum + ag sum + mm sum + mm sum result _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu = torch ones dtype=torch float device=device_type b = torch ones dtype=torch float device=device_type c = torch ones dtype=torch float device=device_type d = torch ones dtype=torch float device=device_type ranks = list range world_size func_c = functools partial func ranks=ranks compiled = torch compile func_c out aten_graph_str = run_and_get_aten_graph compiled b c d The all gathers can bucketed wait should sunk below mms FileCheck check_count _c d_functional all_gather_into_tensor exactly=True check_count ops aten mm exactly=True check_count _c d_functional wait_tensor exactly=True run aten_graph_str correct = func b c d ranks=ranks assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch get_bucket_patches test_bucketing_split_for_overlap Test independent all-gathers split into + buckets better overlap compute func b c d ranks All all-gathers independent - COULD bucketed together ag = _functional_collectives all_gather_tensor ranks ag = _functional_collectives all_gather_tensor b ranks ag = _functional_collectives all_gather_tensor c ranks ag = _functional_collectives all_gather_tensor d ranks First compute - can hide ag ag e = Use avoid fusion mm = torch matmul e e T Force ag ag complete before mm ag ag can still deferred intermediate = ag + ag Small slice minimize compute Second compute - depends ag ag through intermediate can hide ag ag f = b Expand intermediate match mm s shape broadcasting intermediate_expanded = torch nn functional pad intermediate mm = torch matmul mm + intermediate_expanded f T Use all results result = ag sum + ag sum + ag sum + ag sum + mm sum + mm sum result _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu = torch ones dtype=torch float device=device_type b = torch ones dtype=torch float device=device_type c = torch ones dtype=torch float device=device_type d = torch ones dtype=torch float device=device_type ranks = list range world_size func_c = functools partial func ranks=ranks compiled = torch compile func_c out aten_graph_str = run_and_get_aten_graph compiled b c d Should have bucketed all-gathers one ag +ag one ag +ag FileCheck check_count _c d_functional all_gather_into_tensor_out exactly=True run aten_graph_str Verify ordering - first bucket then mm then second bucket then mm FileCheck check _c d_functional all_gather_into_tensor_out check ops aten mm check _c d_functional all_gather_into_tensor_out check ops aten mm run aten_graph_str Verify correctness correct = func b c d ranks=ranks assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch get_bucket_patches test_bucket_exposed_with_hidden_single_overlap Test exposed hidden collectives bucket together when overlap preserved func b c ranks ag will hidden mm ag = _functional_collectives all_gather_tensor ranks ag ag exposed no compute hide them ag = _functional_collectives all_gather_tensor b ranks ag = _functional_collectives all_gather_tensor c ranks can only hide one collective mm = torch matmul T x matmul hides only ag All three can bucket together because bucketing ag ag ag together does prevent ag being hidden mm ag sum + ag sum + ag sum + mm sum _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu = torch ones dtype=torch float device=device_type b = torch ones dtype=torch float device=device_type c = torch ones dtype=torch float device=device_type ranks = list range world_size func_c = functools partial func ranks=ranks compiled = torch compile func_c out aten_graph_str = run_and_get_aten_graph compiled b c Should have bucketed operation containing all all-gathers FileCheck check_count wait_tensor default exactly=True run aten_graph_str Verify bucketed collective overlaps mm FileCheck check functional all_gather_into_tensor check aten mm check wait_tensor run aten_graph_str Verify correctness correct = func b c ranks=ranks assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch get_bucket_patches test_bucketing_split_for_overlap_blocking_deps_inductor Test independent all-gathers split into + buckets better overlap compute check ordering preserved inductor func b c d ranks All all-gathers independent - COULD bucketed together ag = _functional_collectives all_gather_tensor ranks ag = _functional_collectives all_gather_tensor b ranks ag = _functional_collectives all_gather_tensor c ranks ag = _functional_collectives all_gather_tensor d ranks First compute - can hide ag ag e = Use avoid fusion mm = torch matmul e e T Force ag ag complete before mm ag ag can still deferred Use first x elements match mm s shape intermediate = ag + ag Second compute - depends ag ag through intermediate can hide ag ag mm = torch matmul mm + intermediate c Use all results result = ag sum + ag sum + ag sum + ag sum + mm sum + mm sum result li = apply = functools partial apply_reordering_and_get_graph out_li=li _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu torch _inductor config patch aten_distributed_optimizations insert_overlap_deps True torch _inductor config patch post_grad_custom_post_pass=apply = torch ones dtype=torch float device=device_type b = torch ones dtype=torch float device=device_type c = torch ones dtype=torch float device=device_type d = torch ones dtype=torch float device=device_type ranks = list range world_size func_c = functools partial func ranks=ranks compiled = torch compile func_c test_out code = run_and_get_code compiled b c d Check right deps added f = FileCheck _ range f check control_deps check_same all_gather check_same subgraph_mm f check control_deps check_same mm check_same subgraph_wait f run li f = FileCheck _ range f check_count all_gather_into_tensor_out default exactly=True f check_count extern_kernels mm exactly=True f check_count wait_tensor default exactly=True f run code correct = func b c d ranks=ranks assertTrue same test_out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch get_bucket_patches test_custom_estimation_with_fake_tensor_mode Test custom estimation can use FakeTensorMode analysis torch _subclasses fake_tensor FakeTensorMode estimation_calls = estimate_with_fake_mode fx_node compute_multiplier= FakeTensorMode nonlocal estimation_calls estimation_calls += assert isinstance torch rand torch _subclasses FakeTensor patches = get_bucket_patches patches aten_distributed_optimizations custom_runtime_estimation = estimate_with_fake_mode func b ranks Two independent all_gathers should bucketed ag = _functional_collectives all_gather_tensor ranks ag = _functional_collectives all_gather_tensor b ranks Matmul can hide collectives mm = torch matmul ag sum + ag sum + mm sum _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu inputs_a = torch ones dtype=torch float device=device_type inputs_b = torch ones dtype=torch float device=device_type ranks = list range world_size func_c = functools partial func ranks=ranks torch _inductor config patch patches compiled = torch compile func_c out aten_graph_str = run_and_get_aten_graph compiled inputs_a inputs_b Verify custom estimation called assertTrue estimation_calls Custom estimation should have been called correct = func inputs_a inputs_b ranks=ranks assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch get_bucket_patches test_multidtype_bucketing Test all_gathers different dtypes get bucketed together func b c ranks Three all_gathers different dtypes ag = _functional_collectives all_gather_tensor ranks float ag = _functional_collectives all_gather_tensor b ranks float ag = _functional_collectives all_gather_tensor c ranks float Use all results ag sum + ag sum + ag sum _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu = torch ones dtype=torch float device=device_type b = torch ones dtype=torch float device=device_type c = torch ones dtype=torch float device=device_type ranks = list range world_size func_c = functools partial func ranks=ranks compiled = torch compile func_c out aten_graph_str = run_and_get_aten_graph compiled b c Should have bucketed all_gather despite different dtypes FileCheck check_count torch ops _c d_functional wait_tensor default exactly=True run aten_graph_str Verify correctness correct = func b c ranks=ranks assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch get_bucket_patches test_basic_all_reduce_bucketing Test independent all_reduce operations get bucketed together func b c Three independent all_reduces should bucketed ar = _functional_collectives all_reduce sum ar = _functional_collectives all_reduce b sum ar = _functional_collectives all_reduce c sum ar sum + ar sum + ar sum _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu = torch ones dtype=torch float device=device_type + rank b = torch ones dtype=torch float device=device_type c = torch ones dtype=torch float device=device_type compiled = torch compile func out aten_graph_str = run_and_get_aten_graph compiled b c Should see single bucketed all_reduce FileCheck check_count torch ops _c d_functional wait_tensor default exactly=True run aten_graph_str Verify correctness correct = func b c assertTrue same out correct __name__ == __main__ torch _dynamo test_case run_tests run_tests