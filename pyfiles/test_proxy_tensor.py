Owner s module ProxyTensor ruff noqa F torch testing _internal common_utils TestCase run_tests torch torch _dynamo unittest warnings operator collections abc Iterable torch nn utils stateless torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_methods_invocations op_db skip xfail skipOps torch _subclasses fake_tensor DynamicOutputShapeException DataDependentOutputException FakeTensorMode torch _subclasses functional_tensor FunctionalTensor FunctionalTensorMode torch _decomp decomposition_table torch fx experimental symbolic_shapes eval_guards bind_symbols fx_placeholder_vals fx_placeholder_targets guard_int GuardOnDataDependentSymNode torch testing _internal custom_op_db custom_op_db torch testing _internal hop_db hop_db torch testing _internal common_device_type ops torch testing _internal optests optests torch _C _disabled_torch_function_impl torch fx experimental proxy_tensor make_fx DecompositionInterpreter get_isolated_graphmodule torch utils _pytree tree_map torch fx passes runtime_assert insert_deferred_runtime_asserts torch nn torch _functorch config re functools itertools aten = torch ops aten HAS_CUDA = torch cuda is_available strip_end s suffix suffix s endswith suffix s -len suffix s show_guards gm names = strip_end n _ n fx_placeholder_targets gm \n join gm shape_env produce_guards fx_placeholder_vals gm names _simplified=True input_contexts=None process_failures Takes file containing failures like FAILED test test_proxy_tensor py TestProxyTensorOpInfoCPU test_make_fx_symbolic_exhaustive___getitem___cpu_float - RuntimeError aten size default - couldn t find symbolic meta function decomposition noqa B processes them into list opinfo xfails f = open pytest_failures failures = f readlines failures = i strip i failures process_failure_string s matcher out = re search matcher s out groups SYMBOLIC_TRACE_MATCH = r exhaustive_ _cpu failures = process_failure_string s SYMBOLIC_TRACE_MATCH s failures create_normalized_name op op variant_test_name == s = op name s = f op name op variant_test_name s replace _ remap_opinfo = create_normalized_name op op name op variant_test_name op op_db print symbolic_tensor_failures = failure reason failures print f xfail remap_opinfo failure reason print USE_TORCHVISION = False try torchvision USE_TORCHVISION = True except ImportError warnings warn Couldn t torchvision Some our tests use try install commands pytorch org post-fixed ` -- no-deps ` avoid overwriting pytorch installation UserWarning _create_new_input x isinstance x torch Tensor x x dtype = torch float x + x is_leaf torch rand_like x requires_grad=x requires_grad torch rand_like x Delays cos being executed unwraptensor until its used Simulates CommTensor used UnwrapTensor torch Tensor staticmethod __new__ cls tensor torch Tensor r = torch Tensor _make_wrapper_subclass cls tensor size dtype=tensor dtype device=tensor device layout=tensor layout requires_grad=tensor requires_grad r _tensor = tensor r __repr__ TODO consider all_gather local tensors better debugging f UnwrapTensor _tensor __torch_function__ = _disabled_torch_function_impl classmethod __torch_dispatch__ cls func types args= kwargs=None unwrap e ret = e isinstance e UnwrapTensor ret = e _tensor cos ret args = tree_map unwrap args kwargs = tree_map unwrap kwargs func args kwargs TestGenericProxyTensor TestCase WARNING any your inputs index tensors DO NOT use function _test f inps fx_f = make_fx f tracing_mode=self tracing_mode inps new_inps = tree_map _create_new_input inps r = fx_f new_inps r = f new_inps assertEqual r r test_pre_dispatch_mode_stack f b = torch ones torch matmul b We expect see matmul trace - should NOT decomposed into mm Also torch ones doesn t show up trace This annoying expected ones never dispatches Autograd dispatch key so our mode never sees - goes directly BackendSelect key inp = torch ones Test make_fx pre_dispatch=True clears caches properly torch _dispatch python enable_python_dispatcher enable_python_dispatcher out = f inp fx_g = make_fx f pre_dispatch=True inp assertExpectedInline fx_g code strip \ forward a_ ones = torch ops aten ones default device = device type= cpu pin_memory = False matmul = torch ops aten matmul default a_ ones a_ = ones = None matmul test_pre_dispatch_linear f b c torch nn functional linear b c = torch ones b = torch ones c = torch ones fx_g = make_fx f pre_dispatch=True b c out = f b c out = fx_g b c assertEqual out out test_pre_dispatch_no_grad f b = sin torch set_grad_enabled False c = b cos torch set_grad_enabled True b + c sin = torch randn requires_grad=True = detach clone requires_grad_ True a_tmp = detach clone requires_grad_ True fx_g = make_fx f pre_dispatch=True a_tmp out = f out = fx_g assertEqual out out out sum backward out sum backward assertEqual grad grad test_make_fx_simple f x torch sin x _test f torch randn test_scalar_device device= cpu f b + b _test f torch randn device=device torch tensor test_isolated_graphmodule is_any_sum gm any node target == torch ops aten sum default node gm graph nodes is_any_digamma gm any node target == torch ops aten digamma default node gm graph nodes is_any_sigmoid gm any node target == torch ops aten sigmoid default node gm graph nodes inner x torch sum x f x gm = get_isolated_graphmodule inner x assertTrue is_any_sum gm x + torch randn x shape get_isolated_graphmodule uses make_fx internally shouldn t traced outer make_fx call traced = make_fx f torch randn assertFalse is_any_sum traced When factory functions used they should traced outer make_fx call inner_with_factory val = torch tensor float val add_ torch full val sum f x gm = get_isolated_graphmodule inner_with_factory assertTrue is_any_sum gm torch sigmoid x f x gm = get_isolated_graphmodule f x assertFalse is_any_sum gm assertTrue is_any_sigmoid gm torch digamma x traced = make_fx f torch randn assertFalse is_any_sum traced assertFalse is_any_sigmoid traced assertTrue is_any_digamma traced Verify nested make_fx calls don t make factory functions leaked into outer graph Verify ` make_fx ` ` itself does leak its execution f x gm = make_fx f x assertFalse is_any_sum gm assertTrue is_any_sigmoid gm torch digamma x traced = make_fx f torch randn assertFalse is_any_sum traced assertFalse is_any_sigmoid traced assertTrue is_any_digamma traced Verify ` forward ` ` function graph module produced side effect interior ` make_fx ` still traced f x gm = make_fx f x assertFalse is_any_sum gm assertTrue is_any_sigmoid gm ` gm forward ` ` still traced torch digamma gm x traced = make_fx f torch randn assertFalse is_any_sum traced assertTrue is_any_sigmoid traced assertTrue is_any_digamma traced Verify interaction non-ProxyTensor modes torch testing _internal logging_tensor LoggingTensorMode f _logging x LoggingTensorMode gm = get_isolated_graphmodule inner_with_factory assertTrue is_any_sum gm torch sigmoid x f _logging x LoggingTensorMode LoggingTensorMode gm = get_isolated_graphmodule f _logging x assertFalse is_any_sum gm assertTrue is_any_sigmoid gm torch digamma x traced = make_fx f _logging torch randn assertFalse is_any_sum traced assertFalse is_any_sigmoid traced assertTrue is_any_digamma traced Verify interaction another tensor subclass This case currently doesn t work should raise error See https github com pytorch pytorch pull #issuecomment- torch testing _internal logging_tensor LoggingTensor f _logging_tensor x gm = get_isolated_graphmodule inner_with_factory assertTrue is_any_sum gm torch sigmoid x f _logging_tensor x x = LoggingTensor x gm = get_isolated_graphmodule f _logging_tensor x assertFalse is_any_sum gm assertTrue is_any_sigmoid gm torch digamma x traced = make_fx f _logging_tensor torch randn assertFalse is_any_sum traced assertFalse is_any_sigmoid traced fails sigmoid traced LoggingTensor assertTrue is_any_digamma traced See https github com pytorch pytorch issues test_empty_like_doesnt_burn_in_defaults f x torch empty_like x out = make_fx f torch randn assertExpectedInline out code strip \ forward x_ empty_like = torch ops aten empty_like default x_ pin_memory = False x_ = None empty_like test_proxy_tensor_mode_with_decomp_table_preserves_proxy f x y = x new_zeros x size y copy_ x y _new_zeros_decomp inp size dtype=None layout=None device=None pin_memory=None torch zeros size dtype=inp dtype device=inp device factory_func_decomp = torch ops aten new_zeros default _new_zeros_decomp When new_zeros decomposes into torch zero we expect ProxyTensorMode still re-entrantly enabled so ` torch zero ` call returns ProxyTensor out = make_fx f decomposition_table=factory_func_decomp torch ones assertExpectedInline out code \ forward x_ zeros = torch ops aten zeros default dtype = torch float device = device type= cpu pin_memory = False copy_ = torch ops aten copy_ default zeros x_ zeros = x_ = None copy_ test_make_fx_reentrant_dispatch f x torch ops aten norm Scalar x norm_decomp x p= p = raise RuntimeError can t handle p = torch sqrt torch sum torch square x decomp = torch ops aten norm Scalar norm_decomp traced = make_fx f decomposition_table=decomp tracing_mode=self tracing_mode torch rand n traced graph nodes assertTrue square str n target assertTrue norm str n target unittest skipIf USE_TORCHVISION test requires torchvision test_resnet _backward_trace mod = torchvision models resnet An old version test called module directly This works tracing_mode == real fake tensors we also have ensure parameters buffers get wrapped fake tensors because free fake tensors supported Fortunately functional_call does precisely us f x params buffers p params values p grad = None loss = torch func functional_call mod params buffers x sum I could have done functional API there plenty exercising I want show mutating API still works loss backward p grad p params values inp = torch randn _test f inp dict mod named_parameters dict mod named_buffers test_varargs f args sum args _test f torch randn torch randn test_proxy_tensor f_grad x val = x cos cos sum torch autograd grad val x f_backward x val = x cos cos sum val backward x grad f f_grad f_backward _test f torch randn requires_grad=True test_pickle_issue pickle x = torch randn make_fx lambda x x tracing_mode=self tracing_mode x pickle dumps x test_inplace_metadata f x x = x clone x unsqueeze_ - assert x shape - == x _test f torch randn test_mode_tracing_factory_function f x x + torch randn x shape default behavior should trace factory functions traced = make_fx f tracing_mode=self tracing_mode torch randn assertTrue any node target == aten randn default node traced graph nodes test_pre_dispatch_functionalization f x = FunctionalTensorMode pre_dispatch=True export=True x_unwrapped = FunctionalTensor to_functional x y = torch matmul x_unwrapped x_unwrapped y = y + x_unwrapped y mul_ y_unwrapped = torch _from_functional_tensor y elem y_unwrapped torch _dispatch python enable_python_dispatcher enable_python_dispatcher inp = torch randn gm = make_fx f pre_dispatch=True inp TODO actually decompose assertExpectedInline gm code strip \ forward x_ matmul = torch ops aten matmul default x_ x_ add = torch ops aten add Tensor matmul x_ matmul = x_ = None mul = torch ops aten mul Tensor add add = None mul test_pre_dispatch_functionalization_view_op f x = FunctionalTensorMode pre_dispatch=True export=True x_unwrapped = FunctionalTensor to_functional x y = torch matmul x_unwrapped x_unwrapped x_unwrapped = x_unwrapped transpose y = y + x_unwrapped y = y view y_unwrapped = torch _from_functional_tensor y elem y_unwrapped torch _dispatch python enable_python_dispatcher enable_python_dispatcher inp = torch randn gm = make_fx f pre_dispatch=True inp TODO actually decompose assertExpectedInline gm code strip \ forward x_ matmul = torch ops aten matmul default x_ x_ transpose = torch ops aten transpose int x_ x_ = None add = torch ops aten add Tensor matmul transpose matmul = transpose = None view = torch ops aten view default add add = None view test_val_metadata_mutation f x y = x clone y unsqueeze_ y traced = make_fx f tracing_mode=self tracing_mode torch randn requires_grad=True assertEqual tuple node meta val shape node traced graph nodes val node meta test_make_fx_overloads f x x cos + torch randn x shape traced = make_fx f tracing_mode=self tracing_mode torch randn assertTrue all isinstance node target torch _ops OpOverload node traced graph nodes node op == call_function test_tensor_constants f val = torch tensor float inf torch full val _test f test_allclose f b torch allclose b test_f make_fx f tracing_mode=self tracing_mode torch zeros torch zeros tracing_mode = real assertRaises DataDependentOutputException test_f assertRaisesRegex RuntimeError data-dependent test_f test_constant_proxy_tensor_mut f val = torch tensor float val add_ torch full val g = make_fx f tracing_mode=self tracing_mode assertEqual g f In case we mutated shared state g graph assertEqual g f test_constant_unbind f val = torch tensor r = torch unbind val r item g = make_fx f tracing_mode=self tracing_mode assertEqual g f test_constant_blowup f val = torch tensor blowup = val repeat bool blowup sum item == test_f make_fx f tracing_mode=self tracing_mode assertRaisesRegex RuntimeError data-dependent test_f test_constant_random f val = torch tensor val normal_ bool val item == test_f make_fx f tracing_mode=self tracing_mode assertRaisesRegex RuntimeError data-dependent test_f test_decomposition_interpreter fn x torch nn functional silu x x = torch rand fx_module = make_fx fn tracing_mode=self tracing_mode decomposition_table=None x found_silu = False n fx_module graph nodes n target == torch ops aten silu n target == torch ops aten silu default found_silu = True assertTrue found_silu new_graph = torch fx Graph silu_decomp_table = torch ops aten silu default decomposition_table torch ops aten silu default DecompositionInterpreter fx_module new_graph=new_graph decomposition_table=silu_decomp_table run x decomposed_module = torch fx GraphModule fx_module new_graph n decomposed_module graph nodes assertTrue n target = torch ops aten silu assertTrue n target = torch ops aten silu default assertEqual fx_module x decomposed_module x test_make_fx_model_fwd_bwd Foo torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x relu model = Foo f x params out = torch func functional_call model params x sum out backward list params values input = torch randn requires_grad=True params = dict model named_parameters fx_f = make_fx f tracing_mode=self tracing_mode input params fx may change order parameters list so using set compare assertTrue torch allclose fx_f input params f input params torch allclose fx_f input params f input params assertTrue torch allclose fx_f input params f input params torch allclose fx_f input params f input params test_make_fx_model_double_param Emformer torch nn Module __init__ input_dim int = - None super __init__ layer_norm = torch nn LayerNorm input_dim forward mod_self x noqa B assertTrue isinstance mod_self layer_norm weight torch Tensor y = mod_self layer_norm x assertTrue isinstance mod_self layer_norm weight torch Tensor z = mod_self layer_norm y z gm = make_fx Emformer torch randn ops = n target n gm graph nodes n op == call_function assertEqual len ops test_make_fx_model_fwd_bwd_wgtupdate Foo torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x relu model = Foo f args params buffers p params values p grad = None isinstance args Iterable args = args params_and_buffers = params buffers out = torch func functional_call model params_and_buffers args out sum backward p - e- p grad p params values input = torch randn requires_grad=True params = dict model named_parameters buffers = dict model named_buffers fx_f = make_fx f tracing_mode=self tracing_mode input params buffers fx may change order parameters list so using set compare also there numerical difference results so changing atol e- e- assertTrue torch allclose fx_f input params buffers f input params buffers atol= e- torch allclose fx_f input params buffers f input params buffers atol= e- assertTrue torch allclose fx_f input params buffers f input params buffers atol= e- torch allclose fx_f input params buffers f input params buffers atol= e- test_trace_subclasses f x x = UnwrapTensor x y = x y f x wrapped = UnwrapTensor x y = x wrapped y inp = torch randn _test f inp _test f inp test_partial_decomp f b c x = torch addmm b c y = torch addmm b c beta= alpha= x + y inps = torch randn torch randn torch randn fx_g = make_fx f inps addmm b c beta= alpha= beta == alpha == NotImplemented beta + alpha b c decomposed_fx = make_fx f decomposition_table= aten addmm default addmm inps assertEqual fx_g inps decomposed_fx inps assertEqual len n n fx_g graph nodes n target == aten addmm default assertEqual len n n decomposed_fx graph nodes n target == aten addmm default test_decomp_of_capture val = torch randn f x x t + val t nop x x cos traced = make_fx f decomposition_table= torch ops aten t default nop torch randn assertEqual len n n traced graph nodes n target == torch ops aten t default unittest skipIf HAS_CUDA CUDA-only test test_amp_cache layer = torch nn Conv d cuda f x w torch nn functional conv d x w stride=layer stride inp = torch randn device= cuda torch autocast cuda out_graph = make_fx f inp layer weight graph out_graph = make_fx f inp layer weight graph assertEqual len out_graph nodes len out_graph nodes b zip out_graph nodes out_graph nodes assertEqual op b op test_strides f x assertTrue x is_contiguous assertFalse x is_contiguous memory_format=torch channels_last x = x permute assertFalse x is_contiguous assertTrue x is_contiguous memory_format=torch channels_last x make_fx f torch randn f x assertTrue x is_contiguous y = x assertFalse y is_contiguous y = x assertFalse y is_contiguous x cos make_fx f torch randn test_pr_ Tests issue brought up here https github com pytorch pytorch pull #issuecomment- f b torch ops aten nll_loss_forward b None _test f torch randn torch zeros dtype=torch long TestGenericProxyTensorReal TestGenericProxyTensor tracing_mode = real TestGenericProxyTensorFake TestGenericProxyTensor tracing_mode = fake TestGenericProxyTensorSymbolic TestGenericProxyTensor tracing_mode = symbolic del TestGenericProxyTensor TestRealProxyTensor TestCase test_error_on_data_dependent_ops f x = torch randn y = torch randn assert torch allclose x y y x z = float x z = float y Smoke tests make_fx f _error_on_data_dependent_ops=False make_fx f pre_dispatch=True _error_on_data_dependent_ops=False TestFakeProxyTensor TestCase test_issue x = nn Parameter torch randn f torch ops aten t default x assertRaisesRegex Exception Please convert all Tensors lambda make_fx f tracing_mode= fake A torch Tensor pass x = A torch randn assertRaisesRegex TypeError Multiple dispatch failed lambda make_fx f tracing_mode= fake test_use_fake_and_tensor f x y z = torch tensor x + y + z g = make_fx f tracing_mode= fake torch randn torch randn x y = torch randn torch randn assertEqual g x y f x y test_free_fake f x torch add x y FakeTensorMode fake_mode y = torch randn make_fx f tracing_mode= real torch randn test_fused_adam See https github com pytorch pytorch issues params = torch randn _ range grads = torch randn _ range exp_avgs = torch randn _ range exp_avg_sqs = torch randn _ range max_exp_avg_sqs = torch randn _ range state_steps = torch tensor _ range fused_adam params grads exp_avgs exp_avg_sqs max_exp_avg_sqs state_steps new_params _ _ _ _ = aten _fused_adam default params grads exp_avgs exp_avg_sqs max_exp_avg_sqs state_steps lr= beta = beta = weight_decay= eps= e- amsgrad=False maximize=False p new_p zip params new_params p copy_ new_p params gm = make_fx fused_adam tracing_mode= fake params grads exp_avgs exp_avg_sqs max_exp_avg_sqs state_steps ensure_ops_have_val = aten _fused_adam default operator getitem n gm graph nodes n op == call_function n target ensure_ops_have_val assertIn val n meta test_alias f x torch ops aten alias x r = str make_fx f tracing_mode= fake torch randn code strip NB should have detach call assertExpectedInline r \ forward x_ alias = torch ops aten alias default x_ x_ = None alias test_meta f x = x cos b = torch var_mean dim= c = b c out = make_fx f tracing_mode= fake torch randn n out graph nodes n op == output continue assertTrue val n meta test_fake_tensor_mode f d = cos d torch _guards detect_fake_mode existing_fake_mode = FakeTensorMode existing_fake_mode out = make_fx f tracing_mode= real torch tensor fake_mode = detect_fake_mode node meta get val None node out graph nodes assertEqual fake_mode existing_fake_mode _get_node fx_g cond n fx_g graph nodes cond n n raise AssertionError _get_free_symbols shape_env vars = tuple shape_env var_to_val keys len var var vars var shape_env replacements _trace f args inps = torch randn arg arg args make_fx f tracing_mode= symbolic inps TODO Need test guards themselves specifically well TestSymbolicTracing TestCase _test_dynamic fn trace_inputs test_inputs assert_eq=True Tests fn traced trace_inputs against test_inputs Also returns shape env trace_inputs = torch randn shape shape trace_inputs traced_f = make_fx fn tracing_mode= symbolic trace_inputs input test_inputs input = torch randn shape shape input rx ry = traced_f input fn input assert_eq assertEqual rx ry traced_f test_debug_interpreter torch library torch library Library foo = Library foo DEF noqa TOR foo define foo Tensor - Tensor Operator where meta cpu disagree strides torch library impl foo foo CPU foo_cpu x x clone T torch library impl foo foo Meta foo_meta x x clone f x torch ops foo foo default x gm = make_fx f tracing_mode= symbolic torch randn torch _functorch compilers DebugInterpreter interp = DebugInterpreter gm input mismatch caught indicates guard problem assertRaisesRegex AssertionError r = lambda interp run torch randn T Catch incorrect meta assertRaisesRegex AssertionError r \ \ = \ \ lambda interp run torch randn test_int_input f x y x view y r = str make_fx f tracing_mode= symbolic torch empty code strip assertExpectedInline r \ forward x_ y_ view = torch ops aten view default x_ y_ x_ = y_ = None view test_resize_from_zero f x y x resize_ y size r = str make_fx f tracing_mode= symbolic torch empty torch empty code strip assertExpectedInline r \ forward x_ y_ sym_size_int = torch ops aten sym_size int y_ y_ = None resize_ = torch ops aten resize_ default x_ sym_size_int x_ = sym_size_int = resize_ = None None test_broadcast_shapes f x y torch functional broadcast_shapes x size y size r = str make_fx f tracing_mode= symbolic torch empty torch empty code strip assertExpectedInline r \ forward x_ y_ sym_size_int = torch ops aten sym_size int x_ x_ = None sym_size_int_ = torch ops aten sym_size int y_ y_ = None sym_size_int sym_size_int_ test_deduped_shape f s s x y torch functional broadcast_shapes x size y size torch empty x shape x = torch empty y = torch empty torch fx experimental symbolic_shapes ShapeEnv shape_env = ShapeEnv FakeTensorMode shape_env=shape_env static_shapes=False fake_mode x = fake_mode from_tensor x y = fake_mode from_tensor y r = str make_fx f tracing_mode= real x shape y shape x y code strip assertExpectedInline r \ forward s _ s _ x_ y_ empty = torch ops aten empty memory_format s _ device = device type= cpu pin_memory = False s _ s _ empty test_non_deduped_shape f x y torch functional broadcast_shapes x size y size torch empty x shape x = torch empty y = torch empty torch fx experimental symbolic_shapes ShapeEnv shape_env = ShapeEnv FakeTensorMode shape_env=shape_env static_shapes=False fake_mode x = fake_mode from_tensor x y = fake_mode from_tensor y r = str make_fx f tracing_mode= real x y code strip assertExpectedInline r \ forward x_ y_ sym_size_int = torch ops aten sym_size int x_ x_ = None sym_size_int_ = torch ops aten sym_size int y_ y_ = None empty = torch ops aten empty memory_format sym_size_int device = device type= cpu pin_memory = False sym_size_int sym_size_int_ empty test_unary f x assert x shape x cos test_inputs = test_inputs append test_inputs append gm = _test_dynamic f test_inputs assertTrue eval_guards gm torch randn assertEqual repr bind_symbols gm torch randn s s assertFalse eval_guards gm torch randn assertExpectedInline show_guards gm L x size = test_repeat_interleave f src_tokens beam_size_src src_tokens repeat_interleave beam_size_src size prompt_size = vocab_size = batch_size = src_tokens = torch randint vocab_size batch_size prompt_size gm = make_fx f tracing_mode= symbolic src_tokens torch randn assertEqual len gm shape_env guards test_non_symint_size_spec isn t really proxy tensor test s most convenient way get fake tensor symbolic sizes f x torch _C _non_sym_sizes x x + x = torch randn make_fx f tracing_mode= symbolic x https github com pytorch pytorch issues test_symbolic_repeat_interleave f y x y repeat_interleave x dim= y = torch tensor x = torch tensor r = str make_fx f tracing_mode= symbolic y x code strip assertExpectedInline r \ forward y_ x_ repeat_interleave = torch ops aten repeat_interleave Tensor x_ x_ = None index_select = torch ops aten index_select default y_ repeat_interleave y_ = repeat_interleave = None index_select test_mod_gcd_unbacked f _a _b _stride = _a item b = _b item stride = _stride item ta = torch randn stride tb = torch randn b stride r = torch cat ta tb r view + b stride _a = torch tensor _b = torch tensor _stride = torch tensor r = str make_fx f tracing_mode= symbolic _a _b _stride code strip assertExpectedInline r \ forward _a_ _b_ _stride_ _local_scalar_dense = torch ops aten _local_scalar_dense default _a_ _a_ = None _local_scalar_dense_ = torch ops aten _local_scalar_dense default _b_ _b_ = None _local_scalar_dense_ = torch ops aten _local_scalar_dense default _stride_ _stride_ = None mul = _local_scalar_dense _local_scalar_dense_ randn = torch ops aten randn default mul device = device type= cpu pin_memory = False mul = None mul_ = _local_scalar_dense_ _local_scalar_dense_ randn_ = torch ops aten randn default mul_ device = device type= cpu pin_memory = False mul_ = None cat = torch ops aten cat default randn randn_ randn = randn_ = None add = _local_scalar_dense + _local_scalar_dense_ _local_scalar_dense = _local_scalar_dense_ = None view = torch ops aten view default cat add _local_scalar_dense_ cat = add = _local_scalar_dense_ = None view test_cumsum_unbacked f x y = x item z = torch randn y z cumsum r = str make_fx f tracing_mode= symbolic torch tensor code strip assertExpectedInline r \ forward x_ _local_scalar_dense = torch ops aten _local_scalar_dense default x_ x_ = None randn = torch ops aten randn default _local_scalar_dense device = device type= cpu pin_memory = False _local_scalar_dense = None cumsum = torch ops aten cumsum default randn randn = None cumsum noqa B test_repeat_interleave_unbacked_output_size f x y s = x sum item y repeat_interleave x dim= output_size=s r = str make_fx f tracing_mode= symbolic torch tensor torch randn code strip assertExpectedInline r \ forward x_ y_ sum_ = torch ops aten sum default x_ _local_scalar_dense = torch ops aten _local_scalar_dense default sum_ sum_ = None repeat_interleave = torch ops aten repeat_interleave Tensor x_ output_size = _local_scalar_dense x_ = _local_scalar_dense = None index_select = torch ops aten index_select default y_ repeat_interleave y_ = repeat_interleave = None index_select noqa B test_arange_unbacked_output_size f x torch arange x r = str make_fx f tracing_mode= symbolic torch tensor code strip assertExpectedInline r \ forward x_ _local_scalar_dense = torch ops aten _local_scalar_dense default x_ x_ = None arange = torch ops aten arange start _local_scalar_dense device = device type= cpu pin_memory = False _local_scalar_dense = None arange noqa B test_adv_index_batch f src_tokens bsz src_len = src_tokens size start_step = src_tokens shape beam_size = generate_size = max_len = src_len + generate_size tokens = torch zeros bsz beam_size max_len src_tokens long fill_ tokens start_step = src_tokens repeat_interleave beam_size tokens prompt_size = vocab_size = batch_size = src_tokens = torch randint vocab_size batch_size prompt_size gm = make_fx f tracing_mode= symbolic src_tokens Guards rule out batch_size == sys maxsize wobbling between ok assertEqual len gm shape_env guards unittest skipIf HAS_CUDA CUDA-only test test_cpu_scalar_cuda Extracted wave vec f b b b r = str make_fx f tracing_mode= symbolic torch tensor torch randn device= cuda code strip assertExpectedInline r \ forward a_ b_ mul = torch ops aten mul Tensor a_ b_ a_ = None mm = torch ops aten mm default mul b_ mul = b_ = None mm test_binary_broadcast f b c = b c test_inputs = test_inputs append test_inputs append shape_env = _test_dynamic f test_inputs shape_env assert len shape_env guards == test_multiply_shape f torch empty shape r = str make_fx f tracing_mode= symbolic torch empty code strip assertExpectedInline r \ forward a_ sym_size_int = torch ops aten sym_size int a_ a_ = None mul = sym_size_int sym_size_int = None empty = torch ops aten empty memory_format mul device = device type= cpu pin_memory = False mul = None empty test_item f r = item r r = str make_fx f tracing_mode= symbolic torch randn code strip assertExpectedInline r \ forward a_ _local_scalar_dense = torch ops aten _local_scalar_dense default a_ mul = torch ops aten mul Tensor a_ _local_scalar_dense a_ = _local_scalar_dense = None mul test_tensor_symfloat f r = torch tensor size assert r dtype torch float r gm = make_fx f tracing_mode= symbolic torch randn r = str gm code strip NB specializes which fine point make sure dtype inference correct assertExpectedInline r \ forward a_ _tensor_constant = _tensor_constant lift_fresh_copy = torch ops aten lift_fresh_copy default _tensor_constant _tensor_constant = None lift_fresh_copy assertEqual gm _tensor_constant torch tensor test_item_to_constructor f r = item torch empty r r = str make_fx f tracing_mode= symbolic torch randint code strip assertExpectedInline r \ forward a_ _local_scalar_dense = torch ops aten _local_scalar_dense default a_ a_ = None empty = torch ops aten empty memory_format _local_scalar_dense device = device type= cpu pin_memory = False _local_scalar_dense = None empty noqa B test_setitem_symint moco https github com pytorch pytorch issues f x x = x size x r = str make_fx f tracing_mode= symbolic torch randn code strip assertExpectedInline r \ forward x_ sym_size_int = torch ops aten sym_size int x_ scalar_tensor = torch ops aten scalar_tensor default sym_size_int dtype = torch float layout = torch strided device = device type= cpu sym_size_int = None select = torch ops aten select int x_ copy_ = torch ops aten copy_ default select scalar_tensor select = scalar_tensor = copy_ = None x_ noqa B test_dynamic_pointwise_scalar f gravity mask gravity mask = gravity mask - r = str make_fx f tracing_mode= symbolic torch randn torch randint dtype=torch bool code strip assertExpectedInline r \ forward gravity_ mask_ select = torch ops aten select int gravity_ index = torch ops aten index Tensor select mask_ select = None mul = torch ops aten mul Tensor index - index = None select_ = torch ops aten select int gravity_ gravity_ = None index_put_ = torch ops aten index_put_ default select_ mask_ mul select_ = mask_ = mul = index_put_ = None None test_reflect_r_over_x reflect_R_over_x R reflect = torch eye device=R device reflect = - reflect R reflect f crop_camera mask crop_camera mask = reflect_R_over_x crop_camera mask r = str make_fx f tracing_mode= symbolic torch randn torch randint dtype=torch bool code strip assertExpectedInline r \ forward crop_camera_ mask_ index = torch ops aten index Tensor crop_camera_ mask_ eye = torch ops aten eye default device = device type= cpu pin_memory = False _tensor_constant = _tensor_constant lift_fresh_copy = torch ops aten lift_fresh_copy default _tensor_constant _tensor_constant = None select = torch ops aten select int eye select_ = torch ops aten select int select select = None copy_ = torch ops aten copy_ default select_ lift_fresh_copy select_ = lift_fresh_copy = copy_ = None sym_size_int = torch ops aten sym_size int index expand = torch ops aten expand default eye sym_size_int view = torch ops aten view default expand sym_size_int expand = None sym_size_int_ = torch ops aten sym_size int crop_camera_ sym_size_int_ = torch ops aten sym_size int crop_camera_ expand_ = torch ops aten expand default index sym_size_int sym_size_int_ sym_size_int_ index = None view_ = torch ops aten view default expand_ sym_size_int sym_size_int_ sym_size_int_ expand_ = sym_size_int_ = sym_size_int_ = None bmm = torch ops aten bmm default view view_ view = view_ = None view_ = torch ops aten view default bmm sym_size_int bmm = None mul_ = sym_size_int view_ = torch ops aten view default view_ mul_ view_ = mul_ = None mm = torch ops aten mm default view_ eye view_ = eye = None _unsafe_view = torch ops aten _unsafe_view default mm sym_size_int mm = sym_size_int = None index_put_ = torch ops aten index_put_ default crop_camera_ mask_ _unsafe_view crop_camera_ = mask_ = _unsafe_view = index_put_ = None None noqa B test_unbacked_slice f x m x = x m x slice None None None slice None None None slice None None make_fx f tracing_mode= symbolic torch randn torch randint dtype=torch bool unittest skipIf USE_TORCHVISION test requires torchvision test_unbacked_batch_resnet mod = torchvision models resnet f x mask params buffers p itertools chain x mask params values buffers values s p shape guard_int s x = x mask torch _check x shape = p params values p grad = None torch func functional_call mod params buffers x sum make_fx f tracing_mode= symbolic torch randn torch randint dtype=torch bool dict mod named_parameters dict mod named_buffers test_boolean_index f images handedness valid images = images valid handedness = handedness valid right_hand_mask = handedness == images right_hand_mask = images right_hand_mask flip - r = str make_fx f tracing_mode= symbolic torch randint torch randint torch randint dtype=torch bool code strip assertExpectedInline r \ forward images_ handedness_ valid_ index = torch ops aten index Tensor images_ valid_ images_ = None index_ = torch ops aten index Tensor handedness_ valid_ handedness_ = valid_ = None eq = torch ops aten eq Scalar index_ index_ = None index_ = torch ops aten index Tensor index eq flip = torch ops aten flip default index_ - index_ = None index_put_ = torch ops aten index_put_ default index eq flip index = eq = flip = index_put_ = None None test_neg_shape f torch empty -a shape + r = str make_fx f tracing_mode= symbolic torch empty code strip assertExpectedInline r \ forward a_ sym_size_int = torch ops aten sym_size int a_ a_ = None neg = -sym_size_int sym_size_int = None add = neg + neg = None empty = torch ops aten empty memory_format add device = device type= cpu pin_memory = False add = None empty test_unbacked_unification f x y z = torch zeros x item z + y r = str make_fx f tracing_mode= symbolic torch tensor torch randn code strip assertExpectedInline r \ forward x_ y_ _local_scalar_dense = torch ops aten _local_scalar_dense default x_ x_ = None zeros = torch ops aten zeros default _local_scalar_dense device = device type= cpu pin_memory = False _local_scalar_dense = None add = torch ops aten add Tensor zeros y_ zeros = y_ = None add noqa B test_reshape_divisibility_unbacked f x i = x item r = torch zeros i r = r transpose r reshape - make_fx f tracing_mode= symbolic torch tensor test_view_divisibility_unbacked f x i = x item r = torch zeros i r view - make_fx f tracing_mode= symbolic torch tensor unittest skipIf HAS_CUDA CUDA-only test test_view_divisibility_unbacked_relatively_prime See https github com pytorch pytorch issues f x i = x item To trigger original issue max bound has chosen such which torch _check i torch _check i = torch zeros i view - make_fx f tracing_mode= symbolic torch tensor device= cuda test_unbacked_unify_guard f x y z = torch zeros x item torch _check z size == y size refines i = s z size == y y + r = str make_fx f tracing_mode= symbolic torch tensor torch randn code strip assertExpectedInline r \ forward x_ y_ _local_scalar_dense = torch ops aten _local_scalar_dense default x_ x_ = None zeros = torch ops aten zeros default _local_scalar_dense device = device type= cpu pin_memory = False _local_scalar_dense = zeros = None add = torch ops aten add Tensor y_ y_ = None add noqa B unittest skipIf HAS_CUDA CUDA-only test unittest expectedFailure test_unbacked_unify_guard_transitivity f x x y z = torch zeros x item z = torch zeros x item torch _check z size == z size refines i = i torch _check z size == y size refines i = s z size == y y + gm = make_fx f tracing_mode= symbolic torch tensor device= cuda torch tensor device= cuda torch randn device= cuda insert_deferred_runtime_asserts gm gm shape_env test gm recompile r = str gm code strip assertExpectedInline r noqa B unittest skipIf HAS_CUDA CUDA-only test test_unbacked_unify_dependency_violation f x x x y z = x item torch _check z == z = x item z = x item torch _check z == z + z y NB inputs done CUDA ensure they aren t queried backed gm = make_fx f tracing_mode= symbolic torch tensor device= cuda torch tensor device= cuda torch tensor device= cuda torch randn device= cuda insert_deferred_runtime_asserts gm gm shape_env test gm recompile assertEqual gm torch tensor device= cuda torch tensor device= cuda torch tensor device= cuda torch tensor device= cuda torch tensor device= cuda assertRaises RuntimeError gm torch tensor device= cuda torch tensor device= cuda torch tensor device= cuda torch tensor device= cuda test_split_unbacked_sizes f lengths values tolist directly supported atm sizes = lengths i item i range lengths size torch split values sizes r = str make_fx f tracing_mode= symbolic torch tensor torch randn code strip assertExpectedInline r \ forward lengths_ values_ select = torch ops aten select int lengths_ _local_scalar_dense = torch ops aten _local_scalar_dense default select select = None select_ = torch ops aten select int lengths_ _local_scalar_dense_ = torch ops aten _local_scalar_dense default select_ select_ = None select_ = torch ops aten select int lengths_ lengths_ = None _local_scalar_dense_ = torch ops aten _local_scalar_dense default select_ select_ = None split_with_sizes = torch ops aten split_with_sizes default values_ _local_scalar_dense _local_scalar_dense_ _local_scalar_dense_ values_ = _local_scalar_dense = _local_scalar_dense_ = _local_scalar_dense_ = None getitem = split_with_sizes getitem_ = split_with_sizes getitem_ = split_with_sizes split_with_sizes = None getitem getitem_ getitem_ noqa B test_invalidate_nonzero ok = False f nonlocal ok b = clone x = b nonzero x = b nonzero x = b nonzero assert x shape == x shape ok = True b normal_ y = b nonzero try bool x shape == y shape fail didn t raise exception except GuardOnDataDependentSymNode pass make_fx f tracing_mode= symbolic torch randn torch _functorch config patch fake_tensor_propagate_real_tensors=True test_invalidate_nonzero_propagate_real_tensors f b = clone x = b nonzero x = b nonzero x = b nonzero assert x shape == x shape b normal_ y = b nonzero Because you re actually going generate exactly zero normal_ lol assert x shape == y shape make_fx f tracing_mode= symbolic torch randn test_sqrt_size f size - r = str make_fx f tracing_mode= symbolic torch empty code strip assertExpectedInline r \ forward a_ sym_size_int = torch ops aten sym_size int a_ sym_float = torch sym_float sym_size_int sym_size_int = None pow_ = sym_float sym_float = None div = torch ops aten div Tensor a_ pow_ a_ = pow_ = None div test_make_fx_with_custom_tracer_preserving_nn_module_stack Bar torch nn Module __init__ - None super __init__ forward x x + Foo torch nn Module __init__ - None super __init__ bar = Bar forward x x + bar x gm = make_fx Foo torch randn node gm graph nodes assertTrue nn_module_stack node meta foo = Foo functional_call args kwargs stateless _reparametrize_module foo foo args kwargs functional_call _orig_mod = foo gm_with_stack = make_fx functional_call record_module_stack=True torch randn found = False node gm_with_stack graph nodes nn_module_stack node meta len node meta nn_module_stack == assertTrue custom_tracer_preserving_nn_module_stack locals Foo str node meta nn_module_stack found = True len node meta nn_module_stack == assertTrue preserving_nn_module_stack locals Bar str node meta nn_module_stack found = True there can most level assertTrue False assertTrue found gm_without_stack = make_fx functional_call torch randn node gm_without_stack graph nodes assertTrue nn_module_stack node meta test_symint_to_tensor f shape r = str make_fx f tracing_mode= symbolic torch empty code strip assertExpectedInline r \ forward a_ sym_size_int = torch ops aten sym_size int a_ div = torch ops aten div Tensor a_ sym_size_int a_ = sym_size_int = None div r = str make_fx f tracing_mode= symbolic decomposition_table=decomposition_table torch empty code strip assertExpectedInline r \ forward a_ sym_size_int = torch ops aten sym_size int a_ sym_float = torch sym_float sym_size_int sym_size_int = None div = torch ops prims div default a_ sym_float a_ = sym_float = None div test_cat f b val = torch mul b out = torch cat val val out shape out shape out = out cos out test_inputs = test_inputs append test_inputs append gm = _test_dynamic f test_inputs assertTrue eval_guards gm torch randn torch randn assertFalse eval_guards gm torch randn torch randn assertExpectedInline show_guards gm L b size L size test_new_empty f b new_empty b shape b shape _test_dynamic f assert_eq=False shape_env test_size_with_tensor I think I messed up writing test case originally I think I m supposed hit error case code here works both eager tracing f tensor max_size = torch tensor dtype=torch int batch_shape = + list tensor shape - + list max_size tensor new_empty batch_shape = torch randn f make_fx f tracing_mode= symbolic test_fake_tensor_as_size f x r = torch zeros x r fx_g = make_fx f tracing_mode= symbolic torch tensor assertExpectedInline fx_g code strip \ forward x_ _local_scalar_dense = torch ops aten _local_scalar_dense default x_ x_ = None zeros = torch ops aten zeros default _local_scalar_dense device = device type= cpu pin_memory = False _local_scalar_dense = None zeros noqa B test_expand f b = torch mul c = b expand shape c _test_dynamic f _test_dynamic f test_metadata f b d = new_empty shape + b shape d fx_g = make_fx f tracing_mode= symbolic torch randn torch randn meta_c = _get_node fx_g lambda x x target == aten new_empty default meta_d = _get_node fx_g lambda x x target == operator add assertTrue meta_c meta val shape node expr == meta_d meta val node expr test_metadata_fresh f x assert x shape == x cos fx_g = make_fx f tracing_mode= symbolic torch randn meta_cos = _get_node fx_g lambda x x target == aten cos default meta_inp = _get_node fx_g lambda x x op == placeholder assertTrue meta_cos meta val shape == Checks input expr has been updated even though constraint happened afterwards assertTrue meta_inp meta val shape == test_elementwise_meta_with_sym_numbers f x offset as_sym_float=False x = x size as_sym_float x = torch sym_float x torch add x offset fx_g = make_fx f tracing_mode= symbolic torch rand False meta_add = _get_node fx_g lambda x x target == aten add Tensor assertEqual meta_add meta val shape assertEqual meta_add meta val dtype torch float fx_g = make_fx f tracing_mode= symbolic torch rand False meta_add = _get_node fx_g lambda x x target == aten add Tensor assertEqual meta_add meta val shape assertEqual meta_add meta val dtype torch int fx_g = make_fx f tracing_mode= symbolic torch rand True meta_add = _get_node fx_g lambda x x target == aten add Tensor assertEqual meta_add meta val shape assertEqual meta_add meta val dtype torch float test_return_symint f x x shape x cos x shape _test_dynamic f f x x shape _test_dynamic f test_rmethod f x x size + x _test_dynamic f test_mega_guard f b assert shape == b shape cos fx_g = make_fx f tracing_mode= symbolic torch randn torch randn torch _dynamo source LocalSource assertExpectedInline str fx_g shape_env produce_guards fx_placeholder_vals fx_g LocalSource LocalSource b ignore_static=False noqa B L size == L b size L stride == L storage_offset == L b stride == L b storage_offset == = L b size noqa B assertExpectedInline str fx_g shape_env produce_guards fx_placeholder_vals fx_g LocalSource LocalSource b ignore_static=True noqa B L size == L b size = L b size noqa B test_guard_upperbound_range_refinement f assert shape shape cos tensor = make_fx f tracing_mode= symbolic torch randn assertExpectedInline show_guards tensor = L size test_guard_lowerbound_range_refinement f assert shape shape cos tensor = make_fx f tracing_mode= symbolic torch randn assertExpectedInline show_guards tensor L size = test_guard_upperbound_range_refinement_multivariate f assert shape shape assert shape shape shape cos tensor = make_fx f tracing_mode= symbolic torch randn assertExpectedInline show_guards tensor \ L size L size = L size = L size test_guard_lowerbound_range_refinement_multivariate f assert shape shape assert shape shape shape cos tensor = make_fx f tracing_mode= symbolic torch randn assertExpectedInline show_guards tensor \ L size L size = L size L size = L size = test_sym_storage_offset f x y x + y inp = torch randn torch randn fx_g = make_fx f tracing_mode= symbolic inp inp = torch randn torch randn assertEqual fx_g inp f inp _assert_no_guards fx_g free_symbols assert _get_free_symbols fx_g shape_env == free_symbols fx_g shape_env var_to_val assert len fx_g shape_env get_nontrivial_guards == fx_g shape_env format_guards test_guards_equal f b b NB Numbers carefully chosen avoid duck shaping applying fx_g = _trace f _assert_no_guards fx_g fx_g = _trace f _assert_no_guards fx_g fx_g = _trace f _assert_no_guards fx_g f b c d = + b cat = torch cat c d + cat fx_g = _trace f _assert_no_guards fx_g f b c d e vals = b c d e x = idx range len vals - x = torch cat x vals idx + vals idx + x fx_g = _trace f _assert_no_guards fx_g f b = view b shape + b sum fx_g = _trace f _assert_no_guards fx_g fx_g = _trace f _assert_no_guards fx_g fx_g = _trace f _assert_no_guards fx_g test_nonidentity_transitive_guards f b c d e vals = b c d e cat_vals = idx range len vals - cat_vals append torch cat vals idx vals idx final_vals = b reversed list zip cat_vals vals final_vals append + b final_vals fx_g = _trace f assertExpectedInline show_guards fx_g torch fx experimental _config patch translation_validation=True test_constant_specialization f t assert t shape == t tensor = make_fx f tracing_mode= symbolic torch randn assertExpectedInline show_guards tensor make_fx_failures = unknown xfail allclose xfail equal empty skip new_empty skip empty_like skip empty skip empty_permuted flaky skip linalg lstsq grad_oriented skip nn functional max_unpool d device_type= cpu skip nn functional max_unpool d device_type= cpu skip nn functional max_unpool d device_type= cpu skip linalg lstsq flaky probably just precision issue data-dependent control flow skip item xfail cov xfail nn functional gaussian_nll_loss xfail corrcoef xfail quantile xfail nanquantile Seems like s creating sparse tensor isn t captured tensor is_sparse xfail sparse sampled_addmm xfail sparse mm reduce proxy tensor doesn t support sparse correctly right now skip to_sparse segfaults skip block_diag AssertionError Tensor-likes close skip empty_strided device_type= cpu only_real_tensor_failures = xfail narrow xfail tensor_split only_fake_tensor_failures = xfail narrow xfail tensor_split fake_tensor_failures = set symbolic_tensor_failures = xfail combinations xfail geqrf aten geqrf default - couldn t find symbolic meta function decomposition xfail histogram Could run aten histogram bin_ct arguments Meta backend This c xfail histogramdd aten _histogramdd_bin_edges default - couldn t find symbolic meta function decomposition xfail nanquantile Could run aten equal arguments Meta backend xfail nn functional binary_cross_entropy aten new_empty default - couldn t find symbolic meta function decom xfail nn functional cross_entropy aten size default - couldn t find symbolic meta function decomposition xfail nn functional ctc_loss aten _ctc_loss Tensor - couldn t find symbolic meta function decomposition xfail quantile Could run aten equal arguments Meta backend xfail max_pool d_with_indices_backward Expected value type List int argument kernel_size symbolic_tensor_segfaults = skip nn functional batch_norm Segfault symbolic_tensor_failures update symbolic_tensor_segfaults inplace_symbolic_tensor_failures = bugs xfail float_power base given float_power_ has dtype Float operation s result requires dtype Double out_symbolic_tensor_failures = Cast error details Unable cast Tensor This happens because test set up call out variant using ` out ` kwarg torch _some_op arg arg out= out out out However only works torch ops aten ops For ` _batch_norm_with_update ` fails because op has no python bindings so doesn t support ` out ` kwarg way calling its out variant xfail _batch_norm_with_update xfail _native_batch_norm_legit xfail angle xfail argmax xfail argmin xfail gather xfail linalg pinv xfail linalg pinv hermitian xfail scatter_add xfail scatter xfail take_along_dim SymIntArrayRef expected contain only concrete xfail randn RuntimeError Cannot call numel tensor symbolic sizes strides xfail index_reduce prod xfail index_reduce mean xfail index_reduce amax xfail index_reduce amin out_symbolic_tensor_segfaults = skip nanmean out_symbolic_tensor_failures update out_symbolic_tensor_segfaults Copies inputs inplace operations avoid inplace modifications leaves requiring gradient _get_safe_inplace inplace_variant functools wraps inplace_variant _fn t args kwargs inplace_variant t clone args kwargs _fn _test_make_fx_helper device dtype op tracing_mode inplace=False out=False fn = _get_safe_inplace op get_inplace inplace op op sample_inputs_itr = op sample_inputs device dtype requires_grad=False Limit ourselves first inputs so symbolic tracing tests don t take too long count = out count = sample_input itertools islice sample_inputs_itr count inplace sample_input broadcasts_input continue args = sample_input input + list sample_input args kwargs = sample_input kwargs out expected = fn args kwargs kwargs out = expected try optests make_fx_check fn args kwargs tracing_mode assertEqual randomize_data=True except DynamicOutputShapeException skipTest Dynamic output shape operation trace skipIfNameMatches pattern Decorator skip test its name matches given pattern decorator test_func wrapper args kwargs re match pattern test_func __name__ raise unittest SkipTest f Test test_func __name__ skipped because its name matches pattern pattern test_func args kwargs wrapper decorator Auto functionalize shouldn t work make_fx directly filtered_hop_db = op op hop_db op name = auto_functionalize unittest skipIf torch _dynamo is_dynamo_supported Cond requires dynamo TestProxyTensorOpInfo TestCase ops op_db + filtered_hop_db + custom_op_db allowed_dtypes= torch float skipOps TestProxyTensorOpInfo test_make_fx_exhaustive make_fx_failures union only_real_tensor_failures test_make_fx_exhaustive device dtype op _test_make_fx_helper device dtype op real ops op_db + filtered_hop_db + custom_op_db allowed_dtypes= torch float skipOps TestProxyTensorOpInfo test_make_fx_fake_exhaustive make_fx_failures union fake_tensor_failures only_fake_tensor_failures test_make_fx_fake_exhaustive device dtype op _test_make_fx_helper device dtype op fake ops op_db + filtered_hop_db + custom_op_db allowed_dtypes= torch float skipOps TestProxyTensorOpInfo test_make_fx_symbolic_exhaustive make_fx_failures &#124; fake_tensor_failures &#124; symbolic_tensor_failures test_make_fx_symbolic_exhaustive device dtype op _test_make_fx_helper device dtype op symbolic ops op_db + custom_op_db allowed_dtypes= torch float skipOps TestProxyTensorOpInfo test_make_fx_symbolic_exhaustive_inplace make_fx_failures &#124; fake_tensor_failures &#124; symbolic_tensor_failures &#124; inplace_symbolic_tensor_failures test_make_fx_symbolic_exhaustive_inplace device dtype op op get_inplace skipTest No inplace variable op _test_make_fx_helper device dtype op symbolic inplace=True ops op_db + custom_op_db allowed_dtypes= torch float skipOps TestProxyTensorOpInfo test_make_fx_symbolic_exhaustive_out make_fx_failures &#124; fake_tensor_failures &#124; symbolic_tensor_failures &#124; out_symbolic_tensor_failures test_make_fx_symbolic_exhaustive_out device dtype op op supports_out skipTest Op doesn t support out _test_make_fx_helper device dtype op symbolic out=True only_for = cpu instantiate_device_type_tests TestProxyTensorOpInfo globals only_for=only_for __name__ == __main__ run_tests