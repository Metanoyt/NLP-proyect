Owner s module dispatch ruff noqa F itertools os re collections namedtuple torch _C C torch utils cpp_extension torch _python_dispatcher PythonDispatcher torch testing _internal common_utils run_tests TestCase TODO Expand dispatcher API generic API interfacing dispatcher Python These exhaustive tests commutativity dispatch behavior If you re looking more usage-info style tests check op_registration_test cpp Things tested here - Listeners - Top level namespace registrations - Fallback - Exotic overloads CppFunction schema Things directly tested here - Internal state Dispatcher makes sense This indirectly tested invariant testing Result = namedtuple Result state table provenance dispatch_keys_to_check = Undefined CPU CUDA XLA AutogradOther AutogradCPU AutogradCUDA AutogradXLA extract_dispatch_table_with_keys table dispatch_keys extracted = table_entries = table split \n regex = re compile r registered FallbackKernel\ cpp \ k dispatch_keys t table_entries t startswith k mask out file line info in-tree backend fallback entry = regex sub registered pytorch framework t extracted += entry + \n extracted TestDispatch TestCase namespace_index = test_all_invariants Check regular stuff OK C _dispatch_check_all_invariants You probably don t want call directly your constructors don t commute you can still run commute fixed ctor_order so you can test destructors still commute run_ops name ops ctor_order=None dtor_order=None results=None expect_raises=False Given list operator registrations run registrations order specified ctor_order then run deregistrations dtor_order If results specified intermediate results checked consistency results stored results stored results first time we ve seen them Results expected equivalent modulo commutativity inverses thus results keyed frozenset effect registrations ops Results stores namedtuple Result state table provenance where state string contains non-derived kernel registered error message doesn t pass table string contains computed dispatch table entries provenance string describes how exactly we got string If expect_raises True error raise exception Instead we ll store exception string instead dispatcher state results In principle we should flag these differently s very obvious when you get error one case another By allocating every test into fresh namespace makes less likely bug testing framework will result tests interfering each other __class__ namespace_index += results None results = ctor_order None ctor_order = list range len ops dtor_order None dtor_order = list reversed ctor_order Refs which retain c Module object so we can explicitly control when each deregistration happens deregistration occurs when object gets deallocated refs = None len ops Keep track set effect registrations active_ops = set double underscore make less likely we conflict something test_namespace = f __test namespace_index __ check_invariants actual_provenance C _dispatch_check_invariants name Normalize test namespace so expected outputs stable actual_state = C _dispatch_dump f test_namespace name replace test_namespace test actual_table = C _dispatch_dump_table f test_namespace name replace test_namespace test expected_state expected_table expected_provenance = results setdefault frozenset active_ops Result actual_state actual_table actual_provenance assertMultiLineEqual expected_state actual_state f expected expected_provenance actual actual_provenance assertMultiLineEqual expected_table actual_table f expected expected_provenance actual actual_provenance results setdefault frozenset Result hardcoded initial state check_invariants initial state In order specified ctor_order run registrations set_to_report = frozenset range len ops i op_ix enumerate ctor_order It would better DEF here because we manage lifetime multiple registrations multiple Library references refs we can t deal strict checking DEF refs op_ix = C _dispatch_library FRAGMENT test_namespace active_ops add op_ix try ops op_ix refs op_ix check_invariants f running ctors ctor_order i + except RuntimeError e expect_raises raise actual = str e replace test_namespace test actual = actual split \nException raised expected _ expected_provenance = results setdefault frozenset active_ops Result actual f error after running ctors ctor_order i + assertMultiLineEqual expected actual expected_provenance set_to_report = frozenset active_ops active_ops remove op_ix NB finally test asserts registrations fails dispatcher left same state before check_invariants f running ctors ctor_order i then failing run ctor op_ix did failure leave dispatcher wedged state shouldn t break last_ctor = i expect_raises len active_ops == len ops Destroy references first some test frameworks like pytest will retain references exception raised assertTrue EW refs = None assertTrue False expected exception raised nothing raised f after running ctors ctor_order In order specified dtor_order run deregistrations i op_ix enumerate dtor_order Trigger destruction refs op_ix = None discard remove since we may have actually deregistered anything there error raised expect_raises active_ops discard op_ix active_ops remove op_ix check_invariants f running ctors ctor_order last_ctor + then running dtors dtor_order i + results set_to_report Operator registrations commutative static initializers can run any order invertible deregistration Subject some caveats some legacy behavior system commutative -- we want get rid these So while principle we could simply test set operations just running them one one order specified user we can get more assurance about these extra properties doing more work Don t run registrations once fixed order run every possible permutation Similarly run every permutation deregistration order Don t just check end state dispatcher every subset operator registrations ensure computed intermediate state path independent One thing note function we assume each operation unique In general there may duplicated registrations these usually idempotent legacy We test behavior here separately NB checking all permutations means function exponential length ops So don t pass too many ops function commute name ops ctor_order=None expect_raises=False results = go ctor_order dtor_order itertools permutations range len ops run_ops name ops ctor_order dtor_order results=results expect_raises=expect_raises ctor_order None go ctor_order ctor_order itertools permutations range len ops go ctor_order Return full Result namedtuple after all operations run If KeyErrors means there did exist any ordering ctors which got us end That s error test construction means you could have factored test into two smaller ones results frozenset range len ops test_def state = commute foo m foo Tensor x - Tensor lambda m m def_ foo Tensor x - Tensor m impl test_def const Tensor x x lambda m m impl_t_t foo m impl test_def kCPU const Tensor x x lambda m m impl_t_t foo dispatch= CPU m impl test_def kAutograd const Tensor x x lambda m m impl_t_t foo dispatch= Autograd m impl test_def kAutogradCPU const Tensor x x lambda m m impl_t_t foo dispatch= AutogradCPU state assertExpectedInline state \ name test foo schema test foo Tensor x - Tensor debug registered dev null alias analysis kind FROM_SCHEMA CPU impl_t_t Tensor _ - Tensor _ boxed unboxed AutogradCPU impl_t_t Tensor _ - Tensor _ boxed unboxed Autograd alias impl_t_t Tensor _ - Tensor _ boxed unboxed CompositeImplicitAutograd alias impl_t_t Tensor _ - Tensor _ boxed unboxed test_def_impl_schema_mismatch NB impl-impl mismatch reported eagerly you ll find out about because one them won t match state = commute foo m foo Tensor x Tensor y - Tensor lambda m m def_ foo Tensor x Tensor y - Tensor m impl foo const Tensor x x lambda m m impl_t_t foo expect_raises=True state assertExpectedInline state \ Inferred operator schema C++ kernel function doesn t match expected function schema operator test foo expected schema test foo Tensor x Tensor y - Tensor registered dev null inferred schema Tensor _ - Tensor _ impl_t_t reason The number arguments different vs test_def_with_inference state = commute foo m foo const Tensor x x lambda m m def_name_t_t foo m impl foo torch kCPU const Tensor x x lambda m m impl_t_t foo CPU m impl foo torch kAutograd const Tensor x x lambda m m impl_t_t foo Autograd m impl foo torch kAutogradCPU const Tensor x x lambda m m impl_t_t foo AutogradCPU state assertExpectedInline state \ name test foo schema test foo Tensor _ - Tensor _ debug registered dev null alias analysis kind CONSERVATIVE CPU impl_t_t Tensor _ - Tensor _ boxed unboxed AutogradCPU impl_t_t Tensor _ - Tensor _ boxed unboxed Autograd alias impl_t_t Tensor _ - Tensor _ boxed unboxed CompositeImplicitAutograd alias default_def_name_t_t Tensor _ - Tensor _ boxed unboxed test_def_only state = commute foo m foo Tensor x Tensor y - Tensor lambda m m def_ foo Tensor x Tensor y - Tensor state assertExpectedInline state \ name test foo schema test foo Tensor x Tensor y - Tensor debug registered dev null alias analysis kind FROM_SCHEMA test_impl_only state = commute foo m impl foo const Tensor x x lambda m m impl_t_t foo m impl foo torch kCPU const Tensor x x lambda m m impl_t_t foo CPU m impl foo torch kAutograd const Tensor x x lambda m m impl_t_t foo Autograd m impl foo torch kAutogradCPU const Tensor x x lambda m m impl_t_t foo AutogradCPU state assertExpectedInline state \ name test foo schema none CPU impl_t_t Tensor _ - Tensor _ boxed unboxed AutogradCPU impl_t_t Tensor _ - Tensor _ boxed unboxed Autograd alias impl_t_t Tensor _ - Tensor _ boxed unboxed CompositeImplicitAutograd alias impl_t_t Tensor _ - Tensor _ boxed unboxed test_computed_table result = commute foo m foo const Tensor x x lambda m m def_name_t_t foo m impl foo torch kCPU const Tensor x x lambda m m impl_t_t foo CPU debug= fn_cpu m impl foo torch kCUDA const Tensor x x lambda m m impl_t_t foo XLA debug= fn_xla m impl foo torch kAutograd const Tensor x x lambda m m impl_t_t foo Autograd debug= fn_autograd m impl foo torch kAutogradCPU const Tensor x x lambda m m impl_t_t foo AutogradCPU debug= fn_autogradcpu state table = result state result table assertExpectedInline state \ name test foo schema test foo Tensor _ - Tensor _ debug registered dev null alias analysis kind CONSERVATIVE CPU fn_cpu Tensor _ - Tensor _ boxed unboxed XLA fn_xla Tensor _ - Tensor _ boxed unboxed AutogradCPU fn_autogradcpu Tensor _ - Tensor _ boxed unboxed Autograd alias fn_autograd Tensor _ - Tensor _ boxed unboxed CompositeImplicitAutograd alias default_def_name_t_t Tensor _ - Tensor _ boxed unboxed computed dispatch table too big so we only check few entries we re interested extracted_table = extract_dispatch_table_with_keys table dispatch_keys_to_check assertExpectedInline extracted_table \ Undefined default_def_name_t_t math kernel CPU fn_cpu kernel CUDA default_def_name_t_t math kernel XLA fn_xla kernel AutogradOther default_def_name_t_t math kernel AutogradCPU fn_autogradcpu kernel AutogradCUDA default_def_name_t_t math kernel AutogradXLA fn_autograd autograd kernel test_computed_table_with_cpu_math_autogradcpu_fallthrough global_m = C _dispatch_library IMPL _ AutogradCPU result = commute foo m foo const Tensor x x lambda m m def_name_t_t foo m impl foo torch kCPU const Tensor x x lambda m m impl_t_t foo CPU state table = result state result table assertExpectedInline state \ name test foo schema test foo Tensor _ - Tensor _ debug registered dev null alias analysis kind CONSERVATIVE CPU impl_t_t Tensor _ - Tensor _ boxed unboxed CompositeImplicitAutograd alias default_def_name_t_t Tensor _ - Tensor _ boxed unboxed computed dispatch table too big so we only check few entries we re interested extracted_table = extract_dispatch_table_with_keys table dispatch_keys_to_check assertExpectedInline extracted_table \ Undefined default_def_name_t_t math kernel CPU impl_t_t kernel CUDA default_def_name_t_t math kernel XLA default_def_name_t_t math kernel AutogradOther default_def_name_t_t math kernel AutogradCPU registered pytorch framework backend fallback AutogradCUDA default_def_name_t_t math kernel AutogradXLA default_def_name_t_t math kernel test_computed_table_with_math global_m = C _dispatch_library IMPL _ AutogradCPU result = commute foo m foo Tensor x - Tensor lambda m m def_ foo Tensor x - Tensor m impl foo torch kCompositeImplicitAutograd const Tensor x x lambda m m impl_t_t foo CompositeImplicitAutograd state table = result state result table assertExpectedInline state \ name test foo schema test foo Tensor x - Tensor debug registered dev null alias analysis kind FROM_SCHEMA CompositeImplicitAutograd alias impl_t_t Tensor _ - Tensor _ boxed unboxed computed dispatch table too big so we only check few entries we re interested extracted_table = extract_dispatch_table_with_keys table dispatch_keys_to_check assertExpectedInline extracted_table \ Undefined impl_t_t math kernel CPU impl_t_t math kernel CUDA impl_t_t math kernel XLA impl_t_t math kernel AutogradOther impl_t_t math kernel AutogradCPU impl_t_t math kernel AutogradCUDA impl_t_t math kernel AutogradXLA impl_t_t math kernel test_computed_table_with_cpu_math global_m = C _dispatch_library IMPL _ AutogradCPU result = commute foo m foo Tensor x - Tensor lambda m m def_ foo Tensor x - Tensor m impl foo torch kCPU const Tensor x x lambda m m impl_t_t foo CPU debug= fn_cpu m impl foo torch kCompositeImplicitAutograd const Tensor x x lambda m m impl_t_t foo CompositeImplicitAutograd debug= fn_math state table = result state result table assertExpectedInline state \ name test foo schema test foo Tensor x - Tensor debug registered dev null alias analysis kind FROM_SCHEMA CPU fn_cpu Tensor _ - Tensor _ boxed unboxed CompositeImplicitAutograd alias fn_math Tensor _ - Tensor _ boxed unboxed computed dispatch table too big so we only check few entries we re interested extracted_table = extract_dispatch_table_with_keys table dispatch_keys_to_check assertExpectedInline extracted_table \ Undefined fn_math math kernel CPU fn_cpu kernel CUDA fn_math math kernel XLA fn_math math kernel AutogradOther fn_math math kernel AutogradCPU registered pytorch framework backend fallback AutogradCUDA fn_math math kernel AutogradXLA fn_math math kernel test_computed_table_with_autograd global_m = C _dispatch_library IMPL _ AutogradCPU result = commute foo m foo Tensor x - Tensor lambda m m def_ foo Tensor x - Tensor m impl foo torch kAutograd const Tensor x x lambda m m impl_t_t foo Autograd state table = result state result table assertExpectedInline state \ name test foo schema test foo Tensor x - Tensor debug registered dev null alias analysis kind FROM_SCHEMA Autograd alias impl_t_t Tensor _ - Tensor _ boxed unboxed computed dispatch table too big so we only check few entries we re interested extracted_table = extract_dispatch_table_with_keys table dispatch_keys_to_check assertExpectedInline extracted_table \ AutogradOther impl_t_t autograd kernel AutogradCPU impl_t_t autograd kernel AutogradCUDA impl_t_t autograd kernel AutogradXLA impl_t_t autograd kernel Now catchAll maps CompositeImplicitAutograd registering both catchAll CompositeImplicitAutograd breaks commutativity test_computed_table_with_cpu_autograd_math result = commute foo m foo Tensor x - Tensor lambda m m def_ foo Tensor x - Tensor m impl foo torch kCPU const Tensor x x lambda m m impl_t_t foo CPU debug= fn_cpu m impl foo torch kAutograd const Tensor x x lambda m m impl_t_t foo Autograd debug= fn_autograd m impl foo torch kCompositeImplicitAutograd const Tensor x x lambda m m impl_t_t foo CompositeImplicitAutograd debug= fn_math state table = result state result table assertExpectedInline state \ name test foo schema test foo Tensor x - Tensor debug registered dev null alias analysis kind FROM_SCHEMA CPU fn_cpu Tensor _ - Tensor _ boxed unboxed Autograd alias fn_autograd Tensor _ - Tensor _ boxed unboxed CompositeImplicitAutograd alias fn_math Tensor _ - Tensor _ boxed unboxed computed dispatch table too big so we only check few entries we re interested extracted_table = extract_dispatch_table_with_keys table dispatch_keys_to_check assertExpectedInline extracted_table \ Undefined fn_math math kernel CPU fn_cpu kernel CUDA fn_math math kernel XLA fn_math math kernel AutogradOther fn_math math kernel AutogradCPU fn_autograd autograd kernel AutogradCUDA fn_math math kernel AutogradXLA fn_math math kernel test_computed_table_with_ambiguous_autogradother result = commute foo m foo Tensor x - Tensor lambda m m def_ foo Tensor x - Tensor m impl foo torch kCompositeImplicitAutograd const Tensor x x lambda m m impl_t_t foo CompositeImplicitAutograd debug= fn_math m impl foo torch kFPGA const Tensor x x lambda m m impl_t_t foo FPGA debug= fn_fpga state table = result state result table assertExpectedInline state \ name test foo schema test foo Tensor x - Tensor debug registered dev null alias analysis kind FROM_SCHEMA FPGA fn_fpga Tensor _ - Tensor _ boxed unboxed CompositeImplicitAutograd alias fn_math Tensor _ - Tensor _ boxed unboxed computed dispatch table too big so we only check few entries we re interested extracted_table = extract_dispatch_table_with_keys table dispatch_keys_to_check + FPGA assertExpectedInline extracted_table \ Undefined fn_math math kernel CPU fn_math math kernel CUDA fn_math math kernel XLA fn_math math kernel AutogradOther ambiguous_autogradother ambiguous autogradother AutogradCPU fn_math math kernel AutogradCUDA fn_math math kernel AutogradXLA fn_math math kernel FPGA fn_fpga kernel test_computed_table_with_cpu_defaultbackend result = commute foo m foo Tensor x - Tensor lambda m m def_ foo Tensor x - Tensor m impl foo torch kCPU const Tensor x x lambda m m impl_t_t foo CPU debug= fn_cpu m impl foo torch kCompositeExplicitAutograd const Tensor x x lambda m m impl_t_t foo CompositeExplicitAutograd debug= fn_defaultbackend state table = result state result table assertExpectedInline state \ name test foo schema test foo Tensor x - Tensor debug registered dev null alias analysis kind FROM_SCHEMA CPU fn_cpu Tensor _ - Tensor _ boxed unboxed CompositeExplicitAutograd alias fn_defaultbackend Tensor _ - Tensor _ boxed unboxed computed dispatch table too big so we only check few entries we re interested extracted_table = extract_dispatch_table_with_keys table dispatch_keys_to_check assertExpectedInline extracted_table \ Undefined fn_defaultbackend default backend kernel CPU fn_cpu kernel CUDA fn_defaultbackend default backend kernel XLA fn_defaultbackend default backend kernel AutogradOther registered pytorch framework backend fallback AutogradCPU registered pytorch framework backend fallback AutogradCUDA registered pytorch framework backend fallback AutogradXLA registered pytorch framework backend fallback test_computed_table_with_cpu_autograd_defaultbackend result = commute foo m foo Tensor x - Tensor lambda m m def_ foo Tensor x - Tensor m impl foo torch kCPU const Tensor x x lambda m m impl_t_t foo CPU debug= fn_cpu m impl foo torch kAutograd const Tensor x x lambda m m impl_t_t foo Autograd debug= fn_autograd m impl foo torch kCompositeExplicitAutograd const Tensor x x lambda m m impl_t_t foo CompositeExplicitAutograd debug= fn_defaultbackend state table = result state result table assertExpectedInline state \ name test foo schema test foo Tensor x - Tensor debug registered dev null alias analysis kind FROM_SCHEMA CPU fn_cpu Tensor _ - Tensor _ boxed unboxed Autograd alias fn_autograd Tensor _ - Tensor _ boxed unboxed CompositeExplicitAutograd alias fn_defaultbackend Tensor _ - Tensor _ boxed unboxed computed dispatch table too big so we only check few entries we re interested extracted_table = extract_dispatch_table_with_keys table dispatch_keys_to_check + FPGA assertExpectedInline extracted_table \ Undefined fn_defaultbackend default backend kernel CPU fn_cpu kernel CUDA fn_defaultbackend default backend kernel XLA fn_defaultbackend default backend kernel AutogradOther fn_autograd autograd kernel AutogradCPU fn_autograd autograd kernel AutogradCUDA fn_autograd autograd kernel AutogradXLA fn_autograd autograd kernel FPGA fn_defaultbackend default backend kernel test_computed_table_with_cpu_autograd_math_defaultbackend result = commute foo m foo Tensor x - Tensor lambda m m def_ foo Tensor x - Tensor m impl foo torch kCPU const Tensor x x lambda m m impl_t_t foo CPU debug= fn_cpu m impl foo torch kAutograd const Tensor x x lambda m m impl_t_t foo Autograd debug= fn_autograd m impl foo torch kCompositeImplicitAutograd const Tensor x x lambda m m impl_t_t foo CompositeImplicitAutograd debug= fn_math m impl foo torch kCompositeExplicitAutograd const Tensor x x lambda m m impl_t_t foo CompositeExplicitAutograd debug= fn_defaultbackend state table = result state result table assertExpectedInline state \ name test foo schema test foo Tensor x - Tensor debug registered dev null alias analysis kind FROM_SCHEMA CPU fn_cpu Tensor _ - Tensor _ boxed unboxed Autograd alias fn_autograd Tensor _ - Tensor _ boxed unboxed CompositeImplicitAutograd alias fn_math Tensor _ - Tensor _ boxed unboxed CompositeExplicitAutograd alias fn_defaultbackend Tensor _ - Tensor _ boxed unboxed computed dispatch table too big so we only check few entries we re interested extracted_table = extract_dispatch_table_with_keys table dispatch_keys_to_check assertExpectedInline extracted_table \ Undefined fn_defaultbackend default backend kernel CPU fn_cpu kernel CUDA fn_defaultbackend default backend kernel XLA fn_defaultbackend default backend kernel AutogradOther fn_autograd autograd kernel AutogradCPU fn_autograd autograd kernel AutogradCUDA fn_autograd autograd kernel AutogradXLA fn_autograd autograd kernel test_multiple_def_error ops = m foo Tensor x Tensor y - Tensor lambda m m def_ foo Tensor x Tensor y - Tensor m foo Tensor x Tensor y - Tensor lambda m m def_ foo Tensor x Tensor y - Tensor assertExpectedInline commute foo ops expect_raises=True state Tried register operator test foo Tensor x Tensor y - Tensor same name overload name multiple times Each overload s schema should only registered single call Duplicate registration registered dev null Original registration registered dev null test_def_with_explicit_alias state = commute foo m torch schema foo Tensor x Tensor y - Tensor AliasAnalysisKind PURE lambda m m def_ foo Tensor x Tensor y - Tensor alias= PURE_FUNCTION state assertExpectedInline state \ name test foo schema test foo Tensor x Tensor y - Tensor debug registered dev null alias analysis kind PURE_FUNCTION test_multiple_def_alias_defaulting ops = m torch schema foo Tensor x - Tensor c AliasAnalysisKind PURE_FUNCTION lambda m m def_ foo Tensor x - Tensor alias= PURE_FUNCTION RegisterOperators op foo Tensor x - Tensor lambda m m def_legacy foo Tensor x - Tensor assertExpectedInline commute foo ops expect_raises=True state Tried register operator test foo Tensor x - Tensor same name overload name multiple times Each overload s schema should only registered single call Duplicate registration registered dev null Original registration registered dev null test_multiple_def_alias_mismatch ops = m torch schema foo Tensor x - Tensor c AliasAnalysisKind PURE_FUNCTION lambda m m def_ foo Tensor x - Tensor alias= PURE_FUNCTION m torch schema foo Tensor x - Tensor c AliasAnalysisKind CONSERVATIVE lambda m m def_ foo Tensor x - Tensor alias= CONSERVATIVE assertExpectedInline commute foo ops expect_raises=True state Tried register operator test foo Tensor x - Tensor same name overload name multiple times Each overload s schema should only registered single call Duplicate registration registered dev null Original registration registered dev null test_multiple_fallback global_m = C _dispatch_library IMPL _ XLA global_m fallback_fallthrough try global_m fallback_fallthrough except RuntimeError e assertExpectedInline str e Tried register multiple backend fallbacks same dispatch key XLA previous registration registered dev null new registration registered dev null assertTrue False test_overwrite_math ops = lambda m m impl_t_t foo debug= fn lambda m m impl_t_t foo debug= fn Not commutative assertExpectedInline commute foo ops ctor_order= state \ name test foo schema none CompositeImplicitAutograd alias fn Tensor _ - Tensor _ boxed unboxed CompositeImplicitAutograd alias inactive fn Tensor _ - Tensor _ boxed unboxed Definition dangling impl happens when someone does impl function This usually bug e g someone misspelled operator name someone registered impl op no longer exists test_find_dangling_impls dangling_impls = C _dispatch_find_dangling_impls assertEqual len dangling_impls msg=f Expect zero dangling impls found dangling_impls test_find_dangling_impls_ext extension_path = os path join os path dirname os path abspath __file__ cpp_extensions dangling_impl_extension cpp module = torch utils cpp_extension load name= dangling_impl_extension sources= extension_path extra_cflags= -g verbose=True impls = C _dispatch_find_dangling_impls assertEqual len impls assertEqual f \ name __test foo schema none CPU registered extension_path - boxed unboxed impls test_dispatch_print_registrations_for_dispatch_key_invalid assertRaisesRegex RuntimeError could parse dispatch key invalid_key C _dispatch_print_registrations_for_dispatch_key invalid_key TestPythonDispatcher TestCase test_basic dispatcher = PythonDispatcher dispatcher register CPU XLA Lazy CompositeImplicitAutograd assertExpectedInline dispatcher dispatchTable \ Computed Dispatch Table key kernel --------------------------- CPU fn_CPU kernel XLA fn_XLA kernel Lazy fn_Lazy kernel FPGA fn_CompositeImplicitAutograd math kernel AutogradOther fn_CompositeImplicitAutograd math kernel AutogradCPU backend fallback AutogradXLA backend fallback AutogradLazy backend fallback test_math_autogradcpu dispatcher = PythonDispatcher dispatcher register CPU XLA Lazy CompositeImplicitAutograd AutogradCPU assertExpectedInline dispatcher dispatchTable \ Computed Dispatch Table key kernel --------------------------- CPU fn_CPU kernel XLA fn_XLA kernel Lazy fn_Lazy kernel FPGA fn_CompositeImplicitAutograd math kernel AutogradOther fn_CompositeImplicitAutograd math kernel AutogradCPU fn_AutogradCPU kernel AutogradXLA backend fallback AutogradLazy backend fallback assertExpectedInline dispatcher registrations \ Registered Kernels key kernel --------------------------- CPU fn_CPU XLA fn_XLA Lazy fn_Lazy AutogradCPU fn_AutogradCPU CompositeImplicitAutograd alias fn_CompositeImplicitAutograd test_defaultbackend_autogradcpu dispatcher = PythonDispatcher dispatcher register CPU XLA Lazy CompositeExplicitAutograd AutogradCPU assertExpectedInline dispatcher dispatchTable \ Computed Dispatch Table key kernel --------------------------- CPU fn_CPU kernel XLA fn_XLA kernel Lazy fn_Lazy kernel FPGA fn_CompositeExplicitAutograd default backend kernel AutogradOther backend fallback AutogradCPU fn_AutogradCPU kernel AutogradXLA backend fallback AutogradLazy backend fallback assertExpectedInline dispatcher registrations \ Registered Kernels key kernel --------------------------- CPU fn_CPU XLA fn_XLA Lazy fn_Lazy AutogradCPU fn_AutogradCPU CompositeExplicitAutograd alias fn_CompositeExplicitAutograd test_autogradother dispatcher = PythonDispatcher dispatcher register CPU FPGA CompositeImplicitAutograd assertExpectedInline dispatcher dispatchTable \ Computed Dispatch Table key kernel --------------------------- CPU fn_CPU kernel XLA fn_CompositeImplicitAutograd math kernel Lazy fn_CompositeImplicitAutograd math kernel FPGA fn_FPGA kernel AutogradOther ambiguous_autogradother ambiguous autogradother AutogradCPU backend fallback AutogradXLA fn_CompositeImplicitAutograd math kernel AutogradLazy fn_CompositeImplicitAutograd math kernel assertExpectedInline dispatcher registrations \ Registered Kernels key kernel --------------------------- FPGA fn_FPGA CPU fn_CPU CompositeImplicitAutograd alias fn_CompositeImplicitAutograd test_duplicate_registrations dispatcher = PythonDispatcher assertRaisesRegex RuntimeError r Overridden allowed dispatcher register CPU CPU test_defaultbackend_math dispatcher = PythonDispatcher assertRaisesRegex RuntimeError r Registration both CompositeImplicitAutograd CompositeExplicitAutograd allowed dispatcher register CompositeExplicitAutograd CompositeImplicitAutograd test_quantized_structured_not_implemented x = torch zeros y = torch zeros scale zero_point = dtype = torch qint qx = torch quantize_per_tensor x scale zero_point dtype qy = torch quantize_per_tensor y scale zero_point dtype If bmm gets quantized support you need update something implemented assertRaisesRegex NotImplementedError Could run aten bmm out arguments QuantizedCPU backend lambda torch bmm qx qy __name__ == __main__ run_tests