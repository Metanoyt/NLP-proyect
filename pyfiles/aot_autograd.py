mypy ignore-errors contextlib itertools collections abc Callable contextlib nullcontext functools wraps typing Any Optional unittest mock patch torch torch _dynamo logging torch nn nn torch utils _pytree pytree torch utils dlpack torch Tensor torch _decomp decompositions_for_rng PhiloxStateTracker rng_decompositions torch _dispatch python enable_python_dispatcher torch _dynamo compiled_autograd torch _dynamo utils CompileEventLogger dynamo_timed preserve_rng_state set_feature_use torch _guards detect_fake_mode torch _inductor cudagraph_utils BoxedDeviceIndex torch _inductor utils BoxedBool torch _subclasses FakeTensor FakeTensorMode torch export _tree_utils reorder_kwargs torch fx experimental proxy_tensor make_fx torch fx experimental symbolic_shapes ShapeEnv static_inputs_log = torch _logging getArtifactLogger __name__ cudagraph_static_inputs config _aot_autograd autograd_cache noqa F AOTAutogradCache autograd_cache_key should_use_local_autograd_cache should_use_remote_autograd_cache _aot_autograd collect_metadata_analysis noqa F run_functionalized_fw_and_collect_metadata _aot_autograd descriptors AOTInput BufferAOTInput ParamAOTInput PlainAOTInput _aot_autograd frontend_utils _detect_attribute_assignment _try_get_metadata_from_dynamo construct_fake_mode process_inputs _aot_autograd functional_utils noqa F _check_if_mutation_can_be_in_graph are_all_mutations_hidden_from_autograd are_all_mutations_under_no_grad_or_inference_mode assert_functional_graph from_fun gen_alias_from_base has_data_mutation has_metadata_mutation is_fun sync_functional_tensor to_fun _aot_autograd graph_capture_wrappers noqa F aot_dispatch_subclass create_functional_call create_functionalized_fn create_functionalized_rng_ops_wrapper create_joint fn_input_mutations_to_outputs fn_prepped_for_autograd _aot_autograd graph_compile noqa F aot_stage _graph_capture aot_stage _compile aot_stage _export _aot_autograd input_output_analysis noqa F compute_overlapping_inputs create_graph_signature create_synthetic_base_metadata remove_dupe_metadata _aot_autograd logging_utils noqa F callback_set describe_input format_guard_bug_msg get_aot_compilation_context get_aot_graph_name get_graph_being_compiled graph_being_compiled model_name nth_graph set_model_name setup_stacktrace_preservation_hooks track_graph_compiling _aot_autograd runtime_wrappers noqa F AOTDedupeWrapper AOTSyntheticBaseWrapper SerializableCompiledFunction _aot_autograd schemas noqa F AOTConfig AOTDispatchCompiler AOTGraphCapture AOTState BackwardSignature FakifiedFlatArgs FQN GraphInputName GraphOutputName GraphSignature InputAliasInfo JointWithDescriptors MutationType OutputAliasInfo OutputType SerializableAOTDispatchCompiler SubclassCreationMeta SubclassMeta TensorAlias ViewAndMutationMeta _aot_autograd subclass_utils noqa F requires_subclass_dispatch unwrap_tensor_subclasses unwrap_tensor_subclasses_with_indices_to_original wrap_tensor_subclasses wrap_tensor_subclasses_maybe_joint _aot_autograd utils noqa F _get_autocast_states _get_symint_hints call_func_at_runtime_with_args create_tree_flattened_fn KNOWN_TYPES make_boxed_compiler make_boxed_func maybe_to_fresh_input normalize_as_list partial_flatten_asdict root_module_when_exporting_non_strict simple_wraps strict_zip partitioners default_partition zip = strict_zip This global counter increments every time we compile graph AOTAutograd You can use correlate runtime error messages compile time e g you get error runtime saying compiled graph failed you can set breakpoint compile time graph number investigate further compile time NB different get_aot_compilation_context which tracks each underlying graph compiled In contrast AOT_COUNTER corresponds top-level invocations aot_module aot_function one counter allocated per entire compiled block block may involve compiling multiple subgraphs e g forwards backwards AOT_COUNTER = itertools count ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ AOT Autograd contains pretty non-trivial amount logic handle edge cases around aliasing mutation external graph they show up side effects some way when you run graph Take look ` test_aotdispatch py TestAOTAutograd test_input_mutation ` tests some examples functions what they re compiled graphs looks like Below very long comment detailing several edge cases showing how AOT Autograd handles them Note AOT Autograd input data mutations If we compile function mutates inputs then those input mutations real side effects user expects see after running compiled graph However graph we want send backend needs entirely functional The way we reconcile difference we remove mutations completely graph we compile we update graph updated_inputs user_outputs In epilogue runs after compiled graph executed we copy updated inputs back originals Example original user code f x x mul_ out = x mul out After AOT Autograd compiles we end up compiled graph b autograd Function forward method executes compiled graph c wrapper function calls autograd Function forward performs epilogue The output b c all written below compiled_forward_graph x x_updated = x mul out = x_updated mul x_updated out x_updated gets gradient compiled backward compiled_backward_graph grad_x_updated grad_out grad_x = grad_x autograd Function forward x x_updated out = compiled_forward_graph x x_updated out compiled_wrapper x x_updated out = autograd Function apply x x copy_ x_updated out Another important thing note updated inputs due data mutations do participate compiled backward graph Since compiled forward graph gets N extra outputs due updated inputs showing up graph outputs The compiled backward gets additional N inputs That way during x copy_ x_updated bit epilogue gradients will flow updated input back original input Note AOT Autograd input metadata mutations For same reason input mutations we also don t put input metadata mutations graph Instead we updated version input view mutate input s metadata outside graph Example original user code f x x t_ out = x mul out AOT Autograd output compiled graph autograd Function forward wrapper function compiled_forward_graph x x_updated = x t out = x_updated mul x_updated out x_updated does get gradient compiled backward compiled_backward_graph grad_out grad_x = grad_x autograd Function forward x x_updated out = compiled_forward_graph x x_updated out compiled_wrapper x x_updated out = autograd Function apply x x as_strided_ x_updated out Note AOT Autograd outputs aliasing inputs intermediates AOT Autograd needs special handling outputs alias graph inputs intermediates Why autograd Function forward has limitation where views returned forward cannot later mutated views don t need compiled graph anyway - s cheap generate them outside compiled graph epilogue For outputs alias inputs we do following still aliased output graph output b In AOT Autograd wrapper epilogue we don t aliased output Instead we use regenerate output For outputs alias intermediates we do following Return output compiled forward s _base graph intermediates output forward b Use output graph_intermediate regenerate alias user instead compiled fw output You might wonder why we aliased output directly graph making graph compute only instead generate fresh alias off intermediate instead say just storing metadata about size stride output somewhere generate alias There two reasons Getting actual alias tensor allows us use view-replay generate alias instead as_strided call Inductor other backends free change memory format graph outputs results better performance This can result problems user later tries view output expecting have one set strides when has different set strides By including view op directly graph inductor takes into account when deciding what memory format graph intermediate should Another important thing note how our traced backward graph handles aliases applies outputs aliasing inputs outputs aliasing intermediates updated inputs returned compiled forward due metadata-only mutations Any outputs alias either inputs intermediates do NOT participate compiled backward graph It would wasteful include them compiled backward because we regenerate them eagerly end forward Example original user code f x out = x t intermediate = x mul out = intermediate view - out out AOT Autograd output compiled graph autograd Function forward wrapper function compiled_forward_graph x out = x t intermediate = x mul out = intermediate view - compiled graph also returns intermediate out out intermediate intermediate gets gradient compiled backward both output aliases out out do compiled_backward_graph grad_intermediate grad_x = grad_x autograd Function forward x out out intermediate = compiled_forward_graph x out out intermediate compiled_wrapper x out out intermediate = autograd Function apply x regenerate out input out _regenerated = out _view_func x regenerate out intermediate out _regenerated = out _view_func intermediate out _regenerated out _regenerated Note AOT Autograd mutations inputs alias other inputs Another edge case only partially handled today when input mutated itself aliases another input AOT Autograd needs ensure functionalization knows two inputs aliased each other That way when aliased input accessed later graph functionalization knows update alias given mutation occurred This handled updating calling convention we create synthetic base becomes new input compiled function we regenerate original aliased inputs directly off base inside compiled function This logic fully encapsulated aot_wrapper_synthetic_base Example original user code f x x_view x mul_ out = x x_view out f x x view - AOT Autograd output compiled graph autograd Function forward wrapper function compiled_forward_graph base x = generate_x base x_view = generate_x_view base x_updated = x mul x_view_updated = x_updated view - out = x_updated x_view_updated x_updated out The calling convention change aliases - base happens outside autograd Function forward That means forward only has input base backward only has output grad_base compiled_backward_graph grad_out grad_base = grad_base autograd Function forward base x_updated out = compiled_forward_graph base x_updated out The compiled wrapper where we create synthetic bases The info which inputs mutated also tracked before synthetic base creation compiled_wrapper x x_view base = merge_view_inputs x x_view x_updated out = autograd Function apply base x x_view aliased eager mode so mutation x will automatically affect x_view x copy_ x_updated out Note AOT Autograd Views avoid tangents aliasing inputs We view every forward output when creating out tangent tensors handle problematic case which subclass does extra aliasing between graph outputs inputs way visible above subclass Ordinarily when constructing joint function we want trace AOTAutograd we re guaranteed tangent tensors we pass into joint distinct tensors primals This because when decide which forward outputs create tangents we only create tangents forward outputs aliases inputs See Note AOT Autograd outputs aliasing inputs intermediates However when wrapper tensor subclasses enter picture possible have output forward subclass input alias input one its inner tensors alias NestedTensor example Performing out-of-place pointwise op NestedTensor constructs fresh NestedTensor holds onto input s offsets tensor directly Having tangent tensors same primal forward inputs can cause problems during tracing make_fx will specialize our duplicate inputs If we passed same tensor primals_ tangents_ during tracing make_fx will happily sub out all usages tangents_ primals_ graph which what we want To work around we view every forward output when creating out tangent tensors so tangents can never same forward inputs even forward inputs alias forward outputs Note Side-Effectful Tokens AOTAutograd We allow some some side-effectful operators post-AOTAutograd functional graph such prints torchbind operations To ensure these side-effects compatible future graph passes assume graph functional we will thread effect tokens show data dependence between these side-effectful operators Practically speaking effect tokens just dummy values torch tensor The graph would look like following gm token reader token frame = with_token ordered_effect_op reader token frame = frame token frame = with_token ordered_effect_op reader token frame = frame token frame frame We will pass token input graph thread through side-effectful operators using ` with_effects ` high order operator then updated token output So signature graph input would look something like tokens params_buffers user_inputs signature graph output would look something like tokens outputs However Inductor does want concept tokens final generated code s input output Since changing graph signature inside inductor difficult after generating forward graph we will run pass remove tokens inputgenerate following graph Inductor where tokens created sunk within graph rather than inputs outputs gm reader token = torch ops prims _make_token token frame = with_token ordered_effect_op reader token frame = frame token frame = with_token ordered_effect_op reader token frame = frame sink_token = torch ops prims _sink_tokens token frame frame ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ aot_autograd_decompositions = create_aot_state stack contextlib ExitStack flat_fn fake_flat_args FakifiedFlatArgs flat_args_descs list AOTInput aot_config AOTConfig fake_mode FakeTensorMode shape_env Optional ShapeEnv - AOTState Traces forward backward graphs attr ` flat_fn ` generate joint graph The joint graph Fx graph Aten ops Please refer tracing mechanism understand graph capturing details The joint graph then passed through attr ` partition_fn ` isolate forward backward portions which then respectively compiled via provided attr ` fw_compiler ` attr ` bw_compiler ` The resulting compiled forward backward graphs then wrapped up ` ` torch autograd Function ` ` object The calling convention here first aot_config num_params_buffers inputs flat_args parameters buffers rest inputs We use assume parameters buffer s shapes don t change Old name now avoid messing stats Also note pushed stack so extends BEYOND function stack enter_context dynamo_timed create_aot_dispatcher_function log_pt _compile_event=True This main entry point TODO Chillee argues dynamo itself should pass fake tensors list arguments when compiling moment we do do aot_config decompositions None aot_config decompositions = aot_config decompositions = aot_autograd_decompositions aot_config decompositions config functionalize_rng_ops Update decompositions functionalized random decompositions aot_config decompositions = rng_decompositions aot_config decompositions Check flat_args see they re already fake If so use fake mode instead python_dispatcher_mode = enable_python_dispatcher shape_env None nullcontext See NOTE Deferring tensor pack unpack hooks until runtime If any saved tensor hooks active we don t want trace them Instead we ll let them run runtime around custom autograd Function we generate torch compile stack enter_context torch autograd set_multithreading_enabled False stack enter_context preserve_rng_state stack enter_context fake_mode stack enter_context python_dispatcher_mode stack enter_context PhiloxStateTracker stack enter_context torch _dynamo utils _disable_saved_tensors_hooks_during_tracing torch _library fake_class_registry FakeScriptObject maybe_to_fake_obj Tracing may mutate states fake script object so we need duplicate fake script objects so subsequent tracing won t affected _dup_fake_script_obj fake_flat_args maybe_to_fake_obj detect_fake_mode fake_flat_args arg real_obj isinstance arg FakeScriptObject arg arg fake_flat_args needs_autograd = any x requires_grad x fake_flat_args isinstance x Tensor enable_python_dispatcher Patch set_rng_state set_rng_state fake tensors nonsensical This does affect collection metadata patch torch cuda set_rng_state lambda args None mod = root_module_when_exporting_non_strict flat_fn mod None ctx = _detect_attribute_assignment mod ctx = nullcontext torch _functorch config fake_tensor_propagate_real_tensors Running dynamo_timed causes fake tensor issues when propagate real tensor switched dynamo_timed_ctx = nullcontext dynamo_timed_ctx = dynamo_timed aot_collect_metadata log_pt _compile_event=True dynamo_timed_ctx ctx fw_metadata = run_functionalized_fw_and_collect_metadata flat_fn flat_args_descs=flat_args_descs static_input_indices=aot_config static_input_indices keep_input_mutations=aot_config keep_inference_input_mutations is_train=needs_autograd pre_dispatch=aot_config pre_dispatch _dup_fake_script_obj fake_flat_args req_subclass_dispatch = requires_subclass_dispatch fake_flat_args fw_metadata CompileEventLogger try_add_pt _compile backend_compile requires_subclass_dispatch=req_subclass_dispatch output_and_mutation_safe = any x requires_grad view-type operations preserve requires_grad even no_grad Do count aliases inputs requires_grad reason make training graph AOTAutograd will perform view-replay regenerate view outputs runtime setting their grad_fn properly x output_type OutputType alias_of_input OutputType is_input fw_metadata input_info x base_idx requires_grad x fw_metadata output_info any x requires_grad x mutates_data x mutations_under_no_grad_or_inference_mode x mutations_hidden_from_autograd x fw_metadata input_info needs_autograd output_and_mutation_safe We realized none outputs require grad none inputs require grad mutated so we actually have inference graph needs_autograd = False A bit silly right now subclass codepath our ViewAndMutationMeta changes depending whether we pass is_train keep_input_mutations so we re forced recompute metadata TODO refactor subclass path run_functionalized_fw_and_collect_metadata so unnecessary req_subclass_dispatch fw_metadata = run_functionalized_fw_and_collect_metadata flat_fn flat_args_descs=flat_args_descs keep_input_mutations=aot_config keep_inference_input_mutations is_train=False pre_dispatch=aot_config pre_dispatch static_input_indices=aot_config static_input_indices fake_flat_args fw_metadata = ViewAndMutationMeta input_info=fw_metadata input_info output_info=fw_metadata output_info num_intermediate_bases=fw_metadata num_intermediate_bases keep_input_mutations=aot_config keep_inference_input_mutations traced_tangents=fw_metadata traced_tangents traced_tangents_descs=fw_metadata traced_tangents_descs subclass_inp_meta=fw_metadata subclass_inp_meta subclass_fw_graph_out_meta=fw_metadata subclass_fw_graph_out_meta subclass_tangent_meta=fw_metadata subclass_tangent_meta is_train=False tokens=fw_metadata tokens static_input_indices=fw_metadata static_input_indices fw_metadata num_intermediate_bases assert req_subclass_dispatch f \ torch compile currently being used tensor subclass inputs We attempting compile graph two graph outputs alias one another specifically output indices i i x enumerate fw_metadata output_info x output_type == OutputType alias_of_intermediate ANY output aliasing even regular tensors currently unsupported there any subclass outputs If you run into please file github issue aot_config is_export aot_export ban input metadata mutations now keep shared code paths simpler Keeping resize_ graph will require some work Allowing keeping graph functional will require some calling convention changes len x x fw_metadata input_info x mutates_metadata = raise RuntimeError f \ Found input received metadata mutation through e g call ` resize_ ` ` transpose_ ` This currently banned aot_export workflow If you need functionality please file github issue fw_metadata= str fw_metadata In export banning data mutations inputs require grad now This should rare tricky get right When we trace backward we currently trace autograd grad instead backward which makes difficult ensure we run autograd all way through input before saw mutation len x x fw_metadata input_info x requires_grad x mutates_data = aot_config export_trace_joint raise RuntimeError f \ Found graph input requires gradients received mutation This currently banned aot_export workflow If you need functionality please file github issue fw_metadata= str fw_metadata req_subclass_dispatch raise RuntimeError \ aot_export currently supported traceable tensor subclass If you need feature please comment CREATE_ISSUE_LINK Need decide strategy functionalized RNG toggling via global config seems bad turning will require non-trivial calling convention change any export runtime config functionalize_rng_ops raise RuntimeError \ Functionalized RNG currently supported aot_export workflow Please file github issue otherwise set torch _functorch config functionalize_rng_ops = False AOTState needs_autograd=needs_autograd flat_args=_dup_fake_script_obj fake_flat_args flat_args_descs=flat_args_descs fw_metadata=fw_metadata Packaging just later use aot_config=aot_config stack=stack aot_function fn Callable fw_compiler Callable bw_compiler Optional Callable = None partition_fn Callable = default_partition decompositions Optional dict = None num_params_buffers int = keep_inference_input_mutations bool = False inference_compiler Optional Callable = None Whether trace dynamic shapes dynamic=False enable_log=True disable_functionalization=False - Callable Traces forward backward graph attr ` fn ` using torch dispatch mechanism then compiles generated forward backward graphs through attr ` fw_compiler ` attr ` bw_compiler ` func ` aot_function ` traces forward backward graph ahead time generates joint forward backward graph attr ` partition_fn ` then used separate out forward backward graphs The partitioner function can used perform optimizations such recomputation One can set ` decompositions ` dictionary decompose operators into sequence core simpler operators supported backend compilers warning This API experimental likely change Args fn Callable A Python function takes one more arguments Must one more Tensors fw_compiler Callable A Python function accepts Fx graph Aten ops input args returns Callable semantically equivalent input Fx graph bw_compiler Optional Callable A Python function accepts Fx graph Aten ops input args returns Callable semantically equivalent input Fx graph Default None when None defaults attr ` fw_compiler ` partition_fn Callable A Python function takes joint forward backward graph partitions into separate forward backward graphs decompositions Dict A dictionary define decomposition larger Aten ops into simpler core Aten ops inference_compiler Optional Callable A Python function accepts Fx graph Aten ops input args returns Callable semantically equivalent input Fx graph inference_compiler invoked no autograd needed Default None when None defaults attr ` fw_compiler ` Returns Returns ` ` Callable ` ` retains eager behavior original attr ` fn ` forward backward graph compiled via attr ` fw_compile ` attr ` bw_compile ` A simple example usage func ` aot_function ` follows This example will print forward backward graphs function ` ` fn ` ` fn = lambda x x sin cos print_compile_fn fx_module args print fx_module fx_module aot_fn = aot_function fn print_compile_fn x = torch randn requires_grad=True aot_fn x aot_config = AOTConfig fw_compiler=None bw_compiler=None inference_compiler=None partition_fn=None decompositions=decompositions num_params_buffers=num_params_buffers aot_id=next AOT_COUNTER keep_inference_input_mutations=keep_inference_input_mutations dynamic_shapes=dynamic aot_autograd_arg_pos_to_source=None is_export=False no_tangents=False enable_log=enable_log disable_functionalization=disable_functionalization cached_res = None wraps fn returned_function args kwargs nonlocal cached_res Now flatten tensor args flat_args = pytree arg_tree_leaves args kwargs Compile function save cache cached_res None flat_fn out_spec = create_tree_flattened_fn fn args kwargs fake_mode shape_env = construct_fake_mode flat_args aot_config fake_flat_args FakifiedFlatArgs = process_inputs flat_args aot_config fake_mode shape_env TODO We actually could use pytree path make better descs Also descs here bad you do aot_module fake_flat_args_descs = PlainAOTInput i i range len fake_flat_args contextlib ExitStack stack aot_state = create_aot_state stack flat_fn fake_flat_args fake_flat_args_descs aot_config fake_mode shape_env aot_graph_capture = aot_stage _graph_capture aot_state flat_fn compiled_fn _ = aot_stage _compile aot_state aot_graph_capture partition_fn fw_compiler bw_compiler inference_compiler cached_res = compiled_fn out_spec cached_fn out_spec = cached_res out = cached_fn flat_args out_spec unflatten out returned_function aot_module mod nn Module args kwargs - nn Module Traces forward backward graph attr ` mod ` using torch dispatch tracing mechanism It wrapper function underneath uses func ` aot_function ` perform tracing compilation func ` aot_module ` lifts parameters buffers ` ` nn Module ` ` inputs new callable which then compiled through func ` aot_function ` warning This API experimental likely change Args mod Callable A ` ` nn Module ` ` module args args passed func ` aot_function ` kwargs kwargs passed func ` aot_function ` Returns Returns ` ` nn Module ` ` retains eager behavior original attr ` mod ` forward backward graph compiled See Note Fake Modules AOTAutograd torch _dynamo utils assert_no_fake_params_or_buffers mod functional_call named_params named_buffers args kwargs params_and_buffers = named_params named_buffers torch func functional_call mod params_and_buffers args kwargs named_params = dict mod named_parameters remove_duplicate=False named_buffers = dict mod named_buffers remove_duplicate=False num_params_buffers = len named_params + len named_buffers compiled_f = aot_function functional_call args num_params_buffers=num_params_buffers kwargs AOTModule nn Module __init__ - None super __init__ orig_module = mod forward args kwargs compiled_f named_params named_buffers args kwargs AOTModule prepare_aot_module_simplified mod nn Module args kwargs decompositions dict keep_inference_input_mutations boxed_forward_device_index BoxedDeviceIndex ignore_shape_env bool flatten bool force_non_lazy_backward_lowering bool = False disable_functionalization bool = False _record_nn_module_stack bool = False flatten assert kwargs None kwargs None kwargs = TODO There s something bit suspicious here typically simplified module shouldn t actually have any parameters params = dict mod named_parameters remove_duplicate=False buffers = dict dict mod named_buffers remove_duplicate=False params_flat params_spec = list params values list params keys params_len = len params_flat buffers_flat buffers_spec = list buffers values list buffers keys buffers_len = len buffers_flat params_buffers = params buffers params_buffers_flat = params_flat + buffers_flat params_buffers_spec = params_spec + buffers_spec Take break figure what we re doing module NB This doesn t change out convention except adding parameters explicit arguments functional_call = create_functional_call mod params_buffers_spec params_len + buffers_len strict_out_tuple=not flatten We need export run ModuleStackTracer instead PythonKeyTracer store_orig_mod=_record_nn_module_stack full_args = params_flat buffers_flat args in_spec out_spec = None None flatten functional_call out_spec = create_tree_flattened_fn functional_call full_args kwargs full_args in_spec = pytree tree_flatten full_args kwargs del kwargs OK set up descs full_args_descs = full_args_descs extend ParamAOTInput fqn fqn params_spec full_args_descs extend BufferAOTInput fqn fqn buffers_spec TODO would better put pytree information here full_args_descs extend PlainAOTInput i i range len full_args - len full_args_descs TODO These tracing_context fields should become unnecessary once we always maintain sources all arguments tracing_context = torch _guards TracingContext try_get NB TracingContext misnames params here also contains buffers tracing_context params_flat = params_buffers_flat tracing_context params_flat_unwrap_subclasses tracing_context params_unwrapped_to_flat_index = unwrap_tensor_subclasses_with_indices_to_original params_buffers_flat TODO Might nice hold Dynamo source here full_args_descs aot_autograd_arg_pos_to_source static_input_indices = _try_get_metadata_from_dynamo mod params_buffers keys len full_args full_args_descs dynamic_shapes = False x full_args isinstance x FakeTensor dynamic_shapes = x fake_mode shape_env None break aot_config = AOTConfig fw_compiler=None bw_compiler=None inference_compiler=None partition_fn=None decompositions=decompositions num_params_buffers=params_len + buffers_len aot_id=next AOT_COUNTER keep_inference_input_mutations=keep_inference_input_mutations dynamic_shapes=dynamic_shapes aot_autograd_arg_pos_to_source=aot_autograd_arg_pos_to_source static_input_indices=static_input_indices is_export=False no_tangents=False cache_info=None ignore_shape_env=ignore_shape_env precompile_backend_id=getattr mod _backend_id None force_non_lazy_backward_lowering=force_non_lazy_backward_lowering disable_functionalization=False fake_mode shape_env = construct_fake_mode full_args aot_config NB full_args_descs needed here fake_flat_args full_args fake_flat_args = process_inputs full_args aot_config fake_mode shape_env ignore_shape_env functional_call params_buffers_flat params_spec buffers_spec fake_flat_args full_args_descs aot_config fake_mode shape_env in_spec out_spec aot_module_simplified mod nn Module args fw_compiler AOTDispatchCompiler bw_compiler Optional AOTDispatchCompiler = None partition_fn Callable = default_partition decompositions Optional dict = None keep_inference_input_mutations=False inference_compiler Optional AOTDispatchCompiler = None TODO This doesn t seem used any nontrivial way check s actually needed cudagraphs Optional BoxedBool = None boxed_forward_device_index Optional BoxedDeviceIndex = None ignore_shape_env bool = False disable_functionalization bool = False - nn Module This simplified low overhead version aot_module For frontends like TorchDynamo input functions modules AOT static have unpacked inputs outputs This gives us opportunity remove pytree overhead parse inputs outputs AOT Autograd cache Reading params buffers every forward call func ` aot_module_simplified ` removes these overheads cudagraphs None cudagraphs = BoxedBool torch _inductor config triton cudagraphs contextlib ExitStack stack functional_call params_buffers_flat _params_spec _buffers_spec fake_flat_args full_args_descs aot_config fake_mode shape_env _in_spec _out_spec = prepare_aot_module_simplified mod args None decompositions keep_inference_input_mutations boxed_forward_device_index ignore_shape_env flatten=False force_non_lazy_backward_lowering=config force_non_lazy_backward_lowering disable_functionalization=disable_functionalization compiled_fn = None isinstance fw_compiler SerializableAOTDispatchCompiler local = should_use_local_autograd_cache remote = should_use_remote_autograd_cache local remote set_feature_use aot_autograd_remote_cache remote compiled_fn = AOTAutogradCache try_load mod fake_flat_args aot_config cudagraphs boxed_forward_device_index local remote compiled_fn None stack enter_context compiled_autograd _disable aot_state = create_aot_state stack functional_call fake_flat_args full_args_descs aot_config fake_mode shape_env aot_graph_capture = aot_stage _graph_capture aot_state functional_call compiled_fn _ = aot_stage _compile aot_state aot_graph_capture partition_fn fw_compiler bw_compiler inference_compiler isinstance mod torch _dynamo utils GmWrapper This function called flatten_graph_inputs wrapper which boxes inputs so they can freed before end scope For overhead reasons default wrapper see comment https github com pytorch pytorch pull files#r simple_wraps compiled_fn forward runtime_args list Any flat_args = flat_args extend params_buffers_flat flat_args extend runtime_args runtime_args clear compiled_fn flat_args TODO There something deeply wrong here compiled_fn running boxed calling convention aot_module_simplified somehow historically returned function boxed calling convention This should get fixed NB GraphModule nn Module rely non-boxed calling convention here simple_wraps compiled_fn forward runtime_args tuple Any full_args = full_args extend params_buffers_flat full_args extend runtime_args compiled_fn full_args Just convenience forward zero_grad = mod zero_grad forward named_parameters = mod named_parameters forward named_buffers = mod named_buffers Add serialize function grab_serialize_fn fn isinstance fn SerializableCompiledFunction fn serialize_fn hasattr fn __wrapped__ grab_serialize_fn fn __wrapped__ None forward serialize = grab_serialize_fn forward type ignore attr-defined forward boxed_nop_preserve_node_meta fx_g example_inputs run args torch fx traceback preserve_node_meta torch fx Interpreter fx_g boxed_run args run _boxed_call = True run aot_export_joint_with_descriptors stack contextlib ExitStack mod nn Module args kwargs=None decompositions Optional dict = None keep_inference_input_mutations=False ignore_shape_env=False disable_functionalization=False _record_nn_module_stack=False - JointWithDescriptors This API captures joint graph nn Module However unlike aot_export_joint_simple aot_export_module trace_joint=True calling convention produced joint graph follows no fixed positional schema example you cannot rely second argument traced joint graph correspond second argument module you traced However inputs outputs traced graph schematized descriptors annotated meta desc placeholder FX nodes which you can use determine meaning arguments The major benefit using export rather than aot_export_joint_simple we have feature parity all situations torch compile supports via aot_module_simplified including handling more complicated cases such multiple differentiable outputs input mutations must handled outside graph tensor subclasses etc What can you do one these joint graphs descriptors The motivating use case autoparallel involves taking joint graph doing optimizations then turning back into callable so can torch compile d later point time This cannot done traditional torch compile joint graph pass two reasons The sharding parameters must decided before parameter initialization checkpoint load far before torch compile would ordinarily run We need change meaning parameters e g we might replace replicated parameter sharded version changing its input size torch compile ordinarily semantics preserving allowed change meaning inputs Some descriptors can quite exotic so we recommend thinking carefully there safe fallback you can apply descriptors you don t understand For example you should have some way handle finding particular input exactly final FX graph inputs Note When using API you must create enter ExitStack context manager which will passed into function This context manager must remain active you call compile function finish compilation TODO We may relax requirement having AOTAutograd keep track how reconstruct all context managers later point time NB You re obligated do full compile stage instead you can leave forward backward compilers unspecified which case partitioned FX graphs will directly run The overall autograd Function can allowed graph so you can reprocess context potentially larger compiled region later NB These APIs do NOT hit cache we only ever cache final compile results intermediate export result NB If passed nn Module has parameters buffers we will generate extra implicit parameter buffer arguments assign ParamAOTInput BufferAOTInput descriptors them However you generate input nn Module mechanism like Dynamo you will NOT get these descriptors because Dynamo will already have taken care lifting parameters buffers into arguments In case would necessary analyze Sources inputs determine inputs parameters their FQNs functional_call _params_buffers_flat params_spec buffers_spec fake_flat_args full_args_descs aot_config fake_mode shape_env in_spec out_spec = prepare_aot_module_simplified mod args kwargs In contrast decompositions needed stage decompositions keep_inference_input_mutations None ignore_shape_env flatten=True Without we will attempt compile backward lazily runtime pointless because s just boxed_nop s trivial But will get Inductor confused about scoping Metric s is_forward have already been set current context force_non_lazy_backward_lowering=True disable_functionalization=disable_functionalization _record_nn_module_stack=_record_nn_module_stack TODO Maybe should create_aot_state Not sure would increase its scope stack enter_context compiled_autograd _disable aot_state = create_aot_state stack functional_call fake_flat_args full_args_descs aot_config fake_mode shape_env NB no cache lookup aot_graph_capture = aot_stage _graph_capture aot_state functional_call assert out_spec spec None JointWithDescriptors _aot_state=aot_state _aot_graph_capture=aot_graph_capture params_spec=params_spec buffers_spec=buffers_spec in_spec=in_spec out_spec=out_spec spec aot_compile_joint_with_descriptors jd JointWithDescriptors partition_fn Callable = default_partition fw_compiler Optional AOTDispatchCompiler = boxed_nop_preserve_node_meta bw_compiler Optional AOTDispatchCompiler = boxed_nop_preserve_node_meta - callable Companion function aot_export_joint_with_descriptors which compiles joint graph into callable function follows standard calling convention params_flat all arguments Note We do NOT instantiate module gives you flexibility subclass customize its behavior without having worry about FQN rebinding TODO Consider we should allow_in_graph result default compiled_fn _ = aot_stage _compile jd _aot_state jd _aot_graph_capture partition_fn fw_compiler bw_compiler Cribbed torch export pt _archive _package py simple_wraps compiled_fn torch _dynamo nonstrict_trace allow recursive compilation unflattened_compiled_fn args kwargs flat_inputs = pytree tree_flatten args reorder_kwargs kwargs jd in_spec TODO do I need filter I hope flat_outputs = compiled_fn flat_inputs pytree tree_unflatten flat_outputs jd out_spec unflattened_compiled_fn aot_export_module mod nn Module args decompositions Optional dict = None If true we ll joint forward-backward graph As well metadata loss + gradients backward trace_joint bool If trace_joint True we expect your module scalar loss Your module can multiple outputs so you must specify which output loss output_loss_index Optional int = None pre_dispatch bool = False If None will inferred inputs mod graph nodes mod graph module inferred result might wrong dynamic_shapes Optional bool = None kwargs=None - tuple torch fx GraphModule GraphSignature This function takes module returns FX graph can exported some metadata about graph If ` trace_joint=True ` we will joint graph forward + backward The traced FX graph will have following properties compared original module Inputs outputs module will pytree-flattened Parameters buffers module will lifted into graph inputs graph_inputs = parameters buffers user_inputs The graph will fully functionalized Any input mutations will converted into additional outputs graph meaning whoever calls graph responsible applying mutations back original inputs If is_joint provided graph will parameter gradients addition user outputs The graph output will look like graph_outputs = updated_inputs user_outputs param_gradients There also several restrictions what modules can use API In particular If trace_joint specified we expect loss function fused into module forward One outputs forward must scalar loss which specified ` output_loss_index ` All other outputs forward presumed require gradients This API cannot capture optimizers although theory we could build API Metadata mutations params buffers inputs banned Data mutations anything requires gradients banned parameters If input mutated allowed alias any other inputs Parameters must duplicated pre_dispatch trace_joint raise RuntimeError pre_dispatch supported when trace_joint True named_parameters = dict mod named_parameters remove_duplicate=False named_buffers = dict mod named_buffers remove_duplicate=False params_and_buffers = dict named_parameters dict named_buffers params_and_buffers_flat params_spec = pytree tree_flatten params_and_buffers params_and_buffers_flat = tuple params_and_buffers_flat params_len = len params_and_buffers_flat kwargs = kwargs functional_call = create_functional_call mod params_spec params_len store_orig_mod=True num_fw_outs = None trace_joint This helper effectively just adds some extra asserts about what backward will look like Outputs must include scalar loss we compute gradients w r t We don t compute gradients w r t anything so just case we detach other output tensors fn_to_trace args nonlocal num_fw_outs out = functional_call args output_loss_index None raise RuntimeError \ If trace_joint=Trueit required one your forward outputs must scalar loss You must specify which index output loss output_loss_index isinstance out torch Tensor out = out isinstance out tuple list raise RuntimeError f Expected forward output either tensor list tuple tensors found type out i o enumerate out We only want create backward graph w r t loss user passed This implies every other output should require gradients Instead making error forcing user detach all other outputs their forward we ll automatically detach them here o requires_grad i = output_loss_index raise RuntimeError f \ Found output forward requires gradients scalar loss We require all outputs forward scalar loss require gradient because we will only compute backward graph against scalar loss You can fix calling detach each your forward outputs loss You specified output index output_loss_index loss we found output index i requires gradients out_loss = out output_loss_index num_fw_outs = len out out_loss requires_grad raise RuntimeError f \ The output index output_loss_index marked loss does require gradients out_loss numel = raise RuntimeError f \ We require output marked loss index output_loss_index scalar has shape out_loss shape out ctx = nullcontext Run under no_grad so our tracing machinery only traces inference graph However pre_dispatch=True we want correctly trace set_grad_enabled calls training ctx = nullcontext pre_dispatch torch no_grad fn_to_trace = functional_call full_args = First params NB It REQUIRED parameters come first Inductor infers fixed parameters looking difference parameter count outside inside AOTAutograd assumes prefix arguments fixed arguments full_args extend params_and_buffers_flat Next input args full_args extend args ctx fx_g metadata in_spec out_spec = _aot_export_function fn_to_trace full_args decompositions=decompositions num_params_buffers=params_len no_tangents=True pre_dispatch=pre_dispatch dynamic_shapes=dynamic_shapes trace_joint=trace_joint kwargs=kwargs TODO subsume path aot_stage _graph_capture path trace_joint wraps functional_call flattened_joint args The idea here joint graph AOTAutograd creates has some strict properties It accepts two arguments primals tangents pytree_flattens them It returns tuple fw_outs gradients This very useful convention anyone who wants partition joint graph into separate forward backward graph However people exporting single joint graph would preferable have any pytrees graph We guaranteed aot_export_module case forward outputs loss there therefore no tangents needed run joint graph AOTAutograd creates grad_input every input forward including None s inputs grad-requiring tensors we don t want these our export graph there therefore no tangents needed run joint graph This function fixes both above removing any tangent inputs removing pytrees original FX graph fake_tangents = None _ range metadata num_outputs + metadata num_mutated_inp_runtime_indices fw_outs gradients = fx_g args fake_tangents assert len gradients == len args output_gradients = grad zip args gradients isinstance torch Tensor requires_grad assert grad None \ Found parameter did receive gradient This most likely bug needs supported please comment Github issue https github com pytorch pytorch issues output_gradients append grad assert grad None fw_outs output_gradients fx_g = make_fx flattened_joint record_module_stack=True full_args user_args_flat = pytree arg_tree_leaves args kwargs fx_g create_graph_signature fx_g metadata in_spec out_spec user_args_flat=user_args_flat params_and_buffers_flat=params_and_buffers_flat param_names=list named_parameters keys buffer_names=list named_buffers keys trace_joint=trace_joint num_user_fw_outs=num_fw_outs loss_index=output_loss_index aot_export_joint_simple func Callable args trace_joint bool It looks like main consequence API dynamic shapes will assume params buffers static With new inferred dynamic shapes API maybe doesn t matter num_params_buffers int = decompositions Optional dict = None - torch fx GraphModule A simplified version export Used higher order operators This function makes high-level no calling convention changes guarantee - If no inputs require grad so we export inference graph there no calling convention change between exported graph func - If least one input requires grad so we trace out export joint fw-bw graph Then you partition graph into separate forward backward graph The forward graph will have no calling convention changes compared func The above also relies some strong restrictions around which functions API accepts ` args ` cannot contain any pytrees they must have been pytree_flattened already ` func ` cannot mutate any inputs The outputs ` func ` cannot alias any inputs Note function only lightly tested today It will probably tested more heavily higher order ops trace_joint ctx = nullcontext Run under no_grad so our tracing machinery only traces inference graph ctx = torch no_grad ctx fx_g metadata in_spec out_spec = _aot_export_function func args decompositions=decompositions trace_joint=trace_joint in_spec _kw_in_spec = in_spec children At point we can just directly joint inference graph we traced First though bunch assertions make sure our graph doesn t require any calling convention changes compared original function These restrictions addition general restrictions export No input mutations len x x metadata input_info x mutates_data x mutates_metadata = raise RuntimeError f aot_export_joint_simple does support input mutations str metadata No output aliasing len x x metadata output_info x output_type = OutputType non_alias = raise RuntimeError f aot_export_joint_simple does support outputs alias inputs str metadata No pytrees in_spec is_leaf raise RuntimeError f aot_export_joint_simple requires inputs single list tuple in_spec= str in_spec all child is_leaf child in_spec children raise RuntimeError f aot_export_joint_simple requires individual inputs pytrees in_spec= str in_spec out_spec is_leaf raise RuntimeError f aot_export_joint_simple requires outputs single list tuple out_spec= str out_spec all child is_leaf child out_spec children raise RuntimeError f aot_export_joint_simple requires individual outputs pytrees out_spec= str out_spec TODO we might have temporarily patch config functionalize_rng so doesn t run when we re exporting higher order op config debug_assert Smoke test after partitioning we can run forward without any calling convention changes fw_module _bw_module = aot_config default_partition noqa F fx_g args num_fwd_outputs=len fw_metadata output_infos noqa F Attempt run fw_module original user inputs fake_mode = detect_fake_mode args fake_mode None fake_mode = FakeTensorMode fake_mode fw_module args fx_g Private now because we aren t providing contract what joint graphs we could when there s clearer use case In future we may need add more export API s provide their own strong guarantees This meant general helper function handling various export-y use cases _aot_export_function func Callable args num_params_buffers int = decompositions Optional dict = None If we re exporting joint graph we don t want any tangent inputs graph because we backpropping through scalar loss we need explicitly specify include tangents graph It s enough just check our tangent scalar since we also need know no need make graph input something requiring graph input We don t know info trace time though so we need make explicit config no_tangents bool = False pre_dispatch bool = False If None ` dynamic_shapes ` will inferred inputs inferred result might wrong dynamic_shapes Optional bool = None keep_input_mutations bool = False Under export configures whether we getting inference training IR trace_joint bool = False kwargs=None - tuple torch fx GraphModule ViewAndMutationMeta pytree TreeSpec pytree TreeSpec kwargs = kwargs flat_fn out_spec = create_tree_flattened_fn func args kwargs flat_args in_spec = pytree tree_flatten args kwargs fake_mode = None dynamic_shapes None Try infer ` dynamic_shapes inputs graph nodes fake_mode = detect_fake_mode flat_args fake_mode None hasattr func _orig_mod isinstance func _orig_mod torch fx GraphModule vals = node meta val node func _orig_mod graph nodes val node meta fake_mode = detect_fake_mode vals dynamic_shapes = fake_mode None fake_mode shape_env None The export use case doesn t care about several bits AOTConfig compilers we just export graph partitioners export only full graph user can partition themselves aot_config = AOTConfig fw_compiler=None bw_compiler=None inference_compiler=None partition_fn=None decompositions=decompositions num_params_buffers=num_params_buffers aot_id=next AOT_COUNTER For now there s no use case involving keeping input mutations graph which we can only do inference case anyway We can add later we need keep_inference_input_mutations=keep_input_mutations dynamic_shapes=dynamic_shapes aot_autograd_arg_pos_to_source=None is_export=True no_tangents=no_tangents pre_dispatch=pre_dispatch export_trace_joint=trace_joint fake_mode None fake_mode shape_env = construct_fake_mode flat_args aot_config shape_env = fake_mode shape_env fake_flat_args = process_inputs flat_args aot_config fake_mode shape_env TODO Improve descs here pytree information fake_flat_args_descs = PlainAOTInput i i range len fake_flat_args contextlib ExitStack stack aot_state = create_aot_state stack flat_fn fake_flat_args fake_flat_args_descs aot_config fake_mode shape_env aot_graph_capture = aot_stage _graph_capture aot_state flat_fn fx_g meta = aot_stage _export aot_state aot_graph_capture fx_g meta in_spec out_spec spec compiled_function = aot_function compiled_module = aot_module