Owner s oncall quantization torch torch nn nn torch nn functional F torch ao nn intrinsic quantized nniq torch ao nn quantized nnq torch ao quantization default_qconfig torch ao quantization observer MinMaxObserver PerChannelMinMaxObserver torch ao quantization quantize_fx prepare_fx convert_fx torch ao quantization fx _equalize _InputEqualizationObserver _WeightEqualizationObserver calculate_equalization_scale default_equalization_qconfig _convert_equalization_ref get_layer_sqnr_dict get_equalization_qconfig_dict torch testing _internal common_quantization NodeSpec ns QuantizationTestCase SingleLayerLinearModel TwoLayerLinearModel LinearAddModel SingleLayerFunctionalLinearModel TwoLayerFunctionalLinearModel FunctionalLinearAddModel ConvModel TwoLayerConvModel SingleLayerFunctionalConvModel TwoLayerFunctionalConvModel skipIfNoFBGEMM LinearReluModel LinearReluLinearModel LinearReluAddModel FunctionalLinearReluModel FunctionalLinearReluLinearModel ConvReluModel ConvReluConvModel ConvReluAddModel FunctionalConvReluModel FunctionalConvReluConvModel torch testing _internal common_utils raise_on_run_directly Standard Libraries copy numpy np Testing utils hypothesis given hypothesis strategies st default_qconfig_dict = default_qconfig specific_qconfig_dict = None object_type nn Linear default_qconfig F linear default_qconfig nn ReLU default_qconfig F relu default_qconfig nn Conv d default_qconfig F conv d default_qconfig default_equalization_qconfig_dict = None object_type nn Linear default_equalization_qconfig F linear default_equalization_qconfig nn ReLU default_equalization_qconfig F relu default_equalization_qconfig nn Conv d default_equalization_qconfig F conv d default_equalization_qconfig TestEqualizeFx QuantizationTestCase channel_minmax input axis= Finds min max inputs associated specific channel size_of_tensor_dim = input ndim axis_list = list range size_of_tensor_dim axis_list remove axis axis_list sort reverse=True mins = input copy maxs = input copy axis_list mins = mins min maxs = maxs max mins maxs given ndim=st sampled_from input_qdtype=st sampled_from torch qint torch quint input_qscheme=st sampled_from torch per_tensor_affine torch per_tensor_symmetric weight_qdtype=st sampled_from torch qint torch quint weight_qscheme=st sampled_from torch per_channel_affine torch per_channel_symmetric torch per_channel_affine_float_qparams test_input_weight_eq_observer ndim input_qdtype input_qscheme weight_qdtype weight_qscheme sizes = _ range ndim - sizes append np random randint channel = np random randint ndim == x = np random random size= sizes channel w = np random random size= sizes channel ndim == x = np random random size= sizes channel sizes w = np random random size= sizes channel sizes ndim == x = np random random size= sizes channel sizes sizes w = np random random size= sizes channel sizes sizes ndim == x = np random random size= sizes channel sizes sizes sizes w = np random random size= sizes channel sizes sizes sizes x = x round decimals= astype np float w = w round decimals= astype np float input_eq_obs = _InputEqualizationObserver dtype=input_qdtype qscheme=input_qscheme weight_eq_obs = _WeightEqualizationObserver dtype=weight_qdtype qscheme=weight_qscheme ret_x = input_eq_obs torch tensor x ret_w = weight_eq_obs torch tensor w assertEqual ret_x ret_w x w Check min max input columns correct ref_min_inputs ref_max_inputs = channel_minmax x min_inputs max_inputs = input_eq_obs get_input_minmax assertEqual min_inputs torch tensor ref_min_inputs dtype=torch float assertEqual max_inputs torch tensor ref_max_inputs dtype=torch float Check min max weight columns correct ref_min_weights_col ref_max_weights_col = channel_minmax w min_weights_col max_weights_col = weight_eq_obs get_weight_col_minmax assertEqual min_weights_col torch tensor ref_min_weights_col dtype=torch float assertEqual max_weights_col torch tensor ref_max_weights_col dtype=torch float Check equalization scale correct equalization_scale = calculate_equalization_scale input_eq_obs weight_eq_obs ref_equalization_scale = np sqrt ref_max_weights_col - ref_min_weights_col ref_max_inputs - ref_min_inputs assertEqual equalization_scale torch tensor ref_equalization_scale dtype=torch float input_eq_obs set_equalization_scale equalization_scale weight_eq_obs set_equalization_scale equalization_scale Check input scale zero-point values min_input_scaled max_input_scaled = input_eq_obs calculate_scaled_minmax input_quant_obs = MinMaxObserver dtype=input_qdtype qscheme=input_qscheme input_quant_obs min_val = min_input_scaled input_quant_obs max_val = max_input_scaled input_qparams = input_quant_obs calculate_qparams ref_min_input_scaled = np min ref_min_inputs ref_equalization_scale ref_min_input_scaled = min ref_min_input_scaled ref_max_input_scaled = np max ref_max_inputs ref_equalization_scale ref_max_input_scaled = max ref_max_input_scaled input_qscheme == torch per_tensor_symmetric ref_scale = max abs ref_min_input_scaled ref_max_input_scaled ref_zero_point = input_qdtype torch qint ref_scale = ref_max_input_scaled - ref_min_input_scaled quant_min = - input_qdtype torch qint quant_max = input_qdtype torch qint ref_zero_point = quant_min - np round ref_min_input_scaled ref_scale np clip ref_zero_point quant_min quant_max assertEqual input_qparams item ref_scale atol= e- rtol= assertEqual input_qparams item ref_zero_point During input-weight equalization we will scale weights so following weight quantized observer will have correct scaled qparams Check weight scale zero-point values quantized observer weight_quant_obs = PerChannelMinMaxObserver ch_axis= dtype=weight_qdtype qscheme=weight_qscheme Scale weights input-weight equalization new_shape = w ndim new_shape = w shape ref_w_scaled = w np reciprocal ref_equalization_scale reshape tuple new_shape w = torch tensor w new_shape = w size w_scaled = torch mul w torch reciprocal equalization_scale view new_shape assertEqual w_scaled ref_w_scaled Call forward weight quantization observer weight_quant_obs w_scaled Check min max weight rows correct ref_min_weights_scaled ref_max_weights_scaled = channel_minmax ref_w_scaled assertEqual weight_quant_obs min_val torch tensor ref_min_weights_scaled dtype=torch float assertEqual weight_quant_obs max_val torch tensor ref_max_weights_scaled dtype=torch float weight_qparams = weight_quant_obs calculate_qparams weight_qscheme == torch per_channel_symmetric ref_min_weights_scaled = np minimum np zeros ref_min_weights_scaled shape ref_min_weights_scaled ref_max_weights_scaled = np maximum np zeros ref_max_weights_scaled shape ref_max_weights_scaled ref_scales = np maximum np abs ref_min_weights_scaled ref_max_weights_scaled ref_zero_points = np zeros_like ref_scales weight_qdtype torch qint np ones_like ref_scales weight_qscheme == torch per_channel_affine_float_qparams ref_scales = ref_max_weights_scaled - ref_min_weights_scaled ref_scales = np where ref_scales e- ref_scales np ones_like ref_scales ref_zero_points = - ref_min_weights_scaled ref_scales ref_min_weights_scaled = np minimum np zeros_like ref_min_weights_scaled ref_min_weights_scaled ref_max_weights_scaled = np maximum np zeros_like ref_max_weights_scaled ref_max_weights_scaled ref_scales = ref_max_weights_scaled - ref_min_weights_scaled ref_zero_points = - weight_qdtype torch qint ref_zero_points = ref_zero_points - np round ref_min_weights_scaled ref_scales assertEqual weight_qparams torch tensor ref_scales dtype=weight_qparams dtype rtol= e- atol= assertEqual weight_qparams torch tensor ref_zero_points dtype=weight_qparams dtype rtol= e- atol= test_input_weight_equalization_prepare Tests graphs created after prepare_fx expected single_nn_layer_node_occurrence = ns call_module _InputEqualizationObserver ns call_module MinMaxObserver two_nn_layer_node_occurrence = ns call_module _InputEqualizationObserver ns call_module MinMaxObserver single_F_layer_node_occurrence = ns call_module _InputEqualizationObserver ns call_module _WeightEqualizationObserver ns call_module MinMaxObserver two_F_layer_node_occurrence = ns call_module _InputEqualizationObserver ns call_module _WeightEqualizationObserver ns call_module MinMaxObserver fp_F_layer_node_occurrence = ns call_module _InputEqualizationObserver ns call_module _WeightEqualizationObserver ns call_module MinMaxObserver tests = SingleLayerLinearModel single_nn_layer_node_occurrence TwoLayerLinearModel two_nn_layer_node_occurrence TwoLayerFunctionalLinearModel two_F_layer_node_occurrence FunctionalLinearAddModel fp_F_layer_node_occurrence LinearReluModel single_nn_layer_node_occurrence LinearReluLinearModel two_nn_layer_node_occurrence FunctionalLinearReluModel single_F_layer_node_occurrence FunctionalLinearReluLinearModel two_F_layer_node_occurrence ConvModel single_nn_layer_node_occurrence TwoLayerConvModel two_nn_layer_node_occurrence TwoLayerFunctionalConvModel two_F_layer_node_occurrence ConvReluModel single_nn_layer_node_occurrence ConvReluConvModel two_nn_layer_node_occurrence FunctionalConvReluModel single_F_layer_node_occurrence FunctionalConvReluConvModel two_F_layer_node_occurrence M node_occurrence tests m = M eval example_inputs = m get_example_inputs prepared = prepare_fx m specific_qconfig_dict example_inputs=example_inputs _equalization_config=default_equalization_qconfig_dict checkGraphModuleNodes prepared expected_node_occurrence=node_occurrence test_input_weight_equalization_branching Tests graphs containing branches prepared correctly Specifically equalization observers should inserted front branches which both initial layers branches plan quantized Tests we do add equalization observer due both initial nodes branch containing layers need equalized Note should print out warning messages being able equalize layers linear linear because part branch TestBranchingWithoutEqualizationModel nn Module __init__ - None super __init__ linear = nn Linear linear = nn Linear forward x y = linear x z = linear x torch add y z no_eq_branching_node_occurrence = ns call_module _InputEqualizationObserver ns call_module MinMaxObserver m = TestBranchingWithoutEqualizationModel eval example_inputs = torch rand prepared = prepare_fx m specific_qconfig_dict example_inputs=example_inputs _equalization_config=default_equalization_qconfig_dict checkGraphModuleNodes prepared expected_node_occurrence=no_eq_branching_node_occurrence Tests we will add equalization observer because there only one initial node branch needs equalized TestBranchingWithEqualizationModel nn Module __init__ - None super __init__ linear = nn Linear forward x y = linear x z = torch add x torch add y z eq_branching_node_occurrence = ns call_module _InputEqualizationObserver ns call_module MinMaxObserver m = TestBranchingWithEqualizationModel eval example_inputs = torch randn prepared = prepare_fx m specific_qconfig_dict example_inputs=example_inputs _equalization_config=default_equalization_qconfig_dict checkGraphModuleNodes prepared expected_node_occurrence=eq_branching_node_occurrence skipIfNoFBGEMM test_input_weight_equalization_convert Tests modified model equalization before quantization returns same output original model tests = SingleLayerLinearModel LinearAddModel TwoLayerLinearModel SingleLayerFunctionalLinearModel FunctionalLinearAddModel TwoLayerFunctionalLinearModel LinearReluModel LinearReluLinearModel LinearReluAddModel FunctionalLinearReluModel FunctionalLinearReluLinearModel ConvModel TwoLayerConvModel SingleLayerFunctionalConvModel TwoLayerFunctionalConvModel ConvReluModel ConvReluConvModel ConvReluAddModel FunctionalConvReluModel FunctionalConvReluConvModel M ndim tests m = M eval ndim == x = torch rand ndim == x = torch rand example_inputs = x prepared = prepare_fx copy deepcopy m specific_qconfig_dict example_inputs=example_inputs _equalization_config=default_equalization_qconfig_dict output = prepared x convert_ref = _convert_equalization_ref prepared convert_ref_output = convert_ref x prepared = prepare_fx m specific_qconfig_dict example_inputs=example_inputs _equalization_config=default_equalization_qconfig_dict prepared x convert_fx prepared Check compile assertEqual output convert_ref_output calculate_equalization_scale_ref x w Calculates equalization scale based input weight min_inputs = x min axis= max_inputs = x max axis= min_weights_col = w min axis= max_weights_col = w max axis= equalization_scale = np sqrt max_weights_col - min_weights_col max_inputs - min_inputs equalization_scale get_expected_eq_scales model x For each module graph we want calculate equalization scale point This only works models containing single connected linear layers exp_eq_scales = _ module model named_children weight = module weight detach numpy bias = module bias detach numpy eq_scale = calculate_equalization_scale_ref x weight exp_eq_scales append eq_scale x = x weight T + bias exp_eq_scales test_input_weight_equalization_equalization_scales After applying equalization functions check equalization scales expected values tests = SingleLayerLinearModel TwoLayerLinearModel SingleLayerFunctionalLinearModel TwoLayerFunctionalLinearModel x = torch rand M tests m = M eval exp_eq_scales = get_expected_eq_scales m x detach numpy example_inputs = x prepared = prepare_fx m specific_qconfig_dict example_inputs=example_inputs _equalization_config=default_equalization_qconfig_dict prepared example_inputs convert_ref = _convert_equalization_ref prepared convert_ref x counter = node convert_ref graph nodes equalization_scale node name node op == get_attr assertEqual convert_ref get_buffer str node target reshape - exp_eq_scales counter counter += get_expected_weights_bias model x exp_eq_scales For each module graph we want calculate expected scaled weight bias values This only works models containing single connected linear layers exp_weights = exp_bias = i _ module enumerate model named_children weight = module weight detach numpy bias = module bias detach numpy scaled_weight = weight np reciprocal exp_eq_scales i scaled_bias = bias i + len exp_eq_scales scaled_weight = scaled_weight T exp_eq_scales i + T scaled_bias = scaled_bias T exp_eq_scales i + T exp_weights append scaled_weight exp_bias append scaled_bias x = x weight T + bias exp_weights exp_bias test_input_weight_equalization_weights_bias After applying equalization functions check weights biases expected tests = SingleLayerLinearModel TwoLayerLinearModel SingleLayerFunctionalLinearModel TwoLayerFunctionalLinearModel x = torch rand M tests m = M eval exp_eq_scales = get_expected_eq_scales m x detach numpy exp_weights exp_bias = get_expected_weights_bias m x detach numpy exp_eq_scales example_inputs = x prepared = prepare_fx m specific_qconfig_dict example_inputs=example_inputs _equalization_config=default_equalization_qconfig_dict prepared x convert_ref = _convert_equalization_ref prepared convert_ref x modules = dict convert_ref named_modules remove_duplicate=False counter = node convert_ref graph nodes node op == call_module isinstance modules str node target nn Linear assertEqual modules str node target weight exp_weights counter assertEqual modules str node target bias exp_bias counter counter += get_expected_inp_act_vals model x exp_eq_scales exp_weights exp_bias For each module graph we want calculate expected min max values every input activation node This only works models containing only single connected linear layers x = x exp_eq_scales exp_inp_activation_vals = i _ enumerate model named_children exp_inp_activation_vals append x min x max x = x exp_weights i T + exp_bias i exp_inp_activation_vals append x min x max exp_inp_activation_vals get_expected_weight_act_vals exp_weights For each module graph we want calculate expected min max values every weight activation node This assuming weight observers all MinMaxObservers exp_weight_activation_vals = w exp_weights exp_weight_activation_vals append w min w max exp_weight_activation_vals test_input_weight_equalization_activation_values After applying equalization functions check input observer s min max values expected tests = SingleLayerLinearModel TwoLayerLinearModel SingleLayerFunctionalLinearModel x = torch rand torch manual_seed M tests m = M eval exp_eq_scales = get_expected_eq_scales m x detach numpy exp_weights exp_bias = get_expected_weights_bias m x detach numpy exp_eq_scales exp_inp_act_vals = get_expected_inp_act_vals m x exp_eq_scales exp_weights exp_bias exp_weight_act_vals = get_expected_weight_act_vals exp_weights example_inputs = x prepared = prepare_fx m specific_qconfig_dict example_inputs=example_inputs _equalization_config=default_equalization_qconfig_dict prepared x convert_ref = _convert_equalization_ref prepared convert_ref x modules = dict convert_ref named_modules remove_duplicate=False inp_counter = weight_counter = node convert_ref graph nodes users = list node users node op == call_module isinstance modules str node target MinMaxObserver len users == users target == torch nn functional linear users args == node Check min max values weight activation layers exp_min_val exp_max_val = exp_weight_act_vals weight_counter assertEqual modules str node target min_val exp_min_val assertEqual modules str node target max_val exp_max_val weight_counter += Check min max values input activation layers exp_min_val exp_max_val = exp_inp_act_vals inp_counter assertEqual modules str node target min_val exp_min_val assertEqual modules str node target max_val exp_max_val inp_counter += check_orig_and_eq_graphs orig_model eq_model Given non-equalized model equalized model check graphs structured same way except equalized model has additional equalization_scale mul nodes orig_idx = orig_nodes = list orig_model graph nodes orig_modules = dict orig_model named_modules remove_duplicate=False eq_idx = eq_nodes = list eq_model graph nodes eq_modules = dict eq_model named_modules remove_duplicate=False while orig_idx len orig_nodes eq_idx len eq_nodes equalization_scale eq_nodes eq_idx name mul eq_nodes eq_idx + name Skip equalization mul nodes eq_idx += continue orig_nodes orig_idx op = eq_nodes eq_idx op False orig_nodes orig_idx op == call_module Check type call_modules same ex nn Linear MinMaxObserver orig_node = orig_nodes orig_idx eq_node = eq_nodes eq_idx type orig_modules orig_node target type eq_modules eq_node target False orig_nodes orig_idx op == call_function Check call_functions same ex F linear orig_node = orig_nodes orig_idx eq_node = eq_nodes eq_idx orig_node target = eq_node target False eq_idx += orig_idx += True skipIfNoFBGEMM test_input_weight_equalization_graphs Tests modified model equalization has same graph structure model without equalization before after quantization linear_node_list = ns call_function torch mul ns call_function torch quantize_per_tensor ns call_module nnq Linear ns call_method dequantize linearAdd_node_list = ns call_function torch mul ns call_function torch quantize_per_tensor ns call_module nnq Linear ns call_method dequantize ns call_function torch add ns call_function torch mul ns call_function torch quantize_per_tensor ns call_module nnq Linear ns call_method dequantize linear _node_list = ns call_function torch mul ns call_function torch quantize_per_tensor ns call_module nnq Linear ns call_module nnq Linear ns call_method dequantize functionalLinear_node_list = ns call_function torch mul ns call_function torch quantize_per_tensor ns call_function torch ops quantized linear ns call_method dequantize functionalLinearAdd_node_list = ns call_function torch mul ns call_function torch quantize_per_tensor ns call_function torch ops quantized linear ns call_method dequantize ns call_function torch add ns call_function torch mul ns call_function torch quantize_per_tensor ns call_function torch ops quantized linear ns call_method dequantize functionalLinear _node_list = ns call_function torch mul ns call_function torch quantize_per_tensor ns call_function torch ops quantized linear ns call_function torch ops quantized linear ns call_method dequantize linearRelu_node_list = ns call_function torch mul ns call_function torch quantize_per_tensor ns call_module nniq LinearReLU ns call_method dequantize linearReluLinear_node_list = ns call_function torch mul ns call_function torch quantize_per_tensor ns call_module nniq LinearReLU ns call_module nnq Linear ns call_method dequantize functionalLinearRelu_node_list = ns call_function torch mul ns call_function torch quantize_per_tensor ns call_function torch ops quantized linear_relu ns call_method dequantize functionalLinearReluLinear_node_list = ns call_function torch mul ns call_function torch quantize_per_tensor ns call_function torch ops quantized linear_relu ns call_function torch ops quantized linear ns call_method dequantize conv_node_list = ns call_function torch mul ns call_function torch quantize_per_tensor ns call_module nnq Conv d ns call_method dequantize conv _node_list = ns call_function torch mul ns call_function torch quantize_per_tensor ns call_module nnq Conv d ns call_module nnq Conv d ns call_method dequantize functionalConv_node_list = ns call_function torch mul ns call_function torch quantize_per_tensor ns call_function torch ops quantized conv d ns call_method dequantize functionalConv _node_list = ns call_function torch mul ns call_function torch quantize_per_tensor ns call_function torch ops quantized conv d ns call_function torch ops quantized conv d ns call_method dequantize convRelu_node_list = ns call_function torch mul ns call_function torch quantize_per_tensor ns call_module nniq ConvReLU d ns call_method dequantize convReluConv_node_list = ns call_function torch mul ns call_function torch quantize_per_tensor ns call_module nniq ConvReLU d ns call_module nnq Conv d ns call_method dequantize functionalConvRelu_node_list = ns call_function torch mul ns call_function torch quantize_per_tensor ns call_function torch ops quantized conv d_relu ns call_method dequantize functionalConvReluConv_node_list = ns call_function torch mul ns call_function torch quantize_per_tensor ns call_function torch ops quantized conv d_relu ns call_function torch ops quantized conv d ns call_method dequantize tests = SingleLayerLinearModel linear_node_list LinearAddModel linearAdd_node_list TwoLayerLinearModel linear _node_list SingleLayerFunctionalLinearModel functionalLinear_node_list FunctionalLinearAddModel functionalLinearAdd_node_list TwoLayerFunctionalLinearModel functionalLinear _node_list LinearReluModel linearRelu_node_list LinearReluLinearModel linearReluLinear_node_list FunctionalLinearReluModel functionalLinearRelu_node_list FunctionalLinearReluLinearModel functionalLinearReluLinear_node_list ConvModel conv_node_list TwoLayerConvModel conv _node_list SingleLayerFunctionalConvModel functionalConv_node_list TwoLayerFunctionalConvModel functionalConv _node_list ConvReluModel convRelu_node_list ConvReluConvModel convReluConv_node_list FunctionalConvReluModel functionalConvRelu_node_list FunctionalConvReluConvModel functionalConvReluConv_node_list M node_list tests m = M eval example_inputs = m get_example_inputs prepared = prepare_fx m specific_qconfig_dict example_inputs=example_inputs _equalization_config=default_equalization_qconfig_dict equalized_quantized_model = convert_fx prepared Check order nodes graph checkGraphModuleNodes equalized_quantized_model expected_node_list=node_list skipIfNoFBGEMM test_input_weight_equalization_results Tests small models results quantized models have been equalized very close models have been equalized tests = SingleLayerLinearModel TwoLayerLinearModel LinearAddModel SingleLayerFunctionalLinearModel TwoLayerFunctionalLinearModel x = torch rand M tests m = M eval No equalization example_inputs = x prepared = prepare_fx copy deepcopy m specific_qconfig_dict example_inputs=example_inputs _equalization_config= prepared x quantized = convert_fx prepared Check compile quantized_output = quantized x With equalization prepared = prepare_fx copy deepcopy m specific_qconfig_dict example_inputs=example_inputs _equalization_config=default_equalization_qconfig_dict prepared x equalized_and_quantized = convert_fx prepared Check compile equalized_and_quantized_output = equalized_and_quantized x assertEqual quantized_output equalized_and_quantized_output rtol= e- atol= skipIfNoFBGEMM test_selective_equalization Tests we able run numeric suite equalized model construct valid equalization_config equalizing only top layers highest quantization errors torch manual_seed M nn Module __init__ - None super __init__ bot = torch nn Sequential torch nn Linear top = torch nn Sequential torch nn Linear forward x x = bot x x = torch add x x = top x x float_model = M eval Hard coded so top layer has higher quantization error x = torch tensor Quantize float model example_inputs = x prepared_model = prepare_fx copy deepcopy float_model specific_qconfig_dict example_inputs=example_inputs prepared_model x quantized_model = convert_fx copy deepcopy prepared_model Get SQNR between float quantized model layer_to_sqnr_dict = get_layer_sqnr_dict copy deepcopy prepared_model quantized_model x Construct equalization_qconfig_dict equalizing layers highest quantization errors selective_equalization_qconfig_dict = get_equalization_qconfig_dict layer_to_sqnr_dict Create selectively equalized model prepared_model = prepare_fx copy deepcopy float_model specific_qconfig_dict example_inputs=example_inputs _equalization_config=selective_equalization_qconfig_dict prepared_model x equalized_model = convert_fx prepared_model node_list = ns call_function torch quantize_per_tensor ns call_module nnq Linear ns call_method dequantize ns call_function torch add ns call_function torch mul ns call_function torch quantize_per_tensor ns call_module nnq Linear ns call_method dequantize Check order nodes graph checkGraphModuleNodes equalized_model expected_node_list=node_list __name__ == __main__ raise_on_run_directly test test_quantization py