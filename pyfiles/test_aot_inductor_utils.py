Owner s module inductor copy os shutil tempfile types typing Any Optional TYPE_CHECKING Union torch torch _export torch _inductor torch export _trace torch fx _pytree fx_pytree torch _dynamo testing same torch _inductor config torch _inductor test_case TestCase torch testing FileCheck torch testing _internal common_utils IS_FBCODE run_tests torch testing _internal inductor_utils clone_preserve_strides_offset torch utils _pytree pytree TYPE_CHECKING torch _C _aoti AOTIModelContainerRunner WrapperModule torch nn Module __init__ model super __init__ model = model forward args kwargs model args kwargs AOTIRunnerUtil staticmethod legacy_compile model example_inputs options=None dynamic_shapes=None disable_constraint_solver=False isinstance model torch nn Module model = WrapperModule model The exact API subject change torch _inductor config is_predispatch ep = torch export _trace _export model example_inputs dynamic_shapes=dynamic_shapes pre_dispatch=True gm = ep module torch _export config patch use_new_tracer_experimental=True gm = torch export _trace _export_to_torch_ir model example_inputs dynamic_shapes=dynamic_shapes disable_constraint_solver=disable_constraint_solver Disabling flag because instead we can rely mapping dynamo_flat_name_to_original_fqn which coming Dynamo restore_fqn=False IS_FBCODE deeplearning aot_inductor extern_node_thrift_serializer thrift_serializer options None options = options extern_node_serializer = thrift_serializer torch no_grad so_path = torch _inductor aot_compile gm example_inputs options=options type ignore arg-type so_path staticmethod legacy_load_runner device so_path str - AOTIModelContainerRunner IS_FBCODE fb test_aot_inductor_model_runner_pybind manual tempfile TemporaryDirectory temp_dir copy so file unique path just before loading avoid stale dlopen handles when updated so same path loaded repetitively test temp_so_path = os path join temp_dir model so shutil copy so_path temp_so_path We also need copy over serialized extern_kernel_nodes custom ops extern_kernel_nodes_path = f so_path - json os path isfile extern_kernel_nodes_path temp_extern_kernel_nodes_path = os path join temp_dir model json shutil copy extern_kernel_nodes_path temp_extern_kernel_nodes_path test_aot_inductor_model_runner_pybind Runner temp_so_path device == cpu device == cpu torch _C _aoti AOTIModelContainerRunnerCpu so_path device == xpu torch _C _aoti AOTIModelContainerRunnerXpu so_path device device == mps torch _C _aoti AOTIModelContainerRunnerMps so_path torch _C _aoti AOTIModelContainerRunnerCuda so_path device staticmethod legacy_load device so_path TODO unify fbcode oss behavior only use torch _export aot_load IS_FBCODE runner = AOTIRunnerUtil legacy_load_runner device so_path optimized args kwargs call_spec = runner get_call_spec in_spec = pytree treespec_loads call_spec out_spec = pytree treespec_loads call_spec flat_inputs = fx_pytree tree_flatten_spec args kwargs in_spec flat_inputs = x x flat_inputs isinstance x torch Tensor flat_outputs = runner run flat_inputs pytree tree_unflatten flat_outputs out_spec optimized torch _export aot_load so_path device staticmethod legacy_run device str model example_inputs options=None dynamic_shapes=None disable_constraint_solver=False so_path = AOTIRunnerUtil legacy_compile model example_inputs options=options dynamic_shapes=dynamic_shapes disable_constraint_solver=disable_constraint_solver optimized = AOTIRunnerUtil legacy_load device so_path optimized example_inputs staticmethod compile model Union torch nn Module types FunctionType example_inputs tuple torch Tensor inductor_configs Optional dict str Any = None dynamic_shapes Optional Union dict str Any tuple Any list Any = None isinstance model torch nn Module This should really default behavior torch export export model = WrapperModule model torch no_grad torch _export config patch use_new_tracer_experimental=True strict=False needs extra migration work ep = torch export export model example_inputs dynamic_shapes=dynamic_shapes strict=True prefer_deferred_runtime_asserts_over_guards=True package_path = torch _inductor aoti_compile_and_package ep inductor_configs=inductor_configs package_path staticmethod run model Union torch nn Module types FunctionType example_inputs tuple torch Tensor inductor_configs Optional dict str Any = None dynamic_shapes Optional Union dict str Any tuple Any list Any = None package_path = AOTIRunnerUtil compile model example_inputs inductor_configs=inductor_configs dynamic_shapes=dynamic_shapes optimized = torch _inductor aoti_load_package package_path optimized example_inputs staticmethod run_multiple model Union torch nn Module types FunctionType list_example_inputs list tuple torch Tensor inductor_configs Optional dict str Any = None dynamic_shapes Optional Union dict str Any tuple Any list Any = None package_path = AOTIRunnerUtil compile model list_example_inputs inductor_configs=inductor_configs dynamic_shapes=dynamic_shapes optimized = torch _inductor aoti_load_package package_path list_output_tensors = example_inputs list_example_inputs list_output_tensors append optimized example_inputs list_output_tensors check_model TestCase model example_inputs options=None dynamic_shapes=None atol=None rtol=None torch no_grad config patch aot_inductor allow_stack_allocation allow_stack_allocation aot_inductor use_minimal_arrayref_interface use_minimal_arrayref_interface torch manual_seed isinstance model types FunctionType model = model device For non mixed device inputs default cpu set device manually all t device type == cpu t example_inputs isinstance t torch Tensor example_inputs = tuple clone_preserve_strides_offset x device=self device x example_inputs ref_model = copy deepcopy model ref_inputs = copy deepcopy example_inputs expected = ref_model ref_inputs torch manual_seed actual = AOTIRunnerUtil run model example_inputs options dynamic_shapes assertEqual actual expected atol=atol rtol=rtol check_model_with_multiple_inputs TestCase model list_example_inputs options=None dynamic_shapes=None torch no_grad config patch aot_inductor allow_stack_allocation allow_stack_allocation aot_inductor use_minimal_arrayref_interface use_minimal_arrayref_interface torch manual_seed model = model device ref_model = copy deepcopy model ref_inputs = copy deepcopy list_example_inputs list_expected = ref_model inputs inputs ref_inputs torch manual_seed list_actual = AOTIRunnerUtil run_multiple model list_example_inputs options dynamic_shapes assertTrue same list_actual list_expected code_check_count TestCase model example_inputs target_str str target_count int torch no_grad config patch aot_inductor allow_stack_allocation allow_stack_allocation aot_inductor use_minimal_arrayref_interface use_minimal_arrayref_interface package_path = torch _export aot_compile model example_inputs open os path splitext package_path + cpp cpp src_code = cpp read FileCheck check_count target_str target_count exactly=True run src_code __name__ == __main__ run_tests