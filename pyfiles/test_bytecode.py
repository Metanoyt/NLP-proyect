Owner s oncall mobile fnmatch io shutil tempfile pathlib Path torch torch utils show_pickle torch utils mobile_optimizer optimize_for_mobile torch jit mobile _backport_for_mobile _backport_for_mobile_to_buffer _get_mobile_model_contained_types _get_model_bytecode_version _get_model_ops_and_info _load_for_lite_interpreter torch testing _internal common_utils run_tests TestCase pytorch_test_dir = Path __file__ resolve parents script_module_v ptl script_module_v ptl source code TestModule torch nn Module __init__ v super __init__ x = v forward y int increment = torch ones dtype=torch float x + y + increment output_model_path = Path tmpdirname script_module_v ptl script_module = torch jit script TestModule optimized_scripted_module = optimize_for_mobile script_module exported_optimized_scripted_module = optimized_scripted_module _save_for_lite_interpreter str output_model_path SCRIPT_MODULE_V _BYTECODE_PKL = __torch__ TestModule forward instructions STOREN DROPR LOADC LOADC MOVE OP LOADC OP RET operators aten add int aten add Scalar constants torch _utils _rebuild_tensor_v pers obj storage torch DoubleStorage cpu False collections OrderedDict types register_size arguments name type __torch__ TestModule default_value None name y type int default_value None returns name type Tensor default_value None SCRIPT_MODULE_V _BYTECODE_PKL = __torch__ TestModule forward instructions STOREN DROPR LOADC LOADC MOVE OP LOADC OP RET operators aten add int aten add Scalar constants torch _utils _rebuild_tensor_v pers obj storage torch DoubleStorage constants cpu False collections OrderedDict types register_size arguments name type __torch__ TestModule default_value None name y type int default_value None returns name type Tensor default_value None SCRIPT_MODULE_V _BYTECODE_PKL = __torch__ TestModule forward instructions STOREN DROPR LOADC LOADC MOVE OP OP RET operators aten add int aten add Scalar constants torch _utils _rebuild_tensor_v pers obj storage torch DoubleStorage cpu False collections OrderedDict types register_size arguments name type __torch__ TestModule default_value None name y type int default_value None returns name type Tensor default_value None SCRIPT_MODULE_BYTECODE_PKL = bytecode_pkl SCRIPT_MODULE_V _BYTECODE_PKL model_name script_module_v ptl The minimum version model can backported Need updated when bytecode version completely retired MINIMUM_TO_VERSION = testVariousModelVersions TestCase test_get_model_bytecode_version check_model_version model_path expect_version actual_version = _get_model_bytecode_version model_path assert actual_version == expect_version version model_info SCRIPT_MODULE_BYTECODE_PKL items model_path = pytorch_test_dir cpp jit model_info model_name check_model_version model_path version test_bytecode_values_for_all_backport_functions Find maximum version checked models start backporting minimum support version comparing bytecode pkl content It can t merged test ` test_all_backport_functions ` because optimization dynamic content might change when optimize function changes This test focuses bytecode pkl content validation For content validation byte byte check regular expression matching The wildcard can used skip some specific content comparison maximum_checked_in_model_version = max SCRIPT_MODULE_BYTECODE_PKL keys current_from_version = maximum_checked_in_model_version tempfile TemporaryDirectory tmpdirname while current_from_version MINIMUM_TO_VERSION Load model v run forward method model_name = SCRIPT_MODULE_BYTECODE_PKL current_from_version model_name input_model_path = pytorch_test_dir cpp jit model_name A temporary model file will export path run through bytecode pkl content check tmp_output_model_path_backport = Path tmpdirname tmp_script_module_backport ptl current_to_version = current_from_version - backport_success = _backport_for_mobile input_model_path tmp_output_model_path_backport current_to_version assert backport_success expect_bytecode_pkl = SCRIPT_MODULE_BYTECODE_PKL current_to_version bytecode_pkl buf = io StringIO torch utils show_pickle main tmpdirname + + tmp_output_model_path_backport name + bytecode pkl output_stream=buf output = buf getvalue acutal_result_clean = join output split expect_result_clean = join expect_bytecode_pkl split isMatch = fnmatch fnmatch acutal_result_clean expect_result_clean assert isMatch current_from_version -= shutil rmtree tmpdirname Please run test manually when working backport This test passes OSS fails internally likely due missing step build test_all_backport_functions Backport latest bytecode version minimum support version Load run backport model check version TestModule torch nn Module __init__ v super __init__ x = v forward y int increment = torch ones dtype=torch float x + y + increment module_input = expected_mobile_module_result = torch ones dtype=torch float temporary input model file output model file will exported temporary folder tempfile TemporaryDirectory tmpdirname tmp_input_model_path = Path tmpdirname tmp_script_module ptl script_module = torch jit script TestModule optimized_scripted_module = optimize_for_mobile script_module exported_optimized_scripted_module = optimized_scripted_module _save_for_lite_interpreter str tmp_input_model_path current_from_version = _get_model_bytecode_version tmp_input_model_path current_to_version = current_from_version - tmp_output_model_path = Path tmpdirname tmp_script_module_backport ptl while current_to_version = MINIMUM_TO_VERSION Backport latest model ` to_version ` tmp file tmp_script_module_backport backport_success = _backport_for_mobile tmp_input_model_path tmp_output_model_path current_to_version assert backport_success backport_version = _get_model_bytecode_version tmp_output_model_path assert backport_version == current_to_version Load model run forward method mobile_module = _load_for_lite_interpreter str tmp_input_model_path mobile_module_result = mobile_module module_input torch testing assert_close mobile_module_result expected_mobile_module_result current_to_version -= Check backport failure case backport_success = _backport_for_mobile tmp_input_model_path tmp_output_model_path MINIMUM_TO_VERSION - assert backport_success need clean folder before closes otherwise will run into git clean error shutil rmtree tmpdirname Check just test_backport_bytecode_from_file_to_file mechanism function implementations test_backport_bytecode_from_file_to_file maximum_checked_in_model_version = max SCRIPT_MODULE_BYTECODE_PKL keys script_module_v _path = pytorch_test_dir cpp jit SCRIPT_MODULE_BYTECODE_PKL maximum_checked_in_model_version model_name maximum_checked_in_model_version MINIMUM_TO_VERSION tempfile TemporaryDirectory tmpdirname tmp_backport_model_path = Path tmpdirname tmp_script_module_v _backported_to_v ptl backport file success = _backport_for_mobile script_module_v _path tmp_backport_model_path maximum_checked_in_model_version - assert success buf = io StringIO torch utils show_pickle main tmpdirname + + tmp_backport_model_path name + bytecode pkl output_stream=buf output = buf getvalue expected_result = SCRIPT_MODULE_V _BYTECODE_PKL acutal_result_clean = join output split expect_result_clean = join expected_result split isMatch = fnmatch fnmatch acutal_result_clean expect_result_clean assert isMatch Load model v run forward method mobile_module = _load_for_lite_interpreter str tmp_backport_model_path module_input = mobile_module_result = mobile_module module_input expected_mobile_module_result = torch ones dtype=torch float torch testing assert_close mobile_module_result expected_mobile_module_result shutil rmtree tmpdirname Check just _backport_for_mobile_to_buffer mechanism function implementations test_backport_bytecode_from_file_to_buffer maximum_checked_in_model_version = max SCRIPT_MODULE_BYTECODE_PKL keys script_module_v _path = pytorch_test_dir cpp jit SCRIPT_MODULE_BYTECODE_PKL maximum_checked_in_model_version model_name maximum_checked_in_model_version MINIMUM_TO_VERSION Backport model v script_module_v _buffer = _backport_for_mobile_to_buffer script_module_v _path maximum_checked_in_model_version - Check version model v backport bytesio = io BytesIO script_module_v _buffer backport_version = _get_model_bytecode_version bytesio assert backport_version == maximum_checked_in_model_version - Load model v backport run forward method bytesio = io BytesIO script_module_v _buffer mobile_module = _load_for_lite_interpreter bytesio module_input = mobile_module_result = mobile_module module_input expected_mobile_module_result = torch ones dtype=torch float torch testing assert_close mobile_module_result expected_mobile_module_result test_get_model_ops_and_info TODO update more style above tests after backport - exists script_module_v = pytorch_test_dir cpp jit script_module_v ptl ops_v = _get_model_ops_and_info script_module_v assert ops_v aten add int num_schema_args == assert ops_v aten add Scalar num_schema_args == test_get_mobile_model_contained_types MyTestModule torch nn Module forward x x + sample_input = torch tensor script_module = torch jit script MyTestModule script_module sample_input buffer = io BytesIO script_module _save_to_buffer_for_lite_interpreter buffer seek type_list = _get_mobile_model_contained_types buffer assert len type_list = __name__ == __main__ run_tests