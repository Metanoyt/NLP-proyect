Owner s module intel contextlib functools inspect itertools math random functools partial itertools product numpy np torch torch _inductor decomposition torch _higher_order_ops out_dtype out_dtype torch fx experimental proxy_tensor make_fx torch testing make_tensor torch testing _internal common_device_type dtypes instantiate_device_type_tests onlyNativeDeviceTypes precisionOverride torch testing _internal common_quantization _dynamically_quantize_per_channel torch testing _internal common_utils iter_indices parametrize run_tests TestCase contextlib contextmanager tf _off enabled = torch backends mkldnn enabled deterministic = torch backends mkldnn deterministic torch backends mkldnn flags enabled=enabled deterministic=deterministic allow_tf =False yield contextlib contextmanager tf _on tf _precision= e- enabled = torch backends mkldnn enabled deterministic = torch backends mkldnn deterministic old_precision = precision try precision = tf _precision torch backends mkldnn flags enabled=enabled deterministic=deterministic allow_tf =True yield finally precision = old_precision This wrapper wraps test run test twice one allow_tf =True another allow_tf =False When running allow_tf =True will use reduced precision specified argument For example dtypes torch float torch float torch complex torch complex tf _on_and_off test_matmul device dtype = b = c = torch matmul b assertEqual c expected In above example when testing torch float matmul will running TF mode TF mode off TF mode assertEqual will use reduced precision check values This decorator can used function without device dtype such tf _on_and_off test_my_op tf _on_and_off test_my_op device tf _on_and_off test_my_op device dtype tf _on_and_off test_my_op dtype tf _on_and_off tf _precision= e- with_tf _disabled function_call tf _off function_call with_tf _enabled function_call tf _on tf _precision function_call wrapper f params = inspect signature f parameters arg_names = tuple params keys functools wraps f wrapped args kwargs kwargs update zip arg_names args cond = True device kwargs cond = cond torch device kwargs device type == xpu dtype kwargs cond = cond kwargs dtype torch float TODO add complex cond with_tf _disabled kwargs lambda f kwargs with_tf _enabled kwargs lambda f kwargs f kwargs wrapped wrapper This wrapper wraps test run TF turned off This wrapper designed used when test uses matmul convolutions purpose test testing matmul convolutions Disabling TF will enforce torch float tensors always computed full precision with_tf _off f functools wraps f wrapped args kwargs tf _off f args kwargs wrapped TestBasicGEMM TestCase _test_addmm_addmv f t m v alpha=None beta=None transpose_out=False activation=None dtype = t dtype numpy_dtype = dtype dtype torch bfloat torch half numpy_dtype = torch float dtype is_complex alpha = + j alpha None alpha beta = + j beta None beta alpha = alpha None alpha beta = beta None beta activation == gelu res = f t m v alpha=alpha beta=beta use_gelu=True res = f t m v alpha=alpha beta=beta res = torch full_like res math nan transpose_out res = res t clone memory_format=torch contiguous_format t activation == gelu f t m v alpha=alpha beta=beta out=res use_gelu=True f t m v alpha=alpha beta=beta out=res m numpy_dtype cpu numpy v numpy_dtype cpu numpy res = alpha m numpy_dtype cpu numpy v numpy_dtype cpu numpy beta = res += beta t numpy_dtype cpu numpy activation == relu res = res res activation == gelu res _t = torch from_numpy res dtype approximate = tanh t is_cuda none res _t = torch nn functional gelu res _t approximate=approximate res = res _t numpy_dtype cpu numpy assert activation None f unsupported activation activation res = torch from_numpy res dtype assertEqual res res assertEqual res res _test_addmm_impl func activation device dtype M = torch randn device= cpu dtype=torch float dtype device m = torch randn device= cpu dtype=torch float dtype device m = torch randn device= cpu dtype=torch float dtype device _test_addmm_addmv func M m m activation=activation vector-shaped bias beta= result epilogue fusion CUDA V = torch randn device= cpu dtype=torch float dtype device _test_addmm_addmv func V m m beta= activation=activation Test -strided M = torch randn device= cpu dtype=torch float dtype expand device m = torch randn device= cpu dtype=torch float dtype expand device m = torch randn device= cpu dtype=torch float dtype device _test_addmm_addmv func M m m activation=activation Test beta= M=nan M = torch full math nan device= cpu dtype=torch float dtype device m = torch randn device= cpu dtype=torch float dtype device m = torch randn device= cpu dtype=torch float dtype device _test_addmm_addmv func M m m beta= activation=activation Test transpose t t t t itertools product True False repeat= maybe_transpose cond m cond m m t clone memory_format=torch contiguous_format t M = maybe_transpose t torch randn device=device dtype m = maybe_transpose t torch randn device=device dtype m = maybe_transpose t torch randn device=device dtype _test_addmm_addmv func M m m transpose_out=t activation=activation t use vector V instead matrix M epilogue fusion CUDA doesn t depend t _test_addmm_addmv func V m m beta= transpose_out=t activation=activation precisionOverride torch float e- torch double e- torch half e- dtypes torch float torch half torch double torch complex tf _on_and_off test_addmm device dtype _test_addmm_impl torch addmm None device dtype precisionOverride torch float e- torch double e- torch half e- dtypes torch float torch half torch double test_addmm_badmm_scalar_tnesor_input device dtype input = torch tensor device=device dtype=dtype test addmm mat = torch randn device=device dtype mat = torch randn device=device dtype result = torch addmm input mat mat ref = mat cpu numpy mat cpu numpy + assertEqual result ref test baddbmm mat = torch randn device=device dtype mat = torch randn device=device dtype result = torch baddbmm input mat mat ref = mat cpu numpy mat cpu numpy + assertEqual result ref precisionOverride torch bfloat e- torch half e- torch float e- dtypes torch bfloat torch half torch float torch double tf _on_and_off test_addmv device dtype have use torch randn bfloat instead torch randn dtype=bfloat randn does support bfloat yet reduce errors low precision ts = torch randn device=device dtype torch randn device=device dtype expand vs = torch randn device=device dtype torch ones device=device dtype expand reduce errors low precision ms = d torch ones device=device dtype expand reduce errors low precision d torch randn device=device dtype expand initialization reduces errors low precision broadcasted matrices making sure intermediate result values exactly representable low precision type torch randint dtype=torch float device=device dtype expand d torch randn device=device dtype torch randn device=device dtype t m v t itertools product ms vs ts _test_addmm_addmv torch addmv t m v Test beta= t=nan t = torch full math nan device=device dtype m v itertools product ms vs _test_addmm_addmv torch addmv t m v beta= dtypes torch half torch float torch float torch complex tf _on_and_off test_mm device dtype _test_mm n m p dtype genf helper function matrixmultiply mat mat n = mat size m = mat size p = mat size dtype_ = torch float dtype == torch half dtype dtype == torch half mat = mat float mat = mat float res = torch zeros n p dtype=dtype_ device=device i j iter_indices res res i j = sum mat i k mat k j k range m res half dtype == torch half res contiguous case mat = genf n m mat = genf m p res = torch mm mat mat res = matrixmultiply mat mat assertEqual res res non contiguous case mat = genf n m mat = genf p m t res = torch mm mat mat res = matrixmultiply mat mat assertEqual res res non contiguous case mat = genf m n t mat = genf m p res = torch mm mat mat res = matrixmultiply mat mat assertEqual res res non contiguous case mat = genf m n t mat = genf p m t res = torch mm mat mat res = matrixmultiply mat mat assertEqual res res test zero stride mat = genf n m mat = genf m expand m p res = torch mm mat mat res = matrixmultiply mat mat assertEqual res res explicitly exercise _out variant torch mm contiguous case mat = genf n m mat = genf m p res = genf n p torch mm mat mat out=res res = matrixmultiply mat mat assertEqual res res explicitly exercise _out variant torch mm non contiguous case mat = genf m n t mat = genf p m t res = genf n p torch mm mat mat out=res res = matrixmultiply mat mat assertEqual res res genf_int x y torch randint x y dtype=dtype device=device genf_bfloat x y torch randn x y dtype=torch float device=device dtype genf_float x y torch randn x y dtype=dtype device=device genf_Half x y torch randn x y dtype=dtype device=device n m p dtype == torch int dtype == torch int genf = genf_int dtype == torch bfloat genf = genf_bfloat dtype == torch half genf = genf_Half genf = genf_float _test_mm n m p dtype genf precisionOverride torch half torch bfloat dtypes torch float torch bfloat torch half torch float torch complex tf _on_and_off test_bmm device dtype batch_sizes = M N O = numpy_dtype = dtype dtype = torch bfloat torch float invert_perm p d = x i i x enumerate p d d d generate_inputs num_batches transposed tensors perm perm itertools product itertools permutations repeat= b = make_tensor num_batches M N dtype=dtype device=device low=- high= b = make_tensor num_batches N O dtype=dtype device=device low=- high= b = b permute perm contiguous permute invert_perm perm b = b permute perm contiguous permute invert_perm perm yield b b broadcasting tensors b b b b b b itertools product True False repeat= shape = num_batches b M b N b shape = num_batches b N b O b b = make_tensor shape dtype=dtype device=device low=- high= expand num_batches M N b = make_tensor shape dtype=dtype device=device low=- high= expand num_batches N O yield b b zero-sized tensors z z z z itertools product True False repeat= shape = num_batches z M z N z shape = num_batches z N z O z b = torch randn shape dtype=dtype device=device b = torch randn shape dtype=dtype device=device yield b b num_batches batch_sizes b b perm itertools product generate_inputs num_batches itertools permutations res = torch bmm b b res = torch full num_batches M O math nan dtype=dtype device=device permute perm contiguous permute invert_perm perm torch bmm b b out=res expect = torch from_numpy b numpy_dtype cpu numpy b numpy_dtype cpu numpy device=device dtype=dtype assertEqual expect res assertEqual expect res device_type == cuda check mixed arguments rejected assertRaises RuntimeError lambda torch bmm b b cpu assertRaises RuntimeError lambda torch bmm b cpu b assertRaises RuntimeError lambda torch bmm b b out=res cpu _test_addbmm_baddbmm func b b ref out_tensor getattr out_tensor func + _ b b assertEqual out_tensor ref res = out_tensor clone assertWarnsOnceRegex UserWarning f This overload func _ deprecated getattr out_tensor func + _ b b assertEqual out_tensor ref getattr res func + _ b b beta= assertEqual out_tensor res assertWarnsOnceRegex UserWarning f This overload func _ deprecated getattr out_tensor func + _ b b assertEqual out_tensor ref getattr res func + _ b b beta= alpha= assertEqual out_tensor res assertWarnsOnceRegex UserWarning f This overload func deprecated assertEqual out_tensor getattr torch func out_tensor b b res = getattr torch func out_tensor b b beta= alpha= assertEqual res ref nan = torch full_like out_tensor math nan res = getattr torch func nan b b beta= alpha= assertEqual res ref b is_complex res = getattr torch func out_tensor b b beta= j alpha= j assertEqual res out_tensor j + j ref res = getattr torch func out_tensor b b beta= alpha= assertEqual res out_tensor + ref res = torch full_like out_tensor math nan getattr torch func nan b b beta= out=res assertEqual res ref precisionOverride torch half torch bfloat dtypes torch float torch float torch bfloat torch half torch complex tf _on_and_off test_addbmm device dtype num_batches = M N O = invert_perm p d = x i i x enumerate p d d d generate_tensor numpy_dtype = dtype dtype = torch bfloat torch float transposed tensors perm perm itertools product itertools permutations repeat= perm itertools permutations b = make_tensor num_batches M N dtype=dtype device=device low=- high= b = make_tensor num_batches N O dtype=dtype device=device low=- high= b = b permute perm contiguous permute invert_perm perm b = b permute perm contiguous permute invert_perm perm ref = torch from_numpy b numpy_dtype cpu numpy b numpy_dtype cpu numpy device=device dtype=dtype sum out_tensor = torch zeros_like ref permute perm contiguous permute perm yield b b ref out_tensor broadcasting tensors s s s s s s itertools product True False repeat= shape = num_batches s M s N s shape = num_batches s N s O s b = make_tensor shape dtype=dtype device=device low=- high= expand num_batches M N b = make_tensor shape dtype=dtype device=device low=- high= expand num_batches N O ref = torch from_numpy b numpy_dtype cpu numpy b numpy_dtype cpu numpy device=device dtype=dtype sum out_tensor = torch zeros_like ref yield b b ref out_tensor zero-sized tensors z z z z itertools product True False repeat= shape = num_batches z M z N z shape = num_batches z N z O z b = make_tensor shape dtype=dtype device=device low=- high= b = make_tensor shape dtype=dtype device=device low=- high= ref = torch from_numpy b numpy_dtype cpu numpy b numpy_dtype cpu numpy device=device dtype=dtype sum out_tensor = torch zeros_like ref yield b b ref out_tensor b b ref out_tensor generate_tensor _test_addbmm_baddbmm addbmm b b ref out_tensor precisionOverride torch half torch bfloat torch float e- dtypes torch float torch float torch bfloat torch half torch complex tf _on_and_off test_baddbmm device dtype num_batches = M N O = invert_perm p d = x i i x enumerate p d d d generate_tensor numpy_dtype = dtype dtype torch bfloat torch half torch float transposed tensors perm perm perm itertools product itertools permutations repeat= b = make_tensor num_batches M N dtype=dtype device=device low=- high= b = make_tensor num_batches N O dtype=dtype device=device low=- high= b = b permute perm contiguous permute invert_perm perm b = b permute perm contiguous permute invert_perm perm ref = torch from_numpy b numpy_dtype cpu numpy b numpy_dtype cpu numpy device=device dtype=dtype out_tensor = torch zeros_like ref out_tensor = out_tensor permute perm contiguous permute invert_perm perm yield b b ref out_tensor broadcasting tensors s s s s s s itertools product True False repeat= shape = num_batches s M s N s shape = num_batches s N s O s b = make_tensor shape dtype=dtype device=device low=- high= expand num_batches M N b = make_tensor shape dtype=dtype device=device low=- high= expand num_batches N O ref = torch from_numpy b numpy_dtype cpu numpy b numpy_dtype cpu numpy device=device dtype=dtype out_tensor = torch zeros_like ref yield b b ref out_tensor zero-sized tensors z z z z itertools product True False repeat= shape = num_batches z M z N z shape = num_batches z N z O z b = make_tensor shape dtype=dtype device=device low=- high= b = make_tensor shape dtype=dtype device=device low=- high= ref = torch from_numpy b numpy_dtype cpu numpy b numpy_dtype cpu numpy device=device dtype=dtype out_tensor = torch zeros_like ref yield b b ref out_tensor b b ref out_tensor generate_tensor _test_addbmm_baddbmm baddbmm b b ref out_tensor tf _on_and_off test_tensordot device = torch arange device=device reshape b = torch arange device=device reshape c = torch tensordot b dims= cpu cn = torch from_numpy np tensordot cpu numpy b cpu numpy axes= assertEqual c cn cout = torch zeros device=device torch tensordot b dims= out=cout cpu assertEqual c cout = torch randn device=device b = torch randn device=device c = torch tensordot b dims= cpu cn = torch from_numpy np tensordot cpu numpy b cpu numpy axes= assertRaisesRegex RuntimeError expects dims = torch tensordot b dims=- assertEqual c cn c = torch tensordot b cpu cn = torch from_numpy np tensordot cpu numpy b cpu numpy assertEqual c cn = torch tensordot torch tensor torch tensor = torch from_numpy np tensordot np zeros dtype=np float np zeros dtype=np float assertEqual dtypes torch float torch double precisionOverride torch float e- tf _on_and_off test_ _sized_with_ _strided device dtype = make_tensor dtype=dtype device=device a_strided = torch as_strided size= stride= b = make_tensor dtype=dtype device=device b_strided = torch as_strided b size= stride= res = torch bmm a_strided b_strided expect = torch from_numpy a_strided cpu numpy b_strided cpu numpy device=device dtype=dtype assertEqual expect res _select_broadcastable_dims dims_full=None select full dimensionality dims_full None dims_full = ndims = random randint dims_full = random randint _ range ndims ndims = len dims_full select actual dimensions ops larger full ndims individual sizes may reduced smaller possibly reduced ndims sizes may reduced smaller_ndims = random randint ndims dims_small = dims_large = i range ndims - - - j = random randint j == no reduced singleton dimension ds = dims_full i dl = dims_full i j == larger may have reduced singleton dimension ds = dims_full i dl = len dims_small smaller_ndims dims_full i j == smaller may have reduced singleton dimension ds = dl = dims_full i dims_large = dl + dims_large len dims_small smaller_ndims dims_small = ds + dims_small dims_small dims_large dims_full tf _on_and_off test_broadcast_fused_matmul device fns = baddbmm addbmm addmm addmv addr fn fns batch_dim = random randint n_dim = random randint m_dim = random randint p_dim = random randint dims_full_for_fn fn == baddbmm batch_dim n_dim p_dim batch_dim n_dim m_dim batch_dim m_dim p_dim fn == addbmm n_dim p_dim batch_dim n_dim m_dim batch_dim m_dim p_dim fn == addmm n_dim p_dim n_dim m_dim m_dim p_dim fn == addmv n_dim n_dim m_dim m_dim fn == addr n_dim m_dim n_dim m_dim raise AssertionError unknown function t _dims_full t _dims t _dims = dims_full_for_fn t _dims_small _ _ = _select_broadcastable_dims t _dims_full t _small = torch randn t _dims_small device=device float t = torch randn t _dims device=device float t = torch randn t _dims device=device float t _full = t _small expand t _dims_full device fntorch = getattr torch fn r = fntorch t _small t t r = fntorch t _full t t assertEqual r r dtypes torch float torch float tf _on_and_off test_strided_mm_bmm device dtype Tests strided view case stride smaller than corresponding dimension size x = torch tensor dtype=dtype device=device new_shape = new_stride = sx = torch as_strided x size=new_shape stride=new_stride torch_fn = lambda x torch bmm x x noqa E np_fn = lambda x np matmul x x noqa E compare_with_numpy torch_fn np_fn sx torch_fn = lambda x torch mm x x noqa E compare_with_numpy torch_fn np_fn sx tf _on_and_off test_mm_empty_inputs_mixed_dtype_errors device = torch randint dtype=torch int device=device b = torch randn dtype=torch float device=device assertRaisesRegex RuntimeError expected have same dtype got torch mm b tf _on_and_off test_matmul_ device https github com pytorch pytorch issues = torch rand device=device dtype=torch half b = torch rand device=device dtype=torch half c = torch full math nan dtype=torch half device=device cpu_result = torch matmul cpu float b cpu float half torch matmul b out=c assertEqual c cpu_result dtypes torch int torch int torch int torch float torch float torch float tf _on_and_off test_baddbmm_input_dtypes_compatibility device dtype batch = torch rand dtype=torch float device=device batch = torch rand dtype=torch float device=device input_tensor = torch rand device=device dtype dtype = torch float assertRaisesRegex RuntimeError Input dtypes must same torch baddbmm input_tensor batch batch beta= out = torch randn dtype=dtype device=device fill_ torch nan y_ref = torch bmm batch batch torch baddbmm input_tensor batch batch beta= out=out assertEqual out y_ref dtypes torch float tf _on_and_off test_baddbmm_nan_input_with_zero_beta device dtype shape mat mat = torch randn shape dtype=dtype device=device _ range inputs = torch randn shape dtype=dtype device=device torch randn shape dtype=dtype device=device fill_ torch nan outs = None torch randn shape dtype=dtype device=device torch randn shape dtype=dtype device=device fill_ torch nan options = itertools product inputs outs input out options y_ref = torch bmm mat mat y = torch baddbmm input mat mat beta= out=out assertEqual y_ref y precisionOverride torch double e- dtypes torch float torch double tf _on_and_off test_addmm_sizes device dtype m n k M = torch randn n m device=device dtype m = torch randn n k device=device dtype m = torch randn k m device=device dtype _test_addmm_addmv torch addmm M m m m = torch randn n k + device=device dtype m = torch randn k m device=device dtype assertRaisesRegex RuntimeError f n x k + k x m lambda torch addmm M m m assertRaisesRegex RuntimeError f n x k + k x m lambda torch mm m m precisionOverride torch double e- torch float e- torch bfloat e- torch half e- torch cfloat e- torch cdouble e- dtypes torch double torch float torch bfloat torch half tf _on_and_off test_addmm_gelu device dtype _test_addmm_impl torch _addmm_activation gelu device dtype precisionOverride torch double e- torch float e- torch bfloat e- torch half e- torch cfloat e- torch cdouble e- dtypes torch double torch float torch bfloat torch half tf _on_and_off test_addmm_relu device dtype _test_addmm_impl torch _addmm_activation relu device dtype dtypes torch float torch bfloat torch half tf _on_and_off test_addmv_rowmajor_colmajor_incx_incy_lda device dtype tests o s s o output size s summed size o = s = a_data = torch arange o s + device=device dtype=dtype view o s x_data = torch arange s + device=device dtype=dtype y_data = torch ones o device=device dtype=dtype _test row_major incx incy lda_tail row_major a_storage = torch full o s + lda_tail float nan device=device dtype=dtype a_storage = torch full s o + lda_tail float nan device=device dtype=dtype permute = a_storage o s copy_ a_data x_storage = torch full s incx float nan device=device dtype=dtype x = x_storage copy_ x_data y_storage = torch full o incy float nan device=device dtype=dtype y = y_storage copy_ y_data _test_addmm_addmv torch addmv y x row_major incx incy lda_tail itertools product False True _test row_major incx incy lda_tail precisionOverride torch double e- torch float e- torch bfloat torch half e- torch cfloat e- torch cdouble e- dtypes torch double torch bfloat torch half torch float tf _on_and_off test_corner_cases_of_cublasltmatmul device dtype common case M = torch randn device=device dtype m = torch randn device=device dtype m = torch randn device=device dtype torch nn functional linear m m M Ntrans_B has ld rows m = torch rand dtype device t m = torch rand dtype device t M = torch rand dtype device torch addmm M m t m trans_A has ld rows m = torch rand dtype device t m = torch randn device=device dtype M = torch rand dtype device torch addmm M m m large tensor dim M = torch randn device=device dtype m = torch randn device=device dtype m = torch randn device=device dtype torch nn functional linear m m M test_blas_empty device fn torchfn args test_out=False kwargs call_torch_fn args kwargs torchfn tuple torch randn shape device=device isinstance shape tuple shape shape args kwargs result = call_torch_fn args kwargs test_out result out = torch full_like result math nan out = call_torch_fn args kwargs out=out noqa F FIXME rec should out out mm addmm assertEqual fn torch mm shape assertEqual fn torch mm shape assertEqual fn torch mm shape assertEqual fn torch mm shape assertEqual torch zeros device=device fn torch mm assertEqual torch zeros device=device fn torch mm test_out=True assertEqual fn torch addmm shape assertEqual fn torch addmm shape t = torch randn device=device assertEqual t fn torch addmm t assertEqual t fn torch addmm t test_out=True mv addmv assertEqual fn torch mv shape assertEqual fn torch mv shape assertEqual torch zeros device=device fn torch mv assertEqual torch zeros device=device fn torch mv test_out=True assertEqual fn torch addmv shape t = torch randn device=device assertEqual t fn torch addmv t assertEqual t fn torch addmv t test_out=True bmm baddbmm assertEqual fn torch bmm shape assertEqual fn torch bmm shape assertEqual fn torch bmm shape assertEqual torch zeros device=device fn torch bmm assertEqual torch zeros device=device fn torch bmm test_out=True assertEqual fn torch baddbmm shape assertEqual fn torch baddbmm shape assertEqual fn torch baddbmm shape assertEqual fn torch baddbmm shape c = torch arange dtype=torch float device=device reshape assertEqual - c fn torch baddbmm c beta=- Issue assertEqual - c fn torch baddbmm c beta=- test_out=True Issue addbmm assertEqual fn torch addbmm shape assertEqual fn torch addbmm shape t = torch randn device=device assertEqual t fn torch addbmm t assertEqual t fn torch addbmm t test_out=True matmul assertEqual torch tensor device=device fn torch matmul assertEqual torch tensor device=device fn torch matmul test_out=True assertEqual fn torch matmul shape assertEqual fn torch matmul shape assertEqual fn torch matmul shape assertEqual torch zeros device=device fn torch matmul assertEqual torch zeros device=device fn torch matmul test_out=True dot assertEqual torch tensor device=device fn torch dot assertEqual torch tensor device=device fn torch dot test_out=True tf _on_and_off test_large_bmm_backward device A = torch randn device=device mT contiguous mT B = torch randn device=device requires_grad=True G = torch randn device=device Should create intermediary tensor size GB memory OOM A B backward G tf _on_and_off test_large_bmm_mm_backward device A = torch randn device=device mT contiguous mT B = torch randn device=device requires_grad=True G = torch randn device=device Should create intermediary tensor size GB memory OOM A B backward G check_single_matmul x y assertEqual answer expected x dtype is_floating_point x dtype is_complex k = max x shape - Scale atol size matrix assertEqual answer expected msg=f x shape x y shape = answer shape atol=k e- rtol= e- assertEqual answer expected msg=f x shape x y shape = answer shape test x y expected = np matmul x cpu y cpu ans = torch matmul x y assertTrue ans is_contiguous assertEqual ans expected test out out = torch empty_like ans ans = torch matmul x y out=out assertIs ans out assertTrue ans is_contiguous assertEqual ans expected gen_sizes_matmul x_dim y_dim= matrix_size= batch_size= Generates sequences tuples x y size x = x_dim size y = y_dim compatible wrt matmul assert x_dim = assert y_dim = x = x_dim y range y_dim + batch mn product product range batch_size repeat=max x - y - product range matrix_size repeat=min y x == size_x = mn size_y = batch + mn yield size_x size_y k range matrix_size size_x = k + mn x size_x = batch - x - + size_x size_y = mn y size_y = batch - y - + size_y yield size_x size_y dtypes torch float test_matmul_small_brute_force_ d_Nd device dtype make_arg = partial make_tensor device=device dtype=dtype size_x size_y nctg_x nctg_y product gen_sizes_matmul True False True False x = make_arg size_x noncontiguous=nctg_x y = make_arg size_y noncontiguous=nctg_y check_single_matmul x y dtypes torch float test_matmul_small_brute_force_ d_Nd device dtype make_arg = partial make_tensor device=device dtype=dtype size_x size_y nctg_x nctg_y product gen_sizes_matmul True False True False x = make_arg size_x noncontiguous=nctg_x y = make_arg size_y noncontiguous=nctg_y check_single_matmul x y dtypes torch float test_matmul_small_brute_force_ d_Nd device dtype make_arg = partial make_tensor device=device dtype=dtype size_x size_y nctg_x nctg_y product gen_sizes_matmul True False True False x = make_arg size_x noncontiguous=nctg_x y = make_arg size_y noncontiguous=nctg_y check_single_matmul x y dtypes torch float tf _on_and_off test_matmul_out_kernel_errors_with_autograd device dtype = torch empty device=device dtype=dtype requires_grad=True unsqueeze b = torch empty device=device dtype=dtype requires_grad=True transpose - - c = torch empty device=device dtype=dtype movedim torch matmul detach b detach out=c assertRaisesRegex RuntimeError functions out= arguments don t support automatic differentiation torch matmul b out=c torch no_grad torch matmul b out=c _group_quantize_tensor w n_bit= q_group_size= w k n = assert w dim == w n k = w = w transpose contiguous assert q_group_size assert w shape - q_group_size == to_quant n k group_size group_size to_quant = w reshape - q_group_size assert torch isnan to_quant sum == max_val = to_quant amax dim= keepdim=True min_val = to_quant amin dim= keepdim=True max_int = n_bit - min_int = scales = max_val - min_val clamp min= e- max_int assert torch isnan scales sum == zeros = min_int - min_val div scales round zeros = torch clamp zeros min_int max_int zeros = zeros torch int assert torch isnan zeros sum == out = to_quant div scales add zeros round clamp_ min_int max_int assert torch isnan out sum == n k out = out dtype=torch int reshape w shape out device = torch device cpu out = out &#124; out torch uint Scales zeros same q-group should contiguous so we can load -bit word scales = scales view w shape - transpose contiguous zeros = zeros view w shape - transpose contiguous out scales zeros parametrize m parametrize k parametrize n test__int _mm device m k n q_group = inner_k_tiles = torch manual_seed a_bf = torch rand m k dtype=torch float device=device b_bf = torch rand k n dtype=torch float device=device convert_weight_to_int pack b b_uint n k b_uint scales zeros = _group_quantize_tensor b n_bit= q_group_size=q_group b_int pack k n b_int pack = torch _convert_weight_to_int pack b_uint inner_k_tiles b_int pack scales zeros weight_int pack_mm b_int pack qscale qzeros torch _weight_int pack_mm_with_scales_and_zeros b_int pack q_group qscale qzeros b_int pack b_scales zeros_int = convert_weight_to_int pack b_bf dtype torch bfloat torch float = a_bf dtype=dtype b = b_bf dtype=dtype b_scales = b_scales dtype=dtype ref = torch mm b res = weight_int pack_mm b_int pack b_scales zeros_int mean_err = res - ref abs ref mean assertTrue mean_err test_mm_with_offset device torch _dynamo testing rand_strided offset = = rand_strided dtype=torch float device=device extra_size=offset = as_strided storage_offset=offset b = rand_strided dtype=torch float device=device gpu_out = torch matmul b cpu_out = torch matmul cpu b cpu assertEqual gpu_out cpu cpu_out parametrize m parametrize k parametrize n parametrize use_transpose_a True False parametrize use_transpose_b True False parametrize non_contig_type test__int_mm device m k n use_transpose_a use_transpose_b non_contig_type non_contig_type whole data buffer contiguous can transposed stride one dimension whole buffer contiguous Neither stride genf_int_float x y use_transpose non_contig_type use_transpose x y = y x non_contig_type = y = y x_int = torch randint - x y dtype=torch int device=device x_float = x_int torch float non_contig_type == x_int = x_int y x_float = x_float y non_contig_type == x_int = x_int x_float = x_float use_transpose x_int t x_float t x_int x_float non_contig_type = m == k == a_int a_float = genf_int_float m k use_transpose_a non_contig_type b_int b_float = genf_int_float k n use_transpose_b non_contig_type c_int = torch _int_mm a_int b_int assertTrue c_int dtype torch int assertEqual c_int device torch device device assertEqual c_int float torch mm a_float b_float c_int _result = c_int new_empty c_int size Checking out variant torch _int_mm a_int b_int out=c_int _result assertEqual c_int _result float torch mm a_float b_float test_out_dtype_inductor_decomp_trace device - None func x w out_dtype torch ops aten mm default torch int x w w = torch randint - dtype=torch int device=device x = torch randint - dtype=torch int device=device Check make_fx inductor decomps produces _int_mm decomp_table = torch _inductor decomposition select_decomp_table gm = make_fx func decomp_table tracing_mode= symbolic x w assertExpectedInline gm code strip \ forward x_ w_ _int_mm = torch ops aten _int_mm default x_ w_ x_ = w_ = None _int_mm test_out_dtype_int_mm_default_trace device - None func x w out_dtype torch ops aten mm default torch int x w w = torch randint - dtype=torch int device=device x = torch randint - dtype=torch int device=device By default out_dtype preserved trace gm = make_fx func tracing_mode= symbolic x w assertExpectedInline gm code strip \ forward x_ w_ out_dtype = torch ops higher_order out_dtype torch ops aten mm default torch int x_ w_ x_ = w_ = None out_dtype onlyNativeDeviceTypes parametrize m parametrize k parametrize n parametrize compile True False parametrize slice True False test__int _mm device m k n compile slice torch manual_seed slice logits generated LLaMA LM head like - activation LM head slice final hidden state shape batch_size sequence_length hidden dim non-contiguous Using arbitrary batch-size here since d converted D batch_size = = torch rand batch_size m k dtype=torch bfloat device=device Make non-contiguous = - = view - size - = torch rand m k dtype=torch bfloat device=device b = torch rand n k dtype=torch bfloat device=device convert_weight_to_int pack b b_int pack b_scales _ = _dynamically_quantize_per_channel b - torch int b_int pack b_scales weight_int pack_mm b_int pack b_scales torch _weight_int pack_mm b_int pack b_scales b_int pack b_scales = convert_weight_to_int pack b compile mod = torch compile weight_int pack_mm mod = weight_int pack_mm res = mod b_int pack b_scales ref = torch mm b transpose mean_err = res - ref abs ref mean assertTrue mean_err instantiate_device_type_tests TestBasicGEMM globals only_for= xpu allow_xpu=True __name__ == __main__ run_tests