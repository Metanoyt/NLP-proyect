mypy allow-untyped-defs functools torch torch nn utils _expanded_weights expanded_weights_impl ExpandedWeight torch utils _pytree pytree dependency ` functional_call ` means can t exposed utils without creating circular dependency call_for_per_sample_grads module batch_size=None loss_reduction= sum batch_first=True r Return forward function module populating grad_sample per sample gradients backward invocation Args module The ` ` nn Module ` ` get per sample gradients respect All trainable parameters will compute per sample gradients located ` ` grad_sample ` ` field when ` ` backward ` ` invoked batch_size The batch size input If None passed all tensor arguments args kwargs must have same batch size which size first dimension Otherwise must passed manually Default None loss_reduction Indicates loss reduction aggregating gradients sum mean operation If mean per sample gradients will scaled batch size offset crossbatch interaction running mean across batch Must mean sum Default sum batch_first Indicates batch dimension first dimension If True batch dimension first dimension If False s second dimension Default True Examples xdoctest +SKIP model = nn Linear batched_input = torch randn batch size res = call_for_per_sample_grads model batched_input sum res backward assert model weight shape == assert model weight grad_sample shape == assert model weight grad None assert model bias shape == assert model bias grad_sample shape == assert model bias grad None An example using mean loss reduction The grad_sample fields will scaled batch_size what they would we ran same code loss_reduction= sum This because mean end will scale all grad_outputs batch_size cross batch interaction model = nn Linear batched_input = torch randn batch size res = call_for_per_sample_grads model loss_reduction= mean batched_input mean res backward Note Does work any ` nn RNN ` including ` nn GRU ` ` nn LSTM ` Please use custom rewrites wrap ` nn Linear ` module See Opacus example maybe_build_expanded_weight og_tensor batch_size og_tensor requires_grad ExpandedWeight og_tensor batch_size loss_reduction og_tensor compute_batch_size args kwargs args_and_kwargs = pytree arg_tree_leaves args kwargs batch_size = None arg args_and_kwargs isinstance arg torch Tensor continue arg_batch_size = arg shape batch_first arg shape batch_size None batch_size = arg_batch_size raise RuntimeError When computing batch size found least one input batch size f batch_size one batch size arg_batch_size Please specify explicitly using batch size kwarg call_for_per_sample_grads batch_size = arg_batch_size batch_size None raise RuntimeError Unable find tensor passed args kwargs They may pytree-able so ExpandedWeights cannot compute batch size inputs Please specify explicitly batch_size loss_reduction sum mean raise RuntimeError f Expected loss_reduction argument sum mean got loss_reduction isinstance module torch nn Module raise RuntimeError f Module passed must nn Module got type module __name__ batch_size None isinstance batch_size int raise RuntimeError f Batch size passed must None integer got type batch_size __name__ batch_size None batch_size raise RuntimeError f Batch size must positive got batch_size weight module parameters hasattr weight grad_sample weight grad_sample None type ignore attr-defined raise RuntimeError Current Expanded Weights accumulates gradients which will incorrect multiple f calls without clearing gradients Please clear out grad_sample parameter weight post issue pytorch pytorch prioritize correct behavior functools wraps module forward wrapper args kwargs wrapper_batch_size = batch_size wrapper_batch_size None wrapper_batch_size = compute_batch_size args kwargs params = name maybe_build_expanded_weight value wrapper_batch_size name value module named_parameters torch func functional_call module params args kwargs wrapper