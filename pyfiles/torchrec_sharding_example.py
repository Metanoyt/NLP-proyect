mypy allow-untyped-defs The following example demonstrates how represent torchrec s embedding sharding DTensor API argparse os functools cached_property typing TYPE_CHECKING torch torch distributed checkpoint metadata ChunkStorageMetadata TensorProperties TensorStorageMetadata torch distributed tensor DeviceMesh DTensor init_device_mesh Replicate Shard torch distributed tensor debug visualize_sharding TYPE_CHECKING torch distributed tensor placement_types Placement get_device_type cuda torch cuda is_available torch cuda device_count = cpu aten = torch ops aten supported_ops = aten view default aten _to_copy default torch Tensor subclass wrapper around all local shards associated single sharded embedding table LocalShardsWrapper torch Tensor local_shards list torch Tensor storage_meta TensorStorageMetadata staticmethod __new__ cls local_shards list torch Tensor offsets list torch Size - LocalShardsWrapper assert len local_shards assert len local_shards == len offsets assert local_shards ndim == we calculate total tensor size concat second tensor dimension cat_tensor_shape = list local_shards shape len local_shards column-wise sharding shard_size s shape s local_shards cat_tensor_shape += shard_size according DCP each chunk expected have same properties TensorStorageMetadata includes Vice versa wrapper s properties should also same its first chunk wrapper_properties = TensorProperties create_from_tensor local_shards wrapper_shape = torch Size cat_tensor_shape chunks_meta = ChunkStorageMetadata o s shape s o zip local_shards offsets r = torch Tensor _make_wrapper_subclass cls wrapper_shape r shards = local_shards r storage_meta = TensorStorageMetadata properties=wrapper_properties size=wrapper_shape chunks=chunks_meta r necessary ops dispatching subclass its local shards classmethod __torch_dispatch__ cls func types args= kwargs=None type ignore override kwargs = kwargs TODO we shall continually extend function support more ops needed func supported_ops res_shards_list = func shard args kwargs pyrefly ignore index-error shard args shards pyrefly ignore index-error LocalShardsWrapper res_shards_list args shard_offsets raise NotImplementedError f func supported LocalShardsWrapper property shards - list torch Tensor local_shards shards setter shards local_shards list torch Tensor local_shards = local_shards cached_property shard_sizes - list torch Size chunk sizes chunk storage_meta chunks cached_property shard_offsets - list torch Size chunk offsets chunk storage_meta chunks run_torchrec_row_wise_even_sharding_example rank world_size row-wise even sharding example One table evenly sharded rows within global ProcessGroup In our example table s num_embedding embedding dim The global ProcessGroup has ranks so each rank will have one local shard device mesh representation worker ranks create -D device mesh includes every rank device_type = get_device_type device = torch device device_type device_mesh = init_device_mesh device_type=device_type mesh_shape= world_size manually create embedding table s local shards num_embeddings = embedding_dim = tensor shape local_shard_shape = torch Size num_embeddings world_size embedding_dim local_rows local_cols tensor offset local_shard_offset = torch Size rank embedding_dim tensor local_tensor = torch randn local_shard_shape device=device row-wise sharding one shard per rank create local shards wrapper pyrefly ignore no-matching-overload local_shards_wrapper = LocalShardsWrapper local_shards= local_tensor offsets= local_shard_offset ########################################################################### example transform local_shards into DTensor usage TorchRec ShardedEmbeddingCollection stores model parallel params _model_parallel_name_to_sharded_tensor which initialized _initialize_torch_state torch Tensor params transformed into ShardedTensor ShardedTensor _init_from_local_shards This allows state_dict always ShardedTensor objects sharding placement we use DTensor represent row-wise sharding row_wise_sharding_placements means global tensor sharded first dim over -d mesh row_wise_sharding_placements list Placement = Shard create DTensor local shard dtensor = DTensor from_local local_shards_wrapper device_mesh row_wise_sharding_placements run_check=False display DTensor s sharding visualize_sharding dtensor header= Row-wise even sharding example DTensor ########################################################################### example transform DTensor into local_shards usage TorchRec In ShardedEmbeddingCollection s load_state_dict pre hook _pre_load_state_dict_hook source param ShardedTensor then we need transform into its local_shards transform DTensor into LocalShardsWrapper dtensor_local_shards = dtensor to_local assert isinstance dtensor_local_shards LocalShardsWrapper shard_tensor = dtensor_local_shards shards assert torch equal shard_tensor local_tensor assert dtensor_local_shards shard_sizes == local_shard_shape unwrap shape assert dtensor_local_shards shard_offsets == local_shard_offset unwrap offset run_torchrec_row_wise_uneven_sharding_example rank world_size row-wise uneven sharding example One table unevenly sharded rows within global ProcessGroup In our example table s num_embedding embedding dim The global ProcessGroup has ranks each rank will have local shard shape rank rank rank rank device mesh representation worker ranks create -D device mesh includes every rank device_type = get_device_type device = torch device device_type device_mesh = init_device_mesh device_type=device_type mesh_shape= world_size manually create embedding table s local shards num_embeddings = embedding_dim = emb_table_shape = torch Size num_embeddings embedding_dim tensor shape local_shard_shape = torch Size embedding_dim rank == torch Size embedding_dim tensor offset local_shard_offset = torch Size rank + rank embedding_dim tensor local_tensor = torch randn local_shard_shape device=device local shards row-wise sharding one shard per rank create local shards wrapper pyrefly ignore no-matching-overload local_shards_wrapper = LocalShardsWrapper local_shards= local_tensor offsets= local_shard_offset ########################################################################### example transform local_shards into DTensor create DTensorMetadata which torchrec should provide row_wise_sharding_placements list Placement = Shard note uneven sharding we need specify shape stride because DTensor would assume even sharding compute shape stride based assumption Torchrec needs pass information explicitly shape stride global tensor s shape stride dtensor = DTensor from_local local_shards_wrapper torch Tensor subclass device_mesh DeviceMesh row_wise_sharding_placements List Placement run_check=False shape=emb_table_shape required uneven sharding stride= embedding_dim so far visualize_sharding cannot print correctly unevenly sharded DTensor because relies offset computation which assumes even sharding visualize_sharding dtensor header= Row-wise uneven sharding example DTensor check dtensor has correct shape stride all ranks assert dtensor shape == emb_table_shape assert dtensor stride == embedding_dim ########################################################################### example transform DTensor into local_shards note DTensor to_local always returns LocalShardsWrapper dtensor_local_shards = dtensor to_local assert isinstance dtensor_local_shards LocalShardsWrapper shard_tensor = dtensor_local_shards shards assert torch equal shard_tensor local_tensor assert dtensor_local_shards shard_sizes == local_shard_shape unwrap shape assert dtensor_local_shards shard_offsets == local_shard_offset unwrap offset run_torchrec_table_wise_sharding_example rank world_size table-wise example each rank global ProcessGroup holds one different table In our example table s num_embedding embedding dim The global ProcessGroup has ranks so each rank will have one complete table its local shard device_type = get_device_type device = torch device device_type note without initializing mesh following local_tensor will put device cuda init_device_mesh device_type=device_type mesh_shape= world_size manually create embedding table s local shards num_embeddings = embedding_dim = emb_table_shape = torch Size num_embeddings embedding_dim table i current rank holds table then local shard LocalShardsWrapper containing tensor otherwise local shard empty torch Tensor table_to_shards = map table_id local shard table_id table_to_local_tensor = map table_id local tensor table_id create embedding tables place them different ranks each rank will hold one complete table dict will store corresponding local shard i range world_size tensor local_tensor = torch randn emb_table_shape device=device rank == i torch empty device=device table_to_local_tensor i = local_tensor tensor offset local_shard_offset = torch Size wrap local shards into wrapper local_shards_wrapper = pyrefly ignore no-matching-overload LocalShardsWrapper local_shards= local_tensor offsets= local_shard_offset rank == i local_tensor table_to_shards i = local_shards_wrapper ########################################################################### example transform local_shards into DTensor table_to_dtensor = same purpose _model_parallel_name_to_sharded_tensor table_wise_sharding_placements = Replicate table-wise sharding table_id local_shards table_to_shards items create submesh only contains rank we place table note we cannot use ` ` init_device_mesh create submesh so we choose use ` DeviceMesh ` api directly create DeviceMesh device_submesh = DeviceMesh device_type=device_type mesh=torch tensor table_id dtype=torch int table ` ` table_id ` ` placed rank ` ` table_id ` ` create DTensor local shard current table note uneven sharding we need specify shape stride because DTensor would assume even sharding compute shape stride based assumption Torchrec needs pass information explicitly dtensor = DTensor from_local local_shards device_submesh table_wise_sharding_placements run_check=False shape=emb_table_shape required uneven sharding stride= embedding_dim table_to_dtensor table_id = dtensor print each table s sharding table_id dtensor table_to_dtensor items visualize_sharding dtensor header=f Table-wise sharding example DTensor Table table_id check dtensor has correct shape stride all ranks assert dtensor shape == emb_table_shape assert dtensor stride == embedding_dim ########################################################################### example transform DTensor into torch Tensor table_id local_tensor table_to_local_tensor items important note DTensor to_local always returns empty torch Tensor no matter what passed DTensor _local_tensor dtensor_local_shards = table_to_dtensor table_id to_local rank == table_id assert isinstance dtensor_local_shards LocalShardsWrapper shard_tensor = dtensor_local_shards shards assert torch equal shard_tensor local_tensor unwrap tensor assert dtensor_local_shards shard_sizes == emb_table_shape unwrap shape assert dtensor_local_shards shard_offsets == torch Size unwrap offset assert dtensor_local_shards numel == run_example rank world_size example_name dict stores example code name_to_example_code = row-wise-even run_torchrec_row_wise_even_sharding_example row-wise-uneven run_torchrec_row_wise_uneven_sharding_example table-wise run_torchrec_table_wise_sharding_example example_name name_to_example_code print f example example_name does exist example run example_func = name_to_example_code example_name set manual seed torch manual_seed run example example_func rank world_size __name__ == __main__ script launched via torchrun which automatically manages ProcessGroup rank = int os environ RANK world_size = int os environ WORLD_SIZE assert world_size == our example uses worker ranks parse arguments parser = argparse ArgumentParser description= torchrec sharding examples formatter_class=argparse RawTextHelpFormatter example_prompt = choose one sharding example below \n \t row-wise-even \n \t row-wise-uneven\n \t table-wise\n e g you want try row-wise even sharding example please input row-wise-even \n parser add_argument -e -- example help=example_prompt required=True args = parser parse_args run_example rank world_size args example