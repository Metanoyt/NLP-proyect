mypy allow-untyped-defs mypy disable-error-code=arg-type This file exports ONNX ops opset __future__ annotations functools sys warnings typing TYPE_CHECKING torch torch _C torch _C _onnx _C_onnx torch onnx errors torch onnx _internal torchscript_exporter _type_utils jit_utils registration symbolic_helper symbolic_opset opset symbolic_opset opset utils TYPE_CHECKING collections abc Sequence EDITING THIS FILE READ THIS FIRST see Note Edit Symbolic Files README md __all__ = add append arange argsort atleast_ d atleast_ d atleast_ d cat chunk clamp_max clamp_min clamp constant_pad_nd cumsum Delete embedding_bag embedding_renorm flatten gather hardtanh hstack im col index_fill index index_copy index_put insert linalg_det linalg_vector_norm logdet masked_scatter masked_select mm narrow normal pad pixel_shuffle pop prim_constant_chunk reflection_pad relu remainder replication_pad round scatter select size sort split_with_sizes split squeeze stack topk unbind unique_dim unsqueeze vstack _onnx_symbolic = functools partial registration onnx_symbolic opset= _onnx_symbolic aten hardtanh symbolic_helper quantized_args True symbolic_helper parse_args v f f hardtanh g jit_utils GraphContext _C Value min_val float max_val float scalar_type = _type_utils JitScalarType from_value _type_utils JitScalarType FLOAT min_val = g op Constant value_t=torch tensor min_val dtype=scalar_type dtype max_val = g op Constant value_t=torch tensor max_val dtype=scalar_type dtype symbolic_helper _op_with_optional_float_cast g Clip min_val max_val opset_before= _onnx_symbolic aten clamp clamp g jit_utils GraphContext min max _cast_if_not_none tensor dtype tensor None symbolic_helper _is_none tensor g op Cast tensor to_i=dtype onnx_type tensor scalar_type = _type_utils JitScalarType from_value _type_utils JitScalarType UNDEFINED scalar_type = _type_utils JitScalarType UNDEFINED min = _cast_if_not_none min scalar_type max = _cast_if_not_none max scalar_type symbolic_helper _is_none min clamp_max g max symbolic_helper _is_none max clamp_min g min symbolic_helper _get_tensor_rank min == symbolic_helper _get_tensor_rank max == symbolic_helper _op_with_optional_float_cast g Clip min max opset_before= clamp_max g clamp_min g min max _onnx_symbolic aten clamp_min symbolic_helper parse_args v v clamp_min g jit_utils GraphContext min min = g op Cast min to_i=_type_utils JitScalarType from_value onnx_type symbolic_helper _get_tensor_rank min == max = opset unused g symbolic_helper _op_with_optional_float_cast g Clip min max opset_before= symbolic_helper _op_with_optional_float_cast g Max min opset_before= _onnx_symbolic aten clamp_max symbolic_helper parse_args v v clamp_max g jit_utils GraphContext max max = g op Cast max to_i=_type_utils JitScalarType from_value onnx_type symbolic_helper _get_tensor_rank max == min = opset unused g symbolic_helper _op_with_optional_float_cast g Clip min max opset_before= symbolic_helper _op_with_optional_float_cast g Min max opset_before= _onnx_symbolic aten relu relu g jit_utils GraphContext input scalar_type = _type_utils JitScalarType from_value input _type_utils JitScalarType FLOAT min_val = g op Constant value_t=torch tensor dtype=scalar_type dtype max_val = g op Constant value_t=torch tensor dtype=scalar_type dtype clamp g input min_val max_val _onnx_symbolic aten select Opset gather accepts negative indices symbolic_helper quantized_args True symbolic_helper parse_args v i v select g jit_utils GraphContext dim index g op Gather index axis_i=dim _onnx_symbolic aten index_put index_put g jit_utils GraphContext indices_list_value values accumulate=False symbolic_helper _is_packed_list indices_list_value indices_list = symbolic_helper _unpack_list indices_list_value indices_list = indices_list_value accumulate = symbolic_helper _parse_arg accumulate b len indices_list == values len indices_list idx_ range len indices_list symbolic_helper _is_bool indices_list idx_ indices_list idx_ = g op NonZero indices_list idx_ index = indices_list ind indices_list index = opset add g index ind broadcast_index_shape = g op Shape index indices_list = symbolic_helper _unsqueeze_helper g opset expand g ind broadcast_index_shape None - ind indices_list index = g op Concat indices_list axis_i=- Replace index_put node masked_scatter masked_fill when inputs index_put node contains single boolean input index_put - masked_fill input index contains single tensor Bool type e g - input value contains single element e g Torch IR mask Float strides= requires_grad= device=cpu = aten clone Bool strides= requires_grad= device=cpu = aten Float requires_grad= device=cpu = prim Constant value= Bool strides= device=cpu = aten view Tensor = prim ListConstruct Float strides= requires_grad= device=cpu = aten index_put mask index_put - masked_scatter input index contains single tensor Bool type e g - input value contains multiple elements e g Torch IR mask Float strides= requires_grad= device=cpu = aten clone Float strides= requires_grad= device=cpu = prim Constant value= CPUFloatType Bool strides= requires_grad= device=cpu = aten ne mask some_const Bool strides= requires_grad= device=cpu = aten Long requires_grad= device=cpu = prim Constant value= int = prim Constant value= - Bool strides= device=cpu = aten view Tensor = prim ListConstruct Float strides= requires_grad= device=cpu = aten index_put mask index = indices_list bool_inp = index symbolic_helper _is_bool bool_inp rank = symbolic_helper _get_tensor_rank values rank None rank == opset masked_fill g bool_inp values mask_rank = symbolic_helper _get_tensor_rank bool_inp self_rank = symbolic_helper _get_tensor_rank mask_rank None self_rank None self_rank mask_rank Unsqueeze bool_inp broadcastable shape bool_inp = symbolic_helper _unsqueeze_helper g bool_inp list range mask_rank self_rank masked_scatter g bool_inp values broadcast_index_shape = g op Shape index index = symbolic_helper _unsqueeze_helper g index - sub_data_shape = symbolic_helper _slice_helper g g op Shape axes= starts= len indices_list ends= sys maxsize values_shape = g op Concat broadcast_index_shape sub_data_shape axis_i= Check values singular value expand accordingly rank = symbolic_helper _get_tensor_rank values rank None rank == values = opset expand g values values_shape None values = symbolic_helper _reshape_helper g values values_shape self_scalar_type = _type_utils JitScalarType from_value _type_utils JitScalarType UNDEFINED self_scalar_type = _type_utils JitScalarType UNDEFINED values_scalar_type = _type_utils JitScalarType from_value values _type_utils JitScalarType UNDEFINED self_scalar_type = values_scalar_type values = g op Cast values to_i=self_scalar_type onnx_type accumulate raise errors SymbolicValueError does have valid scalar type accumulate zeros = g op ConstantOfShape g op Shape value_t=torch tensor dtype=self_scalar_type dtype result = g op ScatterND zeros index values result = add g result result = g op ScatterND index values result _onnx_symbolic aten pixel_shuffle symbolic_helper parse_args v i pixel_shuffle g jit_utils GraphContext upscale_factor rank = symbolic_helper _get_tensor_rank rank None rank = symbolic_helper _unimplemented pixel_shuffle only support d input g op DepthToSpace blocksize_i=upscale_factor mode_s= CRD _onnx_symbolic aten upsample_nearest d decorate= symbolic_helper _apply_params upsample_nearest d nearest _onnx_symbolic aten upsample_nearest d decorate= symbolic_helper _apply_params upsample_nearest d nearest _onnx_symbolic aten upsample_nearest d decorate= symbolic_helper _apply_params upsample_nearest d nearest _onnx_symbolic aten upsample_linear d decorate= symbolic_helper _apply_params upsample_linear d linear _onnx_symbolic aten upsample_bilinear d decorate= symbolic_helper _apply_params upsample_bilinear d linear _onnx_symbolic aten upsample_trilinear d decorate= symbolic_helper _apply_params upsample_trilinear d linear _onnx_symbolic aten upsample_bicubic d decorate= symbolic_helper _apply_params upsample_bicubic d cubic _interpolate name str dim int interpolate_mode str symbolic_helper _interpolate_helper name dim interpolate_mode _onnx_symbolic aten __interpolate symbolic_helper quantized_args True False False False False False False __interpolate g jit_utils GraphContext input size scale_factor mode align_corners recompute_scale_factor antialias symbolic_helper __interpolate_helper g input size scale_factor mode align_corners recompute_scale_factor _onnx_symbolic aten gather symbolic_helper parse_args v i v v gather g jit_utils GraphContext dim index sparse_grad=False symbolic_helper _maybe_get_const sparse_grad i symbolic_helper _unimplemented gather sparse_grad == True g op GatherElements index axis_i=dim _onnx_symbolic aten scatter symbolic_helper parse_args v i v v scatter g jit_utils GraphContext dim index src src_type = _type_utils JitScalarType from_value src src = symbolic_helper _maybe_get_scalar src symbolic_helper _is_value src g op ScatterElements index src axis_i=dim Check scalar src has same type PyTorch allows different type scalar src when src tensor If insert Cast node _type_utils JitScalarType from_value = src_type src = g op Cast src to_i=_type_utils JitScalarType from_value onnx_type g op ScatterElements index opset expand_as g src index axis_i=dim _onnx_symbolic aten cumsum symbolic_helper parse_args v i none cumsum g jit_utils GraphContext dim dtype=None dim_tensor = g op Constant value_t=torch tensor dim dtype=torch int dtype dtype node kind = prim Constant parsed_dtype = symbolic_helper _get_const dtype i dtype cast = g op Cast to_i=_type_utils JitScalarType parsed_dtype onnx_type cast = csum = g op CumSum cast dim_tensor csum _onnx_symbolic aten masked_select masked_select g jit_utils GraphContext mask index = opset nonzero g opset expand_as g mask g op GatherND index _onnx_symbolic aten masked_scatter masked_scatter g jit_utils GraphContext mask source index = opset nonzero g opset expand_as g mask NOTE source can have more elements than needed It could also have arbitrary shape This supported ONNX ScatterND so we need flatten slice source tensor source = symbolic_helper _reshape_helper g source torch LongTensor - source = symbolic_helper _slice_helper g source axes=torch LongTensor starts=torch LongTensor ends=opset size g index torch LongTensor g op ScatterND index source _onnx_symbolic aten len _len g jit_utils GraphContext symbolic_helper _is_tensor_list node kind == onnx SplitToSequence g op SequenceLength sz_ = size g g op Constant value_t=torch LongTensor symbolic_helper _squeeze_helper g sz_ _onnx_symbolic aten __getitem_ __getitem_ g jit_utils GraphContext i symbolic_helper _is_tensor_list SequenceAt requires input List Tensors g op SequenceAt i torch onnx _internal torchscript_exporter symbolic_opset __getitem_ getitem getitem g i _onnx_symbolic aten _set_item _set_item g jit_utils GraphContext tensor_list i v tensor_list = g op SequenceErase tensor_list i g op SequenceInsert tensor_list v i _onnx_symbolic aten append append g jit_utils GraphContext tensor g op SequenceInsert tensor _onnx_symbolic aten add add g jit_utils GraphContext other alpha=None symbolic_helper _is_value symbolic_helper _is_tensor_list tensor_list_node = other node tensor_list_node kind = prim ListConstruct symbolic_helper _unimplemented add does support adding dynamic tensor list another tensors = symbolic_helper _unpack_list other l = t tensors l = g op SequenceInsert l t l opset add g other alpha _onnx_symbolic aten insert insert g jit_utils GraphContext pos tensor g op SequenceInsert tensor pos _onnx_symbolic aten pop pop g jit_utils GraphContext tensor_list dim g op SequenceErase tensor_list dim _onnx_symbolic aten Delete Delete g jit_utils GraphContext tensor_list dim g op SequenceErase tensor_list dim _onnx_symbolic aten cat symbolic_helper quantized_args True cat g jit_utils GraphContext tensor_list dim symbolic_helper _is_packed_list tensor_list opset cat g tensor_list dim dim = symbolic_helper _get_const dim i dim g op ConcatFromSequence tensor_list axis_i=dim _onnx_symbolic aten stack stack g jit_utils GraphContext tensor_list dim symbolic_helper _is_packed_list tensor_list opset stack g tensor_list dim dim = symbolic_helper _get_const dim i dim g op ConcatFromSequence tensor_list axis_i=dim new_axis_i= _onnx_symbolic aten _unique symbolic_helper parse_args v i i i _unique g jit_utils GraphContext sorted return_inverse return_counts u _indices inverse_indices counts = g op Unique sorted_i=sorted outputs= u inverse_indices counts _onnx_symbolic aten unique_dim symbolic_helper parse_args v i i i i unique_dim g jit_utils GraphContext dim sorted return_inverse return_counts u _indices inverse_indices counts = g op Unique axis_i=dim sorted_i=sorted outputs= u inverse_indices counts _onnx_symbolic aten topk symbolic_helper parse_args v v i i i none topk g jit_utils GraphContext k dim largest sorted out=None symbolic_helper _topk_helper g k dim largest=largest sorted=sorted out=out _onnx_symbolic aten sort symbolic_helper parse_args v i i none sort g jit_utils GraphContext dim descending out=None symbolic_helper _sort_helper g dim descending=descending out=out _onnx_symbolic aten argsort symbolic_helper parse_args v i i none argsort g jit_utils GraphContext dim descending out=None _ indices = symbolic_helper _sort_helper g dim descending=descending out=out indices _onnx_symbolic aten round symbolic_helper parse_args v i round g jit_utils GraphContext decimals= symbolic_helper _is_fp decimals == g op Round mul = g op Mul g op Constant value_t=torch tensor pow decimals round = g op Round mul g op Mul round g op Constant value_t=torch tensor pow - decimals _onnx_symbolic aten remainder remainder g jit_utils GraphContext input other symbolic_helper _is_fp input symbolic_helper _is_fp other opset remainder g input other g op Mod input other fmod_i= _onnx_symbolic aten split symbolic_helper parse_args v v i i split g jit_utils GraphContext split_size_or_sizes dim _outputs=None symbolic_helper _is_split_static split_size_or_sizes _outputs split_out = g op SplitToSequence split_size_or_sizes axis_i=dim _outputs None split_out Convert multiple slice nodes iff number splits number outputs statically known symbolic_helper _is_packed_list split_size_or_sizes len symbolic_helper _unpack_list split_size_or_sizes == _outputs split_sizes = symbolic_helper _unsqueeze_helper g v v symbolic_helper _unpack_list split_size_or_sizes start = g op Constant value_t=torch tensor dtype=torch long axis = g op Constant value_t=torch tensor dim dtype=torch long res = i range _outputs end = g op Add start split_sizes i split_sizes list same length _outputs res append g op Slice start end axis start = end res g op SequenceAt split_out g op Constant value_t=torch tensor i dtype=torch long i range _outputs opset split g split_size_or_sizes dim _outputs _onnx_symbolic aten split_with_sizes symbolic_helper parse_args v v i i split_with_sizes g jit_utils GraphContext split_sizes dim _outputs=None split g split_sizes dim _outputs _onnx_symbolic aten unbind symbolic_helper parse_args v i i unbind g jit_utils GraphContext dim= _outputs=None _outputs None g op SplitToSequence g op Constant value_t=torch tensor dtype=torch long axis_i=dim keepdims_i= opset unbind g dim _outputs _prepare_onnx_paddings g jit_utils GraphContext input pad Generate paddings ONNX order based pad pytorch Args input input tensor pad paddings pytorch The order dim_n_begin dim_n_end dim_n- _begin dim_n- _end dim_m_begin dim_m_end where m range n symbolic_helper _is_packed_list pad symbolic_helper _is_list pad symbolic_helper _is_scalar_list pad pad = g op ConcatFromSequence pad axis_i= new_axis_i= The desired order paddings dim_ _begin dim_ _begin dim_ _end dim_n_end n dimension input Assume zero-dimensions beginning pad pad sequence zeros beginning pad_len = opset size g pad g op Constant value_t=torch tensor Set extension = dim - len pad rank = symbolic_helper _get_tensor_rank input rank None rank = g op Size g op Shape input rank = g op Constant value_t=torch tensor rank dtype=torch int extension = g op Sub g op Mul rank g op Constant value_t=torch tensor dtype=torch int pad_len Concat pad extension paddings = dim_n_begin dim_n_end dim_n- _begin dim_n- _end Currently ONNX only supports int type Pad pad = g op Cast pad to_i=_C_onnx TensorProtoDataType INT paddings = g op Concat pad g op ConstantOfShape extension value_t=torch tensor dtype=torch int axis_i= Reshape reverse order collate first beginnings then ends paddings = dim_n- _begin dim_n_begin dim_n- _end dim_n_end Reshape back -D paddings = dim_n - _begin dim_n_begin dim_n - _end dim_n_end paddings = symbolic_helper _reshape_helper g paddings g op Constant value_t=torch tensor - paddings = g op Transpose opset flip g paddings perm_i= paddings = symbolic_helper _reshape_helper g paddings g op Constant value_t=torch tensor - padding_c = g op Cast paddings to_i=_C_onnx TensorProtoDataType INT padding_c _onnx_symbolic aten constant_pad_nd constant_pad_nd g jit_utils GraphContext input padding value=None mode = constant value = symbolic_helper _maybe_get_scalar value value = symbolic_helper _if_scalar_type_as value input pad = _prepare_onnx_paddings g input padding g op Pad input pad value mode_s=mode _onnx_symbolic aten reflection_pad d _onnx_symbolic aten reflection_pad d _onnx_symbolic aten reflection_pad d reflection_pad g jit_utils GraphContext input padding mode = reflect paddings = _prepare_onnx_paddings g input padding g op Pad input paddings mode_s=mode _onnx_symbolic aten replication_pad d _onnx_symbolic aten replication_pad d _onnx_symbolic aten replication_pad d replication_pad g jit_utils GraphContext input padding mode = edge paddings = _prepare_onnx_paddings g input padding g op Pad input paddings mode_s=mode _onnx_symbolic aten pad pad g jit_utils GraphContext input _C Value pad _C Value mode _C Value value _C Value mode = symbolic_helper _parse_arg mode s mode == replicate replication_pad g input pad mode == reflect reflection_pad g input pad mode == constant constant_pad_nd g input pad value mode == circular opset _pad_circular g input pad raise errors SymbolicValueError f Unrecognized padding mode mode input _onnx_symbolic aten linalg_det linalg_det g jit_utils GraphContext g op Det _onnx_symbolic aten logdet logdet g jit_utils GraphContext input opset log g linalg_det g input _onnx_symbolic aten arange arange g jit_utils GraphContext args _get_arange_dtype dtype dtype = symbolic_helper _maybe_get_const dtype i dtype len args == all isinstance val int val args aten arange Scalar start Scalar end dtype = torch int Start index start = g op Constant value_t=torch tensor args dtype=dtype End exclusive index end = g op Constant value_t=torch tensor args dtype=dtype Step size start end indexes delta_default = g op Constant value_t=torch tensor dtype=dtype g op Range start end delta_default len args == len args == len args == aten arange Scalar end Tensor out dtype = None aten arange Scalar end ScalarType dtype Layout Device bool pin_memory dtype = _get_arange_dtype args type_ end start step = symbolic_helper _arange_cast_helper g end=args dtype=dtype start_default = g op Constant value_t=torch tensor dtype=type_ dtype delta_default = g op Constant value_t=torch tensor dtype=type_ dtype pyrefly ignore bad-argument-type g op Range start_default end delta_default len args == len args == len args == aten arange Scalar start Scalar end Scalar step Tensor out dtype = None aten arange Scalar start Scalar end Scalar step ScalarType dtype Layout Device bool pin_memory dtype = _get_arange_dtype args _ end start step = symbolic_helper _arange_cast_helper g start=args end=args step=args dtype=dtype pyrefly ignore bad-argument-type g op Range start end step len args == aten arange Scalar start Scalar end ScalarType dtype Layout Device bool pin_memory dtype = _get_arange_dtype args type_ end start step = symbolic_helper _arange_cast_helper g start=args end=args dtype=dtype delta_default = g op Constant value_t=torch tensor dtype=type_ dtype pyrefly ignore bad-argument-type g op Range start end delta_default symbolic_helper _unimplemented aten arange f len args arguments _onnx_symbolic aten _dim_arange symbolic_helper parse_args v i _dim_arange g jit_utils GraphContext like dim like_shape = g op Shape like stop = g op Gather like_shape g op Constant value_t=torch tensor dim axis_i= arange g stop None None None _onnx_symbolic aten size symbolic_helper quantized_args True quantize_output=False size g jit_utils GraphContext dim=None dim None g op Shape symbolic_helper _size_helper g dim _onnx_symbolic aten squeeze squeeze g jit_utils GraphContext dim=None dim None g op Squeeze dim tensor symbolic_helper _is_constant dim symbolic_helper _squeeze_helper g dim dim = symbolic_helper _get_const dim i dim input_rank = symbolic_helper _get_tensor_rank adjusted_dim = dim input_rank None dim adjusted_dim += input_rank dim_size = symbolic_helper _get_tensor_dim_size adjusted_dim dim input_rank None dim_size None If onnx shape inference export always dynamic Because we cannot tell observed static shape also static runtime create cond node condition shape i == dim_constant = g op Constant value_t=torch tensor dim size = symbolic_helper _size_helper g dim_constant const_one = g op Constant value_t=torch ones dtype=torch int cond = g op Equal size const_one create If node add then blocks if_op if_context else_context _ = jit_utils add_op_with_blocks g If cond n_blocks= squeeze_ = symbolic_helper _squeeze_helper if_context dim utils _add_output_to_block if_context block squeeze_ identity_ = else_context op Identity utils _add_output_to_block else_context block identity_ if_op For static input shape dim = adjusted_dim dim_size warnings warn This model contains squeeze operation dimension + str dim + The size + dimension given input + str dim_size + The model will + exported without squeeze node If model intended used dynamic + input shapes please export dynamic_axes argument stacklevel= symbolic_helper _squeeze_helper g dim _onnx_symbolic aten unsqueeze unsqueeze g jit_utils GraphContext dim symbolic_helper _is_constant dim dim = symbolic_helper _get_const dim i dim symbolic_helper _unsqueeze_helper g dim _onnx_symbolic aten mm mm g jit_utils GraphContext other g op Gemm other beta_f= alpha_f= _onnx_symbolic aten index index g jit_utils GraphContext index symbolic_helper _is_packed_list index indices = symbolic_helper _unpack_list index indices = index Handle single mask index len indices == index = indices symbolic_helper _is_none index symbolic_helper _is_bool index _type_utils JitScalarType from_value index == _type_utils JitScalarType UINT index = opset nonzero g index g op GatherND index opset index g index _onnx_symbolic aten index_fill index_fill g jit_utils GraphContext dim index value expanded_index_shape expanded_index = symbolic_helper _index_fill_reshape_helper g dim index value = symbolic_helper _maybe_get_scalar value value = symbolic_helper _if_scalar_type_as value expanded_value = opset expand g value expanded_index_shape None scatter g dim expanded_index expanded_value _onnx_symbolic aten index_copy index_copy g jit_utils GraphContext dim index source _expanded_index_shape expanded_index = symbolic_helper _index_fill_reshape_helper g dim index scatter g dim expanded_index source _onnx_symbolic aten bitwise_right_shift _onnx_symbolic aten __rshift_ __rshift_ g jit_utils GraphContext other make sure cast other s type when long make sure other float _type_utils JitScalarType from_value other _type_utils JitScalarType UNDEFINED = _type_utils JitScalarType from_value other = g op Cast other to_i=_type_utils JitScalarType from_value onnx_type _type_utils JitScalarType from_value _type_utils JitScalarType UNDEFINED == _type_utils JitScalarType UINT g op BitShift other direction_s= RIGHT two = g op Constant value_t=torch tensor dtype=torch float exponent same type has float double onnx Pow symbolic_helper _is_fp other = g op Cast other to_i=_C_onnx TensorProtoDataType FLOAT two_pow = g op Pow two other two_pow = g op Cast two_pow to_i=_type_utils JitScalarType from_value onnx_type rshift = g op Div two_pow rshift _onnx_symbolic aten bitwise_left_shift _onnx_symbolic aten __lshift_ __lshift_ g jit_utils GraphContext other make sure cast other s type when long make sure other float _type_utils JitScalarType from_value other _type_utils JitScalarType UNDEFINED = _type_utils JitScalarType from_value other = g op Cast other to_i=_type_utils JitScalarType from_value onnx_type _type_utils JitScalarType from_value _type_utils JitScalarType UNDEFINED == _type_utils JitScalarType UINT g op BitShift other direction_s= LEFT two = g op Constant value_t=torch tensor dtype=torch float exponent same type has float double onnx Pow symbolic_helper _is_fp other = g op Cast other to_i=_C_onnx TensorProtoDataType FLOAT two_pow = g op Pow two other two_pow = g op Cast two_pow to_i=_type_utils JitScalarType from_value onnx_type lshift = g op Mul two_pow lshift _get_im col_indices_along_dim g jit_utils GraphContext input_d kernel_size_d dilation_d padding_d stride_d Input always -D N C H W Calculate indices sliding blocks along spatial dimension Slide kernel over input each dim d each dimension d ranges input d + xpadding d -dilation d x kernel_size d - steps = stride blocks_d = g op Add input_d g op Constant value_t=torch tensor padding_d blocks_d = g op Sub blocks_d g op Constant value_t=torch tensor dilation_d kernel_size_d - Stride kernel over input find starting indices along dim d blocks_d_indices = g op Range g op Constant value_t=torch tensor blocks_d g op Constant value_t=torch tensor stride_d Apply dilation kernel find its indices along dim d kernel_grid = torch arange kernel_size_d dilation_d dilation_d kernel_grid = g op Constant value_t=kernel_grid unsqueeze Broadcast add kernel staring positions indices kernel_grid along dim d get block indices along dim d blocks_d_indices = symbolic_helper _unsqueeze_helper g blocks_d_indices Reshape - kernel_mask = symbolic_helper _reshape_helper g kernel_grid g op Constant value_t=torch tensor - block_mask = g op Add blocks_d_indices kernel_mask block_mask _get_im col_padded_input g jit_utils GraphContext input padding_h padding_w Input always -D tensor N C H W Padding tensor has following format padding_h padding_w Reshape padding follow ONNX format dim _begin dim _begin dim _end dim _end pad = g op Constant value_t=torch LongTensor padding_h padding_w g op Pad input pad _get_im col_output_shape g jit_utils GraphContext input kernel_h kernel_w batch_dim = size g input g op Constant value_t=torch tensor channel_dim = size g input g op Constant value_t=torch tensor channel_unfolded = g op Mul channel_dim g op Constant value_t=torch tensor kernel_h kernel_w g op Concat symbolic_helper _unsqueeze_helper g batch_dim symbolic_helper _unsqueeze_helper g channel_unfolded g op Constant value_t=torch tensor - axis_i= _onnx_symbolic aten im col symbolic_helper parse_args v im col g jit_utils GraphContext input kernel_size dilation padding stride Input always -D tensor N C H W All other args int input_h = size g input g op Constant value_t=torch tensor input_w = size g input g op Constant value_t=torch tensor stride_h stride_w = stride stride padding_h padding_w = padding padding dilation_h dilation_w = dilation dilation kernel_h kernel_w = kernel_size kernel_size blocks_row_indices = _get_im col_indices_along_dim g input_h kernel_h dilation_h padding_h stride_h blocks_col_indices = _get_im col_indices_along_dim g input_w kernel_w dilation_w padding_w stride_w output_shape = _get_im col_output_shape g input kernel_h kernel_w padded_input = _get_im col_padded_input g input padding_h padding_w For D matrix size below kernel_size= stride= dilation= First gather indices along rows dim= blocks_row_indices = get And then gather along cols dim= blocks_row_indices = get Transpose dims depth rows then reshape output shape get output = g op Gather padded_input blocks_row_indices axis_i= output = g op Gather output blocks_col_indices axis_i= output = g op Transpose output perm_i= symbolic_helper _reshape_helper g output output_shape _onnx_symbolic aten narrow narrow g jit_utils GraphContext input dim start length end = g op Add start length symbolic_helper _slice_helper g input axes=dim starts=start ends=end _onnx_symbolic aten flatten symbolic_helper quantized_args True False False symbolic_helper parse_args v i i flatten g jit_utils GraphContext input start_dim end_dim dim = symbolic_helper _get_tensor_rank input dim == input use ONNX s Flatten operator cases where output shape D start_dim == end_dim == - dim None end_dim == dim - g op Flatten input axis_i=start_dim start_dim == end_dim == - dim None end_dim == dim - g op Flatten input axis_i=end_dim + dim None symbolic_helper _unimplemented dim ONNX PyTorch use different strategies split input Input rank must known export time end_dim negative add dim end_dim end_dim = dim + end_dim symbolic_helper _flatten_helper g input start_dim end_dim dim _onnx_symbolic aten linalg_vector_norm symbolic_helper parse_args v f b v linalg_vector_norm g jit_utils GraphContext ord dim Sequence int &#124; None keepdim bool dtype symbolic_helper _linalg_vector_norm_helper g ord dim keepdim dtype _onnx_symbolic aten embedding_bag symbolic_helper parse_args v v v i i i v i i embedding_bag g jit_utils GraphContext embedding_matrix indices offsets scale_grad_by_freq mode sparse per_sample_weights include_last_offset padding_idx symbolic_helper _embedding_bag_helper g embedding_matrix indices offsets scale_grad_by_freq mode sparse per_sample_weights include_last_offset padding_idx _onnx_symbolic aten embedding_renorm symbolic_helper parse_args v v f f embedding_renorm g jit_utils GraphContext weight indices max_norm norm_type unique_indices = g op Unique indices partial_weight = g op Gather weight unique_indices norm_i = int norm_type norm_i == norm_type = ReduceL norm_i == norm_type = ReduceL raise errors SymbolicValueError f Unsupported ONNX export embedding_renorm norm norm_i Only supported weight partial_weight_norm = g op norm_type partial_weight axes_i= keepdims_i= https github com pytorch pytorch blob ed c e e bd c e e cbd aten src ATen native Embedding cpp#L Add e- prevent division zero partial_weight_norm_ = g op Add partial_weight_norm g op Constant value_t=torch tensor e- max_norm = torch tensor max_norm scales = g op Div max_norm partial_weight_norm_ partial_weight_renorm = g op Mul partial_weight scales partial_weight_renorm = g op Where g op Greater partial_weight_norm max_norm partial_weight_renorm partial_weight g op ScatterND weight symbolic_helper _unsqueeze_helper g unique_indices partial_weight_renorm _onnx_symbolic aten chunk chunk g jit_utils GraphContext chunks dim Calculate chunk size dynamic chunk dim_size = g op Gather g op Shape dim axis_i= chunk_size_s = g op Sub chunks g op Constant value_t=torch tensor dtype=torch long chunk_size = g op Div g op Add dim_size chunk_size_s chunks Create splits vector chunk_vec = opset expand g chunk_size chunk_size_s None g op Sub dim_size g op Mul chunk_size chunk_size_s chunk_vec = g op Concat chunk_vec axis_i= split g chunk_vec dim _onnx_symbolic aten normal normal g jit_utils GraphContext mean std sizes=None generator=None dtype=None layout=None device=None pin_memory=None If you can sample given distribution mean variance then you can easily sample scale-location transformation distribution which has mean mu variance sigma s square If x sample mean variance distribution then sigma x+mu sample mean mu variance sigma s square sizes None symbolic_helper _is_none sizes mean = opset expand g mean sizes None result = opset mul g std g op RandomNormalLike mean add g result mean _onnx_symbolic aten atleast_ d atleast_ d g jit_utils GraphContext torch _C Value NOTE If s D reshape D NOTE could packed list tensor symbolic_helper _is_value symbolic_helper _is_packed_list tensor_list = symbolic_helper _unpack_list new_tensor_list = tensor tensor_list new_tensor = tensor tensor_rank = symbolic_helper _get_tensor_rank tensor tensor_rank == new_tensor = symbolic_helper _reshape_helper g new_tensor g op Constant value_t=torch tensor new_tensor_list append new_tensor g op SequenceConstruct new_tensor_list tensor_rank = symbolic_helper _get_tensor_rank tensor_rank == = symbolic_helper _reshape_helper g g op Constant value_t=torch tensor _onnx_symbolic aten atleast_ d atleast_ d g jit_utils GraphContext torch _C Value NOTE If s D reshape D If s D unsqueeze D NOTE could packed list tensor symbolic_helper _is_value symbolic_helper _is_packed_list tensor_list = symbolic_helper _unpack_list new_tensor_list = tensor tensor_list new_tensor = tensor tensor_rank = symbolic_helper _get_tensor_rank tensor tensor_rank == new_tensor = symbolic_helper _reshape_helper g new_tensor g op Constant value_t=torch tensor tensor_rank == new_tensor = symbolic_helper _unsqueeze_helper g new_tensor axes_i= new_tensor_list append new_tensor g op SequenceConstruct new_tensor_list tensor_rank = symbolic_helper _get_tensor_rank tensor_rank == = symbolic_helper _reshape_helper g g op Constant value_t=torch tensor tensor_rank == = symbolic_helper _unsqueeze_helper g axes_i= _onnx_symbolic aten atleast_ d atleast_ d g jit_utils GraphContext torch _C Value NOTE If s D reshape D If s D unsqueeze D If s D unsqueeze D NOTE could packed list tensor symbolic_helper _is_value symbolic_helper _is_packed_list tensor_list = symbolic_helper _unpack_list new_tensor_list = tensor tensor_list new_tensor = tensor tensor_rank = symbolic_helper _get_tensor_rank tensor tensor_rank == new_tensor = symbolic_helper _reshape_helper g new_tensor g op Constant value_t=torch tensor tensor_rank == new_tensor = symbolic_helper _unsqueeze_helper g new_tensor axes_i= new_tensor = symbolic_helper _unsqueeze_helper g new_tensor axes_i= - tensor_rank == new_tensor = symbolic_helper _unsqueeze_helper g new_tensor axes_i= - new_tensor_list append new_tensor g op SequenceConstruct new_tensor_list tensor_rank = symbolic_helper _get_tensor_rank tensor_rank == = symbolic_helper _reshape_helper g g op Constant value_t=torch tensor tensor_rank == = symbolic_helper _unsqueeze_helper g axes_i= = symbolic_helper _unsqueeze_helper g axes_i= - tensor_rank == = symbolic_helper _unsqueeze_helper g axes_i= - _onnx_symbolic prim ConstantChunk prim_constant_chunk g jit_utils GraphContext chunks dim input_shape = g op Shape axis = g op Constant value_t=torch tensor dim dtype=torch long input_shape_dim = g op Gather input_shape axis axis_i= start = g op Constant value_t=torch tensor dtype=torch long chunk_size = g op Constant value_t=torch tensor chunks dtype=torch long chunk_size_minus_ = g op Constant value_t=torch tensor chunks - dtype=torch long input_shape_dim_shift = g op Add input_shape_dim chunk_size_minus_ chunk_dim = g op Div input_shape_dim_shift chunk_size res = i range chunks index = g op Constant value_t=torch tensor i + dtype=torch long end = g op Mul chunk_dim index res append g op Slice start end axis start = end res _onnx_symbolic aten hstack hstack g jit_utils GraphContext tensor_list _C Value tensor_list = atleast_ d g tensor_list first_tensor = g op SequenceAt tensor_list g op Constant value_t=torch tensor dtype=torch long first_tensor_shape = g op Shape first_tensor first_tensor_dim = g op Size first_tensor_shape const_one = g op Constant value_t=torch tensor dtype=torch long equal_to_one = g op Equal first_tensor_dim const_one if_op_greater if_context_equal else_context_equal _ = jit_utils add_op_with_blocks g If equal_to_one n_blocks= outputs= result_if = if_context_equal op ConcatFromSequence tensor_list axis_i= new_axis_i= utils _add_output_to_block if_context_equal block result_if result_else = else_context_equal op ConcatFromSequence tensor_list axis_i= new_axis_i= utils _add_output_to_block else_context_equal block result_else result = if_op_greater node output result _onnx_symbolic aten vstack vstack g jit_utils GraphContext tensor_list _C Value tensor_list = atleast_ d g tensor_list g op ConcatFromSequence tensor_list axis_i= new_axis_i=