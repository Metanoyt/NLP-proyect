mypy allow-untyped-defs collections abc Callable typing Optional Union typing_extensions deprecated torch Tensor torch nn _reduction _Reduction functional F distance PairwiseDistance module Module __all__ = L Loss NLLLoss NLLLoss d PoissonNLLLoss GaussianNLLLoss KLDivLoss MSELoss BCELoss BCEWithLogitsLoss HingeEmbeddingLoss MultiLabelMarginLoss SmoothL Loss HuberLoss SoftMarginLoss CrossEntropyLoss MultiLabelSoftMarginLoss CosineEmbeddingLoss MarginRankingLoss MultiMarginLoss TripletMarginLoss TripletMarginWithDistanceLoss CTCLoss _Loss Module reduction str __init__ size_average=None reduce=None reduction str = mean - None super __init__ size_average None reduce None reduction str = _Reduction legacy_get_string size_average reduce reduction = reduction _WeightedLoss _Loss __init__ weight Optional Tensor = None size_average=None reduce=None reduction str = mean - None super __init__ size_average reduce reduction register_buffer weight weight weight Optional Tensor L Loss _Loss r Creates criterion measures mean absolute error MAE between each element input math ` x ` target math ` y ` The unreduced i e attr ` reduction ` set ` ` none ` ` loss can described math \ell x y = L = \ l_ \dots l_N\ ^\top \quad l_n = \left &#124; x_n - y_n \right &#124; where math ` N ` batch size If attr ` reduction ` ` ` none ` ` default ` ` mean ` ` then math \ell x y = \begin cases \operatorname mean L \text reduction = \text ` mean \\ \operatorname sum L \text reduction = \text ` sum \end cases math ` x ` math ` y ` tensors arbitrary shapes total math ` N ` elements each The sum operation still operates over all elements divides math ` N ` The division math ` N ` can avoided one sets ` ` reduction = sum ` ` Supports real-valued complex-valued inputs Args size_average bool optional Deprecated see attr ` reduction ` By default losses averaged over each loss element batch Note some losses there multiple elements per sample If field attr ` size_average ` set ` ` False ` ` losses instead summed each minibatch Ignored when attr ` reduce ` ` ` False ` ` Default ` ` True ` ` reduce bool optional Deprecated see attr ` reduction ` By default losses averaged summed over observations each minibatch depending attr ` size_average ` When attr ` reduce ` ` ` False ` ` returns loss per batch element instead ignores attr ` size_average ` Default ` ` True ` ` reduction str optional Specifies reduction apply output ` ` none ` ` &#124; ` ` mean ` ` &#124; ` ` sum ` ` ` ` none ` ` no reduction will applied ` ` mean ` ` sum output will divided number elements output ` ` sum ` ` output will summed Note attr ` size_average ` attr ` reduce ` process being deprecated meantime specifying either those two args will override attr ` reduction ` Default ` ` mean ` ` Shape - Input math ` ` where math ` ` means any number dimensions - Target math ` ` same shape input - Output scalar If attr ` reduction ` ` ` none ` ` then math ` ` same shape input Examples loss = nn L Loss input = torch randn requires_grad=True target = torch randn output = loss input target output backward __constants__ = reduction __init__ size_average=None reduce=None reduction str = mean - None super __init__ size_average reduce reduction forward input Tensor target Tensor - Tensor Runs forward pass F l _loss input target reduction=self reduction NLLLoss _WeightedLoss r The negative log likelihood loss It useful train classification problem ` C ` classes If provided optional argument attr ` weight ` should D Tensor assigning weight each classes This particularly useful when you have unbalanced training set The ` input ` given through forward call expected contain log-probabilities each ` input ` has Tensor size either math ` minibatch C ` math ` minibatch C d_ d_ d_K ` math ` K \geq ` ` K ` -dimensional case The latter useful higher dimension inputs such computing NLL loss per-pixel D images Obtaining log-probabilities neural network easily achieved adding ` LogSoftmax ` layer last layer your network You may use ` CrossEntropyLoss ` instead you prefer add extra layer The ` target ` loss expects should index range math ` C- ` where ` C = number classes ` ` ignore_index ` specified loss also accepts index index may necessarily range The unreduced i e attr ` reduction ` set ` ` none ` ` loss can described math \ell x y = L = \ l_ \dots l_N\ ^\top \\ l_n = - w_ y_n x_ n y_n \\ w_ c = \text weight c \cdot \mathbb \ c \not= \text ignore\_index \ where math ` x ` input math ` y ` target math ` w ` weight math ` N ` batch size If attr ` reduction ` ` ` none ` ` default ` ` mean ` ` then math \ell x y = \begin cases \sum_ n= ^N \frac \sum_ n= ^N w_ y_n l_n \text reduction = \text ` mean \\ \sum_ n= ^N l_n \text reduction = \text ` sum \end cases Args weight Tensor optional manual rescaling weight given each If given has Tensor size ` C ` Otherwise treated having all ones size_average bool optional Deprecated see attr ` reduction ` By default losses averaged over each loss element batch Note some losses there multiple elements per sample If field attr ` size_average ` set ` ` False ` ` losses instead summed each minibatch Ignored when attr ` reduce ` ` ` False ` ` Default ` ` None ` ` ignore_index int optional Specifies target value ignored does contribute input gradient When attr ` size_average ` ` ` True ` ` loss averaged over non-ignored targets reduce bool optional Deprecated see attr ` reduction ` By default losses averaged summed over observations each minibatch depending attr ` size_average ` When attr ` reduce ` ` ` False ` ` returns loss per batch element instead ignores attr ` size_average ` Default ` ` None ` ` reduction str optional Specifies reduction apply output ` ` none ` ` &#124; ` ` mean ` ` &#124; ` ` sum ` ` ` ` none ` ` no reduction will applied ` ` mean ` ` weighted mean output taken ` ` sum ` ` output will summed Note attr ` size_average ` attr ` reduce ` process being deprecated meantime specifying either those two args will override attr ` reduction ` Default ` ` mean ` ` Shape - Input math ` N C ` math ` C ` where ` C = number classes ` ` N = batch size ` math ` N C d_ d_ d_K ` math ` K \geq ` case ` K ` -dimensional loss - Target math ` N ` math ` ` where each value math ` \leq \text targets i \leq C- ` math ` N d_ d_ d_K ` math ` K \geq ` case K-dimensional loss - Output If attr ` reduction ` ` ` none ` ` shape math ` N ` math ` N d_ d_ d_K ` math ` K \geq ` case K-dimensional loss Otherwise scalar Examples log_softmax = nn LogSoftmax dim= loss_fn = nn NLLLoss input NLLLoss size N x C = x input = torch randn requires_grad=True each element target must have = value C target = torch tensor loss = loss_fn log_softmax input target loss backward D loss example used example image inputs N C = loss_fn = nn NLLLoss data = torch randn N conv = nn Conv d C log_softmax = nn LogSoftmax dim= output conv forward shape N C output = log_softmax conv data each element target must have = value C target = torch empty N dtype=torch long random_ C input NLLLoss size N x C x height x width loss = loss_fn output target loss backward __constants__ = ignore_index reduction ignore_index int __init__ weight Optional Tensor = None size_average=None ignore_index int = - reduce=None reduction str = mean - None super __init__ weight size_average reduce reduction ignore_index = ignore_index forward input Tensor target Tensor - Tensor Runs forward pass F nll_loss input target weight=self weight ignore_index=self ignore_index reduction=self reduction deprecated ` NLLLoss d ` has been deprecated Please use ` NLLLoss ` instead drop-in replacement see https pytorch org docs main nn html#torch nn NLLLoss more details category=FutureWarning NLLLoss d NLLLoss __init__ weight Optional Tensor = None size_average=None ignore_index int = - reduce=None reduction str = mean - None super __init__ weight size_average ignore_index reduce reduction PoissonNLLLoss _Loss r Negative log likelihood loss Poisson distribution target The loss can described math \text target \sim \mathrm Poisson \text input \text loss \text input \text target = \text input - \text target \log \text input + \log \text target The last term can omitted approximated Stirling formula The approximation used target values more than For targets less equal zeros added loss Args log_input bool optional ` ` True ` ` loss computed math ` \exp \text input - \text target \text input ` ` ` False ` ` loss math ` \text input - \text target \log \text input +\text eps ` full bool optional whether compute full loss i e add Stirling approximation term math \text target \log \text target - \text target + \log \pi\text target size_average bool optional Deprecated see attr ` reduction ` By default losses averaged over each loss element batch Note some losses there multiple elements per sample If field attr ` size_average ` set ` ` False ` ` losses instead summed each minibatch Ignored when attr ` reduce ` ` ` False ` ` Default ` ` True ` ` eps float optional Small value avoid evaluation math ` \log ` when attr ` log_input = False ` Default e- reduce bool optional Deprecated see attr ` reduction ` By default losses averaged summed over observations each minibatch depending attr ` size_average ` When attr ` reduce ` ` ` False ` ` returns loss per batch element instead ignores attr ` size_average ` Default ` ` True ` ` reduction str optional Specifies reduction apply output ` ` none ` ` &#124; ` ` mean ` ` &#124; ` ` sum ` ` ` ` none ` ` no reduction will applied ` ` mean ` ` sum output will divided number elements output ` ` sum ` ` output will summed Note attr ` size_average ` attr ` reduce ` process being deprecated meantime specifying either those two args will override attr ` reduction ` Default ` ` mean ` ` Examples loss = nn PoissonNLLLoss log_input = torch randn requires_grad=True target = torch randn output = loss log_input target output backward Shape - Input math ` ` where math ` ` means any number dimensions - Target math ` ` same shape input - Output scalar default If attr ` reduction ` ` ` none ` ` then math ` ` same shape input __constants__ = log_input full eps reduction log_input bool full bool eps float __init__ log_input bool = True full bool = False size_average=None eps float = e- reduce=None reduction str = mean - None super __init__ size_average reduce reduction log_input = log_input full = full eps = eps forward log_input Tensor target Tensor - Tensor Runs forward pass F poisson_nll_loss log_input target log_input=self log_input full=self full eps=self eps reduction=self reduction GaussianNLLLoss _Loss r Gaussian negative log likelihood loss The targets treated samples Gaussian distributions expectations variances predicted neural network For ` ` target ` ` tensor modelled having Gaussian distribution tensor expectations ` ` input ` ` tensor positive variances ` ` var ` ` loss math \text loss = \frac \left \log\left \text max \left \text var \ \text eps \right \right + \frac \left \text input - \text target \right ^ \text max \left \text var \ \text eps \right \right + \text const where attr ` eps ` used stability By default constant term loss function omitted unless attr ` full ` ` ` True ` ` If ` ` var ` ` same size ` ` input ` ` due homoscedastic assumption must either have final dimension have one fewer dimension all other sizes being same correct broadcasting Args full bool optional include constant term loss calculation Default ` ` False ` ` eps float optional value used clamp ` ` var ` ` see note below stability Default e- reduction str optional specifies reduction apply output ` ` none ` ` &#124; ` ` mean ` ` &#124; ` ` sum ` ` ` ` none ` ` no reduction will applied ` ` mean ` ` output average all batch member losses ` ` sum ` ` output sum all batch member losses Default ` ` mean ` ` Shape - Input math ` N ` math ` ` where math ` ` means any number additional dimensions - Target math ` N ` math ` ` same shape input same shape input one dimension equal allow broadcasting - Var math ` N ` math ` ` same shape input same shape input one dimension equal same shape input one fewer dimension allow broadcasting scalar value - Output scalar attr ` reduction ` ` ` mean ` ` default ` ` sum ` ` If attr ` reduction ` ` ` none ` ` then math ` N ` same shape input Examples loss = nn GaussianNLLLoss input = torch randn requires_grad=True target = torch randn var = torch ones requires_grad=True heteroscedastic output = loss input target var output backward loss = nn GaussianNLLLoss input = torch randn requires_grad=True target = torch randn var = torch ones requires_grad=True homoscedastic output = loss input target var output backward Note The clamping ` ` var ` ` ignored respect autograd so gradients unaffected Reference Nix D A Weigend A S Estimating mean variance target probability distribution Proceedings IEEE International Conference Neural Networks ICNN Orlando FL USA pp - vol doi ICNN __constants__ = full eps reduction full bool eps float __init__ full bool = False eps float = e- reduction str = mean - None super __init__ None None reduction full = full eps = eps forward input Tensor target Tensor var Union Tensor float - Tensor Runs forward pass F gaussian_nll_loss input target var full=self full eps=self eps reduction=self reduction KLDivLoss _Loss r The Kullback-Leibler divergence loss For tensors same shape math ` y_ \text pred \ y_ \text true ` where math ` y_ \text pred ` attr ` input ` math ` y_ \text true ` attr ` target ` we define pointwise KL-divergence math L y_ \text pred \ y_ \text true = y_ \text true \cdot \log \frac y_ \text true y_ \text pred = y_ \text true \cdot \log y_ \text true - \log y_ \text pred To avoid underflow issues when computing quantity loss expects argument attr ` input ` log-space The argument attr ` target ` may also provided log-space attr ` log_target ` \ ` = True ` To summarise function roughly equivalent computing code-block python log_target default loss_pointwise = target target log - input loss_pointwise = target exp target - input then reducing result depending argument attr ` reduction ` code-block python reduction == mean default loss = loss_pointwise mean reduction == batchmean mathematically correct loss = loss_pointwise sum input size reduction == sum loss = loss_pointwise sum reduction == none loss = loss_pointwise note As all other losses PyTorch function expects first argument attr ` input ` output model e g neural network second attr ` target ` observations dataset This differs standard mathematical notation math ` KL P\ &#124; &#124; \ Q ` where math ` P ` denotes distribution observations math ` Q ` denotes model warning attr ` reduction ` \ ` = mean ` doesn t true KL divergence value please use attr ` reduction ` \ ` = batchmean ` which aligns mathematical definition Args size_average bool optional Deprecated see attr ` reduction ` By default losses averaged over each loss element batch Note some losses there multiple elements per sample If field attr ` size_average ` set ` False ` losses instead summed each minibatch Ignored when attr ` reduce ` ` False ` Default ` True ` reduce bool optional Deprecated see attr ` reduction ` By default losses averaged summed over observations each minibatch depending attr ` size_average ` When attr ` reduce ` ` False ` returns loss per batch element instead ignores attr ` size_average ` Default ` True ` reduction str optional Specifies reduction apply output Default ` mean ` log_target bool optional Specifies whether ` target ` log space Default ` False ` Shape - Input math ` ` where math ` ` means any number dimensions - Target math ` ` same shape input - Output scalar default If attr ` reduction ` ` none ` then math ` ` same shape input Examples kl_loss = nn KLDivLoss reduction= batchmean input should distribution log space input = F log_softmax torch randn requires_grad=True dim= Sample batch distributions Usually would come dataset target = F softmax torch rand dim= output = kl_loss input target kl_loss = nn KLDivLoss reduction= batchmean log_target=True log_target = F log_softmax torch rand dim= output = kl_loss input log_target __constants__ = reduction __init__ size_average=None reduce=None reduction str = mean log_target bool = False - None super __init__ size_average reduce reduction log_target = log_target forward input Tensor target Tensor - Tensor Runs forward pass F kl_div input target reduction=self reduction log_target=self log_target MSELoss _Loss r Creates criterion measures mean squared error squared L norm between each element input math ` x ` target math ` y ` The unreduced i e attr ` reduction ` set ` ` none ` ` loss can described math \ell x y = L = \ l_ \dots l_N\ ^\top \quad l_n = \left x_n - y_n \right ^ where math ` N ` batch size If attr ` reduction ` ` ` none ` ` default ` ` mean ` ` then math \ell x y = \begin cases \operatorname mean L \text reduction = \text ` mean \\ \operatorname sum L \text reduction = \text ` sum \end cases math ` x ` math ` y ` tensors arbitrary shapes total math ` N ` elements each The mean operation still operates over all elements divides math ` N ` The division math ` N ` can avoided one sets ` ` reduction = sum ` ` Args size_average bool optional Deprecated see attr ` reduction ` By default losses averaged over each loss element batch Note some losses there multiple elements per sample If field attr ` size_average ` set ` ` False ` ` losses instead summed each minibatch Ignored when attr ` reduce ` ` ` False ` ` Default ` ` True ` ` reduce bool optional Deprecated see attr ` reduction ` By default losses averaged summed over observations each minibatch depending attr ` size_average ` When attr ` reduce ` ` ` False ` ` returns loss per batch element instead ignores attr ` size_average ` Default ` ` True ` ` reduction str optional Specifies reduction apply output ` ` none ` ` &#124; ` ` mean ` ` &#124; ` ` sum ` ` ` ` none ` ` no reduction will applied ` ` mean ` ` sum output will divided number elements output ` ` sum ` ` output will summed Note attr ` size_average ` attr ` reduce ` process being deprecated meantime specifying either those two args will override attr ` reduction ` Default ` ` mean ` ` Shape - Input math ` ` where math ` ` means any number dimensions - Target math ` ` same shape input Examples loss = nn MSELoss input = torch randn requires_grad=True target = torch randn output = loss input target output backward __constants__ = reduction __init__ size_average=None reduce=None reduction str = mean - None super __init__ size_average reduce reduction forward input Tensor target Tensor - Tensor Runs forward pass F mse_loss input target reduction=self reduction BCELoss _WeightedLoss r Creates criterion measures Binary Cross Entropy between target input probabilities The unreduced i e attr ` reduction ` set ` ` none ` ` loss can described math \ell x y = L = \ l_ \dots l_N\ ^\top \quad l_n = - w_n \left y_n \cdot \log x_n + - y_n \cdot \log - x_n \right where math ` N ` batch size If attr ` reduction ` ` ` none ` ` default ` ` mean ` ` then math \ell x y = \begin cases \operatorname mean L \text reduction = \text ` mean \\ \operatorname sum L \text reduction = \text ` sum \end cases This used measuring error reconstruction example auto-encoder Note targets math ` y ` should numbers between Notice math ` x_n ` either one log terms would mathematically undefined above loss equation PyTorch chooses set math ` \log = -\infty ` since math ` \lim_ x\to \log x = -\infty ` However infinite term loss equation desirable several reasons For one either math ` y_n = ` math ` - y_n = ` then we would multiplying infinity Secondly we have infinite loss value then we would also have infinite term our gradient since math ` \lim_ x\to \frac d dx \log x = \infty ` This would make BCELoss s backward method nonlinear respect math ` x_n ` using things like linear regression would straight-forward Our solution BCELoss clamps its log function outputs greater than equal - This way we can always have finite loss value linear backward method Args weight Tensor optional manual rescaling weight given loss each batch element If given has Tensor size ` nbatch ` size_average bool optional Deprecated see attr ` reduction ` By default losses averaged over each loss element batch Note some losses there multiple elements per sample If field attr ` size_average ` set ` ` False ` ` losses instead summed each minibatch Ignored when attr ` reduce ` ` ` False ` ` Default ` ` True ` ` reduce bool optional Deprecated see attr ` reduction ` By default losses averaged summed over observations each minibatch depending attr ` size_average ` When attr ` reduce ` ` ` False ` ` returns loss per batch element instead ignores attr ` size_average ` Default ` ` True ` ` reduction str optional Specifies reduction apply output ` ` none ` ` &#124; ` ` mean ` ` &#124; ` ` sum ` ` ` ` none ` ` no reduction will applied ` ` mean ` ` sum output will divided number elements output ` ` sum ` ` output will summed Note attr ` size_average ` attr ` reduce ` process being deprecated meantime specifying either those two args will override attr ` reduction ` Default ` ` mean ` ` Shape - Input math ` ` where math ` ` means any number dimensions - Target math ` ` same shape input - Output scalar If attr ` reduction ` ` ` none ` ` then math ` ` same shape input Examples m = nn Sigmoid loss = nn BCELoss input = torch randn requires_grad=True target = torch rand requires_grad=False output = loss m input target output backward __constants__ = reduction __init__ weight Optional Tensor = None size_average=None reduce=None reduction str = mean - None super __init__ weight size_average reduce reduction forward input Tensor target Tensor - Tensor Runs forward pass F binary_cross_entropy input target weight=self weight reduction=self reduction BCEWithLogitsLoss _Loss r This loss combines ` Sigmoid ` layer ` BCELoss ` one single This version more numerically stable than using plain ` Sigmoid ` followed ` BCELoss ` combining operations into one layer we take advantage log-sum-exp trick numerical stability The unreduced i e attr ` reduction ` set ` ` none ` ` loss can described math \ell x y = L = \ l_ \dots l_N\ ^\top \quad l_n = - w_n \left y_n \cdot \log \sigma x_n + - y_n \cdot \log - \sigma x_n \right where math ` N ` batch size If attr ` reduction ` ` ` none ` ` default ` ` mean ` ` then math \ell x y = \begin cases \operatorname mean L \text reduction = \text ` mean \\ \operatorname sum L \text reduction = \text ` sum \end cases This used measuring error reconstruction example auto-encoder Note targets ` t i ` should numbers between It s possible trade off recall precision adding weights positive examples In case multi-label classification loss can described math \ell_c x y = L_c = \ l_ c \dots l_ N c \ ^\top \quad l_ n c = - w_ n c \left p_c y_ n c \cdot \log \sigma x_ n c + - y_ n c \cdot \log - \sigma x_ n c \right where math ` c ` number math ` c ` multi-label binary classification math ` c = ` single-label binary classification math ` n ` number sample batch math ` p_c ` weight positive answer math ` c ` math ` p_c ` increases recall math ` p_c ` increases precision For example dataset contains positive negative examples single then ` ` pos_weight ` ` should equal math ` \frac = ` The loss would act dataset contains math ` \times = ` positive examples Examples target = torch ones dtype=torch float classes batch size = output = torch full A prediction logit pos_weight = torch ones All weights equal criterion = torch nn BCEWithLogitsLoss pos_weight=pos_weight criterion output target -log sigmoid tensor In above example ` ` pos_weight ` ` tensor s elements correspond distinct classes multi-label binary classification scenario Each element ` ` pos_weight ` ` designed adjust loss function based imbalance between negative positive samples respective This approach useful datasets varying levels imbalance ensuring loss calculation accurately accounts distribution each Args weight Tensor optional manual rescaling weight given loss each batch element If given has Tensor size ` nbatch ` size_average bool optional Deprecated see attr ` reduction ` By default losses averaged over each loss element batch Note some losses there multiple elements per sample If field attr ` size_average ` set ` ` False ` ` losses instead summed each minibatch Ignored when attr ` reduce ` ` ` False ` ` Default ` ` True ` ` reduce bool optional Deprecated see attr ` reduction ` By default losses averaged summed over observations each minibatch depending attr ` size_average ` When attr ` reduce ` ` ` False ` ` returns loss per batch element instead ignores attr ` size_average ` Default ` ` True ` ` reduction str optional Specifies reduction apply output ` ` none ` ` &#124; ` ` mean ` ` &#124; ` ` sum ` ` ` ` none ` ` no reduction will applied ` ` mean ` ` sum output will divided number elements output ` ` sum ` ` output will summed Note attr ` size_average ` attr ` reduce ` process being deprecated meantime specifying either those two args will override attr ` reduction ` Default ` ` mean ` ` pos_weight Tensor optional weight positive examples broadcasted target Must tensor equal size along dimension number classes Pay close attention PyTorch s broadcasting semantics order achieve desired operations For target size B C H W where B batch size pos_weight size B C H W will apply different pos_weights each element batch C H W same pos_weights across batch To apply same positive weight along all spatial dimensions D multi-class target C H W use C Default ` ` None ` ` Shape - Input math ` ` where math ` ` means any number dimensions - Target math ` ` same shape input - Output scalar If attr ` reduction ` ` ` none ` ` then math ` ` same shape input Examples loss = nn BCEWithLogitsLoss input = torch randn requires_grad=True target = torch empty random_ output = loss input target output backward __init__ weight Optional Tensor = None size_average=None reduce=None reduction str = mean pos_weight Optional Tensor = None - None super __init__ size_average reduce reduction register_buffer weight weight register_buffer pos_weight pos_weight weight Optional Tensor pos_weight Optional Tensor forward input Tensor target Tensor - Tensor Runs forward pass F binary_cross_entropy_with_logits input target weight pos_weight=self pos_weight reduction=self reduction HingeEmbeddingLoss _Loss r Measures loss given input tensor math ` x ` labels tensor math ` y ` containing - This usually used measuring whether two inputs similar dissimilar e g using L pairwise distance math ` x ` typically used learning nonlinear embeddings semi-supervised learning The loss function math ` n ` -th sample mini-batch math l_n = \begin cases x_n \text \ y_n = \\ \max \ margin - x_n\ \text \ y_n = - \end cases total loss functions math \ell x y = \begin cases \operatorname mean L \text reduction = \text ` mean \\ \operatorname sum L \text reduction = \text ` sum \end cases where math ` L = \ l_ \dots l_N\ ^\top ` Args margin float optional Has default value ` ` size_average bool optional Deprecated see attr ` reduction ` By default losses averaged over each loss element batch Note some losses there multiple elements per sample If field attr ` size_average ` set ` ` False ` ` losses instead summed each minibatch Ignored when attr ` reduce ` ` ` False ` ` Default ` ` True ` ` reduce bool optional Deprecated see attr ` reduction ` By default losses averaged summed over observations each minibatch depending attr ` size_average ` When attr ` reduce ` ` ` False ` ` returns loss per batch element instead ignores attr ` size_average ` Default ` ` True ` ` reduction str optional Specifies reduction apply output ` ` none ` ` &#124; ` ` mean ` ` &#124; ` ` sum ` ` ` ` none ` ` no reduction will applied ` ` mean ` ` sum output will divided number elements output ` ` sum ` ` output will summed Note attr ` size_average ` attr ` reduce ` process being deprecated meantime specifying either those two args will override attr ` reduction ` Default ` ` mean ` ` Shape - Input math ` ` where math ` ` means any number dimensions The sum operation operates over all elements - Target math ` ` same shape input - Output scalar If attr ` reduction ` ` ` none ` ` then same shape input __constants__ = margin reduction margin float __init__ margin float = size_average=None reduce=None reduction str = mean - None super __init__ size_average reduce reduction margin = margin forward input Tensor target Tensor - Tensor Runs forward pass F hinge_embedding_loss input target margin=self margin reduction=self reduction MultiLabelMarginLoss _Loss r Creates criterion optimizes multi-class multi-classification hinge loss margin-based loss between input math ` x ` D mini-batch ` Tensor ` output math ` y ` which D ` Tensor ` target indices For each sample mini-batch math \text loss x y = \sum_ ij \frac \max - x y j - x i \text x size where math ` x \in \left\ \ \cdots \ \text x size - \right\ ` \ math ` y \in \left\ \ \cdots \ \text y size - \right\ ` \ math ` \leq y j \leq \text x size - ` \ math ` i \neq y j ` all math ` i ` math ` j ` math ` y ` math ` x ` must have same size The criterion only considers contiguous block non-negative targets starts front This allows different samples have variable amounts target classes Args size_average bool optional Deprecated see attr ` reduction ` By default losses averaged over each loss element batch Note some losses there multiple elements per sample If field attr ` size_average ` set ` ` False ` ` losses instead summed each minibatch Ignored when attr ` reduce ` ` ` False ` ` Default ` ` True ` ` reduce bool optional Deprecated see attr ` reduction ` By default losses averaged summed over observations each minibatch depending attr ` size_average ` When attr ` reduce ` ` ` False ` ` returns loss per batch element instead ignores attr ` size_average ` Default ` ` True ` ` reduction str optional Specifies reduction apply output ` ` none ` ` &#124; ` ` mean ` ` &#124; ` ` sum ` ` ` ` none ` ` no reduction will applied ` ` mean ` ` sum output will divided number elements output ` ` sum ` ` output will summed Note attr ` size_average ` attr ` reduce ` process being deprecated meantime specifying either those two args will override attr ` reduction ` Default ` ` mean ` ` Shape - Input math ` C ` math ` N C ` where ` N ` batch size ` C ` number classes - Target math ` C ` math ` N C ` label targets padded - ensuring same shape input - Output scalar If attr ` reduction ` ` ` none ` ` then math ` N ` Examples loss = nn MultiLabelMarginLoss x = torch FloatTensor target y only consider labels after label - y = torch LongTensor - - - + - - + - - + - - loss x y tensor __constants__ = reduction __init__ size_average=None reduce=None reduction str = mean - None super __init__ size_average reduce reduction forward input Tensor target Tensor - Tensor Runs forward pass F multilabel_margin_loss input target reduction=self reduction SmoothL Loss _Loss r Creates criterion uses squared term absolute element-wise error falls below beta L term otherwise It less sensitive outliers than ` torch nn MSELoss ` some cases prevents exploding gradients e g see paper ` Fast R-CNN ` _ Ross Girshick For batch size math ` N ` unreduced loss can described math \ell x y = L = \ l_ l_N\ ^T math l_n = \begin cases x_n - y_n ^ beta \text &#124; x_n - y_n &#124; beta \\ &#124; x_n - y_n &#124; - beta \text otherwise \end cases If ` reduction ` ` none ` then math \ell x y = \begin cases \operatorname mean L \text reduction = \text ` mean \\ \operatorname sum L \text reduction = \text ` sum \end cases note Smooth L loss can seen exactly ` L Loss ` math ` &#124; x - y &#124; beta ` portion replaced quadratic function such its slope math ` &#124; x - y &#124; = beta ` The quadratic segment smooths L loss near math ` &#124; x - y &#124; = ` note Smooth L loss closely related ` HuberLoss ` being equivalent math ` huber x y beta ` note Smooth L s beta hyper-parameter also known delta Huber This leads following differences As beta - Smooth L loss converges ` L Loss ` while ` HuberLoss ` converges constant loss When beta Smooth L loss equivalent L loss As beta - math ` +\infty ` Smooth L loss converges constant loss while ` HuberLoss ` converges ` MSELoss ` For Smooth L loss beta varies L segment loss has constant slope For ` HuberLoss ` slope L segment beta _ ` Fast R-CNN ` https arxiv org abs Args size_average bool optional Deprecated see attr ` reduction ` By default losses averaged over each loss element batch Note some losses there multiple elements per sample If field attr ` size_average ` set ` ` False ` ` losses instead summed each minibatch Ignored when attr ` reduce ` ` ` False ` ` Default ` ` True ` ` reduce bool optional Deprecated see attr ` reduction ` By default losses averaged summed over observations each minibatch depending attr ` size_average ` When attr ` reduce ` ` ` False ` ` returns loss per batch element instead ignores attr ` size_average ` Default ` ` True ` ` reduction str optional Specifies reduction apply output ` ` none ` ` &#124; ` ` mean ` ` &#124; ` ` sum ` ` ` ` none ` ` no reduction will applied ` ` mean ` ` sum output will divided number elements output ` ` sum ` ` output will summed Note attr ` size_average ` attr ` reduce ` process being deprecated meantime specifying either those two args will override attr ` reduction ` Default ` ` mean ` ` beta float optional Specifies threshold which change between L L loss The value must non-negative Default Shape - Input math ` ` where math ` ` means any number dimensions - Target math ` ` same shape input - Output scalar If attr ` reduction ` ` ` none ` ` then math ` ` same shape input __constants__ = reduction __init__ size_average=None reduce=None reduction str = mean beta float = - None super __init__ size_average reduce reduction beta = beta forward input Tensor target Tensor - Tensor Runs forward pass F smooth_l _loss input target reduction=self reduction beta=self beta HuberLoss _Loss r Creates criterion uses squared term absolute element-wise error falls below delta delta-scaled L term otherwise This loss combines advantages both ` L Loss ` ` MSELoss ` delta-scaled L region makes loss less sensitive outliers than ` MSELoss ` while L region provides smoothness over ` L Loss ` near See ` Huber loss https en wikipedia org wiki Huber_loss ` _ more information For batch size math ` N ` unreduced loss can described math \ell x y = L = \ l_ l_N\ ^T math l_n = \begin cases x_n - y_n ^ \text &#124; x_n - y_n &#124; delta \\ delta &#124; x_n - y_n &#124; - delta \text otherwise \end cases If ` reduction ` ` none ` then math \ell x y = \begin cases \operatorname mean L \text reduction = \text ` mean \\ \operatorname sum L \text reduction = \text ` sum \end cases note When delta set loss equivalent ` SmoothL Loss ` In general loss differs ` SmoothL Loss ` factor delta AKA beta Smooth L See ` SmoothL Loss ` additional discussion differences behavior between two losses Args reduction str optional Specifies reduction apply output ` ` none ` ` &#124; ` ` mean ` ` &#124; ` ` sum ` ` ` ` none ` ` no reduction will applied ` ` mean ` ` sum output will divided number elements output ` ` sum ` ` output will summed Default ` ` mean ` ` delta float optional Specifies threshold which change between delta-scaled L L loss The value must positive Default Shape - Input math ` ` where math ` ` means any number dimensions - Target math ` ` same shape input - Output scalar If attr ` reduction ` ` ` none ` ` then math ` ` same shape input __constants__ = reduction delta __init__ reduction str = mean delta float = - None super __init__ reduction=reduction delta = delta forward input Tensor target Tensor - Tensor Runs forward pass F huber_loss input target reduction=self reduction delta=self delta SoftMarginLoss _Loss r Creates criterion optimizes two-class classification logistic loss between input tensor math ` x ` target tensor math ` y ` containing - math \text loss x y = \sum_i \frac \log + \exp -y i x i \text x nelement Args size_average bool optional Deprecated see attr ` reduction ` By default losses averaged over each loss element batch Note some losses there multiple elements per sample If field attr ` size_average ` set ` ` False ` ` losses instead summed each minibatch Ignored when attr ` reduce ` ` ` False ` ` Default ` ` True ` ` reduce bool optional Deprecated see attr ` reduction ` By default losses averaged summed over observations each minibatch depending attr ` size_average ` When attr ` reduce ` ` ` False ` ` returns loss per batch element instead ignores attr ` size_average ` Default ` ` True ` ` reduction str optional Specifies reduction apply output ` ` none ` ` &#124; ` ` mean ` ` &#124; ` ` sum ` ` ` ` none ` ` no reduction will applied ` ` mean ` ` sum output will divided number elements output ` ` sum ` ` output will summed Note attr ` size_average ` attr ` reduce ` process being deprecated meantime specifying either those two args will override attr ` reduction ` Default ` ` mean ` ` Shape - Input math ` ` where math ` ` means any number dimensions - Target math ` ` same shape input - Output scalar If attr ` reduction ` ` ` none ` ` then math ` ` same shape input __constants__ = reduction __init__ size_average=None reduce=None reduction str = mean - None super __init__ size_average reduce reduction forward input Tensor target Tensor - Tensor Runs forward pass F soft_margin_loss input target reduction=self reduction CrossEntropyLoss _WeightedLoss r This criterion computes cross entropy loss between input logits target It useful when training classification problem ` C ` classes If provided optional argument attr ` weight ` should D ` Tensor ` assigning weight each classes This particularly useful when you have unbalanced training set The ` input ` expected contain unnormalized logits each which do ` ` need positive sum general ` input ` has Tensor size math ` C ` unbatched input math ` minibatch C ` math ` minibatch C d_ d_ d_K ` math ` K \geq ` ` K ` -dimensional case The last being useful higher dimension inputs such computing cross entropy loss per-pixel D images The ` target ` criterion expects should contain either - Class indices range math ` C ` where math ` C ` number classes ` ignore_index ` specified loss also accepts index index may necessarily range The unreduced i e attr ` reduction ` set ` ` none ` ` loss case can described math \ell x y = L = \ l_ \dots l_N\ ^\top \quad l_n = - w_ y_n \log \frac \exp x_ n y_n \sum_ c= ^C \exp x_ n c \cdot \mathbb \ y_n \not= \text ignore\_index \ where math ` x ` input math ` y ` target math ` w ` weight math ` C ` number classes math ` N ` spans minibatch dimension well math ` d_ d_k ` ` K ` -dimensional case If attr ` reduction ` ` ` none ` ` default ` ` mean ` ` then math \ell x y = \begin cases \sum_ n= ^N \frac \sum_ n= ^N w_ y_n \cdot \mathbb \ y_n \not= \text ignore\_index \ l_n \text reduction = \text ` mean \\ \sum_ n= ^N l_n \text reduction = \text ` sum \end cases Note case equivalent applying ` ~torch nn LogSoftmax ` input followed ` ~torch nn NLLLoss ` - Probabilities each useful when labels beyond single per minibatch item required such blended labels label smoothing etc The unreduced i e attr ` reduction ` set ` ` none ` ` loss case can described math \ell x y = L = \ l_ \dots l_N\ ^\top \quad l_n = - \sum_ c= ^C w_c \log \frac \exp x_ n c \sum_ i= ^C \exp x_ n i y_ n c where math ` x ` input math ` y ` target math ` w ` weight math ` C ` number classes math ` N ` spans minibatch dimension well math ` d_ d_k ` ` K ` -dimensional case If attr ` reduction ` ` ` none ` ` default ` ` mean ` ` then math \ell x y = \begin cases \frac \sum_ n= ^N l_n N \text reduction = \text ` mean \\ \sum_ n= ^N l_n \text reduction = \text ` sum \end cases note The performance criterion generally better when ` target ` contains indices allows optimized computation Consider providing ` target ` probabilities only when single label per minibatch item too restrictive Args weight Tensor optional manual rescaling weight given each If given has Tensor size ` C ` size_average bool optional Deprecated see attr ` reduction ` By default losses averaged over each loss element batch Note some losses there multiple elements per sample If field attr ` size_average ` set ` ` False ` ` losses instead summed each minibatch Ignored when attr ` reduce ` ` ` False ` ` Default ` ` True ` ` ignore_index int optional Specifies target value ignored does contribute input gradient When attr ` size_average ` ` ` True ` ` loss averaged over non-ignored targets Note attr ` ignore_index ` only applicable when target contains indices reduce bool optional Deprecated see attr ` reduction ` By default losses averaged summed over observations each minibatch depending attr ` size_average ` When attr ` reduce ` ` ` False ` ` returns loss per batch element instead ignores attr ` size_average ` Default ` ` True ` ` reduction str optional Specifies reduction apply output ` ` none ` ` &#124; ` ` mean ` ` &#124; ` ` sum ` ` ` ` none ` ` no reduction will applied ` ` mean ` ` weighted mean output taken ` ` sum ` ` output will summed Note attr ` size_average ` attr ` reduce ` process being deprecated meantime specifying either those two args will override attr ` reduction ` Default ` ` mean ` ` label_smoothing float optional A float Specifies amount smoothing when computing loss where means no smoothing The targets become mixture original ground truth uniform distribution described ` Rethinking Inception Architecture Computer Vision https arxiv org abs ` __ Default math ` ` Shape - Input Shape math ` C ` math ` N C ` math ` N C d_ d_ d_K ` math ` K \geq ` case ` K ` -dimensional loss - Target If containing indices shape math ` ` math ` N ` math ` N d_ d_ d_K ` math ` K \geq ` case K-dimensional loss where each value should between math ` C ` The target data type required long when using indices If containing probabilities target must same shape input each value should between math ` ` This means target data type required float when using probabilities Note PyTorch does strictly enforce probability constraints probabilities user s responsibility ensure ` ` target ` ` contains valid probability distributions see below examples section more details - Output If reduction none shape math ` ` math ` N ` math ` N d_ d_ d_K ` math ` K \geq ` case K-dimensional loss depending shape input Otherwise scalar where math \begin aligned C = \text number classes \\ N = \text batch size \\ \end aligned Examples Example target indices loss = nn CrossEntropyLoss input = torch randn requires_grad=True target = torch empty dtype=torch long random_ output = loss input target output backward Example target probabilities input = torch randn requires_grad=True target = torch randn softmax dim= output = loss input target output backward note When ` ` target ` ` contains probabilities should consist soft labels â€” each ` ` target ` ` entry should represent probability distribution over possible classes given data sample individual probabilities between ` ` ` ` total distribution summing This why func ` softmax ` function applied ` ` target ` ` probabilities example above PyTorch does validate whether values provided ` ` target ` ` lie range ` ` ` ` whether distribution each data sample sums ` ` ` ` No warning will raised user s responsibility ensure ` ` target ` ` contains valid probability distributions Providing arbitrary values may yield misleading loss values unstable gradients during training Examples xdoctest +SKIP Example target incorrectly specified probabilities loss = nn CrossEntropyLoss torch manual_seed input = torch randn requires_grad=True target = torch randn Provided target probabilities range target tensor - - - - - - Provided target probabilities do sum target sum axis= tensor No error message possible misleading loss value loss input target item Example target correctly specified probabilities Use softmax ensure true probability distribution target_new = target softmax dim= New target probabilities all range target_new tensor New target probabilities sum target_new sum axis= tensor loss input target_new item __constants__ = ignore_index reduction label_smoothing ignore_index int label_smoothing float __init__ weight Optional Tensor = None size_average=None ignore_index int = - reduce=None reduction str = mean label_smoothing float = - None super __init__ weight size_average reduce reduction ignore_index = ignore_index label_smoothing = label_smoothing forward input Tensor target Tensor - Tensor Runs forward pass F cross_entropy input target weight=self weight ignore_index=self ignore_index reduction=self reduction label_smoothing=self label_smoothing MultiLabelSoftMarginLoss _WeightedLoss r Creates criterion optimizes multi-label one-versus-all loss based max-entropy between input math ` x ` target math ` y ` size math ` N C ` For each sample minibatch math loss x y = - \frac C \sum_i y i \log + \exp -x i ^ - + -y i \log\left \frac \exp -x i + \exp -x i \right where math ` i \in \left\ \ \cdots \ \text x nElement - \right\ ` math ` y i \in \left\ \ \right\ ` Args weight Tensor optional manual rescaling weight given each If given has Tensor size ` C ` Otherwise treated having all ones size_average bool optional Deprecated see attr ` reduction ` By default losses averaged over each loss element batch Note some losses there multiple elements per sample If field attr ` size_average ` set ` ` False ` ` losses instead summed each minibatch Ignored when attr ` reduce ` ` ` False ` ` Default ` ` True ` ` reduce bool optional Deprecated see attr ` reduction ` By default losses averaged summed over observations each minibatch depending attr ` size_average ` When attr ` reduce ` ` ` False ` ` returns loss per batch element instead ignores attr ` size_average ` Default ` ` True ` ` reduction str optional Specifies reduction apply output ` ` none ` ` &#124; ` ` mean ` ` &#124; ` ` sum ` ` ` ` none ` ` no reduction will applied ` ` mean ` ` sum output will divided number elements output ` ` sum ` ` output will summed Note attr ` size_average ` attr ` reduce ` process being deprecated meantime specifying either those two args will override attr ` reduction ` Default ` ` mean ` ` Shape - Input math ` N C ` where ` N ` batch size ` C ` number classes - Target math ` N C ` label targets must have same shape input - Output scalar If attr ` reduction ` ` ` none ` ` then math ` N ` __constants__ = reduction __init__ weight Optional Tensor = None size_average=None reduce=None reduction str = mean - None super __init__ weight size_average reduce reduction forward input Tensor target Tensor - Tensor Runs forward pass F multilabel_soft_margin_loss input target weight=self weight reduction=self reduction CosineEmbeddingLoss _Loss r Creates criterion measures loss given input tensors math ` x_ ` math ` x_ ` ` Tensor ` label math ` y ` values - Use math ` y= ` maximize cosine similarity two inputs math ` y=- ` otherwise This typically used learning nonlinear embeddings semi-supervised learning The loss function each sample math \text loss x y = \begin cases - \cos x_ x_ \text y = \\ \max \cos x_ x_ - \text margin \text y = - \end cases Args margin float optional Should number math ` - ` math ` ` math ` ` math ` ` suggested If attr ` margin ` missing default value math ` ` size_average bool optional Deprecated see attr ` reduction ` By default losses averaged over each loss element batch Note some losses there multiple elements per sample If field attr ` size_average ` set ` ` False ` ` losses instead summed each minibatch Ignored when attr ` reduce ` ` ` False ` ` Default ` ` True ` ` reduce bool optional Deprecated see attr ` reduction ` By default losses averaged summed over observations each minibatch depending attr ` size_average ` When attr ` reduce ` ` ` False ` ` returns loss per batch element instead ignores attr ` size_average ` Default ` ` True ` ` reduction str optional Specifies reduction apply output ` ` none ` ` &#124; ` ` mean ` ` &#124; ` ` sum ` ` ` ` none ` ` no reduction will applied ` ` mean ` ` sum output will divided number elements output ` ` sum ` ` output will summed Note attr ` size_average ` attr ` reduce ` process being deprecated meantime specifying either those two args will override attr ` reduction ` Default ` ` mean ` ` Shape - Input math ` N D ` math ` D ` where ` N ` batch size ` D ` embedding dimension - Input math ` N D ` math ` D ` same shape Input - Target math ` N ` math ` ` - Output If attr ` reduction ` ` ` none ` ` then math ` N ` otherwise scalar Examples loss = nn CosineEmbeddingLoss input = torch randn requires_grad=True input = torch randn requires_grad=True target = torch ones output = loss input input target output backward __constants__ = margin reduction margin float __init__ margin float = size_average=None reduce=None reduction str = mean - None super __init__ size_average reduce reduction margin = margin forward input Tensor input Tensor target Tensor - Tensor Runs forward pass F cosine_embedding_loss input input target margin=self margin reduction=self reduction MarginRankingLoss _Loss r Creates criterion measures loss given inputs math ` x ` math ` x ` two D mini-batch D ` Tensors ` label D mini-batch D ` Tensor ` math ` y ` containing - If math ` y = ` then assumed first input should ranked higher have larger value than second input vice-versa math ` y = - ` The loss function each pair samples mini-batch math \text loss x x y = \max -y x - x + \text margin Args margin float optional Has default value math ` ` size_average bool optional Deprecated see attr ` reduction ` By default losses averaged over each loss element batch Note some losses there multiple elements per sample If field attr ` size_average ` set ` ` False ` ` losses instead summed each minibatch Ignored when attr ` reduce ` ` ` False ` ` Default ` ` True ` ` reduce bool optional Deprecated see attr ` reduction ` By default losses averaged summed over observations each minibatch depending attr ` size_average ` When attr ` reduce ` ` ` False ` ` returns loss per batch element instead ignores attr ` size_average ` Default ` ` True ` ` reduction str optional Specifies reduction apply output ` ` none ` ` &#124; ` ` mean ` ` &#124; ` ` sum ` ` ` ` none ` ` no reduction will applied ` ` mean ` ` sum output will divided number elements output ` ` sum ` ` output will summed Note attr ` size_average ` attr ` reduce ` process being deprecated meantime specifying either those two args will override attr ` reduction ` Default ` ` mean ` ` Shape - Input math ` N ` math ` ` where ` N ` batch size - Input math ` N ` math ` ` same shape Input - Target math ` N ` math ` ` same shape inputs - Output scalar If attr ` reduction ` ` ` none ` ` Input size math ` ` then math ` N ` Examples loss = nn MarginRankingLoss input = torch randn requires_grad=True input = torch randn requires_grad=True target = torch randn sign output = loss input input target output backward __constants__ = margin reduction margin float __init__ margin float = size_average=None reduce=None reduction str = mean - None super __init__ size_average reduce reduction margin = margin forward input Tensor input Tensor target Tensor - Tensor Runs forward pass F margin_ranking_loss input input target margin=self margin reduction=self reduction MultiMarginLoss _WeightedLoss r Creates criterion optimizes multi-class classification hinge loss margin-based loss between input math ` x ` D mini-batch ` Tensor ` output math ` y ` which D tensor target indices math ` \leq y \leq \text x size - ` For each mini-batch sample loss terms D input math ` x ` scalar output math ` y ` math \text loss x y = \frac \sum_i \max \text margin - x y + x i ^p \text x size where math ` i \in \left\ \ \cdots \ \text x size - \right\ ` math ` i \neq y ` Optionally you can give non-equal weighting classes passing D attr ` weight ` tensor into constructor The loss function then becomes math \text loss x y = \frac \sum_i w y \max \text margin - x y + x i ^p \text x size Args p int optional Has default value math ` ` math ` ` math ` ` only supported values margin float optional Has default value math ` ` weight Tensor optional manual rescaling weight given each If given has Tensor size ` C ` Otherwise treated having all ones size_average bool optional Deprecated see attr ` reduction ` By default losses averaged over each loss element batch Note some losses there multiple elements per sample If field attr ` size_average ` set ` ` False ` ` losses instead summed each minibatch Ignored when attr ` reduce ` ` ` False ` ` Default ` ` True ` ` reduce bool optional Deprecated see attr ` reduction ` By default losses averaged summed over observations each minibatch depending attr ` size_average ` When attr ` reduce ` ` ` False ` ` returns loss per batch element instead ignores attr ` size_average ` Default ` ` True ` ` reduction str optional Specifies reduction apply output ` ` none ` ` &#124; ` ` mean ` ` &#124; ` ` sum ` ` ` ` none ` ` no reduction will applied ` ` mean ` ` sum output will divided number elements output ` ` sum ` ` output will summed Note attr ` size_average ` attr ` reduce ` process being deprecated meantime specifying either those two args will override attr ` reduction ` Default ` ` mean ` ` Shape - Input math ` N C ` math ` C ` where math ` N ` batch size math ` C ` number classes - Target math ` N ` math ` ` where each value math ` \leq \text targets i \leq C- ` - Output scalar If attr ` reduction ` ` ` none ` ` then same shape target Examples loss = nn MultiMarginLoss x = torch tensor y = torch tensor - - + - - + - - loss x y tensor __constants__ = p margin reduction margin float p int __init__ p int = margin float = weight Optional Tensor = None size_average=None reduce=None reduction str = mean - None super __init__ weight size_average reduce reduction p = p = raise ValueError only p == p == supported weight None weight dim = raise ValueError f MultiMarginLoss expected weight None D tensor got weight dim D instead p = p margin = margin forward input Tensor target Tensor - Tensor Runs forward pass F multi_margin_loss input target p=self p margin=self margin weight=self weight reduction=self reduction TripletMarginLoss _Loss r Creates criterion measures triplet loss given input tensors math ` x ` math ` x ` math ` x ` margin value greater than math ` ` This used measuring relative similarity between samples A triplet composed ` ` ` p ` ` n ` i e ` anchor ` ` positive examples ` ` negative examples ` respectively The shapes all input tensors should math ` N D ` The distance swap described detail paper ` Learning shallow convolutional feature descriptors triplet losses ` _ V Balntas E Riba et al The loss function each sample mini-batch math L p n = \max \ d a_i p_i - d a_i n_i + \rm margin \ where math d x_i y_i = \left\lVert \bf x _i - \bf y _i \right\rVert_p The norm calculated using specified p value small constant math ` \varepsilon ` added numerical stability See also ` ~torch nn TripletMarginWithDistanceLoss ` which computes triplet margin loss input tensors using custom distance function Args margin float optional Default math ` ` p int optional The norm degree pairwise distance Default math ` ` eps float optional Small constant numerical stability Default math ` e- ` swap bool optional The distance swap described detail paper ` Learning shallow convolutional feature descriptors triplet losses ` V Balntas E Riba et al Default ` ` False ` ` size_average bool optional Deprecated see attr ` reduction ` By default losses averaged over each loss element batch Note some losses there multiple elements per sample If field attr ` size_average ` set ` ` False ` ` losses instead summed each minibatch Ignored when attr ` reduce ` ` ` False ` ` Default ` ` True ` ` reduce bool optional Deprecated see attr ` reduction ` By default losses averaged summed over observations each minibatch depending attr ` size_average ` When attr ` reduce ` ` ` False ` ` returns loss per batch element instead ignores attr ` size_average ` Default ` ` True ` ` reduction str optional Specifies reduction apply output ` ` none ` ` &#124; ` ` mean ` ` &#124; ` ` sum ` ` ` ` none ` ` no reduction will applied ` ` mean ` ` sum output will divided number elements output ` ` sum ` ` output will summed Note attr ` size_average ` attr ` reduce ` process being deprecated meantime specifying either those two args will override attr ` reduction ` Default ` ` mean ` ` Shape - Input math ` N D ` math ` D ` where math ` D ` vector dimension - Output A Tensor shape math ` N ` attr ` reduction ` ` ` none ` ` input shape math ` N D ` scalar otherwise Examples triplet_loss = nn TripletMarginLoss margin= p= eps= e- anchor = torch randn requires_grad=True positive = torch randn requires_grad=True negative = torch randn requires_grad=True output = triplet_loss anchor positive negative output backward _Learning shallow convolutional feature descriptors triplet losses https bmva-archive org uk bmvc papers paper index html __constants__ = margin p eps swap reduction margin float p float eps float swap bool __init__ margin float = p float = eps float = e- swap bool = False size_average=None reduce=None reduction str = mean - None super __init__ size_average reduce reduction margin = raise ValueError f TripletMarginLoss expected margin greater than got margin instead margin = margin p = p eps = eps swap = swap forward anchor Tensor positive Tensor negative Tensor - Tensor Runs forward pass F triplet_margin_loss anchor positive negative margin=self margin p=self p eps=self eps swap=self swap reduction=self reduction TripletMarginWithDistanceLoss _Loss r Creates criterion measures triplet loss given input tensors math ` ` math ` p ` math ` n ` representing anchor positive negative examples respectively nonnegative real-valued function distance function used compute relationship between anchor positive example positive distance anchor negative example negative distance The unreduced loss i e attr ` reduction ` set ` ` none ` ` can described math \ell p n = L = \ l_ \dots l_N\ ^\top \quad l_i = \max \ d a_i p_i - d a_i n_i + \rm margin \ where math ` N ` batch size math ` d ` nonnegative real-valued function quantifying closeness two tensors referred attr ` distance_function ` math ` margin ` nonnegative margin representing minimum difference between positive negative distances required loss The input tensors have math ` N ` elements each can any shape distance function can handle If attr ` reduction ` ` ` none ` ` default ` ` mean ` ` then math \ell x y = \begin cases \operatorname mean L \text reduction = \text ` mean \\ \operatorname sum L \text reduction = \text ` sum \end cases See also ` ~torch nn TripletMarginLoss ` which computes triplet loss input tensors using math ` l_p ` distance distance function Args distance_function Callable optional A nonnegative real-valued function quantifies closeness two tensors If specified ` nn PairwiseDistance ` will used Default ` ` None ` ` margin float optional A nonnegative margin representing minimum difference between positive negative distances required loss Larger margins penalize cases where negative examples distant enough anchors relative positives Default math ` ` swap bool optional Whether use distance swap described paper ` Learning shallow convolutional feature descriptors triplet losses ` V Balntas E Riba et al If True positive example closer negative example than anchor swaps positive example anchor loss computation Default ` ` False ` ` reduction str optional Specifies optional reduction apply output ` ` none ` ` &#124; ` ` mean ` ` &#124; ` ` sum ` ` ` ` none ` ` no reduction will applied ` ` mean ` ` sum output will divided number elements output ` ` sum ` ` output will summed Default ` ` mean ` ` Shape - Input math ` N ` where math ` ` represents any number additional dimensions supported distance function - Output A Tensor shape math ` N ` attr ` reduction ` ` ` none ` ` scalar otherwise Examples Initialize embeddings embedding = nn Embedding anchor_ids = torch randint positive_ids = torch randint negative_ids = torch randint anchor = embedding anchor_ids positive = embedding positive_ids negative = embedding negative_ids Built-in Distance Function triplet_loss = \ nn TripletMarginWithDistanceLoss distance_function=nn PairwiseDistance output = triplet_loss anchor positive negative output backward Custom Distance Function l_infinity x x torch max torch abs x - x dim= values xdoctest +SKIP FIXME Would call backwards second time triplet_loss = nn TripletMarginWithDistanceLoss distance_function=l_infinity margin= output = triplet_loss anchor positive negative output backward Custom Distance Function Lambda triplet_loss = nn TripletMarginWithDistanceLoss distance_function=lambda x y - F cosine_similarity x y output = triplet_loss anchor positive negative output backward Reference V Balntas et al Learning shallow convolutional feature descriptors triplet losses https bmva-archive org uk bmvc papers paper index html __constants__ = margin swap reduction margin float swap bool __init__ distance_function Optional Callable Tensor Tensor Tensor = None margin float = swap bool = False reduction str = mean - None super __init__ size_average=None reduce=None reduction=reduction margin = raise ValueError f TripletMarginWithDistanceLoss expected margin greater than got margin instead distance_function Optional Callable Tensor Tensor Tensor = distance_function distance_function None PairwiseDistance margin = margin swap = swap forward anchor Tensor positive Tensor negative Tensor - Tensor Runs forward pass F triplet_margin_with_distance_loss anchor positive negative distance_function=self distance_function margin=self margin swap=self swap reduction=self reduction CTCLoss _Loss r The Connectionist Temporal Classification loss Calculates loss between continuous unsegmented time series target sequence CTCLoss sums over probability possible alignments input target producing loss value which differentiable respect each input node The alignment input target assumed many-to-one which limits length target sequence such must math ` \leq ` input length Args blank int optional blank label Default math ` ` reduction str optional Specifies reduction apply output ` ` none ` ` &#124; ` ` mean ` ` &#124; ` ` sum ` ` ` ` none ` ` no reduction will applied ` ` mean ` ` output losses will divided target lengths then mean over batch taken ` ` sum ` ` output losses will summed Default ` ` mean ` ` zero_infinity bool optional Whether zero infinite losses associated gradients Default ` ` False ` ` Infinite losses mainly occur when inputs too short aligned targets Shape - Log_probs Tensor size math ` T N C ` math ` T C ` where math ` T = \text input length ` math ` N = \text batch size ` math ` C = \text number classes including blank ` The logarithmized probabilities outputs e g obtained func ` torch nn functional log_softmax ` - Targets Tensor size math ` N S ` math ` \operatorname sum \text target\_lengths ` where math ` N = \text batch size ` math ` S = \text max target length shape N S ` It represents target sequences Each element target sequence index And target index cannot blank default= In math ` N S ` form targets padded length longest sequence stacked In math ` \operatorname sum \text target\_lengths ` form targets assumed un-padded concatenated within dimension - Input_lengths Tuple tensor size math ` N ` math ` ` where math ` N = \text batch size ` It represents lengths inputs must each math ` \leq T ` And lengths specified each sequence achieve masking under assumption sequences padded equal lengths - Target_lengths Tuple tensor size math ` N ` math ` ` where math ` N = \text batch size ` It represents lengths targets Lengths specified each sequence achieve masking under assumption sequences padded equal lengths If target shape math ` N S ` target_lengths effectively stop index math ` s_n ` each target sequence such ` ` target_n = targets n s_n ` ` each target batch Lengths must each math ` \leq S ` If targets given d tensor concatenation individual targets target_lengths must add up total length tensor - Output scalar attr ` reduction ` ` ` mean ` ` default ` ` sum ` ` If attr ` reduction ` ` ` none ` ` then math ` N ` input batched math ` ` input unbatched where math ` N = \text batch size ` Examples Target padded T = Input sequence length C = Number classes including blank N = Batch size S = Target sequence length longest target batch padding length S_min = Minimum target length demonstration purposes Initialize random batch input vectors size = T N C input = torch randn T N C log_softmax detach requires_grad_ Initialize random batch targets = blank C = classes target = torch randint low= high=C size= N S dtype=torch long input_lengths = torch full size= N fill_value=T dtype=torch long target_lengths = torch randint low=S_min high=S size= N dtype=torch long ctc_loss = nn CTCLoss loss = ctc_loss input target input_lengths target_lengths loss backward Target un-padded T = Input sequence length C = Number classes including blank N = Batch size Initialize random batch input vectors size = T N C input = torch randn T N C log_softmax detach requires_grad_ input_lengths = torch full size= N fill_value=T dtype=torch long Initialize random batch targets = blank C = classes target_lengths = torch randint low= high=T size= N dtype=torch long target = torch randint low= high=C size= sum target_lengths dtype=torch long ctc_loss = nn CTCLoss loss = ctc_loss input target input_lengths target_lengths loss backward Target un-padded unbatched effectively N= T = Input sequence length C = Number classes including blank Initialize random batch input vectors size = T C xdoctest +SKIP FIXME error doctest input = torch randn T C log_softmax detach requires_grad_ input_lengths = torch tensor T dtype=torch long Initialize random batch targets = blank C = classes target_lengths = torch randint low= high=T size= dtype=torch long target = torch randint low= high=C size= target_lengths dtype=torch long ctc_loss = nn CTCLoss loss = ctc_loss input target input_lengths target_lengths loss backward Reference A Graves et al Connectionist Temporal Classification Labelling Unsegmented Sequence Data Recurrent Neural Networks https www cs toronto edu ~graves icml_ pdf Note In order use CuDNN following must satisfied attr ` targets ` must concatenated format all attr ` input_lengths ` must ` T ` math ` blank= ` attr ` target_lengths ` math ` \leq ` integer arguments must dtype attr ` torch int ` attr ` log_probs ` itself must dtype attr ` torch float ` The regular implementation uses more common PyTorch ` torch long ` dtype Note In some circumstances when using CUDA backend CuDNN operator may select nondeterministic algorithm increase performance If undesirable you can try make operation deterministic potentially performance cost setting ` ` torch backends cudnn deterministic = True ` ` Please see notes doc ` notes randomness ` background __constants__ = blank reduction blank int zero_infinity bool __init__ blank int = reduction str = mean zero_infinity bool = False - None super __init__ reduction=reduction blank = blank zero_infinity = zero_infinity forward log_probs Tensor targets Tensor input_lengths Tensor target_lengths Tensor - Tensor Runs forward pass F ctc_loss log_probs targets input_lengths target_lengths blank reduction zero_infinity TODO L HingeEmbeddingCriterion TODO MSECriterion weight TODO ClassSimplexCriterion