mypy allow-untyped-decorators mypy allow-untyped-defs functools itertools numbers operator sys collections abc Callable Iterable contextlib nullcontext enum Enum functools partial reduce itertools chain product typing Any cast Optional Union torch torch _meta_registrations torch _prims prims torch _prims_common utils torch nn functional F torch sym_float sym_int Tensor torch _decomp register_decomposition torch _higher_order_ops out_dtype out_dtype torch _prims_common IntLike NumberType suggest_memory_format TensorLike TensorSequenceType torch _prims_common wrappers _maybe_convert_to_dtype _maybe_resize_out _safe_copy_out out_wrapper torch utils _pytree pytree torch utils _pytree tree_map DispatchKey = torch _C DispatchKey type ignore attr-defined None these functions publicly accessible get them torch _decomps __all__ list str = aten = torch _ops ops aten Reduction Enum NONE = MEAN = SUM = This wraps decomposition performs various type promotion logic within depending strategy provided We re currently reusing ELEMENTWISE_TYPE_PROMOTION_KIND although some usages non-elementwise ops Will need validate non-elementwise uses type_casts f Callable type_promotion utils ELEMENTWISE_TYPE_PROMOTION_KIND compute_dtype_only bool = False include_non_tensor_args bool = False functools wraps f inner args kwargs allowed_types = Tensor torch types _Number include_non_tensor_args Tensor type ignore arg-type flat_args = x x pytree arg_tree_leaves args kwargs isinstance x allowed_types computation_dtype result_dtype = utils elementwise_dtypes flat_args type_promotion_kind=type_promotion TODO pretty sure quite right increase_prec x isinstance x Tensor x computation_dtype x decrease_prec x isinstance x Tensor x result_dtype x r = f tree_map increase_prec args tree_map increase_prec kwargs compute_dtype_only r tree_map decrease_prec r inner compute_only_pw_cast_for_opmath = partial type_casts type_promotion=utils ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT compute_dtype_only=True pw_cast_for_opmath = partial type_casts type_promotion=utils ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT pw_cast_for_opmath_non_tensor_args = partial type_casts type_promotion=utils ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT include_non_tensor_args=True pw_cast_for_int_to_real = partial type_casts type_promotion=utils ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT This expands x until x dim == dim Might useful operator _unsqueeze_to_dim x Tensor dim int - Tensor _ range dim - x dim x = x unsqueeze - x register_decomposition aten tanh_backward out_wrapper grad_input pw_cast_for_opmath tanh_backward out_grad Tensor y Tensor out_grad - y y conj_physical register_decomposition aten sigmoid_backward out_wrapper grad_input pw_cast_for_opmath sigmoid_backward out_grad Tensor y Tensor out_grad y - y conj_physical register_decomposition aten softplus_backward out_wrapper grad_input pw_cast_for_opmath softplus_backward out_grad Tensor x Tensor beta float threshold float z = x beta exp torch where x beta threshold out_grad out_grad z z + register_decomposition aten elu_backward out_wrapper grad_input pw_cast_for_opmath elu_backward grad_output Tensor alpha float scale float input_scale float is_result bool self_or_result Tensor negcoef = alpha scale poscoef = scale negiptcoef = input_scale is_result torch where self_or_result = grad_output negiptcoef self_or_result + negcoef grad_output poscoef torch where self_or_result = grad_output negiptcoef negcoef torch exp self_or_result negiptcoef grad_output poscoef register_decomposition aten fill Scalar fill_scalar value torch full_like value register_decomposition aten fill Tensor fill_tensor value Tensor torch _check value dim == lambda f fill only supports -dimension value tensor got tensor value dim dimensions aten copy value register_decomposition aten hardsigmoid out_wrapper pw_cast_for_opmath hardsigmoid Tensor - Tensor torch clamp torch clamp + min= max= register_decomposition aten hardsigmoid_backward out_wrapper grad_input pw_cast_for_opmath hardsigmoid_backward grad_output Tensor Tensor torch where - grad_output register_decomposition aten hardtanh_backward out_wrapper grad_input hardtanh_backward grad_output Tensor Tensor min_val float max_val float torch where = min_val &#124; = max_val grad_output register_decomposition aten hardswish out_wrapper pw_cast_for_opmath hardswish Tensor - Tensor torch clamp torch clamp + min= max= register_decomposition aten hardswish_backward out_wrapper pw_cast_for_opmath hardswish_backward grad_output Tensor Tensor - Tensor torch where = - torch where grad_output + grad_output register_decomposition aten threshold_backward out_wrapper grad_input threshold_backward grad_output Tensor Tensor threshold float torch where = threshold grad_output register_decomposition aten leaky_relu_backward out_wrapper grad_input pw_cast_for_opmath leaky_relu_backward grad_output Tensor Tensor negative_slope float self_is_result bool torch where grad_output grad_output negative_slope register_decomposition aten gelu_backward out_wrapper grad_input pw_cast_for_opmath gelu_backward grad Tensor Tensor approximate str = none M_SQRT = M_SQRT _ = M_ _SQRTPI = approximate == tanh kBeta = M_SQRT M_ _SQRTPI kKappa = x_sq = x_cube = x_sq inner = kBeta + kKappa x_cube tanh_inner = torch tanh inner left = right = + tanh_inner left_derivative = right tanh_derivative = - tanh_inner tanh_inner inner_derivative = kBeta + kKappa x_sq right_derivative = left tanh_derivative inner_derivative grad left_derivative + right_derivative kAlpha = M_SQRT _ kBeta = M_ _SQRTPI M_SQRT _ cdf = + torch erf kAlpha pdf = kBeta torch exp - grad cdf + pdf register_decomposition aten mish_backward pw_cast_for_opmath mish_backward grad_output Tensor input Tensor input_tanh_softplus = torch tanh F softplus input input_sigmoid = torch sigmoid input out = input input_sigmoid - input_tanh_softplus input_tanh_softplus grad_output input_tanh_softplus + out register_decomposition aten silu out_wrapper pw_cast_for_opmath silu Tensor - Tensor torch sigmoid register_decomposition aten silu_backward out_wrapper grad_input pw_cast_for_opmath silu_backward grad_output Tensor Tensor - Tensor sigmoid = + torch exp -self grad_output sigmoid + - sigmoid register_decomposition aten _prelu_kernel _prelu_kernel Tensor weight Tensor - Tensor torch where weight register_decomposition aten _prelu_kernel_backward _prelu_kernel_backward grad_output Tensor Tensor weight Tensor - tuple Tensor Tensor input_grad = torch where grad_output weight grad_output weight_grad = torch where grad_output input_grad weight_grad register_decomposition aten rrelu_with_noise_backward out_wrapper pw_cast_for_opmath rrelu_with_noise_backward grad_output Tensor Tensor noise Tensor lower float upper float training bool self_is_result bool - Tensor training upper - lower e- grad_output mul noise negative_slope = lower + upper aten leaky_relu_backward grad_output negative_slope self_is_result register_decomposition aten log_sigmoid_backward out_wrapper grad_input pw_cast_for_opmath log_sigmoid_backward grad_output Tensor Tensor buffer Tensor - Tensor in_negative = max_deriv = torch where in_negative sign = torch where in_negative - z = torch exp -torch abs grad_output max_deriv - sign z + z CPU has special formula uses buffer disabled convenience sake max_deriv - sign buffer + buffer grad_output apply_loss_reduction loss Tensor reduction int reduction == Reduction MEAN value torch mean loss reduction == Reduction SUM value torch sum loss loss to_real_dtype dtype torch dtype dtype == torch complex torch float dtype == torch complex torch float dtype == torch complex torch float TODO None these loss castings quite correct see https github com pytorch pytorch issues Also ATen kernels perform pointwise portion opmath don t maintain between pointwise portion reduction register_decomposition aten mse_loss out_wrapper pw_cast_for_opmath mse_loss Tensor target Tensor reduction int = Reduction MEAN value - Tensor pyrefly ignore unsupported-operation loss = - target apply_loss_reduction loss reduction register_decomposition aten mse_loss_backward out_wrapper grad_input pw_cast_for_opmath mse_loss_backward grad_output Tensor input Tensor target Tensor reduction int norm = input numel reduction == Reduction MEAN value norm input - target grad_output register_decomposition aten _safe_softmax safe_softmax dim dtype=None out = torch softmax dim=dim dtype=dtype masked = eq float -inf masked_rows = torch all masked dim=dim keepdim=True zeros = torch zeros_like out torch where masked_rows zeros out register_decomposition aten smooth_l _loss out_wrapper pw_cast_for_opmath smooth_l _loss Tensor target Tensor reduction int = Reduction MEAN value beta float = loss = - target abs pyrefly ignore unsupported-operation loss = torch where loss beta loss beta loss - beta apply_loss_reduction loss reduction register_decomposition aten smooth_l _loss_backward default pw_cast_for_opmath smooth_l _loss_backward grad_output Tensor Tensor target Tensor reduction int beta float norm = numel reduction == Reduction MEAN value x = - target abs_x = torch abs x norm_grad = norm grad_output torch where abs_x beta norm_grad x beta norm_grad torch sign x register_decomposition aten smooth_l _loss_backward grad_input pw_cast_for_opmath smooth_l _loss_backward_out grad_output Tensor Tensor target Tensor reduction int beta float grad_input Tensor result = smooth_l _loss_backward grad_output target reduction beta _maybe_resize_out grad_input result shape _safe_copy_out copy_from=result copy_to=grad_input exact_dtype=True register_decomposition aten huber_loss_backward default pw_cast_for_opmath huber_loss_backward grad_output Tensor Tensor target Tensor reduction int delta float norm = numel reduction == Reduction MEAN value x = - target torch where x -delta -norm grad_output delta torch where x delta norm grad_output delta norm x grad_output We cannot use out_wrapper here because output tensor named out s grad_input register_decomposition aten huber_loss_backward out pw_cast_for_opmath huber_loss_backward_out grad_output Tensor Tensor target Tensor reduction int delta float grad_input Tensor result = huber_loss_backward grad_output target reduction delta _maybe_resize_out grad_input result shape _safe_copy_out copy_from=result copy_to=grad_input exact_dtype=True _nll_loss_backward grad_output Tensor Tensor target Tensor weight Optional Tensor reduction int ignore_index int total_weight Tensor - Tensor channel_dim = dim reduction == Reduction MEAN value grad_output = grad_output total_weight target = target unsqueeze channel_dim safe_target = torch where target = ignore_index target grad_input = torch zeros_like grad_input = torch scatter grad_input channel_dim safe_target - grad_input dim grad_output dim grad_output = grad_output unsqueeze channel_dim weight None new_shape = _ range dim new_shape channel_dim = weight shape weight = weight reshape new_shape grad_output = grad_output weight grad_output = torch where target = ignore_index grad_output grad_input grad_output register_decomposition aten glu_backward out_wrapper grad_input pw_cast_for_opmath glu_backward grad_output Tensor Tensor dim int - Tensor assert dim glu does support -dimensional tensors wrap_dim = utils canonicalize_dim dim dim nIn = size wrap_dim assert nIn == f Halving dimension must even dimension wrap_dim size nIn inputSize = nIn firstHalf = narrow wrap_dim inputSize secondHalf = narrow wrap_dim inputSize inputSize gradInputFirstHalf = torch sigmoid secondHalf gradInputSecondHalf = - gradInputFirstHalf gradInputFirstHalf firstHalf grad_output gradInputFirstHalf = gradInputFirstHalf grad_output torch cat gradInputFirstHalf gradInputSecondHalf dim=wrap_dim register_decomposition aten nll_loss_backward out_wrapper grad_input nll_loss_backward grad_output Tensor Tensor target Tensor weight Optional Tensor reduction int ignore_index int total_weight Tensor - Tensor assert = dim = input tensor should D D assert target dim = D D target tensor expected multi-target supported no_batch_dim = dim == target dim == assert no_batch_dim shape == target shape f size mismatch got input shape target target shape assert total_weight numel == expected total_weight single element tensor got f total_weight shape total_weight numel elements assert weight None weight numel == shape - weight tensor should defined either all no classes reduction == Reduction NONE value dim == assert grad_output dim == grad_output shape == shape f Expected tensor dimension tensor size == shape f got dimension grad_output dim tensor size == grad_output shape assert grad_output dim = grad_output numel == f Expected single element grad_output tensor got grad_output shape _nll_loss_backward grad_output target weight reduction ignore_index total_weight register_decomposition aten nll_loss d_backward out_wrapper grad_input nll_loss d_backward grad_output Tensor Tensor target Tensor weight Optional Tensor reduction int ignore_index int total_weight Tensor - Tensor assert dim == f only batches spatial inputs supported D tensors got input dimension dim assert target dim == f only batches spatial targets supported D tensors got targets dimension target dim assert shape == target shape shape == target shape shape == target shape f size mismatch got input shape target target shape assert total_weight numel == expected total_weight single element tensor f got total_weight shape total_weight numel elements _nll_loss_backward grad_output target weight reduction ignore_index total_weight register_decomposition aten binary_cross_entropy out_wrapper pw_cast_for_opmath binary_cross_entropy Tensor target Tensor weight Optional Tensor = None reduction int = Reduction MEAN value - Tensor We cannot currently model without introducing data-dependent control flow TORCH_CHECK input_val = input_val = all elements input should between loss = target - torch maximum torch log p -self new_full - - target torch maximum torch log new_full - weight None loss = loss weight apply_loss_reduction loss reduction register_decomposition aten binary_cross_entropy_backward out_wrapper grad_input pw_cast_for_opmath binary_cross_entropy_backward grad_output Tensor Tensor target Tensor weight Optional Tensor = None reduction int = Reduction MEAN value - Tensor EPSILON = e- result = grad_output - target torch clamp - min=EPSILON weight None result = result weight reduction == Reduction MEAN value result = result numel result register_decomposition aten soft_margin_loss out_wrapper pw_cast_for_opmath soft_margin_loss input Tensor target Tensor reduction int = Reduction MEAN value - Tensor loss = torch log p torch exp -input target apply_loss_reduction loss reduction register_decomposition aten soft_margin_loss_backward out_wrapper grad_input pw_cast_for_opmath soft_margin_loss_backward grad_output Tensor Tensor target Tensor reduction int = Reduction MEAN value - Tensor grad_input = target grad_output torch sigmoid target - reduction == Reduction MEAN value grad_input = grad_input numel grad_input register_decomposition aten dist out_wrapper dist input Tensor other Tensor p float = aten norm input - other p=p register_decomposition aten _euclidean_dist out_wrapper _euclidean_dist x Tensor x Tensor - Tensor x _norm = x pow sum - True x _pad = torch ones_like x _norm memory_format=torch contiguous_format x _norm = x pow sum - True x _pad = torch ones_like x _norm memory_format=torch contiguous_format x _ = torch cat x mul - x _norm x _pad - x _ = torch cat x x _pad x _norm - result = x _ matmul x _ mT result clamp_min sqrt register_decomposition aten slice_backward out_wrapper slice_backward grad_output Tensor input_sizes list int dim int start int end int step int grad_input = grad_output new_zeros input_sizes torch slice_scatter grad_input grad_output dim start end step register_decomposition aten slice Tensor slice_forward Tensor int dim= SymInt start=None SymInt end=None SymInt step= Tensor dim int = start Optional int = None end Optional int = None step int = torch fx experimental symbolic_shapes statically_known_true ndim = dim ndim == raise RuntimeError slice cannot applied -dim tensor dim = utils canonicalize_dim dim dim sizes = list size strides = list stride step = raise RuntimeError slice step must positive start_val = start start None end_val = end end None sys maxsize ^ - start_val start_val += sizes dim end_val end_val += sizes dim start_val start_val = start_val sizes dim start_val = sizes dim statically_known_true end_val == sys maxsize end_val = sizes dim end_val start_val end_val = start_val end_val sizes dim end_val = sizes dim storage_offset = storage_offset + start_val strides dim len = end_val - start_val sizes dim = len + step - step strides dim = step is_quantized raise NotImplementedError Slice decomposition quantized tensors aren t implemented as_strided sizes strides storage_offset _normalize_start_end x Tensor dim int start Optional int end Optional int - tuple int int Normalize start end such both range x get_size dim start = end dim_size = x shape dim clamp_wrap val lower upper default - int val None default val val = val + dim_size min max val lower upper start = clamp_wrap start dim_size end = clamp_wrap end start dim_size dim_size start end This torch _refs because aten index used aten _unsafe_masked_index does have decomposition register_decomposition aten slice_scatter out_wrapper slice_scatter input Tensor src Tensor dim int = start Optional int = None end Optional int = None step int = dim = utils canonicalize_dim input ndim dim dim_size = input shape dim start end = _normalize_start_end input dim start end src_size = list input shape src_size dim = end - start + step - step src = src expand src_size start == end == dim_size step == src clone indices list Optional Tensor = None input dim idx = torch arange dim_size device=input device indices dim = idx - start step mask = torch ones dim_size device=input device dtype=torch bool start = mask = torch logical_and mask idx = start end = dim_size mask = torch logical_and mask idx end step = mask = torch logical_and mask idx - start step == mask_shape = input dim mask_shape dim = - mask = mask view mask_shape aten where mask aten _unsafe_masked_index src mask indices input register_decomposition aten select_backward out_wrapper select_backward grad_output Tensor input_sizes list int dim int index int grad_input = grad_output new_zeros input_sizes torch select_scatter grad_input grad_output dim index register_decomposition aten diagonal_backward out_wrapper diagonal_backward grad_output Tensor input_sizes list int offset int dim int dim int grad_input = grad_output new_zeros input_sizes torch diagonal_scatter grad_input grad_output offset dim dim _cast_grad_to_input_dtype grad_output Tensor grad_input Tensor input_dtype torch dtype grad_output dtype = input_dtype grad_input = grad_input input_dtype grad_input register_decomposition aten _softmax_backward_data out_wrapper grad_input compute_only_pw_cast_for_opmath _softmax_backward_data grad_output Tensor output Tensor dim int input_dtype torch dtype new_grad_output = grad_output output grad_input = new_grad_output - output torch sum new_grad_output dim=dim keepdim=True CPU kernel doesn t respect input_dtype following check doesn t work meta tensor grad_output device == torch device cpu grad_input contiguous _cast_grad_to_input_dtype grad_output grad_input input_dtype contiguous register_decomposition aten _log_softmax_backward_data out_wrapper compute_only_pw_cast_for_opmath _log_softmax_backward_data grad_output Tensor output Tensor dim int input_dtype torch dtype grad_input = grad_output - torch exp output torch sum grad_output dim=dim keepdim=True _cast_grad_to_input_dtype grad_output grad_input input_dtype _im col_col im_indices_along_dim input_d kernel_d dilation_d padding_d stride_d device Utility function implement im col col im blocks_d = input_d + padding_d - dilation_d kernel_d - arange_kw = partial torch arange dtype=torch int device=device Stride kernel over input find starting indices along dim d blocks_d_indices = arange_kw blocks_d stride_d unsqueeze Apply dilation kernel find its indices along dim d kernel_grid = arange_kw kernel_d dilation_d dilation_d unsqueeze - Broadcast add kernel starting positions indices kernel_grid along dim d get block indices along dim d blocks_d_indices + kernel_grid register_decomposition aten im col out_wrapper im col input Tensor kernel_size list int dilation list int padding list int stride list int - Tensor torch _check len kernel_size == lambda im col only D kernel supported torch _check len dilation == lambda im col only D dilation supported torch _check len padding == lambda im col only D padding supported torch _check len stride == lambda im col only D stride supported check_positive param param_name strict=True cond = all p p param strict all p = p param torch _check cond lambda f param_name should greater than zero got param check_positive kernel_size kernel_size check_positive dilation dilation check_positive dilation padding strict=False check_positive stride stride shape = input shape ndim = len shape torch _check ndim all d = d shape - lambda Expected D D batch mode tensor input possible batch size f non-zero dimensions got tuple shape output_size = tuple + out + pad - dil ker - - st out pad dil ker st zip shape - padding dilation kernel_size stride torch _check all c c output_size lambda f Given input spatial size tuple shape - f kernel_size= kernel_size dilation= dilation f padding= padding stride= stride calculated shape array sliding blocks f output_size its components must least one batched_input = ndim == batched_input input = input unsqueeze batch_dim channel_dim input_h input_w = input shape stride_h stride_w = stride padding_h padding_w = padding dilation_h dilation_w = dilation kernel_h kernel_w = kernel_size blocks_row_indices = _im col_col im_indices_along_dim input_h kernel_h dilation_h padding_h stride_h input device blocks_col_indices = _im col_col im_indices_along_dim input_w kernel_w dilation_w padding_w stride_w input device Note F pad takes padding_left padding_right padding_top padding_bottom ugh padded_input = F pad input padding_w padding_w padding_h padding_h blocks_row_indices = blocks_row_indices unsqueeze - unsqueeze - output = padded_input blocks_row_indices blocks_col_indices output = output permute num_blocks_row = blocks_row_indices size num_blocks_col = blocks_col_indices size output = output reshape batch_dim channel_dim kernel_h kernel_w num_blocks_row num_blocks_col batched_input output = output squeeze output register_decomposition aten col im out_wrapper pw_cast_for_opmath col im input Tensor output_size list int kernel_size list int dilation list int padding list int stride list int - Tensor torch _check len output_size == lambda only D output_size supported torch _check len kernel_size == lambda only D kernel supported torch _check len dilation == lambda only D dilation supported torch _check len padding == lambda only D padding supported torch _check len stride == lambda only D stride supported check_positive param param_name strict=True cond = all p p param strict all p = p param torch _check cond lambda f param_name should greater than zero got param check_positive kernel_size kernel_size check_positive dilation dilation check_positive padding padding strict=False check_positive stride stride check_positive output_size output_size shape = input shape ndim = len shape torch _check ndim all d = d shape - lambda Expected D D batch mode tensor input possible batch size f non-zero dimensions got tuple shape prod_kernel_size = kernel_size kernel_size torch _check shape - prod_kernel_size == lambda Expected size input s first non-batch dimension divisible f product kernel_size got input shape - = shape - f kernel_size= kernel_size col = + out + pad - dil ker - - st out pad dil ker st zip output_size padding dilation kernel_size stride L = col col torch _check shape - == L lambda f Given output_size= output_size kernel_size= kernel_size f dilation= dilation padding= padding stride= stride f expected input size - L got shape - torch _check L lambda f Given output_size= output_size kernel_size= kernel_size f dilation= dilation padding= padding stride= stride f expected input size - L got shape - batched_input = ndim == batched_input input = input unsqueeze shape = input shape out_h out_w = output_size stride_h stride_w = stride padding_h padding_w = padding dilation_h dilation_w = dilation kernel_h kernel_w = kernel_size col im defined backwards im col so we differentiate its decomposition hand input = input reshape shape shape prod_kernel_size + kernel_size + col input = input permute indices_row = _im col_col im_indices_along_dim out_h kernel_h dilation_h padding_h stride_h input device indices_row = _unsqueeze_to_dim indices_row indices_col = _im col_col im_indices_along_dim out_w kernel_w dilation_w padding_w stride_w input device output_padded_size = o + p o p zip output_size padding output = input new_zeros shape shape prod kernel_size + output_padded_size idx = None None indices_row indices_col output = aten _unsafe_index_put output idx input accumulate=True output = F pad output -padding_w -padding_w -padding_h -padding_h batched_input output = output squeeze output register_decomposition aten native_dropout_backward out_wrapper native_dropout_backward grad_output Tensor mask Tensor scale float According CUDA kernel implementation we should have test seems fail tests torch _check mask dtype == torch bool lambda f Mask should Bool Scalar Type mask dtype Mimicking CUDA kernel s behavior output stride output follow input s memory format This different TensorIterator s behavior r = grad_output mask type_as grad_output scale clone memory_format=utils suggest_memory_format grad_output r register_decomposition aten unfold_backward out_wrapper unfold_backward grad Tensor input_size list int dimension int size int step int - Tensor len input_size == torch squeeze_copy grad dim = utils canonicalize_dim len input_size dimension idx = torch arange input_size dim device=grad device dtype=torch int idx = idx unfold size step flatten grad = grad movedim - dim + flatten dim dim + nb At moment generates two kernels triton It could potentially fused into one call scatter_reduce case step = size provided scatter_reduce generates kernel grad_input = grad new_zeros input_size index = None dim + idx aten _unsafe_index_put grad_input index grad accumulate=True contiguous register_decomposition aten logit_backward default pw_cast_for_opmath logit_backward grad_output Tensor Tensor eps Optional float = None - Tensor eps None lo = eps hi = - lo torch where torch logical_and = lo = hi grad_output - torch where torch logical_and = = grad_output - new_full float nan register_decomposition aten dropout aten dropout default py_impl DispatchKey CompositeImplicitAutograd aten dropout default py_impl DispatchKey Autograd dropout input Tensor p float train Optional bool train p = aten native_dropout input p train input clone register_decomposition aten native_dropout out_wrapper out out native_dropout input Tensor p float train Optional bool train p = p == torch zeros_like input torch zeros_like input dtype=torch bool input dtype is_floating_point raise RuntimeError result type Float can t cast desired output type Long bool_mask = torch rand_like input p res = bool_mask input float - p res bool_mask input torch ones_like input dtype=torch bool register_decomposition aten _softmax out_wrapper _softmax x Tensor dim int half_to_float bool torch fx experimental symbolic_shapes guard_or_false eager softmax returns contiguous tensor Ensure decomp also returns contiguous tensor x = x contiguous half_to_float assert x dtype == torch half computation_dtype result_dtype = utils elementwise_dtypes x type_promotion_kind=utils ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT x = x computation_dtype guard_or_false x numel == unnormalized = torch exp x x_max = torch amax x dim keepdim=True unnormalized = torch exp x - x_max result = unnormalized torch sum unnormalized dim keepdim=True half_to_float result = result result_dtype result register_decomposition aten _log_softmax out_wrapper exact_dtype=True _log_softmax x Tensor dim int half_to_float bool torch fx experimental symbolic_shapes guard_or_false eager log_softmax returns contiguous tensor Ensure decomp also returns contiguous tensor x = x contiguous half_to_float assert x dtype == torch half computation_dtype result_dtype = utils elementwise_dtypes x type_promotion_kind=utils ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT x = x computation_dtype guard_or_false x numel == shifted = x x_max = torch amax x dim keepdim=True shifted = x - x_max shifted_logsumexp = torch log torch sum torch exp shifted dim keepdim=True result = shifted - shifted_logsumexp half_to_float result = result result_dtype result register_decomposition aten embedding out_wrapper embedding weight Tensor indices Tensor padding_idx int = - scale_grad_by_freq bool = False sparse bool = False - Tensor assert weight dim == weight must -D Nb scale_grad_by_freq used forward indices ndim = We need one weight indices calls item these cases out = weight index_select indices indices ndim == out = out squeeze out weight indices register_decomposition aten embedding_dense_backward out_wrapper embedding_dense_backward grad_output Tensor indices Tensor num_weights int padding_idx int scale_grad_by_freq bool computation_dtype result_dtype = utils elementwise_dtypes grad_output type_promotion_kind=utils ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT grad_output = grad_output computation_dtype indices = _maybe_convert_to_dtype indices torch long type ignore assignment scale_grad_by_freq counts = indices new_zeros num_weights ones = torch ones_like indices counts = aten _unsafe_index_put counts indices ones accumulate=True grad_weights_scale = counts indices grad_output = grad_output grad_weights_scale unsqueeze - mask = _unsqueeze_to_dim indices == padding_idx grad_output ndim grad = grad_output masked_fill mask grad_weight = grad_output new_zeros num_weights + grad_output shape indices ndim aten _unsafe_index_put grad_weight indices grad accumulate=True result_dtype prod x list int r = i x r = i r _pad_chunk tensors list Tensor dim int num_chunks int - list Tensor padded_tensors = tensor tensors tensor_size = tensor size pad_along_dim = tensor_size dim + num_chunks - num_chunks num_chunks pad_along_dim = tensor_size dim Use aten constant_pad_nd instead copy_ functionalization pad = tensor ndim - dim - + pad_along_dim - tensor_size dim tensor = aten constant_pad_nd tensor pad view_size = tensor_size dim + torch Size num_chunks - padded_tensors append tensor reshape view_size padded_tensors have_same_ndims tensors list Tensor ndim = tensors ndim tensor tensors tensor ndim = ndim False True leading_dimension_matches tensors list Tensor dim int leading_dim_sizes = tensors size dim tensor tensors torch _check tensor size dim == leading_dim_sizes lambda _chunk_cat expects same sizes dim- dimensions all tensors _preprocess_chunk_cat_inputs tensors list Tensor dim int num_chunks int torch _check num_chunks = lambda _chunk_cat expects positive num_chunks torch _check len tensors lambda _chunk_cat expects non-empty input tensor list expected_dtype = tensors dtype expected_device = tensors device tensor tensors torch _check tensor numel lambda _chunk_cat expects non-empty tensor torch _check tensor dtype == expected_dtype lambda _chunk_cat expects all input tensors same dtype torch _check tensor device == expected_device lambda _chunk_cat expects all inputs tensors same device have_same_ndims tensors dim = utils canonicalize_dim tensors dim dim torch _check dim = lambda _chunk_cat expects non-negative dim when input tensors have different ndims tensor tensors torch _check dim tensor ndim lambda _chunk_cat expects dim ndim all input tensors leading_dimension_matches tensors dim dim register_decomposition aten _chunk_cat default aten _chunk_cat out _chunk_cat tensors list Tensor dim int num_chunks int out Optional Tensor = None - Tensor dim = _preprocess_chunk_cat_inputs tensors dim num_chunks padded_tensors = _pad_chunk tensors dim num_chunks out None torch cat padded_tensors dim + torch cat padded_tensors dim + out=out out out_wrapper currently does allow optional outputs register_decomposition aten split_with_sizes_copy default aten split_with_sizes_copy out split_with_sizes_copy Tensor split_sizes list int dim int = out Optional list Tensor = None - Optional list Tensor splits = aten split_with_sizes split_sizes dim=dim out None s clone memory_format=torch contiguous_format s splits output split zip out splits _maybe_resize_out output split shape _safe_copy_out copy_from=split copy_to=output exact_dtype=True None register_decomposition aten unsafe_split Tensor unsafe_split input Tensor split_size int dim int = - tuple Tensor aten split Tensor input split_size dim register_decomposition aten unsafe_split_with_sizes default unsafe_split_with_sizes input Tensor split_sizes list int dim int = - tuple Tensor aten split_with_sizes default input split_sizes dim register_decomposition aten split Tensor split Tensor split_size int dim int = - tuple Tensor input_sizes = shape dim_size = input_sizes dim split_size == assert dim_size == detach chunks = dim_size + split_size - split_size Avoid importing sympy module level torch fx experimental symbolic_shapes guard_int chunks = guard_int chunks split_sizes = split_size i range chunks split_sizes - = split_size - split_size chunks - dim_size torch split split_sizes dim aten tensor_split tensor_indices_or_sections py_impl DispatchKey CompositeImplicitAutograd tensor_split_tensor_indices_or_sections_py_impl Tensor tensor_indices_or_sections Tensor dim int = - tuple Tensor assert tensor_indices_or_sections device type == cpu assert tensor_indices_or_sections dtype == torch int split_dim = tensor_indices_or_sections dim torch _check split_dim == split_dim == lambda tensor_split expected tensor_indices_or_sections zero-dimensional f one-dimensional tensor got tensor split_dim dims split_dim == sections = tensor_indices_or_sections item assert isinstance sections IntLike tensor_split sections dim ctx = nullcontext fake_mode = torch _guards detect_fake_mode shape_env = fake_mode shape_env ctx = shape_env ignore_fresh_unbacked_symbols type ignore assignment In fake tensor prop we end up calling slice these unbacked indices Because slice has flexible semantics unbacked handling generates new output sizes each slice effectively clobbering over these index symbols To avoid PendingUnbackedSymbolNotFound errors we tell compiler s fine bind these ctx indices = i item i tensor_indices_or_sections WARNING Tempted torch _check x indices here You can t tensor_split works negative values indices torch tensor_split torch randn torch tensor - tensor - tensor tensor - - Sorry I don t make rules Explicitly do item call user code you KNOW they non-negative tensor_split indices dim TODO doesn t appear have enough precision bfloat register_decomposition aten addmm out_wrapper exact_dtype=True pw_cast_for_opmath addmm Tensor mat Tensor mat Tensor beta int = alpha int = is_floating_point is_complex beta = int beta alpha = int alpha out = alpha torch mm mat mat beta == out The output aten addmm contiguous we need match behavior decomposition The original implementation beta + out would strided tensor ` ` strided We thus use ` out ` output torch mm which always contiguous first argument addition This relying TensorIterator s behavior takes higher precedence stride first input Alternative we can write ` beta + out contiguous ` introduces another copy some cases This implementation ideal we should revisit when we have better solution out + beta register_decomposition aten _addmm_activation out_wrapper pw_cast_for_opmath _addmm_activation Tensor mat Tensor mat Tensor beta int = alpha int = use_gelu bool = False out = addmm mat mat beta alpha use_gelu is_cuda aten gelu out approximate= tanh aten gelu out aten relu out register_decomposition aten addmv out_wrapper exact_dtype=True pw_cast_for_opmath addmv Tensor mat Tensor vec Tensor beta int = alpha int = is_floating_point is_complex beta = int beta alpha = int alpha out = alpha torch mv mat vec beta == out out numel == handle empty matrix beta out + beta register_decomposition aten native_group_norm_backward default pw_cast_for_opmath native_group_norm_backward grad_output Tensor input Tensor mean Tensor rstd Tensor gamma Optional Tensor N int C int HxW int group int output_mask list bool - tuple Optional Tensor Optional Tensor Optional Tensor utils check_same_device grad_output input mean rstd allow_cpu_scalar_tensors=False utils check_same_shape input grad_output allow_cpu_scalar_tensors=False utils check_same_shape mean rstd allow_cpu_scalar_tensors=False torch _check input numel == N C HxW lambda f Expect input have N C HxW elements torch _check mean shape == N group lambda f Expect mean have shape N group got mean shape torch _check gamma None gamma numel == C lambda f Expect gamma have C elements got gamma numel gamma None - cpg = C group torch _check C == cpg group lambda f Expect number channels C evenly-divisible number groups group Compute Internal gradients ds = torch mul grad_output input view N C HxW sum dim= db = grad_output view N C HxW sum dim= d_input Optional Tensor = None d_gamma Optional Tensor = None d_bias Optional Tensor = None output_mask s = HxW cpg gamma None ds_val = torch mul ds gamma unsqueeze reshape N group cpg sum db_val = torch mul db gamma unsqueeze reshape N group cpg sum c = torch mul rstd unsqueeze - gamma reshape group cpg ds_val = ds reshape N group cpg sum db_val = db reshape N group cpg sum c = torch mul rstd unsqueeze - torch ones group cpg device=rstd device c = db_val mean - ds_val rstd rstd rstd s c = -c mean - db_val rstd s c = c unsqueeze - c = _unsqueeze_to_dim c c = _unsqueeze_to_dim c d_input = torch mul grad_output reshape N group cpg HxW c + torch mul input reshape N group cpg HxW c + c d_input = d_input reshape input shape input dtype output_mask d_gamma = ds view N group cpg - db view N group cpg mean unsqueeze - rstd unsqueeze - sum dim= reshape C output_mask d_bias = db sum dim= d_input d_gamma d_bias out_wrapper currently does allow optional outputs register_decomposition aten native_group_norm_backward out native_group_norm_backward_out grad_output Tensor input Tensor mean Tensor rstd Tensor gamma Optional Tensor N int C int HxW int group int output_mask list bool out torch Tensor out torch Tensor out torch Tensor - tuple Optional Tensor Optional Tensor Optional Tensor result = native_group_norm_backward grad_output input mean rstd gamma N C HxW group output_mask grad_input = out out out i r enumerate result r None _maybe_resize_out grad_input i r shape _safe_copy_out copy_from=r copy_to=grad_input i exact_dtype=True grad_input _maybe_cast x Optional Tensor dtype - Optional Tensor x None x dtype x TODO Take closer look type promotion semantics register_decomposition aten native_layer_norm_backward default native_layer_norm_backward grad_out Tensor input Tensor normalized_shape list int mean Tensor rstd Tensor weight Optional Tensor bias Optional Tensor output_mask list bool - tuple Optional Tensor Optional Tensor Optional Tensor input_shape = input shape input_ndim = input dim computation_dtype = utils get_computation_dtype input dtype grad_out_cast input_cast weight_cast bias_cast = x computation_dtype memory_format=torch contiguous_format x None x x grad_out input weight bias assert grad_out_cast None axis = input_ndim - len normalized_shape inner_dims = input_shape axis outer_dims = input_shape axis inner_dim_indices list int = outer_dim_indices list int = i range input_ndim i = axis inner_dim_indices append i outer_dim_indices append i N = prod inner_dims type ignore arg-type M = prod outer_dims type ignore arg-type torch fx experimental symbolic_shapes statically_known_true statically_known_true M == statically_known_true N == input new_zeros input_shape output_mask None input new_zeros input_shape axis output_mask None input new_zeros input_shape axis output_mask None mean = _unsqueeze_to_dim mean input_cast dim type ignore union-attr rstd = _unsqueeze_to_dim rstd input_cast dim type ignore union-attr assert input_cast None x_hat = input_cast - mean rstd weight_cast None grad_x_hat = grad_out_cast weight_cast grad_x_hat = grad_out_cast = grad_x_hat N b = torch sum grad_x_hat inner_dim_indices True c = torch mul grad_x_hat x_hat c = torch sum c inner_dim_indices True c = torch mul x_hat c inner = - b - c d_input Optional Tensor = None d_weight Optional Tensor = None d_bias Optional Tensor = None output_mask d_input = rstd N inner output_mask weight_cast None len outer_dim_indices d_weight = torch sum grad_out_cast x_hat outer_dim_indices False d_weight = grad_out_cast x_hat output_mask bias_cast None len outer_dim_indices d_bias = torch sum grad_out_cast outer_dim_indices False d_bias = grad_out_cast clone _maybe_cast d_input input dtype _maybe_cast d_weight weight dtype weight None None _maybe_cast d_bias bias dtype bias None None out_wrapper currently does allow optional outputs register_decomposition aten native_layer_norm_backward out native_layer_norm_backward_out grad_out Tensor input Tensor normalized_shape list int mean Tensor rstd Tensor weight Optional Tensor bias Optional Tensor output_mask list bool out torch Tensor out torch Tensor out torch Tensor - tuple Optional Tensor Optional Tensor Optional Tensor result = native_layer_norm_backward grad_out input normalized_shape mean rstd weight bias output_mask grad_input = out out out i r enumerate result r None _maybe_resize_out grad_input i r shape _safe_copy_out copy_from=r copy_to=grad_input i exact_dtype=True grad_input register_decomposition aten _fused_rms_norm_backward default _fused_rms_norm_backward grad_out Tensor input Tensor normalized_shape list int rstd Tensor weight Optional Tensor output_mask list bool - tuple Optional Tensor Optional Tensor input_shape = input shape input_ndim = input dim computation_dtype = utils get_computation_dtype input dtype grad_out_cast = grad_out computation_dtype memory_format=torch contiguous_format input_cast = input computation_dtype memory_format=torch contiguous_format weight_cast = weight computation_dtype memory_format=torch contiguous_format weight None None assert grad_out_cast None axis = input_ndim - len normalized_shape inner_dims = input_shape axis outer_dims = input_shape axis inner_dim_indices list int = outer_dim_indices list int = i range input_ndim i = axis inner_dim_indices append i outer_dim_indices append i N = prod inner_dims type ignore arg-type M = prod outer_dims type ignore arg-type torch fx experimental symbolic_shapes guard_or_false guard_or_false M == guard_or_false N == input new_zeros input_shape output_mask None input new_zeros input_shape axis output_mask None rstd = _unsqueeze_to_dim rstd input_cast dim type ignore union-attr weight_cast None grad_x_hat = grad_out_cast weight_cast grad_x_hat = grad_out_cast d_input Optional Tensor = None d_weight Optional Tensor = None x_hat = input_cast rstd output_mask sum_val = torch sum x_hat grad_x_hat dim=inner_dim_indices keepdim=True d_input = grad_x_hat - x_hat N sum_val rstd output_mask weight_cast None d_weight_full_shape = grad_out_cast x_hat len outer_dim_indices d_weight = torch sum d_weight_full_shape dim=outer_dim_indices keepdim=False d_weight = d_weight_full_shape _maybe_cast d_input input dtype _maybe_cast d_weight input dtype native_batch_norm_helper input Tensor weight Optional Tensor bias Optional Tensor running_mean Optional Tensor running_var Optional Tensor training bool momentum float eps float functional bool - tuple Tensor Tensor Tensor Optional Tensor Optional Tensor reduction_dims = + list range input dim computation_dtype = utils get_computation_dtype input dtype new_running_mean = running_mean new_running_var = running_var training computation_dtype = utils get_computation_dtype input dtype input_acc = input dtype=computation_dtype biased_var mean = torch var_mean input_acc dim=reduction_dims correction= keepdim=True rstd = torch rsqrt biased_var + eps output = input - mean rstd save_mean = torch squeeze mean reduction_dims save_rstd = torch squeeze rstd reduction_dims running_mean None new_running_mean = momentum save_mean + - momentum running_mean functional running_mean copy_ new_running_mean running_var None n = input numel input shape This doesn t strictly match eager s numerics which accumulates var sum then directly applies correction But would require re-implementing var here negligible numerics gain tensor whose numerics probably don t matter squeezed_var = torch squeeze biased_var reduction_dims unbiased_var = squeezed_var n n - new_running_var = momentum unbiased_var + - momentum running_var functional running_var copy_ new_running_var assert running_mean None running_var None running_mean = running_mean dtype=computation_dtype copy=True new_running_mean = running_mean running_var = running_var dtype=computation_dtype copy=True new_running_var = running_var mean = running_mean invstd = torch sqrt running_var + eps Very annoying inconsistency where CPU CUDA give different shapes input device type = cpu save_mean = running_mean save_rstd = invstd save_mean = input new_zeros save_rstd = input new_zeros mean = _unsqueeze_to_dim mean input dim - invstd = _unsqueeze_to_dim invstd input dim - output = input - mean invstd weight None weight = weight flatten weight = _unsqueeze_to_dim weight input dim - output = output weight bias None bias = bias flatten bias = _unsqueeze_to_dim bias input dim - output = output + bias input device type == cpu save_mean = save_mean dtype=input dtype save_rstd = save_rstd dtype=input dtype output dtype=input dtype save_mean save_rstd new_running_mean new_running_var register_decomposition aten native_batch_norm out_wrapper out save_mean save_invstd native_batch_norm input Tensor weight Optional Tensor bias Optional Tensor running_mean Optional Tensor running_var Optional Tensor training bool momentum float eps float - tuple Tensor Tensor Tensor output save_mean save_rstd _ _ = native_batch_norm_helper input weight bias running_mean running_var training momentum eps False output save_mean save_rstd TODO decomposition NOT here stay We would much prefer replacing native_batch_norm our new correctly schema d _native_batch_norm_legit its variants we cannot do immediately C++ because would forwards incompatible some mobile use cases Since change most impactful aot autograd functionalization we simply register decomposition Autograd key python dispatcher which currently only used aot autograd functionalization no one really In two weeks so we should remove decomposition phase out current native_batch_norm _native_batch_norm_legit have right schema stating there input mutations aten native_batch_norm default py_impl DispatchKey Autograd aten native_batch_norm default py_impl DispatchKey CompositeImplicitAutograd native_batch_norm_decomposition input Tensor weight Optional Tensor bias Optional Tensor running_mean Optional Tensor running_var Optional Tensor training bool momentum float eps float - tuple Tensor Tensor Tensor running_mean None running_var None aten _native_batch_norm_legit input weight bias training momentum eps running_mean None raise RuntimeError running_mean None running_var provided They should both None both provided running_var None raise RuntimeError running_var None running_mean provided They should both None both provided training HACK batch norm consolidation should clean up so op doesn t take training arg aten _native_batch_norm_legit input weight bias running_mean running_var training momentum eps aten _native_batch_norm_legit_no_training input weight bias running_mean running_var momentum eps aten unsafe_chunk default py_impl DispatchKey CompositeImplicitAutograd unsafe_chunk_py_impl tensor chunks dim= - list Tensor dim_size = tensor size dim split_size = dim_size + chunks - chunks split_size == dim_size == split_sizes = split_size _ chunks split_sizes chunks - = split_size - split_size chunks - dim_size torch ops aten unsafe_split_with_sizes default tensor split_sizes dim torch ops aten unsafe_split Tensor tensor split_size dim register_decomposition aten _native_batch_norm_legit_no_training default _native_batch_norm_legit_no_training input Tensor weight Optional Tensor bias Optional Tensor running_mean Tensor running_var Tensor momentum float eps float - tuple Tensor Tensor Tensor aten _native_batch_norm_legit default input weight bias running_mean running_var False training momentum eps register_decomposition aten _native_batch_norm_legit default _native_batch_norm_legit input Tensor weight Optional Tensor bias Optional Tensor running_mean Tensor running_var Tensor training bool momentum float eps float - tuple Tensor Tensor Tensor output save_mean save_rstd _ _ = native_batch_norm_helper input weight bias running_mean running_var training momentum eps False output save_mean save_rstd register_decomposition aten _native_batch_norm_legit no_stats _native_batch_norm_legit_no_stats input Tensor weight Optional Tensor bias Optional Tensor training bool momentum float eps float - tuple Tensor Tensor Tensor output save_mean save_rstd _ _ = native_batch_norm_helper input weight bias None None training momentum eps False output save_mean save_rstd register_decomposition aten _native_batch_norm_legit_functional default _native_batch_norm_legit_functional input Tensor weight Optional Tensor bias Optional Tensor running_mean Tensor running_var Tensor training bool momentum float eps float - tuple Tensor Tensor Tensor Tensor Tensor output save_mean save_rstd new_running_mean new_running_var = native_batch_norm_helper input weight bias running_mean running_var training momentum eps True assert new_running_mean None new_running_mean should None assert new_running_var None new_running_var should None output save_mean save_rstd new_running_mean new_running_var _get_batch_norm_reserve_tensor input Tensor weight Optional Tensor bias Optional Tensor running_mean Tensor running_var Tensor eps float training bool - Tensor Return reserve tensor batch norm used only cudnn pass forward state backward pass This needed ` _batch_norm_with_update ` ` _batch_norm_no_update ` which support variety backends including cudnn We create tensor here get correct shape traced graph we detect will call cudnn kernel rely DCE avoid materializing tensor backend = torch _C _select_batch_norm_backend type ignore attr-defined input weight bias running_mean running_var True eps reserve_size = backend == torch _C _BatchNormBackend Cudnn type ignore attr-defined reserve_size = torch _C _get_cudnn_batch_norm_reserve_space_size type ignore attr-defined input training torch empty reserve_size dtype=torch uint layout=input layout device=input device register_decomposition aten _batch_norm_with_update default _batch_norm_with_update input Tensor weight Optional Tensor bias Optional Tensor running_mean Tensor running_var Tensor momentum float eps float - tuple Tensor Tensor Tensor Tensor output save_mean save_rstd _ _ = native_batch_norm_helper input weight bias running_mean running_var True training momentum eps False functional reserve = _get_batch_norm_reserve_tensor input weight bias running_mean running_var eps training=True output save_mean save_rstd reserve register_decomposition aten _batch_norm_with_update_functional default _batch_norm_with_update_functional input Tensor weight Optional Tensor bias Optional Tensor running_mean Tensor running_var Tensor momentum float eps float - tuple Tensor Tensor Tensor Tensor Tensor Tensor output save_mean save_rstd new_rm new_rv = native_batch_norm_helper input weight bias running_mean running_var True momentum eps True reserve = _get_batch_norm_reserve_tensor input weight bias running_mean running_var eps training=True assert new_rm None new_running_mean should None assert new_rv None new_running_var should None output save_mean save_rstd reserve new_rm new_rv register_decomposition aten _batch_norm_no_update default _batch_norm_no_update input Tensor weight Optional Tensor bias Optional Tensor running_mean Tensor running_var Tensor momentum float eps float - tuple Tensor Tensor Tensor Tensor output save_mean save_rstd _ _ = native_batch_norm_helper input weight bias running_mean running_var False training momentum eps False functional reserve = _get_batch_norm_reserve_tensor input weight bias running_mean running_var eps training=False output save_mean save_rstd reserve register_decomposition aten _fused_dropout out_wrapper out out pw_cast_for_opmath _fused_dropout_decomposition input p generator=None assert generator None mask = torch rand_like input p dtype=torch uint res = mask type_as input input p res mask register_decomposition aten _to_copy out_wrapper _to_copy x Union Tensor NumberType dtype Optional torch dtype = None layout=None device Optional torch device = None pin_memory bool = False non_blocking bool = False memory_format Optional torch memory_format = None assert layout layout == torch strided TODO assert pin_memory TODO assert isinstance x torch Tensor int float bool complex device None dtype None memory_format None isinstance x torch Tensor x clone x dtype_converted = False isinstance x torch Tensor x_tensor = x x_tensor = torch scalar_tensor x device None device = x_tensor device avoid conversions cpu dtype None device type == cpu x_tensor = torch _prims convert_element_type x_tensor dtype dtype_converted = True x_tensor = torch _prims device_put x_tensor device non_blocking dtype None dtype_converted x_tensor = torch _prims convert_element_type x_tensor dtype dtype_converted = True memory_format None no ref prim memory format torch clone x_tensor memory_format=memory_format x_tensor Questionable decompositions This only valid we re running graph without autograd such backward pass has been traced Note decomposition causes issues in-place ops register_decomposition aten detach aten lift aten lift_fresh out_wrapper nop_decomposition x aten alias x Also register Autograd dispatch key so decomp can run above autograd native_batch_norm needs decompose into other ops before autograd aten cudnn_batch_norm default py_impl DispatchKey Autograd register_decomposition aten cudnn_batch_norm out_wrapper out out out out cudnn_batch_norm input Tensor weight Tensor bias Optional Tensor running_mean Optional Tensor running_var Optional Tensor training bool exponential_average_factor float epsilon float b c = aten native_batch_norm input weight bias running_mean running_var training exponential_average_factor epsilon Cudnn running mean variance when training True training b c input new_zeros dtype=torch uint weight new_zeros weight new_zeros input new_zeros dtype=torch uint _broadcast_batch_norm_backward x broadcast_mask axis mask enumerate broadcast_mask mask == axis x ndim x shape axis == mask x = x unsqueeze axis x register_decomposition aten batch_norm_backward default batch_norm_backward grad_out Tensor input Tensor weight Optional Tensor running_mean Optional Tensor running_var Optional Tensor save_mean Optional Tensor save_invstd Optional Tensor train bool eps float output_mask list bool reserve Tensor - tuple Tensor Optional Tensor Optional Tensor native_batch_norm_backward grad_out input weight running_mean running_var save_mean save_invstd train eps output_mask register_decomposition aten native_batch_norm_backward default native_batch_norm_backward grad_out Tensor input Tensor weight Optional Tensor running_mean Optional Tensor running_var Optional Tensor save_mean Optional Tensor save_invstd Optional Tensor train bool eps float output_mask list bool - tuple Tensor Optional Tensor Optional Tensor input_dtype = input dtype weight None weight_dtype = weight dtype weight_dtype = input_dtype computation_dtype = utils get_computation_dtype input dtype grad_out_cast input_cast weight_cast running_mean_cast running_var_cast save_mean_cast save_invstd_cast = x computation_dtype x None x x grad_out input weight running_mean running_var save_mean save_invstd input_shape = input shape input_rank = input dim assert input_rank = rank input must least axis = num_features = prod list input_shape input_shape axis mean = save_mean_cast invstd = save_invstd_cast train assert mean None invstd None assert running_mean_cast None running_var_cast None mean = running_mean_cast invstd = torch rsqrt running_var_cast + eps broadcast_mask list int = input_rank broadcast_mask axis = input_shape axis reduction_axes list int = i range input_rank i = axis reduction_axes append i mean = _broadcast_batch_norm_backward mean broadcast_mask type ignore arg-type norm = num_features grad_output_sum = torch sum grad_out_cast reduction_axes type ignore arg-type dot_p = torch sum grad_out_cast input_cast - mean reduction_axes type ignore operator grad_mean = _broadcast_batch_norm_backward grad_output_sum norm broadcast_mask proj_scale = _broadcast_batch_norm_backward torch mul dot_p norm invstd invstd type ignore operator broadcast_mask weight_cast None grad_scale = _broadcast_batch_norm_backward invstd broadcast_mask type ignore arg-type grad_scale = _broadcast_batch_norm_backward invstd weight_cast broadcast_mask train proj = input_cast - mean proj_scale type ignore operator grad_input = grad_out_cast - proj - grad_mean grad_scale grad_input = grad_out_cast grad_scale output_mask grad_weight = dot_p invstd grad_weight = None None doesn t work vjp should use zeros vjp output_mask grad_bias = grad_output_sum grad_bias = None None doesn t work vjp should use zeros vjp grad_input input_dtype _maybe_cast grad_weight weight_dtype _maybe_cast grad_bias weight_dtype out_wrapper currently does allow optional outputs register_decomposition aten native_batch_norm_backward out native_batch_norm_backward_out grad_out Tensor input Tensor weight Optional Tensor running_mean Optional Tensor running_var Optional Tensor save_mean Optional Tensor save_invstd Optional Tensor train bool eps float output_mask list bool out torch Tensor out torch Tensor out torch Tensor - tuple Tensor Optional Tensor Optional Tensor result = native_batch_norm_backward grad_out input weight running_mean running_var save_mean save_invstd train eps output_mask grad_input = out out out i r enumerate result r None _maybe_resize_out grad_input i r shape _safe_copy_out copy_from=r copy_to=grad_input i exact_dtype=True grad_input register_decomposition aten miopen_batch_norm_backward out_wrapper out out out miopen_batch_norm_backward input Tensor grad_output Tensor weight Tensor running_mean Optional Tensor running_var Optional Tensor save_mean Optional Tensor save_var Optional Tensor epsilon float aten native_batch_norm_backward grad_output input weight running_mean running_var save_mean save_var True epsilon True True True register_decomposition aten cudnn_batch_norm_backward out_wrapper out out out cudnn_batch_norm_backward input Tensor grad_output Tensor weight Tensor running_mean Optional Tensor running_var Optional Tensor save_mean Optional Tensor save_var Optional Tensor epsilon float reserveSpace Tensor aten native_batch_norm_backward grad_output input weight running_mean running_var save_mean save_var True epsilon True True True register_decomposition aten _adaptive_avg_pool d out_wrapper pw_cast_for_opmath adaptive_avg_pool d input Tensor output_size tuple int int Preconditions device = input device shape = input shape ndim = len shape torch _check ndim lambda f adaptive_avg_pool d Expected D D tensor got ndim d input shape - torch _check d = lambda adaptive_avg_pool d Expected input have non-zero size f non-batch dimensions input has shape tuple shape Optimisation we should also do kernel implementation shape - output_size - == shape - output_size - == stride = tuple i o i o zip shape - output_size kernel = tuple i - o - s i o s zip shape - output_size stride torch nn functional avg_pool d input kernel stride start_index b c torch div c b rounding_mode= trunc end_index b c torch div + c + b - b rounding_mode= trunc compute_idx in_size out_size orange = torch arange out_size device=device dtype=torch int i = start_index orange out_size in_size Let length = end_index - start_index i e length pooling kernels length max can computed analytically follows maxlength = in_size out_size + in_size_mod = in_size out_size adaptive = True iff there kernels different lengths adaptive = in_size_mod == out_size in_size_mod == adaptive maxlength += in_size_mod == maxlength -= range_max = torch arange maxlength device=device dtype=torch int idx = i unsqueeze - + range_max adaptive Need clamp avoid accessing out-of-bounds memory TODO make minimum accept scalars maxval = torch scalar_tensor in_size - dtype=idx dtype device=idx device idx = torch minimum idx maxval Compute length i = end_index orange out_size in_size length = i - i length = maxlength idx length range_max adaptive length None s constant otherwise we ll need compute idxh length_h range_max_h adaptive_h = compute_idx shape - output_size - idxw length_w range_max_w adaptive_w = compute_idx shape - output_size - vals = input _unsqueeze_to_dim idxh idxw Shortcut simpler case adaptive_h adaptive_w torch mean vals dim= - - maybe_mask vals length range_max adaptive dim isinstance length IntLike vals length zero-out things we didn t really want select assert dim hack mask = range_max = length unsqueeze - dim == - mask = _unsqueeze_to_dim mask vals = torch masked_fill vals mask Compute length each window length = _unsqueeze_to_dim length -dim vals length vals length_h = maybe_mask vals length_h range_max_h adaptive=adaptive_h dim=- vals length_w = maybe_mask vals length_w range_max_w adaptive=adaptive_w dim=- We unroll sum we assume kernels going small ret = None i j product range vals shape - range vals shape - ret None ret = vals i j ret = ret + vals i j ret length_h length_w _max_unpoolnd TensorLike indices TensorLike output_size list int dim int If input tensors indices came max_pool call required documentation operation deterministic because ensures there two entries ` indices ` tensor equal corresponding values ` ` also equal If condition satisfied operation non-deterministic one different values ` ` wins utils alert_not_deterministic f max_unpooling dim d_forward_out nc = reduce operator mul shape -dim hw = reduce operator mul output_size indices_nc_shape = ndim indices_nc_shape -dim = shape -dim indices_flat = indices + aten arange nc device=self device view indices_nc_shape hw reshape - output = new_zeros list shape -dim + list output_size aten _unsafe_index_put output reshape - indices_flat reshape - accumulate=False view output shape register_decomposition aten max_unpool d out_wrapper max_unpool d TensorLike indices TensorLike output_size list int torch _check indices dtype == torch int lambda f elements indices should type int got indices dtype torch _check len output_size == lambda f There should exactly two elements height width output_size f got len output_size elements torch _check ndim lambda f Input max_unpooling d should d d Tensor f got tensor ndim dimensions torch _check shape == indices shape lambda f Expected shape indices same input tensor shape f got indices tensor shape indices shape i range ndim torch _check size i lambda f max_unpooling d f Expected input have non-zero size non-batch dimensions f got shape dimension i being empty _max_unpoolnd indices output_size register_decomposition aten max_unpool d out_wrapper max_unpool d input TensorLike indices TensorLike output_size list int stride list int padding list int torch _check indices dtype == torch int lambda elements indices should type int torch _check input ndim lambda f Input max_unpooling d should d d Tensor got tensor input ndim dimensions torch _check len output_size == lambda f There should exactly three elements depth height width output_size f got len output_size elements torch _check len stride == lambda f There should exactly three elements depth height width stride got len stride elements torch _check len padding == lambda f There should exactly three elements depth height width padding got len padding elements torch _check input shape == indices shape lambda f Expected shape indices same input tensor input shape f got indices tensor shape indices shape i range input ndim torch _check input size i lambda f max_unpooling d f Expected input have non-zero size non-batch dimensions f got input shape dimension i being empty torch _check stride stride stride lambda f strides should greater than zero got stride stride _max_unpoolnd input indices output_size register_decomposition aten index_add_ index_add_ x TensorLike dim int index TensorLike tensor TensorLike alpha NumberType = _index_add x dim index tensor inplace=True alpha=alpha register_decomposition aten index_add out_wrapper index_add x TensorLike dim int index TensorLike tensor TensorLike alpha NumberType = _index_add x dim index tensor inplace=False alpha=alpha _index_add x TensorLike dim int index TensorLike tensor TensorLike inplace bool alpha NumberType = dim = utils canonicalize_dims x ndim dim torch _check index ndim = lambda f Index should have dimension got index ndim index_size = index size index ndim == tensor_size = tensor size dim tensor ndim torch _check tensor_size == index_size lambda f Number indices index_size should equal tensor size dim tensor_size dim= alpha = python_type = utils dtype_to_type x dtype torch _check python_type bool utils is_weakly_lesser_type type alpha python_type lambda f alpha argument type type alpha cannot safely cast type python_type tensor = tensor alpha Treat scalars elements \R^ zero_dim = x ndim == x = x unsqueeze zero_dim x idx = None dim + index index_put = aten index_put_ inplace aten index_put out = index_put x idx tensor accumulate=True inplace x out squeeze zero_dim out contiguous register_decomposition aten pad_sequence default aten pad_sequence default py_impl DispatchKey CompositeImplicitAutograd pad_sequence sequences batch_first=False padding_value= torch _check len sequences lambda received empty list sequences sequences_size = len sequences max_size = sequences size trailing_dims = max_size max_len = max x size x sequences batch_first out_dims = sequences_size max_len out_dims = max_len sequences_size out_dims = out_dims + trailing_dims out = sequences new_full out_dims padding_value dim_paddings = len trailing_dims i range sequences_size currseq = sequences i row = aten constant_pad_nd currseq dim_paddings + max_len - currseq size padding_value batch_first out = aten select_scatter out row dim= index=i out = aten select_scatter out row dim= index=i out register_decomposition aten index_copy_ index_copy_ x TensorLike dim int index TensorLike tensor TensorLike _index_copy x dim index tensor inplace=True register_decomposition aten index_copy out_wrapper index_copy x TensorLike dim int index TensorLike tensor TensorLike _index_copy x dim index tensor inplace=False _index_copy x TensorLike dim int index TensorLike tensor TensorLike inplace bool dim = utils canonicalize_dims x ndim dim torch _check index ndim = lambda f Index should have dimension got index ndim Treat scalars elements \R^ zero_dim = x ndim == x = x unsqueeze zero_dim x index = index unsqueeze index ndim == index idx = None dim + index index_put = aten index_put_ inplace aten index_put out = index_put x idx tensor inplace x out squeeze zero_dim out contiguous nb Should use acc_t op_math register_decomposition aten log_sigmoid_forward out_wrapper output buffer pw_cast_for_opmath log_sigmoid_forward Tensor - tuple Tensor Tensor min = torch minimum new_zeros z = torch exp -torch abs is_cuda is_xpu buffer = new_zeros buffer = z min - torch log p z buffer register_decomposition aten uniform out_wrapper uniform x Tensor low Union bool int float = high Union bool int float = generator Optional torch Generator = None prims _uniform_helper x shape low=sym_float low high=sym_float high dtype=x dtype device=x device generator=generator register_decomposition aten uniform_ uniform_ low= high= generator=None copy_ uniform low high generator aten src ATen native UpSample cpp compute_output_size upsample_compute_output_size input_size output_size scale_factors spatial_dimensions = len input_size - output_size None torch _check scale_factors None lambda Must specify exactly one output_size scale_factors torch _check len output_size == spatial_dimensions lambda output_size scale_factors None NB isn t necessary lol torch _check output_size None lambda Must specify exactly one output_size scale_factors torch _check len scale_factors == spatial_dimensions lambda output_size = i s enumerate scale_factors int s == s output_size append input_size i + int s output_size append sym_int input_size i + s output_size torch _check False lambda Must specify exactly one output_size scale_factors get_scale_value scales idx scales None None scales idx register_decomposition aten upsample_nearest d vec register_decomposition aten upsample_nearest d vec register_decomposition aten upsample_nearest d vec aten upsample_nearest d vec py_impl DispatchKey CompositeImplicitAutograd aten upsample_nearest d vec py_impl DispatchKey Autograd aten upsample_nearest d vec py_impl DispatchKey CompositeImplicitAutograd aten upsample_nearest d vec py_impl DispatchKey Autograd aten upsample_nearest d vec py_impl DispatchKey CompositeImplicitAutograd aten upsample_nearest d vec py_impl DispatchKey Autograd _upsample_nearest_vec input Tensor output_size Optional list int scale_factors Optional list float - Tensor osize = upsample_compute_output_size input size output_size scale_factors scales = scale_factors scale_factors None len osize type ignore list-item _upsample_nearest input osize scales register_decomposition aten _upsample_nearest_exact d vec register_decomposition aten _upsample_nearest_exact d vec register_decomposition aten _upsample_nearest_exact d vec aten _upsample_nearest_exact d vec py_impl DispatchKey CompositeImplicitAutograd aten _upsample_nearest_exact d vec py_impl DispatchKey Autograd aten _upsample_nearest_exact d vec py_impl DispatchKey CompositeImplicitAutograd aten _upsample_nearest_exact d vec py_impl DispatchKey Autograd aten _upsample_nearest_exact d vec py_impl DispatchKey CompositeImplicitAutograd aten _upsample_nearest_exact d vec py_impl DispatchKey Autograd _upsample_nearest_exact_vec input Tensor output_size Optional list int scale_factors Optional list float - Tensor osize = upsample_compute_output_size input size output_size scale_factors scales = scale_factors scale_factors None len osize type ignore list-item _upsample_nearest input osize scales exact=True _compute_upsample_nearest_indices input output_size scales exact=False For each dim output_size compute set input indices used produce upsampled output indices = num_spatial_dims = len output_size offset = exact d range num_spatial_dims Math matches aten src ATen native cpu UpSampleKernel cpp Indices computed following scale = isize osize Case exact=False input_index = floor output_index scale Same OpenCV INTER_NEAREST Case exact=False index_f = output_index + scale - input_index = round index_f Same Pillow Scikit-Image Scipy ndi zoom osize = output_size d isize = input shape -num_spatial_dims + d scale = isize isize scales d scales d None isize osize output_indices = torch arange osize dtype=torch float device=input device input_indices = output_indices + offset scale torch int _ range num_spatial_dims - - d input_indices = input_indices unsqueeze - indices append input_indices indices register_decomposition aten upsample_nearest d default aten upsample_nearest d out aten upsample_nearest d default py_impl DispatchKey CompositeImplicitAutograd aten upsample_nearest d default py_impl DispatchKey Autograd out_wrapper preserve_memory_format=True exact_dtype=True upsample_nearest d input Tensor output_size list int scales Optional float = None - Tensor _upsample_nearest input output_size scales register_decomposition aten _upsample_nearest_exact d default aten _upsample_nearest_exact d out aten _upsample_nearest_exact d default py_impl DispatchKey CompositeImplicitAutograd aten _upsample_nearest_exact d default py_impl DispatchKey Autograd out_wrapper preserve_memory_format=True exact_dtype=True upsample_nearest_exact d input Tensor output_size list int scales Optional float = None - Tensor _upsample_nearest input output_size scales exact=True register_decomposition aten upsample_nearest d default aten upsample_nearest d out aten upsample_nearest d default py_impl DispatchKey CompositeImplicitAutograd aten upsample_nearest d default py_impl DispatchKey Autograd out_wrapper preserve_memory_format=True exact_dtype=True upsample_nearest d input Tensor output_size list int scales_h Optional float = None scales_w Optional float = None - Tensor _upsample_nearest input output_size scales_h scales_w register_decomposition aten _upsample_nearest_exact d default aten _upsample_nearest_exact d out aten _upsample_nearest_exact d default py_impl DispatchKey CompositeImplicitAutograd aten _upsample_nearest_exact d default py_impl DispatchKey Autograd out_wrapper preserve_memory_format=True exact_dtype=True _upsample_nearest_exact d input Tensor output_size list int scales_h Optional float = None scales_w Optional float = None - Tensor _upsample_nearest input output_size scales_h scales_w exact=True register_decomposition aten upsample_nearest d default aten upsample_nearest d out aten upsample_nearest d default py_impl DispatchKey CompositeImplicitAutograd aten upsample_nearest d default py_impl DispatchKey Autograd out_wrapper preserve_memory_format=True exact_dtype=True upsample_nearest d input Tensor output_size list int scales_d Optional float = None scales_h Optional float = None scales_w Optional float = None - Tensor _upsample_nearest input output_size scales_d scales_h scales_w register_decomposition aten _upsample_nearest_exact d default aten _upsample_nearest_exact d out aten _upsample_nearest_exact d default py_impl DispatchKey CompositeImplicitAutograd aten _upsample_nearest_exact d default py_impl DispatchKey Autograd out_wrapper preserve_memory_format=True exact_dtype=True _upsample_nearest_exact d input Tensor output_size list int scales_d Optional float = None scales_h Optional float = None scales_w Optional float = None - Tensor _upsample_nearest input output_size scales_d scales_h scales_w exact=True pw_cast_for_opmath _upsample_nearest input Tensor output_size list int scales list Optional float exact bool = False - Tensor spatial_indices = _compute_upsample_nearest_indices input output_size scales exact=exact indices = None None + spatial_indices result = aten _unsafe_index input indices result ndim == convert output correct memory format necessary memory_format = utils suggest_memory_format input following heuristic only use channels_last path when s faster than contiguous path n_channels = input shape input device type == cuda n_channels memory_format = torch contiguous_format result = result contiguous memory_format=memory_format result gather_params params has_biases has_projections has_biases has_projections group_size = has_biases group_size = has_projections group_size = group_size = assert len params group_size == len params tuple params i i + group_size i range len params group_size params_hiddens params hiddens i bidirectional bidirectional cur_params cur_hidden = params i hiddens i bidir_params bidir_hidden = params i + hiddens i + cur_params cur_hidden = params i hiddens i bidir_params bidir_hidden = None None cur_params cur_hidden bidir_params bidir_hidden update_hidden_for_packed cur_hidden last_batch_size batch_size hiddens assert last_batch_size batch_size hiddens append cur_hidden narrow batch_size last_batch_size - batch_size cur_hidden narrow batch_size update_hidden_for_packed_reverse cur_hidden last_batch_size batch_size inp_hidden last_batch_size == batch_size cur_hidden assert last_batch_size batch_size torch concat cur_hidden inp_hidden narrow last_batch_size batch_size - last_batch_size one_layer_rnn_data inp hidden params has_biases hidden_fn batch_sizes reverse=False ih_weight = params hh_weight = params ih_bias = params has_biases None hh_bias = params has_biases None step_output = hiddens list torch Tensor = last_batch_size = batch_sizes - reverse batch_sizes cur_hidden = hidden narrow last_batch_size split_inp = torch split inp list batch_sizes reverse split_inp = split_inp - inp split_inp i = inp shape last_batch_size == i pass don t update cur_hidden will only happen when reverse=False since batch sizes sorted largest - smallest reverse cur_hidden = update_hidden_for_packed_reverse cur_hidden last_batch_size i hidden cur_hidden = update_hidden_for_packed cur_hidden last_batch_size i hiddens cur_hidden = hidden_fn inp cur_hidden ih_weight ih_bias hh_weight hh_bias last_batch_size = i step_output append cur_hidden reverse step_output reverse hiddens append cur_hidden hiddens reverse out = torch cat step_output hidden_out = torch cat hiddens reverse cur_hidden out hidden_out rnn_cell nonlinearity inner i cur_hidden ih_weight ih_bias hh_weight hh_bias nonlinearity F linear cur_hidden hh_weight hh_bias + i inner rnn_cell_data nonlinearity inner i cur_hidden ih_weight ih_bias hh_weight hh_bias i = F linear i ih_weight ih_bias nonlinearity F linear cur_hidden hh_weight hh_bias + i inner one_layer_rnn inp hidden params has_biases hidden_fn reverse=False ih_weight = params hh_weight = params ih_bias = params has_biases None hh_bias = params has_biases None precomputed_input = F linear inp ih_weight ih_bias precomputed_input = precomputed_input flip reverse precomputed_input cur_hidden = hidden unsqueeze step_output = i precomputed_input cur_hidden = hidden_fn i cur_hidden ih_weight ih_bias hh_weight hh_bias step_output append cur_hidden reverse step_output reverse out = torch cat step_output out cur_hidden squeeze mkldnn_one_layer_lstm inp hidden params has_biases reverse=False w = params w = params has_biases w = params w = params w = torch zeros w size w = torch zeros w size hx = hidden unsqueeze cx = hidden unsqueeze batch_sizes list int = mode = third_party ideep include ideep abstract_types hpp ideep rnn_kind LSTM = hidden_size = hx size num_layers = _rnn_helper already handles bidirectional batch_first so we hard-code them False here bidirectional = False batch_first = False train = False If batch_first inp has been permuted _rnn_helper Convert contiguous here Same aten src ATen native mkldnn RNN cpp mkldnn_rnn input = input contiguous inp = inp contiguous hx = hx contiguous cx = cx contiguous outputs = torch ops aten mkldnn_rnn_layer default inp w w w w hx cx reverse batch_sizes mode hidden_size num_layers has_biases bidirectional batch_first train y hy cy = outputs outputs outputs y hy squeeze cy squeeze _rnn_helper input hidden params has_biases num_layers dropout train bidirectional batch_first layer_fn input = input transpose batch_first input final_hiddens = i range num_layers cur_params cur_hidden bidir_params bidir_hidden = params_hiddens params hidden i bidirectional dropout = dropout train num_layers i - fwd_inp fwd_hidden = layer_fn input cur_hidden cur_params has_biases final_hiddens append fwd_hidden bidirectional bwd_inp bwd_hidden = layer_fn input bidir_hidden bidir_params has_biases reverse=True final_hiddens append bwd_hidden bidirectional input = torch cat fwd_inp bwd_inp fwd_inp dim - type ignore possibly-undefined input = fwd_inp dropout = train i num_layers - input = torch dropout input dropout train=True input = input transpose batch_first input input final_hiddens register_decomposition aten rnn_tanh input aten rnn_tanh input py_impl DispatchKey CompositeImplicitAutograd aten rnn_tanh input py_impl DispatchKey Autograd rnn_tanh_input input hx params has_biases num_layers dropout train bidirectional batch_first hidden = hx unbind params = gather_params params has_biases False out final_hiddens = _rnn_helper input hidden params has_biases num_layers dropout train bidirectional batch_first partial one_layer_rnn hidden_fn=rnn_cell torch tanh out torch stack final_hiddens register_decomposition aten rnn_relu input aten rnn_relu input py_impl DispatchKey CompositeImplicitAutograd aten rnn_relu input py_impl DispatchKey Autograd rnn_relu_input input hx params has_biases num_layers dropout train bidirectional batch_first hidden = hx unbind params = gather_params params has_biases False out final_hiddens = _rnn_helper input hidden params has_biases num_layers dropout train bidirectional batch_first partial one_layer_rnn hidden_fn=rnn_cell torch relu out torch stack final_hiddens register_decomposition aten rnn_relu data aten rnn_relu data py_impl DispatchKey CompositeImplicitAutograd aten rnn_relu data py_impl DispatchKey Autograd rnn_relu_data data batch_sizes hx params has_biases num_layers dropout train bidirectional hidden = hx unbind params = gather_params params has_biases False out final_hiddens = _rnn_helper data hidden params has_biases num_layers dropout train bidirectional False partial one_layer_rnn_data batch_sizes=batch_sizes hidden_fn=rnn_cell_data torch relu out torch stack final_hiddens register_decomposition aten rnn_tanh data aten rnn_tanh data py_impl DispatchKey CompositeImplicitAutograd aten rnn_tanh data py_impl DispatchKey Autograd rnn_tanh_data data batch_sizes hx params has_biases num_layers dropout train bidirectional hidden = hx unbind params = gather_params params has_biases False out final_hiddens = _rnn_helper data hidden params has_biases num_layers dropout train bidirectional False partial one_layer_rnn_data batch_sizes=batch_sizes hidden_fn=rnn_cell_data torch tanh out torch stack final_hiddens lstm_cell inp hx cx hh_weight hh_bias hr_weight chunk_dim gates = F linear hx hh_weight hh_bias + inp chunked_gates = gates chunk chunk_dim in_gate = chunked_gates sigmoid forget_gate = chunked_gates sigmoid cell_gate = chunked_gates tanh out_gate = chunked_gates sigmoid cy = forget_gate cx + in_gate cell_gate hy = out_gate cy tanh hy = hy hr_weight None F linear hy hr_weight None hy cy one_layer_lstm inp hidden params has_biases reverse=False ih_weight = params hh_weight = params ih_bias = params has_biases None hh_bias = params has_biases None hr_weight = params len params == params len params == None hx = hidden unsqueeze cx = hidden unsqueeze precomputed_input = F linear inp ih_weight ih_bias precomputed_input = precomputed_input flip reverse precomputed_input step_output = inp precomputed_input hx cx = lstm_cell inp hx cx hh_weight hh_bias hr_weight chunk_dim= step_output append hx reverse step_output reverse out = torch cat step_output out hx squeeze cx squeeze one_layer_lstm_data inp hidden params has_biases batch_sizes reverse=False ih_weight = params hh_weight = params ih_bias = params has_biases None hh_bias = params has_biases None hr_weight = params len params == params len params == None step_output = hiddens = last_batch_size = batch_sizes - reverse batch_sizes split_inp = torch split inp list batch_sizes reverse split_inp = split_inp - orig_hx = hidden orig_cx = hidden hx cx = orig_hx narrow last_batch_size orig_cx narrow last_batch_size inp split_inp i = inp shape inp = F linear inp ih_weight ih_bias will only happen when reverse=False since batch sizes sorted largest - smallest i last_batch_size hiddens append hx narrow i last_batch_size - i cx narrow i last_batch_size - i hx cx = hx narrow i cx narrow i will only happen when reverse=True i last_batch_size hx = torch concat hx orig_hx narrow last_batch_size i - last_batch_size cx = torch concat cx orig_cx narrow last_batch_size i - last_batch_size hx cx = lstm_cell inp hx cx hh_weight hh_bias hr_weight chunk_dim= last_batch_size = i step_output append hx reverse step_output reverse hidden_out = hx cx hiddens append hx cx hiddens reverse hidden hidden = zip hiddens hidden_out = torch cat hidden torch cat hidden out = torch cat step_output out hidden_out select_one_layer_lstm_function input hx params r Check whether we could use decompose lstm mkldnn_rnn_layer All below conditions need met ` ` torch _C _get_mkldnn_enabled ` ` returns ` ` True ` ` All input args CPU The dtypes args either torch float torch bfloat Inference ` ` has_projections ` ` returns ` ` False ` ` Args input input sequence LSTM hx tuple input hidden state cell state ` ` h_ c_ ` ` LSTM params weight bias tensors LSTM use_mkldnn input hx params torch _C _get_mkldnn_enabled False tensors = input + list hx + list chain from_iterable params devices = t device t tensors len devices = False device = devices pop device = torch device cpu False With autocast possible have mixed dtype here dtypes = t dtype t tensors dtype dtypes dtype torch float torch bfloat False input requires_grad False has_projections = hx size = hx size has_projections False True mkldnn_one_layer_lstm does depend seq_len while one_layer_lstm will expand over seq_len dim use_mkldnn input hx params mkldnn_one_layer_lstm one_layer_lstm register_decomposition aten lstm input aten lstm input py_impl DispatchKey CompositeImplicitAutograd aten lstm input py_impl DispatchKey Autograd lstm_impl input hx params has_biases num_layers dropout train bidirectional batch_first assert len hx == lstm expects two hidden states params = gather_params params has_biases hx size = hx size hidden = list zip hx hx layer_fn = select_one_layer_lstm_function input hx params out final_hiddens = _rnn_helper input hidden params has_biases num_layers dropout train bidirectional batch_first layer_fn final_hiddens = list zip final_hiddens out torch stack final_hiddens torch stack final_hiddens register_decomposition aten lstm data aten lstm data py_impl DispatchKey CompositeImplicitAutograd aten lstm data py_impl DispatchKey Autograd lstm_data_impl data batch_sizes hx params has_biases num_layers dropout train bidirectional assert len hx == lstm expects two hidden states params = gather_params params has_biases hx size = hx size hidden = list zip hx hx out final_hiddens = _rnn_helper data hidden params has_biases num_layers dropout train bidirectional False partial one_layer_lstm_data batch_sizes=batch_sizes final_hiddens = list zip final_hiddens out torch stack final_hiddens torch stack final_hiddens gru_cell inp cur_hidden ih_weight ih_bias hh_weight hh_bias chunked_igates = inp chunk chunked_hgates = F linear cur_hidden hh_weight hh_bias chunk reset_gate = chunked_hgates + chunked_igates sigmoid input_gate = chunked_hgates + chunked_igates sigmoid new_gate = chunked_igates + chunked_hgates reset_gate tanh cur_hidden - new_gate input_gate + new_gate gru_cell_data inp cur_hidden ih_weight ih_bias hh_weight hh_bias chunked_igates = F linear inp ih_weight ih_bias chunk chunked_hgates = F linear cur_hidden hh_weight hh_bias chunk reset_gate = chunked_hgates + chunked_igates sigmoid input_gate = chunked_hgates + chunked_igates sigmoid new_gate = chunked_igates + chunked_hgates reset_gate tanh cur_hidden - new_gate input_gate + new_gate register_decomposition aten gru data aten gru data py_impl DispatchKey CompositeImplicitAutograd aten gru data py_impl DispatchKey Autograd gru_impl_data data batch_sizes hx params has_biases num_layers dropout train bidirectional params = gather_params params has_biases False out final_hiddens = _rnn_helper data hx unbind params has_biases num_layers dropout train bidirectional False partial one_layer_rnn_data batch_sizes=batch_sizes hidden_fn=gru_cell_data out torch stack final_hiddens register_decomposition aten gru input aten gru input py_impl DispatchKey CompositeImplicitAutograd aten gru input py_impl DispatchKey Autograd gru_impl input hx params has_biases num_layers dropout train bidirectional batch_first params = gather_params params has_biases False out final_hiddens = _rnn_helper input hx unbind params has_biases num_layers dropout train bidirectional batch_first partial one_layer_rnn hidden_fn=gru_cell out torch stack final_hiddens register_decomposition aten _upsample_bilinear d_aa vec aten _upsample_bilinear d_aa vec py_impl DispatchKey CompositeImplicitAutograd aten _upsample_bilinear d_aa vec py_impl DispatchKey Autograd upsample_bilinear d_aa_vec input output_size align_corners scale_factors osize = upsample_compute_output_size input size output_size scale_factors scale_h = get_scale_value scale_factors scale_w = get_scale_value scale_factors torch ops aten _upsample_bilinear d_aa input osize align_corners scale_h scale_w register_decomposition aten _upsample_bicubic d_aa vec aten _upsample_bicubic d_aa vec py_impl DispatchKey CompositeImplicitAutograd aten _upsample_bicubic d_aa vec py_impl DispatchKey Autograd upsample_bicubic d_aa_vec input output_size align_corners scale_factors osize = upsample_compute_output_size input size output_size scale_factors scale_h = get_scale_value scale_factors scale_w = get_scale_value scale_factors torch ops aten _upsample_bicubic d_aa input osize align_corners scale_h scale_w register_decomposition aten upsample_bilinear d vec register_decomposition aten upsample_trilinear d vec aten upsample_linear d vec py_impl DispatchKey CompositeImplicitAutograd aten upsample_linear d vec py_impl DispatchKey Autograd aten upsample_bilinear d vec py_impl DispatchKey CompositeImplicitAutograd aten upsample_bilinear d vec py_impl DispatchKey Autograd aten upsample_trilinear d vec py_impl DispatchKey CompositeImplicitAutograd aten upsample_trilinear d vec py_impl DispatchKey Autograd _upsample_linear_vec input output_size align_corners scale_factors osize = upsample_compute_output_size input size output_size scale_factors scales = scale_factors scale_factors None len osize _upsample_linear input osize align_corners scales register_decomposition aten upsample_linear d default aten upsample_linear d out out_wrapper upsample_linear d input Tensor output_size list int align_corners bool scales_w Optional float = None - Tensor _upsample_linear input output_size align_corners scales_w register_decomposition aten upsample_bilinear d default aten upsample_bilinear d out aten upsample_bilinear d default py_impl DispatchKey Autograd out_wrapper upsample_bilinear d input Tensor output_size list int align_corners bool scales_h Optional float = None scales_w Optional float = None - Tensor _upsample_linear input output_size align_corners scales_h scales_w register_decomposition aten upsample_trilinear d default aten upsample_trilinear d out out_wrapper upsample_trilinear d input Tensor output_size list int align_corners bool scales_d Optional float = None scales_h Optional float = None scales_w Optional float = None - Tensor _upsample_linear input output_size align_corners scales_d scales_h scales_w _compute_scale in_size out_size align_corners scale=None align_corners in_size - out_size - out_size scale scale None scale in_size out_size _compute_source_index scale dst_index align_corners align_corners scale dst_index scale dst_index + - _sum_tensors_uint src Iterable Tensor weights Iterable Tensor weights_precision Tensor - Tensor output = _sum_tensors s torch int c torch int s c zip src weights + weights_precision - output = output weights_precision torch clamp output torch uint _compute_weight_precision weights TensorSequenceType - Tensor max_weight = torch stack weights max max_weight_precision = precisions = torch arange max_weight_precision device=max_weight device values = + max_weight precisions + mask = values = max_weight_precision - mask sum pw_cast_for_opmath _upsample_linear input Tensor output_size list int align_corners bool scales list Optional float - Tensor get dimensions original image n_channels = input shape inp_sizes = input shape n_dims = len inp_sizes _ dtype = utils elementwise_dtypes input type_promotion_kind=utils ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT get_values inp_size out_size scales nsqueeze First Calculate scaling factor scale_factor = _compute_scale inp_size out_size align_corners scales We have create arange int dtype use order avoid additional kernels creation inductor get perf slowdown i = torch arange out_size device=input device dtype=dtype x_f = _compute_source_index scale_factor i align_corners clamp min= x_f = x_f reshape x_f shape nsqueeze x = x_f torch int xp = x + clamp max=inp_size - x_f x xp values = get_values inp_size out_size scales n_dims - - i i inp_size out_size scales enumerate zip inp_sizes output_size scales xs_f xs xp s = list zip values vs = product n_dims idx = None None + xs k k == xp s k k range n_dims v = aten _unsafe_index input idx v = _maybe_convert_to_dtype v dtype vs append v i reversed range n_dims xscale = xs_f i - xs i clamp dtype vs = x - alpha + x alpha == x + x - x alpha v + torch mul v - v xscale v v zip vs vs assert len vs == result = vs convert output correct memory format necessary memory_format = utils suggest_memory_format input following heuristic only use channels_last path when s faster than contiguous path input device type == cuda n_channels memory_format = torch contiguous_format assert isinstance result torch Tensor result = result contiguous memory_format=memory_format input is_floating_point result = result round result We should applying decompositions after all transformations register_decomposition aten is_same_size default is_same_size Tensor b Tensor - bool shape == b shape register_decomposition aten _reshape_alias aten _unsafe_view out_wrapper _reshape_alias x shape args aten view x shape register_decomposition aten _unsafe_index _unsafe_index x indices aten index x indices register_decomposition aten _unsafe_index_put _unsafe_index_put x indices value accumulate=False aten index_put x indices value accumulate register_decomposition aten _unsafe_masked_index _unsafe_masked_index x mask indices fill index indices index None torch _check index dtype torch long torch int lambda tensors used indices must long int tensors torch _check mask dtype == torch bool lambda tensors used masks must bool tensors torch fx experimental symbolic_shapes guard_or_false guard_or_false x numel == meta_result = torch _meta_registrations meta_index_Tensor x indices x new_full meta_result shape fill i range len indices index = indices i index None indices i = index clamp min= max=x size i - aten _unsafe_index x indices masked_fill ~mask fill register_decomposition aten _unsafe_masked_index_put_accumulate _unsafe_masked_index_put_accumulate x mask indices values index indices index None torch _check index dtype torch long torch int lambda tensors used indices must long int tensors torch _check mask dtype == torch bool lambda tensors used masks must bool tensors x numel == x clone i range len indices index = indices i index None indices i = index clamp min=-x size i max=x size i - masked_value = values masked_fill ~mask aten _unsafe_index_put x indices masked_value accumulate=True _nll_loss_forward Tensor target Tensor weight Optional Tensor reduction int ignore_index int - tuple Tensor Tensor can N C C target can N n_dims = dim channel_dim = n_dims channel_dim = weight None n_dims shape = n_dims shape channel_dim = weight shape w = weight view shape w = weight = w safe_target = torch where target = ignore_index target safe_target_ = safe_target unsqueeze channel_dim target can N result = -torch gather channel_dim safe_target_ squeeze channel_dim result = torch where target = ignore_index result reduction == Reduction NONE value n_dims total_weight = new_full result total_weight weight None pyrefly ignore unbound-name w = w expand shape wsum = torch gather w channel_dim safe_target_ squeeze channel_dim wsum = torch where target = ignore_index wsum total_weight = wsum sum total_weight = target = ignore_index sum reduction == Reduction SUM value result = result sum reduction == Reduction MEAN value result = result sum total_weight result total_weight register_decomposition aten nll_loss_forward out_wrapper output total_weight nll_loss_forward Tensor target Tensor weight Optional Tensor reduction int ignore_index int - tuple Tensor Tensor assert dim dim = input tensor should D D assert target dim = D D target tensor expected multi-target supported no_batch_dim = dim == target dim == assert no_batch_dim shape == target shape f size mismatch got input shape target target shape n_classes = shape - assert weight None weight dim == weight numel == n_classes f weight tensor should defined either all n_classes classes no classes f got weight tensor shape weight shape _nll_loss_forward target weight reduction ignore_index register_decomposition aten nll_loss d_forward out_wrapper output total_weight nll_loss d_forward Tensor target Tensor weight Optional Tensor reduction int ignore_index int - tuple Tensor Tensor _nll_loss_forward target weight reduction ignore_index These adapted aten src ATen native UpSample h which based https en wikipedia org wiki Bicubic_interpolation#Bicubic_convolution_algorithm _upsample_cubic_convolution x Tensor A float - Tensor A + x - A + x x + _upsample_cubic_convolution x Tensor A float - Tensor A x - A x + A x - A _upsample_get_cubic_coefficients t Tensor - TensorSequenceType A = - t device == torch device cpu tt = torch stack t - t dim= tt = torch stack t + - t dim= w = _upsample_cubic_convolution tt A w = _upsample_cubic_convolution tt A w w = torch unbind w dim= w w = torch unbind w dim= w w w w _upsample_cubic_convolution t + A _upsample_cubic_convolution t A _upsample_cubic_convolution - t A _upsample_cubic_convolution - t A _upsample_cubic_interp d coeffs TensorSequenceType ts Tensor - Tensor coeffs = _upsample_get_cubic_coefficients ts _sum_tensors c c c c zip coeffs coeffs Need instead just sum keep mypy happy _sum_tensors ts Iterable Tensor - Tensor reduce torch add ts _linspace_from_neg_one num_steps int align_corners bool dtype torch dtype device torch device num_steps = torch tensor device=device dtype=dtype = num_steps - num_steps align_corners torch linspace -a steps=num_steps device=device dtype=dtype _make_base_grid_ d theta Tensor h int w int align_corners bool dtype = theta dtype device = theta device Using padding summation generates single kernel vs using torch stack where kernels generated corresponding each individual tensor grid_x grid_y grid_one grid_x = _linspace_from_neg_one w align_corners dtype device view w grid_y = _linspace_from_neg_one h align_corners dtype device view h grid_one = torch ones dtype=dtype device=device just temporary hack we should use torch stack here once merged grid_x = torch nn functional pad grid_x pad= mode= constant value= grid_y = torch nn functional pad grid_y pad= mode= constant value= grid_one = torch nn functional pad grid_one pad= mode= constant value= grid_x + grid_y + grid_one _make_base_grid_ d theta Tensor d int h int w int align_corners bool dtype = theta dtype device = theta device grid_x = _linspace_from_neg_one w align_corners dtype device view w grid_y = _linspace_from_neg_one h align_corners dtype device view h grid_z = _linspace_from_neg_one d align_corners dtype device view d grid_one = torch ones dtype=dtype device=device just temporary hack we should use torch stack here once merged grid_x = torch nn functional pad grid_x pad= mode= constant value= grid_y = torch nn functional pad grid_y pad= mode= constant value= grid_z = torch nn functional pad grid_z pad= mode= constant value= grid_one = torch nn functional pad grid_one pad= mode= constant value= grid_x + grid_y + grid_z + grid_one _affine_grid_generator_ d theta Tensor size list int align_corners bool n _ h w = size base_grid = _make_base_grid_ d theta h w align_corners=align_corners base_grid shape h w theta shape n We do manually matrix multiplication which faster than mm h w n - n h w grid = base_grid view - theta mT unsqueeze sum - grid view n h w _affine_grid_generator_ d theta Tensor size list int align_corners bool n _ d h w = size base_grid = _make_base_grid_ d theta d h w align_corners=align_corners base_grid shape d h w theta shape n We do manually matrix multiplication which faster than mm d h w n - n h w grid = base_grid view - theta mT unsqueeze sum - grid view n d h w register_decomposition aten affine_grid_generator out_wrapper pw_cast_for_opmath affine_grid_generator theta Tensor size list int align_corners bool torch _check len size lambda affine_grid_generator needs d spatial d volumetric inputs len size == _affine_grid_generator_ d theta size align_corners=align_corners _affine_grid_generator_ d theta size align_corners=align_corners _grid_sampler_ d Tensor grid Tensor interpolation_mode int = padding_mode int = align_corners bool = False _expand_grid bool = True - Tensor This method copy grid_sampler_ d implementation introduced additional arg _expand_grid optionally expand input grid performance reasons Experimenting locally found compiled CUDA code accelerated ~ x CPU code ~ x bicubic mode we expand grid N H W into N C H W However leads slowdown around ~ x CPU bilinear mode channels first Thus we apply hack expand grid case torch _check interpolation_mode lambda f Invalid interpolation mode interpolation_mode torch _check padding_mode lambda f Invalid padding mode padding_mode unnormalize coords Tensor size int - Tensor Rescale coordinates - size - align_corners True - size - align_corners False mul = size - align_corners size ofs = size - coords mul + ofs Reflects coordinates until they fall between low high inclusive The bounds passed twice their value so half-integer values can represented ints reflect_coordinates coords Tensor twice_low int twice_high int - Tensor twice_low == twice_high torch zeros_like coords coords_min = twice_low coords_span = twice_high - twice_low coords = coords - coords_min abs extra = torch fmod coords coords_span flips = coords coords_span floor dtype=torch int torch where flips == extra + coords_min coords_span + coords_min - extra compute_coordinates coords Tensor size int - Tensor padding_mode == Zero coords padding_mode == Borders torch clamp coords size - padding_mode == Reflection align_corners coords_reflected = reflect_coordinates coords size - coords_reflected = reflect_coordinates coords - size - torch clamp coords_reflected size - compute_source_index coords Tensor size int - Tensor coords_un = unnormalize coords size compute_coordinates coords_un size N C iH iW = shape _ oH oW two = grid shape assert two == _expand_grid Let s expand grid N C oH oW This allows generate single triton cuda kernel instead two kernels Two kernels due source indices weights have shape N oH oW xnumel=N oH oW output has shape N C oH oW xnumel=N C oH oW Expanding grid N C oH oW two unifies xnumel N C oH oW grid = grid view N oH oW two expand N C oH oW in_bounds_cond xs Tensor ys Tensor - Tensor torch logical_and = xs torch logical_and xs iW torch logical_and = ys ys iH N_idx = torch arange N device=a device view N C_idx = torch arange C device=a device view C clip xs Tensor ys Tensor ws Tensor - TensorSequenceType cond = in_bounds_cond xs ys To clip inside valid coordinates we map coordinates x y = also set weight We also change shape tensor appropriate one broadcasting N_idx C_idx purposes advanced indexing c = C _expand_grid tuple torch where cond t view N c oH oW t xs dtype=torch int ys dtype=torch int ws get_summand ix Tensor iy Tensor w - Tensor Perform clipping index into input tensor multiply weight idx_x idx_y w_ = clip ix iy w N_idx C_idx idx_y idx_x w_ x = grid y = grid interpolation_mode == Bilinear ix = compute_source_index x iW iy = compute_source_index y iH ix_nw iy_nw = ix floor iy floor ix_ne iy_ne = ix_nw + iy_nw ix_sw iy_sw = ix_nw iy_nw + ix_se iy_se = ix_ne iy_sw w_nw = ix_se - ix iy_se - iy w_ne = ix - ix_sw iy_sw - iy w_sw = ix_ne - ix iy - iy_ne w_se = ix - ix_nw iy - iy_nw _sum_tensors get_summand ix iy w ix iy w ix_nw iy_nw w_nw ix_ne iy_ne w_ne ix_sw iy_sw w_sw ix_se iy_se w_se interpolation_mode == Nearest ix = compute_source_index x iW iy = compute_source_index y iH ix_nearest = ix round iy_nearest = iy round get_summand ix_nearest iy_nearest interpolation_mode == Bicubic ix = unnormalize x iW iy = unnormalize y iH ix_nw = ix floor iy_nw = iy floor tx = ix - ix_nw ty = iy - iy_nw _expand_grid tx = tx unsqueeze ty = ty unsqueeze get_value_bounded ix Tensor iy Tensor - Tensor x = compute_coordinates ix iW y = compute_coordinates iy iH get_summand x y get_coeff ofs int - Tensor iy_ofs = iy_nw + ofs - cs = get_value_bounded ix_nw - iy_ofs get_value_bounded ix_nw iy_ofs get_value_bounded ix_nw + iy_ofs get_value_bounded ix_nw + iy_ofs _upsample_cubic_interp d cs tx coeffs = tuple get_coeff ofs ofs range _upsample_cubic_interp d coeffs ty register_decomposition aten grid_sampler_ d out_wrapper pw_cast_for_opmath grid_sampler_ d Tensor grid Tensor interpolation_mode int = padding_mode int = align_corners bool = False - Tensor _grid_sampler_ d grid=grid interpolation_mode=interpolation_mode padding_mode=padding_mode align_corners=align_corners register_decomposition aten mv out_wrapper exact_dtype=True pw_cast_for_opmath mv vec torch _check dim == vec dim == lambda f matrix vector expected got dim vec dim torch _check size == vec size lambda f size mismatch got input size x size vec vec size vec sum dim= register_decomposition aten binary_cross_entropy_with_logits out_wrapper binary_cross_entropy_with_logits target weight=None pos_weight=None reduction=Reduction MEAN value pos_weight None log_weight = pos_weight - target + loss = - target - log_weight F logsigmoid loss = - target - F logsigmoid weight None loss = loss weight apply_loss_reduction loss reduction should_fold tensor torch Tensor tensor torch Tensor is_out bool - bool For comments logic function see eager native LinearAlgebra cpp t t = tensor tensor tensor ndim = tensor ndim tensor tensor torch fx experimental symbolic_shapes guard_or_false t ndim = t ndim = False t requires_grad is_out True tensor ndim == False guard_or_false t numel == True t _shape = t shape t _stride = t stride Check contiguous we can skip dim size aten https github com pytorch pytorch blob e f aa b c d b c b aten src ATen TensorGeometry cpp#L expected_stride = size reversed t _shape expected_stride append size expected_stride - all guard_or_false size == guard_or_false left == right left right size zip t _stride list reversed expected_stride t _shape aten matmul default py_impl DispatchKey CompositeImplicitAutograd aten matmul out py_impl DispatchKey CompositeImplicitAutograd out_wrapper pass_is_out=True matmul tensor tensor is_out=False dim_tensor = tensor dim dim_tensor = tensor dim assert dim_tensor = dim_tensor = dim_tensor == dim_tensor == torch dot tensor tensor dim_tensor == dim_tensor == torch mv tensor tensor dim_tensor == dim_tensor == torch squeeze torch mm torch unsqueeze tensor tensor dim_tensor == dim_tensor == torch mm tensor tensor should_fold tensor tensor is_out dim_tensor = dim_tensor == &#124; &#124; dim_tensor == &#124; &#124; dim_tensor = dim_tensor == &#124; &#124; dim_tensor == some condition strides fulfilled optimization use mm instead bmm folding batch larger tensor into its leading matrix dimension transpose = dim_tensor dim_tensor t = tensor mT transpose tensor t = tensor transpose tensor t dim_tensor == tensor Invariant t dim = t dim == &#124; &#124; t dim == t t matmul-compatible Why t view - sizes_ - If last dim then view - won t work because - becomes ambiguous This can happen e g sizes_ = t shape output_shape = list sizes_ - folded_dim = reduce operator mul output_shape Readjust output_shape we multiplying matrix t _is_matrix = t dim == t _is_matrix output_shape append t shape This will almost always view It may view t - requires_grad See should_fold aten explanation t _folded = t reshape folded_dim sizes_ - t _is_matrix This copies we perform D D first tensor requires_grad See should_fold native LinearAlgebra cpp why output = torch ops aten _unsafe_view t _folded mm t output_shape output mT contiguous transpose output torch ops aten _unsafe_view t _folded mv t output_shape dim_tensor = dim_tensor = We multiplying b x n x m x x m x p where b can list we track m vs m separately even though they must match nicer error messages n = tensor size - dim_tensor m = tensor size - batch_tensor = tensor shape - m = tensor size - dim_tensor tensor size - p = tensor size - dim_tensor batch_tensor list int = TODO handling slice i range dim_tensor - batch_tensor append tensor size i Same optimization gradients should_fold If we re going broadcast we force go through should_fold branch dim_tensor == dim_tensor == batch_tensor = batch_tensor batch_tensor == tensor requires_grad matmul tensor squeeze tensor batch_tensor == tensor requires_grad matmul tensor tensor squeeze expand batch portion i e cut off matrix dimensions expand rest expand_batch_portion = list torch broadcast_shapes batch_tensor batch_tensor tensor _expand_size = expand_batch_portion + n m expand_batch_product = prod expand_batch_portion HACK We need reshape symint support tensor _expanded = tensor expand tensor _expand_size reshape expand_batch_product n m vector_rhs = dim_tensor == vector_rhs tensor _expand_size = expand_batch_portion + m tensor _expanded = tensor expand tensor _expand_size reshape expand_batch_product m unsqueeze tensor _expand_size = expand_batch_portion + m p tensor _expanded = tensor expand tensor _expand_size reshape expand_batch_product m p output_shape = expand_batch_portion dim_tensor output_shape append n dim_tensor output_shape append p vector_rhs tensor _expanded bmm tensor _expanded squeeze - view output_shape tensor _expanded bmm tensor _expanded view output_shape torch _check False lambda both arguments matmul need least D register_decomposition aten upsample_bicubic d default aten upsample_bicubic d out aten upsample_bicubic d default py_impl DispatchKey Autograd out_wrapper pw_cast_for_opmath upsample_bicubic d_default input Tensor output_size tuple int int align_corners bool scale_h Optional float = None scale_w Optional float = None - Tensor get dimensions original image _ _ in_h in_w = input shape Calculate horizontal vertical scaling factor h_scale_factor = _compute_scale in_h output_size align_corners scale_h w_scale_factor = _compute_scale in_w output_size align_corners scale_w _ dtype = utils elementwise_dtypes input type_promotion_kind=utils ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT We have create arange int dtype use order avoid additional kernels creation inductor get perf slowdown i = torch arange output_size device=input device dtype=dtype j = torch arange output_size device=input device dtype=dtype x_float = _compute_source_index w_scale_factor j align_corners y_float = _compute_source_index h_scale_factor i align_corners y_float = y_float unsqueeze - x = x_float floor y = y_float floor We should also clamp xscale yscale See guard_index_and_lambda UpSample h yscale = y_float - y clamp xscale = x_float - x clamp x = x torch int y = y torch int iys_ofs = y - y y + y + ixs_ofs = x - x x + x + weights_x = _upsample_get_cubic_coefficients xscale weights_y = _upsample_get_cubic_coefficients yscale weights_precision_x weights_precision_y = None None input dtype == torch uint weights_precision_x = _compute_weight_precision weights_x weights_precision_y = _compute_weight_precision weights_y weights_x = w weights_precision_x + torch sign w torch int w weights_x weights_y = w weights_precision_y + torch sign w torch int w weights_y load_bounded ys xs y_idx = torch clamp ys in_h - x_idx = torch clamp xs in_w - v = aten _unsafe_index input None None y_idx x_idx v get_x_interp y src_x = tuple load_bounded y x_ofs x_ofs ixs_ofs input dtype == torch uint assert weights_precision_x None _sum_tensors_uint src_x weights_x weights_precision_x _sum_tensors c c c c zip src_x weights_x src_y = tuple get_x_interp y_ofs y_ofs iys_ofs input dtype == torch uint assert weights_precision_y None result = _sum_tensors_uint src_y weights_y weights_precision_y result = _sum_tensors c c c c zip src_y weights_y convert output correct memory format necessary memory_format = utils suggest_memory_format input result = result contiguous memory_format=memory_format result register_decomposition aten upsample_bicubic d vec aten upsample_bicubic d vec py_impl DispatchKey CompositeImplicitAutograd aten upsample_bicubic d vec py_impl DispatchKey Autograd out_wrapper pw_cast_for_opmath upsample_bicubic d_vec Tensor output_size Optional tuple int int align_corners bool scale_factors Optional tuple float float = None - Tensor torch _check bool output_size + bool scale_factors == lambda Must specify exactly one output_size scale_factors output_size None assert scale_factors None output_size = cast tuple int int tuple sym_int sym_float w scale w scale zip shape scale_factors scale_h scale_w = scale_factors scale_factors None None upsample_bicubic d_default output_size align_corners scale_h scale_w register_decomposition aten reflection_pad d register_decomposition aten reflection_pad d register_decomposition aten reflection_pad d pw_cast_for_opmath out_wrapper _reflection_pad Tensor padding tuple int - Tensor idx left middle right dim_idx = torch arange -left middle + right device=a device middle - - middle - - dim_idx abs abs _reflection_or_replication_pad padding idx register_decomposition aten replication_pad d register_decomposition aten replication_pad d register_decomposition aten replication_pad d pw_cast_for_opmath out_wrapper _replication_pad Tensor padding tuple int - Tensor idx left middle right dim_idx = torch arange -left middle + right device=a device torch clamp dim_idx middle - _reflection_or_replication_pad padding idx _reflection_or_replication_pad Tensor padding tuple int idx_fn Callable int int int Tensor - Tensor dim = len padding torch _check dim dim + dim + lambda f reflection_pad dim d requires dim + D dim + D input inp_shape = shape -dim nc_dim = dim - dim padding_left = padding dim - - i i range dim padding_right = padding dim - - i + i range dim result = i range dim idx list Any = None result dim idx i + nc_dim = idx_fn padding_left i inp_shape i padding_right i result = aten _unsafe_index result idx convert output correct memory format necessary memory_format = utils suggest_memory_format result result = result contiguous memory_format=memory_format result register_decomposition aten reflection_pad d_backward register_decomposition aten reflection_pad d_backward register_decomposition aten reflection_pad d_backward out_wrapper grad_input _reflection_pad_backward grad_output x padding dim = len padding dhw = h - h x shape -dim padding_left = padding dim - - i i range dim padding_right = padding dim - - i + i range dim indices = i range x ndim view_shape = x ndim view_shape i = - indices append torch arange x shape i device=x device view view_shape b = indices -dim xyz = indices -dim index_range_condition index_range i lb ub = index_range torch logical_and i = lb i = ub Areas after reflection top-left &#124; top &#124; top-right ----------------------------------------- left &#124; center &#124; right ----------------------------------------- bottom-left &#124; bottom &#124; bottom-right The center area original matrix Other areas reflections center = xyz i + padding_left i i range dim left_reflect = padding_left i - xyz i i range dim right_reflect = dhw i + padding_left i - xyz i i range dim Accumulate gradients different areas If some padding negative center load always valid range_c = center i dhw i + padding_left i + padding_right i i range dim cond = functools reduce aten logical_and index_range_condition range_c i i range dim grad = aten _unsafe_masked_index grad_output cond b + center accumulate grad out index_ranges If upper bound less than lower bound we can get rid one accumulation This happens when padding size zero i range dim upper_less_than_lower = index_ranges i index_ranges i isinstance upper_less_than_lower bool upper_less_than_lower grad cond = functools reduce aten logical_and index_range_condition index_range index_range index_ranges g = aten _unsafe_masked_index grad_output cond b + out grad + g area itertools product - _ range dim area == tuple dim center already done continue outs = index_ranges = i range dim area i == out = center i index_range = range_c i area i == - out = left_reflect i index_range = xyz i padding_left i area i == out = right_reflect i index_range = xyz i dhw i - padding_right i dhw i - outs append out type ignore possibly-undefined index_ranges append index_range type ignore possibly-undefined grad = accumulate grad outs index_ranges grad register_decomposition aten aminmax out_wrapper min max aminmax dim=None keepdim=False pyrefly ignore bad-argument-type amin = torch amin dim=dim keepdim=keepdim pyrefly ignore bad-argument-type amax = torch amax dim=dim keepdim=keepdim amin amax register_decomposition aten nansum out_wrapper nansum dim=None keepdim=False dtype=None aten sum torch where torch isnan dim keepdim dtype=dtype register_decomposition aten arange default aten arange out out_wrapper arange_default end NumberType dtype Optional torch dtype = None layout torch layout = torch strided device Optional torch device = None pin_memory bool = False aten arange start_step end dtype=dtype layout=layout device=device pin_memory=pin_memory register_decomposition aten arange start arange_start start NumberType end NumberType dtype Optional torch dtype = None layout torch layout = torch strided device Optional torch device = None pin_memory bool = False aten arange start_step start end dtype=dtype layout=layout device=device pin_memory=pin_memory register_decomposition out_dtype out_dtype_decomp args kwargs torch _higher_order_ops out_dtype out_dtype_dense out_dtype_dense args kwargs register_decomposition aten multi_margin_loss aten multi_margin_loss default py_impl DispatchKey Autograd out_wrapper multi_margin_loss input Tensor target Tensor p NumberType = margin NumberType = weight Optional Tensor = None reduction int = Reduction MEAN value - Tensor input = torch atleast_ d input target = torch atleast_ d target nframe = input shape dim = input shape torch _check p == p == lambda only p == p == supported torch _check input ndim == dim = lambda f Expected non-empty vector matrix optional -dim batch size got input shape torch _check target ndim == target numel == nframe lambda f inconsistent target size expected nframe got target shape weight None weight = torch atleast_ d weight torch _check weight ndim == weight numel == dim type ignore union-attr lambda f inconsistent weight size expected dim got weight shape type ignore union-attr target = target unsqueeze u = torch gather input dim= index=target z = margin - u + input z = z clamp_min z = z p == z z weight None z = z weight target idx = torch arange dim device=input device z = torch where idx = target z reduction == Reduction MEAN value z mean reduction == Reduction SUM value z sum z shape z mean dim= register_decomposition aten multilabel_margin_loss_forward aten multilabel_margin_loss_forward default py_impl DispatchKey Autograd out_wrapper output is_target multilabel_margin_loss_forward input Tensor target Tensor reduction int - tuple Tensor Tensor orig_input_shape = input shape orig_target_shape = target shape input = torch atleast_ d input target = torch atleast_ d target dim = input shape torch _check len orig_input_shape = dim = lambda f Expected non-empty vector matrix optional -dim batch size got orig_input_shape torch _check len orig_target_shape = orig_target_shape == orig_input_shape lambda f inconsistent target size orig_target_shape input size orig_input_shape ignores labels after first - detects when - present idx = torch arange dim device=target device is_end = target == - end_idx = torch amin torch where is_end idx dim dim=- keepdim=True target indices target_mask = idx end_idx masks target able use gather which doesn t allow - tidx = torch where target_mask target u = torch gather input dim=- index=tidx is_target tidx = torch where target_mask target - is_target = torch any idx == tidx unsqueeze dim=- dim= loss z = - u T unsqueeze dim=- + input z = z clamp_min z = z dim masks loss z = torch where is_target z reduction reduction == Reduction MEAN value z = z sum dim= - mean reduction == Reduction SUM value z = z sum z = z sum dim= - result is_target = is_target input dtype reshape orig_target_shape z is_target scaled_dot_product_attention used decomposed pre-autograd given calls _scaled_dot_product_attention_math _scaled_dot_product_attention_math only has CompositeImplicitAutograd kernel As result s decomposed into ops finer granularity However recent PRs added new logic scaled_dot_product_attention now calls _scaled_dot_product_flash_attention_for_cpu export path This results _scaled_dot_product_flash_attention_for_cpu showing up export result This decomposition ensures scaled_dot_product_attention still decomposed same way before i e going through _scaled_dot_product_attention_math Notice decomp rule should excluded inductor register_decomposition aten _scaled_dot_product_flash_attention_for_cpu default scaled_dot_product_flash_attention_for_cpu query Tensor key Tensor value Tensor dropout_p float = is_causal bool = False attn_mask Optional Tensor = None scale Optional float = None - tuple Tensor Tensor torch _check torch is_floating_point query lambda f query must FP FP BF FP got query dtype torch _check query dim == key dim == value dim == lambda f q k v must dimensional tensor got query dim key dim value dim torch _check dropout_p == lambda f dropout probability must zero got dropout_p torch _check query shape == value shape key shape == value shape lambda q k v should have same head size output attn = aten _scaled_dot_product_attention_math default query key value attn_mask=attn_mask dropout_p=dropout_p is_causal=is_causal dropout_mask=None scale=scale enable_gqa=query size = key size Why change In pre-dispatch export scaled_dot_product_attention executed via flash_attention flash_attention allocates output tensor N H L E see PR assume x N H L E output sdpa In MHA code output then permuted via get L N H E dim tensor x = x permute contiguous viewed via x = x view L N H E During pre autograd dispatch call contiguous traced because flash_attention output after x permute already contiguous which view valid However during nd stage export post-dispatch we run _match variant instead flash get decomposition _match variant returns x N H L E applying x permute returns x L N H E without converting contiguous tensor subsequent view valid export fails solution maintain tensor view decomp exactly same flash variant Really invariant you want maintain pre-dispatch op-output its decomposed representation must tensor same view dims output = output permute contiguous memory_format=torch contiguous_format permute output attn register_inplace aten_op outplace_op register_decomposition aten_op inplace_op args kwargs out = outplace_op args kwargs args copy_ out inplace_op register_decomposition aten baddbmm out_wrapper exact_dtype=True pw_cast_for_opmath baddbmm batch batch beta= alpha= is_floating_point is_complex beta = int beta alpha = int alpha result = torch bmm batch batch isinstance alpha numbers Number alpha = pyrefly ignore unsupported-operation result = result alpha beta == result isinstance beta numbers Number beta = = beta + result register_decomposition aten floor_divide out_wrapper floor_divide other torch div other rounding_mode= floor register_decomposition aten sym_numel sym_numel t functools reduce operator mul t shape register_decomposition aten sum default aten sum out sum_default Tensor dtype Optional torch dtype = None out Optional Tensor = None - Tensor out None aten sum dim_IntList dtype=dtype aten sum IntList_out dtype=dtype out=out register_decomposition aten squeeze default aten squeeze dim squeeze_default Tensor dim Optional int = None handle scalar directly isinstance torch Tensor perform squeeze dim None aten squeeze dims list range dim aten squeeze dims dim register_decomposition torch ops aten _weight_norm_interface _weight_norm_interface v g dim= https github com pytorch pytorch blob f c adc ecbcc fb aten src ATen native WeightNorm cpp#L keep_dim = tuple i i range len v shape i = dim align cuda behavior keep norm float when g bfloat norm_dtype = torch float g dtype == torch bfloat None norm = v norm keep_dim keepdim=True dtype=norm_dtype v g norm g dtype norm register_decomposition aten isin out_wrapper isin elements test_elements assume_unique=False invert=False handle when either elements test_elements Scalars they can t both isinstance elements torch Tensor elements = torch tensor elements device=test_elements device isinstance test_elements torch Tensor invert torch ne elements test_elements torch eq elements test_elements test_elements numel pow elements numel isin_default elements test_elements invert=invert isin_sorting elements test_elements assume_unique=assume_unique invert=invert register_decomposition aten bernoulli default bernoulli torch Tensor generator Optional torch Generator = None - torch Tensor generator None raw_p = torch rand size dtype=torch float device=self device raw_p = torch rand size generator=generator dtype=torch float device=self device p = raw_p dtype p isin_default elements test_elements invert=False elements numel == torch empty_like elements dtype=torch bool expanded_elem_shape = elements shape + test_elements ndim x = elements view expanded_elem_shape dim = tuple range - -test_elements ndim - - res = x == test_elements any dim=dim ~res invert res isin_sorting elements test_elements assume_unique=False invert=False elements_flat = elements flatten test_elements_flat = test_elements flatten assume_unique This same aten implementation For assume_unique=False we cannot use unique here so we use version searchsorted instead all_elements = torch cat elements_flat test_elements_flat sorted_elements sorted_order = torch sort all_elements stable=True duplicate_mask = sorted_elements == sorted_elements - duplicate_mask = torch constant_pad_nd duplicate_mask False invert duplicate_mask = duplicate_mask logical_not mask = torch empty_like duplicate_mask mask = mask index_copy sorted_order duplicate_mask mask elements numel sorted_test_elements _ = torch sort test_elements_flat idx = torch searchsorted sorted_test_elements elements_flat test_idx = torch where idx sorted_test_elements numel idx cmp = sorted_test_elements test_idx == elements_flat cmp = cmp logical_not invert cmp cmp reshape elements shape register_decomposition aten take out_wrapper take index flattened = reshape - flattened index register_decomposition aten resize_as resize_as other memory_format=None memory_format None memory_format = torch contiguous_format memory_format == torch preserve_format memory_format = suggest_memory_format other aten resize other shape memory_format=memory_format register_inplace aten addbmm_ aten addbmm register_inplace aten addmm_ aten addmm register_inplace aten addmv_ aten addmv register_inplace aten baddbmm_ aten baddbmm register_inplace aten fill_ aten fill register_inplace aten gelu_ aten gelu register_inplace aten hardswish_ aten hardswish register_inplace aten hardtanh_ aten hardtanh register_inplace aten hardsigmoid_ aten hardsigmoid register_inplace aten __iand__ aten __and__ register_inplace aten __ilshift__ aten __lshift__ register_inplace aten index_put_ aten index_put register_inplace aten index_reduce_ aten index_reduce register_inplace aten __ior__ aten __or__ register_inplace aten __irshift__ aten __rshift__ register_inplace aten __ixor__ aten __xor__ register_inplace aten leaky_relu_ aten leaky_relu register_inplace aten logit_ aten logit register_inplace aten relu_ aten relu register_inplace aten renorm_ aten renorm register_inplace aten round_ aten round register_inplace aten scatter_ aten scatter register_inplace aten scatter_add_ aten scatter_add register_inplace aten scatter_reduce_ aten scatter_reduce register_inplace aten silu_ aten silu