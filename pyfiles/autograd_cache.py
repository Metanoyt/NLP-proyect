mypy allow-untyped-defs Utils caching outputs AOTAutograd __future__ annotations base contextlib functools json logging os pickle shutil time traceback copy copy typing Any Optional TYPE_CHECKING Union typing_extensions override torch torch _dynamo precompile_context PrecompileContext torch _dynamo trace_rules torch_non_c_binding_in_graph_functions torch _dynamo utils chromium_event_log_active CompileEventLogger counters torch _functorch config torch _inductor codecache _ident add_ephemeral_timeout_increase_for_distributed BypassFxGraphCache create_cache extract_tensor_metadata_for_cache_key FxGraphCache FxGraphCachePickler FxGraphHashDetails GuardedCache sha _hash write_atomic torch _inductor output_code OutputCode torch _inductor runtime runtime_utils cache_dir torch _inductor utils BoxedBool should_use_remote_fx_graph_cache torch _logging LazyString torch _utils_internal log_cache_bypass torch compiler _cache CacheArtifact CacheArtifactFactory CacheArtifactManager torch fx experimental symbolic_shapes hint_int torch utils _triton has_triton_package aot_autograd_result AOTAutogradResult BundledAOTAutogradCacheArtifact BundledAOTAutogradResult BundledCompiledBackward BundledCompiledForward CompiledBackward CompiledForward GenericAOTAutogradResult SerializedGraphModule runtime_wrappers CompilerWrapper SerializableCompiledFunction SubclassMeta schemas AOTAutogradCacheInfo AOTConfig ViewAndMutationMeta noqa F TYPE_CHECKING collections abc Callable torch _inductor compile_fx _CompileFxKwargs torch _inductor cudagraph_utils BoxedDeviceIndex torch _inductor remote_cache JsonDataTy RemoteCache torch fx node Node log = logging getLogger __name__ BypassAOTAutogradCache Exception pass Used signify when FXGraphCache missed when AOTAutogradCache uses FXGraphCacheMiss BypassAOTAutogradCache pass should_use_remote_autograd_cache torch compiler config force_disable_caches False config enable_remote_autograd_cache None config enable_remote_autograd_cache config is_fbcode False torch _utils_internal is_fb_unit_test False try torch _inductor fb remote_cache REMOTE_CACHE_VERSION except ModuleNotFoundError False jk_name = pytorch remote_cache aot_autograd_cache_version REMOTE_CACHE_VERSION = torch _utils_internal justknobs_getval_int jk_name should_use_local_autograd_cache torch compiler config force_disable_caches False config enable_autograd_cache should_bundle_autograd_cache config bundled_autograd_cache torch _dynamo config caching_precompile check_node_safe node Node Checks node only uses supported operators We starting very conservative cacheability constraints incrementally adding more support we expand Note AOTAutograd Cacheability checks - Our cache key computed FX graph produced Dynamo input example values - A node safe same cache key results compiled artifact has same behavior i e set inputs go into our cache key sufficient distinguish its behavior To accomplish safety check we consider following functions safe - Public functions under modules torch torch functional torch nn functional these allowed graph dynamo so we can assume they safe cache - method calls base tensor types - Any call_module dynamo deemed safe allow AOTAutograd trace - Non callable nodes such placeholder output get_attr The test suite test_aot_autograd_cache py AOTAutogradCachePicklerTests tries its best fully cover specify behavior SAFE_TORCH_MODULES = torch functional torch nn functional SAFE_TORCH_FUNCTIONS = torch Size torch Tensor torch sym_int torch _sym_sqrt torch sym_float torch sym_sum SAFE_NON_TORCH_FUNCTIONS = einops einops rearrange einops einops repeat is_public_torch_api target Don t blindly allow private functions torch namespace is_private = target __name__ startswith _ getattr target __module__ None SAFE_TORCH_MODULES is_private is_safe_torch_function target Allowlisted torch functions function_name = f target __module__ target __name__ Allow torch autograd function FunctionCtx custom autograd functions allowed function_name == torch autograd function FunctionCtx torch _functorch config autograd_cache_allow_custom_autograd_functions Functions torch_non_c_binding_in_graph_functions guaranteed cache safe See NOTE Cacheability in-graph torch functions function_name torch_non_c_binding_in_graph_functions function_name SAFE_TORCH_FUNCTIONS function_name torch _inductor config unsafe_marked_cacheable_functions is_cacheable_function target isinstance target torch _ops OpOverload torch _ops OpOverloadPacket True is_public_torch_api target True Technically FXGraphCache _check_for_hop already checks better error earlier anyway isinstance target torch _ops HigherOrderOperator target cacheable is_builtin_fun_or_type = type target __name__ == builtin_function_or_method is_builtin_fun_or_type True is_safe_torch_function target True function_name = f target __module__ target __name__ function_name SAFE_NON_TORCH_FUNCTIONS True False is_tensor target Tensors always have example values meta field example_value target meta I d love use match statement here wasn t introduced until py node op == call_function node meta node meta get is_wrapped False This fx wrap function By default we BypassAOTAutogradCache unknown functions But user explicitly specified cache hash - allow cache node meta get user_cache_hash None is_cacheable_function node target module = getattr node target __module__ None name = getattr node target __name__ None raise BypassAOTAutogradCache f Unsupported call_function target node target \n Function module module \nFunction name name node op == call_method method_name = node target method_target = node args Only support method calls base tensors is_tensor method_target module = getattr method_target __module__ None name = getattr method_target __name__ None raise BypassAOTAutogradCache f Unsupported call_method target method_target \nMethod module module \nMethod name name type method_name str type method_name __name__ = method_descriptor raise BypassAOTAutogradCache f Unsupported call_method method node target method_name Cache safe node op placeholder get_attr call_module output Assumption today call_module being safe op today only call_module ops can show up graph come built-in-nn-modules dynamo assumes safe trace If dynamo assumes they safely blindly trace then they should safe cache well steady-state some time H we shouldn t see these anymore once inline builtin nn modules default We do allow user made nn modules graph today only function calls pass raise BypassAOTAutogradCache f Unsupported node op node op check_cacheable gm torch fx GraphModule Checks graph module only uses supported operators nodes = gm graph nodes torch _inductor config freezing raise BypassAOTAutogradCache Cannot cache graph freezing enabled torch _inductor config fx_graph_cache should_use_remote_fx_graph_cache raise BypassAOTAutogradCache FX graph cache enabled tracing_context = torch _guards TracingContext try_get tracing_context tracing_context fakify_first_call raise BypassAOTAutogradCache Won t cache graph fakify_first_call enabled node nodes check_node_safe node Saved tensors hooks globally set subgraphs used explicitly main graph They inlined aot_autograd graphs Subgraphs only used caching logic hasattr gm saved_tensors_hooks_pack_ check_cacheable gm saved_tensors_hooks_pack_ type ignore arg-type We have guarantee unpack sugraph existence pack subgraph exists check_cacheable gm saved_tensors_hooks_unpack_ type ignore arg-type AOTAutogradCacheDetails FxGraphHashDetails Object capture all details dynamo graph module relevant computing safe stable cache key AOTAutograd get_triton_source_codes_from_gm gm torch fx GraphModule assert has_triton_package Triton available triton_kernels = module gm modules isinstance module torch fx GraphModule continue node module graph nodes isinstance node target torch _ops OpOverloadPacket attrs = node target _dir attr attrs custom_op = getattr node target attr None kernels = torch _library triton get_triton_kernels_for_op custom_op _name triton_kernels extend kernels isinstance node target torch _ops OpOverload kernels = torch _library triton get_triton_kernels_for_op node target _name triton_kernels extend kernels triton_kernel_source_codes = torch _inductor codegen wrapper user_defined_triton_kernel_transitive_closure_source_code kernel triton_kernels triton runtime autotuner Autotuner isinstance kernel Autotuner Grab Inner JITFunction kernel = kernel fn source_codes = user_defined_triton_kernel_transitive_closure_source_code kernel triton_kernel_source_codes append source_codes triton_kernel_source_codes __init__ gm torch fx GraphModule example_inputs aot_config AOTConfig fx_config _CompileFxKwargs FxGraphHashDetails contains all keys related inductor Also includes some system info aot_config = aot_config grad_enabled = torch is_grad_enabled disable_amp = torch _C _is_any_autocast_enabled deterministic_algorithms = torch are_deterministic_algorithms_enabled autograd_config = config save_config saved_tensors_hooks_fx_wrap_cache_hashes tuple list str list str = has_triton_package triton_kernel_source_codes = get_triton_source_codes_from_gm gm hasattr gm saved_tensors_hooks_pack_ _add_wrapped_user_cache_hashes _gm _l node _gm graph nodes node meta node meta get is_wrapped False _l append node meta user_cache_hash _add_wrapped_user_cache_hashes gm saved_tensors_hooks_pack_ saved_tensors_hooks_fx_wrap_cache_hashes _add_wrapped_user_cache_hashes gm saved_tensors_hooks_unpack_ saved_tensors_hooks_fx_wrap_cache_hashes try FXGraphCache has constraints what can pickled its inductor config Check gm cacheable inductor first raises exception also bypass our end FxGraphCache _check_can_cache gm super __init__ gm example_inputs fx_config except BypassFxGraphCache e Sometimes inductor configs unpickleable can fail raise BypassAOTAutogradCache str e e AOTAutogradCachePickler FxGraphCachePickler __init__ gm torch fx GraphModule super __init__ gm pyrefly ignore bad-override dispatch_table dict dispatch_table update AOTConfig functools partial _reduce_aot_config torch Tensor functools partial _reduce_tensor _reduce_aot_config aot_config AOTConfig Reduce config stable key caching _ident aot_config num_params_buffers aot_config keep_inference_input_mutations aot_config is_export aot_config no_tangents aot_config dynamic_shapes aot_config aot_autograd_arg_pos_to_source aot_config enable_log aot_config pre_dispatch _reduce_tensor tensor Reduce tensor stable key caching metadata = extract_tensor_metadata_for_cache_key tensor _ident metadata contextlib contextmanager normalize_placeholder_names gm torch fx GraphModule Context manager normalizes placeholder names graph module This used while generating cache key AOTAutogradCache so two graphs isomorphic when normalizing names can hit same cache entry This safe because nothing underneath AOTAutograd uses node names original dynamo graph AOTAutograd re-traces its own nodes guards terms original sources rather than placeholder names Standalone inductor we re bypassing AOTAutogradCache anyway so graph as-is config autograd_cache_normalize_inputs hasattr gm graph yield Track all old state placeholders old_placeholder_names = old_used_names = copy gm graph _graph_namespace _used_names i = n gm graph find_nodes op= placeholder sort=True n type = torch SymInt _rename renames node body function doesn t change raw name node target So we also set raw_name node target new placeholder name new_placeholder_name = f p_ i old_placeholder_names append n name n target n target = new_placeholder_name n _rename new_placeholder_name i += gm recompile try yield finally Used_names contains all our old placeholder names so we clear temporarily when we put them back gm graph _graph_namespace _used_names = set Restore placeholder names i = n gm graph find_nodes op= placeholder sort=True n type = torch SymInt name target = old_placeholder_names i n target = target n _rename name i += assert i == len old_placeholder_names Now restore old namespace s used names gm graph _graph_namespace _used_names = old_used_names gm recompile autograd_cache_key gm torch fx GraphModule example_inputs config AOTConfig fx_config _CompileFxKwargs TODO add args parameters - tuple str list str Generate unique hash FX graph caching check_cacheable gm has_triton_package Due https github com triton-lang triton issues triton AOTAutogradCache may cause us attempt load cache entry without initializing CUDA context autograd thread Without caching we naturally do initialization when tracing through graph autograd engine triton triton __version__ raise BypassAOTAutogradCache AOTAutogradCache requires triton details = AOTAutogradCacheDetails gm example_inputs config fx_config pickler = AOTAutogradCachePickler gm The prefix distinguishes among other kinds objects we cache key = + pickler get_hash details debug_lines = pickler debug_lines details log debug Autograd graph cache hash details key s \n s key LazyString lambda \n join debug_lines key debug_lines contextlib contextmanager sanitize_gm_for_cache gm torch fx GraphModule Clears few fields dynamo supplied Graph Module stable between graph inputs don t affect inductor aotdispatch correctness These fields can used code calling into aotdispatch namely dynamo so we can t null them out completely To ensure these fields accessed inductor aotdispatch we clear them during AOTAutogradCache load then put them back before returning This way we generate cache key based off canonical graph without these fields also guarantee they aren t used affect cache s output Mapping each field default value IGNORED_FIELDS dict str Any = meta metadata used export compile_subgraph_reason None Used dynamo only logging no change inductor autograd behavior _param_name_to_source None Encapsulated aot_config aot_autograd_arg_pos_to_source _backend_id None saved_fields = field default_value IGNORED_FIELDS items saved_fields field = getattr gm field None Clear field setattr gm field default_value try normalize_placeholder_names gm yield finally field value saved_fields items setattr gm field value CacheArtifactFactory register AOTAutogradCacheArtifact CacheArtifact override populate_cache AOTAutogradCache _write_to_local_cache key content override staticmethod type aot_autograd AOTAutogradCache GuardedCache GenericAOTAutogradResult Caches results running AOTAutograd This mostly handles save load logic whereas AOTAutogradResult handles wrapping unwrapping logic Cache Inputs AOTAutogradCacheDetails - AOTAutogradCache takes following inputs which analogous inputs given AOTAutograd dynamo - A fx graph module generated dynamo - A list args which consists - Symint inputs graph generated dynamo - The real tensor inputs which inductor uses cudagraphs - Notably real tensor inputs don t have symints their metadata AOTAutograd then retraces those real tensor arguments into FakeTensors later during execution - A set global configurations affect AOTAutograd Inductor behavior It then generates cache key given these values Notably means AOTAutogradCache currently specializes sizes strides real tensor inputs when dynamic shapes turned In later PR we ll likely generate cache key based FakeTensors AOTAutograd generates based real tensor inputs which can contain symints Cache Outputs AOTAutogradResult - AOTAutogradCache caches following values - The compiled forward backward functions inductor via keys FXGraphCache - Metadata reconstruct AOTModule compiled inductor artifacts - See AOTAutogradResult more info Note Caching guards generated AOTAutograd Inductor AOTAutograd inductor both can introduce new guards shape environment FXGraphCache saves guards each compiled graph inductor generates On cache hit AOTAutograd reloads compiled forward backward functions FXGraphCache giving new symint arguments input args FXGraphCache uses those symints its saved guards repopulate ShapeEnv guards No new guards generated into shape env after inductor finishes compiling so guards saved inductor sufficient correctness both AOTAutograd Inductor s caches staticmethod clear Clear cache try shutil rmtree AOTAutogradCache _get_tmp_dir except FileNotFoundError pass staticmethod try_load mod Union torch fx GraphModule torch _dynamo utils GmWrapper args aot_config AOTConfig cudagraphs BoxedBool boxed_forward_device_index Optional BoxedDeviceIndex local bool remote bool - Optional Callable Load result cache reconstruct runtime wrapper around object gm = mod gm isinstance mod torch _dynamo utils GmWrapper mod sanitize_gm_for_cache gm compiled_fn = None cache_info dict str Any = cache_key = None debug_lines list str = cache_event_time = time time_ns cache_state = None fx_config _CompileFxKwargs = cudagraphs cudagraphs boxed_forward_device_index boxed_forward_device_index try cache_key debug_lines = autograd_cache_key gm args aot_config fx_config result Optional tuple GenericAOTAutogradResult bytes = AOTAutogradCache _lookup cache_key local remote args cache_info aot_config result None entry pickled_content = result compiled_fn = entry wrap_post_compile args aot_config fx_config Make compiled_fn serializable where serialize function just makes copy original entry before post compile via pickled content compiled_fn = SerializableCompiledFunction compiled_fn lambda pickle loads pickled_content log info AOTAutograd cache hit key s cache_key counters aot_autograd autograd_cache_hit += cache_state = hit cache_event_time = time time_ns forward_time_saved = entry forward_time_taken_ns e backward_time_saved = entry backward_time_taken_ns e cache_info update forward_time_saved_ms forward_time_saved backward_time_saved_ms backward_time_saved time_saved_ms forward_time_saved + backward_time_saved time_saved_ns = entry forward_time_taken_ns + entry backward_time_taken_ns TODO should we use same field remote cache time saved both FXGraphCache AOTAutogradCache get_metrics_context increment ephemeral_increase = add_ephemeral_timeout_increase_for_distributed time_saved_ns = cache_info ephemeral_timeout_increase = ephemeral_increase compiled_fn None log info AOTAutograd cache miss key s cache_key counters aot_autograd autograd_cache_miss += cache_state = miss cache_event_time = time time_ns Count missing FXGraphCache miss bypass except FXGraphCacheMiss e counters aot_autograd autograd_cache_miss += cache_state = miss config strict_autograd_cache torch _dynamo config strict_precompile raise e Most often BypassAOTAutogradCache there s ever different reason we can t cache we still never want hard throw exception since we can always fallback cache bypass As example user calls autograd via standalone inductor we will sometimes get GraphModule doesn t actually have ` graph ` Instead checking every single case we safely catch exception those cases except Exception e cache_key = None counters aot_autograd autograd_cache_bypass += log info Bypassing autograd cache due s e noqa G cache_state = bypass cache_event_time = time time_ns cache_info cache_bypass_reason = str e cache_info cache_bypass_exception_type = type e __name__ cache_info cache_bypass_traceback = traceback format_exc split \n TODO gets logged implicitly cache_bypass_reason here we explicitly log into tlparse We may want log extra column Scuba though cache_info cache_bypass_hard_exception = isinstance e BypassAOTAutogradCache remote log_cache_bypass bypass_aot_autograd str e config strict_autograd_cache torch _dynamo config strict_precompile raise e compiled_fn None Set cache key so we can save cache result later symints = AOTAutogradCache _filter_backed_symints args cache_key None aot_config cache_info = AOTAutogradCacheInfo cache_key time time_ns forward_symints=symints cache_info update key cache_key cache_state cache_state components debug_lines chromium_event_log_active CompileEventLogger instant f autograd_cache_ cache_state metadata=cache_info time_ns=cache_event_time CompileEventLogger try_add_pt _compile backend_compile cache_state=cache_state cache_event_time=cache_event_time key=cache_info get key components=cache_info get components cache_bypass_reason=cache_info get cache_bypass_reason remote_cache_enabled=remote local_cache_enabled=local torch _logging trace_structured artifact metadata_fn=lambda name f aotautograd_cache_ cache_state encoding json payload_fn=lambda json dumps cache_info compiled_fn classmethod generate_guards_expression cls type AOTAutogradCache cache_info AOTAutogradCacheInfo - Optional str shape_env = cls _get_shape_env assert shape_env None symints = cache_info forward_symints guards = shape_env get_pruned_guards symints shape_env produce_guards_expression placeholders=symints guards=guards classmethod _get_tmp_dir cls type AOTAutogradCache - str Get toplevel temporary directory storing compiled graphs os path join cache_dir aotautograd classmethod _get_tmp_dir_for_key cls type AOTAutogradCache key - str Get toplevel temporary directory storing compiled graphs os path join cls _get_tmp_dir key staticmethod evaluate_guards guard_expr str hints Union list int list torch SymInt torch _inductor config unsafe_skip_cache_dynamic_shape_guards True shape_env = AOTAutogradCache _get_shape_env assert shape_env None result = shape_env evaluate_guards_expression guard_expr hints result staticmethod _lookup key str local bool remote bool args list Any cache_info dict str Any aot_config Optional AOTConfig - Optional tuple GenericAOTAutogradResult bytes Given key generated AOTAutogradCachePickler look up its location cache remote_cache Optional RemoteCache JsonDataTy = None remote remote_cache = AOTAutogradCache get_remote_cache symints = AOTAutogradCache _filter_backed_symints args hints = hint_int s s symints entry = None pickled_content = None try entry pickled_content guard_info = AOTAutogradCache find_guarded_entry key local remote_cache AOTAutogradCache evaluate_guards hints entry None guard_info cache_status_detailed == guard_miss counters aot_autograd autograd_cache_guard_miss += cache_info update guard_info pickled_content None CacheArtifactManager record_artifact AOTAutogradCacheArtifact type key pickled_content should_bundle_autograd_cache aot_config None aot_config precompile_backend_id None NB We don t want use cached aot_config precompile_backend_id because we set None save even we didn t new run cache hit has new backend id associated PrecompileContext record_artifact BundledAOTAutogradCacheArtifact aot_config precompile_backend_id entry except Exception e log info AOTAutograd cache unable load compiled graph s e noqa G config strict_autograd_cache raise e entry None assert pickled_content None entry pickled_content None staticmethod _write_to_local_cache key str content bytes Write entry local cache subdir = AOTAutogradCache _get_tmp_dir_for_key key os path exists subdir os makedirs subdir exist_ok=True Use hash serialized entry get unique file name The specific name doesn t matter since lookup involves iterating over all entries parent subdir path = os path join subdir sha _hash content log info Writing AOTAutograd cache entry s path write_atomic path content staticmethod save key str entry GenericAOTAutogradResult remote bool Save single entry into cache try entry pre_save content = pickle dumps entry CacheArtifactManager record_artifact AOTAutogradCacheArtifact type key content should_bundle_autograd_cache entry sanitized_aot_config precompile_backend_id None precompile_key = entry sanitized_aot_config precompile_backend_id artifact = BundledAOTAutogradCacheArtifact precompile_key entry Now we re saving precompile_backend_id field no longer useful remove entry entry sanitized_aot_config precompile_backend_id = None PrecompileContext record_artifact artifact AOTAutogradCache _write_to_local_cache key content counters aot_autograd autograd_cache_saved += except BypassAOTAutogradCache e counters aot_autograd autograd_cache_bypass += log info Bypassing autograd cache due s e noqa G remote log_cache_bypass bypass_aot_autograd str e None except Exception e log info AOTAutograd cache unable serialize compiled graph s e noqa G remote log_cache_bypass bypass_aot_autograd Unable serialize + str e config strict_autograd_cache raise e None remote remote_cache Optional RemoteCache JsonDataTy = AOTAutogradCache get_remote_cache remote_cache None time_taken_ms = int entry forward_time_taken_ns + entry backward_time_taken_ns e cache_data JsonDataTy = data base b encode content decode ascii time_taken_ms time_taken_ms remote_cache put key cache_data staticmethod functools cache get_remote_cache - Optional RemoteCache JsonDataTy Attempts load remote cache returns None error cache_id = autograd-experimental create_cache cache_id config is_fbcode FbRemoteAOTAutogradCache RemoteAOTAutogradCache staticmethod make_entry compiled_fw_func OutputCode compiled_bw_func Optional OutputCode aot_joint_graph_str Optional str aot_forward_graph_str Optional str aot_backward_graph_str Optional str runtime_metadata ViewAndMutationMeta dispatch_wrappers list CompilerWrapper maybe_subclass_meta Optional SubclassMeta num_fw_outs_saved_for_bw Optional int indices_of_inps_to_detach list int forward_time_taken_ns int backward_time_taken_ns int sanitized_aot_config AOTConfig guards_expr Optional str backward_state_indices Optional list int num_symints_saved_for_bw Optional int serialized_bw_module Optional SerializedGraphModule - GenericAOTAutogradResult should_bundle_autograd_cache Helper function unwrap all wrappers we added during aotdispatch They get reapplied cache load unwrap_output_code obj while hasattr obj __wrapped__ obj = obj __wrapped__ assert isinstance obj OutputCode obj compiled_fw_graph = unwrap_output_code compiled_fw_func bundled_compiled_forward = BundledCompiledForward compiled_fw_graph bundled_compiled_backward = None compiled_bw_func None assert backward_state_indices None assert num_symints_saved_for_bw None compiled_bw_graph = unwrap_output_code compiled_bw_func bundled_compiled_backward = BundledCompiledBackward compiled_bw_graph backward_state_indices num_symints_saved_for_bw BundledAOTAutogradResult compiled_fw=bundled_compiled_forward compiled_bw=bundled_compiled_backward aot_joint_graph_str=aot_joint_graph_str aot_forward_graph_str=aot_forward_graph_str aot_backward_graph_str=aot_backward_graph_str runtime_metadata=runtime_metadata dispatch_wrappers=dispatch_wrappers maybe_subclass_meta=maybe_subclass_meta num_fw_outs_saved_for_bw=num_fw_outs_saved_for_bw indices_of_inps_to_detach=indices_of_inps_to_detach forward_time_taken_ns=forward_time_taken_ns backward_time_taken_ns=backward_time_taken_ns sanitized_aot_config=sanitized_aot_config guards_expr=guards_expr serialized_bw_module=serialized_bw_module fw_key = getattr compiled_fw_func _fx_graph_cache_key None fw_debug_lines = getattr compiled_fw_func _fx_graph_cache_debug_lines assert fw_key None compiled_forward = CompiledForward fx_graph_cache_info= fw_key fw_debug_lines fx_graph_guard_expr=getattr compiled_fw_func guards_expr None compiled_backward = None compiled_bw_func None bw_key = getattr compiled_bw_func _fx_graph_cache_key None bw_debug_lines = getattr compiled_bw_func _fx_graph_cache_debug_lines assert bw_key None assert backward_state_indices None assert num_symints_saved_for_bw None compiled_backward = CompiledBackward fx_graph_cache_info= bw_key bw_debug_lines fx_graph_guard_expr=getattr compiled_bw_func guards_expr None backward_state_indices=backward_state_indices num_symints_saved_for_bw_=num_symints_saved_for_bw AOTAutogradResult compiled_fw=compiled_forward compiled_bw=compiled_backward aot_joint_graph_str=aot_joint_graph_str aot_forward_graph_str=aot_forward_graph_str aot_backward_graph_str=aot_backward_graph_str runtime_metadata=runtime_metadata dispatch_wrappers=dispatch_wrappers maybe_subclass_meta=maybe_subclass_meta num_fw_outs_saved_for_bw=num_fw_outs_saved_for_bw indices_of_inps_to_detach=indices_of_inps_to_detach forward_time_taken_ns=forward_time_taken_ns backward_time_taken_ns=backward_time_taken_ns sanitized_aot_config=sanitized_aot_config guards_expr=guards_expr serialized_bw_module=serialized_bw_module