Copyright c Meta Platforms Inc affiliates Owner s oncall distributed os unittest collections abc Callable functools wraps typing Any numpy np torch torch nn torch distributed tensor DeviceMesh distribute_module distribute_tensor Replicate Shard torch testing _internal common_utils run_tests TestCase wrapper check xla test requirements with_xla func Callable - Callable assert func None wraps func pyre-ignore wrapper args tuple object kwargs dict str Any type ignore misc - None TODO yeounoh replace xr use_spmd when we deprecate flag os environ XLA_USE_SPMD = try torch_xla type ignore noqa F except ImportError exc raise unittest SkipTest torch_xla installed exc device_type = xla func args kwargs type ignore misc os environ XLA_USE_SPMD = wrapper DTensorXLAIntegrationTest TestCase SimpleLinear nn Module __init__ - None super DTensorXLAIntegrationTest SimpleLinear __init__ fc = nn Linear relu = nn ReLU fc = nn Linear forward x y = relu fc x z = fc y z with_xla test_xla_distribute_tensor_ d_shard torch_xla runtime xr type ignore device_count = xr global_runtime_device_count device_count device_mesh = DeviceMesh xla list range device_count shard_spec = Shard requires_grad True False tensor_to_shard = torch randn device_count requires_grad=requires_grad dist_tensor = distribute_tensor tensor_to_shard device_mesh shard_spec TODO yeounoh switch DTensor API when XLAShardedTensor inherits DTensor assert type dist_tensor __name__ == XLAShardedTensor global_tensor = dist_tensor global_tensor type ignore attr-defined assertEqual global_tensor size torch Size device_count local_tensor = dist_tensor local_shards data assertEqual local_tensor size torch Size requires_grad assertTrue dist_tensor global_tensor requires_grad assertTrue dist_tensor is_leaf with_xla test_xla_distribute_tensor_ d_replicate torch_xla runtime xr type ignore device_count = xr global_runtime_device_count device_mesh = DeviceMesh xla list range device_count shard_spec = Replicate requires_grad True False tensor_to_shard = torch randn device_count requires_grad=requires_grad dist_tensor = distribute_tensor tensor_to_shard device_mesh shard_spec TODO yeounoh switch DTensor API when XLAShardedTensor inherits DTensor assert type dist_tensor __name__ == XLAShardedTensor global_tensor = dist_tensor global_tensor type ignore attr-defined assertEqual global_tensor size torch Size device_count local_tensor = dist_tensor local_shards data assertEqual local_tensor size torch Size device_count requires_grad assertTrue dist_tensor global_tensor requires_grad assertTrue dist_tensor is_leaf with_xla test_xla_distribute_tensor_ d torch_xla runtime xr type ignore device_count = xr global_runtime_device_count device_count device_mesh = DeviceMesh xla np array range device_count reshape device_count shard_spec = Replicate Shard requires_grad True False tensor_to_shard = torch randn device_count requires_grad=requires_grad dist_tensor = distribute_tensor tensor_to_shard device_mesh shard_spec TODO yeounoh switch DTensor API when XLAShardedTensor inherits DTensor assert type dist_tensor __name__ == XLAShardedTensor global_tensor = dist_tensor global_tensor type ignore attr-defined assertEqual global_tensor size torch Size device_count local_tensor = dist_tensor local_shards data assertEqual local_tensor size torch Size requires_grad assertTrue dist_tensor global_tensor requires_grad assertTrue dist_tensor is_leaf with_xla text_xla_distribute_module torch_xla type ignore torch_xla core xla_model xm type ignore torch_xla runtime xr type ignore model = SimpleLinear xm xla_device device_count = xr global_runtime_device_count device_mesh = DeviceMesh xla list range device_count shard_params mod_name mod mesh shard_spec = Shard annotate fc fc isinstance mod nn Linear _ param mod named_parameters annotate parameter tensors directly distribute_tensor param mesh shard_spec sharded_model = distribute_module model device_mesh shard_params assertTrue torch_xla _XLAC _get_xla_sharding_spec sharded_model fc weight = assertTrue torch_xla _XLAC _get_xla_sharding_spec sharded_model fc weight = __name__ == __main__ run_tests