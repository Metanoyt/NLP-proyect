mypy allow-untyped-defs typing Optional torch torch nn functional F expanded_weights_utils set_grad_sample_if_exists unpack_expanded_weight_or_tensor THRESHOLD = conv_picker func conv dOpt conv dOpt conv dOpt func F conv d conv dOpt func F conv d conv dOpt assert func F conv d conv dOpt conv_args_and_kwargs kwarg_names expanded_args_and_kwargs args = expanded_args_and_kwargs len expanded_args_and_kwargs - len kwarg_names kwargs = expanded_args_and_kwargs len expanded_args_and_kwargs - len kwarg_names kwargs = dict zip kwarg_names kwargs strict=True conv_normalizer args kwargs conv_normalizer input weight bias=None stride= padding= dilation= groups= input weight bias bias stride stride padding padding dilation dilation groups groups conv_input_for_string_padding func padding_style input dilation kernel_size padding_style == valid input padding = int_padding_for_string_padding func padding_style dilation kernel_size F pad input padding int_padding_for_string_padding func padding_style dilation kernel_size get_dilation i dilation i isinstance dilation tuple dilation padding_style == same padding list int = F pad needs padding reverse order what conv expects i range conv_picker func - - padding += conv_padding_for_same get_dilation i kernel_size i padding padding_style == valid conv_picker func raise RuntimeError f got padding type padding_style only accept same valid conv_padding_for_same dilation kernel_size total_pad = dilation kernel_size - left_pad = total_pad right_pad = total_pad - left_pad left_pad right_pad conv_backward func ctx grad_output weight_grad_sample weight batch_size THRESHOLD groups == conv_group_weight_grad_sample ctx input grad_output weight_shape stride padding dilation batch_size func conv_unfold_weight_grad_sample ctx input grad_output weight_shape kernel_size stride padding dilation groups func expand param isinstance param int conv_picker func param param param param param param param calc_total_padding func was_same padding dilation kernel_size was_same all_padding = int_padding_for_string_padding func same dilation kernel_size F pad needs padding reverse order what conv expects total_padding = tuple all_padding i + all_padding i - i range len all_padding - - - total_padding tuple pad pad padding weight_shape = ctx weight shape stride padding dilation groups = expand ctx stride expand ctx padding expand ctx dilation ctx groups kernel_size = weight_shape i i range conv_picker func batch_size = ctx batch_size results list Optional torch Tensor = results append None kwarg names results append None op reference same padding may give uneven padding either side so we need separate padding attr total padding total_padding = calc_total_padding func ctx was_same_padding padding dilation kernel_size ctx input_required_grad output_padding = input_dims = conv_picker func i range input_dims input_dim = ctx orig_input_shape + i output_padding append total_padding i + input_dim - kernel_size i dilation i - dilation i + stride i weight_ = unpack_expanded_weight_or_tensor ctx weight transpose_func = conv_picker func F conv_transpose d F conv_transpose d F conv_transpose d out = transpose_func grad_output weight_ None stride padding tuple output_padding groups dilation ctx was_same_padding i range len total_padding out = torch narrow out + i total_padding i ctx orig_input_shape + i results append out results append None weight bias don t compute batched gradients no other arguments differentiable results = results + None set grad_sample field weight bias per sample gradients set_grad_sample_if_exists ctx weight weight_grad_sample set_grad_sample_if_exists ctx bias lambda _ grad_output reshape grad_output shape - sum dim= tuple results conv_unfold_weight_grad_sample input grad_output weight_shape kernel_size stride padding dilation groups func numpy np n = input shape in_channels = input shape unfold_func = conv_picker func lambda F unfold input unsqueeze - kernel_size= kernel_size dilation= dilation padding= padding stride= stride lambda F unfold input kernel_size dilation=dilation padding=padding stride=stride lambda unfold d input kernel_size padding stride dilation input = unfold_func grad_output = grad_output reshape n - input shape - n=batch_sz o=num_out_channels p= num_in_channels groups kernel_sz weight_grad_sample = torch einsum noq npq- nop grad_output input rearrange above tensor extract diagonals pyrefly ignore no-matching-overload weight_grad_sample = weight_grad_sample view n groups - groups int in_channels groups np prod kernel_size weight_grad_sample = torch einsum ngrg - ngr weight_grad_sample contiguous shape = n + list weight_shape weight_grad_sample = weight_grad_sample view shape weight_grad_sample conv_group_weight_grad_sample input grad_output weight_shape stride padding dilation batch_size func I = input shape O = grad_output shape input_ = input transpose grad_output_ = grad_output view grad_output shape grad_output shape grad_output shape weight_grad_sample = func input_ grad_output_ None stride=dilation padding=padding dilation=stride groups=batch_size input_dims = conv_picker func i range input_dims weight_grad_sample = weight_grad_sample narrow i weight_shape i weight_grad_sample = weight_grad_sample view I batch_size O weight_grad_sample shape weight_grad_sample = weight_grad_sample movedim weight_grad_sample unfold d tensor kernel_size padding stride dilation r Extract sliding local blocks batched input tensor ` torch nn Unfold ` only supports D inputs batched image-like tensors This method implements same action D inputs Args tensor An input tensor shape ` ` B C D H W ` ` kernel_size size sliding blocks padding implicit zero padding added both sides input stride stride sliding blocks input spatial dimensions dilation spacing between kernel points Returns A tensor shape ` ` B C np prod kernel_size L ` ` where L - output spatial dimensions See ` torch nn Unfold ` more details Example xdoctest +SKIP B C D H W = tensor = torch arange B C D H W + view B C D H W unfold d tensor kernel_size= padding= stride= shape torch Size numpy np len tensor shape = raise ValueError f Input tensor must shape B C D H W Got tensor shape dilation = raise NotImplementedError f dilation= dilation supported batch_size channels _ _ _ = tensor shape Input shape B C D H W tensor = F pad tensor padding padding padding padding padding padding Output shape B C D+ padding H+ padding W+ padding tensor = tensor unfold dimension= size=kernel_size step=stride tensor = tensor unfold dimension= size=kernel_size step=stride tensor = tensor unfold dimension= size=kernel_size step=stride Output shape B C D_out H_out W_out kernel_size kernel_size kernel_size For D_out H_out W_out definitions see ` torch nn Unfold ` tensor = tensor permute Output shape B D_out H_out W_out C kernel_size kernel_size kernel_size tensor = tensor reshape batch_size - channels np prod kernel_size transpose Output shape B D_out H_out W_out C kernel_size kernel_size kernel_size tensor