mypy allow-untyped-defs logging dataclasses dataclass typing Any Optional torch torch _dynamo utils counters torch _inductor runtime triton_compat tl torch _inductor virtualized V torch utils _triton has_triton ir ChoiceCaller Layout TensorBox lowering register_lowering select_algorithm autotune_select_algorithm ExternKernelChoice realize_inputs TritonTemplate utils get_gpu_shared_memory get_num_sms has_free_symbols use_aten_gemm_kernels use_triton_template mm_common _is_static_problem check_supported_striding persistent_grouped_mm_grid log = logging getLogger __name__ aten = torch ops aten dataclass Config kwargs dict str int num_stages int num_warps int _NV_CONFIGS = Config BLOCK_M block_size_m BLOCK_N block_size_n BLOCK_K block_size_k NUM_CONSUMER_GROUPS num_stages=num_stages num_warps=num_warps block_size_m block_size_n block_size_k num_stages num_warps grouped_mm_configs _NV_CONFIGS early_config_prune g m dtsize configs named_args pruned_configs = config configs kw = config kwargs BLOCK_M BLOCK_N BLOCK_K num_stages num_warps num_consumer_groups = kw BLOCK_M kw BLOCK_N kw BLOCK_K config num_stages config num_warps getattr config num_consumer_groups Prune NV configs depending g m has_free_symbols g m a_is_ d b_is_ d = named_args A_IS_ D named_args B_IS_ D m_avg = m g a_is_ d b_is_ d m m_avg = BLOCK_M continue m_avg = BLOCK_M continue m_avg = BLOCK_M = continue BLOCK_M = continue make sure we have enough smem max_shared_memory = get_gpu_shared_memory required_shared_memory = BLOCK_M + BLOCK_N BLOCK_K num_stages dtsize required_shared_memory max_shared_memory continue use_warp_specialization = num_consumer_groups = make sure we can partition ws use_warp_specialization num_warps = continue tritongpu-warp-spec-data-partition m_slice = BLOCK_M num_consumer_groups n_slice = BLOCK_N num_consumer_groups m_slice n_slice continue pruned_configs append config pruned_configs triton_grouped_mm_source = r triton triton language tl triton jit do_tma_loads g a_desc b_desc m_offset n_offset k_offset BLOCK_M tl constexpr BLOCK_N tl constexpr BLOCK_K tl constexpr - A_IS_ D - A_IS_K_MAJOR = a_desc load m_offset k_offset - = a_desc load k_offset m_offset - endif - - A_IS_K_MAJOR = a_desc load g m_offset k_offset reshape BLOCK_M BLOCK_K - = a_desc load g k_offset m_offset reshape BLOCK_K BLOCK_M - endif - endif - B_IS_ D - B_IS_K_MAJOR b = b_desc load n_offset k_offset - b = b_desc load k_offset n_offset - endif - - B_IS_K_MAJOR b = b_desc load g n_offset k_offset reshape BLOCK_N BLOCK_K - b = b_desc load g k_offset n_offset reshape BLOCK_K BLOCK_N - endif - endif b triton jit do_mma b accumulator - USE_FAST_ACCUM - A_IS_K_MAJOR B_IS_K_MAJOR accumulator = tl dot b T accumulator - A_IS_K_MAJOR B_IS_K_MAJOR accumulator = tl dot b accumulator - A_IS_K_MAJOR B_IS_K_MAJOR accumulator = tl dot T b T accumulator - accumulator = tl dot T b accumulator - endif - - A_IS_K_MAJOR B_IS_K_MAJOR accumulator += tl dot b T - A_IS_K_MAJOR B_IS_K_MAJOR accumulator += tl dot b - A_IS_K_MAJOR B_IS_K_MAJOR accumulator += tl dot T b T - accumulator += tl dot T b - endif - endif accumulator - SCALED - A_IS_ D B_IS_ D def_kernel a_ptr b_ptr scale_a_ptr scale_b_ptr offsets_ptr - def_kernel a_ptr b_ptr scale_a_ptr scale_b_ptr - endif - - A_IS_ D B_IS_ D def_kernel a_ptr b_ptr offsets_ptr - def_kernel a_ptr b_ptr - endif - endif tidx = tl program_id INDEX_DTYPE - set M_IS_VARYING = A_IS_ D B_IS_ D - set N_IS_VARYING = A_IS_ D B_IS_ D - set K_IS_VARYING = A_IS_ D B_IS_ D - A_IS_ D - B_IS_ D G = size offsets_ptr - G = size b_ptr - endif - - B_IS_ D G = size a_ptr - G = size a_ptr - endif - endif b_ptr tensor given its last two dims transposed revert here M = size a_ptr - N = size b_ptr - K = size a_ptr - A_STRIDE_M = stride a_ptr - A_STRIDE_K = stride a_ptr - - A_IS_ D A_STRIDE_G = stride a_ptr - SCALED SCALE_A_STRIDE_G = stride scale_a_ptr - endif - endif B_STRIDE_N = stride b_ptr - B_STRIDE_K = stride b_ptr - - B_IS_ D B_STRIDE_G = stride b_ptr - SCALED SCALE_B_STRIDE_G = stride scale_b_ptr - endif - endif - USE_TMA_LOAD - USE_EXPERIMENTAL_MAKE_TENSOR_DESCRIPTOR a_desc = tl _experimental_make_tensor_descriptor - a_desc = tl make_tensor_descriptor - endif a_ptr - A_IS_ D - A_IS_K_MAJOR shape= M K fixme strides= A_STRIDE_M A_STRIDE_K strides= stride a_ptr - stride a_ptr - block_shape= BLOCK_M BLOCK_K - shape= K M fixme strides= A_STRIDE_K A_STRIDE_M strides= stride a_ptr - stride a_ptr - block_shape= BLOCK_K BLOCK_M - endif - - A_IS_K_MAJOR shape= G M K fixme strides= A_STRIDE_G A_STRIDE_M A_STRIDE_K strides= stride a_ptr stride a_ptr - stride a_ptr - block_shape= BLOCK_M BLOCK_K - shape= G K M fixme strides= A_STRIDE_G A_STRIDE_K A_STRIDE_M strides= stride a_ptr stride a_ptr - stride a_ptr - block_shape= BLOCK_K BLOCK_M - endif - endif - USE_EXPERIMENTAL_MAKE_TENSOR_DESCRIPTOR b_desc = tl _experimental_make_tensor_descriptor - b_desc = tl make_tensor_descriptor - endif b_ptr - B_IS_ D - B_IS_K_MAJOR shape= N K fixme strides= B_STRIDE_N B_STRIDE_K strides= stride b_ptr - stride b_ptr - block_shape= BLOCK_N BLOCK_K - shape= K N fixme strides= B_STRIDE_K B_STRIDE_N strides= stride b_ptr - stride b_ptr - block_shape= BLOCK_K BLOCK_N - endif - - B_IS_K_MAJOR shape= G N K fixme strides= B_STRIDE_G B_STRIDE_N B_STRIDE_K strides= stride b_ptr stride b_ptr - stride b_ptr - block_shape= BLOCK_N BLOCK_K - shape= G K N fixme strides= B_STRIDE_G B_STRIDE_K B_STRIDE_N strides= stride b_ptr stride b_ptr - stride b_ptr - block_shape= BLOCK_K BLOCK_N - endif - endif - endif - M_IS_VARYING m_end_offset = - endif - N_IS_VARYING n_end_offset = - endif - K_IS_VARYING k_end_offset = - endif iterated_tiles = g tl range G - M_IS_VARYING Move across groups m_start_offset = m_end_offset m_end_offset = tl load offsets_ptr + g m_size = m_end_offset - m_start_offset - SCALED m_scale_start_offset = m_start_offset - endif - m_start_offset = m_size = M - SCALED m_scale_start_offset = g M - endif - endif - N_IS_VARYING Move across groups n_start_offset = n_end_offset n_end_offset = tl load offsets_ptr + g n_size = n_end_offset - n_start_offset - SCALED n_scale_start_offset = n_start_offset - endif - n_start_offset = n_size = N - SCALED n_scale_start_offset = g N - endif - endif m_size n_size - K_IS_VARYING Move across groups k_start_offset = k_end_offset k_end_offset = tl load offsets_ptr + g k_size = k_end_offset - k_start_offset - k_start_offset = k_size = K - endif num_m_tiles = tl cdiv m_size BLOCK_M num_n_tiles = tl cdiv n_size BLOCK_N num_tiles = num_m_tiles num_n_tiles Move across tiles while tidx = iterated_tiles tidx iterated_tiles + num_tiles gidx = tidx - iterated_tiles Split M first N second tile_m_idx = gidx num_m_tiles tile_n_idx = gidx num_m_tiles accumulator = tl zeros BLOCK_M BLOCK_N dtype=tl float - USE_TMA_LOAD m_tile_offset = tile_m_idx BLOCK_M n_tile_offset = tile_n_idx BLOCK_N m_offset = m_start_offset + m_tile_offset tl int n_offset = n_start_offset + n_tile_offset tl int k_block_offset = k range k_size BLOCK_K k_offset = k_start_offset + k_block_offset b = do_tma_loads g a_desc b_desc m_offset n_offset k_offset BLOCK_M BLOCK_N BLOCK_K accumulator = do_mma b accumulator k_block_offset += BLOCK_K k_size BLOCK_K = k_offset = k_start_offset + k_block_offset b = do_tma_loads g a_desc b_desc m_offset n_offset k_offset BLOCK_M BLOCK_N BLOCK_K - K_IS_VARYING group_offs = k_block_offset + tl arange BLOCK_K k_mask = group_offs k_size - A_IS_K_MAJOR = tl where k_mask None - = tl where k_mask None - endif - B_IS_K_MAJOR b = tl where k_mask None b - b = tl where k_mask None b - endif - endif accumulator = do_mma b accumulator - offs_am = tile_m_idx BLOCK_M + tl arange BLOCK_M offs_bn = tile_n_idx BLOCK_N + tl arange BLOCK_N k_block_offset range k_size BLOCK_K block_offs_k = k_block_offset + tl arange BLOCK_K offs_k = block_offs_k + k_start_offset a_ptrs = a_ptr - A_IS_ D + g A_STRIDE_G - endif + m_start_offset + offs_am None A_STRIDE_M + offs_k None A_STRIDE_K b_ptrs = b_ptr - B_IS_ D + g B_STRIDE_G - endif + n_start_offset + offs_bn None B_STRIDE_N + offs_k None B_STRIDE_K a_mask = offs_am None m_size block_offs_k None k_size b_mask = offs_bn None n_size block_offs_k None k_size = tl load a_ptrs mask=a_mask other=tl zeros dtype=a_ptrs dtype element_ty b = tl load b_ptrs mask=b_mask other=tl zeros dtype=b_ptrs dtype element_ty - USE_FAST_ACCUM accumulator = tl dot b T accumulator - accumulator += tl dot b T - endif a_ptrs += BLOCK_K b_ptrs += BLOCK_K - endif offs_am = tile_m_idx BLOCK_M + tl arange BLOCK_M offs_bn = tile_n_idx BLOCK_N + tl arange BLOCK_N - SCALED scale_a = tl load scale_a_ptr - A_IS_ D + m_scale_start_offset - + g SCALE_A_STRIDE_G - endif + offs_am None mask=offs_am None m_size other=tl zeros dtype=scale_a_ptr dtype element_ty scale_b = tl load scale_b_ptr - B_IS_ D + n_scale_start_offset - + g SCALE_B_STRIDE_G - endif + offs_bn None mask=offs_bn None n_size other=tl zeros dtype=scale_b_ptr dtype element_ty c = accumulator tl float scale_a scale_b - c = accumulator tl float - endif - M_IS_VARYING idx_m = m_start_offset + offs_am None - idx_m = offs_am None - endif - N_IS_VARYING idx_n = n_start_offset + offs_bn None - idx_n = offs_bn None - endif mask = offs_am None m_size offs_bn None n_size - M_IS_VARYING N_IS_VARYING store_output idx_m idx_n c mask indent_width= val_shape= BLOCK_M BLOCK_N - store_output g idx_m idx_n c mask indent_width= val_shape= BLOCK_M BLOCK_N - endif tidx += NUM_SMS iterated_tiles += num_tiles triton_grouped_mm_template = TritonTemplate name= grouped_mm grid=persistent_grouped_mm_grid source=triton_grouped_mm_source triton_scaled_grouped_mm_template = TritonTemplate name= scaled_grouped_mm grid=persistent_grouped_mm_grid source=triton_grouped_mm_source grouped_mm_args mat TensorBox mat TensorBox offs Optional TensorBox layout=None out_dtype=None mat mat = realize_inputs mat mat offs None realize_inputs offs mat _size = mat get_size mat _size = mat get_size m dim m dim = len mat _size len mat _size assert m dim == m dim == assert m dim == m dim == layout None torch _inductor ir FixedLayout out_dtype None out_dtype = mat get_dtype alignment = out_dtype itemsize m dim == m dim == assert offs None out_size = offs get_size mat _size mat _size out_size = mat _size mat _size - m dim == out_size = mat _size mat _size out_size = mat _size mat _size mat _size - size_padded = out_size - + alignment - alignment alignment len out_size == out_stride = size_padded out_stride = out_size size_padded size_padded layout = FixedLayout mat get_device out_dtype out_size out_stride assert out_dtype None out_dtype ignored layout specified mat _size mat _size layout mat mat offs aten__grouped_mm = ExternKernelChoice torch _grouped_mm _grouped_mm op_overload=aten _grouped_mm default has_out_variant=False aten__scaled_grouped_mm = ExternKernelChoice torch _scaled_grouped_mm _scaled_grouped_mm op_overload=aten _scaled_grouped_mm default has_out_variant=False can_use_triton_kernel mat_a TensorBox mat_b TensorBox offs Optional TensorBox bias Optional TensorBox scale_result Optional TensorBox - bool torch cuda is_available torch cuda get_device_capability = torch version hip False has_triton False The _grouped_mm _scaled_grouped_mm operator do support bias nor scale_result yet bias None False scale_result None False len mat_a get_size == len mat_b get_size == offs None offs None create_offsets x m _size m _size offs_size m _is_ d = len m _size == m _is_ d = len m _size == m _is_ d m _is_ d k = V graph sizevars size_hint m _size noffs = V graph sizevars size_hint offs_size step = k noffs torch linspace step k noffs dtype=x get_dtype device=x get_device m = V graph sizevars size_hint m _size noffs = V graph sizevars size_hint offs_size step = m noffs torch linspace step m noffs dtype=x get_dtype device=x get_device m _is_ d n = V graph sizevars size_hint m _size noffs = V graph sizevars size_hint offs_size step = n noffs torch linspace step n noffs dtype=x get_dtype device=x get_device None _tuned_grouped_mm_common operator_name str algorithm_name str extern_kernel_choice ExternKernelChoice kernel_template TritonTemplate mat_a TensorBox mat_b TensorBox scale_a Optional TensorBox = None scale_b Optional TensorBox = None offs Optional TensorBox = None bias Optional TensorBox = None scale_result Optional TensorBox = None out_dtype Optional torch dtype = None use_fast_accum Optional bool = None layout Optional Layout = None - TensorBox assert scale_a None == scale_b None assert scale_result None scale_a None m _size m _size layout mat_a mat_b offs = grouped_mm_args mat_a mat_b offs layout=layout out_dtype=out_dtype counters aten_mm_info operator_name += log_message = f Tuned operator_name mat _shape= s mat _shape= s mat _dtype= s mat _dtype= s output_layout= s log info log_message m _size m _size mat_a get_dtype mat_b get_dtype layout scale_a None scale_b None check_supported_striding mat_a mat_b workaround Inductor supporting optional tensor input arguments input_nodes list Any = mat_a mat_b scale_a None input_nodes append realize_inputs scale_a scale_b None input_nodes append realize_inputs scale_b offs None input_nodes append realize_inputs offs use_fast_accum None aten_choice = extern_kernel_choice bind input_nodes layout out_dtype=out_dtype aten_choice = extern_kernel_choice bind input_nodes layout out_dtype=out_dtype use_fast_accum=use_fast_accum use_fast_accum None use_fast_accum = False choices list ChoiceCaller = use_aten_gemm_kernels choices append aten_choice _ is_nonzero = _is_static_problem layout Checking only equality corresponding dims multiplicands here relying meta function checks everything is_nonzero use_triton_template layout can_use_triton_kernel mat_a mat_b offs bias scale_result scaled = scale_a None len m _size == len m _size == m k = m _size k _ = m _size pyrefly ignore missing-attribute g = offs get_size V graph sizevars check_equals k k a_is_ d b_is_ d = True True pyrefly ignore missing-attribute g = offs layout size m k = m _size g k _ = m _size g = V graph sizevars check_equals_and_simplify g g V graph sizevars check_equals k k a_is_ d b_is_ d = True False len m _size == pyrefly ignore missing-attribute g = offs layout size g m k = m _size k _ = m _size g = V graph sizevars check_equals_and_simplify g g V graph sizevars check_equals k k a_is_ d b_is_ d = False True g m k = m _size g k _ = m _size g = V graph sizevars check_equals_and_simplify g g V graph sizevars check_equals k k a_is_ d b_is_ d = False False a_is_k_major = mat_a get_stride - == b_is_k_major = mat_b get_stride - == triton_has_make_tensor_descriptor = hasattr tl make_tensor_descriptor triton_has_experimental_make_tensor_descriptor = hasattr tl _experimental_make_tensor_descriptor use_tma_load = triton_has_make_tensor_descriptor triton_has_experimental_make_tensor_descriptor kwargs = SCALED scaled A_IS_ D a_is_ d B_IS_ D b_is_ d A_IS_K_MAJOR a_is_k_major B_IS_K_MAJOR b_is_k_major USE_FAST_ACCUM use_fast_accum NUM_SMS get_num_sms USE_TMA_LOAD use_tma_load USE_EXPERIMENTAL_MAKE_TENSOR_DESCRIPTOR triton_has_experimental_make_tensor_descriptor config early_config_prune g m mat_a dtype itemsize grouped_mm_configs kwargs kernel_template maybe_append_choice choices input_nodes=input_nodes layout=layout num_stages=config num_stages num_warps=config num_warps kwargs config kwargs input_gen_fns = lambda x create_offsets x m _size m _size offs get_size offs None None autotune_select_algorithm algorithm_name choices input_nodes layout input_gen_fns=input_gen_fns register_lowering aten _grouped_mm default type_promotion_kind=None tuned_grouped_mm mat_a TensorBox mat_b TensorBox offs Optional TensorBox = None bias Optional TensorBox = None out_dtype Optional torch dtype = None layout Optional Layout = None - TensorBox Auto-tuning _grouped_mm operator _tuned_grouped_mm_common aten _grouped_mm default grouped_mm aten__grouped_mm triton_grouped_mm_template mat_a mat_b None None offs bias None out_dtype None layout register_lowering aten _scaled_grouped_mm default type_promotion_kind=None tuned_scaled_grouped_mm mat_a TensorBox mat_b TensorBox scale_a TensorBox scale_b TensorBox offs Optional TensorBox = None bias Optional TensorBox = None scale_result Optional TensorBox = None out_dtype Optional torch dtype = None use_fast_accum bool = False layout Optional Layout = None - TensorBox Auto-tuning _scaled_grouped_mm operator matching _scaled_grouped_mm_cuda Blas cpp implementation out_dtype = out_dtype torch bfloat _tuned_grouped_mm_common aten _scaled_grouped_mm default scaled_grouped_mm aten__scaled_grouped_mm triton_scaled_grouped_mm_template mat_a mat_b scale_a scale_b offs bias scale_result out_dtype use_fast_accum layout