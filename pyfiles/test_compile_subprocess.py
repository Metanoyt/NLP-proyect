Owner s module fx Tests compiling inductor tests subprocess contextlib importlib os sys time unittest unittest mock unittest mock patch torch torch library torch _inductor compile_fx _InProcessFxCompile FxCompile FxCompileMode torch _inductor graph GraphLowering torch _inductor test_case TestCase torch testing _internal common_utils IS_CI IS_WINDOWS TEST_WITH_ASAN torch testing _internal inductor_utils GPU_TYPE IS_BIG_GPU requires_gpu requires_triton RUN_CPU RUN_GPU IS_WINDOWS IS_CI TODO xuhancn Debug confirm pass_fds status Windows sys stderr write Almost UTs failed pass_fds supported Windows skip them Windows \n __name__ == __main__ sys exit raise unittest SkipTest pass_fds supported Windows Make helper files test importable pytorch_test_dir = os path dirname os path dirname os path realpath __file__ sys path append pytorch_test_dir inductor test_torchinductor manual=fbcode caffe test inductor test_inductor-library inductor test_torchinductor manual=fbcode caffe test inductor test_inductor-library check_model check_model_gpu copy_tests TestFailure importlib import_module filelock xfail default set is_skip=True skip test_failures = TypeError cannot pickle generator object test_layer_norm TestFailure cpu cuda is_skip=True test_remove_noop_slice TestFailure xpu is_skip=True test_remove_noop_slice TestFailure xpu is_skip=True test_remove_noop_slice_scatter TestFailure xpu is_skip=True test_remove_noop_view_default TestFailure xpu is_skip=True test_remove_noop_view_dtype TestFailure xpu is_skip=True TestSubprocess TestCase setUp torch _dynamo reset FxCompile _reset_stats TestCase setUp _stack = contextlib ExitStack _stack enter_context patch torch _inductor compile_fx fx_compile_mode FxCompileMode SUBPROCESS tearDown Check test didn t instigate in-process compile - which would mean something about fx graph failed serialize If some tests expected fail then we should probably add list expected failures here assertEqual FxCompile _compile_stats type _InProcessFxCompile codegen_and_compile _stack close TestCase tearDown torch _dynamo reset requires_gpu requires_triton unittest skipIf IS_BIG_GPU Skipping triton backend only since big GPU enough SM test_progressive triton testing do_bench torch _inductor compile_fx_async _ProgressiveFxCompile torch _inductor compile_fx fx_compile_progressive = True x = torch randn device=GPU_TYPE dtype=torch bfloat y = torch randn device=GPU_TYPE dtype=torch bfloat torch compile fullgraph=True backend= inductor optimized x y x y relu _ProgressiveFxCompile _reset_stats source_codes list str = save_output_code code str - None source_codes append code contextlib ExitStack stack When bug fixed remove cache disabling below assert torch _inductor compile_fx_async BUG_CACHES_DONT_WORK_WITH_ASYNC stack enter_context torch _inductor config patch autotune_local_cache=False fx_graph_cache=False stack enter_context mock patch object GraphLowering save_output_code save_output_code stack enter_context torch _functorch config patch enable_autograd_cache=False How long wait seconds before giving up TIMEOUT = If non-None then how often seconds print TICK message TICK_REPORT = None start = time time last_report = start while _ProgressiveFxCompile _stat_optimized_runs time sleep optimized x y now = time time TICK_REPORT None now - last_report TICK_REPORT print f TICK int now - start last_report = now now - start TIMEOUT raise RuntimeError Test timed out before producing progressively optimized compiled artifact assertEqual _ProgressiveFxCompile _stat_optimized_runs assertGreater _ProgressiveFxCompile _stat_fast_runs assertGreaterEqual _ProgressiveFxCompile _stat_bg_started assertGreaterEqual _ProgressiveFxCompile _stat_bg_finished torch _inductor compile_fx fx_compile_progressive = False torch compile fullgraph=True backend= inductor baseline x y x y relu Warmup baseline x y assertGreater do_bench lambda baseline x y do_bench lambda optimized x y assertTrue max_autotune True source_codes - patch torch _inductor compile_fx fx_compile_async True test_async Test async+subprocess works torch _inductor compile_fx_async _AsyncFxCompile torch compile fullgraph=True backend= inductor model_add x y out = x _ range out = torch add out y out _AsyncFxCompile _reset_stats contextlib ExitStack stack assert torch _inductor compile_fx_async BUG_CACHES_DONT_WORK_WITH_ASYNC stack enter_context torch _inductor config patch autotune_local_cache=False fx_graph_cache=False stack enter_context torch _functorch config patch enable_autograd_cache=False How long wait seconds before giving up TIMEOUT = If non-None then how often seconds print TICK message TICK_REPORT = None start = time time last_report = start while True start_stat_compiled_runs = _AsyncFxCompile _stat_compiled_runs Sleep bit so we don t drive CPU unnecessarily time sleep x = torch randn requires_grad=True y = torch randn requires_grad=True Forward pass output = model_add x y Backward pass output sum backward _AsyncFxCompile _stat_compiled_runs - start_stat_compiled_runs == break DEBUGGING Print periodic message so we know we re still running now = time time TICK_REPORT None now - last_report TICK_REPORT print f TICK int now - start last_report = now now - start TIMEOUT raise RuntimeError Test timed out before producing compiled artifact assertGreater _AsyncFxCompile _stat_compiled_runs Make sure we ran eager least once Normally will something like assertGreater _AsyncFxCompile _stat_eager_runs assertEqual _AsyncFxCompile _stat_bg_started assertEqual _AsyncFxCompile _stat_bg_finished RUN_CPU CpuTests TestSubprocess common = check_model device = cpu copy_tests inductor test_torchinductor CommonTemplate CpuTests cpu test_failures RUN_GPU TEST_WITH_ASAN GPUTests TestSubprocess common = check_model_gpu device = GPU_TYPE copy_tests inductor test_torchinductor CommonTemplate GPUTests GPU_TYPE test_failures __name__ == __main__ torch _inductor test_case run_tests RUN_CPU RUN_GPU run_tests needs= filelock