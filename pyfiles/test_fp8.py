Owner s module inductor functools unittest typing Union torch torch Tensor torch _inductor config utils torch _inductor pattern_matcher PatternMatcherPass torch _inductor test_case run_tests TestCase torch _inductor utils run_and_get_code torch nn functional ScalingType type ignore attr-defined torch testing _internal common_cuda _get_torch_cuda_version PLATFORM_SUPPORTS_FP PLATFORM_SUPPORTS_MX_GEMM torch testing _internal common_quantized ceil_div to_blocked torch testing _internal common_utils instantiate_parametrized_tests parametrize torch testing _internal inductor_utils _quantize_blockwise _quantize_rowwise _quantize_tensorwise _to_fp _saturated HAS_CPU HAS_CUDA_AND_TRITON torch testing _internal jit_utils FileCheck torch utils _triton has_triton_tma_device torch set_float _matmul_precision high f _msg = FP only supported H + SM MI + devices _fix_fp _dtype_for_rocm dtype Union torch dtype list torch dtype tuple torch dtype device - Union torch dtype list torch dtype tuple torch dtype This function used change FP data types MI supported FP types device GPU e m fn - e m fnuz e m - e m fnuz Supports single tuple list dtypes Keeps same test name CUDA ROCm Also allows enable FP inductor tests CPU torch version hip cuda device gfx torch cuda get_device_properties gcnArchName split MI uses different float dtypes isinstance dtype tuple tuple _fix_fp _dtype_for_rocm x device x dtype isinstance dtype list _fix_fp _dtype_for_rocm x device x dtype dtype == torch float _e m fn torch float _e m fnuz dtype == torch float _e m torch float _e m fnuz dtype instantiate_parametrized_tests TestFP Types TestCase parametrize float _dtype torch float _e m fn torch float _e m parametrize device cuda cpu test_xblock_for_small_numel float _dtype torch dtype device str TritonOverrides to_dtype will set min_elem_per_thread depends variant fp type This cause triton_heuristics triton_config pick XBLOCK larger than numel fail config sanity check We should pick XBLOCK larger than xnumel float _dtype = _fix_fp _dtype_for_rocm float _dtype device=device device == cuda PLATFORM_SUPPORTS_FP raise unittest SkipTest f _msg f x x dtype=float _dtype x = torch randn device=device expected = f x actual = torch compile f x torch testing assert_close expected half actual half rtol= e- atol= e- parametrize dtype torch float torch bfloat parametrize device cuda cpu test_eager_fallback dtype torch dtype device torch device device == cuda PLATFORM_SUPPORTS_FP raise unittest SkipTest f _msg weight_shape = e m _type = torch float _e m fn e m _type = _fix_fp _dtype_for_rocm e m _type device=device fp _matmul_unwrapped x a_scale = torch Tensor device=device b_scale = torch Tensor device=device output_scale = None input_bias = torch rand device=device dtype=dtype weight = torch rand weight_shape device=device dtype=dtype T e m _type a_inverse_scale = a_scale b_inverse_scale = b_scale output = torch _scaled_mm x weight bias=input_bias out_dtype=dtype scale_a=a_inverse_scale scale_b=b_inverse_scale scale_result=output_scale output compiled_fp _matmul = torch compile fp _matmul_unwrapped backend= inductor dynamic=True x_shape = x = torch rand x_shape device=device dtype=dtype e m _type y_fp = compiled_fp _matmul x noqa F x_shape = x = torch rand x_shape device=device dtype=dtype e m _type y_fp = compiled_fp _matmul x noqa F parametrize dtype torch float torch bfloat torch float parametrize shape parametrize dst_types torch float _e m fn torch float _e m parametrize device cuda cpu test_valid_cast dtype torch dtype shape str dst_types tuple device torch device device == cuda PLATFORM_SUPPORTS_FP raise unittest SkipTest f _msg dst_types = _fix_fp _dtype_for_rocm dst_types device=device e m e m = dst_types fp _cast x y = x dtype=e m dtype y = x dtype=e m dtype y y compiled_fp _cast = torch compile fp _cast backend= inductor dynamic=True shape = int dim dim shape split x = torch rand shape device=device dtype=dtype y _fp y _fp = compiled_fp _cast x torch testing assert_close y _fp x rtol= e- atol= e- torch testing assert_close y _fp x rtol= e- atol= e- unittest skipIf PLATFORM_SUPPORTS_FP f _msg test_bad_cast fp _cast x dtype x dtype=dtype compiled_fp _cast = torch compile fp _cast backend= inductor dynamic=True x_shape = assertRaisesRegex torch _dynamo exc BackendCompilerFailed Conversions between float _e m float _e m fn supported x = torch rand x_shape device= cuda dtype=torch float _e m fn compiled_fp _cast x torch float _e m assertRaisesRegex torch _dynamo exc BackendCompilerFailed Conversions between float _e m float _e m fn supported x = torch rand x_shape device= cuda dtype=torch float _e m compiled_fp _cast x torch float _e m fn parametrize src_dtype torch float torch bfloat torch float parametrize dst_dtype torch float _e m fn torch float _e m parametrize shape parametrize device cuda cpu test_to_fp _saturated src_dtype torch dtype dst_dtype torch dtype shape str device torch device device == cuda PLATFORM_SUPPORTS_FP raise unittest SkipTest f _msg dst_dtype = _fix_fp _dtype_for_rocm dst_dtype device=device fp _saturated x dtype _to_fp _saturated x dtype compiled_fp _cast = torch compile fp _saturated backend= inductor dynamic=True shape = int dim dim shape split x = torch rand shape device=device dtype=src_dtype y_compiled = compiled_fp _cast x dst_dtype y = fp _saturated x dst_dtype torch testing assert_close y_compiled half y half rtol= e- atol= e- parametrize float _dtype torch float _e m fn torch float _e m parametrize shape parametrize device cuda cpu test_amax_fp _quant float _dtype torch dtype shape str device torch device float _dtype = _fix_fp _dtype_for_rocm float _dtype device=device device == cuda PLATFORM_SUPPORTS_FP raise unittest SkipTest FP only supported H + sm_ MI + devices shape = int dim dim shape split batch_size sequence_length hidden_size = shape amax_fp x Tensor scale Tensor y = torch amax torch abs x y_scaled = y dtype=torch float scale bits_fp = _to_fp _saturated y_scaled float _dtype bits_fp compiled_amax_fp _quant = torch compile amax_fp backend= inductor x_shape = batch_size sequence_length hidden_size x = torch rand x_shape device=device dtype=torch half scale = torch tensor device=device dtype=torch float y_compiled = compiled_amax_fp _quant x scale y = amax_fp x scale torch testing assert_close y_compiled half y half rtol= e- atol= e- parametrize float _dtype torch float _e m fn torch float _e m parametrize shape parametrize device cuda cpu test_amax_along_with_fp _quant float _dtype torch dtype shape str device torch device device == cuda PLATFORM_SUPPORTS_FP raise unittest SkipTest f _msg float _dtype = _fix_fp _dtype_for_rocm float _dtype device=device shape = int dim dim shape split batch_size sequence_length hidden_size = shape amax_fp x Tensor scale Tensor amax_buffer Tensor amax_buffer fill_ torch amax torch abs x x_scaled = x dtype=torch float scale bits_fp = _to_fp _saturated x_scaled float _dtype bits_fp compiled_amax_fp _quant = torch compile amax_fp backend= inductor x_shape = batch_size sequence_length hidden_size x = torch rand x_shape device=device dtype=torch half scale = torch tensor device=device dtype=torch float amax_buffer_compiled = torch zeros device=device dtype=torch half y_compiled = compiled_amax_fp _quant x scale amax_buffer_compiled amax_buffer = torch zeros device=device dtype=torch half y = amax_fp x scale amax_buffer torch testing assert_close y_compiled half y half rtol= e- atol= e- torch testing assert_close amax_buffer_compiled amax_buffer rtol= e- atol= e- parametrize float _dtype torch float _e m fn torch float _e m parametrize amax_keep_dim True False parametrize shape parametrize device cuda cpu test_layernorm_fp _quant float _dtype torch dtype amax_keep_dim bool shape str device torch device device == cuda PLATFORM_SUPPORTS_FP raise unittest SkipTest FP only supported H + sm_ MI + devices float _dtype = _fix_fp _dtype_for_rocm float _dtype device=device shape = int dim dim shape split batch_size sequence_length hidden_size = shape ln_fp x Tensor scale Tensor amax_buffer Tensor x = torch nn functional layer_norm x dtype=torch float hidden_size weight=None bias=None eps= e- amax_buffer fill_ torch amax torch abs x keepdim=amax_keep_dim reshape - x_scaled = x scale bits_fp = _to_fp _saturated x_scaled float _dtype bits_fp compiled_ln_fp _quant = torch compile ln_fp backend= inductor x_shape = batch_size sequence_length hidden_size x = torch rand x_shape device=device dtype=torch half scale = torch tensor device=device dtype=torch float amax_buffer_compiled = torch zeros device=device dtype=torch half y_compiled = compiled_ln_fp _quant x scale amax_buffer_compiled amax_buffer = torch zeros device=device dtype=torch half y = ln_fp x scale amax_buffer torch testing assert_close y_compiled half y half rtol= e- atol= e- torch testing assert_close amax_buffer_compiled amax_buffer rtol= e- atol= e- unittest skipIf PLATFORM_SUPPORTS_FP f _msg parametrize float _dtype torch float _e m fn torch float _e m parametrize shape parametrize keepdim False True test_layernorm_fp _quant_benchmark float _dtype torch dtype shape str keepdim bool float _dtype = _fix_fp _dtype_for_rocm float _dtype device= cuda shape = int dim dim shape split batch_size sequence_length hidden_size = shape ln x Tensor x = torch nn functional layer_norm x dtype=torch float hidden_size weight=None bias=None eps= e- x ln_fp x Tensor scale Tensor amax_buffer Tensor x = torch nn functional layer_norm x dtype=torch float hidden_size weight=None bias=None eps= e- amax = torch amax torch abs x keepdim=keepdim amax_buffer view_as amax copy_ amax x_scaled = x scale bits_fp = _to_fp _saturated x_scaled float _dtype bits_fp compiled_ln_fp _quant = torch compile ln_fp backend= inductor x_shape = batch_size sequence_length hidden_size x = torch rand x_shape device= cuda dtype=torch half scale = torch tensor device= cuda dtype=torch float amax_buffer_compiled = torch zeros device= cuda dtype=torch half amax_buffer = torch zeros device= cuda dtype=torch half _ = compiled_ln_fp _quant x scale amax_buffer_compiled compiled_latency = utils do_bench_using_profiling functools partial compiled_ln_fp _quant x scale amax_buffer_compiled eager_latency = utils do_bench_using_profiling functools partial ln_fp x scale amax_buffer compiled_ln = torch compile ln backend= inductor _ = compiled_ln x ln_latency = utils do_bench_using_profiling functools partial compiled_ln x print f Config float _dtype= shape= keepdim= f Benchmark results Inductor compiled_latency ms Eager eager_latency ms f LN only Inductor ln_latency ms instantiate_parametrized_tests TestFP Lowering TestCase unittest skipIf PLATFORM_SUPPORTS_FP f _msg parametrize dtype torch bfloat torch float parametrize shape parametrize has_bias False True parametrize use_fast_accum False True parametrize persistent_matmul False True has_triton_tma_device False test_tensorwise_scaling dtype torch dtype shape str has_bias bool use_fast_accum bool persistent_matmul bool dtype torch float has_bias skipTest bias supported when output dtype float device = cuda dtype_float = torch float _e m fn dtype_float = _fix_fp _dtype_for_rocm dtype_float device shape = int dim dim shape split M K N = shape Matmul Y = X M K x W N K input output dtypes _scaled_mm do need same typically model they x = torch randn M K dtype=dtype device=device w = torch randn N K dtype=dtype device=device bias = None has_bias bias = torch randn N device=device dtype=torch bfloat quantize weight prior inference w_fp w_inverse_scale = _quantize_tensorwise w dtype_float w_t_fp = w_fp t quantize input x x_fp x_inverse_scale = _quantize_tensorwise x dtype_float linear x_fp x_inverse_scale w_t_fp w_inverse_scale bias y = torch _scaled_mm x_fp w_t_fp x_inverse_scale w_inverse_scale bias out_dtype=dtype use_fast_accum=use_fast_accum y y_eager = linear x_fp x_inverse_scale w_t_fp w_inverse_scale bias config patch triton enable_persistent_tma_matmul persistent_matmul linear_compiled = torch compile linear backend= inductor mode= max-autotune y_compiled = linear_compiled x_fp x_inverse_scale w_t_fp w_inverse_scale bias assertEqual y_eager dtype dtype assertEqual y_compiled dtype dtype depending kernel config BLOCK_M size etc selected during Inductor autotuning compiled case results can different because way blocks results accumulated float addition associative so setting small absolute tolerance these tests dtype == torch bfloat assertEqual y_eager y_compiled rtol= e- atol= assertEqual y_eager y_compiled rtol= e- atol= unittest skipIf PLATFORM_SUPPORTS_FP f _msg test_scaled_mm_preserves_strides Test scaled_mm preserves stride ordering through custom pass GPU_TYPE = cuda f b scale_a scale_b Convert fp correct strides scaled_mm dtype_float = torch float _e m fn dtype_float = _fix_fp _dtype_for_rocm dtype_float GPU_TYPE a_fp = dtype_float contiguous row-major b_fp = b t contiguous t dtype_float column-major torch _scaled_mm a_fp b_fp scale_a scale_b out_dtype=torch bfloat ScaledMMStridePass PatternMatcherPass __init__ - None super __init__ called = False __call__ g torch fx Graph Directly manipulate graph without using pattern matching node g nodes node op == call_function node target == torch ops aten _scaled_mm default Insert clone operations before scaled_mm g inserting_before node a_fp b_fp = node args node args Clone inputs potentially change stride ordering a_cloned = g call_function torch ops aten clone a_fp memory_format torch contiguous_format b_cloned = g call_function torch ops aten clone b_fp memory_format torch contiguous_format Replace arguments scaled_mm call node args = a_cloned b_cloned + node args called = True g lint g stride_pass = ScaledMMStridePass Create inputs correct strides scaled_mm = torch randn dtype=torch bfloat device=GPU_TYPE b = torch randn dtype=torch bfloat device=GPU_TYPE scale_a = torch tensor device=GPU_TYPE scale_b = torch tensor device=GPU_TYPE First verify f works without pass baseline expected = f b scale_a scale_b torch _inductor config config patch post_grad_custom_post_pass=stride_pass f_compiled = torch compile f dynamic=False result = f_compiled b scale_a scale_b Verify pattern called assertTrue stride_pass called Stride ordering pass called Verify correctness - pass should preserve correctness even though modified strides assertEqual expected result atol= e- rtol= e- Verify generated code contains clones inserted our pass _ wrapper = run_and_get_code f_compiled b scale_a scale_b assertIn scaled_mm wrapper lower The clones should visible generated code assertIn clone wrapper lower unittest skipIf PLATFORM_SUPPORTS_FP f _msg unittest skipIf has_triton_tma_device Need device-side TMA support Triton parametrize dtype torch bfloat torch float parametrize shape parametrize use_fast_accum False True test_tensorwise_scaling_tma_template dtype torch dtype shape str use_fast_accum bool device = cuda dtype_float = torch float _e m fn dtype_float = _fix_fp _dtype_for_rocm dtype_float device shape = int dim dim shape split M K N = shape Matmul Y = X M K x W N K input output dtypes _scaled_mm do need same typically model they x = torch randn M K dtype=dtype device=device w = torch randn N K dtype=dtype device=device bias = None quantize weight prior inference w_fp w_inverse_scale = _quantize_tensorwise w dtype_float w_t_fp = w_fp t quantize input x x_fp x_inverse_scale = _quantize_tensorwise x dtype_float linear x_fp x_inverse_scale w_t_fp w_inverse_scale bias y = torch _scaled_mm x_fp w_t_fp x_inverse_scale w_inverse_scale bias out_dtype=dtype use_fast_accum=use_fast_accum y y_eager = linear x_fp x_inverse_scale w_t_fp w_inverse_scale bias config patch triton enable_persistent_tma_matmul True test_configs autotune_choice_name_regex triton_scaled_mm_device_tma max_autotune_gemm_backends TRITON max_autotune True linear_compiled = torch compile linear backend= inductor mode= max-autotune y_compiled code = run_and_get_code linear_compiled x_fp x_inverse_scale w_t_fp w_inverse_scale bias FileCheck check f SCALE_RECIPE_A tl constexpr = ScalingType TensorWise value run code FileCheck check f SCALE_RECIPE_B tl constexpr = ScalingType TensorWise value run code assertEqual y_eager dtype dtype assertEqual y_compiled dtype dtype depending kernel config BLOCK_M size etc selected during Inductor autotuning compiled case results can different because way blocks results accumulated float addition associative so setting small absolute tolerance these tests torch testing assert_close y_eager y_compiled rtol= e- atol= unittest skipIf PLATFORM_SUPPORTS_FP f _msg parametrize shape parametrize has_bias False True parametrize use_fast_accum False True parametrize persistent_matmul False True has_triton_tma_device False test_rowwise_scaling shape str has_bias bool use_fast_accum bool persistent_matmul bool Only bf output type supported row-wise scaling fp dtype torch dtype = torch bfloat device = cuda dtype_float = torch float _e m fn dtype_float = _fix_fp _dtype_for_rocm dtype_float device shape = int dim dim shape split M K N = shape Matmul Y = X M K x W N K x = torch randn M K dtype=dtype device=device w = torch randn N K dtype=dtype device=device bias = None has_bias bias = torch randn N device=device dtype=torch bfloat quantize weight prior inference w_fp w_inverse_scale = _quantize_rowwise w dtype_float w_t_fp = w_fp t w_inverse_scale = w_inverse_scale t scale_b should N quantize input x x_fp x_inverse_scale = _quantize_rowwise x dtype_float linear x_fp x_inverse_scale w_t_fp w_inverse_scale bias y = torch _scaled_mm x_fp w_t_fp x_inverse_scale w_inverse_scale bias out_dtype=dtype use_fast_accum=use_fast_accum y y_eager = linear x_fp x_inverse_scale w_t_fp w_inverse_scale bias config patch triton enable_persistent_tma_matmul persistent_matmul linear_compiled = torch compile linear backend= inductor mode= max-autotune y_compiled = linear_compiled x_fp x_inverse_scale w_t_fp w_inverse_scale bias assertEqual y_eager dtype dtype assertEqual y_compiled dtype dtype torch testing assert_close y_eager y_compiled rtol= e- atol= unittest skipIf PLATFORM_SUPPORTS_FP f _msg unittest skipIf has_triton_tma_device Need device-side TMA support Triton parametrize shape parametrize use_fast_accum False True test_rowwise_scaling_tma_template shape str use_fast_accum bool Only bf output type supported row-wise scaling fp dtype torch dtype = torch bfloat device = cuda dtype_float = torch float _e m fn dtype_float = _fix_fp _dtype_for_rocm dtype_float device shape = int dim dim shape split M K N = shape Matmul Y = X M K x W N K x = torch randn M K dtype=dtype device=device w = torch randn N K dtype=dtype device=device bias = None quantize weight prior inference w_fp w_inverse_scale = _quantize_rowwise w dtype_float w_t_fp = w_fp t w_inverse_scale = w_inverse_scale t scale_b should N quantize input x x_fp x_inverse_scale = _quantize_rowwise x dtype_float linear x_fp x_inverse_scale w_t_fp w_inverse_scale bias y = torch _scaled_mm x_fp w_t_fp x_inverse_scale w_inverse_scale bias out_dtype=dtype use_fast_accum=use_fast_accum y y_eager = linear x_fp x_inverse_scale w_t_fp w_inverse_scale bias config patch triton enable_persistent_tma_matmul True test_configs autotune_choice_name_regex triton_scaled_mm_device_tma max_autotune_gemm_backends TRITON max_autotune True linear_compiled = torch compile linear backend= inductor mode= max-autotune y_compiled code = run_and_get_code linear_compiled x_fp x_inverse_scale w_t_fp w_inverse_scale bias FileCheck check f SCALE_RECIPE_A tl constexpr = ScalingType RowWise value run code FileCheck check f SCALE_RECIPE_B tl constexpr = ScalingType RowWise value run code assertEqual y_eager dtype dtype assertEqual y_compiled dtype dtype torch testing assert_close y_eager y_compiled rtol= e- atol= unittest skipIf PLATFORM_SUPPORTS_FP f _msg unittest skipIf has_triton_tma_device Need device-side TMA support Triton unittest skipIf _get_torch_cuda_version cuBLAS blockwise scaling added CUDA parametrize shape TODO jananisriram add scaling recipe overrides shapes like parametrize use_fast_accum False True test_blockwise x _blockwise x _scaling shape tuple int int int use_fast_accum bool Only bf output type supported non-tensorwise scaling fp dtype torch dtype = torch bfloat device = cuda dtype_float = torch float _e m fn dtype_float = _fix_fp _dtype_for_rocm dtype_float device M N K = shape Matmul Y = X M K x W N K x = torch randn M K dtype=dtype device=device w = torch randn N K dtype=dtype device=device bias = None quantize weight prior inference w_fp w_inverse_scale = _quantize_blockwise w dtype_float block_outer= block_inner= w_t_fp = w_fp t w_inverse_scale = w_inverse_scale t scale_b should N quantize input x x_fp x_inverse_scale = _quantize_blockwise x dtype_float block_outer= block_inner= x_inverse_scale = x_inverse_scale t contiguous t x blocks need scales outer-dim-major linear x_fp x_inverse_scale w_t_fp w_inverse_scale bias y = torch _scaled_mm x_fp w_t_fp x_inverse_scale w_inverse_scale bias out_dtype=dtype use_fast_accum=use_fast_accum y y_eager = linear x_fp x_inverse_scale w_t_fp w_inverse_scale bias config patch triton enable_persistent_tma_matmul True test_configs autotune_choice_name_regex triton_scaled_mm_device_tma max_autotune_gemm_backends TRITON max_autotune True linear_compiled = torch compile linear backend= inductor mode= max-autotune y_compiled code = run_and_get_code linear_compiled x_fp x_inverse_scale w_t_fp w_inverse_scale bias Verify Inductor chooses correct scaling recipes FileCheck check f SCALE_RECIPE_A tl constexpr = ScalingType BlockWise x value run code FileCheck check f SCALE_RECIPE_B tl constexpr = ScalingType BlockWise x value run code assertEqual y_eager dtype dtype assertEqual y_compiled dtype dtype torch testing assert_close y_eager y_compiled rtol= e- atol= unittest skipIf PLATFORM_SUPPORTS_FP f _msg parametrize M parametrize K parametrize N parametrize persistent_matmul False True has_triton_tma_device False test_tensorwise_scaling_acceptable_input_dims M int K int N int persistent_matmul bool alignment requirements K N divisible dtype torch dtype = torch bfloat use_fast_accum = True device = cuda dtype_float = torch float _e m fn dtype_float = _fix_fp _dtype_for_rocm dtype_float device x = torch randn M K dtype=dtype device=device w = torch randn N K dtype=dtype device=device bias = None w_fp w_inverse_scale = _quantize_tensorwise w dtype_float w_t_fp = w_fp t x_fp x_inverse_scale = _quantize_tensorwise x dtype_float linear x_fp x_inverse_scale w_t_fp w_inverse_scale bias y = torch _scaled_mm x_fp w_t_fp x_inverse_scale w_inverse_scale bias out_dtype=dtype use_fast_accum=use_fast_accum y y_eager = linear x_fp x_inverse_scale w_t_fp w_inverse_scale bias config patch triton enable_persistent_tma_matmul persistent_matmul linear_compiled = torch compile linear backend= inductor mode= max-autotune y_compiled = linear_compiled x_fp x_inverse_scale w_t_fp w_inverse_scale bias assertEqual y_eager dtype dtype assertEqual y_compiled dtype dtype torch testing assert_close y_eager y_compiled rtol= e- atol= unittest skipIf PLATFORM_SUPPORTS_FP f _msg parametrize M parametrize K parametrize N parametrize persistent_matmul False True has_triton_tma_device False test_rowwise_scaling_acceptable_input_dims M int K int N int persistent_matmul bool dtype torch dtype = torch bfloat use_fast_accum = True device = cuda dtype_float = torch float _e m fn dtype_float = _fix_fp _dtype_for_rocm dtype_float device x = torch randn M K dtype=dtype device=device w = torch randn N K dtype=dtype device=device bias = torch randn N device=device dtype=torch bfloat w_fp w_inverse_scale = _quantize_rowwise w dtype_float w_t_fp = w_fp t w_inverse_scale = w_inverse_scale t scale_b should N x_fp x_inverse_scale = _quantize_rowwise x dtype_float linear x_fp x_inverse_scale w_t_fp w_inverse_scale bias y = torch _scaled_mm x_fp w_t_fp x_inverse_scale w_inverse_scale bias out_dtype=dtype use_fast_accum=use_fast_accum y y_eager = linear x_fp x_inverse_scale w_t_fp w_inverse_scale bias config patch triton enable_persistent_tma_matmul persistent_matmul linear_compiled = torch compile linear backend= inductor mode= max-autotune y_compiled = linear_compiled x_fp x_inverse_scale w_t_fp w_inverse_scale bias assertEqual y_eager dtype dtype assertEqual y_compiled dtype dtype torch testing assert_close y_eager y_compiled rtol= e- atol= unittest skipIf PLATFORM_SUPPORTS_MX_GEMM Not supported non B test_mx_fp _max_autotune M K N = BLOCK_SIZE = device = cuda dtype = torch bfloat A_ref = torch eye M device=device dtype=torch bfloat B_ref = torch eye N device=device dtype=torch bfloat A = A_ref torch float _e m fn B = B_ref torch float _e m fn A_scale = torch full M ceil_div K BLOCK_SIZE device=device dtype=torch float _e m fnu B_scale = torch full N ceil_div K BLOCK_SIZE device=device dtype=torch float _e m fnu A_scale = to_blocked A_scale B_scale = to_blocked B_scale linear A B A_scale B_scale y = torch _scaled_mm A B t A_scale B_scale out_dtype=torch bfloat use_fast_accum=False y y_eager = linear A B A_scale B_scale linear_compiled = torch compile linear backend= inductor mode= max-autotune y_compiled = linear_compiled A B A_scale B_scale assertEqual y_eager dtype dtype assertEqual y_compiled dtype dtype torch testing assert_close y_eager y_compiled rtol= e- atol= unittest skipIf PLATFORM_SUPPORTS_FP f _msg test_unacceptable_input_dims compiled ops type checking torch _meta_registrations py dtype torch dtype = torch bfloat device = cuda dtype_float = torch float _e m fn dtype_float = _fix_fp _dtype_for_rocm dtype_float device M K N = K needs multiple x = torch randn M K dtype=dtype device=device w = torch randn N K dtype=dtype device=device bias = torch randn N device=device dtype=torch bfloat w_fp w_inverse_scale = _quantize_tensorwise w dtype_float w_t_fp = w_fp t linear x w_t_fp w_inverse_scale bias x_fp x_inverse_scale = _quantize_tensorwise x dtype_float y = torch _scaled_mm x_fp w_t_fp x_inverse_scale w_inverse_scale bias out_dtype=dtype use_fast_accum=True y linear_compiled = torch compile linear backend= inductor mode= max-autotune assertRaises torch _dynamo exc TorchRuntimeError cm linear_compiled x w_t_fp w_inverse_scale bias assertTrue f Expected size divisible got size = K str cm exception unittest skipIf PLATFORM_SUPPORTS_FP f _msg test_unacceptable_scale_dims_rowwise_scaling dtype torch dtype = torch bfloat device = cuda dtype_float = torch float _e m fn dtype_float = _fix_fp _dtype_for_rocm dtype_float device M K N = x = torch randn M K dtype=dtype device=device w = torch randn N K dtype=dtype device=device bias = torch randn N device=device dtype=torch bfloat w_fp w_inverse_scale = _quantize_rowwise w dtype_float w_t_fp = w_fp t linear x w_t_fp w_inverse_scale bias x_fp x_inverse_scale = _quantize_rowwise x dtype_float y = torch _scaled_mm x_fp w_t_fp w_inverse_scale t testing w x scales switched x_inverse_scale bias out_dtype=dtype use_fast_accum=True y linear_compiled = torch compile linear backend= inductor mode= max-autotune assertRaises torch _dynamo exc TorchRuntimeError cm linear_compiled x w_t_fp w_inverse_scale bias assertTrue Invalid scaling configuration str cm exception __name__ == __main__ HAS_CUDA_AND_TRITON HAS_CPU run_tests