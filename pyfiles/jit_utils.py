mypy ignore-errors Torch torch autograd Variable torch autograd function _nested_map torch jit annotations BroadcastingList BroadcastingList noqa F torch onnx OperatorExportTypes torch torch cuda torch jit torch jit _logging torch jit frontend torch jit quantized zipfile functools Testing utils torch testing FileCheck torch testing _internal common_utils IS_WINDOWS \ freeze_rng_state enable_profiling_mode_for_profiling_tests ProfilingMode TEST_BAILOUTS \ is_iterable_of_tensors torch testing _internal common_jit JitCommonTestCase torch testing _internal common_utils enable_profiling_mode noqa F Standard library contextlib contextmanager functools reduce io StringIO collections defaultdict importlib util inspect io math os pickle sys tempfile textwrap importlib abc Loader typing Any Union RUN_CUDA = torch cuda is_available RUN_CUDA_MULTI_GPU = RUN_CUDA torch cuda device_count RUN_CUDA_HALF = RUN_CUDA HIP supports half no version check necessary torch cuda is_available torch version hip CUDA_VERSION = torch _C _cuda_getCompiledVersion d range torch cuda device_count major = torch cuda get_device_capability d major RUN_CUDA_HALF = False execWrapper code glob loc exec code glob loc do_input_map fn input _nested_map lambda t isinstance t torch Tensor fn input clear_class_registry torch _C _jit_clear_class_registry torch jit _recursive concrete_type_store = torch jit _recursive ConcreteTypeStore torch jit _state _clear_class_state get_execution_plan graph_executor_state execution_plans = list graph_executor_state execution_plans values num_plans = len execution_plans num_plans = raise RuntimeError This test assumes GraphExecutor should f only have one execution plan got num_plans execution_plans _AssertRaisesRegexWithHighlightContext A context manager useful checking error messages highlight correct part source code __init__ test_case exception regex highlight test_case = test_case exception_type = exception regex = regex highlight = highlight __enter__ __exit__ type value traceback test_case assertRaisesRegex exception_type regex type raise value highlight FileCheck check_source_highlighted highlight run str value True FUSION_GROUP = prim TensorExprGroup JitTestCase JitCommonTestCase _do_cuda_memory_leak_check = True _restored_warnings = False capture_stdout list Replace sys stdout temporary StringIO __enter__ sys_stdout = sys stdout stringio = StringIO sys stdout = stringio __exit__ args append str stringio getvalue del stringio sys stdout = sys_stdout capture_stderr list Replace sys stderr temporary StringIO __enter__ sys_stderr = sys stderr stringio = StringIO sys stderr = stringio __exit__ args append str stringio getvalue del stringio sys stderr = sys_stderr setHooks torch _C _jit_set_emit_hooks emitModuleHook emitFunctionHook clearHooks torch _C _jit_set_emit_hooks None None setUp super setUp unittest overrides all warning filters forces all them show up after we install our own silence those coming inside PyTorch This will ensure our filter still takes precedence JitTestCase _restored_warnings torch jit TracerWarning ignore_lib_warnings JitTestCase _restored_warnings = True setHooks tearDown super tearDown needs cleared because python might unloaded before callback gets destructed clearHooks clear_class_registry assertAllFused graph except_for= note helper collects nodes fast path only i e true blocks specialized checks get_nodes_and_parents_recursively block kind acc node block nodes node kind == kind acc block append node node kind == prim DifferentiableGraph get_nodes_and_parents_recursively node g Subgraph kind acc node kind == prim If node inputs __next__ node kind == aten all node inputs __next__ node kind == prim TypeCheck node inputs __next__ node kind == prim RequiresGradCheck get_nodes_and_parents_recursively node blocks __next__ kind acc inner_block node blocks get_nodes_and_parents_recursively inner_block kind acc allowed_nodes = prim Constant FUSION_GROUP prim BailoutTemplate prim TupleConstruct prim If prim TypeCheck prim RequiresGradCheck &#124; set except_for fusion_groups dict torch _C Block list torch _C Node = defaultdict list get_nodes_and_parents_recursively graph FUSION_GROUP fusion_groups assertTrue len fusion_groups == f got graph graph fusion_nodes = next iter fusion_groups items block contains one FUSION_GROUP rest nodes ` allowed_nodes ` assertTrue len fusion_nodes == f got graph assertTrue all node kind allowed_nodes node graph nodes f got graph _isHookExceptionOk e se = str e allowed = Could export Python function closures exportable allowed se True False _compared_saved_loaded m extract_files buffer crack open zip format get main module code archive = zipfile ZipFile buffer check we have no duplicate names assertEqual len set archive namelist len archive namelist files = list filter lambda x x startswith archive code archive namelist unwrap all code files into strings code_files_str = filter lambda x x endswith py files code_files_stream = archive open f f code_files_str code_files = join line decode line file file code_files_stream unpickled all debug files debug_files_str = filter lambda f f endswith debug_pkl files debug_files_stream = archive open f f debug_files_str debug_files = pickle load f f debug_files_stream code_files debug_files disable hook while we parse code otherwise we will re-enter hook torch _jit_internal _disable_emit_hooks try short-circuit empty function module len m code == isinstance m torch _C ScriptModule len m _method_names == save module buffer buffer = io BytesIO torch jit save m buffer copy data buffer so we can restore later This because py py have different semantics zipfile s easier just work fresh copy each time buffer_copy = buffer getvalue code_files _debug_files = extract_files buffer except RuntimeError e _isHookExceptionOk e raise model again copy we made original buffer = io BytesIO buffer_copy imported = torch jit load buffer save again saved_module_buffer_ = io BytesIO torch jit save imported saved_module_buffer_ saved_module_buffer_ seek code_files_ _debug_files_ = extract_files saved_module_buffer_ b zip code_files code_files_ strict=True assertMultiLineEqual b isinstance m torch _C ScriptModule assertTrue torch _C _ivalue_tags_match m imported _c emitFunctionHook func func has invalid names export skip jitter check func name == lambda aten func name _compared_saved_loaded func emitModuleHook module _compared_saved_loaded module getExportImportCopyWithPacking m also_test_file=True map_location=None buffer = io BytesIO m apply lambda s s _pack s _c _has_method _pack None torch jit save m buffer m apply lambda s s _unpack s _c _has_method _unpack None buffer seek imported = torch jit load buffer map_location=map_location imported apply lambda s s _unpack s _c _has_method _unpack None also_test_file imported Ideally we would like have manually delete file NamedTemporaryFile opens file cannot opened multiple times Windows To support Windows close file after creation try remove manually f = tempfile NamedTemporaryFile delete=False try f close imported save f name result = torch jit load f name map_location=map_location finally os unlink f name result apply lambda s s _unpack s _c _has_method _unpack None result assertGraphContains graph kind consider_subgraphs=False consider_subgraphs strgraph = str graph count = strgraph count kind - strgraph count f kind assertTrue count nodes block out = node block nodes node kind == kind out append node block node blocks out += nodes block out out_nodes = nodes graph assertTrue len out_nodes assertGraphContainsExactly graph kind num_kind_nodes consider_subgraphs=False perform_assert graph kind actual expected consider_subgraphs actual == expected subgraph = including consider_subgraphs excluding raise AssertionError f graph \nError graph contains actual kind nodes subgraph subgraphs expected expected consider_subgraphs strgraph = str graph count = strgraph count kind - strgraph count f kind perform_assert graph kind count num_kind_nodes consider_subgraphs nodes block out = node block nodes node kind == kind out append node block node blocks out += nodes block out out_nodes = nodes graph perform_assert graph kind len out_nodes num_kind_nodes consider_subgraphs assertExpectedONNXGraph g args kwargs g = torch onnx _optimize_trace g operator_export_type=OperatorExportTypes ONNX assertExpectedGraph g args kwargs assertExpectedGraph trace args kwargs isinstance trace torch _C Graph graph = trace graph = trace graph torch _C _jit_pass_lint graph torch _C _jit_pass_dce graph torch _C _jit_pass_lint graph graph = torch _C _jit_pass_canonicalize graph torch _C _jit_pass_lint graph assertExpected str graph args kwargs run_pass name trace isinstance trace torch _C Graph graph = trace set_graph = False set_graph = True graph = trace graph torch _C _jit_pass_lint graph result = getattr torch _C _jit_pass_ + name graph result None isinstance result bool graph = result torch _C _jit_pass_lint graph set_graph trace set_graph graph graph get_frame_vars frames_up frame = inspect currentframe frame raise RuntimeError failed inspect frame i = while i frames_up + frame = frame f_back frame raise RuntimeError failed get frame i += defined_vars dict str Any = defined_vars update frame f_locals defined_vars update frame f_globals defined_vars assertRaisesRegexWithHighlight exception regex highlight _AssertRaisesRegexWithHighlightContext exception regex highlight checkScriptRaisesRegex script inputs exception regex name=None outputs=None capture_output=False frames_up= profiling=ProfilingMode PROFILING Checks given function will throw correct exception when executed normal python string frontend AST frontend Logic taken ` checkScript ` see comments there details enable_profiling_mode_for_profiling_tests Normal Python assertRaisesRegex exception regex isinstance script str frame = get_frame_vars frames_up the_locals dict str Any = execWrapper script glob=frame loc=the_locals frame update the_locals python_fn = frame name python_fn = script python_fn inputs String frontend assertRaisesRegex exception regex isinstance script str cu = torch jit CompilationUnit script _frames_up=frames_up string_frontend = getattr cu name source = textwrap dedent inspect getsource script cu = torch jit CompilationUnit source _frames_up=frames_up string_frontend = getattr cu script __name__ string_frontend inputs Python AST frontend isinstance script str assertRaisesRegex exception regex ge = torch jit script python_fn ge inputs checkBailouts model inputs expected state = model get_debug_state plan = get_execution_plan state num_bailouts = plan code num_bailouts i range num_bailouts plan code request_bailout i bailout_outputs = model inputs assertEqual bailout_outputs expected checkScript script inputs name= func optimize=True inputs_requires_grad=False capture_output=False frames_up= profiling=ProfilingMode PROFILING atol=None rtol=None Checks given script generates same output Python version using given inputs torch jit optimized_execution optimize enable_profiling_mode_for_profiling_tests extra_profile_runs = any isinstance x torch Tensor x requires_grad x inputs isinstance script str Compile string Script function enable_profiling_mode cu = torch jit CompilationUnit script _frames_up=frames_up Execute Python function so we can run later get its outputs frame = get_frame_vars frames_up the_locals dict str Any = execWrapper script glob=frame loc=the_locals frame update the_locals python_fn = frame name scripted_fn = getattr cu name Check string frontend first source = textwrap dedent inspect getsource script checkScript source inputs script __name__ optimize=optimize inputs_requires_grad=inputs_requires_grad capture_output=capture_output profiling=profiling frames_up= Continue checking Python frontend scripted_fn = torch jit script script _frames_up= python_fn = script inputs_requires_grad recording_inputs = do_input_map lambda t t detach requires_grad_ inputs recording_inputs = inputs capture_output capture_stdout script_stdout script_outputs = scripted_fn recording_inputs capture_stdout opt_script_outputs = scripted_fn recording_inputs capture_stdout python_outputs = python_fn inputs IS_WINDOWS assertExpected script_stdout subname= stdout assertEqual python_outputs opt_script_outputs atol=atol rtol=rtol profiling run script_outputs = scripted_fn recording_inputs inputs_requires_grad extra_profile_runs opt_script_outputs = scripted_fn recording_inputs optimized run opt_script_outputs = scripted_fn recording_inputs TEST_BAILOUTS checkBailouts scripted_fn inputs opt_script_outputs python_outputs = python_fn inputs assertEqual python_outputs script_outputs atol=atol rtol=rtol assertEqual script_outputs opt_script_outputs atol=atol rtol=rtol scripted_fn checkTrace func reference_tensors input_tensors=None drop=None allow_unused=False verbose=False inputs_require_grads=True check_tolerance= e- export_import=True _force_outplace=False grad_atol=None grad_rtol=None TODO check gradients parameters just inputs allSum vs drop allows us remove some values ever being used test unused outputs drop None vs = vs -drop we don t want all grad all outputs same so we multiply each constant sum math log i + v sum i v enumerate vs v None input_tensors None input_tensors = reference_tensors flatten_inputs inputs input_reduce input fn acc isinstance input torch Tensor fn input acc isinstance input dict reduce lambda acc key input_reduce input key fn acc input acc reduce lambda acc val input_reduce val fn acc input acc acc tuple input_reduce recording_inputs lambda t acc acc append t nograd_inputs = reference_tensors inputs_require_grads recording_inputs = do_input_map lambda t t clone requires_grad_ reference_tensors flattened_recording_inputs = flatten_inputs recording_inputs recording_inputs = reference_tensors ` check_trace ` set False because check_trace run no_grad Also ` checkTrace ` already does all checks against python function ge = torch jit trace func input_tensors check_tolerance=check_tolerance _force_outplace=_force_outplace check_trace=False export_import ge = getExportImportCopy ge verbose print ge graph test no gradients case outputs = func nograd_inputs outputs_ge = ge nograd_inputs assertEqual outputs outputs_ge test gradients case outputs = func recording_inputs inputs_require_grads grads = torch autograd grad allSum outputs flattened_recording_inputs allow_unused=allow_unused outputs_ge = ge recording_inputs inputs_require_grads grads_ge = torch autograd grad allSum outputs_ge flattened_recording_inputs allow_unused=allow_unused assertEqual outputs outputs_ge inputs_require_grads assertEqual grads grads_ge atol=grad_atol rtol=grad_rtol test grad grad case outputs = func recording_inputs l = allSum outputs inputs_require_grads grads = torch autograd grad l flattened_recording_inputs create_graph=True allow_unused=allow_unused inputs_require_grads l = allSum grads l grads = torch autograd grad l flattened_recording_inputs allow_unused=allow_unused inputs_require_grads recording_inputs = do_input_map lambda t Variable t requires_grad=True reference_tensors flattened_recording_inputs = flatten_inputs recording_inputs outputs_ge = ge recording_inputs l _ge = allSum outputs_ge inputs_require_grads grads_ge = torch autograd grad l _ge flattened_recording_inputs create_graph=True allow_unused=allow_unused inputs_require_grads l _ge = allSum grads_ge l _ge grads _ge = torch autograd grad l _ge flattened_recording_inputs allow_unused=allow_unused assertEqual outputs outputs_ge inputs_require_grads assertEqual grads grads_ge atol=grad_atol rtol=grad_rtol g g _ge zip grads grads _ge strict=True g None g _ge None continue assertEqual g g _ge atol= e- rtol= e- ge checkModule nn_module args Check nn Module s results Script mode match eager can exported sm = torch jit script nn_module freeze_rng_state eager_out = nn_module args freeze_rng_state script_out = sm args assertEqual eager_out script_out assertExportImportModule sm args sm NoTracerWarnContextManager __enter__ prev = torch _C _jit_get_tracer_state_warn torch _C _jit_set_tracer_state_warn False __exit__ args torch _C _jit_set_tracer_state_warn prev contextmanager inline_everything_mode should_inline old = torch _C _jit_get_inline_everything_mode torch _C _jit_set_inline_everything_mode should_inline try yield finally torch _C _jit_set_inline_everything_mode old contextmanager set_fusion_group_inlining inlining old = torch _C _debug_get_fusion_group_inlining torch _C _debug_set_fusion_group_inlining inlining try yield finally torch _C _debug_set_fusion_group_inlining old note re-entrant use unnested only contextmanager disable_autodiff_subgraph_inlining enabled=True torch _C _debug_set_autodiff_subgraph_inlining enabled try yield finally torch _C _debug_set_autodiff_subgraph_inlining True _inline_everything fn functools wraps fn wrapper args kwargs inline_everything_mode True fn args kwargs wrapper exists forward compatibility reasons temporarily TODO suo remove _tmp_donotuse_dont_inline_everything fn functools wraps fn wrapper args kwargs inline_everything_mode False fn args kwargs wrapper make easy quickly define trace function these tests _trace args kwargs wrapper func torch jit trace func args kwargs wrapper enable_cpu_fuser fn wrapper args kwargs torch _C _jit_override_can_fuse_on_cpu_legacy True torch _C _jit_override_can_fuse_on_cpu True torch _C _jit_set_te_must_use_llvm_cpu False try fn args kwargs finally torch _C _jit_override_can_fuse_on_cpu_legacy False torch _C _jit_override_can_fuse_on_cpu False torch _C _jit_set_te_must_use_llvm_cpu True wrapper enable_cpu_fuser_if cond cond enable_cpu_fuser noop_fuser fn wrapper args kwargs fn args kwargs wrapper noop_fuser get_forward c c _get_method forward get_forward_graph c c _get_method forward graph get_module_method m module method m _c getattr module _get_method method attrs_with_prefix module prefix x x _ module _modules _c items x startswith prefix warmup_backward f args profiling_count = results = _ range profiling_count len args r = torch autograd grad f args results append r f backward retain_graph=True results TODO Remove me once https bugs python org issue resolved make_global args arg args setattr sys modules arg __module__ arg __name__ arg Helper function eval Python code without causing syntax error file under py _get_py _code code fn_name tempfile TemporaryDirectory tmp_dir script_path = os path join tmp_dir script py open script_path w f f write code spec = importlib util spec_from_file_location fn_name script_path module = importlib util module_from_spec spec loader = spec loader assert isinstance loader Loader Assert type meet MyPy requirement loader exec_module module fn = getattr module fn_name fn TensorExprTestOptions __init__ - None old_profiling_executor = torch _C _jit_set_profiling_executor True old_profiling_mode = torch _C _get_graph_executor_optimize True old_cpu_fuser_state = torch _C _jit_can_fuse_on_cpu old_gpu_fuser_state = torch _C _jit_can_fuse_on_gpu torch _C _jit_override_can_fuse_on_cpu True torch _C _jit_override_can_fuse_on_gpu True texpr_fuser_state = torch _C _jit_texpr_fuser_enabled torch _C _jit_set_texpr_fuser_enabled True old_fusion_inlining = torch _C _debug_get_fusion_group_inlining torch _C _debug_set_fusion_group_inlining False old_te_must_use_llvm_cpu = torch _C _jit_get_te_must_use_llvm_cpu torch _C _jit_set_te_must_use_llvm_cpu False restore torch _C _jit_set_profiling_executor old_profiling_executor torch _C _get_graph_executor_optimize old_profiling_mode torch _C _jit_set_texpr_fuser_enabled texpr_fuser_state torch _C _jit_override_can_fuse_on_gpu old_gpu_fuser_state torch _C _jit_override_can_fuse_on_cpu old_cpu_fuser_state torch _C _debug_set_fusion_group_inlining old_fusion_inlining torch _C _jit_set_te_must_use_llvm_cpu old_te_must_use_llvm_cpu clone_inputs args inputs list Union torch Tensor list torch Tensor = arg args isinstance arg torch Tensor inputs append arg detach clone is_iterable_of_tensors arg inputs append t detach clone t arg inputs append arg inputs get_traced_sample_variant_pairs device dtype op tuples variant sample outputs list tuple Any Any = samples = op sample_inputs device dtype Acquires variants test func = op get_op method = op get_method variants = TODO inplace tests currently fail fix add inplace variant function func method method TODO find better way standardize op registration itself has_fake_function = op name resize_ resize_as_ has_fake_function variants = method getattr torch Tensor op name In eager mode these ops can take Tensor bool args JIT they can only take Tensor Scalar bool scalar JIT type system So test these JIT bool converted int test ops_with_unsupported_bool_args = name div_floor_rounding arg_idx name div_no_rounding_mode arg_idx name div_trunc_rounding arg_idx name index_fill arg_idx name full_like arg_idx name mul arg_idx name new_full arg_idx doesn t support tracing has_fake_function outputs sample samples variant variants values variant None continue is_lambda variant continue matching_ops = filter lambda x op formatted_name == x name ops_with_unsupported_bool_args op_data matching_ops idx op_data arg_idx args = list sample args len sample args idx isinstance sample args idx bool args idx = int args idx sample args = tuple args outputs append variant sample outputs types LambdaType gave false positives is_lambda lamb LAMBDA = lambda noqa E isinstance lamb type LAMBDA lamb __name__ == LAMBDA __name__