Owner s module inductor atexit contextlib functools math os sys unittest collections defaultdict enum Enum functools partial unittest mock patch torch torch _dispatch python enable_python_dispatcher torch _inductor test_case run_tests TestCase torch _subclasses fake_tensor DataDependentOutputException DynamicOutputShapeException FakeTensorMode torch testing _internal common_cuda SM OrLater torch testing _internal common_device_type instantiate_device_type_tests onlyNativeDeviceTypes OpDTypes ops skipCPUIf skipXPUIf torch testing _internal common_methods_invocations op_db skipOps torch testing _internal common_utils IS_CI IS_MACOS IS_WINDOWS IS_X skipCUDAMemoryLeakCheckIf skipIfCrossRef skipIfTorchDynamo suppress_warnings TEST_MKL TEST_WITH_ASAN TEST_WITH_ROCM torch testing _internal inductor_utils GPU_TYPE HAS_CPU has_triton HAS_XPU_AND_TRITON maybe_skip_size_asserts torch testing _internal triton_utils requires_gpu_and_triton torch utils _dtype_abbrs dtype_abbrs torch utils _python_dispatch TorchDispatchMode torch utils _pytree tree_map try try test_torchinductor check_model check_model_gpu except ImportError test_torchinductor manual=fbcode caffe test inductor test_inductor-library check_model check_model_gpu except unittest SkipTest ImportError e sys stderr write f type e e \n __name__ == __main__ sys exit raise IS_WINDOWS IS_CI TODO xuhancn improve compiler build performance windows sys stderr write This UT too slow windows will cause out time CI So skip now \n __name__ == __main__ sys exit raise unittest SkipTest skip slow test bf = torch bfloat tested f = torch float f = torch float f = torch float i = torch int tested i = torch int tested i = torch int i = torch int b = torch bool u = torch uint tested except upsampling interpolate ops u = torch uint tested u = torch uint tested u = torch uint tested _ops = partial ops dtypes=OpDTypes supported allowed_dtypes= f f f i i b u u u u Success forces pass failure forces fail skip unconditionally skips testing ExpectedTestResult = Enum ExpectedTestResult SUCCESS XFAILURE SKIP COLLECT_EXPECT = os getenv PYTORCH_COLLECT_EXPECT == ALL_SAMPLES = os getenv PYTORCH_ALL_SAMPLES == START = os getenv PYTORCH_TEST_RANGE_START None END = os getenv PYTORCH_TEST_RANGE_END None START None END None assert END None assert START None START = int START END = int END assert START END START = END = len op_db seen_failed = defaultdict set failed_reasons = defaultdict set print_seen expected_failures = defaultdict list fmt_dtypes dtypes r = join sorted dtype_abbrs d d dtypes + r + sort_key kv k _ = kv _ op = k isinstance op tuple op op device_type op failed_dtypes sorted seen_failed items key=sort_key key = device_type op reasons = failed_reasons key maybe_truncate x length= x = str x replace \n idx = x find \\n idx = x = f x idx len x length f x length - x reasons = sorted set map maybe_truncate failed_reasons key reasons = + join reasons failed_dtypes format_op op isinstance op tuple f op op f op expected_failures device_type append f format_op op fmt_dtypes failed_dtypes reasons device_type cpu GPU_TYPE expected_failures device_type nl = \n print f inductor_expected_failures_single_sample \ device_type \ = nl join expected_failures device_type COLLECT_EXPECT atexit register print_seen Note these skip xfail dictionaries use string key default test tuple two strings variants inductor_skips = defaultdict dict inductor_skips cpu = linalg ldl_factor f f flaky nn functional cosine_embedding_loss b flaky index_reduce prod f flaky index_reduce mean f flaky IS_MACOS IS_X inductor_skips cpu rsqrt = b i inductor_skips cpu nn functional multi_margin_loss = b f f f i i inductor_skips cuda = Jiterator kernel expected work inductor jiterator_ inputs_ outputs b f f f i i jiterator_ inputs_with_extra_args b f f f i i jiterator_binary b f f f i i jiterator_binary_return_by_ref b f f f i i jiterator_unary b f f f i i flaky nn functional cosine_embedding_loss b native_batch_norm f f f _native_batch_norm_legit f f f _batch_norm_with_update f f f SM OrLater inductor_skips cuda bfloat = b f f f i i TEST_WITH_ROCM Tensors alike inductor_skips cuda logcumsumexp = f inductor_skips cuda special modified_bessel_i = f inductor_skips xpu = inductor_expected_failures_single_sample = defaultdict dict inductor_expected_failures_single_sample cpu = _softmax_backward_data f half_to_float only valid CUDA implementation _upsample_bilinear d_aa f f cholesky f f complex f resize_ b f f f i i resize_as_ b f f f i i histc f multinomial f f f nonzero_static b f f f i i normal in_place f f f normal number_mean f f f normal f f f sparse mm reduce f f f sparse sampled_addmm f f to_sparse f f NYI could find kernel aten view default dispatch key DispatchKey SparseCPU view_as_complex f inductor_expected_failures_single_sample cuda = _upsample_bilinear d_aa f f f cholesky f f multinomial f f f normal in_place f f f normal number_mean f f f normal f f f sparse sampled_addmm f f torch ops aten _flash_attention_forward f torch ops aten _efficient_attention_forward f f to_sparse b f f f i i NYI could find kernel aten view default dispatch key DispatchKey SparseCUDA inductor_expected_failures_single_sample xpu = _upsample_bilinear d_aa f f f cholesky f f multinomial f f f normal in_place f f f normal number_mean f f f normal f f f sparse sampled_addmm f f tan f torch ops aten _flash_attention_forward f torch ops aten _efficient_attention_forward f f to_sparse b f f f i i align cuda linalg pinv singular f could create primitive addmv f could create primitive descriptor deconvolution forward propagation primitive nn functional conv_transpose d f f nn functional conv_transpose d f f Begin Incorrect XPU reference due new driver masked prod b i i masked amin i masked amax i amax i amin i std f var f std_mean f var_mean f End intentionally handled intentionally_not_handled = resize_ b f f f i i resize_as_ b f f f i i This only fixed when config set We should eventually always turn torch _functorch config functorch_config functorch_config view_replay_for_aliased_outputs intentionally_not_handled as_strided partial_views = b f f f i i inductor_expected_failures_single_sample cuda update intentionally_not_handled inductor_expected_failures_single_sample xpu update intentionally_not_handled inductor_gradient_expected_failures_single_sample = defaultdict dict inductor_gradient_expected_failures_single_sample cuda = inductor_gradient_expected_failures_single_sample xpu = TEST_MKL inductor_expected_failures_single_sample cpu update inductor_should_fail_with_exception = defaultdict dict inductor_should_fail_with_exception cpu = inductor_should_fail_with_exception cuda = inductor_should_fail_with_exception xpu = get_skips_and_xfails from_dict xfails=True retval = set device d from_dict items op dtypes d items type op tuple op variant_name = op variant_name = retval add op variant_name device tuple dtypes xfails retval Note you get AssertionError Couldn t find OpInfo error OpInfo you sure exists you might trying use test variant you need replace example max reduction_no_dim max reduction_no_dim key one these dictionaries test_skips_or_fails = get_skips_and_xfails inductor_skips xfails=False &#124; get_skips_and_xfails inductor_expected_failures_single_sample xfails=True &#124; get_skips_and_xfails inductor_gradient_expected_failures_single_sample xfails=True wrapper_noop_set_seed op args kwargs op args kwargs wrapper_noop_set_seed_decorator = patch torch testing _internal common_methods_invocations wrapper_set_seed wrapper_noop_set_seed key can either op_name op_name dtype inductor_override_kwargs = defaultdict dict inductor_override_kwargs cpu = value empty undefined empty assert_equal False empty_permuted assert_equal False empty_like assert_equal False new_empty assert_equal False empty_strided assert_equal False new_empty_strided assert_equal False randn assert_equal False nn functional multilabel_soft_margin_loss f atol e- rtol nn functional triplet_margin_loss f atol e- rtol nn functional triplet_margin_with_distance_loss f atol e- rtol softmax f atol e- rtol polygamma polygamma_n_ f atol e- rtol e- polygamma polygamma_n_ f atol e- rtol e- polygamma polygamma_n_ f atol e- rtol e- polygamma polygamma_n_ f atol e- rtol e- polygamma polygamma_n_ f atol e- rtol e- special polygamma special_polygamma_n_ f atol e- rtol e- _unsafe_masked_index_put_accumulate f atol e- rtol Following tests failing strict comparison atol= acceptable due roundings errors nn functional interpolate bilinear u atol rtol nn functional upsample_bilinear u atol rtol nn functional interpolate bicubic u atol rtol High atol due precision loss nn functional interpolate bicubic f atol e- rtol inductor_override_kwargs cuda = value empty undefined empty assert_equal False empty_permuted assert_equal False empty_like assert_equal False new_empty assert_equal False empty_strided assert_equal False new_empty_strided assert_equal False randn assert_equal False cross f reference_in_float True linalg cross f reference_in_float True addr f reference_in_float True baddbmm f atol e- rtol decomp affects accuracy angle f reference_in_float True asin f reference_in_float True atanh f reference_in_float True cauchy reference_in_float True cummax f atol e- rtol cumsum f reference_in_float True cumprod reference_in_float True atol e- rtol logcumsumexp grad_atol e- grad_rtol logcumsumexp f grad_atol e- grad_rtol exponential reference_in_float True geometric reference_in_float True kron f reference_in_float True log_normal reference_in_float True masked softmin f atol e- rtol nn functional batch_norm f reference_in_float True nn functional batch_norm without_cudnn f reference_in_float True nn functional cosine_similarity f reference_in_float True nn functional instance_norm f reference_in_float True nn functional linear f atol e- rtol nn functional local_response_norm f reference_in_float True nn functional normalize f atol e- rtol nn functional rms_norm f reference_in_float True nn functional soft_margin_loss f reference_in_float True nn functional softmin f atol e- rtol nn functional softsign f reference_in_float True nn functional tanhshrink f atol e- rtol outer f reference_in_float True round decimals_ f reference_in_float True nn functional triplet_margin_loss f atol e- rtol nn functional triplet_margin_with_distance_loss f atol e- rtol sinc f atol rtol torch ops aten _safe_softmax default f atol e- rtol softmax f atol e- rtol _softmax_backward_data f atol rtol special log_ndtr f atol e- rtol e- std_mean unbiased f reference_in_float True uniform reference_in_float True _unsafe_masked_index_put_accumulate f atol e- rtol High atol due precision loss nn functional interpolate bilinear f atol e- rtol nn functional upsample_bilinear f atol e- rtol nn functional interpolate bicubic f atol e- rtol Unreasonably high atol requirement index_reduce mean f check_gradient False index_reduce mean f check_gradient False index_reduce mean f check_gradient False Gradient contains non-finite entries index_reduce amin f check_gradient False index_reduce amin f check_gradient False index_reduce amin f check_gradient False index_reduce amax f check_gradient False index_reduce amax f check_gradient False index_reduce amax f check_gradient False tanh f atol e- rtol e- _unsafe_masked_index f reference_in_float True atol e- rtol e- nn functional interpolate linear f reference_in_float True nn functional prelu f reference_in_float True atol e- rtol e- addmm f reference_in_float True logaddexp f reference_in_float True std_mean f reference_in_float True hypot f reference_in_float True atol e- rtol e- cummin f reference_in_float True atol e- rtol e- unfold_copy f reference_in_float True atol e- rtol e- nn functional upsample_bilinear f reference_in_float True atol e- rtol e- nn functional embedding_bag f reference_in_float True atol e- rtol e- fft irfft f reference_in_float True atol e- rtol e- fft irfftn f reference_in_float True atol e- rtol e- inductor_override_kwargs xpu = value empty undefined empty assert_equal False empty_permuted assert_equal False empty_like assert_equal False new_empty assert_equal False empty_strided assert_equal False new_empty_strided assert_equal False randn assert_equal False XPU cross f reference_in_float True addr f reference_in_float True baddbmm f atol e- rtol decomp affects accuracy angle f reference_in_float True asin f reference_in_float True asin f reference_in_float True atol e- rtol e- atanh f reference_in_float True cauchy reference_in_float True cummax f atol e- rtol cumsum f reference_in_float True cumprod reference_in_float True atol e- rtol dot f atol e- rtol logcumsumexp atol e- rtol grad_atol e- grad_rtol logcumsumexp f grad_atol e- grad_rtol exponential reference_in_float True geometric reference_in_float True kron f reference_in_float True linalg cross f reference_in_float True linalg vecdot f atol e- rtol e- log_normal reference_in_float True logsumexp f atol e- rtol e- masked cumprod f reference_in_float True atol e- rtol e- masked cumsum f atol e- rtol e- masked softmin f atol e- rtol masked softmax f atol e- rtol masked var f atol e- rtol e- native_batch_norm f atol e- rtol e- _native_batch_norm_legit f atol e- rtol e- _batch_norm_with_update f atol e- rtol e- native_layer_norm f atol e- rtol e- native_layer_norm f atol e- rtol e- nn functional batch_norm f reference_in_float True nn functional batch_norm f atol e- rtol e- nn functional batch_norm without_cudnn f reference_in_float True nn functional conv d f atol e- rtol e- nn functional conv d f atol e- rtol e- nn functional conv_transpose d f atol e- rtol e- nn functional conv_transpose d f atol e- rtol e- nn functional cosine_embedding_loss f atol e- rtol e- nn functional cosine_similarity f reference_in_float True atol e- rtol e- nn functional instance_norm f reference_in_float True nn functional instance_norm f atol e- rtol e- nn functional layer_norm f atol e- rtol e- nn functional layer_norm f atol e- rtol e- nn functional local_response_norm f reference_in_float True nn functional multilabel_soft_margin_loss f atol e- rtol e- nn functional normalize f atol e- rtol nn functional rms_norm f reference_in_float True nn functional soft_margin_loss f reference_in_float True nn functional softmin f atol e- rtol nn functional softsign f reference_in_float True atol e- rtol nn functional tanhshrink f atol e- rtol outer f reference_in_float True round decimals_ f reference_in_float True nn functional triplet_margin_loss f atol e- rtol nn functional triplet_margin_with_distance_loss f atol e- rtol remainder f atol e- rtol sinc f atol rtol softmax f atol e- rtol _softmax_backward_data f atol rtol special log_ndtr f atol e- rtol e- std_mean unbiased f reference_in_float True atol e- rtol e- trapezoid f atol e- rtol e- trapz f atol e- rtol e- uniform reference_in_float True var_mean f atol e- rtol e- var_mean unbiased f atol e- rtol e- vdot f atol e- rtol e- Following tests failing strict comparison atol= acceptable due roundings errors High atol due precision loss nn functional interpolate bilinear f atol e- rtol nn functional upsample_bilinear f atol e- rtol nn functional interpolate bicubic f atol e- rtol Unreasonably high atol requirement index_reduce mean f check_gradient False index_reduce mean f check_gradient False index_reduce mean f check_gradient False Gradient contains non-finite entries index_reduce amin f check_gradient False index_reduce amin f check_gradient False index_reduce amin f check_gradient False index_reduce amax f check_gradient False index_reduce amax f check_gradient False index_reduce amax f check_gradient False tanh f atol e- rtol e- nn functional embedding_bag f check_gradient False nn functional embedding_bag f check_gradient False _unsafe_masked_index_put_accumulate f atol e- rtol _unsafe_masked_index f reference_in_float True atol e- rtol e- nn functional interpolate linear f reference_in_float True nn functional prelu f reference_in_float True atol e- rtol e- addmm f reference_in_float True logaddexp f reference_in_float True std_mean f reference_in_float True hypot f reference_in_float True atol e- rtol e- cummin f reference_in_float True atol e- rtol e- unfold_copy f reference_in_float True atol e- rtol e- nn functional upsample_bilinear f reference_in_float True atol e- rtol e- nn functional embedding_bag f check_gradient False atol e- rtol e- nn functional max_pool d f reference_in_float True atol e- rtol e- nn functional unfold f reference_in_float True Reference crash Intel LTS driver nn functional interpolate trilinear f check_gradient False Reference crash Intel LTS driver nn functional interpolate trilinear f check_gradient False TEST_WITH_ROCM inductor_override_kwargs cuda update cummin f atol e- rtol e- Test one sample only following ops inductor_one_sample = defaultdict dict inductor_one_sample cpu = _segment_reduce lengths f _segment_reduce offsets f addmv f as_strided partial_views f corrcoef f diff f einsum f i gradient f histogram f f histogramdd f f index_put f f f linalg eig f f linspace f i i linspace tensor_overload f f f i i logspace f logspace tensor_overload f f f i i masked_logsumexp i max_pool d_with_indices_backward f f f new_empty_strided f nn functional adaptive_avg_pool d f nn functional adaptive_max_pool d f f nn functional adaptive_max_pool d f f nn functional bilinear f nn functional conv_transpose d f nn functional conv_transpose d f nn functional conv_transpose d f nn functional cosine_similarity f nn functional cross_entropy f f f nn functional gaussian_nll_loss f nn functional grid_sample f f f nn functional interpolate area f nn functional nll_loss f f f normal f f f put f f f take b f f f i i inductor_one_sample cuda = _segment_reduce lengths f _segment_reduce offsets f addmv f as_strided partial_views f corrcoef f diff f einsum f i gradient f histogram f f histogramdd f f index_put f f f linalg eig f f linspace f i i linspace tensor_overload f f f i i logspace f i i logspace tensor_overload f f f i i masked_logsumexp i max_pool d_with_indices_backward f f f new_empty_strided f nn functional adaptive_avg_pool d f nn functional adaptive_max_pool d f f nn functional adaptive_max_pool d f f nn functional bilinear f nn functional conv_transpose d f nn functional conv_transpose d f nn functional conv_transpose d f nn functional cosine_similarity f nn functional cross_entropy f f f nn functional gaussian_nll_loss f nn functional grid_sample f f f nn functional interpolate area f nn functional nll_loss f f f normal f f f put f f f take b f f f i i __rdiv__ f __rmod__ f i __rmul__ f __rpow__ f _unsafe_masked_index f _unsafe_masked_index_put_accumulate f addcdiv f addcmul f atan f cumsum f cumulative_trapezoid f dist f div no_rounding_mode f fmod f grid_sampler_ d f index_fill f f f ldexp f lerp f linalg householder_product f linalg matrix_norm f linalg vector_norm f masked cumsum f masked logsumexp f masked mean b masked normalize f masked prod f masked std f masked var f mul f nn functional alpha_dropout f f f nn functional avg_pool d f f f nn functional avg_pool d f f f nn functional avg_pool d f f f nn functional binary_cross_entropy f nn functional binary_cross_entropy_with_logits f nn functional conv d f nn functional cosine_embedding_loss f nn functional dropout d f f f nn functional dropout d f f f nn functional dropout f f f nn functional feature_alpha_dropout with_train f f f nn functional fractional_max_pool d f f f nn functional fractional_max_pool d f f f nn functional group_norm f nn functional hinge_embedding_loss f Enabling all tests test fails randomly See https github com pytorch pytorch issues nn functional huber_loss f nn functional interpolate bicubic f nn functional interpolate bilinear f nn functional interpolate trilinear f nn functional kl_div f nn functional margin_ranking_loss f nn functional max_pool d f f f nn functional max_pool d f nn functional mse_loss f nn functional multi_margin_loss f nn functional multilabel_margin_loss f nn functional multilabel_soft_margin_loss f nn functional normalize f nn functional pad replicate f f f nn functional pad reflect f nn functional pairwise_distance f nn functional poisson_nll_loss f nn functional rms_norm f norm f pow f prod f scatter_reduce amax f f f scatter_reduce amin f f f scatter_reduce mean f f f special xlog py f std f std_mean f svd_lowrank f f trapezoid f trapz f true_divide f var f var_mean f xlogy f inductor_one_sample xpu = _segment_reduce lengths f _segment_reduce offsets f addmv f as_strided partial_views f corrcoef f diff f einsum f i gradient f histogram f f histogramdd f f index_put f f f linalg eig f f linspace f i i linspace tensor_overload f f f i i logspace f i i logspace tensor_overload f f f i i masked_logsumexp i max_pool d_with_indices_backward f f f new_empty_strided f nn functional adaptive_avg_pool d f nn functional adaptive_max_pool d f f nn functional adaptive_max_pool d f f nn functional max_pool d f f f nn functional bilinear f nn functional conv_transpose d f nn functional conv_transpose d f nn functional conv_transpose d f nn functional cosine_similarity f nn functional cross_entropy f f f nn functional gaussian_nll_loss f nn functional grid_sample f f f nn functional interpolate area f nn functional nll_loss f f f normal f f f put f f f take b f f f i i __rdiv__ f __rmod__ f i __rmul__ f __rpow__ f _unsafe_masked_index f _unsafe_masked_index_put_accumulate f addcdiv f addcmul f atan f cumsum f cumulative_trapezoid f dist f div no_rounding_mode f fmod f grid_sampler_ d f index_fill f f f ldexp f lerp f linalg householder_product f linalg matrix_norm f linalg vector_norm f masked cumsum f masked logsumexp f masked mean b masked normalize f masked prod f masked std f masked var f mul f nn functional alpha_dropout f f f nn functional avg_pool d f f f nn functional avg_pool d f f f nn functional avg_pool d f f f nn functional binary_cross_entropy f nn functional binary_cross_entropy_with_logits f nn functional conv d f nn functional cosine_embedding_loss f nn functional dropout d f f f nn functional dropout d f f f nn functional dropout f f f nn functional feature_alpha_dropout with_train f f f nn functional fractional_max_pool d f f f nn functional fractional_max_pool d f f f nn functional group_norm f nn functional hinge_embedding_loss f Enabling all tests test fails randomly See https github com pytorch pytorch issues nn functional huber_loss f nn functional interpolate bicubic f nn functional interpolate bilinear f nn functional interpolate trilinear f nn functional kl_div f nn functional margin_ranking_loss f nn functional max_pool d f f f nn functional max_pool d f nn functional mse_loss f nn functional multi_margin_loss f nn functional multilabel_margin_loss f nn functional multilabel_soft_margin_loss f nn functional normalize f nn functional pad replicate f f f nn functional pad reflect f nn functional pairwise_distance f nn functional poisson_nll_loss f nn functional rms_norm f norm f pow f prod f scatter_reduce amax f f f scatter_reduce amin f f f scatter_reduce mean f f f special xlog py f std f std_mean f svd_lowrank f f trapezoid f trapz f true_divide f var f var_mean f xlogy f Custom replacements assertEquals cases where difference value may indicate correctness get_sort_argsort_assert_equal_fn is_argsort args kwargs Use normal assert_equal_fn suffices stable sort stable kwargs True In other cases we need only check sort argsort outputs compatible orig_input = args The sort dimension specified kwarg last dimension dim kwargs dim = orig_input dim - dim = kwargs dim argsort_sort_assert_equal test_case_inst x y atol=None rtol=None equal_nan=True exact_dtype=True exact_stride=False is_argsort assert isinstance x torch Tensor assert isinstance y torch Tensor The first tensor sorted values can asserted via usual means t x y assert isinstance t tuple assert len t == test_case_inst assertEqual x y atol=atol rtol=rtol equal_nan=equal_nan exact_dtype=exact_dtype exact_stride=exact_stride The second tensor same result argsort x = x y = y exact_dtype x dtype = y dtype raise AssertionError f The dtypes do match x dtype = y dtype assert x shape == y shape exact_stride x stride = y stride raise AssertionError f The strides do match x stride = y stride el_to_indices el Turn element number into list indices indices = None x dim cur_dim reversed range x dim indices cur_dim = el x shape cur_dim el = x shape cur_dim assert None indices indices get_val_by_ids t ids Return value tensor given list indices idx ids t = t idx t item Loop through every value tensors check equality compatibility current_el range x numel ids = el_to_indices current_el Simple case check equality arsort indices get_val_by_ids x ids == get_val_by_ids y ids continue Complex case check indices refer same value x_orig_ids = ids copy y_orig_ids = ids copy x_orig_ids dim = get_val_by_ids x ids y_orig_ids dim = get_val_by_ids y ids x_value = get_val_by_ids orig_input x_orig_ids y_value = get_val_by_ids orig_input y_orig_ids x_value == y_value continue equal_nan math isnan x_value math isnan y_value continue raise AssertionError f Non-stable argsort outputs incompatible ids argsort_sort_assert_equal get_argsort_assert_equal_fn args kwargs get_sort_argsort_assert_equal_fn True args kwargs get_sort_assert_equal_fn args kwargs get_sort_argsort_assert_equal_fn False args kwargs CUSTOM_ASSERT_EQUALS_FNS = argsort get_argsort_assert_equal_fn sort get_sort_assert_equal_fn collection_decorator fn functools wraps fn inner device dtype op try fn device dtype op except Exception e COLLECT_EXPECT variant = op variant_test_name op_key = op name variant op name variant device_type = torch device device type failed_reasons device_type op_key add repr e seen_failed device_type op_key add dtype raise e inner wrapper_noop_set_seed_decorator TestInductorOpInfo TestCase tearDown torch _dynamo reset check_model = check_model check_model_gpu = check_model_gpu onlyNativeDeviceTypes suppress_warnings skipCUDAMemoryLeakCheckIf True inductor kernels failing test intermittently requires_gpu_and_triton skipXPUIf HAS_XPU_AND_TRITON Skipped Supported XPU compiler Triton found skipCPUIf HAS_CPU Skipped Supported CPU compiler found unittest skipIf TEST_WITH_ASAN Skipped under ASAN skipIfTorchDynamo Test uses dynamo already skipIfCrossRef _ops op_db START END skipOps TestInductorOpInfo test_comprehensive test_skips_or_fails patch torch _dynamo config raise_on_unsafe_aot_autograd True torch _inductor config patch implicit_fallbacks False triton autotune_pointwise False torch _inductor config patch test_configs runtime_triton_dtype_assert True torch _inductor config patch test_configs static_cpp_dtype_assert True collection_decorator test_comprehensive device dtype op device_type = torch device device type assert device_type GPU_TYPE cpu torch _dynamo reset torch no_grad TODO should we move empty_cache common device interface device_type == cuda torch cuda empty_cache device == xpu torch xpu empty_cache op_name = op name op variant_test_name op_name += f op variant_test_name Skip dtype=torch uint all ops except upsample interpolate allowed_dtypes = f f f i i b op_name nn functional interpolate bilinear nn functional interpolate bicubic nn functional upsample_bilinear nn functional upsample_nearest fill full_like dtype allowed_dtypes raise unittest SkipTest Skipped open test_output txt f print f CONSIDERING OP op_name device_type dtype &#124; inductor_skips device_type get op_name set flush=True file=f print f CONSIDERING OP op_name device_type dtype &#124; inductor_skips device_type get op_name set flush=True dtype inductor_skips device_type get op_name set test_expect = ExpectedTestResult SKIP noqa F open test_output txt f print f SKIPPING OP op_name device_type flush=True file=f print f SKIPPING OP op_name device_type flush=True dtype inductor_expected_failures_single_sample device_type get op_name set dtype inductor_gradient_expected_failures_single_sample device_type get op_name set test_expect = ExpectedTestResult XFAILURE noqa F test_expect = ExpectedTestResult SUCCESS noqa F overridden_kwargs = overridden_kwargs update inductor_override_kwargs get device_type get op_name overridden_kwargs update inductor_override_kwargs get device_type get op_name dtype func = op get_op fn args kwargs func args kwargs requires_grad = op supports_autograd dtype op supported_backward_dtypes device_type TODO OpInfo really ought error out case s exercised test_ops_gradients atm The problem complex per-se which supported data movement only ops when we do backwards we expect other ops like add work dtype = torch complex samples = op sample_inputs device dtype requires_grad=requires_grad dtype inductor_one_sample get device_type get op_name ALL_SAMPLES isinstance samples list tuple samples = samples samples = next samples HasRngOp TorchDispatchMode __init__ - None super __init__ has_rng_op = False __torch_dispatch__ func types args kwargs=None kwargs = kwargs kwargs torch Tag nondeterministic_seeded func tags has_rng_op = True func args kwargs do_nopython_and_has_rng fn args kwargs try mode = FakeTensorMode map_to_fake e isinstance e torch Tensor mode from_tensor e e args kwargs = tree_map map_to_fake args kwargs HasRngOp rng_mode mode enable_python_dispatcher fn args kwargs except DataDependentOutputException DynamicOutputShapeException False rng_mode has_rng_op True rng_mode has_rng_op get_contexts has_rng_op args kwargs has_rng_op TODO - enable running into errors lambda torch _inductor config patch fallback_random True implicit_fallbacks True assert_equal True contextlib nullcontext assert_equal False ctx = functools partial maybe_skip_size_asserts op op_name CUSTOM_ASSERT_EQUALS_FNS assert_equal_fn = CUSTOM_ASSERT_EQUALS_FNS op_name args kwargs ctx assert_equal assert_equal_fn ctx try _get_tolerances dtype _custom_tolerances = torch float e- e- When we running opportunistic_fastatomics we will expect some floating point rounding errors order operation guaranteed dtype _custom_tolerances _custom_tolerances dtype None None sample_input samples args = sample_input input + list sample_input args kwargs = sample_input kwargs UNCOMMENT TO DEBUG SEGFAULTS open test_output txt f print f RUNNING OP op_name device_type dtype flush=True file=f print f RUNNING OP op_name device_type dtype flush=True rtol atol = _get_tolerances dtype no_python has_rng_op = do_nopython_and_has_rng fn args kwargs context_fn kwarg_overrides get_contexts has_rng_op args kwargs context_fn Base kwargs adjusted_kwargs = check_lowp False nopython no_python check_has_compiled no_python atol atol rtol rtol Backend-specific adjustments Triton has_triton adjusted_kwargs update copy_to_gpu=False reference_in_float=False skip checking gradient CPU now device_type == GPU_TYPE adjusted_kwargs update check_gradient=requires_grad output_process_fn_grad=sample_input output_process_fn_grad adjusted_kwargs check_gradient = False Update overridden kwargs context-specific overrides adjusted_kwargs update overridden_kwargs adjusted_kwargs update kwarg_overrides Call appropriate check method based device type device_type == GPU_TYPE check_model_gpu fn args kwargs adjusted_kwargs check_model fn args kwargs adjusted_kwargs except Exception e known_failure = False dtype inductor_should_fail_with_exception device_type get op_name set failure = inductor_should_fail_with_exception device_type op_name dtype failure str e known_failure = True known_failure raise e open test_output txt f print f SUCCEEDED OP op_name device_type dtype flush=True file=f instantiate_device_type_tests TestInductorOpInfo globals allow_xpu=True __name__ == __main__ run_tests