mypy allow-untyped-defs Copyright c Meta Platforms Inc affiliates implement matrix related ops distributed tensor typing cast torch torch distributed tensor _op_schema OpSchema OpStrategy PlacementList StrategyType torch distributed tensor _ops utils expand_to_full_mesh_op_strategy register_op_strategy torch distributed tensor placement_types MaskPartial Partial Replicate Shard aten = torch ops aten register_op_strategy aten embedding default embedding_strategy op_schema OpSchema - StrategyType This strategy handles embedding op We have two possible embedding shardings rowwise colwise weight_strategy = cast OpStrategy op_schema args_schema indices_strategy = cast OpStrategy op_schema args_schema mesh = op_schema get_mesh_from_args weight_shape = weight_strategy shape indices_shape = indices_strategy shape output_emd_dim = len indices_shape single_mesh_dim_strategies = placement list stores placements output weight input_indices first we always have replicate all inputs output all_replicate PlacementList = Replicate single_mesh_dim_strategies append all_replicate colwise sharding output shard last dim weight shard dim input replicate colwise_sharding PlacementList = Shard output_emd_dim Shard Replicate single_mesh_dim_strategies append colwise_sharding rowwise sharding output embedding partial weight shard dim input accepts embedding partial embedding_partial_placement = MaskPartial offset_shape=weight_shape offset_dim= NOTE we want reuse same mask partial placement so we can reuse same mask generates input indices use output reduction rowwise_sharding PlacementList = embedding_partial_placement Shard embedding_partial_placement single_mesh_dim_strategies append rowwise_sharding batch dim sharding weight replicated input can shard any dim output follows input input_dim range len indices_shape batch_sharding PlacementList = Shard input_dim Replicate Shard input_dim single_mesh_dim_strategies append batch_sharding expand_to_full_mesh_op_strategy mesh op_schema single_mesh_dim_strategies register_op_strategy aten embedding_dense_backward default embedding_dense_backward_strategy op_schema OpSchema - StrategyType This strategy handles embedding op We have two possible embedding shardings rowwise colwise grad_out_strategy = cast OpStrategy op_schema args_schema indices_strategy = cast OpStrategy op_schema args_schema mesh = op_schema get_mesh_from_args grad_out_shape = grad_out_strategy shape indices_shape = indices_strategy shape grad_out_ndim = len grad_out_shape single_mesh_dim_strategies = placement list stores placements output weight input_indices first we always have replicate all inputs output all_replicate PlacementList = Replicate single_mesh_dim_strategies append all_replicate colwise sharding backward grad_out shard last dim input replicate weight grad shard colwise colwise_sharding PlacementList = Shard Shard grad_out_ndim - Replicate single_mesh_dim_strategies append colwise_sharding batch dim sharding weight replicated grad_out input have same sharding can shard any dim weight grad partial input_dim range len indices_shape batch_sharding PlacementList = Partial Shard input_dim Shard input_dim single_mesh_dim_strategies append batch_sharding grad_out partial input replicate weight grad keep partial partial_sharding PlacementList = Partial Partial Replicate single_mesh_dim_strategies append partial_sharding expand_to_full_mesh_op_strategy mesh op_schema single_mesh_dim_strategies