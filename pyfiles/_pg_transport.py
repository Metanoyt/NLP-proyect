logging pickle time collections abc Callable Generator contextlib contextmanager dataclasses dataclass datetime timedelta typing cast Optional TypeVar Union torch torch distributed ProcessGroup Work torch distributed _shard sharded_tensor Shard ShardedTensorShard ShardedTensor ShardMetadata torch distributed _shard sharded_tensor metadata ShardedTensorMetadata torch distributed tensor _DTensorSpec DTensor torch utils _pytree KeyPath tree_flatten_with_path tree_unflatten TreeSpec logger logging Logger = logging getLogger __name__ T = TypeVar T dataclass _TensorMeta This metadata tensor used transfer checkpoints It contains shape dtype storage offset stride tensor This must pickleable so can sent over wire shape torch Size dtype torch dtype storage_offset int stride tuple int nbytes int dataclass _DTensorMeta This metadata DTensor used transfer checkpoints It contains metadata local tensor spec DTensor This must pickleable so can sent over wire local _TensorMeta spec _DTensorSpec dataclass _ShardedTensorMeta This metadata ShardedTensor used transfer checkpoints It contains metadata all local shards global tensor metadata This must pickleable so can sent over wire local_shards_meta list _TensorMeta local_shards_shard_metadata list ShardMetadata Original shard metadata each local shard sharded_tensor_metadata ShardedTensorMetadata dataclass _StateDictMeta This metadata state dict used transfer checkpoints It contains step pytree spec state dict metadata each tensor state dict This must pickleable so can sent over wire Args step step checkpoint verify consistency treespec pytree spec state dict paths path each leaf state dict non_tensor_leaves metadata each tensor state dict any non-tensor leaves state dict treespec TreeSpec paths list KeyPath non_tensor_leaves list Union object _TensorMeta _DTensorMeta _ShardedTensorMeta contextmanager _timeit name str - Generator None None None start = time perf_counter yield dur = time perf_counter - start logger info s took ss name dur _prepare_tensor tensor torch Tensor - tuple torch Tensor _TensorMeta _cast_tensor tensor torch uint _TensorMeta shape=tensor shape dtype=tensor dtype storage_offset=cast int tensor storage_offset stride=tensor stride nbytes=tensor untyped_storage nbytes _prepare_state_dict state_dict object device torch device - tuple _StateDictMeta list torch Tensor leaves list tuple KeyPath object leaves treespec = tree_flatten_with_path state_dict paths list KeyPath = non_tensor_leaves list Union object _TensorMeta _DTensorMeta _ShardedTensorMeta = tensors list torch Tensor = key_path v leaves paths append key_path isinstance v DTensor tensor tensor_meta = _prepare_tensor v _local_tensor tensors append tensor non_tensor_leaves append _DTensorMeta local=tensor_meta spec=v _spec isinstance v ShardedTensor Handle ShardedTensor extracting all local shards local_shards = v local_shards Prepare metadata all local shards local_shards_meta = local_shards_shard_metadata = shard local_shards tensor tensor_meta = _prepare_tensor shard tensor tensors append tensor local_shards_meta append tensor_meta local_shards_shard_metadata append shard metadata non_tensor_leaves append _ShardedTensorMeta local_shards_meta=local_shards_meta local_shards_shard_metadata=local_shards_shard_metadata sharded_tensor_metadata=v metadata Complete metadata isinstance v torch Tensor tensor tensor_meta = _prepare_tensor v tensors append tensor non_tensor_leaves append tensor_meta non_tensor_leaves append v _StateDictMeta treespec=treespec paths=paths non_tensor_leaves=non_tensor_leaves tensors _cast_tensor tensor torch Tensor dtype torch dtype - torch Tensor Casts underlying storage tensor given dtype The returned tensor will size ` ` storage nbytes ` ` This works all datatypes supports strided offset tensors caveat cast tensor may larger than original tensor due differences striding type tensor torch Tensor raise AssertionError f can only cast standard tensors type tensor storage = tensor untyped_storage ret = torch tensor storage dtype=dtype device=tensor device ret untyped_storage storage raise AssertionError storage should same ret PGTransport This checkpoint transport uses process group transfer checkpoints This allows fast recovery workers fetching current weights existing worker Args pg process group use communication timeout timeout communication device device use tensors state_dict specified function will called do inplace receive into returned state_dict This much faster than having allocate new tensors transferring them CPU __init__ pg ProcessGroup timeout timedelta device torch device state_dict Optional Callable object = None - None _work list Work = _pg = pg _timeout = timeout pyrefly ignore read-only _device = device _state_dict = state_dict send_checkpoint dst_ranks list int state_dict object - None Send checkpoint multiple destination ranks The process Prepares state dict converting tensors serializable format Sends metadata pickled data Sends each tensor sequentially all destination ranks Args dst_ranks List destination ranks send checkpoint state_dict The state dictionary containing model parameters _timeit preparing state_dict meta tensors = _prepare_state_dict state_dict device=self _device work = _timeit send meta buf = pickle dumps meta len_t = torch tensor len buf dtype=torch int device=self _device buf_t = torch frombuffer buf dtype=torch uint _device dst_rank dst_ranks work append _pg send len_t dst_rank tag= work append _pg send buf_t dst_rank tag= _timeit send tensors i t enumerate tensors original_device = t device t = t _device dst_rank dst_ranks work append _pg send t dst_rank tag= + i we did copy we should wait work complete so we can free memory avoid OOMs original_device == torch device cpu w work w wait work = w work w wait recv_checkpoint src_rank int - object Receive checkpoint source rank The process Receives metadata about checkpoint structure Receives each tensor potentially reusing existing tensors in-place updates Reconstructs original state dict structure Args src_rank The source rank receive checkpoint Returns The reconstructed state dictionary model parameters state_dict = _state_dict _state_dict state_dict_leaves _ = tree_flatten_with_path state_dict dst_tensors dict KeyPath object = dict state_dict_leaves len_t = torch zeros dtype=torch int device=self _device _pg recv len_t src_rank tag= wait length = cast int len_t item buf = torch empty length dtype=torch uint device=self _device _pg recv buf src_rank tag= wait meta _StateDictMeta = pickle loads buf cpu numpy tobytes i int = works list Work = recv path KeyPath v _TensorMeta - torch Tensor nonlocal i inplace = dst_tensors get path isinstance inplace torch Tensor inplace device type == _device type isinstance inplace DTensor inplace = inplace _local_tensor t = _cast_tensor inplace torch uint t nbytes = v nbytes raise AssertionError inplace tensor storage must same size t = torch empty v nbytes dtype=torch uint device=self _device work = _pg recv t src_rank tag= + i i += inplace None inplace we need copy CPU avoid OOMing work wait t = t cpu works append work torch as_strided t view v dtype size=v shape stride=v stride storage_offset=v storage_offset values list object = path v zip meta paths meta non_tensor_leaves isinstance v _TensorMeta values append recv path v isinstance v _DTensorMeta tensor = recv path v local pyrefly ignore bad-argument-type bad-argument-count unexpected-keyword values append DTensor tensor v spec requires_grad=False isinstance v _ShardedTensorMeta Receive all local shards sent us local_shards = current_rank = _pg rank Receive tensors each local shard sent j shard_meta enumerate v local_shards_meta tensor = recv path shard_meta Use original shard metadata stored during preparation update placement reflect current rank device original_shard_metadata = v local_shards_shard_metadata j updated_shard_metadata = ShardMetadata shard_offsets=original_shard_metadata shard_offsets shard_sizes=original_shard_metadata shard_sizes placement=f rank current_rank tensor device type local_shard = ShardedTensorShard tensor=tensor metadata=updated_shard_metadata local_shards append local_shard Use complete metadata reconstruct ShardedTensor sharded_tensor = ShardedTensor _init_from_local_shards_and_global_metadata local_shards=local_shards sharded_tensor_metadata=v sharded_tensor_metadata values append sharded_tensor values append v work works work wait tree_unflatten values meta treespec