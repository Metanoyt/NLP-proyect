Owner s module inductor contextlib io json logging os re shutil sys tempfile unittest zipfile pathlib Path torch torch _C FileCheck torch _dynamo utils detect_fake_mode torch _inductor config torch _inductor debug create_kernel_information_json create_mapping_pre_post_grad_nodes create_node_mapping_kernel_to_post_grad reset_inductor_kernel_provenance_debug_handle torch _inductor fx_passes post_grad post_grad_passes torch _inductor test_case run_tests TestCase torch _inductor utils run_and_get_code run_and_get_cpp_code torch _inductor virtualized V torch testing _internal common_utils IS_MACOS torch testing _internal triton_utils requires_cuda_and_triton try test_aot_inductor_utils AOTIRunnerUtil test_torchinductor copy_tests except ImportError test_aot_inductor_utils AOTIRunnerUtil test_torchinductor copy_tests manual=fbcode caffe test inductor test_inductor-library trace_log = logging getLogger torch __trace Model torch nn Module __init__ super __init__ forward b c x = y = torch addmm c x b z = torch nn functional gelu y z Model torch nn Module test model used combo kernel provenance tracing info __init__ super __init__ forward b c = torch nn functional relu b = torch nn functional sigmoid b c = torch nn functional tanh c b c Model torch nn Module __init__ n k super __init__ weight = torch randn n k device= cuda bias = torch randn n device= cuda forward torch nn functional linear weight bias Model torch nn Module __init__ super __init__ fc = torch nn Linear relu = torch nn ReLU sigmoid = torch nn Sigmoid forward x b c x = fc x x = relu x x = sigmoid x d = y = torch addmm c d b z = torch nn functional gelu y x z config patch trace enabled True config patch trace provenance_tracking_level TestProvenanceTracingArtifact TestCase This test checks generated provenance tracing artifact post_grad corresponding inductor triton kernel node expected _check_provenance_tracing_kernel_to_post_grad filepath expected_data assertTrue filepath is_dir filename = Path filepath inductor_provenance_tracking_node_mappings json open filename f actual_data = json load f actual_data = actual_data cppCodeToPost check generated provenance tracing artifact expected assertEqual sorted actual_data items sorted expected_data items _check_provenance_tracking_node_mappings filepath expected_mapping assertTrue filepath is_dir filename = Path filepath inductor_provenance_tracking_node_mappings json open filename f actual_data = json load f check generated provenance tracing node mapping expected assertEqual sorted actual_data items sorted expected_mapping _test_triton_kernel_to_post_grad_tracing device = torch randn device=device b = torch randn device=device c = torch randn device=device example_inputs = b c model = Model device filepath = None backend aot_inductor inductor reset_inductor_kernel_provenance_debug_handle try config patch trace debug_dir tempfile mkdtemp force_disable_caches True assertLogs logging getLogger torch _inductor debug level=logging WARNING cm backend == aot_inductor AOTIRunnerUtil run model example_inputs ep = torch export _trace _export model example_inputs compiled = torch compile ep module backend=backend compiled example_inputs assertEqual len cm output m = re match r WARNING debug trace cm output assertTrue m filepath = Path m group device == cuda expected_mapping = cppCodeToPost triton_poi_fused_mul_ mul triton_poi_fused_addmm_gelu_ mul_ mul_ add_tensor add erf mul_ postToCppCode mul triton_poi_fused_mul_ mul_ triton_poi_fused_addmm_gelu_ mul_ triton_poi_fused_addmm_gelu_ add_tensor triton_poi_fused_addmm_gelu_ add triton_poi_fused_addmm_gelu_ erf triton_poi_fused_addmm_gelu_ mul_ triton_poi_fused_addmm_gelu_ postToPre mul mul mm_default addmm add_tensor addmm mul_ gelu mul_ gelu erf gelu add gelu mul_ gelu preToPost mul mul addmm mm_default add_tensor gelu mul_ mul_ erf add mul_ backend == aot_inductor expected_mapping aoti_torch_cuda_mm_out = mm_default expected_mapping mm_default = aoti_torch_cuda_mm_out expected_mapping extern_kernels mm = mm_default expected_mapping mm_default = extern_kernels mm _check_provenance_tracking_node_mappings filepath expected_mapping assert device == cpu check inductor kernel post grad nodes mapping expected cpu backend == aot_inductor expected_data = cpp_fused_mul_ mul aoti_torch_cpu_addmm_out addmm cpp_fused_gelu_ mul_ mul_ add erf mul_ backend == inductor expected_data = cpp_fused_mul_ mul cpp_fused_gelu_ mul_ mul_ add erf mul_ extern_kernels addmm addmm _check_provenance_tracing_kernel_to_post_grad filepath expected_data finally filepath shutil rmtree filepath requires_cuda_and_triton test_triton_kernel_to_post_grad_tracing_cuda _test_triton_kernel_to_post_grad_tracing device= cuda test_triton_kernel_to_post_grad_tracing_cpu _test_triton_kernel_to_post_grad_tracing device= cpu requires_cuda_and_triton test_triton_kernel_to_post_grad_tracing_extern_kernel M = N = K = model = Model N K batch = = torch randn batch M K device= cuda example_inputs = filepath = None backend aot_inductor inductor reset_inductor_kernel_provenance_debug_handle try config patch trace debug_dir tempfile mkdtemp force_disable_caches True assertLogs logging getLogger torch _inductor debug level=logging WARNING cm backend == aot_inductor AOTIRunnerUtil run model example_inputs ep = torch export _trace _export model example_inputs compiled = torch compile ep module backend=backend compiled example_inputs assertEqual len cm output m = re match r WARNING debug trace cm output assertTrue m filepath = Path m group backend == inductor expected_data = extern_kernels addmm addmm backend = aot_inductor expected_data = aoti_torch_cuda_addmm_out addmm triton_poi_fused_ _tensor_constant _check_provenance_tracing_kernel_to_post_grad filepath expected_data finally filepath shutil rmtree filepath requires_cuda_and_triton _test_pt_tracing_combo_kernel backend This test checks generated provenance tracing artifact triton combo kernel post grad nodes = torch randn device= cuda b = torch randn device= cuda c = torch randn device= cuda example_inputs = b c model = Model reset_inductor_kernel_provenance_debug_handle config patch trace debug_dir tempfile mkdtemp force_disable_caches True combo_kernels True benchmark_combo_kernel False assertLogs logging getLogger torch _inductor debug level=logging WARNING cm backend == aot_inductor AOTIRunnerUtil run model example_inputs ep = torch export _trace _export model example_inputs compiled = torch compile ep module backend=backend compiled example_inputs assertEqual len cm output m = re match r WARNING debug trace cm output assertTrue m filepath = Path m group resolve expected_data = triton_poi_fused_ relu sigmoid tanh _check_provenance_tracing_kernel_to_post_grad filepath expected_data requires_cuda_and_triton test_triton_kernel_to_post_grad_tracing_combo_kernel _test_pt_tracing_combo_kernel backend= inductor _test_pt_tracing_combo_kernel backend= aot_inductor TestProvenanceTracingNodeMapping TestCase test_create_node_mapping pre_grad_graph_id = post_to_pre_grad_nodes_json = add_tensor from_node from_node from_node graph_id name linear graph_id name addmm graph_id name add mm_default from_node graph_id - name from_node from_node from_node graph_id name linear graph_id name addmm graph_id name mm permute from_node graph_id name linear relu from_node graph_id name relu triton_kernel_to_post_grad_json = triton_poi_fused_addmm_relu_sigmoid_ relu add_tensor result = create_mapping_pre_post_grad_nodes pre_grad_graph_id post_to_pre_grad_nodes_json result = result create_node_mapping_kernel_to_post_grad triton_kernel_to_post_grad_json assertEqual result cppCodeToPost triton_poi_fused_addmm_relu_sigmoid_ relu add_tensor postToCppCode add_tensor triton_poi_fused_addmm_relu_sigmoid_ relu triton_poi_fused_addmm_relu_sigmoid_ postToPre add_tensor linear mm_default linear permute linear relu relu preToPost linear add_tensor mm_default permute relu relu TestProvenanceTracingNodeMeta TestCase get_node_with_target gm target Return first node gm target next iter node node gm graph nodes node target == target requires_cuda_and_triton test only works cuda pattern matcher test_pattern_matcher_transfer_meta Test stack trace transfered when node decomposed post_grad_passes Model torch nn Module __init__ super __init__ fc = torch nn Linear relu = torch nn ReLU sigmoid = torch nn Sigmoid forward x x = fc x x = relu x x = sigmoid x x x = torch randn cuda example_inputs = x model = Model cuda mimic before_post_grad graph ep = torch export export model example_inputs run_decompositions gm = ep module Set fake mode V fake_inputs = node meta get val node gm graph nodes node op == placeholder fake_mode = detect_fake_mode fake_inputs V set_fake_mode fake_mode addmm_node = get_node_with_target gm torch ops aten addmm default stack_trace = addmm_node meta stack_trace post_grad_passes gm True test is_inference doesn t matter mm_node = get_node_with_target gm torch ops aten mm default add_node = get_node_with_target gm torch ops aten add Tensor assertEqual add_node meta stack_trace stack_trace assertEqual mm_node meta stack_trace stack_trace ProvenanceArtifactFilter logging Filter filter record artifact record metadata record metadata artifact name == inductor_provenance_tracking_kernel_stack_traces False StructuredTracePayloadFormatter logging Formatter format record record payload strip TestProvenanceTracingStackTraces TestCase contextlib contextmanager _setup_provenance_capture Helper turn capture inductor_tlparse_runtime structured trace payload_buffer = io StringIO payload_handler = logging StreamHandler payload_buffer payload_handler setLevel logging DEBUG payload_handler setFormatter StructuredTracePayloadFormatter payload_handler addFilter ProvenanceArtifactFilter trace_log addHandler payload_handler try yield payload_buffer finally trace_log removeHandler payload_handler extract_code_line s i=- Extract ith line s split \n i strip torch _inductor config patch trace provenance_tracking_level requires_cuda_and_triton test_tlparse_kernel_stack_traces device = cuda model = Model device x = torch randn device = torch randn device b = torch randn device c = torch randn device example_inputs = x b c expected = triton_poi_fused_addmm_relu_sigmoid_threshold_backward_ x = sigmoid x x = fc x x = relu x triton_poi_fused_mul_ d = triton_poi_fused_addmm_gelu_ z = torch nn functional gelu y y = torch addmm c d b extern_kernels mm x = fc x extern_kernels mm y = torch addmm c d b compiled = torch compile model should produce same provenance there s cache hit _ range reset cache torch _dynamo reset reset_inductor_kernel_provenance_debug_handle _setup_provenance_capture payload_buffer compiled = torch compile model compiled example_inputs payload_content = payload_buffer getvalue strip data = json loads payload_content assertEqual set data keys set expected keys key expected_lines expected items actual_lines = extract_code_line s s data key assertEqual sorted actual_lines sorted expected_lines f Mismatch key key torch _inductor config patch trace provenance_tracking_level max_autotune_gemm_backends ATEN requires_cuda_and_triton test_deferred_triton_kernels foo m inp = m inp foo_c = torch compile mode= max-autotune-no-cudagraphs foo m = torch nn Linear bias=True half cuda inp = torch rand half cuda _setup_provenance_capture payload_buffer torch no_grad _ out_code = run_and_get_code foo_c m inp payload_content = payload_buffer getvalue strip data = json loads payload_content assertTrue = m inp str data Check debug handle output code FileCheck check Topologically Sorted Source Nodes check Provenance debug handles run out_code _check_kernel_information_json kernel_info expected_kernels Validate kernel information JSON structure content assertIsInstance kernel_info dict expected expected_kernels assertIn expected kernel_info f Expected kernel expected found list kernel_info data kernel_info values assertIsInstance data dict field stack_traces post_grad_nodes pre_grad_nodes assertIn field data assertIsInstance data field list item data field assertIsInstance item str requires_cuda_and_triton torch _inductor config patch trace provenance_tracking_level test_kernel_information_generation Test basic kernel information generation AOTI packages model = Model cuda x = torch randn device= cuda = torch randn device= cuda b = torch randn device= cuda c = torch randn device= cuda inputs = x b c tempfile TemporaryDirectory temp_dir ep = torch export export model inputs strict=False pt _file = os path join temp_dir model pt reset_inductor_kernel_provenance_debug_handle torch _inductor aoti_compile_and_package ep package_path=pt _file Extract check kernel_information json exists package zipfile ZipFile pt _file r zip_ref zip_ref extractall temp_dir json_path = os path join temp_dir model data aotinductor model kernel_information json assertTrue os path exists json_path f kernel_information json found extracted package json_path open json_path f kernel_info = json load f expected = triton_poi_fused_addmm_relu_sigmoid_ stack_traces x = sigmoid x x = fc x x = relu x post_grad_nodes sigmoid relu add_tensor_ pre_grad_nodes sigmoid relu linear triton_poi_fused_mul_ stack_traces d = post_grad_nodes mul pre_grad_nodes mul triton_poi_fused_addmm_gelu_ stack_traces z = torch nn functional gelu y y = torch addmm c d b post_grad_nodes mul_ mul_ add_tensor add erf mul_ pre_grad_nodes gelu addmm aoti_torch_cuda_mm_out stack_traces x = fc x post_grad_nodes mm_default_ pre_grad_nodes linear aoti_torch_cuda_mm_out stack_traces y = torch addmm c d b post_grad_nodes mm_default pre_grad_nodes addmm _check_kernel_information_json kernel_info expected keys assertEqual set kernel_info keys set expected keys key data expected items all_lines = join kernel_info key stack_traces s data stack_traces assertTrue s all_lines assertEqual sorted kernel_info key pre_grad_nodes sorted data pre_grad_nodes f Mismatch key key assertEqual sorted kernel_info key post_grad_nodes sorted data post_grad_nodes f Mismatch key key torch _inductor config patch trace provenance_tracking_level test_no_kernel_information_without_provenance_tracking Test kernel_information json generated without provenance tracking SimpleModel torch nn Module forward x x model = SimpleModel x = torch randn Compile AOTI without provenance tracking tempfile TemporaryDirectory temp_dir ep = torch export export model x strict=False pt _file = os path join temp_dir model pt torch _inductor aoti_compile_and_package ep package_path=pt _file Extract check kernel_information json NOT created package extract_dir = os path join temp_dir extracted os makedirs extract_dir exist_ok=True zipfile ZipFile pt _file r zip_ref zip_ref extractall extract_dir expected_json_path = os path join extract_dir kernel_information json assertFalse os path exists expected_json_path kernel_information json should exist package when provenance tracking disabled test_create_kernel_information_json_function Test create_kernel_information_json function directly Test empty state result = create_kernel_information_json assertIsInstance result dict assertEqual len result Should empty no provenance data unittest skipIf IS_MACOS MacOS generates different debug handles torch _inductor config patch trace provenance_tracking_level test_cpu_extern_kernel Foo torch nn Module __init__ - None super __init__ conv = torch nn Conv d forward x conv x model = Foo x = torch randn _setup_provenance_capture payload_buffer reset_inductor_kernel_provenance_debug_handle ep = torch export export model x torch _inductor aoti_compile_and_package ep payload_content = payload_buffer getvalue strip data = json loads payload_content keys = k split k data assertTrue aoti_torch_cpu_convolution keys ProvenanceTracingKernelContextTemplate test_jit_inductor_with_flag Model torch nn Module __init__ super __init__ fc = torch nn Linear relu = torch nn ReLU sigmoid = torch nn Sigmoid forward x b c x = fc x x = relu x x = sigmoid x d = y = torch addmm c d b z = torch nn functional gelu y x z model = Model device x = torch randn device = torch randn device b = torch randn device c = torch randn device example_inputs = x b c config patch cpp enable_kernel_profile True torch compile model example_inputs unittest skipIf sys platform == darwin Different kernel names MacOS test_aoti_python_stack_traces Model torch nn Module __init__ super __init__ fc = torch nn Linear relu = torch nn ReLU sigmoid = torch nn Sigmoid forward x b c x = fc x x = relu x x = sigmoid x d = y = torch addmm c d b z = torch nn functional gelu y x z x = torch randn device = torch randn device b = torch randn device c = torch randn device example_inputs = x b c model = Model device ep = torch export export model example_inputs _ code = run_and_get_cpp_code torch _inductor aoti_compile_and_package ep assertTrue KernelContextGuard code config patch trace provenance_tracking_level cpp enable_kernel_profile True package_path code = run_and_get_cpp_code torch _inductor aoti_compile_and_package ep FileCheck check #include torch csrc inductor aoti_runtime kernel_context_tls h check thread_local KernelContext tls_kernel_context = nullptr run code device == cuda FileCheck check KernelContextGuard _ctx aoti_torch_cuda_mm_out R check AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_cuda_mm_out check KernelContextGuard _ctx triton_poi_fused_addmm_relu_sigmoid_ R check call_triton_poi_fused_addmm_relu_sigmoid_ check KernelContextGuard _ctx triton_poi_fused_mul_ R check call_triton_poi_fused_mul_ check KernelContextGuard _ctx aoti_torch_cuda_mm_out R check AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_cuda_mm_out check KernelContextGuard _ctx triton_poi_fused_addmm_gelu_ R check call_triton_poi_fused_addmm_gelu_ run code FileCheck check KernelContextGuard _ctx aoti_torch_cpu_addmm_out R check AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_cpu_addmm_out check KernelContextGuard _ctx cpp_fused_mul_relu_sigmoid_ R check cpp_fused_mul_relu_sigmoid_ check KernelContextGuard _ctx aoti_torch_cpu_addmm_out R check AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_cpu_addmm_out check KernelContextGuard _ctx cpp_fused_gelu_ R check cpp_fused_gelu_ run code compiled_model = torch _inductor aoti_load_package package_path result = compiled_model example_inputs assertEqual result model example_inputs TestProvenanceTracingKernelContextCpu TestCase device = cpu copy_tests ProvenanceTracingKernelContextTemplate TestProvenanceTracingKernelContextCpu cpu unittest skipIf sys platform == darwin No CUDA MacOS unittest skipIf torch cuda is_available No CUDA TestProvenanceTracingKernelContextGpu TestCase device = cuda copy_tests ProvenanceTracingKernelContextTemplate TestProvenanceTracingKernelContextGpu cuda __name__ == __main__ run_tests