__future__ annotations gc typing collections abc Callable typing Optional overload TYPE_CHECKING TypeAlias Union typing_extensions ParamSpec Self TypeVar torch torch Tensor TYPE_CHECKING importing _POOL_HANDLE runtime toplevel causes cycle torch cuda _POOL_HANDLE _utils _dummy_type __all__ = is_current_stream_capturing graph_pool_handle CUDAGraph graph make_graphed_callables _R = TypeVar _R _P = ParamSpec _P hasattr torch _C _CudaStreamBase Define dummy base classes torch _C __dict__ _CUDAGraph = _dummy_type _CUDAGraph torch _C __dict__ _graph_pool_handle = _dummy_type _graph_pool_handle torch _C __dict__ _cuda_isCurrentStreamCapturing = _dummy_type _cuda_isCurrentStreamCapturing torch _C noqa F _cuda_isCurrentStreamCapturing _CUDAGraph _graph_pool_handle is_current_stream_capturing - bool r Return True CUDA graph capture underway current CUDA stream False otherwise If CUDA context does exist current device returns False without initializing context _cuda_isCurrentStreamCapturing Python shim helps Sphinx process docstrings more reliably graph_pool_handle - _POOL_HANDLE r Return opaque token representing id graph memory pool See ref ` Graph memory management graph-memory-management ` warning This API beta may change future releases torch cuda _POOL_HANDLE _graph_pool_handle Python shim helps Sphinx process docstrings more reliably CUDAGraph torch _C _CUDAGraph r Wrapper around CUDA graph Arguments keep_graph bool optional If ` ` keep_graph=False ` ` cudaGraphExec_t will instantiated GPU end ` ` capture_end ` ` underlying cudaGraph_t will destroyed Users who want query otherwise modify underlying cudaGraph_t before instantiation can set ` ` keep_graph=True ` ` access via ` ` raw_cuda_graph ` ` after ` ` capture_end ` ` Note cudaGraphExec_t will instantiated end ` ` capture_end ` ` case Instead will instantiated via explicit called ` ` instantiate ` ` automatically first call ` ` replay ` ` ` ` instantiate ` ` already called Calling ` ` instantiate ` ` manually before ` ` replay ` ` recommended prevent increased latency first call ` ` replay ` ` It allowed modify raw cudaGraph_t after first calling ` ` instantiate ` ` user must call ` ` instantiate ` ` again manually make sure instantiated graph has these changes Pytorch has no means tracking these changes warning This API beta may change future releases __new__ cls keep_graph bool = False - Self super __new__ cls keep_graph capture_begin pool Optional _POOL_HANDLE = None capture_error_mode str = global - None r Begin capturing CUDA work current stream Typically you shouldn t call ` ` capture_begin ` ` yourself Use ` ~torch cuda graph ` func ` ~torch cuda make_graphed_callables ` which call ` ` capture_begin ` ` internally Arguments pool optional Token returned func ` ~torch cuda graph_pool_handle ` meth ` other_Graph_instance pool torch cuda CUDAGraph pool ` hints graph may share memory indicated pool See ref ` Graph memory management graph-memory-management ` capture_error_mode str optional specifies cudaStreamCaptureMode graph capture stream Can global thread_local relaxed During cuda graph capture some actions such cudaMalloc may unsafe global will error actions other threads thread_local will only error actions current thread relaxed will error these actions Do NOT change setting unless you re familiar ` cudaStreamCaptureMode https docs nvidia com cuda cuda-runtime-api group__CUDART__STREAM html#group__CUDART__STREAM_ g d d cbf b ba ` _ noqa B super capture_begin pool=pool capture_error_mode=capture_error_mode capture_end - None r End CUDA graph capture current stream After ` ` capture_end ` ` ` ` replay ` ` may called instance Typically you shouldn t call ` ` capture_end ` ` yourself Use ` ~torch cuda graph ` func ` ~torch cuda make_graphed_callables ` which call ` ` capture_end ` ` internally super capture_end instantiate - None r Instantiate CUDA graph Will called ` ` capture_end ` ` ` ` keep_graph=False ` ` ` ` replay ` ` ` ` keep_graph=True ` ` ` ` instantiate ` ` has already been explicitly called Does destroy cudaGraph_t returned ` ` raw_cuda_graph ` ` super instantiate replay - None r Replay CUDA work captured graph super replay reset - None r Delete graph currently held instance super reset pool - _POOL_HANDLE r Return opaque token representing id graph s memory pool This id can optionally passed another graph s ` ` capture_begin ` ` which hints other graph may share same memory pool super pool enable_debug_mode - None r Enable debugging mode CUDAGraph debug_dump super enable_debug_mode debug_dump debug_path str - None r Arguments debug_path required Path dump graph Calls debugging function dump graph debugging enabled via CUDAGraph enable_debug_mode super debug_dump debug_path raw_cuda_graph - int r Returns underlying cudaGraph_t ` ` keep_graph ` ` must True See following APIs how manipulate object ` Graph Managmement https docs nvidia com cuda cuda-runtime-api group__CUDART__GRAPH html ` _ ` cuda-python Graph Management bindings https nvidia github io cuda-python cuda-bindings latest module runtime html#graph-management ` _ noqa B super raw_cuda_graph raw_cuda_graph_exec - int r Returns underlying cudaGraphExec_t ` ` instantiate ` ` must have been called ` ` keep_graph ` ` True ` ` capture_end ` ` must have been called ` ` keep_graph ` ` False If you call ` ` instantiate ` ` after ` ` raw_cuda_graph_exec ` ` previously returned cudaGraphExec_t will destroyed It your responsibility use object after destruction See following APIs how manipulate object ` Graph Execution https docs nvidia com cuda cuda-runtime-api group__CUDART__GRAPH__EXEC html ` _ ` cuda-python Graph Execution bindings https nvidia github io cuda-python cuda-bindings latest module runtime html#graph-execution ` _ noqa B super raw_cuda_graph_exec graph r Context-manager captures CUDA work into ` torch cuda CUDAGraph ` object later replay See ref ` CUDA Graphs cuda-graph-semantics ` general introduction detailed use constraints Arguments cuda_graph torch cuda CUDAGraph Graph object used capture pool optional Opaque token returned call func ` ~torch cuda graph_pool_handle ` meth ` other_Graph_instance pool torch cuda CUDAGraph pool ` hinting graph s capture may share memory specified pool See ref ` Graph memory management graph-memory-management ` stream torch cuda Stream optional If supplied will set current stream context If supplied ` ` graph ` ` sets its own internal side stream current stream context capture_error_mode str optional specifies cudaStreamCaptureMode graph capture stream Can global thread_local relaxed During cuda graph capture some actions such cudaMalloc may unsafe global will error actions other threads thread_local will only error actions current thread relaxed will error actions Do NOT change setting unless you re familiar ` cudaStreamCaptureMode https docs nvidia com cuda cuda-runtime-api group__CUDART__STREAM html#group__CUDART__STREAM_ g d d cbf b ba ` _ note For effective memory sharing you pass ` ` pool ` ` used previous capture previous capture used explicit ` ` stream ` ` argument you should pass same ` ` stream ` ` argument capture warning This API beta may change future releases _cudaStreamCaptureMode https docs nvidia com cuda cuda-runtime-api group__CUDART__STREAM html#group__CUDART__STREAM_ g d d cbf b ba noqa B default_capture_stream Optional torch cuda Stream = None __init__ cuda_graph CUDAGraph pool Optional _POOL_HANDLE = None stream Optional torch cuda Stream = None capture_error_mode str = global Lazy-init default_capture_stream helps avoid circular-import errors Not thread safe graphs already have general explicitly documented restriction only one capture may underway time process __class__ default_capture_stream None __class__ default_capture_stream = torch cuda Stream pool Union tuple tuple _POOL_HANDLE = pool None pool capture_stream = stream stream None __class__ default_capture_stream assert capture_stream None stream_ctx = torch cuda stream capture_stream cuda_graph = cuda_graph capture_error_mode = capture_error_mode __enter__ - None Free much memory we can graph torch cuda synchronize torch compiler config force_cudagraph_gc Originally we unconditionally garbage collected here On one hand s nice because we have chance collect more memory other hand REALLY expensive especially doing multiple cudagraph captures row In theory will only help when dead python cycle holding onto CUDA memory gc collect torch cuda empty_cache Stackoverflow seems comfortable pattern https stackoverflow com questions calling-enter-and-exit-manually# stream_ctx __enter__ cuda_graph capture_begin type ignore misc pool pyrefly ignore bad-keyword-argument capture_error_mode=self capture_error_mode __exit__ args object - None cuda_graph capture_end stream_ctx __exit__ args returning None should propagate exceptions either capture_end stream_ctx __exit__ _ModuleOrCallable TypeAlias = Union torch nn Module Callable object overload make_graphed_callables callables _ModuleOrCallable sample_args tuple Tensor num_warmup_iters int = allow_unused_input bool = False pool Optional _POOL_HANDLE = None - _ModuleOrCallable overload make_graphed_callables callables tuple _ModuleOrCallable sample_args tuple tuple Tensor num_warmup_iters int = allow_unused_input bool = False pool Optional _POOL_HANDLE = None - tuple _ModuleOrCallable make_graphed_callables callables Union _ModuleOrCallable tuple _ModuleOrCallable sample_args Union tuple Tensor tuple tuple Tensor num_warmup_iters int = allow_unused_input bool = False pool Optional _POOL_HANDLE = None - Union _ModuleOrCallable tuple _ModuleOrCallable r Accept callables functions ` nn Module torch nn Module ` \ s returns graphed versions Each graphed callable s forward pass runs its source callable s forward CUDA work CUDA graph inside single autograd node The graphed callable s forward pass also appends backward node autograd graph During backward node runs callable s backward work CUDA graph Therefore each graphed callable should drop-in replacement its source callable autograd-enabled training loop See ref ` Partial-network capture partial-network-capture ` detailed use constraints If you pass tuple several callables their captures will use same memory pool See ref ` Graph memory management graph-memory-management ` when appropriate Arguments callables torch nn Module Python function tuple these Callable callables graph See ref ` Graph memory management graph-memory-management ` when passing tuple callables appropriate If you pass tuple callables their order tuple must same order they ll run live workload sample_args tuple Tensors tuple tuples Tensors Samples args each callable If single callable passed ` ` sample_args ` ` must single tuple argument Tensors If tuple callables passed ` ` sample_args ` ` must tuple tuples argument Tensors num_warmup_iters int The number warmup iterations Currently ` ` DataDistributedParallel ` ` needs iterations warm up Default ` ` ` ` allow_unused_input bool If False specifying inputs used when computing outputs therefore their grad always zero error Defaults False pool optional Token returned func ` ~torch cuda graph_pool_handle ` meth ` other_Graph_instance pool torch cuda CUDAGraph pool ` hints graph may share memory indicated pool See ref ` Graph memory management graph-memory-management ` note The ` ` requires_grad ` ` state each Tensor ` ` sample_args ` ` must match state s expected corresponding real input training loop warning This API beta may change future releases warning ` ` sample_args ` ` each callable must contain only Tensors Other types allowed warning Returned callables do support higher order differentiation e g double backward warning In any ` ~torch nn Module ` passed func ` ~make_graphed_callables ` only parameters may trainable Buffers must have ` ` requires_grad=False ` ` warning After you pass ` torch nn Module ` through func ` ~make_graphed_callables ` you may add remove any Module s parameters buffers warning ` torch nn Module ` \s passed func ` ~torch cuda make_graphed_callables ` must have module hooks registered them time they passed However registering hooks modules after passing them through func ` ~torch cuda make_graphed_callables ` allowed warning When running graphed callable you must pass its arguments same order format they appeared callable s ` ` sample_args ` ` warning The automatic mixed precision supported func ` ~torch cuda make_graphed_callables ` only disabled caching The context manager ` torch cuda amp autocast ` must have ` cache_enabled=False ` torch is_autocast_enabled torch is_autocast_cache_enabled raise RuntimeError make_graphed_callables does support autocast caching Please set ` cache_enabled=False ` just_one_callable = False _sample_args tuple tuple Tensor isinstance callables tuple just_one_callable = True callables = callables _sample_args = typing cast tuple Tensor sample_args _sample_args = typing cast tuple tuple Tensor sample_args flatten_sample_args = c args zip callables _sample_args isinstance c torch nn Module assert len c _backward_hooks == len c _forward_hooks == len c _forward_pre_hooks == Modules must have hooks registered time they passed However registering hooks + modules after passing them through make_graphed_callables allowed assert all b requires_grad False b c buffers In any ` ~torch nn Module ` passed + func ` ~make_graphed_callables ` only parameters may trainable All buffers must have + ` ` requires_grad=False ` ` flatten_arg = torch utils _pytree arg_tree_leaves args flatten_sample_args append tuple flatten_arg assert all isinstance arg torch Tensor arg flatten_arg In beta API sample_args + each callable must contain only Tensors Other types allowed If callable nn Module its graph s full input surface args user explicitly passes forward ie its sample_args AND module s parameter attributes per_callable_len_user_args = len args args flatten_sample_args per_callable_module_params = tuple c parameters isinstance c torch nn Module c callables per_callable_static_input_surfaces = flatten_sample_args i + per_callable_module_params i i range len callables fwd_graphs = torch cuda CUDAGraph _ range len callables bwd_graphs = torch cuda CUDAGraph _ range len callables mempool = graph_pool_handle pool None pool Warmup Hopefully prevents cudnn benchmarking other lazy-initialization cuda work ending up any captures torch cuda synchronize torch cuda stream torch cuda Stream func args static_input_surface zip callables _sample_args per_callable_static_input_surfaces grad_inputs outputs outputs_grad = None None None _ range num_warmup_iters outputs = torch utils _pytree tree_leaves func args outputs_grad = tuple o o outputs o requires_grad len outputs_grad grad_inputs = torch autograd grad outputs=outputs_grad inputs=tuple i i static_input_surface i requires_grad grad_outputs=tuple torch empty_like o o outputs o requires_grad only_inputs=True allow_unused=allow_unused_input v outputs outputs_grad grad_inputs del v torch cuda synchronize All captures here share mempool To avoid replays corrupting each other s memory safest approach capture all passes same order they ll run fwd fwd fwd N then bwd N bwd N- bwd Capture forward graphs per_callable_static_outputs = per_callable_output_unflatten_spec = func args fwd_graph zip callables _sample_args fwd_graphs torch cuda graph fwd_graph pool=mempool func_outputs = func args flatten_outputs spec = torch utils _pytree tree_flatten func_outputs per_callable_static_outputs append tuple flatten_outputs per_callable_output_unflatten_spec append spec Capture backward graphs reverse order per_callable_static_grad_outputs = per_callable_static_grad_inputs = static_input_surface static_outputs bwd_graph zip reversed per_callable_static_input_surfaces reversed per_callable_static_outputs reversed bwd_graphs For now assumes all static_outputs require grad assert all o requires_grad o static_outputs Outputs graphed callables must require grad static_grad_outputs = tuple torch empty_like o o requires_grad None o static_outputs outputs_grad = tuple o o static_outputs o requires_grad grad_inputs = None len outputs_grad torch cuda graph bwd_graph pool=mempool grad_inputs = torch autograd grad outputs=outputs_grad inputs=tuple i i static_input_surface i requires_grad grad_outputs=tuple o o static_grad_outputs o None only_inputs=True allow_unused=allow_unused_input Constructs tuple suitable returning Graphed backward Pads out actually-needed grads Nones gradient slots inputs don t require grad I couldn t think slick one-liner pattern static_grad_inputs = grad_idx = arg static_input_surface arg requires_grad grad_inputs None static_grad_inputs append grad_inputs grad_idx grad_idx += static_grad_inputs append None type ignore arg-type static_grad_inputs = tuple static_grad_inputs type ignore assignment per_callable_static_grad_outputs append static_grad_outputs per_callable_static_grad_inputs append static_grad_inputs Reverses most recent two lists per_callable_static_grad_outputs reverse per_callable_static_grad_inputs reverse Now every per_callable list per_callable_ i holds stuff ith callable make_graphed_autograd_function fwd_graph CUDAGraph bwd_graph CUDAGraph module_params tuple torch nn Parameter len_user_args int output_unflatten_spec torch utils _pytree TreeSpec static_input_surface tuple Tensor static_outputs tuple Tensor static_grad_outputs tuple Optional Tensor static_grad_inputs tuple Tensor - Callable object Graphed torch autograd Function staticmethod pyrefly ignore bad-override forward ctx object inputs Tensor - tuple Tensor At stage only user args may potentially new tensors i range len_user_args static_input_surface i data_ptr = inputs i data_ptr static_input_surface i copy_ inputs i fwd_graph replay assert isinstance static_outputs tuple tuple o detach o static_outputs staticmethod torch autograd function once_differentiable pyrefly ignore bad-override backward ctx object grads Tensor - tuple Tensor assert len grads == len static_grad_outputs g grad zip static_grad_outputs grads g None don t copy autograd gods have been kind incoming grad already right place g data_ptr = grad data_ptr g copy_ grad bwd_graph replay Input args didn t require grad expect None gradient assert isinstance static_grad_inputs tuple tuple pyrefly ignore bad-argument-type b detach b None b b static_grad_inputs functionalized user_args object - object Runs autograd function inputs == all inputs graph might require grad explicit user args + module parameters Assumes module params didn t change since capture flatten_user_args = torch utils _pytree arg_tree_leaves user_args out = Graphed apply tuple flatten_user_args + module_params torch utils _pytree tree_unflatten out output_unflatten_spec functionalized Put together final graphed callables ret list _ModuleOrCallable = i func enumerate callables graphed = make_graphed_autograd_function fwd_graphs i bwd_graphs i per_callable_module_params i per_callable_len_user_args i per_callable_output_unflatten_spec i per_callable_static_input_surfaces i per_callable_static_outputs i per_callable_static_grad_outputs i per_callable_static_grad_inputs i isinstance func torch nn Module make_graphed_forward func torch nn Module graph_training_state bool graphed Callable _P _R orig_fwd Callable _P _R - Callable _P _R new_fwd user_args _P args user_kwargs _P kwargs - _R If module s training-or-eval state matches what we graphed run graph otherwise run original forward method func training == graph_training_state graphed user_args user_kwargs orig_fwd user_args user_kwargs new_fwd func forward = make_graphed_forward func func training graphed func forward ret append func ret append graphed just_one_callable ret tuple ret