Owner s module cpp-extensions os re unittest itertools repeat typing get_args get_origin Union torch torch backends cudnn torch testing _internal common_utils common torch utils cpp_extension torch testing _internal common_cuda TEST_CUDA torch testing _internal common_utils IS_WINDOWS skipIfTorchDynamo TEST_XPU xfailIfTorchDynamo try pytest HAS_PYTEST = True except ImportError HAS_PYTEST = False TODO Rewrite these tests so they can collected via pytest without using run_test py try HAS_PYTEST cpp_extension = pytest importorskip torch_test_cpp_extension cpp maia_extension = pytest importorskip torch_test_cpp_extension maia rng_extension = pytest importorskip torch_test_cpp_extension rng torch_test_cpp_extension cpp cpp_extension torch_test_cpp_extension maia maia_extension torch_test_cpp_extension rng rng_extension except ImportError e raise RuntimeError test_cpp_extensions_aot py cannot invoked directly Run ` python run_test py -i test_cpp_extensions_aot_ninja ` instead e torch testing _internal common_utils markDynamoStrictTest TestCppExtensionAOT common TestCase Tests ahead-of-time cpp extensions NOTE run_test py s test_cpp_extensions_aot_ninja target also runs test case ninja enabled If you debugging test failure here CI check logs which target test_cpp_extensions_aot_no_ninja vs test_cpp_extensions_aot_ninja failed test_extension_function x = torch randn y = torch randn z = cpp_extension sigmoid_add x y assertEqual z x sigmoid + y sigmoid test pybind support torch dtype cast assertEqual str torch float str cpp_extension get_math_type torch half test_extension_module mm = cpp_extension MatrixMultiplier weights = torch rand dtype=torch double expected = mm get mm weights result = mm forward weights assertEqual expected result test_backward mm = cpp_extension MatrixMultiplier weights = torch rand dtype=torch double requires_grad=True result = mm forward weights result sum backward tensor = mm get expected_weights_grad = tensor t mm torch ones dtype=torch double assertEqual weights grad expected_weights_grad expected_tensor_grad = torch ones dtype=torch double mm weights t assertEqual tensor grad expected_tensor_grad unittest skipIf TEST_CUDA CUDA found test_cuda_extension torch_test_cpp_extension cuda cuda_extension x = torch zeros device= cuda dtype=torch float y = torch zeros device= cuda dtype=torch float z = cuda_extension sigmoid_add x y cpu sigmoid = = assertEqual z torch ones_like z unittest skipIf torch backends mps is_available MPS found test_mps_extension torch_test_cpp_extension mps mps_extension tensor_length = x = torch randn tensor_length device= cpu dtype=torch float y = torch randn tensor_length device= cpu dtype=torch float cpu_output = mps_extension get_cpu_add_output x y mps_output = mps_extension get_mps_add_output x mps y mps assertEqual cpu_output mps_output cpu unittest skipIf TEST_XPU XPU found unittest skipIf os getenv USE_NINJA == sycl extension requires ninja build test_sycl_extension torch_test_cpp_extension sycl sycl_extension x = torch zeros device= xpu dtype=torch float y = torch zeros device= xpu dtype=torch float z = sycl_extension sigmoid_add x y cpu sigmoid = = assertEqual z torch ones_like z common skipIfRocm unittest skipIf common IS_WINDOWS Windows supported unittest skipIf TEST_CUDA CUDA found test_cublas_extension torch_test_cpp_extension cublas_extension x = torch zeros device= cuda dtype=torch float z = cublas_extension noop_cublas_function x assertEqual z x common skipIfRocm unittest skipIf common IS_WINDOWS Windows supported unittest skipIf TEST_CUDA CUDA found test_cusolver_extension torch_test_cpp_extension cusolver_extension x = torch zeros device= cuda dtype=torch float z = cusolver_extension noop_cusolver_function x assertEqual z x unittest skipIf IS_WINDOWS Not available Windows test_no_python_abi_suffix_sets_the_correct_library_name For test run_test py will call ` python -m pip install -v -- no-build-isolation ` cpp_extensions no_python_abi_suffix_test folder where ` BuildExtension ` has ` no_python_abi_suffix ` option set ` True ` This should mean Python produced shared library does have ABI suffix like cpython- m-x _ -linux-gnu before library suffix e g so root = os path join cpp_extensions no_python_abi_suffix_test build matches = f _ _ fs os walk root f fs f endswith so assertEqual len matches msg=str matches assertEqual matches no_python_abi_suffix_test so msg=str matches test_optional has_value = cpp_extension function_taking_optional torch ones assertTrue has_value has_value = cpp_extension function_taking_optional None assertFalse has_value common skipIfRocm unittest skipIf common IS_WINDOWS Windows supported unittest skipIf TEST_CUDA CUDA found unittest skipIf os getenv USE_NINJA == cuda extension dlink requires ninja build test_cuda_dlink_libs torch_test_cpp_extension cuda_dlink = torch randn dtype=torch float device= cuda b = torch randn dtype=torch float device= cuda ref = + b test = cuda_dlink add b assertEqual test ref torch testing _internal common_utils markDynamoStrictTest TestPybindTypeCasters common TestCase Pybind tests ahead-of-time cpp extensions These tests verify types returned cpp code using custom type casters By exercising pybind we also verify type casters work properly For each type caster ` torch csrc utils pybind h ` we create pybind function takes no arguments returns type_caster type The second argument ` PYBIND _TYPE_CASTER ` should type we expect receive python these tests we verify run-time staticmethod expected_return_type func Our Pybind functions have signature form ` - return_type ` Imports needed ` eval ` below typing List Tuple noqa F UP eval re search - \n func __doc__ group check func val = func expected = expected_return_type func origin = get_origin expected origin list check_list val expected origin tuple check_tuple val expected assertIsInstance val expected check_list vals expected assertIsInstance vals list list_type = get_args expected val vals assertIsInstance val list_type check_tuple vals expected assertIsInstance vals tuple tuple_types = get_args expected tuple_types tuple_types = repeat tuple_types val tuple_type zip vals tuple_types assertIsInstance val tuple_type check_union funcs Special handling Union type casters A single cpp type can sometimes cast different types python In these cases we expect get exactly one function per python type Verify all functions have same type union_type = expected_return_type f f funcs assert len union_type == union_type = union_type pop assertIs Union get_origin union_type SymInt inconvenient test so don t require expected_types = set get_args union_type - torch SymInt func funcs val = func tp expected_types isinstance val tp expected_types remove tp break raise AssertionError f val instance expected_types assertFalse expected_types f Missing functions types expected_types test_pybind_return_types functions = cpp_extension get_complex cpp_extension get_device cpp_extension get_generator cpp_extension get_intarrayref cpp_extension get_memory_format cpp_extension get_storage cpp_extension get_symfloat cpp_extension get_symintarrayref cpp_extension get_tensor union_functions = cpp_extension get_symint func functions subTest msg=f check func __name__ check func funcs union_functions subTest msg=f check f __name__ f funcs check_union funcs torch testing _internal common_utils markDynamoStrictTest TestMAIATensor common TestCase test_unregistered torch arange device= cpu assertRaisesRegex RuntimeError Could run torch arange device= maia skipIfTorchDynamo dynamo cannot model maia device test_zeros = torch empty device= cpu assertEqual device torch device cpu b = torch empty device= maia assertEqual b device torch device maia assertEqual maia_extension get_test_int assertEqual torch get_default_dtype b dtype c = torch empty dtype=torch int device= maia assertEqual maia_extension get_test_int assertEqual torch int c dtype test_add = torch empty device= maia requires_grad=True assertEqual maia_extension get_test_int b = torch empty device= maia assertEqual maia_extension get_test_int + b assertEqual maia_extension get_test_int test_conv_backend_override To simplify tests we use d input here avoid doing view d which needs more overrides _convolution input = torch empty device= maia requires_grad=True weight = torch empty device= maia requires_grad=True bias = torch empty device= maia Make sure forward overridden out = torch nn functional conv d input weight bias assertEqual maia_extension get_test_int assertEqual out shape input shape assertEqual out shape weight shape Make sure backward overridden Double backward dispatched _convolution_double_backward It tested here involves more computation overrides grad = torch autograd grad out input out create_graph=True assertEqual maia_extension get_test_int assertEqual grad shape input shape test_autocast_apis_for_maia_device Default low-precision type MAIA s autocast fast_dtype = torch get_autocast_dtype maia assertEqual fast_dtype torch bfloat assertTrue torch _C _is_autocast_available maia skipIfTorchDynamo dynamo cannot handle maia device Output tensor may have wrong dtype test_matmul_autocast_float _precision Ensure we can change low precision dtype x = torch empty dtype=torch float device= maia w = torch empty dtype=torch float device= maia torch autocast device_type= maia dtype=torch float assertTrue torch is_autocast_enabled maia y = torch ops aten matmul x w assertEqual y dtype torch float assertEqual y shape skipIfTorchDynamo dynamo cannot handle maia device Output tensor may have wrong dtype test_matmul_autocast_default_precision Use default lower precision dtype bfloat x = torch empty dtype=torch float device= maia w = torch empty dtype=torch float device= maia torch autocast device_type= maia assertTrue torch is_autocast_enabled maia y = torch ops aten matmul x w assertEqual y dtype torch bfloat assertEqual y shape torch testing _internal common_utils markDynamoStrictTest TestRNGExtension common TestCase setUp super setUp xfailIfTorchDynamo test_rng fourty_two = torch full dtype=torch int t = torch empty dtype=torch int random_ assertNotEqual t fourty_two gen = torch Generator device= cpu t = torch empty dtype=torch int random_ generator=gen assertNotEqual t fourty_two assertEqual rng_extension getInstanceCount gen = rng_extension createTestCPUGenerator assertEqual rng_extension getInstanceCount copy = gen assertEqual rng_extension getInstanceCount assertEqual gen copy copy = rng_extension identity copy assertEqual rng_extension getInstanceCount assertEqual gen copy t = torch empty dtype=torch int random_ generator=gen assertEqual rng_extension getInstanceCount assertEqual t fourty_two del gen assertEqual rng_extension getInstanceCount del copy assertEqual rng_extension getInstanceCount del copy assertEqual rng_extension getInstanceCount torch testing _internal common_utils markDynamoStrictTest unittest skipIf TEST_CUDA CUDA found TestTorchLibrary common TestCase test_torch_library torch_test_cpp_extension torch_library noqa F f bool b bool torch ops torch_library logical_and b assertTrue f True True assertFalse f True False assertFalse f False True assertFalse f False False s = torch jit script f assertTrue s True True assertFalse s True False assertFalse s False True assertFalse s False False assertIn torch_library logical_and str s graph __name__ == __main__ common run_tests