mypy allow-untyped-defs operator traceback typing collections abc Callable contextlib nullcontext typing Any Optional Union torch torch fx torch _dispatch python enable_python_dispatcher torch _export pass_infra node_metadata NodeMetadata torch _export pass_infra proxy_value ProxyValue torch _higher_order_ops map _unstack_pytree torch _subclasses FakeTensor UnsupportedFakeTensorException torch _subclasses fake_tensor FakeTensorMode torch fx traceback fx_traceback torch fx experimental proxy_tensor PythonKeyTracer torch fx experimental symbolic_shapes compute_unbacked_bindings PropagateUnbackedSymInts torch fx graph CodeGen torch fx passes infra pass_base PassBase PassResult torch fx passes shape_prop _extract_tensor_metadata TensorMetadata torch utils _pytree pytree __all__ = _ExportPassBaseDeprecatedDoNotUse Argument = Any Value = Any Fn = Callable Any PassType = Callable torch fx GraphModule Optional PassResult _TORCH_SYM_OPS set Callable = torch sym_int torch sym_float torch sym_ite torch sym_max torch sym_min torch sym_not torch sym_sqrt ExportPassBaseError RuntimeError pass _ExportPassBaseDeprecatedDoNotUse PassBase Interpreter-based pass help users maintain IR spec while writing transformations staticmethod _create_dummy_node_metadata NodeMetadata stack_trace join traceback format_stack limit= ExportTracer PythonKeyTracer __init__ callback _ExportPassBaseDeprecatedDoNotUse codegen CodeGen - None super __init__ callback = callback root = torch nn Module graph = torch fx Graph graph set_codegen codegen tensor_attrs dict str torch Tensor = type ignore assignment fake_tensor_mode Optional FakeTensorMode = None submodules dict torch nn Module str = trace - None type ignore override raise ExportPassBaseError ExportTracer doesn t support trace create_arg Argument - torch fx Node isinstance torch nn Module submodules name_submodule = f submodule_ len submodules root add_module name_submodule submodules = name_submodule isinstance FakeTensor hasattr constant constant None raise ExportPassBaseError f Cannot add graph = constant node = super create_arg isinstance torch Tensor isinstance node torch fx Node node op == get_attr set_metadata node callback on_attr ProxyValue node node set_metadata node torch fx Node value Argument - None propagate fake tensor sym nodes make_val x Argument - Union FakeTensor torch SymInt torch SymFloat torch SymBool int float bool str None isinstance x FakeTensor x isinstance x torch Tensor x is_quantized TODO tmanlaibaatar properly support Quantized FakeTensor x = torch dequantize x try assert fake_tensor_mode None TODO we should allocate static shapes param buffer values isinstance x torch nn Parameter fake_tensor = fake_tensor_mode from_tensor x static_shapes=True fake_tensor = fake_tensor_mode from_tensor x except UnsupportedFakeTensorException TODO This just workaround get over x as_subclass error print Fakeifying Tensor subclass supported \ right now Instead TensorMetadata used fake_tensor = None fake_tensor isinstance x torch SymInt torch SymFloat torch SymBool int float bool str x None node meta val = pytree tree_map make_val value Set tensor_metadata values do have corresponding FakeTensor make_tensor_meta x Argument - Optional TensorMetadata isinstance x FakeTensor isinstance x torch Tensor x is_quantized TODO tmanlaibaatar properly support Quantized FakeTensor x = torch dequantize x try assert fake_tensor_mode None _ = fake_tensor_mode from_tensor x tensor_meta = None except UnsupportedFakeTensorException TODO This just workaround get over x as_subclass error tensor_meta = _extract_tensor_metadata x tensor_meta None node meta tensor_meta = pytree tree_map make_tensor_meta value ExportInterpreter fx Interpreter __init__ callback _ExportPassBaseDeprecatedDoNotUse gm fx GraphModule - None super __init__ gm callback = callback node torch fx Node = next iter gm graph nodes pyrefly ignore bad-override placeholder target str type ignore override args tuple Argument kwargs dict str Argument - ProxyValue arg = super placeholder target args kwargs callback placeholder target arg NodeMetadata node meta output target torch fx node Target args tuple Argument kwargs dict str Argument - ProxyValue callback output args NodeMetadata node meta data type ignore return-value call_function target torch fx node Target args tuple Argument kwargs dict str Argument - ProxyValue meta = NodeMetadata node meta target operator getitem value key = args callback call_getitem value key meta getattr target __module__ None _operator builtins math assert callable target callback call_sym target args meta target _TORCH_SYM_OPS assert callable target callback call_sym target args meta isinstance target torch _ops OpOverload torch _ops OpOverloadPacket callback call_operator target args kwargs meta target torch ops higher_order cond pred true_fn false_fn inputs = args callback call_cond pred true_fn false_fn inputs meta target torch ops higher_order map_impl f mapped_args operands = args type ignore assignment callback call_map f mapped_args operands meta For other unregistered HigherOrderOps just interpret them blindly isinstance target torch _ops HigherOrderOperator callback _fx call_function target args kwargs meta raise ExportPassBaseError f Unsupported target type target get_attr type ignore override target str args tuple Argument kwargs dict str Argument - Argument super get_attr target args kwargs call_module target torch fx node Target args tuple Argument kwargs dict str Argument - None raise ExportPassBaseError call_module supported call_method type ignore override target str args tuple Argument kwargs dict str Argument - None raise ExportPassBaseError call_method supported run_node n torch fx Node - Argument node = n callback node_debug_str = n format_node super run_node n __init__ - None interpreter = PropagateUnbackedSymInts torch fx GraphModule torch nn Module torch fx Graph tracer = ExportTracer CodeGen fake_tensor_mode Optional FakeTensorMode = None _initialized = True node_debug_str typing Optional str = None _fx kind str target torch fx node Target args tuple Argument kwargs dict str Argument meta NodeMetadata - ProxyValue args_data kwargs_data = pytree tree_map_only ProxyValue lambda x x data args kwargs res_data = getattr interpreter kind target args_data kwargs_data args_proxy kwargs_proxy = pytree tree_map_only ProxyValue lambda x x proxy args kwargs name = None isinstance target torch _ops OpOverload name = tracer graph _target_to_str target overloadpacket __name__ res_proxy = tracer create_proxy kind target args_proxy kwargs_proxy name=name res_proxy node meta update meta data fake_tensor_mode shape_env = fake_tensor_mode shape_env symbol_to_path = compute_unbacked_bindings shape_env res_data res_proxy node meta unbacked_bindings = symbol_to_path tracer set_metadata res_proxy node res_data ProxyValue res_data res_proxy inputs graph_module torch fx GraphModule - list Argument TODO angelayi Update what we decide do metadata exported graph module args = graph_module meta get args None None list args extract_input node torch fx Node - Optional FakeTensor val node meta fake = node meta val hasattr fake constant fake constant None fake constant fake tensor_meta = node meta get tensor_meta assert fake_tensor_mode None FakeTensor fake_tensor_mode torch empty tensor_meta shape dtype=tensor_meta dtype device= meta requires_grad=tensor_meta requires_grad memory_format=tensor_meta memory_format torch device cpu len node users == None raise ExportPassBaseError f Cannot construct input graph module graph_module extract_input node node graph_module graph nodes node op == placeholder on_attr attr ProxyValue - None pass placeholder name str arg Argument meta NodeMetadata - ProxyValue arg_proxy = tracer create_proxy placeholder name arg_proxy node meta = meta data tracer set_metadata arg_proxy node arg ProxyValue arg arg_proxy call_operator op args tuple Argument kwargs dict str Argument meta NodeMetadata - ProxyValue _fx call_function op args kwargs meta call_sym target Fn args tuple Argument meta NodeMetadata - ProxyValue _fx call_function target args meta call_cond pred ProxyValue true_fn torch fx GraphModule false_fn torch fx GraphModule inputs list Argument meta NodeMetadata - ProxyValue true_branch = call_submodule true_fn tuple inputs false_branch = call_submodule false_fn tuple inputs assert true_branch None assert false_branch None _fx call_function torch ops higher_order cond pred true_branch graph_module false_branch graph_module list inputs meta call_map f torch fx GraphModule mapped_args list ProxyValue operands list ProxyValue meta NodeMetadata - ProxyValue xs = _unstack_pytree arg data arg mapped_args f_branch = call_submodule f tuple xs + arg data arg operands assert f_branch None _fx call_function torch ops higher_order map_impl f_branch graph_module mapped_args operands meta call_getitem value ProxyValue key int meta NodeMetadata - ProxyValue _fx call_function operator getitem value key meta output results list Argument meta NodeMetadata - ProxyValue _fx output output results meta call_submodule graph_module fx GraphModule inputs tuple Argument - PassResult prev_tracer tracer = tracer ExportTracer graph_module graph _codegen tracer fake_tensor_mode = prev_tracer fake_tensor_mode interpreter = ExportInterpreter graph_module pyrefly ignore bad-assignment prev_interpreter interpreter = interpreter torch fx Interpreter type ignore assignment torch fx GraphModule torch nn Module torch fx Graph inputs_data = pytree tree_map_only ProxyValue lambda x x data inputs fx_traceback preserve_node_meta interpreter run inputs_data new_graph_module = torch fx GraphModule tracer root tracer graph tracer = prev_tracer interpreter = prev_interpreter PassResult new_graph_module True call graph_module fx GraphModule - PassResult getattr _initialized False raise ExportPassBaseError ExportPass initialized __init__ inputs = inputs graph_module fake_tensor_mode = None i inputs isinstance i FakeTensor assert fake_tensor_mode None fake_tensor_mode i fake_mode Multiple fake tensor mode detected fake_tensor_mode = i fake_mode fake_tensor_mode None tracer fake_tensor_mode = FakeTensorMode allow_non_fake_inputs=True fake_tensor_mode = nullcontext type ignore assignment dispatcher_mode = nullcontext type ignore assignment fake_tensor_mode allow_non_fake_inputs = True tracer fake_tensor_mode = fake_tensor_mode dispatcher_mode = enable_python_dispatcher type ignore assignment fake_tensor_mode = tracer fake_tensor_mode fake_tensor_mode dispatcher_mode type ignore assignment union-attr result = call_submodule graph_module tuple inputs result