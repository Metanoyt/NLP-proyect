mypy allow-untyped-defs Copyright c Meta Platforms Inc affiliates implement matrix related ops distributed tensor typing cast torch torch distributed dist torch distributed tensor _api dtensor aten = torch ops aten _requires_data_exchange padding TODO whether there requires data exchange currently determined padding padding - = _is_supported input_size kernel_size stride padding dilation dilation - = raise RuntimeError Dilation must tensor parallel convolution padding - = stride - = raise RuntimeError Stride must when there padding tensor parallel convolution kernel_size - input_size - raise RuntimeError kernel_size - should less than equal input_size - tensor parallel convolution input_size - stride - == stride - == kernel_size - raise RuntimeError It requires input_size - divisible stride - stride - equals kernel_size - when there padding tensor parallel convolution True _ring_send_recv_construct in_tensor d d left right rank size dist comms reconstruct local input tensor send_to_right = in_tensor -d contiguous send_to_left = in_tensor d contiguous recv_from_right = torch zeros_like send_to_left recv_from_left = torch zeros_like send_to_right send_op_right = dist P POp dist isend send_to_right right send_op_left = dist P POp dist isend send_to_left left recv_op_right = dist P POp dist irecv recv_from_right right recv_op_left = dist P POp dist irecv recv_from_left left reqs = dist batch_isend_irecv send_op_right send_op_left recv_op_left recv_op_right req reqs req wait rank == in_tensor = torch cat in_tensor recv_from_right dim=- rank == size - in_tensor = torch cat recv_from_left in_tensor dim=- in_tensor = torch cat recv_from_left in_tensor recv_from_right dim=- in_tensor _ring_send_recv_aggregate grad_in_tensor d d left right rank size dist comms aggregate gradients edge pixels send_to_right = grad_in_tensor -d contiguous send_to_left = grad_in_tensor d contiguous recv_from_right = torch zeros_like send_to_left recv_from_left = torch zeros_like send_to_right send_op_right = dist P POp dist isend send_to_right right send_op_left = dist P POp dist isend send_to_left left recv_op_right = dist P POp dist irecv recv_from_right right recv_op_left = dist P POp dist irecv recv_from_left left reqs = dist batch_isend_irecv send_op_right send_op_left recv_op_left recv_op_right req reqs req wait rank == grad_in_tensor = grad_in_tensor -d grad_in_tensor -d = torch add grad_in_tensor -d recv_from_right rank == size - grad_in_tensor = grad_in_tensor d grad_in_tensor d = torch add grad_in_tensor d recv_from_left grad_in_tensor = grad_in_tensor d -d grad_in_tensor -d = torch add grad_in_tensor -d recv_from_right grad_in_tensor d = torch add grad_in_tensor d recv_from_left tp_convolution op_call torch _ops OpOverload local_tensor_args tuple object local_tensor_kwargs dict str object - object assert op_call == aten convolution default assert len local_tensor_args == rank = dist get_rank size = dist get_world_size in_tensor = cast torch Tensor local_tensor_args weight = cast torch Tensor local_tensor_args stride padding dilation = local_tensor_args assert _is_supported in_tensor shape weight shape stride padding dilation assert isinstance padding list _requires_data_exchange padding local_results = op_call local_tensor_args local_tensor_kwargs local_results step compute overlap pixels input tensor d = weight shape - - d = d d = d - d assert d + d == d right = rank + size left = rank - + size size step reconstruct local input tensor in_tensor = _ring_send_recv_construct in_tensor d d left right rank size step feed local input tensor op_call local_tensor_args_list = list local_tensor_args local_tensor_args_list = in_tensor local_tensor_args = cast tuple object local_tensor_args_list local_results = op_call local_tensor_args local_tensor_kwargs step remove extra outputs results padding_w = padding - w = local_results size - rank == local_results = local_results w - padding_w rank == size - local_results = local_results padding_w local_results = local_results padding_w w - padding_w local_results tp_convolution_backward op_call torch _ops OpOverload local_tensor_args tuple object local_tensor_kwargs dict str object - object assert op_call == aten convolution_backward default assert len local_tensor_args == rank = dist get_rank size = dist get_world_size grad_out_tensor = cast torch Tensor local_tensor_args in_tensor = cast torch Tensor local_tensor_args weight = cast torch Tensor local_tensor_args stride padding dilation = local_tensor_args assert _is_supported in_tensor shape weight shape stride padding dilation assert isinstance padding list _requires_data_exchange padding local_results = op_call local_tensor_args local_tensor_kwargs local_results step compute overlap pixels input tensor d = weight shape - d = d d = d - d assert d + d == d right = rank + size left = rank - + size size step reconstruct local input tensor in_tensor = _ring_send_recv_construct in_tensor d d left right rank size step reconstruct local gradient output tensor padding_w = padding rank == grad_out_tensor = torch nn functional pad grad_out_tensor padding_w constant rank == size - grad_out_tensor = torch nn functional pad grad_out_tensor padding_w constant grad_out_tensor = torch nn functional pad grad_out_tensor padding_w padding_w constant step feed local input tensor op_call local_tensor_args_list = list local_tensor_args local_tensor_args_list = grad_out_tensor local_tensor_args_list = in_tensor local_tensor_args = cast tuple object local_tensor_args_list local_results = op_call local_tensor_args local_tensor_kwargs step aggregate gradients edge pixels grad_in_tensor = local_results grad_in_tensor None grad_in_tensor = _ring_send_recv_aggregate grad_in_tensor d d left right rank size local_results = list local_results local_results = grad_in_tensor local_results = cast tuple object local_results local_results convolution_handler op_call torch _ops OpOverload args tuple object kwargs dict str object - object extract local tensor sharding infos OpInfo op_info = dtensor DTensor _op_dispatcher unwrap_to_op_info op_call args kwargs sharding propagation dtensor DTensor _op_dispatcher sharding_propagator propagate op_info output_sharding = op_info output_sharding assert output_sharding None output sharding should None local propagation local_results = tp_convolution op_call tuple op_info local_args op_info local_kwargs dtensor DTensor _op_dispatcher wrap local_results output_sharding output_spec convolution_backward_handler op_call torch _ops OpOverload args tuple object kwargs dict str object - object Redistribute grad_output tensor same placement input tensor pyrefly ignore bad-assignment args = list args assert isinstance args dtensor DTensor isinstance args dtensor DTensor pyrefly ignore unsupported-operation args = args redistribute args device_mesh args placements args = tuple args extract local tensor sharding infos OpInfo op_info = dtensor DTensor _op_dispatcher unwrap_to_op_info op_call args kwargs sharding propagation dtensor DTensor _op_dispatcher sharding_propagator propagate op_info output_sharding = op_info output_sharding assert output_sharding None output sharding should None local propagation local_results = tp_convolution_backward op_call tuple op_info local_args op_info local_kwargs dtensor DTensor _op_dispatcher wrap local_results output_sharding output_spec