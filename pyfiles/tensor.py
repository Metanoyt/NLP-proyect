mypy ignore-errors This module contains variable tracker classes handling tensors tensor-related operations Dynamo The main TensorVariable which represents torch Tensor inputs intermediate values FX graph It handles tensor operations method calls maintains metadata about tensor properties like dtype device etc Other key classes include - SymNodeVariable Represents symbolic scalars int float bool used size computation unspecialized values - NumpyNdarrayVariable Handles numpy array interop through torch _numpy - UnspecializedPythonVariable Represents unspecialized Python numeric values -element tensors - TensorSubclassVariable Handles tensor subclasses __torch_function__ overrides - UntypedStorageVariable Represents tensor storage objects - DataPtrVariable Handles tensor data pointer operations These classes work together track tensor operations properties during Dynamo s tracing process functools logging operator textwrap traceback types contextlib nullcontext typing TYPE_CHECKING sympy torch _numpy tnp torch fx torch random torch _dynamo compiled_autograd torch _subclasses meta_utils is_sparse_any torch fx experimental symbolic_shapes guard_scalar GuardOnDataDependentSymNode has_free_symbols is_symbolic SymTypes torch utils _python_dispatch is_traceable_wrapper_subclass config graph_break_hints variables _trace_wrapped_higher_order_op trace_wrapped exc unimplemented_v UnknownPropertiesDuringBackwardTrace UserError UserErrorType external_utils call_hook_from_backward_state guards GuardBuilder install_guard source AttrSource utils fqn get_custom_getattr get_fake_value get_real_value guard_if_dyn object_has_getattribute product proxy_args_kwargs raise_args_mismatch set_example_value tensortype_to_dtype base AttributeMutationNew ValueMutationNew VariableTracker constant ConstantVariable lists ListIteratorVariable SizeVariable user_defined UserDefinedClassVariable try numpy np except ModuleNotFoundError np = None TYPE_CHECKING torch _dynamo codegen PyCodegen torch _dynamo symbolic_convert InstructionTranslator log = logging getLogger __name__ Ops allow tensor op tensor supported_tensor_comparison_ops = operator gt operator lt = operator ge = operator le == operator eq = operator ne operator is_ operator is_not Ops allow tensor op None supported_const_comparison_ops = operator is_ operator is_not == operator eq = operator ne supported_comparison_ops = supported_tensor_comparison_ops supported_const_comparison_ops supported_tensor_comparison_op_values = dict fromkeys supported_tensor_comparison_ops values supported_const_comparison_op_values = dict fromkeys supported_const_comparison_ops values is_bound_tensor_method value callable value torch _dynamo utils object_has_getattribute value hasattr value __self__ isinstance value __self__ torch Tensor getattr value __self__ value __name__ None instead using inspect getattr_static we directly lookup appropriate dicts It necessary keep torch _C TensorBase first operation because second arg takes priority operation when there common keys all_tensor_attrs = torch _C TensorBase __dict__ &#124; torch Tensor __dict__ TensorVariable VariableTracker A torch Tensor input intermediate value FX graph _nonvar_fields = proxy dtype device layout ndim size stride requires_grad is_quantized is_contiguous is_nested is_sparse class_type specialized_value _is_name_set VariableTracker _nonvar_fields get_real_value Get actual value represented variable computation run using user-provided inputs NOTE runs actual tensor computation may slow memory-intensive get_real_value proxy node proxy tracer __init__ proxy torch fx Proxy dtype device layout ndim requires_grad is_nested is_quantized is_sparse class_type has_grad_fn _size=None stride=None is_contiguous=None _is_name_set=None kwargs - None super __init__ kwargs proxy = proxy dtype = dtype device = device layout = layout ndim = ndim _size = _size accessed property validation stride = stride requires_grad = requires_grad is_quantized = is_quantized is_contiguous = is_contiguous is_nested = is_nested is_sparse = is_sparse class_type = class_type has_grad_fn = has_grad_fn _is_name_set None no need rename inputs _is_name_set = proxy node op == placeholder _is_name_set bool = _is_name_set synchronize_attributes tx target_cls=None builder get_specialized_props infer_subclass_type target_cls None target_cls = type example_value = proxy node meta get example_value specialized_props = get_specialized_props target_cls tx example_value infer_subclass_type example_value k v specialized_props items setattr k v debug_repr TODO strip off fake tensor repr here repr proxy node meta example_value as_proxy proxy python_type class_type staticmethod specialize value torch Tensor props = dtype value dtype device value device layout value layout ndim int value ndim requires_grad value requires_grad is_nested value is_nested is_quantized value is_quantized is_sparse value is_sparse class_type type value try props has_grad_fn = value grad_fn None except Exception Workaround issues create_parameter_op Dynamo Reading grad_fn should never cause issue props has_grad_fn = False is_sparse_any value has_free_symbols value props _size = tuple int s is_symbolic s s s value size has_free_symbols value fully static shape keys props here inform specialization We have cast int here because these might get accessed ConstantVariable which has strict no-symint policy If we got here due having free symbols known constant already We could remove discrepancy here having ConstantVariable more permissive constant backed SymInts assert being strict has led some good signal hunting bugs I d like keep around now props _size = tuple non is_symbolic case applies jagged layout NestedTensor case singleton ints symbolic int s is_symbolic s s s value size props stride = tuple value stride torch _C _functorch is_batchedtensor value Batched tensors does support contiguity patterns so we refrain computing ` is_contiguous ` property props is_contiguous = None props is_contiguous = tuple x x torch _prims_common _memory_formats value is_contiguous memory_format=x props dynamic_getattr tx InstructionTranslator name fake_val = proxy node meta example_value For getattrs tensors without sources we can do better than default creating GetAttrVariable tensor traceable tensor subclass We getattr ing inner tensor subclass source is_traceable_wrapper_subclass fake_val attrs _ctx = fake_val __tensor_flatten__ proxy = getattr as_proxy name example_value = getattr fake_val name name attrs attrs returned tensor_flatten always tensors assert isinstance example_value torch Tensor builder wrap_fx_proxy wrap_fx_proxy tx=tx proxy=proxy example_value=example_value any other attributes subclass methods assumed constant metadata callable example_value VariableTracker build tx example_value source source subguards_allowed raise NotImplementedError For local source we associate real value We use real value implementing getattr fallthrough variable tracker base Note - scope construction mirrored guards A subsequent PR will introduce util scope = L tx output local_scope G tx output global_scope try We raise case we get typerror bug w SuperSource SuperSource has bugs atm can produce code like eval super L mod model model encoder embed_positions forward__class__ L mod model model encoder embed_positions scope Which incorrect violates invariant all sources should eval -able against scope _input_associated_real_value = eval source name scope except Exception exc raise NotImplementedError exc _input_associated_real_value None raise NotImplementedError object_has_getattribute _input_associated_real_value raise NotImplementedError get_custom_getattr _input_associated_real_value raise NotImplementedError real_value = getattr _input_associated_real_value name attr_source = AttrSource source name Typically we d want use variable builder here unfortunately id real_value __self__ id original value is_bound_tensor_method real_value No need install guard because its bound tensor method misc GetAttrVariable GetAttrVariable name source=attr_source py_type=type real_value install_guard attr_source make_guard GuardBuilder HASATTR VariableTracker build tx real_value attr_source method_attr_ndim tx ndim None ConstantVariable create ndim call_method tx dim method_attr_dtype tx dtype None ConstantVariable create dtype method_attr_device tx device None ConstantVariable create device method_attr_layout tx layout None ConstantVariable create layout method_attr_is_cuda tx device None ConstantVariable create device type == cuda method_attr_shape tx valid_size sizes = variables ConstantVariable create x x size SizeVariable sizes call_method tx size method_attr_requires_grad tx requires_grad None ConstantVariable create requires_grad method_attr_is_quantized tx is_quantized None ConstantVariable create is_quantized method_attr_is_sparse tx is_sparse None ConstantVariable create is_sparse method_attr_is_nested tx is_nested None ConstantVariable create is_nested method_attr_retain_grad tx unimplemented_v gb_type= Tensor retain_grad AOTDispatcher context=f var_getattr retain_grad explanation= ` Tensor retain_grad ` does work AOTDispatcher hints= method_attr_data tx variables TorchInGraphFunctionVariable torch _C _autograd _get_data_attr call_function tx method_attr_grad_fn tx has_grad_fn unimplemented_v gb_type= Tensor grad_fn context=f var_getattr grad_fn explanation= Dynamo does support tracing tensors grad_fn directly hints= variables ConstantVariable None method_attr__version tx tensor_version_op _tensor_version variables TorchInGraphFunctionVariable _tensor_version call_function tx call_obj_hasattr tx InstructionTranslator name GetAttrVariable builtin BuiltinVariable TODO - This good solution solves accuracy issue Today var_getattr returns GetAttrVariable both non-existent attributes existing attributes This bug requires more deep dive name size stride __iter__ ConstantVariable True try var = BuiltinVariable getattr call_function tx ConstantVariable name event TensorVariable returns NotImplemented BuiltinVariable call_getattr returns GetAttrVariable ret_val = isinstance var GetAttrVariable except AttributeError ret_val = False source install_guard AttrSource source name make_guard GuardBuilder HASATTR ConstantVariable ret_val var_getattr tx InstructionTranslator name is_strict_mode tx name _strict_mode_banned_ops unimplemented_v gb_type= Strict mode banned op context=f var_getattr name explanation=f Getattr invocation name strict mode supported hints= f Remove ` name ` list banned ops setting ` torch _dynamo config _autograd_backward_strict_mode_banned_ops ` name _strict_mode_conditional_banned_ops raise UnknownPropertiesDuringBackwardTrace f Unknown property name during speculating backward dynamo will insert contiguous call ahead speculate again noqa B name == __class__ UserDefinedClassVariable python_type handler = getattr f method_attr_ name None result = handler tx handler None None Add guard type matching these guards checked before tensor guards In some cases tensor attr guard can evaluated first break tensor later changed another type result None source source subguards_allowed name grad requires_grad result is_python_constant install_guard make_guard GuardBuilder TYPE_MATCH result source = AttrSource source name It s hard get inplace view metadata mutation graph input work properly across dynamo aot inductor just fall back source None hasattr torch ops aten name fn = getattr torch ops aten name hasattr fn overloads hasattr fn fn overloads torch Tag inplace_view getattr fn fn overloads tags Delay graph break actual call unsqueeze_ resize_ resize_as_ etc variables misc DelayGraphBreakVariable source=AttrSource source name msg= Getting inplace view graph input supported For attributes methods caught special handling above e g tensor real we handle these generically assuming output type tensor result None name = grad try_generic_attr_handling builder wrap_fx_proxy misc GetAttrVariable static_attr = all_tensor_attrs get name None static_attr None None Make sure attribute method type torch Tensor H should getset_descriptor This because CPython implementation see THPVariableType these attributes implemented under tp_getset which appear ` getset_descriptor ` s compared say methods which appear ` method_descriptor ` s type static_attr types GetSetDescriptorType None proxy = GetAttrVariable create_getattr_proxy as_proxy name source None wrap_fx_proxy tx=tx proxy=proxy source=AttrSource source name wrap_fx_proxy tx=tx proxy=proxy result = try_generic_attr_handling result None result = dynamic_getattr tx name result None raise NotImplementedError result call_id tx source unimplemented_v gb_type= Unsupported call_id without source context=f call_id explanation= call_id supported sourceless TensorVariable hints= For local source we associate real value We use real value scope = L tx output local_scope G tx output global_scope try _input_associated_real_value = eval source name scope except Exception exc unimplemented_v gb_type= Error getting associated real value context=f call_id explanation= Dynamo encountered error while trying get associated real value hints= from_exc=exc _input_associated_real_value None unimplemented_v gb_type= call_id without associated real value context=f call_id explanation= Dynamo could find associated real value tensor hints= install_guard source make_guard GuardBuilder ID_MATCH id_value = id _input_associated_real_value ConstantVariable create id_value has_unpack_var_sequence tx ndim unpack_var_sequence tx InstructionTranslator idxes=None builder wrap_fx_proxy_cls valid_size size_len = len size size_var = call_method tx size assert isinstance size_var SizeVariable size_len = len size_var items Ensure we don t unpack scalar tensor assert size_len = Can t unpack scalar tensors valid_size length = size dyn_length = call_method tx size ConstantVariable create SymNodeVariable symbolic sizes ConstantVariable constants OR values produced through symbolic_shapes end up int sympy Integer assert isinstance dyn_length SymNodeVariable ConstantVariable isinstance dyn_length SymNodeVariable length = dyn_length evaluate_expr tx output length = dyn_length value idxes None idxes = range length assert len idxes == length f Can t unpack tensor length rows into tuple len idxes elements wrap_fx_proxy_cls target_cls=type tx=tx proxy=self as_proxy i i idxes valid_size _size None property size assert _size None accessing None size TensorVariable _size _strict_mode_banned_ops torch _dynamo config _autograd_backward_strict_mode_banned_ops _strict_mode_conditional_banned_ops torch _dynamo config _autograd_backward_strict_mode_conditional_banned_ops call_method tx name args list VariableTracker kwargs dict str VariableTracker - VariableTracker builder SourcelessBuilder VariableBuilder torch_function can_dispatch_torch_function dispatch_torch_function is_strict_mode tx name _strict_mode_banned_ops unimplemented_v gb_type= Illegal method invocation strict mode context=f call_method name args kwargs explanation= Dynamo currently does support method f name invocation strict mode hints= Only override builtin tensor methods The user can manually add override handling decorator other methods e g dispatch subclass other methods static_attr = all_tensor_attrs get name None is_base_tensor_method = static_attr None can_dispatch_torch_function tx tuple + list args kwargs is_base_tensor_method source func_var = VariableBuilder tx AttrSource AttrSource source __class__ name static_attr func_var = SourcelessBuilder create tx getattr torch Tensor name dispatch_torch_function tx func_var tuple + list args kwargs Dispatch method-specific handler defined below If handler returns None doesn t exist we put method call graph This seen inspect signature where we check value default value name == __eq__ isinstance args UserDefinedClassVariable variables ConstantVariable False For historical reasons these ops decompose down syntactically invalid aten ops because they contain python keyword ` ` see discussions more details We graph break now since use case uncommon name == random_ unimplemented_v gb_type= Tensor random_ op context=f Tensor name args= kwargs= explanation= This currently supported hints= Use out-of-place version op graph_break_hints SUPPORTABLE name == uniform_ kwargs unimplemented_v gb_type= Tensor uniform_ op called ` ` keyword context=f Tensor name args= kwargs= explanation= This currently supported hints= Avoid using ` ` keyword graph_break_hints SUPPORTABLE try handler_method = getattr f method_ name except AttributeError pass try result = handler_method args kwargs result result except TypeError e unimplemented_v gb_type= Unhandled args method context=f call_method name args kwargs explanation= Dynamo encountered error while calling f method ` name ` hints= from_exc=e builder wrap_fx_proxy wrap_fx_proxy tx tx output create_proxy call_method name proxy_args_kwargs args kwargs method_size args kwargs _method_size_stride size args kwargs method_stride args kwargs _method_size_stride stride args kwargs _method_size_stride name dim=None dim = guard_if_dyn dim make_const_size_variable x options SizeVariable ConstantVariable create y options y x options RetVariable = make_const_size_variable name == size ConstantVariable create Technically should necessary I m including enhanced BC case example_value sometimes set really should always set though name = size r = getattr name name == size valid_size r = size r = None r None dim None RetVariable r ConstantVariable create r dim It might still constant Consult fake tensor see fake = proxy node meta get example_value None dim None fake_r = getattr fake name has_free_symbols fake_r int conversion safety case SymInt refined constant RetVariable tuple int r r fake_r fake_r = getattr fake name dim has_free_symbols fake_r ConstantVariable create int fake_r method_numel valid_size ConstantVariable create product size It might still constant Consult fake tensor see fake = proxy node meta get example_value None fake_r = fake numel has_free_symbols fake_r ConstantVariable create int fake_r method_nelement = method_numel method_dim ndim None ConstantVariable create ndim method_ndimension = method_dim method_is_floating_point dtype None ConstantVariable create dtype is_floating_point method_is_inference config fake_tensor_disable_inference_mode unimplemented_v gb_type= Encountered tensor is_inference during tracing context= explanation= tensor is_inference supported hints= graph_break_hints FUNDAMENTAL graph_break_hints INFERENCE_MODE fake = proxy node meta get example_value None ConstantVariable create fake is_inference method_is_complex dtype None ConstantVariable create dtype is_complex method_is_contiguous memory_format=None memory_format = memory_format as_python_constant memory_format None torch contiguous_format is_contiguous None ConstantVariable create memory_format is_contiguous fake = proxy node meta get example_value None ConstantVariable create fake is_contiguous memory_format=memory_format method_type dtype=None non_blocking=False kwargs dtype None dtype None isinstance device torch device tensortype = next k k v tensortype_to_dtype items dtype v device type == cpu ConstantVariable create f torch tensortype __name__ ConstantVariable create f torch device type tensortype __name__ dtype None fqn type dtype as_python_constant == torch tensortype torch FloatTensor etc all type torch tensortype torch fx s tracer fails these types because doesn t support arguments torch tensortype type So we pass string which also supported see above implementation type args tensor_type = dtype as_python_constant tensor_type_const = ConstantVariable create fqn tensor_type symbolic_convert InstructionTranslator builder wrap_fx_proxy tx = InstructionTranslator current_tx non_blocking kwargs = non_blocking non_blocking kwargs wrap_fx_proxy tx tx output create_proxy call_method type proxy_args_kwargs tensor_type_const kwargs method_as_subclass cls isinstance cls TensorSubclassVariable cls source symbolic_convert InstructionTranslator torch_function TensorWithTFOverrideVariable tx = InstructionTranslator current_tx py_cls = cls as_python_constant var = TensorWithTFOverrideVariable from_tensor_var tx py_cls cls source See NOTE Side effect tracking newly constructed tensor tx output side_effects _track_obj object var mutation_type_cls=AttributeMutationNew var unimplemented_v gb_type= Argument ` as_subclass ` must non-dispatcher-style tensor subclass context=f as_subclass cls explanation= Currently supported hints= Avoid call move outside ` torch compile ` regione graph_break_hints SUPPORTABLE method_get_device isinstance device torch device index = device index device type = cpu - ConstantVariable create index method_element_size ConstantVariable create dtype itemsize method_numpy force=False config trace_numpy unimplemented_v gb_type= Tensor numpy trace_numpy=False context=f call_method numpy explanation= ` Tensor numpy ` called ` trace_numpy ` configuration manually disabled hints= Set ` torch _dynamo config trace_numpy = True ` allow Dynamo trace through NumPy np unimplemented_v gb_type= Tensor numpy without NumPy installed context=f call_method numpy explanation= ` Tensor numpy ` called NumPy library available current environment hints= Ensure NumPy installed your Python environment layout = torch strided raise TypeError f can t convert layout layout tensor numpy Use Tensor to_dense first symbolic_convert InstructionTranslator tx = InstructionTranslator current_tx We don t check tensor CPU when force False allows us execute NumPy code CUDA Same requires_grad=True force force as_python_constant If user set force=True we try preserve semantics no gradients move CPU t = call_method tx detach proxy = tx output create_proxy call_method cpu t as_proxy Hacky way create view will marked NumpyNdarrayVariable proxy = tx output create_proxy call_method view_as proxy_args_kwargs NumpyNdarrayVariable create tx proxy method_tolist symbolic_convert InstructionTranslator builder wrap_fx_proxy tx = InstructionTranslator current_tx tolist tensor sub_proxy wrap i sub_proxy wrap_fx_proxy tx sub_proxy item tensor dtype torch int torch int torch int torch int unimplemented_v gb_type= Tensor tolist non-integer tensor context=f call_method to_list explanation= Dynamo currently does support tracing ` tolist ` non-integer tensors hints= Ensure input tensor ` tolist ` integer type e g int int int int tensor dim == wrap tensor sub_proxy tensor dim == wrap val sub_proxy i i val enumerate tensor tolist sub_tensor sub_proxy=sub_proxy i i sub_tensor enumerate tensor tensor = as_proxy node meta example_value out = tolist tensor as_proxy VariableTracker build tx out method_backward args kwargs unimplemented_v gb_type= Unsupported Tensor backward call context=f call_method backward args kwargs explanation= Dynamo currently does support tracing ` Tensor backward ` hints= graph_break_hints FUNDAMENTAL method_data_ptr args kwargs DataPtrVariable method_item args kwargs symbolic_convert InstructionTranslator tx = InstructionTranslator current_tx We enable capture_scalar_outputs when full_graph=True default tx one_graph config capture_scalar_outputs _warn_capture_scalar_outputs unimplemented_v gb_type= Unsupported Tensor item call capture_scalar_outputs=False context=f call_method item args kwargs explanation= Dynamo does support tracing ` Tensor item ` config capture_scalar_outputs=False hints= Set ` torch _dynamo config capture_scalar_outputs = True ` ` export TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS= ` include these operations captured graph method___getitem__ args kwargs symbolic_convert InstructionTranslator builder wrap_fx_proxy tx = InstructionTranslator current_tx isinstance args SymNodeVariable Standard indexing will force specialization due __index__ Rewrite regular torch op which will trace fine fn args = torch select variables ConstantVariable create args fn = operator getitem proxy = tx output create_proxy call_function fn proxy_args_kwargs + list args kwargs wrap_fx_proxy tx proxy staticmethod functools cache _warn_capture_scalar_outputs user_stack = torch _guards TracingContext extract_stack user_stack_formatted = join traceback format_list user_stack log warning textwrap dedent \ Graph break ` Tensor item ` consider setting torch _dynamo config capture_scalar_outputs = True env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS= include these operations captured graph Graph break user code s user_stack_formatted method___len__ symbolic_convert InstructionTranslator tx = InstructionTranslator current_tx call_method tx size ConstantVariable create method___iter__ symbolic_convert InstructionTranslator tx = InstructionTranslator current_tx ListIteratorVariable unpack_var_sequence tx mutation_type=ValueMutationNew method_addcmul_ tensor tensor value=None symbolic_convert InstructionTranslator tx = InstructionTranslator current_tx value None polyfills tx inline_user_function_return VariableTracker build tx polyfills addcmul_inplace tensor tensor value method___setitem__ key value symbolic_convert InstructionTranslator tx = InstructionTranslator current_tx proxy = tx output create_proxy call_function operator setitem proxy_args_kwargs key value isinstance value TensorVariable Note Tensor __setitem__ VariableTracker metadata At point we proxied node representing ` key = value ` into graph When executed node will mutate ` ` s tensor metadata so s important even during tracing propagate For example value requires_grad True = requires_grad becomes True value requires_grad True = has_grad_fn becomes True Not sure __setitem__ can ever save activations disabling just case Ignore fresh unbacked symbols could arise internal indexing selection happen code like t idx += when idx unbacked Namely selection during setitem When selection happens idx unbacked we allocate new unbacked symbol storage offset select_meta output operation setitem does depend selection torch _dynamo utils _disable_saved_tensors_hooks_during_tracing tx fake_mode shape_env ignore_fresh_unbacked_symbols tx fake_mode tx fake_mode shape_env nullcontext get_fake_value proxy node tx allow_non_graph_fake=False vt = value isinstance vt variables lazy LazyVariableTracker vt = variables lazy LazyVariableTracker realize_all vt synchronize_attributes tx type vt config use_graph_deduplication config track_nodes_for_deduplication tx output region_tracker add_node_mutation proxy node ConstantVariable create None method_resize_ args kwargs unimplemented_v gb_type= Unsupported Tensor resize_ call context=f call_method resize_ args kwargs explanation= Dynamo currently does support tracing ` Tensor resize_ ` hints= method_resize_as_ args kwargs unimplemented_v gb_type= Unsupported Tensor resize_as_ call context=f call_method resize_as_ args kwargs explanation= Dynamo currently does support tracing ` Tensor resize_as_ ` hints= method_sparse_resize_ args kwargs unimplemented_v gb_type= Unsupported Tensor sparse_resize_ call context=f call_method sparse_resize_ args kwargs explanation= Dynamo currently does support tracing ` Tensor sparse_resize_ ` hints= method_sparse_resize_and_clear_ args kwargs unimplemented_v gb_type= Unsupported Tensor sparse_resize_and_clear_ call context=f call_method sparse_resize_and_clear_ args kwargs explanation= Dynamo currently does support tracing ` Tensor sparse_resize_and_clear_ ` hints= method_set_ args kwargs len args torch Tensor set_ has several overloads aten set_ source_Tensor Tensor gets special handling AOTAutograd functionalization because most common overload used FSDP graph-breaking aten set_source_Tensor_storage_offset now unless we find we need make work unimplemented_v gb_type= Unsupported Tensor set_ call context=f call_method set_ args kwargs explanation= Dynamo currently does support tracing ` Tensor set_ ` overloads include more than one argument hints= graph_break_hints SUPPORTABLE method_add_ other alpha=None alpha None symbolic_convert InstructionTranslator tx = InstructionTranslator current_tx result = variables TorchInGraphFunctionVariable torch mul call_function tx other alpha call_method tx add_ result method_addcdiv_ tensor tensor value=None symbolic_convert InstructionTranslator tx = InstructionTranslator current_tx value None result = variables TorchInGraphFunctionVariable torch div call_function tx tensor tensor result = variables TorchInGraphFunctionVariable torch mul call_function tx result value call_method tx add_ result method___contains__ arg symbolic_convert InstructionTranslator tx = InstructionTranslator current_tx Rewrite __contains__ here so downstream passes can trace through without dealing unbacked symbool Roughly code we translate __contains__ x x == any item result = variables TorchInGraphFunctionVariable torch eq call_function tx arg result = variables TorchInGraphFunctionVariable torch any call_function tx result result call_method tx item method_redistribute args kwargs symbolic_convert InstructionTranslator tx = InstructionTranslator current_tx rewrite non-primitive args kwargs included on-the-fly prim function rewrite args have only proxyable args then insert call_function args_as_value = x as_python_constant x args kwargs_as_value = k v as_python_constant k v kwargs items redistribute_fn_with_prim_types x x redistribute args_as_value kwargs_as_value attach same function name better debugging redistribute_fn_with_prim_types __name__ = prim_redistribute builder wrap_fx_proxy wrap_fx_proxy tx=tx proxy=tx output create_proxy call_function redistribute_fn_with_prim_types proxy_args_kwargs method_to_local args kwargs symbolic_convert InstructionTranslator tx = InstructionTranslator current_tx rewrite non-primitive args kwargs included on-the-fly prim function rewrite args have only proxyable args then insert call_function args_as_value = x as_python_constant x args kwargs_as_value = k v as_python_constant k v kwargs items to_local_fn_with_prim_types x x to_local args_as_value kwargs_as_value attach same function name better debugging to_local_fn_with_prim_types __name__ = prim_to_local builder wrap_fx_proxy wrap_fx_proxy tx=tx proxy=tx output create_proxy call_function to_local_fn_with_prim_types proxy_args_kwargs method_register_hook args kwargs _method_register_hook register_hook args kwargs method_register_post_accumulate_grad_hook args kwargs _method_register_hook register_post_accumulate_grad_hook args kwargs _method_register_hook name str hook VariableTracker Note - do arbitrarily add hooks here - make sure they match same contract see On tensor register_hook symbolic_convert InstructionTranslator tx = InstructionTranslator current_tx source compiled_autograd compiled_autograd_enabled TODO voz We can relax speculating callable ensuring doesn t modify arbitrary python state We Must compiled_autograd here because backward hooks can contain anything unsafe run them compiled bwd without re-entering dynamo compiled_autograd does Discussion point - Should we bypass nopython fullgraph = True No Because going graph break anyway - check does introduce new graph breaks where there none Discussion point - Should we defer check backwards No Because compiled autograd yet ready prime time As such we defer user would have no recourse - their forward traces just fine will fail backwards unless compiled_autograd enabled If compiled_autograd fails there lot failures today then they have nothing they can do except disable compile unimplemented_v gb_type= Compilation intermediate hooks requires compiled autograd context=f var_getattr name explanation= Dynamo must compiled_autograd register hooks hints= hook_name bw_state_proxy = tx output add_backward_state_hook hook _register_hook_trampoline tensor bw_state register_hook = getattr tensor name register_hook functools partial trace_wrapped fn=call_hook_from_backward_state bw_state=bw_state hook_name=hook_name TODO jansel returning None here wrong should RemovableHandle we need some extra work support properly None builder wrap_fx_proxy self_proxy = as_proxy self_proxy node meta has_backward_hook = True wrap_fx_proxy tx tx output create_proxy call_function _register_hook_trampoline self_proxy bw_state_proxy handle_variable = variables RemovableHandleVariable mutation_type=variables base ValueMutationNew tx output side_effects register_hook hook handle_variable name handle_variable method_requires_grad_ requires_grad=True requires_grad True requires_grad = requires_grad as_python_constant as_proxy node meta example_value requires_grad = requires_grad unimplemented_v gb_type= Unsupported Tensor requires_grad_ call context=f call_method requires_grad_ explanation= Dynamo does support changes Tensor s ` requires_grad ` through calling ` requires_grad_ ` hints= method_new args kwargs Convert x new torch Size into x new_empty torch Size Tensor new acts differently Size input versus tuple input len args == isinstance args SizeVariable len args = all isinstance ConstantVariable python_type int args symbolic_convert InstructionTranslator call_method InstructionTranslator current_tx new_empty args kwargs method_untyped_storage UntypedStorageVariable as_proxy node meta example_value untyped_storage set_name_hint name str _is_name_set proxy node _rename name _is_name_set = True SymNodeVariable VariableTracker Represents symbolic scalar either int float bool This most commonly used handle symbolic size computation e g tensor size also used handle logic like float_tensor item unspecialized float inputs _nonvar_fields = proxy sym_num VariableTracker _nonvar_fields debug_repr repr sym_num classmethod create cls tx proxy sym_num=None options sym_num None sym_num = get_fake_value proxy node tx example_value proxy node meta assert proxy node meta example_value == sym_num set_example_value proxy node sym_num isinstance sym_num sympy Integer int bool sym_num = int sym_num isinstance sym_num sympy Integer sym_num ConstantVariable create sym_num SymNodeVariable proxy sym_num options __init__ proxy sym_num kwargs - None super __init__ kwargs proxy = proxy TODO Should we allow non SymTypes here Today allowed sym_num = sym_num _tensor_var = None python_type isinstance sym_num SymTypes sym_num node pytype type sym_num as_proxy proxy as_tensor tx dtype _tensor_var None _tensor_var = VariableTracker build tx torch scalar_tensor call_function tx dtype VariableTracker build tx dtype _tensor_var evaluate_expr output_graph=None try guard_scalar sym_num except GuardOnDataDependentSymNode e torch fx experimental _config no_data_dependent_graph_break raise raise UserError noqa B UserErrorType ANTI_PATTERN f Consider annotating your code using torch _check str e case_name= constrain_as_size_example call_method tx name args list VariableTracker kwargs dict str VariableTracker - VariableTracker builder wrap_fx_proxy wrap_fx_proxy tx tx output create_proxy call_method name proxy_args_kwargs args kwargs NumpyNdarrayVariable TensorVariable Represents np ndarray backed torch Tensor via torch _numpy ndarray Use Tensor numpy call staticmethod create tx InstructionTranslator proxy options builder wrap_fx_proxy_cls wrap_fx_proxy_cls target_cls=NumpyNdarrayVariable tx=tx proxy=proxy options var_getattr tx InstructionTranslator name NB This INTENTIONALLY does call super because there no intrinsic reason ndarray properties related Tensor properties The inheritance here implementation sharing utils numpy_attr_wrapper builder wrap_fx_proxy result = None example_value = as_proxy node meta example_value example_ndarray = tnp ndarray example_value insert_into_graph wrap_fx_proxy tx tx output create_proxy call_function numpy_attr_wrapper as_proxy name name T real imag proxy = tx output create_proxy call_function numpy_attr_wrapper as_proxy name result = NumpyNdarrayVariable create tx proxy These awkward implement The standard playbook torch _numpy interop trace call into torch _numpy wrapper which works Tensor operations However we don t want do calls don t Tensors because those cases we may want trace attribute access into graph all sort harmless do so because AOTAutograd will eliminate them s best trace them begin But any case tracing these into graph like trying fit square peg into round hole best do So instead we painstakingly implement these hand NB only ALWAYS specialized attributes can go here notably size shape allowed name ndim itemsize ConstantVariable create getattr example_ndarray name name shape stride has_free_symbols r = getattr example_ndarray name ConstantVariable create tuple int r r r insert_into_graph name == size has_free_symbols r = example_ndarray size ConstantVariable create int r insert_into_graph name base flags dtype unimplemented_v gb_type= Unsupported ndarray attribute access context=f var_getattr name explanation=f Dynamo currently does support tracing ` ndarray name ` hints= name == __version__ unimplemented_v gb_type= Unsupported ndarray __version__ access context=f var_getattr name explanation=f Dynamo currently does support tracing ` ndarray name ` hints= result None raise NotImplementedError result staticmethod patch_args name args kwargs name == clip kwargs_rename = a_min min a_max max kwargs = kwargs_rename get k k v k v kwargs items args kwargs call_method tx name args list VariableTracker kwargs dict str VariableTracker - VariableTracker exc unimplemented_v utils numpy_method_wrapper args kwargs = patch_args name args kwargs name == astype builtin BuiltinVariable dtype_arg = None dtype kwargs dtype_arg = kwargs dtype len args dtype_arg = args is_object_str = isinstance dtype_arg ConstantVariable dtype_arg value == O is_object_type = isinstance dtype_arg BuiltinVariable dtype_arg fn object is_object_str is_object_type unimplemented_v gb_type= ndarray astype object context=f call_method name args kwargs explanation= ` ndarray astype O ` ` ndarray astype object ` supported torch compile there no equivalent object type torch Tensor This will executed eagerly hints= graph_break_hints FUNDAMENTAL name __len__ size tolist __iter__ delegate back TensorVariable super call_method tx name args kwargs name tostring tobytes __delattr__ unimplemented_v gb_type= Unsupported ndarray method call context=f call_method name args kwargs explanation=f ` ndarray name ` modelled ` torch _numpy ` hints= proxy = tx output create_proxy call_function numpy_method_wrapper name proxy_args_kwargs + list args kwargs NumpyNdarrayVariable create tx proxy python_type np ndarray UnspecializedPythonVariable TensorVariable This -element tensor represents unspecialized python float int _nonvar_fields = raw_value need_unwrap TensorVariable _nonvar_fields __init__ proxy torch fx Proxy raw_value=None need_unwrap=True kwargs - None super __init__ proxy kwargs raw_value = raw_value need_unwrap = need_unwrap classmethod from_tensor_variable cls tensor_variable raw_value need_unwrap=True Convert ` TensorVariable ` instance into ` UnspecializedPythonVariable ` instance UnspecializedPythonVariable dict tensor_variable __dict__ raw_value=raw_value need_unwrap=need_unwrap FakeItemVariable TensorVariable An unspecialized python variable which prevents access underlying raw value This needed item called FakeTensor _nonvar_fields = need_unwrap TensorVariable _nonvar_fields __init__ proxy torch fx Proxy kwargs - None need_unwrap = kwargs pop need_unwrap False super __init__ proxy kwargs need_unwrap = need_unwrap classmethod from_tensor_variable cls tensor_variable FakeItemVariable dict tensor_variable __dict__ TensorSubclassVariable UserDefinedClassVariable call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker Handle ` Subclass existing_tensor ` calls torch_function TensorWithTFOverrideVariable new_func = value __new__ new_func torch Tensor __new__ len args == isinstance args TensorVariable len kwargs == data = args Simulate ` torch Tensor __new__ ` shallow-copying input tensor data new type TODO polyfill var = TensorWithTFOverrideVariable from_tensor_var tx data value source unimplemented_v gb_type= Calling subclass default constructor more than tensor argument context=f value args= args kwargs= kwargs explanation= Currently supported hints= Avoid constructor call move outside ` torch compile ` regione graph_break_hints SUPPORTABLE Let Dynamo trace through custom ` __new__ ` var = VariableTracker build tx new_func call_function tx + args kwargs Let Dynamo trace through custom ` __init__ ` init_func = value __init__ TODO builder should able handle ` torch Tensor __init__ ` which ` object __init__ ` so we can remove check init_func torch Tensor __init__ VariableTracker build tx init_func call_function tx var kwargs See NOTE Side effect tracking newly constructed tensor tx output side_effects _track_obj object var mutation_type_cls=AttributeMutationNew var as_python_constant value UntypedStorageVariable VariableTracker _nonvar_fields = example_value VariableTracker _nonvar_fields __init__ from_tensor TensorVariable example_value torch UntypedStorage kwargs - None super __init__ kwargs from_tensor = from_tensor Example_value will always have device= meta example_value = example_value call_method tx name args list VariableTracker kwargs dict str VariableTracker - VariableTracker name == size args kwargs raise_args_mismatch tx name args kwargs f len args args len kwargs kwargs result = example_value size has_free_symbols result avoid creating node graph ConstantVariable create int result external_utils untyped_storage_size builder wrap_fx_proxy wrap_fx_proxy tx tx output create_proxy call_function untyped_storage_size from_tensor as_proxy name == resize_ len args == kwargs raise_args_mismatch tx name kwargs f len kwargs kwargs tx output create_proxy call_function torch ops inductor resize_storage_bytes_ from_tensor as_proxy args as_proxy super call_method tx name args kwargs reconstruct codegen PyCodegen codegen from_tensor codegen load_method untyped_storage codegen call_method DataPtrVariable VariableTracker __init__ from_tensor TensorVariable kwargs - None super __init__ kwargs from_tensor = from_tensor reconstruct codegen PyCodegen codegen from_tensor codegen load_method data_ptr codegen call_method