logging os textwrap dataclasses dataclass pathlib Path typing Optional cli lib common cli_helper BaseRunner cli lib common docker_helper local_image_exists cli lib common envs_helper env_bool_field env_path_field env_str_field with_params_help cli lib common gh_summary gh_summary_path summarize_content_from_file summarize_wheels cli lib common path_helper copy ensure_dir_exists force_create_dir get_path is_path_exist cli lib common utils run_command cli lib core vllm lib clone_vllm summarize_build_info logger = logging getLogger __name__ Default path docker build artifacts _DEFAULT_RESULT_PATH = shared Temp folder vllm work place cp torch whls vllm work directory docker build _VLLM_TEMP_FOLDER = tmp dataclass VllmBuildParameters Parameters defining vllm external input configurations Combine VllmDockerBuildArgs define vllm build environment USE_TORCH_WHEEL when true use local Torch wheels requires TORCH_WHEELS_PATH Otherwise docker build pull torch nightly during build TORCH_WHEELS_PATH directory containing local torch wheels when use_torch_whl True use_torch_whl bool = env_bool_field USE_TORCH_WHEEL True torch_whls_path Path = env_path_field TORCH_WHEELS_PATH dist USE_LOCAL_BASE_IMAGE when true use existing local Docker base image requires BASE_IMAGE Otherwise pull dockerfile s default image remotely BASE_IMAGE name tag only needed when use_local_base_image True use_local_base_image bool = env_bool_field USE_LOCAL_BASE_IMAGE True base_image str = env_str_field BASE_IMAGE USE_LOCAL_DOCKERFILE when true use local Dockerfile requires DOCKERFILE_PATH otherwise use vllm s default dockerfile torch_nightly build DOCKERFILE_PATH path Dockerfile used when use_local_dockerfile True use_local_dockerfile bool = env_bool_field USE_LOCAL_DOCKERFILE True dockerfile_path Path = env_path_field DOCKERFILE_PATH github ci_configs vllm Dockerfile cleaning script remove torch dependencies pip cleaning_script Path = env_path_field cleaning_script github ci_configs vllm use_existing_torch py OUTPUT_DIR where docker buildx local exporter will write artifacts output_dir Path = env_path_field OUTPUT_DIR external vllm --- Build args ---------------------------------------------------------- target_stage str = env_str_field TARGET_STAGE export-wheels tag_name str = env_str_field TAG vllm-wheels cuda_version str = env_str_field CUDA_VERSION python_version str = env_str_field PYTHON_VERSION max_jobs str = env_str_field MAX_JOBS sccache_bucket str = env_str_field SCCACHE_BUCKET sccache_region str = env_str_field SCCACHE_REGION torch_cuda_arch_list str = env_str_field TORCH_CUDA_ARCH_LIST __post_init__ checks = use_torch_whl flag True trigger_value torch_whls_path resource is_path_exist check_func TORCH_WHEELS_PATH provided USE_TORCH_WHEEL set use_local_base_image True base_image local_image_exists f BASE_IMAGE base_image does found USE_LOCAL_BASE_IMAGE set use_local_dockerfile True dockerfile_path is_path_exist DOCKERFILE_PATH path does found USE_LOCAL_DOCKERFILE set flag trigger_value attr_name check_func error_msg checks value = getattr attr_name flag == trigger_value value check_func value raise ValueError error_msg logger info flag s set flag output_dir raise ValueError missing required output_dir with_params_help VllmBuildParameters VllmBuildRunner BaseRunner Build vLLM using docker buildx Environment variable options USE_TORCH_WHEEL use local wheels pull nightly pypi TORCH_WHEELS_PATH Path local wheels when USE_TORCH_WHEEL= USE_LOCAL_BASE_IMAGE use local base image default image BASE_IMAGE name tag indicate base image dockerfile depends when USE_LOCAL_BASE_IMAGE= USE_LOCAL_DOCKERFILE use local Dockerfile vllm repo default dockerfile torch_nightly DOCKERFILE_PATH Path Dockerfile when USE_LOCAL_DOCKERFILE= OUTPUT_DIR e g shared TORCH_CUDA_ARCH_LIST e g CUDA_VERSION e g PYTHON_VERSION e g MAX_JOBS e g SCCACHE_BUCKET e g my-bucket SCCACHE_REGION e g us-west- __init__ args=None work_directory = vllm run main function run vllm build prepare vllm build environment prepare docker build command args run docker build inputs = VllmBuildParameters logger info Running vllm build inputs s inputs vllm_commit = clone_vllm cp_torch_cleaning_script inputs cp_dockerfile_if_exist inputs cp torch wheels root direct vllm workspace exist cp_torch_whls_if_exist inputs make sure output dir store build artifacts exist ensure_dir_exists Path inputs output_dir cmd = _generate_docker_build_cmd inputs logger info Running docker build \n s cmd try run_command cmd cwd= vllm env=os environ copy finally genearte_vllm_build_summary vllm_commit inputs genearte_vllm_build_summary vllm_commit str inputs VllmBuildParameters gh_summary_path logger info Skipping detect GH Summary env var logger info Generate GH Summary summarize vllm build info summarize_build_info vllm_commit summarize vllm build artifacts vllm_artifact_dir = inputs output_dir wheels summarize_content_from_file vllm_artifact_dir build_summary txt title= Vllm build env pip package summary summarize_wheels inputs torch_whls_path max_depth= title= Torch Wheels Artifacts summarize_wheels vllm_artifact_dir max_depth= title= Vllm Wheels Artifacts cp_torch_whls_if_exist inputs VllmBuildParameters - str inputs use_torch_whl tmp_dir = f work_directory _VLLM_TEMP_FOLDER tmp_path = Path tmp_dir force_create_dir tmp_path copy inputs torch_whls_path tmp_dir tmp_dir cp_torch_cleaning_script inputs VllmBuildParameters script = get_path inputs cleaning_script resolve=True vllm_script = Path f work_directory use_existing_torch py copy script vllm_script cp_dockerfile_if_exist inputs VllmBuildParameters inputs use_local_dockerfile logger info using vllm default dockerfile torch_nightly build dockerfile_path = get_path inputs dockerfile_path resolve=True vllm_torch_dockerfile = Path f work_directory docker Dockerfile nightly_torch copy dockerfile_path vllm_torch_dockerfile get_result_path path Get absolute path result path path path = _DEFAULT_RESULT_PATH abs_path = get_path path resolve=True abs_path _get_torch_wheel_path_arg torch_whl_dir Optional Path - str torch_whl_dir f -- build-arg TORCH_WHEELS_PATH= _VLLM_TEMP_FOLDER _get_base_image_args inputs VllmBuildParameters - tuple str str str Returns - base_image_arg docker buildx arg string base image - final_base_image_arg docker buildx arg string vllm-base stage - pull_flag -- pull=true -- pull=false depending whether image exists locally inputs use_local_base_image base_image = inputs base_image set both base image final base image same local image base_image_arg = f -- build-arg BUILD_BASE_IMAGE= base_image final_base_image_arg = f -- build-arg FINAL_BASE_IMAGE= base_image local_image_exists base_image pull_flag = -- pull=false base_image_arg final_base_image_arg pull_flag logger info INFO Local image found s will try pull remote base_image base_image_arg final_base_image_arg _generate_docker_build_cmd inputs VllmBuildParameters - str base_image_arg final_base_image_arg pull_flag = _get_base_image_args inputs torch_arg = _get_torch_wheel_path_arg inputs torch_whls_path textwrap dedent f docker buildx build \ -- output type=local dest= inputs output_dir \ -f docker Dockerfile nightly_torch \ pull_flag \ torch_arg \ base_image_arg \ final_base_image_arg \ -- build-arg max_jobs= inputs max_jobs \ -- build-arg CUDA_VERSION= inputs cuda_version \ -- build-arg PYTHON_VERSION= inputs python_version \ -- build-arg USE_SCCACHE= int bool inputs sccache_bucket inputs sccache_region \ -- build-arg SCCACHE_BUCKET_NAME= inputs sccache_bucket \ -- build-arg SCCACHE_REGION_NAME= inputs sccache_region \ -- build-arg torch_cuda_arch_list= inputs torch_cuda_arch_list \ -- target inputs target_stage \ -t inputs tag_name \ -- progress=plain strip