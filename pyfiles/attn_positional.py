Copyright c Facebook Inc its affiliates All rights reserved This source code licensed under BSD-style license found LICENSE file root directory source tree math torch torch nn BertSelfAttention nn Module __init__ hidden_size num_attention_heads attention_probs_dropout_prob position_embedding_type=None max_position_embeddings=None super __init__ hidden_size num_attention_heads = raise ValueError f The hidden size hidden_size multiple number attention f heads num_attention_heads num_attention_heads = num_attention_heads attention_head_size = int hidden_size num_attention_heads all_head_size = num_attention_heads attention_head_size query = nn Linear hidden_size all_head_size key = nn Linear hidden_size all_head_size value = nn Linear hidden_size all_head_size dropout = nn Dropout attention_probs_dropout_prob position_embedding_type = position_embedding_type position_embedding_type None assert max_position_embeddings None max_position_embeddings = max_position_embeddings distance_embedding = nn Embedding max_position_embeddings - attention_head_size transpose_for_scores x new_x_shape = x size - + num_attention_heads attention_head_size x = x view new_x_shape x permute forward hidden_states past_key_value=None q = query hidden_states k = key hidden_states v = value hidden_states q = transpose_for_scores q k = transpose_for_scores k v = transpose_for_scores v past_key_value None k = torch cat past_key_value k dim= v = torch cat past_key_value v dim= Take dot product between query key get raw attention scores attention_scores = torch matmul q k transpose - - attention_scores = attention_scores math sqrt attention_head_size position_embedding_type None seq_length = hidden_states size position_ids_l = torch arange seq_length dtype=torch long device=hidden_states device view - position_ids_r = torch arange seq_length dtype=torch long device=hidden_states device view - distance = position_ids_l - position_ids_r positional_embedding = distance_embedding distance + max_position_embeddings - positional_embedding = positional_embedding dtype=q dtype fp compatibility position_embedding_type == relative_key relative_position_scores = torch einsum bhld lrd- bhlr q positional_embedding attention_scores = attention_scores + relative_position_scores position_embedding_type == relative_key_query relative_position_scores_query = torch einsum bhld lrd- bhlr q positional_embedding relative_position_scores_key = torch einsum bhrd lrd- bhlr k positional_embedding attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key attention_probs = attention_scores Normalize attention scores probabilities attention_probs = nn functional softmax attention_scores dim=- This actually dropping out entire tokens attend which might seem bit unusual taken original Transformer paper attention_probs = dropout attention_probs context_layer = torch matmul attention_probs v context_layer = context_layer permute contiguous new_context_layer_shape = context_layer size - + all_head_size context_layer = context_layer view new_context_layer_shape context_layer