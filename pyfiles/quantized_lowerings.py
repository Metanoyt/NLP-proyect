logging typing Any torch torch _inductor kernel mm_common mm_args config lowering codegen cpp_gemm_template CppGemmTemplate CppWoqInt GemmTemplate codegen cpp_utils create_epilogue_with_attr lowering expand register_lowering mkldnn_ir WeightInt PackMatmul select_algorithm autotune_select_algorithm ExternKernelChoice realize_inputs utils use_aten_gemm_kernels use_cpp_gemm_template virtualized V log = logging getLogger __name__ aten__weight_int pack_mm = ExternKernelChoice torch _weight_int pack_mm _weight_int pack_mm has_out_variant=False aten__weight_int pack_mm_cpu = ExternKernelChoice torch ops quantized int mm_packed_weight_cpu native _weight_int pack_mm_cpu_tensor has_out_variant=False kernel_creator=WeightInt PackMatmul create quantized = torch ops quantized _quantized = torch ops _quantized aten = torch ops aten register_quantized_ops - None lowering add_needs_realized_inputs quantized max_pool d _quantized wrapped_fbgemm_pack_gemm_matrix_fp _quantized wrapped_fbgemm_linear_fp _weight lowering make_fallback quantized max_pool d lowering make_fallback _quantized wrapped_fbgemm_pack_gemm_matrix_fp lowering make_fallback _quantized wrapped_fbgemm_linear_fp _weight register_woq_mm_ops - None register_lowering aten _weight_int pack_mm type_promotion_kind=None type ignore misc int pack_mm input torch Tensor weight torch Tensor scale torch Tensor layout Any = None - Any _ _ _ layout mat mat = mm_args input weight layout=layout mat _transposed=True assert mat get_dtype torch bfloat torch float torch float mat get_dtype == torch int aten_layout = layout options tune choices = aten__weight_int pack_mm bind mat mat scale aten_layout use_aten_gemm_kernels scale applied epilogue scale tensor expanded view op broadcasting s D _mul_epilogue buf torch Tensor - Any create_epilogue_with_attr buf mul other=realize_inputs expand scale layout size use_cpp_gemm_template aten_layout mat mat mat _transposed=True CppGemmTemplate add_choices choices aten_layout mat mat scale trans_w=True epilogue_creator=_mul_epilogue type ignore arg-type autotune_select_algorithm _weight_int pack_mm choices mat mat scale aten_layout register_lowering aten _weight_int pack_mm_for_cpu type_promotion_kind=None type ignore misc int pack_mm_cpu input torch Tensor weight torch Tensor qGroupSize int qScaleAndZeros torch Tensor layout Any = None - Any _ _ _ layout mat mat = mm_args input weight layout=layout use_ x _dim=True mat _transposed=True assert mat get_dtype torch bfloat torch float torch float mat get_dtype == torch uint group_size = V graph add_tensor_constant torch tensor qGroupSize dtype=torch int name=None aten_layout = layout options tune choices = aten__weight_int pack_mm_cpu bind mat mat group_size qScaleAndZeros aten_layout use_aten_gemm_kernels config max_autotune config max_autotune_gemm use_cpp_gemm_template aten_layout mat mat mat _transposed=True is_woq_int =True q_group_size=qGroupSize mat get_layout is_contiguous pyrefly ignore bad-specialization missing-attribute not-a-type CppWoqInt GemmTemplate qGroupSize add_choices choices aten_layout mat mat group_size qScaleAndZeros define functions generate example inputs weight group size otherwise autotuner generates example inputs all zeros them get_example_weight x torch _inductor ir IRNode - torch Tensor assert x get_layout is_contiguous shape = x get_size device = x get_device torch randint shape dtype=torch uint device=device input_gen_fns = get_example_weight packed weight lambda x V graph constants x get_name group size autotune_select_algorithm _weight_int pack_mm_for_cpu choices mat mat group_size qScaleAndZeros aten_layout input_gen_fns=input_gen_fns lowering make_fallback aten _dyn_quant_matmul_ bit lowering make_fallback aten _dyn_quant_pack_ bit_weight