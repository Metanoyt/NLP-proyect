mypy allow-untyped-defs typing Optional torch torch nn functional F expanded_weights_impl ExpandedWeight implements_per_sample_grads expanded_weights_utils forward_helper set_grad_sample_if_exists standard_kwargs sum_over_all_but_batch_and_last_n unpack_expanded_weight_or_tensor implements_per_sample_grads F layer_norm LayerNormPerSampleGrad torch autograd Function staticmethod pyrefly ignore bad-override forward ctx kwarg_names _ expanded_args_and_kwargs expanded_args expanded_kwargs = standard_kwargs kwarg_names expanded_args_and_kwargs input = expanded_args normalized_shape = expanded_args len input shape = len normalized_shape raise RuntimeError Expanded Weights Layer norm should normalize over batch dimension per sample gradient f computations got normalized shape normalized_shape matched input shape output mean rstd = forward_helper torch native_layer_norm expanded_args expanded_kwargs ctx args = expanded_args input requires_grad isinstance expanded_kwargs weight ExpandedWeight ctx weight = expanded_kwargs weight input requires_grad isinstance expanded_kwargs bias ExpandedWeight ctx bias = expanded_kwargs bias ctx eps = expanded_kwargs eps ctx mean ctx rstd = mean rstd output staticmethod pyrefly ignore bad-override backward ctx grad_output weight_per_sample_grad weight sum_over_all_but_batch_and_last_n F layer_norm input normalized_shape eps=ctx eps grad_output weight dim input normalized_shape = ctx args mean rstd = ctx mean ctx rstd results list Optional torch Tensor = results append None kwarg names results append None op reference input requires_grad weight_ = unpack_expanded_weight_or_tensor ctx weight bias_ = unpack_expanded_weight_or_tensor ctx bias results append torch ops aten native_layer_norm_backward grad_output input normalized_shape mean rstd weight_ bias_ True False False results append None weight bias don t compute batched gradients no other arguments differentiable results = results + None set grad_sample field weight bias per sample gradients hasattr ctx weight set_grad_sample_if_exists ctx weight weight_per_sample_grad hasattr ctx bias set_grad_sample_if_exists ctx bias lambda bias sum_over_all_but_batch_and_last_n grad_output bias dim tuple results