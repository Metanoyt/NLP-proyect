argparse csv gc itertools json random sys collections defaultdict collections abc Callable contextlib nullcontext dataclasses asdict dataclass functools partial wraps typing Literal Optional Union numpy np config_utils heads_input_type load_config_file print_default_config tabulate tabulate tqdm tqdm torch torch nn functional F torch nn attention sdpa_kernel SDPBackend torch nn attention flex_attention BlockMask create_block_mask create_mask flex_attention noop_mask torch _dynamo config automatic_dynamic_shapes = False Needed since changing args function causes recompiles torch _dynamo config recompile_limit = torch _inductor runtime benchmarking benchmarker cleanup_memory Aggressively free GPU memory torch cuda empty_cache gc collect torch cuda is_available torch cuda synchronize safe_backend backend_name=None return_dict=False Decorator wraps backend functions error handling Args backend_name Name backend error messages return_dict If True returns dict results all backends run_single_experiment If False returns single ExperimentResults individual backend functions decorator func wraps func wrapper config args kwargs try func config args kwargs except torch OutOfMemoryError print f SKIP OOM backend_name func __name__ shape config shape cleanup_memory except RuntimeError e error_msg = str e out resource error_msg OutOfMemoryError error_msg print f SKIP Triton OOM backend_name func __name__ shape config shape cleanup_memory No valid triton configs error_msg print f SKIP No valid Triton config backend_name func __name__ shape config shape print f SKIP Runtime error backend_name func __name__ shape config shape str e except Exception e print f SKIP Error backend_name func __name__ shape config shape str e Return appropriate NaN result based function type return_dict For run_single_experiment dict NaN all backends nan_result = ExperimentResults fwd_time=float nan bwd_time=float nan config calculate_bwd_time None results = dict fromkeys config backends nan_result results flex = ExperimentResults fwd_time=float nan bwd_time=float nan config calculate_bwd_time None sparsity=None results For individual backend functions single ExperimentResults ExperimentResults fwd_time=float nan bwd_time=float nan config calculate_bwd_time None wrapper decorator Type definitions Backend = Literal math efficient cudnn fav fav fakv og-eager AttentionType = Literal noop causal rel head_bias alibi sliding_window document_mask prefix_lm softcap DtypeString = Literal bfloat float float SpeedupType = Literal fwd bwd benchmark_torch_function_in_microseconds func Callable args kwargs - float warmup _ range func args kwargs benchmarker benchmark_gpu lambda func args kwargs e dataclass frozen=True ExperimentConfig shape tuple int B Hq M Hkv N D attn_type str dtype torch dtype calculate_bwd_time bool cal_bandwidth bool backends list str max_autotune bool __post_init__ assert len shape == Shape must length B Hq M Hkv N D asdict Convert dataclass instance dictionary d = asdict Remove calculate_bwd_time ` cal_bandwidth ` key d pop calculate_bwd_time None d pop cal_bandwidth None d shape B Hq M Hkv N D = d pop shape d pop backends None d pop max_autotune False d dataclass frozen=True Times eager_time float compiled_time float dataclass frozen=True ExperimentResults fwd_time float bwd_time Optional float sparsity Optional float = None dataclass frozen=True Experiment config ExperimentConfig results dict str ExperimentResults backend - ExperimentResults asdict dict = config asdict dict = results dict dict generate_inputs batch_size int q_heads int q_sequence_length int kv_heads int kv_sequence_length int head_dim int dtype torch dtype device torch device requires_grad bool nested_tensors bool = False torch manual_seed q_shape = batch_size q_sequence_length q_heads head_dim kv_shape = batch_size kv_sequence_length kv_heads head_dim assert q_heads kv_heads == make_q = partial torch rand q_shape device=device dtype=dtype requires_grad=requires_grad make_kv = partial torch rand kv_shape device=device dtype=dtype requires_grad=requires_grad nested_tensors query = make_q view q_sequence_length batch_size q_heads head_dim transpose key = make_kv view batch_size kv_sequence_length kv_heads head_dim transpose value = make_kv view batch_size kv_sequence_length kv_heads head_dim transpose query = make_q view batch_size q_sequence_length q_heads head_dim transpose key = make_kv view batch_size kv_sequence_length kv_heads head_dim transpose value = make_kv view batch_size kv_sequence_length kv_heads head_dim transpose query key value generate_jagged_inputs shape tuple int query torch Tensor key torch Tensor value torch Tensor offsets torch Tensor B Hq M Hkv N D = shape offsets_to_lengths offsets torch Tensor device Union str torch device - torch tensor Converts list offsets list lengths Reverse op attn_gym masks document_mask length_to_offsets Args offsets A D tensor offsets device The device place output tensor lengths = offsets - offsets - lengths flatten_q = query transpose flatten start_dim= end_dim= flatten_k = key transpose flatten start_dim= end_dim= flatten_v = value transpose flatten start_dim= end_dim= q_list = flatten_q offsets i offsets i + clone detach query dtype i range len offsets - q = torch nested as_nested_tensor q_list device=query device k_list = flatten_k offsets i offsets i + clone detach key dtype i range len offsets - k = torch nested as_nested_tensor k_list device=key device v_list = flatten_v offsets i offsets i + clone detach value dtype i range len offsets - v = torch nested as_nested_tensor v_list device=value device q k v query_key_value_clones query torch Tensor key torch Tensor value torch Tensor dtype torch dtype = None Clones query key value tensors moves them specified dtype dtype None dtype = query dtype query_ref = query clone detach dtype requires_grad_ query requires_grad key_ref = key clone detach dtype requires_grad_ key requires_grad value_ref = value clone detach dtype requires_grad_ value requires_grad query_ref key_ref value_ref safe_backend SDPA run_single_backend_sdpa config ExperimentConfig query torch Tensor key torch Tensor value torch Tensor out_compile torch Tensor score_mod Callable &#124; None block_mask BlockMask &#124; None mask_kwargs backend str - ExperimentResults backend_context = get_backend_context backend backend_context _device = torch device cuda eager_sdpa = generate_eager_sdpa config attn_type config shape config dtype block_mask score_mod config attn_type == document_mask q_eager k_eager v_eager = generate_jagged_inputs config shape query key value mask_kwargs q_eager = q_eager transpose requires_grad_ query requires_grad k_eager = k_eager transpose requires_grad_ key requires_grad v_eager = v_eager transpose requires_grad_ value requires_grad q_eager k_eager v_eager = query_key_value_clones query key value eager_sdpa try out_eager = eager_sdpa query=q_eager key=k_eager value=v_eager except RuntimeError e print f SKIP SDPA Backend backend shape config shape \n\t\t\tError encountered e ExperimentResults fwd_time=float nan bwd_time=float nan config calculate_bwd_time None config attn_type document_mask flatten_o_eager = torch cat torch unbind out_eager transpose flatten_o_compile = out_compile transpose flatten start_dim= end_dim= torch testing assert_close flatten_o_eager flatten_o_compile atol= e- rtol= e- config attn_type rel alibi config dtype torch float torch bfloat rel has accuracy issue bit floats torch testing assert_close out_eager out_compile atol= e- rtol= e- eager_sdpa forward_eager_time = benchmark_torch_function_in_microseconds eager_sdpa query=q_eager key=k_eager value=v_eager forward_eager_time = float nan config calculate_bwd_time TODO debug backward pass njt eager_sdpa config attn_type = document_mask d_out = torch randn_like out_eager transpose transpose backward_eager_time = benchmark_torch_function_in_microseconds out_eager backward d_out retain_graph=True backward_eager_time = float nan ExperimentResults fwd_time=forward_eager_time bwd_time=backward_eager_time ExperimentResults fwd_time=forward_eager_time bwd_time=None safe_backend FlashAttention run_single_backend_FA config ExperimentConfig query torch Tensor key torch Tensor value torch Tensor out_compile torch Tensor score_mod Callable &#124; None block_mask BlockMask &#124; None mask_kwargs backend str - ExperimentResults assert backend fav fakv Generate callable specific backend backend fav FA = generate_FA_callable config attn_type config shape config dtype backend mask_kwargs backend == fakv FA = generate_FD_callable config attn_type config shape config dtype q_FA k_FA v_FA = query_key_value_clones query key value q_FA k_FA v_FA = q_FA transpose k_FA transpose v_FA transpose config attn_type == document_mask q_FA = q_FA flatten start_dim= end_dim= k_FA = k_FA flatten start_dim= end_dim= v_FA = v_FA flatten start_dim= end_dim= FA out_FA = FA q=q_FA k=k_FA v=v_FA config attn_type document_mask out_FA_updated = out_FA None out_FA_updated = out_FA config attn_type rel alibi config dtype torch float torch bfloat torch testing assert_close out_FA_updated out_compile transpose atol= e- rtol= e- FA forward_FA_time = benchmark_torch_function_in_microseconds FA q=q_FA k=k_FA v=v_FA forward_FA_time = float nan config calculate_bwd_time FA d_out = torch randn_like out_FA backward_FA_time = benchmark_torch_function_in_microseconds out_FA backward d_out retain_graph=True backward_FA_time = float nan ExperimentResults fwd_time=forward_FA_time bwd_time=backward_FA_time config calculate_bwd_time None safe_backend flex_attention return_dict=True run_single_experiment config ExperimentConfig dynamic=False - dict str ExperimentResults device = torch device cuda batch_size q_heads q_seq_len kv_heads kv_seq_len head_dim = config shape query key value = generate_inputs batch_size q_heads q_seq_len kv_heads kv_seq_len head_dim config dtype device requires_grad=config calculate_bwd_time nested_tensors=config attn_type == document_mask score_mod = generate_score_mod config attn_type config shape block_mask mask_kwargs = generate_block_mask config attn_type config shape kernel_options = get_kernel_options config attn_type config shape config max_autotune compiled_sdpa = torch compile flex_attention dynamic=dynamic mode= max-autotune-no-cudagraphs compiled_sdpa = torch compile flex_attention dynamic=dynamic out_compile = compiled_sdpa query=query key=key value=value score_mod=score_mod block_mask=block_mask enable_gqa=True kernel_options=kernel_options forward_compiled_time = benchmark_torch_function_in_microseconds compiled_sdpa query key value score_mod=score_mod block_mask=block_mask enable_gqa=True kernel_options=kernel_options results = backend config backends backend fav fakv results backend = run_single_backend_FA config query key value out_compile score_mod block_mask mask_kwargs backend sdpa also supports fav results backend = run_single_backend_sdpa config query key value out_compile score_mod block_mask mask_kwargs backend config calculate_bwd_time d_out = torch randn_like out_compile backward_compile_time = benchmark_torch_function_in_microseconds out_compile backward d_out retain_graph=True sparsity = block_mask sparsity block_mask None sparsity = sparsity config attn_type = document_mask results flex = ExperimentResults fwd_time=forward_compiled_time bwd_time=backward_compile_time config calculate_bwd_time None sparsity=sparsity results calculate_speedup results ExperimentResults baseline_results ExperimentResults type str - float type == fwd baseline_results fwd_time results fwd_time type == bwd assert results bwd_time None baseline_results bwd_time results bwd_time raise ValueError f Invalid type type calculate_bandwidth config ExperimentConfig results ExperimentResults type str - float B Hq M Hkv N D = config shape sparsity = results sparsity M == type == fwd batch_size q_heads q_seq_len kv_heads kv_seq_len head_dim = config shape query_size = batch_size q_heads q_seq_len head_dim torch finfo config dtype bits kv_size = batch_size kv_heads kv_seq_len head_dim torch finfo config dtype bits output_size = query_size total_size = query_size + kv_size - sparsity + output_size e In GB time_in_seconds = results fwd_time e total_size time_in_seconds e raise ValueError f Invalid type type calculate_tflops config ExperimentConfig results ExperimentResults - float B Hq M Hkv N D = config shape qk_flops = M N D softmax_flops = M N Not counting online softmax overhead o_flops = M D N Not counting split k overhead sparsity = results sparsity results sparsity None total_flops = B Hq qk_flops + softmax_flops + o_flops - sparsity total_flops results fwd_time e TFLOPs get_average_speedups results list Experiment type str backend str Calculate speedups speedups = calculate_speedup r results flex r results backend type r results Find indices max min speedups max_speedup_index = np nanargmax speedups min_speedup_index = np nanargmin speedups Get config dictionaries max_config_dict = results max_speedup_index config asdict min_config_dict = results min_speedup_index config asdict Create table data table_data = Type Average Speedup np nanmean speedups dict fromkeys max_config_dict Type Max Speedup speedups max_speedup_index max_config_dict Type Min Speedup speedups min_speedup_index min_config_dict table_data print_results results list Experiment save_path Optional str = None table_data = defaultdict list experiment results backends = experiment config backends + flex key value experiment asdict items key backends value fwd_time table_data f fwd_ key append float value fwd_time value bwd_time table_data f bwd_ key append float value bwd_time table_data key append value Calculate speedups backend results config backends fwd_speedups = calculate_speedup r results flex r results backend type= fwd r results table_data f fwd_speedup_flex_over_ backend = fwd_speedups results config calculate_bwd_time backend results config backends bwd_speedups = calculate_speedup r results flex r results backend type= bwd r results table_data f bwd_speedup_flex_over_ backend = bwd_speedups Calculate mem + computational throughput results config cal_bandwidth fwd_bandwidth = calculate_bandwidth r config r results flex type= fwd r results table_data fwd_mem_bw TB s = fwd_bandwidth fwd_tflops = calculate_tflops r config r results flex r results table_data TFlops s = fwd_tflops print tabulate table_data headers= keys tablefmt= github floatfmt= f backend results config backends np isnan table_data f fwd_speedup_flex_over_ backend all continue print \n print f FWD Speedup Flex over backend center = print \n average_data = get_average_speedups results type= fwd backend=backend print tabulate average_data headers= keys tablefmt= github floatfmt= f results config calculate_bwd_time print \n print f BWD Speedup Flex over backend center = print \n average_data = get_average_speedups results type= bwd backend=backend print tabulate average_data headers= keys tablefmt= github floatfmt= f save_path None open save_path w newline= csvfile writer = csv DictWriter csvfile fieldnames=table_data keys writer writeheader i range len next iter table_data values row = k v i k v table_data items writer writerow row print f \nResults saved save_path Generate score_mods BlockMasks softcap_value = dropout_p = generate_score_mod attn_type str shape tuple int - Callable &#124; None B Hq M Hkv N D = shape is_decoding = M == attn_gym mods generate_alibi_bias generate_tanh_softcap relative_bias score b h m n score + m - n head_bias score b h m n score + h function_dict = noop None causal None rel relative_bias head_bias head_bias alibi generate_alibi_bias Hq sliding_window None document_mask None prefix_lm None softcap generate_tanh_softcap softcap_value approx=True score_mod = function_dict attn_type is_decoding = M == is_decoding score_mod offset = torch tensor N cuda score_mod_w_offset score b h m n score_mod score b h m + offset n new_score_mod = score_mod_w_offset new_score_mod = score_mod new_score_mod sliding_window_size = prefix_length = generate_block_mask attn_type str shape tuple int B Hq M Hkv N D = shape is_decoding = M == causal b h m n m = n gen_offset off offset b h m n m + off = n offset attn_gym masks generate_doc_mask_mod generate_prefix_lm_mask generate_sliding_window attn_gym masks document_mask length_to_offsets generate_random_lengths total_length num_documents Initialize all lengths ensure each document has least one token lengths = num_documents remaining_length = total_length - num_documents Randomly distribute remaining length _ range remaining_length index = random randint num_documents - lengths index += lengths mask_mod_kwargs = assert attn_type = document_mask is_decoding attn_type == document_mask random seed lengths = generate_random_lengths N B B mask_mod_kwargs = dict offsets=length_to_offsets lengths cuda mask_mod_dict = noop None causal causal rel None head_bias None alibi causal sliding_window generate_sliding_window sliding_window_size document_mask partial generate_doc_mask_mod mask_mod=causal prefix_lm generate_prefix_lm_mask prefix_length softcap causal mask_mod = mask_mod_dict attn_type mask_mod_kwargs mask_mod = mask_mod mask_mod_kwargs is_decoding mask_mod cached_seq_len = torch tensor N cuda decoding_w_cached_seq_len b h m n mask_mod b h m + cached_seq_len n new_mask_mod = decoding_w_cached_seq_len new_mask_mod = mask_mod mask_shape = M N attn_type = document_mask M B N B compiled_block_mask = torch compile create_block_mask new_mask_mod block_mask = compiled_block_mask new_mask_mod mask_shape cuda block_mask = compiled_block_mask noop_mask mask_shape cuda block_mask mask_mod_kwargs get_kernel_options attn_type str shape tuple int B Hq M Hkv N D = shape is_decoding = M == kernel_opt_training_dict = noop None causal None rel None head_bias None alibi None sliding_window None document_mask BLOCK_N BLOCK_M fwd_num_warps fwd_num_stages BLOCK_M BLOCK_N BLOCK_M BLOCK_N torch cuda get_device_capability = D = None prefix_lm None softcap None get_default_split_k B int H int Mk int - int num_SM = torch cuda get_device_properties cuda multi_processor_count Heuristic number splits xformer bh = max B H NOTE Handle B h= case split_k = num_SM bh Each SM should least get one block split_k = max split_k split_k kernel_opt_decoding_dict = noop None causal SPLIT_KV get_default_split_k B Hkv N rel None head_bias None alibi SPLIT_KV get_default_split_k B Hkv N sliding_window None document_mask None prefix_lm None softcap SPLIT_KV get_default_split_k B Hkv N kernel_opt_decoding_dict attn_type is_decoding kernel_opt_training_dict attn_type Setup Backend get_backend_context backend str Returns context manager specified backend Args backend str The name backend use Valid options math efficient cudnn fav fav fakv og-eager Returns A context manager specified backend Raises ValueError If invalid backend specified backends = fav sdpa_kernel SDPBackend FLASH_ATTENTION cudnn sdpa_kernel SDPBackend CUDNN_ATTENTION math sdpa_kernel SDPBackend MATH efficient sdpa_kernel SDPBackend EFFICIENT_ATTENTION fav nullcontext fakv nullcontext og-eager nullcontext backend backends raise ValueError f Unknown backend backend Valid options join backends keys backends backend generate_FA_callable attn_type str shape tuple int dtype torch dtype backend str kwargs - Callable &#124; None dtype torch float torch bfloat None backend == fav try flash_attn flash_attn_interface flash_attn_func flash_attn_varlen_func except ImportError print Flash attention installed Please install run fav backend raise print Unknown backend + backend None B Hq M Hkv N D = shape FA_kwargs = attn_type == alibi h = torch arange Hq dtype=torch float device= cuda alibi_slopes = torch exp - h + Hq FA_kwargs = dict alibi_slopes=alibi_slopes attn_type == document_mask FA_kwargs cu_seqlens_q = kwargs offsets torch int FA_kwargs cu_seqlens_k = kwargs offsets torch int offsets_to_lengths offsets torch Tensor device Union str torch device - torch tensor lengths = offsets - offsets - lengths lengths = offsets_to_lengths kwargs offsets cpu max_length = torch max lengths FA_kwargs max_seqlen_q = max_length FA_kwargs max_seqlen_k = max_length FA_dict = noop partial flash_attn_func causal=False causal partial flash_attn_func causal=True rel None head_bias None alibi partial flash_attn_func causal=True FA_kwargs sliding_window partial flash_attn_func window_size= sliding_window_size causal=True document_mask partial flash_attn_varlen_func causal=True FA_kwargs prefix_lm None softcap partial flash_attn_func softcap=softcap_value causal=True FA_dict attn_type generate_FD_callable attn_type str shape tuple int dtype torch dtype - Callable &#124; None dtype torch float torch bfloat None try flash_attn flash_attn_with_kvcache except ImportError print Flash attention installed Please install run fakv backend raise B Hq M Hkv N D = shape assert M == flash_attn_with_kvcache_renamed q k v kwargs flash_attn_with_kvcache q k_cache=k v_cache=v kwargs FA_kwargs = attn_type == alibi h = torch arange Hq dtype=torch float device= cuda alibi_slopes = torch exp - h + Hq FA_kwargs = dict alibi_slopes=alibi_slopes FD_dict = noop partial flash_attn_with_kvcache_renamed causal=False causal partial flash_attn_with_kvcache_renamed cache_seqlens=N rel None head_bias None alibi partial flash_attn_with_kvcache_renamed cache_seqlens=N FA_kwargs sliding_window partial flash_attn_with_kvcache_renamed cache_seqlens=N window_size= sliding_window_size document_mask None prefix_lm None softcap partial flash_attn_with_kvcache_renamed softcap=softcap_value FD_dict attn_type generate_attn_mask_linear_score_mod shape tuple int block_mask BlockMask score_mod Callable dtype torch dtype B Hq M N = shape block_mask None score_mod None None b = torch arange B dtype=int device= cuda h = torch arange Hq dtype=int device= cuda m = torch arange M dtype=int device= cuda n = torch arange N dtype=int device= cuda score = torch zeros B Hq M N dtype=dtype device= cuda bias = score_mod score b None None None h None None None m None None None n None None None bool_mask = create_mask block_mask mask_mod B Hq M N device= cuda attn_mask = bias masked_fill bool_mask logical_not float -inf attn_mask dtype generate_eager_sdpa attn_type str shape tuple int dtype torch dtype block_mask BlockMask score_mod Callable &#124; None = None kwargs - Callable &#124; None B Hq M Hkv N D = shape is_decoding = M == attn_type == sliding_window attn_type == prefix_lm attn_mask = create_mask block_mask mask_mod M N device= cuda attn_type == rel attn_mask = generate_attn_mask_linear_score_mod M N block_mask score_mod dtype attn_type == head_bias h = torch arange Hq dtype=int device= cuda attn_mask = h None None None broadcast_to Hq M N dtype attn_type == alibi attn_mask = generate_attn_mask_linear_score_mod Hq M N block_mask score_mod dtype attn_mask = None sdpa_dict = noop partial F scaled_dot_product_attention is_causal=False enable_gqa= Hq = Hkv causal partial F scaled_dot_product_attention is_causal=True enable_gqa= Hq = Hkv rel partial F scaled_dot_product_attention is_causal=False enable_gqa= Hq = Hkv head_bias partial F scaled_dot_product_attention is_causal=False enable_gqa= Hq = Hkv alibi partial F scaled_dot_product_attention is_causal=False enable_gqa= Hq = Hkv sliding_window partial F scaled_dot_product_attention is_causal=False enable_gqa= Hq = Hkv document_mask partial F scaled_dot_product_attention is_causal=True enable_gqa= Hq = Hkv Hq == Hkv None prefix_lm partial F scaled_dot_product_attention is_causal=False enable_gqa= Hq = Hkv softcap None is_decoding attn_type == causal attn_mask = create_mask block_mask mask_mod M N device= cuda sdpa_dict causal = partial F scaled_dot_product_attention is_causal=False enable_gqa= Hq = Hkv partial sdpa_dict attn_type attn_mask=attn_mask sdpa_dict attn_type None generate_experiment_configs calculate_bwd bool dtype torch dtype batch_sizes list int num_heads list tuple int int seq_lens list int head_dims list int score_mods_str list str decoding bool kv_cache_size list int cal_bandwidth bool backends list str max_autotune bool - list ExperimentConfig assert calculate_bwd decoding Decoding does support backward decoding q_kv_seq_lens = i i seq_lens only testing query length == q_kv_seq_lens = i i i seq_lens only testing q_len == kv_len dtypes = dtype all_configs = bsz q_heads kv_heads q_seq_len kv_seq_len head_dim attn_type dtype itertools product kv_cache_size kv_cache_size batch_sizes num_heads q_kv_seq_lens head_dims score_mods_str dtypes kv_cache_size head_size_bytes = torch finfo dtype bits head_dim bsz = int bsz kv_heads kv_seq_len head_size_bytes bsz = continue assert q_heads kv_heads == all_configs append ExperimentConfig shape= bsz q_heads q_seq_len kv_heads kv_seq_len head_dim attn_type=attn_type dtype=dtype calculate_bwd_time=calculate_bwd cal_bandwidth=cal_bandwidth backends=backends max_autotune=max_autotune all_configs _output_json_for_dashboard experiments output_file benchmark_name= PyTorch operator microbenchmark Write result into JSON format PyTorch OSS dashboard The JSON format defined https github com pytorch pytorch wiki How-to-integrate-with-PyTorch-OSS-benchmark-database Args experiments List experiment results output_file Path output JSON file benchmark_name Name benchmark experiments math platform dataclasses asdict dataclass typing Any Optional Prepare headers records JSON output records = experiment experiments config = experiment config results_dict = experiment results This dict backend - ExperimentResults Process each backend result backend results results_dict items Skip backends run NaN results math isnan results fwd_time continue Extract data experiment test_name = f backend _ config attn_type _ input_config = f shape config shape dtype config dtype Determine mode based backward pass mode = training config calculate_bwd_time inference Extract dtype dtype = str config dtype split str config dtype str config dtype Determine device device = cuda Get device architecture device_arch = torch cuda get_device_name device == cuda platform processor device == cpu unknown Create dataclasses JSON structure dataclass BenchmarkInfo name str mode Optional str dtype str extra_info dict str Any dataclass ModelInfo name str type str origins list str extra_info dict str Any dataclass MetricInfo name str unit str benchmark_values list float target_value Optional float dataclass BenchmarkRecord benchmark BenchmarkInfo model ModelInfo metric MetricInfo Benchmark extra info benchmark_extra_info = input_config input_config device device arch device_arch operator_name backend attn_type config attn_type shape str config shape max_autotune config max_autotune Add record forward latency record_fwd_latency = BenchmarkRecord benchmark=BenchmarkInfo name=benchmark_name mode=mode dtype=dtype extra_info=benchmark_extra_info model=ModelInfo name=test_name + str config shape type= attention-benchmark origins= pytorch extra_info= operator_name backend attn_type config attn_type metric=MetricInfo name= forward latency unit= us benchmark_values= results fwd_time target_value=None records append asdict record_fwd_latency Add record forward memory bandwidth available config cal_bandwidth record_fwd_bandwidth = BenchmarkRecord benchmark=BenchmarkInfo name=benchmark_name mode=mode dtype=dtype extra_info=benchmark_extra_info model=ModelInfo name=test_name + str config shape type= attention-benchmark origins= pytorch extra_info= operator_name backend metric=MetricInfo name= memory bandwidth unit= TB s benchmark_values= calculate_bandwidth config results fwd target_value=None records append asdict record_fwd_bandwidth Add record forward TFLOPS available config cal_bandwidth record_fwd_tflops = BenchmarkRecord benchmark=BenchmarkInfo name=benchmark_name mode=mode dtype=dtype extra_info=benchmark_extra_info model=ModelInfo name=test_name + str config shape type= attention-benchmark origins= pytorch extra_info= operator_name backend metric=MetricInfo name= tflops unit= TFLOPS s benchmark_values= calculate_tflops config results target_value=None records append asdict record_fwd_tflops Add record backward latency available NaN config calculate_bwd_time results bwd_time None math isnan results bwd_time record_bwd_latency = BenchmarkRecord benchmark=BenchmarkInfo name=benchmark_name mode=mode dtype=dtype extra_info=benchmark_extra_info model=ModelInfo name=test_name + str config shape type= attention-benchmark origins= pytorch extra_info= operator_name backend metric=MetricInfo name= backward latency unit= us benchmark_values= results bwd_time target_value=None records append asdict record_bwd_latency Write all records output file open output_file w encoding= utf- f json dump records f indent= main dynamic bool = False calculate_bwd bool = False dtype DtypeString = bfloat b list int &#124; None = None nh list str &#124; None = None s list int &#124; None = None d list int &#124; None = None mods list AttentionType &#124; None = None backend list Backend &#124; None = None max_autotune bool = False decoding bool = False kv_size Optional list int = None throughput bool = True save_path Optional str = None output_json_for_dashboard Optional str = None benchmark_name str = PyTorch operator microbenchmark - None Run sweep over sizes score mods flex attention Usage Examples Use yml config file python score_mod py -- config basic_config yaml Use json config file python score_mod py -- config my_config json Generate config template python score_mod py -- print-config json my_config json For json config python score_mod py -- print-config yaml my_config yaml For yaml config Override config CLI args python score_mod py -- config my_config json -dtype float -- max-autotune Pure CLI usage python score_mod py -b -s -mods causal alibi -- backend efficient Args dynamic Runs dynamic shapes version compiled flex attention calculate_bwd Calculate backward pass times dtype Data type tensors bfloat float float b Batch sizes benchmark nh Number query key value heads format Hq Hkv s Sequence lengths benchmark d Head dimensions benchmark mods Score modifications noop causal rel head_bias alibi sliding_window document_mask prefix_lm softcap backend Backends attention computation math efficient cudnn fav fav fakv og-eager max_autotune Turn max-autotune optimization decoding Benchmark decoding mode query sequence length = kv_size Key value cache size MiB ignores batch size specified throughput Calculate kernel memory bandwidth computational throughput always True save_path Path save results CSV file output_json_for_dashboard Path save results JSON format PyTorch OSS dashboard benchmark_name Name benchmark dashboard output Convert dtype string torch dtype already converted torch isinstance dtype str dtype = getattr torch dtype Always calculate throughput throughput = True print Backend backend seed = np random seed seed torch manual_seed seed results = experiment_count config enumerate tqdm generate_experiment_configs calculate_bwd dtype b nh s d mods decoding kv_size throughput backend max_autotune start= results append Experiment config run_single_experiment config dynamic=dynamic Periodic memory cleanup every experiments experiment_count == cleanup_memory print_results results save_path Output JSON dashboard requested output_json_for_dashboard _output_json_for_dashboard results output_json_for_dashboard benchmark_name __name__ == __main__ Set up argument parser parser = argparse ArgumentParser description= Run sweep over sizes score mods flex attention parser add_argument -- config type=str help= Path JSON config file CLI args override config file values default=None parser add_argument -- dynamic action= store_true help= Runs dynamic shapes version compiled flex attention parser add_argument -- calculate-bwd action= store_true help= Calculate backward pass times parser add_argument -dtype type=str help= dtype default= bfloat parser add_argument -b type=int nargs= + help= batch sizes default= parser add_argument -nh type=heads_input_type nargs= + help= q-heads kv-heads default= parser add_argument -s type=int nargs= + help= sequence lengths default= parser add_argument -d type=int nargs= + help= head dims default= parser add_argument -mods type=str nargs= + help= score mods noop causal rel head_bias alibi sliding_window document_mask prefix_lm softcap default= noop causal alibi sliding_window parser add_argument -- max-autotune action= store_true help= Turn max-autotune parser add_argument -- decoding action= store_true help= Benchmark Decoding query sequence length = parser add_argument -- kv-size type=int nargs= + required=False help= key value size MiB Ignores -b batch size calculate batch size kv size instead when specified parser add_argument -- throughput action= store_true help= Calculate kernel memory bandwidth computational throughput parser add_argument -- save-path type=str help= Path save results JSON file optional default=None parser add_argument -- backend type=str nargs= + choices= math efficient cudnn fav fav fakv default= efficient help= Backend use attention computation parser add_argument -- output-json-for-dashboard type=str help= Path save results JSON format PyTorch OSS dashboard default=None parser add_argument -- benchmark-name type=str help= Name benchmark dashboard output default= PyTorch operator microbenchmark parser add_argument -- print-config type=str choices= json yaml help= Print default config template JSON YAML format exit default=None Parse arguments args = parser parse_args Handle -- print-config args print_config print_default_config args print_config sys exit Load merge config provided args config config = load_config_file args config Merge config CLI args CLI args take precedence json_args = argparse Namespace json_args __dict__ = config args = parser parse_args namespace=json_args Convert dtype string torch dtype only s still string isinstance args dtype str args dtype = getattr torch args dtype Remove config print_config args before passing main args_dict = vars args args_dict pop config None args_dict pop print_config None main args_dict