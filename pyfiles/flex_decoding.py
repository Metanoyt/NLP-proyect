mypy allow-untyped-defs Triton Implementation flex_attention Kernel short query length FlexDecoding typing Any sympy torch torch _inductor virtualized V ir ir FixedLayout FlexibleLayout lowering empty empty_strided lowerings runtime runtime_utils is_power_of_ next_power_of_ select_algorithm autotune_select_algorithm SymbolicGridFn TritonTemplate common create_indices_fake create_num_blocks_fake_generator freeze_irnodes get_fwd_subgraph_outputs load_flex_template maybe_realize set_head_dim_values aten = torch ops aten prims = torch ops prims _use_flex_decoding query kv_indices value kernel_options enable_gqa - bool Decide which kernel use true use flex decoding kernel Note Since number splits calculated based number batch head dims we need ensure batch head dims statically known Otherwise we just use main flex_attention kernel force_flex = kernel_options get FORCE_USE_FLEX_ATTENTION False short_query_length = V graph sizevars evaluate_expr sympy Lt query get_size - non_zero_length = V graph sizevars evaluate_expr sympy Gt query get_size - static_batch = isinstance query get_size int sympy Integer static_num_heads = isinstance query get_size int sympy Integer enable_gqa current flex decoding triton kernel grouped query heads same kv head handled same block So s hard support different kv num blocks grouped query heads We just fall back main flex_attention kernel where each query head handled separate block valid_block_mask_num_heads = V graph sizevars evaluate_expr sympy Eq kv_indices get_size valid_block_mask_num_heads = V graph sizevars evaluate_expr sympy Or sympy Eq kv_indices get_size sympy Eq kv_indices get_size query get_size Hq = query get_size Hkv = value get_size ratio = Hq Hkv pw_of_two = V graph sizevars guard_or_false sympy And sympy Gt ratio sympy Eq ratio ratio - force_flex kernel_options get OUTPUT_MAX False short_query_length static_batch static_num_heads non_zero_length valid_block_mask_num_heads pw_of_two SymbolicGridFn flex_decoding_grid batch_size kv_heads gqa_group_size n_keys d_model meta How kernel parallelized We create grid batch_size kv_heads SPLIT_KV Each block responsible iterating over blocks keys values calculating local output their tile keys values over all full length query groups SPLIT_KV blocks then combine their output produce final result batch_size kv_heads meta SPLIT_KV flex_decoding_template = TritonTemplate name= flex_decoding grid=flex_decoding_grid source=load_flex_template flex_decode + load_flex_template utilities + load_flex_template common get_split_k B int H int Mk int - int torch xpu is_available num_SM = torch xpu get_device_properties xpu gpu_subslice_count num_SM = torch cuda get_device_properties cuda multi_processor_count bh = max B H NOTE Handle B h= case assert isinstance bh int sympy Integer B H must concrete integers split_k = num_SM bh Each SM should least get one block TODO workload evening runtime splits fully masked out Before we have runtime workload evening assign splits per SM split_k = max split_k split_k create_flex_decoding_kernel args kwargs Flex decode lowering optimized small Q_LEN GQA packing query key value block_mask scale kernel_options score_mod_subgraph mask_mod_subgraph score_mod_other_buffers mask_mod_other_buffers = args _ q_length _ kv_length kv_num_blocks kv_indices full_kv_num_blocks full_kv_num_blocks full_kv_indices full_kv_indices _ q_num_blocks _ q_indices _ full_q_num_blocks _ full_q_indices _ SPARSE_Q_BLOCK_SIZE SPARSE_KV_BLOCK_SIZE _ = block_mask Bq Hq seq_len_q qk_head_dim = query get_size Bkv Hkv seq_len_kv v_head_dim = value get_size assert V graph sizevars evaluate_expr sympy Eq Bq Bkv &#124; sympy Eq Bkv f Bq Bkv must broadcastable Got Bq= Bq Bkv= Bkv B = Bq kernel_options = dict kernel_options Mark symbols custom kernel options static shapes add guards kernel_options = k V graph sizevars guard_int v isinstance v sympy Symbol v k v kernel_options items seq_q_divisible = V graph sizevars statically_known_true seq_len_q == seq_kv_divisible = V graph sizevars statically_known_true seq_len_kv == seq_q_divisible seq_kv_divisible kernel_options setdefault IS_DIVISIBLE True kernel_options setdefault IS_DIVISIBLE False Calculate GQA head sharing gqa_shared_heads = Hq Hkv is_power_of_ gqa_shared_heads raise ValueError Number shared query heads sharing same KV head must power kernel_options setdefault GQA_SHARED_HEADS gqa_shared_heads Determine there full blocks where we only need apply score_mod can skip mask_mod has_full_blocks = full_kv_num_blocks None kernel_options setdefault HAS_FULL_BLOCKS has_full_blocks has_full_blocks Create plackeholder full block list case empty full_kv_num_blocks full_kv_indices = empty device=query get_device _ range query key value kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices = maybe_realize query key value kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices score_mod_other_buffers = maybe_realize score_mod_other_buffers mask_mod_other_buffers = maybe_realize mask_mod_other_buffers freeze_irnodes score_mod_other_buffers freeze_irnodes mask_mod_other_buffers choices list Any = dtype = key get_dtype head_dim = V graph sizevars guard_int key get_size - configs = V choices get_flex_decode_configs head_dim dtype query get_device type TODO fix autotuning kernel_options setdefault SM_SCALE scale kernel_options setdefault SPLIT_KV get_split_k B Hkv seq_len_kv MAX_SPLIT_KV = kernel_options SPLIT_KV create config dependent intermediate buffers buf_ACC_shape = B MAX_SPLIT_KV Hq seq_len_q v_head_dim buf_ML_shape = buf_ACC_shape - buf_M = empty_strided buf_ML_shape None dtype=torch float The rowmax always stored fp regardless input dtype device=query get_device buf_L = empty_strided buf_ML_shape None dtype=torch float The intermediate sumexp always stored fp regardless input dtype device=query get_device layout_acc = FixedLayout query get_device torch float buf_ACC_shape FlexibleLayout contiguous_strides buf_ACC_shape set_head_dim_values kernel_options qk_head_dim v_head_dim V graph sizevars kernel_options setdefault BLOCK_M m V graph sizevars evaluate_expr sympy Lt query get_size - Always use BLOCK_M before Triton fix https github com triton-lang triton pull pin max next_power_of_ V graph sizevars size_hint seq_len_q fallback=torch _inductor config unbacked_symint_fallback type ignore arg-type gqa_shared_heads torch xpu is_available query = ir ExternKernel realize_input query stride_b stride_hq stride_seq_len_q stride_qk_head_dim = query get_stride Reshape query GQA B Hq Mq D - B Hkv G Mq D gqa_query_shape = B Hkv gqa_shared_heads seq_len_q qk_head_dim gqa_query_stride = stride_b stride_hq gqa_shared_heads stride_hq stride_seq_len_q stride_qk_head_dim query = lowerings aten as_strided query gqa_query_shape gqa_query_stride V graph sizevars check_leq seq_len_q gqa_shared_heads sympy Integer kernel_options BLOCK_M kernel_options setdefault SAFE_M_BOUNDARY seq_len_q gqa_shared_heads kernel_options BLOCK_M == TODO This feels sketchy kernel_options setdefault SAFE_N_BOUNDARY True Mark SPARSE_KV_BLOCK_SIZE static shapes add guards SPARSE_KV_BLOCK_SIZE = V graph sizevars guard_int SPARSE_KV_BLOCK_SIZE original_kernel_options = kernel_options copy Note we don t need pass captured buffers explicitly because they re implicitly added score_mod function We do need explicitly pass autotuning though Default config warp specialization num_consumer_groups num_buffers_warp_spec = conf configs SPARSE_KV_BLOCK_SIZE conf block_n = continue cur_kernel_options = original_kernel_options copy Remove prefix forward kernels options delete backward kernel options k list cur_kernel_options keys k startswith fwd_ v = cur_kernel_options pop k cur_kernel_options k = v k startswith bwd_ cur_kernel_options pop k Performance tuning cur_kernel_options setdefault BLOCK_N conf block_n cur_kernel_options setdefault SPARSE_KV_BLOCK_SIZE SPARSE_KV_BLOCK_SIZE cur_kernel_options setdefault num_warps conf num_warps cur_kernel_options setdefault num_stages conf num_stages cur_kernel_options get num_consumer_groups False cur_kernel_options setdefault num_consumer_groups num_consumer_groups cur_kernel_options setdefault num_buffers_warp_spec num_buffers_warp_spec Set default False cur_kernel_options setdefault USE_TMA False Add ROCm-specific parameters they exist config attrib kpack matrix_instr_nonkdim waves_per_eu hasattr conf attrib cur_kernel_options attrib = getattr conf attrib flex_decoding_template maybe_append_choice choices=choices input_nodes= query key value buf_M buf_L kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices layout=layout_acc subgraphs= score_mod_subgraph mask_mod_subgraph mutated_inputs= buf_M buf_L call_sizes=query get_size cur_kernel_options filtered_score_mod_buffers = buf buf score_mod_other_buffers isinstance buf sympy Symbol filtered_mask_mod_buffers = buf buf mask_mod_other_buffers isinstance buf sympy Symbol inputs_for_flex_decoding = pyrefly ignore unsupported-operation query key value buf_M buf_L kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices + filtered_score_mod_buffers + filtered_mask_mod_buffers input_gen_fns = create_num_blocks_fake_generator kv_indices create_indices_fake create_num_blocks_fake_generator full_kv_indices create_indices_fake buf_ACC = autotune_select_algorithm flex_decoding choices inputs_for_flex_decoding layout_acc input_gen_fns=input_gen_fns need subgraph inputs outputs analyze all symints used flex attention buf_ACC data data subgraph_inps = list score_mod_other_buffers + list mask_mod_other_buffers buf_ACC data data subgraph_outs = get_fwd_subgraph_outputs score_mod_subgraph mask_mod_subgraph Reduction g_M = lowerings aten max buf_M dim= keepdim=True See Note Handle fully masked out rows g_M Is global max among split kv blocks masked_rows = lowerings aten eq g_M -float inf adj_M = lowerings aten sub buf_M g_M adj_M = lowerings aten where masked_rows adj_M alpha = lowerings aten exp adj_M buf_L = lowerings aten mul buf_L alpha g_L = lowerings aten sum buf_L axis= masked_rows_squeezed = lowerings aten squeeze masked_rows dim= g_L = lowerings aten where masked_rows_squeezed g_L logsumexp = lowerings aten log g_L logsumexp = lowerings aten add logsumexp lowerings aten squeeze g_M dim= alpha_unseq = lowerings aten unsqueeze alpha buf_ACC = lowerings aten mul buf_ACC alpha_unseq output = lowerings aten sum buf_ACC axis= L_unseq = lowerings aten unsqueeze g_L output = lowerings aten div output L_unseq output = lowerings prims convert_element_type output query get_dtype output logsumexp