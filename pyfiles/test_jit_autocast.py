Owner s oncall jit torch torch cuda amp autocast typing Optional unittest torch testing _internal common_cuda TEST_CUDA torch testing _internal common_utils parse_cmd_line_args run_tests skipIfTorchDynamo torch testing FileCheck jit test_models MnistNet __name__ == __main__ The value GRAPH_EXECUTOR depends command line arguments so make sure they re parsed before instantiating tests parse_cmd_line_args test_jit JitTestCase TEST_BFLOAT = TEST_CUDA torch cuda is_bf _supported skipIfTorchDynamo Not TorchDynamo suitable test TestAutocast JitTestCase setUp common input tensors TEST_CUDA a_fp = torch rand dtype=torch float device= cuda b_fp = torch rand dtype=torch float device= cuda c_fp = torch rand dtype=torch float device= cuda d_fp = torch rand dtype=torch float device= cuda a_fp = torch rand dtype=torch float device= cuda b_fp = torch rand dtype=torch float device= cuda c_fp = torch rand dtype=torch float device= cuda d_fp = torch rand dtype=torch float device= cuda old_value = torch _C _jit_set_autocast_mode True super setUp tearDown torch _C _jit_set_autocast_mode old_value super tearDown unittest skipIf TEST_CUDA No cuda test_jit_generic_autocast torch jit script fn_cuda_autocast b autocast x = torch mm b y = torch sum x x y torch jit script fn_generic_autocast b torch amp autocast device_type= cuda x = torch mm b y = torch sum x x y assertEqual fn_cuda_autocast a_fp b_fp fn_generic_autocast a_fp b_fp unittest skipIf TEST_CUDA No cuda test_minimal torch jit script fn b autocast x = torch mm b y = torch sum x x y x y = fn a_fp b_fp assertEqual x dtype torch float assertEqual y dtype torch float unittest skipIf TEST_CUDA TEST_BFLOAT No cuda bfloat support test_linear_bf torch jit script fn b autocast dtype=torch bfloat x = torch mm b y = torch sum x x y x y = fn a_fp b_fp assertEqual x dtype torch bfloat assertEqual y dtype torch float unittest skipIf TEST_CUDA No cuda test_minimal_cpu torch jit script fn b autocast torch mm b result = fn a_fp cpu b_fp cpu assertEqual result dtype torch float unittest skipIf TEST_CUDA No cuda test_minimal_off torch jit script fn b autocast enabled=False torch mm b result = fn a_fp b_fp assertEqual result dtype torch float unittest skipIf TEST_CUDA No cuda test_runtime_autocast_state torch jit script fn b use_amp bool autocast enabled=use_amp torch mm b runtime values autocast enable argument supported assertRaises RuntimeError fn a_fp b_fp True unittest skipIf TEST_CUDA No cuda test_runtime_autocast_state_expr torch jit script fn b autocast enabled=bool item torch mm b runtime values autocast enable argument supported assertRaises RuntimeError fn a_fp b_fp unittest skipIf TEST_CUDA No cuda test_explicit_casts torch jit script fn b c d autocast e = torch mm double b double float f = torch mm c d double g = torch mm c double f e f g e f g = fn a_fp b_fp c_fp d_fp assertEqual e dtype torch float assertEqual f dtype torch float assertEqual g dtype torch float multiple uses same input value unittest skipIf TEST_CUDA No cuda test_duplicate_inputs torch jit script fn b autocast e = torch mm f = torch mm e e e f e f = fn a_fp b_fp assertEqual e dtype torch float assertEqual f dtype torch float unittest skipIf TEST_CUDA No cuda test_fp _policy torch jit script fn autocast enabled=True torch log result = fn a_fp assertEqual result dtype torch float unittest skipIf TEST_CUDA No cuda test_fp _policy_with_fp torch jit script fn autocast enabled=True torch log fp policy should narrow fp fp result = fn a_fp double assertEqual result dtype torch float unittest skipIf TEST_CUDA No cuda test_promote_policy torch jit script fn b c d autocast e = torch mm b f = torch addcmul e c d value= e f e f = fn a_fp b_fp c_fp d_fp assertEqual e dtype torch float assertEqual f dtype torch float unittest skipIf TEST_CUDA No cuda test_promote_policy_fp torch jit script fn b autocast enabled=True torch addcmul b value= result = fn a_fp double b_fp double assertEqual result dtype torch float unittest skipIf TEST_CUDA No cuda test_fp _set_opt_dtype_policy torch jit script fn b c d dtype Optional int autocast enabled=True x = torch softmax y = torch softmax b None z = torch softmax c torch float w = torch softmax d dtype x y z w x y z w = fn a_fp b_fp c_fp d_fp None assertEqual x dtype torch float assertEqual y dtype torch float assertEqual z dtype torch float assertEqual w dtype torch float unittest skipIf TEST_CUDA No cuda test_fp _set_opt_dtype_policy_fp torch jit script fn b c d dtype Optional int autocast enabled=True x = torch softmax y = torch softmax b None z = torch softmax c torch float w = torch softmax d dtype x y z w x y z w = fn a_fp double b_fp double c_fp double d_fp double None assertEqual x dtype torch float assertEqual y dtype torch float assertEqual z dtype torch float assertEqual w dtype torch float unittest skipIf True broken due lack type propagation unittest skipIf TEST_CUDA No cuda test_control_flow torch jit script fn b c d autocast e = torch mm b x = e = torch mm c d x = f = torch mm d e x e f e f = fn a_fp b_fp c_fp d_fp assertEqual e dtype torch float assertEqual f dtype torch float works find regular Python creates delicate situation TorchScript where types consistent across then branches unittest skipIf TEST_CUDA No cuda test_divergent_types torch jit script fn b c d autocast e = torch mm b f = torch mm b float e = torch mm c d float f = torch mm b torch mm e float f float result = fn a_fp b_fp c_fp d_fp assertEqual result dtype torch float another more complex case divergent types unittest skipIf TEST_CUDA No cuda test_divergent_autocast torch jit script fn b c d autocast_on = autocast enabled=True autocast_off = autocast enabled=False autocast_on e = torch mm b autocast_off e = torch mm c d torch mm e e fn a_fp b_fp c_fp d_fp unittest skipIf TEST_CUDA No cuda test_conditional_autocast torch jit script fn b autocast_on = autocast enabled=True autocast_off = autocast enabled=False autocast_on autocast_off torch mm b conditional autocast expressions supported assertRaises RuntimeError fn a_fp b_fp unittest skipIf TEST_CUDA No cuda test_nested_autocast torch jit script fn b c d autocast enabled=False e = torch mm b autocast enabled=True f = torch mm e c autocast enabled=False g = torch mm e d e f g e f g = fn a_fp b_fp c_fp d_fp assertEqual e dtype torch float assertEqual f dtype torch float assertEqual g dtype torch float unittest skipIf TEST_CUDA No cuda test_implicitly_nested_autocast torch jit script fn b autocast enabled=False autocast enabled=True torch mm b result = fn a_fp b_fp assertEqual result dtype torch float unittest skipIf TEST_CUDA No cuda test_reused_autocast torch jit script fn b c d autocast_instance = autocast enabled=True autocast_instance e = torch mm b autocast_instance e = torch mm c d f = torch mm d e g = torch mm e f e f g e f g = fn a_fp b_fp c_fp d_fp assertEqual e dtype torch float assertEqual f dtype torch float assertEqual g dtype torch float TODO fix enable test we could technically fix really worth unittest skipIf True unsupported autocast syntax test_reused_autocast_expr torch jit script fn b c d autocast enabled=True autocast_instance e = torch mm b autocast_instance e = torch mm c d f = torch mm d e g = torch mm e f e f g e f g = fn a_fp b_fp c_fp d_fp assertEqual e dtype torch float assertEqual f dtype torch float assertEqual g dtype torch float unittest skipIf TEST_CUDA No cuda test_callees helper b torch mm b torch jit script fn b autocast enabled=True tmp = helper b tmp = helper tmp tmp tmp = helper tmp tmp tmp = helper tmp tmp helper tmp b result = fn a_fp b_fp assertEqual result dtype torch float unittest skipIf TEST_CUDA No cuda test_callees_with_autocast_on helper b autocast enabled=True torch mm b torch jit script fn b autocast enabled=False helper b result = fn a_fp b_fp assertEqual result dtype torch float unittest skipIf TEST_CUDA No cuda test_callees_with_autocast_off helper b autocast enabled=False torch mm b torch jit script fn b autocast enabled=True helper b result = fn a_fp b_fp assertEqual result dtype torch float scripting inside eager autocast unittest skipIf TEST_CUDA No cuda test_eager_and_script torch jit script fn b torch mm b i range use_autocast = i == expected_dtype = torch float use_autocast torch float autocast enabled=use_autocast result = fn a_fp b_fp assertEqual result dtype expected_dtype traced inside scripting unittest skipIf TEST_CUDA No cuda test_script_and_tracing helper b torch mm b traced = torch jit trace helper a_fp a_fp torch jit script fn b autocast enabled=True traced b result = fn a_fp b_fp assertEqual result dtype torch float traced autocast inside scripting unittest skipIf True autocast False ignored inside traced functions unittest skipIf TEST_CUDA No cuda test_script_and_tracing_with_autocast helper b autocast enabled=False torch mm b traced = torch jit trace helper a_fp a_fp torch jit script fn b autocast enabled=True traced b result = fn a_fp b_fp assertEqual result dtype torch float scripted called traced unittest skipIf TEST_CUDA No cuda test_tracing_and_script torch jit script fn b autocast torch mm b traced b fn b traced = torch jit trace traced a_fp b_fp result = traced a_fp b_fp assertEqual result dtype torch float scripted called traced autocast unittest skipIf True scripted called traced TorchScript yet working unittest skipIf TEST_CUDA No cuda test_tracing_with_autocast_and_script torch jit script fn b torch mm b traced b autocast enabled=True fn b traced = torch jit trace traced a_fp b_fp result = traced a_fp b_fp assertEqual result dtype torch float unittest skipIf TEST_CUDA No cuda test_script_module TestModule torch nn Module __init__ N M super __init__ weight = torch nn Parameter torch rand N M dtype=torch float linear = torch nn Linear N M float forward input autocast enabled=True output = weight mv input output = linear output output scripted_module = torch jit script TestModule cuda input = torch rand dtype=torch float device= cuda result = scripted_module input assertEqual result dtype torch float unittest skipIf True autocast decorators supported unittest skipIf TEST_CUDA No cuda test_autocast_decorator torch jit script autocast enabled=True fn b torch mm b result = fn a_fp b_fp assertEqual result dtype torch float equivalent running scripted functions inside autocast see also test_eager_and_script unittest skipIf TEST_CUDA No cuda test_autocast_decorator_outside_jit autocast enabled=True torch jit script fn b torch mm b result = fn a_fp b_fp assertEqual result dtype torch float unittest skipIf TEST_CUDA No cuda test_inplace torch jit script fn b c autocast enabled=True x = torch addmm b c y = torch addmm b c out=a z = addmm_ b c x y z x y z = fn a_fp b_fp c_fp assertEqual x dtype torch float assertEqual y dtype torch float assertEqual z dtype torch float _test_autocast func cast_op args jit_func = torch jit script func o = func args jit_o = jit_func args cast_op None FileCheck check cast_op run jit_func graph_for args o o zip o jit_o assertEqual o dtype o dtype unittest skipIf TEST_CUDA No cuda test_autocast_api t_autocast_cpu x y torch autocast cpu dtype=torch bfloat torch mm x y t_autocast_cuda x y torch autocast cuda dtype=torch half torch mm x y t_cuda_amp_autocast x y torch autocast device_type= cuda torch mm x y t_cpu_amp_autocast x y torch autocast device_type= cpu torch mm x y x = torch randn device= cuda dtype=torch float y = torch randn device= cuda dtype=torch float _test_autocast t_autocast_cpu aten _autocast_to_reduced_precision x y _test_autocast t_autocast_cuda aten _autocast_to_reduced_precision x y _test_autocast t_cuda_amp_autocast aten _autocast_to_reduced_precision x y _test_autocast t_cpu_amp_autocast aten _autocast_to_reduced_precision x y unittest skipIf True we need provide dtype argument moment unittest skipIf TEST_CUDA No cuda test_autocast_api_not_supported t_autocast_cpu x y no dtype provided currently supported torch autocast cpu torch mm x y t_autocast_cuda x y no dtype provided currently supported torch autocast cuda torch mm x y x = torch randn device= cuda dtype=torch float y = torch randn device= cuda dtype=torch float _test_autocast t_autocast_cpu aten _autocast_to_reduced_precision x y _test_autocast t_autocast_cuda aten _autocast_to_reduced_precision x y unittest skipIf TEST_CUDA No cuda test_autocast_mixed_dtypes t cpu cpu cuda cuda torch autocast cpu torch bfloat torch autocast cuda torch float cpu_o = torch mm cpu cpu cuda_o = torch mm cuda cuda cpu_o cuda_o torch jit script t cpu = torch randn device= cpu dtype=torch float cpu = torch randn device= cpu dtype=torch float cuda = torch randn device= cuda dtype=torch float cuda = torch randn device= cuda dtype=torch float _test_autocast t aten _autocast_to_reduced_precision cpu cpu cuda cuda unittest skipIf TEST_CUDA No cuda test_jit_executor_under_autocast t cpu cpu cuda cuda cpu_o = torch mm cpu cpu cuda_o = torch mm cuda cuda cpu_o cuda_o torch jit script t cpu = torch randn device= cpu dtype=torch float cpu = torch randn device= cpu dtype=torch float cuda = torch randn device= cuda dtype=torch float cuda = torch randn device= cuda dtype=torch float torch autocast cpu torch bfloat torch autocast cuda torch float _test_autocast t aten _autocast_to_reduced_precision cpu cpu cuda cuda torch autocast cpu torch bfloat _test_autocast t aten _autocast_to_reduced_precision cpu cpu cuda cuda torch autocast cuda torch float _test_autocast t aten _autocast_to_reduced_precision cpu cpu cuda cuda no cast op should observed when executing outside autocast context _test_autocast t None cpu cpu cuda cuda unittest skipIf TEST_CUDA No cuda test_autocast_autodiff t t t o = torch mm t t o relu jit_t = torch jit script t t = torch randn device= cuda dtype=torch float requires_grad_ t = torch randn device= cuda dtype=torch float requires_grad_ run optimization _ range torch autocast cuda torch float jit_o = jit_t t t jit_o sum backward t grad = None t grad = None ref_t = t detach requires_grad_ ref_t = t detach requires_grad_ torch autocast cuda torch float o = t ref_t ref_t jit_o = jit_t t t jit_o sum backward o sum backward assertEqual o jit_o assertEqual t grad ref_t grad assertEqual t grad ref_t grad assertEqual o dtype jit_o dtype assertEqual t grad dtype ref_t grad dtype assertEqual t grad dtype ref_t grad dtype unittest skipIf TEST_CUDA No cuda test_jit_call_method_under_autocast torch jit interface Iface torch nn Module forward x y - torch Tensor pass Impl Iface forward x y torch mm x y Thing torch nn Module impl Iface forward x y torch autocast device_type= cuda = torch mm x y b = impl forward x b scripted_impl = torch jit script Impl thing = Thing thing impl = scripted_impl scripted_thing = torch jit script thing x = torch rand y = torch rand make sure doesn t throw error torch autocast device_type= cuda ans = scripted_thing forward x y assertEqual torch mm torch mm x y x ans sanity check isn t supported currently when global autocasting isn t enabled assertRaises RuntimeError lambda scripted_thing forward x y unittest skipIf TEST_CUDA No cuda test_jit_freeze_autocast_basic TestModule torch nn Module forward x y torch autocast device_type= cuda torch mm x y x = torch rand dtype=torch float cuda y = torch rand dtype=torch float cuda mod = TestModule eval sanity check _test_autocast mod aten _autocast_to_reduced_precision x y frozen_mod = torch jit freeze torch jit script mod eval FileCheck check_count aten _autocast_to_reduced_precision True run frozen_mod graph make sure runtime pass doesn t duplicate autocast nodes frozen_mod x y optimized_graph = frozen_mod graph_for x y FileCheck check_count aten _autocast_to_reduced_precision True run optimized_graph unittest skipIf TEST_CUDA No cuda test_jit_freeze_autocast_constants TestModule torch nn Module __init__ - None super __init__ x = torch rand dtype=torch float cuda forward y torch autocast device_type= cuda torch mm x y y = torch rand dtype=torch float cuda mod = TestModule eval frozen_mod = torch jit freeze torch jit script mod eval freezing should pre-cast constant x remove one autocast call FileCheck check_count aten _autocast_to_reduced_precision True run frozen_mod graph runtime autocasting pass will re-insert second autocast call constant propagation will merge constant s casting frozen_mod y optimized_graph = frozen_mod graph_for y FileCheck check_count aten _autocast_to_reduced_precision True run optimized_graph unittest skipIf TEST_CUDA CPU-only test test_jit_autocast_softmax_cpu fn x torch autocast device_type= cpu torch nn functional softmax x dim= fn_s = torch jit script fn x = torch rand dtype=torch bfloat fn_s x y = fn_s x assertTrue y dtype == torch bfloat unittest skipIf TEST_CUDA No cuda test_jit_autocast_softmax_gpu fn x torch autocast device_type= cuda torch nn functional softmax x dim= fn_s = torch jit script fn x = torch rand dtype=torch half cuda fn_s x y = fn_s x assertTrue y dtype == torch float test_ignore_amp torch jit script foo x torch mm x x inp = torch rand dtype=torch float foo _set_ignore_amp True torch autocast device_type= cpu foo inp foo inp g = torch jit last_executed_optimized_graph FileCheck check_not _autocast_to_reduced run g convbn torch nn Module __init__ bias_enabled=True super __init__ conv = torch nn Conv d stride= bias=bias_enabled bn = torch nn BatchNorm d forward x bn conv x skipIfTorchDynamo Not TorchDynamo suitable test TestJitTraceAutocast JitTestCase setUp super setUp previous_default_dtype = torch get_default_dtype torch set_default_dtype torch float models = MnistNet convbn bias_enabled=True convbn bias_enabled=False inputs = torch randn device= cpu torch randn device= cpu torch randn device= cpu previous_jit_autocast_pass = torch _C _jit_set_autocast_mode False tearDown torch _C _jit_set_autocast_mode previous_jit_autocast_pass torch set_default_dtype previous_default_dtype super tearDown test_generate_autocast_jit_trace_model test_generate_autocast_jit_trace_model model x model eval torch autocast device_type= cpu cache_enabled=False torch no_grad traced_model = torch jit trace model x traced_model = torch jit freeze traced_model i range models __len__ test_generate_autocast_jit_trace_model models i inputs i test_nchw_autocast_jit_trace_model test_nchw_autocast_jit_trace_model model x model eval torch autocast device_type= cpu cache_enabled=False torch no_grad traced_model = torch jit trace model x traced_model = torch jit freeze traced_model torch no_grad y = traced_model x clone torch autocast device_type= cpu torch no_grad y = model x clone torch testing assert_close y double y double rtol= e- atol= e- i range models __len__ test_nchw_autocast_jit_trace_model models i inputs i test_nhwc_autocast_jit_trace_model test_nhwc_autocast_jit_trace_model model x model = model memory_format=torch channels_last model eval torch autocast device_type= cpu cache_enabled=False torch no_grad traced_model = torch jit trace model x memory_format=torch channels_last traced_model = torch jit freeze traced_model torch no_grad y = traced_model x clone memory_format=torch channels_last torch autocast device_type= cpu torch no_grad y = model x clone memory_format=torch channels_last torch testing assert_close y double y double rtol= e- atol= e- i range models __len__ inputs i size __len__ == NHWC D case support yet continue test_nhwc_autocast_jit_trace_model models i inputs i test_cat_promote TestModel torch nn Module forward b torch cat b torch jit fuser none In testcase we will check whether cat has done promotion AMP mixed dtype inputs To avoid fusion group TE we will disable fuser here jit_freeze_or_not False True test_model = TestModel eval torch autocast device_type= cpu cache_enabled=False dtype=torch bfloat torch no_grad = torch rand b = torch rand dtype=torch bfloat c = test_model b traced = torch jit trace test_model b jit_freeze_or_not traced = torch jit freeze traced _ range c = traced b assertTrue c dtype torch float assertTrue c dtype torch float traced_graph = traced graph_for b assertTrue any n kind == aten n traced_graph nodes test_script_autocast_cpu fn x torch is_autocast_cpu_enabled x relu x sin fn_s = torch jit script fn x = torch rand - torch autocast device_type= cpu assertEqual fn_s x fn x torch autocast device_type= cpu enabled=True assertEqual fn_s x fn x assertTrue any is_autocast_cpu_enabled x kind x fn_s graph nodes unittest skipIf TEST_CUDA No cuda test_script_autocast_cuda fn x torch is_autocast_enabled x relu x sin fn_s = torch jit script fn x = torch rand - torch autocast device_type= cpu assertEqual fn_s x fn x torch autocast device_type= cuda enabled=True assertEqual fn_s x fn x assertTrue any is_autocast_enabled x kind x fn_s graph nodes test_scripted_aliasing torch is_autocast_enabled should able move inside autocast context fn x torch is_autocast_enabled y = True y = False torch autocast device_type= cuda enabled=True z = x relu y z fn_s = torch jit script fn graph = fn_s graph aliasdb = graph alias_db is_enabled_nodes = graph findAllNodes aten is_autocast_enabled enter_nodes = graph findAllNodes prim Enter assertEqual len is_enabled_nodes assertEqual len enter_nodes assertFalse aliasdb move_after_topologically_valid is_enabled_nodes enter_nodes test_script_autocast_enable_and_check fn x y - tuple torch Tensor bool torch Tensor bool torch Tensor bool b = torch is_autocast_cpu_enabled v = torch mm x y torch autocast device_type= cpu enabled=True b = torch is_autocast_cpu_enabled v = torch mm x y torch autocast device_type= cpu enabled=False b = torch is_autocast_cpu_enabled v = torch mm x y v b v b v b bx = is_autocast_cpu_enabled result should False iff vx = mm x y dtype float check_fn_results arr v b v b v b = arr assertTrue v dtype == torch float = b assertTrue v dtype == torch float = b assertTrue v dtype == torch float = b x = torch rand dtype=torch float y = torch rand dtype=torch float fn_s = torch jit script fn torch autocast device_type= cpu enabled=False check_fn_results fn x y check_fn_results fn_s x y torch autocast device_type= cpu enabled=True check_fn_results fn x y check_fn_results fn_s x y __name__ == __main__ run_tests