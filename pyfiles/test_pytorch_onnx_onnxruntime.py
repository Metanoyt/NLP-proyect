Owner s module onnx ruff noqa F __future__ annotations functools io itertools os unittest collections OrderedDict typing Optional Union numpy np onnx onnx_test_common parameterized torchvision model_defs lstm_flattening_result rnn_model_with_packed_sequence word_language_model pytorch_test_common BATCH_SIZE RNN_BATCH_SIZE RNN_HIDDEN_SIZE RNN_INPUT_SIZE RNN_SEQUENCE_LENGTH skipDtypeChecking skipIfQuantizationBackendQNNPack skipIfUnsupportedMaxOpsetVersion skipIfUnsupportedMinOpsetVersion skipIfUnsupportedOpsetVersion skipScriptTest skipShapeChecking skipTraceTest torch torch Tensor torch nn utils rnn rnn_utils torch onnx errors torch onnx _internal torchscript_exporter verification torch onnx _internal torchscript_exporter _type_utils JitScalarType torch testing _internal common_utils torch testing _internal common_utils skipIfNoLapack _init_test_generalized_rcnn_transform min_size = max_size = image_mean = image_std = transform = torchvision models detection transform GeneralizedRCNNTransform min_size max_size image_mean image_std transform _init_test_rpn anchor_sizes = aspect_ratios = len anchor_sizes rpn_anchor_generator = torchvision models detection rpn AnchorGenerator anchor_sizes aspect_ratios out_channels = rpn_head = torchvision models detection rpn RPNHead out_channels rpn_anchor_generator num_anchors_per_location rpn_fg_iou_thresh = rpn_bg_iou_thresh = rpn_batch_size_per_image = rpn_positive_fraction = rpn_pre_nms_top_n = dict training= testing= rpn_post_nms_top_n = dict training= testing= rpn_nms_thresh = rpn_score_thresh = rpn = torchvision models detection rpn RegionProposalNetwork rpn_anchor_generator rpn_head rpn_fg_iou_thresh rpn_bg_iou_thresh rpn_batch_size_per_image rpn_positive_fraction rpn_pre_nms_top_n rpn_post_nms_top_n rpn_nms_thresh score_thresh=rpn_score_thresh rpn _construct_tensor_for_quantization_test shape tuple int offset Optional Union int float = None max_val Optional Union int float = None - Tensor Helper function generate weights test inputs deterministic way Due difference implementation details between PyTorch ONNXRuntime randomly generated test data quantization tests can flaky To help stablize test helper function used generate weights test inputs deterministic way Args shape Tuple int Shape tensor construct offset Optional Union int float Offset added generated tensor max_val Optional Union int float If any element within tensor has larger absolute value than max_val tensor will scaled max_val tensor abs max This step done after applying offset tensor = torch arange np prod shape dtype=torch float view shape offset None tensor = tensor + offset max_val None tensor abs max max_val tensor = tensor max_val tensor abs max tensor _parameterized_class_attrs_and_values min_opset_version int max_opset_version int attrs = opset_version is_script keep_initializers_as_inputs input_values = input_values extend itertools product True False True Valid opset versions defined torch onnx _constants py Versions intentionally set statically affected changes elsewhere min_opset_version raise ValueError min_opset_version must = input_values extend itertools product range min_opset_version max_opset_version + True False True False attrs attrs input_values input_values _parametrize_rnn_args arg_name options = layers unilayer trilayer bidirectional True bidirectional False forward initial_state True with_initial_state False no_initial_state packed_sequence without_sequence_lengths with_variable_length_sequences with_batch_first_sequence_lengths dropout with_dropout without_dropout arg_str arg_name arg_values options arg_name keys name_fn lambda val options arg_name val parameterized parameterized_class _parameterized_class_attrs_and_values onnx_test_common MIN_ONNX_OPSET_VERSION onnx_test_common MAX_ONNX_OPSET_VERSION class_name_func=onnx_test_common parameterize_class_name common_utils instantiate_parametrized_tests TestONNXRuntime onnx_test_common _TestONNXRuntime test_fuse_conv_bn d Fuse torch nn Module __init__ - None super __init__ conv = torch nn Conv d stride= bn = torch nn BatchNorm d forward x out = conv x bn out model = Fuse x = torch randn requires_grad=True run_test model x test_fuse_conv_bn d Fuse torch nn Module __init__ - None super __init__ conv = torch nn Conv d kernel_size= stride= padding= bias=False bn = torch nn BatchNorm d forward x out = conv x bn out model = Fuse x = torch randn requires_grad=True run_test model x test_fuse_conv_bn d Fuse torch nn Module __init__ - None super __init__ conv = torch nn Conv d stride= padding= bias=False bn = torch nn BatchNorm d forward x out = conv x bn out model = Fuse x = torch randn requires_grad=True run_test model x rtol= e- atol= e- test_fuse_conv_in_block Fuse torch nn Module __init__ - None super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= dilation= bn = torch nn BatchNorm d forward x results_available = True x sum - results_available = False results_available x = conv x x = bn x x model = Fuse x = torch randn requires_grad=True run_test torch jit script model x input_names= x dynamic_axes= x rtol= e- atol= e- test_conv_tbc torch nn modules utils _single ConvTBC torch nn Module __init__ in_channels out_channels kernel_size padding= super __init__ in_channels = in_channels out_channels = out_channels kernel_size = _single kernel_size padding = _single padding weight = torch nn Parameter Tensor kernel_size in_channels out_channels bias = torch nn Parameter Tensor out_channels reset_parameters reset_parameters torch nn init xavier_normal_ weight torch nn init zeros_ bias conv_tbc input torch conv_tbc input contiguous weight bias padding forward input conv_tbc input in_channels = out_channels = kernel_size = model = ConvTBC in_channels out_channels kernel_size padding= x = torch randn in_channels requires_grad=True run_test model x atol= e- test_reshape_constant_fold Reshape torch nn Module __init__ super __init__ weight = torch nn Buffer torch ones forward x scale_ = weight reshape - x scale_ x = torch randn run_test Reshape x rtol= e- atol= e- run_word_language_model model_name ntokens = emsize = nhid = nlayers = dropout = tied = False batchsize = model_name == GRU model = word_language_model RNNModelWithTensorHidden model_name ntokens emsize nhid nlayers dropout tied batchsize model_name == LSTM model = word_language_model RNNModelWithTupleHidden model_name ntokens emsize nhid nlayers dropout tied batchsize model = word_language_model RNNModel model_name ntokens emsize nhid nlayers dropout tied batchsize x = torch arange ntokens long view - batchsize Only support CPU version since tracer working GPU RNN run_test model x model hidden get_image rel_path str size tuple int int - Tensor PIL Image torchvision transforms data_dir = os path join os path dirname __file__ assets path = os path join data_dir rel_path split image = Image open path convert RGB resize size Image BILINEAR transforms ToTensor image get_test_images - tuple list Tensor list Tensor get_image grace_hopper_ x jpg get_image rgb_pytorch png test_paste_mask_in_image masks = torch rand boxes = torch rand boxes += torch rand boxes = o_im_s = torchvision models detection roi_heads paste_masks_in_image out = paste_masks_in_image masks boxes o_im_s jit_trace = torch jit trace paste_masks_in_image masks boxes torch tensor o_im_s torch tensor o_im_s out_trace = jit_trace masks boxes torch tensor o_im_s torch tensor o_im_s assert torch all out eq out_trace masks = torch rand boxes = torch rand boxes += torch rand boxes = o_im_s = torchvision models detection roi_heads paste_masks_in_image out = paste_masks_in_image masks boxes o_im_s out_trace = jit_trace masks boxes torch tensor o_im_s torch tensor o_im_s assert torch all out eq out_trace test_heatmaps_to_keypoints maps = torch rand rois = torch rand torchvision models detection roi_heads heatmaps_to_keypoints out = heatmaps_to_keypoints maps rois jit_trace = torch jit trace heatmaps_to_keypoints maps rois out_trace = jit_trace maps rois assert torch all out eq out_trace assert torch all out eq out_trace maps = torch rand rois = torch rand torchvision models detection roi_heads heatmaps_to_keypoints out = heatmaps_to_keypoints maps rois out_trace = jit_trace maps rois assert torch all out eq out_trace assert torch all out eq out_trace test_word_language_model_RNN_TANH run_word_language_model RNN_TANH test_word_language_model_RNN_RELU run_word_language_model RNN_RELU skipScriptTest scripting prim unchecked_cast prim setattr test_word_language_model_LSTM run_word_language_model LSTM test_word_language_model_GRU run_word_language_model GRU test_index_ d MyModel torch nn Module forward input input m = torch randn run_test MyModel m test_index_ d_ dimslice MyModel torch nn Module forward input input m = torch randn run_test MyModel m test_index_ d_sliceint MyModel torch nn Module forward input input m = torch randn run_test MyModel m test_index_ d_neg_slice MyModel torch nn Module forward input input - m = torch randn run_test MyModel m skipIfUnsupportedMinOpsetVersion test_index_mask MyModel torch nn Module forward input input torch tensor dtype=torch uint m = torch randn run_test MyModel m MyModel torch nn Module forward input input torch tensor dtype=torch bool m = torch randn run_test MyModel m skipIfUnsupportedMinOpsetVersion test_data Data torch jit ScriptModule torch jit script_method forward x x new_zeros x data size x = torch randn run_test Data x input_names= x dynamic_axes= x run_test Data x remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_index_mask_nd MyModel torch nn Module forward input input input m = torch randn run_test MyModel m skipScriptTest test_dict MyModel torch nn Module forward x_in x_out = x_out test_key_out = torch add x_in list x_in keys noqa RUF list x_in keys noqa RUF x_out x = torch tensor torch randn run_test MyModel x skipScriptTest test_dict_str MyModel torch nn Module forward x_in x_out = x_out test_key_out = torch add x_in test_key_in x_out x = test_key_in torch randn run_test MyModel x skipScriptTest User-defined supported test_dict_output DictModelOutput OrderedDict tensor_out Tensor tuple_out Optional tuple Tensor = None list_out Optional list Tensor = None MyModel torch nn Module forward b c d DictModelOutput tensor_out=a tuple_out= b c list_out= d = torch randn b = torch randn c = torch randn d = torch randn run_test MyModel b c d test_tuple_output MyModel torch nn Module forward b c d b c d = torch randn b = torch randn c = torch randn d = torch randn run_test MyModel b c d test_nested_tuple_output MyModel torch nn Module forward b c d b c d = torch randn b = torch randn c = torch randn d = torch randn run_test MyModel b c d test_tuple_input TupleModel torch nn Module forward tuple Tensor Tensor x = torch randn torch randn run_test TupleModel input_args= x test_tuple_primitive_input TupleModel torch nn Module forward tuple int Tensor b + b x = torch randn y = torch randn run_test TupleModel input_args= x y test_nested_tuple_input NestedTupleModel torch nn Module forward b tuple Tensor tuple Tensor Tensor + b + b + b x = torch randn y = torch randn torch randn torch randn run_test NestedTupleModel input_args= x y skipScriptTest Needs https github com pytorch rfcs pull skipIfUnsupportedMinOpsetVersion test_mixed_optional_default_none Model torch nn Module forward x y Optional Tensor = None z Optional Tensor = None y None x + y z None x + z x x = torch randn y = torch randn z = torch randn model = Model Without kwargs dict run_test model x y None run_test model x None z With kwargs dict run_test model x y y z None run_test model x y None z z run_test model x z z run_test model x y y skipScriptTest tracing eliminates None inputs so works differently See _script version below skipIfUnsupportedMinOpsetVersion test_mixed_optional_default_tensor Model torch nn Module forward x y Optional Tensor = torch ones z Optional Tensor = torch zeros y None x + y z None x + z x x = torch randn y = torch randn z = torch randn model = Model run_test model x y None run_test model x None z skipTraceTest tracing verified different set inputs See above skipIfUnsupportedMinOpsetVersion test_mixed_optional_default_tensor_script Model torch nn Module forward x y Optional Tensor = torch ones z Optional Tensor = torch zeros y None x + y z None x + z x x = torch randn y = torch randn z = torch randn model = torch jit script Model run_test model x y z input_names= x y z run_test model x y y z z input_names= x y z run_test model x y y input_names= x y example_inputs example_kwargs x y None x None z x y y z None x y None z z assertRaisesRegex ValueError args contained None s after flattening run_test model example_inputs example_kwargs input_names= x y z skipScriptTest Needs https github com pytorch rfcs pull skipIfUnsupportedMinOpsetVersion test_all_optional_default_none Model torch nn Module forward x Optional Tensor = None y Optional Tensor = None x None x y None y torch tensor - x = torch randn model = Model run_test model x None run_test model x x y None y disappears tracing input_names= x skipScriptTest tracing eliminates None inputs so works differently See _script version below skipIfUnsupportedMinOpsetVersion test_all_optional_default_tensor Model torch nn Module forward x Optional Tensor = torch ones y Optional Tensor = torch zeros x None x y None y torch tensor - x = torch randn y = torch randn model = Model run_test model x None run_test model None y tracing means y never used so s removed exported model inputs we fail when trying run ORT assertRaisesRegex ValueError got too many positional inputs run_test model x y skipTraceTest tracing verified different set inputs See above skipIfUnsupportedMinOpsetVersion test_all_optional_default_tensor_script Model torch nn Module forward x Optional Tensor = torch ones y Optional Tensor = torch zeros x None x y None y torch tensor - x = torch randn y = torch randn model = torch jit script Model Optional supports None inputs run_test model x NOTE default value supported ONNX so torch ONNX has different behavior assertRaisesRegex AssertionError Tensor-likes close run_test model y y input_names= y run_test model x y run_test model x x y y input_names= x y skipIfUnsupportedMinOpsetVersion test_logit Logit torch nn Module __init__ eps super __init__ eps = eps forward x x logit eps model = Logit eps= e- run_test model torch randn Atleast d torch nn Module forward t w x y z torch atleast_ d t w x y z Atleast d torch nn Module forward t w x y z torch atleast_ d t w x y z Atleast d torch nn Module forward t w x y z torch atleast_ d t w x y z Atleast dTensor torch nn Module forward x torch atleast_ d x Atleast dTensor torch nn Module forward x torch atleast_ d x Atleast dTensor torch nn Module forward x torch atleast_ d x skipScriptTest tracing uses prim ListUnpack avoid onnx SequenceConstruct skipIfUnsupportedMinOpsetVersion common_utils parametrize module_class Atleast d Atleast d Atleast d test_atleast_nd_list_input module_class torch nn Module inputs = torch tensor torch randn torch randn torch randn torch randn run_test module_class inputs skipScriptTest tracing uses prim ListUnpack avoid onnx SequenceConstruct skipIfUnsupportedMinOpsetVersion common_utils parametrize module_class Atleast dTensor Atleast dTensor Atleast dTensor common_utils parametrize inputs torch tensor torch randn torch randn torch randn torch randn test_atleast_nd_single_tensor_input module_class torch nn Module inputs torch Tensor run_test module_class inputs skipScriptTest Needs https github com pytorch rfcs pull skipIfUnsupportedMinOpsetVersion test_mixed_optional Model torch nn Module forward x y Optional Tensor y None x + y x x = torch randn model = Model run_test model x None run_test model x x skipScriptTest Needs https github com pytorch rfcs pull skipIfUnsupportedMinOpsetVersion test_tuple_of_optional Model torch nn Module forward x y tuple Optional Tensor Optional Tensor y None x + y y None x + y x x = torch randn y = torch randn run_test Model x None y skipScriptTest tracing eliminates None inputs so works differently See _script version below skipIfUnsupportedMinOpsetVersion test_tuple_of_optional_default_tensor Model torch nn Module forward x y tuple Optional Tensor Optional Tensor = torch zeros torch zeros y y = y y None x + y y None x + y x x = torch randn y = torch randn run_test Model x None y skipTraceTest tracing verified different set inputs See above skipIfUnsupportedMinOpsetVersion test_tuple_of_optional_default_tensor_script Model torch nn Module forward x y tuple Optional Tensor Optional Tensor = torch zeros torch zeros y y = y y None x + y y None x + y x x = torch randn y = torch randn y = torch randn model = torch jit script Model assertRaisesRegex ValueError args contained None s after flattening run_test model x None y run_test model x y y export succeeds running ORT through run_test would fail because exported model has inputs flattened into inputs torch onnx export model x y y y io BytesIO opset_version=self opset_version dynamo=False test_primitive_input_integer Model torch nn Module forward x int y x + y x = y = torch randint run_test Model x y skipDtypeChecking test_primitive_input_floating Model torch nn Module forward x float y x + y x = y = torch randn run_test Model x y test_primitive_input_bool Model torch nn Module forward flag bool x y flag x y flag = True x = torch randn y = torch randn run_test torch jit script Model flag x y skipIfUnsupportedMinOpsetVersion test_cste_script MyModel torch jit ScriptModule torch jit script_method forward x torch zeros x size torch ones x size x size dtype=torch int x = torch randn run_test MyModel x input_names= x dynamic_axes= x run_test MyModel x remained_onnx_input_idx= test_scalar_tensor test torch nn Module forward input torch scalar_tensor input size torch scalar_tensor input size dtype=torch int x = torch randn y = torch randn model = test run_test model x additional_test_inputs= y input_names= input_ dynamic_axes= input_ test_tensor ScalarInputModel torch jit ScriptModule torch jit script_method forward input torch tensor input shape x = torch randn run_test ScalarInputModel x input_names= x dynamic_axes= x run_test ScalarInputModel x remained_onnx_input_idx= TensorInputModel torch jit ScriptModule torch jit script_method forward input torch tensor input shape input shape x = torch randn run_test TensorInputModel x input_names= x dynamic_axes= x run_test TensorInputModel x remained_onnx_input_idx= FloatInputModel torch jit ScriptModule torch jit script_method forward input torch tensor float input x = torch randn run_test FloatInputModel x InputWithDtypeModel torch jit ScriptModule torch jit script_method forward input torch tensor input shape dtype=torch long x = torch randn run_test InputWithDtypeModel x input_names= x dynamic_axes= x run_test InputWithDtypeModel x remained_onnx_input_idx= MixedInputModel torch jit ScriptModule torch jit script_method forward input torch tensor input shape int input x = torch randn run_test MixedInputModel x test_hardtanh model = torch nn Hardtanh - x = torch arange - dtype=torch float run_test model x test_hardtanh_script_with_default_values MyModel torch jit ScriptModule torch jit script_method forward x torch nn functional hardtanh x x = torch arange - dtype=torch float run_test MyModel x test_hardswish model = torch nn Hardswish x = torch rand dtype=torch float run_test model x Testing edge cases x = torch tensor dtype=torch float run_test model x x = torch tensor - dtype=torch float run_test model x test_hardswish_script MyModel torch jit ScriptModule torch jit script_method forward x torch nn functional hardswish x x = torch rand dtype=torch float run_test MyModel x test_hardsigmoid model = torch nn Hardsigmoid x = torch rand dtype=torch float run_test model x corner cases x = torch tensor dtype=torch float run_test model x x = torch tensor - dtype=torch float run_test model x test_tanhshrink model = torch nn Tanhshrink x = torch rand dtype=torch float run_test model x skipIfUnsupportedMinOpsetVersion test_hardshrink model = torch nn Hardshrink x = torch rand dtype=torch float run_test model x Testing edge cases x = torch tensor dtype=torch float run_test model x x = torch tensor - dtype=torch float run_test model x skipIfUnsupportedMinOpsetVersion test_hardshrink_dtype x = torch rand dtype=torch float run_test torch nn Hardshrink x skipIfUnsupportedMinOpsetVersion test_softshrink model = torch nn Softshrink x = torch rand dtype=torch float run_test model x Testing edge cases x = torch tensor dtype=torch float run_test model x x = torch tensor - dtype=torch float run_test model x skipIfUnsupportedMinOpsetVersion test_softshrink_dtype x = torch rand dtype=torch float run_test torch nn Softshrink x test_clamp ClampModel torch nn Module forward x x clamp - x = torch randn run_test ClampModel x ClampMinModel torch nn Module forward x x clamp min=- x = torch randn run_test ClampMinModel x ClampMaxModel torch nn Module forward x x clamp max= x = torch randn run_test ClampMaxModel x skipIfUnsupportedMinOpsetVersion test_clamp_dyn ClampMaxModel torch jit ScriptModule torch jit script_method forward x x clamp None x size x = torch arange view float run_test ClampMaxModel x ClampMinModel torch jit ScriptModule torch jit script_method forward x x clamp x size None x = torch arange view float run_test ClampMinModel x ClampMinMaxModel torch jit ScriptModule torch jit script_method forward x x clamp x size x size x = torch arange view float run_test ClampMinMaxModel x ClampTensorModel torch nn Module forward x min max x clamp min max x = torch randn y = torch randn z = torch randn run_test ClampTensorModel x y z ClampTensorMinModel torch nn Module forward x min x clamp min=min run_test ClampTensorMinModel x y ClampTensorMaxModel torch nn Module forward x max x clamp max=max run_test ClampTensorMaxModel x z skipIfUnsupportedMinOpsetVersion test_full_trace FullModel torch nn Module forward x torch full x dtype=torch long x = torch tensor run_test FullModel x skipIfUnsupportedMinOpsetVersion test_full_script FullModelScripting torch jit ScriptModule torch jit script_method forward x torch full x dtype=torch long x = torch tensor run_test FullModelScripting x test_fuse_addmm AddmmModel torch nn Module forward x torch mm x x + x x = torch ones run_test AddmmModel x test_maxpool model = torch nn MaxPool d stride= x = torch randn run_test model x test_conv TraceModel torch nn Module __init__ - None super __init__ conv = torch nn Conv d stride= conv = torch nn Conv d stride= padding= dilation= conv = torch nn Conv d stride= padding= forward input input input conv input conv input conv input x = torch randn x = torch randn x = torch randn run_test TraceModel x x x atol= e- test_conv_str_padding TraceModel torch nn Module __init__ - None super __init__ conv = torch nn Conv d padding= valid conv = torch nn Conv d stride= padding= valid dilation= conv = torch nn Conv d stride= padding= same forward input input input conv input conv input conv input x = torch randn x = torch randn x = torch randn run_test TraceModel x x x atol= e- test_conv_shape_inference Model torch nn Module __init__ - None super __init__ conv = torch nn Conv d stride= padding= dilation= forward input conv input + x = torch randn run_test Model x atol= e- input_names= x dynamic_axes= x test_conv_transpose TraceModel torch nn Module __init__ - None super __init__ conv = torch nn ConvTranspose d stride= conv = torch nn ConvTranspose d stride= padding= dilation= conv = torch nn ConvTranspose d stride= padding= forward input input input conv input conv input conv input x = torch randn x = torch randn x = torch randn run_test TraceModel x x x atol= e- test_numpy_T NumpyTranspose torch nn Module forward x x T run_test NumpyTranspose torch randn Conversion Transpose depends input shape known The following test only works when onnx shape inference enabled test_transpose_infer_shape TransposeModule torch jit ScriptModule __init__ - None super __init__ conv = torch nn Conv d stride= torch jit script_method forward x x = conv x x transpose x = torch randn y = torch randn run_test TransposeModule x input_names= x dynamic_axes= x additional_test_inputs= y squeeze_model_tests d x x Squeeze torch nn Module __init__ d super __init__ d = d forward x d None torch squeeze x dim=self d torch squeeze x x = x None x len x run_test Squeeze d x input_names= input dynamic_axes= input additional_test_inputs=x run_test Squeeze d x test_squeeze_without_no_op x = torch randn squeeze_model_tests x None skipIfUnsupportedMinOpsetVersion test_squeeze_dynamic x_squeeze = torch randn x_noop = torch randn squeeze_model_tests x_squeeze x_noop test_squeeze_neg_without_no_op x = torch randn squeeze_model_tests - x None skipIfUnsupportedMinOpsetVersion test_squeeze_neg x_squeeze = torch randn x_noop = torch randn squeeze_model_tests - x_squeeze x_noop test_squeeze_all_dims x_squeeze = torch randn x_noop = torch randn squeeze_model_tests None x_squeeze x_noop skipIfUnsupportedMinOpsetVersion test_squeeze_no_op x_noop = torch randn x_squeeze = torch randn squeeze_model_tests x_noop x_squeeze skipIfUnsupportedMinOpsetVersion test_squeeze_runtime_dim Squeeze torch nn Module forward d d t = torch zeros d d t squeeze d = torch tensor d = torch tensor d = torch tensor run_test Squeeze d d additional_test_inputs= d d run_test Squeeze d d additional_test_inputs= d d test_squeeze Squeeze torch nn Module forward x torch squeeze x dim=- x = torch randn run_test Squeeze x skipIfUnsupportedMinOpsetVersion test_squeeze_dynamic_dim Squeeze torch nn Module forward x dim int torch squeeze x dim x = torch randn dim = run_test Squeeze x dim test_unsqueeze Unsqueeze torch nn Module forward x torch unsqueeze x dim=- x = torch randn run_test Unsqueeze x skipIfUnsupportedMinOpsetVersion test_unsqueeze_dynamic_dim Unsqueeze torch nn Module forward x dim int torch unsqueeze x dim x = torch randn dim = - run_test Unsqueeze x dim test_maxpool_default_stride MaxPoolModel torch nn Module forward x torch nn functional max_pool d x model = MaxPoolModel x = torch randn run_test model x skipIfUnsupportedMinOpsetVersion test_maxpool_adaptive model = torch nn AdaptiveMaxPool d return_indices=False x = torch randn requires_grad=True y = torch randn requires_grad=True run_test model x input_names= x dynamic_axes= x additional_test_inputs= y test_maxpool_ d model = torch nn MaxPool d padding= x = torch randn requires_grad=True run_test model x test_maxpool_ d_ceil model = torch nn MaxPool d ceil_mode=True x = torch randn run_test model x test_maxpool_ d_ceil model = torch nn MaxPool d ceil_mode=True x = torch randn run_test model x test_maxpool_ d_ceil model = torch nn MaxPool d ceil_mode=True x = torch randn run_test model x skipIfUnsupportedMinOpsetVersion test_maxpool_dynamic test torch nn Module __init__ in_channels out_channels super __init__ norm_layer = functools partial torch nn BatchNorm d eps= avgpool = torch nn MaxPool d stride= ceil_mode=True conv = torch nn Conv d in_channels out_channels kernel_size= stride= bias=False norm = norm_layer out_channels forward x norm conv avgpool x model = test inputs = torch randn run_test model inputs input_names= input_ dynamic_axes= input_ x y output_ x y output_names= output_ TODO Enable maxpool-ceil family after ONNX + bumped skipIfUnsupportedMaxOpsetVersion test_maxpool_ d_ceil_corner model = torch nn MaxPool d kernel_size= dilation= stride= ceil_mode=True return_indices=False x = torch randn run_test model x skipIfUnsupportedMaxOpsetVersion test_maxpool_ d_ceil_corner model = torch nn MaxPool d kernel_size= dilation= stride= ceil_mode=True return_indices=False x = torch randn run_test model x skipIfUnsupportedMaxOpsetVersion test_maxpool_ d_ceil_corner model = torch nn MaxPool d kernel_size= dilation= stride= padding= ceil_mode=True return_indices=False x = torch randn run_test model x skipIfUnsupportedMaxOpsetVersion skipIfUnsupportedMinOpsetVersion test_maxpool_ d_ceil_corner_with_indices model = torch nn MaxPool d kernel_size= dilation= stride= ceil_mode=True return_indices=True x = torch randn run_test model x skipIfUnsupportedMaxOpsetVersion skipIfUnsupportedMinOpsetVersion test_maxpool_ d_ceil_corner_with_indices model = torch nn MaxPool d kernel_size= dilation= stride= ceil_mode=True return_indices=True x = torch randn run_test model x skipIfUnsupportedMaxOpsetVersion skipIfUnsupportedMinOpsetVersion test_maxpool_ d_ceil_corner_with_indices model = torch nn MaxPool d kernel_size= dilation= stride= padding= ceil_mode=True return_indices=True x = torch randn run_test model x skipIfUnsupportedMinOpsetVersion test_maxpool_with_indices model = torch nn MaxPool d stride= return_indices=True x = torch randn run_test model x skipIfUnsupportedMinOpsetVersion test_maxpool_dilation model = torch nn MaxPool d stride= dilation= x = torch randn run_test model x test_avgpool_default_stride AvgPoolModel torch nn Module forward x torch nn functional avg_pool d x model = AvgPoolModel x = torch randn run_test model x test_avgpool model = torch nn AvgPool d stride= x = torch randn run_test model x test_avgpool_ d_ceil model = torch nn AvgPool d ceil_mode=True x = torch randn run_test model x TODO ceil_mode included test because https github com microsoft onnxruntime issues The ORT PyTorch has different calculation ceil_mode last value common_utils parametrize padding common_utils parametrize count_include_pad True False test_avgpool_ d padding count_include_pad model = torch nn AvgPool d padding=padding count_include_pad=count_include_pad x = torch randn run_test model x TODO ceil_mode included test because https github com microsoft onnxruntime issues The ORT PyTorch has different calculation ceil_mode last value issue requires fix onnx https github com onnx onnx issues fix ORT planned After fixes place we can add ceil_mode test skipIfUnsupportedMinOpsetVersion test_avgpool_ d_ceil model = torch nn AvgPool d ceil_mode=True x = torch randn y = torch randn run_test model x input_names= x dynamic_axes= x additional_test_inputs= y skipIfUnsupportedMinOpsetVersion test_avgpool_dynamic test torch nn Module __init__ in_channels out_channels super __init__ norm_layer = functools partial torch nn BatchNorm d eps= avgpool = torch nn AvgPool d stride= ceil_mode=True count_include_pad=False conv = torch nn Conv d in_channels out_channels kernel_size= stride= bias=False norm = norm_layer out_channels forward x norm conv avgpool x model = test inputs = torch randn run_test model inputs input_names= input_ dynamic_axes= input_ x y output_ x y output_names= output_ skipIfUnsupportedMinOpsetVersion test_floating_point FloatingPoint torch jit ScriptModule torch jit script_method forward x x is_floating_point x new_zeros x shape x new_zeros x shape x = torch randn run_test FloatingPoint x input_names= x dynamic_axes= x run_test FloatingPoint x remained_onnx_input_idx= FloatingPoint torch jit ScriptModule torch jit script_method forward x x size = x + is_floating_point x + x + x x = torch randn run_test FloatingPoint x Operator rank mismatch between outputs two branches opsets below skipIfUnsupportedMinOpsetVersion test_floating_point_infer_dtype FloatingPoint torch jit ScriptModule torch jit script_method forward x x size = x + is_floating_point x new_zeros x shape x new_zeros x shape x x = torch randn run_test FloatingPoint x input_names= x dynamic_axes= x run_test FloatingPoint x remained_onnx_input_idx= FloatingPoint torch jit ScriptModule torch jit script_method forward x x size = x + is_floating_point x + x x x = torch randn torch int run_test FloatingPoint x skipIfUnsupportedMinOpsetVersion test_prim_min torch jit script list_append boxes list Tensor temp = i b enumerate boxes enumerate creating prim min op torch graph temp append torch full_like b i temp Min torch nn Module forward x boxes = x _ range list_append boxes x = torch rand run_test Min x M torch jit ScriptModule torch jit script_method forward x i = min x i i x = torch arange dtype=torch int run_test M x test_arithmetic ArithmeticModule torch nn Module forward x x = x + x = x - x = x x = x x x = torch randn run_test ArithmeticModule x test_arithmetic_prim_long ArithmeticModule torch nn Module forward x y int x = x + y x = x - y x = x y x = x y x x = torch randn y = run_test ArithmeticModule x y ArithmeticModule torch nn Module forward x x = x + x = x - x shape x = torch randn run_test ArithmeticModule x remained_onnx_input_idx= skipDtypeChecking test_arithmetic_prim_float ArithmeticModule torch nn Module forward x y float x = x + y x = x - y x = x y x = x y x x = torch randn y = run_test ArithmeticModule x y ArithmeticModule torch nn Module forward x x = x + x = x - x shape x = torch randn run_test ArithmeticModule x remained_onnx_input_idx= skipDtypeChecking test_arithmetic_prim_bool ArithmeticModule torch nn Module forward x y int z bool t float x = x + y x = x - y z x = x y x = x y x t z x = torch randn y = z = False t = run_test ArithmeticModule x y z t ArithmeticModule torch nn Module forward x int y int x == y x = y = run_test ArithmeticModule x y skipScriptTest reason= In trace Outputs always None removed \ In script Outputs always None removed before opset \ After opset we replace None output Optional node test_tuple_with_none_outputs TupleModel torch nn Module forward x x x None x None x = torch randn run_test TupleModel x In scripting first transpose node do carry shape dtype info The following test only works when onnx shape inference enabled test_arithmetic_infer_dtype ArithmeticModule torch jit ScriptModule torch jit script_method forward x x = x t x = x + x = x - x = x x = x x x = torch randn run_test ArithmeticModule x unittest skip Floor division ONNX inconsistent eager see test_floor_div FloorDivModule torch nn Module forward x y x x x dtype=torch float x dtype=torch float x dtype=torch int x dtype=torch int x y + dtype=torch int x y x dtype=torch float y dtype=torch int x dtype=torch float y dtype=torch float x dtype=torch int y dtype=torch int x dtype=torch int y x = torch arange - reshape y = torch arange + reshape run_test FloorDivModule x y unittest skip Floor division ONNX inconsistent eager see test_floor_div_script FloorDivModule torch jit ScriptModule torch jit script_method forward x y x x x y x = torch arange - reshape y = torch randn run_test FloorDivModule x y unittest skip Floor division ONNX inconsistent eager see skipIfUnsupportedMinOpsetVersion test_floordiv FloordivModule torch nn Module forward x x new_zeros x size x size x = torch randn run_test FloordivModule x input_names= x dynamic_axes= x run_test FloordivModule x remained_onnx_input_idx= test_div DivModule torch nn Module forward x y x y torch true_divide x y x = torch randn torch int y = torch arange + reshape torch int run_test DivModule x y run_test DivModule x float y float Note div cannot generally exported via scripting since its type promotion logic dependent knowing scalar types input tensors That ONNX graph dependent data type inputs This makes appropriate tracing only test_div_promotion_trace DivModule torch nn Module forward x y x y torch true_divide x y x = torch randn torch int y = torch arange + reshape torch int common_utils set_default_dtype torch float run_test torch jit trace DivModule x y x y common_utils set_default_dtype torch double run_test torch jit trace DivModule x y x y In scripting x y do carry shape dtype info The following test only works when onnx shape inference enabled test_div_promotion_script DivModule torch nn Module forward x y Add transpose hide shape type information Otherwise shape type still available input x = x transpose y = y transpose x y torch true_divide x y x = torch randn torch int y = torch arange + reshape torch int x y int output float This can handled default case where both cast float It works even type x y unknown common_utils set_default_dtype torch float run_test torch jit script DivModule x y x y int output double This can handled default case where both cast double It works even type x y unknown common_utils set_default_dtype torch double run_test torch jit script DivModule x y x int y double output double This can only handled when both type x y known x = torch randn torch int y = torch arange + reshape torch double run_test torch jit script DivModule x y skipDtypeChecking test_div_rounding_mode TrueDivModule torch nn Module forward x y x div y rounding_mode=None torch div x y rounding_mode=None TruncDivModule torch nn Module forward x y x div y rounding_mode= trunc torch div x y rounding_mode= trunc FloorDivModule torch nn Module forward x y x div y rounding_mode= floor torch div x y rounding_mode= floor modules = TrueDivModule TruncDivModule FloorDivModule x = torch randn torch int y = torch arange + reshape torch int module modules run_test module x y run_test torch jit trace module x y x y run_test torch jit script module x y x = torch randn y = torch rand + module modules run_test module x y run_test torch jit trace module x y x y run_test torch jit script module x y test_slice_trace MyModule torch nn Module forward x x x = torch randn run_test MyModule x test_slice_neg NegSlice torch nn Module forward x x - x = torch randn run_test NegSlice x test_slice_neg_large NegSlice torch nn Module forward x x - - - x = torch randn run_test NegSlice x test_slice_neg_large_negone NegSlice torch nn Module forward x x - x = torch randn run_test NegSlice x skipIfUnsupportedMinOpsetVersion test_slice_with_input_index InputIndexSlice torch nn Module forward x y x y size = y x x = torch zeros y = torch rand run_test InputIndexSlice x y skipIfUnsupportedMinOpsetVersion skipScriptTest Torchscript doesn t support d index test_slice_with_ d_input_index InputIndexSlice torch nn Module forward x y x y = y x x = torch zeros y = torch tensor dtype=torch int run_test InputIndexSlice x y skipIfUnsupportedMinOpsetVersion test_slice_with_input_step_size InputIndexSlice torch nn Module forward x y z x y z z = x x = torch zeros y = torch tensor dtype=torch int z = torch tensor dtype=torch int run_test InputIndexSlice x y z skipIfUnsupportedMinOpsetVersion skipScriptTest scripting tuple list append test_slice_dynamic DynamicSliceExportMod torch nn Module forward x results = i range results append x x size - i i x size i tuple results x = torch rand y = torch randn run_test DynamicSliceExportMod x additional_test_inputs= y input_names= input_ output_names= output_ dynamic_axes= input_ output_ skipIfUnsupportedMinOpsetVersion test_slice_dynamic_script DynamicSliceModel torch jit ScriptModule torch jit script_method forward x x x size x = torch rand run_test DynamicSliceModel x skipIfUnsupportedMinOpsetVersion test_slice_dynamic_shape_script DynamicSliceModel torch nn Module forward x x new_zeros x shape x size x = torch rand run_test DynamicSliceModel x input_names= x dynamic_axes= x run_test DynamicSliceModel x remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion skipScriptTest scripting tuple list append test_slice_dynamic_to_end DynamicSliceExportMod torch nn Module forward x results = i range results append x i x size - tuple results x = torch rand run_test DynamicSliceExportMod x dynamic_axes= input_ output_ test_square Square torch nn Module forward x torch square x x = torch randn run_test Square x skipIfUnsupportedMinOpsetVersion test_arange_dynamic ArangeModel torch nn Module forward input torch arange input shape torch arange torch arange start=input shape end=input shape + x = torch randn y = torch randn run_test ArangeModel x additional_test_inputs= y input_names= input_ output_names= output_ output_ output_ dynamic_axes= input_ output_ run_test torch jit script ArangeModel x additional_test_inputs= y input_names= input_ output_names= output_ output_ output_ dynamic_axes= input_ output_ skipIfUnsupportedMinOpsetVersion test_dynamic_arange_out ArangeOutModel torch nn Module forward end out_t = torch tensor dtype=torch int torch arange end out=out_t x = torch tensor run_test ArangeOutModel x skipIfUnsupportedMinOpsetVersion test_dynamic_arange_start_out ArangeStartOutModel torch nn Module forward start end out_t = torch tensor dtype=torch int torch arange start size end out=out_t x = torch randn y = torch tensor run_test ArangeStartOutModel x y input_names= x y dynamic_axes= x run_test ArangeStartOutModel x y remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_linspace LinspaceModel torch nn Module forward start end steps torch linspace start end steps x = torch tensor dtype=torch float y = torch tensor dtype=torch float z = torch tensor dtype=torch int run_test LinspaceModel x y z skipIfUnsupportedMinOpsetVersion test_linspace_negative_start LinspaceModel torch nn Module forward start end steps torch linspace start end steps x = torch tensor - dtype=torch float y = torch tensor dtype=torch float z = torch tensor dtype=torch int run_test LinspaceModel x y z skipIfUnsupportedMinOpsetVersion test_arange_with_floats_out ArangeModelEnd torch nn Module forward end out_t = torch tensor dtype=torch float torch arange end out=out_t y = torch tensor dtype=torch float run_test ArangeModelEnd y ArangeModelStep torch nn Module forward start end out_t = torch tensor dtype=torch float torch arange start size end out=out_t x = torch randn y = torch tensor dtype=torch float run_test ArangeModelStep x y input_names= x y dynamic_axes= x run_test ArangeModelStep x y remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_arange_with_floats ArangeModelEnd torch nn Module forward end torch arange end y = torch tensor dtype=torch float run_test ArangeModelEnd y ArangeModelStep torch nn Module forward start end torch arange start size end x = torch randn y = torch tensor dtype=torch float run_test ArangeModelStep x y input_names= x y dynamic_axes= x run_test ArangeModelStep x y remained_onnx_input_idx= ArangeModelStepNeg torch nn Module forward start end torch arange end start size - x = torch randn y = torch tensor dtype=torch float run_test ArangeModelStepNeg x y input_names= x y dynamic_axes= x run_test ArangeModelStepNeg x y remained_onnx_input_idx= ArangeModelStart torch nn Module forward start end torch arange start size end x = torch randn y = torch tensor dtype=torch float run_test ArangeModelStart x y input_names= x y dynamic_axes= x run_test ArangeModelStart x y remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_arange_with_floats_override ArangeModelEnd torch nn Module forward end torch arange end dtype=torch int y = torch tensor dtype=torch float run_test ArangeModelEnd y ArangeModelStep torch nn Module forward start end torch arange start size end dtype=torch int x = torch randn y = torch tensor dtype=torch float run_test ArangeModelStep x y input_names= x y dynamic_axes= x run_test ArangeModelStep x y remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_arange_out ArangeOutModel torch nn Module forward end out_t = torch tensor dtype=torch float torch arange end out=out_t x = torch tensor dtype=torch float run_test ArangeOutModel x skipIfUnsupportedMinOpsetVersion test_arange_start_out ArangeStartOutModel torch nn Module forward start end out_t = torch tensor dtype=torch float torch arange start size end out=out_t x = torch randn y = torch tensor dtype=torch float run_test ArangeStartOutModel x y input_names= x y dynamic_axes= x run_test ArangeStartOutModel x y remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_arange_no_type ArangeModel torch nn Module forward end torch arange end torch arange end x = torch tensor dtype=torch float run_test ArangeModel x skipIfUnsupportedMinOpsetVersion test_size SizeModel torch nn Module forward input torch arange input size torch arange input size - torch ones input shape x = torch randn run_test SizeModel x input_names= x dynamic_axes= x run_test SizeModel x remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion skipScriptTest x stride scriptable test_as_strided Model torch nn Module forward x chunk_size = list x size chunk_size = chunk_size - chunk_stride = list x stride chunk_stride = chunk_stride x as_strided storage_offset= x as_strided chunk_size chunk_stride x = torch randn run_test Model x skipScriptTest Ellipses followed tensor indexing scriptable test_tensor_index_advanced_indexing_ellipsis MyModel torch nn Module forward input input torch tensor torch tensor m = torch randn run_test MyModel m test_tensor_index_advanced_indexing MyModel torch nn Module forward input input torch tensor torch tensor torch tensor m = torch randn run_test MyModel m MyModel torch nn Module forward input input torch tensor None torch tensor run_test MyModel m MyModel torch nn Module forward input input torch tensor torch tensor torch tensor run_test MyModel m test_tensor_index_advanced_indexing_consecutive MyModel torch nn Module forward input input torch tensor torch tensor None m = torch randn run_test MyModel m skipIfUnsupportedMinOpsetVersion test_index_put IndexPutModel torch nn Module forward x ind update x ind = update x x = torch randn ind = torch tensor dtype=torch long update = torch ones run_test IndexPutModel x ind update skipIfUnsupportedMinOpsetVersion test_index_put_singular IndexPutBoolModel torch nn Module forward mask indices mask indices = True mask mask = torch zeros dtype=torch bool indices = torch rand mask shape torch int run_test IndexPutBoolModel mask indices IndexPutFloatModel torch nn Module forward mask indices mask indices = torch tensor mask mask = torch rand dtype=torch float indices = torch rand mask shape torch int run_test IndexPutFloatModel mask indices skipIfUnsupportedMinOpsetVersion test_index_put_accumulate IndexPutModel torch nn Module forward x ind update x index_put ind update accumulate=True x = torch randn ind = torch tensor dtype=torch long update = torch ones run_test IndexPutModel x ind update skipIfUnsupportedMinOpsetVersion test_index_put_slice_index IndexPutModel torch nn Module forward x update x torch tensor += update x x = torch randn update = torch tensor view run_test IndexPutModel x update IndexPutModel torch nn Module forward x update x torch tensor torch tensor += update x x = torch randn update = torch randn run_test IndexPutModel x update IndexPutModel torch nn Module forward x update x torch tensor += update x x = torch randn update = torch tensor view run_test IndexPutModel x update IndexPutModel torch nn Module forward x update x torch tensor += update x x = torch randn update = torch tensor view run_test IndexPutModel x update IndexPutModel torch nn Module forward x update x torch tensor += update x x = torch randn update = torch tensor view run_test IndexPutModel x update IndexPutModel torch nn Module forward x update x = update x x = torch randn update = torch arange torch float view run_test IndexPutModel x update IndexPutModel torch nn Module forward x update x = update x x = torch randn update = torch arange torch float view run_test IndexPutModel x update IndexPutModel torch nn Module forward x update x = update x x = torch randn update = torch arange torch float view run_test IndexPutModel x update IndexPutModel torch nn Module forward poses w = x = poses - w - boxes = torch zeros poses shape boxes = x boxes x = torch zeros dtype=torch int run_test IndexPutModel x IndexPutModel torch nn Module forward x ind update x ind = update view expand x x = torch randn ind = torch tensor update = torch randn run_test IndexPutModel x ind update skipIfUnsupportedMinOpsetVersion skipScriptTest Ellipses followed tensor indexing scriptable test_index_put_ellipsis IndexPutModel torch nn Module forward x update x torch tensor += update x x = torch randn update = torch randn run_test IndexPutModel x update IndexPutModel torch nn Module forward x update x torch tensor += update x x = torch randn update = torch randn run_test IndexPutModel x update unittest skip regression https github com microsoft onnxruntime issues skipIfUnsupportedMinOpsetVersion test_index_put_loop torch jit script ngram_attention_bias sequence_length int ngram int device torch device dtype torch dtype bias = torch ones ngram sequence_length device=device dtype=dtype float -inf stream_idx range ngram i range sequence_length bias = bias bias stream_idx i = bias = bias bias = stream_idx range ngram i range sequence_length bias stream_idx i = bias i = bias ScriptModel torch nn Module __init__ - None super __init__ ngram = max_target_positions = forward hidden_states seq_length batch_size = hidden_states shape predict_causal_mask = ngram_attention_bias max_target_positions ngram hidden_states device hidden_states dtype predict_causal_mask = predict_causal_mask seq_length predict_causal_mask x = torch randn y = torch randn run_test ScriptModel x input_names= x dynamic_axes= x seq_length batch_size additional_test_inputs= y skipIfUnsupportedMinOpsetVersion test_copy_ CopyModel torch nn Module forward x data x = data x x = torch randn update = torch randn run_test CopyModel x update mixed slice select CopyModel torch nn Module forward x data x = data x x = torch randn update = torch tensor dtype=torch float run_test CopyModel x update update = torch tensor dtype=torch float run_test CopyModel x update update = torch randn run_test CopyModel x update CopyModel torch nn Module forward x data x = data x x = torch randn update = torch tensor dtype=torch float run_test CopyModel x update update = torch tensor dtype=torch float run_test CopyModel x update update = torch randn run_test CopyModel x update CopyModel torch nn Module forward x ind data x ind = data x x = torch randn ind = torch tensor data = torch randn run_test CopyModel x ind data CopyModel torch nn Module forward x mask mask None x copy_ mask x x = torch randn mask = torch randn run_test CopyModel x mask skipIfUnsupportedMinOpsetVersion skipScriptTest Model scriptable output shape doesn t match broadcast shape test_copy_tracing CopyModel torch nn Module forward x data x = data x x = torch randn update = torch randn run_test CopyModel x update skipIfUnsupportedMinOpsetVersion test_copy_ellipsis CopyModel torch nn Module forward x update x = update x x = torch randn update = torch ones run_test CopyModel x update x = torch randn update = torch ones run_test CopyModel x update skipIfUnsupportedMinOpsetVersion test_copy_ellipsis_script CopyModel torch nn Module forward x update Insert reshape node ensure no shape type info x scripting without onnx shape inference x = x reshape x = update x x = torch randn update = torch ones run_test CopyModel x update skipIfUnsupportedMinOpsetVersion test_flip MyModule torch nn Module forward x torch flip x dims= x = torch tensor np arange reshape run_test MyModule x skipIfUnsupportedMinOpsetVersion test_randint RandInt torch nn Module forward x randint = torch randint x shape x = randint + x x x = torch randn run_test RandInt x skipIfUnsupportedMinOpsetVersion test_randint_value RandInt torch nn Module forward x This randint call always returns torch randint x shape + x x = torch randn run_test RandInt x skipIfUnsupportedMinOpsetVersion test_randint_like RandInt torch nn Module forward x This randint call always returns torch randint_like x + x x = torch randn run_test RandInt x test_randn RandN torch nn Module forward x torch mul x torch randn + x size x = torch randn run_test RandN x test_rand Rand torch nn Module forward x torch mul x torch rand + x size x = torch randn run_test Rand x test_randn_dtype RandN torch nn Module forward x The resulting node s dtype should double x torch float torch randn dtype=torch double torch tensor dtype=torch float x = torch randn run_test RandN x test_rand_dtype Rand torch nn Module forward x The resulting node s dtype should double x torch float torch rand dtype=torch double torch tensor dtype=torch float x = torch randn run_test Rand x skipIfUnsupportedMinOpsetVersion test_randn_dynamic_size RandN torch nn Module forward x torch mul x torch randn x size size x = torch randn run_test RandN x skipIfUnsupportedMinOpsetVersion test_rand_dynamic_size Rand torch nn Module forward x torch mul x torch rand x size size x = torch randn run_test Rand x test_randn_like RandNLike torch nn Module forward x torch mul x torch randn_like x size x = torch randn run_test RandNLike x run_test torch jit script RandNLike x test_rand_like RandLike torch nn Module forward x torch mul x torch rand_like x size x = torch randn run_test RandLike x run_test torch jit script RandLike x test_randn_like_dtype RandNLike torch nn Module forward x The resulting node s dtype should double x torch float torch randn_like x dtype=torch double torch tensor dtype=torch float x = torch randn run_test RandNLike x test_rand_like_dtype RandLike torch nn Module forward x The resulting node s dtype should double x torch float torch rand_like x dtype=torch double torch tensor dtype=torch float x = torch randn run_test RandLike x test_bernoulli Bernoulli torch nn Module forward x torch mul x torch bernoulli x size x = torch empty uniform_ run_test Bernoulli x x = torch empty dtype=torch double uniform_ run_test Bernoulli x test_bernoulli_p Bernoulli_float torch nn Module forward x torch mul x torch bernoulli x size Bernoulli_tensor torch nn Module forward x torch mul x torch rand_like x bernoulli_ x size x = torch rand run_test Bernoulli_float x run_test Bernoulli_tensor x x = torch rand dtype=torch double run_test Bernoulli_float x run_test Bernoulli_tensor x unittest skip Bug ORT skip test until rel- skipIfUnsupportedMinOpsetVersion test_reshape_allowzero ReshapeModel torch nn Module forward x x = x reshape x x = torch randn run_test ReshapeModel x test_reshape_different_rank ReshapeModel torch nn Module forward x x = x reshape - x x = torch randn run_test ReshapeModel x _interpolate x mode use_size is_upsample align_corners=False MyModel torch nn Module __constants__ = mode use_size is_upsample size scale size_array scale_array align_corners __init__ mode use_size is_upsample align_corners super __init__ mode = mode use_size = use_size is_upsample = is_upsample align_corners = align_corners scale = is_upsample size = is_upsample x dim == scale_array = size_array = x dim == scale_array = size_array = scale_array = size_array = forward x use_size align_corners torch nn functional interpolate x mode=self mode size=self size align_corners=True torch nn functional interpolate x mode=self mode size=self size_array align_corners=True torch nn functional interpolate x mode=self mode size=self size torch nn functional interpolate x mode=self mode size=self size_array align_corners torch nn functional interpolate x mode=self mode scale_factor=self scale recompute_scale_factor=False torch nn functional interpolate x mode=self mode scale_factor=self scale_array recompute_scale_factor=False torch nn functional interpolate x mode=self mode scale_factor=self scale recompute_scale_factor=False torch nn functional interpolate x mode=self mode scale_factor=self scale_array recompute_scale_factor=False model = MyModel mode use_size is_upsample align_corners run_test model x atol= e- _interpolate_tests is_upsample - cubic mode supported opsets below - linear mode does match opsets below modes = nearest linear bicubic opset_version modes = nearest x = torch randn requires_grad=True torch randn requires_grad=True torch randn requires_grad=True mode modes xi x mode_i = mode TODO enable bicubic downsample when ORT precision loss fixed mode == bicubic xi dim = continue mode == linear xi dim == TODO enable when linear mode implemented d inputs ORT continue xi dim == mode_i = bilinear xi dim == TODO enable when linear mode implemented d inputs ORT mode_i = trilinear continue _interpolate xi mode_i True is_upsample test align_corners supported mode = nearest _interpolate xi mode_i True is_upsample True following cases require dynamic sizes scales which which supported opset_version opset_version = _interpolate xi mode_i True is_upsample test align_corners supported mode = nearest _interpolate xi mode_i False is_upsample True _interpolate xi mode_i False is_upsample ONNX export failed interpolate scripting because dynamic size supported opsets below skipIfUnsupportedMinOpsetVersion test_interpolate_upsample _interpolate_tests True skipIfUnsupportedMaxOpsetVersion skipScriptTest Scripting supported opsets See test_interpolate_upsample test_interpolate_upsample_trace _interpolate_tests True skipIfUnsupportedMinOpsetVersion test_interpolate_function_substitution ScriptModel torch jit ScriptModule torch jit script_method forward x torch nn functional interpolate x mode= nearest scale_factor= ScriptModule torch jit ScriptModule __init__ - None super __init__ submodule = ScriptModel torch jit script_method forward input submodule input x = torch randn run_test ScriptModule x torch jit script script_method x torch nn functional interpolate x mode= nearest scale_factor= TracingModule torch nn Module forward x script_method x run_test TracingModule x skipIfUnsupportedMinOpsetVersion test_interpolate_downsample _interpolate_tests False skipIfUnsupportedMinOpsetVersion test_interpolate_half_pixel testing whether uses half_pixel pytorch_half_pixel see https github com onnx onnx blob main docs Operators md#Resize MyModel torch nn Module __init__ mode size super __init__ mode = mode size = size forward x torch nn functional interpolate x mode=self mode size=self size modes = linear bicubic x = torch randn requires_grad=True torch randn requires_grad=True torch randn requires_grad=True mode modes xi x mode_i = mode mode == bicubic xi dim = continue mode == linear xi dim == mode_i = bilinear xi dim == mode_i = trilinear i range xi dim - size = list xi shape size i = run_test MyModel mode_i size xi skipIfUnsupportedMinOpsetVersion test_interpolate_no_shape MyModel torch jit ScriptModule torch jit script_method forward x y x = torch add x x out = torch nn functional interpolate x mode= bilinear size= align_corners=False out = torch nn functional interpolate x mode= nearest size= int y size int y size out out x = torch randn requires_grad=True y = torch randn requires_grad=True run_test MyModel x y input_names= x y dynamic_axes= x y run_test MyModel x y remained_onnx_input_idx= skipScriptTest scripting raises OnnxRuntimeError test_interpolate_adaptive_pooling_error x = torch randn requires_grad=True assertRaises RuntimeError cm _interpolate x area True True assertRaises RuntimeError cm _interpolate x area False True test_groupnorm model = torch nn GroupNorm x = torch randn run_test model x model = torch nn GroupNorm x = torch randn run_test model x model = torch nn GroupNorm x = torch randn run_test model x test_groupnorm_noaffine model = torch nn GroupNorm affine=False x = torch randn run_test model x model = torch nn GroupNorm affine=False x = torch randn run_test model x model = torch nn GroupNorm affine=False x = torch randn run_test model x skipIfUnsupportedMinOpsetVersion test_list_unpack_scripted ListUnpack torch nn Module forward x b = x shape x new_zeros b x = torch randn run_test torch jit script ListUnpack x input_names= x dynamic_axes= x run_test torch jit script ListUnpack x remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_list_unpack_scripted_runs_without_error_with_constructed_list_as_input PackUnpack torch nn Module Create unpack list tensors When scripted should produce graph similar ` ` ` graph __torch__ PackUnpack Tensor b Tensor packed Tensor = prim ListConstruct b c Tensor Tensor = prim ListUnpack packed c ` ` ` forward b packed = b c _ = packed c run_test torch jit script PackUnpack torch tensor torch tensor remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_list_unpack_slice_scripted ListUnpackSlice torch nn Module forward x b = x shape x new_zeros b x = torch randn run_test torch jit script ListUnpackSlice x input_names= x dynamic_axes= x run_test torch jit script ListUnpackSlice x remained_onnx_input_idx= skipDtypeChecking test_pow PowModule torch nn Module forward x y x pow y x = torch randn y = torch randn run_test PowModule x y x = torch randint y = torch randint dtype=torch int run_test PowModule x y x = torch randint y = torch randint run_test PowModule x y x = torch randn dtype=torch float y = torch randint run_test PowModule x y PowModule torch nn Module forward x torch pow x x = torch randn run_test PowModule x x = torch randint run_test PowModule x x = torch randn dtype=torch float run_test PowModule x PowModule torch nn Module forward x y y torch pow x x = torch randint y = torch rand run_test PowModule x y arithmeticOps Add\Sub\Mul\Div\Gemm\Pow\Mod low precision include unit will failed ORT add dtype=torch long avoid ORT output type does match expected type will fixed ONNX version skipIfUnsupportedMaxOpsetVersion skipDtypeChecking test_arithmeticOps_with_low_precision AddModule torch nn Module forward x y x + y SubModule torch nn Module forward x y x - y MulModule torch nn Module forward x y x y DivModule torch nn Module forward x y x y PowModule torch nn Module forward x y x pow y x = torch tensor dtype=torch uint y = torch tensor dtype=torch uint z = torch tensor dtype=torch uint run_test AddModule x y run_test SubModule x y run_test MulModule x y run_test DivModule x y run_test PowModule x z x = torch tensor dtype=torch int y = torch tensor dtype=torch int z = torch tensor dtype=torch int run_test AddModule x y run_test SubModule x y run_test MulModule x y run_test DivModule x y run_test PowModule x z x = torch tensor dtype=torch int y = torch tensor dtype=torch int z = torch tensor dtype=torch int run_test AddModule x y run_test SubModule x y run_test MulModule x y run_test DivModule x y run_test PowModule x z x = torch tensor dtype=torch uint y = torch tensor dtype=torch float z = torch tensor dtype=torch float run_test AddModule x y run_test SubModule x y run_test MulModule x y run_test DivModule x y run_test PowModule x z x = torch tensor dtype=torch uint y = torch tensor dtype=torch int z = torch tensor dtype=torch int run_test AddModule x y run_test SubModule x y run_test MulModule x y run_test DivModule x y run_test PowModule x z test_mul_bool MyModel torch nn Module forward x y torch mul x y x_t = torch tensor True False True False y_t = torch tensor True True False False z_t = torch tensor run_test MyModel x_t y_t run_test MyModel x_t z_t run_test MyModel z_t y_t fmod added version skipIfUnsupportedMinOpsetVersion skipIfUnsupportedMaxOpsetVersion test_mod_with_low_precision ModModule torch nn Module forward x y torch fmod x y dtype=torch long x = torch tensor dtype=torch uint y = torch tensor dtype=torch uint run_test ModModule x y x = torch tensor dtype=torch int y = torch tensor dtype=torch int run_test ModModule x y x = torch tensor dtype=torch int y = torch tensor dtype=torch int run_test ModModule x y x = torch tensor dtype=torch uint y = torch tensor dtype=torch int run_test ModModule x y x = torch tensor dtype=torch uint y = torch tensor dtype=torch float run_test ModModule x y skipIfUnsupportedMinOpsetVersion test_empty_constant_shape Zeros torch nn Module forward x y = torch zeros y += x y x = torch tensor run_test Zeros x Ones torch nn Module forward x y = torch ones y += x y x = torch tensor run_test Ones x Full torch nn Module forward x y = torch full y += x y x = torch tensor run_test Full x Empty torch nn Module forward x y = torch empty fill_ y += x y x = torch tensor run_test Empty x test_std StandardDeviation torch nn Module forward input torch std input unbiased=False x = torch randn model = StandardDeviation run_test model x StandardDeviationUnbiased torch nn Module forward input torch std input unbiased=True model = StandardDeviationUnbiased run_test model x test_std_along_dims StandardDeviation torch nn Module forward input torch std input dim= unbiased=False x = torch randn model = StandardDeviation run_test model x StandardDeviationUnbiased torch nn Module forward input torch std input dim= unbiased=True x = torch randn model = StandardDeviationUnbiased run_test model x test_std_keepdim StandardDeviation torch nn Module forward input torch std input dim= unbiased=False keepdim=True x = torch randn model = StandardDeviation run_test model x StandardDeviationUnbiased torch nn Module forward input torch std input dim= unbiased=True keepdim=True x = torch randn model = StandardDeviationUnbiased run_test model x test_std_correction StandardDeviation torch nn Module forward input torch std input dim= correction= keepdim=True x = torch randn model = StandardDeviation run_test model x test_var Variance torch nn Module forward input torch var input unbiased=False x = torch randn model = Variance run_test model x VarianceUnbiased torch nn Module forward input torch var input unbiased=True model = VarianceUnbiased run_test model x VarianceSqrt torch nn Module forward input y = torch var input torch sqrt y + e- x = torch randn model = VarianceSqrt run_test model x test_var_along_dims Variance torch nn Module forward input torch var input dim= unbiased=False x = torch randn model = Variance run_test model x VarianceUnbiased torch nn Module forward input torch var input dim= unbiased=True x = torch randn model = VarianceUnbiased run_test model x test_var_keepdim Variance torch nn Module forward input torch var input dim= unbiased=False keepdim=True x = torch randn model = Variance run_test model x VarianceUnbiased torch nn Module forward input torch var input dim= unbiased=True keepdim=True x = torch randn model = VarianceUnbiased run_test model x test_var_correction Variance torch nn Module forward input torch var input dim= correction= keepdim=True x = torch randn model = Variance run_test model x test_var_mean Variance torch nn Module forward input torch var_mean input unbiased=False x = torch randn model = Variance run_test model x VarianceUnbiased torch nn Module forward input torch var_mean input unbiased=True model = VarianceUnbiased run_test model x test_var_mean_along_dims Variance torch nn Module forward input torch var_mean input dim= unbiased=False x = torch randn model = Variance run_test model x VarianceUnbiased torch nn Module forward input torch var_mean input dim= unbiased=True x = torch randn model = VarianceUnbiased run_test model x test_var_mean_mixed_dims ReverseDims torch nn Module forward input torch var_mean input dim= unbiased=False x = torch randn model = ReverseDims run_test model x SkipDims torch nn Module forward input torch var_mean input dim= unbiased=False x = torch randn model = SkipDims run_test model x NonZeroDims torch nn Module forward input torch var_mean input dim= unbiased=False x = torch randn model = NonZeroDims run_test model x test_var_mean_keepdim Variance torch nn Module forward input torch var_mean input dim= unbiased=False keepdim=True x = torch randn model = Variance run_test model x VarianceUnbiased torch nn Module forward input torch var_mean input dim= unbiased=True keepdim=True x = torch randn model = VarianceUnbiased run_test model x test_var_mean_correction Variance torch nn Module forward input torch var_mean input dim= correction= keepdim=True x = torch randn model = Variance run_test model x test_std_mean StandardDeviation torch nn Module forward input torch std_mean input unbiased=False x = torch randn model = StandardDeviation run_test model x StandardDeviationUnbiased torch nn Module forward input torch std_mean input unbiased=True model = StandardDeviationUnbiased run_test model x test_std_mean_along_dims StandardDeviation torch nn Module forward input torch std_mean input dim= unbiased=False x = torch randn model = StandardDeviation run_test model x VarianceUnbiased torch nn Module forward input torch std_mean input dim= unbiased=True x = torch randn model = VarianceUnbiased run_test model x test_std_mean_keepdim StandardDeviation torch nn Module forward input torch std_mean input dim= unbiased=False keepdim=True x = torch randn model = StandardDeviation run_test model x StandardDeviationUnbiased torch nn Module forward input torch std_mean input dim= unbiased=True keepdim=True x = torch randn model = StandardDeviationUnbiased run_test model x test_std_mean_correction StandardDeviation torch nn Module forward input torch var_mean input dim= correction= keepdim=True x = torch randn model = StandardDeviation run_test model x test_bitshift BitshiftModel torch nn Module forward input input input input torch tensor input input = torch arange dtype=torch int reshape run_test BitshiftModel input skipIfUnsupportedMinOpsetVersion test_bitwise_and BitwiseAndModel torch nn Module forward input other input torch bitwise_and input other other torch tensor dtype=torch int input = torch randint dtype=torch uint other = torch randint - dtype=torch int run_test BitwiseAndModel input other uint implemented ORT Mul used exporting bitshift opset_version skipIfUnsupportedMinOpsetVersion test_bitshift_uint BitshiftModel torch nn Module forward input input input input input torch tensor dtype=torch uint input input = torch arange dtype=torch uint reshape input = torch arange dtype=torch uint reshape run_test BitshiftModel input input test_narrow NarrowModel torch nn Module forward input torch narrow input x = torch randn requires_grad=True run_test NarrowModel x skipIfUnsupportedMinOpsetVersion test_narrow_dynamic NarrowModel torch nn Module forward input torch narrow input input shape - x = torch randn requires_grad=True run_test NarrowModel x skipIfUnsupportedMinOpsetVersion test_index_fill IndexFillModel torch nn Module forward input index = torch tensor input index_fill index - x = torch randn requires_grad=True run_test IndexFillModel x skipIfUnsupportedMinOpsetVersion test_index_copy IndexCopyModel torch nn Module __init__ dim super __init__ dim = dim forward input index = torch tensor source = torch ones input index_copy dim index source x = torch randn requires_grad=True dim - run_test IndexCopyModel dim x test_select Select torch nn Module forward x x x = torch randn run_test Select x test_select_negative_index Select torch nn Module forward x x - x = torch randn run_test Select x test_index_select_constant_scaler_index IndexSelectScalerIndexModel torch nn Module forward x index = torch index_select x torch tensor index x = torch randn run_test IndexSelectScalerIndexModel x test_index_select_scaler_index IndexSelectScalerIndexModel torch nn Module __init__ index_base super __init__ index_base = torch tensor index_base forward x index_offset index = index_base + index_offset torch index_select x index x = torch randn offset = index_offset = torch tensor offset base = run_test IndexSelectScalerIndexModel base x index_offset test_take TakeModel torch nn Module forward x y torch take x y x = torch randn y = torch tensor run_test TakeModel x y test_topk MyModule torch nn Module forward x torch topk x x = torch arange requires_grad=True run_test MyModule x skipIfUnsupportedMinOpsetVersion test_topk_int _k Model torch nn Module forward x k torch topk x k x = torch arange k = torch tensor dtype=torch int run_test Model x k skipIfUnsupportedMinOpsetVersion test_topk_smallest_unsorted MyModule torch nn Module forward x k When sorted=False order elements output tensors expected match between PyTorch ORT topk_unsorted = torch topk x k largest=False sorted=False topk_sorted = torch topk x k largest=False sorted=True topk_sorted torch sort topk_unsorted values values x = torch arange requires_grad=True k = torch tensor run_test MyModule x k skipIfUnsupportedMinOpsetVersion test_topk_script MyModuleDynamic torch jit ScriptModule torch jit script_method forward x k torch topk x k x = torch arange requires_grad=True k = torch tensor run_test MyModuleDynamic x k skipScriptTest Python builtin apply FunctionMeta object currently supported Torchscript skipIfUnsupportedMinOpsetVersion Clip op min input since opset test_auto_grad MyClip torch autograd Function staticmethod forward ctx input scalar ctx save_for_backward input input clamp min=scalar MyRelu torch autograd Function staticmethod forward ctx input ctx save_for_backward input input clamp min= symbolic_python_op g args kwargs name = kwargs name name == MyClip g op Clip args args name == MyRelu g op Relu args TODO justinchuby Remove reference internal names symbolic_helper torch onnx symbolic_helper _unimplemented prim PythonOp unknown node kind + name torch onnx register_custom_op_symbolic prim PythonOp symbolic_python_op addCleanup torch onnx unregister_custom_op_symbolic prim PythonOp MyClipModule torch nn Module forward x min MyClip apply x min x = torch randn min = torch tensor run_test MyClipModule x min MyReluModule torch nn Module forward x MyRelu apply x x = torch randn run_test MyReluModule x test_clip_int MyClipInt torch nn Module forward x torch clamp x run_test MyClipInt torch randn torch int test_relu_int run_test torch nn ReLU torch randn torch int test_pad_int MyPadInt torch nn Module forward x torch nn functional pad x run_test MyPadInt torch randn torch int test_min_int MyMinInt torch nn Module forward x torch min x x + run_test MyMinInt torch randn torch int test_max_int MyMaxnInt torch nn Module forward x torch max x x + run_test MyMaxnInt torch randn torch int skipIfUnsupportedOpsetVersion test_normalize Model torch nn Module forward x torch nn functional normalize x x = torch randn run_test Model x test_norm_with_dtype Model torch nn Module forward x TODO bowbao There slight gap today s test infrastructure directly test aten ops OpInfo ` torch norm ` ` ` common_methods_invocations py ` will decompose below aten op torch ops aten norm x p= dim= keepdim=True dtype=torch float x = torch randn run_test Model x test_layer_norm As layer_norm works last D dimension please keep test case least three dimension prevent situation axis= mapping same axis axis=- elementwise_affine True False bias True False model = torch nn LayerNorm elementwise_affine=elementwise_affine bias=bias x = torch randn run_test model x test_batchnorm d x = torch randn model = torch nn BatchNorm d affine=True run_test model x x = torch randn run_test model x test_batchnorm d_noaffine x = torch randn model = torch nn BatchNorm d affine=False run_test model x x = torch randn run_test model x test_batchnorm d_norunningstats x = torch randn model = torch nn BatchNorm d track_running_stats=False run_test model x x = torch randn run_test model x test_batchnorm d x = torch randn model = torch nn BatchNorm d affine=True run_test model x test_batchnorm d_noaffine x = torch randn model = torch nn BatchNorm d affine=False run_test model x test_batchnorm d_norunningstats x = torch randn model = torch nn BatchNorm d track_running_stats=False run_test model x test_batchnorm d x = torch randn model = torch nn BatchNorm d affine=True run_test model x test_batchnorm d_noaffine x = torch randn model = torch nn BatchNorm d affine=False run_test model x skipIfUnsupportedMinOpsetVersion Because ConstantOfShape op supported opset test_instancenorm d_runningstats x = torch randn model = torch nn InstanceNorm d affine=True track_running_stats=True run_test model x model = torch nn InstanceNorm d affine=False track_running_stats=True run_test model x test_instancenorm d_norunningstats x = torch randn model = torch nn InstanceNorm d affine=True track_running_stats=False run_test model x model = torch nn InstanceNorm d affine=False track_running_stats=False run_test model x skipIfUnsupportedMinOpsetVersion Because ConstantOfShape op supported opset test_instancenorm d_runningstats x = torch randn model = torch nn InstanceNorm d affine=True track_running_stats=True run_test model x model = torch nn InstanceNorm d affine=False track_running_stats=True run_test model x test_instancenorm d_norunningstats x = torch randn model = torch nn InstanceNorm d affine=True track_running_stats=False run_test model x model = torch nn InstanceNorm d affine=False track_running_stats=False run_test model x skipIfUnsupportedMinOpsetVersion Because ConstantOfShape op supported opset test_instancenorm d_runningstats x = torch randn model = torch nn InstanceNorm d affine=True track_running_stats=True run_test model x model = torch nn InstanceNorm d affine=False track_running_stats=True run_test model x test_instancenorm d_norunningstats x = torch randn model = torch nn InstanceNorm d affine=True track_running_stats=False run_test model x model = torch nn InstanceNorm d affine=False track_running_stats=False run_test model x skipIfUnsupportedMinOpsetVersion test_scatter_with_scalar ScatterModel torch nn Module forward input indices values = input scatter indices values input = torch tensor dtype=torch float indices = torch tensor dtype=torch int run_test ScatterModel input_args= input indices skipIfUnsupportedMinOpsetVersion test_scatter_with_scalar_different_types Tests case when scalar src updates values type different type Happens only scalar src - PyTorch does allow when src tensor ScatterModel torch nn Module forward input indices values = input scatter indices values input = torch tensor dtype=torch float indices = torch tensor dtype=torch int run_test ScatterModel input_args= input indices skipIfUnsupportedMinOpsetVersion test_scatter ScatterModel torch nn Module forward input indices values input scatter indices values input = torch tensor indices = torch tensor dtype=torch int values = torch tensor run_test ScatterModel input_args= input indices values input = torch tensor indices = torch tensor dtype=torch int values = torch tensor run_test ScatterModel input indices values input = torch zeros indices = torch tensor dtype=torch int indices = indices view expand values = torch arange dtype=torch float view run_test ScatterModel input indices values input = torch zeros indices = torch tensor values = torch arange dtype=torch float view run_test ScatterModel input indices values skipIfUnsupportedMinOpsetVersion test_scatter_add ScatterModel torch nn Module forward input indices values input scatter_add indices values input = torch tensor indices = torch tensor dtype=torch int values = torch tensor run_test ScatterModel input_args= input indices values torch jit script scatter_sum src Tensor index Tensor size = src size out = torch zeros size dtype=src dtype out scatter_add_ index src ScatterModel torch nn Module forward src index scatter_sum src index src = torch rand index = torch tensor dtype=torch int run_test ScatterModel src index skipIfUnsupportedMinOpsetVersion test_scatter_add_index_not_unique ScatterModel torch nn Module forward input indices values input scatter_add indices values input = torch tensor indices = torch tensor dtype=torch int values = torch tensor run_test ScatterModel input_args= input indices values torch jit script scatter_sum src Tensor index Tensor size = src size out = torch zeros size dtype=src dtype out scatter_add_ index src ScatterModel torch nn Module forward src index scatter_sum src index src = torch rand index = torch tensor dtype=torch int run_test ScatterModel src index skipIfUnsupportedMinOpsetVersion test_scatter_add_different_size_index_src ScatterModel torch nn Module forward input indices src input scatter_add indices src src = torch ones input = torch zeros dtype=src dtype indices = torch tensor run_test ScatterModel input_args= input indices src common_utils parametrize src indices common_utils subtest torch ones torch tensor name= src_indices_dynamic_combination common_utils subtest torch ones torch tensor name= src_indices_dynamic_combination common_utils subtest torch ones torch tensor name= src_indices_dynamic_combination common_utils subtest torch ones torch tensor name= src_indices_dynamic_combination skipIfUnsupportedMinOpsetVersion test_scatter_add_dynamic_index src indices ScatterModel torch nn Module forward input indices src input scatter_add indices src input = torch zeros dtype=src dtype run_test ScatterModel input_args= input indices src input_names= input indices src dynamic_axes= indices b src c d skipIfUnsupportedMinOpsetVersion test_scatter_reduce Model torch nn Module __init__ - None super __init__ forward x index input y_max = input scatter_reduce index x reduce= amax y_sum = input scatter_reduce index x reduce= sum y_min = input scatter_reduce index x reduce= amin y_mul = input scatter_reduce index x reduce= prod y_max y_sum y_min y_mul model = Model model eval src = torch tensor index = torch tensor input = torch tensor run_test model src index input skipIfUnsupportedMinOpsetVersion test_scatter_reduce_self_rank_zero Model torch nn Module __init__ - None super __init__ forward x index input y_max = input scatter_reduce index x reduce= amax y_sum = input scatter_reduce index x reduce= sum y_min = input scatter_reduce index x reduce= amin y_mul = input scatter_reduce index x reduce= prod y_max y_sum y_min y_mul model = Model model eval empty_tensor = torch tensor empty_idx = torch tensor dtype=torch int run_test model empty_tensor empty_idx empty_tensor skipIfUnsupportedMinOpsetVersion test_bucketize BucketModel torch nn Module forward input boundaries torch bucketize input boundaries torch bucketize input boundaries right=True input = torch tensor boundaries = torch tensor run_test BucketModel input boundaries skipIfUnsupportedMinOpsetVersion test_one_hot OneHot torch nn Module __init__ num_classes super __init__ num_classes = num_classes forward x torch nn functional one_hot x num_classes x = torch arange run_test OneHot x OneHot torch nn Module forward x num_classes num_classes = num_classes torch int torch nn functional one_hot x num_classes x = torch arange num_classes = torch ones run_test OneHot x num_classes skipIfUnsupportedMinOpsetVersion test_gather GatherModel torch nn Module forward input indices input gather indices input = torch tensor indices = torch tensor dtype=torch int run_test GatherModel input_args= input indices skipScriptTest Scripting error Cannot instantiate nn module test_gather_constant_fold GatherModule torch nn Module __init__ - None super __init__ weight = torch nn Buffer torch ones torch nn Embedding converted ONNX Gather Constant folding will triggered constant inputs This pattern common constant mask inputs transformer models embed = torch nn Embedding forward x shape rank shape = weight shape m = - shape y = torch ones dtype=torch long x clamp min=m embed y x = torch randn run_test GatherModule x GatherModule torch nn Module __init__ - None super __init__ weight = torch nn Buffer torch ones forward x shape rank shape = weight shape pad = shape shape shape zero_pad = torch nn ZeroPad d pad zero_pad x x = torch randn run_test GatherModule x GatherModule torch nn Module __init__ - None super __init__ rb = torch nn Buffer torch randn forward x x += rb x x = torch randn run_test GatherModule x dynamic_axes= input batch height width output batch height width input_names= input output_names= output skipIfUnsupportedOpsetVersion skipIfUnsupportedMinOpsetVersion test_expand ExpandModel torch nn Module forward input input expand - input = torch randn run_test ExpandModel input_args= input ExpandInferDimModel torch nn Module forward input input expand - input size input = torch randn run_test ExpandInferDimModel input_args= input ExpandTensorSizeModel torch nn Module forward input size input expand size input = torch randn size = torch tensor - run_test ExpandTensorSizeModel input_args= input size skipIfUnsupportedMinOpsetVersion index_put supported opsets = test_dynamic_expand_as Model torch nn Module forward x x x size = x x = torch ones x = torch randn run_test Model x input_names= x dynamic_axes= x additional_test_inputs= x Model torch nn Module forward x x x size = torch tensor x x = torch ones x = torch randn run_test Model x input_names= x dynamic_axes= x additional_test_inputs= x Model torch nn Module forward x aa = torch tensor aa expand_as x x = torch ones x = torch randn run_test Model x input_names= x dynamic_axes= x additional_test_inputs= x test_multinomial Multinomial torch nn Module forward weight torch multinomial weight replacement=True MultinomialNoReplacement torch nn Module forward weight torch multinomial weight weight = torch tensor dtype=torch float run_test Multinomial weight run_test MultinomialNoReplacement weight _test_reduced_ops op ReducedOpModule torch nn Module forward input op input dim=- op = torch mean torch mean only supports float types x = torch randint dtype=torch uint run_test ReducedOpModule x x = torch randint dtype=torch int run_test ReducedOpModule x x = torch randint dtype=torch int run_test ReducedOpModule x x = torch randint dtype=torch int run_test ReducedOpModule x x = torch randint dtype=torch int run_test ReducedOpModule x torch mean only supports float types ORT does support double ReduceProd double op = torch prod op = torch mean x = torch randn dtype=torch double run_test ReducedOpModule x op = torch prod torch prod implemented Half x = torch randn dtype=torch half run_test ReducedOpModule x x = torch randn dtype=torch float run_test ReducedOpModule x test_reduced_sum _test_reduced_ops op=torch sum test_reduced_mean _test_reduced_ops op=torch mean test_reduced_prod _test_reduced_ops op=torch prod test_reduced_sum_dtypes NoDimModel torch nn Module forward input input sum dtype=torch float DimModel torch nn Module forward input input sum dim=- dtype=torch float input = torch randn dtype=torch half run_test NoDimModel input run_test DimModel input test_reduced_min_max ReducedMinMaxModule torch nn Module forward input torch min input dim=- torch max input dim= x = torch randint dtype=torch int run_test ReducedMinMaxModule x x = torch randint dtype=torch int run_test ReducedMinMaxModule x x = torch randn dtype=torch float run_test ReducedMinMaxModule x test_reduce_log_sum_exp ReduceLogSumExpModel torch nn Module forward input = torch logsumexp input dim= b = torch logsumexp input dim= + b x = torch randn requires_grad=True run_test ReduceLogSumExpModel x test_softmax i range - model = torch nn Softmax dim=i input = torch randn run_test model input SoftmaxUnknownRank torch nn Module __init__ i super __init__ softmax = torch nn Softmax dim=i forward x softmax x reshape model = torch jit script SoftmaxUnknownRank i run_test model input test_softmax_large_values input = torch tensor - e - e - e e - i range - model = torch nn Softmax dim=i run_test model input SoftmaxUnknownRank torch nn Module __init__ i super __init__ softmax = torch nn Softmax dim=i forward x softmax x reshape model = torch jit script SoftmaxUnknownRank i run_test model input test_logsoftmax i range model = torch nn LogSoftmax dim=i - dims = i - + input = torch ones dims requires_grad=True run_test model input test_logsoftmax_dim i range - model = torch nn LogSoftmax dim=i input = torch randn run_test model input test_logsoftmax_dtype Model torch nn Module forward x torch nn functional log_softmax x dim= dtype=torch float x = torch randn requires_grad=True run_test Model x test_softplus BetaOneModel torch nn Module forward x torch nn functional softplus x x = torch randn requires_grad=True run_test BetaOneModel x BetaModel torch nn Module forward x torch nn functional softplus x beta= x = torch randn requires_grad=True run_test BetaModel x BetaFloatModel torch nn Module forward x torch nn functional softplus x beta= x = torch randn requires_grad=True run_test BetaFloatModel x skipIfUnsupportedMinOpsetVersion test_lstm_no_hidden LSTMModel torch nn Module __init__ - None super __init__ rnn = torch nn LSTM input_size= hidden_size= forward x rnn x input = torch randn run_test LSTMModel input skipIfUnsupportedMinOpsetVersion test_lstm_proj_no_hidden LSTMModel torch nn Module __init__ - None super __init__ rnn = torch nn LSTM input_size= hidden_size= proj_size= forward x rnn x input = torch randn assertRaises RuntimeError run_test LSTMModel input skipIfUnsupportedMinOpsetVersion test_lstm LSTMModel torch nn Module __init__ - None super __init__ rnn = torch nn LSTM RNN_INPUT_SIZE RNN_HIDDEN_SIZE bidirectional=False forward x h c rnn x h c input = torch randn RNN_SEQUENCE_LENGTH BATCH_SIZE RNN_INPUT_SIZE h = torch randn BATCH_SIZE RNN_HIDDEN_SIZE c = torch randn BATCH_SIZE RNN_HIDDEN_SIZE run_test LSTMModel input h c skipIfUnsupportedMinOpsetVersion test_lstm_cell LSTMCellModel torch nn Module __init__ bias super __init__ lstm_cell = torch nn LSTMCell RNN_INPUT_SIZE RNN_HIDDEN_SIZE bias=bias forward x h c lstm_cell x h c input = torch randn BATCH_SIZE RNN_INPUT_SIZE h = torch randn BATCH_SIZE RNN_HIDDEN_SIZE c = torch randn BATCH_SIZE RNN_HIDDEN_SIZE bias True False run_test LSTMCellModel bias input h c skipIfUnsupportedMinOpsetVersion test_lstm_default_init_state LSTMModel torch nn Module __init__ - None super __init__ rnn = torch nn LSTM RNN_INPUT_SIZE RNN_HIDDEN_SIZE bidirectional=False forward x rnn x input = torch randn RNN_SEQUENCE_LENGTH BATCH_SIZE RNN_INPUT_SIZE run_test LSTMModel input skipIfUnsupportedMinOpsetVersion test_lstm_fixed_batch_size LSTMModel torch nn Module __init__ - None super __init__ lstm = torch nn LSTM RNN_INPUT_SIZE RNN_HIDDEN_SIZE bidirectional=False RNN_HIDDEN_SIZE = RNN_HIDDEN_SIZE forward input batch_size = input size h = torch ones batch_size RNN_HIDDEN_SIZE c = torch ones batch_size RNN_HIDDEN_SIZE lstm input h c input = torch randn RNN_SEQUENCE_LENGTH BATCH_SIZE RNN_INPUT_SIZE verify different input same batch size input = torch randn RNN_SEQUENCE_LENGTH BATCH_SIZE RNN_INPUT_SIZE run_test LSTMModel input fixed_batch_size=True additional_test_inputs= input skipIfUnsupportedMinOpsetVersion test_lstm_post_fix_init_state LSTMModel torch nn Module __init__ - None super __init__ lstm = torch nn LSTM RNN_INPUT_SIZE RNN_HIDDEN_SIZE bidirectional=False RNN_HIDDEN_SIZE = RNN_HIDDEN_SIZE forward input batch_size = input size h = torch ones batch_size RNN_HIDDEN_SIZE c = torch ones batch_size RNN_HIDDEN_SIZE lstm input h c model = LSTMModel input = torch randn RNN_SEQUENCE_LENGTH RNN_INPUT_SIZE verify different input different batch size input = torch randn RNN_SEQUENCE_LENGTH BATCH_SIZE RNN_INPUT_SIZE run_test model input input_names= input dynamic_axes= input seq batch additional_test_inputs= input test_lstm_constant_folding LstmNet torch nn Module __init__ input_size hidden_size num_layers bidirectional super __init__ lstm = torch nn LSTM input_size hidden_size num_layers bidirectional=bidirectional forward input initial_state tuple Tensor Tensor lstm input initial_state get_LstmNet_model_and_inputs input_size hidden_size num_layers batch_size seq_len bidirectional num_directions = bidirectional model = LstmNet input_size hidden_size num_layers bidirectional input = torch randn seq_len batch_size input_size h = torch randn num_layers num_directions batch_size hidden_size c = torch randn num_layers num_directions batch_size hidden_size model input h c batch_size = model input = get_LstmNet_model_and_inputs batch_size True run_test model input do_constant_folding=True batch_size = model input = get_LstmNet_model_and_inputs batch_size False run_test model input do_constant_folding=True skipIfUnsupportedMinOpsetVersion test_lstm_no_bias LstmNet torch nn Module __init__ num_layers bidirectional super __init__ lstm = torch nn LSTM RNN_INPUT_SIZE RNN_HIDDEN_SIZE num_layers bias=False bidirectional=bidirectional forward input initial_state tuple Tensor Tensor lstm input initial_state get_LstmNet_model_and_inputs num_layers bidirectional input = torch randn RNN_SEQUENCE_LENGTH BATCH_SIZE RNN_INPUT_SIZE num_directions = bidirectional model = LstmNet num_layers bidirectional h = torch randn num_layers num_directions BATCH_SIZE RNN_HIDDEN_SIZE c = torch randn num_layers num_directions BATCH_SIZE RNN_HIDDEN_SIZE model input h c num_layers = bidirectional = True False True False models_and_inputs = get_LstmNet_model_and_inputs n b n b zip num_layers bidirectional model input models_and_inputs run_test model input skipIfUnsupportedMinOpsetVersion test_lstm_sequence LstmNet torch nn Module __init__ - None super __init__ rnn = torch nn LSTM bidirectional=True batch_first=True linear = torch nn Linear rnn = torch nn LSTM bidirectional=True batch_first=True linear = torch nn Linear forward input rnn_output _ = rnn input linear_output = linear rnn_output rnn_output _ = rnn linear_output linear_output = linear rnn_output linear_output input = torch zeros dtype=torch float run_test LstmNet input input_names= input output_names= output dynamic_axes= input batch_size w h output batch_size w h skipScriptTest test_rnn_no_bias make_model layers packed_sequence batch_first = packed_sequence == model = torch nn RNN RNN_INPUT_SIZE RNN_HIDDEN_SIZE layers bidirectional=False batch_first=batch_first bias=False packed_sequence == model = rnn_model_with_packed_sequence RnnModelWithPackedSequence model False packed_sequence == model = rnn_model_with_packed_sequence RnnModelWithPackedSequence model True model make_input batch_size layers packed_sequence batch_first = packed_sequence == seq_lengths = np random randint RNN_SEQUENCE_LENGTH + size=batch_size seq_lengths = sorted map int seq_lengths reverse=True inputs = torch randn l RNN_INPUT_SIZE l seq_lengths inputs = rnn_utils pad_sequence inputs batch_first=batch_first inputs = inputs h = torch randn layers batch_size RNN_HIDDEN_SIZE inputs append h packed_sequence = inputs append torch IntTensor seq_lengths len inputs == input = inputs input = tuple inputs input layers = packed_sequence = models = make_model l p l p zip layers packed_sequence inputs = make_input RNN_BATCH_SIZE l p l p zip layers packed_sequence model input zip models inputs run_test model input test_gru_no_bias GruNet torch nn Module __init__ input_size hidden_size num_layers bidirectional super __init__ mygru = torch nn GRU input_size hidden_size num_layers bidirectional=bidirectional bias=False forward input initial_state out = mygru input initial_state out get_GruNet_model_and_inputs input_size hidden_size num_layers batch_size seq_len bidirectional num_directions = bidirectional model = GruNet input_size hidden_size num_layers bidirectional input = torch randn seq_len batch_size input_size h = torch randn num_layers num_directions batch_size hidden_size model input h input_size = hidden_size = num_layers = batch_size = seq_len = bidirectional = True False models_and_inputs = get_GruNet_model_and_inputs i h n b s bi i h n b s bi zip input_size hidden_size num_layers batch_size seq_len bidirectional model input models_and_inputs run_test model input do_constant_folding=True test_gru_constant_folding GruNet torch nn Module __init__ input_size hidden_size num_layers bidirectional super __init__ mygru = torch nn GRU input_size hidden_size num_layers bidirectional=bidirectional forward input initial_state out = mygru input initial_state out get_GruNet_model_and_inputs input_size hidden_size num_layers batch_size seq_len bidirectional num_directions = bidirectional model = GruNet input_size hidden_size num_layers bidirectional input = torch randn seq_len batch_size input_size h = torch randn num_layers num_directions batch_size hidden_size model input h batch_size = model input = get_GruNet_model_and_inputs batch_size True run_test model input do_constant_folding=True batch_size = model input = get_GruNet_model_and_inputs batch_size False run_test model input do_constant_folding=True skipIfUnsupportedMinOpsetVersion test_max_tensors MaxModel torch nn Module forward input other torch max input other model = MaxModel x = torch randn requires_grad=True y = torch randn requires_grad=True run_test model x y test_amax_amin Model torch nn Module forward x torch amax x dim= keepdim=True torch amin x dim= keepdim=False model = Model x = torch randn run_test model x test_aminmax Model torch nn Module forward x torch aminmax x dim= keepdim=True torch aminmax x keepdim=False model = Model x = torch randn run_test model x skipIfUnsupportedMinOpsetVersion test_arange_end ArangeScript torch jit ScriptModule torch jit script_method forward torch arange size dtype=torch float view - + x = torch randn requires_grad=True outputs = ArangeScript x run_test ArangeScript x ArangeModel torch nn Module forward torch arange size dtype=torch float view - + run_test ArangeModel x skipIfUnsupportedMinOpsetVersion test_arange_end_notype ArangeScript torch jit ScriptModule torch jit script_method forward torch arange size x = torch randn requires_grad=True outputs = ArangeScript x run_test ArangeScript x input_names= x dynamic_axes= x run_test ArangeScript x remained_onnx_input_idx= ArangeModel torch nn Module forward torch arange size run_test ArangeModel x input_names= x dynamic_axes= x run_test ArangeModel x remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_arange_start_end ArangeScript torch jit ScriptModule torch jit script_method forward torch arange size + dtype=torch float view - + x = torch randn requires_grad=True run_test ArangeScript x ArangeModel torch nn Module forward torch arange size + dtype=torch float view - + run_test ArangeModel x skipIfUnsupportedMinOpsetVersion test_arange_start_end_notype ArangeScript torch jit ScriptModule torch jit script_method forward torch arange size + view - + x = torch randn requires_grad=True run_test ArangeScript x ArangeModel torch nn Module forward torch arange size + view - + run_test ArangeModel x skipIfUnsupportedMinOpsetVersion test_arange_start_end_step ArangeScript torch jit ScriptModule torch jit script_method forward torch arange size size + size dtype=torch float view - + x = torch randn requires_grad=True run_test ArangeScript x ArangeModel torch nn Module forward torch arange size size + size dtype=torch float view - + run_test ArangeModel x skipIfUnsupportedMinOpsetVersion test_arange_start_end_step_notype ArangeScript torch jit ScriptModule torch jit script_method forward torch arange size size + size view - + x = torch randn requires_grad=True run_test ArangeScript x ArangeModel torch nn Module forward torch arange size size + size view - + run_test ArangeModel x skipIfUnsupportedMinOpsetVersion test__dim_arange DimArange torch nn Module forward input torch _dim_arange input x = torch ones run_test DimArange x input_names= x dynamic_axes= x remained_onnx_input_idx = None opset_version run_test DimArange x remained_onnx_input_idx=remained_onnx_input_idx _test_compare_ops model num_inputs x_float = torch randn requires_grad=True x_int = torch randint dtype=torch int num_inputs y_float = torch randn requires_grad=True y_int = torch randint dtype=torch int run_test model x_float y_float run_test model x_float y_int run_test model x_int y_float run_test model x_int y_int run_test model x_float run_test model x_int skipIfUnsupportedMinOpsetVersion test_and_or_xor MyModel torch nn Module forward x y x ^ y x &#124; y x y ~x x = torch randint dtype=torch bool y = torch randint dtype=torch bool run_test MyModel input_args= x y skipIfUnsupportedMinOpsetVersion test_logical_and AndModel torch nn Module forward x y torch logical_and x y x = torch randint dtype=torch bool y = torch randint dtype=torch bool run_test AndModel input_args= x y x = torch randint dtype=torch int y = torch randint dtype=torch int run_test AndModel input_args= x y x = torch randint dtype=torch double y = torch randint dtype=torch double run_test AndModel input_args= x y x = torch randint dtype=torch float y = torch randint dtype=torch long run_test AndModel input_args= x y skipIfUnsupportedMinOpsetVersion test_logical_or OrModel torch nn Module forward x y torch logical_or x y x = torch randint dtype=torch bool y = torch randint dtype=torch bool run_test OrModel input_args= x y x = torch randint dtype=torch int y = torch randint dtype=torch int run_test OrModel input_args= x y x = torch randint dtype=torch double y = torch randint dtype=torch double run_test OrModel input_args= x y x = torch randint dtype=torch float y = torch randint dtype=torch long run_test OrModel input_args= x y skipIfUnsupportedMinOpsetVersion test_logical_xor XorModel torch nn Module forward x y torch logical_xor x y x = torch randint dtype=torch bool y = torch randint dtype=torch bool run_test XorModel input_args= x y x = torch randint dtype=torch int y = torch randint dtype=torch int run_test XorModel input_args= x y x = torch randint dtype=torch double y = torch randint dtype=torch double run_test XorModel input_args= x y x = torch randint dtype=torch float y = torch randint dtype=torch long run_test XorModel input_args= x y skipIfUnsupportedMinOpsetVersion test_logical_not NotModel torch nn Module forward x torch logical_not x x = torch randint dtype=torch bool run_test NotModel input_args= x x = torch randint dtype=torch int run_test NotModel input_args= x x = torch randint dtype=torch double run_test NotModel input_args= x x = torch randint dtype=torch float run_test NotModel input_args= x skipIfUnsupportedMinOpsetVersion float equal added after opset test_eq EqualModel torch nn Module forward input other input == other _test_compare_ops EqualModel test_gt GreaterModel torch nn Module forward input other input other _test_compare_ops GreaterModel skipIfUnsupportedMinOpsetVersion test_ge GreaterOrEqualModel torch nn Module forward input other input = other _test_compare_ops GreaterOrEqualModel test_gt_scalar GreaterModel torch nn Module forward input input _test_compare_ops GreaterModel test_gt_primitive GreaterModel torch nn Module __init__ - None super __init__ y int = forward x int y x x = run_test GreaterModel x skipIfUnsupportedMinOpsetVersion test_ge_scalar GreaterOrEqualModel torch nn Module forward input input = _test_compare_ops GreaterOrEqualModel test_lt LessModel torch nn Module forward input other input other _test_compare_ops LessModel skipIfUnsupportedMinOpsetVersion test_le LessOrEqualModel torch nn Module forward input other input = other _test_compare_ops LessOrEqualModel test_lt_scalar LessModel torch nn Module forward input input _test_compare_ops LessModel skipIfUnsupportedMinOpsetVersion test_le_scalar LessOrEqualModel torch nn Module forward input input = _test_compare_ops LessOrEqualModel test_matmul MatmulModel torch nn Module forward input other torch matmul input other x = torch randn requires_grad=True y = torch randn requires_grad=True run_test MatmulModel x y x = torch randint y = torch randint run_test MatmulModel x y test_matmul_batch MatmulModel torch nn Module forward input other torch matmul input other x = torch randn requires_grad=True y = torch randn requires_grad=True run_test MatmulModel x y x = torch randint y = torch randint run_test MatmulModel x y _argmin_argmax_model input ArgminArgmaxModel torch nn Module forward input torch argmin input torch argmax input torch argmin input keepdim=True torch argmax input keepdim=True torch argmin input dim= keepdim=True torch argmax input dim= keepdim=True run_test ArgminArgmaxModel input skipIfUnsupportedMinOpsetVersion test_argmin_argmax input = torch randn _argmin_argmax_model input Argmin Argmax select_last_index supported before opset select_last_index added opset deal corner case where same value appears multiple times tensor skipIfUnsupportedMinOpsetVersion test_argmin_argmax_select_last_index input = torch tensor _argmin_argmax_model input input = torch ones _argmin_argmax_model input test_repeat RepeatModel torch nn Module forward x y x = x repeat y shape y = y view - x + y x = torch tensor y = torch tensor run_test RepeatModel x y skipIfUnsupportedMinOpsetVersion test_repeat_interleave FlattenModel torch nn Module forward x x repeat_interleave shape x = torch randn shape run_test FlattenModel x DimsModel torch nn Module forward x x repeat_interleave dim= x = torch tensor run_test DimsModel x DimsModel torch nn Module forward x repeats = torch tensor torch repeat_interleave x repeats dim= x = torch tensor run_test DimsModel x RepeatsDimsModel torch nn Module forward x repeats = torch tensor torch repeat_interleave x repeats dim= x = torch tensor run_test RepeatsDimsModel x RepeatsDimsModel torch nn Module forward x repeats = torch tensor torch repeat_interleave x repeats dim= x = torch tensor run_test RepeatsDimsModel x skipIfUnsupportedMinOpsetVersion test_repeat_interleave_noop Model torch nn Module forward x x repeat_interleave dim= x = torch randn run_test Model x skipIfUnsupportedMinOpsetVersion test_dynamic_repeat_interleave SingleDynamicModel torch nn Module forward x repeats = torch tensor torch repeat_interleave x repeats dim= x = torch tensor another_x = torch tensor run_test SingleDynamicModel x additional_test_inputs= another_x input_names= input_ dynamic_axes= input_ w NegDynamicModel torch nn Module forward x repeats = torch tensor torch repeat_interleave x repeats dim=- x = torch tensor another_x = torch tensor run_test NegDynamicModel x additional_test_inputs= another_x input_names= input_ dynamic_axes= input_ w SingleDynamicModelFloat torch nn Module forward x repeats = torch tensor torch repeat_interleave x repeats dim= x = torch tensor another_x = torch tensor run_test SingleDynamicModelFloat x additional_test_inputs= another_x input_names= input_ dynamic_axes= input_ h DynamicRepeatsModel torch nn Module forward x repeats torch repeat_interleave x repeats dim= x = torch tensor another_x = torch tensor repeats = torch tensor another_repeats = torch tensor run_test DynamicRepeatsModel x repeats additional_test_inputs= another_x another_repeats input_names= input_ repeats_ dynamic_axes= input_ w repeats_ r DynamicRepeatsModel torch nn Module forward x repeats torch repeat_interleave x repeats dim= x = torch tensor repeats = torch tensor another_repeats = torch tensor run_test DynamicRepeatsModel x repeats additional_test_inputs= x another_repeats input_names= input_ repeats_ dynamic_axes= repeats_ r DynamicFlattenModel torch nn Module forward x x repeat_interleave x = torch tensor run_test DynamicFlattenModel x input_names= input_ dynamic_axes= input_ w skipIfUnsupportedMinOpsetVersion test_multiple_dynamic_repeat_interleave DynamicRepeatsModel torch nn Module forward x repeats torch repeat_interleave x repeats dim= x = torch tensor repeats = torch tensor another_repeats = torch tensor run_test DynamicRepeatsModel x repeats additional_test_inputs= x another_repeats input_names= input_ repeats_ dynamic_axes= repeats_ r DynamicRepeatsModel torch nn Module forward x repeats torch repeat_interleave x repeats dim= x = torch tensor repeats = torch tensor another_repeats = torch tensor run_test DynamicRepeatsModel x repeats additional_test_inputs= x another_repeats input_names= input_ repeats_ dynamic_axes= repeats_ r test_view ViewModel torch nn Module forward input input view x = torch randint dtype=torch int run_test ViewModel x test_view_dynamic ViewModel torch nn Module forward input other input view other shape x = torch randn shape = torch randn run_test ViewModel x shape input_names= x shape dynamic_axes= x shape run_test ViewModel x shape remained_onnx_input_idx= test_view_dynamic_zero_dim ViewModel torch nn Module forward input input = input view - input view - x = torch ones another_x = torch empty run_test ViewModel x additional_test_inputs= another_x input_names= input_ dynamic_axes= input_ test_view_as ViewModel torch nn Module forward input other input view_as other x = torch randn y = torch randn run_test ViewModel x y test_linear LinearModel torch nn Module __init__ - None super __init__ fc = torch nn Linear forward x out = fc x out = fc out out x = torch randn run_test LinearModel x LinearModel torch nn Module forward input weight bias torch nn functional linear input weight bias input rank x = torch randn y = torch randn z = torch randn run_test LinearModel x y z input rank x = torch randn y = torch randn z = torch randn run_test LinearModel x y z skipScriptTest test_weight_norm addmm -d inputs converts onnx MatMul model = torch nn utils weight_norm torch nn Linear dim= x = torch randn requires_grad=True run_test model x addmm -d inputs converts onnx Gemm model = torch nn utils weight_norm torch nn Linear dim= x = torch randn requires_grad=True run_test model x model = torch nn utils weight_norm torch nn Conv d x = torch randn requires_grad=True run_test model x model = torch nn utils weight_norm torch nn Conv d dim=- x = torch randn requires_grad=True run_test model x model = torch nn utils weight_norm torch nn Conv d name= weight x = torch randn requires_grad=True run_test model x skipScriptTest test_weight_norm_nodim addmm -d inputs converts onnx MatMul model = torch nn utils weight_norm torch nn Linear dim=None x = torch randn requires_grad=True run_test model x addmm -d inputs converts onnx Gemm model = torch nn utils weight_norm torch nn Linear dim=None x = torch randn requires_grad=True run_test model x test_flatten FlattenModel torch nn Module forward input torch flatten input model = FlattenModel flatten d input x = torch randint run_test model x flatten d input x = torch randn run_test model x flatten d input x = torch randn run_test model x test_flatten d FlattenModel torch nn Module forward input torch flatten input x = torch randint run_test FlattenModel x test_flatten d_neg FlattenModel torch nn Module forward x torch flatten x - torch flatten x - torch flatten x - x = torch randint run_test FlattenModel x skipIfUnsupportedMinOpsetVersion test_flatten_dynamic_axes MyModule torch nn Module forward x torch flatten x start_dim= end_dim= batch_size = x = torch randn batch_size y = torch randn model = MyModule run_test model x additional_test_inputs= y input_names= input output_names= output dynamic_axes= input batch_size output batch_size skipIfUnsupportedMinOpsetVersion test_getitem GetItemModel torch jit ScriptModule torch jit script_method forward x y z ind will create prim ListConstruct x y z + aten __getitem__ arr = x y z arr ind x = torch randn y = torch randn z = torch randn ind = torch tensor dtype=torch long run_test GetItemModel x y z ind ind = torch tensor - dtype=torch long run_test GetItemModel x y z ind skipDtypeChecking test_item M torch nn Module forward x y i int int x y i item x = torch arange dtype=torch float y = torch tensor dtype=torch long i = run_test torch jit script M x y i skipScriptTest torch nonzero x as_tuple=True scriptable skipIfUnsupportedMinOpsetVersion test_nonzero NonzeroModel torch nn Module forward x x nonzero x nonzero as_tuple=True x = torch randn index_fill_ torch randint view run_test NonzeroModel x test_unbind UnbindModel torch nn Module forward input _ out _ = input unbind out x = torch randn run_test UnbindModel x UnbindModel torch nn Module forward input _ out _ _ = input unbind out x = torch randn run_test UnbindModel x UnbindModel torch nn Module forward input _ out _ _ = input unbind - out x = torch randn run_test UnbindModel x skipIfUnsupportedMinOpsetVersion test_len LenModel torch jit ScriptModule torch jit script_method forward input len input unbind + input x = torch randn run_test LenModel x input_names= input dynamic_axes= input seq additional_test_inputs= torch randn skipIfUnsupportedMinOpsetVersion test_len_list LenListModel torch jit ScriptModule torch jit script_method forward input torch ones len input shape x = torch randn run_test LenListModel x remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_unbind_dynamic UnbindModel torch jit ScriptModule torch jit script_method forward input input unbind x = torch randn run_test UnbindModel x UnbindModel torch jit ScriptModule torch jit script_method forward input input unbind - x = torch randn run_test UnbindModel x skipScriptTest scripting tests run opsets See test_split_script test_split SplitModel torch nn Module forward input input split input split x = torch randn run_test SplitModel x SplitModel torch nn Module forward input input split - input split - - x = torch randn run_test SplitModel x SplitModel torch nn Module forward input input split x = torch randn run_test SplitModel x skipIfUnsupportedMinOpsetVersion test_split_script SplitModel torch nn Module forward input input split input split x = torch randn run_test SplitModel x SplitModel torch nn Module forward input input split - input split - - x = torch randn run_test SplitModel x SplitModel torch nn Module forward input input split x = torch randn run_test SplitModel x skipIfUnsupportedMinOpsetVersion skipScriptTest test_split_size_as_list SplitModel torch nn Module forward input split_sizes list int out = split_list list Tensor = input split split_sizes ob split_list out append ob noqa PERF torch cat out dim= x = torch randn split_sizes = torch tensor torch tensor run_test SplitModel x split_sizes skipIfUnsupportedMinOpsetVersion test_split_size_with_slice SplitModule torch nn Module forward x y t splits = x size y size out out = torch split t splits dim= out out x = torch randn y = torch randn t = torch randn run_test SplitModule x y t input_names= x y t dynamic_axes= x y t run_test SplitModule x y t remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_split_dynamic SplitModel torch jit ScriptModule torch jit script_method forward input input split x = torch randn run_test SplitModel x SplitModel torch jit ScriptModule torch jit script_method forward input input split - x = torch randn run_test SplitModel x skipIfUnsupportedMinOpsetVersion test_split_dynamic_axes Split torch nn Module forward x x split dim=- x = torch randn input_names = logits run_test Split x input_names=input_names dynamic_axes= input_names batch skipIfUnsupportedMinOpsetVersion test_chunk ChunkModel torch nn Module __init__ dim= super __init__ dim = dim forward x torch chunk x dim=self dim model = ChunkModel model eval model_neg_dim = ChunkModel - model_neg_dim eval x = torch randn dim_size_ range y = torch randn dim_size_ run_test model x additional_test_inputs= y input_names= x dynamic_axes= x batch_size dims run_test model_neg_dim x additional_test_inputs= y input_names= x dynamic_axes= x batch_size dims skipIfUnsupportedMinOpsetVersion test_dynamic_chunk ChunkModel torch nn Module __init__ dim= super __init__ dim = dim forward x torch chunk x x size dim=self dim model = ChunkModel model eval model_neg_dim = ChunkModel - model_neg_dim eval x = torch randn dim_size_ range y = torch randn dim_size_ run_test model x additional_test_inputs= y input_names= x dynamic_axes= x batch_size dims run_test model_neg_dim x additional_test_inputs= y input_names= x dynamic_axes= x batch_size dims test_concat ConcatModel torch nn Module forward x y z torch cat x y z x = torch randn y = torch randn z = torch randn run_test ConcatModel x y z skipIfUnsupportedMinOpsetVersion test_concat_dynamic ConcatDynamicModel torch jit ScriptModule torch jit script_method forward x torch cat x unbind x = torch randn run_test ConcatDynamicModel x test_stack StackModel torch nn Module forward x y z torch stack x y z x = torch randn y = torch randn z = torch randn run_test StackModel x y z skipIfUnsupportedMinOpsetVersion test_stack_dynamic StackDynamicModel torch jit ScriptModule torch jit script_method forward x torch stack x unbind x = torch randn run_test StackDynamicModel x test_loop_dynamic LoopModel torch jit ScriptModule torch jit script_method forward x i range x size x = x + i x model = LoopModel inputs = torch zeros dtype=torch long run_test model inputs skipIfUnsupportedMinOpsetVersion test_loop_nested NestedLoopsModel torch jit ScriptModule torch jit script_method forward x _ range = while += x = x + x model = NestedLoopsModel inputs = torch zeros dtype=torch long run_test model inputs skipIfUnsupportedMinOpsetVersion test_loop_with_list ListLoopModel torch jit ScriptModule torch jit script_method forward x res = res = arr = x split res = torch zeros dtype=torch long res = res = i range len arr res append arr i sum False res append arr - - i sum False res += res = res + arr i sum False res += arr - - i sum False res res res torch stack res torch stack res model = ListLoopModel inputs = torch randn run_test model inputs skipIfUnsupportedMinOpsetVersion test_loop_transpose LoopModel torch nn Module forward x res = torch zeros_like x _ range x size res += x transpose res model = torch jit script LoopModel x = torch randn run_test model x skipIfUnsupportedMinOpsetVersion test_loop_multi_dim LoopMultiDimModel torch jit ScriptModule torch jit script_method forward x y x_ torch flip x narrow y = x_ y y model = LoopMultiDimModel x = torch randint dtype=torch long y = torch ones dtype=torch long run_test model x y skipIfUnsupportedMinOpsetVersion test_list ListModel torch jit ScriptModule torch jit script_method forward x tensors = x unbind res = res append tensors res append tensors res pop res insert tensors res append tensors res += tensors tensors res = res + tensors torch ones len res model = ListModel inputs = torch randn run_test model inputs skipIfUnsupportedMinOpsetVersion test_list_append ListModel torch nn Module forward x y res = i range x size res += torch matmul x i y res model = torch jit script ListModel x = torch randn y = torch randn run_test model x y skipIfUnsupportedMinOpsetVersion test_list_append_nested ListModel torch nn Module forward x y res = i range x size j range x size res += torch matmul x i j y res model = torch jit script ListModel x = torch randn y = torch randn run_test model x y skipIfUnsupportedMinOpsetVersion Need onnx Identity sequence opset test_list_append_nested_ ListModel torch nn Module forward x res = res_replicate = i range x size len res j range x size res append x i j res_replicate append res - res append res_replicate - res res_replicate model = torch jit script ListModel x = torch randn run_test model x skipIfUnsupportedMinOpsetVersion test_list_append_nested_mixed_dtype ListModel torch nn Module forward x y res = i range x size j range x size i == j res append x == y res append x = y res model = torch jit script ListModel x = torch randn y = torch randn run_test model x y skipIfUnsupportedMinOpsetVersion test_list_pop ListModel torch nn Module forward x y res = i range x size res += torch matmul x i y res pop res model = torch jit script ListModel x = torch randn y = torch randn run_test model x y skipIfUnsupportedMinOpsetVersion test_list_pop_nested ListModel torch nn Module forward x y res = i range x size j range x size res += torch matmul x i j y res pop res += torch matmul x i y res model = torch jit script ListModel x = torch randn y = torch randn run_test model x y skipIfUnsupportedMinOpsetVersion test_list_del ListModel torch nn Module forward x y res = i range x size res += torch matmul x i y del res res model = torch jit script ListModel x = torch randn y = torch randn run_test model x y skipIfUnsupportedMinOpsetVersion test_list_del_nested ListModel torch nn Module forward x y res = i range x size j range x size res += torch matmul x i j y del res i res += torch matmul x i y res model = torch jit script ListModel x = torch randn y = torch randn run_test model x y skipIfUnsupportedMinOpsetVersion test_list_set ListModel torch nn Module forward x y res = i range x size res append x i res y = x y res model = torch jit script ListModel x = torch randn y = torch tensor dtype=torch long run_test model x y skipIfUnsupportedMinOpsetVersion test_list_idx_sum ListModel torch nn Module forward x y indices = torch arange x size res = i range x size res append x i res torch sum indices y model = torch jit script ListModel x = torch randn y = torch tensor dtype=torch long run_test model x y skipIfUnsupportedMinOpsetVersion test_tensor_factories TensorFactory torch nn Module forward x torch zeros x size + torch ones x size x = torch randn run_test TensorFactory x input_names= x dynamic_axes= x run_test TensorFactory x remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_tensor_factories_script TensorFactory torch jit ScriptModule torch jit script_method forward x torch zeros x shape dtype=torch float + torch ones x shape dtype=torch float x = torch randn run_test TensorFactory x input_names= x dynamic_axes= x run_test TensorFactory x remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_tensor_like_factories_script TensorFactory torch jit ScriptModule torch jit script_method forward x zeros = torch zeros_like x dtype=torch float layout=torch strided device=torch device cpu ones = torch ones_like x dtype=torch float layout=torch strided device=torch device cpu zeros + ones x = torch randn run_test TensorFactory x input_names= x dynamic_axes= x run_test TensorFactory x remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_tensor_split TensorSplitModel torch nn Module forward input input tensor_split test output indexing input tensor_split test split specific dim input tensor_split dim=- test split specific dim output indexing input tensor_split dim=- - test out bound end index input tensor_split run_test TensorSplitModel torch randn skipIfUnsupportedMinOpsetVersion test_tensor_split_scalar TensorSplitModel torch nn Module forward x torch tensor_split x x size run_test TensorSplitModel torch randn skipIfUnsupportedMinOpsetVersion test_tensor_split_dynamic_axes TensorSplitModel torch nn Module forward x x tensor_split dim=- x = torch randn input_names = logits run_test TensorSplitModel x input_names=input_names dynamic_axes= input_names batch skipIfUnsupportedMinOpsetVersion test_eye TensorFactory torch nn Module forward x torch eye x size torch eye dtype=torch long torch eye x size dtype=torch long torch eye x shape torch eye x shape dtype=torch float x = torch randn another_x = torch randn run_test TensorFactory x additional_test_inputs= another_x input_names= input_ dynamic_axes= input_ skipIfUnsupportedMinOpsetVersion test_diagonal DiagonalModel torch nn Module forward x torch diagonal x x = torch randn Other test inputs test dynamic behavior another_x = torch randn run_test DiagonalModel x additional_test_inputs= another_x input_names= input_ dynamic_axes= input_ DiagonalModelNegOffset torch nn Module forward x torch diagonal x offset=- x = torch randn Other test inputs test dynamic behavior another_x = torch randn run_test DiagonalModelNegOffset x additional_test_inputs= another_x input_names= input_ dynamic_axes= input_ DiagonalModelPosOffset torch nn Module forward x torch diagonal x offset= x = torch randn Other test inputs test dynamic behavior another_x = torch randn run_test DiagonalModelPosOffset x additional_test_inputs= another_x input_names= input_ dynamic_axes= input_ DiagonalModelWithDims torch nn Module forward x torch diagonal x offset=- dim = dim = x = torch randn Other test inputs test dynamic behavior another_x = torch randn run_test DiagonalModelWithDims x additional_test_inputs= another_x input_names= input_ dynamic_axes= input_ DiagonalModelWithNegativeDims torch nn Module forward x torch diagonal x offset= dim =- dim =- x = torch randn Other test inputs test dynamic behavior another_x = torch randn run_test DiagonalModelWithNegativeDims x additional_test_inputs= another_x input_names= input_ dynamic_axes= input_ DiagonalModelOffsetOverrun torch nn Module forward x torch diagonal x offset=- torch diagonal x offset= x = torch randn Other test inputs test dynamic behavior another_x = torch randn run_test DiagonalModelOffsetOverrun x additional_test_inputs= another_x input_names= input_ dynamic_axes= input_ skipIfUnsupportedMinOpsetVersion test_inplace_zero Zero_ torch nn Module forward x x zero_ x x = torch randn run_test Zero_ x input_names= x dynamic_axes= x run_test Zero_ x remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_inplace_zero_qkv Zero_ torch nn Module forward x x zero_ x = torch randn run_test Zero_ x input_names= x dynamic_axes= x skipIfUnsupportedMinOpsetVersion test_new_zeros Zero_ torch nn Module forward x x new_zeros x shape x new_zeros x shape dtype=torch long x = torch randn run_test Zero_ x input_names= x dynamic_axes= x run_test Zero_ x remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_new_zeros_with_dtype MyModel torch nn Module __init__ - None super __init__ emb = torch nn Embedding forward x inp = x new_zeros x shape emb inp model = MyModel x = torch Tensor torch int run_test model x input_names= x dynamic_axes= x skipIfUnsupportedMinOpsetVersion test_new_ones OnesModel torch nn Module forward x x new_ones x shape x new_ones x shape dtype=torch long x = torch randn run_test OnesModel x input_names= x dynamic_axes= x run_test OnesModel x remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion skipScriptTest torch zeros torch ones size tensor dim = scriptable test_zeros_ones_with_tensor_input ZeroAndOnes torch nn Module forward x torch zeros x torch ones x x = torch tensor run_test ZeroAndOnes x skipIfUnsupportedMinOpsetVersion skipShapeChecking test_tolist List torch jit ScriptModule torch jit script_method forward input res list int = input tolist res run_test List torch randint skipIfUnsupportedMinOpsetVersion test_list_pass Slice torch nn Module forward x y x new_zeros x shape + y shape x = torch randn y = torch randn run_test Slice x y input_names= x y dynamic_axes= x y run_test Slice x y remained_onnx_input_idx= Size torch nn Module forward x y x new_zeros x shape + y shape x = torch randn y = torch randn run_test Size x y input_names= x y dynamic_axes= x y run_test Size x y remained_onnx_input_idx= Array torch nn Module forward x y arr = x shape x shape arr = y shape y shape x new_zeros arr + arr x = torch randn y = torch randn run_test Array x y input_names= x y dynamic_axes= x y run_test Array x y remained_onnx_input_idx= List torch nn Module forward x y l = list x shape l = list y shape x new_zeros l + l x = torch randn y = torch randn run_test List x y input_names= x y dynamic_axes= x y run_test List x y remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_new_empty Emtpy torch nn Module forward x x new_empty x shape fill_ x new_empty x shape dtype=torch long x = torch randn run_test Emtpy x input_names= x dynamic_axes= x run_test Emtpy x remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_new_full Full torch nn Module forward x x new_full x shape x new_full x shape dtype=torch long x = torch randn run_test Full x input_names= x dynamic_axes= x run_test Full x remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_inplace_list Arithmetic torch jit ScriptModule torch jit script_method forward x y torch cat x add_ y fill_ x = torch randn y = torch randn run_test Arithmetic x y input_names= x y dynamic_axes= x y run_test Arithmetic x y remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_inplace_fill Fill_ torch nn Module forward x x fill_ x x = torch randn run_test Fill_ x input_names= x dynamic_axes= x run_test Fill_ x remained_onnx_input_idx= test_inplace_arithmetic Arithmetic torch jit ScriptModule torch jit script_method forward x y x add_ y mul_ x x y x = torch randn y = torch randn run_test Arithmetic x y test_inplace_arithmetic_half InplaceAddModel torch nn Module forward x y x add_ y InplaceMulModel torch nn Module forward x y x mul_ y x = torch randn dtype=torch half y = torch randn dtype=torch float run_test InplaceAddModel x y rtol= e- atol= e- run_test InplaceMulModel x y rtol= e- atol= e- skipIfUnsupportedMinOpsetVersion test_inplace_with_loop M torch nn Module forward x = torch ones _ range add_ torch ones + x m = M x = torch randn run_test torch jit script M x skipIfUnsupportedMinOpsetVersion test_inplace_with_loop_ M torch nn Module forward x _bias = torch ones = torch ones used loop altered a_ref = used loop should altered b = x clone used loop altered b_ref = b used loop should altered i range i == _ range += _bias _bias add_ torch ones b = b + torch ones _bias add_ torch ones += _bias TODO value a_ref incorrect a_ref += torch ones b_ref += torch ones _bias + x b b_ref m = M x = torch zeros run_test torch jit script M x skipIfUnsupportedMinOpsetVersion test_inplace_attr_with_loop M torch nn Module __init__ - None super __init__ _bias = torch arange forward x _bias = torch arange i range i == _ range _bias += torch arange _bias + x m = M x = torch zeros run_test torch jit script M x skipIfUnsupportedMinOpsetVersion test_inplace_attr_copy_with_loop M torch nn Module __init__ - None super __init__ _bias = torch arange forward x _bias = torch arange i range i == _ range _bias copy_ torch arange _bias copy_ _bias + torch arange _bias copy_ _bias + torch arange _bias + x m = M x = torch zeros run_test torch jit script M x skipIfUnsupportedMinOpsetVersion Need onnx Identity sequence opset test_inplace_sequence_with_loop M torch nn Module process beam_hyps list Tensor done Tensor x batch_size = x shape i range batch_size done i continue beam_idx = _ token enumerate x i beam_hyps append token beam_idx += noqa SIM beam_idx == break done i = len beam_hyps beam_hyps done forward x beam_hyps list Tensor = batch_size = x shape cur_len = max_len = x shape done = torch zeros batch_size dtype=torch bool while cur_len max_len beam_hyps done = process beam_hyps done x cur_len = cur_len + beam_hyps m = torch jit script M x = torch randn run_test torch jit script M x skipScriptTest Sort dynamic dim supported ONNX test_sort SortModel torch nn Module forward x out = i range - out append torch sort x dim=i descending=True out x = torch randn run_test SortModel x skipIfUnsupportedMinOpsetVersion skipScriptTest Sort dynamic dim supported ONNX test_sort_ascending SortModel torch nn Module forward x out = i range - out append torch sort x dim=i descending=False out x = torch randn run_test SortModel x skipIfUnsupportedMinOpsetVersion test_argsort ArgSortModel torch nn Module forward x torch argsort x dim= descending=False x = torch randn run_test ArgSortModel x skipIfUnsupportedMinOpsetVersion test_masked_fill MaskedFillModel torch nn Module forward x mask = torch tensor dtype=torch bool x masked_fill mask x = torch zeros requires_grad=True run_test MaskedFillModel x MaskedFillModel torch nn Module forward x x masked_fill x - x = torch arange view torch float run_test MaskedFillModel x skipIfUnsupportedMinOpsetVersion test_masked_fill_inplace MaskedFillModel torch jit ScriptModule torch jit script_method forward x mask = torch tensor dtype=torch bool x masked_fill_ mask x x = torch zeros requires_grad=True run_test MaskedFillModel x MaskedFillModel torch jit ScriptModule torch jit script_method forward x x masked_fill_ x - x x = torch arange view torch float run_test MaskedFillModel x skipIfUnsupportedMinOpsetVersion test_masked_scatter MaskedScatterModel torch nn Module forward x torch masked_scatter x x ge torch ones x = torch randn requires_grad=True run_test MaskedScatterModel x skipIfUnsupportedMinOpsetVersion test_masked_select MaskedSelectModel torch nn Module forward x torch masked_select x x ge x = torch randn requires_grad=True run_test MaskedSelectModel x skipIfUnsupportedMinOpsetVersion test_index_put_to_masked_fill MaskedFillModel torch nn Module forward input_mask some_const mask = input_mask clone mask mask = some_const = mask mask == some_const = mask mask = torch randn requires_grad=True constant = torch tensor dtype=torch float run_test MaskedFillModel mask constant skipIfUnsupportedMinOpsetVersion test_index_put_to_masked_scatter MaskedScatterModel torch nn Module forward input_mask some_const mask = input_mask clone mask mask = some_const = torch ones mask mask = torch randn requires_grad=True constant = torch tensor dtype=torch float run_test MaskedScatterModel mask constant skipIfUnsupportedMinOpsetVersion test_index_put_with_ d_mask_to_masked_scatter MaskedScatterModel torch nn Module forward tensor mask some_const tensor mask = some_const tensor mask = torch tensor dtype=torch bool tensor = torch randn requires_grad=True some_const = torch randn dtype=torch float run_test MaskedScatterModel tensor mask some_const skipIfUnsupportedMinOpsetVersion test_pixel_shuffle PixelShuffle torch nn Module forward x torch pixel_shuffle x upscale_factor= x = torch randn requires_grad=True y = torch randn requires_grad=True run_test PixelShuffle x run_test PixelShuffle x input_names= x dynamic_axes= x additional_test_inputs= y skipIfUnsupportedMinOpsetVersion test_pixel_unshuffle PixelUnshuffle torch nn Module forward x torch pixel_unshuffle x downscale_factor= x = torch randn requires_grad=True y = torch randn requires_grad=True run_test PixelUnshuffle x run_test PixelUnshuffle x input_names= x dynamic_axes= x additional_test_inputs= y skipIfUnsupportedMinOpsetVersion test_reciprocal ReciprocalModel torch nn Module forward x torch reciprocal x model = ReciprocalModel x = torch tensor run_test model x torch long run_test model x torch float run_test model x torch double skipIfUnsupportedMinOpsetVersion test_scalar_type ArithmeticModel torch nn Module forward x x size x - x x = torch ones dtype=torch float run_test ArithmeticModel x ComparisonModel torch nn Module forward x y = torch tensor x lt y le x le x gt y x lt y ge x size x = torch ones dtype=torch int y = torch ones dtype=torch float run_test ComparisonModel x y MatMulModel torch nn Module forward x torch mm x x + x + torch mm x x + x x = torch ones run_test MatMulModel x AddMMModel torch nn Module forward x torch mm x x + x x = torch ones run_test AddMMModel x FullModel torch nn Module add used exporting full forward x torch full x x = torch tensor run_test FullModel x CatModel torch nn Module forward fp fp torch cat fp fp fp = Tensor fp = fp half fp = Tensor run_test CatModel fp fp skipIfUnsupportedMinOpsetVersion test_scalar_type_does_not_trigger_upcast_type_promotion DoNotUpcastModel torch nn Module forward x scale = x size - - scale exported onnx float rank tensor The following Mul should NOT promoted float x scale x = torch ones dtype=torch float run_test DoNotUpcastModel x skipIfUnsupportedMinOpsetVersion test_scalar_type_promotion_onnx_where_two_prim_const TwoPrimConstCastWhereModel torch nn Module forward c torch where c c = torch ones dtype=torch bool run_test TwoPrimConstCastWhereModel c skipIfUnsupportedMinOpsetVersion test_scalar_type_promotion_onnx_where_one_prim_const OnePrimConstCastWhereModel torch nn Module forward c x torch where c x c = torch ones dtype=torch bool x = torch ones dtype=torch float run_test OnePrimConstCastWhereModel c x skipIfUnsupportedMinOpsetVersion test_scalar_type_promotion_onnx_where_one_tensor_const OneTensorConstCastWhereModel torch nn Module forward c x torch where c x torch ones size= dtype=torch float c = torch ones dtype=torch bool x = torch ones dtype=torch float run_test OneTensorConstCastWhereModel c x skipIfUnsupportedMinOpsetVersion test_scalar_type_upcast_type_promotion_onnx_where_no_const OnnxWhereUpcastModel torch nn Module forward c x y torch where c x y c = torch ones dtype=torch bool x = torch ones dtype=torch float y = torch ones dtype=torch float run_test OnnxWhereUpcastModel c x y skipIfUnsupportedMinOpsetVersion test_full_like FullLikeModel torch nn Module forward x torch full_like x dtype=torch int x = torch tensor run_test FullLikeModel x skipIfUnsupportedMinOpsetVersion skipDtypeChecking test_full_like_value FullLikeModel torch nn Module forward x y out = y + torch full_like x out x = torch tensor y = torch tensor run_test FullLikeModel x y test_l _norm NormModel torch nn Module forward x torch norm x p= dim=- keepdim=False x = torch randn requires_grad=True run_test NormModel x test_l _norm NormModel torch nn Module forward x torch norm x p= dim=- keepdim=False x = torch randn requires_grad=True run_test NormModel x test_frobenius_norm NormModel torch nn Module forward x torch norm x p= fro dim= keepdim=False x = torch randn requires_grad=True run_test NormModel x test_frobenius_norm_keepdim NormModel torch nn Module forward x torch norm x p= fro dim= keepdim=True x = torch randn requires_grad=True run_test NormModel x test_unfold UnfoldModel torch nn Module forward x x unfold dimension= size= step= x = torch randn requires_grad=True y = torch randn requires_grad=True run_test UnfoldModel x dynamic_axes= x input_names= x additional_test_inputs= y test_unfold_infer_shape UnfoldModule torch jit ScriptModule __init__ - None super __init__ conv = torch nn Conv d stride= torch jit script_method forward x x = conv x x unfold dimension= size= step= x = torch randn run_test UnfoldModule x skipIfUnsupportedMinOpsetVersion test_unfold_dynamic_inputs UnfoldModel torch nn Module forward x x unfold dimension= size=x shape step=x shape - x = torch randn requires_grad=True run_test UnfoldModel x UnfoldModel torch nn Module forward x x unfold dimension= size=x shape step= x = torch randn requires_grad=True run_test UnfoldModel x skipIfUnsupportedMinOpsetVersion MatMul long inputs added ONNX opset test_mv MatmulModel torch nn Module forward input other torch mv input other x = torch randn requires_grad=True y = torch randn requires_grad=True run_test MatmulModel x y x = torch randint y = torch randint run_test MatmulModel x y skipIfUnsupportedMinOpsetVersion MatMul long inputs added ONNX opset test_dot MatmulModel torch nn Module forward input other torch dot input other x = torch randn requires_grad=True y = torch randn requires_grad=True run_test MatmulModel x y x = torch randint y = torch randint run_test MatmulModel x y skipScriptTest SpectralNorm TorchScript compatible test_spectral_norm m = torch nn utils spectral_norm torch nn Linear x = torch randn run_test m x test_prelu PReluModel torch nn Module __init__ - None super __init__ prelu = torch nn PReLU forward x prelu x x = torch randn y = torch randn run_test PReluModel x input_names= x dynamic_axes= x additional_test_inputs= y test_prelu_scalar x = torch scalar_tensor run_test torch nn PReLU x input_names= x test_relu Relu Model torch nn Module __init__ - None super __init__ relu = torch nn ReLU forward x relu x x = torch randn y = torch randn run_test Relu Model x input_names= x dynamic_axes= x additional_test_inputs= y test_silu SiLUModel torch nn Module __init__ - None super __init__ silu = torch nn SiLU forward x silu x x = torch randn run_test SiLUModel x skipIfUnsupportedMinOpsetVersion test_tril trilModel torch nn Module forward x torch tril x x = torch randn run_test trilModel x trilModelwithDiagonal torch nn Module forward x torch tril x diagonal= x = torch randn run_test trilModelwithDiagonal x trilModelwithNegDiagonal torch nn Module forward x torch tril x diagonal=- x = torch randn run_test trilModelwithNegDiagonal x trilModelWithDiagonalInput torch nn Module forward x diagnonal int torch tril x diagonal=diagnonal x = torch randn run_test trilModelWithDiagonalInput x skipIfUnsupportedMinOpsetVersion test_triu triuModel torch nn Module forward x torch triu x x = torch randn run_test triuModel x triuModelwithDiagonal torch nn Module forward x torch triu x diagonal= x = torch randn run_test triuModelwithDiagonal x triuModelwithNegDiagonal torch nn Module forward x torch triu x diagonal=- x = torch randn run_test triuModelwithNegDiagonal x triuModelWithDiagonalInput torch nn Module forward x diagnonal int torch triu x diagonal=diagnonal x = torch randn run_test triuModelWithDiagonalInput x test_mish MishModel torch nn Module __init__ - None super __init__ mish = torch nn Mish forward x mish x x = torch randn run_test MishModel x test_remainder RemainderModel torch nn Module forward input other torch remainder input other x = torch randn y = torch randn run_test RemainderModel x y x = torch tensor - - dtype=torch long y = torch tensor dtype=torch long run_test RemainderModel x y x = x torch float run_test RemainderModel x y y = y torch float run_test RemainderModel x y x = x torch int run_test RemainderModel x y test_remainder_scalar RemainderModel torch nn Module __init__ scalar= super __init__ scalar = scalar forward input torch remainder input scalar x = torch randint run_test RemainderModel x x = torch tensor - - dtype=torch long run_test RemainderModel x skipIfUnsupportedMinOpsetVersion test_fmod FModModel torch nn Module forward input other torch fmod input other x = torch randn y = torch randn run_test FModModel x y skipIfUnsupportedMinOpsetVersion test_fmod_scalar FModModel torch nn Module forward input torch fmod input x = torch randint run_test FModModel x skipIfUnsupportedMinOpsetVersion test_glu GluModel torch nn Module forward x torch nn functional glu x x = torch randn requires_grad=True run_test GluModel x skipIfUnsupportedMinOpsetVersion test_gelu GeluModel torch nn Module forward x torch nn functional gelu x approximate= none x = torch randn requires_grad=True run_test GeluModel x skipIfUnsupportedMinOpsetVersion test_tanh_gelu GeluModel torch nn Module forward x torch nn functional gelu x approximate= tanh x = torch randn requires_grad=True run_test GeluModel x test_add_inplace InplaceAddModel torch nn Module forward x x += x x = torch randn requires_grad=True run_test InplaceAddModel x test_addcmul AddcmulModel torch nn Module forward x t t torch addcmul x t t torch addcmul x t t value= x = torch randn t = torch randn t = torch randn run_test AddcmulModel x t t test_rsqrt RsqrtModel torch nn Module forward x x rsqrt x = torch randn requires_grad=True dtype=torch float run_test RsqrtModel x test_rsqrt_zeros RsqrtModel torch nn Module forward x x rsqrt x = torch zeros requires_grad=True dtype=torch float run_test RsqrtModel x skipIfUnsupportedMinOpsetVersion test_unique UniqueModel torch nn Module forward x torch unique x sorted=True return_inverse=False return_counts=True x = torch tensor dtype=torch long run_test UniqueModel x skipIfUnsupportedMinOpsetVersion test_unique_along_dim UniqueModel torch nn Module forward x torch unique x dim= sorted=True return_inverse=True return_counts=False x = torch tensor dtype=torch long run_test UniqueModel x skipIfUnsupportedMinOpsetVersion test_cumsum CumSum torch nn Module forward input torch cumsum input dim= x = torch randn model = CumSum run_test model x skipIfUnsupportedMinOpsetVersion test_cumsum_with_cast CumSum torch nn Module forward input torch cumsum input dim= dtype=torch float model = CumSum x = torch tensor dtype=torch int run_test model x x = torch tensor False True True run_test model x skipScriptTest error propagate assign input shape skipIfUnsupportedMinOpsetVersion test_embedding_bag model = torch nn EmbeddingBag mode= sum scale_grad_by_freq=True input = torch randint offset = torch tensor run_test model input offset model = torch nn EmbeddingBag mode= sum include_last_offset=True input = torch randint offset = torch tensor run_test model input offset model = torch nn EmbeddingBag mode= max input = torch randint run_test model input skipIfUnsupportedMinOpsetVersion test_embedding_bag_ d_per_sample_weights EmbeddingModel torch nn Module forward embedding_matrix input offset weights torch nn functional embedding_bag input embedding_matrix offsets=offset mode= sum per_sample_weights=weights model = EmbeddingModel x = torch randint w = torch randn offset = torch tensor embedding_matrix = torch rand run_test model embedding_matrix x offset w skipIfUnsupportedMinOpsetVersion unittest skip This test broken ONNXRuntime when running onnxruntime test fails following error FAIL Non-zero status code returned while running If node Name If Status Message cc Compute If nodes condition input must have exactly one element https github com pytorch pytorch issues test_embedding_bag_ d_per_sample_weights EmbeddingModel torch nn Module forward embedding_matrix input weights torch nn functional embedding_bag input embedding_matrix mode= sum per_sample_weights=weights embedding_matrix = torch rand model = EmbeddingModel x = torch randint w = torch randn x = torch randint w = torch randn run_test model embedding_matrix x w input_names= embed x w dynamic_axes= x w additional_test_inputs= embedding_matrix x w skipScriptTest scripting prim Uninitialized prim dtype prim unchecked_cast skipIfUnsupportedMinOpsetVersion unittest skip Due ONNX Loop shape inference issue https msdata visualstudio com Vienna _workitems edit test_embedding_bag_dynamic_input EmbeddingModel D torch nn Module forward embedding_matrix input weights offsets torch nn functional embedding_bag input embedding_matrix offsets=offsets mode= sum per_sample_weights=weights model = EmbeddingModel D x = torch randint w = torch randn offsets = torch tensor dtype=torch long embedding_matrix = torch rand x = torch randint w = torch randn embedding_matrix = torch rand offsets = torch tensor dtype=torch long run_test model embedding_matrix x w offsets additional_test_inputs= embedding_matrix x w offsets input_names= embedding_matrix x offsets w dynamic_axes= embedding_matrix x offsets w EmbeddingModel D torch nn Module forward embedding_matrix input weights torch nn functional embedding_bag input embedding_matrix mode= sum per_sample_weights=weights model = EmbeddingModel D x = torch randint w = torch randn embedding_matrix = torch rand x = torch randint w = torch randn embedding_matrix = torch rand run_test model embedding_matrix x w additional_test_inputs= embedding_matrix x w input_names= embedding_matrix x w dynamic_axes= embedding_matrix x w skipIfUnsupportedMinOpsetVersion test_meshgrid Meshgrid torch nn Module forward x y z output output output = torch meshgrid x y z output output output x = torch randn requires_grad=True y = torch zeros requires_grad=True z = torch randn requires_grad=True run_test Meshgrid x y z skipIfUnsupportedMinOpsetVersion test_meshgrid_indexing Meshgrid torch nn Module __init__ indexing super __init__ indexing = indexing forward x y z output output output = torch meshgrid x y z indexing=self indexing output output output x = torch randn requires_grad=True y = torch zeros requires_grad=True z = torch randn requires_grad=True indexing xy ij run_test Meshgrid indexing x y z skipIfUnsupportedMinOpsetVersion test_meshgrid_scalar Meshgrid torch nn Module forward x y z output output output = torch meshgrid x y z output output output x = torch ones requires_grad=True y = torch zeros requires_grad=True z = torch tensor run_test Meshgrid x y z test_baddbmm MyModule torch nn Module forward input batch batch torch baddbmm input batch batch alpha=torch tensor beta= x = torch randn batch = torch randn batch = torch randn model = MyModule run_test model x batch batch test_baddbmm_dynamic MyModule torch nn Module forward input batch batch alpha beta torch baddbmm input batch batch alpha=alpha beta=beta x = torch randn batch = torch randn batch = torch randn alpha = torch tensor beta = torch tensor model = MyModule run_test model x batch batch alpha beta test_numel MyModule torch nn Module forward input input numel input x = torch randn x = torch randn model = MyModule run_test model x input_names= x dynamic_axes= x additional_test_inputs= x test_numel_empty MyModule torch nn Module forward input input numel input x = torch randn x = torch randn model = MyModule run_test model x input_names= x dynamic_axes= x additional_test_inputs= x test_dtype MyModel torch jit ScriptModule torch jit script_method forward input other input dtype=other dtype + other x = torch randn y = torch randn run_test MyModel x y test_dtype_eq MyModel torch jit ScriptModule torch jit script_method forward input other input dtype == other dtype input + other input x = torch randn y = torch randn run_test MyModel x y test_cast_to MyModule torch jit ScriptModule torch jit script_method forward input other input other + other x = torch randn y = torch tensor dtype=torch int model = MyModule run_test model x y test_cast_to_bool MyModule torch nn Module forward input other torch cat input other other x = torch randn y = torch zeros dtype=torch bool model = MyModule run_test model x y ONNX supports bfloat opsets = skipIfUnsupportedMinOpsetVersion test_cast_type_as_with_bfloat MyModule torch nn Module forward x y = torch ones dtype=torch bfloat x = x type_as y x dtype=torch float x = torch ones dtype=torch float model = MyModule run_test model x skipIfUnsupportedMinOpsetVersion test_type_as MyModule torch nn Module forward x y = torch tensor x type_as y = torch tensor True False dtype=torch bool b = torch randn dtype=torch double c = torch ones dtype=torch int model = MyModule run_test model run_test model b run_test model c skipIfUnsupportedMinOpsetVersion test_ones_bool MyModule torch nn Module forward input true = torch ones input shape dtype=torch bool input true true x = torch randn model = MyModule run_test model x test_log Log torch nn Module forward input torch log input x = torch rand model = Log run_test model x test_log p Log p torch nn Module forward input torch log p input x = torch rand model = Log p run_test model x test_log Log torch nn Module forward input torch log input x = torch rand model = Log run_test model x test_log Log torch nn Module forward input torch log input x = torch tensor model = Log run_test model x skipIfUnsupportedMinOpsetVersion test_round Round torch nn Module forward x torch round x x = torch tensor - - requires_grad=True run_test Round x int_x = torch tensor - dtype=torch int run_test Round int_x skipIfUnsupportedMinOpsetVersion test_round_with_decimals Round torch nn Module __init__ decimals super __init__ decimals = decimals forward x torch round x decimals=self decimals x = torch tensor - - decimals - run_test Round decimals x skipIfUnsupportedMinOpsetVersion test_stft_default STFT torch nn Module forward x n_fft = torch stft x n_fft=n_fft center=False return_complex=False x = torch randn requires_grad=True run_test STFT x atol= e- skipIfUnsupportedMinOpsetVersion test_stft_hop_length STFT torch nn Module forward x n_fft = hop_length = torch stft x n_fft=n_fft center=False hop_length=hop_length return_complex=False x = torch randn requires_grad=True run_test STFT x atol= e- skipIfUnsupportedMinOpsetVersion test_stft_non_divisible_hop_length STFT torch nn Module forward x n_fft = hop_length = torch stft x n_fft=n_fft center=False hop_length=hop_length return_complex=False x = torch randn requires_grad=True run_test STFT x atol= e- skipIfUnsupportedMinOpsetVersion test_stft_window_int_same_size STFT torch nn Module forward x n_fft = win_length = torch stft x n_fft=n_fft center=False win_length=win_length return_complex=False x = torch randn requires_grad=True run_test STFT x atol= e- skipIfUnsupportedMinOpsetVersion test_stft_window_int_different_size STFT torch nn Module forward x n_fft = win_length = torch stft x n_fft=n_fft center=False win_length=win_length return_complex=False x = torch randn requires_grad=True run_test STFT x atol= e- skipIfUnsupportedMinOpsetVersion test_stft_window_custom STFT torch nn Module forward x n_fft = window = torch hann_window torch stft x n_fft=n_fft center=False window=window return_complex=False x = torch randn requires_grad=True run_test STFT x atol= e- skipIfUnsupportedMinOpsetVersion test_stft_wrong_custom_window_size STFT torch nn Module forward x n_fft = window = torch hann_window torch stft x n_fft=n_fft window=window center=False return_complex=False x = torch randn requires_grad=True assertRaises AssertionError RuntimeError run_test STFT x skipIfUnsupportedMinOpsetVersion test_stft_wrong_window_length STFT torch nn Module forward x n_fft = win_len = torch stft x n_fft=n_fft win_length=win_len center=False return_complex=False x = torch randn requires_grad=True assertRaises RuntimeError run_test STFT x skipIfUnsupportedMinOpsetVersion test_stft_window_size_with_win_len STFT torch nn Module forward x n_fft = window = torch hann_window win_len = torch stft x n_fft=n_fft window=window win_length=win_len center=False return_complex=False x = torch randn requires_grad=True run_test STFT x atol= e- skipIfUnsupportedMinOpsetVersion test_stft_one_dimension STFT torch nn Module forward x n_fft = torch stft x n_fft=n_fft center=False return_complex=False x = torch randn requires_grad=True run_test STFT x atol= e- skipIfUnsupportedMinOpsetVersion test_stft_wrong_input_size STFT torch nn Module forward x n_fft = torch stft x n_fft=n_fft center=False return_complex=False x = torch randn requires_grad=True assertRaises RuntimeError run_test STFT x skipIfUnsupportedMinOpsetVersion test_stft_wrong_return_complex STFT torch nn Module forward x n_fft = torch stft x n_fft=n_fft center=False return_complex=True x = torch randn requires_grad=True assertRaises errors SymbolicValueError run_test STFT x skipIfUnsupportedMinOpsetVersion test_stft_normalize STFT torch nn Module forward x n_fft = torch stft x n_fft=n_fft center=False normalized=True return_complex=False x = torch randn requires_grad=True run_test STFT x atol= e- skipIfUnsupportedMinOpsetVersion test_stft_not_onesided STFT torch nn Module forward x n_fft = torch stft x n_fft=n_fft center=False onesided=False return_complex=False x = torch randn requires_grad=True run_test STFT x atol= e- test_constant_pad model = torch nn ConstantPad d x = torch randn run_test model x model = torch nn ConstantPad d x = torch randn run_test model x common_utils parametrize pad common_utils subtest name= scalar_list common_utils subtest torch tensor dtype=torch int torch tensor dtype=torch int name= scalar_tensor_list skipIfUnsupportedMinOpsetVersion Dynamic padding added opset test_pad_types pad Test different pad integer types Pad torch nn Module forward x pad list int torch nn functional pad x pad x = torch randn run_test Pad x pad skipIfUnsupportedMinOpsetVersion test_pad_circular PadModel torch nn Module forward x out = torch nn functional pad x mode= circular out x = torch randn run_test PadModel x skipIfUnsupportedMinOpsetVersion test_pad_circular_negative Test different pad integer types PadModel torch nn Module forward x out = torch nn functional pad x - - mode= circular out x = torch randn run_test PadModel x skipIfUnsupportedMinOpsetVersion test_pad_circular_dynamic_axes PadModel torch nn Module forward x out = torch nn functional pad x mode= circular out x = torch randn run_test PadModel x input_names= input_ dynamic_axes= input_ skipIfUnsupportedMaxOpsetVersion skipScriptTest TODO logic symbolic_opset doesn t handle script test_unsupported_pad Pad torch nn Module forward x pad list int torch nn functional pad x pad x = torch randn y = assertRaisesRegex RuntimeError Unsupported ONNX export Pad + The sizes padding must constant run_test Pad x y skipIfUnsupportedMinOpsetVersion test_if_fold IfFoldModel torch nn Module forward y y dim == y = y + y = y + y = y - y x = torch ones dtype=torch int run_test IfFoldModel x IfFoldModel torch nn Module forward y y numel y = y + y = y + y x = torch ones dtype=torch int run_test IfFoldModel x IfFoldModel torch nn Module forward y y dim = y = y + y = y + y y x = torch ones dtype=torch int run_test IfFoldModel x IfFoldModel torch nn Module forward y y dim = y = y + y = y - y x = torch ones dtype=torch int run_test IfFoldModel x IfFoldModel torch nn Module forward y y dim = y = y + y = y + y x = torch ones dtype=torch int run_test IfFoldModel x IfFoldModel torch nn Module forward y y dim y dtype == torch int y = y + y = y + y y x = torch ones dtype=torch int run_test IfFoldModel x IfFoldModel torch nn Module forward y y dim == y dtype == torch int y = y + y = y + y = y + y x = torch ones dtype=torch int run_test IfFoldModel x IfFoldModel torch nn Module forward y y numel = y dim == y = y + y = y + y y x = torch ones dtype=torch int run_test IfFoldModel x IfFoldModel torch nn Module forward x y x numel == y numel y = x + y y = y - x y x = torch ones dtype=torch int y = torch ones dtype=torch int run_test IfFoldModel x y IfFoldModel torch nn Module forward x y x numel = y numel y = x + y y = y - x y x = torch ones dtype=torch int y = torch ones dtype=torch int run_test IfFoldModel x y skipIfUnsupportedMinOpsetVersion test_uninitialized UninitializedModel torch nn Module forward y y shape y size == y = y + y y x = torch ones dtype=torch int run_test UninitializedModel x skipIfUnsupportedMinOpsetVersion test_uninitialized_dynamic UninitializedModel torch nn Module forward y y shape y size == y = y + y y x = torch ones dtype=torch int y = torch ones dtype=torch int run_test UninitializedModel x additional_test_inputs= y input_names= input_ dynamic_axes= input_ onnx Identity sequence supported ONNX opset = skipIfUnsupportedMinOpsetVersion test_uninitialized_tensorList UninitializedTensorListModel torch nn Module forward x x shape x size == x = x + x x x = torch ones dtype=torch int run_test torch jit script UninitializedTensorListModel x onnx Identity sequence supported ONNX opset = skipIfUnsupportedMinOpsetVersion test_uninitialized_tensorList_dynamic UninitializedTensorListModel torch nn Module forward x x shape x size == x += x list x list x x = torch ones dtype=torch double run_test torch jit script UninitializedTensorListModel x input_names= input_ dynamic_axes= input_ onnx Identity sequence supported ONNX opset = skipIfUnsupportedMinOpsetVersion test_uninitialized_intList UninitializedListModel torch nn Module forward x y = list range x size y x size = ORT will throw type error x size == y append y y x = torch ones dtype=torch int run_test torch jit script UninitializedListModel x input_names= input_ dynamic_axes= input_ onnx Identity sequence supported ONNX opset = skipIfUnsupportedMinOpsetVersion test_uninitialized_tensorList_shape UninitializedModel torch nn Module forward x x shape x size == x = x + x_list = list x x_list append x x_list x x x = torch ones dtype=torch int y = torch ones dtype=torch int run_test torch jit script UninitializedModel x additional_test_inputs= y input_names= input_ dynamic_axes= input_ Sequence type loop-carried dependencies only supported ONNX opset = skipIfUnsupportedMinOpsetVersion test_sequance_loopcarried SequanceLoopModel torch nn Module forward x outputs = _ range outputs += x torch stack outputs transpose x = torch ones dtype=torch int run_test torch jit script SequanceLoopModel x test_reflection_pad model = torch nn ReflectionPad d x = torch randn run_test model x model = torch nn ReflectionPad d x = torch randn run_test model x test_replication_pad model = torch nn ReplicationPad d x = torch randn run_test model x model = torch nn ReplicationPad d x = torch randn run_test model x skipIfUnsupportedMinOpsetVersion test_im col Unfold torch nn Module forward input torch nn functional unfold input kernel_size= dilation= padding= stride= torch nn functional unfold input kernel_size= dilation= padding= stride= torch nn functional unfold input kernel_size= dilation= padding= stride= x = torch rand run_test Unfold x skipIfNoLapack skipIfUnsupportedMinOpsetVersion test_det Det torch nn Module forward x torch linalg det x x = torch randn run_test Det x test_linalg_norm LinalgSingleDimModel torch nn Module __init__ ord_val super __init__ ord = ord_val forward x torch linalg norm x ord=self ord dim= x = torch randn run_test LinalgSingleDimModel None x run_test LinalgSingleDimModel x run_test LinalgSingleDimModel float inf x run_test LinalgSingleDimModel -float inf x run_test LinalgSingleDimModel - x run_test LinalgSingleDimModel x LinalgMultiDimModel torch nn Module __init__ ord_val super __init__ ord = ord_val forward x torch linalg norm x ord=self ord dim= x = torch randn run_test LinalgMultiDimModel fro x run_test LinalgMultiDimModel float inf x run_test LinalgMultiDimModel -float inf x run_test LinalgMultiDimModel x run_test LinalgMultiDimModel - x LinalgNoDimNoOrdModel torch nn Module forward x torch linalg norm x x = torch randn run_test LinalgNoDimNoOrdModel x y = torch randn run_test LinalgNoDimNoOrdModel y z = torch randn run_test LinalgNoDimNoOrdModel z LinalgNoDim DModel torch nn Module __init__ ord_val super __init__ ord = ord_val forward x torch linalg norm x ord=self ord x = torch randn run_test LinalgNoDim DModel None x run_test LinalgNoDim DModel x run_test LinalgNoDim DModel float inf x run_test LinalgNoDim DModel -float inf x run_test LinalgNoDim DModel - x run_test LinalgNoDim DModel x LinalgNoDim DModel torch nn Module __init__ ord_val super __init__ ord = ord_val forward x torch linalg norm x ord=self ord x = torch randn run_test LinalgNoDim DModel fro x run_test LinalgNoDim DModel float inf x run_test LinalgNoDim DModel -float inf x run_test LinalgNoDim DModel x run_test LinalgNoDim DModel - x skipIfUnsupportedMinOpsetVersion test_linalg_vector_norm_zero LinalgVectorNormModel torch nn Module __init__ ord_val super __init__ ord = ord_val forward x torch linalg vector_norm x ord=self ord x = torch randn run_test LinalgVectorNormModel x test_linalg_vector_norm LinalgVectorNormModel torch nn Module __init__ ord_val dim_info super __init__ ord = ord_val dim keepdim = dim_info forward x torch linalg vector_norm x ord=self ord dim=self dim keepdim=self keepdim x = torch randn ord_options = float inf -float inf - dim_options = None False False False True ord_val ord_options dim_info dim_options run_test LinalgVectorNormModel ord_val dim_info x test_linalg_matrix_norm LinalgMatrixNormModel torch nn Module __init__ ord_val dim_val= - - keepdim_val=False super __init__ ord = ord_val dim = dim_val keepdim = keepdim_val forward x torch linalg matrix_norm x ord=self ord dim=self dim keepdim=self keepdim x = torch randn ord_options = fro float inf -float inf - ord_val ord_options run_test LinalgMatrixNormModel ord_val x run_test LinalgMatrixNormModel ord_val x run_test LinalgMatrixNormModel ord_val True x skipIfUnsupportedMinOpsetVersion test_linalg_cross Cross torch nn Module forward x y torch linalg cross x y dim= torch linalg cross x y x = torch randn y = torch randn run_test Cross input_args= x y This test checks output scalar type ONNX graph should null https github com pytorch pytorch issues skipIfUnsupportedMinOpsetVersion test_trace_script torch jit script center_slice_helper input h_offset input h_offset CenterCrop torch nn Module forward input center_slice_helper input torch tensor input shape - x = torch randn run_test CenterCrop x skipIfNoLapack skipIfUnsupportedMinOpsetVersion test_logdet LogDet torch nn Module forward x torch logdet x x = torch randn run_test LogDet x test_dim DimModel torch jit ScriptModule torch jit script_method forward input out = input out = out dim out empty_input = torch randn requires_grad=True multi_dim_input = torch randn requires_grad=True run_test DimModel empty_input run_test DimModel multi_dim_input skipIfUnsupportedMinOpsetVersion test_dim_ M torch jit ScriptModule torch jit script_method forward poses boxes = torch zeros poses shape batch_boxes = kp_boxes boxes kp_boxes = torchvision ops clip_boxes_to_image kp_boxes batch_boxes append kp_boxes batch_boxes dummy_inputs = torch rand run_test M dummy_inputs input_names= x dynamic_axes= x skipIfUnsupportedMinOpsetVersion skipDtypeChecking test_outer Outer torch nn Module forward x y torch outer x y x = torch arange y = torch arange run_test Outer input_args= x y x = torch arange dtype=torch float y = torch arange dtype=torch long run_test Outer input_args= x y x = torch arange dtype=torch float y = torch arange dtype=torch float run_test Outer input_args= x y x = torch arange dtype=torch int y = torch arange dtype=torch long run_test Outer input_args= x y skipIfUnsupportedMinOpsetVersion test_movedim MovedimModel torch nn Module forward x x movedim x movedim x movedim x movedim x movedim x movedim x = torch randn run_test MovedimModel x skipIfUnsupportedMinOpsetVersion test_moveaxis moveaxis alias movedim thus mostly copied ` test_movedim ` MoveaxisModel torch nn Module forward x x moveaxis x moveaxis x moveaxis x moveaxis x moveaxis x moveaxis x = torch randn run_test MoveaxisModel x skipIfUnsupportedMinOpsetVersion test_einsum EinsumModelBatchDiagonal torch nn Module forward x eqn = ii - i torch einsum eqn x x torch randn torch randn dtype=torch bool run_test EinsumModelBatchDiagonal input_args= x EinsumModelBatchMatmul torch nn Module forward x y eqn = bij bjk - bik torch einsum eqn x y x = torch randn y = torch randn run_test EinsumModelBatchMatmul input_args= x y EinsumModelInnerProd torch nn Module forward x y eqn = i i torch einsum eqn x y x = torch randn y = torch randn run_test EinsumModelInnerProd input_args= x y EinsumModelTranspose torch nn Module forward x eqn = ij- ji torch einsum eqn x x torch randn torch randn dtype=torch bool run_test EinsumModelTranspose input_args= x skipIfUnsupportedMinOpsetVersion test_cosine_similarity x = torch randn y = torch randn run_test torch nn CosineSimilarity dim= input_args= x y skipIfUnsupportedMinOpsetVersion test_pairwise_distance x = torch randn y = torch randn run_test torch nn PairwiseDistance p= input_args= x y skipIfUnsupportedMinOpsetVersion test_cross Cross torch nn Module forward x y torch cross x y dim= torch cross x y x = torch randn y = torch randn run_test Cross input_args= x y skipIfUnsupportedMinOpsetVersion test_cdist Cdist torch nn Module forward x y torch cdist x y x = torch randn y = torch randn run_test Cdist input_args= x y skipIfUnsupportedMinOpsetVersion test_cdist_euclid_dist Cdist torch nn Module forward x y torch cdist x y p= compute_mode= use_mm_for_euclid_dist x = torch randn y = torch randn run_test Cdist input_args= x y skipIfUnsupportedMinOpsetVersion test_cdist_euclid_dist_if_necessary Cdist torch nn Module forward x y torch cdist x y p= compute_mode= use_mm_for_euclid_dist_if_necessary x = torch randn y = torch randn run_test Cdist input_args= x y skipIfUnsupportedMinOpsetVersion test_cdist_no_euclid_dist Cdist torch nn Module forward x y torch cdist x y p= compute_mode= donot_use_mm_for_euclid_dist x = torch randn y = torch randn run_test Cdist input_args= x y skipIfUnsupportedMinOpsetVersion test_crossentropyloss ignore_index - x = torch randn y = torch empty dtype=torch long random_ y y == = ignore_index _crossentropyloss x y ignore_index x = torch randn y = torch empty dtype=torch long random_ y y == = ignore_index _crossentropyloss x y ignore_index x = torch randn y = torch empty dtype=torch long random_ y y == = ignore_index _crossentropyloss x y ignore_index _crossentropyloss x y ignore_index CrossEntropyLossNone torch nn Module __init__ ignore_index super __init__ ignore_index == - loss = torch nn CrossEntropyLoss reduction= none loss = torch nn CrossEntropyLoss reduction= none ignore_index=ignore_index forward input target loss input target run_test CrossEntropyLossNone ignore_index input_args= x y CrossEntropyLossNoneWeight torch nn Module __init__ ignore_index super __init__ ignore_index == - loss = torch nn CrossEntropyLoss reduction= none weight=torch randn loss = torch nn CrossEntropyLoss reduction= none weight=torch randn ignore_index=ignore_index forward input target loss input target run_test CrossEntropyLossNoneWeight ignore_index input_args= x y CrossEntropyLossSum torch nn Module __init__ ignore_index super __init__ ignore_index == - loss = torch nn CrossEntropyLoss reduction= sum loss = torch nn CrossEntropyLoss reduction= sum ignore_index=ignore_index forward input target loss input target run_test CrossEntropyLossSum ignore_index input_args= x y CrossEntropyLossSumWeight torch nn Module __init__ ignore_index super __init__ ignore_index == - loss = torch nn CrossEntropyLoss reduction= sum weight=torch randn loss = torch nn CrossEntropyLoss reduction= sum weight=torch randn ignore_index=ignore_index forward input target loss input target run_test CrossEntropyLossSumWeight ignore_index input_args= x y CrossEntropyLossMean torch nn Module __init__ ignore_index super __init__ ignore_index == - loss = torch nn CrossEntropyLoss loss = torch nn CrossEntropyLoss ignore_index=ignore_index forward input target loss input target run_test CrossEntropyLossMean ignore_index input_args= x y CrossEntropyLossMeanWeight torch nn Module __init__ ignore_index super __init__ ignore_index == - loss = torch nn CrossEntropyLoss weight=torch randn loss = torch nn CrossEntropyLoss weight=torch randn ignore_index=ignore_index forward input target loss input target run_test CrossEntropyLossMeanWeight ignore_index input_args= x y skipIfUnsupportedMinOpsetVersion test_MSELoss MSELoss torch nn Module __init__ - None super __init__ loss = torch nn MSELoss reduction= none loss = torch nn MSELoss reduction= sum loss = torch nn MSELoss reduction= mean forward input target loss input target loss input target loss input target x = torch randn y = torch randn run_test MSELoss input_args= x y skipIfUnsupportedMinOpsetVersion test_kldiv_loss x = torch rand log y = torch rand _kldiv_loss x y x = torch rand log y = torch rand _kldiv_loss x y x = torch rand log y = torch rand _kldiv_loss x y _kldiv_loss x y KLDivLossNone torch nn Module __init__ - None super __init__ loss = torch nn KLDivLoss reduction= none log_target=True forward input target loss input target log run_test KLDivLossNone input_args= x y KLDivLossMean torch nn Module __init__ - None super __init__ loss = torch nn KLDivLoss reduction= mean log_target=False forward input target loss input target run_test KLDivLossMean input_args= x y KLDivLossSum torch nn Module __init__ - None super __init__ loss = torch nn KLDivLoss reduction= sum log_target=True forward input target loss input target log run_test KLDivLossSum input_args= x y KLDivLossBatchMean torch nn Module __init__ - None super __init__ loss = torch nn KLDivLoss reduction= batchmean log_target=False forward input target loss input target run_test KLDivLossBatchMean input_args= x y KLDivLossMiniBatchMean torch nn Module __init__ - None super __init__ loss = torch nn KLDivLoss reduction= batchmean size_average=False log_target=True forward input target loss input target log run_test KLDivLossMiniBatchMean input_args= x y skipIfUnsupportedMinOpsetVersion test_nllloss NLLModel torch nn Module __init__ - None super __init__ loss = torch nn NLLLoss reduction= none m = torch nn LogSoftmax dim= forward input target output = loss m input target output N C = input = torch randn N target = torch empty N dtype=torch long random_ C using test data containing default ignore_index=- target target == = - run_test NLLModel input target skipIfUnsupportedMinOpsetVersion test_nllloss_ d_none NLLModel torch nn Module __init__ - None super __init__ loss = torch nn NLLLoss reduction= none conv = torch nn Conv d C m = torch nn LogSoftmax dim= forward input target output = loss m conv input target output N C = input = torch randn N target = torch empty N dtype=torch long random_ C using test data containing default ignore_index=- target target == = - run_test NLLModel input target skipIfUnsupportedMinOpsetVersion test_nllloss_ d_mean NLLModel torch nn Module __init__ - None super __init__ loss = torch nn NLLLoss reduction= mean conv = torch nn Conv d C m = torch nn LogSoftmax dim= forward input target output = loss m conv input target output N C = input = torch randn N target = torch empty N dtype=torch long random_ C using test data containing default ignore_index=- target target == = - run_test NLLModel input target skipIfUnsupportedMinOpsetVersion test_nllloss_ d_sum NLLModel torch nn Module __init__ - None super __init__ loss = torch nn NLLLoss reduction= sum conv = torch nn Conv d C m = torch nn LogSoftmax dim= forward input target output = loss m conv input target output N C = input = torch randn N target = torch empty N dtype=torch long random_ C using test data containing default ignore_index=- target target == = - run_test NLLModel input target skipIfUnsupportedMinOpsetVersion test_nllloss_ d_mean_weights NLLModel torch nn Module __init__ - None super __init__ loss = torch nn NLLLoss reduction= mean weight=torch randn C conv = torch nn Conv d C m = torch nn LogSoftmax dim= forward input target output = loss m conv input target output N C = input = torch randn N target = torch empty N dtype=torch long random_ C using test data containing default ignore_index=- target target == = - run_test NLLModel input target skipIfUnsupportedMinOpsetVersion test_nllloss_ d_mean_ignore_index NLLModel torch nn Module __init__ - None super __init__ loss = torch nn NLLLoss reduction= mean ignore_index= conv = torch nn Conv d C m = torch nn LogSoftmax dim= forward input target output = loss m conv input target output N C = input = torch randn N target = torch empty N dtype=torch long random_ C run_test NLLModel input target skipIfUnsupportedMinOpsetVersion test_nllloss_dynamic_ignore_index torch nn functional F linear_combination x y epsilon epsilon x + - epsilon y reduce_loss loss reduction= mean loss mean reduction == mean loss sum reduction == sum loss LabelSmoothingCrossEntropy torch nn Module __init__ epsilon float = reduction= mean super __init__ epsilon = epsilon reduction = reduction forward preds target start_position n = preds size - log_preds = F log_softmax preds dim=- ignore_index = start_position size nll = F nll_loss log_preds target reduction=self reduction ignore_index=ignore_index nll + start_position float N = preds = torch randn N target = torch randint N start_position = torch randint N N run_test LabelSmoothingCrossEntropy preds target start_position skipIfUnsupportedMinOpsetVersion test_nllloss_ d_mean_ignore_index_weights NLLModel torch nn Module __init__ - None super __init__ loss = torch nn NLLLoss reduction= mean weight=torch randn C ignore_index= conv = torch nn Conv d C m = torch nn LogSoftmax dim= forward input target output = loss m conv input target output N C = input = torch randn N target = torch empty N dtype=torch long random_ C run_test NLLModel input target skipIfUnsupportedMinOpsetVersion test_binary_cross_entropy_with_logits x = torch randn y = torch empty random_ _bce_logits x y x = torch randn y = torch empty random_ weight = torch tensor _bce_logits_wegiht x y weight x = torch randn y = torch empty random_ pos_weight = torch empty random_ _bce_logits_posweight x y pos_weight x = torch randn y = torch empty random_ weight = torch tensor pos_weight = torch empty random_ _bce_logits_loss_weight_posweight x y weight pos_weight _bce_logits x y BCEWithLogitsLossNone torch nn Module forward input target torch nn functional binary_cross_entropy_with_logits input target reduction= none run_test BCEWithLogitsLossNone input_args= x y BCEWithLogitsLossMean torch nn Module forward input target torch nn functional binary_cross_entropy_with_logits input target reduction= mean run_test BCEWithLogitsLossMean input_args= x y BCEWithLogitsLossSum torch nn Module forward input target torch nn functional binary_cross_entropy_with_logits input target reduction= sum run_test BCEWithLogitsLossSum input_args= x y _bce_logits_wegiht x y weight BCEWithLogitsLossWegihtNone torch nn Module forward input target weight torch nn functional binary_cross_entropy_with_logits input target weight=weight reduction= none run_test BCEWithLogitsLossWegihtNone input_args= x y weight BCEWithLogitsLossWegihtMean torch nn Module forward input target weight torch nn functional binary_cross_entropy_with_logits input target weight=weight reduction= mean run_test BCEWithLogitsLossWegihtMean input_args= x y weight BCEWithLogitsLossWegihtSum torch nn Module forward input target weight torch nn functional binary_cross_entropy_with_logits input target weight=weight reduction= sum run_test BCEWithLogitsLossWegihtSum input_args= x y weight _bce_logits_posweight x y pos_weight BCEWithLogitsLossPosWegihtNone torch nn Module forward input target pos_weight torch nn functional binary_cross_entropy_with_logits input target pos_weight=pos_weight reduction= none run_test BCEWithLogitsLossPosWegihtNone input_args= x y pos_weight BCEWithLogitsLossPosWegihtMean torch nn Module forward input target pos_weight torch nn functional binary_cross_entropy_with_logits input target pos_weight=pos_weight reduction= mean run_test BCEWithLogitsLossPosWegihtMean input_args= x y pos_weight BCEWithLogitsLossPosWegihtSum torch nn Module forward input target pos_weight torch nn functional binary_cross_entropy_with_logits input target pos_weight=pos_weight reduction= sum run_test BCEWithLogitsLossPosWegihtSum input_args= x y pos_weight _bce_logits_loss_weight_posweight x y weight pos_weight BCEWithLogitsLossWeightPosweightNone torch nn Module forward input target weight pos_weight torch nn functional binary_cross_entropy_with_logits input target weight=weight pos_weight=pos_weight reduction= none run_test BCEWithLogitsLossWeightPosweightNone input_args= x y weight pos_weight BCEWithLogitsLossWeightPosweightMean torch nn Module forward input target weight pos_weight torch nn functional binary_cross_entropy_with_logits input target weight=weight pos_weight=pos_weight reduction= mean run_test BCEWithLogitsLossWeightPosweightMean input_args= x y weight pos_weight BCEWithLogitsLossWeightPosweightSum torch nn Module forward input target weight pos_weight torch nn functional binary_cross_entropy_with_logits input target weight=weight pos_weight=pos_weight reduction= sum run_test BCEWithLogitsLossWeightPosweightSum input_args= x y weight pos_weight test_torch_mm M torch nn Module forward mat mat mm = torch mm mat mat mm mat = torch randn mat = torch randn run_test M input_args= mat mat skipIfUnsupportedMinOpsetVersion Because where op supported opset test_where_with_bool_tensor M torch nn Module forward mat mat out = torch where mat mat mat out mat = torch randn mat = torch ones run_test M input_args= mat mat skipIfUnsupportedMinOpsetVersion Because where op supported opset test_where_with_byte_tensor M torch nn Module forward cond mat mat out = torch where cond mat mat out cond = torch ones dtype=torch uint cond = mat = torch randn mat = torch ones run_test M input_args= cond mat mat skipIfUnsupportedMinOpsetVersion ONNX IsInf op added opset test_isinf M torch nn Module forward x x isinf x = torch tensor float inf float nan float inf run_test M x skipIfUnsupportedMinOpsetVersion test_isfinite M torch nn Module forward x x isfinite x = torch tensor float inf float nan -float inf run_test M x skipIfUnsupportedMinOpsetVersion ONNX IsNaN op added opset test_isnan M torch nn Module forward x x isnan x = torch tensor float inf float nan float inf run_test M x skipIfUnsupportedMinOpsetVersion ONNX IsNaN IsInf op added opset respectively test_nan_to_num NoParams torch nn Module forward x x nan_to_num x = torch tensor float inf float nan -float inf xint = torch ones dtype=torch int xhalf = torch ones dtype=torch half run_test NoParams x run_test NoParams xint run_test NoParams xhalf WithParams torch nn Module forward x x nan_to_num nan= posinf= neginf= x = torch tensor float inf float nan -float inf run_test WithParams x skipIfUnsupportedMinOpsetVersion test_maximum_minimum ModelWithNan torch nn Module forward x y torch maximum x y torch minimum x y x = torch tensor - - float nan y = torch rand run_test ModelWithNan x y skipIfUnsupportedMinOpsetVersion test_minimum_dtypes MinimumModel torch nn Module forward x y torch minimum x y x = torch randn dtype=torch float y = torch randn dtype=torch float run_test MinimumModel x y x = torch randn dtype=torch float y = torch randint dtype=torch int run_test MinimumModel x y x = torch randint dtype=torch int y = torch randint dtype=torch int run_test MinimumModel x y x = torch randint dtype=torch int y = torch full_like x True run_test MinimumModel x y skipIfUnsupportedMinOpsetVersion test_maximum_dtypes MaximumModel torch nn Module forward x y torch maximum x y x = torch randn dtype=torch float y = torch randn dtype=torch float run_test MaximumModel x y x = torch randn dtype=torch float y = torch randint dtype=torch int run_test MaximumModel x y x = torch randint dtype=torch int y = torch randint dtype=torch int run_test MaximumModel x y x = torch randint dtype=torch int y = torch full_like x True run_test MaximumModel x y skipIfUnsupportedMinOpsetVersion test_any M torch nn Module forward x x any x = torch tensor True False False False run_test M x MDim torch nn Module forward x x any dim= x = torch rand bool run_test MDim x MKeepdim torch nn Module forward x x any dim= keepdim=True x = torch rand bool run_test MKeepdim x skipIfUnsupportedMinOpsetVersion test_all M torch nn Module forward x x all x = torch tensor True False False False run_test M x MDim torch nn Module forward x x all dim= x = torch rand bool run_test MDim x MKeepdim torch nn Module forward x x all dim= keepdim=True x = torch rand bool run_test MKeepdim x test_dropout M torch nn Module __init__ - None super __init__ dropout = torch nn Dropout forward x dropout = dropout x dropout x = torch randn run_test M x test_rrelu_eval x = torch tensor - run_test torch nn RReLU eval x test_shape_constant_fold ShapeModule torch nn Module __init__ - None super __init__ weight = torch nn Buffer torch ones forward x shape = weight shape x + shape x = torch randn run_test ShapeModule x rtol= e- atol= e- skipIfUnsupportedMinOpsetVersion test_celu Celu torch nn Module __init__ - None super __init__ celu = torch nn CELU alpha= forward input celu input input = torch randn run_test Celu input skipIfUnsupportedMinOpsetVersion test_celu_default Celu torch nn Module __init__ - None super __init__ celu = torch nn CELU forward input celu input input = torch randn run_test Celu input skipIfUnsupportedMinOpsetVersion test_celu_alpha Celu torch nn Module __init__ - None super __init__ celu = torch nn CELU alpha= forward input celu input input = torch randn run_test Celu input skipIfUnsupportedMinOpsetVersion test_celu_cast Celu torch nn Module __init__ - None super __init__ celu = torch nn CELU forward input celu input input = torch randn dtype=torch float run_test Celu input test_lower_tuple TupleModule torch nn Module forward input Tensor input Tensor input Tensor - Tensor = input input b = c = input input input _ range d = _ range e f = = d f f = c f size = input size - g = b b = g f k = c b = f k m n = b c = input n m p q r = c p + q + r input = torch randn input = torch randn input = torch randn run_test TupleModule input input input test_lower_tuple_ TupleModule torch nn Module forward input Tensor input Tensor - tuple Tensor Tensor = input input _ range c d = = c d input = torch randn input = torch randn run_test TupleModule input input test_lower_tuple_ TupleModule torch nn Module forward input tuple Tensor Tensor input tuple Tensor Tensor - tuple tuple Tensor Tensor tuple Tensor Tensor = input b = input _ range c d = e f = b c shape == e shape e = e + c f = f + d = e f b = c d b input = torch randn torch randn input = torch randn torch randn run_test TupleModule input input skipIfUnsupportedMinOpsetVersion test_where Model torch nn Module forward cond input other torch where cond input other x = torch randint dtype=torch bool y = torch randn z = torch ones run_test Model x y z skipIfUnsupportedMinOpsetVersion skipScriptTest scripting tests run opsets See test_where_condition_script test_where_condition Model torch nn Module forward input torch stack torch where input dim= x = torch randint dtype=bool run_test Model x Model torch nn Module forward input other torch stack torch where input other dim= x = torch randint dtype=bool y = torch randint dtype=bool run_test Model x y skipIfUnsupportedOpsetVersion skipIfUnsupportedMinOpsetVersion test_where_condition_script Model torch nn Module forward input torch stack torch where input dim= x = torch randint dtype=bool run_test Model x Model torch nn Module forward input other torch stack torch where input other dim= x = torch randint dtype=bool y = torch randint dtype=bool run_test Model x y test_empty_branch EmptyBranchModel torch jit ScriptModule torch jit script_method forward input out = input + out dim out dim out += pass pass out x = torch randn requires_grad=True run_test EmptyBranchModel x skipIfUnsupportedMinOpsetVersion test_derive_index_scripting MyModule torch nn Module forward x Tensor j = idx range len x - -len x - y = x idx j += x y j x = torch randn run_test MyModule x MyModule torch nn Module forward x Tensor j = idx range -len x len x - y = x idx j += x y j x = torch randn run_test MyModule x MyModule torch nn Module forward x Tensor j = idx range len x - -len x - y = x idx j += x y j run_test MyModule x MyModule torch nn Module forward x Tensor j = idx range -len x len x - y = x idx j += x y j run_test MyModule x skipScriptTest Scripting fails add lists opsets Chek test_derive_index_scripting test_derive_index MyModule torch nn Module forward x Tensor j = idx range len x - -len x - y = x idx j += x y j x = torch randn run_test MyModule x MyModule torch nn Module forward x Tensor j = idx range -len x len x - y = x idx j += x y j x = torch randn run_test MyModule x MyModule torch nn Module forward x Tensor j = idx range len x - -len x - y = x idx j += x y j run_test MyModule x MyModule torch nn Module forward x Tensor j = idx range -len x len x - y = x idx j += x y j run_test MyModule x skipIfUnsupportedMinOpsetVersion test_if_transpose IfModel torch nn Module forward x x = x transpose x size == x transpose x x = torch randn run_test torch jit script IfModel x output_names= output_ dynamic_axes= output_ skipIfUnsupportedMinOpsetVersion test_if_list IfModel torch nn Module forward x y cond res = cond res = res + x res = res + y res x = torch randn y = torch randn cond = torch tensor dtype=torch bool run_test torch jit script IfModel x y cond skipIfUnsupportedMinOpsetVersion test_if_view IfModel torch nn Module forward x y cond bs seq = y shape cond res = x view bs seq - res = y res transpose x = torch randn y = torch randn cond = torch tensor dtype=torch bool run_test torch jit script IfModel x y cond output_names= output_ dynamic_axes= output_ skipScriptTest skip_before_opset_version= reason= dynamic split support added test_split_tensor_scalar SplitModel torch nn Module forward x torch split x x size x = torch randn requires_grad=True run_test SplitModel x test_split_tensor_multi SplitModel torch nn Module forward x torch split x torch ones x = torch randn requires_grad=True run_model SplitModel x assertRaises TypeError run_model skipIfUnsupportedMinOpsetVersion test_embedding EmbedModel torch nn Module forward input emb torch nn functional embedding input emb padding_idx= model = EmbedModel x = torch randint x = x = embedding_matrix = torch rand run_test model x embedding_matrix x = torch randint x = x = run_test model x embedding_matrix run_test model x embedding_matrix training=torch onnx TrainingMode TRAINING EmbedModelWithoutPaddingIdx torch nn Module forward input emb torch nn functional embedding input emb model = EmbedModelWithoutPaddingIdx x = torch randint run_test model x embedding_matrix skipIfUnsupportedMinOpsetVersion test_embedding_module EmbedModel torch nn Module __init__ - None super __init__ emb = torch nn Embedding padding_idx= emb = torch nn Embedding padding_idx= torch no_grad emb weight = torch ones forward input emb input emb input model = EmbedModel x = torch randint x = x = run_test model x x = torch randint x = x = run_test model x EmbedModelWithoutPaddingIdx torch nn Module __init__ - None super __init__ emb = torch nn Embedding forward input emb input model = EmbedModelWithoutPaddingIdx x = torch randint run_test model x skipIfUnsupportedMinOpsetVersion test_embedding_renorm n d = embedding = torch nn Embedding n d max_norm= idx = torch tensor run_test embedding idx embedding = torch nn Embedding n d max_norm= norm_type= idx = torch tensor run_test embedding idx _dispatch_rnn_test name args kwargs name == elman _elman_rnn_test args kwargs name == lstm _lstm_test args kwargs name == gru _gru_test args kwargs _elman_rnn_test layers nonlinearity bidirectional initial_state packed_sequence dropout extra_kwargs ElmanWithStateModel torch nn Module __init__ layers nonlinearity bidirect dropout batch_first super __init__ batch_first = batch_first inner_model = torch nn RNN RNN_INPUT_SIZE RNN_HIDDEN_SIZE layers nonlinearity=nonlinearity bidirectional=bidirectional dropout=dropout batch_first=batch_first forward input rnn_utils PackedSequence hx=None inner_model input hx ElmanWithoutStateModel torch nn Module __init__ layers nonlinearity bidirect dropout batch_first super __init__ batch_first = batch_first inner_model = torch nn RNN RNN_INPUT_SIZE RNN_HIDDEN_SIZE layers nonlinearity=nonlinearity bidirectional=bidirectional dropout=dropout batch_first=batch_first forward input rnn_utils PackedSequence inner_model input batch_first = packed_sequence == initial_state model = ElmanWithStateModel layers=layers bidirect=bidirectional nonlinearity=nonlinearity dropout=dropout batch_first=batch_first packed_sequence model = rnn_model_with_packed_sequence RnnModelWithPackedSequenceWithState model batch_first model = ElmanWithoutStateModel layers=layers bidirect=bidirectional nonlinearity=nonlinearity dropout=dropout batch_first=batch_first packed_sequence model = rnn_model_with_packed_sequence RnnModelWithPackedSequenceWithoutState model batch_first make_input batch_size seq_lengths = np random randint RNN_SEQUENCE_LENGTH + size=batch_size seq_lengths = sorted map int seq_lengths reverse=True inputs = torch randn l RNN_INPUT_SIZE l seq_lengths inputs = rnn_utils pad_sequence inputs batch_first=batch_first inputs = inputs input_names = input directions = bidirectional initial_state h = torch randn directions layers batch_size RNN_HIDDEN_SIZE inputs append h input_names append h packed_sequence = inputs append torch IntTensor seq_lengths input_names append seq_lengths len inputs == input = inputs input = tuple inputs input input_names input input_names = make_input RNN_BATCH_SIZE dynamic_axes = input seq_lengths initial_state dynamic_axes update h export_options = input_names input_names dynamic_axes dynamic_axes test model still runs different batch size other_input _ = make_input RNN_BATCH_SIZE + run_test model input additional_test_inputs= other_input export_options _lstm_test layers bidirectional initial_state packed_sequence dropout extra_kwargs batch_first = packed_sequence == packed_sequence model = lstm_flattening_result LstmFlatteningResultWithSeqLength RNN_INPUT_SIZE RNN_HIDDEN_SIZE layers bidirectional dropout batch_first initial_state model = rnn_model_with_packed_sequence RnnModelWithPackedSequenceWithState model batch_first model = rnn_model_with_packed_sequence RnnModelWithPackedSequenceWithoutState model batch_first model = lstm_flattening_result LstmFlatteningResultWithoutSeqLength RNN_INPUT_SIZE RNN_HIDDEN_SIZE layers bidirectional dropout batch_first make_input batch_size seq_lengths = np random randint RNN_SEQUENCE_LENGTH + size=batch_size seq_lengths = sorted map int seq_lengths reverse=True inputs = torch randn l RNN_INPUT_SIZE l seq_lengths inputs = rnn_utils pad_sequence inputs batch_first=batch_first inputs = inputs input_names = input directions = bidirectional initial_state h = torch randn directions layers batch_size RNN_HIDDEN_SIZE c = torch randn directions layers batch_size RNN_HIDDEN_SIZE inputs append h c input_names append h input_names append c packed_sequence = inputs append torch IntTensor seq_lengths input_names append seq_lengths len inputs == input = inputs input = tuple inputs input input_names input input_names = make_input RNN_BATCH_SIZE dynamic_axes = input seq_lengths initial_state dynamic_axes update h c export_options = input_names input_names dynamic_axes dynamic_axes test model still runs different batch size other_input _ = make_input RNN_BATCH_SIZE + run_test model input additional_test_inputs= other_input export_options _gru_test layers bidirectional initial_state packed_sequence dropout extra_kwargs GRUWithStateModel torch nn Module __init__ layers bidirect dropout batch_first super __init__ batch_first = batch_first inner_model = torch nn GRU RNN_INPUT_SIZE RNN_HIDDEN_SIZE num_layers=layers bidirectional=bidirectional dropout=dropout batch_first=batch_first forward input rnn_utils PackedSequence hx inner_model input hx GRUWithoutStateModel torch nn Module __init__ layers bidirect dropout batch_first super __init__ batch_first = batch_first inner_model = torch nn GRU RNN_INPUT_SIZE RNN_HIDDEN_SIZE num_layers=layers bidirectional=bidirectional dropout=dropout batch_first=batch_first forward input rnn_utils PackedSequence inner_model input GRUNoSeqLengthWithoutStateModel torch nn Module __init__ layers bidirect dropout batch_first super __init__ batch_first = batch_first inner_model = torch nn GRU RNN_INPUT_SIZE RNN_HIDDEN_SIZE num_layers=layers bidirectional=bidirectional dropout=dropout batch_first=batch_first forward input inner_model input GRUNoSeqLengthWithStateModel torch nn Module __init__ layers bidirect dropout batch_first super __init__ batch_first = batch_first inner_model = torch nn GRU RNN_INPUT_SIZE RNN_HIDDEN_SIZE num_layers=layers bidirectional=bidirectional dropout=dropout batch_first=batch_first forward input hx inner_model input hx batch_first = packed_sequence == packed_sequence initial_state model = GRUWithStateModel layers=layers bidirect=bidirectional dropout=dropout batch_first=batch_first model = rnn_model_with_packed_sequence RnnModelWithPackedSequenceWithState model batch_first model = GRUWithoutStateModel layers=layers bidirect=bidirectional dropout=dropout batch_first=batch_first model = rnn_model_with_packed_sequence RnnModelWithPackedSequenceWithoutState model batch_first initial_state model = GRUNoSeqLengthWithStateModel layers=layers bidirect=bidirectional dropout=dropout batch_first=batch_first model = GRUNoSeqLengthWithoutStateModel layers=layers bidirect=bidirectional dropout=dropout batch_first=batch_first make_input batch_size seq_lengths = np random randint RNN_SEQUENCE_LENGTH + size=batch_size seq_lengths = sorted map int seq_lengths reverse=True inputs = torch randn l RNN_INPUT_SIZE l seq_lengths inputs = rnn_utils pad_sequence inputs batch_first=batch_first inputs = inputs input_names = input directions = bidirectional initial_state h = torch randn directions layers batch_size RNN_HIDDEN_SIZE inputs append h input_names append h packed_sequence = inputs append torch IntTensor seq_lengths input_names append seq_lengths len inputs == input = inputs input = tuple inputs input input_names input input_names = make_input RNN_BATCH_SIZE dynamic_axes = input seq_lengths initial_state dynamic_axes update h export_options = input_names input_names dynamic_axes dynamic_axes test model still runs different batch size other_input _ = make_input RNN_BATCH_SIZE + run_test model input additional_test_inputs= other_input export_options skipIfUnsupportedMinOpsetVersion test_fake_quantize_per_tensor FakeQuantizePerTensorModel torch nn Module forward input scale = zero_point = quant_min = - quant_max = torch fake_quantize_per_tensor_affine input scale zero_point quant_min quant_max x = torch randn run_test FakeQuantizePerTensorModel x skipIfUnsupportedMinOpsetVersion test_fake_quantize_per_tensor_dynamic_scale_zeropoint FakeQuantizePerTensorModel torch nn Module forward input scale zero_point quant_min = - quant_max = torch fake_quantize_per_tensor_affine input scale zero_point quant_min quant_max x = torch randn scale = torch tensor zero_point = torch tensor run_test FakeQuantizePerTensorModel x scale zero_point skipIfUnsupportedMinOpsetVersion test_fake_quantize_per_channel FakeQuantizePerChannelModel torch nn Module forward input amax = torch ones scale = amax zero_point = torch zeros_like amax dtype=torch int Quantize twice test different branches y = torch fake_quantize_per_channel_affine input scale zero_point torch fake_quantize_per_channel_affine y scale zero_point - x = torch randn run_test FakeQuantizePerChannelModel x skipIfUnsupportedMinOpsetVersion RuntimeError Can t redefine method forward __torch__ torch nn modules linear Linear skipScriptTest test_fake_quantize_activation torch ao quantization m = torch nn Linear m qconfig = quantization QConfig activation=quantization default_fake_quant weight=quantization default_per_channel_weight_fake_quant quantization prepare_qat m train inplace=True m apply quantization enable_observer m apply quantization enable_fake_quant module m modules isinstance module quantization FakeQuantize module calculate_qparams m apply quantization disable_observer m eval Fake quantize activation special case restricts quantized range while standard bit quantization range - Set fixed weight bias inputs test ONNX handles overflow correctly m weight = torch nn Parameter torch tensor m bias = torch nn Parameter torch tensor x = torch tensor - run_test m x test_batchnorm_training MyModule torch nn Module __init__ - None super __init__ bn = torch nn BatchNorm d affine=False cv = torch nn Conv d bn = torch nn BatchNorm d affine=True cv = torch nn Conv d bn = torch nn BatchNorm d affine=False forward x x = bn x x = cv x x = bn x x = cv x x = bn x x x = torch randn model_export = MyModule run_test model_export x training=torch onnx TrainingMode TRAINING rtol= e- atol= e- model_export train run_test model_export x training=torch onnx TrainingMode PRESERVE rtol= e- atol= e- test_batchnorm_training_mode_fix_layer MyModule torch nn Module __init__ - None super __init__ bn = torch nn BatchNorm d affine=True cv = torch nn Conv d bn = torch nn BatchNorm d affine=False cv = torch nn Conv d bn = torch nn BatchNorm d affine=True bn eval forward x x = bn x x = cv x x = bn x x = cv x x = bn x x x = torch randn model_export = MyModule run_test model_export x training=torch onnx TrainingMode TRAINING rtol= e- atol= e- model_export train run_test model_export x training=torch onnx TrainingMode PRESERVE rtol= e- atol= e- test_batchnorm_eval_mode_train_layer MyModule torch nn Module __init__ - None super __init__ bn = torch nn BatchNorm d affine=True cv = torch nn Conv d bn = torch nn BatchNorm d affine=False cv = torch nn Conv d bn = torch nn BatchNorm d affine=True bn train forward x x = bn x x = cv x x = bn x x = cv x x = bn x x x = torch randn model_export = MyModule run_test model_export x training=torch onnx TrainingMode EVAL rtol= e- atol= e- model_export eval run_test model_export x training=torch onnx TrainingMode PRESERVE rtol= e- atol= e- test_instancenorm_training MyModule torch nn Module __init__ - None super __init__ = torch nn InstanceNorm d affine=True cv = torch nn Conv d = torch nn InstanceNorm d affine=False cv = torch nn Conv d = torch nn InstanceNorm d affine=True forward x x = x x = cv x x = x x = cv x x = x x x = torch randn model_export = MyModule run_test model_export x training=torch onnx TrainingMode TRAINING rtol= e- atol= e- model_export train run_test model_export x training=torch onnx TrainingMode PRESERVE rtol= e- atol= e- test_instancenorm_training_mode_fix_layer MyModule torch nn Module __init__ - None super __init__ = torch nn InstanceNorm d affine=True cv = torch nn Conv d = torch nn InstanceNorm d affine=False cv = torch nn Conv d = torch nn InstanceNorm d affine=True eval forward x x = x x = cv x x = x x = cv x x = x x x = torch randn model_export = MyModule run_test model_export x training=torch onnx TrainingMode TRAINING rtol= e- atol= e- model_export train run_test model_export x training=torch onnx TrainingMode PRESERVE rtol= e- atol= e- test_instancenorm_eval_mode_train_layer MyModule torch nn Module __init__ - None super __init__ = torch nn InstanceNorm d affine=True cv = torch nn Conv d = torch nn InstanceNorm d affine=False cv = torch nn Conv d = torch nn InstanceNorm d affine=True train forward x x = x x = cv x x = x x = cv x x = x x x = torch randn model_export = MyModule run_test model_export x training=torch onnx TrainingMode EVAL rtol= e- atol= e- model_export eval run_test model_export x training=torch onnx TrainingMode PRESERVE rtol= e- atol= e- skipIfUnsupportedMinOpsetVersion test_dropout_training MyModule torch nn Module __init__ - None super __init__ dropout = torch nn Dropout forward x dropout = dropout x dropout model = MyModule x = torch randn model train model_onnx = io BytesIO torch onnx export model x model_onnx opset_version=self opset_version do_constant_folding=False training=torch onnx TrainingMode TRAINING dynamo=False ort_sess = verification _ort_session model_onnx ort_outs = verification _run_onnx ort_sess x assert torch all torch eq x torch from_numpy ort_outs script_model = torch jit script model output = model x model_onnx = io BytesIO torch onnx export model x model_onnx opset_version=self opset_version do_constant_folding=False training=torch onnx TrainingMode TRAINING dynamo=False ort_outs = verification _run_onnx ort_sess x assert torch all torch eq x torch from_numpy ort_outs skipIfUnsupportedMinOpsetVersion test_dropout_training_zero MyModule torch nn Module __init__ - None super __init__ dropout = torch nn Dropout forward x dropout = dropout x dropout model = MyModule ensure there no zeros input x = torch randn y = x numpy y_mask = np where y == y input = torch from_numpy y_mask nb_elements = torch numel input model train model_onnx = io BytesIO torch onnx export model x model_onnx opset_version=self opset_version do_constant_folding=False training=torch onnx TrainingMode TRAINING dynamo=False ort_sess = verification _ort_session model_onnx ort_outs = verification _run_onnx ort_sess x y = model input output = y cpu numpy ort_mask = np where ort_outs = pyt_mask = np where output = ratio_pytorch = np sum pyt_mask nb_elements ratio_ort = np sum ort_mask nb_elements np testing assert_allclose ratio_pytorch ratio_ort rtol= atol= script_model = torch jit script model y = model input output = y cpu numpy model_onnx = io BytesIO torch onnx export model x model_onnx opset_version=self opset_version do_constant_folding=False training=torch onnx TrainingMode TRAINING dynamo=False ort_sess = verification _ort_session model_onnx ort_outs = verification _run_onnx ort_sess x ort_mask = np where ort_outs = pyt_mask = np where output = ratio_pytorch = np sum pyt_mask nb_elements ratio_ort = np sum ort_mask nb_elements np testing assert_allclose ratio_pytorch ratio_ort rtol= atol= test_conv_bn MyModule torch nn Module __init__ - None super __init__ conv = torch nn Conv d kernel_size= stride= padding= bias=True bn = torch nn BatchNorm d affine=True forward x x = conv x bn = bn x bn model_export = MyModule x = torch randn run_test model_export x training=torch onnx TrainingMode EVAL run_test model_export x training=torch onnx TrainingMode TRAINING rtol= e- atol= e- test_multiple_conv_bn MyModule torch nn Module __init__ - None super __init__ conv = torch nn Conv d kernel_size= stride= padding= bias=False conv = torch nn Conv d kernel_size= stride= padding= bias=False conv = torch nn Conv d kernel_size= stride= padding= bias=False bn = torch nn BatchNorm d bn = torch nn BatchNorm d relu = torch nn ReLU inplace=True maxpool = torch nn MaxPool d kernel_size= stride= padding= forward x x = conv x x = bn x x = relu x x = maxpool x x = conv x x = bn x x = relu x x = conv x x = bn x x = relu x x model_export = MyModule x = torch randn run_test model_export x training=torch onnx TrainingMode TRAINING rtol= e- atol= e- run_test model_export x training=torch onnx TrainingMode EVAL skipIfUnsupportedMinOpsetVersion test_nms num_boxes = boxes = torch rand num_boxes boxes += boxes scores = torch randn num_boxes Module torch nn Module forward boxes scores torchvision ops nms boxes scores run_test Module boxes scores skipIfUnsupportedMinOpsetVersion test_batched_nms num_boxes = boxes = torch rand num_boxes boxes += boxes scores = torch randn num_boxes idxs = torch randint size= num_boxes Module torch nn Module forward boxes scores idxs torchvision ops batched_nms boxes scores idxs run_test Module boxes scores idxs skipIfUnsupportedMinOpsetVersion skipScriptTest test_clip_boxes_to_image boxes = torch randn boxes += boxes size = torch randn size_ = torch randn Module torch nn Module forward boxes size shape = size shape size shape torchvision ops boxes clip_boxes_to_image boxes shape run_test Module boxes size input_names= boxes size dynamic_axes= size additional_test_inputs= boxes size boxes size_ skipScriptTest reason= Conditioning input type via prim isinstance unsupported ONNX skipIfUnsupportedMinOpsetVersion test_roi_align x = torch rand dtype=torch float single_roi = torch tensor dtype=torch float model = torchvision ops RoIAlign run_test model x single_roi skipScriptTest reason= Conditioning input type via prim isinstance unsupported ONNX skipIfUnsupportedMinOpsetVersion test_roi_align_aligned x = torch rand dtype=torch float single_roi = torch tensor dtype=torch float model = torchvision ops RoIAlign aligned=True run_test model x single_roi x = torch rand dtype=torch float single_roi = torch tensor dtype=torch float model = torchvision ops RoIAlign aligned=True run_test model x single_roi x = torch rand dtype=torch float single_roi = torch tensor dtype=torch float model = torchvision ops RoIAlign aligned=True run_test model x single_roi x = torch rand dtype=torch float single_roi = torch tensor dtype=torch float model = torchvision ops RoIAlign aligned=True run_test model x single_roi skipScriptTest reason= Conditioning input type via prim isinstance unsupported ONNX skipIfUnsupportedMinOpsetVersion test_roi_pool x = torch rand dtype=torch float rois = torch tensor dtype=torch float pool_h = pool_w = model = torchvision ops RoIPool pool_h pool_w run_test model x rois skipIfUnsupportedMinOpsetVersion test_resize_images TransformModule torch nn Module __init__ - None super __init__ transform = _init_test_generalized_rcnn_transform forward images transform resize images None input = torch rand input_test = torch rand run_test TransformModule input input_names= input dynamic_axes= input additional_test_inputs= input input_test skipIfUnsupportedMinOpsetVersion skipScriptTest test_transform_images TransformModule torch nn Module __init__ - None super __init__ transform = _init_test_generalized_rcnn_transform forward images list Tensor transform images tensors input = torch rand torch rand input_test = torch rand torch rand run_test TransformModule input additional_test_inputs= input input_test get_features images s s = images shape - features = torch rand s s torch rand s s torch rand s s torch rand s s torch rand s s features = OrderedDict features features skipIfUnsupportedMinOpsetVersion skipScriptTest test_rpn RPNModule torch nn Module __init__ - None super __init__ rpn = _init_test_rpn forward images features dict str Tensor images_m = torchvision models detection image_list ImageList images i shape - i shape - i images rpn images_m features images = torch rand features = get_features images images = torch rand test_features = get_features images model = RPNModule model eval model images features run_test model images features input_names= input input input input input input dynamic_axes= input input input input input input additional_test_inputs= images features images test_features dict_check=False skipIfUnsupportedMaxOpsetVersion TODO Opset RoiAlign result mismatch skipIfUnsupportedMinOpsetVersion skipScriptTest test_multi_scale_roi_align TransformModule torch nn Module __init__ - None super __init__ model = torchvision ops MultiScaleRoIAlign feat feat image_sizes = forward input dict str Tensor boxes list Tensor - Tensor model input boxes image_sizes i = OrderedDict i feat = torch rand i feat = torch rand boxes = torch rand boxes += boxes i = OrderedDict i feat = torch rand i feat = torch rand boxes = torch rand boxes += boxes run_test TransformModule i boxes additional_test_inputs= i boxes i boxes test_set_ M torch nn Module forward x y x set_ y x x = torch ones y = torch randn run_test M x y remained_onnx_input_idx= y = torch randn run_test M x y remained_onnx_input_idx= input_names= x y dynamic_axes= x y additional_test_inputs= y y skipIfUnsupportedMinOpsetVersion test_set_attr_modules InnerModule torch nn Module __init__ embedding_dim super __init__ weights = InnerModule get_embedding embedding_dim _float_tensor = torch nn Buffer torch FloatTensor const = staticmethod get_embedding embedding_dim int emb = embedding_dim - emb = torch exp torch arange embedding_dim dtype=torch float -emb emb forward input incremental_state Optional Tensor = None bsz seq_len = input shape input shape const = weights None weights = InnerModule get_embedding embedding_dim weights = weights _float_tensor weights = weights const incremental_state None pos = seq_len weights + pos expand bsz - weights index_select torch ones bsz seq_len dtype=torch int view bsz seq_len - InnerModule torch nn Module __init__ embedding_dim super __init__ weights = InnerModule get_embedding embedding_dim module = InnerModule embedding_dim= staticmethod get_embedding embedding_dim int emb = embedding_dim - emb = torch exp torch arange embedding_dim dtype=torch float -emb emb forward x module x + weights Module torch nn Module __init__ - None super __init__ module = InnerModule embedding_dim= forward x module x x = torch randn run_test Module x input_names= x dynamic_axes= x run_test Module x remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_set_attr_modules_ InnerModule torch nn Module __init__ embedding_dim super __init__ embedding_dim = embedding_dim const = weights = InnerModule get_embedding embedding_dim _float_tensor = torch nn Buffer torch FloatTensor staticmethod get_embedding embedding_dim int emb = embedding_dim - emb = torch exp torch arange embedding_dim dtype=torch float -emb emb forward input incremental_state Optional Tensor = None bsz seq_len = input shape input shape const = weights = InnerModule get_embedding embedding_dim weights index_select torch ones bsz seq_len dtype=torch int view bsz seq_len - const Module torch nn Module __init__ - None super __init__ module = InnerModule embedding_dim= forward x module x x = torch randn run_test Module x input_names= x dynamic_axes= x run_test Module x remained_onnx_input_idx= test_set_attr MyModule torch nn Module __init__ - None super __init__ conv = torch nn Conv d b = False forward box_regression weight b = True conv weight = weight w = torch softmax conv weight dim= conv weight = w + w b box_regression + conv weight box_regression - conv weight model = torch jit script MyModule weight = torch ones box_regression = torch randn run_test model box_regression weight skipIfUnsupportedMinOpsetVersion test_set_attr_ MyModule torch nn Module __init__ - None super __init__ conv = torch nn Conv d conv bias = torch nn Parameter torch zeros set_cell_anchors anchors conv bias None b = conv bias assert b None conv bias = anchors + b conv weight None conv weight = torch randn conv bias = conv weight forward anchors - Optional Tensor set_cell_anchors anchors conv bias model = torch jit script MyModule anchors = torch ones run_test model anchors skipIfUnsupportedMinOpsetVersion test_set_attr_ MyModule torch nn Module __init__ - None super __init__ conv = torch nn Conv d conv weight = torch nn Parameter torch zeros conv bias = torch nn Parameter torch zeros set_cell_anchors anchors boxes conv weight = torch ones conv bias None conv bias = torch randn conv weight = anchors + conv weight boxes = torch zeros forward anchors - tuple Tensor Tensor boxes = torch ones set_cell_anchors anchors boxes conv bias None conv weight boxes anchors boxes model = torch jit script MyModule anchors = torch rand run_test model anchors skipIfUnsupportedMinOpsetVersion test_set_attr_ MyModule torch nn Module __init__ - None super __init__ conv = torch nn Conv d conv bias = torch nn Parameter torch zeros set_cell_anchors anchors conv weight = torch zeros conv bias None w = conv bias assert w None conv bias = anchors + w conv bias = torch ones forward feature_maps anchors - tuple Tensor Tensor set_cell_anchors anchors result = conv bias None = conv bias assert None result += result += feature_maps result result model = torch jit script MyModule x = torch rand anchors = torch ones run_test model x anchors skipIfUnsupportedMinOpsetVersion test_set_attr_ MyModule torch nn Module __init__ - None super __init__ conv = torch nn Conv d conv bias = torch nn Parameter torch zeros set_cell_anchors anchors conv weight = torch arange i range i == _ range w = conv weight conv weight = torch arange + w conv weight = conv weight + torch arange NOTE ` None ` ` assert ` passing torchscript conv bias None = conv bias assert None conv bias = anchors + forward anchors set_cell_anchors anchors conv weight conv bias model = torch jit script MyModule anchors = torch ones run_test model anchors skipIfUnsupportedMinOpsetVersion test_set_attr_in_loop MyModule torch nn Module __init__ - None super __init__ conv = torch nn Conv d conv weight = torch nn Parameter torch zeros conv bias = torch nn Parameter torch zeros set_cell_anchors anchors boxes conv weight = torch randn i range conv weight size j range conv bias = torch randn conv weight = anchors i boxes j += torch ones forward anchors - tuple Tensor Tensor boxes = torch ones set_cell_anchors anchors boxes conv bias None conv weight boxes anchors boxes model = torch jit script MyModule anchors = torch rand run_test model anchors skipIfUnsupportedMinOpsetVersion test_set_attr_in_loop_with_list MyModule torch nn Module __init__ - None super __init__ conv = torch nn Conv d conv weight = torch nn Parameter torch zeros conv bias = torch nn Parameter torch zeros boxes list Tensor = torch ones Workaround placeholder TorchScript set_cell_anchors anchors conv weight = torch randn i range conv weight size _ range conv bias = torch randn conv weight = anchors i boxes append torch ones forward anchors - tuple Tensor list Tensor boxes = set_cell_anchors anchors conv bias None conv weight boxes anchors boxes model = torch jit script MyModule anchors = torch rand run_test model anchors skipIfUnsupportedMinOpsetVersion test_index_put_if torch jit script check_init input_data Tensor hidden_size int prev_state Tensor - tuple Tensor Tensor batch_size = input_data size spatial_size_ = input_data size spatial_size_ = input_data size generate empty prev_state None provided state_size = batch_size hidden_size spatial_size_ spatial_size_ state = torch zeros state_size device=input_data device state_copy = torch zeros state_size device=input_data device prev_state size == state = torch zeros batch_size hidden_size spatial_size_ spatial_size_ + state state_copy = torch ones batch_size hidden_size spatial_size_ spatial_size_ state_copy = torch zeros batch_size hidden_size spatial_size_ spatial_size_ state = torch ones batch_size hidden_size spatial_size_ spatial_size_ state state_copy Example torch nn Module __init__ hidden_size super __init__ hidden_size = hidden_size forward input_data prev_state prev_state = check_init input_data hidden_size prev_state prev_state prev_state model = Example random_data = torch rand empty_tensor = torch tensor dtype=torch float view run_test model random_data empty_tensor input_names= random_data empty_tensor dynamic_axes= random_data empty_tensor run_test model random_data empty_tensor remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_index_put_if_ torch jit script check_init input_data Tensor hidden_size int prev_state Tensor - tuple Tensor Tensor batch_size = input_data size spatial_size_ = input_data size spatial_size_ = input_data size generate empty prev_state None provided state_size = batch_size hidden_size spatial_size_ spatial_size_ state = torch zeros state_size device=input_data device state_copy = torch zeros state_size device=input_data device prev_state size == i range state = torch ones batch_size hidden_size spatial_size_ spatial_size_ i state_copy = torch ones batch_size hidden_size spatial_size_ spatial_size_ i prev_state size == s = state state = prev_state + s prev_state size == state = torch ones batch_size hidden_size spatial_size_ spatial_size_ state state_copy Example torch nn Module __init__ hidden_size super __init__ hidden_size = hidden_size forward input_data prev_state prev_state = check_init input_data hidden_size prev_state prev_state prev_state model = Example random_data = torch rand empty_tensor = torch tensor dtype=torch float view random_state = torch rand run_test model random_data empty_tensor input_names= data state dynamic_axes= data state additional_test_inputs= random_data random_state run_test model random_data empty_tensor input_names= data state dynamic_axes= state additional_test_inputs= random_data random_state remained_onnx_input_idx= run_test model random_data empty_tensor remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_index_put_if_ torch jit script check_init input_data Tensor hidden_size int prev_state Tensor - Tensor batch_size = input_data size spatial_size_ = input_data size spatial_size_ = input_data size generate empty prev_state None provided state_size = batch_size hidden_size spatial_size_ spatial_size_ state = torch zeros state_size device=input_data device prev_state size state = state prev_state size == state = torch ones batch_size hidden_size spatial_size_ spatial_size_ state = state + state Example torch nn Module __init__ hidden_size super __init__ hidden_size = hidden_size forward input_data prev_state prev_state = check_init input_data hidden_size prev_state prev_state model = Example random_data = torch rand empty_tensor = torch tensor dtype=torch float view run_test model random_data empty_tensor input_names= random_data empty_tensor dynamic_axes= random_data empty_tensor run_test model random_data empty_tensor remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_index_put_if_ torch jit script check_init input_data Tensor hidden_size int prev_state Tensor - Tensor batch_size = input_data size spatial_size_ = input_data size spatial_size_ = input_data size generate empty prev_state None provided state_size = batch_size hidden_size spatial_size_ spatial_size_ state = torch zeros state_size device=input_data device prev_state size == state = state + state = torch ones batch_size hidden_size spatial_size_ spatial_size_ state = state + state = torch ones batch_size hidden_size spatial_size_ spatial_size_ state = state + state Example torch nn Module __init__ hidden_size super __init__ hidden_size = hidden_size forward input_data prev_state prev_state = check_init input_data hidden_size prev_state prev_state model = Example random_data = torch rand empty_tensor = torch tensor dtype=torch float view run_test model random_data empty_tensor input_names= random_data empty_tensor dynamic_axes= random_data empty_tensor run_test model random_data empty_tensor remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_index_put_if_ torch jit script check_init input_data Tensor hidden_size int prev_state Tensor - tuple Tensor Tensor batch_size = input_data size spatial_size_ = input_data size spatial_size_ = input_data size generate empty prev_state None provided state_size = batch_size hidden_size spatial_size_ spatial_size_ state = torch zeros state_size device=input_data device state_ref = state prev_state size == state = torch ones batch_size hidden_size spatial_size_ spatial_size_ state = state + state = torch ones batch_size hidden_size spatial_size_ spatial_size_ state = state + state state_ref Example torch nn Module __init__ hidden_size super __init__ hidden_size = hidden_size forward input_data prev_state prev_state state_ref = check_init input_data hidden_size prev_state prev_state state_ref model = Example random_data = torch rand empty_tensor = torch tensor dtype=torch float view run_test model random_data empty_tensor input_names= random_data empty_tensor dynamic_axes= random_data empty_tensor run_test model random_data empty_tensor remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_list_append_in_block ListModel torch nn Module forward x y res = i range x size res append torch matmul x i y res model = torch jit script ListModel x = torch randn y = torch randn run_test model x y skipIfUnsupportedMinOpsetVersion test_list_append_in_nested_block ListModel torch nn Module forward x y res = i range x size j range x size res append torch matmul x i j y res model = torch jit script ListModel x = torch randn y = torch randn run_test model x y skipIfUnsupportedMinOpsetVersion test_list_pop_in_block ListModel torch nn Module forward x y res = elem = torch matmul x y i range x size res append torch matmul x i y _ range x size elem = res pop i range x size res append torch matmul x i y elem = res pop res append elem model = torch jit script ListModel x = torch randn y = torch randn run_test model x y skipIfUnsupportedMinOpsetVersion test_list_del_in_block ListModel torch nn Module forward x y res = elem = torch matmul x y i range x size res append torch matmul x i y _ range x size del res i range x size res append torch matmul x i y del res res append elem model = torch jit script ListModel x = torch randn y = torch randn run_test model x y skipIfUnsupportedMinOpsetVersion test_list_unpack ListModel torch nn Module forward x y res = elem = torch matmul x y i range x size res append torch matmul x i y b c = res b model = torch jit script ListModel x = torch randn y = torch randn run_test model x y skipIfUnsupportedMinOpsetVersion test_index_put_inplace_ops torch jit script check_init input_data Tensor hidden_size int - Tensor batch_size = input_data size spatial_size_ = input_data size spatial_size_ = input_data size generate empty prev_state None provided state_size = batch_size hidden_size spatial_size_ spatial_size_ state = torch zeros state_size device=input_data device input_data size == state += torch ones batch_size hidden_size spatial_size_ spatial_size_ state = torch ones batch_size hidden_size spatial_size_ spatial_size_ i range input_data size state += torch ones batch_size hidden_size spatial_size_ spatial_size_ state = torch ones batch_size hidden_size spatial_size_ spatial_size_ i state Example torch nn Module __init__ hidden_size super __init__ hidden_size = hidden_size forward input_data state = check_init input_data hidden_size state model = Example random_data = torch rand run_test model random_data input_names= random_data dynamic_axes= random_data run_test model random_data remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_input_mask_model InputMaskModel torch nn Module __init__ output_size super __init__ bias = torch nn Parameter torch empty output_size dtype=torch float torch no_grad bias zero_ forward model_input y input_mask = model_input = &#124; model_input y input_mask = output = y + bias output output_size = m = InputMaskModel output_size x = torch tensor dtype=torch int y = torch tensor dtype=torch float run_test m x y InputMaskModel torch nn Module __init__ output_size super __init__ forward model_input_ model_input_ y input_mask_ = model_input_ = &#124; model_input_ input_mask_ = model_input_ &#124; model_input_ = y input_mask_ input_mask_ = y output_size = m = InputMaskModel output_size x = torch tensor dtype=torch int x = torch tensor dtype=torch int y = torch tensor dtype=torch float run_test m x x y skipScriptTest test_unsafe_chunk ChunkModel torch nn Module forward x torch unsafe_chunk x dim= model = ChunkModel model eval x = torch randn run_test model x input_names= x test_symbolic_shape_inference ConstantOfShape tested test_embedding_bag Tile tested test_repeat test Shape Reshape Transpose Gather ShapeModel torch nn Module forward x y shape = x size + - shape batch - y = y reshape shape batch batch y transpose model = ShapeModel model eval x = torch ones y = torch ones run_test model x y input_names= x y dynamic_axes= x y run_test model x y remained_onnx_input_idx= ViewModel torch nn Module forward x x view - model = ViewModel model eval x = torch tensor run_test model x test prim ListConstruct Reshape input ViewModel_ torch nn Module forward x N C H W = x shape x shape x shape x shape x = x view N - C H W x = x permute x reshape N - C model = ViewModel_ model eval x = torch ones run_test model x skipIfUnsupportedMinOpsetVersion test_symbolic_shape_inference_arange test Range ArangeModel torch nn Module forward signal frame_step = outer_dimensions = signal size - frames frame_length = signal size - subframe_length = signal size subframe_step = frame_step subframe_length subframes_per_frame = frame_length subframe_length output_size = frame_step frames - + frame_length output_subframes = output_size subframe_length frame = torch arange output_subframes frame model = ArangeModel model eval M C K N = x = torch randint M C K N y = torch randint M C + K + N + run_test model x input_names= x dynamic_axes= x run_test model x remained_onnx_input_idx= run_test model x input_names= x dynamic_axes= x additional_test_inputs= x y skipIfUnsupportedMinOpsetVersion test_symbolic_shape_inference_box test NonZero BoxModel torch nn Module forward boxes min_size = e- ws hs = boxes - boxes boxes - boxes keep = ws = min_size hs = min_size keep = torch where keep keep model = BoxModel model eval x = torch ones y = torch ones run_test model x run_test model x input_names= x dynamic_axes= x additional_test_inputs= x y skipIfUnsupportedMinOpsetVersion test_symbolic_shape_inference_box_if test If BoxIfModel torch nn Module forward boxes scores score_thresh = inds = torch where scores score_thresh boxes_ = boxes inds boxes_ numel boxes_ boxes_ model = BoxIfModel model eval boxes = torch ones scores = torch ones run_test model boxes scores skipIfUnsupportedMinOpsetVersion skipDtypeChecking test_symbolic_shape_inference_arange_ test Range ArangeModel torch nn Module forward start torch arange start size dtype=torch int x = torch randn run_test ArangeModel x input_names= x dynamic_axes= x run_test ArangeModel x remained_onnx_input_idx= ArangeModel torch nn Module forward start torch arange start size dtype=torch double x = torch randn run_test ArangeModel x input_names= x dynamic_axes= x run_test ArangeModel x remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_symbolic_shape_inference_nonzero OneLikeModel torch nn Module forward x ones = torch ones_like x dtype=torch float layout=torch strided device=torch device cpu torch nonzero ones x = torch randn run_test OneLikeModel x input_names= x dynamic_axes= x run_test OneLikeModel x remained_onnx_input_idx= x = torch randn run_test OneLikeModel x input_names= x dynamic_axes= x run_test OneLikeModel x remained_onnx_input_idx= ZeroLikeModel torch nn Module forward x zeros = torch zeros_like x dtype=torch float layout=torch strided device=torch device cpu torch nonzero zeros x = torch randn run_test ZeroLikeModel x input_names= x dynamic_axes= x run_test ZeroLikeModel x remained_onnx_input_idx= x = torch randn run_test ZeroLikeModel x input_names= x dynamic_axes= x run_test ZeroLikeModel x remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_symbolic_shape_inference_expand_ ExpandModel torch nn Module forward x x expand x = torch randn requires_grad=True run_test ExpandModel x skipIfUnsupportedMinOpsetVersion test_symbolic_shape_inference_expand_ M torch nn Module forward x input_shape = x size batch_size seq_length = input_shape seq_ids = torch arange seq_length causal_mask = seq_ids None None repeat batch_size seq_length = seq_ids None None causal_mask transpose x = torch randn run_test M x input_names= x dynamic_axes= x run_test M x remained_onnx_input_idx= skipIfUnsupportedMinOpsetVersion test_symbolic_shape_inference_slice M torch nn Module forward x position_bias input_shape = x size batch_size seq_length = input_shape position_bias = position_bias -seq_length position_bias transpose x = torch randn position_bias = torch randn run_test M x position_bias input_names= x position_bias dynamic_axes= x position_bias run_test M x position_bias remained_onnx_input_idx= test_symbolic_shape_inference_slice_ M torch nn Module forward position_bias position_bias = position_bias - position_bias transpose position_bias = torch randn run_test M position_bias skipIfUnsupportedMinOpsetVersion skipScriptTest test_symbolic_shape_inference_time input = torch randn RNN_SEQUENCE_LENGTH BATCH_SIZE RNN_INPUT_SIZE h = torch randn BATCH_SIZE RNN_HIDDEN_SIZE c = torch randn BATCH_SIZE RNN_HIDDEN_SIZE model_lstm = torch nn LSTM RNN_INPUT_SIZE RNN_HIDDEN_SIZE bidirectional=False run_test model_lstm input h c input_names= x y dynamic_axes= x model_gru = torch nn GRU RNN_INPUT_SIZE RNN_HIDDEN_SIZE bidirectional=False bias=False run_test model_gru input h input_names= x y dynamic_axes= x model_rnn = torch nn RNN RNN_INPUT_SIZE RNN_HIDDEN_SIZE bidirectional=False bias=False run_test model_rnn input h input_names= x y dynamic_axes= x test_symbolic_shape_inference_dynamic_axes M torch nn Module forward input_ids input_shape = input_ids size input_ids = input_ids view - input_shape - input_ids transpose x = torch randn run_test M x input_names= input_ids dynamic_axes= input_ids batch sequence skipIfUnsupportedMinOpsetVersion test_hann_window_periodic HannWindowModule_Periodic torch nn Module __init__ - None super __init__ window_length = forward x window_length int window_length = window_length torch add x torch hann_window window_length periodic=True dtype=torch float win_length = x = torch randn win_length module = HannWindowModule_Periodic run_test module x win_length skipIfUnsupportedMinOpsetVersion test_hann_window_not_periodic HannWindowModule_NotPeriodic torch nn Module __init__ - None super __init__ window_length = forward x window_length int window_length = window_length torch add x torch hann_window window_length periodic=False dtype=torch float win_length = x = torch randn win_length module = HannWindowModule_NotPeriodic run_test module x win_length skipIfUnsupportedMinOpsetVersion skipScriptTest test_hann_window_default_values HannWindowModule torch nn Module __init__ - None super __init__ window_length = forward x window_length int torch nn functional F window_length = window_length torch add x F relu torch hann_window window_length win_length = x = torch randn win_length dtype=torch float module = HannWindowModule output = module x win_length run_test module x win_length skipIfUnsupportedMinOpsetVersion test_tensordot_dim_count M torch nn Module forward x y output = torch tensordot x y output x = torch randint y = torch randint run_test M x y skipIfUnsupportedMinOpsetVersion test_tensordot_dim_list M torch nn Module forward x y output = torch tensordot x y - - output x = torch randint y = torch randint run_test M x y skipIfUnsupportedMinOpsetVersion test_tensordot_dynamic_dim M torch nn Module forward x y output = torch tensordot x y output x = torch randint y = torch randint new_x = torch randint new_y = torch randint run_test M x y additional_test_inputs= new_x new_y input_names= input_x input_y dynamic_axes= input_x input_y skipIfUnsupportedMinOpsetVersion test_to_device M_ToDevice torch nn Module forward x y x y device y M_ToDeviceDtype torch nn Module forward x y x y device dtype=torch long y x = torch randn y = torch randn run_test M_ToDevice x y run_test M_ToDeviceDtype x y skipIfUnsupportedMinOpsetVersion test_fill FillModule torch nn Module forward x filled_value int x fill_ filled_value x = torch randn filled_value = run_test FillModule x filled_value FillFloatModule torch nn Module forward x filled_value float x fill_ filled_value x = torch randn filled_value = run_test FillFloatModule x filled_value FillScalarModule torch nn Module forward x res = x + res fill_ res x x = torch ones dtype=torch long run_test FillScalarModule x skipIfUnsupportedMinOpsetVersion test_index_add_normal M torch nn Module __init__ dim index updates super __init__ dim = dim index = index updates = updates forward x x index_add_ dim index updates x x = torch ones updates = torch tensor dtype=torch float index = torch tensor run_test M index updates x x = torch ones updates = torch tensor dtype=torch float index = torch tensor run_test M index updates x updates = torch tensor dtype=torch float index = torch tensor run_test M index updates x skipIfUnsupportedMinOpsetVersion test_index_add_dim_size_differ M torch nn Module __init__ dim index updates super __init__ dim = dim index = index updates = updates forward x x index_add_ dim index updates x x = torch ones updates = torch tensor dtype=torch float index = torch tensor run_test M index updates x skipIfUnsupportedMinOpsetVersion test_index_add_in_loop M torch nn Module __init__ dim index updates loop_count super __init__ dim = dim index = index updates = updates loop_count = loop_count forward x _ range loop_count x index_add_ dim index updates x x = torch ones updates = torch tensor dtype=torch float index = torch tensor loop_count = torch randint item run_test M index updates loop_count x skipIfUnsupportedMinOpsetVersion test_index_add_if M torch nn Module __init__ dim updates index_true index_false super __init__ dim = dim updates = updates index_true = index_true index_false = index_false forward x cond cond x index_add_ dim index_true updates x index_add_ dim index_false updates x x = torch ones updates = torch tensor dtype=torch float index_true = torch tensor index_false = torch tensor cond = torch tensor dtype=torch bool run_test torch jit script M updates index_true index_false x cond skipIfUnsupportedMinOpsetVersion test_index_add_dynamic_axes M torch nn Module __init__ dim index updates super __init__ dim = dim index = index updates = updates forward x x index_add_ dim index updates x x = torch ones updates = torch tensor dtype=torch float index = torch tensor run_test M index updates x input_names= input_ dynamic_axes= input_ test_roll M torch nn Module __init__ shifts dims super __init__ shifts = shifts dims = dims forward x torch roll x shifts dims x = torch randn run_test M x run_test M x run_test M x run_test M - - - x test_sum M torch nn Module forward x torch sum x x = torch ones run_test M x input_names= x dynamic_axes= x skipShapeChecking test_sum_empty_tensor M torch nn Module forward x x sum x sum x = torch ones run_test M x x = torch ones run_test M x x = torch ones run_test M x skipIfUnsupportedMinOpsetVersion test_broad_cast_tensors M torch nn Module forward x y m = torch broadcast_tensors x y m x = torch randint y = torch randint run_test M x y x = torch randint y = torch randint run_test M x y x = torch randn y = torch randn run_test M x y skipIfUnsupportedMinOpsetVersion test_scaled_dot_product_attention M torch nn Module forward q k v torch nn functional scaled_dot_product_attention q k v scale= Parameters batch_size = Number samples batch num_heads = Number attention heads seq_length = Sequence length head_dim = Dimensionality each head Create random query key value tensors q = torch randn batch_size num_heads seq_length head_dim k = torch randn batch_size num_heads seq_length head_dim v = torch randn batch_size num_heads seq_length head_dim run_test M q k v skipScriptTest skipIfUnsupportedMinOpsetVersion test_dist_normal M torch nn Module forward x y torch distributions Normal x y sample size x y run_test M torch tensor torch tensor run_test M torch tensor torch tensor run_test M torch tensor torch tensor skipScriptTest skipIfUnsupportedMinOpsetVersion test_dist_normal_correctness M torch nn Module forward x y torch distributions Normal x y sample expected_mean = expected_std = model_export = M dummy_input = torch tensor expected_mean torch tensor expected_std model_onnx = io BytesIO torch onnx export model_export dummy_input model_onnx opset_version=self opset_version dynamo=False ort_sess = verification _ort_session model_onnx ort_out = verification _run_onnx ort_sess inputs=dummy_input actual_std = np std ort_out actual_mean = np mean ort_out assert abs abs actual_mean - expected_mean = expected_mean gap mean between ort outputs expected one unacceptable assert abs abs actual_std - expected_std = expected_std gap variance between ort outputs expected one unacceptable skipScriptTest skipIfUnsupportedMinOpsetVersion test_nn_init_normal_correctness expected_mean = expected_std = M torch nn Module forward x = torch ones new_empty torch nn init normal_ x expected_mean expected_std x model_export = M model_onnx = io BytesIO test_inputs = torch onnx export model_export test_inputs model_onnx opset_version=self opset_version dynamo=False ort_sess = verification _ort_session model_onnx ort_out = verification _run_onnx ort_sess inputs=test_inputs actual_std = np std ort_out actual_mean = np mean ort_out assert abs abs actual_mean - expected_mean = expected_mean gap mean between ort outputs expected one unacceptable assert abs abs actual_std - expected_std = expected_std gap variance between ort outputs expected one unacceptable skipScriptTest skipIfUnsupportedMinOpsetVersion test_dist_uniform M torch nn Module forward x y torch distributions Uniform x y sample size x y run_test M torch tensor torch tensor run_test M torch tensor torch tensor run_test M torch tensor torch tensor skipScriptTest skipIfUnsupportedMinOpsetVersion test_dist_uniform_correctness M torch nn Module forward x y torch distributions Uniform x y sample expected_min = expected_max = expected_mean = expected_min + expected_max model_export = M dummy_input = torch tensor expected_min torch tensor expected_max model_onnx = io BytesIO torch onnx export model_export dummy_input model_onnx opset_version=self opset_version dynamo=False ort_sess = verification _ort_session model_onnx ort_out = verification _run_onnx ort_sess inputs=dummy_input actual_min = np min ort_out actual_max = np max ort_out actual_mean = np mean ort_out assert actual_min = expected_min minimum value ort outputs out scope assert actual_max = expected_max maximum value ort outputs out scope assert abs actual_mean - expected_mean = expected_mean mean value ort outputs out scope skipIfUnsupportedMinOpsetVersion test_sequence_to_int M torch nn Module forward x result = torch tensor i range x size dtype=torch int x result x = torch randn run_test M x skipIfUnsupportedMinOpsetVersion test_sequence_to_float M torch nn Module forward x result = torch tensor i range x size dtype=torch float x result x = torch randn run_test M x skipIfUnsupportedMinOpsetVersion test_sequence_to_bool M torch nn Module forward x result = torch tensor False i range x size dtype=torch bool x result x = torch randn run_test M x test_tuple_output_from_if_with_raised_exception M torch nn Module forward t Tensor - tuple Tensor Tensor float t raise Exception Negative input noqa TRY torch zeros torch zeros x = torch zeros run_test torch jit script M x NOTE For quantization tests choose scale zero point carefully such inputs outputs do always overflow underflow Otherwise test results could inaccurate skipIfUnsupportedMinOpsetVersion test_quantized_linear model = torch ao nn quantized Linear Set fixed weight avoid flaky test weight = torch quantize_per_tensor torch arange dtype=torch float view torch qint Set non-zero bias bias = torch arange dtype=torch float model set_weight_bias weight bias Set fixed input avoid flaky test input = torch randn input = torch arange dtype=torch float view - input_tensor = torch quantize_per_tensor input torch quint run_test model input_tensor skipIfUnsupportedMinOpsetVersion test_quantized_conv d model = torch ao nn quantized Conv d stride= Manually initialize model weight bias random numbers By default all zeros q_weight = torch quantize_per_tensor torch randn torch qint bias = torch arange torch float - model set_weight_bias q_weight bias input = torch randn q_input = torch quantize_per_tensor input torch quint run_test model q_input skipIfUnsupportedMinOpsetVersion test_quantized_conv d model = torch ao nn quantized Conv d stride= Manually initialize model weight bias random numbers By default all zeros q_weight = torch quantize_per_tensor torch randn torch qint bias = torch arange torch float - model set_weight_bias q_weight bias input = torch randn q_input = torch quantize_per_tensor input torch quint run_test model q_input skipIfUnsupportedMinOpsetVersion skipIfQuantizationBackendQNNPack test_quantized_conv d model = torch ao nn quantized Conv d stride= Manually initialize model weight bias random numbers By default all zeros q_weight = torch quantize_per_tensor torch randn torch qint bias = torch arange torch float - model set_weight_bias q_weight bias input = torch randn q_input = torch quantize_per_tensor input torch quint run_test model q_input skipIfUnsupportedMinOpsetVersion test_quantized_adaptive_avg_pool d model = torch nn AdaptiveAvgPool d input = torch randn q_input = torch quantize_per_tensor input torch quint run_test model q_input skipIfUnsupportedMinOpsetVersion test_quantized_conv d_relu model = torch ao nn intrinsic quantized ConvReLU d stride= Manually initialize model weight bias random numbers By default all zeros q_weight = torch quantize_per_tensor torch randn torch qint bias = torch arange torch float - model set_weight_bias q_weight bias input = torch randn q_input = torch quantize_per_tensor input torch quint run_test model q_input skipIfUnsupportedMinOpsetVersion test_quantized_conv d_relu model = torch ao nn intrinsic quantized ConvReLU d stride= Manually initialize model weight bias random numbers By default all zeros q_weight = torch quantize_per_tensor torch randn torch qint bias = torch arange torch float - model set_weight_bias q_weight bias input = torch randn q_input = torch quantize_per_tensor input torch quint run_test model q_input skipIfUnsupportedMinOpsetVersion skipIfQuantizationBackendQNNPack test_quantized_conv d_relu model = torch ao nn intrinsic quantized ConvReLU d stride= Manually initialize model weight bias random numbers By default all zeros q_weight = torch quantize_per_tensor torch randn torch qint bias = torch arange torch float - model set_weight_bias q_weight bias input = torch randn q_input = torch quantize_per_tensor input torch quint run_test model q_input skipIfUnsupportedMinOpsetVersion test_quantized_conv_transpose d model = torch ao nn quantized ConvTranspose d output_padding= stride= Manually initialize model weight bias random numbers By default all zeros q_weight = torch quantize_per_tensor torch randn torch qint bias = torch arange torch float - model set_weight_bias q_weight bias input = torch randn q_input = torch quantize_per_tensor input torch quint run_test model q_input skipIfUnsupportedMinOpsetVersion test_quantized_conv_transpose d model = torch ao nn quantized ConvTranspose d output_padding= stride= Manually initialize model weight bias random numbers By default all zeros q_weight = torch quantize_per_tensor torch randn torch qint bias = torch arange torch float - model set_weight_bias q_weight bias input = torch randn q_input = torch quantize_per_tensor input torch quint run_test model q_input skipIfUnsupportedMinOpsetVersion skipIfQuantizationBackendQNNPack test_quantized_conv_transpose d model = torch ao nn quantized ConvTranspose d output_padding= stride= Manually initialize model weight bias random numbers By default all zeros q_weight = torch quantize_per_tensor torch randn torch qint bias = torch arange torch float - model set_weight_bias q_weight bias input = torch randn q_input = torch quantize_per_tensor input torch quint run_test model q_input common_utils parametrize function_or_module common_utils subtest torch nn ReLU name= relu common_utils subtest torch nn LeakyReLU name= leaky_relu common_utils subtest torch ao nn quantized LeakyReLU name= quantized_leaky_relu common_utils subtest torch ao nn quantized Hardswish name= quantized_hardswish common_utils subtest torch nn Sigmoid name= sigmoid common_utils subtest torch ao nn quantized Sigmoid name= quantized_sigmoid common_utils subtest torch nn Hardsigmoid name= hardsigmoid common_utils subtest torch nn Tanh name= tanh common_utils subtest torch nn Hardtanh name= hardtanh common_utils subtest lambda x torch transpose x name= transpose common_utils subtest lambda x x expand name= expand common_utils subtest lambda x x view name= view common_utils subtest lambda x x select name= select common_utils subtest torch ao nn quantized LayerNorm torch nn Parameter torch ones torch nn Parameter torch zeros name= layer_norm common_utils subtest torch ao nn quantized InstanceNorm d torch nn Parameter torch ones torch nn Parameter torch zeros name= instance_norm common_utils subtest torch ao nn quantized GroupNorm torch nn Parameter torch zeros torch nn Parameter torch zeros name= group_norm common_utils subtest lambda x torch as_strided x name= as_strided skipScriptTest skipIfUnsupportedMinOpsetVersion test_quantized_unary_ops function_or_module input = torch randn q_input = torch quantize_per_tensor input torch quint Model torch nn Module __init__ function_or_module super __init__ function_or_module = function_or_module forward x function_or_module x run_test Model function_or_module q_input skipIfUnsupportedMinOpsetVersion test_quantized_flatten FlattenModel torch nn Module forward input torch flatten input x = torch quantize_per_tensor torch randn torch quint run_test FlattenModel x skipIfUnsupportedMinOpsetVersion skipScriptTest torch jit frontend FrontendError Cannot instantiate QFunctional script function test_quantized_cat_when_concatinating_the_same_tensor QuantizedSelfConcatenationModel torch nn Module forward x torch ao nn quantized QFunctional cat x x dim= q_input = torch quantize_per_tensor torch ones torch quint run_test QuantizedSelfConcatenationModel q_input common_utils parametrize x y common_utils subtest torch quantize_per_tensor torch ones torch quint torch quantize_per_tensor torch zeros torch quint name= different_shape common_utils subtest torch quantize_per_tensor torch ones torch quint torch quantize_per_tensor torch ones torch quint name= different_scale common_utils subtest torch quantize_per_tensor torch ones torch quint torch quantize_per_tensor torch ones torch quint name= different_zero_point common_utils subtest torch quantize_per_tensor torch ones torch quint torch quantize_per_tensor torch ones torch quint name= different_zero_point_and_scale skipIfUnsupportedMinOpsetVersion skipScriptTest torch jit frontend FrontendError Cannot instantiate QFunctional script function test_quantized_cat x torch Tensor y torch Tensor QuantizedConcatenationModel torch nn Module forward x y torch ao nn quantized QFunctional cat x y dim= run_test QuantizedConcatenationModel x y skipIfUnsupportedMinOpsetVersion torch jit frontend FrontendError Cannot instantiate QFunctional script function skipScriptTest test_quantized_arithmetic_qfunctional x = torch quantize_per_tensor torch randn torch quint y = torch quantize_per_tensor torch randn torch quint ArithmeticModel torch nn Module forward x y o = torch ao nn quantized QFunctional add x y o = torch ao nn quantized QFunctional mul o x o run_test ArithmeticModel x y skipIfUnsupportedMinOpsetVersion test_quantized_arithmetic x = torch quantize_per_tensor torch randn torch quint y = torch quantize_per_tensor torch randn torch quint ArithmeticModel torch nn Module forward x y o = torch ops quantized add x y o = torch ops quantized mul o x o run_test ArithmeticModel x y skipIfUnsupportedMinOpsetVersion test_quantize_per_tensor Module torch nn Module forward x torch quantize_per_tensor x torch qint torch quantize_per_tensor x torch quint x = torch randn run_test Module x skipIfUnsupportedMinOpsetVersion test_dequantize Module torch nn Module forward x torch dequantize x x = torch quantize_per_tensor torch randn torch qint run_test Module x skipIfUnsupportedMinOpsetVersion test_qat_linear_per_channel M torch nn Module __init__ - None super __init__ quant = torch ao quantization QuantStub linear = torch nn Linear dequant = torch ao quantization DeQuantStub forward x x = quant x x = linear x x = dequant x x model = M model qconfig = torch ao quantization get_default_qconfig fbgemm model = torch ao quantization prepare_qat model Set fixed weight bias avoid flaky test model linear weight = torch nn Parameter _construct_tensor_for_quantization_test model linear bias = torch nn Parameter torch arange dtype=torch float model = torch ao quantization convert model Set fixed input avoid flaky test input = _construct_tensor_for_quantization_test offset=- run_test model input unittest skip ORT fails Validating no unexpected access using invalid node_index torch converted model skipIfUnsupportedMinOpsetVersion test_quantized_list_of_inputs_with_cat TestModel torch nn Module __init__ - None super __init__ quant = torch ao quantization QuantStub dequant = torch ao quantization DeQuantStub forward x x = quant x x = torch cat x x x = dequant x x model = TestModel model qconfig = torch ao quantization get_default_qconfig fbgemm model = torch ao quantization prepare_qat model model = torch ao quantization convert model x = torch randn run_test model x skipIfUnsupportedMinOpsetVersion test_qat_relu M torch nn Module __init__ - None super __init__ quant = torch ao quantization QuantStub relu = torch nn ReLU dequant = torch ao quantization DeQuantStub forward x x = quant x x = relu x x = dequant x x model = M model qconfig = torch ao quantization get_default_qconfig fbgemm model = torch ao quantization prepare_qat model model = torch ao quantization convert model input = torch randn run_test model input skipIfUnsupportedMinOpsetVersion test_qat_conv d M torch nn Module __init__ - None super __init__ quant = torch ao quantization QuantStub conv = torch nn Conv d stride= dequant = torch ao quantization DeQuantStub forward x x = quant x x = conv x x = dequant x x model = M model qconfig = torch ao quantization get_default_qconfig fbgemm model = torch ao quantization prepare_qat model Set fixed weight bias avoid flaky test model conv weight = torch nn Parameter _construct_tensor_for_quantization_test max_val= model conv bias = torch nn Parameter torch tensor model = torch ao quantization convert model Set fixed input avoid flaky test input = _construct_tensor_for_quantization_test offset=- max_val= run_test model input skipIfUnsupportedMinOpsetVersion test_qat_conv d_relu M torch nn Module __init__ - None super __init__ quant = torch ao quantization QuantStub conv = torch nn Conv d stride= relu = torch nn ReLU dequant = torch ao quantization DeQuantStub forward x x = quant x x = conv x x = relu x x = dequant x x model = M model qconfig = torch ao quantization get_default_qconfig fbgemm model = torch ao quantization prepare_qat model Set fixed weight bias avoid flaky test model conv weight = torch nn Parameter _construct_tensor_for_quantization_test max_val= model conv bias = torch nn Parameter torch tensor model = torch ao quantization convert model Set fixed input avoid flaky test input = _construct_tensor_for_quantization_test offset=- max_val= run_test model input skipIfUnsupportedMinOpsetVersion test_qat_conv d_relu_fused M torch nn Module __init__ - None super __init__ quant = torch ao quantization QuantStub conv = torch nn Conv d stride= relu = torch nn ReLU dequant = torch ao quantization DeQuantStub forward x x = quant x x = conv x x = relu x x = dequant x x model = M model qconfig = torch ao quantization get_default_qconfig fbgemm model = torch ao quantization fuse_modules model eval conv relu model = torch ao quantization prepare_qat model train Set fixed weight bias avoid flaky test model conv weight = torch nn Parameter _construct_tensor_for_quantization_test max_val= model conv bias = torch nn Parameter torch tensor model = torch ao quantization convert model Set fixed input avoid flaky test input = _construct_tensor_for_quantization_test offset=- max_val= run_test model input skipIfUnsupportedMinOpsetVersion test_qat_linear_relu_fused M torch nn Module __init__ - None super __init__ quant = torch ao quantization QuantStub linear = torch nn Linear relu = torch nn ReLU dequant = torch ao quantization DeQuantStub forward x x = quant x x = linear x x = relu x x = dequant x x model = M model qconfig = torch ao quantization get_default_qconfig fbgemm model = torch ao quantization fuse_modules model eval linear relu model = torch ao quantization prepare_qat model train Set fixed weight bias avoid flaky test model linear weight = torch nn Parameter _construct_tensor_for_quantization_test max_val= model linear bias = torch nn Parameter torch tensor model = torch ao quantization convert model Set fixed input avoid flaky test input = _construct_tensor_for_quantization_test offset=- max_val= run_test model input skipIfUnsupportedMinOpsetVersion test_qat_maxpool d M torch nn Module __init__ - None super __init__ quant = torch ao quantization QuantStub pool = torch nn MaxPool d kernel_size= stride= padding= dequant = torch ao quantization DeQuantStub forward x x = quant x x = pool x x = dequant x x model = M model qconfig = torch ao quantization get_default_qconfig fbgemm model = torch ao quantization prepare_qat model train model = torch ao quantization convert model Set fixed input avoid flaky test input = _construct_tensor_for_quantization_test run_test model input skipIfUnsupportedMinOpsetVersion skipScriptTest Scale Zero-point must scalar ORT optimization test_qat_avg_pool d model = torch nn Sequential torch ao quantization QuantStub torch nn AvgPool d kernel_size= stride= padding= torch ao quantization DeQuantStub model qconfig = torch ao quantization get_default_qconfig fbgemm model = torch ao quantization prepare_qat model train model = torch ao quantization convert model input = _construct_tensor_for_quantization_test run_test model input skipIfUnsupportedMinOpsetVersion test_qat_upsample_nearest d model = torch nn Sequential torch ao quantization QuantStub torch nn UpsamplingNearest d scale_factor= torch ao quantization DeQuantStub model qconfig = torch ao quantization get_default_qconfig fbgemm model = torch ao quantization prepare_qat model train model = torch ao quantization convert model input = _construct_tensor_for_quantization_test run_test model input test_ d_tensor_broadcast fn torch nn Module forward x y = torch add x y b = torch mul y y + b x = torch ones y = torch ones run_test fn x y input_names= x y output_names= output skipIfUnsupportedMinOpsetVersion test_convolution_allow_tf Module torch nn Module __init__ allow_tf super __init__ allow_tf = allow_tf weight = torch rand weight = torch nn Parameter weight forward x allow_tf torch _convolution x weight None False False False True True torch _convolution x weight None False False False True x = torch randn run_test Module False x rtol= e- atol= e- run_test Module True x rtol= e- atol= e- AffineGridModule torch nn Module __init__ align_corners - None super __init__ align_corners = align_corners forward theta size torch nn functional affine_grid theta size align_corners skipIfUnsupportedMinOpsetVersion skipScriptTest common_utils parametrize align_corners True False common_utils parametrize theta_params np array - np array np array - - np array common_utils parametrize size test_affine_grid_ d align_corners theta_params size angle translation scale = theta_params theta = np array dtype=np float _ range size angle_radian = angle np pi theta = np append theta np cos angle_radian scale -np sin angle_radian translation np sin angle_radian np cos angle_radian scale translation theta = theta reshape size theta = torch Tensor theta run_test TestONNXRuntime AffineGridModule align_corners theta size skipIfUnsupportedMinOpsetVersion skipScriptTest common_utils parametrize align_corners True False common_utils parametrize theta_params np array - np array - np array - - np array common_utils parametrize size test_affine_grid_ d align_corners theta_params size angle translation scale = theta_params theta = np array dtype=np float _ range size angle_radian_x = angle np pi angle_radian_y = angle np pi rot_matrix_x = np array np cos angle_radian_x -np sin angle_radian_x np sin angle_radian_x np cos angle_radian_x rot_matrix_y = np array np cos angle_radian_y np sin angle_radian_y -np sin angle_radian_y np cos angle_radian_y rot_matrix = np matmul rot_matrix_x rot_matrix_y rot_matrix = rot_matrix scale reshape rot_matrix = np append rot_matrix np reshape translation axis= theta = np append theta rot_matrix flatten theta = theta reshape size theta = torch Tensor theta run_test TestONNXRuntime AffineGridModule align_corners theta size skipIfUnsupportedMinOpsetVersion common_utils parametrize mode bilinear nearest bicubic common_utils parametrize padding_mode zeros border reflection common_utils parametrize align_corners True False name_fn=lambda align_corners str align_corners test_grid_sample mode padding_mode align_corners n c d_in h_in w_in d_out h_out w_out = atol_rtol = mode padding_mode == bicubic border align_corners atol_rtol update atol rtol atol_rtol update atol rtol input grid = torch randn n c h_in w_in torch randn n h_out w_out GridSampleModule torch nn Module __init__ mode padding_mode align_corners - None super __init__ mode padding_mode align_corners = mode padding_mode align_corners forward input grid torch nn functional grid_sample input grid mode padding_mode align_corners run_test GridSampleModule mode padding_mode align_corners input grid atol_rtol ONNX Opset GridSample D volumetric input supported volumetric_input_tensor = torch randn n c d_in h_in w_in volumetric_grid_tensor = torch randn n d_out h_out w_out mode padding_mode align_corners itertools product bilinear nearest PyTorch grid_sample bicubic mode does support D volumetric input zeros border reflection True False opset_version assertRaises torch onnx OnnxExporterError run_test GridSampleModule mode padding_mode align_corners volumetric_input_tensor volumetric_grid_tensor atol_rtol run_test GridSampleModule mode padding_mode align_corners volumetric_input_tensor volumetric_grid_tensor atol_rtol IfNoneInput torch nn Module forward x - Optional Tensor y Optional Tensor = None x size y = x y IfNoneOutput torch nn Module forward x - Optional Tensor y Optional Tensor = x x size y = None y LoopNoneInput torch nn Module forward x - Optional Tensor y Optional Tensor = None _ range x size y = x y LoopNoneOutput torch nn Module forward x - Optional Tensor y Optional Tensor = x _ range x size y = None y common_utils parametrize module_class IfNoneOutput IfNoneInput LoopNoneOutput LoopNoneInput name_fn=lambda module_class module_class __name__ common_utils parametrize x_size name_fn=lambda x_size str x_size skipTraceTest skipIfUnsupportedMinOpsetVersion test_optional_output module_class type torch nn Module x_size int Need scripting preserve control flow test meaningful model = torch jit script module_class f = io BytesIO x = torch ones x_size dynamic_axis_name = condition torch onnx export model x f opset_version=self opset_version Ensure condition constant dynamic_axes= x dynamic_axis_name input_names= x dynamo=False exported = onnx load_from_string f getvalue expected_elem_type = JitScalarType from_value x onnx_type expected_output_type = onnx helper make_optional_type_proto onnx helper make_tensor_type_proto expected_elem_type dynamic_axis_name assertEqual expected_output_type exported graph output type node exported graph node Both branches output types should match node op_type == If attr node attribute attr name then_branch else_branch assertEqual expected_output_type attr g output type run_test module_class x Ensure condition constant dynamic_axes= x dynamic_axis_name input_names= x skipTraceTest skipIfUnsupportedMinOpsetVersion test_uninitialized_optional Module torch nn Module forward y Optional Tensor - Optional Tensor y None y shape y size == y = y + y y run_test Module torch ones dtype=torch int dynamic_axes= y y y input_names= y skipIfUnsupportedMinOpsetVersion test_device_eq M torch nn Module forward exercise both Tensor device prim device torch device prim Constant device = torch device cpu torch zeros_like mod = torch jit script M preserve control flow run_test mod In order ONNX model behavior match torch model we need construct input has same device checked forward In ONNX there no such thing device so condition always false torch randn device= cpu Force dynamic axes so output shape depends input Otherwise entire model will just constant have any inputs input_names= dynamic_axes= skipIfUnsupportedMinOpsetVersion test_lerp LerpModel torch nn Module forward x x lerp torch full_like x x lerp torch full_like x x lerp torch full_like x torch tensor x lerp torch full_like x x x lerp torch tensor x x lerp torch tensor x lerp torch tensor torch tensor run_test LerpModel torch rand common_utils parametrize input_dtype torch cfloat torch float skipIfUnsupportedMinOpsetVersion test_print_tensor_within_torch_nn_module input_dtype torch dtype PrintTensorOnMyModel torch nn Module forward x print has side effect calling resolve_conj resolve_neg x_firsts = x print f x_firsts x_firsts tolist has side effect calling resolve_conj resolve_neg Annotation added pass torch script _ list float = x tolist x_firsts m = PrintTensorOnMyModel x = torch randn dtype=input_dtype input_dtype == torch cfloat assertRaises RuntimeError run_test m x run_test m x skipScriptTest skipIfUnsupportedMinOpsetVersion unittest skipIf torch hub _check_module_exists torch_geometric torch_geometric installed test_sage_conv torch_geometric nn torch_geometric_nn Input coords = torch randn coords = torch randn coords = torch transpose torch cat coords coords dim= adj = torch_geometric_nn knn_graph coords k= batch=None loop=True edge_from = adj edge_to = adj inputs = coords coords edge_from edge_to MySAGEConv torch nn Module __init__ - None super __init__ SAGEConvBlock = torch_geometric_nn SAGEConv normalize=True bano = torch_geometric_nn BatchNorm relu = torch nn ReLU dense = torch nn Seq Lin noqa F sigmoid = torch nn Sigmoid forward coords coords edge_from edge_to adj = torch cat edge_from edge_to dim= gra = torch transpose torch cat coords coords dim= x = SAGEConvBlock gra edge_index=adj x = torch unsqueeze torch sum x dim= x input_names = coords coords edge_from edge_to output_names = outputs dynamic_axes = coords batch_size features coords batch_size features edge_from batch_size features edge_to batch_size features outputs batch_size run_test MySAGEConv inputs input_names=input_names output_names=output_names dynamic_axes=dynamic_axes Cannot export older opsets because ConstantFill op ConstantFill temp op removed opset This no longer supported onnxruntime There still some issues prevent us enabling script test these scenarios test_gru_ Operator aten as_tensor supported exporter yet - https msdata visualstudio com Vienna _workitems edit Operator aten _pack_padded_sequence supported exporter yet - https msdata visualstudio com Vienna _workitems edit test_elman_ Compiling script mode fails errors like torch jit frontend UnsupportedNodeError annotated assignments without assigned value aren t supported - https msdata visualstudio com Vienna _workitems edit test_lstm_ Compiling script mode fails errors like RuntimeError Arguments call valid - https msdata visualstudio com Vienna _workitems edit skipScriptTest skipIfUnsupportedMinOpsetVersion common_utils parametrize name nonlinearity elman relu elman tanh lstm None gru None common_utils parametrize _parametrize_rnn_args layers common_utils parametrize _parametrize_rnn_args bidirectional common_utils parametrize _parametrize_rnn_args initial_state common_utils parametrize _parametrize_rnn_args packed_sequence common_utils parametrize _parametrize_rnn_args dropout test_rnn args kwargs _dispatch_rnn_test args kwargs __name__ == __main__ common_utils TestCase _default_dtype_check_enabled = True common_utils run_tests