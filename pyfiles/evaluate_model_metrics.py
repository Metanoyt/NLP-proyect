mypy allow-untyped-defs argparse numpy np type ignore pandas pd type ignore sklearn type ignore dlrm_s_pytorch unpack_batch type ignore dlrm_utils type ignore dlrm_wrap fetch_model make_test_data_loader torch inference_and_evaluation dlrm test_dataloader device Perform inference evaluation test dataset The function returns dictionary contains evaluation metrics such accuracy f auc precision recall Note This function rewritten version ` ` ` inference ` ` ` present dlrm_s_pytorch py Args dlrm nn Module dlrm model object test_data_loader torch dataloader dataloader test dataset device torch device device which inference happens nbatches = len test_dataloader scores = targets = i testBatch enumerate test_dataloader early exit nbatches set user exceeded nbatches i = nbatches break X_test lS_o_test lS_i_test T_test _ _ = unpack_batch testBatch forward pass X_test lS_o_test lS_i_test = dlrm_wrap X_test lS_o_test lS_i_test device ndevices= Z_test = dlrm X_test lS_o_test lS_i_test S_test = Z_test detach cpu numpy numpy array T_test = T_test detach cpu numpy numpy array scores append S_test targets append T_test scores = np concatenate scores axis= targets = np concatenate targets axis= metrics = recall lambda y_true y_score sklearn metrics recall_score y_true=y_true y_pred=np round y_score precision lambda y_true y_score sklearn metrics precision_score y_true=y_true y_pred=np round y_score f lambda y_true y_score sklearn metrics f _score y_true=y_true y_pred=np round y_score ap sklearn metrics average_precision_score roc_auc sklearn metrics roc_auc_score accuracy lambda y_true y_score sklearn metrics accuracy_score y_true=y_true y_pred=np round y_score log_loss lambda y_true y_score sklearn metrics log_loss y_true=y_true y_pred=y_score all_metrics = metric_name metric_function metrics items all_metrics metric_name = round metric_function targets scores all_metrics evaluate_metrics test_dataloader sparse_model_metadata Evaluates metrics sparsified metrics dlrm model various sparsity levels block shapes norms This function evaluates model test dataset dumps evaluation metrics csv file model_performance csv metadata = pd read_csv sparse_model_metadata device = torch device cuda torch cuda is_available torch device cpu metrics_dict dict str list = norm sparse_block_shape sparsity_level precision recall f roc_auc accuracy log_loss _ row metadata iterrows norm sbs sl = row norm row sparse_block_shape row sparsity_level model_path = row path model = fetch_model model_path device model_metrics = inference_and_evaluation model test_dataloader device key = f norm _ sbs _ sl print key = model_metrics metrics_dict norm append norm metrics_dict sparse_block_shape append sbs metrics_dict sparsity_level append sl key value model_metrics items key metrics_dict metrics_dict key append value sparse_model_metrics = pd DataFrame metrics_dict print sparse_model_metrics filename = sparse_model_metrics csv sparse_model_metrics to_csv filename index=False print f Model metrics file saved filename __name__ == __main__ parser = argparse ArgumentParser parser add_argument -- raw-data-file -- raw_data_file type=str parser add_argument -- processed-data-file -- processed_data_file type=str parser add_argument -- sparse-model-metadata -- sparse_model_metadata type=str args = parser parse_args Fetch test data loader test_dataloader = make_test_data_loader args raw_data_file args processed_data_file Evaluate metrics evaluate_metrics test_dataloader args sparse_model_metadata