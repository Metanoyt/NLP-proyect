mypy allow-untyped-defs r Autograd anomaly mode warnings torch __all__ = detect_anomaly set_detect_anomaly detect_anomaly r Context-manager enable anomaly detection autograd engine This does two things - Running forward pass detection enabled will allow backward pass print traceback forward operation created failing backward function - If ` ` check_nan ` ` ` ` True ` ` any backward computation generate nan value will raise error Default ` ` True ` ` warning This mode should enabled only debugging different tests will slow down your program execution Example xdoctest +REQUIRES env TORCH_DOCTEST_ANOMALY torch torch autograd MyFunc autograd Function staticmethod forward ctx inp inp clone staticmethod backward ctx gO Error during backward pass raise RuntimeError Some error backward gO clone run_fn out = MyFunc apply out sum inp = torch rand requires_grad=True out = run_fn inp out backward Traceback most recent call last File stdin line module File your pytorch install torch _tensor py line backward torch autograd backward gradient retain_graph create_graph File your pytorch install torch autograd __init__ py line backward allow_unreachable=True allow_unreachable flag File your pytorch install torch autograd function py line apply _forward_cls backward args File stdin line backward RuntimeError Some error backward autograd detect_anomaly inp = torch rand requires_grad=True out = run_fn inp out backward Traceback forward call caused error File tmp py line module out = run_fn inp File tmp py line run_fn out = MyFunc apply Traceback most recent call last File stdin line module File your pytorch install torch _tensor py line backward torch autograd backward gradient retain_graph create_graph File your pytorch install torch autograd __init__ py line backward allow_unreachable=True allow_unreachable flag File your pytorch install torch autograd function py line apply _forward_cls backward args File stdin line backward RuntimeError Some error backward __init__ check_nan=True - None noqa D prev = torch is_anomaly_enabled check_nan = check_nan prev_check_nan = torch is_anomaly_check_nan_enabled warnings warn Anomaly Detection has been enabled This mode will increase runtime should only enabled debugging stacklevel= __enter__ - None noqa D torch set_anomaly_enabled True check_nan __exit__ args object - None noqa D torch set_anomaly_enabled prev prev_check_nan set_detect_anomaly r Context-manager sets anomaly detection autograd engine off ` ` set_detect_anomaly ` ` will enable disable autograd anomaly detection based its argument attr ` mode ` It can used context-manager function See ` ` detect_anomaly ` ` above details anomaly detection behaviour Args mode bool Flag whether enable anomaly detection ` ` True ` ` disable ` ` False ` ` check_nan bool Flag whether raise error when backward generate nan __init__ mode bool check_nan bool = True - None noqa D prev = torch is_anomaly_enabled prev_check_nan = torch is_anomaly_check_nan_enabled torch set_anomaly_enabled mode check_nan __enter__ - None noqa D pass __exit__ args object - None noqa D torch set_anomaly_enabled prev prev_check_nan