Taken https github com pytorch audio blob master torchaudio models wav letter py So we don t need torchaudio installed math collections OrderedDict typing Optional torch torch nn functional F torch nn Tensor __all__ = Wav Letter Wav Letter nn Module r Wav Letter model architecture ` Wav Letter End-to-End ConvNet-based Speech Recognition System https arxiv org abs ` _ paper math ` \text padding = \frac \text ceil \text kernel - \text stride ` Args num_classes int optional Number classes classified Default ` ` ` ` input_type str optional Wav Letter can use input ` ` waveform ` ` ` ` power_spectrum ` ` ` ` mfcc ` ` Default ` ` waveform ` ` num_features int optional Number input features network will receive Default ` ` ` ` __init__ num_classes int = input_type str = waveform num_features int = - None super __init__ acoustic_num_features = input_type == waveform num_features acoustic_model = nn Sequential nn Conv d in_channels=acoustic_num_features out_channels= kernel_size= stride= padding= nn ReLU inplace=True nn Conv d in_channels= out_channels= kernel_size= stride= padding= nn ReLU inplace=True nn Conv d in_channels= out_channels= kernel_size= stride= padding= nn ReLU inplace=True nn Conv d in_channels= out_channels= kernel_size= stride= padding= nn ReLU inplace=True nn Conv d in_channels= out_channels= kernel_size= stride= padding= nn ReLU inplace=True nn Conv d in_channels= out_channels= kernel_size= stride= padding= nn ReLU inplace=True nn Conv d in_channels= out_channels= kernel_size= stride= padding= nn ReLU inplace=True nn Conv d in_channels= out_channels= kernel_size= stride= padding= nn ReLU inplace=True nn Conv d in_channels= out_channels= kernel_size= stride= padding= nn ReLU inplace=True nn Conv d in_channels= out_channels= kernel_size= stride= padding= nn ReLU inplace=True nn Conv d in_channels= out_channels=num_classes kernel_size= stride= padding= nn ReLU inplace=True input_type == waveform waveform_model = nn Sequential nn Conv d in_channels=num_features out_channels= kernel_size= stride= padding= nn ReLU inplace=True acoustic_model = nn Sequential waveform_model acoustic_model input_type power_spectrum mfcc acoustic_model = acoustic_model forward x Tensor - Tensor r Args x Tensor Tensor dimension batch_size num_features input_length Returns Tensor Predictor tensor dimension batch_size number_of_classes input_length x = acoustic_model x x = nn functional log_softmax x dim= x Taken https github com SeanNaren deepspeech pytorch modifications SequenceWise nn Module __init__ module Collapses input dim T N H T N H applies module Allows handling variable sequence lengths minibatch sizes param module Module apply input super __init__ module = module forward x t n = x size x size x = x view t n - x = module x x = x view t n - x __repr__ tmpstr = __class__ __name__ + \n tmpstr += module __repr__ tmpstr += tmpstr MaskConv nn Module __init__ seq_module Adds padding output module based given lengths This ensure results model do change when batch sizes change during inference Input needs shape BxCxDxT param seq_module The sequential module containing conv stack super __init__ seq_module = seq_module forward x lengths param x The input size BxCxDxT param lengths The actual length each sequence batch Masked output module module seq_module x = module x mask = torch BoolTensor x size fill_ x is_cuda mask = mask cuda i length enumerate lengths length = length item mask i size - length mask i narrow length mask i size - length fill_ x = x masked_fill mask x lengths InferenceBatchSoftmax nn Module forward input_ training F softmax input_ dim=- input_ BatchRNN nn Module __init__ input_size hidden_size rnn_type=nn LSTM bidirectional=False batch_norm=True super __init__ input_size = input_size hidden_size = hidden_size bidirectional = bidirectional batch_norm = SequenceWise nn BatchNorm d input_size batch_norm None rnn = rnn_type input_size=input_size hidden_size=hidden_size bidirectional=bidirectional bias=True num_directions = bidirectional flatten_parameters rnn flatten_parameters forward x output_lengths batch_norm None x = batch_norm x x = nn utils rnn pack_padded_sequence x output_lengths enforce_sorted=False x h = rnn x x _ = nn utils rnn pad_packed_sequence x bidirectional x = x view x size x size - sum view x size x size - TxNxH - TxNxH sum x Lookahead nn Module Wang et al - Lookahead Convolution Layer Unidirectional Recurrent Neural Networks input shape - sequence batch feature - TxNxH output shape - same input __init__ n_features context super __init__ assert context context = context n_features = n_features pad = context - conv = nn Conv d n_features n_features kernel_size=self context stride= groups=self n_features padding= bias=None forward x x = x transpose transpose x = F pad x pad=self pad value= x = conv x x = x transpose transpose contiguous x __repr__ __class__ __name__ + + n_features= + str n_features + context= + str context + DeepSpeech nn Module __init__ rnn_type labels rnn_hidden_size nb_layers audio_conf bidirectional context= super __init__ hidden_size = rnn_hidden_size hidden_layers = nb_layers rnn_type = rnn_type audio_conf = audio_conf labels = labels bidirectional = bidirectional sample_rate = audio_conf sample_rate window_size = audio_conf window_size num_classes = len labels conv = MaskConv nn Sequential nn Conv d kernel_size= stride= padding= nn BatchNorm d nn Hardtanh inplace=True nn Conv d kernel_size= stride= padding= nn BatchNorm d nn Hardtanh inplace=True Based above convolutions spectrogram size using conv formula W - F + P S+ rnn_input_size = int math floor sample_rate window_size + rnn_input_size = int math floor rnn_input_size + - + rnn_input_size = int math floor rnn_input_size + - + rnn_input_size = rnns = rnn = BatchRNN input_size=rnn_input_size hidden_size=rnn_hidden_size rnn_type=rnn_type bidirectional=bidirectional batch_norm=False rnns append rnn x range nb_layers - rnn = BatchRNN input_size=rnn_hidden_size hidden_size=rnn_hidden_size rnn_type=rnn_type bidirectional=bidirectional rnns append f x + d rnn rnns = nn Sequential OrderedDict rnns lookahead = nn Sequential consider adding batch norm Lookahead rnn_hidden_size context=context nn Hardtanh inplace=True bidirectional None fully_connected = nn Sequential nn BatchNorm d rnn_hidden_size nn Linear rnn_hidden_size num_classes bias=False fc = nn Sequential SequenceWise fully_connected inference_softmax = InferenceBatchSoftmax forward x lengths lengths = lengths cpu int output_lengths = get_seq_lens lengths x _ = conv x output_lengths sizes = x size x = x view sizes sizes sizes sizes Collapse feature dimension x = x transpose transpose contiguous TxNxH rnn rnns x = rnn x output_lengths bidirectional no need lookahead layer bidirectional x = lookahead x x = fc x x = x transpose identity training mode softmax eval mode x = inference_softmax x x output_lengths get_seq_lens input_length Given D Tensor Variable containing integer sequence lengths D tensor variable containing size sequences will output network param input_length D Tensor D Tensor scaled model seq_len = input_length m conv modules type m nn modules conv Conv d seq_len = seq_len + m padding - m dilation m kernel_size - - seq_len = seq_len true_divide m stride + seq_len int Taken https github com pytorch examples blob master word_language_model model py#L -L PositionalEncoding nn Module r Inject some information about relative absolute position tokens sequence The positional encodings have same dimension embeddings so two can summed Here we use sine cosine functions different frequencies math \text PosEncoder pos i = sin pos ^ i d_model \text PosEncoder pos i+ = cos pos ^ i d_model \text where pos word position i embed idx Args d_model embed dim required dropout dropout value default= max_len max length incoming sequence default= Examples pos_encoder = PositionalEncoding d_model __init__ d_model dropout= max_len= super __init__ dropout = nn Dropout p=dropout pe = torch zeros max_len d_model position = torch arange max_len dtype=torch float unsqueeze div_term = torch exp torch arange d_model float -math log d_model pe = torch sin position div_term pe = torch cos position div_term pe = pe unsqueeze transpose register_buffer pe pe forward x r Inputs forward function Args x sequence fed positional encoder model required Shape x sequence length batch size embed dim output sequence length batch size embed dim Examples output = pos_encoder x x = x + pe x size dropout x TransformerModel nn Module Container module encoder recurrent transformer module decoder __init__ ntoken ninp nhead nhid nlayers dropout= super __init__ try torch nn TransformerEncoder TransformerEncoderLayer except Exception e raise ImportError TransformerEncoder module does exist PyTorch lower e model_type = Transformer src_mask = None pos_encoder = PositionalEncoding ninp dropout encoder_layers = TransformerEncoderLayer ninp nhead nhid dropout transformer_encoder = TransformerEncoder encoder_layers nlayers encoder = nn Embedding ntoken ninp ninp = ninp decoder = nn Linear ninp ntoken init_weights init_weights initrange = nn init uniform_ encoder weight -initrange initrange Not sure how works original code nn init zeros_ decoder nn init uniform_ decoder weight -initrange initrange forward src has_mask=True has_mask device = src device This will created once during warmup src_mask None src_mask size = len src mask = nn Transformer generate_square_subsequent_mask len src device src_mask = mask src_mask = None src = encoder src math sqrt ninp src = pos_encoder src output = transformer_encoder src src_mask output = decoder output F log_softmax output dim=- From https github com pytorch text tree master torchtext nn modules MultiheadAttentionContainer torch nn Module __init__ nhead in_proj_container attention_layer out_proj r A multi-head attention container Args nhead number heads multiheadattention model in_proj_container A container multi-head in-projection linear layers k nn Linear attention_layer The attention layer out_proj The multi-head out-projection layer k nn Linear Examples torch embed_dim num_heads bsz = in_proj_container = InProjContainer torch nn Linear embed_dim embed_dim torch nn Linear embed_dim embed_dim torch nn Linear embed_dim embed_dim MHA = MultiheadAttentionContainer num_heads in_proj_container ScaledDotProduct torch nn Linear embed_dim embed_dim query = torch rand bsz embed_dim key = value = torch rand bsz embed_dim attn_output attn_weights = MHA query key value print attn_output shape torch Size super __init__ nhead = nhead in_proj_container = in_proj_container attention_layer = attention_layer out_proj = out_proj forward query torch Tensor key torch Tensor value torch Tensor attn_mask Optional torch Tensor = None bias_k Optional torch Tensor = None bias_v Optional torch Tensor = None - tuple torch Tensor torch Tensor r Args query key value Tensor map query set key-value pairs output See Attention Is All You Need more details attn_mask bias_k bias_v Tensor optional keyword arguments passed attention layer See definitions attention Shape - Inputs - query math ` L N E ` - key math ` S N E ` - value math ` S N E ` - attn_mask bias_k bias_v same shape corresponding args attention layer - Outputs - attn_output math ` L N E ` - attn_output_weights math ` N H L S ` where where L target length S sequence length H number attention heads N batch size E embedding dimension tgt_len src_len bsz embed_dim = query size - key size - query size - query size - q k v = in_proj_container query key value assert q size - nhead == query s embed_dim must divisible number heads head_dim = q size - nhead q = q reshape tgt_len bsz nhead head_dim assert k size - nhead == key s embed_dim must divisible number heads head_dim = k size - nhead k = k reshape src_len bsz nhead head_dim assert v size - nhead == value s embed_dim must divisible number heads head_dim = v size - nhead v = v reshape src_len bsz nhead head_dim attn_output attn_output_weights = attention_layer q k v attn_mask=attn_mask bias_k=bias_k bias_v=bias_v attn_output = attn_output reshape tgt_len bsz embed_dim attn_output = out_proj attn_output attn_output attn_output_weights ScaledDotProduct torch nn Module __init__ dropout= r Processes projected query key-value pair apply scaled dot product attention Args dropout float probability dropping attention weight Examples SDP = torchtext models ScaledDotProduct q = torch randn k = v = torch randn attn_output attn_weights = SDP q k v print attn_output shape attn_weights shape torch Size torch Size super __init__ dropout = dropout forward query torch Tensor key torch Tensor value torch Tensor attn_mask Optional torch Tensor = None bias_k Optional torch Tensor = None bias_v Optional torch Tensor = None - tuple torch Tensor torch Tensor r Uses scaled dot product projected key-value pair update projected query Args query Tensor Projected query key Tensor Projected key value Tensor Projected value attn_mask BoolTensor optional D mask prevents attention certain positions bias_k bias_v Tensor optional one more key value sequence added sequence dim dim=- Those used incremental decoding Users should provide non-None both arguments order activate them Shape - query math ` L N H E H ` - key math ` S N H E H ` - value math ` S N H E H ` - attn_mask math ` N H L S ` positions ` ` True ` ` allowed attend while ` ` False ` ` values will unchanged - bias_k bias_v bias math ` N H E H ` - Output math ` L N H E H ` math ` N H L S ` where L target length S source length H number attention heads N batch size E embedding dimension bias_k None bias_v None assert key size - == bias_k size - key size - == bias_k size - bias_k size - == Shape bias_k supported assert value size - == bias_v size - value size - == bias_v size - bias_v size - == Shape bias_v supported key = torch cat key bias_k value = torch cat value bias_v attn_mask None _attn_mask = attn_mask attn_mask = torch nn functional pad _attn_mask tgt_len head_dim = query size - query size - assert query size - == key size - == value size - The feature dim query key value must equal assert key size == value size Shape key value must match src_len = key size - batch_heads = max query size - key size - Scale query query key value = query transpose - - key transpose - - value transpose - - query = query float head_dim - attn_mask None attn_mask dim = raise RuntimeError attn_mask must D tensor attn_mask size - = src_len attn_mask size - = tgt_len attn_mask size - = attn_mask size - = batch_heads raise RuntimeError The size attn_mask correct attn_mask dtype = torch bool raise RuntimeError Only bool tensor supported attn_mask Dot product q k attn_output_weights = torch matmul query key mT attn_mask None attn_output_weights masked_fill_ attn_mask - e attn_output_weights = torch nn functional softmax attn_output_weights dim=- attn_output_weights = torch nn functional dropout attn_output_weights p=self dropout training=self training attn_output = torch matmul attn_output_weights value attn_output transpose - - attn_output_weights InProjContainer torch nn Module __init__ query_proj key_proj value_proj r A in-proj container process inputs Args query_proj proj layer query key_proj proj layer key value_proj proj layer value super __init__ query_proj = query_proj key_proj = key_proj value_proj = value_proj forward query torch Tensor key torch Tensor value torch Tensor - tuple torch Tensor torch Tensor torch Tensor r Projects input sequences using in-proj layers Args query key value Tensors sequence projected Shape - query key value math ` S N E ` - Output math ` S N E ` where S sequence length N batch size E embedding dimension query_proj query key_proj key value_proj value