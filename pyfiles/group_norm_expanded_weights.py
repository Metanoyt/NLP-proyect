mypy allow-untyped-defs operator functools reduce typing Optional torch torch nn functional F expanded_weights_impl ExpandedWeight implements_per_sample_grads expanded_weights_utils forward_helper set_grad_sample_if_exists standard_kwargs unpack_expanded_weight_or_tensor implements_per_sample_grads F group_norm GroupNormPerSampleGrad torch autograd Function staticmethod pyrefly ignore bad-override forward ctx kwarg_names _ expanded_args_and_kwargs expanded_args expanded_kwargs = standard_kwargs kwarg_names expanded_args_and_kwargs input num_groups = expanded_args N = input shape C = input shape HxW = reduce operator mul input shape weight bias eps = expanded_kwargs weight expanded_kwargs bias expanded_kwargs eps output mean rstd = forward_helper torch native_group_norm input weight bias N C HxW num_groups eps ctx input ctx num_groups = input num_groups ctx weight ctx eps = weight eps ctx mean ctx rstd = mean rstd isinstance bias ExpandedWeight ctx bias = bias input requires_grad isinstance weight ExpandedWeight ctx weight = weight output staticmethod pyrefly ignore bad-override backward ctx grad_output input num_groups = ctx input ctx num_groups weight bias eps = ctx weight ctx bias ctx eps mean rstd = ctx mean ctx rstd results list Optional torch Tensor = results append None kwarg names results append None op reference input requires_grad weight_c = unpack_expanded_weight_or_tensor weight lambda t t contiguous input_c = input contiguous grad_output_c = grad_output contiguous grad_output None None N = input shape C = input shape HxW = s input shape HxW = s bw_fn = torch ops aten native_group_norm_backward results append bw_fn grad_output_c input_c mean rstd weight_c N C HxW num_groups True False False results append None weight bias don t compute batched gradients no other arguments differentiable results = results + None set grad_sample field weight bias per sample gradients hasattr ctx weight set_grad_sample_if_exists weight lambda _ torch einsum ni - ni pyrefly ignore unsupported-operation F group_norm input num_groups eps=eps grad_output hasattr ctx bias set_grad_sample_if_exists bias lambda _ torch einsum ni - ni grad_output tuple results