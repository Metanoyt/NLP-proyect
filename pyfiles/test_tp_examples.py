Copyright c Meta Platforms Inc affiliates Owner s oncall distributed itertools copy deepcopy typing NamedTuple Optional torch torch distributed dist torch nn functional F torch distributed algorithms _checkpoint checkpoint_wrapper checkpoint_wrapper CheckpointImpl torch distributed tensor DeviceMesh distribute_tensor DTensor Replicate Shard torch distributed tensor debug CommDebugMode torch distributed tensor parallel ColwiseParallel loss_parallel parallelize_module RowwiseParallel torch distributed tensor parallel input_reshard input_reshard torch testing _internal common_device_type skipXPUIf torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests torch testing _internal distributed _tensor common_dtensor DTensorTestBase MLPModule ModelArgs NUM_DEVICES skip_unless_torch_gpu Transformer with_comms c d_functional = torch ops c d_functional reduce_scatter all_gather all_reduce = c d_functional reduce_scatter_tensor c d_functional all_gather_into_tensor c d_functional all_reduce ExpCommCounts NamedTuple fwd Optional dict = None bwd Optional dict = None optim Optional dict = None DistTensorParallelExampleTest DTensorTestBase _check_module m m check_grad=False named_parameters = dict m named_parameters name param_m m named_parameters assertTrue name named_parameters param_m = named_parameters name check_grad param_m = param_m grad param_m = param_m grad isinstance param_m DTensor replicate = Replicate param_m = param_m redistribute device_mesh=param_m device_mesh placements=replicate to_local assertEqual param_m param_m _test_mlp_training_e e is_seq_parallel=False recompute_activation=False inp_size = Ensure all tp ranks have same input rng_seed = rank is_seq_parallel torch manual_seed rng_seed inp = torch rand inp_size device=self device_type model = MLPModule device_type model_tp = deepcopy model Ensure model initialized same way _check_module model model_tp Shard module initialize optimizer LR = device_mesh = DeviceMesh device_type torch arange NUM_DEVICES parallelize_plan = net ColwiseParallel input_layouts=Shard is_seq_parallel ColwiseParallel net RowwiseParallel output_layouts=Shard is_seq_parallel RowwiseParallel model_tp = parallelize_module model_tp device_mesh parallelize_plan recompute_activation model_tp = input_reshard checkpoint_wrapper model_tp checkpoint_impl=CheckpointImpl NO_REENTRANT device_mesh None is_seq_parallel optim = torch optim SGD model parameters lr=LR optim_tp = torch optim SGD model_tp parameters lr=LR output = model inp output sum backward comm_mode = CommDebugMode comm_mode output_tp = model_tp inp output_tp sum backward assertEqual output output_tp is_seq_parallel assertEqual comm_mode get_comm_counts c d_functional all_gather_into_tensor assertEqual comm_mode get_comm_counts c d_functional reduce_scatter_tensor assertEqual comm_mode get_comm_counts c d_functional all_reduce is_seq_parallel Sum gradients different ranks since input different across ranks sequence parallel dist all_reduce model net weight grad dist all_reduce model net bias grad dist all_reduce model net weight grad dist all_reduce model net bias grad Ensure gradients same _check_module model model_tp check_grad=True optim step optim_tp step Ensure model weights still same after update Due trick we use Partial aggregation we only check weight when local_rank = _check_module model model_tp inp = torch rand inp_size device=self device_type output = model inp output_tp = model_tp inp assertEqual output output_tp _test_mlp_inference device_mesh inp_size = Ensure all tp ranks have same input torch manual_seed inp = torch rand inp_size device=self device_type model = MLPModule device_type model_tp = deepcopy model Ensure model initialized same way _check_module model model_tp Shard module initialize optimizer parallelize_plan = net ColwiseParallel net RowwiseParallel model_tp = parallelize_module model_tp device_mesh parallelize_plan output = model inp output_tp = model_tp inp assertEqual output output_tp with_comms parametrize is_seq_parallel True False TODO need revisit input_reshard API about why failed multi-gpu tests parametrize recompute_activation True False parametrize recompute_activation False test_mlp_training is_seq_parallel recompute_activation _test_mlp_training_e e is_seq_parallel=is_seq_parallel recompute_activation=recompute_activation with_comms test_mlp_inference device_mesh = DeviceMesh device_type torch arange NUM_DEVICES torch inference_mode _test_mlp_inference device_mesh _setup_single_gpu_model model_args dtype Transformer model_args device=self device_type dtype=dtype _setup_tp_model model is_seq_parallel dtype model_tp = deepcopy model _check_module model model_tp device_mesh = DeviceMesh device_type torch arange NUM_DEVICES local_output_for_attn = dtype torch float Transformer parallelize model_tp device_mesh is_seq_parallel local_output_for_attn=local_output_for_attn _setup_optimizer model model_tp Step Run test comparing outputs single-gpu multi-gpu models LR = optim = torch optim Adam model parameters lr=LR optim_tp = torch optim Adam model_tp parameters lr=LR optim optim_tp _validate_fwd model model_tp inp expected_comms_dict=None check_comms=True Compare outputs same input output = model inp CommDebugMode comm_mode output_tp = model_tp inp assertEqual output output_tp check_comms assertDictEqual comm_mode get_comm_counts expected_comms_dict output output_tp _validate_bwd model model_tp output output_tp expected_comms_dict=None check_comms=True Ensure gradients equal output sum backward CommDebugMode comm_mode output_tp sum backward _check_module model model_tp check_grad=True check_comms assertDictEqual comm_mode get_comm_counts expected_comms_dict _validate_optim_step model model_tp optim optim_tp expected_comms_dict=None check_comms=True optim step Ensure model weights still same after update torch distributed tensor experimental implicit_replication implicit_replication CommDebugMode comm_mode optim_tp step _check_module model model_tp check_comms assertDictEqual comm_mode get_comm_counts expected_comms_dict staticmethod _thaw_params thaw_params model model_tp thaw_params target_model model model_tp n p target_model named_parameters n thaw_params p requires_grad_ False with_comms skip_unless_torch_gpu parametrize is_seq_parallel True False parametrize dtype torch float torch float skipXPUIf True https github com intel torch-xpu-ops issues test_transformer_training is_seq_parallel dtype torch dtype EXP_BASE_CC = ExpCommCounts fwd= all_reduce all_gather bwd= all_reduce EXP_SEQ_PARALLEL_CC = ExpCommCounts fwd= reduce_scatter all_gather bwd= reduce_scatter all_gather optim= all_reduce Disable dropout test since we cannot reproduce same random behaviors when comparing single-gpu models multi-gpu models model_args = ModelArgs dropout_p= model = _setup_single_gpu_model model_args dtype Step Initialize single-gpu models model_tp = _setup_tp_model model is_seq_parallel dtype Step Setup tp model place onto device mesh optim optim_tp = _setup_optimizer model model_tp Step Setup optimizers both models Initialize input make sure all ranks have same input inp_size = batch_size seq_len is_seq_parallel assert inp_size world_size == torch manual_seed steps = type model torch float _ range steps inp = torch randint model_args vocab_size inp_size device=self device_type expected_fwd_comms = EXP_SEQ_PARALLEL_CC fwd is_seq_parallel EXP_BASE_CC fwd output output_tp = _validate_fwd model model_tp inp expected_fwd_comms expected_bwd_comms = EXP_SEQ_PARALLEL_CC bwd is_seq_parallel EXP_BASE_CC bwd _validate_bwd model model_tp output output_tp expected_bwd_comms expected_optim_comms = EXP_SEQ_PARALLEL_CC optim is_seq_parallel EXP_BASE_CC optim _validate_optim_step model model_tp optim optim_tp expected_optim_comms with_comms skip_unless_torch_gpu parametrize thaw_params is_seq_parallel dtype exp_cnts None all require grad seq_parallel float baseline True torch float ExpCommCounts bwd= reduce_scatter all_gather optim= all_reduce None all require grad no seq_parallel float baseline False torch float ExpCommCounts bwd= all_reduce test subset LayerNorm bwd output_masks output weight norm weight norm bias False True True True torch float ExpCommCounts bwd= reduce_scatter optim= all_reduce tok_embeddings weight output weight True False False True torch float ExpCommCounts bwd= reduce_scatter all_gather tok_embeddings weight output weight norm weight norm bias True True True True torch float ExpCommCounts bwd= reduce_scatter all_gather optim= all_reduce tok_embeddings weight output weight norm weight norm bias layers ffn_norm weight layers ffn_norm bias single transformerblock layernorm True torch float ExpCommCounts bwd= reduce_scatter all_gather optim= all_reduce tok_embeddings weight layers attention wv weight layers feed_forward w bias layers ffn_norm bias layers feed_forward w weight output weight varied layer param types True torch float ExpCommCounts bwd= reduce_scatter all_gather optim= all_reduce name_fn=lambda thaw seq dtype _ f seq_parallel_ seq + f str dtype split - _ + f thaw_ __ join sorted n rpartition replace _ n thaw thaw all skipXPUIf True https github com intel torch-xpu-ops issues test_transformer_req_grad thaw_params is_seq_parallel dtype exp_cnts Sample subset ` requires_grad ` patterns disabling dropout facilitate single gpu multi-device comparison disable weight-tying enable more fine-tuning configurations model_args = ModelArgs dropout_p= weight_tying=False model = _setup_single_gpu_model model_args dtype Step Initialize single-gpu models model_tp = _setup_tp_model model is_seq_parallel dtype Step Setup tp model place onto device mesh optim optim_tp = _setup_optimizer model model_tp Step Setup optimizers both models DistTensorParallelExampleTest _thaw_params thaw_params model model_tp Step set ` requires_grad ` patterns Initialize input make sure all ranks have same input inp_size = batch_size seq_len is_seq_parallel assert inp_size world_size == torch manual_seed inp = torch randint model_args vocab_size inp_size device=self device_type output output_tp = _validate_fwd model model_tp inp check_comms=False _validate_bwd model model_tp output output_tp exp_cnts bwd check_comms=True _validate_optim_step model model_tp optim optim_tp exp_cnts optim check_comms=True with_comms test_weight_tying TestModule torch nn Module __init__ - None super __init__ Initialize different weights embedding fc torch manual_seed embedding = torch nn Embedding torch manual_seed fc = torch nn Linear forward x fc embedding x model = TestModule device_type parallelize_plan = embedding ColwiseParallel fc RowwiseParallel device_mesh = DeviceMesh device_type list range world_size parallelize_module model device_mesh parallelize_plan input_size = torch manual_seed inp = torch randint input_size device=self device_type Without weight tying assertNotEqual model embedding weight to_local model fc weight to_local output = model inp output sum backward assertNotEqual model embedding weight grad to_local model fc weight grad to_local model zero_grad With weight tying model fc weight = model embedding weight assertEqual model embedding weight model fc weight assertEqual id model embedding weight id model fc weight output = model inp output sum backward assertEqual model embedding weight grad model fc weight grad assertEqual id model embedding weight grad id model fc weight grad with_comms test_loss_parallel device_mesh = build_device_mesh comm_mode = CommDebugMode channel_size channel_dim = test_setup = channel_size calling aten nll_loss_forward channel_size calling aten nll_loss d_forward weight = torch rand channel_size device=self device_type input_ndim input_size target_size test_setup x = torch rand input_size device=self device_type requires_grad=True target = torch randint channel_size target_size device=self device_type shard_dims = list range input_ndim reductions = none mean sum shard_dim reduction itertools product shard_dims reductions dist_x = distribute_tensor x device_mesh Shard shard_dim y = F cross_entropy x target weight reduction=reduction loss_parallel shard_dim == channel_dim comm_mode dist_y = F cross_entropy dist_x target weight reduction=reduction assertEqual comm_mode get_total_counts assertEqual comm_mode get_comm_counts c d_functional all_reduce assertTrue dist_y placements is_replicate assertEqual dist_y to_local y comm_mode reduction == none y sum backward dist_y sum backward y backward dist_y backward assertEqual comm_mode get_total_counts assertTrue dist_x grad placements is_shard shard_dim assertEqual dist_x grad full_tensor x grad x grad zero_ assertRaisesRegex ValueError loss_parallel dist_y = F cross_entropy dist_x target reduction=reduction instantiate_parametrized_tests DistTensorParallelExampleTest __name__ == __main__ run_tests