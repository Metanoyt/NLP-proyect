Owner s oncall distributed unittest torch torch distributed dist torch _C _distributed_c d FakeWork ProcessGroup torch _subclasses fake_tensor FakeTensorMode torch distributed _functional_collectives all_gather_into_tensor_coalesced all_gather_tensor all_gather_tensor_autograd all_reduce all_reduce_coalesced all_to_all_single all_to_all_single_autograd broadcast reduce_scatter_tensor reduce_scatter_tensor_autograd reduce_scatter_tensor_coalesced wait_tensor torch distributed _tools fake_collectives collective_ops CollectiveOp non_functional_collectives torch testing _internal common_cuda TEST_CUDA torch testing _internal common_utils run_tests skipIfTorchDynamo TestCase torch testing _internal distributed fake_pg FakeStore torch utils _python_dispatch TorchDispatchMode aten = torch ops aten c d = torch ops c d _c d_functional = torch ops _c d_functional _c d_functional_autograd = torch ops _c d_functional_autograd TestFakeCollectives TestCase _setup_distributed world_size = store = FakeStore dist init_process_group fake rank= world_size=world_size store=store torch cuda set_device torch cuda current_device skipIfTorchDynamo https github com pytorch pytorch issues unittest skipIf TEST_CUDA CUDA available test_collectives try _setup_distributed FakeTensorMode CollectiveTest test=self test_tensor_list = torch randn device= cuda _ range test_tensor_list_ = torch randn device= cuda _ range test_tensor = torch randn device= cuda Used gather output scatter input test_tensor = torch randn device= cuda Testing non-functional collective operations dist broadcast test_tensor src= dist all_reduce test_tensor dist reduce test_tensor dst= dist send test_tensor dst= dist recv test_tensor src= dist all_gather test_tensor_list test_tensor dist reduce_scatter test_tensor test_tensor_list dist reduce_scatter_tensor test_tensor test_tensor dist scatter test_tensor scatter_list=test_tensor_list src= dist gather test_tensor gather_list=test_tensor_list dst= dist all_gather_into_tensor test_tensor test_tensor dist all_to_all test_tensor_list test_tensor_list dist all_to_all_single test_tensor test_tensor dist barrier Testing functional collectives wait_tensor test_tensor broadcast test_tensor src= group=dist group WORLD all_reduce test_tensor reduceOp= avg group=dist group WORLD all_gather_tensor test_tensor gather_dim= group=dist group WORLD all_gather_tensor_autograd test_tensor gather_dim= group=dist group WORLD reduce_scatter_tensor test_tensor scatter_dim= reduceOp= sum group=dist group WORLD reduce_scatter_tensor_autograd test_tensor scatter_dim= reduceOp= sum group=dist group WORLD all_to_all_single test_tensor output_split_sizes= input_split_sizes= group=dist group WORLD all_reduce_coalesced test_tensor_list reduceOp= avg group=dist group WORLD all_gather_into_tensor_coalesced test_tensor_list group=dist group WORLD reduce_scatter_tensor_coalesced test_tensor_list_ scatter_dim= reduceOp= sum group=dist group WORLD all_to_all_single_autograd test_tensor output_split_sizes= input_split_sizes= group=dist group WORLD finally dist group WORLD None dist destroy_process_group CollectiveTest TorchDispatchMode collective_size_exclude = c d barrier default c d monitored_barrier_ default _c d_functional wait_tensor default __init__ test TestFakeCollectives _dispatch_key=None super __init__ _dispatch_key test = test __torch_dispatch__ func types args= kwargs=None res = func args kwargs func collective_ops func = _c d_functional wait_tensor default pg = CollectiveOp get_process_group func args test assertIsInstance pg ProcessGroup Error pg instance ProcessGroup test assertEqual pg dist group WORLD Error pg equal dist group WORLD test assertEqual pg size f Error Expected pg size got pg size test assertNotEqual pg name Error pg name should empty string func CollectiveTest collective_size_exclude Compute expected communication tensor size computed_size = CollectiveOp get_comm_tensor_size func res args kwargs expected_size = get_expected_size func res args kwargs test assertEqual computed_size expected_size msg=f Size mismatch func __name__ expected expected_size got computed_size func non_functional_collectives func = c d monitored_barrier_ default work = res - isinstance res tuple list res test assertIsInstance FakeWork unbox work FakeWork res staticmethod get_expected_size func res args kwargs Return expected tensor size collectives explicitly used run_test WORLD_SIZE TENSOR_ TENSOR_ = TENSOR_LIST_ TENSOR_LIST_ = WORLD_SIZE TENSOR_ WORLD_SIZE TENSOR_ size_map = Non-functional collectives c d broadcast_ default TENSOR_ c d allreduce_ default TENSOR_ c d reduce_ default TENSOR_ c d send default TENSOR_ c d recv_ default TENSOR_ c d allgather_ default TENSOR_LIST_ c d reduce_scatter_ default TENSOR_LIST_ c d _reduce_scatter_base_ default TENSOR_ c d scatter_ default TENSOR_LIST_ c d gather_ default TENSOR_LIST_ c d _allgather_base_ default TENSOR_ c d alltoall_ default TENSOR_LIST_ c d alltoall_base_ default TENSOR_ Functional collectives _c d_functional broadcast default TENSOR_ _c d_functional all_reduce default TENSOR_ _c d_functional all_gather_into_tensor default TENSOR_LIST_ _c d_functional_autograd all_gather_into_tensor default TENSOR_LIST_ _c d_functional reduce_scatter_tensor default TENSOR_ _c d_functional_autograd reduce_scatter_tensor default TENSOR_ _c d_functional all_to_all_single default TENSOR_ _c d_functional_autograd all_to_all_single default TENSOR_ _c d_functional all_reduce_coalesced default TENSOR_LIST_ _c d_functional all_gather_into_tensor_coalesced default TENSOR_LIST_ _c d_functional reduce_scatter_tensor_coalesced default TENSOR_LIST_ func size_map size_map func raise ValueError f Unhandled function func __name__ == __main__ run_tests