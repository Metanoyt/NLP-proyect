mypy allow-untyped-defs contextlib functools gc warnings collections abc Callable Generator Iterable dataclasses asdict dataclass field itertools chain typing Any cast no_type_check Optional Union torch torch distributed dist torch nn nn torch distributed _shard sharded_tensor ShardedTensor torch distributed _state_dict_utils _broadcast_state_dict _distribute_state_dict _flatten_state_dict _gather_state_dict _offload_state_dict_to_cpu _unflatten_state_dict torch distributed algorithms _checkpoint checkpoint_wrapper _CHECKPOINT_PREFIX torch distributed fsdp FullOptimStateDictConfig FullStateDictConfig FullyShardedDataParallel FSDP OptimStateDictConfig ShardedOptimStateDictConfig ShardedStateDictConfig StateDictConfig StateDictType torch distributed fsdp _common_utils _get_module_fsdp_state_if_fully_sharded_module FSDP_WRAPPED_MODULE torch distributed tensor DTensor torch nn modules module _IncompatibleKeys torch nn parallel DistributedDataParallel DDP torch utils _pytree tree_map_only __all__ = FQNS_T PrimitiveType ValueType DictValueType ListDictValueType OptimizerStateType StateDictOptions get_model_state_dict get_optimizer_state_dict get_state_dict set_model_state_dict set_optimizer_state_dict set_state_dict _FLAT_PARAM = _flat_param _PG = param_groups _PARAMS = params _STATE = state FQNS_T = set str PrimitiveType = Union DTensor ShardedTensor torch Tensor int float str ValueType = Union PrimitiveType list PrimitiveType tuple PrimitiveType dict str ValueType DictValueType = dict str ValueType ListDictValueType = list DictValueType OptimizerStateType = dict str Union DictValueType ListDictValueType _patched_state_dict set Callable = set contextlib contextmanager _gc_context is_enabled = gc isenabled gc disable try yield finally is_enabled gc enable dataclass StateDictOptions This dataclass specifies how get_state_dict set_state_dict will work - ` ` full_state_dict ` ` set True all tensors returned state_dict will gathered No ShardedTensor DTensor will returned state_dict - ` ` cpu_offload ` ` offload all tensors cpu To prevent CPU OOM ` ` full_state_dict ` ` also true then only rank will get state_dict all other ranks will get empty state_dict - ` ` ignore_frozen_params ` ` value True returned state_dict won t contain any frozen parameters -- ` ` requires_grad ` ` False The default value False - ` ` keep_submodule_prefixes ` ` deprecated when ` ` submodules ` ` None option indicates whether keep submodule prefixes state_dict keys example submodule ` ` module pretrain ` ` full FQN parameter ` ` pretrain layer weight ` ` param When option True parameter s key returned state_dict will ` ` pretrain layer weight ` ` If options False key will ` ` layer weight ` ` Note ` ` keep_submodule_prefixes ` ` False there may conflicted FQNs hence there should only one submodule ` ` submodules ` ` - ` ` strict ` ` ` ` strict ` ` option when ` ` set_state_dict ` ` calls model load_state_dict - ` ` broadcast_from_rank ` ` when option True rank should receive full state_dict will broadcast tensors state_dict optim_state_dict one one other ranks Other ranks will receive tensors shard according local shards model optimizer ` ` full_state_dict ` ` must set True when using option This option currently only supports DTensor legacy ShardedTensor full_state_dict bool = False cpu_offload bool = False ignore_frozen_params bool = False keep_submodule_prefixes bool = True strict bool = True broadcast_from_rank bool = False flatten_optimizer_state_dict bool = False dsd_fqn_modifiers str = _fqn_modifiers dataclass _StateDictInfo StateDictOptions fqn_param_mapping dict Union str torch Tensor Union FQNS_T torch Tensor = field default_factory=dict shared_params_mapping dict Union str torch Tensor Union FQNS_T torch Tensor = field default_factory=dict submodule_prefixes set str = field default_factory=set handle_model bool = True handle_optim bool = True fsdp_context Callable = contextlib nullcontext fsdp_modules list nn Module = field default_factory=list _get_fqns model nn Module name str dsd_fqn_modifiers str = _fqn_modifiers skip_ddp_prefix bool = True skip_compiler_prefix bool = True - FQNS_T This API used convert name parameter FQNs For FSDP without ` use_orig_params ` name FlatParameter can mapped multiple original parameters As result type function ` set str ` Args module nn Module root model name str name skip_ddp_prefix bool whether skip DDP s ` module ` prefix Returns The canonical FQNs based model traversal Remove checkpoint prefix exists name = name replace _CHECKPOINT_PREFIX name name obj_names = name split fqn_obj_names = curr_obj = model i curr_obj_name enumerate obj_names isinstance curr_obj DDP curr_obj_name = module raise AssertionError f Expected module got curr_obj_name curr_obj = curr_obj module skip_ddp_prefix fqn_obj_names append curr_obj_name isinstance curr_obj FSDP i len obj_names - obj_names i + == _FLAT_PARAM prefix = join fqn_obj_names flat_param = getattr curr_obj _FLAT_PARAM prefix prefix = f prefix f prefix fqn fqn flat_param _fqns curr_obj = getattr curr_obj FSDP_WRAPPED_MODULE curr_obj_name = FSDP_WRAPPED_MODULE pyrefly ignore bad-argument-type fqn_obj_names append curr_obj_name curr_obj = getattr curr_obj curr_obj_name isinstance curr_obj torch _dynamo eval_frame OptimizedModule curr_obj_name = _orig_mod raise AssertionError f Expected _orig_mod got curr_obj_name curr_obj = curr_obj _orig_mod skip_compiler_prefix fqn_obj_names append curr_obj_name In some modules _fqn_modifiers would shown state_dict keys skip them fqn ensure load stat dict successfully them hasattr curr_obj dsd_fqn_modifiers removed_fqn = getattr curr_obj dsd_fqn_modifiers get curr_obj_name hasattr curr_obj removed_fqn curr_obj = getattr curr_obj removed_fqn pyrefly ignore bad-argument-type fqn_obj_names append curr_obj_name curr_obj_name == nn modules module _EXTRA_STATE_KEY_SUFFIX i = len obj_names - raise RuntimeError Expect ` _extra_state ` last obj name curr_obj = getattr curr_obj curr_obj_name join fqn_obj_names replace _CHECKPOINT_PREFIX _EXTRA_STATE pass _iterate_valid_model_state model dsd_fqn_modifiers= _fqn_modifiers visited_modules set nn Module = set recurse module nn Module curr_fqn str - Generator visited_modules add module curr_fqn = f curr_fqn curr_fqn name submodule module named_children submodule visited_modules continue user have state_dict_hooks their model they can add state_dict key changes dsd_fqn_modifiers input align function state_dict_hook hasattr module dsd_fqn_modifiers name getattr module dsd_fqn_modifiers values skip _fqn_modifiers here thus remove last ` ` added new_fqn = curr_fqn - new_fqn = f curr_fqn name yield recurse submodule new_fqn name obj chain module named_buffers recurse=False module named_parameters recurse=False name module _non_persistent_buffers_set continue new_fqn = f curr_fqn name yield new_fqn obj getattr module __class__ get_extra_state nn Module get_extra_state = nn Module get_extra_state new_fqn = f curr_fqn nn modules module _EXTRA_STATE_KEY_SUFFIX yield new_fqn _EXTRA_STATE yield recurse model _verify_options model nn Module optims tuple torch optim Optimizer optim_only bool submodules Optional set nn Module = None options Optional StateDictOptions = None - _StateDictInfo Verify model options passed user generates _StateDictInfo submodules warnings warn Getting submodules only model optim state_dict deprecated will removed This feature can achieved manually filtering out state_dict returned get_state_dict FutureWarning stacklevel= optim_only optims raise RuntimeError Optimizers passed optim_only set True options = options StateDictOptions fqn_param_mapping dict Union str torch Tensor Union set str torch Tensor = shared_params_mapping dict Union str torch Tensor Union set str torch Tensor = name param _iterate_valid_model_state model isinstance param _EXTRA_STATE continue fqns = _get_fqns model name fqn = fqn_param_mapping get param fqn None cast set str fqn_param_mapping param update fqns shared_params_mapping param = fqn_param_mapping param We need do copy _get_fqns lru_cached fqn_param_mapping param = fqns copy fqn fqns isinstance param _EXTRA_STATE fqn_param_mapping fqn = param param_ fqns_ list shared_params_mapping items fqn fqns_ shared_params_mapping fqn = cast torch Tensor param_ submodule_prefixes set str = set submodules submodules = set submodules name module model named_modules module submodules continue fqns = _get_fqns model name len fqns = raise AssertionError Submodule FQN should only have instance submodule_prefixes update f fqn fqn fqns options broadcast_from_rank options full_state_dict raise ValueError full_state_dict must True when broadcast_from_rank True fsdp_modules = FSDP fsdp_modules model state_dict_config StateDictConfig optim_state_dict_config OptimStateDictConfig fsdp_context Callable fsdp_modules FSDP API only work least one FSDP instance exists options full_state_dict state_dict_config = FullStateDictConfig offload_to_cpu=options cpu_offload rank _only=options cpu_offload optim_state_dict_config = FullOptimStateDictConfig offload_to_cpu=options cpu_offload rank _only= options cpu_offload options broadcast_from_rank state_dict_type = StateDictType FULL_STATE_DICT state_dict_config = ShardedStateDictConfig offload_to_cpu=options cpu_offload optim_state_dict_config = ShardedOptimStateDictConfig offload_to_cpu=options cpu_offload state_dict_type = StateDictType SHARDED_STATE_DICT contextlib contextmanager fsdp_state_dict_type_without_warning module state_dict_type state_dict_config optim_state_dict_config warnings catch_warnings warnings filterwarnings ignore message= FSDP state_dict_type category=FutureWarning FSDP state_dict_type module=module state_dict_type=state_dict_type state_dict_config=state_dict_config optim_state_dict_config=optim_state_dict_config yield fsdp_context = functools partial fsdp_state_dict_type_without_warning module=model state_dict_type=state_dict_type state_dict_config=state_dict_config optim_state_dict_config=optim_state_dict_config fsdp_context = contextlib nullcontext _StateDictInfo asdict options fqn_param_mapping=fqn_param_mapping shared_params_mapping=shared_params_mapping submodule_prefixes=submodule_prefixes fsdp_context=fsdp_context fsdp_modules=cast list nn Module fsdp_modules handle_model=not optim_only handle_optim= len optims _verify_state_dict model_state_dict dict str ValueType optim_state_dict OptimizerStateType info _StateDictInfo - None module info fsdp_modules fsdp_state = _get_module_fsdp_state_if_fully_sharded_module module fsdp_state None raise AssertionError Expected fsdp_state fsdp module Verify model_state_dict optim_state_dict valid This API should give users explicit error message debug report info handle_model model_state_dict info submodule_prefixes info ignore_frozen_params info cpu_offload info full_state_dict info strict info broadcast_from_rank raise RuntimeError The option indicates model state_dict required save load model state_dict empty f rank = dist get_rank = info handle_optim optim_state_dict info cpu_offload info full_state_dict info broadcast_from_rank raise RuntimeError The option indicates model state_dict required save f load optim state_dict empty optim_state_dict key model_state_dict keys _FLAT_PARAM key raise RuntimeError f key contains _FLAT_PARAM This can happen model root module _state_dict_fn obj Union nn Module torch optim Optimizer api str - Callable call = getattr obj api call _patched_state_dict call = functools partial getattr obj __class__ api self=obj call _maybe_full_or_cpu_state_dict state_dict dict str Any info _StateDictInfo - dict str Any info full_state_dict ranks_only = info cpu_offload torch distributed is_initialized _gather_state_dict state_dict cpu_offload=info cpu_offload ranks_only=ranks_only info cpu_offload _offload_state_dict_to_cpu state_dict state_dict torch no_grad _get_model_state_dict model nn Module info _StateDictInfo - dict str ValueType info handle_model info fsdp_context state_dict = _state_dict_fn model state_dict key list state_dict keys fqns = _get_fqns model key len fqns = raise AssertionError f Expected FQN key key got len fqns fqns fqn = next iter fqns fqn = key As we only support FSDP DDP TP only cases wrapper-based DDP compiler Verify assumption correct verify key fqn - bool len fqn = len key False fqn_split = fqn split key_split = key split fqn_idx = key_idx key_name enumerate key_split key_name == fqn_split fqn_idx fqn_idx += fqn_idx == len fqn_split key_idx == len key_split - key_name module _orig_mod continue False True verify key fqn raise RuntimeError f An unexpected key key exists FQN fqn state_dict fqn = state_dict pop key info submodule_prefixes new_state_dict dict str ValueType = TODO make faster fqn state_dict keys prefix info submodule_prefixes fqn startswith prefix continue info keep_submodule_prefixes new_state_dict fqn = state_dict fqn new_fqn = fqn len prefix new_state_dict new_fqn = state_dict fqn state_dict = new_state_dict info ignore_frozen_params key param model named_parameters param requires_grad continue fqns = _get_fqns model key fqn fqns state_dict pop fqn _maybe_full_or_cpu_state_dict state_dict info torch no_grad _load_model_state_dict model nn Module state_dict dict str ValueType info _StateDictInfo - _IncompatibleKeys info handle_model state_dict info broadcast_from_rank _IncompatibleKeys local_state_dict = key value _iterate_valid_model_state model info dsd_fqn_modifiers fqns = _get_fqns model key info dsd_fqn_modifiers fqns_with_prefix = _get_fqns model key info dsd_fqn_modifiers skip_ddp_prefix=False skip_compiler_prefix=False fqn fqn_with_prefix zip fqns fqns_with_prefix info broadcast_from_rank dist get_rank == fqn = fqn_with_prefix load_value = state_dict pop fqn None load_value None info strict raise RuntimeError f Missing key fqn state_dict fqn_with_prefix = load_value local_state_dict fqn_with_prefix = value assign = False info broadcast_from_rank info full_state_dict devices = set value local_state_dict values torch is_tensor value value dim devices add value device In lora state_dict there could multiple devices meta device inside Take other device broadcast distribtue set assign True torch device meta devices devices remove torch device meta assign = True len devices == devices add dist distributed_c d _get_pg_default_device len devices raise ValueError Multiple devices found info broadcast_from_rank _broadcast_state_dict state_dict local_state_dict device=devices pop strict=info strict cpu_offload=info cpu_offload info full_state_dict _distribute_state_dict state_dict local_state_dict device=devices pop state_dict update local_state_dict info fsdp_context cast _IncompatibleKeys _state_dict_fn model load_state_dict state_dict=state_dict strict=info strict assign=assign _init_optim_state optim torch optim Optimizer - None Initialize optim states calling step zero grads optim state The optimizer state initialized There some stateless optimizers like SGD These optimizer will above condition So gradients exist we should also If gradients do exist following initialization should disturb SGD because gradients lr both zero param_group optim param_groups param param_group _PARAMS param grad None param_group optim param_groups param param_group _PARAMS param requires_grad param grad = torch zeros_like param Some optimizers will update parameters regardless grads due lr so make lr zero when calling ` step ` lrs = param_group optim param_groups lr param_group lrs append param_group lr param_group lr = torch tensor isinstance param_group lr torch Tensor optim step closure=None Whether recover lr should matter too much we will restore checkpointing later param_group optim param_groups lr param_group param_group lr = lrs pop optim zero_grad set_to_none=True _flatten_optim_state_dict state_dict OptimizerStateType - dict str ValueType This API flattens optimizer state_dict support optimizer resharding MPMD e g pipeline parallelism Without API original optimizer state_dict looks like state layer weight step exp_avg SomeTensor exp_avg_sq SomeTensor layer weight step exp_avg SomeTensor exp_avg_sq SomeTensor param_groups lr betas params layer weight layer weight With API optimizer state_dict looks like state layer weight step state layer weight step state layer weight exp_avg SomeTensor state layer weight exp_avg SomeTensor state layer weight exp_avg_sq SomeTensor state layer weight exp_avg_sq SomeTensor param_groups layer weight lr param_groups layer weight lr param_groups layer weight betas param_groups layer weight betas The state section supports arbitrary levels nesting optimizers like Shampoo _flatten_state_nested_dict nested_dict dict str Any prefix str - dict str ValueType Recursively flatten nested dictionary dot-separated keys Args nested_dict The dictionary flatten prefix The prefix prepend all keys Returns Flattened dictionary dot-separated keys flattened dict str ValueType = key value nested_dict items Convert all keys strings flattening str_key = str key full_key = f prefix str_key prefix str_key isinstance value dict Recursively flatten nested dictionaries flattened update _flatten_state_nested_dict value full_key Base case store value flattened key _raise_if_type_not_supported value flattened full_key = value flattened _raise_if_type_not_supported v isinstance v torch Tensor int float dict raise NotImplementedError Flattening optimizer state_dict only supports tensor int float dict states now f Type type v ret dict str ValueType = Handle state section recursive flattening fqn state cast DictValueType state_dict _STATE items state_prefix = f _STATE fqn ret update _flatten_state_nested_dict cast dict str Any state state_prefix Handle param_groups section two-level flattening param_group cast ListDictValueType state_dict _PG fqns = param_group pop _PARAMS fqn cast list str fqns k v param_group items ret f _PG fqn k = v ret _unflatten_optim_state_dict optim torch optim Optimizer state_dict dict str ValueType info _StateDictInfo - OptimizerStateType This API unflattens state_dict generated _flatten_optim_state_dict Supports arbitrary levels nesting state section through recursive reconstruction See docstring _flatten_optim_state_dict more detail _reconstruct_nested_dict flattened_key str flattened_dict dict str ValueType - dict str ValueType Reconstructs potentially nested value flattened keys For non-nested values returns value directly For nested values reconstructs nested structure string keys Create prefix search nested keys e g flattened_key state layer weight prefix becomes state layer weight prefix = f flattened_key Initialize empty dictionary build our nested structure nested_dict dict str Any = Iterate through all keys flattened dictionary key value flattened_dict items Check key nested under our target key e g state layer weight exp_avg starts state layer weight key startswith prefix Skip keys don t belong nested structure continue Remove prefix get just nested part e g state layer weight exp_avg - exp_avg remaining_key = key len prefix Split remaining key into parts build nested structure e g step - step momentum_buffer - momentum_buffer parts = remaining_key split Start root our new nested dictionary current = nested_dict Navigate through create nested dictionary structure For each part except last one which will hold value part parts - Create nested dictionary doesn t exist yet part current current part = Move deeper into nested structure assert isinstance current part dict current = current part Set value final level using last part key e g current exp_avg = tensor current parts - = value Return reconstructed nested dictionary empty dict no keys matched all nested_dict state DictValueType = pg_state ListDictValueType = return_osd OptimizerStateType = _STATE state _PG pg_state param_group optim param_groups pg_state append _PARAMS param param_group _PARAMS fqn info fqn_param_mapping param If parameter shared only one FQN will used So we need verify which fqn actually used state_dict fqn info shared_params_mapping in_params = False k param_group keys k == _PARAMS continue flatten_key = f _PG fqn k flatten_key state_dict in_params = True break in_params = True in_params continue params = pg_state - _PARAMS isinstance params list raise AssertionError f Expected list got type params params append fqn Only add state param requires grad param requires_grad continue Reconstruct state parameter state fqn = state_name optim state param keys flattened_state_key = f _STATE fqn state_name flattened_state_key state_dict Try reconstruct value reconstructed_value = _reconstruct_nested_dict flattened_state_key state_dict cast DictValueType state fqn state_name = reconstructed_value Existing keys mean no nesting directly use value cast DictValueType state fqn state_name = state_dict flattened_state_key first_param_fqn = cast list str pg_state - _PARAMS k param_group keys k == _PARAMS continue value = state_dict f _PG first_param_fqn k k pg_state - pg_state - k = value pg_state - k = value raise RuntimeError All parameters same parameter group should have f same saved param_group value But first_param_fqn k f value while other s pg_state - k return_osd torch no_grad _get_optim_state_dict model nn Module optimizers tuple torch optim Optimizer info _StateDictInfo - OptimizerStateType info handle_optim optim_state_dict OptimizerStateType = _STATE _PG optim optimizers _init_optim_state optim osd = _state_dict_fn optim state_dict info fsdp_modules info fsdp_context osd = FSDP optim_state_dict model optim osd We need specially handle FlatParameter FSDP FlatParameter FSDP converts FQNs There no easy ways do conversion systematically We can only use string replacement without correctness check osd continue k list osd _STATE keys _orig_mod k osd _STATE k replace _orig_mod = osd _STATE pop k g osd _PG params = k replace _orig_mod k g _PARAMS g _PARAMS = params params = list chain from_iterable g _PARAMS g optim param_groups param_pid_mapping = dict zip params range len params fqn_pid_mapping = key param model named_parameters fqns = _get_fqns model key len fqns = raise AssertionError f Expected FQN key key got len fqns fqn = next iter fqns param param_pid_mapping continue pid = param_pid_mapping param fqn_pid_mapping fqn = pid fqn_pid_mapping pid = fqn Only convert top-level parameter IDs FQNs preserve nested key types key list osd _STATE keys fqn = fqn_pid_mapping key Move entire state dict value which may contain nested integer keys without modifying its internal structure osd _STATE fqn = osd _STATE pop key group osd _PG group _PARAMS = fqn_pid_mapping pid pid group _PARAMS osd continue cast DictValueType optim_state_dict _STATE update osd _STATE cast ListDictValueType optim_state_dict _PG extend osd _PG info flatten_optimizer_state_dict optim_state_dict = cast OptimizerStateType _flatten_optim_state_dict optim_state_dict _maybe_full_or_cpu_state_dict optim_state_dict info _split_optim_state_dict model nn Module optim torch optim Optimizer optim_state_dict OptimizerStateType info _StateDictInfo - OptimizerStateType Extract corresponding optim state_dict ` ` optim_state_dict ` ` ` ` optim ` ` result optim state_dict Args model nn Module root model optim torch optim Optimizer optimizer optim_state_dict Dict str ValueType superset optim state_dict contains optim state_dict ` ` optim ` ` info _StateDictInfo state dict information Returns The optim state_dict ` ` optim ` ` state DictValueType = pg_state ListDictValueType = return_osd OptimizerStateType = _STATE state _PG pg_state pg_mapping dict int int = all isinstance k int k cast DictValueType optim_state_dict _STATE keys optim_state_dict param_group optim param_groups pg_state append _PARAMS param param_group _PARAMS fqn info fqn_param_mapping param fqn info shared_params_mapping in_params = False loaded_param_group cast ListDictValueType optim_state_dict _PG fqn cast list str loaded_param_group _PARAMS in_params = True break in_params = True in_params continue params = pg_state - _PARAMS isinstance params list raise AssertionError f Expected list got type params params append fqn param requires_grad state fqn = cast DictValueType optim_state_dict _STATE fqn loaded_param_group cast ListDictValueType optim_state_dict _PG fqn cast list str loaded_param_group _PARAMS pg_mapping id loaded_param_group = len return_osd _PG - len param_group _PARAMS == Param_group empty params ret = loaded_param_group cast ListDictValueType optim_state_dict _PG len cast list str loaded_param_group _PARAMS == ret append loaded_param_group len ret = raise ValueError There param groups have zero parameters In such case DSD only support exactly one param group zero parameters But loaded state_dict has zero more than one param groups have zero parameters len optim_state_dict _PG = len optim param_groups raise ValueError When there parameter group has zero parameters multiple optimizers supported pg_mapping id loaded_param_group = len return_osd _PG - param_group cast ListDictValueType optim_state_dict _PG pg_idx = pg_mapping get id param_group - pg_idx == - continue key value param_group items key == _PARAMS continue TODO check value same exists pg_state pg_idx key = value return_osd torch no_grad _load_optim_state_dict model nn Module optimizers tuple torch optim Optimizer state_dict OptimizerStateType info _StateDictInfo - None info handle_optim optim optimizers _init_optim_state optim state_dict _STATE state_dict optim_state_dict = _split_optim_state_dict model optim state_dict info optim_state_dict = _unflatten_optim_state_dict optim cast dict str ValueType state_dict info optim_state_dict = info fsdp_modules We need specially handle FlatParameter FSDP FlatParameter FSDP converts FQNs original_fqn _ model named_parameters fqns = _get_fqns model original_fqn fqns_with_compiler = _get_fqns model original_fqn skip_compiler_prefix=False fqns == fqns_with_compiler continue len fqns = raise AssertionError f Expected FQN original_fqn got len fqns fqn = fqns pop fqn_with_compiler = fqns_with_compiler pop g optim_state_dict _PG val = cast dict str Any g params = key replace fqn fqn_with_compiler key val _PARAMS val _PARAMS = params osd_state = cast DictValueType optim_state_dict _STATE k list osd_state keys fqn k osd_state k replace fqn fqn_with_compiler = osd_state pop k info fsdp_context optim_state_dict = FSDP optim_state_dict_to_load model optim optim_state_dict info full_state_dict info full_state_dict = False local_state_dict = _get_optim_state_dict model optim info info full_state_dict = True device = None _device t t dim nonlocal device device None device = t device device = t device raise ValueError Device mismatch t _ = tree_map_only torch Tensor _device local_state_dict device None raise AssertionError Expected device set flatten_osd osd_mapping = _flatten_state_dict optim_state_dict flatten_local_osd local_osd_mapping = _flatten_state_dict local_state_dict info broadcast_from_rank _broadcast_state_dict flatten_osd flatten_local_osd device=device _distribute_state_dict flatten_osd flatten_local_osd device=device The modifications listed seek address problem where optim might possess dissimilar parameters comparison optim_state_dict This achieved incorporating differential parameters within local which may result optim having additional parameters ultimately optim_key flatten_osd keys optim_key flatten_local_osd optim_key osd_mapping raise AssertionError f Expected key optim_key osd_mapping flatten_local_osd optim_key = flatten_osd optim_key local_osd_mapping optim_key = osd_mapping optim_key optim_state_dict = _unflatten_state_dict flatten_local_osd local_osd_mapping pg optim_state_dict _PG _PARAMS pg cast dict str ValueType pg _PARAMS = Note we do have convert FQN back param id here order optim param_groups idx _PARAMS same one optim_state_dict _PG idx _PARAMS _state_dict_fn optim load_state_dict state_dict=optim_state_dict get_model_state_dict model nn Module submodules Optional set nn Module = None options Optional StateDictOptions = None - dict str ValueType Return model state_dict ` ` model ` ` See ` ` get_state_dict ` ` detail usage Args model nn Module nn Module model submodules deprecated Optional set nn Module only model parameters belong submodules options StateDictOptions options control how model state_dict optimizer state_dict should returned See ` StateDictOptions ` details Returns The state_dict ` ` model ` ` rtype typing Dict str ValueType _gc_context info = _verify_options model optim_only=False submodules=submodules options=options model_state_dict = _get_model_state_dict model info _verify_state_dict model_state_dict info model_state_dict get_optimizer_state_dict model nn Module optimizers Union torch optim Optimizer Iterable torch optim Optimizer submodules Optional set nn Module = None options Optional StateDictOptions = None - OptimizerStateType Return combined state_dict optimizers See ` ` get_state_dict ` ` detail usage Args model nn Module nn Module model optimizers Union None Optimizer Iterable Optimizer The optimizers used optimize ` ` model ` ` submodules deprecated Optional set nn Module only model parameters belong submodules options StateDictOptions options control how model state_dict optimizer state_dict should returned See ` StateDictOptions ` details Returns The state_dict ` ` optimizers ` ` rtype OptimizerStateType _gc_context optimizers = optimizers isinstance optimizers torch optim Optimizer tuple optimizers info = _verify_options model optimizers optim_only=True submodules=submodules options=options optim_state_dict = _get_optim_state_dict model optimizers info _verify_state_dict optim_state_dict info optim_state_dict get_state_dict model nn Module optimizers Union torch optim Optimizer Iterable torch optim Optimizer submodules Optional set nn Module = None options Optional StateDictOptions = None - tuple dict str ValueType OptimizerStateType Return model state_dict optimizers state_dict ` ` get_state_dict ` ` can process any module parallelized PyTorch FSDP fully_shard DDP replicate tensor_parallel parallelize_module any combination these parallelisms The main functions ` ` get_state_dict ` ` returning model optimizer state_dict can resharded different number trainers different parallelisms hiding parallelism-specific state_dict APIs Users don t have call these APIs sanity checking result state_dict The keys result state dictionary canonical FQNs Fully Qualified Names A canonical FQN refers FQN based parameter s position nn Module hierarchy More specifically canonical FQN parameter FQN returned ` ` module named_parameters ` ` ` ` module named_buffers ` ` when module distributed any parallelisms Since optimizer internally uses parameter IDs represent parameter there will conversion parameter IDs canonical FQNs when calling API ` ` get_state_dict ` ` can also process module parallelized In such case ` ` get_state_dict ` ` only performs one function -- converting optimizer parameter IDs canonical FQNs Example xdoctest +SKIP torch torch distributed fsdp FullyShardedDataParallel FSDP torch nn parallel DistributedDataParallel DDP torch distributed checkpoint state_dict get_state_dict fsdp_model = FSDP copy deepcopy model fsdp_optim = torch optim Adam model parameters lr= e- ddp_model = DDP copy deepcopy model ddp_optim = torch optim Adam model parameters lr= e- ddp_state_dict ddp_optim_state_dict = get_state_dict ddp_model ddp_optim fsdp_state_dict fsdp_optim_state_dict = get_state_dict fsdp_model fsdp_optim we simply call ddp_model state_dict fsdp_model state_dict asserts will fail assert ddp_state_dict == fsdp_state_dict assert ddp_optim_state == fsdp_optim_state_dict Args model nn Module nn Module model optimizers Union None Optimizer Iterable Optimizer The optimizers used optimize ` ` model ` ` submodules deprecated Optional set nn Module only model parameters belong submodules options StateDictOptions options control how model state_dict optimizer state_dict should returned See ` StateDictOptions ` details Returns ` ` Tuple ` ` contain model state_dict optimizer state_dict rtype typing Tuple typing Dict str ValueType OptimizerStateType _gc_context optimizers = optimizers isinstance optimizers torch optim Optimizer tuple optimizers info = _verify_options model optimizers optim_only=False submodules=submodules options=options model_state_dict = _get_model_state_dict model info optim_state_dict = _get_optim_state_dict model optimizers info _verify_state_dict model_state_dict optim_state_dict info model_state_dict optim_state_dict _unflatten_model_state_dict model nn Module state_dict Union dict nn Module dict str ValueType dict str ValueType - dict str ValueType state_dict isinstance next iter state_dict keys nn Module warnings warn Passing model_state_dict ` ` Dict nn Module Dict str Any ` ` deprecated will removed If you need feature please preprocessing model_state_dict achieve same functionality FutureWarning stacklevel= cast_state_dict = cast dict nn Module dict str ValueType state_dict new_state_dict dict str ValueType = submodule sub_state_dict cast_state_dict items name m model named_modules m = submodule continue fqns = _get_fqns model name len fqns = raise AssertionError FQNs submodule should only have element prefix = f next iter fqns new_state_dict update prefix + subfqn value subfqn value sub_state_dict items new_state_dict cast dict str ValueType state_dict set_model_state_dict model nn Module model_state_dict dict str ValueType options Optional StateDictOptions = None - _IncompatibleKeys Load model state_dict The counterpart ` ` get_model_state_dict ` ` set state_dict model See ` ` set_state_dict ` ` detail usage Args model nn Module nn Module model model_state_dict Dict str ValueType model state_dict load If key ` ` model_state_dict ` ` nn Module key submodule ` ` model ` ` value should state_dict submodule When loading state_dict prefix submodule will append state_dict options StateDictOptions options control how model state_dict optimizer state_dict should loaded See ` StateDictOptions ` details Returns ` ` NamedTuple ` ` ` ` missing_keys ` ` ` ` unexpected_keys ` ` fields missing_keys list str containing missing keys unexpected_keys list str containing unexpected keys type model_state_dict typing Dict str ValueType model_state_dict dict str ValueType = _unflatten_model_state_dict model model_state_dict _gc_context info = _verify_options model optim_only=False options=options _verify_state_dict model_state_dict info _load_model_state_dict model model_state_dict info set_optimizer_state_dict model nn Module optimizers Union torch optim Optimizer Iterable torch optim Optimizer optim_state_dict OptimizerStateType options Optional StateDictOptions = None - None Load optimizers state_dict The counterpart ` ` get_optimizer_state_dict ` ` set state_dict optimizers See ` ` set_state_dict ` ` detail usage WARN ` ` set_optimizer_state_dict ` ` can only called before ` ` backward ` ` after ` ` step ` ` called optimizers Otherwise optimizer states won t initialized correctly Args model nn Module nn Module model optimizers Union Optimizer Iterable Optimizer The optimizers used optimize ` ` model ` ` optim_state_dict OptimizerStateType optimizer state_dict load options StateDictOptions options control how model state_dict optimizer state_dict should loaded See ` StateDictOptions ` details Returns None type optim_state_dict typing OptimizerStateType _gc_context optimizers = optimizers isinstance optimizers torch optim Optimizer tuple optimizers info = _verify_options model optimizers optim_only=True options=options _verify_state_dict optim_state_dict info _load_optim_state_dict model optimizers optim_state_dict info set_state_dict model nn Module optimizers Union torch optim Optimizer Iterable torch optim Optimizer model_state_dict dict str ValueType optim_state_dict OptimizerStateType options Optional StateDictOptions = None - _IncompatibleKeys Load model state_dict optimizers state_dict The counterpart ` ` get_state_dict ` ` set state_dict model optimizers The given ` ` model_state_dict ` ` ` ` optim_state_dict ` ` do have returned ` ` get_state_dict ` ` must meet following requirements all FQNs canonical FQNs defined ` ` get_state_dict ` ` tensor sharded must either ShardedTensor DTensor optimizer state_dict cannot contain parameter IDs keys should canonical FQNs WARN ` ` set_state_dict ` ` can only called before ` ` backward ` ` after ` ` step ` ` called optimizers Otherwise optimizer states won t initialized correctly Args model nn Module nn Module model optimizers Union Optimizer Iterable Optimizer The optimizers used optimize ` ` model ` ` model_state_dict Union Dict nn Module Dict str ValueType Dict str ValueType model state_dict load If key ` ` model_state_dict ` ` nn Module key submodule ` ` model ` ` value should state_dict submodule When loading state_dict prefix submodule will append state_dict optim_state_dict OptimizerStateType optimizer state_dict load options StateDictOptions options control how model state_dict optimizer state_dict should loaded See ` StateDictOptions ` details Returns ` ` NamedTuple ` ` ` ` missing_keys ` ` ` ` unexpected_keys ` ` fields missing_keys list str containing missing keys model state_dict unexpected_keys list str containing unexpected keys model state_dict type model_state_dict typing Dict str ValueType type optim_state_dict typing OptimizerStateType model_state_dict dict str ValueType = _unflatten_model_state_dict model model_state_dict _gc_context optimizers = optimizers isinstance optimizers torch optim Optimizer tuple optimizers info = _verify_options model optimizers optim_only=not model_state_dict options=options _verify_state_dict model_state_dict optim_state_dict info _load_optim_state_dict model optimizers optim_state_dict info _load_model_state_dict model model_state_dict info TODO correct state_dict function signature TODO API yet fully tested Make private no_type_check _patch_model_state_dict model nn Module options Optional StateDictOptions = None - None Patch ` ` state_dict ` ` ` ` load_state_dict ` ` attributes ` ` model ` ` Patch ` ` state_dict ` ` ` ` load_state_dict ` ` attributes ` ` model ` ` partial function call ` ` get_state_dict ` ` ` ` set_state_dict ` ` Example torch distributed fsdp FullyShardedDataParallel FSDP torch distributed checkpoint state_dict patch_model_state_dict model = fsdp model patch_model_state_dict model Args model nn Module nn Module model options StateDictOptions options control how model state_dict optimizer state_dict should loaded See ` StateDictOptions ` details Returns None _state_dict_call = functools partial get_model_state_dict model=model options=options state_dict_call _state_dict_call model state_dict = state_dict_call _load_state_dict_call = functools partial set_model_state_dict model=model options=options load_state_dict_call state_dict dict str Any _load_state_dict_call model_state_dict=state_dict model load_state_dict = load_state_dict_call _patched_state_dict add state_dict_call _patched_state_dict add load_state_dict_call TODO correct load_state_dict function signature TODO API yet fully tested Make private no_type_check _patch_optimizer_state_dict model nn Module optimizers tuple torch optim Optimizer options Optional StateDictOptions = None - None Patch ` ` state_dict ` ` ` ` load_state_dict ` ` attributes ` ` optimizers ` ` Patch ` ` state_dict ` ` ` ` load_state_dict ` ` attributes ` ` optimizers ` ` partial function call ` ` get_state_dict ` ` ` ` set_state_dict ` ` Note there multiple optimizers all optimizers will patched So users only need call one state_dict get full result Example torch distributed fsdp FullyShardedDataParallel FSDP torch distributed checkpoint state_dict patch_model_state_dict model = fsdp model patch_model_state_dict model Args model nn Module nn Module model options StateDictOptions options control how model state_dict optimizer state_dict should loaded See ` StateDictOptions ` details Returns None _state_dict_call = functools partial get_optimizer_state_dict model=model optimizers=optimizers options=options state_dict_call _state_dict_call _load_state_dict_call = functools partial set_optimizer_state_dict model=model optimizers=optimizers options=options load_state_dict_call state_dict dict str Any _load_state_dict_call optim_state_dict=state_dict _patched_state_dict add state_dict_call _patched_state_dict add load_state_dict_call optimizers = optimizers isinstance optimizers torch optim Optimizer tuple optimizers optim optimizers optim state_dict = state_dict_call optim load_state_dict = load_state_dict_call