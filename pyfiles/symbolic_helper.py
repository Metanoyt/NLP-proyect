mypy allow-untyped-defs __future__ annotations __all__ = _apply_params _arange_cast_helper _arange_helper _argmin_argmax_helper _as_list_type _avgpool_helper _batchnorm_helper _block_list_in_opset _embedding_bag_helper _flatten_helper _generate_wrapped_number _get_const _get_dim_for_cross _get_interpolate_attributes _get_tensor_dim_size _get_tensor_rank _get_tensor_sizes _handle_reduce_dim_none _if_scalar_type_as _index_fill_reshape_helper _interpolate_get_scales_and_mode _interpolate_get_scales_if_available _interpolate_get_scales _interpolate_helper _interpolate_size_to_scales _interpolate_warning _is_bool _is_constant _is_fp _is_list _is_none _is_onnx_constant _is_packed_list _is_scalar_list _is_split_static _is_tensor_list _is_tensor _is_tuple_construct _is_value _linalg_vector_norm_helper _lt_helper _max_helper _maybe_cast_reduce_op_input _maybe_cast_to_type _maybe_get_const _maybe_get_scalar _min_helper _node_get _numel_helper _onnx_opset_unsupported_detailed _onnx_opset_unsupported _onnx_unsupported _op_with_optional_float_cast _optional_input_placeholder_tensor _overload_by_arg_count _parse_arg _reduce_op_symbolic_helper _reduce_with_dtype_helper _reducesum_helper _repeat_interleave_single_value_repeat_helper _repeat_interleave_split_helper _reshape_helper _scalar _scatter_helper _select_helper _size_helper _slice_helper _sort_helper _squeeze_helper _topk_helper _try_get_scalar_type _type_promote_from_values _unbind_helper _unimplemented _unpack_list _unpack_quantized_tensor _unpack_tuple _unsqueeze_helper _var_mean_helper args_have_same_dtype cast_pytorch_to_onnx check_training_mode dequantize_helper is_complex_value parse_args pytorch_name_to_type quantize_helper quantized_args requantize_bias_helper scalar_name_to_pytorch scalar_type_to_onnx scalar_type_to_pytorch_type functools inspect math sys typing warnings typing Any Concatenate _Concatenate Literal NoReturn TypeVar _TypeVar typing_extensions ParamSpec _ParamSpec torch torch _C _onnx _C_onnx torch _C torch onnx _constants errors torch onnx _internal torchscript_exporter _type_utils jit_utils utils torch onnx _internal torchscript_exporter _globals GLOBALS typing TYPE_CHECKING collections abc Callable Sequence torch types Number _T = _TypeVar _T _U = _TypeVar _U _P = _ParamSpec _P --------------------------------------------------------------------------------- Helper functions --------------------------------------------------------------------------------- _ValueDescriptor = Literal v i f fs b s t none _parse_arg value desc _ValueDescriptor arg_name str &#124; None = None node_name str &#124; None = None desc == none value desc == v _is_value value value node = value node node mustBeNone None node kind == onnx Constant node_val = _node_get node value desc == i int node_val desc == f float node_val desc == b bool node_val desc == s str node_val desc == t node_val desc == int v v node_val desc == fs float v v node_val raise errors SymbolicValueError f ONNX symbolic does understand Constant node node f specified descriptor desc value node kind == prim ListConstruct desc == v node inputs element_node = v node element_node kind = onnx Constant raise errors SymbolicValueError f Failed export node element_node f list node node f because constant f Please try make things e g kernel sizes static possible value int _node_get v node value v value node inputs raise errors SymbolicValueError f ONNX symbolic does know how unpack ListConstruct node f list integers node value arg_name None node_name None raise errors SymbolicValueError f Expected node type onnx Constant got node kind value raise errors SymbolicValueError Expected node type onnx Constant f argument arg_name node node_name got node kind value _node_get node _C Node key str Gets attributes node which polymorphic over type assert isinstance node _C Node sel = node kindOf key getattr node sel key _is_onnx_constant value _C Value Whether Value ONNX constant value node kind == onnx Constant _maybe_get_const value _C Value &#124; torch Tensor &#124; Number &#124; Sequence &#124; None descriptor _ValueDescriptor NOTE prim Constant stage usually means something compatible ONNX otherwise d converted onnx Constant TODO justinchuby Replace insinstance _is_value once we figure out mypy isinstance value _C Value _is_onnx_constant value _parse_arg value descriptor value _maybe_get_scalar value value_t = _maybe_get_const value t isinstance value_t torch Tensor value_t shape == value_t value _get_const value desc arg_name _is_constant value raise errors SymbolicValueError f ONNX symbolic expected constant value arg_name argument f got value value _parse_arg value desc _unpack_list list_value _C Value - list _C Value list_node = list_value node list_node kind = prim ListConstruct raise errors SymbolicValueError f ONNX symbolic expected node type prim ListConstruct got list_node list_value list list_node inputs _unpack_tuple tuple_value _C Value - tuple _C Value tuple_node = tuple_value node _is_tuple_construct tuple_value raise errors SymbolicValueError f ONNX symbolic expected node type prim TupleConstruct f got tuple_node kind tuple_value tuple tuple_node inputs _unpack_quantized_tensor tuple_value _C Value - tuple _C Value Unpacks quantized tensor into tuple tensor scale zero_point Args tuple_value A tuple tensor scale zero_point optionally axis Returns A tuple tensor scale zero_point optionally axis tuple_node = tuple_value node A quantized tensor represented tuple form tensor scale zero_point axis _is_tuple_construct tuple_value raise errors SymbolicValueError f ONNX symbolic expected output ` tuple_node ` quantized f tensor Is likely due missing support quantized f ` tuple_node kind ` Please create issue _constants PYTORCH_GITHUB_ISSUES_URL tuple_value unpacked = tuple tuple_node inputs assert len unpacked == len unpacked == unpacked Check list_value output prim ListConstruct This usually called before _unpack_list ensure list can unpacked _is_packed_list list_value Any - bool _is_value list_value list_value node kind == prim ListConstruct parse_args arg_descriptors _ValueDescriptor - Callable Callable _Concatenate _U _P _T Callable _Concatenate _U _P _T A decorator which converts args torch _C Value built-in types For example ` ` ` parse_args v i fs foo g b c assert isinstance torch _C Value assert isinstance b int assert isinstance c list assert isinstance c float ` ` ` Args arg_descriptors list str where each element string specifies type convert Valid descriptors v no conversion keep torch _C Value i int list int f float fs list float b bool s str t torch Tensor none variable unused decorator fn Callable _Concatenate _U _P _T - Callable _Concatenate _U _P _T fn _arg_descriptors = arg_descriptors type ignore attr-defined functools wraps fn wrapper g _U args _P args kwargs _P kwargs - _T some args may optional so length may smaller FILE_BUG_MSG = If you believe due custom symbolic implementation within your code external library please file issue https github com pytorch pytorch issues new template=bug-report yml report bug assert len arg_descriptors = len args f A mismatch between number arguments len args f their descriptors len arg_descriptors found symbolic function fn __name__ f FILE_BUG_MSG try sig = inspect signature fn arg_names = list sig parameters keys fn_name = fn __name__ except Exception FIXME justinchuby Avoid catching Exception Catch more specific exception instead arg_names = None len args type ignore list-item fn_name = None args = _parse_arg arg arg_desc arg_name fn_name type ignore method-assign arg arg_desc arg_name zip args arg_descriptors arg_names only support _outputs kwargs assert len kwargs = f Symbolic function fn __name__ s kwargs can contain single f key value entry f FILE_BUG_MSG len kwargs == assert _outputs kwargs f Symbolic function fn __name__ s kwargs can only contain f _outputs key kwargs f FILE_BUG_MSG fn g args kwargs wrapper decorator quantized_args arg_q_descriptors bool scale float &#124; None = None zero_point int &#124; None = None quantize_output bool = True - Callable Callable _P _T Callable _P _T A decorator which extends support quantized version base operator Quantization detected examining arguments annotated ` arg_q_descriptors ` If quantization detected base operator symbolic function will wrapped argument de-quantization output quantization Otherwise only base symbolic function will invoked For example ` ` ` quantized_args True False foo g x y x + y ` ` ` equivalent ` ` ` q_foo g x y is_quantized_tensor x x = dequantize x out = foo g x y quantize out foo g x y ` ` ` Args arg_q_descriptors A sequence bool where each element represents argument QTensor quantized version operator It defaults False unspecified variable length arguments scale Quantized output scale If None derive first quantized input scale zero_point Quantized output zero point If None derive first quantized input zero point quantize_output If True quantize output base operator Default True decorator fn functools wraps fn wrapper g args kwargs nonlocal scale nonlocal zero_point scale None _scale = g op Constant value_t=torch tensor scale _scale = None zero_point None _zero_point = g op Constant value_t=torch tensor zero_point _zero_point = None Support variable length arguments marking unspecified ones non-quantized arg_q_descriptors_extended = arg_q_descriptors + False len args - len arg_q_descriptors descriptor_args = tuple zip arg_q_descriptors_extended args _is_arg_quantized descriptor arg descriptor _is_value arg _is_tuple_construct arg Run regular symbolic function none argument QTensor is_quantized list bool = descriptor arg descriptor_args ListConstruct _is_packed_list arg is_quantized extend _is_arg_quantized descriptor arg_input arg_input arg node inputs is_quantized append _is_arg_quantized descriptor arg any is_quantized fn g args kwargs Dequantize arguments quantized non_quantized_args = descriptor arg descriptor_args _is_arg_quantized descriptor arg Quantized arg tuple value scale zero_point dequantized_arg arg_scale arg_zero_point _ = dequantize_helper g arg non_quantized_args append dequantized_arg Set scale zero_point first quantized input already set _scale None _scale = arg_scale _zero_point None _zero_point = arg_zero_point ListConstruct _is_packed_list arg arg_input arg node inputs _is_arg_quantized descriptor arg_input Quantized arg tuple value scale zero_point dequantized_arg arg_scale arg_zero_point _ = dequantize_helper g arg_input Set scale zero_point first quantized input already set _scale None _scale = arg_scale _zero_point None _zero_point = arg_zero_point arg_input replaceAllUsesWith dequantized_arg non_quantized_args append arg Non-quantized arg non_quantized_args append arg TODO justinchuby Only single output supported now We may want support multiple outputs future output = fn g non_quantized_args kwargs assert _scale None Bug Scale must set quantized operator assert _zero_point None Bug Zero point must set quantized operator quantize_output quantize_helper g output _scale _zero_point output wrapper decorator _scalar x Any - Number &#124; None Convert scalar tensor into Python value isinstance x torch Tensor x shape == x item None _if_scalar_type_as tensor Convert into same type tensor necessary We only support implicit casting scalars so we never actually need insert ONNX cast operator here just fix up scalar isinstance _C Value scalar_type = _type_utils JitScalarType from_value tensor _type_utils JitScalarType UNDEFINED scalar_type = _type_utils JitScalarType UNDEFINED ty = scalar_type scalar_name lower getattr ty _is_none x Any - bool x None x node mustBeNone isinstance x _C Value False _is_value x Any - bool isinstance x _C Value _is_constant value Any - bool _is_value value value node kind onnx Constant prim Constant _is_tensor x _C Value - bool x type isSubtypeOf _C TensorType get Note _C JitType exposed Python cannot checked runtime _as_list_type jit_type _C JitType - _C ListType &#124; None isinstance jit_type _C ListType jit_type None _is_list x _C Value - bool _as_list_type x type None _is_tensor_list x _C Value - bool x_type = _as_list_type x type x_type None False isinstance x_type getElementType _C TensorType _is_scalar_list x _C Value - bool Checks x scalar list example List float List int Besides checking type ListType we also check data type valid ONNX data type x_type = _as_list_type x type x_type None False scalar_type = _type_utils JitScalarType from_value x scalar_type onnx_compatible _is_tuple_construct x _C Value - bool x node kind == prim TupleConstruct is_complex_value x _C Value - bool assert _is_value x _type_utils JitScalarType from_value x _type_utils JitScalarType UNDEFINED _type_utils JitScalarType COMPLEX _type_utils JitScalarType COMPLEX _type_utils JitScalarType COMPLEX _get_tensor_rank x _C Value - int &#124; None _is_tensor x x type None None x_type = x type x_type = typing cast _C TensorType x_type x_type dim _get_tensor_sizes x _C Value allow_nonstatic bool = True _is_tensor x x type None None x_type = x type x_type = typing cast _C TensorType x_type allow_nonstatic Each individual symbol returned None e g b - None None x_type varyingSizes returns None exists any symbol sizes e g b - None x_type sizes _get_tensor_dim_size x _C Value dim int - int &#124; None sizes = _get_tensor_sizes x sizes dim sizes None _get_dim_for_cross x _C Value dim int &#124; None dim == - tensor_rank = _get_tensor_rank x assert tensor_rank None dim + tensor_rank If dim given defaults first dimension found size dim None sizes = _get_tensor_sizes x assert sizes None index size enumerate sizes size None size == index dim _unimplemented op str msg str value _C Value &#124; None = None - None For BC reasons behavior Caffe does raise exception unimplemented operators GLOBALS operator_export_type == _C_onnx OperatorExportTypes ONNX _onnx_unsupported f op msg value _onnx_unsupported op_name str value _C Value &#124; None = None - NoReturn message = f Unsupported ONNX export operator op_name f Please feel free request support submit pull request f PyTorch GitHub _constants PYTORCH_GITHUB_ISSUES_URL isinstance value _C Value raise errors SymbolicValueError message value raise errors OnnxExporterError message _onnx_opset_unsupported op_name str current_opset int supported_opset int value _C Value &#124; None = None - NoReturn message = f Unsupported ONNX export op_name opset current_opset f Please try opset version supported_opset isinstance value _C Value raise errors SymbolicValueError message value raise errors OnnxExporterError message _onnx_opset_unsupported_detailed op_name str current_opset int supported_opset int reason str value _C Value &#124; None = None - NoReturn message = f Unsupported ONNX export op_name f opset current_opset reason Please try opset version supported_opset isinstance value _C Value raise errors SymbolicValueError message value raise errors OnnxExporterError message _block_list_in_opset name str symbolic_fn args kwargs raise errors OnnxExporterError f ONNX export failed name which implemented opset f GLOBALS export_onnx_opset_version Try exporting other opset versions symbolic_fn _try_get_scalar_type args - _type_utils JitScalarType &#124; None arg args scalar_type = _type_utils JitScalarType from_value arg _type_utils JitScalarType UNDEFINED scalar_type = _type_utils JitScalarType UNDEFINED scalar_type None _type_promote_from_values args - _type_utils JitScalarType undef = _type_utils JitScalarType UNDEFINED jit_types = _try_get_scalar_type arg arg args len jit_types == undef len jit_types == jit_types type ignore return-value new_dtype = jit_types dtype type ignore union-attr t jit_types new_dtype = torch promote_types new_dtype t dtype type ignore union-attr _type_utils JitScalarType from_dtype new_dtype _maybe_cast_to_type g jit_utils GraphContext value jit_type _type_utils JitScalarType _type_utils JitScalarType from_value value _type_utils JitScalarType UNDEFINED = jit_type g op Cast value to_i=jit_type onnx_type value _select_helper g jit_utils GraphContext dim index apply_reshape=True index_const = _maybe_get_scalar index index_dim = _get_tensor_rank index _is_value index_const Index constant scalar Make size constant tensor index = g op Constant value_t=torch LongTensor index_const index_dim None apply_reshape index_dim == Index scalar Reshape size tensor index = _reshape_helper g index g op Constant value_t=torch LongTensor index_scalar_type = _type_utils JitScalarType from_value index _type_utils JitScalarType UNDEFINED index_scalar_type _type_utils JitScalarType INT _type_utils JitScalarType INT index = g op Cast index to_i=_C_onnx TensorProtoDataType INT g op Gather index axis_i=dim _slice_helper g jit_utils GraphContext input axes starts ends steps=None g opset = torch onnx _internal torchscript_exporter symbolic_opset _slice _slice _slice g input axes starts ends torch onnx _internal torchscript_exporter symbolic_opset _slice _slice _slice g input axes starts ends steps _is_fp value - bool _type_utils JitScalarType from_value value _type_utils JitScalarType UNDEFINED _type_utils JitScalarType FLOAT _type_utils JitScalarType DOUBLE _type_utils JitScalarType HALF _type_utils JitScalarType BFLOAT _is_bool value - bool _type_utils JitScalarType from_value value _type_utils JitScalarType UNDEFINED == _type_utils JitScalarType BOOL _generate_wrapped_number g jit_utils GraphContext scalar Creates wrapped number based https github com pytorch pytorch issues A Tensor considered wrapped number auto-wrapped C++ Python number type Integer types wrapped -dim int tensors floating-point types wrapped -dim double tensors The input function constant value If data type floating point type converted -dim double tensor converted -dim tensor its original type assert isinstance scalar torch Tensor isinstance scalar float g op Constant value_t=torch tensor scalar dtype=torch double g op Constant value_t=torch tensor scalar _sort_helper g jit_utils GraphContext input dim descending=True out=None out None _unimplemented Sort Out parameter supported shape_ = g op Shape input dim_size_ = g op Gather shape_ g op Constant value_t=torch tensor dim dtype=torch int g opset = descending _unimplemented Sort Ascending supported g op TopK input dim_size_ axis_i=dim outputs= g op TopK input dim_size_ axis_i=dim largest_i=descending outputs= _topk_helper g jit_utils GraphContext input k dim largest=True sorted=False out=None out None _unimplemented TopK Out parameter supported _is_value k k = g op Constant value_t=torch tensor k dtype=torch int k = _reshape_helper g k g op Constant value_t=torch tensor _try_get_scalar_type k = _type_utils JitScalarType INT k = g op Cast k to_i=_C_onnx TensorProtoDataType INT g opset = largest _unimplemented TopK Ascending supported g op TopK input k axis_i=dim outputs= g op TopK input k axis_i=dim largest_i=largest sorted_i=sorted outputs= _lt_helper g jit_utils GraphContext input other g opset = torch onnx _internal torchscript_exporter symbolic_opset lt _lt _lt g input other torch onnx _internal torchscript_exporter symbolic_opset lt _lt _lt g input other _interpolate_warning interpolate_mode onnx_op = onnx Resize GLOBALS export_onnx_opset_version = onnx Upsample warnings warn You trying export model + onnx_op + ONNX opset version + str GLOBALS export_onnx_opset_version + This operator might cause results match expected results PyTorch \n ONNX s Upsample Resize operator did match Pytorch s Interpolation until opset Attributes determine how transform input added onnx Resize opset support Pytorch s behavior like coordinate_transformation_mode nearest_mode \n We recommend using opset above models using operator stacklevel= _unsqueeze_helper g jit_utils GraphContext input axes_i len axes_i == unnecessary unsqueeze axes length== input _is_constant axes_i g opset = axes = g op Constant value_t=torch tensor axes_i dtype=torch long g op Unsqueeze input axes g op Unsqueeze input axes_i=axes_i Tensor type g opset raise errors SymbolicValueError Opset version must = Unsqueeze dynamic axes input g op Unsqueeze input axes_i _squeeze_helper g jit_utils GraphContext input axes_i _is_constant axes_i g opset = axes = g op Constant value_t=torch tensor axes_i dtype=torch long g op Squeeze input axes g op Squeeze input axes_i=axes_i Tensor type g opset raise errors SymbolicValueError Opset version must = Squeeze dynamic axes input axes_t = axes_i axes_rank = _get_tensor_rank axes_t assert axes_rank None axes_rank raise errors SymbolicValueError For Squeeze axses input axes rank must one ONNX spec input axes_rank == The axes scalar Unsqueeze rank tensor axes_t = _unsqueeze_helper g axes_t g op Squeeze input axes_t g op Squeeze input axes_t _reducesum_helper g jit_utils GraphContext input axes_i=None keepdims_i= noop_with_empty_axes_i= keepdims_i = _maybe_get_const keepdims_i i g opset = axes_i _is_value axes_i axes_i = g op Constant value_t=torch tensor axes_i dtype=torch long g op ReduceSum input axes_i keepdims_i=keepdims_i noop_with_empty_axes_i=noop_with_empty_axes_i g op ReduceSum input keepdims_i=keepdims_i noop_with_empty_axes_i=noop_with_empty_axes_i g op ReduceSum input axes_i=axes_i keepdims_i=keepdims_i _interpolate_size_to_scales g jit_utils GraphContext input output_size dim output_size = _maybe_get_const output_size _is_value output_size offset = offsets = g op Constant value_t=torch ones offset dtype=torch float dividend = g op Cast output_size to_i=_C_onnx TensorProtoDataType FLOAT divisor = _slice_helper g g op Shape input axes= ends= sys maxsize starts= offset divisor = g op Cast divisor to_i=_C_onnx TensorProtoDataType FLOAT scale_dims = g op Div dividend divisor scales = g op Concat offsets scale_dims axis_i= scales_constant = i float output_size - dim - i float input type sizes - dim - i i range dim scales = g op Constant value_t=torch tensor scales_constant dtype=torch float scales _interpolate_get_scales_if_available g jit_utils GraphContext scales available_scales = _maybe_get_const scales fs = - _is_none scales available_scales None offsets = g op Constant value_t=torch ones dtype=torch float scales_list = g op Constant value_t=torch tensor _maybe_get_const scales fs scales = g op Concat offsets scales_list axis_i= scales _get_interpolate_attributes g jit_utils GraphContext mode args mode == nearest align_corners = None scales = args align_corners = args scales = args scales = _interpolate_get_scales_if_available g scales scales align_corners _interpolate_get_scales g jit_utils GraphContext scale_factor dim offsets = g op Constant value_t=torch ones dtype=torch float scale_factor_rank = _get_tensor_rank scale_factor isinstance scale_factor type _C ListType scale_factor_rank None scale_factor_rank g op Concat offsets scale_factor axis_i= scale_factor = _unsqueeze_helper g scale_factor scale_factor = g op Cast scale_factor to_i=_C_onnx TensorProtoDataType FLOAT scales = scale_factor i range dim - scale_factor = g op Concat offsets scales axis_i= scale_factor _interpolate_get_scales_and_mode g jit_utils GraphContext input size scale_factor mode align_corners mode = _maybe_get_const mode s linear mode mode = linear cubic mode mode = cubic _interpolate_warning mode align_corners = _maybe_get_const align_corners b isinstance align_corners bool align_corners _unimplemented interpolate align_corners == True input type dim _unimplemented interpolate missing input shape dim = input type dim _is_none scale_factor scale_factor = _interpolate_get_scales g scale_factor dim _is_none size _is_packed_list size is_scalar = _maybe_get_const size t dim == is_scalar size = _unsqueeze_helper g size size = size i range dim - size = g op Concat size axis_i= scale_factor = _interpolate_size_to_scales g input size dim _unimplemented interpolate Both size scales None __interpolate scale_factor mode _argmin_argmax_helper g jit_utils GraphContext input torch _C Value dim torch _C Value keepdim bool op_name str op_wrapper input axis_i keepdims_i g opset = g op op_name input axis_i=axis_i keepdims_i=keepdims_i select_last_index_i=False g op op_name input axis_i=axis_i keepdims_i=keepdims_i _is_none dim flattened = _reshape_helper g input g op Constant value_t=torch tensor - output = op_wrapper flattened axis_i= keepdims_i=False keepdim input_shape = g op Shape input input_shape_shape = g op Shape input_shape new_shape = g op ConstantOfShape input_shape_shape value_t=torch tensor dtype=torch int output = g op Reshape output new_shape output dim = _parse_arg dim i op_wrapper input axis_i=dim keepdims_i=keepdim _interpolate_helper name dim interpolate_mode quantized_args True False False symbolic_fn g input output_size args scales align_corners = _get_interpolate_attributes g interpolate_mode args align_corners = _maybe_get_scalar align_corners coordinate_transformation_mode = asymmetric interpolate_mode == nearest align_corners align_corners half_pixel scales None input_size = g op Shape input input_size_beg = _slice_helper g input_size axes= ends= starts= output_size = g op Cast output_size to_i=_C_onnx TensorProtoDataType INT output_size = g op Concat input_size_beg output_size axis_i= g opset = empty_roi = _optional_input_placeholder_tensor g empty_scales = _optional_input_placeholder_tensor g empty_roi = g op Constant value_t=torch tensor dtype=torch float empty_scales = g op Constant value_t=torch tensor dtype=torch float g op Resize input empty_roi empty_scales output_size coordinate_transformation_mode_s=coordinate_transformation_mode cubic_coeff_a_f=- only valid when mode= cubic mode_s=interpolate_mode nearest linear cubic nearest_mode_s= floor only valid when mode= nearest g opset = empty_roi = _optional_input_placeholder_tensor g empty_roi = g op Constant value_t=torch tensor dtype=torch float g op Resize input empty_roi scales coordinate_transformation_mode_s=coordinate_transformation_mode cubic_coeff_a_f=- only valid when mode= cubic mode_s=interpolate_mode nearest linear cubic nearest_mode_s= floor only valid when mode= nearest symbolic_fn __interpolate_helper g jit_utils GraphContext input size scale_factor mode align_corners recompute_scale_factor mode = _maybe_get_const mode s linear mode mode = linear cubic mode mode = cubic align_corners = _maybe_get_const align_corners b align_corners = False isinstance align_corners bool align_corners coordinate_transformation_mode = asymmetric mode == nearest align_corners align_corners half_pixel _is_none size input_size = g op Shape input input_size = _slice_helper g input_size axes= ends= starts= some cases size packed list size scalar We need also verify _maybe_get_const size t dim == information always available Try get dim assume scalar try is_scalar = _is_packed_list size _maybe_get_const size t dim == except AttributeError is_scalar = _is_packed_list size is_scalar warnings warn Cannot verify output_size scalar while exporting interpolate Assuming scalar stacklevel= is_scalar rank = _get_tensor_rank input rank None _unimplemented interpolate scalar output_size missing input shape try giving array output_size values size = _unsqueeze_helper g size size = size i range rank - size = g op Concat size axis_i= size = g op Cast size to_i=_C_onnx TensorProtoDataType INT size = g op Concat input_size size axis_i= g opset = empty_roi = _optional_input_placeholder_tensor g empty_scales = _optional_input_placeholder_tensor g empty_roi = g op Constant value_t=torch tensor dtype=torch float empty_scales = g op Constant value_t=torch tensor dtype=torch float g op Resize input empty_roi empty_scales size coordinate_transformation_mode_s=coordinate_transformation_mode cubic_coeff_a_f=- only valid when mode= cubic mode_s=mode nearest linear cubic nearest_mode_s= floor _is_none scales rank = _get_tensor_rank input rank None _unimplemented interpolate scales missing input shape g opset = empty_roi = _optional_input_placeholder_tensor g empty_roi = g op Constant value_t=torch tensor dtype=torch float scales = _interpolate_get_scales g scale_factor rank g op Resize input empty_roi scales coordinate_transformation_mode_s=coordinate_transformation_mode cubic_coeff_a_f=- only valid when mode= cubic mode_s=mode nearest linear cubic nearest_mode_s= floor only valid when mode= nearest _unbind_helper g jit_utils GraphContext dim _outputs g opset torch onnx _internal torchscript_exporter symbolic_opset unbind g opset = torch onnx _internal torchscript_exporter symbolic_opset unbind type ignore no-redef torch onnx _internal torchscript_exporter symbolic_opset unbind type ignore no-redef unbind g dim _outputs _scatter_helper g jit_utils GraphContext dim index src g opset = torch onnx _internal torchscript_exporter symbolic_opset scatter mypy scatter imported two lines above torch onnx _internal torchscript_exporter symbolic_opset scatter type ignore no-redef scatter g dim index src _repeat_interleave_split_helper g jit_utils GraphContext reps dim g opset = split_out = g op Split split_i= reps axis_i=dim outputs=reps torch onnx _internal torchscript_exporter symbolic_opset split repeats = g op Constant value_t=torch tensor reps split_out = split g repeats dim _outputs=reps split_out reps split_out _repeat_interleave_single_value_repeat_helper g jit_utils GraphContext repeats dim torch onnx _internal torchscript_exporter symbolic_opset flatten unsqueeze _is_tensor repeats repeats = g op Constant value_t=torch LongTensor repeats const_repeats bool = _is_constant repeats reps = _maybe_get_const repeats t Convert repeats -d -d _get_tensor_rank repeats == repeats = g op Reshape repeats g op Constant value_t=torch tensor Create new dim size then expand repeats long finally collapse unsqueezed = unsqueeze g dim + repeats_per_dim all dims except new unsqueezed dim where has value repeats const_repeats Repeats constant repeats_per_dim can constant onehot = torch ones _get_tensor_rank unsqueezed dtype=torch int type ignore arg-type onehot dim + = reps repeats_per_dim = g op Constant value_t=onehot Repeats variable repeats_per_dim cannot constant onehot = g op OneHot unsqueeze g dim + indices must = -dimensional g op Constant value_t=torch tensor _get_tensor_rank unsqueezed depth g op Concat g op Constant value_t=torch tensor repeats axis_i= off values repeats_per_dim = flatten g onehot tiled = g op Tile unsqueezed repeats_per_dim flatten g tiled dim dim + _arange_cast_helper g jit_utils GraphContext end start=None step=None dtype=None - tuple _type_utils JitScalarType _C Value &#124; None _C Value &#124; None _C Value &#124; None _is_all_integral scalars scalar scalars scalar_type = _type_utils JitScalarType from_value scalar _type_utils JitScalarType UNDEFINED scalar_type = _type_utils JitScalarType INT scalar_type = _type_utils JitScalarType UNDEFINED False True This logic based torch arange docs If dtype provided infer input types dtype If then check any start stop step floating point infer type get_default Otherwise dtype inferred torch int dtype None _is_value dtype _is_none dtype _is_all_integral start end step scalar_type = _type_utils JitScalarType INT scalar_type = _type_utils JitScalarType from_dtype torch get_default_dtype assert isinstance dtype int TODO justinchuby Check dtype indeed int scalar_type = _type_utils JitScalarType dtype start = g op Cast start to_i=scalar_type onnx_type start None end = g op Cast end to_i=scalar_type onnx_type end None step = g op Cast step to_i=scalar_type onnx_type step None scalar_type end start step _arange_helper g jit_utils GraphContext args g opset = torch onnx _internal torchscript_exporter symbolic_opset arange torch onnx _internal torchscript_exporter symbolic_opset arange type ignore no-redef arange g args _size_helper g jit_utils GraphContext dim full_shape = g op Shape torch onnx _internal torchscript_exporter symbolic_opset select select g full_shape g op Constant value_t=torch tensor dim _index_fill_reshape_helper g jit_utils GraphContext dim index reshape index = dim expand index = dim same shape except dim expand value well apply onnx scatter torch onnx _internal torchscript_exporter symbolic_opset expand g opset = torch onnx _internal torchscript_exporter symbolic_opset scatter mypy scatter imported two lines above torch onnx _internal torchscript_exporter symbolic_opset scatter type ignore no-redef type dim None _unimplemented index_fill input rank accessible self_dim = type dim dim_value = _parse_arg dim i dim_value dim_value += self_dim unsqueezed_index = _unsqueeze_helper g index i i range self_dim i = dim_value expanded_index_shape = scatter g g op Shape _unsqueeze_helper g dim g op Shape index expanded_index = expand g unsqueezed_index expanded_index_shape None expanded_index_shape expanded_index By default when any value shape input equal zero corresponding dimension value copied input tensor dynamically allowzero= indicates any value shape input set zero zero value honored similar NumPy allowzero= only supported opset version = _reshape_helper g jit_utils GraphContext input shape allowzero= shape = _maybe_get_const shape _is_value shape shape = g op Constant value_t=torch LongTensor shape g opset = allowzero == _onnx_opset_unsupported Reshape allowzero= GLOBALS export_onnx_opset_version input g op Reshape input shape g op Reshape input shape allowzero_i=allowzero _batchnorm_helper g jit_utils GraphContext input weight bias running_mean running_var torch onnx _internal torchscript_exporter symbolic_opset _var_mean batch_size = _get_tensor_dim_size input channel_size = _get_tensor_dim_size input weight None _is_none weight channel_size None raise errors SymbolicValueError Unsupported ONNX export batch_norm unknown channel size input weight_value = torch tensor channel_size dtype=_type_utils JitScalarType from_value input dtype weight = g op Constant value_t=weight_value bias None _is_none bias channel_size None raise errors SymbolicValueError Unsupported ONNX export batch_norm unknown channel size input bias_value = torch tensor channel_size dtype=_type_utils JitScalarType from_value input dtype bias = g op Constant value_t=bias_value If track_running_stats set False batch statistics instead used during evaluation time running_mean None _is_none running_mean running_var None _is_none running_var assert batch_size None channel_size None reshape_in = _reshape_helper g input g op Constant value_t=torch tensor batch_size channel_size - dtype=torch int trans_in = g op Transpose reshape_in perm_i= running_var running_mean = _var_mean g trans_in g op Constant value_t=torch tensor dtype=torch int False False weight bias running_mean running_var _avgpool_helper tuple_fn Callable Any Sequence int padding int &#124; Sequence int kernel_size stride divisor_override name - tuple int divisor_override divisor_override node kind = prim Constant _unimplemented name divisor_override tuple tuple_fn padding check_training_mode op_train_mode int op_name str - None Warns user model s training mode export mode do agree GLOBALS training_mode == _C_onnx TrainingMode PRESERVE op_train_mode op_mode_enum = _C_onnx TrainingMode TRAINING op_mode_enum = _C_onnx TrainingMode EVAL op_mode_enum == GLOBALS training_mode The modes agree Do nothing op_mode_text = f train= bool op_train_mode Setting model mode could result op_mode = GLOBALS training_mode model FuncModule In case we warn user state export depending op_mode This support use-cases fixing certain layer weights training warnings warn f ONNX export mode set GLOBALS training_mode operator op_name f set op_mode_text Exporting op_mode_text stacklevel= _flatten_helper g jit_utils GraphContext input start_dim end_dim dim input_size = g op Shape input slice = _slice_helper g input_size axes= starts= ends= start_dim slices = slice g op Constant value_t=torch tensor - dtype=torch long end_dim dim - slice = _slice_helper g input_size axes= starts= end_dim + ends= dim slices = slice g op Constant value_t=torch tensor - dtype=torch long slice final_shape = g op Concat slices axis_i= torch onnx _internal torchscript_exporter symbolic_opset _reshape_from_tensor _reshape_from_tensor g input final_shape _is_split_static split_size_or_sizes _outputs _outputs None False _is_value split_size_or_sizes split_size_or_sizes node kind = onnx Constant False True _optional_input_placeholder_tensor g n = g op prim Constant n setType _C OptionalType ofTensor n _handle_reduce_dim_none g jit_utils GraphContext op_name rank = _get_tensor_rank rank None any _get_tensor_dim_size i == i range rank If input tensor empty according ONNX ReduceSum definition set keepdims= so resulted tensor has same rank input g op op_name keepdims_i= g op op_name keepdims_i= dequantize_helper g jit_utils GraphContext qtensor _C Value qdtype _C_onnx TensorProtoDataType &#124; None = None - tuple _C Value _C Value _C Value _C Value &#124; None Appends graph ` g ` ONNX nodes dequantizes ` qtensor ` into ` tensor ` Args g Graph ONNX IR graph under construction qtensor torch _C Value either tuple quantized_tensor scale zero_point per tensor quantization quantized_tensor scale zero_point axis per channel quantization representing quantized tensor qdtype torch onnx TensorProtoDataType default None None represents data type quantized tensor It must either torch onnx TensorProtoDataType UINT torch onnx TensorProtoDataType INT unpacked_qtensors = _unpack_quantized_tensor qtensor tensor scale zero_point = unpacked_qtensors axis = unpacked_qtensors len unpacked_qtensors = None axis_i = _get_const axis i axis input_qdtype = _type_utils JitScalarType from_value tensor qdtype None input_qdtype None qdtype = input_qdtype onnx_type qdtype = _C_onnx TensorProtoDataType UINT value = g op Cast tensor to_i=qdtype scale = g op Cast scale to_i=_C_onnx TensorProtoDataType FLOAT zero_point = g op Cast zero_point to_i=qdtype axis_i None GLOBALS export_onnx_opset_version _onnx_opset_unsupported_detailed DequantizeLinear GLOBALS export_onnx_opset_version Attribute axis supported qtensor g op DequantizeLinear value scale zero_point axis_i=axis_i scale zero_point axis quantize_helper g jit_utils GraphContext tensor _C Value scale _C Value zero_point _C Value axis _C Value &#124; None = None - _C Value Appends graph ` g ` ONNX nodes quantizes ` tensor ` based ` scale ` ` zero_point ` ` axis ` Args g Graph ONNX IR graph under construction tensor torch _C Value representing tensor quantized scale torch _C Value quantized scale zero_point torch _C Value quantized zero point axis Optional torch _C Value default None None represents per tensor quantization Otherwise represents per channel quantization along given axis Returns A TupleConstruct storing information quantized tensor axis None _is_none axis GLOBALS export_onnx_opset_version _onnx_opset_unsupported_detailed QuantizeLinear GLOBALS export_onnx_opset_version Attribute axis supported tensor assert scale None _type_utils JitScalarType from_value scale _type_utils JitScalarType UNDEFINED = _type_utils JitScalarType FLOAT scale = g op Cast scale to_i=_C_onnx TensorProtoDataType FLOAT assert zero_point None _type_utils JitScalarType from_value zero_point _type_utils JitScalarType UNDEFINED _type_utils JitScalarType UINT _type_utils JitScalarType INT zero_point = g op Cast zero_point to_i=_C_onnx TensorProtoDataType UINT output = g op QuantizeLinear tensor scale zero_point axis_i=_get_const axis i axis args = output scale zero_point axis None _is_none axis args append axis g op prim TupleConstruct args requantize_bias_helper g jit_utils GraphContext bias input_scale weight_scale axis=None In PyTorch bias float quantized int implicitly inside quantized ATen op kernel In ONNX we need make quantization explicit because operators expect all their inputs quantized Since int supported output type ONNX operator ` QuantizeLinear ` quantization exported using regular operators bias_scale = g op Mul weight_scale input_scale bias_scale_shape = g op Shape bias_scale bias_zero_point = g op ConstantOfShape bias_scale_shape value_t=torch tensor dtype=torch int q_bias = g op Cast g op Div bias bias_scale to_i=_C_onnx TensorProtoDataType INT axis_args = axis None _is_none axis axis_args append axis g op prim TupleConstruct q_bias bias_scale bias_zero_point axis_args args_have_same_dtype args assert args base_dtype = _type_utils JitScalarType from_value args has_same_dtype = all _type_utils JitScalarType from_value elem == base_dtype elem args has_same_dtype _op_with_optional_float_cast g jit_utils GraphContext op_name args kwargs Some PyTorch operators e g Clip Min ReLU Pad super set ONNX terms data types This function maximizes exportability PyTorch-ONNX allowing ONNX-unsupported PyTorch operator data type For example ` Cast int Clip float Cast float INPUT ` can used mimic ` Clip int INPUT ` opset version Args g torch _C Graph graph write ONNX representation into op_name str operator name ONNX args tuple operands operator kwargs dict attributes operator along opset_before optional None default indicating smallest opset version trigger such casting behavior target_float_t optional torch onnx JitScalarType FLOAT default indicating data type internal operator Returns Optional torch _C Value Tuple torch _C Value output s operator opset_before = kwargs pop opset_before None target_float_t = kwargs pop target_float_t _type_utils JitScalarType FLOAT inputs = list args dtype_ = _type_utils JitScalarType from_value inputs require_cast = _is_fp inputs opset_before None GLOBALS export_onnx_opset_version opset_before require_cast input inputs pyrefly ignore missing-attribute input isCompleteTensor input_scalar_type = _type_utils JitScalarType from_value input input_scalar_type = dtype_ raise errors SymbolicValueError f Inputs op_name must have same dtype f Got dtype_ scalar_name input_scalar_type scalar_name pyrefly ignore bad-argument-type input i input enumerate inputs pyrefly ignore missing-attribute input isCompleteTensor _is_fp input inputs i = g op Cast pyrefly ignore bad-argument-type input to_i=target_float_t onnx_type pyrefly ignore bad-argument-type = g op op_name inputs kwargs require_cast = g op Cast to_i=dtype_ onnx_type _maybe_cast_reduce_op_input g jit_utils GraphContext scalar_type = _type_utils JitScalarType from_value _type_utils JitScalarType UNDEFINED scalar_type = _type_utils JitScalarType UNDEFINED This check only covers traced modules where dtype present pytorch reduce-ops cast all other integral types int _is_fp scalar_type = _type_utils JitScalarType INT = g op Cast to_i=_C_onnx TensorProtoDataType INT _apply_params args kwargs Returns decorator calls decorated higher-order function given parameters _apply fn fn args kwargs _apply _reduce_op_symbolic_helper onnx_op_name allow_multi_dim_support=True symbolic g dim=None keepdim=None = _maybe_cast_reduce_op_input g dim None dim == Dim can which will cause dim == True So we don t want do dim all-reduce path _handle_reduce_dim_none g onnx_op_name dim-reduce path keepdim = _get_const keepdim i keepdim g opset desc = allow_multi_dim_support i dim = _get_const dim desc dim dim_list = dim allow_multi_dim_support dim g op onnx_op_name axes_i=dim_list keepdims_i=keepdim _is_value dim axes = dim allow_multi_dim_support axes = g op Constant value_t=torch tensor dim dtype=torch long axes = g op Constant value_t=torch tensor dim dtype=torch long g op onnx_op_name axes keepdims_i=keepdim symbolic _overload_by_arg_count fn functools wraps fn wrapper g args overloads = fn g args overload overloads arg_descriptors = overload _arg_descriptors len arg_descriptors == len args overload g args _unimplemented f aten fn __name__ f len args arguments wrapper _reduce_with_dtype_helper onnx_op str name str allow_multi_dim_support bool = True symbolic = _reduce_op_symbolic_helper onnx_op allow_multi_dim_support=allow_multi_dim_support _overload_by_arg_count reduce g args kwargs quantized_args True parse_args v none reduce_nodim g dtype dtype_onnx = None dtype node kind == onnx Constant dtype = _get_const dtype i dtype dtype_onnx = _type_utils JitScalarType dtype onnx_type = g op Cast to_i=dtype_onnx dtype node kind = prim Constant _unimplemented name dtype dtype result = symbolic g dtype_onnx None result_dtype_onnx = _type_utils JitScalarType from_value result onnx_type result_dtype_onnx = dtype_onnx result = g op Cast result to_i=dtype_onnx result dim_desc = allow_multi_dim_support i quantized_args True parse_args v dim_desc i none type ignore arg-type reduce_dim g dim keepdim dtype dtype_onnx = None dtype node kind == onnx Constant dtype = _get_const dtype i dtype dtype_onnx = _type_utils JitScalarType dtype onnx_type = g op Cast to_i=dtype_onnx dtype node kind = prim Constant _unimplemented name dtype dtype result = symbolic g dim keepdim dtype_onnx None result_dtype_onnx = _type_utils JitScalarType from_value result onnx_type result_dtype_onnx = dtype_onnx result = g op Cast result to_i=dtype_onnx result reduce_nodim reduce_dim reduce _max_helper g jit_utils GraphContext dim_or_y=None keepdim=None torch max input dim_or_y None keepdim None g op ReduceMax keepdims_i= torch max input other keepdim None _op_with_optional_float_cast g Max dim_or_y opset_before= torch max input dim keepdim keepdim = _get_const keepdim i keepdim dim = _get_const dim_or_y i dim g opset max = g op ReduceMax axes_i= dim keepdims_i=keepdim axes = g op Constant value_t=torch tensor dim dtype=torch long max = g op ReduceMax axes keepdims_i=keepdim indices = g op ArgMax axis_i=dim keepdims_i=keepdim max indices _min_helper g jit_utils GraphContext dim_or_y=None keepdim=None torch min input dim_or_y None keepdim None g op ReduceMin keepdims_i= torch min input other keepdim None _op_with_optional_float_cast g Min dim_or_y opset_before= torch min input dim keepdim keepdim = _get_const keepdim i keepdim dim = _get_const dim_or_y i dim g opset min = g op ReduceMin axes_i= dim keepdims_i=keepdim axes = g op Constant value_t=torch tensor dim dtype=torch long min = g op ReduceMin axes keepdims_i=keepdim indices = g op ArgMin axis_i=dim keepdims_i=keepdim min indices _numel_helper g jit_utils GraphContext shape = g op Shape g op ReduceProd shape keepdims_i= parse_args v i i _var_mean_helper g jit_utils GraphContext input dim correction keepdim g opset dim None mean = g op ReduceMean input keepdims_i= t_mean = mean num_elements = _numel_helper g input mean = g op ReduceMean input axes_i=dim keepdims_i=keepdim t_mean = g op ReduceMean input axes_i=dim keepdims_i= redudced_dims = g op Shape input dim could contain one multiple dimensions redudced_dims = g op Gather redudced_dims g op Constant value_t=torch tensor dim axis_i= num_elements = g op ReduceProd redudced_dims keepdims_i= sub_v = g op Sub input t_mean sqr_sub = g op Mul sub_v sub_v keepdim_mean = dim None keepdim var = g op ReduceMean sqr_sub axes_i=dim keepdims_i=keepdim_mean Correct bias calculating variance dividing over N - correction instead N correction None correction = correction = num_elements = g op Cast num_elements to_i=_C_onnx TensorProtoDataType FLOAT one = g op Constant value_t=torch tensor correction dtype=torch float mul = g op Mul var num_elements var = g op Div mul g op Sub num_elements one var mean axes = None dim None mean = g op ReduceMean input keepdims_i= t_mean = mean num_elements = _numel_helper g input axes = g op Constant value_t=torch tensor dim dtype=torch long mean = g op ReduceMean input axes keepdims_i=keepdim t_mean = g op ReduceMean input axes keepdims_i= redudced_dims = g op Shape input dim could contain one multiple dimensions redudced_dims = g op Gather redudced_dims g op Constant value_t=torch tensor dim axis_i= num_elements = g op ReduceProd redudced_dims keepdims_i= sub_v = g op Sub input t_mean sqr_sub = g op Mul sub_v sub_v keepdim_mean = dim None keepdim axes None var = g op ReduceMean sqr_sub keepdims_i=keepdim_mean var = g op ReduceMean sqr_sub axes keepdims_i=keepdim_mean Correct bias calculating variance dividing over N - correction instead N correction None correction = correction = num_elements = g op Cast num_elements to_i=_C_onnx TensorProtoDataType FLOAT one = g op Constant value_t=torch tensor correction dtype=torch float mul = g op Mul var num_elements var = g op Div mul g op Sub num_elements one var mean _embedding_bag_helper g jit_utils GraphContext embedding_matrix indices offsets scale_grad_by_freq mode sparse per_sample_weights include_last_offset padding_idx scale_grad_by_freq GLOBALS export_training _onnx_unsupported embedding_bag scale_grad_by_freq training mode padding_idx None padding_idx = raise RuntimeError embedding_bag padding_idx loop_condition = g op Constant value_t=torch tensor loop_condition = g op Cast loop_condition to_i=_C_onnx TensorProtoDataType BOOL zero = g op Constant value_t=torch tensor indices_len = _unsqueeze_helper g _size_helper g indices g op Constant value_t=torch tensor include_last_offset offsets = offsets indices_len offsets = g op Concat offsets axis_i= Offsets holds starting index position each bag So we create list indices slices determined offsets gather those indices indices_row Then we use subset indices gather embeddings The embeddings output loop scan output so we can avoid creating sequence inserting elements offsets_starts = _slice_helper g offsets axes= starts= ends= sys maxsize steps= offsets_ends = _slice_helper g offsets axes= starts= ends= sys maxsize steps= loop_len = _size_helper g offsets_ends g op Constant value_t=torch tensor loop loop_context _ = jit_utils add_op_with_blocks g Loop loop_len loop_condition n_blocks= loop_block = loop_context block FIXME justinchuby We need handle what happens when we call b op node block_input_iter = utils _add_input_to_block loop_block utils _add_input_to_block loop_block indices_start = loop_context op Gather offsets_starts block_input_iter axis_i= indices_end = loop_context op Gather offsets_ends block_input_iter axis_i= indices_start = _unsqueeze_helper loop_context indices_start indices_end = _unsqueeze_helper loop_context indices_end indices_row = loop_context op Slice indices indices_start indices_end zero embeddings = loop_context op Gather embedding_matrix indices_row axis_i= _is_none per_sample_weights per_sample_weights_row = loop_context op Slice per_sample_weights indices_start indices_end zero per_sample_weights_row = _unsqueeze_helper loop_context per_sample_weights_row embeddings = loop_context op Mul embeddings per_sample_weights_row mode == embeddings = _reducesum_helper loop_context embeddings axes_i= keepdims_i= mode == loop_context opset embeddings = loop_context op ReduceMean embeddings axes_i= keepdims_i= axes = loop_context op Constant value_t=torch tensor dtype=torch long embeddings = loop_context op ReduceMean embeddings axes keepdims_i= loop_context opset embeddings = loop_context op ReduceMax embeddings axes_i= keepdims_i= axes = loop_context op Constant value_t=torch tensor dtype=torch long embeddings = loop_context op ReduceMax embeddings axes keepdims_i= cond_out = loop_context op Cast loop_condition to_i=_C_onnx TensorProtoDataType BOOL utils _add_output_to_block loop_block cond_out utils _add_output_to_block loop_block embeddings aten embedding_bag returns tuple elements output offset bag bag_size max_indices But last three outputs used torch nn EmbeddingBag torch nn functional embedding_bag loop node output None None None _linalg_vector_norm_helper g jit_utils GraphContext torch _C Value ord float dim Sequence int &#124; None keepdim bool dtype torch _C Value axes = None Conditions based https pytorch org docs stable generated torch linalg vector_norm html _is_none dim = _reshape_helper g - keepdim = False g opset = axes = g op Constant value_t=torch tensor dim dtype=torch long ord == math inf g opset result = g op ReduceMax g op Abs axes_i=dim keepdims_i=keepdim axes None result = g op ReduceMax g op Abs keepdims_i=keepdim result = g op ReduceMax g op Abs axes keepdims_i=keepdim ord == -math inf g opset result = g op ReduceMin g op Abs axes_i=dim keepdims_i=keepdim axes None result = g op ReduceMin g op Abs keepdims_i=keepdim result = g op ReduceMin g op Abs axes keepdims_i=keepdim ord == g opset _onnx_opset_unsupported_detailed linalg_vector_norm ord= supported dim None = _reshape_helper g g op Constant value_t=torch tensor - dtype=torch int keepdim = False cond_op = g op Not g op Equal g op Constant value_t=torch LongTensor cond_op = g op Cast cond_op to_i=_type_utils JitScalarType from_value onnx_type _reducesum_helper g cond_op axes_i=dim keepdims_i=keepdim ord == g opset result = _reduce_op_symbolic_helper ReduceL g dim=dim keepdim=keepdim axes None result = _reduce_op_symbolic_helper ReduceL g keepdim=keepdim result = _reduce_op_symbolic_helper ReduceL g axes keepdim=keepdim ord == g opset result = _reduce_op_symbolic_helper ReduceL g dim=dim keepdim=keepdim axes None result = _reduce_op_symbolic_helper ReduceL g keepdim=keepdim result = _reduce_op_symbolic_helper ReduceL g axes keepdim=keepdim ord_op = g op Constant value_t=torch tensor ord dtype=torch float result = _reducesum_helper g g op Pow g op Abs ord_op axes_i=dim keepdims_i=keepdim result = g op Pow result g op Div g op Constant value_t=torch tensor dtype=torch float ord_op _is_none dtype dtype = _get_const dtype i dtype result = g op Cast result to_i=_type_utils JitScalarType dtype onnx_type type ignore arg-type result Deprecated Internally use _type_utils ScalarType TODO remove these once we support Type s JIT IR we can once again use unified toType operator cast_pytorch_to_onnx = Byte _C_onnx TensorProtoDataType UINT Char _C_onnx TensorProtoDataType INT Double _C_onnx TensorProtoDataType DOUBLE Float _C_onnx TensorProtoDataType FLOAT Half _C_onnx TensorProtoDataType FLOAT Int _C_onnx TensorProtoDataType INT Long _C_onnx TensorProtoDataType INT Short _C_onnx TensorProtoDataType INT Bool _C_onnx TensorProtoDataType BOOL ComplexFloat _C_onnx TensorProtoDataType COMPLEX ComplexDouble _C_onnx TensorProtoDataType COMPLEX BFloat _C_onnx TensorProtoDataType BFLOAT Undefined _C_onnx TensorProtoDataType UNDEFINED Deprecated Internally use _type_utils ScalarType scalar_name_to_pytorch = uint _t Byte int _t Char double Double float Float half Half int Int int _t Long int _t Short bool Bool complex ComplexFloat complex ComplexDouble qint QInt quint QUInt qint QInt bfloat BFloat Deprecated Internally use _type_utils ScalarType This indicates each scalar type s corresponding torch type Related source https github com pytorch pytorch blob defc fee d c d f f e c core ScalarType h scalar_type_to_pytorch_type = torch uint torch int torch short torch int torch int torch half torch float torch double torch complex torch complex torch complex torch bool torch qint torch quint torch qint torch bfloat Deprecated Internally use _type_utils ScalarType source truth https github com pytorch pytorch blob master torch csrc utils tensor_dtypes cpp pytorch_name_to_type = Byte torch uint Char torch int Double torch double Float torch float Half torch half Int torch int Long torch int Short torch short Bool torch bool ComplexFloat torch complex ComplexDouble torch complex QInt torch qint QUInt torch quint QInt torch qint BFloat torch bfloat Deprecated Internally use _type_utils ScalarType scalar_type_to_onnx = cast_pytorch_to_onnx Byte cast_pytorch_to_onnx Char cast_pytorch_to_onnx Short cast_pytorch_to_onnx Int cast_pytorch_to_onnx Long cast_pytorch_to_onnx Half cast_pytorch_to_onnx Float cast_pytorch_to_onnx Double cast_pytorch_to_onnx Undefined cast_pytorch_to_onnx ComplexFloat cast_pytorch_to_onnx ComplexDouble cast_pytorch_to_onnx Bool cast_pytorch_to_onnx Char cast_pytorch_to_onnx Byte cast_pytorch_to_onnx Int cast_pytorch_to_onnx BFloat Global set store list quantized operators network This currently only used conversion quantized ops PT - C via ONNX _quantized_ops set int = set