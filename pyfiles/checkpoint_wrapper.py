mypy allow-untyped-defs warnings abc ABC abstractmethod collections abc Callable Iterator enum auto Enum functools partial typing Any Optional torch torch nn nn torch autograd graph save_on_cpu torch distributed utils _pack_kwargs _replace_by_prefix _unpack_kwargs torch utils checkpoint checkpoint torch_utils_checkpoint _CHECKPOINT_WRAPPED_MODULE = _checkpoint_wrapped_module _CHECKPOINT_PREFIX = _CHECKPOINT_WRAPPED_MODULE + CheckpointImpl Enum REENTRANT = auto NO_REENTRANT = auto ActivationWrapper torch nn Module ABC Base Activation Checkpoint Activation Offload Not meant instantiated directly __init__ mod super __init__ _checkpoint_wrapped_module = mod state_dict post hook remove prefix allow loading into non-checkpoint wrapped module _register_state_dict_hook _post_state_dict_hook load_state_dict pre-hook allow loading back into checkpoint-wrapped module register_load_state_dict_pre_hook _pre_load_state_dict_hook abstractmethod forward args kwargs raise ValueError Subclasses should implement forward __getattr__ name str - Any Forward missing attributes wrapped module try super __getattr__ name defer nn Module s logic except AttributeError getattr _checkpoint_wrapped_module name __getitem__ key int - Any Forward indexing calls case module nn Sequential _checkpoint_wrapped_module __getitem__ key type ignore operator named_parameters args kwargs - Iterator tuple str torch nn Parameter Override meth ` named_parameters ` intercept parameter names remove all occurrences ` ` _CHECKPOINT_PREFIX ` ` param_name param super named_parameters args kwargs yield param_name replace _CHECKPOINT_PREFIX param staticmethod _post_state_dict_hook module nn Module state_dict dict str Any prefix str args Any - dict str Any _post_state_dict_hook called after state_dict FSDP module executed For ` ` checkpoint_wrapper ` ` will strip checkpoint-wrapped module prefix so module can loaded into non-checkpointed modules It would still able loaded into checkpoint-wrapped modules adds prefix back before loading state_dict _replace_by_prefix state_dict f prefix _CHECKPOINT_PREFIX prefix state_dict staticmethod _pre_load_state_dict_hook module nn Module state_dict dict str Any prefix str args Any - None ` ` _pre_state_dict_hook ` called before ` ` _load_from_state_dict ` ` called For ` ` checkpoint_wrapper ` ` will add back module prefix so non-checkpointed modules can loaded into checkpoint_wrapper modules properly _replace_by_prefix state_dict prefix prefix + f _CHECKPOINT_PREFIX OffloadWrapper ActivationWrapper __init__ mod super __init__ mod forward args kwargs save_on_cpu pin_memory=True _checkpoint_wrapped_module args kwargs CheckpointWrapper ActivationWrapper An ` ` nn Module ` ` wraps another ` ` nn Module ` ` checkpointing Note module meant used directly instead used through ` ` checkpoint_wrapper ` ` function __init__ mod torch nn Module checkpoint_impl CheckpointImpl = CheckpointImpl NO_REENTRANT checkpoint_fn=None checkpoint_fn_kwargs super __init__ mod checkpoint_impl = checkpoint_impl checkpoint_fn None use torch utils checkpoint checkpoint_fn = partial torch_utils_checkpoint use_reentrant= checkpoint_impl == CheckpointImpl REENTRANT checkpoint_fn_kwargs Construct user-specified checkpoint function checkpoint_fn = partial checkpoint_fn checkpoint_fn_kwargs forward args kwargs Support keyword arguments reentrant checkpoint Note only works user has specified checkpoint_impl using their own custom checkpoint_fn checkpoint_impl == CheckpointImpl REENTRANT kwargs = Pack args kwargs flat_args kwarg_keys = _pack_kwargs args kwargs Function only takes packed args can unpack them into original args kwargs checkpointed function runs function my_function inputs unpack back into args kwargs unpacked_args unpacked_kwargs = _unpack_kwargs inputs kwarg_keys run original module _checkpoint_wrapped_module unpacked_args unpacked_kwargs Pass function only takes packed args into reentrant checkpoint API checkpoint_fn type ignore misc my_function flat_args checkpoint_fn type ignore misc _checkpoint_wrapped_module args kwargs offload_wrapper module torch nn Module - torch nn Module Wrap module activation offloading CPU Offloads intermediate activations CPU modules wrapped function Wrappers activation offload can composed ones do recomputation-based checkpoint trade off increased compute versus increased CPU memory usage additional H D transfers Usage offloaded_module = offload_wrapper module outputs = checkpointed_module inputs Args module nn Module The module wrapped Returns nn Module Wrapped module OffloadWrapper module checkpoint_wrapper module torch nn Module checkpoint_impl CheckpointImpl = CheckpointImpl NO_REENTRANT checkpoint_fn=None checkpoint_fn_kwargs - torch nn Module Wrap module activation checkpointing If module wrapped function all subsequent calls module will automatically perform checkpointing without user having explicitly call ` ` checkpoint ` ` function Usage checkpointed_module = checkpoint_wrapper module outputs = checkpointed_module inputs Args module nn Module The module wrapped checkpoint_impl Optional CheckpointImpl The checkpointing implementation use Note will only passed into ` ` torch utils checkpoint checkpoint ` ` implementation ignored custom ` ` checkpoint_fn ` ` specified Note implementations using reentrant checkpoint ` ` torch utils checkpoint ` ` keyword arguments will only supported ` ` checkpoint_impl ` ` passed ` ` CheckpointImpl REENTRANT ` checkpoint_fn Optional Callable Functional checkpoint implementation use If specified will used over default ` ` torch utils checkpoint checkpoint ` ` implementation ` checkpoint_impl ` argument will ignored checkpoint_fn_kwargs Dict str Any Keyword arguments pass into ` checkpoint_fn ` Returns nn Module Wrapped module checkpoint_impl == CheckpointImpl REENTRANT warnings warn f Please specify CheckpointImpl NO_REENTRANT f CheckpointImpl REENTRANT will soon removed default eventually deprecated FutureWarning stacklevel= CheckpointWrapper module checkpoint_impl checkpoint_fn checkpoint_fn_kwargs apply_activation_checkpointing model checkpoint_wrapper_fn=checkpoint_wrapper check_fn=lambda _ True auto_wrap_policy Optional Callable nn Module bool int bool = None Apply func ` checkpoint_wrapper ` modules within ` model ` based user-defined configuration For each module within ` model ` ` check_fn ` used decide whether ` module ` should wrapped func ` checkpoint_wrapper ` Note This function modifies ` model ` place replaces appropriate layers their checkpoint-wrapped modules Note This function will wrap overall root module If needed please directly use func ` checkpoint_wrapper ` func ` offload_wrapper ` Usage model = nn Sequential nn Linear nn Linear nn Linear check_fn = lambda l isinstance l nn Linear checkpoint activations apply_activation_checkpointing model checkpoint_wrapper_fn=checkpoint_wrapper check_fn=check_fn Or offload activations CPU apply_activation_checkpointing model checkpoint_wrapper_fn=offload_wrapper check_fn=check_fn Args model nn Module The model whose submodules should wrapped activation checkpointing checkpoint_wrapper_fn Optional Callable nn Module A ` ` Callable ` ` which will wrap modules check_fn Optional Callable nn Module nn Module A lambda function which will passed each child submodule ` ` model ` ` returns ` ` True ` ` ` ` False ` ` depending whether submodule should wrapped auto_wrap_policy Optional Callable nn Module bool int bool A policy wrap model s submodules AC Note specified takes precedence over ` ` check_fn ` ` Returns None ` model ` modified inplace TODO Importing inside function avoid circular issue between FSDP checkpoint_wrapper This can resolved once wrap APIs decoupled FSDP code torch distributed fsdp _wrap_utils _construct_wrap_fn _post_order_apply torch distributed fsdp wrap _Policy _recursive_wrap lambda_auto_wrap_policy policy = auto_wrap_policy auto_wrap_policy None partial lambda_auto_wrap_policy lambda_fn=check_fn callable policy isinstance policy _Policy raise ValueError f Expected policy callable pre-defined wrap policy target_module_to_kwargs = policy _run_policy model ignored_modules=set root_kwargs= wrap_fn = _construct_wrap_fn model target_module_to_kwargs checkpoint_wrapper_fn _post_order_apply model wrap_fn _recursive_wrap module=model auto_wrap_policy=policy type ignore arg-type wrapper_cls=checkpoint_wrapper_fn ignored_modules=set ignored_params=set only_wrap_children=True