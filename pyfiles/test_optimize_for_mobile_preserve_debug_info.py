Owner s oncall mobile torch torch _C torch nn functional F torch testing _internal common_utils raise_on_run_directly skipIfNoXNNPACK torch testing _internal jit_utils JitTestCase TestOptimizeForMobilePreserveDebugInfo JitTestCase check_replacement model replacements jit_pass model Model which optimization performed replacements Dict mapping nodes kinds optimized model kinds nodes they replaced original model jit_pass Function perform optimization original_kinds = set replacements values original_source_ranges = node kind node sourceRange node model graph nodes node kind original_kinds jit_pass model _c node model graph nodes node kind replacements assertEqual node sourceRange original_source_ranges replacements node kind skipIfNoXNNPACK test_replace_conv d_with_conv d TestConv d torch nn Module __init__ weight bias super __init__ weight = weight bias = bias forward x F conv d x weight bias check_replacement model=torch jit script TestConv d weight=torch rand bias=torch rand replacements= prim ListUnpack aten conv d prim ListConstruct aten conv d aten unsqueeze aten conv d aten conv d aten conv d aten squeeze aten conv d jit_pass=torch _C _jit_pass_transform_conv d_to_conv d skipIfNoXNNPACK test_insert_pre_packed_linear_before_inline_and_conv_ d_op TestPrepackedLinearBeforeInlineAndConv dOp torch nn Module __init__ linear_weight linear_bias conv d_weight conv d_bias conv_transpose d_weight conv_transpose d_bias super TestPrepackedLinearBeforeInlineAndConv dOp __init__ linear_weight = linear_weight float linear_bias = linear_bias float conv d_weight = conv d_weight float conv d_bias = conv d_bias float conv_transpose d_weight = conv_transpose d_weight float conv_transpose d_bias = conv_transpose d_bias float forward x linear_res = F linear x float linear_weight linear_bias conv d_res = F conv d input=linear_res unsqueeze dim= float weight=self conv d_weight bias=self conv d_bias F conv_transpose d input=conv d_res weight=self conv_transpose d_weight bias=self conv_transpose d_bias in_channels = iW = out_channels = kH = kW = check_replacement model=torch jit script TestPrepackedLinearBeforeInlineAndConv dOp linear_weight=torch rand iW linear_bias=torch rand iW conv d_weight=torch rand out_channels in_channels kH kW conv d_bias=torch rand out_channels conv_transpose d_weight=torch rand out_channels in_channels kH kW conv_transpose d_bias=torch rand out_channels replacements= prepacked linear_clamp_prepack aten linear prepacked linear_clamp_run aten linear prepacked conv d_clamp_prepack aten conv d prepacked conv d_clamp_run aten conv d prepacked conv d_transpose_clamp_prepack aten conv_transpose d prepacked conv d_transpose_clamp_run aten conv_transpose d jit_pass=torch _C _jit_pass_insert_prepacked_ops skipIfNoXNNPACK test_insert_pre_packed_linear_op check_replacement model=torch jit trace torch nn Linear torch rand replacements= prepacked linear_clamp_prepack aten linear prepacked linear_clamp_run aten linear jit_pass=torch _C _jit_pass_insert_prepacked_ops run_test_fuse_activation_with_pack_ops_linear_conv d linear_activation linear_activation_kind conv d_activation conv d_activation_kind TestFuseActivationLinearConv d torch nn Module __init__ linear_weight linear_bias conv d_weight conv d_bias super __init__ linear_weight = linear_weight linear_bias = linear_bias conv d_weight = conv d_weight conv d_bias = conv d_bias forward x x = F linear input=x weight=self linear_weight bias=self linear_bias x = linear_activation x x = F conv d input=x unsqueeze dim= weight=self conv d_weight bias=self conv d_bias conv d_activation x linear_in_features = linear_out_features = conv d_in_channels = conv d_out_channels = conv d_kernel = x_shape = model = torch jit trace TestFuseActivationLinearConv d linear_weight=torch nn Parameter data=torch rand linear_out_features linear_in_features requires_grad=False linear_bias=torch nn Parameter data=torch rand linear_out_features requires_grad=False conv d_weight=torch rand conv d_out_channels conv d_in_channels conv d_kernel conv d_kernel conv d_bias=torch rand conv d_out_channels torch rand x_shape torch _C _jit_pass_insert_prepacked_ops model _c check_replacement model=model replacements= prepacked linear_clamp_prepack prepacked linear_clamp_prepack prepacked linear_clamp_run linear_activation_kind prepacked conv d_clamp_prepack prepacked conv d_clamp_prepack prepacked conv d_clamp_run conv d_activation_kind jit_pass=torch _C _jit_pass_fuse_clamp_w_prepacked_linear_conv skipIfNoXNNPACK test_fuse_activation_with_pack_ops_linear_conv d_ run_test_fuse_activation_with_pack_ops_linear_conv d linear_activation=F hardtanh linear_activation_kind= aten hardtanh conv d_activation=F hardtanh_ conv d_activation_kind= aten hardtanh_ skipIfNoXNNPACK test_fuse_activation_with_pack_ops_linear_conv d_ run_test_fuse_activation_with_pack_ops_linear_conv d linear_activation=F hardtanh_ linear_activation_kind= aten hardtanh_ conv d_activation=F hardtanh conv d_activation_kind= aten hardtanh skipIfNoXNNPACK test_fuse_activation_with_pack_ops_linear_conv d_ run_test_fuse_activation_with_pack_ops_linear_conv d linear_activation=F relu linear_activation_kind= aten relu conv d_activation=F relu_ conv d_activation_kind= aten relu_ skipIfNoXNNPACK test_fuse_activation_with_pack_ops_linear_conv d_ run_test_fuse_activation_with_pack_ops_linear_conv d linear_activation=F relu_ linear_activation_kind= aten relu_ conv d_activation=F relu conv d_activation_kind= aten relu __name__ == __main__ raise_on_run_directly test test_jit py