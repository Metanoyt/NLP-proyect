mypy allow-untyped-decorators mypy allow-untyped-defs mypy disable-error-code=arg-type This file exports ONNX ops opset Opset supported ONNX release release __future__ annotations builtins functools math sys warnings typing TYPE_CHECKING typing_extensions deprecated torch torch _C _onnx _C_onnx torch nn modules utils torch onnx torch _C torch onnx _constants errors torch onnx _internal torchscript_exporter _type_utils jit_utils registration symbolic_helper torch onnx _internal torchscript_exporter _globals GLOBALS TYPE_CHECKING collections abc Callable Sequence torch types Number EDITING THIS FILE READ THIS FIRST see Note Edit Symbolic Files README md __all__ = abs acos add addcmul addmm alias amax amin aminmax arange argmax argmin as_strided as_tensor asin atan atan baddbmm batch_norm bernoulli bitwise_not bitwise_or bmm broadcast_tensors broadcast_to bucketize cat cdist ceil clamp_max clamp_min clamp clone constant_pad_nd contiguous conv_tbc conv_transpose d conv_transpose d conv_transpose d conv d conv d conv d convert_element_type convolution cos cosine_similarity cross cumsum detach dim div dot dropout elu embedding_bag embedding empty_like empty eq erf exp expand_as expand eye fill flatten floor_divide floor floordiv frobenius_norm full_like full gather ge gelu get_pool_ceil_padding glu group_norm gt hann_window hardshrink hardsigmoid hardswish hardtanh index_add index_copy index_fill index_put index_select index instance_norm is_floating_point is_pinned isnan item kl_div layer_norm le leaky_relu lerp lift linalg_cross linalg_matrix_norm linalg_norm linalg_vector_norm linear linspace log_sigmoid log_softmax log log log p log logical_and logical_not logical_or logical_xor logit logsumexp lstm_cell lstm lt masked_fill masked_fill_ matmul max_pool d_with_indices max_pool d_with_indices max_pool d_with_indices max maximum meshgrid min minimum mish mm movedim mse_loss mul multinomial mv narrow native_layer_norm ne neg new_empty new_full new_ones new_zeros nonzero_numpy nonzero norm numel numpy_T one_hot ones_like ones onnx_placeholder pad pairwise_distance permute pixel_shuffle pixel_unshuffle pow prelu prim_constant_chunk prim_constant_split prim_constant prim_data prim_device prim_dtype prim_if prim_layout prim_list_construct prim_list_unpack prim_loop prim_max prim_min prim_shape prim_tolist prim_tuple_construct prim_type prim_unchecked_cast prim_uninitialized rand_like rand randint_like randint randn_like randn reciprocal reflection_pad relu relu remainder repeat_interleave repeat replication_pad reshape_as reshape roll rrelu rsqrt rsub scalar_tensor scatter_add scatter select selu sigmoid sign silu sin size slice softmax softplus softshrink sort split_with_sizes split sqrt square squeeze stack std_mean std sub t take tan tanh tanhshrink tensor threshold topk transpose true_divide type_as unbind unfold unsafe_chunk unsafe_split_with_sizes unsafe_split unsqueeze unsupported_complex_operators noop_complex_operators unused var_mean var view_as view where wrap_logical_op_with_cast_to wrap_logical_op_with_negation zeros_like zeros zero _onnx_symbolic = functools partial registration onnx_symbolic opset= _export name str Exports function current global namespace wrapper func globals name = func __all__ append name func wrapper unused g Represents missing optional inputs n = g op prim Constant n setType _C OptionalType ofTensor n _onnx_symbolic aten _shape_as_tensor _shape_as_tensor g jit_utils GraphContext input g op Shape input _onnx_symbolic aten _reshape_from_tensor _reshape_from_tensor g jit_utils GraphContext input shape isinstance shape list shape = g op Concat shape axis_i= reshape g input shape _onnx_symbolic aten reshape symbolic_helper quantized_args True reshape g jit_utils GraphContext shape symbolic_helper _reshape_helper g shape _onnx_symbolic aten reshape_as symbolic_helper quantized_args True reshape_as g jit_utils GraphContext other shape = g op Shape other reshape g shape _onnx_symbolic aten add add g jit_utils GraphContext other alpha=None This function takes add function returns corresponding ONNX operator This function meant called directly user Args g GraphContext The graph context Tensor The first operand other Tensor The second operand alpha float optional The scaling factor second operand Defaults None Returns ONNX operator symbolic_helper _is_value symbolic_helper _is_tensor_list symbolic_helper _onnx_opset_unsupported_detailed Add Add between list tensors supported alpha symbolic_helper _scalar symbolic_helper _maybe_get_scalar alpha = other = g op Mul other alpha g op Add other _onnx_symbolic aten sub sub g jit_utils GraphContext other alpha=None Consumes sub function returns corresponding ONNX operator This function meant called directly user Args g GraphContext The graph context Tensor The first operand other Tensor The second operand alpha Optional Tensor A scaling factor apply second operand If ` alpha ` provided defaults Returns ONNX operator alpha symbolic_helper _scalar symbolic_helper _maybe_get_scalar alpha = other = g op Mul other alpha g op Sub other _onnx_symbolic aten rsub rsub g jit_utils GraphContext other alpha=None sub g other alpha=alpha _onnx_symbolic aten mul mul g jit_utils GraphContext other symbolic_helper _is_bool symbolic_helper _is_bool other ONNX Mul doesn t support Boolean so use And equivalent operator g op And other g op Mul other _onnx_symbolic aten div div g jit_utils GraphContext other args len args == true_divide g other _div_rounding_mode g other args _onnx_symbolic aten addcmul symbolic_helper parse_args v v v f addcmul g jit_utils GraphContext tensor tensor value= value_tens = g op Constant value_t=torch tensor value add g mul g mul g tensor tensor value_tens symbolic_helper parse_args v v s _div_rounding_mode g jit_utils GraphContext other rounding_mode rounding_mode None true_divide g other rounding_mode == floor _floor_divide g other rounding_mode == trunc _trunc_divide g other raise errors SymbolicValueError f Unsupported rounding mode rounding_mode Expected None floor trunc _trunc_divide g jit_utils GraphContext other out = g op Div other correct operation truncate which supported ONNX we cannot call floor since will behave differently negative numbers eg - should become - - scalar_type information available assume we need call floor treat float out = g op Cast out to_i=_C_onnx TensorProtoDataType INT Matching PyTorch s behavior - fp output s type s type - fp other fp output type JitScalarType FLOAT - fp other fp output s type s output type - output type defaults Float scalar_type = _type_utils JitScalarType from_value _type_utils JitScalarType UNDEFINED scalar_type = _type_utils JitScalarType UNDEFINED symbolic_helper _is_fp symbolic_helper _is_fp other out = g op Cast out to_i=_C_onnx TensorProtoDataType FLOAT out = g op Cast out to_i=scalar_type onnx_type out = g op Cast out to_i=_C_onnx TensorProtoDataType FLOAT out _floor_divide g jit_utils GraphContext other symbolic_helper _is_fp symbolic_helper _is_fp other out = true_divide g other g op Floor out Integer division does truncation rounding div = g op Div other Division negative = other zero = g op Constant value_t=torch tensor dtype=torch int negative = g op Xor symbolic_helper _lt_helper g zero symbolic_helper _lt_helper g other zero For negative numbers other = subtract round down instead up mod = g op Sub g op Mul div other fixup_mask = g op And negative g op Not g op Equal mod zero one = g op Constant value_t=torch tensor dtype=torch int fixup = g op Mul fixup_mask one g op Sub div fixup _onnx_symbolic aten floor_divide floor_divide g jit_utils GraphContext other Deprecated behavior floor_divide actually truncates _trunc_divide g other _onnx_symbolic aten floordiv floordiv g jit_utils GraphContext other floor_divide g other _onnx_symbolic aten true_divide true_divide g jit_utils GraphContext other Division where both inputs cast floating types If both inputs floating performs div usual If only one input floating type other input cast its type If neither input floating type both inputs cast default scalar type Case either values floating Performs div usual Implicit casting will handled scalar type analysis pass symbolic_helper _is_fp symbolic_helper _is_fp other g op Div other Case neither floating Casts both inputs default scalar type scalar_type = torch get_default_dtype onnx_scalar_type = _C_onnx TensorProtoDataType FLOAT assert scalar_type torch float scalar_type torch double torch get_default_dtype torch double onnx_scalar_type = _C_onnx TensorProtoDataType DOUBLE = g op Cast to_i=onnx_scalar_type other = g op Cast other to_i=onnx_scalar_type g op Div other _onnx_symbolic aten reciprocal reciprocal g jit_utils GraphContext torch reciprocal implicitly casts float so we do same symbolic_helper _is_fp = g op Cast to_i=_C_onnx TensorProtoDataType FLOAT g op Reciprocal _onnx_symbolic aten cat symbolic_helper parse_args v i cat g jit_utils GraphContext tensor_list dim Implement concatenation pytorch tensors ONNX along specified ` dim ` dimension Parameters g jit_utils GraphContext Graph context tensor_list List torch Tensor List tensors concatenate dim int Dimension along which concatenate tensors Returns ONNX graph node representing concatenated tensor tensors = symbolic_helper _unpack_list tensor_list torch cat ignores empty tensors such ` torch Tensor ` These needs removed input ONNX s concat too otherwise shape inference will likely fail due inputs different ranks empty tensor anything nonempty_tensors = t tensors symbolic_helper _is_constant t symbolic_helper _get_tensor_dim_size t continue nonempty_tensors append t assert len nonempty_tensors assert all symbolic_helper _get_tensor_rank nonempty_tensors None symbolic_helper _get_tensor_rank t None symbolic_helper _get_tensor_rank t == symbolic_helper _get_tensor_rank nonempty_tensors t nonempty_tensors tensor_list node removeAllInputs t nonempty_tensors tensor_list node addInput t tensors = symbolic_helper _unpack_list tensor_list g op Concat tensors axis_i=dim _onnx_symbolic aten stack symbolic_helper parse_args v i stack g jit_utils GraphContext tensor_list dim unsqueezed = symbolic_helper _unsqueeze_helper g t dim t symbolic_helper _unpack_list tensor_list g op Concat unsqueezed axis_i=dim _onnx_symbolic aten list _list g jit_utils GraphContext _onnx_symbolic aten mm mm g jit_utils GraphContext other Create dummy C tensor Only needed API purposes value since beta = C = g op Constant value_t=torch tensor g op Gemm other C beta_f= alpha_f= _onnx_symbolic aten bmm bmm g jit_utils GraphContext other g op MatMul other _onnx_symbolic aten matmul matmul g jit_utils GraphContext other g op MatMul other _onnx_symbolic aten addmm symbolic_helper parse_args v v v t t addmm g jit_utils GraphContext mat mat beta alpha scalar_type = None self_scalar_type = symbolic_helper _try_get_scalar_type mat _scalar_type = symbolic_helper _try_get_scalar_type mat mat _scalar_type = symbolic_helper _try_get_scalar_type mat self_scalar_type None scalar_type = self_scalar_type mat _scalar_type None scalar_type = mat _scalar_type mat _scalar_type None scalar_type = mat _scalar_type mat _rank = symbolic_helper _get_tensor_rank mat mat _rank = symbolic_helper _get_tensor_rank mat is_not_none_nor v u v None v = u scalar_type None is_not_none_nor mat _rank is_not_none_nor mat _rank res = g op MatMul mat mat res = alpha = symbolic_helper _scalar alpha beta = symbolic_helper _scalar beta alpha = alpha = g op Constant value_t=torch tensor alpha dtype=scalar_type dtype res = g op Mul res alpha beta = beta = g op Constant value_t=torch tensor symbolic_helper _scalar beta dtype=scalar_type dtype res = g op Mul res beta g op Add res res g op Gemm mat mat beta_f=symbolic_helper _scalar beta alpha_f=symbolic_helper _scalar alpha _onnx_symbolic aten neg neg g jit_utils GraphContext g op Neg _onnx_symbolic aten sqrt sqrt g jit_utils GraphContext _type_utils JitScalarType from_value _type_utils JitScalarType UNDEFINED _type_utils JitScalarType UINT _type_utils JitScalarType INT _type_utils JitScalarType INT _type_utils JitScalarType INT _type_utils JitScalarType INT torch converts all int inputs sqrt float = g op Cast to_i=_C_onnx TensorProtoDataType FLOAT g op Sqrt _onnx_symbolic aten rsqrt rsqrt g jit_utils GraphContext g op Div symbolic_helper _if_scalar_type_as torch ones sqrt g _onnx_symbolic aten tanh Fixed scale zero_point discovered aten src ATen native quantized cpu qtanh cpp symbolic_helper quantized_args True scale= zero_point= tanh g jit_utils GraphContext g op Tanh _onnx_symbolic aten sin sin g jit_utils GraphContext g op Sin _onnx_symbolic aten cos cos g jit_utils GraphContext g op Cos _onnx_symbolic aten tan tan g jit_utils GraphContext g op Tan _onnx_symbolic aten asin asin g jit_utils GraphContext g op Asin _onnx_symbolic aten acos acos g jit_utils GraphContext g op Acos _onnx_symbolic aten atan atan g jit_utils GraphContext g op Atan _onnx_symbolic aten atan atan g jit_utils GraphContext other y other x coordinate slope = g op Div other atan = g op Atan slope const_zero = g op Constant value_t=torch tensor const_pi = g op Constant value_t=torch tensor math pi condition_second_or_third_quadrant = g op Greater const_zero second_third_quadrant = g op Where condition_second_or_third_quadrant g op Add atan const_pi g op Sub atan const_pi condition_ _or_ _quadrant = g op Less other const_zero result = g op Where condition_ _or_ _quadrant second_third_quadrant atan result _onnx_symbolic aten sigmoid Fixed scale zero_point discovered aten src ATen native quantized cpu qsigmoid cpp symbolic_helper quantized_args True scale= zero_point= sigmoid g jit_utils GraphContext Converts corresponding PyTorch function into ONNX operators It meant called directly user Args g jit_utils GraphContext Graph context Tensor input tensor Returns ONNX operator g op Sigmoid _onnx_symbolic aten sign sign g jit_utils GraphContext g op Sign symbolic_helper quantized_args True _slice g jit_utils GraphContext input axes starts ends assert len starts == len ends len starts == starts == ends == _constants INT _MAX input g op Slice input axes_i=axes starts_i=starts ends_i=ends _onnx_symbolic aten sum decorate= symbolic_helper _apply_params ReduceSum sum _onnx_symbolic aten mean decorate= symbolic_helper _apply_params ReduceMean mean torch prod does support multidimensional dim _onnx_symbolic aten prod decorate= symbolic_helper _apply_params ReduceProd prod allow_multi_dim_support=False _reduce_with_dtype onnx_op str name str allow_multi_dim_support bool = True symbolic_helper _reduce_with_dtype_helper onnx_op name allow_multi_dim_support _onnx_symbolic aten cumsum symbolic_helper parse_args v i none cumsum g jit_utils GraphContext input dim dtype symbolic_helper _onnx_opset_unsupported cumsum input _onnx_symbolic aten _sample_dirichlet _sample_dirichlet g jit_utils GraphContext generator symbolic_helper _onnx_unsupported _sample_dirichlet _onnx_symbolic aten _standard_gamma _standard_gamma g jit_utils GraphContext generator symbolic_helper _onnx_unsupported _standard_gamma _onnx_symbolic aten t t g jit_utils GraphContext rank = symbolic_helper _get_tensor_rank rank None rank The transpose d d tensor itself ONNX does define behavior clearly onnxruntime fails these cases So we add Identity node mirror behavior eager mode g op Identity g op Transpose perm_i= _onnx_symbolic aten numpy_T symbolic_helper quantized_args True numpy_T g jit_utils GraphContext input ndim = symbolic_helper _get_tensor_rank input assert ndim None perm = list reversed range ndim g op Transpose input perm_i=perm _onnx_symbolic aten expand symbolic_helper quantized_args True expand g jit_utils GraphContext size implicit Implement expand function pytorch tensor ONNX according specified ` size ` size = symbolic_helper _maybe_get_const size symbolic_helper _is_value size size = g op Constant value_t=torch LongTensor size symbolic_helper _is_packed_list size Expand - dim value means dim unchanged Since onnx expand supports two-way broadcasting - dim value can exported onnx size = symbolic_helper _reshape_helper g stack g size g op Constant value_t=torch tensor - dtype = _type_utils JitScalarType INT ones = ones_like g size dtype neg_ones = mul g ones g op Constant value_t=torch tensor - size = where g g op Equal size neg_ones ones size g op Expand size _onnx_symbolic aten broadcast_to symbolic_helper quantized_args True broadcast_to g jit_utils GraphContext size size = symbolic_helper _maybe_get_const size symbolic_helper _is_value size size = g op Constant value_t=torch LongTensor size symbolic_helper _is_packed_list size Expand - dim value means dim unchanged Since onnx expand supports two-way broadcasting - dim value can exported onnx size = symbolic_helper _reshape_helper g stack g size g op Constant value_t=torch tensor - dtype = _type_utils JitScalarType INT ones = ones_like g size dtype neg_ones = mul g ones g op Constant value_t=torch tensor - size = where g g op Equal size neg_ones ones size g op Expand size _onnx_symbolic aten expand_as symbolic_helper quantized_args True True expand_as g jit_utils GraphContext other self_t = symbolic_helper _maybe_get_const t isinstance self_t torch Tensor orig_type = self_t dtype self_t = self_t torch double dims = d range self_t dim torch equal self_t mean d unsqueeze d expand_as self_t self_t dims append d = g op Constant value_t=self_t mean dims keepdim=True orig_type shape = g op Shape other g op Expand shape _onnx_symbolic aten embedding symbolic_helper quantized_args True symbolic_helper parse_args v v i b v embedding g jit_utils GraphContext weight indices padding_idx scale_grad_by_freq sparse scale_grad_by_freq GLOBALS export_training raise errors SymbolicValueError Unsupported ONNX export embedding scale_grad_by_freq=True training mode ONNX does support scaling gradients weight padding_idx = GLOBALS export_training warnings warn Warning ONNX export embedding padding_idx = training mode ONNX does support updating embedding vector padding_idx during training stacklevel= g op Gather weight indices _onnx_symbolic aten embedding_bag symbolic_helper quantized_args True symbolic_helper parse_args v v v i i i v i i embedding_bag g jit_utils GraphContext embedding_matrix indices offsets scale_grad_by_freq mode sparse per_sample_weights include_last_offset padding_idx symbolic_helper _is_none per_sample_weights symbolic_helper _onnx_unsupported embedding_bag per_sample_weights symbolic_helper _onnx_unsupported embedding_bag embedding_matrix _onnx_symbolic aten size symbolic_helper quantized_args True quantize_output=False size g jit_utils GraphContext dim=None dim None g op Shape symbolic_helper _maybe_get_const dim i rank = symbolic_helper _get_tensor_rank rank None dim = symbolic_helper _maybe_get_const dim i + rank dim = g op Constant value_t=torch tensor dim symbolic_helper _size_helper g dim _onnx_symbolic aten transpose symbolic_helper quantized_args True symbolic_helper parse_args v i i transpose g jit_utils GraphContext dim dim dim == dim micro-optimization NB Transpose ONNX actually Permute rank = symbolic_helper _get_tensor_rank rank None axes = list range rank axes dim axes dim = axes dim axes dim g op Transpose perm_i=axes raise errors SymbolicValueError Unsupported ONNX export transpose tensor unknown rank _onnx_symbolic aten permute symbolic_helper parse_args v permute g jit_utils GraphContext dims dims == list range len dims g op Transpose perm_i=dims _onnx_symbolic aten view symbolic_helper quantized_args True view g jit_utils GraphContext size reshape g size _onnx_symbolic aten view_as view_as g jit_utils GraphContext other shape = g op Shape other reshape g shape _onnx_symbolic aten unsafe_chunk symbolic_helper parse_args v i i i unsafe_chunk g jit_utils GraphContext chunks dim _outputs=None _outputs None symbolic_helper _onnx_opset_unsupported_detailed unsafe_chunk Dynamic number outputs supported size = symbolic_helper _get_tensor_dim_size dim size None symbolic_helper _unimplemented unsafe_chunk unknown dimension size split_size = size + chunks - chunks splits = split_size size split_size leftover = size split_size leftover splits append leftover g op Split split_i=splits axis_i=dim outputs=_outputs _onnx_symbolic aten split symbolic_helper parse_args v v i i split g jit_utils GraphContext split_size_or_sizes dim _outputs=None symbolic_helper _is_split_static split_size_or_sizes _outputs symbolic_helper _onnx_opset_unsupported_detailed split Dynamic number outputs supported split_val = symbolic_helper _node_get split_size_or_sizes node value split_val dim split_with_sizes g split_size_or_sizes dim _outputs split_size = symbolic_helper _get_const split_size_or_sizes i split_size size = symbolic_helper _get_tensor_dim_size dim size None _outputs None size = split_size _outputs symbolic_helper _onnx_opset_unsupported_detailed split Unknown dimension size supported splits = split_size size split_size leftover = size split_size leftover splits append leftover pyrefly ignore bad-argument-type g op Split split_i=splits axis_i=dim outputs=_outputs _onnx_symbolic aten unsafe_split unsafe_split g jit_utils GraphContext split_size_or_sizes dim _outputs=None split g split_size_or_sizes dim _outputs _onnx_symbolic aten split_with_sizes symbolic_helper parse_args v i i split_with_sizes g jit_utils GraphContext split_sizes dim _outputs=None symbolic_helper _is_split_static split_sizes _outputs symbolic_helper _onnx_opset_unsupported_detailed split_with_sizes Dynamic number outputs supported pyrefly ignore bad-argument-type g op Split split_i=split_sizes axis_i=dim outputs=_outputs _onnx_symbolic aten unsafe_split_with_sizes unsafe_split_with_sizes g jit_utils GraphContext split_sizes dim _outputs=None split_with_sizes g split_sizes dim _outputs _onnx_symbolic aten unbind symbolic_helper parse_args v i i unbind g jit_utils GraphContext dim= _outputs=None _outputs None symbolic_helper _onnx_opset_unsupported_detailed unbind Dynamic number outputs supported outputs = g op Split split_i= _outputs axis_i=dim outputs=_outputs outputs = outputs _outputs == outputs squeezed_outputs = symbolic_helper _squeeze_helper g out dim out outputs squeezed_outputs _onnx_symbolic aten select symbolic_helper quantized_args True symbolic_helper parse_args v i v select g jit_utils GraphContext dim index Implement select functionality pytorch tensor ONNX Selects elements input tensor along specified ` dim ` dimension based ` index ` tensor index = symbolic_helper _maybe_get_scalar index symbolic_helper _is_value index index index == - end_index = _constants INT _MAX end_index = index + slice_node = symbolic_helper _slice_helper g axes= dim starts= index ends= end_index symbolic_helper _squeeze_helper g slice_node dim FIXME justinchuby can index int value g op Gather index axis_i=dim _onnx_symbolic aten square square g jit_utils GraphContext g op Mul _onnx_symbolic aten squeeze squeeze g jit_utils GraphContext dim=None dim None g op Squeeze squeeze_dim = symbolic_helper _get_const dim i dim Handle negative dims squeeze_dim rank = symbolic_helper _get_tensor_rank rank None warnings warn ONNX export squeeze negative axis + str squeeze_dim + might cause onnx model incorrect + Negative axis supported ONNX + Axis converted + str squeeze_dim + rank + based input shape export time + Passing tensor different rank execution will incorrect stacklevel= squeeze_dim += rank symbolic_helper _unimplemented squeeze negative axis unknown input rank dim_size = symbolic_helper _get_tensor_dim_size squeeze_dim dim_size None warnings warn This model contains squeeze operation dimension + str squeeze_dim + input + unknown shape Note size dimension + str squeeze_dim + input + ONNX model will error Opset version supports squeezing + non-singleton dimensions recommended export model using opset + version higher stacklevel= symbolic_helper _squeeze_helper g axes_i= squeeze_dim dim_size warnings warn This model contains squeeze operation dimension + str squeeze_dim + The size + dimension given input + str dim_size + The model will + exported without squeeze node If model intended used dynamic + input shapes please use opset version + export model stacklevel= warnings warn This model contains squeeze operation dimension + str squeeze_dim + If model + intended used dynamic input shapes please use opset version export model stacklevel= symbolic_helper _squeeze_helper g axes_i= squeeze_dim _onnx_symbolic aten prelu prelu g jit_utils GraphContext weight self_rank = symbolic_helper _get_tensor_rank weight_sizes = symbolic_helper _get_tensor_sizes weight weight_rank = len weight_sizes self_rank None self_rank make weight unidirectional broadcastable weight = symbolic_helper _unsqueeze_helper g weight list range self_rank - self_rank == weight_sizes == weight both scalar weight has rank == squeeze weight weight = symbolic_helper _squeeze_helper g weight weight_rank = self_rank None weight_rank None assert self_rank = weight_rank f rank x should = rank slope got self_rank weight_rank g op PRelu weight _onnx_symbolic aten silu silu g jit_utils GraphContext input g op Mul input g op Sigmoid input _onnx_symbolic aten mish mish g jit_utils GraphContext input g op Mul input g op Tanh g op Softplus input _onnx_symbolic aten relu symbolic_helper quantized_args True relu g jit_utils GraphContext input symbolic_helper _op_with_optional_float_cast g Relu input opset_before= _onnx_symbolic aten relu symbolic_helper quantized_args True relu g jit_utils GraphContext input clamp g input _onnx_symbolic aten ceil ceil g jit_utils GraphContext input g op Ceil input _onnx_symbolic aten floor floor g jit_utils GraphContext input g op Floor input _onnx_symbolic aten len _len g jit_utils GraphContext sz_ = size g g op Constant value_t=torch LongTensor symbolic_helper _squeeze_helper g sz_ _onnx_symbolic aten threshold symbolic_helper parse_args v t t threshold g jit_utils GraphContext threshold value See Note Export inplace symbolic_helper _scalar threshold = symbolic_helper _unimplemented threshold non-zero threshold symbolic_helper _scalar value = symbolic_helper _unimplemented threshold non-zero value g op Relu _onnx_symbolic aten leaky_relu symbolic_helper quantized_args True symbolic_helper parse_args v f b leaky_relu g jit_utils GraphContext input _C Value negative_slope float inplace bool = False See Note Export inplace g op LeakyRelu input alpha_f=negative_slope _onnx_symbolic aten glu symbolic_helper parse_args v i glu g jit_utils GraphContext input dim dim_size = symbolic_helper _get_tensor_dim_size input dim dim_size None assert dim_size == first second = g op Split input axis_i=dim outputs= g op Mul first g op Sigmoid second _onnx_symbolic aten softmax symbolic_helper parse_args v i none softmax g jit_utils GraphContext input dim dtype=None Softmax does normalization vector level PyTorch ONNX use different strategies split input tensor into vectors Thus dim axis have different meanings PyTorch slices input tensor into vectors along ` dim ` -th dimension ONNX reshapes input into -D tensor ` axis ` indicates where input coerced If input x tensor input = dim = result result = axis = result result = So only when dim axis both equal ndim - last dimension their semantics equivalent So use softmax when dim axis both equal ndim - otherwise transpose input put vectors normalized last dimension When input rank known export time we compute softmax using subgraph other operators input_dim = symbolic_helper _get_tensor_rank input input_dim None TODO remove onnx opset spec allows negative axes dim dim = input_dim + dim is_transpose_required = input_dim = dim + is_transpose_required axes = list range input_dim axes dim axes - = axes - axes dim input = g op Transpose input perm_i=axes dim = input_dim - softmax = g op Softmax input axis_i=dim dtype dtype node kind = prim Constant parsed_dtype = symbolic_helper _get_const dtype i dtype softmax = g op Cast softmax to_i=_type_utils JitScalarType parsed_dtype onnx_type is_transpose_required softmax = g op Transpose softmax perm_i=axes type ignore possibly-undefined softmax Apply max normalization input = g op Sub input g op ReduceMax input axes_i= dim keepdims_i= exp = g op Exp input sum = symbolic_helper _reducesum_helper g exp axes_i= dim softmax = g op Div exp sum dtype dtype node kind = prim Constant parsed_dtype = symbolic_helper _get_const dtype i dtype softmax = g op Cast softmax to_i=_type_utils JitScalarType parsed_dtype onnx_type softmax _onnx_symbolic aten softplus softplus g jit_utils GraphContext beta threshold beta_const = symbolic_helper _maybe_get_const beta f beta_const = g op Div g op Softplus g op Mul beta beta g op Softplus _onnx_symbolic aten get_pool_ceil_padding get_pool_ceil_padding input kernel_size stride padding TODO justinchuby Looks like op deprecated torch sizes = symbolic_helper _get_tensor_sizes input dim = sizes -len padding sizes None None dim None any i None i dim symbolic_helper _unimplemented get_pool_ceil_padding input size accessible input ceiled_output_dim = math ceil dim i + padding i - kernel_size i float stride i + i range len padding ensure last pooling starts inside ceiled_output_dim = ceiled_output_dim i - ceiled_output_dim i - stride i = dim i + padding i ceiled_output_dim i i range len ceiled_output_dim padding_ceil = stride i == kernel_size i - dim i + padding i - ceiled_output_dim i - stride i + i range len padding ensure padding kernel_size padding_ceil = int padding_ceil i padding_ceil i kernel_size i - int kernel_size i - padding_ceil i + padding i = kernel_size i int padding_ceil i i range len padding_ceil padding_ceil _onnx_symbolic aten max_pool d decorate= symbolic_helper _apply_params max_pool d torch nn modules utils _single return_indices=False _export max_pool d _onnx_symbolic aten max_pool d decorate= symbolic_helper _apply_params max_pool d torch nn modules utils _pair return_indices=False _export max_pool d _onnx_symbolic aten max_pool d decorate= symbolic_helper _apply_params max_pool d torch nn modules utils _triple return_indices=False _export max_pool d _max_pool name tuple_fn ndims return_indices symbolic_helper quantized_args True False False False False False symbolic_helper parse_args v i symbolic_fn g input kernel_size stride padding dilation ceil_mode set tuple_fn dilation = symbolic_helper _unimplemented name dilation input stride stride = kernel_size padding = tuple tuple_fn padding ceil_mode padding_ceil = get_pool_ceil_padding input kernel_size stride padding padding = padding + tuple + b b zip padding_ceil padding padding = padding kwargs = kernel_shape_i tuple_fn kernel_size pads_i padding strides_i tuple_fn stride easy hacky way get flattened indices values used convert indices values non-flattened In ONNX indices computed flatten -D tensor so values indices N x C x D x x Dn To convert indices same format used Pytorch we first execute maxpool kernel stride same input This will result tensor indices which each index will have s own value Using tensor reference we extract first index each axis subtract each index axis indices convert This step will result tensor each dimension has values indices within dimension For more information https github com pytorch pytorch pull #issuecomment- return_indices r indices = g op MaxPool input outputs= kwargs _ flattened_indices = g op MaxPool input outputs= kernel_shape_i= _ range ndims strides_i= _ range ndims convert indices have non-flattened indices values s = symbolic_helper _slice_helper g flattened_indices axes= + i i range ndims starts=list tuple_fn ends=list tuple_fn indices = sub g indices s r indices r = g op MaxPool input outputs= kwargs r symbolic_fn max_pool d_with_indices = _onnx_symbolic aten max_pool d_with_indices _max_pool max_pool d_with_indices torch nn modules utils _single return_indices=True max_pool d_with_indices = _onnx_symbolic aten max_pool d_with_indices _max_pool max_pool d_with_indices torch nn modules utils _pair return_indices=True max_pool d_with_indices = _onnx_symbolic aten max_pool d_with_indices _max_pool max_pool d_with_indices torch nn modules utils _triple return_indices=True _onnx_symbolic aten avg_pool d decorate= symbolic_helper _apply_params avg_pool d torch nn modules utils _single _export avg_pool d _onnx_symbolic aten avg_pool d decorate= symbolic_helper _apply_params avg_pool d torch nn modules utils _pair _export avg_pool d _onnx_symbolic aten avg_pool d decorate= symbolic_helper _apply_params avg_pool d torch nn modules utils _triple _export avg_pool d _avg_pool name tuple_fn symbolic_helper quantized_args True symbolic_helper parse_args v i i none symbolic_fn g input _C Value kernel_size Sequence int stride Sequence int padding int &#124; Sequence int ceil_mode int count_include_pad int divisor_override=None stride stride = kernel_size padding = symbolic_helper _avgpool_helper tuple_fn padding kernel_size stride divisor_override name assert isinstance padding tuple adjusted_padding = padding Although onnx AvgPool provides count_include_pad The corner case Average Pooling ceil_mode PyTorch allows sliding window go off bound which leads accommodation More detail https github com pytorch pytorch issues count_include_pad input = symbolic_helper _op_with_optional_float_cast g Pad input pads_i= + padding mode_s= constant value_f= opset_before= adjusted_padding = len padding ceil_mode padding_ceil = get_pool_ceil_padding input kernel_size stride padding adjusted_padding = adjusted_padding + tuple + b b zip padding_ceil adjusted_padding adjusted_padding = adjusted_padding output = g op AveragePool input kernel_shape_i=tuple_fn kernel_size strides_i=tuple_fn stride pads_i=adjusted_padding output symbolic_fn _onnx_symbolic aten adaptive_avg_pool d decorate= symbolic_helper _apply_params adaptive_avg_pool d AveragePool torch nn modules utils _single _export adaptive_avg_pool d _onnx_symbolic aten adaptive_avg_pool d decorate= symbolic_helper _apply_params adaptive_avg_pool d AveragePool torch nn modules utils _pair _export adaptive_avg_pool d _onnx_symbolic aten adaptive_avg_pool d decorate= symbolic_helper _apply_params adaptive_avg_pool d AveragePool torch nn modules utils _triple _export adaptive_avg_pool d _onnx_symbolic aten adaptive_max_pool d decorate= symbolic_helper _apply_params adaptive_max_pool d MaxPool torch nn modules utils _single max_pool d_with_indices _export adaptive_max_pool d _onnx_symbolic aten adaptive_max_pool d decorate= symbolic_helper _apply_params adaptive_max_pool d MaxPool torch nn modules utils _pair max_pool d_with_indices _export adaptive_max_pool d _onnx_symbolic aten adaptive_max_pool d decorate= symbolic_helper _apply_params adaptive_max_pool d MaxPool torch nn modules utils _triple max_pool d_with_indices _export adaptive_max_pool d _adaptive_pool name type tuple_fn fn=None symbolic_helper quantized_args True False symbolic_fn g input output_size _adaptive_pool supported cases where output_size all dimensions executing GlobalPool It also supported cases where output size factor input size For these cases stride kernel size uniform along all indices same dimension which makes possible export ONNX MaxPool GlobalMaxPool does indices so we try using max_poolxd_with_indices possible input complete tensor output size factor input size then we call GlobalAveragePool None indices output_size_value = output_size try output_size = symbolic_helper _parse_arg output_size except Exception FIXME justinchuby Avoid catching Exception Catch more specific exception instead symbolic_helper _onnx_unsupported adaptive pooling since output_size constant input output_size == len output_size type == AveragePool g op GlobalAveragePool input sizes = symbolic_helper _get_tensor_sizes input try dim = sizes except Exception FIXME justinchuby Avoid catching Exception Catch more specific exception instead dim = None dim None any i None i dim output_size == len output_size g op GlobalMaxPool input None symbolic_helper _unimplemented name input size accessible input verify output size input size = all dim mod = dim i output_size i i range len dim mod = len mod output_size == len output_size g op GlobalMaxPool input None symbolic_helper _unimplemented name output size factor input size output_size_value k = int dim i output_size i i range len dim call max_poolxd_with_indices get indices output type == MaxPool pyrefly ignore not-callable fn g input k k len dim len dim False output = g op type input kernel_shape_i=tuple_fn k strides_i=tuple_fn k output symbolic_fn _prepare_onnx_paddings dim int pad Generate paddings ONNX order based pad pytorch Args dim dimension tensor pad paddings pytorch The order dim_n_begin dim_n_end dim_n- _begin dim_n- _end The desired order paddings dim_ _begin dim_ _begin dim_ _end dim_n_end n dimension input assume zero-dimensions beginning paddings = list pad + dim - len pad reverse order collate first beginnings then ends paddings = paddings - - + paddings - - paddings _convert_padding_node input padding = symbolic_helper _maybe_get_const input symbolic_helper _is_value padding symbolic_helper _is_packed_list padding input_list = symbolic_helper _unpack_list padding try padding = symbolic_helper _get_const v i padding v input_list except Exception FIXME justinchuby Avoid catching Exception Catch more specific exception instead symbolic_helper _onnx_opset_unsupported_detailed Pad The sizes padding must constant input padding _onnx_symbolic aten constant_pad_nd constant_pad_nd g jit_utils GraphContext input padding value mode = constant try value = symbolic_helper _get_const value f value except Exception FIXME justinchuby Avoid catching Exception Catch more specific exception instead symbolic_helper _onnx_opset_unsupported_detailed Pad The value padding must constant value padding = _convert_padding_node padding pyrefly ignore bad-argument-type paddings = _prepare_onnx_paddings symbolic_helper _get_tensor_rank input padding symbolic_helper _op_with_optional_float_cast g Pad input pads_i=paddings mode_s=mode value_f=value opset_before= _pad_circular g jit_utils GraphContext input _C Value pad _C Value padding = _convert_padding_node pad assert len padding == ndim = len padding cur = input idx range ndim pad_r = padding - idx + pad_l = padding - idx + tensors = pad_l left = symbolic_helper _slice_helper g cur axes= + idx starts= - pad_l ends= _constants INT _MAX tensors append left pad_l pad_r start = builtins max -pad_l end = - builtins max -pad_r middle = symbolic_helper _slice_helper g cur axes= + idx starts= start ends= end tensors append middle tensors append cur pad_r right = symbolic_helper _slice_helper g cur axes= + idx starts= ends= pad_r tensors append right cur = g op Concat tensors axis_i= + idx cur _onnx_symbolic aten reflection_pad d _onnx_symbolic aten reflection_pad d _onnx_symbolic aten reflection_pad d reflection_pad g jit_utils GraphContext input padding mode = reflect padding = _convert_padding_node padding pyrefly ignore bad-argument-type paddings = _prepare_onnx_paddings symbolic_helper _get_tensor_rank input padding symbolic_helper _op_with_optional_float_cast g Pad input pads_i=paddings mode_s=mode opset_before= _onnx_symbolic aten replication_pad d _onnx_symbolic aten replication_pad d _onnx_symbolic aten replication_pad d replication_pad g jit_utils GraphContext input padding mode = edge padding = _convert_padding_node padding pyrefly ignore bad-argument-type paddings = _prepare_onnx_paddings symbolic_helper _get_tensor_rank input padding symbolic_helper _op_with_optional_float_cast g Pad input pads_i=paddings mode_s=mode opset_before= _onnx_symbolic aten pad pad g jit_utils GraphContext input _C Value pad _C Value mode _C Value value _C Value mode = symbolic_helper _parse_arg mode s mode == replicate replication_pad g input pad mode == reflect reflection_pad g input pad mode == constant constant_pad_nd g input pad value mode == circular _pad_circular g input pad raise errors SymbolicValueError f Unrecognized padding mode mode input _onnx_symbolic aten upsample_nearest d decorate= symbolic_helper _apply_params upsample_nearest d nearest _export upsample_nearest d _onnx_symbolic aten upsample_nearest d decorate= symbolic_helper _apply_params upsample_nearest d nearest _export upsample_nearest d _onnx_symbolic aten upsample_nearest d decorate= symbolic_helper _apply_params upsample_nearest d nearest _export upsample_nearest d _onnx_symbolic aten upsample_linear d decorate= symbolic_helper _apply_params upsample_linear d linear _export upsample_linear d _onnx_symbolic aten upsample_bilinear d decorate= symbolic_helper _apply_params upsample_bilinear d linear _export upsample_bilinear d _onnx_symbolic aten upsample_trilinear d decorate= symbolic_helper _apply_params upsample_trilinear d linear _export upsample_trilinear d _interpolate name str dim int interpolate_mode str symbolic_fn g input output_size args scales align_corners = symbolic_helper _get_interpolate_attributes g interpolate_mode args symbolic_helper _interpolate_warning interpolate_mode align_corners = symbolic_helper _maybe_get_scalar align_corners align_corners symbolic_helper _unimplemented name align_corners == True input scales None scales = symbolic_helper _interpolate_size_to_scales g input output_size dim g op Upsample input scales mode_s=interpolate_mode symbolic_fn _onnx_symbolic aten __interpolate __interpolate g jit_utils GraphContext input size scale_factor mode align_corners recompute_scale_factor antialias scales mode = symbolic_helper _interpolate_get_scales_and_mode g input size scale_factor mode align_corners g op Upsample input scales mode_s=mode _onnx_symbolic aten bitwise_not bitwise_not g jit_utils GraphContext input symbolic_helper _is_bool input raise errors SymbolicValueError ONNX export does NOT support exporting bitwise Not non-boolean input values input g op Not input _onnx_symbolic aten bitwise_or bitwise_or g other symbolic_helper _is_bool raise errors SymbolicValueError ONNX export does NOT support exporting bitwise OR non-boolean input values symbolic_helper _is_bool other raise errors SymbolicValueError ONNX export does NOT support exporting bitwise OR non-boolean input values other other g op Or other wrap_logical_op_with_cast_to to_type decorator fn functools wraps fn wrap_with_cast g input other to_cast_func = globals f _cast_ to_type fn g to_cast_func g input False to_cast_func g other False wrap_with_cast decorator wrap_logical_op_with_negation func Callable - Callable functools wraps func wrap_with_not g input other g op Not func g input other wrap_with_not _onnx_symbolic aten __not_ __not_ g jit_utils GraphContext symbolic_helper _is_bool raise errors SymbolicValueError ONNX export does NOT support exporting bitwise Not non-boolean input values g op Not _onnx_symbolic aten eq symbolic_helper quantized_args True True eq g jit_utils GraphContext other isinstance type _C DeviceObjType isinstance other type _C DeviceObjType ONNX doesn t have devices so consider them all equal The no-op check equality will get constant-folded g op Constant value_t=torch tensor True dtype=torch bool self_node = node other_node = other node self_node kind == other_node kind == onnx Constant self_node kindOf value == other_node kindOf value == s Exporting strings ONNX supported If both strings constant we can compare them directly The no-op check equality will get constant-folded g op Constant value_t=torch tensor self_node s value == other_node s value dtype=torch bool g op Equal other _onnx_symbolic aten ne symbolic_helper quantized_args True True wrap_logical_op_with_negation ne g jit_utils GraphContext other eq g other _onnx_symbolic aten gt symbolic_helper quantized_args True True gt g jit_utils GraphContext input other _gt_impl g input other _gt_impl g jit_utils GraphContext input other symbolic_helper _is_bool input symbolic_helper _is_bool other input = g op Cast input to_i=_C_onnx TensorProtoDataType INT other = g op Cast other to_i=_C_onnx TensorProtoDataType INT g op Greater input other _onnx_symbolic aten lt symbolic_helper quantized_args True True lt g jit_utils GraphContext input other _lt_impl g input other _lt_impl g jit_utils GraphContext input other symbolic_helper _is_bool input symbolic_helper _is_bool other input = g op Cast input to_i=_C_onnx TensorProtoDataType INT other = g op Cast other to_i=_C_onnx TensorProtoDataType INT g op Less input other _onnx_symbolic aten ge symbolic_helper quantized_args True True wrap_logical_op_with_negation ge g jit_utils GraphContext input other _lt_impl g input other _onnx_symbolic aten le symbolic_helper quantized_args True True wrap_logical_op_with_negation le g jit_utils GraphContext input other _gt_impl g input other _onnx_symbolic aten __and_ __and_ g jit_utils GraphContext input other symbolic_helper _is_bool input raise errors SymbolicValueError ONNX export does NOT support exporting bitwise AND non-boolean input values input symbolic_helper _is_bool other raise errors SymbolicValueError ONNX export does NOT support exporting bitwise AND non-boolean input values other g op And input other _onnx_symbolic aten __or_ __or_ g jit_utils GraphContext input other symbolic_helper _is_bool input raise errors SymbolicValueError ONNX export does NOT support exporting bitwise OR non-boolean input values input symbolic_helper _is_bool other raise errors SymbolicValueError ONNX export does NOT support exporting bitwise OR non-boolean input values other g op Or input other _onnx_symbolic aten __xor_ __xor_ g jit_utils GraphContext input other symbolic_helper _is_bool input raise errors SymbolicValueError ONNX export does NOT support exporting bitwise XOR non-boolean input values input symbolic_helper _is_bool other raise errors SymbolicValueError ONNX export does NOT support exporting bitwise XOR non-boolean input values other g op Xor input other _onnx_symbolic aten logical_and wrap_logical_op_with_cast_to Bool logical_and g jit_utils GraphContext input other g op And input other _onnx_symbolic aten logical_or wrap_logical_op_with_cast_to Bool logical_or g jit_utils GraphContext input other g op Or input other _onnx_symbolic aten logical_xor wrap_logical_op_with_cast_to Bool logical_xor g jit_utils GraphContext input other g op Xor input other _onnx_symbolic aten logical_not logical_not g jit_utils GraphContext input g op Not g op Cast input to_i=_C_onnx TensorProtoDataType BOOL _onnx_symbolic aten __rshift_ __rshift_ g jit_utils GraphContext other make sure cast other s type when long make sure other float self_scalar_type = _type_utils JitScalarType from_value _type_utils JitScalarType from_value other _type_utils JitScalarType UNDEFINED = self_scalar_type other = g op Cast other to_i=self_scalar_type onnx_type two = g op Constant value_t=torch tensor dtype=torch float exponent same type has float double onnx Pow symbolic_helper _is_fp other = g op Cast other to_i=_C_onnx TensorProtoDataType FLOAT two_pow = g op Pow two other two_pow = g op Cast two_pow to_i=self_scalar_type onnx_type rshift = g op Div two_pow rshift _onnx_symbolic aten __lshift_ __lshift_ g jit_utils GraphContext other make sure cast other s type when long make sure other float self_scalar_type = _type_utils JitScalarType from_value _type_utils JitScalarType from_value other _type_utils JitScalarType UNDEFINED = self_scalar_type other = g op Cast other to_i=self_scalar_type onnx_type two = g op Constant value_t=torch tensor dtype=torch float exponent same type has float double onnx Pow symbolic_helper _is_fp other = g op Cast other to_i=_C_onnx TensorProtoDataType FLOAT two_pow = g op Pow two other two_pow = g op Cast two_pow to_i=self_scalar_type onnx_type lshift = g op Mul two_pow lshift _onnx_symbolic aten where symbolic_helper parse_args v v v i where g jit_utils GraphContext condition self=None other=None _outputs=None Assumes torch where s first argument takes only Bool Byte tensors symbolic_helper _is_bool condition condition = g op Cast condition to_i=_C_onnx TensorProtoDataType BOOL None condition = nonzero g condition symbolic_helper _unbind_helper g condition g op Constant value_t=torch tensor _outputs pyrefly ignore bad-argument-type g op Where condition other _onnx_symbolic aten log_softmax symbolic_helper parse_args v i none log_softmax g jit_utils GraphContext input dim dtype=None PyTorch dim ONNX axis have different meanings See Softmax comment details TODO remove onnx opset spec allows negative axes input_dim = symbolic_helper _get_tensor_rank input input_dim None symbolic_helper _unimplemented dim ONNX PyTorch use different strategies split input Input rank must known export time dim dim = input_dim + dim is_transpose_required = input_dim = dim + ONNX only supports log_softmax dim = - Transpose must added before after log_softmax support other cases is_transpose_required axes = list range input_dim axes dim axes - = axes - axes dim input = g op Transpose input perm_i=axes dim = input_dim - return_op = g op LogSoftmax input axis_i=dim dtype dtype node kind = prim Constant parsed_dtype = symbolic_helper _get_const dtype i dtype return_op = g op Cast return_op to_i=_type_utils JitScalarType parsed_dtype onnx_type is_transpose_required return_op = g op Transpose return_op perm_i=axes type ignore possibly-undefined return_op _onnx_symbolic aten _log_softmax symbolic_helper parse_args v i i _log_softmax g jit_utils GraphContext input dim half_to_float half_to_float _type_utils JitScalarType from_value input _type_utils JitScalarType UNDEFINED == _type_utils JitScalarType HALF input = g op Cast input to_i=_C_onnx TensorProtoDataType FLOAT log_softmax g input dim _onnx_symbolic aten _convolution symbolic_helper parse_args v v v i i i i i i _convolution g jit_utils GraphContext input weight bias stride padding dilation transposed output_padding groups benchmark deterministic cudnn_enabled allow_tf =None weight_size = symbolic_helper _get_tensor_sizes weight try kernel_shape = weight_size except Exception FIXME justinchuby Avoid catching Exception Catch more specific exception instead kernel_shape = None kernel_shape None any i None i kernel_shape raise errors SymbolicValueError Unsupported ONNX export convolution kernel unknown shape input args = input weight ONNX only supports D bias symbolic_helper _is_none bias symbolic_helper _get_tensor_rank bias == args append bias kwargs = kernel_shape_i weight_size strides_i stride NB ONNX supports asymmetric padding whereas PyTorch supports only symmetric padding pads_i padding + padding dilations_i dilation group_i groups any o = o output_padding ONNX supports both output_shape output_padding they equivalent expressive output_padding more straightforward so we use here output_shape = stride input_shape - + output_padding + kernel_shape - padding assert transposed assert len stride == len output_padding kwargs output_padding_i = output_padding n = g op ConvTranspose transposed Conv args kwargs symbolic_helper _is_none bias symbolic_helper _get_tensor_rank bias = g op Add n bias n _onnx_symbolic aten _convolution_mode symbolic_helper parse_args v v v s i _convolution_mode g jit_utils GraphContext input weight bias stride padding dilation groups weight_size = symbolic_helper _get_tensor_sizes weight try kernel_shape = weight_size except Exception FIXME justinchuby Avoid catching Exception Catch more specific exception instead kernel_shape = None kernel_shape None any i None i kernel_shape raise errors SymbolicValueError Unsupported ONNX export convolution kernel unknown shape input args = input weight ONNX only supports D bias symbolic_helper _is_none bias symbolic_helper _get_tensor_rank bias == args append bias padding == valid padding = VALID padding == same padding = SAME_UPPER kwargs = kernel_shape_i weight_size strides_i stride auto_pad_s padding dilations_i dilation group_i groups pyrefly ignore bad-argument-type n = g op Conv args kwargs symbolic_helper _is_none bias symbolic_helper _get_tensor_rank bias = g op Add n bias n _onnx_symbolic aten convolution symbolic_helper parse_args v v v i i convolution g jit_utils GraphContext input weight bias stride padding dilation transposed output_padding groups _convolution g input weight bias stride padding dilation transposed output_padding groups None None None None _onnx_symbolic aten conv d symbolic_helper parse_args v v v v i conv d g jit_utils GraphContext input weight bias stride padding dilation groups str_padding = symbolic_helper _parse_arg padding s str_padding valid same _convolution_mode g input weight bias stride str_padding dilation groups padding = symbolic_helper _parse_arg padding _convolution g input weight bias stride padding dilation False groups None None None None _onnx_symbolic aten conv d symbolic_helper parse_args v v v v i conv d g jit_utils GraphContext input weight bias stride padding dilation groups str_padding = symbolic_helper _parse_arg padding s str_padding valid same _convolution_mode g input weight bias stride str_padding dilation groups padding = symbolic_helper _parse_arg padding _convolution g input weight bias stride padding dilation False groups None None None None _onnx_symbolic aten conv d symbolic_helper parse_args v v v v i conv d g jit_utils GraphContext input weight bias stride padding dilation groups str_padding = symbolic_helper _parse_arg padding s str_padding valid same _convolution_mode g input weight bias stride str_padding dilation groups padding = symbolic_helper _parse_arg padding _convolution g input weight bias stride padding dilation False groups None None None None _onnx_symbolic aten conv_transpose d symbolic_helper parse_args v v v i conv_transpose d g jit_utils GraphContext input weight bias stride padding output_padding groups dilation _convolution g input weight bias stride padding dilation True output_padding groups None None None None _onnx_symbolic aten conv_transpose d symbolic_helper parse_args v v v i conv_transpose d g jit_utils GraphContext input weight bias stride padding output_padding groups dilation _convolution g input weight bias stride padding dilation True output_padding groups None None None None _onnx_symbolic aten conv_transpose d symbolic_helper parse_args v v v i conv_transpose d g jit_utils GraphContext input weight bias stride padding output_padding groups dilation _convolution g input weight bias stride padding dilation True output_padding groups None None None None _onnx_symbolic aten batch_norm symbolic_helper parse_args v v v v v i f f i batch_norm g jit_utils GraphContext input weight bias running_mean running_var training momentum eps cudnn_enabled symbolic_helper check_training_mode training batch_norm torch is_autocast_enabled symbolic_helper args_have_same_dtype input weight bias running_mean running_var GLOBALS export_onnx_opset_version symbolic_helper _onnx_opset_unsupported_detailed BatchNormalization All input tensors must have same ` dtype ` Turn off Autocast export using opset version input weight bias running_mean running_var = symbolic_helper _batchnorm_helper g input weight bias running_mean running_var out = g op BatchNormalization input weight bias running_mean running_var epsilon_f=eps momentum_f= - momentum outputs= training training out res new_running_mean new_running_var saved_mean saved_var = out new_running_mean setType running_mean type new_running_var setType running_var type saved_mean setDebugName batch_norm_dead_output- + saved_mean debugName saved_var setDebugName batch_norm_dead_output- + saved_var debugName res _onnx_symbolic aten native_layer_norm symbolic_helper quantized_args True False False False symbolic_helper parse_args v v v f native_layer_norm g jit_utils GraphContext input _C Value normalized_shape Sequence int weight _C Value bias _C Value eps float - tuple _C Value _C Value _C Value axes = -i i range len normalized_shape - two_cst = symbolic_helper _generate_wrapped_number g eps_cst = symbolic_helper _generate_wrapped_number g eps g opset mean = g op ReduceMean input axes_i=axes mean = g op ReduceMean input g op Constant value_t=torch tensor axes dtype=torch long numerator = sub g input mean Cast eps dtype avoid precision loss is_type_half = _type_utils JitScalarType from_value numerator == _type_utils JitScalarType HALF is_type_half eps_dtype = _type_utils JitScalarType from_value eps_cst numerator = g op Cast numerator to_i=_type_utils JitScalarType eps_dtype onnx_type variance = e x - e x ^ x - e x numerator layer_norm formula g opset pyrefly ignore no-matching-overload variance = g op ReduceMean pow g numerator two_cst axes_i=axes variance = g op ReduceMean pyrefly ignore no-matching-overload pow g numerator two_cst g op Constant value_t=torch tensor axes dtype=torch long denominator = sqrt g g op Add variance eps_cst normalized = g op Div numerator denominator Cast back input type eps related ops all done is_type_half input_dtype = _type_utils JitScalarType from_value input normalized = g op Cast normalized to_i=_type_utils JitScalarType input_dtype onnx_type weight None symbolic_helper _is_none weight normalized = mul g normalized weight bias None symbolic_helper _is_none bias normalized = add g normalized bias rdenominator = sqrt variance + eps According aten native_layer_norm rdenominator should have same dtype input mean normalized so we need Cast back is_type_half denominator = g op Cast denominator to_i=_type_utils JitScalarType input_dtype onnx_type type ignore possibly-undefined rdenominator = g op Reciprocal denominator rdenominator = reciprocal g denominator normalized mean rdenominator _onnx_symbolic aten layer_norm symbolic_helper quantized_args True False False False symbolic_helper parse_args v v v f b layer_norm g jit_utils GraphContext input _C Value normalized_shape Sequence int weight _C Value bias _C Value eps float cudnn_enable bool - _C Value normalized _ _ = native_layer_norm g input normalized_shape weight bias eps normalized _onnx_symbolic aten instance_norm symbolic_helper parse_args v v v v v b f f b instance_norm g jit_utils GraphContext input weight bias running_mean running_var use_input_stats bool momentum Number eps Number cudnn_enabled bool symbolic_helper check_training_mode use_input_stats instance_norm channel_size = symbolic_helper _get_tensor_dim_size input weight None symbolic_helper _is_none weight channel_size None raise errors SymbolicValueError Unsupported ONNX export instance_norm unknown channel size input weight_value = torch tensor channel_size dtype=_type_utils JitScalarType from_value input dtype weight = g op Constant value_t=weight_value bias None symbolic_helper _is_none bias channel_size None raise errors SymbolicValueError Unsupported ONNX export instance_norm unknown channel size input bias_value = torch tensor channel_size dtype=_type_utils JitScalarType from_value input dtype bias = g op Constant value_t=bias_value running_mean None symbolic_helper _is_none running_mean running_var None symbolic_helper _is_none running_var g op InstanceNormalization input weight bias epsilon_f=eps input_size = symbolic_helper _get_tensor_sizes input If input shape N C H W reshape N C H W call batch_norm For more information instance_norm https github com pytorch pytorch blob master aten src ATen native Normalization cpp#L input_size_reshape = input_size copy n = input_size n None raise errors SymbolicValueError Unsupported ONNX export instance_norm training unknown batch size input c = input_size input_size_reshape = input_size_reshape = n c weight_ = repeat g weight g op Constant value_t=torch tensor n dtype=torch int bias_ = repeat g bias g op Constant value_t=torch tensor n dtype=torch int running_mean_ = repeat g running_mean g op Constant value_t=torch tensor n dtype=torch int running_var_ = repeat g running_var g op Constant value_t=torch tensor n dtype=torch int input_reshaped = g op Reshape input g op Constant value_t=torch LongTensor input_size_reshape out = batch_norm g input_reshaped weight_ bias_ running_mean_ running_var_ use_input_stats momentum eps cudnn_enabled view g out g op Constant value_t=torch tensor input_size _onnx_symbolic aten unfold symbolic_helper parse_args v i i i unfold g jit_utils GraphContext input dimension size step sizes = symbolic_helper _get_tensor_sizes input FIXME justinchuby Get rid try catch here improve readability try sizedim = sizes dimension except Exception FIXME justinchuby Avoid catching Exception Catch more specific exception instead sizedim = None sizedim None low_indices = range sizedim step hi_indices = range size sizedim + step stack = symbolic_helper _slice_helper g input axes= dimension starts= low ends= hi low hi zip low_indices hi_indices ndim = len sizes perm = list range ndim perm append perm pop dimension unsqueeze = symbolic_helper _unsqueeze_helper g g op Transpose t perm_i=perm dimension t stack g op Concat unsqueeze axis_i=dimension symbolic_helper _unimplemented Unfold input size accessible input _onnx_symbolic aten elu symbolic_helper quantized_args True symbolic_helper parse_args v t t t elu g jit_utils GraphContext input alpha scale input_scale scale scale = symbolic_helper _unimplemented scale does support scale Elu scale input_scale input_scale = symbolic_helper _unimplemented input_scale does support input_scale Elu input_scale See Note Export inplace g op Elu input alpha_f=symbolic_helper _scalar alpha _onnx_symbolic aten selu symbolic_helper quantized_args True selu g jit_utils GraphContext input g op Selu input _onnx_symbolic aten index_select symbolic_helper parse_args v i v index_select g jit_utils GraphContext dim index In case scalar index index_select returns tensor same rank input To match behavior ONNX we make index D tensor so following gather also produces tensor same rank input symbolic_helper _select_helper g dim index _onnx_symbolic aten index_put index_put g jit_utils GraphContext indices_list_value values accumulate symbolic_helper _is_packed_list indices_list_value indices_list = symbolic_helper _unpack_list indices_list_value indices_list = indices_list_value accumulate = symbolic_helper _parse_arg accumulate b len indices_list == accumulate add g values values symbolic_helper _onnx_opset_unsupported index_put _onnx_symbolic aten index_fill index_fill g jit_utils GraphContext dim index value expanded_index_shape expanded_index = symbolic_helper _index_fill_reshape_helper g dim index value = symbolic_helper _maybe_get_scalar value value = symbolic_helper _if_scalar_type_as value expanded_value = expand g value expanded_index_shape None scatter g dim expanded_index expanded_value _onnx_symbolic aten index_copy index_copy g jit_utils GraphContext dim index source _expanded_index_shape expanded_index = symbolic_helper _index_fill_reshape_helper g dim index scatter g dim expanded_index source _onnx_symbolic aten bucketize symbolic_helper parse_args v v b b bucketize g jit_utils GraphContext boundaries out_int =False right=False out_type = _C_onnx TensorProtoDataType INT out_int out_type = _C_onnx TensorProtoDataType INT A tensor expanded_boundaries created such contains copy boundaries each element new_shape = g op Concat g op Shape boundaries g op Shape axis_i= Unsqueeze step performed respect ONNX s numpy style broadcasting comparison ops https github com onnx onnx blob main docs Broadcasting md tensor_rank = symbolic_helper _get_tensor_rank assert tensor_rank None unsqueeze_axes = list range tensor_rank + expanded_boundaries = expand g symbolic_helper _unsqueeze_helper g boundaries unsqueeze_axes new_shape None Compare each element boundaries get tensor leading s trailing s e g = The index last bucket where element should go right cond = ge g expanded_boundaries cond = gt g expanded_boundaries cond_out = g op Cast cond to_i=out_type Sum get number s corresponding each element which same bucket index e g sum = sum = symbolic_helper _reducesum_helper g cond_out axes_i= keepdims_i= _onnx_symbolic aten type_as type_as g jit_utils GraphContext other self_dtype = symbolic_helper _try_get_scalar_type other_dtype = symbolic_helper _try_get_scalar_type other self_dtype == other_dtype self_dtype None other_dtype None g op Cast to_i=other_dtype onnx_type raise errors SymbolicValueError Unsupported ONNX export type_as tensor unknown dtype Please check dtype parameter passed type_as function correct other _onnx_symbolic aten cosine_similarity symbolic_helper parse_args v v i f cosine_similarity g jit_utils GraphContext x x dim eps cross = symbolic_helper _reducesum_helper g mul g x x axes_i= dim keepdims_i= x _l = symbolic_helper _reducesum_helper g mul g x x axes_i= dim keepdims_i= x _l = symbolic_helper _reducesum_helper g mul g x x axes_i= dim keepdims_i= div_tens = max g sqrt g mul g x _l x _l g op Constant value_t=torch tensor eps div g cross div_tens _onnx_symbolic aten pairwise_distance pairwise_distance g jit_utils GraphContext input input p eps keepdim symbolic_helper _is_value eps eps = g op Constant value_t=torch tensor eps inv_p = div g g op Constant value_t=torch tensor dtype=torch float add g p eps summation = symbolic_helper _reducesum_helper g pyrefly ignore no-matching-overload pow g sub g input input p axes_i= - keepdims_i=symbolic_helper _parse_arg keepdim i pyrefly ignore no-matching-overload pow g summation inv_p _onnx_symbolic aten clone ignore clone operators inserted PyTorch autograd clone g jit_utils GraphContext input unused_memory_format input _onnx_symbolic aten abs abs g jit_utils GraphContext g op Abs _onnx_symbolic aten log log g jit_utils GraphContext g op Log _onnx_symbolic aten log p log p g jit_utils GraphContext log g add g symbolic_helper _if_scalar_type_as torch ones _onnx_symbolic aten log log g jit_utils GraphContext _ln = g op Div log g g op Constant value_t=torch tensor _ln _onnx_symbolic aten pow pow g jit_utils GraphContext exponent f_dtype = _type_utils JitScalarType from_value symbolic_helper _is_fp f_dtype = _type_utils JitScalarType FLOAT = g op Cast to_i=f_dtype onnx_type symbolic_helper _is_fp exponent exponent = g op Cast exponent to_i=f_dtype onnx_type pow = g op Pow exponent pow _onnx_symbolic aten clamp clamp g jit_utils GraphContext min max min max may None we need dispatch Clip separately ONNX does have None syntax symbolic_helper _is_none min clamp_max g max symbolic_helper _is_none max clamp_min g min symbolic_helper _is_constant min symbolic_helper _is_constant max symbolic_helper _op_with_optional_float_cast g Clip min_f=symbolic_helper _parse_arg min f max_f=symbolic_helper _parse_arg max f opset_before= clamp_max g clamp_min g min max _onnx_symbolic aten clamp_min symbolic_helper parse_args v v clamp_min g jit_utils GraphContext min symbolic_helper _is_constant min symbolic_helper _op_with_optional_float_cast g Clip min_f=symbolic_helper _parse_arg min f opset_before= dtype = _type_utils JitScalarType from_value min = g op Cast min to_i=dtype onnx_type symbolic_helper _op_with_optional_float_cast g Max min opset_before= _onnx_symbolic aten clamp_max symbolic_helper parse_args v v clamp_max g jit_utils GraphContext max symbolic_helper _is_constant max symbolic_helper _op_with_optional_float_cast g Clip max_f=symbolic_helper _parse_arg max f opset_before= dtype = _type_utils JitScalarType from_value max = g op Cast max to_i=dtype onnx_type symbolic_helper _op_with_optional_float_cast g Min max opset_before= _onnx_symbolic aten max torch max same torch min actually has two interfaces smashed together torch max x dim keepdim torch max x y TODO justinchuby Support multiple quantized args output max g jit_utils GraphContext dim_or_y=None keepdim=None symbolic_helper _max_helper g dim_or_y keepdim _onnx_symbolic aten maximum symbolic_helper quantized_args True True maximum g jit_utils GraphContext input other pyrefly ignore no-matching-overload max g input dim_or_y=other _onnx_symbolic aten min TODO justinchuby Support multiple quantized args output min g jit_utils GraphContext dim_or_y=None keepdim=None symbolic_helper _min_helper g dim_or_y keepdim _onnx_symbolic aten minimum symbolic_helper quantized_args True True minimum g jit_utils GraphContext input other pyrefly ignore no-matching-overload min g input dim_or_y=other _onnx_symbolic aten amax symbolic_helper quantized_args True symbolic_helper parse_args v i amax g jit_utils GraphContext dim keepdim g op ReduceMax axes_i=dim keepdims_i=keepdim _onnx_symbolic aten amin symbolic_helper quantized_args True symbolic_helper parse_args v i amin g jit_utils GraphContext dim keepdim g op ReduceMin axes_i=dim keepdims_i=keepdim _onnx_symbolic aten aminmax symbolic_helper quantized_args True symbolic_helper parse_args v v i aminmax g jit_utils GraphContext dim keepdim reduce_kwargs = keepdims_i keepdim symbolic_helper _is_none dim dim = symbolic_helper _get_const dim i dim reduce_kwargs axes_i = dim g op ReduceMin reduce_kwargs g op ReduceMax reduce_kwargs _onnx_symbolic aten exp exp g jit_utils GraphContext g op Exp _onnx_symbolic aten dropout_ _onnx_symbolic aten dropout symbolic_helper parse_args v f i dropout g jit_utils GraphContext input p train symbolic_helper check_training_mode train dropout train False dropout no-op train input r _ = g op Dropout input ratio_f=p outputs= r _onnx_symbolic aten alpha_dropout_ decorate= symbolic_helper _apply_params aten alpha_dropout_ See Note Export inplace _onnx_symbolic aten feature_alpha_dropout_ decorate= symbolic_helper _apply_params aten feature_alpha_dropout_ _onnx_symbolic aten feature_dropout_ decorate= symbolic_helper _apply_params aten feature_dropout_ _onnx_symbolic aten feature_alpha_dropout decorate= symbolic_helper _apply_params aten feature_alpha_dropout _onnx_symbolic aten alpha_dropout decorate= symbolic_helper _apply_params aten alpha_dropout _onnx_symbolic aten feature_dropout decorate= symbolic_helper _apply_params aten feature_dropout _unsupported_dropout name str symbolic_helper parse_args v none b feature_dropout g input p train NB In inference mode FeatureDropout exported identity op train symbolic_helper _unimplemented name training mode input input feature_dropout _onnx_symbolic aten norm symbolic_helper parse_args v t i v norm g jit_utils GraphContext p dim keepdim dtype=None p == f = symbolic_helper _reduce_op_symbolic_helper ReduceL p == f = symbolic_helper _reduce_op_symbolic_helper ReduceL raise errors SymbolicValueError ONNX export only p-norms p result = f g dim=dim keepdim=keepdim dtype None dtype = symbolic_helper _get_const dtype i dtype result = g op Cast result to_i=_type_utils JitScalarType dtype onnx_type result _onnx_symbolic aten conv_tbc symbolic_helper parse_args v v v i conv_tbc g jit_utils GraphContext input weight bias pad input must have dimensions see https github com pytorch pytorch blob master aten src ATen native ConvolutionTBC cpp#L -L input = time batch in_channels weight = kernel_width in_channels out_channels bias = out_channels input = g op Transpose input perm_i= weight = g op Transpose weight perm_i= conv = conv d g input weight bias pad g op Transpose conv perm_i= _onnx_symbolic aten _unique symbolic_helper parse_args v i i _unique g jit_utils GraphContext input sorted return_inverse symbolic_helper _onnx_unsupported _unique input _onnx_symbolic aten _unique symbolic_helper parse_args v i i i _unique g jit_utils GraphContext input sorted return_inverse return_counts symbolic_helper _onnx_opset_unsupported _unique input _onnx_symbolic aten _cast_Byte deprecated Avoid using function create Cast node instead _cast_Byte g jit_utils GraphContext input non_blocking g op Cast input to_i=_C_onnx TensorProtoDataType UINT _onnx_symbolic aten _cast_Char deprecated Avoid using function create Cast node instead _cast_Char g jit_utils GraphContext input non_blocking g op Cast input to_i=_C_onnx TensorProtoDataType INT _onnx_symbolic aten _cast_Short deprecated Avoid using function create Cast node instead _cast_Short g jit_utils GraphContext input non_blocking g op Cast input to_i=_C_onnx TensorProtoDataType INT _onnx_symbolic aten _cast_Int deprecated Avoid using function create Cast node instead _cast_Int g jit_utils GraphContext input non_blocking g op Cast input to_i=_C_onnx TensorProtoDataType INT _onnx_symbolic aten _cast_Long deprecated Avoid using function create Cast node instead _cast_Long g jit_utils GraphContext input non_blocking g op Cast input to_i=_C_onnx TensorProtoDataType INT _onnx_symbolic aten _cast_Half deprecated Avoid using function create Cast node instead _cast_Half g jit_utils GraphContext input non_blocking g op Cast input to_i=_C_onnx TensorProtoDataType FLOAT _onnx_symbolic aten _cast_Float deprecated Avoid using function create Cast node instead _cast_Float g jit_utils GraphContext input non_blocking g op Cast input to_i=_C_onnx TensorProtoDataType FLOAT _onnx_symbolic aten _cast_Double deprecated Avoid using function create Cast node instead _cast_Double g jit_utils GraphContext input non_blocking g op Cast input to_i=_C_onnx TensorProtoDataType DOUBLE _onnx_symbolic aten _cast_Bool deprecated Avoid using function create Cast node instead _cast_Bool g jit_utils GraphContext input non_blocking g op Cast input to_i=_C_onnx TensorProtoDataType BOOL _onnx_symbolic aten empty symbolic_helper parse_args v i v v v v empty g jit_utils GraphContext sizes dtype layout device pin_memory=False memory_format=None zeros g sizes dtype layout device pin_memory _onnx_symbolic aten empty_like symbolic_helper parse_args v i v v v v empty_like g jit_utils GraphContext input dtype=None layout=None device=None pin_memory=False memory_format=None zeros_like g input dtype layout device pin_memory _onnx_symbolic aten new_empty new_empty g jit_utils GraphContext sizes dtype layout device pin_memory=False self_dtype = symbolic_helper _try_get_scalar_type symbolic_helper _is_none dtype self_dtype None dtype = self_dtype empty g sizes dtype layout device pin_memory _onnx_symbolic aten scalar_tensor scalar_tensor g jit_utils GraphContext scalar dtype options dtype = symbolic_helper _get_const dtype i dtype dtype None dtype = _type_utils JitScalarType FLOAT scalar = g op Cast scalar to_i=_type_utils JitScalarType dtype onnx_type scalar _onnx_symbolic aten tensor tensor g jit_utils GraphContext data dtype=None device=None requires_grad=False dtype = symbolic_helper _get_const dtype i dtype symbolic_helper _is_packed_list data dtype None dtype = _type_utils JitScalarType from_value symbolic_helper _unpack_list data input_list = t symbolic_helper _unpack_list data shape_reference = g op Constant value_t=torch LongTensor t = symbolic_helper _reshape_helper g t shape_reference t = g op Cast t to_i=_type_utils JitScalarType dtype onnx_type input_list append t g op Concat input_list axis_i= dtype None dtype = _type_utils JitScalarType from_value data symbolic_helper _is_list data symbolic_helper _is_tensor_list data symbolic_helper _is_scalar_list data data = g op ConcatFromSequence data axis_i= new_axis_i= g op Cast data to_i=_type_utils JitScalarType dtype onnx_type _onnx_symbolic aten as_tensor as_tensor g jit_utils GraphContext data dtype=None device=None tensor g data dtype device _onnx_symbolic aten zeros symbolic_helper parse_args v i v v v zeros g jit_utils GraphContext sizes dtype layout device pin_memory=False NOTE no way set device layout pin_memory ONNX so we ignore dtype None scalar_type = _type_utils JitScalarType FLOAT scalar_type = _type_utils JitScalarType dtype sizes_ = symbolic_helper _maybe_get_const sizes isinstance sizes_ list len sizes_ == sizes = g op Constant value_t=torch tensor torch int g op ConstantOfShape sizes value_t=torch tensor dtype=scalar_type dtype _onnx_symbolic aten zeros_like symbolic_helper parse_args v i v v v v zeros_like g jit_utils GraphContext input dtype=None layout=None device=None pin_memory=False memory_format=None shape = g op Shape input symbolic_helper _is_none dtype scalar_type = _type_utils JitScalarType from_value input _type_utils JitScalarType FLOAT pyrefly ignore bad-argument-type scalar_type = _type_utils JitScalarType dtype g op ConstantOfShape shape value_t=torch tensor dtype=scalar_type dtype _onnx_symbolic aten new_zeros new_zeros g jit_utils GraphContext sizes dtype layout device pin_memory=False self_dtype = symbolic_helper _try_get_scalar_type symbolic_helper _is_none dtype self_dtype None dtype = self_dtype zeros g sizes dtype layout device pin_memory _onnx_symbolic aten zero zero g jit_utils GraphContext self_dtype = symbolic_helper _try_get_scalar_type zeros_like g self_dtype _onnx_symbolic aten ones symbolic_helper parse_args v i v v v ones g jit_utils GraphContext sizes dtype layout device pin_memory=False dtype None scalar_type = _type_utils JitScalarType FLOAT scalar_type = _type_utils JitScalarType dtype sizes_ = symbolic_helper _maybe_get_const sizes isinstance sizes_ list len sizes_ == sizes = g op Constant value_t=torch tensor torch int g op ConstantOfShape sizes value_t=torch tensor dtype=scalar_type dtype _onnx_symbolic aten ones_like symbolic_helper parse_args v i v v v v ones_like g jit_utils GraphContext input dtype=None layout=None device=None pin_memory=False memory_format=None shape = g op Shape input symbolic_helper _is_none dtype scalar_type = _type_utils JitScalarType from_value input _type_utils JitScalarType FLOAT pyrefly ignore bad-argument-type scalar_type = _type_utils JitScalarType dtype g op ConstantOfShape shape value_t=torch tensor dtype=scalar_type dtype _onnx_symbolic aten new_ones new_ones g jit_utils GraphContext sizes dtype layout device pin_memory=False self_dtype = symbolic_helper _try_get_scalar_type symbolic_helper _is_none dtype self_dtype None dtype = self_dtype ones g sizes dtype layout device pin_memory _onnx_symbolic aten full full g jit_utils GraphContext sizes value dtype layout device pin_memory=False const_value = symbolic_helper _maybe_get_const value t symbolic_helper _is_value const_value dtype = _type_utils JitScalarType FLOAT dtype None dtype tmp = zeros g sizes dtype layout device add g tmp value g op Constant value_t=torch tensor dtype = symbolic_helper _get_const dtype i dtype dtype None scalar_type = _type_utils JitScalarType FLOAT scalar_type = _type_utils JitScalarType dtype sizes_ = symbolic_helper _maybe_get_const sizes isinstance sizes_ list len sizes_ == sizes = g op Constant value_t=torch tensor torch int g op ConstantOfShape sizes value_t=const_value view scalar_type dtype _onnx_symbolic aten full_like full_like g jit_utils GraphContext input fill_value dtype=None layout=None device=None pin_memory=False memory_format=None fill_value = symbolic_helper _maybe_get_const fill_value f dtype = symbolic_helper _get_const dtype i dtype dtype None scalar_type = _type_utils JitScalarType from_value input _type_utils JitScalarType FLOAT scalar_type = _type_utils JitScalarType dtype symbolic_helper _is_value fill_value tmp = zeros_like g input dtype layout device fill_value = g op Cast fill_value to_i=scalar_type onnx_type add g tmp fill_value g op Constant value_t=torch tensor shape = g op Shape input g op ConstantOfShape shape value_t=torch tensor fill_value dtype=scalar_type dtype _onnx_symbolic aten new_full new_full g jit_utils GraphContext size fill_value dtype layout device pin_memory=False self_dtype = symbolic_helper _try_get_scalar_type symbolic_helper _is_none dtype self_dtype None dtype = self_dtype full g size fill_value dtype layout device pin_memory _onnx_symbolic aten eye eye g jit_utils GraphContext args len args == aten eye n dtype layout device pin_memory n dtype layout device _pin_memory = args dim_size = symbolic_helper _unsqueeze_helper g n shape = g op Concat dim_size dim_size axis_i= tensor = zeros g shape dtype layout device g op EyeLike tensor len args == aten eye n m dtype layout device pin_memory n m dtype layout device _pin_memory = args shape = g op Concat symbolic_helper _unsqueeze_helper g n symbolic_helper _unsqueeze_helper g m axis_i= tensor = zeros g shape dtype layout device g op EyeLike tensor symbolic_helper _unimplemented aten eye f len args arguments _onnx_symbolic aten slice slice g jit_utils GraphContext args len args == aten slice Tensor int dim int start int end int step - Tensor dim start end step = args step = symbolic_helper _parse_arg step i step = raise errors SymbolicValueError step = currently supported is_start_none = start node kind == prim Constant isinstance start type _C NoneType is_end_none = end node kind == prim Constant isinstance end type _C NoneType is_start_onnx_const = start node kind == onnx Constant is_end_onnx_const = end node kind == onnx Constant is_start_none is_start_onnx_const is_end_none is_end_onnx_const dim node kind = onnx Constant GLOBALS operator_export_type == _C_onnx OperatorExportTypes ONNX raise errors SymbolicValueError Unsupported ONNX export Slice dynamic inputs DynamicSlice deprecated experimental op Please use statically allocated variables export higher opset version start_unsqueezed = symbolic_helper _unsqueeze_helper g start end_unsqueezed = symbolic_helper _unsqueeze_helper g end dim_unsqueezed = symbolic_helper _unsqueeze_helper g dim g op DynamicSlice start_unsqueezed end_unsqueezed dim_unsqueezed start = is_start_none symbolic_helper _parse_arg start i end = _constants INT _MAX is_end_none symbolic_helper _parse_arg end i dim = symbolic_helper _parse_arg dim i symbolic_helper _slice_helper g axes= dim starts= start ends= end len args == aten slice t l int start int end int step - t start end step = args dim = is_start_none = start node kind == prim Constant isinstance start type _C NoneType is_end_none = end node kind == prim Constant isinstance end type _C NoneType start = is_start_none symbolic_helper _parse_arg start i end = _constants INT _MAX is_end_none symbolic_helper _parse_arg end i symbolic_helper _slice_helper g axes= dim starts= start ends= end symbolic_helper _unimplemented aten slice f len args arguments _onnx_symbolic aten hardtanh symbolic_helper quantized_args True symbolic_helper parse_args v f f hardtanh g jit_utils GraphContext _C Value min_val float max_val float symbolic_helper _op_with_optional_float_cast g Clip min_f=min_val max_f=max_val opset_before= _onnx_symbolic aten hardswish symbolic_helper quantized_args True symbolic_helper parse_args v hardswish g jit_utils GraphContext hs = hardsigmoid g g op Mul hs _onnx_symbolic aten hardsigmoid Fixed scale zero_point discovered aten src ATen native quantized cpu qhardsigmoid cpp symbolic_helper quantized_args True scale= zero_point= symbolic_helper parse_args v hardsigmoid g jit_utils GraphContext Set alpha_f make op equivalent PyTorch s definition Hardsigmoid See https pytorch org docs stable generated torch nn Hardsigmoid html g op HardSigmoid alpha_f= _onnx_symbolic aten tanhshrink symbolic_helper parse_args v tanhshrink g jit_utils GraphContext g op Sub tanh g _onnx_symbolic aten hardshrink symbolic_helper parse_args v f hardshrink g jit_utils GraphContext lambd scalar_type = _type_utils JitScalarType from_value _type_utils JitScalarType FLOAT lambd_op = g op Constant value_t=torch tensor lambd dtype=scalar_type dtype cond = logical_or g gt g lambd_op lt g neg g lambd_op g op Where cond g op Constant value_t=torch tensor dtype=scalar_type dtype _onnx_symbolic aten softshrink symbolic_helper parse_args v f softshrink g jit_utils GraphContext lambd scalar_type = _type_utils JitScalarType from_value _type_utils JitScalarType FLOAT lambd_op = g op Constant value_t=torch tensor lambd dtype=scalar_type dtype gt_cond = gt g lambd_op gt_out = g op Where gt_cond sub g lambd_op g op Constant value_t=torch tensor dtype=scalar_type dtype lt_cond = lt g neg g lambd_op lt_out = g op Where lt_cond add g lambd_op g op Constant value_t=torch tensor dtype=scalar_type dtype add g gt_out lt_out _onnx_symbolic aten alias alias g jit_utils GraphContext _onnx_symbolic aten unsqueeze symbolic_helper parse_args v i unsqueeze g jit_utils GraphContext dim Implement unsqueezing pytorch tensor ONNX inserting new dimension specified ` dim ` Handle negative dim dim rank = symbolic_helper _get_tensor_rank rank None warnings warn ONNX export unsqueeze negative axis + str dim + might cause onnx model incorrect + Negative axis supported ONNX + Axis converted + str dim + rank + + based input shape export time + Passing tensor different rank execution will incorrect stacklevel= dim = dim + rank + symbolic_helper _unimplemented unsqueeze negative axis unknown input rank symbolic_helper _unsqueeze_helper g axes_i= dim _onnx_symbolic aten sort TODO justinchuby Support multiple quantized args output symbolic_helper parse_args v i i none sort g jit_utils GraphContext dim descending out=None out None symbolic_helper _unimplemented Sort Out parameter supported sort self_sizes = symbolic_helper _get_tensor_sizes try dim_size = self_sizes dim except Exception FIXME justinchuby Avoid catching Exception Catch more specific exception instead dim_size = None dim_size None symbolic_helper _unimplemented Sort input size accessible g op TopK k_i=dim_size axis_i=dim outputs= _onnx_symbolic aten numel numel g jit_utils GraphContext symbolic_helper _numel_helper g _onnx_symbolic aten topk TODO justinchuby Support multiple quantized args output symbolic_helper parse_args v i i i i none topk g jit_utils GraphContext k dim largest sorted out=None out None symbolic_helper _unimplemented TopK Out parameter supported topk largest symbolic_helper _unimplemented TopK Ascending TopK supported g op TopK k_i=k axis_i=dim outputs= _onnx_symbolic prim convert_element_type convert_element_type g jit_utils GraphContext args dtype = symbolic_helper _get_const args i dtype g op Cast to_i=_type_utils JitScalarType dtype onnx_type _onnx_symbolic aten g jit_utils GraphContext args is_aten_to_device_only args len args == aten Tensor Device bool bool memory_format args node kind == prim device args type isSubtypeOf _C ListType ofInts isinstance args type _C DeviceObjType len args == aten Tensor Device ScalarType bool bool memory_format When dtype None aten device call dtype = symbolic_helper _get_const args i dtype dtype None len args aten Tensor ScalarType Layout Device bool bool memory_format - Tensor aten Tensor ScalarType Layout Device bool bool bool memory_format - Tensor When dtype None aten device call dtype = symbolic_helper _get_const args i dtype dtype None False ONNX doesn t have concept device so we ignore device-only casts is_aten_to_device_only args len args == TestONNXRuntime test_ones_bool shows args aten can onnx Constant value= Tensor In case constant value tensor int so symbolic_helper _maybe_get_const args i would work dtype = args symbolic_helper _is_value args args node kind == onnx Constant tval = symbolic_helper _node_get args node value isinstance tval torch Tensor len tval shape == tval = tval item dtype = int tval dtype = tval symbolic_helper _is_value dtype isinstance dtype torch Tensor aten Tensor Tensor bool bool memory_format dtype = _type_utils JitScalarType from_value args g op Cast to_i=dtype onnx_type aten Tensor ScalarType bool bool memory_format memory_format ignored g op Cast to_i=_type_utils JitScalarType dtype onnx_type len args == aten Tensor Device ScalarType bool bool memory_format dtype = symbolic_helper _get_const args i dtype memory_format ignored g op Cast to_i=_type_utils JitScalarType dtype onnx_type len args == aten Tensor ScalarType Layout Device bool bool memory_format - Tensor dtype = symbolic_helper _get_const args i dtype Layout device memory_format ignored g op Cast to_i=_type_utils JitScalarType dtype onnx_type len args == aten Tensor ScalarType Layout Device bool bool bool memory_format - Tensor dtype = symbolic_helper _get_const args i dtype Layout device memory_format ignored g op Cast to_i=_type_utils JitScalarType dtype onnx_type symbolic_helper _onnx_unsupported Unknown aten signature _onnx_symbolic aten repeat repeat g jit_utils GraphContext repeats dtype = _type_utils JitScalarType INT shape_ = ones_like g repeats dtype = g op Expand shape_ g op Tile repeats _onnx_symbolic aten repeat_interleave repeat_interleave g jit_utils GraphContext repeats dim=None output_size=None repeats_dim = symbolic_helper _get_tensor_rank repeats repeats_sizes = symbolic_helper _get_tensor_sizes repeats input_sizes = symbolic_helper _get_tensor_sizes repeats_dim None raise errors SymbolicValueError Unsupported ONNX export repeat_interleave unknown repeats rank repeats_sizes None raise errors SymbolicValueError Unsupported ONNX export repeat_interleave unknown repeats size input_sizes None raise errors SymbolicValueError Unsupported ONNX export repeat_interleave unknown input size dim None flatten By default use flattened input array flat output array symbolic_helper _is_none dim = symbolic_helper _reshape_helper g g op Constant value_t=torch tensor - dim = torch tensor dtype=torch int dim = symbolic_helper _maybe_get_scalar dim Handle cases where dim negative dim dim += len input_sizes input_sizes_temp = input_sizes copy idx input_size enumerate input_sizes input_size None input_sizes idx input_sizes_temp idx = - Cases where repeats int single value tensor repeats_dim == repeats_dim == repeats_sizes == input_sizes dim == symbolic_helper _onnx_opset_unsupported_detailed repeat_interleave Unsupported along dimension unknown input size symbolic_helper _repeat_interleave_single_value_repeat_helper g repeats dim Cases where repeats dim Tensor repeats_dim == input_sizes dim == symbolic_helper _onnx_opset_unsupported_detailed repeat_interleave Unsupported along dimension unknown input size repeats_sizes None symbolic_helper _onnx_opset_unsupported_detailed repeat_interleave Unsupported cases dynamic repeats assert repeats_sizes == input_sizes dim repeats must have same size input along dim reps = repeats_sizes raise errors SymbolicValueError repeats must -dim -dim tensor final_splits = r_splits = symbolic_helper _repeat_interleave_split_helper g repeats reps i_splits = symbolic_helper _repeat_interleave_split_helper g reps dim input_sizes dim input_sizes_temp dim = - idx r_split enumerate r_splits i_split = unsqueeze g i_splits idx dim + r_concat = g op Constant value_t=torch LongTensor input_sizes_temp dim + r_split g op Constant value_t=torch LongTensor input_sizes_temp dim + r_concat = g op Concat r_concat axis_i= i_split = expand g i_split r_concat None i_split = symbolic_helper _reshape_helper g i_split g op Constant value_t=torch LongTensor input_sizes allowzero= final_splits append i_split g op Concat final_splits axis_i=dim _onnx_symbolic aten pixel_shuffle symbolic_helper parse_args v i pixel_shuffle g jit_utils GraphContext upscale_factor dims = symbolic_helper _get_tensor_sizes len dims = symbolic_helper _unimplemented pixel_shuffle only support d input any i None i dims after_view = symbolic_helper _reshape_helper g symbolic_helper _unsqueeze_helper g g op Constant value_t=torch tensor - upscale_factor upscale_factor allowzero= after_transpose = g op Transpose after_view perm_i= For dynamic input shapes two reshapes performed reshape_h = symbolic_helper _reshape_helper g after_transpose g op Constant value_t=torch tensor - allowzero= reshape_w = symbolic_helper _reshape_helper g reshape_h g op Constant value_t=torch tensor - allowzero= symbolic_helper _squeeze_helper g reshape_w output_channel = dims upscale_factor upscale_factor after_view = symbolic_helper _reshape_helper g g op Constant value_t=torch tensor - output_channel upscale_factor upscale_factor dims dims allowzero= after_transpose = g op Transpose after_view perm_i= symbolic_helper _reshape_helper g after_transpose g op Constant value_t=torch tensor - output_channel dims upscale_factor dims upscale_factor allowzero= _onnx_symbolic aten pixel_unshuffle symbolic_helper parse_args v i pixel_unshuffle g jit_utils GraphContext downscale_factor dims = symbolic_helper _get_tensor_sizes len dims = symbolic_helper _unimplemented pixel_shuffle only support d input any i None i dims For dynamic input shapes two reshapes performed reshape_h = symbolic_helper _reshape_helper g symbolic_helper _unsqueeze_helper g g op Constant value_t=torch tensor - downscale_factor allowzero= reshape_w = symbolic_helper _reshape_helper g reshape_h g op Constant value_t=torch tensor - downscale_factor allowzero= after_transpose = g op Transpose reshape_w perm_i= final_reshape = symbolic_helper _reshape_helper g after_transpose g op Constant value_t=torch tensor - allowzero= symbolic_helper _squeeze_helper g final_reshape output_channel = dims downscale_factor downscale_factor after_view = symbolic_helper _reshape_helper g g op Constant value_t=torch tensor - dims dims downscale_factor downscale_factor dims downscale_factor downscale_factor allowzero= after_transpose = g op Transpose after_view perm_i= symbolic_helper _reshape_helper g after_transpose g op Constant value_t=torch tensor - output_channel dims downscale_factor dims downscale_factor allowzero= _generic_rnn g jit_utils GraphContext variant input initial_states all_weights has_biases num_layers dropout train bidirectional batch_first=None batch_sizes=None warnings warn Exporting model ONNX batch_size other than + variable length + variant + can cause error + when running ONNX model different batch size + Make sure save model batch size + define initial states h c inputs model stacklevel= onnxActivations = Relu Tanh Sigmoid Affine LeakyRelu ThresholdedRelu ScaledTanh HardSigmoid Elu Softsign Softplus variantToOnnxActivationMap = dict zip act_fun lower act_fun onnxActivations onnxActivations weights_per_layer = has_biases means projections used inside LSTM so need tell user s supported variant == LSTM len all_weights = num_layers weights_per_layer + bidirectional symbolic_helper _unimplemented LSTM LSTMs projections input assert len all_weights == num_layers weights_per_layer + bidirectional layer_weights = all_weights i i + weights_per_layer i range len all_weights weights_per_layer batch_first batch seq feat - seq batch feat input = g op Transpose input perm_i= dropout train symbolic_helper _unimplemented RNN GRU LSTM dropout training mode input variant startswith RNN nonlinearity = variantToOnnxActivationMap variant lower variant = RNN w_hh = all_weights hidden_size = symbolic_helper _get_tensor_dim_size w_hh hidden_size None symbolic_helper _unimplemented RNN GRU LSTM unknown hidden size input unidirectional = bidirectional prev_output = input h_outs = variant == RNN variant == GRU h = initial_states variant == LSTM h c = initial_states c_outs = sequence_lens = unused g batch_sizes None batch_sizes variant == GRU pytorch reset input hidden onnx input reset hidden reform_permutation = variant == LSTM pytorch input forget cell output onnx input output forget cell reform_permutation = reform_weights g w n intervals slices = symbolic_helper _slice_helper g w axes= starts= x n ends= y n x y intervals g op Concat slices axis_i= transform_weights_no_bias layer_index weights = layer_weights layer_index variant == RNN weight_ih weight_hh = weights variant == GRU variant == LSTM weight_ih weight_hh = reform_weights g w hidden_size reform_permutation w weights tuple symbolic_helper _unsqueeze_helper g x x weight_ih weight_hh type ignore possibly-undefined transform_weights layer_index weights = layer_weights layer_index variant == RNN weight_ih weight_hh bias_ih bias_hh = weights variant == GRU variant == LSTM weight_ih weight_hh bias_ih bias_hh = reform_weights g w hidden_size reform_permutation w weights bias_concat = g op Concat bias_ih bias_hh axis_i= type ignore possibly-undefined tuple symbolic_helper _unsqueeze_helper g x x weight_ih weight_hh bias_concat type ignore possibly-undefined retrieve_state x start end x num_layers == symbolic_helper _slice_helper g x axes= starts= start ends= end i range num_layers unidirectional weights_per_layer == weight_ih weight_hh bias_concat = transform_weights i weight_ih weight_hh = transform_weights_no_bias i bias_concat = unused g state_indices = i i + weights_per_layer == weight_ih_f weight_hh_f bias_f = transform_weights i weight_ih_b weight_hh_b bias_b = transform_weights i + bias_concat = g op Concat bias_f bias_b axis_i= weight_ih_f weight_hh_f = transform_weights_no_bias i weight_ih_b weight_hh_b = transform_weights_no_bias i + bias_concat = unused g weight_ih = g op Concat weight_ih_f weight_ih_b axis_i= weight_hh = g op Concat weight_hh_f weight_hh_b axis_i= state_indices = i i + inputs = prev_output weight_ih weight_hh bias_concat sequence_lens inputs append retrieve_state h state_indices type ignore possibly-undefined variant == LSTM inputs append retrieve_state c state_indices type ignore possibly-undefined extra_kwargs = unidirectional direction_s bidirectional variant == RNN bidirectional activation = nonlinearity nonlinearity type ignore possibly-undefined activation = nonlinearity type ignore possibly-undefined prev_output h_out = g op RNN inputs outputs= hidden_size_i=hidden_size activations_s=activation extra_kwargs variant == GRU prev_output h_out = g op GRU inputs outputs= hidden_size_i=hidden_size linear_before_reset_i= extra_kwargs variant == LSTM prev_output h_out c_out = g op LSTM inputs outputs= hidden_size_i=hidden_size extra_kwargs bidirectional The ONNX RNN GRU LSTM produce output dimensions seq_len num_directions batch hidden_size We have convert match pytorch s expected seq_len batch num_directions hidden_size first moving num_directions before hidden_size Transpose then combining hidden_size Reshape prev_output = g op Transpose prev_output perm_i= prev_output = symbolic_helper _reshape_helper g prev_output g op Constant value_t=torch LongTensor - allowzero= prev_output = symbolic_helper _squeeze_helper g prev_output h_outs append h_out type ignore possibly-undefined variant == LSTM c_outs append c_out type ignore possibly-undefined batch_first seq batch num_directions hidden_size - batch seq num_directions hidden_size prev_output = g op Transpose prev_output perm_i= h_outs = h_out num_layers == g op Concat h_outs axis_i= type ignore possibly-undefined variant == RNN variant == GRU prev_output h_outs variant == LSTM c_outs = c_out num_layers == g op Concat c_outs axis_i= type ignore possibly-undefined prev_output h_outs c_outs symbolic_helper parse_args v v v i i f i i i _lstm_full g jit_utils GraphContext input hidden_v weight_v has_biases num_layers dropout train bidirectional batch_first hidden weight = symbolic_helper _unpack_list hidden_v symbolic_helper _unpack_list weight_v _generic_rnn g LSTM input hidden weight has_biases num_layers dropout train bidirectional batch_first symbolic_helper parse_args v v v v i i f i i _lstm_packed g jit_utils GraphContext input batch_sizes hidden_v weight_v has_biases num_layers dropout train bidirectional hidden weight = symbolic_helper _unpack_list hidden_v symbolic_helper _unpack_list weight_v _generic_rnn g LSTM input hidden weight has_biases num_layers dropout train bidirectional batch_sizes=batch_sizes _onnx_symbolic aten lstm lstm g jit_utils GraphContext args symbolic_helper _is_tensor_list args _lstm_packed g args _lstm_full g args _onnx_symbolic aten lstm_cell lstm_cell g jit_utils GraphContext hidden w_ih w_hh b_ih b_hh input = symbolic_helper _unsqueeze_helper g hidden = symbolic_helper _unpack_list hidden hidden = symbolic_helper _unsqueeze_helper g x x hidden weight = w_ih w_hh b_ih b_hh symbolic_helper _is_tensor b_ih w_ih w_hh has_biases = bool symbolic_helper _is_tensor b_ih _ h_outs c_outs = _generic_rnn g LSTM input hidden weight has_biases num_layers= dropout= train= bidirectional=False batch_first=False symbolic_helper _squeeze_helper g h_outs symbolic_helper _squeeze_helper g c_outs _onnx_symbolic aten gru decorate= symbolic_helper _apply_params GRU _export gru _onnx_symbolic aten rnn_tanh decorate= symbolic_helper _apply_params RNN_TANH _export rnn_tanh _onnx_symbolic aten rnn_relu decorate= symbolic_helper _apply_params RNN_RELU _export rnn_relu _one_hidden_rnn kind str symbolic_helper parse_args v v v i i f i i i _rnn_full g input hidden weight_v has_biases num_layers dropout train bidirectional batch_first weight = symbolic_helper _unpack_list weight_v _generic_rnn g kind input hidden weight has_biases num_layers dropout train bidirectional batch_first symbolic_helper parse_args v v v v i i f i i _rnn_packed g input batch_sizes hidden weight_v has_biases num_layers dropout train bidirectional weight = symbolic_helper _unpack_list weight_v _generic_rnn g kind input hidden weight has_biases num_layers dropout train bidirectional batch_sizes=batch_sizes symbolic g args symbolic_helper _is_tensor_list args _rnn_packed g args _rnn_full g args symbolic _onnx_symbolic aten _dim_arange symbolic_helper parse_args v i _dim_arange g jit_utils GraphContext like dim like_shape = g op Shape like stop = g op Gather like_shape g op Constant value_t=torch tensor dim axis_i= aten arange Scalar end ScalarType dtype Layout Device bool pin_memory arange g stop None None None _onnx_symbolic aten detach detach g jit_utils GraphContext input Erase aten detach nodes because ONNX inference only input _onnx_symbolic aten contiguous symbolic_helper parse_args v i contiguous g jit_utils GraphContext input memory_format memory_format allower values any preserve contiguous_format raise errors SymbolicValueError onnx memory_format support implemented input input _onnx_symbolic aten _pack_padded_sequence symbolic_helper parse_args v v i _pack_padded_sequence g jit_utils GraphContext input lengths batch_first Currently there no PackPadded operator ONNX We rely optimization pass remove later It error all PackPadded operators cannot optimized out batch_first input = g op Transpose input perm_i= lengths type isSubtypeOf torch _C TensorType get raise errors SymbolicValueError lengths must Tensor ONNX export input We know s TensorType so check now safe It s really only necessary because those operators expand something only works int types Caffe _type_utils JitScalarType from_value lengths _type_utils JitScalarType UNDEFINED = _type_utils JitScalarType INT lengths = g op Cast lengths to_i=_C_onnx TensorProtoDataType INT g op prim PackPadded input lengths outputs= _onnx_symbolic aten _pad_packed_sequence symbolic_helper parse_args v v i t v _pad_packed_sequence g jit_utils GraphContext data batch_sizes batch_first padding_value total_length Ignore total_length supported _symbolic_pad_packed_sequence It only useful used when training using data_parallel model so It shouldn t relevant ONNX anyway data lengths = g op prim PadPacked data batch_sizes outputs= batch_first data = g op Transpose data perm_i= data lengths _onnx_symbolic aten randint randint g jit_utils GraphContext low high shapes dtype options dtype = symbolic_helper _get_const dtype i dtype low_i = symbolic_helper _get_const low i low high_i = symbolic_helper _get_const high i high dtype None scalar_type = _type_utils JitScalarType INT scalar_type = _type_utils JitScalarType dtype low_i None raise symbolic_helper _onnx_unsupported randint low high_i None raise symbolic_helper _onnx_unsupported randint high shape = symbolic_helper _maybe_get_const shapes symbolic_helper _is_value shape shape_const = g op ConstantOfShape shapes value_t=torch tensor dtype=torch float randn = g op RandomUniformLike shape_const low_f=low_i high_f=high_i randn = g op RandomUniform shape_i=shape low_f=low_i high_f=high_i cast integer type int_dtype = _type_utils JitScalarType INT randint = g op Cast randn to_i=int_dtype onnx_type int_dtype = scalar_type randint = g op Cast randint to_i=scalar_type onnx_type randint _onnx_symbolic aten randint_like randint_like g jit_utils GraphContext low high dtype options dtype = symbolic_helper _get_const dtype i dtype low_i = symbolic_helper _get_const low i low high_i = symbolic_helper _get_const high i high dtype None scalar_type = _type_utils JitScalarType INT scalar_type = _type_utils JitScalarType dtype low_i None raise symbolic_helper _onnx_unsupported randint low high_i None raise symbolic_helper _onnx_unsupported randint high randn = g op RandomUniformLike low_f=low_i high_f=high_i cast integer type int_dtype = _type_utils JitScalarType INT randint = g op Cast randn to_i=int_dtype onnx_type int_dtype = scalar_type randint = g op Cast randint to_i=scalar_type onnx_type randint _onnx_symbolic aten randn randn g jit_utils GraphContext shapes dtype options dtype = symbolic_helper _get_const dtype i dtype dtype None scalar_type = _type_utils JitScalarType FLOAT scalar_type = _type_utils JitScalarType dtype shape = symbolic_helper _maybe_get_const shapes symbolic_helper _is_value shape shape_const = g op ConstantOfShape shapes value_t=torch tensor dtype=torch float g op RandomNormalLike shape_const dtype_i=scalar_type onnx_type g op RandomNormal shape_i=shape dtype_i=scalar_type onnx_type _onnx_symbolic aten rand rand g jit_utils GraphContext shapes dtype options dtype = symbolic_helper _get_const dtype i dtype dtype None scalar_type = _type_utils JitScalarType FLOAT scalar_type = _type_utils JitScalarType dtype shape = symbolic_helper _maybe_get_const shapes symbolic_helper _is_value shape shape_const = g op ConstantOfShape shapes value_t=torch tensor dtype=torch float g op RandomUniformLike shape_const dtype_i=scalar_type onnx_type g op RandomUniform shape_i=shape dtype_i=scalar_type onnx_type _onnx_symbolic aten randn_like randn_like g jit_utils GraphContext dtype layout=None device=None pin_memory=False memory_format=None dtype = symbolic_helper _get_const dtype i dtype dtype None scalar_type = _type_utils JitScalarType from_value _type_utils JitScalarType FLOAT scalar_type = _type_utils JitScalarType dtype g op RandomNormalLike dtype_i=scalar_type onnx_type _onnx_symbolic aten rand_like rand_like g jit_utils GraphContext dtype layout=None device=None pin_memory=False memory_format=None dtype = symbolic_helper _get_const dtype i dtype dtype None dtype = _type_utils JitScalarType from_value _type_utils JitScalarType FLOAT g op RandomUniformLike dtype_i=_type_utils JitScalarType dtype onnx_type _onnx_symbolic aten rrelu symbolic_helper parse_args v f f i none rrelu g jit_utils GraphContext input lower upper training generator training slope = upper + lower g op LeakyRelu input alpha_f=slope p = g op RandomUniformLike input high_f=upper low_f=lower g op PRelu input p _onnx_symbolic aten bernoulli bernoulli g jit_utils GraphContext input p=None generator=None out=None out None symbolic_helper _is_none out symbolic_helper _unimplemented Bernoulli out parameter supported bernoulli input generator None symbolic_helper _is_none generator symbolic_helper _unimplemented Bernoulli generator supported bernoulli input dtype = _type_utils JitScalarType from_value input _type_utils JitScalarType UNDEFINED dtype == _type_utils JitScalarType UNDEFINED symbolic_helper _unimplemented Bernoulli input dtype accessible input rands = g op RandomUniformLike input high_f= low_f= dtype_i=dtype onnx_type prob = p p None symbolic_helper _is_none p input output = g op Less rands prob g op Cast output to_i=dtype onnx_type _onnx_symbolic aten log_sigmoid symbolic_helper parse_args v log_sigmoid g jit_utils GraphContext input p = g op Sigmoid input g op Log p _onnx_symbolic aten erf symbolic_helper parse_args v erf g jit_utils GraphContext input g op Erf input _onnx_symbolic aten flatten symbolic_helper quantized_args True False False symbolic_helper parse_args v i i flatten g jit_utils GraphContext input start_dim end_dim dim = symbolic_helper _get_tensor_rank input dim None symbolic_helper _unimplemented dim ONNX PyTorch use different strategies split input Input rank must known export time input dim == symbolic_helper _reshape_helper g input dim == g op Identity input TODO remove onnx opset spec allows negative axes end_dim end_dim = dim + end_dim use ONNX s Flatten operator cases where output shape D start_dim == end_dim == dim - g op Flatten input axis_i=start_dim start_dim == end_dim == dim - g op Flatten input axis_i=end_dim + symbolic_helper _flatten_helper g input start_dim end_dim dim _onnx_symbolic aten nonzero symbolic_helper parse_args v nonzero g jit_utils GraphContext input Emitted ` torch nonzero x as_tuple=False ` t g g op NonZero input _onnx_symbolic aten nonzero_numpy Emitted ` torch nonzero x as_tuple=True ` nonzero_numpy g jit_utils GraphContext input _outputs=None unbind g nonzero g input _outputs=_outputs _onnx_symbolic aten isnan symbolic_helper parse_args v isnan g jit_utils GraphContext input output = g op IsNaN input output _onnx_symbolic aten any _any g jit_utils GraphContext args aten any Tensor len args == input = args dim keepdim = None aten any Tensor int dim bool keepdim input dim keepdim = args Can int list single int dim = symbolic_helper _parse_arg dim t dim = int d d dim view - keepdim = symbolic_helper _parse_arg keepdim i input = g op Cast input to_i=_C_onnx TensorProtoDataType INT input_sum = symbolic_helper _reducesum_helper g input axes_i=dim keepdims_i=keepdim gt g input_sum g op Constant value_t=torch tensor dtype=torch long _onnx_symbolic aten all _all g jit_utils GraphContext args input = g op Not args aten all Tensor len args == g op Not _any g input aten all Tensor int dim bool keepdim g op Not _any g input args args _onnx_symbolic aten narrow symbolic_helper parse_args v i i i narrow g jit_utils GraphContext input dim start length symbolic_helper _slice_helper g input axes= dim starts= start ends= start + length _onnx_symbolic aten argmax symbolic_helper parse_args v v b argmax g jit_utils GraphContext input torch _C Value dim torch _C Value keepdim bool symbolic_helper _argmin_argmax_helper g input dim keepdim ArgMax _onnx_symbolic aten argmin symbolic_helper parse_args v v b argmin g jit_utils GraphContext input torch _C Value dim torch _C Value keepdim bool symbolic_helper _argmin_argmax_helper g input dim keepdim ArgMin _onnx_symbolic aten scatter symbolic_helper parse_args v i v v scatter g jit_utils GraphContext dim index src src_type = _type_utils JitScalarType from_value src _type_utils JitScalarType UNDEFINED src = symbolic_helper _maybe_get_scalar src symbolic_helper _is_value src g op Scatter index src axis_i=dim Check scalar src has same type PyTorch allows different type scalar src when src tensor If insert Cast node self_scalar_type = _type_utils JitScalarType from_value self_scalar_type = src_type src = g op Cast src to_i=self_scalar_type onnx_type g op Scatter index expand_as g src index axis_i=dim _onnx_symbolic aten scatter_add symbolic_helper parse_args v i v v scatter_add g jit_utils GraphContext dim index src scalar_type = symbolic_helper _try_get_scalar_type scalar_type None symbolic_helper _unimplemented scatter_add input dtype accessible sizes = symbolic_helper _get_tensor_sizes allow_nonstatic=False sizes to_add = g op Constant value_t=torch zeros sizes dtype=scalar_type dtype to_add = zeros_like g scalar_type to_add = symbolic_helper _scatter_helper g to_add dim index src add g to_add _onnx_symbolic aten log log g jit_utils GraphContext _ln = g op Div log g g op Constant value_t=torch tensor _ln _onnx_symbolic aten is_floating_point is_floating_point g jit_utils GraphContext symbolic_helper _is_fp g op Constant value_t=torch BoolTensor g op Constant value_t=torch BoolTensor _onnx_symbolic aten __is_ __is_ g jit_utils GraphContext other symbolic_helper _is_none other symbolic_helper _is_none g op Constant value_t=torch BoolTensor g op Constant value_t=torch BoolTensor eq g other _onnx_symbolic aten __isnot_ wrap_logical_op_with_negation __isnot_ g jit_utils GraphContext other __is_ g other _onnx_symbolic aten one_hot one_hot g jit_utils GraphContext num_classes values = g op Constant value_t=torch LongTensor onnxruntime supports limited type combinations OneHot _type_utils JitScalarType from_value num_classes _type_utils JitScalarType UNDEFINED _type_utils JitScalarType UINT _type_utils JitScalarType INT _type_utils JitScalarType INT _type_utils JitScalarType INT num_classes = g op Cast num_classes to_i=_C_onnx TensorProtoDataType INT g op OneHot num_classes values axis_i=- _onnx_symbolic aten gather symbolic_helper parse_args v i v v gather g jit_utils GraphContext dim index sparse_grad=False symbolic_helper _maybe_get_const sparse_grad i symbolic_helper _unimplemented gather sparse_grad == True NOTE This workaround needed since GatherElement only supported since opset Gather ONNX same torch gather scalar_type = _type_utils JitScalarType from_value values = g op Constant value_t=torch LongTensor depth = size g g op Constant value_t=torch LongTensor dim index = g op Cast g op OneHot index depth values axis_i=dim to_i=scalar_type onnx_type mul = g op Mul symbolic_helper _unsqueeze_helper g dim + index symbolic_helper _reducesum_helper g mul axes_i= dim keepdims_i= symbolic_helper parse_args v i i _var_mean g jit_utils GraphContext input dim correction keepdim symbolic_helper _var_mean_helper g input dim correction keepdim _onnx_symbolic aten std std g jit_utils GraphContext input args var _ = var_mean g input args g op Sqrt var _onnx_symbolic aten var var g jit_utils GraphContext input args var _ = var_mean g input args var _onnx_symbolic aten var_mean var_mean g jit_utils GraphContext input args len args == _var_mean g input None args None _var_mean g input args _onnx_symbolic aten std_mean std_mean g jit_utils GraphContext input args var mean = var_mean g input args g op Sqrt var mean _onnx_symbolic aten logsumexp symbolic_helper parse_args v i logsumexp g jit_utils GraphContext input dim keepdim g op ReduceLogSumExp input axes_i=dim keepdims_i=keepdim _onnx_symbolic aten arange arange g jit_utils GraphContext args _get_arange_dtype dtype dtype = symbolic_helper _maybe_get_const dtype i dtype _float_step_convert range_tensor symbolic_helper _is_fp range_tensor range_tensor = g op Cast g op Ceil range_tensor to_i=_type_utils JitScalarType INT onnx_type range_tensor len args == len args == len args == aten arange Scalar end Tensor out dtype = None aten arange Scalar end ScalarType dtype Layout Device bool pin_memory dtype = _get_arange_dtype args dtype end start step = symbolic_helper _arange_cast_helper g end=args dtype=dtype end = symbolic_helper _unsqueeze_helper g end range_tensor = _float_step_convert end arange_tensor = symbolic_helper _squeeze_helper g nonzero g ones g range_tensor dtype None None g op Cast arange_tensor to_i=_type_utils JitScalarType dtype onnx_type len args == len args == len args == aten arange Scalar start Scalar end Scalar step Tensor out dtype = None aten arange Scalar start Scalar end Scalar step ScalarType dtype Layout Device bool pin_memory dtype = _get_arange_dtype args dtype end start step = symbolic_helper _arange_cast_helper g start=args end=args step=args dtype=dtype step = symbolic_helper _unsqueeze_helper g step end = symbolic_helper _unsqueeze_helper g end start = symbolic_helper _unsqueeze_helper g start range_tensor = _float_step_convert g op Div g op Sub end start step arange_tensor = symbolic_helper _squeeze_helper g nonzero g ones g range_tensor None None None arange_tensor = g op Add g op Mul arange_tensor step start g op Cast arange_tensor to_i=_type_utils JitScalarType dtype onnx_type len args == aten arange Scalar start Scalar end ScalarType dtype Layout Device bool pin_memory dtype = _get_arange_dtype args dtype end start step = symbolic_helper _arange_cast_helper g start=args end=args dtype=dtype end = symbolic_helper _unsqueeze_helper g end start = symbolic_helper _unsqueeze_helper g start range_tensor = _float_step_convert g op Sub end start arange_tensor = g op Add symbolic_helper _squeeze_helper g nonzero g ones g range_tensor dtype args start g op Cast arange_tensor to_i=_type_utils JitScalarType dtype onnx_type symbolic_helper _unimplemented aten arange f len args arguments _onnx_symbolic aten linspace linspace g jit_utils GraphContext start end steps dtype layout device pin_memory range_tensor = symbolic_helper _arange_helper g steps None step = div g sub g end start sub g steps g op Constant value_t=torch tensor dtype=torch int add g mul g range_tensor step start _onnx_symbolic aten lift lift g jit_utils GraphContext lift no-op perspective tracing onnx _onnx_symbolic aten masked_fill masked_fill g jit_utils GraphContext mask value Implement masked_fill functionality available pytorch tensor ONNX Fills elements input tensor ` value ` where ` mask ` True mask = g op Cast mask to_i=_C_onnx TensorProtoDataType BOOL value = symbolic_helper _maybe_get_scalar value g op Where mask symbolic_helper _if_scalar_type_as value _onnx_symbolic aten masked_fill_ masked_fill_ g jit_utils GraphContext mask value masked_fill g mask value _onnx_symbolic aten index index g jit_utils GraphContext index symbolic_helper _is_packed_list index indices = symbolic_helper _unpack_list index indices = index try_mask_to_index index symbolic_helper _is_none index _type_utils JitScalarType from_value index _type_utils JitScalarType UNDEFINED == _type_utils JitScalarType UINT symbolic_helper _is_bool index g opset raise errors SymbolicValueError Exporting masked indices only supported after ONNX opset warnings warn Exporting aten index operator indices type Byte Only -D indices supported In any other case will produce incorrect ONNX graph stacklevel= index = symbolic_helper _squeeze_helper g nonzero g index index indices = try_mask_to_index idx idx indices len indices == symbolic_helper _select_helper g indices apply_reshape=False Multiple tensors indices Each tensor could either prim Constant representing python indexing E g tensor prim Constant value= tensor output representing advanced indexing E g tensor For more info advanced indexing check https numpy org doc stable user basics indexing html#advanced-indexing Consider general case t x_ y_ y_ x_m y_n where t tensor rank m+n x_i axes where tensor index provided y_i axes Same results can achieved through transposing t into t x_ x_ x_m y_ y_ y_n use gatherND However ONNX does have gatherND use d gather we ll need flatten t process tensor indices t x_ x_ x_m y_ y_ y_n tensor index = \sum_ i= ^m ind_i \prod_ j=i+ ^m x_j After gather reshape transpose back adv_idx_indices = i i idx enumerate indices symbolic_helper _is_none idx len adv_idx_indices == len adv_idx_indices == index_select g adv_idx_indices indices adv_idx_indices rank = symbolic_helper _get_tensor_rank rank None symbolic_helper _unimplemented aten index operator advanced indexing tensor unknown rank TODO If indexing supported natively ONNX future opsets update warning recommend exporting higher opset version warnings warn Exporting aten index operator advanced indexing opset f GLOBALS export_onnx_opset_version achieved combination multiple ONNX operators including Reshape Transpose Concat Gather If indices include negative values exported graph will produce incorrect results stacklevel= adv_idx_count = len adv_idx_indices shape_tensor = _shape_as_tensor g dim_tensor_list = g op Gather shape_tensor g op Constant value_t=torch LongTensor dim axis_i= dim range rank = g op Transpose perm_i=adv_idx_indices + i i range rank i adv_idx_indices = g op Flatten axis_i=adv_idx_count Note tensor indices will broadcasted while accumulating Thus we get final subarray shape well cum_adv_index = indices adv_idx_indices - multiplier = dim_tensor_list adv_idx_indices - i range adv_idx_count - - - adv_index = g op Mul indices adv_idx_indices i multiplier cum_adv_index = g op Add cum_adv_index adv_index multiplier = g op Mul multiplier dim_tensor_list adv_idx_indices i perform gather = index_select g cum_adv_index cum_adv_index_shape_tensor = _shape_as_tensor g cum_adv_index check all advanced indices consecutive Refer https numpy org doc stable user basics indexing html#combining-advanced-and-basic-indexing understand how subarray position decided adv_idx_indices == list range adv_idx_indices adv_idx_indices - + unfold regular index axes folded_adv_idx_shape_list = g op Constant value_t=torch LongTensor - + dim_tensor_list i i range rank i adv_idx_indices folded_adv_idx_shape = g op Concat folded_adv_idx_shape_list axis_i= = symbolic_helper _reshape_helper g folded_adv_idx_shape Transpose folded advanced indexed axis its original location adv_idx_permute = list range adv_idx_indices + + + list range adv_idx_indices + rank - adv_idx_count + = g op Transpose perm_i=adv_idx_permute unfold advanced index axes final_shape_list = dim_tensor_list i i range adv_idx_indices + cum_adv_index_shape_tensor + dim_tensor_list i i range adv_idx_indices rank i adv_idx_indices final_shape = g op Concat final_shape_list axis_i= final_shape = g op Concat cum_adv_index_shape_tensor dim_tensor_list i i range rank i adv_idx_indices axis_i= symbolic_helper _reshape_helper g final_shape _onnx_symbolic aten linalg_norm symbolic_helper parse_args v v b v linalg_norm g jit_utils GraphContext torch _C Value ord torch _C Value dim Sequence int &#124; None keepdim bool dtype torch _C Value Conditions based https pytorch org docs stable generated torch linalg norm html ord_value = None dim None symbolic_helper _is_none ord = symbolic_helper _reshape_helper g - ord = g op Constant value_t=torch LongTensor self_dim = symbolic_helper _get_tensor_rank self_dim None symbolic_helper _unimplemented dim Input rank must known export time self_dim == ord_value = symbolic_helper _parse_arg ord f dim = len dim == symbolic_helper _is_none ord ord = g op Constant value_t=torch LongTensor ord_value = symbolic_helper _parse_arg ord f ord_value linalg_vector_norm g ord_value dim keepdim dtype linalg_matrix_norm g ord dim keepdim dtype _onnx_symbolic aten linalg_vector_norm symbolic_helper parse_args v f b v linalg_vector_norm g jit_utils GraphContext torch _C Value ord float dim Sequence int &#124; None keepdim bool dtype torch _C Value symbolic_helper _linalg_vector_norm_helper g ord dim keepdim dtype _onnx_symbolic aten linalg_matrix_norm symbolic_helper parse_args v v b v linalg_matrix_norm g jit_utils GraphContext torch _C Value ord torch _C Value dim list int keepdim bool dtype torch _C Value Conditions based https pytorch org docs stable generated torch linalg matrix_norm html ord_value = symbolic_helper _parse_arg ord s ord_value == fro frobenius_norm g dim keepdim ord_value == nuc symbolic_helper _unimplemented linalg matrix_norm ord==nuc ord_value = symbolic_helper _parse_arg ord f ord_value None frobenius_norm g dim keepdim ord_value == ord_value == - ord = - unimplemented due lack operators used calculate singular values symbolic_helper _unimplemented linalg matrix_norm ord== Wrap dim vector handle negative dim values self_dim = symbolic_helper _get_tensor_rank self_dim None symbolic_helper _unimplemented linalg matrix_norm Input rank must known export time Common implementation cases ord = - ord = inf -inf dim dim += self_dim dim dim += self_dim ord_value == math inf ord_value == -math inf dim dim = dim dim dim dim keepdim dim -= sum = symbolic_helper _reducesum_helper g g op Abs axes_i= dim keepdims_i=keepdim ord_value pyrefly ignore no-matching-overload result _indices = max g sum dim_or_y=g op Constant value_t=torch LongTensor dim keepdim=keepdim pyrefly ignore no-matching-overload result _indices = min g sum dim_or_y=g op Constant value_t=torch LongTensor dim keepdim=keepdim result _onnx_symbolic aten linalg_cross symbolic_helper parse_args v v i linalg_cross g jit_utils GraphContext input other dim=- cross g input other dim _onnx_symbolic aten frobenius_norm symbolic_helper parse_args v b frobenius_norm g jit_utils GraphContext dim=None keepdim=False sqr = g op Mul sumsqr = symbolic_helper _reducesum_helper g sqr axes_i=dim keepdims_i=keepdim g op Sqrt sumsqr _onnx_symbolic aten multinomial symbolic_helper parse_args v i b v multinomial g jit_utils GraphContext input num_samples replacement=False generator=None generator None symbolic_helper _is_none generator symbolic_helper _unimplemented Multinomial generator supported multinomial input replacement num_samples symbolic_helper _unimplemented Multinomial replacement=False when num_samples supported multinomial input log_input = log g input g op Multinomial log_input dtype_i=_C_onnx TensorProtoDataType INT sample_size_i=num_samples _onnx_symbolic aten baddbmm baddbmm g jit_utils GraphContext batch batch beta alpha scalar_type = _type_utils JitScalarType from_value batch_mul = matmul g batch batch mul_a = mul g batch_mul g op Cast alpha to_i=scalar_type onnx_type mul_b = mul g g op Cast beta to_i=scalar_type onnx_type add g mul_a mul_b _onnx_symbolic aten meshgrid symbolic_helper parse_args v s meshgrid g jit_utils GraphContext tensor_list indexing str &#124; None = None indexing None indexing = ij indexing ij xy raise errors SymbolicValueError f Unsupported indexing indexing tensor_list unpacked_tensor_list = symbolic_helper _unpack_list tensor_list indexing == xy unpacked_tensor_list = unpacked_tensor_list - tensors = symbolic_helper _reshape_helper g t g op Constant value_t=torch LongTensor - t unpacked_tensor_list tensors_shape = g op Shape t t tensors out_shape = g op Concat tensors_shape axis_i= out = i t enumerate tensors shape_i = g op Constant value_t=torch ones dtype=torch int len tensors shape_i i = tensors_shape i t_reshaped = _reshape_from_tensor g t g op Concat shape_i axis_i= out append g op Expand t_reshaped out_shape indexing == xy out out = out out g op prim ListConstruct out _onnx_symbolic aten remainder remainder g jit_utils GraphContext input other div = _floor_divide g input other quo = g op Mul div other g op Sub input quo _onnx_symbolic aten gelu symbolic_helper parse_args v s gelu g jit_utils GraphContext torch _C Value approximate str = none approximate == tanh kBeta = math sqrt math pi kKappa = beta = torch tensor kBeta dtype=torch double kappa = torch tensor kKappa dtype=torch double one = torch tensor dtype=torch double half = torch tensor dtype=torch double self_cube = mul g mul g inner = mul g beta add g mul g kappa self_cube mul g half mul g add g one g op Tanh inner _sqrt = erf = g op Erf g op Div torch tensor _sqrt dtype=torch double erf_plusone = add g erf g op Constant value_t=torch tensor dtype=torch double mul g mul g erf_plusone g op Constant value_t=torch tensor dtype=torch double _onnx_symbolic aten group_norm symbolic_helper quantized_args True False False False symbolic_helper parse_args v i v v f i group_norm g jit_utils GraphContext input num_groups weight bias eps cudnn_enabled channel_size = symbolic_helper _get_tensor_dim_size input channel_size None assert channel_size num_groups == input_rank = symbolic_helper _get_tensor_rank input input_rank None symbolic_helper _unimplemented group_norm unknown input rank input shape list keeps dimension value unchanged shape = num_groups - input_reshaped = symbolic_helper _reshape_helper g input g op Constant value_t=torch LongTensor shape C always divisible num_groups Due shape difference we need apply weight bias after instance norm computation reshape weight_ = g op Constant value_t=torch tensor num_groups dtype=_type_utils JitScalarType from_value input dtype bias_ = g op Constant value_t=torch tensor num_groups dtype=_type_utils JitScalarType from_value input dtype norm_reshaped = g op InstanceNormalization input_reshaped weight_ bias_ epsilon_f=eps norm = symbolic_helper _reshape_helper g norm_reshaped g op Shape input weight None weight node mustBeNone weight_value = torch tensor dtype=_type_utils JitScalarType from_value input dtype weight = g op Constant value_t=weight_value bias None bias node mustBeNone bias_value = torch tensor dtype=_type_utils JitScalarType from_value input dtype bias = g op Constant value_t=bias_value Norm has shape N C so we reshape weight bias C axes = list range input_rank - add g mul g norm symbolic_helper _unsqueeze_helper g weight axes symbolic_helper _unsqueeze_helper g bias axes _onnx_symbolic aten _weight_norm symbolic_helper parse_args v v i _weight_norm g jit_utils GraphContext weight_v weight_g dim rank = symbolic_helper _get_tensor_rank weight_v rank None W = g v &#124; &#124; v &#124; &#124; Compute norm_except_dim l norm dim = None means over all dims torch s weight_norm module sets dim = - s None This conflicts logic negative axes access dims backwards TODO Might need fix torch group_norm module axes = list range rank dim None dim - dim += rank dim = - axes remove dim norm_v = norm g weight_v axes div = g op Div weight_v norm_v g op Mul div weight_g raise errors SymbolicValueError Unsupported ONNX export _weight_norm tensor unknown rank weight_v _onnx_symbolic aten dim dim g jit_utils GraphContext Implement dim functionality available pytorch tensor ONNX ONNX does support dim directly opset so we can use ops get info shape = g op Shape g op Size shape _onnx_symbolic aten __contains_ __contains_ g jit_utils GraphContext element unpacked_list = symbolic_helper _unpack_list all symbolic_helper _is_constant x x unpacked_list symbolic_helper _is_constant element g op Constant value_t=torch tensor symbolic_helper _node_get element node value symbolic_helper _node_get x node value x unpacked_list raise errors SymbolicValueError Unsupported ONNX export __contains__ non-constant list element _onnx_symbolic aten __getitem_ __getitem_ g jit_utils GraphContext i select g g op Constant value_t=torch tensor i _onnx_symbolic aten item item g jit_utils GraphContext _onnx_symbolic aten take take g jit_utils GraphContext index self_flattened = symbolic_helper _reshape_helper g g op Constant value_t=torch tensor - dtype=torch int out = index_select g self_flattened index out = reshape_as g out index out _kl_div_log_target_impl g jit_utils GraphContext input target diff_ = sub g target input exp_ = exp g target output = mul g exp_ diff_ output _kl_div_non_log_target_impl g jit_utils GraphContext input target log_ = log g target diff_ = sub g log_ input output_pos = mul g target diff_ zeros_ = zeros_like g output_pos mask_ = gt g target g op Constant value_t=torch tensor output = where g mask_ output_pos zeros_ output _onnx_symbolic aten kl_div symbolic_helper parse_args v v i b kl_div g jit_utils GraphContext input target reduction log_target log_target output = _kl_div_log_target_impl g input target output = _kl_div_non_log_target_impl g input target reduction == output reduction == g op ReduceMean output keepdims_i= reduction == symbolic_helper _reducesum_helper g output keepdims_i= symbolic_helper _onnx_unsupported kl_div reduction other than none mean sum input _onnx_symbolic aten mse_loss symbolic_helper parse_args v v i mse_loss g jit_utils GraphContext input target reduction output = mul g sub g input target sub g input target reduction == output reduction == g op ReduceMean output keepdims_i= reduction == symbolic_helper _reducesum_helper g output keepdims_i= symbolic_helper _onnx_unsupported mse_loss reduction other than none mean sum input _onnx_symbolic aten as_strided symbolic_helper quantized_args True symbolic_helper parse_args v v i as_strided g jit_utils GraphContext sizes strides offset=None sizes = symbolic_helper _maybe_get_const sizes rank = len strides self_ d = symbolic_helper _reshape_helper g g op Constant value_t=torch tensor - dtype=torch int ind torch Tensor &#124; None symbolic_helper _is_value sizes ind = torch tensor dtype=torch long i size stride enumerate zip sizes strides r_size = rank r_size i = - ind = ind + torch arange size view r_size stride offset ind = ind + offset g op Gather self_ d g op Constant value_t=ind ind = None i stride enumerate strides r_size = rank r_size i = - size = select g sizes g op Constant value_t=torch tensor g op Constant value_t=torch tensor i tmp_ind = symbolic_helper _reshape_helper g arange g size None None None g op Constant value_t=torch tensor r_size tmp_ind = g op Mul tmp_ind g op Constant value_t=torch tensor stride ind None ind = tmp_ind ind = g op Add ind tmp_ind offset pyrefly ignore bad-argument-type ind = g op Add ind g op Constant torch tensor offset pyrefly ignore bad-argument-type g op Gather self_ d ind _onnx_symbolic aten __derive_index __derive_index g jit_utils GraphContext index start step g op Add start g op Mul index step _onnx_symbolic aten __range_length Source code aten op can found here pytorch torch csrc jit runtime register_prim_ops cpp step lo hi push stack + hi - - lo step step lo hi push stack + lo - - hi - step push stack __range_length g jit_utils GraphContext lo hi step sub = g op Sub hi lo div = g op Ceil true_divide g sub step g op Cast div to_i=_C_onnx TensorProtoDataType INT _onnx_symbolic aten linear linear g jit_utils GraphContext input weight bias rank = symbolic_helper _get_tensor_rank input weight = t g weight rank == bias node mustBeNone alpha = g op Constant value_t=torch tensor dtype=torch int beta = g op Constant value_t=torch tensor dtype=torch int output = addmm g bias input weight alpha beta output = matmul g input weight bias node mustBeNone output = add g bias output output _onnx_symbolic aten hann_window symbolic_helper parse_args v b i v v v v hann_window g jit_utils GraphContext window_length periodic=True dtype int &#124; None = None layout=None device=None pin_memory=None requires_grad=False dtype None dtype_ = torch get_default_dtype dtype_ dtype_ is_floating_point dtype_ = torch float scalar_type = _type_utils JitScalarType from_dtype dtype_ scalar_type = _type_utils JitScalarType dtype n_array = arange g window_length None None None output = g op Cast n_array to_i=_C_onnx TensorProtoDataType FLOAT output = mul g g op Constant value_t=torch tensor math pi dtype=torch float output periodic False window_length = sub g window_length g op Constant value_t=torch tensor dtype=torch int output = div g output window_length output = g op Cast square g sin g output to_i=scalar_type onnx_type output _onnx_symbolic aten mv mv g jit_utils GraphContext vec matmul g vec _onnx_symbolic aten dot dot g jit_utils GraphContext other matmul g other _onnx_symbolic aten movedim symbolic_helper parse_args v t t movedim g jit_utils GraphContext source destination This pythonic implementation mostly taken aten src ATen native TensorShape cpp movedim source = source view - destination = destination view - assert source size == destination size source == destination all self_rank = symbolic_helper _get_tensor_rank assert self_rank None perm = list range self_rank src_dims = perm copy dst_dims = perm copy src dst zip source tolist destination tolist perm dst = src src_dims src = - dst_dims dst = - src_dims = dim dim src_dims dim = - dst_dims = dim dim dst_dims dim = - src dst zip src_dims dst_dims perm dst = src g op Transpose perm_i=perm _onnx_symbolic aten fill symbolic_helper parse_args v v fill g jit_utils GraphContext value scalar_type = _type_utils JitScalarType from_value _type_utils JitScalarType FLOAT full_like g value scalar_type _onnx_symbolic aten index_add index_add g jit_utils GraphContext dim index other alpha=None warnings warn Warning ONNX export does support duplicated values index field + will cause ONNX model incorrect stacklevel= ONNX does support alpha argument unlike aten index_add See https github com pytorch pytorch pull #issuecomment- more context alpha symbolic_helper _scalar symbolic_helper _maybe_get_scalar alpha = symbolic_helper _unimplemented index_add alpha = dim = symbolic_helper _maybe_get_const dim i dim None raise errors SymbolicValueError ONNX export does NOT support exporting index_add_ function unknown dim value self_dim_rank = symbolic_helper _get_tensor_rank other_dim_rank = symbolic_helper _get_tensor_rank other self_dim_rank None other_dim_rank None raise errors SymbolicValueError ONNX export does NOT support exporting index_add_ function while rank tensor tensor added unknown other_dim_rank = self_dim_rank delta = self_dim_rank - other_dim_rank _ range delta other = symbolic_helper _unsqueeze_helper g other symbolic_helper _get_tensor_rank other other_dim_size = symbolic_helper _get_tensor_dim_size other dim self_dim_size = symbolic_helper _get_tensor_dim_size dim other_dim_size None self_dim_size None other_dim_size self_dim_size raise errors SymbolicValueError ONNX export does support exporting index_add_ function duplicated values index parameter yet Construct new shape It s almost same except size dim dimension so we can expand other dimensions expected new_shape_axes = list range self_dim_rank new_shape_starts = i range self_dim_rank new_shape_ends = sys maxsize i = dim i range self_dim_rank new_shape = symbolic_helper _slice_helper g axes=new_shape_axes starts=new_shape_starts ends=new_shape_ends other = expand_as g other new_shape _ range dim index = symbolic_helper _unsqueeze_helper g index _ range self_dim_rank - dim - index = symbolic_helper _unsqueeze_helper g index symbolic_helper _get_tensor_rank index scatter_add g dim expand_as g index other other _onnx_symbolic aten roll symbolic_helper parse_args v roll g jit_utils GraphContext shifts dims assert len shifts == len dims result = i range len shifts shapes = shape = symbolic_helper _slice_helper g result axes= dims i starts= -shifts i ends= sys maxsize shapes append shape shape = symbolic_helper _slice_helper g result axes= dims i starts= ends= -shifts i shapes append shape result = g op Concat shapes axis_i=dims i result _onnx_symbolic aten cross symbolic_helper parse_args v v i cross g jit_utils GraphContext input other dim=None dim = symbolic_helper _get_dim_for_cross input dim If we have two tensors such A = b c B = d e f we permute tensor such we have After first roll A = b c B = f d e so we calculate b f c d e roll_x_ = roll g input dim roll_y_ = roll g other dim After second roll A = c b B = e f d so we calculate c e f b d roll_x_ = roll g input dim roll_y_ = roll g other dim cross product calculated result = b f - c e c d - f e - b d sub g mul g roll_x_ roll_y_ mul g roll_x_ roll_y_ _onnx_symbolic aten cdist cdist g jit_utils GraphContext x x p= compute_mode= use_mm_for_euclid_dist_if_necessary X shape = B P D X shape = B R D In order respect numpy style broadcasting demonstrated https github com onnx onnx blob main docs Broadcasting md we unsqueeze both input tensors row_size_x = symbolic_helper _get_tensor_dim_size x - row_size_x = symbolic_helper _get_tensor_dim_size x - assert row_size_x None assert row_size_x None p_float = symbolic_helper _parse_arg p f compute_mode = symbolic_helper _parse_arg compute_mode i p_float == compute_mode == compute_mode None row_size_x = row_size_x = _euclidean_dist g x x rank = symbolic_helper _get_tensor_rank x assert rank None broadcasted_x = symbolic_helper _unsqueeze_helper g x rank - broadcasted_x = symbolic_helper _unsqueeze_helper g x rank - pairwise_distance g broadcasted_x broadcasted_x p eps= e- keepdim=False _euclidean_dist g jit_utils GraphContext x x X shape = B P D X shape = B R D using matrix multiplication accelerate calculation euclidean distance rank = symbolic_helper _get_tensor_rank x assert rank None x _norm = symbolic_helper _reducesum_helper g pyrefly ignore no-matching-overload pow g x symbolic_helper _generate_wrapped_number g axes_i= - keepdims_i=True x _pad = ones_like g x _norm x _norm = symbolic_helper _reducesum_helper g pyrefly ignore no-matching-overload pow g x symbolic_helper _generate_wrapped_number g axes_i= - keepdims_i=True x _pad = ones_like g x _norm x _ = g op Concat mul g symbolic_helper _generate_wrapped_number g - x x _norm x _pad axis_i=- x _ = g op Concat x x _pad x _norm axis_i=- result = matmul g x _ transpose g x _ - - dtype = _type_utils JitScalarType from_value result min = g op Cast symbolic_helper _generate_wrapped_number g to_i=dtype onnx_type result = symbolic_helper _op_with_optional_float_cast g Max result min opset_before= result = sqrt g result result _onnx_symbolic aten lerp lerp g jit_utils GraphContext end weight Conditional better numeric This has been discussed https github com pytorch pytorch pull diff = g op Sub end where g g op Less weight g op Constant value_t=torch tensor g op Add g op Mul weight diff g op Sub end g op Mul diff g op Sub g op Constant value_t=torch tensor weight _onnx_symbolic aten broadcast_tensors broadcast_tensors g jit_utils GraphContext all_tensors = symbolic_helper _unpack_list t_with_final_shape = zeros_like g all_tensors Add operator supports multidirectional broadcasting So we leverage function infer final shape generated broadcast t all_tensors t_with_final_shape = add g t_with_final_shape t t_list = expand_as g t t_with_final_shape t all_tensors g op prim ListConstruct t_list _onnx_symbolic aten is_pinned is_pinned g jit_utils GraphContext device=None Unused ONNX None _onnx_symbolic prim ConstantSplit prim_constant_split g jit_utils GraphContext split_size dim size = symbolic_helper _get_tensor_dim_size dim size None symbolic_helper _unimplemented prim ConstantSplit unknown dimension size splits = split_size size split_size leftover = size split_size leftover splits append leftover g op Split split_i=splits axis_i=dim outputs=len splits TODO It would better export chunk directly less sensitive changes input size TODO Once we have proper scoping stop reimplementing chunk delete method use desugared version _onnx_symbolic prim ConstantChunk prim_constant_chunk g jit_utils GraphContext chunks dim dim_size = symbolic_helper _get_tensor_dim_size dim dim_size None symbolic_helper _unimplemented prim ConstantChunk unknown dimension size split_size = dim_size + chunks - chunks prim_constant_split g split_size dim _onnx_symbolic prim shape prim_shape g jit_utils GraphContext g op Shape _onnx_symbolic prim max prim_max g jit_utils GraphContext other symbolic_helper _op_with_optional_float_cast g Max other opset_before= _onnx_symbolic prim min prim_min g jit_utils GraphContext other=None other symbolic_helper _is_packed_list = stack g g op Constant value_t=torch tensor min g min g other _onnx_symbolic prim data prim_data g jit_utils GraphContext _onnx_symbolic prim layout prim_layout g jit_utils GraphContext Always torch strided Other layout types supported JIT TensorType Layout defined c core Layout h g op Constant value_t=torch tensor _onnx_symbolic prim ListConstruct prim_list_construct g jit_utils GraphContext inputs kwargs None _onnx_symbolic prim ListUnpack prim_list_unpack g jit_utils GraphContext inputs kwargs - list _C Value &#124; None len inputs == inputs node kind == prim ListConstruct Cancel previous node ListConstruct returning its inputs TODO justinchuby Use public method helper module symbolic_helper _unpack_list inputs None _onnx_symbolic prim TupleConstruct prim_tuple_construct g jit_utils GraphContext inputs kwargs None _onnx_symbolic prim Uninitialized prim_uninitialized g jit_utils GraphContext inputs kwargs None exists refine type Value x optional Tensor unchecked_cast will cast x Tensor so rest graph knows x Tensor doesn t do anything runtime noop ONNX _onnx_symbolic prim unchecked_cast prim_unchecked_cast g jit_utils GraphContext _onnx_symbolic prim dtype prim_dtype g jit_utils GraphContext scalar_type = symbolic_helper _try_get_scalar_type scalar_type None scalar_type = _type_utils JitScalarType FLOAT This node records torch dtype int g op Constant value_t=torch tensor scalar_type _onnx_symbolic prim tolist prim_tolist g jit_utils GraphContext input dim_val elem_ty_val tolist currently supported only D input tensors dim_val elem_ty_val represent dimension type annotations need match dimension type input tensor dim = symbolic_helper _maybe_get_const dim_val i dim symbolic_helper _unimplemented prim tolist dim_val input input ----------------------------------------------------------------------------- Symbolic functions need extra context ----------------------------------------------------------------------------- _onnx_symbolic prim device prim_device g jit_utils GraphContext inputs kwargs - None output_type = g original_node output type isinstance output_type _C DeviceObjType None symbolic_helper _unimplemented prim device f output type should DeviceObjType output_type kind g original_node output _onnx_symbolic prim Loop prim_loop g jit_utils GraphContext inputs attrs - list _C Value node = g original_node env = g env values_in_env = g values_in_env params_dict = g params_dict operator_export_type = GLOBALS operator_export_type opset_version = GLOBALS export_onnx_opset_version old_blocks = tuple node blocks _new_op_outputs new_block_contexts new_node = jit_utils add_op_with_blocks g Loop inputs outputs=node outputsSize n_blocks=len old_blocks old_block new_block_context zip old_blocks new_block_contexts Copy input metadata subblock prim Loop iter cond input_ input_n block iter input_ input_n For ` Loop ` node copy metadata ` iter ` ` input_ ` ` input_n ` i b_in enumerate old_block inputs i == i len inputs b_in setType inputs i type For optional block inputs they may switch between None not-None inside loop body so loop input optional block input may still need optional i i + len inputs isinstance b_in type _C OptionalType b_in setType inputs i + type torch _C _jit_pass_onnx_block old_block new_block_context block operator_export_type env values_in_env False fixed_outputs = torch _C _jit_pass_fixup_onnx_controlflow_node new_node opset_version Run shape type inference Loop after subblock converted GLOBALS onnx_shape_inference torch _C _jit_pass_onnx_node_shape_type_inference new_node params_dict opset_version fixed_outputs _onnx_symbolic prim If prim_if g jit_utils GraphContext inputs attrs - list _C Value n = g original_node block = g block env = g env values_in_env = g values_in_env params_dict = g params_dict operator_export_type = GLOBALS operator_export_type opset_version = GLOBALS export_onnx_opset_version static_if = inputs node kind == onnx Constant static_if Fold static The torch IR graph embedding_matrix Float strides= requires_grad= device=cpu input Long strides= requires_grad= device=cpu Bool requires_grad= device=cpu = prim Constant value= Long device=cpu = aten eq Long device=cpu = prim If block Long device=cpu = aten is_floating_point input - block - input Tensor weight Tensor = prim If block - embedding_matrix input block - input embedding_matrix int = aten size input The converted ONNX graph Bool device=cpu = onnx Constant value= Bool device=cpu = onnx Equal Bool requires_grad= device=cpu = onnx Constant value= Long strides= device=cpu = onnx Shape input input_flag = symbolic_helper _node_get inputs node value tolist const_value = all input_flag isinstance input_flag list bool input_flag block_idx = const_value current_b = list n blocks block_idx env = torch _C _jit_pass_onnx_block current_b block operator_export_type env values_in_env True if_output_list = list n outputs current_b_list = list current_b outputs final_b_list = idx range len if_output_list current_b_list idx env raise errors SymbolicValueError f The sub block ATen output current_b_list idx env current_b_list idx type ignore operator onnx_b = env current_b_list idx final_b_list append onnx_b final_b_list old_blocks = tuple n blocks _new_op_outputs new_block_contexts new_node = jit_utils add_op_with_blocks g If inputs outputs=n outputsSize n_blocks=len old_blocks old_block new_block_context zip old_blocks new_block_contexts torch _C _jit_pass_onnx_block old_block new_block_context block operator_export_type env values_in_env False fixed_outputs = torch _C _jit_pass_fixup_onnx_controlflow_node new_node opset_version Run shape type inference If after subblock converted GLOBALS onnx_shape_inference torch _C _jit_pass_onnx_node_shape_type_inference new_node params_dict opset_version fixed_outputs _onnx_symbolic prim Constant prim_constant g jit_utils GraphContext inputs attrs node = g original_node node mustBeNone None This must go before checking string values because some device constants have string values we want keep them unconverted Device types so eq can work them isinstance node output type _C DeviceObjType None node kindOf value == t g op Constant value_t=symbolic_helper _node_get node value node kindOf value == s g op Constant value_s=symbolic_helper _node_get node value node output type isSubtypeOf _C ListType ofInts node output type isSubtypeOf _C ListType ofFloats g op Constant value_t=torch tensor symbolic_helper _node_get node value node output type isSubtypeOf _C ListType ofStrings str_constants = g op Constant value_s=s s symbolic_helper _node_get node value g op prim ListConstruct str_constants raise errors SymbolicValueError f Unsupported prim Constant kind node kindOf value f Please send bug report _constants PYTORCH_GITHUB_ISSUES_URL node output _onnx_symbolic prim type prim_type g jit_utils GraphContext device_value _C Value args kwargs device_value node kind == prim device device = jit_utils get_device_from_value device_value node input device None g op Constant value_s=str device symbolic_helper _unimplemented prim type Device type cannot statically determined device_value _onnx_symbolic onnx Placeholder onnx_placeholder g jit_utils GraphContext inputs attrs node = g original_node block = g block env = g env values_in_env = g values_in_env torch _C _jit_onnx_convert_pattern_from_subblock block node env values_in_env _onnx_symbolic aten resolve_conj _onnx_symbolic aten resolve_neg noop_complex_operators g jit_utils GraphContext input _C Value ONNX does have operators directly manipulate real imaginary components However few torch APIs e g tolist use complex operations when input real which results failures due missing operators complex numbers ` aten resolve_conj ` ` aten resolve_neg ` can safely implemented no-op input _onnx_symbolic aten _conj _onnx_symbolic aten conj_physical unsupported_complex_operators g jit_utils GraphContext input _C Value ONNX does have operators directly manipulate real imaginary components However few torch APIs e g tolist use complex operations when input real which results failures due missing operators complex numbers While ` aten _conj ` ` aten conj_physical ` raise exception when input complex symbolic_helper is_complex_value input FIXME justinchuby report correct name symbolic being executed symbolic_helper _onnx_unsupported aten _conj aten conj_physical input they can safely implemented no-op real numbers only noop_complex_operators g input _onnx_symbolic aten logit logit g jit_utils GraphContext torch _C Value eps torch _C Value one = g op Constant value_t=torch tensor symbolic_helper _is_none eps eps = g op Cast eps to_i=_type_utils JitScalarType from_value onnx_type one_sub_eps = g op Sub one eps self_less_equal_one_sub_eps = g op Greater one_sub_eps temporary_self = g op Where self_less_equal_one_sub_eps one_sub_eps temporary_self_less_eps = g op Less temporary_self eps z = g op Where temporary_self_less_eps eps temporary_self z = sub = g op Sub one z div = g op Div z sub g op Log div