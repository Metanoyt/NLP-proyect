mypy allow-untyped-defs torch torch nn parameter Parameter __all__ list str = _LearnableFakeQuantize torch ao quantization FakeQuantizeBase r Generalized extension FakeQuantize module fake_quantize py This extension FakeQuantize module fake_quantize py which supports more generalized lower-bit quantization supports learning scale zero point parameters through backpropagation In addition attributes original FakeQuantize module _LearnableFakeQuantize module also includes following attributes support quantization parameter learning attr ` channel_len ` defines length channel when initializing scale zero point per channel case attr ` use_grad_scaling ` defines flag whether gradients scale zero point normalized constant which proportional square root number elements tensor The related literature justifying use particular constant can found here https openreview net pdf id=rkgO VKDS attr ` fake_quant_enabled ` defines flag enabling fake quantization output attr ` static_enabled ` defines flag using observer s static estimation scale zero point attr ` learning_enabled ` defines flag enabling backpropagation scale zero point __init__ observer quant_min= quant_max= scale= zero_point= channel_len=- use_grad_scaling=False observer_kwargs super __init__ assert quant_min quant_max quant_min must strictly less than quant_max quant_min = quant_min quant_max = quant_max also pass quant_min quant_max observer observer_kwargs quant_min = quant_min observer_kwargs quant_max = quant_max use_grad_scaling = use_grad_scaling channel_len == - scale = Parameter torch tensor scale zero_point = Parameter torch tensor zero_point assert isinstance channel_len int channel_len Channel size must positive integer scale = Parameter torch tensor scale channel_len zero_point = Parameter torch tensor zero_point channel_len activation_post_process = observer observer_kwargs assert torch iinfo activation_post_process dtype min = quant_min quant_min out bound assert quant_max = torch iinfo activation_post_process dtype max quant_max out bound dtype = activation_post_process dtype qscheme = activation_post_process qscheme ch_axis = activation_post_process ch_axis hasattr activation_post_process ch_axis - register_buffer fake_quant_enabled torch tensor dtype=torch uint register_buffer static_enabled torch tensor dtype=torch uint register_buffer learning_enabled torch tensor dtype=torch uint bitrange = torch tensor quant_max - quant_min + double bitwidth = int torch log bitrange item register_buffer eps torch tensor torch finfo torch float eps torch jit export enable_param_learning r Enable parameter learning over static observer estimates Enables learning quantization parameters disables static observer estimates Forward path returns fake quantized X toggle_qparam_learning enabled=True toggle_fake_quant enabled=True toggle_observer_update enabled=False torch jit export enable_static_estimate Enable static estimates quantization parameters Enables static observer estimates disables learning quantization parameters Forward path returns fake quantized X toggle_qparam_learning enabled=False toggle_fake_quant enabled=True toggle_observer_update enabled=True torch jit export enable_static_observation Enable accumulation data without updating quantization parameters Enables static observer accumulating data input doesn t update quantization parameters Forward path returns original X toggle_qparam_learning enabled=False toggle_fake_quant enabled=False toggle_observer_update enabled=True torch jit export toggle_observer_update enabled=True static_enabled = int enabled type ignore operator torch jit export enable_observer enabled=True toggle_observer_update enabled torch jit export toggle_qparam_learning enabled=True learning_enabled = int enabled type ignore operator scale requires_grad = enabled zero_point requires_grad = enabled torch jit export toggle_fake_quant enabled=True fake_quant_enabled = int enabled torch jit export observe_quant_params print f _LearnableFakeQuantize Scale scale detach print f _LearnableFakeQuantize Zero Point zero_point detach torch jit export calculate_qparams type ignore override scale data clamp_ min=self eps item type ignore operator scale = scale detach zero_point = zero_point detach round clamp quant_min quant_max long scale zero_point forward X static_enabled == type ignore index activation_post_process X detach _scale _zero_point = activation_post_process calculate_qparams _scale = _scale scale device _zero_point = _zero_point zero_point device scale data copy_ _scale zero_point data copy_ _zero_point scale data clamp_ min=self eps item type ignore operator fake_quant_enabled == qscheme torch per_channel_symmetric torch per_tensor_symmetric zero_point data zero_ use_grad_scaling grad_factor = X numel quant_max grad_factor = qscheme torch per_channel_symmetric torch per_channel_affine X = torch _fake_quantize_learnable_per_channel_affine X scale zero_point ch_axis quant_min quant_max grad_factor X = torch _fake_quantize_learnable_per_tensor_affine X scale zero_point quant_min quant_max grad_factor X