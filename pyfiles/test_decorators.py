Owner s module dynamo functools operator os re unittest mock mock unittest mock patch torch torch _dynamo test_case torch _dynamo testing torch _dynamo exc IncorrectUsage Unsupported torch _dynamo utils counters torch testing _internal common_utils skipIfWindows my_custom_function x x + DecoratorTests torch _dynamo test_case TestCase test_disallow_in_graph cnts = torch _dynamo testing CompileCounter torch compile backend=cnts fn x = torch add x = torch add x x = torch sub x x = torch add x x = torch add x x torch _dynamo disallow_in_graph torch sub fn torch randn torch _dynamo allow_in_graph torch sub check graph break sub assertEqual cnts frame_count assertEqual cnts op_count test_disable_for_custom_op torch library torch library Library foo = Library foo DEF noqa TOR foo define custom Tensor - Tensor Dynamic shape data dependent operator For static shape compilation Dynamo should graph break But meta kernel implemented properly torch library impl foo custom CPU foo_cpu x x nonzero Disallow does work because extra python frames torch library python API torch ops foo custom = torch _dynamo disable torch ops foo custom fn x = torch nn functional relu x b = torch ops foo custom c = torch cos b c x = torch randint ref = fn x cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts res = opt_fn x assertEqual cnts frame_count assertEqual ref res test_disable_ignores_outer_wraps orig_inner pass inner pass inner _torchdynamo_orig_callable = orig_inner functools wraps inner wrapper raise AssertionError wrapper called This behavior ideal supporting would add overhead callsites eval_frame innermost_fn A warning would also very noisy torch _dynamo disable fn=wrapper recursive=True test_disable_nn_modules_forward_hook SimpleLinear torch nn Module __init__ - None super __init__ layer = torch nn Linear forward inp layer torch sigmoid inp SimpleModel torch nn Module __init__ - None super __init__ layer = SimpleLinear layer = torch nn Linear forward inp z = layer torch sin inp layer z hook module args inp = args sigmoid inp model = SimpleModel model layer register_forward_pre_hook hook Disable my monkeypatching model layer = torch _dynamo disable model layer cnts = torch _dynamo testing CompileCounterWithBackend eager opt_model = torch compile model backend=cnts opt_model torch randn check no graph break assertEqual cnts frame_count gm = cnts graphs Check first graph has sin node no sigmoid assertTrue any node target torch sin node gm graph nodes assertTrue all node target torch sigmoid node gm graph nodes gm = cnts graphs Check first graph does have sigmoid sigmoid used both hook disabled module assertTrue all node target torch sigmoid node gm graph nodes test_disable_nn_module_with_class_decorator cnts = torch _dynamo testing CompileCounterWithBackend eager torch _dynamo disable SimpleLinear torch nn Module __init__ - None super __init__ layer = torch nn Linear forward inp layer torch sigmoid inp torch compile backend=cnts SimpleModel torch nn Module __init__ - None super __init__ layer = SimpleLinear layer = torch nn Linear forward inp z = layer torch sin inp layer z hook module args inp = args sigmoid inp model = SimpleModel model layer register_forward_pre_hook hook model torch randn check no graph break assertEqual cnts frame_count gm = cnts graphs Check first graph has sin node no sigmoid assertTrue any node target torch sin node gm graph nodes assertTrue all node target torch sigmoid node gm graph nodes gm = cnts graphs Check first graph does have sigmoid sigmoid used both hook disabled module assertTrue all node target torch sigmoid node gm graph nodes test_allow_in_graph cnts = torch _dynamo testing CompileCounter torch compile backend=cnts fn x = torch add x = torch add x x = my_custom_function x x = torch add x x = torch add x x torch _dynamo allow_in_graph my_custom_function fn torch randn torch _dynamo disallow_in_graph my_custom_function check no graph break assertEqual cnts frame_count assertEqual cnts op_count test_allow_in_graph_no_id_reuse cnts = torch _dynamo testing CompileCounter do_allow_in_graph x x + torch _dynamo allow_in_graph do_allow_in_graph del do_allow_in_graph ` id dont_allow_in_graph ` would likely match ` id do_allow_in_graph ` We want make sure Dynamo always trace through ` dont_allow_in_graph ` checking explicit graph break dont_allow_in_graph x torch _dynamo graph_break x + torch compile backend=cnts fn x = torch add x = torch add x x = dont_allow_in_graph x x = torch add x x = torch add x x fn torch randn Check graph break assertEqual cnts frame_count test_incorrect_usage_disallow_in_graph assertRaises IncorrectUsage torch _dynamo disallow_in_graph fn x x cos test_nonstrict_trace_tensor_args torch _dynamo nonstrict_trace trace_me x y z torch _dynamo graph_break x y + z fn x y t = x + t = trace_me x y t t = t + y t t x y = torch randn torch randn opt_fn = torch compile fn fullgraph=True backend= aot_eager ref = fn x y res = opt_fn x y assertEqual ref res test_nonstrict_trace_pre_existing_dict torch _dynamo nonstrict_trace trace_me x d torch _dynamo graph_break x d fn x d t = trace_me x d t + x = torch randn d = opt_fn = torch compile fn fullgraph=True backend= aot_eager ref = fn x d res = opt_fn x d assertEqual ref res test_nonstrict_trace_newly_constructed_dict_with_side_effects torch _dynamo nonstrict_trace trace_me x d torch _dynamo graph_break x d fn x d = d = t = trace_me x d t + x = torch randn opt_fn = torch compile fn fullgraph=True backend= aot_eager ref = fn x res = opt_fn x assertEqual ref res test_nonstrict_trace_pre_existing_dict_with_side_effects torch _dynamo nonstrict_trace trace_me x d torch _dynamo graph_break x d fn x d d = x + t = trace_me x d t + x = torch randn d = d = dict d opt_fn = torch compile fn fullgraph=True backend= aot_eager ref = fn x d res = opt_fn x d assertEqual ref res assertEqual d d test_nonstrict_trace_pre_existing_custom_class Point x torch Tensor y torch Tensor __init__ x y x = x y = y torch utils _pytree register_pytree_node Point lambda p p x p y lambda xy _ Point xy xy torch _dynamo nonstrict_trace trace_me p torch _dynamo graph_break p x p y fn p res = trace_me p res p x p y p = Point torch ones torch ones opt_fn = torch compile fn fullgraph=True backend= aot_eager ref = fn p res = opt_fn p assertEqual ref res test_nonstrict_trace_pre_existing_custom_class_with_side_effects Point x torch Tensor y torch Tensor __init__ x y x = x y = y torch utils _pytree register_pytree_node Point lambda p p x p y lambda xy _ Point xy xy torch _dynamo nonstrict_trace trace_me p torch _dynamo graph_break p x p y fn p p x = p x + p y = p y + res = trace_me p res p x p y p = Point torch ones torch ones p = Point torch ones torch ones opt_fn = torch compile fn fullgraph=True backend= aot_eager ref = fn p res = opt_fn p assertEqual ref res assertEqual p x p x assertEqual p y p y test_nonstrict_trace_newly_constructed_custom_class_with_side_effects Point x torch Tensor y torch Tensor __init__ x y x = x y = y torch utils _pytree register_pytree_node Point lambda p p x p y lambda xy _ Point xy xy torch _dynamo nonstrict_trace trace_me p torch _dynamo graph_break p x p y fn x y p = Point x y p x = p x + p y = p y + res = trace_me p res p x p y x y = torch ones torch ones opt_fn = torch compile fn fullgraph=True backend= aot_eager ref = fn x y res = opt_fn x y assertEqual ref res test_nonstrict_trace_nested_custom_class Point x torch Tensor y torch Tensor __init__ x y x = x y = y PointTensor p Point t torch Tensor __init__ p t p = p t = t torch utils _pytree register_pytree_node PointTensor lambda pt pt p pt t lambda pt _ PointTensor pt pt torch utils _pytree register_pytree_node Point lambda p p x p y lambda xy _ Point xy xy trace_point p torch _dynamo graph_break p x p y torch _dynamo nonstrict_trace trace_point_tensor pt torch _dynamo graph_break pt t + trace_point pt p fn x y p = Point x y t = x + y pt = PointTensor p t res = trace_point_tensor pt res x y = torch ones torch ones opt_fn = torch compile fn fullgraph=True backend= aot_eager ref = fn x y res = opt_fn x y assertEqual ref res test_nonstrict_trace_pre_existing_register_constant_type_guard State __init__ n n = n get_num torch _dynamo graph_break n __eq__ other isinstance other State n == other n __hash__ hash n Assume ` State ` implemented C author didn t bother provide pytree decomposition its instances safe treat constant ` torch compile ` torch utils _pytree register_constant State torch _dynamo nonstrict_trace trace_me x s x s get_num cnts = torch _dynamo testing CompileCounterWithBackend aot_eager torch compile fullgraph=True backend=cnts fn x s res = trace_me x s res x = torch ones Make sure recompilation didn t happen assertEqual cnts frame_count fn x State assertEqual cnts frame_count fn x State assertEqual cnts frame_count Make sure recompilation did happen fn x State assertEqual cnts frame_count test_nonstrict_trace_int_and_float_output torch _dynamo nonstrict_trace trace_me x torch _dynamo graph_break len x shape fn x n n = trace_me x x n + n x = torch randn opt_fn = torch compile fn fullgraph=True backend= aot_eager ref = fn x res = opt_fn x assertEqual ref res test_nonstrict_trace_tuple_and_sym_int_output torch _dynamo nonstrict_trace trace_me x torch _dynamo graph_break x + x size fn x t n = trace_me x t n x = torch randn opt_fn = torch compile fn dynamic=True fullgraph=True backend= aot_eager ref = fn x res = opt_fn x assertEqual ref res test_nonstrict_trace_inside_compiled_function trace_me x torch _dynamo graph_break x + fn x res = torch _dynamo nonstrict_trace trace_me x res + x = torch randn opt_fn = torch compile fn fullgraph=True backend= aot_eager ref = fn x res = opt_fn x assertEqual ref res test_nonstrict_trace_inside_compiled_function_kwarg trace_me x torch _dynamo graph_break x + fn x res = torch _dynamo nonstrict_trace traceable_fn=trace_me x res + x = torch randn opt_fn = torch compile fn fullgraph=True backend= aot_eager ref = fn x res = opt_fn x assertEqual ref res test_nonstrict_trace_on_method Num __init__ n n = n torch _dynamo nonstrict_trace trace_me t torch _dynamo graph_break t + n torch utils _pytree register_pytree_node Num lambda num num n lambda n _ Num n fn x n num = Num n num trace_me x x n = torch randn opt_fn = torch compile fn fullgraph=True backend= aot_eager ref = fn x n res = opt_fn x n assertEqual ref res test_nonstrict_trace_captured_external_tensor cst = torch ones torch _dynamo nonstrict_trace trace_me x y torch _dynamo graph_break x y + cst fn x y trace_me x y x y = torch randn torch randn opt_fn = torch compile fn fullgraph=True backend= aot_eager ref = fn x y res = opt_fn x y assertEqual ref res test_nonstrict_trace_no_action_at_a_distance trace_me x torch _dynamo graph_break x + No effect traceability ` trace_me ` torch _dynamo nonstrict_trace trace_me fn x res = trace_me x res + x = torch randn cnts = torch _dynamo testing CompileCounterWithBackend aot_eager opt_fn = torch compile fn backend=cnts ref = fn x res = opt_fn x assertEqual ref res There should graph break assertEqual cnts frame_count test_nonstrict_trace_inside_compiled_function_error torch compile fullgraph=True backend= aot_eager fn x y trace_me x y torch _dynamo graph_break x y res = torch _dynamo nonstrict_trace trace_me x y res + try fn torch ones torch ones assertFalse True must raise error before except torch _dynamo exc Unsupported e msg = Applying ` nonstrict_trace ` function trace_me however ` nonstrict_trace ` currently requires function defined outside ` torch compile ` region NOQA B assertIn msg str e test_nonstrict_trace_custom_class_error Point x torch Tensor y torch Tensor __init__ x y x = x y = y torch _dynamo nonstrict_trace trace_me p torch _dynamo graph_break p x p y torch compile fullgraph=True backend= aot_eager fn p res = trace_me p res + try p = Point torch ones torch ones fn p assertFalse True must raise error before except torch _dynamo exc Unsupported e assertIn Invalid input type nonstrict_trace-ed function str e test_nonstrict_trace_nested_custom_class_error Point x torch Tensor y torch Tensor __init__ x y x = x y = y PointTensor p Point t torch Tensor __init__ p t p = p t = t torch utils _pytree register_pytree_node PointTensor lambda pt pt p pt t lambda pt _ PointTensor pt pt trace_point p torch _dynamo graph_break p x p y torch _dynamo nonstrict_trace trace_point_tensor pt torch _dynamo graph_break pt t + trace_point pt p torch compile fullgraph=True backend= aot_eager fn x y p = Point x y t = x + y pt = PointTensor p t res = trace_point_tensor pt res try fn torch ones torch ones assertFalse True must raise error before except torch _dynamo exc Unsupported e assertIn Invalid input type nonstrict_trace-ed function str e test_nonstrict_trace_custom_class_output_error Point x torch Tensor y torch Tensor __init__ x y x = x y = y torch _dynamo nonstrict_trace trace_me x torch _dynamo graph_break Point x x + torch compile fullgraph=True backend= aot_eager fn x p = trace_me x p x p y try x = torch ones fn x assertFalse True must raise error before except torch _dynamo exc Unsupported e assertIn Unsupported output type nonstrict_trace-ed function str e test_nonstrict_newly_constructed_trace_register_constant_type_error State __init__ n n = n get_num torch _dynamo graph_break n __eq__ other isinstance other State n == other n __hash__ hash n Assume ` State ` implemented C author didn t bother provide pytree decomposition its instances safe treat constant ` torch compile ` torch utils _pytree register_constant State torch _dynamo nonstrict_trace trace_me x s x s get_num torch compile fullgraph=True backend= aot_eager fn x s = State res = trace_me x s res try x = torch ones fn x assertFalse True must raise error before except torch _dynamo exc Unsupported e assertIn Input marked ` pytree register_constant ` constructed ` torch compile ` region str e test_nonstrict_trace_object_in_context_error Point x torch Tensor y torch Tensor __init__ x y x = x y = y PointTensor p Point t torch Tensor __init__ p t p = p t = t torch utils _pytree register_pytree_node PointTensor lambda pt pt t pt p lambda ts p PointTensor p ts torch _dynamo nonstrict_trace trace_me pt torch _dynamo graph_break pt t + pt p x pt p y torch compile fullgraph=True backend= aot_eager fn x y p = Point x y t = x + y pt = PointTensor p t res = trace_me pt res try x y = torch ones torch ones fn x y assertFalse True must raise error before except torch _dynamo exc Unsupported e assertIn Invalid use pytree_flatten nonstrict_trace-ed function str e test_graph_break cnts = torch _dynamo testing CompileCounter torch compile backend=cnts fn x x = torch cos x x = torch cos x torch _dynamo graph_break x = torch cos x x = torch cos x torch _dynamo graph_break x = torch cos x x = torch cos x x fn torch randn assertEqual cnts frame_count assertEqual cnts op_count test_skip_frame cnts = torch _dynamo testing CompileCounter torch compile backend=cnts fn x x = x + torch _dynamo skip_frame x + inp = torch ones assertEqual fn inp inp + assertEqual cnts frame_count torch compile backend=cnts gn x x = x + torch _dynamo graph_break x = x + torch _dynamo skip_frame x + assertEqual gn inp inp + assertEqual cnts frame_count test_step_unsupported cnts = torch _dynamo testing CompileCounter torch compile backend=cnts fn x x = x + + torch _dynamo step_unsupported x + inp = torch ones assertEqual fn inp inp + assertEqual cnts frame_count assertEqual cnts op_count test_step_unsupported_empty_checkpoint torch compile backend= eager fn x torch _dynamo step_unsupported x + inp = torch ones assertEqual fn inp inp + skipIfWindows msg= TODO xuhancn confirm torch compiler disable work Windows test_disable_recursive_false fn x x + torch _dynamo disable recursive=False fn x torch compiler is_compiling raise RuntimeError bad x = x sigmoid fn x cos fn x fn x tan cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts opt_fn torch randn assertEqual cnts frame_count test applying disable nonrecursive doesn t modify original function fn x torch compiler is_compiling x - fn x + torch compile backend=cnts outer f x f x inp = torch ones fn _disabled = torch _dynamo disable fn recursive=False torch _dynamo reset cnts clear res = outer fn inp assertEqual cnts frame_count assertEqual res inp - cnts clear res = outer fn _disabled inp assertEqual cnts frame_count assertEqual res inp + torch _dynamo reset cnts clear res = outer fn _disabled inp assertEqual cnts frame_count assertEqual res inp + cnts clear res = outer fn inp assertEqual cnts frame_count assertEqual res inp - directly compiling disabled function should result compile torch _dynamo reset cnts clear res = torch compile fn _disabled backend=cnts inp assertEqual cnts frame_count assertEqual res inp - test_disable_recursive_false_weird torch _dynamo types FrameAction FrameExecStrategy test case where next invocation function manually skipped fn x torch compiler is_compiling x - x + fn_disabled = torch _dynamo disable fn recursive=False torch _dynamo eval_frame set_code_exec_strategy fn __code__ FrameExecStrategy FrameAction SKIP FrameAction DEFAULT torch compile backend= eager outer fn x fn x inp = torch ones assertEqual outer fn_disabled inp inp + torch _dynamo eval_frame set_code_exec_strategy fn __code__ FrameExecStrategy FrameAction DEFAULT FrameAction DEFAULT assertEqual torch compile fn backend= eager inp inp - test_substitute_in_graph counters clear NB Choose another C function test when we support operator indexOf out box cnts = torch _dynamo testing CompileCounter fn = operator indexOf opt_fn = torch compile fn backend=cnts out = fn opt_out = opt_fn assertEqual out opt_out assertEqual cnts frame_count assertEqual len counters graph_break torch _dynamo reset counters clear assertRaisesRegex TypeError Signature mismatch torch _dynamo substitute_in_graph operator indexOf _ sequence x i item enumerate sequence item x item == x i raise ValueError sequence index x x sequence torch _dynamo substitute_in_graph operator indexOf polyfill b i item enumerate item b item == b i raise ValueError sequence index x x sequence cnts = torch _dynamo testing CompileCounter fn = operator indexOf opt_fn = torch compile fn backend=cnts fullgraph=True out = fn opt_out = opt_fn assertEqual out opt_out assertEqual cnts frame_count assertEqual len counters graph_break torch _dynamo reset counters clear cnts = torch _dynamo testing CompileCounter fn = polyfill opt_fn = torch compile fn backend=cnts fullgraph=True out = fn opt_out = opt_fn assertEqual out opt_out assertEqual cnts frame_count assertEqual len counters graph_break patch object torch _dynamo config suppress_errors True test_nested_disable_decorator cnts = torch _dynamo testing CompileCounter torch _dynamo disable fn x torch sin x torch compile backend=cnts fn x x = x + x = x + x = fn x graph break x = x + x = x + x torch compile backend=cnts fullgraph=True fn x fn x fn torch randn assertEqual cnts frame_count assertEqual cnts op_count assertRaisesRegex Unsupported r Skip calling ` torch compiler disable\ \ ` d function fn torch randn test_disable_optimize cnt = torch _dynamo testing CompileCounter torch compile backend=cnt disable=True f x x + f torch ones assertEqual cnt frame_count torch compile backend=cnt disable=True f x x + f torch ones assertEqual cnt frame_count patch dict os environ TORCHDYNAMO_DISABLE torch compile backend=cnt f x x + f torch ones assertEqual cnt frame_count test_torch_guards_stack_frame_register_inlining_disable x = torch tensor encoder torch nn Module __init__ y super __init__ = y torch _dynamo disable helper x y x y forward args x = + helper x e = encoder seen_frames = contextlib contextlib contextmanager global_context_capture_fn frame_summary frame_summary None seen_frames append frame_summary yield mock patch torch _guards TracingContext current_frame side_effect=global_context_capture_fn torch compile e backend= eager x assertEqual len seen_frames test_torch_guards_stack_frame_register_inlining_partially_disable y = torch nn Parameter torch tensor x = torch tensor encoder torch nn Module __init__ y super __init__ register_parameter param y torch _dynamo disable helper_disabled x y x sin y cos helper x y x y forward args x = + helper x param + helper_disabled x param e = encoder y cnt = torch _dynamo testing CompileCounter torch compile e backend=cnt x first frame before disable second frame after disable assertEqual cnt frame_count assertEqual cnt op_count _test_mark_static_address guarded This test verifies dynamo properly marks inputs static when using mark_static_address API For both inline_inbuilt_nn_modules True False we expect tensor present buffers attribute graph compiles_with_buffers = compiles = debug_compiler gm _ nonlocal compiles_with_buffers nonlocal compiles compiles_with_buffers += len gm _buffers compiles += gm torch compile backend=debug_compiler fn x x + inp = torch ones torch _dynamo mark_static_address inp guard=guarded fn inp guarded assertEqual compiles_with_buffers inp = torch ones guarded should trigger another recompile since marked static compiles buffers should incremented fn inp guarded assertEqual compiles_with_buffers assertEqual compiles guarded test_mark_static_address_guarded torch _dynamo config patch inline_inbuilt_nn_modules True _test_mark_static_address guarded=True _test_mark_static_address guarded=True test_mark_static_address_unguarded torch _dynamo config patch inline_inbuilt_nn_modules True _test_mark_static_address guarded=False _test_mark_static_address guarded=False test_class_methods A classmethod my_class_method cls arg cls arg staticmethod my_static_method arg None arg my_regular_method arg arg B A my_class_method arg super my_class_method arg my_static_method arg super my_static_method arg C A classmethod my_class_method cls arg super my_class_method arg cnt = torch _dynamo testing CompileCounter torch compile backend=cnt fn b c We want function does graph break does generate custom bytecode v = my_class_method v = A my_class_method v = my_static_method v = A my_static_method v = my_regular_method v = b my_class_method v = b my_static_method v = c my_class_method v = C my_class_method torch rand v v v v v v v v v b c = A B C v v v v v _ v v v = fn b c assertEqual v A assertEqual v A assertEqual v None assertEqual v None assertEqual v TODO fix me we do resolve classmethods properly regular method assertEqual v B assertEqual v None assertEqual v C assertEqual v C assertEqual cnt frame_count test_assume_constant_result_on_user_defined_fn torch _dynamo assume_constant_result const_fn n s torch full n s fn B B = const_fn B size X = B X tolist B_list = B = torch tensor B_list dtype=torch int torch _dynamo decorators mark_static B torch _dynamo config capture_scalar_outputs = True torch _dynamo config capture_dynamic_output_shape_ops = True assertEqual fn B torch compile fn backend= eager fullgraph=True dynamic=True B test_assume_constant_result_on_computation_with_graph_input torch _dynamo assume_constant_result check y y item == fn x y check y x + x + y = torch tensor x = torch tensor assertEqual fn x y torch compile fn x y test_set_stance_aot_eager_then_compile cnts = torch _dynamo testing CompileCounter torch compile backend=cnts fn x y z x y z torch compiler set_stance aot_eager_then_compile fn torch randn torch randn fn torch randn torch randn fn torch randn torch randn Would have been without stance assertEqual cnts op_count torch _dynamo config patch inline_inbuilt_nn_modules True test_mark_static_nn_module torch _dynamo mark_static Mock torch nn Module __init__ c super __init__ c = c forward x x c cnts = torch _dynamo testing CompileCounter mod = Mock mod = Mock mod = Mock opt_mod = torch compile mod backend=cnts fullgraph=True opt_mod = torch compile mod backend=cnts fullgraph=True opt_mod = torch compile mod backend=cnts fullgraph=True x = torch randn opt_mod x opt_mod x opt_mod x Must compilations If marked static there would because c would converted symints assertEqual cnts frame_count test_set_stance_eager_then_compile cnts = torch _dynamo testing CompileCounter torch compile backend=cnts fn x y z x y z torch compiler set_stance eager_then_compile fn torch randn torch randn fn torch randn torch randn fn torch randn torch randn assertEqual cnts frame_count test_set_stance_eager_then_compile_with_graph_break cnts = torch _dynamo testing CompileCounter torch compile backend=cnts fn x y z y = torch sin y torch _dynamo graph_break y = torch cos y x y z torch compiler set_stance eager_then_compile fn torch randn torch randn fn torch randn torch randn fn torch randn torch randn frame count since we added graph break assertEqual cnts frame_count test_set_stance_force_eager torch compile backend= eager x torch _dynamo is_compiling x + x + torch compiler set_stance force_eager b x x c x out = x torch compiler set_stance force_eager out = x out out x inp = torch ones test decorating b has no overall side effect assertEqual inp inp + assertEqual b inp inp + assertEqual c inp inp + inp + inp + torch compiler set_stance force_eager assertEqual inp inp + torch compiler set_stance default assertEqual inp inp + test_set_stance_eager_on_recompile torch compile backend= eager dynamic=False x n torch _dynamo is_compiling x + n + x + n + inp = torch ones out = inp torch compiler set_stance eager_on_recompile out = inp out = inp assertEqual out inp + assertEqual out inp + assertEqual out inp + test_set_stance_fail_on_recompile torch compile backend= eager dynamic=False x n torch _dynamo is_compiling x + n + x + n + inp = torch ones out = inp torch compiler set_stance fail_on_recompile out = inp assertRaisesRegex RuntimeError fail_on_recompile inp assertEqual out inp + assertEqual out inp + test_fail_on_recompile_shows_guard_details torch compile backend= eager dynamic=False f x x + f torch ones f torch ones post_munge s re sub r line number \d+ line number N s torch compiler set_stance fail_on_recompile f torch ones assertExpectedInlineMunged RuntimeError lambda f torch ones \ Detected recompile when torch compile stance fail_on_recompile filename test_decorators py function name f line number N triggered following guard failure s - tensor x size mismatch index expected actual - tensor x size mismatch index expected actual noqa B post_munge=post_munge test_set_stance_fail_on_recompile_with_disable torch compiler disable inner x x torch compile backend= eager f x inner x f torch randn should raise error torch compiler set_stance fail_on_recompile f torch randn test_set_stance_forbid_in_graph torch compiler set_stance force_eager x x + torch compile backend= eager b x x assertRaisesRegex AssertionError Attempt trace forbidden callable b torch ones torch compile backend= eager c x torch compiler set_stance force_eager x + assertRaisesRegex AssertionError Attempt trace forbidden callable c torch ones torch compile backend= eager torch compiler set_stance force_eager d x x + assertRaisesRegex AssertionError Attempt trace forbidden callable d torch ones torch compile backend= eager e x torch _dynamo set_stance force_eager x + assertRaisesRegex AssertionError Attempt trace forbidden callable e torch ones torch compile backend= eager f x torch _dynamo eval_frame _set_stance force_eager x + assertRaisesRegex AssertionError Attempt trace forbidden callable f torch ones torch compile backend= eager g x torch _dynamo skip_frame NOTE torch _dynamo is_compiling will get traced true torch compiler is_compiling skipped will false torch compiler is_compiling raise RuntimeError Expect frame skipped should traced eval frame callback still set torch compiler set_stance force_eager x + assertRaisesRegex RuntimeError set_stance torch compile g torch ones test_set_stance_force_backend torch compile x x + cnts = torch _dynamo testing CompileCounter torch compiler set_stance default force_backend=cnts b x x b torch ones assertEqual cnts frame_count torch compiler set_stance default force_backend= eager c x x just make sure doesn t crash c torch ones assertRaisesRegex RuntimeError force_backend torch compiler set_stance force_eager force_backend= eager d x pass test_set_stance_force_backend_with_disable torch compiler disable inner x x torch compile backend= eager f x inner x f torch randn fail_backend gm ex raise RuntimeError fail should raise error torch compiler set_stance default force_backend=fail_backend f torch randn also tests lot torch _dynamo patch_dynamo_config functionality test_dont_skip_tracing torch _dynamo test_dont_skip_tracing_functions f f f f f cnts = torch _dynamo testing CompileCounter make sure test_dont_skip_tracing_functions actually skipped trace rules torch compile f backend=cnts torch randn assertEqual cnts frame_count f _unskip = torch _dynamo dont_skip_tracing f basic test g x f _unskip x cnts clear torch compile g backend=cnts fullgraph=True torch randn assertEqual cnts frame_count test dont_skip_tracing traceable g x torch _dynamo dont_skip_tracing f x cnts clear torch compile g backend=cnts fullgraph=True torch randn assertEqual cnts frame_count test dont_skip_tracing recursive applied non-skipped function torch _dynamo dont_skip_tracing g x f x cnts clear torch compile g backend=cnts fullgraph=True torch randn assertEqual cnts frame_count test dont_skip_tracing recursive applied skipped function f _unskip = torch _dynamo dont_skip_tracing f cnts clear torch compile f _unskip backend=cnts fullgraph=True torch randn assertEqual cnts frame_count test dont_skip_tracing graph breaks inp = torch ones res = torch compile f backend=cnts inp assertEqual res inp + torch compile backend=cnts g x x = f x x = torch _dynamo dont_skip_tracing f x x = f x x res = g inp assertEqual res inp + test nested dont_skip_tracing also happens test previously skipped frame f can actually compiled called top-level function case graph break TODO reset necessary now since attempting trace f previously resulted unconditional skip torch _dynamo reset f _unskip = torch _dynamo dont_skip_tracing f res = torch compile f _unskip backend=cnts inp assertEqual res inp + test dont_skip_tracing activated outside torch compile f _unskip = torch _dynamo dont_skip_tracing torch compile f backend=cnts res = f _unskip inp assertEqual res inp + test context manager inside torch compile backend=cnts g x x = f x torch _dynamo dont_skip_tracing x = f x torch _dynamo graph_break x = f x x = f x x res = g inp assertEqual res inp + test context manager outside torch _dynamo dont_skip_tracing res = torch compile f backend=cnts inp assertEqual res inp + test skipped function different dont_skip_tracing regions torch compile backend=cnts g x fn = f torch _dynamo dont_skip_tracing fn = f x = fn x x = fn x x res = g inp assertEqual res inp + test_patch_dynamo_config_errors torch compile backend= eager f x torch _dynamo patch_dynamo_config nonexistent=False x + assertRaisesRegex Exception patch_dynamo_config does support f torch randn torch compile backend= eager f x torch _dynamo patch_dynamo_config verbose x + assertRaisesRegex Exception patch_dynamo_config does support non-safe-constant f torch randn torch compile backend= eager f x torch _dynamo patch_dynamo_config recompile_limit x + assertRaisesRegex Exception patch_dynamo_config does support f torch randn torch compile backend= eager f x torch _dynamo patch_dynamo_config verbose=object x + assertRaisesRegex Exception Cannot convert patch_dynamo_config args kwargs constants f torch randn test_error_on_graph_break cnts = torch _dynamo testing CompileCounter torch _dynamo error_on_graph_break True torch compile backend=cnts f x x = x + torch _dynamo error_on_graph_break False torch _dynamo graph_break x + inp = torch ones assertEqual f inp inp + assertEqual cnts frame_count torch compile backend=cnts f x x = x + torch _dynamo error_on_graph_break True torch _dynamo graph_break x + assertRaises Unsupported f inp torch _dynamo error_on_graph_break True torch compile backend=cnts f x x = x + torch _dynamo error_on_graph_break False torch _dynamo graph_break x = x + torch _dynamo graph_break x + cnts clear assertEqual f inp inp + assertEqual cnts frame_count inner_f x x = x + torch _dynamo graph_break x + torch _dynamo error_on_graph_break True torch compile backend=cnts f x x = x + torch _dynamo error_on_graph_break False torch _dynamo skip_frame inner_f x cnts clear assertEqual f inp inp + assertEqual cnts frame_count test_error_on_graph_break_nested error_on_graph_break nested frame cnts = torch _dynamo testing CompileCounter torch _dynamo error_on_graph_break False inner_f x x = x + torch _dynamo graph_break x + torch _dynamo error_on_graph_break True torch compile backend=cnts f x x = x + inner_f x inp = torch ones assertEqual f inp inp + assertEqual cnts frame_count inner_f x x = x + torch _dynamo error_on_graph_break False torch _dynamo graph_break x + torch _dynamo error_on_graph_break True torch compile backend=cnts f x x = x + inner_f x cnts clear assertEqual f inp inp + assertEqual cnts frame_count inner_f x x = x + torch _dynamo error_on_graph_break True torch _dynamo graph_break x + torch _dynamo error_on_graph_break False torch compile backend=cnts f x x = x + inner_f x assertRaises Unsupported f inp test_error_on_graph_break_nested_with_skip error_on_graph_break nested frame skipped frame between cnts = torch _dynamo testing CompileCounter torch _dynamo error_on_graph_break False inner _f x x = x + torch _dynamo graph_break x + inner _f x torch _dynamo error_on_graph_break False torch _dynamo skip_frame inner _f x torch _dynamo error_on_graph_break True torch compile backend=cnts f x x = x + inner _f x inp = torch ones assertEqual f inp inp + assertEqual cnts frame_count inner _f x x = x + torch _dynamo error_on_graph_break True torch _dynamo graph_break x + torch _dynamo disable recursive=False inner _f x inner _f x torch _dynamo error_on_graph_break False torch compile backend=cnts f x x = x + inner _f x assertRaises Unsupported f inp test export error_on_graph_break False still errors test_error_on_graph_break_export torch _dynamo error_on_graph_break False inner x x = x + torch _dynamo graph_break x + f x x = x + inner x assertRaises Unsupported torch _dynamo export f torch ones test_error_on_graph_break_nested_deep cnts = torch _dynamo testing CompileCounter inner _f x x = x + torch _dynamo graph_break x + inner _f x inner _f x inner _f x torch _dynamo error_on_graph_break False inner _f x inner _f x inner _f x torch _dynamo error_on_graph_break True torch compile backend=cnts f x x = x + inner _f x inp = torch ones assertEqual f inp inp + assertEqual cnts frame_count inner _f x x = x + torch _dynamo graph_break x + inner _f x inner _f x inner _f x torch _dynamo error_on_graph_break True inner _f x inner _f x inner _f x torch _dynamo error_on_graph_break False torch compile backend=cnts f x x = x + inner _f x assertRaises Unsupported f inp test_error_on_graph_break_error torch compile backend= eager f torch _dynamo error_on_graph_break foo= bar pass torch compile backend= eager f torch _dynamo error_on_graph_break pass torch compile backend= eager f torch _dynamo error_on_graph_break foo pass assertRaises Exception f assertRaises Exception f assertRaises Exception f test_nested_compile_error_on_graph_break inp = torch ones torch _dynamo error_on_graph_break True torch compile backend= eager inner_f x x = x + torch _dynamo graph_break x + torch _dynamo error_on_graph_break False torch compile backend= eager f x inner_f x assertRaises Unsupported f inp torch _dynamo error_on_graph_break False torch compile backend= eager inner_f x x = x + torch _dynamo graph_break x + torch _dynamo error_on_graph_break True torch compile backend= eager f x inner_f x assertEqual f inp inp + test_error_on_graph_break_fullgraph Test error_on_graph_break=False cannot override fullgraph=True inp = torch ones torch compile backend= eager fullgraph=True f x x = x + torch _dynamo error_on_graph_break False torch _dynamo graph_break x + assertRaises Unsupported f inp test_error_on_graph_break_empty_graph torch _dynamo error_on_graph_break True torch compile backend= eager f assertEqual f test_error_on_graph_break_nonempty_checkpoint cnts = torch _dynamo testing CompileCounter torch compile backend=cnts fn x x = x + x = x + x = x + torch _dynamo error_on_graph_break True torch _dynamo graph_break x + assertRaises Unsupported fn torch ones assertEqual cnts frame_count test_nested_compile_fullgraph Test fullgraph=True cannot toggled back fullgraph=False inp = torch ones torch compile backend= eager fullgraph=True inner_f x torch _dynamo graph_break x + torch compile backend= eager fullgraph=False outer_f x inner_f x assertRaises Unsupported outer_f inp torch compile backend= eager fullgraph=False inner_f x torch _dynamo graph_break x + torch compile backend= eager fullgraph=True outer_f x inner_f x assertRaises Unsupported outer_f inp __name__ == __main__ torch _dynamo test_case run_tests run_tests