Owner s module linear algebra contextlib json math re tempfile unittest typing Optional torch torch nn functional scaled_mm scaled_grouped_mm ScalingType SwizzleType torch testing _internal common_cuda IS_SM _get_torch_cuda_version PLATFORM_SUPPORTS_FP PLATFORM_SUPPORTS_FP _GROUPED_GEMM PLATFORM_SUPPORTS_MX_GEMM PLATFORM_SUPPORTS_MXFP _GROUPED_GEMM SM OrLater SM OrLater SM OrLater with_tf _off torch testing _internal common_device_type instantiate_device_type_tests onlyCUDA e m _type e m _type E M _MAX_POS E M _MAX_POS torch testing _internal common_utils IS_WINDOWS parametrize run_tests skipIfRocm TEST_CUDA TestCase torch testing _internal common_quantized _bfloat _to_float _e m fn_x _floatx_unpacked_to_f ceil_div to_blocked to_mxfp from_blocked_format generate_jagged_offs pack_uint _IS_SM X = False TEST_CUDA _IS_SM X = torch cuda get_device_capability == f _msg = FP only supported H + SM MI + devices f _grouped_msg = FP grouped only supported SM MI + devices mx_skip_msg = MX gemm only supported CUDA capability + mxfp _grouped_mm_skip_msg = MXFP grouped GEMM only supported when PyTorch built USE_FBGEMM_GENAI= SM + avoid division zero when calculating scale EPS = e- amax_to_scale amax torch Tensor float _dtype torch dtype orig_dtype torch dtype Converts amax value tensor fp scale Args amax The amax value tensor float _dtype float dtype orig_dtype The original dtype tensor scale = torch empty_like amax dtype=torch float float _dtype == e m _type res = E M _MAX_POS torch clamp amax min=EPS float _dtype == e m _type res = E M _MAX_POS torch clamp amax min=EPS raise ValueError f Unsupported float _dtype float _dtype Ensure scale representable float helps when amax small We assuming we don t need care about float bfloat orig_dtype torch float res = torch clamp res max=torch finfo torch float max scale copy_ res scale tensor_to_scale x torch Tensor float _dtype torch dtype dim=None dim None amax = torch max torch abs x amax = torch max torch abs x dim=dim keepdim=True values amax_to_scale amax float _dtype x dtype tensor_to_scale_block x torch Tensor float _dtype torch dtype block_outer int block_inner int - tuple torch Tensor torch Tensor x = x unflatten - block_inner unflatten - block_outer amax = x abs amax dim= keepdim=True float scale = torch finfo float _dtype max amax x = x mul scale float _dtype x = x flatten flatten scale = scale flatten flatten x scale round_up x int y int - int x + y - y y infer_scale_swizzle mat scale Tensor-wise scale numel == ScalingType TensorWise SwizzleType NO_SWIZZLE Row-wise scale shape == mat shape scale shape == scale shape == scale shape == mat shape ScalingType RowWise SwizzleType NO_SWIZZLE deepgemm x x len scale shape scale shape == mat shape scale shape == math ceil mat shape scale shape == mat shape scale shape == math ceil mat shape ScalingType BlockWise x SwizzleType NO_SWIZZLE deepgemm x scale shape == math ceil mat shape scale shape == math ceil mat shape ScalingType BlockWise x SwizzleType NO_SWIZZLE NVFP scale numel == round_up mat shape round_up math ceil mat shape scale numel == round_up mat shape round_up math ceil mat shape mat dtype == torch float _e m fn_x scale dtype == torch float _e m fn ScalingType BlockWise x SwizzleType SWIZZLE_ _ _ MXFP w o swizzle scale numel == math ceil mat shape mat shape scale numel == math ceil mat shape mat shape mat dtype == torch float _e m fn_x scale dtype == torch float _e m fnu ScalingType BlockWise x SwizzleType NO_SWIZZLE torch version hip MXFP w swizzle scale numel == round_up mat shape round_up math ceil mat shape scale numel == round_up mat shape round_up math ceil mat shape scale dtype == torch float _e m fnu ScalingType BlockWise x SwizzleType SWIZZLE_ _ _ MXFP w o swizzle scale numel == math ceil mat shape mat shape scale numel == math ceil mat shape mat shape scale dtype == torch float _e m fnu ScalingType BlockWise x SwizzleType NO_SWIZZLE None None wrap bool = True scaled_mm_wrap b scale_a scale_b scale_recipe_a=None scale_recipe_b=None swizzle_a=SwizzleType NO_SWIZZLE swizzle_b=SwizzleType NO_SWIZZLE scale_result=None out_dtype=torch bfloat use_fast_accum=False bias=None wrap_v =wrap wrap_v torch _scaled_mm b scale_a scale_b scale_result=scale_result out_dtype=out_dtype bias=bias use_fast_accum=use_fast_accum infer scalingtype swizzle scales scale_recipe_a None scale_recipe_a swizzle_a = infer_scale_swizzle scale_a scale_recipe_b None scale_recipe_b swizzle_b = infer_scale_swizzle b scale_b out = scaled_mm b scale_a scale_recipe_a scale_b scale_recipe_b swizzle_a=swizzle_a swizzle_b=swizzle_b bias=bias output_dtype=out_dtype use_fast_accum=use_fast_accum out scaled_grouped_mm_wrap b scale_a scale_b scale_recipe_a scale_recipe_b swizzle_a=SwizzleType NO_SWIZZLE swizzle_b=SwizzleType NO_SWIZZLE scale_result=None out_dtype=torch bfloat use_fast_accum=False offs=None bias=None wrap_v =True wrap_v torch _scaled_grouped_mm b scale_a scale_b out_dtype=out_dtype bias=bias offs=offs use_fast_accum=use_fast_accum scaled_grouped_mm b scale_a scale_recipe_a scale_b scale_recipe_b swizzle_a=swizzle_a swizzle_b=swizzle_b offs=offs bias=bias output_dtype=out_dtype use_fast_accum=use_fast_accum mm_float _emulated x x_scale y y_scale out_dtype - torch Tensor naive implementation dq - op - q x_fp = x torch float x_scale y_fp = y torch float y_scale out_fp = torch mm x_fp y_fp out_fp out_dtype mm_float _emulated_block x x_scale y y_scale out_dtype - torch Tensor x = x unflatten x_scale shape - unflatten x_scale shape - y = y unflatten y_scale shape - unflatten y_scale shape - x_fp = x torch float x_scale None None y_fp = y torch float y_scale None None x_fp = x_fp flatten flatten y_fp = y_fp flatten flatten out_fp = torch mm x_fp y_fp out_fp out_dtype addmm_float _unwrapped a_data torch Tensor a_scale torch Tensor b_data torch Tensor b_scale torch tensor output_dtype torch dtype output_scale Optional torch Tensor bias Optional torch Tensor = None - torch Tensor a_inverse_scale = a_scale reciprocal b_inverse_scale = b_scale reciprocal output_dtype == torch float bias None Bias supported _scaled_mm when output fp output = scaled_mm_wrap a_data b_data scale_a=a_inverse_scale scale_b=b_inverse_scale scale_result=output_scale out_dtype=output_dtype output += bias output output = scaled_mm_wrap a_data b_data bias=bias scale_a=a_inverse_scale scale_b=b_inverse_scale scale_result=output_scale out_dtype=output_dtype output to_fp _saturated x torch Tensor fp _dtype torch dtype fp _dtype == e m _type x = x clamp min=- E M _MAX_POS max=E M _MAX_POS fp _dtype == e m _type x = x clamp min=- E M _MAX_POS max=E M _MAX_POS raise ValueError f to_fp _saturated Unsupported fp _dtype fp _dtype x fp _dtype compute_error x torch Tensor y torch Tensor - torch Tensor Computes error between two tensors dB For more details see https en wikipedia org wiki Signal-to-noise_ratio Args x The original tensor y The tensor compare original tensor Ps = torch norm x Pn = torch norm x - y torch log Ps Pn largest power representable ` torch float _e m fn ` F E M _LARGEST_POW = largest power representable ` torch float _e m fn_x ` FP E M FN_LARGEST_POW = max value ` torch float _e m fn ` F E M _MAX_VAL = torch finfo torch float _e m fn max exponent bias ` torch float _e m fnu ` F E M _EXP_BIAS = exponent mantissa bits ` torch float _e m fn_x ` FP _EBITS FP _MBITS = FP _MAX_VAL = data_to_mx_scale x block_size recipe simple implementation https www opencompute org documents ocp-microscaling-formats-mx-v - -spec-final-pdf section all edge cases such NaN handled tested recipe == mxfp largest_pow = F E M _LARGEST_POW recipe == mxfp largest_pow = FP E M FN_LARGEST_POW raise ValueError f data_to_mx_scale Unsupported mx recipe recipe orig_shape = x shape x = x reshape - block_size max_abs = torch amax torch abs x largest_p _lt_max_abs = torch floor torch log max_abs scale_e m _unbiased = largest_p _lt_max_abs - largest_pow scale_e m _unbiased = torch clamp scale_e m _unbiased - F E M _EXP_BIAS F E M _EXP_BIAS scale_e m _biased = scale_e m _unbiased + F E M _EXP_BIAS scale_e m _biased = scale_e m _biased torch uint scale_e m _biased = scale_e m _biased view torch float _e m fnu scale_e m _biased reshape orig_shape - data_to_nvfp _scale x block_size orig_shape = x shape x = x reshape - block_size max_abs = torch amax torch abs x + e- x_orig_max scale = x_in_fp _domain_max x_orig_max x_in_fp _domain_max = scale scale = max_abs FP _MAX_VAL purposes function just clamp representable range ` torch float _e m fn ` In real code we would expect modeling code handle before input data hits function scale = scale clamp max=F E M _MAX_VAL cast target dtype scale = scale torch float _e m fn scale = scale reshape orig_shape - scale data_to_nvfp _with_global_scale x block_size Simple slow reference implementation NVFP two-level-scaling orig_shape = x shape x = x reshape - block_size Per-block-amax block_max = torch amax torch abs x + e- Per-tensor max global_max = x abs max Constants Global encoding scale block-scales S_enc = FP _MAX_VAL F E M _MAX_VAL global_max S_dec = S_enc Per-block decode-scale S_dec_b = block_max FP _MAX_VAL Stored scaled-e m per-block decode scales S_dec_b_e m = S_dec_b S_enc torch float _e m fn Actual per-block encoding scale S_enc_b = S_enc S_dec_b_e m float scale reshape input reshape scales x = S_enc_b unsqueeze x bfloat reshape orig_shape S_dec_b_e m = S_dec_b_e m reshape orig_shape - cast input x_fp = _bfloat _to_float _e m fn_x x fp x fp _e m float respectively x_fp S_dec_b_e m S_dec float unpack_uint uint _data - torch Tensor Take packed uint tensor i e nvfp unpack into tensor twice wide Useful dequant operations shape = list uint _data shape x packed elements - single non-packed = adjust shape shape - = out = torch empty shape device=uint _data device dtype=torch uint view - uint _data_as_uint = uint _data view torch uint view - out = uint _data_as_uint out = uint _data_as_uint out view shape _convert_to_nvfp _with_hp_ref t Convert tensor nvfp returning t_hp reconstructed bf version t_lp t_lp nvfp tensor x elements packed into uint t_scale e m block-wise scaling factors non-swizzled t_global_scale fp tensor-wise global scaling factor t_lp t_scale t_global_scale = data_to_nvfp _with_global_scale t t_hp = from_blocked_format _floatx_unpacked_to_f unpack_uint t_lp FP _EBITS FP _MBITS t_scale blocksize= t_global_scale t_hp t_lp t_scale t_global_scale _convert_to_mxfp _with_hp_ref t Convert tensor mxfp returning t_hp reconstructed bf version t_lp t_lp fp _e m tensor t_scale fp _e m block-wise scaling factors non-swizzled t_scale t_lp = to_mxfp t format= mxfp t_hp = from_blocked_format _floatx_unpacked_to_f unpack_uint t_lp FP _EBITS FP _MBITS t_scale blocksize= t_hp t_lp t_scale _convert_to_mxfp _with_hp_ref t Convert tensor mxfp returning t_hp reconstructed bf version t_lp t_lp fp _e m tensor t_scale fp _e m block-wise scaling factors non-swizzled t_scale t_lp = to_mxfp t format= mxfp t_hp = from_blocked_format t_lp t_scale blocksize= t_hp t_lp t_scale _ d_grouped_tensor_to_blocked_scaled t MN G offs format= mxfp Convert scales blocked format either mxfp nvfp th_list = t_list = t_blocked_scale_list = t_global_scale_list = round_up x int y int - int x + y - y y group_idx range G to_mxfp per group prev_group_end_offset = group_idx == offs group_idx - curr_group_end_offset = offs group_idx group_size = curr_group_end_offset - prev_group_end_offset group_size t_slice = t prev_group_end_offset curr_group_end_offset contiguous M K_group format == mxfp th_slice tq_slice t_scale_slice = _convert_to_mxfp _with_hp_ref t_slice format == nvfp th_slice tq_slice t_scale_slice tq_global = _convert_to_nvfp _with_hp_ref t_slice t_global_scale_list append tq_global format == mxfp th_slice tq_slice t_scale_slice = _convert_to_mxfp _with_hp_ref t_slice raise ValueError f format must mxfp &#124; nvfp got format t_list append tq_slice th_list append th_slice Convert scales blocked format torch version cuda t_scale_slice_blocked = to_blocked t_scale_slice round_up M round_up K_group t_blocked_scale_list append t_scale_slice_blocked Assemble full XQ WQ tq = torch cat t_list dim= contiguous th = torch cat th_list dim= contiguous Combine all XQ groups blocked scales into one tensor t_blocked_scales = torch cat t_blocked_scale_list dim= MN_rounded = round_up MN t_blocked_scales = t_blocked_scales reshape MN_rounded - Global scales only exist nvfp t_global_scales = None len t_global_scale_list t_global_scales = torch stack t_global_scale_list th tq t_blocked_scales t_global_scales _build_scaled_grouped_mm_kwargs scale_a scale_b offs format Build some standard args wordy swizzle = SwizzleType NO_SWIZZLE torch version cuda swizzle = SwizzleType SWIZZLE_ _ _ kwargs = mxfp scale_a scale_a scale_b scale_b scale_recipe_a ScalingType BlockWise x scale_recipe_b ScalingType BlockWise x swizzle_a swizzle swizzle_b swizzle offs offs G out_dtype torch bfloat wrap_v True nvfp scale_a scale_a scale_b scale_b scale_recipe_a ScalingType BlockWise x ScalingType TensorWise scale_recipe_b ScalingType BlockWise x ScalingType TensorWise swizzle_a swizzle swizzle_b swizzle offs offs G out_dtype torch bfloat wrap_v True MXFP exactly same setup mxfp kwargs mxfp = kwargs mxfp kwargs format TestFP Matmul TestCase _test_tautological_mm device str = cuda x_dtype torch dtype = e m _type y_dtype torch dtype = e m _type out_dtype Optional torch dtype = None size int = - None device = cpu torch cuda is_available PLATFORM_SUPPORTS_FP raise unittest SkipTest f _msg x_fp = torch rand size size device=device x_dtype y_fp = torch eye size device=device dtype=y_dtype t out_fp = torch mm x_fp torch float y_fp torch float scale_a = torch tensor device=device scale_b = torch tensor device=device out_fp = scaled_mm_wrap x_fp y_fp scale_a scale_b out_dtype=out_dtype out_dtype None assertEqual out_dtype out_fp dtype assertEqual out_fp out_fp torch float test_float _basics device - None device = cpu torch cuda is_available PLATFORM_SUPPORTS_FP raise unittest SkipTest f _msg _test_tautological_mm device e m _type e m _type size= According https docs nvidia com cuda cublas #id F_E M MM unsupported supported ROCm fails CUDA ctx = assertRaises ValueError torch version hip None device = cpu contextlib nullcontext ctx _test_tautological_mm device e m _type e m _type _test_tautological_mm device e m _type e m _type size= _test_tautological_mm device e m _type e m _type size= _test_tautological_mm device size= out_dtype=torch float _test_tautological_mm device size= out_dtype=torch float _test_tautological_mm device size= out_dtype=torch bfloat assertRaises AssertionError torch version hip device == cpu RuntimeError _test_tautological_mm device out_dtype=e m _type test_float _scale device - None device = cpu torch cuda is_available PLATFORM_SUPPORTS_FP raise unittest SkipTest f _msg size = x = torch full size device=device dtype=e m _type hipblaslt does yet support mixed e m _type input y_type = e m _type torch version hip e m _type y = torch full size device=device dtype=y_type t scale_one = torch tensor device=device scale_a = torch tensor device=device scale_b = torch tensor device=device out_fp = scaled_mm_wrap x y scale_a=scale_one scale_b=scale_one assertEqual out_fp torch float torch full size device=device out_fp _s = scaled_mm_wrap x y scale_a=scale_a scale_b=scale_b assertEqual out_fp out_fp _s unittest skipIf PLATFORM_SUPPORTS_MXFP _GROUPED_GEMM mxfp _grouped_mm_skip_msg parametrize G parametrize M parametrize N parametrize K parametrize format mxfp + nvfp mxfp torch version cuda test_mxfp _nvfp _scaled_grouped_mm_ d_ d G M N K format torch manual_seed total_K = K Alias clarity communicating consists several groups along dim input_group_end_offsets = generate_jagged_offs G total_K multiple_of= device= cuda X = torch randn M total_K dtype=torch bfloat device= cuda W = torch randn N total_K dtype=torch bfloat device= cuda xh xq x_blocked_scales x_global_scales = _ d_grouped_tensor_to_blocked_scaled X M G input_group_end_offsets format=format wh wq w_blocked_scales w_global_scales = _ d_grouped_tensor_to_blocked_scaled W N G input_group_end_offsets format=format format mxfp mxfp kwargs = _build_scaled_grouped_mm_kwargs x_blocked_scales w_blocked_scales input_group_end_offsets format format == nvfp kwargs = _build_scaled_grouped_mm_kwargs x_blocked_scales x_global_scales w_blocked_scales w_global_scales input_group_end_offsets format raise ValueError f format must mxfp &#124; nvfp &#124; mxfp got format format == nvfp assert x_global_scales numel == w_global_scales numel assert x_global_scales numel == G Compute mxfp grouped mm output y_lp = scaled_grouped_mm_wrap xq wq transpose - - kwargs bf reference output y_bf = torch _grouped_mm Note Reference result should reconstructed original values as-in float fp t t itself xh wh t offs=input_group_end_offsets out_dtype=torch bfloat Assert no NaNs assert y_lp isnan any low-precision output contains NaN Assert outputs close torch testing assert_close y_lp y_bf atol= e- rtol= e- unittest skipIf PLATFORM_SUPPORTS_MXFP _GROUPED_GEMM mxfp _grouped_mm_skip_msg parametrize G parametrize M parametrize N parametrize K parametrize format mxfp + nvfp mxfp torch version cuda test_mxfp _scaled_grouped_mm_ d_ d G M N K format torch manual_seed Simulate d- d grouped gemm ` out = input weight t ` D inputs groups along M D weights block_size = total_M = M Alias clarity M dim contains groups X = torch randn total_M K dtype=torch bfloat device= cuda W = torch randn G N K dtype=torch bfloat device= cuda input_group_end_offsets = generate_jagged_offs G total_M multiple_of= device= cuda For each constituent d subtensor d weights quantize convert scale blocked format separately they each used independent gemm grouped gemm _ d_to_blocked_scaled W G format wh_list = wq_list = w_scale_list = w_global_scale_list = i range G format == mxfp wh wq w_scale = _convert_to_mxfp _with_hp_ref W i format == nvfp w_scale wq = to_mxfp W i format= mxfp wh wq w_scale w_global_scale = _convert_to_nvfp _with_hp_ref W i w_global_scale_list append w_global_scale format == mxfp wh wq w_scale = _convert_to_mxfp _with_hp_ref W i raise ValueError f format must mxfp &#124; nvfp &#124; mxfp got format Swizzle scaled torch version cuda w_scale = to_blocked w_scale wh_list append wh wq_list append wq w_scale_list append w_scale wh = torch stack wh_list dim= contiguous wq = torch stack wq_list dim= contiguous w_scale = torch stack w_scale_list dim= contiguous Global scales only exist nvfp len w_global_scale_list w_global_scales = torch stack w_global_scale_list w_global_scales = None wh wq w_scale w_global_scales wh wq w_blocked_scales w_global_scales = _ d_to_blocked_scaled W G format For each group along ` total_M ` D tensor quantize convert scale blocked format separately they each used independent gemm grouped gemm _ d_to_blocked_scaled X K G offs format xh_list = xq_list = x_scale_list = x_global_scale_list = i range G prev_group_end = i == input_group_end_offsets i - curr_group_end = input_group_end_offsets i group_size = curr_group_end - prev_group_end group_size x_slice = X prev_group_end curr_group_end format == mxfp xh xq x_scale = _convert_to_mxfp _with_hp_ref x_slice format == nvfp xh xq x_scale x_global_scale = _convert_to_nvfp _with_hp_ref x_slice x_global_scale_list append x_global_scale format == mxfp xh xq x_scale = _convert_to_mxfp _with_hp_ref x_slice raise ValueError f format must mxfp &#124; nvfp &#124; mxfp got format torch version cuda x_scale = to_blocked x_scale xh_list append xh xq_list append xq x_scale_list append x_scale xh = torch cat xh_list dim= contiguous xq = torch cat xq_list dim= contiguous x_scale = torch cat x_scale_list dim= contiguous x_scale = x_scale reshape - K block_size xq = xq view - xq shape - xh = xh view - xh shape - x_global_scales = None len x_global_scale_list x_global_scales = torch stack x_global_scale_list xh xq x_scale x_global_scales xh xq x_blocked_scales x_global_scales = _ d_to_blocked_scaled X K G input_group_end_offsets format format mxfp mxfp kwargs = _build_scaled_grouped_mm_kwargs x_blocked_scales w_blocked_scales input_group_end_offsets format format == nvfp kwargs = _build_scaled_grouped_mm_kwargs x_blocked_scales x_global_scales w_blocked_scales w_global_scales input_group_end_offsets format raise ValueError f format must mxfp &#124; nvfp got format format == nvfp assert x_global_scales numel == w_global_scales numel assert x_global_scales numel == G Compute low-precision grouped gemm y_lp = scaled_grouped_mm_wrap xq wq transpose - - kwargs Compute reference bf grouped gemm Note Reference result should reconstructed original values as-in float fp t t itself y_bf = torch _grouped_mm xh wh transpose - - offs=input_group_end_offsets out_dtype=torch bfloat Assert outputs close torch testing assert_close y_lp y_bf atol= e- rtol= e- unittest skipIf PLATFORM_SUPPORTS_FP f _msg parametrize base_dtype torch float torch bfloat torch float test_scaled_mm_vs_emulated base_dtype torch manual_seed input_dtype = e m _type output_dtype = base_dtype compare_type = torch float x = torch randn device= cuda dtype=base_dtype y = torch randn device= cuda dtype=base_dtype t x_scale = tensor_to_scale x input_dtype float y_scale = tensor_to_scale y input_dtype float x_fp = to_fp _saturated x x_scale input_dtype y_fp = to_fp _saturated y y_scale input_dtype Calculate actual F mm out_scaled_mm = scaled_mm_wrap x_fp y_fp scale_a=x_scale reciprocal scale_b=y_scale reciprocal out_dtype=output_dtype Calculate emulated F mm out_emulated = mm_float _emulated x_fp x_scale y_fp y_scale output_dtype output_dtype = base_dtype out_scaled_mm = out_scaled_mm compare_type out_scaled_mm = out_scaled_mm tensor_to_scale out_scaled_mm input_dtype out_emulated = out_emulated compare_type out_emulated = out_emulated tensor_to_scale out_emulated input_dtype base_dtype torch bfloat torch float atol rtol = e- e- atol rtol = e- e- torch testing assert_close out_scaled_mm out_emulated atol=atol rtol=rtol unittest skipIf PLATFORM_SUPPORTS_FP f _msg parametrize base_dtype torch float torch bfloat torch float test_scaled_mm_change_stride base_dtype torch manual_seed input_dtype = e m _type output_dtype = base_dtype compare_type = torch float x = torch empty_strided device= cuda dtype=base_dtype y = torch empty_strided device= cuda dtype=base_dtype x normal_ y normal_ x_scale = tensor_to_scale x input_dtype float y_scale = tensor_to_scale y input_dtype float x_fp = to_fp _saturated x x_scale input_dtype y_fp = to_fp _saturated y y_scale input_dtype Calculate actual F mm out_scaled_mm = scaled_mm_wrap x_fp y_fp scale_a=x_scale reciprocal scale_b=y_scale reciprocal out_dtype=output_dtype Calculate emulated F mm out_emulated = mm_float _emulated x_fp x_scale y_fp y_scale output_dtype output_dtype = base_dtype out_scaled_mm = out_scaled_mm compare_type out_scaled_mm = out_scaled_mm tensor_to_scale out_scaled_mm input_dtype out_emulated = out_emulated compare_type out_emulated = out_emulated tensor_to_scale out_emulated input_dtype base_dtype torch bfloat torch float atol rtol = e- e- atol rtol = e- e- torch testing assert_close out_scaled_mm out_emulated atol=atol rtol=rtol onlyCUDA test_float _bias device - None device = cpu torch cuda is_available PLATFORM_SUPPORTS_FP raise unittest SkipTest f _msg k l m = x = torch ones k l device=device e m _type y = torch full m l device=device dtype=e m _type t bias = torch full m device=device dtype=torch bfloat scale_a = torch tensor device=device scale_b = torch tensor device=device out_fp = scaled_mm_wrap x y scale_a=scale_a scale_b=scale_b outb_fp = scaled_mm_wrap x y scale_a=scale_a scale_b=scale_b bias=bias fails ROCm currently because hipblaslt doesn t have amax op out_fp = out_fp torch float outb_fp = outb_fp torch float difference = torch abs out_fp - outb_fp assertEqual difference torch tensor device=device expand_as out_fp onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FP f _msg parametrize bias True False test_non_divisible_leading_dim device bias bool - None x = torch rand device=device e m _type y = torch rand device=device e m _type t scale_a = torch tensor device=device scale_b = torch tensor device=device input_bias = None bias input_bias = torch rand device=device torch bfloat _ = scaled_mm_wrap x y scale_a scale_b bias=input_bias onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FP f _msg test_float _bias_relu_edgecase device - None k l m = x = torch full k l device=device e m _type y = torch full m l device=device dtype=e m _type t bias = torch full m - device=device dtype=torch bfloat scale_a = torch tensor device=device scale_b = torch tensor device=device outb_fp = scaled_mm_wrap x y scale_a scale_b bias=bias outb_fp = outb_fp torch float assertEqual outb_fp torch tensor - device=device expand_as outb_fp onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FP f _msg test_float _output_errors_with_bias device - None k l m = x = torch rand k l device=device e m _type y = torch full m l device=device dtype=e m _type t scale_a = torch tensor device=device scale_b = torch tensor device=device bias = torch full m device=device dtype=torch bfloat assertRaisesRegex ValueError Bias supported when out_dtype set Float lambda scaled_mm_wrap x y scale_a scale_b bias=bias out_dtype=torch float onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FP torch cuda is_available f _msg test_error_message_fp _pre_sm device - None k l m = x = torch rand k l device=device e m _type y = torch rand m l device=device e m _type t scale_a = torch tensor device=device scale_b = torch tensor device=device assertRaisesRegex RuntimeError r torch\ \_scaled\_mm only supported CUDA devices compute capability \ \= \ \ ROCm MI \+ lambda scaled_mm_wrap x y scale_a scale_b out_dtype=torch float unittest skipIf PLATFORM_SUPPORTS_FP f _msg unittest skipIf SM OrLater fast_accum SM -only test_float _scale_fast_accum device - None size = x = torch full size device=device dtype=e m _type hipblaslt does yet support mixed e m _type input y_type = e m _type torch version hip e m _type y = torch full size device=device dtype=y_type t scale_a = torch tensor device=device scale_b = torch tensor device=device out_fp = scaled_mm_wrap x y scale_a scale_b out_dtype=e m _type use_fast_accum=True assertEqual out_fp torch float torch full size device=device out_fp _s = scaled_mm_wrap x y scale_a=scale_a scale_b=scale_b out_dtype=e m _type use_fast_accum=True assertEqual out_fp out_fp _s onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FP IS_WINDOWS f _msg unittest skipIf SM OrLater rowwise implementation currently sm -sm specific parametrize use_fast_accum True False test_float _rowwise_scaling_sanity device use_fast_accum bool - None M K N = fill_value = x = torch full M K fill_value device=device y = torch full N K fill_value device=device x_scales = torch ones x shape device=device dtype=torch float y_scales = torch ones y shape device=device dtype=torch float x_fp = x e m _type y_fp = y e m _type t out_fp = scaled_mm_wrap x_fp y_fp scale_a=x_scales scale_b=y_scales out_dtype=torch bfloat use_fast_accum=use_fast_accum assertEqual out_fp torch float torch full M N K fill_value device=device onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FP IS_WINDOWS f _msg test_float _error_messages device - None M K N = fill_value = x = torch full M K fill_value device=device y = torch full N K fill_value device=device x_fp = x e m _type y_fp = y e m _type t assertRaisesRegex ValueError re escape scale_b must have Float element scaled_mm_wrap x_fp y_fp scale_a=torch ones device= cuda scale_b=torch ones device= cuda scale_recipe_a=ScalingType TensorWise scale_recipe_b=ScalingType TensorWise out_dtype=torch bfloat assertRaisesRegex ValueError re escape f scale_b must have N Float elements got N + scaled_mm_wrap x_fp y_fp scale_a=torch ones M device= cuda scale_b=torch ones N + device= cuda scale_recipe_a=ScalingType RowWise scale_recipe_b=ScalingType RowWise out_dtype=torch bfloat assertRaisesRegex IndexError re escape Dimension out range scaled_mm_wrap x_fp y_fp scale_a=torch ones M device= cuda scale_b=torch ones N device= cuda scale_recipe_a=ScalingType RowWise scale_recipe_b=ScalingType RowWise out_dtype=torch bfloat assertRaisesRegex ValueError re escape expected scale_b stride got scaled_mm_wrap x_fp y_fp scale_a=torch ones M device= cuda scale_b=torch ones N device= cuda scale_recipe_a=ScalingType RowWise scale_recipe_b=ScalingType RowWise out_dtype=torch bfloat e m out = scaled_mm_wrap x_fp y_fp e m _type scale_a=torch ones M device= cuda scale_b=torch ones N device= cuda out_dtype=torch bfloat out torch cuda get_device_capability == torch version cuda torch version cuda = out = e m assertEqual out torch ones_like out torch version hip Note re compile used re escape This accommodate fn vs fnuz type message assertRaisesRegex ValueError r expected mat_b\ dtype\ \ kFloat _e m fn uz got c Float _e m fnuz e m assertRaisesRegex RuntimeError r Expected b\ dtype\ \ == kFloat _e m fn true got false\ e m unittest skipIf PLATFORM_SUPPORTS_FP IS_WINDOWS f _msg unittest skipIf SM OrLater rowwise implementation currently sm -sm specific parametrize base_dtype torch bfloat torch float with_tf _off test_scaled_mm_vs_emulated_row_wise base_dtype Fp out_dtype only supported cuBLAS which however only started shipping row-wise kernels CUDA only sm + base_dtype torch float torch version hip raise unittest SkipTest hipblaslt rowwise _scaled_mm only supports BFloat _get_torch_cuda_version raise unittest SkipTest Need CUDA + row-wise fp w cuBLAS torch cuda get_device_capability raise unittest SkipTest Need sm + row-wise fp w cuBLAS torch manual_seed input_dtype = e m _type output_dtype = base_dtype x = torch randn device= cuda dtype=base_dtype y = torch randn device= cuda dtype=base_dtype t x_scales = tensor_to_scale x input_dtype dim= float y_scales = tensor_to_scale y input_dtype dim= float x_fp = to_fp _saturated x x_scales e m _type y_fp = to_fp _saturated y y_scales e m _type test Calculate actual F mm out_scaled_mm = scaled_mm_wrap x_fp y_fp scale_a=x_scales reciprocal scale_b=y_scales reciprocal out_dtype=output_dtype Calculate emulated F mm out_emulated = mm_float _emulated x_fp x_scales y_fp y_scales output_dtype base_dtype torch bfloat torch float atol rtol = e- e- atol rtol = e- e- assertEqual out_scaled_mm out_emulated atol=atol rtol=rtol only cuBLAS supports rowwise fp output cuBLAS only supports rowwise SM torch cuda get_device_capability = output_dtype == torch float assertRaisesRegex ValueError Only bf high precision output types supported row-wise scaling test test Note Removed parameterization over M N K failed tests as-is unittest skipIf PLATFORM_SUPPORTS_FP IS_WINDOWS f _msg unittest skipIf IS_SM cuBLAS blockwise scaling requires sm + unittest skipIf _get_torch_cuda_version cuBLAS blockwise scaling added CUDA parametrize output_dtype torch bfloat torch float parametrize lhs_block rhs_block parametrize M N K with_tf _off test_scaled_mm_vs_emulated_block_wise output_dtype lhs_block rhs_block M N K torch manual_seed x = torch randn M K device= cuda dtype=output_dtype pow y = torch randn N K device= cuda dtype=output_dtype pow x_fp x_scales = tensor_to_scale_block x e m _type lhs_block y_fp y_scales = tensor_to_scale_block y e m _type rhs_block x blocks need scales outer-dim-major lhs_block == x_scales = x_scales t contiguous t lhs_recipe = ScalingType BlockWise x lhs_recipe = ScalingType BlockWise x rhs_block == y_scales = y_scales t contiguous t rhs_recipe = ScalingType BlockWise x rhs_recipe = ScalingType BlockWise x Calculate actual F mm out_scaled_mm = scaled_mm_wrap x_fp y_fp t scale_a=x_scales reciprocal scale_b=y_scales reciprocal t out_dtype=output_dtype scale_recipe_a=lhs_recipe scale_recipe_b=rhs_recipe Calculate emulated F mm out_emulated = mm_float _emulated_block x_fp x_scales y_fp t y_scales t output_dtype cosine_sim = torch nn functional cosine_similarity out_scaled_mm flatten float out_emulated flatten float dim= assertGreaterEqual float cosine_sim output_dtype torch bfloat torch float atol rtol = e- e- atol rtol = e- e- assertEqual out_scaled_mm out_emulated atol=atol rtol=rtol One last check against full-precision reference ensure we didn t mess up scaling itself made test trivial cosine_sim = torch nn functional cosine_similarity out_scaled_mm flatten float x y t flatten float dim= assertGreaterEqual float cosine_sim unittest skipIf PLATFORM_SUPPORTS_FP IS_WINDOWS f _msg unittest skipIf IS_SM cuBLAS blockwise scaling requires sm + unittest skipIf _get_torch_cuda_version cuBLAS blockwise scaling added CUDA parametrize output_dtype torch bfloat torch float parametrize lhs_block rhs_block parametrize M N K test_scaled_mm_vs_emulated_block_wise_verify_small_shapes output_dtype lhs_block rhs_block M N K torch manual_seed x = torch randn M K device= cuda dtype=output_dtype pow y = torch randn N K device= cuda dtype=output_dtype pow x_fp x_scales = tensor_to_scale_block x e m _type lhs_block y_fp y_scales = tensor_to_scale_block y e m _type rhs_block x blocks need scales outer-dim-major lhs_block == x_scales = x_scales t contiguous t lhs_recipe = ScalingType BlockWise x lhs_recipe = ScalingType BlockWise x rhs_block == y_scales = y_scales t contiguous t rhs_recipe = ScalingType BlockWise x rhs_recipe = ScalingType BlockWise x Verify actual F mm doesn t error scaled_mm_wrap x_fp y_fp t scale_a=x_scales scale_recipe_a=lhs_recipe scale_b=y_scales t scale_recipe_b=rhs_recipe out_dtype=output_dtype Verify emulated F mm doesn t error mm_float _emulated_block x_fp x_scales y_fp t y_scales t output_dtype skipIfRocm onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FP IS_WINDOWS f _msg unittest skipIf IS_SM cuBLAS blockwise scaling works sm unittest skipIf _get_torch_cuda_version cuBLAS blockwise scaling added CUDA parametrize output_dtype torch bfloat parametrize lhs_block rhs_block parametrize M N K test_scaled_mm_deepseek_error_messages output_dtype lhs_block rhs_block M N K torch manual_seed x = torch randn M K device= cuda dtype=output_dtype pow y = torch randn N K device= cuda dtype=output_dtype pow x_fp x_scales = tensor_to_scale_block x e m _type lhs_block y_fp y_scales = tensor_to_scale_block y e m _type rhs_block x blocks need scales outer-dim-major lhs_block == x_scales = x_scales t contiguous t lhs_recipe = ScalingType BlockWise x lhs_recipe = ScalingType BlockWise x rhs_block == y_scales = y_scales t contiguous t rhs_recipe = ScalingType BlockWise x rhs_recipe = ScalingType BlockWise x Verify actual F mm doesn t error assertRaisesRegex NotImplementedError DeepSeek scaling only supported CUDA SM scaled_mm_wrap x_fp y_fp t scale_a=x_scales scale_recipe_a=lhs_recipe scale_b=y_scales t scale_recipe_b=rhs_recipe out_dtype=output_dtype unittest skipIf PLATFORM_SUPPORTS_FP f _msg parametrize which_dim_zero parametrize use_torch_compile False True test_zero_dim_tensorwise which_dim_zero use_torch_compile - None device = cuda x_dtype y_dtype = e m _type e m _type out_dtype = torch bfloat M K N = which_dim_zero == M = which_dim_zero == K = which_dim_zero == N = x_fp = torch zeros M K device=device x_dtype y_fp = torch zeros N K device=device dtype=y_dtype t out_fp = torch mm x_fp torch float y_fp torch float scale_a = torch tensor float -inf device=device scale_b = torch tensor float -inf device=device f = scaled_mm_wrap use_torch_compile f = torch compile scaled_mm_wrap out_fp = f x_fp y_fp scale_a scale_b out_dtype=out_dtype assertEqual out_dtype out_fp dtype assertEqual out_fp out_fp torch float unittest skipIf IS_WINDOWS Windows doesn t support row-wise scaling unittest skipIf PLATFORM_SUPPORTS_FP f _msg unittest skipIf SM OrLater sm kernel isn t opted into carveout yet test_honor_sm_carveout - None torch manual_seed x = torch randn device= cuda dtype=torch float y = torch randn device= cuda dtype=torch float t x_scales = tensor_to_scale x e m _type dim= reciprocal y_scales = tensor_to_scale y e m _type dim= reciprocal x_fp = to_fp _saturated x x_scales e m _type y_fp = to_fp _saturated y y_scales e m _type cu_count = torch cuda get_device_properties multi_processor_count carveout = torch version cuda cu_count tempfile NamedTemporaryFile f torch profiler profile activities= torch profiler ProfilerActivity CUDA prof assertIsNone torch _C _get_sm_carveout_experimental scaled_mm_wrap x_fp y_fp scale_a=x_scales scale_b=y_scales out_dtype=torch bfloat torch _C _set_sm_carveout_experimental assertEqual torch _C _get_sm_carveout_experimental scaled_mm_wrap x_fp y_fp scale_a=x_scales scale_b=y_scales out_dtype=torch bfloat torch _C _set_sm_carveout_experimental assertEqual torch _C _get_sm_carveout_experimental scaled_mm_wrap x_fp y_fp scale_a=x_scales scale_b=y_scales out_dtype=torch bfloat torch _C _set_sm_carveout_experimental None assertIsNone torch _C _get_sm_carveout_experimental scaled_mm_wrap x_fp y_fp scale_a=x_scales scale_b=y_scales out_dtype=torch bfloat prof export_chrome_trace f name torch version hip events = evt evt json load open f name traceEvents evt get cat == kernel events returned out order need sorted ts timestamp events = sorted events key=lambda x x ts ROCm carveout invisible except kernels running slower fewer CUs no_carveout carveout_ carveout no_carveout_again = float evt get dur evt events True no_carveout carveout carveout_ carveout no_carveout_again carveout noqa SIM something went wrong print more info help debug flaky test print ROCm debug info test_honor_sm_carveout print cu_count cu_count print no_carveout no_carveout print carveout_ carveout_ print carveout carveout print no_carveout_again no_carveout_again assertTrue no_carveout carveout assertTrue carveout_ carveout assertTrue no_carveout_again carveout ROCm carveout will create new streams when enabled go back original stream when disabled no_carveout carveout_ carveout no_carveout_again = int evt get tid evt events assertTrue no_carveout == no_carveout_again assertTrue no_carveout == carveout_ assertTrue no_carveout = carveout assertTrue carveout_ = carveout no_carveout carveout_ carveout_ no_carveout_again = math prod evt get args get grid evt json load open f name traceEvents evt get cat == kernel assertEqual no_carveout no_carveout_again capability = torch cuda get_device_capability capability expected failure CUTLASS only supports SM carveout via green contexts SM assertEqual no_carveout carveout_ assertEqual carveout_ carveout_ correct behavior assertNotEqual no_carveout carveout_ assertNotEqual carveout_ carveout_ test_pack_uint Verify given tensor high precision values val val x packed representation val val MSB LSB val val Note packing function private file s still good test we packing expected way hp_data = torch tensor b b dtype=torch uint lp_data_actual = pack_uint hp_data lp_data_expected = torch tensor b dtype=torch uint torch testing assert_close lp_data_actual lp_data_expected atol= rtol= skipIfRocm onlyCUDA unittest skipIf PLATFORM_SUPPORTS_MX_GEMM mx_skip_msg parametrize mkn Nice shapes Very unbalanced Mixed large small name_fn=lambda mkn f mkn _ mkn _ mkn test_blockwise_nvfp _with_global_scale mkn - None device = cuda M K N = mkn BLOCK_SIZE = Note SQNR target ` test_blockwise_mxfp _nvfp _mxfp _numerics ` test approx_match_sqnr_target = A_ref = torch randn M K device=device dtype=torch bfloat B_ref = torch randn N K device=device dtype=torch bfloat A A_scale A_global_scale = data_to_nvfp _with_global_scale A_ref BLOCK_SIZE B B_scale B_global_scale = data_to_nvfp _with_global_scale B_ref BLOCK_SIZE torch version cuda A_scale = to_blocked A_scale B_scale = to_blocked B_scale swizzle = SwizzleType SWIZZLE_ _ _ SwizzleType NO_SWIZZLE swizzle = SwizzleType NO_SWIZZLE SwizzleType NO_SWIZZLE C_ref = A_ref B_ref t C = scaled_mm A B t scale_a= A_scale A_global_scale scale_recipe_a= ScalingType BlockWise x ScalingType TensorWise scale_b= B_scale B_global_scale scale_recipe_b= ScalingType BlockWise x ScalingType TensorWise swizzle_a=swizzle swizzle_b=swizzle output_dtype=torch bfloat sqnr = compute_error C_ref C assert sqnr item approx_match_sqnr_target unittest skipIf PLATFORM_SUPPORTS_MX_GEMM mx_skip_msg parametrize test_case_name a_eye_b_eye a_ones_b_ones a_ones_modified_b_ones a_ones_b_ones_modified a_scale_modified_b_ones a_ones_b_scale_modified data_random_scales_one data_random_scales_from_data parametrize fast_accum False True parametrize mkn Nice shapes Non block multiples K multiple skipped fp Very unbalanced Mixed large small name_fn=lambda mkn f mkn _ mkn _ mkn parametrize recipe mxfp mxfp torch version hip nvfp test_blockwise_mxfp _nvfp _mxfp _numerics test_case_name fast_accum mkn recipe - None recipe == nvfp recipe == mxfp fast_accum raise unittest SkipTest fast_accum supported nvfp mxfp cublas gemm skipping device = cuda M K N = mkn recipe == nvfp K = raise unittest SkipTest K must divisible nvfp cublas gemm skipping torch version hip M == K == N == raise unittest SkipTest M N must multiples K must multiple ROCm skipping fp _scaling_dtype = torch float _e m fnu torch version hip torch float _e m fn BLOCK_SIZE = torch version hip recipe == nvfp require_exact_match = True approx_match_sqnr_target = test_case_name == a_eye_b_eye M == K M == N raise unittest SkipTest test only defined M == K == N skipping A_ref = torch eye M device=device dtype=torch bfloat B_ref = torch eye M device=device dtype=torch bfloat recipe == mxfp A = A_ref torch float _e m fn B = B_ref torch float _e m fn A_scale = torch full M ceil_div K BLOCK_SIZE device=device dtype=torch float _e m fnu B_scale = torch full N ceil_div K BLOCK_SIZE device=device dtype=torch float _e m fnu nvfp mxfp A = _bfloat _to_float _e m fn_x A_ref B = _bfloat _to_float _e m fn_x B_ref A_scale = torch full M ceil_div K BLOCK_SIZE device=device dtype=fp _scaling_dtype B_scale = torch full N ceil_div K BLOCK_SIZE device=device dtype=fp _scaling_dtype test_case_name == a_ones_b_ones A_ref = torch ones M K device=device dtype=torch bfloat B_ref = torch ones N K device=device dtype=torch bfloat recipe == mxfp A = A_ref torch float _e m fn B = B_ref torch float _e m fn A_scale = torch full M ceil_div K BLOCK_SIZE device=device dtype=torch float _e m fnu B_scale = torch full N ceil_div K BLOCK_SIZE device=device dtype=torch float _e m fnu nvfp mxfp A = _bfloat _to_float _e m fn_x A_ref B = _bfloat _to_float _e m fn_x B_ref A_scale = torch full M ceil_div K BLOCK_SIZE device=device dtype=fp _scaling_dtype B_scale = torch full N ceil_div K BLOCK_SIZE device=device dtype=fp _scaling_dtype test_case_name == a_ones_modified_b_ones A_ref = torch ones M K device=device dtype=torch bfloat B_ref = torch ones N K device=device dtype=torch bfloat A_ref BLOCK_SIZE = recipe == mxfp A = A_ref torch float _e m fn B = B_ref torch float _e m fn A_scale = torch full M ceil_div K BLOCK_SIZE device=device dtype=torch float _e m fnu B_scale = torch full N ceil_div K BLOCK_SIZE device=device dtype=torch float _e m fnu nvfp mxfp A = _bfloat _to_float _e m fn_x A_ref B = _bfloat _to_float _e m fn_x B_ref A_scale = torch full M ceil_div K BLOCK_SIZE device=device dtype=fp _scaling_dtype B_scale = torch full N ceil_div K BLOCK_SIZE device=device dtype=fp _scaling_dtype test_case_name == a_ones_b_ones_modified A_ref = torch ones M K device=device dtype=torch bfloat B_ref = torch ones N K device=device dtype=torch bfloat B_ref BLOCK_SIZE = recipe == mxfp A = A_ref torch float _e m fn B = B_ref torch float _e m fn A_scale = torch full M ceil_div K BLOCK_SIZE device=device dtype=torch float _e m fnu B_scale = torch full N ceil_div K BLOCK_SIZE device=device dtype=torch float _e m fnu nvfp mxfp A = _bfloat _to_float _e m fn_x A_ref B = _bfloat _to_float _e m fn_x B_ref A_scale = torch full M ceil_div K BLOCK_SIZE device=device dtype=fp _scaling_dtype B_scale = torch full N ceil_div K BLOCK_SIZE device=device dtype=fp _scaling_dtype test_case_name == a_scale_modified_b_ones A_ref = torch ones M K device=device dtype=torch bfloat B_ref = torch ones N K device=device dtype=torch bfloat recipe == mxfp A = A_ref torch float _e m fn B = B_ref torch float _e m fn A_scale = torch full M ceil_div K BLOCK_SIZE device=device dtype=torch float _e m fnu B_scale = torch full N ceil_div K BLOCK_SIZE device=device dtype=torch float _e m fnu A_ref BLOCK_SIZE = A BLOCK_SIZE = A_scale = nvfp mxfp A = _bfloat _to_float _e m fn_x A_ref B = _bfloat _to_float _e m fn_x B_ref A_scale = torch full M ceil_div K BLOCK_SIZE device=device dtype=fp _scaling_dtype B_scale = torch full N ceil_div K BLOCK_SIZE device=device dtype=fp _scaling_dtype A_ref BLOCK_SIZE = A view torch uint BLOCK_SIZE = b A_scale = test_case_name == a_ones_b_scale_modified A_ref = torch ones M K device=device dtype=torch bfloat B_ref = torch ones N K device=device dtype=torch bfloat recipe == mxfp A = A_ref torch float _e m fn B = B_ref torch float _e m fn A_scale = torch full M ceil_div K BLOCK_SIZE device=device dtype=torch float _e m fnu B_scale = torch full N ceil_div K BLOCK_SIZE device=device dtype=torch float _e m fnu B_ref BLOCK_SIZE = B BLOCK_SIZE = B_scale = nvfp mxfp A = _bfloat _to_float _e m fn_x A_ref B = _bfloat _to_float _e m fn_x B_ref A_scale = torch full M ceil_div K BLOCK_SIZE device=device dtype=fp _scaling_dtype B_scale = torch full N ceil_div K BLOCK_SIZE device=device dtype=fp _scaling_dtype B_ref BLOCK_SIZE = B view torch uint BLOCK_SIZE = b B_scale = test_case_name == data_random_scales_one require_exact_match = False recipe == mxfp scales all-ones element data random while being exactly representable float _e m fn generate integers interpret float _e m fn A_ref = torch randint M K device=device dtype=torch uint view torch float _e m fn torch bfloat B_ref = torch randint N K device=device dtype=torch uint view torch float _e m fn torch bfloat modification don t allow NaN values A_ref torch isnan A_ref = B_ref torch isnan B_ref = A = A_ref torch float _e m fn B = B_ref torch float _e m fn A_scale = torch full M ceil_div K BLOCK_SIZE device=device dtype=torch float _e m fnu B_scale = torch full N ceil_div K BLOCK_SIZE device=device dtype=torch float _e m fnu nvfp mxfp scales all-ones element data random while being exactly representable float _e m fn_x generate integers cast bfloat A_ref = _floatx_unpacked_to_f torch randint M K device=device dtype=torch uint FP _EBITS FP _MBITS bfloat B_ref = _floatx_unpacked_to_f torch randint N K device=device dtype=torch uint FP _EBITS FP _MBITS bfloat A = _bfloat _to_float _e m fn_x A_ref B = _bfloat _to_float _e m fn_x B_ref A_scale = torch full M ceil_div K BLOCK_SIZE device=device dtype=fp _scaling_dtype B_scale = torch full N ceil_div K BLOCK_SIZE device=device dtype=fp _scaling_dtype test_case_name == data_random_scales_from_data K BLOCK_SIZE == raise unittest SkipTest f test only defined K multiple BLOCK_SIZE skipping require_exact_match = False random data scales data A_ref = torch randn M K device=device dtype=torch bfloat B_ref = torch randn N K device=device dtype=torch bfloat recipe == mxfp Calculate scales based inputs A_scale = data_to_mx_scale A_ref BLOCK_SIZE recipe B_scale = data_to_mx_scale B_ref BLOCK_SIZE recipe max_val = F E M _MAX_VAL min_val = - max_val A = A_ref reshape - BLOCK_SIZE A_scale reshape M ceil_div K BLOCK_SIZE float reshape M K A = A clamp min=min_val max=max_val torch float _e m fn B = B_ref reshape - BLOCK_SIZE B_scale reshape N ceil_div K BLOCK_SIZE float reshape N K B = B clamp min=min_val max=max_val torch float _e m fn nvfp mxfp recipe == mxfp A_scale = data_to_mx_scale A_ref BLOCK_SIZE recipe B_scale = data_to_mx_scale B_ref BLOCK_SIZE recipe A_scale = data_to_nvfp _scale A_ref BLOCK_SIZE B_scale = data_to_nvfp _scale B_ref BLOCK_SIZE max_val = FP _MAX_VAL min_val = - max_val A = A_ref reshape - BLOCK_SIZE A_scale reshape M ceil_div K BLOCK_SIZE bfloat reshape M K A = A clamp min=min_val max=max_val A = _bfloat _to_float _e m fn_x A B = B_ref reshape - BLOCK_SIZE B_scale reshape N ceil_div K BLOCK_SIZE bfloat reshape N K B = B clamp min=min_val max=max_val B = _bfloat _to_float _e m fn_x B approx_match_sqnr_target = torch version hip C_ref = A_ref B_ref t convert swizzled format torch version hip A_scale = to_blocked A_scale B_scale = to_blocked B_scale C = scaled_mm_wrap A B t A_scale B_scale out_dtype=torch bfloat use_fast_accum=fast_accum require_exact_match torch testing assert_close C C_ref atol= rtol= sqnr = compute_error C_ref C assert sqnr item approx_match_sqnr_target unittest skipIf PLATFORM_SUPPORTS_MX_GEMM IS_WINDOWS mx_skip_msg parametrize recipe mxfp mxfp torch version hip nvfp test_blockwise_mxfp _nvfp _error_messages device recipe - None M K N = BLOCK_SIZE_K = recipe == nvfp BLOCK_SIZE_MN = fill_value = scale_dtype = torch float _e m fn recipe == nvfp torch float _e m fnu x = torch full M K fill_value device=device y = torch full N K fill_value device=device recipe == mxfp x_lowp = x e m _type y_lowp = y e m _type t nvfp #mxfp x_lowp = _bfloat _to_float _e m fn_x x bfloat y_lowp = _bfloat _to_float _e m fn_x y bfloat t num_k_blocks = ceil_div K BLOCK_SIZE_K padded_num_k_blocks = ceil_div num_k_blocks expected_a_size = BLOCK_SIZE_MN ceil_div M BLOCK_SIZE_MN padded_num_k_blocks expected_b_size = BLOCK_SIZE_MN ceil_div N BLOCK_SIZE_MN padded_num_k_blocks block = ScalingType BlockWise x recipe == nvfp ScalingType BlockWise x torch version hip swizzle = SwizzleType NO_SWIZZLE swizzle = SwizzleType SWIZZLE_ _ _ Test wrong scale tensor size scale_a correct dtype assertRaisesRegex ValueError f For Block W w ise scaling scale_a should have expected_a_size f elements incorrect_size_a = torch ones expected_a_size - device=device dtype=scale_dtype correct_size_b = torch ones expected_b_size device=device dtype=scale_dtype scaled_mm_wrap x_lowp y_lowp scale_a=incorrect_size_a scale_recipe_a=block scale_b=correct_size_b scale_recipe_b=block swizzle_a=swizzle swizzle_b=swizzle out_dtype=torch bfloat Test wrong scale tensor size scale_b correct dtype assertRaisesRegex ValueError f For Block W w ise scaling scale_b should have expected_b_size f elements correct_size_a = torch ones expected_a_size device=device dtype=scale_dtype incorrect_size_b = torch ones expected_b_size + device=device dtype=scale_dtype scaled_mm_wrap x_lowp y_lowp scale_a=correct_size_a scale_recipe_a=block scale_b=incorrect_size_b scale_recipe_b=block swizzle_a=swizzle swizzle_b=swizzle out_dtype=torch bfloat Test non-contiguous scale tensors correct dtype assertRaisesRegex ValueError For Block W w ise scaling both scales should contiguous non_contiguous_a = torch ones expected_a_size device=device dtype=scale_dtype contiguous_b = torch ones expected_b_size device=device dtype=scale_dtype scaled_mm_wrap x_lowp y_lowp scale_a=non_contiguous_a scale_b=contiguous_b out_dtype=torch bfloat scaled_grouped_mm_helper alist blist ascalelist bscalelist outlist use_fast_accum b ascale bscale out zip alist blist ascalelist bscalelist outlist out_ref = scaled_mm_wrap b t ascale view - bscale view - out_dtype=torch bfloat use_fast_accum=use_fast_accum assertEqual out out_ref atol= e- rtol= e- Testing only _scaled_grouped_mm multiple shapes _scaled_mm already has more combinations parameters than _scaled_grouped_mm supporting more than one inputs layout combinations unittest skipIf PLATFORM_SUPPORTS_FP _GROUPED_GEMM f _grouped_msg parametrize fast_accum False True AMD does support non-contiguous inputs yet parametrize strided False + True torch version cuda AMD does support NVFP parametrize wrap_v True False test_scaled_grouped_gemm_ d_ d fast_accum strided wrap_v device = cuda fp _dtype = e m _type m n k n_groups = = torch randn m k n_groups + k int strided device=device fp _dtype k n_groups b = torch randn n k n_groups + k int strided device=device fp _dtype k n_groups scale_a = torch rand m n_groups device=device dtype=torch float scale_b = torch rand n n_groups device=device dtype=torch float offs = torch arange k n_groups k + k device=device dtype=torch int f = scaled_grouped_mm_wrap out = f b t scale_a scale_b scale_recipe_a=ScalingType RowWise scale_recipe_b=ScalingType RowWise offs=offs out_dtype=torch bfloat use_fast_accum=fast_accum wrap_v =wrap_v offs_cpu = offs cpu alist blist ascalelist bscalelist = start = i range n_groups alist append start offs_cpu i blist append b start offs_cpu i ascalelist append scale_a i m i + m bscalelist append scale_b i n i + n start = offs_cpu i scaled_grouped_mm_helper alist blist ascalelist bscalelist out fast_accum unittest skipIf PLATFORM_SUPPORTS_FP _GROUPED_GEMM f _grouped_msg parametrize fast_accum False True AMD does support non-contiguous inputs yet parametrize strided False + True torch version cuda parametrize wrap_v True False test_scaled_grouped_gemm_ d_ d fast_accum strided wrap_v device = cuda fp _dtype = e m _type m n k n_groups = s_int = int strided = torch randn m n_groups k + s_int device=device fp _dtype k b = torch randn n_groups + s_int n k + s_int device=device fp _dtype + s_int k assertTrue is_contiguous strided assertTrue b is_contiguous strided check_zero_size True False check_zero_size n_groups = continue offs = torch arange m n_groups m + m device= cuda dtype=torch int check_zero_size offs = offs scale_a = torch rand n_groups m device= cuda dtype=torch float scale_b = torch rand n_groups n device= cuda dtype=torch float view n_groups n f = scaled_grouped_mm_wrap out = f b transpose - - scale_a scale_b scale_recipe_a=ScalingType RowWise scale_recipe_b=ScalingType RowWise offs=offs out_dtype=torch bfloat use_fast_accum=fast_accum wrap_v =wrap_v offs_cpu = offs cpu alist ascalelist outlist = start = i range n_groups alist append start offs_cpu i ascalelist append scale_a start offs_cpu i outlist append out start offs_cpu i start = offs_cpu i scaled_grouped_mm_helper alist b ascalelist scale_b outlist fast_accum unittest skipIf PLATFORM_SUPPORTS_FP _GROUPED_GEMM f _grouped_msg parametrize fast_accum False True AMD does support non-contiguous inputs yet parametrize strided False + True torch version cuda test_scaled_grouped_gemm_ d_ d fast_accum strided device = cuda fp _dtype = e m _type m n k n_groups = s_int = int strided = torch randn n_groups + s_int m k + s_int device=device fp _dtype + s_int k b = torch randn n_groups + s_int n k + s_int device=device fp _dtype + s_int k assertTrue is_contiguous strided assertTrue b is_contiguous strided scale_a = torch rand n_groups m device= cuda dtype=torch float view n_groups m scale_b = torch rand n_groups n device= cuda dtype=torch float view n_groups n f = torch _scaled_grouped_mm out = f b transpose - - scale_a scale_b out_dtype=torch bfloat use_fast_accum=fast_accum scaled_grouped_mm_helper b scale_a scale_b out fast_accum unittest skipIf PLATFORM_SUPPORTS_FP _GROUPED_GEMM f _grouped_msg parametrize fast_accum False True AMD does support non-contiguous inputs yet parametrize strided False + True torch version cuda test_scaled_grouped_gemm_ d_ d fast_accum strided device = cuda fp _dtype = e m _type m n k n_groups = s_int = int strided = torch randn n_groups + s_int m k + s_int device=device fp _dtype + s_int k b = torch randn n n_groups k + s_int device=device fp _dtype k assertTrue is_contiguous strided assertTrue b is_contiguous strided scale_a = torch rand n_groups m device= cuda dtype=torch float view n_groups m scale_b = torch rand n_groups n device= cuda dtype=torch float check_zero_size True False check_zero_size n_groups = continue offs = torch arange n n_groups n + n device= cuda dtype=torch int check_zero_size offs = offs f = torch _scaled_grouped_mm out = f b transpose - - scale_a scale_b offs=offs out_dtype=torch bfloat use_fast_accum=fast_accum offs_cpu = offs cpu blist bscalelist outlist = start = i range n_groups blist append b start offs_cpu i bscalelist append scale_b start offs_cpu i outlist append out start offs_cpu i start = offs_cpu i scaled_grouped_mm_helper blist scale_a bscalelist outlist fast_accum unittest skipIf PLATFORM_SUPPORTS_MX_GEMM mx_skip_msg test_blockwise_mxfp _compile - None device = cuda M K N = BLOCK_SIZE = A_ref = torch eye M device=device dtype=torch bfloat B_ref = torch eye M device=device dtype=torch bfloat A = A_ref torch float _e m fn B = B_ref torch float _e m fn A_scale = torch full M ceil_div K BLOCK_SIZE device=device dtype=torch float _e m fnu B_scale = torch full N ceil_div K BLOCK_SIZE device=device dtype=torch float _e m fnu C_ref = A_ref B_ref t compiled_scaled_mm = torch compile scaled_mm_wrap backend= inductor C = compiled_scaled_mm A B t A_scale B_scale out_dtype=torch bfloat use_fast_accum=False torch testing assert_close C C_ref atol= rtol= unittest skipIf PLATFORM_SUPPORTS_MX_GEMM mx_skip_msg test_blockwise_nvfp _compile - None device = cuda M K N = BLOCK_SIZE = torch version hip fp _scaling_dtype = torch float _e m fnu torch version hip torch float _e m fn A_ref = torch eye M device=device dtype=torch bfloat B_ref = torch eye M device=device dtype=torch bfloat A = _bfloat _to_float _e m fn_x A_ref B = _bfloat _to_float _e m fn_x B_ref A_scale = torch full M ceil_div K BLOCK_SIZE device=device dtype=fp _scaling_dtype B_scale = torch full N ceil_div K BLOCK_SIZE device=device dtype=fp _scaling_dtype C_ref = A_ref B_ref t compiled_scaled_mm = torch compile scaled_mm_wrap backend= inductor C = scaled_mm_wrap C = compiled_scaled_mm A B t A_scale B_scale out_dtype=torch bfloat use_fast_accum=False torch testing assert_close C C_ref atol= rtol= instantiate_device_type_tests TestFP Matmul globals except_for= cpu __name__ == __main__ TestCase _default_dtype_check_enabled = True run_tests