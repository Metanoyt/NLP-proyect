itertools collections defaultdict dataclasses dataclass typing Any cast NamedTuple Optional torch torch distributed device_mesh DeviceMesh torch distributed tensor placement_types _StridedShard Partial Placement Replicate Shard torch utils _debug_mode _stringify_shape torch utils _dtype_abbrs dtype_abbrs ShardOrderEntry NamedTuple Represents how single tensor dimension sharded across mesh dimensions Attributes tensor_dim The tensor dimension being sharded e g D tensor mesh_dims Tuple mesh dimensions across which tensor dimension sharded execution order The first mesh dim applied first second applied second etc This tuple guaranteed non-empty Examples xdoctest +REQUIRES env TORCH_DOCTEST_DISTRIBUTED Tensor dim sharded across mesh dim then mesh dim ShardOrderEntry tensor_dim= mesh_dims= Tensor dim sharded only mesh dim ShardOrderEntry tensor_dim= mesh_dims= tensor_dim int mesh_dims tuple int guaranteed non-empty Type alias complete shard order specification A tuple ShardOrderEntry one per sharded tensor dimension Example shard_order = ShardOrderEntry tensor_dim= mesh_dims= ShardOrderEntry tensor_dim= mesh_dims= This means - Tensor dimension sharded mesh dimension - Tensor dimension sharded mesh dimension first then mesh dimension ShardOrder = tuple ShardOrderEntry TensorMeta NamedTuple simple named tuple represent tensor metadata intentionally stay simple only sharding propagation purposes shape torch Size stride tuple int dtype torch dtype used internally propagate placements dataclass DTensorSpec mesh DeviceMesh placements tuple Placement tensor meta will only set during sharding propagation tensor_meta Optional TensorMeta = None When tensor dimension sharded across multiple mesh axes ` shard_order ` specifies sequence which these shardings applied This order determines how tensor shards mapped distributed across devices Example For tensor shape D device mesh dim sharded over mesh dim dim sharded over mesh dim then mesh dim shard_order would shard_order = ShardOrderEntry tensor_dim= mesh_dims= ShardOrderEntry tensor_dim= mesh_dims= shard_order ShardOrder = None type ignore assignment __post_init__ - None isinstance placements tuple placements = tuple placements shard_order None pyrefly ignore bad-assignment shard_order = DTensorSpec compute_default_shard_order placements _hash int &#124; None = None staticmethod compute_default_shard_order placements tuple Placement - ShardOrder Compute default shard order placements Returns ShardOrder where each ShardOrderEntry maps tensor dimension mesh dimensions s sharded left-to-right order follow default left-to-right device order shard_order specified tensor_dim_to_mesh_dims defaultdict int list int = defaultdict list mesh_ndim = len placements mesh_dim range mesh_ndim shard_order doesn t work _StridedShard isinstance placements mesh_dim _StridedShard isinstance placements mesh_dim Shard placement = cast Shard placements mesh_dim shard_dim = placement dim assert shard_dim = f Shard dim shard_dim placements placements must normalized tensor_dim_to_mesh_dims shard_dim append mesh_dim Convert dict into ShardOrderEntry tuples default_shard_order = tuple ShardOrderEntry tensor_dim=key mesh_dims=tuple value key value sorted tensor_dim_to_mesh_dims items value default_shard_order _verify_shard_order shard_order ShardOrder - None Verify shard_order valid matches placements total_shard = any isinstance p _StridedShard p placements prev_tensor_dim = - entry shard_order tensor_dim = entry tensor_dim mesh_dims = entry mesh_dims assert len mesh_dims f shard_order shard_order has empty mesh dim assert tensor_dim = f shard_order shard_order has invalid tensor dim tensor_dim assert tensor_dim prev_tensor_dim tensor dim should sorted shard_order prev_tensor_dim = tensor_dim total_shard += len mesh_dims mesh_dim mesh_dims assert = mesh_dim len placements f shard_order shard_order has invalid mesh dim mesh_dims assert placements mesh_dim == Shard tensor_dim f placement mesh_dim doesn t have matching shard shard_order assert total_shard == sum p placements isinstance p Shard __setattr__ attr str value Any - None attr == shard_order value None _verify_shard_order value super __setattr__ attr value Make sure recompute hash case any hashed attributes change though we do expect ` mesh ` ` placements ` ` shard_order ` change hasattr _hash attr mesh placements tensor_meta shard_order _hash = None This assert triggered buggy handling dict outputs some FX passes where you accidentally iterate over dict try put keys into TensorMeta See https github com pytorch pytorch issues attr == tensor_meta value None torch fx passes shape_prop TensorMetadata TODO TensorMetadata arises test distributed tensor experimental test_tp_transform py TensorParallelTest test_tp_transform_e e I actually can t reproduce maybe also bug assert isinstance value TensorMeta &#124; TensorMetadata value _hash_impl - int hashing equality check DTensorSpec used cache sharding propagation results We only need consider mesh placements shape dtype stride Caveat we need keep mind sync hash eq we add more fields them tensor_meta None hash mesh placements shard_order tensor_meta shape tensor_meta stride tensor_meta dtype hash mesh placements shard_order __hash__ - int We lazily cache spec avoid recomputing hash upon each use where we make sure update hash when ` tensor_meta ` changes overriding ` __setattr__ ` This must lazy so Dynamo does try hash non-singleton ` SymInt ` s stride _hash None _hash = _hash_impl _hash _check_equals other object skip_shapes bool = False - bool isinstance other DTensorSpec mesh == other mesh placements == other placements shard_order == other shard_order False tensor_meta None other tensor_meta None tensor_meta == other tensor_meta skip_shapes tensor_meta dtype == other tensor_meta dtype tensor_meta shape == other tensor_meta shape type ignore union-attr tensor_meta stride == other tensor_meta stride type ignore union-attr tensor_meta dtype == other tensor_meta dtype type ignore union-attr __eq__ other object - bool _check_equals other __str__ - str human readable representation DTensorSpec placement_str = format_shard_order_str placements shard_order tensor_meta None tensor_shape = _stringify_shape tensor_meta shape tensor_dtype = dtype_abbrs tensor_meta dtype tensor_shape = unknown shape tensor_dtype = unknown dtype f Spec tensor_dtype tensor_shape placement_str staticmethod is_default_device_order shard_order ShardOrder - bool Check device order default left-to-right order entry shard_order mesh_dims = entry mesh_dims is_increasing = all prev nxt prev nxt itertools pairwise mesh_dims is_increasing False True staticmethod format_shard_order_str placements tuple Placement shard_order Optional ShardOrder = None - str Format DTensor sharding information human-readable string This method formats sharding pattern mesh-centric order showing placement each mesh dimension sequentially When tensor dimension sharded across multiple mesh dimensions order index indicates execution sequence sharding operations Args placements Tuple placement objects each mesh dimension shard_order Optional ShardOrder specifying sharding order Returns String representation sharding pattern mesh-centric format Example For D tensor x x x mesh devices placements = Partial Shard Shard Replicate shard_order = ShardOrderEntry tensor_dim= mesh_dims= Mesh configuration - mesh_dim_ Partial reduction sum - mesh_dim_ Shard tensor dimension executed second order index - mesh_dim_ Shard tensor dimension executed first order index - mesh_dim_ Replicate Output ` ` PS S R ` ` Explanation - ` ` P ` ` mesh dimension has partial reduction - ` ` S ` ` mesh dimension shards tensor dimension order index means second - ` ` S ` ` mesh dimension shards tensor dimension order index means first - ` ` R ` ` mesh dimension replicates The format follows mesh dimension order when tensor dimension sharded across multiple mesh dimensions bracketed index shows execution order ` ` ` ` executed first ` ` ` ` executed second etc out_str = native dtensor-style sharding representation map mesh dim tensor dim mesh_dim placement enumerate placements isinstance placement Shard shard_order None entry shard_order tensor_dim = entry tensor_dim mesh_dims = entry mesh_dims placement dim == tensor_dim assert mesh_dim mesh_dims len mesh_dims out_str += f placement mesh_dims index mesh_dim no need show device order tensor dim only sharded one mesh dim out_str += str placement break out_str += str placement out_str += str placement out_str property shape - torch Size tensor_meta None raise ValueError tensor_meta set tensor_meta shape property stride - tuple int tensor_meta None raise ValueError tensor_meta set tensor_meta stride property ndim - int tensor_meta None raise ValueError tensor_meta set len tensor_meta shape property num_shards - int num_shards = i placement enumerate placements placement is_shard num_shards = mesh size i num_shards property device_mesh - DeviceMesh simple aliasing mesh field make some checks mixes DTensor DTensorSpec easier mesh property dim_map - list int dim_map property we derive ` placements ` distributed tensor It simply list ints where dim_map i denotes sharding mapping mesh dimension len dim_map == dist_tensor ndim dim_map i = - means tensor dim i replicate mesh dim_map i = j means tensor dim i shard mesh dim j For example we have dist tensor have shape device_mesh placements Shard dim_map placement would - - This representation pretty helpful during sharding propagation where we could know exactly each tensor dimension sharded Note placements contains ` _Partial ` we have explicitly deal so when we create DTensorSpec dim_map we could properly record pending sums dims mapping dist tensor sharding size tensor ndim - represent replicate int = represent shard device mesh dim r = - ndim i placement enumerate placements placement is_shard shard_dim = cast Shard placement dim r shard_dim - raise ValueError f Tensor dim shard_dim already sharded mesh dim r shard_dim DTensor operator implementation does support things like hybrid sharding strategies yet i e Shard Shard r shard_dim = i r property num_shards_map - list int dim_map property we derive ` placements ` distributed tensor Unlike ` dim_map ` ` num_shards_map ` denotes how many shards each tensor dim has Like ` dim_map ` len num_shards_map == dist_tensor ndim num_shards_map i = means tensor dim i sharded num_shards_map i = j means tensor dim i has j shards total For example we have dist tensor shape device_mesh placements Shard Shard num_shards_map distributed tensor would r = ndim i placement enumerate placements placement is_shard shard_dim = cast Shard placement dim r shard_dim = mesh size i r property sums - list int sums property we derive ` placements ` distributed tensor It simply list ints where sums i denotes pending sum partial mesh dim i idx idx placement enumerate placements placement is_partial classmethod from_dim_map cls mesh DeviceMesh dim_map list int sums list int tensor_meta Optional TensorMeta = None - DTensorSpec Construct DTensorSpec dim_map list pending sum Args mesh ` DeviceMesh ` device mesh used DTensorSpec dim_map List int list integer represents sharding each tensor dimension see ` dim_map ` property doc details sums List int list integer represents dist tensor have pending sum which device mesh dimension tensor meta TensorMeta DTensor metadata Return ` DTensorSpec ` object default replicate device mesh dims placements list Placement = Replicate _ range mesh ndim find all mesh dims need pending reductions s sums placements s = Partial i m enumerate dim_map m = placement = placements m placement is_shard placement = cast Shard placement raise RuntimeError f DeviceMesh dimension can t mapped two dimension same tensor i placement dim placement is_partial raise RuntimeError f DeviceMesh dimension m cannot both shard partial placements m = Shard i cls mesh tuple placements tensor_meta=tensor_meta is_replicated - bool True current DTensorSpec replicates all mesh dims devices all placement is_replicate placement placements is_sharded - bool True current DTensorSpec sharded any mesh dims devices any placement is_shard placement placements shallow_copy_with_tensor_meta tensor_meta Optional TensorMeta - DTensorSpec Shallow copy DTensorSpec new tensor_meta assert tensor_meta None shallow copy no tensor_meta DTensorSpec mesh placements tensor_meta=tensor_meta