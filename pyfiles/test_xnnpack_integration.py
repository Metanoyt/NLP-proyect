Owner s oncall mobile io itertools unittest hypothesis assume given strategies st torch torch backends xnnpack torch testing _internal hypothesis_utils hu torch nn functional F torch testing FileCheck torch testing _internal common_utils IS_FBCODE run_tests slowTest TEST_WITH_TSAN TestCase torch utils mobile_optimizer optimize_for_mobile unittest skipUnless torch backends xnnpack enabled XNNPACK must enabled these tests Please build USE_XNNPACK= unittest skipIf TEST_WITH_TSAN TSAN fails XNNPACK Does seem have good reason failures TestXNNPACKOps TestCase unittest skip Fails some platforms see https github com pytorch pytorch issues given batch_size=st integers data_shape=hu array_shapes weight_output_dim=st integers use_bias=st booleans test_linear batch_size data_shape weight_output_dim use_bias data_shape = batch_size + list data_shape input_data = torch rand data_shape weight = torch rand weight_output_dim data_shape - use_bias bias = torch rand weight_output_dim bias = None ref_result = F linear input_data weight bias packed_weight_bias = torch ops prepacked linear_clamp_prepack weight bias output_linearprepacked = torch ops prepacked linear_clamp_run input_data packed_weight_bias torch testing assert_close ref_result output_linearprepacked rtol= e- atol= e- given input_size=st integers weight_output_dim=st integers use_bias=st booleans test_linear_ d_input input_size weight_output_dim use_bias input_data = torch rand input_size weight = torch rand weight_output_dim input_data shape - use_bias bias = torch rand weight_output_dim bias = None ref_result = F linear input_data weight bias packed_weight_bias = torch ops prepacked linear_clamp_prepack weight bias output_linearprepacked = torch ops prepacked linear_clamp_run input_data packed_weight_bias torch testing assert_close ref_result output_linearprepacked rtol= e- atol= e- given batch_size=st integers input_channels_per_group=st integers height=st integers width=st integers output_channels_per_group=st integers groups=st integers kernel_h=st integers kernel_w=st integers stride_h=st integers stride_w=st integers pad_h=st integers pad_w=st integers dilation=st integers use_bias=st booleans format=st sampled_from None torch preserve_format torch contiguous_format torch channels_last test_conv d batch_size input_channels_per_group height width output_channels_per_group groups kernel_h kernel_w stride_h stride_w pad_h pad_w dilation use_bias format input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups kernels = kernel_h kernel_w strides = stride_h stride_w paddings = pad_h pad_w dilations = dilation dilation assume height + paddings = dilations kernels - + assume width + paddings = dilations kernels - + input_data = torch rand batch_size input_channels height width format None input_data = input_data contiguous memory_format=format weight = torch rand output_channels input_channels_per_group kernel_h kernel_w bias = None use_bias bias = torch rand output_channels ref_result = F conv d input_data weight bias strides paddings dilations groups packed_weight_bias = torch ops prepacked conv d_clamp_prepack weight bias strides paddings dilations groups xnnpack_result = torch ops prepacked conv d_clamp_run input_data packed_weight_bias torch testing assert_close ref_result xnnpack_result rtol= e- atol= e- given batch_size=st integers input_channels_per_group=st integers height=st integers width=st integers output_channels_per_group=st integers groups=st integers kernel_h=st integers kernel_w=st integers stride_h=st integers stride_w=st integers pad_h=st integers pad_w=st integers output_pad_h=st integers output_pad_w=st integers dilation=st integers use_bias=st booleans format=st sampled_from None torch preserve_format torch contiguous_format torch channels_last test_conv d_transpose batch_size input_channels_per_group height width output_channels_per_group groups kernel_h kernel_w stride_h stride_w pad_h pad_w output_pad_h output_pad_w dilation use_bias format input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups kernels = kernel_h kernel_w strides = stride_h stride_w paddings = pad_h pad_w output_paddings = output_pad_h output_pad_w dilations = dilation dilation assume height + paddings = dilations kernels - + assume width + paddings = dilations kernels - + assume output_pad_h stride_h output_pad_h dilation assume output_pad_w stride_w output_pad_w dilation input_data = torch rand batch_size input_channels height width format None input_data = input_data contiguous memory_format=format weight = torch rand input_channels output_channels_per_group kernel_h kernel_w bias = None use_bias bias = torch rand output_channels Note groups dilation reverse order conv d ref_result = F conv_transpose d input_data weight bias strides paddings output_paddings groups dilation packed_weight_bias = torch ops prepacked conv d_transpose_clamp_prepack weight bias strides paddings output_paddings dilations groups xnnpack_result = torch ops prepacked conv d_transpose_clamp_run input_data packed_weight_bias torch testing assert_close ref_result contiguous xnnpack_result contiguous rtol= e- atol= e- unittest skipUnless torch backends xnnpack enabled XNNPACK must enabled these tests Please build USE_XNNPACK= unittest skipIf TEST_WITH_TSAN TSAN fails XNNPACK Does seem have good reason failures TestXNNPACKSerDes TestCase unittest skip Fails some platforms see https github com pytorch pytorch issues given batch_size=st integers data_shape=hu array_shapes weight_output_dim=st integers use_bias=st booleans test_linear batch_size data_shape weight_output_dim use_bias Linear torch nn Module __init__ weight bias=None super __init__ weight = weight bias = bias forward x F linear x weight bias LinearPrePacked torch nn Module __init__ weight bias=None super __init__ packed_weight_bias = torch ops prepacked linear_clamp_prepack weight bias forward x torch ops prepacked linear_clamp_run x packed_weight_bias data_shape = batch_size + list data_shape weight = torch rand weight_output_dim data_shape - use_bias bias = torch rand weight_output_dim bias = None scripted_linear = torch jit script Linear weight bias scripted_linear_clamp_prepacked = torch jit script LinearPrePacked weight bias input_data = torch rand data_shape ref_result = scripted_linear input_data output_linearprepacked = scripted_linear_clamp_prepacked input_data torch testing assert_close ref_result output_linearprepacked rtol= e- atol= e- Serialize modules then deserialize input_data = torch rand data_shape buffer = io BytesIO torch jit save scripted_linear buffer buffer seek deserialized_linear = torch jit load buffer buffer = io BytesIO torch jit save scripted_linear_clamp_prepacked buffer buffer seek deserialized_linear_clamp_prepacked = torch jit load buffer ref_result = deserialized_linear input_data output_linearprepacked = deserialized_linear_clamp_prepacked input_data torch testing assert_close ref_result output_linearprepacked rtol= e- atol= e- given batch_size=st integers input_channels_per_group=st integers height=st integers width=st integers output_channels_per_group=st integers groups=st integers kernel_h=st integers kernel_w=st integers stride_h=st integers stride_w=st integers pad_h=st integers pad_w=st integers dilation=st integers use_bias=st booleans format=st sampled_from None torch preserve_format torch contiguous_format torch channels_last test_conv d batch_size input_channels_per_group height width output_channels_per_group groups kernel_h kernel_w stride_h stride_w pad_h pad_w dilation use_bias format Conv D torch nn Module __init__ weight bias strides paddings dilations groups super __init__ weight = weight bias = bias strides = strides paddings = paddings dilations = dilations groups = groups forward x F conv d x weight bias strides paddings dilations groups Conv DPrePacked torch nn Module __init__ weight bias strides paddings dilations groups super __init__ packed_weight_bias = torch ops prepacked conv d_clamp_prepack weight bias strides paddings dilations groups forward x torch ops prepacked conv d_clamp_run x packed_weight_bias input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups kernels = kernel_h kernel_w strides = stride_h stride_w paddings = pad_h pad_w dilations = dilation dilation assume height + paddings = dilations kernels - + assume width + paddings = dilations kernels - + input_data = torch rand batch_size input_channels height width format None input_data = input_data contiguous memory_format=format weight = torch rand output_channels input_channels_per_group kernel_h kernel_w bias = None use_bias bias = torch rand output_channels scripted_conv d = torch jit script Conv D weight bias strides paddings dilations groups scripted_conv d_clamp_prepacked = torch jit script Conv DPrePacked weight bias strides paddings dilations groups ref_result = scripted_conv d input_data xnnpack_result = scripted_conv d_clamp_prepacked input_data torch testing assert_close ref_result xnnpack_result rtol= e- atol= e- Serialize modules then deserialize input_data = torch rand batch_size input_channels height width format None input_data = input_data contiguous memory_format=format buffer = io BytesIO torch jit save scripted_conv d buffer buffer seek deserialized_conv d = torch jit load buffer buffer = io BytesIO torch jit save scripted_conv d_clamp_prepacked buffer buffer seek deserialized_conv d_clamp_prepacked = torch jit load buffer ref_result = deserialized_conv d input_data xnnpack_result = deserialized_conv d_clamp_prepacked input_data torch testing assert_close ref_result xnnpack_result rtol= e- atol= e- given batch_size=st integers input_channels_per_group=st integers height=st integers width=st integers output_channels_per_group=st integers groups=st integers kernel_h=st integers kernel_w=st integers stride_h=st integers stride_w=st integers pad_h=st integers pad_w=st integers output_pad_h=st integers output_pad_w=st integers dilation=st integers use_bias=st booleans format=st sampled_from None torch preserve_format torch contiguous_format torch channels_last test_conv d_transpose batch_size input_channels_per_group height width output_channels_per_group groups kernel_h kernel_w stride_h stride_w pad_h pad_w output_pad_h output_pad_w dilation use_bias format Conv DT torch nn Module __init__ weight bias strides paddings output_paddings dilations groups super __init__ weight = weight bias = bias strides = strides paddings = paddings output_paddings = output_paddings dilations = dilations groups = groups forward x F conv_transpose d x weight bias strides paddings output_paddings groups dilations Conv DTPrePacked torch nn Module __init__ weight bias strides paddings output_paddings dilations groups super __init__ packed_weight_bias = torch ops prepacked conv d_transpose_clamp_prepack weight bias strides paddings output_paddings dilations groups forward x torch ops prepacked conv d_transpose_clamp_run x packed_weight_bias input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups kernels = kernel_h kernel_w strides = stride_h stride_w paddings = pad_h pad_w output_paddings = output_pad_h output_pad_w dilations = dilation dilation assume height + paddings = dilations kernels - + assume width + paddings = dilations kernels - + assume output_pad_h stride_h output_pad_h dilation assume output_pad_w stride_w output_pad_w dilation input_data = torch rand batch_size input_channels height width format None input_data = input_data contiguous memory_format=format weight = torch rand input_channels output_channels_per_group kernel_h kernel_w bias = None use_bias bias = torch rand output_channels scripted_conv d = torch jit script Conv DT weight bias strides paddings output_paddings dilations groups scripted_conv d_clamp_prepacked = torch jit script Conv DTPrePacked weight bias strides paddings output_paddings dilations groups ref_result = scripted_conv d input_data xnnpack_result = scripted_conv d_clamp_prepacked input_data torch testing assert_close ref_result xnnpack_result rtol= e- atol= e- Serialize modules then deserialize input_data = torch rand batch_size input_channels height width format None input_data = input_data contiguous memory_format=format buffer = io BytesIO torch jit save scripted_conv d buffer buffer seek deserialized_conv d = torch jit load buffer buffer = io BytesIO torch jit save scripted_conv d_clamp_prepacked buffer buffer seek deserialized_conv d_clamp_prepacked = torch jit load buffer ref_result = deserialized_conv d input_data xnnpack_result = deserialized_conv d_clamp_prepacked input_data torch testing assert_close ref_result xnnpack_result rtol= e- atol= e- unittest skip Fails some platforms see https github com pytorch pytorch issues given batch_size=st integers input_channels_per_group=st integers height=st integers width=st integers output_channels_per_group=st integers groups=st integers kernel_h=st integers kernel_w=st integers stride_h=st integers stride_w=st integers pad_h=st integers pad_w=st integers dilation=st integers linear_weight_output_dim=st integers use_bias=st booleans format=st sampled_from None torch preserve_format torch contiguous_format torch channels_last test_combined_model batch_size input_channels_per_group height width output_channels_per_group groups kernel_h kernel_w stride_h stride_w pad_h pad_w dilation linear_weight_output_dim use_bias format M torch nn Module __init__ conv_weight conv_bias linear_weight linear_bias strides paddings dilations groups super __init__ conv_weight = conv_weight conv_bias = conv_bias linear_weight = linear_weight linear_bias = linear_bias strides = strides paddings = paddings dilations = dilations groups = groups forward x o = F conv d x conv_weight conv_bias strides paddings dilations groups o = o permute o = F linear o linear_weight linear_bias F relu o MPrePacked torch nn Module __init__ conv_weight conv_bias linear_weight linear_bias strides paddings dilations groups super __init__ conv d_clamp_run_weight_bias = torch ops prepacked conv d_clamp_prepack conv_weight conv_bias strides paddings dilations groups linear_clamp_run_weight_bias = torch ops prepacked linear_clamp_prepack linear_weight linear_bias forward x o = torch ops prepacked conv d_clamp_run x conv d_clamp_run_weight_bias o = o permute o = torch ops prepacked linear_clamp_run o linear_clamp_run_weight_bias F relu o input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups kernels = kernel_h kernel_w strides = stride_h stride_w paddings = pad_h pad_w dilations = dilation dilation assume height + paddings = dilations kernels - + assume width + paddings = dilations kernels - + input_data = torch rand batch_size input_channels height width format None input_data = input_data contiguous memory_format=format conv_weight = torch rand output_channels input_channels_per_group kernel_h kernel_w conv_bias = None use_bias conv_bias = torch rand output_channels This done just find output shape result so shape weight following linear layer can determined result = F conv d input_data conv_weight conv_bias strides paddings dilations groups linear_input_shape = result shape linear_weight = torch rand linear_weight_output_dim linear_input_shape linear_bias = None use_bias linear_bias = torch rand linear_weight_output_dim scripted_m = torch jit script M conv_weight conv_bias linear_weight linear_bias strides paddings dilations groups scripted_m_prepacked = torch jit script MPrePacked conv_weight conv_bias linear_weight linear_bias strides paddings dilations groups ref_result = scripted_m input_data xnnpack_result = scripted_m_prepacked input_data torch testing assert_close ref_result xnnpack_result rtol= e- atol= e- Serialize modules then deserialize input_data = torch rand batch_size input_channels height width input_data = input_data contiguous memory_format=torch channels_last buffer = io BytesIO torch jit save scripted_m buffer buffer seek deserialized_m = torch jit load buffer buffer = io BytesIO torch jit save scripted_m_prepacked buffer buffer seek deserialized_m_prepacked = torch jit load buffer ref_result = deserialized_m input_data xnnpack_result = deserialized_m_prepacked input_data torch testing assert_close ref_result xnnpack_result rtol= e- atol= e- unittest skipUnless torch backends xnnpack enabled XNNPACK must enabled these tests Please build USE_XNNPACK= unittest skipIf TEST_WITH_TSAN TSAN fails XNNPACK Does seem have good reason failures TestXNNPACKRewritePass TestCase staticmethod validate_transformed_module To please flake pattern_count_map data_shape prepack_removal=False fuse_clamping_ops=False input_data = torch normal size=data_shape jit_method script trace module_instance = jit_method == script scripted_model = torch jit script module_instance scripted_model = torch jit trace module_instance input_data scripted_model eval ref_result = scripted_model input_data torch _C _jit_pass_insert_prepacked_ops scripted_model _c fuse_clamping_ops prepack_removal scripted_model _c = torch _C _freeze_module scripted_model _c fuse_clamping_ops torch _C _jit_pass_fuse_clamp_w_prepacked_linear_conv scripted_model _c prepack_removal torch _C _jit_pass_fold_prepacking_ops scripted_model _c buffer = io BytesIO torch jit save scripted_model buffer buffer seek deserialized_scripted_model = torch jit load buffer pattern v pattern_count_map items v == FileCheck check pattern run deserialized_scripted_model graph v == - FileCheck check_not pattern run deserialized_scripted_model graph FileCheck check_count pattern v exactly=True run deserialized_scripted_model graph xnnpack_result = deserialized_scripted_model input_data torch testing assert_close ref_result xnnpack_result rtol= e- atol= e- test_linear data_shape = weight_output_dim = weight_shape = weight_output_dim data_shape - Linear torch nn Module __init__ - None super __init__ weight = torch nn Parameter torch rand weight_shape requires_grad=False bias = torch nn Parameter torch rand weight_output_dim requires_grad=False forward x F linear x weight bias LinearNoBias torch nn Module __init__ - None super __init__ weight = torch nn Parameter torch rand weight_shape requires_grad=False forward x F linear x weight None Linear bias pattern pattern_count_map = Tensor = prim CallFunction - prepacked linear_clamp_prepack prepacked linear_clamp_run TestXNNPACKRewritePass validate_transformed_module Linear pattern_count_map data_shape TestXNNPACKRewritePass validate_transformed_module LinearNoBias pattern_count_map data_shape Conv params batch_size = input_channels_per_group = height = width = output_channels_per_group = groups = kernel_h = kernel_w = stride_h = stride_w = pad_h = pad_w = output_pad_h = output_pad_w = dilation = input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups strides = stride_h stride_w paddings = pad_h pad_w output_paddings = output_pad_h output_pad_w dilations = dilation dilation conv_weight_shape = output_channels input_channels_per_group kernel_h kernel_w conv_transpose_weight_shape = input_channels output_channels_per_group kernel_h kernel_w conv_bias_shape = output_channels Conv D torch nn Module __init__ - None super __init__ weight = torch nn Parameter torch rand conv_weight_shape requires_grad=False bias = torch nn Parameter torch rand conv_bias_shape requires_grad=False strides = strides paddings = paddings dilations = dilations groups = groups forward x F conv d x weight bias strides paddings dilations groups Conv DT torch nn Module __init__ - None super __init__ weight = torch nn Parameter torch rand conv_transpose_weight_shape requires_grad=False bias = torch nn Parameter torch rand conv_bias_shape requires_grad=False strides = strides paddings = paddings output_paddings = output_paddings dilations = dilations groups = groups forward x F conv_transpose d x weight bias strides paddings output_paddings groups dilations data_shape = batch_size input_channels height width pattern_count_map = Tensor = aten conv d - prepacked conv d_clamp_prepack prepacked conv d_clamp_run TestXNNPACKRewritePass validate_transformed_module Conv D pattern_count_map data_shape transpose_data_shape = batch_size input_channels height width noqa F transpose_pattern_count_map = Tensor = aten conv_transpose d - prepacked conv d_transpose_clamp_prepack prepacked conv d_transpose_clamp_run TestXNNPACKRewritePass validate_transformed_module Conv DT transpose_pattern_count_map data_shape input_data = torch rand batch_size input_channels height width conv_weight = torch rand output_channels input_channels_per_group kernel_h kernel_w conv_bias = torch rand output_channels result = F conv d input_data conv_weight conv_bias strides paddings dilations groups linear_input_shape = result shape linear_weight_shape = weight_output_dim linear_input_shape M torch nn Module __init__ activation_fn=F relu super __init__ conv_weight = torch nn Parameter torch rand conv_weight_shape requires_grad=False conv_bias = torch nn Parameter torch rand conv_bias_shape requires_grad=False linear_weight = torch nn Parameter torch rand linear_weight_shape requires_grad=False linear_bias = torch nn Parameter torch rand weight_output_dim requires_grad=False strides = strides paddings = paddings dilations = dilations groups = groups activation_fn = activation_fn forward x o = F conv d x conv_weight conv_bias strides paddings dilations groups o = activation_fn o o = o permute o = F linear o linear_weight linear_bias activation_fn o pattern_count_map = Tensor = aten conv d - prepacked conv d_clamp_prepack prepacked conv d_clamp_run prepacked linear_clamp_prepack prepacked linear_clamp_run TestXNNPACKRewritePass validate_transformed_module M pattern_count_map data_shape pattern_count_map prepacked conv d_clamp_prepack = - pattern_count_map Tensor = prim CallFunction = - pattern_count_map prepacked linear_clamp_prepack = - TestXNNPACKRewritePass validate_transformed_module M pattern_count_map data_shape prepack_removal=True Not inplace relu fusion test pattern_count_map = aten relu prepacked conv d_clamp_prepack - prepacked conv d_clamp_run prepacked linear_clamp_prepack - prepacked linear_clamp_run TestXNNPACKRewritePass validate_transformed_module M pattern_count_map data_shape prepack_removal=True pattern_count_map prepacked conv d_clamp_prepack = - pattern_count_map prepacked linear_clamp_prepack = - pattern_count_map aten relu = - TestXNNPACKRewritePass validate_transformed_module M pattern_count_map data_shape prepack_removal=True fuse_clamping_ops=True Inplace relu fusion test pattern_count_map = aten relu prepacked conv d_clamp_prepack - prepacked conv d_clamp_run prepacked linear_clamp_prepack - prepacked linear_clamp_run TestXNNPACKRewritePass validate_transformed_module M F relu_ pattern_count_map data_shape prepack_removal=True pattern_count_map prepacked conv d_clamp_prepack = - pattern_count_map prepacked linear_clamp_prepack = - pattern_count_map aten relu = - TestXNNPACKRewritePass validate_transformed_module M F relu_ pattern_count_map data_shape prepack_removal=True fuse_clamping_ops=True Not inplace hardtanh fusion test pattern_count_map = aten hardtanh prepacked conv d_clamp_prepack - prepacked conv d_clamp_run prepacked linear_clamp_prepack - prepacked linear_clamp_run TestXNNPACKRewritePass validate_transformed_module M F hardtanh pattern_count_map data_shape prepack_removal=True pattern_count_map prepacked conv d_clamp_prepack = - pattern_count_map prepacked linear_clamp_prepack = - pattern_count_map aten hardtanh = - TestXNNPACKRewritePass validate_transformed_module M F hardtanh pattern_count_map data_shape prepack_removal=True fuse_clamping_ops=True Inplace hardtanh fusion test pattern_count_map = aten hardtanh_ prepacked conv d_clamp_prepack - prepacked conv d_clamp_run prepacked linear_clamp_prepack - prepacked linear_clamp_run TestXNNPACKRewritePass validate_transformed_module M F hardtanh_ pattern_count_map data_shape prepack_removal=True pattern_count_map prepacked conv d_clamp_prepack = - pattern_count_map prepacked linear_clamp_prepack = - pattern_count_map aten hardtanh_ = - TestXNNPACKRewritePass validate_transformed_module M F hardtanh_ pattern_count_map data_shape prepack_removal=True fuse_clamping_ops=True MFusionAntiPattern torch nn Module __init__ - None super __init__ linear_weight = torch nn Parameter torch rand linear_weight_shape requires_grad=False linear_bias = torch nn Parameter torch rand weight_output_dim requires_grad=False strides = strides paddings = paddings dilations = dilations groups = groups forward x o = F linear x linear_weight linear_bias o = F relu o o = F hardtanh o o Unfusable hardtanh pattern_count_map = aten hardtanh hardtanh cannot aten relu - relu fused prepacked linear_clamp_prepack - prepacked linear_clamp_run TestXNNPACKRewritePass validate_transformed_module MFusionAntiPattern pattern_count_map linear_weight_shape prepack_removal=True fuse_clamping_ops=True MFusionAntiPatternParamMinMax torch nn Module __init__ - None super __init__ linear_weight = torch nn Parameter torch rand linear_weight_shape requires_grad=False linear_bias = torch nn Parameter torch rand weight_output_dim requires_grad=False strides = strides paddings = paddings dilations = dilations groups = groups forward x min = x max = min + o = F linear x linear_weight linear_bias o = F hardtanh o min max o Unfusable hardtanh pattern_count_map = aten hardtanh hardtanh cannot prepacked linear_clamp_prepack - prepacked linear_clamp_run TestXNNPACKRewritePass validate_transformed_module MFusionAntiPatternParamMinMax pattern_count_map linear_weight_shape prepack_removal=True fuse_clamping_ops=True test_decomposed_linear data_shape = weight_output_dim = weight_shape = weight_output_dim data_shape - DecomposedLinearAddmm torch nn Module __init__ - None super __init__ weight = torch nn Parameter torch rand weight_shape requires_grad=False bias = torch nn Parameter torch rand weight_output_dim requires_grad=False forward x weight_t = weight t torch addmm bias x weight_t DecomposedLinearMatmulAdd torch nn Module __init__ - None super __init__ weight = torch nn Parameter torch rand weight_shape requires_grad=False bias = torch nn Parameter torch rand weight_output_dim requires_grad=False forward x weight_t = weight t y = torch matmul x weight_t res = y add_ bias res DecomposedLinearMatmul torch nn Module __init__ - None super __init__ weight = torch nn Parameter torch rand weight_shape requires_grad=False bias = torch nn Parameter torch rand weight_output_dim requires_grad=False forward x weight_t = weight t res = torch matmul x weight_t res Linear bias pattern pattern_count_map = Tensor = prim CallFunction - prepacked linear_clamp_prepack prepacked linear_clamp_run TestXNNPACKRewritePass validate_transformed_module DecomposedLinearAddmm pattern_count_map data_shape TestXNNPACKRewritePass validate_transformed_module DecomposedLinearMatmulAdd pattern_count_map data_shape TestXNNPACKRewritePass validate_transformed_module DecomposedLinearMatmul pattern_count_map data_shape unittest skipUnless torch backends xnnpack enabled XNNPACK must enabled these tests Please build USE_XNNPACK= unittest skipIf TEST_WITH_TSAN TSAN fork-safe since we re forking multi-threaded environment TestXNNPACKConv dTransformPass TestCase staticmethod validate_transform_conv d_to_conv d pattern_count_transformed_map pattern_count_optimized_map data_shape input_data = torch normal size=data_shape jit_method script trace module_instance = jit_method == script scripted_model = torch jit script module_instance scripted_model = torch jit trace module_instance input_data scripted_model eval ref_result = scripted_model input_data torch _C _jit_pass_transform_conv d_to_conv d scripted_model _c optimized_scripted_model = optimize_for_mobile scripted_model buffer = io BytesIO torch jit save scripted_model buffer buffer seek deserialized_scripted_model = torch jit load buffer pattern v pattern_count_transformed_map items v == FileCheck check pattern run deserialized_scripted_model graph v == - FileCheck check_not pattern run deserialized_scripted_model graph FileCheck check_count pattern v exactly=True run deserialized_scripted_model graph transformed_result = deserialized_scripted_model input_data torch testing assert_close ref_result transformed_result rtol= e- atol= e- optimized_buffer = io BytesIO torch jit save optimized_scripted_model optimized_buffer optimized_buffer seek deserialized_optimized_scripted_model = torch jit load optimized_buffer pattern v pattern_count_optimized_map items v == FileCheck check pattern run deserialized_optimized_scripted_model graph v == - FileCheck check_not pattern run deserialized_optimized_scripted_model graph FileCheck check_count pattern v exactly=True run deserialized_optimized_scripted_model graph xnnpack_result = deserialized_optimized_scripted_model input_data torch testing assert_close ref_result xnnpack_result rtol= e- atol= e- unittest skipIf IS_FBCODE T test_conv d_basic batch_size_list = range input_channels_per_group_list = range width_list = range output_channels_per_group_list = range groups_list = range kernel_list = range stride_list = range padding_list = range dilation_list = range hparams itertools product batch_size_list input_channels_per_group_list width_list output_channels_per_group_list groups_list kernel_list stride_list padding_list dilation_list batch_size input_channels_per_group width output_channels_per_group groups kernel stride padding dilation = hparams input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups conv_weight_shape = output_channels input_channels_per_group kernel conv_bias_shape = output_channels Conv D torch nn Module __init__ - None super __init__ weight = torch nn Parameter torch rand conv_weight_shape requires_grad=False bias = torch nn Parameter torch rand conv_bias_shape requires_grad=False stride = stride padding = padding dilation = dilation groups = groups forward x F conv d x weight bias stride padding dilation groups data_shape = batch_size input_channels width pattern_count_transformed_map = Tensor = aten conv d - Tensor = aten conv d pattern_count_optimized_map = Tensor = aten conv d - Tensor = aten conv d - prepacked conv d_clamp_prepack - prepacked conv d_clamp_run TestXNNPACKConv dTransformPass validate_transform_conv d_to_conv d Conv D pattern_count_transformed_map pattern_count_optimized_map data_shape See https github com pytorch pytorch issues slowTest test_conv d_with_relu_fc batch_size_list = range input_channels_per_group_list = range width_list = range output_channels_per_group_list = range groups_list = range kernel_list = range stride_list = range padding_list = range dilation_list = range output_features_list = range hparams itertools product batch_size_list input_channels_per_group_list width_list output_channels_per_group_list groups_list kernel_list stride_list padding_list dilation_list output_features_list batch_size input_channels_per_group width output_channels_per_group groups kernel stride padding dilation output_features = hparams input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups conv_weight_shape = output_channels input_channels_per_group kernel conv_bias_shape = output_channels conv_output_width = int width + padding - dilation kernel - - stride + fc_weight_shape = output_features output_channels conv_output_width fc_bias_shape = output_features Net torch nn Module __init__ - None super __init__ conv_weight = torch nn Parameter torch rand conv_weight_shape requires_grad=False conv_bias = torch nn Parameter torch rand conv_bias_shape requires_grad=False stride = stride padding = padding dilation = dilation groups = groups fc_weight = torch nn Parameter torch rand fc_weight_shape requires_grad=False fc_bias = torch nn Parameter torch rand fc_bias_shape requires_grad=False forward x x = F conv d x conv_weight conv_bias stride padding dilation groups x = F relu x x = x view x size - x = F linear x fc_weight fc_bias x data_shape = batch_size input_channels width pattern_count_transformed_map = Tensor = aten conv d - Tensor = aten conv d pattern_count_optimized_map = Tensor = aten conv d - Tensor = aten conv d - prepacked conv d_clamp_prepack - prepacked conv d_clamp_run TestXNNPACKConv dTransformPass validate_transform_conv d_to_conv d Net pattern_count_transformed_map pattern_count_optimized_map data_shape __name__ == __main__ run_tests