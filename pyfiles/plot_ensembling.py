========================== Model ensembling ========================== This example illustrates how vectorize model ensembling using vmap What model ensembling -------------------------------------------------------------------- Model ensembling combines predictions multiple models together Traditionally done running each model some inputs separately then combining predictions However you re running models same architecture then may possible combine them together using ` ` vmap ` ` ` ` vmap ` ` function transform maps functions across dimensions input tensors One its use cases eliminating for-loops speeding them up through vectorization Let s demonstrate how do using ensemble simple CNNs torch torch nn nn torch nn functional F torch manual_seed Here s simple CNN SimpleCNN nn Module __init__ super __init__ conv = nn Conv d conv = nn Conv d fc = nn Linear fc = nn Linear forward x x = conv x x = F relu x x = conv x x = F relu x x = F max_pool d x x = torch flatten x x = fc x x = F relu x x = fc x output = F log_softmax x dim= output = x output Let s generate some dummy data Pretend we re working MNIST dataset where images Furthermore let s say we wish combine predictions different models device = cuda num_models = data = torch randn device=device targets = torch randint device=device models = SimpleCNN device _ range num_models We have couple options generating predictions Maybe we want give each model different randomized minibatch data maybe we want run same minibatch data through each model e g we testing effect different model initializations Option different minibatch each model minibatches = data num_models predictions = model minibatch model minibatch zip models minibatches Option Same minibatch minibatch = data predictions = model minibatch model models ###################################################################### Using vmap vectorize ensemble -------------------------------------------------------------------- Let s use ` ` vmap ` ` speed up for-loop We must first prepare models use ` ` vmap ` ` First let s combine states model together stacking each parameter For example model i fc weight has shape we going stack fc weight each models produce big weight shape functorch offers following convenience function do It returns stateless version model fmodel stacked parameters buffers functorch combine_state_for_ensemble fmodel params buffers = combine_state_for_ensemble models p requires_grad_ p params Option get predictions using different minibatch each model By default vmap maps function across first dimension all inputs passed-in function After ` combine_state_for_ensemble ` each ` ` params ` ` ` ` buffers ` ` have additional dimension size ` ` num_models ` ` front ` ` minibatches ` ` has dimension size ` ` num_models ` ` print p size p params assert minibatches shape == num_models functorch vmap predictions _vmap = vmap fmodel params buffers minibatches assert torch allclose predictions _vmap torch stack predictions atol= e- rtol= e- Option get predictions using same minibatch data vmap has in_dims arg specify which dimensions map over Using ` ` None ` ` we tell vmap we want same minibatch apply all models predictions _vmap = vmap fmodel in_dims= None params buffers minibatch assert torch allclose predictions _vmap torch stack predictions atol= e- rtol= e- A quick note there limitations around what types functions can transformed vmap The best functions transform ones pure functions function where outputs only determined inputs have no side effects e g mutation vmap unable handle mutation arbitrary Python data structures able handle many in-place PyTorch operations