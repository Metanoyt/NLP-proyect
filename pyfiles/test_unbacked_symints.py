Owner s module inductor functools unittest torch torch _dynamo config dynamo_config torch _inductor config inductor_config torch _inductor test_case TestCase InductorTestCase torch testing make_tensor torch testing _internal common_device_type instantiate_device_type_tests skipCPUIf skipGPUIf torch testing _internal common_utils parametrize skipIfXpu torch testing _internal inductor_utils HAS_GPU TestUnbackedSymints InductorTestCase skipGPUIf HAS_GPU requires gpu triton dynamo_config patch capture_dynamic_output_shape_ops True test_expand device fn x y nz = torch nonzero x unbacked symint nz size x_exp = nz expand - unbacked symint target sizes y_exp = y expand - nz size x_exp y_exp example_inputs = torch randn device=device torch randn device=device actual = torch compile fn fullgraph=True example_inputs expected = fn example_inputs torch testing assert_close actual expected skipIfXpu msg= The OP aten nonzero implemented XPU has different memory layout fake tensor Remove skip after fixed skipGPUIf HAS_GPU requires gpu triton dynamo_config patch capture_dynamic_output_shape_ops True test_expand_ok_with_runtime_assert device fn x nz = x nonzero torch _check nz size == nz expand - x = make_tensor device=device dtype=torch float exclude_zero=True torch compile fn fullgraph=True x skipGPUIf HAS_GPU requires gpu triton dynamo_config patch capture_dynamic_output_shape_ops True test_broadcast_tensors device fn x nz = x nonzero = torch zeros nz size b = torch ones nz size b x = torch randn device=device actual = torch compile fn fullgraph=True x expected = fn x torch testing assert_close actual expected skipGPUIf HAS_GPU requires gpu triton dynamo_config patch capture_dynamic_output_shape_ops True test_autotuning device fn x y nz = torch nonzero x unbacked symint GEMM input shape = x new_ones nz size y size y example_inputs = torch randn device=device torch randn device=device inductor_config patch max_autotune_gemm True actual = torch compile fn fullgraph=True example_inputs expected = fn example_inputs torch testing assert_close actual expected skipGPUIf HAS_GPU requires gpu triton dynamo_config patch capture_scalar_outputs True test_split_with_sizes device fn x y l = y tolist s = torch split x l d = l + l + l s sum d example_inputs = torch randn device=device torch tensor actual = torch compile fn fullgraph=True example_inputs expected = fn example_inputs torch testing assert_close actual expected skipGPUIf HAS_GPU requires gpu triton dynamo_config patch capture_dynamic_output_shape_ops True test_view_of_slice device Tests View create slice size_with_unbacked_symint fn x nz = torch nonzero x introduce unbacked symint squared = nz nz avoid ReinterpretView when lowering Slice sliced = torch ops aten slice Tensor squared dim= start=- end=None view = sliced unsqueeze dim= view squeeze dim= make sure no unbacked symint output s stride example_inputs = torch randn device=device actual = torch compile fn fullgraph=True example_inputs expected = fn example_inputs torch testing assert_close actual expected skipGPUIf HAS_GPU requires gpu triton dynamo_config patch capture_scalar_outputs True test_triton_kernel_grid device device == cpu raise unittest SkipTest Triton kernel requires GPU torch testing _internal triton_utils add_kernel fn x maxlen = max x item = torch ones maxlen device=device b = torch ones maxlen device=device out = torch zeros_like unbacked symint grid add_kernel maxlen b out maxlen out example_inputs = torch randint high= size= device=device actual = torch compile fn fullgraph=True example_inputs expected = fn example_inputs torch testing assert_close actual expected skipGPUIf HAS_GPU requires gpu triton dynamo_config patch capture_dynamic_output_shape_ops True test_nonzero_in_inference_mode device fn x torch nonzero x example_inputs = torch randint device=device torch inference_mode actual = torch compile fn fullgraph=True example_inputs expected = fn example_inputs torch testing assert_close actual expected skipGPUIf HAS_GPU requires gpu triton inductor_config patch max_autotune True dynamo_config patch capture_scalar_outputs True test_equivalent_backed_unbacked device Tests scenario when there two equivalent backed unbacked symints when we look-up size hint unbacked symint we ignorantly use default fallback hint fn x w b Make tensors where st dim unbacked backed u s = item b size unbacked = x expand u x shape backed = x expand s x shape The cat unifies u s -- i e u == s cat = torch cat backed unbacked unbacked dim= s mat = torch permute cat s mat = w expand u w shape u bmm = torch ops aten bmm mat mat bmm example_inputs = torch randn dtype=torch float device=device torch randn dtype=torch float device=device torch tensor device=device backed = torch randn device=device torch _dynamo mark_dynamic backed create backed symint actual = torch compile fn fullgraph=True example_inputs expected = fn example_inputs torch testing assert_close actual expected skipCPUIf True precision good enough CPU skipGPUIf HAS_GPU requires gpu triton dynamo_config patch capture_scalar_outputs True test_vertical_pointwise_reduction_fusion device reset case we run both cpu cuda tests torch _inductor metrics reset Tests fusing pointwise reduction op unbacked numel rnumel fn x y repeats u = repeats item unbacked = y expand u y shape u Note We add x both pointwise reduction Otherwise scheduler will refuse fuse ops whose only common buffer has unbacked symints pointwise = unbacked + x reduction = torch sum pointwise + x pointwise reduction example_inputs = torch randn device=device torch randn device=device torch tensor device=device actual = torch compile fn fullgraph=True example_inputs expected = fn example_inputs torch testing assert_close actual expected assertEqual torch _inductor metrics generated_kernel_count skipGPUIf HAS_GPU requires gpu triton dynamo_config patch capture_scalar_outputs True parametrize torch_fn torch mm torch bmm torch addmm name_fn=lambda fn fn __name__ parametrize coordinate_descent_tuning True False name_fn=str test_mm_and_friends device torch_fn coordinate_descent_tuning torch_fn == torch addmm torch_fn = functools partial torch_fn torch ones device=device fn x w repeats is_bmm u = repeats item x_unbacked = x expand u w_unbacked = w expand u is_bmm Make sure inputs batched x_unbacked = x_unbacked expand x_unbacked shape w_unbacked = w_unbacked expand w_unbacked shape torch_fn x_unbacked w_unbacked example_inputs = torch randn device=device torch randn device=device torch tensor device=device torch_fn == torch bmm inductor_config patch coordinate_descent_tuning has its own path during decomp coordinate_descent_tuning coordinate_descent_tuning actual = torch compile fn fullgraph=True example_inputs expected = fn example_inputs torch testing assert_close actual expected skipGPUIf HAS_GPU requires gpu triton torch _dynamo config patch capture_scalar_outputs=True test_unbacked_range_tree_divisor device fn x num u = num item zeros = torch zeros u device=device dtype=torch int torch ops aten index x None zeros example_inputs = torch randn device=device torch tensor device=device actual = torch compile fn fullgraph=True example_inputs expected = fn example_inputs torch testing assert_close actual expected skipGPUIf HAS_GPU requires gpu triton dynamo_config patch capture_scalar_outputs True test_unbacked_masked_scatter device fn value mask u = mask count_nonzero source = torch ones u dtype=torch float device=device torch masked_scatter value mask source value = make_tensor dtype=torch float device=device mask = make_tensor dtype=torch bool device=device example_inputs = value mask actual = torch compile fn fullgraph=True example_inputs expected = fn example_inputs torch testing assert_close actual expected skipGPUIf HAS_GPU requires gpu triton dynamo_config patch capture_scalar_outputs True test_unbacked_repeat device fn x b u u = item b item x repeat u repeat u example_inputs = make_tensor dtype=torch float device=device torch scalar_tensor dtype=torch int device=device torch scalar_tensor dtype=torch int device=device actual = torch compile fn fullgraph=True example_inputs expected = fn example_inputs torch testing assert_close actual expected skipGPUIf HAS_GPU requires gpu triton dynamo_config patch capture_scalar_outputs True parametrize dynamic False True None test_unbacked_slice_on_subclass device dynamic torch testing _internal common_subclass WrapperTensor torch utils _pytree tree_map NB error we re testing only triggers when unbacked SymInts created within subclass s torch_dispatch because they re seen Dynamo thus considered freshly-created when subclass instance value torch_dispatch handled Subclass forwards everything along single underlying dense tensor component except slice which handles via data-dependent bounds access CustomSliceSubclass WrapperTensor classmethod get_wrapper_properties cls t slice_bounds=None t __init__ t slice_bounds=None t = t slice_bounds = slice_bounds __repr__ t_repr = repr t slice_bounds_repr = repr slice_bounds f CustomSliceSubclass t_repr slice_bounds_repr __tensor_flatten__ t slice_bounds None classmethod __tensor_unflatten__ cls inner_tensors meta outer_size outer_stride t = inner_tensors t slice_bounds = inner_tensors slice_bounds cls t slice_bounds classmethod __torch_dispatch__ cls func types args= kwargs=None func torch ops aten slice Tensor inp = args start = inp slice_bounds item torch _check start = torch _check start = inp size length = args slice_bounds - args slice_bounds item torch _check length = torch _check start + length = inp size CustomSliceSubclass func args t dim= start=start end= start + length slice_bounds=args slice_bounds all issubclass cls t t types NotImplemented kwargs None kwargs = unwrap e e t isinstance e CustomSliceSubclass e wrap e CustomSliceSubclass e isinstance e torch Tensor e rs = tree_map wrap func tree_map unwrap args tree_map unwrap kwargs rs fn t start length torch ops aten slice Tensor t dim= start=start end=start + length t = make_tensor dtype=torch float device=device sub = CustomSliceSubclass t slice_bounds=torch tensor device=t device start = length = example_inputs = sub start length actual = torch compile fn dynamic=dynamic fullgraph=True example_inputs expected = fn example_inputs torch testing assert_close actual t expected t skipGPUIf HAS_GPU requires gpu triton dynamo_config patch capture_dynamic_output_shape_ops=True test_issue_ device Model torch nn Module __init__ - None super __init__ forward arg _ arg _ arg _ arg _ arg _ arg _ arg _ index = torch ops aten index Tensor arg _ arg _ index_ = torch ops aten index Tensor arg _ arg _ unsqueeze = torch ops aten unsqueeze default index unsqueeze_ = torch ops aten unsqueeze default index_ cat = torch ops aten cat default unsqueeze unsqueeze_ - select = torch ops aten select int cat index_put = torch ops aten index_put default arg _ select arg _ arg _ index_put example_inputs = torch tensor - - - - - - - - - - device=device dtype=torch int torch tensor device=device dtype=torch int torch tensor False False True False False False False False False False True False device=device dtype=torch bool torch tensor device=device dtype=torch int torch tensor device=device dtype=torch int torch zeros device=device dtype=torch int torch tensor device=device dtype=torch int model = Model assertEqual torch compile model example_inputs model example_inputs skipGPUIf HAS_GPU torch compile gpu requires triton torch _dynamo config patch capture_scalar_outputs=True test_einsum device fn q k vector scalar unbacked = scalar item q = q repeat unbacked k = k repeat unbacked qk = torch einsum bcxd bcyd- bcxy q k qk = torch einsum b b - b q k qvec = torch einsum b b- b q vector qk qk qvec example_inputs = torch empty_strided device=device uniform_ torch empty_strided device=device uniform_ torch randn device=device torch scalar_tensor device=device dtype=torch int actual = torch compile fn fullgraph=True example_inputs expected = fn example_inputs torch testing assert_close actual expected skipGPUIf HAS_GPU requires gpu triton dynamo_config patch capture_dynamic_output_shape_ops True test_softmax device fn x nz = x nonzero float soft = torch softmax nz dim= logsoft = torch nn functional log_softmax nz dim= soft logsoft example_inputs = torch randint low= high= size= device=device dtype=torch int actual = torch compile fn fullgraph=True example_inputs expected = fn example_inputs torch testing assert_close actual expected skipGPUIf HAS_GPU requires gpu triton skipIfXpu msg= _scaled_dot_product_flash_attention supported XPU yet dynamo_config patch capture_dynamic_output_shape_ops True test_sdpfa device device == cpu raise unittest SkipTest scaled_dot_product_flash_attention has no CPU backend fn x B H d_h = nz = torch nonzero x seq_len = nz size q = torch randn B H seq_len d_h device=device dtype=torch float k = torch randn B H seq_len d_h device=device dtype=torch float v = torch randn B H seq_len d_h device=device dtype=torch float result = torch ops aten _scaled_dot_product_flash_attention default q k v dropout_p= is_causal=False scale=None result x = torch tensor device=device torch compile fn fullgraph=True x skipGPUIf HAS_GPU requires gpu triton skipIfXpu msg= scaled_dot_product_attention supported XPU yet dynamo_config patch capture_dynamic_output_shape_ops True test_sdfpa_unbacked_strides device device == cpu raise unittest SkipTest scaled_dot_product_attention has no CPU backend fn x y B H d_h = nz = torch nonzero x seq_len = nz size y = torch nonzero y size strides = H seq_len d_h seq_len d_h d_h y q = torch randn B H seq_len d_h device=device dtype=torch float k = torch randn B H seq_len d_h device=device dtype=torch float v = torch randn B H seq_len d_h device=device dtype=torch float q = torch as_strided q size= B H seq_len d_h stride=strides k = torch as_strided k size= B H seq_len d_h stride=strides v = torch as_strided v size= B H seq_len d_h stride=strides result = torch ops aten _scaled_dot_product_flash_attention default q k v dropout_p= is_causal=False scale=None result x = torch tensor device=device y = torch tensor device=device torch compile fn fullgraph=True x y skipGPUIf HAS_GPU torch compile gpu requires triton dynamo_config patch capture_dynamic_output_shape_ops True test_unbacked_linear_layer_norm_input device MyModel torch nn Module __init__ super __init__ linear = torch nn Linear bias=True device=device layer_norm = torch nn LayerNorm device=device layer_norm = torch nn LayerNorm device=device forward x mask masked_select = x masked_select mask view = masked_select view - linear = linear view layer_norm = layer_norm view layer_norm = layer_norm linear linear layer_norm layer_norm model = MyModel inputs = torch randn dtype=torch float device=device torch randint low= high= size= dtype=torch bool device=device actual = torch compile model fullgraph=True inputs expected = model inputs torch testing assert_close actual expected skipGPUIf HAS_GPU torch compile gpu requires triton dynamo_config patch capture_scalar_outputs True test_to_int_with_unbacked_size device fn x unbacked = x item Transpose avoid contig short-circuit unbacked_size = torch ones size= unbacked device=device transpose unbacked_size int example_inputs = torch tensor device=device actual = torch compile fn fullgraph=True example_inputs expected = fn example_inputs torch testing assert_close actual expected skipGPUIf HAS_GPU requires gpu triton dynamo_config patch capture_dynamic_output_shape_ops True inductor_config patch combo_kernels True benchmark_combo_kernel True test_combo_kernel_size_hint_failure device A size hint failure TypeError Cannot convert symbols int device == cpu raise unittest SkipTest Combo kernels must GPU fn x nz = torch nonzero x u = nz size t = torch ones u device=device t = torch zeros u + device=device t = torch zeros u device=device t = torch zeros u - x size device=device out = t - out = t + out = t out = t out out out out example_inputs = torch randn device=device dtype=torch float torch _dynamo mark_dynamic example_inputs actual = torch compile fn fullgraph=True example_inputs expected = fn example_inputs torch testing assert_close actual expected skipGPUIf HAS_GPU requires gpu triton dynamo_config patch capture_dynamic_output_shape_ops True inductor_config patch benchmark_kernel True test_triton_kernel_with_unbacked_symint_fallback device The benchmark_kernel=True config exercises codegen_kernel_benchmark code path Test isinstance arg_sig SizeArg == True fallback path fn x Create unbacked SymInt nz = torch nonzero x u = nz size Create indices index_select operation indices = torch tensor u - device=device Create SizeArg object x = torch index_select x indices x example_inputs = torch randn device=device dtype=torch float torch _dynamo mark_dynamic example_inputs actual = torch compile fn fullgraph=True example_inputs expected = fn example_inputs torch testing assert_close actual expected skipGPUIf HAS_GPU requires gpu triton inductor_config patch max_autotune True dynamo_config patch capture_scalar_outputs True test_autotune_with_unbacked_stride device fn x y u = item torch _check u = unbacked = x expand u x shape clone unbacked = torch permute unbacked y = y expand y shape bmm = torch ops aten bmm unbacked y bmm example_inputs = torch randn dtype=torch bfloat device=device torch randn dtype=torch bfloat device=device torch tensor device=device actual = torch compile fn fullgraph=True example_inputs expected = fn example_inputs torch testing assert_close actual expected instantiate_device_type_tests TestUnbackedSymints globals allow_xpu=True __name__ == __main__ torch _inductor test_case run_tests run_tests