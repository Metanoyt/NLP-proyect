mypy ignore-errors Copyright c Facebook Inc its affiliates All rights reserved This source code licensed under BSD-style license found LICENSE file root directory source tree contextlib collections abc Callable functools partial wraps typing Any Optional Union torch torch autograd forward_ad fwAD torch _C _functorch _assert_wrapped_functional _func_decrement_nesting _func_increment_nesting _grad_decrement_nesting _grad_increment_nesting _jvp_decrement_nesting _jvp_increment_nesting _propagate_functional_input_mutation _unwrap_for_grad _unwrap_functional_tensor _wrap_for_grad _wrap_functional_tensor get_inplace_requires_grad_allowed get_unwrapped is_functorch_wrapped_tensor set_inplace_requires_grad_allowed torch _functorch utils argnums_t exposed_in torch _subclasses functional_tensor FunctionalTensor torch fx experimental const_fold torch fx experimental proxy_tensor make_fx torch utils _pytree pytree torch utils _pytree tree_flatten tree_map tree_map_ tree_map_only tree_unflatten treespec_pprint apis vmap vmap doesnt_support_saved_tensors_hooks get_chunk_sizes lazy_dynamo_disallow func torch _dynamo torch _dynamo disallow_in_graph func contextlib contextmanager enable_inplace_requires_grad enabled prev_state = get_inplace_requires_grad_allowed set_inplace_requires_grad_allowed enabled try yield finally set_inplace_requires_grad_allowed prev_state _set_tensor_requires_grad x avoid graph-break x requires_grad_ https github com pytorch pytorch pull x requires_grad_ _create_differentiable inps level=None create_differentiable x isinstance x torch Tensor enable_inplace_requires_grad True _set_tensor_requires_grad x raise ValueError f Thing passed transform API must Tensor got type x tree_map create_differentiable inps _undo_create_differentiable inps level=None unwrap_tensors x isinstance x torch Tensor _unwrap_for_grad x level TODO Remove following hack namedtuples isinstance x tuple tree_map unwrap_tensors tuple x raise RuntimeError f Expected tensors got unsupported type type x tree_map unwrap_tensors inps _is_differentiable maybe_tensor isinstance maybe_tensor torch Tensor False maybe_tensor requires_grad _any_differentiable tensor_or_tuple_of_tensors flat_args _ = tree_unflatten tensor_or_tuple_of_tensors any tuple map _is_differentiable flat_args _wrap_tensor_for_grad maybe_tensor level isinstance maybe_tensor torch Tensor maybe_tensor _wrap_for_grad maybe_tensor level _wrap_all_tensors tensor_pytree level tree_map partial _wrap_tensor_for_grad level=level tensor_pytree _as_tuple val isinstance val tuple val val Version autograd grad handles outputs don t depend inputs _autograd_grad outputs inputs grad_outputs=None retain_graph=False create_graph=True grad_outputs None diff_outputs = tuple out out outputs out requires_grad result = tuple out go out go zip outputs grad_outputs out requires_grad len result == diff_outputs grad_outputs = diff_outputs grad_outputs = zip result len diff_outputs == tuple torch zeros_like inp inp inputs torch _dynamo compiled_autograd _disable grad_inputs = torch autograd grad diff_outputs inputs grad_outputs retain_graph=retain_graph create_graph=create_graph allow_unused=True grad_inputs = tuple torch zeros_like inp gi None gi gi inp zip grad_inputs inputs grad_inputs NOTE grad vjp interaction no_grad f x torch no_grad c = x x - c The thing consider enable_grad off before grad gets called Case enable_grad grad f x In case ` grad ` should respect inner torch no_grad Case enable_grad off torch no_grad grad f x In case ` grad ` should respect inner torch no_grad outer one This because ` grad ` function transform its result should depend result context manager outside ` f ` This gives us following desired behavior - nested grad transforms must obey torch no_grad inside them - nested grad transforms should obey torch no_grad outside them To achieve behavior upon entering grad vjp - we save current previous is_grad_enabled - we unconditionally enable grad Inside DynamicLayerBackFallback when we re temporarily popping ` grad ` layer off stack - grad_mode disabled then we do nothing there torch no_grad active all subsequent grad transforms must obey - grad_mode enabled previous is_grad_enabled False then we temporarily restore previous ` is_grad_enabled ` This because we re crossing boundary ` grad ` outside no_grad ` grad ` inside no_grad NB vjp has some interesting behavior because vjp s callable can called under different grad_mode than forward computation NB forward-mode AD forward-mode AD doesn t respect torch no_grad respects c AutoFwGradMode We ve implemented same logic our jvp transform will have special handling FwGradMode disabled How do we increment decrement nesting I don t think we can exposed_in torch func vjp func Callable primals has_aux bool = False Standing vector-Jacobian product returns tuple containing results ` ` func ` ` applied ` ` primals ` ` function when given ` ` cotangents ` ` computes reverse-mode Jacobian ` ` func ` ` respect ` ` primals ` ` times ` ` cotangents ` ` Args func Callable A Python function takes one more arguments Must one more Tensors primals Tensors Positional arguments ` ` func ` ` must all Tensors The returned function will also computing derivative respect these arguments has_aux bool Flag indicating ` ` func ` ` returns ` ` output aux ` ` tuple where first element output function differentiated second element other auxiliary objects will differentiated Default False Returns Returns ` ` output vjp_fn ` ` tuple containing output ` ` func ` ` applied ` ` primals ` ` function computes vjp ` ` func ` ` respect all ` ` primals ` ` using cotangents passed returned function If ` ` has_aux True ` ` then instead returns ` ` output vjp_fn aux ` ` tuple The returned ` ` vjp_fn ` ` function will tuple each VJP When used simple cases func ` vjp ` behaves same func ` grad ` x = torch randn f = lambda x x sin sum _ vjpfunc = torch func vjp f x grad = vjpfunc torch tensor assert torch allclose grad torch func grad f x However func ` vjp ` can support functions multiple outputs passing cotangents each outputs x = torch randn f = lambda x x sin x cos _ vjpfunc = torch func vjp f x vjps = vjpfunc torch ones torch ones assert torch allclose vjps x cos + -x sin func ` vjp ` can even support outputs being Python structs x = torch randn f = lambda x first x sin second x cos _ vjpfunc = torch func vjp f x cotangents = first torch ones second torch ones vjps = vjpfunc cotangents assert torch allclose vjps x cos + -x sin The function returned func ` vjp ` will compute partials respect each ` ` primals ` ` x y = torch randn torch randn _ vjpfunc = torch func vjp torch matmul x y cotangents = torch randn vjps = vjpfunc cotangents assert len vjps == assert torch allclose vjps torch matmul cotangents y transpose assert torch allclose vjps torch matmul x transpose cotangents ` ` primals ` ` positional arguments ` ` f ` ` All kwargs use their default value x = torch randn f x scale= x scale _ vjpfunc = torch func vjp f x vjps = vjpfunc torch ones_like x assert torch allclose vjps torch full x shape note Using PyTorch ` ` torch no_grad ` ` together ` ` vjp ` ` Case Using ` ` torch no_grad ` ` inside function f x torch no_grad c = x x - c In case ` ` vjp f x ` ` will respect inner ` ` torch no_grad ` ` Case Using ` ` vjp ` ` inside ` ` torch no_grad ` ` context manager xdoctest +SKIP failing torch no_grad vjp f x In case ` ` vjp ` ` will respect inner ` ` torch no_grad ` ` outer one This because ` ` vjp ` ` function transform its result should depend result context manager outside ` ` f ` ` _vjp_with_argnums func primals has_aux=has_aux contextlib contextmanager grad_increment_nesting try grad_level = _grad_increment_nesting yield grad_level finally _grad_decrement_nesting enter_jvp_nesting global JVP_NESTING jvp_level = _jvp_increment_nesting JVP_NESTING += jvp_level exit_jvp_nesting global JVP_NESTING _jvp_decrement_nesting JVP_NESTING -= contextlib contextmanager jvp_increment_nesting try yield enter_jvp_nesting finally exit_jvp_nesting doesnt_support_saved_tensors_hooks _vjp_with_argnums func Callable primals argnums Optional argnums_t = None has_aux bool = False This same function vjp also accepts argnums argument All args same vjp except added argument argnums Optional int tuple int Optional specifies argument s compute gradients respect If None computes gradients respect all inputs used vjp Default None WARN Users should NOT call function directly should just calling vjp It only separated so inputs passed jacrev differentiated get correct wrappers NOTE All error messages produced vjp being called even called jacrev Returns same two elements func ` vjp ` function returned vjp_fn returns tuple VJPs only primal elements given argnums grad_increment_nesting level See NOTE grad vjp interaction no_grad torch enable_grad primals = _wrap_all_tensors primals level argnums None diff_primals = _create_differentiable primals level diff_primals = _slice_argnums primals argnums as_tuple=False tree_map_ partial _create_differentiable level=level diff_primals primals_out = func primals has_aux isinstance primals_out tuple len primals_out == raise RuntimeError vjp f primals output function f should tuple output aux has_aux True primals_out aux = primals_out aux = _undo_create_differentiable aux level flat_primals_out primals_out_spec = tree_flatten primals_out assert_non_empty_tensor_output flat_primals_out vjp f primals flat_diff_primals primals_spec = tree_flatten diff_primals results = _undo_create_differentiable primals_out level primal_out flat_primals_out assert isinstance primal_out torch Tensor primal_out is_floating_point primal_out is_complex continue raise RuntimeError vjp f All outputs f must floating-point complex Tensors got Tensor f dtype primal_out dtype wrapper cotangents retain_graph=True create_graph=None create_graph None create_graph = torch is_grad_enabled flat_cotangents cotangents_spec = tree_flatten cotangents primals_out_spec = cotangents_spec raise RuntimeError f Expected pytree structure cotangents same f pytree structure outputs function f cotangents treespec_pprint cotangents_spec f primal output treespec_pprint primals_out_spec result = _autograd_grad flat_primals_out flat_diff_primals flat_cotangents retain_graph=retain_graph create_graph=create_graph tree_unflatten result primals_spec has_aux results wrapper aux results wrapper _safe_zero_index x assert len x == x jacrev jacfwd don t support complex functions Helper function throw appropriate error error_if_complex func_name args is_input flat_args = pytree tree_leaves args idx arg enumerate flat_args isinstance arg torch Tensor arg dtype is_complex input_or_output = inputs is_input outputs err_msg = f func_name Expected all input_or_output f real received complex tensor flattened input idx idx raise RuntimeError err_msg exposed_in torch func jacrev func Callable argnums Union int tuple int = has_aux=False chunk_size Optional int = None _preallocate_and_copy=False Computes Jacobian ` ` func ` ` respect arg s index ` ` argnum ` ` using reverse mode autodiff note Using attr ` chunk_size= ` equivalent computing jacobian row-by-row for-loop i e constraints func ` vmap ` applicable Args func function A Python function takes one more arguments one which must Tensor returns one more Tensors argnums int Tuple int Optional integer tuple integers saying which arguments get Jacobian respect Default has_aux bool Flag indicating ` ` func ` ` returns ` ` output aux ` ` tuple where first element output function differentiated second element auxiliary objects will differentiated Default False chunk_size None int If None default use maximum chunk size equivalent doing single vmap over vjp compute jacobian If then compute jacobian row-by-row for-loop If None then compute jacobian attr ` chunk_size ` rows time equivalent doing multiple vmap over vjp If you run into memory issues computing jacobian please try specify non-None chunk_size Returns Returns function takes same inputs ` ` func ` ` returns Jacobian ` ` func ` ` respect arg s ` ` argnums ` ` If ` ` has_aux True ` ` then returned function instead returns ` ` jacobian aux ` ` tuple where ` ` jacobian ` ` Jacobian ` ` aux ` ` auxiliary objects returned ` ` func ` ` A basic usage pointwise unary operation will give diagonal array Jacobian torch func jacrev x = torch randn jacobian = jacrev torch sin x expected = torch diag torch cos x assert torch allclose jacobian expected If you would like compute output function well jacobian function use ` ` has_aux ` ` flag output auxiliary object torch func jacrev x = torch randn f x x sin g x result = f x result result jacobian_f f_x = jacrev g has_aux=True x assert torch allclose f_x f x func ` jacrev ` can composed vmap produce batched Jacobians torch func jacrev vmap x = torch randn jacobian = vmap jacrev torch sin x assert jacobian shape == Additionally func ` jacrev ` can composed itself produce Hessians torch func jacrev f x x sin sum x = torch randn hessian = jacrev jacrev f x assert torch allclose hessian torch diag -x sin By default func ` jacrev ` computes Jacobian respect first input However can compute Jacboian respect different argument using ` ` argnums ` ` torch func jacrev f x y x + y x y = torch randn torch randn jacobian = jacrev f argnums= x y expected = torch diag y assert torch allclose jacobian expected Additionally passing tuple ` ` argnums ` ` will compute Jacobian respect multiple arguments torch func jacrev f x y x + y x y = torch randn torch randn jacobian = jacrev f argnums= x y expectedX = torch diag torch ones_like x expectedY = torch diag y assert torch allclose jacobian expectedX assert torch allclose jacobian expectedY note Using PyTorch ` ` torch no_grad ` ` together ` ` jacrev ` ` Case Using ` ` torch no_grad ` ` inside function f x torch no_grad c = x x - c In case ` ` jacrev f x ` ` will respect inner ` ` torch no_grad ` ` Case Using ` ` jacrev ` ` inside ` ` torch no_grad ` ` context manager torch no_grad jacrev f x In case ` ` jacrev ` ` will respect inner ` ` torch no_grad ` ` outer one This because ` ` jacrev ` ` function transform its result should depend result context manager outside ` ` f ` ` chunk_size None chunk_size raise ValueError jacrev ` chunk_size ` should greater than wraps func wrapper_fn args error_if_complex jacrev args is_input=True vjp_out = _vjp_with_argnums func args argnums=argnums has_aux=has_aux has_aux output vjp_fn aux = vjp_out output vjp_fn = vjp_out See NOTE Computing jacobian vmap vjp multiple outputs flat_output output_spec = tree_flatten output error_if_complex jacrev flat_output is_input=False NB vjp already checks all outputs tensors Step Construct grad_outputs splitting standard basis flat_output_numels = tuple out numel out flat_output primals = _slice_argnums args argnums flat_primals primals_spec = tree_flatten primals compute_jacobian_stacked Helper function compute chunked Jacobian The intermediate chunked calculation only scoped function level chunked_results = flat_basis_chunk _chunked_standard_basis_for_ flat_output flat_output_numels chunk_size=chunk_size chunk_size == sanity check t flat_basis_chunk assert t size == flat_basis_chunk = tree_map lambda t torch squeeze t flat_basis_chunk basis = tree_unflatten flat_basis_chunk output_spec chunk_size == Behaviour ` chunk_size= ` same ` for-loop ` i e user shouldn t deal limitations vmap chunked_result = vjp_fn basis chunk_size None chunk_size = chunked_result = vmap vjp_fn basis flat_results = pytree tree_leaves chunked_result chunk_size == flat_results = tree_map lambda t torch unsqueeze t flat_results chunked_results append flat_results len chunked_results == Short-circuit we used single chunk chunked_results Concatenate chunks flat_results = Iterate concat jacobians different inputs idx range len flat_primals r = tuple r_ idx r_ chunked_results flat_results append torch cat r flat_results compute_jacobian_preallocate_and_copy Helper function compute chunked Jacobian The intermediate chunked calculation only scoped function level out_vec_size = sum flat_output_numels Don t pre-allocate we have single chunk chunk_size None chunk_size = out_vec_size stacked_results = primal new_zeros out_vec_size primal shape primal flat_primals idx flat_basis_chunk enumerate _chunked_standard_basis_for_ flat_output flat_output_numels chunk_size=chunk_size chunk_size == sanity check t flat_basis_chunk assert t size == flat_basis_chunk = torch squeeze t t flat_basis_chunk basis = tree_unflatten flat_basis_chunk output_spec chunk_size == Behaviour ` chunk_size= ` same ` for-loop ` i e user shouldn t deal limitations vmap chunked_result = vjp_fn basis chunk_size None chunk_size = chunked_result = vmap vjp_fn basis flat_results = pytree tree_leaves chunked_result Short-circuit we have single chunk chunk_size None chunk_size = out_vec_size chunk_size == out_vec_size == Since we squeezed output dim flat_results = tree_map lambda t torch unsqueeze t flat_results flat_results r sr zip flat_results stacked_results sr idx chunk_size idx + chunk_size copy_ r stacked_results _preallocate_and_copy flat_jacobians_per_input = compute_jacobian_preallocate_and_copy flat_jacobians_per_input = compute_jacobian_stacked Step The returned jacobian one big tensor per input In step we split each Tensor output flat_jacobians_per_input = result split flat_output_numels dim= result flat_jacobians_per_input flat_input_flat_output = tuple split view out shape + primal shape split out zip splits flat_output splits primal zip flat_jacobians_per_input flat_primals Step Right now ` jacobian ` List List Tensor The outer List corresponds number primals inner List corresponds number outputs We need Exchange order outer List inner List b tree_unflatten inner Lists which correspond primals c handle argnums=int case d tree_unflatten outer List which corresponds outputs flat_output_flat_input = tuple zip flat_input_flat_output flat_output_input = tuple tree_unflatten flat_input primals_spec flat_input flat_output_flat_input isinstance argnums int flat_output_input = tuple _safe_zero_index flat_input flat_input flat_output_input output_input = tree_unflatten flat_output_input output_spec has_aux output_input aux output_input wrapper_fn NOTE Computing jacobian vmap vjp multiple outputs Let s consider f x = x x sum let x = torch randn It turns out we can compute jacobian function single call autograd grad using vmap over correct grad_outputs Firstly one way compute jacobian stack x x sum into D vector E g use g x = torch stack x x sum To get first row jacobian we call autograd grad g x x grad_outputs=torch tensor To get nd row jacobian we call autograd grad g x x grad_outputs=torch tensor so Using vmap we can vectorize all these computations into one passing standard basis R^ grad_output vmap partial autograd grad g x x torch eye Now how do we compute jacobian without stacking output We can just split standard basis across outputs So compute jacobian f x we d use autograd grad f x x grad_outputs=_construct_standard_basis_for The grad_outputs looks like following torch tensor torch tensor But we re done yet vmap partial autograd grad f x x grad_outputs= returns Tensor shape We have remember split jacobian shape into two - one shape first output - one shape second output _chunked_standard_basis_for_ tensors tensor_numels chunk_size=None This function - constructs N=sum tensor_numels standard basis i e NxN identity matrix - Splits identity matrix into chunks each chunk size determined ` tensor_numels ` - Each chunk corresponds one tensor The chunk has same dtype device tensor For example tensor_numels = function returns tensor tensor tensor Precondition tensor_numels == tuple tensor numel tensor tensors Precondition tensors always has least one element See NOTE Computing jacobian vmap grad multiple tensors context behind function NOTE Argument ` chunk_size ` used generate chunked basis instead one huge basis matrix ` chunk_size ` dictates maximum size basis matrix along dim= assert len tensors == len tensor_numels assert len tensors assert chunk_size None chunk_size total_numel = sum tensor_numels chunk_size chunk_size total_numel chunk_numels = get_chunk_sizes total_numel chunk_size chunk_size None chunk_size = total_numel chunk_size = total_numel chunk_numels = total_numel diag_start_indices = torch tensor tensor_numels cumsum dim= - neg unbind chunk_idx total_numel enumerate chunk_numels chunks = tuple tensor new_zeros total_numel tensor_numel tensor tensor_numel zip tensors tensor_numels chunk diag_start_idx zip chunks diag_start_indices chunk diagonal diag_start_idx + chunk_idx chunk_size fill_ chunks = tuple chunk view total_numel tensor shape chunk tensor zip chunks tensors yield chunks _construct_standard_basis_for tensors tensor_numels basis _chunked_standard_basis_for_ tensors tensor_numels chunk_size=None basis _validate_and_wrap_argnum argnum num_args isinstance argnum int raise RuntimeError f argnum must int got type argnum argnum = argnum num_args argnum argnum argnum = -num_args argnum + num_args raise RuntimeError f Got argnum= argnum only num_args positional inputs _check_unique_non_empty argnums isinstance argnums tuple len argnums == raise RuntimeError argnums must non-empty len set argnums = len argnums raise RuntimeError f argnums elements must unique got argnums _replace_args old_args new_args argnums isinstance argnums int len new_args = raise RuntimeError f new_args should size size len new_args tuple new_args i == argnums old_args i i range len old_args isinstance argnums tuple len new_args = len argnums raise RuntimeError new_args should have same size argnums f Argnums size len argnums new_args size len new_args get_right_elem i new_args argnums index i i argnums old_args i tuple get_right_elem i i range len old_args raise RuntimeError f argnums must int Tuple int got type argnums _validate_and_wrap_argnums argnums num_args isinstance argnums int _validate_and_wrap_argnum argnums num_args isinstance argnums tuple tuple _validate_and_wrap_argnum argnum num_args argnum argnums raise AssertionError Should never get here _slice_argnums args argnums as_tuple=True isinstance argnums int isinstance argnums tuple raise RuntimeError f argnums must int Tuple int got type argnums argnums = _validate_and_wrap_argnums argnums len args _check_unique_non_empty argnums isinstance argnums int as_tuple args argnums args argnums tuple args i i argnums JVP_NESTING = assert_flat_tuple_of_tensors elts Any api str argname str - None isinstance elts tuple raise RuntimeError f api Expected argname tuple Tensors got type elts elt elts isinstance elt torch Tensor continue raise RuntimeError f api Expected argname tuple Tensors got f tuple element type type elt len elts == raise RuntimeError f api Expected argname non-empty tuple Tensors assert_non_empty_tensor_output output list Any api str - None len output == output None len output raise RuntimeError f api Expected f function has non-empty output got output = output o output isinstance o torch Tensor raise RuntimeError f api expected f primals only tensors f got unsupported type type o assert_output_is_tensor_or_tensors output Any api str - None isinstance output torch Tensor isinstance output tuple raise RuntimeError f api Expected output f Tensor Tensors got type output len output == raise RuntimeError f api Expected output f non-empty tuple Tensors out output isinstance out torch Tensor continue raise RuntimeError f api Expected output f Tensor Tensors got f type out output assert_non_empty_list_of_tensors output list torch Tensor api str argname str - None len output == raise RuntimeError f api Expected argname contain least one Tensor out output isinstance out torch Tensor continue raise RuntimeError f api Expected argname only contain Tensors got type out jvp_str = jvp f primals tangents safe_unpack_dual dual strict isinstance dual torch Tensor raise RuntimeError f jvp_str expected f args only tensors f got unsupported type type dual primal tangent = fwAD unpack_dual dual tangent None strict raise RuntimeError jvp f primals tangents strict=True The output f independent inputs This allowed strict=True tangent = torch zeros_like primal primal tangent exposed_in torch func jvp func Callable primals Any tangents Any strict bool = False has_aux bool = False Standing Jacobian-vector product returns tuple containing output ` func primals ` Jacobian ` ` func ` ` evaluated ` ` primals ` ` times ` ` tangents ` ` This also known forward-mode autodiff Args func function A Python function takes one more arguments one which must Tensor returns one more Tensors primals Tensors Positional arguments ` ` func ` ` must all Tensors The returned function will also computing derivative respect these arguments tangents Tensors The vector which Jacobian-vector-product computed Must same structure sizes inputs ` ` func ` ` has_aux bool Flag indicating ` ` func ` ` returns ` ` output aux ` ` tuple where first element output function differentiated second element other auxiliary objects will differentiated Default False Returns Returns ` ` output jvp_out ` ` tuple containing output ` ` func ` ` evaluated ` ` primals ` ` Jacobian-vector product If ` ` has_aux True ` ` then instead returns ` ` output jvp_out aux ` ` tuple note You may see API error out forward-mode AD implemented operator X If so please file bug report we will prioritize jvp useful when you wish compute gradients function R^ - R^N torch func jvp x = torch randn f = lambda x x torch tensor value grad = jvp f x torch tensor assert torch allclose value f x assert torch allclose grad torch tensor func ` jvp ` can support functions multiple inputs passing tangents each inputs torch func jvp x = torch randn y = torch randn f = lambda x y x y _ output = jvp f x y torch ones torch ones assert torch allclose output x + y _jvp_with_argnums func primals tangents argnums=None strict=strict has_aux=has_aux _jvp_with_argnums func Callable primals Any tangents Any argnums Optional argnums_t strict bool = False has_aux bool This same function jvp also accepts argnums argument Most args same jvp except added argument argnums Optional int tuple int Optional specifies argument s compute gradients respect If None computes gradients respect all inputs used jvp Default None Because tangents must length argnums matches up corresponding primal whose index given argnums WARN Users should NOT call function directly should just calling jvp It only separated so inputs passed jacfwd differentiated get correct wrappers NOTE All error messages produced jvp being called even called jacfwd Returns same two elements func ` jvp ` returned tuple ` ` jvp_out ` ` only has JVPs respect primals given argnums isinstance primals tuple raise RuntimeError f jvp_str Expected primals tuple f E g should valid call f primals diff_args = primals argnums None _slice_argnums primals argnums flat_primals primals_spec = tree_flatten diff_args flat_tangents tangents_spec = tree_flatten tangents primals_spec = tangents_spec raise RuntimeError f jvp_str Expected primals tangents have same python f structure For example primals tuple tensors f tangents also must Got primals structure primals_spec f tangents structure tangents_spec assert_non_empty_list_of_tensors flat_primals jvp_str primals assert_non_empty_list_of_tensors flat_tangents jvp_str tangents global JVP_NESTING jvp_increment_nesting level fwAD _set_fwd_grad_enabled True ctx = fwAD dual_level JVP_NESTING == contextlib nullcontext ctx flat_duals = tuple fwAD make_dual p t p t zip flat_primals flat_tangents duals = tree_unflatten flat_duals primals_spec argnums None primals = _wrap_all_tensors primals level duals = _replace_args primals duals argnums result_duals = func duals has_aux isinstance result_duals tuple len result_duals == raise RuntimeError f jvp_str output function f should tuple output aux has_aux True result_duals aux = result_duals aux = _undo_create_differentiable aux level result_duals spec = tree_flatten result_duals assert_non_empty_tensor_output result_duals jvp_str primals_out tangents_out = zip safe_unpack_dual dual strict dual result_duals primals_out = tree_map partial _undo_create_differentiable level=level primals_out tangents_out = tree_map partial _undo_create_differentiable level=level tangents_out primals_out_unflatten = tree_unflatten primals_out spec tangents_out_unflatten = tree_unflatten tangents_out spec has_aux primals_out_unflatten tangents_out_unflatten aux primals_out_unflatten tangents_out_unflatten safe_unflatten tensor dim shape len shape == assert tensor shape dim == tensor squeeze dim tensor unflatten dim shape exposed_in torch func jacfwd func Callable argnums argnums_t = has_aux bool = False randomness str = error Computes Jacobian ` ` func ` ` respect arg s index ` ` argnum ` ` using forward-mode autodiff Args func function A Python function takes one more arguments one which must Tensor returns one more Tensors argnums int Tuple int Optional integer tuple integers saying which arguments get Jacobian respect Default has_aux bool Flag indicating ` ` func ` ` returns ` ` output aux ` ` tuple where first element output function differentiated second element auxiliary objects will differentiated Default False randomness str Flag indicating what type randomness use See func ` vmap ` more detail Allowed different same error Default error Returns Returns function takes same inputs ` ` func ` ` returns Jacobian ` ` func ` ` respect arg s ` ` argnums ` ` If ` ` has_aux True ` ` then returned function instead returns ` ` jacobian aux ` ` tuple where ` ` jacobian ` ` Jacobian ` ` aux ` ` auxiliary objects returned ` ` func ` ` note You may see API error out forward-mode AD implemented operator X If so please file bug report we will prioritize An alternative use func ` jacrev ` which has better operator coverage A basic usage pointwise unary operation will give diagonal array Jacobian torch func jacfwd x = torch randn jacobian = jacfwd torch sin x expected = torch diag torch cos x assert torch allclose jacobian expected func ` jacfwd ` can composed vmap produce batched Jacobians torch func jacfwd vmap x = torch randn jacobian = vmap jacfwd torch sin x assert jacobian shape == If you would like compute output function well jacobian function use ` ` has_aux ` ` flag output auxiliary object torch func jacfwd x = torch randn f x x sin g x result = f x result result jacobian_f f_x = jacfwd g has_aux=True x assert torch allclose f_x f x Additionally func ` jacrev ` can composed itself func ` jacrev ` produce Hessians torch func jacfwd jacrev f x x sin sum x = torch randn hessian = jacfwd jacrev f x assert torch allclose hessian torch diag -x sin By default func ` jacfwd ` computes Jacobian respect first input However can compute Jacboian respect different argument using ` ` argnums ` ` torch func jacfwd f x y x + y x y = torch randn torch randn jacobian = jacfwd f argnums= x y expected = torch diag y assert torch allclose jacobian expected Additionally passing tuple ` ` argnums ` ` will compute Jacobian respect multiple arguments torch func jacfwd f x y x + y x y = torch randn torch randn jacobian = jacfwd f argnums= x y expectedX = torch diag torch ones_like x expectedY = torch diag y assert torch allclose jacobian expectedX assert torch allclose jacobian expectedY wraps func wrapper_fn args error_if_complex jacfwd args is_input=True primals = args argnums None _slice_argnums args argnums flat_primals primals_spec = tree_flatten primals flat_primals_numels = tuple p numel p flat_primals flat_basis = _construct_standard_basis_for flat_primals flat_primals_numels basis = tree_unflatten flat_basis primals_spec push_jvp basis output = _jvp_with_argnums func args basis argnums=argnums has_aux=has_aux output output ` func args ` error_if_complex jacfwd output is_input=False has_aux _ jvp_out aux = output jvp_out aux _ jvp_out = output jvp_out results = vmap push_jvp randomness=randomness basis has_aux results aux = results aux standard basis format e g NxN matrix We need fetch first element original ` func ` output flat_aux aux_spec = tree_flatten aux flat_aux = value value flat_aux aux = tree_unflatten flat_aux aux_spec jac_outs spec = tree_flatten results Most probably below output check can never raise error jvp should test output before assert_non_empty_output jac_outs jacfwd f args jac_outs_ins = tuple tuple safe_unflatten jac_out_in - primal shape primal jac_out_in zip flat_primals jac_out movedim - split flat_primals_numels dim=- jac_out jac_outs jac_outs_ins = tuple tree_unflatten jac_ins primals_spec jac_ins jac_outs_ins isinstance argnums int jac_outs_ins = tuple jac_ins jac_ins jac_outs_ins has_aux tree_unflatten jac_outs_ins spec aux tree_unflatten jac_outs_ins spec wrapper_fn exposed_in torch func hessian func argnums= Computes Hessian ` ` func ` ` respect arg s index ` ` argnum ` ` via forward-over-reverse strategy The forward-over-reverse strategy composing ` ` jacfwd jacrev func ` ` good default good performance It possible compute Hessians through other compositions func ` jacfwd ` func ` jacrev ` like ` ` jacfwd jacfwd func ` ` ` ` jacrev jacrev func ` ` Args func function A Python function takes one more arguments one which must Tensor returns one more Tensors argnums int Tuple int Optional integer tuple integers saying which arguments get Hessian respect Default Returns Returns function takes same inputs ` ` func ` ` returns Hessian ` ` func ` ` respect arg s ` ` argnums ` ` note You may see API error out forward-mode AD implemented operator X If so please file bug report we will prioritize An alternative use ` ` jacrev jacrev func ` ` which has better operator coverage A basic usage R^N - R^ function gives N x N Hessian torch func hessian f x x sin sum x = torch randn hess = hessian f x equivalent jacfwd jacrev f x assert torch allclose hess torch diag -x sin jacfwd jacrev func argnums argnums doesnt_support_saved_tensors_hooks grad_and_value_impl func argnums has_aux args kwargs - Callable grad_increment_nesting level output aux grad_input = None None None See NOTE grad vjp interaction no_grad torch enable_grad args = _wrap_all_tensors args level kwargs = _wrap_all_tensors kwargs level diff_args = _slice_argnums args argnums as_tuple=False tree_map_ partial _create_differentiable level=level diff_args output = func args kwargs has_aux isinstance output tuple len output == raise RuntimeError grad_and_value f args output function f should tuple output aux has_aux True output aux = output isinstance output torch Tensor raise RuntimeError grad_and_value f args Expected f args f Tensor got type output output dim = raise RuntimeError grad_and_value f args Expected f args scalar Tensor got tensor f output dim dims Maybe you wanted use vjp jacrev APIs instead flat_diff_args spec = tree_flatten diff_args NB need create_graph so backward pass isn t run no_grad mode flat_outputs = _as_tuple output flat_grad_input = _autograd_grad flat_outputs flat_diff_args create_graph=True grad_input = tree_unflatten flat_grad_input spec grad_input = _undo_create_differentiable grad_input level output = _undo_create_differentiable output level has_aux aux = _undo_create_differentiable aux level has_aux grad_input output aux grad_input output grad_impl func Callable argnums argnums_t has_aux bool args kwargs results = grad_and_value_impl func argnums has_aux args kwargs has_aux grad _ aux = results grad aux grad _ = results grad _maybe_wrap_functional_tensor maybe_tensor level _python_functionalize bool = False isinstance maybe_tensor torch Tensor maybe_tensor wrapped = _wrap_functional_tensor maybe_tensor level _assert_wrapped_functional maybe_tensor wrapped _python_functionalize out = FunctionalTensor wrapped torch _mirror_autograd_meta_to maybe_tensor out out wrapped _wrap_all_tensors_to_functional tensor_pytree level _python_functionalize bool = False tree_map partial lambda x _maybe_wrap_functional_tensor x level _python_functionalize=_python_functionalize tensor_pytree _maybe_unwrap_functional_tensor maybe_tensor reapply_views bool isinstance maybe_tensor torch Tensor maybe_tensor isinstance maybe_tensor FunctionalTensor maybe_tensor = maybe_tensor elem torch _is_functional_tensor maybe_tensor If s functional tensor just This can happen we functionalize fn returns global which never wrapped properly maybe_tensor Sync any pending updates output tensor torch _sync maybe_tensor _unwrap_functional_tensor maybe_tensor reapply_views _unwrap_all_tensors_from_functional tensor_pytree reapply_views bool tree_map lambda t _maybe_unwrap_functional_tensor t reapply_views=reapply_views tensor_pytree exposed_in torch func functionalize func Callable remove str = mutations - Callable functionalize transform can used remove intermediate mutations aliasing function while preserving function s semantics ` ` functionalize func ` ` returns new function same semantics ` ` func ` ` all intermediate mutations removed Every inplace operation performed intermediate tensor ` ` intermediate foo_ ` ` gets replaced its out-of-place equivalent ` ` intermediate_updated = intermediate foo ` ` functionalize useful shipping pytorch program off backends compilers aren t able easily represent mutations aliasing operators Args func Callable A Python function takes one more arguments remove str An optional string argument takes either value mutations mutations_and_views If mutations passed then all mutating operators will replaced their non-mutating equivalents If mutations_and_views passed then additionally all aliasing operators will replaced their non-aliasing equivalents Default mutations Returns Returns new functionalized function It takes same inputs ` ` func ` ` has same behavior any mutations optionally aliasing performed intermediate tensors function will removed functionalize will also remove mutations views performed function inputs However preserve semantics functionalize will fix up mutations after transform has finished running detecting any tensor inputs should have been mutated copying new data back inputs necessary Example xdoctest +SKIP torch torch fx experimental proxy_tensor make_fx torch func functionalize A function uses mutations views only intermediate tensors f b = + c = b view - c add_ b inpt = torch randn out = f inpt out = functionalize f inpt semantics same outputs equivalent print torch allclose out out True f_traced = make_fx f inpt f_no_mutations_traced = make_fx functionalize f inpt f_no_mutations_and_views_traced = make_fx functionalize f remove= mutations_and_views inpt print f_traced code forward a_ add = torch ops aten add a_ a_ = None view = torch ops aten view add - add_ = torch ops aten add_ view view = None add print f_no_mutations_traced code forward a_ add = torch ops aten add a_ a_ = None view = torch ops aten view add - add = None add_ = torch ops aten add view view = None view_ = torch ops aten view add_ add_ = None view_ print f_no_mutations_and_views_traced code forward a_ add = torch ops aten add a_ a_ = None view_copy = torch ops aten view_copy add - add = None add_ = torch ops aten add view_copy view_copy = None view_copy_ = torch ops aten view_copy add_ add_ = None view_copy_ A function mutates its input tensor f b = view - b add_ f_no_mutations_and_views_traced = make_fx functionalize f remove= mutations_and_views inpt All mutations views have been removed there extra copy_ graph correctly apply mutation input after function has completed print f_no_mutations_and_views_traced code forward a_ view_copy = torch ops aten view_copy a_ - add = torch ops aten add view_copy view_copy = None view_copy_ = torch ops aten view_copy add add = None copy_ = torch ops aten copy_ a_ view_copy_ a_ = None view_copy_ There few failure modes functionalize worth calling out Like other torch func transforms ` functionalize ` doesn t work functions directly use ` backward ` The same true torch autograd grad If you want use autograd you can compute gradients directly ` functionalize grad f ` Like other torch func transforms ` functionalize ` doesn t work global state If you call ` functionalize f ` function takes views mutations non-local state functionalization will simply no-op pass view mutation calls directly backend One way work around ensure any non-local state creation wrapped into larger function which you then call functionalize ` resize_ ` has some limitations functionalize will only work programs use resize_ ` long tensor being resized view ` as_strided ` has some limitations functionalize will work ` as_strided ` calls result tensors overlapping memory Finally helpful mental model understanding functionalization most user pytorch programs writing public torch API When executed torch operators generally decomposed into our internal C++ ATen API The logic functionalization happens entirely level ATen Functionalization knows how take every aliasing operator ATen map its non-aliasing equivalent e g ` ` tensor view - ` ` - ` ` view_copy tensor - ` ` how take every mutating operator ATen map its non-mutating equivalent e g ` ` tensor add_ ` ` - ` ` add tensor - ` ` while tracking aliases mutations out-of-line know when fix things up Information about which ATen operators aliasing mutating all comes https github com pytorch pytorch blob master aten src ATen native native_functions yaml remove == mutations reapply_views = True remove == mutations_and_views reapply_views = False raise RuntimeError f functionalize f remove= mutations received invalid argument remove= remove Valid options \n remove= mutations all inplace out= operators will removed program replaced their out-of-place equivalents \n remove= mutations_and_views In addition above all aliasing operators view will replaced their non-aliasing counterparts view _copy \n wraps func wrapped args kwargs try func_level = _func_increment_nesting reapply_views func_args = _wrap_all_tensors_to_functional args func_level func_kwargs = _wrap_all_tensors_to_functional kwargs func_level flattened_unwrapped_args = pytree arg_tree_leaves args flattened_wrapped_args = pytree arg_tree_leaves func_args flattened_unwrapped_kwargs = pytree arg_tree_leaves kwargs flattened_wrapped_kwargs = pytree arg_tree_leaves func_kwargs func_outputs = func func_args func_kwargs outputs = _unwrap_all_tensors_from_functional func_outputs reapply_views=reapply_views flattened_wrapped_args + flattened_wrapped_kwargs isinstance torch Tensor Call sync_ inputs ensure any pending mutations have been applied torch _sync And any mutations applied inputs we need propagate them back user unwrapped wrapped zip flattened_unwrapped_args flattened_wrapped_args isinstance unwrapped torch Tensor isinstance wrapped torch Tensor _propagate_functional_input_mutation unwrapped wrapped unwrapped wrapped zip flattened_unwrapped_kwargs flattened_wrapped_kwargs isinstance unwrapped torch Tensor isinstance wrapped torch Tensor _propagate_functional_input_mutation unwrapped wrapped outputs finally _func_decrement_nesting wrapped exposed_in torch func linearize func Callable primals - tuple Any Callable Returns value ` ` func ` ` ` ` primals ` ` linear approximation ` ` primals ` ` Args func Callable A Python function takes one more arguments primals Tensors Positional arguments ` ` func ` ` must all Tensors These values which function linearly approximated Returns Returns ` ` output jvp_fn ` ` tuple containing output ` ` func ` ` applied ` ` primals ` ` function computes jvp ` ` func ` ` evaluated ` ` primals ` ` linearize useful jvp computed multiple times ` ` primals ` ` However achieve linearize saves intermediate computation has higher memory requirements than directly applying ` jvp ` So all ` ` tangents ` ` known maybe more efficient compute vmap jvp instead using linearize note linearize evaluates ` ` func ` ` twice Please file issue implementation single evaluation Example torch torch func linearize fn x x sin output jvp_fn = linearize fn torch zeros jvp_fn torch ones tensor Note We evaluate ` fn ` twice Once returning output other while tracing graph If becomes bottle-neck we should update make_fx such also returns output output = func primals _ output_spec = tree_flatten output flat_primals primals_argspec = tree_flatten primals tangents tracing flat_tangents = tuple p new_empty expand_as p p flat_primals function trace trace_fn flat_tangents fwAD dual_level flat_duals = tuple fwAD make_dual p t p t zip flat_primals flat_tangents duals = tree_unflatten flat_duals primals_argspec output = func duals tangents = tree_map_only torch Tensor lambda dual safe_unpack_dual dual False output tangents jvp_graph = lazy_dynamo_disallow make_fx trace_fn flat_tangents const_folded_jvp_graph = lazy_dynamo_disallow const_fold split_const_subgraphs jvp_graph Hold only meta-data regarding primals flat_primals_shape = tuple p shape p flat_primals flat_primals_device = tuple p device p flat_primals flat_primals_dtype = tuple p dtype p flat_primals forward_ad_checks flat_tangents idx t enumerate flat_tangents t shape = flat_primals_shape idx msg = f tangent idx shape t shape flattened f pytree doesn t match shape flat_primals_shape idx corresponding primal raise RuntimeError msg t device = flat_primals_device idx msg = f tangent idx device t device flattened f pytree doesn t match device flat_primals_device idx corresponding primal raise RuntimeError msg t dtype = flat_primals_dtype idx msg = f tangent idx dtype t dtype flattened f pytree doesn t match dtype flat_primals_dtype idx corresponding primal raise RuntimeError msg jvp_fn callable It takes care checking argspec tangents calling folded fx graph unflattening fx graph output jvp_fn tangents flat_tangents tangent_argspec = tree_flatten tangents tangent_argspec = primals_argspec raise RuntimeError f Expected tangents tangent_argspec have f same argspec primals primals_argspec forward_ad_checks flat_tangents flat_output = const_folded_jvp_graph flat_tangents const folded graph can flat output so transform output tree_unflatten flat_output output_spec output jvp_fn exposed_in torch func debug_unwrap tensor torch Tensor recurse=True - torch Tensor Unwraps functorch tensor e g BatchedTensor GradTrackingTensor its underlying tensor This function should only used debug setting e g trying print value Tensor debugger Otherwise using result function inside function being transformed will lead undefined behavior is_functorch_wrapped_tensor tensor tensor result = get_unwrapped tensor recurse debug_unwrap result result