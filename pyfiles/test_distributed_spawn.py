Owner s oncall distributed os sys torch torch distributed dist torch backends cuda matmul allow_tf = False dist is_available print Distributed available skipping tests file=sys stderr sys exit torch testing _internal common_utils run_tests TEST_WITH_DEV_DBG_ASAN torch testing _internal distributed distributed_test DistributedTest TestDistBackend TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit _allowed_backends = gloo nccl ucc BACKEND os environ WORLD_SIZE os environ TEMP_DIR os environ TODO can we actually have ` run_tests py ` emit complete instructions when prints repro command raise RuntimeError Missing expected env vars ` test_distributed_spawn py ` Please ensure specify following \n f BACKEND = one _allowed_backends \n f WORLD_SIZE = int = \n TEMP_DIR specifying directory containing barrier file named barrier \n\n f e g \ntouch tmp barrier TEMP_DIR= tmp BACKEND= nccl WORLD_SIZE= python __file__ BACKEND = os environ BACKEND BACKEND _allowed_backends TestDistBackendWithSpawn TestDistBackend DistributedTest _DistTestBase setUp super setUp _spawn_processes torch backends cudnn flags enabled=True allow_tf =False __enter__ print f Invalid backend BACKEND Tests will run __name__ == __main__ run_tests