Copyright c Meta Platforms Inc affiliates Owner s oncall distributed copy deepcopy torch torch nn nn torch distributed device_mesh init_device_mesh torch distributed tensor distribute_tensor DTensor Replicate Shard torch distributed tensor debug CommDebugMode torch distributed tensor parallel parallelize_module torch distributed tensor parallel style ColwiseParallel PrepareModuleInput PrepareModuleOutput RowwiseParallel SequenceParallel torch distributed tensor placement_types _Partial torch testing _internal common_utils run_tests torch testing _internal distributed _tensor common_dtensor DTensorTestBase NUM_DEVICES RMSNormPython with_comms c d_functional = torch ops c d_functional TensorParallelStyleTest DTensorTestBase property world_size NUM_DEVICES with_comms test_colwise_parallel_style mesh = init_device_mesh device_type world_size comm_mode = CommDebugMode tensor = torch rand device=self device_type requires_grad=True model = nn Linear device=self device_type default_col_parallel = ColwiseParallel colwise_mod = parallelize_module deepcopy model mesh default_col_parallel comm_mode out = colwise_mod tensor ensure output shard last dim assertEqual out shape world_size ensure no communication happened fwd assertEqual comm_mode get_total_counts out sum backward allreduce bwd assertEqual comm_mode get_comm_counts c d_functional all_reduce assertEqual comm_mode get_total_counts sharded_col_parallel = ColwiseParallel input_layouts=Shard colwise_mod = parallelize_module deepcopy model mesh sharded_col_parallel comm_mode out = colwise_mod tensor ensure output shard last dim assertEqual out shape world_size world_size allgather fwd assertEqual comm_mode get_comm_counts c d_functional all_gather_into_tensor assertEqual comm_mode get_total_counts out sum backward reduce_scatter bwd assertEqual comm_mode get_comm_counts c d_functional reduce_scatter_tensor assertEqual comm_mode get_total_counts with_comms test_colwise_parallel_embedding mesh = init_device_mesh device_type world_size comm_mode = CommDebugMode tensor = torch arange device=self device_type reshape model = nn Embedding device=self device_type default_col_parallel = ColwiseParallel colwise_mod = parallelize_module deepcopy model mesh default_col_parallel comm_mode out = colwise_mod tensor ensure output shard last dim assertEqual out shape world_size ensure no communication happened fwd assertEqual comm_mode get_total_counts out sum backward no comm bwd assertEqual comm_mode get_total_counts with_comms test_rowwise_parallel_style mesh = init_device_mesh device_type world_size comm_mode = CommDebugMode tensor = torch rand world_size device=self device_type requires_grad=True model = nn Linear device=self device_type default_row_parallel = RowwiseParallel rowwise_mod = parallelize_module deepcopy model mesh default_row_parallel comm_mode out = rowwise_mod tensor ensure output replicated assertEqual out shape allreduce fwd assertEqual comm_mode get_comm_counts c d_functional all_reduce assertEqual comm_mode get_total_counts out sum backward no op bwd assertEqual comm_mode get_total_counts sharded_row_parallel = RowwiseParallel output_layouts=Shard rowwise_mod = parallelize_module deepcopy model mesh sharded_row_parallel comm_mode out = rowwise_mod tensor ensure output replicated assertEqual out shape world_size reduce_scatter fwd assertEqual comm_mode get_comm_counts c d_functional reduce_scatter_tensor assertEqual comm_mode get_total_counts out sum backward allgather bwd assertEqual comm_mode get_comm_counts c d_functional all_gather_into_tensor assertEqual comm_mode get_total_counts with_comms test_rowwise_parallel_embedding mesh = init_device_mesh device_type world_size comm_mode = CommDebugMode tensor = torch arange device=self device_type reshape model = nn Embedding device=self device_type rowwise_mod = parallelize_module deepcopy model mesh RowwiseParallel input_layouts=Replicate comm_mode out = rowwise_mod tensor ensure output shard last dim assertEqual out shape ensure allreduce communication happened fwd assertEqual comm_mode get_total_counts assertEqual comm_mode get_comm_counts c d_functional all_reduce out sum backward no comm bwd assertEqual comm_mode get_total_counts sharded_row_parallel = RowwiseParallel input_layouts=Replicate output_layouts=Shard rowwise_mod = parallelize_module deepcopy model mesh sharded_row_parallel inp_indices = torch arange device=self device_type comm_mode out = rowwise_mod inp_indices ensure output shard last dim assertEqual out shape world_size reduce scatter fwd assertEqual comm_mode get_total_counts assertEqual comm_mode get_comm_counts c d_functional reduce_scatter_tensor out sum backward allgather comm bwd assertEqual comm_mode get_total_counts assertEqual comm_mode get_comm_counts c d_functional all_gather_into_tensor with_comms test_prepare_module_input mesh = init_device_mesh device_type world_size tensor = torch ones device=self device_type expected_tensor = torch ones world_size device=self device_type prepare_inp_style = PrepareModuleInput input_layouts=Shard desired_input_layouts=Replicate model = nn Identity allgather_mod = parallelize_module model mesh prepare_inp_style output = allgather_mod tensor full_tensor assertEqual output expected_tensor with_comms test_prepare_module_input_multiple_inputs mesh = init_device_mesh device_type world_size TestModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y linear x + y Raise assertion error input_layouts desired_input_layouts do have same length test_mod = TestModule device_type assertRaisesRegex AssertionError input_layouts desired_input_layouts should have same length PrepareModuleInput input_layouts=Shard desired_input_layouts= Replicate None Raise assertion error module inputs input_layouts do have same length prepare_inps_short_dimension = PrepareModuleInput input_layouts=Shard desired_input_layouts=Replicate parallelize_module test_mod linear mesh ColwiseParallel parallelize_module test_mod mesh prepare_inps_short_dimension assertRaisesRegex ValueError module inputs input_layouts should have same length output = test_mod torch randn device=self device_type torch ones world_size world_size device=self device_type test_mod = TestModule device_type prepare_inps = PrepareModuleInput input_layouts= Shard None desired_input_layouts= Replicate None parallelize_module test_mod linear mesh ColwiseParallel parallelize_module test_mod mesh prepare_inps output = test_mod torch randn device=self device_type torch ones world_size world_size device=self device_type assertEqual output shape world_size world_size with_comms test_prepare_module_kwargs_input mesh = init_device_mesh device_type world_size TestKwargModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y z= linear x + y + z test_mod = TestKwargModule device_type prepare_inps_simple = PrepareModuleInput input_kwarg_layouts= y Shard desired_input_kwarg_layouts= y Replicate parallelize_module test_mod linear mesh ColwiseParallel use_local_output=False parallelize_module test_mod mesh prepare_inps_simple comm_mode = CommDebugMode comm_mode output = test_mod torch randn world_size device=self device_type y=torch ones device=self device_type assertEqual comm_mode get_total_counts assertEqual output shape world_size TestKwargOnlyModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y= z=None linear x + y + z test_kwonly_mod = TestKwargOnlyModule device_type prepare_inps_simple = PrepareModuleInput input_kwarg_layouts= x Shard z Shard desired_input_kwarg_layouts= x Replicate z Replicate parallelize_module test_kwonly_mod linear mesh ColwiseParallel use_local_output=False parallelize_module test_kwonly_mod mesh prepare_inps_simple comm_mode output = test_kwonly_mod x=torch randn device=self device_type z=torch ones device=self device_type assertEqual comm_mode get_total_counts assertEqual output shape world_size test case where x DTensor x_dt = DTensor from_local torch randn device=self device_type mesh Shard comm_mode output = test_kwonly_mod x=x_dt z=torch ones device=self device_type assertEqual comm_mode get_total_counts assertEqual output shape world_size with_comms test_prepare_module_output mesh = init_device_mesh device_type world_size tensor = torch ones device=self device_type expected_tensor = torch ones world_size device=self device_type prepare_out_style = PrepareModuleOutput output_layouts=Replicate desired_output_layouts=Shard model = nn Identity chunk_mod = parallelize_module model mesh prepare_out_style output = chunk_mod tensor assertEqual output expected_tensor with_comms test_sequence_parallel_style mesh = init_device_mesh device_type world_size early init RNG tracker torch distributed tensor _random manual_seed mesh comm_mode = CommDebugMode batch N embedding_dim = global_input = torch rand batch N world_size embedding_dim device=self device_type requires_grad=True sharded_input = distribute_tensor global_input mesh Shard test LayerNorm elementwise_affine True False norm = nn LayerNorm embedding_dim elementwise_affine=elementwise_affine device=self device_type sp_norm = parallelize_module deepcopy norm mesh SequenceParallel output = norm global_input output sum backward comm_mode sharded_out = sp_norm sharded_input grad_out = torch ones_like sharded_out sharded_out backward grad_out assertIsInstance sharded_out DTensor assertEqual sharded_out placements Shard assertEqual comm_mode get_total_counts assertEqual comm_mode get_comm_counts c d_functional all_reduce elementwise_affine assertEqual sp_norm weight grad placements _Partial assertEqual sp_norm bias grad placements _Partial assertEqual sharded_out full_tensor output test RMSNorm rmsnorm = RMSNormPython embedding_dim device_type sp_rmsnorm = parallelize_module deepcopy rmsnorm mesh SequenceParallel output = rmsnorm global_input output sum backward comm_mode sharded_out = sp_rmsnorm sharded_input grad_out = torch ones_like sharded_out sharded_out backward grad_out assertIsInstance sharded_out DTensor assertEqual sharded_out placements Shard assertEqual sp_rmsnorm weight grad placements _Partial assertEqual comm_mode get_total_counts assertEqual comm_mode get_comm_counts c d_functional all_reduce assertEqual sharded_out full_tensor output test dropout dropout = nn Dropout device_type sp_dropout = parallelize_module deepcopy dropout mesh SequenceParallel output = dropout global_input output sum backward comm_mode sharded_out = sp_dropout sharded_input grad_out = torch ones_like sharded_out sharded_out backward grad_out assertIsInstance sharded_out DTensor assertEqual sharded_out placements Shard assertEqual comm_mode get_total_counts test sharded non-sequence dim input sharded_batch_input = distribute_tensor global_input mesh Shard rmsnorm = RMSNormPython embedding_dim device_type sp_rmsnorm = parallelize_module deepcopy rmsnorm mesh SequenceParallel comm_mode sharded_out = sp_rmsnorm sharded_batch_input grad_out = torch ones_like sharded_out sharded_out backward grad_out assertIsInstance sharded_out DTensor output still sharded sequence dimension assertEqual sharded_out placements Shard assertEqual sp_rmsnorm weight grad placements _Partial communication happens both fwd bwd redistribute input assertEqual comm_mode get_total_counts __name__ == __main__ run_tests