Owner s module inductor contextlib importlib math operator os sys unittest functools partial torch torch library torch _dynamo testing CompileCounterWithBackend make_test_cls_with_patches torch _inductor metrics torch _inductor choices InductorChoices torch _inductor codegen wrapper PythonWrapperCodegen torch _inductor test_case TestCase torch _inductor utils run_and_get_code torch _inductor virtualized V torch testing FileCheck torch testing _internal common_cuda IS_SM torch testing _internal common_device_type instantiate_device_type_tests onlyCPU onlyOn torch testing _internal common_utils IS_ARM IS_FBCODE parametrize serialTest TEST_CUDA_MEM_LEAK_CHECK TEST_WITH_ASAN TEST_WITH_ROCM torch testing _internal inductor_utils GPU_TYPE HAS_CPU HAS_GPU HAS_MPS patch_inductor_backend Make helper files test importable pytorch_test_dir = os path dirname os path dirname os path realpath __file__ sys path append pytorch_test_dir inductor test_torchinductor manual=fbcode caffe test inductor test_inductor-library check_model check_model_gpu CommonTemplate copy_tests TestFailure importlib import_module filelock xfail default set is_skip=True skip test_failures = test_kwargs_dynamic_shapes TestFailure cpu calling div only symint args test_AllenaiLongformerBase_repro_dynamic_shapes TestFailure cpu cuda xpu mps test_argmax_argmin_with_duplicates_dynamic_shapes TestFailure mps test_batch_norm_ d_ _dynamic_shapes TestFailure mps test_buffer_batch_norm_dynamic_shapes TestFailure mps test_convolution _dynamic_shapes TestFailure mps test_index_propagation_abs_dynamic_shapes TestFailure mps test_index_propagation_floordiv_dynamic_shapes TestFailure mps test_index_propagation_remainder_dynamic_shapes TestFailure mps test_multilayer_var_dynamic_shapes TestFailure mps test_multilayer_var_lowp_dynamic_shapes TestFailure mps test_reduction _dynamic_shapes TestFailure mps test_reduction _dynamic_shapes TestFailure mps test_reduction _dynamic_shapes TestFailure mps test_reflection_pad d_dynamic_shapes TestFailure mps test_roll_dynamic_shapes TestFailure mps test_std_dynamic_shapes TestFailure mps test_var_correction_dynamic_shapes TestFailure mps test_var_mean_div_by_dynamic_shapes TestFailure mps test_var_mean_tile_reduction_False_dynamic_shapes TestFailure mps test_var_mean_tile_reduction_True_dynamic_shapes TestFailure mps test_vectorized_ops_masked_var_novec_dynamic_shapes TestFailure mps test_reflection_pad d_backward_dynamic_shapes TestFailure mps is_skip=True torch _inductor config cpp_wrapper test_failures test_conv_inference_heuristics_dynamic_shapes = TestFailure cuda TEST_WITH_ROCM Tensor-likes close test_failures test_dynamic_stride_nobreak = TestFailure cpu cuda is_skip=True test_failures test_item_to_inputs_kernel_nobreak = TestFailure cpu cuda is_skip=True test_failures test_unbacked_reduction = TestFailure cpu is_skip=True any os getenv BUILD_ENVIRONMENT endswith x x -debug -asan Fails TORCH_INTERNAL_ASSERT is_heap_allocated see https github com pytorch pytorch issues After https github com pytorch pytorch pull starts failing UBSAN so we can t even xfail Root cause seems SymInt issues StorageImpl see https github com pytorch pytorch pull #issuecomment- test_failures test_resize_as_dynamic_shapes = TestFailure cpu cuda is_skip=True test_failures test_resize_dynamic_shapes = TestFailure cpu cuda is_skip=True make_dynamic_cls cls xfail_prop= _expected_failure_dynamic make_test_cls_with_patches cls DynamicShapes _dynamic_shapes torch _dynamo config assume_static_by_default False xfail_prop=xfail_prop DynamicShapesCommonTemplate = make_dynamic_cls CommonTemplate HAS_CPU DynamicShapesCpuTests TestCase common = check_model device = cpu copy_tests DynamicShapesCommonTemplate DynamicShapesCpuTests cpu test_failures HAS_GPU HAS_MPS TEST_WITH_ASAN DynamicShapesGPUTests TestCase common = check_model_gpu device = GPU_TYPE copy_tests DynamicShapesCommonTemplate DynamicShapesGPUTests GPU_TYPE test_failures TestInductorDynamic TestCase compile_fn = partial torch compile dynamic=True setUp HAS_CUDA_AND_TRITON also checks compute capability skip tests older devices HAS_GPU skipTest Triton available torch _dynamo reset TestCase setUp should setUpClass device-generic tests don t work setUpClass well non-deterministically wrong setUpClass resolved so put test setUp s cheap _stack = contextlib ExitStack _stack enter_context torch _inductor config patch debug False cpp min_chunk_size triton autotune_pointwise False too slow implicit_fallbacks False tearDown _stack close TestCase tearDown torch _dynamo reset test_constant_fold_uniform_value_dynamic device full_add_zero x = torch full x shape dtype=x dtype device=x device b = - x + b full_mul_one x = torch full x shape - dtype=x dtype device=x device b = + x b full_view_op x = torch ones dtype=x dtype device=x device = None x full_mul_symint x = torch full x shape - dtype=x dtype device=x device b = + b x shape fns = full_add_zero full_mul_one full_view_op x = torch randn device=device y = torch randn device=device dynamic False True torch _dynamo reset fn fns ref = fn x fn_c = torch compile fn dynamic=dynamic actual source_codes = run_and_get_code fn_c x fn full_mul_symint due constant folding fn returns x directly device == cpu FileCheck check_not cpp_fused run source_codes FileCheck check_not triton jit run source_codes assertEqual ref actual assertEqual fn x fn_c x assertEqual fn y fn_c y test_arange_dynamic device fn batch_size = numel max_len = max ~ torch arange max_len device=a device type_as repeat batch_size lt unsqueeze = torch randint device=device = fix max_len opt = compile_fn fn res = opt ref = fn assertEqual res ref test_shape_as_constant_reciprocal_float_exp device fn x x - x = torch rand device=device opt = compile_fn fn res = opt x x size ref = fn x x size assertEqual res ref supported yet cpu https github com pytorch pytorch issues torch _dynamo config patch capture_dynamic_output_shape_ops=True test_bool_mask_nobreak device f x b x b sum opt_f = torch compile f fullgraph=True x = torch randn device=device b = torch tensor True True False False True device=device r = f x b opt_r = opt_f x b assertEqual r opt_r test_adaptive_max_pool d_with_indices device x = y = torch rand dtype=torch float device=device fn x y torch nn functional adaptive_max_pool d_with_indices output_size=x input=y return_indices=True opt_f = compile_fn fn r = fn x y opt_r = opt_f x y assertEqual r opt_r torch _dynamo config patch capture_scalar_outputs=True test_unwrap_storage_didnt_work_repro device f full = torch full i = full item torch full i opt_f = torch compile f fullgraph=True r = f opt_r = opt_f assertEqual r opt_r torch _dynamo config patch capture_scalar_outputs=True test_sym_sum_unbacked device f xs = tolist y = sum xs torch tensor y splits = torch randint device=device opt_f = torch compile f fullgraph=True r = f splits opt_r = opt_f splits assertEqual r opt_r torch _dynamo config patch capture_dynamic_output_shape_ops=True test_nonzero_size_factory_nobreak device f x b y = torch nonzero b x new_zeros y size opt_f = torch compile f fullgraph=True x = torch randn device=device b = torch tensor True True False False True device=device r = f x b opt_r = opt_f x b assertEqual r opt_r torch _dynamo config patch capture_dynamic_output_shape_ops=True test_nonzero_no_realloc device torch compile fullgraph=True dynamic=True f x y z = x nonzero torch split z y size f torch tensor torch randn torch _dynamo config patch capture_scalar_outputs=True test_item_nobreak device torch compile fullgraph=True f x y = x item torch empty y f torch tensor device=device torch _dynamo config patch capture_scalar_outputs=True test_item_bool_nobreak device torch compile fullgraph=True f x x item f torch tensor True device=device torch _dynamo config patch capture_scalar_outputs=True capture_dynamic_output_shape_ops=True test_noops_tensor_repropagate device torch compile fullgraph=True f x b = torch ops prims convert_element_type default x torch int r = b nonzero r f torch tensor dtype=torch int device=device torch _dynamo config patch capture_scalar_outputs=True test_item_zeros_nobreak device torch compile fullgraph=True f x y = x item torch empty y This will avoid NopSchedulerNode x new_zeros y f torch tensor device=device torch _dynamo config patch capture_scalar_outputs=True test_item_return device torch compile fullgraph=True f x y = x item z = x item y + z f torch tensor device=device torch _dynamo config patch capture_scalar_outputs=True capture_dynamic_output_shape_ops=True test_float_item_inf device torch compile fullgraph=True f x x item == math inf f torch tensor device=device torch _dynamo config patch capture_scalar_outputs=True capture_dynamic_output_shape_ops=True test_float_item_neginf device torch compile fullgraph=True f x x item == -math inf f torch tensor device=device torch _dynamo config patch capture_scalar_outputs=True torch _inductor config patch implicit_fallbacks=True test_item_to_inputs_kernel_nobreak device torch library custom_op test_inductor_dynamic_shapes nobreak_test mutates_args= nobreak_test x torch Tensor y int - torch Tensor x clone nobreak_test register_fake _ x torch Tensor y int - torch Tensor x clone torch compile fullgraph=True f x r y = x item torch ops test_inductor_dynamic_shapes nobreak_test r y f torch tensor device=device torch randn device=device unittest skipUnless IS_FBCODE torch _dynamo config patch capture_scalar_outputs=True capture_dynamic_output_shape_ops=True test_float_item_return device torch compile fullgraph=True f x x item f torch tensor device=device unittest skipIf TEST_CUDA_MEM_LEAK_CHECK failing memory leak check torch _dynamo config patch capture_scalar_outputs=True test_unbacked_index_select device Tests unbacked symbols captured inner_fn properly tracked f x y = x item torch index_select torch ones y device=device torch tensor device=device cf = torch compile fullgraph=True f arg = torch tensor device=device assertEqual f arg cf arg torch _dynamo config patch capture_scalar_outputs=True capture_dynamic_output_shape_ops=True test_return_unbacked_view_split device f values length_per_key u u = length_per_key tolist v v = torch functional split values u u v v cf = torch compile fullgraph=True f args = torch randn requires_grad=True device=device torch tensor device=device assertEqual f args cf args torch _dynamo config patch capture_scalar_outputs=True test_unbacked_matmul device f x y = x item torch ones y device=device torch ones y device=device cf = torch compile fullgraph=True f arg = torch tensor device=device assertEqual f arg cf arg torch _dynamo config patch capture_scalar_outputs=True capture_dynamic_output_shape_ops=True torch _inductor config patch implicit_fallbacks=True test_unbacked_save_for_backwards device - None torch library custom_op _test _cat mutates_args= _cat t torch Tensor ds list int - torch Tensor t t new_ones sum ds torch library register_fake _test _cat _cat_fake t torch Tensor ds list int - torch Tensor t new_empty sum ds _cat_setup_context ctx inputs output pass _cat_backward ctx grad grad sum None torch library register_autograd _test _cat _cat_backward setup_context=_cat_setup_context fn t sizes r = torch ops _test _cat t sizes tolist r t t = torch randn requires_grad=True device=device sizes = torch tensor dtype=torch int device= cpu out = fn t sizes out sum backward expect = t grad t grad = None torch compile fn backend= inductor fullgraph=True dynamic=True t sizes sum backward assertEqual t grad expect torch _dynamo config patch capture_scalar_outputs=True test_unbacked_reduction device expect_fail = device == cpu IS_ARM torch _inductor config cpp_wrapper try f x y = x item torch ones y device=device sum cf = torch compile fullgraph=True f arg = torch tensor device=device assertEqual f arg cf arg except Exception expect_fail raise expect_fail fail expected fail actually passed torch _dynamo config patch capture_scalar_outputs=True capture_dynamic_output_shape_ops=True test_cat_unbacked_duplicate_size device f x device = x device s s = x tolist g = torch zeros s device=device g = torch ones s device=device torch ops aten cat default g g g cf = torch compile fullgraph=True f arg = torch tensor device=GPU_TYPE assertEqual f arg cf arg torch _dynamo config patch capture_scalar_outputs=True capture_dynamic_output_shape_ops=True test_unbacked_cat_backwards device f x w device = w device b = x tolist ta = torch ones device=device tb = torch ones b device=device pa = ta w make require gradients pb = tb w r = torch cat pa pb r sum x = torch tensor w = torch randn requires_grad=True f x w backward orig_w = w grad w grad = None torch compile fullgraph=True f x w backward assertEqual orig_w w grad torch _dynamo config patch capture_scalar_outputs=True capture_dynamic_output_shape_ops=True test_unbacked_cat_backwards_save_data_dependent device f x w device = w device b = x tolist ta = torch ones device=device tb = torch ones b device=device pa = ta w make require gradients pb = tb w r = torch cat pa pb r x = torch tensor w = torch randn requires_grad=True f x w sum backward orig_w = w grad w grad = None torch compile fullgraph=True f x w sum backward assertEqual orig_w w grad torch _dynamo config patch capture_scalar_outputs=True capture_dynamic_output_shape_ops=True torch _inductor config patch implicit_fallbacks=True test_dynamic_stride_nobreak device torch library custom_op test_dynamic_stride_nobreak foo mutates_args= foo x torch Tensor - torch Tensor stride = x item torch empty_strided stride device=x device foo register_fake _ x torch Tensor - torch Tensor ctx = torch library get_ctx stride = ctx new_dynamic_size torch empty_strided stride device=x device torch compile fullgraph=True f x r = torch ops test_dynamic_stride_nobreak foo x y = r stride torch empty y device=x device f torch tensor device=device unittest skipIf IS_SM Fails OOMS SM see https github com pytorch pytorch issues torch _dynamo config patch capture_scalar_outputs=True capture_dynamic_output_shape_ops=True torch _inductor config patch implicit_fallbacks=True test_multi_output_unbacked_custom_op device torch library custom_op test_inductor_dynamic_shapes unbacked_test mutates_args= unbacked_test x torch Tensor - tuple torch Tensor torch Tensor torch empty device=x device torch empty device=x device unbacked_test register_fake _ x torch Tensor - torch Tensor ctx = torch library get_ctx u = ctx new_dynamic_size torch empty u device=x device torch empty device=x device torch compile fullgraph=True f x b = torch ops test_inductor_dynamic_shapes unbacked_test x sum + b sum f torch tensor device=device test_meta_dynamic_shapes foobar x y x y foo_c = torch compile dynamic=True foobar t = torch empty device= meta y = torch rand assertEqual foo_c t y foobar t y test_floor fn x n = x size - y = x + int n + y opt = compile_fn fn The first run doesn t trigger dynamic shapes x = torch rand ref = fn x res = opt x assertEqual ref res The second run triggers dynamic shapes x = torch rand ref = fn x res = opt x assertEqual ref res onlyOn GPU_TYPE test_pad_dynamic device get_same_padding x int k int s int d int max math ceil x s - s + k - d + - x pad_same x k s d= value= ih iw = x size - pad_h pad_w = get_same_padding ih k s d get_same_padding iw k s d pad_h pad_w x = torch nn functional pad x pad_w pad_w - pad_w pad_h pad_h - pad_h value=value x x = torch randn device=device opt = compile_fn pad_same res = opt x ref = pad_same x assertEqual res ref atol= rtol= test_slice_scatter device fn i s = i size x = torch ones s device=device y = torch ones s device=device torch slice_scatter x y s s = torch randn device=device cfn = compile_fn fn expect = fn actual = cfn assertEqual expect actual test_slice_index_changing_sign device fn x y y y = y shape x y - y clone = torch randn device=device cfn = compile_fn fn y y - y - y positive b = torch randn device=device expect = fn b actual = cfn b assertEqual expect actual y y - y - y negative b = torch randn device=device expect = fn b actual = cfn b assertEqual expect actual test_sym_stride_lowering device fn x s = x + stride x s = torch randn device=device cfn = compile_fn fn assertEqual fn cfn torch _dynamo config patch capture_scalar_outputs=True test_item_materialize device fn x x sum dim= view tolist cfn = torch compile fullgraph=True fn = torch ones dtype=torch int device=device assertEqual cfn fn test_abs device fn x y y y = y shape Slicing checks abs wrapper code multiplication tests abs kernel code x abs y - y abs y - y = torch randn device=device cfn = compile_fn fn y y - y - y positive b = torch randn device=device expect = fn b actual = cfn b assertEqual expect actual y y - y - y negative b = torch randn device=device expect = fn b actual = cfn b assertEqual expect actual test_float_is_integer device fn x mul dim=- size = x size dim m = size mul m is_integer m size = torch randn device=device cfn = compile_fn fn expect = fn actual = cfn assertEqual expect actual onlyCPU test_arithmetic_constant_folding device test fn cfn = compile_fn fn expect = fn actual = cfn assertEqual expect actual add x x + torch zeros test add mul x x torch ones test mul div x x torch ones test div onlyCPU test_sub_constant_folding device sub x x - torch zeros cfn = compile_fn sub expect = sub actual = cfn assertEqual expect actual test_full_symbolic_value device fn torch full torch full torch sym_float cfn = compile_fn fn expect = fn actual = cfn assertEqual expect actual test_interpolate_ceil_eq device ceiling = math ceil IntTrueDiv = operator truediv fn t s s s = t size x = torch zeros s ceiling IntTrueDiv s - + ceiling IntTrueDiv s - + dtype=torch bfloat torch nn functional interpolate x scale_factor= mode= nearest cfn = compile_fn fn arg = torch randn expect = fn arg actual = cfn arg assertEqual expect actual test_full_recompiles device fn x _ L = x shape torch full L L torch finfo torch float min device=device cfn = compile_fn fn functools input_fn = functools partial torch randint device=device cfn input_fn cfn input_fn expect don t recompile here check compiled times frame torch _dynamo convert_frame FRAME_COMPILE_COUNTER assertEqual FRAME_COMPILE_COUNTER parametrize op math sqrt math sin math cos math cosh math sin math sinh math tan math tanh math asin math acos math atan test_math_ops device op func x fn x + fn cfunc = compile_fn func fullgraph=True x = torch rand device=device = - op math asin math acos expected = func x op output = cfunc x op assertEqual output expected serialTest test_wrapper_codegen_statically_known_int_or_none torch _dynamo reset _x = torch randn torch _dynamo maybe_mark_dynamic _x Simple functions introducing constraints x shape fn_ x no constraint x sin fn_ x constrain two directions x shape x cos x shape x x shape == point x sin fn_ x equality constraint which matches example shape x size == x sin x cos call_count = _test_wrapper_codegen_statically_known_int_or_none_in_context nonlocal call_count call_count += graph = V graph input_layouts = inp layout inp graph graph_inputs values hasattr inp layout batch_dim = input_layouts size call_count == testing fn_ assert PythonWrapperCodegen statically_known_int_or_none batch_dim None Should statically known first call call_count == testing fn_ assert PythonWrapperCodegen statically_known_int_or_none batch_dim == Should limited exactly second call due multiple constraints call_count == testing fn_ assert PythonWrapperCodegen statically_known_int_or_none batch_dim == Should exactly third call TestWrapperCodegen PythonWrapperCodegen __init__ args kwargs super __init__ args kwargs generate is_inference args kwargs _test_wrapper_codegen_statically_known_int_or_none_in_context super generate is_inference args kwargs patch_inductor_backend cpu python_wrapper_codegen=TestWrapperCodegen Compile each functions above example input has first dimension marked dynamic torch compile backend= inductor dynamic=None fn_ _x torch compile backend= inductor dynamic=None fn_ _x torch compile backend= inductor dynamic=None fn_ _x torch _dynamo config patch capture_scalar_outputs=True test_item_unbacked_stride_nobreak device torch compile fullgraph=True dynamic=True f x = x item torch _check = torch _check = torch ones f torch tensor device=device torch _dynamo config patch capture_scalar_outputs=True test_symint_sum_list device torch compile f xt xs = xt tolist y = sum xs torch zeros y device=device f torch tensor test_mark_unbacked_slice torch compile backend= inductor mode= reduce-overhead fullgraph=True f x x sum x = torch empty_strided device=GPU_TYPE torch _dynamo decorators mark_unbacked x f x torch _dynamo config patch specialize_float=False capture_scalar_outputs=True test_unspecialized_float_operations operations = multiply operator mul add operator add subtract operator sub divide operator truediv i name op enumerate operations items subTest operation=name fn x y op x y cnt = CompileCounterWithBackend inductor fn_opt = torch compile fn backend=cnt x = torch arange assertEqual fn x fn_opt x assertEqual fn x fn_opt x assertEqual fn x fn_opt x i == Automatic dynamic state persists across compiles so only first compile goes through automatic dynamic step assertEqual cnt frame_count assertEqual cnt frame_count torch _dynamo config patch specialize_float=False test_unspecialized_float_fallback_specialization fn x y z torch tensor z torch exp torch tensor z x y x size math sqrt x size math floor math sqrt x size math floor math sqrt x numel math floor math sqrt x dim math floor math sqrt z cnt = CompileCounterWithBackend inductor fn_opt = torch compile fn backend=cnt x = torch arange z = assertEqual fn x z fn_opt x z assertEqual fn x z fn_opt x z assertEqual fn x z fn_opt x z Automatic dynamic float arguments assertEqual cnt frame_count torch _dynamo config patch specialize_float=False test_unspecialized_float_softshrink This test particularly interesting since exercises both standard operator replacements ie torch ops aten mul Tensor well comparison replacements ie torch ops aten ge Scalar fn x y torch _C _nn softshrink x lambd=y cnt = CompileCounterWithBackend inductor fn_opt = torch compile fn backend=cnt x = torch randn print fn x fn_opt x assertEqual fn x fn_opt x assertEqual fn x fn_opt x assertEqual fn x fn_opt x assertEqual cnt frame_count onlyOn GPU_TYPE test_dynamic_rblock_bounds ForcePersistent InductorChoices staticmethod should_use_cooperative_reduction args kwargs - bool False staticmethod should_use_persistent_reduction args kwargs - bool True fn x x sum x = torch rand device=GPU_TYPE V set_choices_handler ForcePersistent torch _dynamo mark_dynamic x min= max= fn_c = torch compile fn actual source_codes = run_and_get_code fn_c x assertEqual fn x actual FileCheck check R _BLOCK tl constexpr = run source_codes torch _dynamo reset torch _dynamo mark_dynamic x min= max= fn_c = torch compile fn actual source_codes = run_and_get_code fn_c x assertEqual fn x actual FileCheck check R _BLOCK tl constexpr = run source_codes test_non_persistent_dynamic_rblock reduce_bounded x y Reduce over dimension bounded size x shape batch features reduction_dim reduction_dim dynamic bounded max assert x shape = f Reduction dim x shape exceeds max Perform reduction sum over last dimension result = torch sum x y dim= result Create tensors where reduction dimension could up batch = features = reduction_dim = Actual size small x = torch randn reduction_dim batch features device=GPU_TYPE permute y = torch randn reduction_dim batch features device=GPU_TYPE permute torch _dynamo mark_dynamic x min= max= torch _dynamo mark_dynamic y min= max= compiled_fn = torch compile reduce_bounded result source_codes = run_and_get_code compiled_fn x y FileCheck check_not triton_heuristics persistent run source_codes expected = reduce_bounded x y assert torch allclose result expected atol= e- rtol= e- test_unspecialized_float_dynamic fn x y x y cnt = CompileCounterWithBackend inductor fn_opt = torch compile fn dynamic=True backend=cnt x = torch randn assertEqual fn x fn_opt x assertEqual fn x fn_opt x assertEqual fn x fn_opt x assertEqual cnt frame_count torch _dynamo config patch specialize_float=False test_unspecialized_float_fallback_symint_specialization fn x y math floor x y cnt = CompileCounterWithBackend inductor fn_opt = torch compile fn backend=cnt y = torch arange assertEqual fn y fn_opt y assertEqual fn y fn_opt y assertEqual fn y fn_opt y N + automatic dynamic float arguments assertEqual cnt frame_count test_sort_dynamic_shape_with_check device torch device device type = GPU_TYPE check_count n assertEqual metrics generated_kernel_count check_count n assertEqual metrics generated_kernel_count n Test dynamic shapes statically known small enough generate persistent sort kernel fn descending torch _check shape - = sort dim=- stable=True descending=descending inp = torch rand dtype=torch float device=device inp = inp = metrics reset opt_fn = torch compile fn dynamic=True expect = fn inp False actual = opt_fn inp False assertEqual actual expect check_count expect = fn inp True actual = opt_fn inp True assertEqual actual expect check_count Non-power two inp expect = fn inp False actual = opt_fn inp False assertEqual actual expect check_count Reused existing kernel expect = fn inp True actual = opt_fn inp True assertEqual actual expect check_count Reused existing kernel instantiate_device_type_tests TestInductorDynamic globals allow_xpu=True __name__ == __main__ torch _inductor test_case run_tests Slow ASAN after https github com pytorch pytorch pull HAS_CPU HAS_GPU HAS_MPS TEST_WITH_ASAN run_tests needs= filelock