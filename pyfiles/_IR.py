mypy allow-untyped-defs Copyright c Meta Platforms Inc affiliates copy logging operator collections defaultdict collections abc Callable enum Enum inspect Parameter Signature signature types MethodType typing Any Optional Union torch torch fx fx torch distributed ProcessGroup torch export ExportedProgram torch export unflatten _assign_attr _AttrKind _sink_params InterpreterModule torch fx node map_aggregate torch fx passes split_module split_module _backward _null_coalesce_accumulate stage_backward _unflatten _outline_submodules _utils PipeInfo stage _PipelineStage logger = logging getLogger __name__ TODO investigate gradient sync shared parameters how does DDP do Add parameter movement split_module PP_SUBMOD_PREFIX = submod_pp get_submod_name stage_idx int Returns name submod given stage index For example submod_pp_ submod_pp_ etc _ join PP_SUBMOD_PREFIX str stage_idx _find_loss_from_output_and_spec output_val spec_val spec_val False None spec_val True isinstance output_val fx Node raise RuntimeError f Loss spec must specify dynamic value got output_val output_val isinstance spec_val tuple list isinstance output_val tuple list raise RuntimeError f Output value output_val must match type loss specification f spec_val len output_val = len spec_val raise RuntimeError f Output value output_val must match length loss specification f spec_val out spec zip output_val spec_val loss_val = _find_loss_from_output_and_spec out spec loss_val None loss_val raise RuntimeError f Did find loss value specification spec_val isinstance spec_val dict isinstance output_val dict raise RuntimeError f Output value output_val must match type loss specification f spec_val set output_val keys = set spec_val keys raise RuntimeError f Output value output_val must match keys loss specification f spec_val k spec_val loss_val = _find_loss_from_output_and_spec output_val k spec_val k loss_val None loss_val raise RuntimeError f Did find loss value specification spec_val raise RuntimeError f Unsupported type type spec_val loss specification _find_loss_output mod torch nn Module g fx Graph output_loss_value_spec output_nodes = n n g nodes n op == output assert len output_nodes == output_node = output_nodes output_val = output_node args generated_spec Any = None isinstance mod TrivialLossWrapper TrivialLossWrapper pre-defined PiPPy It has loss only output so we can safely assume first output arg loss assert len output_node args == loss_node = output_val generated_spec = TrivialLossWrapper loss_spec output_loss_value_spec None Use default spec i e search loss output values isinstance output_val dict loss output_val keys loss_node = output_val loss generated_spec = k k == loss k output_val loss_node = None generated_spec = None loss_node = _find_loss_from_output_and_spec output_val output_loss_value_spec generated_spec = output_loss_value_spec loss_node output_node generated_spec _insert_stage_symbolic_backward g fx Graph loss_node fx Node output_node fx Node Collect metadata about tuple output values TODO move split_module FX IR tuples dict fx Node tuple = node reversed g nodes node op == call_function In forward pass only emit placeholder module calls getitem calls If we have target other than getitem forward-only code there bug assert node target operator getitem Found non-getitem call forward pass Please report bug PiPPy assert len node args == Found malformed getitem call Please report bug PiPPy indexed_value node_idx = tuple node args indexed_value collection we indexing into It could exist tuples map we ve processed another ` getitem ` already existing_list_size = len tuples indexed_value indexed_value tuples - new_list_size = max node_idx + existing_list_size reconstructed_list = None _ range new_list_size Copy over existing elements present indexed_value tuples i val enumerate tuples indexed_value reconstructed_list i = val Populate value represented node reconstructed_list node_idx = node tuples indexed_value = tuple reconstructed_list Keep track nodes dominate loss node We will only emit backward operations nodes can contribute specified loss value live_nodes = loss_node None val_to_grad dict fx Node Optional fx Node = loss_node None assign_or_accumulate_grad forward_node grad_value forward_node val_to_grad forward_node op = placeholder grad_value = g call_function _null_coalesce_accumulate val_to_grad forward_node grad_value val_to_grad forward_node = grad_value g inserting_before output_node node reversed g nodes node live_nodes continue add_to_live_nodes n live_nodes setdefault n None fx node map_arg node args add_to_live_nodes fx node map_arg node kwargs add_to_live_nodes node op == call_module output_grads Union tuple Optional fx Node Optional fx Node node tuples stage_output = tuples node output_grads = tuple val_to_grad get n n tuples node outputs_with_grads_idxs = i i n enumerate tuples node n live_nodes stage_output = node output_grads = val_to_grad node outputs_with_grads_idxs = output_grads = output_grads isinstance output_grads tuple output_grads grad_call = g call_function stage_backward kwargs= stage_output stage_output output_grads output_grads input_values list node all_input_nodes outputs_with_grads_idxs outputs_with_grads_idxs Insert backward stage debug info kwargs_copy = dict grad_call kwargs grad_call kwargs = kwargs_copy grad_call_proxy = fx Proxy grad_call grads = grad_call_proxy node input_nodes = list node all_input_nodes grads_proxy = fx Proxy grads i input_node enumerate input_nodes assign_or_accumulate_grad input_node grads_proxy i node type ignore index g PipeSequential torch nn Sequential staticmethod from_sequential sequential_instance torch nn Sequential PipeSequential copy copy m m sequential_instance forward input i module enumerate input = module input i = len - pipe_split input LossWrapper torch nn Module LossWrapper convenient abstract allows you wrap up both your model well its loss function specify connectivity between inputs model loss function output value Example MyModelWrapper LossWrapper forward x targets model_out = module x loss_value = loss_fn model_out targets loss_value The above example defines connectivity where we expect forward loss backward training procedure take two arguments x targets pass x into module get output feedforward computation pass model output targets value into loss function get loss value which will backpropagated PiPPy The above would then instantiated like model = instantiate model loss_fn = torch nn MSELoss sake demonstration wrapper = MyModelWrapper model loss_fn pipe = Pipe from_tracing wrapper __init__ module loss_fn super __init__ module = module loss_fn = loss_fn forward args kwargs raise NotImplementedError This instance LossWrapper does have overridden forward Please implement forward specify arguments connection between module loss loss output value TrivialLossWrapper LossWrapper pyrefly ignore bad-override forward x targets model_out = module x loss_fn model_out targets loss_spec = True Pipe model representation Pipe can thought ` nn Sequential++ ` That say specifies single topological ordering pipeline stages when run series constitutes all operations program However unlike ` nn Sequential ` Pipe allows non-local usages values so long those uses still respect topological ordering In particular Non-local activations This type usage can appear example skip connections These values will directly transmitted stage all stages use them skipping intermediate stages During autograd gradients will propagated back through skip connection reverse how activations propagated forward pass Non-local parameter module invocations This occurs when parameter used stage downstream where resident These values can carried forward similarly addition one might want replicate value multiple stages Gradients these shared parameters will accumulated separately each stage there will additional gradient accumulation before optimizer step Register ` _pipe_split ` ATen operator This required Export preserve marker graph torch library define pippy _pipe_split - torch library impl pippy _pipe_split BackendSelect _pipe_split None torch library register_fake pippy _pipe_split type ignore no-redef _pipe_split noqa F None Add alias convenience aten_pipe_split_alias = torch ops pippy _pipe_split default Ask Export preserve ` _pipe_split ` op See examples pytorch torch fx node py fx node _side_effectful_functions add aten_pipe_split_alias User facing API pipe_split pipe_split special operator used mark boundary between stages module It used split module into stages It no-op your annotated module run eagerly Example xdoctest +SKIP forward x x = torch mm x mm_param x = torch relu x pipe_split x = lin x x The above example will split into two stages torch ops pippy _pipe_split MultiUseParameterConfig Enum TRANSMIT = REPLICATE = MultiUseParamSpec = Union MultiUseParameterConfig dict str MultiUseParameterConfig DetachExecutor fx Interpreter Special interpreter run split_gm testing detaches all inputs module invocation This needed so values boundary leaf modules autograd execution __init__ module garbage_collect_values=True garbage_collect_values = False super __init__ module garbage_collect_values value_remap = run args initial_env=None type ignore override value_remap = super run args initial_env=initial_env call_module target args kwargs detach_tensors isinstance torch Tensor requires_grad value_remap new_val = detach requires_grad_ True value_remap = new_val value_remap dont_traverse_size type torch Size args = map_aggregate args detach_tensors dont_traverse_size kwargs = map_aggregate kwargs detach_tensors dont_traverse_size super call_module target args kwargs call_function target args kwargs HACK reroute saved input tensors point detach ed version target stage_backward kwargs = dict kwargs kwargs input_values = value_remap get v v v kwargs input_values super call_function target args kwargs _NodeReference __init__ name name = name name str _LinearNodeList __init__ node_list serialize_node_list = node node_list node_args = fx node map_arg node args lambda n _NodeReference n name type ignore arg-type return-value node_kwargs = fx node map_arg node kwargs lambda n _NodeReference n name type ignore arg-type return-value serialize_node = fx Node graph=None type ignore arg-type name=node name op=node op target=node target args=node_args type ignore arg-type kwargs=node_kwargs type ignore arg-type return_type=node type serialize_node meta = copy copy node meta serialize_node_list append serialize_node to_graph graph = fx Graph ref_str_to_node dict str fx Node = ref_to_node arg isinstance arg _NodeReference ref_str_to_node arg name arg node serialize_node_list node_args = map_aggregate node args ref_to_node node_kwargs = map_aggregate node kwargs ref_to_node deser_node = graph create_node op=node op target=node target args=node_args type ignore arg-type kwargs=node_kwargs type ignore arg-type name=node name type_expr=node type ref_str_to_node node name = deser_node graph _direct_serialization_deserialize body nodes Custom ` __reduce__ ` method serialization DO AS I SAY -- NOT AS I DO This violates principle GraphModules serialize via code export re-tracing We allow here because PIPE STAGES SHOULD NOT BE PERSISTED TO DISK -- THIS IS ONLY FOR TRANSMISSION VIA RPC Persisting these instances disk will expose internal implementation details ` fx Graph ` related data structures NOT advised DummyModule torch nn Module __init__ body super __init__ __dict__ update body dummy = DummyModule body fx GraphModule dummy nodes to_graph _direct_serialization_reduce serialization_dict = dict __dict__ serialization_dict pop _graph _direct_serialization_deserialize serialization_dict _LinearNodeList graph nodes _modify_graph_op_device gm torch fx GraphModule new_device torch device Modify device argument all call_function nodes graph This useful moving graph different device In particular generator ops like torch ones modified = False node gm graph nodes node op == call_function device node kwargs node kwargs device = new_device logger debug f Changing device Node node name node kwargs device new_device noqa G node update_kwarg device new_device modified = True node op == call_module Recursively modify device submodules submod = gm get_submodule node target isinstance submod torch fx GraphModule _modify_graph_op_device submod new_device isinstance submod InterpreterModule If unflattening has been performed we need access its graph module ` graph_module ` _modify_graph_op_device submod graph_module new_device type ignore arg-type logger warning f Skipping device modification submodule node target because type submod noqa G modified gm recompile Pipe torch nn Module __init__ split_gm fx GraphModule num_stages int has_loss_and_backward bool loss_spec TODO there way hard wire init torch nn Module __init__ split_gm fx GraphModule = split_gm executor DetachExecutor = DetachExecutor split_gm num_stages int = num_stages has_loss_and_backward = has_loss_and_backward loss_spec = loss_spec node split_gm graph nodes assert node op call_module placeholder output node op node target == call_function operator getitem node op node target == call_method backward node op node target == call_function stage_backward node op node target == call_function _null_coalesce_accumulate node Detect replicated parameters so we know we have do additional allreduce before applying optimizer Note also handles case where there multiple calls single module different stages regardless whether module invocation handled logic above Map parameter value dictionary maps user pipeline module local qualname within module params_to_users dict torch nn Parameter dict str str = m_qualname mod split_gm named_children p_qualname param mod named_parameters params_to_users setdefault param params_to_users param m_qualname = p_qualname replicated_params list dict str str = use_mapping _ use_mapping params_to_users items len use_mapping We must break aliasing relationship between replicated parameters correct numerics reference runs If we do do autograd tape separate stages will have reference same tensor value will erroneously apply gradient updates multiple times Therefore each replicated parameter set we deepcopy values so we have separate instances param_mapping replicated_params submod_name param_qualname param_mapping items submod = getattr split_gm submod_name atoms = param_qualname split atom atoms - submod = getattr submod atom setattr submod atoms - copy deepcopy getattr submod atoms - throw args kwargs raise RuntimeError To run pipeline locally invoke Pipe object directly ` split_gm ` split_gm forward = throw Make submodules use custom direct-serialized GraphModule i = while True try name = get_submod_name i submod = getattr split_gm name submod __class__ __reduce__ = _direct_serialization_reduce i += except AttributeError break forward args kwargs executor_args = args len kwargs parameters = node split_gm graph nodes node op == placeholder node args len node args parameters append Parameter node target Parameter POSITIONAL_OR_KEYWORD default=node args parameter_kind = Parameter POSITIONAL_OR_KEYWORD param_name = node target node target startswith parameter_kind = Parameter VAR_KEYWORD type ignore assignment param_name = param_name node target startswith parameter_kind = Parameter VAR_POSITIONAL type ignore assignment param_name = param_name parameters append Parameter param_name parameter_kind signature = Signature parameters ba = signature bind args kwargs ba apply_defaults executor_args = ba arguments values type ignore assignment res = executor run executor_args res get_stage_module stage_idx int - torch nn Module Return stage module corresponding ` stage_idx ` ` pipe ` stage_idx stage_idx = num_stages raise ValueError f Invalid stage index stage_idx submod_name = get_submod_name stage_idx getattr split_gm submod_name staticmethod _number_and_count_forward_stages gm fx GraphModule num_stages = found_idxs dict int None = node gm graph nodes node op == call_module node target startswith PP_SUBMOD_PREFIX node meta stage_idx = int node target len PP_SUBMOD_PREFIX + found_idxs setdefault node meta stage_idx num_stages += assert will fail split point inserted before first layer which creates empty first submodule Update following assert may fail against some torch versions = submod_ submod_ submod_ may named submod_ submod_ submod_ TODO investigate assert all i found_idxs i range num_stages num_stages staticmethod _from_traced mod torch nn Module exported_program ExportedProgram multi_use_param_spec Optional MultiUseParamSpec = None output_loss_value_spec=None split_policy Optional Callable torch fx GraphModule torch fx GraphModule = None Additionally ` ` output_loss_value_spec ` ` value can specified disambiguate which value output ` forward ` loss value which PiPPy should apply backpropagation For example your ` ` forward ` ` returns tuple ` ` loss model_out ` ` you can specify ` ` output_loss_value_spec= True False ` ` Or your ` ` forward ` ` returns dict ` ` loss loss_value model_out model_out ` ` you can specify ` ` output_loss_value_spec= loss True model_out False ` ` traced = exported_program module check_guards=False split_policy None logger info Auto-splitting model traced = split_policy traced type ignore arg-type logger debug traced print_readable print_output=False type ignore operator Deduplicate ` get_attr ` nodes refer same parameter Downstream code moving parameters relies invariant parameter accesses happen once This necessarily case especially custom tracers so fix up here get_attr_nodes dict str fx Node = node traced graph nodes type ignore union-attr node op == get_attr get_attr_nodes setdefault node target node get_attr_nodes node target = node node replace_all_uses_with get_attr_nodes node target traced graph erase_node node type ignore operator union-attr avoid looking next node keeping track previous pipe_split prev_pipe_split_idx = - pipe_split_nodes_to_erase = set i node enumerate traced graph nodes type ignore arg-type union-attr node op node target == call_function pipe_split prev_pipe_split_idx == i - pipe_split_nodes_to_erase add node prev_pipe_split_idx = i node pipe_split_nodes_to_erase traced graph erase_node node type ignore operator union-attr traced recompile type ignore operator part_idx = split_callback n fx Node nonlocal part_idx n op n target == call_function aten_pipe_split_alias logger debug f Found pipe_split part_idx noqa G part_idx += part_idx TODO what does split do module invocations does move modules into submodules split = split_module traced mod split_callback partition_affix= pp type ignore arg-type custom tracer can produce dead code like orphan get_attr nodes split graph eliminate_dead_code peephole remove pipe_split submodule split modules isinstance submodule fx GraphModule node submodule graph nodes node op node target == call_function aten_pipe_split_alias submodule graph erase_node node submodule recompile name submodule split named_children isinstance submodule fx GraphModule new_submod = _outline_submodules submodule graph Replace old submod split register_module name new_submod TODO backport into split_module delete_user_reference node user Delete reference ` node ` ` user ` s arg list Args - node ` get_attr ` node root - user submodule node uses ` node ` assert len user kwargs == use_idxs = i i arg enumerate user args arg == node assert len use_idxs == args_copy = list user args args_copy pop use_idxs user args = tuple args_copy logger debug f Deleted node user user arg index = use_idxs noqa G A list param referrals deferred deletion To accumulated ` move_param_to_callee ` to_delete = _recursive_getattr_with_parent mod fqn Returns getattr call given nested FQN last parent atoms = fqn split atom atoms - hasattr mod atom None None mod = getattr mod atom hasattr mod atoms - mod None attr = getattr mod atoms - mod attr move_param_to_callee root callee_name param_fqn Move parameter root module submodule Args root The root module callee_name The name submodule move parameter param_fqn The fully qualified name parameter move ` atoms ` list strings representing path parameter original model atoms = param_fqn split mod_itr param_val = _recursive_getattr_with_parent split param_fqn Check whether parameter buffer parameter is_buffer = atoms - mod_itr _buffers Check whether parameter tensor assert isinstance param_val torch Tensor f Expected param_fqn torch Tensor got type param_val + f It might happen module param_fqn passed some leaf function f see https pytorch org docs stable fx html#fx wrap Please inspect f usages param_fqn traced graph isinstance param_val torch nn Module Get submodule callee = root get_submodule callee_name assert hasattr callee param_fqn f Module callee_name already has parameter named param_fqn Assign parameter submodule is_buffer _assign_attr param_val callee param_fqn attr_kind=_AttrKind BUFFER persistent=True TODO handle non-persistent buffer _assign_attr param_val callee param_fqn attr_kind=_AttrKind PARAMETER logger debug f Moved parameter param_fqn callee_name noqa G Next step replace placeholder submodule get_attr Those placeholders created ` split_module ` inside each submodule Update step now moved ` _sink_params ` because ` _sink_params ` can do recursively i e modules inside submodule to_delete append mod_itr atoms - Get list all parameters root module attr_nodes = list filter lambda n n op == get_attr split graph nodes node attr_nodes Check whether parameter used only one submodule len node users logger info f Parameter node target used multiple stages node users noqa G user node users assert user op == call_module Move parameter into submodule move_param_to_callee split user target node target aliasing store tensor id - list FQNs built state dict Also assign non-persistent buffers id_to_fqns dict int set str = defaultdict set fqn tensor mod state_dict keep_vars=True items id_to_fqns id tensor add fqn fqn tensor mod named_buffers id_to_fqns id tensor add fqn After moving params their corresponding hierarchies we also need move ` get_attr ` nodes root graph those hierarchies aliasing use id - fqn mapping list out all valid FQNs inputs_to_state dict str list str = attr attr_nodes _ tensor = _recursive_getattr_with_parent mod attr target fqns = list id_to_fqns id tensor fqns inputs_to_state attr name = fqns attr target exported_program constants lifted constants inputs_to_state attr name = attr target aliasing each submodule split assign attributes FQNs may used We determine based whether FQN attribute parent exists i e last submodule exists assign attribute added_attributes dict str list str = defaultdict list fqn tensor mod state_dict keep_vars=True items name submod split named_children isinstance submod fx GraphModule parent child = _recursive_getattr_with_parent submod fqn parent child None parent exists attribute doesn t - assign added_attributes name append fqn setattr parent fqn split - tensor Deferral deletion Remove original attributes params root GraphModule mod_itr last_atom to_delete try delattr mod_itr last_atom except AttributeError This expected parameter used multiple stages pass This done ` _sink_params ` each submodule submod split children isinstance submod fx GraphModule _sink_params submod inputs_to_state submod graph lint submod recompile aliasing This step super necessary helps reduce parameter usage memory After _sink_params routine has run clean up unused attributes we previously added Determine based get_attr nodes - used remove name attributes added_attributes items submod = getattr split name unused_attributes = set attributes track used attributes submodule running DFS subgraph hierarchy stack = submod scope submodule while stack scope _mod = stack pop isinstance _mod fx GraphModule InterpreterModule node _mod graph nodes node op == get_attr get_attr might get access deeper level attribute fqn = scope + + node target scope node target unused_attributes discard fqn _name _submod _mod named_children stack append scope + + _name scope _name _submod delete unused attributes attr unused_attributes mod_itr atoms = submod attr split atom atoms - mod_itr = getattr mod_itr atom delattr mod_itr atoms - node attr_nodes And remove ` get_attr ` node submod s arg list user copy copy node users assert user op == call_module delete_user_reference node user And remove ` get_attr ` node root graph split graph erase_node node split delete_all_unused_submodules split graph lint split recompile num_stages = Pipe _number_and_count_forward_stages split has_loss_and_backward = False generated_loss_spec = output_loss_value_spec output_loss_value_spec None loss_node output_node generated_loss_spec = _find_loss_output mod split graph output_loss_value_spec loss_node None _insert_stage_symbolic_backward split graph loss_node output_node split recompile has_loss_and_backward = True logger debug Pipeline training mode backward pass generated raise RuntimeError f Did find any loss value according output_loss_value_spec= logger debug Pipeline inference mode backward pass generated logger debug f Full pipe model \n split noqa G Pipe split num_stages has_loss_and_backward generated_loss_spec print_readable Print pipe human-readable format This will print both root pipe each stage module split_gm print_readable staticmethod _trace_with_export mod torch nn Module example_args tuple Any example_kwargs Optional dict str Any = None - ExportedProgram logger info Tracing model try ep = torch export export mod example_args example_kwargs except Exception e raise RuntimeError It seems we cannot capture your model full graph Typical reasons include graph breaks data shape-dependent control flow missing meta kernels custom operators You can use our manual pipeline interfaces try fix graph breaks see https pytorch org docs stable export html e ep staticmethod from_tracing mod torch nn Module example_args tuple Any example_kwargs Optional dict str Any = None split_policy Optional Callable fx GraphModule fx GraphModule = None If param will used multiple pipeline stages we default strategy REPLICATE ing param across stages instead TRANSMIT ting multi_use_param_spec = MultiUseParameterConfig REPLICATE Figure out which output loss output_chunk_spec output_loss_value_spec Any = None Deprecated output_chunk_spec None output_loss_value_spec = map_aggregate output_chunk_spec lambda v isinstance v _LossReducer Trace export exported_program = Pipe _trace_with_export mod example_args example_kwargs pipe = Pipe _from_traced mod exported_program multi_use_param_spec output_loss_value_spec=output_loss_value_spec split_policy=split_policy Users want first pipeline stage accept kwargs original program does This controlled ` _codegen ` field graph so we make copy here Note we only want input spec output spec because output spec last stage Maybe TODO Not sure yet split = pipe split_gm traced = exported_program module submod = next iter split children submod _sign = signature submod forward model_sign = signature traced forward len model_sign parameters = len submod _sign parameters We don t change signature first stage takes different number args than original model logger info f Original model takes len model_sign parameters args noqa G f first pipeline stage takes len submod _sign parameters Please provide args respective pipeline stages Support kwargs first stage submod graph _codegen = copy deepcopy traced graph _codegen type ignore union-attr ` _replace ` actually private internal based doc To prevent conflicts field names method attribute names start underscore submod graph _codegen pytree_info = type ignore union-attr submod graph _codegen pytree_info _replace out_spec=None type ignore operator union-attr submod recompile pipe __str__ split_gm __str__ __repr__ split_gm __repr__ info - PipeInfo Get information about pipe Returns ------- PipeInfo A dataclass containing information about pipe PipeInfo graph=self split_gm graph num_stages=self num_stages has_loss_and_backward=self has_loss_and_backward build_stage stage_index int device torch device group Optional ProcessGroup = None - _PipelineStage Create ` PipelineStage ` given stage index distributed group The ` PipelineStage ` can run ` PipelineSchedule ` s Find stage module stage_module = get_stage_module stage_index Move ops argument device Today PT tracer does treat ` x device ` symbolic device instead device tracing time got burned into generated code Here we provide workaround users manually modify device kwarg operations Such operation may include ` torch ones ` ` torch zeros ` ` torch rand ` etc isinstance stage_module torch fx GraphModule _modify_graph_op_device stage_module device logger warning f Expected ` torch fx GraphModule ` got type stage_module noqa G Detach pipe info Note careful what s included ` pipe_info ` We don t want keep reference ` Pipe ` ` Pipe split_gm ` which stops python recycling them When python recycles them other stage modules which irrelevant current rank can automatically freed pipe_info = info _PipelineStage stage_module stage_index pipe_info device group SplitPoint Enum Enum representing points which split can occur execution submodule Attributes BEGINNING Represents adding split point before execution certain submodule ` forward ` function END Represents adding split point after execution certain submodule ` forward ` function BEGINNING = END = For backward compatibility we kept PipeSplitWrapper because ` SplitPoint ` used defined PipeSplitWrapper Create alias BC SplitPoint = SplitPoint _split_before_forward args kwargs pipe_split _orig_forward args kwargs _split_after_forward args kwargs try _orig_forward args kwargs finally pipe_split annotate_split_points mod torch nn Module spec dict str SplitPoint TODO make implementation out-of-place qualname split_type spec items atoms = qualname split predecessor_module = mod i atom enumerate atoms - try predecessor_module = getattr predecessor_module atom except AttributeError e raise AttributeError f Specified target qualname referenced f nonexistent module join atoms i + e mod_to_wrap = getattr predecessor_module atoms - mod_to_wrap _orig_forward = mod_to_wrap forward split_type == SplitPoint BEGINNING mod_to_wrap forward = MethodType _split_before_forward mod_to_wrap split_type == SplitPoint END mod_to_wrap forward = MethodType _split_after_forward mod_to_wrap raise ValueError Unknown split point type pipeline module torch nn Module mb_args tuple Any mb_kwargs Optional dict str Any = None split_spec Optional dict str SplitPoint = None split_policy Optional Callable fx GraphModule fx GraphModule = None - Pipe Split module based specification See ` Pipe ` more details Arguments --------- module The module split mb_args Example positional inputs micro-batch form mb_kwargs Example keyword inputs micro-batch form default ` None ` split_spec A dictionary using submodule names split marker default ` None ` split_policy The policy use splitting module default ` None ` Returns ------- A pipeline representation ` Pipe ` split_spec None split_policy None raise ValueError Cannot specify both ` split_spec ` ` split_policy ` Please use only one them split_spec None Annotate split points module based user spec annotate_split_points module split_spec Pipe from_tracing mod=module example_args=mb_args example_kwargs=mb_kwargs Use split policy Pipe from_tracing mod=module example_args=mb_args example_kwargs=mb_kwargs split_policy=split_policy