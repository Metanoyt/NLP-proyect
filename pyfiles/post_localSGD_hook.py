mypy allow-untyped-defs logging torch torch distributed dist default_hooks default logger = logging getLogger __name__ PostLocalSGDState r Store state all-reducing gradients globally until given step then locally after Stores state all-reducing gradients globally using ` ` process_group ` ` until step ` ` start_localSGD_iter ` ` all-reducing gradients locally using ` ` subgroup ` ` afterwards If ` ` process_group ` ` ` ` None ` ` global process group will used If ` ` subgroup ` ` ` ` None ` ` intra-node process group each machine will used Additionally ` ` post_local_gradient_allreduce ` ` may worth tuning because both true false may give faster convergence __slots__ = process_group subgroup start_localSGD_iter post_local_gradient_allreduce iter __init__ process_group subgroup start_localSGD_iter post_local_gradient_allreduce=True Initialize state object given parameters log when localSGD start logger info Local SGD will started after s iterations start_localSGD_iter The group used all-reducing gradients globally process_group = process_group The group used all-reducing gradients locally subgroup = subgroup start_localSGD_iter = start_localSGD_iter Allreduce gradients locally since iteration ` start_localSGD_iter ` This may help convergence efficiency cost relatively cheap intra-subgroup communication post_local_gradient_allreduce = post_local_gradient_allreduce Iteration step training loop iter = maybe_increase_iter bucket Track iterations trigger log message start local SGD Since bucket last bucket allreduce iteration Only increase ` iter ` when bucket processed bucket is_last iter += iter == start_localSGD_iter logger info Start apply local SGD after s iterations iter post_localSGD_hook state PostLocalSGDState bucket dist GradBucket - torch futures Future torch Tensor Run post-localSGD algorithm This DDP communication hook used running post-localSGD algorithm combining model averaging component e g ` ~torch distributed algorithms model_averaging averagers PeriodicModelAverager ` runs after optimizer step Args state PostLocalSGDState State information run post-localSGD Users mainly need tune ` ` start_localSGD_iter ` ` determine when start local SGD bucket dist GradBucket Bucket stores D flattened gradient tensor batches multiple per-variable tensors Note since DDP comm hook only supports single process single device mode only exactly one tensor stored bucket Returns Future handler communication which updates gradients place Example xdoctest +SKIP state = PostLocalSGDState process_group=process_group subgroup=subgroup start_localSGD_iter= ddp_model register_comm_hook state post_localSGD_hook Also need establish model averaging module run model averaging after ` ` optimizer step ` ` Please refer examples ` ` torch distributed algorithms model_averaging averagers ` ` module global_group_to_use = state process_group state process_group None dist group WORLD The input tensor flattened D tensor input_tensor = bucket buffer Run allreduce using ` global_group_to_use ` first ` start_localSGD_iter ` iterations state iter state start_localSGD_iter state maybe_increase_iter bucket default _allreduce_fut global_group_to_use input_tensor type ignore arg-type If ` post_local_gradient_allreduce ` set then no gradient synchronization after first ` start_localSGD_iter ` iterations state post_local_gradient_allreduce fut torch futures Future torch Tensor = torch futures Future fut set_result input_tensor fut Run allreduce using ` subgroup ` after first ` start_localSGD_iter ` iterations Note default separate subgroup each node created which causes intra-node allreduce done each training step From moment model averaging should run after optimizer step globally allreduce all parameters state subgroup None state subgroup _ = dist new_subgroups default _allreduce_fut state subgroup input_tensor