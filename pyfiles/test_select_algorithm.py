Owner s module inductor contextlib functools unittest mock collections abc Callable typing Any Optional Union unittest mock patch torch torch _dynamo config dynamo_config torch _inductor config inductor_config torch _inductor select_algorithm select_algorithm torch nn functional F torch _dynamo testing expectedFailureDynamicWrapper torch _dynamo utils counters torch _inductor config torch _inductor autotune_process TritonBenchmarkRequest torch _inductor choices InductorChoices torch _inductor codegen common KernelTemplate torch _inductor ir FixedLayout torch _inductor kernel_inputs KernelInputs torch _inductor select_algorithm autotune_select_algorithm ExternKernelChoice TritonTemplate TritonTemplateKernel torch _inductor test_case run_tests TestCase torch _inductor utils is_big_gpu run_and_get_kernels torch _inductor virtualized V torch _prims_common ELEMENTWISE_TYPE_PROMOTION_KIND torch testing _internal common_utils IS_LINUX skipIfRocm skipIfXpu torch testing _internal inductor_utils GPU_TYPE HAS_GPU requires_gpu requires_triton aten = torch ops aten patches fn skip_cache choices name key benchmark hint_override=None benchmark None benchmark choices patcher dynamo_config patch verbose=True inductor_config patch debug=True max_autotune=True epilogue_fusion=True patch object select_algorithm VERIFY dict atol= e- rtol= e- patch object select_algorithm AlgorithmSelectorCache lookup skip_cache torch backends cudnn flags allow_tf =False fn = patcher fn functools wraps fn wrapped args kwargs counters clear torch manual_seed assert torch backends cuda matmul allow_tf correctness testing allergic tf fn args kwargs wrapped TestSelectAlgorithm TestCase setUp super setUp is_big_gpu skipTest Need big GPU run max_autotune=True Clear preprocessing functions ensure clean state select_algorithm clear_preprocessing_fns patches test_linear_relu torch compile foo input weight bias F relu F linear input weight bias foo torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch randn device=GPU_TYPE Autotuning checks correctness each version assertEqual counters inductor select_algorithm_autotune It would nice assert got fused into single kernel only happens we select triton template aten patches test_addmm torch compile foo input weight bias torch addmm bias input weight inps = torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch randn device=GPU_TYPE foo inps assertEqual counters inductor select_algorithm_autotune patches test_preprocessing_single_choice pass list preprocessing function assert actually called func_called = False Register preprocessing function returns only first choice This turn will lead autotuning being skipped s single choice counter itself will bumped return_first_choice_only choices func_called = True choices choices select_algorithm add_preprocessing_fn return_first_choice_only torch compile foo input weight bias torch addmm bias input weight inps = torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch randn device=GPU_TYPE foo inps Since we only have one choice autotuning should skipped assertEqual counters inductor select_algorithm_autotune The preprocessing function should have been called assertTrue func_called patch object select_algorithm VERIFY dict atol= e- rtol= e- patches test_addmm_fp torch compile foo input weight bias torch addmm bias input weight inps = torch randn device=GPU_TYPE dtype=torch half torch randn device=GPU_TYPE dtype=torch half t torch empty device=GPU_TYPE dtype=torch half foo inps Autotuning checks correctness each version assertEqual counters inductor select_algorithm_autotune patches test_mm torch compile foo b torch mm b foo torch randn device=GPU_TYPE torch randn device=GPU_TYPE assertEqual counters inductor select_algorithm_autotune patches test__int_mm torch compile foo b torch _int_mm b foo torch randint - device=GPU_TYPE dtype=torch int torch randint - device=GPU_TYPE dtype=torch int assertEqual counters inductor select_algorithm_autotune patches skipIfXpu msg= Double datatype matmul supported oneDNN test_mm_skip torch compile foo b torch mm b foo torch randn device=GPU_TYPE dtype=torch float torch randn device=GPU_TYPE dtype=torch float float supported tl dot assertEqual counters inductor select_algorithm_autotune patches test_bmm torch compile foo b torch bmm b foo torch randn device=GPU_TYPE torch randn device=GPU_TYPE Autotuning checks correctness each version assertEqual counters inductor select_algorithm_autotune patches test_mm_not_even_k torch compile foo b torch mm b foo torch randn device=GPU_TYPE torch randn device=GPU_TYPE assertEqual counters inductor select_algorithm_autotune patches test_baddbmm torch compile foo b c torch baddbmm c b foo torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch randn device=GPU_TYPE Autotuning checks correctness each version assertEqual counters inductor select_algorithm_autotune patches test_mm_plus_mm torch compile foo b c d b + c d foo torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch randn device=GPU_TYPE Autotuning checks correctness each version assertEqual counters inductor select_algorithm_autotune TODO fix accuracy failure triton template XPU enable test case skipIfXpu patches test_mm_plus_mm torch compile foo b c d b + c d foo torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch randn device=GPU_TYPE Autotuning checks correctness each version assertEqual counters inductor select_algorithm_autotune expectedFailureDynamicWrapper patches test_mm_plus_mm torch compile foo b c d b + c d foo torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch randn device=GPU_TYPE Autotuning checks correctness each version assertEqual counters inductor select_algorithm_autotune patches test_mm_dup_args torch compile foo torch mm foo torch randn device=GPU_TYPE assertEqual counters inductor select_algorithm_autotune patches test_mm_dup_args_view torch compile foo q = k = torch mm q k transpose foo torch randn device=GPU_TYPE assertEqual counters inductor select_algorithm_autotune expectedFailureDynamicWrapper patches test_convolution torch compile foo x w b aten convolution x + w b stride= padding= dilation= transposed=False output_padding= groups= foo torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch randn device=GPU_TYPE Autotuning checks correctness each version assertEqual counters inductor select_algorithm_autotune skipIfRocm patches test_mm_dropout torch compile fn x x seed mm_ = torch ops aten mm default x x rnd = torch ops prims inductor_random default mm_ shape seed rand mm_ rnd GPU_TYPE == xpu patcher = patch object select_algorithm VERIFY dict atol= e- rtol= e- fn = patcher fn sizes picked so triton autotuning wins fn torch randn dtype=torch float device=GPU_TYPE torch randn dtype=torch float device=GPU_TYPE torch tensor device=GPU_TYPE assertEqual counters inductor select_algorithm_autotune patches torch _inductor config patch conv_ x _as_mm=False test_convolution torch compile foo x w b aten convolution x w b stride= padding= dilation= transposed=False output_padding= groups= foo torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch randn device=GPU_TYPE Autotuning checks correctness each version assertEqual counters inductor select_algorithm_autotune patches torch _inductor config patch conv_ x _as_mm=True test_convolution_as_mm torch compile foo x w b aten convolution x + w b stride= padding= dilation= transposed=False output_padding= groups= foo torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch randn device=GPU_TYPE Autotuning checks correctness each version assertEqual counters inductor select_algorithm_autotune patches torch _inductor config patch conv_ x _as_mm True max_autotune_gemm_backends TRITON test_convolution_as_mm_triton_only To convert x conv matmul x converted channels last tensor channels dimension permuted innermost This prologue should fused matmul since prologue writes discontiguously whilst mm template currently only supports reading input contiguously Before change associated PR fusion would occur because actual kernel input nodes which don t include views e g permute would passed ` TritonTemplateCaller ` rather than input nodes include views For example after x converted channels last its layout shape stride prologue writes value discontiguously After permute mm template fixes layout reads input contiguously If kernel input node x passed ` TritonTemplateCaller ` then scheduler will fuse prologue since write compatible read If however viewed input passed ` TritonTemplateCaller ` then write won t compatible read prologue won t fused foo x w b aten convolution x + w b stride= padding= dilation= transposed=False output_padding= groups= x = torch randn device=GPU_TYPE w = torch randn device=GPU_TYPE b = torch randn device=GPU_TYPE SingleMMConfigChoice InductorChoices get_template_configs kernel_inputs KernelInputs templates list Union KernelTemplate ExternKernelChoice op_name str kwarg_overrides Optional dict str dict str Any = None super get_template_configs kernel_inputs templates op_name kwarg_overrides V set_choices_handler SingleMMConfigChoice result_compile = torch compile foo x w b result_eager = foo x w b If prologue has been fused should fail torch testing assert_close result_compile result_eager There should any autotuning assertEqual counters inductor select_algorithm_autotune patches torch _inductor config patch conv_ x _as_mm=False test_convolution _group torch compile foo x w b aten convolution x w b stride= padding= dilation= transposed=False output_padding= groups= group foo torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch randn device=GPU_TYPE Autotuning checks correctness each version assertEqual counters inductor select_algorithm_autotune test_TritonTemplateCaller_str Make sure str TritonTemplateCaller does raise exceptions module_path = abc py bmreq = TritonBenchmarkRequest module_path=module_path module_cache_key=None kernel_name=None extra_args=None num_stages=None num_warps=None num_consumer_groups=None num_buffers_warp_spec=None input_tensor_meta=None output_tensor_meta=None caller = select_algorithm TritonTemplateCaller None None None None extra bmreq caller_str = str caller assertEqual caller_str f TritonTemplateCaller module_path extra contextlib contextmanager patch_lowering lowering_overrides - Callable None torch _inductor lowering inductor_lowering unittest mock patch dict inductor_lowering lowerings fn decomp_fn broadcast type_promotion_kind convert_input_to_bool lowering_overrides items inductor_lowering _register_lowering fn decomp_fn broadcast=broadcast type_promotion_kind=type_promotion_kind convert_input_to_bool=convert_input_to_bool lowering_dict=inductor_lowering lowerings yield TestTemplateRender TestCase requires_gpu requires_triton config patch cuda_backend= triton test_finalized_subclass_hooks Tests all registered triton template hooks have been finalized especially case hooks finalized manually caller i e calling template finalize_hook hook_name hook_identifier = CUSTOM_HOOK ExtensionTritonTemplateKernel TritonTemplateKernel __init__ args kwargs super __init__ args kwargs _register_extra_template_env_fns custom_hook custom_hook - str Custom hook just returns test string validation hook - str hook_identifier _register_hook CUSTOM_HOOK hook inductor_meta_common super inductor_meta_common ExtensionTritonTemplate TritonTemplate kernel_type = ExtensionTritonTemplateKernel add_template = ExtensionTritonTemplate name= add grid=lambda args kwargs source= r def_kernel A B custom_hook xoffset = tl program_id xindex = xoffset + tl arange XBLOCK xmask = tl full XBLOCK True tl int tmp = tl load A + xindex tmp = tl load B + xindex tmp = tmp + tmp store_output xindex tmp mask= xmask val_shape= XBLOCK XBLOCK = add_override b alpha=None layout = FixedLayout get_device get_dtype get_size choices = add_template maybe_append_choice choices input_nodes= b layout=layout num_stages= num_warps= XBLOCK=XBLOCK autotune_select_algorithm add choices b layout patch_lowering torch ops aten add Tensor add_override True ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT False torch compile add b + b = torch zeros XBLOCK device=GPU_TYPE b = torch zeros XBLOCK device=GPU_TYPE _result kernels = run_and_get_kernels add b assert len kernels == assert hook_identifier kernels __name__ == __main__ IS_LINUX HAS_GPU is_big_gpu run_tests