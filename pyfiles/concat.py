numpy np torch benchmark Concat D InputBench benchmark Benchmark __init__ mode device dtype I _D I _D I _D I _D concat_dim super __init__ mode device dtype I _D = I _D I _D = I _D I _D = I _D I _D = I _D concat_dim = concat_dim input = randn I _D I _D device=device dtype=dtype requires_grad=self requires_grad input = randn I _D I _D device=device dtype=dtype requires_grad=self requires_grad inputs = input input forward input input x = add input x = add input y = cat x x dim=self concat_dim y reference np concatenate numpy input numpy input axis=self concat_dim config I _D I _D I _D I _D concat_dim staticmethod module concat d input memory_workload mode == fwd sol_count = + algorithmic_count = + sol_count = + + + algorithmic_count = + + + buffer_size = I _D I _D + I _D I _D sol buffer_size sol_count algorithmic buffer_size algorithmic_count staticmethod default_configs benchmark register_benchmark_class Concat D InputBench ConcatGraphOptBench benchmark Benchmark __init__ mode device dtype I _D I _D I _D I _D concat_dim super __init__ mode device dtype I _D = I _D I _D = I _D I _D = I _D I _D = I _D concat_dim = concat_dim input = randn I _D I _D device=device dtype=dtype requires_grad=self requires_grad input = randn I _D I _D device=device dtype=dtype requires_grad=self requires_grad inputs = input input torch _C _jit_override_can_fuse_on_cpu True torch _C _jit_cat_wo_conditionals True forward input input x = add input x = add input y = cat x x dim=self concat_dim z = relu y z reference np concatenate numpy input numpy input axis=self concat_dim config I _D I _D I _D I _D concat_dim staticmethod module concatGraphOpt memory_workload mode == fwd sol_count = + algorithmic_count = + sol_count = + + + algorithmic_count = + + + buffer_size = I _D I _D + I _D I _D sol buffer_size sol_count algorithmic buffer_size algorithmic_count staticmethod default_configs benchmark register_benchmark_class ConcatGraphOptBench