mypy ignore-errors r Importing file includes common utility methods checking quantized tensors modules numpy np torch torch Tensor contextlib contextmanager torch testing _internal common_utils TEST_WITH_TSAN IS_PPC IS_MACOS IS_WINDOWS supported_qengines = torch backends quantized supported_engines Note We currently do run QNNPACK tests WINDOWS MACOS flaky Issue QNNPACK supported PPC qnnpack supported_qengines any IS_PPC TEST_WITH_TSAN IS_MACOS IS_WINDOWS supported_qengines remove qnnpack _conv_output_shape input_size kernel_size padding stride dilation output_padding= Computes output shape given convolution parameters np floor input_size + padding - kernel_size - kernel_size - dilation - stride + output_padding + Quantization references _quantize x scale zero_point qmin=None qmax=None dtype=np uint Quantizes numpy array qmin None qmin = np iinfo dtype min qmax None qmax = np iinfo dtype max qx = np round x scale + zero_point astype np int qx = np clip qx qmin qmax qx = qx astype dtype qx _dequantize qx scale zero_point Dequantizes numpy array x = qx astype float - zero_point scale x _requantize x multiplier zero_point qmin= qmax= qtype=np uint Requantizes numpy array i e intermediate int int values converted back given type qx = x multiplier round + zero_point qx = np clip qx qmin qmax astype qtype qx _calculate_dynamic_qparams X dtype reduce_range=False qscheme=torch per_tensor_affine Calculate dynamic quantization parameters scale zero_point according min max element tensor assert qscheme torch per_tensor_affine torch per_tensor_symmetric qscheme == torch per_tensor_symmetric assert dtype == torch qint isinstance X torch Tensor X = X numpy dtype == torch qint reduce_range qmin qmax = - qmin qmax = - dtype == torch quint reduce_range qmin qmax = qmin qmax = min_val = X min max_val = X max is_symmetric = qscheme == torch per_tensor_symmetric min_val == max_val scale = zero_point = is_symmetric max_val = max max_val -min_val min_val = -max_val scale = max_val - min_val qmax - qmin scale = max scale np finfo np float eps zero_point = max_val = max max_val min_val = min min_val scale = max_val - min_val qmax - qmin scale = max scale np finfo np float eps zero_point = qmin - round min_val scale zero_point = max qmin zero_point zero_point = min qmax zero_point float scale int zero_point _calculate_dynamic_per_channel_qparams X dtype Calculate dynamic quantization parameters scale zero_point according min max element tensor isinstance X torch Tensor X = X numpy qmin qmax = torch iinfo dtype min torch iinfo dtype max n_levels = qmax - qmin scale = np zeros X shape dtype=np float zero_point = np zeros X shape dtype=np int i range zero_point shape min_val = X min max_val = X max min_val == max_val scale i = zero_point i = max_val = max max_val min_val = min min_val scale i = max_val - min_val n_levels scale i = max scale i np finfo np float eps zero_point i = qmin - round min_val scale i zero_point i = max qmin zero_point i zero_point i = min qmax zero_point i scale zero_point _snr x x_hat Calculates signal noise ratio returns signal noise power well SNR dB If input list tuple function called recursively each element The result will have same nested structure inputs Args x x_hat Either tensor nested list tuple tensors Returns signal noise SNR dB Either floats nested list floats isinstance x list tuple assert len x == len x_hat res = _snr x idx x_hat idx idx range len x res x_hat is_quantized x_hat = x_hat dequantize x is_quantized x = x dequantize noise = x - x_hat norm noise == float inf float inf signal = x norm snr = signal noise snr_db = snr log signal noise snr_db contextmanager override_quantized_engine qengine previous = torch backends quantized engine torch backends quantized engine = qengine try yield finally torch backends quantized engine = previous contextmanager override_cpu_allocator_for_qnnpack qengine_is_qnnpack try qengine_is_qnnpack torch _C _set_default_mobile_cpu_allocator yield finally qengine_is_qnnpack torch _C _unset_default_mobile_cpu_allocator TODO Update all quantization tests use decorator Currently some tests seems have inconsistent params fbgemm vs qnnpack override_qengines qfunction test_fn args kwargs qengine supported_qengines override_quantized_engine qengine qfunction should anything qfunction args kwargs test_fn qengine_is_fbgemm torch backends quantized engine == fbgemm qengine_is_qnnpack torch backends quantized engine == qnnpack qengine_is_onednn torch backends quantized engine == onednn qengine_is_x torch backends quantized engine == x Helper function used simulate per-channel fake-quant against any axis _permute_to_axis_zero X axis new_axis_list = list range X dim new_axis_list axis = new_axis_list = axis y = X permute tuple new_axis_list y new_axis_list Reference method fake quantize Note because scale zero_point left float actual kernel mimics how fake_quant works float _fake_quantize_per_channel_affine_reference X per_channel_scale per_channel_zero_point axis quant_min quant_max dtype = X dtype X permute_axis_list = _permute_to_axis_zero X torch float axis res = torch zeros_like X i range X size res i = torch clamp torch round X i per_channel_scale i + per_channel_zero_point i quant_min quant_max - per_channel_zero_point i per_channel_scale i out = res permute tuple permute_axis_list out dtype Reference method gradient fake quantize operator Note because scale zero_point left float actual kernel mimics how fake_quant works float _fake_quantize_per_channel_affine_grad_reference dY X per_channel_scale per_channel_zero_point axis quant_min quant_max dtype = X dtype X permute_axis_list = _permute_to_axis_zero X torch float axis Xq = torch zeros_like X i range X size Xq i = torch round X i per_channel_scale i + per_channel_zero_point i Xq = Xq permute tuple permute_axis_list mask = Xq = quant_min Xq = quant_max res = torch zeros_like dY res mask = dY mask res dtype to_tensor X device isinstance X torch Tensor X = torch tensor X X = X detach clone X device=torch device device dtype=torch float copy-pasted https github com pytorch ao blob bc f da da db da e c df torchao prototype custom_fp_utils py#L C -L C _n_ones n int - int n - EBITS_F MBITS_F = F _EXP_BIAS = _n_ones EBITS_F - copy-pasted https github com pytorch ao blob bc f da da db da e c df torchao prototype custom_fp_utils py#L C -L C _f _to_floatx_unpacked x Tensor ebits int mbits int - Tensor Convert FP numbers sub-byte floating point numbers given number exponent mantissa bits Input torch Tensor dtype torch float Output torch Tensor dtype torch uint where bit encoding stored least significant bits e g fp bits - empty bits - fp _e m encoding fp bits - empty bits - fp _e m fp _e m encoding Note there no special values NaN inf support code Values outside representable range Floatx after rounding clamped maximum Floatx magnitude sign preserved Code below adaptation https fburl com code ciwofcg Background last answer https stackoverflow com q Background Computer Organization Design RISC-V edition Chapter assert x dtype == torch float assert + ebits + mbits = calculate constants exp_bias = _n_ones ebits - max_int = _n_ones ebits + mbits sign_mask = ebits + mbits TODO document better magic_adder = _n_ones MBITS_F - mbits - all E bits M bits s max_normal = _n_ones ebits - exp_bias _n_ones mbits + mbits E bits = M bits = min_normal = - exp_bias denorm_exp = exp bias conversion between formats F _EXP_BIAS - exp_bias mantissa length difference between formats + MBITS_F - mbits add one encoded exponent denormalized numbers + denorm_mask_int = denorm_exp MBITS_F reinterpret int float denorm_mask_float = torch tensor denorm_mask_int dtype=torch int view torch float save sign Note we have torch uint some ops like cpu bit shifts do work So we stay int x = x view torch int sign = x x set everything positive will add sign back end x = x ^ sign TODO can branch floating point comparisons below done without converting float probably need verify x = x view torch float rewrite saturate denorm norm branches without explicit data dependent control flow more compiler friendly saturate_mask = x = max_normal denormal_mask = torch logical_and torch logical_not saturate_mask x min_normal normal_mask = torch logical_not torch logical_or saturate_mask denormal_mask branch saturate max val - handled later code which combines branches branch conversion denormal well rounding up normal denormal_x = x + denorm_mask_float denormal_x = denormal_x view torch int denormal_x -= denorm_mask_int denormal_x = denormal_x torch uint branch stay normal range adjust exponent round normal_x = x view torch int resulting mantissa odd mant_odd = normal_x MBITS_F - mbits update exponent rounding bias part val_to_add = exp_bias - F _EXP_BIAS MBITS_F + magic_adder normal_x += val_to_add rounding bias part normal_x += mant_odd take bits normal_x = normal_x MBITS_F - mbits normal_x = normal_x torch uint combine branches x = torch full_like x max_int dtype=torch uint x = torch where denormal_mask denormal_x x x = torch where normal_mask normal_x x add sign back sign_lp = sign MBITS_F + EBITS_F - mbits - ebits sign_lp = sign_lp torch uint Right shift negative signed integer can fill least significant bits either s s depending implementation Since PyTorch doesn t have uint dtype we mask out these bits get just f sign bit sign_lp = sign_lp sign_mask x = x &#124; sign_lp x torch uint copy-pasted https github com pytorch ao blob d af f f c b bbeae torchao prototype custom_fp_utils py#L _floatx_unpacked_to_f x Tensor ebits int mbits int - Tensor Convert sub-byte floating point numbers given number exponent mantissa bits FP Input torch Tensor dtype uint where bit encoding stored least significant bits e g fp bits - empty bits - fp _e m encoding fp bits - empty bits - fp _e m fp _e m encoding Output torch Tensor dtype fp dequantized value assert x dtype == torch uint assert + ebits + mbits = sign_mask = ebits + mbits exp_bias = _n_ones ebits - mantissa_mask = _n_ones mbits save sign sign_lp = x sign_mask set everything positive will add sign back end x_pos = x ^ sign_lp Calculate zero mask zero_mask = x_pos == Calculate denormal path mask denormal_mask = torch logical_and x_pos x_pos mbits == Calculate normal path calculate new exponent shift bits result exp_biased_lp = x_pos mbits exp_biased_f = exp_biased_lp - exp_bias + F _EXP_BIAS exp_biased_f = exp_biased_f torch int MBITS_F shift mantissa bits result mantissa_lp_int = x_pos mantissa_mask torch int mantissa_f = mantissa_lp_int MBITS_F - mbits result = exp_biased_f &#124; mantissa_f Add zero denormal casts already casted normal path result zero_mask = denormal_exp_biased = - exp_bias + F _EXP_BIAS fast path without performance FP _E M slower x mbits == result denormal_mask = denormal_exp_biased - mbits MBITS_F iterate over all possible values mantissa i= j= i= j= i= j= so i range mbits mantissa_cmp range i i + left shift mantissa until overflows create implicit subtract exponent same amount left_shift = mbits - i mantissa_f = mantissa_cmp - i left_shift + MBITS_F - mbits exp_biased_f = denormal_exp_biased - left_shift MBITS_F we can update in-place since values won t overlap torch compile may complain unsupported operand type s &#124; SymInt int thus we use + instead &#124; here mantissa_lp_int mantissa_lp_int == mantissa_cmp = exp_biased_f + mantissa_f result = torch where denormal_mask mantissa_lp_int result add sign back sign_f = sign_lp torch int MBITS_F - mbits + EBITS_F - ebits result = result &#124; sign_f result view torch float copied https github com drisspg transformer_nuggets blob main transformer_nuggets mx to_blocked py ceil_div b + b - b NVIDIA Blackwell HW requires scales MX NV blocked formats x tile layout weird x x internal layout tile If we want take swizzled scales use them non-gemm purposes like testing we need de-swizzle them then they can applied much more naturally from_blocked input input_scales blocksize - torch Tensor Matrix x pattern internally blocked x x nonsense Output should input size input size blocksize scales output_scales = torch zeros input size input size blocksize device=input device dtype=input_scales dtype Swizzled scales padded tiles x we need replicate how padding happened offset purposes There K blocksize scales padded groups num_col_tiles = ceil_div ceil_div input size blocksize Very slow reference implementation using horrifying loops i range input size j range input size blocksize which x tile scaling factors am I scale_tile_h = i scale_tile_w = j There padded input_scales size tiles along w dim So offset h_tile tiles_per_row + tile_in_row tile_offset = scale_tile_h num_col_tiles + scale_tile_w indices within tile - use nomenclature directly cublas docs outer = i outer cublas docs inner = j inner cublas docs Note offset given terms bytes cublas docs our scales e m anyway so B == value = use offset directly Formula directly cublas docs offset = tile_offset + outer + outer + inner output_scales i j = input_scales offset output_scales from_blocked_format x_mxfp scales_unswizzled blocksize= expand scales scales = torch repeat_interleave scales_unswizzled blocksize dim= de-scale convert x_f = x_mxfp torch float scales torch float x_f torch bfloat to_blocked input_matrix - torch Tensor Rearrange large matrix breaking into blocks applying rearrangement pattern See https docs nvidia com cuda cublas index html#d-block-scaling-factors-layout Args input_matrix Input tensor shape H W Returns Rearranged tensor shape ceil_div H ceil_div W rows cols = input_matrix shape n_row_blocks = ceil_div rows n_col_blocks = ceil_div cols Calculate padded shape padded_rows = n_row_blocks padded_cols = n_col_blocks padded = input_matrix Ideally we would use torch nn pad doesn t support float _e m fnu now rows cols = padded_rows padded_cols padded = torch zeros padded_rows padded_cols device=input_matrix device dtype=input_matrix dtype padded rows cols = input_matrix Rearrange blocks blocks = padded view n_row_blocks n_col_blocks permute rearranged = blocks reshape - transpose reshape - rearranged flatten down_size size assert size - == f size last dim divisible two size - size - pack_uint uint _data - torch Tensor converting uint operations shape = uint _data shape assert shape - == uint _data = uint _data contiguous view - uint _data &#124; uint _data view down_size shape exponent mantissa bits ` torch float _e m fn_x ` FP _EBITS FP _MBITS = _bfloat _to_float _e m fn_x x assert x dtype == torch bfloat x = _f _to_floatx_unpacked x float FP _EBITS FP _MBITS x = pack_uint x x = x view torch float _e m fn_x x This function extracted https github com pytorch ao blob v torchao prototype mx_formats mx_tensor py#L to_mxfp data_hp torch Tensor block_size int = format str = mxfp assert data_hp dtype torch bfloat torch float f data_hp dtype supported yet assert data_hp shape - block_size == f last dimension shape data_hp shape must divisible block_size block_size assert data_hp is_contiguous unsupported orig_shape = data_hp shape data_hp = data_hp reshape orig_shape - orig_shape - block_size block_size max_abs = torch amax torch abs data_hp - unsqueeze - data_hp = data_hp torch float max_abs = max_abs torch float format == mxfp F E M _MAX = torch finfo torch float _e m fn max max_pos = F E M _MAX format == mxfp F E M _MAX = max_pos = F E M _MAX RCEIL _to_mx_rceil data_hp torch Tensor max_abs torch Tensor max_pos float - tuple torch Tensor torch Tensor E M _EXPONENT_BIAS = descale = max_abs max_pos exponent = torch where torch isnan descale xFF Handle biased exponent nan NOTE descale torch finfo torch float smallest_normal handled through clamping torch clamp torch ceil torch log descale min=-E M _EXPONENT_BIAS max=E M _EXPONENT_BIAS + E M _EXPONENT_BIAS torch uint descale_fp = torch where exponent == torch exp E M _EXPONENT_BIAS - exponent torch float scale saturated cast data elements max target dtype data_lp = torch clamp data_hp descale_fp min=- max_pos max=max_pos exponent data_lp scale_e m _biased data_lp = _to_mx_rceil data_hp max_abs max_pos cast target dtype format == mxfp data_lp = data_lp torch float _e m fn need reshape end help inductor fuse things data_lp = data_lp reshape orig_shape format == mxfp data_lp = _bfloat _to_float _e m fn_x data_lp torch bfloat final_shape = list orig_shape final_shape - = data_lp = data_lp reshape final_shape scale_e m _biased = scale_e m _biased view torch float _e m fnu scale_e m _biased = scale_e m _biased squeeze - scale_e m _biased data_lp Source https github com pytorch ao blob c ae f d da dc e ed torchao prototype moe_training utils py#L generate_jagged_offs E M multiple_of= dtype=torch int device= cuda Utility function tests benchmarks Generates tensor length E containing random values divisible ` multiple_of ` M sorted order where final value tensor always M Args E int The length tensor M int The maximum value tensor Returns torch Tensor A tensor length E specified properties random Ensure M divisible M multiple_of = raise ValueError f M must divisible multiple_of Generate list possible values possible_values = list range multiple_of M + multiple_of If E larger than number possible values raise error E len possible_values raise ValueError E cannot larger than number possible values Randomly select E - values possible values excluding M selected_values = torch tensor random sample possible_values - E - Append M selected values selected_values = torch cat selected_values torch tensor M Sort selected values selected_values _ = torch sort selected_values selected_values dtype device