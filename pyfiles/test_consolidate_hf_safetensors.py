Owner s oncall distributed checkpointing importlib json os torch torch distributed checkpoint dist_cp torch distributed dist torch distributed checkpoint _consolidate_hf_safetensors _calculate_max_contiguous_elements _write_sub_tensor_to_file_optimized consolidate_safetensors_files consolidate_safetensors_files_on_every_rank torch distributed checkpoint _hf_utils _metadata_fn torch distributed device_mesh init_device_mesh torch distributed tensor DTensor Shard torch testing _internal common_utils run_tests torch testing _internal distributed _tensor common_dtensor DTensorTestBase skip_if_lt_x_gpu with_comms torch testing _internal distributed checkpoint_utils with_temp_dir TestConsolidateHFSafeTensors DTensorTestBase _create_d_tensors - None global_tensor = torch arange dtype=torch float view mesh_shape = world_size mesh_ d = init_device_mesh device_type mesh_shape Create local tensor row-wise sharding rows_per_rank = global_tensor shape world_size start_row = rank rows_per_rank end_row = start_row + rows_per_rank local_tensor = global_tensor start_row end_row clone Create DTensor row-wise sharding dtensor = DTensor from_local local_tensor device_mesh=mesh_ d placements= Shard shape=global_tensor shape stride= Create local tensor column-wise sharding cols_per_rank = global_tensor shape world_size start_col = rank cols_per_rank end_col = start_col + cols_per_rank local_tensor_col = global_tensor start_col end_col clone Create DTensor column-wise sharding dtensor_col = DTensor from_local local_tensor_col device_mesh=mesh_ d placements= Shard Column-wise sharding shape=global_tensor shape stride= state_dict_to_save = dtensor dtensor dtensor_col dtensor_col dist_cp save state_dict=state_dict_to_save storage_writer=dist_cp HuggingFaceStorageWriter path=self temp_dir save_distributed=True dist barrier os sync with_comms with_temp_dir skip_if_lt_x_gpu test_consolidate_to_one_file - None importlib util find_spec safetensors None print safetensors installed safetensors checkpoint_dir = temp_dir output_dir = os path join checkpoint_dir consolidated os makedirs output_dir exist_ok=True _create_d_tensors global_tensor = torch arange dtype=torch float view rank == consolidate_safetensors_files checkpoint_dir output_dir fqn_to_index_mapping= dtensor dtensor_col file_path = os path join output_dir model- -of- safetensors loaded_dict = safetensors torch load_file file_path assertEqual loaded_dict keys dtensor dtensor_col assertTrue torch equal loaded_dict dtensor global_tensor assertTrue torch equal loaded_dict dtensor_col global_tensor open os path join output_dir _metadata_fn f metadata = json load f assertEqual metadata metadata total_size assertEqual metadata weight_map dtensor model- -of- safetensors dtensor_col model- -of- safetensors dist barrier with_comms with_temp_dir skip_if_lt_x_gpu test_consolidate_to_two_files importlib util find_spec safetensors None print safetensors installed safetensors checkpoint_dir = temp_dir output_dir = os path join checkpoint_dir consolidated os makedirs output_dir exist_ok=True _create_d_tensors global_tensor = torch arange dtype=torch float view rank == fqn_to_index_mapping = dtensor dtensor_col consolidate_safetensors_files checkpoint_dir output_dir fqn_to_index_mapping=fqn_to_index_mapping file _path = os path join output_dir model- -of- safetensors file _path = os path join output_dir model- -of- safetensors loaded_dict = safetensors torch load_file file _path assertEqual loaded_dict keys dtensor assertTrue torch equal loaded_dict dtensor global_tensor loaded_dict_col = safetensors torch load_file file _path assertEqual loaded_dict_col keys dtensor_col assertTrue torch equal loaded_dict_col dtensor_col global_tensor open os path join output_dir _metadata_fn f metadata = json load f assertEqual metadata metadata total_size assertEqual metadata weight_map dtensor model- -of- safetensors dtensor_col model- -of- safetensors dist barrier test_calculate_max_contiguous_elements_validations - None Test validation logic _calculate_max_contiguous_elements function Test empty lists validation assertRaisesRegex ValueError Input lists cannot empty _calculate_max_contiguous_elements Test mismatched list lengths validation assertRaisesRegex ValueError All input lists must have same length _calculate_max_contiguous_elements Test indices out bounds validation assertRaisesRegex ValueError Index dimension out bounds sub-tensor shape _calculate_max_contiguous_elements indices = sub_tensor_shape Test sub-tensor dimensions exceeding tensor dimensions validation assertRaisesRegex ValueError Sub-tensor dimension position exceeds tensor dimension _calculate_max_contiguous_elements sub_tensor_shape tensor_shape test_calculate_max_contiguous_elements_valid_cases - None Test valid cases _calculate_max_contiguous_elements function Test D case - simple remaining elements result = _calculate_max_contiguous_elements assertEqual result - = elements remaining Test D case - start row can write complete rows result = _calculate_max_contiguous_elements assertEqual result rows columns = elements Test D case - middle row only remaining current row result = _calculate_max_contiguous_elements assertEqual result - = elements remaining row Test D case - start D slice can write complete slices result = _calculate_max_contiguous_elements assertEqual result slices rows columns = elements Test edge case - last position result = _calculate_max_contiguous_elements assertEqual result Only element remaining Test case where sub-tensor spans full width result = _calculate_max_contiguous_elements assertEqual result rows columns = elements Test column-wise sharded case - sub-tensor doesn t span full width Even start row can only write width one row due column sharding result = _calculate_max_contiguous_elements assertEqual result Only elements width sub-tensor can written contiguously Test another column-wise sharded case - middle tensor result = _calculate_max_contiguous_elements assertEqual result Only elements width sub-tensor can written contiguously with_comms with_temp_dir skip_if_lt_x_gpu test_consolidate_with_two_ranks importlib util find_spec safetensors None print safetensors installed safetensors checkpoint_dir = temp_dir output_dir = os path join checkpoint_dir consolidated os makedirs output_dir exist_ok=True _create_d_tensors global_tensor = torch arange dtype=torch float view fqn_to_index_mapping = dtensor dtensor_col consolidate_safetensors_files_on_every_rank checkpoint_dir output_dir fqn_to_index_mapping=fqn_to_index_mapping file _path = os path join output_dir model- -of- safetensors file _path = os path join output_dir model- -of- safetensors loaded_dict = safetensors torch load_file file _path assertEqual loaded_dict keys dtensor assertTrue torch equal loaded_dict dtensor global_tensor loaded_dict_col = safetensors torch load_file file _path assertEqual loaded_dict_col keys dtensor_col assertTrue torch equal loaded_dict_col dtensor_col global_tensor dist barrier with_comms with_temp_dir skip_if_lt_x_gpu test_consolidate_one_file_with_two_ranks importlib util find_spec safetensors None print safetensors installed safetensors testing case where one rank has no data write other rank has two tensors write rank no work should wait properly other rank finish checkpoint_dir = temp_dir output_dir = os path join checkpoint_dir consolidated os makedirs output_dir exist_ok=True _create_d_tensors global_tensor = torch arange dtype=torch float view fqn_to_index_mapping = dtensor dtensor_col consolidate_safetensors_files_on_every_rank checkpoint_dir output_dir fqn_to_index_mapping=fqn_to_index_mapping file _path = os path join output_dir model- -of- safetensors loaded_dict = safetensors torch load_file file _path assertEqual loaded_dict keys dtensor dtensor_col assertTrue torch equal loaded_dict dtensor global_tensor assertTrue torch equal loaded_dict dtensor_col global_tensor test_write_sub_tensor_to_file_optimized - None Test _write_sub_tensor_to_file_optimized function various scenarios Test case Simple D tensor row-wise sharding full_tensor_shape = sub_tensor_shape = sub_tensor_offsets = element_size = float Create test data sub_tensor_data = torch arange dtype=torch float sub_tensor_bytes = sub_tensor_data numpy tobytes Create full tensor buffer full_tensor_buffer = bytearray element_size full_tensor_mv = memoryview full_tensor_buffer Call function _write_sub_tensor_to_file_optimized full_tensor_mv sub_tensor_bytes element_size full_tensor_shape sub_tensor_offsets sub_tensor_shape Verify result result_tensor = torch frombuffer full_tensor_buffer dtype=torch float view expected_tensor = torch zeros dtype=torch float expected_tensor = sub_tensor_data view assertTrue torch equal result_tensor expected_tensor Test case Column-wise sharding full_tensor_shape = sub_tensor_shape = sub_tensor_offsets = sub_tensor_data = torch arange dtype=torch float sub_tensor_bytes = sub_tensor_data numpy tobytes full_tensor_buffer = bytearray element_size full_tensor_mv = memoryview full_tensor_buffer _write_sub_tensor_to_file_optimized full_tensor_mv sub_tensor_bytes element_size full_tensor_shape sub_tensor_offsets sub_tensor_shape result_tensor = torch frombuffer full_tensor_buffer dtype=torch float view expected_tensor = torch zeros dtype=torch float expected_tensor = sub_tensor_data view assertTrue torch equal result_tensor expected_tensor __name__ == __main__ run_tests