mypy allow-untyped-defs functools collections abc Callable contextlib contextmanager torch torch _decomp decomposition_table torch utils _pytree tree_map_only HANDLED_FUNCTIONS dict Callable torch autograd Function = aten = torch _ops ops aten __torch_function__ runs before pydispatcher so we need manually use same decompositions indexed their torch equivalent expanded_weights_rnn_decomps = func input_decomp data_decomp torch rnn_relu decomposition_table aten rnn_relu input decomposition_table aten rnn_relu data torch rnn_tanh decomposition_table aten rnn_tanh input decomposition_table aten rnn_tanh data torch lstm decomposition_table aten lstm input decomposition_table aten lstm data torch gru decomposition_table aten gru input decomposition_table aten gru data all RNN decomps run linear batch dimension second even batch_first set contextmanager batch_second args kwargs set_batch_second ew ew set_batch_first False reset_batch_first ew ew set_batch_first True tree_map_only ExpandedWeight set_batch_second args tree_map_only ExpandedWeight set_batch_second kwargs try yield finally tree_map_only ExpandedWeight reset_batch_first args tree_map_only ExpandedWeight reset_batch_first kwargs support packed sequences we need allow smaller batches Expanded weights represents largest batch contextmanager allow_smaller_batches args kwargs allow ew ew set_allow_smaller_batches True reset ew ew set_allow_smaller_batches False tree_map_only ExpandedWeight allow args tree_map_only ExpandedWeight allow kwargs try yield finally tree_map_only ExpandedWeight reset args tree_map_only ExpandedWeight reset kwargs contextmanager setup_rnn use_input_variant args kwargs batch_second args kwargs use_input_variant allow_smaller_batches args kwargs yield implements_per_sample_grads torch_function functools wraps torch_function decorator autograd_func HANDLED_FUNCTIONS torch_function = autograd_func autograd_func decorator ExpandedWeight represents weight parameter Tensor has expanded batch dimension Operations ExpandedWeight Tensor act exactly like those without expanded batch dimension call backward populates original unexpanded tensor per-sample-gradients grad_sample field ExpandedWeight has fallback always fails since we cannot know what batch dimension input tensor therefore cannot know valid call This __torch_function__ object could have also been Tensor Extension dispatch key Needs tensor subclass allow reparameterization ExpandedWeight torch Tensor __init__ orig_weight batch_size loss_reduction batch_size = batch_size batch_first = True allow_smaller_batches = False orig_weight = orig_weight loss_reduction = loss_reduction handled_functions = HANDLED_FUNCTIONS __new__ cls orig_weight batch_size loss_reduction isinstance orig_weight torch Tensor raise RuntimeError f Can only make Expanded Weights Tensors got type orig_weight __name__ orig_weight requires_grad raise RuntimeError Can only build ExpandedWeights objects tensors require_grad ret = torch Tensor _make_subclass cls orig_weight True ret classmethod __torch_function__ cls func _ args= kwargs=None kwargs None kwargs = func expanded_weights_rnn_decomps aten choosing input data variants done parsing logic This mimics some decomp_opts = expanded_weights_rnn_decomps func use_input_variant = isinstance pyrefly ignore index-error args list data variant uses list here decomp = decomp_opts use_input_variant decomp_opts decomp None setup_rnn use_input_variant args kwargs decomp args kwargs func torch _cudnn_rnn_flatten_weight since we aren t using fused cuda kernels RNNs don t do func cls handled_functions cls handled_functions func apply tuple kwargs keys func args + tuple kwargs values We cannot use fallback here because we do know batch dimension any regular tensor inputs i e torch add torch Tensor ExpandedWeight raise RuntimeError f Expanded Weights encountered cannot handle function func __name__ property dtype type ignore override orig_weight dtype property data type ignore override orig_weight data property shape type ignore override orig_weight shape property device type ignore override orig_weight device property is_cuda type ignore override orig_weight is_cuda data_ptr orig_weight data_ptr get_device orig_weight get_device set_allow_smaller_batches is_allow_smaller_batches allow_smaller_batches = is_allow_smaller_batches set_batch_first is_batch_first=True batch_first = is_batch_first