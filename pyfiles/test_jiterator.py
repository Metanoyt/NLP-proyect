Owner s module cuda torch torch cuda jiterator _create_jit_fn create_jit_fn torch cuda jiterator _create_multi_output_jit_fn create_multi_output_jit_fn sys itertools product torch testing _internal common_utils TestCase parametrize run_tests TEST_CUDA NoTest torch testing _internal common_dtype all_types_and_complex_and torch testing _internal common_device_type skipCUDAIfVersionLessThan instantiate_device_type_tests dtypes toleranceOverride tol TEST_CUDA print CUDA available skipping tests file=sys stderr TestCase = NoTest noqa F code_string = template typename T T my_fused_kernel T x T y T alpha T beta alpha x + beta y jitted_fn = create_jit_fn code_string alpha= beta= ref_fn x y alpha= beta= alpha x + beta y TestPythonJiterator TestCase parametrize shape_strides contiguous dtypes product all_types_and_complex_and torch half torch bfloat all_types_and_complex_and torch half torch bfloat test_all_dtype_contiguous device dtypes shape_strides a_buffer = torch rand device=device mul type dtypes b_buffer = torch rand device=device mul type dtypes = a_buffer as_strided shape_strides b = b_buffer as_strided shape_strides expected = ref_fn b result = jitted_fn b assertEqual expected result See https github com pytorch pytorch pull #issuecomment- details On cuda nvrtcCompileProgram taking too long compile jiterator generated kernels non-contiguous input requires dynamic-casting skipCUDAIfVersionLessThan parametrize shape_strides non-contiguous dtypes product all_types_and_complex_and torch half torch bfloat all_types_and_complex_and torch half torch bfloat test_all_dtype_noncontiguous device dtypes shape_strides a_buffer = torch rand device=device mul type dtypes b_buffer = torch rand device=device mul type dtypes = a_buffer as_strided shape_strides b = b_buffer as_strided shape_strides expected = ref_fn b result = jitted_fn b assertEqual expected result dtypes torch float torch double torch float torch bfloat parametrize alpha - None parametrize beta - None toleranceOverride torch float tol atol= e- rtol= e- test_extra_args device dtype alpha beta = torch rand device=device mul type dtype b = torch rand device=device mul type dtype extra_args = alpha None extra_args alpha = alpha beta None extra_args beta = beta expected = ref_fn b extra_args result = jitted_fn b extra_args assertEqual expected result parametrize is_train True False test_bool_extra_args device is_train code_string = template typename T T conditional T x T mask bool is_train is_train x mask x jitted_fn = create_jit_fn code_string is_train=False ref_fn x mask is_train x mask is_train x = torch rand device=device b = torch rand device=device expected = ref_fn b is_train=is_train result = jitted_fn b is_train=is_train assertEqual expected result test_multiple_functors device code_string = template typename T T fn T x T mask x mask template typename T T main_fn T x T mask T y fn x mask + y jitted_fn = create_jit_fn code_string ref_fn x mask y x mask + y = torch rand device=device b = torch rand device=device c = torch rand device=device expected = ref_fn b c result = jitted_fn b c assertEqual expected result parametrize num_inputs test_various_num_inputs num_inputs inputs = _ range num_inputs inputs append torch rand device= cuda mul input_string = join f T i i i range num_inputs function_body = + join f i i i range num_inputs code_string = f template typename T T my_kernel input_string function_body jitted_fn = create_jit_fn code_string ref_fn inputs torch sum torch stack inputs dim= expected = ref_fn inputs result = jitted_fn inputs assertEqual expected result parametrize num_outputs test_various_num_outputs num_outputs input = torch rand device= cuda output_string = join f T out i i range num_outputs function_body = i range num_outputs function_body += f out i = input + i \n NB type must void otherwise ROCm silently fails code_string = f template typename T void my_kernel T input output_string function_body jitted_fn = create_multi_output_jit_fn code_string num_outputs ref_fn input outputs = i range num_outputs outputs append input + i num_outputs == outputs tuple outputs expected = ref_fn input result = jitted_fn input i range num_outputs assertEqual expected i result i parametrize code_string template typename T T my _kernel T x x template typename T Tmy_kernel T x x test_invalid_function_name code_string assertRaises Exception create_jit_fn code_string instantiate_device_type_tests TestPythonJiterator globals only_for= cuda __name__ == __main__ run_tests