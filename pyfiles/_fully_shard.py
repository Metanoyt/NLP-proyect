mypy allow-untyped-decorators mypy allow-untyped-defs __future__ annotations functools contextlib contextmanager typing Any cast NoReturn Optional overload TYPE_CHECKING Union typing_extensions deprecated torch torch nn nn torch distributed _composable contract torch distributed utils _get_root_modules _fsdp_api AllGather MixedPrecisionPolicy OffloadPolicy ReduceScatter _fsdp_common FSDPMeshInfo HSDPMeshInfo _fsdp_init _get_device_from_mesh _get_managed_modules _get_managed_states _get_post_forward_mesh_info _init_default_fully_shard_mesh _move_states_to_device _fsdp_param_group FSDPParamGroup _fsdp_state _get_module_fsdp_state FSDPState TYPE_CHECKING collections abc Callable Iterable Iterator torch distributed tensor DeviceMesh Shard __all__ = fully_shard FSDPModule UnshardHandle register_fsdp_forward_method get_cls_to_fsdp_cls disable_fsdp_module_new_init share_comm_ctx cls_to_fsdp_cls dict type type = get_cls_to_fsdp_cls - dict type type cls_to_fsdp_cls overload pyrefly ignore inconsistent-overload fully_shard module nn Module mesh Optional DeviceMesh = reshard_after_forward Union bool int = shard_placement_fn Optional Callable nn Parameter Optional Shard = mp_policy MixedPrecisionPolicy = offload_policy OffloadPolicy = ignored_params Optional set nn Parameter = - FSDPModule overload pyrefly ignore inconsistent-overload fully_shard module list nn Module mesh Optional DeviceMesh = reshard_after_forward Union bool int = shard_placement_fn Optional Callable nn Parameter Optional Shard = mp_policy MixedPrecisionPolicy = offload_policy OffloadPolicy = ignored_params Optional set nn Parameter = - list FSDPModule The decorator adds state object ` module ` can accessed via ` fully_shard state module ` The state object module Python runtime decorator does play well static type checking so suppressing some type checks support type overloads such caller can still get correct types based input type contract state_cls=FSDPState type ignore misc see fully_shard module mesh Optional DeviceMesh = None reshard_after_forward Optional Union bool int = None shard_placement_fn Optional Callable nn Parameter Optional Shard = None mp_policy MixedPrecisionPolicy = MixedPrecisionPolicy offload_policy OffloadPolicy = OffloadPolicy ignored_params Optional set nn Parameter = None Apply fully sharded data parallelism FSDP ` ` module ` ` where FSDP shards module parameters gradients optimizer states across data parallel workers save memory cost communication At initialization FSDP shards module s parameters across data parallel workers given ` ` mesh ` ` Before forward FSDP all-gathers sharded parameters across data-parallel workers get unsharded parameters forward computation If ` ` reshard_after_forward ` ` ` ` True ` ` then FSDP frees unsharded parameters after forward re-all-gathers them backward before gradient computation After gradient computation FSDP frees unsharded parameters reduce-scatters unsharded gradients across data-parallel workers This implementation represents sharded parameters ` DTensor ` s sharded dim- while unsharded parameters will like original parameters ` ` module ` ` e g ` torch Tensor ` originally ` torch Tensor ` A module ` forward pre-hook https pytorch org docs main generated torch nn Module html#torch nn Module register_forward_pre_hook ` _ ` ` module ` ` all-gathers parameters module ` forward hook https pytorch org docs main generated torch nn Module html#torch nn Module register_forward_hook ` _ ` ` module ` ` frees them needed Similar backward hooks all-gather parameters later free parameters reduce-scatter gradients Since grouping multiple tensors together one collective critical communication efficiency implementation makes grouping first Calling meth ` fully_shard ` ` ` module ` ` constructs one group includes parameters ` ` module parameters ` ` except those already assigned group earlier call submodule This means meth ` fully_shard ` should called bottom-up your model Each group s parameters all-gathered one collective its gradients reduce-scattered one collective Partitioning model into multiple groups layer layer allows peak memory savings communication computation overlap Users generally should call meth ` fully_shard ` only topmost root module Args module Union nn Module List nn Module The module modules shard FSDP group together communication mesh Optional DeviceMesh This data parallel mesh defines sharding device If D then parameters fully sharded across D mesh FSDP ` ` Shard ` ` placement If D then parameters sharded across st dim replicated across th dim HSDP ` ` Replicate Shard ` ` placement The mesh s device type gives device type used communication CUDA CUDA-like device type then we use current device reshard_after_forward Optional Union bool int This controls parameter behavior after forward can trade off memory communication - If ` ` True ` ` then reshards parameters after forward re-all-gathers backward - If ` ` False ` ` then keeps unsharded parameters memory after forward avoids all-gather backward For best performance we usually set ` ` False ` ` root module because root module typically required immediately when backward pass begins - If ` ` None ` ` set ` ` True ` ` non-root modules ` ` False ` ` root modules - If ` ` int ` ` then represents world size reshard after forward It should non-trivial divisor ` ` mesh ` ` shard dim size i e excluding dim size itself A choice may intra-node size e g ` ` torch cuda device_count ` ` This allows all-gather backward over smaller world size cost higher memory usage than setting ` ` True ` ` - After forward parameters registered module depend The registered parameters sharded parameters ` ` True ` ` unsharded parameters ` ` False ` ` parameters resharded smaller mesh otherwise To modify parameters between forward backward registered parameters must sharded parameters For ` ` False ` ` ` ` int ` ` can done manually resharding via meth ` reshard ` shard_placement_fn Optional Callable nn Parameter Optional Shard This callable can used override sharding placement parameter shard parameter dimension other than dim- If callable returns ` Shard ` placement ` ` None ` ` then FSDP will shard according placement e g ` ` Shard ` ` If sharding nonzero dim we currently require even sharding i e tensor dim size dim must divisible FSDP shard mesh size mp_policy MixedPrecisionPolicy This controls mixed precision policy which offers parameter reduction mixed precision module See ` MixedPrecisionPolicy ` details offload_policy OffloadPolicy This controls offloading policy which offers parameter gradient optimizer state offloading See ` OffloadPolicy ` its subclasses details ignored_params Optional Set nn Parameter The set parameters ignored FSDP They will sharded nor moved device during init nor have their gradients reduced backward Returns FSDPModule The module FSDP applied in-place torch _C _log_api_usage_once torch distributed fsdp fully_shard isinstance module nn ModuleList nn ModuleDict raise ValueError f fully_shard does support containers do implement forward module mesh = mesh _init_default_fully_shard_mesh mesh ndim raise ValueError f fully_shard expects D D DeviceMesh got mesh mesh ndim == mesh_info = FSDPMeshInfo mesh shard_mesh_dim= mesh mesh_dim_names None raise AssertionError Please init D mesh HSDP mesh_dim_names specified mesh_info = HSDPMeshInfo mesh shard_mesh_dim= replicate_mesh_dim= device = _get_device_from_mesh mesh auto_reshard_after_forward = reshard_after_forward None If user does provide ` ` reshard_after_forward ` ` we set True During lazy_init we identify which module root override its value False post_forward_mesh_info = _get_post_forward_mesh_info reshard_after_forward auto_reshard_after_forward True type ignore arg-type mesh_info arg_module = module modules = module isinstance module nn Module tuple _get_root_modules module state = fully_shard state modules type ignore attr-defined see state init modules device mp_policy auto_reshard_after_forward managed_modules = _get_managed_modules modules ignored_params params buffers = _get_managed_states managed_modules ignored_params _move_states_to_device params buffers device params state _fsdp_param_group = FSDPParamGroup params modules mesh_info post_forward_mesh_info device shard_placement_fn mp_policy offload_policy For Dynamo managed_module managed_modules managed_module _is_fsdp_managed_module = True type ignore assignment managed_module _fsdp_use_orig_params = True type ignore assignment Place FSDP leftmost highest priority method resolution order module modules cls = module __class__ new_cls = cls_to_fsdp_cls get cls new_cls dct = __deepcopy__ _unimplemented_deepcopy new_cls = type f FSDP cls __name__ FSDPModule cls dct cls_to_fsdp_cls cls = new_cls module __class__ = new_cls arg_module _unimplemented_deepcopy args Any kwargs Any - NoReturn raise AssertionError FSDP does support deepcopy Please use state dict serialization _enable_fsdp_module_new_init bool = True contextmanager disable_fsdp_module_new_init - Iterator None global _enable_fsdp_module_new_init prev _enable_fsdp_module_new_init = _enable_fsdp_module_new_init False try yield finally _enable_fsdp_module_new_init = prev FSDPModule __new__ cls args kwargs Override ` ` __new__ ` ` remove FSDP directly construct original cases like indexing into container module Use index since dynamically constructed ` FSDP ` index ` FSDPModule ` itself orig_cls = cls __mro__ = orig_cls __new__ orig_cls args kwargs _enable_fsdp_module_new_init __init__ args kwargs reshard - None Reshards module s parameters freeing unsharded parameters they allocated registering sharded parameters module This method recursive state = _get_fsdp_state fsdp_param_group = state _fsdp_param_group fsdp_param_group reshard unshard async_op bool = False - Optional UnshardHandle Unshards module s parameters allocating memory all-gathering parameters This method recursive The unshard follows ` MixedPrecisionPolicy ` so will all-gather following ` ` param_dtype ` ` set Args async_op bool If ` ` True ` ` then returns ` UnshardHandle ` has meth ` wait ` method wait unshard op If ` ` False ` ` then returns ` ` None ` ` waits handle inside function note If ` ` async_op=True ` ` then FSDP will wait pending unshard module s pre-forward user The user only needs call meth ` wait ` explicitly wait should happen before pre-forward state = _get_fsdp_state fsdp_param_group = state _fsdp_param_group fsdp_param_group None fsdp_param_group lazy_init fsdp_param_group unshard async_op=async_op handle = _UnshardHandleImpl fsdp_param_group async_op handle handle wait None set_is_last_backward is_last_backward bool - None Sets whether next backward last one On last backward FSDP waits pending gradient reduction clears internal data data structures backward prefetching This can useful microbatching state = _get_fsdp_state state _state_ctx is_last_backward = is_last_backward set_requires_gradient_sync requires_gradient_sync bool recurse bool = True - None Sets module should sync gradients This can used implement gradient accumulation without communication For HSDP controls both reduce-scatter all-reduce together This equivalence ` no_sync ` FSDP Args requires_gradient_sync bool Whether reduce gradients module s parameters recurse bool Whether set all FSDP submodules just passed-in module self_module = cast nn Module modules = list self_module modules recurse self_module module modules isinstance module FSDPModule state = module _get_fsdp_state fsdp_param_group = state _fsdp_param_group fsdp_param_group reduce_grads = requires_gradient_sync fsdp_param_group all_reduce_grads = requires_gradient_sync set_requires_all_reduce requires_all_reduce bool recurse bool = True - None Sets module should all-reduce gradients This can used implement gradient accumulation only reduce-scatter all-reduce HSDP self_module = cast nn Module modules = list self_module modules recurse self_module module modules isinstance module FSDPModule state = module _get_fsdp_state fsdp_param_group = state _fsdp_param_group fsdp_param_group all_reduce_grads = requires_all_reduce set_reshard_after_forward reshard_after_forward bool recurse bool = True - None Sets module should reshard parameters after forward This can used change ` ` reshard_after_forward ` ` FSDP arg runtime For example can used set FSDP root module s value ` ` True ` ` since otherwise specially set ` ` False ` ` can set FSDP module s value ` ` False ` ` running evals set back ` ` True ` ` training Args reshard_after_forward bool Whether reshard parameters after forward recurse bool Whether set all FSDP submodules just passed-in module isinstance reshard_after_forward bool raise ValueError f reshard_after_forward should bool got type reshard_after_forward self_module = cast nn Module modules = list self_module modules recurse self_module module modules isinstance module FSDPModule state = module _get_fsdp_state state _auto_reshard_after_forward = False fsdp_param_group = state _fsdp_param_group fsdp_param_group post_forward_mesh_info = _get_post_forward_mesh_info reshard_after_forward fsdp_param_group mesh_info set_reshard_after_backward reshard_after_backward bool recurse bool = True - None Sets module should reshard parameters after backward This can used during gradient accumulation trade off higher memory reduced communication since unsharded parameters do need re-all-gathered before next forward Args reshard_after_backward bool Whether reshard parameters after backward recurse bool Whether set all FSDP submodules just passed-in module self_module = cast nn Module modules = list self_module modules recurse self_module module modules isinstance module FSDPModule state = module _get_fsdp_state fsdp_param_group = state _fsdp_param_group fsdp_param_group reshard_after_backward = reshard_after_backward set_modules_to_forward_prefetch modules list FSDPModule - None Sets FSDP modules which FSDP module should explicitly prefetch all-gathers forward The prefetching runs after module s all-gather copy-out Passing singleton list containing next FSDP module gives same all-gather overlap behavior default overlap behavior except prefetched all-gather issued earlier CPU Passing list least length two required more aggressive overlap will use more reserved memory Args modules List FSDPModule FSDP modules prefetch _assert_all_fsdp_modules modules _get_fsdp_state _states_to_forward_prefetch = module _get_fsdp_state module modules set_modules_to_backward_prefetch modules list FSDPModule - None Sets FSDP modules which FSDP module should explicitly prefetch all-gathers backward This overrides default backward pretching implementation prefetches next FSDP module based reverse post-forward order Passing singleton list containing previous FSDP module gives same all-gather overlap behavior default overlap behavior Passing list least length two required more aggressive overlap will use more reserved memory Args modules List FSDPModule FSDP modules prefetch _assert_all_fsdp_modules modules _get_fsdp_state _states_to_backward_prefetch = module _get_fsdp_state module modules set_custom_all_gather comm AllGather - None Overrides default ` ` all_gather ` ` communication behavior have better control over communication memory usage See ` Comm ` ` ReduceScatter ` details Args comm AllGather Custom all-gather communication state = _get_fsdp_state fsdp_param_group = state _fsdp_param_group None fsdp_param_group _all_gather_comm = comm set_custom_reduce_scatter comm ReduceScatter - None Overrides default ` ` reduce_scatter ` ` communication behavior have better control over communication memory usage See ` Comm ` ` ReduceScatter ` details Args comm ReduceScatter Custom reduce_scatter communication state = _get_fsdp_state fsdp_param_group = state _fsdp_param_group None fsdp_param_group _reduce_scatter_comm = comm set_all_reduce_hook hook Callable torch Tensor None stream Optional torch cuda Stream = None Args hook Callable torch Tensor None User-defined all-reduce hook expected signature ` ` hook reduce_output torch Tensor - None ` ` where ` ` reduce_output ` ` reduce-scatter output only using FSDP all-reduce output using native HSDP stream Optional torch cuda Stream Stream run all-reduce hook This should only set using native HSDP If using native HSDP hook will run internally defined all-reduce stream used native HSDP all-reduce state = _get_fsdp_state fsdp_param_group = state _fsdp_param_group None fsdp_param_group _all_reduce_hook = hook stream None fsdp_param_group _is_hsdp raise ValueError stream cannot set when using native HSDP fsdp_param_group _all_reduce_hook_stream = stream set_post_optim_event event torch Event - None Sets post-optimizer-step event root FSDP module wait all-gather streams By default root FSDP module waits all-gather streams current stream ensure optimizer step has finished before all-gathering However may introduce false dependencies there unrelated computation after optimizer step This API allows user provide their own event wait After root waits event event discarded so API should called new event each iteration Args event torch Event Event recorded after optimizer step wait all-gather streams _get_fsdp_state _state_ctx post_optim_event = event deprecated Use ` set_gradient_divide_factor ` instead set_reduce_scatter_divide_factor factor float - None Use py meth ` set_gradient_divide_factor ` instead set_gradient_divide_factor factor set_gradient_divide_factor factor float - None Sets custom divide factor gradient reduction This might use custom reduce op using NCCL s PreMulSum which allows multiplying factor before reduction Args factor float Custom divide factor state = _get_fsdp_state fsdp_param_group = state _fsdp_param_group None fsdp_param_group gradient_divide_factor = factor set_force_sum_reduction_for_comms enable bool - None Sets whether require low-level collective communication primitives exclusively use sum -type reductions even comes cost separate additional pre- post-scaling operations This needed example because NCCL currently supports zero-copy transfers only kind collectives NB MTIA devices always implicitly enabled NB ` set_all_reduce_hook ` used under FSDP setup caller needs ensure custom all-reduce across FSDP units follow strategy well FSDP can no longer automatically handle Args enable bool Whether only ever use ReduceOp SUM comms state = _get_fsdp_state fsdp_param_group = state _fsdp_param_group None fsdp_param_group force_sum_reduction_for_comms = enable set_unshard_in_backward unshard_in_backward bool - None Sets whether FSDP module s parameters need unsharded backward This can used expert cases when user knows all parameters FSDP module s parameter group needed backward computation e g embedding state = _get_fsdp_state fsdp_param_group = state _fsdp_param_group None fsdp_param_group unshard_in_backward = unshard_in_backward set_allocate_memory_from_process_group_for_comm enable bool - None Sets whether temporary staging buffers used send receive data over collective communications should allocated using custom optimized allocator provided ProcessGroup itself any This might allow ProcessGroup more efficient For example when using NCCL enables leverage zero-copy transfers over SHARP NVLink InfiniBand This cannot used together meth ` set_custom_all_gather ` meth ` set_custom_reduce_scatter ` those APIs allow finer-grained control over each communication method cannot determine their staging buffer allocation strategy Args enable bool Whether turn ProcessGroup allocation state = _get_fsdp_state fsdp_param_group = state _fsdp_param_group None fsdp_param_group set_allocate_memory_from_process_group enable _set_unshard_async_op async_op bool Sets whether use ` ` async_op=True ` ` ` ` False ` ` pre-forward pre-backward unshard op This defaults ` ` False ` ` can set ` ` True ` ` method Setting ` ` True ` ` allows all-gather allocations happen default stream avoiding inter-stream memory fragmentation However you must use explicit prefetching e g via meth ` unshard ` forward still get overlap pre-all-gather ops like dtype casting copy-in will overlap compute self_module = cast nn Module module self_module modules isinstance module FSDPModule state = module _get_fsdp_state fsdp_param_group = state _fsdp_param_group fsdp_param_group unshard_async_op = async_op _get_fsdp_state - FSDPState state = _get_module_fsdp_state cast nn Module None raise AssertionError f No FSDP state found state _apply args Any kwargs Any - Any Reshard ensure sharded parameters registered reshard ret = super _apply args kwargs type ignore misc state = _get_fsdp_state fsdp_param_group = state _fsdp_param_group ret TODO Remove padding logic once DTensor pads local tensor https github com pytorch pytorch issues torch no_grad fsdp_param fsdp_param_group fsdp_params fsdp_param reset_sharded_param ret UnshardHandle A handle wait meth ` FSDPModule unshard ` op wait - None Waits unshard op This ensures current stream can use unsharded parameters which now registered module _UnshardHandleImpl UnshardHandle __init__ fsdp_param_group Optional FSDPParamGroup _fsdp_param_group = fsdp_param_group wait _fsdp_param_group None _fsdp_param_group wait_for_unshard Avoid keeping reference _fsdp_param_group = None register_fsdp_forward_method module nn Module method_name str - None Registers method ` ` module ` ` considered forward method FSDP FSDP all-gathers parameters pre-forward optionally frees parameters post-forward depending ` ` reshard_after_forward ` ` FSDP only knows do meth ` nn Module forward ` default This function patches user-specified method run pre post-forward hooks before after method respectively If ` ` module ` ` ` FSDPModule ` then no-op Args module nn Module Module register forward method method_name str Name forward method isinstance module FSDPModule Make no-op allow including both when using using FSDP hasattr module method_name raise ValueError f type module does have method method_name orig_method = getattr module method_name functools wraps orig_method wrapped_method args kwargs fsdp_state = _get_fsdp_state args kwargs = fsdp_state _pre_forward args kwargs out = orig_method args kwargs fsdp_state _post_forward args out Use ` __get__ ` make ` wrapped_method ` instance method setattr module method_name wrapped_method __get__ module type module type ignore attr-defined share_comm_ctx modules list FSDPModule - None Share cuda streams multiple FSDPModules Example usage torch distributed fsdp share_comm_ctx share_comm_ctx fsdp_model_ fsdp_model_ For Pipeline Parallelism PP each model chunk FSDP root We want share cuda streams all-gather reduce-scatter all-reduce This avoids allocating inter-stream memory framgmentation Args modules List FSDPModule modules share cuda streams len modules == module modules isinstance module FSDPModule raise ValueError f Expects list FSDPModules got module fsdp_states = module _get_fsdp_state module modules comm_ctx = fsdp_states _comm_ctx fsdp_state fsdp_states fsdp_state _comm_ctx = comm_ctx fsdp_param_group = fsdp_state _fsdp_param_group fsdp_param_group comm_ctx = comm_ctx _assert_all_fsdp_modules modules Iterable Any - None module modules isinstance module FSDPModule raise ValueError f Expects FSDPModule got type module module