Owner s module inductor ruff noqa F contextlib dataclasses functools io itertools logging os re subprocess sys tempfile unittest copy deepcopy importlib machinery SourceFileLoader pathlib Path string Template unittest mock torch torch distributed dist torch nn nn torch nn functional F torch _inductor inductor torch _dynamo compiled_autograd config torch _dynamo backends debugging aot_eager torch _dynamo device_interface get_interface_for_device torch _dynamo testing normalize_gm torch _dynamo utils counters torch _inductor config inductor_config torch _inductor cpp_builder is_msvc_cl torch _inductor test_case run_tests TestCase torch nn attention flex_attention flex_attention torch nn parallel DistributedDataParallel DDP torch overrides BaseTorchFunctionMode torch testing _internal common_device_type instantiate_device_type_tests ops torch testing _internal common_utils instantiate_parametrized_tests IS_S X IS_WINDOWS parametrize scoped_load_inline skipIfWindows torch testing _internal hop_db hop_db torch testing _internal inductor_utils GPU_TYPE HAS_CPU HAS_CUDA_AND_TRITON HAS_GPU torch testing _internal logging_utils logs_to_string torch testing _internal triton_utils requires_cuda_and_triton torch utils _python_dispatch TorchDispatchMode note these tests run windows due inductor_utils HAS_CPU make_compiler_fn fullgraph=True dynamic=True backend= inductor gm_hook=lambda gm None assert backend inductor aot_eager eager ca_eager _compiler_fn gm Same torch compile counts number compiles gm_hook gm _backend = backend backend == ca_eager gm backend = eager _inner_compiler gm_ example_inputs_ counters compiled_autograd compiles += backend == inductor inductor compile gm_ example_inputs_ backend == aot_eager aot_eager gm_ example_inputs_ _backend = _inner_compiler torch compile gm backend=_backend fullgraph=fullgraph dynamic=dynamic _compiler_fn compiler_fn = make_compiler_fn TODO jansel hooks lambdas creates recompiles dynamo we should fix hook grad grad hook grads grads + hook gI gO torch sin gI + gO reset torch _logging set_logs compiled_autograd_verbose=False config compiled_autograd = False compiled_autograd reset torch _dynamo utils counters clear BaseCustomOp torch autograd Function staticmethod forward ctx x x staticmethod backward ctx grad_output raise NotImplementedError must override TestCompiledAutograd TestCase setUp - None exit_stack = contextlib ExitStack exit_stack enter_context config patch record_runtime_overhead False super setUp reset tearDown - None exit_stack close super tearDown reset check_output_and_recompiles fn count= compiler_fn=compiler_fn compile_fn=False isinstance count list captures compiles = count captures compiles = count count torch autograd set_multithreading_enabled False torch _dynamo reset counters compiled_autograd clear torch manual_seed expected = list fn torch manual_seed compiled_autograd _enable compiler_fn mock patch torch _functorch aot_autograd AOT_COUNTER new_callable=itertools count opt_fn = torch compile fn compile_fn fn actual = list opt_fn assertEqual expected actual assertEqual counters compiled_autograd captures captures assertEqual counters compiled_autograd compiles compiles run_as_subprocess script - bytes try subprocess check_output sys executable -c script stderr=subprocess STDOUT On Windows opening subprocess default CWD makes ` torch ` fail so just set CWD script s directory cwd=os path dirname os path realpath __file__ except subprocess CalledProcessError e fail f Subprocess exited code e returncode test_hipify_not_loaded_with_import_torch script = torch assert globals get hipify False False run_as_subprocess script test_hipify_not_loaded_with_import_cpp_extension script = torch utils cpp_extension assert globals get hipify False False run_as_subprocess script test_dynamo_flaky_segfault script = torch main compiler_fn gm torch compile gm backend= eager inner x = torch randn w = torch randn requires_grad=True model i torch nn functional linear i w out = model x loss = out sum torch _dynamo compiled_autograd _enable compiler_fn loss backward assert w grad None inner torch _dynamo reset inner main Run three times catch bad dynamo state resets _ range run_as_subprocess script gen_cache_miss_log_prefix IS_WINDOWS is_msvc_cl Cache miss due new autograd node struct fail Compilers other than msvc have yet been verified Windows Cache miss due new autograd node test_reset compiled_autograd compiled_autograd_enabled = True torch _C _dynamo compiled_autograd set_autograd_compiler lambda None True TODO prior verbose logger torch _C _dynamo compiled_autograd set_verbose_logger dummy compiled_autograd COMPILE_COUNTER = None state should clean after reset compiled_autograd reset assert compiled_autograd compiled_autograd_enabled False prior_compiler prior_dynamic = torch _C _dynamo compiled_autograd set_autograd_compiler None False assert prior_compiler None assert prior_dynamic False assert compiled_autograd COMPILE_COUNTER None next compiled_autograd COMPILE_COUNTER == test_basic fn model = torch nn Sequential torch nn Linear torch nn ReLU torch nn Linear torch nn ReLU x = torch randn result = model x sum result backward yield model weight grad yield model bias grad yield model weight grad yield model bias grad check_output_and_recompiles fn test_cache_hit fn _ range model = torch nn Sequential torch nn Linear torch nn ReLU torch nn Linear torch nn ReLU x = torch randn result = model x sum result backward yield model weight grad yield model bias grad yield model weight grad yield model bias grad check_output_and_recompiles fn test_graph_break_custom_op torch library custom_op mylib sin mutates_args= sin x torch Tensor - torch Tensor x sin setup_context ctx inputs output x = inputs ctx save_for_backward x backward ctx grad x = ctx saved_tensors grad x cos sin register_autograd backward setup_context=setup_context x = torch randn requires_grad=True y = sin x clone sum compiled_autograd _enable compiler_fn y backward test_tensor_grad_hook fn _ range model = torch nn Sequential torch nn Linear torch nn ReLU x = torch randn model weight register_hook hook result = model x sum result backward yield model weight grad yield model bias grad check_output_and_recompiles fn test_tensor_grad_hook fn _ range model = torch nn Sequential torch nn Linear torch nn ReLU x = torch randn result = model x sum result grad_fn register_prehook hook result backward yield model weight grad yield model bias grad check_output_and_recompiles fn test_tensor_grad_hook fn _ range model = torch nn Sequential torch nn Linear torch nn ReLU x = torch randn result = model x sum result grad_fn register_hook hook result backward yield model weight grad yield model bias grad check_output_and_recompiles fn test_reorder_acc_grad model = torch nn Sequential torch nn Conv d bias=True torch nn Conv d bias=True compiled_model = torch compile model x = torch randn model x sum backward ref_res = model weight grad model bias grad model weight grad model bias grad model weight grad = None model bias grad = None model weight grad = None model bias grad = None compiled_autograd _enable compiler_fn compiled_model x sum backward retain_graph=True res = model weight grad model bias grad model weight grad model bias grad assertEqual res ref_res assertEqual res ref_res assertEqual res ref_res assertEqual res ref_res test_reorder_post_hook grad_div param param grad = param grad Module torch nn Module __init__ ioc super __init__ fc = torch nn Linear ioc ioc bias=False fc = torch nn Linear ioc ioc bias=False grad_acc_hooks = grad_acc = params = fc weight fc weight param params wrapper param param_tmp = param expand_as param grad_acc = param_tmp grad_fn next_functions grad_acc_hook notneeded grad_div param grad_acc append grad_acc grad_acc_hooks append grad_acc register_hook grad_acc_hook wrapper param forward x x = fc x x = fc x x sum bs = ioc = model = Module ioc input = torch randn bs ioc eager ref model input backward ref_res = model fc weight grad model fc weight grad cag model fc weight grad = None model fc weight grad = None model_to_train = torch compile model backend= inductor compiled_autograd _enable compiler_fn model_to_train input backward res = model_to_train fc weight grad model_to_train fc weight grad assertEqual res ref_res assertEqual res ref_res test_reorder_post_hook x = torch randn requires_grad=True y = torch sigmoid x z = torch tanh y assert isinstance z grad_fn torch autograd graph Node assert isinstance y grad_fn torch autograd graph Node handle_z = z grad_fn register_hook lambda gI gO gO handle_y = y grad_fn register_hook lambda gI gO gI z sum backward retain_graph=True ref_res = x grad x grad = None compiled_autograd _enable compiler_fn z sum backward retain_graph=True res = x grad assertEqual res ref_res test_reorder_post_hook conv = torch nn Conv d bias=False x = torch randn y = conv x assert isinstance y grad_fn torch autograd graph Node hook will mul conv weight gradient handle_y = y grad_fn register_hook lambda gI gO gI gI gI y sum backward retain_graph=True ref_res = x grad x grad = None compiled_autograd _enable compiler_fn y sum backward retain_graph=True res = x grad assertEqual res ref_res test_reorder_all_bwd_hooks tensor_hook grad grad sub acc_grad_node_pre_hook grad_out grad_out div post_acc_grad_hook tensor tensor grad add_ TestModel torch nn Module __init__ super __init__ conv = torch nn Conv d bias=False conv = torch nn Conv d bias=False acc_grad = conv weight view_as conv weight grad_fn next_functions conv weight register_hook tensor_hook conv weight register_post_accumulate_grad_hook post_acc_grad_hook acc_grad register_prehook acc_grad_node_pre_hook acc_grad_node_post_hook grad_in grad_out conv weight grad mul_ acc_grad register_hook acc_grad_node_post_hook acc_grad = conv weight view_as conv weight grad_fn next_functions conv weight register_hook tensor_hook conv weight register_post_accumulate_grad_hook post_acc_grad_hook acc_grad register_prehook acc_grad_node_pre_hook acc_grad_node_post_hook grad_in grad_out conv weight grad mul_ acc_grad register_hook acc_grad_node_post_hook forward x y = conv x y = conv y y sum input = torch randn eager ref model = TestModel model input backward ref_results = model conv weight grad model conv weight grad cag model conv weight grad = None model conv weight grad = None compiled_model = torch compile model backend= inductor compiled_autograd _enable compiler_fn compiled_model input backward results = compiled_model conv weight grad compiled_model conv weight grad assertEqual results ref_results assertEqual results ref_results test_reorder_multi_post_hooks TestModel torch nn Module __init__ super __init__ conv = torch nn Conv d bias=False conv = torch nn Conv d bias=False acc_grad = conv weight view_as conv weight grad_fn next_functions acc_grad_node _post_hook grad_in grad_out conv weight grad mul_ acc_grad_node _post_hook grad_in grad_out conv weight grad sub_ acc_grad register_hook acc_grad_node _post_hook acc_grad register_hook acc_grad_node _post_hook acc_grad = conv weight view_as conv weight grad_fn next_functions acc_grad_node _post_hook grad_in grad_out conv weight grad mul_ acc_grad_node _post_hook grad_in grad_out conv weight grad sub_ acc_grad register_hook acc_grad_node _post_hook acc_grad register_hook acc_grad_node _post_hook forward x y = conv x y = conv y y sum input = torch randn eager ref model = TestModel model input backward ref_results = model conv weight grad model conv weight grad cag model conv weight grad = None model conv weight grad = None compiled_model = torch compile model backend= inductor compiled_autograd _enable compiler_fn compiled_model input backward results = compiled_model conv weight grad compiled_model conv weight grad assertEqual results ref_results assertEqual results ref_results test_reorder_multi_pre_hooks acc_grad_node_pre_hook grad_out grad_out div acc_grad_node_pre_hook grad_out grad_out sub TestModel torch nn Module __init__ super __init__ conv = torch nn Conv d bias=False conv = torch nn Conv d bias=False acc_grad = conv weight view_as conv weight grad_fn next_functions acc_grad register_prehook acc_grad_node_pre_hook acc_grad register_prehook acc_grad_node_pre_hook acc_grad = conv weight view_as conv weight grad_fn next_functions acc_grad register_prehook acc_grad_node_pre_hook acc_grad register_prehook acc_grad_node_pre_hook forward x y = conv x y = conv y y sum input = torch randn eager ref model = TestModel model input backward ref_results = model conv weight grad model conv weight grad cag model conv weight grad = None model conv weight grad = None compiled_model = torch compile model backend= inductor compiled_autograd _enable compiler_fn compiled_model input backward results = compiled_model conv weight grad compiled_model conv weight grad assertEqual results ref_results assertEqual results ref_results test_reorder_multi_tensor_pre_hooks tensor_hook grad grad sub tensor_hook grad grad mul TestModel torch nn Module __init__ super __init__ conv = torch nn Conv d bias=False conv = torch nn Conv d bias=False acc_grad = conv weight view_as conv weight grad_fn next_functions conv weight register_hook tensor_hook conv weight register_hook tensor_hook acc_grad = conv weight view_as conv weight grad_fn next_functions conv weight register_hook tensor_hook conv weight register_hook tensor_hook forward x y = conv x y = conv y y sum input = torch randn eager ref model = TestModel model input backward ref_results = model conv weight grad model conv weight grad cag model conv weight grad = None model conv weight grad = None compiled_model = torch compile model backend= inductor compiled_autograd _enable compiler_fn compiled_model input backward results = compiled_model conv weight grad compiled_model conv weight grad assertEqual results ref_results assertEqual results ref_results test_torch_compile fn model = torch nn Sequential torch nn Linear torch nn Sigmoid opt_model = torch compile model fullgraph=True _ range x = torch randn result = opt_model x sum result backward yield model weight grad yield model bias grad model zero_grad check_output_and_recompiles fn parametrize api compile optimize parametrize backend eager aot_eager inductor test_compile_api api backend wrap fn backend api == compile torch compile fn backend=backend api == optimize torch _dynamo optimize backend fn fn model inputs res = inp inputs result = model inp sum result backward res append model weight grad res append model bias grad model zero_grad res torch manual_seed model = torch nn Sequential torch nn Linear torch nn Sigmoid inputs = torch randn torch randn torch randn expected = fn model inputs config patch compiled_autograd=True compiled_fn = wrap fn backend actual = compiled_fn model inputs assertEqual expected actual assertEqual counters compiled_autograd captures parametrize api compile optimize parametrize backend eager aot_eager inductor test_compile_api_disable api backend wrap fn backend api == compile torch compile fn backend=backend api == optimize torch _dynamo optimize backend fn fn model inputs res = inp inputs result = model inp sum result backward res append model weight grad res append model bias grad model zero_grad res torch manual_seed model = torch nn Sequential torch nn Linear torch nn Sigmoid inputs = torch randn torch randn torch randn expected = fn model inputs config patch compiled_autograd=True compiled_fn = wrap fn backend torch _dynamo compiled_autograd _disable actual = compiled_fn model inputs assertEqual expected actual assertTrue compiled_autograd counters parametrize backend eager aot_eager inductor test_optimize_assert backend can merged into test above once we support no graph break backward fn model inp NOTE calling backward compiled fn model inp sum torch manual_seed model = torch nn Sequential torch nn Linear torch nn Sigmoid inp = torch randn out = fn model inp out backward expected = p grad p model parameters model zero_grad config patch compiled_autograd=True compiled_fn = torch _dynamo optimize_assert backend fn should error due undefined ` rebuild_ctx ` out = compiled_fn model inp out backward actual = p grad p model parameters assertEqual expected actual assertEqual counters compiled_autograd captures config patch compiled_autograd=True test_nested_context_manager ctx compiled_autograd _enable torch compile ok outer = ctx inner = ctx outer __enter__ inner __enter__ inner __exit__ None None None outer __exit__ None None None ok outer = ctx inner = ctx outer __enter__ inner __enter__ assertRaisesRegex AssertionError Nested Compiled Autograd Contexts must before their parent context outer __exit__ None None None config patch compiled_autograd=True test_nested_compile torch library _scoped_library testlib FRAGMENT lib lib define square Tensor x - Tensor torch library impl testlib square CPU square_impl x torch Tensor - torch Tensor nested inference graph compile torch compile backend= eager fn x x fn x MyFn torch autograd Function staticmethod forward ctx x x staticmethod backward ctx x torch ops testlib square x x = torch tensor requires_grad=True torch compile fn x MyFn apply x fn x sum backward config patch compiled_autograd=True test_no_nested_compiled_autograd We disable CA before entering CA graph So re-entrants should running eager autograd engine unrelated_autograd_call x = torch randn requires_grad=True y = torch randn requires_grad=True loss = torch matmul x y sum loss backward MyFn torch autograd Function staticmethod forward ctx x x staticmethod backward ctx gO unrelated_autograd_call gO x = torch randn requires_grad=True loss = MyFn apply x sum torch compile lambda loss backward create_graph=True assertEqual counters compiled_autograd captures test_multiple_torch_compile model = torch nn Sequential torch nn Linear torch nn Sigmoid x = torch randn fn result = model x sum result backward model = torch nn Linear x = torch randn fn result = model x sum result backward no_ca = torch compile fn no_ca assertEqual counters compiled_autograd captures counters clear config patch compiled_autograd=True with_ca = torch compile fn with_ca assertEqual counters compiled_autograd captures counters clear no_ca = torch compile fn no_ca assertEqual counters compiled_autograd captures test_torch_compile_graph_break model = torch nn Sequential torch nn Linear torch nn Sigmoid x = torch randn torch _dynamo disable fn result = model x sum result backward config patch compiled_autograd=True opt_fn = torch compile fn opt_fn assertEqual counters compiled_autograd captures test_torch_compile_graph_break model = torch nn Sequential torch nn Linear torch nn Sigmoid x = torch randn torch _dynamo disable inner_fn loss loss backward fn result = model x sum inner_fn result config patch compiled_autograd=True opt_fn = torch compile fn opt_fn assertEqual counters compiled_autograd captures test_torch_compile_only_backward_call model = torch nn Sequential torch nn Linear torch nn Sigmoid x = torch randn result = model x sum config patch compiled_autograd=True opt_bwd = torch compile lambda result backward opt_bwd assertEqual counters compiled_autograd captures test_dynamo_boxed get_placeholders gm_ placeholders = node gm_ graph nodes node op == placeholder placeholders append node placeholders eager_with_check gm is_bwd inner_compiler gm_ example_inputs_ placeholders = get_placeholders gm_ is_bwd boxed inputs assert isinstance placeholders meta example_value list boxed inputs assert isinstance placeholders meta example_value list gm_ torch compile gm backend=inner_compiler bwd_compiler_fn = functools partial eager_with_check is_bwd=True fn inputs args_ args_ args_ = inputs out = torch mm args_ args_ out = torch mm out args_ loss = out sum compiled_autograd _enable bwd_compiler_fn loss backward yield args_ grad yield args_ grad yield args_ grad inputs = torch randn requires_grad=True torch randn requires_grad=True torch randn requires_grad=True compiled_fn = eager_with_check fn is_bwd=False grads = list compiled_fn inputs assertEqual len grads assertNotEqual grads None assertNotEqual grads None assertNotEqual grads None test_inputs_aliasing_bytecode_attr_mutations Freeze compiled autograd graph compiler = torch _dynamo compiled_autograd AutogradCompilerInstance compiler_fn param = torch ones active = torch ones inputs = param active _ proxies _ _ = compiler begin_capture inputs=inputs sizes= scalars= origins= accumulate_grad=False check_nans=False param_proxy activ_proxy = proxies buf = activ_proxy torch ops inductor accumulate_grad_ default param_proxy buf runtime_wrapper compiled_fn = compiler end_capture buf bytecode_hook code out_code dis sys sys version_info call_op = CALL_FUNCTION call_op = CALL insts = list dis get_instructions out_code call_graph_idx = next i i inst enumerate insts inst opname == call_op pre-graph should alias inputs_ref_ = inputs matches = inst inst insts call_graph_idx inst opname == STORE_FAST inst argval == inputs_ref_ assertTrue len matches == post-graph should access inputs_ref_ instead inputs matches = inst inst insts call_graph_idx inst argval == inputs assertTrue len matches == matches = inst inst insts call_graph_idx inst opname == LOAD_FAST inst argval == inputs_ref_ assertTrue len matches == torch _dynamo reset handle = torch _dynamo convert_frame register_bytecode_hook bytecode_hook try runtime_wrapper compiled_fn=compiled_fn inputs= param active sizes= scalars= hooks= packed_inputs= finally handle remove test_inputs_aliasing_bytecode_stack_restore logging getLogger setLevel logging WARNING torch testing _internal logging_tensor LoggingTensor Create graph allows inputs stealing forward inputs add = inputs + add_ = add + inputs handled suffix tensor subclass out = add_ cpu out gm = torch fx symbolic_trace forward torch _dynamo utils set_locals_to_steal gm inputs compiled_fn = torch compile gm inputs = torch ones dtype=torch float LoggingTensor torch ones match_done = False bytecode_hook code out_code dis sys nonlocal match_done test sensitive what Dynamo traces So soon main graph tested we skip bytecode hook checks future frames match_done sys version_info call_op = CALL_FUNCTION call_op = CALL insts = list dis get_instructions out_code call_graph_idx = next i i inst enumerate insts inst opname == call_op pre-graph should alias inputs_ref_ = inputs matches = inst inst insts call_graph_idx inst opname == STORE_FAST inst argval == inputs_ref_ assertTrue len matches == post-graph should access inputs_ref_ instead inputs matches = inst inst insts call_graph_idx inst argval == inputs assertTrue len matches == matches = inst inst insts call_graph_idx inst opname == LOAD_FAST inst argval == inputs_ref_ assertTrue len matches == match_done = True torch _dynamo reset handle = torch _dynamo convert_frame register_bytecode_hook bytecode_hook try compiled_fn inputs assertTrue len inputs == finally handle remove test_implicit_add fn y = torch randn requires_grad=True model x y used multiple times gradients get added torch sigmoid x y + torch sin y + torch cos y _ range x = torch randn result = model x sum result backward yield result yield y grad y grad = None check_output_and_recompiles fn test_output_nodes_all_leaves fn y = torch randn requires_grad=True z = torch randn requires_grad=True model x torch sigmoid x z + torch sin y + torch cos y _ range x = torch randn result = model x sum gy gz = torch autograd grad result inputs= y z assert y grad None assert z grad None yield gy yield gz check_output_and_recompiles fn test_output_nodes_some_leaves fn UnreachableBwd torch autograd Function staticmethod forward ctx x x staticmethod backward ctx gO raise RuntimeError y = torch randn requires_grad=True z = torch randn requires_grad=True model x torch sigmoid UnreachableBwd apply y z _ range x = torch randn result = model x sum gz = torch autograd grad result inputs= z assert y grad None assert z grad None yield gz check_output_and_recompiles fn test_no_output_nodes_all_leaves fn y = torch randn requires_grad=True z = torch randn requires_grad=True model x torch sigmoid x z + torch sin y + torch cos y _ range x = torch randn result = model x sum out = result backward assert out None assert y grad None assert z grad None yield y grad yield z grad y grad = None z grad = None check_output_and_recompiles fn test_no_output_nodes_some_leaves fn UnreachableBwd torch autograd Function staticmethod forward ctx x x staticmethod backward ctx gO raise RuntimeError y = torch randn requires_grad=True z = torch randn requires_grad=True = torch randn requires_grad=True model x torch sigmoid x y z UnreachableBwd apply _ range x = torch randn result = model x sum out = result backward inputs= y z assert out None assert y grad None assert z grad None assert grad None yield y grad yield z grad y grad = None z grad = None check_output_and_recompiles fn test_no_output_nodes_different_leaves_will_recompile fn fwd x y z out = x y MulBackward out = out z MulBackward out sum SumBackward x = torch randn requires_grad=True y = torch randn requires_grad=True z = torch randn requires_grad=True loss = fwd x y z torch compile lambda torch autograd backward loss inputs= x yield x grad x grad = None loss = fwd x y z torch compile lambda torch autograd backward loss inputs= y yield y grad Guarded TensorArg id mismatch last MulBackward check_output_and_recompiles fn test_dynamic_shapes fn model = torch nn Sequential torch nn Linear torch nn ReLU torch nn Linear torch nn ReLU opt_model = torch compile model dynamic=True b range x = torch randn b result = opt_model x sum result backward yield model weight grad yield model bias grad yield model weight grad yield model bias grad model zero_grad check_output_and_recompiles fn test_dynamic_shapes_from_forward ToyModel nn Module __init__ in_feat= hidden_feat= out_feat= super __init__ linear = nn Linear in_feat hidden_feat linear = nn Linear hidden_feat hidden_feat linear = nn Linear hidden_feat out_feat mse_loss = torch nn MSELoss forward inputs output out = linear inputs out = linear out out = linear out mse_loss out output m = ToyModel m = torch compile m run i torch _dynamo utils counters clear inp = torch randn i target = torch randn i loss = m inp target compiled_autograd _enable make_compiler_fn dynamic=None loss backward counters = torch _dynamo utils counters run assertEqual counters compiled_autograd captures assertEqual counters compiled_autograd compiles run assertEqual counters compiled_autograd captures assertEqual counters compiled_autograd compiles run assertEqual counters compiled_autograd captures assertEqual counters compiled_autograd compiles run assertEqual counters compiled_autograd captures assertEqual counters compiled_autograd compiles test_dynamic_shapes_eager_node Here we have no way marking symbolic sizes using SumBackward dynamic fn model = torch nn Sequential torch nn Linear torch nn ReLU torch nn Linear torch nn ReLU opt_model = torch compile model dynamic=True b s zip x = torch randn b result = opt_model x view = result view s - sum will save dynamic sizes loss = view sum loss backward yield model weight grad yield model bias grad yield model weight grad yield model bias grad model zero_grad check_output_and_recompiles fn test_dynamic_shapes_annotations torch compile f x x sin sin torch _dynamo compiled_autograd _enable torch compile x = torch randn requires_grad=True torch _dynamo mark_dynamic x out = f x out sum backward x = torch randn requires_grad=True torch _dynamo mark_dynamic x out = f x out sum backward mark_dynamic should cause ConstraintViolationError assertEqual counters compiled_autograd captures test_torch_compile_api_dynamic_shapes Here we have no way marking symbolic sizes using SumBackward dynamic fn call_backward model = torch nn Sequential torch nn Linear torch nn ReLU torch nn Linear torch nn ReLU b s zip x = torch randn b result = model x view = result view s - sum will save dynamic sizes loss = view sum call_backward loss yield model weight grad yield model bias grad yield model weight grad yield model bias grad model zero_grad call_backward loss loss backward eager_out = list fn call_backward config patch compiled_autograd=True compiled_out = list fn torch compile call_backward dynamic=True assertEqual counters compiled_autograd captures test_accumulate_without_zero fn model = torch nn Sequential torch nn Linear torch nn ReLU torch nn Linear torch nn ReLU opt_model = torch compile model dynamic=True _ range x = torch randn result = opt_model x sum result backward yield model weight grad clone yield model bias grad clone yield model weight grad clone yield model bias grad clone check_output_and_recompiles fn count= test_inplace_grad_update fn model = torch nn Sequential torch nn Linear torch nn ReLU opt_model = torch compile model dynamic=True _ range w_grad = torch rand_like model weight b_grad = torch rand_like model bias model weight grad = w_grad model bias grad = b_grad x = torch randn result = opt_model x sum result backward assert model weight grad w_grad assert model bias grad b_grad yield w_grad clone yield b_grad clone check_output_and_recompiles fn count= unittest skipIf HAS_GPU requires gpu test_issue DEVICE = torch device GPU_TYPE NUM_FEATURES = bias_sigmoid_mul x x bias x = torch sigmoid x + bias y = x x y bias_sigmoid_mul_jit = torch compile bias_sigmoid_mul ModuleWithJit nn Module __init__ - None super __init__ linear_ = nn Linear NUM_FEATURES NUM_FEATURES bias=True linear_ = nn Linear NUM_FEATURES NUM_FEATURES bias=False linear_ _bias = nn Parameter torch zeros NUM_FEATURES forward input_tensor x = linear_ input_tensor x = linear_ input_tensor output = bias_sigmoid_mul_jit x x linear_ _bias output Model nn Module __init__ - None super __init__ module_with_jit_ = ModuleWithJit module_with_jit_ = ModuleWithJit forward x gradient_checkpointing bool gradient_checkpointing y = torch utils checkpoint checkpoint _forward x use_reentrant=True y = _forward x y _forward x x = x + module_with_jit_ x x = x + module_with_jit_ x transpose - - transpose - - x device_interface = get_interface_for_device GPU_TYPE device_interface set_device device=DEVICE torch manual_seed model = Model model train model device=DEVICE model_parameters = list model parameters torch manual_seed input_tensor = torch randn NUM_FEATURES device=DEVICE input_tensor requires_grad = True target_tensor = torch randn NUM_FEATURES dtype=input_tensor dtype device=DEVICE _ range param model_parameters param grad = None output_tensor = model x=input_tensor clone gradient_checkpointing=True loss = torch mean torch abs target_tensor - output_tensor loss backward test_keep_graph_simple x = torch tensor requires_grad=True y = x First backward pass keep computation graph y backward retain_graph=True assertEqual x grad torch Tensor dy dx x= Note - will run under both eager compiled regime fn Reset gradients x grad = torch tensor Second Third backward pass keep computation graph y backward retain_graph=True assertEqual x grad torch Tensor dy dx x= x grad check_output_and_recompiles fn count= test_keep_graph_usage_after_compiled x = torch tensor requires_grad=True y = x First backward pass keep computation graph eager_check y backward retain_graph=True assertEqual x grad torch Tensor dy dx x= x grad = torch tensor eager_check _ range compiled_autograd _enable compiler_fn eager_check eager_check test_custom_fn_saved_tensors fn MySin torch autograd Function staticmethod forward ctx x ctx save_for_backward x torch sin x staticmethod backward ctx gO x = ctx saved_tensors gO torch cos x i x = torch arange i requires_grad=True out = MySin apply x loss = out sum loss backward yield x grad check_output_and_recompiles fn test_custom_fn_saved_multiple_tensors fn MyFn torch autograd Function staticmethod forward ctx x y ctx save_for_backward x y torch sin x torch sin y staticmethod backward ctx gO_x gO_y x y = ctx saved_tensors gO_x torch cos x gO_y torch cos y i x = torch arange i requires_grad=True y = torch arange i requires_grad=True out out = MyFn apply x y loss = out out sum loss backward yield x grad check_output_and_recompiles fn test_custom_fn_saved_multiple_tensors_dedup fn MyFn torch autograd Function staticmethod forward ctx x ctx save_for_backward x x torch sin x staticmethod backward ctx gO x x = ctx saved_tensors gO torch cos x torch cos x i x = torch arange i requires_grad=True out = MyFn apply x loss = out sum loss backward yield x grad check_output_and_recompiles fn test_custom_fn_saved_shape_tensor fn MyFn torch autograd Function staticmethod forward ctx x ctx save_for_backward x x staticmethod backward ctx gO x = ctx saved_tensors gO x shape i x = torch arange i requires_grad=True out = MyFn apply x loss = out sum loss backward yield x grad check_output_and_recompiles fn test_custom_fn_saved_attr fn MyFn torch autograd Function staticmethod forward ctx x ctx shape = x shape x staticmethod backward ctx gO x_shape = ctx shape gO x_shape i x = torch arange i requires_grad=True out = MyFn apply x loss = out sum loss backward yield x grad check_output_and_recompiles fn compiler_fn=make_compiler_fn fullgraph=False test_custom_fn_multiple_grads fn MyFn torch autograd Function staticmethod forward ctx x y x + y y staticmethod backward ctx gO_ gO_ gO_ gO_ i x = torch arange i requires_grad=True y = torch arange i requires_grad=True out out = MyFn apply x y loss = out + out sum loss backward yield x grad yield y grad check_output_and_recompiles fn test_custom_fn_non_variable_input fn MyFn torch autograd Function staticmethod forward ctx x y z x y z staticmethod backward ctx gO_ gO_ gO_ gO_ gO_ gO_ i x = torch arange i requires_grad=True y = z = torch arange i requires_grad=True out out out = MyFn apply x y z loss = out + out + out sum loss backward yield x yield y yield z check_output_and_recompiles fn unittest skipIf HAS_GPU requires gpu test_logging_tensor_flaky - None when you first run some test using triton then run test_inputs_aliasing_bytecode_stack_restore resulting - pytest ` TypeError unsupported operand type s + Tensor LoggingTensor ` - python ` TypeError all arguments converted during string formatting ` some triton involving test fn _fn x x x = torch arange requires_grad=True dtype=torch float device=GPU_TYPE out = _fn x loss = out sum loss backward compiled_autograd _enable compiler_fn fn logging getLogger setLevel logging WARNING triton setup overwrote INFO test_inputs_aliasing_bytecode_stack_restore torch testing _internal logging_tensor LoggingTensor forward inputs add = inputs + add_ = add + inputs out = add_ cpu out gm = torch fx symbolic_trace forward print gm print_readable torch _dynamo utils set_locals_to_steal gm inputs compiled_fn = torch compile gm inputs = torch ones dtype=torch float LoggingTensor torch ones compiled_fn inputs unittest skipIf HAS_GPU requires gpu test_custom_fn_output_metadata my_compiler_fn gm node gm graph nodes isinstance node target torch _ops OpOverload assert node target _name = aten _to_copy there should no implicit copies e g dtype casting inner_compiler gm_ example_inputs_ counters compiled_autograd compiles += inductor compile gm_ example_inputs_ torch compile gm backend=inner_compiler fullgraph=True dynamic=True fn MyFn torch autograd Function staticmethod forward ctx x x staticmethod backward ctx gO gO x = torch arange requires_grad=True dtype=torch float device=GPU_TYPE x_view = x view out = MyFn apply x_view loss = out sum loss backward yield x dtype yield x device yield x grad check_output_and_recompiles fn count= test_custom_fn_with_same_graph fn MyFn torch autograd Function staticmethod forward ctx x x staticmethod backward ctx gO gO same MyFn different autograd function id should using same graph MyFn MyFn torch autograd Function staticmethod forward ctx x x staticmethod backward ctx gO gO myfn MyFn MyFn MyFn MyFn x = torch arange requires_grad=True out = myfn apply x loss = out sum loss backward yield x grad check_output_and_recompiles fn count= should compile once MyFn once MyFn test_custom_fn_dynamically_defined_class fn create_class multiplier int DynamicFn torch autograd Function staticmethod forward ctx x x multiplier staticmethod backward ctx gO gO multiplier DynamicFn multiplier x = torch arange requires_grad=True out = create_class multiplier apply x loss = out sum loss backward yield x grad check_output_and_recompiles fn count= test_custom_fn_bw_graph_break fn MySin torch autograd Function staticmethod forward ctx x ctx save_for_backward x torch sin x staticmethod backward ctx gO print graph break x = ctx saved_tensors print graph break gO torch cos x i x = torch arange i requires_grad=True out = MySin apply x loss = out sum loss backward yield x grad check_output_and_recompiles fn count= compiler_fn=make_compiler_fn fullgraph=False test_custom_fn_compiled_fw_graph_break fn MySin torch autograd Function staticmethod forward ctx x print graph break ctx save_for_backward x torch sin x staticmethod backward ctx gO x = ctx saved_tensors gO torch cos x opt_model = torch compile MySin apply i x = torch arange i requires_grad=True out = opt_model x loss = out sum loss backward yield x grad check_output_and_recompiles fn count= compiler_fn=make_compiler_fn fullgraph=False assertEqual counters stats unique_graphs fw bw test_custom_fn_compiled_fw_bw_graph_break fn MySin torch autograd Function staticmethod forward ctx x print graph break ctx save_for_backward x torch sin x staticmethod backward ctx gO print graph break x = ctx saved_tensors gO torch cos x opt_model = torch compile MySin apply i x = torch arange i requires_grad=True out = opt_model x loss = out sum loss backward yield x grad check_output_and_recompiles fn count= compiler_fn=make_compiler_fn fullgraph=False assertEqual counters stats unique_graphs fw bw test_mismatch_fake_tensor_mode dynamic_shape=False Repro failure training nanogpt both compiled-autograd _LazyGraphModule Check https github com pytorch pytorch pull more context B = x = torch rand B y = torch rand B requires_grad=True dynamic_shape torch _dynamo mark_dynamic x torch _dynamo mark_dynamic y f y grad = None out = x + y make sure backward call does trigger any error when compiling backward graph out sum backward out y grad check_output_and_recompiles f compile_fn=True test_mismatch_fake_tensor_mode_dynamic_shape test_mismatch_fake_tensor_mode dynamic_shape=True test_accumulate_grad_accuracy fn model = torch nn Sequential torch nn Linear bias=False torch nn Linear bias=False x = torch randn out = model x loss = out sum torch manual_seed loss backward yield model weight grad yield model weight grad check_output_and_recompiles fn test_trace_run_with_rng_state sdpa xq xk F scaled_dot_product_attention xq xk xk is_causal=True g xq_ xk_ xq_ xk_ xq bs n_local_heads seqlen head_dim xk bs n_local_heads cache_len + seqlen head_dim y = sdpa xq_ xk_ y = torch utils checkpoint checkpoint sdpa xq_ xk_ use_reentrant=False y = torch mul y y z = torch matmul y y z f bs = n_local_heads = seqlen = head_dim = cache_len = xq_list = torch ones bs n_local_heads seqlen head_dim requires_grad=True device= cpu _ range xk_list = torch ones bs n_local_heads cache_len + seqlen head_dim requires_grad=True device= cpu _ range out = torch compile g fullgraph=True xq_list xk_list xq_list xk_list out sum backward out x grad x xq_list + xk_list Walkthrough what happens ` run_with_rng_state ` ` run_with_rng_state ` only shows up backward graph op inserted partitioner The Dynamo graph captured Compiled Autograd looks like ` ` ` ===== __compiled_fn_ ===== torch fx _lazy_graph_module py GraphModule torch nn Module forward L_inputs_ list run_with_rng_state = torch ops higher_order run_with_rng_state getitem_ torch ops aten _scaled_dot_product_flash_attention_for_cpu default getitem_ getitem_ getitem_ True ` ` ` We want preserve ` run_with_rng_state ` op when going through AOTAutograd We do having special handling ` run_with_rng_state ` op s py_functionalize_impl _run_with_rng_state_op_check inductor_post_grad_graph Checks ` run_with_rng_state ` op exists Compiled Autograd s Inductor post-grad graph op_set = node target node inductor_post_grad_graph nodes torch ops higher_order run_and_save_rng_state op_set This backward graph so check existence ` run_with_rng_state ` op assertTrue torch ops higher_order run_with_rng_state op_set torch _inductor config patch post_grad_custom_post_pass=_run_with_rng_state_op_check compiler_fn = make_compiler_fn fullgraph=True make_compiler_fn_with_op_check _compiler_fn gm Checks ` run_with_rng_state ` op exists Compiled Autograd s Dynamo graph assertTrue any node target torch ops higher_order run_with_rng_state node gm graph nodes compiler_fn gm _compiler_fn compiler_fn_with_op_check = make_compiler_fn_with_op_check check_output_and_recompiles f compiler_fn=compiler_fn_with_op_check compile_fn=False torch _inductor config patch enable_auto_functionalized_v =True test_trace_auto_functionalized_v trace_auto_functionalized_base torch _inductor config patch enable_auto_functionalized_v =False test_trace_auto_functionalized trace_auto_functionalized_base trace_auto_functionalized_base torch library _scoped_library testlib FRAGMENT lib torch library define testlib foo Tensor x - Tensor tags=torch Tag pt _compliant_tag lib=lib torch library define testlib foo_mutated Tensor x - Tensor tags=torch Tag pt _compliant_tag lib=lib torch library impl testlib foo cpu lib=lib foo x x add_ x torch library impl testlib foo Meta lib=lib foo_meta x x torch library impl testlib foo_mutated CompositeImplicitAutograd lib=lib foo_mutated x torch ops testlib foo x _get_custom_policy must_recompute_list=None _custom_policy ctx func args kwargs must_recompute_list None func must_recompute_list torch utils checkpoint CheckpointPolicy MUST_RECOMPUTE torch utils checkpoint CheckpointPolicy PREFER_RECOMPUTE _custom_policy context_fn must_recompute_list = torch ops higher_order auto_functionalized torch utils checkpoint create_selective_checkpoint_contexts _get_custom_policy must_recompute_list=must_recompute_list g x x = torch matmul x x torch ops testlib foo_mutated x torch matmul x x g_cp x torch utils checkpoint checkpoint g x use_reentrant=False context_fn=context_fn f inps = torch randn requires_grad=True output = torch compile g_cp backend= aot_eager fullgraph=True inps output sum backward output inps grad Walkthrough what happens ` auto_functionalized ` ` auto_functionalized ` op inserted into graph during AOTAutograd functionalization We force op recomputed using SAC so appears backward graph The AOT backward graph looks like ` ` ` ===== Backward graph ===== forward primals_ f cpu tangents_ f cpu X = torch ops higher_order auto_functionalized torch ops testlib foo default x = mm add_ ` ` ` The Compiled Autograd graph looks like ` ` ` ===== Compiled autograd graph ===== forward inputs sizes scalars hooks X = torch ops higher_order auto_functionalized torch ops testlib foo default x = aot _mm ` ` ` The Dynamo graph captured Compiled Autograd looks like ` ` ` ===== __compiled_fn_ ===== forward L_inputs_ list X = torch ops higher_order auto_functionalized torch ops testlib foo default x = aot _mm new_grad ` ` ` The Compiled Autograd s AOT forward-only graph looks like ` ` ` ===== Forward graph ===== forward arg _ f cpu arg _ f cpu X = torch ops higher_order auto_functionalized torch ops testlib foo default x = mm clone_ ` ` ` The ` auto_functionalized ` op should then lowered using normal lowering path Inductor compiler_fn = make_compiler_fn fullgraph=True backend= aot_eager make_compiler_fn_with_op_check _compiler_fn gm auto_functionalize_func = torch ops higher_order auto_functionalized torch _inductor config enable_auto_functionalized_v torch ops higher_order auto_functionalized_v Checks ` auto_functionalized ` op exists Compiled Autograd s Dynamo graph assertTrue any node target auto_functionalize_func node gm graph nodes f auto_functionalize_func op found gm graph compiler_fn gm _compiler_fn compiler_fn_with_op_check = make_compiler_fn_with_op_check check_output_and_recompiles f compiler_fn=compiler_fn_with_op_check compile_fn=False scoped_load_inline test_autograd_cpp_node_non_traceable load_inline cpp_source = struct CustomOpAutogradFunction public torch autograd Function CustomOpAutogradFunction static constexpr bool is_traceable = false static torch Tensor forward torch autograd AutogradContext ctx const torch Tensor x x static torch autograd variable_list backward torch autograd AutogradContext ctx torch autograd variable_list grad_output grad_output torch Tensor custom_op_backed_by_autograd_fn torch Tensor x CustomOpAutogradFunction apply x TORCH_LIBRARY test_non_traceable_autograd_cpp_node m m custom_op_backed_by_autograd_fn custom_op_backed_by_autograd_fn module = load_inline name= test_non_traceable_autograd_cpp_node cpp_sources=cpp_source functions= custom_op_backed_by_autograd_fn verbose=True fn x = torch ones requires_grad=True out = module custom_op_backed_by_autograd_fn x loss = out sum loss backward yield x grad should raise check_output_and_recompiles fn count= compiler_fn=make_compiler_fn fullgraph=False parametrize is_traceable True False scoped_load_inline test_autograd_cpp_node_basic load_inline is_traceable cpp_source = Template struct CustomOpAutogradFunction public torch autograd Function CustomOpAutogradFunction static constexpr bool is_traceable = $ is_traceable static torch Tensor forward torch autograd AutogradContext ctx const torch Tensor x x static torch autograd variable_list backward torch autograd AutogradContext ctx torch autograd variable_list grad_output grad_output torch Tensor custom_op_backed_by_autograd_fn torch Tensor x CustomOpAutogradFunction apply x TORCH_LIBRARY test_autograd_cpp_node_basic_$ is_traceable m m custom_op_backed_by_autograd_fn custom_op_backed_by_autograd_fn module = load_inline name= test_autograd_cpp_node_basic cpp_sources=cpp_source substitute is_traceable= true is_traceable false functions= custom_op_backed_by_autograd_fn verbose=True fn i x = torch ones i i requires_grad=True out = module custom_op_backed_by_autograd_fn x loss = out sum loss backward yield x grad is_traceable check_output_and_recompiles fn compiles static dynamic each graph break check_output_and_recompiles fn count= compiler_fn=make_compiler_fn fullgraph=False parametrize is_traceable True False scoped_load_inline test_autograd_cpp_node_id load_inline is_traceable cpp_source = Template struct CustomOpAutogradFunction public torch autograd Function CustomOpAutogradFunction static constexpr bool is_traceable = $ is_traceable static torch Tensor forward torch autograd AutogradContext ctx const torch Tensor x x static torch autograd variable_list backward torch autograd AutogradContext ctx torch autograd variable_list grad_output grad_output struct CustomOpAutogradFunction public torch autograd Function CustomOpAutogradFunction static constexpr bool is_traceable = $ is_traceable static torch Tensor forward torch autograd AutogradContext ctx const torch Tensor x x static torch autograd variable_list backward torch autograd AutogradContext ctx torch autograd variable_list grad_output grad_output torch Tensor custom_op_backed_by_autograd_fn torch Tensor x CustomOpAutogradFunction apply x torch Tensor custom_op_backed_by_autograd_fn torch Tensor x CustomOpAutogradFunction apply x TORCH_LIBRARY test_autograd_cpp_node_id_$ is_traceable m m custom_op_backed_by_autograd_fn custom_op_backed_by_autograd_fn m custom_op_backed_by_autograd_fn custom_op_backed_by_autograd_fn module = load_inline name= test_autograd_cpp_node_id cpp_sources=cpp_source substitute is_traceable= true is_traceable false functions= custom_op_backed_by_autograd_fn custom_op_backed_by_autograd_fn verbose=True same_autograd_fn fn x = torch ones requires_grad=True out = module custom_op_backed_by_autograd_fn x loss = out sum loss backward yield x grad yield fn compile yield fn reuse yield fn reuse yield fn reuse is_traceable check_output_and_recompiles same_autograd_fn check_output_and_recompiles same_autograd_fn count= compiler_fn=make_compiler_fn fullgraph=False different_autograd_fn fn op x = torch ones requires_grad=True out = op x loss = out sum loss backward yield x grad op = module custom_op_backed_by_autograd_fn op = module custom_op_backed_by_autograd_fn yield fn op compile yield fn op compile yield fn op reuse yield fn op reuse is_traceable check_output_and_recompiles different_autograd_fn check_output_and_recompiles same_autograd_fn count= compiler_fn=make_compiler_fn fullgraph=False parametrize is_traceable True False scoped_load_inline test_autograd_cpp_node_saved_basic load_inline is_traceable cpp_source = Template struct CustomOpAutogradFunction public torch autograd Function CustomOpAutogradFunction static constexpr bool is_traceable = $ is_traceable static torch Tensor forward torch autograd AutogradContext ctx const torch Tensor x const torch Tensor y const torch Tensor fixed ctx- save_for_backward x y ctx- saved_data fixed_tensor = fixed ctx- saved_data bool = true ctx- saved_data int = c List std string list string ctx- saved_data list = std move list c Dict std string double dict dict insert string ctx- saved_data dict = std move dict x static torch autograd variable_list backward torch autograd AutogradContext ctx torch autograd variable_list grad_output const auto saved_variables = ctx- get_saved_variables assert saved_variables size == torch Tensor x = saved_variables torch Tensor y = saved_variables torch Tensor fixed = ctx- saved_data fixed_tensor toTensor assert ctx- saved_data bool isBool c SymInt i = ctx- saved_data int toSymInt c List c IValue list = ctx- saved_data list toList assert list size == assert list get toStringRef == string c Dict c IValue c IValue dict = ctx- saved_data dict toGenericDict assert dict size == assert dict string == torch autograd variable_list grad_inputs grad_inputs = x + y + torch sum fixed + i grad_inputs torch Tensor custom_op_backed_by_autograd_fn const torch Tensor x const torch Tensor y const torch Tensor fixed CustomOpAutogradFunction apply x y fixed TORCH_LIBRARY test_autograd_cpp_node_saved_basic_$ is_traceable m m custom_op_backed_by_autograd_fn custom_op_backed_by_autograd_fn module = load_inline name= test_autograd_cpp_node_saved_basic cpp_sources=cpp_source substitute is_traceable= true is_traceable false functions= custom_op_backed_by_autograd_fn verbose=True fn fixed = torch ones i x = torch ones i i requires_grad=True y = torch randn i i out = module custom_op_backed_by_autograd_fn x y fixed loss = out sum loss backward yield x grad is_traceable check_output_and_recompiles fn check_output_and_recompiles fn count= compiler_fn=make_compiler_fn fullgraph=False parametrize is_traceable True False scoped_load_inline test_autograd_cpp_node_saved_dynamic load_inline is_traceable cpp_source = Template struct CustomOpAutogradFunction public torch autograd Function CustomOpAutogradFunction static constexpr bool is_traceable = $ is_traceable static torch Tensor forward torch autograd AutogradContext ctx const torch Tensor x ctx- save_for_backward x ctx- saved_data dynamic = x view - x static torch autograd variable_list backward torch autograd AutogradContext ctx torch autograd variable_list grad_output const auto saved_variables = ctx- get_saved_variables assert saved_variables size == torch Tensor x = saved_variables torch Tensor z = ctx- saved_data dynamic toTensor torch autograd variable_list grad_inputs grad_inputs = x + torch sum z grad_inputs torch Tensor custom_op_backed_by_autograd_fn const torch Tensor x CustomOpAutogradFunction apply x TORCH_LIBRARY test_autograd_cpp_node_saved_dynamic_$ is_traceable m m custom_op_backed_by_autograd_fn custom_op_backed_by_autograd_fn module = load_inline name= test_autograd_cpp_node_saved_dynamic cpp_sources=cpp_source substitute is_traceable= true is_traceable false functions= custom_op_backed_by_autograd_fn verbose=True fn i x = torch ones i i requires_grad=True out = module custom_op_backed_by_autograd_fn x loss = out sum loss backward yield x grad compiles static dynamic is_traceable check_output_and_recompiles fn check_output_and_recompiles fn count= compiler_fn=make_compiler_fn fullgraph=False parametrize is_traceable True False scoped_load_inline test_autograd_cpp_node_saved_int load_inline is_traceable cpp_source = Template struct CustomOpAutogradFunction public torch autograd Function CustomOpAutogradFunction static constexpr bool is_traceable = $ is_traceable static torch Tensor forward torch autograd AutogradContext ctx const torch Tensor x int _t y ctx- save_for_backward x ctx- saved_data int = y ctx- saved_data symint = c SymInt y x static torch autograd variable_list backward torch autograd AutogradContext ctx torch autograd variable_list grad_output const auto saved_variables = ctx- get_saved_variables assert saved_variables size == torch Tensor x = saved_variables c SymInt y = ctx- saved_data int toSymInt c SymInt ys = ctx- saved_data symint toSymInt torch autograd variable_list grad_inputs grad_inputs = x + y + ys grad_inputs torch Tensor custom_op_backed_by_autograd_fn const torch Tensor x int _t y CustomOpAutogradFunction apply x y TORCH_LIBRARY test_autograd_cpp_node_saved_int_$ is_traceable m m custom_op_backed_by_autograd_fn custom_op_backed_by_autograd_fn module = load_inline name= test_autograd_cpp_node_saved_int cpp_sources=cpp_source substitute is_traceable= true is_traceable false functions= custom_op_backed_by_autograd_fn verbose=True fn y x = torch ones requires_grad=True out = module custom_op_backed_by_autograd_fn x y loss = out sum loss backward yield x grad is_traceable check_output_and_recompiles fn check_output_and_recompiles fn count= compiler_fn=make_compiler_fn fullgraph=False parametrize is_traceable True False scoped_load_inline test_autograd_cpp_node_saved_float load_inline is_traceable cpp_source = Template struct CustomOpAutogradFunction public torch autograd Function CustomOpAutogradFunction static constexpr bool is_traceable = $ is_traceable static torch Tensor forward torch autograd AutogradContext ctx const torch Tensor x double z ctx- save_for_backward x ctx- saved_data float = z ctx- saved_data symfloat = c SymFloat z x static torch autograd variable_list backward torch autograd AutogradContext ctx torch autograd variable_list grad_output const auto saved_variables = ctx- get_saved_variables assert saved_variables size == torch Tensor x = saved_variables c SymFloat z = ctx- saved_data float toSymFloat c SymFloat zs = ctx- saved_data symfloat toSymFloat torch autograd variable_list grad_inputs grad_inputs = x + z + zs grad_inputs torch Tensor custom_op_backed_by_autograd_fn const torch Tensor x double z CustomOpAutogradFunction apply x z TORCH_LIBRARY test_autograd_cpp_node_saved_float_$ is_traceable m m custom_op_backed_by_autograd_fn custom_op_backed_by_autograd_fn module = load_inline name= test_autograd_cpp_node_saved_float cpp_sources=cpp_source substitute is_traceable= true is_traceable false functions= custom_op_backed_by_autograd_fn verbose=True fn z x = torch ones requires_grad=True out = module custom_op_backed_by_autograd_fn x z loss = out sum loss backward yield x grad is_traceable compiled autograd dynamo both support symfloat backend check_output_and_recompiles fn restart analysis due specialize_float=False assertEqual counters stats unique_graphs check_output_and_recompiles fn count= compiler_fn=make_compiler_fn fullgraph=False assertEqual counters stats unique_graphs parametrize is_traceable True False scoped_load_inline test_autograd_cpp_node_data_dependent load_inline is_traceable cpp_source = Template struct CustomOpAutogradFunction public torch autograd Function CustomOpAutogradFunction static constexpr bool is_traceable = $ is_traceable static int iteration static torch autograd variable_list forward torch autograd AutogradContext ctx const torch Tensor x const torch Tensor y ctx- save_for_backward x y ctx- saved_data bool = true ctx- saved_data int = switch iteration case break case recompile ctx- saved_data forces_recompile = iteration break case recompile ctx- set_materialize_grads false break case reuse break default throw std runtime_error unexpected iteration iteration++ x y static torch autograd variable_list backward torch autograd AutogradContext ctx torch autograd variable_list grad_output const auto saved_variables = ctx- get_saved_variables assert saved_variables size == torch Tensor x = saved_variables torch Tensor y = saved_variables c SymInt i = ctx- saved_data int toSymInt torch autograd variable_list grad_inputs grad_inputs = x + y + i grad_inputs int CustomOpAutogradFunction iteration = torch autograd variable_list custom_op_backed_by_autograd_fn const torch Tensor x const torch Tensor y CustomOpAutogradFunction apply x y void reset CustomOpAutogradFunction iteration = TORCH_LIBRARY test_autograd_cpp_node_data_dependent_$ is_traceable m m custom_op_backed_by_autograd_fn custom_op_backed_by_autograd_fn m reset reset module = load_inline name= test_autograd_cpp_node_data_dependent cpp_sources=cpp_source substitute is_traceable= true is_traceable false functions= custom_op_backed_by_autograd_fn reset verbose=True fn module reset i x = torch ones i i requires_grad=True y = torch randn i i out out = module custom_op_backed_by_autograd_fn x y loss = out + out sum loss backward yield x grad is_traceable check_output_and_recompiles fn check_output_and_recompiles fn count= compiler_fn=make_compiler_fn fullgraph=False unittest skipIf HAS_GPU requires gpu test_free_activation_memory script = torch torch _dynamo device_interface get_interface_for_device torch testing _internal inductor_utils GPU_TYPE main device_interface = get_interface_for_device GPU_TYPE assert device_interface memory_allocated == Use op check memory freed time op executed assertion_impl to_clone mem_allocated = device_interface memory_allocated assert mem_allocated some activations should freed to_clone clone torch library _scoped_library test_compiled_autograd FRAGMENT lib lib define assertion_op Tensor x - Tensor tags= torch Tag pt _compliant_tag lib impl assertion_op assertion_impl CPU lib impl assertion_op lambda x x clone Meta Create graph allows inputs stealing forward activations add = activations + out = add cpu cloned_out = torch ops test_compiled_autograd assertion_op out cloned_out gm = torch fx symbolic_trace forward torch _dynamo utils set_locals_to_steal gm activations compiled_fn = torch compile gm allocate least bytes bytes activations = torch ones dtype=torch float device=GPU_TYPE assert device_interface memory_allocated out = compiled_fn activations assert len activations == main run_as_subprocess script unittest skipIf HAS_GPU requires gpu test_free_activation_memory_subclass cover case when aot inputs have subclasses resulting different runtime wrapper script = torch torch _dynamo device_interface get_interface_for_device torch testing _internal inductor_utils GPU_TYPE main device_interface = get_interface_for_device GPU_TYPE assert device_interface memory_allocated == Use op check memory freed time op executed assertion_impl to_clone mem_allocated = device_interface memory_allocated assert mem_allocated some activations should freed assert mem_allocated currently subclasses don t seem freed inductor to_clone clone torch library _scoped_library test_compiled_autograd FRAGMENT lib lib define assertion_op Tensor x - Tensor tags= torch Tag pt _compliant_tag lib impl assertion_op assertion_impl CPU lib impl assertion_op lambda x x clone Meta lib impl assertion_op lambda x x clone NestedTensor fn inputs _ y = inputs out = y cpu cloned_out = torch ops test_compiled_autograd assertion_op out cloned_out gm = torch fx symbolic_trace fn torch _dynamo utils set_locals_to_steal gm inputs compiled_fn = torch compile gm torch nested _internal nested_tensor jagged_from_list activations = jagged_from_list torch ones device=GPU_TYPE bytes torch ones device=GPU_TYPE bytes None NestedTensor torch ones device=GPU_TYPE bytes bytes bytes assert device_interface memory_allocated out = compiled_fn activations assert len activations == main run_as_subprocess script test_callback_graph_break_throws_error called = callback_final called += MyFunc torch autograd Function staticmethod forward ctx input input staticmethod torch autograd function once_differentiable backward ctx grad torch autograd Variable _execution_engine queue_callback callback_final torch _dynamo graph_break grad = torch rand requires_grad=True assertRaisesRegex AssertionError only supported when Compiled Autograd enabled fullgraph=True compiled_autograd _enable make_compiler_fn fullgraph=False b = MyFunc apply b sum backward requires_cuda_and_triton test_cudagraphs_cpu_division torch _dynamo testing reduce_to_scalar_loss model = torch nn Linear dtype=torch float cuda inputs = torch randn dtype=torch float cuda out = model inputs loss = reduce_to_scalar_loss out stderr_msgs = io StringIO mock patch sys stderr stderr_msgs compiled_autograd _enable compiler_fn torch _inductor config triton cudagraphs = True loss backward torch _inductor config triton cudagraphs = False inductor_config cpp_wrapper assertIn skipping cudagraphs stderr_msgs getvalue assertEqual counters inductor cudagraph_skips assertNotIn skipping cudagraphs stderr_msgs getvalue assertEqual counters inductor cudagraph_skips test_cudagraphs_cpu_graph torch _dynamo testing reduce_to_scalar_loss model = torch nn Linear dtype=torch float inputs = torch randn dtype=torch float out = model inputs loss = reduce_to_scalar_loss out compiled_autograd _enable compiler_fn torch _inductor config triton cudagraphs = True loss backward torch _inductor config triton cudagraphs = False assertEqual counters inductor cudagraph_skips requires_cuda_and_triton test_cudagraphs_sdpa query = torch rand dtype=torch float device= cuda requires_grad=True key = torch rand dtype=torch float device= cuda value = torch rand dtype=torch float device= cuda out = torch nn functional scaled_dot_product_attention query key value config patch compiled_autograd=True inductor_config patch triton cudagraphs True opt_bwd = torch compile lambda out sum backward opt_bwd assertEqual counters compiled_autograd captures assertEqual counters inductor cudagraph_skips inductor_config cpp_wrapper requires_cuda_and_triton test_cudagraphs_cpu_scalar_used_in_python_custom_op MyFn torch autograd Function staticmethod forward ctx x cpu_tensor = torch tensor ctx save_for_backward x cpu_tensor visible c++ autograd ctx cpu_scalar = opaque c++ autograd x sum staticmethod backward ctx gO x cpu_tensor = ctx saved_tensors expand = gO torch ones_like x expand cpu_tensor ctx cpu_scalar x = torch randn requires_grad=True device= cuda out = MyFn apply x config patch compiled_autograd=True inductor_config patch triton cudagraphs True opt_bwd = torch compile lambda out backward opt_bwd assertEqual counters compiled_autograd captures Compiled autograd lifts custom autograd Function bwd instead tracing Must skip since we do know cpu scalar will used only ATen prim ops inductor_config graph_partition instead skipping cudagraph graph partition splits off cpu inputs outputs ops cudagraphify remaining computation So there no cudagraph skip expected_cudagraph_skips = expected_cudagraph_skips = assertEqual counters inductor cudagraph_skips expected_cudagraph_skips scoped_load_inline requires_cuda_and_triton test_cudagraphs_cpu_scalar_used_in_cpp_custom_op load_inline cpp_source = struct CustomOpAutogradFunction public torch autograd Function CustomOpAutogradFunction static constexpr bool is_traceable = true static torch Tensor forward torch autograd AutogradContext ctx const torch Tensor x const auto cpu_tensor = torch tensor ctx- save_for_backward x cpu_tensor ctx- saved_data cpu_scalar = x static torch autograd variable_list backward torch autograd AutogradContext ctx torch autograd variable_list grad_output const auto saved_variables = ctx- get_saved_variables assert saved_variables size == torch Tensor x = saved_variables torch Tensor cpu_tensor = saved_variables int cpu_scalar = ctx- saved_data cpu_scalar toInt auto expand = grad_output torch ones_like x torch autograd variable_list grad_inputs grad_inputs = expand cpu_tensor cpu_scalar autograd engine asserts tensors same device grad_inputs torch Tensor custom_op_backed_by_autograd_fn const torch Tensor x CustomOpAutogradFunction apply x TORCH_LIBRARY test_cudagraphs_cpu_scalar_used_in_cpp_custom_op m m custom_op_backed_by_autograd_fn custom_op_backed_by_autograd_fn module = load_inline name= test_cudagraphs_cpu_scalar_used_in_cpp_custom_op cpp_sources=cpp_source functions= custom_op_backed_by_autograd_fn verbose=True x = torch randn requires_grad=True device= cuda config patch compiled_autograd=True inductor_config patch triton cudagraphs True out = torch ops test_cudagraphs_cpu_scalar_used_in_cpp_custom_op custom_op_backed_by_autograd_fn x opt_bwd = torch compile lambda out sum backward opt_bwd assertEqual counters compiled_autograd captures Compiled autograd s initial capture lifts custom C++ autograd Function bwd instead tracing into We must skip since we do know cpu scalar will used only ATen prim ops In future we can consider having cpu scalar movement pass sometime after we trace into custom C++ autograd Function like AOTDispatcher inductor_config graph_partition instead skipping cudagraph graph partition splits off cpu inputs outputs ops cudagraphify remaining computation So there no cudagraph skip expected_cudagraph_skips = inductor_config cpp_wrapper expected_cudagraph_skips = expected_cudagraph_skips = assertEqual counters inductor cudagraph_skips expected_cudagraph_skips test_logs logs ctx = logs_to_string torch _dynamo compiled_autograd __name__ compiled_autograd compiled_autograd _enable compiler_fn ctx torch randn requires_grad=True sum backward assertEqual counters compiled_autograd captures assertEqual counters compiled_autograd compiles assert torch autograd AccumulateGrad NodeCall logs getvalue assert gen_cache_miss_log_prefix + torch autograd GraphRoot logs getvalue test_logs_aot_bwd_reuse torch compile backend= aot_eager fn x x sum compiled_autograd _enable compiler_fn x = torch randn requires_grad=True y = torch randn requires_grad=True z = torch randn requires_grad=True reuse same AOT bwd graph times out = fn x + fn y + fn z out backward should RuntimeError Node redefined name aot _expand test_verbose_logs_graph fn model = torch nn Sequential torch nn Linear torch nn ReLU torch nn Linear torch nn ReLU x = torch randn result = model x sum result backward yield model weight grad yield model bias grad yield model weight grad yield model bias grad logs ctx = logs_to_string torch _dynamo compiled_autograd __name__ compiled_autograd_verbose ctx check_output_and_recompiles fn expected_logs = torch autograd GraphRoot NodeCall ReluBackward NodeCall AddmmBackward NodeCall ReluBackward NodeCall TBackward NodeCall torch autograd AccumulateGrad NodeCall torch autograd AccumulateGrad NodeCall TBackward NodeCall torch autograd AccumulateGrad NodeCall SumBackward NodeCall ReluBackward NodeCall AddmmBackward NodeCall torch autograd AccumulateGrad NodeCall TBackward NodeCall torch autograd AccumulateGrad NodeCall ReluBackward NodeCall AddmmBackward NodeCall torch autograd AccumulateGrad NodeCall TBackward NodeCall torch autograd AccumulateGrad NodeCall torch autograd AccumulateGrad NodeCall found = line logs getvalue split \n found == len expected_logs break expected_logs found line found += assertEqual found len expected_logs mock patch torch _functorch aot_autograd AOT_COUNTER new_callable=itertools count mock patch torch _dynamo config inline_inbuilt_nn_modules True test_verbose_logs_aot_id _ fn model = torch nn Sequential torch nn Linear torch nn ReLU torch nn Linear torch nn ReLU x = torch randn torch compile forward model x model x result = forward model x sum result backward yield model weight grad yield model bias grad yield model weight grad yield model bias grad logs ctx = logs_to_string torch _dynamo compiled_autograd __name__ compiled_autograd_verbose ctx check_output_and_recompiles fn expected_logs = code CompiledFunctionBackward NodeCall found = line logs getvalue split \n found == len expected_logs break expected_logs found line found += assertEqual found len expected_logs mock patch torch _functorch aot_autograd AOT_COUNTER new_callable=itertools count test_verbose_logs_aot_dispatcher_nodes _ fn torch compile f x tmp = x sin tmp = x cos torch _dynamo graph_break tmp sin + tmp cos x = torch randn requires_grad=True out = f x out sum backward yield x grad logs ctx = logs_to_string torch _dynamo compiled_autograd __name__ compiled_autograd_verbose ctx check_output_and_recompiles fn expected_logs = CompiledFunctionBackward aot _sin_ aot _neg aot _tangents_ aot _cos_ aot _tangents_ CompiledFunctionBackward aot _sin_ aot _neg aot _mul aot _cos_ aot _mul_ aot _add assertEqual sum e expected_logs e logs getvalue len expected_logs mock patch torch _functorch aot_autograd AOT_COUNTER new_callable=itertools count test_verbose_logs_aot_dispatcher_nodes_hop _ dataclasses dataclass CustomObj val torch Tensor fn x obj y = x sin closure_var = y + y register_hook lambda grad grad + obj val + closure_var z = y sin z opt_fn = torch compile fn x = torch ones requires_grad=True y = torch ones requires_grad=True obj = CustomObj torch tensor fn x obj sum backward logs ctx = logs_to_string torch _dynamo compiled_autograd __name__ compiled_autograd_verbose ctx compiled_autograd _enable compiler_fn opt_fn y obj sum backward assertEqual x grad y grad expected_logs = CompiledFunctionBackward aot _primals_ aot _tangents_ aot _tangents_ aot _sin aot _cos aot _mul aot _add_ aot _trace_wrapped aot _cos_ aot _mul_ assertEqual sum e expected_logs e logs getvalue len expected_logs test_verbose_logs_cpp torch _logging set_logs compiled_autograd_verbose=True fn model = torch nn Sequential torch nn Linear torch nn ReLU torch nn Linear torch nn ReLU i model zero_grad x = torch randn i result = model x sum result backward yield model weight grad yield model bias grad yield model weight grad yield model bias grad logs ctx = logs_to_string torch _dynamo compiled_autograd __name__ compiled_autograd_verbose ctx check_output_and_recompiles fn patterns = r + gen_cache_miss_log_prefix + r torch autograd GraphRoot \ NodeCall \ key size \d+ previous key sizes=\ \ \n all_logs = logs getvalue pattern = r join patterns matches = re findall pattern all_logs assertEqual len matches assert isinstance matches str single match matches = match multiple matches matches = match match assertEqual len matches len patterns skipIfWindows msg= node name demangling inconsistent windows test_verbose_logs_dynamic_shapes logs ctx = logs_to_string torch _dynamo compiled_autograd __name__ compiled_autograd_verbose model = torch nn Sequential torch nn Linear torch nn ReLU torch nn Linear torch nn ReLU i j zip model zero_grad x = torch randn i y = torch randn j result = model x sum + model y sum ctx compiled_autograd _enable torch compile backend= eager result backward assertEqual counters compiled_autograd captures actual_logs = logs getvalue expected_logs = gen_cache_miss_log_prefix + torch autograd GraphRoot NodeCall key size previous key sizes= expected expected_logs assertTrue expected actual_logs test_verbose_logs_snapshot fn model = torch nn Sequential torch nn Linear torch nn ReLU torch nn Linear torch nn ReLU x = torch randn result = model x sum result backward yield model weight grad yield model bias grad yield model weight grad yield model bias grad logs ctx = logs_to_string torch _dynamo compiled_autograd __name__ compiled_autograd_verbose ctx compiled_autograd _enable compiler_fn unused verbose level already snapshot contextmanager torch _logging set_logs compiled_autograd_verbose=True fn unexpected_logs = gen_cache_miss_log_prefix + torch autograd GraphRoot NodeCall assertEqual sum e unexpected_logs e logs getvalue test_tensor_subclass_basic torch testing _internal two_tensor TwoTensor TwoTensorMode torch library _scoped_library mylib FRAGMENT lib lib define to_twotensor Tensor Tensor b - Tensor lib define from_twotensor Tensor c - Tensor Tensor to_twotensor_backward ctx grad torch ops mylib from_twotensor grad from_twotensor_backward ctx grad_a grad_b raise AssertionError shouldn t get hit torch library register_autograd mylib to_twotensor to_twotensor_backward lib=lib torch library register_autograd mylib from_twotensor from_twotensor_backward lib=lib torch library register_torch_dispatch mylib to_twotensor TwoTensorMode lib=lib _ _ _ _ args kwargs assert kwargs b = args TwoTensor clone b clone torch library register_torch_dispatch mylib from_twotensor TwoTensor lib=lib _ _ _ _ args kwargs assert kwargs c = args c clone c b clone torch compile backend= aot_eager fullgraph=True fn x x x + param = torch randn requires_grad=True param = torch randn requires_grad=True TwoTensorMode x = torch ops mylib to_twotensor param param inner_compiler_fn = make_compiler_fn fullgraph=True backend= aot_eager graphs = compiler_fn gm graphs append gm inner_compiler_fn gm compiled_autograd _enable compiler_fn mock patch torch _functorch aot_autograd AOT_COUNTER new_callable=itertools count res = fn x res sum backward assertEqual param grad param assertEqual param grad param assertEqual len graphs graph_code = normalize_gm graphs print_readable print_output=False The graph should have make_subclass calls assertExpectedInline graph_code \ CompiledAutograd torch nn Module forward inputs sizes scalars hooks packed_data getitem = inputs getitem_ = inputs getitem_ = inputs getitem_ = inputs getitem_ = inputs inputs = None getitem_ = sizes getitem_ = sizes getitem_ = sizes getitem_ = sizes getitem_ = sizes getitem_ = sizes getitem_ = sizes getitem_ = sizes sizes = None unwrap_maybe_dynamic_int = torch__dynamo_external_utils_unwrap_maybe_dynamic_int getitem_ getitem_ = None unwrap_maybe_dynamic_int_ = torch__dynamo_external_utils_unwrap_maybe_dynamic_int getitem_ getitem_ = None unwrap_maybe_dynamic_int_ = torch__dynamo_external_utils_unwrap_maybe_dynamic_int getitem_ getitem_ = None unwrap_maybe_dynamic_int_ = torch__dynamo_external_utils_unwrap_maybe_dynamic_int getitem_ getitem_ = None unwrap_maybe_dynamic_int_ = torch__dynamo_external_utils_unwrap_maybe_dynamic_int getitem_ getitem_ = None unwrap_maybe_dynamic_int_ = torch__dynamo_external_utils_unwrap_maybe_dynamic_int getitem_ getitem_ = None unwrap_maybe_dynamic_int_ = torch__dynamo_external_utils_unwrap_maybe_dynamic_int getitem_ getitem_ = None unwrap_maybe_dynamic_int_ = torch__dynamo_external_utils_unwrap_maybe_dynamic_int getitem_ getitem_ = None validate_outputs = torch__dynamo_compiled_autograd_ops_validate_outputs getitem None None device type= cpu None True getitem = None getitem_ = validate_outputs validate_outputs = None sum_backward = torch__dynamo_compiled_autograd_ops_SumBackward getitem_ True unwrap_maybe_dynamic_int unwrap_maybe_dynamic_int_ getitem_ = unwrap_maybe_dynamic_int = unwrap_maybe_dynamic_int_ = None getitem_ = sum_backward sum_backward = None validate_outputs_ = torch__dynamo_compiled_autograd_ops_validate_outputs getitem_ None None device type= cpu None unwrap_maybe_dynamic_int_ unwrap_maybe_dynamic_int_ True getitem_ = unwrap_maybe_dynamic_int_ = unwrap_maybe_dynamic_int_ = None getitem_ = validate_outputs_ validate_outputs_ = None getitem_ = hooks getitem_ = None call_aot_bwd_prologue = torch__dynamo_compiled_autograd_call_aot_bwd_prologue getitem_ getitem_ getitem_ getitem_ = getitem_ = getitem_ = None aot _primals_ = call_aot_bwd_prologue aot _primals_ = call_aot_bwd_prologue aot _tangents_ = call_aot_bwd_prologue aot _tangents_ = call_aot_bwd_prologue call_aot_bwd_prologue = None aot _mul_ = torch ops aten mul Tensor aot _tangents_ aot _primals_ aot _tangents_ = aot _primals_ = None aot _mul_ = torch ops aten mul Tensor aot _tangents_ aot _primals_ aot _tangents_ = aot _primals_ = None aot _add_ = torch ops aten add Tensor aot _mul_ aot _mul_ aot _mul_ = None aot _add_ = torch ops aten add Tensor aot _mul_ aot _mul_ aot _mul_ = None make_subclass = torch__dynamo_compiled_autograd_make_subclass aot _add_ aot _add_ aot _add_ = aot _add_ = None getitem_ = hooks hooks = None call_backward = torch__dynamo_external_utils_call_backward getitem_ make_subclass getitem_ = make_subclass = None getitem_ = call_backward getitem_ = call_backward call_backward = None validate_outputs_ = torch__dynamo_compiled_autograd_ops_validate_outputs getitem_ getitem_ None None device type= cpu None unwrap_maybe_dynamic_int_ unwrap_maybe_dynamic_int_ False None None device type= cpu None unwrap_maybe_dynamic_int_ unwrap_maybe_dynamic_int_ False getitem_ = getitem_ = unwrap_maybe_dynamic_int_ = unwrap_maybe_dynamic_int_ = unwrap_maybe_dynamic_int_ = unwrap_maybe_dynamic_int_ = None getitem_ = validate_outputs_ call_accumulate_grad_ = torch__dynamo_external_utils_call_accumulate_grad getitem_ getitem_ False getitem_ = getitem_ = call_accumulate_grad_ = None getitem_ = validate_outputs_ validate_outputs_ = None call_accumulate_grad = torch__dynamo_external_utils_call_accumulate_grad getitem_ getitem_ False getitem_ = getitem_ = call_accumulate_grad = None _exec_final_callbacks_stub = torch__dynamo_external_utils__exec_final_callbacks_stub _exec_final_callbacks_stub = None noqa B https github com pytorch pytorch issues Inductor has joint graph pattern remove pointless view pairs That will remove no-op view pairs test checking Disable pattern matcher test inductor_config patch pattern_matcher=False test_compiled_autograd_does_not_specialize_on_bw_symints Mod torch nn Module __init__ b c super __init__ = c = c b = b lin = torch nn Linear b b c device= cpu forward x x = x view - b y = lin x y = y view - c b contiguous y = torch flatten y start_dim= y Mod torch nn Module __init__ b c super __init__ mod = Mod b c forward s tensor_dict args = tensor_dict s x = torch cat list args out = mod x out Mod torch nn Module __init__ mods super __init__ mods = mods forward strs tensor_dict x outs = x i m enumerate mods s = strs i print graph break out = m s tensor_dict outs append out torch cat outs sum gen_tensor_dict sizes tensor_dict = torch randn sizes device= cpu _ range b torch randn sizes device= cpu _ range tensor_dict mods = Mod Mod m = Mod mods strs = b m = torch compile m graphs = compiler_fn gm inner_compiler gm_ example_inputs_ graphs append gm_ gm_ torch compile gm backend=inner_compiler fullgraph=True dynamic=True x = torch zeros device= cpu tensor_dict = gen_tensor_dict out = m strs tensor_dict x torch _dynamo compiled_autograd _enable compiler_fn ctx out sum backward x = torch zeros device= cpu tensor_dict = gen_tensor_dict out = m strs tensor_dict x torch _dynamo compiled_autograd _enable compiler_fn ctx out sum backward This test bit fragile I failed create better repro The important bit second CA graph has specialized value aot _sym_size_int_ constant This happens via suppressing any dynamic shape guards CA generates when runs make_fx Suppressing these guards strictly better than current state because we ignore all these guards anyway CA Once we stop using make_fx CA we won t have worry about specialization view_nodes = graphs graph find_nodes op= call_function target=torch ops aten reshape default First view nodes have first argument SymInt int burned into graph assertTrue isinstance view_nodes args torch fx Node assertTrue isinstance view_nodes args torch fx Node requires_cuda_and_triton test_flex_attention _squared score b h m n Joint graph needed correctness score score fn torch compile backend= aot_eager fwd_bwd x torch Tensor flex_attention x x x score_mod=_squared sum backward b zip v = torch zeros b b dtype=torch bfloat device= cuda requires_grad=True fwd_bwd v yield v grad check_output_and_recompiles fn count= compiler_fn=make_compiler_fn backend= aot_eager test_saved_tensor_unpack_hook_ordering f x y x y pack_count = unpack_count = pack_hook x nonlocal pack_count pack_count += x unpack_hook x nonlocal unpack_count unpack_count += x tensor_hook _ assertEqual unpack_count x = torch ones requires_grad=True y = torch ones requires_grad=False torch autograd graph saved_tensors_hooks pack_hook unpack_hook compiled_autograd _enable make_compiler_fn fullgraph=False out_test = f x y assertEqual pack_count assertEqual unpack_count loss = out_test sum loss register_hook tensor_hook scheduled fire before any saved activations loss backward assertEqual pack_count assertEqual unpack_count parametrize reentrant True False test_checkpointing_simple reentrant fn _fn x y = x sin z = y cos y z sum inp = torch rand requires_grad=True out = torch utils checkpoint checkpoint _fn inp use_reentrant=reentrant out backward yield inp grad reentrant check_output_and_recompiles fn count= compiler_fn=make_compiler_fn fullgraph=False dynamo issues just run CA graph directly now check gm graph_code = normalize_gm gm print_readable print_output=False assertExpectedInline graph_code \ CompiledAutograd torch nn Module forward inputs sizes scalars hooks packed_data getitem = inputs getitem_ = inputs inputs = None getitem_ = sizes getitem_ = sizes getitem_ = sizes getitem_ = sizes getitem_ = sizes getitem_ = sizes getitem_ = sizes getitem_ = sizes getitem_ = sizes getitem_ = sizes getitem_ = sizes getitem_ = sizes sizes = None unwrap_maybe_dynamic_int = torch__dynamo_external_utils_unwrap_maybe_dynamic_int getitem_ getitem_ = None unwrap_maybe_dynamic_int_ = torch__dynamo_external_utils_unwrap_maybe_dynamic_int getitem_ getitem_ = None unwrap_maybe_dynamic_int_ = torch__dynamo_external_utils_unwrap_maybe_dynamic_int getitem_ getitem_ = None unwrap_maybe_dynamic_int_ = torch__dynamo_external_utils_unwrap_maybe_dynamic_int getitem_ getitem_ = None unwrap_maybe_dynamic_int_ = torch__dynamo_external_utils_unwrap_maybe_dynamic_int getitem_ getitem_ = None unwrap_maybe_dynamic_int_ = torch__dynamo_external_utils_unwrap_maybe_dynamic_int getitem_ getitem_ = None unwrap_maybe_dynamic_int_ = torch__dynamo_external_utils_unwrap_maybe_dynamic_int getitem_ getitem_ = None unwrap_maybe_dynamic_int_ = torch__dynamo_external_utils_unwrap_maybe_dynamic_int getitem_ getitem_ = None unwrap_maybe_dynamic_int_ = torch__dynamo_external_utils_unwrap_maybe_dynamic_int getitem_ getitem_ = None unwrap_maybe_dynamic_int_ = torch__dynamo_external_utils_unwrap_maybe_dynamic_int getitem_ getitem_ = None unwrap_maybe_dynamic_int_ = torch__dynamo_external_utils_unwrap_maybe_dynamic_int getitem_ getitem_ = None unwrap_maybe_dynamic_int_ = torch__dynamo_external_utils_unwrap_maybe_dynamic_int getitem_ getitem_ = None validate_outputs = torch__dynamo_compiled_autograd_ops_validate_outputs getitem None None device type= cpu None False getitem = None getitem_ = validate_outputs validate_outputs = None sum_backward = torch__dynamo_compiled_autograd_ops_SumBackward getitem_ True unwrap_maybe_dynamic_int unwrap_maybe_dynamic_int_ getitem_ = unwrap_maybe_dynamic_int = unwrap_maybe_dynamic_int_ = None getitem_ = sum_backward sum_backward = None validate_outputs_ = torch__dynamo_compiled_autograd_ops_validate_outputs getitem_ None None device type= cpu None unwrap_maybe_dynamic_int_ unwrap_maybe_dynamic_int_ False getitem_ = unwrap_maybe_dynamic_int_ = unwrap_maybe_dynamic_int_ = None getitem_ = validate_outputs_ validate_outputs_ = None getitem_ = hooks getitem_ = packed_data getitem_ = hooks getitem_ = packed_data call_hook = torch__dynamo_external_utils_call_hook getitem_ getitem_ hook_type = unpack_hook getitem_ = getitem_ = None call_hook_ = torch__dynamo_external_utils_call_hook getitem_ getitem_ hook_type = unpack_hook getitem_ = getitem_ = None mul_backward = torch__dynamo_compiled_autograd_ops_MulBackward getitem_ True True call_hook call_hook_ getitem_ = call_hook = call_hook_ = None getitem_ = mul_backward getitem_ = mul_backward mul_backward = None validate_outputs_ = torch__dynamo_compiled_autograd_ops_validate_outputs getitem_ getitem_ None None device type= cpu None unwrap_maybe_dynamic_int_ unwrap_maybe_dynamic_int_ False None None device type= cpu None unwrap_maybe_dynamic_int_ unwrap_maybe_dynamic_int_ False getitem_ = getitem_ = unwrap_maybe_dynamic_int_ = unwrap_maybe_dynamic_int_ = unwrap_maybe_dynamic_int_ = unwrap_maybe_dynamic_int_ = None getitem_ = validate_outputs_ getitem_ = validate_outputs_ validate_outputs_ = None getitem_ = hooks getitem_ = packed_data call_hook_ = torch__dynamo_external_utils_call_hook getitem_ getitem_ hook_type = unpack_hook getitem_ = getitem_ = None cos_backward = torch__dynamo_compiled_autograd_ops_CosBackward getitem_ True call_hook_ getitem_ = call_hook_ = None getitem_ = cos_backward cos_backward = None validate_outputs_ = torch__dynamo_compiled_autograd_ops_validate_outputs getitem_ None None device type= cpu None unwrap_maybe_dynamic_int_ unwrap_maybe_dynamic_int_ False getitem_ = unwrap_maybe_dynamic_int_ = unwrap_maybe_dynamic_int_ = None getitem_ = validate_outputs_ validate_outputs_ = None add = torch add getitem_ getitem_ getitem_ = getitem_ = None getitem_ = hooks hooks = None getitem_ = packed_data packed_data = None call_hook_ = torch__dynamo_external_utils_call_hook getitem_ getitem_ hook_type = unpack_hook getitem_ = getitem_ = None sin_backward = torch__dynamo_compiled_autograd_ops_SinBackward add True call_hook_ add = call_hook_ = None getitem_ = sin_backward sin_backward = None validate_outputs_ = torch__dynamo_compiled_autograd_ops_validate_outputs getitem_ None None device type= cpu None unwrap_maybe_dynamic_int_ unwrap_maybe_dynamic_int_ False getitem_ = unwrap_maybe_dynamic_int_ = unwrap_maybe_dynamic_int_ = None getitem_ = validate_outputs_ validate_outputs_ = None call_accumulate_grad = torch__dynamo_external_utils_call_accumulate_grad getitem_ getitem_ False getitem_ = getitem_ = call_accumulate_grad = None _exec_final_callbacks_stub = torch__dynamo_external_utils__exec_final_callbacks_stub _exec_final_callbacks_stub = None noqa B check_output_and_recompiles fn count= compiler_fn=make_compiler_fn backend= ca_eager gm_hook=check requires_cuda_and_triton test_cpu_offloading fn pack x x cpu unpack x x cuda MyMatMul torch autograd Function staticmethod forward ctx x ctx save_for_backward x torch matmul x x staticmethod backward ctx grad_out x = ctx saved_tensors grad_out x torch autograd graph saved_tensors_hooks pack unpack i x = torch randn i requires_grad=True cuda MyMatMul apply x sum backward yield x grad i = check gm nonlocal i i == i += graph_code = normalize_gm gm print_readable print_output=False assertExpectedInline graph_code \ CompiledAutograd torch nn Module forward inputs sizes scalars hooks packed_data getitem = inputs getitem_ = inputs inputs = None getitem_ = sizes getitem_ = None getitem_ = sizes getitem_ = sizes sizes = None validate_outputs = torch__dynamo_compiled_autograd_ops_validate_outputs getitem None None device type= cuda index= None False getitem = None getitem_ = validate_outputs validate_outputs = None sum_backward = torch__dynamo_compiled_autograd_ops_SumBackward getitem_ True getitem_ = None getitem_ = sum_backward sum_backward = None validate_outputs_ = torch__dynamo_compiled_autograd_ops_validate_outputs getitem_ None None device type= cuda index= None False getitem_ = None getitem_ = validate_outputs_ validate_outputs_ = None getitem_ = hooks getitem_ = packed_data packed_data = None getitem_ = hooks hooks = None call_hook = torch__dynamo_external_utils_call_hook getitem_ getitem_ hook_type = unpack_hook getitem_ = getitem_ = None call_backward = torch__dynamo_external_utils_call_backward getitem_ call_hook getitem_ getitem_ = call_hook = getitem_ = None getitem_ = call_backward call_backward = None validate_outputs_ = torch__dynamo_compiled_autograd_ops_validate_outputs getitem_ None None device type= cuda index= None getitem_ False getitem_ = getitem_ = None getitem_ = validate_outputs_ validate_outputs_ = None to_copy_backward = torch__dynamo_compiled_autograd_ops_ToCopyBackward getitem_ True None None device type= cpu None getitem_ = None getitem_ = to_copy_backward to_copy_backward = None validate_outputs_ = torch__dynamo_compiled_autograd_ops_validate_outputs getitem_ None None device type= cpu None getitem_ False getitem_ = getitem_ = None getitem_ = validate_outputs_ validate_outputs_ = None accumulate_grad__default = torch ops inductor accumulate_grad_ default getitem_ getitem_ getitem_ = getitem_ = accumulate_grad__default = None _exec_final_callbacks_stub = torch__dynamo_external_utils__exec_final_callbacks_stub _exec_final_callbacks_stub = None noqa B check_output_and_recompiles fn compiler_fn=make_compiler_fn gm_hook=check skipIfWindows msg= temp dir compatible test_disk_offloading tempfile TemporaryDirectory d fn pack_count = pack x nonlocal pack_count path = f d pack_count pt torch save x path path unpack path x = torch load path x MyMatMul torch autograd Function staticmethod forward ctx x ctx save_for_backward x torch matmul x x staticmethod backward ctx grad_out x = ctx saved_tensors grad_out x torch autograd graph saved_tensors_hooks pack unpack i x = torch randn i requires_grad=True MyMatMul apply x sum backward yield x grad i = check gm nonlocal i i == i += graph_code = normalize_gm gm print_readable print_output=False assertExpectedInline graph_code \ CompiledAutograd torch nn Module forward inputs sizes scalars hooks packed_data getitem = inputs getitem_ = inputs inputs = None getitem_ = sizes getitem_ = None getitem_ = sizes sizes = None validate_outputs = torch__dynamo_compiled_autograd_ops_validate_outputs getitem None None device type= cpu None False getitem = None getitem_ = validate_outputs validate_outputs = None sum_backward = torch__dynamo_compiled_autograd_ops_SumBackward getitem_ True getitem_ = None getitem_ = sum_backward sum_backward = None validate_outputs_ = torch__dynamo_compiled_autograd_ops_validate_outputs getitem_ None None device type= cpu None False getitem_ = None getitem_ = validate_outputs_ validate_outputs_ = None getitem_ = hooks getitem_ = packed_data packed_data = None getitem_ = hooks hooks = None call_hook = torch__dynamo_external_utils_call_hook getitem_ getitem_ hook_type = unpack_hook getitem_ = getitem_ = None call_backward = torch__dynamo_external_utils_call_backward getitem_ call_hook getitem_ getitem_ = call_hook = getitem_ = None getitem_ = call_backward call_backward = None validate_outputs_ = torch__dynamo_compiled_autograd_ops_validate_outputs getitem_ None None device type= cpu None getitem_ False getitem_ = getitem_ = None getitem_ = validate_outputs_ validate_outputs_ = None accumulate_grad__default = torch ops inductor accumulate_grad_ default getitem_ getitem_ getitem_ = getitem_ = accumulate_grad__default = None _exec_final_callbacks_stub = torch__dynamo_external_utils__exec_final_callbacks_stub _exec_final_callbacks_stub = None noqa B graph break torch load - dynamo graphs check_output_and_recompiles fn count= compiler_fn=make_compiler_fn fullgraph=False gm_hook=check skipIfWindows msg= node name demangling inconsistent windows test_backward_hook_relative_ordering_partial test backward hooks cases CA matches eager fn order = MyModule nn Module __init__ super __init__ linear = torch nn Linear bias=False forward x linear x x = torch randn module = MyModule make_pre_hook id lambda _ order append f pre_hook_ id make_post_hook id lambda _ _ order append f post_hook_ id count = register_hooks_on_all_nodes nodes nonlocal count node _ nodes node None continue count += id = f node name _ count node register_prehook make_pre_hook id node register_hook make_post_hook id register_hooks_on_all_nodes node next_functions loss = module x sum register_hooks_on_all_nodes loss grad_fn None make_tensor_pre_hook id lambda _ order append f tensor_pre_hook_ id make_post_acc_grad_hook id lambda _ order append f post_acc_grad_hook_ id module linear weight register_hook make_tensor_pre_hook weight module linear weight register_post_accumulate_grad_hook make_post_acc_grad_hook weight loss backward yield tuple order check_output_and_recompiles fn test_checkpointing_sac circular torch utils checkpoint checkpoint CheckpointPolicy create_selective_checkpoint_contexts fn mlp nn Module __init__ super __init__ layer = nn Linear layer = nn Linear layer = nn Linear layer = nn Linear forward x x = layer x x = layer x x = layer x x = layer x x recompute_list = torch ops aten addmm default recompute_policy ctx op args kwargs op recompute_list CheckpointPolicy MUST_RECOMPUTE CheckpointPolicy PREFER_SAVE context_fn create_selective_checkpoint_contexts recompute_policy model = mlp input = torch randn out = checkpoint model input use_reentrant=False context_fn=context_fn out sum backward yield model layer weight grad yield model layer bias grad yield model layer weight grad yield model layer bias grad yield model layer weight grad yield model layer bias grad yield model layer weight grad yield model layer bias grad check_output_and_recompiles fn count= compiler_fn=make_compiler_fn fullgraph=False test_dont_dce_side_effects SideEffectfulBackward torch autograd Function staticmethod forward ctx x x staticmethod backward ctx gO torch randn gO x = torch randn requires_grad=True https github com pytorch pytorch issues torch _inductor config fallback_random = True torch compile backend= aot_eager fn x SideEffectfulBackward apply x sum gm = None extract ca_gm nonlocal gm gm = ca_gm ca_gm compiled_autograd _enable extract fn x backward assertTrue aten randn str gm test_aot_bwd_gm_runnable This test ensures bw_module saved CompiledFunction _lazy_backward_info executable ensuring post grad passes have ran post_grad_graphs = post_grad_pass graph nonlocal post_grad_graphs post_grad_graphs append graph graph x = torch randn requires_grad=True y = torch randn requires_grad=True forces symints saved backward forces aot compilation backward torch _dynamo mark_dynamic x torch _dynamo mark_dynamic y torch compile fn x y torch matmul x y sum inductor_config patch post_grad_custom_post_pass=post_grad_pass loss = fn x y assertEqual len post_grad_graphs fwd bwd assertTrue loss grad_fn name CompiledFunctionBackward assertIsNot post_grad_graphs loss grad_fn _forward_cls _lazy_backward_info bw_module graph compiled_autograd _enable lambda gm gm loss backward test_anomaly_mode_already_nan fn torch autograd detect_anomaly = torch randn requires_grad=True grad = torch full float nan b = torch randn out = torch matmul b loss = out sum torch _dynamo compiled_autograd _enable lambda gm gm loss backward assertRaisesRegex AssertionError already having NaN gradient This supported fn test_anomaly_mode_backward fn MyFn torch autograd Function staticmethod forward ctx x x staticmethod backward ctx gO torch full gO size float nan torch autograd detect_anomaly = torch randn requires_grad=True out = MyFn apply loss = out sum torch _dynamo compiled_autograd _enable lambda gm gm loss backward assertRaisesRegex RuntimeError Compiled Autograd returned NaN gradients parameters fn test_anomaly_mode_grad fn MyFn torch autograd Function staticmethod forward ctx x x staticmethod backward ctx gO torch full gO size float nan torch autograd detect_anomaly = torch randn requires_grad=True out = MyFn apply loss = out sum torch _dynamo compiled_autograd _enable lambda gm gm torch autograd grad loss inputs=a assertRaisesRegex RuntimeError Compiled Autograd returned NaN gradients output nodes fn test_higher_order_gradients f x x fn fwd_compiler ca_compiler torch manual_seed x = torch tensor requires_grad=True first second third fourth = None None None None try compiled_autograd _enable ca_compiler first = torch autograd grad fwd_compiler f x x create_graph=True second = torch autograd grad first x create_graph=True third = torch autograd grad second x create_graph=True fourth = torch autograd grad third x create_graph=True except RuntimeError e assert does currently support higher order gradients str e first second third fourth first second third fourth eager torch compile backend= eager aot_eager torch compile backend= aot_eager Without AOTAutograd no problem first second third fourth = fn eager eager assertEqual counters compiled_autograd captures assertEqual first x^ assertEqual second x assertEqual third assertEqual fourth should cache hit counters clear _ = fn eager eager assertEqual counters compiled_autograd captures torch _dynamo reset With AOTAutograd can t create_graph first second third fourth = fn aot_eager aot_eager assertIsNone second first second third fourth = fn aot_eager eager assertIsNone second first second third fourth = fn eager aot_eager assertIsNone third unittest skipIf torch distributed is_available FakePG relies distributed build test_ddp_cpp_reducer_error torch testing _internal distributed fake_pg FakeStore store = FakeStore dist init_process_group backend= fake rank= world_size= store=store try model = torch nn Sequential nn Linear nn ReLU nn Linear model = DDP model inputs = torch randn loss = model inputs sum compiled_autograd _enable compiler_fn assertRaisesRegex RuntimeError r Compiled autograd compatible C\+\+ DDP Reducer r please use torch _dynamo config optimize_ddp= python_reducer loss backward finally dist destroy_process_group unittest skipIf torch distributed is_available FakePG relies distributed build config patch optimize_ddp= python_reducer test_ddp_python_reducer torch testing _internal distributed fake_pg FakeStore store = FakeStore dist init_process_group backend= fake rank= world_size= store=store try model = torch nn Sequential nn Linear nn ReLU nn Linear model = DDP model inputs = torch randn loss = model inputs sum compiled_autograd _enable compiler_fn no error expected loss backward assertEqual counters compiled_autograd captures finally dist destroy_process_group Case Stealable dense new_grad GradMode is_enabled new_grad is_sparse new_grad is_sparse_csr variable is_sparse_csr new_grad layout == kStrided caching adjusted_use_count new_grad = num_expected_refs new_grad is_mkldnn &#124; &#124; utils obeys_layout_contract new_grad variable unittest expectedFailure test_accumulate_grad_polyfill_case_ _ fn StealableDenseOp BaseCustomOp staticmethod backward ctx grad_output torch ones_like grad_output requires_grad=False pre_hook_storage_id = None check grad nonlocal pre_hook_storage_id assert pre_hook_storage_id None pre_hook_storage_id = id grad untyped_storage var = torch randn requires_grad=True var register_hook check output = StealableDenseOp apply var output backward torch ones_like output assert var grad None Grad should defined assert torch equal var grad torch ones_like var Grad content should returned backward assert var grad requires_grad False Detached grad should require grad assert id var grad untyped_storage == pre_hook_storage_id Should stolen yield var grad check_output_and_recompiles fn compiler_fn=make_compiler_fn fullgraph=False count= Case Stealable sparse new_grad GradMode is_enabled new_grad is_sparse new_grad _indices is_contiguous new_grad _values is_contiguous new_grad _indices use_count = new_grad _values use_count = new_grad use_count = num_expected_refs unittest expectedFailure test_accumulate_grad_polyfill_case_ _ fn StealableSparseOp BaseCustomOp staticmethod backward ctx grad_output size = grad_output size indices = torch tensor dtype=torch int values = torch tensor torch sparse_coo_tensor indices values size requires_grad=False pre_hook_storages_id = None check grad nonlocal pre_hook_storages_id assert pre_hook_storages_id None pre_hook_storages_id = id grad _indices untyped_storage id grad _values untyped_storage var = torch randn requires_grad=True var register_hook check output = StealableSparseOp apply var output backward torch ones_like output assert var grad None Grad should defined assert var grad is_sparse Grad should sparse expected_dense_grad = torch tensor assert torch equal var grad to_dense expected_dense_grad Content should equal after shallow copy assert var grad requires_grad False Detached grad should require grad assert id var grad _indices untyped_storage == pre_hook_storages_id Should stolen assert id var grad _values untyped_storage == pre_hook_storages_id Should stolen yield var grad check_output_and_recompiles fn compiler_fn=make_compiler_fn fullgraph=False count= Case Cloning sparse nested new_grad new_grad is_sparse &#124; &#124; new_grad is_sparse_csr &#124; &#124; new_grad is_nested test_accumulate_grad_polyfill_case_ _ fn CloneSparseGradOp BaseCustomOp staticmethod backward ctx grad_output size = grad_output size indices = torch tensor dtype=torch int values = torch tensor requires_grad=True Requires grad torch sparse_coo_tensor indices values size requires_grad=True pre_hook_storages_id = None check grad nonlocal pre_hook_storages_id assert pre_hook_storages_id None pre_hook_storages_id = id grad _indices untyped_storage id grad _values untyped_storage var = torch randn requires_grad=True var register_hook check output = CloneSparseGradOp apply var output backward torch ones_like output create_graph=True grad mode == create_graph assert var grad None Grad should defined assert var grad is_sparse Grad should sparse expected_dense_grad = torch tensor assert torch equal var grad to_dense expected_dense_grad Content should equal after clone assert var grad requires_grad Grad should require grad double backward assert id var grad _indices untyped_storage = pre_hook_storages_id Should copied assert id var grad _values untyped_storage = pre_hook_storages_id Should copied yield var grad check_output_and_recompiles fn compiler_fn=make_compiler_fn fullgraph=False count= Case Dense variable gradient layout contract Covers various deep copy scenarios covered specific stealable paths new_grad is_mkldnn Deep copies new_grad according Gradient Layout Contract update_grad utils clone_obey_contract new_grad variable test_accumulate_grad_polyfill_case_ _ _ fn NotStealableRefsOp BaseCustomOp staticmethod backward ctx grad_output torch ones_like grad_output requires_grad=False var = torch randn requires_grad=True grad_ref_holder = None check grad forces clone due refcount grad_ref_holder = grad grad var register_hook check output = NotStealableRefsOp apply var output backward torch ones_like output assert var grad None Grad should defined assert torch equal var grad torch ones_like var Grad content should returned backward assert grad_ref_holder untyped_storage var grad untyped_storage Should copied yield var grad check_output_and_recompiles fn Case Non-dense variable gradient layout contract Covers various deep copy scenarios covered specific stealable paths new_grad is_mkldnn Deep copies new_grad according Gradient Layout Contract update_grad utils clone_obey_contract new_grad variable test_accumulate_grad_polyfill_case_ _ _ fn SimpleDenseGradOp BaseCustomOp staticmethod backward ctx grad_output torch ones_like grad_output requires_grad=False Create non-contiguous variable base_tensor = torch randn var = base_tensor assert var is_contiguous Variable should non-contiguous test var requires_grad_ True grad_ref_holder = None check grad forces clone due refcount grad_ref_holder = grad grad var register_hook check output = SimpleDenseGradOp apply var output backward torch ones_like output assert var grad None Grad should defined The ` clone_obey_contract ` branch ` new_grad clone MemoryFormat Contiguous ` will make resulting grad contiguous assert var grad is_contiguous Resulting grad should contiguous due branch clone_obey_contract assert torch equal var grad torch ones_like var Grad content should returned backward assert grad_ref_holder untyped_storage var grad untyped_storage Should copied yield var grad check_output_and_recompiles fn Case Sparse variable_grad + Dense new_grad GradMode is_enabled variable_grad is_sparse new_grad is_sparse auto result = new_grad + variable_grad test_accumulate_grad_polyfill_case_ _ fn SparseVarGradDenseNewGradOp BaseCustomOp staticmethod backward ctx grad_output torch ones_like grad_output var = torch randn requires_grad=True indices = torch tensor dtype=torch int values = torch tensor var grad = torch sparse_coo_tensor indices values var size requires_grad=False initial_grad_ref = var grad output = SparseVarGradDenseNewGradOp apply var expected_sum = torch ones_like var + initial_grad_ref to_dense output backward torch ones_like output assert var grad None Grad should defined assert var grad is_sparse Resulting grad should dense assert torch equal var grad expected_sum Grad content should sum assert var grad initial_grad_ref Grad object should replaced out-of-place yield var grad check_output_and_recompiles fn compiler_fn=lambda gm gm https github com pytorch pytorch issues count= Case Dense Dense in-place addition GradMode is_enabled variable_grad += new_grad test_accumulate_grad_polyfill_case_ _ _ fn DenseVarGradDenseNewGradOp BaseCustomOp staticmethod backward ctx grad_output torch ones_like grad_output var = torch randn requires_grad=True var grad = torch ones_like var initial_grad_ref = var grad output = DenseVarGradDenseNewGradOp apply var expected_sum = initial_grad_ref + torch ones_like var output backward torch ones_like output assert var grad None Grad should defined assert var grad is_sparse Resulting grad should dense assert torch equal var grad expected_sum Grad content should sum assert var grad initial_grad_ref Grad object should modified in-place same object yield var grad check_output_and_recompiles fn Case Sparse Sparse in-place addition GradMode is_enabled variable_grad += new_grad test_accumulate_grad_polyfill_case_ _ _ fn SparseVarGradSparseNewGradOp BaseCustomOp staticmethod backward ctx grad_output size = grad_output size indices = torch tensor dtype=torch int values = torch tensor torch sparse_coo_tensor indices values size requires_grad=False var = torch randn requires_grad=True indices_v = torch tensor dtype=torch int values_v = torch tensor var grad = torch sparse_coo_tensor indices_v values_v var size requires_grad=False initial_grad_ref = var grad output = SparseVarGradSparseNewGradOp apply var new_grad_for_sum = torch sparse_coo_tensor torch tensor dtype=torch int torch tensor var size expected_sum_dense = initial_grad_ref to_dense + new_grad_for_sum to_dense output backward torch ones_like output assert var grad None Grad should defined assert var grad is_sparse Resulting grad should remain sparse assert torch equal var grad to_dense expected_sum_dense Grad content should sum sparse grads assert var grad initial_grad_ref Grad object should modified in-place same object yield var grad check_output_and_recompiles fn compiler_fn=lambda gm gm https github com pytorch pytorch issues count= Case Dense Sparse in-place addition GradMode is_enabled variable_grad += new_grad test_accumulate_grad_polyfill_case_ _ _ fn DenseVarGradSparseNewGradOp BaseCustomOp staticmethod backward ctx grad_output size = grad_output size indices = torch tensor dtype=torch int values = torch tensor New sparse values torch sparse_coo_tensor indices values size requires_grad=False var = torch randn requires_grad=True var grad = torch ones_like var Initial value initial_grad_ref = var grad output = DenseVarGradSparseNewGradOp apply var new_grad_for_sum = torch sparse_coo_tensor torch tensor dtype=torch int torch tensor var size to_dense expected_sum = initial_grad_ref + new_grad_for_sum output backward torch ones_like output assert var grad None Grad should defined assert var grad is_sparse Resulting grad should dense assert torch equal var grad expected_sum Grad content should sum assert var grad initial_grad_ref Grad object should modified in-place same object yield var grad check_output_and_recompiles fn compiler_fn=make_compiler_fn fullgraph=False count= Case Sparse variable_grad + Dense new_grad reorder into Dense + Sparse GradMode is_enabled Tensor result variable_grad is_sparse new_grad is_sparse result = new_grad + variable_grad test_accumulate_grad_polyfill_case_ _ fn SparseVarGradDenseNewGradDoubleBackwardOp BaseCustomOp staticmethod backward ctx grad_output torch ones_like grad_output requires_grad=True var = torch randn requires_grad=True indices = torch tensor dtype=torch int values = torch tensor requires_grad=True var grad = torch sparse_coo_tensor indices values var size requires_grad=True initial_grad_ref = var grad output = SparseVarGradDenseNewGradDoubleBackwardOp apply var expected_sum = torch ones_like var requires_grad=True + initial_grad_ref to_dense output backward torch ones_like output create_graph=True assert var grad None Grad should defined assert var grad is_sparse Resulting grad should dense assert torch equal var grad expected_sum Grad content should sum assert var grad initial_grad_ref Grad object should replaced out-of-place assert var grad requires_grad Resulting grad should track history double backward yield var grad check_output_and_recompiles fn compiler_fn=lambda gm gm https github com pytorch pytorch issues count= Case variable_grad defined GradMode is_enabled - Double backward dense variable_grad + dense new_grad GradMode is_enabled Tensor result result = variable_grad + new_grad test_accumulate_grad_polyfill_case_ _ fn DenseVarGradDenseNewGradDoubleBackwardOp BaseCustomOp staticmethod backward ctx grad_output torch ones_like grad_output requires_grad=True var = torch randn requires_grad=True var grad = torch ones_like var initial_grad_ref = var grad output = DenseVarGradDenseNewGradDoubleBackwardOp apply var expected_sum = initial_grad_ref + torch ones_like var requires_grad=True output backward torch ones_like output create_graph=True assert var grad None Grad should defined assert var grad is_sparse Resulting grad should dense assert torch equal var grad expected_sum Grad content should sum assert var grad initial_grad_ref Grad object should replaced out-of-place assert var grad requires_grad Resulting grad should track history double backward yield var grad check_output_and_recompiles fn compiler_fn=make_compiler_fn fullgraph=False count= test_torch_function_mode called_funcs = LoggingTorchFunctionMode BaseTorchFunctionMode __torch_function__ func types args= kwargs=None called_funcs append str func __name__ super __torch_function__ func types args kwargs MyLoss torch autograd Function staticmethod forward ctx out ctx save_for_backward out out sum staticmethod backward ctx grad_output saved = ctx saved_tensors torch ones_like saved grad_output x = torch randn requires_grad=True y = torch randn z = torch randn fwd x y z out = x y z loss = MyLoss apply out loss LoggingTorchFunctionMode called_funcs append Forward loss = fwd x y z called_funcs append Backward torch _dynamo compiled_autograd _enable torch compile loss backward assertExpectedInline \n join called_funcs \ Forward mul mul sum Backward _set_multithreading_enabled backward _set_multithreading_enabled noqa B test_torch_dispatch_mode called_funcs = LoggingTorchDispatchMode TorchDispatchMode __torch_dispatch__ func types args= kwargs=None called_funcs append str func __name__ func args kwargs MyLoss torch autograd Function staticmethod forward ctx out ctx save_for_backward out out sum staticmethod backward ctx grad_output saved = ctx saved_tensors torch ones_like saved grad_output x = torch randn requires_grad=True y = torch randn z = torch randn fwd x y z out = x y z loss = MyLoss apply out loss LoggingTorchDispatchMode called_funcs append Forward loss = fwd x y z called_funcs append Backward torch _dynamo compiled_autograd _enable lambda gm gm loss backward assertExpectedInline \n join called_funcs \ Forward mul Tensor mul Tensor sum default Backward ones_like default empty memory_format empty memory_format empty memory_format empty memory_format empty memory_format empty memory_format ones_like default mul Tensor mul Tensor mul Tensor new_empty_strided default copy_ default noqa B load_test_module name testdir = Path __file__ absolute parent parent mock patch sys path sys path str testdir SourceFileLoader name str testdir f name replace py load_module make_wrapped fn ctxs functools wraps fn wrapped torch _dynamo reset stack = contextlib ExitStack ctx ctxs stack enter_context ctx out = fn stack close out wrapped lookup_backend test_name test_name xfail_by_backend inductor aot_eager test_name xfail_by_backend aot_eager eager test_name xfail_by_backend eager ca_eager assert test_name xfail_by_backend ca_eager inductor wrap_test_class orig_cls dct = orig_cls __dict__ copy name list dct keys fn = dct name callable fn name skipped_tests continue xfail_re match name name xfail_by_backend ca_eager name xfail_divergence_from_eager dct name = unittest expectedFailure name startswith test_ backend = lookup_backend name HAS_CUDA_AND_TRITON backend == inductor continue ctxs = compiled_autograd _enable make_compiler_fn backend=backend fullgraph=name known_graph_breaks_tests test_contexts get name contextlib nullcontext dct name = make_wrapped fn ctxs cls = type orig_cls __name__ + WithCompiledAutograd orig_cls __bases__ dct cls __file__ = __file__ cls known_graph_breaks_tests = test_hook_none uses assert hook test_post_accumulate_grad_hook_e e optim Adam manually graph breaks test_tensor_hooks_inplace uses assert hook test_tensor_hooks_inplace_over_view uses assert hook test_grad_fn_prehooks uses assert hook test_grad_fn_prehooks_multiple_outputs uses assert hook test_grad_fn_prehooks_remove_hooks uses handle remove hook test_tensor_hooks_inplace_multiple_outputs uses assert hook test_hooks uses assert hook test_accumulate_grad_posthooks_can_observe_tensor_prehook allclose test_saved_tensors_hook_version_counter_not_shared assertEqual test_post_accumulate_grad_hook_returns_not_None throws test_custom_function_cycle assertEqual test_mark_non_differentiable_mixed assertTrue test_materialize_grads assertEqual test_return_leaf assertEqual test_save_none_for_backward assertIsNone test_saved_variables_deprecated warnings warn test_autograd_node_isinstance assertIsInstance test_set_materialize_non_diff_grads assertIsNone test_backward_dict_grad_for_nontensor torch _custom_op autograd py skip files test_backward_dict_invalid_keys torch _custom_op autograd py skip files test_backward_dict_requires_keys_for_input_optional_tensors torch _custom_op autograd py skip files test_backward_dict_requires_keys_for_input_tensors torch _custom_op autograd py skip files test_backward_grads_are_tensor_or_none torch _custom_op autograd py skip files test_backward_impl_on_existing_op torch _custom_op autograd py skip files test_backward_returns_dict torch _custom_op autograd py skip files test_backward_tensorlist_input_requires_list_grads torch _custom_op autograd py skip files test_backward_tensorlist_input_requires_list_grads_none_or_Tensor torch _custom_op autograd py skip files test_backward_tensorlist_input_requires_list_grads_with_same_numel torch _custom_op autograd py skip files test_save_for_backward_inputs_are_namedtuple torch _custom_op autograd py skip files test_reentrant_with_leaf_variable_hook reentrant backward test_reentrant_with_non_leaf_variable_hook reentrant backward test_reentrant_child_error reentrant backward test_deep_reentrant reentrant backward test_reentrant_priority reentrant backward test_simple_reentrant reentrant backward test_checkpoint_detects_non_determinism unpack hook skip files test_checkpoint_valid_reset_on_error unpack hook skip files test_checkpointing_non_reentrant_autocast_cpu unpack hook skip files test_checkpointing_non_reentrant_autocast_gpu unpack hook skip files test_checkpointing_without_reentrant_arbitrary_input_output unpack hook skip files test_checkpointing_without_reentrant_correct_grad unpack hook skip files test_checkpointing_without_reentrant_custom_function_works unpack hook skip files test_checkpointing_without_reentrant_dataparallel _get_device_index skip files test_checkpointing_without_reentrant_detached_tensor_use_reentrant_True reentrant backward test_checkpointing_without_reentrant_parameter_used_in_an_out unpack hook skip files test_checkpointing_without_reentrant_with_context_fn unpack hook skip files test_save_on_cpu_and_checkpoint unpack hook skip files test_saved_tensor_hooks_custom_error_propagation CustomError test_access_saved_tensor_twice_without_recomputation_works unpack hook skip files test_saved_tensor_hooks_extra_enter_during_bw_no_leak ctx skip files test_saved_tensor_hooks_extra_exit_during_bw_no_crash ctx skip files test_checkpointing reentrant backward test_checkpointing_without_reentrant_input_requires_grad_False reentrant backward test_checkpointing_without_reentrant_input_requires_grad_True reentrant backward test_checkpointing_without_reentrant_memory_savings reentrant backward test_dtensor_basic torch _dynamo exc Unsupported Failed convert args kwargs proxy test_dtensor_contiguous_dtensor_noncontiguous_local_as_tangent subclass constructor test_retain_grad retains_grad_hooks test_retain_grad_cycle retains_grad_hooks test_retain_grad_inplace retains_grad_hooks test_retain_grad_inplace_over_view retains_grad_hooks test_retains_grad_can_always_observe_tensor_prehook retains_grad_hooks test_retains_grad_inplace_multiple_outputs retains_grad_hooks test_hook_edge_case_when_called_with_grad retains_grad_hooks test_multi_grad_all_hooks retains_grad_hooks test_prehook_ordering retains_grad_hooks test_will_engine_execute_node retains_grad_hooks test_backward_to_node retains_grad_hooks test_backward_with_nonleaf_inputs retains_grad_hook non-leaf input test_create_graph_and_full_backward_hook_cycle _pack_with_none test_full_backward_hook_double_backward _pack_with_none test_grad_mode_restored_reentrant assertTrue test_multi_grad_any_hooks register_multi_grad_hook test_saved_variable_packing_unpacking_did_not_save_original_with_hooks register_hooks test_graph_save_on_cpu dynamo disabled test_nested_checkpoint_early_stop_False dynamo disable test_nested_checkpoint_early_stop_True dynamo disable test_nested_checkpoint_kwargs_early_stop_False dynamo disable test_nested_checkpoint_kwargs_early_stop_True dynamo disable test_nested_checkpoint_non_tensor_inputs_and_outputs_early_stop_False dynamo disable test_nested_checkpoint_non_tensor_inputs_and_outputs_early_stop_True dynamo disable test_nested_checkpoint_reentrant_backwards_early_stop_False dynamo disable test_nested_checkpoint_reentrant_backwards_early_stop_True dynamo disable test_nested_checkpoint_same_graph_early_stop_False dynamo disable test_nested_checkpoint_same_graph_early_stop_True dynamo disable test_nested_checkpoint_set_early_stop dynamo disable test_nested_checkpoint_two_children_early_stop_False dynamo disable test_nested_checkpoint_two_children_early_stop_True dynamo disable test_custom_autograd_ac_early_stop marked skipped test_dropout dynamo disable test_dropout_inductor dynamo disable test_function_with_kwargs dynamo disable test_module dynamo disable test_contexts = test_setitem_mask config patch capture_dynamic_output_shape_ops=True test_index_backward_does_not_save_tensor config patch capture_dynamic_output_shape_ops=True These groups tests aren t supported yet xfail_re = re compile r ^test_ sparse &#124; profiler &#124; gradcheck &#124; named_tensor Tests fail different stages we categorize them wrt their backends We run only last passing backend order ca_eager - eager - aot_eager - inductor xfail_by_backend = ca_eager xfail test_callback_propagates_errors_from_device_thread fullgraph queue_callback graph break RuntimeError test_reentrant_with_callbacks_both_depths queue_callback test_reentrant_with_callbacks_depth_ queue_callback test_reentrant_with_callbacks_depth_ queue_callback test_current_graph_task_execution_order nodes already freed time dynamo traces lifted hook test_autograd_inplace_views_cross_dtype view_fn supported compiled autograd test_post_accumulate_grad_hook_ordering accuracy error test_current_graph_task_id autograd state already cleared once dynamo called test_custom_function_forward_mode_forward_is_no_op forward AD test_custom_function_forward_mode_inplace_checks forward AD test_custom_function_forward_mode_view_checks forward AD test_custom_function_forward_mode_wrong_formula forward AD test_node_post_hook_registered_during_unpack_hook NoneType object has no attribute register_hook test_custom_function_error forward AD test_custom_function_save_for_forward forward AD test_dont_materialize_grads undefined grad test_no_grad_copy setting static member lifted backward test_no_grad_copy_sparse setting static member lifted backward test_node_ordering_when_none_returned torch _dynamo exc Unsupported TypeError built-in method clone test_save_output_nr output_nr grad passed None IndexError list index out range NB x grad = y where both x y input tensors test_grad_nonleaf_register_hook test_backward_twice_without_saved_values https github com pytorch pytorch issues Category Higher Order Gradients test_default_saved_tensors_hooks_double_backward wrong when pack hook returns non-leaf test_saved_variable_packing_unpacking_saved_original_with_hooks wrong when pack hook returns non-leaf test_nested_anomaly_detect_nan nested anomaly test_select_sum batched gradients test_custom_autograd_no_early_free batched gradients test_grad_batched_grad batched gradients Uncategorized test_lobpcg NaNs test_autograd_simple_views_python gradient None test_function_returns_undefined_tensor gradient None test_input_buffer_accum add sparse dense test_return_duplicate batched gradients test_return_duplicate_inplace batched gradients test_naughty_autograd_function_stashing_ctx error raised test_unrelated_inputs batched gradients test_nested_checkpoint_early_stop_False unpack hook grad_fn semantics test_nested_checkpoint_early_stop_True unpack hook grad_fn semantics test_nested_checkpoint_two_children_early_stop_False unpack hook grad_fn semantics test_nested_checkpoint_two_children_early_stop_True unpack hook grad_fn semantics test_dropout functionalize_rng_ops yet supported test_dropout_inductor functionalize_rng_ops yet supported test_function_with_kwargs functionalize_rng_ops yet supported test_module functionalize_rng_ops yet supported test_grad_dtype AttributeError args Float did match Double eager will run without torch compiling CA graph test_setup_context_when_forward_has_default_args autograd Function methods test_accumulate_grad_tensor_reference Out bounds frame_state_entry stride i None test_custom_function_exception torch no_grad torch _dynamo exc Unsupported missing WITH_EXCEPT_START test_to_sparse_backward Out bounds frame_state_entry stride i None test_custom_function_non_tensor_inputs_outputs gradient batching rule implemented aten sym_size int test_setitem CopySlices accuracy error test_checkpointing_without_reentrant_saved_object_identity same https github com pytorch pytorch issues test_dtensor_different_gradient_placement Dynamo failed run FX node fake tensors test_dtensor_noncontiguous_output Dynamo failed run FX node fake tensors test_dtensor_partial_placement_graph_output Dynamo failed run FX node fake tensors test_unwrap_async_collective_tensor_tangent AttributeError PlainTensorMeta object has no attribute attrs test_graph_save_on_cpu torch save should no-op recorded graph test_saving_variable_to_disk torch save should no-op recorded graph test_nested_checkpoint_early_stop_False AOT backward higher order gradients Slow tests these tests close CI timeout we try torch compile them test_checkpointing test_checkpointing_without_reentrant_memory_savings test_checkpointing_without_reentrant_input_requires_grad_True test_checkpointing_without_reentrant_input_requires_grad_False aot_eager will run torch compile backend= eager Category FakeTensor test_wrapped_number_saved_tensors_hooks Proxy tensor should carryover is_wrapped_number_ its original test_scalar_grad_mixed_device Fake Tensors aren t propagating device properly -dim grads test_grad AOT backward higher order gradients test_grad_materialize_grads AOT backward higher order gradients inductor will run torch compile backend= aot_eager tests present dict will run torch compile backend= inductor These tests fail due difference semantics we won t fix xfail_divergence_from_eager = test_invalid_gradients can t give autograd error due inaccurate output metadata lifted backward test_autograd_node_isinstance backward ctx fake cls directly Node instance test_backward_hook_relative_ordering compiled autograd collects breadth first module backward hook supported test_checkpointing_without_reentrant_custom_function_works ctx saved_tensors cached CA test_anomaly_mode_no_check_nan different error messages test_anomaly_grad_warnings different error messages test_anomaly_detect_nan fake tensor errors NaN test_once_differentiable different node name CompiledFunctionBackward test_function different node name CompiledFunctionBackward test_inplace_on_view_backward different node name CompiledFunctionBackward test_nested_anomaly_printstack_cleanup anomaly NaN error message different test_not_implemented_grad Dynamo changes types exceptions test_grad_call_compiled_backward_fn different functorch error test_vjp_call_compiled_backward_fn different functorch error test_vmap_call_compiled_backward_fn different functorch error test_accumulate_grad always out place add compiled autograd test_current_node slightly different dispatched ops skipped_tests = set HAS_CUDA_AND_TRITON Found Tesla M which too old supported triton GPU compiler skipped_tests add test_type_conversions IS_S X skipped_tests add test_deep_reentrant test_autograd = load_test_module test_autograd test_custom_ops = load_test_module test_custom_ops test_higher_order_ops = load_test_module dynamo test_higher_order_ops TestAutogradWithCompiledAutograd = wrap_test_class test_autograd TestAutograd TestNestedCheckpointWithCompiledAutograd = wrap_test_class test_autograd TestNestedCheckpoint TestCustomOpWithCompiledAutograd = wrap_test_class test_custom_ops TestCustomOp HigherOrderOpTestsWithCompiledAutograd = wrap_test_class test_higher_order_ops HigherOrderOpTests FuncTorchHigherOrderOpTestsWithCompiledAutograd = wrap_test_class test_higher_order_ops FuncTorchHigherOrderOpTests ActivationCheckpointingTestsWithCompiledAutograd = wrap_test_class test_higher_order_ops ActivationCheckpointingTests torch distributed is_available HAS_CUDA_AND_TRITON test_dtensor = load_test_module distributed tensor test_dtensor_compile TestDTensorCompileWithCompiledAutograd = wrap_test_class test_dtensor TestDTensorCompile xfail_hops = local_map_hop TestCompiledAutogradOpInfo TestCase setUp - None super TestCase setUp reset tearDown - None super TestCase tearDown reset ops list filter lambda op op name xfail_hops hop_db allowed_dtypes= torch float test_hops_in_bwd device dtype op create_bwd_fn_closure op_args op_kwargs op_out_ref = Foo torch autograd Function staticmethod forward ctx x x staticmethod backward ctx grad out = op op op_args op_kwargs op_out_ref append out grad fn x Foo apply x sum fn op_out_ref Note requires_grad=False because aot dispatch already covered elsewhere inp op sample_inputs device dtype requires_grad=False input = inp input isinstance inp input tuple inp input eager_args = input inp args eager_kwargs = inp kwargs compiled_args = deepcopy eager_args compiled_kwargs = deepcopy eager_kwargs Run eager torch manual_seed dummy = torch randn dtype=dtype device=device requires_grad=True fn op_out_ref = create_bwd_fn_closure eager_args eager_kwargs fn dummy backward assertEqual len op_out_ref expected = op_out_ref Run under CA torch manual_seed dummy = torch randn dtype=dtype device=device requires_grad=True fn op_out_ref = create_bwd_fn_closure compiled_args compiled_kwargs compiled_autograd _enable make_compiler_fn backend= aot_eager fn dummy backward assertEqual len op_out_ref actual = op_out_ref assertEqual expected actual instantiate_device_type_tests TestCompiledAutogradOpInfo globals instantiate_parametrized_tests TestCompiledAutograd __name__ == __main__ HAS_CPU run_tests needs= filelock