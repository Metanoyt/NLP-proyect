mypy ignore-errors warnings numpy np pandas pd type ignore import-untyped scipy stats gmean type ignore import-untyped sklearn model_selection train_test_split type ignore import-untyped sklearn tree DecisionTreeRegressor type ignore import-untyped train AHTrain torch _inductor autoheuristic autoheuristic_utils CHOICE_COL FEEDBACK_COL TODO AlnisM Fix these warnings warnings filterwarnings ignore message= The behavior DataFrame concatenation empty all-NA entries deprecated warnings filterwarnings ignore message= DataFrameGroupBy apply operated grouping columns AHTrainRegressionTree AHTrain This responsible generating heuristic using data collected AutoHeuristic It will learn regression tree predicts score represents how well specific choice will perform given input A higher score means better choice The heuristic will generated file named heuristic_name py torch _inductor autoheuristic artifacts directory __init__ super __init__ main log_path other_datasets nrows heuristic_name save_dot=False ranking=False Main function trains decision tree generates heuristic df choices cat_feature cats dummy_col_ _col_val metadata = get_df log_path nrows=nrows apply_filters=True df_train df_val df_test feature_columns = custom_train_test_split df datasets = train df_train val df_val test df_test add_real_datasets datasets other_datasets cat_feature cats We will do grid search over these values Only trying out max_depths because we want keep tree generated code small smaller than does perform well enough max_depths = min_samples_leafs = choice_columns = f CHOICE_COL _ choice choice choices results_df best_model threshold = train_and_evaluate_models datasets feature_columns choice_columns max_depths min_samples_leafs prints results all models datasets print results_df to_string prints results grouped dataset set_name results_df dataset unique dataset_results = results_df results_df dataset == set_name dataset_results = dataset_results sort_values by= correct print dataset_results to_string + \n feature_names = feature_columns + choice_columns dt_to_python best_model metadata feature_names dummy_col_ _col_val heuristic_name threshold get_df log_path cat_feature cats=None nrows=None apply_filters=False Parses log file processes data into dataframe can used training df metadata feature_columns categorical_features choices = parse_log log_path nrows process_data df feature_columns apply_filters min_count_measurements= max_relative_std= Calculate statistics each input choice combination calculate_stats group count = len group mean = group FEEDBACK_COL mean std = group FEEDBACK_COL std relative_std = std mean mean = np inf median = group FEEDBACK_COL median pd Series count count median_execution_time median relative_std relative_std stats = df groupby feature_columns + CHOICE_COL apply calculate_stats reset_index apply_filters Remove unstables measurements valid_stats = stats stats count = min_count_measurements stats relative_std = max_relative_std Keep only inputs least two valid choices valid_inputs = valid_stats groupby feature_columns filter lambda x len x = valid_inputs = stats Compute winner ratios each input get_winner_and_speedups group mean_time = group median_execution_time mean winner = group loc group median_execution_time idxmin CHOICE_COL min_time = group median_execution_time min max_time = group median_execution_time max group winner = winner group speedup = max_time min_time group target = mean_time group median_execution_time group feature_columns + CHOICE_COL winner speedup target results = valid_inputs groupby feature_columns apply get_winner_and_speedups reset_index drop=True results results = process_data df feature_columns apply_filters results added_categorical_features = add_new_features results categorical_features += added_categorical_features categorical_features += CHOICE_COL results cat_feature cats dummy_col_ _col_val = handle_categorical_features cat_feature cats categorical_features results results choices cat_feature cats dummy_col_ _col_val metadata custom_train_test_split df test_size= val_size= random_state= Splits dataframe into train val test sets Also adds other datasets specified user train set We need careful because we want make sure rows same input different choice kept same set e g Rows looks like input_ choice input_ choice should same set We want make sure rows same input different choice kept same set exclude_columns = speedup winner target feature_columns = col col df columns col exclude_columns col startswith CHOICE_COL + _ df input_id = df groupby feature_columns ngroup Get unique input IDs unique_inputs = df input_id unique Split unique inputs into train+val test train_val_inputs test_inputs = train_test_split unique_inputs test_size=test_size random_state=random_state Split train+val inputs into train val train_inputs val_inputs = train_test_split train_val_inputs test_size=val_size random_state=random_state Create masks each set train_mask = df input_id isin train_inputs val_mask = df input_id isin val_inputs test_mask = df input_id isin test_inputs Split dataframe df_train = df train_mask df_val = df val_mask df_test = df test_mask Remove temporary input_id column df_train = df_train drop input_id axis= df_val = df_val drop input_id axis= df_test = df_test drop input_id axis= df_train df_val df_test feature_columns train_and_evaluate_models datasets feature_columns choice_columns max_depths min_samples_leafs threshold= Does grid search over max_depths min_samples_leafs returns best model results = df_train = datasets train df_val = datasets val best_model = None best_model_threshold = max_correct_predictions = - max_depth max_depths min_samples_leaf min_samples_leafs print f Evaluating max_depth= max_depth min_samples_leaf= min_samples_leaf model = DecisionTreeRegressor random_state= max_depth=max_depth min_samples_leaf=min_samples_leaf model fit df_train feature_columns + choice_columns df_train target we first compute safe threshold threshold ensures validation set heuristic returns choice choice will correct although high threshold can lead lot unsure choices eval_result = evaluate_model model df_val feature_columns choice_columns threshold safe_threshold = eval_result wrong_max_ratio dataset_name dataset datasets items eval_result = evaluate_model model dataset feature_columns choice_columns safe_threshold print eval_result dataset_name == val eval_correct = eval_result correct eval_correct max_correct_predictions best_model = model best_model_threshold = safe_threshold max_correct_predictions = eval_correct results append max_depth max_depth min_samples_leaf min_samples_leaf dataset dataset_name correct eval_result correct wrong eval_result wrong unsure eval_result unsure total eval_result total max_wrong_speedup eval_result max_wrong_speedup gman_wrong_speedup eval_result gman_wrong_speedup threshold safe_threshold pd DataFrame results best_model best_model_threshold evaluate_model model df feature_columns choice_columns threshold Custom evaluation function evaluates learned decision tree predict_winner group predictions = model predict group feature_columns + choice_columns Find index maximum prediction best choice best_choice_index = np argmax predictions Get corresponding choice predicted_choice = group choice_columns iloc best_choice_index idxmax split _ - Calculate ratio between best second-best prediction sorted_predictions = np sort predictions - top_pred_ratio = sorted_predictions sorted_predictions len sorted_predictions np inf If best choice significantly better than second best choice learned heuristic will unsure top_pred_ratio = threshold predicted_winner = unsure predicted_winner = predicted_choice actual_winner = group winner iloc is_correct = predicted_winner == actual_winner predicted_winner = unsure unsure pd Series predicted_winner predicted_winner ratio top_pred_ratio actual_winner actual_winner is_correct is_correct speedup group speedup iloc Speedup same all rows group results = df groupby feature_columns apply predict_winner reset_index correct = results is_correct eq True sum unsure = results is_correct == unsure sum wrong_results = results results is_correct eq False wrong = len wrong_results Calculate max geometric mean speedup wrong predictions Used debugging purposes wrong_speedups = wrong_results speedup max_wrong_speedup = wrong_speedups max wrong_speedups empty np nan geo_mean_wrong_speedup = gmean wrong_speedups wrong_speedups empty np nan wrong_max_ratio = wrong_results ratio max total = correct + wrong + unsure correct correct wrong wrong unsure unsure total total max_wrong_speedup max_wrong_speedup gman_wrong_speedup geo_mean_wrong_speedup wrong_max_ratio wrong_max_ratio dt_to_python dt metadata feature_names dummy_col_ _col_val heuristic_name threshold unsafe_leaves=None tree_ = dt tree_ feature_name = feature_names i i = - undefined i tree_ feature lines = device_capa = metadata device_capa device_capa_str = f device_capa device_capa opt_name = metadata name lines append codegen_boilerplate heuristic_name opt_name threshold metadata shared_memory device_capa_str dt fn_def = f \n gen_predict_fn_def lines append fn_def dt_to_python node depth indent = depth + tree_ feature node = - name = feature_name node threshold = tree_ threshold node name dummy_col_ _col_val orig_name value = dummy_col_ _col_val name predicate = f indent str context get_value orig_name = value assert threshold == f expected threshold threshold predicate = f indent context get_value name = threshold lines append predicate dt_to_python tree_ children_left node depth + lines append f indent dt_to_python tree_ children_right node depth + lines append handle_leaf tree_ node indent unsafe_leaves dt_to_python write_heuristic_to_file lines heuristic_name handle_leaf tree_ node indent unsafe_leaves Generates code leaf node This just value predicted regression tree value = tree_ value node f indent str value gen_predict_fn_def predict context AHContext - float codegen_boilerplate heuristic_name opt_name threshold shared_memory device_capa classes Generates boilerplate code generated heuristic This includes things like imports definition etc boiler_plate = f flake noqa B fmt off This file generated AutoHeuristic Do modify manually To regenerate file take look steps README md file inside torchgen _autoheuristic opt_name torch _inductor autoheuristic autoheuristic_utils AHContext AHMetadata Choice CHOICE_COL torch _inductor autoheuristic learnedheuristic_interface LearnedHeuristicRegression heuristic_name LearnedHeuristicRegression __init__ - None pass gen_precondition opt_name shared_memory device_capa get_feedback context AHContext choice Choice - float context context_dict CHOICE_COL = choice predict context get_confidence_threshold - float threshold get_name - str opt_name boiler_plate __name__ == __main__ train = AHTrain train generate_heuristic