mypy allow-untyped-defs threading torch torch distributed autograd dist_autograd torch distributed rpc rpc torch optim torch distributed optim DistributedOptimizer torch testing _internal dist_utils dist_init torch testing _internal distributed rpc rpc_agent_test_fixture RpcAgentTestFixture MyModule lock = threading Lock __init__ requires_grad=True cannot directly use torch manual_seed all threads share same default generator The race multiple RPC threads could mess up draw order default RNG instance leading non-deterministic behavior Hence create dedicated RNG here g_cpu = torch Generator g_cpu manual_seed w = torch rand requires_grad=requires_grad generator=g_cpu forward t torch mm w t get_w w FailingOptimizer optim Optimizer __init__ params super __init__ params step closure=None raise ValueError Error running optimizer OptimizerFailingOnConstructor optim Optimizer __init__ params super __init__ params raise ValueError Error creating optimizer step closure=None raise NotImplementedError _call_method method obj_rref args kwargs method obj_rref local_value args kwargs remote_method method obj_rref args kwargs Call rpc remote method remote object Args method method example Class method obj_rref RRef remote reference object args positional arguments pass method kwargs keyword arguments pass method Returns RRef remote method call result rpc remote obj_rref owner _call_method args= method obj_rref + list args kwargs=kwargs rpc_async_method method obj_rref args kwargs Call rpc rpc_async method remote object Args method method example Class method obj_rref RRef remote reference object args positional arguments pass method kwargs keyword arguments pass method Returns Future method call result rpc rpc_async obj_rref owner _call_method args= method obj_rref + list args kwargs=kwargs DistOptimizerTest RpcAgentTestFixture dist_init test_dist_optim_exception distributed version owner = f worker rank + world_size d owner = f worker rank + world_size d remote_module = rpc remote owner MyModule remote_module = rpc remote owner MyModule remote_param = remote_method MyModule get_w remote_module remote_param = remote_method MyModule get_w remote_module dist_optim = DistributedOptimizer FailingOptimizer remote_param remote_param dist_autograd context context_id g_cpu = torch Generator g_cpu manual_seed t = torch rand requires_grad=True generator=g_cpu t = torch rand requires_grad=True generator=g_cpu output = rpc_async_method MyModule forward remote_module t output = rpc_async_method MyModule forward remote_module output wait loss = torch add output wait t sum dist_autograd backward context_id loss assertRaisesRegex Exception Error running optimizer dist_optim step context_id dist_init test_dist_optim_exception_on_constructor distributed version owner = f worker rank + world_size d owner = f worker rank + world_size d remote_module = rpc remote owner MyModule remote_module = rpc remote owner MyModule remote_param = remote_method MyModule get_w remote_module remote_param = remote_method MyModule get_w remote_module assertRaisesRegex Exception Error creating optimizer DistributedOptimizer OptimizerFailingOnConstructor remote_param remote_param _test_dist_optim_base optim_cls args kwargs local version module = MyModule module = MyModule params = module get_w module get_w local_optim = optim_cls params args kwargs old_w = module w detach clone old_w = module w detach clone g_cpu = torch Generator g_cpu manual_seed t = torch rand requires_grad=True generator=g_cpu t = torch rand requires_grad=True generator=g_cpu output = module forward t output = module forward output loss = torch add output t sum loss backward local_optim step distributed version owner = f worker rank + world_size d owner = f worker rank + world_size d remote_module = rpc remote owner MyModule remote_module = rpc remote owner MyModule remote_param = remote_method MyModule get_w remote_module remote_param = remote_method MyModule get_w remote_module sanity check local remote initial weights should match assertEqual old_w remote_param to_here assertEqual old_w remote_param to_here dist_optim = DistributedOptimizer optim_cls remote_param remote_param args kwargs dist_autograd context context_id g_cpu manual_seed t = torch rand requires_grad=True generator=g_cpu t = torch rand requires_grad=True generator=g_cpu output = rpc_async_method MyModule forward remote_module t output = rpc_async_method MyModule forward remote_module output wait loss = torch add output wait t dist_autograd backward context_id loss sum dist_optim step context_id new_w = rpc_async_method MyModule get_w remote_module wait new_w = rpc_async_method MyModule get_w remote_module wait ensure optimizer changed weights assertNotEqual old_w new_w assertNotEqual old_w new_w ensure local equals remote assertEqual new_w module get_w assertEqual new_w module get_w dist_init test_dist_optim _test_dist_optim_base optim Adagrad lr= _test_dist_optim_base optim Adam lr= e- amsgrad=True _test_dist_optim_base optim AdamW lr= amsgrad=True _test_dist_optim_base optim SGD lr= _test_dist_optim_base optim SGD lr= e- momentum= weight_decay= nesterov=True _test_dist_optim_base optim Adadelta rho= _test_dist_optim_base optim RMSprop lr= _test_dist_optim_base optim Adamax lr= _test_dist_optim_base optim Rprop lr= _test_dist_optim_none_grads optim_cls args kwargs local version module = MyModule module = MyModule requires_grad=False params = module get_w module get_w local_optim = optim_cls params args kwargs old_w = module w detach clone old_w = module w detach clone g_cpu = torch Generator g_cpu manual_seed t = torch rand requires_grad=True generator=g_cpu t = torch rand requires_grad=True generator=g_cpu output = module forward t output = module forward output loss = torch add output t sum loss backward local_optim step distributed version owner = f worker rank + world_size d owner = f worker rank + world_size d remote_module = rpc remote owner MyModule remote_module = rpc remote owner MyModule args= False remote_param = remote_module remote get_w remote_param = remote_module remote get_w sanity check local remote initial weights should match assertEqual old_w remote_param to_here assertEqual old_w remote_param to_here dist_optim = DistributedOptimizer optim_cls remote_param remote_param args kwargs dist_autograd context context_id g_cpu manual_seed t = torch rand requires_grad=True generator=g_cpu t = torch rand requires_grad=True generator=g_cpu output = remote_module rpc_async forward t output = remote_module rpc_async forward output wait loss = torch add output wait t dist_autograd backward context_id loss sum dist_optim step context_id new_w = remote_module rpc_async get_w wait new_w = remote_module rpc_async get_w wait ensure optimizer changed weights w assertNotEqual old_w new_w ensure optimizer changed weights w assertEqual old_w new_w ensure local equals remote assertEqual new_w module get_w assertEqual new_w module get_w dist_init test_dist_optim_none_grads _test_dist_optim_none_grads optim SGD lr= _test_dist_optim_none_grads optim RMSprop lr= _test_dist_optim_none_grads optim Rprop lr= _test_dist_optim_none_grads optim Adadelta rho=