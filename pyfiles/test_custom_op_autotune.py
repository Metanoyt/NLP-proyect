Owner s module inductor Tests custom operation autotuning PyTorch Inductor Validates custom ops can registered multiple CustomOpConfigs where each config specifies optional decomposition function its associated parameters Inductor benchmarks all variants automatically selects best performing one torch torch _inductor config torch _inductor kernel custom_op CustomOpConfig register_custom_op_autotuning torch _inductor test_case run_tests TestCase torch testing _internal common_utils skipIfXpu torch testing _internal inductor_utils HAS_GPU torch set_float _matmul_precision high TestCustomOpAutoTune TestCase Test custom operation autotuning functionality setUp - None Set up test environment appropriate device dtype super setUp device = cuda HAS_GPU cpu dtype = torch float device == cuda torch float _run_autotune_test op_object inputs expected test_name Shared test infrastructure autotuning tests torch compile test_model args op_object args torch _dynamo reset autotune_backends = TRITON device == cuda ATEN config patch max_autotune=True max_autotune_gemm_backends=autotune_backends fx_graph_cache=False benchmark_kernel=True compiled_result = test_model inputs assertEqual compiled_result shape expected shape f test_name shape mismatch torch testing assert_close compiled_result expected rtol= e- atol= e- msg=f test_name numerical mismatch _assert_implementations_equivalent decompositions inputs op_name Utility assert all implementations produce equivalent results implementations = func __name__ func func decompositions results = name impl implementations result = impl inputs results name = result Basic sanity checks assertTrue torch isfinite result all f op_name name produced non-finite values Verify numerical equivalence reference_name reference_result = next iter results items name result results items name = reference_name rtol = e- Approximated name e- atol = e- Approximated name e- torch testing assert_close result reference_result rtol=rtol atol=atol msg=f op_name name differs reference_name _create_rmsnorm_inputs batch_size= seq_len= hidden_dim= Create test inputs RMSNorm operations input_tensor = torch randn batch_size seq_len hidden_dim device=self device dtype=self dtype requires_grad=False weight = torch randn hidden_dim device=self device dtype=self dtype requires_grad=False input_tensor weight _create_mlp_inputs batch_size= seq_len= hidden_dim= intermediate_dim= output_dim= Create test inputs MLP operations input_tensor = torch randn batch_size seq_len hidden_dim device=self device dtype=self dtype requires_grad=False gate_weight = torch randn hidden_dim intermediate_dim device=self device dtype=self dtype requires_grad=False up_weight = torch randn hidden_dim intermediate_dim device=self device dtype=self dtype requires_grad=False down_weight = torch randn intermediate_dim output_dim device=self device dtype=self dtype requires_grad=False input_tensor gate_weight up_weight down_weight skipIfXpu test_rmsnorm_custom_op_autotune_with_dynamic_shape Test RMSNorm autotuning multiple decomposition variants dynamic shapes Validates - Multiple decomposition implementations different computational approaches - Dynamic shape handling across multiple compilations test_op_name = f test_lib rmsnorm_ id rmsnorm_decomposition x torch Tensor weight torch Tensor eps float = e- - torch Tensor Variance-based approach compute variance then rsqrt variance = x pow mean dim=- keepdim=True rstd = torch rsqrt variance + eps x rstd weight rmsnorm_decomposition x torch Tensor weight torch Tensor eps float = e- - torch Tensor Separate normalization scaling compute normalized value then scale x_var = x variance = x_var pow mean dim=- keepdim=True x = x torch rsqrt variance + eps x = x weight x torch library custom_op test_op_name mutates_args= test_rmsnorm_op input_tensor torch Tensor weight torch Tensor eps float = e- - torch Tensor torch nn functional rms_norm input_tensor input_tensor shape - weight eps=eps test_rmsnorm_op register_fake _ input_tensor torch Tensor weight torch Tensor eps float = e- torch empty_like input_tensor decompositions = rmsnorm_decomposition rmsnorm_decomposition register_custom_op_autotuning test_rmsnorm_op configs= CustomOpConfig decomp decomp decompositions name= test_rmsnorm_autotuned input_gen_fns= x lambda x torch randn_like x device=self device weight lambda weight torch ones_like weight device=self device Test multiple shapes verify dynamic shape handling test_shapes = i batch_size seq_len hidden_dim enumerate test_shapes input_tensor weight = _create_rmsnorm_inputs batch_size seq_len hidden_dim Test numerical equivalence all decompositions _assert_implementations_equivalent decompositions input_tensor weight f RMSNorm_ i Test autotuning expected = rmsnorm_decomposition input_tensor weight _run_autotune_test test_rmsnorm_op input_tensor weight expected f RMSNorm_ i skipIfXpu test_mlp_custom_op_autotune Test MLP autotuning method parameter controlling different decomposition variants Validates parametric tuning where same decomposition function uses different algorithmic approaches based method parameter standard matmul batched mm fused weights test_op_name = f test_lib mlp_ id mlp_variants input_tensor torch Tensor gate_weight torch Tensor up_weight torch Tensor down_weight torch Tensor method int = - torch Tensor MLP implementation different computational approaches controlled method parameter method == gate_proj = torch matmul input_tensor gate_weight up_proj = torch matmul input_tensor up_weight gated = torch relu gate_proj up_proj torch matmul gated down_weight method == batch_shape = input_tensor shape - hidden_dim = input_tensor shape - output_dim = down_weight shape - input_ d = input_tensor view - hidden_dim gate_proj = torch mm input_ d gate_weight up_proj = torch mm input_ d up_weight gated = torch relu gate_proj up_proj output_ d = torch mm gated down_weight output_ d view batch_shape output_dim torch library custom_op test_op_name mutates_args= test_mlp_op input_tensor torch Tensor gate_weight torch Tensor up_weight torch Tensor down_weight torch Tensor method int = - torch Tensor mlp_variants input_tensor gate_weight up_weight down_weight method=method test_mlp_op register_fake _ input_tensor torch Tensor gate_weight torch Tensor up_weight torch Tensor down_weight torch Tensor method int = torch empty input_tensor shape - + down_weight shape - device=input_tensor device dtype=input_tensor dtype Use explicit config method parameter tuning knob register_custom_op_autotuning test_mlp_op configs= CustomOpConfig method= CustomOpConfig method= name= test_mlp_autotuned input_gen_fns= input_tensor lambda fake_tensor torch randn_like fake_tensor device=self device gate_weight lambda fake_tensor torch randn_like fake_tensor device=self device up_weight lambda fake_tensor torch randn_like fake_tensor device=self device down_weight lambda fake_tensor torch randn_like fake_tensor device=self device Create test inputs input_tensor gate_weight up_weight down_weight = _create_mlp_inputs Test all method variants produce numerically equivalent results expected = mlp_variants input_tensor gate_weight up_weight down_weight method= Test autotuning _run_autotune_test test_mlp_op input_tensor gate_weight up_weight down_weight expected MLP _create_decompose_k_inputs m= k= n= Create test inputs decompose_k matrix multiplication - divisible all k_splits values Ensure k divisible all k_splits values k = k + Round up nearest multiple = torch randn m k device=self device dtype=self dtype requires_grad=False b = torch randn k n device=self device dtype=self dtype requires_grad=False b skipIfXpu test_decompose_k_custom_op_autotune Test decompose_k autotuning parametric tuning k_splits values Validates numerical parameter sweep where k_splits controls how K dimension decomposed matrix multiplication k_splits test_op_name = f test_lib decompose_k_ id decompose_k_implementation torch Tensor b torch Tensor k_splits int = - torch Tensor Matrix multiply k-way decomposition - Python implementation m = shape n = b shape k = shape k_parts = k k_splits B = k_splits a_reshaped = torch permute reshape m B k_parts B m k_parts b_reshaped = b reshape B k_parts n B k_parts n result = torch bmm a_reshaped b_reshaped B m n torch sum result dim= m n torch library custom_op test_op_name mutates_args= test_decompose_k_op torch Tensor b torch Tensor k_splits int = - torch Tensor Matrix multiply k-way decomposition - custom op using decomposition decompose_k_implementation b k_splits test_decompose_k_op register_fake _ torch Tensor b torch Tensor k_splits int = torch empty shape b shape device=a device dtype=a dtype Register autotuning different k_splits values using decomposition function register_custom_op_autotuning test_decompose_k_op configs= CustomOpConfig k_splits= CustomOpConfig k_splits= CustomOpConfig k_splits= CustomOpConfig k_splits= CustomOpConfig k_splits= CustomOpConfig k_splits= CustomOpConfig k_splits= name= test_decompose_k_autotuned input_gen_fns= lambda fake_tensor torch randn_like fake_tensor device=self device b lambda fake_tensor torch randn_like fake_tensor device=self device b = _create_decompose_k_inputs expected = b _run_autotune_test test_decompose_k_op b expected DecomposeK skipIfXpu test_multi_parameter_tuning Test autotuning multiple parameters combinatorial parameter exploration Validates parametric tuning multiple parameters scale_mode chunk_size test combinatorial exploration parameter space test_op_name = f test_lib multi_param_ id multi_param_scaling x torch Tensor factor torch Tensor scale_mode int = chunk_size int = - torch Tensor Different scaling approaches controlled scale_mode parameter scale_mode == Simple broadcasting x factor scale_mode == Process chunks batch_size seq_len = x shape chunks = start range seq_len chunk_size end = min start + chunk_size seq_len chunk = x start end chunks append chunk factor torch cat chunks dim= scale_mode == Using einsum scaling torch einsum i i- i x factor torch library custom_op test_op_name mutates_args= multi_param_op x torch Tensor factor torch Tensor scale_mode int = chunk_size int = - torch Tensor multi_param_scaling x factor scale_mode chunk_size multi_param_op register_fake _ x torch Tensor factor torch Tensor scale_mode int = chunk_size int = torch empty_like x Use explicit configs scale_mode chunk_size parameters tuning knobs register_custom_op_autotuning multi_param_op configs= CustomOpConfig scale_mode= Broadcast CustomOpConfig scale_mode= chunk_size= Chunked CustomOpConfig scale_mode= chunk_size= Chunked CustomOpConfig scale_mode= Einsum name= multi_param_autotuned input_gen_fns= x lambda t torch randn_like t device=self device factor lambda t torch ones t shape - device=self device dtype=t dtype Create test inputs test_x = torch randn device=self device dtype=self dtype test_factor = torch ones device=self device dtype=self dtype Verify numerical equivalence across all approaches expected_result = test_x test_factor Test each scale_mode variant configs = broadcast chunk_size ignored chunked size chunked size einsum chunk_size ignored scale_mode chunk_size configs result = multi_param_scaling test_x test_factor scale_mode=scale_mode chunk_size=chunk_size torch testing assert_close result expected_result rtol= e- atol= e- msg=f scale_mode scale_mode chunk_size chunk_size equivalent expected Test autotuning _run_autotune_test multi_param_op test_x test_factor expected_result MultiParam __name__ == __main__ run_tests