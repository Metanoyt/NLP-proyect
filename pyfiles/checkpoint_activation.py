mypy allow-untyped-defs collections abc Generator contextlib AbstractContextManager contextmanager nullcontext typing Any Optional torch torch nn nn torch utils checkpoint _checkpoint_without_reentrant_generator _DEFAULT_DETERMINISM_MODE contract _State contract contextmanager _no_hook module nn Module user_ctx Optional AbstractContextManager = None r Disable hooks installed checkpoint avoid unintentional recursion during backward recomputation user_ctx user_ctx nullcontext orig_enable_hook = checkpoint state module enable_hook checkpoint state module enable_hook = False try yield finally checkpoint state module enable_hook = orig_enable_hook _CheckpointState _State enable_hook bool = False _ac_generator Optional Generator None None None contract _CheckpointState checkpoint module nn Module kwargs - nn Module r This composable activation checkpointing API Unlike functional activation checkpointing APIs one does require changing model source code Unlike ` ` nn Module ` ` wrapper activation checkpointing APIs one does modify model structure fully-qualified names either Under hood registers activation checkpointing logic pre- post-forward hooks Hence API can easily applied any model sub-modules model Args module nn Module target model sub-module apply activation checkpointing Example xdoctest +SKIP torch nn nn MyModel nn Module __init__ - None super __init__ l = nn Linear l = nn Linear forward x l l x model = MyModel checkpoint model l apply activation checkpointing only l model torch zeros sum backward torch _C _log_api_usage_once torch distributed checkpoint use_reentrant = kwargs pop use_reentrant False use_reentrant raise NotImplementedError use_reentrant=True supported composable checkpoint Please use torch utils checkpoint checkpoint instead preserve_rng_state = kwargs pop preserve_rng_state True user_context_fns = kwargs pop context_fn None determinism_check = kwargs pop determinism_check _DEFAULT_DETERMINISM_MODE debug = kwargs pop debug False early_stop = kwargs pop early_stop True kwargs raise ValueError Unexpected keyword arguments + join arg arg kwargs forward_pre_hook module nn Module args tuple Any kwargs dict str Any - None checkpoint state module enable_hook context_fns user_context_fns None ctx ctx = user_context_fns ctx _no_hook module ctx nullcontext _no_hook module gen = _checkpoint_without_reentrant_generator module preserve_rng_state context_fns determinism_check debug early_stop args kwargs checkpoint state module _ac_generator = gen next gen forward_hook module nn Module inputs tuple Any output Any - Any checkpoint state module enable_hook try gen = checkpoint state module _ac_generator assert gen None next gen except StopIteration pass raise RuntimeError Expected non-reentrant activation checkpoint generator exhausted Ensure we no longer hold generator always_call=True helps ensure we clear even case exception fwd pass checkpoint state module _ac_generator = None checkpoint state module enable_hook = True module register_forward_pre_hook forward_pre_hook with_kwargs=True module register_forward_hook forward_hook prepend=True always_call=True module