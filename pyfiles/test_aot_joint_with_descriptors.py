Owner s oncall pt Copyright c Facebook Inc its affiliates All rights reserved This source code licensed under BSD-style license found LICENSE file root directory source tree contextlib ExitStack torch torch fx traceback fx_traceback torch nn nn torch utils _pytree pytree torch _decomp decomposition_table torch _dynamo functional_export _dynamo_graph_capture_for_export torch _dynamo testing normalize_gm torch _functorch _aot_autograd descriptors BufferAOTInput ParamAOTInput PlainAOTInput PlainAOTOutput torch _functorch _aot_autograd fx_utils get_all_input_and_grad_nodes get_all_output_and_tangent_nodes get_buffer_nodes get_named_buffer_nodes get_named_param_nodes get_param_and_grad_nodes get_param_nodes get_plain_input_and_grad_nodes get_plain_output_and_tangent_nodes torch _functorch aot_autograd aot_compile_joint_with_descriptors aot_export_joint_with_descriptors torch _guards tracing TracingContext torch nn attention flex_attention create_block_mask flex_attention torch testing _internal common_utils requires_cuda run_tests skipIfCrossRef TestCase graph_capture model inputs with_export gm = model fake_mode = None with_export torch _dynamo config patch install_free_tensors=True fx_traceback preserve_node_meta TODO switch use official graph_capture API once ready gm = _dynamo_graph_capture_for_export model inputs fake_mode = gm meta get fake_mode None tracing TracingContext fake_mode ExitStack stack joint_with_descriptors = aot_export_joint_with_descriptors stack gm inputs joint_with_descriptors graph_module TestAOTJointWithDescriptors TestCase test_simple_linear_module Test basic linear module aot_export_joint_with_descriptors SimpleLinear nn Module __init__ super __init__ linear = nn Linear forward x linear x model = SimpleLinear inputs = torch randn ExitStack stack Export joint descriptors joint_with_descriptors = aot_export_joint_with_descriptors stack model inputs decompositions=decomposition_table Test exported graph structure graph_code = joint_with_descriptors graph_module print_readable print_output=False expanded_def=True assertExpectedInline normalize_gm graph_code \ inner_f torch nn Module forward primals tangents primals_ f ParamAOTInput target= linear weight primals_ f ParamAOTInput target= linear bias primals_ f PlainAOTInput idx= tangents_ f TangentAOTInput output=PlainAOTOutput idx= primals_ primals_ primals_ tangents_ = fx_pytree tree_flatten_spec primals tangents _in_spec transpose f = torch ops prims transpose default primals_ primals_ = None mm f = torch ops aten mm default primals_ transpose transpose = None mul f = torch ops prims mul default mm mm = None mul_ f = torch ops prims mul default primals_ primals_ = None broadcast_in_dim f = torch ops prims broadcast_in_dim default mul_ mul_ = None add f = torch ops prims add default mul broadcast_in_dim mul = broadcast_in_dim = None transpose_ f = torch ops prims transpose default tangents_ mm_ f = torch ops aten mm default transpose_ primals_ transpose_ = primals_ = None transpose_ f = torch ops prims transpose default mm_ mm_ = None sum_ f = torch ops prims sum default tangents_ tangents_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default sum_ sum_ = None as_strided f = torch ops aten as_strided default broadcast_in_dim_ broadcast_in_dim_ = None transpose_ f = torch ops prims transpose default transpose_ transpose_ = None pytree tree_unflatten add PlainAOTOutput idx= transpose_ GradAOTOutput grad_of=ParamAOTInput target= linear weight as_strided GradAOTOutput grad_of=ParamAOTInput target= linear bias None None _out_spec Compile result parallel_model_fn = aot_compile_joint_with_descriptors joint_with_descriptors Test functional correctness expected_output = model inputs actual_output = parallel_model_fn dict model named_parameters values inputs assertEqual expected_output actual_output test_conv_bn_module Test convolutional + batch norm module ConvBN nn Module __init__ super __init__ conv = nn Conv d padding= bn = nn BatchNorm d forward x x = conv x x = bn x torch relu x model = ConvBN model train Important batch norm inputs = torch randn ExitStack stack Export joint descriptors joint_with_descriptors = aot_export_joint_with_descriptors stack model inputs decompositions=decomposition_table Test exported graph structure graph_code = joint_with_descriptors graph_module print_readable print_output=False expanded_def=True Test parameter buffer specs assertIn conv weight joint_with_descriptors params_spec assertIn conv bias joint_with_descriptors params_spec assertIn bn weight joint_with_descriptors params_spec assertIn bn bias joint_with_descriptors params_spec assertIn bn running_mean joint_with_descriptors buffers_spec assertIn bn running_var joint_with_descriptors buffers_spec assertIn bn num_batches_tracked joint_with_descriptors buffers_spec Expect test printed graph assertExpectedInline normalize_gm graph_code \ inner_f torch nn Module forward primals tangents primals_ f ParamAOTInput target= conv weight primals_ f ParamAOTInput target= conv bias primals_ f ParamAOTInput target= bn weight primals_ f ParamAOTInput target= bn bias primals_ f BufferAOTInput target= bn running_mean primals_ f BufferAOTInput target= bn running_var primals_ i BufferAOTInput target= bn num_batches_tracked primals_ f PlainAOTInput idx= tangents_ f TangentAOTInput output=PlainAOTOutput idx= primals_ primals_ primals_ primals_ primals_ primals_ primals_ primals_ tangents_ = fx_pytree tree_flatten_spec primals tangents _in_spec convolution f = torch ops aten convolution default primals_ primals_ primals_ False primals_ = None add i = torch ops prims add default primals_ primals_ = None var f = torch ops prims var default convolution broadcast_in_dim f = torch ops prims broadcast_in_dim default var var = None sum_ f = torch ops prims sum default convolution broadcast_in_dim_ f = torch ops prims broadcast_in_dim default sum_ sum_ = None div f = torch ops prims div default broadcast_in_dim_ broadcast_in_dim_ = None add_ f = torch ops prims add default broadcast_in_dim e- rsqrt f = torch ops prims rsqrt default add_ add_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default div sub f = torch ops prims sub default convolution broadcast_in_dim_ broadcast_in_dim_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default rsqrt mul f = torch ops prims mul default sub broadcast_in_dim_ sub = broadcast_in_dim_ = None squeeze f = torch ops prims squeeze default div div = None squeeze_ f = torch ops prims squeeze default squeeze squeeze = None squeeze_ f = torch ops prims squeeze default squeeze_ squeeze_ = None squeeze_ f = torch ops prims squeeze default rsqrt rsqrt = None squeeze_ f = torch ops prims squeeze default squeeze_ squeeze_ = None squeeze_ f = torch ops prims squeeze default squeeze_ squeeze_ = None mul_ f = torch ops prims mul default squeeze_ mul_ f = torch ops prims mul default primals_ primals_ = None add_ f = torch ops prims add default mul_ mul_ mul_ = mul_ = None squeeze_ f = torch ops prims squeeze default broadcast_in_dim broadcast_in_dim = None squeeze_ f = torch ops prims squeeze default squeeze_ squeeze_ = None squeeze_ f = torch ops prims squeeze default squeeze_ squeeze_ = None mul_ f = torch ops prims mul default squeeze_ squeeze_ = None mul_ f = torch ops prims mul default mul_ mul_ = None mul_ f = torch ops prims mul default primals_ primals_ = None add_ f = torch ops prims add default mul_ mul_ mul_ = mul_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default primals_ broadcast_in_dim_ f = torch ops prims broadcast_in_dim default broadcast_in_dim_ broadcast_in_dim_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default broadcast_in_dim_ broadcast_in_dim_ = None mul_ f = torch ops prims mul default mul broadcast_in_dim_ mul = broadcast_in_dim_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default primals_ primals_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default broadcast_in_dim_ broadcast_in_dim_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default broadcast_in_dim_ broadcast_in_dim_ = None add_ f = torch ops prims add default mul_ broadcast_in_dim_ mul_ = broadcast_in_dim_ = None le b = torch ops prims le default add_ where f = torch ops prims where default le add_ le = add_ = None view_of f = torch ops prims view_of default where view_of_ f = torch ops prims view_of default view_of view_of = None le_ b = torch ops prims le default view_of_ view_of_ = None where_ f = torch ops prims where default le_ tangents_ le_ = tangents_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default squeeze_ squeeze_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default broadcast_in_dim_ broadcast_in_dim_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default broadcast_in_dim_ broadcast_in_dim_ = None sum_ f = torch ops prims sum default where_ broadcast_in_dim_ f = torch ops prims broadcast_in_dim default broadcast_in_dim_ sub_ f = torch ops prims sub default convolution broadcast_in_dim_ broadcast_in_dim_ = None mul_ f = torch ops prims mul default where_ sub_ sub_ = None sum_ f = torch ops prims sum default mul_ mul_ = None mul_ f = torch ops prims mul default sum_ broadcast_in_dim_ f = torch ops prims broadcast_in_dim default mul_ mul_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default broadcast_in_dim_ broadcast_in_dim_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default broadcast_in_dim_ broadcast_in_dim_ = None mul_ f = torch ops prims mul default sum_ mul_ f = torch ops prims mul default squeeze_ squeeze_ mul_ f = torch ops prims mul default mul_ mul_ mul_ = mul_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default mul_ mul_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default broadcast_in_dim_ broadcast_in_dim_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default broadcast_in_dim_ broadcast_in_dim_ = None mul_ f = torch ops prims mul default squeeze_ primals_ primals_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default mul_ mul_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default broadcast_in_dim_ broadcast_in_dim_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default broadcast_in_dim_ broadcast_in_dim_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default broadcast_in_dim_ broadcast_in_dim_ = None sub_ f = torch ops prims sub default convolution broadcast_in_dim_ convolution = broadcast_in_dim_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default broadcast_in_dim_ broadcast_in_dim_ = None mul_ f = torch ops prims mul default sub_ broadcast_in_dim_ sub_ = broadcast_in_dim_ = None sub_ f = torch ops prims sub default where_ mul_ where_ = mul_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default broadcast_in_dim_ broadcast_in_dim_ = None sub_ f = torch ops prims sub default sub_ broadcast_in_dim_ sub_ = broadcast_in_dim_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default broadcast_in_dim_ broadcast_in_dim_ = None mul_ f = torch ops prims mul default sub_ broadcast_in_dim_ sub_ = broadcast_in_dim_ = None mul_ f = torch ops prims mul default sum_ squeeze_ sum_ = squeeze_ = None convolution_backward = torch ops aten convolution_backward default mul_ primals_ primals_ False False True True mul_ = primals_ = primals_ = None getitem_ f = convolution_backward getitem_ f = convolution_backward convolution_backward = None pytree tree_unflatten add_ InputMutationAOTOutput mutated_input=BufferAOTInput target= bn running_mean add_ InputMutationAOTOutput mutated_input=BufferAOTInput target= bn running_var add InputMutationAOTOutput mutated_input=BufferAOTInput target= bn num_batches_tracked where PlainAOTOutput idx= getitem_ GradAOTOutput grad_of=ParamAOTInput target= conv weight getitem_ GradAOTOutput grad_of=ParamAOTInput target= conv bias mul_ GradAOTOutput grad_of=ParamAOTInput target= bn weight sum_ GradAOTOutput grad_of=ParamAOTInput target= bn bias None None None None None None None None _out_spec noqa B Compile result parallel_model_fn = aot_compile_joint_with_descriptors joint_with_descriptors Test functional correctness expected_output = model inputs all_params_buffers = dict model named_parameters values dict model named_buffers values actual_output = parallel_model_fn all_params_buffers inputs torch testing assert_close expected_output actual_output rtol= e- atol= e- test_module_with_kwargs Test module keyword arguments ModuleWithKwargs nn Module __init__ super __init__ linear = nn Linear forward x scale linear x scale model = ModuleWithKwargs inputs = torch randn kwargs = scale torch tensor gm = _dynamo_graph_capture_for_export model inputs kwargs ExitStack stack Export joint descriptors joint_with_descriptors = aot_export_joint_with_descriptors stack gm inputs kwargs decompositions=decomposition_table Test exported graph structure graph_code = joint_with_descriptors graph_module print_readable print_output=False expanded_def=True For some reason PYTORCH_TEST_WITH_CROSSREF will add extra spaces I tried fix normalize_gm there too many files depending behavior graph_code_str = normalize_gm graph_code graph_code_str = \n join line line graph_code_str split \n len line rstrip Expect test printed graph assertExpectedInline graph_code_str \ inner_f torch nn Module forward primals tangents primals_ f ParamAOTInput target= L__self___linear_weight primals_ f ParamAOTInput target= L__self___linear_bias primals_ f PlainAOTInput idx= primals_ f PlainAOTInput idx= tangents_ f TangentAOTInput output=PlainAOTOutput idx= primals_ primals_ primals_ primals_ tangents_ = fx_pytree tree_flatten_spec primals tangents _in_spec transpose f = torch ops prims transpose default primals_ primals_ = None mm f = torch ops aten mm default primals_ transpose transpose = None mul f = torch ops prims mul default mm mm = None mul_ f = torch ops prims mul default primals_ primals_ = None broadcast_in_dim f = torch ops prims broadcast_in_dim default mul_ mul_ = None add f = torch ops prims add default mul broadcast_in_dim mul = broadcast_in_dim = None mul_ f = torch ops prims mul default add primals_ add = None mul_ f = torch ops prims mul default tangents_ primals_ tangents_ = primals_ = None transpose_ f = torch ops prims transpose default mul_ mm_ f = torch ops aten mm default transpose_ primals_ transpose_ = primals_ = None transpose_ f = torch ops prims transpose default mm_ mm_ = None sum_ f = torch ops prims sum default mul_ mul_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default sum_ sum_ = None as_strided f = torch ops aten as_strided default broadcast_in_dim_ broadcast_in_dim_ = None transpose_ f = torch ops prims transpose default transpose_ transpose_ = None pytree tree_unflatten mul_ PlainAOTOutput idx= transpose_ GradAOTOutput grad_of=ParamAOTInput target= L__self___linear_weight as_strided GradAOTOutput grad_of=ParamAOTInput target= L__self___linear_bias None None None None _out_spec Compile result parallel_model_fn = aot_compile_joint_with_descriptors joint_with_descriptors Test functional correctness expected_output = model inputs kwargs actual_output = parallel_model_fn dict model named_parameters values inputs kwargs assertEqual expected_output actual_output test_multiple_outputs_module Test module multiple outputs MultiOutputModule nn Module __init__ super __init__ linear = nn Linear linear = nn Linear forward x out = linear x out = linear x out out model = MultiOutputModule inputs = torch randn ExitStack stack Export joint descriptors joint_with_descriptors = aot_export_joint_with_descriptors stack model inputs decompositions=decomposition_table Test exported graph structure graph_code = joint_with_descriptors graph_module print_readable print_output=False expanded_def=True Expect test printed graph assertExpectedInline normalize_gm graph_code \ inner_f torch nn Module forward primals tangents primals_ f ParamAOTInput target= linear weight primals_ f ParamAOTInput target= linear bias primals_ f ParamAOTInput target= linear weight primals_ f ParamAOTInput target= linear bias primals_ f PlainAOTInput idx= tangents_ f TangentAOTInput output=PlainAOTOutput idx= tangents_ f TangentAOTInput output=PlainAOTOutput idx= primals_ primals_ primals_ primals_ primals_ tangents_ tangents_ = fx_pytree tree_flatten_spec primals tangents _in_spec transpose f = torch ops prims transpose default primals_ primals_ = None mm f = torch ops aten mm default primals_ transpose transpose = None mul f = torch ops prims mul default mm mm = None mul_ f = torch ops prims mul default primals_ primals_ = None broadcast_in_dim f = torch ops prims broadcast_in_dim default mul_ mul_ = None add f = torch ops prims add default mul broadcast_in_dim mul = broadcast_in_dim = None transpose_ f = torch ops prims transpose default primals_ primals_ = None mm_ f = torch ops aten mm default primals_ transpose_ transpose_ = None mul_ f = torch ops prims mul default mm_ mm_ = None mul_ f = torch ops prims mul default primals_ primals_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default mul_ mul_ = None add_ f = torch ops prims add default mul_ broadcast_in_dim_ mul_ = broadcast_in_dim_ = None transpose_ f = torch ops prims transpose default tangents_ mm_ f = torch ops aten mm default transpose_ primals_ transpose_ = None transpose_ f = torch ops prims transpose default mm_ mm_ = None sum_ f = torch ops prims sum default tangents_ tangents_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default sum_ sum_ = None as_strided f = torch ops aten as_strided default broadcast_in_dim_ broadcast_in_dim_ = None transpose_ f = torch ops prims transpose default transpose_ transpose_ = None transpose_ f = torch ops prims transpose default tangents_ mm_ f = torch ops aten mm default transpose_ primals_ transpose_ = primals_ = None transpose_ f = torch ops prims transpose default mm_ mm_ = None sum_ f = torch ops prims sum default tangents_ tangents_ = None broadcast_in_dim_ f = torch ops prims broadcast_in_dim default sum_ sum_ = None as_strided_ f = torch ops aten as_strided default broadcast_in_dim_ broadcast_in_dim_ = None transpose_ f = torch ops prims transpose default transpose_ transpose_ = None pytree tree_unflatten add PlainAOTOutput idx= add_ PlainAOTOutput idx= transpose_ GradAOTOutput grad_of=ParamAOTInput target= linear weight as_strided_ GradAOTOutput grad_of=ParamAOTInput target= linear bias transpose_ GradAOTOutput grad_of=ParamAOTInput target= linear weight as_strided GradAOTOutput grad_of=ParamAOTInput target= linear bias None None _out_spec noqa B Compile result parallel_model_fn = aot_compile_joint_with_descriptors joint_with_descriptors Test functional correctness expected_output = model inputs actual_output = parallel_model_fn dict model named_parameters values inputs Check both outputs assertEqual len expected_output len actual_output exp act zip expected_output actual_output assertEqual exp act test_in_out_specs Test in_spec out_spec properly set SimpleModule nn Module __init__ super __init__ linear = nn Linear forward x linear x model = SimpleModule inputs = torch randn ExitStack stack Export joint descriptors joint_with_descriptors = aot_export_joint_with_descriptors stack model inputs decompositions=decomposition_table Test specs available assertIsNotNone joint_with_descriptors in_spec assertIsNotNone joint_with_descriptors out_spec assertIsNotNone joint_with_descriptors params_spec assertIsNotNone joint_with_descriptors buffers_spec Test they work pytree operations flat_inputs _ = pytree tree_flatten inputs assertTrue len flat_inputs Test parameter buffer specs contain expected entries assertIn linear weight joint_with_descriptors params_spec assertIn linear bias joint_with_descriptors params_spec assertEqual len joint_with_descriptors buffers_spec No buffers simple linear Compile result ensure everything works together parallel_model_fn = aot_compile_joint_with_descriptors joint_with_descriptors Test functional correctness expected_output = model inputs actual_output = parallel_model_fn dict model named_parameters values inputs assertEqual expected_output actual_output test_fx_utils_simple_linear Test FX utilities simple linear module SimpleLinear nn Module __init__ super __init__ linear = nn Linear forward x linear x model = SimpleLinear inputs = torch randn ExitStack stack joint_with_descriptors = aot_export_joint_with_descriptors stack model inputs decompositions=decomposition_table graph = joint_with_descriptors graph_module graph Test get_named_param_nodes named_params = get_named_param_nodes graph assertIn linear weight named_params assertIn linear bias named_params assertEqual len named_params Test get_param_nodes param_nodes = get_param_nodes graph assertEqual len param_nodes Test get_named_buffer_nodes should empty simple linear named_buffers = get_named_buffer_nodes graph assertEqual len named_buffers Test get_buffer_nodes buffer_nodes = get_buffer_nodes graph assertEqual len buffer_nodes Test get_all_input_and_grad_nodes input_grad_nodes = get_all_input_and_grad_nodes graph assertEqual len input_grad_nodes params + input + tangent Verify parameters have gradients param_grads = get_param_and_grad_nodes graph assertEqual len param_grads desc param_node grad_node param_grads items assertIsInstance desc ParamAOTInput assertIsNotNone param_node assertIsNotNone grad_node Should have gradients Test get_plain_input_and_grad_nodes plain_input_grads = get_plain_input_and_grad_nodes graph assertEqual len plain_input_grads plain input desc input_node grad_node plain_input_grads items assertIsInstance desc PlainAOTInput assertIsNotNone input_node assertIsNone grad_node Plain inputs don t have gradients Test get_all_output_and_tangent_nodes output_tangent_nodes = get_all_output_and_tangent_nodes graph assertEqual len output_tangent_nodes output + grad outputs Test get_plain_output_and_tangent_nodes plain_output_tangents = get_plain_output_and_tangent_nodes graph assertEqual len plain_output_tangents desc output_node tangent_node plain_output_tangents items assertIsInstance desc PlainAOTOutput assertIsNotNone output_node assertIsNotNone tangent_node Should have tangents backward pass test_fx_utils_conv_bn_module Test FX utilities conv+batchnorm module buffers ConvBN nn Module __init__ super __init__ conv = nn Conv d padding= bn = nn BatchNorm d forward x x = conv x x = bn x torch relu x model = ConvBN model train Important batch norm inputs = torch randn ExitStack stack joint_with_descriptors = aot_export_joint_with_descriptors stack model inputs decompositions=decomposition_table graph = joint_with_descriptors graph_module graph Test get_named_param_nodes named_params = get_named_param_nodes graph expected_params = conv weight conv bias bn weight bn bias param_name expected_params assertIn param_name named_params assertEqual len named_params Test get_named_buffer_nodes named_buffers = get_named_buffer_nodes graph expected_buffers = bn running_mean bn running_var bn num_batches_tracked buffer_name expected_buffers assertIn buffer_name named_buffers assertEqual len named_buffers Test get_buffer_nodes buffer_nodes = get_buffer_nodes graph assertEqual len buffer_nodes Test all inputs include params buffers plain inputs input_grad_nodes = get_all_input_and_grad_nodes graph assertEqual len input_grad_nodes params + buffers + input + tangent Verify buffer handling buffer_count = desc node _grad_node input_grad_nodes items isinstance desc BufferAOTInput buffer_count += assertIsNotNone node Buffers typically don t have gradients unless they re trainable assertEqual buffer_count test_fx_utils_multiple_outputs Test FX utilities module multiple outputs MultiOutputModule nn Module __init__ super __init__ linear = nn Linear linear = nn Linear forward x out = linear x out = linear x out out model = MultiOutputModule inputs = torch randn ExitStack stack joint_with_descriptors = aot_export_joint_with_descriptors stack model inputs decompositions=decomposition_table graph = joint_with_descriptors graph_module graph Test get_all_output_and_tangent_nodes output_tangent_nodes = get_all_output_and_tangent_nodes graph assertEqual len output_tangent_nodes outputs + grad outputs Test get_plain_output_and_tangent_nodes plain_output_tangents = get_plain_output_and_tangent_nodes graph assertEqual len plain_output_tangents Verify each output has tangent desc output_node tangent_node plain_output_tangents items assertIsInstance desc PlainAOTOutput assertIsNotNone output_node assertIsNotNone tangent_node Test parameter handling multiple outputs param_grads = get_param_and_grad_nodes graph assertEqual len param_grads weights + biases All parameters should have gradients desc param_node grad_node param_grads items assertIsInstance desc ParamAOTInput assertIsNotNone param_node assertIsNotNone grad_node test_fx_utils_node_consistency Test FX utilities consistent node references SimpleModule nn Module __init__ super __init__ linear = nn Linear forward x linear x model = SimpleModule inputs = torch randn ExitStack stack joint_with_descriptors = aot_export_joint_with_descriptors stack model inputs decompositions=decomposition_table graph = joint_with_descriptors graph_module graph Get nodes through different APIs verify consistency named_params = get_named_param_nodes graph param_nodes = get_param_nodes graph param_grads = get_param_and_grad_nodes graph all_input_grads = get_all_input_and_grad_nodes graph Check get_param_nodes returns same nodes get_named_param_nodes assertEqual len param_nodes len named_params node param_nodes assertIn node named_params values Check param_grads contains same parameter nodes desc param_node _grad_node param_grads items assertIn param_node param_nodes assertEqual param_node named_params desc target Check all_input_grads contains parameter nodes param_count = desc input_node _grad_node all_input_grads items isinstance desc ParamAOTInput param_count += assertIn input_node param_nodes assertEqual input_node named_params desc target assertEqual param_count len param_nodes test_export_and_compile SimpleModule nn Module __init__ super __init__ linear = nn Linear forward x linear x model = SimpleModule inputs = torch randn ExitStack stack joint_with_descriptors = aot_export_joint_with_descriptors stack model inputs model_fn = aot_compile_joint_with_descriptors joint_with_descriptors compiled_fn = torch compile fullgraph=True model_fn compiled_fn dict model named_parameters values inputs sum backward assertIsNotNone model linear weight grad test_preserve_annotate_simple Test basic linear module aot_export_joint_with_descriptors SimpleLinear nn Module __init__ super __init__ linear = nn Linear forward x fx_traceback annotate pp_stage y = linear x y - inputs = torch randn model = SimpleLinear with_export True False graph_module = graph_capture model inputs with_export custom_metadata = fx_traceback _get_custom_metadata graph_module assertExpectedInline str custom_metadata \ call_function t pp_stage call_function addmm pp_stage call_function t_ pp_stage call_function mm pp_stage call_function t_ pp_stage call_function sum_ pp_stage call_function view pp_stage call_function t_ pp_stage requires_cuda test_preserve_annotate_flex_attention score_mod score b h m n score _get_block_causal_mask_mod seq_idx block_causal_mask b h q_idx kv_idx must use more complicated mask_mod so autograd seq_nr increases seq_idx b q_idx == seq_idx b kv_idx q_idx = kv_idx block_causal_mask = b = batch_size = seqlen = b device = cuda Create seq_idx tensor - maps each position document sequence ID Example Split sequence into documents each batch First half belongs document second half document seq_idx = torch zeros batch_size seqlen dtype=torch int device=device seq_idx seqlen = Second half belongs document Get mask_mod function seq_idx captured closure mask_mod = _get_block_causal_mask_mod seq_idx Create block_mask mask_mod function which only takes args Note We don t compile create_block_mask itself just flex_attention block_mask = create_block_mask mask_mod None None seqlen seqlen FlexAttentionModule torch nn Module Flex attention submodule similar sdpa Llama Attention forward xq xk xv Args xq Query tensor bs n_heads seqlen head_dim xk Key tensor bs n_heads seqlen head_dim xv Value tensor bs n_heads seqlen head_dim Returns Output tensor bs n_heads seqlen head_dim fx_traceback annotate compile_with_inductor flex_attention output = flex_attention xq xk xv block_mask=block_mask score_mod=score_mod output Model configuration n_heads = head_dim = Create input tensors shape expected FlexAttentionModule Shape bs n_heads seqlen head_dim xq = torch randn batch_size n_heads seqlen head_dim requires_grad=True device=device xk = torch randn batch_size n_heads seqlen head_dim requires_grad=True device=device xv = torch randn batch_size n_heads seqlen head_dim requires_grad=True device=device model = FlexAttentionModule device inputs = xq xk xv gm = graph_capture model inputs with_export=True custom_metadata = fx_traceback _get_custom_metadata gm using assertExpectedInline because some CI runs has fewer detach nodes graph than other CI runs so we can t use fixed string compare against assertTrue get_attr sdpa_score compile_with_inductor flex_attention custom_metadata assertTrue get_attr sdpa_mask compile_with_inductor flex_attention custom_metadata assertTrue call_function flex_attention compile_with_inductor flex_attention custom_metadata assertTrue get_attr fw_graph compile_with_inductor flex_attention custom_metadata assertTrue get_attr joint_graph compile_with_inductor flex_attention custom_metadata assertTrue get_attr mask_graph compile_with_inductor flex_attention custom_metadata assertTrue call_function flex_attention_backward compile_with_inductor flex_attention custom_metadata test_preserve_annotate_function Test basic annotate_fn usage fx_traceback annotate_fn pp_stage example_function x x x SimpleLinear nn Module __init__ super __init__ linear = nn Linear forward x fx_traceback annotate pp_stage y = linear x y = example_function y y - inputs = torch randn model = SimpleLinear with_export True False graph_module = graph_capture model inputs with_export custom_metadata = fx_traceback _get_custom_metadata graph_module assertExpectedInline str custom_metadata \ call_function t pp_stage call_function addmm pp_stage call_function mul pp_stage call_function mul_ pp_stage call_function mul_ pp_stage call_function t_ pp_stage call_function mm pp_stage call_function t_ pp_stage call_function sum_ pp_stage call_function view pp_stage call_function t_ pp_stage skipIfCrossRef test_custom_op_stack_trace torch library custom_op my_lib foo mutates_args= foo x torch Tensor y torch Tensor - torch Tensor x + y foo register_fake foo_fake_impl x y torch empty_like x foo_setup_context ctx inputs output pass foo_backward ctx grad_output grad_output grad_output foo register_autograd foo_backward setup_context=foo_setup_context CustomOpModule torch nn Module forward x y foo x y model = CustomOpModule inputs = torch randn torch randn gm = graph_capture model inputs with_export=True foo_node = None node gm graph nodes node op == call_function node name == foo foo_node = node break assertTrue foo_node None assertTrue foo x y foo_node meta get stack_trace None assertTrue foo x y gm print_readable print_output=False assertFalse _opoverload foo_node meta get stack_trace None assertFalse _opoverload gm print_readable print_output=False test_preserve_annotate_replay_view Test stack trace annotation correct nodes regenerated functionalization _unpermute out input_shape permuted_indices Unpermute operation torchtitan MoE utils out_unpermuted = out new_empty input_shape out_unpermuted permuted_indices = out out = out_unpermuted - out Module nn Module __init__ super __init__ input_shape = permuted_indices = torch tensor forward x fx_traceback annotate pp_stage routed_output = _unpermute x input_shape permuted_indices routed_output cos inputs = torch randn requires_grad=True model = Module graph_module = graph_capture model inputs True custom_metadata = fx_traceback _get_custom_metadata graph_module slice_nodes = graph_module graph find_nodes op= call_function target=torch ops aten slice Tensor assertEqual len slice_nodes slice_backward_nodes = graph_module graph find_nodes op= call_function target=torch ops aten slice_backward default assertEqual len slice_backward_nodes slice_node = slice_nodes slice_backward_node = slice_backward_nodes assertEqual slice_node meta seq_nr slice_backward_node meta seq_nr assertTrue out = out_unpermuted - slice_node meta stack_trace assertExpectedInline str custom_metadata \ call_function new_empty pp_stage call_function index_put pp_stage call_function slice_ pp_stage call_function slice_backward pp_stage call_function index pp_stage test_static_input_indices Test basic linear module aot_export_joint_with_descriptors SimpleLinear nn Module __init__ super __init__ linear = nn Linear forward x linear x model = SimpleLinear inputs = torch randn gm = _dynamo_graph_capture_for_export model inputs fake_mode = gm meta get fake_mode None tracing TracingContext fake_mode ExitStack stack joint = aot_export_joint_with_descriptors stack gm inputs assertEqual joint _aot_state fw_metadata static_input_indices __name__ == __main__ run_tests