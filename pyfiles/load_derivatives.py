Parses derivatives yaml into autograd functions Each autograd function represented ` DifferentiabilityInfo ` containing list ` Derivative ` See ` torchgen api autograd ` data models __future__ annotations re collections Counter defaultdict typing Any TYPE_CHECKING yaml torchgen api cpp torchgen api autograd Derivative DifferentiabilityInfo ForwardDerivative SavedAttribute torchgen api types BaseCType Binding boolT CppSignatureGroup layoutT longT NamedCType OptionalCType scalarTypeT SpecialArgName stringT symIntArrayRefT SymIntT tensorGeometryT tensorOptionsT typeAndSizeT VectorCType torchgen context with_native_function torchgen gen get_grouped_by_view_native_functions parse_native_yaml torchgen model AUTOGRAD_KEYS FunctionSchema NativeFunction NativeFunctionsViewGroup OperatorName SchemaKind Type Variant torchgen utils concatMap IDENT_REGEX split_name_params torchgen yaml_utils YamlLoader TYPE_CHECKING collections abc Sequence DerivativeRet = tuple dict FunctionSchema dict str DifferentiabilityInfo set str _GLOBAL_LOAD_DERIVATIVE_CACHE dict tuple str str DerivativeRet = _VALID_AUTOGRAD_KEYS = set AUTOGRAD_KEYS This function directly adds per-dispatchkey derivative entries view _copy variants each view op Since every view view _copy op shares same derivative formula we generate them here instead duplicating them yaml See Note Codegen d view _copy Operators add_view_copy_derivatives infos dict FunctionSchema dict str DifferentiabilityInfo view_groups list NativeFunctionsViewGroup - None Get map each view op s name its corresponding view group view_name_to_group dict OperatorName NativeFunctionsViewGroup = g view func name g g view_groups view_infos = info_dispatch_dict infos values maybe_view_group only needs calculated once per info_dispatch_dict maybe_view_group = None view_copy_differentiability_infos = dispatch_key info info_dispatch_dict items maybe_view_group = view_name_to_group get info func func name None maybe_view_group None maybe_view_group view_copy None view_copy_info = info create_view_copy_from_view_derivative maybe_view_group view_copy_info None fn_schema = view_copy_info func func view_copy_differentiability_infos dispatch_key = view_copy_info break prefer manually-defined derivatives any pyrefly ignore unbound-name len view_copy_differentiability_infos fn_schema infos pyrefly ignore unbound-name assert fn_schema None pyrefly ignore unbound-name view_infos fn_schema = view_copy_differentiability_infos infos update view_infos load_derivatives derivatives_yaml_path str native_yaml_path str tags_yaml_path str - DerivativeRet Do some caching deterministic function global _GLOBAL_LOAD_DERIVATIVE_CACHE key = derivatives_yaml_path native_yaml_path key _GLOBAL_LOAD_DERIVATIVE_CACHE open derivatives_yaml_path f definitions = yaml load f Loader=YamlLoader funcs = parse_native_yaml native_yaml_path tags_yaml_path native_functions From parsed native functions separate out generated view_copy functions so we can generate derivatives them separately native_functions_with_view_groups = get_grouped_by_view_native_functions funcs native_functions = concatMap lambda g g isinstance g NativeFunction list g functions include_copy=True native_functions_with_view_groups view_groups = g g native_functions_with_view_groups isinstance g NativeFunctionsViewGroup What s difference between function schema v s signature function schema complete declaration including mutability annotation default value etc signature canonical schema group functions in-place out functional variants semantically related functions_by_signature dict FunctionSchema list NativeFunction = defaultdict list functions_by_schema dict str NativeFunction = function native_functions functions_by_signature function func signature append function assert str function func functions_by_schema functions_by_schema str function func = function Keep track how many which ops we ve seen so we can disambiguate them numeric suffix op_counter = Counter str infos dict maps FunctionSchema - dict per dispatch key DifferentiabilityInfos useful because tools autograd gen_autograd py match_differentiability_info we ultimately need categorize DifferentiabilityInfos FunctionSchema infos dict FunctionSchema dict str DifferentiabilityInfo = used_dispatch_keys set str = set defn_dict definitions Ensure old derivatives yaml schema no dispatch key can loaded dispatch defn_dict specification = defn_dict pop name output_differentiability = defn_dict pop output_differentiability None defn_dict = name specification dispatch Default defn_dict output_differentiability defn_dict output_differentiability = output_differentiability name per_dispatch_diffinfos = create_differentiability_info defn_dict functions_by_signature functions_by_schema op_counter used_dispatch_keys infos name = per_dispatch_diffinfos add_view_copy_derivatives infos view_groups cache both loaded infos well set all dispatch_keys aliases appear derivatives yaml used_dispatch_keys useful generating VariableType cpp where we need TORCH_LIBRARY_IMPL every autograd dispatch key used _GLOBAL_LOAD_DERIVATIVE_CACHE key = infos used_dispatch_keys _GLOBAL_LOAD_DERIVATIVE_CACHE key TODO Why going through CppSignatureGroup doesn t make sense with_native_function cpp_arguments f NativeFunction - Sequence Binding sigs = CppSignatureGroup from_native_function f method=False sigs symint_signature None sigs symint_signature arguments sigs signature arguments create_derivative f NativeFunction formula str var_names tuple str available_named_gradients Sequence str - Derivative original_formula = formula arguments list NamedCType = nctype remove_const_ref cpp_arguments f return_names = tuple n n = result n cpp return_names f return_types = tuple cpp return_type r symint=True remove_const_ref r f func returns named_returns = NamedCType name type name type zip return_names return_types formula saved_inputs = saved_variables formula arguments var_names formula saved_outputs = saved_variables formula named_returns var_names used_named_gradients = name name available_named_gradients re search IDENT_REGEX format name formula Check referenced derivatives formula bounds i used_gradient_indices formula i = len f func returns raise RuntimeError f Out bounds grads access derivative formula cpp name f func f used grads i forward only returns len f func returns outputs Derivative formula=formula original_formula=original_formula var_names=var_names saved_inputs=saved_inputs saved_outputs=saved_outputs named_gradients=used_named_gradients create_forward_derivative f NativeFunction formula str names tuple str - ForwardDerivative var_names = names var_types tuple Type &#124; None = None r f func returns r name var_names var_types None var_types = var_types = var_types + r type Handle default names var_types None var_names == result assert len f func returns == var_types = f func returns type var_name var_names res = re findall r ^result \d+ $ var_name len res == var_types None var_types = arg_idx = int res var_types = var_types + f func returns arg_idx type assert var_types None No matching output forward derivative definition ForwardDerivative formula=formula var_names=var_names var_types=var_types required_inputs_fw_grad=None required_inputs_primal=None required_original_self_value=False is_reusing_outplace_formula=False postprocess_forward_derivatives f NativeFunction defn_name str all_arg_names list str derivatives list Derivative forward_derivatives list ForwardDerivative args_with_derivatives Sequence Binding - list ForwardDerivative find_required_inputs formula str postfix str - tuple str is_foreach = f func name name base startswith _foreach_ required_inputs = set arg args_with_derivatives arg type TensorList const ITensorListRef is_foreach The functions taking TensorList handle everything internally continue arg_name = arg name found = re search IDENT_REGEX format arg_name formula found raise RuntimeError f The forward formula defn_name using base name arg_name f argument which ambiguous You should use arg_name _p access primal f value arg_name _t access tangent found = re search IDENT_REGEX format arg_name + postfix formula found required_inputs add arg_name tuple required_inputs updated_derivatives list ForwardDerivative = defn forward_derivatives formula = defn formula required_inputs_tangent = find_required_inputs formula _t formula == auto_element_wise assert f func kind = SchemaKind inplace f Cannot use auto_element_wise f func name because in-place variant len args_with_derivatives == len forward_derivatives len forward_derivatives var_names raise RuntimeError f Derivative definition defn_name derivatives yaml defines forward definition gradient element_wise only works functions single differentiable input single differentiable output len derivatives == raise RuntimeError f Derivative definition defn_name derivatives yaml defines forward definition gradient element_wise does defines gradient formula its argument which required This transformation based observation element-wise functions Jacobian matrix diagonal thus doing J v same v^T J ^T practice we ignore transpositions For complex case we use hermitian transpose get v conj J conj So here we going reuse backward formula replace two things all occurrences grad foo_t conj where foo name unique differentiable input all usage original input foo its primal value foo_p conjugate final result For example abs backward formula grad sgn And function generates forward formula self_t conj self_p sgn conj backward_formula = derivatives original_formula input_name = args_with_derivatives name Do replacement grad repl m Any - str f m group input_name _t conj m group fw_formula = re sub IDENT_REGEX format grad repl backward_formula Do replacement input variables arg args_with_derivatives arg_name = arg name repl m Any - str f m group arg_name _p m group fw_formula = re sub IDENT_REGEX format arg_name repl fw_formula Do final conjugate fw_formula = f fw_formula conj Since there single differentiable inputs we necessarily need its tangent we can simply require all differentiable input s tangent required_inputs_tangent = tuple all_arg_names formula = fw_formula formula == auto_linear len forward_derivatives len forward_derivatives var_names raise RuntimeError f Derivative definition defn_name derivatives yaml defines forward definition gradient linear only works functions single differentiable output This transformation based observation linear functions can written y = f x = A x For some matrix A Jacobian function f also A So doing J v = A v = f v Hence do jvp we simply need evaluate function point v instead x We do calling forward again replacing any occurrence differentiable input foo s tangent foo_t Note multiple inputs problem long function truly linear wrt vector where all differentiable inputs stacked diff_arg_names = arg name arg args_with_derivatives assert len diff_arg_names Do replacement input variables new_args = arg_name all_arg_names arg_name diff_arg_names arg_name = arg_name + _t pyrefly ignore bad-argument-type new_args append arg_name TODO we trolling f func has_symint defn_name += _symint Call into forward again We need two cases here handle both Tensor methods functions Variant function f variants fw_formula = f defn_name join new_args assert Variant method f variants fw_formula = f new_args defn_name join new_args All input tangents always used so all them required here required_inputs_tangent = tuple diff_arg_names formula = fw_formula At point formula final modified anymore During forward formula we use primal instead input Tensors This call inspects formula find which input s primal used required_inputs_primal = find_required_inputs formula _p updated_derivatives append ForwardDerivative formula=formula var_names=defn var_names var_types=defn var_types required_inputs_fw_grad=required_inputs_tangent required_inputs_primal=required_inputs_primal required_original_self_value=False is_reusing_outplace_formula=False updated_derivatives is_forward_derivative_definition all_arg_names list str names tuple str - bool name names name all_arg_names raise RuntimeError Expected ` names ` non-empty create_differentiability_info defn_dict dict Any Any functions_by_signature dict FunctionSchema list NativeFunction functions_by_schema dict str NativeFunction op_counter Counter str used_dispatch_keys set str - tuple FunctionSchema dict str DifferentiabilityInfo Processes single entry ` defn ` derivatives yaml canonical_function functions Sequence NativeFunction name str - NativeFunction f functions f func is_functional_fn f func is_out_fn name == str f func name name f some functions only have in-place variants assert name + _ == cpp name functions func functions split_names raw_names str - tuple str Given foo bar foo bar tuple x strip x raw_names split check_grad_usage defn_name str derivatives Sequence Derivative - None Check some subtle mistakes one might make when writing derivatives These mistakes will compile will latent until function used double backwards uses_grad = False true any derivative uses grad num_grads_uses = count uses grads grads INDEX uses_named_grads = False true any derivative uses grad_ name used_grads_indices list int = which indices grads used d derivatives formula = d formula uses_grad = uses_grad bool re findall IDENT_REGEX format grad formula num_grads_uses += len re findall IDENT_REGEX format grads formula uses_named_grads = uses_named_grads bool d named_gradients used_grads_indices extend used_gradient_indices formula This basic sanity check number places we see grads should no fewer than number indices we see inside grads They may equal because we may use grads without index assert num_grads_uses = len used_grads_indices Thus number equal every use grads also indexed only_used_grads_indices = num_grads_uses == len used_grads_indices uses_grad num_grads_uses raise RuntimeError f Derivative definition defn_name derivatives yaml illegally mixes use grad grads Consider replacing occurrences grad grads only_used_grads_indices set used_grads_indices == raise RuntimeError f Derivative definition defn_name derivatives yaml solely refers grads If first output indeed only differentiable output replace grads grad otherwise there likely error your derivatives declaration uses_named_grads uses_grad num_grads_uses raise RuntimeError f Derivative definition defn_name derivatives yaml illegally mixes use grad_RETURN_NAME grad grads x Use only one method identifying gradients with_native_function set_up_derivatives f NativeFunction - tuple Sequence Derivative Sequence ForwardDerivative Sequence Binding Sequence str Sequence str Set up derivative information derivatives list Derivative = forward_derivatives list ForwardDerivative = non_differentiable_arg_names list str = args_with_derivatives_set set str = set all_arg_names = name cpp_arguments f all_ret_names = r name r f func returns only used assert below output_differentiability captured enclosed scope Don t modify If present then no output explicitly undifferentiable It may present shorter than length values If s case any value does have corresponding entry considered differentiable differentiability = output_differentiability True len f func returns A available named gradient available_named_gradients = f grad_ ret name ret differentiable zip f func returns differentiability has been explicitly made undifferentiable differentiable has name ret name None its type differentiable ret type is_tensor_like raw_names sorted defn keys formula = defn raw_names names = split_names raw_names name names assert name all_arg_names name all_ret_names f While processing derivative formula f func name wrt name f expected name both input arg named is_forward_derivative_definition all_arg_names names forward_derivatives append create_forward_derivative f formula names formula lower strip == non_differentiable non_differentiable_arg_names += names derivative = create_derivative f formula names available_named_gradients derivatives append derivative args_with_derivatives_set &#124; = set names overlap = args_with_derivatives_set intersection non_differentiable_arg_names overlap raise RuntimeError f derivatives definition defn have overlapped non_differentiable f differentiable variables overlap Next let us determine list inputs order TODO do we need eagerly calculate save here Can derived NativeFunction ` derivatives ` callsites instead args_with_derivatives = cpp_arguments f name args_with_derivatives_set Postprocess forward derivatives definitions now we know differentiable arguments forward_derivatives = postprocess_forward_derivatives f defn_name all_arg_names derivatives forward_derivatives args_with_derivatives Test see use grads makes sense check_grad_usage defn_name derivatives derivatives forward_derivatives args_with_derivatives non_differentiable_arg_names available_named_gradients NB Removes name defn dictionary specification = defn_dict pop name defn_name _ = split_name_params specification NB Removes output_differentiability defn dictionary ` None ` means all differentiable output_differentiability = defn_dict pop output_differentiability None output_differentiability_conditions = None output_differentiability any isinstance diff str diff output_differentiability len output_differentiability = raise RuntimeError f Not supported specification f output_differentiability must either f list bool list str where each str f condition In case where condition f we only support single-output functions f Please file us issue output_differentiability_conditions = output_differentiability output_differentiability = True schema_function = functions_by_schema get specification schema_function avail = \n join k k v functions_by_schema items cpp name v func == defn_name raise RuntimeError f could find ATen function schema specification f Available signatures \n avail now map legacy schema isn t technically necessary we d need some logic here map in-place schemas out-of-place variants TODO maybe logic handle legacy schema no longer necessary signature = schema_function func signature functions = functions_by_signature signature len functions == avail = \n join str k k v functions_by_signature items cpp name k == defn_name raise RuntimeError f could find ATen function legacy signature signature f corresponding schema specification Please report bug PyTorch f Available signatures \n avail canonical = canonical_function functions defn_name grad_input_mask name cpp_arguments canonical raise RuntimeError f Schema defn_name has argument named grad_input_mask name would shadowed our codegen Please use different name native_functions yaml result name cpp_arguments canonical raise RuntimeError f Schema defn_name has argument named result only allowed outputs Please use different name native_functions yaml diffinfo_dict = key defn defn_dict dispatch items key = Default key _VALID_AUTOGRAD_KEYS raise RuntimeError f Invalid dispatch key key derivatives yaml specification f expected key one _VALID_AUTOGRAD_KEYS key used_dispatch_keys used_dispatch_keys add key derivatives forward_derivatives args_with_derivatives non_differentiable_arg_names available_named_gradients = set_up_derivatives canonical used_named_gradients set str = set d derivatives used_named_gradients &#124; = d named_gradients only assign op name we actually going calculate derivative op = None args_with_derivatives op_prefix = _create_op_prefix defn_name key = Default op_prefix = op_prefix + key op = f op_prefix op_counter op_prefix op_counter op_prefix += diffinfo_dict key = DifferentiabilityInfo name=defn_name func=canonical op=op derivatives=derivatives forward_derivatives=forward_derivatives all_saved_inputs=dedup_vars v d derivatives v d saved_inputs all_saved_outputs=dedup_vars v d derivatives v d saved_outputs available_named_gradients=available_named_gradients used_named_gradients=used_named_gradients args_with_derivatives=args_with_derivatives non_differentiable_arg_names=non_differentiable_arg_names output_differentiability=output_differentiability output_differentiability_conditions=output_differentiability_conditions canonical func diffinfo_dict GRAD_INDEX_REGEX = r ^ &#124; \W grads\ \d+ \ used_gradient_indices formula str - list int Determine list gradient indices i grads i used formula used_gradient_indices foo grads grads int i i re findall GRAD_INDEX_REGEX formula saved_variables formula str nctypes list NamedCType var_names tuple str - tuple str tuple SavedAttribute stride_expr name str - str assert var_names == name Replacement strides currently only supported single derivatives same tensor strides being called f strides_or_error name name REPLACEMENTS list tuple str dict str Any = replace sym_sizes self_sym_sizes r sym_sizes\ \ suffix _sym_sizes nctype lambda name NamedCType name BaseCType symIntArrayRefT replace self- sym_sizes self_sym_sizes_opt r - sym_sizes\ \ suffix _sym_sizes_opt nctype lambda name NamedCType name OptionalCType BaseCType symIntArrayRefT expr lambda name f name has_value std optional c SymIntArrayRef name - sym_sizes std nullopt replace sym_blocksize self_sym_blocksize_opt r sym_blocksize\ \ suffix _self_sym_blocksize_opt nctype lambda name NamedCType name OptionalCType BaseCType symIntArrayRefT expr lambda name f sparse_csr getSymIntBlockSize name replace options self_options r options\ \ suffix _options nctype lambda name NamedCType name BaseCType tensorOptionsT replace zeros_like self_info r zeros_like\ \ suffix _info nctype lambda name NamedCType name BaseCType typeAndSizeT expr lambda name name save-time res lambda name name + _info zeros eval-time replace sym_size self_sym_size_ r sym_size\ - \w+ \ suffix lambda m f _sym_argsize_ m groups replace - minus_ nctype lambda name NamedCType name BaseCType SymIntT replace numel self_numel r numel\ \ suffix _numel nctype lambda name NamedCType name BaseCType longT replace sym_numel self_sym_numel r sym_numel\ \ suffix _sym_numel nctype lambda name NamedCType name BaseCType SymIntT replace to_args_sizes self_args_sizes r to_args_sizes\ \ suffix _args_sizes nctype lambda name NamedCType name VectorCType VectorCType BaseCType longT replace to_args_sizes_symint self_args_sizes r to_args_sizes_symint\ \ suffix _args_sizes_symint nctype lambda name NamedCType name VectorCType VectorCType BaseCType SymIntT replace to_args_scalartypes self_args_scalartypes r to_args_scalartypes\ \ suffix _args_scalartypes nctype lambda name NamedCType name VectorCType BaseCType scalarTypeT replace TensorGeometry self_geometry r TensorGeometry\ \ suffix _geometry nctype lambda name NamedCType name BaseCType tensorGeometryT r scalar_type\ \ suffix _scalar_type nctype lambda name NamedCType name BaseCType scalarTypeT replace dim self_dim r dim\ \ suffix _dim nctype lambda name NamedCType name BaseCType longT replace sym_strides self_sym_strides r sym_strides\ \ suffix _sym_strides nctype lambda name NamedCType name BaseCType symIntArrayRefT expr stride_expr replace layout self_layout r layout\ \ suffix _layout nctype lambda name NamedCType name BaseCType layoutT replace is_conj self_conjugate r is_conj\ \ suffix _conjugate nctype lambda name NamedCType name BaseCType boolT find which arguments need saved saved list SavedAttribute = sizes formula - sizes formula raise RuntimeError sizes supported derivative formulas Instead please use SymInt version + f sym_sizes which returned c SymIntArrayRef formula= formula re search r \ size\ - \d+\ formula re search r - size\ - \d+\ formula raise RuntimeError size int supported derivative formulas Instead please use SymInt version + f sym_size int which returned c SymIntArrayRef formula= formula strides formula - strides formula raise RuntimeError strides supported derivative formulas Instead please use SymInt version + f sym_strides which returned c SymIntArrayRef formula= formula nctype nctypes pyrefly ignore bad-assignment name = nctype name name isinstance nctype name SpecialArgName nctype name First search formula expressions which can evaluated when autograd Function created avoid saving variables regex info REPLACEMENTS repl m re Match str - str suffix str = pyrefly ignore bad-assignment info suffix m callable info suffix info suffix expr str = info expr name expr info m group saved append SavedAttribute nctype=info nctype name + suffix expr=expr res info replacement str = info res name replacement name + suffix formula = re sub regex format name repl formula std optional std string types stored Backward nodes must converted std optional std string_view before being passed into backward function nctype type == OptionalCType BaseCType stringT formula = re sub rf \b name \b f name has_value std optional std string_view name value std nullopt formula Find any variables which remain formula save them re search IDENT_REGEX format name formula saved append SavedAttribute nctype=nctype expr=name formula tuple saved _create_op_prefix name str - str r Takes native function name converts op prefix name Note name parameter must native function name without optional variant suffix so add instead add out OP names correspond classes hence change title case Example _create_op_prefix add AddBackward camel_case = join p title p name split _ camel_case + Backward replace ForwardBackward Backward dedup_vars vars Sequence SavedAttribute - Sequence SavedAttribute seen set str = set saved list SavedAttribute = var vars name = var nctype name name isinstance var nctype name SpecialArgName var nctype name name seen continue seen add name saved append var saved