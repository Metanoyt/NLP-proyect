Owner s module fx operator torch torch fx torch fx experimental const_fold torch fx passes shape_prop _extract_tensor_metadata ShapeProp torch testing _internal common_utils raise_on_run_directly TestCase TestConstFold TestCase _get_attr node mod = node graph owning_module target = str node target target_atoms = target split curr_obj = mod i atom enumerate target_atoms hasattr curr_obj atom raise RuntimeError f Node referenced nonexistent target join target_atoms i f original whole target target curr_obj = getattr curr_obj atom curr_obj _verify_const_fold_mod mod_folded const_fold FoldedGraphModule assertTrue mod_folded const_subgraph_module None Check we don t have const non-const fold graphs gm we do have const folded get_attr found_folded_attrs = False n mod_folded graph nodes n op == get_attr n target startswith _FX_CONST_FOLDED_ATTRS found_folded_attrs = True n op == call_module assertTrue n target submod_ submod_ assertTrue found_folded_attrs test_const_fold_basic_one_attr_no_name_collision r Perform constant folding conversion original mod split constant folding module two split subgraphs where there s single attr fold single output attr result replace attr attr &#124; &#124; &#124; &#124; x add add \ &#124; sub y output becomes attr add_ \ == ------- + ------- const base subgraph split mul attr x input previous subgraph \ \ attr add sub y &#124; \ output mul attr \ add &#124; output ConstFoldTestModule torch nn Module __init__ - None super __init__ attr_ = torch nn Parameter torch tensor - attr_ = torch nn Parameter torch tensor forward x y = attr_ + attr_ x = x - x y + attr_ mod = ConstFoldTestModule mod_folded const_fold FoldedGraphModule = const_fold split_const_subgraphs mod _verify_const_fold_mod mod_folded Now run both folded non-folded check results equal in_x in_y = torch tensor - torch tensor base_result = mod in_x in_y fold_result = mod_folded in_x in_y assertTrue torch equal fold_result base_result test_const_fold_basic_one_attr_name_collision r Perform constant folding conversion original mod split constant folding module two split subgraphs where there s single attr fold single output attr result replace Name attrs such they will collide name folded attrs add_ add_ &#124; &#124; &#124; &#124; x add add \ &#124; sub y output becomes attr add_ \ == ------- + ------- const base subgraph split mul add_ x input previous subgraph \ \ attr add sub y &#124; \ output mul add_ \ add &#124; output ConstFoldTestModule torch nn Module __init__ - None super __init__ Note Named such result name collision add_ __CF = torch nn Parameter torch tensor add_ __CF = torch nn Parameter torch tensor forward x y = add_ __CF + add_ __CF x = x - x y + add_ __CF mod = ConstFoldTestModule mod_folded const_fold FoldedGraphModule = const_fold split_const_subgraphs mod _verify_const_fold_mod mod_folded Now run both folded non-folded check results equal in_x in_y = torch tensor torch tensor base_result = mod in_x in_y fold_result = mod_folded in_x in_y assertTrue torch equal fold_result base_result test_const_fold_basic_placeholder_reordered Test code path where placeholder comes after normal op node FX ConstFoldTestModule torch nn Module forward x y x + y mod = ConstFoldTestModule mod = torch fx symbolic_trace mod yy = None n mod graph nodes n op == placeholder n target == y yy = n yy None n op == call_function yy prepend n break mod_folded const_fold FoldedGraphModule = const_fold split_const_subgraphs mod assertTrue mod_folded const_subgraph_module None Now run both folded non-folded check results equal in_x = torch tensor - in_y = torch tensor base_result = mod in_x in_y fold_result = mod_folded in_x in_y assertTrue torch equal fold_result base_result test_const_fold_noop r Check graph no constant folding handled correctly x attr \ sub &#124; output ConstFoldTestModule torch nn Module __init__ - None super __init__ attr = torch nn Parameter torch tensor - forward x x - attr mod = ConstFoldTestModule mod_folded const_fold FoldedGraphModule = const_fold split_const_subgraphs mod Check folded graph module None since there no folding do assertTrue mod_folded const_subgraph_module None Now run both folded non-folded check results equal in_x = torch tensor - base_result = mod in_x fold_result = mod_folded in_x assertTrue torch equal fold_result base_result test_const_fold_basic_two_attr_three_input r Perform constant folding conversion original mod split constant folding module two split subgraphs where there two attrs fold into single output there three placeholder inputs attr attr attr attr \ \ x add add \ &#124; sub y output becomes attr add_ \ == ------- + ------- const base subgraph split mul z x input previous subgraph \ \ attr div sub y &#124; \ output mul z \ div &#124; output ConstFoldTestModule torch nn Module __init__ - None super __init__ attr = torch nn Parameter torch tensor - attr = torch nn Parameter torch tensor forward x y z = attr + attr sub = x - mul = sub y mul z mod = ConstFoldTestModule mod_folded const_fold FoldedGraphModule = const_fold split_const_subgraphs mod _verify_const_fold_mod mod_folded Now run both folded non-folded check results equal in_x in_y in_z = torch tensor - torch tensor torch tensor base_result = mod in_x in_y in_z fold_result = mod_folded in_x in_y in_z assertTrue torch equal fold_result base_result test_const_fold_basic_two_attr r Perform constant folding conversion original mod split constant folding module two split subgraphs where there two attrs fold into single output attr attr attr attr \ \ x add add becomes attr add_ \ == ------- + ------- const base subgraph split sub x &#124; input previous subgraph attr &#124; \ output sub &#124; output ConstFoldTestModule torch nn Module __init__ - None super __init__ attr = torch nn Parameter torch randn attr = torch nn Parameter torch randn forward x y = attr + attr x + y mod = ConstFoldTestModule mod_folded const_fold FoldedGraphModule = const_fold split_const_subgraphs mod _verify_const_fold_mod mod_folded Now run both folded non-folded check results equal in_x = torch randn fold_result = mod_folded in_x base_result = mod in_x assertTrue torch equal fold_result base_result test_const_fold_multi_const_folded_attrs r Perform constant folding conversion original mod split constant folding module two split subgraphs where there two attrs fold into two new attrs attr attr attr attr \ &#124; \ &#124; permute &#124; sum permute &#124; sum \ \ &#124; x add y add &#124; \ \ &#124; &#124; sub add output output become attrs add_ mul_ \ == -------- + ------- + ------ const base subgraph split \ x &#124; y &#124; inputs previous subgraph add \ \ attrs &#124; sub add linear \ &#124; add sigmoid &#124; &#124; linear output &#124; sigmoid &#124; output ConstFoldTestModule torch nn Module __init__ - None super __init__ attr = torch nn Parameter torch randn attr = torch nn Parameter torch randn lin = torch nn Linear forward x y = attr + attr permute x = x - amax = torch sum attr dim= y = y + amax torch sigmoid lin x + y mod = ConstFoldTestModule mod_folded const_fold FoldedGraphModule = const_fold split_const_subgraphs mod _verify_const_fold_mod mod_folded Now run both folded non-folded check results equal in_x in_y = torch randn torch randn fold_result = mod_folded in_x in_y base_result = mod in_x in_y assertTrue torch equal fold_result base_result test_const_fold_submod_hierarchy r Perform constant folding conversion original mod split constant folding module where one folded attrs comes submod deeper hierarchy base module TracedThroughModule torch nn Module __init__ - None super __init__ internal_attr = torch nn Parameter torch randn forward internal_attr ConstFoldTestModule torch nn Module __init__ - None super __init__ my_mod = TracedThroughModule attr = torch nn Parameter torch randn forward x attr + my_mod + x mod = ConstFoldTestModule mod_folded const_fold FoldedGraphModule = const_fold split_const_subgraphs mod _verify_const_fold_mod mod_folded Now run both folded non-folded check results equal in_x = torch randn fold_result = mod_folded in_x base_result = mod in_x assertTrue torch equal fold_result base_result test_retain_node_meta r Perform constant folding conversion validate node meta retained ConstFoldTestModule torch nn Module __init__ - None super __init__ attr = torch nn Parameter torch randn forward x = attr + attr x - mod = ConstFoldTestModule gm = torch fx symbolic_trace mod Add count each node check after we const fold idx node enumerate gm graph nodes node op = output node meta meta_idx = idx Pre-folding idx placeholder idx get_attr will no longer used hence removed idx add will folded into get_attr idx sub gm_folded const_fold FoldedGraphModule = const_fold split_const_subgraphs gm _verify_const_fold_mod gm_folded Post-folding idx placeholder idx get_attr replaced original add original get_attr removed idx sub Check expected indices still here node gm_folded graph nodes node op == placeholder assertEqual node meta meta_idx node op == get_attr assertEqual node meta meta_idx node op == call_function node target == operator sub assertEqual node meta meta_idx assertEqual node op output Now run both folded non-folded check results equal in_x = torch randn fold_result = gm_folded in_x base_result = mod in_x assertTrue torch equal fold_result base_result test_const_fold_has_inlined_call_module_node ConstFoldTestModule torch nn Module __init__ - None super __init__ attr = torch nn Parameter torch randn mod = torch nn Identity mod relu = torch nn ReLU forward x = attr + attr mod relu x - mod = ConstFoldTestModule gm_folded = const_fold split_const_subgraphs mod Now run both folded non-folded check results equal in_x = torch randn fold_result = gm_folded in_x base_result = mod in_x assertTrue torch equal fold_result base_result test_const_fold_module_attr ConstFoldTestModule torch nn Module __init__ - None super __init__ const = torch nn Parameter torch randn mod = torch nn Identity mod attr = torch nn Parameter torch randn forward x = const + mod attr x = x + x + mod attr mod = ConstFoldTestModule gm_folded = const_fold split_const_subgraphs mod Now run both folded non-folded check results equal in_x = torch randn fold_result = gm_folded in_x base_result = mod in_x assertTrue torch equal fold_result base_result test_const_fold_unused_placeholder ConstFoldTestModule torch nn Module __init__ - None super __init__ const = torch nn Parameter torch randn forward x y z = const + const y + mod = ConstFoldTestModule gm_folded = const_fold split_const_subgraphs mod Now run both folded non-folded check results equal in_x = torch randn fold_result = gm_folded in_x in_x in_x base_result = mod in_x in_x in_x assertTrue torch equal fold_result base_result test_dict_output ConstFoldTestModule torch nn Module __init__ - None super __init__ const = torch nn Parameter torch randn forward x = const + const result x + mod = ConstFoldTestModule gm_folded = const_fold split_const_subgraphs mod Now run both folded non-folded check results equal in_x = torch randn fold_result = gm_folded in_x base_result = mod in_x assertTrue torch equal fold_result result base_result result test_two_outputs ConstFoldTestModule torch nn Module __init__ - None super __init__ const = torch nn Parameter torch randn forward x = const + const x x + mod = ConstFoldTestModule gm_folded = const_fold split_const_subgraphs mod Now run both folded non-folded check results equal in_x = torch randn fold_result = gm_folded in_x base_result = mod in_x assertTrue torch equal fold_result base_result assertTrue torch equal fold_result base_result test_three_outputs ConstFoldTestModule torch nn Module __init__ - None super __init__ const = torch nn Parameter torch randn forward x = const + const x x + x + mod = ConstFoldTestModule gm_folded = const_fold split_const_subgraphs mod Now run both folded non-folded check results equal in_x = torch randn fold_result = gm_folded in_x base_result = mod in_x assertTrue torch equal fold_result base_result assertTrue torch equal fold_result base_result assertTrue torch equal fold_result base_result test_check_inline_non_const r Perform constant folding conversion check non-const module inlined correctly ConstFoldTestModule torch nn Module __init__ - None super __init__ attr = torch nn Parameter torch randn forward x = attr + attr x - x mod = ConstFoldTestModule gm = torch fx symbolic_trace mod gm_folded const_fold FoldedGraphModule = const_fold split_const_subgraphs gm _verify_const_fold_mod gm_folded Check there no call modules because they ve been inlined extracted const folding node gm_folded graph nodes assertNotEqual node op call_module Now run both folded non-folded check results equal in_x = torch randn fold_result = gm_folded in_x base_result = mod in_x assertTrue torch equal fold_result base_result test_check_inline_non_const_mult_return r Perform constant folding conversion check non-const module inlined correctly ConstFoldTestModule torch nn Module __init__ - None super __init__ attr = torch nn Parameter torch randn forward x = attr + attr x - x mod = ConstFoldTestModule gm = torch fx symbolic_trace mod gm_folded const_fold FoldedGraphModule = const_fold split_const_subgraphs gm _verify_const_fold_mod gm_folded Check there no call modules because they ve been inlined extracted const folding node gm_folded graph nodes assertNotEqual node op call_module Now run both folded non-folded check results equal in_x = torch randn fold_result = gm_folded in_x base_result = mod in_x assertTrue torch equal fold_result base_result assertTrue torch equal fold_result base_result test_check_skip_folding_quant_dequant_pattern r Set up skip_folding_quant_dequant function skip quant dequant pattern This example shows how use skip_folding_node_fn ConstFoldTestModule torch nn Module __init__ - None super __init__ weight = torch nn Parameter torch randn bias = torch nn Parameter torch randn relu = torch nn ReLU forward x quant_weight = torch quantize_per_tensor weight torch quint dequant_weight = torch dequantize quant_weight output = torch nn functional linear x dequant_weight bias relu output mod = ConstFoldTestModule in_x = torch randn gm = torch fx symbolic_trace mod skip_folding_quant_dequant node torch fx Node node target = torch quantize_per_tensor False If quantize_per_node - dequantize then skip folding user node users user target == torch dequantize True False gm_folded const_fold FoldedGraphModule = const_fold split_const_subgraphs gm skip_folding_node_fn=skip_folding_quant_dequant Check folded graph module None since there no folding do assertTrue gm_folded const_subgraph_module None Now run both folded non-folded check results equal fold_result = gm_folded in_x base_result = mod in_x assertTrue torch equal fold_result base_result test_fold_module r Perform constant folding call_module node ConstFoldTestModule torch nn Module __init__ - None super __init__ lin_input = torch nn Parameter torch randn lin = torch nn Linear forward x lin lin_input + x mod = ConstFoldTestModule mod_folded const_fold FoldedGraphModule = const_fold split_const_subgraphs mod _verify_const_fold_mod mod_folded Now run both folded non-folded check results equal inp = torch randn assertTrue torch equal mod_folded inp mod inp test_const_fold_tensor_meta _test_const_fold_tensor_meta True _test_const_fold_tensor_meta False _test_const_fold_tensor_meta requires_grad Verify tensor_meta handled correctly ConstFoldTestModule torch nn Module __init__ - None super __init__ attr_ = torch nn Parameter torch tensor - requires_grad attr_ = torch nn Parameter torch tensor requires_grad forward x y = attr_ + attr_ x = x - x y + attr_ mod = ConstFoldTestModule gm = torch fx symbolic_trace mod in_x in_y = torch tensor - torch tensor ShapeProp gm propagate in_x in_y mod_folded const_fold FoldedGraphModule = const_fold split_const_subgraphs gm device_for_folded_attrs= cpu _verify_const_fold_mod mod_folded mod_folded run_folding n mod_folded graph nodes n op == get_attr attr = _get_attr n assertEqual _extract_tensor_metadata attr n meta tensor_meta Now run both folded non-folded check results equal base_result = mod in_x in_y fold_result = mod_folded in_x in_y assertTrue torch equal fold_result base_result test_fold_pure_subgraph SubModule torch nn Module forward torch full + Create parent graph module subgraph output ep = torch export export SubModule parent_graph = torch fx Graph call_mod = parent_graph call_module sub args= get_item = parent_graph call_function operator getitem args= call_mod slice None parent_graph output get_item parent = torch fx GraphModule sub ep module parent_graph mod_folded const_fold FoldedGraphModule = const_fold split_const_subgraphs parent device_for_folded_attrs= cpu _verify_const_fold_mod mod_folded test_do_not_fold_impure_subgraph Skip folding any subgraph containing impure ops SubModule torch nn Module forward torch randn + Create parent graph module subgraph output ep = torch export export SubModule parent_graph = torch fx Graph call_mod = parent_graph call_module sub args= get_item = parent_graph call_function operator getitem args= call_mod slice None parent_graph output get_item parent = torch fx GraphModule sub ep module parent_graph mod_folded const_fold FoldedGraphModule = const_fold split_const_subgraphs parent device_for_folded_attrs= cpu assertIsNone mod_folded const_subgraph_module __name__ == __main__ raise_on_run_directly test test_fx py