mypy allow-untyped-defs contextlib logging math functools lru_cache typing Any Callable cast Optional TypeVar Union unittest mock patch torch torch utils torch utils _ordered_set OrderedSet _dynamo utils counters config ir lowering L kernel mm_common mm_args select_algorithm DataProcessorTemplateWrapper utils has_free_symbols is_same_mkldnn_tensor is_same_tensor parallel_num_threads virtualized ops V cpp get_export_declaration cpp_micro_gemm CppMicroBrgemm CppMicroGemm CppMicroGemmAMX CppMicroGemmFP Vec create_micro_gemm is_int _woq_gemm_small_m_dim_corner_case LayoutType cpp_template CppTemplate cpp_template_kernel CppTemplateKernel cpp_utils create_epilogue_with_attr DTYPE_TO_CPP GemmBlocking get_gemm_template_output_and_compute_dtype log = logging getLogger __name__ GEMM_TEMPLATE_INIT_BLOCKING_BASIC_BLOCK = r constexpr int _t num_threads = num_threads constexpr int _t N = N constexpr int _t K = K constexpr int _t Mr = micro_gemm register_blocking block_m constexpr int _t Nr = micro_gemm register_blocking block_n constexpr int _t Kr = micro_gemm register_blocking block_k constexpr int _t Nr_blocks = N + Nr - Nr constexpr int _t Kr_blocks = K + Kr - Kr - is_dynamic_M const int _t M = kernel size GemmOut const int _t Mr_blocks = M + Mr - Mr - constexpr int _t M = kernel size GemmOut constexpr int _t Mr_blocks = M + Mr - Mr - endif GEMM_TEMPLATE_INIT_BLOCKING_EXTENDED = r - is_dynamic_M - num_threads int _t Mt_blocks Nt_blocks Kt_blocks mm_get_thread_blocking num_threads config cpp gemm_max_k_slices M N K Mr Nr Kr Mt_blocks Nt_blocks Kt_blocks - const auto Mt_blocks = Mr_blocks const auto Nt_blocks = Nr_blocks const auto Kt_blocks = Kr_blocks - endif int _t Mc_blocks Nc_blocks Kc_blocks uint _t L _cache_size = L _cache_size uint _t L _cache_size = L _cache_size mm_get_cache_blocking kernel dtype X kernel dtype W num_threads M N K Mr Nr Kr Mt_blocks Nt_blocks Kt_blocks Mc_blocks Nc_blocks Kc_blocks L _cache_size L _cache_size const int _t num_Mc_blocks = Mr_blocks + Mc_blocks - Mc_blocks const int _t num_Nc_blocks = Nr_blocks + Nc_blocks - Nc_blocks const int _t num_Mt_blocks = Mr_blocks + Mt_blocks - Mt_blocks const int _t num_Nt_blocks = Nr_blocks + Nt_blocks - Nt_blocks const int _t num_Kt_blocks = Kr_blocks + Kt_blocks - Kt_blocks - constexpr int _t Mt_blocks = template thread_blocking num_threads block_m constexpr int _t Nt_blocks = template thread_blocking num_threads block_n constexpr int _t Kt_blocks = template thread_blocking num_threads block_k constexpr int _t Mc_blocks = template cache_blocking num_threads block_m constexpr int _t Nc_blocks = template cache_blocking num_threads block_n constexpr int _t Kc_blocks = template cache_blocking num_threads block_k constexpr int _t num_Mc_blocks = Mr_blocks + Mc_blocks - Mc_blocks constexpr int _t num_Nc_blocks = Nr_blocks + Nc_blocks - Nc_blocks constexpr int _t num_Mt_blocks = Mr_blocks + Mt_blocks - Mt_blocks constexpr int _t num_Nt_blocks = Nr_blocks + Nt_blocks - Nt_blocks constexpr int _t num_Kt_blocks = Kr_blocks + Kt_blocks - Kt_blocks - endif - is_woq_int int _t group_size = q_group_size - endif make sure all partitions assigned kernel assert_function Mt_blocks Nt_blocks Kt_blocks num_threads = Mr_blocks Nr_blocks Kr_blocks Not all partitions assigned GEMM_TEMPLATE_MULTI_THREADS_PARAMS = r const int tid = omp_get_thread_num const int _t k_group_id = tid num_Kt_blocks const int _t k_slice_id = tid num_Kt_blocks const int _t n_group_id = k_group_id num_Nt_blocks const int _t n_slice_id = k_group_id num_Nt_blocks const int _t k_block_start = k_slice_id Kt_blocks const int _t k_block_end = std min k_block_start + Kt_blocks Kr_blocks const int _t n_block_start = n_slice_id Nt_blocks const int _t n_block_end = std min n_block_start + Nt_blocks Nr_blocks const int _t m_block_start = std min n_group_id Mt_blocks Mr_blocks const int _t m_block_end = std min m_block_start + Mt_blocks Mr_blocks const int _t num_Mc_blocks_per_thread = m_block_end - m_block_start + Mc_blocks - Mc_blocks GEMM_TEMPLATE_SINGLE_THREAD_PARAMS = r constexpr int tid = constexpr int _t k_group_id = constexpr int _t k_slice_id = constexpr int _t n_group_id = constexpr int _t n_slice_id = constexpr int _t m_block_start = constexpr int _t n_block_start = constexpr int _t n_block_end = Nr_blocks constexpr int _t k_block_start = constexpr int _t k_block_end = Kr_blocks - is_dynamic_M const int _t num_Mc_blocks_per_thread = num_Mc_blocks const int _t m_block_end = Mr_blocks - constexpr int _t num_Mc_blocks_per_thread = num_Mc_blocks constexpr int _t m_block_end = Mr_blocks - endif GEMM_TEMPLATE_M_LOOP_PARAMS = r const int _t my_mc_block_id = mc_block_id + n_slice_id num_Mc_blocks_per_thread const int _t mc = m_block_start + my_mc_block_id Mc_blocks const int _t m_start = mc Mr const int _t m_end = std min std min mc + Mc_blocks m_block_end Mr M const int _t m_size = m_end - m_start GEMM_TEMPLATE_N_LOOP_PARAMS = r const int _t n_start = nc Nr const int _t n_end = std min std min nc + Nc_blocks n_block_end Nr N const int _t n_size = n_end - n_start NB assume we pad N nc_block_end won t exceed padded N here const int _t nc_block_end = std min nc + Nc_blocks n_block_end GEMM_TEMPLATE_MICROKERNEL_DEF = r template header getvalue micro_gemm codegen_define kernel GEMM_TEMPLATE_STUB_DEF = r - x_scale none - set kernel_args = X X W W inp inp x_scale x_scale x_zp x_zp w_scale w_scale w_zp w_zp - is_woq_int - set kernel_args = X X W W q_group_size q_group_size qscale_and_zeros qscale_and_zeros - - set kernel_args = X X W W inp inp - endif extern C export_declaration kernel def_kernel inputs=kernel_args outputs= Y Y aliases=aliases GEMM_TEMPLATE = r template codegen_gemm_stub_def kernel maybe_codegen_profile template codegen_blocks num_threads N K micro_gemm is_dynamic_M kernel GemmOut config L _cache_size L _cache_size X W - maybe_k_slicing std unique_ptr std unique_ptr DTYPE_TO_CPP acc_buf_dtype local_buf_ptrs num_Kt_blocks local_buf_ptrs reset new std unique_ptr DTYPE_TO_CPP acc_buf_dtype num_Mc_blocks num_Nc_blocks num_Kt_blocks - endif - num_threads #pragma omp parallel num_threads num_threads template codegen_multi_threads_params &#124; indent false - template codegen_single_thread_params is_dynamic_M &#124; indent false - endif micro_gemm codegen_init kernel - use_local_acc - set acc_buf_name = local_acc_buf kernel define_buffer acc_buf_name Mc_blocks Mr Nc_blocks Nr acc_buf_dtype - endif int _t mc_block_id = mc_block_id num_Mc_blocks_per_thread mc_block_id++ template codegen_m_loop_params &#124; indent false int _t nc = n_block_start nc n_block_end nc += Nc_blocks template codegen_n_loop_params &#124; indent false - use_local_acc - set acc = kernel local_buffers acc_buf_name kernel reinit_buffer_if_null acc_buf_name - - set acc = kernel slice_nd GemmOut m_start m_end n_start n_end - endif int _t kc = k_block_start kc k_block_end kc += Kc_blocks int _t k_start = kc Kr int _t k_end = std min std min kc + Kc_blocks k_block_end Kr K - set tile_X = kernel slice_nd X m_start m_end k_start k_end int _t nci = nc nci nc_block_end nci++ - set acc_slice = kernel slice_nd acc m_end - m_start nci - nc Nr nci - nc + Nr - template should_block_weights is_woq_int - set tile_W_ d = kernel slice_nd W nci nci + k_start k_end - set tile_W = kernel view tile_W_ d k_end - k_start micro_gemm register_blocking block_n - - is_woq_int - set tile_W = kernel slice_nd W nci Nr nci + Nr k_start Nr k_end Nr - set tile_qparam = kernel slice_nd qscale_and_zeros k_start group_size k_end group_size nci Nr nci + Nr - - set tile_W = kernel slice_nd W k_start k_end n_start n_start + n_size - set tile_qparam = None - endif - endif kc == k_block_start micro_gemm codegen_call kernel tile_X tile_W acc_slice accum=False qscale_and_zeros=tile_qparam &#124; indent false micro_gemm codegen_call kernel tile_X tile_W acc_slice accum=True qscale_and_zeros=tile_qparam &#124; indent false - maybe_k_slicing num_Kt_blocks const int _t mxn_cache_block_id = mc Mc_blocks num_Nc_blocks + nc local_buf_ptrs mxn_cache_block_id num_Kt_blocks + k_slice_id reset kernel release_buffer acc_buf_name - endif - set tile_Y = kernel slice_nd Y_ d m_start m_end n_start n_end - set tile_acc = kernel slice_nd acc m_end - m_start n_end - n_start kernel store_output tile_Y tile_acc GemmOut epilogue_nodes offsets= m_start n_start reindexers=reindexers &#124; indent false - maybe_k_slicing num_Kt_blocks #pragma omp barrier int _t mc = m_block_start mc m_block_end mc += Mc_blocks We slice M-dim each thread k-slicing group works slice const int _t m_start_unsliced = mc Mr const int _t m_end_unsliced = std min std min mc + Mc_blocks m_block_end Mr M const int _t m_size_unsliced = m_end_unsliced - m_start_unsliced const int _t m_slice_size = m_size_unsliced + num_Kt_blocks - num_Kt_blocks const int _t m_start = std min m_start_unsliced + m_slice_size k_slice_id m_end_unsliced const int _t m_end = std min m_start_unsliced + m_slice_size k_slice_id + m_end_unsliced const int _t m_size = m_end - m_start const int _t m_offset = m_start - m_start_unsliced int _t nc = n_block_start nc n_block_end nc += Nc_blocks const int _t n_start = nc Nr const int _t n_end = std min std min nc + Nc_blocks n_block_end Nr N const int _t n_size = n_end - n_start const int _t mxn_cache_block_id = mc Mc_blocks num_Nc_blocks + nc auto acc_buf_name = local_buf_ptrs mxn_cache_block_id num_Kt_blocks get int _t other_slice = other_slice num_Kt_blocks other_slice++ auto other_acc = local_buf_ptrs mxn_cache_block_id num_Kt_blocks + other_slice get int _t m = m_offset m m_offset + m_size m++ #pragma omp simd int _t n = n n_size n++ acc_buf_name m Nr + n += other_acc m Nr + n - set tile_acc_m_slice = kernel slice_nd tile_acc m_offset m_offset + m_end - m_start kernel store_output tile_Y tile_acc_m_slice GemmOut epilogue_nodes offsets= m_start n_start reindexers=reindexers &#124; indent false - endif micro_gemm codegen_finalize kernel SMALL_M_GEMM_TEMPLATE = r template codegen_gemm_stub_def kernel maybe_codegen_profile template codegen_blocks num_threads N K micro_gemm is_dynamic_M kernel GemmOut config L _cache_size L _cache_size X W pragma omp parallel #pragma omp nowait int _t nr_block_id = nr_block_id Nr_blocks nr_block_id++ Handle one output M Nr block each thread int _t n_start = nr_block_id Nr int _t n_end = nr_block_id + Nr - use_local_acc - set acc_buf_name = local_acc_buf kernel define_stack_allocated_buffer acc_buf_name M Nr acc_buf_dtype - set acc = kernel local_buffers acc_buf_name - - set acc = kernel slice_nd GemmOut M n_start n_end - endif int _t kr_block_id = kr_block_id Kr_blocks kr_block_id++ loop parallelized int _t k_start = kr_block_id Kr int _t k_end = std min kr_block_id + Kr K - set tile_X = kernel slice_nd X M k_start k_end - set tile_W_ d = kernel slice_nd W nr_block_id nr_block_id + k_start k_end - set tile_W = kernel view tile_W_ d k_end - k_start micro_gemm register_blocking block_n C _UNLIKELY kr_block_id == micro_gemm codegen_call kernel tile_X tile_W acc accum=False prefetch=True &#124; indent false C _UNLIKELY k_end == K micro_gemm codegen_call kernel tile_X tile_W acc accum=True prefetch=False &#124; indent false micro_gemm codegen_call kernel tile_X tile_W acc accum=True prefetch=True &#124; indent false - set tile_Y = kernel slice_nd Y_ d M n_start n_end - set tile_acc = kernel slice_nd acc M n_end - n_start kernel store_output tile_Y tile_acc GemmOut epilogue_nodes offsets= n_start reindexers=reindexers &#124; indent false _is_int _gemm inputs isinstance inputs ir IRNode inputs get_dtype torch uint torch int isinstance inputs torch Tensor inputs dtype torch uint torch int get_padded_n n block_n n + block_n - block_n block_n _T = TypeVar _T ir IRNode torch Tensor transpose_w W _T trans_w bool - _T Transpose W based trans_w flag isinstance W ir IRNode trans_w isinstance W ir TensorBox pyrefly ignore bad-assignment W = ir TensorBox W W = L permute W trans_w assert isinstance W torch Tensor pyrefly ignore bad-assignment W = W transpose pyrefly ignore bad-return W expand_bias B Optional _T X _T - Optional _T Expand Bias same size X B None isinstance B ir IRNode isinstance B ir TensorBox pyrefly ignore bad-assignment B = ir TensorBox B assert hasattr X get_size pyrefly ignore missing-attribute B = L expand B X get_size B get_size - assert isinstance B torch Tensor assert isinstance X torch Tensor pyrefly ignore bad-assignment B = B expand X shape B shape - B prune_tensors input_nodes list ir IRNode new_input_nodes list ir IRNode Prune unused tensors ` V graph ` since GEMM Template use new packed weight share_storage base_tensor torch Tensor comp_tensor torch Tensor base_tensor is_mkldnn == comp_tensor is_mkldnn is_same_tensor base_tensor comp_tensor is_same_mkldnn_tensor base_tensor comp_tensor get_candidates input_nodes new_input_nodes Only Constant Buffer like weight bias might changed GEMM Template The Inductor IR Node may changed still share storage For example bias bfloat case which only do expand node node input_nodes node new_input_nodes isinstance node ir TensorBox ir StorageBox node get_name V graph constants any isinstance new_node ir TensorBox ir StorageBox new_node get_name V graph constants share_storage V graph constants node get_name V graph constants new_node get_name new_node new_input_nodes candidate_node get_candidates input_nodes new_input_nodes By using new packed weight GEMM template we can prune old weight has no other users This saves memory makes FX graph non-retraceable To support retracing we can add repack node FX graph For example mkldnn _linear_pointwise - repack_linear_wgt - packed_wgt_for_template candidate_tensor_users = candidate_tensor = V graph constants candidate_node get_name node reversed V graph graph nodes Case may happen when candidate tensor used more than get_attr node https github com pytorch pytorch issues node op == get_attr hasattr V graph module node target candidate tensor might already deleted comp_tensor = getattr V graph module node target isinstance comp_tensor torch Tensor share_storage candidate_tensor comp_tensor candidate_tensor_users += node reversed V graph graph nodes The get_attr node has only user fx node The candidate tensor has been used only get_attr node node op == get_attr node target == candidate_node get_name len node users == candidate_tensor_users == del V graph constants node target delattr V graph module node target delattr V graph graph owning_module node target counters inductor select_algorithm_weight_prune += gen_ d_view_of_epilogue_buf Y ir Buffer template_buffer ir Buffer epilogue_nodes list ir IRNode reindexers list Optional Callable list Any list Any default_reindexers list Optional Callable list Any list Any - tuple Union ir Buffer ir ReinterpretView list Optional Callable list Any list Any The dimension indexing could different between GEMM output i e ` template_buffer ` which D MxN output template after epilogues i e ` Y ` In GEMM template code we aware dimension indexing epilogues always work D tiles according indexing GEMM output In function we D buffer ` Y_ d ` according GEMM output reinterpreted ` Y ` needed build reindexer converts indexing ` Y ` into ` Y_ d ` Y_ d Union ir Buffer ir ReinterpretView = Y Y get_size == template_buffer get_size Y get_stride == template_buffer get_stride reindexers extend default_reindexers Y_ d = Y get_reindexer epilogue_node default_reindexer=None From template_buffer epilogue_node_ordered ordered stride decreasingly dense format example template_buffer size stride epilogue_node_ordered ordered stride decreasingly dense format size stride stride_order = list ir get_stride_order V graph sizevars size_hints epilogue_node get_stride fill_order = ir stride_order fill_order stride_order reversed_fill_order = list reversed fill_order size_with_stride_ordered_decreasingly = epilogue_node get_size i i reversed_fill_order reshape_reindex = ir View dynamic_reshape_indexer size_with_stride_ordered_decreasingly template_buffer get_size default_reindexer reshape_reindex = ir fuse_reindexing reshape_reindex default_reindexer From epilogue_node_ordered ordered stride decreasingly dense format epilogue_node example epilogue_node_ordered ordered stride decreasingly dense format size stride epilogue_node size stride from_stride_ordered_decreasingly_to_epilogue_node_order = len stride_order - - stride_order i i range len stride_order stride_reindex = ir same_reorder from_stride_ordered_decreasingly_to_epilogue_node_order reindexer = ir fuse_reindexing stride_reindex reshape_reindex type ignore var-annotated reindexer default_reindexers None default_reindexers = None len epilogue_nodes new_reindexers = get_reindexer epilogue_node default_reindexer epilogue_node default_reindexer zip epilogue_nodes default_reindexers reindexers extend new_reindexers isinstance Y ir BaseView storage = ir StorageBox Y unwrap_view assert isinstance Y ir Buffer storage = ir StorageBox Y Y_ d = ir ReinterpretView data=storage layout=template_buffer get_layout Y_ d reindexers CppGemmTemplate CppTemplate GEMM Template Inductor CPP Backend __init__ input_nodes layout ir Layout num_threads int register_blocking GemmBlocking beta= alpha= has_bias=False epilogue_creator Optional Callable ir Buffer ir Pointwise = None should_block_weights bool = True name= packed_gemm - None assert layout dtype torch float torch bfloat torch half torch uint super __init__ name input_nodes layout num_threads epilogue_creator=epilogue_creator beta = beta alpha = alpha has_bias = has_bias register_blocking = register_blocking m n = layout size - k = input_nodes get_size - m n k = m n k padded_n = get_padded_n n register_blocking block_n is_dynamic_M = has_free_symbols m should_block_weights = should_block_weights thread_blocking = make_thread_blocking_cache cache_blocking = make_cache_blocking_cache make_thread_blocking_cache cache = lru_cache _thread_blocking thread_blocking num_threads int - GemmBlocking cache num_threads thread_blocking _thread_blocking num_threads int - GemmBlocking NOTE Thread blocking Cpp GEMM We use simple heuristics decide thread blocking Make sure all threads occupied much possible For m n blocks favor more square-sized thread blocks better data reuse If m n blocks cannot occupy all threads we consider k-slicing TODO jgong allow tuning various blocking options get_factors number factors = i range int number - number i == factors append number i factors append i factors get_blocking m_factor n_factor k_factor m_blocks n_blocks k_blocks thread_block_k = math ceil k_blocks k_factor thread_block_n = math ceil n_blocks n_factor thread_block_m = math ceil m_blocks m_factor GemmBlocking thread_block_m thread_block_n thread_block_k assert is_dynamic_M Unable determine thread blocking dynamic M register_blocking = register_blocking m_blocks = math ceil m register_blocking block_m n_blocks = math ceil n register_blocking block_n k_blocks = math ceil k register_blocking block_k factors = get_factors num_threads assert len factors config cpp gemm_thread_factors None factors = int i i config cpp gemm_thread_factors split assert len factors == assert math prod factors == num_threads get_blocking factors factors factors m_blocks n_blocks k_blocks we favor square-sized thread blocks good data reuse get_better_blocking blocking best_blocking best_blocking None best_blocking = blocking block_m_size = blocking block_m register_blocking block_m block_n_size = blocking block_n register_blocking block_n best_block_m_size = best_blocking block_m register_blocking block_m best_block_n_size = best_blocking block_n register_blocking block_n blocking block_k best_blocking block_k best_blocking = blocking blocking block_k == best_blocking block_k block_m_size + block_n_size best_block_m_size + best_block_n_size best_blocking = blocking best_blocking best_blocking = None check we can have thread-blocking occupy all threads without k-slicing n_factor factors m_factor = num_threads n_factor n_blocks = n_factor m_blocks = m_factor blocking = get_blocking m_factor n_factor m_blocks n_blocks k_blocks best_blocking = get_better_blocking blocking best_blocking best_blocking None k_factor factors k_blocks = k_factor config cpp gemm_max_k_slices == k_factor = config cpp gemm_max_k_slices n_factors = get_factors num_threads k_factor n_factor n_factors m_factor = num_threads k_factor n_factor n_blocks = n_factor m_blocks = m_factor blocking = get_blocking m_factor n_factor k_factor m_blocks n_blocks k_blocks best_blocking = get_better_blocking blocking best_blocking best_blocking None n_factor factors m_factor = num_threads n_factor n_blocks = n_factor m_blocks = m_factor blocking = get_blocking m_factor n_factor m_blocks n_blocks k_blocks best_blocking = get_better_blocking blocking best_blocking assert best_blocking None best_blocking make_cache_blocking_cache cache = lru_cache _cache_blocking cache_blocking num_threads int - GemmBlocking cache num_threads cache_blocking _cache_blocking num_threads int - GemmBlocking get_cache_blocking register_blocking thread_blocking Mr = register_blocking block_m Nr = register_blocking block_n Kr = register_blocking block_k Mt_blocks = thread_blocking block_m Nt_blocks = thread_blocking block_n Kt_blocks = thread_blocking block_k config cpp gemm_cache_blocking None blockings = int i i config cpp gemm_cache_blocking split assert len blockings == Mc_blocks Nc_blocks Kc_blocks = blockings min Mc_blocks Mt_blocks min Nc_blocks Nt_blocks min Kc_blocks Kt_blocks The ratios below empirically determined decide effective sizes L L TODO tune factor here L _limit_factor = L _limit_factor = L _cache_size = torch _C _cpu _L d_cache_size per core cache size Bytes assert L _cache_size f Expect L _cache_size got L _cache_size L = L _cache_size L _limit_factor L _cache_size = torch _C _cpu _L _cache_size per core cache size Bytes assert L _cache_size f Expect L _cache_size got L _cache_size L = L _cache_size L _limit_factor get_num_byte dtype torch tensor dtype=dtype element_size dtype_A = input_nodes get_dtype dtype_B = input_nodes get_dtype num_byte_A = get_num_byte dtype_A num_byte_B = get_num_byte dtype_B dtype_A torch bfloat dtype_B torch int Kr = We will cache dequantized weights BF L D AMX micro-kernel In case choice micro-kernel being used can t decoupled cache blocking TODO Decouple choice micro-kernel cache blocking num_byte_B = num_byte_A NOTE CPP GEMM Cache Blocking Algorithm Our overall strategy Make cache blocks B L -reside reused multiple rows A i e Mc Here B Kc x Nr where Nr single register block We use L size decide Kc We want make Mc large enough better reuse B Make cache blocks A L -reside which would limit Mc We want reuse A along N where we have two sub-strategies see notes below decide Mc Nc Step Decide Kc assuming B block L -reside size_cache_B = Kr Kt_blocks Nr num_byte_B Kc_blocks = Kt_blocks size_cache_B L Kc_blocks = math floor L Kr Nr num_byte_B config cpp use_small_dequant_buffer dtype_A torch bfloat Mt_blocks == dtype_B torch uint A W Make small dequant_B buffer woq int q_group_size Nr Since when Mt_blocks == L -reside B block can t reused A Kc_blocks Kr = q_group_size Kc_blocks = q_group_size Kr dtype_B torch int A W Make A B C buffer L A_buf_size_div_K = m num_byte_A B_buf_size_div_K = Nr num_byte_B assume acc float int Mc_blocks = Nc_blocks = C_buf_size = Mr Nr K_block_size = L - C_buf_size A_buf_size_div_K + B_buf_size_div_K Kc_blocks Kr = K_block_size Kc_blocks = K_block_size + Kr - Kr Step Decide Mc assuming A block L -reside min_Mc_ratio = TODO jgong something tune min_Mc_blocks = math ceil min_Mc_ratio Mr Nr assert min_Mc_blocks = Kt_bytes = Kt_blocks Kr num_byte_A min_Mc_blocks Mr Kt_bytes L Strategy A Mc x Kt resides L reused all Nt when Nc_blocks kept Mc should large enough = min_Mc_blocks reuse B Kc x Nr L This makes C Mc x Nr small enough reside L Mc_blocks = min Mt_blocks math floor L Mr Kt_bytes Nc_blocks = Strategy Kt too large hold A Mc x Kt L we reuse A Mc x Kc L B Kc x Nc C Mc x Nc resides L Mc_blocks = Mt_blocks Nc_blocks = min math ceil Mc_blocks Mr Nr Nt_blocks Nc_bytes = Nc_blocks Nr assume C acc float int Kc_bytes = Kc_blocks Kr num_byte_A Mc_blocks Mr Kc_bytes + Nc_bytes L The following solution Mc Nc + Mc Kc_bytes = L assuming Mc == Nc good data reuse M_max = math sqrt Kc_bytes Kc_bytes + L - Kc_bytes M_max Mc_blocks Mr Mc_blocks = math floor M_max Mr Nc_blocks = min math ceil Mc_blocks Mr Nr Nt_blocks Mc_blocks Nc_blocks Kc_blocks assert is_dynamic_M Unable determine cache blocking dynamic M register_blocking = register_blocking thread_blocking = thread_blocking num_threads GemmBlocking get_cache_blocking register_blocking thread_blocking log_blockings log debug f Register blocking register_blocking noqa G is_dynamic_M thread cache blockings determined runtime dynamic shapes log debug f Cache blocking cache_blocking num_threads noqa G thread_blocking = thread_blocking num_threads log debug f Thread blocking thread_blocking noqa G get_occupancy m_blocks = math ceil m register_blocking block_m n_blocks = math ceil n register_blocking block_n k_blocks = math ceil k register_blocking block_k m = math ceil m_blocks thread_blocking block_m n = math ceil n_blocks thread_blocking block_n k = math ceil k_blocks thread_blocking block_k m n k log debug f Number threads num_threads occupancy get_occupancy noqa G maybe_k_slicing num_threads == False is_dynamic_M TODO jgong perhaps use size hint decide True register_blocking = register_blocking k_blocks = math ceil k register_blocking block_k thread_blocking = thread_blocking num_threads k_blocks thread_blocking block_k classmethod add_choices cls choices layout input_nodes beta= alpha= has_bias=False trans_w=False input_indices=None epilogue_creator Optional Callable ir Buffer ir Pointwise = None act_mapping Optional dict int ir IRNode = None Add choices GEMM template Fast path save epilogue calculation when x_scale x_zp w_scale constant use_int _fast_compensation_path = _is_int _gemm input_nodes all isinstance input_nodes idx ir TensorBox isinstance input_nodes idx data data ir ConstantBuffer idx input_indices None input_indices = list range len input_nodes reorder_and_filter inputs layout_or_out has_bias assert len input_indices = Assume input order inp x w we reorder x w inp inp_idx = input_indices x_idx = input_indices w_idx = input_indices inputs x_idx inputs w_idx inputs inp_idx inputs idx idx input_indices layout_or_out len inputs = len input_indices assert len input_indices = inputs idx idx input_indices layout_or_out For when input used x w i e X X T similar Assumes first input only input assert len inputs == inputs len input_indices layout_or_out new_inputs new_layout = reorder_and_filter input_nodes layout is_mkldnn_wgt = new_inputs get_name V graph constants V graph constants new_inputs get_name is_mkldnn is_mkldnn_wgt It shouldn t happen viewing mkldnn tensor we can extend implementation does assert isinstance new_inputs ir BaseView Note layout MKLDNN Tensor wrong stride view_size = new_inputs layout size view_stride = new_inputs layout stride view_offset = new_inputs layout offset maybe_to_dense inputs layout_or_out new_inputs = list inputs isinstance inputs torch Tensor W = inputs new_inputs = W to_dense W is_mkldnn W new_inputs layout_or_out normalize_shapes inputs layout_or_out new_inputs = list inputs is_mkldnn_wgt isinstance new_inputs torch Tensor has_free_symbols view_size If batch size B dynamic we need set batch size possibly stride assert has_free_symbols view_size view_size = V graph sizevars size_hints view_size view_stride = V graph sizevars size_hints view_stride With assumptation W storage unwrap view thus view back here new_inputs = new_inputs as_strided view_size view_stride view_offset trans_w new_inputs layout_or_out X = new_inputs W = new_inputs B = new_inputs has_bias None W = transpose_w W trans_w B = expand_bias B X type ignore arg-type new_inputs = W B None new_inputs = B new_inputs layout_or_out TODO jgong decide proper number threads per problem size num_threads = parallel_num_threads new_inputs _ = normalize_shapes maybe_to_dense new_inputs new_layout m n k _ = mm_args new_inputs new_inputs mat _transposed=cls is_woq_int use_ x _dim=cls is_woq_int output_dtype compute_dtype = get_gemm_template_output_and_compute_dtype new_inputs get_dtype micro_gemm = create_micro_gemm micro_gemm m n k input_dtype=new_inputs get_dtype input _dtype=new_inputs get_dtype output_dtype=output_dtype compute_dtype=compute_dtype alpha=alpha num_threads=num_threads use_ref=not cls is_woq_int q_group_size=cls q_group_size assert micro_gemm None pre_block_weights = cls check_if_block_weight new_inputs micro_gemm micro_gemm use_local_vnni_blocking pre_block_weights only_one_input = input_nodes == input_nodes len input_nodes False pre_block_weights If weights blocked use second input preprocessor inputs layout new_inputs new_layout = normalize_shapes maybe_to_dense reorder_and_filter inputs layout only_one_input isinstance new_inputs torch Tensor new_inputs new_layout cls prep_weight new_inputs new_layout pyrefly ignore bad-argument-type micro_gemm pre_block_weights use_int _fast_compensation_path postprocessor output isinstance output ir TensorBox prepack weight input template buffer template_buffer = ir InputsKernel unwrap_storage_for_input output assert isinstance template_buffer ir CppTemplateBuffer new_input_nodes _ = reorder_and_filter input_nodes layout W_node = new_input_nodes W_node get_name V graph constants output W = V graph constants W_node get_name new_input_nodes = W new_input_nodes new_layout = normalize_shapes maybe_to_dense new_input_nodes layout new_input_nodes _ = cls prep_weight new_input_nodes new_layout pyrefly ignore bad-argument-type micro_gemm pre_block_weights use_int _fast_compensation_path skip_int _compensation=True W_packed = new_input_nodes W_packed_constant = V graph add_tensor_constant W_packed new_input_nodes = W_packed_constant Prune unused tensors prune_tensors input_nodes new_input_nodes template_buffer inputs = ir InputsKernel unwrap_storage_for_input W_packed_constant output template = DataProcessorTemplateWrapper cls preprocessor postprocessor input_nodes=input_nodes layout=layout num_threads=num_threads register_blocking=micro_gemm register_blocking beta=beta alpha=alpha has_bias=has_bias epilogue_creator=epilogue_creator should_block_weights=pre_block_weights name=micro_gemm __class__ __name__ template maybe_append_choice choices template staticmethod get_padded_size n block_n k should_block_weight padded_n = get_padded_n n block_n We assume all GEMM weight tensors should blocked padded new_size = padded_n block_n k block_n new_size padded_n staticmethod _maybe_remove_storage_offset node ir IRNode node get_layout offset == node node may contiguous still have non-zero storage offset GEMM_TEMPLATE emits code like W data_ptr node offset + runtime W data_ptr after normalize_shapes already includes offset To avoid double-offsetting we remove offset node also generated code W data_ptr ir ExternKernel copy_input node classmethod prep_weight cls inputs layout ir Layout micro_gemm CppMicroGemm should_block_weight bool use_int _fast_compensation_path bool = False skip_int _compensation bool = False NOTE Weight prep consists separate steps Blocking weight tensor into D shape n block_n k block_n This always done weight tensor constant i e all GEMM some BMM For BMM we also block non-contiguous weight tensors since they would reshaped anyway This assumes blocked contiguous weights will more efficient GEMM kernel worth overhead reshape blocking This blocking includes additional padding when n multiple block_n This padding allows more efficient microkernel implementation For BMM only done reshape would happen anyway i e weight tensor constant contiguous using AMX VNNI layout Packing weight tensor into VNNI-friendly shape For constant input done same time weight blocking At compile time constant weight tensors blocked packed For non-constant tensors e g BMM which will blocked non-contiguous VNNI-layout tensors weight tensor blocked packed runtime CppBmmTemplate overrides methods get_padded_size block_weight order accommodate additional dimension batch size determine weight tensor should blocked W = inputs new_inputs = list inputs cls is_woq_int assert len W get_size == isinstance W ir IRNode len W shape == n k = W get_size isinstance W ir IRNode W shape k n = W get_size - isinstance W ir IRNode W shape - _ block_n _ = micro_gemm register_blocking new_size padded_n = cls get_padded_size n block_n k should_block_weight padding = padded_n - n should_block_weight cls is_woq_int blocked_w = cls block_weight W new_size padding new_inputs = cls pack_vnni_weight blocked_w micro_gemm new_size should_block_weight assert cls is_woq_int new_inputs = cls block_weight W new_size padding isinstance W ir IRNode Require W layout fixed contiguous happens inplace ir ExternKernel require_contiguous W new_inputs = cls _maybe_remove_storage_offset W skip_int _compensation _is_int _gemm new_inputs BCompensate = None x_w_scale = None _get_compensation_node W use_int _fast_compensation_path BCompensate = V graph add_tensor_constant V graph constants W get_name + _BMatrixCompens W get_name + _BMatrixCompens x_w_scale = None use_int _fast_compensation_path x_w_scale = V graph add_tensor_constant V graph constants W get_name + _x_w_compens W get_name + _x_w_compens BCompensate x_w_scale use_int _fast_compensation_path new_inputs has been reordered x w optional bias x_scale x_zp w_scale w_zp x_scale = new_inputs - x_zp = new_inputs - w_scale = new_inputs - isinstance W ir IRNode BCompensate x_w_scale = _get_compensation_node W use_int _fast_compensation_path Use original W blocked_w new_inputs calculate BCompensate BCompensate = torch sum W to_dense torch float dim= type ignore assignment assert all isinstance item torch Tensor item x_scale x_zp w_scale BCompensate = BCompensate x_scale w_scale x_zp x_w_scale = x_scale w_scale new_inputs append BCompensate new_inputs append x_w_scale isinstance W ir IRNode BCompensate _ = _get_compensation_node W use_int _fast_compensation_path Use original W blocked_w new_inputs calculate BCompensate BCompensate = torch sum W to_dense torch float dim= type ignore assignment new_inputs append BCompensate new_inputs layout staticmethod check_if_block_weight W micro_gemm True classmethod block_weight cls W new_size padding These separated into two methods allow subclasses override them separately isinstance W ir IRNode W get_name V graph constants Create new buffer representing constant blocked tensor blocked_w = ir Buffer name=W get_name Borrow registered buffer name layout=ir FixedLayout W get_device_or_error W get_dtype new_size ir FlexibleLayout contiguous_strides new_size isinstance W ir TensorBox W = ir TensorBox W permute_dims = list range len new_size permute_dims - permute_dims - = permute_dims - permute_dims - permute_size = list new_size permute_size - permute_size - = permute_size - permute_size - blocked_w = L constant_pad_nd W padding blocked_w = L permute L view blocked_w permute_size type ignore arg-type permute_dims assert isinstance W torch Tensor Pad weight tensor reshape into D blocked shape blocked_size = list new_size blocked_size - blocked_size - = blocked_size - blocked_size - blocked_w = torch nn functional pad W padding type ignore assignment reshape blocked_size transpose - - contiguous blocked_w classmethod pack_vnni_weight cls W micro_gemm new_size WOQ INT weights reordered microkernel so do pack them here should_pack = micro_gemm get_b_layout = LayoutType NORMAL micro_gemm is_woq_int These separated into two methods allow subclasses override them separately isinstance W ir IRNode isinstance W ir Buffer W get_name V graph constants W k = new_size - isinstance W ir TensorBox W = ir TensorBox W should_pack permute_dims = list range len new_size + permute_dims - permute_dims - = permute_dims - permute_dims - vnni_size = micro_gemm get_b_layout == LayoutType VNNI vnni_view_size = list new_size vnni_view_size - = k vnni_size vnni_view_size insert - vnni_size W = L view L permute L view W vnni_view_size permute_dims new_size W = ir ExternKernel realize_input W W = ir ExternKernel require_contiguous W W k = new_size - Apply VNNI packing weight tensor should_pack TODO Move VNNI weight packing non-constant tensors into template improve cache locality avoid full-tensor copy layout_str = VNNI micro_gemm get_b_layout == LayoutType VNNI VNNI assert micro_gemm get_b_layout LayoutType VNNI LayoutType VNNI f We only support layout_str now vnni_size = micro_gemm get_b_layout == LayoutType VNNI assert k vnni_size == f k should divisible vnni_size layout_str layout vnni_view_size = list new_size vnni_view_size - = k vnni_size vnni_view_size insert - vnni_size W = W view vnni_view_size transpose - - contiguous view new_size normalize stride contiguous_strides per size avoids problems L view during template codegen new_stride = sz reversed W shape new_stride insert new_stride sz W = W as_strided W shape new_stride W get_default_reindexers epilogue_nodes None len epilogue_nodes get_options kernel CppTemplateKernel template_buffer_node Optional ir CppTemplateBuffer = None flag_template_buffer_has_other_users Optional bool = None epilogue_nodes Optional list ir IRNode = None - dict str Any assert len input_nodes = int _gemm = input_nodes get_dtype torch uint torch int x_scale = None x_zp = None w_scale = None w_zp = None inp = None q_group_size_node = None qscale_and_zeros = None int _gemm X W = input_nodes input_nodes bias_idx = has_bias inp = input_nodes bias_idx has_bias None x_scale = input_nodes bias_idx + x_zp = input_nodes bias_idx + w_scale = input_nodes bias_idx + w_zp = input_nodes bias_idx + Y = output_node is_woq_int X W = input_nodes input_nodes Y = output_node q_group_size_node = input_nodes qscale_and_zeros = input_nodes X W = input_nodes input_nodes Y = output_node inp = input_nodes has_bias None template_buffer_has_other_users = None template_buffer_node None Use updated prepacked weight buffer W = template_buffer_node inputs Y = template_buffer_node assert flag_template_buffer_has_other_users None template_buffer_has_other_users = flag_template_buffer_has_other_users template_buffer = Y gemm_output_buffer = template_buffer epilogues list ir IRNode = reindexers list Optional Callable list Any list Any = epilogue_creators list Callable ir Buffer ir Pointwise = fake_buffers list ir Buffer = Y_aliases OrderedSet str = OrderedSet use_local_acc = layout dtype = torch float template_buffer_has_other_users int _gemm padded_n = n maybe_k_slicing epilogue_nodes epilogue_nodes - get_dtype = layout dtype TODO jgong int gemm bias-add handled outside gemm template we d better move here align fp inp None beta = int _gemm add epilogue bias add _bias_add_epilogue buf create_epilogue_with_attr buf bias_add other=inp beta=self beta dtype=self layout dtype epilogue_creators append _bias_add_epilogue epilogue_creator None epilogue_creators append epilogue_creator When GEMM output buffer localized has users other than epilogue nodes we need copy value GEMM output local buffer global buffer need_copy_from_local_to_global_buffer_epilogue use_local_acc template_buffer_has_other_users epilogue_creators The GEMM output buffer global buffer thus copy needed use_local_acc False The possible value template_buffer_has_other_users None False True It None when generating gemm template during autotune will have value during scheduler codegen extra copy_from_local_to_global_buffer_epilogue needed either below two cases template_buffer_has_other_users None i e when doing codegen during autotune template_buffer_has_other_users False which means s safe keep value GEMM output buffer local buffer only no users outside epilogues will use its value template_buffer_has_other_users False When bias None epilogue_creator None there will epilogue_creators after GEMM The GEMM output buffer localized while output buffer epilogue_creators global buffer epilogue_creators False True need_copy_from_local_to_global_buffer_epilogue use_local_acc template_buffer_has_other_users epilogue_creators copy_from_local_to_global_buffer_epilogue input_buffer ir Buffer dtype = layout dtype input_loader = input_buffer make_loader copy_inner index input = input_loader index result = ops to_dtype input dtype result ir Pointwise device=input_buffer get_device_or_error dtype=self layout dtype inner_fn=copy_inner ranges=input_buffer get_size epilogue_creators append copy_from_local_to_global_buffer_epilogue NOTE How CPP GEMM template epilogues organized gemm_output_buffer -- zero more in-template epilogues created ` epilogue_creators ` -- template_buffer -- zero more out-of-template epilogues ` epilogue_nodes ` -- Y epilogue_creators assert isinstance template_buffer ir IRNode gemm_output_name = f template_buffer get_name _GemmOut gemm_output_buffer = ir Buffer name=gemm_output_name pyrefly ignore missing-attribute layout=template_buffer layout current_input_buffer = gemm_output_buffer i creator enumerate epilogue_creators i == len epilogue_creators - buffer_name = template_buffer get_name buffer_name = f gemm_output_name _epilogue_ i epilogues append ir ComputedBuffer name=buffer_name pyrefly ignore missing-attribute layout=template_buffer layout data=creator current_input_buffer fake_buffers append current_input_buffer Y_aliases add current_input_buffer get_name reindexers append None i len epilogue_creators - current_input_buffer = ir Buffer name=buffer_name pyrefly ignore missing-attribute layout=template_buffer layout assert isinstance Y ir Buffer ir ReinterpretView Y_ d Union ir Buffer ir ReinterpretView = Y epilogue_nodes template_buffer_has_other_users assert isinstance template_buffer ir IRNode Y_aliases add template_buffer get_name epilogues extend epilogue_nodes assert Y get_numel == epilogues - get_numel Y = cast ir Buffer epilogues - assert isinstance template_buffer ir Buffer Y_ d reindexers = gen_ d_view_of_epilogue_buf Y template_buffer epilogue_nodes reindexers default_reindexers=self get_default_reindexers epilogue_nodes output_dtype compute_dtype = get_gemm_template_output_and_compute_dtype X get_dtype micro_gemm = create_micro_gemm f kernel kernel_name _micro_gemm m n k input_dtype=X get_dtype pyrefly ignore missing-attribute input _dtype=W get_dtype output_dtype=output_dtype compute_dtype=compute_dtype alpha=self alpha num_threads=self num_threads use_ref=not is_woq_int q_group_size=self q_group_size assert micro_gemm None micro_gemm use_local_vnni_blocking should_block_weights assert register_blocking == micro_gemm register_blocking log_blockings isinstance micro_gemm CppMicroGemmAMX counters inductor cpp_micro_gemm_amx_counter += isinstance micro_gemm CppMicroBrgemm counters inductor cpp_micro_brgemm_counter += L _cache_size = torch _C _cpu _L d_cache_size per core cache size Bytes assert L _cache_size f Expect L _cache_size got L _cache_size L _cache_size = torch _C _cpu _L _cache_size per core cache size Bytes assert L _cache_size f Expect L _cache_size got L _cache_size options = dict X=X W=W inp=inp Y=Y N=self n K=self k PADDED_N=self padded_n GemmOut=gemm_output_buffer aliases= alias Y get_name alias Y_aliases beta=self beta alpha=self alpha num_threads=self num_threads micro_gemm=micro_gemm is_dynamic_M=self is_dynamic_M template=self kernel=kernel export_declaration=get_export_declaration epilogue_nodes=epilogues reindexers=reindexers Y_ d=Y_ d use_local_acc=use_local_acc maybe_k_slicing=self maybe_k_slicing x_scale=x_scale x_zp=x_zp w_scale=w_scale w_zp=w_zp acc_buf_dtype=torch int int _gemm torch float DTYPE_TO_CPP=DTYPE_TO_CPP L _cache_size=L _cache_size L _cache_size=L _cache_size config=config fake_buffers=fake_buffers is_woq_int =self is_woq_int q_group_size=q_group_size_node qscale_and_zeros=qscale_and_zeros options is_int _woq_gemm_small_m_dim X ir ReinterpretView W ir ReinterpretView N K micro_gemm Use SMALL_M_GEMM_TEMPLATE isinstance micro_gemm CppMicroGemmFP Vec is_int _woq_gemm_small_m_dim_corner_case micro_gemm X get_size N K X get_dtype torch bfloat W get_dtype torch int render type ignore override kernel CppTemplateKernel template_buffer_node Optional ir CppTemplateBuffer = None flag_template_buffer_has_other_users Optional bool = None epilogue_nodes Optional list ir IRNode = None kwargs - str options = get_options kernel=kernel template_buffer_node=template_buffer_node flag_template_buffer_has_other_users=flag_template_buffer_has_other_users epilogue_nodes=epilogue_nodes render_options = options contextlib ExitStack stack buf options fake_buffers stack enter_context patch object V graph get_dtype _fake_get_dtype buf options is_dynamic_M is_int _woq_gemm_small_m_dim options X options W options N options K options micro_gemm template_str = SMALL_M_GEMM_TEMPLATE template_str = GEMM_TEMPLATE _template_from_string template_str render options codegen_blocks num_threads N K micro_gemm is_dynamic_M kernel GemmOut config L _cache_size L _cache_size X W options = dict num_threads=num_threads N=N K=K micro_gemm=micro_gemm is_dynamic_M=is_dynamic_M kernel=kernel GemmOut=GemmOut config=config L _cache_size=L _cache_size L _cache_size=L _cache_size template=self X=X W=W is_woq_int =self is_woq_int template_str = GEMM_TEMPLATE_INIT_BLOCKING_BASIC_BLOCK is_dynamic_M is_int _woq_gemm_small_m_dim X W N K micro_gemm template_str += GEMM_TEMPLATE_INIT_BLOCKING_EXTENDED _template_from_string template_str render options codegen_microkernel_def _template_from_string GEMM_TEMPLATE_MICROKERNEL_DEF render render_options codegen_gemm_stub_def microkernel = codegen_microkernel_def microkernel + _template_from_string GEMM_TEMPLATE_STUB_DEF render render_options codegen_multi_threads_params _template_from_string GEMM_TEMPLATE_MULTI_THREADS_PARAMS render codegen_single_thread_params is_dynamic_M options = dict is_dynamic_M=is_dynamic_M _template_from_string GEMM_TEMPLATE_SINGLE_THREAD_PARAMS render options codegen_m_loop_params _template_from_string GEMM_TEMPLATE_M_LOOP_PARAMS render codegen_n_loop_params _template_from_string GEMM_TEMPLATE_N_LOOP_PARAMS render classmethod is_woq_int cls False classmethod q_group_size cls None CppWoqInt GemmTemplateMeta type __getitem__ cls q_group_size CppWoqInt GemmTemplateInstance CppGemmTemplate __init__ args kwargs - None super __init__ args kwargs classmethod is_woq_int cls True classmethod q_group_size cls q_group_size staticmethod check_if_block_weight W micro_gemm For WOQ INT weight already packed However AMX microkernel we want change blocking weight cpp_micro_gemm CppMicroGemmWoQInt Amx isinstance micro_gemm CppMicroGemmWoQInt Amx classmethod block_weight cls W new_size padding This method called only AMX microkernels used In case we unpack repack weight so block_n= format packed weight described here https github com pytorch pytorch blob eee ed d f fbbcb c b b c c c c aten src ATen native cpu int mm_kernel cpp#L isinstance W ir IRNode case we do nothing ir ExternKernel require_contiguous W blocked_w = W case we unpack repack weight assert isinstance W torch Tensor assert W dim == N = W size K = W size - G = cls q_group_size x qscales_and_zeros bfloat instead float use optimized kernel so unpacking process faster x = torch eye K bfloat Here we use scale= qzero= because we want unpack weight without dequantizing The qzero here instead because int values converted - _weight_int pack_mm_for_cpu kernel https github com pytorch pytorch blob eee ed d f fbbcb c b b c c c c aten src ATen native cpu int mm_kernel cpp#L qscales_and_zeros = torch tensor bfloat expand K G N contiguous shape K N unpacked_w = torch ops aten _weight_int pack_mm_for_cpu x W G qscales_and_zeros torch uint block_n = shape N block_n K block_n w_blocked = unpacked_w view K N block_n block_n permute contiguous pack int - int block_n b b b - xf &#124; b xf &#124; b shape N block_n K block_n w_blocked = w_blocked view N block_n K block_n shape N block_n K block_n w_blocked_packed = w_blocked xF &#124; w_blocked shape N K blocked_w = w_blocked_packed view N K blocked_w CppWoqInt GemmTemplateInstance CppWoqInt GemmTemplate metaclass=CppWoqInt GemmTemplateMeta pass