Provides functionality compiling PyTorch s autograd automatic differentiation system This module implements compiled autograd which traces optimizes backward pass computations runtime The key components - AutogradCompilerInstance Traces compiles autograd graphs using FX - Context managers _enable _disable Control when compiled autograd active - Utility functions Support graph manipulation tensor operations hooks Compiled autograd can significantly improve backward pass performance removing Python overhead enabling additional optimizations It works capturing backward computations into FX graph can compiled optimized while maintaining same semantics eager mode autograd contextlib functools itertools operator time collections Counter defaultdict collections abc Callable Generator Sequence typing Any Optional TYPE_CHECKING Union torch torch utils _pytree pytree torch _dispatch python enable_python_dispatcher torch _dynamo external_utils call_accumulate_grad call_backward call_hook FakeCompiledAutogradEngine unwrap_maybe_dynamic_int torch _dynamo source GetItemSource LocalSource torch _dynamo utils counters get_chromium_event_logger lazy_format_graph_code set_locals_to_steal torch _functorch _aot_autograd runtime_wrappers AutogradLazyBackwardCompileInfo CachedAutogradLazyBackwardCompileInfo torch _guards compile_context CompileContext CompileId Source torch _logging getArtifactLogger trace_structured torch _prims_common clone_preserve_strides torch _subclasses FakeTensorMode torch _subclasses fake_tensor FakeTensor torch fx GraphModule torch fx experimental _backward_state BackwardState torch fx experimental proxy_tensor decompose disable_autocast_cache disable_proxy_modes_tracing fetch_object_proxy ProxyTorchDispatchMode PythonKeyTracer track_tensor_tree torch fx experimental symbolic_shapes DimDynamic ShapeEnv torch fx traceback preserve_node_meta set_stack_trace torch types FloatLikeType IntLikeType torch utils _ordered_set OrderedSet torch utils _traceback CapturedTraceback TYPE_CHECKING torch fx proxy Proxy TURN_OFF_MSG = You can turn off compiled autograd either Moving unsupported autograd call outside torch compile d region Wrapping unsupported autograd call torch _dynamo compiled_autograd _disable context manager Setting torch _dynamo config compiled_autograd=False torch compile call containing unsupported autograd call Setting torch _dynamo config compiled_autograd=False start program compiled_autograd_log = getArtifactLogger __name__ compiled_autograd verbose_log = getArtifactLogger __name__ compiled_autograd_verbose snapshot_verbose_logging_enabled - bool torch _logging _internal log_state is_artifact_enabled compiled_autograd_verbose snapshot_cudagraph_enabled - bool torch _inductor config triton cudagraphs maybe_clone x Optional torch Tensor - Optional torch Tensor x None clone_preserve_strides x x extract_bw_module CompiledFunction Any - Callable Any isinstance CompiledFunction _lazy_backward_info AutogradLazyBackwardCompileInfo CompiledFunction _lazy_backward_info bw_module isinstance CompiledFunction _lazy_backward_info CachedAutogradLazyBackwardCompileInfo torch _subclasses fake_tensor unset_fake_temporarily CompiledFunction _lazy_backward_info bw_module_fn raise AssertionError Unexpected Lazy Backward Compilation Info Type Please file issue Note Anomaly Mode Semantics Compiled Autograd In eager autograd engine anomaly mode able detect NaNs after each node This useful because executed code without anomaly mode same So assuming determinism NaN regular mode should also happen anomaly mode With torch compile following eager semantics would require inserting runtime asserts check NaNs which could prevent some fusions This results different code being run without anomaly mode So different semantics needed implementation below will check NaNs end autograd call instead after each node NaNChecker __init__ accumulate_grad bool - None accumulate_grad = accumulate_grad params_indices list int = params_to_check dict str torch Tensor = output_names list str = prep_with_graph graph torch fx Graph - None inputs_node = next iter graph nodes acc_grad_nodes = graph find_nodes op= call_function target=call_accumulate_grad output_nodes = graph find_nodes op= output args assert accumulate_grad == bool acc_grad_nodes accumulate_grad == output_nodes node acc_grad_nodes param_node = node args AccumulateGrad always saves reference param so Compiled Autograd will always lift param should always true assert param_node target operator getitem param_node args inputs_node type ignore possibly-undefined isinstance param_node args int params_indices append param_node args output_names = node name node output_nodes prep_with_inputs inputs tuple torch Tensor - None accumulate_grad Using grad nothing prep Using backward we must check existing grads params any idx params_indices grad = inputs idx grad grad None assert torch isnan grad any f Compiled autograd running under anomaly mode inputs idx already f having NaN gradient This supported TURN_OFF_MSG params_to_check f inputs idx = inputs idx check out tuple torch Tensor - None accumulate_grad Using backward graph outputs empty assert out nan_params list str = inputs_str param params_to_check items assert param grad None true autograd grad torch isnan param grad any nan_params append inputs_str nan_params raise RuntimeError f Compiled Autograd returned NaN gradients parameters join nan_params Using grad graph outputs grads nan_grads list str = i grad enumerate out torch isnan grad any nan_grads append output_names i nan_grads raise RuntimeError f Compiled Autograd returned NaN gradients output nodes join nan_grads We lazily bind functional backward variants PyTorch built-in autograd nodes Example torch _dynamo compiled_autograd ops MulBackward Each functional backward bound first time node s apply_with_saved function called It s possible avoid lazy binding instead bind all upfront perhaps time via codegen changes OpNamespace __init__ - None custom_function_name_counter Counter str = Counter add name str fn Callable Any is_custom_function bool is_traceable bool - str is_custom_function name = CppNode + name count = custom_function_name_counter name custom_function_name_counter name += name = f name count assert hasattr name result = Op name fn is_custom_function is_traceable setattr name torch _dynamo allow_in_graph result C++ autograd function marked traceable Dynamo can t dry run compile time so must fallback eager torch _dynamo disable type ignore misc run_non_traceable_cpp_in_eager args Any kwargs Any - Any result args kwargs setattr name run_non_traceable_cpp_in_eager name get name str - Any getattr name Op __init__ name str fn Callable Any is_custom_function bool - None fn = fn is_custom_function = is_custom_function __name__ = name __module__ = torch _dynamo compiled_autograd ops __call__ args Any kwargs Any - Any fn args kwargs __repr__ - str __module__ + + __name__ ops = OpNamespace _graph_placeholders = inputs sizes scalars hooks packed_data _impure_targets = OrderedSet call_hook call_backward FakeCompiledAutogradEngine _exec_final_callbacks_stub call_accumulate_grad COMPILE_COUNTER = itertools count make_compile_context compiled_autograd_id int - Any compile_context CompileContext CompileId compiled_autograd_id=compiled_autograd_id frame_id=None frame_compile_id=None AutogradCompilerInstance __init__ compiler_fn Callable Any - None compiler_fn = compiler_fn stack = contextlib ExitStack close = stack close shape_env = ShapeEnv fake_tensor_mode = FakeTensorMode allow_fallback_kernels=True allow_non_fake_inputs=True shape_env=self shape_env fx_tracer = PythonKeyTracer proxy_mode = ProxyTorchDispatchMode fx_tracer symbolic hooks_proxy Optional Proxy = None wrap_fake x torch Tensor source Optional Source - FakeTensor assert isinstance x torch Tensor fake_tensor_mode from_tensor x source=source staticmethod source name str idx Any - GetItemSource GetItemSource LocalSource name idx begin_capture inputs list torch Tensor sizes list int scalars list Union int float origins list list tuple int str accumulate_grad bool check_nans bool - tuple str list torch Tensor list IntLikeType list FloatLikeType counters compiled_autograd captures += id = next COMPILE_COUNTER aot_id_counter dict int int = defaultdict int compile_context = make_compile_context id compile_context __enter__ nan_checker = NaNChecker accumulate_grad check_nans None start_time_ns = time time_ns get_chromium_event_logger log_event_start compiled_autograd start_time_ns graph_id id log_pt _compile_event=True fx_tracer root = torch nn Module fx_tracer graph = torch fx Graph tracer_cls=PythonKeyTracer fx_tracer tensor_attrs = symnode_proxy_lookup = args_proxy sizes_proxy scalars_proxy hooks_proxy packed_data_proxy = fx_tracer create_proxy placeholder name name _graph_placeholders stack enter_context preserve_node_meta inputs_origins sizes_origins scalars_origins = origins Turn PythonDispatcher during initial trace make identifiable tracing happening which needed prevent hashing symints stack enter_context enable_python_dispatcher tensor inputs fake tensors x = inputs mypy will complain about unbound x try idx x enumerate inputs inputs idx = wrap_fake x source inputs idx except Exception e raise NotImplementedError f Found tensor type type x which supported FakeTensorMode TURN_OFF_MSG e bind_objects_to_proxies inputs args_proxy inputs_origins size inputs symints sym_sizes = shape_env create_unspecified_symint_and_symbol val source sizes idx DimDynamic DYNAMIC idx val enumerate sizes We want mark every size dynamic since there s no way mark primitive ` int ` dynamic we need wrap tensor In graph we unwrap ` unwrap_maybe_dynamic_int ` back into primitive proxies = sizes_proxy i i range len sym_sizes type ignore index i symint enumerate sym_sizes proxies i = fx_tracer create_proxy call_function unwrap_maybe_dynamic_int proxies i symnode_proxy_lookup symint node = proxies i proxies = bind_objects_to_proxies sym_sizes proxies sizes_origins idx val enumerate scalars source = source scalars idx isinstance val int scalars idx = shape_env create_unspecified_symint_and_symbol val source DimDynamic DYNAMIC isinstance val float scalars idx = shape_env create_symfloatnode shape_env create_unspecified_symbol val source=source dynamic_dim=DimDynamic DYNAMIC hint=val source=source raise AssertionError Unexpected scalar type type val bind_objects_to_proxies scalars scalars_proxy scalars_origins i symval enumerate scalars symnode_proxy_lookup symval node = scalars_proxy i type ignore union-attr TODO jansel all these modes needed stack enter_context decompose stack enter_context fake_tensor_mode stack enter_context proxy_mode stack enter_context disable_autocast_cache Needed make sure we don t accidentally specialize any symbols assert fake_tensor_mode shape_env None env = fake_tensor_mode shape_env stack enter_context torch fx experimental symbolic_shapes _suppress_guards env str CompileContext current_compile_id inputs sym_sizes scalars type ignore return-value log_compile_reasons compile_reasons list str - None assert compile_reasons trace_structured artifact metadata_fn=lambda name compiled_autograd_compile_reasons encoding json payload_fn=lambda compile_reasons proxy_call_aot_backward pinputs Sequence Any psaved_tensors Sequence torch Tensor saved_tensors Sequence torch Tensor pctx Any ctx Any maybe_backward_state_idx Optional int - Sequence Any The AOTBackward call consists three things prologue backward graph epilogue Our strategy - allow_in_graph prologue CA graph Dynamo graph - copy-paste backward graph into CA graph so CA passes Dynamo can see - trace directly through epilogue Anything gets baked constant metadata example metadata about number outputs removing RNG arguments effect tokens If Dynamo graph capture better then we could add node prologue into CA graph have Dynamo trace into psymints = to_proxy e e ctx _get_compiled_autograd_symints NOTE we should only close over constants CompiledFunction = ctx _forward_cls bw_module = extract_bw_module CompiledFunction metadata = CompiledFunction metadata maybe_subclass_metadata = CompiledFunction maybe_subclass_metadata aot_id = CompiledFunction _aot_id del CompiledFunction torch is_grad_enabled output_alias_info metadata output_info output_alias_info requires_grad raise RuntimeError torch compile does currently support higher order gradients torch _dynamo allow_in_graph type ignore misc call_aot_bwd_prologue ctx_saved_tensors Sequence torch Tensor ctx_symints Sequence IntLikeType flat_args Sequence Any - Any out = torch _functorch _aot_autograd runtime_wrappers _backward_prologue_functional ctx_saved_tensors ctx_symints metadata maybe_subclass_metadata flat_args out pgrads = fx_tracer create_proxy kind= call_function target=call_aot_bwd_prologue args= psaved_tensors psymints pinputs kwargs= pbackward_state = None maybe_backward_state_idx None pbackward_state = hooks_proxy maybe_backward_state_idx type ignore index Copy-paste AOT backward graph into compiled autograd graph copy_paste_aot_backward_graph - list torch Tensor num_inputs graph torch fx Graph - int num_args = node graph nodes node op == placeholder num_args += continue break num_args set up proxy inputs bw_module calling convention symints args primals tangents backward_state num_args = num_inputs bw_module graph type ignore attr-defined pall_args = pgrads i i range num_args - int pbackward_state None replace symints our symints symints = ctx _get_compiled_autograd_symints assert len symints == len ctx symints psymints = to_proxy e e symints pall_args len symints = psymints Add backward_state pbackward_state None pall_args append pbackward_state run over all nodes aot_backward graph copy paste them all into compiled autograd graph args_idx = value_remap = poutputs Optional list torch fx Proxy = None names nodes must appear only once fx Graph dedup AOT backwards appear multiple times deduped_aot_id = str aot_id aot_id_counter aot_id deduped_aot_id += f _ aot_id_counter aot_id aot_id_counter aot_id += make_unique node_name str - str make both informative unique f aot deduped_aot_id _ node_name node bw_module graph nodes type ignore attr-defined node op == placeholder ph = pall_args args_idx node ph name = make_unique node name value_remap node = ph args_idx += node op == output assert len node args == poutputs = torch fx Proxy value_remap n fx_tracer isinstance n torch fx Node n n node args node op == get_attr name = node target qualname = fx_tracer get_fresh_qualname name setattr fx_tracer root qualname getattr bw_module name result = fx_tracer create_node get_attr qualname result name = make_unique node name value_remap node = result node op == call_function node target torch ops aten view default aot bwd graph being lazily compiled we must manually apply view_to_reshape post grad pass since already applied aot fwd baked into gradients node target = torch ops aten reshape default result = fx_tracer graph node_copy node lambda n value_remap n result name = make_unique node name value_remap node = result node op == call_module name = node target qualname = fx_tracer get_fresh_qualname name setattr fx_tracer root qualname getattr bw_module name result = fx_tracer graph node_copy node lambda n value_remap n result target = qualname value_remap node = result raise AssertionError shouldn t get here assert poutputs None In general we don t know what shapes outputs so allocate some dummy sizes them dummy - torch Tensor disable_proxy_modes_tracing torch zeros outputs = dummy isinstance o torch fx Proxy o o poutputs bind_objects_to_proxies outputs poutputs outputs outputs = copy_paste_aot_backward_graph proxy_subclass_constructor subclass_meta Any is_runtime bool unwrapped_args Sequence Any - torch Tensor torch _dynamo allow_in_graph type ignore misc make_subclass unwrapped_args Any - Any subclass_meta creation_fn unwrapped_args is_runtime=is_runtime punwrapped_args = pytree tree_map to_proxy unwrapped_args poutput = fx_tracer create_proxy kind= call_function target=make_subclass args=tuple punwrapped_args kwargs= output = allocate_dummy bind_objects_to_proxies output poutput output results = torch _functorch _aot_autograd runtime_wrappers _backward_epilogue_functional metadata maybe_subclass_metadata outputs make_subclass_override=proxy_subclass_constructor presults = pytree tree_map to_proxy results presults proxy_call_backward inputs Sequence Any output_metadatas Sequence Any saved_tensors Sequence torch Tensor backward_idx int ctx torch autograd function BackwardCFunction maybe_backward_state_idx Optional int - tuple Optional torch Tensor assert hooks_proxy None pctx = hooks_proxy backward_idx type ignore index pinputs = to_proxy inputs psaved_tensors = to_proxy saved_tensors hasattr ctx _forward_cls _aot_id type ignore attr-defined AOT backward proxies = proxy_call_aot_backward pinputs psaved_tensors saved_tensors pctx ctx maybe_backward_state_idx proxies = fx_tracer create_proxy kind= call_function target=call_backward args= pctx psaved_tensors pinputs kwargs= assert proxies None disable_proxy_modes_tracing create fake Tensors grad_ins list Optional torch Tensor = idx output_metadata enumerate output_metadatas output_metadata None proxies idx None grad_ins append None continue layout device dtype size = output_metadata grad_ins append torch empty size=size dtype=dtype layout=layout device=device bind_objects_to_proxies grad_ins proxies tuple grad_ins call_copy_slices_prologue inputs Sequence Any base_sizes Sequence Any base_strides Sequence Any base_storage_offset Any view_sizes Sequence Any view_strides Sequence Any view_storage_offset Any - Sequence torch Tensor args = inputs to_proxy base_sizes to_proxy base_strides to_proxy base_storage_offset to_proxy view_sizes to_proxy view_strides to_proxy view_storage_offset proxy_call copy_slices_prologue args None call_copy_slices_epilogue needs_input_grad Sequence bool result torch Tensor res Sequence Any grad_slice torch Tensor - Sequence torch Tensor proxy_call copy_slices_epilogue needs_input_grad result res grad_slice None len needs_input_grad allocate_dummy - torch Tensor disable_proxy_modes_tracing Weird quantity so s easy grep torch zeros bind_function fn_name str fn Callable Any is_custom_function bool is_traceable bool - str Binds ops fn_name = fn ops add fn_name fn is_custom_function is_traceable apply_functional fn_name str grads Sequence Any args Any output_metadata Sequence Any - Sequence torch Tensor Proxies call ops fn_name grads args into graph op = ops get fn_name proxy_call op grads args output_metadata proxy_call fn Callable Any args Any output_metadata Sequence Any - Sequence torch Tensor Proxies call fn args into graph flat_args _ = pytree tree_flatten args proxy_args = pytree tree_map lambda e to_proxy e args proxy_out = fx_tracer create_proxy call_function fn args=proxy_args kwargs= result = allocate_dummy _ output_metadata bind_objects_to_proxies result proxy_out i i range len result result validate_outputs _ Any outputs Sequence Any args Any output_metadata Sequence Any - Sequence torch Tensor Proxies call ops validate_outputs outputs args into graph op = ops get validate_outputs proxy_args = pytree tree_map to_proxy outputs args new_proxy_outputs = fx_tracer create_proxy call_function op args=proxy_args kwargs= assert len output_metadata == len outputs bind_objects_to_proxies outputs new_proxy_outputs outputs accumulate old_var Any new_var Any - torch Tensor old_var_proxy = to_proxy old_var new_var_proxy = to_proxy new_var proxy_out = fx_tracer create_proxy call_function torch add args= old_var_proxy new_var_proxy kwargs= result = allocate_dummy bind_objects_to_proxies result proxy_out result accumulate_grad variable torch Tensor grad torch Tensor has_post_hooks bool - None fx_tracer create_proxy call_function call_accumulate_grad args= to_proxy variable to_proxy grad has_post_hooks kwargs= proxy_call_hook hook Callable Any args Any kwargs Any - torch fx Proxy fx_tracer create_proxy call_function call_hook hook to_proxy x x args kwargs unpack_hook hook_id int data_id int - torch Tensor assert hooks_proxy None hook = hooks_proxy hook_id type ignore index data = packed_data_proxy data_id type ignore index proxy = proxy_call_hook hook data hook_type= unpack_hook out = allocate_dummy bind_objects_to_proxies out proxy out tensor_pre_hook inputs list torch Tensor hook_id int i int - list torch Tensor assert hooks_proxy None hook = hooks_proxy hook_id type ignore index proxy = proxy_call_hook hook inputs i hook_type= tensor_pre_hook disable_proxy_modes_tracing inputs i = maybe_clone inputs i type ignore assignment bind_objects_to_proxies inputs i proxy inputs cpp_tensor_pre_hook inputs list torch Tensor hook_id int i int - list torch Tensor proxy = fx_tracer create_proxy call_function torch _C _dynamo compiled_autograd call_cpp_tensor_pre_hooks hook_id to_proxy inputs i disable_proxy_modes_tracing inputs i = maybe_clone inputs i type ignore assignment bind_objects_to_proxies inputs i proxy inputs pre_hook inputs Sequence Any hook_id int - list torch Tensor assert hooks_proxy None hook = hooks_proxy hook_id type ignore index proxies = proxy_call_hook hook inputs hook_type= pre_hook disable_proxy_modes_tracing inputs = maybe_clone x x inputs bind_objects_to_proxies inputs proxies inputs post_hook outputs list torch Tensor inputs Sequence torch Tensor hook_id int - list torch Tensor assert hooks_proxy None hook = hooks_proxy hook_id type ignore index proxies = proxy_call_hook hook outputs inputs hook_type= post_hook disable_proxy_modes_tracing outputs = maybe_clone x x outputs type ignore misc bind_objects_to_proxies outputs proxies outputs post_acc_grad_hook input torch Tensor hook_id int - list torch Tensor assert isinstance input torch Tensor assert hooks_proxy None hook = hooks_proxy hook_id type ignore index proxy = proxy_call_hook hook input hook_type= post_acc_grad_hook disable_proxy_modes_tracing res = maybe_clone input bind_objects_to_proxies res proxy res type ignore return-value Note Compiled autograd cudagraphs Eager autograd backward implements scalars -dim tensors see DivBackward other_ When compiled autograd traces those nodes lifts scalar tensors resulting graph some cpu -dim tensor inputs To prevent entire graph skipping cudagraph we move scalars tensors cuda This works because ATen prims ops will accept cuda -dim tensors too move_graph_nodes_to_cuda graph torch fx Graph - list int to_move dict int torch fx Node = has_cuda_inputs = False nodes = list graph nodes assert nodes target == inputs inputs = nodes inputs_users = list inputs users keys input access nodes should immediately follow placeholder nodes first_getitem_idx = len _graph_placeholders assert nodes first_getitem_idx == inputs_users last_getitem_idx = first_getitem_idx + len inputs_users - assert nodes last_getitem_idx == inputs_users - getitem nodes inputs i node enumerate inputs_users has_cuda_inputs node meta val device type == cuda has_cuda_inputs = True continue is_cpu = node meta val device type == cpu is_scalar = len node meta val size == is_cpu is_scalar node_users = list node users keys We can only move cpu scalar exposed user code all isinstance user target torch _ops OpOverload user target namespace prims aten isinstance user target Op user target is_custom_function user node_users all users prims aten can move safely to_move i = node only move cpu scalars cuda there cuda activations graph handle case where cudagraphs enabled cpu-only graph has_cuda_inputs node to_move values verbose_log debug Moving node s cpu cuda node node meta val = node meta val cuda runtime indices we need move cuda list to_move keys is_sym_node node Any - bool isinstance node torch fx Node node op == call_function node target torch ops aten sym_size int torch ops aten sym_numel default dce - None Most these removed nodes would have been removed during Dynamo AOTDispatch Remove some these nodes earlier improve compilation speed Dynamo guards will error instead creating aliasing guards unless we unpack them graph unpack_nodes OrderedSet torch fx Node = OrderedSet i int &#124; None = None i node enumerate fx_tracer graph find_nodes op= placeholder noqa B unpack_nodes update node users keys assert i == len _graph_placeholders - is_impure node torch fx Node - bool node unpack_nodes node op == call_function node target _impure_targets True node is_impure before = len fx_tracer graph nodes fx_tracer graph eliminate_dead_code is_impure after = len fx_tracer graph nodes verbose_log debug DCE removed d nodes before - after remove_unused_sizes - set int used_sizes = unused_sizes = seek placeholder should nodes = iter fx_tracer graph nodes next sizes_node = next assert sizes_node name == sizes getitem_node sizes_node users keys assert getitem_node target operator getitem getitem_node users used_sizes append getitem_node remove graph unused_sizes append getitem_node used_sizes_idx set int = set used used_sizes assert isinstance used args tuple assert used args == sizes_node assert isinstance used args int next_size_idx = len used_sizes_idx used later reindex runtime sizes arg used_sizes_idx add used args reindex graph used args = used args next_size_idx unused unused_sizes fx_tracer graph erase_node unused used_sizes_idx create_graph_module id str - GraphModule GraphModule fx_tracer root fx_tracer graph id end_capture outputs Any - tuple Callable Any Any fx_tracer create_proxy call_function FakeCompiledAutogradEngine _exec_final_callbacks_stub stack close fx_tracer create_node output output fx_tracer create_arg to_proxy outputs runtime_inputs_to_move list int = snapshot_cudagraph_enabled runtime_inputs_to_move = move_graph_nodes_to_cuda fx_tracer graph We traced using dummy tensors Delete all metadata dummy tensors It s probably better refactor use different tracer than make_fx tracer larger change node fx_tracer graph nodes field tensor_meta example_value val field node meta del node meta field trace_structured artifact metadata_fn=lambda name compiled_autograd_graph_pre_reordering encoding string payload_fn=lambda GraphModule fx_tracer root fx_tracer graph f CompiledAutograd id PreReordering print_readable print_output=False delay_unpack_hook_nodes reorder_tensor_pre_hook_nodes reorder_pre_hook_nodes_to_schedule_asap reorder_accumulate_grad_nodes reorder_pre_hook_nodes_to_mimic_eager reorder_post_acc_grad_hook_nodes reorder_post_hook_nodes TODO yf work around remove dead codes like ` sym_size ` ` sym_numel ` which used downstream e g ` ` ` sym_numel_default = torch ops aten sym_numel default sum_ sum_ = None eq_ = == sym_numel_default sym_numel_default = eq_ = None sym_size_int_ = torch ops aten sym_size int getitem_ getitem_ = None eq_ = == sym_size_int_ eq_ = None eq_ = == sym_size_int_ sym_size_int_ = eq_ = None ` ` ` Proper fix Richard s Python compiled autograd effort which will avoid calling make_fx should prevent these ops going into CA graph dce nan_checker nan_checker prep_with_graph fx_tracer graph keep only sizes actually used graph used_sizes_idx = remove_unused_sizes graph = create_graph_module f CompiledAutograd id set_locals_to_steal graph inputs lazy_graph_code = lazy_format_graph_code Compiled autograd graph graph include_device=True include_stride=True colored=True compiled_autograd_log info s lazy_graph_code verbose_log debug s lazy_graph_code trace_structured compiled_autograd_graph payload_fn=lambda graph print_readable print_output=False runtime_wrapper compiled_fn Callable Any inputs Any sizes Any scalars Any hooks Any packed_inputs Any - tuple Any Any global in_compiled_autograd_region try in_compiled_autograd_region = True nan_checker nan_checker prep_with_inputs inputs filtered_sizes = idx integer enumerate sizes idx used_sizes_idx can t create negative size integer filtered_sizes append torch empty integer torch _dynamo maybe_mark_dynamic filtered_sizes - filtered_sizes append integer i runtime_inputs_to_move inputs i = inputs i pin_memory cuda non_blocking=True _disable make_compile_context id out = compiled_fn inputs filtered_sizes scalars hooks packed_inputs nan_checker nan_checker check out out finally in_compiled_autograd_region = False get_chromium_event_logger log_event_end compiled_autograd time time_ns graph_id id start_time_ns log_pt _compile_event=True compile_context __exit__ None None None runtime_wrapper compiler_fn graph staticmethod get_all_nodes args Sequence Any - list torch fx Node filter out non-Node args like None nodes = n n args type n torch fx Node nodes staticmethod is_placeholder node torch fx Node - bool node op == placeholder node op == call_function node target operator getitem node args op == placeholder type ignore union-attr arg-type True False reorder_accumulate_grad_nodes - None Usage AOTAutograd causes all accumulate_grad_ nodes get pushed end graph This differs eager mode which schedules them soon possible This pass attempts reorder graph mimic eager behavior node fx_tracer graph find_nodes op= call_function target=call_accumulate_grad param_node grad_node = node args node args getitem_node = None grad_node target operator getitem getitem_node = grad_node grad_node = getitem_node args arg = max param_node grad_node last arg arg node prev is_placeholder arg arg append node getitem_node None arg append getitem_node delay_unpack_hook_nodes - None We can delay unpack hooks until they needed even later than eager autograd engine node fx_tracer graph find_nodes op= call_function target=call_hook node kwargs get hook_type None = unpack_hook continue first_user = min node users first_user prepend node reorder_tensor_pre_hook_nodes - None Usage AOTAutograd causes all tensor_pre_hook nodes get pushed end graph This differs eager mode which schedules them soon possible This pass attempts reorder graph mimic eager behavior node fx_tracer graph find_nodes op= call_function target=call_hook node kwargs get hook_type None = tensor_pre_hook continue getitem_node = node args input_node = node args tensor_pre_hook handle only one grad tensor input_node node prev is_placeholder input_node input_node append getitem_node getitem_node append node reorder_pre_hook_nodes_to_schedule_asap - None In function we schedule pre hooks soon possible This does match eager behavior schedule pre hook right before its registered node can make acc grad scheduled properly when pre hooks registered them After reordering acc grad node we will reorder pre hooks again mimic eager behavior node fx_tracer graph find_nodes op= call_function target=call_hook node kwargs get hook_type None = pre_hook continue getitem_node = node args pre_hook handle tuple grad tensors input_nodes = get_all_nodes node args to_remove = to_append = hook_block = node contain hook hook args getitem n input_nodes n op == call_function n target operator getitem to_append append n args to_remove append n hook_block append n b zip to_remove to_append input_nodes remove input_nodes append b type ignore arg-type arg = max input_nodes last input arg node prev is_placeholder arg arg append getitem_node n hook_block getitem_node append n reorder_pre_hook_nodes_to_mimic_eager - None Usage AOTAutograd causes all pre_hook nodes get pushed end graph This differs eager mode which schedules them right before their registered node execution This pass attempts reorder graph mimic eager behavior pre_hooks = node fx_tracer graph find_nodes op= call_function target=call_hook node kwargs get hook_type None = pre_hook continue pre_hooks append node node reversed pre_hooks hook_getitem_node = node args users = list node users keys len users == continue users all getitem ops they used same registered node assert all user op == call_function user target operator getitem user users registered_node = next iter users users keys registered_node node next registered_node prepend hook_getitem_node registered_node prepend node getitem users registered_node prepend getitem reorder_post_acc_grad_hook_nodes - None Usage AOTAutograd causes all post_acc_grad_hook nodes get pushed end graph This differs eager mode which schedules them soon possible This pass attempts reorder graph mimic eager behavior post_acc_grad_hooks = node fx_tracer graph find_nodes op= call_function target=call_hook node kwargs get hook_type None = post_acc_grad_hook continue post_acc_grad_hooks append node nodes post_acc_grad_hooks topo order For hooks registered same node we should keep their relative order node reversed post_acc_grad_hooks getitem_node = node args param_node = node args post_acc_grad_hook handle one param find corresponding acc_grad node acc_grad_node = None n list param_node users keys n op == call_function n target call_accumulate_grad acc_grad_node = n break assert acc_grad_node None post_acc_grad_hook must have corresponding acc grad node append post_acc_grad_hook after acc_grad node acc_grad_node append getitem_node getitem_node append node reorder_post_hook_nodes - None Usage AOTAutograd causes all post_hook nodes get pushed end graph This differs eager mode which schedules them soon possible This pass attempts reorder graph mimic eager behavior post_hooks = node fx_tracer graph find_nodes op= call_function target=call_hook node kwargs get hook_type None = post_hook continue post_hooks append node node reversed post_hooks getitem_node = node args output_nodes = node args input_nodes = node args len output_nodes continue input_nodes_and_users = input_nodes_and_users extend list input_nodes input_node input_nodes input_nodes_and_users extend user user list input_node users keys user op == call_function user target call_hook node kwargs get hook_type None == post_hook arg = max input_nodes_and_users last input users arg op == call_function arg target call_accumulate_grad param_node = arg args post_acc_grad_hook_node = None n list param_node users keys n op == call_function n target call_hook n kwargs get hook_type None == post_acc_grad_hook post_acc_grad_hook_node = n post_acc_grad_hook_node None post_acc_grad_hook_node append getitem_node getitem_node append node continue arg node prev is_placeholder arg arg append getitem_node getitem_node append node to_proxy t Any - Any t None None isinstance t list to_proxy x x t isinstance t tuple tuple to_proxy x x t isinstance t torch SymInt torch SymFloat symnode_proxy_lookup t node isinstance t torch Tensor constant types like device dtype str t proxy_tensor = fetch_object_proxy fx_tracer t assert isinstance proxy_tensor torch fx experimental proxy_tensor _ProxyTensor proxy_tensor proxy bind_objects_to_proxies objects Sequence Any proxies Any origins Optional list tuple int str = None - Sequence Any isinstance proxies torch fx Proxy origins assert len origins == len objects bound_proxies = i range len objects nodecall_index node_name = origins i set_node_origin node_name nodecall_index None bound_proxies append proxies i type ignore index proxies = bound_proxies proxies = proxies i i range len objects type ignore index assert len objects == len proxies track_tensor_tree objects proxies constant=None tracer=self fx_tracer proxies bind_backward_state index int - BackwardState assert hooks_proxy None proxy = hooks_proxy index type ignore index bw_state = BackwardState track_tensor_tree bw_state proxy constant=None tracer=self fx_tracer bw_state set_node_origin node_name str nodecall_index int pyobj Optional torch autograd Function - None maybe_aot_id = pyobj None forward_cls = pyobj _forward_cls type ignore attr-defined hasattr forward_cls _aot_id backward created AOT Dispatcher forward_cls _lazy_backward_info None raise RuntimeError This compiled backward function saved AOTAutogradCache which does support compiled autograd Please turn off AOTAutogradCache using ` TORCHINDUCTOR_AUTOGRAD_CACHE= ` maybe_aot_id = forward_cls _aot_id new_code = f node_name maybe_aot_id NodeCall nodecall_index raw_stack_trace = CapturedTraceback extract format - new_stack_trace = raw_stack_trace replace raw_stack_trace = CapturedTraceback extract format - new_code set_stack_trace new_stack_trace state autograd engine dispatch kept sync enable disable context managers compiled_autograd_enabled = False global flag check compiled autograd enabled Dynamo stance force_eager compiled_autograd_enabled_force_eager = False global flag check we processing graphs produced compiled autograd graph in_compiled_autograd_region = False active_disable_ctx = False depth = contextlib contextmanager _enable compiler_fn Callable Any dynamic bool = True ignore_active_disable_ctx bool = True - Generator None None None The entrypoint enable CA It recommended enable via ` torch _dynamo config compiled_autograd = True ` rather than using context manager directly If you torch compiling corresponding forward pass make sure they wrapped under context well Example train model inputs target compiled_model = torch compile model pred = compiled_model data loss = compute_loss pred target loss backward _enable compiler_fn train model inputs target Inputs - compiler_fn The wrapper will consume compiled autograd graph e g ` torch compile ` - dynamic Whether compiled autograd will treat tensors autograd graph params activations dynamic This doesn t affect dynamic configuration compilation wrapper ignore_active_disable_ctx active_disable_ctx yield dynamic assert type dynamic bool torch _dynamo eval_frame eval_frame _stance stance == force_eager If user explicitly sets Dynamo stance force_eager we want Compiled Autograd fall back eager well global compiled_autograd_enabled_force_eager compiled_autograd_enabled_force_eager = True try yield finally compiled_autograd_enabled_force_eager = False we need because user might have imported they directly use context manager we need lazily because circular dependencies torch cuda is_available torch _inductor cudagraph_trees noqa F prior_compiler prior_dynamic = torch _C _dynamo compiled_autograd set_autograd_compiler functools partial AutogradCompilerInstance compiler_fn dynamic snapshot_verbose_logging_enabled torch _C _dynamo compiled_autograd set_verbose_logger verbose_log type ignore arg-type global compiled_autograd_enabled compiled_autograd_enabled = True global depth prior_depth = depth depth += try torch autograd set_multithreading_enabled False yield finally prior_compiler compiled_autograd_enabled = False torch _C _dynamo compiled_autograd set_autograd_compiler prior_compiler prior_dynamic depth -= assert depth == prior_depth Nested Compiled Autograd Contexts must before their parent context contextlib contextmanager _disable - Generator None None None prior_compiler prior_dynamic = torch _C _dynamo compiled_autograd set_autograd_compiler None False global compiled_autograd_enabled compiled_autograd_enabled = False global active_disable_ctx active_disable_ctx active_disable_ctx = True try yield finally prior_compiler compiled_autograd_enabled = True active_disable_ctx = False torch _C _dynamo compiled_autograd set_autograd_compiler prior_compiler prior_dynamic starting state new process reset - None global compiled_autograd_enabled compiled_autograd_enabled = False assert in_compiled_autograd_region torch _C _dynamo compiled_autograd set_autograd_compiler None False torch _C _dynamo compiled_autograd set_verbose_logger None torch _C _dynamo compiled_autograd clear_cache global COMPILE_COUNTER COMPILE_COUNTER = itertools count Reimplementation part CopySlices apply Python The shared code really similar so we re going try deduplicate copy_slices_prologue inputs Sequence torch Tensor base_sizes Sequence IntLikeType base_strides Sequence IntLikeType base_storage_offset IntLikeType view_sizes Sequence IntLikeType view_strides Sequence IntLikeType view_storage_offset IntLikeType - list torch Tensor grad = inputs result = grad new_empty_strided base_sizes base_strides assert grad None result copy_ grad offset = view_storage_offset - base_storage_offset grad_slice = result as_strided view_sizes view_strides offset result grad_slice grad_slice clone memory_format=torch contiguous_format Reimplementation part CopySlices apply Python The shared code really similar so we re going try deduplicate copy_slices_epilogue needs_input_grad Sequence bool result torch Tensor res Sequence Optional torch Tensor grad_slice torch Tensor - list Optional torch Tensor grad_inputs list Optional torch Tensor = None len needs_input_grad i range len needs_input_grad needs_input_grad i res i None continue i == to_copy = res i assert to_copy None grad_slice copy_ to_copy grad_inputs i = result grad_inputs i = res i grad_inputs