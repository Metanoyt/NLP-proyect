mypy allow-untyped-defs EDITING THIS FILE READ THIS FIRST see Note Edit Symbolic Files README md This file exports ONNX ops opset functools torch torch _C _onnx _C_onnx torch onnx _constants errors torch onnx _internal torchscript_exporter _type_utils jit_utils registration symbolic_helper symbolic_opset opset symbolic_opset opset utils _onnx_symbolic = functools partial registration onnx_symbolic opset= _onnx_symbolic aten softmax symbolic_helper parse_args v i none softmax g jit_utils GraphContext input dim dtype=None softmax = g op Softmax input axis_i=dim dtype dtype node kind = prim Constant parsed_dtype = symbolic_helper _get_const dtype i dtype softmax = g op Cast softmax to_i=_type_utils JitScalarType parsed_dtype onnx_type softmax _onnx_symbolic aten log_softmax symbolic_helper parse_args v i none log_softmax g jit_utils GraphContext input dim dtype=None return_op = g op LogSoftmax input axis_i=dim dtype dtype node kind = prim Constant parsed_dtype = symbolic_helper _get_const dtype i dtype return_op = g op Cast return_op to_i=_type_utils JitScalarType parsed_dtype onnx_type return_op _onnx_symbolic aten frobenius_norm symbolic_helper parse_args v v i frobenius_norm g jit_utils GraphContext dim=None keepdim=False dim_val = symbolic_helper _maybe_get_const dim symbolic_helper _is_value dim_val len dim_val == g op ReduceL keepdims_i= sqr = g op Mul sumsqr = symbolic_helper _reducesum_helper g sqr dim keepdims_i=keepdim g op Sqrt sumsqr _onnx_symbolic aten split symbolic_helper parse_args v v i i split g jit_utils GraphContext split_size_or_sizes dim _outputs=None symbolic_helper _is_split_static split_size_or_sizes _outputs split_out = g op SplitToSequence split_size_or_sizes axis_i=dim _outputs None split_out Convert multiple slice nodes iff number splits number outputs statically known symbolic_helper _is_packed_list split_size_or_sizes len symbolic_helper _unpack_list split_size_or_sizes == _outputs split_sizes = symbolic_helper _unsqueeze_helper g v v symbolic_helper _unpack_list split_size_or_sizes start = g op Constant value_t=torch tensor dtype=torch long axis = g op Constant value_t=torch tensor dim dtype=torch long res = i range _outputs end = g op Add start split_sizes i split_sizes list same length _outputs res append g op Slice start end axis start = end res g op SequenceAt split_out g op Constant value_t=torch tensor i dtype=torch long i range _outputs split_val = symbolic_helper _node_get split_size_or_sizes node value split_val dim pyrefly ignore bad-argument-type g op Split split_size_or_sizes axis_i=dim outputs=_outputs split_size = symbolic_helper _get_const split_size_or_sizes i split_size size = symbolic_helper _get_tensor_dim_size dim size None _outputs None size = split_size _outputs raise errors SymbolicValueError Unknown dimension size supported splits = split_size size split_size leftover = size split_size leftover splits append leftover splits = g op Constant value_t=torch tensor splits pyrefly ignore bad-argument-type g op Split splits axis_i=dim outputs=_outputs _onnx_symbolic aten split_with_sizes split_with_sizes g jit_utils GraphContext split_sizes dim _outputs=None split g split_sizes dim _outputs _onnx_symbolic aten unsafe_split unsafe_split g jit_utils GraphContext split_size_or_sizes dim _outputs=None split g split_size_or_sizes dim _outputs _onnx_symbolic aten unsafe_split_with_sizes unsafe_split_with_sizes g jit_utils GraphContext split_sizes dim _outputs=None split_with_sizes g split_sizes dim _outputs _onnx_symbolic aten tensor_split symbolic_helper parse_args v v i i tensor_split g jit_utils GraphContext indices_or_sections dim _outputs=None axis = g op Constant value_t=torch tensor dim dtype=torch long axis = opset unsqueeze g axis const_ = g op Constant value_t=torch tensor dtype=torch long symbolic_helper _is_split_static indices_or_sections _outputs split_val = symbolic_helper _node_get indices_or_sections node value split_val dim start = g op Constant value_t=torch tensor dtype=torch long res = assert _outputs None i range _outputs - end = g op Gather indices_or_sections g op Constant value_t=torch tensor i dtype=torch long axis_i= res append g op Slice start end axis start = end end = symbolic_helper _size_helper g axis res append g op Slice start end axis res split_size = symbolic_helper _get_const indices_or_sections i indices_or_sections size = symbolic_helper _get_tensor_dim_size dim size None _outputs None size = split_size _outputs raise errors SymbolicValueError Unknown dimension size supported min_split_size = size split_size num_splits_one_extra = size split_size splits = num_splits_one_extra min_split_size + leftover = split_size - num_splits_one_extra min_split_size splits = g op Constant value_t=torch tensor splits + leftover dtype=torch long pyrefly ignore bad-argument-type g op Split splits axis_i=dim outputs=_outputs symbolic_helper _is_tensor indices_or_sections symbolic_helper _get_tensor_rank indices_or_sections == loop_len = symbolic_helper _size_helper g indices_or_sections g op Constant value_t=torch tensor loop_len = opset unsqueeze g loop_len loop_condition = g op Cast const_ to_i=_C_onnx TensorProtoDataType BOOL To make first slice below loop work we pad zero first position so will initial start slice padding_ = g op Constant value_t=torch tensor dtype=torch long indices_or_sections = g op Concat padding_ indices_or_sections axis_i= final_splits = g op SequenceEmpty Loop inputs loop loop_context _ = jit_utils add_op_with_blocks g Loop loop_len loop_condition final_splits outputs= n_blocks= loop_block = loop_context block block_input_iter = utils _add_input_to_block loop_block cond = utils _add_input_to_block loop_block noqa F final_splits = utils _add_input_to_block loop_block start = loop_context op Gather indices_or_sections block_input_iter axis_i= end = loop_context op Gather indices_or_sections loop_context op Add block_input_iter const_ axis_i= slice = loop_context op Slice start end axis final_splits = loop_context op SequenceInsert final_splits slice Loop outputs cond_out = loop_context op Identity loop_condition utils _add_output_to_block loop_block cond_out utils _add_output_to_block loop_block final_splits loop_out = loop node output start = g op Gather indices_or_sections g op Constant value_t=torch tensor - dtype=torch long axis_i= start = opset unsqueeze g start end = symbolic_helper _size_helper g axis last_slice = g op Slice start end axis g op SequenceInsert loop_out last_slice scalar tensor dim_size = symbolic_helper _size_helper g axis min_split_size = g op Div dim_size indices_or_sections min_split_size_plus_ = g op Add min_split_size const_ num_splits_one_extra = g op Mod dim_size indices_or_sections splits = g op Tile min_split_size_plus_ num_splits_one_extra leftover = g op Tile min_split_size g op Sub opset unsqueeze g indices_or_sections num_splits_one_extra splits = g op Concat splits leftover axis_i= _outputs None g op SplitToSequence splits axis_i=dim g op Split splits axis_i=dim outputs=_outputs _onnx_symbolic aten unbind symbolic_helper parse_args v i i unbind g jit_utils GraphContext dim= _outputs=None _outputs None g op SplitToSequence g op Constant value_t=torch tensor dtype=torch long axis_i=dim keepdims_i= splits = g op Constant value_t=torch tensor _outputs outputs = g op Split splits axis_i=dim outputs=_outputs outputs = outputs _outputs == outputs squeezed_outputs = g op Squeeze out g op Constant value_t=torch tensor dim out outputs squeezed_outputs _onnx_symbolic aten nonzero_numpy Emitted ` torch nonzero x as_tuple=True ` nonzero_numpy g jit_utils GraphContext input _outputs=None unbind g opset nonzero g input _outputs=_outputs _onnx_symbolic aten where symbolic_helper parse_args v v v i where g jit_utils GraphContext condition self=None other=None _outputs=None Assumes torch where s first argument takes only Bool Byte tensors symbolic_helper _is_bool condition condition = g op Cast condition to_i=_C_onnx TensorProtoDataType BOOL None condition = opset nonzero g condition symbolic_helper _unbind_helper g condition g op Constant value_t=torch tensor _outputs pyrefly ignore bad-argument-type g op Where condition other _onnx_symbolic aten fake_quantize_per_channel_affine symbolic_helper parse_args v v v i i i fake_quantize_per_channel_affine g jit_utils GraphContext inputs scale zero_point axis quant_min=- quant_max= NOTE allowed special case PyTorch restricts activations range https github com pytorch pytorch blob b b d b c f e c c ede bd torch ao quantization observer py#L quant_min quant_max - raise errors SymbolicValueError For quant_min quant_max ONNX allows only - f Got quant_min quant_max inputs ONNX defines zero_point int uint quant_min == zero_point = g op Cast zero_point to_i=_C_onnx TensorProtoDataType UINT zero_point = g op Cast zero_point to_i=_C_onnx TensorProtoDataType INT quantized = g op QuantizeLinear inputs scale zero_point axis_i=axis quant_min quant_max == quantized = g op Clip quantized opset unused g g op Constant value_t=torch tensor dtype=torch uint g op DequantizeLinear quantized scale zero_point axis_i=axis _onnx_symbolic aten fake_quantize_per_tensor_affine symbolic_helper parse_args v v v i i fake_quantize_per_tensor_affine g jit_utils GraphContext inputs scale zero_point quant_min=- quant_max= NOTE allowed special case PyTorch restricts activations range https github com pytorch pytorch blob b b d b c f e c c ede bd torch ao quantization observer py#L quant_min quant_max - raise errors SymbolicValueError For quant_min quant_max ONNX allows only - f Got quant_min quant_max inputs quant_min == zero_point = g op Cast zero_point to_i=_C_onnx TensorProtoDataType UINT zero_point = g op Cast zero_point to_i=_C_onnx TensorProtoDataType INT _type_utils JitScalarType from_value scale _type_utils JitScalarType UNDEFINED = _type_utils JitScalarType FLOAT scale = g op Cast scale to_i=_C_onnx TensorProtoDataType FLOAT quantized = g op QuantizeLinear inputs scale zero_point quant_min quant_max == quantized = g op Clip quantized opset unused g g op Constant value_t=torch tensor dtype=torch uint g op DequantizeLinear quantized scale zero_point _reduce_op_symbolic onnx_op_name symbolic g dim=None keepdim=None = symbolic_helper _maybe_cast_reduce_op_input g dim None all-reduce path symbolic_helper _handle_reduce_dim_none g onnx_op_name keepdim = symbolic_helper _get_const keepdim i keepdim g op onnx_op_name dim keepdims_i=keepdim symbolic _onnx_symbolic aten sum decorate= symbolic_helper _apply_params ReduceSum sum _reduce_with_dtype onnx_op name symbolic = _reduce_op_symbolic onnx_op symbolic_helper _overload_by_arg_count reduce g args kwargs symbolic_helper parse_args v none reduce_nodim g dtype dtype_onnx = None dtype node kind == onnx Constant dtype = symbolic_helper _get_const dtype i dtype dtype_onnx = _type_utils JitScalarType dtype onnx_type = g op Cast to_i=dtype_onnx dtype node kind = prim Constant symbolic_helper _unimplemented name dtype dtype result = symbolic g dtype_onnx None result_dtype_onnx = _type_utils JitScalarType from_value result onnx_type result_dtype_onnx = dtype_onnx result = g op Cast result to_i=dtype_onnx result symbolic_helper parse_args v v i none reduce_dim g dim keepdim dtype dtype_onnx = None dtype node kind == onnx Constant dtype = symbolic_helper _get_const dtype i dtype dtype_onnx = _type_utils JitScalarType dtype onnx_type = g op Cast to_i=dtype_onnx dtype node kind = prim Constant symbolic_helper _unimplemented name dtype dtype result = symbolic g dim keepdim dtype_onnx None result_dtype_onnx = _type_utils JitScalarType from_value result onnx_type result_dtype_onnx = dtype_onnx result = g op Cast result to_i=dtype_onnx result reduce_nodim reduce_dim reduce Ported https github com microsoft onnxscript blob b b b f d c d e d ef d b onnxscript function_libs torch_aten ops core py#L NOTE Supporting aten unflatten before opset needs helper function adjust ONNX op changes Concat Slice _onnx_symbolic aten unflatten unflatten g jit_utils GraphContext input dim unflattened_size input_dim = symbolic_helper _get_tensor_rank input input_dim None symbolic_helper _unimplemented dim ONNX PyTorch use different strategies split input Input rank must known export time dim could negative input_dim = g op Constant value_t=torch tensor input_dim dtype=torch int dim = g op Add input_dim dim dim = g op Mod dim input_dim input_size = g op Shape input head_start_idx = g op Constant value_t=torch tensor dtype=torch int head_end_idx = g op Reshape dim g op Constant value_t=torch tensor dtype=torch int head_part_rank = g op Slice input_size head_start_idx head_end_idx dim_plus_one = g op Add dim g op Constant value_t=torch tensor dtype=torch int tail_start_idx = g op Reshape dim_plus_one g op Constant value_t=torch tensor dtype=torch int tail_end_idx = g op Constant value_t=torch tensor _constants INT _MAX dtype=torch int tail_part_rank = g op Slice input_size tail_start_idx tail_end_idx final_shape = g op Concat head_part_rank unflattened_size tail_part_rank axis_i= symbolic_helper _reshape_helper g input final_shape _onnx_symbolic aten unsafe_chunk symbolic_helper parse_args v i i i unsafe_chunk g jit_utils GraphContext chunks dim _outputs=None _outputs None g op SplitToSequence g op Constant value_t=torch tensor dtype=torch long axis_i=dim keepdims_i= size = symbolic_helper _get_tensor_dim_size dim size None symbolic_helper _unimplemented unsafe_chunk unknown dimension size split_size = size + chunks - chunks splits = split_size size split_size leftover = size split_size leftover splits append leftover TODO So far we don t have module using method We ll keep constant unless we see request dynamics any user s modules splits = g op Constant value_t=torch tensor splits dtype=torch long g op Split splits axis_i=dim outputs=_outputs _onnx_symbolic aten tile tile g jit_utils GraphContext dims self_shape = g op Shape self_rank = g op Size self_shape dims_rank = g op Size dims diff = g op Sub self_rank dims_rank const_zero = g op Constant value_t=torch tensor If dims shorter than shape pad dims dims_shorter_than_self_shape = g op Greater diff const_zero if_op_greater if_context_greater else_context_greater _ = jit_utils add_op_with_blocks g If dims_shorter_than_self_shape n_blocks= outputs= const_one = if_context_greater op Constant value_t=torch LongTensor diff_ d_greater = if_context_greater op Reshape diff const_one exapnd_ones_greater = if_context_greater op Expand const_one diff_ d_greater dims_ = if_context_greater op Concat exapnd_ones_greater dims axis_i= utils _add_output_to_block if_context_greater block dims_ identity_dim = else_context_greater op Identity dims utils _add_output_to_block else_context_greater block identity_dim dims_final = if_op_greater node output If dims longer than shape pad shape dims_longer_than_self_shape = g op Less diff const_zero if_op_less if_context_less else_context_less _ = jit_utils add_op_with_blocks g If dims_longer_than_self_shape n_blocks= outputs= const_one = if_context_less op Constant value_t=torch LongTensor diff_ d_less = if_context_less op Reshape if_context_less op Abs diff const_one exapnd_ones_less = if_context_less op Expand const_one diff_ d_less self_final_shape = if_context_less op Concat exapnd_ones_less self_shape axis_i= self_ = if_context_less op Reshape self_final_shape utils _add_output_to_block if_context_less block self_ identity_self = else_context_less op Identity utils _add_output_to_block else_context_less block identity_self self_final = if_op_less node output dims_final = g op Cast dims_final to_i=_C_onnx TensorProtoDataType INT g op Tile self_final dims_final _onnx_symbolic aten repeat_interleave repeat_interleave g jit_utils GraphContext repeats dim=None output_size=None repeats_dim = symbolic_helper _get_tensor_rank repeats repeats_sizes = symbolic_helper _get_tensor_sizes repeats input_sizes = symbolic_helper _get_tensor_sizes repeats_dim None raise errors SymbolicValueError Unsupported ONNX export repeat_interleave unknown repeats rank repeats_sizes None raise errors SymbolicValueError Unsupported ONNX export repeat_interleave unknown repeats size input_sizes None raise errors SymbolicValueError Unsupported ONNX export repeat_interleave unknown input size final_dim = dim dim None flatten By default use flattened input array flat output array symbolic_helper _is_none dim = symbolic_helper _reshape_helper g g op Constant value_t=torch tensor - dim = torch tensor dtype=torch int dim = symbolic_helper _maybe_get_scalar dim Handle cases where dim negative dim dim += len input_sizes output_sizes = input_sizes copy idx input_size enumerate input_sizes input_size None output_sizes idx input_sizes idx = - Check all indices should repeated same number times repeats_dim == repeats_dim == repeats_sizes == symbolic_helper _repeat_interleave_single_value_repeat_helper g repeats dim cond_dynamic_repeats = repeats_dim == repeats_sizes None If input size dynamic repeats vector dynamic output_sizes dim == cond_dynamic_repeats reps = symbolic_helper _size_helper g dim reps = opset unsqueeze g reps Check repeats dynamic As repeats dynamic we use where node substitute statement If repests_dim = expand repeats otherwise use original tensor cond_dynamic_repeats repeat_dim = symbolic_helper _size_helper g repeats g op Constant value_t=torch LongTensor repeat_cond = g op Equal repeat_dim g op Constant value_t=torch LongTensor repeats = where g repeat_cond g op Expand repeats reps repeats There cases when repeats -d tensor multiple repeats dim provided along one dynamic axes provided A simple example would input shape - where represents dynamic axes dim = Now repeat interleaving can performed pytorch when value matches number elements repeat example - number repeats should well opset repeat_interleave g repeats final_dim reps_like = g op ConstantOfShape g op Shape repeats value_t=torch tensor dtype=torch long r_splits = split g repeats reps_like i_splits = split g reps_like dim output_sizes dim input_sizes dim = - Create loop iterate over each value along dimension perform individual interleaving using repeats tensor Loop following pattern input trip_count cond int trip_count = bool cond = int i= i trip_count cond ++i cond = Loop conditions loop_condition = g op Constant value_t=torch tensor loop_condition = g op Cast loop_condition to_i=_C_onnx TensorProtoDataType BOOL loop_len = reps Create empty sequence store final expansions final_splits = g op SequenceEmpty Loop inputs loop loop_context _ = jit_utils add_op_with_blocks g Loop loop_len loop_condition final_splits n_blocks= loop_block = loop_context block block_input_iter = utils _add_input_to_block loop_block cond = utils _add_input_to_block loop_block noqa F final_splits = utils _add_input_to_block loop_block r_split = loop_context op SequenceAt r_splits block_input_iter i_split = loop_context op SequenceAt i_splits block_input_iter i_split = opset unsqueeze loop_context i_split dim + r_concat = loop_context op Constant value_t=torch LongTensor input_sizes dim + r_split loop_context op Constant value_t=torch LongTensor input_sizes dim + r_concat = loop_context op Concat r_concat axis_i= i_split = opset expand loop_context i_split r_concat None i_split = symbolic_helper _reshape_helper loop_context i_split g op Constant value_t=torch LongTensor output_sizes final_splits = loop_context op SequenceInsert final_splits i_split Loop outputs cond_out = loop_context op Cast loop_condition to_i=_C_onnx TensorProtoDataType BOOL utils _add_output_to_block loop_block cond_out utils _add_output_to_block loop_block final_splits loop_out = loop node output loop_out = g op ConcatFromSequence loop_out axis_i=dim loop_out _onnx_symbolic aten diagonal symbolic_helper parse_args v i i i diagonal g jit_utils GraphContext offset dim dim rank = symbolic_helper _get_tensor_rank Replace negative indexing when rank known rank None dim = dim dim = dim + rank dim = dim dim = dim + rank dim _size = opset size g dim=g op Constant value_t=torch LongTensor dim dim _size = opset size g dim=g op Constant value_t=torch LongTensor dim Create appropriate mask mask_shape = g op Concat dim _size dim _size axis_i= mask = opset zeros g mask_shape None None None mask = g op EyeLike mask k_i=offset dim dim appended dimension end shape rank None axes = list range rank axes remove dim axes remove dim = g op Transpose perm_i=axes + dim dim symbolic_helper _unimplemented diagonal unknown input rank Multiply input mask calculate values along diagonal The mask consists one values where diagonal values calculated For example = result = g op Mul mask result = symbolic_helper _reducesum_helper g result axes_i= - keepdims_i= Calculate gather indices based offset dims If offset greater than zero set offset zero aids calculation selection window offset_op = g op Constant value_t=torch LongTensor offset offset = diag_size = g op Max g op Min dim _size g op Sub dim _size offset_op g op Constant value_t=torch LongTensor offset = diag_size = g op Max g op Min g op Add dim _size offset_op dim _size g op Constant value_t=torch LongTensor diag_size = g op Concat diag_size axis_i= Calculate which diagonal values select For example cases offsets we need select last two columns so we create tensor all columns selected So example select_window_ones_fill = opset ones g diag_size None None select_window = g op CumSum select_window_ones_fill g op Constant value_t=torch LongTensor select_window = g op Add select_window g op Constant value_t=torch LongTensor abs offset - gather_shape = opset size g result dim=g op Constant value_t=torch LongTensor axis axis list range rank - gather_shape append diag_size gather_shape = g op Concat gather_shape axis_i= gather_indices = opset zeros g gather_shape None None There might cases where offset value greater than number rows columns might cause diagonal overrun result diag_size would zero For example offset = dim _size = columns dim _size = rows diag_size = max min - = based calculation above Cases diagonal overrun always result diag_size = max -ve value = In cases without diagonal overrun we select appropriate rows columns along which we calculating diagonal values In cases diagonal overrun we tensor which has dimension row column where overrun occurred -dim we essentially returning empty tensor overrun_cond = g op Not g op Equal diag_size g op Constant value_t=torch tensor dtype=torch int if_op if_context else_context _ = jit_utils add_op_with_blocks g If overrun_cond n_blocks= gather_indices_if_block = if_context op Add gather_indices select_window gather_indices_if_block = symbolic_helper _unsqueeze_helper if_context gather_indices_if_block rank - final_non_overrun = if_context op GatherND result gather_indices_if_block batch_dims_i=rank - final_overrun = opset zeros else_context gather_shape None None utils _add_output_to_block if_context block final_non_overrun utils _add_output_to_block else_context block final_overrun if_op Quantized ops _onnx_symbolic quantized linear quantized_linear g jit_utils GraphContext q_input q_weight bias op_scale op_zero_point input input_scale _ _ = symbolic_helper dequantize_helper g q_input weight weight_scale _ axis = symbolic_helper dequantize_helper g q_weight q_bias = symbolic_helper requantize_bias_helper g bias input_scale weight_scale axis bias _ _ _ = symbolic_helper dequantize_helper g q_bias output = opset linear g input weight bias symbolic_helper quantize_helper g output op_scale op_zero_point _onnx_symbolic quantized linear_relu quantized_linear_relu g jit_utils GraphContext q_input q_weight bias op_scale op_zero_point input input_scale _ _ = symbolic_helper dequantize_helper g q_input weight weight_scale _ axis = symbolic_helper dequantize_helper g q_weight q_bias = symbolic_helper requantize_bias_helper g bias input_scale weight_scale axis bias _ _ _ = symbolic_helper dequantize_helper g q_bias output = opset linear g input weight bias output = opset relu g output symbolic_helper quantize_helper g output op_scale op_zero_point _onnx_symbolic quantized conv d_relu quantized_conv d_relu g jit_utils GraphContext q_input q_weight bias stride padding dilation groups op_scale op_zero_point input input_scale _ _ = symbolic_helper dequantize_helper g q_input weight weight_scale _ axis = symbolic_helper dequantize_helper g q_weight q_bias = symbolic_helper requantize_bias_helper g bias input_scale weight_scale axis bias _ _ _ = symbolic_helper dequantize_helper g q_bias output = opset conv d g input weight bias stride padding dilation groups output = opset relu g output symbolic_helper quantize_helper g output op_scale op_zero_point _onnx_symbolic quantized conv d_relu quantized_conv d_relu g jit_utils GraphContext q_input q_weight bias stride padding dilation groups op_scale op_zero_point input input_scale _ _ = symbolic_helper dequantize_helper g q_input weight weight_scale _ axis = symbolic_helper dequantize_helper g q_weight q_bias = symbolic_helper requantize_bias_helper g bias input_scale weight_scale axis bias _ _ _ = symbolic_helper dequantize_helper g q_bias output = opset conv d g input weight bias stride padding dilation groups output = opset relu g output symbolic_helper quantize_helper g output op_scale op_zero_point _onnx_symbolic quantized conv d_relu quantized_conv d_relu g jit_utils GraphContext q_input q_weight bias stride padding dilation groups op_scale op_zero_point input input_scale _ _ = symbolic_helper dequantize_helper g q_input weight weight_scale _ axis = symbolic_helper dequantize_helper g q_weight q_bias = symbolic_helper requantize_bias_helper g bias input_scale weight_scale axis bias _ _ _ = symbolic_helper dequantize_helper g q_bias output = opset conv d g input weight bias stride padding dilation groups output = opset relu g output symbolic_helper quantize_helper g output op_scale op_zero_point _onnx_symbolic quantized conv d quantized_conv d g jit_utils GraphContext q_input q_weight bias stride padding dilation groups op_scale op_zero_point input input_scale _ _ = symbolic_helper dequantize_helper g q_input weight weight_scale _ axis = symbolic_helper dequantize_helper g q_weight q_bias = symbolic_helper requantize_bias_helper g bias input_scale weight_scale axis bias _ _ _ = symbolic_helper dequantize_helper g q_bias output = opset conv d g input weight bias stride padding dilation groups symbolic_helper quantize_helper g output op_scale op_zero_point _onnx_symbolic quantized conv d quantized_conv d g jit_utils GraphContext q_input q_weight bias stride padding dilation groups op_scale op_zero_point input input_scale _ _ = symbolic_helper dequantize_helper g q_input weight weight_scale _ axis = symbolic_helper dequantize_helper g q_weight q_bias = symbolic_helper requantize_bias_helper g bias input_scale weight_scale axis bias _ _ _ = symbolic_helper dequantize_helper g q_bias output = opset conv d g input weight bias stride padding dilation groups symbolic_helper quantize_helper g output op_scale op_zero_point _onnx_symbolic quantized conv d quantized_conv d g jit_utils GraphContext q_input q_weight bias stride padding dilation groups op_scale op_zero_point input input_scale _ _ = symbolic_helper dequantize_helper g q_input weight weight_scale _ axis = symbolic_helper dequantize_helper g q_weight q_bias = symbolic_helper requantize_bias_helper g bias input_scale weight_scale axis bias _ _ _ = symbolic_helper dequantize_helper g q_bias output = opset conv d g input weight bias stride padding dilation groups symbolic_helper quantize_helper g output op_scale op_zero_point _onnx_symbolic quantized conv_transpose d quantized_conv_transpose d g jit_utils GraphContext q_input q_weight bias stride padding output_padding dilation groups op_scale op_zero_point input input_scale _ _ = symbolic_helper dequantize_helper g q_input weight weight_scale _ axis = symbolic_helper dequantize_helper g q_weight q_bias = symbolic_helper requantize_bias_helper g bias input_scale weight_scale axis bias _ _ _ = symbolic_helper dequantize_helper g q_bias output = opset conv_transpose d g input weight bias stride padding output_padding groups dilation symbolic_helper quantize_helper g output op_scale op_zero_point _onnx_symbolic quantized conv_transpose d quantized_conv_transpose d g jit_utils GraphContext q_input q_weight bias stride padding output_padding dilation groups op_scale op_zero_point input input_scale _ _ = symbolic_helper dequantize_helper g q_input weight weight_scale _ axis = symbolic_helper dequantize_helper g q_weight q_bias = symbolic_helper requantize_bias_helper g bias input_scale weight_scale axis bias _ _ _ = symbolic_helper dequantize_helper g q_bias output = opset conv_transpose d g input weight bias stride padding output_padding groups dilation symbolic_helper quantize_helper g output op_scale op_zero_point _onnx_symbolic quantized conv_transpose d quantized_conv_transpose d g jit_utils GraphContext q_input q_weight bias stride padding output_padding dilation groups op_scale op_zero_point input input_scale _ _ = symbolic_helper dequantize_helper g q_input weight weight_scale _ axis = symbolic_helper dequantize_helper g q_weight q_bias = symbolic_helper requantize_bias_helper g bias input_scale weight_scale axis bias _ _ _ = symbolic_helper dequantize_helper g q_bias output = opset conv_transpose d g input weight bias stride padding output_padding groups dilation symbolic_helper quantize_helper g output op_scale op_zero_point