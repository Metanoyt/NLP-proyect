Owner s oncall jit operator unittest textwrap dedent typing Any List torch torch nn Tensor torch testing FileCheck torch testing _internal common_methods_invocations sample_inputs_cat_concat torch testing _internal common_utils make_tensor raise_on_run_directly torch testing _internal jit_utils execWrapper JitTestCase XXX still prototype TestSymbolicShapeAnalysis JitTestCase setUp super JitTestCase setUp prev_symbolic_shapes_test_enabled = torch _C _jit_symbolic_shapes_test_mode_enabled torch _C _jit_set_symbolic_shapes_test_mode True tearDown torch _C _jit_set_symbolic_shapes_test_mode prev_symbolic_shapes_test_enabled test_shape_analysis torch jit script foo x y x y inputs = list foo graph inputs prop_shapes_on_graph inp inp inputs setType inputs type with_sizes inp inputs setType inputs type with_sizes inp torch _C _jit_pass_propagate_shapes_on_graph foo graph prop_shapes_on_graph FileCheck check run foo graph None implicitly creates new symbolic symbol prop_shapes_on_graph None None None None None output_shape = foo graph findNode aten mul output type symbolic_sizes inp _shape = inputs type symbolic_sizes inp _shape = inputs type symbolic_sizes output shape dim should taken second inp dim other two dims we cannot infer given new symbolic shape assertEqual output_shape inp _shape assertFalse output_shape inp _shape + inp _shape assertFalse output_shape inp _shape + inp _shape XXX symbolic shapes represented increasing counter unique values use ` _new_symbolic_shape_symbol ` api instead specifying negative dimensions directly so there no chance collision between manual number current counter value sym = torch _C _new_symbolic_shape_symbol sym = torch _C _new_symbolic_shape_symbol sym = torch _C _new_symbolic_shape_symbol prop_shapes_on_graph sym sym sym sym output_shape = foo graph findNode aten mul output type symbolic_sizes assertEqual output_shape sym assertEqual output_shape sym assertEqual output_shape sym test_shared_shape_graph torch jit script foo x y x y x y mul_node = foo graph findNode aten mul div_node = foo graph findNode aten div mul_graph = torch _C _jit_shape_compute_graph_for_node mul_node div_graph = torch _C _jit_shape_compute_graph_for_node div_node assertIsNotNone mul_graph assertIs mul_graph div_graph test_write torch jit script foo b b broadcast appends cant removed so we bail propagation torch _C _jit_pass_propagate_shapes_on_graph foo graph FileCheck check Tensor = aten mul run foo graph torch jit script foo y x = x = y view x torch _C _jit_pass_propagate_shapes_on_graph foo graph FileCheck check Tensor = aten view run foo graph test_if_propagation torch jit script foo i int z x = torch ones y = z view z size i z size i i == x y torch _C _jit_pass_constant_propagation foo graph torch _C _jit_pass_propagate_shapes_on_graph foo graph view = foo graph findNode aten view neg_to_one li elem elem = - elem li assertEqual neg_to_one view output type symbolic_sizes - - if_out = next foo graph findNode prim If outputs assertEqual neg_to_one if_out type symbolic_sizes - - - test_unary_shape_functions unary_ops = torch nn functional hardtanh fn unary_ops t = torch jit trace fn torch rand ten_input = next t graph inputs ten_input setType ten_input type with_sizes torch _C _jit_pass_propagate_shapes_on_graph t graph assertEqual next t graph outputs type symbolic_sizes test_unary_shape_fns_inplace mul_inplace x torch Tensor y = x mul_ y unary_ops = mul_inplace fn unary_ops t = torch jit trace fn torch rand For some reason tracing erroring out t = torch jit script fn ten_input = next t graph inputs ten_input setType ten_input type with_sizes torch _C _jit_pass_propagate_shapes_on_graph t graph assertEqual next t graph outputs type symbolic_sizes test_binary_shape_functions binary_ops = operator __mul__ operator __truediv__ operator __gt__ operator __add__ fn binary_ops size_ = size_ = t = torch jit trace fn torch rand torch rand inputs = list t graph inputs inputs setType inputs type with_sizes size_ inputs setType inputs type with_sizes size_ torch _C _jit_pass_propagate_shapes_on_graph t graph assertEqual next t graph outputs type symbolic_sizes test_binary_shape_fns_inplace div_inplace_tensor x torch Tensor y torch Tensor z = x div_ y z add_inplace_tensor x torch Tensor y torch Tensor z = x add_ y z binary_ops = div_inplace_tensor add_inplace_tensor fn binary_ops size_ = x can t broadcast because s inplace op t = torch jit script fn inputs = list t graph inputs inputs setType inputs type with_sizes size_ Intentionally populate type inputs torch _C _jit_pass_propagate_shapes_on_graph t graph assertEqual next t graph outputs type symbolic_sizes test_size_and_sizes torch jit script foo x y x view y size y size - torch jit script foo x y x view y size graph foo graph foo graph inputs = list graph inputs sym = torch _C _new_symbolic_shape_symbol inputs setType inputs type with_sizes sym torch _C _jit_pass_propagate_shapes_on_graph graph assertEqual next graph outputs type symbolic_sizes sym test_adaptive_avg_pool d inps = None None None None inp inps t = torch randn inp out_size = torch nn functional adaptive_avg_pool d t inp size foo x torch nn functional adaptive_avg_pool d x inp fn = torch jit trace foo t torch _C _jit_erase_non_input_shape_information fn graph torch _C _jit_pass_peephole fn graph torch _C _jit_pass_constant_propagation fn graph checkShapeAnalysis out_size fn graph assert_propagation=True test_conv_deconv inp_shape weight_shape bias stride padding output_padding dilation groups mod None torch nn functional conv d None torch nn functional conv_transpose d None torch nn functional conv d None torch nn functional conv_transpose d None torch nn functional conv d None torch nn functional conv_transpose d inp = torch rand inp_shape weight = torch rand weight_shape mod torch nn functional conv d torch nn functional conv d torch nn functional conv d res = mod inp weight bias stride padding dilation groups size res = mod inp weight bias stride padding output_padding dilation groups size foo inp weight mod torch nn functional conv d torch nn functional conv d torch nn functional conv d mod inp weight bias stride padding dilation groups mod inp weight bias stride padding output_padding dilation groups fn = torch jit trace foo inp weight torch _C _jit_erase_non_input_shape_information fn graph torch _C _jit_pass_peephole fn graph torch _C _jit_pass_constant_propagation fn graph checkShapeAnalysis res fn graph assert_propagation=True test_arange_shape no opinfo tensor constructors inps = - - - True TODO https github com pytorch pytorch issues False TODO https github com pytorch pytorch issues + e- - e- - + e- - - - - - e- - inp inps funcs_template = dedent func torch arange args inp_s = str inp - remove tuple parens funcs_str = funcs_template format args=inp_s scope = execWrapper funcs_str globals scope cu = torch jit CompilationUnit funcs_str checkShapeAnalysis list cu func size cu func graph assert_propagation=True constant_prop=False test_shape_embedding_bag TODO merge into opinfos having difficulties there torch no_grad make_arg shape low=None high=None make_tensor shape device= cpu dtype=torch int low=low high=high requires_grad=False nn_inps = make_arg torch nn Embedding embedding_dim= max_norm= make_arg torch nn Embedding sparse=True make_arg torch nn Embedding sparse=True make_arg torch nn Embedding sparse=True make_arg torch nn Embedding max_norm= make_arg torch nn Embedding from_pretrained torch arange view max_norm= norm_type= scale_grad_by_freq=False sparse=True inp module nn_inps kwargs = weight module weight detach padding_idx module padding_idx max_norm module max_norm norm_type module norm_type scale_grad_by_freq module scale_grad_by_freq sparse module sparse out_size = torch nn functional embedding inp kwargs size foo x torch nn functional embedding inp kwargs fn = torch jit trace foo inp detach check_trace=False checkShapeAnalysis out_size fn graph assert_propagation=True constant_prop=False test_shape_concat TODO unify opinfo tests traces lists dont preserve sizes IR sample_inputs = sample_inputs_cat_concat None cpu torch float False CatMod nn Module __constants__ = dim __init__ dim= super __init__ dim = dim forward x y torch cat x y dim=self dim inp sample_inputs mod = torch jit script CatMod inp kwargs eval args = inp input This test hard-coded only work two sample inputs OpInfo may have more less len args = continue out_size = mod args size inps = list mod graph inputs inps setType inps type with_sizes args size inps setType inps type with_sizes args size checkShapeAnalysis out_size mod graph assert_propagation=True assert_shape_equal_scripted script_fn given_ins expected_res = script_fn given_ins g = script_fn graph graph_ins = list g inputs assertEqual len given_ins len graph_ins inp graph_in zip given_ins graph_ins graph_in setType graph_in type with_sizes inp size out_sizes = out size out expected_res checkShapeAnalysis out_sizes g assert_propagation=True test_convolution_backward No opinfos ops part Python API Also shapes input weight bias shape there no point really complicated test input = torch randn dtype=torch float device= cpu requires_grad=True weight = torch randn dtype=torch float device= cpu requires_grad=True out_grad = torch randn dtype=torch float device= cpu torch jit script conv_bwd input weight grad bias_sizes = args = False True True True torch ops aten convolution_backward grad input weight bias_sizes args assert_shape_equal_scripted conv_bwd input weight out_grad torch jit script conv_bwd_ input weight grad bias_sizes = None args = False True True True torch ops aten convolution_backward grad input weight bias_sizes args assert_shape_equal_scripted conv_bwd_ input weight out_grad test_returning_input_symbolic_shapes mm = torch jit freeze torch jit script nn Conv d stride= eval inps = list mm graph inputs inps setType inps type with_sizes None None None None shape_compute_graph = torch _C _jit_pass_propagate_shapes_on_graph_and_build_compute mm graph g = shape_compute_graph partial_eval_shape_graph make into jit function cant have multiple outputs g makeMultiOutputIntoTuple func = torch _C _create_function_from_graph partial_eval_graph g out = func first four outputs should unknown symbolic shapes input assertEqual out last two two new symbolic dims - height width assertEqual out list mm torch rand size test_partial_eval_graph_conv mm = torch jit freeze torch jit script nn Conv d stride= eval shape_compute_graph = torch _C _jit_pass_propagate_shapes_on_graph_and_build_compute mm graph output_sizes = mm graph findNode aten conv d output type symbolic_sizes calculating index i assertTrue output_sizes i assertTrue output_sizes = g = shape_compute_graph partial_eval_shape_graph make into jit function cant have multiple outputs g makeMultiOutputIntoTuple func = torch _C _create_function_from_graph partial_eval_graph g inp = torch randn output = func output_eager = list mm inp size o oe zip output output_eager + output_eager assertEqual o oe checkSymShapeCompute shape_compute_graph nodes node_output_sizes shape_inputs g = shape_compute_graph partial_eval_shape_graph assertTrue len list g inputs == len shape_inputs output_sym_map = shape_compute_graph graph_output_to_symbolic_shape_dim map sym shape - index sym_shape_to_index = index output enumerate g outputs sym_shape_to_index output_sym_map output = index g makeMultiOutputIntoTuple func = torch _C _create_function_from_graph partial_eval_graph g sym_outputs = func shape_inputs node output_shape zip nodes node_output_sizes output_type_sizes = node output type symbolic_sizes i sym_shape enumerate output_type_sizes sym_shape = assertEqual sym_shape output_shape i sym_shape_index = sym_shape_to_index sym_shape assertEqual sym_outputs sym_shape_index output_shape i test_partial_eval_stitching conv = torch nn Conv d kernel_size= stride= padding= bias=False max_pool = torch nn MaxPool d kernel_size= stride= padding= dilation= ceil_mode=False conv = nn Conv d kernel_size= stride= padding= bias=False mod = torch jit freeze torch jit script nn Sequential conv max_pool conv eval conv _output = conv torch rand max_pool_output = max_pool conv _output conv _output = conv max_pool_output shape_compute_graph = torch _C _jit_pass_propagate_shapes_on_graph_and_build_compute mod graph nodes = mod graph findNode aten max_pool d + list mod graph findAllNodes aten conv d output_shapes = max_pool_output size conv _output size conv _output size checkSymShapeCompute shape_compute_graph nodes output_shapes test_refinement_through_graph_stitching TwoConvs torch nn Module __init__ - None super __init__ conv = torch nn Conv d kernel_size= stride= padding= bias=False conv = torch nn Conv d kernel_size= stride= padding= bias=False forward x = conv x b = conv x + b mod = torch jit freeze torch jit script TwoConvs eval inp_tensor = list mod graph inputs inp_tensor setType inp_tensor type with_sizes None None None None torch _C _jit_pass_propagate_shapes_on_graph mod graph outs = list next mod graph outputs node inputs out = outs type symbolic_sizes out = outs type symbolic_sizes assertTrue out = out assertTrue out = out joining partial eval graphs both convs we able recognize output shapes equivalent torch _C _jit_pass_propagate_shapes_on_graph_and_build_compute mod graph out = outs type symbolic_sizes out = outs type symbolic_sizes assertEqual out out test_stitching_multi_output max_pool = torch nn MaxPool d kernel_size= stride= padding= dilation= ceil_mode=False return_indices=True tensor = torch rand mod = torch jit trace max_pool tensor mod = torch jit freeze mod eval inp = list mod graph inputs inp setType inp type with_sizes None None None None output_tensor = list mod tensor size run_pass lower_all_tuples mod graph shape_compute_graph = torch _C _jit_pass_propagate_shapes_on_graph_and_build_compute mod graph max_pool_node = mod graph findNode aten max_pool d_with_indices outs = list max_pool_node outputs assertEqual outs type symbolic_sizes outs type symbolic_sizes g = shape_compute_graph partial_eval_shape_graph make into jit function cant have multiple outputs g makeMultiOutputIntoTuple func = torch _C _create_function_from_graph partial_eval_graph g mapping = shape_compute_graph graph_output_to_symbolic_shape_dim noqa F output_shape = func tensor size first dims input sym dimensions then assertEqual list output_shape list tensor size assertEqual list output_shape output_tensor test_sym_ir_parsing graph_str = graph x Float SS - SS - int = prim Constant value= Tensor = aten add x x g = torch _C parse_ir graph_str inp = next g inputs out = inp type symbolic_sizes assertEqual out - - test_stitching_concat torch jit script foo b x y b + torch cat x y torch jit script foo b x y b + torch cat x y dim=- foo foo foo g = foo graph inp foo graph inputs inp setType inp type with_sizes None None shape_compute_graph = torch _C _jit_pass_propagate_shapes_on_graph_and_build_compute foo graph nodes = g findNode aten div + g findNode aten add + g findNode aten cat inps = output_shapes = checkSymShapeCompute shape_compute_graph nodes output_shapes inps unittest skipIf hasattr torch jit _shapes shape functions loaded python test_shape_function_includes inp_shape = weight_shape = bias = None stride = padding = dilation = groups = res = torch jit _shapes conv d inp_shape weight_shape bias stride padding dilation groups assertEqual res m _shape = m _shape = res = torch jit _shapes matmul m _shape m _shape assertEqual res test_register_function_error_checking will error before registering global map so no issue overwriting schema mappings torch jit script foo x y x + y node = foo graph findNode aten add torch jit script wrong_input_types x y x List int = x assertRaisesRegex RuntimeError Expected supertype int torch _C _jit_register_shape_compute_graph_for_node node wrong_input_types graph torch jit script wrong_output_types x List int y List int x List Tensor = x assertRaisesRegex RuntimeError got graph_type torch _C _jit_register_shape_compute_graph_for_node node wrong_output_types graph torch jit script too_many_inputs x List int y List int z Any z Any x List int = x assertRaises RuntimeError error torch _C _jit_register_shape_compute_graph_for_node node too_many_inputs graph assertTrue fewer arguments than schema str error exception test_cross_entropy_loss torch jit script foo x y torch ops aten cross_entropy_loss x y reduction= inputs = list foo graph inputs inputs setType inputs type with_sizes inputs setType inputs type with_sizes torch _C _jit_pass_propagate_shapes_on_graph foo graph assertEqual next foo graph outputs type sizes test_squeeze_dims torch jit script foo x torch ops aten squeeze x dim= input = next foo graph inputs input setType input type with_sizes torch _C _jit_pass_propagate_shapes_on_graph foo graph assertEqual next foo graph outputs type symbolic_sizes __name__ == __main__ raise_on_run_directly test test_jit py