Owner s module dynamo logging re traceback unittest unittest mock warnings functools lru_cache torch torch _dynamo torch _dynamo config torch _dynamo test_case torch utils _pytree python_pytree torch _dynamo exc ResumePrologueTracingError Unsupported torch _dynamo testing skipIfNotPy skipIfOnlyNotPy torch _dynamo utils counters torch testing _internal common_utils IS_FBCODE munge_exc scoped_load_inline torch testing _internal logging_utils LoggingTestCase make_logging_test NOTE Adding tests file It good practice add minimal repro each graph break site i e ` unimplemented ` call make sure there aren t any errors occur when generating graph break messages If graph break message test fails because graph break no longer repros good practice find new minimal repro causes graph break If too much work likely safe skip remove test assuming previously passing graph break message changed However you add new graph break modify graph break message you should make sure there test GenericCtxMgr __enter__ __exit__ exc_type exc_value traceback pass ErrorMessagesTest LoggingTestCase test_dynamic_shape_operator_no_meta_kernel fn torch linalg lstsq torch rand torch rand torch _dynamo config patch capture_dynamic_output_shape_ops=True assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True \ Dynamic shape operator no meta kernel Explanation Operator ` aten linalg_lstsq default ` does have meta kernel supports dynamic output shapes Hint Please report issue PyTorch Developer debug context aten linalg_lstsq default For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn torch linalg lstsq torch rand torch rand test_data_dependent_operator fn x torch equal x x torch _dynamo config patch capture_scalar_outputs=True assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True torch ones \ Data dependent operator Explanation Operator ` aten equal default ` has non-Tensor output whose value dependent data Tensor inputs Hint Consider wrapping operator into PyTorch-understood custom operator see https pytorch org tutorials advanced custom_ops_landing_page html Developer debug context aten equal default For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn torch equal x x test_sort_with_nonconstant_keys lst = torch tensor torch tensor torch tensor torch tensor fn lst sorted lst assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True lst \ sort non-constant keys Explanation Cannot perform sort non-constant key First non-constant key type torch Tensor Most notably we cannot sort Tensor SymInt keys we can sort ints Hint Use something key Developer debug context LazyVariableTracker realized TensorVariable For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn sorted lst test_super_call_method fn x + x assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True zip range range \ Unsupported method call Explanation Dynamo does know how trace method ` __iter__ ` ` zip ` Hint Avoid calling ` zip __iter__ ` your code Hint Please report issue PyTorch Hint Dynamo does fully support tracing builtin iterators e g ` map ` ` zip ` ` enumerate ` passed uncompiled compiled regions e g ` torch compile fn enumerate ` This can happen unintentionally previous graph break happens builtin iterator local scope Hint List dict comprehensions Python = result implicit function calls which Dynamo cannot trace top level frame Possible workarounds use loop instead comprehension fix any graph breaks function above comprehension wrap comprehension function use Python + Developer debug context call_method UserDefinedObjectVariable zip __iter__ For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn x + x test_dict_items_input fn x items = iter items next x sin x = torch randn dct = b assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True x dct items \ Unsupported method call Explanation Dynamo does know how trace method ` __iter__ ` ` dict_items ` Hint Avoid calling ` dict_items __iter__ ` your code Hint Please report issue PyTorch Hint Consider moving creation dict view object e g ` dict keys ` ` dict items ` compiled region instead passing input compiled region Hint Dynamo does fully support tracing builtin iterators e g ` map ` ` zip ` ` enumerate ` passed uncompiled compiled regions e g ` torch compile fn enumerate ` This can happen unintentionally previous graph break happens builtin iterator local scope Hint List dict comprehensions Python = result implicit function calls which Dynamo cannot trace top level frame Possible workarounds use loop instead comprehension fix any graph breaks function above comprehension wrap comprehension function use Python + Developer debug context call_method UserDefinedObjectVariable dict_items __iter__ For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn = iter items test_super_call_function fn x + x assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True zip range range \ Unsupported function call Explanation Dynamo does know how trace function ` UserDefinedObjectVariable zip ` Hint Avoid calling ` UserDefinedObjectVariable zip ` your code Hint Please report issue PyTorch Developer debug context call_function UserDefinedObjectVariable zip For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn x + x test_unsupported_context fn obj obj assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True \ Unsupported context manager Explanation Dynamo does know how enter ` int ` context manager Hint Avoid using unsupported context manager Hint If context manager seems like should supported e g torch set_grad_enabled then may case created outside compiled region which Dynamo does support Supported context managers can cross graph break boundaries only they local non-closure variables intermediate values Hint File issue PyTorch Simple context managers can potentially supported note context managers can t supported general Developer debug context Attempted SETUP_WITH BEFORE_WITH LOAD_SPECIAL LazyVariableTracker realized ConstantVariable int For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn obj test_backend_fake_tensor_exc bad_backend gm ex raise torch _subclasses fake_tensor UnsupportedFakeTensorException test fn x x + assertExpectedInlineMunged Unsupported lambda torch compile fn backend=bad_backend fullgraph=True torch ones \ Backend compiler exception Explanation Backend compiler ` bad_backend ` failed test Adding graph break Hint Report issue backend compiler repo Developer debug context Backend bad_backend Exception test Traceback File test_error_messages py line N fn x + For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html test_unsupported_builtin fn print abc assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True \ Failed trace builtin operator Explanation Dynamo does know how trace builtin operator ` print ` argument types str has_kwargs False Hint Avoid calling builtin ` print ` argument types str Consider using equivalent alternative function method ` print ` Hint If you attempting call logging function e g ` print ` you can try adding ` torch _dynamo config reorderable_logging_functions ` Hint Please report issue PyTorch Developer debug context builtin print torch _dynamo variables constant ConstantVariable False For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn print abc test_skipfile_call fn unittest skip test post_munge s re sub r file ` case\ py ` file ` case py ` s assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True \ Attempted call function marked skipped Explanation Dynamo developers have intentionally marked function ` skip ` file ` case py ` should traced Hint Avoid calling function ` skip ` Hint Apply ` torch _dynamo dont_skip_tracing ` function ` skip ` force tracing into function More graph breaks may occur result attempting trace into function Hint Please file issue PyTorch Developer debug context module unittest case qualname skip skip reason missing reason For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn unittest skip test post_munge=post_munge test_skipfile_dynamo_call fn torch _dynamo disable assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True \ Attempted call function marked skipped Explanation Dynamo developers have intentionally marked function ` disable ` file ` _dynamo decorators py ` should traced Hint Avoid calling function ` disable ` Developer debug context module torch _dynamo decorators qualname disable skip reason missing reason For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn torch _dynamo disable test_skipfile_inline Foo fn = unittest skip fn Foo fn post_munge s re sub r ` case\ py ` ` case py ` s assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True \ Attempted inline function marked skipped Explanation Dynamo developers have intentionally marked function ` skip ` should traced Hint Avoid calling function ` skip ` Hint Apply ` torch _dynamo dont_skip_tracing ` function ` skip ` force tracing into function More graph breaks may occur result attempting trace into function Hint Please file issue PyTorch Developer debug context qualname skip name skip filename ` case py ` skip reason skipped according trace_rules lookup unittest For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn Foo fn post_munge=post_munge test_dynamo_graph_break_fn fn torch _dynamo graph_break assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True \ Call ` torch _dynamo graph_break ` Explanation User-inserted graph break Message None Hint Remove ` torch _dynamo graph_break ` call Developer debug context Called ` torch _dynamo graph_break ` args ` ` kwargs ` ` For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn torch _dynamo graph_break test_dynamo_graph_break_fn_with_msg fn torch _dynamo graph_break msg= test graph break assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True \ Call ` torch _dynamo graph_break ` Explanation User-inserted graph break Message test graph break Hint Remove ` torch _dynamo graph_break ` call Developer debug context Called ` torch _dynamo graph_break ` args ` ` kwargs ` msg ConstantVariable str test graph break ` For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn torch _dynamo graph_break msg= test graph break test_warnings fn warnings warn test assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True \ Attempted call function marked skipped Explanation Dynamo does know how trace Python builtin ` _warnings warn ` Hint If you attempting call logging function e g ` _warnings warn ` you can try adding ` torch _dynamo config reorderable_logging_functions ` Hint Please file issue GitHub so PyTorch team can add support Developer debug context module _warnings qualname warn skip reason missing reason For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn warnings warn test unittest skipIf python_pytree _cxx_pytree_exists missing optree package test_optree_graph_break_message optree torch compile backend= eager fn x d = optree tree_flatten_with_path d torch sin x fn torch randn assertEqual len counters graph_break first_graph_break = next iter counters graph_break keys assertExpectedInline first_graph_break \ Attempted call function marked skipped Explanation Dynamo cannot trace optree C C++ function optree _C PyCapsule flatten_with_path Hint Consider using torch utils _pytree - https github com pytorch pytorch blob main torch utils _pytree py Developer debug context module optree _C qualname PyCapsule flatten_with_path skip reason missing reason For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html scoped_load_inline torch _dynamo config patch inline_inbuilt_nn_modules=False unittest skipIf IS_FBCODE inline cpp_extension doesn t work fbcode test_cpp_extension_recommends_custom_ops load_inline cpp_source = #include torch extension h Tensor foobar const Tensor x x clone module = load_inline name= mylib cpp_sources=cpp_source functions= foobar verbose=True x = torch ones requires_grad=True counters clear torch compile backend= eager f x module foobar x assertWarnsOnceRegex UserWarning s https pytorch org tutorials advanced custom_ops_landing_page html f x assertEqual len counters graph_break first_graph_break = next iter counters graph_break keys first_graph_break = re sub r mylib _v\d+ mylib first_graph_break HACK patches around fact PyBind improperly sets __qualname__ attribute functions methods see https github com pybind pybind issues This should removed issue fixed first_graph_break = re sub r pybind _detail_function_record_v ^ + PyCapsule first_graph_break assertExpectedInline first_graph_break \ Attempted call function marked skipped Explanation Dynamo does know how trace builtin ` mylib PyCapsule foobar ` This function either Python builtin e g _warnings warn third-party C C++ Python extension perhaps created pybind Hint If Python builtin please file issue GitHub so PyTorch team can add support see next case workaround Hint If third-party C C++ Python extension please either wrap into PyTorch-understood custom operator see https pytorch org tutorials advanced custom_ops_landing_page html more details traceable use ` torch compiler allow_in_graph ` Developer debug context module mylib qualname PyCapsule foobar skip reason missing reason For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html cpp_source = #include torch extension h Tensor baz const Tensor x x clone module = load_inline name= mylib cpp_sources=cpp_source functions= baz verbose=True torch _dynamo reset Test each warning only happens once torch compile backend= eager f x module baz x module foobar x module foobar x module baz x module foobar x module baz x x clone warnings catch_warnings record=True ws warnings simplefilter always f x f x assertEqual len ws test_observed_exception fn raise RuntimeError test assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True \ Observed exception Explanation Dynamo found no exception handler top-level compiled function when encountering exception Exception will propagate outside compiled region Hint Dynamo has detected tracing code will result error when running eager Please double check your code doesn t contain similar error when actually running eager uncompiled Hint It may possible write Dynamo tracing rules code Please report issue PyTorch you encounter graph break often causing performance issues Developer debug context raised exception RuntimeError ConstantVariable str test For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn raise RuntimeError test test_uninitialized_module Foo torch nn Module __init__ pass fn mod mod assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True Foo \ Uninitialized nn Module Explanation Attempted trace uninitialized nn Module type Foo Hint Dynamo has detected tracing code will result error when running eager Please double check your code doesn t contain similar error when actually running eager uncompiled Hint Ensure your nn Module instance has called ` super __init__ ` Developer debug context Foo For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn mod torch _dynamo config patch inline_inbuilt_nn_modules=False test_class_property Foo torch nn Module attr = unittest fn mod x mod attr assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True Foo torch randn \ Unsupported nn Module attribute type Explanation Dynamo does support tracing nn Module attributes type ` module ` Hint Refactor your code so ` attr ` type ` module ` attribute ` Foo ` Hint Currently supported attribute types methods classmethods staticmethods properties constants tensors Hint It may possible write Dynamo tracing rules code Please report issue PyTorch you encounter graph break often causing performance issues Developer debug context nn Module subclass Foo name attr attribute type module For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn mod attr test_generic_ctx_mgr_graph_break fn GenericCtxMgr GenericCtxMgr pass GenericCtxMgr GenericCtxMgr pass torch _dynamo graph_break assertRaises Unsupported cm torch compile fn backend= eager fullgraph=True assertExpectedInline munge_exc cm exception suppress_suffix=True skip= \ Graph break under GenericContextWrappingVariable Explanation Attempted graph break active context manager s doesn t support graph breaking Hint Move offending context manager s outside compiled region Hint This graph break may have been caused earlier graph break Resolving earlier graph break may resolve one Developer debug context Active generic context managers GenericContextWrappingVariable GenericCtxMgr GenericContextWrappingVariable GenericCtxMgr For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn torch _dynamo graph_break assertExpectedInline munge_exc cm exception __cause__ suppress_suffix=True skip= \ Call ` torch _dynamo graph_break ` Explanation User-inserted graph break Message None Hint Remove ` torch _dynamo graph_break ` call Developer debug context Called ` torch _dynamo graph_break ` args ` ` kwargs ` ` For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html test_load_build_class fn Foo pass Foo assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True \ Attempted call function marked skipped Explanation Dynamo does know how trace builtin ` builtins __build_class__ ` This function either Python builtin e g _warnings warn third-party C C++ Python extension perhaps created pybind Hint If Python builtin please file issue GitHub so PyTorch team can add support see next case workaround Hint If third-party C C++ Python extension please either wrap into PyTorch-understood custom operator see https pytorch org tutorials advanced custom_ops_landing_page html more details traceable use ` torch compiler allow_in_graph ` Developer debug context module builtins qualname __build_class__ skip reason missing reason For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn Foo skipIfNotPy test_unsupported_bytecode async fn async i range print i post_munge s s = re sub r x - A-Fa-f + xmem_addr s s = re sub r Instruction\ opname= GET_AITER \ \n Instruction GET_AITER s s assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True \ Missing bytecode handler Explanation Dynamo does know how handle bytecode instruction ` GET_AITER ` Hint Do trace code produces ` GET_AITER ` bytecode instruction see https docs python org library dis html bytecode semantics Hint It may possible write Dynamo tracing rules code Please report issue PyTorch you encounter graph break often causing performance issues Developer debug context GET_AITER args torch _dynamo symbolic_convert InstructionTranslator object xmem_addr Instruction GET_AITER For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn async i range post_munge=post_munge test_reconstruction_failure Foo meth fn Foo meth post_munge s re sub r x - A-Fa-f + xmem_addr s assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True \ Reconstruction failure Explanation Dynamo has no bytecode reconstruction implemented sourceless variable UserMethodVariable function ErrorMessagesTest test_reconstruction_failure locals Foo meth xmem_addr UserDefinedObjectVariable Foo Hint If Dynamo attempting trace statement your code attempting variable Dynamo cannot reconstruct then remove statement Hint This graph break may have been caused earlier graph break Resolving earlier graph break may resolve one Hint Report issue PyTorch you need reconstrtuction support Note objects don t have reconstruction rules may fundamentally unreconstructable Developer debug context UserMethodVariable function ErrorMessagesTest test_reconstruction_failure locals Foo meth xmem_addr UserDefinedObjectVariable Foo For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn Foo meth post_munge=post_munge make_logging_test graph_breaks=True test_reconstruction_failure_gb records Foo meth fn f = Foo meth torch _dynamo graph_break f post_munge s re sub r x - A-Fa-f + xmem_addr s torch compile fn backend= eager assertExpectedInline post_munge munge_exc records getMessage suppress_suffix=True skip= \ Graph break user code test_error_messages py N Graph Break Reason Call ` torch _dynamo graph_break ` Explanation User-inserted graph break Message None Hint Remove ` torch _dynamo graph_break ` call Developer debug context Called ` torch _dynamo graph_break ` args ` ` kwargs ` ` For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html User code traceback File test_error_messages py line N test_reconstruction_failure_gb torch compile fn backend= eager File test_error_messages py line N fn torch _dynamo graph_break assertExpectedInline post_munge munge_exc records exc_info suppress_suffix=True skip= \ Reconstruction failure Explanation Dynamo has no bytecode reconstruction implemented sourceless variable UserMethodVariable function ErrorMessagesTest test_reconstruction_failure_gb locals Foo meth xmem_addr UserDefinedObjectVariable Foo Hint If Dynamo attempting trace statement your code attempting variable Dynamo cannot reconstruct then remove statement Hint This graph break may have been caused earlier graph break Resolving earlier graph break may resolve one Hint Report issue PyTorch you need reconstrtuction support Note objects don t have reconstruction rules may fundamentally unreconstructable Developer debug context UserMethodVariable function ErrorMessagesTest test_reconstruction_failure_gb locals Foo meth xmem_addr UserDefinedObjectVariable Foo For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn torch _dynamo graph_break test_faketensor_nyi torch library custom_op mylib foo mutates_args= foo x torch Tensor - torch Tensor x sin foo register_fake _ x raise NotImplementedError fn x torch ops mylib foo x assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True torch randn \ NotImplementedError UnsupportedFakeTensorException when running FX node Explanation Dynamo failed run FX node fake tensors call_function mylib foo FakeTensor size= got NotImplementedError Hint If op PyTorch op please file issue PyTorch Developer debug context For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn torch ops mylib foo x test_data_dependent_branching_fullgraph fn x x sum x sin x cos assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True torch randn \ Data-dependent branching Explanation Detected data-dependent branching e g ` my_tensor sum ` Dynamo does support tracing dynamic control flow Hint This graph break fundamental - unlikely Dynamo will ever able trace through your code Consider finding workaround Hint Use ` torch cond ` express dynamic control flow Developer debug context attempted jump TensorVariable For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn x sum Test bytecode source attribution correct VariableTracker make_logging_test trace_bytecode=True test_variable_tracker_source_attribution records inner x x + torch compile backend= eager fn x x = inner x inner x fn torch ones find_trace_bytecode_lines long_string Split string into lines lines = long_string split \n More comprehensive pattern capture LazyVariableTracker info pattern = r LazyVariableTracker\ ^ \ Find all lines containing pattern result = line line lines re search pattern line result Get all log messages just last one all_messages = record records msg = munge_exc record getMessage skip= all_messages append msg Combine all messages search through combined_msg = \n join all_messages all_lines = find_trace_bytecode_lines combined_msg For now just check we found some lines LazyVariableTracker assertGreater len all_lines Should find least one LazyVariableTracker line assertIn LazyVariableTracker unrealized function all_lines assertIn LazyVariableTracker realized UserFunctionVariable all_lines make_logging_test graph_breaks=True test_data_dependent_branching_gb records fn x x sum x sin x cos torch compile fn backend= eager torch randn assertExpectedInline munge_exc records getMessage suppress_suffix=True skip= \ Graph break user code test_error_messages py N Graph Break Reason Data-dependent branching Explanation Detected data-dependent branching e g ` my_tensor sum ` Dynamo does support tracing dynamic control flow Hint This graph break fundamental - unlikely Dynamo will ever able trace through your code Consider finding workaround Hint Use ` torch cond ` express dynamic control flow Developer debug context attempted jump TensorVariable User code traceback File test_error_messages py line N test_data_dependent_branching_gb torch compile fn backend= eager torch randn File test_error_messages py line N fn x sum unittest skipIf IS_FBCODE assert gets patched internal pytest make_logging_test graph_breaks=True test_assert_failure_in_generic_ctx_mgr records fn x GenericCtxMgr assert x None assertRaises AssertionError torch compile fn backend= eager torch randn only graph break message assertEqual len records assertExpectedInline munge_exc records getMessage suppress_suffix=True skip= \ Graph break skip user code File test_error_messages py line N fn assert x None assertExpectedInline munge_exc records exc_info suppress_suffix=True skip= \ Data-dependent assertion failed cannot compile partial graph Explanation Dynamo has determined when encountering data-dependent assert failure should compile partial graph Hint This graph break fundamental - unlikely Dynamo will ever able trace through your code Consider finding workaround Hint Use ` torch _assert ` raise hard AssertionError when check fails This error will propagate back user code called compiled function i e Dynamo will trace any exception handling Hint Remove assert statement Hint Move assert statement outside any context managers order graph break partial graph compilation fullgraph=False Developer debug context value ConstantVariable bool False For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn assert x None test_no_internal_compiler_stacktrace fn gn gn torch _dynamo graph_break assertRaises suppresses traceback so manually catch e = None try torch compile fn backend= eager fullgraph=True except Exception exn e = exn assertIsNotNone e msg = join traceback format_exception type e e e __traceback__ only keep filenames traceback msg = re sub r File \W \w+\ py File \\ msg remove line numbers msg = re sub r line \d+ line N msg remove carets msg = re sub r \n\s ~ \^+\n \n msg assertExpectedInline msg \ Traceback most recent call last File test_error_messages py line N test_no_internal_compiler_stacktrace torch compile fn backend= eager fullgraph=True File eval_frame py line N compile_wrapper raise e with_traceback None e __cause__ User compiler error torch _dynamo exc Unsupported Call ` torch _dynamo graph_break ` Explanation User-inserted graph break Message None Hint Remove ` torch _dynamo graph_break ` call Developer debug context Called ` torch _dynamo graph_break ` args ` ` kwargs ` ` For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn gn File test_error_messages py line N gn torch _dynamo graph_break Set TORCHDYNAMO_VERBOSE= internal stack trace please do especially you re reporting bug PyTorch For even more developer context set TORCH_LOGS= +dynamo torch _dynamo config patch verbose=True test_internal_compiler_stacktrace_verbose fn gn gn torch _dynamo graph_break assertRaises suppresses traceback so manually catch e = None try torch compile fn backend= eager fullgraph=True except Exception exn e = exn assertIsNotNone e msg = join traceback format_exception type e e e __traceback__ only keep filenames traceback msg = re sub r File \W \w+\ py File \\ msg remove line numbers msg = re sub r line \d+ line N msg msg = re sub r s Traceback \ most recent call last\ File exc py line N unimplemented_v raise Unsupported\ msg\ Internal traceback \n msg assertExpectedInline msg \ Internal traceback torch _dynamo exc Unsupported Call ` torch _dynamo graph_break ` Explanation User-inserted graph break Message None Hint Remove ` torch _dynamo graph_break ` call Developer debug context Called ` torch _dynamo graph_break ` args ` ` kwargs ` ` For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn gn File test_error_messages py line N gn torch _dynamo graph_break make_logging_test graph_breaks=True test_nested_compile_user_frames records fn x gn x + gn x hn x + hn x torch _dynamo graph_break torch _dynamo graph_break torch compile fn backend= eager torch randn assertExpectedInline munge_exc records - getMessage skip= \ Graph break user code test_error_messages py N Graph Break Reason Call ` torch _dynamo graph_break ` Explanation User-inserted graph break Message None Hint Remove ` torch _dynamo graph_break ` call Developer debug context Called ` torch _dynamo graph_break ` args ` ` kwargs ` ` For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html User code traceback File test_error_messages py line N test_nested_compile_user_frames torch compile fn backend= eager torch randn File test_error_messages py line N fn gn x + File test_error_messages py line N gn hn x + File test_error_messages py line N hn torch _dynamo graph_break torch _dynamo config patch verbose=True make_logging_test graph_breaks=True test_latest_bytecode_to_graph_break_fullgraph records fn x y = x + z = x + y torch _dynamo graph_break z assertExpectedInlineMunged Unsupported lambda torch compile fn backend= eager fullgraph=True torch randn \ Call ` torch _dynamo graph_break ` Explanation User-inserted graph break Message None Hint Remove ` torch _dynamo graph_break ` call Developer debug context Called ` torch _dynamo graph_break ` args ` ` kwargs ` ` For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn torch _dynamo graph_break skipIfOnlyNotPy torch _dynamo config patch verbose=True make_logging_test graph_breaks=True test_latest_bytecode_to_graph_break_python_versioning records torch compile backend= eager fn x y = x + z = x + y torch _dynamo graph_break z fn torch ones s = munge_exc records getMessage skip= assertExpectedInline s \ Graph break user code test_error_messages py N Graph Break Reason Call ` torch _dynamo graph_break ` Explanation User-inserted graph break Message None Hint Remove ` torch _dynamo graph_break ` call Developer debug context Called ` torch _dynamo graph_break ` args ` ` kwargs ` ` For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html User code traceback File test_error_messages py line N test_latest_bytecode_to_graph_break_python_versioning fn torch ones ========== most recent ` torch compile ` tracing attempt started here ========== File test_error_messages py line N fn torch _dynamo graph_break NOTE most recent ` torch compile ` tracing attempt might where you applied ` torch compile ` This due how graph breaks implemented - optimized code object returned Dynamo will call another Dynamo-generated resume function tracing re-enabled calling resume function normal Python function which Dynamo intercepts top-level frame Most recent bytecode instructions traced max TRACE RESUME TRACE LOAD_FAST x TRACE LOAD_CONST LazyVariableTracker unrealized torch Tensor TRACE BINARY_OP LazyVariableTracker unrealized torch Tensor ConstantVariable int TRACE STORE_FAST y TensorVariable TRACE LOAD_FAST x TRACE LOAD_FAST y TensorVariable TRACE BINARY_OP TensorVariable TensorVariable TRACE STORE_FAST z TensorVariable TRACE LOAD_GLOBAL torch TRACE LOAD_ATTR _dynamo LazyVariableTracker unrealized module TRACE LOAD_ATTR graph_break LazyVariableTracker unrealized module TRACE CALL NullVariable LazyVariableTracker unrealized function torch _dynamo config patch verbose=True make_logging_test graph_breaks=True test_latest_bytecode_to_graph_break records torch compile backend= eager fn x y = x + z = x + y torch _dynamo graph_break z fn torch ones pattern = r TRACE s = munge_exc records getMessage skip= matches = re findall pattern s assertEqual len matches True assertEqual len matches = True assertIn Most recent bytecode instructions traced max s torch _dynamo config patch verbose=True make_logging_test graph_breaks=True test_graph_break_traceback_above_dynamo_shows_user_code records torch compile backend= eager NOTE comments test used differentiate lines f x torch _dynamo graph_break torch _dynamo graph_break torch _dynamo graph_break torch compile backend= eager f x x sum x = x + x sum x = x + x sum x = x + Foo __setattr__ name value torch _dynamo graph_break torch compile backend= eager f x Foo attr = x Foo attr = x Foo attr = x f torch randn assertIn torch _dynamo graph_break records - getMessage assertIn torch _dynamo graph_break records - getMessage f torch ones assertIn x sum records - getMessage assertIn x sum records - getMessage f torch randn assertIn Foo attr = x records - getMessage assertIn Foo attr = x records - getMessage post_munge s s = re sub r torch_dynamo_resume_in_f \d _at_ \d+ r torch_dynamo_resume_in_f\ _at_N s remove most recent bytecode instructions DOTALL needed entirely remove TRACE lines including newline re sub r TRACE $ s flags=re DOTALL assertExpectedInline post_munge munge_exc records - getMessage skip= \ Graph break user code test_error_messages py N Graph Break Reason Encountered graph break when attempting store object s attribute STORE_ATTR Call ` torch _dynamo graph_break ` Explanation User-inserted graph break Message None Hint Remove ` torch _dynamo graph_break ` call Developer debug context Called ` torch _dynamo graph_break ` args ` ` kwargs ` ` For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html User code traceback File test_error_messages py line N test_graph_break_traceback_above_dynamo_shows_user_code f torch randn File test_error_messages py line N f Foo attr = x File test_error_messages py line N torch_dynamo_resume_in_f _at_N Foo attr = x ========== most recent ` torch compile ` tracing attempt started here ========== File test_error_messages py line N torch_dynamo_resume_in_f _at_N Foo attr = x File test_error_messages py line N __setattr__ torch _dynamo graph_break NOTE most recent ` torch compile ` tracing attempt might where you applied ` torch compile ` This due how graph breaks implemented - optimized code object returned Dynamo will call another Dynamo-generated resume function tracing re-enabled calling resume function normal Python function which Dynamo intercepts top-level frame Most recent bytecode instructions traced max make_logging_test graph_breaks=True test_graph_break_traceback_collapsed_resume_frames records torch compile backend= eager f x torch _dynamo graph_break torch _dynamo graph_break torch _dynamo graph_break f x f x torch _dynamo graph_break torch _dynamo graph_break torch _dynamo graph_break f x f x torch _dynamo graph_break torch _dynamo graph_break torch _dynamo graph_break correct x + f torch randn assertExpectedInline munge_exc records - getMessage skip= \ Graph break user code test_error_messages py N Graph Break Reason Call ` torch _dynamo graph_break ` Explanation User-inserted graph break Message None Hint Remove ` torch _dynamo graph_break ` call Developer debug context Called ` torch _dynamo graph_break ` args ` ` kwargs ` ` For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html User code traceback File test_error_messages py line N test_graph_break_traceback_collapsed_resume_frames f torch randn File test_error_messages py line N f f x File test_error_messages py line N f f x File test_error_messages py line N f torch _dynamo graph_break correct make_logging_test dynamo=logging DEBUG test_lru_cache_warning_logs_user_stack_trace records lru_cache foo x x + torch compile foo backend= eager torch randn lru_cache_log = None record records call lru_cache wrapped function record getMessage lru_cache_log = record getMessage break assertIsNotNone lru_cache_log No lru_cache warning logged assertExpectedInline munge_exc lru_cache_log \ call lru_cache wrapped function _dynamo external_utils py N File test_error_messages py line N test_lru_cache_warning_logs_user_stack_trace torch compile foo backend= eager torch randn make_logging_test dynamo=logging DEBUG test_lru_cache_warning_logs_nested_call records lru_cache foo x x + nested x foo x torch compile nested backend= eager torch randn lru_cache_log = None record records call lru_cache wrapped function record getMessage lru_cache_log = record getMessage break assertIsNotNone lru_cache_log No lru_cache warning logged assertExpectedInline munge_exc lru_cache_log \ call lru_cache wrapped function test_error_messages py N File test_error_messages py line N test_lru_cache_warning_logs_nested_call torch compile nested backend= eager torch randn File test_error_messages py line N nested foo x test_disable_message torch compile backend= eager fullgraph=True outer fn x fn x torch compiler disable f x x + post_munge s re sub r x - A-Fa-f + xmem_addr s assertExpectedInlineMunged Unsupported lambda outer f torch randn \ Skip calling ` torch compiler disable ` d function Explanation Skip calling function ` function ErrorMessagesTest test_disable_message locals f xmem_addr ` since wrapped ` torch compiler disable ` reason None Hint Remove ` torch compiler disable ` call Developer debug context function ErrorMessagesTest test_disable_message locals f xmem_addr For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N outer fn x post_munge=post_munge torch compiler disable reason= test message g x x + assertExpectedInlineMunged Unsupported lambda outer g torch randn \ Skip calling ` torch compiler disable ` d function Explanation Skip calling function ` function ErrorMessagesTest test_disable_message locals g xmem_addr ` since wrapped ` torch compiler disable ` reason test message Hint Remove ` torch compiler disable ` call Developer debug context function ErrorMessagesTest test_disable_message locals g xmem_addr For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N outer fn x post_munge=post_munge Mod torch nn Module forward x x + mod = Mod mod compile mod = torch compiler disable mod reason= test message assertExpectedInlineMunged Unsupported lambda outer mod torch randn \ Unsupported function call delayed Explanation Dynamo determined graph break should occur when calling ` L fn ` Reason Optimized ` nn Module ` wrapped ` torch compiler disable ` reason test message Developer debug context source LocalSource local_name= fn is_input=True dynamism=None is_derefed_cell_contents=False For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N outer fn x post_munge=post_munge Test errors while tracing resume function prologues do get suppressed test_graph_break_in_buggy_resume_prologue torch _dynamo bytecode_transformation bt torch _dynamo resume_execution rex NOTE do define non_global global file torch compile backend= eager fn non_global non_global = non_global + torch _dynamo graph_break non_global + orig_clean_and_assemble_instructions = bt clean_and_assemble_instructions bad_clean_and_assemble_instructions instructions args Inject invalid LOAD_GLOBAL after first STORE_FAST IS_TRACING_RESUME_PROLOGUE_VARNAME i inst enumerate instructions inst opname == STORE_FAST inst argval == rex IS_TRACING_RESUME_PROLOGUE_VARNAME instructions = instructions i + + should cause graph break bt create_instruction LOAD_GLOBAL argval= non_global + instructions i + break orig_clean_and_assemble_instructions instructions args unittest mock patch torch _dynamo bytecode_transformation clean_and_assemble_instructions bad_clean_and_assemble_instructions assertRaisesRegex ResumePrologueTracingError Error while tracing through Dynamo-generated resume function prologue fn torch randn make_logging_test graph_breaks=True test_step_graph_break records torch compile backend= eager fn x x = x + x = x + torch _dynamo step_unsupported x + fn torch ones assertExpectedInline munge_exc records getMessage suppress_suffix=True skip= \ Graph break user code test_error_messages py N Graph Break Reason Encountered graph break we cannot resume Compiling up previous resumable state then skipping rest function Graph break encountered User code traceback File test_error_messages py line N test_step_graph_break fn torch ones File test_error_messages py line N fn torch _dynamo step_unsupported torch _dynamo reset torch _dynamo error_on_graph_break True assertExpectedInlineMunged Unsupported lambda fn torch ones \ cannot resume torch _dynamo step_unsupported Explanation traced torch _dynamo step_unsupported Dynamo instructed error graph break This graph break used debugging only Hint Remove torch _dynamo step_unsupported call Hint Make sure fullgraph=False error_on_graph_break=False Hint This likely Dynamo bug Please report issue PyTorch Developer debug context For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn torch _dynamo step_unsupported make_logging_test graph_breaks=True test_store_attr_graph_break records Foo __setattr__ name value torch _dynamo graph_break torch compile backend= eager fn x Foo attr = x fn torch ones assertExpectedInline munge_exc records getMessage suppress_suffix=True skip= \ Graph break user code test_error_messages py N Graph Break Reason Encountered graph break when attempting store object s attribute STORE_ATTR Call ` torch _dynamo graph_break ` Explanation User-inserted graph break Message None Hint Remove ` torch _dynamo graph_break ` call Developer debug context Called ` torch _dynamo graph_break ` args ` ` kwargs ` ` For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html User code traceback File test_error_messages py line N test_store_attr_graph_break fn torch ones File test_error_messages py line N fn Foo attr = x File test_error_messages py line N __setattr__ torch _dynamo graph_break torch _dynamo reset torch _dynamo error_on_graph_break True assertExpectedInlineMunged Unsupported lambda fn torch ones \ Call ` torch _dynamo graph_break ` Explanation User-inserted graph break Message None Hint Remove ` torch _dynamo graph_break ` call Developer debug context Called ` torch _dynamo graph_break ` args ` ` kwargs ` ` For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_error_messages py line N fn Foo attr = x File test_error_messages py line N __setattr__ torch _dynamo graph_break __name__ == __main__ torch _dynamo test_case run_tests run_tests