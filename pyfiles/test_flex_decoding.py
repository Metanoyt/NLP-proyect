Owner s module inductor flake noqa B functools sys unittest collections namedtuple collections abc Callable typing Optional Union unittest expectedFailure unittest mock patch torch torch _inductor test_case TestCase InductorTestCase torch _inductor utils run_and_get_code torch nn attention experimental _paged_attention PagedAttention torch nn attention flex_attention _create_empty_block_mask _identity BlockMask create_block_mask flex_attention noop_mask torch testing FileCheck torch testing _internal common_utils torch testing _internal common_cuda PLATFORM_SUPPORTS_BF with_tf _off torch testing _internal common_device_type flex_attention_supported_platform supported_platform instantiate_device_type_tests skipXPUIf torch testing _internal common_utils IS_CI IS_WINDOWS torch testing _internal inductor_utils HAS_GPU torch utils _triton has_triton_tma_device IS_WINDOWS IS_CI TODO xuhancn Need track requirement windows sys stderr write This UT validated windows lot crash Skip \n __name__ == __main__ sys exit raise unittest SkipTest skip Windows Tolerances = namedtuple Tolerances atol rtol torch version hip torch set_float _matmul_precision highest torch set_float _matmul_precision high index = torch ops aten index Tensor = torch Tensor TEST_ON_CUDA = torch cuda is_available torch utils _triton has_triton torch cuda get_device_capability = TEST_ON_XPU = torch xpu is_available torch utils _triton has_triton HAS_GPU TEST_ON_CUDA test_device = cuda test_dtypes = torch float torch bfloat torch float PLATFORM_SUPPORTS_BF torch float torch float test_dtypes_fast = torch float SKIP_UT_ON_CPU = False TEST_ON_XPU torch _C _set_onednn_allow_tf True test_device = xpu test_dtypes = torch float torch bfloat torch float test_dtypes_fast = torch float SKIP_UT_ON_CPU = False test_device = cpu torch_config_string = torch __config__ show SKIP_UT_ON_CPU = True LONG_COMPILATION_ON_CPU = False CLANG torch_config_string upper compiler clang skip UT CPU due long compilation time found CI TODO check reason long compile time LONG_COMPILATION_ON_CPU = True test_dtypes = torch float torch bfloat torch backends mkldnn is_available torch ops mkldnn _is_mkldnn_bf _supported torch float test_dtypes_fast = torch float skip_on_xpu test_func Decorator skip tests supported Intel GPU decorated_func = skipXPUIf True Not supported Intel GPU test_func decorated_func create_attention score_mod block_mask enable_gqa=False kernel_options=None functools partial flex_attention score_mod=score_mod block_mask=block_mask enable_gqa=enable_gqa kernel_options=kernel_options create_block_mask_test score_mod query key block_mask = create_block_mask score_mod query shape - key shape - query device block_mask test_page_sizes = --------- Useful score mod functions testing --------- _causal score Tensor batch Tensor head Tensor token_q Tensor token_kv Tensor - Tensor torch where token_q = token_kv score float -inf _generate_windowed offset _windowed score b h q kv torch where q + offset = kv score float -inf _windowed _get_windowed_sdpa_mask Mq Mkv offset torch tril torch ones Mkv Mkv dtype=torch bool device=test_device offset offset + Mq _rel_bias score Tensor batch Tensor head Tensor token_q Tensor token_kv Tensor - Tensor score + token_q - token_kv _rel_causal score Tensor batch Tensor head Tensor token_q Tensor token_kv Tensor - Tensor torch where token_q = token_kv score + token_q - token_kv float -inf _generate_alibi_bias num_heads int _alibi_bias score Tensor batch Tensor head Tensor token_q Tensor token_kv Tensor - Tensor scale = torch exp - head + num_heads score + token_kv - token_q scale _alibi_bias _inverse_causal score b h m n torch where m = n score float -inf _times_two score b h m n Joint graph needed correctness score _squared score b h m n Joint graph needed correctness score score _head_offset dtype torch dtype Captured Buffer head_offset = torch rand Hq device=test_device dtype=dtype score_mod score b h m n score head_offset h score_mod _trig score b h m n Joint graph needed correctness torch sin torch cos score + torch tan b _trig score b h m n Branching joint graph cos_score = torch cos score sin_score = torch sin score z = cos_score sin_score + torch tan b z test_score_mods = _identity _times_two _squared _causal _inverse_causal _rel_bias _rel_causal _generate_alibi_bias _generate_windowed captured_buffers_map = _head_offset _head_offset B = S = D = test_Hq_Hkv = test_Bq_Bkv = test_block_size = Hq Hkv = input_strides_ B H S D H S D S D D offset input_strides_ B H S D H D D B H D transposed dimensions input_strides_ B H S D S D + B S D + D + additional buffer input_strides_ B H S D D B + H + D shared dimension test_input_strides = input_strides_ input_strides_ input_strides_ input_strides_ query_key_value_clones query torch Tensor key torch Tensor value torch Tensor dtype torch dtype = None Clones query key value tensors moves them specified dtype dtype None dtype = query dtype query_ref = query detach clone dtype requires_grad_ query requires_grad key_ref = key detach clone dtype requires_grad_ key requires_grad value_ref = value detach clone dtype requires_grad_ value requires_grad query_ref key_ref value_ref batch_reserve paged_attention PagedAttention target_seq_len Tensor B = target_seq_len shape b range B paged_attention reserve torch tensor b target_seq_len b TestFlexDecoding InductorTestCase setUp super setUp test_inference_only = False test_device == cpu LONG_COMPILATION_ON_CPU skipTest skip UT CPU due long compilation time found CI test_inference_only = True _check_equal golden_out torch Tensor ref_out torch Tensor compiled_out torch Tensor fudge_factor float tensor_name Optional str = None compiled_error = golden_out - compiled_out abs mean ref_error = golden_out - ref_out abs mean torch isnan compiled_error any torch isnan ref_error any assertTrue False Output Grad NaN ref_error e- golden_out abs mean print very small ref error ref_error torch float e golden_out abs mean tolerance = Tolerances atol= e- rtol= e- torch testing assert_close golden_out dtype=compiled_out dtype compiled_out atol=tolerance atol rtol=tolerance rtol compiled_error ref_error fudge_factor name = tensor_name tensor_name None msg = f name Compiled error compiled_error greater than ref error ref_error more than fudge_factor X assertTrue False msg _check_out golden_out torch Tensor ref_out torch Tensor compiled_out torch Tensor dtype = ref_out dtype torch no_grad Note seems like we really less accurate than float computation likely due online softmax dtype == torch float fudge_factor = fudge_factor = Checkout output _check_equal golden_out ref_out compiled_out fudge_factor Out run_test score_mod Optional Callable = None dtype torch dtype = torch float Q_B int = B Q_H int = Hq Q_S int = Q_D int = D KV_B int = B KV_H int = Hkv KV_S int = S V_D int = D block_mask Optional BlockMask = None device= cuda kernel_options=None assert score_mod None block_mask None Must provide score_mod block_mask assert Q_H KV_H == device == cpu dtype torch float dtype = torch float q = torch randn Q_B Q_H Q_S Q_D dtype=dtype device=device requires_grad=not test_inference_only k = torch randn KV_B KV_H KV_S Q_D dtype=dtype device=device requires_grad=not test_inference_only v = torch randn KV_B KV_H KV_S V_D dtype=dtype device=device requires_grad=not test_inference_only q_ref k_ref v_ref = query_key_value_clones q k v q_gold k_gold v_gold = query_key_value_clones q k v torch float sdpa_partial = create_attention score_mod block_mask enable_gqa= Q_H = KV_H kernel_options=kernel_options compiled_sdpa = torch compile sdpa_partial test_inference_only golden_out gold_lse = sdpa_partial q_gold k_gold v_gold return_lse=True ref_out ref_lse = sdpa_partial q_ref k_ref v_ref return_lse=True compiled_out compiled_lse = compiled_sdpa q k v return_lse=True _check_out gold_lse ref_lse compiled_lse golden_out = sdpa_partial q_gold k_gold v_gold return_lse=False ref_out = sdpa_partial q_ref k_ref v_ref return_lse=False compiled_out = compiled_sdpa q k v return_lse=False _check_out golden_out ref_out compiled_out run_test_with_call sdpa_call Callable golden_call Optional Callable = None dtype torch dtype = torch float Q_B int = B Q_H int = Hq Q_S int = Q_D int = D KV_B int = B KV_H int = Hkv KV_S int = S V_D int = D device= cuda golden_call golden_call = sdpa_call device == cpu dtype torch float dtype = torch float q = torch randn Q_B KV_H Q_S Q_D dtype=dtype device=device requires_grad=False k = torch randn KV_B KV_H KV_S Q_D dtype=dtype device=device requires_grad=False v = torch randn KV_B KV_H KV_S V_D dtype=dtype device=device requires_grad=False q_ref k_ref v_ref = query_key_value_clones q k v q_gold k_gold v_gold = query_key_value_clones q k v torch float compiled_sdpa = torch compile sdpa_call golden_out = golden_call q_gold k_gold v_gold ref_out = golden_call q_ref k_ref v_ref compiled_out = compiled_sdpa q k v _check_out golden_out ref_out compiled_out preprocess_paged_attention score_mod Optional Callable q Tensor k Tensor v Tensor block_mask dtype torch dtype = torch float page_size int = device= cuda assert block_mask None Must provide block_mask device == cpu dtype torch float dtype = torch float Q_B Q_H Q_S _ = q shape KV_B KV_H KV_S QK_D = k shape _ _ _ V_D = v shape test different batch size max_batch_size = max Q_B KV_B + n_pages = KV_S + page_size - page_size max_batch_size allocate cache MAX_CACHED_SEQ_LEN = n_pages page_size k_cache = torch zeros KV_H MAX_CACHED_SEQ_LEN QK_D device=device dtype=dtype v_cache = torch zeros KV_H MAX_CACHED_SEQ_LEN V_D device=device dtype=dtype randomly initialize page table paged_attention = PagedAttention n_pages page_size max_batch_size device=device batch_reserve paged_attention torch tensor KV_S KV_S KV_S KV_S device=device batch_reserve paged_attention torch tensor KV_S KV_S KV_S KV_S device=device batch_reserve paged_attention torch tensor KV_S KV_S KV_S KV_S device=device batch_reserve paged_attention torch tensor KV_S KV_S KV_S KV_S device=device update cache k v input_pos = torch arange KV_S device=device dtype=torch int unsqueeze expand KV_B KV_S batch_idx = torch arange KV_B device=device dtype=torch int paged_attention assign batch_idx input_pos k v k_cache v_cache convert block mask score mod kv_len_tensor = torch full KV_B KV_S device=device dtype=torch int converted_block_mask = paged_attention convert_logical_block_mask block_mask kv_len=kv_len_tensor converted_score_mod = paged_attention get_score_mod score_mod kv_len=kv_len_tensor k_cache v_cache converted_block_mask converted_score_mod run_paged_attention score_mod Optional Callable q Tensor k Tensor v Tensor dtype torch dtype = torch float block_mask Optional BlockMask = None device= cuda Q_B Q_H KV_H = q shape q shape k shape device == cpu dtype torch float dtype = torch float block_mask None block_mask = create_block_mask noop_mask Q_B S device=device k_cache v_cache converted_block_mask converted_score_mod = preprocess_paged_attention score_mod q k v block_mask dtype block_mask BLOCK_SIZE device compiled_sdpa = torch compile flex_attention compute test_inference_only compiled_out compiled_lse = compiled_sdpa q k_cache v_cache return_lse=True block_mask=converted_block_mask score_mod=converted_score_mod enable_gqa= Q_H = KV_H compiled_lse = None compiled_out = compiled_sdpa q k_cache v_cache return_lse=False block_mask=converted_block_mask score_mod=converted_score_mod enable_gqa= Q_H = KV_H compiled_out compiled_lse run_test_with_paged_attention score_mod Optional Callable dtype torch dtype = torch float Q_B int = B Q_H int = Hq Q_S int = QK_D int = D KV_B int = B KV_H int = Hkv KV_S int = S V_D int = D block_mask Optional BlockMask = None device= cuda assert Q_H KV_H == device == cpu dtype torch float dtype = torch float q = torch randn Q_B Q_H Q_S QK_D dtype=dtype device=device requires_grad=False k = torch randn KV_B KV_H KV_S QK_D dtype=dtype device=device requires_grad=False v = torch randn KV_B KV_H KV_S V_D dtype=dtype device=device requires_grad=False q_ref k_ref v_ref = query_key_value_clones q k v q_gold k_gold v_gold = query_key_value_clones q k v torch float block_mask None block_mask = create_block_mask noop_mask Q_B KV_S device=device sdpa_partial = create_attention score_mod block_mask enable_gqa= Q_H = KV_H golden_out gold_lse = sdpa_partial q_gold k_gold v_gold return_lse=True ref_out ref_lse = sdpa_partial q_ref k_ref v_ref return_lse=True compiled_out compiled_lse = run_paged_attention score_mod q k v dtype block_mask device _check_out golden_out ref_out compiled_out test_inference_only _check_out gold_lse ref_lse compiled_lse run_test_with_call_paged_attention score_mod Optional Callable mask_mod Optional Callable sdpa_mask Tensor dtype torch dtype = torch float Q_B int = B Q_H int = Hq Q_S int = Q_D int = D KV_B int = B KV_H int = Hkv KV_S int = S V_D int = D device= cuda device == cpu dtype torch float dtype = torch float q = torch randn Q_B KV_H Q_S Q_H KV_H Q_D dtype=dtype device=device requires_grad=False k = torch randn KV_B KV_H KV_S Q_D dtype=dtype device=device requires_grad=False v = torch randn KV_B KV_H KV_S V_D dtype=dtype device=device requires_grad=False q_ref k_ref v_ref = query_key_value_clones q k v q_gold k_gold v_gold = query_key_value_clones q k v torch float golden_call = functools partial torch nn functional scaled_dot_product_attention attn_mask=sdpa_mask golden_out = golden_call q_gold k_gold v_gold ref_out = golden_call q_ref k_ref v_ref mask_mod None block_mask = create_block_mask mask_mod Q_B Q_S KV_S device=device block_mask = create_block_mask noop_mask Q_B Q_S KV_S device=device compiled_out _ = run_paged_attention score_mod q k v dtype block_mask device _check_out golden_out ref_out compiled_out supported_platform expectedFailure tl dot does support embedding size less than unittest skipIf SKIP_UT_ON_CPU Skip CPU supported common_utils parametrize dtype test_dtypes_fast test_bw_decoding_fails device dtype make_kv = functools partial torch randn dtype=dtype device=device requires_grad=True make_q = functools partial torch randn dtype=dtype device=device requires_grad=True q k v backward_grad = make_q make_kv make_kv make_q block_mask = _create_empty_block_mask q k torch compile sdpa_hop q k v score_mod block_mask flex_attention q k v score_mod output = sdpa_hop q k v _identity block_mask output backward backward_grad supported_platform common_utils parametrize dtype test_dtypes common_utils parametrize score_mod test_score_mods common_utils parametrize head_dims test_Hq_Hkv with_tf _off test_builtin_score_mods device dtype torch dtype score_mod Callable head_dims Hq Hkv = head_dims assert Hq Hkv == run_test score_mod dtype Q_H=Hq KV_H=Hkv device=device run_test_with_paged_attention score_mod dtype Q_H=Hq KV_H=Hkv device=device supported_platform common_utils parametrize dtype test_dtypes_fast common_utils parametrize score_mod test_score_mods common_utils parametrize head_dims test_Hq_Hkv common_utils parametrize page_size test_page_sizes test_paged_attention_page_size device dtype torch dtype score_mod Callable head_dims tuple int int page_size int Hq Hkv = head_dims assert Hq Hkv == generate_causal_offset offset torch Tensor causal_offset_mask b h q_idx kv_idx offset + q_idx = kv_idx causal_offset_mask mod = generate_causal_offset torch tensor device=device dtype=torch int block_mask = create_block_mask mod B S BLOCK_SIZE=page_size device=device run_test_with_paged_attention score_mod dtype Q_B=B Q_H=Hq KV_B=B KV_H=Hkv KV_S=S block_mask=block_mask device=device supported_platform common_utils parametrize dtype test_dtypes common_utils parametrize score_mod test_score_mods common_utils parametrize BLOCK_SIZE test_block_size test_builtin_score_mods_different_block_size device dtype torch dtype score_mod Callable BLOCK_SIZE Union int tuple int int block_mask = create_block_mask noop_mask B S BLOCK_SIZE=BLOCK_SIZE device=device run_test score_mod dtype block_mask=block_mask device=device unittest skipIf has_triton_tma_device Skip when TMA available common_utils parametrize dtype test_dtypes_fast test_tma_decoding device dtype torch dtype n_heads head_dim seq_len = score_mod = _generate_alibi_bias n_heads kernel_options = USE_TMA True run_test score_mod=score_mod dtype=dtype Q_B= Q_H=n_heads Q_S= Q_D=head_dim KV_B= KV_H=n_heads KV_S=seq_len V_D=head_dim device=device kernel_options=kernel_options supported_platform common_utils parametrize dtype test_dtypes_fast common_utils parametrize k_s test_input_strides common_utils parametrize v_s test_input_strides common_utils parametrize head_dims test_Hq_Hkv test_strided_inputs device dtype torch dtype k_s v_s head_dims Hq Hkv = head_dims assert Hq Hkv == q = torch randn B Hq D dtype=dtype device=device k = torch randn B Hkv S D dtype=dtype device=device v = torch randn B Hkv S D dtype=dtype device=device k_shape = B Hkv S D v_shape = B Hkv S D q = q view Hq B D transpose k_strides k_offset = k_s B Hkv S D k_max = x y - x y zip k_strides k_shape assert sum k_max + k_offset B Hkv S D assert k_strides - == k = torch as_strided k k_shape k_strides k_offset v_strides v_offset = v_s B Hkv S D v_max = x y - x y zip v_strides v_shape assert sum v_max + v_offset B Hkv S D assert v_strides - == v = torch as_strided v v_shape v_strides v_offset score_mod = _generate_alibi_bias sdpa_partial = create_attention score_mod=score_mod block_mask=None enable_gqa= Hq = Hkv compiled_sdpa = torch compile sdpa_partial ref_out = sdpa_partial q k v compiled_out = compiled_sdpa q k v tolerance = Tolerances atol= e- rtol= e- torch testing assert_close ref_out compiled_out atol=tolerance atol rtol=tolerance rtol paged_compiled_out _ = run_paged_attention score_mod q k v dtype device=device torch testing assert_close ref_out paged_compiled_out atol=tolerance atol rtol=tolerance rtol supported_platform common_utils parametrize dtype test_dtypes_fast common_utils parametrize head_dims test_Hq_Hkv common_utils parametrize batch_dims test_Bq_Bkv common_utils parametrize score_mod test_score_mods test_kv_batch_broadcast device dtype torch dtype head_dims tuple int int batch_dims tuple int int score_mod Callable Hq Hkv = head_dims assert Hq Hkv == Bq Bkv = batch_dims assert Bq Bkv == block_mask = create_block_mask noop_mask Bq S device=device run_test score_mod dtype Bq Hq D Bkv Hkv S D block_mask device=device supported_platform common_utils parametrize dtype test_dtypes test_skip_odd_keys device dtype torch dtype score_mod score b h q kv torch where kv == score float -inf run_test score_mod dtype device=device run_test_with_paged_attention score_mod dtype device=device supported_platform common_utils parametrize dtype test_dtypes test_function_composition device dtype torch dtype score_mod_ score b h m n score + m - n score_mod_ score b h m n torch where m = n score float -inf composed_score_mod score b h m n score_mod_ score_mod_ score b h m n b h m n run_test composed_score_mod dtype device=device run_test_with_paged_attention composed_score_mod dtype device=device supported_platform common_utils parametrize dtype test_dtypes test_captured_buffers device dtype torch dtype head_offset = torch rand Hq device=device dtype=dtype score_mod score b h m n score + head_offset h run_test score_mod dtype device=device run_test_with_paged_attention score_mod dtype device=device supported_platform common_utils parametrize dtype test_dtypes test_captured_buffers_all_dims device dtype torch dtype head_scale = torch randn Hq device=device batch_scale = torch randn B device=device kv_scale = torch randn S device=device q_scale = torch randn device=device all_bias score batch head token_q token_kv score = score + kv_scale token_kv score = score + q_scale token_q score = score + head_scale head score = score + batch_scale batch score run_test all_bias dtype device=device run_test_with_paged_attention all_bias dtype device=device supported_platform common_utils parametrize dtype test_dtypes_fast test_seq_masking device dtype seq_idx = torch zeros S device=device dtype=torch bool seq_idx S = seq_mask_mod score b h q kv torch where seq_idx q == seq_idx kv score float -inf run_test seq_mask_mod dtype device=device run_test_with_paged_attention seq_mask_mod dtype device=device supported_platform test_non_divisible_offset_mask device KV_S = S - offset_tensor = torch tensor S - device=device dtype=torch int mask_mod b h q kv kv = q + offset_tensor block_mask = create_block_mask mask_mod B KV_S device=device run_test KV_S=KV_S block_mask=block_mask device=device supported_platform test_non_divisible_offset_mask_with_captured_buffer device KV_S = S - offset_kv = torch randn KV_S device=device dtype=torch bfloat offset_tensor = torch tensor S - device=device dtype=torch int score_mod score b h q kv score + offset_kv kv mask_mod b h q kv kv = q + offset_tensor block_mask = create_block_mask mask_mod B KV_S device=device run_test KV_S=KV_S block_mask=block_mask score_mod=score_mod device=device supported_platform test_non_divisible_multi_token_offset_mask device KV_S = S - Q_S = offset_tensor = torch tensor S - device=device dtype=torch int mask_mod b h q kv kv = q + offset_tensor block_mask = create_block_mask mask_mod B Q_S KV_S device=device run_test Q_S=Q_S KV_S=KV_S block_mask=block_mask device=device supported_platform unittest skipIf SKIP_UT_ON_CPU Skip CPU supported test_non_divisible_multi_token_offset_mask_with_captured_buffer device KV_S = S - Q_S = offset_kv = torch randn KV_S device=device dtype=torch bfloat offset_q = torch randn Q_S device=device dtype=torch bfloat offset_tensor = torch tensor S - device=device dtype=torch int score_mod score b h q kv score + offset_kv kv + offset_q q mask_mod b h q kv kv = q + offset_tensor block_mask = create_block_mask mask_mod B Q_S KV_S device=device run_test Q_S=Q_S KV_S=KV_S block_mask=block_mask score_mod=score_mod device=device supported_platform common_utils parametrize dtype test_dtypes_fast test_load_from_bias_seq_only device dtype bias = torch randn S device=device dtype=dtype bias_mod score b h q kv score + bias q kv run_test bias_mod dtype device=device run_test_with_paged_attention bias_mod dtype device=device supported_platform common_utils parametrize dtype test_dtypes_fast test_load_from_bias_seq_batch device dtype bias = torch randn B S device=device dtype=dtype bias_mod score b h q kv score + bias b q kv run_test bias_mod dtype device=device run_test_with_paged_attention bias_mod dtype device=device supported_platform common_utils parametrize dtype test_dtypes_fast test_load_from_bias_head_seq_batch device dtype bias = torch randn B Hq S device=device dtype=dtype bias_mod score b h q kv score + bias b h q kv run_test bias_mod dtype device=device run_test_with_paged_attention bias_mod dtype device=device supported_platform common_utils parametrize score_mod test_score_mods common_utils parametrize dtype test_dtypes common_utils parametrize head_dims D D D D with_tf _off test_non_equal_head_dims device dtype score_mod head_dims qk_d v_d = head_dims run_test score_mod dtype B Hq qk_d B Hkv S V_D=v_d device=device run_test_with_paged_attention score_mod dtype B Hq qk_d B Hkv S V_D=v_d device=device supported_platform common_utils parametrize dtype test_dtypes_fast common_utils parametrize score_mod test_score_mods common_utils parametrize head_dims test_Hq_Hkv test_head_dependent_mask_mod device dtype torch dtype score_mod head_dims Hq Hkv = head_dims assert Hq Hkv == head_attention_mod kv_head_num head_type = torch tensor i kv_head_num = i range kv_head_num dtype=torch bool device=device mask_mod b h q_idx kv_idx bi_mask = head_type h causal_mask = q_idx = kv_idx bi_mask causal_mask mask_mod mask_mod = head_attention_mod Hq mask = create_block_mask mask_mod Hq S device=device run_test score_mod dtype Q_H=Hq KV_H=Hkv block_mask=mask device=device run_test_with_paged_attention score_mod dtype Q_H=Hq KV_H=Hkv device=device supported_platform common_utils parametrize dtype test_dtypes_fast test_subgraph_respect_decompostion device dtype torch _decomp core_aten_decompositions torch fx experimental proxy_tensor make_fx score_mod_func score b h q kv score - q + kv make_kv = functools partial torch randn dtype=dtype device=device requires_grad=True make_q = functools partial torch randn dtype=dtype device=device requires_grad=True query key value = make_q make_kv make_kv floor_div decomposed decomposition_table empty attention = functools partial flex_attention score_mod=score_mod_func gm = make_fx attention decomposition_table= query key value assertExpectedInline gm sdpa_score code strip \ forward arg _ arg _ arg _ arg _ arg _ add = torch ops aten add Tensor arg _ arg _ = None floor_divide = torch ops aten floor_divide default arg _ add arg _ = add = None sub = torch ops aten sub Tensor arg _ floor_divide arg _ = floor_divide = None sub floor_div decomposed core_aten_decompositions gm = make_fx attention decomposition_table=core_aten_decompositions query key value assertExpectedInline gm sdpa_score code strip \ forward arg _ arg _ arg _ arg _ arg _ add = torch ops aten add Tensor arg _ arg _ = None div = torch ops aten div Tensor_mode arg _ add rounding_mode = floor arg _ = add = None sub = torch ops aten sub Tensor arg _ div arg _ = div = None sub supported_platform common_utils parametrize dtype test_dtypes_fast test_silu_on_score device dtype silu_score score b h q kv torch nn functional silu score run_test silu_score dtype device=device run_test_with_paged_attention silu_score dtype device=device supported_platform common_utils parametrize dtype test_dtypes_fast test_padded_dense_causal device dtype seq_len = torch arange B device=device dtype=torch int + create_padded_dense_wrapper orig_score_mod njt_score_mod qk b h q kv torch where qk = seq_len b orig_score_mod qk b h q kv -float inf njt_score_mod causal_njt = create_padded_dense_wrapper _causal run_test causal_njt dtype device=device run_test_with_paged_attention causal_njt dtype device=device supported_platform common_utils parametrize dtype test_dtypes_fast test_captured_scale device dtype scale = torch ones device=device dtype=torch int score_mod_scale qk b h q kv qk + scale run_test score_mod_scale dtype device=device run_test_with_paged_attention score_mod_scale dtype device=device supported_platform common_utils parametrize dtype test_dtypes_fast test_recompile_changed_score_mod device dtype scale = torch ones device=device dtype=torch int ADD = True score_mod_scale qk b h q kv ADD qk + scale qk scale run_test score_mod_scale dtype device=device run_test_with_paged_attention score_mod_scale dtype device=device ADD = False run_test score_mod_scale dtype device=device run_test_with_paged_attention score_mod_scale dtype device=device supported_platform common_utils parametrize head_dim common_utils parametrize dtype test_dtypes_fast common_utils serialTest test_non_pow_ _headdim device dtype head_dim run_test _rel_bias dtype B Hq S head_dim B Hkv S head_dim device=device supported_platform expectedFailure If we capture tensor then we can perform reduction shouldn t allowed common_utils parametrize dtype test_dtypes_fast test_captured_reduction device dtype scale = torch randn B device=device score_mod_scale qk b h q kv qk + scale b sum dim=- run_test score_mod_scale dtype device=device supported_platform test_multiple_score_mod_calls device query = torch randn dtype=torch float device=device keys = torch randn dtype=torch float device=device _ range values = torch randn dtype=torch float device=device _ range scoremod_ qk b h q kv qk + q - kv scoremod_ qk b h q kv torch where q = kv qk -float inf f q k k v v q = flex_attention q k v score_mod=scoremod_ flex_attention q k v score_mod=scoremod_ out = f query keys values out = torch compile f query keys values tolerance = Tolerances atol= e- rtol= e- torch testing assert_close out out atol=tolerance atol rtol=tolerance rtol supported_platform test_multiple_score_mod_calls device query = torch randn dtype=torch float device=device keys = torch randn dtype=torch float device=device _ range values = torch randn dtype=torch float device=device _ range scoremod_ qk b h q kv qk + q - kv scoremod_ qk b h q kv torch where q = kv qk -float inf attention = functools partial flex_attention score_mod=scoremod_ f q k k k v v v q = attention q k v q = flex_attention q k v score_mod=scoremod_ flex_attention q k v score_mod=scoremod_ out = f query keys values out = torch compile f query keys values assertTrue out - out abs mean e- supported_platform test_multiple_score_mod_calls_paged_attention device query = torch randn dtype=torch float device=device keys = torch randn dtype=torch float device=device _ range values = torch randn dtype=torch float device=device _ range scoremod_ qk b h q kv qk + q - kv scoremod_ qk b h q kv torch where q = kv qk -float inf block_mask = create_block_mask noop_mask device=device f q k k v v q = flex_attention q k v score_mod=scoremod_ block_mask=block_mask flex_attention q k v score_mod=scoremod_ block_mask=block_mask eager_out = f query keys values k_cache v_cache converted_block_mask converted_score_mod = preprocess_paged_attention scoremod_ query keys values block_mask torch float device=device k_cache v_cache converted_block_mask converted_score_mod = preprocess_paged_attention scoremod_ query keys values block_mask torch float device=device paged_f q k k v v q = flex_attention q k v score_mod=converted_score_mod block_mask=converted_block_mask flex_attention q k v score_mod=converted_score_mod block_mask=converted_block_mask compiled_out = torch compile paged_f query k_cache k_cache v_cache v_cache tolerance = Tolerances atol= e- rtol= e- torch testing assert_close eager_out compiled_out atol=tolerance atol rtol=tolerance rtol supported_platform test_multiple_score_mod_calls_paged_attention device query = torch randn dtype=torch float device=device keys = torch randn dtype=torch float device=device _ range values = torch randn dtype=torch float device=device _ range scoremod_ qk b h q kv qk + q - kv scoremod_ qk b h q kv torch where q = kv qk -float inf block_mask = create_block_mask noop_mask device=device attention = functools partial flex_attention score_mod=scoremod_ block_mask=block_mask f q k k k v v v q = attention q k v q = flex_attention q k v score_mod=scoremod_ block_mask=block_mask flex_attention q k v score_mod=scoremod_ block_mask=block_mask eager_out = f query keys values k_cache v_cache converted_block_mask converted_score_mod = preprocess_paged_attention scoremod_ query keys values block_mask torch float device=device k_cache v_cache converted_block_mask converted_score_mod = preprocess_paged_attention scoremod_ query keys values block_mask torch float device=device k_cache v_cache converted_block_mask converted_score_mod = preprocess_paged_attention scoremod_ query keys values block_mask torch float device=device paged_attention = functools partial flex_attention score_mod=converted_score_mod block_mask=converted_block_mask paged_f q k k k v v v q = paged_attention q k v q = flex_attention q k v score_mod=converted_score_mod block_mask=converted_block_mask flex_attention q k v score_mod=converted_score_mod block_mask=converted_block_mask compiled_out = torch compile paged_f query k_cache k_cache k_cache v_cache v_cache v_cache tolerance = Tolerances atol= e- rtol= e- torch testing assert_close eager_out compiled_out atol=tolerance atol rtol=tolerance rtol supported_platform common_utils parametrize dtype test_dtypes test_njt_causal device dtype offsets = torch tensor + S device=device dtype=torch int seq_idx = torch zeros S device=device dtype=torch int idx range len offsets - seq_idx offsets idx offsets idx + = idx create_njt_wrapper orig_score_mod offsets seq_idx njt_score_mod qk b h q kv q_nested = q - offsets seq_idx q kv_nested = kv - offsets seq_idx kv orig_score_mod qk b h q_nested kv_nested njt_score_mod causal_njt = create_njt_wrapper _causal offsets seq_idx run_test causal_njt dtype device=device run_test_with_paged_attention causal_njt dtype device=device supported_platform test_mixed_dtypes_fails device query = torch randn dtype=torch float device=device key = torch randn dtype=torch float device=device value = torch randn dtype=torch float device=device assertRaisesRegex ValueError Expected query key value have same dtype flex_attention query key value _identity supported_platform patch object torch _inductor config max_autotune True test_max_autotune device score_mod score b h m n score run_test score_mod device=device run_test_with_paged_attention score_mod device=device run_test_with_paged_attention score_mod=score_mod dtype=torch bfloat Q_B= Q_H= Q_S= QK_D= KV_B= KV_H= KV_S= V_D= device=device supported_platform patch object torch _inductor config max_autotune True test_max_autotune_with_captured device head_scale = torch randn Hq device=device batch_scale = torch randn B device=device tok_scale = torch randn S device=device q_scale = torch randn device=device bias_mod score batch head token_q token_kv score = score + tok_scale token_kv score = score + q_scale token_q score = score + batch_scale batch score = score + head_scale head score run_test bias_mod device=device run_test_with_paged_attention bias_mod device=device supported_platform test_fully_masked_out_rows_ _check_gqa device Ensure fully masked out rows won t cause NaNs query = torch randn B Hq S D dtype=torch float device=device requires_grad=not test_inference_only key = torch randn B Hkv S D dtype=torch float device=device requires_grad=not test_inference_only value = torch randn B Hkv S D dtype=torch float device=device requires_grad=not test_inference_only M = S mask_mod b h q kv q M block_mask = create_block_mask mask_mod S S device=device flex = torch compile flex_attention dynamic=False test_inference_only out lse = flex query key value block_mask=block_mask enable_gqa=True return_lse=True assertTrue lse M == -float inf all loss = out sum + lse sum loss backward assertEqual query grad M sum out = flex query key value block_mask=block_mask enable_gqa=True return_lse=False assertEqual out M sum supported_platform test_windowed_no_mask_vs_sdpa device score_mod = _generate_windowed attention = functools partial flex_attention score_mod=score_mod sdpa_mask = _get_windowed_sdpa_mask S sdpa_attention = functools partial torch nn functional scaled_dot_product_attention attn_mask=sdpa_mask run_test_with_call attention sdpa_attention Q_H= KV_H= Q_S= device=device supported_platform test_windowed_full_mask_vs_sdpa device mask_mod b h q kv q + = kv score_mod = _generate_windowed block_mask = create_block_mask mask_mod S device=device attention = functools partial flex_attention block_mask=block_mask score_mod=score_mod sdpa_mask = _get_windowed_sdpa_mask S sdpa_attention = functools partial torch nn functional scaled_dot_product_attention attn_mask=sdpa_mask run_test_with_call attention sdpa_attention Q_H= KV_H= Q_S= device=device supported_platform test_windowed_partial_block_vs_sdpa device mask_mod b h q kv q + = kv block_mask = create_block_mask mask_mod S device=device attention = functools partial flex_attention block_mask=block_mask sdpa_mask = _get_windowed_sdpa_mask S sdpa_attention = functools partial torch nn functional scaled_dot_product_attention attn_mask=sdpa_mask run_test_with_call attention sdpa_attention Q_H= KV_H= Q_S= device=device supported_platform test_windowed_no_mask_vs_sdpa_paged_attention device score_mod = _generate_windowed sdpa_mask = _get_windowed_sdpa_mask S run_test_with_call_paged_attention score_mod None sdpa_mask Q_H= KV_H= Q_S= device=device supported_platform test_windowed_full_mask_vs_sdpa_paged_attention device mask_mod b h q kv q + = kv score_mod = _generate_windowed sdpa_mask = _get_windowed_sdpa_mask S run_test_with_call_paged_attention score_mod mask_mod sdpa_mask Q_H= KV_H= Q_S= device=device supported_platform test_windowed_partial_block_vs_sdpa_paged_attention device mask_mod b h q kv q + = kv sdpa_mask = _get_windowed_sdpa_mask S run_test_with_call_paged_attention None mask_mod sdpa_mask Q_H= KV_H= Q_S= device=device supported_platform unittest skipIf SKIP_UT_ON_CPU Skip CPU supported common_utils parametrize dtype test_dtypes common_utils parametrize score_mod _identity _causal test_logsumexp_correctness device dtype score_mod make_kv = functools partial torch randn B Hkv S D dtype=dtype device=device requires_grad=True make_q = functools partial torch randn B Hkv Hq Hkv D dtype=dtype device=device requires_grad=True q k v = make_q make_kv make_kv torch compile sdpa_hop q k v score_mod flex_attention q k v score_mod return_lse=True torch compile backend= aot_eager eager_sdpa_hop q k v score_mod flex_attention q k v score_mod return_lse=True ref_out ref_lse = eager_sdpa_hop q torch float k torch float v torch float score_mod compiled_out compiled_lse = sdpa_hop q k v score_mod assertTrue ref_lse dtype == torch float assertTrue compiled_lse dtype == torch float tolerance = Tolerances atol= e- rtol= e- torch testing assert_close ref_out dtype=torch float compiled_out dtype=torch float atol=tolerance atol rtol=tolerance rtol torch testing assert_close ref_lse dtype=torch float compiled_lse dtype=torch float atol=tolerance atol rtol=tolerance rtol supported_platform unittest skipIf SKIP_UT_ON_CPU Skip CPU supported test_not_pw_of_two device query = torch randn device=device key = torch randn device=device value = torch randn device=device flex_compiled = torch compile flex_attention flex_compiled query key value enable_gqa=True supported_platform unittest skipIf SKIP_UT_ON_CPU Skip CPU supported test_logsumexp_only_return device make_q = functools partial torch randn B Hkv Hq Hkv D dtype=torch float device=device requires_grad=True make_kv = functools partial torch randn B Hkv S D dtype=torch float device=device requires_grad=True q k v = make_q make_kv make_kv torch compile func q k v score_mod _ lse = flex_attention q k v score_mod return_lse=True lse_ = lse lse_ _ code = run_and_get_code func q k v _identity Ensure we re still generating flexattention kernel FileCheck check_count run primals_ primals_ primals_ True run code supported_platform skip_on_xpu TODO SYCL acc issue test_non_sparse_mulitple_block_size device generate_causal_offset offset torch Tensor causal_offset_mask b h q_idx kv_idx offset + q_idx = kv_idx causal_offset_mask noop score b h q_idx kv_idx noqa F score mod = generate_causal_offset torch tensor device=device dtype=torch int block_mask = create_block_mask mod device=device run_test score_mod=None dtype=torch float block_mask=block_mask Q_B= Q_H= Q_S= Q_D= KV_B= KV_H= KV_S= V_D= device=device run_test_with_paged_attention score_mod=None dtype=torch float block_mask=block_mask Q_B= Q_H= Q_S= QK_D= KV_B= KV_H= KV_S= V_D= device=device supported_platform test_do_not_trigger_dynamic_shapes_on_empty_block_mask device torch _dynamo reset H = Hq q = torch randn B H D device=device i range k = torch randn B H S + i D device=device v = torch randn B H S + i D device=device compiled_flex_attention = torch compile flex_attention ref = flex_attention q k v res = compiled_flex_attention q k v tolerance = Tolerances atol= e- rtol= e- torch testing assert_close ref res atol=tolerance atol rtol=tolerance rtol Ensure no more re-compilation after second automatic dynamic shape version i == assertEqual torch _dynamo utils counters frames ok assertEqual torch _dynamo utils counters frames ok supported_platform common_utils parametrize dtype test_dtypes_fast test_larger_block_mask_bug device dtype mask_mod b h q_idx kv_idx q_idx = kv_idx mask_ = create_block_mask mask_mod=mask_mod B= H=None Q_LEN= KV_LEN= device=device Compile flex attention flex_attention_compiled = torch compile flex_attention dynamic=False Create input tensors shape = q = torch normal shape device=device dtype=dtype k = torch normal shape device=device dtype=dtype v = torch normal shape device=device dtype=dtype eager = flex_attention q k v block_mask=mask_ out = flex_attention_compiled q k v block_mask=mask_ torch testing assert_close eager out atol= e- rtol= e- common_utils parametrize dtype test_dtypes_fast common_utils parametrize score_mod test_score_mods supported_platform test_decode_at_different_input_position device dtype torch dtype score_mod Callable n_pages page_size max_batch_size max_seq_len = n_heads head_dim = causal_mask b h q kv q = kv block_mask = create_block_mask causal_mask max_batch_size max_seq_len max_seq_len device=device BLOCK_SIZE=page_size init requests different prefill length prefill_length = queries keys values = seq_len prefill_length q = torch randn n_heads head_dim device=device dtype=dtype requires_grad=False k = torch randn n_heads seq_len head_dim device=device dtype=dtype requires_grad=False v = torch randn n_heads seq_len head_dim device=device dtype=dtype requires_grad=False queries append q keys append k values append v get ground truth output ref_outs golden_outs = q k v zip queries keys values q_ref k_ref v_ref = query_key_value_clones q k v q_gold k_gold v_gold = query_key_value_clones q k v torch float slice_block_mask = block_mask _adjust k_ref shape slice_block_mask seq_lengths = k_ref shape ref_out = flex_attention q_ref k_ref v_ref score_mod slice_block_mask enable_gqa=False golden_out = flex_attention q_gold k_gold v_gold score_mod slice_block_mask enable_gqa=False ref_outs append ref_out golden_outs append golden_out ref_outs = torch cat ref_outs golden_outs = torch cat golden_outs init paged attention paged_cache = PagedAttention n_pages page_size max_batch_size device=device batch_reserve paged_cache torch tensor device=device batch_reserve paged_cache torch tensor device=device batch_reserve paged_cache torch tensor device=device batch_reserve paged_cache torch tensor device=device batch_reserve paged_cache torch tensor device=device allocate paged kv cache MAX_CACHED_SEQ_LEN = n_pages page_size k_cache = torch zeros n_heads MAX_CACHED_SEQ_LEN head_dim device=device dtype=dtype v_cache = torch zeros n_heads MAX_CACHED_SEQ_LEN head_dim device=device dtype=dtype prefill paged kv cache i seq_len enumerate prefill_length batch_idx = torch tensor i device=device dtype=torch int input_pos = torch arange seq_len device=device dtype=torch int view seq_len paged_cache assign batch_idx input_pos keys i values i k_cache v_cache get paged out check correctness batch_idx = torch arange max_batch_size device=device dtype=torch int input_pos = torch tensor prefill_length device=device dtype=torch int view max_batch_size kv_len_tensor = torch full max_batch_size max_seq_len device=device dtype=torch int new_block_mask = paged_cache convert_logical_block_mask block_mask kv_len=kv_len_tensor new_block_mask seq_lengths = new_block_mask seq_lengths compiled_sdpa = torch compile create_attention paged_cache get_score_mod score_mod kv_len=kv_len_tensor new_block_mask enable_gqa=False paged_out = compiled_sdpa torch cat queries k_cache v_cache block_mask=new_block_mask torch no_grad dtype = paged_out dtype dtype == torch float fudge_factor = fudge_factor = Checkout output _check_equal golden_outs ref_outs paged_out fudge_factor Out instantiate_device_type_tests TestFlexDecoding globals only_for=test_device allow_xpu=True __name__ == __main__ torch _inductor test_case run_tests run_tests