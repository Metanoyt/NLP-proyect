mypy allow-untyped-defs contextlib dataclasses functools math sys collections namedtuple collections abc Sequence typing Any Callable Optional unittest mock patch sympy torch torch _prims_common is_integer_dtype torch utils _ordered_set OrderedSet torch utils _sympy printers CppPrinter _CppPrinter torch utils _sympy symbol symbol_is_type SymT torch utils _sympy value_ranges ValueRanges ir dependencies Dep loop_body LoopBody scheduler BaseSchedulerNode SchedulerBuffer shape_propagation BlockShapeType utils IndentedBuffer sympy_index_symbol_with_prefix sympy_subs virtualized ops OpsValue V common CSEVariable Kernel KernelArgs OptimizationContext DTYPE_TO_CPP = torch float float torch float double torch float Half torch int int _t torch int int _t torch int int _t torch int int _t torch uint uint _t torch uint uint _t torch uint uint _t torch uint uint _t torch bool bool torch bfloat BFloat torch complex complex Half torch complex complex float torch complex complex double torch float _e m fn Float _e m fn torch float _e m Float _e m torch float _e m fnuz Float _e m fnuz torch float _e m fnuz Float _e m fnuz DTYPE_TO_ATEN = torch float kFloat torch float kDouble torch float kHalf torch int kLong torch int kInt torch int kShort torch int kChar torch uint kUInt torch uint kUInt torch uint kUInt torch uint kByte torch uint kUInt torch uint kUInt torch bool kBool torch bfloat kBFloat torch complex kComplexHalf torch complex kComplexFloat torch complex kComplexDouble torch float _e m fn kFloat _e m fn torch float _e m kFloat _e m torch float _e m fnuz kFloat _e m fnuz torch float _e m fnuz kFloat _e m fnuz DEVICE_TO_ATEN = meta kMeta cpu kCPU cuda kCUDA xpu kXPU mps kMPS LAYOUT_TO_ATEN = torch strided kStrided torch _mkldnn kMkldnn type ignore attr-defined matches c core DeviceType h DEVICE_TO_INT = cpu cuda _IS_WINDOWS = sys platform == win INDEX_TYPE = int _t GemmBlocking = namedtuple GemmBlocking block_m block_n block_k get_promote_dtype args functools reduce torch promote_types type ignore arg-type n dtype n args isinstance n CppCSEVariable all n dtype None n args isinstance n CppCSEVariable None enough info calculate promote dtype promote_args new_args promote_arg arg promote_type isinstance arg CppCSEVariable arg dtype promote_type arg dtype = promote_type arg = ops to_dtype arg promote_type arg = arg value isinstance arg OpsValue arg arg dtype = promote_type arg promote_type = get_promote_dtype new_args promote_fn = functools partial promote_arg promote_type=promote_type all new_arg dtype None new_arg new_args isinstance new_arg CppCSEVariable promote_type new_args = list map promote_fn new_args new_args CppCSEVariable CSEVariable __init__ name bounds ValueRanges Any dtype Optional torch dtype = None shape BlockShapeType = None - None super __init__ name bounds dtype shape=shape is_vec = False dependent_itervars = OrderedSet sympy Symbol __repr__ - str f CppCSEVariable name name bounds bounds is_vec is_vec dtype dtype f dependent_itervars dependent_itervars update_on_args name args kwargs name == load args index _set_dependent_itervars args propagate relevant itervars is_vec args dependent_itervars update arg dependent_itervars arg args isinstance arg CppCSEVariable name == index_expr _set_dependent_itervars args any arg is_vec arg args isinstance arg CppCSEVariable is_vec = True _set_dependent_itervars index sympy Expr Set relevant itervars variable based ` index ` expression This includes itervars directly used ` index ` well relevant itervars other cse variables used ` index ` s index free_symbols s V kernel itervars dependent_itervars add s type ignore arg-type s name V kernel cse varname_map type ignore attr-defined dependent_itervars update V kernel cse varname_map s name dependent_itervars type ignore attr-defined depends_on itervar sympy Symbol itervar dependent_itervars CppPrinter _CppPrinter doprint expr simplify bool = True p=True TODO why people passing strings printer here think simplify isinstance expr sympy Expr hasattr V graph sizevars expr = V graph sizevars simplify expr super doprint expr parenthesize item sympy Expr level int strict bool = False - str isinstance item sympy Mod use parenthesis enforce precedence sympy - Mod x y becomes - x y which wrong f _print item super parenthesize item level strict A function print useful printing sympy symbols cexpr = CppPrinter doprint cexpr_index index f static_cast INDEX_TYPE cexpr index value_to_cpp value cpp_type value == float -inf f -std numeric_limits cpp_type infinity value == float inf f std numeric_limits cpp_type infinity isinstance value bool f static_cast cpp_type str value lower math isnan value f std numeric_limits cpp_type quiet_NaN f static_cast cpp_type repr value rewrite_index_for_function localize_buffer_handler LocalizeBufferHandler index sympy Expr global_buf_name str Local buffer inner dimensions snode = V graph scheduler name_to_buf global_buf_name defining_op assert snode None local_buf = localize_buffer_handler global_to_local global_buf_name scheduler_nodes = snode get_nodes _ group reduction_group = max scheduler_nodes key=lambda x int x is_reduction group call_ranges = tuple group + tuple reduction_group indices_to_keep = f x len call_ranges - idx + idx range len local_buf get_layout size sorted_symbols = sorted index free_symbols key=lambda s s name type ignore attr-defined replacements = x sorted_symbols x name startswith x x name indices_to_keep type ignore attr-defined Only keep index used local buffer replacements x = sympy core numbers Zero index = sympy_subs index replacements type ignore arg-type index rewrite_index_for_nodes localize_buffer_handler LocalizeBufferHandler index sympy Expr global_buf_name str used_vars = OrderedSet s s index free_symbols symbol_is_type s SymT INDEX index_vars = local_buf = localize_buffer_handler global_to_local global_buf_name i range len local_buf get_size var = sympy_index_symbol_with_prefix SymT INDEX i index_vars append var var used_vars index = local_buf get_layout make_indexer index_vars index LocalizeBufferHandler V WrapperHandler type ignore name-defined __init__ inner global_to_local dict str ir Buffer rewrite_index Callable LocalizeBufferHandler sympy Expr str sympy Expr - None super __init__ inner global_to_local = global_to_local rewrite_index = rewrite_index localize name str index sympy Expr global_to_local name global_to_local assert rewrite_index None index = rewrite_index index name name = global_to_local name get_name name index load name str index sympy Expr _inner load localize name index store name index value mode=None local_buffer_name local_buffer_index = localize name index res = _inner store local_buffer_name local_buffer_index value mode global_to_local name global_to_local isinstance V kernel Kernel Remove name local buffer Kernel store_buffer_names local_buffer_name added Kernel store_buffer_names Kernel CSEProxy store V kernel store_buffer_names discard local_buffer_name res store_reduction name index value pyrefly ignore bad-argument-count _inner store_reduction localize name index value LocalBufferContext This creates context helps generate code involving Inductor IR function local buffers These buffers constructed during codegen process used store intermediate results such local accumulators We do want add them ` V graph ` since they global we do want add them function arguments either So we patch codegen processes under scope support these buffers without exposure outside world __init__ kernel_args KernelArgs - None kernel_args = kernel_args exit_stack = contextlib ExitStack map local buffer name local buffer local_buffers dict str ir Buffer = map global buffer name global buffer global_buffers dict str ir Buffer = map global buffer name local buffer global_to_local dict str ir Buffer = record global buffers removed LocalBufferContext removed_buffers OrderedSet str = OrderedSet __enter__ exit_stack __enter__ original_get_dtype = V graph get_dtype get_dtype name name local_buffers local_buffers name get_dtype original_get_dtype name exit_stack enter_context patch object V graph get_dtype get_dtype original_input = kernel_args input input name name local_buffers name original_input name exit_stack enter_context patch object kernel_args input input original_output = kernel_args output output name name local_buffers name original_output name exit_stack enter_context patch object kernel_args output output Set current LocalBufferContext into V exit_stack enter_context V set_local_buffer_context __exit__ exc_type exc_val exc_tb local_buffers clear exit_stack __exit__ exc_type exc_val exc_tb add_local_buffer local_buffer ir Buffer global_buffers Optional list ir Buffer = None assert local_buffer get_name local_buffers local_buffers local_buffer get_name = local_buffer global_buffers global_buffer global_buffers global_buffer_name = global_buffer get_name assert global_buffer_name global_buffers global_buffer_name global_to_local global_buffers global_buffer_name = global_buffer global_to_local global_buffer_name = local_buffer global_buffer_name V graph removed_buffers Record global buffers removed LocalBufferContext since which may need restore Refer issue https github com pytorch pytorch issues removed_buffers add global_buffer_name V graph removed_buffers add global_buffer_name localize_function fn Callable Any rewrite_index Callable LocalizeBufferHandler sympy Expr str sympy Expr = rewrite_index_for_function inner args kwargs V set_ops_handler LocalizeBufferHandler V get_ops_handler global_to_local=self global_to_local rewrite_index=rewrite_index fn args kwargs inner localize_nodes nodes list ir IRNode rewrite_index Callable LocalizeBufferHandler sympy Expr str sympy Expr = rewrite_index_for_nodes - list ir IRNode Given ` local_buf ` ` global_buf ` registered current ` LocalBufferContext ` though method ` add_local_buffer ` localizes ` global_buf ` ` local_buf ` given ` nodes ` returns new list IR nodes work ` local_buf ` instead ` global_buf ` i e all loads stores redirected ` local_buf ` This helps fused loops work smaller-sized local buffers better data locality The data access ` local_buf ` assumed contiguous same order ` global_buf ` assert len nodes wrap_inner_fn_for_node node ir IRNode loops = node data isinstance node ir ComputedBuffer node assert isinstance loops ir Loops new_inner_fn = localize_function loops inner_fn rewrite_index new_loops = dataclasses replace loops inner_fn=new_inner_fn isinstance node ir ComputedBuffer new_node = ir ComputedBuffer name=node get_name layout=node get_layout data=new_loops new_node = new_loops type ignore assignment new_node wrap_inner_fn_for_node node node nodes unify_mask_base_type buffer IndentedBuffer vars tuple CSEVariable dtype=torch float Given list cse variables Cast each new mask base dtype casted cse variable new_vars = V kernel cse generate buffer f V kernel _get_mask_cast var dtype var vars new_vars may_unify_binary_op_mask_type b Given two cse variables when dtype bool unify them same mask dtype casted cse variable dtype == torch bool assert b dtype == torch bool mask_dtype = torch int unify_mask_base_type V kernel compute b mask_dtype b codegen_rand offset code rand_function dst_dtype=torch float assert is_integer_dtype offset dtype code writeline code indent code writeline f DTYPE_TO_CPP offset dtype offset V kernel tiling_factor code writeline f DTYPE_TO_CPP dst_dtype result V kernel tiling_factor code writeline f offset store offset code writeline f DTYPE_TO_CPP offset dtype offset_idx = offset_idx V kernel tiling_factor offset_idx++ code indent code writeline rand_function num_vectors = V kernel _get_num_vectors dtype=dst_dtype num_vectors == code writeline f vec Vectorized DTYPE_TO_CPP dst_dtype loadu result code writeline f vec VectorizedN DTYPE_TO_CPP dst_dtype num_vectors loadu result code writeline code get_gemm_template_output_and_compute_dtype input_dtype input_dtype torch uint torch int torch int torch int torch float torch float create_epilogue_with_attr input_buffer attr kwargs input_loader = input_buffer make_loader dtype = input_buffer get_dtype attr == relu inner_fn index input = input_loader index zero = ops constant dtype ops maximum input zero attr == gelu assert algorithm kwargs kwargs algorithm == none inner_fn index input = input_loader index dtype = torch float input = ops to_dtype input torch float half = ops constant torch float one = ops constant torch float const = ops constant torch float result = input half ops erf input const + one dtype = torch float result = ops to_dtype result dtype result assert kwargs algorithm == tanh inner_fn index input = input_loader index dtype = torch float input = ops to_dtype input torch float half = ops constant torch float one = ops constant torch float const = ops constant torch float const = ops constant torch float result = half input one + ops tanh const input + const input input input dtype = torch float result = ops to_dtype result dtype result attr == swish inner_fn index input = input_loader index result = input ops sigmoid input result attr == sigmoid inner_fn index ops sigmoid input_loader index attr == tanh inner_fn index ops tanh input_loader index attr == hardswish attr == hardsigmoid hardsigmoid_float input zero = ops constant torch float six = ops constant torch float three = ops constant torch float one_over_six = ops constant torch float max = ops maximum input + three zero min = ops minimum max six min one_over_six inner_fn index input = input_loader index dtype = torch float input = ops to_dtype input torch float result = hardsigmoid_float input attr == hardswish result = input result dtype = torch float result = ops to_dtype result dtype result attr == leaky_relu assert scalars kwargs assert len kwargs scalars == negative_slope = kwargs scalars inner_fn index input = input_loader index dtype = torch float input = ops to_dtype input torch float zero = ops constant torch float result = ops where input zero input input ops constant negative_slope torch float dtype = torch float result = ops to_dtype result dtype result attr == hardtanh assert scalars kwargs assert len kwargs scalars == min_value = kwargs scalars max_value = kwargs scalars inner_fn index input = input_loader index dtype = torch float input = ops to_dtype input torch float result = ops minimum ops maximum input ops constant min_value torch float ops constant max_value torch float dtype = torch float result = ops to_dtype result dtype result attr add sub mul assert other kwargs other = kwargs other num_input_dims = len input_buffer get_size num_other_dims = len other get_size dims_diff = num_input_dims - num_other_dims other_loader = other make_loader inner_fn index op = getattr ops attr dims_diff = op input_loader index other_loader index dims_diff op input_loader index other_loader index attr == bias_add assert other kwargs assert beta kwargs assert dtype kwargs beta = kwargs beta other = kwargs other dtype = kwargs dtype bias_loader = other make_loader inner_fn index bias = bias_loader index input = input_loader index beta = result = ops constant beta torch float bias + input result = bias + input result raise ValueError f Unsupported epilogue attribute attr ir Pointwise device=input_buffer get_device dtype=dtype inner_fn=inner_fn ranges=input_buffer get_size _get_loop_body fn_list all isinstance fn LoopBody fn fn_list loop_bodies = fn_list hasattr fn_list original_fn For case local buffer we wrap fn localize_function assert all hasattr fn original_fn fn fn_list assert all isinstance fn original_fn args _body LoopBody fn fn_list loop_bodies = fn original_fn args _body fn fn_list assert all isinstance fn functools partial fn fn_list assert all isinstance fn args _body LoopBody fn fn_list loop_bodies = fn args _body fn fn_list assert loop_bodies None loop_bodies _get_dtype_from_loopbodies loop_bodies dtypes = OrderedSet torch dtype loop_body loop_bodies graphs = loop_body root_block graph + body graph body list loop_body subblocks values graph graphs node graph nodes node op = call_method continue dtypes add node meta OptimizationContext key dtype dtypes template_fusion_with_epilogues_supported template BaseSchedulerNode epilogues list BaseSchedulerNode - tuple bool bool _get_indexes_of_template_buf_read epilogue_node ir Operation template_buf_names list str - list sympy Expr read index read epilogue_node get_reads read name template_buf_names _check_supported_and_same_indexes index_of_template_buf_read Sequence sympy Expr epilogue_writes OrderedSet Dep - tuple bool bool num_indexes = len OrderedSet index_of_template_buf_read num_indexes same_index = False supported = False Different read indexes supported num_indexes == same_index = True supported = True No reads automatically supported num_indexes == iotbr = index_of_template_buf_read same_index = all write index == iotbr write epilogue_writes TODO Add support fusion when read template buffer write epilogue output epilogue node don t have same index change supported True supported = same_index raise AssertionError Should reach here supported same_index _template_fusion_supported template_outputs Sequence SchedulerBuffer epilogue_nodes list ir Operation - tuple bool bool template_buf_names = x get_name x template_outputs indexes_of_template_buf_reads = _get_indexes_of_template_buf_read epilogue_node template_buf_names epilogue_node epilogue_nodes epilogue_nodes_writes = epilogue_node get_read_writes writes epilogue_node epilogue_nodes results = _check_supported_and_same_indexes reads writes reads writes zip indexes_of_template_buf_reads epilogue_nodes_writes supported same_indexes = zip results all supported all same_indexes assert template is_template template_outputs = template get_outputs epilogue_nodes = n node epilogue epilogues n epilogue get_nodes n node None _template_fusion_supported template_outputs epilogue_nodes