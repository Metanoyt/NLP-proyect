Owner s oncall distributed copy sys collections OrderedDict typing Optional torch torch distributed dist torch distributed device_mesh init_device_mesh torch distributed fsdp fully_sharded_data_parallel CPUOffload FullyShardedDataParallel FSDP ShardingStrategy torch distributed tensor DeviceMesh distribute_module DTensor Replicate Shard torch distributed tensor debug CommDebugMode torch distributed tensor parallel ColwiseParallel parallelize_module RowwiseParallel torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp FSDPTest torch testing _internal common_utils instantiate_parametrized_tests run_tests TEST_WITH_DEV_DBG_ASAN torch testing _internal distributed _tensor common_dtensor MLPModule RMSNormPython dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit device_type = acc type acc = torch accelerator current_accelerator cpu SimpleModel torch nn Module __init__ - None super __init__ net = torch nn Linear relu = torch nn ReLU net = torch nn Linear net = torch nn Linear forward x net net relu net x staticmethod get_sharded_param_names - list str net weight net bias net weight staticmethod get_non_sharded_param_names - list str net weight net bias distribute_rmsnorm module device_mesh prepare_input_fn mod inputs device_mesh shard_tensor = DTensor from_local inputs device_mesh Shard shard_tensor prepare_output_fn mod outputs device_mesh outputs to_local distribute_module module device_mesh input_fn=prepare_input_fn output_fn=prepare_output_fn TestTPFSDPIntegration FSDPTest _get_params_and_sharding_info model SimpleModel sharded_param_names list str tensor_parallel_size int - tuple dict str int dict str tuple torch Size int assert type model SimpleModel Expects ` SimpleModel ` since sharding cases model definition param_name_to_numel = OrderedDict param_name_to_sharding_info = OrderedDict param_name param model named_parameters param_name sharded_param_names param_name_to_numel param_name = param numel param_name_to_numel param_name = param numel tensor_parallel_size param_name_to_sharding_info param_name = param size net param_name param_name_to_numel param_name_to_sharding_info _get_sub_pgs tensor_parallel_size int Generates TP FSDP subprocess groups ` ` tensor_parallel_size ` ` gives TP process group size For example global world size tensor parallel size then creates - TP subprocess groups - FSDP subprocess groups -D mesh dp tp twod_mesh = DeviceMesh device_type=device_type mesh=torch arange world_size view - tensor_parallel_size fsdp_pg = twod_mesh get_group mesh_dim= tp_pg = twod_mesh get_group mesh_dim= twod_mesh fsdp_pg tp_pg _sync_tp_grads tp_fsdp_model FSDP tp_pg dist ProcessGroup param_name_to_numel dict str int non_sharded_param_names list str - None Syncs tensor parallel parameters gradients following data parallel paradigm where gradients averaged over ranks case ones tensor parallel process group tp_world_size = tp_pg size fsdp_world_size = world_size tp_world_size assert type tp_fsdp_model FSDP len m m tp_fsdp_model modules type m FSDP == The following logic assumes single top-level-only FSDP wrapping model TP already applied flat_param tp_fsdp_model params splits = tuple param_name_to_numel values Create mask over gradient elements manually reduce unsharded_size = torch Size flat_param numel fsdp_world_size unsharded_zeros = torch zeros unsharded_size device=flat_param device per_param_masks = unsharded_zeros split splits param_idx param_name enumerate param_name_to_numel keys assumes fixed order param_name non_sharded_param_names per_param_masks param_idx = unsharded_mask = torch cat per_param_masks contiguous type torch BoolTensor sharded_mask = unsharded_mask chunk fsdp_world_size rank tp_world_size grad_device = flat_param grad device grad = flat_param grad detach clone rank dist all_reduce grad op=dist ReduceOp SUM group=tp_pg grad = grad grad_device flat_param grad ~sharded_mask = grad ~sharded_mask Average all gradient elements match FSDP only semantics flat_param grad = tp_world_size _get_grads_as_flattened model FSDP uses_tp bool param_name_to_numel dict str int param_name_to_sharding_info dict str tuple torch Size int tp_pg Optional dist ProcessGroup fsdp_pg Optional dist ProcessGroup sharded_param_names Optional list str - torch Tensor Returns all unsharded gradients single flattened tensor This returns same value all ranks local_grads_as_flattened = torch cat torch flatten param grad param grad None torch zeros_like torch flatten param param model parameters contiguous rank all_grads_as_flattened = torch cat torch empty_like local_grads_as_flattened _ range fsdp_pg size contiguous dist all_gather_into_tensor all_grads_as_flattened local_grads_as_flattened group=fsdp_pg uses_tp all_grads_as_flattened splits = tuple param_name_to_numel values all_grads_per_param = list all_grads_as_flattened split splits param_idx param_name enumerate param_name_to_numel keys assumes fixed order param_name sharded_param_names local_tensor_size = list param_name_to_sharding_info param_name sharding_dim = param_name_to_sharding_info param_name local_tensor_size sharding_dim = tp_pg size local_tensor = all_grads_per_param param_idx view local_tensor_size local_tensors = torch empty_like local_tensor _ range tp_pg size dist all_gather local_tensors local_tensor group=tp_pg all_grads_per_param param_idx = torch cat local_tensors dim=sharding_dim reshape - torch cat all_grads_per_param contiguous skip_if_lt_x_gpu test_fsdp_tp_integration run_subtests cpu_offload CPUOffload offload_params=False CPUOffload offload_params=True sharding_strategy None ShardingStrategy SHARD_GRAD_OP use_orig_params False True _test_fsdp_tp_integration _test_fsdp_tp_integration cpu_offload sharding_strategy use_orig_params Tests training TP + FSDP integration comparing FSDP-only model TP + FSDP model tensor_parallel_size = LR = e- torch manual_seed model = SimpleModel rank tp_fsdp_model = copy deepcopy model sharded_param_names = SimpleModel get_sharded_param_names non_sharded_param_names = SimpleModel get_non_sharded_param_names param_name_to_numel param_name_to_sharding_info = _get_params_and_sharding_info model sharded_param_names tensor_parallel_size input_seed = rank torch manual_seed input_seed + inp_size = inp = torch rand inp_size rank assertEqual model inp tp_fsdp_model inp sanity check mesh_ d = init_device_mesh device_type world_size fsdp_model = FSDP model cpu_offload=cpu_offload device_mesh=mesh_ d sharding_strategy=sharding_strategy use_orig_params=use_orig_params mesh_ d = init_device_mesh device_type world_size tensor_parallel_size tensor_parallel_size mesh_dim_names= dp tp Shard TP then wrap FSDP sequence_parallelize_plan = net ColwiseParallel input_layouts=Shard net RowwiseParallel output_layouts=Shard tp_fsdp_model = parallelize_module tp_fsdp_model mesh_ d tp sequence_parallelize_plan tp_pg = mesh_ d tp get_group mesh_dim= assert isinstance tp_fsdp_model net weight DTensor assert isinstance tp_fsdp_model net weight DTensor tp_fsdp_model = FSDP tp_fsdp_model cpu_offload=cpu_offload device_mesh=mesh_ d dp sharding_strategy=sharding_strategy use_orig_params=use_orig_params fsdp_pg = mesh_ d dp get_group mesh_dim= Check forward checking output equality fsdp_out = fsdp_model inp tp_fsdp_out = tp_fsdp_model inp assertEqual fsdp_out tp_fsdp_out Check backward checking gradient equality fsdp_out sum backward tp_fsdp_out sum backward _sync_tp_grads tp_fsdp_model tp_pg param_name_to_numel non_sharded_param_names model_grads = _get_grads_as_flattened fsdp_model False param_name_to_numel param_name_to_sharding_info None process_group None model_tp_grads = _get_grads_as_flattened tp_fsdp_model True param_name_to_numel param_name_to_sharding_info tp_pg fsdp_pg sharded_param_names assertEqual model_grads model_tp_grads Check optimizer step performing second forward pass fsdp_optim = torch optim SGD fsdp_model parameters lr=LR tp_fsdp_optim = torch optim SGD tp_fsdp_model parameters lr=LR fsdp_optim step tp_fsdp_optim step torch manual_seed input_seed + inp = torch rand inp_size rank fsdp_out = fsdp_model inp tp_fsdp_out = tp_fsdp_model inp assertEqual fsdp_out tp_fsdp_out skip_if_lt_x_gpu test_fsdp_tp_extension_grad Tests TP + FSDP extension correct gradient i e no ACT mesh_ d = init_device_mesh device_type world_size mesh_dim_names= dp tp TestModel torch nn Module __init__ - None super __init__ mlp = MLPModule device_type mlp_norm = RMSNormPython forward x mlp mlp_norm x model = TestModel rank Shard TP test gradient tp_mesh = mesh_ d tp tp_model = parallelize_module model tp_mesh mlp net ColwiseParallel input_layouts=Shard mlp net RowwiseParallel output_layouts=Shard distribute_rmsnorm tp_model mlp_norm tp_mesh fsdp_ d_model = FSDP tp_model device_mesh=mesh_ d dp comm_mode = CommDebugMode comm_mode fsdp_ d_model torch rand rank sum backward funcol = torch ops c d_functional c d_ops = torch ops c d comm_counts = comm_mode get_comm_counts assertEqual comm_mode get_total_counts TP comms assertEqual comm_counts funcol reduce_scatter_tensor assertEqual comm_counts funcol all_gather_into_tensor assertEqual comm_counts funcol all_reduce FSDP comms assertEqual comm_counts c d_ops _allgather_base_ assertEqual comm_counts c d_ops _reduce_scatter_base_ grads = p grad p fsdp_ d_model parameters p grad None grad grads assertFalse grad isnan any item skip_if_lt_x_gpu test_fsdp_tp_sync_module_state mesh_ d = init_device_mesh device_type world_size mesh_dim_names= dp tp tp_mesh = mesh_ d tp dp_mesh = mesh_ d dp set random seed each rank torch manual_seed mesh_ d get_rank TestModel torch nn Module __init__ - None super __init__ replicated_dt = DTensor from_local torch randn tp_mesh Replicate run_check=False replicated_buffer_dt = DTensor from_local torch randn tp_mesh Replicate run_check=False param = torch nn Parameter replicated_dt buf = torch nn Buffer replicated_buffer_dt forward x param + buffer + model = TestModel assert_local_shard_across_ranks local_tensor group check_equal=True gathered_tensors = torch empty_like local_tensor _ range group size dist all_gather gathered_tensors local_tensor group=group dp mesh dim local tensor does equal tensor_to_compare = gathered_tensors tensor gathered_tensors check_equal assertTrue torch equal tensor tensor_to_compare assertFalse torch equal tensor tensor_to_compare dp_group = dp_mesh get_group check dp mesh dim param local tensor does equal local_param = model param to_local assert_local_shard_across_ranks local_param dp_group check_equal=False check dp mesh dim buffer local tensor does equal local_buf = model buf to_local assert_local_shard_across_ranks local_buf dp_group check_equal=False wrap fsdp sync param should sync dp mesh dim fsdp_mod = FSDP model device_mesh=dp_mesh sync_module_states=True fsdp_mod summon_full_params fsdp_mod dp mesh dim local param does equal after sync_module_states local_param = fsdp_mod param to_local assert_local_shard_across_ranks local_param dp_group check_equal=True dp mesh dim local buf does equal after sync_module_states local_buf = fsdp_mod buf to_local assert_local_shard_across_ranks local_buf dp_group check_equal=True instantiate_parametrized_tests TestTPFSDPIntegration __name__ == __main__ run_tests