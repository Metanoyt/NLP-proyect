itertools logging typing Any Callable Optional Union torch torch _inductor config config torch _inductor ir torch _inductor codegen common KernelTemplate torch _inductor ir Buffer FixedLayout get_free_symbols get_symbolic_inputs gm_original_output_strides ir_node_to_tensor Layout torch _inductor runtime benchmarking benchmarker torch _inductor utils do_bench_using_profiling torch _inductor virtualized V log = logging getLogger __name__ SubgraphChoiceCaller ir ChoiceCaller Represents Subgraph Autotuning choice subgraph can any arbitrary GraphModule Compiles Subgraph down module benchmarking __init__ name str input_nodes list Buffer layout Layout description str make_fx_graph Callable Any - None super __init__ name input_nodes layout description example_inputs = V fake_mode inp input_nodes Here there will no unbacked symbols SubgraphBuffer does support them assert len get_free_symbols inp get_size unbacked_only=True == assert len get_free_symbols inp get_stride unbacked_only=True == inp data freeze_layout type ignore attr-defined example_inputs append ir_node_to_tensor inp gm = make_fx_graph example_inputs gm_original_output_strides gm sym_inputs = get_symbolic_inputs input_nodes __str__ - str f SubgraphCaller name benchmark args list Any out torch Tensor - float Codegen Subgraph benchmarking Need GraphLowering instead SubgraphLowering generate fully callable module torch _inductor config inductor_config torch _inductor graph GraphLowering bm_graph_lowering = GraphLowering gm=self gm example_inputs=self example_inputs shape_env=V graph _shape_env cpp_wrapper=V graph cpp_wrapper aot_mode=V graph aot_mode extern_node_serializer=V graph extern_node_serializer is_inference=V graph is_inference is_backward=V graph is_backward name=f benchmark_ name sym_inp sym_inputs bm_graph_lowering graph_inputs sym_inp name = sym_inp bm_graph_lowering graph_input_names append sym_inp name sym_inputs = pyrefly ignore no-matching-overload int V graph sizevars shape_env size_hint sym_var sym_var sym_inputs len sym_inputs == Sanity check args same layout example inputs Only do there no symbolic inputs otherwise dynamic dim will realized same size args ar example_inp zip args example_inputs Sanity check args same layout example inputs isinstance ar torch Tensor assert isinstance example_inp torch Tensor assert ar shape == example_inp shape assert ar stride == example_inp stride V set_graph_handler bm_graph_lowering Don t bother autotuning Triton here inductor_config patch max_autotune=False max_autotune_gemm=False max_autotune_gemm_backends= ATEN bm_graph_lowering run example_inputs mod = bm_graph_lowering compile_to_module bm_func = mod call bm_func sym_inputs args config profile_bandwidth_with_do_bench_using_profiling do_bench_using_profiling lambda bm_func sym_inputs args layout device type == cpu benchmarker benchmark_cpu lambda bm_func sym_inputs args benchmarker benchmark_gpu lambda bm_func sym_inputs args hash_key - str - join name rsplit _ str inp get_size inp input_nodes str inp get_stride inp input_nodes str gm graph output_node - Union ir TensorBox ir ShapeAsConstantBuffer ir TensorBox create ir SubgraphBuffer layout=self layout input_nodes=self input_nodes gm=self gm example_inputs=self example_inputs subgraph_name=self name info_dict - dict str Any Information returned here logged autotune log file when enabled backend subgraph kernel_name name autoheuristic_id - str f subgraph_ name SubgraphTemplate KernelTemplate A template subgraph evaluation used autotuning This allows creating customized subgraphs can appended choices during autotuning process enabling selection optimal implementations complex operations index_counter = itertools count __init__ name str Initialize subgraph template Args name The name template graph The FX graph super __init__ name=name generate type ignore override name str input_nodes list Buffer layout Layout make_fx_graph Callable Any description str = kwargs Any - SubgraphChoiceCaller Generate SubgraphChoiceCaller instance autotuning Args name The name subgraph choice input_nodes List input nodes subgraph layout Memory layout information output make_fx_graph Callable creates FX graph subgraph description Optional description choice kwargs Additional keyword arguments Returns SubgraphChoiceCaller A callable object can used autotuning SubgraphChoiceCaller name=f name _ next SubgraphTemplate index_counter input_nodes=input_nodes layout=layout description=description make_fx_graph=make_fx_graph generate_custom_op_choices name str decompositions list Callable Any input_nodes list Buffer non_tensor_args list dict str Any default_impl Optional Callable Any = None - list SubgraphChoiceCaller Generate multiple SubgraphChoiceCaller instances custom op autotuning This method extends SubgraphTemplate support custom op decompositions allowing multiple implementations compete autotuning Args name Base name choices decompositions List decomposition functions compete autotuning input_nodes List tensor inputs All tensor arguments must passed here non_tensor_args List non-tensor kwargs only one dict per corresponding decomposition default_impl Default implementation layout inference Returns List SubgraphChoiceCaller instances autotuning decompositions assert len decompositions == len non_tensor_args f decompositions non_tensor_args must have same length f got len decompositions decompositions len non_tensor_args kwargs Infer layouts ensure layout consistency fair autotuning comparison layouts = _infer_custom_op_layout input_nodes decomp kwargs default_impl decomp kwargs zip decompositions non_tensor_args Validate all decompositions produce equivalent layouts fair comparison _validate_layout_equivalence name decompositions layouts layout = layouts All layouts now validated equivalent choices list SubgraphChoiceCaller = decomp decomp_kwargs zip decompositions non_tensor_args Create make_fx_graph function decomposition functools make_fx_graph args Any decomp Callable Any = decomp decomp_kwargs dict str Any = decomp_kwargs - Any decomp_kwargs contains all merged parameters CustomOpConfig params + runtime kwargs torch fx experimental proxy_tensor make_fx make_fx functools partial decomp decomp_kwargs args Generate descriptive name variant variant_name = _generate_variant_name decomp decomp_kwargs choice = generate name=f name _ variant_name input_nodes=input_nodes layout=layout make_fx_graph=make_fx_graph description=f CustomOp decomp __name__ choices append choice choices _generate_variant_name decomp Callable Any kwargs dict str Any - str Generate descriptive name decomposition variant its parameters base_name = decomp __name__ kwargs base_name param_suffix = _ join f k _ v k v sorted kwargs items f base_name _ param_suffix _validate_non_tensor_kwargs kwargs dict str Any - None Validate kwargs contains only non-tensor arguments key value kwargs items assert isinstance value torch Tensor Buffer f kwargs key contains tensor type value f Tensor arguments should input_nodes kwargs f Only scalar non-tensor parameters should kwargs _validate_layout_equivalence op_name str decompositions list Callable Any layouts list Layout - None Ensure all layouts have consistent stride device dtype sizes fair autotuning layouts reference = layouts i layout enumerate layouts start= layout device layout dtype layout size layout stride = reference device reference dtype reference size reference stride raise AssertionError f Layout mismatch custom op op_name f decomposition decompositions i __name__ produces f layout device layout dtype layout size layout stride f decompositions __name__ produces f reference device reference dtype reference size reference stride _infer_custom_op_layout input_nodes list Buffer function_decomposition Callable Any kwargs dict str Any default_impl Optional Callable Any = None - Layout Infer output layout custom ops using default implementation when available Note Subgraph assumes custom ops exactly one tensor output TODO Add support multiple output custom ops functools torch _inductor virtualized V Assert kwargs contain only non-tensor arguments _validate_non_tensor_kwargs kwargs V fake_mode example_inputs = inp input_nodes raw_shape = inp get_size concrete_shape = V graph sizevars size_hints raw_shape fallback=config unbacked_symint_fallback fake_tensor = torch empty concrete_shape dtype=inp get_dtype device=inp get_device example_inputs append fake_tensor fn = functools partial function_decomposition kwargs output = fn example_inputs Assert single output assert isinstance output torch Tensor f Expected single tensor output got type output f Multi-output custom ops yet supported autotuning FixedLayout device=output device dtype=output dtype size=output shape stride=output stride