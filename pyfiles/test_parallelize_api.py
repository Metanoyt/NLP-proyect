Owner s oncall distributed collections OrderedDict copy deepcopy torch torch distributed tensor DeviceMesh DTensor Replicate Shard torch distributed tensor debug CommDebugMode torch distributed tensor parallel api parallelize_module torch distributed tensor parallel style ColwiseParallel PrepareModuleInput PrepareModuleInputOutput PrepareModuleOutput RowwiseParallel torch testing _internal common_utils run_tests torch testing _internal distributed _tensor common_dtensor DTensorTestBase MLPModule MLPStacked with_comms DummyModule torch nn Module __init__ - None super __init__ forward x x TensorParallelAPITests DTensorTestBase property world_size gpu_num = torch accelerator device_count gpu_num gpu_num == gpu_num _compare_params local_module dist_module rank _only skip_rowwise_bias=False compare_grad=False replicate = Replicate name param local_module named_parameters dist_param = dist_module get_parameter name param = param grad compare_grad param dist_param = dist_param grad compare_grad dist_param rank _only rank == name net bias skip_rowwise_bias name bias net bias assertEqual param dist_param redistribute device_mesh=dist_param device_mesh placements=replicate to_local f name equal between dist non-dist _compare_module local_module dist_module inp_size rank _only=True rowwise=False LR = learning rate we use testing local_optim = torch optim SGD local_module parameters lr=LR dist_optim = torch optim SGD dist_module parameters lr=LR torch manual_seed inp = torch rand inp_size device=self device_type _compare_params local_module dist_module rank _only check forward correctness local_output = local_module inp inp = inp chunk world_size dim=- rank rowwise inp dist_output = dist_module inp dist_output = dist_output redistribute dist_output device_mesh Replicate to_local isinstance dist_output DTensor dist_output assertEqual local_output dist_output local_output sum backward dist_output sum backward check backward ensure gradients same _compare_params local_module dist_module rank _only rowwise True local_optim step dist_optim step _compare_params local_module dist_module rank _only rowwise with_comms test_parallelize_mlp_with_module_api inp_size = model = MLPModule device_type model_tp = deepcopy model Parallelize module device_mesh = DeviceMesh device_type torch arange world_size model_tp = parallelize_module model_tp device_mesh net ColwiseParallel output_layouts=Replicate net ColwiseParallel output_layouts=Replicate _compare_module model model_tp inp_size rank _only=False with_comms test_parallelize_mlp_with_module_api_nested inp_size = model = torch nn Sequential OrderedDict dummy_encoder MLPModule device_type model_tp = deepcopy model Parallelize module device_mesh = DeviceMesh device_type torch arange world_size model_tp = parallelize_module model_tp device_mesh dummy_encoder net ColwiseParallel output_layouts=Replicate dummy_encoder net ColwiseParallel output_layouts=Replicate _compare_module model model_tp inp_size rank _only=False with_comms test_linear_row_wise_parallel test RowwiseParallel inp_size = rowwise = RowwiseParallel torch manual_seed model = torch nn Linear device=self device_type model_tp = deepcopy model parallelize model_tp device_mesh = DeviceMesh device_type list range world_size model_tp = parallelize_module model_tp device_mesh rowwise let each rank generate unique local input torch manual_seed rank _compare_module model model_tp inp_size rowwise=True with_comms test_linear_col_wise_parallel test ColwiseParallel inp_size = colwise = ColwiseParallel output_layouts=Replicate torch manual_seed model = torch nn Linear device=self device_type model_tp = deepcopy model parallelize model_tp device_mesh = DeviceMesh device_type list range world_size model_tp = parallelize_module model_tp device_mesh colwise _compare_module model model_tp inp_size with_comms test_prepare_module_input module = DummyModule device_mesh = DeviceMesh device_type list range world_size parallelize_module module device_mesh PrepareModuleInput input_layouts=Shard desired_input_layouts=Replicate inp = torch rand device=self device_type output = module inp redistribute device_mesh Shard to_local assertEqual inp output with_comms test_prepare_module_output module = DummyModule device_mesh = DeviceMesh device_type list range world_size parallelize_module module device_mesh PrepareModuleOutput output_layouts=Replicate desired_output_layouts=Shard torch manual_seed inp = torch rand device=self device_type dtensor = DTensor from_local inp device_mesh Replicate run_check=False output = module dtensor inp = dtensor redistribute device_mesh Shard to_local assertEqual inp output with_comms test_prepare_module_input_output module = DummyModule device_mesh = DeviceMesh device_type list range world_size parallelize_module module device_mesh PrepareModuleInputOutput input_layouts=Shard desired_input_layouts=Replicate output_layouts=Replicate desired_output_layouts=Shard inp = torch rand device=self device_type output = module inp inp = DTensor from_local inp device_mesh Shard run_check=False redistribute device_mesh Shard to_local assertEqual inp output with_comms test_parallelize_module_with_star inp_size = model = MLPModule device_type device_mesh = DeviceMesh device_type torch arange world_size model_tp = deepcopy model model_tp = parallelize_module model_tp device_mesh net ColwiseParallel output_layouts=Replicate _compare_module model model_tp inp_size rank _only=False with_comms test_parallelize_module_src_data_rank set seed different each rank torch manual_seed rank model = MLPModule device_type device_mesh = DeviceMesh device_type torch arange world_size comm_mode = CommDebugMode test src_data_rank == comm_mode model_tp = deepcopy model model_tp = parallelize_module model_tp device_mesh net ColwiseParallel output_layouts=Replicate src_data_rank= assertTrue comm_mode get_total_counts tp_full_params = param full_tensor param model_tp parameters rank == orig_model_params = list model parameters idx param enumerate tp_full_params assertEqual param orig_model_params idx test src_data_rank == None model_tp_no_comm = deepcopy model comm_mode parallelize_module model_tp_no_comm device_mesh net ColwiseParallel net RowwiseParallel src_data_rank=None assertEqual comm_mode get_total_counts with_comms test_parallelize_module_with_question inp_size = model = MLPModule device_type device_mesh = DeviceMesh device_type torch arange world_size model_tp = deepcopy model model_tp = parallelize_module model_tp device_mesh net ColwiseParallel output_layouts=Replicate _compare_module model model_tp inp_size rank _only=False with_comms test_parallelize_module_with_digit inp_size = model = MLPModule device_type device_mesh = DeviceMesh device_type torch arange world_size model_tp = deepcopy model model_tp = parallelize_module model_tp device_mesh net - ColwiseParallel output_layouts=Replicate _compare_module model model_tp inp_size rank _only=False with_comms test_parallelize_module_multi_wildcard inp_size = model = MLPStacked device_type n_layers= device_mesh = DeviceMesh device_type torch arange world_size model_tp = deepcopy model model_tp = parallelize_module model_tp device_mesh layers net ColwiseParallel layers net RowwiseParallel _compare_module model model_tp inp_size rank _only=False with_comms test_parallelize_module_with_root_module inp_size = model = MLPModule device_type device_mesh = DeviceMesh device_type torch arange world_size model_tp = deepcopy model model_tp = parallelize_module model_tp device_mesh PrepareModuleInputOutput input_layouts=Replicate desired_input_layouts=Shard output_layouts=Shard desired_output_layouts=Replicate net ColwiseParallel input_layouts=Shard net RowwiseParallel output_layouts=Shard _compare_module model model_tp inp_size rank _only=False with_comms test_parallelize_module_with_no_match inp_size = model = MLPModule device_type device_mesh = DeviceMesh device_type torch arange world_size model_tp = deepcopy model assertWarns UserWarning model_tp = parallelize_module model_tp device_mesh net hello world ColwiseParallel net ColwiseParallel net RowwiseParallel net ColwiseParallel _compare_module model model_tp inp_size rank _only=False with_comms test_under_devicemesh_context test ColwiseParallel inp_size = colwise = ColwiseParallel output_layouts=Replicate torch manual_seed model = torch nn Linear device=self device_type model_tp = deepcopy model Call parallelize_module under DeviceMesh context device_mesh = DeviceMesh device_type list range world_size device_mesh model_tp = parallelize_module model_tp parallelize_plan=colwise _compare_module model model_tp inp_size with_comms test_empty_plan torch manual_seed model = torch nn Linear device=self device_type Call parallelize_module empty plan Goal crash device_mesh = DeviceMesh device_type list range world_size assertWarns UserWarning parallelize_module model device_mesh __name__ == __main__ run_tests