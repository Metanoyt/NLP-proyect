Owner s oncall distributed This test file contains positive tests c d NCCL backend During test expected ProcessGroup will aborted destroyed incur fatal error Please mindful when adding tests here If you need add tests group creation abort destroy please add tests test_c d_nccl py There two ways launch tests file Run file directly ` python test_c d_ops_nccl py ` Use multi-process launcher e g ` torchrun -- standalone -- nproc-per-node test_c d_ops_nccl py ` math os sys torch torch distributed c d c d is_available c d is_nccl_available print c d NCCL available skipping tests file=sys stderr sys exit torch distributed dist torch testing _internal common_cuda TEST_MULTIGPU torch testing _internal common_distributed init_multigpu_helper MultiProcContinuousTest requires_nccl requires_nccl_version sm_is_or_higher_than torch testing _internal common_utils run_tests skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN TEST_WITH_DEV_DBG_ASAN print Skip ASAN torch + multiprocessing spawn have known issues file=sys stderr sys exit ProcessGroupNCCLOpTest MultiProcContinuousTest classmethod backend_str cls - str nccl classmethod opts cls high_priority_stream=False opts = c d ProcessGroupNCCL Options opts is_high_priority_stream = high_priority_stream opts property rank_to_GPU rank GPU map init_multigpu_helper world_size nccl requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_empty_tensors pg = pg local_device_idx = rank_to_GPU rank xs = torch FloatTensor cuda local_device_idx pg broadcast xs wait assertEqual xs numel pg allreduce xs wait assertEqual xs numel pg reduce xs wait assertEqual xs numel ys = torch FloatTensor cuda local_device_idx _ range world_size pg allgather ys xs wait y ys assertEqual y numel ys = torch FloatTensor cuda local_device_idx xs = torch FloatTensor cuda local_device_idx _ range world_size pg reduce_scatter ys xs wait assertEqual ys numel requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_broadcast_ops pg = pg broadcast xs rootRank rootTensor opts = c d BroadcastOptions opts rootRank = rootRank opts rootTensor = rootTensor work = pg broadcast xs opts work wait xs Every rank root once i range world_size Run input tensor x = torch tensor rank cuda rank_to_GPU rank output = broadcast x i assertEqual torch tensor i output expected_tensor = torch empty i + i + fill_ i + xs = torch empty i + i + fill_ - cuda device=device_idx device_idx rank_to_GPU rank test multiple input tensors multiple gpu one rank j range len xs rank == i xs j = expected_tensor cuda device=self rank_to_GPU rank j broadcast xs i j tensor xs assertEqual tensor expected_tensor requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_sparse_allreduce_ops pg = pg indices = torch tensor values = torch tensor sparse_tensor = torch sparse_coo_tensor indices values size= rank sparse allreduce call wrapped try catch since c d API only available nccl experimental branch try tensor_list = sparse_tensor work = pg allreduce tensor_list work wait tensor_list list size allreduce output dense tensor = torch tensor rank assertEqual tensor_list except RuntimeError e NCCL does support all_reduce sparse tensors str e pass Rethrow exception s different error raise requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_allreduce_ops pg = pg local_device_id = rank_to_GPU rank allreduce tensors op opts = c d AllreduceOptions opts reduceOp = op work = pg allreduce tensors opts work wait Sum tensors = torch tensor rank + cuda local_device_id allreduce tensors c d ReduceOp SUM ndev = world_size assertEqual torch tensor ndev ndev + tensors Avg only available NCCL + torch cuda nccl version = tensors = torch tensor rank + cuda local_device_id allreduce tensors c d ReduceOp AVG ndev = world_size assertEqual torch tensor ndev ndev + ndev tensors Premul Sum torch cuda nccl version = dtype torch half torch float torch double factor torch tensor device=local_device_id dtype=dtype tensors = torch tensor rank + cuda local_device_id dtype=dtype allreduce tensors c d _make_nccl_premul_sum factor assertEqual factor torch tensor world_size world_size + dtype=dtype device=local_device_id tensors Product tensors = torch tensor rank + cuda local_device_id allreduce tensors c d ReduceOp PRODUCT assertEqual torch tensor math factorial world_size tensors Min tensors = torch tensor rank + cuda local_device_id allreduce tensors c d ReduceOp MIN assertEqual torch tensor tensors Max tensors = torch tensor rank + cuda local_device_id allreduce tensors c d ReduceOp MAX assertEqual torch tensor world_size tensors op err zip c d ReduceOp BAND c d ReduceOp BOR c d ReduceOp BXOR ReduceOp BAND ReduceOp BOR ReduceOp BXOR assertRaisesRegex ValueError Cannot use + err + NCCL allreduce tensors op requires_nccl_version Need NCCL + Float skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_allreduce_float device = torch device cuda rank_to_GPU rank sm_is_or_higher_than device skipTest Float requires sm = numel = tensor = torch ones numel dtype=torch float device=device torch float _e m fn dist all_reduce tensor expected = torch empty_like tensor fill_ world_size torch float _e m fn torch testing assert_close tensor expected requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_alltoall_ops_with_cudafree_race pg = pg opts = c d AllToAllOptions local_device = f cuda rank_to_GPU rank torch cuda set_device local_device input = torch rand device=local_device output = torch rand device=local_device race_tensors = create some tensors race alltoall collective _ range tmp = i range tmp append torch rand + i device=local_device race_tensors append tmp _ range race_tensors pop work = pg alltoall_base output input opts triggers cudaFree torch cuda empty_cache work wait torch cuda synchronize device=local_device requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_allreduce_in_cudagraph local_device_idx = rank_to_GPU rank This device setting needed CUDAGraph API understand which device find current stream torch cuda set_device local_device_idx xs = torch FloatTensor cuda local_device_idx single warmup c d all_reduce xs group=self pg + + = world_size expected_val = world_size assertEqual xs item expected_val Use loop test re-capture _ range graph = torch cuda CUDAGraph torch cuda graph graph c d all_reduce xs group=self pg c d broadcast xs src= group=self pg Graph capture should change tensor value assertEqual xs item expected_val graph replay expected_val = world_size graph replay expected_val = world_size assertEqual xs item expected_val requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_nccl_watchdog_cudagraph test watchdog does crash graphs disallowed event query pg = pg rank = rank_to_GPU rank torch cuda device rank _ range xs = torch FloatTensor cuda rank _ range pg allreduce xs wait graph = torch cuda CUDAGraph torch cuda graph graph xs += pg allreduce xs wait pg allreduce xs wait pg allreduce xs wait xs += _ range graph replay requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_reduce_ops pg = pg local_device_id = rank_to_GPU rank reduce xs rootRank rootTensor op=None opts = c d ReduceOptions opts rootRank = rootRank opts rootTensor = rootTensor op opts reduceOp = op work = pg reduce xs opts work wait every root tensor rt range world_size tensors = torch tensor rank + cuda local_device_id reduce tensors rt rank == rt assertEqual torch tensor world_size world_size + tensors assertEqual torch tensor rank + tensors op err zip c d ReduceOp BAND c d ReduceOp BOR c d ReduceOp BXOR ReduceOp BAND ReduceOp BOR ReduceOp BXOR assertRaisesRegex ValueError Cannot use + err + NCCL reduce tensors rank rt op Premul sum torch cuda nccl version = factor torch tensor device=local_device_id isinstance factor torch Tensor factor_ref = factor cpu item factor_ref = factor float_tensors = torch tensor rank + device=f cuda local_device_id float_tensors_ref = torch tensor rank + factor_ref device=f cuda local_device_id reduce float_tensors_ref rt reduce float_tensors rt c d _make_nccl_premul_sum factor rank == rt assertEqual float_tensors_ref float_tensors requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_allgather_ops pg = pg local_device_ids = rank_to_GPU rank allgather output_ts input_ts work = pg allgather output_ts input_ts work wait tensors = torch empty fill_ cuda device=i i local_device_ids output_tensors = expected_output = output_per_gpu = torch empty fill_ - len local_device_ids world_size expected_per_gpu = torch empty fill_ len local_device_ids world_size gpu local_device_ids output_tensors append t cuda device=gpu t output_per_gpu expected_output append t cuda device=gpu t expected_per_gpu allgather output_tensors tensors Verification assertEqual output_tensors expected_output requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_allgather_base_ops pg = pg local_device_id = rank_to_GPU rank allgather_base output_t input_t work = pg _allgather_base output_t input_t work wait allgather_base GPU number agnostic Each rank contribute one tensor regardless GPU counts tensor = torch tensor rank cuda local_device_id output_t = torch empty world_size dtype=tensor dtype cuda local_device_id allgather_base output_t tensor Verification assertEqual torch arange world_size output_t requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_allgather_base_basics pg = pg local_device_id = rank_to_GPU rank allgather_base output_t input_t work = pg _allgather_base output_t input_t work wait anticipate error assertRaisesRegex ValueError output tensor size must equal world_size times input tensor size tensor = torch tensor rank cuda local_device_id output_t = torch empty world_size + dtype=tensor dtype cuda local_device_id fails check because output_t correctly sized allgather_base output_t tensor anticipate error assertRaisesRegex TypeError output tensor must have same type input tensor tensor = torch tensor rank dtype=torch float cuda local_device_id output_t = torch empty world_size + dtype=torch long cuda local_device_id fails check because dtype different allgather_base output_t tensor requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_gather_ops pg = pg local_device_ids = rank_to_GPU rank num_gpus = len local_device_ids gather output_t input_t rootRank opts = c d GatherOptions opts rootRank = rootRank rootRank == rank work = pg gather output_t input_t opts work = pg gather input_t opts work wait init input tensors = device_id local_device_ids tensors append torch tensor rank cuda device_id init output output_ts = idx range num_gpus gpu_idx = local_device_ids idx output_ts append rank range world_size output_ts idx append torch tensor - cuda gpu_idx expected = torch tensor rank rank range world_size rank range world_size gather output_ts tensors rank rank == rank assertEqual expected output_ts requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_gather_stress pg = pg local_device_ids = rank_to_GPU rank num_gpus = len local_device_ids gather output_t input_t rootRank opts = c d GatherOptions opts rootRank = rootRank rootRank == rank work = pg gather output_t input_t opts work = pg gather input_t opts work wait stress_length = init input tensors = i range stress_length tensors append device_id local_device_ids tensors i append torch tensor rank cuda device_id init output output_ts = i range stress_length output_ts append _ range num_gpus idx ls enumerate output_ts i gpu_idx = local_device_ids idx _ range world_size ls append torch tensor - cuda gpu_idx expected = torch tensor rank rank range world_size i range stress_length rank range world_size gather output_ts i tensors i rank Verification rank == rank assertEqual output_ts i expected requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_gather_checks pg = pg device_id = rank_to_GPU rank init input tensor = torch tensor rank cuda device_id init output output_ts = _ range world_size output_ts append torch tensor - cuda device_id assertRaisesRegex ValueError invalid root rank opts = c d GatherOptions opts rootRank = - pg gather output_ts tensor opts assertRaisesRegex TypeError incompatible function arguments pg gather output_ts tensor assertRaisesRegex ValueError invalid root rank opts = c d GatherOptions opts rootRank = world_size pg gather output_ts tensor opts assertRaisesRegex throws error message dispatcher RuntimeError There no tensor arguments function opts = c d GatherOptions opts rootRank = pg gather output_ts opts requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_scatter_ops pg = pg local_device_ids = rank_to_GPU rank num_gpus = len local_device_ids scatter output_t input_t rootRank opts = c d ScatterOptions opts rootRank = rootRank rootRank == rank work = pg scatter output_t input_t opts work = pg scatter output_t opts work wait init output tensors = device_id local_device_ids tensors append torch tensor - cuda device_id init input scatter_list = idx range num_gpus gpu_idx = local_device_ids idx scatter_list append rank range world_size scatter_list idx append torch tensor rank cuda gpu_idx test each rank scatter expected = torch tensor rank rank range world_size scatter tensors scatter_list rank assertEqual expected tensors requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_scatter_stress pg = pg local_device_ids = rank_to_GPU rank num_gpus = len local_device_ids scatter output_t input_t rootRank opts = c d ScatterOptions opts rootRank = rootRank rootRank == rank work = pg scatter output_t input_t opts work = pg scatter output_t opts work wait stress_length = init output tensors = i range stress_length tensors append device_id local_device_ids tensors i append torch tensor - cuda device_id init input scatter_list = i range stress_length scatter_list append _ range num_gpus idx ls enumerate scatter_list i gpu_idx = local_device_ids idx rank range world_size ls append torch tensor rank cuda gpu_idx test each rank scatter expected = torch tensor rank i range stress_length rank range world_size scatter tensors i scatter_list i rank Verification assertEqual tensors i expected requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_scatter_checks pg = pg local_device_ids = rank_to_GPU rank num_gpus = len local_device_ids init output tensors = device_id local_device_ids tensors append torch tensor - cuda device_id init input scatter_list = idx range num_gpus gpu_idx = local_device_ids idx scatter_list append rank range world_size scatter_list idx append torch tensor rank cuda gpu_idx assertRaisesRegex ValueError invalid root rank opts = c d ScatterOptions opts rootRank = - pg scatter tensors scatter_list opts assertRaisesRegex TypeError incompatible function arguments pg scatter tensors scatter_list assertRaisesRegex ValueError invalid root rank opts = c d ScatterOptions opts rootRank = world_size pg scatter tensors scatter_list opts assertRaisesRegex throws error message dispatcher RuntimeError There no tensor arguments function opts = c d ScatterOptions opts rootRank = pg scatter scatter_list opts requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_reduce_scatter_base_basics pg = pg local_device_id = rank_to_GPU rank reduce_scatter_base output_t input_t work = pg _reduce_scatter_base output_t input_t work wait anticipate error assertRaisesRegex ValueError input tensor must same size output size times world size input_t = torch tensor rank cuda local_device_id output_t = torch empty world_size + dtype=input_t dtype cuda local_device_id fails check because output_t correctly sized reduce_scatter_base output_t input_t anticipate error assertRaisesRegex TypeError input tensor must same type output tensor tensor = torch tensor rank dtype=torch float cuda local_device_id output_t = torch empty world_size + dtype=torch long cuda local_device_id fails check because dtype different reduce_scatter_base output_t tensor requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_reduce_scatter_v device = torch device cuda rank_to_GPU rank A list tensors different sizes input_list = torch ones i device=device i range world_size The i-th output should have size i output = torch zeros rank device=device work = c d reduce_scatter output input_list group=self pg async_op=True expected = torch ones rank device=device world_size work wait assertEqual expected output requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_all_gather_v device = torch device cuda rank_to_GPU rank A list tensors different sizes output_list = torch zeros i device=device i range world_size The i-th input has size i filled value i input = torch ones rank device=device rank work = c d all_gather output_list input group=self pg async_op=True expected = torch ones i device=device i i range world_size work wait assertEqual expected output_list requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_reduce_scatter_ops pg = pg local_device_ids = rank_to_GPU rank num_gpus = len local_device_ids reduce_scatter outputs input_lists op opts = c d ReduceScatterOptions opts reduceOp = op work = pg reduce_scatter outputs input_lists opts work wait output = torch tensor cuda i i local_device_ids GPU rank Sum tensor_lists = input_per_gpu = i range world_size input_per_gpu append torch tensor rank + i + gpu local_device_ids tensor_lists append t cuda device=gpu t input_per_gpu reduce_scatter output tensor_lists c d ReduceOp SUM i range num_gpus expected = torch tensor + world_size world_size + world_size rank assertEqual expected output i Min reduce_scatter output tensor_lists c d ReduceOp MIN i range num_gpus expected = torch tensor rank + + i assertEqual expected output i Max reduce_scatter output tensor_lists c d ReduceOp MAX i range num_gpus expected = torch tensor rank + world_size + i assertEqual expected output i Product reduce_scatter output tensor_lists c d ReduceOp PRODUCT i range num_gpus prod_val = math perm rank + world_size world_size expected = torch tensor prod_val assertEqual expected output i Test input params overridden scenarios aka when input list output just one tensor Sum output_tensor = torch empty_like input_per_gpu cuda rank input_list = tensor cuda rank tensor input_per_gpu pg reduce_scatter output_tensor input_list c d ReduceOp SUM wait expected = torch tensor + world_size world_size + world_size rank assertEqual expected output_tensor Min pg reduce_scatter output_tensor input_list c d ReduceOp MIN wait expected = torch tensor rank + assertEqual expected output_tensor Max pg reduce_scatter output_tensor input_list c d ReduceOp MAX wait expected = torch tensor rank + world_size assertEqual expected output_tensor Product pg reduce_scatter output_tensor input_list c d ReduceOp PRODUCT wait prod_val = rank + k range world_size prod_val = prod_val rank + + k expected = torch tensor prod_val assertEqual expected output_tensor torch cuda nccl version = factor torch tensor device=self rank isinstance factor torch Tensor factor_ref = factor cpu item factor_ref = factor output = t float t output tensor_lists = t float t tl tl tensor_lists output_ref = t float t output tensor_lists_ref = t float factor_ref t tl tl tensor_lists reduce_scatter output tensor_lists c d _make_nccl_premul_sum factor reduce_scatter output_ref tensor_lists_ref c d ReduceOp SUM assertEqual output_ref output requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_reduce_scatter_base_ops pg = pg local_device_id = rank_to_GPU rank reduce_scatter_base output_t input_t work = pg _reduce_scatter_base output_t input_t work wait reduce_scatter_base GPU number agnostic Each rank contribute one tensor regardless GPU counts output_t = torch empty cuda local_device_id tensor = torch arange world_size dtype=output_t dtype cuda local_device_id reduce_scatter_base output_t tensor Verification assertEqual output_t rank world_size requires_nccl_version Need NCCL + Float skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_reduce_scatter_float device = torch device cuda rank_to_GPU rank sm_is_or_higher_than device skipTest Float requires sm = numel = output_tensor = torch zeros numel dtype=torch float device=device torch float _e m input_tensor = torch ones world_size numel dtype=torch float device=device torch float _e m dist reduce_scatter_tensor output_tensor input_tensor expected = torch empty_like output_tensor fill_ world_size torch float _e m torch testing assert_close output_tensor expected skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_reduce_scatter_bfloat device = torch device cuda rank_to_GPU rank numel = output_tensor = torch zeros numel dtype=torch float device=device torch bfloat input_tensor = torch ones world_size numel dtype=torch float device=device torch bfloat currently only reduce_scatter_tensor supports bfloat dist reduce_scatter_tensor output_tensor input_tensor expected = torch empty_like output_tensor fill_ world_size torch bfloat torch testing assert_close output_tensor expected requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_barrier pg = pg local_device_ids = rank_to_GPU rank allreduce tensors opts = c d AllreduceOptions work = pg allreduce tensors opts work Making collective operate len local_device_ids GPUs tensors_list = _ range len local_device_ids i range len local_device_ids + j range i tensors_list i - append torch tensor j + cuda local_device_ids j works = tensors tensors_list work = allreduce tensors works append work Barrier will ensure all previous work completed pg barrier wait i range len local_device_ids + j range i assertEqual torch tensor j + world_size tensors_list i - j requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_send_recv device = rank_to_GPU rank Generate same random tensor torch manual_seed send_tensor = torch rand device=device rank == dist send send_tensor rank == recv_tensor = torch rand device=device dist recv recv_tensor assertEqual send_tensor recv_tensor requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_send_recv_complex device = rank_to_GPU rank Generate same random tensor torch manual_seed send_tensor = torch rand dtype=torch cfloat device=device rank == dist send send_tensor rank == recv_tensor = torch rand dtype=torch cfloat device=device dist recv recv_tensor assertEqual send_tensor recv_tensor requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_send_recv_object_list device = rank_to_GPU rank val = rank == None object_list = val world_size rank == dist send_object_list object_list device=device rank == dist recv_object_list object_list device=device assertEqual object_list requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_tensor_register_hook os environ TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK = pg = pg local_device_id = rank_to_GPU rank allgather_base output_t input_t work = pg _allgather_base output_t input_t work wait allgather_base GPU number agnostic Each rank contribute one tensor regardless GPU counts tensor = torch tensor rank cuda local_device_id output_t = torch empty world_size dtype=tensor dtype cuda local_device_id allgather_base output_t tensor Verification assertEqual torch arange world_size output_t Unset env del os environ TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK __name__ == __main__ run_tests