collections namedtuple torch torch Tensor cells flat_lstm_cell lstm_cell premul_lstm_cell premul_lstm_cell_no_bias list list T - list T flatten_list lst result = inner lst result extend inner result Define creator function options - inputs params forward backward_setup backward inputs inputs returned forward One can call forward inputs directly params List Tensor all requires_grad=True parameters forward function graph executor module One can call rnn rnn_inputs using outputs creator backward_setup backward_inputs = backward_setup outputs Then we pass backward_inputs backward If None then assumed identity function backward Given ` output = backward_setup forward inputs ` performs backpropagation If None then nothing happens fastrnns bench times forward backward invocations ModelDef = namedtuple ModelDef inputs params forward backward_setup backward lstm_backward_setup lstm_outputs seed=None hx _ = lstm_outputs simple_backward_setup hx seed simple_backward_setup output seed=None assert isinstance output torch Tensor seed torch manual_seed seed grad_output = torch randn_like output output grad_output simple_backward output grad_output kwargs output backward grad_output kwargs pytorch_lstm_creator kwargs input hidden _ module = lstm_inputs return_module=True kwargs ModelDef inputs= input hidden params=flatten_list module all_weights forward=module backward_setup=lstm_backward_setup backward=simple_backward lstm_creator script=True kwargs input hidden params _ = lstm_inputs return_module=False kwargs inputs = input hidden + params ModelDef inputs=inputs params=flatten_list params forward=lstm_factory lstm_cell script backward_setup=lstm_backward_setup backward=simple_backward lnlstm_creator script=True decompose_layernorm=False kwargs assert script True custom_lstms script_lnlstm input_size = kwargs inputSize hidden_size = kwargs hiddenSize seq_len = kwargs seqLength batch_size = kwargs miniBatch ge = script_lnlstm input_size hidden_size decompose_layernorm=decompose_layernorm cuda input = torch randn seq_len batch_size input_size device= cuda states = torch randn batch_size hidden_size device= cuda torch randn batch_size hidden_size device= cuda ModelDef inputs= input states params=ge parameters forward=ge backward_setup=lstm_backward_setup backward=simple_backward dropoutlstm_creator script=True kwargs assert script True custom_lstms LSTMState script_lstm input_size = kwargs inputSize hidden_size = kwargs hiddenSize seq_len = kwargs seqLength batch_size = kwargs miniBatch num_layers = kwargs numLayers ge = script_lstm input_size hidden_size num_layers dropout=True cuda input = torch randn seq_len batch_size input_size device= cuda states = LSTMState torch randn batch_size hidden_size device= cuda torch randn batch_size hidden_size device= cuda _ range num_layers ModelDef inputs= input states params=ge parameters forward=ge backward_setup=lstm_backward_setup backward=simple_backward lstm_premul_creator script=True kwargs input hidden params _ = lstm_inputs return_module=False kwargs inputs = input hidden + params ModelDef inputs=inputs params=flatten_list params forward=lstm_factory_premul premul_lstm_cell script backward_setup=lstm_backward_setup backward=simple_backward lstm_premul_bias_creator script=True kwargs input hidden params _ = lstm_inputs return_module=False kwargs inputs = input hidden + params ModelDef inputs=inputs params=flatten_list params forward=lstm_factory_premul_bias premul_lstm_cell_no_bias script backward_setup=lstm_backward_setup backward=simple_backward lstm_simple_creator script=True kwargs input hidden params _ = lstm_inputs return_module=False kwargs inputs = input + h h hidden + params ModelDef inputs=inputs params=flatten_list params forward=lstm_factory_simple flat_lstm_cell script backward_setup=lstm_backward_setup backward=simple_backward lstm_multilayer_creator script=True kwargs input hidden params _ = lstm_inputs return_module=False kwargs inputs = input hidden flatten_list params ModelDef inputs=inputs params=flatten_list params forward=lstm_factory_multilayer lstm_cell script backward_setup=lstm_backward_setup backward=simple_backward imagenet_cnn_creator arch jit=True creator device= cuda kwargs model = arch device x = torch randn device=device jit model = torch jit trace model x ModelDef inputs= x params=list model parameters forward=model backward_setup=simple_backward_setup backward=simple_backward creator varlen_lstm_inputs minlen= maxlen= numLayers= inputSize= hiddenSize= miniBatch= return_module=False device= cuda seed=None kwargs seed None torch manual_seed seed lengths = torch randint low=minlen high=maxlen size= miniBatch dtype=torch long device=device x = torch randn length inputSize device=device length lengths hx = torch randn numLayers miniBatch hiddenSize device=device cx = torch randn numLayers miniBatch hiddenSize device=device lstm = torch nn LSTM inputSize hiddenSize numLayers device return_module x lengths hx cx lstm all_weights lstm NB lstm all_weights format w_ih w_hh b_ih b_hh = lstm all_weights layer x lengths hx cx lstm all_weights None varlen_lstm_backward_setup forward_output seed=None seed torch manual_seed seed rnn_utils = torch nn utils rnn sequences = forward_output padded = rnn_utils pad_sequence sequences grad = torch randn_like padded padded grad varlen_pytorch_lstm_creator kwargs rnn_utils = torch nn utils rnn sequences _ hidden _ module = varlen_lstm_inputs return_module=True kwargs forward sequences hidden packed = rnn_utils pack_sequence sequences enforce_sorted=False out new_hidden = module packed hidden padded lengths = rnn_utils pad_packed_sequence out XXX It s more efficient store output its padded form might conducive loss computation Un-padding output also makes backward pass x slower padded lengths i i i range lengths size padded new_hidden ModelDef inputs= sequences hidden params=flatten_list module all_weights forward=forward backward_setup=lstm_backward_setup backward=simple_backward varlen_lstm_factory cell script dynamic_rnn sequences list Tensor hiddens tuple Tensor Tensor w_ih Tensor w_hh Tensor b_ih Tensor b_hh Tensor - tuple list Tensor tuple list Tensor list Tensor hx cx = hiddens hxs = hx unbind cxs = cx unbind List output hx cx outputs = hx_outs = cx_outs = batch range len sequences output = hy cy = hxs batch cxs batch inputs = sequences batch unbind seq_idx range len inputs hy cy = cell inputs seq_idx unsqueeze hy cy w_ih w_hh b_ih b_hh output += hy outputs += torch stack output hx_outs += hy unsqueeze cx_outs += cy unsqueeze outputs hx_outs cx_outs script cell = torch jit script cell dynamic_rnn = torch jit script dynamic_rnn dynamic_rnn varlen_lstm_creator script=False kwargs sequences _ hidden params _ = varlen_lstm_inputs return_module=False kwargs inputs = sequences hidden + params ModelDef inputs=inputs params=flatten_list params forward=varlen_lstm_factory lstm_cell script backward_setup=varlen_lstm_backward_setup backward=simple_backward cudnn_layernorm_lstm since cudnn does have Layernorm LSTM we cannot benchmark lowerbound directly Instead we only benchmark forward pass mimicking computation cudnn lstm + seq_len layernorm computation This should serve perf lowerbound Layernorm LSTM forward pass given Layernorm itself invariant lowerbound backward pass hard get since we lose intermediate results we can still optimize layernorm implementation make faster forward lowerbound though layernorm_pytorch_lstm_creator kwargs input hidden _ module = lstm_inputs return_module=True kwargs batch_size = kwargs miniBatch hidden_size = kwargs hiddenSize ln_i = torch nn LayerNorm hidden_size cuda ln_h = torch nn LayerNorm hidden_size cuda ln_c = torch nn LayerNorm hidden_size cuda ln_input = torch randn batch_size hidden_size device= cuda forward input hidden out new_hidden = module input hidden plus seq_len three laynorm cell computation mimic lower bound Layernorm cudnn LSTM forward pass seq_len = len input unbind hy cy = new_hidden i range seq_len ln_i ln_input ln_h ln_input cy = ln_c cy out hy cy ModelDef inputs= input hidden params=flatten_list module all_weights forward=forward backward_setup=lstm_backward_setup backward=None input lstm all_weights format w_ih w_hh b_ih b_hh = lstm all_weights layer output packed_weights format packed_weights w_ih size layer hiddenSize inputSize packed_weights w_hh size layer hiddenSize hiddenSize packed_weights b_ih size layer hiddenSize packed_weights b_hh size layer hiddenSize stack_weights weights unzip_columns mat assert isinstance mat list assert isinstance mat list layers = len mat columns = len mat mat layer col layer range layers col range columns XXX script fns have problems indexing multidim lists so we try avoid them stacking tensors all_weights = weights packed_weights = torch stack param param unzip_columns all_weights packed_weights returns x hx cx all_weights lstm module all_weights params lstm_inputs seqLength= numLayers= inputSize= hiddenSize= miniBatch= dropout= return_module=False device= cuda seed=None seed None torch manual_seed seed x = torch randn seqLength miniBatch inputSize device=device hx = torch randn numLayers miniBatch hiddenSize device=device cx = torch randn numLayers miniBatch hiddenSize device=device lstm = torch nn LSTM inputSize hiddenSize numLayers dropout=dropout cuda device lstm = lstm cuda return_module x hx cx lstm all_weights lstm NB lstm all_weights format w_ih w_hh b_ih b_hh = lstm all_weights layer x hx cx lstm all_weights None lstm_factory cell script dynamic_rnn input Tensor hidden tuple Tensor Tensor w_ih Tensor w_hh Tensor b_ih Tensor b_hh Tensor - tuple Tensor tuple Tensor Tensor hx cx = hidden outputs = inputs = input unbind hy cy = hx cx seq_idx range len inputs hy cy = cell inputs seq_idx hy cy w_ih w_hh b_ih b_hh outputs += hy torch stack outputs hy unsqueeze cy unsqueeze script cell = torch jit script cell dynamic_rnn = torch jit script dynamic_rnn dynamic_rnn premul we re going premultiply inputs weights lstm_factory_premul premul_cell script dynamic_rnn input Tensor hidden tuple Tensor Tensor w_ih Tensor w_hh Tensor b_ih Tensor b_hh Tensor - tuple Tensor tuple Tensor Tensor hx cx = hidden outputs = inputs = torch matmul input w_ih t unbind hy cy = hx cx seq_idx range len inputs hy cy = premul_cell inputs seq_idx hy cy w_hh b_ih b_hh outputs += hy torch stack outputs hy unsqueeze cy unsqueeze script premul_cell = torch jit script premul_cell dynamic_rnn = torch jit script dynamic_rnn dynamic_rnn premul we re going premultiply inputs weights add bias lstm_factory_premul_bias premul_cell script dynamic_rnn input Tensor hidden tuple Tensor Tensor w_ih Tensor w_hh Tensor b_ih Tensor b_hh Tensor - tuple Tensor tuple Tensor Tensor hx cx = hidden outputs = inpSize = input size add bias all timesteps instead going step-by-step results single reduction kernel backward FIXME matmul x y + bias currently goes through jit AD backward formula AD optimized case Workaround mm views inpSize = input size inputs = torch mm input view - inpSize w_ih t + b_ih inputs = inputs view inpSize inpSize - unbind hy cy = hx cx seq_idx range len inputs hy cy = premul_cell inputs seq_idx hy cy w_hh b_hh outputs += hy torch stack outputs hy unsqueeze cy unsqueeze script premul_cell = torch jit script premul_cell dynamic_rnn = torch jit script dynamic_rnn dynamic_rnn simple flat inputs no tuples no list accumulate outputs useful mostly benchmarking older JIT versions lstm_factory_simple cell script dynamic_rnn input hx cx w_ih w_hh b_ih b_hh hy = hx scoping cy = cx scoping inputs = input unbind seq_idx range len inputs hy cy = cell inputs seq_idx hy cy w_ih w_hh b_ih b_hh hy cy script cell = torch jit script cell dynamic_rnn = torch jit script dynamic_rnn dynamic_rnn lstm_factory_multilayer cell script dynamic_rnn input Tensor hidden tuple Tensor Tensor params list Tensor - tuple Tensor tuple Tensor Tensor params_stride = NB assumes biases there hx cx = hidden hy cy = hidden scoping inputs outputs = input unbind layer range hx size hy = hx layer cy = cx layer base_idx = layer params_stride w_ih = params base_idx w_hh = params base_idx + b_ih = params base_idx + b_hh = params base_idx + seq_idx range len inputs hy cy = cell inputs seq_idx hy cy w_ih w_hh b_ih b_hh outputs += hy inputs outputs = outputs torch stack inputs hy unsqueeze cy unsqueeze script cell = torch jit script cell dynamic_rnn = torch jit script dynamic_rnn dynamic_rnn