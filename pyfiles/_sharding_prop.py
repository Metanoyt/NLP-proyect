mypy allow-untyped-defs threading collections abc Callable Sequence functools lru_cache itertools chain typing cast Optional Union torch torch _ops OpOverload torch _subclasses FakeTensorMode torch distributed _functional_collectives _are_we_tracing torch distributed tensor _dtensor_spec DTensorSpec TensorMeta torch distributed tensor _op_schema OpInfo OpSchema OpSpec OpStrategy OutputSharding OutputSpecType RuntimeSchemaInfo StrategyType TupleStrategy torch distributed tensor _utils compute_local_shape_and_global_offset compute_local_stride aten = torch ops aten _length obj - int obj None isinstance obj Sequence len obj LocalLRUCache threading local __init__ user_function Callable - None cache = lru_cache None user_function __call__ args kwargs - object cache args kwargs cache_info cache cache_info cache_clear cache cache_clear ShardingPropagator __init__ - None op_to_rules dict OpOverload Callable OpSchema OutputSharding = op_strategy_funcs dict OpOverload Callable OpSchema StrategyType = op map save static argnum decide reuse sharding prop cache re-run sharding prop op_to_schema_info dict OpOverload RuntimeSchemaInfo = propagate_op_sharding = LocalLRUCache propagate_op_sharding_non_cached op map save indices shape stride args which may need modified sharding prop op_to_shape_and_stride_idx dict OpOverload Union int tuple int int = new factory ops aten new_empty default aten new_full default aten new_ones default aten new_zeros default aten new_empty_strided default view ops aten expand default aten reshape default aten view default aten _unsafe_view default aten select_backward default aten slice_backward default register_sharding_prop_rule op_overload OpOverload rule_func Callable OpSchema OutputSharding schema_info Optional RuntimeSchemaInfo = None Register sharding propagation rule operator op_to_rules op_overload = rule_func schema_info None op_to_schema_info op_overload = schema_info register_op_strategy op_overload OpOverload strategy_func Callable OpSchema StrategyType schema_info Optional RuntimeSchemaInfo = None Register ` OpStrategy ` generator operator During sharding propagation DTensor wants enumerate all acceptable sharding specs ` OpSpec ` operator acceptable we mean operator can executed ` ` _local_tensor ` ` DTensor args kwargs ` ` OpSpec input_specs ` ` output s constitute valid DTensor s ` ` OpSpec output_specs ` ` ` ` strategy_func ` ` function enumerates such acceptable specs operator ` ` op_overload ` ` One general approach write ` ` strategy_func ` ` operator has simple arguments structure e g mm bmm first enumerating all sharding specs operands then filtering out ones valid For example ` ` mm ` ` operands two D tensors both ` ` input ` ` ` ` mat ` ` have sharding placements ` ` Shard ` ` then acceptable ` ` input_specs ` ` Once we have way enumerate all acceptable sharding specs we can use each them construct ` OpSpec ` The ` ` OpSpec input_specs ` ` directly comes sharding spec ` ` OpSpec output_specs ` ` therefore determined e g ` ` Shard ` ` ` ` Shard ` ` yields ` ` Partial ` ` In addition ` OpSpec ` also contains ` ` redistribute_cost ` ` which records redistribution cost each ` OpSpec ` source ` OpStrategy strategies ` target sharding spec each operand The ` ` strategy_func ` ` should ` OpStrategy ` which contains list all ` OpSpec ` s generated above The optional ` ` schema_info ` ` tells which non-DTensor args kwargs could affect cache whether ` ` pytree ` ` needed flatten nested args ` ` static_argnum ` ` marks starting index non-DTensor args should hashed into sharding propagation hash key ` ` static_kwargkey ` ` marks keys non-DTensor kwargs should hashed ` ` needs_pytree ` ` should used when input arg has ` list ` ` dict ` structure For example ` ` aten cat default ` ` op has ` ` List Tensor ` ` argument ` ` tensors ` ` ` ` int ` ` argument ` ` dim ` ` Because ` ` dim ` ` affects sharding propagation result we want pass ` ` RuntimeSchemaInfo static_argnum= ` ` because argument index ` ` dim ` ` Besides we also want set ` ` needs_pytree=True ` ` because ` ` tensors ` ` needs flattened sharding propagation Another example ` ` aten histc default ` ` ` ` histc ` ` has arguments bins min max last two would affect sharding propagation along ` DTensor ` argument ` ` ` ` Since argument index ` ` min ` ` ` schema_info ` should ` RuntimeSchemaInfo static_argnum= ` op_strategy_funcs op_overload = strategy_func schema_info None op_to_schema_info op_overload = schema_info _propagate_tensor_meta_non_cached op_schema OpSchema - Union None TensorMeta Sequence Optional TensorMeta Propagate tensor metadata could either TensorMeta list tuple TensorMetas op_schema op == aten equal default data dependent ops can t used fake propagation None NOTE We must call tracing fake tensor mode so avoids materializing memory Also disable proxy mode tracing prevent these operators inserted fx graph torch fx experimental proxy_tensor disable_proxy_modes_tracing FakeTensorMode disable_proxy_modes_tracing fake_args = op_schema gen_fake_args fake_kwargs = op_schema gen_fake_kwargs fake_out = op_schema op fake_args fake_kwargs isinstance fake_out torch Tensor TensorMeta shape=fake_out shape stride=fake_out stride dtype=fake_out dtype isinstance fake_out tuple list tensor_meta_list list Optional TensorMeta = fake_out_item fake_out isinstance fake_out_item torch Tensor tensor_meta_list append TensorMeta shape=fake_out_item shape stride=fake_out_item stride dtype=fake_out_item dtype tensor_meta_list append None tuple tensor_meta_list isinstance fake_out tuple tensor_meta_list fake tensor tuple tensor none None lru_cache noqa B _propagate_tensor_meta op_schema OpSchema - Union None TensorMeta Sequence Optional TensorMeta Cached version _propagate_tensor_meta_non_cached This private API Use propagate_tensor_meta instead _propagate_tensor_meta_non_cached op_schema propagate_tensor_meta op_schema OpSchema - Union None TensorMeta Sequence Optional TensorMeta Propagate tensor metadata could either TensorMeta list tuple TensorMetas This public API should used cache should used _are_we_tracing _propagate_tensor_meta_non_cached op_schema _propagate_tensor_meta op_schema _create_output_spec_with_new_tensor_meta op OpOverload output_specs OutputSpecType output_tensor_meta Union None TensorMeta Sequence Optional TensorMeta - OutputSpecType Wrap output_specs tensor metadata output isinstance output_specs DTensorSpec isinstance output_tensor_meta TensorMeta Either error due ShardingPropagator due incorrect OutputSpec isinstance output_tensor_meta tuple list raise ValueError ShardingPropagator error output does have associated TensorMeta raise ValueError f For op op name ` output_specs ` has output which does equal f number op outputs len output_tensor_meta output_specs shallow_copy_with_tensor_meta output_tensor_meta isinstance output_specs tuple list new_specs list Optional DTensorSpec = isinstance output_tensor_meta tuple list len output_specs = len output_tensor_meta raise ValueError f For op op name ` output_specs ` has len output_specs outputs which does equal f number op outputs _length output_tensor_meta i spec enumerate output_specs isinstance spec DTensorSpec output_tensor_meta_i = output_tensor_meta i isinstance output_tensor_meta_i TensorMeta NOTE aten convolution_backward default exception needs extra handling because first Tensor output tuple can ` None ` input Tensor convolution op has ` requires_grad=False ` e g convolution layer first layer model We explicitly allow its corresponding TensorMeta ` None ` op == aten convolution_backward default i == output_tensor_meta_i None assert isinstance output_specs list new_specs append None continue raise ValueError f ShardingPropagator error output i op name does have associated TensorMeta new_specs append spec shallow_copy_with_tensor_meta output_tensor_meta_i new_specs append spec tuple new_specs assert output_specs None output_specs _wrap_with_op_strategy op_schema OpSchema - OpSchema wrap op_schema contains DTensorSpec another op_schema contains OpStrategy TupleStrategy returned op_schema then used sharding strategy propagation pytorch operators spec_to_strategy spec object - object isinstance spec DTensorSpec OpStrategy OpSpec spec isinstance spec list tuple len spec isinstance spec DTensorSpec tensor list create tuple strategy tuple_strategy = spec_to_strategy s s spec tuple_strategy = cast Sequence StrategyType tuple_strategy TupleStrategy tuple tuple_strategy isinstance spec tuple tuple_strategy spec args_op_strategy = spec_to_strategy i i op_schema args_schema kwargs_op_strategy = k spec_to_strategy v k v op_schema kwargs_schema items OpSchema op=op_schema op args_schema=tuple args_op_strategy kwargs_schema=kwargs_op_strategy schema_info=op_schema schema_info propagate op_info OpInfo - None We cannot use lru cache we know inputs will have dynamic shapes because SymInts hashable This generally ok because only happens during tracing torch compile tracing does need fast eagermode DTensor usages _are_we_tracing output_sharding = propagate_op_sharding_non_cached op_info schema output_sharding = cast OutputSharding propagate_op_sharding op_info schema op_info output_sharding = output_sharding propagate_op_sharding_non_cached op_schema OpSchema - OutputSharding Propagate sharding operator given op_schema special case op we don t need propagate local scalar TODO figure out better way handle op_schema op aten _local_scalar_dense default OutputSharding None op_schema out_tensor_meta = _propagate_tensor_meta_non_cached op_schema op_schema op op_strategy_funcs wrap op_schema op strategy sharding strategy propagation strategy_schema = _wrap_with_op_strategy op_schema run sharding strategy propagation generation op_strategy = op_strategy_funcs op_schema op strategy_schema isinstance op_strategy OpStrategy single Op strategy output_strategy = _select_strategy op_strategy op_schema check we need redistribute input needs_redistribute = False check we want use args value redistribute_schema use_val_from_redistribute_schema = False expected_input_specs list DTensorSpec = case where op does specify input_specs output_specs DTensorSpec we use output_specs spec each DTensor input arg output_strategy input_specs None assert isinstance output_strategy output_specs DTensorSpec idx input_spec enumerate op_schema args_spec desired_spec = output_strategy output_spec output_strategy input_specs None output_strategy input_specs idx expected_input_specs append desired_spec shallow_copy_with_tensor_meta input_spec tensor_meta input_spec placements = desired_spec placements needs_redistribute = True suggestion_schema = None needs_redistribute suggestion_schema = OpSchema op_schema op tuple expected_input_specs suggestion_schema _inplace_rewrap_schema_suggestion op_schema shape stride args need modified view ops new factory ops potentially op_schema op op_to_shape_and_stride_idx assert isinstance output_strategy output_spec DTensorSpec It happens when output has same shape input input placements all Replicate output_strategy output_spec is_sharded schema = suggestion_schema op_schema assert isinstance out_tensor_meta TensorMeta suggestion_schema = _adjust_shape_and_stride_args out_tensor_meta schema output_strategy output_spec needs_redistribute = True use_val_from_redistribute_schema = True construct output spec op op_schema return_type_tuple_tensor_like ops multiple tensors output_specs tuple we use tuple single output spec new output_specs output_specs OutputSpecType = output_strategy output_specs isinstance output_specs DTensorSpec output_specs = tuple create new DTensorSpec same placement output_specs output_strategy DTensorSpec mesh=output_specs mesh placements=output_specs placements tensor_meta=output_specs tensor_meta _ range len op_schema op _schema returns op_schema return_type_tensor op_schema return_type_list_tensor_like output_specs = output_strategy output_specs output_specs = None output_sharding = OutputSharding output_specs suggestion_schema needs_redistribute=needs_redistribute use_val_from_redistribute_schema=use_val_from_redistribute_schema isinstance op_strategy TupleStrategy tuple strategy output sharding processing runtime select OpSpec each TupleStrategy input arg selected_strategies list OpSpec = out_spec_list list DTensorSpec = strategy op_strategy children assert isinstance strategy OpStrategy selected_strategy = _select_strategy strategy selected_strategies append selected_strategy out_spec_list append selected_strategy output_spec needs_redistribute = False suggestion_args list object = tensor_or_list_tensor_arg_idx = arg op_schema args_schema arg isinstance arg list tuple isinstance arg DTensorSpec expected_input_spec_list list DTensorSpec = idx arg_spec enumerate arg expected_input_spec = selected_strategies idx input_spec tensor_or_list_tensor_arg_idx expected_input_spec = expected_input_spec shallow_copy_with_tensor_meta arg_spec tensor_meta arg_spec placements = expected_input_spec placements needs_redistribute = True expected_input_spec_list append expected_input_spec suggestion_args append tuple expected_input_spec_list isinstance arg tuple expected_input_spec_list tensor_or_list_tensor_arg_idx += isinstance arg DTensorSpec expected_input_spec = selected_strategies input_spec tensor_or_list_tensor_arg_idx expected_input_spec = expected_input_spec shallow_copy_with_tensor_meta arg tensor_meta arg placements = expected_input_spec placements needs_redistribute = True suggestion_args append expected_input_spec tensor_or_list_tensor_arg_idx += suggestion_args append arg suggestion_schema = None needs_redistribute suggestion_schema = OpSchema op_schema op tuple suggestion_args op_schema kwargs_schema output_sharding = OutputSharding tuple out_spec_list out_tensor_meta None None suggestion_schema needs_redistribute=needs_redistribute use_val_from_redistribute_schema=False raise ValueError Unsupported op strategy type associate output sharding output tensor metadata new_output_spec = _create_output_spec_with_new_tensor_meta op_schema op output_sharding output_spec out_tensor_meta output_sharding output_spec = new_output_spec output_sharding op_schema op op_to_rules propagate sharding rule sharding_prop_func = op_to_rules op_schema op step there s sharding propagation rule run sharding propagation get output sharding try output_sharding = sharding_prop_func op_schema except NotImplementedError e raise e except Exception e raise RuntimeError f Sharding propagation failed op op_schema \nError e e step can t get output_spec sharding propagation i e no rules apply input placements we output sharding schema suggestions which can used decide how do redistribute inputs output_sharding output_spec None output_sharding redistribute_schema None raise RuntimeError f Sharding propagation failed op op_schema we do auto redistribute inputs necessary run sharding propagation again suggested schema propagation_res = sharding_prop_func output_sharding redistribute_schema we set output sharding new propagation result so dispatching know both output_spec redistribute_schema exist which indicates reshard needed output_sharding output_spec = propagation_res output_spec output_sharding needs_redistribute = True associate output sharding output tensor metadata new_output_spec = _create_output_spec_with_new_tensor_meta op_schema op output_sharding output_spec out_tensor_meta output_sharding output_spec = new_output_spec output_sharding raise NotImplementedError f Operator op_schema op does have sharding strategy registered _select_strategy strategy OpStrategy op_schema Optional OpSchema = None - OpSpec len strategy strategies == short cut only one possible OpSpec strategy strategies op_spec_costs list float = no_redistribute_strategy_index int = - strategy_idx op_spec enumerate strategy strategies assert op_spec redistribute_cost None must set redistribute cost each OpSpec redistribute_cost = sum chain from_iterable op_spec redistribute_cost op_spec_costs append redistribute_cost If there s no redistribute cost we record index strategy which doesn t need redistribute TODO Currently only applies OpStrategy selection Requires extra logic make work TupleStrategy needed op_schema None redistribute_cost == needs_redistribute = False spec_idx input_spec enumerate op_schema args_spec desired_spec = op_spec output_spec op_spec input_specs None op_spec input_specs spec_idx input_spec placements = desired_spec placements needs_redistribute = True break needs_redistribute no_redistribute_strategy_index = strategy_idx eager execution we just select one minimal redistribute cost min_cost = min op_spec_costs min_cost If there s negative cost we select one minimal cost even means we need redistribute e g via local chunking E g can happen ops op_to_shape_and_stride_idx when inputs outputs sharded selected_strategy_index = op_spec_costs index min_cost min_cost == no_redistribute_strategy_index = - If there s no redistribute cost we select one no redistribute selected_strategy_index = no_redistribute_strategy_index selected_strategy_index = op_spec_costs index min_cost strategy strategies selected_strategy_index _adjust_shape_and_stride_args out_tensor_meta TensorMeta schema OpSchema spec DTensorSpec - OpSchema shape_stride_idx = op_to_shape_and_stride_idx schema op isinstance shape_stride_idx tuple shape_idx stride_idx = shape_stride_idx shape_idx = shape_stride_idx stride_idx = None expected_input_schema = list schema args_schema adjust shape same _local_tensor DTensor input arg index which inferred expected_input_schema shape_idx _ = compute_local_shape_and_global_offset out_tensor_meta shape spec mesh spec placements adjust stride arg aten new_empty_strided default stride_idx expected_input_schema stride_idx = compute_local_stride out_tensor_meta stride spec mesh spec placements OpSchema schema op tuple expected_input_schema schema kwargs_schema