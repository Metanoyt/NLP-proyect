Owner s module sdpa contextlib functools partial collections namedtuple os sys torch torch nn nn torch nn functional F torch nn functional scaled_dot_product_attention torch nn attention sdpa_kernel SDPBackend torch nn attention bias CausalVariant causal_lower_right causal_upper_left torch nn parameter Parameter unittest unittest mock patch MagicMock ANY math itertools torch optim optim torch testing _internal common_device_type expectedFailureMPS instantiate_device_type_tests onlyCUDA largeTensorTest typing Optional torch utils cpp_extension torch testing _internal common_nn NNTestCase torch testing _internal common_utils TEST_WITH_ROCM skipIfRocm skipIfRocmArch MI _ARCH skipIfTorchDynamo TEST_FAIRSEQ run_tests parametrize freeze_rng_state TEST_WITH_CROSSREF slowTest set_default_dtype gradcheck make_tensor NOTEST_CPU IS_WINDOWS TEST_WITH_TORCHDYNAMO TEST_XPU torch _dynamo testing CompileCounterWithBackend torch testing _internal common_methods_invocations wrapper_set_seed torch testing _internal common_cuda IS_JETSON SM OrLater PLATFORM_SUPPORTS_FLASH_ATTENTION PLATFORM_SUPPORTS_MEM_EFF_ATTENTION PLATFORM_SUPPORTS_FUSED_ATTENTION PLATFORM_SUPPORTS_CUDNN_ATTENTION tf _on_and_off tf _enabled TEST_FAIRSEQ fairseq models transformer fairseq_transformer SdpaShape = namedtuple Sdpa_Shape batch num_heads seq_len head_dim Tolerances = namedtuple Tolerances atol rtol contextlib contextmanager use_deterministic_algorithims mode bool warn_only bool r This context manager can used temporarily enable disable deterministic algorithms Upon exiting context manager previous state flag will restored previous_mode bool = torch are_deterministic_algorithms_enabled previous_warn_only bool = torch is_deterministic_algorithms_warn_only_enabled try torch use_deterministic_algorithms mode warn_only=warn_only yield finally torch use_deterministic_algorithms previous_mode warn_only=previous_warn_only Found torch testing _comparison py default_atol = torch float e- torch bfloat e- torch float e- default_rtol = torch float e- torch bfloat e- torch float e- isSM XDevice = torch cuda is_available torch cuda get_device_capability isSM Device = torch cuda is_available torch cuda get_device_capability == isSM Device = torch cuda is_available torch cuda get_device_capability isSM xDevice = torch cuda is_available torch cuda get_device_capability == isLessThanSM Device = torch cuda is_available torch cuda get_device_capability TEST_WITH_CK = TEST_WITH_ROCM torch backends cuda preferred_rocm_fa_library == torch backends cuda _ROCmFABackends ck _check_equal golden torch Tensor reference torch Tensor test torch Tensor fudge_factor float tensor_name Optional str = None - None Compare test tensor against golden reference tensors Golden highest precision possible serving ground truth Reference same precision test should also serve less precisie ground truth We calcculate reference error comparing golden reference use measruing stick test tensor Raises ValueError compiled error exceeds threshold Args golden torch Tensor The golden tensor compare against reference torch Tensor The reference tensor error calculation test torch Tensor The test tensor evaluated fudge_factor float A multiplier reference error determine threshold tensor_name Optional str optional Name tensor error reporting Defaults None Raises ValueError If test tensor contains NaN values while reference does test error exceeds calculated threshold Notes - For nested tensors function recursively calls itself each nested element - The error threshold calculated maximum default tolerance float product reference error fudge_factor - If test error exceeds threshold ValueError raised detailed message golden is_nested reference is_nested test is_nested gold ref tst zip golden unbind reference unbind test unbind _check_equal gold ref tst fudge_factor tensor_name Compute error between golden test_error = golden - test abs max ref_error = golden - reference abs max torch isnan test_error any torch isnan ref_error any raise ValueError Output Grad NaN Calculate error threshold maximum A predefined default tolerance float The reference error multiplied fudge factor threshold = max default_atol torch float ref_error fudge_factor test_error threshold name = tensor_name msg = f name Test error test_error greater than threshold threshold raise ValueError msg check_out_and_grad out_tuple tuple torch Tensor torch Tensor torch Tensor grad_query_tuple tuple torch Tensor torch Tensor torch Tensor grad_key_tuple tuple torch Tensor torch Tensor torch Tensor grad_value_tuple tuple torch Tensor torch Tensor torch Tensor grad_attn_mask_tuple Optional tuple torch Tensor torch Tensor torch Tensor = None fudge_factors Optional dict str float = None - None Check output gradients attention mechanism tensors Compares compiled results against reference low-precision reference tensors Args out_tuple Tuple ref lp_ref compiled output tensor grad_query_tuple Tuple ref lp_ref compiled query gradient grad_key_tuple Tuple ref lp_ref compiled key gradient grad_value_tuple Tuple ref lp_ref compiled value gradient grad_attn_mask_tuple Optional tuple ref lp_ref compiled attention mask gradient fudge_factors Dictionary fudge factors each tensor type default uses all default_fudge_factor = fudge_factors None fudge_factors = out_ref out_lp_ref out = out_tuple torch no_grad _check_equal out_ref out_lp_ref out fudge_factors get out default_fudge_factor out grad_checks = grad_query_tuple grad_query grad_key_tuple grad_key grad_value_tuple grad_value grad_tuple name grad_checks ref_grad lp_ref_grad comp_grad = grad_tuple _check_equal ref_grad lp_ref_grad comp_grad fudge_factors get name default_fudge_factor name grad_attn_mask_tuple attn_mask_ref_grad attn_mask_ref_lp_grad attn_mask_grad = grad_attn_mask_tuple _check_equal attn_mask_ref_grad attn_mask_ref_lp_grad attn_mask_grad fudge_factors get grad_attn_mask default_fudge_factor grad_attn_mask query_key_value_clones query torch Tensor key torch Tensor value torch Tensor dtype torch dtype = None Clones query key value tensors moves them specified dtype dtype None dtype = query dtype query_ref = query detach clone dtype requires_grad_ query requires_grad key_ref = key detach clone dtype requires_grad_ key requires_grad value_ref = value detach clone dtype requires_grad_ value requires_grad query_ref key_ref value_ref get_platform_specific_sdpa ret = PLATFORM_SUPPORTS_FLASH_ATTENTION ret append SDPBackend FLASH_ATTENTION PLATFORM_SUPPORTS_MEM_EFF_ATTENTION ret append SDPBackend EFFICIENT_ATTENTION PLATFORM_SUPPORTS_CUDNN_ATTENTION ret append SDPBackend CUDNN_ATTENTION ret Add placeholder empty list causes An empty arg_values passed parametrize ret append SDPBackend EFFICIENT_ATTENTION ret PLATFORM_SPECIFIC_SDPA = get_platform_specific_sdpa Indicate Efficient attention backend can support sequence longher than head dimsion larger than MEM_EFF_CAPABILITY_MATCHES_SM = SM OrLater TEST_WITH_ROCM rand_sdpa_tensor shape SdpaShape device str dtype torch dtype type str requires_grad bool = False packed bool = False - torch Tensor Creates rand dense nested tensor given shape type Args shape Tuple int Shape Tensor construct device str which device create tensor dtype torch dtype Tensors dtype type str Nested Dense requires_grad bool optional Tensors grad status Defaults False packed bool optional Whether create single QKV packed Defaults False Returns torch Tensor A new tensor batch num_heads seq_len head_dim = shape batch shape num_heads shape seq_len shape head_dim type == nested isinstance seq_len list _size i seq_len i num_heads head_dim packed seq_len i num_heads head_dim torch nested nested_tensor torch randn _size i device=device dtype=dtype requires_grad=requires_grad i range batch size = seq_len num_heads head_dim packed seq_len num_heads head_dim torch nested nested_tensor torch randn size device=device dtype=dtype requires_grad=requires_grad _ range batch assert isinstance seq_len int size = batch seq_len num_heads head_dim packed batch seq_len num_heads head_dim torch randn size device=device dtype=dtype requires_grad=requires_grad TestTransformers NNTestCase _do_cuda_memory_leak_check = True _do_cuda_non_default_stream = True onlyCUDA unittest skip D mask supported yet - activate when D mask supported test_self_attn_TxT_attn_mask device embed_dim = num_heads = batch_size = tgt_len = query = torch rand batch_size tgt_len embed_dim device=device N T D attn_mask = torch randint tgt_len tgt_len cuda float T T attn_mask = attn_mask masked_fill attn_mask == float -inf masked_fill attn_mask == attn_mask_ d = attn_mask expand batch_size num_heads tgt_len tgt_len mta_model = torch nn MultiheadAttention embed_dim num_heads batch_first=True cuda mta_model eval Generate D results torch inference_mode output_mask_ d = mta_model query query query attn_mask=attn_mask_ d output_mask_ d = output_mask_ d transpose N T D output_mask_TxT = mta_model query query query attn_mask=attn_mask output_mask_TxT = output_mask_TxT transpose N T D assertEqual output_mask_ d output_mask_TxT slowTest test_train_with_pad_and_catch_error device iters = pad_mask = torch tensor dtype=torch bool device layer = nn TransformerEncoderLayer d_model= dim_feedforward= nhead= batch_first=True activation= gelu dropout= criterion = nn MSELoss encoder = nn TransformerEncoder layer device optimizer = optim SGD encoder parameters lr= momentum= encoder train _ range iters encoder train optimizer zero_grad inputs = torch cat torch randn torch zeros dim= device outputs = encoder inputs src_key_padding_mask=pad_mask loss = criterion outputs inputs loss backward optimizer step torch no_grad test = torch cat torch randn torch zeros dim= device Expect uint type supported e = None try encoder test src_key_padding_mask=pad_mask torch uint except AssertionError continue assertFalse e Failed catch unsupported uint type exception test_train_bool = encoder test src_key_padding_mask=pad_mask encoder eval Expect long type supported e = None try encoder test src_key_padding_mask=pad_mask torch int except AssertionError e continue assertFalse e Failed catch unsupported Long type exception test_eval_bool = encoder test src_key_padding_mask=pad_mask l _bool = nn L Loss test_train_bool test_eval_bool item assertTrue l _bool e- Eval Train difference pad_mask BOOL tf _on_and_off parametrize attn_mask_dim None parametrize key_padding_mask_dim None parametrize mask_dtype torch bool torch float test_multiheadattention_fastpath_attn_mask device attn_mask_dim key_padding_mask_dim mask_dtype MHA converts all torch no_grad B = L = D = H = attn_mask_dim == attn_mask = make_tensor L L dtype=mask_dtype device=device attn_mask_dim == attn_mask = make_tensor B L L dtype=mask_dtype device=device expand B H L L reshape B H L L attn_mask_dim None attn_mask = None key_padding_mask_dim == key_padding_mask = make_tensor B L dtype=mask_dtype device=device key_padding_mask_dim None key_padding_mask = None mha = nn MultiheadAttention D H batch_first=True device=device X = torch randn B L D device=device mha train disable fast path out _ = mha X X X attn_mask=attn_mask key_padding_mask=key_padding_mask need_weights=False mha eval enable fast path out_fp _ = mha X X X attn_mask=attn_mask key_padding_mask=key_padding_mask need_weights=False The FP kernel will NaNs while sdpa kernel which ran when fast path turned off returns instead NaNs fully masked rows assertEqual out out_fp nan_to_num parametrize nhead test_transformerencoderlayer_src_mask device nhead batch_size = seqlen = d_model = dim_feedforward = model = torch nn TransformerEncoderLayer d_model=d_model nhead=nhead dim_feedforward=dim_feedforward batch_first=True device src = torch rand batch_size seqlen d_model device bs seqlen d_model src_mask = torch zeros seqlen seqlen torch bool device model src src_mask=src_mask model eval torch no_grad model src src_mask=src_mask parametrize nhead test_transformerencoderlayer_no_fastpath_with_hooks device nhead batch_size = seqlen = d_model = model = torch nn TransformerEncoderLayer d_model=d_model nhead=nhead dim_feedforward=d_model batch_first=True device eval src = torch rand batch_size seqlen d_model device bs seqlen d_model cache = forward hook save output hook module inputs output cache append output detach register hook get output self-attention layer handle = model self_attn register_forward_hook hook forward pass torch inference_mode model src output self-attention layer assert len cache == f Expected output got len cache remove hook handle remove skipIfRocmArch MI _ARCH tf _on_and_off parametrize use_torchscript False parametrize enable_nested_tensor True False parametrize use_autocast True False parametrize d_model test_transformerencoder_fastpath device use_torchscript enable_nested_tensor use_autocast d_model Test TransformerEncoder fastpath output matches slowpath output torch manual_seed nhead = dim_feedforward = d_model batch_first = True model = torch nn TransformerEncoder torch nn TransformerEncoderLayer d_model=d_model nhead=nhead dim_feedforward=dim_feedforward batch_first=batch_first num_layers= enable_nested_tensor=enable_nested_tensor device eval use_torchscript model = torch jit script model each input input mask input_mask_pairs = torch rand d_model torch rand d_model + + softmax cu switches fast- slowpath masked seqlen test torch rand d_model + torch rand d_model + softmax cu switches fast- slowpath masked seqlen test range masks above torch rand d_model + + + input_mask_pairs = torch tensor pair device=device dtype=torch get_default_dtype float input torch tensor pair device=device dtype=torch bool bool mask pair input_mask_pairs maybe_autocast = torch autocast cuda dtype=torch float use_autocast contextlib nullcontext maybe_autocast input src_key_padding_mask input_mask_pairs torch no_grad fastpath_output = model input src_key_padding_mask=src_key_padding_mask slowpath_output = model input src_key_padding_mask=src_key_padding_mask reference Make sure fastpath_output same shape slowpath_output mask When enable_nested_tensor=true fastpath_output may smaller than input tensor Eg input bs= seqlen= we mask out tokens fastpath_output will have bs= seqlen= Expand back old size match bs true_seqlen embed_dim = fastpath_output shape expanded_seqlen = src_key_padding_mask shape fastpath_output_expanded = torch zeros bs expanded_seqlen embed_dim device=device fastpath_output_expanded true_seqlen = fastpath_output no garauntees output corresponding masked tokens so they may vary between slow fast path set all fastpath_output_expanded = fastpath_output_expanded masked_fill src_key_padding_mask unsqueeze - slowpath_output = slowpath_output masked_fill src_key_padding_mask unsqueeze - assertEqual fastpath_output_expanded slowpath_output tf _on_and_off parametrize with_no_grad True False parametrize training True False parametrize enable_nested_tensor False test_transformerencoder_square_input with_no_grad training enable_nested_tensor device Test edge cases when input shape batch size sequence length embedding dimension has batch size == sequence length model = torch nn TransformerEncoder torch nn TransformerEncoderLayer d_model= nhead= dim_feedforward= dropout= batch_first=True num_layers= enable_nested_tensor=enable_nested_tensor device torch no_grad set constant weights model p model parameters x = p data sz = x view - size shape = x shape x = torch cos torch arange sz float view shape p data copy_ x training model = model train model = model eval x = torch arange reshape torch get_default_dtype device src_mask = torch Tensor torch bool device with_no_grad cm = torch no_grad cm = contextlib nullcontext cm result = model x mask=src_mask ref_output = torch Tensor - - - - - - - - device assertEqual tuple result shape tuple ref_output shape assertEqual result ref_output parametrize batch_first True False parametrize training True False parametrize enable_nested_tensor True False test_transformerencoder batch_first training enable_nested_tensor device get_a_test_layer activation batch_first=False d_model = nhead = dim_feedforward = dropout = layer = nn TransformerEncoderLayer d_model nhead dim_feedforward=dim_feedforward dropout=dropout activation=activation batch_first=batch_first device torch no_grad set constant weights model p layer parameters x = p data sz = x view - size shape = x shape x = torch cos torch arange sz float view shape p data copy_ x layer deterministic test TransformerEncoder activation = F relu _test batch_first training enable_nested_tensor perm_fn x x transpose batch_first x encoder_layer = get_a_test_layer activation=activation batch_first=batch_first model = nn TransformerEncoder encoder_layer enable_nested_tensor=enable_nested_tensor device training model = model eval deterministic input encoder_input = perm_fn torch tensor device result = model encoder_input ref_output = perm_fn torch tensor - - - - - - - - - - - - - - - - - - - - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- all src_mask src_mask = torch zeros device == result = model encoder_input mask=src_mask assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- all mask = torch zeros device == result = model encoder_input src_key_padding_mask=mask assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- mask = mask = mask = result = model encoder_input src_key_padding_mask=mask ref_output = perm_fn torch tensor - - - - - - - - - - - - - - - - - - - - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- test case multiple layers no norm model = nn TransformerEncoder encoder_layer enable_nested_tensor=enable_nested_tensor device training model = model eval result = model encoder_input src_key_padding_mask=mask ref_output = perm_fn torch tensor - - - - - - - - - - - - - - - - - - - - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- model = nn TransformerEncoder encoder_layer enable_nested_tensor=enable_nested_tensor device training model = model eval result = model encoder_input src_key_padding_mask=mask ref_output = perm_fn torch tensor - - - - - - - - - - - - - - - - - - - - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- test case multiple layers norm d_model = norm = nn LayerNorm model = nn TransformerEncoder encoder_layer norm=norm enable_nested_tensor=enable_nested_tensor device training model = model eval result = model encoder_input src_key_padding_mask=mask ref_output = perm_fn torch tensor - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- model = nn TransformerEncoder encoder_layer norm=norm enable_nested_tensor=enable_nested_tensor device training model = model eval result = model encoder_input src_key_padding_mask=mask ref_output = perm_fn torch tensor - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- TODO remove set default dtype double making ref_output more precise Added because test copied test_nn py which has default dtype double If default dtype float tests will say tensors close because ref output precision too low set_default_dtype torch double training cm = contextlib nullcontext cm = torch no_grad transformer fast path requires no grad cm _test batch_first training enable_nested_tensor unittest skipIf sys version_info supported pre- Python test_encoder_padding_and_src_mask_bool encoder_layer = nn TransformerEncoderLayer d_model= nhead= dim_feedforward= dropout= activation= relu batch_first=True encoder_norm = nn LayerNorm encoder = nn TransformerEncoder encoder_layer encoder_norm inputs = torch randn src_mask = torch ones dtype=torch bool triu_ diagonal= input_seq_len = torch tensor padding_mask = torch arange None cpu = input_seq_len None assertNoLogs None TEST_WITH_TORCHDYNAMO contextlib nullcontext encoder inputs mask=src_mask src_key_padding_mask=padding_mask unittest skipIf sys version_info supported pre- Python test_decoder_padding_and_src_mask_bool transformer_decoder inputs input_seq_len memory decoder_layer = nn TransformerDecoderLayer d_model= nhead= dim_feedforward= dropout= activation= relu batch_first=True decoder_norm = nn LayerNorm decoder = nn TransformerDecoder decoder_layer decoder_norm src_mask = torch ones inputs shape inputs shape dtype=torch bool triu_ diagonal= padding_mask = torch arange inputs shape None cpu = input_seq_len None decoder inputs memory tgt_mask=src_mask tgt_key_padding_mask=padding_mask memory_key_padding_mask=padding_mask inputs = torch randn memory = torch randn input_seq_len = torch tensor assertNoLogs None transformer_decoder inputs input_seq_len memory test_encoder_is_causal d_model = layer = torch nn TransformerEncoderLayer d_model batch_first=True layer eval x = torch randn d_model mask = torch nn Transformer generate_square_subsequent_mask x size is_causal_output = layer x src_mask=mask is_causal=True masked_output = layer x src_mask=mask assertEqual masked_output is_causal_output onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Platform does supposrt pre-SM hardware test_math_backend_high_precision xq = torch rand device= cuda dtype=torch bfloat xk = torch rand device= cuda dtype=torch bfloat xv = torch randn device= cuda dtype=torch bfloat mask = None scaled_dot_product_attention xq torch Tensor xk torch Tensor xv torch Tensor mask Optional torch Tensor backend SDPBackend - torch Tensor n_rep = xq xk xv = tensor transpose tensor xq xk xv xk = xk repeat_interleave n_rep dim= xv = xv repeat_interleave n_rep dim= sdpa_kernel backends= backend attn_output = F scaled_dot_product_attention xq xk xv attn_mask=mask dropout_p= attn_output transpose torch backends cuda allow_fp _bf _reduction_math_sdp True sdp_math_low_prec_out = scaled_dot_product_attention xq xk xv mask SDPBackend MATH torch backends cuda allow_fp _bf _reduction_math_sdp False sdp_math_high_prec_out = scaled_dot_product_attention xq xk xv mask SDPBackend MATH sdp_math_fp _out_ref = scaled_dot_product_attention xq double xk double xv double mask SDPBackend MATH bfloat torch testing assert_close sdp_math_high_prec_out sdp_math_fp _out_ref atol= e- rtol= e- assertRaisesRegex AssertionError Tensor-likes close torch testing assert_close sdp_math_low_prec_out sdp_math_fp _out_ref atol= e- rtol= e- onlyCUDA parametrize nb_heads parametrize bias True False test_mha_native_args nb_heads bias B L F = batch_first = True fast_path = True use_pad_mask = bias == mha = nn MultiheadAttention embed_dim=F num_heads=nb_heads batch_first=batch_first bias=bias cuda mha eval ctx = torch no_grad fast_path contextlib nullcontext ctx x = torch randn B L F cuda batch_first x = x transpose pad_mask = None use_pad_mask pad_mask = torch zeros B L dtype=torch bool cuda mha query=x key=x value=x key_padding_mask=pad_mask test_kpm_mask_trailing_column_with_nested_tensor device encoder_layer = nn TransformerEncoderLayer d_model= nhead= dim_feedforward= activation= gelu norm_first=False batch_first=False transformer_encoder = nn TransformerEncoder encoder_layer num_layers= enable_nested_tensor=True device x = torch randn device mask = torch ones mask = here I masked columns instead just one mask = mask bool device out = transformer_encoder src=x src_key_padding_mask=mask assertEqual out shape CPU unit test has_torch_functions test environment preventing successful completion onlyCUDA test_with_nested_tensor_input device encoder_layer = nn TransformerEncoderLayer d_model= nhead= dim_feedforward= activation= gelu norm_first=False batch_first=True transformer_encoder = nn TransformerEncoder encoder_layer num_layers= enable_nested_tensor=True device transformer_encoder eval torch no_grad x = torch randn device mask = torch ones mask = here I masked columns instead just one mask = here I masked columns instead just one mask = here I masked columns instead just one mask = here I masked columns instead just one mask = mask bool device x = torch _nested_tensor_from_mask x mask logical_not mask_check=False out = transformer_encoder src=x src_key_padding_mask=None assertEqual out is_nested True test_script_encoder_subclass device MyCustomLayer nn TransformerEncoderLayer pass encoder = nn TransformerEncoder MyCustomLayer d_model= nhead= num_layers= device=device torch jit script encoder brazenly adapted test_transformerencoderlayer_src_mask test execution torchscripted transformerencoderlayer subclass test_transformerencoderlayer_subclass device MyCustomLayer nn TransformerEncoderLayer pass nhead = batch_size = seqlen = d_model = dim_feedforward = model = MyCustomLayer d_model=d_model nhead=nhead dim_feedforward=dim_feedforward batch_first=True device script_model = torch jit script model src = torch rand batch_size seqlen d_model device bs seqlen d_model src_mask = torch zeros seqlen seqlen torch bool device torch manual_seed result = model src src_mask=src_mask torch manual_seed scripted_result = script_model src src_mask=src_mask assertEqual result scripted_result model eval script_model = torch jit script model torch no_grad result = model src src_mask=src_mask scripted_result = script_model src src_mask=src_mask assertEqual result scripted_result test_transformerencoderlayer_subclass_model device MyCustomLayer nn TransformerEncoderLayer pass nhead = batch_size = seqlen = d_model = dim_feedforward = layer = MyCustomLayer d_model=d_model nhead=nhead dim_feedforward=dim_feedforward batch_first=True model = nn TransformerEncoder layer num_layers= device=device script_model = torch jit script model src = torch rand batch_size seqlen d_model device bs seqlen d_model src_mask = torch zeros seqlen seqlen torch bool device torch manual_seed result = model src mask=src_mask torch manual_seed scripted_result = script_model src mask=src_mask assertEqual result scripted_result model eval script_model = torch jit script model torch no_grad result = model src mask=src_mask scripted_result = script_model src mask=src_mask assertEqual result scripted_result onlyCUDA unittest skipIf TEST_FAIRSEQ Fairseq found test_decoder_only_layer FairseqDecoder torch nn Module __init__ embed_dim attention_heads ffn_embed_dim num_layers embedding_layer torch nn Embedding Must have padding_idx field dropout= normalize_before=False torch_encoder=None torch encoder you can map weights activation= relu super __init__ cfg = fairseq_transformer TransformerConfig cfg decoder embed_dim = embed_dim cfg decoder output_dim = embed_dim cfg decoder attention_heads = attention_heads cfg decoder ffn_embed_dim = ffn_embed_dim cfg dropout = dropout cfg decoder normalize_before = normalize_before cfg decoder layers = num_layers make embedding behavior same other encoders cfg no_token_positional_embeddings = True cfg no_scale_embedding = True cfg activation_fn = activation dictionary = TODO verify what decoder = fairseq_transformer TransformerDecoder cfg dictionary embedding_layer no_encoder_attn=True output_projection=None torch_encoder None decoder = torch_to_fairseq torch_encoder decoder noqa F decoder = decoder eval cuda half forward tokens src_lengths=None with_triangle_mask=False incremental_state=None decoder prev_output_tokens=tokens encoder_out=None incremental_state=incremental_state features_only=True full_context_alignment=not with_triangle_mask alignment_layer=None alignment_heads=None src_lengths=src_lengths return_all_hiddens=False tf _on_and_off parametrize batch_size parametrize input_dim attn_mask_dim is_causal None False False True False True None False False True False True name_fn=lambda input_dim attn_dim is_causal f input_dim D_input_dim_ + f attn_dim D_ causal_ is_causal attn_mask attn_dim None no_attn_mask parametrize dropout_p sdpa_kernel backends= SDPBackend MATH test_scaled_dot_product_attention device batch_size input_dim attn_mask_dim is_causal dropout_p sdp_ref q k v attn_mask=None dropout_p= E = q size - q = q math sqrt E B Nt E x B E Ns - B Nt Ns attn_mask None attn = torch baddbmm attn_mask q k transpose - - attn = torch bmm q k transpose - - attn = torch nn functional softmax attn dim=- dropout_p attn = torch nn functional dropout attn p=dropout_p B Nt Ns x B Ns E - B Nt E output = torch bmm attn v output TODO Support cross-device dtype testing properly when instantiate_device_type_tests used dtypes = torch double torch float dtype dtypes N = batch_size rand_tensor shape torch randn shape device=device dtype=dtype This test compares python C++ implementations SDP N_prime L S E = input_dim == query = rand_tensor N L E key = rand_tensor N S E value = rand_tensor N S E input_dim == query = rand_tensor N N_prime L E key = rand_tensor N N_prime S E value = rand_tensor N N_prime S E fail f Invalid input_dim input_dim encountered SDP test attn_mask = None attn_mask_dim None assert attn_mask_dim input_dim mask_size = L S attn_mask_dim == N L S input_dim == N N_prime L S attn_mask = torch ones mask_size device=device dtype=torch bool tril is_causal torch randint size=mask_size device=device dtype=torch bool freeze_rng_state Python impl only supports float mask D inputs attn_mask_float = attn_mask attn_mask_float None attn_mask_float = torch zeros_like attn_mask dtype=query dtype attn_mask_float masked_fill_ attn_mask logical_not float -inf q k v = query view - L E key view - S E value view - S E = attn_mask_float None attn_mask_dim = view - L S expected = sdp_ref q k v attn_mask=a dropout_p=dropout_p input_dim expected = expected view - N_prime L E freeze_rng_state is_causal NB Don t pass attn_mask here actual = torch nn functional scaled_dot_product_attention query key value None dropout_p is_causal Error case both explicit attn_mask is_causal set assertRaisesRegex RuntimeError Explicit attn_mask should set when is_causal=True torch nn functional scaled_dot_product_attention query key value attn_mask dropout_p is_causal actual = torch nn functional scaled_dot_product_attention query key value attn_mask dropout_p is_causal This test fully masked out rows case torch isnan expected any row_sums = attn_mask sum dim=- masked_out_rows = row_sums == _ range input_dim - attn_mask_dim - masked_out_rows = masked_out_rows unsqueeze masked_out_rows = masked_out_rows expand expected shape - Slice out fully masked rows expected actual expected_masked_out = expected masked_out_rows actual_masked_out = actual masked_out_rows expected_all_nan = torch isnan expected_masked_out all actual_all_zero = actual_masked_out abs sum == assertTrue expected_all_nan assertTrue actual_all_zero assertEqual actual expected attn_mask_dim None q = q double clone k = k double clone v = v double clone q requires_grad_ k requires_grad_ v requires_grad_ assert gradcheck lambda args kwargs wrapper_set_seed sdp_ref args kwargs q k v attn_mask dropout_p assert gradcheck lambda args kwargs wrapper_set_seed torch nn functional scaled_dot_product_attention args kwargs q k v attn_mask dropout_p test_incompatible_mask device ones_tensor shape torch ones shape dtype=torch float S L E H = qkv = ones_tensor S L E mha = nn MultiheadAttention E H mha in_proj_weight = Parameter torch ones E E mha out_proj weight = Parameter torch ones E E qkv = qkv float kpm = ones_tensor S L float -inf am = ones_tensor L L bool func mha qkv qkv qkv need_weights=False key_padding_mask=kpm attn_mask=am assertRaises RuntimeError func unittest skipIf TEST_WITH_CROSSREF Fastpath available crossref torch no_grad test_mask_check_fastpath Test fastpath executed independently masks passed If passed key padding mask left aligned mask_check=False test nested tensors used sparsity fastpath otherwise use fastpath traditional tensors Also test fast path executed both key padding mask attention mask passed same time x = torch Tensor torch float _test_fastpath model key_padding_mask mock_return_value attn_mask=None nested_tensors=True patch torch _transformer_encoder_layer_fwd fastpath_mock fastpath_mock return_value = mock_return_value model x src_key_padding_mask=key_padding_mask mask=attn_mask If mock called fastpath taken assertTrue fastpath_mock called If mock called nested tensors sparsity fastpath taken call_args _ fastpath_mock call_args_list assertEqual call_args is_nested nested_tensors encoder_layer = torch nn TransformerEncoderLayer d_model= nhead= dim_feedforward= batch_first=True model = torch nn TransformerEncoder encoder_layer num_layers= enable_nested_tensor=True mask_check=True model eval aligned_key_padding_mask = torch Tensor torch bool not_aligned_key_padding_mask = torch Tensor torch bool attn_mask = torch Tensor torch bool nested_tensor_return_value = torch nested nested_tensor torch ones dtype=torch float tensor_return_value = torch ones dtype=torch float Left aligned mask results sparsity fastpath _test_fastpath model aligned_key_padding_mask nested_tensor_return_value nested_tensors=True Not aligned mask results fastpath _test_fastpath model not_aligned_key_padding_mask tensor_return_value nested_tensors=False model = torch nn TransformerEncoder encoder_layer num_layers= enable_nested_tensor=False mask_check=True model eval If nested tensor disabled fastpath always taken _test_fastpath model aligned_key_padding_mask tensor_return_value nested_tensors=False _test_fastpath model not_aligned_key_padding_mask tensor_return_value nested_tensors=False Fast path taken both attention mask key padding mask present _test_fastpath model aligned_key_padding_mask tensor_return_value attn_mask=attn_mask nested_tensors=False model = torch nn TransformerEncoder encoder_layer num_layers= enable_nested_tensor=True mask_check=False model eval Mask check disabled results sparisty fastpath independently mask _test_fastpath model aligned_key_padding_mask nested_tensor_return_value nested_tensors=True _test_fastpath model not_aligned_key_padding_mask nested_tensor_return_value nested_tensors=True Test failing MHA when bias NoneType test_bias_is_none x = torch rand model = torch nn modules activation MultiheadAttention bias=False batch_first=True model eval model x x x completes without error test_transformer_bias_is_none device batch_size = seqlen = d_model = nhead = encoder_layer = torch nn TransformerEncoderLayer d_model nhead bias=False batch_first=True device=device encoder_layer eval x = torch randn batch_size seqlen d_model device=device runs without error encoder_layer x assertWarnsRegex UserWarning encoder_layer self_attn passed bias=False encoder = torch nn TransformerEncoder encoder_layer num_layers= eval encoder x assertWarnsRegex UserWarning self_attn passed bias=False transformer = torch nn Transformer d_model=d_model nhead=nhead bias=False batch_first=True device=device eval transformer x x test_train_with_is_causal device training is_causal S L E H = layer = nn TransformerEncoderLayer d_model= dim_feedforward= nhead=H batch_first=True activation= gelu dropout= criterion = nn MSELoss encoder = nn TransformerEncoder layer device optimizer = optim SGD encoder parameters lr= momentum= encoder train encoder train optimizer zero_grad inputs = torch randn S L E device mask = torch nn Transformer generate_square_subsequent_mask inputs size device=device outputs = encoder inputs mask=mask is_causal=True loss = criterion outputs inputs loss backward optimizer step inference is_causal t_qvk = torch randn S L E device=device dtype=torch float mha = nn MultiheadAttention E H device mask = torch nn Transformer generate_square_subsequent_mask S device=device attn_out _ = mha t_qvk t_qvk t_qvk attn_mask=mask is_causal=True Can t give only is_causal assertRaises RuntimeError mha t_qvk t_qvk t_qvk is_causal=True Passing causal mask sets is_causal causal_mask = torch triu torch ones L L device=inputs device float -inf diagonal= torch bool mock_layer = MagicMock torch nn MultiheadAttention E H return_value=inputs encoder layers = mock_layer outputs = encoder inputs mask=causal_mask mock_layer assert_called_with ANY src_mask=ANY is_causal=True src_key_padding_mask=ANY check expected numerical values all kernels is_causal_kernels SDPBackend MATH device is_causal_kernels kernels device ones_tensor shape torch ones shape device=device dtype=torch float device S L E H = qkv = ones_tensor S L E mha = nn MultiheadAttention E H device mha in_proj_weight = Parameter torch ones E E device=device mha out_proj weight = Parameter torch ones E E device=device expected = torch ones size= S L E device mask = torch nn Transformer generate_square_subsequent_mask qkv size device=device kernel kernels sdpa_kernel backends= kernel actual _ = mha qkv qkv qkv attn_mask=mask need_weights=False is_causal=True assertTrue torch equal actual expected kernel = SDPBackend MATH fails embedding size multiple assertRaisesRegex RuntimeError No available kernel qkv_f mha_f = ones_tensor S L nn MultiheadAttention H device mask = torch nn Transformer generate_square_subsequent_mask qkv_f size device=device _ = mha_f qkv_f qkv_f qkv_f attn_mask=mask need_weights=False is_causal=True torch cuda synchronize unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Platform does supposrt fused SDPA pre-SM hardware test_is_causal_gpu device = cuda is_causal_kernels SDPBackend MATH SDPBackend EFFICIENT_ATTENTION device test_script_mha_in_proj_weight_none mha = torch nn MultiheadAttention embed_dim= num_heads= kdim= vdim= eval torch jit script mha unittest skipIf TEST_WITH_CROSSREF Fastpath available crossref torch no_grad test_disable_fastpath device _test_te_fastpath_called model args kwargs=None return_value=None is_called=True kwargs None kwargs = patch torch _transformer_encoder_layer_fwd fastpath_mock fastpath_mock return_value = return_value model args kwargs assertTrue fastpath_mock called == is_called _test_mha_fastpath_called model args kwargs=None return_value=None is_called=True kwargs None kwargs = patch torch _native_multi_head_attention fastpath_mock fastpath_mock return_value = return_value model args kwargs assertTrue fastpath_mock called == is_called inp = torch tensor dtype=torch float device=device src_key_padding_mask = torch tensor dtype=torch bool device=device te_return_value = torch ones dtype=torch float encoder_layer = torch nn TransformerEncoderLayer d_model= nhead= dim_feedforward= batch_first=True te = torch nn TransformerEncoder encoder_layer num_layers= enable_nested_tensor=True mask_check=True te = te device eval t = torch nn Transformer d_model= nhead= batch_first=True device=device eval src = torch tensor dtype=torch float device=device tgt = torch tensor dtype=torch float device=device t_return_value = torch ones dtype=torch float device=device mha = nn MultiheadAttention batch_first=True device=device eval q = torch tensor dtype=torch float device=device mha_return_value = torch ones dtype=torch float device=device _test_te_fastpath_called te inp kwargs= src_key_padding_mask src_key_padding_mask return_value=te_return_value is_called=True _test_te_fastpath_called t src tgt return_value=t_return_value is_called=True _test_mha_fastpath_called mha q q q return_value=mha_return_value is_called=True torch backends mha set_fastpath_enabled False _test_te_fastpath_called te inp kwargs= src_key_padding_mask src_key_padding_mask return_value=te_return_value is_called=False _test_te_fastpath_called t src tgt return_value=t_return_value is_called=False _test_mha_fastpath_called mha q q q return_value=mha_return_value is_called=False torch backends mha set_fastpath_enabled True _test_te_fastpath_called te inp kwargs= src_key_padding_mask src_key_padding_mask return_value=te_return_value is_called=True _test_te_fastpath_called t src tgt return_value=t_return_value is_called=True _test_mha_fastpath_called mha q q q return_value=mha_return_value is_called=True TestSDPAFailureModes NNTestCase Used test failure modes scaled_dot_product_attention _do_cuda_memory_leak_check = True _do_cuda_non_default_stream = True onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION isSM XDevice isSM Device Does support fused SDPA SM + hardware parametrize head_dim parametrize dropout_p test_flash_backward_failure_sm plus device head_dim int dropout_p float dtype = torch float make_tensor = partial torch rand device=device dtype=dtype See check_requires_grad_and_head_dim_gt _constraints_on_sm _ pytorch aten src ATen native transformers cuda sdp_utils h size = head_dim q k v = make_tensor size make_tensor size make_tensor size sdpa_kernel backends= SDPBackend MATH math_ref = torch nn functional scaled_dot_product_attention q k v None False sdpa_kernel backends= SDPBackend FLASH_ATTENTION Should fail because inputs don t require grad flash_ref = torch nn functional scaled_dot_product_attention q k v None False assertEqual math_ref flash_ref atol= e- rtol= e- Should fail because inputs require grad q = make_tensor size requires_grad=True k = make_tensor size requires_grad=True v = make_tensor size requires_grad=True head_dim = head_dim dropout_p = assertRaises RuntimeError lambda torch nn functional scaled_dot_product_attention q k v None dropout_p False flash_ref = torch nn functional scaled_dot_product_attention q k v None dropout_p False onlyCUDA test_dispatch_fails_no_backend device dtype = torch float sdpa_kernel backends= SDPBackend ERROR size = q = torch randn size device=device dtype=dtype k = torch randn size device=device dtype=dtype v = torch randn size device=device dtype=dtype assertRaisesRegex RuntimeError No viable backend scaled_dot_product_attention found lambda torch _fused_sdp_choice q k v assertRaisesRegex RuntimeError No viable backend scaled_dot_product_attention found lambda torch nn functional scaled_dot_product_attention q k v onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FUSED_ATTENTION Does support fused scaled dot product attention parametrize kernel PLATFORM_SPECIFIC_SDPA test_invalid_fused_inputs_dim_ device kernel SDPBackend sdpa_kernel backends= kernel Dim size = dtype = torch float q = torch randn size device=device dtype=dtype k = torch randn size device=device dtype=dtype v = torch randn size device=device dtype=dtype assertWarnsRegex UserWarning All fused kernels requires query key value dimensional assertRaises RuntimeError lambda torch nn functional scaled_dot_product_attention q k v None False onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FUSED_ATTENTION Does support fused scaled dot product attention parametrize kernel PLATFORM_SPECIFIC_SDPA test_invalid_fused_inputs_broadcast device kernel SDPBackend sdpa_kernel backends= kernel Fused Kernels don t support broadcasting dense inputs dtype = torch float size = size_broadcast = q = torch randn size_broadcast device=device dtype=dtype k = torch randn size device=device dtype=dtype v = torch randn size device=device dtype=dtype assertRaises RuntimeError lambda torch nn functional scaled_dot_product_attention q k v None False onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FUSED_ATTENTION Does support fused scaled dot product attention parametrize kernel PLATFORM_SPECIFIC_SDPA test_invalid_sequence_lengths device kernel SDPBackend sdpa_kernel backends= kernel Passing q k v length sequences will error dtype = torch float make_tensor = partial torch rand device=device dtype=dtype size = SdpaShape q k v = make_tensor size make_tensor size make_tensor size assertWarnsRegex UserWarning All fused kernels do support zero seq_len_q seq_len_kv assertRaises RuntimeError lambda torch nn functional scaled_dot_product_attention q k v None False onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FUSED_ATTENTION Does support fused scaled dot product attention parametrize kernel PLATFORM_SPECIFIC_SDPA test_invalid_last_dim_stride device kernel SDPBackend sdpa_kernel backends= kernel Passing q k v last dim stride equal will error dtype = torch float make_tensor = partial torch rand device=device dtype=dtype size = SdpaShape q k v = make_tensor size make_tensor size make_tensor size q as_strided_ size assertWarnsRegex UserWarning All fused kernels require last dimension input have stride assertRaises RuntimeError lambda torch nn functional scaled_dot_product_attention q k v None False onlyCUDA unittest skipIf PLATFORM_SUPPORTS_MEM_EFF_ATTENTION PLATFORM_SUPPORTS_CUDNN_ATTENTION Efficient cuDNN Attention built system parametrize kernel SDPBackend EFFICIENT_ATTENTION SDPBackend CUDNN_ATTENTION test_mask_invalid_last_dim_stride device kernel sdpa_kernel backends= kernel dtype = torch float make_tensor = partial torch rand device=device dtype=dtype size = SdpaShape q k v = make_tensor size make_tensor size make_tensor size attn_mask = make_tensor Passing attn_mask last dim stride equal will error attn_mask as_strided_ size assertWarnsRegex UserWarning GPU backends require attn_mask s last dimension have stride while CPU does assertRaises RuntimeError lambda torch nn functional scaled_dot_product_attention q k v attn_mask False onlyCUDA unittest skipIf PLATFORM_SUPPORTS_MEM_EFF_ATTENTION Does support SDPA pre-SM hardware parametrize fused_kernel SDPBackend EFFICIENT_ATTENTION test_invalid_sdpa_kernel_grouped_query_attention_cuda device fused_kernel rand_query = torch rand device=device dtype=torch float requires_grad=True rand_key = torch rand device=device dtype=torch float requires_grad=True rand_value = torch rand device=device dtype=torch float requires_grad=True sdpa_kernel fused_kernel assertRaisesRegex RuntimeError No available kernel assertWarnsRegex UserWarning For dense inputs both fused kernels require query key value have F scaled_dot_product_attention rand_query rand_key rand_value dropout_p= is_causal=False enable_gqa=True onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Does flash_attention fused scaled dot product attention parametrize kernel PLATFORM_SPECIFIC_SDPA test_invalid_fused_inputs_head_dim device kernel SDPBackend sdpa_kernel backends= kernel The embed dim per head divisible flash attention dtype = torch float make_tensor = partial torch rand device=device dtype=dtype size = SdpaShape kernel == SDPBackend EFFICIENT_ATTENTION SdpaShape TEST_WITH_ROCM On ROCM FA EA share backend GPU kernels size = SdpaShape q k v = make_tensor size make_tensor size make_tensor size assertRaises RuntimeError lambda torch nn functional scaled_dot_product_attention q k v None False onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FUSED_ATTENTION Does support fused scaled dot product attention parametrize kernel PLATFORM_SPECIFIC_SDPA test_invalid_fused_inputs_invalid_dtype device kernel SDPBackend sdpa_kernel backends= kernel Invalid dtype both Flash Attention Mem Efficient Attention size = SdpaShape make_tensor = partial torch rand device=device dtype=torch float q k v = make_tensor size make_tensor size make_tensor size assertRaises RuntimeError lambda torch nn functional scaled_dot_product_attention q k v None False onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Does support flash attention parametrize kernel SDPBackend FLASH_ATTENTION test_invalid_fused_inputs_attn_mask_present device kernel SDPBackend sdpa_kernel backends= kernel Failures unsupported SDP args size = SdpaShape make_tensor = partial torch rand size device=device dtype=torch float q k v = make_tensor make_tensor make_tensor Non-None attention mask mask = torch ones device=device dtype=q dtype assertRaises RuntimeError lambda torch nn functional scaled_dot_product_attention q k v mask False onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Does support fused SDPA pre-SM hardware test_unaligned_tensors device The alignment dependent arch so we specify SM OrLater dtype = torch float size = SdpaShape make_tensor = partial torch rand size device=device dtype=dtype q k v = make_tensor make_tensor make_tensor sdpa_kernel backends= SDPBackend EFFICIENT_ATTENTION ctxmgr = assertRaises RuntimeError ctxmgr torch nn functional scaled_dot_product_attention q k v None False onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Does support fused SDPA pre-SM hardware test_flash_fail_fp device dtype = torch float size = SdpaShape make_tensor = partial torch rand size device=device dtype=dtype q k v = make_tensor make_tensor make_tensor sdpa_kernel backends= SDPBackend FLASH_ATTENTION assertWarnsRegex UserWarning Expected query key value all dtype Half BFloat assertRaises RuntimeError lambda torch nn functional scaled_dot_product_attention q k v None False onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Does support SDPA pre-SM hardware test_flash_autocast_fp _float device dtype = torch float size = SdpaShape make_tensor = partial torch rand size device=device dtype=dtype q k v = make_tensor make_tensor make_tensor torch autocast device_type= cuda dtype=torch float sdpa_kernel backends= SDPBackend FLASH_ATTENTION _ = torch nn functional scaled_dot_product_attention q k v None False onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Does support SDPA pre-SM hardware test_flash_autocast_fp _bfloat device dtype = torch float size = SdpaShape make_tensor = partial torch rand size device=device dtype=dtype q k v = make_tensor make_tensor make_tensor torch autocast device_type= cuda dtype=torch bfloat sdpa_kernel backends= SDPBackend FLASH_ATTENTION _ = torch nn functional scaled_dot_product_attention q k v None False Note do truncate list according platforms These tests should always raise errors parametrize kernel SDPBackend MATH SDPBackend FLASH_ATTENTION SDPBackend EFFICIENT_ATTENTION test_invalid_inputs_different_datatypes device kernel SDPBackend sdpa_kernel backends= kernel Different datatypes shape = query = torch randn shape dtype=torch float device=device key = torch randn shape dtype=torch float device=device value = torch randn shape dtype=torch float device=device assertRaises RuntimeError lambda F scaled_dot_product_attention query key value onlyCUDA parametrize kernel SDPBackend MATH SDPBackend FLASH_ATTENTION SDPBackend EFFICIENT_ATTENTION test_invalid_inputs_different_devices device kernel SDPBackend Different devices shape = query = torch randn shape dtype=torch float device=device key = torch randn shape dtype=torch float device= cpu value = torch randn shape dtype=torch float device= cpu assertRaises RuntimeError lambda F scaled_dot_product_attention query key value parametrize kernel SDPBackend MATH SDPBackend FLASH_ATTENTION SDPBackend EFFICIENT_ATTENTION test_invalid_inputs_ _dimensional_inputs device kernel SDPBackend sdpa_kernel backends= kernel dimensional input shape = query = torch randn dtype=torch float device=device key = torch randn shape dtype=torch float device=device value = torch randn shape dtype=torch float device=device assertRaises RuntimeError lambda F scaled_dot_product_attention query key value onlyCUDA unittest skipIf PLATFORM_SUPPORTS_MEM_EFF_ATTENTION Fused SDPA built system test_fused_kernels_nested_broadcasting_error_cases device one k v needs broadcasted other has non consistent seq_len dim rand_nested_tensor = partial rand_sdpa_tensor type= nested device=device dtype=torch float batch num_heads head_dim = seq_lens_q = torch randint low= high= size= batch tolist seq_lens_v = torch randint low= high= size= batch tolist q_shape = SdpaShape batch num_heads seq_lens_q head_dim k_shape = SdpaShape num_heads head_dim v_shape = SdpaShape batch num_heads seq_lens_v head_dim query = rand_nested_tensor q_shape transpose key = rand_nested_tensor k_shape transpose value = rand_nested_tensor v_shape transpose sdpa_kernel backends= SDPBackend EFFICIENT_ATTENTION assertRaisesRegex RuntimeError No available kernel torch nn functional scaled_dot_product_attention query key value attn_mask=None dropout_p= is_causal=False onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Fused SDPA built system test_nested_fails_on_padding_head_dim device dtype = torch bfloat seq_len_list = shape = SdpaShape seq_len_list make_tensor = partial rand_sdpa_tensor shape=shape type= nested device=device dtype=dtype q k v = make_tensor transpose make_tensor transpose make_tensor transpose sdpa_kernel backends= SDPBackend FLASH_ATTENTION assertWarnsRegex UserWarning For NestedTensor inputs Flash attention requires assertRaises RuntimeError lambda torch nn functional scaled_dot_product_attention q k v None False onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FUSED_ATTENTION isLessThanSM Device Current platform does support fused SDPA SM + device test_mem_efficient_fail_bfloat _less_than_sm device dtype = torch bfloat size = SdpaShape make_tensor = partial torch rand size device=device dtype=dtype q k v = make_tensor make_tensor make_tensor sdpa_kernel backends= SDPBackend EFFICIENT_ATTENTION assertWarnsRegex UserWarning Expected query key value all dtype Half Float assertRaises RuntimeError lambda torch nn functional scaled_dot_product_attention q k v None False onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Does support flash attention test_flash_atteention_large_bf _nan_values device query = torch full dtype=torch bfloat device= cuda key = torch full dtype=torch bfloat device= cuda value = torch full dtype=torch bfloat device= cuda sdpa_kernel SDPBackend FLASH_ATTENTION out = torch nn functional scaled_dot_product_attention query key value assertFalse torch isnan out any Output should contain NaNs onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FUSED_ATTENTION Fused SDPA built system parametrize fused_kernel SDPBackend FLASH_ATTENTION SDPBackend EFFICIENT_ATTENTION PLATFORM_SUPPORTS_FLASH_ATTENTION SDPBackend EFFICIENT_ATTENTION test_fused_kernels_seq_len_ _inputs device fused_kernel rand_nested_tensor = partial rand_sdpa_tensor type= nested device=device dtype=torch float batch num_heads head_dim = seq_lens = torch randint low= high= size= batch make sure some seq_lens num_zeros = indices = torch randint low= high=batch size= num_zeros seq_lens scatter_ indices shape = SdpaShape batch num_heads seq_lens tolist head_dim query = rand_nested_tensor shape key = rand_nested_tensor shape value = rand_nested_tensor shape query = query transpose key = key transpose value = value transpose sdpa_kernel backends= fused_kernel assertRaisesRegex RuntimeError No available kernel torch nn functional scaled_dot_product_attention query key value attn_mask=None dropout_p= is_causal=False onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Fused SDPA built system test_fused_kernels_nested_broadcasting_requires_grad_failure device rand_nested_tensor = partial rand_sdpa_tensor type= nested device=device dtype=torch float requires_grad=True batch num_heads head_dim head_dim_v = seq_lens = torch randint low= high= size= batch tolist q_shape = SdpaShape num_heads head_dim k_shape = SdpaShape batch num_heads seq_lens head_dim v_shape = SdpaShape batch seq_lens head_dim_v create dense query query = torch randn q_shape device=device dtype=torch float requires_grad=True key = rand_nested_tensor k_shape value = rand_nested_tensor v_shape query = query transpose key = key transpose value = value transpose sdpa_kernel backends= SDPBackend FLASH_ATTENTION assertWarnsRegex UserWarning Both fused kernels do support training broadcasted NT inputs assertRaisesRegex RuntimeError No available kernel torch nn functional scaled_dot_product_attention query key value attn_mask=None dropout_p= is_causal=False onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Does support flash attention test_flash_attention_fail_with_non_square_causal_attention device dtype = torch bfloat q_shape = SdpaShape kv_shape = SdpaShape make_q = partial torch rand q_shape device=device dtype=dtype make_kv = partial torch rand kv_shape device=device dtype=dtype q k v = make_q make_kv make_kv warning_str = Flash attention does support is_causal flag when seqlen_q = seqlen_k sdpa_kernel backends= SDPBackend FLASH_ATTENTION assertWarnsRegex UserWarning warning_str assertRaises RuntimeError lambda torch nn functional scaled_dot_product_attention q k v None is_causal=True onlyCUDA test_mem_eff_attention_fail_with_batch_size_geq_ batch_size = query = torch rand batch_size device= cuda dtype=torch float requires_grad=True key = torch rand batch_size device= cuda dtype=torch float requires_grad=True value = torch rand batch_size device= cuda dtype=torch float requires_grad=True q_cpu k_cpu v_cpu = query detach cpu requires_grad_ True key detach cpu requires_grad_ True value detach cpu requires_grad_ True sdpa_kernel backends=SDPBackend EFFICIENT_ATTENTION out = F scaled_dot_product_attention query key value out_cpu = F scaled_dot_product_attention q_cpu k_cpu v_cpu grad_out = torch rand_like out out backward grad_out out_cpu backward grad_out cpu assertEqual out out_cpu atol= e- rtol= e- assertEqual query grad q_cpu grad atol= e- rtol= e- assertEqual key grad k_cpu grad atol= e- rtol= e- assertEqual value grad v_cpu grad atol= e- rtol= e- onlyCUDA test_mem_eff_attention_fail_with_batch_size_geq_ _error query = torch rand device= cuda dtype=torch float key = torch rand device= cuda dtype=torch float value = torch rand device= cuda dtype=torch float error_str = r Efficient attention cannot produce valid seed offset outputs when r batch size exceeds \ \ \ assertRaisesRegex RuntimeError error_str torch _scaled_dot_product_efficient_attention query key value attn_bias=None compute_log_sumexp=True dropout_p= largeTensorTest GB cuda onlyCUDA test_mem_eff_attention_large_seq_len_uniform_attention device = torch device cuda dtype = torch bfloat num_queries = num_heads = feature_dim = Q K all zeros - uniform attention query = torch zeros num_heads num_queries feature_dim device=device dtype=dtype requires_grad=True key = torch zeros num_heads num_queries feature_dim device=device dtype=dtype requires_grad=True value = torch ones num_heads num_queries feature_dim device=device dtype=dtype requires_grad=True mask = torch ones num_queries num_queries dtype=torch bool device=device sdpa_kernel backends= SDPBackend EFFICIENT_ATTENTION output = torch nn functional scaled_dot_product_attention query key value attn_mask=mask expected = torch ones_like output grad_output = torch ones_like output output backward grad_output assertTrue torch allclose output expected assertTrue torch allclose query grad torch zeros_like query assertTrue torch allclose key grad torch zeros_like key For value since each input position contributed num_queries each output grad should sum accordingly all ones grad_output each value position receives grad because sum all softmax weights per row assertTrue torch allclose value grad torch ones_like value _get_block_size_n device head_dim is_dropout is_causal This should match block sizes CUDA kernel assert head_dim = major minor = torch cuda get_device_capability device is_sm x = major == minor Only include sm sm exclude sm A head_dim = head_dim = is_dropout head_dim = head_dim = is_sm x is_dropout is_causal is_dropout head_dim = is_sm x head_dim = head_dim = head_dim = pad_last_dim input_tensor alignment_size slice bool = False last_dim_size = input_tensor size - last_dim_size alignment_size == input_tensor last_dim_size pad_count = alignment_size - last_dim_size alignment_size padded_tensor = F pad input_tensor pad_count slice padded_tensor last_dim_size last_dim_size padded_tensor last_dim_size TestSDPA NNTestCase Used test generic functionality scaled_dot_product_attention Summary If you adding new test make sure runs both cpu cuda If you re test only applicable cuda add TestSDPACudaOnly expectedFailureMPS No double support parametrize contiguous_inputs True False test_sdp_math_gradcheck device contiguous_inputs bool batch_size seq_len num_heads head_dim = shape = SdpaShape batch_size num_heads seq_len head_dim make_tensor = partial rand_sdpa_tensor type= dense device=device dtype=torch float requires_grad=True packed=True qkv = make_tensor shape query key value = qkv chunk dim=- query = query view batch_size - num_heads head_dim transpose key = key view batch_size - num_heads head_dim transpose value = value view batch_size - num_heads head_dim transpose contiguous_inputs query = query contiguous key = key contiguous value = value contiguous sdpa_kernel backends= SDPBackend MATH assert gradcheck lambda args kwargs wrapper_set_seed torch nn functional scaled_dot_product_attention args kwargs query key value None False parametrize kernel SDPBackend MATH test_scaled_dot_product_attention_math_with_negative_scale device kernel SDPBackend https github com pytorch pytorch issues ref x v = torch matmul x x transpose - - v = v - v = v softmax dim=- v = torch matmul v x v x = torch randn device=device ref_result = ref x sdpa_kernel backends= kernel sdp_math = torch nn functional scaled_dot_product_attention x x x scale=- assertEqual ref_result sdp_math test_scaled_dot_product_attention_fp _overflow device Regression test https github com pytorch pytorch issues x = torch full dtype=torch half device=device y = torch nn functional scaled_dot_product_attention x x x assertFalse y isnan any item TestSDPACpuOnly NNTestCase Used test CPU only functionality scaled_dot_product_attention parametrize type dense nested parametrize dropout parametrize dtype torch float torch float torch bfloat torch half skipIfTorchDynamo test_fused_sdp_choice_cpu device type str dropout float dtype torch dtype Test cpu nestedtensor cpu MATH backend make_tensor = partial rand_sdpa_tensor type=type device=device dtype=dtype size = SdpaShape q k v = make_tensor size make_tensor size make_tensor size type == nested \ dropout \ dtype torch float torch float torch bfloat torch float assert torch _fused_sdp_choice q k v dropout_p=dropout == SDPBackend MATH value assert torch _fused_sdp_choice q k v dropout_p=dropout == SDPBackend FLASH_ATTENTION value _generate_fixed_qkv_helper device dtype batch_size q_n_head kv_n_head q_seq_len kv_seq_len head_dim torch manual_seed make_tensor = partial rand_sdpa_tensor type= dense device=device dtype=dtype requires_grad=False q_shape = SdpaShape batch_size q_n_head q_seq_len head_dim kv_shape = SdpaShape batch_size kv_n_head kv_seq_len head_dim q = make_tensor q_shape transpose k = make_tensor kv_shape transpose v = make_tensor kv_shape transpose q k v parametrize fused_kernel SDPBackend FLASH_ATTENTION parametrize dtype torch float torch float torch bfloat torch float parametrize batch_size parametrize q_seq_len parametrize kv_seq_len parametrize n_head parametrize head_dim parametrize mask_dim parametrize bool_mask False True parametrize train True False parametrize casual True False parametrize set_attn_mask True False test_scaled_dot_product_fused_attention_mask_vs_math_cpu device fused_kernel dtype batch_size q_seq_len kv_seq_len n_head head_dim mask_dim bool_mask train casual set_attn_mask tol = Tolerances e- e- dtype torch bfloat tol = Tolerances e- e- dtype torch float tol = Tolerances e- e- tol_grad = Tolerances e- e- dtype torch bfloat tol_grad = Tolerances e- e- dtype torch float tol_grad = Tolerances e- e- mask_shape itertools product q_seq_len kv_seq_len mask_dim == itertools product batch_size n_head q_seq_len kv_seq_len q k v = _generate_fixed_qkv_helper device dtype batch_size n_head n_head q_seq_len kv_seq_len head_dim q k v = _generate_fixed_qkv_helper device dtype batch_size n_head n_head q_seq_len kv_seq_len head_dim train q requires_grad_ True k requires_grad_ True v requires_grad_ True q requires_grad_ True k requires_grad_ True v requires_grad_ True set_attn_mask casual bool_mask attn_mask = torch randint size=mask_shape dtype=torch bool device=device attn_mask = torch randn mask_shape dtype=dtype device=device attn_mask = None sdpa_kernel backends= fused_kernel actual = torch nn functional scaled_dot_product_attention q k v attn_mask=attn_mask dropout_p= is_causal=casual sdpa_kernel backends= SDPBackend MATH math_ref = torch nn functional scaled_dot_product_attention q k v attn_mask=attn_mask dropout_p= is_causal=casual dtype torch bfloat torch float math_ref = math_ref dtype assertFalse torch isnan math_ref any assertFalse torch isnan actual any assertEqual actual math_ref atol=tol atol rtol=tol rtol train actual sum backward math_ref sum backward grad_q_actual grad_k_actual grad_v_actual = q grad k grad v grad grad_q_ref grad_k_ref grad_v_ref = q grad k grad v grad assertFalse grad_q_actual None assertFalse grad_k_actual None assertFalse grad_v_actual None assertEqual grad_q_actual grad_q_ref atol=tol_grad atol rtol=tol_grad rtol assertEqual grad_k_actual grad_k_ref atol=tol_grad atol rtol=tol_grad rtol assertEqual grad_v_actual grad_v_ref atol=tol_grad atol rtol=tol_grad rtol parametrize fused_kernel SDPBackend FLASH_ATTENTION parametrize dtype torch float torch float torch bfloat torch float parametrize n_heads parametrize train False True test_scaled_dot_product_fused_attention_gqa_vs_math_cpu device fused_kernel dtype n_heads train tol = Tolerances e- e- dtype torch bfloat tol = Tolerances e- e- dtype torch float tol = Tolerances e- e- tol_grad = Tolerances e- e- dtype torch bfloat tol_grad = Tolerances e- e- dtype torch float tol_grad = Tolerances e- e- q_n_head kv_n_head = n_heads batch_size q_seq_len kv_seq_len head_dim = q k v = _generate_fixed_qkv_helper device dtype batch_size q_n_head kv_n_head q_seq_len kv_seq_len head_dim q k v = _generate_fixed_qkv_helper device dtype batch_size q_n_head kv_n_head q_seq_len kv_seq_len head_dim train q requires_grad_ True k requires_grad_ True v requires_grad_ True q requires_grad_ True k requires_grad_ True v requires_grad_ True mask_shape = batch_size q_n_head q_seq_len kv_seq_len attn_mask = torch randn mask_shape dtype=dtype device=device sdpa_kernel backends= fused_kernel actual = torch nn functional scaled_dot_product_attention q k v attn_mask=attn_mask dropout_p= enable_gqa=True sdpa_kernel backends= SDPBackend MATH math_ref = torch nn functional scaled_dot_product_attention q k v attn_mask=attn_mask dropout_p= enable_gqa=True dtype torch bfloat torch float math_ref = math_ref dtype assertEqual actual math_ref atol=tol atol rtol=tol rtol train actual sum backward math_ref sum backward grad_q_actual grad_k_actual grad_v_actual = q grad k grad v grad grad_q_ref grad_k_ref grad_v_ref = q grad k grad v grad assertFalse grad_q_actual None assertFalse grad_k_actual None assertFalse grad_v_actual None assertEqual grad_q_actual grad_q_ref atol=tol_grad atol rtol=tol_grad rtol assertEqual grad_k_actual grad_k_ref atol=tol_grad atol rtol=tol_grad rtol assertEqual grad_v_actual grad_v_ref atol=tol_grad atol rtol=tol_grad rtol test_sdpa_with_inf device https github com pytorch pytorch issues full = torch full float -inf device=device mask = torch triu full diagonal= + torch tril full diagonal=- make_tensor = partial rand_sdpa_tensor type= dense device=device dtype=torch float requires_grad=False input_shape = SdpaShape q = make_tensor input_shape k = make_tensor input_shape v = make_tensor input_shape sdpa_kernel backends= SDPBackend MATH math_ref = torch nn functional scaled_dot_product_attention q k v attn_mask=mask sdpa_kernel backends= SDPBackend FLASH_ATTENTION actual = torch nn functional scaled_dot_product_attention q k v attn_mask=mask assertEqual math_ref actual test_sdpa_backward_with_gradient device https github com pytorch pytorch issues sdpa_helper torch manual_seed query = torch empty size= dtype=torch float device=device uniform_ - requires_grad_ True key = torch empty size= dtype=torch float device=device uniform_ - requires_grad_ True value = torch empty size= dtype=torch float device=device uniform_ - requires_grad_ True res = torch nn functional scaled_dot_product_attention query key value None False res_grad = torch empty_like res device=device uniform_ - res backward res_grad retain_graph=True res query grad key grad value grad sdpa_kernel backends= SDPBackend MATH res_ref query_grad_ref key_grad_ref value_grad_ref = sdpa_helper sdpa_kernel backends= SDPBackend FLASH_ATTENTION res_actual query_grad_actual key_grad_actual value_grad_actual = sdpa_helper assertEqual res_ref res_actual assertEqual query_grad_ref query_grad_actual assertEqual key_grad_ref key_grad_actual assertEqual value_grad_ref value_grad_actual unittest skipIf PLATFORM_SUPPORTS_FUSED_ATTENTION Fused SDPA built system parametrize backend SDPBackend EFFICIENT_ATTENTION SDPBackend FLASH_ATTENTION parametrize seq_len parametrize head_dim parametrize dtype torch float torch float test_fully_masked_out_rows backend device seq_len head_dim dtype attention_inputs seq_len head_dim device dtype mask_every_n_rows= query = torch rand seq_len head_dim requires_grad=True device=device dtype=dtype key = torch rand seq_len head_dim requires_grad=True device=device dtype=dtype value = torch rand seq_len head_dim requires_grad=True device=device dtype=dtype Create mask deterministic row masking mask = torch ones seq_len seq_len dtype=torch bool device=device Mask every nth row mask mask_every_n_rows = False Create fixed pattern element-wise masking element_mask = torch zeros seq_len seq_len dtype=torch bool device=device element_mask torch arange seq_len None == torch arange seq_len = True Combine row masking element-wise masking mask = mask element_mask unsqueeze unsqueeze query key value mask compute_output_and_grads query key value mask backend sdpa_kernel backend masked_out = scaled_dot_product_attention query key value attn_mask=mask loss = masked_out sum grads = torch autograd grad loss query key value masked_out grads backend == SDPBackend FLASH_ATTENTION cuda str device unittest skip FlashAttention does support masks cuda backend == SDPBackend EFFICIENT_ATTENTION cpu str device unittest skip EfficientAttention does support masks cpu query key value mask = attention_inputs seq_len head_dim device dtype Compute results tested backend backend_out backend_grads = compute_output_and_grads query key value mask backend Compute results Math backend math_out math_grads = compute_output_and_grads query key value mask SDPBackend MATH Compare outputs torch testing assert_close backend_out math_out atol= e- rtol= assertFalse backend_out isnan any assertFalse math_out isnan any Compare gradients bg mg zip backend_grads math_grads torch testing assert_close bg mg atol= e- rtol= assertFalse bg isnan any assertFalse mg isnan any Check masked rows zero output mask_sum = mask sum dim=- keepdim=True masked_rows = mask_sum == expand_as backend_out assertTrue mask_sum == sum No fully masked out rows found assert torch all backend_out masked_rows == \ f Non-zero values fully masked rows backend= Check gradients masked rows zero grad_query = backend_grads assert torch all grad_query masked_rows == f Non-zero gradients fully masked rows backend= parametrize dtype torch float torch float parametrize fill_val float inf test_non_masked_rows_nan_props device dtype fill_val query = torch randn device=device dtype=dtype single NaN query input query = fill_val query = query detach requires_grad_ True key = torch randn device=device dtype=dtype requires_grad=True value = torch randn device=device dtype=dtype requires_grad=True out = torch nn functional scaled_dot_product_attention query key value assertTrue torch isnan out any out sum backward assertTrue torch isnan query grad any parametrize dtype torch float torch float test_cpu_flash_attn_nan_propagation dtype Setup tensors query = torch full torch nan dtype=dtype key = torch randn dtype=dtype value = torch randn dtype=dtype sdpa_kernel SDPBackend FLASH_ATTENTION out = torch nn functional scaled_dot_product_attention query key value attn_mask=None dropout_p= is_causal=False Check output contains NaN assertTrue torch isnan out all parametrize kernel SDPBackend MATH test_scaled_dot_product_attention_math_with_negative_scale device kernel SDPBackend https github com pytorch pytorch issues ref x v = torch matmul x x transpose - - v = v - v = v softmax dim=- v = torch matmul v x v x = torch randn device=device ref_result = ref x sdpa_kernel backends= kernel sdp_math = torch nn functional scaled_dot_product_attention x x x scale=- assertEqual ref_result sdp_math TestSDPACudaOnly NNTestCase Used test CUDA only functionality scaled_dot_product_attention Quarks There some trickiness function Its runtime behavior dependent CUDA architecture you testing See ` PLATFORM_SUPPORTS_FUSED_ATTENTION ` top file Summary Math always supported FlashAttention Supported sm newer hardware MemEfficientAttention Supported sm newer hardware _do_cuda_memory_leak_check = True _do_cuda_non_default_stream = True TODO USED FOR TESTING THE SCORES e g testing ALIBI we don t need now normalize_flash_attn_S attn_unnorm q k v query_padding_mask=None key_padding_mask=None attn_bias=None is_dropout=False causal=False window_size= - - - means infinite window size scale=None Arguments q batch_size seqlen_q nheads head_dim k v batch_size seqlen_k nheads head_dim key_padding_mask batch_size seqlen_q attn_bias broadcastable batch_size nheads seqlen_q seqlen_k Output softmax_lse batch_size nheads seqlen_q softmax_max batch_size nheads seqlen_q q = q transpose k = k transpose v = v transpose causal window_size = window_size q k v = q float k float v float _ seqlen_q _ head_dim = q shape seqlen_k = k shape b = q shape torch nn attention bias _calculate_scale scale = _calculate_scale head_dim scale scores = torch matmul q transpose scale k permute key_padding_mask None scores masked_fill_ ~key_padding_mask view b - float -inf window_size = window_size = local_mask = construct_local_mask seqlen_q seqlen_k window_size query_padding_mask key_padding_mask q device scores masked_fill_ local_mask float -inf attn_bias None scores = scores + attn_bias dtype=scores dtype block_size_n = _get_block_size_n scores device head_dim is_dropout causal scores_block = scores split block_size_n dim=- lse_block = torch stack torch logsumexp s dim=- s scores_block dim=- lse = torch logsumexp lse_block dim=- lse could -inf i e all values scores -inf we want set those inf so when we do torch exp m - lse we get instead NaN lse lse == float -inf = float inf scores_max_block = torch stack torch amax s dim=- s scores_block dim=- cummax_block = torch cummax scores_max_block flip - dim=- values flip - unbind dim=- attn_unnorm_block = attn_unnorm split block_size_n dim=- attn_norm = torch cat torch exp m - lse unsqueeze - m zip attn_unnorm_block cummax_block dim=- query_padding_mask None attn_norm masked_fill_ ~query_padding_mask view b - attn_norm masked_fill_ rearrange ~query_padding_mask b s - b s attn_norm dtype=attn_unnorm dtype construct_local_mask seqlen_q seqlen_k window_size query_padding_mask key_padding_mask device row_idx = rearrange torch arange seqlen_q device=device dtype=torch long s - s row_idx = torch arange seqlen_q device=device dtype=torch long view - col_idx = torch arange seqlen_k device=device dtype=torch long sk = seqlen_k key_padding_mask None key_padding_mask sum - view - rearrange key_padding_mask sum - b - b sq = seqlen_q query_padding_mask None query_padding_mask sum - view - rearrange query_padding_mask sum - b - b window_size col_idx row_idx + sk - sq + window_size sk = torch full_like col_idx seqlen_k key_padding_mask None sk torch logical_or col_idx torch minimum row_idx + sk - sq + window_size sk col_idx row_idx + sk - sq - window_size convert_flash_attn_S_to_softmax S seqlen_q seqlen_k query_padding_mask key_padding_mask causal=False window_size= - - - means infinite window size FlashAttention stores S matrix different way Arguments S batch_size nheads seqlen_q seqlen_k query_padding_mask batch_size seqlen_q key_padding_mask batch_size seqlen_k TEST_WITH_ROCM S b = S shape causal window_size = window_size seqlen_q_rounded seqlen_k_rounded = S shape - S_converted = S window_size = window_size = local_mask = construct_local_mask seqlen_q seqlen_k window_size query_padding_mask key_padding_mask S device local_mask = F pad local_mask seqlen_k_rounded - seqlen_k seqlen_q_rounded - seqlen_q value=True S_converted = S_converted masked_fill local_mask Need zero out things attention_mask case S initialized random values some those values aren t overwritten seqlen_q_og = query_padding_mask shape - query_padding_mask None seqlen_q_rounded query_padding_mask None query_padding_mask = F pad query_padding_mask seqlen_q_rounded - seqlen_q_og S_converted = S_converted masked_fill rearrange ~query_padding_mask b s - b s S_converted = S_converted masked_fill ~query_padding_mask view b - seqlen_k_og = key_padding_mask shape - key_padding_mask None seqlen_k key_padding_mask None key_padding_mask = F pad key_padding_mask seqlen_k_rounded - seqlen_k_og S_converted = S_converted masked_fill ~key_padding_mask view b - S_converted = S_converted masked_fill rearrange ~key_padding_mask b s - b s S_converted = F pad S_converted seqlen_q_og - seqlen_q_rounded S_converted = F pad S_converted seqlen_k_og - seqlen_k_rounded S_converted seqlen_q seqlen_k unittest skipIf PLATFORM_SUPPORTS_CUDNN_ATTENTION cuDNN Attention supported system test_cudnn_attention_different_dk_dv device dtype = torch bfloat make_tensor = partial torch rand device=device dtype=dtype requires_grad=True batch num_heads head_dim_k head_dim_v = seq_len = q_shape = SdpaShape batch num_heads seq_len head_dim_k k_shape = SdpaShape batch num_heads seq_len head_dim_k v_shape = SdpaShape batch num_heads seq_len head_dim_v query key value = make_tensor q_shape make_tensor k_shape make_tensor v_shape sdpa_kernel backends= SDPBackend CUDNN_ATTENTION actual = torch nn functional scaled_dot_product_attention query key value attn_mask=None dropout_p= is_causal=False sdpa_kernel backends= SDPBackend MATH math_ref = torch nn functional scaled_dot_product_attention query contiguous torch float key contiguous torch float value contiguous torch float attn_mask=None dropout_p= is_causal=False assertEqual actual contiguous math_ref contiguous dtype atol= e- rtol= e- unittest skipIf PLATFORM_SUPPORTS_CUDNN_ATTENTION cuDNN Attention supported system test_cudnn_attention_gqa device batch = seq_len_q = seq_len_kv = D = Sample call SDPA - GQ query = torch rand batch seq_len_q D device= cuda dtype=torch bfloat key = torch rand batch seq_len_kv D device= cuda dtype=torch bfloat cuDNN supports h_k = h_v value = torch rand batch seq_len_kv D device= cuda dtype=torch bfloat sdpa_kernel SDPBackend MATH output_math = scaled_dot_product_attention query key value is_causal=True enable_gqa=True assertRaisesRegex RuntimeError No available kernel sdpa_kernel SDPBackend CUDNN_ATTENTION output_cudnn = scaled_dot_product_attention query key value is_causal=True enable_gqa=False sdpa_kernel SDPBackend CUDNN_ATTENTION output_cudnn = scaled_dot_product_attention query key value is_causal=True enable_gqa=True assertEqual output_math output_cudnn unittest skipIf PLATFORM_SUPPORTS_CUDNN_ATTENTION cuDNN Attention supported system test_cudnn_attention_d _heuristic device dtype = torch bfloat make_tensor = partial torch rand device=device dtype=dtype requires_grad=True batch num_heads head_dim_k head_dim_v = seq_len = q_shape = SdpaShape batch num_heads seq_len head_dim_k k_shape = SdpaShape batch num_heads seq_len head_dim_k v_shape = SdpaShape batch num_heads seq_len head_dim_v query key value = make_tensor q_shape make_tensor k_shape make_tensor v_shape test sdpa_kernel backends= SDPBackend CUDNN_ATTENTION set_priority=True actual = torch nn functional scaled_dot_product_attention query key value attn_mask=None dropout_p= is_causal=False actual backward torch randn_like actual sdpa_kernel backends= SDPBackend MATH math_ref = torch nn functional scaled_dot_product_attention query contiguous torch float key contiguous torch float value contiguous torch float attn_mask=None dropout_p= is_causal=False assertEqual actual contiguous math_ref contiguous dtype atol= e- rtol= e- torch cuda get_device_capability test assertRaisesRegex RuntimeError No available kernel test unittest skipIf PLATFORM_SUPPORTS_CUDNN_ATTENTION cuDNN Attention supported system test_fused_attention_different_dk_dv device dtype = torch bfloat make_tensor = partial torch rand device=device dtype=dtype requires_grad=True batch num_heads head_dim_k head_dim_v = q_shape = SdpaShape batch num_heads head_dim_k k_shape = SdpaShape batch num_heads head_dim_k v_shape = SdpaShape batch num_heads head_dim_v query key value = make_tensor q_shape make_tensor k_shape make_tensor v_shape test we do dispatch cuDNN unsupported case actual = torch nn functional scaled_dot_product_attention query key value attn_mask=None dropout_p= is_causal=False sdpa_kernel backends= SDPBackend MATH math_ref = torch nn functional scaled_dot_product_attention query contiguous torch float key contiguous torch float value contiguous torch float attn_mask=None dropout_p= is_causal=False assertEqual actual contiguous math_ref contiguous dtype atol= e- rtol= e- unittest skipIf PLATFORM_SUPPORTS_CUDNN_ATTENTION cuDNN Attention supported system unittest skipIf True broken cuDNN test_cudnn_attention_fail_d device Test cuDNN attention dispatching correctly bails out d b h = s_q s_kv = d_qk d_v = q = torch randn b h s_q d_qk device=device dtype=torch bfloat k = torch randn b h s_kv d_qk device=device dtype=torch bfloat v = torch randn b h s_kv d_v device=device dtype=torch bfloat device_cap = torch cuda get_device_capability ISSM = device_cap == ISSM = device_cap == sdpa_kernel backends= SDPBackend CUDNN_ATTENTION ISSM ISSM torch backends cudnn version = torch nn functional scaled_dot_product_attention q k v assertRaisesRegex RuntimeError No available kernel torch nn functional scaled_dot_product_attention q k v unittest skipIf PLATFORM_SUPPORTS_CUDNN_ATTENTION cudnn Attention supported system test_cudnn_attention_trivial_output_transpose device see also https github com pytorch pytorch issues x = torch randn device= cuda dtype=torch float requires_grad=True x = x transpose torch nn attention sdpa_kernel torch nn attention SDPBackend CUDNN_ATTENTION o = torch nn functional scaled_dot_product_attention x x x transpose reshape o backward o x_cpu = x clone cpu detach x_cpu requires_grad = True x _cpu = x_cpu transpose o = torch nn functional scaled_dot_product_attention x _cpu x _cpu x _cpu transpose reshape o backward o torch testing assert_close x grad x_cpu grad cuda atol= e- rtol= e- unittest skipIf PLATFORM_SUPPORTS_CUDNN_ATTENTION cudnn Attention supported system test_cudnn_attention_nonmodulo seqlen device see also https github com pytorch pytorch issues mask = torch randint device= cuda dtype=torch bool q = torch randn device= cuda dtype=torch float requires_grad=True k = torch randn device= cuda dtype=torch float requires_grad=True v = torch randn device= cuda dtype=torch float requires_grad=True q_cpu = q detach clone cpu k_cpu = k detach clone cpu v_cpu = v detach clone cpu q_cpu requires_grad = True k_cpu requires_grad = True v_cpu requires_grad = True mask_cpu = mask detach clone cpu torch nn attention sdpa_kernel torch nn attention SDPBackend CUDNN_ATTENTION out = nn functional scaled_dot_product_attention q k v attn_mask=mask dropout_p= is_causal=False out_cpu = nn functional scaled_dot_product_attention q_cpu k_cpu v_cpu attn_mask=mask_cpu dropout_p= is_causal=False out sum backward out_cpu sum backward torch testing assert_close q grad q_cpu grad cuda atol= e- rtol= e- torch testing assert_close k grad k_cpu grad cuda atol= e- rtol= e- torch testing assert_close v grad v_cpu grad cuda atol= e- rtol= e- unittest skipIf PLATFORM_SUPPORTS_CUDNN_ATTENTION cudnn Attention supported system test_cudnn_attention_preserves_query_layout device test_attention backend SDPBackend permute_order list list int BHSqD = BHSkvD = shape_q = BHSqD idx idx permute_order shape_kv = BHSkvD idx idx permute_order reverse = permute_order index idx idx range q = torch randn shape_q dtype=torch bfloat device= cuda requires_grad=True permute reverse k = torch randn shape_kv dtype=torch bfloat device= cuda requires_grad=True permute reverse v = torch randn shape_kv dtype=torch bfloat device= cuda requires_grad=True permute reverse assertEqual q shape BHSqD assertEqual k shape BHSkvD assertEqual v shape BHSkvD sdpa_kernel backend out = F scaled_dot_product_attention q k v assertTrue out permute permute_order is_contiguous out sum backward permute_orders = list permutable = permute_orders = itertools permutations permutable permute_order permute_orders test_attention SDPBackend CUDNN_ATTENTION list permute_order + unittest skipIf PLATFORM_SUPPORTS_CUDNN_ATTENTION cudnn Attention supported system test_cudnn_attention_compiles q = torch randn dtype=torch half device= cuda requires_grad=True grad = torch randn_like q torch compile func torch nn attention sdpa_kernel torch nn attention SDPBackend CUDNN_ATTENTION out = torch nn functional scaled_dot_product_attention q q q out backward grad out out = func q_cpu = q float cpu detach clone q_cpu requires_grad = True grad_cpu = grad cpu float out_cpu = torch nn functional scaled_dot_product_attention q_cpu q_cpu q_cpu out_cpu backward grad_cpu assertEqual out out_cpu cuda half atol= e- rtol= e- assertEqual q grad q_cpu grad cuda half atol= e- rtol= e- unittest skipIf PLATFORM_SUPPORTS_CUDNN_ATTENTION cudnn Attention supported system test_cudnn_attention_seqlen _dropout_heuristic q = torch randn dtype=torch half device= cuda requires_grad=True grad = torch randn_like q torch nn attention sdpa_kernel SDPBackend CUDNN_ATTENTION SDPBackend FLASH_ATTENTION out = torch nn functional scaled_dot_product_attention q q q dropout_p= out backward grad unittest skipIf PLATFORM_SUPPORTS_MEM_EFF_ATTENTION Fused SDPA built system parametrize mask_dim test_mem_efficient_attention_mask_variants device mask_dim list int dtype = torch float make_tensor = partial torch rand device=device dtype=dtype requires_grad=True batch num_heads head_dim = seq_len_q seq_len_kv = query = make_tensor SdpaShape batch num_heads seq_len_q head_dim kv_shape = SdpaShape batch num_heads seq_len_kv head_dim key value = make_tensor kv_shape make_tensor kv_shape mask_dim == mask = torch randn seq_len_kv device=device dtype=dtype mask_dim == mask = torch randn seq_len_q seq_len_kv device=device dtype=dtype mask_dim == mask = torch randn num_heads seq_len_q seq_len_kv device=device dtype=dtype mask_dim == mask = torch randn batch num_heads seq_len_q seq_len_kv device=device dtype=dtype sdpa_kernel backends= SDPBackend EFFICIENT_ATTENTION out = F scaled_dot_product_attention query key value mask out sum backward unittest skipIf PLATFORM_SUPPORTS_MEM_EFF_ATTENTION Fused SDPA built system parametrize dtype torch float torch float test_mem_eff_attention_non_contiguous_mask device dtype make_tensor = partial torch rand device=device dtype=dtype requires_grad=True batch num_heads head_dim = seq_len_q seq_len_kv = query = make_tensor SdpaShape batch num_heads seq_len_q head_dim kv_shape = SdpaShape batch num_heads seq_len_kv head_dim key value = make_tensor kv_shape make_tensor kv_shape mask = torch randn batch num_heads seq_len_q seq_len_kv device=device dtype=dtype mask = torch as_strided mask batch num_heads seq_len_q seq_len_kv sdpa_kernel backends= SDPBackend EFFICIENT_ATTENTION out = F scaled_dot_product_attention query key value mask out sum backward unittest skipIf PLATFORM_SUPPORTS_MEM_EFF_ATTENTION Fused SDPA built system parametrize dtype torch float torch float test_mem_eff_attention_long_sequence_mask device dtype torch cuda get_device_properties cuda total_memory unittest skip This test requires substatnial GPU memory make_tensor = partial torch rand device=device dtype=dtype requires_grad=True batch num_heads head_dim = seq_len_q seq_len_kv = query = make_tensor SdpaShape batch num_heads seq_len_q head_dim kv_shape = SdpaShape batch num_heads seq_len_kv head_dim key value = make_tensor kv_shape make_tensor kv_shape mask = torch randn batch num_heads seq_len_q seq_len_kv device=device dtype=dtype sdpa_kernel backends= SDPBackend EFFICIENT_ATTENTION out = F scaled_dot_product_attention query key value mask out sum backward unittest skipIf PLATFORM_SUPPORTS_MEM_EFF_ATTENTION Fused SDPA built system test_mem_eff_attention_non_contig_mask_bug device Without fix produces ` AssertionError assert e- ` Shapes taken repro query_size = query_strides = key_size = key_strides = value_size = value_strides = attention_mask_size = attn_mask_strides = Calculate number elements needed each tensor query_num_elements = max size stride size stride zip query_size query_strides key_num_elements = max size stride size stride zip key_size key_strides value_num_elements = max size stride size stride zip value_size value_strides attention_mask_num_elements = max size stride size stride zip attention_mask_size attn_mask_strides Create tensors specified sizes strides query = torch randn query_num_elements device=device as_strided query_size query_strides key = torch randn key_num_elements device=device as_strided key_size key_strides value = torch randn value_num_elements device=device as_strided value_size value_strides bias = torch randn attention_mask_num_elements device=device as_strided attention_mask_size attn_mask_strides sdpa_kernel backends= SDPBackend EFFICIENT_ATTENTION out = F scaled_dot_product_attention query key value bias out_contig = F scaled_dot_product_attention query key value bias contiguous max_diff = out - out_contig abs mean assertTrue max_diff item e- unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Fused SDPA built system test_singelton_head_dim_stride_ne_ device query = torch tensor dtype=torch float device=device query = query transpose - - key = torch tensor dtype=torch float device=device value = torch tensor dtype=torch float device=device torch backends cuda sdp_kernel enable_math=False enable_flash=True enable_mem_efficient=False scaled_dot_product_attention query key value unittest skipIf PLATFORM_SUPPORTS_MEM_EFF_ATTENTION Fused SDPA built system parametrize type dense nested parametrize is_contiguous True False test_scaled_dot_product_attention_fused_kernels_packed device type str is_contiguous bool make_tensor = partial rand_sdpa_tensor type=type device=device dtype=torch float packed=True batch_size seq_len num_heads head_dim = shape = SdpaShape batch_size num_heads seq_len head_dim Test Packed qkv = make_tensor shape query key value = qkv chunk dim=- query = query view batch_size - num_heads head_dim transpose value = value view batch_size - num_heads head_dim transpose key = key view batch_size - num_heads head_dim transpose is_contiguous query = query contiguous key = key contiguous value = value contiguous sdpa_kernel backends= SDPBackend EFFICIENT_ATTENTION actual = torch nn functional scaled_dot_product_attention query key value attn_mask=None dropout_p= is_causal=False sdpa_kernel backends= SDPBackend MATH math_ref = torch nn functional scaled_dot_product_attention query contiguous key contiguous value contiguous attn_mask=None dropout_p= is_causal=False assertEqual actual contiguous math_ref contiguous atol= e- rtol= e- unittest skipIf PLATFORM_SUPPORTS_CUDNN_ATTENTION Fused SDPA built system unittest skipIf TORCH_CUDNN_SDPA_NESTED_TENSOR_ENABLED os environ cuDNN Nested Tensor support enabled parametrize type nested parametrize is_contiguous True False test_scaled_dot_product_attention_cudnn_nested device type str is_contiguous bool TEST_WITH_ROCM type == nested skipTest ROCM does support efficient attention nested tensors now make_tensor = partial rand_sdpa_tensor type=type device=device dtype=torch float packed=True batch_size seq_len num_heads head_dim = shape = SdpaShape batch_size num_heads seq_len head_dim Test Packed qkv = make_tensor shape query key value = qkv chunk dim=- query = query view batch_size - num_heads head_dim transpose value = value view batch_size - num_heads head_dim transpose key = key view batch_size - num_heads head_dim transpose is_contiguous query = query contiguous key = key contiguous value = value contiguous sdpa_kernel backends= SDPBackend CUDNN_ATTENTION actual = torch nn functional scaled_dot_product_attention query key value attn_mask=None dropout_p= is_causal=False sdpa_kernel backends= SDPBackend MATH math_ref = torch nn functional scaled_dot_product_attention query contiguous key contiguous value contiguous attn_mask=None dropout_p= is_causal=False assertEqual actual contiguous math_ref contiguous atol= e- rtol= e- unittest skipIf PLATFORM_SUPPORTS_FUSED_ATTENTION Fused SDPA built system parametrize type dense nested parametrize fused_kernel SDPBackend FLASH_ATTENTION SDPBackend EFFICIENT_ATTENTION PLATFORM_SUPPORTS_FLASH_ATTENTION SDPBackend EFFICIENT_ATTENTION test_scaled_dot_product_attention_fused_kernels_packed_accuracy device type str fused_kernel str rand_nt shape batch seq_len num_heads head_dim = shape tensors = torch rand seq_len num_heads head_dim device=device dtype=torch float - _ range batch torch nested nested_tensor tensors device=device dtype=torch float torch nested nested_tensor tensors device=device dtype=torch float rand_tensor shape batch seq_len num_heads head_dim = shape tensor = torch rand batch seq_len num_heads head_dim device=device dtype=torch float - tensor tensor dtype=torch float batch_size seq_len num_heads head_dim = shape = batch_size seq_len num_heads head_dim Test Packed qkv qkv_low_precision = rand_tensor shape type == dense rand_nt shape query key value = qkv chunk dim=- query_lp key_lp value_lp = qkv_low_precision chunk dim=- query = query view batch_size - num_heads head_dim transpose key = key view batch_size - num_heads head_dim transpose value = value view batch_size - num_heads head_dim transpose query_lp = query_lp view batch_size - num_heads head_dim transpose key_lp = key_lp view batch_size - num_heads head_dim transpose value_lp = value_lp view batch_size - num_heads head_dim transpose sdpa_kernel backends= fused_kernel actual = torch nn functional scaled_dot_product_attention query_lp key_lp value_lp attn_mask=None dropout_p= is_causal=False sdpa_kernel backends= SDPBackend MATH math_ref_lp = torch nn functional scaled_dot_product_attention query_lp contiguous key_lp contiguous value_lp contiguous attn_mask=None dropout_p= is_causal=False math_query = query contiguous math_key = key contiguous math_value = value contiguous math_ref = torch nn functional scaled_dot_product_attention math_query math_key math_value attn_mask=None dropout_p= is_causal=False actual_test = actual math_ref_test = math_ref math_ref_lp_test = math_ref_lp actual_test is_nested actual_test = torch nested to_padded_tensor actual_test contiguous padding= math_ref_test = torch nested to_padded_tensor math_ref_test padding= math_ref_lp_test = torch nested to_padded_tensor math_ref_lp_test padding= actual_test = actual_test dtype=torch float contiguous math_ref_test = math_ref_test dtype=torch float contiguous math_ref_lp_test = math_ref_lp_test dtype=torch float contiguous assertEqual math_ref_test math_ref_lp_test atol= e- rtol= e- assertEqual actual_test math_ref_test atol= e- rtol= e- unittest skipIf PLATFORM_SUPPORTS_MEM_EFF_ATTENTION Efficient Attention built system parametrize contiguous_inputs True False parametrize is_causal True False test_sdp_mem_efficient_grad_against_math device contiguous_inputs bool is_causal bool batch_size seq_len num_heads head_dim = make_tensor = partial rand_sdpa_tensor type= dense device=device dtype=torch float requires_grad=True packed=True qkv = make_tensor SdpaShape batch_size num_heads seq_len head_dim qkv_lp = qkv detach clone torch float requires_grad_ query key value = qkv chunk dim=- query_lp key_lp value_lp = qkv_lp chunk dim=- query = query view batch_size - num_heads head_dim transpose key = key view batch_size - num_heads head_dim transpose value = value view batch_size - num_heads head_dim transpose query_lp = query_lp view batch_size - num_heads head_dim transpose key_lp = key_lp view batch_size - num_heads head_dim transpose value_lp = value_lp view batch_size - num_heads head_dim transpose contiguous_inputs query = query contiguous key = key contiguous value = value contiguous query_lp = query_lp contiguous key_lp = key_lp contiguous value_lp = value_lp contiguous sdpa_kernel backends= SDPBackend MATH out = torch nn functional scaled_dot_product_attention query key value None is_causal sdpa_kernel backends= SDPBackend EFFICIENT_ATTENTION out_lp = torch nn functional scaled_dot_product_attention query_lp key_lp value_lp None is_causal rand_upward = torch rand_like out rand_upward_lp = rand_upward torch float out backward rand_upward out_lp backward rand_upward_lp Cast up compare assertEqual qkv grad qkv_lp grad torch float atol= e- rtol= e- unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Flash Attention built system parametrize contiguous_inputs True False parametrize is_causal True False parametrize dtype torch float torch bfloat test_sdp_flash_attention_grad_against_math device contiguous_inputs bool is_causal bool dtype torch dtype batch_size seq_len num_heads head_dim = make_tensor = partial rand_sdpa_tensor type= dense device=device dtype=torch float requires_grad=True packed=True qkv = make_tensor SdpaShape batch_size num_heads seq_len head_dim qkv_lp = qkv detach clone dtype requires_grad_ query key value = qkv chunk dim=- query_lp key_lp value_lp = qkv_lp chunk dim=- query = query view batch_size - num_heads head_dim transpose key = key view batch_size - num_heads head_dim transpose value = value view batch_size - num_heads head_dim transpose query_lp = query_lp view batch_size - num_heads head_dim transpose key_lp = key_lp view batch_size - num_heads head_dim transpose value_lp = value_lp view batch_size - num_heads head_dim transpose contiguous_inputs query = query contiguous key = key contiguous value = value contiguous query_lp = query_lp contiguous key_lp = key_lp contiguous value_lp = value_lp contiguous sdpa_kernel backends= SDPBackend MATH out = torch nn functional scaled_dot_product_attention query key value None is_causal sdpa_kernel backends= SDPBackend FLASH_ATTENTION out_lp = torch nn functional scaled_dot_product_attention query_lp key_lp value_lp None is_causal rand_upward = torch rand_like out rand_upward_lp = rand_upward dtype out backward rand_upward out_lp backward rand_upward_lp Cast up compare Since we doing compute fp we have bump tolerance Bump down tolerance blfoat atol = e- dtype == torch float e- rtol = e- dtype == torch float e- TEST_WITH_ROCM atol = e- dtype == torch float e- assertEqual qkv grad qkv_lp grad torch float atol=atol rtol=rtol unittest skipIf PLATFORM_SUPPORTS_FUSED_ATTENTION Platform does support fused SDPA parametrize type dense nested test_fused_sdp_choice device type str batch_size seq_len num_heads head_dim = shape = SdpaShape batch_size num_heads seq_len head_dim make_tensor = partial rand_sdpa_tensor device=device dtype=torch float packed=True requires_grad=True qkv = make_tensor shape type=type query key value = qkv chunk dim=- query = query view batch_size - num_heads head_dim transpose value = value view batch_size - num_heads head_dim transpose key = key view batch_size - num_heads head_dim transpose device_capability = None cuda str device device_capability = torch cuda get_device_capability prefer_cudnn = TORCH_CUDNN_SDPA_PREFERRED os environ bool os environ TORCH_CUDNN_SDPA_PREFERRED prefer_cudnn = prefer_cudnn device_capability device_capability == device_capability == TODO we currently disabling default lets assert returns FlashAttention we need change when we make remove opt-in cudnn type = nested PLATFORM_SUPPORTS_CUDNN_ATTENTION prefer_cudnn assertEqual torch _fused_sdp_choice query key value SDPBackend CUDNN_ATTENTION value PLATFORM_SUPPORTS_FLASH_ATTENTION assertEqual torch _fused_sdp_choice query key value SDPBackend FLASH_ATTENTION value type = nested PLATFORM_SUPPORTS_CUDNN_ATTENTION prefer_cudnn e g we re Windows assertEqual torch _fused_sdp_choice query key value SDPBackend EFFICIENT_ATTENTION value sdpa_kernel backends= SDPBackend CUDNN_ATTENTION assertEqual torch _fused_sdp_choice query key value SDPBackend CUDNN_ATTENTION value assertEqual torch _fused_sdp_choice query key value SDPBackend EFFICIENT_ATTENTION value Change dtype float so efficient attention should get chosen make_tensor = partial rand_sdpa_tensor device=device dtype=torch float packed=True qkv = make_tensor shape type=type query key value = qkv chunk dim=- query = query view batch_size - num_heads head_dim transpose value = value view batch_size - num_heads head_dim transpose key = key view batch_size - num_heads head_dim transpose assert torch _fused_sdp_choice query key value == SDPBackend EFFICIENT_ATTENTION value skipIfRocm Missing triton float triton prefix locate skipped UTs deterministic algo unittest skipIf PLATFORM_SUPPORTS_MEM_EFF_ATTENTION Platform does support fused SDPA parametrize warn_only True False test_sdp_choice_with_determinism device warn_only batch_size seq_len num_heads head_dim = shape = SdpaShape batch_size num_heads seq_len head_dim make_tensor = partial rand_sdpa_tensor type= dense device=device dtype=torch float packed=False query key value = make_tensor shape make_tensor shape make_tensor shape use_deterministic_algorithims True warn_only=warn_only sdpa_kernel backends= SDPBackend EFFICIENT_ATTENTION SDPBackend MATH assert torch _fused_sdp_choice query key value == SDPBackend EFFICIENT_ATTENTION value onlyCUDA unittest skipIf PLATFORM_SUPPORTS_CUDNN_ATTENTION cuDNN Attention supported system unittest skipIf PLATFORM_SUPPORTS_MEM_EFF_ATTENTION Platform does support fused SDPA parametrize use_compile True False test_fused_sdp_priority_order device use_compile torch compile compiled_func order sdpa_kernel order set_priority=True out = scaled_dot_product_attention q q q out q = torch randn dtype=torch half device= cuda default_order = torch _C _get_sdp_priority_order orders = SDPBackend CUDNN_ATTENTION SDPBackend MATH SDPBackend EFFICIENT_ATTENTION SDPBackend MATH SDPBackend CUDNN_ATTENTION SDPBackend EFFICIENT_ATTENTION SDPBackend EFFICIENT_ATTENTION SDPBackend CUDNN_ATTENTION SDPBackend MATH SDPBackend FLASH_ATTENTION SDPBackend CUDNN_ATTENTION SDPBackend MATH time times = list order orders use_compile compiled_func order sdpa_kernel order set_priority=True scaled_dot_product_attention q q q torch cuda synchronize t = time perf_counter use_compile compiled_func order sdpa_kernel order set_priority=True scaled_dot_product_attention q q q torch cuda synchronize t = time perf_counter times append t - t assertTrue times times expected cuDNN SDPA faster than Math backend assertTrue times times expected Eff Attn backend faster than Math backend assertTrue times times expected Flash Attn backend faster than Math backend assertTrue times times expected cuDNN Attn backend faster than Eff Attn backend reset_order = torch _C _get_sdp_priority_order assertEqual default_order reset_order expected SDPA context manager reset priority order skipIfRocm Missing deterministic algo unittest skipIf PLATFORM_SUPPORTS_FUSED_ATTENTION Fused SDPA built system parametrize fused_kernel PLATFORM_SPECIFIC_SDPA parametrize warn_only True False test_fused_backwards_throws_determinism_warning device warn_only fused_kernel batch_size seq_len num_heads head_dim = shape = SdpaShape batch_size num_heads seq_len head_dim make_tensor = partial rand_sdpa_tensor type= dense device=device dtype=torch float packed=False requires_grad=True query key value = make_tensor shape make_tensor shape make_tensor shape kernel_name = Memory Efficient attention fused_kernel == SDPBackend EFFICIENT_ATTENTION \ Flash Attention fused_kernel == SDPBackend FLASH_ATTENTION cuDNN Attention warning_context = assertWarnsRegex UserWarning f kernel_name defaults non-deterministic algorithm warn_only contextlib nullcontext use_deterministic_algorithims True warn_only=warn_only sdpa_kernel backends= fused_kernel warning_context warn_only fused_kernel = SDPBackend CUDNN_ATTENTION torch nn functional scaled_dot_product_attention query key value sum backward cuDNN attention has no deterministic fallback assertRaises RuntimeError lambda torch nn functional scaled_dot_product_attention query key value sum backward unittest skip This test behaving deterministaclly non-deterministaclly CI CD unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Platform does support fused SDPA test_mem_eff_backwards_determinism device Need big seq_len ensure num_splits dtype = torch float batch_size seq_len n_heads head_dim = query = torch rand batch_size n_heads seq_len head_dim device=device dtype=dtype requires_grad=True key = torch rand batch_size n_heads seq_len head_dim device=device dtype=dtype requires_grad=True value = torch rand batch_size n_heads seq_len head_dim device=device dtype=dtype requires_grad=True sdpa_kernel backends= SDPBackend EFFICIENT_ATTENTION Run once establish baseline out = F scaled_dot_product_attention query key value upward_grad = torch rand_like out out backward upward_grad initial_query_grad = query grad Re-run op same upward grad check backward deterministic diff_anwser_once = False _ range query grad = None out = F scaled_dot_product_attention query key value out backward upward_grad torch equal initial_query_grad query grad diff_anwser_once = True break assertTrue diff_anwser_once use_deterministic_algorithims True warn_only=False query grad = None out = F scaled_dot_product_attention query key value upward_grad = torch rand_like out out backward upward_grad initial_query_grad = query grad Re-run op same upward grad check backward deterministic now we have enforced diff_anwser_once = False _ range query grad = None out = F scaled_dot_product_attention query key value out backward upward_grad torch equal initial_query_grad query grad diff_anwser_once = True break assertFalse diff_anwser_once verified passing successfully H unittest skipIf PLATFORM_SUPPORTS_MEM_EFF_ATTENTION Does support SDPA unittest skipIf IS_JETSON causing sigkill Jetson parametrize batch_size parametrize seq_len_q MEM_EFF_CAPABILITY_MATCHES_SM parametrize seq_len_k MEM_EFF_CAPABILITY_MATCHES_SM parametrize head_dim MEM_EFF_CAPABILITY_MATCHES_SM isSM Device parametrize is_causal False True parametrize dropout_p parametrize dtype torch float torch bfloat torch float MEM_EFF_CAPABILITY_MATCHES_SM torch float torch float parametrize scale None l tf _enabled test_mem_efficient_attention_vs_math_ref_grads device batch_size int seq_len_q int seq_len_k int head_dim int is_causal bool dropout_p float dtype torch dtype scale str _get_mem_eff_drop_mask batch_size n_heads q_len kv_len p seed offset device=device mask = torch empty batch_size n_heads q_len kv_len device=device dtype=torch float rand_uniform = torch _fill_mem_eff_dropout_mask_ mask p seed offset On ROCM _fill_mem_eff_dropout_mask fills prng p otherwise - tensor tester_p = p TEST_WITH_ROCM mask = rand_uniform tester_p torch float mask max seq_len_q seq_len_k = torch cuda get_device_properties cuda total_memory unittest skip Reference implementation OOM TEST_WITH_ROCM seq_len_q seq_len_k head_dim batch_size torch cuda empty_cache Prevent memory fragmentation seed = scale = scale scale None head_dim n_heads = query = torch rand batch_size n_heads seq_len_q head_dim device=device dtype=dtype requires_grad=True key = torch rand batch_size n_heads seq_len_k head_dim device=device dtype=dtype requires_grad=True value = torch rand batch_size n_heads seq_len_k head_dim device=device dtype=dtype requires_grad=True higher_precision_dtype = torch float query_ref key_ref value_ref = query_key_value_clones query key value dtype=higher_precision_dtype Create real output sdpa_kernel backends= SDPBackend EFFICIENT_ATTENTION Set seed run kernel torch manual_seed seed out = F scaled_dot_product_attention query key value dropout_p=dropout_p is_causal=is_causal scale=scale dropout_p == sdpa_kernel backends= SDPBackend MATH High Precision Math Reference out_ref = F scaled_dot_product_attention query_ref key_ref value_ref dropout_p=dropout_p is_causal=is_causal scale=scale Low Precision Math Reference out_lp_ref = F scaled_dot_product_attention query key value dropout_p=dropout_p is_causal=is_causal scale=scale seq_len_q skipTest Will call _fill_mem_eff_dropout_mask too many threads Create dropout_mask torch manual_seed seed dropout_mask = _get_mem_eff_drop_mask batch_size n_heads seq_len_q seq_len_k dropout_p seed device=device High Precision Math Reference out_ref = torch ops aten _scaled_dot_product_attention_math query_ref key_ref value_ref dropout_p=dropout_p is_causal=is_causal scale=scale dropout_mask=dropout_mask Low Precision Math Reference out_lp_ref = torch ops aten _scaled_dot_product_attention_math query key value dropout_p=dropout_p is_causal=is_causal scale=scale dropout_mask=dropout_mask upstream_grad = torch rand_like out requires_grad=False grads = torch autograd grad out query key value upstream_grad grads_ref_lp = torch autograd grad out_lp_ref query key value upstream_grad grads_ref = torch autograd grad out_ref query_ref key_ref value_ref upstream_grad fudge_factors = out grad_query grad_key grad_value TEST_WITH_ROCM fudge_factors out = fudge_factors grad_key = fudge_factors grad_query = seq_len_k = fudge_factors grad_key = seq_len_k = fudge_factors grad_key = fudge_factors grad_query = dtype == torch float fudge_factors grad_key = gfx torch cuda get_device_properties gcnArchName fudge_factors grad_value = check_out_and_grad out_ref out_lp_ref out zip grads_ref grads_ref_lp grads fudge_factors=fudge_factors unittest skipIf PLATFORM_SUPPORTS_MEM_EFF_ATTENTION Does support SDPA unittest skipIf IS_JETSON causing sigkill Jetson parametrize batch_size parametrize seq_len_q MEM_EFF_CAPABILITY_MATCHES_SM parametrize seq_len_k MEM_EFF_CAPABILITY_MATCHES_SM parametrize head_dim MEM_EFF_CAPABILITY_MATCHES_SM isSM Device parametrize is_causal False parametrize dropout_p parametrize dtype torch float torch bfloat torch float MEM_EFF_CAPABILITY_MATCHES_SM torch float torch float parametrize scale None l tf _enabled test_mem_efficient_attention_attn_mask_vs_math_ref_grads device batch_size int seq_len_q int seq_len_k int head_dim int is_causal bool dropout_p float dtype torch dtype scale str _get_mem_eff_drop_mask batch_size n_heads q_len kv_len p seed offset device=device mask = torch empty batch_size n_heads q_len kv_len device=device dtype=torch float rand_uniform = torch _fill_mem_eff_dropout_mask_ mask p seed offset On ROCM _fill_mem_eff_dropout_mask fills prng p otherwise - tensor tester_p = p TEST_WITH_ROCM mask = rand_uniform tester_p torch float mask max seq_len_q seq_len_k = torch cuda get_device_properties cuda total_memory unittest skip Reference implementation OOM TEST_WITH_ROCM seq_len_q seq_len_k head_dim batch_size torch cuda empty_cache Prevent memory fragmentation seed = scale = scale scale None head_dim n_heads = query = torch rand batch_size n_heads seq_len_q head_dim device=device dtype=dtype requires_grad=True key = torch rand batch_size n_heads seq_len_k head_dim device=device dtype=dtype requires_grad=True value = torch rand batch_size n_heads seq_len_k head_dim device=device dtype=dtype requires_grad=True attn_mask = torch rand seq_len_q seq_len_k device=device dtype=dtype requires_grad=True higher_precision_dtype = torch float dtype == torch float torch float query_ref key_ref value_ref = query_key_value_clones query key value dtype=higher_precision_dtype attn_mask_ref = attn_mask detach higher_precision_dtype requires_grad_ True Create real output sdpa_kernel backends= SDPBackend EFFICIENT_ATTENTION Set seed run kernel torch manual_seed seed out = F scaled_dot_product_attention query key value attn_mask dropout_p=dropout_p is_causal=is_causal scale=scale dropout_p == sdpa_kernel backends= SDPBackend MATH High Precision Math Reference out_ref = F scaled_dot_product_attention query_ref key_ref value_ref attn_mask_ref dropout_p=dropout_p is_causal=is_causal scale=scale Low Precision Math Reference out_lp_ref = F scaled_dot_product_attention query key value attn_mask dropout_p=dropout_p is_causal=is_causal scale=scale seq_len_q skipTest Will call _fill_mem_eff_dropout_mask too many threads Create dropout_mask torch manual_seed seed dropout_mask = _get_mem_eff_drop_mask batch_size n_heads seq_len_q seq_len_k dropout_p seed device=device High Precision Math Reference out_ref = torch ops aten _scaled_dot_product_attention_math query_ref key_ref value_ref attn_mask_ref dropout_p=dropout_p is_causal=is_causal scale=scale dropout_mask=dropout_mask Low Precision Math Reference out_lp_ref = torch ops aten _scaled_dot_product_attention_math query key value attn_mask dropout_p=dropout_p is_causal=is_causal scale=scale dropout_mask=dropout_mask upstream_grad = torch rand_like out requires_grad=False grads = torch autograd grad out query key value attn_mask upstream_grad grads_ref_lp = torch autograd grad out_lp_ref query key value attn_mask upstream_grad grads_ref = torch autograd grad out_ref query_ref key_ref value_ref attn_mask_ref upstream_grad fudge_factors = out grad_query grad_key grad_value grad_attn_mask TEST_WITH_ROCM fudge_factors out = fudge_factors grad_key = fudge_factors grad_query = seq_len_k = fudge_factors grad_key = seq_len_k = fudge_factors grad_key = fudge_factors grad_query = gfx dtype == torch float fudge_factors grad_key = gfx torch cuda get_device_properties gcnArchName fudge_factors grad_value = check_out_and_grad out_ref out_lp_ref out zip grads_ref grads_ref_lp grads fudge_factors=fudge_factors unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Does support SDPA pre-SM hardware unittest skipIf IS_JETSON causing sigkill Jetson parametrize batch_size parametrize seq_len_q parametrize seq_len_k parametrize head_dim parametrize is_causal True False parametrize dropout_p parametrize dtype torch float torch bfloat parametrize scale None l parametrize enable_gqa True False parametrize n_heads tf _enabled test_flash_attention_vs_math_ref_grads device batch_size int seq_len_q int seq_len_k int head_dim int is_causal bool dropout_p float dtype torch dtype scale str enable_gqa bool n_heads list int isSM XDevice isSM Device head_dim range + skipTest Flash attention sm sm sm headdim currently disabled is_causal seq_len_q = seq_len_k skipTest Flash V does accept is_casual when seq_len_q = seq_len_k TEST_WITH_ROCM seq_len_q = seq_len_k = batch_size torch cuda empty_cache Prevent memory fragmentation max seq_len_q seq_len_k = torch cuda get_device_properties cuda total_memory unittest skip Reference implementation OOM TEST_WITH_CK dropout_p = skipTest CK does support tensor format dropout masks TEST_WITH_CK head_dim skipTest CK does support head dims over scale = scale scale None head_dim num_heads_q = num_heads_kv = enable_gqa num_heads_q = n_heads num_heads_kv = n_heads query = torch rand batch_size num_heads_q seq_len_q head_dim device=device dtype=dtype requires_grad=True key = torch rand batch_size num_heads_kv seq_len_k head_dim device=device dtype=dtype requires_grad=True value = torch rand batch_size num_heads_kv seq_len_k head_dim device=device dtype=dtype requires_grad=True higher_precision_dtype = torch float dtype == torch float torch float query_ref key_ref value_ref = query_key_value_clones query key value dtype=higher_precision_dtype is_dropout = dropout_p is_dropout sdpa_kernel backends= SDPBackend FLASH_ATTENTION out = F scaled_dot_product_attention query key value dropout_p=dropout_p is_causal=is_causal scale=scale enable_gqa=enable_gqa sdpa_kernel backends= SDPBackend MATH High Precision Math Reference out_ref = F scaled_dot_product_attention query_ref key_ref value_ref is_causal=is_causal scale=scale enable_gqa=enable_gqa Low Precision Math Reference out_lp_ref = F scaled_dot_product_attention query key value is_causal=is_causal scale=scale enable_gqa=enable_gqa Problem We pad sizes composite region top level SDPA But we need Debug mask when have dropout So I am going manually pad up here when testing dropout q_padded q_og_size = pad_last_dim query k_padded k_og_size = pad_last_dim key v_padded v_og_size = pad_last_dim value scale needs calculated og head_size scale None scale = math sqrt q_og_size output_tuple = torch ops aten _scaled_dot_product_flash_attention q_padded k_padded v_padded dropout_p=dropout_p is_causal=is_causal scale=scale return_debug_mask=is_dropout out = output_tuple out = out v_og_size Build dropout_mask dbug_mask = output_tuple - query_padding_mask = torch ones batch_size seq_len_q device=device dtype=torch bool key_padding_mask = torch ones batch_size seq_len_k device=device dtype=torch bool softmax_mask = convert_flash_attn_S_to_softmax dbug_mask seq_len_q seq_len_k query_padding_mask key_padding_mask causal=is_causal seq_len_q seq_len_k dropout_mask = softmax_mask = High Precision Math Reference out_ref = torch ops aten _scaled_dot_product_attention_math query_ref key_ref value_ref dropout_p=dropout_p is_causal=is_causal scale=scale dropout_mask=dropout_mask enable_gqa=enable_gqa Low Precision Math Reference out_lp_ref = torch ops aten _scaled_dot_product_attention_math query key value dropout_p=dropout_p is_causal=is_causal scale=scale dropout_mask=dropout_mask enable_gqa=enable_gqa upstream_grad = torch rand_like out requires_grad=False backward flash attention sm sm sm headdim = currently disabled isSM XDevice isSM Device head_dim range assertRaises RuntimeError lambda out backward upstream_grad grads = torch autograd grad out query key value upstream_grad grads_ref_lp = torch autograd grad out_lp_ref query key value upstream_grad grads_ref = torch autograd grad out_ref query_ref key_ref value_ref upstream_grad fudge_factors = out grad_query grad_key grad_value TEST_WITH_ROCM fudge_factors grad_value = TEST_WITH_CK fudge_factors out = fudge_factors grad_key = fudge_factors grad_query = ck min = seq_len_k = fudge_factors grad_key = seq_len_k = fudge_factors grad_key = fudge_factors grad_query = NEW CK MIN seq_len_q = fudge_factors grad_query = dtype == torch float fudge_factors grad_key = fudge_factors out = fudge_factors grad_key = fudge_factors grad_query = seq_len_k = fudge_factors grad_key = seq_len_k = fudge_factors grad_key = fudge_factors grad_query = seq_len_q = fudge_factors grad_query = dtype == torch float fudge_factors grad_key = check_out_and_grad out_ref out_lp_ref out zip grads_ref grads_ref_lp grads fudge_factors=fudge_factors unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Does support SDPA pre-SM hardware parametrize batch_size parametrize seq_len_q parametrize seq_len_k parametrize head_dim parametrize is_causal True False parametrize dropout_p parametrize dtype torch float parametrize scale None l parametrize fused_kernel PLATFORM_SPECIFIC_SDPA tf _enabled test_fused_attention_vs_math_ref_grads_cudagraph device batch_size int seq_len_q int seq_len_k int head_dim int is_causal bool dropout_p float dtype torch dtype scale str fused_kernel SDPBackend _get_mem_eff_drop_mask batch_size n_heads q_len kv_len dropout_p seed offset device=device mask = torch empty batch_size n_heads q_len kv_len device=device dtype=torch float rand_uniform = torch _fill_mem_eff_dropout_mask_ mask dropout_p seed offset On ROCM _fill_mem_eff_dropout_mask fills prng p otherwise - tensor tester_p = dropout_p TEST_WITH_ROCM mask = rand_uniform tester_p torch float mask get_dropout_mask output fused_kernel batch_size n_heads q_len kv_len dropout_p device=device fused_kernel == SDPBackend EFFICIENT_ATTENTION output_seed output_offset = output_tuple output_tuple output_seed = output_seed item output_offset = output_offset item _get_mem_eff_drop_mask batch_size n_heads q_len kv_len dropout_p output_seed output_offset device=device Build dropout_mask dbug_mask = output_tuple - query_padding_mask = torch ones batch_size seq_len_q device=device dtype=torch bool key_padding_mask = torch ones batch_size seq_len_k device=device dtype=torch bool softmax_mask = convert_flash_attn_S_to_softmax dbug_mask seq_len_q seq_len_k query_padding_mask key_padding_mask causal=is_causal seq_len_q seq_len_k dropout_mask = softmax_mask = dropout_mask fused_kernel == SDPBackend FLASH_ATTENTION is_causal seq_len_q = seq_len_k skipTest Flash V does accept is_casual when seq_len_q = seq_len_k seed = n_heads = query = torch rand batch_size n_heads seq_len_q head_dim device=device dtype=dtype requires_grad=True key = torch rand batch_size n_heads seq_len_k head_dim device=device dtype=dtype requires_grad=True value = torch rand batch_size n_heads seq_len_k head_dim device=device dtype=dtype requires_grad=True fused_op = torch ops aten _scaled_dot_product_efficient_attention fused_kernel == SDPBackend EFFICIENT_ATTENTION torch ops aten _scaled_dot_product_flash_attention fused_kernel == SDPBackend FLASH_ATTENTION torch ops aten _scaled_dot_product_cudnn_attention higher_precision_dtype = torch float dtype == torch float torch float query_ref key_ref value_ref = query_key_value_clones query key value dtype=higher_precision_dtype warmup s = torch cuda Stream s wait_stream torch cuda current_stream Set global seed before capture torch manual_seed seed kwargs = dropout_p dropout_p is_causal is_causal fused_kernel == SDPBackend EFFICIENT_ATTENTION kwargs compute_log_sumexp = True kwargs attn_bias = None fused_kernel == SDPBackend FLASH_ATTENTION kwargs return_debug_mask = dropout_p fused_kernel == SDPBackend CUDNN_ATTENTION kwargs compute_log_sumexp = True kwargs attn_bias = None return_debug_mask kwargs kwargs pop return_debug_mask torch cuda stream s Create real output output_tuple = fused_op query key value kwargs torch cuda current_stream wait_stream s out = output_tuple upstream_grad = torch rand_like out requires_grad=False s wait_stream torch cuda current_stream torch cuda stream s out backward upstream_grad x query key value x grad = None g = torch cuda CUDAGraph Create real output torch cuda graph g torch rand_like query device=query device test non-zero intragraph offset Create real output output_tuple = fused_op query key value kwargs assert all isinstance o torch Tensor o is_cuda o output_tuple g replay out_first = output_tuple clone g replay out = output_tuple dropout_p == assertEqual out_first out atol= rtol= replays produce different results assertNotEqual out_first out sdpa_kernel backends= SDPBackend MATH dropout_p == High Precision Math Reference out_ref = F scaled_dot_product_attention query_ref key_ref value_ref dropout_p=dropout_p is_causal=is_causal Low Precision Math Reference out_lp_ref = F scaled_dot_product_attention query key value dropout_p=dropout_p is_causal=is_causal cuDNN attention doesn t support returning dropout mask fused_kernel = SDPBackend CUDNN_ATTENTION Create dropout_mask dropout_mask = get_dropout_mask output_tuple fused_kernel batch_size n_heads seq_len_q seq_len_k dropout_p device High Precision Math Reference out_ref = torch ops aten _scaled_dot_product_attention_math query_ref key_ref value_ref dropout_p=dropout_p is_causal=is_causal dropout_mask=dropout_mask Low Precision Math Reference out_lp_ref = torch ops aten _scaled_dot_product_attention_math query key value dropout_p=dropout_p is_causal=is_causal dropout_mask=dropout_mask g = torch cuda CUDAGraph torch cuda graph g grads = torch autograd grad out query key value upstream_grad g replay fused_kernel = SDPBackend CUDNN_ATTENTION dropout_p == grads_ref_lp = torch autograd grad out_lp_ref query key value upstream_grad grads_ref = torch autograd grad out_ref query_ref key_ref value_ref upstream_grad fudge_factors = out grad_query grad_key grad_value TEST_WITH_ROCM fudge_factors out = fudge_factors grad_value = check_out_and_grad out_ref out_lp_ref out zip grads_ref grads_ref_lp grads fudge_factors=fudge_factors unittest skipIf PLATFORM_SUPPORTS_FUSED_ATTENTION Fused SDPA built system parametrize fused_kernel SDPBackend FLASH_ATTENTION SDPBackend EFFICIENT_ATTENTION PLATFORM_SUPPORTS_FLASH_ATTENTION SDPBackend EFFICIENT_ATTENTION test_fused_kernels_seq_len_ _inputs device fused_kernel rand_nested_tensor = partial rand_sdpa_tensor type= nested device=device dtype=torch float batch num_heads head_dim = seq_lens = torch randint low= high= size= batch make sure some seq_lens num_ones = indices = torch randint low= high=batch size= num_ones seq_lens scatter_ indices shape = SdpaShape batch num_heads seq_lens tolist head_dim query = rand_nested_tensor shape key = rand_nested_tensor shape value = rand_nested_tensor shape query = query transpose key = key transpose value = value transpose sdpa_kernel backends= fused_kernel actual = torch nn functional scaled_dot_product_attention query key value attn_mask=None dropout_p= is_causal=False sdpa_kernel backends= SDPBackend MATH math_ref = torch nn functional scaled_dot_product_attention query contiguous torch float key contiguous torch float value contiguous torch float attn_mask=None dropout_p= is_causal=False assertEqual actual contiguous math_ref contiguous torch float atol= e- rtol= e- unittest skipIf PLATFORM_SUPPORTS_FUSED_ATTENTION Fused SDPA built system parametrize kernel SDPBackend FLASH_ATTENTION SDPBackend EFFICIENT_ATTENTION PLATFORM_SUPPORTS_FLASH_ATTENTION SDPBackend EFFICIENT_ATTENTION parametrize expand_q_batch True False parametrize expand_k_batch True False parametrize expand_v_batch True False parametrize expand_q_num_heads True False parametrize expand_k_num_heads True False parametrize expand_v_num_heads True False test_fused_kernels_nested_broadcasting device kernel expand_q_batch expand_k_batch expand_v_batch expand_q_num_heads expand_k_num_heads expand_v_num_heads is_efficient = kernel == SDPBackend EFFICIENT_ATTENTION dtype = torch float is_efficient torch float rand_nested_tensor = partial rand_sdpa_tensor type= nested device=device dtype=dtype batch num_heads head_dim = head_dim_v = is_efficient head_dim TEST_WITH_ROCM head_dim = head_dim_v skipTest head_dim = head_dim_v unsupported ROCm now seq_lens_q = torch randint low= high= size= item expand_q_batch torch randint low= high= size= batch tolist seq_lens_kv = torch randint low= high= size= item expand_k_batch expand_v_batch torch randint low= high= size= batch tolist batch_q = expand_q_batch batch batch_k = expand_k_batch batch batch_v = expand_v_batch batch handle case where all batch_sizes batch = max batch_q batch_k batch_v num_heads_q = expand_q_num_heads num_heads num_heads_k = expand_k_num_heads num_heads num_heads_v = expand_v_num_heads num_heads handle case where all num_heads num_heads = max num_heads_q num_heads_k num_heads_v q_shape = SdpaShape batch_q num_heads_q seq_lens_q head_dim k_shape = SdpaShape batch_k num_heads_k seq_lens_kv head_dim v_shape = SdpaShape batch_v num_heads_v seq_lens_kv head_dim_v query = rand_nested_tensor q_shape key = rand_nested_tensor k_shape value = rand_nested_tensor v_shape _broadcast t batch_broadcasted num_heads_broadcasted batch_broadcasted num_heads_broadcasted seq_len head_dim - batch seq_len num_heads head_dim result = torch nested nested_tensor t expand - num_heads t size - _ range batch dtype=torch float batch_broadcasted seq_len num_heads head_dim - batch seq_len num_heads head_dim result = torch nested nested_tensor t _ range batch dtype=torch float num_heads_broadcasted batch seq_len head_dim - batch seq_len num_heads head_dim result = torch nested nested_tensor x expand - num_heads t size - x t unbind dtype=torch float result = t torch float result query_expanded = _broadcast query expand_q_batch expand_q_num_heads transpose key_expanded = _broadcast key expand_k_batch expand_k_num_heads transpose value_expanded = _broadcast value expand_v_batch expand_v_num_heads transpose query = query transpose key = key transpose value = value transpose sdpa_kernel backends= kernel actual = torch nn functional scaled_dot_product_attention query key value attn_mask=None dropout_p= is_causal=False sdpa_kernel backends= SDPBackend MATH math_ref = torch nn functional scaled_dot_product_attention query_expanded contiguous key_expanded contiguous value_expanded contiguous attn_mask=None dropout_p= is_causal=False assertEqual actual contiguous math_ref contiguous dtype atol= e- rtol= e- skipIfRocm msg= Efficient Attention ROCM does support head_dim = head_dim_v now unittest skipIf PLATFORM_SUPPORTS_MEM_EFF_ATTENTION Fused SDPA built system test_fused_kernels_nested_broadcasting_query_dense device rand_nested_tensor = partial rand_sdpa_tensor type= nested device=device dtype=torch float batch num_heads head_dim head_dim_v = seq_lens = torch randint low= high= size= batch tolist q_shape = num_heads head_dim k_shape = SdpaShape batch num_heads seq_lens head_dim v_shape = SdpaShape batch seq_lens head_dim_v create dense query query = torch randn q_shape device=device dtype=torch float key = rand_nested_tensor k_shape value = rand_nested_tensor v_shape num_heads head_dim - batch num_heads head_dim query_expanded = torch nested nested_tensor query squeeze _ range batch transpose batch seq_lens head_dim - batch seq_lens num_heads head_dim value_expanded = torch nested nested_tensor t expand - num_heads head_dim_v t value unbind transpose query = query transpose key = key transpose value = value transpose sdpa_kernel backends= SDPBackend EFFICIENT_ATTENTION actual = torch nn functional scaled_dot_product_attention query key value attn_mask=None dropout_p= is_causal=False sdpa_kernel backends= SDPBackend MATH math_ref = torch nn functional scaled_dot_product_attention query_expanded contiguous key contiguous value_expanded contiguous attn_mask=None dropout_p= is_causal=False assertEqual actual contiguous math_ref contiguous atol= e- rtol= e- unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Does support SDPA pre-SM hardware parametrize batch_size parametrize max_seq_len_q parametrize max_seq_len_kv parametrize head_dim parametrize dropout_p parametrize dtype torch float parametrize scale None l parametrize is_causal True False test_flash_attention_vs_math_ref_grads_nestedtensor device batch_size int max_seq_len_q int max_seq_len_kv int head_dim int dropout_p float dtype torch dtype scale str is_causal bool is_causal TODO we should support assertRaisesRegex RuntimeError Nested tensors query key supported when is_causal=True scale = scale scale None head_dim n_heads = seq_lens_q = torch randint low= high=max_seq_len_q size= batch_size Set one entry max length seq_lens_q torch randint batch_size size= = max_seq_len_q seq_lens_kv = torch randint low= high=max_seq_len_kv size= batch_size seq_lens_kv torch randint batch_size size= = max_seq_len_kv rand_nt sequence_list num_heads head_dim tensors = torch rand num_heads seq_len head_dim seq_len sequence_list torch nested nested_tensor tensors requires_grad=True device=device dtype=dtype query = rand_nt seq_lens_q n_heads head_dim key = rand_nt seq_lens_kv n_heads head_dim value = rand_nt seq_lens_kv n_heads head_dim Run math kernel low precision references query_ref_lp = query detach clone requires_grad_ True key_ref_lp = key detach clone requires_grad_ True value_ref_lp = value detach clone requires_grad_ True query_ref = query detach clone torch float requires_grad_ True key_ref = key detach clone torch float requires_grad_ True value_ref = value detach clone torch float requires_grad_ True is_dropout = dropout_p is_dropout sdpa_kernel backends= SDPBackend FLASH_ATTENTION out = F scaled_dot_product_attention query key value dropout_p=dropout_p is_causal=is_causal scale=scale sdpa_kernel backends= SDPBackend MATH High Precision Math Reference out_ref = F scaled_dot_product_attention query_ref key_ref value_ref is_causal=is_causal scale=scale Low Precision Math Reference out_lp_ref = F scaled_dot_product_attention query_ref_lp key_ref_lp value_ref_lp is_causal=is_causal scale=scale Create real output output_tuple = torch ops aten _scaled_dot_product_flash_attention query key value dropout_p=dropout_p is_causal=is_causal scale=scale return_debug_mask=is_dropout out = output_tuple dbug_mask = output_tuple - query_padding_mask = torch arange max_seq_len_q unsqueeze expand batch_size max_seq_len_q seq_lens_q unsqueeze - query_padding_mask = query_padding_mask cuda key_padding_mask = torch arange max_seq_len_kv unsqueeze expand batch_size max_seq_len_kv seq_lens_kv unsqueeze - key_padding_mask = key_padding_mask cuda softmax_mask = convert_flash_attn_S_to_softmax dbug_mask max_seq_len_q max_seq_len_kv query_padding_mask key_padding_mask causal=is_causal dropout_mask = softmax_mask = nt_stack = tensor_component range batch_size batch_stack = head range n_heads batch_stack append dropout_mask tensor_component head seq_lens_q tensor_component seq_lens_kv tensor_component unsqueeze nt_stack append torch cat batch_stack nested_dropout_mask = torch nested nested_tensor nt_stack High Precision Math Reference out_ref = torch ops aten _scaled_dot_product_attention_math query_ref key_ref value_ref dropout_p=dropout_p is_causal=is_causal scale=scale dropout_mask=nested_dropout_mask Low Precision Math Reference out_lp_ref = torch ops aten _scaled_dot_product_attention_math query_ref_lp key_ref_lp value_ref_lp dropout_p=dropout_p is_causal=is_causal scale=scale dropout_mask=nested_dropout_mask upstream_grad = out detach clone contiguous out backward upstream_grad out_ref backward upstream_grad out_ref dtype out_lp_ref backward upstream_grad out_lp_ref dtype dropout_fudge_factor = dropout_p == check_out_and_grad out_ref out_lp_ref out query_ref query_ref_lp query key_ref key_ref_lp key value_ref value_ref_lp value fudge_factors= out dropout_fudge_factor grad_query dropout_fudge_factor grad_key dropout_fudge_factor grad_value dropout_fudge_factor TestSDPAXpuOnly NNTestCase Used test XPU only functionality scaled_dot_product_attention Mostly migrate TestSDPACudaOnly test test_transformers py parametrize type dense parametrize dropout parametrize dtype torch float torch float torch bfloat torch half skipIfTorchDynamo test_fused_sdp_choice_xpu device type str dropout float dtype torch dtype Migrate test_fused_sdp_choice_cpu make_tensor = partial rand_sdpa_tensor type=type device=device dtype=dtype size = SdpaShape q k v = make_tensor size make_tensor size make_tensor size dropout dtype torch float torch bfloat torch float assert torch _fused_sdp_choice q k v dropout_p=dropout == SDPBackend MATH value assert torch _fused_sdp_choice q k v dropout_p=dropout == SDPBackend OVERRIDEABLE value test_fused_attention_different_dk_dv device dtype = torch bfloat make_tensor = partial torch rand device=device dtype=dtype requires_grad=False batch num_heads head_dim_k head_dim_v = q_shape = SdpaShape batch num_heads head_dim_k k_shape = SdpaShape batch num_heads head_dim_k v_shape = SdpaShape batch num_heads head_dim_v query key value = make_tensor q_shape make_tensor k_shape make_tensor v_shape actual = F scaled_dot_product_attention query key value attn_mask=None dropout_p= is_causal=False math_ref = torch ops aten _scaled_dot_product_attention_math query float key float value float attn_mask=None dropout_p= is_causal=False assertEqual actual contiguous math_ref contiguous dtype atol= e- rtol= e- parametrize dtype torch half torch bfloat parametrize batch_size n_head n_head_kv q_size kv_size head_dim parametrize is_causal True False test_fused_attention_gqa device dtype batch_size n_head n_head_kv q_size kv_size head_dim is_causal tol = Tolerances e- e- dtype torch bfloat tol = Tolerances e- e- dtype torch float tol = Tolerances e- e- make_tensor = partial torch rand device=device dtype=dtype requires_grad=False q_shape = SdpaShape batch_size n_head q_size head_dim k_shape = SdpaShape batch_size n_head_kv kv_size head_dim v_shape = SdpaShape batch_size n_head_kv kv_size head_dim query key value = make_tensor q_shape make_tensor k_shape make_tensor v_shape actual = F scaled_dot_product_attention query key value attn_mask=None dropout_p= is_causal=is_causal enable_gqa=True math_ref = torch ops aten _scaled_dot_product_attention_math query float key float value float attn_mask=None dropout_p= is_causal=is_causal enable_gqa=True assertEqual actual contiguous math_ref contiguous dtype atol=tol atol rtol=tol rtol test_onednn_attention_fail_d device Test onednn graph attention dispatching correctly bails out d b h = s_q s_kv = d_qk d_v = q = torch randn b h s_q d_qk device=device dtype=torch bfloat k = torch randn b h s_kv d_qk device=device dtype=torch bfloat v = torch randn b h s_kv d_v device=device dtype=torch bfloat sdpa_kernel backends= SDPBackend OVERRIDEABLE assertRaisesRegex RuntimeError No available kernel _ = F scaled_dot_product_attention q k v test_fused_attention_broadcasted_input device dtype = torch bfloat make_tensor = partial torch rand device=device dtype=dtype requires_grad=False batch num_heads seqlen head_dim = q_shape = SdpaShape batch num_heads seqlen head_dim k_shape = SdpaShape batch num_heads seqlen head_dim v_shape = SdpaShape batch num_heads seqlen head_dim query key value = make_tensor q_shape make_tensor k_shape make_tensor v_shape attn_mask_shape = seqlen attn_mask = make_tensor attn_mask_shape attn_mask = attn_mask expand seqlen seqlen test we do dispatch onednn unsupported case actual = F scaled_dot_product_attention query key value attn_mask=attn_mask dropout_p= is_causal=False math_ref = torch ops aten _scaled_dot_product_attention_math query float key float value float attn_mask=attn_mask dropout_p= is_causal=False assertEqual actual contiguous math_ref contiguous dtype atol= e- rtol= e- test_attention_preserves_query_layout device test_attention permute_order list list int BHSqD = BHSkvD = shape_q = BHSqD idx idx permute_order shape_kv = BHSkvD idx idx permute_order reverse = permute_order index idx idx range q = torch randn shape_q dtype=torch bfloat device=device requires_grad=False permute reverse k = torch randn shape_kv dtype=torch bfloat device=device requires_grad=False permute reverse v = torch randn shape_kv dtype=torch bfloat device=device requires_grad=False permute reverse assertEqual q shape BHSqD assertEqual k shape BHSkvD assertEqual v shape BHSkvD out = F scaled_dot_product_attention q k v assertTrue out permute permute_order is_contiguous permutable = permute_orders = itertools permutations permutable permute_order permute_orders test_attention list permute_order + test_backends_set_to_math device dtype = torch bfloat q_shape = SdpaShape kv_shape = SdpaShape make_q = partial torch rand q_shape device=device dtype=dtype make_kv = partial torch rand kv_shape device=device dtype=dtype q k v = make_q make_kv make_kv sdpa_kernel backends= SDPBackend MATH assertTrue torch _C _get_math_sdp_enabled assertFalse torch _C _get_overrideable_sdp_enabled _ = F scaled_dot_product_attention q k v test_default_priority_order device The default priority order xpu overridable math flash efficient cudnn For xpu backend we need make sure overridable math flash dtype = torch bfloat shape = SdpaShape make_tensor = partial torch rand shape device=device dtype=dtype t = make_tensor run sdp_choice make sure priority_order set XPU default priority_order torch _fused_sdp_choice t t t torch nn attention _cur_sdpa_kernel_backends default_priority = _cur_sdpa_kernel_backends with_priority=True flash_index = default_priority index SDPBackend FLASH_ATTENTION overrideable_index = default_priority index SDPBackend OVERRIDEABLE math_index = default_priority index SDPBackend MATH assertTrue overrideable_index math_index flash_index f Expected overrideable math flash got overrideable_index math_index flash_index test_scaled_dot_product_attention_fused_kernels_safe_softmax device dtype = torch bfloat make_tensor = partial torch rand device=device dtype=dtype requires_grad=False batch num_heads seqlen head_dim = q_shape = SdpaShape batch num_heads seqlen head_dim k_shape = SdpaShape batch num_heads seqlen head_dim v_shape = SdpaShape batch num_heads seqlen head_dim query key value = make_tensor q_shape make_tensor k_shape make_tensor v_shape attn_mask = torch full seqlen seqlen float -inf device=device dtype=torch bfloat actual = F scaled_dot_product_attention query key value attn_mask=attn_mask dropout_p= is_causal=False math_ref = torch ops aten _scaled_dot_product_attention_math query float key float value float attn_mask=attn_mask dropout_p= is_causal=False assertEqual actual contiguous math_ref contiguous dtype atol= e- rtol= e- parametrize type dense parametrize is_contiguous True False test_scaled_dot_product_attention_fused_kernels_packed device type str is_contiguous bool make_tensor = partial rand_sdpa_tensor type=type device=device dtype=torch float packed=True batch_size seq_len num_heads head_dim = shape = SdpaShape batch_size num_heads seq_len head_dim Test Packed qkv = make_tensor shape query key value = qkv chunk dim=- query = query view batch_size - num_heads head_dim transpose value = value view batch_size - num_heads head_dim transpose key = key view batch_size - num_heads head_dim transpose is_contiguous query = query contiguous key = key contiguous value = value contiguous sdpa_kernel backends= SDPBackend OVERRIDEABLE actual = torch nn functional scaled_dot_product_attention query key value attn_mask=None dropout_p= is_causal=False math_ref = torch ops aten _scaled_dot_product_attention_math query contiguous key contiguous value contiguous attn_mask=None dropout_p= is_causal=False assertEqual actual contiguous math_ref contiguous atol= e- rtol= e- parametrize fused_kernel SDPBackend MATH SDPBackend OVERRIDEABLE parametrize dtype torch half torch bfloat torch float parametrize batch_size n_head q_size kv_size head_dim parametrize mask_type float causal parametrize train False test_scaled_dot_product_fused_attention_mask_vs_math device fused_kernel dtype batch_size q_size kv_size n_head head_dim mask_type train Migrate TestSDPACpuOnly tol = Tolerances e- e- dtype torch bfloat tol = Tolerances e- e- dtype torch float tol = Tolerances e- e- mask_shape = batch_size kv_size make_tensor = partial rand_sdpa_tensor type= dense device=device dtype=dtype requires_grad=False q_shape = SdpaShape batch_size n_head q_size head_dim kv_shape = SdpaShape batch_size n_head kv_size head_dim q = make_tensor q_shape k = make_tensor kv_shape v = make_tensor kv_shape q k v = q clone k clone v clone train q requires_grad_ True k requires_grad_ True v requires_grad_ True q requires_grad_ True k requires_grad_ True v requires_grad_ True B nh T hs q = q view batch_size q_size n_head head_dim transpose k = k view batch_size kv_size n_head head_dim transpose v = v view batch_size kv_size n_head head_dim transpose attn_mask = None is_causal = False mask_type == bool attn_mask = torch randint size=mask_shape dtype=torch bool device=device mask_type == float attn_mask = torch randn mask_shape dtype=dtype device=device mask_type == causal is_causal = True q k v = q float k float v float q = q view batch_size q_size n_head head_dim transpose k = k view batch_size kv_size n_head head_dim transpose v = v view batch_size kv_size n_head head_dim transpose attn_mask = attn_mask float attn_mask None None fused_kernel == SDPBackend MATH actual = torch ops aten _scaled_dot_product_attention_math q k v attn_mask=attn_mask dropout_p= is_causal=is_causal fused_kernel == SDPBackend OVERRIDEABLE actual = torch ops aten _scaled_dot_product_fused_attention_overrideable q k v attn_bias=attn_mask dropout_p= is_causal=is_causal math_ref = torch ops aten _scaled_dot_product_attention_math q k v attn_mask=attn_mask dropout_p= is_causal=is_causal assertEqual actual float math_ref atol=tol atol rtol=tol rtol TestAttnBias NNTestCase run_test device make_q make_kv attn_bias=None forw_tolerances Optional Tolerances = None grad_tolerances Optional Tolerances = None backend=None causal_variant=None backend None torch _dynamo reset query key value = make_q make_kv make_kv query_prototype key_prototype value_prototype = query_key_value_clones query key value realized = attn_bias _materialize device attn_bias None None pytorch_output = scaled_dot_product_attention query key value attn_mask=realized dropout_p= is_causal=False sdpa_op = torch compile scaled_dot_product_attention backend=backend backend None scaled_dot_product_attention sdpa_output = sdpa_op query_prototype key_prototype value_prototype attn_mask=attn_bias dropout_p= is_causal=False scale=None dOut = torch randn_like pytorch_output pytorch_output backward dOut sdpa_output backward dOut Use default assert_close tolerances dtypes forw_tolerances None forw_tolerances = Tolerances atol=None rtol=None grad_tolerances None grad_tolerances = Tolerances atol=None rtol=None torch testing assert_close pytorch_output sdpa_output rtol=forw_tolerances rtol atol=forw_tolerances atol torch testing assert_close query grad query_prototype grad rtol=grad_tolerances rtol atol=grad_tolerances atol torch testing assert_close key grad key_prototype grad rtol=grad_tolerances rtol atol=grad_tolerances atol torch testing assert_close value grad value_prototype grad rtol=grad_tolerances rtol atol=grad_tolerances atol parametrize causal_variant CausalVariant UPPER_LEFT CausalVariant LOWER_RIGHT parametrize shape test_causal_variants device causal_variant CausalVariant shape list tuple int make_tensor = partial torch rand device=device dtype=torch float requires_grad=True bsz num_heads seq_len_q seq_len_kv head_dim = shape make_q_tensor = partial make_tensor SdpaShape bsz num_heads seq_len_q head_dim make_kv_tensor = partial make_tensor SdpaShape bsz num_heads seq_len_kv head_dim causal_variant == CausalVariant LOWER_RIGHT seq_len_q seq_len_kv skipTest Lower right causal mask will produce NaNs output when seq_len_q seq_len_kv forw_tol = Tolerances e- e- grad_tol = Tolerances e- e- causal_variant == CausalVariant UPPER_LEFT attn_bias = causal_upper_left seq_len_q seq_len_kv attn_bias = causal_lower_right seq_len_q seq_len_kv sdpa_kernel backends= SDPBackend EFFICIENT_ATTENTION SDPBackend FLASH_ATTENTION SDPBackend MATH SDPBackend CUDNN_ATTENTION run_test device make_q_tensor make_kv_tensor attn_bias forw_tol grad_tol backend=None parametrize causal_variant CausalVariant UPPER_LEFT CausalVariant LOWER_RIGHT parametrize shape unittest skipIf IS_WINDOWS torch compile supported windows skipIfTorchDynamo This function already calls torch compile test_causal_variants_compile device causal_variant CausalVariant shape list tuple int cnts = CompileCounterWithBackend aot_eager make_tensor = partial torch rand device=device dtype=torch float requires_grad=True bsz num_heads seq_len_q seq_len_kv head_dim = shape make_q_tensor = partial make_tensor SdpaShape bsz num_heads seq_len_q head_dim make_kv_tensor = partial make_tensor SdpaShape bsz num_heads seq_len_kv head_dim causal_variant == CausalVariant LOWER_RIGHT seq_len_q seq_len_kv skipTest Lower right causal mask will produce NaNs output when seq_len_q seq_len_kv forw_tol = Tolerances e- e- grad_tol = Tolerances e- e- causal_variant == CausalVariant UPPER_LEFT attn_bias = causal_upper_left seq_len_q seq_len_kv attn_bias = causal_lower_right seq_len_q seq_len_kv sdpa_kernel backends= SDPBackend EFFICIENT_ATTENTION SDPBackend FLASH_ATTENTION SDPBackend MATH SDPBackend CUDNN_ATTENTION run_test device make_q_tensor make_kv_tensor attn_bias forw_tol grad_tol backend=cnts assertEqual cnts frame_count Compiled graph should have frame parametrize shape test_is_causal_equals_upper_left device shape list tuple int make_tensor = partial torch rand device=device dtype=torch float requires_grad=True bsz num_heads seq_len_q seq_len_kv head_dim = shape make_q_tensor = partial make_tensor SdpaShape bsz num_heads seq_len_q head_dim make_kv_tensor = partial make_tensor SdpaShape bsz num_heads seq_len_kv head_dim forw_tol = Tolerances e- e- query = make_q_tensor key = make_kv_tensor value = make_kv_tensor attn_bias = causal_upper_left seq_len_q seq_len_kv out_attn_bias = scaled_dot_product_attention query key value attn_mask=attn_bias dropout_p= out_is_causal = scaled_dot_product_attention query key value is_causal=True dropout_p= torch testing assert_close out_attn_bias out_is_causal rtol=forw_tol rtol atol=forw_tol atol test_is_causal_and_mask_fails device make_tensor = partial torch rand device=device dtype=torch float requires_grad=True make_q_tensor = partial make_tensor SdpaShape make_kv_tensor = partial make_tensor SdpaShape query = make_q_tensor key = make_kv_tensor value = make_kv_tensor attn_bias = causal_upper_left assertRaisesRegex ValueError CausalBias should used causal=True scaled_dot_product_attention query key value attn_mask=attn_bias is_causal=True dropout_p= NOTEST_CPU device_types = cuda mps device_types = cpu cuda mps TEST_XPU device_types += xpu instantiate_device_type_tests TestTransformers globals only_for=device_types instantiate_device_type_tests TestSDPAFailureModes globals only_for=device_types allow_mps=True instantiate_device_type_tests TestSDPA globals only_for=device_types allow_mps=True allow_xpu=True instantiate_device_type_tests TestSDPACudaOnly globals only_for= cuda instantiate_device_type_tests TestSDPACpuOnly globals only_for= cpu instantiate_device_type_tests TestAttnBias globals only_for=device_types allow_xpu=True instantiate_device_type_tests TestSDPAXpuOnly globals only_for= xpu allow_xpu=True __name__ == __main__ run_tests