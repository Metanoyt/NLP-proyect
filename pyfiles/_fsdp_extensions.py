abc ABC abstractmethod typing Any Optional torch torch distributed dist torch distributed _shard sharded_tensor api ShardedTensor torch distributed _shard sharded_tensor shard Shard torch distributed fsdp _shard_utils _all_gather_dtensor _create_chunk_dtensor _create_chunk_sharded_tensor torch distributed tensor DeviceMesh DTensor FSDPExtensions ABC This enables some customizable hooks enable composability tensor parallelism To activate these hooks use func ` _set_fsdp_extensions ` set custom ` FSDPExtensions ` implements hooks abstractmethod pre_flatten_transform tensor torch Tensor - tuple torch Tensor Optional Any E g converting ` ` DistributedTensor ` ` local tensor abstractmethod post_unflatten_transform tensor torch Tensor param_extension Any - torch Tensor E g converting local tensor ` ` DistributedTensor ` ` abstractmethod chunk_tensor tensor torch Tensor rank int world_size int num_devices_per_node int pg dist ProcessGroup device Optional torch device = None - torch Tensor Shards tensor chunks returns local chunk abstractmethod chunk_dtensor tensor torch Tensor rank int device_mesh DeviceMesh - torch Tensor Shards tensor DTensor DTensor returns local DTensor abstractmethod pre_load_state_dict_transform tensor torch Tensor - tuple torch Tensor list Shard This called before loading sharded model state dict should tensor list shards which load data abstractmethod all_gather_dtensor tensor DTensor parent_mesh Optional DeviceMesh - torch Tensor This called before loading sharded DTensor state dict This gathers tensor FSDP dimension returns local tensor TP DTensor _extensions Optional FSDPExtensions = None _set_fsdp_extensions flattener FSDPExtensions - None global _extensions _extensions = flattener _ext_pre_flatten_transform tensor torch Tensor fsdp_extension Optional FSDPExtensions = None - tuple torch Tensor Optional Any fsdp_extension None new_tensor param_extension = fsdp_extension pre_flatten_transform tensor param_extension None new_tensor param_extension tensor None _ext_post_unflatten_transform tensor torch Tensor param_extension Any fsdp_extension Optional FSDPExtensions = None - torch Tensor fsdp_extension None param_extension None fsdp_extension post_unflatten_transform tensor param_extension tensor _ext_chunk_tensor tensor torch Tensor rank int world_size int num_devices_per_node int pg dist ProcessGroup fsdp_extension Optional FSDPExtensions = None - torch Tensor chunk_tensor_fn = fsdp_extension chunk_tensor fsdp_extension None _create_chunk_sharded_tensor chunk_tensor_fn tensor rank world_size num_devices_per_node pg _ext_chunk_dtensor tensor torch Tensor rank int device_mesh DeviceMesh fsdp_extension Optional FSDPExtensions = None - torch Tensor chunk_dtensor_fn = fsdp_extension chunk_dtensor fsdp_extension None _create_chunk_dtensor chunk_dtensor_fn tensor rank device_mesh _ext_pre_load_state_dict_transform tensor torch Tensor fsdp_extension Optional FSDPExtensions = None - tuple torch Tensor list Shard fsdp_extension None fsdp_extension pre_load_state_dict_transform tensor type tensor ShardedTensor raise AssertionError f Expected ShardedTensor got type tensor shards = tensor local_shards tensor shards _ext_all_gather_dtensor tensor DTensor parent_mesh Optional DeviceMesh fsdp_extension Optional FSDPExtensions = None - torch Tensor all_gather_dtensor_fn = fsdp_extension all_gather_dtensor fsdp_extension None _all_gather_dtensor all_gather_dtensor_fn tensor parent_mesh