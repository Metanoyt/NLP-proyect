copy operator typing Any Optional TYPE_CHECKING torch torch ao quantization default_weight_fake_quant default_weight_observer FakeQuantizeBase QConfig QConfigMapping torch ao quantization backend_config BackendConfig torch ao quantization observer _PartialWrapper torch ao quantization quantize_fx convert_to_reference_fx prepare_fx TYPE_CHECKING collections abc Callable TODO move all LSTM util functions fx utils py file _get_lstm_with_individually_observed_parts float_lstm torch nn LSTM example_inputs tuple Any backend_config Optional BackendConfig = None linear_output_obs_ctr Optional _PartialWrapper = None sigmoid_obs_ctr Optional _PartialWrapper = None tanh_obs_ctr Optional _PartialWrapper = None cell_state_obs_ctr Optional _PartialWrapper = None hidden_state_obs_ctr Optional _PartialWrapper = None split_gates bool = False - torch ao nn quantizable LSTM Return observed ` torch ao nn quantizable LSTM ` created ` torch nn LSTM ` specific observers fake quantizes assigned inner ops submodules In both eager FX graph mode quantization ` torch ao nn quantizable LSTM ` used observed custom module which responsible inserting its own observers By default all inner ops inherit parent custom module s QConfig Users who wish override behavior may extend ` torch ao nn quantizable LSTM ` use helper function customize observer insertion logic This meant used convert float module observed module custom module flow Args ` float_lstm ` The float LSTM module ` example_inputs ` example inputs forward function LSTM module ` backend_config ` BackendConfig use observe LSTM module ` linear_output_obs_ctr ` observer fake quantize linear outputs Wx + b where W weight matrix b bias x either inputs hidden state previous layer any ` sigmoid_obs_ctr ` observer fake quantize sigmoid activations ` tanh_obs_ctr ` observer fake quantize tanh activations ` cell_state_obs_ctr ` observer fake quantize cell state ` hidden_state_obs_ctr ` observer fake quantize hidden state output Return A ` torch ao nn quantizable LSTM ` specified observers fake quantizes assigned inner ops make_qconfig obs_ctr _PartialWrapper - QConfig Make QConfig fixed qparams observers fake quantizes isinstance obs_ctr FakeQuantizeBase weight = default_weight_fake_quant weight = default_weight_observer QConfig activation=obs_ctr weight=weight quantizable_lstm = torch ao nn quantizable LSTM float_lstm input_size float_lstm hidden_size float_lstm num_layers float_lstm bias float_lstm batch_first float_lstm dropout float_lstm bidirectional split_gates=split_gates quantizable_lstm qconfig = float_lstm qconfig idx range float_lstm num_layers quantizable_lstm layers idx = torch ao nn quantizable modules rnn _LSTMLayer from_float float_lstm idx float_lstm qconfig batch_first=False split_gates=split_gates Build QConfigMapping LSTM cell Note FloatFunctional qconfigs will configured separately below cell_qm = QConfigMapping set_global float_lstm qconfig type ignore arg-type sigmoid_obs_ctr None cell_qm set_module_name input_gate make_qconfig sigmoid_obs_ctr cell_qm set_module_name forget_gate make_qconfig sigmoid_obs_ctr cell_qm set_module_name output_gate make_qconfig sigmoid_obs_ctr tanh_obs_ctr None cell_qm set_module_name cell_gate make_qconfig tanh_obs_ctr Insert observers into each LSTM cell TODO maybe make work layer_bw well layer quantizable_lstm layers cell = layer layer_fw cell type ignore union-attr isinstance cell torch nn Module raise AssertionError cell should nn Module cell = prepare_fx cell cell_qm example_inputs backend_config=backend_config HACK Manually replace activation_post_process following these ops This needed FloatFunctional ops because there currently no way configure these ops FX graph mode quantization today This because FloatFunctional modules simply disappear graph after tracing In future we should rewrite quantizable LSTM without FloatFunctionals split_gates op_index_to_activation_post_process_ctr = torch add linear_output_obs_ctr gates add torch mul cell_state_obs_ctr fgate_cx mul torch mul cell_state_obs_ctr igate_cgate mul torch add cell_state_obs_ctr fgate_cx_igate_cgate add torch mul hidden_state_obs_ctr ogate_cy mul op_index_to_activation_post_process_ctr = torch add linear_output_obs_ctr gates add input torch add linear_output_obs_ctr gates add forget torch add linear_output_obs_ctr gates add cell torch add linear_output_obs_ctr gates add output torch mul cell_state_obs_ctr fgate_cx mul torch mul cell_state_obs_ctr igate_cgate mul torch add cell_state_obs_ctr fgate_cx_igate_cgate add torch mul hidden_state_obs_ctr ogate_cy mul add_count = mul_count = node cell graph nodes op_index Optional tuple Callable int = None e g torch add node target torch add op_index = torch add add_count add_count += node target torch mul op_index = torch mul mul_count mul_count += Neither torch add nor torch mul continue op_index op_index_to_activation_post_process_ctr continue len node users = raise AssertionError expected exactly one user node activation_post_process_name = next iter node users keys name activation_post_process_ctr = op_index_to_activation_post_process_ctr op_index activation_post_process_ctr None setattr cell activation_post_process_name activation_post_process_ctr layer layer_fw cell = cell type ignore union-attr quantizable_lstm _get_reference_quantized_lstm_module observed_lstm torch ao nn quantizable LSTM backend_config Optional BackendConfig = None - torch ao nn quantized LSTM Return ` torch ao nn quantized LSTM ` created ` torch ao nn quantizable LSTM ` observers fake quantizes inserted through ` prepare_fx ` e g ` _get_lstm_with_individually_observed_parts ` This meant used convert observed module quantized module custom module flow Args ` observed_lstm ` ` torch ao nn quantizable LSTM ` observed through ` prepare_fx ` ` backend_config ` BackendConfig use produce reference quantized model Return A reference ` torch ao nn quantized LSTM ` module quantized_lstm = torch ao nn quantized LSTM observed_lstm input_size observed_lstm hidden_size observed_lstm num_layers observed_lstm bias observed_lstm batch_first observed_lstm dropout observed_lstm bidirectional i layer enumerate quantized_lstm layers cell = copy deepcopy observed_lstm layers get_submodule str i layer_fw cell type ignore union-attr cell = convert_to_reference_fx cell backend_config=backend_config type ignore arg-type isinstance cell torch fx GraphModule raise AssertionError cell must converted torch fx GraphModule HACK Manually remove input quantize nodes output dequantize nodes since custom modules expect quint inputs outputs now Note functionality supposedly handled through PrepareCustomConfig s ` set_input_quantized_indexes ` ` set_output_quantized_indexes ` API doesn t currently handle tuple inputs outputs so we have do manually now In future we should relax restriction custom module input output dtypes expand support complex input output structures node cell graph nodes node target torch quantize_per_tensor arg = node args Remove quantize x quantize hidden quantize hidden arg target == x arg target operator getitem arg args target == hidden cell graph inserting_before node node replace_all_uses_with arg cell graph erase_node node node target == output Remove all dequantize nodes output tuple arg node args cell graph inserting_before node node replace_input_with arg arg args cell graph eliminate_dead_code cell recompile layer layer_fw cell = cell type ignore union-attr quantized_lstm