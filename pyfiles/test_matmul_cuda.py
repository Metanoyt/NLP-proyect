Owner s module linear algebra contextlib time unittest itertools product functools partial typing Callable torch torch quantization _quantized_conversions pack_int _to_int quantized_weight_reorder_for_mixed_dtypes_linear_cutlass torch testing make_tensor torch testing _internal common_cuda PLATFORM_SUPPORTS_BF PLATFORM_SUPPORTS_GREEN_CONTEXT SM OrLater SM OrLater SM OrLater SM OrLater _get_torch_cuda_version torch testing _internal common_device_type dtypes instantiate_device_type_tests onlyCUDA tol xtol toleranceOverride torch testing _internal common_utils IS_JETSON IS_WINDOWS MI _ARCH NAVI_ARCH getRocmVersion isRocmArchAnyOf parametrize run_tests runOnRocmArch serialTest skipIfRocm TEST_CUDA TEST_WITH_ROCM TestCase decorateIf torch testing _internal inductor_utils IS_BIG_GPU torch _inductor test_case TestCase InductorTestCase _IS_SM X = False TEST_CUDA _IS_SM X = torch cuda get_device_capability == Protects against includes accidentally setting default dtype assert torch get_default_dtype torch float xfailIfSM OrLaterNonRTXAndCondition condition_fn Conditionally xfail tests SM + datacenter SKUs based condition function The condition function receives test parameters dict returns True xfail computeCapabilityCheck = SM OrLater torch cuda get_device_capability = decorateIf unittest expectedFailure lambda params computeCapabilityCheck condition_fn params contextlib contextmanager blas_library_context backend prev_backend = torch backends cuda preferred_blas_library torch backends cuda preferred_blas_library backend try yield finally torch backends cuda preferred_blas_library prev_backend TestMatmulCuda InductorTestCase setUp super setUp torch backends cuda matmul allow_tf = False tearDown torch backends cuda matmul allow_tf = True super tearDown cublas_addmm size int dtype torch dtype reduced_precision bool = False fp _accumulate bool = False bias_shape_modifier Callable &#124; None = None Check catastrophic cuBLAS inaccuracy measuring deviation between results CUDA invocation torch addmm CPU invocation which does use CUDA backend Get dims m k n = size + size size + Disable reduced precision reductions BFloat bypass some kernels which fail threshold check orig_bf = torch backends cuda matmul allow_bf _reduced_precision_reduction orig_fp = torch backends cuda matmul allow_fp _reduced_precision_reduction orig_fp _accumulate = torch backends cuda matmul allow_fp _accumulation torch backends cuda matmul allow_bf _reduced_precision_reduction = reduced_precision torch backends cuda matmul allow_fp _reduced_precision_reduction = reduced_precision torch backends cuda matmul allow_fp _accumulation = fp _accumulate Make random tensors CPU seed set common_utils py Not using numpy because does support bfloat make_arg = partial make_tensor dtype=dtype device= cpu bias_shape_modifier = lambda shape shape bias_shape_modifier None bias_shape_modifier m_input = make_arg bias_shape_modifier m n m_ = make_arg m k m_ = make_arg k n m_beta = make_arg scale abate overflows fp accum fp _accumulate m_ = m_ m_ = m_ B FLOAT Special Handling Backend does tensorize float CPU bloat may present accuracy issues so convert float these cases keep same other types e g float int dtype == torch float dtype == torch bfloat m_beta = m_beta dtype=torch float m_input = m_input dtype=torch float m_ = m_ dtype=torch float m_ = m_ dtype=torch float Get CPU result res_cpu = torch addmm m_input m_ m_ beta=m_beta item B FLOAT Special Handling ` ` Convert back b float dtype == torch float dtype == torch bfloat m_beta = m_beta dtype=dtype m_input = m_input dtype=dtype m_ = m_ dtype=dtype m_ = m_ dtype=dtype res_cpu = res_cpu dtype=dtype Move arg tensors CUDA m_beta = m_beta cuda m_input = m_input cuda m_ = m_ cuda m_ = m_ cuda Get CUDA result res_cuda = torch addmm m_input m_ m_ beta=m_beta item Move CPU comparison res_cuda = res_cuda cpu Compare assertEqual res_cpu res_cuda torch backends cuda matmul allow_bf _reduced_precision_reduction = orig_bf torch backends cuda matmul allow_fp _reduced_precision_reduction = orig_fp torch backends cuda matmul allow_fp _accumulation = orig_fp _accumulate onlyCUDA imported tol xtol avoid aliasing code above toleranceOverride torch float xtol atol= e- rtol= e- torch bfloat xtol atol= e- rtol= e- torch float xtol atol= e- rtol= e- dtypes torch float torch bfloat torch float parametrize size parametrize backend cublas cublaslt test_cublas_addmm size int dtype torch dtype backend blas_library_context backend TEST_WITH_ROCM backend == cublas isRocmArchAnyOf NAVI_ARCH getRocmVersion dtype == torch float size = skipTest f failed Navi ROCm due hipblas backend dtype= dtype size= size cublas_addmm size dtype False onlyCUDA xfailIfSM OrLaterNonRTXAndCondition lambda params params get dtype == torch bfloat params get size == imported tol xtol avoid aliasing code above toleranceOverride torch float xtol atol= e- rtol= e- torch bfloat xtol atol= e rtol= e- dtypes torch float torch bfloat parametrize size parametrize backend cublas cublaslt test_cublas_addmm_reduced_precision size int dtype torch dtype backend blas_library_context backend cublas_addmm size dtype True onlyCUDA imported tol xtol avoid aliasing code above toleranceOverride torch float xtol atol= e- rtol= e- torch bfloat xtol atol= e- rtol= e- torch float xtol atol= e- rtol= e- dtypes torch bfloat torch float torch float parametrize size parametrize backend cublas cublaslt test_cublas_addmm_bias_shapes size int dtype torch dtype backend blas_library_context backend D bias cublas_addmm size dtype bias_shape_modifier=lambda shape shape D bias which row-broadcast D cublas_addmm size dtype bias_shape_modifier=lambda shape shape - D bias which row-broadcasts cublas_addmm size dtype bias_shape_modifier=lambda shape shape - onlyCUDA dtypes torch float m == chooses OUTPUT_TYPE reduction H m == chooses OUTPUT_TYPE reduction A parametrize small_size parametrize size parametrize backend cublaslt cublas test_cublas_addmm_no_reduced_precision small_size int size int dtype torch dtype backend blas_library_context backend torch backends cuda preferred_blas_library backend orig_precision = torch backends cuda matmul allow_fp _reduced_precision_reduction torch backends cuda matmul allow_fp _reduced_precision_reduction = False m = torch full small_size size dtype=dtype device= cuda m = torch ones size small_size dtype=dtype device= cuda m size = - b = torch zeros small_size dtype=dtype device= cuda out = torch addmm b m m beta= assertEqual out sum item torch backends cuda matmul allow_fp _reduced_precision_reduction = orig_precision onlyCUDA imported tol xtol avoid aliasing code above toleranceOverride torch float xtol atol= e- rtol= e- torch bfloat xtol atol= e rtol= e- dtypes torch float torch bfloat parametrize size parametrize backend cublas cublaslt test_cublas_addmm_reduced_precision_fp _accumulate size int dtype torch dtype backend blas_library_context backend cublas_addmm size dtype False True onlyCUDA test_cublas_and_lt_reduced_precision_fp _accumulate orig_fp _accumulate = torch backends cuda matmul allow_fp _accumulation torch backends cuda matmul allow_fp _accumulation = True x = torch rand device= cuda dtype=torch half w = torch rand device= cuda dtype=torch half b = torch rand device= cuda dtype=torch half out = torch nn functional linear x w b out_cpu = torch nn functional linear x cpu w cpu b cpu assertEqual out out_cpu atol= e- rtol= e- = torch rand device= cuda dtype=torch half b = torch rand device= cuda dtype=torch half c = torch rand device= cuda dtype=torch half out = torch baddbmm b c out_cpu = torch baddbmm cpu b cpu c cpu assertEqual out out_cpu atol= e- rtol= e- torch backends cuda matmul allow_fp _accumulation = orig_fp _accumulate onlyCUDA toleranceOverride torch float xtol atol= e- rtol= e- dtypes torch float test_cublas_addmm_alignment dtype device = cuda perturb X A B alignment idx range offset range offsets = offsets idx = offset x_offset a_offset b_offset = offsets A = torch rand + a_offset requires_grad=True dtype=dtype device=device A = A a_offset reshape X = torch rand + x_offset requires_grad=True dtype=dtype device=device X = X x_offset reshape B = torch rand + b_offset requires_grad=True dtype=dtype device=device B = B b_offset reshape out = torch nn functional linear X A B assertEqual out torch matmul X A transpose + B onlyCUDA unittest skipIf IS_JETSON Too large Jetson toleranceOverride torch float xtol atol= e- rtol= e- dtypes torch float torch float + torch bfloat TEST_WITH_ROCM SM OrLater parametrize batch_size N M P name_fn=lambda batch_size N M P f batch_size _ N _ M _ P test_cublas_baddbmm_large_input device batch_size N M P dtype cpu_dtype = dtype dtype == torch float dtype == torch bfloat cpu_dtype = torch float M = torch rand N M device=device dtype=dtype M = torch rand M P device=device dtype=dtype A = torch rand N P device=device dtype=dtype _convert_to_cpu t t device= cpu dtype=cpu_dtype M _cpu M _cpu A_cpu = map _convert_to_cpu M M A linear out _cpu = torch nn functional linear M _cpu M _cpu t A_cpu dtype=dtype out _gpu = torch nn functional linear M M t A cpu assertEqual out _cpu out _gpu test multiply identity matrix N == M M == P M _eye = torch eye N device=device dtype=dtype out _eye_gpu = torch nn functional linear M M _eye t torch zeros_like A runOnRocmArch MI _ARCH dtype == torch float assertEqual M _cpu dtype=dtype out _eye_gpu cpu atol= e- rtol= assertEqual M _cpu dtype=dtype out _eye_gpu cpu baddbmm _expand_to_batch t torch Tensor t expand batch_size + t size alpha beta = M M A M _cpu M _cpu A_cpu = map _expand_to_batch M M A M _cpu M _cpu A_cpu out _cpu = torch baddbmm A_cpu M _cpu M _cpu beta=beta alpha=alpha dtype=dtype out _gpu = torch baddbmm A M M beta=beta alpha=alpha cpu assertEqual out _cpu out _gpu test multiply identity matrix N == M M == P M _eye = torch eye N device=device dtype=dtype expand batch_size N N out _eye_gpu = torch baddbmm torch zeros_like A M M _eye beta=beta alpha=alpha runOnRocmArch MI _ARCH dtype == torch float assertEqual M _cpu dtype=dtype out _eye_gpu cpu atol= e- rtol= assertEqual M _cpu dtype=dtype out _eye_gpu cpu cross comparison assertEqual out _gpu out _gpu onlyCUDA skipIfRocm parametrize shape i i range dtypes torch float torch half torch bfloat test_cublas_deterministic device shape dtype inp = torch randn shape shape device=device dtype=dtype first = torch matmul inp inp _ range assertEqual first torch matmul inp inp atol= rtol= grouped_mm_helper alist blist gOlist agradlist bgradlist outlist b gO agrad bgrad out zip alist blist gOlist agradlist bgradlist outlist = clone detach requires_grad_ b = b clone detach requires_grad_ out_ref = torch mm b t out_ref backward gO assertEqual out out_ref agrad None assertEqual agrad grad assertEqual bgrad b grad unittest skipIf SM OrLater Grouped gemm supported only SM greater parametrize strided False True parametrize a_row_major False True parametrize b_row_major False True dtypes torch bfloat torch float torch float test_grouped_gemm_ d_ d strided a_row_major b_row_major dtype device = cuda m n k n_groups = a_row_major = torch randn m k n_groups + k int strided device=device dtype=dtype k n_groups = torch randn k n_groups + k int strided m device=device dtype=dtype t k n_groups b_row_major b = torch randn n k n_groups + k int strided device=device dtype=dtype k n_groups b = torch randn k n_groups + k int strided n device=device dtype=dtype t k n_groups requires_grad_ True b requires_grad_ True offs = torch arange k n_groups k + k device=device dtype=torch int f = torch _grouped_mm out = f b t offs=offs out_dtype=dtype gO = torch rand_like out out backward gO offs_cpu = offs cpu alist blist agradlist bgradlist = start = i range n_groups alist append start offs_cpu i blist append b start offs_cpu i agradlist append grad start offs_cpu i bgradlist append b grad start offs_cpu i start = offs_cpu i grouped_mm_helper alist blist gO agradlist bgradlist out unittest skipIf SM OrLater Grouped gemm supported only SM greater parametrize strided False True parametrize a_row_major False True parametrize b_row_major False True dtypes torch bfloat torch float torch float test_grouped_gemm_ d_ d strided a_row_major b_row_major dtype device = cuda s_int = int strided m n k n_groups = a_row_major = torch randn m n_groups k + s_int device=device dtype=dtype k = torch randn k m + s_int n_groups device=device dtype=dtype t m n_groups b_row_major b = torch randn n_groups + s_int n k + s_int device=device dtype=dtype + s_int k b = torch randn n_groups + s_int k + s_int n device=device dtype=dtype transpose - - + s_int k requires_grad_ True b requires_grad_ True a_contig = a_row_major t assertTrue a_contig is_contiguous strided b_contig = b b_row_major b transpose - - assertTrue b_contig is_contiguous strided check_zero_size False True check_zero_size n_groups = continue grad = None b grad = None offs = torch arange m n_groups m + m device=device dtype=torch int check_zero_size offs = offs f = torch _grouped_mm out = f b transpose - - offs=offs out_dtype=dtype gO = torch rand_like out check_zero_size out backward gO offs_cpu = offs cpu alist agradlist gOlist outlist = bgradlist = None n_groups check_zero_size b grad start = i range n_groups alist append start offs_cpu i agradlist append None check_zero_size grad start offs_cpu i outlist append out start offs_cpu i gOlist append gO start offs_cpu i start = offs_cpu i grouped_mm_helper alist b gOlist agradlist bgradlist outlist unittest skipIf SM OrLater Grouped gemm supported only SM greater parametrize strided False True parametrize a_row_major False True parametrize b_row_major False True dtypes torch bfloat torch float torch float test_grouped_gemm_ d_ d strided a_row_major b_row_major dtype device = cuda s_int = int strided m n k n_groups = a_row_major = torch randn n_groups + s_int m k + s_int device=device dtype=dtype + s_int k = torch randn n_groups + s_int k + s_int m device=device dtype=dtype transpose - - + s_int k b_row_major b = torch randn n_groups + s_int n k + s_int device=device dtype=dtype + s_int k b = torch randn n_groups + s_int k + s_int n device=device dtype=dtype transpose - - + s_int k requires_grad_ True b requires_grad_ True a_contig = a_row_major transpose - - assertTrue a_contig is_contiguous strided b_contig = b b_row_major b transpose - - assertTrue b_contig is_contiguous strided f = torch _grouped_mm out = f b transpose - - out_dtype=dtype gO = torch rand_like out out backward gO grouped_mm_helper b gO grad b grad out unittest skipIf SM OrLater Grouped gemm supported only SM greater parametrize strided False True parametrize a_row_major False True parametrize b_row_major False True dtypes torch bfloat torch float torch float test_grouped_gemm_ d_ d strided a_row_major b_row_major dtype TEST_WITH_ROCM a_row_major b_row_major dtype torch bfloat torch float skipTest failed using hipblaslt rocm device = cuda s_int = int strided m n k n_groups = a_row_major = torch randn n_groups + s_int m k + s_int device=device dtype=dtype + s_int k = torch randn n_groups + s_int k + s_int m device=device dtype=dtype transpose - - + s_int k b_row_major b = torch randn n n_groups k + s_int device=device dtype=dtype k b = torch randn k n n_groups + s_int device=device dtype=dtype transpose - - n n_groups requires_grad_ True b requires_grad_ True a_contig = a_row_major transpose - - assertTrue a_contig is_contiguous strided b_contig = b b_row_major b transpose - - assertTrue b_contig is_contiguous strided check_zero_size False True check_zero_size n_groups = continue offs = torch arange n n_groups n + n device=device dtype=torch int check_zero_size offs = offs f = torch _grouped_mm out = f b transpose - - offs=offs out_dtype=dtype gO = torch rand_like out check_zero_size out backward gO offs_cpu = offs cpu blist outlist bgradlist gOlist = agradlist = None n_groups check_zero_size grad start = i range n_groups blist append b start offs_cpu i bgradlist append b grad start offs_cpu i outlist append out start offs_cpu i gOlist append gO start offs_cpu i start = offs_cpu i grouped_mm_helper blist gOlist agradlist bgradlist outlist unittest skipIf TEST_WITH_ROCM ROCm doesn t support CUTLASS TODO future PR enable compile torch _grouped_mm fallback path unittest skipIf SM OrLater Grouped gemm compile supported SM parametrize op d d d d d d d d parametrize a_row_major False True parametrize b_row_major False True parametrize max_autotune False True test_grouped_gemm_compiled op a_row_major b_row_major max_autotune device = cuda dtype_AB = torch bfloat dtype_offset = torch int align = dtype_AB itemsize f_ref = torch _grouped_mm options = max_autotune options update max_autotune True max_autotune_gemm_backends TRITON f = torch compile f_ref options=options op == d d m n = m_align = m + align - align align n_align = n + align - align align a_row_major b_row_major offs = torch tensor device=device dtype=dtype_offset offs = torch tensor device=device dtype=dtype_offset ngroups = offs shape k = offs - k_align = k + align - align align a_row_major A = torch randn m k_align device=device dtype=dtype_AB k A = torch randn k m_align device=device dtype=dtype_AB t m b_row_major B = torch randn n k_align device=device dtype=dtype_AB k B = torch randn k n_align device=device dtype=dtype_AB t n op == d d n k = k larger here validate iterating over k tiles op n_align = n + align - align align k_align = k + align - align align a_row_major offs = torch tensor device=device dtype=dtype_offset offs = torch tensor device=device dtype=dtype_offset ngroups = offs shape m = offs - m_align = m + align - align align a_row_major A = torch randn m k_align device=device dtype=dtype_AB k A = torch randn k m_align device=device dtype=dtype_AB t m b_row_major B = torch randn ngroups n k_align device=device dtype=dtype_AB k B = torch randn ngroups k n_align device=device dtype=dtype_AB transpose - - n op == d d m k = m_align = m + align - align align k_align = k + align - align align offs = torch tensor device=device dtype=dtype_offset ngroups = offs shape n = offs - n_align = n + align - align align a_row_major A = torch randn ngroups m k_align device=device dtype=dtype_AB k A = torch randn ngroups k m_align device=device dtype=dtype_AB transpose - - m b_row_major B = torch randn n k_align device=device dtype=dtype_AB k B = torch randn k n_align device=device dtype=dtype_AB t n op == d d offs = None ngroups = m n k = m_align = m + align - align align n_align = n + align - align align k_align = k + align - align align a_row_major A = torch randn ngroups m k_align device=device dtype=dtype_AB k A = torch randn ngroups k m_align device=device dtype=dtype_AB transpose - - m b_row_major B = torch randn ngroups n k_align device=device dtype=dtype_AB k B = torch randn ngroups k n_align device=device dtype=dtype_AB transpose - - n raise AssertionError f Invalid op op C_ref = f_ref A B transpose - - offs=offs IS_BIG_GPU max_autotune assertRaisesRegex torch _inductor exc InductorError NoValidChoicesError C = f A B transpose - - offs=offs C = f A B transpose - - offs=offs assertEqual C C_ref onlyCUDA parametrize input_dtype torch float torch float torch bfloat parametrize M parametrize N parametrize K parametrize batch_size None parametrize backend cublas cublaslt test_mm_bmm_dtype_overload input_dtype M N K batch_size backend torch version hip msg = accuracy regression hipblas hipblaslt ROCm certain shapes input_dtype == torch bfloat N == K == batch_size raise unittest SkipTest msg input_dtype == torch bfloat N == K == batch_size raise unittest SkipTest msg input_dtype == torch float M == N == K == batch_size == raise unittest SkipTest msg input_dtype == torch float M == N == K == batch_size == raise unittest SkipTest msg device = cuda dtype = input_dtype blas_library_context backend create_inputs B=None B None = torch randn M K device=device dtype=dtype b = torch randn K N device=device dtype=dtype = torch randn B M K device=device dtype=dtype b = torch randn B K N device=device dtype=dtype b b = create_inputs batch_size a_fp b_fp = torch float b torch float output_dtypes = torch float input_dtype = torch float output_dtypes append input_dtype output_dtype output_dtypes Catch edge case incompat bfloat major version input_dtype == torch bfloat PLATFORM_SUPPORTS_BF output_dtype == torch bfloat continue batch_size assertRaises RuntimeError torch bmm b out_dtype=output_dtype assertRaises RuntimeError torch mm b out_dtype=output_dtype batch_size out = torch bmm b out_dtype=output_dtype baseline = torch bmm a_fp b_fp output_dtype == torch float torch bmm b out = torch mm b out_dtype=output_dtype baseline = torch mm a_fp b_fp output_dtype == torch float torch mm b assertEqual out dtype output_dtype torch testing assert_close out baseline atol= e- rtol= e- onlyCUDA parametrize input_dtype torch float torch float torch bfloat parametrize M parametrize N parametrize K parametrize batch_size None parametrize backend cublas cublaslt test_addmm_baddmm_dtype_overload input_dtype M N K batch_size backend torch version hip msg = accuracy regression hipblas hipblaslt ROCm certain shapes input_dtype == torch bfloat N == K == batch_size raise unittest SkipTest msg input_dtype == torch bfloat N == K == batch_size raise unittest SkipTest msg input_dtype == torch float M == N == K == batch_size == raise unittest SkipTest msg input_dtype == torch float M == N == K == batch_size == raise unittest SkipTest msg device = cuda dtype = input_dtype blas_library_context backend create_inputs B=None B None = torch randn M K device=device dtype=dtype b = torch randn K N device=device dtype=dtype c = torch randn M N device=device dtype=dtype = torch randn B M K device=device dtype=dtype b = torch randn B K N device=device dtype=dtype c = torch randn B M N device=device dtype=dtype b c b c = create_inputs batch_size a_fp b_fp c_fp = torch float b torch float c torch float output_dtypes = torch float input_dtype = torch float output_dtypes append input_dtype output_dtype output_dtypes Catch edge case incompat bfloat major version input_dtype == torch bfloat PLATFORM_SUPPORTS_BF output_dtype == torch bfloat continue batch_size assertRaises RuntimeError torch baddbmm c b out_dtype=output_dtype assertRaises RuntimeError torch addmm c b out_dtype=output_dtype batch_size out = torch baddbmm c b out_dtype=output_dtype output_dtype == torch float baseline = torch baddbmm c_fp a_fp b_fp baseline = torch baddbmm c b out = torch addmm c b out_dtype=output_dtype output_dtype == torch float baseline = torch addmm c_fp a_fp b_fp baseline = torch addmm c b assertEqual out dtype output_dtype torch testing assert_close out baseline atol= e- rtol= e- onlyCUDA parametrize batch_size parametrize backend cublas cublaslt test_fp _accum_and_fp _out_failure batch_size backend M N K = device = cuda dtype = torch float blas_library_context backend torch backends cuda preferred_blas_library backend orig_fp _accum = torch backends cuda matmul allow_fp _accumulation torch backends cuda matmul allow_fp _accumulation = True create_inputs = torch randn M K device=device dtype=dtype b = torch randn K N device=device dtype=dtype c = torch randn M N device=device dtype=dtype b c expand tensor tensor unsqueeze expand batch_size tensor shape b c = create_inputs assertRaises Exception torch baddbmm expand c expand expand b out_dtype=torch float assertRaises Exception torch addmm c b out_dtype=torch float assertRaises Exception torch bmm expand expand b out_dtype=torch float assertRaises Exception torch mm b out_dtype=torch float torch backends cuda matmul allow_fp _accumulation = orig_fp _accum onlyCUDA parametrize ops mm torch mm bmm torch bmm addmm torch addmm baddbmm torch baddbmm test_input_dimension_checking_out_dtype ops op_name op = ops B = M N K = is_addmm add op_name is_batched bmm op_name is_batched = torch randn B M K device= cuda dtype=torch bfloat mismatch_k_b = torch randn B K + N device= cuda dtype=torch bfloat c = torch randn B M N device= cuda dtype=torch bfloat extra_dim_b = clone unsqueeze mismatch_k_err = Expected size first two dimensions batch tensor extra_dim_err = batch must D tensor = torch randn M K device= cuda dtype=torch bfloat mismatch_k_b = torch randn K + N device= cuda dtype=torch bfloat c = torch randn M N device= cuda dtype=torch bfloat extra_dim_b = clone unsqueeze mismatch_k_err = mat mat shapes cannot multiplied extra_dim_err = mat must matrix got -D tensor Test mismatch K assertRaisesRegex RuntimeError mismatch_k_err is_addmm op c mismatch_k_b out_dtype=torch float op mismatch_k_b out_dtype=torch float Test extra dimension assertRaisesRegex RuntimeError extra_dim_err is_addmm op c extra_dim_b out_dtype=torch float op c extra_dim_b out_dtype=torch float is_batched assertRaisesRegex RuntimeError Expected size first two dimensions batch tensor Test mismatch B bmm baddbmm mismatch_batch_dim_b = torch randn B + K N device= cuda dtype=torch bfloat is_addmm op c mismatch_batch_dim_b out_dtype=torch float op mismatch_batch_dim_b out_dtype=torch float unittest skipIf PLATFORM_SUPPORTS_GREEN_CONTEXT Green contexts supported serialTest test_greencontext_carveout = torch randn device= cuda dtype=torch bfloat ctx = torch cuda green_contexts GreenContext create ctx set_context torch matmul torch cuda synchronize t = time perf_counter partial_res = torch matmul torch cuda synchronize t = time perf_counter ctx pop_context torch matmul torch cuda synchronize t = time perf_counter full_res = torch matmul torch cuda synchronize t = time perf_counter assertEqual partial_res full_res assertGreater t - t t - t unittest skipIf TEST_WITH_ROCM ROCm doesn t support CUTLASS unittest skipIf IS_WINDOWS Windows doesn t support CUTLASS extensions unittest skipIf _IS_SM X mixed dtypes linear only supported SM x TestMixedDtypesLinearCuda TestCase dtypes torch float torch bfloat test_mixed_dtypes_linear dtype torch dtype device str = cuda version = _get_torch_cuda_version version skipTest _mixed_dtypes_linear only compiled CUDA + run_test batch_shape m n k add_bias activation dtype dtypeq device rtol atol add_bias activation = none val_lo val_hi = - valq_lo valq_hi = - input = make_tensor batch_shape m k low=val_lo high=val_hi dtype=dtype device=device weight = make_tensor n k low=valq_lo high=valq_hi dtype=torch int device=device scale = make_tensor n low=val_lo high=val_hi dtype=input dtype device=device bias = make_tensor n low=val_lo high=val_hi dtype=input dtype device=device add_bias None input_ref = input reshape - input shape - First test plain multiplication weight_ref = weight T input dtype scale view n weightq = pack_int _to_int weight T dtypeq == torch quint x weight T output_ref = torch mm input_ref weight_ref reshape input shape - n output = torch ops aten _mixed_dtypes_linear input quantized_weight_reorder_for_mixed_dtypes_linear_cutlass weightq dtypeq transpose=False scale torch testing assert_close output output_ref rtol=rtol atol=atol Second test linear operator itself weight_ref = weight input dtype scale view n weightq = pack_int _to_int weight dtypeq == torch quint x weight bias_ref = bias view n add_bias None output_ref = torch nn functional linear input_ref weight_ref bias=bias_ref reshape input shape - n activation == relu relu = torch nn ReLU output_ref = relu output_ref activation == silu silu = torch nn SiLU output_ref = silu output_ref output = torch ops aten _mixed_dtypes_linear input quantized_weight_reorder_for_mixed_dtypes_linear_cutlass weightq dtypeq transpose=True scale bias=bias activation=activation torch testing assert_close output output_ref rtol=rtol atol=atol dtypeqs = torch int torch quint x batch_shapes = shapes = activations = None relu silu rtol atol = e- e- dtype == torch bfloat rtol atol = e- e- dtypeq batch_shape m n k add_bias activation product dtypeqs batch_shapes shapes False True activations run_test batch_shape m n k add_bias activation dtype dtypeq device rtol atol instantiate_device_type_tests TestMatmulCuda globals except_for= cpu instantiate_device_type_tests TestMixedDtypesLinearCuda globals except_for= cpu __name__ == __main__ TestCase _default_dtype_check_enabled = True run_tests