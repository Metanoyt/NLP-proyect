operator_benchmark op_bench torch torch nn Microbenchmarks RNNs qrnn_configs = op_bench config_list attrs= names input_size hidden_size num_layers attr_names= I H NL cross_product_configs= B True Bias always True quantized D False True Bidirectional dtype torch qint Only qint dtype works now tags= short LSTMBenchmark op_bench TorchBenchmarkBase init I H NL B D dtype sequence_len = batch_size = The quantized dynamic LSTM has bug That s why we create regular LSTM quantize later See issue cell_nn = nn LSTM input_size=I hidden_size=H num_layers=NL bias=B batch_first=False dropout= bidirectional=D cell_temp = nn Sequential cell_nn cell = torch ao quantization quantize_dynamic cell_temp nn LSTM nn Linear dtype=dtype x = torch randn sequence_len sequence length batch_size batch size I Number features X h = torch randn NL D + layer_num dir_num batch_size batch size H hidden size c = torch randn NL D + layer_num dir_num batch_size batch size H hidden size inputs = x x h h c c set_module_name QLSTM forward x h c cell x h c op_bench generate_pt_test qrnn_configs LSTMBenchmark __name__ == __main__ op_bench benchmark_runner main