mypy allow-untyped-defs collections dataclasses io json operator os pickle queue threading uuid warnings abc ABC abstractmethod collections abc Callable Generator Iterable Iterator Sequence contextlib contextmanager dataclasses dataclass enum Enum io UnsupportedOperation pathlib Path typing Any cast Final IO Optional Union introduced collections abc Buffer Python typing_extensions Buffer torch torch Tensor torch _utils _get_available_device_type _get_device_module torch distributed _shard _utils narrow_tensor_by_index torch distributed checkpoint _extension ExtensionRegistry StreamTransformExtension torch distributed checkpoint _hf_utils CUSTOM_METADATA_KEY DCP_VERSION_KEY FORMAT_KEY FORMAT_VALUE HF_DCP_VERSION torch distributed checkpoint metadata Metadata STATE_DICT_TYPE StorageMeta torch distributed checkpoint planner LoadItemType LoadPlan LoadPlanner ReadItem SavePlan SavePlanner WriteItem WriteItemType torch distributed checkpoint staging BlockingAsyncStager torch distributed checkpoint storage StorageReader StorageWriter WriteResult torch distributed checkpoint utils _create_file_view torch futures Future __all__ = FileSystemWriter FileSystemReader FileSystem FileSystemBase SerializationFormat _metadata_fn str = metadata CURRENT_DCP_VERSION Final str = dataclass _StorageInfo This per entry storage info relative_path str offset int length int transform_descriptors Optional Sequence str = None __getstate__ k v k v __dict__ items v None dataclass _StoragePrefix prefix str SerializationFormat Enum TORCH_SAVE = torch_save SAFETENSORS = safetensors DEFAULT_SUFFIX = distcp _generate_uuid - str str uuid uuid _TensorLoader ABC abstractmethod add size int obj object - None pass abstractmethod start_loading - None pass abstractmethod values - Iterator tuple torch Tensor object pass _SerialCpuLoader _TensorLoader __init__ resolve_fun Callable - None resolve_fun = resolve_fun items list tuple int object = add size int obj object - None items append size obj start_loading - None pass values - Iterator tuple torch Tensor object _ obj items tensor = resolve_fun obj detach tensor = tensor cpu tensor storage size = tensor numel tensor = tensor clone yield tensor obj _OverlappingCpuLoader _TensorLoader __init__ resolve_fun Callable stream Optional torch Stream = None inflight_threshhold int = _ _ - None resolve_fun = resolve_fun items list tuple int object = inflight_threshhold = inflight_threshhold in_flight_data = current_items collections deque = collections deque idx = started = False device_type = stream device_type stream _get_available_device_type device_module = _get_device_module device_type stream = cast torch cuda Stream stream device_module current_stream stream = device_module current_stream stream wait_stream device_module current_stream property _done - bool idx = len items _drain - list tuple torch Tensor object drained = in_flight_data = inflight_threshhold stream synchronize while in_flight_data = inflight_threshhold val = current_items popleft in_flight_data -= val numel val element_size drained append val drained _refill - None device_module stream stream while _done in_flight_data inflight_threshhold _ obj = items idx idx += tensor = resolve_fun obj detach tensor device type == device_type tensor = tensor device= cpu non_blocking=True tensor device == torch device cpu tensor untyped_storage size = tensor numel tensor itemsize forces tensor both contiguous minimal storage tensor = tensor clone current_items append tensor obj in_flight_data += tensor numel tensor element_size _finish - Iterable tuple torch Tensor object _done raise AssertionError _finish called before all items processed len current_items stream synchronize current_items add size int obj object - None started raise RuntimeError cannot add items after loading started items append size obj start_loading - None started started = True items sort key=operator itemgetter _refill values - Iterator tuple torch Tensor object start_loading while _done drained = _drain _refill yield drained yield _finish _StorageWriterTransforms This experimental will likely move elsewhere future It lives here minimize changes while we still learning gathering feedback __init__ extensions Optional Sequence StreamTransformExtension = None - None If extensions arg None means implementation should provide whatever defaults chooses An empty sequence indicates no extensions should used At time default extensions sequence empty extensions = extensions None extensions transform_save_stream write_item WriteItem raw_stream io IOBase - tuple IO bytes list str In order avoid leaking fds transformers close must cascade wrapped streams since function can append raw stream we can t close actual stream So we use put wrapper around raw stream s close make noop gets closed once all files appended NoCloseWriter io IOBase __init__ raw io IOBase raw = raw writeable - bool True write b Buffer - int raw write b close flush raw flush close transform_to = cast IO bytes NoCloseWriter raw_stream ex extensions transform_to = ex transform_to transform_to transform_to ex get_descriptor ex reversed extensions _item_size item WriteItem - int size = item tensor_data None raise AssertionError WriteItem tensor_data must None can t use math prod PT needs support older python s item tensor_data size size = s dtype = item tensor_data properties dtype size torch _utils _element_size dtype _split_by_size_and_type bins int items list WriteItem - list list WriteItem bins == items bytes_w = wi wi items wi type == WriteItemType BYTE_IO tensor_w = wi wi items wi type = WriteItemType BYTE_IO buckets list list WriteItem = _ range bins bucket_sizes = _ range bins tensor_w sort key=_item_size reverse=True i wi enumerate bytes_w buckets i bins append wi wi tensor_w TODO replace headq idx = min enumerate bucket_sizes key=operator itemgetter buckets idx append wi bucket_sizes idx += _item_size wi buckets _write_item transforms _StorageWriterTransforms stream io IOBase data Union io BytesIO torch Tensor write_item WriteItem storage_key str serialization_format SerializationFormat - WriteResult offset = stream tell transform_to transform_descriptors = transforms transform_save_stream write_item stream write_item type == WriteItemType BYTE_IO isinstance data io BytesIO raise AssertionError Data must io BytesIO BYTE_IO write items transform_to write data getbuffer isinstance data torch Tensor raise AssertionError Data must torch Tensor non-BYTE_IO write items data device = torch device cpu raise AssertionError Tensor must CPU device serialization_format == SerializationFormat TORCH_SAVE torch save data transform_to transform_to close serialization_format == SerializationFormat TORCH_SAVE isinstance data io BytesIO length = stream tell - offset length = data numel data element_size For consistency earlier versions leave field out metadata there no extensions info_transform_descriptors = None len transform_descriptors == transform_descriptors WriteResult index=write_item index size_in_bytes=length storage_data=_StorageInfo storage_key offset length transform_descriptors=info_transform_descriptors _write_files_from_queue create_stream Callable file_queue queue Queue result_queue queue Queue planner SavePlanner transforms _StorageWriterTransforms inflight_threshhold int use_fsync bool thread_count int serialization_format SerializationFormat - None try while True file_name storage_key write_items = file_queue get_nowait loader _TensorLoader custom_backend_name = torch _C _get_privateuse _backend_name custom_device_mod = getattr torch custom_backend_name None TODO Using OverlappingCpuLoader multiple threads creates significant performance degradation observed being related cuda stream syncs We should try fix use _OverlappingCpuLoader all threaded cases thread_count == torch cuda is_available custom_device_mod custom_device_mod is_available inflight_threshhold loader = _OverlappingCpuLoader planner resolve_data inflight_threshhold=inflight_threshhold loader = _SerialCpuLoader planner resolve_data tensor_w = wi wi write_items wi type = WriteItemType BYTE_IO write_item tensor_w loader add _item_size write_item write_item loader start_loading bytes_w = wi wi write_items wi type == WriteItemType BYTE_IO write_results = create_stream file_name wb stream write_item bytes_w data = planner resolve_data write_item write_results append _write_item transforms stream data write_item storage_key serialization_format tensor_dict = metadata_dict = tensor write_item loader values tensor is_cpu raise AssertionError Tensor must CPU write_results append _write_item transforms stream tensor write_item type ignore arg-type storage_key serialization_format tensor_dict write_item index fqn = tensor type ignore attr-defined metadata_dict write_item index fqn = type ignore attr-defined saved_offsets write_item tensor_data chunk offsets type ignore attr-defined serialization_format == SerializationFormat SAFETENSORS safetensors torch save type ignore import-not-found stream write save tensor_dict metadata= CUSTOM_METADATA_KEY json dumps metadata_dict DCP_VERSION_KEY str HF_DCP_VERSION FORMAT_KEY FORMAT_VALUE use_fsync try os fsync stream fileno except AttributeError UnsupportedOperation os sync stream close result_queue put write_results except queue Empty pass FileSystemBase ABC contextmanager abstractmethod create_stream path Union str os PathLike mode str - Generator io IOBase None None abstractmethod concat_path path Union str os PathLike suffix str - Union str os PathLike abstractmethod rename path Union str os PathLike new_path Union str os PathLike - None abstractmethod init_path path Union str os PathLike - Union str os PathLike abstractmethod mkdir path Union str os PathLike - None classmethod abstractmethod validate_checkpoint_id cls checkpoint_id Union str os PathLike - bool abstractmethod exists path Union str os PathLike - bool abstractmethod rm_file path Union str os PathLike - None FileSystem FileSystemBase contextmanager create_stream path Union str os PathLike mode str - Generator io IOBase None None isinstance path Path path = Path path path open mode stream yield cast io IOBase stream concat_path path Union str os PathLike suffix str - Union str os PathLike isinstance path Path path = Path path path suffix init_path path Union str os PathLike - Union str os PathLike isinstance path Path path = Path path path rename path Union str os PathLike new_path Union str os PathLike - None isinstance path Path path = Path path path rename cast Path new_path mkdir path Union str os PathLike - None isinstance path Path path = Path path path mkdir parents=True exist_ok=True classmethod validate_checkpoint_id cls checkpoint_id Union str os PathLike - bool isinstance checkpoint_id Path True str checkpoint_id False p Path checkpoint_id parents p exists os access str p os W_OK True False exists path Union str os PathLike - bool isinstance path Path path = Path path path exists rm_file path Union str os PathLike - None isinstance path Path path = Path path path unlink ls path Union str os PathLike - list str isinstance path Path path = Path path str p p path iterdir _FileSystemWriter StorageWriter Basic implementation StorageWriter using file IO This implementation makes following assumptions simplifications The checkpoint path empty non-existing directory File creation atomic The checkpoint consist one file per write request plus ` metadata ` file serialized metadata __init__ path Union str os PathLike single_file_per_rank bool = True sync_files bool = True thread_count int = per_thread_copy_ahead int = _ _ overwrite bool = True _extensions Optional Sequence StreamTransformExtension = None serialization_format SerializationFormat = SerializationFormat TORCH_SAVE args Any kwargs Any - None Initialize writer pointing ` path ` Args path directory where checkpoint will written single_file_per_rank Produce one file per rank instead one file per tensor blob Default True sync_files force files synced permanent storage Default True thread_count Number IO threads use write Default per_thread_copy_ahead How many bytes copy GPU ahead saving then Default Mb overwrite Whether allow overwriting existing checkpoints Defaults True _extensions Extensions apply output streams EXPERIMENTAL N B If sync_files disabled there s no guarantee checkpoint will consistent case failure super __init__ fs = FileSystem path = fs init_path path single_file_per_rank = single_file_per_rank sync_files = sync_files thread_count = thread_count per_thread_copy_ahead = per_thread_copy_ahead save_id = _generate_uuid overwrite = overwrite transforms = _StorageWriterTransforms _extensions serialization_format = serialization_format rank Optional int = None use_collectives bool = True reset checkpoint_id Union str os PathLike None = None - None checkpoint_id path = fs init_path checkpoint_id save_id = _generate_uuid set_up_storage_writer is_coordinator bool args Any kwargs Any - None rank = kwargs get rank use_collectives = kwargs get use_collectives True _metadata_exists - bool use_collectives A global checkpoint metadata file metadata_path = _get_metadata_path rank=None A rank specific metadata file every rank has written its own metadata Just looking lowest rank metadata file sufficient metadata_path = _get_metadata_path rank= fs exists metadata_path prepare_local_plan plan SavePlan - SavePlan fs mkdir path _metadata_exists overwrite warnings warn f Detected existing checkpoint path overwriting since overwrite= Past version PyTorch ` overwrite ` will default False Set variable True maintain functionality False raise when existing checkpoint found stacklevel= raise RuntimeError f Checkpoint already exists overwrite= rank None use_collectives plan = dataclasses replace plan storage_data=_StoragePrefix f __ rank _ plan prepare_global_plan plans list SavePlan - list SavePlan new_plans = dataclasses replace plan storage_data=_StoragePrefix f __ i _ plan storage_data None plan i plan enumerate plans new_plans write_data plan SavePlan planner SavePlanner - Future list WriteResult storage_plan _StoragePrefix = plan storage_data file_count = gen_file nonlocal file_count file_name = f storage_plan prefix file_count DEFAULT_SUFFIX file_count += file_name file_queue queue Queue = queue Queue single_file_per_rank bucket _split_by_size_and_type thread_count plan items file_name = gen_file path = fs concat_path path file_name file_queue put path file_name bucket item plan items file_name = gen_file path = fs concat_path path file_name file_queue put path file_name item _write_data planner file_queue _write_data planner SavePlanner file_queue queue Queue - Future list WriteResult result_queue queue Queue = queue Queue threads = _ range thread_count t = threading Thread target=_write_files_from_queue args= fs create_stream file_queue result_queue planner transforms per_thread_copy_ahead sync_files thread_count serialization_format t start threads append t _write_files_from_queue create_stream=self fs create_stream file_queue=file_queue result_queue=result_queue planner=planner transforms=self transforms inflight_threshhold=self per_thread_copy_ahead use_fsync=self sync_files thread_count=self thread_count serialization_format=self serialization_format t threads t join res = try while True res += result_queue get_nowait except queue Empty fut Future list WriteResult = Future fut set_result res fut finish metadata Metadata results list list WriteResult - None metadata = dataclasses replace metadata version=CURRENT_DCP_VERSION storage_md = wr_list results storage_md update wr index wr storage_data wr wr_list metadata storage_data = storage_md metadata storage_meta = storage_meta tmp_filename = f __ rank _metadata_fn tmp use_collectives rank None f _metadata_fn tmp tmp_path = cast Path fs concat_path path tmp_filename fs create_stream tmp_path wb metadata_file pickle dump metadata metadata_file sync_files try os fsync metadata_file fileno except AttributeError UnsupportedOperation os sync delete in-case other checkpoints present use_collectives rank None metadata_path = _get_metadata_path rank metadata_path = _get_metadata_path fs exists metadata_path fs rm_file metadata_path fs rename tmp_path metadata_path storage_meta - Optional StorageMeta StorageMeta checkpoint_id=self checkpoint_id save_id=self save_id _get_metadata_path rank Optional int = None - os PathLike filename = f _metadata_fn rank None f __ rank _metadata_fn cast Path fs concat_path path filename property checkpoint_id - Union str os PathLike checkpoint_id will used save checkpoint path classmethod validate_checkpoint_id cls checkpoint_id Union str os PathLike - bool FileSystem validate_checkpoint_id checkpoint_id _StorageReaderTransforms This experimental will likely move elsewhere future It lives here minimize changes while we still learning gathering feedback __init__ extension_registry Optional ExtensionRegistry = None - None extension_registry = ExtensionRegistry extension_registry None extension_registry transform_load_stream read_item ReadItem transform_descriptors Sequence str raw_stream IO bytes - IO bytes extensions = extension_registry from_descriptor_list transform_descriptors transform_from = raw_stream ex extensions isinstance ex StreamTransformExtension transform_from = ex transform_from transform_from transform_from FileSystemReader StorageReader __init__ path Union str os PathLike _extension_registry Optional ExtensionRegistry = None EXPERIMENTAL - None super __init__ fs = FileSystem path = fs init_path path storage_data dict Any Any = load_id = _generate_uuid transforms = _StorageReaderTransforms _extension_registry rank = None use_collectives = True _slice_file file sinfo _StorageInfo - IO bytes cast IO bytes _create_file_view file sinfo offset sinfo length reset checkpoint_id Union str os PathLike None = None - None storage_data = checkpoint_id path = fs init_path checkpoint_id load_id = _generate_uuid read_data plan LoadPlan planner LoadPlanner - Future None group requests file per_file dict str list ReadItem = read_item plan items item_md _StorageInfo = storage_data read_item storage_index path = item_md relative_path per_file setdefault path append read_item relative_path reqs per_file items new_path = fs concat_path path relative_path fs create_stream new_path rb stream TODO sort offset cache reading req reqs item_md = storage_data req storage_index file_slice = _slice_file stream item_md transform_from = transforms transform_load_stream req This field wasn t present older implementations so provide fallback item_md transform_descriptors file_slice req type == LoadItemType BYTE_IO read_bytes = io BytesIO transform_from read - read_bytes seek planner load_bytes req read_bytes transform_from seekable seekable = transform_from torch load requires seekable input so read transform stream now store output needed seekable = io BytesIO transform_from read - seekable seek tensor = cast Tensor torch load seekable map_location= cpu weights_only=True tensor = narrow_tensor_by_index tensor req storage_offsets req lengths target_tensor = planner resolve_tensor req detach target_tensor size = tensor size raise AssertionError f req req storage_index mismatch sizes target_tensor size vs tensor size target_tensor copy_ tensor planner commit_tensor req target_tensor fut Future = Future fut set_result None fut _get_metadata_path rank Optional int = None - os PathLike filename = f _metadata_fn rank None f __ rank _metadata_fn cast Path fs concat_path path filename Implementing abstract function StorageReader read_metadata args Any kwargs Any - Metadata rank = kwargs get rank path = _get_metadata_path rank fs create_stream path rb metadata_file metadata = pickle load metadata_file getattr metadata storage_meta None None metadata storage_meta = StorageMeta metadata storage_meta load_id = load_id metadata set_up_storage_reader metadata Metadata is_coordinator bool args Any kwargs Any - None storage_data = metadata storage_data rank = kwargs get rank use_collectives = kwargs get use_collectives True storage_data None raise AssertionError storage_data must None metadata prepare_local_plan plan LoadPlan - LoadPlan plan prepare_global_plan plans list LoadPlan - list LoadPlan plans property checkpoint_id - Union str os PathLike checkpoint_id will used load checkpoint path classmethod validate_checkpoint_id cls checkpoint_id Union str os PathLike - bool FileSystem validate_checkpoint_id checkpoint_id FileSystemWriter _FileSystemWriter BlockingAsyncStager Basic implementation StorageWriter using file IO This implementation makes following assumptions simplifications The checkpoint path empty non-existing directory File creation atomic The checkpoint consist one file per write request plus global ` metadata ` file serialized metadata rank coordination enabled rank local ` __ rank metadata ` file serialized metadata rank coordination NOT enabled __init__ path Union str os PathLike single_file_per_rank bool = True sync_files bool = True thread_count int = per_thread_copy_ahead int = _ _ cache_staged_state_dict bool = False overwrite bool = True _extensions Optional Sequence StreamTransformExtension = None serialization_format SerializationFormat = SerializationFormat TORCH_SAVE - None Initialize writer pointing ` path ` Args path directory where checkpoint will written single_file_per_rank Produce one file per rank instead one file per tensor blob Default True sync_files force files synced permanent storage Default True thread_count Number IO threads use write Default per_thread_copy_ahead How many bytes copy GPU ahead saving then Default Mb cache_staged_state_dict Whether cache staged state_dict This option decreases staging latency cost increases memory usage Additionally parameter set True s expectation stager maintained reused multiple dcp async_save calls Default False overwrite Whether allow overwriting existing checkpoints Defaults True _extensions Extensions apply output streams EXPERIMENTAL N B If sync_files disabled there s no guarantee checkpoint will consistent case failure _FileSystemWriter __init__ path=path single_file_per_rank=single_file_per_rank sync_files=sync_files thread_count=thread_count per_thread_copy_ahead=per_thread_copy_ahead overwrite=overwrite _extensions=_extensions serialization_format=serialization_format BlockingAsyncStager __init__ cache_staged_state_dict=cache_staged_state_dict stage state_dict STATE_DICT_TYPE - STATE_DICT_TYPE Override AsyncStager stage async case state dict already CPU so maintaining buffer makes no sense per_thread_copy_ahead = super stage state_dict