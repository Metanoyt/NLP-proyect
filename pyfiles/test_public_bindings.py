Owner s module autograd importlib inspect json logging os pkgutil unittest collections abc Callable torch torch _utils_internal get_file_path_ manual torch testing _internal common_utils IS_JETSON IS_MACOS IS_WINDOWS run_tests skipIfTorchDynamo TestCase log = logging getLogger __name__ TestPublicBindings TestCase test_no_new_reexport_callables This test aims stop introduction new re-exported callables into torch whose names do start _ Such callables made available torch XXX which may desirable reexported_callables = sorted k k v vars torch items callable v v __module__ startswith torch assertTrue all k startswith _ k reexported_callables reexported_callables test_no_new_bindings This test aims stop introduction new JIT bindings into torch _C whose names do start _ Such bindings made available torch XXX which may desirable If your change causes test fail add your new binding relevant submodule torch _C such torch _C _jit other relevant submodule torch _C If your binding really needs available torch XXX add torch _C add allowlist below If you have removed binding remove allowlist well This allowlist contains every binding torch _C copied into torch time writing It generated elem elem dir torch _C elem startswith _ torch_C_allowlist_superset = AcceleratorError AggregationType AliasDb AnyType Argument ArgumentSpec AwaitType autocast_decrement_nesting autocast_increment_nesting AVG BenchmarkConfig BenchmarkExecutionStats Block BoolType BufferDict StorageBase CallStack Capsule ClassType clear_autocast_cache Code CompilationUnit CompleteArgumentSpec ComplexType ConcreteModuleType ConcreteModuleTypeBuilder cpp CudaBFloat TensorBase CudaBoolTensorBase CudaByteTensorBase CudaCharTensorBase CudaComplexDoubleTensorBase CudaComplexFloatTensorBase CudaDoubleTensorBase CudaFloatTensorBase CudaHalfTensorBase CudaIntTensorBase CudaLongTensorBase CudaShortTensorBase DeepCopyMemoTable default_generator DeserializationStorageContext device DeviceObjType DictType DisableTorchFunction DisableTorchFunctionSubclass DispatchKey DispatchKeySet dtype EnumType ErrorReport ExcludeDispatchKeyGuard ExecutionPlan FatalError FileCheck finfo FloatType fork FunctionSchema Future FutureType Generator GeneratorType GreenContext get_autocast_cpu_dtype get_autocast_dtype get_autocast_ipu_dtype get_default_dtype get_num_interop_threads get_num_threads Gradient Graph GraphExecutorState has_cuda has_cudnn has_lapack has_mkl has_mkldnn has_mps has_openmp has_spectral iinfo import_ir_module_from_buffer import_ir_module InferredType init_num_threads InterfaceType IntType SymFloatType SymBoolType SymIntType IODescriptor is_anomaly_enabled is_anomaly_check_nan_enabled is_autocast_cache_enabled is_autocast_cpu_enabled is_autocast_ipu_enabled is_autocast_enabled is_grad_enabled is_inference_mode_enabled JITException layout ListType LiteScriptModule LockingLogger LoggerBase memory_format merge_type_from_type_comment ModuleDict Node NoneType NoopLogger NumberType OperatorInfo OptionalType OutOfMemoryError ParameterDict parse_ir parse_schema parse_type_comment PyObjectType PyTorchFileReader PyTorchFileWriter qscheme read_vitals RRefType ScriptClass ScriptClassFunction ScriptDict ScriptDictIterator ScriptDictKeyIterator ScriptList ScriptListIterator ScriptFunction ScriptMethod ScriptModule ScriptModuleSerializer ScriptObject ScriptObjectProperty SerializationStorageContext set_anomaly_enabled set_autocast_cache_enabled set_autocast_cpu_dtype set_autocast_dtype set_autocast_ipu_dtype set_autocast_cpu_enabled set_autocast_ipu_enabled set_autocast_enabled set_flush_denormal set_num_interop_threads set_num_threads set_vital Size StaticModule Stream StreamObjType Event StringType SUM SymFloat SymInt TensorType ThroughputBenchmark TracingState TupleType Type unify_type_list UnionType Use Value set_autocast_gpu_dtype get_autocast_gpu_dtype vitals_enabled wait Tag set_autocast_xla_enabled set_autocast_xla_dtype get_autocast_xla_dtype is_autocast_xla_enabled torch_C_bindings = elem elem dir torch _C elem startswith _ torch TensorBase explicitly removed torch __init__ py so included here explicitly_removed_torch_C_bindings = TensorBase torch_C_bindings = torch_C_bindings - explicitly_removed_torch_C_bindings Check torch _C bindings all allowlist Since bindings can change based how PyTorch compiled e g without CUDA two may exact match bindings should subset allowlist difference = torch_C_bindings difference torch_C_allowlist_superset msg = f torch _C had bindings present allowlist \n difference assertTrue torch_C_bindings issubset torch_C_allowlist_superset msg staticmethod _is_mod_public modname split_strs = modname split elem split_strs elem startswith _ False True unittest skipIf IS_WINDOWS IS_MACOS Inductor Distributed modules hard fail windows macos skipIfTorchDynamo Broken relevant now test_modules_can_be_imported failures = onerror modname failures append modname ImportError exception occurred importing package mod pkgutil walk_packages torch __path__ torch onerror=onerror modname = mod name try __main__ modname continue importlib import_module modname except Exception e Some current failures ImportError log exception import_module failed failures append modname e It ok add new entries here please careful these modules do get imported public code DO NOT add public modules here private_allowlist = torch _inductor codegen cuda cuda_kernel TODO Remove onnx _internal entries after onnx onnxscript installed CI torch onnx _internal exporter torch onnx _internal exporter _analysis torch onnx _internal exporter _building torch onnx _internal exporter _capture_strategies torch onnx _internal exporter _compat torch onnx _internal exporter _core torch onnx _internal exporter _decomp torch onnx _internal exporter _dispatching torch onnx _internal exporter _fx_passes torch onnx _internal exporter _ir_passes torch onnx _internal exporter _isolated torch onnx _internal exporter _onnx_program torch onnx _internal exporter _registration torch onnx _internal exporter _reporting torch onnx _internal exporter _schemas torch onnx _internal exporter _tensors torch onnx _internal exporter _torchlib ops torch onnx _internal exporter _verification torch onnx _internal fx _pass torch onnx _internal fx analysis torch onnx _internal fx analysis unsupported_nodes torch onnx _internal fx decomposition_skip torch onnx _internal fx diagnostics torch onnx _internal fx fx_onnx_interpreter torch onnx _internal fx fx_symbolic_graph_extractor torch onnx _internal fx onnxfunction_dispatcher torch onnx _internal fx op_validation torch onnx _internal fx passes torch onnx _internal fx passes _utils torch onnx _internal fx passes decomp torch onnx _internal fx passes functionalization torch onnx _internal fx passes modularization torch onnx _internal fx passes readability torch onnx _internal fx passes type_promotion torch onnx _internal fx passes virtualization torch onnx _internal fx type_utils torch testing _internal common_distributed torch testing _internal common_fsdp torch testing _internal dist_utils torch testing _internal distributed common_state_dict torch testing _internal distributed _shard sharded_tensor torch testing _internal distributed _shard test_common torch testing _internal distributed _tensor common_dtensor torch testing _internal distributed ddp_under_dist_autograd_test torch testing _internal distributed distributed_test torch testing _internal distributed distributed_utils torch testing _internal distributed fake_pg torch testing _internal distributed multi_threaded_pg torch testing _internal distributed nn api remote_module_test torch testing _internal distributed rpc dist_autograd_test torch testing _internal distributed rpc dist_optimizer_test torch testing _internal distributed rpc examples parameter_server_test torch testing _internal distributed rpc examples reinforcement_learning_rpc_test torch testing _internal distributed rpc faulty_agent_rpc_test torch testing _internal distributed rpc faulty_rpc_agent_test_fixture torch testing _internal distributed rpc jit dist_autograd_test torch testing _internal distributed rpc jit rpc_test torch testing _internal distributed rpc jit rpc_test_faulty torch testing _internal distributed rpc rpc_agent_test_fixture torch testing _internal distributed rpc rpc_test torch testing _internal distributed rpc tensorpipe_rpc_agent_test_fixture torch testing _internal distributed rpc_utils torch _inductor codegen cuda cuda_template torch _inductor codegen cutedsl _cutedsl_utils torch _inductor codegen cuda gemm_template torch _inductor codegen cpp_template torch _inductor codegen cpp_gemm_template torch _inductor codegen cpp_micro_gemm torch _inductor codegen cpp_template_kernel torch _inductor runtime triton_helpers torch ao pruning _experimental data_sparsifier lightning callbacks data_sparsity torch backends _coreml preprocess torch contrib _tensorboard_vis torch distributed _composable torch distributed _functional_collectives torch distributed _functional_collectives_impl torch distributed _shard torch distributed _sharded_tensor torch distributed _sharding_spec torch distributed _spmd api torch distributed _spmd batch_dim_utils torch distributed _spmd comm_tensor torch distributed _spmd data_parallel torch distributed _spmd distribute torch distributed _spmd experimental_ops torch distributed _spmd parallel_mode torch distributed _tensor torch distributed _tools sac_ilp torch distributed algorithms _checkpoint checkpoint_wrapper torch distributed algorithms _optimizer_overlap torch distributed rpc _testing faulty_agent_backend_registry torch distributed rpc _utils torch ao pruning _experimental data_sparsifier benchmarks dlrm_utils torch ao pruning _experimental data_sparsifier benchmarks evaluate_disk_savings torch ao pruning _experimental data_sparsifier benchmarks evaluate_forward_time torch ao pruning _experimental data_sparsifier benchmarks evaluate_model_metrics torch ao pruning _experimental data_sparsifier lightning tests test_callbacks torch csrc jit tensorexpr scripts bisect torch csrc lazy test_mnist torch distributed _shard checkpoint _fsspec_filesystem torch distributed _tensor examples visualize_sharding_example torch distributed checkpoint _fsspec_filesystem torch distributed examples memory_tracker_example torch testing _internal distributed rpc fb thrift_rpc_agent_test_fixture torch utils _cxx_pytree torch utils tensorboard _convert_np torch utils tensorboard _embedding torch utils tensorboard _onnx_graph torch utils tensorboard _proto_graph torch utils tensorboard _pytorch_graph torch utils tensorboard _utils errors = mod exc failures mod private_allowlist make sure mod actually private assert any t startswith _ t mod split continue errors append f mod failed error type exc __qualname__ str exc assertEqual \n join errors AttributeError module torch distributed has no attribute _shard unittest skipIf IS_WINDOWS IS_JETSON Distributed Attribute Error skipIfTorchDynamo Broken relevant now test_correct_module_names An API considered public its ` __module__ ` starts ` torch ` there no name ` __module__ ` object itself starts _ Each public package should either - preferred Define ` __all__ ` all callables classes there must have their ` __module__ ` start current submodule s path Things ` __all__ ` should NOT have their ` __module__ ` start current submodule - simple python-only modules Not define ` __all__ ` all elements ` dir submod ` must have their ` __module__ ` start current submodule failure_list = open get_file_path_ os path dirname __file__ allowlist_for_publicAPI json json_file no new entries should added allow_dict New APIs must follow public API guidelines allow_dict = json load json_file Because we want minimal modifications ` allowlist_for_publicAPI json ` we adding entries migrated modules here original locations modname allow_dict being_migrated modname allow_dict allow_dict allow_dict being_migrated modname = allow_dict modname test_module modname try __main__ modname mod = importlib import_module modname except Exception It ok ignore here we have test above ensures should never happen _is_mod_public modname verifies each public API has correct module name naming semantics check_one_element elem modname mod is_public is_all obj = getattr mod elem torch dtype nor callable so we need check separately isinstance obj Callable torch dtype inspect isclass obj elem_module = getattr obj __module__ None Only used nice error message below why_not_looks_public = elem_module None why_not_looks_public = because does have ` __module__ ` attribute If module being migrated foo bar entry foo bar module s starting package would referred new location even there foo inside bar py modname = allow_dict being_migrated get modname modname elem_modname_starts_with_mod = elem_module None elem_module startswith modname _ elem_module why_not_looks_public elem_modname_starts_with_mod why_not_looks_public = f because its ` __module__ ` attribute ` elem_module ` within f torch library does start submodule where defined ` modname ` elem s name must NOT begin ` _ ` s module name SHOULD start s current module since s public API looks_public = elem startswith _ elem_modname_starts_with_mod why_not_looks_public looks_public why_not_looks_public = f because starts ` _ ` ` elem ` is_public = looks_public modname allow_dict elem allow_dict modname is_public why_is_public = f inside module s ` modname ` ` __all__ ` is_all attribute does start ` _ ` module does have ` __all__ ` defined fix_is_public = f remove modules ` modname ` ` __all__ ` is_all f either define ` __all__ ` ` modname ` add ` _ ` beginning name assert is_all why_is_public = f inside module s ` modname ` ` __all__ ` fix_is_public = f add modules ` modname ` ` __all__ ` looks_public why_looks_public = does look public because follows rules doc above does start ` _ ` has proper ` __module__ ` fix_looks_public = make its name start ` _ ` why_looks_public = why_not_looks_public elem_modname_starts_with_mod fix_looks_public = make sure ` __module__ ` properly set points submodule f ` modname ` fix_looks_public = remove ` _ ` beginning name failure_list append f modname elem is_public_str = is_public NOT failure_list append f - Is is_public_str public why_is_public looks_public_str = looks_public NOT failure_list append f - Does looks_public_str look public why_looks_public Swap str below avoid having create NOT again failure_list append - You can do either these two things fix problem failure_list append f - To make looks_public_str public fix_is_public failure_list append f - To make is_public_str look public fix_looks_public hasattr mod __all__ public_api = mod __all__ all_api = dir mod elem all_api check_one_element elem modname mod is_public=elem public_api is_all=True all_api = dir mod elem all_api elem startswith _ check_one_element elem modname mod is_public=True is_all=False mod pkgutil walk_packages torch __path__ torch modname = mod name test_module modname test_module torch msg = All APIs below do meet our guidelines public API https github com pytorch pytorch wiki Public-API-definition-and-documentation \n msg += Make sure everything public expected particular module has properly populated ` __all__ ` attribute everything supposed public does look public does start ` _ ` has ` __module__ ` properly populated msg += \n\nFull list \n msg += \n join map str failure_list empty lists considered false python assertTrue failure_list msg __name__ == __main__ run_tests