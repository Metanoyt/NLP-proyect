Owner s oncall distributed contextlib sys enum Enum torch torch nn nn torch optim optim torch distributed dist torch distributed fsdp FullyShardedDataParallel FSDP torch nn parallel DistributedDataParallel torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp FSDPTest get_full_params torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests TEST_WITH_DEV_DBG_ASAN dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit device_type = acc type acc = torch accelerator current_accelerator cpu Model nn Module __init__ with_fsdp freeze_after_wrap_fsdp disable_autograd fsdp_kwargs super __init__ trunk = nn Sequential nn Conv d kernel_size= nn ReLU inplace=True nn AdaptiveAvgPool d output_size= nn Flatten head = nn Linear with_fsdp freeze_after_wrap_fsdp fsdp_wrap fsdp_kwargs autograd_ctx = torch no_grad disable_autograd contextlib nullcontext fsdp_wrap fsdp_kwargs trunk = FSDP trunk fsdp_kwargs head = FSDP head fsdp_kwargs forward x autograd_ctx x = trunk x head x NestedTrunkModel nn Module __init__ with_fsdp freeze_after_wrap_fsdp disable_autograd fsdp_kwargs super __init__ trunk = nn Sequential _create_block with_fsdp freeze_after_wrap_fsdp _create_block with_fsdp freeze_after_wrap_fsdp head = nn Sequential nn AdaptiveAvgPool d output_size= nn Flatten nn Linear with_fsdp freeze_after_wrap_fsdp fsdp_wrap fsdp_kwargs autograd_ctx = torch no_grad disable_autograd contextlib nullcontext fsdp_wrap fsdp_kwargs name child trunk named_children wrapped_child = FSDP child fsdp_kwargs setattr trunk name wrapped_child trunk = FSDP trunk fsdp_kwargs head = FSDP head fsdp_kwargs forward x autograd_ctx x = trunk x head x _create_block in_channels out_channels with_fsdp freeze_after_wrap_fsdp block = nn Sequential nn Conv d in_channels out_channels kernel_size= nn ReLU inplace=True block FreezingMethod str Enum GradToNone = grad_to_none RequiresGrad = requires_grad TestFreezingWeights FSDPTest _create_model with_fsdp with_nested_trunk freeze_after_wrap_fsdp disable_autograd fsdp_kwargs with_nested_trunk model = NestedTrunkModel with_fsdp freeze_after_wrap_fsdp disable_autograd fsdp_kwargs model = Model with_fsdp freeze_after_wrap_fsdp disable_autograd fsdp_kwargs model _dist_train with_nested_trunk freezing_method freeze_after_wrap_fsdp with_fsdp disable_autograd forward_prefetch torch manual_seed batch = torch randn size= device_type fsdp_kwargs = device_id rank forward_prefetch forward_prefetch ddp_kwargs = device_ids rank find_unused_parameters bool disable_autograd model = _create_model with_fsdp with_nested_trunk freeze_after_wrap_fsdp disable_autograd fsdp_kwargs model = model device_type freezing trunk using requires_grad freezing_method == FreezingMethod RequiresGrad param model trunk parameters param requires_grad = False with_fsdp freeze_after_wrap_fsdp model fsdp_wrap fsdp_kwargs model = FSDP model fsdp_kwargs model = DistributedDataParallel model ddp_kwargs target = torch tensor dtype=torch long device_type criterion = nn CrossEntropyLoss optimizer = optim SGD model parameters lr= momentum= _ range out = model batch fake_loss = criterion out target optimizer zero_grad fake_loss backward freezing_method == FreezingMethod GradToNone param model module trunk parameters param grad = None optimizer step with_fsdp get_full_params model list model parameters skip_if_lt_x_gpu parametrize with_nested_trunk True False parametrize freezing_method FreezingMethod RequiresGrad FreezingMethod GradToNone parametrize freeze_after_wrap_fsdp True False parametrize disable_autograd True False parametrize forward_prefetch True False test_freezing_weights with_nested_trunk freezing_method freeze_after_wrap_fsdp disable_autograd forward_prefetch DDP ddp_state = _dist_train with_nested_trunk freezing_method freeze_after_wrap_fsdp with_fsdp=False disable_autograd=disable_autograd forward_prefetch=False does apply DDP FSDP fsdp_state = _dist_train with_nested_trunk freezing_method freeze_after_wrap_fsdp with_fsdp=True disable_autograd=disable_autograd forward_prefetch=forward_prefetch assertEqual ddp_state fsdp_state exact_device=True msg= FullyShardedDataParallel states didn t match PyTorch DDP states freezing_method == FreezingMethod RequiresGrad ddp_param fsdp_param zip ddp_state fsdp_state assertEqual ddp_param requires_grad fsdp_param requires_grad instantiate_parametrized_tests TestFreezingWeights __name__ == __main__ run_tests