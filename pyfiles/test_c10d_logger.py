Owner s oncall distributed json logging re sys functools partial wraps torch torch distributed dist torch distributed c d_logger _c d_logger _exception_logger dist is_available print Distributed available skipping tests file=sys stderr sys exit torch testing _internal common_distributed DistributedTestBase TEST_SKIPS torch testing _internal common_fsdp get_devtype torch testing _internal common_utils run_tests TEST_WITH_DEV_DBG_ASAN device_type = str get_devtype TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit WORLD_SIZE = min max torch get_device_module device_type device_count with_comms func=None func None partial with_comms wraps func wrapper args kwargs torch get_device_module device_type device_count world_size sys exit TEST_SKIPS f multi-gpu- world_size exit_code create_pg device_type func destroy_comms wrapper C dErrorLoggerTest DistributedTestBase property world_size WORLD_SIZE property process_group dist group WORLD destroy_comms Wait all ranks reach here before starting shutdown dist barrier dist destroy_process_group test_get_or_create_logger assertIsNotNone _c d_logger assertEqual len _c d_logger handlers assertIsInstance _c d_logger handlers logging NullHandler _exception_logger _failed_broadcast_raise_exception tensor = torch arange dtype=torch int dist broadcast tensor world_size + _exception_logger _failed_broadcast_not_raise_exception try tensor = torch arange dtype=torch int dist broadcast tensor world_size + except Exception pass with_comms test_exception_logger - None assertRaises Exception _failed_broadcast_raise_exception assertLogs _c d_logger level= DEBUG captured _failed_broadcast_not_raise_exception error_msg_dict = json loads re search + captured output group replace NCCL adds additional nccl_version data error_msg_dict backend device_type == dist Backend NCCL assertEqual len error_msg_dict assertEqual len error_msg_dict assertIn pg_name error_msg_dict keys assertEqual None error_msg_dict pg_name assertIn func_name error_msg_dict keys assertEqual broadcast error_msg_dict func_name assertIn backend error_msg_dict keys assertEqual backend device_type error_msg_dict backend backend device_type == dist Backend NCCL assertIn nccl_version error_msg_dict keys nccl_ver = torch cuda nccl version assertEqual join str v v nccl_ver error_msg_dict nccl_version In test case group_size = world_size since we don t have multiple processes one node assertIn group_size error_msg_dict keys assertEqual str world_size error_msg_dict group_size assertIn world_size error_msg_dict keys assertEqual str world_size error_msg_dict world_size assertIn global_rank error_msg_dict keys assertIn str dist get_rank error_msg_dict global_rank In test case local_rank = global_rank since we don t have multiple processes one node assertIn local_rank error_msg_dict keys assertIn str dist get_rank error_msg_dict local_rank __name__ == __main__ run_tests