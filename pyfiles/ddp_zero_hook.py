mypy allow-untyped-defs weakref collections abc Callable typing Any Optional torch torch distributed dist torch distributed optim ZeroRedundancyOptimizer torch distributed optim zero_redundancy_optimizer _OverlapStatus torch nn parallel distributed DistributedDataParallel __all__ = hook_with_zero_step hook_with_zero_step_interleaved Functional optimizers require passing list gradients their ` step ` method ZeRO requires functional optimizer overlap DDP Passing ` None ` instead actual gradient indicates optimizer update corresponding parameter _NO_PARAM_UPDATE None = None _perform_local_step bucket dist GradBucket zero ZeroRedundancyOptimizer rank int r Perform local optimizer step using gradients provided ` ` bucket ` ` Arguments bucket dist GradBucket bucket providing gradients zero ZeroRedundancyOptimizer ` ZeroRedundancyOptimizer ` instance perform meth ` _local_step ` rank int calling process s rank warning This function assumes appropriate synchronization has taken place so bucket s gradients can used overlap_info = zero _overlap_info bucket_index = bucket index assert len zero optim param_groups == Overlapping DDP ZeRO only supports single parameter group Construct ` gradients ` input local optimizer step which expects ` None ` list position indicate corresponding parameter should updated num_local_optim_params = len zero optim param_groups params gradients list Optional torch Tensor = _NO_PARAM_UPDATE _ range num_local_optim_params assert bucket_index overlap_info offsets f Bucket index bucket_index assigned rank rank gradients_offset = overlap_info offsets bucket_index bucket_assignment = zero _bucket_assignments_per_rank rank bucket_index bucket_offset = bucket_assignment offset length = len bucket_assignment parameters bucket_gradients = bucket gradients bucket_offset bucket_offset + length i grad enumerate bucket_gradients gradients gradients_offset + i = grad zero _local_step gradients _broadcast_bucket bucket_index int zero ZeroRedundancyOptimizer r Broadcasts bucket s parameters Arguments bucket_index int index bucket corresponding parameters broadcast zero ZeroRedundancyOptimizer calling process s ` ZeroRedundancyOptimizer ` instance overlap_info = zero _overlap_info assert len overlap_info assigned_ranks_per_bucket bucket_index ` assigned_ranks_per_bucket ` fully constructed Sort ensure same ordering across ranks assigned_ranks = sorted overlap_info assigned_ranks_per_bucket bucket_index assert len assigned_ranks f Bucket bucket_index should assigned least one rank assigned_rank assigned_ranks bucket_assignments = zero _bucket_assignments_per_rank assigned_rank bucket_index bucket_assignments send_tensor = bucket_assignments bucket_index tensor assert send_tensor None overlap_info broadcast_handles append dist broadcast send_tensor src=dist get_global_rank zero process_group assigned_rank group=zero process_group async_op=True _save_ddp_bucket_info bucket dist GradBucket zero ZeroRedundancyOptimizer r Save ` DistributedDataParallel ` gradient bucket information ` ZeroRedundancyOptimizer ` instance ` ` zero ` ` In particular function meant called upon seeing each gradient bucket use when overlapping meaning does save compute any global information Arguments bucket dist GradBucket current gradient bucket zero ZeroRedundancyOptimizer calling process s ` ZeroRedundancyOptimizer ` instance overlap_info = zero _overlap_info bucket_params = bucket parameters assert len bucket_params Empty bucket Save parameters bucket overlap_info params_per_bucket append bucket_params overlap_info shard_buckets Additionally save bucket size assignment heuristic use bucket_size = param bucket_params bucket_size += param numel assert overlap_info total_size None overlap_info total_size += bucket_size _hook_with_zero_step_setup ddp_ref weakref ReferenceType zero ZeroRedundancyOptimizer bucket dist GradBucket r Encapsulate setup logic func ` hook_with_zero_step ` func ` hook_with_zero_step_interleaved ` This means logic run hook before backward pass optimizer step can actually overlapped This factored out since common both func ` hook_with_zero_step ` func ` hook_with_zero_step_interleaved ` Arguments ddp_ref weakref ReferenceType weak reference process s ` DistributedDataParallel ` instance zero ZeroRedundancyOptimizer calling process s ` ZeroRedundancyOptimizer ` instance bucket dist GradBucket current gradient bucket Proceed normal until DDP buckets have been rebuilt ddp_ref _has_rebuilt_buckets type ignore union-attr assert zero _overlap_info status == _OverlapStatus UNINITIALIZED bucket_index = bucket index overlap_info = zero _overlap_info overlap_info status == _OverlapStatus UNINITIALIZED overlap_info status = _OverlapStatus DDP_HAS_REBUILT_BUCKETS overlap_info status == _OverlapStatus DDP_HAS_REBUILT_BUCKETS bucket_index == len overlap_info params_per_bucket This corresponds first bucket backward pass immediately after all information has been saved so we can perform delayed ZeRO initialization zero _init_zero_for_overlap Once DDP buckets have been rebuilt ZeRO has been properly initialized yet save information needed _save_ddp_bucket_info bucket zero hook_with_zero_step hook Callable Any dist GradBucket torch futures Future ddp DistributedDataParallel zero ZeroRedundancyOptimizer shard_buckets bool = False - Callable Any dist GradBucket torch futures Future torch Tensor r Modify ` ` hook ` ` overlap ` ZeroRedundancyOptimizer ` optimizer step ` DistributedDataParallel ` backward pass This approach overlaps optimizer computation communication backward communication In particular backward computation proceeds contiguously optimizer computation follows overlapping outstanding backward communication i e all-reduces possibly other optimizer communication i e broadcasts The optimizer step computation begins after last gradient bucket computation has finished This approach may preferred over meth ` hook_with_zero_step_interleaved ` communication relatively slow compared computation Arguments hook Callable Any dist GradBucket torch futures Future hook modify ddp DistributedDataParallel ` DistributedDataParallel ` instance use zero ZeroRedundancyOptimizer ` ZeroRedundancyOptimizer ` instance use shard_buckets bool ` ` True ` ` then assignment each ` DistributedDataParallel ` bucket partitioned across possibly multiple ` ZeroRedundancyOptimizer ` instances i e across possibly multiple ranks approximate uniformity ` ` False ` ` then each bucket wholly assigned single ` ZeroRedundancyOptimizer ` instance i e single rank Returns The modified hook Raises ValueError ` ` zero ` ` constructed ` ` overlap_with_ddp=False ` ` RuntimeError using any backend other than NCCL HCCL since currently Gloo may hang warning Given way overlapping ` DistributedDataParallel ` ` ZeroRedundancyOptimizer ` currently implemented first two three training iterations do perform parameter updates optimizer step depending ` ` static_graph=False ` ` ` ` static_graph=True ` ` respectively This because needs information about gradient bucketing strategy used ` DistributedDataParallel ` which finalized until second forward pass ` ` static_graph=False ` ` until third forward pass ` ` static_graph=True ` ` zero _overlap_with_ddp raise ValueError ZeroRedundancyOptimizer must constructed ` overlap_with_ddp=True ` use hook properly ddp_ref = weakref ref ddp NOTE Gloo may hang overlapping approach see https github com pytorch pytorch issues pg = dist get_backend ddp_ref process_group type ignore union-attr pg == dist Backend GLOO raise RuntimeError Gloo backend using Overlapping DDP ZeRO may meet hangs shard_buckets zero _overlap_info shard_buckets = True zero _overlap_info total_size = hook_with_zero_fn state Any bucket dist GradBucket - torch futures Future torch Tensor r Return ` Future ` runs optimizer step corresponds last gradient bucket Perform equivalent ` ZeroRedundancyOptimizer ` meth ` step ` ` ` bucket ` ` last gradient bucket The function gives gradient bucket tensor performs additional computation iteration ` DistributedDataParallel ` buckets rebuilt collect information used implement modified hook Arguments state Any any state hook bucket dist GradBucket ` DistributedDataParallel ` gradient bucket fut = hook state bucket _hook_with_zero_step_setup ddp_ref zero bucket zero _overlap_info status = _OverlapStatus INITIALIZED fut overlap_info = zero _overlap_info bucket_index = bucket index rank = zero global_rank assert overlap_info status == _OverlapStatus INITIALIZED assert len overlap_info assigned_ranks_per_bucket bucket_index ` assigned_ranks_per_bucket ` fully constructed assigned_to_bucket = rank overlap_info assigned_ranks_per_bucket bucket_index Save bucket reference all-reduce future final bucket assigned_to_bucket overlap_info bucket_index_to_bucket bucket_index = bucket overlap_info bucket_index_to_future bucket_index = fut Check buckets indexed incrementally starting order their autograd hooks firing len overlap_info bucket_indices_seen assert overlap_info bucket_indices_seen - == bucket_index - Bucket indices incremental order assert bucket_index == Bucket indices do start overlap_info bucket_indices_seen append bucket_index Directly future without any optimizer computation last bucket num_buckets = len overlap_info params_per_bucket is_last_bucket = bucket_index == num_buckets - is_last_bucket fut Perform partial optimizer step all buckets after final bucket has been computed NOTE This should chained callback last bucket s all-reduce future since would add synchronization delays all optimizer computation wait last all-reduce bucket_index range num_buckets assigned_ranks = overlap_info assigned_ranks_per_bucket bucket_index rank assigned_ranks Wait bucket s all-reduce future ensure correct gradients assert bucket_index overlap_info bucket_index_to_future f All-reduce future bucket bucket_index saved f rank rank allreduce_future = overlap_info bucket_index_to_future bucket_index allreduce_future wait Perform partial optimizer step curr_bucket = overlap_info bucket_index_to_bucket bucket_index _perform_local_step curr_bucket zero rank _broadcast_bucket bucket_index zero Ensure all parameter updates finished before next forward pass overlap_info wait_for_broadcasts overlap_info clear_per_iter_info fut hook_with_zero_fn hook_with_zero_step_interleaved hook Callable Any dist GradBucket torch futures Future ddp DistributedDataParallel zero ZeroRedundancyOptimizer shard_buckets bool = False - Callable Any dist GradBucket torch futures Future torch Tensor r Modify ` ` hook ` ` overlap ` ZeroRedundancyOptimizer ` optimizer step ` DistributedDataParallel ` backward pass This approach overlaps optimizer computation communication backward computation communication In particular once bucket s gradients have been computed optimizer computation using those gradients launched though actual computation must wait bucket s all-reduce complete This yields interleaving all- reduces broadcasts communication stream This approach may preferred over meth ` hook_with_zero_step ` communication relatively fast compared computation Arguments hook Any dist GradBucket - torch futures Future hook modify ddp DistributedDataParallel ` DistributedDataParallel ` instance use zero ZeroRedundancyOptimizer ` ZeroRedundancyOptimizer ` instance use shard_buckets bool ` ` True ` ` then assignment each ` DistributedDataParallel ` bucket partitioned across possibly multiple ` ZeroRedundancyOptimizer ` instances i e across possibly multiple ranks approximate uniformity ` ` False ` ` then each bucket wholly assigned single ` ZeroRedundancyOptimizer ` instance i e single rank Returns The modified hook Raises ValueError ` ` zero ` ` constructed ` ` overlap_with_ddp=False ` ` RuntimeError using any backend other than NCCL since currently Gloo may hang warning Given way overlapping ` DistributedDataParallel ` ` ZeroRedundancyOptimizer ` currently implemented first two three training iterations do perform parameter updates optimizer step depending ` ` static_graph=False ` ` ` ` static_graph=True ` ` respectively This because needs information about gradient bucketing strategy used ` DistributedDataParallel ` which finalized until second forward pass ` ` static_graph=False ` ` until third forward pass ` ` static_graph=True ` ` zero _overlap_with_ddp raise ValueError ZeroRedundancyOptimizer must constructed ` overlap_with_ddp=True ` use hook properly ddp_ref = weakref ref ddp NOTE Gloo may hang overlapping approach see https github com pytorch pytorch issues pg = dist get_backend ddp_ref process_group type ignore union-attr pg == dist Backend GLOO raise RuntimeError Gloo backend using Overlapping DDP ZeRO may meet hangs shard_buckets zero _overlap_info shard_buckets = True zero _overlap_info total_size = hook_with_zero_interleaved_fn state bucket dist GradBucket - torch futures Future torch Tensor r Return ` Future ` gives gradient bucket tensor performs partial ` ZeroRedundancyOptimizer ` meth ` step ` This function uses gradients gradient given bucket perform partial ` ZeroRedundancyOptimizer ` meth ` step ` Arguments state any state hook bucket dist GradBucket ` DistributedDataParallel ` gradient bucket fut = hook state bucket _hook_with_zero_step_setup ddp_ref zero bucket zero _overlap_info status = _OverlapStatus INITIALIZED fut zero_step fut torch futures Future - torch Tensor r Perform partial ` ZeroRedundancyOptimizer ` meth ` step ` using gradients ` DistributedDataParallel ` Returns A ` torch Tensor ` representing contents gradient bucket overlap_info = zero _overlap_info bucket_index = bucket index rank = zero global_rank assigned_ranks = overlap_info assigned_ranks_per_bucket bucket_index overlap_info bucket_indices_seen append bucket_index rank assigned_ranks _perform_local_step bucket zero rank _broadcast_bucket bucket_index zero num_buckets = len overlap_info params_per_bucket len overlap_info bucket_indices_seen == num_buckets Ensure all parameter updates finished before next forward pass overlap_info wait_for_broadcasts overlap_info clear_per_iter_info bucket buffer fut then zero_step hook_with_zero_interleaved_fn