mypy allow-untyped-defs operator collections abc Callable functools reduce typing Optional Union torch torch nn functional F base_sparsifier BaseSparsifier __all__ = WeightNormSparsifier _flat_idx_to_ d idx shape rows = idx shape cols = idx shape rows cols WeightNormSparsifier BaseSparsifier r Weight-Norm Sparsifier This sparsifier computes norm every sparse block zeroes-out ones lowest norm The level sparsity defines how many blocks removed This sparsifier controlled three variables ` sparsity_level ` defines number sparse blocks zeroed-out ` sparse_block_shape ` defines shape sparse blocks Note sparse blocks originate zero-index tensor ` zeros_per_block ` number zeros we expecting each sparse block By default we assume all elements within block zeroed-out However setting variable sets target number zeros per block The zeros within each block chosen smallest absolute values Args sparsity_level The target level sparsity sparse_block_shape The shape sparse block see note below zeros_per_block Number zeros sparse block norm Norm use Could either ` int ` callable If ` int ` only L L implemented Note The ` sparse_block_shape ` tuple representing block_ROWS block_COLS irrespective what rows cols mean data tensor That means you sparsify weight tensor nn Linear which has weight shape ` Cout Cin ` ` block_ROWS ` would refer output channels while ` block_COLS ` would refer input channels Note All arguments WeightNormSparsifier constructor default arguments could overridden configuration provided ` prepare ` step __init__ sparsity_level float = sparse_block_shape tuple int int = zeros_per_block Optional int = None norm Optional Union Callable int = None zeros_per_block None zeros_per_block = reduce operator mul sparse_block_shape defaults = sparsity_level sparsity_level sparse_block_shape sparse_block_shape zeros_per_block zeros_per_block norm None norm = callable norm norm_fn = norm norm == norm_fn = lambda T T abs norm == norm_fn = lambda T T T raise NotImplementedError f L- norm yet implemented super __init__ defaults=defaults _scatter_fold_block_mask output_shape dim indices block_shape mask=None input_shape=None device=None r Creates patches size ` block_shape ` after scattering indices mask None input_shape None raise AssertionError input_shape must provided when mask None mask = torch ones input_shape device=device mask scatter_ dim=dim index=indices value= mask data = F fold mask output_size=output_shape kernel_size=block_shape stride=block_shape mask _make_tensor_mask data input_shape sparsity_level sparse_block_shape mask=None r Creates tensor-level mask Tensor-level mask described mask where granularity sparsification smallest patch sparse_block_shape That means given mask sparse_block_shape smallest patch zeros ones could sparse_block_shape In context ` sparsity_level ` describes fraction sparse patches h w = data shape - block_h block_w = sparse_block_shape dh = block_h - h block_h block_h dw = block_w - w block_w block_w mask None mask = torch ones h + dh w + dw device=data device sparsity_level = mask data = torch zeros_like mask mask sparsity_level = mask data = torch ones_like mask mask values_per_block = reduce operator mul sparse_block_shape values_per_block Reduce data data = F avg_pool d data None None kernel_size=sparse_block_shape stride=sparse_block_shape ceil_mode=True data = data flatten num_blocks = len data data = data repeat values_per_block threshold_idx = round sparsity_level num_blocks threshold_idx = max min num_blocks - threshold_idx Sanity check _ sorted_idx = torch topk data k=threshold_idx dim= largest=False Temp reshape mask mask_reshape = mask reshape data shape data might reshaped _scatter_fold_block_mask dim= output_shape= h + dh w + dw indices=sorted_idx block_shape=sparse_block_shape mask=mask_reshape mask data = mask_reshape squeeze reshape mask shape h w contiguous mask _make_block_mask data sparse_block_shape zeros_per_block mask=None r Creates block-level mask Block-level mask described mask where granularity sparsification largest patch sparse_block_shape That means given mask sparse_block_shape sparsity computed only within patch size sparse_block_shape In context ` zeros_per_block ` describes number zeroed-out elements within patch h w = data shape - block_h block_w = sparse_block_shape dh = block_h - h block_h block_h dw = block_w - w block_w block_w values_per_block = reduce operator mul sparse_block_shape mask None mask = torch ones h + dh w + dw device=data device values_per_block == zeros_per_block Everything should sparsified mask data = torch zeros_like mask mask create new padded tensor like data match block_shape padded_data = torch ones h + dh w + dw dtype=data dtype device=data device padded_data fill_ torch nan padded_data h w = data unfolded_data = F unfold padded_data None None kernel_size=sparse_block_shape stride=sparse_block_shape Temp reshape mask mask_reshape = mask reshape unfolded_data shape _ sorted_idx = torch topk unfolded_data k=zeros_per_block dim= largest=False _scatter_fold_block_mask dim= indices=sorted_idx output_shape=padded_data shape block_shape=sparse_block_shape mask=mask_reshape mask data = mask_reshape squeeze reshape mask shape contiguous mask update_mask type ignore call-override override module tensor_name sparsity_level sparse_block_shape zeros_per_block kwargs values_per_block = reduce operator mul sparse_block_shape zeros_per_block values_per_block raise ValueError Number zeros per block cannot more than total number elements block zeros_per_block raise ValueError Number zeros per block should positive mask = getattr module parametrizations tensor_name mask sparsity_level = zeros_per_block == mask data = torch ones_like mask sparsity_level = zeros_per_block == values_per_block mask data = torch zeros_like mask ww = norm_fn getattr module tensor_name tensor_mask = _make_tensor_mask data=ww pyrefly ignore missing-attribute input_shape=ww shape sparsity_level=sparsity_level sparse_block_shape=sparse_block_shape values_per_block = zeros_per_block block_mask = _make_block_mask data=ww sparse_block_shape=sparse_block_shape zeros_per_block=zeros_per_block tensor_mask = torch logical_or tensor_mask block_mask mask data = tensor_mask