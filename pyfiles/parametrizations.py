mypy allow-untyped-defs enum auto Enum typing Optional torch torch nn functional F torch Tensor torch nn modules Module torch nn utils parametrize __all__ = orthogonal spectral_norm weight_norm _is_orthogonal Q eps=None n k = Q size - Q size - Id = torch eye k dtype=Q dtype device=Q device A reasonable eps too large eps = n torch finfo Q dtype eps torch allclose Q mH Q Id atol=eps _make_orthogonal A Assume A tall matrix Compute Q factor s t A = QR A may complex diag R real non-negative X tau = torch geqrf A Q = torch linalg householder_product X tau The diagonal X diagonal R which always real so we normalise its signs Q = X diagonal dim =- dim =- sgn unsqueeze - Q _OrthMaps Enum matrix_exp = auto cayley = auto householder = auto _Orthogonal Module base Tensor __init__ weight orthogonal_map _OrthMaps use_trivialization=True - None super __init__ Note Householder complex For complex tensors possible compute tensor ` tau ` necessary linalg householder_product reflectors To see note reflectors have shape like which complex matrices give n n- real parameters Now you need n^ parameters parametrize unitary matrices Saving tau its own does work either because every combination ` A tau ` gives unitary matrix meaning we optimise them independent tensors we would maintain constraint An equivalent reasoning holds rectangular matrices weight is_complex orthogonal_map == _OrthMaps householder raise ValueError The householder parametrization does support complex tensors shape = weight shape orthogonal_map = orthogonal_map use_trivialization register_buffer base None forward X torch Tensor - torch Tensor n k = X size - X size - transposed = n k transposed X = X mT n k = k n Here n k X tall matrix orthogonal_map == _OrthMaps matrix_exp orthogonal_map == _OrthMaps cayley We just need n x k - k k- parameters X = X tril n = k Embed into square matrix X = torch cat X X new_zeros n n - k expand X shape - - - dim=- A = X - X mH A skew-symmetric skew-hermitian orthogonal_map == _OrthMaps matrix_exp Q = torch matrix_exp A orthogonal_map == _OrthMaps cayley Computes Cayley retraction I+A I-A ^ - Id = torch eye n dtype=A dtype device=A device Q = torch linalg solve torch add Id A alpha=- torch add Id A alpha= Q now orthogonal unitary size n n n = k pyrefly ignore unbound-name Q = Q k Q now size X albeit perhaps transposed X real here we do support householder complex numbers A = X tril diagonal=- tau = + A A sum dim=- Q = torch linalg householder_product A tau The diagonal X s - s We do want differentiate through update diagonal X hence casting Q = Q X diagonal dim =- dim =- int unsqueeze - hasattr base pyrefly ignore unbound-name Q = base Q transposed pyrefly ignore unbound-name Q = Q mT Q type ignore possibly-undefined torch autograd no_grad right_inverse Q torch Tensor - torch Tensor Q shape = shape raise ValueError f Expected matrix batch matrices shape shape f Got tensor shape Q shape Q_init = Q n k = Q size - Q size - transpose = n k transpose Q = Q mT n k = k n We always make sure always copy Q every path hasattr base Note right_inverse expm cayley If we do have use_trivialization=True we just implement inverse forward map Householder To see why think Cayley map we would need find matrix X \in R^ n x k such Y = torch cat X tril X new_zeros n n - k expand X shape - - - dim=- A = Y - Y mH cayley A k gives original tensor It clear how do Perhaps via some algebraic manipulation involving QR like Corollary Edelman Arias Smith orthogonal_map == _OrthMaps cayley orthogonal_map == _OrthMaps matrix_exp raise NotImplementedError It possible assign matrix exponential Cayley parametrizations when use_trivialization=False If parametrization == _OrthMaps householder make Q orthogonal via QR decomposition Here Q always real because we do support householder complex matrices See note Householder complex A tau = torch geqrf Q We want have decomposition X = QR diag R otherwise we could decompose orthogonal matrix Q Q = -Q -Id which valid QR decomposition The diagonal Q diagonal R qr decomposition A diagonal dim =- dim =- sign_ Equality zero ok because LAPACK returns exactly zero when does want use particular reflection A diagonal dim =- dim =- tau == = - A mT transpose A n == k We check whether Q orthogonal _is_orthogonal Q Q = _make_orthogonal Q Is orthogonal Q = Q clone Complete Q into full n x n orthogonal matrix N = torch randn Q size - + n n - k dtype=Q dtype device=Q device Q = torch cat Q N dim=- Q = _make_orthogonal Q base = Q It necessary -Id we use diagonal Householder parametrization Using -Id makes householder torch zeros m n == torch eye m n Poor man s version eye_like neg_Id = torch zeros_like Q_init neg_Id diagonal dim =- dim =- fill_ - neg_Id orthogonal module Module name str = weight orthogonal_map Optional str = None use_trivialization bool = True - Module r Apply orthogonal unitary parametrization matrix batch matrices Letting math ` \mathbb K ` math ` \mathbb R ` math ` \mathbb C ` parametrized matrix math ` Q \in \mathbb K ^ m \times n ` orthogonal math \begin align Q^ \text H Q = \mathrm I _n \mathrlap \qquad \text m \geq n \\ QQ^ \text H = \mathrm I _m \mathrlap \qquad \text m n \end align where math ` Q^ \text H ` conjugate transpose when math ` Q ` complex transpose when math ` Q ` real-valued math ` \mathrm I _n ` ` n ` -dimensional identity matrix In plain words math ` Q ` will have orthonormal columns whenever math ` m \geq n ` orthonormal rows otherwise If tensor has more than two dimensions we consider batch matrices shape ` m n ` The matrix math ` Q ` may parametrized via three different ` ` orthogonal_map ` ` terms original tensor - ` ` matrix_exp ` ` ` ` cayley ` ` func ` ~torch matrix_exp ` math ` Q = \exp A ` ` Cayley map ` _ math ` Q = \mathrm I _n + A \mathrm I _n - A ^ - ` applied skew-symmetric math ` A ` give orthogonal matrix - ` ` householder ` ` computes product Householder reflectors func ` ~torch linalg householder_product ` ` ` matrix_exp ` ` ` ` cayley ` ` often make parametrized weight converge faster than ` ` householder ` ` they slower compute very thin very wide matrices If ` ` use_trivialization=True ` ` default parametrization implements Dynamic Trivialization Framework where extra matrix math ` B \in \mathbb K ^ n \times n ` stored under ` ` module parametrizations weight base ` ` This helps convergence parametrized layer expense some extra memory use See ` Trivializations Gradient-Based Optimization Manifolds ` _ Initial value math ` Q ` If original tensor parametrized ` ` use_trivialization=True ` ` default initial value math ` Q ` original tensor orthogonal unitary complex case orthogonalized via QR decomposition otherwise see func ` torch linalg qr ` Same happens when parametrized ` ` orthogonal_map= householder ` ` even when ` ` use_trivialization=False ` ` Otherwise initial value result composition all registered parametrizations applied original tensor note This function implemented using parametrization functionality func ` ~torch nn utils parametrize register_parametrization ` _ ` Cayley map ` https en wikipedia org wiki Cayley_transform#Matrix_map _ ` Trivializations Gradient-Based Optimization Manifolds ` https arxiv org abs Args module nn Module module which register parametrization name str optional name tensor make orthogonal Default ` ` weight ` ` orthogonal_map str optional One following ` ` matrix_exp ` ` ` ` cayley ` ` ` ` householder ` ` Default ` ` matrix_exp ` ` matrix square complex ` ` householder ` ` otherwise use_trivialization bool optional whether use dynamic trivialization framework Default ` ` True ` ` Returns The original module orthogonal parametrization registered specified weight Example xdoctest +REQUIRES env TORCH_DOCTEST_LAPACK orth_linear = orthogonal nn Linear orth_linear ParametrizedLinear in_features= out_features= bias=True parametrizations ModuleDict weight ParametrizationList _Orthogonal xdoctest +IGNORE_WANT Q = orth_linear weight torch dist Q T Q torch eye tensor e- weight = getattr module name None isinstance weight Tensor raise ValueError f Module module has no parameter buffer name name We could implement -dim tensors maps sphere I believe d bite more people than d help weight ndim raise ValueError Expected matrix batch matrices f Got tensor weight ndim dimensions orthogonal_map None orthogonal_map = matrix_exp weight size - == weight size - weight is_complex householder orth_enum = getattr _OrthMaps orthogonal_map None orth_enum None raise ValueError orthogonal_map has one matrix_exp cayley householder f Got orthogonal_map orth = _Orthogonal weight orth_enum use_trivialization=use_trivialization parametrize register_parametrization module name orth unsafe=True module _WeightNorm Module __init__ dim Optional int = - None super __init__ dim None dim = - dim = dim forward weight_g weight_v torch _weight_norm weight_v weight_g dim right_inverse weight weight_g = torch norm_except_dim weight dim weight_v = weight weight_g weight_v weight_norm module Module name str = weight dim int = r Apply weight normalization parameter given module math \mathbf w = g \dfrac \mathbf v \ &#124; \mathbf v \ &#124; Weight normalization reparameterization decouples magnitude weight tensor its direction This replaces parameter specified attr ` name ` two parameters one specifying magnitude one specifying direction By default ` ` dim= ` ` norm computed independently per output channel plane To compute norm over entire weight tensor use ` ` dim=None ` ` See https arxiv org abs Args module Module containing module name str optional name weight parameter dim int optional dimension over which compute norm Returns The original module weight norm hook Example m = weight_norm nn Linear name= weight m ParametrizedLinear in_features= out_features= bias=True parametrizations ModuleDict weight ParametrizationList _WeightNorm m parametrizations weight original size torch Size m parametrizations weight original size torch Size _weight_norm = _WeightNorm dim parametrize register_parametrization module name _weight_norm unsafe=True _weight_norm_compat_hook state_dict prefix local_metadata strict missing_keys unexpected_keys error_msgs g_key = f prefix name _g v_key = f prefix name _v g_key state_dict v_key state_dict original = state_dict pop g_key original = state_dict pop v_key state_dict f prefix parametrizations name original = original state_dict f prefix parametrizations name original = original module _register_load_state_dict_pre_hook _weight_norm_compat_hook module _SpectralNorm Module __init__ weight torch Tensor n_power_iterations int = dim int = eps float = e- - None super __init__ ndim = weight ndim dim = ndim dim -ndim raise IndexError Dimension out range expected range f - ndim ndim - got dim n_power_iterations = raise ValueError Expected n_power_iterations positive f got n_power_iterations= n_power_iterations dim = dim dim = dim + ndim eps = eps ndim For ndim == we do need approximate anything see _SpectralNorm forward n_power_iterations = n_power_iterations weight_mat = _reshape_weight_to_matrix weight h w = weight_mat size u = weight_mat new_empty h normal_ v = weight_mat new_empty w normal_ register_buffer _u F normalize u dim= eps=self eps register_buffer _v F normalize v dim= eps=self eps Start u v initialized some reasonable values performing number iterations power method _power_method weight_mat _reshape_weight_to_matrix weight torch Tensor - torch Tensor Precondition assert weight ndim dim = permute dim front weight = weight permute dim d d range weight dim d = dim weight flatten torch autograd no_grad _power_method weight_mat torch Tensor n_power_iterations int - None See original note torch nn utils spectral_norm py NB If ` do_power_iteration ` set ` u ` ` v ` vectors updated power iteration in-place This very important because ` DataParallel ` forward vectors being buffers broadcast parallelized module each module replica which new module object created fly And each replica runs its own spectral norm power iteration So simply assigning updated vectors module function runs will cause update lost forever And next time parallelized module replicated same randomly initialized vectors broadcast used Therefore make change propagate back we rely two important behaviors also enforced via tests ` DataParallel ` doesn t clone storage broadcast tensor already correct device makes sure parallelized module already ` device ` If out tensor ` out= ` kwarg has correct shape will just fill values Therefore since same power iteration performed all devices simply updating tensors in-place will make sure module replica ` device ` will update _u vector parallelized module shared storage However after we update ` u ` ` v ` in-place we need clone them before using them normalize weight This support backproping through two forward passes e g common pattern GAN training loss = D real - D fake Otherwise engine will complain variables needed do backward first forward i e ` u ` ` v ` vectors changed second forward Precondition assert weight_mat ndim _ range n_power_iterations Spectral norm weight equals ` u^T W v ` where ` u ` ` v ` first left right singular vectors This power iteration produces approximations ` u ` ` v ` _u = F normalize torch mv weight_mat _v type ignore has-type dim= eps=self eps out=self _u type ignore has-type _v = F normalize torch mv weight_mat H _u type ignore has-type dim= eps=self eps out=self _v type ignore has-type forward weight torch Tensor - torch Tensor weight ndim == Faster more exact path no need approximate anything F normalize weight dim= eps=self eps weight_mat = _reshape_weight_to_matrix weight training _power_method weight_mat n_power_iterations See above why we need clone u = _u clone memory_format=torch contiguous_format v = _v clone memory_format=torch contiguous_format The proper way computing should through F bilinear seems have some efficiency issues https github com pytorch pytorch issues sigma = torch vdot u torch mv weight_mat v weight sigma right_inverse value torch Tensor - torch Tensor we may want assert here passed value already satisfies constraints value spectral_norm module Module name str = weight n_power_iterations int = eps float = e- dim Optional int = None - Module r Apply spectral normalization parameter given module math \mathbf W _ SN = \dfrac \mathbf W \sigma \mathbf W \sigma \mathbf W = \max_ \mathbf h \mathbf h \ne \dfrac \ &#124; \mathbf W \mathbf h \ &#124; _ \ &#124; \mathbf h \ &#124; _ When applied vector simplifies math \mathbf x _ SN = \dfrac \mathbf x \ &#124; \mathbf x \ &#124; _ Spectral normalization stabilizes training discriminators critics Generative Adversarial Networks GANs reducing Lipschitz constant model math ` \sigma ` approximated performing one iteration ` power method ` _ every time weight accessed If dimension weight tensor greater than reshaped D power iteration method get spectral norm See ` Spectral Normalization Generative Adversarial Networks ` _ _ ` power method ` https en wikipedia org wiki Power_iteration _ ` Spectral Normalization Generative Adversarial Networks ` https arxiv org abs note This function implemented using parametrization functionality func ` ~torch nn utils parametrize register_parametrization ` It reimplementation func ` torch nn utils spectral_norm ` note When constraint registered singular vectors associated largest singular value estimated rather than sampled random These then updated performing attr ` n_power_iterations ` ` power method ` _ whenever tensor accessed module ` training ` mode note If ` _SpectralNorm ` module i e ` module parametrization weight idx ` training mode removal will perform another power iteration If you d like avoid iteration set module eval mode before its removal Args module nn Module containing module name str optional name weight parameter Default ` ` weight ` ` n_power_iterations int optional number power iterations calculate spectral norm Default ` ` ` ` eps float optional epsilon numerical stability calculating norms Default ` ` e- ` ` dim int optional dimension corresponding number outputs Default ` ` ` ` except modules instances ConvTranspose d when ` ` ` ` Returns The original module new parametrization registered specified weight Example xdoctest +REQUIRES env TORCH_DOCTEST_LAPACK xdoctest +IGNORE_WANT non-deterministic snm = spectral_norm nn Linear snm ParametrizedLinear in_features= out_features= bias=True parametrizations ModuleDict weight ParametrizationList _SpectralNorm torch linalg matrix_norm snm weight tensor grad_fn= AmaxBackward weight = getattr module name None isinstance weight Tensor raise ValueError f Module module has no parameter buffer name name dim None isinstance module torch nn ConvTranspose d torch nn ConvTranspose d torch nn ConvTranspose d dim = dim = parametrize register_parametrization module name _SpectralNorm weight n_power_iterations dim eps module