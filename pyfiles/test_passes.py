PYTEST_DONT_REWRITE prevents pytest rewriting assertions which interferes test_functionalization_with_native_python_assertion copy Owner s oncall export math operator unittest re escape torch functorch experimental control_flow cond torch _dynamo eval_frame is_dynamo_supported torch _export config torch _export non_strict_utils _fakify_script_objects _gather_constant_attrs torch _export pass_base _ExportPassBaseDeprecatedDoNotUse torch _export passes replace_set_grad_with_hop_pass _is_set_grad_enabled_node _is_set_grad_enabled_sub_mod torch _export passes replace_view_ops_with_view_copy_ops_pass get_view_copy_of_view_op is_view_op ReplaceViewOpsWithViewCopyOpsPass torch _export utils node_inline_ nodes_count nodes_filter nodes_map sequential_split torch _higher_order_ops auto_functionalize auto_functionalized torch _subclasses fake_tensor FakeTensorMode torch export export torch export _remove_auto_functionalized_pass unsafe_remove_auto_functionalized_pass torch export _remove_effect_tokens_pass _remove_effect_tokens torch export passes move_to_device_pass torch fx experimental symbolic_shapes ShapeEnv torch fx passes infra partitioner Partition torch fx passes operator_support OperatorSupport torch library _scoped_library impl torch testing FileCheck torch testing _internal common_cuda TEST_CUDA torch testing _internal common_utils IS_WINDOWS run_tests skipIfTorchDynamo TestCase torch testing _internal torchbind_impls init_torchbind_implementations torch utils _pytree pytree count_call_function graph torch fx Graph target torch ops OpOverload - int count = node graph nodes node op == call_function node target == target count += count _AddOperatorSupport OperatorSupport is_node_supported submodules node torch fx Node - bool node op == call_function node target operator add _AtenAddOperatorSupport OperatorSupport is_node_supported submodules node torch fx Node - bool node op == call_function node target torch ops aten add Tensor _to_partition_names partitions list Partition - list set str n name n p nodes p partitions _get_output_names gm torch fx GraphModule - list str output_node = next n n gm graph nodes n op == output args = pytree tree_leaves output_node args isinstance args tuple len args == args = args str arg arg args ModelsWithScriptObjectAttr Simple torch nn Module __init__ - None super __init__ attr = torch classes _TorchScriptTesting _Foo SimpleWithAttrInContainer torch nn Module __init__ - None super __init__ attr = torch classes _TorchScriptTesting _Foo pytree_attr = torch classes _TorchScriptTesting _Foo torch classes _TorchScriptTesting _Foo foo torch classes _TorchScriptTesting _Foo NestedWithAttrInContainer torch nn Module __init__ - None super __init__ attr = torch classes _TorchScriptTesting _Foo pytree_attr = torch classes _TorchScriptTesting _Foo torch classes _TorchScriptTesting _Foo foo torch classes _TorchScriptTesting _Foo sub_mod = ModelsWithScriptObjectAttr Simple sub_mod = ModelsWithScriptObjectAttr SimpleWithAttrInContainer MoreNestedWithAttrInContainer torch nn Module __init__ - None super __init__ attr = torch classes _TorchScriptTesting _Foo pytree_attr = torch classes _TorchScriptTesting _Foo torch classes _TorchScriptTesting _Foo foo torch classes _TorchScriptTesting _Foo sub_mod = ModelsWithScriptObjectAttr Simple sub_mod = ModelsWithScriptObjectAttr NestedWithAttrInContainer _set_grad_enabled_tests torch export _trace _export SetGradOp torch nn Module forward x x = x + torch _C _set_grad_enabled True c = x sin sum torch _C _set_grad_enabled False d = c + torch _C _set_grad_enabled True e = d - d e SetGradCtxManager torch nn Module forward x x = x + torch enable_grad c = x sin sum torch no_grad d = c + torch enable_grad e = d - d e SetGradCtxManagerMultiDep torch nn Module forward x x = x + torch enable_grad c = x sin sum c = x cos sum torch no_grad d = c + d = c + torch enable_grad e = d - e = d - d d e e x = torch randn _get_predispatch_module mod args ambient_grad_enabled=True torch set_grad_enabled ambient_grad_enabled config patch use_new_tracer_experimental=True _export mod args pre_dispatch=True module ctx_manager SetGradCtxManager _get_predispatch_module SetGradCtxManager x x ctx_manager_under_no_grad SetGradCtxManager _get_predispatch_module SetGradCtxManager x False x ctx_manager_multi_dep SetGradCtxManagerMultiDep _get_predispatch_module SetGradCtxManagerMultiDep x x ctx_manager_multi_dep_no_grad SetGradCtxManagerMultiDep _get_predispatch_module SetGradCtxManagerMultiDep x False x op SetGradOp _get_predispatch_module SetGradOp x x op_under_no_grad SetGradOp _get_predispatch_module SetGradOp x False x _with_autocast_tests torch export _trace _export WithAutocastOp torch nn Module forward x x = x + torch autocast device_type= cpu enabled=True c = x sin sum torch autocast device_type= cpu enabled=False d = c + torch autocast device_type= cpu enabled=True e = d - d e WithAutocastOpMultiDep torch nn Module forward x x = x + torch autocast device_type= cpu enabled=True c = x sin sum c = x cos sum torch autocast device_type= cpu enabled=False d = c + d = c + torch autocast device_type= cpu enabled=True e = d - e = d - d d e e SplitAutocastOp torch nn Module forward x x = x + torch autocast device_type= cpu enabled=True c = x sin sum d = c + torch autocast device_type= cpu enabled=True e = d - d e NestedAutocastOp torch nn Module forward x x = x + torch autocast device_type= cpu enabled=True c = x sin sum torch autocast device_type= cpu enabled=False d = c + torch autocast device_type= cpu enabled=True e = d - d e x = torch randn _get_predispatch_module mod args _export mod args pre_dispatch=True module ctx_manager WithAutocastOp _get_predispatch_module WithAutocastOp x x ctx_manager_multi_dep WithAutocastOpMultiDep _get_predispatch_module WithAutocastOpMultiDep x x ctx_manager_split SplitAutocastOp _get_predispatch_module SplitAutocastOp x x ctx_manager_nested NestedAutocastOp _get_predispatch_module NestedAutocastOp x x _with_mixed_autocast_set_grad_tests torch export _trace _export WithAutocastSetGradOp torch nn Module forward x x = x + torch _C _set_grad_enabled True c = x sin torch _C _set_grad_enabled False c = c cos torch autocast device_type= cpu enabled=False d = c + e = d - d e x = torch randn _get_predispatch_module mod args torch _export config patch use_new_tracer_experimental=True ep = _export mod args pre_dispatch=True module ep multi_ctx_manager WithAutocastSetGradOp _get_predispatch_module WithAutocastSetGradOp x x _sequential_split_inline_tests torch export _trace _export Simple torch nn Module forward x x = x + c = x sin sum d = c + e = d - d e MultiDep torch nn Module forward x x x = x + x = x + c = x sin c = x cos d = c + d = c + e = d - e = d - d d e e _get_predispatch_module mod args _export mod args pre_dispatch=True module _insert_dilimiter_nodes gm torch fx GraphModule step int = insert_locs = i node enumerate nodes_filter gm graph nodes lambda n n op == call_function i step == insert_locs append node i node enumerate insert_locs gm graph inserting_before node gm graph call_function torch _C _set_grad_enabled i == gm x = torch randn simple = _get_predispatch_module Simple x simple = _get_predispatch_module Simple x multi_dep = _get_predispatch_module MultiDep x x sin multi_dep = _get_predispatch_module MultiDep x x sin simple_step _insert_dilimiter_nodes simple x simple_step _insert_dilimiter_nodes simple x multi_dep_step _insert_dilimiter_nodes multi_dep x x sin multi_dep_step _insert_dilimiter_nodes multi_dep x x sin skipIfTorchDynamo recursively running dynamo export unlikely unittest skipIf is_dynamo_supported Dynamo supported TestPasses TestCase setUp super setUp MIXED_AUTOCAST_SET_GRAD_TESTS = _with_mixed_autocast_set_grad_tests SEQUENTIAL_SPLIT_INLINE_TESTS = _sequential_split_inline_tests SET_GRAD_ENABLED_TESTS = _set_grad_enabled_tests WITH_AUTOCAST_TESTS = _with_autocast_tests init_torchbind_implementations tearDown SEQUENTIAL_SPLIT_INLINE_TESTS clear SET_GRAD_ENABLED_TESTS clear WITH_AUTOCAST_TESTS clear MIXED_AUTOCAST_SET_GRAD_TESTS clear super tearDown _check_node_users_in_the_same_graph gm node gm graph nodes user node users assertTrue user graph gm graph test_runtime_assert_one_dim - None M torch nn Module __init__ - None super __init__ forward x x cos x = torch zeros dim _x = torch export Dim dim _x min= max= ep = torch export export M x dynamic_shapes= x dim _x strict=True assertRaisesRegex AssertionError escape Guard failed x size = expected = got ep module torch zeros assertEqual ep module torch ones M forward torch ones test_runtime_assert_multiple_dims - None M torch nn Module __init__ - None super __init__ forward x y x cos sum + y sin sum x = torch zeros y = torch zeros dim _x = torch export Dim dim _x min= max= dim _x dim _y = torch export dims dim _x dim _y min= ep = torch export export M x y dynamic_shapes= x dim _x dim _x y dim _y strict=True assertRaisesRegex AssertionError escape Guard failed x size = expected = got ep module torch zeros torch ones assertRaisesRegex AssertionError escape Guard failed y size = expected = got ep module torch zeros torch ones test_runtime_assert_some_dims_not_specified - None M torch nn Module __init__ - None super __init__ forward x y x cos sum + y sin sum x = torch zeros y = torch zeros dim _x = torch export Dim dim _x min= max= dim _x = torch export Dim dim _x min= ep = torch export export M x y dynamic_shapes= x dim _x dim _x y None strict=True assertRaisesRegex AssertionError escape Guard failed x size = expected = got ep module torch zeros torch ones y specialized assertRaisesRegex AssertionError escape Guard failed y size == expected got ep module torch zeros torch ones Since we didn t insert constraint x = should work case where x == gm_result_for_ _size = ep module torch ones torch ones eager_result_for_ _size = M forward torch ones torch ones assertEqual gm_result_for_ _size eager_result_for_ _size test_runtime_assert_some_inps_not_used - None M torch nn Module __init__ - None super __init__ forward x y y cos sum x = torch zeros y = torch zeros dim _y = torch export Dim dim _y min= max= ep = torch export export M x y dynamic_shapes= x None y dim _y strict=True assertRaisesRegex AssertionError escape Guard failed x size == expected got ep module torch zeros torch ones y specialized assertRaisesRegex AssertionError escape Guard failed y size == expected got ep module torch zeros torch ones Since we didn t insert constraint x = should work case where x == gm_result_for_ _size = ep module torch zeros torch ones eager_result_for_ _size = M forward torch zeros torch ones assertEqual gm_result_for_ _size eager_result_for_ _size test_view_to_view_copy - None M torch nn Module __init__ - None super __init__ forward x z = x view x shape z cos sum x = torch zeros ep = export M x strict=True assertEqual count_call_function ep graph torch ops aten view default ep = ep _transform_do_not_use ReplaceViewOpsWithViewCopyOpsPass assertEqual count_call_function ep graph torch ops aten view default test_functionalization_with_view_copy - None Module torch nn Module forward x y = x + y add_ z = y view y shape x cos + z cos x = torch zeros foo = Module ep = export foo x strict=True _transform_do_not_use ReplaceViewOpsWithViewCopyOpsPass After pass there shouldn t any view nodes graph assertTrue count_call_function ep graph torch ops aten view default == assertTrue count_call_function ep graph torch ops aten view_copy default test_views_op_having_view_copy - None schemas = torch _C _dispatch_get_registrations_for_dispatch_key aten_schemas = s s schemas s startswith aten aten_schema aten_schemas val = aten_schema split assert len val = name = overload = len val == name = val overload = default name overload = val val op_overload = getattr getattr torch ops aten name overload torch Tag core op_overload tags is_view_op op_overload _schema assertIsNotNone get_view_copy_of_view_op op_overload _schema test_custom_obj_tuple_out MyModule torch nn Module __init__ - None super __init__ attr = torch classes _TorchScriptTesting _Foo forward x = torch ops _TorchScriptTesting takes_foo_tuple_return attr x y = + b = torch ops _TorchScriptTesting takes_foo attr y b m = MyModule inputs = torch ones ep = torch export export m inputs strict=False inp = torch randn orig_res = m inp ep_res = ep module inp without_token_ep = _remove_effect_tokens ep without_token_ep verifier check without_token_ep without_token_res = without_token_ep module inp assertTrue torch allclose orig_res ep_res assertTrue torch allclose orig_res without_token_res test_remove_effect_token_kwargs MyModule torch nn Module __init__ - None super __init__ attr = torch classes _TorchScriptTesting _Foo forward x = torch ops _TorchScriptTesting takes_foo_tuple_return foo=self attr x=x y = + b = torch ops _TorchScriptTesting takes_foo foo=self attr x=y b m = MyModule inputs = torch ones ep = export m inputs strict=False run_decompositions without_token_ep = _remove_effect_tokens ep assertExpectedInline without_token_ep graph_module code strip \ forward token obj_attr x with_effects = torch ops higher_order with_effects token torch ops _TorchScriptTesting takes_foo_tuple_return default foo = obj_attr x = x token = x = None getitem = with_effects getitem_ = with_effects getitem_ = with_effects with_effects = None add = torch ops aten add Tensor getitem_ getitem_ getitem_ = getitem_ = None with_effects_ = torch ops higher_order with_effects getitem torch ops _TorchScriptTesting takes_foo default foo = obj_attr x = add getitem = obj_attr = add = None getitem_ = with_effects_ getitem_ = with_effects_ with_effects_ = None getitem_ getitem_ noqa B test_fakify_script_objects m ModelsWithScriptObjectAttr Simple ModelsWithScriptObjectAttr SimpleWithAttrInContainer ModelsWithScriptObjectAttr NestedWithAttrInContainer ModelsWithScriptObjectAttr MoreNestedWithAttrInContainer constant_attrs = _gather_constant_attrs m fake_mode = FakeTensorMode shape_env=ShapeEnv tracked_fakes= allow_non_fake_inputs=True _fakify_script_objects m fake_mode _ _ _ fake_constant_attrs fake_to_real assertEqual len fake_constant_attrs len constant_attrs fake_obj fqn fake_constant_attrs items assertEqual constant_attrs fake_to_real fake_obj fqn TODO _gather_constants doesn t recursively look into pytree containers unittest expectedFailure test_fakify_script_objects_properly_handle_containers m = ModelsWithScriptObjectAttr SimpleWithAttrInContainer fake_mode = FakeTensorMode shape_env=ShapeEnv tracked_fakes= allow_non_fake_inputs=True _fakify_script_objects m fake_mode _ _ _ fake_constant_attrs _ assertTrue attr fake_constant_attrs values assertTrue pytree_attr fake_constant_attrs values test_runtime_assert_inline_constraints_for_item - None M torch nn Module __init__ - None super __init__ forward x b = x item torch _check b = torch _check b = b x = torch tensor mod = M ep = export mod x strict=True assertRaisesRegex RuntimeError r Runtime assertion failed expression u \d+ \ \= ep module torch tensor new_inp = torch tensor assertEqual mod new_inp ep module new_inp test_runtime_assert_inline_constraints_for_nonzero - None M torch nn Module __init__ - None super __init__ forward x b = x nonzero torch _check b shape = torch _check b shape = b x = torch tensor mod = M dim _x = torch export Dim dim _x ep = torch export export mod x dynamic_shapes= x dim _x strict=True num_assert = count_call_function ep graph torch ops aten _assert_scalar default assertEqual num_assert num_constrain_range = count_call_function ep graph torch ops aten sym_constrain_range default assertEqual num_constrain_range assertRaisesRegex RuntimeError r Runtime assertion failed expression u \d+ \ \= ep module torch tensor assertRaisesRegex RuntimeError r Runtime assertion failed expression u \d+ \ \= ep module torch ones new_inp = torch tensor assertEqual mod new_inp ep module new_inp unittest skipIf IS_WINDOWS Windows supported unittest expectedFailure TODO pianpwk add back runtime asserts subgraphs test_runtime_assert_inline_constraints_for_cond - None M torch nn Module __init__ - None super __init__ forward pred x y true_fn x y b = x item torch _check b = torch _check b = x - b false_fn x y c = y item torch _check c = torch _check c = y - c ret = cond pred true_fn false_fn x y ret x = torch tensor y = torch tensor mod = M ep = export mod torch tensor True x y strict=True assertRaisesRegex RuntimeError outside inline constraint \\ \\ ep module torch tensor False torch tensor torch tensor test_math_ops Module torch nn Module forward x torch tensor math ceil x item torch tensor math floor x item func = Module x = torch randn dtype=torch float ep = torch export export func args= x strict=True _ExportPassBaseDeprecatedDoNotUse ep graph_module test_predispatch_set_grad mod_orig mod args = SET_GRAD_ENABLED_TESTS op _check_node_users_in_the_same_graph mod assertEqual mod_orig args mod args assertExpectedInline mod code strip \n \ forward x x = fx_pytree tree_flatten_spec x _in_spec _guards_fn = _guards_fn x _guards_fn = None add = torch ops aten add Tensor x x = None sin = torch ops aten sin default add add = None sum_ = torch ops aten sum default sin sin = None submod_ = submod_ add_ = torch ops higher_order wrap_with_set_grad_enabled False submod_ sum_ submod_ = sum_ = None getitem = add_ add_ = None sub = torch ops aten sub Tensor getitem pytree tree_unflatten getitem sub _out_spec mod_orig mod args = SET_GRAD_ENABLED_TESTS op_under_no_grad _check_node_users_in_the_same_graph mod assertEqual mod_orig args mod args assertExpectedInline mod code strip \n \ forward x x = fx_pytree tree_flatten_spec x _in_spec _guards_fn = _guards_fn x _guards_fn = None add = torch ops aten add Tensor x x = None submod_ = submod_ sum_ = torch ops higher_order wrap_with_set_grad_enabled True submod_ add submod_ = add = None getitem = sum_ sum_ = None add_ = torch ops aten add Tensor getitem getitem = None submod_ = submod_ sub = torch ops higher_order wrap_with_set_grad_enabled True submod_ add_ submod_ = None getitem_ = sub sub = None pytree tree_unflatten add_ getitem_ _out_spec mod_orig mod args = SET_GRAD_ENABLED_TESTS ctx_manager _check_node_users_in_the_same_graph mod assertEqual mod_orig args mod args assertExpectedInline mod code strip \n \ forward x x = fx_pytree tree_flatten_spec x _in_spec _guards_fn = _guards_fn x _guards_fn = None add = torch ops aten add Tensor x x = None sin = torch ops aten sin default add add = None sum_ = torch ops aten sum default sin sin = None submod_ = submod_ add_ = torch ops higher_order wrap_with_set_grad_enabled False submod_ sum_ submod_ = sum_ = None getitem = add_ add_ = None sub = torch ops aten sub Tensor getitem pytree tree_unflatten getitem sub _out_spec mod_orig mod args = SET_GRAD_ENABLED_TESTS ctx_manager_under_no_grad _check_node_users_in_the_same_graph mod assertEqual mod_orig args mod args assertExpectedInline mod code strip \n \ forward x x = fx_pytree tree_flatten_spec x _in_spec _guards_fn = _guards_fn x _guards_fn = None add = torch ops aten add Tensor x x = None submod_ = submod_ sum_ = torch ops higher_order wrap_with_set_grad_enabled True submod_ add submod_ = add = None getitem = sum_ sum_ = None add_ = torch ops aten add Tensor getitem getitem = None submod_ = submod_ sub = torch ops higher_order wrap_with_set_grad_enabled True submod_ add_ submod_ = None getitem_ = sub sub = None pytree tree_unflatten add_ getitem_ _out_spec mod_orig mod args = SET_GRAD_ENABLED_TESTS ctx_manager_multi_dep _check_node_users_in_the_same_graph mod assertEqual mod_orig args mod args assertExpectedInline mod code strip \n \ forward x x = fx_pytree tree_flatten_spec x _in_spec _guards_fn = _guards_fn x _guards_fn = None add = torch ops aten add Tensor x x = None sin = torch ops aten sin default add sum_ = torch ops aten sum default sin sin = None cos = torch ops aten cos default add add = None sum_ = torch ops aten sum default cos cos = None submod_ = submod_ wrap_with_set_grad_enabled = torch ops higher_order wrap_with_set_grad_enabled False submod_ sum_ sum_ submod_ = sum_ = sum_ = None add_ = wrap_with_set_grad_enabled add_ = wrap_with_set_grad_enabled wrap_with_set_grad_enabled = None sub = torch ops aten sub Tensor add_ sub_ = torch ops aten sub Tensor add_ pytree tree_unflatten add_ add_ sub sub_ _out_spec noqa B mod_orig mod args = SET_GRAD_ENABLED_TESTS ctx_manager_multi_dep_no_grad _check_node_users_in_the_same_graph mod assertEqual mod_orig args mod args assertExpectedInline mod code strip \n \ forward x x = fx_pytree tree_flatten_spec x _in_spec _guards_fn = _guards_fn x _guards_fn = None add = torch ops aten add Tensor x x = None submod_ = submod_ wrap_with_set_grad_enabled = torch ops higher_order wrap_with_set_grad_enabled True submod_ add submod_ = add = None sum_ = wrap_with_set_grad_enabled sum_ = wrap_with_set_grad_enabled wrap_with_set_grad_enabled = None add_ = torch ops aten add Tensor sum_ sum_ = None add_ = torch ops aten add Tensor sum_ sum_ = None submod_ = submod_ wrap_with_set_grad_enabled_ = torch ops higher_order wrap_with_set_grad_enabled True submod_ add_ add_ submod_ = None sub = wrap_with_set_grad_enabled_ sub_ = wrap_with_set_grad_enabled_ wrap_with_set_grad_enabled_ = None pytree tree_unflatten add_ add_ sub sub_ _out_spec noqa B test_sequential_split gm args SEQUENTIAL_SPLIT_INLINE_TESTS values set_grad_counts = nodes_count gm graph nodes _is_set_grad_enabled_node new_gm = sequential_split gm _is_set_grad_enabled_node new_set_grad_counts = nodes_count new_gm graph nodes _is_set_grad_enabled_sub_mod assertEqual set_grad_counts new_set_grad_counts assertEqual gm args new_gm args test_sequential_split_graph gm args = SEQUENTIAL_SPLIT_INLINE_TESTS multi_dep_step new_gm = sequential_split gm _is_set_grad_enabled_node assertEqual gm args new_gm args assertExpectedInline new_gm code strip \n \ forward x x x x = fx_pytree tree_flatten_spec x x _in_spec submod_ = submod_ x x submod_ = None submod_ = submod_ x x x = x = None getitem = submod_ getitem_ = submod_ submod_ = None submod_ = submod_ getitem getitem_ getitem = getitem_ = None getitem_ = submod_ getitem_ = submod_ submod_ = None submod_ = submod_ getitem_ getitem_ getitem_ = getitem_ = None getitem_ = submod_ getitem_ = submod_ submod_ = None submod_ = submod_ getitem_ getitem_ getitem_ = submod_ getitem_ = submod_ submod_ = None pytree tree_unflatten getitem_ getitem_ getitem_ getitem_ _out_spec assertExpectedInline new_gm submod_ code strip \n \ forward x x _set_grad_enabled = torch _C _set_grad_enabled True _set_grad_enabled = None add = torch ops aten add Tensor x x = None add_ = torch ops aten add Tensor x x = None add add_ assertExpectedInline new_gm submod_ code strip \n \ forward add add_ _set_grad_enabled_ = torch _C _set_grad_enabled False _set_grad_enabled_ = None sin = torch ops aten sin default add add = None cos = torch ops aten cos default add_ add_ = None sin cos assertExpectedInline new_gm submod_ code strip \n \ forward sin cos _set_grad_enabled_ = torch _C _set_grad_enabled True _set_grad_enabled_ = None add_ = torch ops aten add Tensor sin sin = None add_ = torch ops aten add Tensor cos cos = None add_ add_ test_predispatch_autocast_and_set_grad mod_orig mod args = MIXED_AUTOCAST_SET_GRAD_TESTS multi_ctx_manager _check_node_users_in_the_same_graph mod assertEqual mod_orig args mod args assertExpectedInline mod code strip \n \ forward x x = fx_pytree tree_flatten_spec x _in_spec _guards_fn = _guards_fn x _guards_fn = None add = torch ops aten add Tensor x x = None sin = torch ops aten sin default add add = None submod_ = submod_ wrap_with_set_grad_enabled = torch ops higher_order wrap_with_set_grad_enabled False submod_ sin submod_ = sin = None add_ = wrap_with_set_grad_enabled sub = wrap_with_set_grad_enabled wrap_with_set_grad_enabled = None pytree tree_unflatten add_ sub _out_spec assertExpectedInline mod submod_ code strip \n \ forward sin cos = torch ops aten cos default sin sin = None submod_ = submod_ add_ = torch ops higher_order wrap_with_autocast cpu None False None submod_ cos submod_ = cos = None getitem = add_ add_ = None sub = torch ops aten sub Tensor getitem getitem sub assertExpectedInline mod submod_ submod_ code strip \n \ forward cos add_ = torch ops aten add Tensor cos cos = None add_ test_predispatch_autocast mod_orig mod args = WITH_AUTOCAST_TESTS ctx_manager_nested _check_node_users_in_the_same_graph mod assertEqual mod_orig args mod args assertExpectedInline mod code strip \n \ forward x x = fx_pytree tree_flatten_spec x _in_spec _guards_fn = _guards_fn x _guards_fn = None add = torch ops aten add Tensor x x = None submod_ = submod_ add_ = torch ops higher_order wrap_with_autocast cpu None True None submod_ add submod_ = add = None getitem = add_ add_ = None submod_ = submod_ sub = torch ops higher_order wrap_with_autocast cpu None True None submod_ getitem submod_ = None getitem_ = sub sub = None pytree tree_unflatten getitem getitem_ _out_spec assertExpectedInline mod submod_ code strip \n \ forward add sin = torch ops aten sin default add add = None sum_ = torch ops aten sum default sin sin = None submod_ = submod_ add_ = torch ops higher_order wrap_with_autocast cpu None False None submod_ sum_ submod_ = sum_ = None getitem = add_ add_ = None getitem mod_orig mod args = WITH_AUTOCAST_TESTS ctx_manager _check_node_users_in_the_same_graph mod assertEqual mod_orig args mod args assertExpectedInline mod code strip \n \ forward x x = fx_pytree tree_flatten_spec x _in_spec _guards_fn = _guards_fn x _guards_fn = None add = torch ops aten add Tensor x x = None submod_ = submod_ sum_ = torch ops higher_order wrap_with_autocast cpu None True None submod_ add submod_ = add = None getitem = sum_ sum_ = None submod_ = submod_ add_ = torch ops higher_order wrap_with_autocast cpu None False None submod_ getitem submod_ = getitem = None getitem_ = add_ add_ = None submod_ = submod_ sub = torch ops higher_order wrap_with_autocast cpu None True None submod_ getitem_ submod_ = None getitem_ = sub sub = None pytree tree_unflatten getitem_ getitem_ _out_spec assertExpectedInline mod submod_ code strip \n \ forward add sin = torch ops aten sin default add add = None sum_ = torch ops aten sum default sin sin = None sum_ assertExpectedInline mod submod_ code strip \n \ forward sum_ add_ = torch ops aten add Tensor sum_ sum_ = None add_ assertExpectedInline mod submod_ code strip \n \ forward add_ sub = torch ops aten sub Tensor add_ add_ = None sub mod_orig mod args = WITH_AUTOCAST_TESTS ctx_manager_multi_dep _check_node_users_in_the_same_graph mod assertEqual mod_orig args mod args assertExpectedInline mod code strip \n \ forward x x = fx_pytree tree_flatten_spec x _in_spec _guards_fn = _guards_fn x _guards_fn = None add = torch ops aten add Tensor x x = None submod_ = submod_ wrap_with_autocast = torch ops higher_order wrap_with_autocast cpu None True None submod_ add submod_ = add = None sum_ = wrap_with_autocast sum_ = wrap_with_autocast wrap_with_autocast = None submod_ = submod_ wrap_with_autocast_ = torch ops higher_order wrap_with_autocast cpu None False None submod_ sum_ sum_ submod_ = sum_ = sum_ = None add_ = wrap_with_autocast_ add_ = wrap_with_autocast_ wrap_with_autocast_ = None submod_ = submod_ wrap_with_autocast_ = torch ops higher_order wrap_with_autocast cpu None True None submod_ add_ add_ submod_ = None sub = wrap_with_autocast_ sub_ = wrap_with_autocast_ wrap_with_autocast_ = None pytree tree_unflatten add_ add_ sub sub_ _out_spec noqa B assertExpectedInline mod submod_ code strip \n \ forward add sin = torch ops aten sin default add sum_ = torch ops aten sum default sin sin = None cos = torch ops aten cos default add add = None sum_ = torch ops aten sum default cos cos = None sum_ sum_ assertExpectedInline mod submod_ code strip \n \ forward sum_ sum_ add_ = torch ops aten add Tensor sum_ sum_ = None add_ = torch ops aten add Tensor sum_ sum_ = None add_ add_ assertExpectedInline mod submod_ code strip \n \ forward add_ add_ sub = torch ops aten sub Tensor add_ add_ = None sub_ = torch ops aten sub Tensor add_ add_ = None sub sub_ mod_orig mod args = WITH_AUTOCAST_TESTS ctx_manager_split _check_node_users_in_the_same_graph mod assertEqual mod_orig args mod args assertExpectedInline mod code strip \n \ forward x x = fx_pytree tree_flatten_spec x _in_spec _guards_fn = _guards_fn x _guards_fn = None add = torch ops aten add Tensor x x = None submod_ = submod_ sum_ = torch ops higher_order wrap_with_autocast cpu None True None submod_ add submod_ = add = None getitem = sum_ sum_ = None add_ = torch ops aten add Tensor getitem getitem = None submod_ = submod_ sub = torch ops higher_order wrap_with_autocast cpu None True None submod_ add_ submod_ = None getitem_ = sub sub = None pytree tree_unflatten add_ getitem_ _out_spec assertExpectedInline mod submod_ code strip \n \ forward add sin = torch ops aten sin default add add = None sum_ = torch ops aten sum default sin sin = None sum_ assertExpectedInline mod submod_ code strip \n \ forward add_ sub = torch ops aten sub Tensor add_ add_ = None sub test_inline_ gm args SEQUENTIAL_SPLIT_INLINE_TESTS values before_str = gm print_readable print_output=False new_gm = sequential_split gm _is_set_grad_enabled_node nodes_map new_gm graph nodes lambda node node_inline_ node node op == call_module node after_inline_str = new_gm print_readable print_output=False assertEqual before_str after_inline_str new_gm _guards_fn = gm _guards_fn assertEqual gm args new_gm args test_remove_auto_functionalized_pass - None _scoped_library DO_NOT_USE_TEST_ONLY DEF lib lib define custom_mutator Tensor x Tensor y - Tensor impl lib custom_mutator Meta custom_mutator_meta x torch Tensor y torch Tensor - torch Tensor torch empty_like x impl lib custom_mutator CompositeExplicitAutograd custom_mutator x torch Tensor y torch Tensor - torch Tensor x + y add_ M torch nn Module __init__ - None super __init__ state = torch nn Buffer torch zeros forward x torch ops DO_NOT_USE_TEST_ONLY custom_mutator x state mod = M x = torch randn ep = export mod x strict=True inplace_ep = unsafe_remove_auto_functionalized_pass ep nodes = inplace_ep graph nodes node nodes node op == call_function assertFalse node target auto_functionalized assertFalse node target operator getitem spec inplace_ep graph_signature output_specs assertFalse getitem spec arg name test_remove_auto_functionalized_pass_tuple - None _scoped_library DO_NOT_USE_TEST_ONLY DEF lib lib define custom_mutator_tuple Tensor x Tensor y - Tensor Tensor impl lib custom_mutator_tuple Meta custom_mutator_tuple_meta x torch Tensor y torch Tensor torch empty_like x torch empty_like x impl lib custom_mutator_tuple CompositeExplicitAutograd custom_mutator_tuple x torch Tensor y torch Tensor x x + y add_ M torch nn Module __init__ - None super __init__ state = torch nn Buffer torch zeros forward x torch ops DO_NOT_USE_TEST_ONLY custom_mutator_tuple x state mod = M x = torch randn ep = export mod x strict=True run_decompositions inplace_ep = unsafe_remove_auto_functionalized_pass ep graph_text = str inplace_ep graph assertExpectedInline graph_text \ graph b_state num_users= = placeholder target=b_state x num_users= = placeholder target=x custom_mutator_tuple_default num_users= = call_function target=torch ops DO_NOT_USE_TEST_ONLY custom_mutator_tuple \ default args = x b_state kwargs = getitem_ num_users= = call_function target=operator getitem args = custom_mutator_tuple_default kwargs = getitem_ num_users= = call_function target=operator getitem args = custom_mutator_tuple_default kwargs = b_state getitem_ getitem_ unittest skipIf TEST_CUDA requires cuda test_move_device_to M torch nn Module forward x x = torch ops aten device x device= cuda dtype=torch float x + x ep = torch export export M torch ones ep = move_to_device_pass ep cuda ep graph_module recompile assertExpectedInline ep graph_module code strip \n \ forward x _assert_tensor_metadata_default = torch ops aten _assert_tensor_metadata default x dtype = torch float device = cuda layout = torch strided _assert_tensor_metadata_default = None = torch ops aten device x cuda torch float x = None add = torch ops aten add Tensor = None add noqa B unittest skipIf TEST_CUDA requires cuda test_move_device_submod M torch nn Module forward x torch autocast device_type= cuda dtype=torch bfloat x = x device= cuda x + x ep = torch export export M torch ones ep = move_to_device_pass ep cuda ep graph_module submod_ recompile assertExpectedInline ep graph_module submod_ code strip \n \ forward arg _ _assert_tensor_metadata_default = torch ops aten _assert_tensor_metadata default arg _ dtype = torch float device = cuda layout = torch strided _assert_tensor_metadata_default = None = torch ops aten dtype_layout arg _ dtype = torch float layout = torch strided device = cuda arg _ = None add = torch ops aten add Tensor = None add noqa B unittest skipIf TEST_CUDA requires cuda test_move_to_device_pass Model torch nn Module __init__ size= h_dim= super __init__ rnn = torch nn GRU size h_dim batch_first=True forward x _ states = rnn x states move exported program cpu cuda mod = Model example_inputs = torch rand ep = export mod example_inputs strict=True location = torch device cuda ep = move_to_device_pass ep location=location gm = ep module test_inputs = torch rand cuda outputs = gm test_inputs assertEqual outputs device torch device cuda move back cpu location = cpu ep = move_to_device_pass ep location=location gm = ep module test_inputs = torch rand cpu outputs = gm test_inputs assertEqual outputs device torch device cpu move cuda again location = cpu cuda ep = move_to_device_pass ep location=location gm = ep module test_inputs = torch rand cuda outputs = gm test_inputs assertEqual outputs device torch device cuda unittest skipIf TEST_CUDA requires cuda test_move_device_example_inputs Model torch nn Module __init__ super __init__ linear = torch nn Linear forward x y z linear x + y + z Create model example inputs CPU mod = Model example_args = torch rand torch rand example_kwargs = z torch tensor Export example inputs ep = export mod example_args example_kwargs Verify initial state - all tensors should CPU assertEqual ep example_inputs device torch device cpu assertEqual ep example_inputs device torch device cpu assertEqual ep example_inputs z device torch device cpu Move CUDA location = torch device cuda ep_cuda = move_to_device_pass ep location=location Verify example_inputs moved CUDA assertEqual ep_cuda example_inputs device torch device cuda assertEqual ep_cuda example_inputs device torch device cuda assertEqual ep_cuda example_inputs z device torch device cuda test_constant_folding_pass torch ao quantization observer MappingType PerGroup PerToken torch ao quantization pt e _affine_quantization AffineQuantizedMinMaxObserver torch ao quantization quantize_pt e convert_pt e prepare_pt e torch ao quantization quantizer QuantizationAnnotation QuantizationSpec Quantizer BackendAQuantizer Quantizer annotate model torch fx GraphModule - torch fx GraphModule node model graph nodes node op == call_function node target == torch ops aten linear default input_act = node args assert isinstance input_act torch fx Node weight = node args assert isinstance weight torch fx Node act_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=None is_dynamic=False observer_or_fake_quant_ctr=AffineQuantizedMinMaxObserver with_args TODO maybe align arg name here target_dtype=torch uint mapping_type=MappingType SYMMETRIC granularity=PerToken weight_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=None is_dynamic=False observer_or_fake_quant_ctr=AffineQuantizedMinMaxObserver with_args target_dtype=torch uint mapping_type=MappingType SYMMETRIC granularity=PerGroup group_size= node meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act act_qspec weight weight_qspec _annotated=True validate model torch fx GraphModule - None pass M torch nn Module __init__ super __init__ linear = torch nn Linear forward x linear x example_inputs = torch randn model = M quantizer = BackendAQuantizer m = torch export export model eval example_inputs strict=True module m = prepare_pt e m quantizer Calibration m example_inputs Get quantized model m_fold = copy deepcopy m m_fold = convert_pt e m_fold fold_quantize=True If fold check graph only contains frozed params no linear_weight FileCheck check _frozen_param check_not linear_weight run m_fold code m_not_fold = copy deepcopy m m_not_fold = convert_pt e m_not_fold fold_quantize=False If fold check graph doesn t contain frozed params contain linear_weight FileCheck check_not _frozen_param check linear_weight run m_not_fold code __name__ == __main__ run_tests