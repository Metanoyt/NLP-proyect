mypy ignore-errors unittest collections abc Sequence functools partial numpy np torch torch testing make_tensor torch testing _internal common_device_type tol toleranceOverride torch testing _internal common_dtype all_types_and all_types_and_complex_and complex_types floating_and_complex_types_and floating_types_and integral_types torch testing _internal opinfo core DecorateInfo gradcheck_wrapper_masked_operation gradcheck_wrapper_masked_pointwise_operation M OpInfo ReductionOpInfo S sample_inputs_reduction SampleInput torch testing _internal opinfo utils prod_numpy reference_reduction_numpy Used log_softmax softmax softmin sample_inputs_softmax_variant op_info device dtype requires_grad with_dtype=False use_zero_dimensions=True kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad cases = S S S S S S S - S M S S - use_zero_dimensions kwargs = dict dtype=torch float with_dtype None PyTorch XLA throws error when passed dim argument d tensor See https github com pytorch xla issues more details torch device device type = xla cases append SampleInput make_arg shape args=dim kwargs=kwargs shape dim cases _generate_masked_op_mask input_shape device kwargs make_arg = partial make_tensor dtype=torch bool device=device requires_grad=False yield None yield make_arg input_shape len input_shape broadcast last mask dimension yield make_arg input_shape - + broadcast middle mask dimension yield make_arg input_shape + + input_shape broadcast first mask dimension yield make_arg + input_shape mask ndim input ndim yield make_arg input_shape mask ndim == yield make_arg input_shape - masks require broadcasting inputs mask ndim input ndim will supported however we may reconsider there will demand kind degenerate cases sample_inputs_masked_reduction op_info device dtype requires_grad kwargs Sample inputs masked reduction operators Masked reduction operator reduction operator trailing mask optional argument A mask bool tensor same shape input shape broadcastable input shape kwargs supports_multiple_dims = op_info supports_multiple_dims sample_input sample_inputs_reduction op_info device dtype requires_grad kwargs mask _generate_masked_op_mask sample_input input shape device kwargs sample_input_args sample_input_kwargs = sample_input args dict mask=mask sample_input kwargs yield SampleInput sample_input input detach requires_grad_ requires_grad args=sample_input_args kwargs=sample_input_kwargs requires_grad dtype is_floating_point sample_input input ndim == mask None mask shape == sample_input input shape v torch inf -torch inf torch nan t = sample_input input detach t diagonal - - fill_ v yield SampleInput t requires_grad_ requires_grad args=sample_input_args kwargs=sample_input_kwargs sample_inputs_sparse_coo_masked_reduction op_info device dtype requires_grad kwargs Sample inputs masked reduction operators support inputs sparse coo layouts op_info supports_sparse op_name = op_info name replace masked sample_input sample_inputs_masked_reduction op_info device dtype requires_grad kwargs mask = sample_input kwargs get mask mask None sample_input_kwargs = sample_input kwargs copy sample_input_kwargs update mask=mask to_sparse yield SampleInput sample_input input to_sparse args=sample_input args kwargs=sample_input_kwargs op_name prod amax amin FIXME now reductions non-zero reduction identity unspecified mask supported sparse COO tensors see torch masked prod implementation details continue yield SampleInput sample_input input to_sparse args=sample_input args kwargs=sample_input kwargs sample_inputs_sparse_csr_masked_reduction op_info device dtype requires_grad kwargs Sample inputs masked reduction operators support inputs sparse csr layouts op_info supports_sparse_csr op_name = op_info name replace masked sample_input sample_inputs_masked_reduction op_info device dtype requires_grad kwargs sample_input input ndim == sample_input kwargs get keepdim - sparse CSR tensors always -D tensors - masked reduction CSR tensors defined only keepdim True continue mask = sample_input kwargs get mask mask None sample_input_kwargs = sample_input kwargs copy sample_input_kwargs update mask=mask to_sparse_csr new_sample = SampleInput sample_input input to_sparse_csr args=sample_input args kwargs=sample_input_kwargs op_name prod amax amin mean reductions non-zero reduction identity unspecified mask supported sparse CSR tensors see torch masked prod implementation details continue new_sample = SampleInput sample_input input to_sparse_csr args=sample_input args kwargs=sample_input kwargs yield new_sample sample_input kwargs dim == Reductions CSR tensors use different implementations inner outer dimensions So minimum testing CSR implementations following kwargs must generated dict dim= keepdim=True dict dim= keepdim=True dict dim= keepdim=True Here we generate dim= case dim= case sample_input_kwargs = new_sample kwargs copy sample_input_kwargs update dim= yield SampleInput new_sample input clone args=sample_input args kwargs=sample_input_kwargs sample_inputs_masked_norm op_info device dtype requires_grad kwargs Sample inputs masked norm ord float inf float -inf sample_input sample_inputs_masked_reduction op_info device dtype requires_grad kwargs sample_input_args sample_input_kwargs = ord + sample_input args sample_input kwargs copy yield SampleInput sample_input input clone requires_grad_ requires_grad args=sample_input_args kwargs=sample_input_kwargs reference_masked_std_var numpy_fn ref = reference_reduction_numpy numpy_fn Translate unbiased correction arguments into ddof func input dim=None unbiased=None correction=None kwargs ddof = unbiased None ddof = unbiased correction None ddof = correction isinstance dim Sequence dim = tuple dim ref input dim ddof=ddof kwargs func sample_inputs_masked_std_var op_info device dtype requires_grad kwargs Sample inputs masked std var kwargs supports_multiple_dims = op_info supports_multiple_dims torch testing _internal common_methods_invocations sample_inputs_std_var masked_samples sample_input sample_inputs_std_var op_info device dtype requires_grad kwargs len sample_input args isinstance sample_input args bool continue masked std var doesn t support ` var unbiased ` mask _generate_masked_op_mask sample_input input shape device kwargs sample_input_args sample_input_kwargs = sample_input args dict mask=mask sample_input kwargs yield SampleInput sample_input input detach requires_grad_ requires_grad args=sample_input_args kwargs=sample_input_kwargs requires_grad dtype is_floating_point sample_input input ndim == mask None mask shape == sample_input input shape v torch inf -torch inf torch nan t = sample_input input detach t diagonal - - fill_ v yield SampleInput t requires_grad_ requires_grad args=sample_input_args kwargs=sample_input_kwargs sample_input masked_samples correction = sample_input kwargs get correction correction None correction = int sample_input kwargs get unbiased True dim = sample_input kwargs get dim None sample_input kwargs get mask None orig_count = torch masked sum torch ones sample_input input shape dtype=torch int dim keepdim=True inmask = torch masked _input_mask sample_input input sample_input args sample_input kwargs orig_count = torch masked sum inmask new_ones sample_input input shape dtype=torch int dim keepdim=True mask=inmask orig_count min = correction + Skip samples lead nans var computation continue yield sample_input sample_inputs_masked_softmax op_info device dtype requires_grad with_dtype=False kwargs Sample inputs masked softmax log_softmax softmin Masked normalization operator reduction operator trailing mask optional argument A mask bool tensor same shape input shape broadcastable input shape sample_input sample_inputs_softmax_variant op_info device dtype requires_grad with_dtype=with_dtype kwargs mask _generate_masked_op_mask sample_input input shape device kwargs yield SampleInput sample_input input clone requires_grad_ requires_grad sample_input args mask=mask sample_input kwargs sample_inputs_masked_cumops op_info device dtype requires_grad kwargs Sample inputs masked cumsum cumprod sample_input sample_inputs_softmax_variant op_info device dtype requires_grad kwargs mask _generate_masked_op_mask sample_input input shape device kwargs type mask torch Tensor continue sample_input_args sample_input_kwargs = sample_input args dict mask=mask sample_input kwargs keepdim sample_input_kwargs sample_input_kwargs pop keepdim dimension required sample_input_args dim = sample_input args dim sample_input_kwargs continue dim = sample_input_kwargs pop dim sample_input_args = dim yield SampleInput sample_input input clone requires_grad_ requires_grad sample_input_args sample_input_kwargs sample_inputs_masked_logaddexp op_info device dtype requires_grad kwargs Sample inputs masked logaddexp shapes = S S S S M S input_mask_lists = list _generate_masked_op_mask shape device kwargs shape shapes other_mask_lists = list _generate_masked_op_mask shape device kwargs shape shapes make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad shape input_masks other_masks zip shapes input_mask_lists other_mask_lists strict=True input_mask other_mask zip input_masks other_masks strict=True yield SampleInput make_arg shape make_arg shape input_mask=input_mask other_mask=other_mask sample_inputs_masked_normalize op_info device dtype requires_grad kwargs Sample inputs masked normalize ord float inf float -inf sample_input sample_inputs_softmax_variant op_info device dtype requires_grad use_zero_dimensions=False kwargs yield SampleInput sample_input input clone requires_grad_ requires_grad ord sample_input args sample_input kwargs op_db list OpInfo = ReductionOpInfo masked sum ref=reference_reduction_numpy np sum method_variant=None identity= nan_policy= propagate supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse=True supports_sparse_csr=True promotes_int_to_int =True dtypes=all_types_and_complex_and torch bool torch float torch bfloat skips= DecorateInfo unittest skip Failing some jobs TestReductions test_reference_masked dtypes= torch bool torch int torch int torch int DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive FIXME sum reduces all dimensions when dim= DecorateInfo unittest expectedFailure TestReductions test_dim_empty DecorateInfo unittest expectedFailure TestReductions test_dim_empty_keepdim RuntimeError undefined value tensor DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit decorators= DecorateInfo toleranceOverride torch bfloat tol atol= e- rtol= e- torch float tol atol= e- rtol= e- TestReductions test_reference_masked DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestReductions test_ref_small_input DecorateInfo toleranceOverride torch bfloat tol atol= rtol= torch float tol atol= e- rtol= e- TestMasked test_mask_layout sample_inputs_func=sample_inputs_masked_reduction sample_inputs_sparse_coo_func=sample_inputs_sparse_coo_masked_reduction sample_inputs_sparse_csr_func=sample_inputs_sparse_csr_masked_reduction ReductionOpInfo masked prod ref=prod_numpy method_variant=None identity= nan_policy= propagate https github com pytorch pytorch issues gradcheck_fast_mode=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse=True supports_sparse_csr=True promotes_int_to_int =True dtypes=all_types_and_complex_and torch bool torch float torch bfloat skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest skip Failing some jobs TestReductions test_reference_masked dtypes= torch bool torch int torch int torch int DecorateInfo TestReductions test_ref_small_input dtypes= torch int torch int torch int FIXME cuda_scatter_gather_base_kernel_func implemented used sparse_coo inputs DecorateInfo unittest skip Skipped TestMasked test_mask_layout device_type= cuda dtypes= torch bool integral_types complex_types decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestReductions test_reference_masked DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestReductions test_ref_duplicate_values DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestReductions test_ref_small_input DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestMasked test_mask_layout device_type= cpu DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestOperators test_jvp device_type= cuda sample_inputs_func=sample_inputs_masked_reduction sample_inputs_sparse_coo_func=sample_inputs_sparse_coo_masked_reduction sample_inputs_sparse_csr_func=sample_inputs_sparse_csr_masked_reduction OpInfo masked cumsum dtypes=all_types_and_complex_and torch float torch bfloat method_variant=None Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive NotSupportedError Compiled functions can t use keyword-only arguments defaults DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit Can reuse same inputs dim required both sample_inputs_func=sample_inputs_masked_cumops gradcheck_wrapper=gradcheck_wrapper_masked_operation OpInfo masked cumprod dtypes=all_types_and_complex_and torch float torch bfloat method_variant=None Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True skips= NotSupportedError Compiled functions can t use keyword-only arguments defaults DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive NotSupportedError Compiled functions can t use keyword-only arguments defaults DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestCompositeCompliance test_backward device_type= cuda DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cuda Can reuse same inputs dim required both sample_inputs_func=sample_inputs_masked_cumops gradcheck_wrapper=gradcheck_wrapper_masked_operation ReductionOpInfo masked amax nan_policy= propagate supports_out=False dtypes=all_types_and torch float torch bfloat supports_sparse=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_sparse_csr=True ref=reference_reduction_numpy np amax skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive FIXME amax reduces all dimensions when dim= DecorateInfo unittest expectedFailure TestReductions test_dim_empty DecorateInfo unittest expectedFailure TestReductions test_dim_empty_keepdim RuntimeError Unknown builtin op aten iinfo DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit FIXME cuda_scatter_gather_base_kernel_func implemented used sparse_coo inputs FIXME _segment_reduce_lengths_cpu cuda implemented used sparse_csr inputs DecorateInfo unittest skip Skipped TestMasked test_mask_layout dtypes= torch bool integral_types complex_types sample_inputs_func=sample_inputs_masked_reduction sample_inputs_sparse_coo_func=sample_inputs_sparse_coo_masked_reduction sample_inputs_sparse_csr_func=sample_inputs_sparse_csr_masked_reduction gradcheck_wrapper=gradcheck_wrapper_masked_operation ReductionOpInfo masked amin nan_policy= propagate supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True dtypes=all_types_and torch float torch bfloat supports_sparse=True supports_sparse_csr=True ref=reference_reduction_numpy np amin skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive FIXME amax reduces all dimensions when dim= DecorateInfo unittest expectedFailure TestReductions test_dim_empty DecorateInfo unittest expectedFailure TestReductions test_dim_empty_keepdim RuntimeError Unknown builtin op aten iinfo DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit FIXME cuda_scatter_gather_base_kernel_func implemented used sparse_coo inputs FIXME _segment_reduce_lengths_cpu cuda implemented used sparse_csr inputs DecorateInfo unittest skip Skipped TestMasked test_mask_layout dtypes= torch bool integral_types complex_types sample_inputs_func=sample_inputs_masked_reduction sample_inputs_sparse_coo_func=sample_inputs_sparse_coo_masked_reduction sample_inputs_sparse_csr_func=sample_inputs_sparse_csr_masked_reduction gradcheck_wrapper=gradcheck_wrapper_masked_operation ReductionOpInfo masked argmax supports_out=False supports_multiple_dims=False supports_autograd=False dtypes=all_types_and torch float torch bfloat ref=reference_reduction_numpy np argmax supports_keepdims=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive initial keyword argmax DecorateInfo unittest expectedFailure TestReductions test_reference_masked NotSupportedError Compiled functions can t use keyword-only arguments defaults DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit sample_inputs_func=sample_inputs_masked_reduction gradcheck_wrapper=gradcheck_wrapper_masked_operation ReductionOpInfo masked argmin supports_out=False supports_multiple_dims=False supports_autograd=False dtypes=all_types_and torch float torch bfloat ref=reference_reduction_numpy np argmin supports_keepdims=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive initial keyword argmin DecorateInfo unittest expectedFailure TestReductions test_reference_masked NotSupportedError Compiled functions can t use keyword-only arguments defaults DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit sample_inputs_func=sample_inputs_masked_reduction gradcheck_wrapper=gradcheck_wrapper_masked_operation ReductionOpInfo masked mean ref=reference_reduction_numpy np mean np lib NumpyVersion np __version__ = None method_variant=None nan_policy= propagate supports_out=False supports_sparse_csr=True supports_forward_ad=True supports_fwgrad_bwgrad=True promotes_int_to_float=True dtypes=floating_and_complex_types_and torch float torch bfloat skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive FIXME sum reduces all dimensions when dim= DecorateInfo unittest expectedFailure TestReductions test_dim_empty DecorateInfo unittest expectedFailure TestReductions test_dim_empty_keepdim RuntimeError undefined value tensor DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit FIXME _segment_reduce_lengths_cpu cuda implemented used sparse_csr inputs DecorateInfo unittest skip Skipped TestMasked test_mask_layout dtypes= torch bool integral_types complex_types decorators= DecorateInfo toleranceOverride torch bfloat tol atol= e- rtol= torch float tol atol= e- rtol= e- TestReductions test_reference_masked DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestReductions test_ref_small_input DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestSparseCompressed test_consistency device_type= cuda sample_inputs_func=sample_inputs_masked_reduction sample_inputs_sparse_csr_func=sample_inputs_sparse_csr_masked_reduction gradcheck_wrapper=gradcheck_wrapper_masked_operation OpInfo masked median dtypes=floating_types_and torch bfloat torch float method_variant=None supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive NotSupportedError Compiled functions can t use keyword-only arguments defaults DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit sample_inputs_func=partial sample_inputs_masked_softmax use_zero_dimensions=False gradcheck_wrapper=gradcheck_wrapper_masked_operation ReductionOpInfo masked norm identity= method_variant=None nan_policy= propagate supports_out=False promotes_int_to_float=True dtypes=floating_types_and torch float torch bfloat skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive FIXME sum reduces all dimensions when dim= DecorateInfo unittest expectedFailure TestReductions test_dim_empty DecorateInfo unittest expectedFailure TestReductions test_dim_empty_keepdim torch jit frontend NotSupportedError Compiled functions can t take variable number arguments use keyword-only arguments defaults DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_masked_norm gradcheck_wrapper=gradcheck_wrapper_masked_operation ReductionOpInfo masked var ref=reference_masked_std_var np var np lib NumpyVersion np __version__ = None method_variant=None nan_policy= propagate supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False promotes_int_to_float=True dtypes=all_types_and_complex_and torch float torch bfloat skips= Issue conj torch dispatch see https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestSchemaCheckModeOpInfo test_schema_correctness dtypes= torch complex torch complex DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive FIXME sum reduces all dimensions when dim= DecorateInfo unittest expectedFailure TestReductions test_dim_empty DecorateInfo unittest expectedFailure TestReductions test_dim_empty_keepdim RuntimeError undefined value tensor DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch bfloat tol atol= e- rtol= e- TestReductions test_reference_masked DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestReductions test_ref_small_input DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestMasked test_reference_masked DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch bfloat tol atol= e- rtol= e- TestMasked test_reference_masked DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cuda sample_inputs_func=sample_inputs_masked_std_var gradcheck_wrapper=gradcheck_wrapper_masked_operation check_batched_grad=True ReductionOpInfo masked std ref=reference_masked_std_var np std np lib NumpyVersion np __version__ = None method_variant=None nan_policy= propagate Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True See https github com pytorch pytorch pull check_batched_forward_grad=False promotes_int_to_float=True dtypes=all_types_and_complex_and torch half torch bfloat skips= Issue conj torch dispatch see https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestSchemaCheckModeOpInfo test_schema_correctness dtypes= torch complex torch complex DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive FIXME sum reduces all dimensions when dim= DecorateInfo unittest expectedFailure TestReductions test_dim_empty DecorateInfo unittest expectedFailure TestReductions test_dim_empty_keepdim RuntimeError undefined value tensor DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit decorators= DecorateInfo toleranceOverride torch bfloat tol atol= e- rtol= e- torch float tol atol= e- rtol= e- TestReductions test_reference_masked DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestReductions test_ref_small_input DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch bfloat tol atol= e- rtol= e- TestMasked test_reference_masked sample_inputs_func=sample_inputs_masked_std_var gradcheck_wrapper=gradcheck_wrapper_masked_operation check_batched_grad=True OpInfo masked softmax method_variant=None dtypes=floating_types_and torch half torch bfloat sample_inputs_func=sample_inputs_masked_softmax skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit gradcheck_wrapper=gradcheck_wrapper_masked_operation supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False OpInfo masked log_softmax method_variant=None dtypes=floating_types_and torch half torch bfloat sample_inputs_func=sample_inputs_masked_softmax skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit decorators= DecorateInfo toleranceOverride torch bfloat tol atol= e- rtol= e- TestMasked test_reference_masked gradcheck_wrapper=gradcheck_wrapper_masked_operation supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False OpInfo masked softmin method_variant=None dtypes=floating_types_and torch half torch bfloat sample_inputs_func=sample_inputs_masked_softmax skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit FIXME Mismatched elements Greatest absolute difference nan index up allowed Greatest relative difference nan index up allowed DecorateInfo unittest skip Skipped TestOperators test_vmapvjpvjp device_type= cpu gradcheck_wrapper=gradcheck_wrapper_masked_operation supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False OpInfo masked normalize method_variant=None dtypes=floating_and_complex_types_and torch half torch bfloat sample_inputs_func=sample_inputs_masked_normalize decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestInductorOpInfo test_comprehensive device_type= cuda skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit gradcheck_wrapper=gradcheck_wrapper_masked_operation Runs very slowly slow gradcheck - alternatively reduce input sizes gradcheck_fast_mode=True supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False OpInfo masked logaddexp dtypes=floating_types_and torch float torch bfloat supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True check_batched_forward_grad=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive NotSupportedError Compiled functions can t use keyword-only arguments defaults DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit DecorateInfo unittest skip Skipped TestFwdGradients test_fn_gradgrad DecorateInfo unittest skip Skipped TestBwdGradients test_fn_gradgrad sample_inputs_func=sample_inputs_masked_logaddexp gradcheck_wrapper=gradcheck_wrapper_masked_pointwise_operation ReductionOpInfo masked logsumexp dtypes=all_types_and_complex_and torch half torch bfloat method_variant=None nan_policy= propagate supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True skips= DecorateInfo unittest skip Skipped TestNormalizeOperators test_normalize_operator_exhaustive FIXME reduces all dimensions when dim= DecorateInfo unittest skip Skipped TestReductions test_dim_empty DecorateInfo unittest skip Skipped TestReductions test_dim_empty_keepdim Identity can t -torch inf without overflow DecorateInfo unittest skip Skipped TestReductions test_empty_tensor_empty_slice NotSupportedError Compiled functions can t use keyword-only arguments defaults DecorateInfo unittest skip Skipped TestJit test_variant_consistency_jit all values same except -inf vs nan DecorateInfo unittest skip Skipped TestDecomp test_comprehensive FIXME Mismatched elements Greatest absolute difference index Greatest relative difference index DecorateInfo unittest skip Skipped TestInductorOpInfo test_comprehensive device_type= cpu sample_inputs_func=sample_inputs_masked_reduction gradcheck_wrapper=gradcheck_wrapper_masked_operation