Owner s module inductor unittest mock patch torch torch _inductor config torch _inductor async_compile AsyncCompile shutdown_compile_workers torch _inductor compile_worker subproc_pool SubprocException torch _inductor runtime triton_compat Config torch _inductor runtime triton_heuristics generate_lookup_hash_from_source_code torch _inductor test_case run_tests TestCase torch _inductor utils fresh_cache torch testing _internal common_utils instantiate_parametrized_tests parametrize torch testing _internal inductor_utils GPU_TYPE requires_gpu requires_triton instantiate_parametrized_tests TestAsyncCompile TestCase requires_gpu requires_triton parametrize method subprocess fork spawn test_pool method fn x y x + y x = torch rand GPU_TYPE y = torch rand GPU_TYPE config patch worker_start_method method shutdown_compile_workers AsyncCompile wait_pool_ready fresh_cache compiled_fn = torch compile fn assertEqual fn x y compiled_fn x y requires_gpu requires_triton test_bad_kernel shutdown_compile_workers config patch worker_start_method= subprocess compile_threads= async_compile = AsyncCompile AsyncCompile wait_pool_ready assertRaises SubprocException async_compile triton fake_kernel_name source_code= This definitely doesn t exist result requires_gpu requires_triton test_wait_pool_ready shutdown_compile_workers config patch worker_start_method= subprocess compile_threads= AsyncCompile wait_pool_ready assertTrue AsyncCompile _ready_future done assertTrue AsyncCompile use_process_pool requires_gpu requires_triton patch torch _inductor runtime coordinate_descent_tuner CoordescTuner autotune parametrize method subprocess fork spawn test_autotune_lookup_table mock_autotune method f b b torch float sum dim= Fake name make sure lookup table name agnostic When codegen triton py changed func_def must updated loop_header = r _offset tl range r _numel R _BLOCK num_stages = torch version hip r _offset tl range r _numel R _BLOCK func_def = f triton_fused_fake_name in_ptr out_ptr xnumel r _numel XBLOCK tl constexpr R _BLOCK tl constexpr xnumel = r _numel = rnumel = r _numel RBLOCK tl constexpr = R _BLOCK xoffset = tl program_id XBLOCK xindex = xoffset + tl arange XBLOCK None xmask = xindex xnumel r _base = tl arange R _BLOCK None rbase = r _base x = xindex _tmp = tl full XBLOCK R _BLOCK tl float loop_header r _index = r _offset + r _base r _mask = r _index r _numel roffset = r _offset rindex = r _index r _ = r _index tmp = tl load in_ptr + r _ + x r _mask xmask eviction_policy= evict_first other= tl float tmp = tmp tl float tmp = tl broadcast_to tmp XBLOCK R _BLOCK tmp = _tmp + tmp _tmp = tl where r _mask xmask tmp _tmp tmp = tl sum _tmp None tl store out_ptr + x tmp xmask fn_hash = generate_lookup_hash_from_source_code str x r _ func_def block_configs = XBLOCK R _BLOCK num_warps = num_stages = autotune_lookup_table = fn_hash block_configs num_warps num_warps num_stages num_stages autotune_config = Config block_configs num_warps=num_warps num_stages=num_stages mock_autotune return_value = autotune_config = torch randn device=GPU_TYPE dtype=torch float T b = torch randn device=GPU_TYPE dtype=torch float compiled_f = torch compile f config patch autotune_lookup_table autotune_lookup_table coordinate_descent_tuning True worker_start_method method shutdown_compile_workers AsyncCompile wait_pool_ready fresh_cache compiled_f b Check input coordinate descent resulting chosen config same one lookup table mock_autotune assert_called_once args _ = mock_autotune call_args assertTrue isinstance args Config assertEqual args kwargs autotune_config kwargs assertEqual args num_warps autotune_config num_warps assertEqual args num_stages autotune_config num_stages __name__ == __main__ run_tests