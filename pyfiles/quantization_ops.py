torch torch nn nn GeneralQuantModule torch nn Module __init__ - None super __init__ embedding = torch ao nn quantized Embedding num_embeddings= embedding_dim= embedding_input = torch tensor func = torch ao nn quantized QFunctional conv = torch ao nn quantized ConvTranspose d stride= conv = torch ao nn quantized ConvTranspose d stride= conv = torch ao nn quantized ConvTranspose d stride= forward = torch quantize_per_tensor torch tensor torch qint b = torch quantize_per_tensor torch tensor torch qint c = torch quantize_per_tensor torch tensor torch tensor torch tensor torch qint input = torch randn input = torch randn len func add b func cat func mul b func add_relu b func add_scalar b func mul_scalar b embedding embedding_input conv torch quantize_per_tensor input scale= zero_point= dtype=torch quint conv torch quantize_per_tensor input scale= zero_point= dtype=torch quint c conv torch quantize_per_tensor input scale= zero_point= dtype=torch quint failed iOS DynamicQuantModule __init__ - None super __init__ module = M getModule torch ao quantization quantize_dynamic module dtype=torch qint M torch nn Module __init__ - None super DynamicQuantModule M __init__ rnn = nn RNN rnncell = nn RNNCell gru = nn GRU grucell = nn GRUCell lstm = nn LSTM lstmcell = nn LSTMCell linears = nn ModuleList nn Identity nn Linear nn Bilinear transformers = nn ModuleList nn Transformer d_model= nhead= num_encoder_layers= num_decoder_layers= nn TransformerEncoder nn TransformerEncoderLayer d_model= nhead= num_layers= nn TransformerDecoder nn TransformerDecoderLayer d_model= nhead= num_layers= = torch nn utils rnn pad_sequence torch tensor torch tensor batch_first=True forward input = torch randn h = torch randn c = torch randn linear_input = torch randn trans_input = torch randn tgt = torch rand len rnn input h rnncell input h gru input h grucell input h lstm input h c lstm torch nn utils rnn pack_padded_sequence lengths=torch tensor h c lstmcell input h c transformers trans_input tgt transformers trans_input transformers trans_input tgt linears linear_input linears linear_input linears linear_input linear_input StaticQuantModule getModule model_fp = M model_fp eval model_fp qconfig = torch ao quantization get_default_qconfig qnnpack model_fp _prepared = torch ao quantization prepare model_fp model_int = torch ao quantization convert model_fp _prepared model_int M torch nn Module __init__ - None super StaticQuantModule M __init__ quant = torch ao quantization QuantStub input d = torch randn input d = torch randn input d = torch randn linear_input = torch randn layer = nn Sequential nn Conv d nn InstanceNorm d nn Hardswish layer = nn Sequential nn Conv d nn BatchNorm d nn InstanceNorm d nn LeakyReLU layer = nn Sequential nn Conv d nn BatchNorm d nn InstanceNorm d nn ReLU layer = nn Sequential nn Linear dequant = torch ao quantization DeQuantStub forward x = quant input d x = layer x x = dequant x y = input d y = quant y y = layer y y = layer y y = dequant y z = quant input d z = layer z z = dequant z x y z FusedQuantModule getModule model_fp = M model_fp eval model_fp qconfig = torch ao quantization get_default_qconfig qnnpack model_fp _fused = torch ao quantization fuse_modules model_fp conv d relu conv d relu conv d relu linear relu model_fp _prepared = torch ao quantization prepare model_fp _fused model_int = torch ao quantization convert model_fp _prepared model_int M torch nn Module __init__ - None super FusedQuantModule M __init__ quant = torch ao quantization QuantStub input d = torch randn input d = torch randn input d = torch randn conv d = nn Conv d conv d = nn Conv d conv d = nn Conv d linear = nn Linear relu = nn ReLU relu = nn ReLU relu = nn ReLU relu = nn ReLU dequant = torch ao quantization DeQuantStub forward x = input d y = input d z = input d x = quant x x = conv d x x = relu x x = dequant x y = quant y y = conv d y y = relu y y = dequant y z = quant z z = conv d z z = relu z z = linear z z = relu z z = dequant z x y z