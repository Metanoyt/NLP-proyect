logging collections abc Callable typing Any Union torch torch _higher_order_ops utils create_bw_fn materialize_as_graph logger logging Logger = logging getLogger __name__ logger setLevel logging DEBUG _find_hop_subgraph_outputs gm torch fx GraphModule - tuple torch fx Node output_node_args = gm graph find_nodes op= output args assert isinstance output_node_args tuple output_node_args is_complex_expr expr Any - bool expr is_symbol expr is_constant HopPartitionedGraph __init__ fw_gm torch fx GraphModule bw_gm torch fx GraphModule n_fw_outputs int n_intermediates int no_complex_exprs_at_boundary bool fw_gm = fw_gm bw_gm = bw_gm n_fw_outputs = n_fw_outputs n_intermediates = n_intermediates no_complex_exprs_at_boundary = no_complex_exprs_at_boundary _reorder_fw_output _check_partition_boundary _check_partition_boundary - None check partitioned graph valid state invalid_reasons = fw_outputs = _find_hop_subgraph_outputs fw_gm i out enumerate fw_outputs val out meta invalid_reasons append f fw_gm output i doesn t have val meta isinstance out meta val torch SymInt torch Tensor invalid_reasons append f fw_gm output i type type out meta val only SymInt Tensor allowed isinstance out meta val torch SymInt is_complex_expr out meta val node expr no_complex_exprs_at_boundary invalid_reasons append f fw_gm output i must type SymInt basic symbols f Tensor got type out meta val out meta val len fw_outputs = n_fw_outputs + n_intermediates invalid_reasons append f len fw_outputs len fw_outputs = n_fw_outputs n_fw_outputs + n_intermediates n_intermediates noqa B bw_phs = list bw_gm graph find_nodes op= placeholder len fw_outputs = len bw_phs invalid_reasons append f Expect number fw_gm s output same bw_gm s input f fw_gm has len fw_outputs outputs bw_gm takes len bw_phs inputs original_forward_outputs = fw_outputs n_fw_outputs fw_intermediates = fw_outputs n_fw_outputs bw_intermediates = bw_phs -self n_fw_outputs bw_grads = bw_phs -self n_fw_outputs _match_size_or_expr val Union torch SymInt torch Tensor val Union torch SymInt torch Tensor - bool type val type val False isinstance val torch SymInt isinstance val torch SymInt val node expr == val node expr isinstance val torch Tensor isinstance val torch Tensor val size == val size False fw bw zip fw_intermediates bw_intermediates fw name = bw name _match_size_or_expr fw meta val bw meta val invalid_reasons append fw intermediates don t match bw intermediates fw_out bw_grad zip original_forward_outputs bw_grads _match_size_or_expr fw_out meta val bw_grad meta val invalid_reasons append fw outputs don t match bw gradients len invalid_reasons newline = \n raise RuntimeError Invalid HopPartitionedGraph Reasons \n f newline join invalid_reasons _reorder_fw_output - None Before pass fw_gm returns fw_outputs intermediates bw_gm takes intermediates grad_fw_outputs input intermediates intermediates share same node names they might different order E g could happen there inputs contain symints To simplify downstream processing graph pass normalizes output fw_gm consistent bacwkard inputs fw_gm - input fw_args - output fw_outputs intermediates bw_gm - input intermediates grad_fw_outputs - output grad_fw_args Example fw_gm x y z b c = f x g y k z b c f_tmp g_tmp k_tmp where b c fw_outputs f_tmp g_tmp k_tmp intermediates The corresponding bw_gm has following signature bw_gm f_tmp g_tmp k_tmp grad_a grad_b grac grad_x grad_y grad_z fw_gm_output_nodes = _find_hop_subgraph_outputs fw_gm fw_outputs_nodes = fw_gm_output_nodes n_fw_outputs fw_intermediates_nodes = fw_gm_output_nodes n_fw_outputs len fw_intermediates_nodes fw_intermediates_name_to_node = n name n n fw_intermediates_nodes First n_intermediates placeholders bw_names list str = ph name ph list bw_gm graph find_nodes op= placeholder n_intermediates new_fw_outputs = list fw_outputs_nodes + fw_intermediates_name_to_node name name bw_names output_node = fw_gm graph find_nodes op= output output_node args = tuple new_fw_outputs fw_gm graph lint fw_gm recompile HopJointGraph __init__ joint_gm torch fx GraphModule n_primals int n_fw_outputs int functionalized bool joint_gm = joint_gm n_primals = n_primals n_fw_outputs = n_fw_outputs functionalized = functionalized _rename_phs _remove_redundant_sym_size_ops _rename_phs - None Rename placeholders joint_gm so partitioner could recognize which inputs primals which tangents n_tangents = i ph enumerate joint_gm graph find_nodes op= placeholder i n_primals ph target = f primals_ i ph name = f primals_ i n_tangents += ph target = f tangents_ i - n_primals ph name = f tangents_ i - n_primals joint_gm graph lint joint_gm compile _remove_redundant_sym_size_ops - None Deletes torch ops sym_size int operators whose output corresponding placeholder holds same symbol replace all usage sym_size node directly using placeholders This make sure all basic symbols come inputs placeholder_exprs = node joint_gm graph nodes isinstance node torch fx Node node op == placeholder hasattr node meta val node meta val = node meta val isinstance val torch SymInt placeholder_exprs val node expr = node nodes_to_remove = node joint_gm graph find_nodes op= call_function target=torch ops aten sym_size int assert hasattr node meta val node meta node val = node meta val expr = val node expr expr placeholder_exprs placeholder_node = placeholder_exprs expr node replace_all_uses_with placeholder_node nodes_to_remove append node node nodes_to_remove joint_gm graph erase_node node joint_gm graph lint joint_gm recompile _mark_complex_exprs_as_must_recompute - None For control flow operators such scan we don t want have symint partitioning boundaries because otherwise we would need support stacking symints up which causes more entropy stack By marking recompute polify complex nodes MUST_RECOMPUTE partitioning boundary no longer contains complex expressions Note pass doesn t exclude basic symbols partitioning boundary s up downstream decide whether basic symbol have separate graph pass remove them torch _functorch partitioners CheckpointPolicy n node node joint_gm graph nodes node op == call_function val n meta continue val = n meta val isinstance val torch SymInt is_complex_expr val node expr assert n meta get recompute None None n meta recompute = CheckpointPolicy MUST_RECOMPUTE joint_gm graph lint joint_gm recompile partition partition_fn Callable always_recompute_complex_exprs bool - HopPartitionedGraph logger isEnabledFor logging DEBUG logger debug before min_cut_partition \n s joint_gm print_readable print_output=False always_recompute_complex_exprs _mark_complex_exprs_as_must_recompute fw_gm bw_gm = partition_fn joint_gm None num_fwd_outputs=self n_fw_outputs logger isEnabledFor logging DEBUG logger debug after partition_fn logger debug fw_gm \n s fw_gm print_readable print_output=False logger debug bw_gm \n s bw_gm print_readable print_output=False n_intermediates = len _find_hop_subgraph_outputs fw_gm - n_fw_outputs HopPartitionedGraph fw_gm bw_gm n_fw_outputs n_intermediates always_recompute_complex_exprs create_hop_joint_graph fw_fn Callable fw_args tuple Union torch Tensor torch SymInt functionalize bool - HopJointGraph fw_gm = materialize_as_graph fw_fn fw_args force_enable_grad=True fw_gm_output_nodes = _find_hop_subgraph_outputs fw_gm assert all isinstance n torch fx Node val n meta n fw_gm_output_nodes fw_gm_output_vals = tuple n meta val n fw_gm_output_nodes type ignore arg-type assert all isinstance val torch Tensor val fw_gm_output_vals example_grads = tuple torch zeros_like val val fw_gm_output_vals joint_fn = create_bw_fn fw_fn fw_args return_fw_outputs=True joint_gm = materialize_as_graph joint_fn fw_args + example_grads force_enable_grad=True functionalize Need first trace out joint_fn autograd info then functionalize graph otherwise grad information lost joint_gm = materialize_as_graph torch func functionalize joint_gm remove= mutations_and_views fw_args + example_grads HopJointGraph joint_gm len fw_args len fw_gm_output_nodes functionalized=functionalize HopGraphMinCutPartitioner staticmethod create_partitioned_graph fw_fn Callable fw_args tuple Union torch Tensor torch SymInt always_recompute_complex_exprs bool = False - HopPartitionedGraph Inputs - fw_fn forward function we ll use create joint graph partition - fw_args flat_args fw_fn - always_recompute_complex_exprs when set True bw_gm will do re-compute inputs complex expressions such partitioning boundary only consists basic symbols tensors Returns HopPartitionedGraph torch _functorch partitioners min_cut_rematerialization_partition joint_graph HopJointGraph = create_hop_joint_graph fw_fn fw_args functionalize=True joint_graph partition min_cut_rematerialization_partition always_recompute_complex_exprs