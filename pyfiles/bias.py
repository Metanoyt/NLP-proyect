mypy allow-untyped-defs Defines bias subclasses work scaled_dot_product_attention enum auto IntEnum typing Optional warnings warn torch torch nn functional F torch backends cuda can_use_efficient_attention can_use_flash_attention is_flash_attention_available SDPAParams torch nn attention _raise_kernel_warnings torch nn attention _utils _calculate_scale _input_requires_grad _postprocess_flash_output _validate_sdpa_input __all__ = causal_upper_left causal_lower_right CausalVariant CausalBias torch _dynamo allow_in_graph is_flash_attention_available torch _dynamo allow_in_graph can_use_flash_attention torch _dynamo allow_in_graph can_use_efficient_attention torch _dynamo allow_in_graph SDPAParams CausalVariant IntEnum r Enum causal variants used attention mechanisms Defines two types causal biases ` ` UPPER_LEFT ` ` Represents upper-left triangular bias standard causal attention The equivalent pytorch code constructing bias code-block python torch tril torch ones size dtype=torch bool For instance ` ` shape= ` ` materialized bias tensor will code-block text ` ` LOWER_RIGHT ` ` Represents lower-right triangular bias include values aligned lower right corner matrix The equivalent pytorch code constructing bias code-block python diagonal_offset = size - size torch tril torch ones size dtype=torch bool diagonal=diagonal_offset For instance ` ` shape= ` ` materialized bias tensor will code-block text Note these variants equivalent each other when sequence lengths query key value tensors equal since triangular matrix square warning This enum prototype subject change UPPER_LEFT = auto LOWER_RIGHT = auto CausalBias torch Tensor A bias representing causal attention patterns For overview bias structure see ` CausalVariant ` enum This used defining causal triangular attention biases For construing bias there exist two factory functions func ` causal_upper_left ` func ` causal_lower_right ` Example code-block python torch nn attention bias causal_lower_right bsz num_heads seqlen_q seqlen_kv head_dim = Create lower-right causal bias attn_bias = causal_lower_right seqlen_q seqlen_kv q = torch randn bsz num_heads seqlen_q head_dim device= cuda dtype=torch float k = torch randn bsz num_heads seqlen_kv head_dim device= cuda dtype=torch float v = torch randn bsz num_heads seqlen_kv head_dim device= cuda dtype=torch float out = F scaled_dot_product_attention q k v attn_bias warning This prototype subject change __init__ variant CausalVariant seq_len_q int seq_len_kv int Initializes CausalBias instance specified variant sequence lengths Args variant CausalVariant The type causal bias use either UPPER_LEFT LOWER_RIGHT seq_len_q int The sequence length query tensor seq_len_kv int The sequence length key value tensor Raises warning LOWER_RIGHT variant used seq_len_q seq_len_kv may produce NaNs assert isinstance variant CausalVariant variant = variant seq_len_q = seq_len_q seq_len_kv = seq_len_kv seq_len_q seq_len_kv variant == CausalVariant LOWER_RIGHT warn Lower right causal bias will produce NaNs output when seq_len_q seq_len_kv stacklevel= _upper_left device torch device - torch Tensor Upper left causal bias torch tril torch ones seq_len_q seq_len_kv device=device dtype=torch bool _lower_right device torch device - torch Tensor Lower right causal bias diagonal_offset = seq_len_kv - seq_len_q torch tril torch ones seq_len_q seq_len_kv device=device dtype=torch bool diagonal=diagonal_offset pyrefly ignore bad-return _materialize device Optional torch device = None - torch Tensor Materializes causal bias into tensor form Depending variant method generates either upper-left lower-right triangular matrix represent causal bias Args device Optional torch device The device which create tensor Defaults CPU Returns torch Tensor The materialized bias tensor device None device = torch device cpu variant == CausalVariant UPPER_LEFT _upper_left device variant == CausalVariant LOWER_RIGHT _lower_right device staticmethod _dispatch query torch Tensor key torch Tensor value torch Tensor attn_mask CausalBias dropout_p float = is_causal bool = False scale Optional float = None enable_gqa bool = False - torch Tensor r Handles logic computing attention specified causal bias Args query Tensor Query tensor shape math ` N L E ` key Tensor Key tensor shape math ` N S E ` value Tensor Value tensor shape math ` N S Ev ` attn_mask CausalBias The type causal attention apply A boolean mask where value True indicates element should take part attention A float mask same type query key value added attention score dropout_p float Dropout probability greater than dropout applied is_causal bool If true assumes upper left causal attention masking errors both attn_mask is_causal set scale optional float Scaling factor applied prior softmax If None default value set math ` \frac \sqrt E ` enable_gqa optional bool If set True Grouped Query Attention GQA enabled default set False Returns output Tensor Attention output shape math ` N L Ev ` Raises ValueError If causal bias variant CausalVariant type is_causal raise ValueError CausalBias should used causal=True attn_mask seq_len_q == attn_mask seq_len_kv attn_mask variant == CausalVariant UPPER_LEFT F scaled_dot_product_attention query key value attn_mask=None dropout_p=dropout_p is_causal=True scale=scale enable_gqa=enable_gqa attn_mask variant == CausalVariant LOWER_RIGHT _validate_sdpa_input query key value None dropout_p is_causal scale sdpa_params = SDPAParams query key value None dropout_p is_causal enable_gqa can_use_flash_attention sdpa_params needs_padding = query size - = og_head_size = query size - og_scale = _calculate_scale og_head_size scale needs_padding query = torch nn functional pad query - query size - key = torch nn functional pad key - key size - value = torch nn functional pad value - value size - out = torch ops aten _scaled_dot_product_flash_attention query key value dropout_p is_causal=True TODO Flash accepts causal = True particular op means lower right return_debug_mask=False scale=og_scale _postprocess_flash_output out og_head_size can_use_efficient_attention sdpa_params compute_log_sumexp = False _input_requires_grad query key value compute_log_sumexp = True torch ops aten _efficient_attention_forward query transpose key transpose value transpose bias=None cu_seqlens_q=None cu_seqlens_k=None max_seqlen_q=None max_seqlen_k=None dropout_p=dropout_p custom_mask_type=int attn_mask variant compute_log_sumexp=compute_log_sumexp scale=scale seqlen_k=None transpose _raise_kernel_warnings sdpa_params We can t use efficient attention only support lower right via materialization F scaled_dot_product_attention query key value attn_mask=attn_mask _materialize query device dropout_p=dropout_p is_causal=False scale=scale enable_gqa=enable_gqa raise ValueError f CausalBias variant must CausalVariant type found attn_mask variant classmethod __torch_function__ cls func types args= kwargs=None Defines behavior torch nn functional scaled_dot_product_attention when attn_bias AttnBias kwargs None kwargs = func torch nn functional scaled_dot_product_attention cls _dispatch args kwargs super __torch_function__ func types args kwargs __repr__ type ignore override _materialize __repr__ causal_upper_left size - CausalBias Creates upper-left triangular causal bias This function generates upper-left triangular matrix represent causal attention bias diagonal offset set so inclusive values aligned upper left corner matrix This equivalent ` is_causal=True ` argument ` scaled_dot_product_attention ` The equivalent pytorch code constructing bias code-block python torch tril torch ones size dtype=torch bool For instance ` shape= ` materialized bias tensor will code-block text Args size The size bias matrix Returns CausalBias The UPPER_LEFT triangular causal bias variant assert len size == causal_upper_left only supports D tensors seq_len_q seq_len_kv = size CausalBias CausalVariant UPPER_LEFT seq_len_q seq_len_kv causal_lower_right size - CausalBias Creates lower-right triangular causal bias This function generates lower-right triangular matrix represent causal attention bias diagonal offset set so inclusive values aligned lower right corner matrix The equivalent pytorch code constructing bias code-block python diagonal_offset = size - size torch tril torch ones size dtype=torch bool diagonal=diagonal_offset For instance ` shape= ` materialized bias tensor will code-block text Args size The size bias matrix Returns CausalBias The LOWER_RIGHT triangular causal bias variant assert len size == causal_lower_right only supports D tensors seq_len_q seq_len_kv = size CausalBias CausalVariant LOWER_RIGHT seq_len_q seq_len_kv