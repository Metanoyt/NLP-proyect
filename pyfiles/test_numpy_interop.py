mypy ignore-errors Owner s module numpy sys itertools product numpy np torch torch testing make_tensor torch testing _internal common_device_type dtypes instantiate_device_type_tests onlyCPU skipMeta torch testing _internal common_dtype all_types_and_complex_and torch testing _internal common_utils run_tests skipIfTorchDynamo TestCase For testing handling NumPy objects sending tensors accepting arrays NumPy TestNumPyInterop TestCase Note warning tests only appears once per program so other instances warning should addressed avoid tests depending order which they re run onlyCPU test_numpy_non_writeable device arr = np zeros arr flags WRITEABLE = False assertWarns UserWarning lambda torch from_numpy arr onlyCPU test_numpy_unresizable device - None x = np zeros y = torch from_numpy x noqa F assertRaises ValueError x resize z = torch randn w = z numpy assertRaises RuntimeError z resize_ assertRaises ValueError w resize onlyCPU test_to_numpy device - None get_castable_tensor shape dtype dtype is_floating_point dtype_info = torch finfo dtype can t directly use min max because double max - min greater than double range sampling always gives inf low = max dtype_info min - e high = min dtype_info max e t = torch empty shape dtype=torch float uniform_ low high can t directly use min max because int _t max - min greater than int _t range triggers UB low = max torch iinfo dtype min int - e high = min torch iinfo dtype max int e t = torch empty shape dtype=torch int random_ low high t dtype dtypes = torch uint torch int torch short torch int torch half torch float torch double torch long dtp dtypes D sz = x = get_castable_tensor sz dtp y = x numpy i range sz assertEqual x i y i D storage offset xm = get_castable_tensor sz dtp x = xm narrow sz - sz assertTrue x storage_offset y = x numpy i range sz assertEqual x i y i check d x y i range sz j range sz assertEqual x i j y i j empty x = torch tensor dtp y = x numpy assertEqual y size contiguous D sz = sz = x = get_castable_tensor sz sz dtp y = x numpy check d x y assertTrue y flags C_CONTIGUOUS storage offset xm = get_castable_tensor sz sz dtp x = xm narrow sz - sz y = x numpy assertTrue x storage_offset check d x y assertTrue y flags C_CONTIGUOUS non-contiguous D x = get_castable_tensor sz sz dtp t y = x numpy check d x y assertFalse y flags C_CONTIGUOUS storage offset xm = get_castable_tensor sz sz dtp x = xm narrow sz - sz t y = x numpy assertTrue x storage_offset check d x y non-contiguous D holes xm = get_castable_tensor sz sz dtp x = xm narrow sz - sz narrow sz - sz t y = x numpy assertTrue x storage_offset check d x y dtp = torch half check writeable x = get_castable_tensor dtp y = x numpy assertTrue y flags writeable y = assertTrue x == y = x t numpy assertTrue y flags writeable y = assertTrue x == test_to_numpy_bool device - None x = torch tensor True False dtype=torch bool assertEqual x dtype torch bool y = x numpy assertEqual y dtype np bool_ i range len x assertEqual x i y i x = torch tensor True dtype=torch bool assertEqual x dtype torch bool y = x numpy assertEqual y dtype np bool_ assertEqual x y skipIfTorchDynamo can t check value ZeroTensor since _is_zerotensor returns bool TensorVariable test_to_numpy_zero_tensor device - None dtypes = torch uint torch int torch short torch int torch half torch float torch double torch long torch bool dtype dtypes x = torch _efficientzerotensor dtype=dtype assertRaises RuntimeError lambda x numpy y = x numpy force=True i range assertEqual y i skipIfTorchDynamo conj bit implemented TensorVariable yet test_to_numpy_force_argument device - None force False True requires_grad False True sparse False True conj False True data = + j - + j - - j - j x = torch tensor data requires_grad=requires_grad device=device y = x sparse requires_grad continue x = x to_sparse conj x = x conj y = x resolve_conj expect_error = requires_grad sparse conj device = cpu error_msg = r Use t &#124; T ensor\ \ numpy\ \ force expect_error assertRaisesRegex RuntimeError TypeError error_msg lambda x numpy assertRaisesRegex RuntimeError TypeError error_msg lambda x numpy force=False force sparse assertRaisesRegex TypeError error_msg lambda x numpy force=True assertEqual x numpy force=force y test_from_numpy device - None dtypes = np double np float np float np complex np complex np int np int np int np int np uint np longlong np bool_ complex_dtypes = np complex np complex dtype dtypes array = np array dtype=dtype tensor_from_array = torch from_numpy array TODO change tensor equality check once HalfTensor implements ` == ` i range len array assertEqual tensor_from_array i array i ufunc remainder supported complex dtypes dtype complex_dtypes This special test case Windows https github com pytorch pytorch issues array = array tensor_from_array = torch from_numpy array i range len array assertEqual tensor_from_array i array i Test unsupported type array = np array foo bar dtype=np dtype np str_ assertRaises TypeError tensor_from_array = torch from_numpy array check storage offset x = np linspace x shape = x = x expected = torch arange dtype=torch float view assertEqual torch from_numpy x expected check noncontiguous x = np linspace x shape = expected = torch arange dtype=torch float view t assertEqual torch from_numpy x T expected check noncontiguous holes x = np linspace x shape = x = x expected = torch arange dtype=torch float view assertEqual torch from_numpy x expected check zero dimensional x = np zeros assertEqual torch from_numpy x shape x = np zeros assertEqual torch from_numpy x shape check ill-sized strides raise exception x = np array x strides = assertRaises ValueError lambda torch from_numpy x skipIfTorchDynamo No need test invalid dtypes should fail design test_from_numpy_no_leak_on_invalid_dtype This used leak memory ` from_numpy ` call raised exception didn t decref temporary object See https github com pytorch pytorch issues x = np array b value _ range try torch from_numpy x except TypeError pass assertTrue sys getrefcount x == skipIfTorchDynamo No need test invalid dtypes should fail design onlyCPU test_from_numpy_zero_element_type This tests dtype check happens before strides check which results div-by-zero on-x x = np ndarray dtype=str assertRaises TypeError lambda torch from_numpy x skipMeta test_from_list_of_ndarray_warning device warning_msg = r Creating tensor list numpy ndarrays extremely slow assertWarnsOnceRegex UserWarning warning_msg torch tensor np array np array device=device test_ctor_with_invalid_numpy_array_sequence device Invalid list numpy array assertRaisesRegex ValueError expected sequence length torch tensor np random random size= np random random size= device=device Invalid list list numpy array assertRaisesRegex ValueError expected sequence length torch tensor np random random size= np random random size= device=device assertRaisesRegex ValueError expected sequence length torch tensor np random random size= np random random size= np random random size= np random random size= device=device expected shape ` ` hence we try iterate over -D array leading type error sequence assertRaisesRegex TypeError sequence torch tensor np random random size= np random random device=device list list numpy array assertRaisesRegex ValueError expected sequence length torch tensor np random random size= device=device onlyCPU test_ctor_with_numpy_scalar_ctor device - None dtypes = np double np float np float np int np int np int np uint np bool_ dtype dtypes assertEqual dtype torch tensor dtype item onlyCPU test_numpy_index device i = np array dtype=np int x = torch randn idx i assertFalse isinstance idx int assertEqual x idx x int idx onlyCPU test_numpy_index_multi device dim_sz i = np zeros dim_sz dim_sz dim_sz dtype=np int i dim_sz = x = torch randn dim_sz dim_sz dim_sz assertTrue x i == numel == np sum i onlyCPU test_numpy_array_interface device types = torch DoubleTensor torch FloatTensor torch HalfTensor torch LongTensor torch IntTensor torch ShortTensor torch ByteTensor dtypes = np float np float np float np int np int np int np uint tp dtype zip types dtypes Only concrete can given where Type number _ Bit expected np dtype dtype kind == u type ignore misc type expects XxxTensor which have no type hints purpose so ignore during mypy type checking x = torch tensor type tp type ignore call-overload array = np array dtype=dtype x = torch tensor - - type tp type ignore call-overload array = np array - - dtype=dtype Test __array__ w o dtype argument asarray = np asarray x assertIsInstance asarray np ndarray assertEqual asarray dtype dtype i range len x assertEqual asarray i x i Test __array_wrap__ same dtype abs_x = np abs x abs_array = np abs array assertIsInstance abs_x tp i range len x assertEqual abs_x i abs_array i Test __array__ dtype argument dtype dtypes x = torch IntTensor - - asarray = np asarray x dtype=dtype assertEqual asarray dtype dtype Only concrete can given where Type number _ Bit expected np dtype dtype kind == u type ignore misc wrapped_x = np array - - astype dtype i range len x assertEqual asarray i wrapped_x i i range len x assertEqual asarray i x i Test some math functions float types float_types = torch DoubleTensor torch FloatTensor float_dtypes = np float np float tp dtype zip float_types float_dtypes x = torch tensor type tp type ignore call-overload array = np array dtype=dtype func sin sqrt ceil ufunc = getattr np func res_x = ufunc x res_array = ufunc array assertIsInstance res_x tp i range len x assertEqual res_x i res_array i Test functions boolean value tp dtype zip types dtypes x = torch tensor type tp type ignore call-overload array = np array dtype=dtype geq _x = np greater_equal x geq _array = np greater_equal array astype uint assertIsInstance geq _x torch ByteTensor i range len x assertEqual geq _x i geq _array i onlyCPU test_multiplication_numpy_scalar device - None np_dtype np float np float np int np int np int np uint t_dtype torch float torch double mypy raises error when np floatXY called even though valid code np_sc = np_dtype type ignore abstract arg-type t = torch ones requires_grad=True dtype=t_dtype r = t np_sc assertIsInstance r torch Tensor assertTrue r dtype == t_dtype assertTrue r requires_grad r = np_sc t assertIsInstance r torch Tensor assertTrue r dtype == t_dtype assertTrue r requires_grad onlyCPU skipIfTorchDynamo test_parse_numpy_int_overflow device assertRaises uses try-except which dynamo has issues Only concrete can given where Type number _ Bit expected np __version__ assertRaisesRegex OverflowError out bounds lambda torch mean torch randn np uint - type ignore call-overload assertRaisesRegex ValueError Overflow &#124; integer required lambda torch mean torch randn np uint - type ignore call-overload onlyCPU test_parse_numpy_int device https github com pytorch pytorch issues nptype np int np int np uint np int np int scalar = np_arr = np array scalar dtype=nptype np_val = np_arr np integral type can treated python int native functions int parameters assertEqual torch ones diag scalar torch ones diag np_val assertEqual torch ones mean scalar torch ones mean np_val numpy integral type parses like python int custom python bindings assertEqual torch Storage np_val size scalar type ignore attr-defined tensor = torch tensor dtype=torch int tensor = np_val assertEqual tensor np_val Original reported issue np integral type parses correct PyTorch integral type when passed ` Scalar ` parameter arithmetic operations t = torch from_numpy np_arr assertEqual t + np_val dtype t dtype assertEqual np_val + t dtype t dtype test_has_storage_numpy device dtype np float np float np int np int np int np uint arr = np array dtype=dtype assertIsNotNone torch tensor arr device=device dtype=torch float storage assertIsNotNone torch tensor arr device=device dtype=torch double storage assertIsNotNone torch tensor arr device=device dtype=torch int storage assertIsNotNone torch tensor arr device=device dtype=torch long storage assertIsNotNone torch tensor arr device=device dtype=torch uint storage dtypes all_types_and_complex_and torch half torch bfloat torch bool test_numpy_scalar_cmp device dtype dtype is_complex tensors = torch tensor complex dtype=dtype device=device torch tensor complex j dtype=dtype device=device torch tensor complex - j dtype=dtype device=device tensors = torch tensor dtype=dtype device=device torch tensor - dtype=dtype device=device torch tensor - dtype=dtype device=device tensor tensors dtype == torch bfloat assertRaises TypeError np_array = tensor cpu numpy continue np_array = tensor cpu numpy t product tensor flatten tensor flatten item np_array flatten np_array flatten item assertEqual t dtype == torch complex torch is_tensor t type np complex TODO Imaginary part dropped case Need fix https github com pytorch pytorch issues assertFalse t == assertTrue t == onlyCPU dtypes all_types_and_complex_and torch half torch bool test___eq__ device dtype = make_tensor dtype=dtype device=device low=- high= b = detach clone b_np = b numpy Check all elements equal res_check = torch ones_like dtype=torch bool assertEqual == b_np res_check assertEqual b_np == res_check Check one element unequal dtype == torch bool b = b b += res_check = False assertEqual == b_np res_check assertEqual b_np == res_check Check random elements unequal rand = torch randint shape dtype=torch bool res_check = rand logical_not b copy_ dtype == torch bool b rand = b rand logical_not b rand += assertEqual == b_np res_check assertEqual b_np == res_check Check all elements unequal dtype == torch bool b copy_ logical_not b copy_ + res_check fill_ False assertEqual == b_np res_check assertEqual b_np == res_check onlyCPU test_empty_tensors_interop device x = torch rand dtype=torch float y = torch tensor np random rand dtype=torch float Same can achieved running y = torch empty_strided dtype=torch float Regression test https github com pytorch pytorch issues assertEqual torch true_divide x y shape y shape Regression test https github com pytorch pytorch issues assertEqual torch mul x y shape y shape Regression test https github com pytorch pytorch issues assertEqual torch div x y rounding_mode= floor shape y shape test_ndarray_astype_object_graph_break torch compile backend= eager fullgraph=True f xs xs astype O xs = np array assertRaisesRegex torch _dynamo exc Unsupported ndarray astype\\ object\\ f xs test_ndarray_astype_object_graph_break_ torch compile backend= eager fullgraph=True f xs xs astype object xs = np array assertRaisesRegex torch _dynamo exc Unsupported ndarray astype\\ object\\ f xs instantiate_device_type_tests TestNumPyInterop globals __name__ == __main__ run_tests