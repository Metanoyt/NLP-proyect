usr bin env python Benchmark script PyTorch DataLoader different worker methods This script measures Dataloader initialization time Dataloading speed time per batch CPU memory utilization Usage python dataloader_benchmark py -- data_path path dataset -- batch_size -- num_workers argparse copy gc time psutil torchvision torchvision transforms transforms torchvision models resnet torch torch nn nn torch optim optim torch utils data DataLoader torch utils data dataset ConcatDataset get_memory_usage Get current memory usage MB This includes all child processes Returns Total memory usage MB process = psutil Process main_memory = process memory_full_info pss Add memory usage all child processes child process children recursive=True try child_mem = child memory_full_info pss main_memory += child_mem except psutil NoSuchProcess psutil AccessDenied AttributeError Process might have terminated doesn t support PSS fall back USS print f Failed get PSS child falling back USS child_mem = child memory_info uss main_memory += child_mem main_memory print_detailed_memory Print detailed memory information process = psutil Process print \nDetailed memory information try print f USS Unique Set Size process memory_full_info uss f MB print f PSS Proportional Set Size process memory_full_info pss f MB print f RSS Resident Set Size process memory_info rss f MB except Exception print Detailed memory info available create_model Create simple model benchmarking model = resnet model benchmark_dataloader dataset batch_size num_workers num_epochs= max_batches= multiprocessing_context=None logging_freq= Benchmark dataloader specific configuration print \n --- Benchmarking DataLoader --- Clear memory before starting gc collect torch cuda empty_cache Create model model = create_model Measure memory before dataloader creation memory_before = get_memory_usage print f Memory before DataLoader creation memory_before f MB print_detailed_memory Measure dataloader initialization time start = time perf_counter dataloader = DataLoader dataset batch_size=batch_size shuffle=True num_workers=num_workers pin_memory=torch cuda is_available prefetch_factor= num_workers None multiprocessing_context=multiprocessing_context = iter dataloader dataloader_init_time = time perf_counter - start Measure memory after dataloader creation memory_after = get_memory_usage print f Memory after DataLoader creation memory_after f MB print f Memory increase memory_after - memory_before f MB Create model optimizer device = torch device cuda torch cuda is_available cpu model = model device criterion = nn CrossEntropyLoss optimizer = optim SGD model parameters lr= momentum= Benchmark dataloading speed model train total_batches = total_samples = total_time = total_data_load_time = Measure peak memory during training peak_memory = memory_after print f \nStarting training loop num_epochs epochs max max_batches batches per epoch epoch range num_epochs while total_batches max_batches batch_start = time perf_counter try inputs labels = next except StopIteration break Move data device inputs = inputs device labels = labels device Capture data fetch time including sending device data_load_time = time perf_counter - batch_start Forward pass outputs = model inputs loss = criterion outputs labels Backward optimize optimizer zero_grad loss backward optimizer step Capture batch time batch_time = time perf_counter - batch_start total_batches += total_samples += inputs size total_data_load_time += data_load_time total_time += batch_time Update peak memory log memory usage periodically total_batches == Force garbage collection before measuring memory gc collect current_memory = get_memory_usage current_memory peak_memory peak_memory = current_memory total_batches logging_freq == print f Epoch epoch + Batch total_batches f Time batch_time f s f Memory current_memory f MB Calculate statistics avg_data_load_time = total_data_load_time total_batches total_batches avg_batch_time = total_time total_batches total_batches samples_per_second = total_samples total_time total_time results = dataloader_init_time dataloader_init_time num_workers num_workers batch_size batch_size total_batches total_batches avg_batch_time avg_batch_time avg_data_load_time avg_data_load_time samples_per_second samples_per_second peak_memory_mb peak_memory memory_increase_mb peak_memory - memory_before print \nResults print f DataLoader init time dataloader_init_time f seconds print f Average data loading time avg_data_load_time f seconds print f Average batch time avg_batch_time f seconds print f Samples per second samples_per_second f print f Peak memory usage peak_memory f MB print f Memory increase peak_memory - memory_before f MB Clean up del model optimizer del dataloader Force garbage collection gc collect torch cuda empty_cache results main parser = argparse ArgumentParser description= Benchmark PyTorch DataLoader different worker methods parser add_argument -- data_path required=True help= Path dataset parser add_argument -- batch_size type=int default= help= Batch size parser add_argument -- num_workers type=int default= help= Number workers parser add_argument -- max_batches type=int default= help= Maximum number batches per epoch parser add_argument -- num_epochs type=int default= help= Number epochs parser add_argument -- multiprocessing_context choices= fork spawn forkserver default= forkserver help= Multiprocessing context use fork spawn forkserver parser add_argument -- dataset_copies type=int default= help= Number copies dataset concatenate testing memory usage parser add_argument -- logging_freq type=int default= help= Frequency logging memory usage during training args = parser parse_args Print system info print System Information The following handy debugging building source worked correctly print f PyTorch version torch __version__ print f PyTorch location torch __file__ print f Torchvision version torchvision __version__ print f Torchvision location torchvision __file__ print f CUDA available torch cuda is_available torch cuda is_available print f CUDA device torch cuda get_device_name print f CPU count psutil cpu_count logical=True print f Physical CPU cores psutil cpu_count logical=False print f Total system memory psutil virtual_memory total f GB Define transforms transform = transforms Compose transforms Resize transforms CenterCrop transforms ToTensor transforms Normalize mean= std= Load dataset print f \nLoading dataset args data_path args dataset_copies copies Try load ImageFolder datasets = _ range args dataset_copies base_dataset = torchvision datasets ImageFolder args data_path transform=transform datasets append copy deepcopy base_dataset del base_dataset dataset = ConcatDataset datasets print f Dataset size len dataset Run benchmark specified worker method benchmark_dataloader dataset batch_size=args batch_size num_workers=args num_workers multiprocessing_context=args multiprocessing_context num_epochs=args num_epochs max_batches=args max_batches logging_freq=args logging_freq __name__ == __main__ main