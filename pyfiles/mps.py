This feature-complete compiler backend Just early prototype shows one can compile elementwise ops into Metal shader __future__ annotations functools itertools logging math pathlib Path typing Any Optional TYPE_CHECKING sympy sympy printing precedence PRECEDENCE torch torch utils _cpp_embed_headers _embed_headers torch utils _ordered_set OrderedSet torch utils _sympy printers CppPrinter ExprPrinter ExprPrinter_ torch utils _sympy value_ranges ValueRanges utils ceildiv get_bounds_index_expr get_kernel_metadata virtualized ops OpsWrapper V common CSEVariable DeferredLine DTYPE_TO_COMPUTATION_DTYPE IndentedBuffer OpOverrides PythonPrinter simd IterationRangesEntry SIMDKernel SIMDScheduling TYPE_CHECKING typing Union ops_handler ReductionType StoreMode scheduler Scheduler SchedulerNode common OpVarT log = logging getLogger __name__ DTYPE_TO_METAL = torch bool bool torch int char torch int short torch int int torch int long torch uint uchar torch float float torch half half torch bfloat bfloat value_to_metal val Union float int bool str CSEVariable - str isinstance val float val == torch inf HUGE_VALF val == -torch inf -HUGE_VALF val = val Only float equal nan NAN str val isinstance val bool true val false str val MetalExprPrinter ExprPrinter_ Converts sympy expression Metal code snippet _print_FloorDiv expr sympy Expr - str x div = expr args x = doprint x div = doprint div expr is_integer f c metal floor_divide x div f metal floor x div _print_ModularIndexing expr sympy Expr - str x div mod = expr args x = doprint x div = div = doprint div expr is_integer x = f x div x = f metal floor x div mod = doprint mod f x mod _print_Min expr sympy Expr - str len expr args = raise RuntimeError metal min only supported args b = map _print expr args typecast_a = f static_cast decltype + b typecast_b = f static_cast decltype + b b f metal min typecast_a typecast_b _print_Max expr sympy Expr - str len expr args = raise RuntimeError metal max only supported args b = map _print expr args typecast_a = f static_cast decltype + b typecast_b = f static_cast decltype + b b f metal max typecast_a typecast_b _print_Abs expr sympy Expr - str assert len expr args == f metal abs _print expr args _print_RoundToInt expr sympy Expr - str assert len expr args == f static_cast long metal rint _print expr args _print_RoundDecimal expr sympy Expr - str assert len expr args == number ndigits = expr args number is_integer ndigits should have been filtered sympy function assert ndigits raise ValueError f For integer inputs only non-negative ndigits currently supported got ndigits number_str = parenthesize number PRECEDENCE Mul f static_cast float metal rint e ndigits number_str e -ndigits _print_IntTrueDiv expr sympy Expr - str lhs rhs = expr args TODO This only accurate up f static_cast float _print lhs static_cast float _print rhs _print_PowByNatural expr sympy Expr - str assert len expr args == x y = map doprint expr args f metal pow static_cast float x static_cast float y _print_ToFloat expr sympy Expr - str assert len expr args == x = doprint expr args f static_cast float x _print_Float expr sympy Expr - str expr is_integer sympy considers integer Metal doesn t workaround prints float integer xref https github com sympy sympy issues str int expr str expr _print_FloorToInt expr sympy Expr - str assert len expr args == x = doprint expr args f static_cast int metal floor static_cast float x _print_floor = _print_FloorToInt _print_TruncToInt expr sympy Expr - str assert len expr args == x = doprint expr args f static_cast int metal trunc x _print_OpaqueUnaryFn_log expr sympy Expr - str assert len expr args == x = doprint expr args f metal log x MetalOverrides OpOverrides Implements Metal-specific overrides ops Base emits Python-friendly overrides staticmethod to_dtype x CSEVariable dtype torch dtype src_dtype Optional torch dtype = None use_compute_types bool = True - str dtype == torch double log warning float cast requested probably tensorify_python_scalars f static_cast float x f static_cast DTYPE_TO_METAL dtype x staticmethod to_dtype_bitcast x CSEVariable dtype torch dtype src_dtype torch dtype - str f as_type DTYPE_TO_METAL dtype static_cast DTYPE_TO_METAL src_dtype x staticmethod constant val Union bool float int dtype torch dtype - str value_to_metal val staticmethod index_expr expr sympy Expr dtype torch dtype - str idx_str = V kernel index_to_str V kernel prepare_indexing expr var = V kernel cse generate V kernel compute idx_str bounds=get_bounds_index_expr expr ops to_dtype var dtype staticmethod masked mask CSEVariable body sympy Expr other CSEVariable - str TODO Type annotation other wrong s often float int V kernel mask_loads mask other new_mask result = body result bounds is_bool other = bool other type ignore assignment ops where new_mask result other staticmethod where OpVarT b OpVarT c OpVarT - str f b value_to_metal c staticmethod remainder OpVarT b OpVarT - str f c metal remainder b staticmethod maximum CSEVariable b CSEVariable - str typecast_a = f static_cast decltype + b typecast_b = f static_cast decltype + b b f c metal max typecast_a typecast_b staticmethod minimum CSEVariable b CSEVariable - str typecast_a = f static_cast decltype + b typecast_b = f static_cast decltype + b b f c metal min typecast_a typecast_b staticmethod logical_or CSEVariable b CSEVariable - str f &#124; &#124; b staticmethod logical_and CSEVariable b CSEVariable - str f b staticmethod isnan x CSEVariable - str f metal isnan x staticmethod isinf x CSEVariable - str f metal isinf x staticmethod log x CSEVariable - str f metal log x staticmethod exp x CSEVariable - str f metal exp x staticmethod abs x CSEVariable - str f metal abs x staticmethod signbit x CSEVariable - str f metal signbit x staticmethod sin x CSEVariable - str f metal precise sin x staticmethod sinc x CSEVariable - str f c metal sinc x staticmethod cos x CSEVariable - str f metal precise cos x staticmethod tan x CSEVariable - str f metal tan x staticmethod asin x CSEVariable - str f metal asin x staticmethod acos x CSEVariable - str f metal acos x staticmethod atan x CSEVariable - str f metal atan x staticmethod atan x CSEVariable y CSEVariable - str f metal atan x y staticmethod sqrt x CSEVariable - str f metal sqrt x staticmethod neg x CSEVariable - str TODO Does rely undefined behavior If so add special logic unsigned types f static_cast decltype x - x staticmethod rsqrt x CSEVariable - str f metal rsqrt x staticmethod tanh x CSEVariable - str f metal tanh x staticmethod atanh x CSEVariable - str f metal atanh x staticmethod floordiv CSEVariable b CSEVariable - str b must integer type f c metal floor_divide b staticmethod floor x CSEVariable - str f metal floor x staticmethod sign x CSEVariable - str f metal sign x staticmethod fmod CSEVariable b CSEVariable - str typecast_a = f static_cast decltype + b typecast_b = f static_cast decltype + b b f metal fmod typecast_a typecast_b staticmethod trunc x CSEVariable - str f metal trunc x staticmethod truncdiv CSEVariable b CSEVariable - str quot = f b dtype None dtype is_floating_point b dtype None b dtype is_floating_point f metal trunc quot quot staticmethod ceil x CSEVariable - str f metal ceil x staticmethod rand seed CSEVariable offset CSEVariable - str V kernel headers add random f c metal rand seed offset staticmethod randn seed CSEVariable offset CSEVariable - str V kernel headers add random f c metal randn seed offset staticmethod randint seed CSEVariable offset CSEVariable low CSEVariable high CSEVariable - str V kernel headers add random f c metal randint seed offset low high staticmethod round x CSEVariable - str f metal rint x staticmethod pow CSEVariable b CSEVariable - str cast_a = f static_cast decltype + b cast_b = f static_cast decltype + b b f metal pow cast_a cast_b _special_unary CSEVariable name str - str V kernel headers add special_math f c metal name _special_binary CSEVariable b CSEVariable name str - str V kernel headers add special_math f c metal name b classmethod _initialize_special_ops cls - None Unary special ops name erf erfinv i i e i i e digamma spherical_bessel_j setattr cls name functools partialmethod cls _special_unary name=name cls lgamma = functools partialmethod cls _special_unary name= log_gamma type ignore assignment Unary special ops forward method name name bessel_j bessel_j bessel_y bessel_y modified_bessel_i modified_bessel_i modified_bessel_k modified_bessel_k scaled_modified_bessel_k scaled_modified_bessel_k setattr cls name functools partialmethod cls _special_unary name=name + _forward Binary special ops name polygamma igamma igammac zeta setattr cls name functools partialmethod cls _special_binary name=name Binary special ops forward method name name chebyshev_polynomial_t chebyshev_polynomial_u chebyshev_polynomial_v chebyshev_polynomial_w hermite_polynomial_h hermite_polynomial_he shifted_chebyshev_polynomial_t shifted_chebyshev_polynomial_u shifted_chebyshev_polynomial_v shifted_chebyshev_polynomial_w setattr cls name functools partialmethod cls _special_binary name=name + _forward MetalOverrides _initialize_pointwise_overrides mps MetalOverrides _initialize_special_ops MetalKernel SIMDKernel Implement Metal codegen based SIMDKernel abstraction overrides = MetalOverrides type ignore assignment suffix = newvar_prefix = auto max_threadgroup_size = simd_group_size = pexpr = PythonPrinter doprint cexpr = CppPrinter doprint sexpr = MetalExprPrinter doprint kexpr = sexpr headers OrderedSet str = OrderedSet utils multistage_reduction_entry list IterationRangesEntry = __init__ tiling dict str sympy Expr kwargs Any - None super __init__ tiling kwargs acc_var_ids = itertools count dtype_to_str dtype torch dtype - str DTYPE_TO_METAL dtype load name str index sympy Expr - CSEVariable Codegen load InputBuffer var = args input name index = prepare_indexing index dtype = V graph get_dtype name line = f var index_to_str index dtype torch float torch bfloat TODO NS Figure out right balance between optype casts op_math_t half-precision floats should float Otherwise can lead correctness issues eager line = f static_cast float line dtype = torch float cse generate loads line dtype=dtype store name str index sympy Expr value CSEVariable mode StoreMode = None - None var = args output name index = prepare_indexing index dtype_str = dtype_to_str V graph get_dtype name cast_val = f static_cast dtype_str value mode None line = f var index_to_str index = cast_val mode == atomic_add headers add atomic atomic_type = f c metal AtomicType dtype_str cast_var = f reinterpret_cast device atomic_type type var line = f atomic_type atomic_add cast_var index_to_str index cast_val raise RuntimeError f Unimplemented store mode mode inside_reduction compute writeline DeferredLine name line stores writeline DeferredLine name line store_reduction name str index sympy Expr value CSEVariable - None var = args output name index = prepare_indexing index dtype_str = dtype_to_str V graph get_dtype name pyrefly ignore missing-argument reduction_dim = next t t range_trees t is_reduction Only one thread reduction group needs store results line = f var index_to_str index = static_cast dtype_str value line = f reduction_dim name == line stores writeline DeferredLine name line _new_idxvar dtype Union str &#124; torch dtype elem_count Optional int = None default_value Optional Any = None is_threadgroup bool = True bounds ValueRanges Any = ValueRanges unknown - CSEVariable isinstance dtype torch dtype dtype = dtype_to_str dtype var_name = f tmp_acc_ next acc_var_ids var = V kernel create_cse_var var_name bounds dtype var_def = threadgroup is_threadgroup var_def += f dtype var_name elem_count var_def += f sexpr elem_count default_value None assert is_threadgroup Thread group var can have default value var_def += f = default_value indexing_code writeline var_def + suffix var reduction dtype torch dtype src_dtype torch dtype reduction_type ReductionType value Union CSEVariable tuple CSEVariable - Union CSEVariable tuple CSEVariable Caching wrapper around _reduction_nocache cache_key = src_dtype reduction_type value Return cached reduction cache_key cse reduction_cache cse reduction_cache cache_key result = _reduction_nocache dtype src_dtype reduction_type value cse reduction_cache cache_key = result type ignore assignment result _reduction_nocache dtype torch dtype src_dtype torch dtype reduction_type ReductionType value Union CSEVariable tuple CSEVariable - Union CSEVariable tuple CSEVariable Codegen reduction operation Only sum prod operations somewhat reasonable optimized assert inside_reduction assert _load_mask _unwrap_helper res CSEVariable - tuple CSEVariable Uwraps vec dtype into individual components OpsWrapper _unwrap CSEVariable f res t res bounds res dtype t xyz Establish reduction buffer size index expression reduction_idx = acc_buf_size = rd range_trees pyrefly ignore missing-argument rd is_reduction continue reduction_idx reduction_idx += + reduction_idx += f rd name acc_buf_size isinstance rd numel sympy Integer acc_buf_size = rd numel acc_buf_size = sympy Symbol f rd prefix numel integer=True positive=True acc_buf_size = sympy Min acc_buf_size max_threadgroup_size acc_buf_size_str = sexpr acc_buf_size shmem_buf_size = ceildiv acc_buf_size simd_group_size isinstance acc_buf_size sympy Integer simd_group_size reduction_type == any acc = _new_idxvar dtype indexing_code writeline f acc = false indexing_code writeline threadgroup_barrier metal mem_flags mem_threadgroup compute splice f value acc = true stores writeline threadgroup_barrier metal mem_flags mem_threadgroup acc headers add reduction_utils reduction_type prod sum acc_dtype = DTYPE_TO_COMPUTATION_DTYPE src_dtype acc_buf = _new_idxvar acc_dtype shmem_buf_size multistage_reduction_entry val = value default_val reduction_op = + reduction_type == sum val = _new_idxvar acc_dtype default_value=default_val is_threadgroup=False compute splice f val reduction_op = value cse generate stores f c metal threadgroup_ reduction_type acc_buf val reduction_idx acc_buf_size_str dtype=DTYPE_TO_COMPUTATION_DTYPE dtype reduction_type max min acc_buf = _new_idxvar src_dtype shmem_buf_size src_metal_type = DTYPE_TO_METAL src_dtype cast_value = f static_cast src_metal_type value multistage_reduction_entry val = cast_value type ignore assignment lim_fn = lowest reduction_type endswith max max limit_val = f metal numeric_limits src_metal_type lim_fn val = _new_idxvar src_dtype default_value=limit_val is_threadgroup=False compute splice f val = c metal reduction_type val cast_value cse generate stores f c metal threadgroup_ reduction_type acc_buf val reduction_idx acc_buf_size_str dtype=DTYPE_TO_COMPUTATION_DTYPE dtype reduction_type argmin argmax data_acc_buf = _new_idxvar src_dtype shmem_buf_size idx_acc_buf = _new_idxvar dtype shmem_buf_size src_metal_type = DTYPE_TO_METAL src_dtype cast_value = f static_cast src_metal_type value multistage_reduction_entry val = cast_value type ignore assignment idx_val = f static_cast DTYPE_TO_METAL dtype reduction_idx lim_fn = lowest reduction_type endswith max max limit_val = f metal numeric_limits src_metal_type lim_fn val = _new_idxvar src_dtype default_value=limit_val is_threadgroup=False idx_val = _new_idxvar dtype default_value= is_threadgroup=False type ignore assignment idx_var = next t t range_tree_nodes values pyrefly ignore missing-argument t is_reduction cmp_op = reduction_type == argmax nan_suffix = f &#124; &#124; metal isnan value src_dtype is_floating_point compute splice f value cmp_op val nan_suffix val = value idx_val = idx_var name cse generate stores f c metal threadgroup_ reduction_type data_acc_buf idx_acc_buf f val idx_val reduction_idx acc_buf_size_str dtype=dtype reduction_type == welford_reduce multistage_reduction_entry acc_buf = _new_idxvar src_dtype acc_buf_size compute splice f acc_buf reduction_idx = value wf_res = cse generate compute f c metal threadgroup_ reduction_type acc_buf acc_buf_size_str dtype=torch float _unwrap_helper wf_res acc_buf = _new_idxvar float acc_buf_size acc_thread_var = f acc_buf reduction_idx indexing_code splice f acc_thread_var = compute writeline f acc_thread_var = c metal welford_combine acc_thread_var float value wf_res = cse generate stores f c metal threadgroup_welford_combine acc_buf acc_buf_size dtype=torch float _unwrap_helper wf_res reduction_type == welford_combine assert isinstance value tuple Input welford combine must tuple acc_buf = _new_idxvar float acc_buf_size acc_thread_var = f acc_buf reduction_idx inp_value = f float value value value indexing_code splice f acc_thread_var = multistage_reduction_entry indexing_code splice f acc_thread_var = compute writeline f acc_thread_var = c metal welford_combine acc_thread_var inp_value compute writeline f acc_thread_var = inp_value wf_res = cse generate stores multistage_reduction_entry compute f c metal threadgroup_ reduction_type acc_buf acc_buf_size_str dtype=torch float _unwrap_helper wf_res raise NotImplementedError reduction_type codegen_iteration_ranges_entry entry IterationRangesEntry - None index_expr = rename_indexing entry expr index_str = sexpr index_expr type ignore misc pyrefly ignore missing-argument entry is_reduction isinstance entry root numel sympy Integer entry root numel = max_threadgroup_size indexing_code writeline f index_dtype entry name = index_str acc_size = entry root numel isinstance entry root numel sympy Integer sympy Symbol f entry root prefix numel integer=True positive=True multistage_reduction_entry append entry When reducing tensor whose size exceeds max threadgroup size loop over extra indices per reduction thread perform part operation using values shared memory Use floats so doesn t do integer division loop_size = acc_size + float max_threadgroup_size - float max_threadgroup_size loop_size_str = sexpr loop_size body writeline f auto entry name _cnt = entry name _cnt loop_size_str ++ entry name _cnt body indent isinstance acc_size sympy Symbol body writeline f index_dtype entry name = max_threadgroup_size entry name _cnt + index_str body writeline f index_dtype entry name = loop_size_str index_str + entry name _cnt Check reduction performed only within tensor boundary isinstance acc_size sympy Symbol loop_size max_threadgroup_size = acc_size body writeline f entry name = acc_size break codegen_body - None Concat output code index_code loads compute stores suffix into body For pointwise kernels called just once end For reduction kernels generates loop over reduction axis multistage_reduction_entry body indent body splice loads body splice compute body writeline len multistage_reduction_entry Invalidate variables instantiated inside loop But results reduction alive Reduction cache values can either CSEVariable tuple CSEVariables which case all variables tuple must preserved cse invalidate OrderedSet v item cse reduction_cache values v item isinstance item tuple item And loop codegen while multistage_reduction_entry multistage_reduction_entry pop cache_clear body splice loads body splice compute body splice stores loads clear compute clear stores clear codegen_kernel name Optional str = None - str Called end generate final kernel string codegen_body code = IndentedBuffer V graph cpp_wrapper code writeline R MTL code writeline compile_mps_shader idx_vars = active_range_trees code indent V graph cpp_wrapper header headers code writeline f #include c metal header h headers = f #include c metal header h header headers header_contents = _embed_headers headers Path __file__ parent parent parent include OrderedSet type ignore arg-type code writeline header_contents inside_reduction total_reduction_size = math prod t numel t range_trees pyrefly ignore missing-argument t is_reduction If using dynamic shapes set threadgroup size max possible size threadgroup_size = min total_reduction_size max_threadgroup_size isinstance total_reduction_size sympy Integer max_threadgroup_size code writeline f max_total_threads_per_threadgroup threadgroup_size code writeline kernel void generated_kernel code indent outer inner args output_buffers items outer removed_buffers continue dtype_str = dtype_to_str V graph get_dtype outer code writeline f device dtype_str inner outer inner args input_buffers items dtype = V graph get_dtype outer MPS does support float scalar inputs fine dtype == torch float outer_buf = V graph try_get_buffer outer outer_buf None outer_buf get_size = raise RuntimeError float supported MPS dtype_str = float dtype_str = dtype_to_str dtype code writeline f constant dtype_str inner inner args sizevars values code writeline f constant long inner Write dynamic values inputs idx_var idx_vars isinstance idx_var numel sympy Integer pass code writeline f constant long idx_var prefix numel assert len idx_vars Up index variables supported thread_pos_dtype = f uint len idx_vars len idx_vars uint thread_pos_var_name = idx_vars name len idx_vars == thread_pos thread_pos_suffix = inside_reduction code writeline f thread_pos_dtype thread_pos_var_name thread_position_in_grid thread_pos_suffix inside_reduction code writeline f thread_pos_dtype group_pos thread_position_in_threadgroup code writeline code indent len idx_vars idx var enumerate idx_vars code writeline f auto var name = thread_pos chr + idx code splice indexing_code code splice body code writeline V graph cpp_wrapper code writeline MTL code writeline code getvalue call_kernel name str node Any = None deallocate_ws bool = True - None Codegens call kernel wrapper = V graph wrapper_code Make sure sizevars has been computed v args sizevars keys wrapper ensure_size_computed v _ call_args _ arg_types = args python_argdefs arg_name_to_type = str call_arg arg_type call_arg arg_type zip call_args arg_types args = args output_buffers keys args input_buffers keys args = arg arg args arg removed_buffers args += str v v args sizevars keys arg_types = arg_name_to_type arg arg args Add any dynamic ints inputs tree range_trees isinstance tree numel sympy Integer int Don t need pass integers inputs continue isinstance tree numel sympy Symbol expr = tree numel expr = V graph wrapper_code generate_numel_expr name tree inner pyrefly ignore missing-argument tree is_reduction inside_reduction args append str expr arg_types append int expr_printer = cexpr V graph cpp_wrapper pexpr format_threads threads list str kwarg str - str V graph cpp_wrapper threads = f static_cast uint _t t t threads f join threads f kwarg = join threads For reduction kernels limit maximum size over reduction dimensions maximum threadgroup size len active_range_trees threads = expr_printer sympy Min v numel max_threadgroup_size type ignore misc pyrefly ignore missing-argument v is_reduction v numel v active_range_trees args append format_threads threads threads arg_types append list V graph cpp_wrapper raise RuntimeError We should always have threads inside_reduction threads = expr_printer sympy Min v numel max_threadgroup_size type ignore misc pyrefly ignore missing-argument v is_reduction v active_range_trees args append format_threads threads group_size arg_types append list V graph cpp_wrapper Add None so we always have group_size arguments We won t use value None args += None type ignore list-item arg_types append None wrapper generate_kernel_call name args device=torch device mps triton=False arg_types=arg_types check_bounds expr sympy Expr size sympy Expr lower bool upper bool - None lower upper TODO malfet support asserts See https github com pytorch pytorch issues expr_str = index_to_str expr lower_expr = f expr_str lower TODO malfet Is upper bound inclusive exclusive upper_expr = f expr_str index_to_str size upper lower upper line = f lower_expr upper_expr line = f lower_expr upper_expr cse generate compute line assignment=False MetalScheduling SIMDScheduling kernel_type = MetalKernel type ignore assignment __init__ scheduler Optional Scheduler - None super __init__ scheduler wrapper = V graph wrapper_code wrapper None V graph cpp_wrapper wrapper header splice torch _inductor runtime runtime_utils compile_mps_shader define_kernel src_code str node_schedule list SchedulerNode kernel MetalKernel - str wrapper = V graph wrapper_code src_code wrapper src_to_kernel kernel_name = wrapper src_to_kernel src_code TODO Merge multiple kernels into single library Either using MultiKernel concept overriding SIMDScheduling codegen_node_scheduling mps_lib_name = f mps_lib_ wrapper next_kernel_suffix kernel_name = f mps_lib_name wrapper src_to_kernel src_code = kernel_name V graph cpp_wrapper For shimified version generate source constant instead direct instantiation src_code = f const char mps_lib_name _source = + src_code origins detailed_origins = get_kernel_metadata node_schedule wrapper metadata_comment = f origins \n detailed_origins wrapper define_kernel mps_lib_name src_code metadata_comment gpu=False kernel_name