functools heapq itertools logging sys collections Counter defaultdict collections abc Iterable dataclasses dataclass typing Any Callable torch torch fx fx torch _dynamo utils counters dynamo_timed torch _inductor fx_passes bucketing is_wait_tensor torch _inductor fx_passes memory_estimator _is_releasable build_memory_profile MemoryTracker torch fx operator_schemas normalize_function torch utils _ordered_set OrderedSet torch utils _python_dispatch _disable_current_modes log = logging getLogger __name__ torch _inductor fx_passes bucketing bucket_key pattern_matcher stable_topological_sort get_group_name n fx Node - str Extract group name collective operation node opt_args_kwargs = normalize_function n target type ignore arg-type args=n args kwargs=n kwargs normalize_to_only_use_kwargs=True assert opt_args_kwargs None _ kwargs = opt_args_kwargs kwargs group_name get_custom_estimation n fx Node custom_runtime_estimation Callable fx Node float &#124; None &#124; None = None - float &#124; None custom_runtime_estimation None None custom_runtime_estimation n estimate_collective_time n fx Node override_size int &#124; None = None custom_runtime_estimation Callable fx Node float &#124; None &#124; None = None - float Estimate runtime collective operation optionally overridden size est = get_custom_estimation n custom_runtime_estimation None est torch _inductor comm_analysis estimate_nccl_collective_runtime_from_fx_node n override_size estimate_fx_collective_size fx_node torch fx Node - int size = node fx_node all_input_nodes t = node meta get val None todo - symbolic size += t numel t element_size size is_compute_node n fx Node - bool Should we consider node computationally expensive Currently uses flop registration we could expand more generally getattr n target overloadpacket None torch utils flop_counter flop_registry get_hint x int &#124; torch SymInt - int &#124; None isinstance x int x assert isinstance x torch SymInt x node has_hint None x node hint get_collective_do_bench - Callable Callable Any float dynamo_timed collective_compute_do_bench functools partial pyrefly ignore bad-argument-type torch _inductor runtime benchmarking benchmarker benchmark_gpu warmup= benchmark_node_with_cache_key n fx Node custom_runtime_estimation Callable fx Node float &#124; None &#124; None = None - tuple float str &#124; None assert is_compute_node n torch _dynamo testing rand_strided todo - skip unbacked symbolic success args kwargs = torch _inductor fx_utils get_fake_args_kwargs n success None unbacked_tensor = False key = f str n target to_real t torch Tensor - torch Tensor &#124; None shape = get_hint dim dim t shape stride = get_hint s s t stride any s None s itertools chain shape stride nonlocal unbacked_tensor unbacked_tensor = True None nonlocal key key += f T shape stride t dtype rand_strided shape stride device=t device dtype=t dtype type ignore arg-type _disable_current_modes args kwargs = torch utils _pytree tree_map_only torch Tensor lambda t to_real t args kwargs val = get_cached_node_time key val key unbacked_tensor key est = get_custom_estimation n custom_runtime_estimation None set_cached_node_time key est est key bench = get_collective_do_bench out = bench lambda n target args kwargs type ignore operator set_cached_node_time key out out key benchmark_node n fx Node custom_runtime_estimation Callable fx Node float &#124; None &#124; None = None - float benchmark_node_with_cache_key n custom_runtime_estimation functools cache get_benchmark_cache - torch _inductor codecache LocalCache torch _inductor codecache LocalCache get_cached_node_time key str - float get_benchmark_cache lookup key type ignore return-value set_cached_node_time key str value float - None get_benchmark_cache set_value key value=value dataclass CollectiveInfo Track info about collective operation start_node fx Node wait_node fx Node size_bytes int estimated_time_ms float exposed_time_ms float How much collective still exposed hiding_node fx Node &#124; None = None Node hides collective property is_exposed - bool exposed_time_ms = dataclass CollBucket Track information about bucket collectives collectives list fx Node Original collective starts bucketed_start fx Node &#124; None = None After bucketing bucketed_wait fx Node &#124; None = None After bucketing total_bytes int = gb_to_bytes gb float - int Convert gigabytes bytes int gb OverlapScheduler Scheduler reorders operations maximize compute-collective overlap The reordering done scheduling pass We maintain priority queue schedulable nodes The nodes ranked compute node index they dominate allows reordering locally such parallel mms also allows overlapping reduce scatter nodes outputs backward compute deferring their waits whether current node collective wait currently exposed has compute node which could overlapped original order graph stability When we schedule compute nodes we first overlap exposed in-flight collectives then look unscheduled collectives can scheduled concurrently TODO - experiment other priority scores allow other mechanisms reorder more strict adherence original graph - memory limit deferred scheduling reduce_scatter nodes __init__ gm torch fx GraphModule max_in_flight_gb float max_compute_pre_fetch int collective_bucketing bool insert_overlap_deps bool compute_overlap_multipler float max_coll_distance int custom_runtime_estimation Callable fx Node float &#124; None &#124; None gm = gm graph = gm graph compute_overlap_multipler = compute_overlap_multipler max_node_distance = max_coll_distance max_in_flight_bytes int = gb_to_bytes max_in_flight_gb custom_runtime_estimation = custom_runtime_estimation collective_bucketing = collective_bucketing insert_overlap_deps = insert_overlap_deps max_compute_pre_fetch = max_compute_pre_fetch Build structures stable_topological_sort graph nodes = list graph nodes node_idx = n i i n enumerate nodes node_ancestors dict fx Node OrderedSet fx Node = _collect_node_ancestors Identify collectives compute nodes collective_info dict fx Node CollectiveInfo = unscheduled_collectives OrderedSet fx Node = OrderedSet Memory tracking using abstracted MemoryTracker original_peak_memory = max build_memory_profile graph _is_releasable memory_tracker = MemoryTracker graph wait_to_start dict fx Node fx Node = _identify_collectives compute_index_domination = _calculate_compute_node_domination_index compute_nodes = n n nodes is_compute_node n current_compute_index = Scheduling state potentially_hidden_collectives = compute_potential_hidden_collectives potentially_hidden_waits = compute_potential_hidden_waits in_degree = Counter user node nodes user node users ready list tuple object fx Node = node nodes in_degree node == heapq heappush ready _compute_score node node in_flight dict fx Node CollectiveInfo = start - info in_flight_bytes = scheduled OrderedSet fx Node = OrderedSet max_compute_pre_fetch = max_compute_pre_fetch _collect_node_ancestors - dict fx Node OrderedSet fx Node Collect all ancestors each node ancestors dict fx Node OrderedSet fx Node = defaultdict OrderedSet node nodes input_node node all_input_nodes ancestors node add input_node ancestors node &#124; = ancestors input_node ancestors off_compute_path n fx Node - bool Check node off compute path doesn t block any compute compute_index_domination n == sys maxsize _identify_collectives - None Identify all collective operations node nodes is_wait_tensor node start = node args coll_time_ms = estimate_collective_time start custom_runtime_estimation=self custom_runtime_estimation info = CollectiveInfo start_node=start wait_node=node size_bytes=estimate_fx_collective_size start estimated_time_ms=coll_time_ms exposed_time_ms=coll_time_ms Initially fully exposed collective_info start = info wait_to_start node = start unscheduled_collectives add start _calculate_compute_node_domination_index - dict fx Node int Compute topological index earliest compute node each node dominates Compute nodes assigned indices based their topological order For each node returns minimum index compute nodes blocks dominates Returns sys maxsize node doesn t block any compute nodes compute_node_index dict fx Node int = node graph nodes is_compute_node node compute_node_index node = len compute_node_index domination_index dict fx Node int = node reversed graph nodes node compute_node_index Compute nodes dominate themselves their own index domination_index node = compute_node_index node domination_index node = min domination_index succ succ node users default=sys maxsize domination_index _align_compute_nodes_runtime_estimations_across_all_distributed_ranks - None log info Overlap scheduling Aligning runtime estimations across all distributed ranks runtime_estimations_keys list str &#124; None = runtime_estimations list float = n compute_nodes val key = benchmark_node_with_cache_key n custom_runtime_estimation runtime_estimations append val runtime_estimations_keys append key torch distributed dist torch _subclasses fake_tensor unset_fake_temporarily torch distributed distributed_c d _get_default_group world_size = dist get_world_size pg = _get_default_group unset_fake_temporarily gathered_runtime_estimations list list float = _ range world_size dist all_gather_object gathered_runtime_estimations runtime_estimations pg median_runtime_estimations = torch median torch tensor gathered_runtime_estimations dim= values tolist key median_runtime_estimation zip runtime_estimations_keys median_runtime_estimations key None continue set_cached_node_time key median_runtime_estimation log info Overlap scheduling Runtime estimations across all distributed ranks aligned run - torch fx GraphModule Run scheduling algorithm All ranks must make identical decisions overlap reordering Thus we must have identical runtime estimations across ranks For now we do benchmarking only compute nodes _align_compute_nodes_runtime_estimations_across_all_distributed_ranks while ready _should_force_wait_for_memory _force_oldest_wait continue _ node = heapq heappop ready we don t always remove nodes heap when we schedule them node scheduled continue is_compute_node node _handle_compute node node collective_info _handle_collective_start node is_wait_tensor node _handle_wait node _handle_other node _reorder_graph collective_bucketing _bucket_collectives insert_overlap_deps If bucketing add effect tokens preserve hiding dependencies _add_effect_tokens_for_overlap gm _add_effect_tokens_for_overlap - None Add effect tokens preserve hiding dependency relationships when bucketing This ensures communication-compute overlap preserved through effect tokens when overlap preserving bucketing enabled torch _inductor fx_passes control_dependencies preserve_node_ordering Collect hiding dependencies hiding_node - collective_start wait - hiding_node additional_deps dict fx Node OrderedSet fx Node = defaultdict OrderedSet start_node info collective_info items info hiding_node info is_exposed Compute depends collective start compute must wait collective start additional_deps info hiding_node add start_node Wait depends compute wait must wait compute finish additional_deps info wait_node add info hiding_node Apply effect tokens preserve these dependencies additional_deps preserve_node_ordering graph additional_deps _handle_other node fx Node - None _schedule node _schedule node fx Node - None Schedule node assert node scheduled assert all n scheduled n node all_input_nodes scheduled add node memory_tracker schedule_node node log debug Scheduled node s current_memory= d bytes total_scheduled= d node name memory_tracker get_current_memory_bytes len scheduled user node users in_degree user -= in_degree user == heapq heappush ready _compute_score user user _compute_score node fx Node - object Compute priority score node is_wait_tensor node info = collective_info wait_to_start node defer waits locally they exposed compute_local_priority = int info is_exposed we re scheduling collective via its queue then pre-fetched we might well maximize overlap local non-mm nodes prior next compute node in_overlappable_collective_unary_chain node compute_local_priority = - compute_local_priority = compute_index_domination node what index compute blocks compute_local_priority collective_start=- wait= neither= node_idx node Original order stability staticmethod is_cheap_fn node fx Node - bool getattr node target is_view False torch Tag pointwise getattr node target tags in_overlappable_collective_unary_chain curr fx Node - bool while True len curr users = False user = next iter curr users len user all_input_nodes = False user unscheduled_collectives True is_cheap_fn user False curr = user False _should_force_wait_for_memory - bool Check we need force wait due memory pressure in_flight False in_flight_bytes = max_in_flight_bytes memory_tracker current_memory_bytes - original_peak_memory gb_to_bytes _force_oldest_wait - None Schedule oldest flight wait _handle_wait _get_oldest_wait _handle_collective_start node fx Node - None Handle scheduling collective start info = collective_info node should_assume_bucketed node latency = estimate_collective_time node custom_runtime_estimation=self custom_runtime_estimation assert latency = info exposed_time_ms info exposed_time_ms = info exposed_time_ms - latency in_flight node = info in_flight_bytes += info size_bytes unscheduled_collectives discard node _schedule node _handle_wait node fx Node - None Handle scheduling wait assert node wait_to_start coll_start = wait_to_start node assert coll_start in_flight Scheduling wait collective also forces wait every node enqueued prior collective same process group group_name = get_group_name coll_start to_schedule list fx Node = in_flight_coll in_flight in_flight_coll == coll_start break get_group_name in_flight_coll == group_name to_schedule append in_flight_coll coll_to_schedule to_schedule _handle_wait collective_info coll_to_schedule wait_node in_flight_bytes -= in_flight coll_start size_bytes del in_flight coll_start _schedule node _handle_compute node fx Node - None Handle scheduling compute finding overlaps compute_time = benchmark_node node custom_runtime_estimation available_compute = compute_time compute_overlap_multipler TODO separate overlap time per process group First reduce exposed time in-flight collectives info in_flight values info exposed_time_ms == continue overlap_amount = min info exposed_time_ms available_compute info exposed_time_ms -= overlap_amount available_compute -= overlap_amount info exposed_time_ms == info hiding_node = node available_compute == break Then look unscheduled collectives we can overlap available_compute _schedule_collectives_for_overlap node available_compute _schedule node current_compute_index += _schedule_collectives_for_overlap compute_node fx Node available_compute_time float - None Opportunistically schedule collectives can hidden compute compute_ancestors = node_ancestors compute_node Filter collectives distance compute index domination possible_collectives = collective unscheduled_collectives distance = abs node_idx compute_node - node_idx collective distance max_node_distance break Skip collectives too far ahead compute index allow scheduling collectives which off compute path which typically release memory TODO we could potentially more strict about limiting amount pre-fetched memory before memory peak adjust allowed collective mem off_compute_path collective compute_index_domination collective - current_compute_index max_compute_pre_fetch continue possible_collectives append collective possible_collectives = sorted possible_collectives key=lambda n compute_index_domination n node_idx n log debug Scheduling collectives overlap compute_node= s available_time= f ms candidates= d current_memory= d bytes compute_node name available_compute_time len possible_collectives memory_tracker current_memory_bytes collective possible_collectives available_compute_time == break info = collective_info collective Skip compute depends collective vice versa collective compute_ancestors compute_node node_ancestors collective continue while in_flight max_in_flight_bytes - in_flight_bytes info size_bytes _wait_is_hidden _get_oldest_wait compute_node _force_oldest_wait max_in_flight_bytes - in_flight_bytes info size_bytes continue Check we can reach collective without scheduling compute other collectives waits path = _find_schedulable_path collective compute_node path None continue log debug Overlapping collective s compute s coll_domination= d current_depth= d collective name compute_node name compute_index_domination collective current_compute_index Schedule path collective _schedule_path_to_collective path compute_node _handle_collective_start collective Update exposed time newly scheduled collective after scheduling which will account latency reduction bucketing overlap_amount = min available_compute_time info exposed_time_ms info exposed_time_ms -= overlap_amount info exposed_time_ms == info hiding_node = compute_node available_compute_time -= overlap_amount _find_schedulable_path target fx Node curr_compute_node fx Node &#124; None - OrderedSet fx Node &#124; None Find path target collecting unscheduled dependencies TODO - following path faster than doing set difference here unscheduled_ancestors = node_ancestors target - scheduled only schedule non distributed non compute nodes node unscheduled_ancestors is_compute_node node None node unscheduled_collectives None we schedule wait tensor whose start collective hidden current compute node we scheduling then we effectively exposing similarly dont schedule wait collective could otherwise hidden thus forcing exposed however already hidden cannot possible hidden s fine schedule is_wait_tensor node info = collective_info wait_to_start node info hiding_node info hiding_node = curr_compute_node continue node potentially_hidden_waits continue None unscheduled_ancestors should_assume_bucketed node fx Node - bool Check there s in-flight collective can bucketed given node If so assume they will bucket This optimistic heuristic account latency reduction bucketing The two nodes may get bucketed torch _inductor config test_configs assume_bucketing_reduces_latency False key = bucket_key node mode= custom_ops_multidtype key None False in_flight_coll in_flight keys bucket_key in_flight_coll mode= custom_ops_multidtype == key True False _get_oldest_wait - fx Node oldest_start = next iter in_flight collective_info oldest_start wait_node _wait_is_hidden wait_node fx Node compute_node fx Node &#124; None = None - bool assert is_wait_tensor wait_node info = collective_info wait_to_start wait_node info is_exposed info hiding_node = compute_node _schedule_path_to_collective path OrderedSet fx Node curr_compute_node fx Node - None Schedule all nodes needed reach collective assert all n scheduled n path node sorted path key=lambda n node_idx n assert is_compute_node node node unscheduled_collectives is_wait_tensor node When we schedule wait tensors we also force realization all collectives enqueued prior their corresponding collective It s possible scheduling one wait tensor here has forced another path If so skip scheduling node scheduled continue info = collective_info wait_to_start node assert info hiding_node = curr_compute_node _handle_wait node continue _schedule node reorder_graph - None output_node = graph output_node node scheduled node op == placeholder continue output_node prepend node graph lint _reorder_graph - None Reorder graph based schedule exposed = c c collective_info values c exposed_time_ms == c estimated_time_ms potentially_hidden_collectives = compute_potential_hidden_collectives limit_coll_per_compute=True bad_exposed = c c exposed c start_node potentially_hidden_collectives counters inductor overlap_scheduling_exposed += len exposed counters inductor overlap_scheduling_bad_exposed += len bad_exposed counters inductor overlap_scheduling_potentially_hidden += len potentially_hidden_collectives counters inductor overlap_original_mem = original_peak_memory counters inductor rescheduled_mem = memory_tracker peak_memory log info Overlap scheduling results exposed= d bad_exposed= d potentially_hidden= d original_peak_memory= d bytes rescheduled_peak_memory= d bytes len exposed len bad_exposed len potentially_hidden_collectives original_peak_memory memory_tracker peak_memory reorder_graph _bucket_collectives - None torch _inductor fx_passes overlap_preserving_bucketer OverlapPreservingBucketer bucketer = OverlapPreservingBucketer graph=self graph collective_info=self collective_info node_ancestors=self node_ancestors scheduled=self scheduled max_bucket_memory_gb= Could make configurable max_coll_distance=self max_node_distance insert_overlap_deps=self insert_overlap_deps bucketer bucket_collectives compute_potential_hidden_nodes nodes_to_check Iterable fx Node limit_coll_per_compute bool = False - dict fx Node fx Node Returns dict containing mapping nodes which could potentially hidden their hiding node used_compute_nodes OrderedSet fx Node = OrderedSet could_be_hidden start fx Node - fx Node &#124; None compute_node compute_nodes limit_coll_per_compute compute_node used_compute_nodes continue start node_ancestors compute_node compute_node node_ancestors start limit_coll_per_compute used_compute_nodes add compute_node compute_node None TODO We could potentially limit compute nodes per overlap time today optimistic just serves avoid deferring collectives waits have no possible overlap well analysis how successfully we hid compute potentially_hidden = node nodes_to_check mm = could_be_hidden node potentially_hidden node = mm potentially_hidden compute_potential_hidden_collectives limit_coll_per_compute bool = False - dict fx Node fx Node Compute which collective operations could hidden compute compute_potential_hidden_nodes collective_info keys limit_coll_per_compute compute_potential_hidden_waits limit_coll_per_compute bool = False - dict fx Node fx Node Compute which wait operations could hidden compte wait_nodes = info wait_node info collective_info values compute_potential_hidden_nodes wait_nodes limit_coll_per_compute schedule_overlap_bucketing gm torch fx GraphModule max_in_flight_gb float = max_compute_pre_fetch int = collective_bucketing bool = False insert_overlap_deps bool = False compute_overlap_multipler float = max_coll_distance int = custom_runtime_estimation Callable fx Node float &#124; None &#124; None = None - torch fx GraphModule Schedule nodes maximize compute-collective overlap Args gm Input graph module optimize max_in_flight_gb Maximum GB concurrent collective data Too much flight memory can cause memory fragmentation within CUDA Caching Allocator max_compute_pre_fetch Maximum compute node prefetch distance collective_bucketing Enable overlap-preserving collective bucketing insert_overlap_deps Insert overlap dependencies using control deps operator This should only used compiling inductor subsequent passes before removing ops prior execution compute_overlap_multipler Scale factor compute time used hide collectives This can used address over under aggressive overlapping max_coll_distance Maximum node distance overlap bucketing Mostly intended reduce compile time custom_runtime_estimation Custom runtime estimation function estimates runtime ms fx node If None uses default estimations This currently limited collectives compute nodes OverlapScheduler gm compute_overlap_multipler=compute_overlap_multipler max_in_flight_gb=max_in_flight_gb max_coll_distance=max_coll_distance max_compute_pre_fetch=max_compute_pre_fetch custom_runtime_estimation=custom_runtime_estimation collective_bucketing=collective_bucketing insert_overlap_deps=insert_overlap_deps run