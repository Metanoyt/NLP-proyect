typing Optional TypeAlias torch torch Tensor torch autograd grad_mode no_grad _get_foreach_kernels_supported_devices - list str r Return device type list supports foreach kernels cuda xpu mtia torch _C _get_privateuse _backend_name _get_fused_kernels_supported_devices - list str r Return device type list supports fused kernels optimizer mps cuda xpu hpu cpu mtia torch _C _get_privateuse _backend_name TensorListList TypeAlias = list list Optional Tensor Indices TypeAlias = list int _foreach_supported_types = torch Tensor This util function splits tensors into groups device dtype which useful before sending tensors off foreach implementation which requires tensors one device dtype If tensorlistlist contains more than one tensorlist following assumptions made BUT NOT verified - tensorlists CAN None - all tensors first specified list cannot None - given index i all specified tensorlist i s match dtype device with_indices bool optional whether track previous indices last list per dictionary entry It comes handy there Nones literals tensorlists getting scattered out Whereas mutating tensor resulting split-up tensorlists WILL propagate changes back original input tensorlists changing up Nones literals WILL NOT propagate manual propagation may necessary Check out torch optim sgd py example no_grad _group_tensors_by_device_and_dtype tensorlistlist TensorListList with_indices bool = False - dict tuple torch device torch dtype tuple TensorListList Indices torch _C _group_tensors_by_device_and_dtype tensorlistlist with_indices _device_has_foreach_support device torch device - bool device type _get_foreach_kernels_supported_devices + cpu torch jit is_scripting _has_foreach_support tensors list Tensor device torch device - bool _device_has_foreach_support device all t None type t _foreach_supported_types t tensors