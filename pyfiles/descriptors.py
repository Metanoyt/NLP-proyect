AOTAutograd descriptors path-like data structure similar pytree paths sources describe semantic meaning input output FX graphs Although you may know input output meaning top level original function you traced because we have many graph capture wrappers change calling convention can difficult tell how these correspond actual FX graph you get back say nothing about extra arguments outputs tangents gradients etc Descriptors describe meaning arguments Examples -------- Before we talk about precise semantics s helpful look some examples get some intuition meaning descriptors Here some input descriptors you might find joint FX graph PlainAOTInput idx= - first input original callable ParamAOTInput target= mod weight - parameter FQN mod weight TangentAOTInput output=PlainAOTOutput idx= - input tangent corresponding gradients second output forward graph ViewBaseAOTInput base_of=PlainAOTInput idx= - turned out first input actually differentiable view tensor which aliased another input tensor We replaced input single input base all these inputs replacing original inputs one which mentioned base_of We would generate GradAOTOutput input original PlainAOTInputs If you have joint graph where view base like undesirable you can eliminate cloning views outside compiled region assuming you aren t mutating tensor SubclassGetAttrAOTInput base=AOTInput idx= attr= inner - tensor corresponds inner tensor tensor subclass first index In general joint graphs AOTAutograd never take tensor subclasses inputs they always unpacked into their constituent plain tensor pieces use descriptors identify parts tensor related Note can nested you have nested tensor subclasses Here some output descriptors you might find Joint FX graph PlainAOTOutput idx= - first output original forward function GradAOTOutput grad_of=PlainAOTInput idx= - computed gradient second input graph output backward graph InputMutationAOTOutput mutated_input=PlainAOTInput idx= - when first input mutated new value copied into first input graph Sometimes these outputs can elided ` ` copy_ ` ` done directly graph controlled keep_input_mutations input mutation must differentiated through we always generate output like IntermediateBaseAOTOutput base_of=PlainAOTOutput idx= - we multiple outputs which alias each other we instead replace them single output tensor representing base all aliases This output indicates base one those original outputs If undesirable joint graph clone all outputs before returning graph SubclassGetAttrAOTOutput base=PlainAOTOutput idx= idx= inner - tensor correspondings inner tensor first original output which tensor subclass This other subclass components output will get repacked into tensor subclass High level semantics -------------------- OK let s formally define descriptor Intuitively suppose we have wrapped_graph args ret = graph in_transform args out_transform ret Then descriptor input i graph describes function fin_i such fin_i args == in_transform args i descriptor output j graph describes function fout_j such fout_j out_transform ret == ret j AKA input descriptors tell you how get outer inputs inner inputs while output descriptors tell you how get outer outputs inner outputs inverse data flow We haven t said anything about what these transformations actually do There three major transformations AOTAutograd does performed order View mutation handling Autograd Subclasses So intuitively descriptors built like PlainAOTInput PlainAOTOutput We start off descriptors describing exact inputs outputs original flattened user function This user function assumed already flattened you would chain pytree KeyPaths further describe where pytree each input output lived you needed deal unflattened functions can done userland top descriptors so main descriptors mechanism doesn t handle SyntheticBaseAOTInput ViewBaseAOTInput MetadataMutationAOTOutput InputMutationAOTOutput IntermediateBaseAOTOutput We deal mutations aliasing removing duplicate PlainAOTInputs introduce some new artificial inputs outputs These inputs do have straightforward correspondence original user inputs you implementing pass doesn t care about exact semantics inputs you should handle all these uniformly same way regular inputs TangentAOTInput GradAOTOutput We deal autograd introducing tangent input every differentiable AOTOutput including new ones introduced above gradient output every differentiable AOTInput also including new ones introduced above The arguments these AOTInput AOTOutput can ONLY ones we already have above steps - As AOTAutograd does currently support double backwards you never have tangents grads vice versa future we could SubclassGetAttrAOTInput SubclassGetAttrAOTOutput et al We deal subclasses introducing flattened inputs outputs including potentially symbolic sizes strides every AOTInput AOTOutput subclass As above arguments these AOTInput AOTOutput can ONLY ones we have above steps - Recursive subclasses supported so these descriptors can nest each other so descriptors step fair game well ForwardTokenAOTInput ForwardTokenAOTOutput BackwardTokenAOTInput BackwardTokenAOTOutput Some extra token inputs outputs get added these synthetic just here prevent DCE reordering The important thing about pipeline descriptors can ONLY created top-to-bottom So example you can have SubclassGetAttrAOTInput TangentAOTInput PlainAOTOutput OK As you can see PlainAOTOutput - TangentAOTInput - SubclassGetAttrAOTInput consistent pipeline ordering you can NEVER have TangentAOTInput SubclassGetAttrAOTOutput PlainAOTOutput BAD This inconsistent we always do autograd BEFORE we process subclasses Similarly example illegal GradAOTOutput SubclassGetAttrAOTInput PlainAOTInput BAD It illegal because subclasses handled after create joint during wrapper construction Instead you would have SubclassGetAttrAOTOutput GradAOTOutput PlainAOTInput OK This intuitively captures fact we always autograd directly subclass rather than after desugaring subclass into its inner tensors Descriptor index ---------------- Here list all AOTInput AOTOutput organized how likely you need handle them AOTInput Important PlainAOTInput primals ParamAOTInput TangentAOTInput SubclassGetAttrAOTInput et al you use subclasses View related can eliminated cloning inputs graph you don t eliminate them make sure handle pairing them GradAOTOutput ViewBaseAOTInput SyntheticBaseAOTInput Non-tensor mostly just ignore them DummyAOTInput PhiloxForwardSeedAOTInput PhiloxForwardBaseOffsetAOTInput PhiloxBackwardSeedAOTInput PhiloxBackwardBaseOffsetAOTInput ForwardTokenAOTInput BackwardTokenAOTInput AOTOutput Important PlainAOTOutput GradAOTOutput SubclassGetAttrAOTOutput et al you use subclasses More obscure eliminated make sure you handle pairing them TangentAOTInput InputMutationAOTOutput can eliminated mutations non-differentiable IntermediateBaseAOTOutput can eliminated cloning outputs graph MetadataMutationAOTOutput uhh just don t mutate metadata Non-tensor mostly just ignore them PhiloxUpdatedForwardOffsetAOTOutput PhiloxUpdatedBackwardOffsetAOTOutput ForwardTokenAOTOutput BackwardTokenAOTOutput DummyAOTOutput For convenience we also have DifferentiableAOTInput DifferentiableAOTOutput help you classify which inputs outputs can wrapped GradAOTOutput TangentAOTInput respectively which essentially all tensor AOTInput AOTOutput excluding subclass descriptors Implementation details ---------------------- The stylized view above good understanding how interpret descriptors way descriptors generated code bit more complicated Specifically AOTAutograd structured series wrappers original user function which composed together form final function trace As result AOTAutograd ends up first building full AOTInputs function traced builds wrappers modifies flat arguments compatible new input signature wrapper then reverse builds up AOTOutput tracing There one major exception general idea build AOTInput first then build AOTOutput second when we create TangentAOTInput we need reference AOTOutputs which output we tangents which we generally haven t created yet There s two ways we deal - After precompile steps dedup synthetic base handling we do initial pass collect forward metadata produces initial set PlainAOTOutputs which we use create tangent inputs - We also sometimes just violate causality predict AOTOutput will created particular way some later point time when we build AOTInput As July here exhaustive description how inputs outputs traverse wrappers AOTAutograd what descriptors can introduced these phases Build wrappers FLOWS DOWN Run trace FLOWS UP ------------------------------------------------------------------------------------------------- Begin PlainAOTInput n ParamAOTInput Precompile dedupe remove dupes nothing Precompile synthetic base SyntheticBaseAOTInput MetadataMutationAOTOutput ViewBaseAOTInput Forward metadata trace PlainAOTOutput n MetadataMutationAOTOutput Prepare autograd nothing InputMutationAOTOutput IntermediateBaseAOTOutput Create joint TangentAOTInput GradAOTOutput w InputMutationAOTOutput w IntermediateBaseAOTOutput Precompile subclass SubclassGetAttrAOTInput et al SubclassGetAttrAOTOutput et al Effect tokens ForwardTokenAOTInput ForwardTokenAOTOutput BackwardTokenAOTInput BackwardTokenAOTOutput End n PlainAOTOutput It can helpful separately write down input flow output flow ease understanding data flow Input desc propagation happens we build wrappers IN Begin original calling convention PlainAOTInput ParamAOTInput IN Precompile dedupe removes duplicate AOTInputs IN Precompile synthetic base SyntheticBaseAOTInput ViewBaseAOTInput Forward metadata trace mini output desc propagation OUT Original output convention PlainAOTOutput OUT Precompile synthetic base MetadataMutationAOTOutput IN Prepare autograd nothing IN Create joint TangentAOTInput potentially w IntermediateBaseAOTOutput InputMutationAOTOutput IN Precompile subclass SubclassGetAttrAOTInput et al IN Effect tokens ForwardTokenAOTInput BackwardTokenAOTInput Note BackwardTokenAOTInput technically generated wrapper actually done token_discovery which implicitly adds extra arguments FX trace on-the-fly Trigger trace modified inputs wrapper Output desc propagation happens we unwind user function call trace OUT Begin original calling convention PlainAOTOutput OUT Effect tokens ForwardTokenAOTOutput BackwardTokenAOTOutput OUT Precompile subclass SubclassGetAttrAOTOutput et al OUT Create joint GradAOTOutput OUT Prepare autograd InputMutationAOTOutput IntermediateBaseAOTOutput OUT Precompile synthetic base MetadataMutationAOTOutput OUT Precompile dedupe nothing dataclasses TODO is_ predicates little suspicious because they re used anything they always report False even when parameter got swizzled into view base deduped non-parameter It pretty difficult exercise these cases s clear you will write code works correctly those cases dataclasses dataclass frozen=True AOTInput Describes where input AOTAutograd produced FX graph comes expr - str raise NotImplementedError Subclasses must implement expr is_param - bool True input parameter derived parameter e g subclass attr False is_buffer - bool True input buffer derived buffer e g subclass attr False is_tangent - bool True input tangent derived tangent e g subclass attr False Note Currently our typing discipline differentiable versus very good so feel free rely runtime tests instead dataclasses dataclass frozen=True DifferentiableAOTInput AOTInput A subclass classifies AOTInput can wrapped GradAOTOutput dataclasses dataclass frozen=True AOTOutput Describes where output AOTAutograd produced FX graph will eventually bundled into final output expr - str raise NotImplementedError Subclasses must implement expr is_grad - bool True output grad derived grad e g subclass attr False dataclasses dataclass frozen=True DifferentiableAOTOutput AOTOutput A subclass classifies AOTOutput can wrapped TangentAOTInput ------------ AOTInput ------------ dataclasses dataclass frozen=True ParamAOTInput DifferentiableAOTInput The input parameter whose FQN target target str expr - str f get_parameter target r is_param - bool True is_buffer - bool False dataclasses dataclass frozen=True BufferAOTInput DifferentiableAOTInput The input buffer whose FQN target target str expr - str f get_buffer target r is_param - bool False is_buffer - bool True dataclasses dataclass frozen=True DummyAOTInput AOTInput In some circumstances we want call into function expects AOTInput we don t actually care about logic most typically because some code being used both compile-time run-time AOTInput processing needed situation Pass dummy situation better just have version function doesn t have all idx int expr - str f __dummy idx dataclasses dataclass frozen=True PlainAOTInput DifferentiableAOTInput The input plain input corresponding particular positional index Note AOTInput always relative function flat calling convention e g accepted ` aot_module_simplified ` There some AOTAutograd APIs flatten pytrees we don t record PyTree key paths flattening we could should idx int expr - str f args idx dataclasses dataclass frozen=True SubclassGetAttrAOTInput AOTInput Subclass inputs get unpacked into their constituent pieces before going into FX graph This tells you which particular attribute subclass particular input corresponds base originally subclass argument base AOTInput attr str expr - str f base expr attr is_param - bool base is_param is_buffer - bool base is_buffer is_tangent - bool base is_tangent dataclasses dataclass frozen=True SubclassSizeAOTInput AOTInput Which subclass particular outer size SymInt input dim idx came base AOTInput idx int expr - str f base expr size idx dataclasses dataclass frozen=True SubclassStrideAOTInput AOTInput Which subclass particular outer stride SymInt input dim idx came base AOTInput idx int expr - str f base expr stride idx dataclasses dataclass frozen=True ViewBaseAOTInput DifferentiableAOTInput When multiple differentiable inputs views same input AOTAutograd will replace all these views single input representing base If undesirable you can clone views example inputs before passing them into AOTAutograd TODO In principle we could report ALL inputs who base base_of AOTInput expr - str f base_of expr _base dataclasses dataclass frozen=True SyntheticBaseAOTInput DifferentiableAOTInput This similar ViewBaseAOTInput happens when none views differentiable so we weren t able get our hands true original view constructed synthetic one instead sake autograd base_of AOTInput expr - str f __make_synthetic_base base_of expr dataclasses dataclass frozen=True PhiloxForwardSeedAOTInput AOTInput The seed functionalized Philox RNG calls specifically forward graph expr - str __philox_forward_seed dataclasses dataclass frozen=True PhiloxForwardBaseOffsetAOTInput AOTInput The offset functionalized Philox RNG calls specifically forward graph expr - str __philox_forward_base_offset dataclasses dataclass frozen=True PhiloxBackwardSeedAOTInput AOTInput The seed functionalized Philox RNG calls specifically backward graph expr - str __philox_backward_seed dataclasses dataclass frozen=True PhiloxBackwardBaseOffsetAOTInput AOTInput The offset functionalized Philox RNG calls specifically backward graph expr - str __philox_backward_base_offset dataclasses dataclass frozen=True ForwardTokenAOTInput AOTInput The world token which threaded through side-effectful operations idx int expr - str f __forward_token idx dataclasses dataclass frozen=True BackwardTokenAOTInput AOTInput The world token which threaded through side-effectful operations backwards idx int expr - str f __backward_token idx Technically output here redundant tangents always correspond outputs NB marked differentiable would differentiable we support double backwards we never generate today because we don t support double backwards dataclasses dataclass frozen=True TangentAOTInput DifferentiableAOTInput An input joint graph representing tangent output output DifferentiableAOTOutput __post_init__ - None assert isinstance output DifferentiableAOTOutput expr - str f __output_tangent output expr is_tangent - bool True ------------ AOTOutput ------------ dataclasses dataclass frozen=True PlainAOTOutput DifferentiableAOTOutput A plain tensor output position idx output tuple idx int expr - str f output idx dataclasses dataclass frozen=True InputMutationAOTOutput DifferentiableAOTOutput The mutated value input tensor returned so we can appropriately propagate autograd mutated_input AOTInput expr - str f __input_mutation mutated_input expr dataclasses dataclass frozen=True IntermediateBaseAOTOutput DifferentiableAOTOutput An intermediate base multiple outputs which alias each other We only report ONE outputs contributed base base_of AOTOutput expr - str f __intermediate_base base_of expr TODO s little dodgy differentiable lol we do generate these BEFORE autograd handled dataclasses dataclass frozen=True MetadataMutationAOTOutput DifferentiableAOTOutput idx int expr - str f __aliased_arg_with_metadata_mutation idx NB marked differentiable would differentiable we support double backwards we never generate today because we don t support double backwards dataclasses dataclass frozen=True GradAOTOutput DifferentiableAOTOutput An output representing computed gradient differentiable input joint graph grad_of DifferentiableAOTInput __post_init__ - None assert isinstance grad_of DifferentiableAOTInput expr - str f __grad grad_of expr is_grad - bool True dataclasses dataclass frozen=True PhiloxUpdatedForwardOffsetAOTOutput AOTOutput The final offset functionalized RNG calls forward only expr - str __philox_updated_forward_offset dataclasses dataclass frozen=True PhiloxUpdatedBackwardOffsetAOTOutput AOTOutput The final offset functionalized RNG calls backward only expr - str __philox_updated_backward_offset dataclasses dataclass frozen=True ForwardTokenAOTOutput AOTOutput The world token output side-effectful calls returned so we cannot DCE forward only idx int expr - str f __forward_token idx dataclasses dataclass frozen=True BackwardTokenAOTOutput AOTOutput The world token output side-effectful calls returned so we cannot DCE backward only idx int expr - str f __backward_token idx These seemingly symmetric their AOTInput counterparts The way think about subclass could input output they get exploded into plain tensors way out So we need descriptors both dataclasses dataclass frozen=True SubclassGetAttrAOTOutput AOTOutput This output will bundled into subclass location base AOTOutput attr str expr - str f base expr attr is_grad - bool base is_grad dataclasses dataclass frozen=True SubclassSizeAOTOutput AOTOutput This output size will bundled into subclass location base AOTOutput idx int expr - str f base expr size idx dataclasses dataclass frozen=True SubclassStrideAOTOutput AOTOutput This output stride will bundled into subclass location base AOTOutput idx int expr - str f base expr stride idx dataclasses dataclass frozen=True DummyAOTOutput AOTOutput For cases when you don t actually care about descriptor propagation do use under normal circumstances idx int expr - str f __dummy idx dataclasses dataclass frozen=True SavedForBackwardsAOTOutput AOTOutput idx int expr - str f __saved_for_backwards_ idx