mypy allow-untyped-defs Copyright c Meta Platforms Inc affiliates All rights reserved This source code licensed under BSD-style license found LICENSE file root directory source tree __future__ annotations torch torch utils _pytree pytree torch _ops HigherOrderOperator torch _subclasses fake_tensor FakeTensor FakeTensorMode torch fx experimental proxy_tensor disable_proxy_modes_tracing ProxyTorchDispatchMode track_tensor_tree AOTI_LOWERED_MODULE = AOTInductorEPModule AOTInductorRunnerWrapper AOTICallDelegate HigherOrderOperator aoti_call_delegate HOP calling AOTInductor lowered submodule ExportedProgram It has following signature aoti_call_delegate lowered_module Union AOTInductorEPModule AOTInductorRunnerWrapper original_gm fx GraphModule weight_args List Tensor input_args List Tensor - outputs List Tensor where - lowered_module AOTInductor lowered submodule backed compiled so file supporting real tensor inputs - original_gm stateless version original GraphModule before lowering allowing FakeTensor propagation - weight_args list weights original GraphModule including parameters buffers - input_args list flatten inputs __init__ - None super __init__ aoti_call_delegate __call__ lowered_module AOTI_LOWERED_MODULE type ignore valid-type original_gm torch fx GraphModule weight_args list torch Tensor input_args list torch Tensor - list torch Tensor super __call__ lowered_module original_gm weight_args input_args aoti_call_delegate = AOTICallDelegate aoti_call_delegate fallthrough torch _C DispatchKey PythonDispatcher aoti_call_delegate fallthrough torch _C DispatchKey PythonTLSSnapshot aoti_call_delegate fallthrough torch _C DispatchKey ADInplaceOrView aoti_call_delegate fallthrough torch _C DispatchKey AutocastCPU aoti_call_delegate py_impl torch _C DispatchKey CompositeExplicitAutograd call_delegate_cpu lowered_module AOTI_LOWERED_MODULE type ignore valid-type original_gm torch fx GraphModule weight_args list torch Tensor input_args list torch Tensor - list torch Tensor FX creates immutable_dict list concept Get rid map_types dict type type = torch fx immutable_collections immutable_dict dict torch fx immutable_collections immutable_list list new_args = pytree tree_map_only tuple map_types keys lambda map_types type weight_args + input_args lambda isinstance tuple map_types keys has_fake_args = any isinstance arg FakeTensor arg new_args has_fake_args use stateless original_gm tracing fake tensors fake_out = original_gm new_args fake_out use AOTI Runner real tensors new_input_args = new_args len weight_args type lowered_module __name__ == AOTInductorRunnerWrapper lowered_module new_input_args type ignore misc type lowered_module __name__ == AOTInductorEPModule lowered_module new_input_args type ignore misc raise RuntimeError f Unexpected lowered_module type type lowered_module trace_aoti_call_delegate proxy_mode func_overload lowered_module original_gm weight_args input_args proxy_mode tracer root register_module lowered_module lowered_module proxy_mode tracer root register_module original_gm original_gm node_args = lowered_module original_gm weight_args input_args proxy_args = pytree tree_map proxy_mode tracer unwrap_proxy node_args out_proxy = proxy_mode tracer create_proxy call_function func_overload proxy_args name= aoti_call_delegate disable_proxy_modes_tracing out = call_delegate_cpu lowered_module original_gm weight_args input_args track_tensor_tree out out_proxy constant=None tracer=proxy_mode tracer aoti_call_delegate py_impl ProxyTorchDispatchMode call_delegate_proxy_torch_dispatch_mode mode ProxyTorchDispatchMode lowered_module AOTI_LOWERED_MODULE type ignore valid-type original_gm torch fx GraphModule weight_args list torch Tensor input_args list torch Tensor res = trace_aoti_call_delegate mode aoti_call_delegate lowered_module original_gm weight_args input_args res aoti_call_delegate py_impl FakeTensorMode call_delegate_fake_tensor_mode mode FakeTensorMode lowered_module AOTI_LOWERED_MODULE type ignore valid-type original_gm torch fx GraphModule weight_args list torch Tensor input_args list torch Tensor - list torch Tensor mode call_delegate_cpu lowered_module original_gm weight_args input_args aoti_call_delegate py_functionalize_impl call_delegate_functionalize ctx lowered_module AOTI_LOWERED_MODULE type ignore valid-type original_gm torch fx GraphModule weight_args list torch Tensor input_args list torch Tensor unwrapped_weight_args = tuple ctx unwrap_tensors weight_arg weight_arg weight_args unwrapped_input_args = tuple ctx unwrap_tensors input_arg input_arg input_args ctx redispatch_to_next res = aoti_call_delegate lowered_module original_gm unwrapped_weight_args type ignore arg-type unwrapped_input_args type ignore arg-type ctx wrap_tensors res