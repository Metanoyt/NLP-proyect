mypy allow-untyped-decorators mypy allow-untyped-defs functools types typing warnings collections abc Callable typing cast Optional TypeAlias TypeVar Union typing_extensions deprecated ParamSpec torch torch Tensor torch utils _foreach_utils _device_has_foreach_support _group_tensors_by_device_and_dtype _has_foreach_support __all__ list str = clip_grad_norm clip_grad_norm_ clip_grad_value_ _tensor_or_tensors TypeAlias = Union noqa PYI torch Tensor typing Iterable torch Tensor noqa UP - needed until XLA s patch updated _P = ParamSpec _P _R = TypeVar _R _no_grad func Callable _P _R - Callable _P _R This wrapper needed avoid circular when using torch no_grad exposed functions clip_grad_norm_ clip_grad_value_ themselves _no_grad_wrapper args kwargs torch no_grad pyrefly ignore invalid-param-spec func args kwargs functools update_wrapper _no_grad_wrapper func pyrefly ignore bad-return _no_grad_wrapper _no_grad _get_total_norm tensors _tensor_or_tensors norm_type float = error_if_nonfinite bool = False foreach Optional bool = None - torch Tensor r Compute norm iterable tensors The norm computed over norms individual tensors norms individual tensors concatenated into single vector Args tensors Iterable Tensor Tensor iterable Tensors single Tensor will normalized norm_type float type used p-norm Can ` ` inf ` ` infinity norm error_if_nonfinite bool True error thrown total norm attr ` tensors ` ` ` nan ` ` ` ` inf ` ` ` ` -inf ` ` Default ` ` False ` ` foreach bool use faster foreach-based implementation If ` ` None ` ` use foreach implementation CUDA CPU native tensors silently fall back slow implementation other device types Default ` ` None ` ` Returns Total norm tensors viewed single vector isinstance tensors torch Tensor tensors = tensors tensors = list tensors norm_type = float norm_type len tensors == torch tensor first_device = tensors device grouped_tensors dict tuple torch device torch dtype tuple list list Tensor list int = _group_tensors_by_device_and_dtype tensors type ignore list-item type ignore assignment norms list Tensor = device _ device_tensors _ grouped_tensors items foreach None _has_foreach_support device_tensors device foreach _device_has_foreach_support device norms extend torch _foreach_norm device_tensors norm_type foreach raise RuntimeError f foreach=True passed can t use foreach API device type tensors norms extend torch linalg vector_norm g norm_type g device_tensors total_norm = torch linalg vector_norm torch stack norm first_device norm norms norm_type error_if_nonfinite torch logical_or total_norm isnan total_norm isinf raise RuntimeError f The total norm order norm_type gradients ` parameters ` non-finite so cannot clipped To disable error scale gradients non-finite norm anyway set ` error_if_nonfinite=False ` total_norm _no_grad _clip_grads_with_norm_ parameters _tensor_or_tensors max_norm float total_norm torch Tensor foreach Optional bool = None - None r Scale gradients iterable parameters given pre-calculated total norm desired max norm The gradients will scaled following calculation math grad = grad \min \frac max\_norm total\_norm + e- Gradients modified in-place Note The scale coefficient clamped maximum prevent gradient amplification This ensures gradients only scaled down when total norm exceeds max_norm This function equivalent func ` torch nn utils clip_grad_norm_ ` pre-calculated total norm Args parameters Iterable Tensor Tensor iterable Tensors single Tensor will have gradients normalized max_norm float max norm gradients total_norm Tensor total norm gradients use clipping foreach bool use faster foreach-based implementation If ` ` None ` ` use foreach implementation CUDA CPU native tensors silently fall back slow implementation other device types Default ` ` None ` ` Returns None isinstance parameters torch Tensor parameters = parameters grads = p grad p parameters p grad None max_norm = float max_norm len grads == grouped_grads dict tuple torch device torch dtype tuple list list Tensor list int = _group_tensors_by_device_and_dtype grads type ignore assignment clip_coef = max_norm total_norm + e- Note multiplying clamped coef redundant when coef clamped doing so avoids ` clip_coef ` conditional which can require CPU = device synchronization when gradients do reside CPU memory clip_coef_clamped = torch clamp clip_coef max= device _ device_grads _ grouped_grads items foreach None _has_foreach_support device_grads device foreach _device_has_foreach_support device torch _foreach_mul_ device_grads clip_coef_clamped device foreach raise RuntimeError f foreach=True passed can t use foreach API device type tensors clip_coef_clamped_device = clip_coef_clamped device g device_grads g mul_ clip_coef_clamped_device _no_grad clip_grad_norm_ parameters _tensor_or_tensors max_norm float norm_type float = error_if_nonfinite bool = False foreach Optional bool = None - torch Tensor r Clip gradient norm iterable parameters The norm computed over norms individual gradients all parameters norms individual gradients concatenated into single vector Gradients modified in-place This function equivalent func ` torch nn utils get_total_norm ` followed func ` torch nn utils clip_grads_with_norm_ ` ` ` total_norm ` ` returned ` ` get_total_norm ` ` Args parameters Iterable Tensor Tensor iterable Tensors single Tensor will have gradients normalized max_norm float max norm gradients norm_type float optional type used p-norm Can ` ` inf ` ` infinity norm Default error_if_nonfinite bool optional True error thrown total norm gradients attr ` parameters ` ` ` nan ` ` ` ` inf ` ` ` ` -inf ` ` Default False foreach bool optional use faster foreach-based implementation If ` ` None ` ` use foreach implementation CUDA CPU native tensors silently fall back slow implementation other device types Default ` ` None ` ` Returns Total norm parameter gradients viewed single vector isinstance parameters torch Tensor parameters = parameters is_generator = isinstance parameters types GeneratorType prevent generators being exhausted parameters = list parameters is_generator len parameters == warnings warn ` parameters ` empty generator no gradient clipping will occur stacklevel= grads = p grad p parameters p grad None total_norm = _get_total_norm grads norm_type error_if_nonfinite foreach _clip_grads_with_norm_ parameters max_norm total_norm foreach total_norm deprecated ` torch nn utils clip_grad_norm ` now deprecated favor ` torch nn utils clip_grad_norm_ ` category=FutureWarning clip_grad_norm parameters _tensor_or_tensors max_norm float norm_type float = error_if_nonfinite bool = False foreach Optional bool = None - torch Tensor r Clip gradient norm iterable parameters warning This method now deprecated favor func ` torch nn utils clip_grad_norm_ ` clip_grad_norm_ parameters max_norm norm_type error_if_nonfinite foreach _no_grad clip_grad_value_ parameters _tensor_or_tensors clip_value float foreach Optional bool = None - None r Clip gradients iterable parameters specified value Gradients modified in-place Args parameters Iterable Tensor Tensor iterable Tensors single Tensor will have gradients normalized clip_value float maximum allowed value gradients The gradients clipped range math ` \left \text -clip\_value \text clip\_value \right ` foreach bool optional use faster foreach-based implementation If ` ` None ` ` use foreach implementation CUDA CPU native tensors silently fall back slow implementation other device types Default ` ` None ` ` isinstance parameters torch Tensor parameters = parameters clip_value = float clip_value grads = p grad p parameters p grad None pyrefly ignore bad-argument-type grouped_grads = _group_tensors_by_device_and_dtype grads device _ grads _ grouped_grads items foreach None _has_foreach_support cast list Tensor grads device=device foreach _device_has_foreach_support device torch _foreach_clamp_min_ cast list Tensor grads -clip_value torch _foreach_clamp_max_ cast list Tensor grads clip_value foreach raise RuntimeError f foreach=True passed can t use foreach API device type tensors grad grads cast Tensor grad clamp_ min=-clip_value max=clip_value