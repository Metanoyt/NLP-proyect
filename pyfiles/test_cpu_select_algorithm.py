Owner s oncall cpu inductor contextlib functools sys unittest typing Optional unittest mock patch torch torch _dynamo config torch _dynamo config dynamo_config torch _inductor config inductor_config torch _inductor cpu_vec_isa torch _inductor select_algorithm select_algorithm torch _dynamo utils counters torch _inductor test_operators torch _inductor cpu_vec_isa VecAMX torch _inductor test_case run_tests TestCase torch testing _internal common_device_type dtypes instantiate_device_type_tests torch testing _internal common_quantization _generate_qdq_quantized_model torch testing _internal common_quantized _calculate_dynamic_per_channel_qparams torch testing _internal common_utils IS_MACOS IS_WINDOWS parametrize skipIfWindows TEST_MKL try try test_cpu_repro test_torchinductor except ImportError test_cpu_repro manual=fbcode caffe test inductor test_cpu_repro-library test_torchinductor manual=fbcode caffe test inductor test_inductor-library except unittest SkipTest __name__ == __main__ sys exit raise check_model = test_torchinductor check_model set_num_threads = test_cpu_repro set_num_threads run_and_get_cpp_code = test_torchinductor run_and_get_cpp_code aten = torch ops aten patches fn skip_cache choices name key benchmark hint_override=None benchmark None timings = benchmark choices choice timing timings items isinstance choice select_algorithm ExternKernelCaller we intentionally make ATEN kernel slower cover cases where template kernels always chosen fusions applied correctness checks runtime timings choice = timing timings patcher dynamo_config patch verbose=True dynamo_config patch inline_inbuilt_nn_modules=True inductor_config patch debug=True max_autotune=True epilogue_fusion=True max_autotune_gemm_backends= CPP ATEN patch object select_algorithm VERIFY dict atol= e- rtol= e- patch object select_algorithm AlgorithmSelectorCache lookup skip_cache fn = patcher fn functools wraps fn wrapped args kwargs counters clear torch manual_seed fn args kwargs wrapped contextlib contextmanager verify dtype For bfloat half we have relax tolerance due difference associave orders different kernel implementations atol rtol = e- e- dtype == torch half dtype == torch bfloat atol rtol = e- e- patch object select_algorithm VERIFY dict atol=atol rtol=rtol yield atol rtol _get_epilogue epilogue str other Optional torch Tensor = None epilogue == none lambda x x epilogue == relu torch nn ReLU epilogue == gelu torch nn GELU epilogue == silu torch nn SiLU epilogue == sigmoid torch nn Sigmoid epilogue == tanh torch nn Tanh epilogue == hardswish torch nn Hardswish epilogue == hardsigmoid torch nn Hardsigmoid epilogue == leaky_relu torch nn LeakyReLU epilogue == hardtanh torch nn Hardtanh epilogue == add lambda x x + other epilogue == sub lambda x x - other epilogue == mul lambda x x other epilogue == div lambda x x other BaseTestSelectAlgorithm TestCase _check_amx_counter vec_amx vec_amx assertTrue counters inductor cpp_micro_gemm_amx_counter assertEqual counters inductor cpp_micro_gemm_amx_counter _check_brgemm_counter vec_amx vec_amx torch cpu _is_amx_fp _supported assertTrue counters inductor cpp_micro_brgemm_counter assertEqual counters inductor cpp_micro_brgemm_counter TestSelectAlgorithm BaseTestSelectAlgorithm common = check_model inductor_config patch freezing True patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize batch_size parametrize in_features parametrize out_features parametrize bias True False parametrize input_ d True False dtypes torch float torch bfloat torch half test_linear_static_shapes batch_size in_features out_features bias input_ d dtype M torch nn Module __init__ bias super __init__ linear = torch nn Linear in_features out_features bias forward x linear x counters clear mod = M bias=bias dtype=dtype eval B = batch_size input_ d batch_size v = torch randn B in_features dtype=dtype verify dtype atol rtol common mod v atol=atol rtol=rtol counters inductor decompose_mm counters inductor decompose_addmm This special case where we go directly vectorized codegen assertEqual counters inductor cpp_templated_kernel_counter assertEqual counters inductor cpp_templated_kernel_counter inductor_config patch freezing True patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize in_features parametrize out_features parametrize bias True dtypes torch float test_linear_wgt_multi_users in_features out_features bias dtype M torch nn Module __init__ bias super __init__ embeddings = torch nn Embedding out_features in_features linear = torch nn Linear in_features out_features bias linear weight = embeddings weight forward x x = embeddings x linear x counters clear mod = M bias=bias dtype=dtype eval v = torch LongTensor verify dtype atol rtol common mod v atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter inductor_config patch freezing True patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize bias True False dtypes torch float test_linear_input_transpose bias dtype batch_size = in_features = out_features = M torch nn Module __init__ bias super __init__ linear = torch nn Linear in_features out_features bias torch compile forward x linear x counters clear mod = M bias=bias dtype=dtype eval v = torch randn in_features batch_size dtype=dtype common mod v transpose TODO jgong support transposed input assertEqual counters inductor cpp_templated_kernel_counter inductor_config patch freezing True patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize batch_size parametrize in_features parametrize out_features parametrize bias True False parametrize epilogue relu gelu silu sigmoid tanh hardswish hardsigmoid leaky_relu hardtanh add sub mul div dtypes torch float torch bfloat torch half torch fx experimental _config patch use_duck_shape=False torch _dynamo config patch specialize_float=True test_linear_with_pointwise batch_size in_features out_features bias epilogue dtype M torch nn Module __init__ bias epilogue other super __init__ linear = torch nn Linear in_features out_features bias epilogue = _get_epilogue epilogue other forward x epilogue linear x counters clear v = torch randn batch_size in_features dtype=dtype u = torch randn batch_size out_features dtype=dtype mod = M bias=bias epilogue=epilogue other=u dtype=dtype eval verify dtype atol rtol common mod v atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter dtype == torch bfloat torch ops mkldnn _is_mkldnn_bf _supported dtype == torch float torch ops mkldnn _is_mkldnn_fp _supported dtype == torch float dynamo_config assume_static_by_default epilogue = mul epilogue = div dtype torch float torch bfloat epilogue == add bias Several scenarios where epilogue fusion counted For bfloat epilogue fusion part template fused via scheduler This will also true float when hardware has float instruction And will also true float dynamic mode The exception mul div fusion which supported oneDNN linear For bfloat float when oneDNN linear applied linear w o bias plus epilogue add treated linear w bias assertEqual counters inductor cpp_epilogue_fusion_counter assertEqual counters inductor cpp_epilogue_fusion_counter inductor_config patch freezing True patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize batch_size parametrize in_features parametrize out_features parametrize bias True False parametrize epilogue none relu add sub mul dtypes torch float torch bfloat torch half test_linear_with_transpose batch_size in_features out_features bias epilogue dtype M torch nn Module __init__ bias epilogue other super __init__ epilogue = _get_epilogue epilogue other linear = torch nn Linear in_features out_features bias forward x y epilogue linear x transpose + y counters clear v = torch randn batch_size in_features dtype=dtype u = torch randn out_features batch_size dtype=dtype other = torch randn batch_size out_features dtype=dtype mod = M bias=bias epilogue=epilogue other=other dtype=dtype eval verify dtype atol rtol common mod v u atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter assertEqual counters inductor cpp_epilogue_fusion_counter inductor_config patch freezing True patches torch no_grad parametrize batch_size parametrize in_features parametrize image_size parametrize out_features parametrize bias False True parametrize has_non_epilogue_users True False dtypes torch bfloat test_linear_with_permute batch_size in_features image_size out_features bias has_non_epilogue_users dtype Reproducer convnext model timm M torch nn Module __init__ bias has_non_epilogue_users super __init__ linear = torch nn Linear in_features out_features bias _frozen_param = torch randn batch_size out_features conv = torch nn Conv d out_features out_features kernel_size= padding= groups=out_features linear = torch nn Linear out_features out_features bias _frozen_param = torch randn batch_size out_features has_non_epilogue_users = has_non_epilogue_users forward mul_ _convolution_pointwise_default_ out = torch ops prims convert_element_type default mul_ torch bfloat mul_ = None _linear_pointwise_default_ = linear out permute_ = torch ops aten permute default _linear_pointwise_default_ mul_ = torch ops aten mul Tensor permute_ _frozen_param add_ = torch ops aten add Tensor mul_ _convolution_pointwise_default_ convert_element_type_ = torch ops prims convert_element_type default add_ torch bfloat _convolution_pointwise_default_ = conv convert_element_type_ permute_ = torch ops aten permute default _convolution_pointwise_default_ permute_ = linear permute_ permute_ = torch ops aten permute default permute_ permute_ = torch ops aten mul Tensor permute_ _frozen_param If template_buffer will used nodes other than epilogue nodes we can t alias template_buffer Y buffer has_non_epilogue_users add_ = torch ops aten add Tensor permute_ add_ add_ permute_ view_ = torch randn batch_size image_size image_size in_features _convolution_pointwise_default_ = torch randn batch_size out_features image_size image_size memory_format=torch channels_last mod = M bias=bias has_non_epilogue_users=has_non_epilogue_users eval verify dtype atol rtol torch cpu amp autocast common mod view_ _convolution_pointwise_default_ atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter assertEqual counters inductor cpp_epilogue_fusion_counter inductor_config patch freezing True patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize batch_size parametrize in_features parametrize linear_in_features parametrize out_features parametrize bias True dtypes torch float test_linear_with_input_of_flexible_layout batch_size in_features linear_in_features out_features bias dtype Reproducer resmlp_ _ model timm flatten_BS = int batch_size linear_in_features M torch nn Module __init__ bias super __init__ conv = torch nn Conv d in_features linear_in_features kernel_size= padding= stride= dilation= groups= _frozen_param = torch randn linear_in_features _frozen_param = torch randn linear_in_features _frozen_param = torch randn linear_in_features linear = torch nn Linear out_features out_features bias forward arg _ _convolution_pointwise_default = conv arg _ view_ = torch ops aten reshape default _convolution_pointwise_default batch_size linear_in_features out_features _convolution_pointwise_default = None permute_ = torch ops aten permute default view_ view_ = None mul_ = torch ops aten mul Tensor _frozen_param permute_ add_ = torch ops aten add Tensor _frozen_param mul_ permute_ = torch ops aten permute default add_ add_ = None view_ = torch ops aten reshape default permute_ flatten_BS out_features permute_ = None _mkl_linear_ = linear view_ view_ = torch ops aten reshape default _mkl_linear_ batch_size linear_in_features out_features _mkl_linear_ = None permute_ = torch ops aten permute default view_ view_ = None mul_ = torch ops aten mul Tensor _frozen_param permute_ _frozen_param = permute_ = None add_ = torch ops aten add Tensor permute_ mul_ permute_ = mul_ = None add_ v = torch randn batch_size in_features dtype=dtype mod = M bias=bias dtype=dtype eval verify dtype atol rtol common mod v atol=atol rtol=rtol assertEqual counters inductor cpp_epilogue_fusion_counter assertEqual counters inductor cpp_templated_kernel_counter inductor_config patch freezing True patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize batch_size parametrize in_features parametrize size_ parametrize size_ parametrize out_features parametrize out_features_conv parametrize bias False True parametrize epilogue False True dtypes torch float test_linear_unsupported_epilogue_fusion batch_size in_features size_ size_ out_features out_features_conv bias epilogue dtype img_size_ = int size_ size_ img_size_ = int size_ size_ conv_shape = int size_ size_ flatten_BS = int batch_size size_ size_ size_ size_ Reproducer jx_nest_base model timm M torch nn Module __init__ bias super __init__ linear = torch nn Linear in_features in_features bias=bias linear = torch nn Linear out_features in_features bias=bias conv = torch nn Conv d in_features out_features_conv kernel_size= padding= stride= dilation= groups= epilogue = epilogue forward mul_ view_ add_ _mkl_linear_ = linear view_ view_ = torch ops aten reshape default _mkl_linear_ batch_size img_size_ img_size_ in_features _mkl_linear_ = None add_ = torch ops aten add Tensor add_ view_ add_ = view_ = None view_ = torch ops aten reshape default mul_ flatten_BS out_features mul_ = None _mkl_linear_ = linear view_ epilogue _mkl_linear_ = torch pow _mkl_linear_ _mkl_linear_ = test_operators realize _mkl_linear_ view_ = torch ops aten reshape default _mkl_linear_ batch_size img_size_ img_size_ in_features _mkl_linear_ = None add_ = torch ops aten add Tensor add_ view_ add_ = view_ = None view_ = torch ops aten reshape default add_ batch_size size_ size_ size_ size_ in_features add_ = None permute_ = torch ops aten permute default view_ view_ = None clone_ = torch ops aten clone default permute_ memory_format=torch contiguous_format permute_ = None view_ = torch ops aten reshape default clone_ batch_size conv_shape conv_shape in_features clone_ = None permute_ = torch ops aten permute default view_ view_ = None _convolution_pointwise_default_ = conv permute_ _convolution_pointwise_default_ mul_ = torch randn batch_size img_size_ img_size_ out_features view_ = torch randn flatten_BS in_features add_ = torch randn batch_size img_size_ img_size_ in_features mod = M bias=bias eval verify dtype atol rtol torch cpu amp autocast enabled=dtype == torch bfloat common mod mul_ view_ add_ atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter TODO change cpp_epilogue_fusion_counter once supported assertEqual counters inductor cpp_epilogue_fusion_counter epilogue inductor_config patch freezing True patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize batch_size parametrize in_features parametrize out_features parametrize bias True False parametrize unary relu parametrize binary add sub mul div dtypes torch float torch bfloat torch half test_linear_with_unary_binary batch_size in_features out_features bias unary binary dtype M torch nn Module __init__ bias unary binary other super __init__ linear = torch nn Linear in_features out_features bias unary = _get_epilogue unary binary = _get_epilogue binary other forward x binary unary linear x counters clear v = torch randn batch_size in_features dtype=dtype u = torch randn batch_size out_features dtype=dtype mod = M bias=bias unary=unary binary=binary other=u dtype=dtype eval verify dtype atol rtol common mod v atol=atol rtol=rtol assertEqual counters inductor cpp_epilogue_fusion_counter assertEqual counters inductor cpp_templated_kernel_counter inductor_config patch freezing True patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize batch_size parametrize in_features parametrize out_features parametrize bias True False parametrize binary add dtypes torch float torch bfloat torch half test_linear_with_binary_input_ d batch_size in_features out_features bias binary dtype M torch nn Module __init__ bias binary other super __init__ linear = torch nn Linear in_features out_features bias binary = _get_epilogue binary other forward x binary linear x counters clear B = batch_size v = torch randn B in_features dtype=dtype u = torch randn B out_features dtype=dtype mod = M bias=bias binary=binary other=u dtype=dtype eval verify dtype atol rtol common mod v atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter inductor_config patch freezing True patches torch no_grad unittest skipIf TEST_MKL Test requires MKL set_num_threads dynamo_config patch dynamic_shapes True assume_static_by_default False parametrize batch_size parametrize in_features parametrize out_features parametrize out_features parametrize bias True False dtypes torch float test_linear_local_and_global_buffer_dynamic_shapes batch_size in_features out_features out_features bias dtype Reproducer soft_actor_critic M torch nn Module __init__ bias super __init__ linear = torch nn Linear in_features out_features bias linear = torch nn Linear out_features out_features bias linear = torch nn Linear out_features out_features bias forward arg _ addmm_ = linear arg _ relu_ = torch ops aten relu default addmm_ addmm_ = linear relu_ relu_ = torch ops aten relu default addmm_ addmm_ = linear relu_ split_ = torch ops aten split Tensor addmm_ getitem_ = split_ getitem_ = split_ tanh_ = torch ops aten tanh default getitem_ add_ = torch ops aten add Tensor tanh_ mul_ = torch ops aten mul Tensor add_ add_ = torch ops aten add Tensor mul_ - exp_ = torch ops aten exp default add_ getitem_ exp_ counters clear v = torch randn batch_size in_features dtype=dtype mod = M bias=bias dtype=dtype eval verify dtype atol rtol common mod v atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter assertEqual counters inductor cpp_epilogue_fusion_counter unittest skipIf torch _C _cpu _is_amx_tile_supported AMX ISA support required inductor_config patch freezing True patches torch no_grad parametrize batch_size parametrize in_features parametrize out_features parametrize bias True False dtypes torch bfloat torch half test_linear_amx batch_size in_features out_features bias dtype M torch nn Module __init__ bias super __init__ linear = torch nn Linear in_features out_features bias forward x linear x counters clear v = torch randn batch_size in_features dtype=dtype mod = M bias=bias dtype=dtype eval verify dtype atol rtol common mod v atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter vec_amx = VecAMX Currently brgemm config only added half dtype == torch half vec_amx is_amx_fp _supported _check_brgemm_counter vec_amx _check_amx_counter vec_amx inductor_config patch freezing True patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize batch_size parametrize in_features parametrize in_features_ parametrize out_features parametrize bias True dtypes torch float test_linear_with_multiple_reindexers batch_size in_features in_features_ out_features bias dtype flatten_BS = int batch_size in_features_ Reproducer levit_ model timm M torch nn Module __init__ bias super __init__ conv = torch nn Conv d kernel_size= padding= stride= dilation= groups= linear = torch nn Linear in_features out_features bias=False _frozen_param = torch randn out_features _frozen_param = torch randn out_features _frozen_param = torch randn out_features _frozen_param = torch randn out_features forward view_ _mkl_linear_ = linear view_ view_ = torch ops aten reshape default _mkl_linear_ batch_size in_features_ out_features _mkl_linear_ = None view_ = torch ops aten reshape default view_ flatten_BS out_features view_ = None sub_ = torch ops aten sub Tensor view_ _frozen_param view_ = _frozen_param = None mul_ = torch ops aten mul Tensor sub_ _frozen_param sub_ = _frozen_param = None mul_ = torch ops aten mul Tensor mul_ _frozen_param mul_ = _frozen_param = None add_ = torch ops aten add Tensor mul_ _frozen_param mul_ = _frozen_param = None view_ = torch ops aten reshape default add_ batch_size in_features_ out_features add_ = None add_ = torch ops aten add Tensor view_ clamp_min_ = torch ops aten clamp_min default add_ add_ = None clamp_max_ = torch ops aten clamp_max default clamp_min_ clamp_min_ = None mul_ = torch ops aten mul Tensor view_ clamp_max_ view_ = clamp_max_ = None div_ = torch ops aten div Tensor mul_ mul_ = None div_ view_ = torch randn flatten_BS in_features mod = M bias=bias eval verify dtype atol rtol common mod view_ atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter assertEqual counters inductor cpp_epilogue_fusion_counter inductor_config patch freezing True patches torch no_grad parametrize batch_size parametrize in_features parametrize out_features parametrize bias True False dtypes torch bfloat test_linear_with_embedding batch_size in_features out_features bias dtype M torch nn Module __init__ bias super __init__ linear = torch nn Linear in_features out_features bias dtype=dtype emb = torch nn Embedding out_features forward idx x emb idx + linear x idx = torch randint batch_size x = torch randn batch_size in_features dtype=dtype mod = M bias=bias eval verify dtype atol rtol common mod idx x atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter assertEqual counters inductor cpp_epilogue_fusion_counter inductor_config patch freezing True patches torch no_grad parametrize batch_size parametrize in_features parametrize seq_lens parametrize out_features parametrize bias True dtypes torch bfloat test_linear_with_indirect_indexing batch_size in_features seq_lens out_features bias dtype Reproducer GPT ForSequenceClassification model HuggingFace M torch nn Module __init__ bias super __init__ wte = torch nn Embedding seq_lens wpe = torch nn Embedding in_features seq_lens linear = torch nn Linear out_features seq_lens bias forward view_ input_ids view_ inputs_embeds = wte input_ids position_ids = torch arange in_features dtype=torch long position_ids = position_ids unsqueeze position_embeds = wpe position_ids add = inputs_embeds + position_embeds add_ = view_ + add _linear_pointwise_default_ = linear view_ view_ = torch ops aten reshape default _linear_pointwise_default_ batch_size in_features seq_lens out = torch ops aten add Tensor add_ view_ out view_ = torch randn batch_size in_features out_features input_ids = torch randint batch_size in_features view_ = torch randn batch_size in_features seq_lens mod = M bias=bias eval verify dtype atol rtol torch cpu amp autocast common mod view_ input_ids view_ atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter assertEqual counters inductor cpp_epilogue_fusion_counter inductor_config patch freezing True patches torch no_grad parametrize batch_size parametrize in_features parametrize in_features parametrize image_size parametrize out_features parametrize bias True dtypes torch float test_linear_with_in_out_buffer batch_size in_features in_features image_size out_features bias dtype Reproducer coat_lite_mini model timm M torch nn Module __init__ bias super __init__ _frozen_param = torch randn batch_size out_features conv = torch nn Conv d in_features out_features kernel_size= padding= stride= dilation= groups= conv = torch nn Conv d out_features out_features kernel_size= padding= stride= dilation= groups=out_features conv = torch nn Conv d kernel_size= padding= stride= dilation= groups= conv = torch nn Conv d kernel_size= padding= stride= dilation= groups= conv = torch nn Conv d kernel_size= padding= stride= dilation= groups= linear = torch nn Linear out_features in_features bias linear = torch nn Linear out_features out_features bias _frozen_param = torch randn out_features _frozen_param = torch randn out_features _frozen_param = torch randn out_features _frozen_param = torch randn out_features _frozen_param = torch randn batch_size out_features forward arg _ _convolution_pointwise_default_ = conv arg _ arg _ = None view_ = torch ops aten reshape default _convolution_pointwise_default_ _convolution_pointwise_default_ = None permute_ = torch ops aten permute default view_ view_ = None clone_ = torch ops aten clone default permute_ memory_format=torch contiguous_format permute_ = None var_mean_ = torch ops aten var_mean correction clone_ correction= keepdim=True getitem_ = var_mean_ getitem_ = var_mean_ var_mean_ = None add_ = torch ops aten add Tensor getitem_ e- getitem_ = None rsqrt_ = torch ops aten rsqrt default add_ add_ = None sub_ = torch ops aten sub Tensor clone_ getitem_ clone_ = getitem_ = None mul_ = torch ops aten mul Tensor sub_ rsqrt_ sub_ = rsqrt_ = None mul_ = torch ops aten mul Tensor mul_ _frozen_param mul_ = None add_ = torch ops aten add Tensor mul_ _frozen_param mul_ = None _frozen_param = _frozen_param cat_ = torch ops aten cat default _frozen_param add_ _frozen_param = add_ = None slice_ = torch ops aten slice Tensor cat_ slice_ = torch ops aten slice Tensor cat_ cat_ = None permute_ = torch ops aten permute default slice_ slice_ = None view_ = torch ops aten reshape default permute_ permute_ = None _convolution_pointwise_default_ = conv view_ add_ = torch ops aten add Tensor _convolution_pointwise_default_ view_ _convolution_pointwise_default_ = view_ = None view_ = torch ops aten reshape default add_ add_ = None permute_ = torch ops aten permute default view_ view_ = None cat_ = torch ops aten cat default slice_ permute_ slice_ = permute_ = None var_mean_ = torch ops aten var_mean correction cat_ correction= keepdim=True getitem_ = var_mean_ getitem_ = var_mean_ var_mean_ = None add_ = torch ops aten add Tensor getitem_ e- getitem_ = None rsqrt_ = torch ops aten rsqrt default add_ add_ = None sub_ = torch ops aten sub Tensor cat_ getitem_ getitem_ = None mul_ = torch ops aten mul Tensor sub_ rsqrt_ sub_ = rsqrt_ = None mul_ = torch ops aten mul Tensor mul_ _frozen_param mul_ = None add_ = torch ops aten add Tensor mul_ _frozen_param mul_ = None view_ = torch ops aten reshape default add_ add_ = None _mkl_linear_ = linear view_ view_ = None view_ = torch ops aten reshape default _mkl_linear_ _mkl_linear_ = None view_ = torch ops aten reshape default view_ view_ = None permute_ = torch ops aten permute default view_ view_ = None unbind_ = torch ops aten unbind int permute_ permute_ = None getitem_ = unbind_ getitem_ = unbind_ getitem_ = unbind_ unbind_ = None clone_ = torch ops aten clone default getitem_ memory_format=torch contiguous_format getitem_ = None amax_ = torch ops aten amax default clone_ True sub_ = torch ops aten sub Tensor clone_ amax_ clone_ = amax_ = None exp_ = torch ops aten exp default sub_ sub_ = None sum_ = torch ops aten sum dim_IntList exp_ True div_ = torch ops aten div Tensor exp_ sum_ exp_ = sum_ = None permute_ = torch ops aten permute default div_ div_ = None expand_ = torch ops aten expand default permute_ permute_ = None view_ = torch ops aten reshape default expand_ expand_ = None expand_ = torch ops aten expand default getitem_ clone_ = torch ops aten clone default expand_ memory_format=torch contiguous_format expand_ = None view_ = torch ops aten reshape default clone_ clone_ = None bmm_ = torch ops aten bmm default view_ view_ view_ = view_ = None view_ = torch ops aten reshape default bmm_ bmm_ = None expand_ = torch ops aten expand default getitem_ clone_ = torch ops aten clone default expand_ memory_format=torch contiguous_format expand_ = None view_ = torch ops aten reshape default clone_ clone_ = None expand_ = torch ops aten expand default view_ view_ = None view_ = torch ops aten reshape default expand_ expand_ = None bmm_ = torch ops aten bmm default view_ view_ view_ = view_ = None view_ = torch ops aten reshape default bmm_ bmm_ = None slice_ = torch ops aten slice Tensor getitem_ getitem_ = None slice_ = torch ops aten slice Tensor getitem_ getitem_ = None permute_ = torch ops aten permute default slice_ slice_ = None view_ = torch ops aten reshape default permute_ permute_ = None split_with_sizes_ = torch ops aten split_with_sizes default view_ view_ = None getitem_ = split_with_sizes_ getitem_ = split_with_sizes_ getitem_ = split_with_sizes_ split_with_sizes_ = None _convolution_pointwise_default_ = conv getitem_ _convolution_pointwise_default_ = conv getitem_ _convolution_pointwise_default_ = conv getitem_ cat_ = torch ops aten cat default _convolution_pointwise_default_ _convolution_pointwise_default_ _convolution_pointwise_default_ _convolution_pointwise_default_ = _convolution_pointwise_default_ = _convolution_pointwise_default_ = None view_ = torch ops aten reshape default cat_ cat_ = None permute_ = torch ops aten permute default view_ view_ = None mul_ = torch ops aten mul Tensor slice_ permute_ slice_ = permute_ = None constant_pad_nd_ = torch ops aten constant_pad_nd default mul_ mul_ = None mul_ = torch ops aten mul Tensor view_ view_ = None add_ = torch ops aten add Tensor mul_ constant_pad_nd_ mul_ = constant_pad_nd_ = None add_ view_ = torch randn batch_size in_features image_size image_size mod = M bias=bias eval verify dtype atol rtol common mod view_ atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter assertEqual counters inductor cpp_epilogue_fusion_counter TEST_MKL inductor_config patch freezing True patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize batch_size parametrize in_features parametrize out_features parametrize bias False True parametrize input_ d False True dtypes torch float torch bfloat parametrize epilogue none relu gelu skipIfWindows msg= Windows don t support quantize test_quantized_linear_with_pointwise batch_size in_features out_features bias input_ d dtype epilogue B = batch_size input_ d batch_size input = torch randn B in_features dtype=torch float M torch nn Module __init__ bias super __init__ linear = torch nn Linear in_features out_features bias epilogue = _get_epilogue epilogue linear = torch nn Linear out_features out_features bias epilogue = _get_epilogue epilogue forward x res = epilogue linear x res = epilogue linear res res counters clear ref_quantized_mod = _generate_qdq_quantized_model M bias=bias eval input atol rtol = e- e- dtype == torch bfloat atol rtol = e- e- patch object select_algorithm VERIFY dict atol=atol rtol=rtol torch no_grad torch autocast cpu enabled= dtype == torch bfloat dtype=dtype ref_res = ref_quantized_mod input cfn = torch compile ref_quantized_mod res = cfn input assertEqual res ref_res atol=atol rtol=rtol equal_nan=True exact_dtype=True assertEqual counters inductor cpp_templated_kernel_counter assertEqual counters inductor cpp_epilogue_fusion_counter inductor_config patch freezing True patches torch no_grad dtypes torch bfloat parametrize batch_size parametrize mid_dim parametrize in_features parametrize out_features test_int _woq_mm dtype batch_size mid_dim in_features out_features _convert_weight_to_int pack w scale zp = _calculate_dynamic_per_channel_qparams w torch float torch int scale = torch from_numpy scale zp = torch from_numpy zp w_int = torch ao quantization fx _decomposed quantize_per_channel input=w scales=scale zero_points=zp axis= quant_min=- quant_max= dtype=torch int w_int scale torch bfloat M torch nn Module __init__ w super __init__ linear_weight = torch nn Parameter w requires_grad=False forward x scale torch nn functional linear x linear_weight x dtype scale counters clear Currently corresponding torch fx pattern only supports D x Add D X case once corresponding pattern-matcher pattern added x = torch rand batch_size mid_dim in_features dtype=dtype w = torch rand out_features in_features dtype=dtype w_int pack w_scales = _convert_weight_to_int pack w mod = M w_int pack eval common mod x w_scales assertEqual counters inductor cpp_templated_kernel_counter batch_size mid_dim = vec_amx = VecAMX _check_amx_counter vec_amx inductor_config patch freezing True cpp enable_concat_linear True patches torch no_grad dtypes torch bfloat parametrize batch_size parametrize mid_dim parametrize in_features parametrize out_features test_int _woq_mm_concat dtype batch_size mid_dim in_features out_features _convert_weight_to_int pack w scale zp = _calculate_dynamic_per_channel_qparams w torch float torch int scale = torch from_numpy scale zp = torch from_numpy zp w_int = torch ao quantization fx _decomposed quantize_per_channel input=w scales=scale zero_points=zp axis= quant_min=- quant_max= dtype=torch int w_int scale torch bfloat M torch nn Module __init__ w w w super __init__ w = torch nn Parameter w requires_grad=False w = torch nn Parameter w requires_grad=False w = torch nn Parameter w requires_grad=False forward x scale scale scale Ref _linear_fp_act_int _weight_impl torchao dtypes uintx plain_layout py y = torch mm x reshape - x shape - w t x dtype scale y = torch mm x reshape - x shape - w t x dtype scale y = torch mm x reshape - x shape - w t x dtype scale y reshape x shape - y shape - y reshape x shape - y shape - y reshape x shape - y shape - counters clear Currently corresponding torch fx pattern only supports D x Add D X case once corresponding pattern-matcher pattern added x = torch rand batch_size mid_dim in_features dtype=dtype w = torch rand out_features in_features dtype=dtype w = torch rand out_features in_features dtype=dtype w = torch rand out_features in_features dtype=dtype w _int pack w _scales = _convert_weight_to_int pack w w _int pack w _scales = _convert_weight_to_int pack w w _int pack w _scales = _convert_weight_to_int pack w mod = M w _int pack w _int pack w _int pack eval common mod x w _scales w _scales w _scales assertEqual counters inductor cpp_templated_kernel_counter batch_size mid_dim = vec_amx = VecAMX _check_amx_counter vec_amx unittest skipIf torch _C _cpu _is_amx_tile_supported AMX ISA support required inductor_config patch freezing True patches torch no_grad We set allow_ignore_mark_dynamic True because Dynamo may end up specializing M dimension despite being marked dynamic mark_dynamic dynamo_config patch allow_ignore_mark_dynamic True parametrize has_bias True False parametrize dtype torch float torch bfloat parametrize per_channel_quant True False parametrize reshape_a True False parametrize expand_a_scale True False parametrize dynamic True False parametrize M test_da w _sym_act_sym_wgt_with_int_mm has_bias dtype per_channel_quant reshape_a expand_a_scale dynamic M r This testcase check we can match int _dynamic_activation_int _weight int linear pattern torchao when activation symmetrically quantized dynamically weights symmetrically quantized statically The pattern no bias _int_mm - convert_element_type - maybe_expand_a_scale - mul - mul bias pattern_no_bias - add Expansion scale activation optional The pattern depiction doesn t mean convert_element_type output fed into expand_a input simply activation scale may applied after expand operation dtype == torch bfloat torch ops mkldnn _is_mkldnn_bf _supported in_feature = out_feature = q_min q_max = - Mod torch nn Module __init__ dtype torch dtype has_bias bool super __init__ dtype = dtype has_bias = has_bias b = torch randint q_min q_max in_feature out_feature dtype=torch int per_channel_quant = per_channel_quant a_scale_per_tensor = torch rand dtype=dtype + a_scale_per_channel = torch rand M dtype=dtype + a_scale = a_scale_per_channel per_channel_quant a_scale_per_tensor b_scale = torch rand out_feature + b_scale = b_scale dtype bias = torch rand out_feature dtype=dtype has_bias None forward reshape_a a_reshaped = reshape - size - a_reshaped = c = torch _int_mm a_reshaped b c = c dtype expand_a_scale a_scale = a_scale a_scale = a_scale expand c shape c = c a_scale c = c b_scale has_bias c = c + bias c mod = Mod dtype has_bias eval = torch randint q_min q_max M in_feature dtype=torch int dynamic torch _dynamo mark_dynamic torch _dynamo mark_static common mod atol= e- dtype torch bfloat None rtol= e- dtype torch bfloat None vec_amx = VecAMX _check_amx_counter vec_amx torch _C _cpu _is_amx_tile_supported Only AMX ISA based micro-kernel currently supported da w assertEqual counters inductor cpp_templated_kernel_counter inductor_config patch freezing True patches torch no_grad dtypes torch bfloat parametrize batch_size parametrize in_features parametrize out_features parametrize group_size test_int _woq_mm_avx dtype batch_size in_features out_features group_size M torch nn Module __init__ K N group_size super __init__ linear_weight = torch randint N K dtype=torch uint qscale_and_zeros = torch rand K group_size N dtype=dtype group_size = group_size forward x x_shape = x shape x = x reshape - x_shape - y = torch _weight_int pack_mm_for_cpu x linear_weight group_size qscale_and_zeros y reshape x_shape - out_features counters clear seq_len = x = torch rand batch_size seq_len in_features dtype=dtype mod = M in_features out_features group_size eval common mod x reference_in_float=False available_isa = torch _inductor cpu_vec_isa pick_vec_isa avx _available = avx str available_isa autotune_count = avx _available assertEqual counters inductor select_algorithm_autotune autotune_count unittest skipIf torch _C _cpu _is_amx_tile_supported AMX ISA support required inductor_config patch freezing True patches torch no_grad dtypes torch bfloat parametrize batch_size parametrize in_features parametrize out_features parametrize group_size set_num_threads test_int _woq_mm_amx_Nc_larger_than_one dtype batch_size in_features out_features group_size Note ` torch _weight_int pack_mm_for_cpu ` computes float while AMX-based GEMM template computes bfloat So difference computation results may big But we need ` _weight_int pack_mm_for_cpu ` its pattern Therefore we define module M its pattern parameters define module M reference computation M s forward function gets dequantized unpacked weight bfloat then computes GEMM bfloat Besides we need skip VERIFY patch cannot use common testing M torch nn Module __init__ K N group_size super __init__ linear_weight = torch randint N K dtype=torch uint qscale_and_zeros = torch rand K group_size N dtype=dtype group_size = group_size forward x x_shape = x shape x = x reshape - x_shape - y = torch _weight_int pack_mm_for_cpu x linear_weight group_size qscale_and_zeros y reshape x_shape - out_features M torch nn Module __init__ mod M super __init__ mod = mod forward x x_eye = torch eye x shape - device=x device dtype=x dtype dq_w = mod x_eye T contiguous torch nn functional linear x dq_w counters clear seq_len = x = torch rand batch_size seq_len in_features dtype=dtype mod = M in_features out_features group_size eval mod = M mod Skip VERIFY during torch compile don t use common See explanation above patch object select_algorithm VERIFY None m = torch compile mod y_ref = mod x y = m x assertEqual y y_ref atol= e- rtol= e- assertEqual counters inductor select_algorithm_autotune unittest skipIf torch _C _cpu _is_amx_tile_supported AMX ISA support required inductor_config patch freezing True inductor_config patch cpp use_small_dequant_buffer True patches torch no_grad dtypes torch bfloat parametrize batch_size parametrize in_features parametrize out_features parametrize group_size set_num_threads test_int _woq_mm_with_small_buffer_config dtype batch_size in_features out_features group_size M torch nn Module __init__ K N group_size super __init__ linear_weight = torch randint N K dtype=torch uint qscale_and_zeros = torch rand K group_size N dtype=dtype group_size = group_size forward x x_shape = x shape x = x reshape - x_shape - y = torch _weight_int pack_mm_for_cpu x linear_weight group_size qscale_and_zeros y reshape x_shape - out_features counters clear seq_len = x = torch rand batch_size seq_len in_features dtype=dtype mod = M in_features out_features group_size eval patch object select_algorithm VERIFY None m = torch compile mod _ code = run_and_get_cpp_code m x kr = only kr= supported woq int amx kernel _target_code_check = f constexpr int _t Kc_blocks = group_size kr torch _C FileCheck check _target_code_check run code unittest skipIf torch _C _cpu _is_amx_tile_supported AMX ISA support required inductor_config patch freezing True patches torch no_grad dtypes torch bfloat parametrize batch_size parametrize in_features parametrize out_features parametrize group_size test_int _woq_mm_amx dtype batch_size in_features out_features group_size Note ` torch _weight_int pack_mm_for_cpu ` computes float while AMX-based GEMM template computes bfloat So difference computation results may big But we need ` _weight_int pack_mm_for_cpu ` its pattern Therefore we define module M its pattern parameters define module M reference computation M s forward function gets dequantized unpacked weight bfloat then computes GEMM bfloat Besides we need skip VERIFY patch cannot use common testing M torch nn Module __init__ K N group_size super __init__ linear_weight = torch randint N K dtype=torch uint qscale_and_zeros = torch rand K group_size N dtype=dtype group_size = group_size forward x x_shape = x shape x = x reshape - x_shape - y = torch _weight_int pack_mm_for_cpu x linear_weight group_size qscale_and_zeros y reshape x_shape - out_features M torch nn Module __init__ mod M super __init__ mod = mod forward x x_eye = torch eye x shape - device=x device dtype=x dtype dq_w = mod x_eye T contiguous torch nn functional linear x dq_w counters clear seq_len = x = torch rand batch_size seq_len in_features dtype=dtype mod = M in_features out_features group_size eval mod = M mod Skip VERIFY during torch compile don t use common See explanation above patch object select_algorithm VERIFY None m = torch compile mod y_ref = mod x y = m x assertEqual y y_ref atol= e- rtol= e- assertEqual counters inductor select_algorithm_autotune unittest skipIf torch _C _cpu _is_amx_tile_supported AMX ISA support required inductor_config patch freezing True inductor_config patch cpp enable_concat_linear True patches torch no_grad dtypes torch bfloat parametrize batch_size parametrize in_features parametrize out_features parametrize group_size test_int _concat_woq_mm dtype batch_size in_features out_features group_size M torch nn Module __init__ K out_features group_size super __init__ linear_weight = torch randint N K dtype=torch uint N out_features qscale_and_zeros = torch rand K group_size N dtype=dtype N out_features group_size = group_size out_features = out_features forward x x_shape = x shape x = x reshape - x_shape - y = torch _weight_int pack_mm_for_cpu x linear_weight idx group_size qscale_and_zeros idx idx range len out_features y idx reshape x_shape - out_features idx idx range len out_features M torch nn Module __init__ mod M super __init__ mod = mod forward x x_eye = torch eye x shape - device=x device dtype=x dtype dq_w_list = idx range len mod out_features x_shape = x_eye shape dq_w = torch _weight_int pack_mm_for_cpu x_eye mod linear_weight idx mod group_size mod qscale_and_zeros idx dq_w_list append dq_w reshape x_shape - mod out_features idx T contiguous torch nn functional linear x dq_w dq_w dq_w_list counters clear seq_len = x = torch rand batch_size seq_len in_features dtype=dtype mod = M in_features out_features group_size eval mod = M mod Skip VERIFY during torch compile don t use common See explanation above patch object select_algorithm VERIFY None y_ref = mod x m = torch compile mod y = m x assertEqual y y_ref atol= e- rtol= e- Only do once tuning since wgt has been concat assertEqual counters inductor select_algorithm_autotune inductor_config patch freezing True patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize batch_size parametrize in_features parametrize out_features parametrize bias False True parametrize input_ d False True parametrize int _mixed_bf False True dtypes torch float torch bfloat parametrize epilogue none relu skipIfWindows msg= Windows don t support quantize test_quantized_linear_with_pointwise_binary batch_size in_features out_features bias input_ d int _mixed_bf dtype epilogue int _mixed_bf dtype == torch bfloat B = batch_size input_ d batch_size input = torch randn B in_features dtype=torch float other = torch randn B out_features dtype=dtype Avoid hitting qlinear inplace sum fusion input_ d other = torch randn B B out_features dtype=dtype other = torch randn B out_features dtype=dtype M torch nn Module __init__ bias input_ d super __init__ linear = torch nn Linear in_features out_features bias epilogue = _get_epilogue epilogue linear = torch nn Linear out_features out_features bias epilogue = _get_epilogue epilogue input_ d = input_ d forward x other other res = epilogue linear x + other Avoid hitting qlinear inplace sum fusion input_ d other = other view other size other size other = other view other size other size res = epilogue linear res + other res counters clear ref_quantized_mod = _generate_qdq_quantized_model M bias=bias input_ d=input_ d eval input other other atol rtol = e- e- patch object select_algorithm VERIFY dict atol=atol rtol=rtol torch no_grad torch autocast cpu enabled=int _mixed_bf dtype=torch bfloat ref_res = ref_quantized_mod input other other cfn = torch compile ref_quantized_mod res = cfn input other other assertEqual res ref_res atol=atol rtol=rtol equal_nan=True exact_dtype=True assertEqual counters inductor cpp_templated_kernel_counter assertEqual counters inductor cpp_epilogue_fusion_counter unittest skipIf torch _C _cpu _is_amx_tile_supported AMX ISA support required inductor_config patch freezing True patches torch no_grad parametrize batch_size parametrize in_features k should multiple parametrize out_features parametrize bias True False skipIfWindows msg= Windows don t support quantize test_quantized_linear_amx batch_size in_features out_features bias M torch nn Module __init__ bias super __init__ linear = torch nn Linear in_features out_features bias forward x linear x counters clear v = torch randn batch_size in_features dtype=torch float ref_quantized_mod = _generate_qdq_quantized_model M bias=bias eval v atol rtol = e- e- patch object select_algorithm VERIFY dict atol=atol rtol=rtol common ref_quantized_mod v atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter vec_amx = VecAMX _check_amx_counter vec_amx inductor_config patch freezing True inductor_config patch cpp gemm_max_k_slices patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize batch_size parametrize in_features parametrize out_features parametrize bias True False parametrize epilogue none relu dtypes torch float torch bfloat torch half test_linear_k_slicing batch_size in_features out_features bias epilogue dtype M torch nn Module __init__ bias epilogue other super __init__ linear = torch nn Linear in_features out_features bias epilogue = _get_epilogue epilogue other forward x epilogue linear x counters clear v = torch randn batch_size in_features dtype=dtype u = torch randn batch_size out_features dtype=dtype mod = M bias=bias epilogue=epilogue other=u dtype=dtype eval verify dtype atol rtol common mod v atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter inductor_config patch freezing True inductor_config patch cpp gemm_cache_blocking patches torch no_grad unittest skipIf TEST_MKL Test requires MKL set_num_threads parametrize batch_size parametrize in_features parametrize out_features parametrize bias True False dtypes torch float torch bfloat torch half test_linear_cache_blocking batch_size in_features out_features bias dtype M torch nn Module __init__ bias super __init__ linear = torch nn Linear in_features out_features bias forward x linear x counters clear v = torch randn batch_size in_features dtype=dtype mod = M bias=bias dtype=dtype eval verify dtype atol rtol common mod v atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter inductor_config patch freezing True inductor_config patch cpp gemm_thread_factors patches torch no_grad unittest skipIf TEST_MKL Test requires MKL set_num_threads parametrize batch_size parametrize in_features parametrize out_features parametrize bias True False dtypes torch float torch bfloat torch half test_linear_thread_factors batch_size in_features out_features bias dtype M torch nn Module __init__ bias super __init__ linear = torch nn Linear in_features out_features bias forward x linear x counters clear v = torch randn batch_size in_features dtype=dtype mod = M bias=bias dtype=dtype eval verify dtype atol rtol common mod v atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter inductor_config patch freezing False patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize batch_size parametrize in_features parametrize out_features parametrize bias True dtypes torch float test_aoti_linear batch_size in_features out_features bias dtype try try test_aot_inductor_utils except ImportError test_aot_inductor_utils except Exception skip UT failed M torch nn Module __init__ bias=bias - None super __init__ mlp = torch nn Sequential torch nn Linear in_features out_features bias=bias torch nn ReLU forward x mlp x assert torch _inductor config freezing False counters clear v = torch randn batch_size in_features dtype=dtype mod = M bias=bias dtype=dtype eval torch _dynamo reset torch _inductor metrics reset torch manual_seed verify dtype atol rtol torch no_grad expected = mod v actual = test_aot_inductor_utils AOTIRunnerUtil run mod v assertEqual actual expected atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter inductor_config patch freezing True inductor_config patch cpp enable_grouped_gemm_template True patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize batch_size parametrize in_features parametrize out_features parametrize gemm_num test_grouped_linear_invalid batch_size in_features out_features gemm_num M torch nn Module __init__ in_feature out_feature gemm_num super __init__ linears = torch nn Linear in_feature out_feature + gemm_idx bias=False gemm_idx range gemm_num forward x linear x linear linears each linear has different num out features thus invalid grouped gemm dtypes = torch ops mkldnn _is_mkldnn_bf _supported dtypes append torch bfloat torch ops mkldnn _is_mkldnn_fp _supported dtypes append torch float dtype dtypes torch _dynamo reset torch _inductor metrics reset counters clear mod = M in_features out_features gemm_num eval v = torch randn batch_size in_features dtype verify dtype atol rtol torch autocast device_type= cpu dtype=dtype torch no_grad common mod v atol=atol rtol=rtol gemm_num independent template instead grouped gemm template assertEqual counters inductor cpp_templated_kernel_counter gemm_num assertEqual counters inductor cpp_grouped_gemm_template inductor_config patch freezing True inductor_config patch cpp enable_grouped_gemm_template True patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize batch_size parametrize in_features parametrize out_features parametrize input_ d False True parametrize gemm_num test_grouped_linear batch_size in_features out_features input_ d gemm_num M torch nn Module __init__ in_feature out_feature gemm_num super __init__ linears = torch nn Linear in_feature out_feature bias=False _ range gemm_num forward x linear x linear linears dtypes = torch ops mkldnn _is_mkldnn_bf _supported dtypes append torch bfloat torch ops mkldnn _is_mkldnn_fp _supported dtypes append torch float dtype dtypes dtype == torch float input_ d reduce number tests continue torch _dynamo reset torch _inductor metrics reset counters clear mod = M in_features out_features gemm_num eval B = batch_size input_ d batch_size v = torch randn B in_features dtype verify dtype atol rtol torch autocast device_type= cpu dtype=dtype torch no_grad common mod v atol=atol rtol=rtol assertEqual counters inductor cpp_grouped_gemm_template inductor_config patch freezing True inductor_config patch cpp enable_grouped_gemm_template True patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize batch_size parametrize in_features parametrize out_features parametrize input_ d True False parametrize bias True True True False False True False False parametrize epilogue none none relu none none relu relu relu silu mul test_grouped_linear_epilogue batch_size in_features out_features input_ d bias epilogue M torch nn Module __init__ in_feature out_feature bias epilogue super __init__ linear = torch nn Linear in_feature out_feature bias=bias linear = torch nn Linear in_feature out_feature bias=bias epilogue = epilogue epilogue = epilogue forward x res = linear x res = linear x epilogue == silu epilogue == mul torch nn functional silu res res epilogue == relu res = torch nn functional relu res epilogue == relu res = torch nn functional relu res res res dtypes = torch ops mkldnn _is_mkldnn_bf _supported dtypes append torch bfloat torch ops mkldnn _is_mkldnn_fp _supported dtypes append torch float dtype dtypes input_ d dtype == torch float Reduce number test cases continue torch _dynamo reset torch _inductor metrics reset counters clear mod = M in_features out_features bias epilogue eval B = batch_size input_ d batch_size v = torch randn B in_features dtype verify dtype atol rtol torch autocast device_type= cpu dtype=dtype torch no_grad common mod v atol=atol rtol=rtol assertEqual counters inductor cpp_grouped_gemm_template any e = none e epilogue assertGreater counters inductor cpp_epilogue_fusion_counter inductor_config patch freezing False patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize batch_size parametrize in_features parametrize out_features dtypes torch float test_aoti_linear_multi_view_operations batch_size in_features out_features dtype try try test_aot_inductor_utils except ImportError test_aot_inductor_utils except Exception skip UT failed M torch nn Module __init__ - None super __init__ bias = torch randn out_features weight = torch randn out_features in_features relu = torch nn ReLU forward x tmp = torch addmm bias x weight permute view in_features out_features relu tmp assert torch _inductor config freezing False counters clear v = torch randn batch_size in_features dtype=dtype mod = M dtype=dtype eval torch _dynamo reset torch _inductor metrics reset torch manual_seed verify dtype atol rtol torch no_grad expected = mod v actual = test_aot_inductor_utils AOTIRunnerUtil run mod v assertEqual actual expected atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter inductor_config patch freezing True inductor_config patch coordinate_descent_tuning True patches torch no_grad unittest skipIf TEST_MKL Test requires MKL test_cpp_coordinate_descent_tuning M torch nn Module __init__ super __init__ linear = torch nn Linear bias=False forward x linear x v = torch randn mod = M eval torch _dynamo reset torch _inductor metrics reset counters clear verify torch bfloat atol rtol torch autocast device_type= cpu common mod v atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter inductor_config patch freezing True patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize batch_size parametrize in_features parametrize out_features parametrize bias True False test_linear_to_lowp_fp batch_size in_features out_features bias M torch nn Module __init__ bias super __init__ linear = torch nn Linear in_features out_features bias forward x linear x torch float counters clear dtype = torch float mod = M bias=bias dtype=dtype eval B = batch_size v = torch randn B in_features dtype=dtype verify dtype atol rtol common mod v atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter inductor_config patch freezing True patches torch no_grad unittest skipIf TEST_MKL Test requires MKL test_cpp_weight_prune M torch nn Module __init__ super __init__ linear = torch nn Linear bias=False forward x linear x v = torch randn torch bfloat mod = M eval torch bfloat torch _dynamo reset torch _inductor metrics reset counters clear verify torch bfloat atol rtol common mod v atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter assertEqual counters inductor select_algorithm_weight_prune patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize bs parametrize Mdim parametrize Kdim parametrize Ndim dtypes torch float torch bfloat torch half test_bmm dtype bs Mdim Kdim Ndim M torch nn Module __init__ super __init__ forward x y x y counters clear u = torch randn bs Mdim Kdim dtype=dtype v = torch randn bs Kdim Ndim dtype=dtype mod = M dtype=dtype eval verify dtype atol rtol common mod u v atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize bs parametrize Mdim parametrize Kdim parametrize Ndim dtypes torch bfloat torch half test_bmm_amx dtype bs Mdim Kdim Ndim M torch nn Module __init__ super __init__ forward x y x y counters clear u = torch randn bs Mdim Kdim dtype=dtype v = torch randn bs Kdim Ndim dtype=dtype mod = M dtype=dtype eval verify dtype atol rtol common mod u v atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter vec_amx = VecAMX Currently brgemm config only added half dtype == torch half _check_brgemm_counter vec_amx _check_amx_counter vec_amx patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize bs parametrize Mdim parametrize Kdim parametrize Ndim dtypes torch float torch bfloat torch half test_bmm_amp dtype bs Mdim Kdim Ndim M torch nn Module __init__ super __init__ forward x y x y counters clear u = torch randn bs Mdim Kdim dtype=dtype v = torch randn bs Kdim Ndim dtype=dtype mod = M dtype=dtype eval verify dtype atol rtol torch amp autocast cpu common mod u v atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter inductor_config patch freezing True patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize bs parametrize Mdim parametrize Kdim parametrize Ndim dtypes torch float torch bfloat torch half test_bmm_freezing dtype bs Mdim Kdim Ndim M torch nn Module __init__ w super __init__ w = torch nn Parameter w requires_grad=False forward x x w counters clear u = torch randn bs Mdim Kdim dtype=dtype v = torch randn bs Kdim Ndim dtype=dtype mod = M v dtype=dtype eval verify dtype atol rtol common mod u atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize Ndim parametrize order First BMM hf_Reformer First BMM hf_DistilBert Second BMM hf_DistilBert hf_T Third BMM hf_Reformer First hf_T dtypes torch float torch bfloat torch half test_bmm_ d_permute Ndim order dtype TODO Support bmm transposed X bs = Mdim = Kdim = x_args = bs Mdim Kdim w_args = bs Kdim Ndim inverse_order = torch argsort torch tensor o tolist o order M torch nn Module __init__ super __init__ forward x w order = x_order = x_args i i inverse_order x = x reshape x_order x_order x_order clone x = x reshape x_order permute order order = w_order = w_args i i inverse_order w = w reshape w_order w_order w_order clone w = w reshape w_order permute order y = x w y counters clear u = torch randn bs Mdim Kdim dtype=dtype v = torch randn bs Kdim Ndim dtype=dtype mod = M dtype=dtype eval verify dtype atol rtol common mod u v atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter order == patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize bs parametrize Mdim parametrize Kdim dtypes torch float torch float torch bfloat test_bmm_self_permute bs Mdim Kdim dtype M torch nn Module __init__ super __init__ forward x x x permute counters clear u = torch randn bs Mdim Kdim dtype=dtype mod = M dtype=dtype eval verify dtype atol rtol common mod u atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize bs parametrize Mdim Test small Mdim which uses reshaped weights dtypes torch float test_bmm_self_square bs Mdim dtype M torch nn Module __init__ super __init__ forward x x x counters clear u = torch randn bs Mdim Mdim dtype=dtype mod = M dtype=dtype eval verify dtype atol rtol common mod u atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize bs parametrize Mdim parametrize Kdim parametrize Ndim parametrize epilogue relu add sub mul div dtypes torch float torch bfloat torch half test_bmm_with_pointwise bs Mdim Kdim Ndim epilogue dtype M torch nn Module __init__ epilogue other super __init__ epilogue = _get_epilogue epilogue other forward x w epilogue x w counters clear x = torch randn bs Mdim Kdim dtype=dtype w = torch randn bs Kdim Ndim dtype=dtype other = torch randn bs Mdim Ndim dtype=dtype mod = M epilogue other dtype=dtype eval verify dtype atol rtol common mod x w atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter assertEqual counters inductor cpp_epilogue_fusion_counter patches torch no_grad unittest skipIf TEST_MKL Test requires MKL dtypes torch float torch bfloat torch half test_bmm_with_fused_epilogues dtype M torch nn Module __init__ super __init__ mul = torch randn as_strided forward x w x = torch ops aten reshape default x w = torch ops aten reshape default w bmm = torch ops aten bmm default x w bmm = torch ops aten reshape default bmm constant_pad_nd = torch ops aten constant_pad_nd default mul mul_ = torch ops aten mul Tensor bmm add = torch ops aten add Tensor mul_ constant_pad_nd add counters clear x = torch randn dtype=dtype w = torch randn dtype=dtype mod = M dtype=dtype eval verify dtype atol rtol common mod x w atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter assertEqual counters inductor cpp_epilogue_fusion_counter patches torch no_grad parametrize bs parametrize Mdim parametrize Kdim parametrize Ndim dtypes torch float torch bfloat torch half test_bmm_with_y_storage_offset dtype bs Mdim Kdim Ndim M torch nn Module __init__ super __init__ forward x y y_with_offset contiguous has non-zero storage offset y_with_offset = torch empty y shape dtype=y dtype device=y device copy_ y x y_with_offset counters clear u = torch randn bs Mdim Kdim dtype=dtype v = torch randn bs Kdim Ndim dtype=dtype mod = M dtype=dtype eval verify dtype atol rtol common mod u v atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter patches torch no_grad dtypes torch float test_aoti_bmm_unique_identifiers dtype try try test_aot_inductor_utils except ImportError test_aot_inductor_utils except Exception skip UT failed M torch nn Module __init__ super __init__ forward x w y = x w y w counters clear x = torch randn dtype=dtype w = torch randn dtype=dtype mod = M dtype=dtype eval verify dtype atol rtol torch no_grad expected = mod x w actual = test_aot_inductor_utils AOTIRunnerUtil run mod x w assertEqual actual expected atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter patches torch no_grad unittest skipIf TEST_MKL Test requires MKL set_num_threads avoid k_slicing make test deterministic parametrize out_features dtypes torch float test_local_and_global_accumulator out_features dtype batch_size = in_features = out_features = in_features = bias = True try try test_aot_inductor_utils except ImportError test_aot_inductor_utils except Exception skip UT failed M torch nn Module __init__ super __init__ linear = torch nn Linear in_features out_features bias linear = torch nn Linear in_features out_features bias forward x y = linear x view = torch ops aten view default y - in_features linear view counters clear x = torch randn batch_size in_features dtype=dtype mod = M dtype=dtype eval verify dtype atol rtol torch no_grad expected = mod x actual = test_aot_inductor_utils AOTIRunnerUtil run mod x assertEqual actual expected atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter patches inductor_config patch freezing=True unittest skipIf torch _C _has_mkldnn MKLDNN enabled test_bmm_flexible_layout M torch nn Module __init__ - None super __init__ forward u v view_ = torch ops aten reshape default u - clone_ = torch ops aten clone default v memory_format=torch contiguous_format view_ = torch ops aten reshape default clone_ - permute_ = torch ops aten permute default view_ div = torch ops aten div Tensor permute_ view_ ReinterpretView div FlexibleLayout which will become FixedLayout bmm = torch ops aten bmm default view_ div bmm mod = M eval u = torch randn v = torch randn verify u dtype atol rtol common mod u v unittest skipIf torch _C _cpu _is_amx_tile_supported AMX ISA support required inductor_config patch freezing True patches torch no_grad parametrize batch_size parametrize in_features parametrize out_features dtypes torch bfloat test_linear_reuse_kernels batch_size in_features out_features dtype M torch nn Module __init__ super __init__ linear_x = torch nn Linear in_features out_features linear_y = torch nn Linear out_features in_features linear_z = torch nn Linear in_features out_features forward x out = linear_x x out = linear_y out out = linear_z out out x = torch randn batch_size in_features dtype=dtype mod = M dtype=dtype eval verify dtype atol rtol ref_res = mod x m = torch compile mod res code = run_and_get_cpp_code m x assertEqual res ref_res atol=atol rtol=rtol equal_nan=True exact_dtype=True Check only kernels generated code assert code count AMXState amx_state == dynamo_config patch dynamic_shapes True assume_static_by_default False _DynamicShapesTestBase BaseTestSelectAlgorithm pass TestSelectAlgorithmDynamicShapes _DynamicShapesTestBase common = check_model test_linear_dynamic_shapes = TestSelectAlgorithm test_linear_static_shapes test_linear_with_pointwise_dynamic_shapes = TestSelectAlgorithm test_linear_with_pointwise test_linear_with_transpose_dynamic_shapes = TestSelectAlgorithm test_linear_with_transpose test_linear_with_unary_binary_dynamic_shapes = TestSelectAlgorithm test_linear_with_unary_binary test_linear_amx_dynamic_shapes = TestSelectAlgorithm test_linear_amx test_linear_with_embedding_dynamic_shapes = TestSelectAlgorithm test_linear_with_embedding test_quantized_linear_with_pointwise_dynamic_shapes = TestSelectAlgorithm test_quantized_linear_with_pointwise test_quantized_linear_with_pointwise_binary_dynamic_shapes = TestSelectAlgorithm test_quantized_linear_with_pointwise_binary test_quantized_linear_amx_dynamic_shapes = TestSelectAlgorithm test_quantized_linear_amx test_grouped_linear_dynamic_shapes = TestSelectAlgorithm test_grouped_linear test_grouped_linear_epilogue_dynamic_shapes = TestSelectAlgorithm test_grouped_linear_epilogue test_linear_k_slicing_dynamic_shapes = TestSelectAlgorithm test_linear_k_slicing test_linear_cache_blocking_dynamic_shapes = TestSelectAlgorithm test_linear_cache_blocking test_linear_thread_factors_dynamic_shapes = TestSelectAlgorithm test_linear_thread_factors patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize bs parametrize Mdim parametrize Kdim parametrize Ndim dtypes torch float torch bfloat torch half test_bmm_with_pointwise_dynamic_shapes bs Mdim Kdim Ndim dtype M torch nn Module __init__ super __init__ epilogue = torch nn ReLU forward x other epilogue x other counters clear u = torch randn bs Mdim Kdim dtype=dtype v = torch randn bs Kdim Ndim dtype=dtype torch _dynamo mark_dynamic u torch _dynamo mark_dynamic u torch _dynamo mark_static u torch _dynamo mark_static v mod = M dtype=dtype eval verify dtype atol rtol common mod u v atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter assertEqual counters inductor cpp_epilogue_fusion_counter patches torch no_grad unittest skipIf TEST_MKL Test requires MKL parametrize bs parametrize Mdim parametrize Kdim parametrize Ndim dtypes torch float torch bfloat torch half test_bmm_with_pointwise_with_reshape_dynamic_shapes bs Mdim Kdim Ndim dtype M torch nn Module __init__ super __init__ epilogue = torch nn ReLU forward x other noise result = x reshape - Mdim Kdim other reshape - Kdim Ndim epilogue result + noise counters clear u = torch randn bs Mdim Kdim dtype=dtype v = torch randn bs Kdim Ndim dtype=dtype noise = torch randn bs Mdim Ndim dtype=dtype torch _dynamo mark_dynamic u torch _dynamo mark_dynamic u torch _dynamo mark_static u torch _dynamo mark_static u torch _dynamo mark_static v torch _dynamo mark_static v mod = M dtype=dtype eval verify dtype atol rtol common mod u v noise atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter assertEqual counters inductor cpp_epilogue_fusion_counter patches torch no_grad unittest skipIf TEST_MKL Test requires MKL dtypes torch float torch bfloat test_bmm_epilogue_dynamic_reshape dtype bs = M torch nn Module __init__ super __init__ epilogue = torch nn ReLU forward x w arg _ arg _ = x shape mul_ = arg _ view_ = torch ops aten reshape default x mul_ view_ = torch ops aten reshape default w mul_ bmm_ = torch ops aten bmm default view_ view_ view_ = torch ops aten reshape default bmm_ arg _ abs_ = torch ones dtype=torch int lt_ = torch ops aten lt Scalar abs_ add_ = torch ones dtype=torch int add_ = torch ones dtype=torch int full_default_ = torch ops aten full default dtype=torch int layout=torch strided minimum_ = torch ops aten minimum default add_ full_default_ where_ = torch ops aten where lt_ abs_ minimum_ add_ = torch ops aten add Tensor add_ where_ embedding_ = torch ops aten embedding default arg _ add_ permute_ = torch ops aten permute default embedding_ unsqueeze_ = torch ops aten unsqueeze default permute_ full_default = torch ops aten full default arg _ - dtype=torch float layout=torch strided add_ = torch ops aten add Tensor unsqueeze_ full_default add_ = torch ops aten add Tensor view_ add_ add_ counters clear u = torch randn bs dtype=dtype v = torch randn bs dtype=dtype arg = torch randn torch _dynamo mark_dynamic u torch _dynamo mark_static u torch _dynamo mark_static u torch _dynamo mark_static u torch _dynamo mark_static v torch _dynamo mark_static v mod = M dtype=dtype eval verify dtype atol rtol common mod u v arg atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter assertEqual counters inductor cpp_epilogue_fusion_counter patches torch no_grad unittest skipIf TEST_MKL Test requires MKL test_bmm_dynamic_bm_stride bs = Mdim = Kdim = dtype = torch float M torch nn Module __init__ super __init__ forward x weight x weight permute counters clear u = torch randn bs Mdim Kdim dtype=dtype v = torch randn Kdim Mdim bs dtype=dtype torch _dynamo mark_dynamic u torch _dynamo mark_dynamic u torch _dynamo mark_static u torch _dynamo mark_static v torch _dynamo mark_static v mod = M dtype=dtype eval verify dtype atol rtol common mod u v atol=atol rtol=rtol assertEqual counters inductor cpp_templated_kernel_counter instantiate_device_type_tests TestSelectAlgorithm globals only_for= cpu instantiate_device_type_tests TestSelectAlgorithmDynamicShapes globals only_for= cpu __name__ == __main__ torch testing _internal inductor_utils HAS_CPU HAS_CPU IS_MACOS IS_WINDOWS run_tests