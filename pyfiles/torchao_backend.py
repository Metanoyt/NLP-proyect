collections abc Callable typing Any torch setup_baseline torchao quantization utils recommended_inductor_config_setter recommended_inductor_config_setter torch _dynamo config automatic_dynamic_shapes = False torch _dynamo config recompile_limit = torchao_optimize_ctx quantization str torchao quantization quant_api autoquant int _weight_only int _dynamic_activation_int _weight int _weight_only quantize_ torchao utils unwrap_tensor_subclass inner model_iter_fn Callable _torchao_apply module torch nn Module example_inputs Any getattr module _quantized None None quantization == int dynamic quantize_ module int _dynamic_activation_int _weight set_inductor_config=False quantization == int weightonly quantize_ module int _weight_only set_inductor_config=False quantization == int weightonly quantize_ module int _weight_only set_inductor_config=False quantization == autoquant autoquant module error_on_unseen=False set_inductor_config=False isinstance example_inputs dict module example_inputs module example_inputs torchao quantization autoquant AUTOQUANT_CACHE len AUTOQUANT_CACHE == raise Exception noqa TRY NotAutoquantizable f Found no autoquantizable layers model type module stopping autoquantized run unwrap_tensor_subclass module setattr module _quantized True noqa B model_iter_fn module example_inputs _torchao_apply inner