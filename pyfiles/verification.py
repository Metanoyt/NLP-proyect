mypy allow-untyped-defs The ONNX verification module provides set tools verify correctness ONNX models __future__ annotations __all__ = OnnxBackend VerificationOptions verify contextlib copy dataclasses enum io os tempfile warnings collections abc Mapping Sequence typing Any Union numpy np numpy typing npt torch torch _C _onnx _C_onnx torch onnx _internal torchscript_exporter utils torch types Number Everything below deprecated ############################################## _ORT_PROVIDERS = CPUExecutionProvider _NumericType = Union Number torch Tensor np ndarray _ModelType = Union torch nn Module torch jit ScriptModule _InputArgsType = Union torch Tensor tuple Any _InputKwargsType = Mapping str Any _OutputsType = Union Sequence _NumericType Sequence OnnxBackend enum Enum Enum ONNX backend used export verification deprecated Consider using ` ` torch onnx export dynamo=True ` ` use returned ` ` ONNXProgram ` ` test ONNX model REFERENCE = ONNXReferenceEvaluator ONNX_RUNTIME_CPU = CPUExecutionProvider ONNX_RUNTIME_CUDA = CUDAExecutionProvider dataclasses dataclass VerificationOptions Options ONNX export verification deprecated Consider using ` ` torch onnx export dynamo=True ` ` use returned ` ` ONNXProgram ` ` test ONNX model Attributes flatten If True unpack nested list tuple dict inputs into flattened list Tensors ONNX Set False nested structures preserved ONNX which usually case exporting ScriptModules Default True ignore_none Whether ignore None type torch output which usually case tracing Set False torch output should keep None type which usually case exporting ScriptModules Default True check_shape Whether check shapes between PyTorch ONNX Runtime outputs exactly same Set False allow output shape broadcasting Default True check_dtype Whether check dtypes between PyTorch ONNX Runtime outputs consistent Default True backend ONNX backend verification Default OnnxBackend ONNX_RUNTIME_CPU rtol relative tolerance comparison between ONNX PyTorch outputs atol absolute tolerance comparison between ONNX PyTorch outputs remained_onnx_input_idx If provided only specified inputs will passed ONNX model Supply list when there unused inputs model Since unused inputs will removed exported ONNX model supplying all inputs will cause error unexpected inputs This parameter tells verifier which inputs pass into ONNX model acceptable_error_percentage acceptable percentage element mismatches comparison It should float value between flatten bool = True ignore_none bool = True check_shape bool = True check_dtype bool = True backend OnnxBackend = OnnxBackend ONNX_RUNTIME_CPU rtol float = e- atol float = e- remained_onnx_input_idx Sequence int &#124; None = None acceptable_error_percentage float &#124; None = None _flatten_tuples elem flattened = t elem isinstance t tuple flattened extend _flatten_tuples t flattened append t flattened _to_numpy elem - list &#124; npt NDArray isinstance elem torch Tensor elem requires_grad elem detach cpu numpy elem cpu numpy isinstance elem list tuple _to_numpy inp inp elem isinstance elem bool int float np array elem isinstance elem dict flattened = k elem flattened extend _to_numpy k _to_numpy elem k flattened elem _inline_flatten_list inputs res_list - list i inputs res_list append i isinstance i list tuple _inline_flatten_list i res_list res_list _unpack_to_numpy values cast_onnx_accepted=True - list value_unpacked = value values value_unpacked extend utils unpack_quantized_tensor value cast_onnx_accepted=cast_onnx_accepted _to_numpy v v value_unpacked _run_onnx onnx_session inputs - _OutputsType kw_inputs = inputs isinstance inputs - dict kw_inputs = inputs - inputs = inputs - inputs = _unpack_to_numpy _flatten_tuples inputs ort_inputs = input_name input kw_inputs items ort_inputs input_name = _to_numpy input inputs = _to_numpy inputs hasattr onnx_session get_inputs onnxruntime InferenceSession input_names = i name i onnx_session get_inputs hasattr onnx_session input_names onnx reference ReferenceEvaluator input_names = onnx_session input_names raise ValueError f Unknown ONNX backend type type onnx_session i input enumerate inputs i == len input_names input_names i ort_inputs raise ValueError f got too many positional inputs inputs inputs kw_inputs kw_inputs f input names input_names ort_inputs input_names i = input onnx_outs = onnx_session run None ort_inputs onnx_outs _ort_session model str &#124; io BytesIO ort_providers Sequence str = _ORT_PROVIDERS try onnxruntime type ignore except ImportError e raise ImportError onnxruntime required export verification e ort_providers None ort_providers = _ORT_PROVIDERS session_options = onnxruntime SessionOptions suppress ort warnings Verbose Info Warning Error Fatal Default session_options log_severity_level = ort_session = onnxruntime InferenceSession model isinstance model str model getvalue session_options providers=ort_providers ort_session _onnx_backend_session model str &#124; io BytesIO backend OnnxBackend backend == OnnxBackend REFERENCE raise NotImplementedError backend OnnxBackend ONNX_RUNTIME_CPU OnnxBackend ONNX_RUNTIME_CUDA onnx_session = _ort_session model backend value raise ValueError f Unsupported backend backend onnx_session _compare_onnx_pytorch_outputs_in_np onnx_outs _OutputsType pt_outs _OutputsType options VerificationOptions assert len onnx_outs == len pt_outs f Number outputs differ ONNX runtime len onnx_outs PyTorch len pt_outs acceptable_error_percentage = options acceptable_error_percentage acceptable_error_percentage acceptable_error_percentage acceptable_error_percentage raise ValueError If set acceptable_error_percentage should between ort_out pt_out zip onnx_outs pt_outs try TODO Remove ` check_shape ` option once every shape inconsistent issue addressed options check_shape Allow different broadcastable output shapes ort_out pt_out = np broadcast_arrays ort_out pt_out torch testing assert_close ort_out pt_out rtol=options rtol atol=options atol check_dtype=options check_dtype equal_nan=True except AssertionError e acceptable_error_percentage error_percentage = - np sum np isclose ort_out pt_out rtol=options rtol atol=options atol np prod ort_out shape pyrefly ignore missing-attribute error_percentage = acceptable_error_percentage warnings warn f Suppressed AssertionError \n e \n f Error percentage error_percentage f within acceptable range acceptable_error_percentage stacklevel= continue pyrefly ignore missing-attribute ort_out dtype == np uint ort_out dtype == np int warnings warn ONNX output quantized stacklevel= pyrefly ignore missing-attribute pt_out dtype == np uint pt_out dtype == np int warnings warn PyTorch output quantized stacklevel= raise _compare_onnx_pytorch_outputs onnx_outs _OutputsType pt_outs Any options VerificationOptions Compare ONNX PyTorch outputs Args onnx_outs outputs ONNX backend pt_outs outputs PyTorch options options verification Raises AssertionError outputs ONNX model PyTorch model equal up specified precision ValueError arguments provided invalid options ignore_none torch jit _flatten filters None type pt_outs _ = torch jit _flatten pt_outs pt_outs = _inline_flatten_list pt_outs pt_outs_np = _unpack_to_numpy pt_outs cast_onnx_accepted=False onnx_outs = _inline_flatten_list onnx_outs _compare_onnx_pytorch_outputs_in_np onnx_outs pt_outs_np options _prepare_input_for_pytorch args kwargs Prepare input PyTorch model execution Any future changes formatting input before dispatching PyTorch model should made function Args args positional arguments PyTorch model forward method kwargs keyword arguments PyTorch model forward method Returns args positional arguments PyTorch model forward method kwargs keyword arguments PyTorch model forward method isinstance args torch Tensor dict args = args In-place operators will update input tensor data well Thus inputs replicated before every forward call args = copy deepcopy args kwargs kwargs = copy deepcopy kwargs kwargs = args kwargs _prepare_input_for_export args kwargs Prepare input ONNX model export Any future changes formatting input before dispatching func ` torch onnx export ` api should made function Args args positional arguments PyTorch model forward method kwargs keyword arguments PyTorch model forward method Returns onnx_inputs positional arguments ONNX model export ` args ` func ` torch onnx export ` args kwargs = _prepare_input_for_pytorch args kwargs kwargs len args isinstance args - dict onnx_inputs = args + kwargs onnx_inputs = args + kwargs onnx_inputs = args onnx_inputs _prepare_input_for_onnx args kwargs remained_onnx_input_idx Sequence int &#124; None flatten bool Prepare input ONNX model execution ONNX backend Any future changes formatting input before dispatching ONNX backend run should made function Args args positional arguments PyTorch model forward method kwargs keyword arguments PyTorch model forward method remained_onnx_input_idx indices inputs used ONNX model execution flatten whether flatten input before dispatching ONNX model execution Returns onnx_inputs positional arguments ONNX model execution ONNX backend onnx_inputs = _prepare_input_for_export args kwargs flatten onnx_inputs _ = torch jit _flatten onnx_inputs onnx_inputs onnx_inputs - == Handle empty kwargs normally removed flatten onnx_inputs = onnx_inputs - remained_onnx_input_idx None onnx_inputs i i remained_onnx_input_idx onnx_inputs _try_clone_model model Used preserving original model case forward mutates model states try copy deepcopy model except Exception warnings warn Failed clone model Model state might mutated during verification stacklevel= model _compare_onnx_pytorch_model pt_model _ModelType onnx_model_f str &#124; io BytesIO input_args _InputArgsType input_kwargs _InputKwargsType &#124; None additional_test_inputs Sequence _InputArgsType &#124; None options VerificationOptions Compare outputs ONNX model runs outputs PyTorch model runs Args pt_model PyTorch model onnx_model_f ONNX model file path file-like object input_args positional arguments PyTorch model forward method input_kwargs keyword arguments PyTorch model forward method additional_test_inputs additional positional arguments PyTorch model forward method options options verification Raises AssertionError outputs ONNX model PyTorch model equal up specified precision onnx_session = _onnx_backend_session onnx_model_f options backend compare_onnx_pytorch_model_with_input input_args input_kwargs pt_args pt_kwargs = _prepare_input_for_pytorch input_args input_kwargs TODO remove treat mutating model separately See pt_model_copy = _try_clone_model pt_model pt_outs = pt_model_copy pt_args pt_kwargs onnx_inputs = _prepare_input_for_onnx input_args input_kwargs options remained_onnx_input_idx options flatten onnx_outs = _run_onnx onnx_session onnx_inputs _compare_onnx_pytorch_outputs onnx_outs=onnx_outs pt_outs=pt_outs options=options compare_onnx_pytorch_model_with_input input_args input_kwargs additional_test_inputs test_input_args additional_test_inputs compare_onnx_pytorch_model_with_input test_input_args verify model _ModelType input_args _InputArgsType input_kwargs _InputKwargsType &#124; None = None do_constant_folding bool = True dynamic_axes Mapping str Mapping int str &#124; Mapping str Sequence int &#124; None = None input_names Sequence str &#124; None = None output_names Sequence str &#124; None = None training _C_onnx TrainingMode = _C_onnx TrainingMode EVAL opset_version int &#124; None = None keep_initializers_as_inputs bool = True verbose bool = False fixed_batch_size bool = False use_external_data bool = False additional_test_inputs Sequence _InputArgsType &#124; None = None options VerificationOptions &#124; None = None Verify model export ONNX against original PyTorch model deprecated Consider using ` ` torch onnx export dynamo=True ` ` use returned ` ` ONNXProgram ` ` test ONNX model Args model See func ` torch onnx export ` input_args See func ` torch onnx export ` input_kwargs See func ` torch onnx export ` do_constant_folding See func ` torch onnx export ` dynamic_axes See func ` torch onnx export ` input_names See func ` torch onnx export ` output_names See func ` torch onnx export ` training See func ` torch onnx export ` opset_version See func ` torch onnx export ` keep_initializers_as_inputs See func ` torch onnx export ` verbose See func ` torch onnx export ` fixed_batch_size Legacy argument used only rnn test cases use_external_data Explicitly specify whether export model external data additional_test_inputs List tuples Each tuple group input arguments test Currently only ` ` args ` ` supported options A VerificationOptions object controls verification behavior Raises AssertionError outputs ONNX model PyTorch model equal up specified precision ValueError arguments provided invalid options None options = VerificationOptions training == torch onnx TrainingMode TRAINING model train training == torch onnx TrainingMode EVAL model eval torch no_grad contextlib ExitStack stack model_f str &#124; io BytesIO = io BytesIO use_external_data tmpdir_path = stack enter_context tempfile TemporaryDirectory model_f = os path join tmpdir_path model onnx inputs_for_export = _prepare_input_for_export input_args input_kwargs TODO remove treat mutating model separately model_copy = _try_clone_model model utils _export model inputs_for_export model_f opset_version=opset_version do_constant_folding=do_constant_folding keep_initializers_as_inputs=keep_initializers_as_inputs dynamic_axes=dynamic_axes input_names=input_names output_names=output_names fixed_batch_size=fixed_batch_size training=training verbose=verbose _compare_onnx_pytorch_model pt_model=model_copy onnx_model_f=model_f input_args=input_args input_kwargs=input_kwargs additional_test_inputs=additional_test_inputs options=options