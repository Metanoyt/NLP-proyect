Owner s module intel gc re subprocess sys tempfile time unittest torch torch xpu _gpu_trace gpu_trace torch testing make_tensor torch testing _internal autocast_test_lists AutocastTestLists TestAutocast torch testing _internal common_device_type instantiate_device_type_tests onlyXPU OpDTypes ops skipXPUIf torch testing _internal common_methods_invocations ops_and_refs torch testing _internal common_utils find_library_location IS_LINUX IS_WINDOWS run_tests serialTest suppress_warnings TEST_XPU TestCase torch utils checkpoint checkpoint_sequential TEST_MULTIXPU = torch xpu device_count cpu_device = torch device cpu xpu_device = torch device xpu any_common_cpu_xpu_one = OpDTypes any_common_cpu_cuda_one _xpu_computation_op_list = fill zeros zeros_like clone view_as_real view_as_complex view resize_ resize_as_ add sub mul div abs _xpu_tensor_factory_op_list = as_strided empty empty_strided _xpu_not_test_dtype_op_list = resize_ Skipped CPU resize_as_ Skipped CPU abs Not aligned dtype _xpu_all_op_list = _xpu_computation_op_list + _xpu_tensor_factory_op_list _xpu_all_ops = op op ops_and_refs op name _xpu_all_op_list _xpu_computation_ops = op op ops_and_refs op name _xpu_computation_op_list unittest skipIf TEST_XPU XPU available skipping tests TestXpu TestCase test_device_behavior current_device = torch xpu current_device torch xpu set_device current_device assertEqual current_device torch xpu current_device unittest skipIf TEST_MULTIXPU only one GPU detected test_multi_device_behavior current_device = torch xpu current_device target_device = current_device + torch xpu device_count torch xpu device target_device assertEqual target_device torch xpu current_device assertEqual current_device torch xpu current_device torch xpu _DeviceGuard target_device assertEqual target_device torch xpu current_device assertEqual current_device torch xpu current_device test_get_device_properties current_device = torch xpu current_device device_properties = torch xpu get_device_properties current_device assertEqual device_properties torch xpu get_device_properties None assertEqual device_properties torch xpu get_device_properties device_name = torch xpu get_device_name current_device assertEqual device_name torch xpu get_device_name None assertEqual device_name torch xpu get_device_name device_capability = torch xpu get_device_capability current_device assertTrue device_capability device_id assertTrue device_capability max_work_group_size assertTrue device_capability max_num_sub_groups assertEqual device_properties driver_version device_capability driver_version assertEqual device_properties has_fp device_capability has_fp assertEqual device_properties has_fp device_capability has_fp assertEqual device_properties has_atomic device_capability has_atomic assertEqual device_properties has_bfloat _conversions device_capability has_bfloat _conversions assertEqual device_properties has_subgroup_matrix_multiply_accumulate device_capability has_subgroup_matrix_multiply_accumulate assertEqual device_properties has_subgroup_matrix_multiply_accumulate_tensor_float device_capability has_subgroup_matrix_multiply_accumulate_tensor_float assertEqual device_properties has_subgroup_ d_block_io device_capability has_subgroup_ d_block_io int torch version xpu = assertEqual device_properties architecture device_capability architecture assertEqual len str device_properties uuid xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx assertEqual len device_properties uuid bytes unittest skipIf IS_WINDOWS applicable Windows only fails fork test_wrong_xpu_fork stderr = TestCase runWithPytorchAPIUsageStderr \ torch torch multiprocessing Process run rank torch xpu set_device rank __name__ == __main__ size = processes = rank range size would work fine without line below torch xpu set_device p = Process target=run args= rank p start processes append p p processes p join assertRegex stderr Cannot re-initialize XPU forked subprocess unittest skipIf IS_WINDOWS Only lazy initialization Linux applicable Windows test_lazy_init Validate no XPU calls made during ` torch ` call check_output script str - str subprocess check_output sys executable -c script decode ascii strip test_script = \ torch torch multiprocessing Process copy run_model model input input_xpu = input clone xpu model_xpu = copy deepcopy model xpu loss_xpu = model_xpu input_xpu sum loss = model input sum torch testing assert_close loss_xpu cpu loss test_multi_process model input p = Process target=run_model args= model input p start p join assert p exitcode == input = torch rand model = torch nn Sequential torch nn Conv d stride= torch nn ReLU torch nn MaxPool d __name__ == __main__ test_multi_process model input test_multi_process model input print torch xpu device_count rc = check_output test_script assertEqual rc str torch xpu device_count test_streams s = torch xpu Stream torch xpu set_stream s s = torch xpu current_stream assertEqual s s s = torch xpu Stream assertFalse s == s torch xpu set_stream s torch xpu stream s assertEqual s torch xpu current_stream assertEqual s torch xpu current_stream test_stream_priority low high = torch xpu Stream priority_range s = torch xpu Stream device= priority=low assertEqual low s priority assertEqual torch device xpu s device s = torch xpu Stream device= priority=high assertEqual high s priority assertEqual torch device xpu s device test_stream_event_repr s = torch xpu current_stream assertTrue torch xpu Stream str s e = torch xpu Event assertTrue torch xpu Event uninitialized str e s record_event e assertTrue torch xpu Event str e test_events stream = torch xpu current_stream event = torch xpu Event assertTrue event query stream record_event event event synchronize assertTrue event query start_event = torch xpu Event enable_timing=True end_event = torch xpu Event enable_timing=True stream record_event start_event time sleep stream record_event end_event torch xpu synchronize int torch version xpu = assertGreater start_event elapsed_time end_event assertRaisesRegex NotImplementedError elapsed_time XPUEvent requires PyTorch built SYCL compiler version newer start_event elapsed_time end_event event = torch xpu Event enable_timing=True assertEqual event sycl_event assertEqual event event_id event record assertNotEqual event sycl_event assertNotEqual event event_id assertEqual event sycl_event event event_id test_generic_stream_event stream = torch Stream xpu assertEqual stream device_index torch xpu current_device xpu_stream = torch xpu Stream stream_id=stream stream_id device_index=stream device_index device_type=stream device_type assertIsInstance xpu_stream torch Stream assertTrue issubclass type xpu_stream torch Stream assertTrue torch Stream type xpu_stream mro assertEqual stream stream_id xpu_stream stream_id assertNotEqual stream stream_id torch xpu current_stream stream_id event = torch Event xpu enable_timing=True event = torch Event xpu enable_timing=True assertEqual event event_id = torch randn b = torch randn torch xpu stream xpu_stream a_xpu = xpu non_blocking=True b_xpu = b xpu non_blocking=True assertEqual stream stream_id torch xpu current_stream stream_id event record stream event synchronize assertTrue event query c_xpu = a_xpu + b_xpu Here intendionly records another stream event record event synchronize assertTrue event query assertNotEqual event event_id event event_id assertEqual c_xpu cpu + b int torch version xpu = assertGreater event elapsed_time event assertRaisesRegex NotImplementedError elapsedTime requires PyTorch built SYCL compiler version newer event elapsed_time event xpu_event = torch xpu Event assertIsInstance xpu_event torch Event assertTrue issubclass type xpu_event torch Event assertTrue torch Event type xpu_event mro test_stream_compatibility s = torch xpu Stream s = torch xpu Stream torch accelerator set_stream s assertEqual torch accelerator current_stream stream_id s stream_id torch accelerator set_stream s assertEqual torch accelerator current_stream stream_id s stream_id assertRaisesRegex RuntimeError The device index out range torch accelerator current_stream torch accelerator device_count test_device_context_manager prev_device = torch xpu current_device torch accelerator device_index None assertEqual torch xpu current_device prev_device assertEqual torch xpu current_device prev_device torch accelerator device_index assertEqual torch xpu current_device assertEqual torch xpu current_device prev_device unittest skipIf TEST_MULTIXPU only one GPU detected test_multi_device_context_manager src_device = dst_device = torch xpu set_device src_device torch accelerator device_index dst_device assertEqual torch xpu current_device assertEqual torch xpu current_device src_device test_stream_context_manager prev_stream = torch xpu current_stream torch xpu Stream stream assertEqual stream torch xpu current_stream assertEqual prev_stream torch xpu current_stream unittest skipIf TEST_MULTIXPU only one GPU detected test_multi_device_stream_context_manager src_device = dst_device = torch xpu set_device src_device src_prev_stream = torch xpu current_stream src_device dst_prev_stream = torch xpu current_stream dst_device torch xpu Stream dst_device dst_stream assertEqual dst_device torch xpu current_device assertEqual dst_stream torch xpu current_stream assertEqual src_prev_stream torch xpu current_stream src_device assertEqual src_device torch xpu current_device assertEqual src_prev_stream torch xpu current_stream assertEqual dst_prev_stream torch xpu current_stream dst_device test_generator torch manual_seed g_state = torch xpu get_rng_state torch manual_seed g_state = torch xpu get_rng_state assertNotEqual g_state g_state torch xpu manual_seed g_state = torch xpu get_rng_state assertEqual g_state g_state torch xpu set_rng_state g_state assertEqual g_state torch xpu get_rng_state torch manual_seed torch xpu set_rng_state g_state assertEqual torch xpu initial_seed onlyXPU suppress_warnings ops _xpu_computation_ops dtypes=any_common_cpu_xpu_one test_compare_cpu device dtype op to_cpu arg isinstance arg torch Tensor arg device= cpu arg samples = op reference_inputs device dtype sample samples cpu_sample = sample transform to_cpu xpu_results = op sample input sample args sample kwargs cpu_results = op cpu_sample input cpu_sample args cpu_sample kwargs xpu_results = sample output_process_fn_grad xpu_results cpu_results = cpu_sample output_process_fn_grad cpu_results Lower tolerance because we running ` slowTest ` Don t want periodic tests fail frequently assertEqual xpu_results cpu_results atol= e- rtol= e- onlyXPU ops _xpu_computation_ops allowed_dtypes= torch bool test_non_standard_bool_values device dtype op Test boolean values other than x x gh- convert_boolean_tensors x isinstance x torch Tensor x dtype = torch bool x Map False - True - Random value true_vals = torch randint x shape dtype=torch uint device=x device false_vals = torch zeros dtype=torch uint device=x device x_int = torch where x true_vals false_vals ret = x_int view torch bool assertEqual ret x ret sample op sample_inputs device dtype expect = op sample input sample args sample kwargs transformed = sample transform convert_boolean_tensors actual = op transformed input transformed args transformed kwargs assertEqual expect actual test_serialization_array_with_storage x = torch randn xpu y = torch zeros dtype=torch int device= xpu q = x y x y storage tempfile NamedTemporaryFile f torch save q f f seek q_copy = torch load f assertEqual q_copy q atol= rtol= q_copy fill_ assertEqual q_copy q_copy atol= rtol= assertEqual q_copy dtype torch float assertEqual q_copy dtype torch int assertEqual q_copy dtype torch float assertTrue isinstance q_copy torch storage TypedStorage assertTrue isinstance q_copy _untyped_storage torch UntypedStorage q_copy fill_ y fill_ assertEqual q_copy y storage test_serialization_array_with_empty x = torch randn xpu torch tensor dtype=torch float device=torch device xpu tempfile NamedTemporaryFile f torch save x f f seek x_copy = torch load f original copy zip x x_copy assertEqual copy original assertIs type copy type original assertEqual copy get_device original get_device test_out_of_memory tensor = torch zeros device= xpu noqa F assertRaisesRegex RuntimeError Tried allocate GiB torch empty dtype=torch int device= xpu assertRaisesRegex RuntimeError XPU out memory torch empty dtype=torch int device= xpu test_raises_oom torch xpu memory empty_cache assertRaises torch OutOfMemoryError torch empty device= xpu serialTest test_set_per_process_memory_fraction gc collect torch xpu empty_cache total_memory = torch xpu get_device_properties total_memory fraction = orig_fraction = torch xpu get_per_process_memory_fraction assertRaisesRegex ValueError invalid fraction torch xpu set_per_process_memory_fraction - assertRaisesRegex ValueError invalid fraction torch xpu set_per_process_memory_fraction torch xpu set_per_process_memory_fraction fraction allowed_memory = int total_memory reserved_memory = torch xpu memory_reserved application_memory = allowed_memory - reserved_memory tensor = torch empty application_memory dtype=torch int device= xpu del tensor gc collect torch xpu empty_cache assertEqual fraction torch xpu get_per_process_memory_fraction application_memory = int total_memory assertRaises torch OutOfMemoryError _ = torch empty application_memory dtype=torch int device= xpu torch xpu set_per_process_memory_fraction orig_fraction test_memory_allocation torch xpu empty_cache prev_allocated = torch xpu memory_allocated prev_reserved = torch xpu memory_reserved assertGreaterEqual prev_allocated assertGreaterEqual prev_reserved = torch ones device= xpu assertGreater torch xpu memory_allocated prev_allocated assertGreaterEqual torch xpu memory_reserved prev_reserved del assertEqual torch xpu memory_allocated prev_allocated torch xpu empty_cache assertLessEqual torch xpu memory_reserved prev_reserved torch xpu reset_accumulated_memory_stats Activate kB memory prev_active_current = torch xpu memory_stats active_bytes all current = torch randn device= xpu Detect current active memory kB assertEqual torch xpu memory_stats active_bytes all current + prev_active_current assertEqual torch xpu memory_stats active_bytes all freed del assertEqual torch xpu memory_stats active_bytes all current prev_active_current assertEqual torch xpu memory_stats active_bytes all freed unittest skipIf TEST_MULTIXPU only one GPU detected test_device_memory_allocated device_count = torch xpu device_count current_alloc = torch xpu memory_allocated idx idx range device_count = torch ones device= xpu assertGreater torch xpu memory_allocated current_alloc assertTrue all torch xpu memory_allocated idx == current_alloc idx idx range device_count del test_memory_stats gc collect torch xpu empty_cache torch xpu reset_peak_memory_stats torch xpu reset_accumulated_memory_stats prev_allocated = torch accelerator memory_allocated prev_reserved = torch accelerator memory_reserved prev_max_allocated = torch accelerator max_memory_allocated prev_max_reserved = torch accelerator max_memory_reserved assertEqual prev_allocated prev_max_allocated assertEqual prev_reserved prev_max_reserved Activate kB memory prev_active_current = torch accelerator memory_stats active_bytes all current tmp = torch randn device= xpu Detect current active memory kB assertEqual torch accelerator memory_stats active_bytes all current + prev_active_current assertEqual torch accelerator memory_stats active_bytes all freed del tmp gc collect torch accelerator empty_cache assertEqual torch accelerator memory_stats active_bytes all current prev_active_current assertEqual torch accelerator memory_stats active_bytes all freed torch accelerator reset_peak_memory_stats assertEqual torch accelerator max_memory_allocated prev_max_allocated assertEqual torch accelerator max_memory_reserved prev_max_reserved skipXPUIf int torch version xpu Test requires SYCL compiler version newer test_mem_get_info torch xpu synchronize torch xpu empty_cache before_free_bytes before_total_bytes = torch xpu mem_get_info increasing MB force acquiring new block torch randn device= xpu torch xpu synchronize after_free_bytes after_total_bytes = torch xpu mem_get_info assertGreaterEqual before_free_bytes after_free_bytes assertEqual before_total_bytes after_total_bytes test_get_arch_list arch_list = torch xpu get_arch_list arch_list flags = torch xpu get_gencode_flags arch arch_list assertTrue arch flags unittest skipIf TEST_MULTIXPU only one GPU detected test_can_device_access_peer device_count = torch xpu device_count device range device_count peer range device_count assertEqual torch xpu can_device_access_peer device peer torch xpu can_device_access_peer peer device test_torch_version_xpu assertEqual len torch version xpu compiler_version = int torch version xpu assertGreater compiler_version IS_LINUX library = find_library_location libtorch_xpu so cmd = f ldd library &#124; grep libsycl results = subprocess check_output cmd shell=True strip split b \n There should only one libsycl so assertEqual len results result results assertTrue b libsycl so result test_dlpack_conversion x = make_tensor dtype=torch float device= xpu IS_WINDOWS int torch version xpu assertRaisesRegex NotImplementedError Default context supported XPU default Windows SYCL compiler versions earlier than torch to_dlpack x z = torch from_dlpack torch to_dlpack x z = z + assertEqual z x instantiate_device_type_tests TestXpu globals only_for= xpu allow_xpu=True unittest skipIf TEST_XPU XPU available skipping tests TestXpuAutocast TestAutocast These operators implemented XPU backend we can NOT fall back them CPU So we have skip them moment TODO remove these operators skip list when they implemented XPU backend lstm_cell The operator aten _thnn_fused_lstm_cell currently implemented XPU device skip_list = gru_cell lstm_cell setUp super setUp autocast_lists = AutocastTestLists torch device xpu tearDown del autocast_lists super tearDown test_autocast_torch_fp op_with_args autocast_lists torch_fp skip_test = False op args = op_with_args op_with_args op skip_list skip_test = True skip unimplemented op len op_with_args == skip_test = True skip cudnn op skip_test _run_autocast_outofplace op args torch float device= xpu amp_dtype=torch float test_autocast_torch_bf op_with_args autocast_lists torch_fp skip_test = False op args = op_with_args op_with_args op skip_list skip_test = True skip unimplemented op len op_with_args == skip_test = True skip cudnn op skip_test _run_autocast_outofplace op args torch bfloat device= xpu test_autocast_torch_need_autocast_promote op args autocast_lists torch_need_autocast_promote _run_autocast_outofplace op args torch float device= xpu amp_dtype=torch float test_autocast_torch_expect_builtin_promote op args out_type autocast_lists torch_expect_builtin_promote _run_autocast_outofplace op args torch float device= xpu out_type=out_type amp_dtype=torch float test_autocast_checkpointing model = torch nn Sequential torch nn Linear torch nn Linear torch nn Linear xpu input = torch rand device= xpu dtype=torch float requires_grad=True reentrant True False torch autocast xpu output = checkpoint_sequential model input use_reentrant=reentrant assertTrue output requires_grad assertTrue output dtype torch float output sum backward test_xpu_autocast_dtype dtype = torch get_autocast_dtype xpu assertEqual dtype torch float mat _fp = torch randn dtype=torch float device= xpu mat _fp = torch randn dtype=torch float device= xpu torch amp autocast xpu result = torch mm mat _fp mat _fp assertEqual result dtype torch float unittest skipIf TEST_XPU XPU available skipping tests TestXpuTrace TestCase setUp torch _C _activate_gpu_trace mock = unittest mock MagicMock test_event_creation_callback gpu_trace register_callback_for_event_creation mock event = torch xpu Event event record mock assert_called_once_with event _as_parameter_ value test_event_deletion_callback gpu_trace register_callback_for_event_deletion mock event = torch xpu Event event record event_id = event _as_parameter_ value del event mock assert_called_once_with event_id test_event_record_callback gpu_trace register_callback_for_event_record mock event = torch xpu Event event record mock assert_called_once_with event _as_parameter_ value torch xpu current_stream sycl_queue test_event_wait_callback gpu_trace register_callback_for_event_wait mock event = torch xpu Event event record event wait mock assert_called_once_with event _as_parameter_ value torch xpu current_stream sycl_queue test_device_synchronization_callback gpu_trace register_callback_for_device_synchronization mock torch xpu synchronize mock assert_called test_stream_synchronization_callback gpu_trace register_callback_for_stream_synchronization mock stream = torch xpu Stream stream synchronize mock assert_called_once_with stream sycl_queue test_event_synchronization_callback gpu_trace register_callback_for_event_synchronization mock event = torch xpu Event event record event synchronize mock assert_called_once_with event _as_parameter_ value TestXPUAPISanity TestCase test_is_bf _supported assertEqual torch xpu is_bf _supported including_emulation=True torch xpu is_available test_is_tf _supported torch xpu is_available assertFalse torch xpu is_tf _supported test_get_arch_list torch xpu _is_compiled assertEqual len torch xpu get_arch_list test_torch_config_for_xpu config = torch __config__ show value = re search r USE_XPU= ^ + config assertIsNotNone value torch xpu _is_compiled assertTrue value group ON value = re search r USE_XCCL= ^ + config torch distributed is_xccl_available assertTrue value group ON assertTrue value group OFF assertTrue value group OFF assertFalse torch distributed is_xccl_available value = re search r USE_XCCL= ^ + config assertIsNotNone value assertTrue value group OFF __name__ == __main__ run_tests