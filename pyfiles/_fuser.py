mypy allow-untyped-defs contextlib torch contextlib contextmanager optimized_execution should_optimize Context manager controls whether JIT s executor will run optimizations before executing function stored_flag = torch _C _get_graph_executor_optimize torch _C _set_graph_executor_optimize should_optimize try yield finally torch _C _set_graph_executor_optimize stored_flag contextlib contextmanager fuser name Context manager facilitates switching between backend fusers Valid names ` ` fuser ` ` - enables only legacy fuser ` ` fuser ` ` - enables only NNC ` ` fuser ` ` - enables only nvFuser ` ` fuser ` ` - enables oneDNN Graph old_cpu_fuse = torch _C _jit_can_fuse_on_cpu old_gpu_fuse = torch _C _jit_can_fuse_on_gpu old_texpr_fuser_state = torch _C _jit_texpr_fuser_enabled old_nvfuser_state = torch _C _jit_nvfuser_enabled old_llga_state = torch _C _jit_llga_enabled name == fuser legacy fuser torch _C _jit_override_can_fuse_on_cpu True torch _C _jit_override_can_fuse_on_gpu True torch _C _jit_set_texpr_fuser_enabled False torch _C _jit_set_nvfuser_enabled False torch _C _jit_set_llga_enabled False name == fuser NNC old_profiling_executor = torch _C _jit_set_profiling_executor True old_profiling_mode = torch _C _get_graph_executor_optimize True torch _C _jit_override_can_fuse_on_cpu True torch _C _jit_override_can_fuse_on_gpu True torch _C _jit_set_texpr_fuser_enabled True torch _C _jit_set_nvfuser_enabled False torch _C _jit_set_llga_enabled False name == fuser nvFuser torch _C _jit_override_can_fuse_on_cpu False torch _C _jit_override_can_fuse_on_gpu False torch _C _jit_set_texpr_fuser_enabled False torch _C _jit_set_nvfuser_enabled True torch _C _jit_set_llga_enabled False name == fuser oneDNN Graph old_profiling_executor = torch _C _jit_set_profiling_executor True old_profiling_mode = torch _C _get_graph_executor_optimize True torch _C _jit_override_can_fuse_on_cpu True torch _C _jit_override_can_fuse_on_gpu False torch _C _jit_set_texpr_fuser_enabled True torch _C _jit_set_nvfuser_enabled False torch _C _jit_set_llga_enabled True name == none Turn Pytorch fuser off torch _C _jit_override_can_fuse_on_cpu False torch _C _jit_override_can_fuse_on_gpu False torch _C _jit_set_texpr_fuser_enabled False torch _C _jit_set_nvfuser_enabled False torch _C _jit_set_llga_enabled False raise Exception f unrecognized fuser option name name noqa TRY try yield finally name fuser fuser NNC oneDNN Graph torch _C _jit_set_profiling_executor old_profiling_executor type ignore possibly-undefined torch _C _get_graph_executor_optimize old_profiling_mode type ignore possibly-undefined recover previous values torch _C _jit_override_can_fuse_on_cpu old_cpu_fuse torch _C _jit_override_can_fuse_on_gpu old_gpu_fuse torch _C _jit_set_texpr_fuser_enabled old_texpr_fuser_state torch _C _jit_set_nvfuser_enabled old_nvfuser_state torch _C _jit_set_llga_enabled old_llga_state last_executed_optimized_graph = torch _C _last_executed_optimized_graph _get_differentiable_graph_node node diff_node node kind == prim DifferentiableGraph diff_node append node block node blocks n block nodes _get_differentiable_graph_node n diff_node _graph_for args kwargs _script_method_graph_for args kwargs _script_method_graph_for parent args kwargs try dbs = parent get_debug_state eps = list dbs execution_plans values assert len eps == graph = eps graph copy graph_executor_states differentiable node fw_states = eps code differentiable_op_executor_states diff_nodes list torch _C Node = n graph nodes _get_differentiable_graph_node n diff_nodes assert len fw_states == len diff_nodes swap each differentiable graph optimized graph their execution plan n state zip diff_nodes fw_states fw_execution_plans = list state execution_plans values we can only update subgraph when there s unique execution plan Avoid assert here so we would skip ones can t updated while try best effort update other nodes len fw_execution_plans == n g_ Subgraph fw_execution_plans graph graph except Exception fallback approach we just ran graph recorded optimized graph args kwargs last_executed_optimized_graph set_fusion_strategy strategy list tuple str int Set type number specializations can occur during fusion Usage provide list pairs type depth where type one STATIC DYNAMIC depth integer Behavior - static vs dynamic In STATIC fusion fused ops compiled have fixed input shapes The shape determined based some initial profiling runs In DYNAMIC fusion fused ops compiled have variable input shapes so multiple shapes possible In both cases we also recompile new striding behavior device dtype Behavior - fallback functions depth When input doesn t match format required specialized compiled op will run fallback function Fallback functions recursively compiled specialized based observed tensor shapes Since compilation can slow depth parameter provided limit number specializations can compiled before giving up recompiling falling back completely un-fused un-specialized implementation The list type depth pairs controls type specializations number specializations For example STATIC DYNAMIC indicates first two specializations will use static fusions following two specializations will use dynamic fusion any inputs satisfy none options will run unfused implementation NB future more more fusion backends added there may more granular apis specific fusers torch _C _jit_set_fusion_strategy strategy