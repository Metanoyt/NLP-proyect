Owner s oncall distributed os unittest datetime timedelta torch torch distributed dist torch distributed _dist dist torch testing _internal common_distributed MultiProcessTestCase requires_gloo requires_nccl skip_if_lt_x_gpu torch testing _internal common_utils run_tests TestCase synchronize_accelerator torch accelerator is_available torch accelerator synchronize ProcessGroupTest TestCase test_context_manager os environ RANK = str os environ WORLD_SIZE = str os environ MASTER_ADDR = os environ MASTER_PORT = pg = dist new_group backend= gloo timeout=timedelta seconds= device= cpu pg = dist new_group backend= gloo timeout=timedelta seconds= device= cpu assertIsNone dist current_process_group dist process_group pg assertIs dist current_process_group pg dist process_group pg assertIs dist current_process_group pg assertIs dist current_process_group pg assertIsNone dist current_process_group Dist MultiProcessTestCase MultiProcessTestCase property device - torch device raise NotImplementedError device setter device value torch device - None _device = value property world_size - int setUp super setUp _spawn_processes new_group - torch distributed ProcessGroup raise unittest SkipTest new_group must implemented subclasses test_allreduce - None pg = new_group t = torch ones device=self device pg allreduce t timeout=timedelta seconds= wait synchronize_accelerator assertEqual t torch full_like t world_size pg shutdown test_barrier - None pg = new_group pg barrier timeout=timedelta seconds= wait synchronize_accelerator pg shutdown test_broadcast - None pg = new_group t = torch full rank device=self device pg broadcast t root= timeout=timedelta seconds= wait synchronize_accelerator assertEqual t torch full_like t pg shutdown test_allgather - None pg = new_group t = torch full rank + device=self device dtype=torch float out = torch zeros device=self device _ range world_size pg allgather out t timeout=timedelta seconds= wait synchronize_accelerator i range world_size assertEqual out i torch full_like t i + pg shutdown test_gather - None pg = new_group inp = torch full rank + device=self device dtype=torch float out = torch zeros device=self device _ range world_size rank == pg gather out inp root= timeout=timedelta seconds= wait synchronize_accelerator rank == i range world_size assertEqual out i torch full_like inp i + pg shutdown test_scatter - None pg = new_group inp = torch torch full i + device=self device dtype=torch float i range world_size rank == out = torch zeros device=self device pg scatter out inp root= timeout=timedelta seconds= wait synchronize_accelerator assertEqual out torch full_like out rank + pg shutdown test_reduce - None pg = new_group t = torch full device=self device dtype=torch float pg reduce t root= op=dist ReduceOp SUM timeout=timedelta seconds= wait synchronize_accelerator rank == assertEqual t torch full_like t world_size pg shutdown test_reduce_scatter - None pg = new_group inp = torch full i + device=self device dtype=torch float i range world_size out = torch zeros device=self device pg reduce_scatter out inp op=dist ReduceOp SUM timeout=timedelta seconds= wait synchronize_accelerator assertEqual out torch full_like out world_size rank + pg shutdown test_alltoall_base - None pg = new_group out = torch zeros world_size device=self device inp = torch full world_size rank + device=self device dtype=torch float split_sizes = _ range world_size pg alltoall_base out inp split_sizes split_sizes timeout=timedelta seconds= wait synchronize_accelerator i range world_size out_range = out i i + assertEqual out_range torch full_like out_range i + test_group_split - None group = new_group subgroup = group split_group timeout=timedelta seconds= group_name= subgroup_ rank == assert subgroup None assertEqual subgroup size backend = subgroup _get_backend device assertEqual backend options _timeout timedelta seconds= assertEqual subgroup group_name subgroup_ assertEqual subgroup None test_remote_group_merge - None group = new_group subgroup_ = group split_group timeout=timedelta seconds= subgroup_ = group split_group timeout=timedelta seconds= rank == assert subgroup_ None tcp_store = dist TCPStore host_name=os environ MASTER_ADDR port= world_size= is_master=True merged_pg = subgroup_ merge_remote_group tcp_store timedelta seconds= merged_pg assertEqual merged_pg size backend = merged_pg _get_backend device assertEqual backend options _timeout timedelta seconds= assertEqual merged_pg group_name merged_pg assert subgroup_ None tcp_store = dist TCPStore host_name=os environ MASTER_ADDR port= world_size= is_master=False merged_pg = subgroup_ merge_remote_group tcp_store timedelta seconds= merged_pg assertEqual merged_pg size backend = merged_pg _get_backend device assertEqual backend options _timeout timedelta seconds= assertEqual merged_pg group_name merged_pg ProcessGroupGlooTest Dist MultiProcessTestCase property device - torch device torch device cpu requires_gloo new_group - torch distributed ProcessGroup os environ RANK = str rank os environ WORLD_SIZE = str world_size os environ MASTER_ADDR = os environ MASTER_PORT = dist new_group backend= gloo timeout=timedelta seconds= device=self device ProcessGroupNCCLTest Dist MultiProcessTestCase property device - torch device torch device cuda rank requires_nccl skip_if_lt_x_gpu new_group - torch distributed ProcessGroup os environ RANK = str rank os environ WORLD_SIZE = str world_size os environ MASTER_ADDR = os environ MASTER_PORT = dist new_group backend= nccl timeout=timedelta seconds= device=self device __name__ == __main__ assert torch cuda _initialized test_distributed must have initialized CUDA context main process run_tests