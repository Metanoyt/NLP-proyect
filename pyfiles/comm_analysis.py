functools logging math enum IntEnum typing Optional sympy torch torch fx operator_schemas normalize_function ir utils get_dtype_size snode_args_kwargs sympy_product virtualized V log = logging getLogger __name__ NCCL_COLL IntEnum ALL_REDUCE = ALL_GATHER = REDUCE_SCATTER = ALL_TO_ALL = NVIDIA_GPU_TYPE IntEnum VOLTA = AMPERE = HOPPER = functools lru_cache get_gpu_type - NVIDIA_GPU_TYPE gpu_info = torch utils collect_env get_gpu_info torch utils collect_env run V gpu_info NVIDIA_GPU_TYPE VOLTA A gpu_info NVIDIA_GPU_TYPE AMPERE H gpu_info NVIDIA_GPU_TYPE HOPPER other gpu types assume Ampere NVIDIA_GPU_TYPE AMPERE get_collective_type_from_kernel_name kernel_name str - NCCL_COLL assert kernel_name None all_reduce kernel_name NCCL_COLL ALL_REDUCE all_gather kernel_name NCCL_COLL ALL_GATHER reduce_scatter kernel_name NCCL_COLL REDUCE_SCATTER torch ops _dtensor shard_dim_alltoall default kernel_name NCCL_COLL ALL_TO_ALL raise ValueError f Unsupported collective kernel kernel_name get_collective_type node ir IRNode - NCCL_COLL isinstance node ir _CollectiveKernel raise ValueError f node collective kernel node name = node python_kernel_name assert name None get_collective_type_from_kernel_name name get_collective_input_size_bytes node ir IRNode - int sz_bytes = inp node inputs type ignore attr-defined numel = sympy_product inp layout size isinstance numel sympy Integer For ease testing numel = int numel numel = V graph sizevars size_hint numel fallback= sz_bytes += numel get_dtype_size inp layout dtype sz_bytes get_collective_group_size node ir IRNode - int isinstance node ir _CollectiveKernel isinstance node ir _WaitKernel torch distributed distributed_c d _get_group_size_by_name _get_group_size_by_name node constant_args - raise TypeError f Unsupported collective type node #################################################################################################################### The following code constants adapted https github com NVIDIA nccl blob master src graph tuning cc #################################################################################################################### NCCL_HW IntEnum NVLINK = PCI = NET = NCCL_ALGO IntEnum TREE = RING = NCCL_PROTO IntEnum The ordering enum values here matches original https github com NVIDIA nccl blob b e c bad c c b dca de c src include devcomm h#L For difference between these protocols see https github com NVIDIA nccl issues #issuecomment- LL = Low-latency LL = Low-latency -byte SIMPLE = Latencies us len NCCL_ALGO x len NCCL_PROTO NOTE use array instead tensor prevent incompatibility fake mode baseLat = Tree LL Ring LL Latencies us len NCCL_HW x len NCCL_ALGO x len NCCL_PROTO hwLat = NVLINK Tree LL Ring LL PCI Tree LL Ring LL NET Tree LL Ring LL LL max BW per channel llMaxBws = Volta-N Intel-N Intel-N Ampere-N AMD-N AMD-N avg ring tree Hopper-N AMD-N AMD-N avg ring tree estimate_nccl_collective_runtime_nccl_estimator snode - Optional float type ignore no-untyped-def kernel = snode node assert kernel None py_kernel_name = getattr kernel python_kernel_name all_gather py_kernel_name reduce_scatter py_kernel_name NCCL version sometimes unrecoverably fail all_to_all all_reduce None torch distributed distributed_c d _resolve_process_group pg_name = kernel constant_args - type ignore attr-defined pg = _resolve_process_group pg_name rank int = torch distributed get_rank pg TODO ivankobzarev Figure out how we can use time estimations without cuda allocations device = torch device f cuda rank fn = eval py_kernel_name args kwargs = snode_args_kwargs snode TODO ivankobzarev fix out variants snode_args_kwargs all_gather_into_tensor_out py_kernel_name args = args + args try torch distributed _time_estimator group=pg device=device time_estimator w = fn args kwargs torch ops _c d_functional wait_tensor default w except Exception e NCCL estimator can fail log info e noqa G None est_time_us = time_estimator estimated_time - constant NCCL case error during estimations Observed all_to_all estimations est_time_us None est_time_ms = est_time_us e est_time_ms estimate_nccl_collective_runtime_impl tensor_storage_size_bytes int group_size int coll NCCL_COLL - float Returns estimated NCCL collective runtime milliseconds ms The following heuristics copied https github com NVIDIA nccl blob master src graph tuning cc We aim estimate runtime accurately possible Assumptions - only ring algorithm NCCL_ALGO_RING used - only Low-Latency protocol NCCL_PROTO_LL used i e Simple LL used - gpus per node TODO Need find way get accurate gpus per node nodes info - collective one allreduce reducescatter allgather Convert bytes GB tensor_storage_size_GB = tensor_storage_size_bytes Currently assumes each node has gpus And when node used assumes each node uses all gpus TODO Need find way get accurate gpus per node nodes info num_gpus_per_node = nNodes = math ceil group_size num_gpus_per_node nRanks = group_size total gpus globally participate collective op nRanks = Assumes ring algorithm nccl_algo = NCCL_ALGO RING nccl_proto = NCCL_PROTO LL =============== bandwidth computation =============== First compute bandwidth GB s then end convert GB ns bwIntra = torch _inductor config intra_node_bw bwInter = torch _inductor config inter_node_bw compCapIndex = get_gpu_type index = nNodes - nNodes = LL single node we look GPU type multi-node we look CPU type index = compCapIndex nNodes == llMaxBw = llMaxBws index index NOTE each step ring algorithm synchronized bottlenecked slowest link which inter-node interconnect hence when nNodes = bw inter-node bandwidth NOTE original code https github com NVIDIA nccl blob master src graph tuning cc have ` nNodes = ` which seems wrong Corrected here bw = bwIntra nNodes == bwInter nChannels = Assume channels busBw = nChannels bw Various model refinements busBw = min llMaxBw busBw nNodes coll == NCCL_COLL ALL_REDUCE coll == NCCL_COLL ALL_REDUCE nsteps = nRanks - coll == NCCL_COLL ALL_TO_ALL nsteps = nRanks - coll NCCL_COLL REDUCE_SCATTER NCCL_COLL ALL_GATHER nsteps = nRanks - Convert bus BW algorithm BW tensor bytes algoBW = actual execution time ratio = nRanks nsteps type ignore possibly-undefined bandwidth = busBw ratio Convert GB s GB ns bandwidth_GB_per_ns = bandwidth e =============== latency computation =============== intraHw = NCCL_HW NVLINK coll == NCCL_COLL ALL_REDUCE nNodes nInterSteps = nNodes nInterSteps = coll NCCL_COLL REDUCE_SCATTER NCCL_COLL ALL_GATHER NCCL_COLL ALL_TO_ALL nInterSteps = nNodes - First compute latency us then end convert ns latency = baseLat nccl_algo nccl_proto intraLat = hwLat intraHw nccl_algo nccl_proto interLat = hwLat NCCL_HW NET nccl_algo nccl_proto Inter-node rings still have launch nsteps net overhead netOverhead = nNodes netOverhead = getNetOverhead comm intraLat = max intraLat netOverhead latency += nsteps - nInterSteps intraLat + nInterSteps interLat type ignore possibly-undefined Convert us ns latency_ns = latency e =============== final result =============== transport_ns = tensor_storage_size_GB bandwidth_GB_per_ns ns = transport_ns + latency_ns ms = ns e ms ################################################################################################################ The above code constants adapted https github com NVIDIA nccl blob master src graph tuning cc ################################################################################################################ estimate_nccl_collective_runtime node ir IRNode - float Returns estimated NCCL collective runtime nanoseconds ns The following heuristics copied https github com NVIDIA nccl blob master src graph tuning cc We aim estimate runtime accurately possible Assumptions - only ring algorithm NCCL_ALGO_RING used - only Low-Latency protocol NCCL_PROTO_LL used i e Simple LL used - gpus per node TODO Need find way get accurate gpus per node nodes info - collective one allreduce reducescatter allgather tensor_storage_size_bytes = get_collective_input_size_bytes node group_size = get_collective_group_size node coll = get_collective_type node estimate_nccl_collective_runtime_impl tensor_storage_size_bytes group_size coll estimate_fx_collective_size fx_node torch fx Node - int size = node fx_node all_input_nodes t = node meta get val None size += t numel t element_size TODO - symbolic size estimate_nccl_collective_runtime_from_fx_node fx_node torch fx Node override_size Optional int = None - float Returns estimated NCCL collective runtime nanoseconds ns The following heuristics copied https github com NVIDIA nccl blob master src graph tuning cc We aim estimate runtime accurately possible Assumptions - only ring algorithm NCCL_ALGO_RING used - only Low-Latency protocol NCCL_PROTO_LL used i e Simple LL used - gpus per node TODO Need find way get accurate gpus per node nodes info - collective one allreduce reducescatter allgather torch distributed distributed_c d _get_group_size_by_name override_size None tensor_storage_size_bytes = estimate_fx_collective_size fx_node tensor_storage_size_bytes = override_size assert isinstance fx_node target str opt_args_kwargs = normalize_function fx_node target args=fx_node args kwargs=fx_node kwargs normalize_to_only_use_kwargs=True assert opt_args_kwargs None _ kwargs = opt_args_kwargs group_size = _get_group_size_by_name kwargs group_name assert isinstance fx_node target torch _ops OpOverload coll = get_collective_type_from_kernel_name fx_node target name estimate_nccl_collective_runtime_impl tensor_storage_size_bytes group_size coll