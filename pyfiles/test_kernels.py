Owner s module sparse copy io logging itertools product numpy np torch torch ao quantization tq torch nn torch ao pruning sparsifier utils fqn_to_module torch testing _internal common_quantized override_cpu_allocator_for_qnnpack override_qengines qengine_is_fbgemm qengine_is_onednn qengine_is_qnnpack qengine_is_x torch testing _internal common_utils raise_on_run_directly skipIfTorchDynamo TestCase TODO Once more test files created move contents ao folder logger = logging getLogger __name__ TestQuantizedSparseKernels TestCase skipIfTorchDynamo TorchDynamo fails here unknown reasons override_qengines test_sparse_qlinear batch_size = input_channels = output_channels = decimal_val = row_block_size = col_block_size = X implementation sparse ops qnnpack only support block pattern x arm kernels have support both x x This distinction only because x implementations exist only enable testing integration path We do plan add x well so testing does have special case like At moment deprioritized due other higher priority works qengine_is_qnnpack row_block_size == col_block_size == ONEDNN X do support yet qengine_is_onednn qengine_is_x dense_prepack = torch ops quantized linear_prepack dense_qlinear = torch ops quantized linear dense_qlinear_dynamic = torch ops quantized linear_dynamic sparse_prepack = torch ops sparse qlinear_prepack sparse_qlinear = torch ops sparse qlinear sparse_qlinear_dynamic = torch ops sparse qlinear_dynamic X_scale = X_zp = X_fp = torch randn batch_size input_channels dtype=torch float float_bias = torch randn output_channels dtype=torch float W_scales = torch rand output_channels dtype=torch float W_zps = torch zeros output_channels dtype=torch int W_fp = torch randn output_channels input_channels dtype=torch float override_cpu_allocator_for_qnnpack qengine_is_qnnpack X_q = torch quantize_per_tensor X_fp scale=X_scale zero_point=X_zp dtype=torch quint use_channelwise dynamic_mode product True False True False qengine_is_fbgemm dynamic_mode logger info dynamic sparse qlinear only available qnnpack continue qengine_is_qnnpack dynamic_mode logger info static sparse qlinear only available fbgemm continue use_channelwise W_q = torch quantize_per_channel W_fp scales=W_scales zero_points=W_zps axis= dtype=torch qint W_q = torch quantize_per_tensor W_fp scale=W_scales zero_point=W_zps dtype=torch qint Y_scale = Y_zp = W_prepack_dense = dense_prepack W_q float_bias W_prepack_sparse = sparse_prepack W_q float_bias row_block_size col_block_size dynamic_mode Y = sparse_qlinear_dynamic X_fp W_prepack_sparse Y_ref = dense_qlinear_dynamic X_fp W_prepack_dense np testing assert_array_almost_equal Y_ref numpy Y numpy decimal=decimal_val Y_q = sparse_qlinear X_q W_prepack_sparse Y_scale Y_zp Y_q_ref = dense_qlinear X_q W_prepack_dense Y_scale Y_zp np testing assert_array_almost_equal Y_q_ref int_repr numpy Y_q int_repr numpy decimal=decimal_val _sparse_layer_test_helper model_class sparse_mapping ref_mapping qconfig_dict fqn_to_check test_class test_scripting SET UP TEST PARAMETERS INPUTS AND WEIGHTS ------------------------------------------ batch_size = input_channels = output_channels = model = model_class input_channels output_channels For sparse kernels both activation weight ZP = X_scale = X_zp = W_scale = e- W_zp = X_fp = torch randn batch_size input_channels dtype=torch float generate weight which we ll insert into model W_fp = torch randn output_channels input_channels dtype=torch float mask = torch randint W_fp shape W_fp = mask override_cpu_allocator_for_qnnpack qengine_is_qnnpack X_q = torch quantize_per_tensor X_fp scale=X_scale zero_point=X_zp dtype=torch quint X_fp = X_q dequantize W_q = torch quantize_per_tensor W_fp W_scale W_zp torch qint PREPARE MODELS FOR QUANTIZATION ------------------------------- model linear weight = nn Parameter W_q dequantize model eval Add ` sparse_params ` model The test correct sparse_param addition sparsifier tests model linear sparse_params = sparse_block_shape generate model versions qmodel = copy deepcopy model sqmodel = copy deepcopy model generate model versions apply qconfigs tq propagate_qconfig_ qmodel qconfig_dict tq propagate_qconfig_ sqmodel qconfig_dict tq prepare qmodel inplace=True tq prepare sqmodel inplace=True calibrate torch no_grad qmodel X_fp sqmodel X_fp ACTUAL TESTING BEGINS HERE -------------------------- Make sure quantization parameters computed same way qparams = qmodel linear qconfig weight calculate_qparams sqparams = sqmodel linear qconfig weight calculate_qparams test_class assertEqual qparams sqparams sqmodule_to_check = fqn_to_module sqmodel fqn_to_check sqmodule_start_class = sqmodule_to_check __class__ sqmodule_expected_converted_class = sparse_mapping sqmodule_start_class qmodule_to_check = fqn_to_module qmodel fqn_to_check qmodule_start_class = qmodule_to_check __class__ qmodule_expected_converted_class = ref_mapping qmodule_start_class need determine whether dynamic quantization being performed since input dtype will different end is_dynamic = isinstance qmodule_to_check activation_post_process tq PlaceholderObserver tq convert sqmodel inplace=True mapping=sparse_mapping tq convert qmodel inplace=True mapping=ref_mapping code duplicate above since references do update post-convert modules sqmodule_to_check = fqn_to_module sqmodel fqn_to_check qmodule_to_check = fqn_to_module qmodel fqn_to_check check modules converted expected assert isinstance sqmodule_to_check sqmodule_expected_converted_class Convert failed assert isinstance qmodule_to_check qmodule_expected_converted_class Mapping failed row_block_size col_block_size = sqmodel linear _packed_params _weight_bias assert row_block_size == col_block_size == only run during serialization deserialization tests makes sure script save load doesn t malform sqmodel test_scripting scripted_sqmodel = torch jit script sqmodel scripted_sqmodel eval buffer = io BytesIO torch jit save scripted_sqmodel buffer buffer seek sqmodel = torch jit load buffer use correct input dtype is_dynamic Y_ref = qmodel X_fp Y_hat = sqmodel X_fp test_class assertEqual Y_ref Y_hat Y_ref = qmodel X_q Y_hat = sqmodel X_q test_class assertEqual Y_ref dequantize Y_hat dequantize SparseQuantizedModel nn Module __init__ in_channels out_channels super __init__ linear = nn Linear in_channels out_channels forward x linear x TestQuantizedSparseLayers TestCase override_qengines test_sparse_qlinear Note At moment sparse kernels fbgemm supports only static quantized sparse linear qnnpack supports only dynamically quantized sparse linear Hence we have two different tests fbgemm tests static flow qnnpack tests dynamic Should unified later tests should fixed appropriately model_class = SparseQuantizedModel fqn_to_check = linear qengine_is_fbgemm sparse_mapping = tq get_default_static_sparse_quant_module_mappings ref_mapping = tq get_default_static_quant_module_mappings qconfig_dict = nn Linear tq get_default_qconfig fbgemm qengine_is_qnnpack sparse_mapping = tq get_default_dynamic_sparse_quant_module_mappings ref_mapping = tq get_default_dynamic_quant_module_mappings qconfig_dict = nn Linear tq qconfig default_dynamic_qconfig _sparse_layer_test_helper model_class=model_class sparse_mapping=sparse_mapping ref_mapping=ref_mapping qconfig_dict=qconfig_dict fqn_to_check=fqn_to_check test_class=self test_scripting=False override_qengines test_sparse_qlinear_serdes Note At moment sparse kernels fbgemm supports only static quantized sparse linear qnnpack supports only dynamically quantized sparse linear Hence we have two different tests fbgemm tests static flow qnnpack tests dynamic Should unified later tests should fixed appropriately model_class = SparseQuantizedModel fqn_to_check = linear qengine_is_fbgemm sparse_mapping = tq get_default_static_sparse_quant_module_mappings ref_mapping = tq get_default_static_quant_module_mappings qconfig_dict = nn Linear tq get_default_qconfig fbgemm qengine_is_qnnpack sparse_mapping = tq get_default_dynamic_sparse_quant_module_mappings ref_mapping = tq get_default_dynamic_quant_module_mappings qconfig_dict = nn Linear tq qconfig default_dynamic_qconfig _sparse_layer_test_helper model_class=model_class sparse_mapping=sparse_mapping ref_mapping=ref_mapping qconfig_dict=qconfig_dict fqn_to_check=fqn_to_check test_class=self test_scripting=True __name__ == __main__ raise_on_run_directly test test_ao_sparsity py