Owner s module onnx Common utils testing operators __future__ annotations contextlib copy dataclasses multiprocessing os pprint sys unittest warnings collections abc Callable Collection Iterable Mapping Sequence typing Any Optional TypeVar error_reproduction numpy np onnx onnxruntime ort onnxruntime capi onnxruntime_pybind _state onnxscript onnxscript evaluator pytest onnxscript ir torch torch onnx _internal exporter _building _ir_passes _tensors torch testing _internal opinfo core opinfo_core T = TypeVar T Convenience tuples creating dtype lists when skipping xfailing tests BOOL_TYPES = torch bool INT_TYPES = torch int torch int torch int torch int torch uint FLOAT_TYPES = torch float torch float torch float TEST_OPSET_VERSION = IS_MACOS = sys platform startswith darwin IS_WINDOWS = os name == nt dataclasses dataclass DecorateMeta A dataclass storing information about test case skip xfail Adapted functorch functorch test common_utils py op_name str variant_name str decorator Callable Any dtypes Optional Collection torch dtype device_type Optional str reason str test_behavior str matcher Optional Callable Any bool = None enabled_if bool = True The test_class_name apply decorator If None decorator applied all test classes test_class_name Optional str = None xfail op_name str variant_name str = reason str dtypes Optional Collection torch dtype = None device_type Optional str = None matcher Optional Callable Any Any = None enabled_if bool = True test_class_name Optional str = None - DecorateMeta Expects OpInfo test fail Args op_name The name operator variant_name Optional OpInfo variant_test_name reason The reason failure dtypes The dtypes expect failure device_type Device type E g cpu cuda matcher A function matches test sample input It used only when xfail SKIP_XFAIL_SUBTESTS list enabled_if Whether xfail enabled test_class_name The test name apply xfail If None xfail applied all test classes DecorateMeta op_name=op_name variant_name=variant_name decorator=unittest expectedFailure dtypes=dtypes device_type=device_type matcher=matcher reason=reason enabled_if=enabled_if test_class_name=test_class_name test_behavior= xfail skip op_name str variant_name str = reason str dtypes Optional Collection torch dtype = None device_type Optional str = None matcher Optional Callable Any Any = None enabled_if bool = True test_class_name Optional str = None - DecorateMeta Skips OpInfo test Args op_name The name operator variant_name Optional OpInfo variant_test_name reason The reason skipping dtypes The dtypes skip device_type Device type E g cpu cuda matcher A function matches test sample input It used only when skip SKIP_XFAIL_SUBTESTS list enabled_if Whether skip enabled test_class_name The test name apply skip If None skip applied all test classes DecorateMeta op_name=op_name variant_name=variant_name decorator=unittest skip f Skip reason dtypes=dtypes device_type=device_type reason=reason matcher=matcher enabled_if=enabled_if test_class_name=test_class_name test_behavior= skip add_decorate_info all_opinfos Sequence opinfo_core OpInfo test_class_name str base_test_name str skip_or_xfails Iterable DecorateMeta - Callable T T Decorates OpInfo tests decorators based skip_or_xfails list ops_mapping = info name info variant_test_name info info all_opinfos decorate_meta skip_or_xfails opinfo = ops_mapping get decorate_meta op_name decorate_meta variant_name opinfo None decorate_meta enabled_if If OpInfo doesn t exist enabled we skip OpInfo because could OpInfo torch-nightly older versions continue assert opinfo None f Couldn t find OpInfo decorate_meta Did you need specify variant_name decorators = list opinfo decorators new_decorator = opinfo_core DecorateInfo decorate_meta decorator decorate_meta test_class_name test_class_name base_test_name dtypes=decorate_meta dtypes device_type=decorate_meta device_type active_if=decorate_meta enabled_if decorators append new_decorator opinfo decorators = tuple decorators This decorator doesn t modify fn any way wrapped fn fn wrapped duplicate_opinfo opinfos list opinfo_core OpInfo name str new_names tuple str Duplicate opinfo opinfo database give new name duplicated = all_info_names = opinfo name opinfo opinfos opinfo opinfos opinfo name == name new_name new_names new_name all_info_names NOTE Avoid duplicating opinfo already exists database New opinfos expected added torch-nightly warnings warn f OpInfo new_name already exists database stacklevel= continue new_opinfo = copy deepcopy opinfo new_opinfo name = new_name duplicated append new_opinfo opinfos extend duplicated duplicate_opinfo_for_prims opinfos list opinfo_core OpInfo name str prims_name str &#124; None = None Duplicate opinfo opinfo database prims op The function sets new OpInfo use variation torch ops prims The new OpInfo will have name prims_ prims_name where ` prims_name ` name prims op If ` prims_name ` None will set prims_ name Args opinfos The list opinfo_core OpInfo add new opinfo name The name opinfo duplicate prims_name The name prims op If None will set ` name ` prims_name None prims_name = name The name new OpInfo new_name = f prims_ prims_name all_info_names = opinfo name opinfo opinfos opinfo opinfos opinfo name == name new_name all_info_names NOTE Avoid duplicating opinfo already exists database warnings warn f OpInfo new_name already exists database stacklevel= continue new_opinfo = copy deepcopy opinfo new_opinfo name = new_name new_opinfo op = getattr torch ops prims prims_name opinfos append new_opinfo noqa B raise RuntimeError f OpInfo name found database TORCH_TYPE_TO_ONNX = torch bool onnx TensorProto BOOL torch uint onnx TensorProto UINT torch int onnx TensorProto INT torch int onnx TensorProto INT torch int onnx TensorProto INT torch int onnx TensorProto INT torch float onnx TensorProto FLOAT torch float onnx TensorProto FLOAT torch float onnx TensorProto DOUBLE torch complex onnx TensorProto COMPLEX torch complex onnx TensorProto COMPLEX torch bfloat onnx TensorProto BFLOAT convert_tensor_to_numpy input Any - Any isinstance input torch Tensor torch is_complex input complex real representation input = torch view_as_real input input detach cpu numpy isinstance input complex torch view_as_real torch tensor input detach cpu numpy isinstance input list len input == np array dtype=np int any isinstance x torch Tensor x input The list can Optional Tensor e g None Tensor None etc convert_tensor_to_numpy x x input isinstance input bool np array input dtype=np bool_ Just sequence numbers isinstance input int np array input dtype=np int isinstance input float np array input input convert_kwargs_for_onnx kwargs dict str Any - dict str Any Converts kwargs compatible ONNX Runtime new_kwargs = key value kwargs items key == device continue key == dtype value = TORCH_TYPE_TO_ONNX value isinstance value torch Tensor value = np array value cpu new_kwargs key = value new_kwargs OrtAbortedError RuntimeError ONNX Runtime Aborted _ort_session_run serialized_model bytes ort_inputs Mapping str Any Run model ONNX Runtime Disable all ORT optimizations session_options = onnxruntime SessionOptions session_options graph_optimization_level = onnxruntime GraphOptimizationLevel ORT_DISABLE_ALL session = ort InferenceSession serialized_model session_options providers= CPUExecutionProvider session run None ort_inputs _ort_session_run_return_dict serialized_model bytes ort_inputs Mapping str Any return_dict - None Run model ONNX Runtime store results return_dict try return_dict results = _ort_session_run serialized_model ort_inputs return_dict error = None except Exception e pylint disable=broad-except return_dict results = None return_dict error = e _safe_ort_session_run serialized_model bytes ort_inputs Mapping str Any Run model ONNX Runtime separate process Args serialized_model Serialized ONNX model proto ort_inputs Inputs model Returns The inference result Raises OrtAbortedError process did execute successfully manager = multiprocessing Manager return_dict = manager dict process = multiprocessing Process target=_ort_session_run_return_dict args= serialized_model ort_inputs return_dict process start process join process close return_dict raise OrtAbortedError return_dict error None raise return_dict error return_dict results _format_model_and_input_information onnx_model inputs f Inputs \n pprint pformat inputs \nModel \n onnx printer to_text onnx_model _TORCH_DTYPE_TO_ONNX_STRING = torch bool tensor bool torch uint tensor uint torch int tensor int torch int tensor int torch int tensor int torch int tensor int torch float tensor float torch float tensor float torch float tensor double torch complex tensor complex torch complex tensor complex torch bfloat tensor bfloat _TORCH_DTYPE_TO_ONNX dict torch dtype ir DataType = torch bfloat ir DataType BFLOAT torch bool ir DataType BOOL torch complex ir DataType COMPLEX torch complex ir DataType COMPLEX torch float ir DataType FLOAT torch float ir DataType FLOAT torch float ir DataType DOUBLE torch float _e m fn ir DataType FLOAT E M FN torch float _e m fnuz ir DataType FLOAT E M FNUZ torch float _e m ir DataType FLOAT E M torch float _e m fnuz ir DataType FLOAT E M FNUZ torch int ir DataType INT torch int ir DataType INT torch int ir DataType INT torch int ir DataType INT torch uint ir DataType UINT torch uint ir DataType UINT torch uint ir DataType UINT torch uint ir DataType UINT dtype_op_schema_compatible dtype torch dtype schema onnx defs OpSchema - bool Checks dtype compatible schema When dtype compatible schema means we can use dtype create sample inputs OpInfo test ONNX function expect outputs match Args dtype The torch dtype used create sample inputs OpInfo schema The ONNX schema function Returns True dtype compatible schema schema inputs If there no inputs we can t check compatibility Assume compatible e g aten_randn has only attributes True schema inputs name input If name first input input usually input same type output We assume support case For example ` aten_ones size IntType dtype int = FLOAT dtype ` has first input ` size ` which integer can support any dtype True Otherwise we check type constraints first input For example when dtype=torch float op being tested has schema ` ` ` OpSchema name= aten_abs domain= pkg onnxscript torch_lib since_version= doc= abs Tensor - Tensor type_constraints= OpSchema TypeConstraintParam type_param_str= TReal allowed_type_strs= tensor float tensor int tensor int tensor int tensor int tensor float tensor double tensor bfloat description= inputs= OpSchema FormalParameter name= type_str= TReal description= param_option= FormalParameterOption Single is_homogeneous=True min_arity= differentiation_category= DifferentiationCategory Unknown outputs= OpSchema FormalParameter name= return_val type_str= TReal description= param_option= FormalParameterOption Single is_homogeneous=True min_arity= differentiation_category= DifferentiationCategory Unknown attributes= ` ` ` we see first input type TReal corresponding type constraint allowed types tensor float tensor int tensor int tensor int tensor int tensor float tensor double tensor bfloat Since torch float tensor float allowed types we True first_input_type_name = schema inputs type_str Find type constraint first input matching parameter name first_input_type_constraint = next x x schema type_constraints first_input_type_name x type_param_str None assert first_input_type_constraint None allowed_type_strs = first_input_type_constraint allowed_type_strs Here we consider seq tensor float compatible tensor float well any _TORCH_DTYPE_TO_ONNX_STRING dtype type_str type_str allowed_type_strs graph_executor test_name str outputs Sequence Any opset_version int = TEST_OPSET_VERSION - Callable Callable Any tuple Any dict str Any None Eagerly executes function _capture_graph_and_evaluate_torch_script_evaluator function Callable args kwargs - tuple Any onnx ModelProto Captures graph function evaluates using TorchScriptEvaluator Initialize ONNX graph graph = ir Graph nodes= opset_imports= opset_version pkg torch onnx name= main_graph opset = onnxscript values Opset opset_version tracer = _building OpRecorder opset ort_inputs = onnxscript_args list Any = onnxscript_kwargs = i arg enumerate args isinstance arg np ndarray input_name = f input_ i input = _tensors SymbolicTensor opset=opset name=input_name shape=ir Shape arg shape type=ir TensorType _TORCH_DTYPE_TO_ONNX torch tensor arg dtype graph inputs append input onnxscript_args append input ort_inputs input_name = arg isinstance arg list tuple str also sequence we do want treat tensor sequence_input = j subarg enumerate arg isinstance subarg np ndarray input_name = f input_ i _ j tensor = torch tensor subarg input = _tensors SymbolicTensor opset=opset name=input_name shape=ir Shape tensor shape type=ir TensorType _TORCH_DTYPE_TO_ONNX tensor dtype graph inputs append input sequence_input append input ort_inputs input_name = subarg Include non-numpy inputs as-is For example could None value we want keep sequence_input append subarg onnxscript_args append sequence_input onnxscript_args append arg key value kwargs items isinstance value np ndarray input = _tensors SymbolicTensor opset=opset name=key shape=ir Shape torch tensor value shape type=ir TensorType _TORCH_DTYPE_TO_ONNX torch tensor value dtype graph inputs append input ort_inputs key = value onnxscript_kwargs key = input onnxscript_kwargs key = value onnxscript evaluator default_as tracer symbolic_outputs = function onnxscript_args onnxscript_kwargs isinstance symbolic_outputs Sequence symbolic_outputs = symbolic_outputs We need set size output tensors ONNX model valid output symbolic_output zip outputs symbolic_outputs isinstance output Sequence Output sequence elem_dtype = _TORCH_DTYPE_TO_ONNX output dtype symbolic_output type = ir SequenceType ir TensorType elem_dtype continue output = output isinstance output torch Tensor torch tensor output device= cpu symbolic_output shape = ir Shape output shape symbolic_output dtype = _TORCH_DTYPE_TO_ONNX output dtype graph outputs extend symbolic_outputs graph extend tracer nodes onnx_model = ir Model graph ir_version= producer_name= torch_test identifier onnxscript_function tracer functions items identifier onnx_model functions continue isinstance onnxscript_function ir Function ir_function = onnxscript_function TODO Get IR function directly when onnxscript updated proto = onnxscript_function to_function_proto ir_function = ir serde deserialize_function proto onnx_model functions identifier = ir_function _ir_passes add_opset_imports onnx_model Make sure model valid model_proto = ir to_proto onnx_model try onnx checker check_model model_proto full_check=True except onnx checker ValidationError onnx shape_inference InferenceError e raise AssertionError f ONNX model invalid Model \n onnx_model e model_proto = onnx shape_inference infer_shapes model_proto data_prop=True try os environ get CATCH_ORT_SEGFAULT == os environ get CREATE_REPRODUCTION_REPORT == Use individual process run ONNX Runtime catch segfaults _safe_ort_session_run model_proto SerializeToString ort_inputs model_proto _ort_session_run model_proto SerializeToString ort_inputs model_proto except pylint disable=c-extension-no-member onnxruntime capi onnxruntime_pybind _state Fail onnxruntime capi onnxruntime_pybind _state RuntimeException onnxruntime capi onnxruntime_pybind _state InvalidArgument onnxruntime capi onnxruntime_pybind _state InvalidGraph onnxruntime capi onnxruntime_pybind _state NotImplemented pylint enable=c-extension-no-member e os environ get CREATE_REPRODUCTION_REPORT == error_reproduction create_reproduction_report test_name model_proto ort_inputs e test onnx torchlib test_ops py raise RuntimeError ONNX Runtime failed evaluate \n + _format_model_and_input_information model_proto ort_inputs e except OrtAbortedError e os environ get CREATE_REPRODUCTION_REPORT == Save model inputs file reproduction error_reproduction create_reproduction_report test_name model_proto ort_inputs e test onnx torchlib test_ops py raise OrtAbortedError ONNX Runtime aborted \n + _format_model_and_input_information model_proto ort_inputs e except Exception e os environ get CREATE_REPRODUCTION_REPORT == error_reproduction create_reproduction_report test_name model_proto ort_inputs e test onnx torchlib test_ops py raise _capture_graph_and_evaluate_torch_script_evaluator contextlib contextmanager normal_xfail_skip_test_behaviors test_behavior Optional str = None reason Optional str = None This context manager used handle different behaviors xfail skip Args test_behavior optional str From DecorateMeta name can skip xfail None reason optional str The reason failure skip Raises e Any exception raised test case s expected failure We need skip soon possible SegFault might also case test_behavior == skip pytest skip reason=reason try yield We could use ` except AssertionError RuntimeError e ` needs go over all test cases find right exception type except Exception pylint disable=broad-exception-caught test_behavior None raise test_behavior == xfail pytest xfail reason=reason test_behavior == xfail pytest fail Test unexpectedly passed