r This module exposes TunableOp interface Some operations such GEMMs could implemented using more than one library more than one technique For example GEMM could implemented CUDA ROCm using either blas blasLt libraries Further ROCm s rocblas hipblaslt libraries allow user query all possible algorithms then choose one How does one know which implementation fastest should chosen That s what TunableOp provides Enabling TunableOp Tuning Separately ======================================== The TunableOp feature enabled separately enabling tuning phase itself Enabling TunableOp means PyTorch will replace any standard operators their Tunable implementations Any call TunableOp first checks whether has already been tuned given operator inputs If so will immediately call tuned operation no further tuning will take place even when tuning setting enabled Instead no tuning result found tuning enabled TunableOp will benchmark every registered implementation operator given set inputs select fastest File Input Output ===================== The first time any TunableOp invoked internal database tuned operations will prepared attempting read results given file The default filename tunableop_results csv To support tuning when multiple GPUs used across multiple processes GPU device ordinal automatically inserted into filename avoid multiple processes overwriting same file If tuning enabled new tunings discovered during course your workload will also write out same filename all tunings both ones read startup well new ones found runtime This can used example build up tunings file across many workloads reusing same file The output file automatically created when application terminates This behavior can controlled C++ Python APIs environment variables Assuming you specified filename you ll end up CSV file contents like so Validator PT_VERSION Validator ROCM_VERSION - - e Validator HIPBLASLT_VERSION -a c cc Validator ROCBLAS_VERSION - e -dirty GemmTunableOp_float_NT nt_ _ _ Gemm_Hipblaslt_ GemmTunableOp_float_NT nt_ _ _ Gemm_Rocblas_ Note Validator lines If you change library version ROCm version PyTorch version TunableOp will detect reject tunings file because prior tunings likely affected other software changes The remaining lines tuned solutions each TunableOp encountered during your execution Each line consists comma-separated fields operator name operator parameters solution name average execution time The execution time optional field The CSV file can edited caution For example solution name field can changed Default will fall back original PyTorch untuned implementation Or case ROCm s hipBLAS hipBLASLt libraries you know specific solution index you can override solution TunableOp selected replacing value The operator name parameters fields internally named should modified In case GemmTunableOp field indicates datatype whether inputs transposed T N field indicates M N K input shapes There option enable verbose output only recommended debugging purposes This will produce lot diagnostic messages may useful see TunableOp being used all Otherwise TunableOp completely silent besides file output unless there warning error during its use The verbose option only available setting environment variable PYTORCH_TUNABLEOP_VEROBSE= A Note Tuning Behavior Warmup Cache Effects ==================================================== Tuning operator consists iterating through list registered implementations profiling each one The profile established running single implementation loop multiple times taking average execution time There also optional warmup phase prior tuning can help reaching stable power states hardware During tuning workload various hardware caches will more likely produce hits than when tuning There options flushing instruction cache rotate input tensors which might help produce more faithful profile tuned operator operator run within larger workload instead tight repetitive loop By default each possible solution given operator will run either iterations many iterations can run within ms whichever smaller its average execution will calculated The fastest solution among all successfully profiled will chosen A profile might fail given solution doesn t achieve same accuracy default implementation solution returns error code Current Tunable Operators ========================= TunableGemm ROCm -------------------- Currently only TunableGemm ROCm implemented Note CUDA builds PyTorch will function correctly when using TunableOp only solution available CUDA builds Default implementation i e original cuBLAS default now called through TunableOp Any call cuda blas gemm bgemm will routed through TunableOp when enabled Calling gemm given set input arguments transa transb m n k will attempt use fastest available implementation across both rocblas hipblaslt Offline Tuning ============== Motivation ---------- There several use cases offline tuning One use case involves workload high-memory utilization where regular tuning might lead running out memory Another use case compute-intensive workloads In such cases more resource-efficient collect GEMMs workload once then tune repeatedly different tuning parameters libraries Workflow -------- There basically two steps Set environment variables collect untuned GEMM will generate ` ` tunableop_untuned csv ` ` code-block bash export PYTORCH_TUNABLEOP_ENABLED= export PYTORCH_TUNABLEOP_TUNING= export PYTORCH_TUNABLEOP_RECORD_UNTUNED= Run Python script reads ` ` tunableop_untuned csv ` ` generates ` ` tunableop_results csv ` ` like code-block python torch cuda tunable tunable os os putenv PYTORCH_TUNABLEOP_ENABLED os putenv PYTORCH_TUNABLEOP_TUNING os putenv PYTORCH_TUNABLEOP_RECORD_UNTUNED tunable tune_gemm_in_file tunableop_untuned csv It also possible take multiple untuned files distribute GEMMs tuning multiple GPUs within single node In first step GEMMs first gathered duplicate GEMMs eliminated Next GEMMs distributed different GPUs tuning After all GEMMs tuned results all GPUs then gathered into single file whose base filename has ` ` _full ` ` appended example ` ` tunableop_results_full csv ` ` Finally new file containing gathered results will duplicated N times once each GPU convenience user will run workload tuned configuration N GPUs code-block python __name__ == __main__ num_gpus = number GPUs will used during tuning process tunable mgpu_tune_gemm_in_file tunableop_untuned csv num_gpus Note usage ` ` mgpu_tune_gemm_in_file ` ` API different its single GPU counterpart ` ` tune_gemm_in_file ` ` The body Python script calls API must wrapped ` ` main ` ` shown due use concurrent futures module The argument ` ` mgpu_tune_gemm_in_file ` ` must contain wild card expression ` ` ` ` ` ` ` ` generate list untuned files containing GEMMs processed The ` ` num_gpus ` ` must between total number GPUs available Tuning Context ============== The behavior TunableOp currently manipulated through environment variables C++ interface cuda tunable getTuningContext torch cuda tunable python interfaces The environment variables take precedence over any setting you manipulate using C++ Python APIs Environment Variable Interface ------------------------------ Environment variables cached first time they read You cannot use environment variable interface programmatically since settings become fixed Use C++ Python APIs instead concurrent futures glob multiprocessing mp os shutil warnings typing Optional torch __all__ = enable is_enabled tuning_enable tuning_is_enabled record_untuned_enable record_untuned_is_enabled set_max_tuning_duration get_max_tuning_duration set_max_tuning_iterations get_max_tuning_iterations set_filename get_filename get_results get_validators read_file tune_gemm_in_file mgpu_tune_gemm_in_file set_rotating_buffer_size get_rotating_buffer_size set_numerical_check_tolerances enable val bool = True - None r This big off switch all TunableOp implementations torch _C _cuda_tunableop_enable val type ignore attr-defined is_enabled - bool r Returns whether TunableOp feature enabled torch _C _cuda_tunableop_is_enabled type ignore attr-defined tuning_enable val bool = True - None r Enable tuning TunableOp implementations When enabled tuned entry isn t found run tuning step record entry torch _C _cuda_tunableop_tuning_enable val type ignore attr-defined tuning_is_enabled - bool r Returns whether TunableOp implementations can tuned torch _C _cuda_tunableop_tuning_is_enabled type ignore attr-defined record_untuned_enable val bool = True - None r Enable recording untuned TunableOp perations offline tuning When enabled tuned entry isn t found write untuned file torch _C _cuda_record_untuned_enable val type ignore attr-defined record_untuned_is_enabled - bool r Returns whether TunableOp operations recorded offline tuning torch _C _cuda_record_untuned_is_enabled type ignore attr-defined set_max_tuning_duration duration int - None r Set max time milliseconds spend tuning given solution If both max tuning duration iterations set smaller two will honored At minimum tuning iteration will always run torch _C _cuda_tunableop_set_max_tuning_duration duration type ignore attr-defined get_max_tuning_duration - int r Get max time spend tuning given solution torch _C _cuda_tunableop_get_max_tuning_duration type ignore attr-defined set_max_tuning_iterations iterations int - None r Set max number iterations spend tuning given solution If both max tuning duration iterations set smaller two will honored At minimum tuning iteration will always run torch _C _cuda_tunableop_set_max_tuning_iterations iterations type ignore attr-defined get_max_tuning_iterations - int r Get max iterations spend tuning given solution torch _C _cuda_tunableop_get_max_tuning_iterations type ignore attr-defined set_filename filename str insert_device_ordinal bool = False - None r Set filename use input output tuning results If attr ` insert_device_ordinal ` ` ` True ` ` then current device ordinal will added given filename automatically This can used -process-per-gpu scenario ensure all processes write separate file torch _C _cuda_tunableop_set_filename filename insert_device_ordinal type ignore attr-defined get_filename - str r Get results filename torch _C _cuda_tunableop_get_filename type ignore attr-defined get_results - tuple str str str float r Return all TunableOp results torch _C _cuda_tunableop_get_results type ignore attr-defined get_validators - tuple str str r Return TunableOp validators torch _C _cuda_tunableop_get_validators type ignore attr-defined read_file filename Optional str = None - bool r Read results TunableOp CSV file If attr ` filename ` given ` ` get_filename ` ` called filename None filename = get_filename torch _C _cuda_tunableop_read_file filename type ignore attr-defined set_rotating_buffer_size buffer_size int - None r Set rotating buffer size value MB buffer size greater than zero If less than zero query L cache size If equal zero means deactivate rotating buffer torch _C _cuda_tunableop_set_rotating_buffer_size buffer_size type ignore attr-defined get_rotating_buffer_size - int r Get rotating buffer size kilobytes torch _C _cuda_tunableop_get_rotating_buffer_size type ignore attr-defined set_numerical_check_tolerances enable bool atol float = e- rtol float = e- - None r Set atol rtol values numeric check torch _C _cuda_tunableop_set_numerical_check_tolerances enable atol rtol type ignore attr-defined tune_gemm_in_file filename str - None r tune GEMM file assert is_enabled assert tuning_is_enabled deviceid = torch cuda current_device open filename file line file line startswith Gemm ScaledGemm _process_single_offline_gemm line deviceid _gather_unique_untuned_gemm_from_files filename_pattern str - set str r Process multiple untuned results file set duplicates removed unique_gemm_entries = set set will avoid duplicates file_path glob glob filename_pattern open file_path file line file line startswith Gemm ScaledGemm unique_gemm_entries add line unique_gemm_entries _gather_tunableop_results - None r Gather results multiple tunableop results file create single file gemm_lines = set validator_lines = Need allow possibility results filename set Python API instead environment variable Also possible results filename set all There several test cases check ultimately we need glob-able expression results_filename = get_filename Note empty string could returned here results_filename None results_filename = Case Python API used set filename dot_pos = results_filename find dot_pos = - dot_pos Replace character just left dot filename_pattern = results_filename dot_pos - + + results_filename dot_pos filename_pattern = Needed make linter happy Case where environment variable used set filename results_filename_env = os getenv PYTORCH_TUNABLEOP_FILENAME results_filename_env None results_filename_env == filename_pattern = tunableop_results csv d results_filename_env filename_pattern = results_filename_env replace d filename_pattern = results_filename_env replace assert filename_pattern FirstFile = False matching_files = glob glob filename_pattern num_matching_files = len matching_files file_path matching_files open file_path file line file line startswith Validator FirstFile Only read Validator first file validator_lines append line gemm_lines add line FirstFile = True output_file = filename_pattern replace _full open output_file w out_file line validator_lines out_file write line line gemm_lines out_file write line Create num_matching_copies results file i range num_matching_files duplicate_file = output_file replace str i shutil copy output_file duplicate_file _create_matrices m int n int k int lda int ldb int ldc int transA bool transB bool dtypeA torch dtype deviceid str dtypeB Optional torch dtype = None randn bool = True subMatrix bool = False - tuple torch Tensor torch Tensor r Helper function _process_single_offline_gemm Creates matrices then consumed one Torch GEMM APIs Fill parameters set use ScaledGEMM fillA = fillB = dtypeB None dtypeB = dtypeA subMatrix User reference understanding leading dimension https github com Reference-LAPACK lapack blob master BLAS SRC dgemm f TO DO According lines - there no lower bound rowsA there restriction rowsB Using formula now seems work all UTs rowsA = rowsB = max ldc k randn matA = torch randn rowsA lda dtype=dtypeA device=deviceid matB = torch randn rowsB ldb dtype=dtypeA device=deviceid matA = torch full rowsA lda fillA dtype=dtypeB device=deviceid matB = torch full rowsB ldb fillB dtype=dtypeB device=deviceid subA = matA k m t transA matA m k subB = matB n k t transB matB k n subA subB randn matA = torch rand k m dtype=dtypeA device=deviceid t transA torch rand m k dtype=dtypeA device=deviceid matB = torch rand n k dtype=dtypeB device=deviceid t transB torch rand k n dtype=dtypeB device=deviceid matA = torch full k m fillA dtype=dtypeA device=deviceid t transA torch full m k fillA dtype=dtypeA device=deviceid matB = torch full n k fillB dtype=dtypeB device=deviceid t transB torch full k n fillB dtype=dtypeB device=deviceid matA matB _create_batch_matrices m int n int k int b int lda int ldb int ldc int transA bool transB bool dtype torch dtype deviceid str subMatrix bool = False - tuple torch Tensor torch Tensor r Helper function _process_single_offline_gemm Creates batch matrices then consumed one Torch GEMM APIs Similar _create_matrices D batch matrices subMatrix User reference understanding leading dimension https github com Reference-LAPACK lapack blob master BLAS SRC dgemm f TO DO According lines - there no lower bound rowsA there restriction rowsB Using formula now seems work all UTs rowsA = rowsB = max ldc k matA = torch randn b rowsA lda dtype=dtype device=deviceid matB = torch randn b rowsB ldb dtype=dtype device=deviceid subA = matA b k m transpose transA matA b m k subB = matB b n k transpose transB matB b k n subA subB matA = torch rand b k m dtype=dtype device=deviceid transA torch rand b m k dtype=dtype device=deviceid matB = torch rand b n k dtype=dtype device=deviceid transB torch rand b k n dtype=dtype device=deviceid matA = matA transpose transA matA matB = matB transpose transB matB matA matB _process_single_offline_gemm untuned_gemm_line str gpu_id int - None r Process single untuned GEMM deviceid = cuda + str gpu_id dtype_dict = float torch float tf torch float double torch float BFloat torch bfloat Half torch half c complex double torch complex c complex float torch complex Float _e m fn torch float _e m fn Float _e m torch float _e m Float _e m fnuz torch float _e m fnuz Float _e m fnuz torch float _e m fnuz untuned_gemm = untuned_gemm_line strip split underscore_count = untuned_gemm count _ Initialize dtype make linter happy dtype = None dtypeA = None dtypeB = None dtypeC = None Extract BLAS parameters underscore_count == op_sig data_type layout = untuned_gemm split _ transB = layout == T transA = layout == T dtype = dtype_dict get data_type data_type == tf torch backends cuda matmul allow_tf = True torch backends cuda matmul allow_tf = False ScaledGEMM count = untuned_gemm count _ assert count untuned_gemm_temp = untuned_gemm split _ dtypeC = might FP type keep track number underscores op_sig = untuned_gemm_temp data_typeA = untuned_gemm_temp + _ + untuned_gemm_temp data_typeB = untuned_gemm_temp + _ + untuned_gemm_temp count == data_typeC = untuned_gemm_temp + _ + untuned_gemm_temp data_typeC = untuned_gemm_temp transB = untuned_gemm_temp count == T transA = untuned_gemm_temp count == T dtypeA = dtype_dict get data_typeA dtypeB = dtype_dict get data_typeB dtypeC = dtype_dict get data_typeC untuned_gemm_temp = untuned_gemm split _ n m k = int g g untuned_gemm_temp op_sig == GemmStridedBatchedTunableOp assert untuned_gemm_temp == ld ldb lda ldc = int g g untuned_gemm_temp assert untuned_gemm_temp == ld ldb lda ldc = int g g untuned_gemm_temp Detect subMatrix case all item n m k item lda ldb ldc subMatrix = False subMatrix = True op_sig == GemmTunableOp Warnings unsupported cases m == n == k == transA transB pass case supported transA n == pass case supported warnings warn Offline tuning supported GEMM Use online tuning instead + f Skipped tuning untuned_gemm stacklevel= Resolve linter issue dtype None isinstance dtype torch dtype raise TypeError f dtype must torch dtype got dtype matA matB = _create_matrices m n k lda ldb ldc transA transB dtype deviceid subMatrix=subMatrix torch mm matA matB op_sig == GemmStridedBatchedTunableOp Warnings unsupported cases m == n == k == warnings warn Offline tuning support GEMM Use online tuning instead + f Skipped tuning untuned_gemm stacklevel= b = int g g untuned_gemm_temp Resolve linter issue dtype None isinstance dtype torch dtype raise TypeError f dtype must torch dtype got dtype matA matB = _create_batch_matrices m n k b lda ldb ldc transA transB dtype deviceid subMatrix=subMatrix torch bmm matA matB op_sig == ScaledGemmTunableOp Only combination supported PyTorch assert transB True assert transA False Resolve linter issue dtypeA None isinstance dtypeA torch dtype raise TypeError f dtype must torch dtype got dtypeA matA matB = _create_matrices m n k lda ldb ldc transA transB dtypeA deviceid dtypeB=dtypeB randn=False subMatrix=subMatrix assert untuned_gemm_temp == rw untuned_gemm_temp == rowwise = True rowwise = False rowwise scaleA = torch ones m device=deviceid transA torch ones m device=deviceid scaleB = torch ones n device=deviceid transB torch ones n device=deviceid scaleA = torch tensor device=deviceid scaleB = torch tensor device=deviceid assert untuned_gemm_temp == bias untuned_gemm_temp == None no bias vector torch _scaled_mm matA matB scale_a=scaleA scale_b=scaleB out_dtype=dtypeC bias vector present fillbias = bias_dtype = dtype_dict get untuned_gemm_temp bias = torch full n fillbias dtype=bias_dtype device=deviceid transB torch full m fillbias dtype=bias_dtype device=deviceid torch _scaled_mm matA matB scale_a=scaleA scale_b=scaleB out_dtype=dtypeC bias=bias op_sig == GemmAndBiasTunableOp y = x A^T + b assert transA = transB Resolve linter issue dtype None isinstance dtype torch dtype raise TypeError f dtype must torch dtype got dtype bias = torch rand n dtype=dtype device=deviceid X matA = _create_matrices m n k lda ldb ldc transA transB dtype deviceid subMatrix=subMatrix matA = matA t torch nn functional linear X matA bias warnings warn f error unknown op op_sig stacklevel= _check_tuning_assertions - None r Helper function multi-GPU tuning case Need check TunableOp feature enabled tuning enabled is_enabled False warnings warn TunableOp disabled Trying enable now stacklevel= enable True assert is_enabled True assert tuning_is_enabled True assert record_untuned_is_enabled False mgpu_tune_gemm_in_file filename_pattern str num_gpus int - None r Process one more files distribute work over one more GPUs unique_gemm_entries = _gather_unique_untuned_gemm_from_files filename_pattern total_gpus = torch cuda device_count assert = num_gpus = total_gpus mp_context = mp get_context spawn futures = empty list hold futures GEMM assigned GPUs round robin manner h = concurrent futures ProcessPoolExecutor max_workers=num_gpus mp_context=mp_context initializer=_check_tuning_assertions executor The workers separate process TunableOp will enabled child processes PYTORCH_TUNABLEOP_ENABLED= In initializer we also try enable TunableOP th environment variable NOT set line unique_gemm_entries future = executor submit _process_single_offline_gemm line h futures append future h = h + num_gpus future concurrent futures as_completed futures future result torch cuda synchronize _gather_tunableop_results