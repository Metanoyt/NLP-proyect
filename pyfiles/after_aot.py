Utilities reproducing debugging issues PyTorch s Dynamo AOT compilation This module provides tools infrastructure Generating minimal reproducible test cases repros failing compilations Analyzing accuracy issues between eager compiled execution Minifying large models inputs isolate problematic patterns Debugging compiler errors accuracy divergences The main components include - Repro generation Creates standalone Python files reproduce compiler issues - Minification Reduces large graphs minimal failing examples - Accuracy analysis Compares compiled vs eager execution fp reference - Debug tools Dumps graph state tracks intermediates analyzes divergences This primarily used PyTorch developers researchers debug issues Dynamo AOT compilation pipeline particularly Inductor backend __future__ annotations argparse copy functools io logging os shutil subprocess sys textwrap uuid importlib import_module tempfile TemporaryFile typing Any IO Optional TYPE_CHECKING Union typing_extensions Unpack sympy try triton runtime autotuner Autotuner Heuristics triton runtime jit JITFunction except ImportError Autotuner type ignore no-redef pass JITFunction type ignore no-redef pass Heuristics type ignore no-redef pass torch torch fx fx torch nn nn torch _dynamo debug_utils _cuda_system_info_comment AccuracyError backend_accuracy_fails BuckTargetWriter cast_to_fp extra_deps extra_imports generate_config_string generate_env_vars_string helper_for_dump_minify InputReader InputWriter MAX_CONSTANT_NUMEL_INLINE minifier_dir NNModuleToString NopInputReader same_two_models torch _dynamo utils clone_inputs counters same torch _environment is_fbcode torch _higher_order_ops triton_kernel_wrap kernel_side_table torch _inductor cpp_builder normalize_path_separator torch _library fake_class_registry FakeScriptObject torch _ops OpOverload torch fx experimental proxy_tensor make_fx torch fx experimental symbolic_shapes fx_placeholder_targets has_free_symbols torch hub tqdm config TYPE_CHECKING collections abc Callable Sequence torch _inductor compile_fx _CompileFxCallable _CompileFxKwargs torch _inductor output_code OutputCode torch _inductor utils InputType log = logging getLogger __name__ inductor_config = import_module torch _inductor config use_buck = is_fbcode ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ MAIN ENTRY POINT ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ wrap_compiler_debug unconfigured_compiler_fn _CompileFxCallable compiler_name str - _CompileFxCallable Minifier Fx Graph modules after Aot Autograd has finished We wrap both forward backward call separately backend compiler_fn - like inductor nvfuser Intercepting after Aot Autograd presents neat abstraction where all params lifted graph inputs making easy save graph string functools wraps unconfigured_compiler_fn debug_wrapper gm torch fx GraphModule example_inputs Sequence InputType kwargs Unpack _CompileFxKwargs - OutputCode torch _subclasses FakeTensorMode compiler_fn = functools partial unconfigured_compiler_fn kwargs torch _functorch aot_autograd get_aot_graph_name graph_name = get_aot_graph_name TODO why do we need deepcopy original graph orig_graph = copy deepcopy gm graph assert config repro_after dynamo aot None try Call compiler_fn - which either aot_autograd inductor fake inputs inner_compiled_fn = compiler_fn gm example_inputs except Exception TODO Failures here troublesome because no real inputs need different serialization strategy config repro_after == aot config repro_level == dump_compiler_graph_state fx GraphModule gm orig_graph example_inputs compiler_name config repro_level == dump_to_minify fx GraphModule gm orig_graph example_inputs compiler_name log error CompilerError raise We may run regular PyTorch compute may trigger Dynamo do NOT recursively attempt accuracy minify case deferred_for_real_inputs real_inputs Sequence InputType _kwargs object - Any This bit obscure we recursively try accuracy minify SAME function would trigger But most time we should never hit branch assert _kwargs config repro_after = aot assert isinstance inner_compiled_fn str inner_compiled_fn real_inputs config patch repro_after=None inner_debug_fn real_inputs inner_debug_fn real_inputs Sequence InputType - Any Aot Autograd fw_compiler bw_compiler can have fake tensors So example_inputs can fake tensors We can call compiler_fn which inductor nvfuser fake tensors actually compiled_fn should called real tensors Therefore actual invocation deferred Copy tensor attrs like shape stride etc converting Fake Tensor because inductor clears tensor list its codegen And example_inputs available only first invocation fake_mode = FakeTensorMode copy_tensor_attrs = fake_mode from_tensor x isinstance x torch Tensor x x real_inputs config repro_level == Always dump original module case we have segfaults dump_to_minify fx GraphModule gm orig_graph real_inputs compiler_name config repro_level == compiler_name = inductor raise NotImplementedError Accuracy minification supported inductor only failed = same_two_models gm inner_compiled_fn type ignore arg-type real_inputs only_fwd=True ignore_non_fp=config repro_ignore_non_fp failed log warning Accuracy failed AOT Autograd graph s graph_name dump_compiler_graph_state fx GraphModule gm orig_graph real_inputs f compiler_name _accuracy dump_to_minify fx GraphModule gm orig_graph real_inputs f compiler_name _accuracy raise AccuracyError Bad accuracy detected Call compiled function real inputs inner_compiled_fn real_inputs type ignore operator try Call compiled function real inputs out = inner_compiled_fn real_inputs type ignore operator sync cuda kernels ensure IMA detection arg example_inputs isinstance arg torch Tensor arg is_cuda torch cuda synchronize break out except Exception config repro_level == dump_compiler_graph_state fx GraphModule gm orig_graph copy_tensor_attrs compiler_name config repro_level == dump_to_minify fx GraphModule gm orig_graph copy_tensor_attrs compiler_name raise config repro_after == aot compiled_fn = deferred_for_real_inputs compiled_fn _boxed_call = True type ignore attr-defined compiled_fn type ignore return-value inner_compiled_fn debug_wrapper ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ DUMP REPROS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ maybe_fbcode_instructions - str is_fbcode extra_deps_formatted = \n join f dep dep extra_deps len extra_deps_formatted extra_deps_formatted = \n + extra_deps_formatted f \ \ \ \ To run script fbcode - Create directory scripts your_unixname repro - Put file scripts your_unixname repro fx_graph_runnable py - Add TARGETS file looks like following - ` buck run scripts your_unixname repro repro ` NOTE you may need additional deps actually able run script ` ` ` Contents TARGETS file load fbcode_macros build_defs python_binary bzl python_binary python_binary name = repro main_src = fx_graph_runnable py deps = caffe torch extra_deps_formatted ` ` ` \ \ \ generate_compiler_repro_string gm torch fx GraphModule args Sequence Any stable_output bool = False save_dir Optional str = None stable_hash bool = False has_distributed_ops bool = False - str save_dir None save_dir = normalize_path_separator save_dir Add distributed imports needed distributed_imports = has_distributed_ops distributed_imports = textwrap dedent torch distributed dist torch testing _internal distributed fake_pg FakeStore strip triton_imports = len kernel_side_table id_to_kernel triton_imports = textwrap dedent triton triton language tl strip model_str = textwrap dedent f generate_env_vars_string stable_output=stable_output torch torch tensor device torch fx fx torch _dynamo testing rand_strided math inf torch _inductor inductor_prims distributed_imports triton_imports generate_config_string stable_output=stable_output isolate_fails_code_str = None extra_imports maybe_fbcode_instructions stable_output model_str += f torch version torch version __version__ \n hasattr torch version cuda model_str += f torch cuda version torch version cuda \n hasattr torch version git_version model_str += f torch git version torch version git_version \n\n\n model_str += _cuda_system_info_comment kernel_side_table_prefix = torch _higher_order_ops triton_kernel_wrap kernel_side_table Track which grid entry corresponds best config id kernel_side_table id_to_kernel kernel = kernel_side_table get_kernel id try isinstance kernel Autotuner pyrefly ignore missing-attribute isinstance kernel fn Heuristics model_str += ERROR Repro will work intended model_str += triton runtime autotuner Heuristics currently supported\n break config_strs = pyrefly ignore missing-attribute kernel_config kernel configs pyrefly ignore bad-argument-type config_strs append f triton Config str kernel_config kwargs num_warps= kernel_config num_warps num_stages= kernel_config num_stages config_str = join config_strs model_str += textwrap dedent f triton autotune configs= config_str key= strip model_str += \n triton jit\n pyrefly ignore missing-attribute src_code = kernel src isinstance kernel JITFunction kernel fn src fn_name = pyrefly ignore missing-attribute kernel _fn_name isinstance kernel JITFunction kernel fn _fn_name fn_name = fn_name split - model_str += src_code model_str += \n model_str += f kernel_side_table_prefix add_kernel fn_name \n except AttributeError e model_str += ERROR Repro will work intended model_str += f User defined triton kernel exception e \n pyrefly ignore unbound-name len kernel_side_table constant_args pyrefly ignore unbound-name model_str += f kernel_side_table_prefix constant_args= kernel_side_table constant_args \n model_str += NNModuleToString convert gm writer = InputWriter save_dir stable_hash=stable_hash used_syms = Extract graph placeholders their corresponding arguments placeholder_targets = fx_placeholder_targets gm placeholder arg zip placeholder_targets args pyrefly ignore unbound-name isinstance arg int torch SymInt writer symint placeholder arg pyrefly ignore unbound-name isinstance arg torch Tensor TODO improve these names FQN writer tensor placeholder arg arg None writer const placeholder writer unsupported placeholder arg Extract symbolic variables same arguments pyrefly ignore unbound-name isinstance arg torch SymInt By checking sympy Symbol we excluding any symbolic expressions TODO we may need solve expressions extract symbol definitions isinstance arg node expr sympy Symbol arg node hint None used_syms str arg node = arg node hint pyrefly ignore unbound-name isinstance arg torch Tensor Extract symbolic variables tensor shapes strides dim arg shape pyrefly ignore unbound-name isinstance dim torch SymInt isinstance dim node expr sympy Symbol dim node hint None used_syms str dim node = dim node hint stride arg stride pyrefly ignore unbound-name isinstance stride torch SymInt isinstance stride node expr sympy Symbol stride node hint None used_syms str stride node = stride node hint Add symbolic variable definitions top generated code used_syms hint_lines = \n join f name = hint name hint sorted used_syms items model_str = f hint_lines \n\n model_str load_args_lines = writer lines load_args_code = \n join load_args_lines model_str += load_args_code + \n model_str += mod = Repro \n model_str save_graph_repro fd IO Any gm torch fx GraphModule args Sequence Any compiler_name str stable_output bool = False save_dir Optional str = None command str = run accuracy Optional Union str bool = None tracing_mode Optional str = None check_str Optional str = None stable_hash bool = False - None any isinstance arg torch fx experimental _backward_state BackwardState arg args fd write Repro generated due existence BackwardState graph input save_dir None save_dir = normalize_path_separator save_dir Check graph contains distributed operations has_distributed_ops = any node op == call_function isinstance node target OpOverload node target namespace _c d_functional c d_functional node gm graph nodes fd write generate_compiler_repro_string gm args stable_output=stable_output save_dir=save_dir stable_hash=stable_hash has_distributed_ops=has_distributed_ops accuracy None accuracy = _accuracy compiler_name tracing_mode None tracing_mode = real any has_free_symbols args isinstance FakeScriptObject tracing_mode = symbolic fd write __name__ == __main__ \n fd write torch _dynamo repro after_aot run_repro\n Add distributed initialization before run_repro needed has_distributed_ops fd write Initialize FakeProcessGroup distributed operations\n store = FakeStore \n dist init_process_group \n backend= fake \n rank= \n world_size= \n store=store\n \n fd write f torch no_grad \n f run_repro mod load_args accuracy= accuracy r command= command r f save_dir= save_dir r tracing_mode= tracing_mode r check_str= check_str r \n f To run separately do \n f mod args = run_repro mod load_args accuracy= accuracy r command= get_args f save_dir= save_dir r tracing_mode= tracing_mode r check_str= check_str r \n f mod args Add distributed cleanup after run_repro has_distributed_ops fd write \n dist destroy_process_group \n dump_compiler_graph_state gm torch fx GraphModule args Sequence Any compiler_name str accuracy Optional Union str bool = None - None subdir = os path join minifier_dir checkpoints os path exists subdir os makedirs subdir exist_ok=True file_name = os path join subdir f len gm graph nodes py log warning Writing checkpoint s nodes s len gm graph nodes file_name open file_name w fd save_graph_repro fd gm args compiler_name save_dir=subdir accuracy=accuracy curdir = os getcwd repro_path = os path join curdir repro py try shutil copyfile file_name repro_path log warning Copying repro file convenience s repro_path use_buck BuckTargetWriter file_name write except OSError log warning No write permissions s repro_path ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ DUMP MINIFIER ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ dump_to_minify gm torch fx GraphModule args Sequence Any compiler_name str - None out = io StringIO TODO factor out subdir = os path join minifier_dir checkpoints os path exists subdir os makedirs subdir exist_ok=True save_graph_repro out gm args compiler_name save_dir=subdir command= minify helper_for_dump_minify out getvalue isolate_fails fx_g torch fx GraphModule args Sequence Any compiler_name str env Optional dict str Any = None save_dir Optional str = None accuracy Optional Union bool str = None tracing_mode Optional str = None check_str Optional str = None - bool env None env = subdir = os path join os getcwd isolate os path exists subdir os makedirs subdir exist_ok=True file_name = os path join subdir f str uuid uuid py open file_name w fd save_graph_repro fd fx_g args compiler_name save_dir=save_dir command= minifier-query accuracy=accuracy tracing_mode=tracing_mode check_str=check_str open file_name r fd print fd read new_env = os environ copy new_env = new_env env stdout stderr = TemporaryFile TemporaryFile use_buck cmd = BuckTargetWriter file_name write print_msg=False cmd = sys executable file_name p = subprocess Popen cmd cwd=subdir stdout=stdout stderr=stderr env=new_env p wait stdout seek stderr seek print textwrap indent stdout read decode utf- prefix= file=sys stdout print textwrap indent stderr read decode utf- prefix= file=sys stderr print f Isolated test failed - file_name p returncode = ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ MINIFIER TOOLS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ inductor_fails fx_g torch fx GraphModule args Sequence Any check_str Optional str = None - bool has_cuda = False arg args isinstance arg torch Tensor arg is_cuda has_cuda = True break sync - None has_cuda Ensures segfaults surfaced torch cuda synchronize torch _inductor compile_fx compile_fx_inner try result = fx_g args assert isinstance result tuple list assert any isinstance x tuple list x result except Exception False sync try compile_mod = compile_fx_inner fx_g args assert isinstance compile_mod str compile_mod args sync except Exception e check_str None check_str repr e False print repr e True False inductor_accuracy_fails fx_g torch fx GraphModule args Sequence Any check_str Optional str = None require_fp bool = False ignore_non_fp bool = False - bool torch _inductor compile_fx compile_fx_inner backend_aot_accuracy_fails fx_g args type ignore arg-type compile_fx_inner type ignore arg-type require_fp =require_fp ignore_non_fp=ignore_non_fp backend_aot_accuracy_fails = functools partial backend_accuracy_fails only_fwd=True ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ REPRO MAIN ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ repro_common options Any mod nn Module load_args Any - tuple torch fx GraphModule Sequence Any Invariant graphs we generate repro script assert any mod named_parameters n b mod named_buffers b numel MAX_CONSTANT_NUMEL_INLINE log warning Constant s serialized generated random data instead If you think affecting you please comment https github com pytorch pytorch issues n hasattr load_args _version log warning load_args does have _version attribute please file bug PyTorch describe how you generate repro script load_args _version log warning load_args version s version PyTorch only supports version We will try run anyway there may incompatibility so try upgrading your version PyTorch load_args _version nop_reader = NopInputReader load_args nop_reader tqdm desc= Loading inputs total=nop_reader total pbar input_reader = InputReader save_dir=options save_dir pbar=pbar load_args input_reader args = input_reader args Turn mod into GraphModule slow way TODO speed up mod = make_fx mod tracing_mode=options tracing_mode args pyrefly ignore bad-assignment torch _inductor config generate_intermediate_hooks = True mod args ACCURACY_FAILS dict str Callable torch fx GraphModule Any bool = inductor_fails This might look inverted s strict_accuracy means we will minify any time we see anything diverges whereas accuracy more conservative will only minify there meaningful fp divergence accuracy functools partial inductor_accuracy_fails require_fp =True ignore_non_fp=True strict_accuracy inductor_accuracy_fails repro_minifier_query options Any mod nn Module load_args Any - None mod args = repro_common options mod load_args fail_fn = functools partial ACCURACY_FAILS options accuracy check_str=options check_str type ignore call-arg fail_fn mod args sys exit sys exit repro_minify options Any mod nn Module load_args Any - None functorch compile minifier mod args = repro_common options mod load_args compiler_name = inductor_accuracy options accuracy = inductor favored_device = torch cuda device_count = env_variables = CUDA_VISIBLE_DEVICES str favored_device module_fails Any options isolate module_fails = functools partial isolate_fails env=env_variables compiler_name=compiler_name save_dir=options save_dir accuracy=options accuracy tracing_mode=options tracing_mode module_fails = ACCURACY_FAILS options accuracy minifier mod args module_fails=functools partial module_fails check_str=options check_str dump_state=functools partial dump_compiler_graph_state compiler_name=compiler_name save_dir=options save_dir offload_to_disk=options offload_to_disk skip_offload=options skip_saving_eager_intermediates skip_sanity=options skip_sanity max_granularity=options max_granularity repro_analyze options Any mod nn Module load_args Any - None torch _inductor compile_fx compile_fx_inner torch _inductor hooks intermediate_hook mod args = repro_common options mod load_args TODO The logic cloning inputs models here intentionally modeled off run_fwd_maybe_bwd arguably better clone inputs you doubling your effective GPU memory usage It certainly faster though It probably makes sense let user specify offload strategy tqdm desc= Compiling compiled = compile_fx_inner mod args total = counters inductor intermediate_hooks known_names = set save_hook name str val Any - None known_names add name options skip_saving_inductor_intermediates writer write_tensor os path join inductor name val pbar update type ignore has-type writer = torch utils _content_store ContentStoreWriter options save_dir stable_hash=options stable_hash reader = torch utils _content_store ContentStoreReader options save_dir new_args = clone_inputs args intermediate_hook save_hook tqdm desc= Saving inductor intermediates total=total pbar assert isinstance compiled str compiled new_args type ignore arg-type assert new_args compare_tuples tuple tuple Any tuple tuple Any - Optional str diff_indices = i i range len tuple tuple i = tuple i diff_values = tuple i tuple i i diff_indices diff_values None join f = b b diff_values check_hook name str val Any - None meta = writer compute_tensor_metadata val meta = reader read_tensor_metadata os path join inductor name reason = compare_tuples meta meta reason None pbar write f NONDETERMINISTIC INDUCTOR name reason pbar update options skip_check_deterministic new_args = clone_inputs args intermediate_hook check_hook tqdm desc= Checking inductor determinism total=total pbar compiled new_args type ignore arg-type assert new_args WriterInterp fx Interpreter __init__ mod torch nn Module subdir str - None super __init__ mod subdir = subdir run_node n torch fx Node - Any r = super run_node n name = n name name known_names pbar update writer write_tensor os path join subdir name r r NB module cast doesn t actually do anything since there no parameters buffers module options skip_saving_float _intermediates new_mod new_args = cast_to_fp copy deepcopy mod clone_inputs args type ignore arg-type tqdm desc= Saving float intermediates total=total pbar WriterInterp new_mod float boxed_run new_args assert new_args ExactReaderInterp fx Interpreter run_node n torch fx Node - Any r = super run_node n name = n name name known_names meta = writer compute_tensor_metadata r meta = reader read_tensor_metadata os path join float name reason = compare_tuples meta meta reason None pbar write f NONDETERMINISTIC FLOAT name reason pbar update r TODO check eager determinism options skip_check_deterministic new_mod new_args = cast_to_fp copy deepcopy mod clone_inputs args type ignore arg-type tqdm desc= Checking float determinism total=total pbar ExactReaderInterp new_mod boxed_run new_args assert new_args Now we ve saved everything interp through eager graph do comparisons ReaderInterp fx Interpreter run_node n torch fx Node - Any r = super run_node n name = n name name known_names inductor = reader read_tensor os path join inductor name float = reader read_tensor os path join float name logged = False log_error msg str args Any - None nonlocal logged logged = True pbar write f DIVERGED name msg args same r inductor float tol=torch _dynamo config repro_tolerance equal_nan=True log_error=log_error assert logged pbar update r tqdm desc= Checking divergence total=total pbar ReaderInterp mod boxed_run args assert args repro_get_args options Any mod nn Module load_args Any - tuple torch fx GraphModule list Any mod args = repro_common options mod load_args mod args type ignore return-value repro_run options Any mod nn Module load_args Any - None torch _inductor compile_fx compile_fx_inner mod args = repro_common options mod load_args torch cuda synchronize compiled = compile_fx_inner mod args assert isinstance compiled str options accuracy = We don t really respect -- accuracy vs -- strict-accuracy here seems counterintuitive same_two_models mod compiled type ignore arg-type args only_fwd=True ignore_non_fp=config repro_ignore_non_fp raise AccuracyError Bad accuracy detected need_sync = False arg args isinstance arg torch Tensor arg is_cuda need_sync = True break compiled list args need_sync synchronize ensure segfaults surfaced TODO lazily load inputs something rather than cloning them run_repro mod nn Module load_args Any command str = run accuracy Union bool str = save_dir Optional str = None tracing_mode Optional str = None patch_code Optional str = None check_str Optional str = None kwargs Any - Any k kwargs log warning Unrecognized kwarg s perhaps repro made newer version PyTorch k accuracy True accuracy = accuracy accuracy False accuracy = patch_code None log warning patch_code no longer works version PyTorch silently ignoring parser = argparse ArgumentParser description=f \ An after_aot repro script typically triggering bug PyTorch Inductor When run no arguments script defaults running command Extra flags may available find out more try command -- help There also alternate subcommands available see below default settings script accuracy= tracing_mode= save_dir= check_str= formatter_class=argparse RawTextHelpFormatter common_flags parser argparse ArgumentParser - None accuracy_group = parser add_mutually_exclusive_group accuracy_group add_argument -- no-accuracy dest= accuracy action= store_const const= default=accuracy help= do test accuracy just run module see errors accuracy_group add_argument -- accuracy action= store_const const= accuracy default=accuracy help= \ test RMSE between compiled module fp reference greater than eager fp reference This usually more reliable than standard allclose test we expect numeric differences compiling often improving accuracy over eager RMSE test allows compiled module diverge greatly eager long divergence moves closer true mathematical value network Caveats double precision can still suffer rounding error so perfect reference see example Herbie Automatically Improving Floating Point Accuracy approaches detect necessary working precision compute arbitrary precision floating point unfortunately practical tensor computation there enough samples output being compared we may get unlucky have unlucky greater RMSE than eager could overcome applying more rigorous statistical test some p-value which we leave future work accuracy_group add_argument -- strict-accuracy dest= accuracy action= store_const const= strict_accuracy default=accuracy help= \ default when doing accuracy minification we will reject reductions which change divergence floating point divergence integral boolean divergence This because some operations like ReLU involve temporarily sharp boundaries smooth out again afterwards without requiring divergence floating point minifier will often fixate divergent boolean tensor even though true source divergence However rejecting these reductions makes more difficult minifier make process Using option will let minifier progress ALL divergences -- you just might end up useful repro end parser add_argument -- save-dir type=str default=save_dir metavar= DIR help= directory where saved inputs live parser add_argument -- no-save-dir dest= save_dir action= store_const const=None help= don t use any directory saved inputs parser add_argument -- tracing-mode type=str metavar= real fake symbolic default=tracing_mode help= how trace repro module into GraphModule metadata subparsers = parser add_subparsers dest= command metavar= run minify analyze required=True parser_run = subparsers add_parser run help= just run repro common_flags parser_run parser_minify = subparsers add_parser minify help= run minifier repro common_flags parser_minify parser_get_args = subparsers add_parser get_args help= get args common_flags parser_get_args parser_minify_isolate = parser_minify add_mutually_exclusive_group parser_minify_isolate add_argument -- isolate action= store_true default=True help= run separate processes avoid interference default parser_minify_isolate add_argument -- no-isolate dest= isolate action= store_false help= speed up running all compilation same process parser_minify add_argument -- skip-saving-eager-intermediates action= store_true help= skip saving eager intermediates -- minify TODO make option -- analyze too parser_minify add_argument -- offload-to-disk action= store_true help= during minification offload delta debugging intermediates disk Use you re OOMing parser_minify add_argument -- skip-sanity action= store_true help= skip sanity check beginning minification original graph parser_minify add_argument -- max-granularity type=int default=None help= start granularity work down must power parser_minify add_argument -- check-str type=str default=check_str help= require minified program fail error containing string parser_analyze = subparsers add_parser analyze help= run accuracy analyzer repro common_flags parser_analyze parser_analyze add_argument -- skip-saving-inductor-intermediates action= store_true help= skip saving inductor intermediates -- analyze parser_analyze add_argument -- skip-saving-float -intermediates action= store_true help= skip saving float intermediates parser_analyze add_argument -- skip-check-deterministic action= store_true help= skip checking network deterministic parser_analyze add_argument -- stable-hash action= store_true help= use SHA- checksum instead fast possibly unsound hash Run repro context minification inverting exit code meaning parser_minifier_query = subparsers add_parser minifier-query common_flags parser_minifier_query parser_minifier_query add_argument -- check-str type=str default=check_str help= require minified program fail error containing string args = None len sys argv = args = command sys argv options = parser parse_args args COMMAND_FNS = minify repro_minify analyze repro_analyze minifier-query repro_minifier_query run repro_run get_args repro_get_args COMMAND_FNS options command options mod load_args