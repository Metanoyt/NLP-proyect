Owner s oncall distributed copy torch torch nn nn torch amp grad_scaler GradScaler OptState torch distributed device_mesh init_device_mesh torch distributed fsdp fully_shard torch distributed tensor parallel ColwiseParallel parallelize_module RowwiseParallel torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp FSDPTest get_devtype MLP torch testing _internal common_utils run_tests device_type = torch device get_devtype TestFullyShardGradientScaler FSDPTest skip_if_lt_x_gpu test_gradient_scaler run_subtests has_inf True False test_ d True False _test_gradient_scaler _test_gradient_scaler has_inf bool test_ d bool torch manual_seed model = nn Sequential nn Linear device=device_type bias=False _ range layer model fully_shard layer fully_shard model input = torch randn device=device_type test_ d mesh_ d = init_device_mesh device_type type world_size mesh_dim_names= dp tp dp_mesh tp_mesh = mesh_ d dp mesh_ d tp model = nn Sequential MLP MLP MLP tp_parallelize_plan = in_proj ColwiseParallel out_proj RowwiseParallel in_proj ColwiseParallel out_proj RowwiseParallel in_proj ColwiseParallel out_proj RowwiseParallel model = parallelize_module model device_mesh=tp_mesh parallelize_plan=tp_parallelize_plan module model fully_shard module mesh=dp_mesh fully_shard model mesh=dp_mesh input = torch randn device=device_type loss = model input sum scaler = GradScaler init_scale= enabled=True device=device_type type opt = torch optim Adam model parameters lr= e- scaler scale loss backward inv_scale = scaler _scale double reciprocal float has_inf True opt param_groups params grad _local_tensor device index == opt param_groups params grad _local_tensor fill_ float inf initial_grad = opt param_groups params grad to_local clone scaler unscale_ opt found_inf scaler _per_optimizer_states id opt found_inf_per_device values assertEqual found_inf has_inf assertEqual scaler _per_optimizer_states id opt stage value OptState UNSCALED value unscaled_grad = opt param_groups params grad to_local clone assertEqual unscaled_grad initial_grad inv_scale initial_scale = scaler get_scale initial_state = copy copy opt state scaler step opt steped_state = copy copy opt state has_inf assert parameters same before after assertEqual steped_state initial_state new parameters here no inf found during unscale_ assertNotEqual steped_state items initial_state items scaler update updated_scale = scaler get_scale has_inf assert scale updated backoff_factor = scaler get_backoff_factor assertEqual updated_scale initial_scale backoff_factor scale updated assertEqual updated_scale initial_scale __name__ == __main__ run_tests