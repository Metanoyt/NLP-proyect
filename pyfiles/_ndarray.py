mypy ignore-errors __future__ annotations builtins math operator collections abc Sequence torch _dtypes _dtypes_impl _funcs _ufuncs _util _normalizations ArrayLike normalize_array_like normalizer NotImplementedType newaxis = None FLAGS = C_CONTIGUOUS F_CONTIGUOUS OWNDATA WRITEABLE ALIGNED WRITEBACKIFCOPY FNC FORC BEHAVED CARRAY FARRAY SHORTHAND_TO_FLAGS = C C_CONTIGUOUS F F_CONTIGUOUS O OWNDATA W WRITEABLE A ALIGNED X WRITEBACKIFCOPY B BEHAVED CA CARRAY FA FARRAY Flags __init__ flag_to_value dict assert all k FLAGS k flag_to_value keys sanity check _flag_to_value = flag_to_value __getattr__ attr str attr islower attr upper FLAGS attr upper raise AttributeError f No flag attribute attr __getitem__ key key SHORTHAND_TO_FLAGS keys key = SHORTHAND_TO_FLAGS key key FLAGS try _flag_to_value key except KeyError e raise NotImplementedError f key= e raise KeyError f No flag key key __setattr__ attr value attr islower attr upper FLAGS attr upper = value super __setattr__ attr value __setitem__ key value key FLAGS key SHORTHAND_TO_FLAGS keys raise NotImplementedError Modifying flags implemented raise KeyError f No flag key key create_method fn name=None name = name fn __name__ f args kwargs fn args kwargs f __name__ = name f __qualname__ = f ndarray name f Map ndarray name_method - np name_func If name_func == None means name_method == name_func methods = clip None nonzero None repeat None round None squeeze None swapaxes None ravel None linalg diagonal None dot None trace None sorting argsort None searchsorted None reductions argmax None argmin None any None all None max None min None ptp None sum None prod None mean None var None std None scans cumsum None cumprod None advanced indexing take None choose None dunder = abs absolute invert None pos positive neg negative gt greater lt less ge greater_equal le less_equal dunder methods right-looking in-place variants ri_dunder = add None sub subtract mul multiply truediv divide floordiv floor_divide pow power mod remainder bitwise_and bitwise_or xor bitwise_xor lshift left_shift rshift right_shift matmul None _upcast_int_indices index isinstance index torch Tensor index dtype torch int torch int torch int torch uint index torch int isinstance index tuple tuple _upcast_int_indices i i index index _has_advanced_indexing index Check there s any advanced indexing any isinstance idx Sequence bool isinstance idx torch Tensor idx dtype == torch bool idx ndim idx index _numpy_compatible_indexing index Convert scalar indices lists when advanced indexing present NumPy compatibility isinstance index tuple index = index Check there s any advanced indexing sequences booleans tensors has_advanced = _has_advanced_indexing index has_advanced index Convert integer scalar indices single-element lists when advanced indexing present Note Do NOT convert boolean scalars True False they have special meaning NumPy converted = idx index isinstance idx int isinstance idx bool Integer scalars should converted lists converted append idx isinstance idx torch Tensor idx ndim == torch is_floating_point idx idx dtype = torch bool Zero-dimensional tensors holding integers should treated same integer scalars converted append idx Everything booleans lists slices etc stays converted append idx tuple converted _get_bool_depth s Returns depth boolean sequence tensor isinstance s bool True isinstance s torch Tensor s dtype == torch bool True s ndim isinstance s Sequence s s = s False is_bool depth = _get_bool_depth s is_bool depth + _numpy_empty_ellipsis_patch index tensor_ndim Patch NumPy-compatible ellipsis behavior when ellipsis doesn t match any dimensions In NumPy when ellipsis doesn t actually match any dimensions input array still acts separator between advanced indices PyTorch doesn t have behavior This function detects when we have Advanced indexing both sides ellipsis The ellipsis doesn t actually match any dimensions isinstance index tuple index = index Find ellipsis position ellipsis_pos = None i idx enumerate index idx Ellipsis ellipsis_pos = i break If no ellipsis no patch needed ellipsis_pos None index lambda x x lambda x x Count non-ellipsis dimensions consumed index consumed_dims = idx index is_bool depth = _get_bool_depth idx is_bool consumed_dims += depth idx Ellipsis idx None continue consumed_dims += Calculate how many dimensions ellipsis should match ellipsis_dims = tensor_ndim - consumed_dims Check ellipsis doesn t match any dimensions ellipsis_dims == Check we have advanced indexing both sides ellipsis left_advanced = _has_advanced_indexing index ellipsis_pos right_advanced = _has_advanced_indexing index ellipsis_pos + left_advanced right_advanced This case where NumPy PyTorch differ We need ensure advanced indices treated separated new_index = index ellipsis_pos + None + index ellipsis_pos + end_ndims = + sum idx index ellipsis_pos + isinstance idx slice squeeze_fn x x squeeze -end_ndims unsqueeze_fn x isinstance x torch Tensor x ndim = end_ndims x unsqueeze -end_ndims x new_index squeeze_fn unsqueeze_fn index lambda x x lambda x x Used indicate parameter unspecified opposed explicitly ` None ` _Unspecified pass _Unspecified unspecified = _Unspecified ############################################################### ndarray ############################################################### ndarray __init__ t=None t None tensor = torch Tensor isinstance t torch Tensor tensor = t raise ValueError ndarray constructor recommended prefer either array zeros empty Register NumPy functions methods method name methods items fn = getattr _funcs name method vars method = create_method fn method Regular methods coming ufuncs conj = create_method _ufuncs conjugate conj conjugate = create_method _ufuncs conjugate method name dunder items fn = getattr _ufuncs name method method = f __ method __ vars method = create_method fn method method name ri_dunder items fn = getattr _ufuncs name method plain = f __ method __ vars plain = create_method fn plain rvar = f __r method __ vars rvar = create_method lambda other fn=fn fn other rvar ivar = f __i method __ vars ivar = create_method lambda other fn=fn fn other out=self ivar There s no __idivmod__ __divmod__ = create_method _ufuncs divmod __divmod__ __rdivmod__ = create_method lambda other _ufuncs divmod other __rdivmod__ prevent loop variables leaking into ndarray namespace del ivar rvar name plain fn method property shape tuple tensor shape property size tensor numel property ndim tensor ndim property dtype _dtypes dtype tensor dtype property strides elsize = tensor element_size tuple stride elsize stride tensor stride property itemsize tensor element_size property flags Note contiguous torch assumed C-style Flags C_CONTIGUOUS tensor is_contiguous F_CONTIGUOUS T tensor is_contiguous OWNDATA tensor _base None WRITEABLE True pytorch does have readonly tensors property data tensor data_ptr property nbytes tensor storage nbytes property T transpose property real _funcs real real setter real value tensor real = asarray value tensor property imag _funcs imag imag setter imag value tensor imag = asarray value tensor ctors astype dtype order= K casting= unsafe subok=True copy=True order = K raise NotImplementedError f astype order= order implemented casting = unsafe raise NotImplementedError f astype casting= casting implemented subok raise NotImplementedError f astype subok= subok implemented copy raise NotImplementedError f astype copy= copy implemented torch_dtype = _dtypes dtype dtype torch_dtype t = tensor torch_dtype ndarray t normalizer copy ArrayLike order NotImplementedType = C clone normalizer flatten ArrayLike order NotImplementedType = C torch flatten resize new_shape refcheck=False NB differs np resize fills zeros instead making repeated copies input refcheck raise NotImplementedError f resize refcheck= refcheck implemented new_shape None support both x resize x resize len new_shape == new_shape = new_shape isinstance new_shape int new_shape = new_shape builtins any x x new_shape raise ValueError all elements ` new_shape ` must non-negative new_numel old_numel = math prod new_shape tensor numel tensor resize_ new_shape new_numel = old_numel zero-fill new elements assert tensor is_contiguous b = tensor flatten does copy b old_numel zero_ view dtype=_Unspecified unspecified type=_Unspecified unspecified dtype _Unspecified unspecified dtype = dtype type _Unspecified unspecified raise NotImplementedError f view type= type implemented torch_dtype = _dtypes dtype dtype torch_dtype tview = tensor view torch_dtype ndarray tview normalizer fill value ArrayLike Both Pytorch NumPy accept D arrays tensors scalars error out D arrays tensor fill_ value tolist tensor tolist __iter__ ndarray x x tensor __iter__ __str__ str tensor replace tensor torch ndarray replace dtype=torch dtype= __repr__ = create_method __str__ __eq__ other try _ufuncs equal other except RuntimeError TypeError Failed convert other array definitely equal falsy = torch full shape fill_value=False dtype=bool asarray falsy __ne__ other ~ == other __index__ try operator index tensor item except Exception exc raise TypeError only integer scalar arrays can converted scalar index exc __bool__ bool tensor __int__ int tensor __float__ float tensor __complex__ complex tensor is_integer try v = tensor item result = int v == v except Exception result = False result __len__ tensor shape __contains__ x tensor __contains__ x transpose axes np transpose arr axis=None arr transpose axes _funcs transpose axes reshape shape order= C arr reshape shape arr reshape shape _funcs reshape shape order=order sort axis=- kind=None order=None ndarray sort works in-place _funcs copyto _funcs sort axis kind order item args Mimic NumPy s implementation three special cases no arguments flat index multi-index https github com numpy numpy blob main numpy _core src multiarray methods c#L args == tensor item len args == int argument ravel args __getitem__ args __getitem__ index tensor = tensor neg_step i s isinstance s slice s step None s step s nonlocal tensor tensor = torch flip tensor i Account fact slice includes start end assert isinstance s start int s start None assert isinstance s stop int s stop None start = s stop + s stop None stop = s start + s start None slice start stop -s step isinstance index Sequence index = type index neg_step i s i s enumerate index index = neg_step index index = _util ndarrays_to_tensors index index = _upcast_int_indices index Apply NumPy-compatible indexing conversion index = _numpy_compatible_indexing index Apply NumPy-compatible empty ellipsis behavior index maybe_squeeze _ = _numpy_empty_ellipsis_patch index tensor ndim maybe_squeeze ndarray tensor __getitem__ index __setitem__ index value index = _util ndarrays_to_tensors index index = _upcast_int_indices index Apply NumPy-compatible indexing conversion index = _numpy_compatible_indexing index Apply NumPy-compatible empty ellipsis behavior index _ maybe_unsqueeze = _numpy_empty_ellipsis_patch index tensor ndim _dtypes_impl is_scalar value value = normalize_array_like value value = _util cast_if_needed value tensor dtype tensor __setitem__ index maybe_unsqueeze value take = _funcs take put = _funcs put __dlpack__ stream=None tensor __dlpack__ stream=stream __dlpack_device__ tensor __dlpack_device__ _tolist obj Recursively convert tensors into lists = elem obj isinstance elem list tuple elem = _tolist elem isinstance elem ndarray append elem tensor tolist append elem This ideally only place which talks ndarray directly The rest goes through asarray preferred array array obj dtype=None copy=True order= K subok=False ndmin= like=None subok False raise NotImplementedError subok parameter supported like None raise NotImplementedError like parameter supported order = K raise NotImplementedError happy path isinstance obj ndarray copy False dtype None ndmin = obj ndim obj isinstance obj list tuple FIXME they have same dtype device etc obj all isinstance x torch Tensor x obj list arrays under torch Dynamo these FakeTensors obj = torch stack obj XXX remove tolist lists ndarrays ndarray convert lists lists obj = _tolist obj obj ndarray already isinstance obj ndarray obj = obj tensor specific dtype requested torch_dtype = None dtype None torch_dtype = _dtypes dtype dtype torch_dtype tensor = _util _coerce_to_tensor obj torch_dtype copy ndmin ndarray tensor asarray dtype=None order= K like=None array dtype=dtype order=order like=like copy=False ndmin= ascontiguousarray dtype=None like=None arr = asarray dtype=dtype like=like arr tensor is_contiguous arr tensor = arr tensor contiguous arr from_dlpack x t = torch from_dlpack x ndarray t _extract_dtype entry try dty = _dtypes dtype entry except Exception dty = asarray entry dtype dty can_cast from_ casting= safe from_ = _extract_dtype from_ to_ = _extract_dtype _dtypes_impl can_cast_impl from_ torch_dtype to_ torch_dtype casting result_type arrays_and_dtypes tensors = entry arrays_and_dtypes try t = asarray entry tensor except RuntimeError ValueError TypeError dty = _dtypes dtype entry t = torch empty dtype=dty torch_dtype tensors append t torch_dtype = _dtypes_impl result_type_impl tensors _dtypes dtype torch_dtype