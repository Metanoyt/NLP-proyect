mypy allow-untyped-defs itertools collections namedtuple collections abc Sequence torch torch nn functional F torch Tensor container ModuleList Sequential linear Linear module Module __all__ = AdaptiveLogSoftmaxWithLoss _ASMoutput = namedtuple _ASMoutput output loss AdaptiveLogSoftmaxWithLoss Module Efficient softmax approximation As described ` Efficient softmax approximation GPUs Edouard Grave Armand Joulin Moustapha Ciss\u e David Grangier Herv\u e J\u e gou https arxiv org abs ` __ r Adaptive softmax approximate strategy training models large output spaces It most effective when label distribution highly imbalanced example natural language modelling where word frequency distribution approximately follows ` Zipf s law ` _ Adaptive softmax partitions labels into several clusters according their frequency These clusters may contain different number targets each Additionally clusters containing less frequent labels assign lower dimensional embeddings those labels which speeds up computation For each minibatch only clusters which least one target present evaluated The idea clusters which accessed frequently like first one containing most frequent labels should also cheap compute -- contain small number assigned labels We highly recommend taking look original paper more details attr ` cutoffs ` should ordered Sequence integers sorted increasing order It controls number clusters partitioning targets into clusters For example setting ` ` cutoffs = ` ` means first ` ` targets will assigned head adaptive softmax targets ` ` will assigned first cluster targets ` ` will assigned second cluster while targets ` n_classes - ` will assigned last third cluster attr ` div_value ` used compute size each additional cluster which given math ` \left\lfloor\frac \texttt in\_features \texttt div\_value ^ idx \right\rfloor ` where math ` idx ` cluster index clusters less frequent words having larger indices indices starting math ` ` attr ` head_bias ` set True adds bias term head adaptive softmax See paper details Set False official implementation warning Labels passed inputs module should sorted according their frequency This means most frequent label should represented index ` ` least frequent label should represented index ` n_classes - ` note This module returns ` ` NamedTuple ` ` ` ` output ` ` ` ` loss ` ` fields See further documentation details note To compute log-probabilities all classes ` ` log_prob ` ` method can used Args in_features int Number features input tensor n_classes int Number classes dataset cutoffs Sequence Cutoffs used assign targets their buckets div_value float optional value used exponent compute sizes clusters Default head_bias bool optional If ` ` True ` ` adds bias term head adaptive softmax Default ` ` False ` ` Returns ` ` NamedTuple ` ` ` ` output ` ` ` ` loss ` ` fields output Tensor size ` ` N ` ` containing computed target log probabilities each example loss Scalar representing computed negative log likelihood loss Shape - input math ` N \texttt in\_features ` math ` \texttt in\_features ` - target math ` N ` math ` ` where each value satisfies math ` = \texttt target i = \texttt n\_classes ` - output math ` N ` math ` ` - output ` ` Scalar ` ` _Zipf s law https en wikipedia org wiki Zipf s_law in_features int n_classes int cutoffs list int div_value float head_bias bool head Linear tail ModuleList __init__ in_features int n_classes int cutoffs Sequence int div_value float = head_bias bool = False device=None dtype=None - None factory_kwargs = device device dtype dtype super __init__ cutoffs = list cutoffs len cutoffs == raise ValueError cutoffs should sequence length larger than cutoffs = sorted cutoffs min cutoffs = max cutoffs n_classes - len set cutoffs = len cutoffs any int c = c c cutoffs raise ValueError cutoffs should sequence unique positive integers sorted increasing order where each value between n_classes- in_features = in_features n_classes = n_classes cutoffs = cutoffs + n_classes div_value = div_value head_bias = head_bias shortlist_size = cutoffs n_clusters = len cutoffs - head_size = shortlist_size + n_clusters head = Linear in_features head_size bias=self head_bias factory_kwargs tail = ModuleList i range n_clusters hsz = int in_features div_value i + osz = cutoffs i + - cutoffs i projection = Sequential Linear in_features hsz bias=False factory_kwargs Linear hsz osz bias=False factory_kwargs tail append projection reset_parameters - None Resets parameters based their initialization used ` ` __init__ ` ` head reset_parameters i h h o tail type ignore misc i h reset_parameters type ignore has-type h o reset_parameters type ignore has-type forward input_ Tensor target_ Tensor - _ASMoutput Runs forward pass targ_dim = target_ dim targ_dim == input_ size = target_ size raise RuntimeError Input target should have same size batch dimension input_ dim = raise RuntimeError D target tensor expects D input tensors found inputs size input_ size targ_dim == input_ dim = raise RuntimeError D target tensor expects D input tensors found inputs size input_ size raise RuntimeError D D target tensor expected multi-target supported is_batched = targ_dim input = input_ is_batched input_ unsqueeze target = target_ is_batched target_ unsqueeze used_rows = batch_size = target size output = input new_zeros batch_size gather_inds = target new_empty batch_size cutoff_values = + cutoffs i range len cutoff_values - low_idx = cutoff_values i high_idx = cutoff_values i + target_mask = target = low_idx target high_idx row_indices = target_mask nonzero squeeze row_indices numel == continue i == gather_inds index_copy_ row_indices target target_mask relative_target = target target_mask - low_idx input_subset = input index_select row_indices cluster_output = tail i - input_subset cluster_index = shortlist_size + i - gather_inds index_fill_ row_indices cluster_index cluster_logprob = F log_softmax cluster_output dim= local_logprob = cluster_logprob gather relative_target unsqueeze output index_copy_ row_indices local_logprob squeeze used_rows += row_indices numel used_rows = batch_size raise RuntimeError f Target values should n_classes - f values range target min item target max item found head_output = head input head_logprob = F log_softmax head_output dim= output += head_logprob gather gather_inds unsqueeze squeeze loss = -output mean is_batched output = output squeeze _ASMoutput output loss _get_full_log_prob input head_output Given input tensor output ` ` head ` ` compute log full distribution out = input new_empty head_output size n_classes head_logprob = F log_softmax head_output dim= out shortlist_size = head_logprob shortlist_size i start_idx stop_idx enumerate itertools pairwise cutoffs cluster_output = tail i input cluster_logprob = F log_softmax cluster_output dim= output_logprob = cluster_logprob + head_logprob shortlist_size + i unsqueeze out start_idx stop_idx = output_logprob out log_prob input Tensor - Tensor r Compute log probabilities all math ` \texttt n\_classes ` Args input Tensor minibatch examples Returns log-probabilities each math ` c ` range math ` = c = \texttt n\_classes ` where math ` \texttt n\_classes ` parameter passed ` ` AdaptiveLogSoftmaxWithLoss ` ` constructor Shape - Input math ` N \texttt in\_features ` - Output math ` N \texttt n\_classes ` head_output = head input _get_full_log_prob input head_output predict input Tensor - Tensor r Return highest probability each example input minibatch This equivalent ` ` log_prob input argmax dim= ` ` more efficient some cases Args input Tensor minibatch examples Returns output Tensor highest probability each example Shape - Input math ` N \texttt in\_features ` - Output math ` N ` head_output = head input output = torch argmax head_output dim= not_in_shortlist = output = shortlist_size all_in_shortlist = not_in_shortlist any all_in_shortlist output not_in_shortlist all log_prob = _get_full_log_prob input head_output torch argmax log_prob dim= log_prob = _get_full_log_prob input not_in_shortlist head_output not_in_shortlist output not_in_shortlist = torch argmax log_prob dim= output