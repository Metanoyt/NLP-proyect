mypy allow-untyped-defs contextlib logging math warnings collections abc Callable Generator Iterator typing Any cast no_type_check torch torch distributed dist torch distributed algorithms _checkpoint checkpoint_wrapper checkpoint_wrapper torch nn nn torch nn functional F torch distributed _shard sharded_tensor init_from_local_shards Shard ShardedTensor torch distributed fsdp _common_utils _FSDPState _get_module_fsdp_state_if_fully_sharded_module _has_fsdp_params _is_composable _module_handle clean_tensor_name FSDP_PREFIX FSDP_WRAPPED_MODULE torch distributed fsdp _debug_utils SimpleProfiler torch distributed fsdp _runtime_utils _cast_buffers_to_dtype_and_device _get_orig_buffer_dtypes _lazy_init _reset_flat_param_grad_info_if_needed torch distributed fsdp api FullStateDictConfig ShardingStrategy StateDictType torch distributed tensor DTensor torch distributed utils _replace_by_prefix _fsdp_extensions _ext_all_gather_dtensor _ext_chunk_dtensor _ext_chunk_tensor _ext_post_unflatten_transform _ext_pre_load_state_dict_transform _unshard_param_utils _unshard_fsdp_state_params FLAT_PARAM logger = logging getLogger __name__ _should_unshard_params fsdp_state _FSDPState - bool fsdp_state sharding_strategy == ShardingStrategy NO_SHARD _is_composable fsdp_state fsdp_state _use_orig_params _convert_to_wrapped_module_name module_name str - str module_name = module_name replace f FSDP_PREFIX module_name = module_name replace f FSDP_WRAPPED_MODULE module_name module_name = f module_name ` CheckpointWrapper ` adds prefix has removed well module_name = module_name replace checkpoint_wrapper _CHECKPOINT_PREFIX module_name _param_name_infos module nn Module fsdp_state _FSDPState - Iterator tuple str str str _has_fsdp_params fsdp_state module param_name module_name _module_handle fsdp_state module param_module_names module_name = _convert_to_wrapped_module_name module_name fqn = f module_name param_name yield fqn param_name module_name _shared_param_name_infos module nn Module fsdp_state - Iterator tuple str str str param_name module_name _module_handle fsdp_state module shared_param_module_names module_name = _convert_to_wrapped_module_name module_name fqn = f module_name param_name yield fqn param_name module_name no_type_check _enter_unshard_params_ctx module nn Module fsdp_state _FSDPState writeback bool = False rank _only bool = False offload_to_cpu bool = False with_grads bool = False - None state_dict hooks cannot use pure context call checkpoint flow requires enter context pre-hook leave context post-hook This API enters context ` ` _unshard_fsdp_state_params ` ` module fsdp_state _unshard_params_ctx raise AssertionError Entering ` ` _unshard_fsdp_state_params ` ` context _unshard_params_ctx module None fsdp_state _unshard_params_ctx module = _unshard_fsdp_state_params module fsdp_state writeback=writeback rank _only=rank _only offload_to_cpu=offload_to_cpu with_grads=with_grads fsdp_state _unshard_params_ctx module __enter__ no_type_check _exit_unshard_params_ctx module nn Module fsdp_state _FSDPState - None A helper function exit ` ` _unshard_fsdp_state_params ` ` context fsdp_state _unshard_params_ctx module __exit__ None None None fsdp_state _unshard_params_ctx pop module _common_pre_state_dict_hook module nn Module fsdp_state _FSDPState - None Performs pre-state_dict tasks shared all state_dict types fsdp_state _device_handle is_available fsdp_state _device_handle synchronize TODO need check always correct composable FSDP _lazy_init fsdp_state module fsdp_state _is_root _reset_flat_param_grad_info_if_needed fsdp_state _all_handles _common_unshard_pre_state_dict_hook module nn Module fsdp_state _FSDPState offload_to_cpu bool rank _only bool - None Performs pre-state_dict tasks shared all state_dict types require ` ` _unshard_fsdp_state_params ` ` FULL_STATE_DICT SHARDED_STATE_DICT use hook For composable ` fully_shard ` does need unshard parameters ` NO_SHARD ` cases _should_unshard_params fsdp_state _enter_unshard_params_ctx module fsdp_state writeback=False offload_to_cpu=offload_to_cpu rank _only=rank _only no_type_check _common_unshard_post_state_dict_hook module nn Module fsdp_state _FSDPState state_dict dict str Any prefix str param_hook Callable - dict str Any The post-state_dict flow shared all state_dict types require ` ` _unshard_fsdp_state_params ` ` FULL_STATE_DICT SHARDED_STATE_DICT use hook _replace_by_prefix state_dict prefix + f FSDP_PREFIX prefix Return early trivial cases state_dict _has_fsdp_params fsdp_state module _should_unshard_params fsdp_state _exit_unshard_params_ctx module fsdp_state state_dict If rank does have unsharded parameters when ` rank _only=True ` ` rank = ` then rank only needed participate all-gather does need save state dict We simply check rank _only ensure issue rank _only = fsdp_state _state_dict_type == StateDictType FULL_STATE_DICT cast FullStateDictConfig fsdp_state _state_dict_config rank _only no_fsdp_return means state_dict returned rank should contain only non-FSDP controlled parameters buffers no_fsdp_return = rank _only fsdp_state rank = no_fsdp_return fsdp_state _use_orig_params clean_key fsdp_state _buffer_names This hack support activation checkpoint clean_key = clean_key replace f checkpoint_wrapper _CHECKPOINT_PREFIX state_dict pop f prefix clean_key None Non-zero ranks have flat_param key when rank _only=True because rank _only=True passed unshard context nonzero ranks reshard early causing flat_param appear state_dict state_dict pop f prefix FLAT_PARAM _exit_unshard_params_ctx module fsdp_state state_dict Loop only parameters saved instance s wrapped module avoid processing buffers fqn param_name module_name _param_name_infos module fsdp_state fqn = f prefix fqn no_fsdp_return state_dict pop fqn continue fqn state_dict raise AssertionError f FSDP assumes fqn state_dict state_dict only f has state_dict keys f prefix= prefix module_name= module_name f param_name= param_name rank= fsdp_state rank param_hook state_dict prefix fqn _should_unshard_params fsdp_state _exit_unshard_params_ctx module fsdp_state cpu_device = torch device cpu buffer_clean_fqns = buffers = clean_key fsdp_state _buffer_names This hack support activation checkpoint clean_key = clean_tensor_name clean_key fqn = f prefix clean_key fqn state_dict A buffer can registered non-persistent continue no_fsdp_return state_dict pop fqn buffer = state_dict fqn fsdp_state _state_dict_config offload_to_cpu buffer device = cpu_device state_dict fqn = buffer cpu_device skip upcasting ignored buffers clean_key fsdp_state _ignored_buffer_names buffer_clean_fqns append clean_key buffers append state_dict fqn buffers mixed_precision_enabled_for_buffers = fsdp_state _mixed_precision_enabled_for_buffers _is_composable fsdp_state fsdp_state mixed_precision buffer_dtype None mixed_precision_enabled_for_buffers buffer_dtypes = _get_orig_buffer_dtypes fsdp_state buffer_clean_fqns _cast_buffers_to_dtype_and_device buffers buffer_dtypes fsdp_state compute_device buffer clean_fqn zip buffers buffer_clean_fqns fqn = f prefix clean_fqn logger info FSDP casting dtype s s fqn buffer dtype state_dict fqn = buffer clone state_dict no_type_check _full_pre_state_dict_hook fsdp_state _FSDPState module nn Module args kwargs - None Hook runs before model state_dict called pre-state_dict hook actually supported ` ` nn Module ` ` As result API called ` ` _full_post_state_dict_hook ` ` simulate case Once pre-state_dict supported ` ` nn Module ` ` hook will registered hook ` ` nn Module ` ` getattr fsdp_state _device_mesh False fsdp_state _device_mesh _get_root_mesh _common_pre_state_dict_hook module fsdp_state _common_unshard_pre_state_dict_hook module fsdp_state offload_to_cpu=fsdp_state _state_dict_config offload_to_cpu rank _only=cast FullStateDictConfig fsdp_state _state_dict_config rank _only no_type_check _full_post_state_dict_hook module nn Module fsdp_state _FSDPState state_dict dict str Any prefix str - dict str Any Hook runs after model state_dict called before returning result user For FSDP we may have clone tensors state_dict params go back sharded version after _unshard_fsdp_state_params ends also remove ` ` FSDP_WRAPPED_MODULE ` ` prefix param_hook state_dict dict str Any prefix str fqn str - None clean_key = fqn clean_prefix = clean_tensor_name prefix Strip prefix out key needed buffer names param names do have prefix considered they computed ` state_dict ` call clean_key = clean_key removeprefix clean_prefix Clone parameters before exiting ` _unshard_fsdp_state_params ` context getattr state_dict fqn _has_been_cloned False try state_dict fqn = state_dict fqn detach clone state_dict fqn _has_been_cloned = True type ignore attr-defined except BaseException e noqa B warnings warn f Failed clone tensor name fqn rank fsdp_state rank This may mean state_dict entry could point invalid memory regions after returning state_dict call parameter managed FSDP Please check clone f implementation fqn Error str e stacklevel= _common_unshard_post_state_dict_hook module fsdp_state state_dict prefix param_hook _full_pre_load_state_dict_hook module nn Module fsdp_state _FSDPState state_dict dict str Any prefix str - None _lazy_init fsdp_state module _should_unshard_params fsdp_state SimpleProfiler profile _enter_unshard_params_ctx _enter_unshard_params_ctx module fsdp_state writeback=True Add FSDP_PREFIX only wrapper-based FSDP _is_composable fsdp_state _replace_by_prefix state_dict prefix prefix + f FSDP_PREFIX _full_post_load_state_dict_hook module nn Module fsdp_state _FSDPState args kwargs - None _should_unshard_params fsdp_state SimpleProfiler profile _exit_unshard_params_ctx _exit_unshard_params_ctx module fsdp_state _local_pre_state_dict_hook fsdp_state _FSDPState module nn Module args kwargs - None Hook runs before model state_dict called Right now pre-state_dict hook supported PyTorch core So API called ` _local_post_state_dict_hook ` simulate case _has_fsdp_params fsdp_state module _module_handle fsdp_state module uses_sharded_strategy raise RuntimeError ` ` local_state_dict ` ` can only used when parameters flatten sharded _common_pre_state_dict_hook module fsdp_state no_type_check _local_post_state_dict_hook module nn Module fsdp_state _FSDPState state_dict dict str Any prefix str - dict str Any This hook create ShardedTensor local flat_param replace state_dict f prefix FLAT_PARAM ShardedTensor No copy will happen The underlying storage same _replace_by_prefix state_dict f prefix FSDP_PREFIX prefix _has_fsdp_params fsdp_state module state_dict state_dict f prefix FLAT_PARAM exists has same tensor value flat_param pure Tensor because nn Module state_dict will detach parameter Therefore we need get flat_param get metadata _module_handle fsdp_state module raise AssertionError Should have returned early flat_param = _module_handle fsdp_state module flat_param Constructs ShardedTensor flat_param without padding Removing padding allows users change number ranks when loading local_state_dict full_numel = flat_param _unpadded_unsharded_size numel type ignore attr-defined shard_offset = flat_param numel fsdp_state rank valid_data_size = flat_param numel - flat_param _shard_numel_padded valid_data_size If FlatParameter returned FlatParameter _local_shard cause pickling issue can torch save torch load Since there no benefit state_dict actual FlatParameter view which tensor FlatParameter will returned flat_param = flat_param valid_data_size view valid_data_size local_shards = Shard from_tensor_and_offsets flat_param shard_offset fsdp_state rank local_shards = sharded_tensor = init_from_local_shards local_shards full_numel process_group=fsdp_state process_group type ignore assignment TODO Add DTensor state_dict support LOCAL_STATE_DICT fsdp_state _state_dict_config offload_to_cpu sharded_tensor = sharded_tensor cpu state_dict f prefix FLAT_PARAM = sharded_tensor state_dict _local_post_load_state_dict_hook module nn Module fsdp_state _FSDPState args kwargs - None pass _local_pre_load_state_dict_hook module nn Module fsdp_state _FSDPState state_dict dict str Any prefix str - None This hook finds local flat_param FSDP module state_dict The flat_param should ShardedTensor This hook converts ShardedTensor tensor No copy happen unless padding required _lazy_init fsdp_state module _replace_by_prefix state_dict prefix f prefix FSDP_PREFIX fqn = f prefix FSDP_PREFIX FLAT_PARAM fqn state_dict _has_fsdp_params fsdp_state module raise AssertionError No ` FlatParameter ` ` state_dict ` FSDP instance has parameters load_tensor = state_dict fqn isinstance load_tensor ShardedTensor raise AssertionError Tensors local_state_dict should ShardedTensor Convert ShardedTensor Tensor flat_param = _module_handle fsdp_state module flat_param flat_param None raise AssertionError Expected flat_param set valid_data_size = flat_param numel - flat_param _shard_numel_padded shards = load_tensor local_shards valid_data_size len shards raise AssertionError load_local_state_dict assume one shard per ShardedTensor load_tensor = shards tensor Get metadata flat_param decide whether pad loaded tensor flat_param _shard_numel_padded load_tensor numel = flat_param numel raise AssertionError f Local shard size = flat_param numel tensor f state_dict load_tensor numel load_tensor = F pad load_tensor flat_param _shard_numel_padded load_tensor = flat_param TODO Add DTensor state_dict support LOCAL_STATE_DICT state_dict fqn = load_tensor _sharded_pre_state_dict_hook fsdp_state _FSDPState module nn Module args kwargs - None Hook runs before model state_dict called Check ` ` _full_pre_load_state_dict_hook ` ` detail _has_fsdp_params fsdp_state module _module_handle fsdp_state module uses_sharded_strategy raise RuntimeError ` ` sharded_state_dict ` ` can only used when parameters flatten sharded _common_pre_state_dict_hook module fsdp_state Setting offload_to_cpu here does work even offload_to_cpu True We have create ShardedTensor first then move CPU _common_unshard_pre_state_dict_hook module fsdp_state offload_to_cpu=False rank _only=False no_type_check _sharded_post_state_dict_hook module nn Module fsdp_state _FSDPState state_dict dict str Any prefix str - dict str Any The hook replaces unflattened unsharded parameter state_dict unflattened sharded parameter ShardedTensor param_hook state_dict dict str Any prefix str fqn str param = state_dict fqn fsdp_state _state_dict_config _use_dtensor sharded_tensor = _ext_chunk_tensor tensor=param rank=fsdp_state rank world_size=fsdp_state world_size num_devices_per_node=fsdp_state _device_handle device_count pg=fsdp_state process_group fsdp_extension=fsdp_state _fsdp_extension sharded_tensor = _ext_chunk_dtensor tensor=param rank=fsdp_state rank device_mesh=fsdp_state _device_mesh fsdp_extension=fsdp_state _fsdp_extension fsdp_state _state_dict_config offload_to_cpu sharded_tensor = sharded_tensor cpu state_dict fqn = sharded_tensor _common_unshard_post_state_dict_hook module fsdp_state state_dict prefix param_hook no_type_check _sharded_post_load_state_dict_hook module nn Module fsdp_state _FSDPState args kwargs - None _has_fsdp_params fsdp_state module SimpleProfiler profile _exit_unshard_params_ctx _exit_unshard_params_ctx module fsdp_state no_type_check _sharded_pre_load_state_dict_hook module nn Module fsdp_state _FSDPState state_dict dict str Any prefix str - None The hook combines unflattened sharded parameters ShardedTensor new FlatParameter shards new FlatParameter local chunk _lazy_init fsdp_state module _is_composable fsdp_state _replace_by_prefix state_dict prefix prefix + f FSDP_PREFIX _has_fsdp_params fsdp_state module handle = _module_handle fsdp_state module handle uses_sharded_strategy raise RuntimeError load_sharded_state_dict can only called when parameters flattened sharded fqn_to_param_ext = dict zip handle flat_param _fqns handle flat_param _param_extensions fqn _ _ _param_name_infos module fsdp_state _is_composable fsdp_state fqn_from_global_root = f prefix FSDP_PREFIX fqn fqn_from_global_root = f prefix fqn try param = state_dict pop fqn_from_global_root except KeyError logger warning f Did find param FQN fqn_from_global_root skipping noqa G The weight will filled you expect continue TODO Improve unittesting state_dict finetuning cases https github com pytorch pytorch issues fsdp_state _state_dict_config _use_dtensor All-gather param ShardedTensor param shards = _ext_pre_load_state_dict_transform param fsdp_state _fsdp_extension len shards = raise AssertionError Expects shard per rank f got len shards shards rank fsdp_state rank param_numel = param size numel dim_ _size = param size chunk_size = math ceil dim_ _size fsdp_state world_size param_numel dim_ _size len shards == local_tensor = shards tensor flatten SimpleProfiler profile SimpleProfiler Type H D local_tensor = local_tensor fsdp_state compute_device num_padding = chunk_size - local_tensor numel num_padding local_tensor = F pad local_tensor num_padding local_tensor = torch zeros chunk_size dtype=param dtype device=fsdp_state compute_device tensor = torch empty chunk_size fsdp_state world_size dtype=local_tensor dtype device=fsdp_state compute_device SimpleProfiler profile SimpleProfiler Type ALLGATHER dist all_gather_into_tensor tensor local_tensor group=fsdp_state process_group tensor = tensor narrow param_numel reshape param size state_dict fqn_from_global_root = tensor param device = fsdp_state _device_mesh device_type param = param fsdp_state _device_mesh device_type root_mesh = fsdp_state _device_mesh _get_root_mesh local_tensor = _ext_all_gather_dtensor param root_mesh fsdp_state _fsdp_extension fqn_to_param_ext get fqn None ext = fqn_to_param_ext fqn local_tensor = _ext_post_unflatten_transform local_tensor ext fsdp_state _fsdp_extension state_dict fqn_from_global_root = local_tensor SimpleProfiler profile _enter_unshard_params_ctx _enter_unshard_params_ctx module fsdp_state writeback=True contextlib contextmanager _replace_with_full_state_dict_type fsdp_state _FSDPState - Generator old_state_dict_config = fsdp_state _state_dict_config old_state_dict_type = fsdp_state _state_dict_type fsdp_state _state_dict_config = FullStateDictConfig fsdp_state _state_dict_type = StateDictType FULL_STATE_DICT yield fsdp_state _state_dict_config = old_state_dict_config fsdp_state _state_dict_type = old_state_dict_type no_type_check torch no_grad _post_state_dict_hook module nn Module state_dict dict str Any prefix str args Any - dict str Any _post_state_dict_hook called after state_dict FSDP module executed ` ` fsdp_state _state_dict_type ` ` used decide what postprocessing will done fsdp_state = _get_module_fsdp_state_if_fully_sharded_module module fsdp_state sharding_strategy == ShardingStrategy NO_SHARD context = _replace_with_full_state_dict_type fsdp_state warnings warn When using ` ` NO_SHARD ` ` ` ` ShardingStrategy ` ` full_state_dict will returned stacklevel= context = contextlib nullcontext context _post_state_dict_hook_fn = StateDictType FULL_STATE_DICT _full_post_state_dict_hook StateDictType LOCAL_STATE_DICT _local_post_state_dict_hook StateDictType SHARDED_STATE_DICT _sharded_post_state_dict_hook processed_state_dict = _post_state_dict_hook_fn fsdp_state _state_dict_type module fsdp_state state_dict prefix fsdp_state _is_root logger info FSDP finished processing state_dict prefix= s prefix key tensor sorted processed_state_dict items key startswith prefix isinstance tensor torch Tensor local_shape = tensor shape device = None isinstance tensor ShardedTensor local_shape = None shards = tensor local_shards shards local_shape = shards tensor shape device = shards tensor device isinstance tensor DTensor local_shape = tensor to_local shape device = tensor device device = tensor device logger info FQN= s type= s shape= s local_shape= s dtype= s device= s key type tensor tensor shape local_shape tensor dtype device processed_state_dict no_type_check torch no_grad _pre_state_dict_hook module nn Module args kwargs - None This called before core state dict saving logic ` ` module ` ` ` ` fsdp_state _state_dict_type ` ` used decide what postprocessing will done fsdp_state = _get_module_fsdp_state_if_fully_sharded_module module fsdp_state sharding_strategy == ShardingStrategy NO_SHARD context = _replace_with_full_state_dict_type fsdp_state warnings warn When using ` ` NO_SHARD ` ` ` ` ShardingStrategy ` ` full_state_dict will returned stacklevel= _set_use_dtensor fsdp_state context = contextlib nullcontext context _pre_state_dict_hook_fn = StateDictType FULL_STATE_DICT _full_pre_state_dict_hook StateDictType LOCAL_STATE_DICT _local_pre_state_dict_hook StateDictType SHARDED_STATE_DICT _sharded_pre_state_dict_hook _pre_state_dict_hook_fn fsdp_state _state_dict_type fsdp_state module args kwargs no_type_check _set_use_dtensor fsdp_state _FSDPState - None If device_mesh passed when initializing FSDP we automatically turn _use_dtensor flag true ShardedStateDictConfig getattr fsdp_state _device_mesh None state_dict_type = fsdp_state _state_dict_type state_dict_type == StateDictType LOCAL_STATE_DICT raise RuntimeError Found state_dict_type LOCAL_STATE_DICT DeviceMesh compatible LOCAL_STATE_DICT Please set state_dict_type SHARDED_STATE_DICT get DTensor state_dict fsdp_state _state_dict_config _use_dtensor = True no_type_check torch no_grad _pre_load_state_dict_hook module nn Module state_dict dict str Any prefix str args Any - None This called before ` ` module _load_from_state_dict ` ` ` ` fsdp_state _state_dict_type ` ` used decide what preprocessing will done fsdp_state = _get_module_fsdp_state_if_fully_sharded_module module fsdp_state sharding_strategy == ShardingStrategy NO_SHARD context = _replace_with_full_state_dict_type fsdp_state warnings warn When using ` ` NO_SHARD ` ` ` ` ShardingStrategy ` ` full_state_dict will returned stacklevel= _set_use_dtensor fsdp_state context = contextlib nullcontext _lazy_init fsdp_state module fsdp_state _is_root SimpleProfiler reset context _pre_load_state_dict_hook_fn = StateDictType FULL_STATE_DICT _full_pre_load_state_dict_hook StateDictType LOCAL_STATE_DICT _local_pre_load_state_dict_hook StateDictType SHARDED_STATE_DICT _sharded_pre_load_state_dict_hook Code common all state_dict impls fsdp_state _device_handle is_available fsdp_state _device_handle synchronize Dispatch into state_dict specific implementation pre-hook _pre_load_state_dict_hook_fn fsdp_state _state_dict_type module fsdp_state state_dict prefix no_type_check torch no_grad _post_load_state_dict_hook module nn Module incompatible_keys tuple list str list str args Any - None fsdp_state = _get_module_fsdp_state_if_fully_sharded_module module fsdp_state sharding_strategy == ShardingStrategy NO_SHARD context = _replace_with_full_state_dict_type fsdp_state warnings warn When using ` ` NO_SHARD ` ` ` ` ShardingStrategy ` ` full_state_dict will returned stacklevel= context = contextlib nullcontext context _post_load_state_dict_hook_fn = StateDictType FULL_STATE_DICT _full_post_load_state_dict_hook StateDictType LOCAL_STATE_DICT _local_post_load_state_dict_hook StateDictType SHARDED_STATE_DICT _sharded_post_load_state_dict_hook Code common all state_dict impls Dispatch into state_dict type specific implementation post-hook loading state_dict _post_load_state_dict_hook_fn fsdp_state _state_dict_type module fsdp_state When reporting incompatible keys trim FSDP prefixes missing_keys = incompatible_keys unexpected_keys = incompatible_keys i range len missing_keys missing_keys i = clean_tensor_name missing_keys i i range len unexpected_keys unexpected_keys i = clean_tensor_name unexpected_keys i fsdp_state _is_root SimpleProfiler dump_and_reset FSDP model load_state_dict profiling _register_all_state_dict_hooks state _FSDPState Registers pre-save post-save pre-load post-load state dict hooks hook_registration_fn_str hook hook_registration_fn_kwargs register_state_dict_pre_hook _pre_state_dict_hook _register_state_dict_hook _post_state_dict_hook _register_load_state_dict_pre_hook _pre_load_state_dict_hook with_module True register_load_state_dict_post_hook _post_load_state_dict_hook _register_state_dict_hooks_base state hook_registration_fn_str hook hook_registration_fn_kwargs no_type_check _register_state_dict_hooks_base state _FSDPState hook_registration_fn_name str hook Callable hook_registration_fn_kwargs dict str Any - None Registers ` ` hook ` ` using ` ` hook_registration_fn ` ` _is_composable state getattr state hook_registration_fn_name hook hook_registration_fn_kwargs handle = state _handle handle getattr handle _fully_sharded_module hook_registration_fn_name hook hook_registration_fn_kwargs