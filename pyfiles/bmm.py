mypy allow-untyped-defs logging typing TYPE_CHECKING Union torch torch _dynamo utils counters torch _inductor codegen rocm ck_universal_gemm_template CKGemmTemplate config inductor_config ir lowering L kernel_inputs MMKernelInputs lowering lowerings make_pointwise make_reduction transform_args select_algorithm autotune_select_algorithm ExternKernelChoice SymbolicGridFn TritonTemplate utils _use_cutlass_for_op use_aten_gemm_kernels use_ck_gemm_template use_cpp_bmm_template use_cutlass_template use_triton_template virtualized ops V mm_common _is_static_problem is_batch_stride_largest_or_zero mm_args use_native_matmul TYPE_CHECKING ir ChoiceCaller select_algorithm KernelTemplate log = logging getLogger __name__ aten = torch ops aten SymbolicGridFn bmm_grid b m n meta cdiv cdiv m meta BLOCK_M cdiv n meta BLOCK_N b bmm_template = TritonTemplate name= bmm grid=bmm_grid source=r def_kernel A B M = size A - N = size B - K = size A - stride_aq = stride A stride_am = stride A stride_ak = stride A stride_bq = stride B stride_bk = stride B stride_bn = stride B based triton ops matmul pid = tl program_id INDEX_DTYPE grid_m = M + BLOCK_M - BLOCK_M grid_n = N + BLOCK_N - BLOCK_N re-order program ID better L performance width = GROUP_M grid_n group_id = pid width group_size = min grid_m - group_id GROUP_M GROUP_M pid_m = group_id GROUP_M + pid group_size pid_n = pid width group_size tl assume pid_m = tl assume pid_n = rm = pid_m BLOCK_M + tl arange BLOCK_M rn = pid_n BLOCK_N + tl arange BLOCK_N stride_am == stride_ak == M stride_am == K stride_ak == ram = tl max_contiguous tl multiple_of rm M BLOCK_M BLOCK_M ram = rm M stride_bk == stride_bn == K stride_bk == N stride_bn == rbn = tl max_contiguous tl multiple_of rn N BLOCK_N BLOCK_N rbn = rn N rk = tl arange BLOCK_K idx_q = tl program_id INDEX_DTYPE batch dimension BMM A = A + ram None stride_am + rk None stride_ak + idx_q stride_aq B = B + rk None stride_bk + rbn None stride_bn + idx_q stride_bq acc = tl zeros BLOCK_M BLOCK_N dtype=ACC_TYPE k range K -BLOCK_K EVEN_K = tl load A b = tl load B = tl load A mask=rk None k other= b = tl load B mask=rk None k other= acc += tl dot b allow_tf =ALLOW_TF A += BLOCK_K stride_ak B += BLOCK_K stride_bk rematerialize rm rn save registers rm = pid_m BLOCK_M + tl arange BLOCK_M rn = pid_n BLOCK_N + tl arange BLOCK_N idx_q = tl program_id INDEX_DTYPE batch dimension BMM idx_m = rm None idx_n = rn None mask = idx_m M idx_n N inductor generates suffix store_output idx_q idx_m idx_n acc mask val_shape= BLOCK_M BLOCK_N cache_codegen_enabled_for_template=True aten_bmm = ExternKernelChoice torch bmm bmm_out op_overload=aten bmm out aten_bmm_dtype = ExternKernelChoice torch bmm _bmm_out_dtype_cuda name= bmm_dtype op_overload=aten bmm dtype_out aten_baddbmm = ExternKernelChoice torch baddbmm baddbmm_out op_overload=aten baddbmm out L register_lowering aten bmm tuned_bmm mat mat out_dtype=None layout=None Lowering autotuning aten bmm different backends Aten Triton CUTLASS etc all x get_device type == cpu x mat mat decompose small ops when memory bound mat get_size == mat get_size == mat = L unsqueeze mat - mat = L unsqueeze mat L sum_ L mul mat mat axis= is_valid_to_require_contiguous t ir is_storage_and_layout t True _ layout = ir as_storage_and_layout t freeze=False isinstance layout ir FlexibleLayout is_preferred_layout_as_bmm_input sizes strides contiguous one last two dims strides - == sizes - == strides - = sizes - strides - == sizes - == strides - = sizes - Make input bmm contiguous contiguous either last two dims because bmm cpu implementation would do contiguous This avoid additional copies bmm may_require_contiguous t meta_t sizes = meta_t meta val size strides = meta_t meta val stride is_preferred_layout_as_bmm_input sizes strides t = ir ExternKernel require_contiguous t t is_valid_to_require_contiguous mat meta_mat = V graph current_node args mat = may_require_contiguous mat meta_mat is_valid_to_require_contiguous mat meta_mat = V graph current_node args mat = may_require_contiguous mat meta_mat use_native_matmul mat mat mat = lowerings aten unsqueeze mat - mat = lowerings aten unsqueeze mat args kwargs = transform_args args= mat mat kwargs= broadcast=True type_promotion_kind=None convert_input_to_bool=False Handles broadcasting arguments inductor_config triton codegen_upcast_to_fp mat dtype torch float torch bfloat _to_dtype x ops to_dtype x mat dtype use_compute_types=False args = make_pointwise _to_dtype x x args mul_pointwise = make_pointwise ops dot args dot_reduction = make_reduction dot mul_pointwise dot_reduction TODO coconutruben integrate into MMKernelInputs when all callsites use m n k layout mat mat = mm_args mat mat layout=layout out_dtype=out_dtype name = bmm Create MMKernelInputs BMM top kernel_inputs = MMKernelInputs mat mat out_dtype=out_dtype below getting overview logging info inductor mms batch_size = mat get_size Extract batch dimension counters aten_mm_info f aten bmm_ batch_size _ m _ n _ k += log info Tuned aten bmm batch= s m= s n= s k= s mat _dtype= s mat _dtype= s output_layout= s batch_size m n k mat get_dtype mat get_dtype layout aten_handler ExternKernelChoice = aten_bmm aten_extra_kwargs = out_dtype assert mat get_device type == cuda out_dtype only supported CUDA aten_handler = aten_bmm_dtype aten_extra_kwargs = out_dtype out_dtype choices list ChoiceCaller = Collect all templates unified call templates_to_use list Union ExternKernelChoice KernelTemplate = kwarg_overrides = use_aten_gemm_kernels templates_to_use append aten_handler kwarg_overrides aten_handler uid = aten_extra_kwargs use_triton_template layout check_max_autotune=False out_dtype None out_dtype == mat get_dtype TODO add out_dtype support Triton Template templates_to_use append bmm_template Single unified call all templates choices extend V choices get_template_configs kernel_inputs templates_to_use name kwarg_overrides=kwarg_overrides _ is_nonzero = _is_static_problem layout batch_stride_largest_or_zero = is_batch_stride_largest_or_zero mat mat layout batch_stride_largest_or_zero is_nonzero use_cutlass_template layout m n k _use_cutlass_for_op name codegen cuda gemm_template CUTLASS xGemmTemplate CUTLASS xGemmTemplate add_cutlass_gemm_choices choices layout kernel_inputs nodes type ignore arg-type use_cpp_bmm_template layout mat mat codegen cpp_bmm_template CppBmmTemplate CppBmmTemplate add_choices choices layout kernel_inputs nodes use_ck_gemm_template layout m n k CKGemmTemplate add_ck_gemm_choices choices layout kernel_inputs nodes autotune_select_algorithm name choices kernel_inputs nodes layout L register_lowering aten baddbmm tuned_baddbmm inp mat mat alpha= beta= layout=None Lowering autotuning aten mm different backends Aten Triton CUTLASS etc use_native_matmul mat mat beta == arg = arg = lowerings aten mul beta inp alpha == arg = arg = lowerings aten mul alpha lowerings aten bmm mat mat lowerings aten add arg arg TODO coconutruben integrate into MMKernelInputs when all callsites use m n k layout mat mat inp = mm_args mat mat inp layout=layout Create MMKernelInputs BadDBMM top kernel_inputs = MMKernelInputs inp mat mat scalars=dict alpha=alpha beta=beta below getting overview logging info inductor mms batch_size = mat get_size counters aten_mm_info f aten baddbmm_ batch_size _ m _ n _ k += log info Tuned aten baddbmm batch_size= s m= s n= s k= s mat _dtype= s mat _dtype= s inp= s output_layout= s batch_size m n k mat get_dtype mat get_dtype inp get_dtype layout name = baddbmm options tune choices list ChoiceCaller = Collect all templates unified call templates_to_use list Union ExternKernelChoice KernelTemplate = use_aten_gemm_kernels templates_to_use append aten_baddbmm use_triton_template layout check_max_autotune=False templates_to_use append bmm_template Single unified call all templates choices extend V choices get_template_configs kernel_inputs templates_to_use name autotune_select_algorithm name choices kernel_inputs nodes layout