Owner s module dynamo datetime functools unittest collections Counter typing Optional unittest mock patch torch torch _dynamo torch _dynamo logging torch _dynamo test_case torch distributed c d some reason importing functional collectives after dynamo breaks collectives handling torch distributed _functional_collectives _functional_collectives torch _C FileCheck torch _dynamo testing CompileCounter torch _dynamo utils same torch _inductor comms _reorder_communication_preserving_peak_memory_internal ReorderInfo sink_waits_iterative torch _inductor compile_fx compile_fx inductor_compile_fx torch _inductor scheduler _get_mm_like_fn BaseSchedulerNode get_estimate_runtime_cache get_estimate_runtime_cache_key_from_snode torch _inductor utils fresh_inductor_cache run_and_get_triton_code torch distributed distributed_c d GroupMember torch fx experimental proxy_tensor make_fx torch testing _internal common_cuda SM OrLater torch testing _internal common_distributed _dynamo_dist_per_rank_init DynamoDistributedMultiProcTestCase DynamoDistributedSingleProcTestCase MultiProcessTestCase requires_accelerator_dist_backend skip_if_lt_x_gpu torch testing _internal common_utils instantiate_parametrized_tests parametrize skipIfRocm skipIfXpu TEST_XPU xfailIf torch testing _internal inductor_utils HAS_GPU torch utils _python_dispatch TorchDispatchMode requires_accelerator_dist_backend nccl xccl instantiate_parametrized_tests TestCollectivesMultiProc DynamoDistributedMultiProcTestCase Run correctness checks multi-proc runner mark minimum GPUs run under device = acc type acc = torch accelerator current_accelerator cpu get_world_trs tag ranks list range world_size group_size world_size property world_size - int hack no matter whether we have gpus just run works around issue skipif workers unpredictable #s gpu unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu test_broadcast_inductor Testing broadcast works correctly when using inductor example tensor src tag ranks group_size res = torch ops c d_functional broadcast tensor src tag ranks group_size res = torch ops c d_functional wait_tensor res res compile func example_inputs graph = make_fx func example_inputs inductor_compile_fx graph example_inputs _dynamo_dist_per_rank_init rank world_size example = functools partial example get_world_trs t = torch randn device=self device inputs = t rank == torch zeros device=self device eager_out = example inputs assertTrue same t eager_out compiled_func = compile example inputs compiled_out = compiled_func inputs assertTrue same eager_out compiled_out unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu test_allreduce_inductor This matmul cat allreduce pattern we aim optimize matmul_cat_col b c d e f tag ranks group_size x = torch matmul b y = torch matmul c d z = torch cat x y ar = torch ops c d_functional all_reduce z sum tag ranks group_size g = torch matmul e f ar = torch ops c d_functional wait_tensor ar out = torch add ar g repeat out compile func example_inputs graph = make_fx func example_inputs inductor_compile_fx graph example_inputs _dynamo_dist_per_rank_init rank world_size matmul_cat_col = functools partial matmul_cat_col get_world_trs inputs = torch ones device=self device + rank eager_out = matmul_cat_col inputs compiled_matmul_cat_col = compile matmul_cat_col inputs inductor_out = compiled_matmul_cat_col inputs assertTrue same eager_out inductor_out tol= unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu test_allreduce_inductor_cudagraph_trees Tests whether cudagraph trees support all_reduce nccl torch distributed dist dist all_reduce inplace op eager mode functionanlized op compiled mode so we define eager_func func separately same semantic eager_func x y = x x dist all_reduce y op=dist ReduceOp SUM x = torch nn functional silu x x y func x y = x x y = dist all_reduce y op=dist ReduceOp SUM x = torch nn functional silu x x y options = triton cudagraphs True triton cudagraph_trees True _dynamo_dist_per_rank_init rank world_size compiled_func = torch compile func backend= inductor fullgraph=True options=options dynamic=None nelem CI Tesla T does support bfloat compilation natively using float x = torch randn nelem device=self device dtype=torch float golden_out = eager_func x _ range compiled_out = compiled_func x assertEqual golden_out compiled_out test_c d_functional_tagged_pt _compliant op = torch ops _c d_functional all_reduce default assertIn torch Tag pt _compliant_tag op tags op = torch ops c d_functional all_reduce default assertIn torch Tag pt _compliant_tag op tags unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu test_eager_allreduce_inductor_wait eager_func b c d tag ranks group_size x = torch matmul b y = torch matmul c d z = torch cat x y ar = torch ops c d_functional all_reduce z sum tag ranks group_size ar inductor_func ar e f g = torch matmul e f ar = torch ops c d_functional wait_tensor ar out = torch add ar g repeat out compile func example_inputs graph = make_fx func example_inputs inductor_compile_fx graph example_inputs _dynamo_dist_per_rank_init rank world_size eager_func = functools partial eager_func get_world_trs eager_inputs = torch ones device=self device + rank inductor_inputs = torch ones device=self device + rank eager_out = inductor_func eager_func eager_inputs inductor_inputs compiled_inductor_func = compile inductor_func eager_func eager_inputs + list inductor_inputs inductor_out = compiled_inductor_func eager_func eager_inputs inductor_inputs print f eager_out eager_out print f inductor_out inductor_out assertTrue same eager_out inductor_out tol= unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu test_inductor_allreduce_eager_wait inductor_func b c d tag ranks group_size x = torch matmul b y = torch matmul c d z = torch cat x y ar = torch ops c d_functional all_reduce z sum tag ranks group_size ar eager_func ar e f g = torch matmul e f ar = torch ops c d_functional wait_tensor ar out = torch add ar g repeat out compile func example_inputs graph = make_fx func example_inputs inductor_compile_fx graph example_inputs _dynamo_dist_per_rank_init rank world_size inductor_func = functools partial inductor_func get_world_trs inductor_inputs = torch ones device=self device + rank eager_inputs = torch ones device=self device + rank eager_out = eager_func inductor_func inductor_inputs eager_inputs compiled_inductor_func = compile inductor_func inductor_inputs inductor_out = eager_func compiled_inductor_func inductor_inputs eager_inputs assertTrue same eager_out inductor_out tol= unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu xfailIf TEST_XPU https github com intel torch-xpu-ops issues skipIfRocm xfailIf TEST_XPU https github com intel torch-xpu-ops issues test_eager_async_allreduce_inductor_wait torch distributed dist torch _inductor utils run_and_get_code all_reduce_non_functional_eager x y = x x work = dist all_reduce y op=dist ReduceOp SUM async_op=True assert isinstance work torch distributed Work work y all_reduce_wait work y potentially compiled torch compiler is_dynamo_compiling torch ops c d_functional wait_tensor y work wait datetime timedelta seconds= Under compile ` wait_tensor y ` above correctly executed ` y ` s data its final form output function will match eager otherwise ` y y ` will run parallel ` all_reduce y ` output function will match eager y y _dynamo_dist_per_rank_init rank world_size x = torch ones device=self device + rank assertEqual torch _C _distributed_c d _get_work_registry_size NOTE We run iterations each ensure GPU execution way behind CPU ` y y ` CPU side will issued before ` all_reduce y ` GPU side done thus guaranteeing bad case ` y y ` GPU side will run parallel ` all_reduce y ` thus will produce wrong result fails unit test _run_loop_collective_wait x wait_fn expected_registry_size _ range assertEqual torch _C _distributed_c d _get_work_registry_size work y = all_reduce_non_functional_eager x assertEqual torch _C _distributed_c d _get_work_registry_size expected_registry_size out = wait_fn work y assertEqual torch _C _distributed_c d _get_work_registry_size work y out Test Pure-eager all_reduce_wait_eager = all_reduce_wait work y out_ref = _run_loop_collective_wait x wait_fn=all_reduce_wait_eager expected_registry_size= all_reduce_wait_compiled = torch compile all_reduce_wait backend= inductor fullgraph=True Test Issue comm eager - wait comm compile Use context manager _functional_collectives allow_inflight_collective_as_graph_input_ctx work y out_compiled = _run_loop_collective_wait x wait_fn=all_reduce_wait_compiled expected_registry_size= assertEqual out_ref out_compiled Check ` wait_tensor ` Inductor generated code _ triton_codes = run_and_get_code all_reduce_wait_compiled work y FileCheck check torch ops _c d_functional wait_tensor default run triton_codes Failure Case Issue comm eager - wait comm compile Doesn t use context manager _ _ out_compiled = _run_loop_collective_wait x wait_fn=all_reduce_wait_compiled expected_registry_size= In case ` wait_tensor y ` compiled region will able find corresponding work object invoke wait thus result will match eager assertNotEqual out_ref out_compiled unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu patch object torch _inductor config allow_buffer_reuse True test_allreduce_input_buffer_reuse func tag ranks group_size ar = _functional_collectives all_reduce sum ranks tag c = torch relu d = torch matmul c c e = d + ar e _dynamo_dist_per_rank_init rank world_size inputs = torch ones device=self device + rank compiled = torch compile func out = compiled inputs get_world_trs correct = func inputs get_world_trs assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu test_permute_tensor func tensor src_dst_pairs tag ranks group_size _functional_collectives permute_tensor tensor src_dst_pairs ranks tag _dynamo_dist_per_rank_init rank world_size inputs = rank rank torch arange dtype=torch float device=self device + rank compiled = torch compile func out = compiled inputs get_world_trs correct = func inputs get_world_trs assertTrue same out correct rank rank expected = torch arange dtype=torch float device=self device + rank - + world_size world_size assertEqual out expected assertEqual correct expected unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu patch object torch _inductor config allow_buffer_reuse True test_allgather_output_buffer_reuse Model torch nn Module __init__ args kwargs - None super __init__ args kwargs emb = torch nn Embedding forward x world_size tag ranks group_size y = emb x last_dim = y dim - res = _functional_collectives all_gather_tensor y ranks tag out = torch cat torch chunk res world_size dim= dim=last_dim out _dynamo_dist_per_rank_init rank world_size model = Model device model_compiled = torch compile model inp = torch tensor dtype=torch long device=self device out = model_compiled inp world_size get_world_trs correct = model inp world_size get_world_trs assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu test_allgather_scalar_tensor_input func tensor world_size tensor_list = torch empty_like tensor _ range world_size torch distributed all_gather tensor_list tensor tensor_list _dynamo_dist_per_rank_init rank world_size func_compiled = torch compile func inp = torch tensor rank dtype=torch long device=self device out = func_compiled inp world_size correct = func inp world_size assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu test_allgather_contiguous_input Model torch nn Module __init__ args kwargs - None super __init__ args kwargs emb = torch nn Embedding forward x world_size tag ranks group_size y = emb x last_dim = y dim - y = y transpose_ last_dim contiguous _functional_collectives all_gather_tensor y ranks tag out = y transpose_ last_dim contiguous out _dynamo_dist_per_rank_init rank world_size model = Model device model_compiled = torch compile model inp = torch tensor dtype=torch long device=self device out = model_compiled inp world_size get_world_trs correct = model inp world_size get_world_trs assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu test_allgather_into_tensor_inductor This matmul cat allreduce pattern we aim optimize example b tag ranks group_size c = torch matmul b ag = torch ops c d_functional all_gather_into_tensor c tag ranks group_size ag = torch ops c d_functional wait_tensor ag ag compile func example_inputs graph = make_fx func example_inputs inductor_compile_fx graph example_inputs _dynamo_dist_per_rank_init rank world_size example = functools partial example get_world_trs inputs = torch ones device=self device + rank eager_out = example inputs compiled_matmul_cat_col = compile example inputs inductor_out = compiled_matmul_cat_col inputs assertTrue same eager_out inductor_out tol= unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu test_reduce_scatter_tensor_inductor example b tag ranks group_size c = torch matmul b ag = torch ops c d_functional reduce_scatter_tensor c sum tag ranks group_size ag = torch ops c d_functional wait_tensor ag ag compile func example_inputs graph = make_fx func example_inputs inductor_compile_fx graph example_inputs _dynamo_dist_per_rank_init rank world_size example = functools partial example get_world_trs inputs = torch ones device=self device + rank eager_out = example inputs compiled_fn = compile example inputs inductor_out = compiled_fn inputs assertTrue same eager_out inductor_out tol= unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu patch object torch _dynamo config capture_scalar_outputs True test_all_to_all_single_inductor example inp input_split_sizes_tensor output_split_sizes_tensor tag ranks group_size input_split_sizes = input_split_sizes_tensor tolist output_split_sizes = output_split_sizes_tensor tolist = torch ops c d_functional all_to_all_single inp output_split_sizes input_split_sizes tag ranks group_size = torch ops c d_functional wait_tensor out = sum dim= out _dynamo_dist_per_rank_init rank world_size torch _dynamo config patch dynamic_shapes=True capture_dynamic_output_shape_ops=True capture_scalar_outputs=True row = world_size rank + world_size + input_split_sizes_tensor = torch tensor i + rank + i range world_size dtype=torch int output_split_sizes_tensor = torch tensor i + rank + i range world_size dtype=torch int inputs = torch ones int row device=self device rank + input_split_sizes_tensor output_split_sizes_tensor trs = get_world_trs compiled_fn = torch compile example fullgraph=True dynamic=True code = run_and_get_triton_code compiled_fn inputs trs FileCheck check_regex torch ops _c d_functional all_to_all_single default\\ arg\\d+_\\d+ \\ u\\d+ u\\d+\\ \\ u\\d+ u\\d+\\ run code eager_out = example inputs trs inductor_out = compiled_fn inputs trs assertTrue same eager_out inductor_out tol= The goal test when ` unsafe_allow_recompute_of_collectives=False ` The partitioner will never recompute collectives backward even activation_memory_budget partitioner being used unless there manual user checkpoint region which we know makes safe recompute collective since we assume user applied AC region consistently across all ranks unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu patch object torch _dynamo config capture_scalar_outputs True patch object torch _functorch config activation_memory_budget parametrize override_with_ac False True test_all_to_all_recompute_is_always_banned override_with_ac torch library custom_op custom_ns foo mutates_args= foo x torch Tensor - torch Tensor x + foo register_fake _ x torch empty_like x setup_context ctx inputs output ctx save_for_backward inputs backward ctx grad x = ctx saved_tensors grad x foo register_autograd backward setup_context=setup_context AllToAllSingle torch autograd Function staticmethod forward ctx input torch Tensor output_split_sizes input_split_sizes tag ranks group_size int - torch Tensor ctx output_split_sizes = input_split_sizes ctx input_split_sizes = output_split_sizes ctx group_size = group_size = torch ops _c d_functional all_to_all_single default input output_split_sizes input_split_sizes = torch ops c d_functional wait_tensor staticmethod backward ctx grad grad = torch ops _c d_functional all_to_all_single default grad ctx output_split_sizes ctx input_split_sizes torch ops c d_functional wait_tensor grad None None None None None alltoall_autograd inp output_split_sizes input_split_sizes tag ranks group_size out = AllToAllSingle apply inp output_split_sizes input_split_sizes tag ranks group_size out simple mode track how many collective ops we saw backward TrackingMode TorchDispatchMode __init__ super __init__ ops_counter = Counter __torch_dispatch__ func types args= kwargs=None kwargs None kwargs = rs = func args kwargs ops_counter func += rs example inp input_split_sizes_tensor output_split_sizes_tensor tag ranks group_size input_split_sizes = input_split_sizes_tensor tolist output_split_sizes = output_split_sizes_tensor tolist = torch ops custom_ns alltoall_autograd default inp output_split_sizes input_split_sizes tag ranks group_size torch ops custom_ns foo _dynamo_dist_per_rank_init rank world_size torch _dynamo config patch dynamic_shapes=True capture_dynamic_output_shape_ops=True capture_scalar_outputs=True torch library _scoped_library custom_ns FRAGMENT lib lib define alltoall_autograd Tensor input SymInt output_split_sizes SymInt input_split_sizes str tag int ranks int group_size - Tensor noqa B lib impl alltoall_autograd alltoall_autograd Autograd lib impl alltoall_autograd alltoall_autograd Meta row = world_size rank + world_size + input_split_sizes_tensor = torch tensor i + rank + i range world_size dtype=torch int output_split_sizes_tensor = torch tensor i + rank + i range world_size dtype=torch int inputs = torch ones int row device=self device requires_grad=True rank + input_split_sizes_tensor output_split_sizes_tensor trs = get_world_trs compiled_fn = torch compile example fullgraph=True dynamic=True backend= aot_eager_decomp_partition override_with_ac compiled_fn_wrapper args example inputs trs out = torch utils checkpoint checkpoint compiled_fn_wrapper inputs use_reentrant=False out = compiled_fn inputs trs track how many all_to_alls we saw backward TrackingMode m out sum backward override_with_ac We wrapped our test AC which overrides partitioner decision never recomputing collectives So we should properly see all all recomputed backward assertEqual m ops_counter torch ops _c d_functional all_to_all_single default there all all fw all all backward notably even though activation_memory_budget == recompute_everything we still choosing recompute all all fw assertEqual m ops_counter torch ops _c d_functional all_to_all_single default unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu test_all_to_all_single_inductor_split_sizes_none example inp tag ranks group_size = torch ops c d_functional all_to_all_single inp None None tag ranks group_size = torch ops c d_functional wait_tensor out = sum dim= out _dynamo_dist_per_rank_init rank world_size inputs = torch ones world_size world_size device=self device rank + trs = get_world_trs compiled_fn = torch compile example fullgraph=True dynamic=True code = run_and_get_triton_code compiled_fn inputs trs FileCheck check_regex torch ops _c d_functional all_to_all_single default\\ arg\\d+_\\d+ \\ s\\d+ \\d s\\d+ \\d\\ \\ s\\d+ \\d s\\d+ \\d\\ run code eager_out = example inputs trs inductor_out = compiled_fn inputs trs assertTrue same eager_out inductor_out tol= instantiate_parametrized_tests requires_accelerator_dist_backend nccl xccl unittest skipIf torch accelerator is_available No accelerator available TestCollectivesInductor DynamoDistributedSingleProcTestCase Prefer single-proc test runner basic tests easier work get_world_trs world_size= tag ranks list range world_size group_size world_size unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch debug=True test_inductor_single_op func inp tag ranks group_size ar = torch ops c d_functional all_reduce inp sum tag ranks group_size ar = torch ops c d_functional wait_tensor ar ar inputs = torch ones device=self device compiled = torch compile func out = compiled inputs get_world_trs code = run_and_get_triton_code compiled inputs get_world_trs NOTE Make sure we unnecessarily copying outputs wait_tensors before they returned graph FileCheck check buf = empty_strided check run arg _ buf check torch ops _c d_functional all_reduce_ default buf check torch ops _c d_functional wait_tensor default buf check buf run code correct = func inputs get_world_trs assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch debug=True test_inductor_steal_buffer s ok optimal inductor allreduce mutates buffer intermediate isn t going used again func inp tag ranks group_size x = inp + ar = torch ops c d_functional all_reduce x sum tag ranks group_size ar = torch ops c d_functional wait_tensor ar ensure other incorrectly aliasing ar s buffer other = torch ones_like inp + ar other inputs = torch ones device=self device compiled = torch compile func code = run_and_get_triton_code compiled inputs get_world_trs FileCheck check buf = empty_strided check run arg _ buf check torch ops _c d_functional all_reduce_ default buf check torch ops _c d_functional wait_tensor default buf check buf = empty_strided check run buf check buf buf run code out = compiled inputs get_world_trs correct = func inputs get_world_trs assertTrue same out correct _test_inductor_doesnt_mutate_shared make sure intermediate s going reuse isn t mutated unless copied func inp tag ranks group_size x = inp + ar = torch ops c d_functional all_reduce x sum tag ranks group_size y = x + ar = torch ops c d_functional wait_tensor ar ensure other incorrectly aliasing ar s buffer other = torch ones_like inp + ar y other inputs = torch ones device=self device compiled = torch compile func code = run_and_get_triton_code compiled inputs get_world_trs NOTE Make sure we unnecessarily copying outputs wait_tensors before they returned graph FileCheck check buf = empty_strided check buf = buf check buf = empty_strided check run buf arg _ buf check torch ops _c d_functional all_reduce_ default buf check torch ops _c d_functional wait_tensor default buf check buf = empty_strided check run buf check buf buf buf run code out = compiled inputs get_world_trs correct = func inputs get_world_trs assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch debug True triton descriptive_names False test_inductor_doesnt_mutate_shared _test_inductor_doesnt_mutate_shared unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch debug True triton descriptive_names False torch _inductor config patch graph_partition True test_inductor_doesnt_mutate_shared_graph_partition checks graph partition reorder does change relative order ops when all ops cuda _test_inductor_doesnt_mutate_shared test_dynamo_trace_allreduce func inp ar = _functional_collectives all_reduce inp sum ar inputs = torch ones device=self device counter = CompileCounter compiled = torch compile func backend=counter out = compiled inputs correct = func inputs assertEqual counter frame_count should test more precisely supposed all_reduce wait assertEqual counter op_count assertTrue same out correct skipIfXpu https github com intel torch-xpu-ops issues test_dynamo_trace_all_gather_tensor func inp ar = _functional_collectives all_gather_tensor inp ar inputs = torch ones device=self device counter = CompileCounter compiled = torch compile func backend=counter out = compiled inputs correct = func inputs assertEqual counter frame_count should test more precisely supposed all_gather wait assertEqual counter op_count assertTrue same out correct skipIfXpu https github com intel torch-xpu-ops issues test_dynamo_trace_all_gather_tensor_pg func inp pg ar = _functional_collectives all_gather_tensor inp pg ar inputs = torch ones device=self device counter = CompileCounter compiled = torch compile func backend=counter fullgraph=True out = compiled inputs pg=GroupMember WORLD correct = func inputs pg=GroupMember WORLD assertEqual counter frame_count should test more precisely supposed all_gather wait assertEqual counter op_count assertTrue same out correct skipIfXpu https github com intel torch-xpu-ops issues test_dynamo_rewrite_dist_all_gather func inp out pg torch distributed all_gather_into_tensor out inp pg local_size = single-proc test global_size = local_size inputs = torch ones local_size device=self device outputs = torch empty global_size device=self device correct_outputs = torch empty global_size device=self device counter = CompileCounter compiled = torch compile func backend=counter fullgraph=True compiled inputs outputs pg=GroupMember WORLD func inputs correct_outputs pg=GroupMember WORLD assert counter frame_count == should test more precisely supposed all_gather wait copy_ assert counter op_count == assert same outputs correct_outputs skipIfXpu https github com intel torch-xpu-ops issues test_dynamo_rewrite_dist_all_gather_list func inp out pg torch distributed all_gather out inp pg local_size = single-proc test global_size = local_size inputs = torch ones local_size device=self device outputs = torch empty global_size device=self device correct_outputs = torch empty global_size device=self device counter = CompileCounter compiled = torch compile func backend=counter fullgraph=True compiled inputs outputs pg=GroupMember WORLD func inputs correct_outputs pg=GroupMember WORLD assert counter frame_count == assert same outputs correct_outputs skipIfXpu https github com intel torch-xpu-ops issues test_dynamo_rewrite_dist_all_gather_args_match Duplicated most structure test_dynamo_rewrite_dist_all_gather except uses kwargs ensure rewrite has matching arg names func inp out pg torch distributed all_gather_into_tensor output_tensor=out input_tensor=inp group=pg async_op=False local_size = single-proc test global_size = local_size inputs = torch ones local_size device=self device outputs = torch empty global_size device=self device correct_outputs = torch empty global_size device=self device counter = CompileCounter compiled = torch compile func backend=counter fullgraph=True compiled inputs outputs pg=GroupMember WORLD func inputs correct_outputs pg=GroupMember WORLD assert counter frame_count == should test more precisely supposed all_gather wait copy_ assert counter op_count == assert same outputs correct_outputs skipIfXpu https github com intel torch-xpu-ops issues test_dynamo_rewrite_dist_reduce_scatter func inp out pg torch distributed reduce_scatter_tensor out inp group=pg local_size = single-proc test global_size = local_size inputs = torch ones local_size device=self device outputs = torch empty global_size device=self device correct_outputs = torch empty global_size device=self device counter = CompileCounter compiled = torch compile func backend=counter fullgraph=True compiled inputs outputs pg=GroupMember WORLD func inputs correct_outputs pg=GroupMember WORLD assert counter frame_count == should test more precisely supposed reduce_scatter wait copy_ assert counter op_count == assert same outputs correct_outputs parametrize pg_mode positional positional_none kwargs kwargs_none unspecified test_dynamo_rewrite_dist_allreduce pg_mode func tensor args kwargs torch distributed all_reduce tensor args kwargs counter = CompileCounter compiled = torch compile func backend=counter fullgraph=True args = kwargs = pg_mode == positional args append torch distributed ReduceOp MAX args append GroupMember WORLD pg_mode == positional_none args append torch distributed ReduceOp MAX args append None pg_mode == kwargs kwargs group = GroupMember WORLD pg_mode == kwargs_none kwargs group = None assert pg_mode == unspecified inputs_compiled = torch ones device=self device inputs_eager = torch ones device=self device compiled inputs_compiled args kwargs func inputs_eager args kwargs assert counter frame_count == should test more precisely supposed all_reduce wait copy_ assert counter op_count == assert same inputs_compiled inputs_eager test_dynamo_rewrite_dist_all_to_all_single func output input pg torch distributed all_to_all_single output input group=pg counter = CompileCounter compiled = torch compile func backend=counter fullgraph=True input_compiled = torch ones device=self device input_eager = torch ones device=self device output_compiled = torch empty device=self device output_eager = torch empty device=self device compiled output_compiled input_compiled GroupMember WORLD func output_eager input_eager GroupMember WORLD assert counter frame_count == assert same output_compiled output_eager parametrize reduce_op torch distributed ReduceOp SUM torch distributed ReduceOp AVG torch distributed ReduceOp PRODUCT torch distributed ReduceOp MIN torch distributed ReduceOp MAX test_dynamo_rewrite_dist_allreduce_reduce_op reduce_op torch distributed _functional_collectives REDUCE_OP_TO_STR verify_rewrite gm _ ar_nodes = node gm graph nodes node target torch ops c d_functional all_reduce torch ops _c d_functional all_reduce ar_nodes append node assertEqual len ar_nodes reduce_op_str = ar_nodes args assertEqual REDUCE_OP_TO_STR reduce_op reduce_op_str gm compiled = torch compile torch distributed all_reduce backend=verify_rewrite fullgraph=True inputs = torch ones device=self device reduce_op GroupMember WORLD compiled inputs parametrize source GroupMember WORLD group WORLD _get_default_group test_dynamo_get_world_group source func tensor source == GroupMember WORLD group = torch distributed GroupMember WORLD source == group WORLD group = torch distributed group WORLD assert source == _get_default_group group = torch distributed distributed_c d _get_default_group torch distributed all_reduce tensor group=group verify gm _ ar_nodes = node gm graph nodes node target torch ops c d_functional all_reduce torch ops _c d_functional all_reduce ar_nodes append node assertEqual len ar_nodes gm compiled = torch compile func backend=verify fullgraph=True input = torch ones device=self device compiled input skipIfXpu https github com intel torch-xpu-ops issues test_dynamo_support_collective_op_with_async_op_False func inp out pg user explicitly set attribute ` async_op ` False there should no graph break torch distributed reduce_scatter_tensor out inp group=pg async_op=False local_size = single-proc test global_size = local_size inputs = torch ones local_size device=self device outputs = torch empty global_size device=self device correct_outputs = torch empty global_size device=self device counter = CompileCounter compiled = torch compile func backend=counter compiled inputs outputs pg=GroupMember WORLD func inputs correct_outputs pg=GroupMember WORLD assert counter frame_count == assert counter op_count == assert same outputs correct_outputs test_dynamo_graphbreaks_unsupported_async_op func inp out pg work = torch distributed reduce_scatter_tensor out inp group=pg async_op=True work wait local_size = single-proc test global_size = local_size inputs = torch ones local_size device=self device outputs = torch empty global_size device=self device correct_outputs = torch empty global_size device=self device counter = CompileCounter compiled = torch compile func backend=counter compiled inputs outputs pg=GroupMember WORLD func inputs correct_outputs pg=GroupMember WORLD assert counter frame_count == assert counter op_count == assert same outputs correct_outputs test_dynamo_pg_var func inp pg x = pg rank + pg size inp + x local_size = inputs = torch ones local_size device=self device correct_outputs = torch empty local_size device=self device counter = CompileCounter compiled = torch compile func backend=counter fullgraph=True outputs = compiled inputs pg=GroupMember WORLD correct_outputs = func inputs pg=GroupMember WORLD assert counter frame_count == assert counter op_count == assert same outputs correct_outputs skipIfXpu https github com intel torch-xpu-ops issues test_dynamo_trace_reduce_scatter_tensor func inp ar = _functional_collectives reduce_scatter_tensor inp sum ar inputs = torch ones device=self device counter = CompileCounter compiled = torch compile func backend=counter out = compiled inputs correct = func inputs assertEqual counter frame_count should test more precisely supposed reduce_scatter wait assertEqual counter op_count assertTrue same out correct skipIfXpu https github com intel torch-xpu-ops issues test_dynamo_trace_allgather_coalesced func inp tag ranks group_size ar = torch ops c d_functional all_gather_into_tensor_coalesced inp tag ranks group_size ar inputs = torch ones device=self device torch ones device=self device counter = CompileCounter compiled = torch compile func backend=counter out = compiled inputs get_world_trs correct = func inputs get_world_trs assert counter frame_count == assert counter op_count == It generates getattr unpack array assert same out correct test_backwards It s probably common need backwards support collectives However I wanted least see possible support design goal func inp ar = _functional_collectives all_reduce inp sum ar input = torch ones device=self device requires_grad=True compiled = torch compile func backend= aot_eager inductor bug single-op allreduce graph out = compiled input out sum backward correct_input = input detach clone requires_grad_ correct = func correct_input correct sum backward assertTrue same out correct assertTrue same input grad correct_input grad test_meta x = torch rand device= meta out = torch ops c d_functional all_reduce x sum get_world_trs assertEqual x size out size skipIfXpu https github com intel torch-xpu-ops issues unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch debug True triton descriptive_names False test_inductor_all_gather_coalesced make sure intermediate s going reuse isn t mutated unless copied func inp tag ranks group_size x = inp + tensor_list = torch ops c d_functional all_gather_into_tensor_coalesced x inp tag ranks group_size y = x + ar = torch ops c d_functional wait_tensor tensor_list ar = torch ops c d_functional wait_tensor tensor_list ensure other incorrectly aliasing ar s buffer other = torch ones_like inp + ar y other ar inputs = torch ones device=self device compiled = torch compile func code = run_and_get_triton_code compiled inputs get_world_trs NOTE Make sure we unnecessarily copying outputs wait_tensors before they returned graph FileCheck check buf = empty_strided check buf = empty_strided check run arg _ buf buf check buf = torch ops _c d_functional all_gather_into_tensor_coalesced default buf arg _ check buf = buf check buf = buf check torch ops _c d_functional wait_tensor default buf check buf = buf del buf reuse check run buf check torch ops _c d_functional wait_tensor default buf check buf buf buf buf run code out = compiled inputs get_world_trs correct = func inputs get_world_trs assert same out correct f out va correct skipIfXpu https github com intel torch-xpu-ops issues unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch debug True triton descriptive_names False test_inductor_reduce_scatter_coalesced make sure intermediate s going reuse isn t mutated unless copied func inp tag ranks group_size x = inp + tensor_list = torch ops c d_functional reduce_scatter_tensor_coalesced x inp sum tag ranks group_size y = x + ar = torch ops c d_functional wait_tensor tensor_list ar = torch ops c d_functional wait_tensor tensor_list ensure other incorrectly aliasing ar s buffer other = torch ones_like inp + ar y other ar inputs = torch ones device=self device compiled = torch compile func code = run_and_get_triton_code compiled inputs get_world_trs NOTE The first value should output first wait_tensor We want make sure no unnecessary copy made FileCheck check buf = empty_strided check buf = empty_strided check run arg _ buf buf check buf = torch ops _c d_functional reduce_scatter_tensor_coalesced default buf arg _ check buf = buf check buf = buf check torch ops _c d_functional wait_tensor default buf check buf = buf del buf reuse check run buf check torch ops _c d_functional wait_tensor default buf check buf buf buf buf run code out = compiled inputs get_world_trs correct = func inputs get_world_trs assert same out correct f out va correct skipIfXpu https github com intel torch-xpu-ops issues unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_reorder_peak_memory TODO whc - check each ` limiting_factor ` cases - confirm peak memory respected some adversarial case - check whether expected correct buf = buf del buf reuse statement materially changes func inp tag ranks group_size x = inp + tensor_list = torch ops c d_functional reduce_scatter_tensor_coalesced x inp sum tag ranks group_size y = x + ar = torch ops c d_functional wait_tensor tensor_list ar = torch ops c d_functional wait_tensor tensor_list ensure other incorrectly aliasing ar s buffer other = torch ones_like inp + ar y other ar inputs = torch ones device=self device get stats directly internal helper without affecting real pass s signature node_stats Optional dict BaseSchedulerNode ReorderInfo = None _reorder_communication_preserving_peak_memory snodes list BaseSchedulerNode - list BaseSchedulerNode nonlocal node_stats reordered_snodes node_stats = _reorder_communication_preserving_peak_memory_internal snodes reordered_snodes torch _inductor config patch reorder_for_compute_comm_overlap True reorder_for_compute_comm_overlap_passes sink_waits same reorder_communication_preserving_peak_memory returns debug info structures directly _reorder_communication_preserving_peak_memory compiled = torch compile func code = run_and_get_triton_code compiled inputs get_world_trs NOTE The first value should output first wait_tensor We want make sure no unnecessary copy made FileCheck check buf = empty_strided check buf = empty_strided check run arg _ buf buf check buf = torch ops _c d_functional reduce_scatter_tensor_coalesced default buf arg _ check buf = buf check buf = buf check torch ops _c d_functional wait_tensor default buf check buf = buf del buf reuse check run buf check torch ops _c d_functional wait_tensor default buf check buf buf buf buf run code out = compiled inputs get_world_trs correct = func inputs get_world_trs assert same out correct f out va correct TODO make test case more interesting validate actual desired behavior assert node_stats None assertTrue isinstance node_stats dict assertEqual len node_stats stats node_stats values assertEqual stats initial_exposed assertEqual stats limiting_factor None assertEqual stats moves unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch unittest skipIf SM OrLater bfloat parametrize bucket_mode all all_custom_ops test_all_gather_bucket bucket_mode func x w ag_ ag_ ag_ ag_ tag ranks group_size do some unrelated matmuls y = torch mm x w ag_ _cast = ag_ torch bfloat group_name = torch distributed distributed_c d _get_default_group group_name ag_ _out = torch ops _c d_functional all_gather_into_tensor ag_ group_size group_name ag_ _out = torch ops c d_functional wait_tensor ag_ _out ag_ = ag_ _out + ag_ ag_ _cast = ag_ torch bfloat ag_ _out = torch ops _c d_functional all_gather_into_tensor ag_ _cast group_size group_name ag_ _out = torch ops c d_functional wait_tensor ag_ _out ag_ _out = ag_ _out ag_ _out = torch ops _c d_functional all_gather_into_tensor ag_ _cast group_size group_name ag_ _out = torch ops c d_functional wait_tensor ag_ _out ag_ _out = torch ops _c d_functional all_gather_into_tensor ag_ group_size group_name ag_ _out = torch ops c d_functional wait_tensor ag_ _out y ag_ _out ag_ _out ag_ _out ag_ _out x = torch ones device= cuda dtype=torch float w = torch ones device= cuda dtype=torch float ag_ = torch ones device= cuda dtype=torch float ag_ = torch ones device= cuda dtype=torch float ag_ = torch ones device= cuda dtype=torch float ag_ = torch ones device= cuda dtype=torch float inputs = x w ag_ ag_ ag_ ag_ correct = func inputs get_world_trs torch _inductor config patch bucket_all_gathers_fx bucket_mode reorder_for_compute_comm_overlap False runtime_estimations_mms_benchmark True torch _inductor config_comms patch runtime_estimations_align_across_all_distributed_ranks True Clearing cache cover runtime_estimations_mms_benchmark use LocalCache fresh_inductor_cache compiled = torch compile func code = run_and_get_triton_code compiled inputs get_world_trs NOTE The first value should output first wait_tensor We want make sure no unnecessary copy made FileCheck check = torch ops _c d_functional all_gather_into_tensor check torch ops _c d_functional all_gather_into_tensor_out default check = torch ops _c d_functional all_gather_into_tensor run code out = compiled inputs get_world_trs assert same out correct f out va correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch unittest skipIf SM OrLater bfloat test_all_gather_bucket_path func x w ag_ ag_ tag ranks group_size do some unrelated matmuls y = torch mm x w cast inputs ag_ _cast = ag_ torch bfloat ag_ _cast = ag_ torch bfloat first allgather group_name = torch distributed distributed_c d _get_default_group group_name ag_ _out = torch ops _c d_functional all_gather_into_tensor ag_ _cast group_size group_name ag_ _out = torch ops c d_functional wait_tensor ag_ _out ag_ _out = ag_ _out Create dependency second allgather input depends first allgather output This prevents fusion two allgather operations ag_ _modified = ag_ _cast + ag_ _out ag_ _cast shape Use part ag_ _out second allgather now depends first one ag_ _out = torch ops _c d_functional all_gather_into_tensor ag_ _modified group_size group_name ag_ _out = torch ops c d_functional wait_tensor ag_ _out y ag_ _out ag_ _out x = torch ones device=self device dtype=torch float w = torch ones device=self device dtype=torch float ag_ = torch ones device=self device dtype=torch float ag_ = torch ones device=self device dtype=torch float inputs = x w ag_ ag_ torch _inductor config patch bucket_all_gathers_fx all reorder_for_compute_comm_overlap False compiled = torch compile func code = run_and_get_triton_code compiled inputs get_world_trs shouldnt have bucketed FileCheck check_count wait_tensor default exactly=True run code unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch unittest skipIf SM OrLater bfloat parametrize bucket_mode all all_custom_ops test_reduce_scatter_bucket bucket_mode func x w rs_ rs_ tag ranks group_size do some unrelated matmuls y = torch mm x w cast inputs rs_ _cast = rs_ torch bfloat rs_ _cast = rs_ torch bfloat reduce_scatter group_name = torch distributed distributed_c d _get_default_group group_name rs_ _out = torch ops _c d_functional reduce_scatter_tensor rs_ _cast sum group_size group_name rs_ _out = torch ops _c d_functional reduce_scatter_tensor rs_ _cast sum group_size group_name wait op rs_ _out = torch ops c d_functional wait_tensor rs_ _out rs_ _out = torch ops c d_functional wait_tensor rs_ _out y rs_ _out rs_ _out test fsdp mode allow convert_element_type after wait func x w rs_ rs_ tag ranks group_size y rs_ _out rs_ _out = func x w rs_ rs_ tag ranks group_size y rs_ _out torch float rs_ _out torch float f func func x = torch ones device= cuda dtype=torch float w = torch ones device= cuda dtype=torch float rs_ = torch ones device= cuda dtype=torch float rs_ = torch ones device= cuda dtype=torch float inputs = x w rs_ rs_ f inputs get_world_trs torch _inductor config patch bucket_reduce_scatters_fx bucket_mode reorder_for_compute_comm_overlap False compiled = torch compile f compiled inputs get_world_trs code = run_and_get_triton_code compiled inputs get_world_trs NOTE The first value should output first wait_tensor We want make sure no unnecessary copy made FileCheck check_count torch ops _c d_functional reduce_scatter_tensor default count= exactly=True run code out = compiled inputs get_world_trs correct = f inputs get_world_trs assert same out correct f out va correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch unittest skipIf SM OrLater bfloat parametrize bucket_mode all test_all_reduce_bucket bucket_mode func x w ar_ ar_ tag ranks group_size y = torch mm x w group_name = torch distributed distributed_c d _get_default_group group_name ar_ _out = torch ops _c d_functional all_reduce default ar_ sum group_name ar_ _out = torch ops _c d_functional all_reduce default ar_ sum group_name ar_ _w = torch ops c d_functional wait_tensor ar_ _out ar_ _w = torch ops c d_functional wait_tensor ar_ _out y ar_ _w ar_ _w f = func x = torch ones device= cuda dtype=torch float w = torch ones device= cuda dtype=torch float ar_ = torch ones device= cuda dtype=torch float ar_ = torch ones device= cuda dtype=torch float inputs = x w ar_ ar_ f inputs get_world_trs _pass g torch _inductor fx_passes bucketing bucket_all_reduce bucket_all_reduce g owning_module lambda _ torch _inductor config post_grad_custom_post_pass = _pass torch _inductor config patch reorder_for_compute_comm_overlap False compiled = torch compile f compiled inputs get_world_trs code = run_and_get_triton_code compiled inputs get_world_trs NOTE The first value should output first wait_tensor We want make sure no unnecessary copy made FileCheck check_count torch ops _c d_functional all_reduce_ default count= exactly=True run code out = compiled inputs get_world_trs correct = f inputs get_world_trs assert same out correct f out va correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch unittest skipIf SM OrLater bfloat parametrize bucket_mode all_custom_ops_multidtype test_all_gather_bucket_multidtype bucket_mode func x w ag_ ag_ tag ranks group_size do some unrelated matmuls y = torch mm x w group_name = torch distributed distributed_c d _get_default_group group_name ag_ _w = torch ops _c d_functional all_gather_into_tensor ag_ group_size group_name ag_ _out = torch ops c d_functional wait_tensor ag_ _w ag_ _out = ag_ _out ag_ _w = torch ops _c d_functional all_gather_into_tensor ag_ group_size group_name ag_ _out = torch ops c d_functional wait_tensor ag_ _w y ag_ _out ag_ _out x = torch ones device= cuda dtype=torch float w = torch ones device= cuda dtype=torch float ag_ = torch ones device= cuda dtype=torch bfloat ag_ = torch ones device= cuda dtype=torch float inputs = x w ag_ ag_ correct = func inputs get_world_trs torch _inductor config patch bucket_all_gathers_fx bucket_mode reorder_for_compute_comm_overlap False compiled = torch compile func code = run_and_get_triton_code compiled inputs get_world_trs FileCheck check_count torch ops _c d_functional all_gather_into_tensor_out default count= exactly=True run code out = compiled inputs get_world_trs _ y_ag y_ag = out assert y_ag dtype == ag_ dtype assert y_ag dtype == ag_ dtype assert same out correct f out va correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch unittest skipIf SM OrLater bfloat parametrize bucket_mode all all_custom_ops test_reorder_peak_memory_bucketed bucket_mode Simulate case where bucketing pass ran grouped several inputs into one bucketed allgather Ensure whole bucketed group including copy-ops get moved together rather than copy ops preventing comm moving due data dependency func x w ag_ ag_ ag_ ag_ tag ranks group_size do some unrelated matmuls y = torch mm x w cast inputs ag_ _cast = ag_ torch bfloat ag_ _cast = ag_ torch bfloat allgather group_name = torch distributed distributed_c d _get_default_group group_name ag_ _out = torch ops _c d_functional all_gather_into_tensor ag_ _cast group_size group_name ag_ _out = torch ops _c d_functional all_gather_into_tensor ag_ _cast group_size group_name wait op ag_ _out = torch ops c d_functional wait_tensor ag_ _out ag_ _out = torch ops c d_functional wait_tensor ag_ _out rs_ _out = torch ops _c d_functional reduce_scatter_tensor ag_ _cast sum group_size group_name rs_ _out = torch ops _c d_functional reduce_scatter_tensor ag_ _cast sum group_size group_name wait op rs_ _out = torch ops c d_functional wait_tensor rs_ _out rs_ _out = torch ops c d_functional wait_tensor rs_ _out y += torch mm x w cast inputs ag_ _cast = ag_ torch bfloat ag_ _cast = ag_ torch bfloat ag_ _out = torch ops _c d_functional all_gather_into_tensor ag_ _cast group_size group_name ag_ _out = torch ops _c d_functional all_gather_into_tensor ag_ _cast group_size group_name wait op ag_ _out = torch ops c d_functional wait_tensor ag_ _out ag_ _out = torch ops c d_functional wait_tensor ag_ _out rs_ _out = torch ops _c d_functional reduce_scatter_tensor ag_ _cast sum group_size group_name rs_ _out = torch ops _c d_functional reduce_scatter_tensor ag_ _cast sum group_size group_name wait op rs_ _out = torch ops c d_functional wait_tensor rs_ _out rs_ _out = torch ops c d_functional wait_tensor rs_ _out y ag_ _out ag_ _out ag_ _out ag_ _out rs_ _out rs_ _out rs_ _out rs_ _out x = torch ones device=self device dtype=torch float w = torch ones device=self device dtype=torch float ag_ = torch ones device=self device dtype=torch float ag_ = torch ones device=self device dtype=torch float ag_ = torch ones device=self device dtype=torch float ag_ = torch ones device=self device dtype=torch float inputs = x w ag_ ag_ ag_ ag_ get stats directly internal helper without affecting real pass s signature node_stats Optional dict BaseSchedulerNode ReorderInfo = None _reorder_communication_preserving_peak_memory snodes list BaseSchedulerNode - list BaseSchedulerNode torch _inductor config runtime_estimations_mms_benchmark cache = get_estimate_runtime_cache snode snodes _get_mm_like_fn snode None continue cache_key = get_estimate_runtime_cache_key_from_snode snode assert cache lookup cache_key None torch _inductor config_comms runtime_estimations_align_across_all_distributed_ranks snode snodes assert snode override_estimated_runtime None nonlocal node_stats reordered_snodes node_stats = _reorder_communication_preserving_peak_memory_internal snodes reordered_snodes torch _inductor config patch bucket_all_gathers_fx bucket_mode bucket_all_gathers_fx_bucket_size_determinator lambda _ bucket_reduce_scatters_fx bucket_mode bucket_reduce_scatters_fx_bucket_size_determinator lambda _ reorder_for_compute_comm_overlap True reorder_for_compute_comm_overlap_passes sink_waits_iterative _reorder_communication_preserving_peak_memory allow_buffer_reuse False test_configs track_memory_lifecycle error runtime_estimations_mms_benchmark True torch _inductor config_comms patch runtime_estimations_align_across_all_distributed_ranks True Clearing cache cover runtime_estimations_mms_benchmark use LocalCache fresh_inductor_cache compiled = torch compile func fullgraph=True code = run_and_get_triton_code compiled inputs get_world_trs make sure memory tracking codegen ops will then do runtime checking assertion FileCheck check check_memory_step check tracked_empty_strided run code NOTE The first value should output first wait_tensor We want make sure no unnecessary copy made torch _inductor config triton native_matmul FileCheck check_count torch ops _c d_functional all_gather_into_tensor_out default count= exactly=True check extern_kernels mm check extern_kernels addmm run code FileCheck check_count torch ops _c d_functional reduce_scatter_tensor default count= exactly=True check extern_kernels mm check extern_kernels addmm run code out = compiled inputs get_world_trs correct = func inputs get_world_trs assert same out correct f out va correct assert node_stats None assertTrue isinstance node_stats dict assertEqual len node_stats = iter node_stats values node_stat = next assertTrue node_stat limiting_factor == None node_stat = next assertTrue collective ordering node_stat limiting_factor skipIfXpu https github com intel torch-xpu-ops issues unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_reorder_respects_wait_dep Covers case where output one collective feeds input another collective e g TP + FSDP - all_gather tp+dp sharded param TP dim - allgather dp_sharded buffer DP dim func inp tag ranks group_size group_name = torch distributed distributed_c d _get_default_group group_name ag_ _out = torch ops _c d_functional all_gather_into_tensor inp group_size group_name ag_ _wait = torch ops c d_functional wait_tensor ag_ _out ag_ _out = torch ops _c d_functional all_gather_into_tensor ag_ _wait group_size group_name ag_ _wait = torch ops c d_functional wait_tensor ag_ _out ensure other incorrectly aliasing ar s buffer ag_ _wait inputs = torch ones device=self device get stats directly internal helper without affecting real pass s signature node_stats Optional dict BaseSchedulerNode ReorderInfo = None _reorder_communication_preserving_peak_memory snodes list BaseSchedulerNode - list BaseSchedulerNode nonlocal node_stats reordered_snodes node_stats = _reorder_communication_preserving_peak_memory_internal snodes reordered_snodes torch _inductor config patch reorder_for_compute_comm_overlap True reorder_for_compute_comm_overlap_passes sink_waits same reorder_communication_preserving_peak_memory returns debug info structures directly _reorder_communication_preserving_peak_memory compiled = torch compile func code = run_and_get_triton_code compiled inputs get_world_trs NOTE The first value should output first wait_tensor We want make sure no unnecessary copy made FileCheck check all_gather check wait check all_gather check wait run code out = compiled inputs get_world_trs correct = func inputs get_world_trs assert same out correct f out va correct TODO make test case more interesting validate actual desired behavior assert node_stats None assertTrue isinstance node_stats dict assertEqual len node_stats stats node_stats values assertEqual stats moves requires_accelerator_dist_backend nccl xccl TestSyncDecisionCrossRanks MultiProcessTestCase setUp - None super setUp _spawn_processes property world_size - int property ranks - list int list range world_size property device - torch device device_type = torch accelerator current_accelerator type torch device f device_type rank _init_process_group - None torch _inductor config triton store_cubin = True torch _inductor config debug = True torch get_device_module device set_device device store = torch distributed FileStore file_name world_size backend = c d get_default_backend_for_device torch accelerator current_accelerator type torch distributed init_process_group backend=backend world_size=self world_size rank=self rank store=store torch _C _distributed_c d _register_process_group default torch distributed group WORLD skip_if_lt_x_gpu test_sync_decision_cross_ranks torch _functorch partitioners _sync_decision_cross_ranks test_graph = torch fx Graph node = test_graph placeholder x ag = test_graph create_node call_function torch ops _c d_functional all_gather_into_tensor default node wt = test_graph create_node call_function torch ops _c d_functional wait_tensor default ag wt meta val = torch randn ag = test_graph create_node call_function torch ops _c d_functional all_gather_into_tensor default node wt = test_graph create_node call_function torch ops _c d_functional wait_tensor default ag wt meta val = torch randn rank == saved_values = wt saved_values = wt _init_process_group saved_values = _sync_decision_cross_ranks test_graph saved_values assertEqual saved_values wt __name__ == __main__ torch _dynamo test_case run_tests run_tests