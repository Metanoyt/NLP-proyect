mypy allow-untyped-defs Copyright c Meta Platforms Inc affiliates collections logging collections abc Iterator typing Any Optional Union torch torch autograd graph GradientEdge Node torch nn Parameter _debug map_debug_info logger = logging getLogger __name__ _get_grad_fn_or_grad_acc t torch Tensor - Union Node None Get grad function grad accumulator tensor Accumulate grad nodes lazily created so we need dummy view order trigger its creation t requires_grad t grad_fn None no grad function leaf tensors we use view viewed_t = t view_as t grad_fn = viewed_t grad_fn grad_fn None grad_fn next_functions raise RuntimeError Attempted get grad_fn got None Is being created no-grad context t grad_fn reverse_closure roots list Node target_nodes set Node reverse_edges_dict - tuple set Node set Node This function returns reverse closure given roots i e set nodes can reached roots following reverse edges graph The target_nodes nodes we want include closure Recurse until we reach target node closure set Node = set visited_target_nodes = set q collections deque Node = collections deque node roots node None node closure closure add node q append node while q node = q popleft reverse_edges = reverse_edges_dict node fn reverse_edges fn closure fn None continue fn target_nodes visited_target_nodes add fn continue closure add fn q append fn closure visited_target_nodes construct_reverse_graph roots list Node - dict Node list Node q collections deque Node = collections deque root_seen set Node = set reverse_edges_dict dict Node list Node = collections defaultdict list node roots node None node root_seen q append node root_seen add node while q node = q popleft fn _ node next_functions fn None len reverse_edges_dict fn == q append fn reverse_edges_dict fn append node reverse_edges_dict get_param_groups inputs list Node params list Node reverse_edges_dict - list dict str Any Given list inputs list parameters list parameter groups where each group contains parameters intermediates connected parameters The returned list parameter groups list dictionaries where each dictionary contains following keys - params set parameters - intermediates set intermediates The returned list parameter groups list dictionaries reverse graph starts inputs goes up dOutput loss omits weights any subgraphs connecting weights closure inputs_closure _ = reverse_closure inputs set reverse_edges_dict param_groups dict Node dict str set = dict keyed intermediates param params closure intersected = reverse_closure param inputs_closure reverse_edges_dict param_group dict str set = params param intermediates intersected input_node intersected existing = param_groups get input_node existing None existing params = existing params union param_group params existing intermediates = existing intermediates union param_group intermediates param_group = existing param_groups input_node = param_group Sanity check union all param_groups params should equal all params union_params set Node = set seen_ids set int = set unique_param_groups = param_group param_groups values id param_group seen_ids seen_ids add id param_group unique_param_groups append param_group union_params = union_params union param_group params The assert will only true input tensor requires gradients otherwise autograd graph will miss first layer inputs assert union_params == set params unique_param_groups stage_backward_input stage_outputs_or_loss list torch Tensor output_grads Optional list torch Tensor input_values list torch Tensor weights Iterator Parameter - tuple tuple Optional torch Tensor list dict str Any Compute gradients only stage inputs respect stage outputs non-last stage loss last stage After computing input gradients we save intermediate nodes ` param_groups ` later use stage_backward_weight We don t need save any other intermediate nodes aren t needed dW because when we do dW calculation we start saved intermediates Detaching stage_outputs_or_loss end function important frees up memory autograd graph anticipating used later doesn t actually need stage_output_grad_fns list Node = list filter None map _get_grad_fn_or_grad_acc stage_outputs_or_loss stage_input_grad_fns list Node = list filter None map _get_grad_fn_or_grad_acc input_values weight_grad_fns list Node = list filter None map _get_grad_fn_or_grad_acc weights reverse_edges_dict = construct_reverse_graph stage_output_grad_fns param_groups = get_param_groups stage_input_grad_fns weight_grad_fns reverse_edges_dict handles = param_group param_groups i intermediate enumerate param_group intermediates get_hook param_group i hook grad_inputs param_group get grads None None param_group grads = None len param_group intermediates param_group grads i = grad_inputs hook These always split nodes we need recompute so save their inputs handle = intermediate register_prehook get_hook param_group i handles append handle output_grads None In case loss there no output_grads then we just use s output_grads = torch ones_like stage_output stage_output stage_outputs_or_loss Some inputs may used may require gradients so we filter them out input_values = inp inp input_values inp requires_grad dinputs = torch autograd grad stage_outputs_or_loss inputs=input_values grad_outputs=output_grads retain_graph=True Update gradients inputs inp dinput zip input_values dinputs inp grad None inp grad = dinput inp grad += dinput stage_outputs_or_loss used backwards after point so we can safely remove autograd graph allows autograd clear up graph dedicated tensor free up significant memory t stage_outputs_or_loss t detach_ hooks no longer necessary clean up consistency handle handles handle remove dinputs param_groups stage_backward_weight weights Iterator Parameter param_groups list dict str Any retain_graph=False - tuple Optional torch Tensor map weights param_group_weights grad_acc_to_weight = weight_grads list Optional torch Tensor = index weight enumerate weights grad_acc = _get_grad_fn_or_grad_acc weight grad_acc_to_weight grad_acc = weight index weight_grads append weight grad param_group param_groups valid_edges = valid_grad_outputs list torch Tensor = grads_tuple intermediate zip param_group grads param_group intermediates non_none_grads = g g grads_tuple g None non_none_grads summed_grad = sum non_none_grads valid_edges append GradientEdge intermediate pyrefly ignore bad-argument-type valid_grad_outputs append summed_grad Break reference cycle caused inside stage_backward_input- get_hook- hook The summarized cycle ` hook ` - cell - param_group - intermediates - ` hook ` because we install hook function onto each intermediate autograd nodes We need keep intermediates alive up until backward_weight we can free now del param_group intermediates valid_edges Only call autograd grad we have valid gradients NEW Able pass GradientEdge autograd grad output weights_edges = tuple GradientEdge w w param_group params dweights = torch autograd grad valid_edges weights_edges grad_outputs=valid_grad_outputs retain_graph=retain_graph release grad memory early after use del param_group grads grad_acc dw zip param_group params dweights weight index = grad_acc_to_weight grad_acc weight grad None weight grad = dw weight grad += dw grads original order weights provided tuple weight_grads stage_backward stage_output output_grads input_values outputs_with_grads_idxs Optional list int = None deprecated used - tuple Optional torch Tensor This helper function compute gradients stage inputs accumulate gradients stage module s parameters Given input value s corresponding gradient output value s compute accumulate gradients all parameter values leaves autograd trace well list gradients input values outputs_with_grads_idxs None Deprecated used runtime calls only exists compiler stage_output = stage_output i i outputs_with_grads_idxs output_grads = output_grads i i outputs_with_grads_idxs try stage_output may composite datatype like dict Extract all individual tensor values here stage_output_tensors list torch Tensor = output_grad_tensors list Optional torch Tensor = extract_tensors_with_grads output_val grad_val Don t delete me- see Note ref cycle extract_tensors_with_grads isinstance output_val torch Tensor output_val requires_grad output_val grad_fn None assert isinstance grad_val torch Tensor type None f Expected Tensor None gradient got type grad_val stage_output_tensors append output_val output_grad_tensors append grad_val isinstance output_val tuple list grad_val None assert isinstance grad_val tuple list f grad_value expected have type type output_val got type grad_val assert len output_val == len grad_val ov gv zip output_val grad_val extract_tensors_with_grads ov gv extract_tensors_with_grads isinstance output_val dict grad_val None assert isinstance grad_val dict assert set output_val keys == set grad_val keys k output_val keys extract_tensors_with_grads output_val k grad_val k extract_tensors_with_grads Output non-tensor type just ignore pass Note ref cycle break ref cycle would keep tensors alive until GC runs extract_tensors_with_grads refers cell holds refs any vars defined stage_backward used extract_tensors_with_grads extract_tensors_with_grads referred both stage_output_tensors output_grad_tensors itself extract_tensors_with_grads since makes recursive call stage_output_tensors kept alive above refcycle holds activation tensors which bad fix - explicitly pass ref fn so there no gc cycle anymore extract_tensors_with_grads stage_output output_grads extract_tensors_with_grads torch autograd backward stage_output_tensors grad_tensors=output_grad_tensors type ignore arg-type Extract gradients wrt input values grad_inputs list Optional torch Tensor = val input_values isinstance val torch Tensor grad_inputs append val grad Since gradients will pass back previous stages do require gradient accumulation decrementing gradients reference count point memory gradients will returned allocator soon next micro batch s get_bwd_send_ops comes current asynchronous send completes This prevents gradients persisting GPU memory entire duration step_microbatches until clear_runtime_states called val grad = None grad_inputs append None Alternative impl ` torch autograd grad ` Note ` torch autograd grad ` will accumulate gradients into model s parameters inputs_with_grad = val input_values isinstance val torch Tensor val requires_grad inputs_with_grad append val grad_inputs = torch autograd grad stage_output_tensors inputs_with_grad output_grad_tensors type ignore arg-type except Exception e exc_msg = f Failed run stage backward Stage output map_debug_info stage_output Output gradient map_debug_info output_grads Input map_debug_info input_values raise RuntimeError exc_msg e tuple grad_inputs TODO handling requires_grad=False dynamically Can we analyze during initial IR emission _null_coalesce_accumulate lhs rhs Coalesce two values even one them null returning non-null value lhs None rhs rhs None lhs torch add lhs rhs