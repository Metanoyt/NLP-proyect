collections abc Iterable typing Any no_type_check torch __all__ list str = WeakTensorKeyDictionary store relevant meta-data Tensor Parameter without changing s life-time NOTE Alternative add meta-data attribute tensor will serialize meta-data Tensor serialized param_to_optim_hook_handle_map = torch utils weak WeakTensorKeyDictionary param_to_acc_grad_map = torch utils weak WeakTensorKeyDictionary no_type_check _apply_optimizer_in_backward optimizer_class type torch optim Optimizer params Iterable torch nn Parameter optimizer_kwargs dict str Any register_hook bool = True - None Upon ` ` backward ` ` optimizer specified each parameter will fire after gradient has been accumulated into parameter Note - gradients these parameters will set None after ` ` backward ` ` This means any other optimizer specified via ` _apply_optimizer_in_backward ` over parameter will no-op Args optimizer_class Type torch optim Optimizer Optimizer apply parameter params Iterator nn Parameter parameters apply optimizer state optimizer_kwargs Dict str Any kwargs pass optimizer constructor register_hook bool whether register hook runs optimizer after gradient parameter accumulated This default way optimizer backward implemented specific use cases such DDP may wish override implement custom behavior Default = True Example params_generator = model parameters param_ = next params_generator remainder_params = list params_generator apply_optimizer_in_backward torch optim SGD param_ lr apply_optimizer_in_backward torch optim Adam remainder_params lr model sum backward after backward parameters will already have their registered optimizer s applied torch _C _log_api_usage_once torch distributed optim apply_optimizer_in_backward no_type_check _apply_optimizer_in_backward_to_param param torch nn Parameter - None view_as creates node autograd graph allows us access parameter s AccumulateGrad autograd function object We register hook object fire optimizer when gradient parameter ready has been accumulated into grad field Don t create new acc_grad we already have one i e shared parameters attaching multiple optimizers param param param_to_acc_grad_map param_to_acc_grad_map param = param view_as param grad_fn next_functions optimizer = optimizer_class param optimizer_kwargs hasattr param _in_backward_optimizers param _in_backward_optimizers = type ignore attr-defined TODO Remove these attributes once we have better way accessing optimizer classes kwargs parameter param _optimizer_classes = type ignore attr-defined param _optimizer_kwargs = type ignore attr-defined param _in_backward_optimizers append optimizer type ignore attr-defined param _optimizer_classes append optimizer_class type ignore attr-defined param _optimizer_kwargs append optimizer_kwargs type ignore attr-defined register_hook optimizer_hook _unused - None opt param _in_backward_optimizers type ignore attr-defined opt step param grad = None handle = param_to_acc_grad_map param register_hook optimizer_hook type ignore attr-defined param param_to_optim_hook_handle_map param_to_optim_hook_handle_map param = param_to_optim_hook_handle_map param append handle param params _apply_optimizer_in_backward_to_param param _get_in_backward_optimizers module torch nn Module - list torch optim Optimizer Return list in-backward optimizers applied ` ` module ` ` s parameters Note these optimizers intended directly have their ` ` step ` ` ` ` zero_grad ` ` methods called user intended used things like checkpointing Args module torch nn Module model retrieve in-backward optimizers Returns List torch optim Optimizer in-backward optimizers Example _apply_optimizer_in_backward torch optim SGD model parameters lr optims = _get_optimizers_in_backward model optims list torch optim Optimizer = param module parameters optims extend getattr param _in_backward_optimizers optims