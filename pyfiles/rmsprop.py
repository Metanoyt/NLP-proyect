mypy allow-untyped-defs r Implementation RMSprop algorithm typing cast Optional Union torch torch Tensor optimizer _capturable_doc _default_to_fused_or_foreach _differentiable_doc _disable_dynamo_if_unsupported _foreach_doc _get_capturable_supported_devices _get_scalar_dtype _maximize_doc _params_doc _to_scalar _use_grad_for_differentiable _view_as_real Optimizer ParamsT __all__ = RMSprop rmsprop RMSprop Optimizer noqa D __init__ params ParamsT lr Union float Tensor = e- alpha float = eps float = e- weight_decay float = momentum float = centered bool = False capturable bool = False foreach Optional bool = None maximize bool = False differentiable bool = False noqa D isinstance lr Tensor lr numel = raise ValueError Tensor lr must -element = lr raise ValueError f Invalid learning rate lr = eps raise ValueError f Invalid epsilon value eps = momentum raise ValueError f Invalid momentum value momentum = weight_decay raise ValueError f Invalid weight_decay value weight_decay = alpha raise ValueError f Invalid alpha value alpha defaults = lr lr momentum momentum alpha alpha eps eps centered centered weight_decay weight_decay capturable capturable foreach foreach maximize maximize differentiable differentiable super __init__ params defaults __setstate__ state noqa D super __setstate__ state group param_groups group setdefault momentum group setdefault centered False group setdefault foreach None group setdefault maximize False group setdefault differentiable False group setdefault capturable False p group params p_state = state get p len p_state = torch is_tensor p_state step step_val = float p_state step p_state step = torch tensor step_val dtype=_get_scalar_dtype device=p device group capturable torch tensor step_val dtype=_get_scalar_dtype _init_group group params_with_grad grads square_avgs momentum_buffer_list grad_avgs state_steps has_complex = False p group params p grad None continue has_complex &#124; = torch is_complex p params_with_grad append p p grad is_sparse raise RuntimeError RMSprop does support sparse gradients grads append p grad state = state p State initialization len state == state step = torch zeros dtype=_get_scalar_dtype device=p device group capturable torch zeros dtype=_get_scalar_dtype state square_avg = torch zeros_like p memory_format=torch preserve_format group momentum state momentum_buffer = torch zeros_like p memory_format=torch preserve_format group centered state grad_avg = torch zeros_like p memory_format=torch preserve_format square_avgs append state square_avg state_steps append state step group momentum momentum_buffer_list append state momentum_buffer group centered grad_avgs append state grad_avg has_complex _use_grad_for_differentiable step closure=None Perform single optimization step Args closure Callable optional A closure reevaluates model returns loss _cuda_graph_capture_health_check loss = None closure None torch enable_grad loss = closure group param_groups params_with_grad list Tensor = grads list Tensor = square_avgs list Tensor = grad_avgs list Tensor = momentum_buffer_list list Tensor = state_steps list Tensor = has_complex = _init_group group params_with_grad grads square_avgs momentum_buffer_list grad_avgs state_steps rmsprop params_with_grad grads square_avgs grad_avgs momentum_buffer_list state_steps lr=group lr alpha=group alpha eps=group eps weight_decay=group weight_decay momentum=group momentum centered=group centered foreach=group foreach maximize=group maximize differentiable=group differentiable capturable=group capturable has_complex=has_complex loss RMSprop __doc__ = r Implements RMSprop algorithm math \begin aligned \rule mm pt \\ \textbf input \alpha \text alpha \ \gamma \text lr \ \theta_ \text params \ f \theta \text objective \\ \hspace mm \lambda \text weight decay \ \mu \text momentum \ centered \ \epsilon \text epsilon \\ \textbf initialize v_ \leftarrow \text square average \ \textbf b _ \leftarrow \text buffer \ g^ ave _ \leftarrow \\ - ex \rule mm pt \\ \textbf \ t= \ \textbf \ \ldots \ \textbf do \\ \hspace mm g_t \leftarrow \nabla_ \theta f_t \theta_ t- \\ \hspace mm \ \lambda \neq \\ \hspace mm g_t \leftarrow g_t + \lambda \theta_ t- \\ \hspace mm v_t \leftarrow \alpha v_ t- + - \alpha g^ _t \hspace mm \\ \hspace mm \tilde v_t \leftarrow v_t \\ \hspace mm \ centered \\ \hspace mm g^ ave _t \leftarrow g^ ave _ t- \alpha + -\alpha g_t \\ \hspace mm \tilde v_t \leftarrow \tilde v_t - \big g^ ave _ t \big ^ \\ \hspace mm \ \mu \\ \hspace mm \textbf b _t\leftarrow \mu \textbf b _ t- + g_t \big \sqrt \tilde v_t + \epsilon \big \\ \hspace mm \theta_t \leftarrow \theta_ t- - \gamma \textbf b _t \\ \hspace mm \\ \hspace mm \theta_t \leftarrow \theta_ t- - \gamma g_t \big \sqrt \tilde v_t + \epsilon \big \hspace mm \\ \rule mm pt \\ - ex \bf \ \theta_t \\ - ex \rule mm pt \\ - ex \end aligned For further details regarding algorithm we refer ` lecture notes https www cs toronto edu ~tijmen csc slides lecture_slides_lec pdf ` _ G Hinton centered version ` Generating Sequences With Recurrent Neural Networks https arxiv org pdf v pdf ` _ The implementation here takes square root gradient average before adding epsilon note TensorFlow interchanges these two operations The effective learning rate thus math ` \gamma \sqrt v + \epsilon ` where math ` \gamma ` scheduled learning rate math ` v ` weighted moving average squared gradient + rf Args _params_doc lr float Tensor optional learning rate default e- alpha float optional smoothing constant default eps float optional term added denominator improve numerical stability default e- weight_decay float optional weight decay L penalty default momentum float optional momentum factor default centered bool optional ` ` True ` ` compute centered RMSProp gradient normalized estimation its variance _capturable_doc _foreach_doc _maximize_doc _differentiable_doc _single_tensor_rmsprop params list Tensor grads list Tensor square_avgs list Tensor grad_avgs list Tensor momentum_buffer_list list Tensor state_steps list Tensor lr float alpha float eps float weight_decay float momentum float centered bool maximize bool differentiable bool capturable bool has_complex bool torch jit is_scripting lr = _to_scalar lr i param enumerate params step = state_steps i If compiling compiler will handle cudagraph checks see note torch compile x capturable torch compiler is_compiling capturable capturable_supported_devices = _get_capturable_supported_devices param device type == step device type param device type capturable_supported_devices raise AssertionError f If capturable=True params state_steps must supported devices capturable_supported_devices grad = grads i grad = grad maximize -grad square_avg = square_avgs i step += weight_decay = grad = grad add param alpha=weight_decay is_complex_param = torch is_complex param is_complex_param param = torch view_as_real param grad = torch view_as_real grad square_avg = torch view_as_real square_avg square_avg mul_ alpha addcmul_ grad grad value= - alpha centered grad_avg = grad_avgs i is_complex_param grad_avg = torch view_as_real grad_avg grad_avg lerp_ grad - alpha avg = square_avg addcmul grad_avg grad_avg value=- sqrt_ avg = square_avg sqrt differentiable avg = avg add eps avg = avg add_ eps momentum buf = momentum_buffer_list i is_complex_param buf = torch view_as_real buf buf mul_ momentum addcdiv_ grad avg param add_ buf alpha=-lr param addcdiv_ grad avg value=-lr _multi_tensor_rmsprop params list Tensor grads list Tensor square_avgs list Tensor grad_avgs list Tensor momentum_buffer_list list Tensor state_steps list Tensor lr float alpha float eps float weight_decay float momentum float centered bool maximize bool differentiable bool capturable bool has_complex bool len params == differentiable raise AssertionError _foreach ops don t support autograd If compiling compiler will handle cudagraph checks see note torch compile x capturable torch compiler is_compiling capturable capturable_supported_devices = _get_capturable_supported_devices all p device type == step device type p device type capturable_supported_devices p step zip params state_steps strict=True raise AssertionError f If capturable=True params state_steps must supported devices capturable_supported_devices lr = _to_scalar lr grouped_tensors = Optimizer _group_tensors_by_device_and_dtype params grads square_avgs grad_avgs momentum_buffer_list state_steps type ignore list-item grouped_params_ grouped_grads_ grouped_square_avgs_ grouped_grad_avgs_ grouped_momentum_buffer_list_ grouped_state_steps_ _ grouped_tensors values grouped_params = cast list Tensor grouped_params_ grouped_grads = cast list Tensor grouped_grads_ grouped_square_avgs = cast list Tensor grouped_square_avgs_ grouped_state_steps = cast list Tensor grouped_state_steps_ has_complex state_and_grads = grouped_grads grouped_square_avgs momentum grouped_momentum_buffer_list = cast list Tensor grouped_momentum_buffer_list_ state_and_grads append grouped_momentum_buffer_list centered grouped_grad_avgs = cast list Tensor grouped_grad_avgs_ state_and_grads append grouped_grad_avgs _view_as_real grouped_params state_and_grads maximize grouped_grads = torch _foreach_neg grouped_grads type ignore assignment Update steps If steps CPU foreach will fall back slow path which for-loop calling t add over over will then wrapped into Tensor over over again which slower than we just wrapped once now The alpha required assure we go right overload torch compiler is_compiling grouped_state_steps is_cpu torch _foreach_add_ grouped_state_steps torch tensor device= cpu alpha= torch _foreach_add_ grouped_state_steps weight_decay = Reuse intermediate memory grouped_grads already allocated maximize maximize torch _foreach_add_ grouped_grads grouped_params alpha=weight_decay grouped_grads = torch _foreach_add type ignore assignment grouped_grads grouped_params alpha=weight_decay torch _foreach_mul_ grouped_square_avgs alpha torch _foreach_addcmul_ grouped_square_avgs grouped_grads grouped_grads value= - alpha centered grouped_grad_avgs = cast list Tensor grouped_grad_avgs_ torch _foreach_lerp_ grouped_grad_avgs grouped_grads - alpha avg = torch _foreach_addcmul grouped_square_avgs grouped_grad_avgs grouped_grad_avgs value=- torch _foreach_sqrt_ avg torch _foreach_add_ avg eps avg = torch _foreach_sqrt grouped_square_avgs torch _foreach_add_ avg eps momentum grouped_momentum_buffer_list = cast list Tensor grouped_momentum_buffer_list_ torch _foreach_mul_ grouped_momentum_buffer_list momentum torch _foreach_addcdiv_ grouped_momentum_buffer_list grouped_grads avg If LR tensor branch will internally call item which will cause silent incorrectness we capturing capturable isinstance lr torch Tensor momentum_lr = torch _foreach_mul grouped_momentum_buffer_list -lr torch _foreach_add_ grouped_params momentum_lr torch _foreach_add_ grouped_params grouped_momentum_buffer_list alpha=-lr If LR tensor branch will internally call item which will cause silent incorrectness we capturing capturable isinstance lr torch Tensor torch _foreach_div_ avg -lr torch _foreach_addcdiv_ grouped_params grouped_grads avg torch _foreach_addcdiv_ grouped_params grouped_grads avg value=-lr _disable_dynamo_if_unsupported single_tensor_fn=_single_tensor_rmsprop rmsprop params list Tensor grads list Tensor square_avgs list Tensor grad_avgs list Tensor momentum_buffer_list list Tensor state_steps list Tensor kwonly args defaults supported functions compiled torchscript issue setting kwarg now functional API compiled torch distributed optim foreach Optional bool = None maximize bool = False differentiable bool = False capturable bool = False has_complex bool = False lr float alpha float eps float weight_decay float momentum float centered bool r Functional API performs rmsprop algorithm computation See ` ~torch optim RMSProp ` details check slow during compilation so we skip s strictly needed we can add check back dynamo torch compiler is_compiling all isinstance t torch Tensor t state_steps raise RuntimeError API has changed ` state_steps ` argument must contain list singleton tensors foreach None _ foreach = _default_to_fused_or_foreach params differentiable use_fused=False foreach torch jit is_scripting raise RuntimeError torch jit script supported foreach optimizers foreach torch jit is_scripting func = _multi_tensor_rmsprop func = _single_tensor_rmsprop func params grads square_avgs grad_avgs momentum_buffer_list state_steps lr=lr alpha=alpha eps=eps weight_decay=weight_decay momentum=momentum centered=centered maximize=maximize capturable=capturable differentiable=differentiable has_complex=has_complex