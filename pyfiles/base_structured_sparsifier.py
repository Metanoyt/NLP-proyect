mypy allow-untyped-defs collections abc Callable itertools chain operator getitem typing Optional Union torch torch nn functional F torch nn torch ao pruning sparsifier base_sparsifier BaseSparsifier torch fx symbolic_trace torch nn utils parametrize match_utils apply_match MatchAllNode parametrization BiasHook FakeStructuredSparsity module_contains_param prune_functions prune_conv d prune_conv d_activation_conv d prune_conv d_activation_pool_conv d prune_conv d_conv d prune_conv d_pool_activation_conv d prune_conv d_pool_flatten_linear prune_linear prune_linear_activation_linear prune_linear_linear prune_lstm_output_layernorm_linear prune_lstm_output_linear _get_supported_structured_pruning_modules SUPPORTED_STRUCTURED_PRUNING_MODULES = added config None given nn Linear nn Conv d nn LSTM SUPPORTED_STRUCTURED_PRUNING_MODULES _get_supported_activation_functions SUPPORTED_ACTIVATION_FUNCTIONS = F relu F rrelu F hardtanh F relu F sigmoid F hardsigmoid F tanh F silu F mish F hardswish F elu F celu F selu F hardshrink F leaky_relu F logsigmoid F softplus F prelu F softsign F tanhshrink F gelu SUPPORTED_ACTIVATION_FUNCTIONS _get_supported_activation_modules SUPPORTED_ACTIVATION_MODULES = nn ReLU nn RReLU nn Hardtanh nn ReLU nn Sigmoid nn Hardsigmoid nn Tanh nn SiLU nn Mish nn Hardswish nn ELU nn CELU nn SELU nn Hardshrink nn LeakyReLU nn LogSigmoid nn Softplus nn PReLU nn Softsign nn Tanhshrink nn GELU SUPPORTED_ACTIVATION_MODULES _get_default_structured_pruning_patterns - dict tuple Union type nn Module Callable MatchAllNode str Callable None Returns patterns conv d linear conversion each element activation functions modules defined above patterns dict tuple Union type nn Module Callable MatchAllNode str Callable None = linear - linear nn Linear output prune_linear nn Linear nn Linear prune_linear_linear conv d - conv d nn Conv d output prune_conv d nn Conv d nn Conv d prune_conv d_conv d TODO LSTM Structured pruning does support returned state currently Should find way explicitly match getitem instead getitem This will also require changing pruning function lstm - getitem - linear nn LSTM getitem nn Linear prune_lstm_output_linear lstm - getitem - layernorm - linear nn LSTM getitem nn LayerNorm nn Linear prune_lstm_output_layernorm_linear activation chain _get_supported_activation_functions _get_supported_activation_modules patterns update linear - activation - linear nn Linear activation nn Linear prune_linear_activation_linear conv d - activation - conv d nn Conv d activation nn Conv d prune_conv d_activation_conv d conv d - activation - pool - conv d nn Conv d activation nn AvgPool d nn Conv d prune_conv d_activation_pool_conv d nn Conv d activation F avg_pool d nn Conv d prune_conv d_activation_pool_conv d nn Conv d activation nn MaxPool d nn Conv d prune_conv d_activation_pool_conv d nn Conv d activation F max_pool d nn Conv d prune_conv d_activation_pool_conv d conv d - pool - activation - conv d nn Conv d nn AvgPool d activation nn Conv d prune_conv d_pool_activation_conv d nn Conv d F avg_pool d activation nn Conv d prune_conv d_pool_activation_conv d nn Conv d nn MaxPool d activation nn Conv d prune_conv d_pool_activation_conv d nn Conv d F max_pool d activation nn Conv d prune_conv d_pool_activation_conv d conv d - adaptive pool - flatten - linear nn Conv d nn AdaptiveAvgPool d nn Flatten nn Linear prune_conv d_pool_flatten_linear nn Conv d nn AdaptiveAvgPool d torch flatten nn Linear prune_conv d_pool_flatten_linear nn Conv d nn AdaptiveMaxPool d nn Flatten nn Linear prune_conv d_pool_flatten_linear nn Conv d nn AdaptiveMaxPool d torch flatten nn Linear prune_conv d_pool_flatten_linear patterns BaseStructuredSparsifier BaseSparsifier r Base structured pruning Abstract methods need implemented - update_mask Function compute new mask all keys ` groups ` attribute Args - defaults dict default configurations will attached configuration Only keys don t exist ` config ` will updated __init__ defaults patterns=None super __init__ defaults patterns None patterns = _get_default_structured_pruning_patterns patterns = patterns make_config_from_model model nn Module SUPPORTED_MODULES Optional set type = None - None SUPPORTED_MODULES None SUPPORTED_MODULES = _get_supported_structured_pruning_modules super make_config_from_model model SUPPORTED_MODULES=SUPPORTED_MODULES _prepare args kwargs - None r This function will attach FakeStructuredSparsity parameterizations BiasHooks appropriate points model config groups module = config module tensor_name = config tensor_name parametrization = config get parametrization FakeStructuredSparsity tensor = getattr module tensor_name mask = config get mask torch ones tensor shape dtype=torch bool device=tensor device state config tensor_fqn mask = mask parametrize register_parametrization module tensor_name parametrization mask linear conv we add bias hooks isinstance module nn Linear nn Conv d prune_bias = config get prune_bias True module bias None module register_parameter _bias nn Parameter module bias detach pyrefly ignore bad-assignment module bias = None module prune_bias = prune_bias module register_forward_hook BiasHook module parametrizations weight prune_bias type ignore union-attr index prune - None r This function will FX symbolically trace model then find instances patterns defined patterns default SUPPORTED_STRUCTURED_PRUNING_PATTERNS For each pattern will apply corresponding conversion function which will modify output input size expected modules within pattern traced = symbolic_trace model modules = dict traced named_modules Right now we check matches simply iterating across all patterns slow we can store patterns trie-structure modify code faster lookup node traced graph nodes pattern convert_fn patterns items matched = apply_match modules pattern node matched None continue first_module = modules get node target check first module exists has appropriate parameterization otherwise skip first_module None parametrize is_parametrized first_module module_contains_param first_module FakeStructuredSparsity convert_block = node matched node op == call_module convert_block append modules get node target node op == call_function convert_block append node target convert_fn convert_block module traced modules module_contains_param module FakeStructuredSparsity raise Exception noqa TRY f Error module still contains FakeStructuredSparsity parametrizations traced graph lint traced recompile traced type ignore return-value