usr bin env python gc importlib logging os re sys warnings collections namedtuple os path abspath exists torch try common BenchmarkRunner load_yaml_file main except ImportError common BenchmarkRunner load_yaml_file main torch _dynamo testing collect_results reduce_to_scalar_loss torch _dynamo utils clone_inputs We primarily interested tf datatype torch backends cuda matmul allow_tf = True Enable FX graph caching TORCHINDUCTOR_FX_GRAPH_CACHE os environ torch _inductor config fx_graph_cache = True Enable Autograd caching TORCHINDUCTOR_AUTOGRAD_CACHE os environ torch _functorch config enable_autograd_cache = True _reassign_parameters model torch_geometric models register parameter tensors due https github com pyg-team pytorch_geometric blob master torch_geometric nn dense linear py#L -L Since unusual thing do we just reassign them parameters state_dict_hook module destination prefix local_metadata name param module named_parameters isinstance destination name torch Tensor isinstance destination name torch nn Parameter destination name = torch nn Parameter destination name model _register_state_dict_hook state_dict_hook setup_torchbench_cwd original_dir = abspath os getcwd os environ KALDI_ROOT = tmp avoids some spam torchbench_dir torchbenchmark torchbenchmark torchbench benchmark torchbenchmark torchbench benchmark torchbenchmark torchbench benchmark exists torchbench_dir break exists torchbench_dir torchbench_dir = abspath torchbench_dir os chdir torchbench_dir sys path append torchbench_dir original_dir process_train_model_output = TorchBenchmarkRunner BenchmarkRunner __init__ super __init__ suite_name = torchbench optimizer = None property _config load_yaml_file torchbench yaml property _skip _config skip property _batch_size _config batch_size property _tolerance _config tolerance property _require_larger_multiplier_for_smaller_tensor _config require_larger_multiplier_for_smaller_tensor property _accuracy _config accuracy property skip_models _skip all property skip_models_for_cpu _skip device cpu property skip_models_for_cpu_aarch _skip device cpu_aarch property skip_models_for_cuda _skip device cuda property skip_models_for_xpu _skip device xpu property skip_models_for_freezing_cuda _skip freezing cuda property disable_cudagraph_models _config disable_cudagraph property skip_models_for_freezing_cpu _skip freezing cpu property slow_models _config slow property very_slow_models _config very_slow property non_deterministic_models _config non_deterministic property get_output_amp_train_process_func process_train_model_output property skip_not_suitable_for_training_models _skip test training property failing_fx trt_models _config trt_not_yet_working property force_amp_for_fp _bf _models _config dtype force_amp_for_fp _bf _models property force_fp _for_bf _models _config dtype force_fp _for_bf _models property skip_accuracy_checks_large_models_dashboard args dashboard args accuracy _accuracy skip large_models set property skip_accuracy_check_as_eager_non_deterministic args accuracy args training _accuracy skip eager_not_deterministic set property skip_multiprocess_models _skip multiprocess property skip_models_due_to_control_flow _skip control_flow property skip_models_due_to_export_not_supported _skip export_not_supported property guard_on_nn_module_models vision_maskrcnn property inline_inbuilt_nn_modules_models basic_gnn_edgecnn drq hf_Reformer DALLE _pytorch detectron _maskrcnn_r_ _fpn detectron _maskrcnn_r_ _fpn vision_maskrcnn doctr_reco_predictor load_model device model_name batch_size=None part=None extra_args=None args enable_activation_checkpointing raise NotImplementedError Activation checkpointing implemented Torchbench models is_training = args training use_eval_mode = args use_eval_mode candidates = f torchbenchmark models model_name f torchbenchmark canary_models model_name f torchbenchmark models fb model_name c candidates try module = importlib import_module c break except ModuleNotFoundError e e name = c raise raise ImportError f could any candidates benchmark_cls = getattr module Model None benchmark_cls None raise NotImplementedError f model_name Model None hasattr benchmark_cls name benchmark_cls name = model_name cant_change_batch_size = getattr benchmark_cls ALLOW_CUSTOMIZE_BSIZE True model_name _config dont_change_batch_size cant_change_batch_size batch_size = None batch_size None is_training model_name _batch_size training batch_size = _batch_size training model_name batch_size None is_training model_name _batch_size inference batch_size = _batch_size inference model_name Control memory footprint few models args accuracy model_name _accuracy max_batch_size batch_size = min batch_size _accuracy max_batch_size model_name workaround RuntimeError allowed set torch backends cudnn flags torch backends __allow_nonbracketed_mutation_flag = True extra_args None extra_args = part extra_args += -- part part sam_fast only runs amp model_name == sam_fast args amp = True setup_amp model_name == vision_maskrcnn is_training Output vision_maskrcnn model list bounding boxes sorted basis their scores This makes accuracy comparison hard torch compile torch compile can cause minor divergences output because how fusion works amp TorchInductor compared eager Therefore instead looking all bounding boxes we compare only top model_kwargs = box_detections_per_img benchmark = benchmark_cls test= train device=device batch_size=batch_size extra_args=extra_args model_kwargs=model_kwargs use_eval_mode = True is_training benchmark = benchmark_cls test= train device=device batch_size=batch_size extra_args=extra_args benchmark = benchmark_cls test= eval device=device batch_size=batch_size extra_args=extra_args model example_inputs = benchmark get_module model_name basic_gnn_edgecnn basic_gnn_gcn basic_gnn_sage basic_gnn_gin _reassign_parameters model Models must train mode while training is_training use_eval_mode model_name _config only_training model train model eval gc collect batch_size = benchmark batch_size model_name == torchrec_dlrm batch_namedtuple = namedtuple Batch dense_features sparse_features labels example_inputs = tuple batch_namedtuple dense_features=batch dense_features sparse_features=batch sparse_features labels=batch labels batch example_inputs Torchbench has quite different setup yolov so directly passing right example_inputs model_name == yolov example_inputs = torch rand batch_size device See https github com pytorch benchmark issues model_name == maml_omniglot batch_size = assert example_inputs shape == batch_size model_name == vision_maskrcnn batch_size = global current_name current_device current_device = device current_name = benchmark name args trace_on_xla work around https github com pytorch xla issues torch_xla noqa F Turning off kv cache torchbench models This right thing do torchbench models way outdated since we using torchbench pt dashboard track regressions rather than improving performance we just setting kv cache false Real transformers benchmarks will added soon using different infra model_name startswith hf hasattr model config hasattr model config use_cache model config use_cache = False validate_model model example_inputs device benchmark name model example_inputs batch_size iter_model_names args torchbenchmark _list_canary_model_paths _list_model_paths models = _list_model_paths models += f f _list_canary_model_paths os path basename f _config canary_models models sort start end = get_benchmark_indices len models index model_path enumerate models index start index = end continue model_name = os path basename model_path re search &#124; join args filter model_name re IGNORECASE re search &#124; join args exclude model_name re IGNORECASE model_name args exclude_exact model_name skip_models continue yield model_name pick_grad name is_training is_training name maml torch enable_grad torch no_grad use_larger_multiplier_for_smaller_tensor name name _require_larger_multiplier_for_smaller_tensor get_tolerance_and_cosine_flag is_training current_device name tolerance = e- cosine = args cosine Increase tolerance torch allclose args float args amp args freezing freezing = _tolerance freezing higher_fp = freezing get higher_fp None even_higher = freezing get even_higher None higher_fp name higher_fp e- cosine even_higher name even_higher e- cosine name _tolerance higher_fp e- cosine name _tolerance even_higher e- cosine e- cosine args bfloat name _tolerance higher_bf e- cosine current_device == xpu name _tolerance higher_bf _xpu e- cosine is_training current_device == cuda current_device == xpu tolerance = e- name _tolerance cosine cosine = True name _tolerance higher tolerance = e- name _tolerance even_higher tolerance = e- tolerance cosine compute_loss pred reduce_to_scalar_loss pred forward_pass mod inputs collect_outputs=True autocast autocast_arg isinstance inputs dict mod inputs mod inputs forward_and_backward_pass mod inputs collect_outputs=True cloned_inputs = clone_inputs inputs optimizer_zero_grad mod autocast autocast_arg isinstance cloned_inputs dict pred = mod cloned_inputs pred = mod cloned_inputs loss = compute_loss pred grad_scaler scale loss backward optimizer_step collect_outputs collect_results mod None loss cloned_inputs None torchbench_main original_dir = setup_torchbench_cwd logging basicConfig level=logging WARNING warnings filterwarnings ignore main TorchBenchmarkRunner original_dir __name__ == __main__ torchbench_main