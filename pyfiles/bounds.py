logging operator functools partial typing Any Callable Optional Union sympy sympy Expr torch torch utils _sympy value_ranges bound_sympy SymPyValueRangeAnalysis ValueRanges utils _sympy functions PowByNatural utils _sympy numbers int_oo loop_body InterpreterShim LoopBody LoopBodyBlock ops_handler DefaultHandler ReductionType StoreMode utils cache_on_self dominated_nodes virtualized V log = logging getLogger __name__ BoundVars Performs Value Range Analysis LoopBody s fx graph calling BoundVars run It exposes ranges nodes ` bounds ` variable Note A current limitation analysis just works per-loop basis We should able propagate bounds between across whole graph This may benefit case bounded variable returned kernel fed into another __init__ loop_body LoopBody - None upper_bound v Union Expr int - int bound_sympy v upper isinstance v Expr v loop_body = loop_body replacement_vals = k ValueRanges Expr upper_bound v - k v loop_body var_ranges items avoid computing these values pessimistically assume they unbounded unbounded_vars = dominated_nodes node node loop_body get_nodes node target load reduction operator getitem masked_subblock node target To access variable call ` get_bounds ` _bounds dict torch fx Node ValueRanges Expr = __repr__ - str f __class__ __name__ f loop_body= loop_body \n f replacement_vals= replacement_vals \n f unbounded_vars= unbounded_vars \n f _bounds= _bounds cache_on_self get_bounds - dict torch fx Node ValueRanges Expr submodules = swap_submodules loop_body submodules Initialize environment unbounded variables node unbounded_vars we need evaluate masked_subblock recurse we need set indirect values isinstance node target str masked_subblock node target set_indirect node target _bounds node = ValueRanges Expr unknown V set_ops_handler ValueRangeAnalysis interpreter = InterpreterShim loop_body root_block graph submodules log debug get_bounds \n s loop_body root_block graph interpreter run V get_ops_handler initial_env=self _bounds _bounds swap_submodules submodules dict str Callable Any - dict str Callable ValueRanges Expr result dict str Callable ValueRanges Expr = key submodules keys key == get_index result key = get_index masked_subblock key subblock = loop_body subblocks key The result within lambda will reference final set modules end for-loop stores reference bind subblock function because python lambdas close over reference moving lambda out make_fn would close over reference subblock so all lambdas would have same subblock reference final subblock loop make_fn subblock LoopBodyBlock - Callable Any Any ValueRanges Expr lambda mask value masked_subblock subblock _bounds mask value result result key = make_fn subblock set_indirect key idx = int key len set_indirect var = loop_body indirect_vars idx indirect = partial set_indirect var result key = indirect assert scan key result key = submodules key result masked_subblock subblock LoopBodyBlock env dict torch fx Node ValueRanges Expr mask Any value Any submodules dict str Callable Any - ValueRanges Expr interp = InterpreterShim subblock graph submodules interp run V get_ops_handler initial_env=env output = node node subblock graph nodes node target == output assert len output == dont bother unioning value since load buffer will pessimistically assumed inf anyway interp env output set_indirect old Expr new ValueRanges Expr - ValueRanges Expr assert isinstance new ValueRanges replacement_vals old = new new get_index name str - ValueRanges Expr expr = loop_body indexing_exprs name bound = replacement_vals get expr bound None bound = bound_sympy expr replacement_vals The following assertion true time writing We don t assert execute bound_sympy when bound None assert bound None bound == bound_sympy expr replacement_vals replacement_vals name = bound bound ValueRangeAnalysis SymPyValueRangeAnalysis DefaultHandler __init__ - None name = ValueRangeAnalysis boolean_operators = xor logical_and logical_or logical_not op boolean_operators setattr op bool_handler staticmethod bool_handler args Any kwargs Any - ValueRanges Any just assuming bools can have both values ValueRanges sympy false sympy true type ignore arg-type _default name str args tuple Any kwargs dict str Any - Any many ops unlikely show up optimizable indexing compute so we dont have full coverage ValueRanges unknown load name str index sympy Expr - ValueRanges Any ValueRanges unknown store name str index sympy Expr value Any mode StoreMode = None - None reduction dtype torch dtype src_dtype torch dtype reduction_type ReductionType value Any - ValueRanges Any ValueRanges unknown classmethod index_expr cls index Any dtype torch dtype - ValueRanges Any assert isinstance index ValueRanges cls to_dtype index dtype staticmethod to_dtype x Any dtype torch dtype src_dtype Optional torch dtype = None use_compute_types bool = True - ValueRanges Any x = ValueRanges wrap x dtype == torch bool x is_singleton ValueRanges wrap x lower = x is_bool x x ValueRanges wrap sympy true ValueRanges sympy false sympy true cast x Any dtype torch dtype - sympy Expr dtype int float dtype is_floating_point sympy Float x x int_oo -int_oo x try sympy Integer x except TypeError inf cannot cast Integer x x is_bool x is_singleton val = x lower ValueRanges wrap cast val dtype ValueRanges cast dtype cast dtype int float float int ValueRanges cast x lower dtype cast x upper dtype staticmethod square x Any - ValueRanges Any ValueRanges convex_min_zero_map x lambda y PowByNatural y staticmethod neg x Any - ValueRanges Any ValueRanges decreasing_map x operator neg TODO slightly inaccurate because truncdiv operates integer precision we re going through float truediv which means we can potentially lose precision bounds classmethod truncdiv cls Any b Any - ValueRanges Any x = cls truediv b x == ValueRanges unknown x cls trunc x classmethod sub cls Any b Any - ValueRanges Any cls add cls neg b