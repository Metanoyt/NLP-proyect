Owner s module cuda collections contextlib ctypes gc io queue sys tempfile threading unittest itertools chain repeat typing NamedTuple Union torch torch cuda comm comm torch nn parallel scatter_gather torch testing _internal common_cuda _create_scaling_case _create_scaling_models_optimizers TEST_MULTIGPU torch testing _internal common_utils get_cycles_per_ms instantiate_parametrized_tests IS_JETSON IS_REMOTE_GPU IS_SANDCASTLE NoTest run_tests serialTest skipCUDANonDefaultStreamIf TEST_CUDA TestCase TEST_CUDAMALLOCASYNC = TEST_CUDA torch cuda get_allocator_backend == cudaMallocAsync TEST_CUDA print CUDA available skipping tests file=sys stderr TestCase = NoTest noqa F TestCudaMultiGPU TestCase FIFTY_MIL_CYCLES = _check_memory_stat_consistency snapshot = torch cuda memory_snapshot expected_each_device = collections defaultdict lambda collections defaultdict int segment snapshot expandable = segment is_expandable expected = expected_each_device segment device pool_str = segment segment_type + _pool expandable expected segment all current += expected segment + pool_str + current += expected allocated_bytes all current += segment allocated_size expected allocated_bytes + pool_str + current += segment allocated_size expected reserved_bytes all current += segment total_size expected reserved_bytes + pool_str + current += segment total_size expected active_bytes all current += segment active_size expected active_bytes + pool_str + current += segment active_size expected requested_bytes all current += segment requested_size expected requested_bytes + pool_str + current += segment requested_size sum_requested = is_split = len segment blocks block segment blocks block state == active_allocated expected allocation all current += expected allocation + pool_str + current += block state startswith active_ sum_requested += block requested_size expected active all current += expected active + pool_str + current += block state == inactive is_split expandable expected inactive_split all current += expected inactive_split + pool_str + current += expected inactive_split_bytes all current += block size expected inactive_split_bytes + pool_str + current += block size assertEqual sum_requested segment requested_size device expected expected_each_device items stats = torch cuda memory_stats device k v expected items assertEqual v stats k test_cuda_synchronize torch cuda synchronize torch cuda synchronize cuda torch cuda synchronize cuda torch cuda synchronize torch cuda synchronize torch device cuda TEST_MULTIGPU torch cuda synchronize cuda torch cuda synchronize torch cuda synchronize torch device cuda assertRaisesRegex ValueError Expected cuda device torch cuda synchronize torch device cpu assertRaisesRegex ValueError Expected cuda device torch cuda synchronize cpu staticmethod _test_memory_stats_generator device=None N= device None device = torch cuda current_device m = torch cuda memory_allocated device last_m_arr = torch cuda memory_allocated device max_m_arr = torch cuda max_memory_allocated device last_r_arr = torch cuda memory_reserved device max_r_arr = torch cuda max_memory_reserved device alloc size torch cuda device device NOTE do use methods can have additional memory overhead e g inplace random sampling methods they can leave some memory occupied even after being deallocated e g initialized RNG state causing some memory checks below fail torch cuda FloatTensor size assert_change comp= empty_cache=False reset_peak=False comp increased comp = equal comp decreased new_m = torch cuda memory_allocated device new_max_m = torch cuda max_memory_allocated device comp assertGreater new_m last_m_arr comp assertLess new_m last_m_arr assertEqual new_m last_m_arr assertLessEqual new_m new_max_m assertGreaterEqual new_max_m max_m_arr last_m_arr = new_m max_m_arr = new_max_m new_r = torch cuda memory_reserved device new_max_r = torch cuda max_memory_reserved device emptying cache may happen due allocation empty_cache so we can t assert new_c = last_c assertLessEqual new_r new_max_r assertGreaterEqual new_max_r max_r_arr last_r_arr = new_r max_r_arr = new_max_r stat_key_n_sync = num_sync_all_streams stat_key_n_alloc = num_device_alloc stat_key_n_free = num_device_free empty_cache num_sync_ = torch cuda memory_stats device get stat_key_n_sync - assertGreaterEqual num_sync_ num_alloc_ = torch cuda memory_stats device get stat_key_n_alloc - current memory usage greater than zero we must have allocated something assertGreaterEqual num_alloc_ new_m == num_free_ = torch cuda memory_stats device get stat_key_n_free - assertGreaterEqual num_free_ empty_cache will enforce call release_cached_blocks torch cuda empty_cache num_sync_ = torch cuda memory_stats device get stat_key_n_sync - assertEqual num_sync_ + num_sync_ num_alloc_ = torch cuda memory_stats device get stat_key_n_alloc - assertGreaterEqual num_alloc_ num_alloc_ num_free_ = torch cuda memory_stats device get stat_key_n_free - assertGreaterEqual num_free_ num_free_ new_r = torch cuda memory_reserved device new_max_r = torch cuda max_memory_reserved device assertLessEqual new_r last_r_arr assertLessEqual new_r new_max_r assertEqual new_max_r max_r_arr last_r_arr = new_r reset_peak torch cuda reset_peak_memory_stats device assertEqual torch cuda memory_allocated device last_m_arr assertEqual torch cuda max_memory_allocated device last_m_arr max_m_arr = last_m_arr assertEqual torch cuda memory_reserved device last_r_arr assertEqual torch cuda max_memory_reserved device last_r_arr max_r_arr = last_r_arr assert_change assert_change reset_peak=True assert_change empty_cache=True assert_change reset_peak=True assert_change yield tensors = alloc alloc alloc m = torch cuda memory_allocated device assert_change yield tensors = i range int N + small ones tensors append alloc i i assert_change yield i range int N + large ones tensors append alloc i i i i assert_change reset_peak= i == yield tensors append alloc assert_change yield permute = i torch randperm len tensors permute append tensors i assert_change yield del tensors assert_change yield tensors = permute assert_change yield del permute assert_change reset_peak=True yield i range int N x = tensors i numel del tensors i assert_change -x case tensors i empty yield i range int N + tensors append alloc i i i assert_change yield del tensors assert_change - reset_peak=True assert_change assertEqual torch cuda memory_allocated device m yield True del tensors assert_change - reset_peak=True assertEqual torch cuda memory_allocated device m test empty_cache reset_peak assert_change empty_cache=True assert_change reset_peak=True unittest skipIf TEST_CUDAMALLOCASYNC temporarily disabled serialTest test_memory_stats gc collect torch cuda empty_cache _ _test_memory_stats_generator _check_memory_stat_consistency unittest skipIf TEST_CUDAMALLOCASYNC temporarily disabled unittest skipIf TEST_MULTIGPU only one GPU detected test_memory_stats_multigpu advance generator end flag advance gen end end try next gen except StopIteration end = True end interlace torch cuda empty_cache gen = _test_memory_stats_generator device= cuda N= gen = _test_memory_stats_generator device=torch device cuda N= end = end = False while end end end = advance gen end end = advance gen end semi-random order torch cuda empty_cache gen = _test_memory_stats_generator device= N= gen = _test_memory_stats_generator device=torch device cuda N= end = end = False while end end end = advance gen end end gen _max_times = torch LongTensor random_ gen _max_times = torch inf t = while t gen _max_times end end = advance gen end t += unittest skipIf TEST_MULTIGPU only one GPU detected test_autogpu x = torch randn cuda y = torch randn cuda assertEqual x get_device assertEqual x get_device torch cuda device z = torch randn cuda assertEqual z get_device q = x add y assertEqual q get_device w = torch randn cuda assertEqual w get_device assertEqual y cuda get_device z = z cuda assertEqual z get_device unittest skipIf TEST_MULTIGPU only one GPU detected test_new x = torch randn cuda assertEqual x new get_device assertEqual x new device= get_device torch cuda device assertEqual x new get_device assertEqual x new device= get_device unittest skipIf TEST_MULTIGPU only one GPU detected test_copy_device x = torch randn cuda torch cuda device y = x cuda assertEqual y get_device assertIs y cuda y z = y cuda assertEqual z get_device assertIs z cuda z x = torch randn torch cuda device y = x cuda assertEqual y get_device assertIs y cuda y z = y cuda assertEqual z get_device assertIs z cuda z _test_copy_sync_current_stream x y x_plus_one = x + s = torch cuda Stream device=x device s = torch cuda Stream device=y device s = torch cuda Stream device=x device s = torch cuda Stream device=y device same dst stream different src streams torch cuda stream s torch cuda _sleep TestCudaMultiGPU FIFTY_MIL_CYCLES torch cuda stream s y copy_ x_plus_one torch cuda stream s torch cuda stream s y copy_ x s synchronize The copy synchronized current streams both src dst In above test _sleep op s will block copy s both copies synchronized s dst device Hence x copied y after x_plus_one copied y If x y same device both copy ops synchronized s assertEqual y x same src stream different dst streams torch cuda stream s torch cuda _sleep TestCudaMultiGPU FIFTY_MIL_CYCLES torch cuda stream s y copy_ x_plus_one torch cuda stream s torch cuda stream s y copy_ x s synchronize Similarly both copy ops synchronized s assertEqual y x unittest skipIf TEST_MULTIGPU only one GPU detected test_copy_streams d = torch device cuda x = torch zeros device=d d = torch device cuda x = torch zeros device=d _test_copy_sync_current_stream x x x = torch zeros device=d _test_copy_sync_current_stream x x unittest skipIf TEST_MULTIGPU only one GPU detected test_cat_autogpu x = torch randn cuda y = torch randn cuda z = torch cat x y assertEqual z get_device x get_device unittest skipIf torch cuda device_count = Loading cuda tensor test_load_nonexistent_device Setup create serialized file object cuda restore location tensor = torch randn device= cuda buf = io BytesIO torch save tensor buf NB might work future serialization changes buf = io BytesIO buf getvalue replace b cuda b cuda msg = r Attempting deserialize object CUDA device assertRaisesRegex RuntimeError msg _ = torch load buf unittest skipIf TEST_MULTIGPU detected only one GPU test_multigpu_serialization_remap x = torch randn cuda torch randn cuda gpu_remap storage location location == cuda storage cuda tempfile NamedTemporaryFile f torch save x f f seek x_copy = torch load f map_location=gpu_remap original copy zip x x_copy assertEqual copy original assertIs type copy type original assertEqual copy get_device unittest skipIf TEST_MULTIGPU detected only one GPU test_multigpu_serialization_remap_dict x = torch randn cuda torch randn cuda tempfile NamedTemporaryFile f torch save x f f seek x_copy = torch load f map_location= cuda cuda original copy zip x x_copy assertEqual copy original assertIs type copy type original assertEqual copy get_device unittest skipIf TEST_MULTIGPU detected only one GPU test_multigpu_storage_clone x = torch randn device= cuda storage y = x clone assertEqual x get_device y get_device t byte char short int long half double assertEqual getattr x t get_device x get_device unittest skipIf TEST_MULTIGPU detected only one GPU test_cuda_set_device x = torch randn torch cuda device assertEqual x cuda get_device torch cuda set_device assertEqual x cuda get_device torch cuda device assertEqual x cuda get_device assertEqual x cuda get_device torch cuda set_device assertEqual x cuda get_device unittest skipIf TEST_MULTIGPU detected only one GPU test_current_stream d = torch device cuda d = torch device cuda s = torch cuda current_stream s = torch cuda current_stream device= s = torch cuda current_stream device= assertEqual d s device assertEqual d s device assertEqual d s device assertEqual s s torch cuda device d s = torch cuda current_stream s = torch cuda current_stream s = torch cuda current_stream d assertEqual d s device assertEqual d s device assertEqual d s device assertEqual s s assertRaisesRegex ValueError Expected cuda device got cpu torch cuda current_stream torch device cpu unittest skipIf TEST_MULTIGPU detected only one GPU skipCUDANonDefaultStreamIf True test_default_stream d = torch device cuda d = torch device cuda torch cuda device d s = torch cuda default_stream torch cuda device d s = torch cuda default_stream s = torch cuda default_stream device= s = torch cuda default_stream d assertEqual d s device assertEqual d s device assertEqual d s device assertEqual d s device assertEqual s s assertEqual s s torch cuda device d assertEqual torch cuda current_stream s torch cuda device d assertEqual torch cuda current_stream s assertRaisesRegex ValueError Expected cuda device got cpu torch cuda default_stream torch device cpu unittest skipIf TEST_MULTIGPU detected only one GPU test_stream_event_device d = torch device cuda d = torch device cuda e = torch cuda Event assertEqual None e device torch cuda device d s = torch cuda current_stream s record_event e torch cuda device d s = torch cuda Stream e = s record_event assertEqual s device torch device cuda assertEqual e device torch device cuda assertEqual s device torch device cuda assertEqual e device torch device cuda unittest skipIf TEST_MULTIGPU detected only one GPU test_stream_context s = torch cuda current_stream s = torch cuda Stream device= s = torch cuda Stream device= torch cuda device s device prev_stream_on_cuda = torch cuda current_stream assertEqual torch cuda current_stream s assertEqual torch cuda current_device torch cuda stream s assertEqual torch cuda current_stream s assertEqual torch cuda current_device torch cuda stream s assertEqual torch cuda current_stream s assertEqual torch cuda current_device torch cuda stream s assertEqual torch cuda current_stream s assertEqual torch cuda current_device assertEqual torch cuda current_stream s assertEqual torch cuda current_device assertEqual torch cuda current_stream s assertEqual torch cuda current_device torch cuda device s device assertEqual prev_stream_on_cuda torch cuda current_stream assertEqual torch cuda current_stream s assertEqual torch cuda current_device unittest skipIf TEST_MULTIGPU detected only one GPU test_streams_multi_gpu default_stream = torch cuda current_stream assertEqual default_stream device torch device cuda stream = torch cuda Stream device= assertEqual stream device torch device cuda torch cuda device assertEqual torch cuda current_stream device torch device cuda assertNotEqual torch cuda current_stream default_stream unittest skipIf TEST_MULTIGPU detected only one GPU test_streams_multi_gpu_query d = torch device cuda d = torch device cuda torch cuda synchronize d torch cuda synchronize d torch cuda device d s = torch cuda current_stream torch cuda device d s = torch cuda current_stream torch cuda _sleep TestCudaMultiGPU FIFTY_MIL_CYCLES assertTrue s query assertFalse s query torch cuda device d assertTrue s query assertFalse s query torch cuda device d assertTrue s query assertFalse s query deliberately using different device torch cuda device d s synchronize assertTrue s query assertTrue s query torch cuda device d assertTrue s query assertTrue s query torch cuda device d assertTrue s query assertTrue s query unittest skipIf TEST_MULTIGPU detected only one GPU test_streams_multi_gpu_eq d = torch device cuda d = torch device cuda torch cuda device d s = torch cuda current_stream s = torch cuda current_stream torch cuda device d s = torch cuda current_stream s = torch cuda current_stream assertTrue s == s assertTrue s == s assertTrue s == s assertTrue s == s assertFalse s == s assertFalse s == s assertEqual s device s device assertEqual s cuda_stream s cuda_stream assertEqual s device s device assertEqual s cuda_stream s cuda_stream assertNotEqual s device s device assertEqual hash s hash s assertEqual hash s hash s assertNotEqual hash s hash s unittest skipIf TEST_MULTIGPU multi-GPU supported test_streams_priority low high = torch cuda Stream priority_range s = torch cuda Stream device= priority=low assertEqual low s priority assertEqual torch device cuda s device s = torch cuda Stream device= priority=high assertEqual high s priority assertEqual torch device cuda s device unittest skipIf TEST_MULTIGPU multi-GPU supported test_tensor_device assertEqual torch cuda FloatTensor get_device assertEqual torch cuda FloatTensor device= get_device torch cuda device assertEqual torch cuda FloatTensor get_device assertEqual torch cuda FloatTensor device= get_device assertEqual torch cuda FloatTensor device=None get_device staticmethod _stream_synchronize spin_time_cycles s = torch cuda current_stream e_tik = torch cuda Event enable_timing=True e_tok = torch cuda Event enable_timing=True e_tik record s torch cuda _sleep spin_time_cycles e_tok record s s synchronize assertTrue s query necessary check e_tik e_tok elapsed_time would throw exception otherwise e_tik elapsed_time e_tok staticmethod _event_synchronize spin_time_cycles s = torch cuda current_stream e_tik = torch cuda Event enable_timing=True e_tok = torch cuda Event enable_timing=True e_tik record s torch cuda _sleep spin_time_cycles s record_event e_tok e_tok synchronize assertTrue s query necessary check e_tik e_tok elapsed_time would throw exception otherwise e_tik elapsed_time e_tok staticmethod _event_wait spin_time_cycles s = torch cuda current_stream s = torch cuda Stream e_tik = torch cuda Event blocking=True enable_timing=True e_tok = torch cuda Event blocking=True enable_timing=True e_tik record s torch cuda _sleep spin_time_cycles - e_sync = torch cuda Event blocking=True e_sync record e_sync wait s torch cuda stream s torch cuda _sleep s synchronize e_tok record e_tok synchronize assertTrue s query assertTrue s query assertTrue e_sync query necessary check e_tik e_tok elapsed_time would throw exception otherwise e_tik elapsed_time e_tok staticmethod _test_stream_event_nogil sync_func p c c p torch cuda device cuda c p put p c get c p put sync_func TestCudaMultiGPU FIFTY_MIL_CYCLES unittest skipIf TEST_MULTIGPU detected only one GPU test_stream_event_nogil sync_func TestCudaMultiGPU _stream_synchronize TestCudaMultiGPU _event_synchronize TestCudaMultiGPU _event_wait p c = queue Queue c p = queue Queue e_tik = torch cuda Event enable_timing=True e_tok = torch cuda Event enable_timing=True t = threading Thread target=TestCudaMultiGPU _test_stream_event_nogil args= sync_func p c c p t daemon = True t start c p get torch cuda device cuda e_tik record p c put parent_time = sync_func TestCudaMultiGPU FIFTY_MIL_CYCLES child_time = c p get e_tok record e_tok synchronize total_time = e_tik elapsed_time e_tok Without GIL synchronizations parent child threads can overlap The total execution time should little bit longer than spinning fifty million cycles much shorter than twice However testing absolute execution time reliable may vary different hardware different environments Therefore test uses relative comparisons checking sum parent child threads execution time greater than real execution time least assertGreater parent_time + child_time total_time This test flaky ROCm see issue unittest skipIf TEST_MULTIGPU detected only one GPU test_events_wait d = torch device cuda d = torch device cuda torch cuda synchronize d torch cuda synchronize d torch cuda device d s = torch cuda current_stream torch cuda _sleep TestCudaMultiGPU FIFTY_MIL_CYCLES e = torch cuda Event s record_event e torch cuda device d s = torch cuda current_stream assertFalse s query assertTrue s query s wait_event e s synchronize assertTrue e query assertTrue s query assertTrue s query unittest skipIf TEST_MULTIGPU detected only one GPU test_events_multi_gpu_query d = torch device cuda d = torch device cuda torch cuda device d s = torch cuda current_stream e = s record_event s synchronize torch cuda device d s = torch cuda current_stream torch cuda _sleep TestCudaMultiGPU FIFTY_MIL_CYCLES e = s record_event assertTrue e query assertFalse e query torch cuda device d assertTrue e query assertFalse e query torch cuda device d assertTrue e query assertFalse e query deliberately using different device torch cuda device d e synchronize assertTrue e query assertTrue e query torch cuda device d assertTrue e query assertTrue e query torch cuda device d assertTrue e query assertTrue e query unittest skipIf TEST_MULTIGPU detected only one GPU test_events_multi_gpu_elapsed_time d = torch device cuda d = torch device cuda torch cuda device d s = torch cuda current_stream e = torch cuda Event enable_timing=True torch cuda _sleep s record_event e torch cuda device d s = torch cuda current_stream e = torch cuda Event enable_timing=True torch cuda _sleep TestCudaMultiGPU FIFTY_MIL_CYCLES s record_event e e synchronize e synchronize torch cuda device d assertRaises RuntimeError assertGreater e elapsed_time e torch cuda device d assertRaises RuntimeError assertGreater e elapsed_time e torch cuda device d s = torch cuda current_stream e = torch cuda Event enable_timing=True torch cuda _sleep TestCudaMultiGPU FIFTY_MIL_CYCLES s record_event e s synchronize assertGreater e elapsed_time e deliberately calling different device torch cuda device d assertGreater e elapsed_time e contextlib contextmanager _get_external_stream device cudart = torch cuda cudart stream = ctypes c_ulonglong stream_p = ctypes POINTER ctypes c_void_p stream stream_p_int = ctypes cast stream_p ctypes c_void_p value device try out = cudart cudaStreamCreate stream_p_int assertEqual out assertNotEqual stream value yield stream value finally out = cudart cudaStreamDestroy stream value assertEqual out test_external_streams device = torch cuda device _get_external_stream device stream_v ext_stream = torch cuda ExternalStream stream_v assertEqual stream_v ext_stream cuda_stream assertEqual ext_stream device index device idx ext_stream = torch cuda get_stream_from_external stream_v device assertEqual stream_v ext_stream cuda_stream assertEqual ext_stream device index device idx unittest skipIf TEST_MULTIGPU detected only one GPU test_external_streams_multi_device device = torch cuda device _get_external_stream device stream_v ext_stream = torch cuda ExternalStream stream_v device=device assertEqual stream_v ext_stream cuda_stream assertEqual ext_stream device index device idx ext_stream = torch cuda get_stream_from_external stream_v device assertEqual stream_v ext_stream cuda_stream assertEqual ext_stream device index device idx unittest skipIf TEST_MULTIGPU only one GPU detected test_caching_pinned_memory_multi_gpu checks events preventing pinned memory being reused too early recorded correct GPU cycles_per_ms = get_cycles_per_ms t = torch FloatTensor pin_memory ptr = t data_ptr gpu_tensor = torch cuda FloatTensor device= gpu_tensor = torch cuda FloatTensor device= torch cuda device torch cuda _sleep int cycles_per_ms delay copy s gpu_tensor copy_ t non_blocking=True del t t = torch FloatTensor pin_memory assertNotEqual t data_ptr ptr msg= allocation reused too soon torch cuda device gpu_tensor copy_ t non_blocking=True assertEqual gpu_tensor assertEqual gpu_tensor unittest skipIf TEST_MULTIGPU only one GPU detected test_get_set_rng_state_all states = torch cuda get_rng_state_all before = torch cuda FloatTensor device= normal_ before = torch cuda FloatTensor device= normal_ torch cuda set_rng_state_all states after = torch cuda FloatTensor device= normal_ after = torch cuda FloatTensor device= normal_ assertEqual before after atol= rtol= assertEqual before after atol= rtol= unittest skipIf TEST_MULTIGPU only one GPU detected test_rng_state_offset before = torch cuda get_rng_state torch cuda _set_rng_state_offset offset = torch cuda _get_rng_state_offset torch cuda set_rng_state before assertEqual offset Verifies mem_get_info works including when called different device test_mem_get_info _test device Union str int torch device Prevent PyTorch reusing allocated memory torch cuda empty_cache torch cuda synchronize before_free_bytes before_available_bytes = torch cuda mem_get_info device increasing MB force acquiring new block overcome blocksize differences across platforms t = torch randn device=device noqa F IS_JETSON w o syncing mem_get_info will run before memory allocated has actually increased This race condition causes consistent failure torch cuda synchronize after_free_bytes after_available_bytes = torch cuda mem_get_info device assertLess after_free_bytes before_free_bytes assertEqual before_available_bytes after_available_bytes Test calls different device representations _test _test torch device cuda _test torch device cuda _test cuda _test cuda TEST_MULTIGPU _test _test torch device cuda _test cuda Test wrap_with_cuda_memory_check successfully detects leak test_cuda_memory_leak_detection l = wrap_with_cuda_memory_check no_leak pass wrap_with_cuda_memory_check leak_gpu increasing MB force acquiring new block overcome blocksize differences across platforms l append torch randn device=torch device cuda no_leak regex = r CUDA driver API confirmed + device + IS_JETSON try leak_gpu except RuntimeError e re assert re match regex str e str e + \n does match \n + regex assertRaisesRegex does pass Python Jetson even though RuntimeError matches regex using re match assertRaisesRegex RuntimeError regex leak_gpu TEST_MULTIGPU wrap_with_cuda_memory_check leak_gpu increasing MB force acquiring new block overcome blocksize differences across platforms l append torch randn device=torch device cuda assertRaisesRegex RuntimeError r CUDA driver API confirmed + device + leak_gpu unittest skipIf TEST_MULTIGPU only one GPU detected test_streaming_backwards_device_transfer This function must run non-default current streams all devices otherwise s meaningless The intention test s backward CopyBackward interacts properly synchronization logic torch csrc autograd input_buffer cpp dev = torch device cuda dev = torch device cuda Unfortunately I need make tensors largeish Bigger tensors = longer D D transfers = more likely expose races size = = torch full size device=dev dtype=torch float requires_grad=True b = torch full size device=dev dtype=torch float requires_grad=True Here to_backward_recipient = b used only once so MulBackward s InputBuffer slot only expects input This tests situation where we don t call InputBuffer accumulate MulBackward s InputBuffer to_backward_recipient = b s = to_backward_recipient device= cuda sum torch cuda synchronize device=dev torch cuda synchronize device=dev s backward assertTrue grad sum item == size assertTrue b grad sum item == size Here to_backward_recipient = b used twice so MulBackward s InputBuffer slot expects inputs This tests situation where we do call InputBuffer accumulate MulBackward s InputBuffer grad = None b grad = None to_backward_recipient = b Multiply here so s backward creates gradient values different case above mitigate weirdness caching allocator happens reuse memory regions populated s case above s = to_backward_recipient device= cuda sum s = to_backward_recipient device= cuda sum torch cuda synchronize device=dev torch cuda synchronize device=dev s backward retain_graph=True s backward assertTrue grad sum item == size assertTrue b grad sum item == size unittest skipIf TEST_MULTIGPU only one GPU detected unittest skipIf IS_SANDCASTLE IS_REMOTE_GPU Does work Sandcastle test_cuda_init_race See https github com pytorch pytorch issues subprocess subprocess check_call sys executable -c \ torch threading worker rank torch tensor cuda rank t = threading Thread target=worker args= t = threading Thread target=worker args= t start t start unittest skipIf TEST_MULTIGPU only one GPU detected test_grad_scaling_device_as_key Ensure different instances device objects point same device treated identical keys dicts GradScaler relies behavior may error otherwise way s difficult detect silent performance hit d = t = torch empty device= cuda dev = torch device cuda dev b = torch device cuda dev = torch device cuda dev b = torch device cuda assertTrue hash dev == hash dev b assertTrue hash dev == hash dev b d dev = d dev b = b assertTrue len d == assertTrue d dev == b d t device = t assertTrue len d == assertTrue d dev == t d dev = d dev b = b assertTrue len d == assertTrue d dev == b unittest skipIf TEST_MULTIGPU only one GPU detected test_grad_scaling_scale scaler = torch amp GradScaler device= cuda init_scale= t = torch full dtype=torch float device= cuda t = torch full dtype=torch float device= cuda Create some nested iterables tensors different devices outputs = t clone t clone t clone t clone t clone t clone outputs = scaler scale outputs assertTrue outputs == outputs == outputs == outputs == outputs == outputs == assertTrue scaler _scale device == t device unittest skipIf TEST_MULTIGPU only one GPU detected test_grad_scaling_multigpu Same above runs some models device GradScaler should transparently handle losses gradients multiple devices This test could combined test above I think makes sense treat multi-GPU operations separately dev = torch device cuda dev = torch device cuda enabled True False mod_control mod_scaling opt_control opt_scaling data loss_fn skip_iter = _create_scaling_case mod_control mod_scaling opt_control opt_scaling = _create_scaling_models_optimizers device=dev scaler = torch amp GradScaler device= cuda init_scale= growth_factor= enabled=enabled growth_interval= run model model optimizer optimizer try_scaling_api i input target enumerate data optimizer zero_grad optimizer zero_grad output = model input output = model input dev loss = loss_fn output + output dev target loss = loss_fn output dev - output target dev try_scaling_api scaler scale loss backward retain_graph=True scaler scale loss backward i == skip_iter scaler is_enabled model weight grad data fill_ float inf As additional stress test separately unscale one optimizers scaler unscale_ optimizer scaler step optimizer scaler step optimizer Make sure found_infs collected properly across optimizers devices scaler is_enabled assertTrue len scaler _found_inf_per_device optimizer == assertTrue len scaler _found_inf_per_device optimizer == assertTrue scaler _found_inf_per_device optimizer dev item == assertTrue scaler _found_inf_per_device optimizer dev item == float i == skip_iter scaler update loss backward retain_graph=True loss backward optimizer step scaler is_enabled i = skip_iter optimizer step run mod_control mod_control opt_control opt_control False run mod_scaling mod_scaling opt_scaling opt_scaling True The loss scale should have been multiplied growth factor times backoff factor once assertTrue scaler get_scale == scaler get_growth_factor scaler get_backoff_factor enabled Copy mod_control mod_scaling back device comparison mod_control dev mod_scaling dev c s zip chain mod_control parameters mod_control parameters chain mod_scaling parameters mod_scaling parameters assertEqual c s rtol= e- atol= e- unittest skipIf TEST_MULTIGPU Test needs multiple GPUs test_cuda_device_memory_allocated torch cuda memory_allocated device_count = torch cuda device_count current_alloc = memory_allocated idx idx range device_count _x = torch ones device= cuda assertGreater memory_allocated current_alloc assertTrue all memory_allocated torch cuda device idx == current_alloc idx idx range device_count TestCudaComm TestCase _test_broadcast input TEST_MULTIGPU raise unittest SkipTest only one GPU detected test regular results = comm broadcast input i t enumerate results assertEqual t get_device i assertEqual t input input is_cuda input get_device == i test copying same device assertEqual t data_ptr input data_ptr test out= inplace True False inplace outputs = torch empty_like input device= torch empty_like input device= outputs = input cuda torch empty_like input device= results = comm broadcast input out=outputs r o zip results outputs assertIs r o i t enumerate results assertEqual t get_device i assertEqual t input test error msg assertRaisesRegex RuntimeError r Exactly one devices out comm broadcast input out=outputs assertRaisesRegex RuntimeError r Expected all output tensors CUDA tensors output tensor index comm broadcast input out= input cuda input cpu assertRaisesRegex RuntimeError r Expected all output tensors have same shape source + index comm broadcast input out= input cuda input cuda unsqueeze test_broadcast_cpu _test_broadcast torch randn test_broadcast_gpu _test_broadcast torch randn cuda _test_broadcast_coalesced tensors buffer_size b_tensors = comm broadcast t t tensors _ bt t zip b_tensors tensors assertEqual bt get_device assertEqual bt t assertIsInstance bt type t bc_tensors = comm broadcast_coalesced tensors buffer_size=buffer_size bc_tensors_t = list zip bc_tensors assertEqual b_tensors bc_tensors_t _ bt _ bct zip b_tensors bc_tensors_t assertEqual bt get_device bct get_device assertIsInstance bct type bt check tensors device returned as-is out_tensors b_tensors bc_tensors_t inp_t out_t _ zip tensors out_tensors assertIs inp_t out_t check tensors device have different version counters NOTE Version Counter comm _coalesced versions = t _version _ t bc_tensors_t old_version _ t zip versions bc_tensors_t assertEqual t _version old_version t zero_ assertEqual t _version old_version + unittest skipIf TEST_MULTIGPU only one GPU detected Note fails sometimes CI passes dual gfx test_broadcast_coalesced numel = num_bytes = numel tensors = genSparseTensor False cuda torch float torch randn numel long cuda torch randn numel cuda genSparseTensor False cuda torch float genSparseTensor False cuda torch float genSparseTensor False cuda torch int genSparseTensor False cuda torch float torch randn numel long cuda torch randn numel long cuda genSparseTensor False cuda torch int torch randn numel int cuda int x shorter torch randn numel cuda _test_broadcast_coalesced tensors num_bytes unittest skipIf TEST_MULTIGPU only one GPU detected test_broadcast_coalesced_dense_only numel = num_bytes = numel tensors = torch randn numel long cuda torch randn numel cuda torch randn numel long cuda torch randn numel long cuda torch randn numel int cuda int x shorter torch randn numel cuda _test_broadcast_coalesced tensors num_bytes unittest skipIf TEST_MULTIGPU only one GPU detected test_broadcast_coalesced_empty_tensors tensors = torch tensor byte cuda torch randn cuda torch randn double cuda _test_broadcast_coalesced tensors unittest skipIf TEST_MULTIGPU only one GPU detected test_reduce_add x = torch randn y = torch randn x_cuda = x cuda y_cuda = y cuda result = comm reduce_add x_cuda y_cuda assertEqual result get_device assertEqual result cpu x + y _test_reduce_add_coalesced tensors buffer_size dup_tensors = tensors t cuda t tensors r_tensors = comm reduce_add t t zip dup_tensors r t zip r_tensors tensors assertEqualTypeString r t assertEqual r coalesce r is_sparse r t rc_tensors = comm reduce_add_coalesced dup_tensors buffer_size=buffer_size assertEqual r_tensors rc_tensors r rc zip r_tensors rc_tensors assertEqualTypeString rc r Since we have both cuda cuda inputs outputs must new We can check they have different version counters NOTE Version Counter comm _coalesced versions = t _version t rc_tensors old_version t zip versions rc_tensors assertEqual t _version old_version t zero_ assertEqual t _version old_version + unittest skipIf TEST_MULTIGPU only one GPU detected test_reduce_add_coalesced numel = num_bytes = numel tensors = genSparseTensor False cuda torch float torch randn numel long cuda torch randn numel cuda genSparseTensor False cuda torch float genSparseTensor False cuda torch float genSparseTensor False cuda torch int genSparseTensor False cuda torch float torch randn numel long cuda torch randn numel long cuda genSparseTensor False cuda torch int torch randn numel int cuda int x shorter torch randn numel cuda _test_reduce_add_coalesced tensors num_bytes unittest skipIf TEST_MULTIGPU only one GPU detected test_reduce_add_coalesced_dense_only numel = num_bytes = numel tensors = torch randn numel long cuda torch randn numel cuda torch randn numel long cuda torch randn numel long cuda torch randn numel int cuda int x shorter torch randn numel cuda _test_reduce_add_coalesced tensors num_bytes _test_scatter input chunk_sizes=None dim= TEST_MULTIGPU raise unittest SkipTest only one GPU detected chunk_sizes None ref_chunk_sizes = tuple repeat input size dim ref_chunk_sizes = chunk_sizes test regular result = comm scatter input chunk_sizes dim assertEqual len result chunk_start = i r enumerate result chunk_end = chunk_start + ref_chunk_sizes i index = slice None None _ range input dim index dim = slice chunk_start chunk_end assertEqual r input tuple index atol= rtol= chunk_start = chunk_end r device == input device assertEqual r data_ptr input data_ptr target same device view should returned test out out = torch empty_like t t result result = comm scatter input dim=dim out=out assertEqual len result chunk_start = i r enumerate result assertIs r out i chunk_end = chunk_start + ref_chunk_sizes i index = slice None None _ range input dim index dim = slice chunk_start chunk_end assertEqual r input tuple index atol= rtol= chunk_start = chunk_end test error msg chunk_sizes None assertRaisesRegex RuntimeError r Expected devices chunk_sizes same length comm scatter input _ range len chunk_sizes + dim=dim chunk_sizes=chunk_sizes assertRaisesRegex RuntimeError r devices must specified comm scatter input dim=dim out=out assertRaisesRegex RuntimeError r Expected least one device scatter comm scatter input dim=dim assertRaisesRegex RuntimeError r Expected least one output tensor scatter comm scatter input dim=dim out= assertRaisesRegex RuntimeError r Expected all output tensors CUDA tensors output tensor index comm scatter input dim=dim out= out cpu + out assertRaisesRegex RuntimeError r Output tensor index has incorrect shape comm scatter input dim=dim out= out unsqueeze + out assertRaisesRegex RuntimeError r Total size output tensors along scatter dim \d+ does match index = slice None None _ range input dim index dim = slice None comm scatter input dim=dim out= out tuple index + out test_scatter_cpu _test_scatter torch randn dim= test_scatter_cpu_dim _test_scatter torch randn dim= test_scatter_cpu_neg_dim _test_scatter torch randn dim=- test_scatter_cpu_sizes _test_scatter torch randn chunk_sizes= test_scatter_gpu _test_scatter torch randn cuda dim= test_scatter_gpu_dim _test_scatter torch randn cuda dim= test_scatter_gpu_neg_dim _test_scatter torch randn cuda dim=- test_scatter_gpu_sizes _test_scatter torch randn cuda chunk_sizes= _test_gather dim TEST_MULTIGPU raise unittest SkipTest only one GPU detected x = torch randn device= y = torch randn device= expected_size = list x size expected_size dim += y size dim expected_size = torch Size expected_size destinations = None torch device cuda torch device cpu torch cuda device_count destinations append torch device cuda torch cuda device destination destinations destination None expected_device = torch device cuda torch cuda current_device expected_device = destination use_out True False use_out out = torch empty expected_size device=expected_device result = comm gather x y dim out=out assertIs out result result = comm gather x y dim destination=destination assertEqual result device expected_device assertEqual result size expected_size index = slice None None slice None None index dim = slice x size dim assertEqual result tuple index x index dim = slice x size dim x size dim + y size dim assertEqual result tuple index y test error msg assertRaisesRegex RuntimeError r destination must specified comm gather x y dim destination= cpu out=torch empty expected_size device= cpu assertRaisesRegex RuntimeError r Expected least one tensor gather comm gather assertRaisesRegex RuntimeError r Expected all input tensors CUDA tensors comm gather x cpu y assertRaisesRegex RuntimeError r Expected all input tensors have same number dimensions comm gather x y unsqueeze assertRaisesRegex RuntimeError r Input tensor index has invalid shape dim - comm gather x y dim=dim dim - comm gather x y dim=dim test_gather _test_gather test_gather_dim _test_gather test_gather_neg_dim _test_gather - unittest skipIf TEST_MULTIGPU only one GPU detected test_memory_format_scatter_gather nhwc = torch randn device= cpu contiguous memory_format=torch channels_last results = torch cuda comm scatter nhwc None result results assertFalse result is_contiguous assertTrue result is_contiguous memory_format=torch channels_last gathered = torch cuda comm gather results assertTrue gathered is_contiguous memory_format=torch channels_last unittest skipIf TEST_MULTIGPU Test needs multiple GPUs test_scatter_namedtuple tests ability scatter namedtuples retrieve list where each element expected namedtuple type fields = b TestNamedTupleInput_ = collections namedtuple NamedTuple fields num_gpus = torch cuda device_count = torch rand num_gpus device= b = torch rand num_gpus device= a_tensors_for_gpu = i i + i i range num_gpus b_tensors_for_gpu = b i i + i i range num_gpus inp = TestNamedTupleInput_ b target_gpus = torch device i i range num_gpus scatter_out = scatter_gather scatter inp target_gpus i x enumerate scatter_out assertTrue isinstance x type inp assertEqual x _fields fields expected_a = a_tensors_for_gpu i expected_b = b_tensors_for_gpu i assertEqual expected_a x assertEqual expected_b x b TestNamedTupleInput_ NamedTuple torch tensor b torch tensor = torch rand num_gpus device= b = torch rand num_gpus device= a_tensors_for_gpu = i i + i i range num_gpus b_tensors_for_gpu = b i i + i i range num_gpus inp = TestNamedTupleInput_ b scatter_out = scatter_gather scatter inp target_gpus i x enumerate scatter_out assertTrue isinstance x type inp assertEqual x _fields fields expected_a = a_tensors_for_gpu i expected_b = b_tensors_for_gpu i assertEqual expected_a x assertEqual expected_b x b unittest skipIf TEST_MULTIGPU Test needs multiple GPUs test_gather_namedtuple tests ability gather list namedtuples namedtuple where each element expected tensor type fields = b TestNamedTupleInput_ = collections namedtuple NamedTuple fields num_gpus = torch cuda device_count = torch rand num_gpus device= b = torch rand num_gpus device= out = TestNamedTupleInput_ b = torch rand num_gpus device= b = torch rand num_gpus device= out = TestNamedTupleInput_ b outputs = out out out = scatter_gather gather outputs cpu test CPU i x enumerate out assertTrue isinstance x type out - x must tensor cat = torch cat outputs i cpu outputs i cpu assertTrue torch equal x cat out = scatter_gather gather outputs test GPU i x enumerate out assertTrue isinstance x type out - cat = torch cat outputs i outputs i assertTrue torch equal x cat TestNamedTupleInput_ NamedTuple torch tensor b torch tensor = torch rand num_gpus device= b = torch rand num_gpus device= out = TestNamedTupleInput_ b = torch rand num_gpus device= b = torch rand num_gpus device= out = TestNamedTupleInput_ b outputs = out out out = scatter_gather gather outputs test GPU i x enumerate out assertTrue isinstance x type out - cat = torch cat outputs i outputs i assertTrue torch equal x cat out = scatter_gather gather outputs cpu test CPU i x enumerate out assertTrue isinstance x type out - cat = torch cat outputs i cpu outputs i cpu assertTrue torch equal x cat instantiate_parametrized_tests TestCudaMultiGPU __name__ == __main__ run_tests