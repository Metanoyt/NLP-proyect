mypy allow-untyped-defs re collections abc Callable typing Optional Union torch fx torch fx node map_arg torch fx passes split_module split_module __all__ = FoldedGraphModule get_unique_attr_name_in_module split_const_subgraphs FoldedGraphModule torch fx GraphModule FoldedGraphModule GraphModule which also contains another ` const_subgraph_module ` representing subgraph which has all const attr inputs which can run once before running main standard ` graph ` The ` const_output_names ` ordered list names attrs which represent what each respective output const_subgraph should set which attrs __init__ root torch nn Module graph torch fx Graph const_subgraph Optional torch fx Graph = None fx_const_folded_attrs_name Optional str = None device_for_folded_attrs str = cuda super __init__ root graph const_subgraph_module = None const_subgraph None torch fx GraphModule root const_subgraph has_folding_been_run = False fx_const_folded_attrs_name = fx_const_folded_attrs_name device_for_folded_attrs = device_for_folded_attrs __call__ args kwargs has_folding_been_run run_folding super __call__ args run_folding If there s no const subgraph module attr output names use early there no const folding perform const_subgraph_module None fx_const_folded_attrs_name None assert has_folding_been_run has_folding_been_run = True Actually run const folding subgraph Note single attr const fold subgraphs output single Tensor while multiple outputs returned Tuple Tensor folded_attrs = const_subgraph_module _create_param i torch nn Parameter i detach clone isinstance i int torch Tensor i device=self device_for_folded_attrs requires_grad=i requires_grad isinstance i torch Tensor False params = torch nn ParameterList _create_param i i folded_attrs isinstance folded_attrs tuple _create_param folded_attrs setattr fx_const_folded_attrs_name params _inline_module gm torch fx GraphModule inline_mod_name str Given ` gm ` some graph module which called target name ` inline_mod_name ` helper will inline all nodes called graph module into ` gm ` Fetch inner graph module we want inline inside ` gm ` inline_mod = dict gm named_modules inline_mod_name assert isinstance inline_mod torch fx GraphModule call_mod_node_to_replace = None node gm graph nodes node op == call_module node target == inline_mod_name call_mod_node_to_replace = node break assert call_mod_node_to_replace None Now actually do swap Note we have keep track new nodes copied into ` gm ` -- we do via replacement_mapping call_mod_args = call_mod_node_to_replace args call_mod_kwargs = call_mod_node_to_replace kwargs replacement_mapping dict torch fx Node torch fx Node = ph_count = replacement_fn node new_node = replacement_mapping node new_node meta = node meta copy new_node inline_node inline_mod graph nodes inline_node op == placeholder replacement_mapping inline_node = call_mod_kwargs inline_node name inline_node name call_mod_kwargs call_mod_args ph_count ph_count += continue inline_node op == output outputs = inline_node args output_replacements = map_arg outputs replacement_fn call_mod_node_to_replace replace_all_uses_with output_replacements continue gm graph inserting_before call_mod_node_to_replace new_node = gm graph node_copy inline_node replacement_fn replacement_mapping inline_node = new_node gm graph eliminate_dead_code get_unique_attr_name_in_module mod_traced torch fx GraphModule name str - str Make sure name unique module can represents attr Delete all characters illegal Python identifier name = re sub ^ - a-zA-Z_ + _ name name isdigit name = f _ name Now make sure fact unique module incrementing suffix value while hasattr mod_traced name match = re match r _ \d+ $ name match None name = name + _ base num = match group name = f base _ int num + name split_const_subgraphs module Union torch nn Module torch fx GraphModule skip_folding_node_fn Optional Callable torch fx Node bool = None device_for_folded_attrs str = cpu - FoldedGraphModule Looks through ` module ` any nodes have all constant attribute inputs separates them out into their own constant subgraph returns FoldedGraphModule which runs constant subgraph first run set attributes module prior running non-constant portion graph sympy isinstance module torch fx GraphModule mod_traced = torch fx symbolic_trace module mod_traced = module Build up list const_nodes defined nodes themselves get_attrs have all get_attr other constant node inputs const_nodes set torch fx Node = set found_const_folding = False node mod_traced graph nodes Skip over placeholders outputs because they can t const folded we don t want add tags them node op placeholder output continue If node itself constant all its inputs constant then tag constant node op = get_attr set node all_input_nodes issubset const_nodes continue If provided skip folding function says skip then skip skip_folding_node_fn skip_folding_node_fn node continue Skip folding side-effectful functions node is_impure continue Skip folding nodes have symbolic fill_value isinstance node kwargs get fill_value None sympy Expr continue Must constant foldable node point const_nodes add node node op = get_attr found_const_folding = True If we did find any const folding then early without const fold subgraph found_const_folding FoldedGraphModule mod_traced mod_traced graph Partition module into two submod_ constant folding subgraph submod_ rest mod_partition node torch fx Node node const_nodes split = split_module mod_traced module mod_partition const_mod_name non_const_mod_name = submod_ submod_ Safely get submod_ case there no non-const nodes const_gm non_const_gm = split submod_ getattr split non_const_mod_name None The module call_module node refers gets copied submodules during split The path module also gets inlined i e mod b - mod_a_b Here we need attach inlined modules ` split ` s owning module now node non_const_gm graph nodes non_const_gm node op == call_module setattr split node target getattr non_const_gm node target node const_gm graph nodes node op == call_module setattr split node target getattr const_gm node target split_module currently does use get_attrs attrs Instead passes them args parent module which used get_attrs Here we set them get_attrs inside const_gm allowing running folding without somehow priori knowing attrs should passed args We can unconditionally do all placeholders because we know all placeholders const_gm must constants accessible via get_attr call_const_gm_args = None node split graph nodes node op == call_module node target == const_mod_name call_const_gm_args = node args break assert call_const_gm_args None Here we do actual replacement placeholders get_attrs Note here we set const_gm graph into new root_const_gm split root module because we fetching attributes directly root module instead fetching them const_gm Example The const_gm must have some format like graph inp num_users= = placeholder target=const_inp add num_users= = call_function target=operator add args = inp inp kwargs = add We replace following which does have any placeholders graph inp_ num_users= = get_attr target=const_inp add num_users= = call_function target=operator add args = inp_ inp_ kwargs = add root_const_gm = torch fx GraphModule split const_gm graph The order placeholders const_gm graph should match order args outer module so we can simply use index placeholder mapping ph_idx = node root_const_gm graph nodes node op == output multiple_outputs = isinstance node args tuple continue node op = placeholder continue assert ph_idx len call_const_gm_args in_node = call_const_gm_args ph_idx ph_idx += assert in_node op == get_attr root_const_gm graph inserting_before node new_node = root_const_gm graph get_attr in_node target new_node meta = node meta copy node replace_all_uses_with new_node root_const_gm graph erase_node node assert multiple_outputs locals Now find call const_gm inside split replace getattr folded tensor s result constant folding Note we don t need worry about whether one more tensors because original graph correctly uses getitem extract individual tensors there multiple folded fx_const_folded_attrs_name = get_unique_attr_name_in_module mod_traced _FX_CONST_FOLDED_ATTRS setattr split fx_const_folded_attrs_name torch nn ParameterList multiple_outputs torch nn Parameter type ignore possibly-undefined node split graph nodes node op == call_module node target == const_mod_name node graph inserting_before node folded_attrs = node graph get_attr fx_const_folded_attrs_name folded_attrs meta = node meta copy node replace_all_uses_with folded_attrs break Finally inline non-constant submod exists into split submod This so original caller who may have passed graph module will get back out graph module whose graph traced same granularity hasattr split non_const_mod_name _inline_module split non_const_mod_name split graph eliminate_dead_code FoldedGraphModule split split graph root_const_gm graph fx_const_folded_attrs_name device_for_folded_attrs