mypy allow-untyped-defs itertools warnings enum auto Enum typing Optional Union torch torch distributed dist torch distributed fsdp _traversal_utils traversal_utils torch nn nn torch distributed fsdp _common_utils _FSDPState _get_param_to_fqns torch distributed fsdp _flat_param FlatParamHandle _ExecOrderWarnStatus Enum Used internally execution order validation NONE = auto no deviation yet WARNING = auto deviated iteration currently issuing warnings WARNED = auto deviated previous iteration _ExecOrderData This contains data structures track execution order We track pre-forward order first iteration forward prefetching which thus assumes static graph post-forward order every iteration backward prefetching which thus does assume static graph may provide incorrect order __init__ debug_level dist DebugLevel backward_prefetch_limit int forward_prefetch_limit int - None Tracks static pre-forward order execution order validation forward prefetching handles_pre_forward_order list FlatParamHandle = Tracks post-forward order pre-backward prefetching handles_post_forward_order list Optional FlatParamHandle = _iter = Gives max number backward forward prefetched all-gathers single module _backward_prefetch_limit = backward_prefetch_limit _forward_prefetch_limit = forward_prefetch_limit Data structures execution order validation _checking_order bool = debug_level == dist DebugLevel DETAIL process_group Optional dist ProcessGroup = None world_size Optional int = None all_handles list FlatParamHandle = Names prefixed root module param_to_fqn dict nn Parameter list str = Current index pre-forward execution order current_order_index = warn_status = _ExecOrderWarnStatus NONE init state _FSDPState root_module nn Module process_group dist ProcessGroup - None Initializes data structures needed checking forward order This should called after root FSDP instance has been set during lazy initialization process_group = process_group rank = process_group rank world_size = process_group size Fix order over handles which should same across ranks handle traversal_utils _get_fsdp_handles root_module index = len all_handles all_handles append handle handle _handle_index = index param_to_fqn = _get_param_to_fqns root_module TODO awgu We can broadcast metadata rank s ` all_handles ` check all ranks have same handles same order https github com pytorch pytorch issues property is_first_iter - bool _iter == get_handle_to_backward_prefetch current_handle FlatParamHandle - Optional FlatParamHandle Returns ` list ` handles keys handles backward prefetch given current handles key If there no valid handles keys prefetch then returns empty ` list ` current_index = current_handle _post_forward_index current_index None None target_index = current_index - target_handle Optional FlatParamHandle = None _ range _backward_prefetch_limit target_index break target_handle = handles_post_forward_order target_index target_index -= target_handle get_handle_to_forward_prefetch current_handle FlatParamHandle - Optional FlatParamHandle Returns ` list ` handles keys handles forward prefetch given current handles key If there no valid handles keys prefetch then returns empty ` list ` current_index = current_handle _pre_forward_order_index current_index None None target_index = current_index + target_handle Optional FlatParamHandle = None _ range _forward_prefetch_limit target_index = len handles_pre_forward_order break target_handle = handles_pre_forward_order target_index target_index += target_handle record_post_forward handle Optional FlatParamHandle - None Records ` ` handles ` ` post-forward order where ` ` handles ` ` should group handles used same module s forward If ` ` handles ` ` empty then omitted Unlike meth ` record_pre_forward ` records order every iteration expectation recorded order reset meth ` next_iter ` handle Only record first usage handles key handle _post_forward_index handles_post_forward_order append handle index = len handles_post_forward_order handle _post_forward_index = index handles_post_forward_order append handle record_pre_forward handle Optional FlatParamHandle is_training bool - None Records ` ` handles ` ` pre-forward order where ` ` handles ` ` should group handles used same module s forward If ` ` handles ` ` empty then omitted On first iteration checks execution order across ranks See meth ` _check_order ` details handle _check_order handle is_training Fix order after first iteration only record first usage handles key is_first_iter handle _pre_forward_order_index None index = len handles_pre_forward_order handle _pre_forward_order_index = index handles_pre_forward_order append handle _check_order handle FlatParamHandle is_training bool - None Checks forward execution order long ` ` is_training ` ` ` ` True ` ` since checking eval mode supported This only checks distributed debug level DETAIL - On first iteration uses all-gathers check all ranks all-gathering same handles hence ` ` FlatParameter ` ` s raising error - On subsequent iterations checks each rank locally consistent its own forward order first iteration issuing warning This issues warning first deviating iteration stops warning thereafter Do check order eval mode since post-backward callback does run so cannot used mark end iteration is_training _checking_order is_first_iter msg_prefix = Forward order differs across ranks optional_local_indices tuple Optional int = _get_handle_indices handle device = handle device guaranteed non-CPU num_valid_indices = sum index None index optional_local_indices tensor_kwargs dict str Union torch dtype torch device = dtype torch int device device world_num_valid_indices = torch zeros world_size tensor_kwargs type ignore arg-type call-overload local_num_valid_indices = torch tensor num_valid_indices tensor_kwargs type ignore arg-type call-overload dist all_gather_into_tensor world_num_valid_indices local_num_valid_indices group=self process_group Copy entire tensor D H once avoid per element D H copies world_num_valid_indices = world_num_valid_indices cpu Check all ranks plan all-gather same number parameters TODO awgu Since every module has most one handle current implementation should never raise error world_size None raise AssertionError Expected world_size None torch distributed _functional_collectives is_torchdynamo_compiling TODO voz Don t graph break - dynamo hates n = n tensor comparison control flow https github com pytorch pytorch issues r n r n itertools combinations rank world_num_valid_indices rank rank range world_size n = n raise RuntimeError f msg_prefix rank r all-gathering n parameters f while rank r all-gathering n parameters world_indices = torch zeros type ignore call-overload world_size num_valid_indices tensor_kwargs local_indices = torch tensor optional_local_indices tensor_kwargs type ignore arg-type dist all_gather_into_tensor world_indices local_indices group=self process_group Copy entire tensor D H once avoid per element D H copies world_indices = world_indices cpu Check all ranks plan all-gather same index parameters torch distributed _functional_collectives is_torchdynamo_compiling TODO voz Don t graph break - dynamo hates i = i tensor comparison control flow https github com pytorch pytorch issues r i r i itertools combinations rank world_indices rank num_valid_indices rank + num_valid_indices rank range world_size i = i r _param_names = _get_names_from_handle_indices i r _param_names = _get_names_from_handle_indices i raise RuntimeError f msg_prefix rank r all-gathering parameters f r _param_names while rank r all-gathering f parameters r _param_names Only issue warnings first deviating iteration stop checking thereafter avoid flooding console warn_status == _ExecOrderWarnStatus WARNED msg_prefix = None non- ` None ` means we should warn current_order_index = len handles_pre_forward_order This iteration sees extra all-gather s compared first msg_prefix = Expected all-gather any more parameters forward trying all-gather parameters expected_handle = handles_pre_forward_order current_order_index expected_handle = handle expected_param_names = _get_names_from_handles expected_handle msg_prefix = f Expected all-gather expected_param_names trying all-gather parameters msg_prefix None param_names = _get_names_from_handles handle msg_suffix = f param_names param_names newly-added parameter since construction time warnings warn Forward order differs first iteration f rank rank Collectives unchecked may f give incorrect results hang \n msg_prefix msg_suffix stacklevel= warn_status = _ExecOrderWarnStatus WARNING current_order_index += _get_handle_indices handle FlatParamHandle - tuple Optional int Returns handle indices i e indices into ` ` all_handles ` ` corresponding handles ` ` handle ` ` An entry returned tuple ` ` None ` ` handle invalid indices list Optional int = handle indices append handle _handle_index tuple indices _get_names_from_handle_indices handle_indices tuple int - list list str Returns list FQNs each handle ` ` handle_indices ` ` If handle index invalid then its FQNs omitted returned list fqns list list str = index handle_indices index None index index = len all_handles continue handle = all_handles index flat_param = handle flat_param fqns append param_to_fqn flat_param fqns _get_names_from_handles handle FlatParamHandle - list list str Returns list FQNs each handle ` ` handles_key ` ` If handle invalid then its FQNs omitted returned list fqns list list str = handle flat_param = handle flat_param flat_param param_to_fqn fqns append param_to_fqn flat_param fqns next_iter Advances internal data structures per iteration This should called post-backward callback since marks true end iteration _iter += handles_post_forward_order clear _checking_order current_order_index = warn_status == _ExecOrderWarnStatus WARNING warn_status = _ExecOrderWarnStatus WARNED