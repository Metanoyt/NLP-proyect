argparse math pickle random dataclasses dataclass itertools chain pathlib Path typing Dict List common pandas pd torchtext torchtext functional to_tensor tqdm tqdm torch torch nn nn XLMR_BASE = torchtext models XLMR_BASE_ENCODER This should here works now device = cuda torch cuda is_available cpu HAS_IMBLEARN = False try imblearn HAS_IMBLEARN = True except ImportError HAS_IMBLEARN = False all files captured len good hyperparameter play around MAX_LEN_FILE = UNKNOWN_TOKEN = Unknown Utilities working truncated file graph truncate_file file Path max_len int = join file parts max_len build_file_set all_files List Path max_len int truncated_files = truncate_file file max_len file all_files set truncated_files dataclass CommitClassifierInputs title List str files List str author List str dataclass CategoryConfig categories List str input_dim int = inner_dim int = dropout float = activation = nn ReLU embedding_dim int = file_embedding_dim int = CommitClassifier nn Module __init__ encoder_base torchtext models XLMR_BASE_ENCODER author_map Dict str int file_map str int config CategoryConfig super __init__ encoder = encoder_base get_model requires_grad_ False transform = encoder_base transform author_map = author_map file_map = file_map categories = config categories num_authors = len author_map num_files = len file_map embedding_table = nn Embedding num_authors config embedding_dim file_embedding_bag = nn EmbeddingBag num_files config file_embedding_dim mode= sum dense_title = nn Linear config input_dim config inner_dim dense_files = nn Linear config file_embedding_dim config inner_dim dense_author = nn Linear config embedding_dim config inner_dim dropout = nn Dropout config dropout out_proj_title = nn Linear config inner_dim len categories out_proj_files = nn Linear config inner_dim len categories out_proj_author = nn Linear config inner_dim len categories activation_fn = config activation forward input_batch CommitClassifierInputs Encode input title title List str = input_batch title model_input = to_tensor transform title padding_value= device title_features = encoder model_input title_embed = title_features title_embed = dropout title_embed title_embed = dense_title title_embed title_embed = activation_fn title_embed title_embed = dropout title_embed title_embed = out_proj_title title_embed files list str = input_batch files batch_file_indexes = file files paths = truncate_file Path file_part MAX_LEN_FILE file_part file split batch_file_indexes append file_map get file file_map UNKNOWN_TOKEN file paths flat_indexes = torch tensor list chain from_iterable batch_file_indexes dtype=torch long device=device offsets = offsets extend len files files batch_file_indexes - offsets = torch tensor offsets dtype=torch long device=device offsets = offsets cumsum dim= files_embed = file_embedding_bag flat_indexes offsets files_embed = dense_files files_embed files_embed = activation_fn files_embed files_embed = dropout files_embed files_embed = out_proj_files files_embed Add author embedding authors List str = input_batch author author_ids = author_map get author author_map UNKNOWN_TOKEN author authors author_ids = torch tensor author_ids device author_embed = embedding_table author_ids author_embed = dense_author author_embed author_embed = activation_fn author_embed author_embed = dropout author_embed author_embed = out_proj_author author_embed title_embed + files_embed + author_embed convert_index_to_category_name most_likely_index isinstance most_likely_index int categories most_likely_index isinstance most_likely_index torch Tensor categories i i most_likely_index get_most_likely_category_name input Input will dict title author keys logits = forward input most_likely_index = torch argmax logits dim= convert_index_to_category_name most_likely_index get_train_val_data data_folder Path regen_data bool train_percentage= regen_data Path data_folder train_df csv exists Path data_folder val_df csv exists train_data = pd read_csv data_folder train_df csv val_data = pd read_csv data_folder val_df csv train_data val_data print Train Val Test Split found generating scratch commit_list_df = pd read_csv data_folder commitlist csv test_df = commit_list_df commit_list_df category == Uncategorized all_train_df = commit_list_df commit_list_df category = Uncategorized We going drop skip training set since so imbalanced print We removing skip categories YOU MIGHT WANT TO CHANGE THIS BUT THIS IS A MORE HELPFUL CLASSIFIER FOR LABELING all_train_df = all_train_df all_train_df category = skip all_train_df = all_train_df sample frac= reset_index drop=True split_index = math floor train_percentage len all_train_df train_df = all_train_df split_index val_df = all_train_df split_index print Train data size len train_df print Val data size len val_df test_df to_csv data_folder test_df csv index=False train_df to_csv data_folder train_df csv index=False val_df to_csv data_folder val_df csv index=False train_df val_df get_author_map data_folder Path regen_data assert_stored=False regen_data Path data_folder author_map pkl exists open data_folder author_map pkl rb f pickle load f assert_stored raise FileNotFoundError Author map found you loading inference you need have author map print Regenerating Author Map all_data = pd read_csv data_folder commitlist csv authors = all_data author unique tolist authors append UNKNOWN_TOKEN author_map = author i i author enumerate authors open data_folder author_map pkl wb f pickle dump author_map f author_map get_file_map data_folder Path regen_data assert_stored=False regen_data Path data_folder file_map pkl exists open data_folder file_map pkl rb f pickle load f assert_stored raise FileNotFoundError File map found you loading inference you need have file map print Regenerating File Map all_data = pd read_csv data_folder commitlist csv Lets explore files files = all_data files_changed to_list all_files = file files paths = Path file_part file_part file split all_files extend paths all_files append Path UNKNOWN_TOKEN file_set = build_file_set all_files MAX_LEN_FILE file_map = file i i file enumerate file_set open data_folder file_map pkl wb f pickle dump file_map f file_map Generate dataset training get_title_files_author_categories_zip_list dataframe pd DataFrame title = dataframe title to_list files_str = dataframe files_changed to_list author = dataframe author fillna UNKNOWN_TOKEN to_list category = dataframe category to_list list zip title files_str author category generate_batch batch title files author category = zip batch title = list title files = list files author = list author category = list category targets = torch tensor common categories index cat cat category device CommitClassifierInputs title files author targets train_step batch model optimizer loss input targets = batch optimizer zero_grad output = model input l = loss output targets l backward optimizer step l torch no_grad eval_step batch model loss input targets = batch output = model input l = loss output targets l balance_dataset dataset List HAS_IMBLEARN dataset title files author category = zip dataset category = common categories index cat cat category inpt_data = list zip title files author imblearn over_sampling RandomOverSampler imblearn under_sampling RandomUnderSampler rus = RandomOverSampler random_state= X y = rus fit_resample inpt_data category merged = list zip X y merged = random sample merged k= len dataset X y = zip merged rebuilt_dataset = i range len X rebuilt_dataset append X i common categories y i rebuilt_dataset gen_class_weights dataset List collections Counter epsilon = e- title files author category = zip dataset category = common categories index cat cat category counter = Counter category percentile_ = len category most_common = counter most_common percentile_ least_common = counter most_common -percentile_ smoothed_top = sum i + epsilon i most_common len most_common smoothed_bottom = sum i + epsilon i least_common len least_common class_weights = torch tensor min max counter i smoothed_bottom smoothed_top + epsilon i range len common categories device=device class_weights train save_path Path data_folder Path regen_data bool resample bool train_data val_data = get_train_val_data data_folder regen_data train_zip_list = get_title_files_author_categories_zip_list train_data val_zip_list = get_title_files_author_categories_zip_list val_data classifier_config = CategoryConfig common categories author_map = get_author_map data_folder regen_data file_map = get_file_map data_folder regen_data commit_classifier = CommitClassifier XLMR_BASE author_map file_map classifier_config device Lets train bag bits class_weights = gen_class_weights train_zip_list loss = torch nn CrossEntropyLoss weight=class_weights optimizer = torch optim Adam commit_classifier parameters lr= e- num_epochs = batch_size = resample Lets use train_zip_list = balance_dataset train_zip_list data_size = len train_zip_list print f Training data_size examples We can fit all val into one batch val_batch = generate_batch val_zip_list i tqdm range num_epochs desc= Epochs start = random shuffle train_zip_list while start data_size end = start + batch_size make last batch bigger needed end data_size end = data_size train_batch = train_zip_list start end train_batch = generate_batch train_batch l = train_step train_batch commit_classifier optimizer loss start = end val_l = eval_step val_batch commit_classifier loss tqdm write f Finished epoch i train loss l item val_loss val_l item torch no_grad commit_classifier eval val_inpts val_targets = val_batch val_output = commit_classifier val_inpts val_preds = torch argmax val_output dim= val_acc = torch sum val_preds == val_targets item len val_preds print f Final Validation accuracy val_acc print f Jobs done Saving save_path torch save commit_classifier state_dict save_path main parser = argparse ArgumentParser description= Tool create classifier helping categorize commits parser add_argument -- train action= store_true help= Train new classifier parser add_argument -- commit_data_folder default= results classifier parser add_argument -- save_path default= results classifier commit_classifier pt parser add_argument -- regen_data action= store_true help= Regenerate training data helps labeled more examples want re-train parser add_argument -- resample action= store_true help= Resample training data balanced Only works imblearn installed args = parser parse_args args train train Path args save_path Path args commit_data_folder args regen_data args resample print Currently file only trains new classifier please pass -- train train new classifier __name__ == __main__ main