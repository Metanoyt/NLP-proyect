typing Optional torch nn functional F torch Tensor torch nn common_types _ratio_ _t _ratio_ _t _size_ _t _size_ _opt_t _size_ _t _size_ _opt_t _size_ _t _size_any_opt_t _size_any_t module Module utils _pair _single _triple __all__ = MaxPool d MaxPool d MaxPool d MaxUnpool d MaxUnpool d MaxUnpool d AvgPool d AvgPool d AvgPool d FractionalMaxPool d FractionalMaxPool d LPPool d LPPool d LPPool d AdaptiveMaxPool d AdaptiveMaxPool d AdaptiveMaxPool d AdaptiveAvgPool d AdaptiveAvgPool d AdaptiveAvgPool d _MaxPoolNd Module __constants__ = kernel_size stride padding dilation return_indices ceil_mode return_indices bool ceil_mode bool __init__ kernel_size _size_any_t stride Optional _size_any_t = None padding _size_any_t = dilation _size_any_t = return_indices bool = False ceil_mode bool = False - None super __init__ kernel_size = kernel_size stride = stride stride None kernel_size padding = padding dilation = dilation return_indices = return_indices ceil_mode = ceil_mode extra_repr - str kernel_size= kernel_size stride= stride padding= padding dilation= dilation ceil_mode= ceil_mode format __dict__ MaxPool d _MaxPoolNd r Applies D max pooling over input signal composed several input planes In simplest case output value layer input size math ` N C L ` output math ` N C L_ out ` can precisely described math out N_i C_j k = \max_ m= \ldots \text kernel\_size - input N_i C_j stride \times k + m If attr ` padding ` non-zero then input implicitly padded negative infinity both sides attr ` padding ` number points attr ` dilation ` stride between elements within sliding window This ` link ` _ has nice visualization pooling parameters Note When ceil_mode=True sliding windows allowed go off-bounds they start within left padding input Sliding windows would start right padded region ignored Args kernel_size The size sliding window must stride The stride sliding window must Default value attr ` kernel_size ` padding Implicit negative infinity padding added both sides must = = kernel_size dilation The stride between elements within sliding window must return_indices If ` ` True ` ` will argmax along max values Useful ` torch nn MaxUnpool d ` later ceil_mode If ` ` True ` ` will use ` ceil ` instead ` floor ` compute output shape This ensures every element input tensor covered sliding window Shape - Input math ` N C L_ ` math ` C L_ ` - Output math ` N C L_ out ` math ` C L_ out ` where ` ` ceil_mode = False ` ` math L_ out = \left\lfloor \frac L_ + \times \text padding - \text dilation \times \text kernel\_size - - \text stride \right\rfloor + where ` ` ceil_mode = True ` ` math L_ out = \left\lceil \frac L_ + \times \text padding - \text dilation \times \text kernel\_size - - + stride - \text stride \right\rceil + - Ensure last pooling starts inside image make math ` L_ out = L_ out - ` when math ` L_ out - \text stride = L_ + \text padding ` Examples pool size= stride= m = nn MaxPool d stride= input = torch randn output = m input _link https github com vdumoulin conv_arithmetic blob master README md kernel_size _size_ _t stride _size_ _t padding _size_ _t dilation _size_ _t forward input Tensor Runs forward pass F max_pool d input kernel_size stride padding dilation ceil_mode=self ceil_mode return_indices=self return_indices MaxPool d _MaxPoolNd r Applies D max pooling over input signal composed several input planes In simplest case output value layer input size math ` N C H W ` output math ` N C H_ out W_ out ` attr ` kernel_size ` math ` kH kW ` can precisely described math \begin aligned out N_i C_j h w = \max_ m= \ldots kH- \max_ n= \ldots kW- \\ \text input N_i C_j \text stride \times h + m \text stride \times w + n \end aligned If attr ` padding ` non-zero then input implicitly padded negative infinity both sides attr ` padding ` number points attr ` dilation ` controls spacing between kernel points It harder describe ` link ` _ has nice visualization what attr ` dilation ` does Note When ceil_mode=True sliding windows allowed go off-bounds they start within left padding input Sliding windows would start right padded region ignored The parameters attr ` kernel_size ` attr ` stride ` attr ` padding ` attr ` dilation ` can either - single ` ` int ` ` -- which case same value used height width dimension - ` ` tuple ` ` two ints -- which case first ` int ` used height dimension second ` int ` width dimension Args kernel_size size window take max over stride stride window Default value attr ` kernel_size ` padding Implicit negative infinity padding added both sides dilation parameter controls stride elements window return_indices ` ` True ` ` will max indices along outputs Useful ` torch nn MaxUnpool d ` later ceil_mode when True will use ` ceil ` instead ` floor ` compute output shape Shape - Input math ` N C H_ W_ ` math ` C H_ W_ ` - Output math ` N C H_ out W_ out ` math ` C H_ out W_ out ` where math H_ out = \left\lfloor\frac H_ + \text padding - \text dilation \times \text kernel\_size - - \text stride + \right\rfloor math W_ out = \left\lfloor\frac W_ + \text padding - \text dilation \times \text kernel\_size - - \text stride + \right\rfloor Examples pool square window size= stride= m = nn MaxPool d stride= pool non-square window m = nn MaxPool d stride= input = torch randn output = m input _link https github com vdumoulin conv_arithmetic blob master README md kernel_size _size_ _t stride _size_ _t padding _size_ _t dilation _size_ _t forward input Tensor Runs forward pass F max_pool d input kernel_size stride padding dilation ceil_mode=self ceil_mode return_indices=self return_indices MaxPool d _MaxPoolNd r Applies D max pooling over input signal composed several input planes In simplest case output value layer input size math ` N C D H W ` output math ` N C D_ out H_ out W_ out ` attr ` kernel_size ` math ` kD kH kW ` can precisely described math \begin aligned \text out N_i C_j d h w = \max_ k= \ldots kD- \max_ m= \ldots kH- \max_ n= \ldots kW- \\ \text input N_i C_j \text stride \times d + k \text stride \times h + m \text stride \times w + n \end aligned If attr ` padding ` non-zero then input implicitly padded negative infinity both sides attr ` padding ` number points attr ` dilation ` controls spacing between kernel points It harder describe ` link ` _ has nice visualization what attr ` dilation ` does Note When ceil_mode=True sliding windows allowed go off-bounds they start within left padding input Sliding windows would start right padded region ignored The parameters attr ` kernel_size ` attr ` stride ` attr ` padding ` attr ` dilation ` can either - single ` ` int ` ` -- which case same value used depth height width dimension - ` ` tuple ` ` three ints -- which case first ` int ` used depth dimension second ` int ` height dimension third ` int ` width dimension Args kernel_size size window take max over stride stride window Default value attr ` kernel_size ` padding Implicit negative infinity padding added all three sides dilation parameter controls stride elements window return_indices ` ` True ` ` will max indices along outputs Useful ` torch nn MaxUnpool d ` later ceil_mode when True will use ` ceil ` instead ` floor ` compute output shape Shape - Input math ` N C D_ H_ W_ ` math ` C D_ H_ W_ ` - Output math ` N C D_ out H_ out W_ out ` math ` C D_ out H_ out W_ out ` where math D_ out = \left\lfloor\frac D_ + \times \text padding - \text dilation \times \text kernel\_size - - \text stride + \right\rfloor math H_ out = \left\lfloor\frac H_ + \times \text padding - \text dilation \times \text kernel\_size - - \text stride + \right\rfloor math W_ out = \left\lfloor\frac W_ + \times \text padding - \text dilation \times \text kernel\_size - - \text stride + \right\rfloor Examples pool square window size= stride= m = nn MaxPool d stride= pool non-square window m = nn MaxPool d stride= input = torch randn output = m input _link https github com vdumoulin conv_arithmetic blob master README md kernel_size _size_ _t stride _size_ _t padding _size_ _t dilation _size_ _t forward input Tensor Runs forward pass F max_pool d input kernel_size stride padding dilation ceil_mode=self ceil_mode return_indices=self return_indices _MaxUnpoolNd Module extra_repr - str f kernel_size= kernel_size stride= stride padding= padding MaxUnpool d _MaxUnpoolNd r Computes partial inverse ` MaxPool d ` ` MaxPool d ` fully invertible since non-maximal values lost ` MaxUnpool d ` takes input output ` MaxPool d ` including indices maximal values computes partial inverse which all non-maximal values set zero Note This operation may behave nondeterministically when input indices has repeat values See https github com pytorch pytorch issues doc ` notes randomness ` more information note ` MaxPool d ` can map several input sizes same output sizes Hence inversion process can get ambiguous To accommodate you can provide needed output size additional argument attr ` output_size ` forward call See Inputs Example below Args kernel_size int tuple Size max pooling window stride int tuple Stride max pooling window It set attr ` kernel_size ` default padding int tuple Padding added input Inputs - ` input ` input Tensor invert - ` indices ` indices given out ` ~torch nn MaxPool d ` - ` output_size ` optional targeted output size Shape - Input math ` N C H_ ` math ` C H_ ` - Output math ` N C H_ out ` math ` C H_ out ` where math H_ out = H_ - \times \text stride - \times \text padding + \text kernel\_size given attr ` output_size ` call operator Example xdoctest +IGNORE_WANT do other tests modify global state pool = nn MaxPool d stride= return_indices=True unpool = nn MaxUnpool d stride= input = torch tensor output indices = pool input unpool output indices tensor Example showcasing use output_size input = torch tensor output indices = pool input unpool output indices output_size=input size tensor unpool output indices tensor kernel_size _size_ _t stride _size_ _t padding _size_ _t __init__ kernel_size _size_ _t stride Optional _size_ _t = None padding _size_ _t = - None super __init__ kernel_size = _single kernel_size stride = _single stride stride None kernel_size padding = _single padding forward input Tensor indices Tensor output_size Optional list int = None - Tensor Runs forward pass F max_unpool d input indices kernel_size stride padding output_size MaxUnpool d _MaxUnpoolNd r Computes partial inverse ` MaxPool d ` ` MaxPool d ` fully invertible since non-maximal values lost ` MaxUnpool d ` takes input output ` MaxPool d ` including indices maximal values computes partial inverse which all non-maximal values set zero Note This operation may behave nondeterministically when input indices has repeat values See https github com pytorch pytorch issues doc ` notes randomness ` more information note ` MaxPool d ` can map several input sizes same output sizes Hence inversion process can get ambiguous To accommodate you can provide needed output size additional argument attr ` output_size ` forward call See Inputs Example below Args kernel_size int tuple Size max pooling window stride int tuple Stride max pooling window It set attr ` kernel_size ` default padding int tuple Padding added input Inputs - ` input ` input Tensor invert - ` indices ` indices given out ` ~torch nn MaxPool d ` - ` output_size ` optional targeted output size Shape - Input math ` N C H_ W_ ` math ` C H_ W_ ` - Output math ` N C H_ out W_ out ` math ` C H_ out W_ out ` where math H_ out = H_ - \times \text stride - \times \text padding + \text kernel\_size math W_ out = W_ - \times \text stride - \times \text padding + \text kernel\_size given attr ` output_size ` call operator Example pool = nn MaxPool d stride= return_indices=True unpool = nn MaxUnpool d stride= input = torch tensor output indices = pool input unpool output indices tensor Now using output_size resolve ambiguous size inverse input = torch tensor output indices = pool input This call will work without specifying output_size unpool output indices output_size=input size tensor kernel_size _size_ _t stride _size_ _t padding _size_ _t __init__ kernel_size _size_ _t stride Optional _size_ _t = None padding _size_ _t = - None super __init__ kernel_size = _pair kernel_size stride = _pair stride stride None kernel_size padding = _pair padding forward input Tensor indices Tensor output_size Optional list int = None - Tensor Runs forward pass F max_unpool d input indices kernel_size stride padding output_size MaxUnpool d _MaxUnpoolNd r Computes partial inverse ` MaxPool d ` ` MaxPool d ` fully invertible since non-maximal values lost ` MaxUnpool d ` takes input output ` MaxPool d ` including indices maximal values computes partial inverse which all non-maximal values set zero Note This operation may behave nondeterministically when input indices has repeat values See https github com pytorch pytorch issues doc ` notes randomness ` more information note ` MaxPool d ` can map several input sizes same output sizes Hence inversion process can get ambiguous To accommodate you can provide needed output size additional argument attr ` output_size ` forward call See Inputs section below Args kernel_size int tuple Size max pooling window stride int tuple Stride max pooling window It set attr ` kernel_size ` default padding int tuple Padding added input Inputs - ` input ` input Tensor invert - ` indices ` indices given out ` ~torch nn MaxPool d ` - ` output_size ` optional targeted output size Shape - Input math ` N C D_ H_ W_ ` math ` C D_ H_ W_ ` - Output math ` N C D_ out H_ out W_ out ` math ` C D_ out H_ out W_ out ` where math D_ out = D_ - \times \text stride - \times \text padding + \text kernel\_size math H_ out = H_ - \times \text stride - \times \text padding + \text kernel\_size math W_ out = W_ - \times \text stride - \times \text padding + \text kernel\_size given attr ` output_size ` call operator Example pool square window size= stride= pool = nn MaxPool d stride= return_indices=True unpool = nn MaxUnpool d stride= output indices = pool torch randn unpooled_output = unpool output indices unpooled_output size torch Size kernel_size _size_ _t stride _size_ _t padding _size_ _t __init__ kernel_size _size_ _t stride Optional _size_ _t = None padding _size_ _t = - None super __init__ kernel_size = _triple kernel_size stride = _triple stride stride None kernel_size padding = _triple padding forward input Tensor indices Tensor output_size Optional list int = None - Tensor Runs forward pass F max_unpool d input indices kernel_size stride padding output_size _AvgPoolNd Module __constants__ = kernel_size stride padding ceil_mode count_include_pad extra_repr - str f kernel_size= kernel_size stride= stride padding= padding AvgPool d _AvgPoolNd r Applies D average pooling over input signal composed several input planes In simplest case output value layer input size math ` N C L ` output math ` N C L_ out ` attr ` kernel_size ` math ` k ` can precisely described math \text out N_i C_j l = \frac k \sum_ m= ^ k- \text input N_i C_j \text stride \times l + m If attr ` padding ` non-zero then input implicitly zero-padded both sides attr ` padding ` number points Note When ceil_mode=True sliding windows allowed go off-bounds they start within left padding input Sliding windows would start right padded region ignored note pad should most half effective kernel size The parameters attr ` kernel_size ` attr ` stride ` attr ` padding ` can each ` ` int ` ` one-element tuple Args kernel_size size window stride stride window Default value attr ` kernel_size ` padding implicit zero padding added both sides ceil_mode when True will use ` ceil ` instead ` floor ` compute output shape count_include_pad when True will include zero-padding averaging calculation Shape - Input math ` N C L_ ` math ` C L_ ` - Output math ` N C L_ out ` math ` C L_ out ` where math L_ out = \left\lfloor \frac L_ + \times \text padding - \text kernel\_size \text stride + \right\rfloor Per note above ` ` ceil_mode ` ` True math ` L_ out - \times \text stride \geq L_ + \text padding ` we skip last window would start right padded region resulting math ` L_ out ` being reduced one Examples pool window size= stride= m = nn AvgPool d stride= m torch tensor tensor kernel_size _size_ _t stride _size_ _t padding _size_ _t ceil_mode bool count_include_pad bool __init__ kernel_size _size_ _t stride _size_ _t = None padding _size_ _t = ceil_mode bool = False count_include_pad bool = True - None super __init__ kernel_size = _single kernel_size stride = _single stride stride None kernel_size padding = _single padding ceil_mode = ceil_mode count_include_pad = count_include_pad forward input Tensor - Tensor Runs forward pass F avg_pool d input kernel_size stride padding ceil_mode count_include_pad AvgPool d _AvgPoolNd r Applies D average pooling over input signal composed several input planes In simplest case output value layer input size math ` N C H W ` output math ` N C H_ out W_ out ` attr ` kernel_size ` math ` kH kW ` can precisely described math out N_i C_j h w = \frac kH kW \sum_ m= ^ kH- \sum_ n= ^ kW- input N_i C_j stride \times h + m stride \times w + n If attr ` padding ` non-zero then input implicitly zero-padded both sides attr ` padding ` number points Note When ceil_mode=True sliding windows allowed go off-bounds they start within left padding input Sliding windows would start right padded region ignored note pad should most half effective kernel size The parameters attr ` kernel_size ` attr ` stride ` attr ` padding ` can either - single ` ` int ` ` single-element tuple -- which case same value used height width dimension - ` ` tuple ` ` two ints -- which case first ` int ` used height dimension second ` int ` width dimension Args kernel_size size window stride stride window Default value attr ` kernel_size ` padding implicit zero padding added both sides ceil_mode when True will use ` ceil ` instead ` floor ` compute output shape count_include_pad when True will include zero-padding averaging calculation divisor_override specified will used divisor otherwise size pooling region will used Shape - Input math ` N C H_ W_ ` math ` C H_ W_ ` - Output math ` N C H_ out W_ out ` math ` C H_ out W_ out ` where math H_ out = \left\lfloor\frac H_ + \times \text padding - \text kernel\_size \text stride + \right\rfloor math W_ out = \left\lfloor\frac W_ + \times \text padding - \text kernel\_size \text stride + \right\rfloor Per note above ` ` ceil_mode ` ` True math ` H_ out - \times \text stride \geq H_ + \text padding ` we skip last window would start bottom padded region resulting math ` H_ out ` being reduced one The same applies math ` W_ out ` Examples pool square window size= stride= m = nn AvgPool d stride= pool non-square window m = nn AvgPool d stride= input = torch randn output = m input __constants__ = kernel_size stride padding ceil_mode count_include_pad divisor_override kernel_size _size_ _t stride _size_ _t padding _size_ _t ceil_mode bool count_include_pad bool __init__ kernel_size _size_ _t stride Optional _size_ _t = None padding _size_ _t = ceil_mode bool = False count_include_pad bool = True divisor_override Optional int = None - None super __init__ kernel_size = kernel_size stride = stride stride None kernel_size padding = padding ceil_mode = ceil_mode count_include_pad = count_include_pad divisor_override = divisor_override forward input Tensor - Tensor Runs forward pass F avg_pool d input kernel_size stride padding ceil_mode count_include_pad divisor_override AvgPool d _AvgPoolNd r Applies D average pooling over input signal composed several input planes In simplest case output value layer input size math ` N C D H W ` output math ` N C D_ out H_ out W_ out ` attr ` kernel_size ` math ` kD kH kW ` can precisely described math \begin aligned \text out N_i C_j d h w = \sum_ k= ^ kD- \sum_ m= ^ kH- \sum_ n= ^ kW- \\ \frac \text input N_i C_j \text stride \times d + k \text stride \times h + m \text stride \times w + n kD \times kH \times kW \end aligned If attr ` padding ` non-zero then input implicitly zero-padded all three sides attr ` padding ` number points Note When ceil_mode=True sliding windows allowed go off-bounds they start within left padding input Sliding windows would start right padded region ignored note pad should most half effective kernel size The parameters attr ` kernel_size ` attr ` stride ` can either - single ` ` int ` ` -- which case same value used depth height width dimension - ` ` tuple ` ` three ints -- which case first ` int ` used depth dimension second ` int ` height dimension third ` int ` width dimension Args kernel_size size window stride stride window Default value attr ` kernel_size ` padding implicit zero padding added all three sides ceil_mode when True will use ` ceil ` instead ` floor ` compute output shape count_include_pad when True will include zero-padding averaging calculation divisor_override specified will used divisor otherwise attr ` kernel_size ` will used Shape - Input math ` N C D_ H_ W_ ` math ` C D_ H_ W_ ` - Output math ` N C D_ out H_ out W_ out ` math ` C D_ out H_ out W_ out ` where math D_ out = \left\lfloor\frac D_ + \times \text padding - \text kernel\_size \text stride + \right\rfloor math H_ out = \left\lfloor\frac H_ + \times \text padding - \text kernel\_size \text stride + \right\rfloor math W_ out = \left\lfloor\frac W_ + \times \text padding - \text kernel\_size \text stride + \right\rfloor Per note above ` ` ceil_mode ` ` True math ` D_ out - \times \text stride \geq D_ + \text padding ` we skip last window would start padded region resulting math ` D_ out ` being reduced one The same applies math ` W_ out ` math ` H_ out ` Examples pool square window size= stride= m = nn AvgPool d stride= pool non-square window m = nn AvgPool d stride= input = torch randn output = m input __constants__ = kernel_size stride padding ceil_mode count_include_pad divisor_override kernel_size _size_ _t stride _size_ _t padding _size_ _t ceil_mode bool count_include_pad bool __init__ kernel_size _size_ _t stride Optional _size_ _t = None padding _size_ _t = ceil_mode bool = False count_include_pad bool = True divisor_override Optional int = None - None super __init__ kernel_size = kernel_size stride = stride stride None kernel_size padding = padding ceil_mode = ceil_mode count_include_pad = count_include_pad divisor_override = divisor_override forward input Tensor - Tensor Runs forward pass F avg_pool d input kernel_size stride padding ceil_mode count_include_pad divisor_override __setstate__ d super __setstate__ d __dict__ setdefault padding __dict__ setdefault ceil_mode False __dict__ setdefault count_include_pad True FractionalMaxPool d Module r Applies D fractional max pooling over input signal composed several input planes Fractional MaxPooling described detail paper ` Fractional MaxPooling ` _ Ben Graham The max-pooling operation applied math ` kH \times kW ` regions stochastic step size determined target output size The number output features equal number input planes note Exactly one ` ` output_size ` ` ` ` output_ratio ` ` must defined Args kernel_size size window take max over Can single number k square kernel k x k tuple ` kh kw ` output_size target output size image form ` oH x oW ` Can tuple ` oH oW ` single number oH square image ` oH x oH ` Note we must have math ` kH + oH - = H_ ` math ` kW + oW - = W_ ` output_ratio If one wants have output size ratio input size option can given This has number tuple range Note we must have math ` kH + output\_ratio\_H H_ - = H_ ` math ` kW + output\_ratio\_W W_ - = W_ ` return_indices ` ` True ` ` will indices along outputs Useful pass meth ` nn MaxUnpool d ` Default ` ` False ` ` Shape - Input math ` N C H_ W_ ` math ` C H_ W_ ` - Output math ` N C H_ out W_ out ` math ` C H_ out W_ out ` where math ` H_ out W_ out =\text output\_size ` math ` H_ out W_ out =\text output\_ratio \times H_ W_ ` Examples pool square window size= target output size x m = nn FractionalMaxPool d output_size= pool square window target output size being half input image size m = nn FractionalMaxPool d output_ratio= input = torch randn output = m input _Fractional MaxPooling https arxiv org abs __constants__ = kernel_size return_indices output_size output_ratio kernel_size _size_ _t return_indices bool output_size _size_ _t output_ratio _ratio_ _t __init__ kernel_size _size_ _t output_size Optional _size_ _t = None output_ratio Optional _ratio_ _t = None return_indices bool = False _random_samples=None - None super __init__ kernel_size = _pair kernel_size return_indices = return_indices register_buffer _random_samples _random_samples output_size = _pair output_size output_size None None output_ratio = _pair output_ratio output_ratio None None output_size None output_ratio None raise ValueError FractionalMaxPool d requires specifying either output size pooling ratio output_size None output_ratio None raise ValueError only one output_size output_ratio may specified output_ratio None output_ratio output_ratio raise ValueError f output_ratio must between got output_ratio forward input Tensor F fractional_max_pool d input kernel_size output_size output_ratio return_indices _random_samples=self _random_samples FractionalMaxPool d Module r Applies D fractional max pooling over input signal composed several input planes Fractional MaxPooling described detail paper ` Fractional MaxPooling ` _ Ben Graham The max-pooling operation applied math ` kT \times kH \times kW ` regions stochastic step size determined target output size The number output features equal number input planes note Exactly one ` ` output_size ` ` ` ` output_ratio ` ` must defined Args kernel_size size window take max over Can single number ` k ` square kernel ` k x k x k ` tuple ` kt x kh x kw ` ` k ` must greater than output_size target output size image form ` oT x oH x oW ` Can tuple ` oT oH oW ` single number oH square image ` oH x oH x oH ` output_ratio If one wants have output size ratio input size option can given This has number tuple range return_indices ` ` True ` ` will indices along outputs Useful pass meth ` nn MaxUnpool d ` Default ` ` False ` ` Shape - Input math ` N C T_ H_ W_ ` math ` C T_ H_ W_ ` - Output math ` N C T_ out H_ out W_ out ` math ` C T_ out H_ out W_ out ` where math ` T_ out H_ out W_ out =\text output\_size ` math ` T_ out H_ out W_ out =\text output\_ratio \times T_ H_ W_ ` Examples pool cubic window size= target output size x x m = nn FractionalMaxPool d output_size= pool cubic window target output size being half input size m = nn FractionalMaxPool d output_ratio= input = torch randn output = m input _Fractional MaxPooling https arxiv org abs __constants__ = kernel_size return_indices output_size output_ratio kernel_size _size_ _t return_indices bool output_size _size_ _t output_ratio _ratio_ _t __init__ kernel_size _size_ _t output_size Optional _size_ _t = None output_ratio Optional _ratio_ _t = None return_indices bool = False _random_samples=None - None super __init__ isinstance kernel_size int kernel_size = isinstance kernel_size tuple list all k k kernel_size raise ValueError f kernel_size must greater than got kernel_size kernel_size = _triple kernel_size return_indices = return_indices register_buffer _random_samples _random_samples output_size = _triple output_size output_size None None output_ratio = _triple output_ratio output_ratio None None output_size None output_ratio None raise ValueError FractionalMaxPool d requires specifying either output size pooling ratio output_size None output_ratio None raise ValueError only one output_size output_ratio may specified output_ratio None output_ratio output_ratio output_ratio raise ValueError f output_ratio must between got output_ratio forward input Tensor F fractional_max_pool d input kernel_size output_size output_ratio return_indices _random_samples=self _random_samples _LPPoolNd Module __constants__ = norm_type kernel_size stride ceil_mode norm_type float ceil_mode bool __init__ norm_type float kernel_size _size_any_t stride Optional _size_any_t = None ceil_mode bool = False - None super __init__ norm_type = norm_type kernel_size = kernel_size stride = stride ceil_mode = ceil_mode extra_repr - str norm_type= norm_type kernel_size= kernel_size stride= stride ceil_mode= ceil_mode format __dict__ LPPool d _LPPoolNd r Applies D power-average pooling over input signal composed several input planes On each window function computed math f X = \sqrt p \sum_ x \in X x^ p - At p = math ` \infty ` one gets Max Pooling - At p = one gets Sum Pooling which proportional Average Pooling note If sum power ` p ` zero gradient function defined This implementation will set gradient zero case Args kernel_size single int size window stride single int stride window Default value attr ` kernel_size ` ceil_mode when True will use ` ceil ` instead ` floor ` compute output shape Note When attr ` ceil_mode ` ` ` True ` ` sliding windows may go off-bounds they start within left padding input Sliding windows would start right padded region ignored Shape - Input math ` N C L_ ` math ` C L_ ` - Output math ` N C L_ out ` math ` C L_ out ` where math L_ out = \left\lfloor\frac L_ - \text kernel\_size \text stride + \right\rfloor Examples power- pool window length stride m = nn LPPool d stride= input = torch randn output = m input kernel_size _size_ _t stride _size_ _t forward input Tensor - Tensor Runs forward pass F lp_pool d input float norm_type kernel_size stride ceil_mode LPPool d _LPPoolNd r Applies D power-average pooling over input signal composed several input planes On each window function computed math f X = \sqrt p \sum_ x \in X x^ p - At p = math ` \infty ` one gets Max Pooling - At p = one gets Sum Pooling which proportional average pooling The parameters attr ` kernel_size ` attr ` stride ` can either - single ` ` int ` ` -- which case same value used height width dimension - ` ` tuple ` ` two ints -- which case first ` int ` used height dimension second ` int ` width dimension note If sum power ` p ` zero gradient function defined This implementation will set gradient zero case Args kernel_size size window stride stride window Default value attr ` kernel_size ` ceil_mode when True will use ` ceil ` instead ` floor ` compute output shape Note When attr ` ceil_mode ` ` ` True ` ` sliding windows may go off-bounds they start within left padding input Sliding windows would start right padded region ignored Shape - Input math ` N C H_ W_ ` math ` C H_ W_ ` - Output math ` N C H_ out W_ out ` math ` C H_ out W_ out ` where math H_ out = \left\lfloor\frac H_ - \text kernel\_size \text stride + \right\rfloor math W_ out = \left\lfloor\frac W_ - \text kernel\_size \text stride + \right\rfloor Examples power- pool square window size= stride= m = nn LPPool d stride= pool non-square window power m = nn LPPool d stride= input = torch randn output = m input kernel_size _size_ _t stride _size_ _t forward input Tensor - Tensor Runs forward pass F lp_pool d input float norm_type kernel_size stride ceil_mode LPPool d _LPPoolNd r Applies D power-average pooling over input signal composed several input planes On each window function computed math f X = \sqrt p \sum_ x \in X x^ p - At p = math ` \infty ` one gets Max Pooling - At p = one gets Sum Pooling which proportional average pooling The parameters attr ` kernel_size ` attr ` stride ` can either - single ` ` int ` ` -- which case same value used height width depth dimension - ` ` tuple ` ` three ints -- which case first ` int ` used depth dimension second ` int ` height dimension third ` int ` width dimension note If sum power ` p ` zero gradient function defined This implementation will set gradient zero case Args kernel_size size window stride stride window Default value attr ` kernel_size ` ceil_mode when True will use ` ceil ` instead ` floor ` compute output shape Note When attr ` ceil_mode ` ` ` True ` ` sliding windows may go off-bounds they start within left padding input Sliding windows would start right padded region ignored Shape - Input math ` N C D_ H_ W_ ` math ` C D_ H_ W_ ` - Output math ` N C D_ out H_ out W_ out ` math ` C D_ out H_ out W_ out ` where math D_ out = \left\lfloor\frac D_ - \text kernel\_size \text stride + \right\rfloor math H_ out = \left\lfloor\frac H_ - \text kernel\_size \text stride + \right\rfloor math W_ out = \left\lfloor\frac W_ - \text kernel\_size \text stride + \right\rfloor Examples power- pool square window size= stride= m = nn LPPool d stride= pool non-square window power m = nn LPPool d stride= input = torch randn output = m input kernel_size _size_ _t stride _size_ _t forward input Tensor - Tensor Runs forward pass F lp_pool d input float norm_type kernel_size stride ceil_mode _AdaptiveMaxPoolNd Module __constants__ = output_size return_indices return_indices bool __init__ output_size _size_any_opt_t return_indices bool = False - None super __init__ output_size = output_size return_indices = return_indices extra_repr - str f output_size= output_size FIXME ssnl Improve adaptive pooling docs specify what input output shapes how operation computes output AdaptiveMaxPool d _AdaptiveMaxPoolNd r Applies D adaptive max pooling over input signal composed several input planes The output size math ` L_ out ` any input size The number output features equal number input planes Args output_size target output size math ` L_ out ` return_indices ` ` True ` ` will indices along outputs Useful pass nn MaxUnpool d Default ` ` False ` ` Shape - Input math ` N C L_ ` math ` C L_ ` - Output math ` N C L_ out ` math ` C L_ out ` where math ` L_ out =\text output\_size ` Examples target output size m = nn AdaptiveMaxPool d input = torch randn output = m input output_size _size_ _t forward input Tensor Runs forward pass F adaptive_max_pool d input output_size return_indices AdaptiveMaxPool d _AdaptiveMaxPoolNd r Applies D adaptive max pooling over input signal composed several input planes The output size math ` H_ out \times W_ out ` any input size The number output features equal number input planes Args output_size target output size image form math ` H_ out \times W_ out ` Can tuple math ` H_ out W_ out ` single math ` H_ out ` square image math ` H_ out \times H_ out ` math ` H_ out ` math ` W_ out ` can either ` ` int ` ` ` ` None ` ` which means size will same input return_indices ` ` True ` ` will indices along outputs Useful pass nn MaxUnpool d Default ` ` False ` ` Shape - Input math ` N C H_ W_ ` math ` C H_ W_ ` - Output math ` N C H_ out W_ out ` math ` C H_ out W_ out ` where math ` H_ out W_ out =\text output\_size ` Examples target output size x m = nn AdaptiveMaxPool d input = torch randn output = m input target output size x square m = nn AdaptiveMaxPool d input = torch randn output = m input target output size x m = nn AdaptiveMaxPool d None input = torch randn output = m input output_size _size_ _opt_t forward input Tensor Runs forward pass F adaptive_max_pool d input output_size return_indices AdaptiveMaxPool d _AdaptiveMaxPoolNd r Applies D adaptive max pooling over input signal composed several input planes The output size math ` D_ out \times H_ out \times W_ out ` any input size The number output features equal number input planes Args output_size target output size image form math ` D_ out \times H_ out \times W_ out ` Can tuple math ` D_ out H_ out W_ out ` single math ` D_ out ` cube math ` D_ out \times D_ out \times D_ out ` math ` D_ out ` math ` H_ out ` math ` W_ out ` can either ` ` int ` ` ` ` None ` ` which means size will same input return_indices ` ` True ` ` will indices along outputs Useful pass nn MaxUnpool d Default ` ` False ` ` Shape - Input math ` N C D_ H_ W_ ` math ` C D_ H_ W_ ` - Output math ` N C D_ out H_ out W_ out ` math ` C D_ out H_ out W_ out ` where math ` D_ out H_ out W_ out =\text output\_size ` Examples target output size x x m = nn AdaptiveMaxPool d input = torch randn output = m input target output size x x cube m = nn AdaptiveMaxPool d input = torch randn output = m input target output size x x m = nn AdaptiveMaxPool d None None input = torch randn output = m input output_size _size_ _opt_t forward input Tensor Runs forward pass F adaptive_max_pool d input output_size return_indices _AdaptiveAvgPoolNd Module __constants__ = output_size __init__ output_size _size_any_opt_t - None super __init__ output_size = output_size extra_repr - str f output_size= output_size AdaptiveAvgPool d _AdaptiveAvgPoolNd r Applies D adaptive average pooling over input signal composed several input planes The output size math ` L_ out ` any input size The number output features equal number input planes Args output_size target output size math ` L_ out ` Shape - Input math ` N C L_ ` math ` C L_ ` - Output math ` N C L_ out ` math ` C L_ out ` where math ` L_ out =\text output\_size ` Examples target output size m = nn AdaptiveAvgPool d input = torch randn output = m input output_size _size_ _t forward input Tensor - Tensor Runs forward pass F adaptive_avg_pool d input output_size AdaptiveAvgPool d _AdaptiveAvgPoolNd r Applies D adaptive average pooling over input signal composed several input planes The output size H x W any input size The number output features equal number input planes Args output_size target output size image form H x W Can tuple H W single H square image H x H H W can either ` ` int ` ` ` ` None ` ` which means size will same input Shape - Input math ` N C H_ W_ ` math ` C H_ W_ ` - Output math ` N C S_ S_ ` math ` C S_ S_ ` where math ` S=\text output\_size ` Examples target output size x m = nn AdaptiveAvgPool d input = torch randn output = m input target output size x square m = nn AdaptiveAvgPool d input = torch randn output = m input target output size x m = nn AdaptiveAvgPool d None input = torch randn output = m input output_size _size_ _opt_t forward input Tensor - Tensor Runs forward pass F adaptive_avg_pool d input output_size AdaptiveAvgPool d _AdaptiveAvgPoolNd r Applies D adaptive average pooling over input signal composed several input planes The output size D x H x W any input size The number output features equal number input planes Args output_size target output size form D x H x W Can tuple D H W single number D cube D x D x D D H W can either ` ` int ` ` ` ` None ` ` which means size will same input Shape - Input math ` N C D_ H_ W_ ` math ` C D_ H_ W_ ` - Output math ` N C S_ S_ S_ ` math ` C S_ S_ S_ ` where math ` S=\text output\_size ` Examples target output size x x m = nn AdaptiveAvgPool d input = torch randn output = m input target output size x x cube m = nn AdaptiveAvgPool d input = torch randn output = m input target output size x x m = nn AdaptiveAvgPool d None None input = torch randn output = m input output_size _size_ _opt_t forward input Tensor - Tensor Runs forward pass F adaptive_avg_pool d input output_size