Owner s oncall quantization Torch torch torch ao nn quantized functional qF torch nn functional F Standard library numpy np Testing utils hypothesis assume given hypothesis strategies st torch testing _internal common_quantization QuantizationTestCase _make_conv_test_input torch testing _internal common_quantized override_quantized_engine torch testing _internal common_utils raise_on_run_directly IS_PPC TestQuantizedFunctionalOps QuantizationTestCase test_relu_api X = torch arange - dtype=torch float scale = zero_point = qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch quint qY = torch relu qX qY_hat = F relu qX assertEqual qY qY_hat _test_conv_api_impl qconv_fn conv_fn batch_size in_channels_per_group input_feature_map_size out_channels_per_group groups kernel_size stride padding dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias use_channelwise i range len kernel_size assume input_feature_map_size i + padding i = dilation i kernel_size i - + X X_q W W_q b = _make_conv_test_input batch_size in_channels_per_group input_feature_map_size out_channels_per_group groups kernel_size X_scale X_zero_point W_scale W_zero_point use_bias use_channelwise Y_exp = conv_fn X W b stride padding dilation groups Y_exp = torch quantize_per_tensor Y_exp scale=Y_scale zero_point=Y_zero_point dtype=torch quint Y_act = qconv_fn X_q W_q b stride padding dilation groups padding_mode= zeros scale=Y_scale zero_point=Y_zero_point Make sure results match assert_array_almost_equal compares using following formula abs desired-actual -decimal https numpy org doc stable reference generated numpy testing assert_almost_equal html We use decimal = ignore off-by- differences between reference test Off-by- differences arise due order round zero_point addition operation i e addition followed round used reference round followed addition used test results may differ For example result round + while round + assuming rounding mode round-to-nearest ties-to-even np testing assert_array_almost_equal Y_exp int_repr numpy Y_act int_repr numpy decimal= given batch_size=st integers in_channels_per_group=st sampled_from L=st integers out_channels_per_group=st sampled_from groups=st integers kernel=st integers stride=st integers pad=st integers dilation=st integers X_scale=st floats X_zero_point=st integers W_scale=st lists st floats min_size= max_size= W_zero_point=st lists st integers - min_size= max_size= Y_scale=st floats Y_zero_point=st integers use_bias=st booleans use_channelwise=st booleans qengine=st sampled_from qnnpack fbgemm test_conv d_api batch_size in_channels_per_group L out_channels_per_group groups kernel stride pad dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias use_channelwise qengine Tests correctness conv d function qengine torch backends quantized supported_engines qengine == qnnpack IS_PPC use_channelwise = False input_feature_map_size = L kernel_size = kernel stride = stride padding = pad dilation = dilation override_quantized_engine qengine qconv_fn = qF conv d conv_fn = F conv d _test_conv_api_impl qconv_fn conv_fn batch_size in_channels_per_group input_feature_map_size out_channels_per_group groups kernel_size stride padding dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias use_channelwise given batch_size=st integers in_channels_per_group=st sampled_from H=st integers W=st integers out_channels_per_group=st sampled_from groups=st integers kernel_h=st integers kernel_w=st integers stride_h=st integers stride_w=st integers pad_h=st integers pad_w=st integers dilation=st integers X_scale=st floats X_zero_point=st integers W_scale=st lists st floats min_size= max_size= W_zero_point=st lists st integers - min_size= max_size= Y_scale=st floats Y_zero_point=st integers use_bias=st booleans use_channelwise=st booleans qengine=st sampled_from qnnpack fbgemm test_conv d_api batch_size in_channels_per_group H W out_channels_per_group groups kernel_h kernel_w stride_h stride_w pad_h pad_w dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias use_channelwise qengine Tests correctness conv d function qengine torch backends quantized supported_engines qengine == qnnpack IS_PPC input_feature_map_size = H W kernel_size = kernel_h kernel_w stride = stride_h stride_w padding = pad_h pad_w dilation = dilation dilation override_quantized_engine qengine qconv_fn = qF conv d conv_fn = F conv d _test_conv_api_impl qconv_fn conv_fn batch_size in_channels_per_group input_feature_map_size out_channels_per_group groups kernel_size stride padding dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias use_channelwise given batch_size=st integers in_channels_per_group=st sampled_from D=st integers H=st integers W=st integers out_channels_per_group=st sampled_from groups=st integers kernel_d=st integers kernel_h=st integers kernel_w=st integers stride_d=st integers stride_h=st integers stride_w=st integers pad_d=st integers pad_h=st integers pad_w=st integers dilation=st integers X_scale=st floats X_zero_point=st integers W_scale=st lists st floats min_size= max_size= W_zero_point=st lists st integers - min_size= max_size= Y_scale=st floats Y_zero_point=st integers use_bias=st booleans use_channelwise=st booleans qengine=st sampled_from fbgemm test_conv d_api batch_size in_channels_per_group D H W out_channels_per_group groups kernel_d kernel_h kernel_w stride_d stride_h stride_w pad_d pad_h pad_w dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias use_channelwise qengine Tests correctness conv d function Currently conv d only supports FbGemm engine qengine torch backends quantized supported_engines input_feature_map_size = D H W kernel_size = kernel_d kernel_h kernel_w stride = stride_d stride_h stride_w padding = pad_d pad_h pad_w dilation = dilation dilation dilation override_quantized_engine qengine qconv_fn = qF conv d conv_fn = F conv d _test_conv_api_impl qconv_fn conv_fn batch_size in_channels_per_group input_feature_map_size out_channels_per_group groups kernel_size stride padding dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias use_channelwise given N=st integers C=st integers H=st integers H_out=st integers W=st integers W_out=st integers scale=st floats zero_point=st integers test_grid_sample N C H H_out W W_out scale zero_point X = torch rand N C H W X_q = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch quint grid = torch rand N H_out W_out out = F grid_sample X_q grid out_exp = torch quantize_per_tensor F grid_sample X grid scale=scale zero_point=zero_point dtype=torch quint np testing assert_array_almost_equal out int_repr numpy out_exp int_repr numpy decimal= __name__ == __main__ raise_on_run_directly test test_quantization py