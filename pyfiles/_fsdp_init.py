itertools logging typing Optional Union torch torch distributed dist torch nn nn torch _logging warning_once torch distributed device_mesh _get_device_handle torch distributed tensor DeviceMesh DTensor init_device_mesh torch utils _python_dispatch is_traceable_wrapper_subclass _fsdp_common _is_composable_with_fsdp FSDPMeshInfo HSDPMeshInfo _fsdp_state _get_module_fsdp_state logger = logging getLogger torch distributed fsdp fully_shard _get_post_forward_mesh_info reshard_after_forward Union bool int mesh_info FSDPMeshInfo - Optional FSDPMeshInfo shard_mesh_size = mesh_info shard_mesh_size isinstance reshard_after_forward bool int raise ValueError reshard_after_forward should bool int representing f group size reshard reshard_after_forward NOTE ` isinstance False int ` returns ` True ` isinstance reshard_after_forward bool isinstance reshard_after_forward int reshard_after_forward reshard_after_forward shard_mesh_size shard_mesh_size reshard_after_forward = raise ValueError If passing reshard_after_forward int should f factor shard_mesh_size reshard_after_forward reshard_after_forward == msg = reshard_after_forward= int means resharding parameters world size instead reshard_after_forward=True bool warning_once logger msg stacklevel= reshard_after_forward = False reshard_after_forward == shard_mesh_size reshard_after_forward = True post_forward_mesh_info = None reshard_after_forward True post_forward_mesh_info = mesh_info reshard_after_forward False int case For HSDP we can flatten two replicate dims into th dim post_forward_mesh_tensor = mesh_info mesh mesh view - reshard_after_forward post_forward_mesh = DeviceMesh mesh_info mesh device_type post_forward_mesh_tensor post_forward_mesh_info = HSDPMeshInfo post_forward_mesh shard_mesh_dim= replicate_mesh_dim= post_forward_mesh_info _init_default_fully_shard_mesh - DeviceMesh Default global CUDA mesh possible global CPU mesh dist distributed_c d is_initialized dist distributed_c d init_process_group default_pg = dist distributed_c d _get_default_group device = torch _C _get_accelerator mesh = init_device_mesh device type mesh_shape= default_pg size mesh _get_device_from_mesh mesh DeviceMesh - torch device mesh device_type == cpu torch device cpu device_handle = _get_device_handle mesh device_type torch device mesh device_type device_handle current_device _ignore_module module nn Module ignored_params set nn Parameter ignore_decision dict nn Module bool - bool Decide safe ignore module applying fully_shard module ignore_decision ignore_decision module len list module buffers recurse=False Cannot ignore module any buffer ignore_decision module = False False _ param module named_parameters recurse=False param ignored_params least one param ignored So module shouldn t ignore_decision module = False False Need consider descendants module child list module children ignore_child = _ignore_module child ignored_params ignore_decision ignore_child Cannot ignore module one its children ignored ignore_decision module = False False Safe ignore module ignore_decision module = True True _adjust_managed_modules modules list nn Module ignored_params set nn Parameter - list nn Module Adjust given list managed modules removing those all parameters ignored ignore_decision dict nn Module bool = new_modules = module modules ignored = _ignore_module module ignored_params ignore_decision ignored new_modules append module new_modules _get_managed_modules root_modules tuple nn Module ignored_params Optional set nn Parameter = None - list nn Module modules list nn Module = root_modules_set = set root_modules Track visisted modules avoid visiting shared modules multiple times visited_modules set nn Module = set dfs module nn Module - None Runs DFS collect managed modules recursing into modules non-composable API ` ` fully_shard ` ` already applied _is_composable_with_fsdp module module root_modules_set _get_module_fsdp_state module None nested ` fully_shard ` module visited_modules add module submodule module children submodule visited_modules dfs submodule modules append module root_module root_modules dfs root_module ignored_params None modules adjusted_modules = _adjust_managed_modules modules ignored_params adjusted_modules _verify_managed_param name str param nn Parameter - None Verify parameter accepted fully_shard The only restriction now parameter cannot scalar tensor param numel == since we need least one dim shard len param shape == raise ValueError fully_shard doesn t support scalar parameters f Change name D tensor numel equal _get_managed_states modules list nn Module ignored_params Optional set nn Parameter = None - tuple list nn Parameter list torch Tensor params list nn Parameter = buffers list torch Tensor = Track visited parameters buffers avoid visiting shared parameters buffers multiple times visited_params set nn Parameter = set visited_buffers set torch Tensor = set ignored_params None ignored_params = set module modules name param module named_parameters recurse=False param ignored_params do include ignored parameters continue param visited_params _verify_managed_param name param params append param visited_params add param buffer module buffers recurse=False buffer visited_buffers buffers append buffer visited_buffers add buffer params buffers _move_states_to_device params list nn Parameter buffers list torch Tensor device torch device - None We have FSDP move states device simpler faster initialization since FSDP almost always uses CUDA training We move parameters buffers rather than modules since modules support ignoring parameters buffers future Follow logic ` nn Module _apply ` pyrefly ignore bad-argument-type tensor itertools chain params buffers tensor device == device tensor device type == meta Keep meta-device tensors meta device deferred init continue isinstance tensor DTensor dtensor_mesh_type = tensor device_mesh device_type = device type raise ValueError Requires DTensor have mesh same type FSDP mesh f got dtensor_mesh_type DTensor device type FSDP raise AssertionError f Expects DTensor moved dtensor_mesh_type got tensor device tensor_ = tensor is_traceable_wrapper_subclass tensor_ torch no_grad avoid autograd increasing C++ refcount tensor_on_device = nn Parameter tensor device torch utils swap_tensors tensor tensor_on_device tensor data = tensor device