This provides abstract which parametrizes over output code concept Inductor Intuitively represents compiled callable which Inductor produces which you can call get optimized code However callable has some other capabilities - It serializable so you can save load product disk without having do compilation again - When using remote cache addressable so you can save just key which you can use load product remote cache later This abstract because we have several different implementations serialized format - Python wrapper default - AOTInductor produces ABI stable binaries which work across PyTorch versions __future__ annotations dataclasses logging os functools partial typing Any Callable Optional TYPE_CHECKING Union typing_extensions TypeAlias torch torch _dynamo utils counters get_runtime_metrics_context torch _inductor cudagraph_utils BoxedDeviceIndex CudagraphCachedInfo CudagraphMetadata get_partition_cudagraph_metadata get_placeholder_info log_cudagraph_skip_and_bump_counter torch _inductor freezing_utils has_frozen_params is_frozen_param torch _inductor utils _unstable_customized_partition_wrapper align_inputs_from_check_idxs BoxedBool CUDAGraphWrapperMetadata GraphPartitionMap InputType output_node set_tracing_context_output_strides torch autograd profiler record_function torch utils _ordered_set OrderedSet config runtime autotune_cache AutotuneCacheBundler TYPE_CHECKING collections Counter collections abc Sequence torch _inductor metrics torch _inductor graph GraphLowering torch _library fake_class_registry FakeScriptObject torch export pt _archive _package_weights Weights compile_fx _CompileFxKwargs triton_bundler TritonBundle log = logging getLogger __name__ dataclasses dataclass OutputCode TODO Remove underscores here None output remote cacheable _fx_graph_cache_key Optional str = dataclasses field default=None init=False _fx_graph_cache_debug_lines Optional list str = dataclasses field default=None init=False How long took compile OutputCode end end _time_taken_ns Optional int = dataclasses field default=None init=False __call__ inputs Sequence Any - Any raise NotImplementedError type prepare_for_serialization - None raise NotImplementedError type post_compile example_inputs Sequence InputType constants CompiledFxGraphConstants graph_kwargs _CompileFxKwargs - None raise NotImplementedError type TODO Get rid set_triton_bundle triton_bundle Any - None raise NotImplementedError type _StrideExprStr TypeAlias = str copy_ fails when trying write tensors memory overlap expanded dimensions dimension which used have size - we can select one element dimension write achieve writing all values dimension input tensor get_expanded_dims t torch Tensor - list int isinstance t torch Tensor pyrefly ignore bad-return None i i range t ndim t stride i == t size i = index_expanded_dims t torch Tensor expanded_dims list int - torch Tensor expanded_dim expanded_dims t = torch ops aten slice t expanded_dim t complex_memory_overlap t torch Tensor - bool config always_complex_memory_overlap_TESTING_ONLY True torch _debug_has_internal_overlap thinks tensor potentially has memory overlap internally let s dig deeper find out whether s true Call squeeze so dimension size does cause false positive t = index_expanded_dims t get_expanded_dims t squeeze torch _debug_has_internal_overlap t = strides = t stride sizes = t shape indices = list range len strides indices = x _ x sorted zip strides indices i range len strides prev_stride = i == strides indices i - prev_size = i == sizes indices i - strides indices i prev_stride prev_size True False maybe_handle_backward_generation compiled_graph CompiledFxGraph boxed_forward_device_index Optional BoxedDeviceIndex - None assert compiled_graph current_callable None is_backward = compiled_graph fx_kwargs is_backward See Backward Generation Handling cudagraph d forward set device we need let cudagraph manager know we we running backward even we will run cudagraphs is_backward config triton cudagraph_trees assert boxed_forward_device_index None assert boxed_forward_device_index value None compiled_graph_callable = compiled_graph current_callable manager = torch _inductor cudagraph_trees get_manager boxed_forward_device_index value create_if_none_exists=False should already exist forward assert manager None compiled_artifact new_inputs list Any - Callable Any manager set_to_running_backward type ignore union-attr compiled_graph_callable new_inputs compiled_graph current_callable = compiled_artifact prepare_cudagraph_post_compile compiled_graph CompiledFxGraph example_inputs Sequence InputType boxed_forward_device_index Optional BoxedDeviceIndex - None config triton cudagraph_trees Force specialize all inputs so CUDA graphs will work t example_inputs isinstance t torch SymInt int t guard is_inference = compiled_graph fx_kwargs is_inference is_backward = compiled_graph fx_kwargs is_backward boxed_forward_device_index None is_inference is_backward boxed_forward_device_index set next iter compiled_graph device_idxs cudagraph_post_compile example_inputs Sequence InputType compiled_graph CompiledFxGraph cudagraphs BoxedBool constants dict str torch Tensor boxed_forward_device_index Optional BoxedDeviceIndex - None Checks any reasons run cudagraphs then runs compiled_graph Mutates ` compiled_graph current_callable ` ` cudagraphs ` assert compiled_graph current_callable None assert compiled_graph cudagraph_info None cached_info = compiled_graph cudagraph_info cudagraph_fail_reasons = cached_info cudagraph_fail_reasons is_inference = compiled_graph fx_kwargs is_inference is_backward = compiled_graph fx_kwargs is_backward cudagraph_fail_reasons fx_kwargs = compiled_graph fx_kwargs static_input_idxs = fx_kwargs static_input_idxs placeholders = cached_info placeholders stack_traces = cached_info stack_traces prepare_cudagraph_post_compile compiled_graph example_inputs boxed_forward_device_index compile_fx cudagraphify current_callable = compiled_graph current_callable assert current_callable None compiled_graph current_callable = cudagraphify current_callable static_input_idxs=static_input_idxs device_index=next iter compiled_graph device_idxs stack_traces=stack_traces is_backward=is_backward is_inference=is_inference constants=tuple constants values placeholders=placeholders mutated_input_idxs=tuple compiled_graph mutated_input_idxs BoxedBool disable cudagraphs maybe_handle_backward_generation compiled_graph boxed_forward_device_index cuda compiled_graph device_types prefer better disable_cudagraphs_reason bc stack trace TODO migrate all disable reasons stack trace refactor compiled_graph disabled_cudagraphs_reason log_cudagraph_skip_and_bump_counter compiled_graph disabled_cudagraphs_reason log_cudagraph_skip_and_bump_counter f skipping cudagraphs due cudagraph_fail_reasons cudagraph_partition_post_compile example_inputs Sequence InputType compiled_graph CompiledFxGraph cudagraphs BoxedBool constants dict str torch Tensor boxed_forward_device_index Optional BoxedDeviceIndex - None Cudagraphify each partition functions which first prepares necessary metadata then applies cudagraphify function each partition Assuming all partition functions cudagraphified share same order ` compiled_graph partition_maps ` See Note Graph Partition Map CUDAGraph assert compiled_graph cudagraph_info None cudagraph_fail_reasons = compiled_graph cudagraph_info cudagraph_fail_reasons cudagraph_fail_reasons compiled_graph partition_maps None len compiled_graph partition_maps == cudagraphify called there no partitions BoxedBool disable cudagraphs maybe_handle_backward_generation compiled_graph boxed_forward_device_index compile_fx cudagraphify assert compiled_graph current_callable None assert compiled_graph recursively_apply_fns None is_inference = compiled_graph fx_kwargs is_inference is_backward = compiled_graph fx_kwargs is_backward static_input_idxs = OrderedSet compiled_graph fx_kwargs static_input_idxs mutated_input_idxs = compiled_graph mutated_input_idxs device_index = next iter compiled_graph device_idxs graph_metadata = CudagraphMetadata compiled_graph cudagraph_info placeholders static_input_idxs mutated_input_idxs compiled_graph cudagraph_info stack_traces constants prepare_cudagraph_post_compile compiled_graph example_inputs boxed_forward_device_index cudagraphify each partition function assuming every graph partition function cudagraphable Non-cudagraphable ops e g cpu ops inlined into ` call ` function included partition functions cudagraphify_fns = partition_map compiled_graph partition_maps partition_metadata = get_partition_cudagraph_metadata partition_map graph_metadata cudagraphify_fn = partial cudagraphify static_input_idxs=tuple partition_metadata static_input_idxs device_index=device_index stack_traces=partition_metadata stack_traces is_backward=is_backward is_inference=is_inference constants=tuple partition_metadata constants values placeholders=partition_metadata placeholders mutated_input_idxs=tuple partition_metadata mutated_input_idxs cudagraphify_fns append cudagraphify_fn compiled_graph recursively_apply_fns cudagraphify_fns maybe_realign_inputs ran_cudagraphs BoxedBool compiled_graph CompiledFxGraph inputs_to_check Sequence int mutated_inputs_idxs OrderedSet int - None Realigns input strides inputs_to_check we didn t end up running cudagraphs Mutates ` compiled_graph current_callable ` cudagraphs run Otherwise does nothing ran_cudagraphs assert compiled_graph current_callable None new_callable = align_inputs_from_check_idxs compiled_graph current_callable inputs_to_check mutated_inputs_idxs new_callable compiled_graph current_callable compiled_graph current_callable = new_callable CompiledFxGraphConstants Wrapper unwraps constants compiled fx graph This version only supports directly grabbing saved constants off CompiledFxGraph With freezing FxGraphCache doesn t store constants input GraphModule gets AOTAutograd Instead saves just names those constants grabs constant values directly graph module passed runtime Thing we don t always have graph module available runtime hence existence its CompiledFxGraphConstantsWithGm counterpart To support freezing FXGraphCache gets passed CompiledFxGraphConstantsWithGm during post compile Otherwise CompiledFxGraphConstants supports basic case loading value constants directly off original saved object unwrap g CompiledFxGraph - dict str torch Tensor assert g constants None g constants CompiledFxGraphConstantsWithGm CompiledFxGraphConstants This version CompiledFxGraphConstants instead grabbing constants directly saved CompiledFxGraphs will just grab their names Then takes second GraphModule grab corresponding constant values out This necessary supporting freezing FxGraphCache __init__ gm torch fx GraphModule - None gm = gm unwrap g CompiledFxGraph - dict str torch Tensor frozen_params = name getattr gm orig_name name orig_name g frozen_param_names items constants = g constants constants frozen_params dataclasses dataclass CompiledFxGraph OutputCode Class holding compiled FX graph This object serialized disk support FxGraph caching current_callable Optional Callable Any recursively_apply_fns Optional Callable Any compiled_fn_runner Optional Any cache_key str source_code str = dataclasses field repr=False Do display source_code runnable_graph_str str = dataclasses field repr=False Do display graph inductor_post_grad_graph_str str = dataclasses field repr=False Do display graph cache_linemap Optional list tuple int str device_types OrderedSet str device_idxs OrderedSet int mutated_inputs OrderedSet str mutated_input_idxs OrderedSet int constants Optional dict str torch Tensor frozen_param_names dict str str torchbind_constants dict str torch _C ScriptObject &#124; FakeScriptObject output_strides Optional list Optional tuple _StrideExprStr disabled_cudagraphs_reason Optional str metrics_deltas metrics CachedMetricsDeltas counter_deltas Counter str This string representation expression we serialize object so guards can evaluated different context order verify validity serving cached fx graph The expression must generated ShapeEnv produce_guards_expression guards_expr Optional str inductor_provenance_mapping_str Optional str inductor_provenance_stack_traces_str Optional str cudagraph_info Optional CudagraphCachedInfo partition_maps Optional list GraphPartitionMap fx_kwargs _CompileFxKwargs inputs_to_check Sequence int _boxed_call Optional bool = None _triton_bundle Optional TritonBundle = None __init__ current_callable Optional Callable Any graph GraphLowering gm torch fx GraphModule output_strides list Optional tuple _StrideExprStr disabled_cudagraphs_reason Optional str metrics_deltas metrics CachedMetricsDeltas counter_deltas Counter str cudagraphs BoxedBool example_inputs Sequence InputType static_input_idxs Sequence int fx_kwargs _CompileFxKwargs inputs_to_check Sequence int runnable_graph_str str inductor_post_grad_graph_str str compiled_fn_runner Optional Any = None inductor_provenance_mapping_str Optional str = None inductor_provenance_stack_traces_str Optional str = None - None current_callable = current_callable compiled_fn_runner = compiled_fn_runner recursively_apply_fns = compiled_fn_runner recursively_apply_fns compiled_fn_runner None None cache_key = graph cache_key graph cache_path open graph cache_path f source_code = f read runnable_graph_str = runnable_graph_str inductor_post_grad_graph_str = inductor_post_grad_graph_str inductor_provenance_mapping_str = inductor_provenance_mapping_str inductor_provenance_stack_traces_str = inductor_provenance_stack_traces_str cache_linemap = graph cache_linemap TODO - ordered set device_types = OrderedSet graph device_types device_idxs = OrderedSet graph device_idxs mutated_inputs = OrderedSet graph mutated_inputs mutated_input_idxs = OrderedSet graph mutated_input_idxs We store constant attributes cache entry re-attach them module created PyCodeCache load_by_key_path In case graph has frozen parameters we save mapping attribute names GraphLowering original name attribute GraphModule When we create module cache entry we then look up constants current GraphModule This scheme allows us support caching freezing has_frozen_params gm constants = graph constants frozen_param_names = constants = frozen_param_names = k v graph constants items is_frozen_param v frozen_param_names k = graph allocated_constant_name k constants k = v torchbind_constants = graph torchbind_constants output_strides = output_strides disabled_cudagraphs_reason = disabled_cudagraphs_reason metrics_deltas = metrics_deltas counter_deltas = counter_deltas guards_expr = None cudagraph_info = None partition_maps = graph partition_maps fx_kwargs = inputs_to_check = cudagraph_info = None cudagraphs check cudagraph disabling reasons inductor lowering disabled_cudagraphs_reason cuda device_types log_cudagraph_skip_and_bump_counter f skipping cudagraphs due disabled_cudagraphs_reason counters inductor cudagraph_skips += BoxedBool disable cudagraphs complex_memory_overlap_inputs = any complex_memory_overlap t t example_inputs isinstance t torch Tensor config triton cudagraph_support_input_mutation Skip supports cudagraph-managed tensors torch _inductor cudagraph_utils check_for_mutation_ignore_cuda_graph_managed_tensor has_mutation_str = check_for_mutation_ignore_cuda_graph_managed_tensor gm mutated_inputs mutated_input_idxs static_input_idxs has_mutation = has_mutation_str None has_mutation disabled_cudagraphs_reason = has_mutation_str Check mutation later support cudagraph-managed tensors has_mutation = None cudagraph_tests = has_mutation mutated inputs complex_memory_overlap_inputs complex memory overlap all isinstance t torch Tensor torch SymInt torch Generator t example_inputs non-Tensor inputs output = output_node gm output args tuple first argument assert len output args == stack_traces = arg stack_trace isinstance arg torch fx node Node None arg output args type ignore union-attr cudagraph_fail_reasons = s b s cudagraph_tests b placeholders = tuple get_placeholder_info gm graph cudagraph_info = CudagraphCachedInfo placeholders stack_traces cudagraph_fail_reasons cudagraph_info = cudagraph_info inputs_to_check = inputs_to_check fx_kwargs = fx_kwargs aot autograd needs know pass inputs list _boxed_call = True __del__ - None compiled_fn_runner None For torch _inductor config graph_partition = True compiled_fn_runner partitions hold cudagraphified functions which prevents deallocation When CompiledFxGraph deleted compiled_fn_runner will called future so we should also delete these partitions del compiled_fn_runner partitions __call__ inputs Sequence Any - Any assert current_callable None torch _inductor debug RECORD_GRAPH_EXECUTION torch _inductor debug GRAPH_EXECUTION_ORDER None graph_id = fx_kwargs get graph_id compile_id = torch _inductor debug GRAPH_COMPILE_IDS get graph_id graph_id None torch _inductor debug GRAPH_COMPILE_IDS None None torch _inductor debug GRAPH_EXECUTION_ORDER append compile_id compile_id try Checking profiler directly faster than nullcontext torch autograd profiler _is_profiler_enabled record_function f ## Call CompiledFxGraph _fx_graph_cache_key ## current_callable inputs current_callable inputs finally get_runtime_metrics_context finish AutotuneCacheBundler end_compile post_compile example_inputs Sequence InputType constants CompiledFxGraphConstants graph_kwargs _CompileFxKwargs - None Run set post processing steps after loading cache These involve - Setting tracing context output strides - Running cudagraphs enabled - Realigning inputs This runs whether we have cache hit always runs directly after we get CompiledFxGraph The results function saved cache itself config graph_partition _unstable_customized_partition_wrapper wrapper Mechanically apply user-specified cudagraph wrappers without modification assert recursively_apply_fns None assert compiled_fn_runner None num_partitions = len compiled_fn_runner partitions wrapper_metadatas = CUDAGraphWrapperMetadata num_partitions i i range num_partitions customized_wrapper = _unstable_customized_partition_wrapper wrapper customized_wrappers_with_metadata = lambda f m=metadata customized_wrapper f m metadata wrapper_metadatas recursively_apply_fns customized_wrappers_with_metadata set_tracing_context_output_strides example_inputs assert graph_kwargs cudagraphs None assert graph_kwargs is_backward None is_backward = graph_kwargs is_backward cudagraphs BoxedBool = graph_kwargs cudagraphs cudagraphs It s possible cudagraphs enabled disabled during previous compilation we re loading cache If so we need disable new process too disabled_cudagraphs_reason cuda device_types log_cudagraph_skip_and_bump_counter f skipping cudagraphs due disabled_cudagraphs_reason counters inductor cudagraph_skips += BoxedBool disable cudagraphs is_backward assert boxed_forward_device_index graph_kwargs boxed_forward_device_index = graph_kwargs boxed_forward_device_index On forward we don t know whether boxed_forward_device_index set yet boxed_forward_device_index = graph_kwargs get boxed_forward_device_index None config graph_partition graph_partition=True we skip some cudagraph checks s supported partition So we have use cudagraph_partition_post_compile cudagraph_partition_post_compile example_inputs cudagraphs constants unwrap boxed_forward_device_index cudagraph_post_compile example_inputs cudagraphs constants unwrap boxed_forward_device_index inputs_to_check = inputs_to_check cudagraphs could have been disabled earlier conditions so we still need realign inputs happens maybe_realign_inputs cudagraphs inputs_to_check mutated_input_idxs set_triton_bundle triton_bundle Any - None _triton_bundle = triton_bundle prepare_for_serialization - None We can t really serialize callables may C++ Triton etc so we serialize their PyCodeCache disk cache location instead TODO This could better we re ever able serialize compiled models disk current_callable = None recursively_apply_fns = None compiled_fn_runner = None write_to_disk - str torch _dynamo utils counters torch _inductor codecache get_path write_atomic See _save_graph we don t store callable cache entry so recreate here PyCodeCache disk cache artifact_path = get_path cache_key py code = source_code os path exists artifact_path counters inductor fxgraph_lookup_write_file += write_atomic artifact_path code make_dirs=True artifact_path after_deserialization constants CompiledFxGraphConstants - str torch _dynamo utils dynamo_timed torch _inductor codecache PyCodeCache artifact_path = write_to_disk try dynamo_timed PyCodeCache load_by_key_path log_pt _compile_event=True code_cache = PyCodeCache load_by_key_path cache_key artifact_path cache_linemap constants unwrap current_callable = code_cache call recursively_apply_fns = getattr code_cache recursively_apply_fns None compiled_fn_runner = getattr code_cache runner None except OSError log error Failed load artifact s artifact_path raise artifact_path dataclasses dataclass CompiledAOTI OutputCode Class holding AOTInductor compiled so filename Union str list Union str Weights torch fx GraphModule __call__ inputs Sequence Any - Any raise NotImplementedError NYI post_compile example_inputs Sequence InputType constants CompiledFxGraphConstants graph_kwargs _CompileFxKwargs - None pass prepare_for_serialization - None pass set_triton_bundle triton_bundle Any - None pass dataclasses dataclass MockFXGraphCacheOutput OutputCode gm Any = None __post_init__ - None _boxed_call = True post_compile example_inputs Sequence InputType constants CompiledFxGraphConstants graph_kwargs _CompileFxKwargs - None pass __call__ inputs Sequence Any - Any gm inputs set_triton_bundle triton_bundle Any - None pass dataclasses dataclass RegionalOutputCode OutputCode OutputCode regional inductor compilation results Regional inductor returns torch fx GraphModule contains both compiled regions via standalone_compile eager regions This needs special serialization using GraphPickler instead standard pickle The serialization strategy stores GraphModule bytes using GraphPickler dumps which handles FakeTensors AOTCompiledArtifacts other special objects standard pickle cannot handle The serialized graph module bytes using GraphPickler _serialized_graph_module Optional bytes = dataclasses field default=None init=False The actual graph module cleared during serialization _graph_module Optional torch fx GraphModule = dataclasses field default=None init=False __init__ graph_module torch fx GraphModule Args graph_module The torch fx GraphModule returned regional_inductor super __init__ _graph_module = graph_module _serialized_graph_module = None __call__ inputs Sequence Any - Any Execute regional compiled graph _graph_module None raise RuntimeError RegionalOutputCode has no graph module loaded Did you forget call post_compile _graph_module inputs post_compile example_inputs Sequence InputType constants CompiledFxGraphConstants graph_kwargs _CompileFxKwargs - None Post-compile processing regional inductor This deserializes GraphModule bytes using GraphPickler extracting fake_mode example_inputs _graph_module None assert _serialized_graph_module None Get fake mode example inputs torch _guards detect_fake_mode fake_mode = detect_fake_mode example_inputs fake_mode None raise RuntimeError Could detect fake mode example inputs Regional inductor requires fake mode deserialization Deserialize graph module torch fx _graph_pickler GraphPickler gm = GraphPickler loads _serialized_graph_module fake_mode assert isinstance gm torch fx GraphModule gm recompile _graph_module = gm set_triton_bundle triton_bundle Any - None Regional inductor doesn t use triton bundles directly prepare_for_serialization - None Prepare serialization converting GraphModule bytes This uses GraphPickler serialize graph module since contains special objects like FakeTensors AOTCompiledArtifacts need custom pickling _graph_module None torch fx _graph_pickler GraphPickler _serialized_graph_module = GraphPickler dumps _graph_module Clear graph module avoid pickling standard pickle _graph_module = None