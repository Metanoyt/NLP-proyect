mypy allow-untyped-defs hashlib json coremltools ct type ignore coremltools converters mil input_types TensorType type ignore coremltools converters mil mil types type ignore coremltools models neural_network quantization_utils type ignore torch CT_METADATA_VERSION = com github apple coremltools version CT_METADATA_SOURCE = com github apple coremltools source ScalarType Float = Double = Int = Long = Undefined = Supported Tensor types coremltools https github com apple coremltools blob main coremltools converters mil frontend torch converter py#L torch_to_mil_types = ScalarType Float types fp ScalarType Double types fp ScalarType Int types int ScalarType Long types int CoreMLComputeUnit CPU = cpuOnly CPUAndGPU = cpuAndGPU ALL = all CoreMLQuantizationMode LINEAR = linear LINEAR_SYMMETRIC = linear_symmetric NONE = none TensorSpec shape dtype=ScalarType Float shape dtype CompileSpec inputs outputs backend=CoreMLComputeUnit CPU allow_low_precision=True quantization_mode=CoreMLQuantizationMode NONE mlmodel_export_path=None convert_to=None inputs outputs backend allow_low_precision quantization_mode mlmodel_export_path convert_to _check_enumerated_shape shape s shape isinstance s list tuple False True _convert_to_mil_type shape dtype name str mil_shape = shape _check_enumerated_shape shape mil_shape = ct EnumeratedShapes shape ml_type = TensorType shape=mil_shape dtype=torch_to_mil_types dtype ml_type name = name ml_type preprocess script_module torch _C ScriptObject compile_spec dict str tuple spec = compile_spec forward input_specs output_specs backend allow_low_precision quantization_mode mlmodel_export_path convert_to = spec mil_inputs = inputs = index input enumerate input_specs shape dtype = input name = input_ + str index inputs append name str dtype str shape ml_type = _convert_to_mil_type shape dtype name mil_inputs append ml_type model = torch jit RecursiveScriptModule _construct script_module lambda x None mlmodel = ct convert model inputs=mil_inputs convert_to=convert_to quantization_mode = CoreMLQuantizationMode NONE quant_model_spec = quantization_utils quantize_weights mlmodel nbits= quantization_mode=quantization_mode mlmodel = ct models MLModel quant_model_spec spec = mlmodel get_spec assert len spec description output == len output_specs type ignore attr-defined outputs = index output enumerate output_specs shape dtype = output name = spec description output index name type ignore attr-defined outputs append name str dtype str shape mlmodel = ct models model MLModel spec print mlmodel mlmodel_export_path None print f Saving CoreML mlmodel file mlmodel_export_path mlmodel save mlmodel_export_path config = spec_ver str spec specificationVersion type ignore attr-defined backend backend allow_low_precision str allow_low_precision metadata = coremltool_ver mlmodel user_defined_metadata CT_METADATA_VERSION torch_ver mlmodel user_defined_metadata CT_METADATA_SOURCE coreml_compile_spec = inputs inputs outputs outputs config config metadata metadata mlmodel = spec SerializeToString type ignore attr-defined model mlmodel hash str hashlib sha mlmodel hexdigest extra json dumps coreml_compile_spec