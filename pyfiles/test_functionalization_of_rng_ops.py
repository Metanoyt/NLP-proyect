Owner s oncall pt functools sys unittest unittest mock patch torch torch utils checkpoint functorch compile aot_function min_cut_rematerialization_partition nop torch testing _internal common_device_type dtypes instantiate_device_type_tests torch testing _internal common_utils IS_CI IS_WINDOWS run_tests TestCase IS_WINDOWS IS_CI sys stderr write torch compile supported windows __name__ == __main__ sys exit raise unittest SkipTest torch compile supported windows count_philox_rand gm args freq assert node target node gm graph nodes count torch ops rngprims philox_rand default == freq gm TestFunctionalizationRngOps TestCase dtypes torch float patch object torch _functorch config functionalize_rng_ops True test_rand_like dtype device fn x = torch rand_like x x = torch rand_like x x = torch rand device=device dtype=dtype seed range torch cuda manual_seed seed ref = fn x torch cuda manual_seed seed aot_fn = aot_function fn functools partial count_philox_rand freq= res = aot_fn x assertEqual ref res dtypes torch float patch object torch _functorch config functionalize_rng_ops True test_rand_like_dynamic dtype device fn x = torch rand_like x x = torch rand_like x seed range shape = seed seed x = torch rand shape device=device dtype=dtype torch cuda manual_seed seed ref = fn x torch cuda manual_seed seed opt_fn = torch compile fn backend= aot_eager dynamic=True res = opt_fn x assertEqual ref res dtypes torch float patch object torch _functorch config functionalize_rng_ops True test_rand_like_dynamic_bwd dtype device fn x = torch rand_like x x = torch rand_like x seed range shape = seed seed x = torch rand shape device=device dtype=dtype requires_grad=True torch cuda manual_seed seed ref = fn x ref sum backward torch cuda manual_seed seed opt_fn = torch compile fn backend= aot_eager dynamic=True res = opt_fn x res sum backward assertEqual ref res dtypes torch float patch object torch _functorch config functionalize_rng_ops True test_rand dtype device shape = fn x = torch rand shape device=device dtype=dtype x = torch rand shape device=device dtype=dtype x = torch rand shape device=device dtype=dtype seed range torch cuda manual_seed seed ref = fn x torch cuda manual_seed seed aot_fn = aot_function fn functools partial count_philox_rand freq= res = aot_fn x assertEqual ref res dtypes torch float patch object torch _functorch config functionalize_rng_ops True test_autograd_function dtype device shape = Custom torch autograd Function staticmethod forward ctx x ctx save_for_backward x = torch rand_like x x = torch rand_like x staticmethod backward ctx grad_out x = ctx saved_tensors grad_out torch rand_like grad_out torch cos x custom = Custom apply x = torch rand shape device=device dtype=dtype requires_grad=True x_clone = x detach clone requires_grad_ True torch cuda manual_seed ref = custom x ref sum backward torch cuda manual_seed fwd_compiler = functools partial count_philox_rand freq= bwd_compiler = functools partial count_philox_rand freq= aot_custom = aot_function custom fwd_compiler bwd_compiler res = aot_custom x_clone res sum backward assertEqual ref res assertEqual x grad x_clone grad dtypes torch float patch object torch _functorch config functionalize_rng_ops True test_multiple_subgraphs dtype device Checks rng state maintained when there multiple aot traced graphs shape = CustomOp torch autograd Function staticmethod forward ctx x ctx save_for_backward x = torch rand_like x x = torch rand_like x staticmethod backward ctx grad_out x = ctx saved_tensors grad_out torch rand_like grad_out torch cos x CustomOp torch autograd Function staticmethod forward ctx x ctx save_for_backward x = torch rand_like x x staticmethod backward ctx grad_out x = ctx saved_tensors grad_out torch rand_like grad_out torch rand_like x custom_op = CustomOp apply custom_op = CustomOp apply fn x = custom_op x b = sin custom_op b fwd_compiler = functools partial count_philox_rand freq= bwd_compiler = functools partial count_philox_rand freq= aot_custom_op = aot_function custom_op fwd_compiler bwd_compiler fwd_compiler = functools partial count_philox_rand freq= bwd_compiler = functools partial count_philox_rand freq= aot_custom_op = aot_function custom_op fwd_compiler bwd_compiler aot_fn x = aot_custom_op x b = sin aot_custom_op b seed range torch cuda manual_seed seed x = torch rand shape device=device dtype=dtype requires_grad=True x_clone = x detach clone requires_grad_ True torch cuda manual_seed seed ref = fn x ref sum backward torch cuda manual_seed seed res = aot_fn x_clone res sum backward assertEqual ref res assertEqual x grad x_clone grad dtypes torch float patch object torch _functorch config functionalize_rng_ops True test_set_get_rng_state dtype device fn x = torch rand_like x x state = torch cuda get_rng_state = torch rand_like x torch cuda set_rng_state state = torch rand_like x x = torch rand device=device dtype=dtype seed range torch cuda manual_seed seed ref = fn x torch cuda manual_seed seed fwd_compiler = functools partial count_philox_rand freq= aot_fn = aot_function fn fwd_compiler res = aot_fn x assertEqual ref res dtypes torch float patch object torch _functorch config functionalize_rng_ops True test_min_cut_partitioner dtype device Checks calling convention maintained shape = fn x = torch rand_like x x = torch rand_like x = torch sin = torch sin = torch sin x = torch rand shape device=device dtype=dtype requires_grad=True x_clone = x detach clone requires_grad_ True torch cuda manual_seed ref = fn x ref sum backward torch cuda manual_seed fwd_compiler = functools partial count_philox_rand freq= bwd_compiler = functools partial count_philox_rand freq= aot_custom = aot_function fn fwd_compiler bwd_compiler partition_fn=min_cut_rematerialization_partition aot_custom = aot_function fn fwd_compiler bwd_compiler res = aot_custom x_clone res sum backward assertEqual ref res assertEqual x grad x_clone grad TODO - Dropout needs more work because offset calculation patch object torch _functorch config functionalize_rng_ops True dtypes torch float test_checkpoint dtype device g x y torch nn functional dropout x fn x y torch utils checkpoint checkpoint g x y use_reentrant=False x = torch rand device= cuda requires_grad=True x = torch ones device= cuda requires_grad=True y = torch rand device= cuda requires_grad=True torch cuda manual_seed fn x y With checkpointing we should recompute dropout bwd philox_rand passed fwd fwd_compiler = functools partial count_philox_rand freq= bwd_compiler = functools partial count_philox_rand freq= aot_fn = aot_function fn fwd_compiler bwd_compiler We can t check accuracy here because rand_like generated different rand numbers than dropout res = aot_fn x y res sum backward dtypes torch float patch object torch _functorch config functionalize_rng_ops True test_dropout_decomp dtype device fn x torch nn functional dropout x x x = torch rand device=device dtype=dtype Ensure decomp happening aot_fn = aot_function fn functools partial count_philox_rand freq= We can t check accuracy here because rand_like generated different rand numbers than dropout aot_fn x only_for = cuda instantiate_device_type_tests TestFunctionalizationRngOps globals only_for=only_for NegativeTest TestCase dtypes torch float patch object torch _functorch config functionalize_rng_ops True test_on_cpu dtype device fn x = torch rand_like x x = torch rand_like x x = torch rand device=device dtype=dtype aot_fn = aot_function fn nop assertRaises RuntimeError aot_fn x only_for = cpu instantiate_device_type_tests NegativeTest globals only_for=only_for __name__ == __main__ run_tests