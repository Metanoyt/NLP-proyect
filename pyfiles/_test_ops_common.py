mypy allow-untyped-defs builtins torch torch distributed _shard sharding_spec ChunkShardingSpec EnumerableShardingSpec ShardMetadata torch distributed _shard sharding_spec _internals get_chunked_dim_size get_split_size generate_chunk_sharding_specs_for_test sharding_dim ChunkShardingSpec dim=sharding_dim placements= rank cuda rank cuda rank cuda rank cuda Test different ordering Case ChunkShardingSpec dim=sharding_dim placements= rank cuda rank cuda rank cuda rank cuda Test different ordering Case ChunkShardingSpec dim=sharding_dim placements= rank cuda rank cuda rank cuda rank cuda generate_enumerable_sharding_specs_for_test EnumerableShardingSpec ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda generate_local_weight_sharding_params_for_test local_weight sharded_dim gpu_num spec rank Shard local weight based given spec so we can compare against one sharded tensor Args local_weight weight matrix sharded sharded_dim The dimension which we shard gpu_num number ranks spec sharding spec rank cuda process Returns start_pos start position sharded weight given rank chunk_size chunk size sharded weight given rank sharding_dim_size = local_weight size sharded_dim split_size = get_split_size sharding_dim_size gpu_num current_offsets = start_pos = current_offsets idx placement enumerate spec placements chunk_size = get_chunked_dim_size sharding_dim_size split_size idx rank == placement rank start_pos = current_offsets break current_offsets += chunk_size start_pos chunk_size clone_module_parameter module param_name Clone parameter given existing module Args module ` torch nn Module ` Module whose parameter needs cloned param_name str Name parameter ` ` module ` ` needs cloned Returns cloned tensor ` torch nn Parameter ` tensor = getattr module param_name torch nn Parameter tensor detach clone gen_binary_op_func python_op inplace=False src_lines = f lhs rhs torch python_op src_lines append f python_op lhs rhs \n inplace src_lines append f lhs python_op = rhs\n lhs\n src_lines append f lhs python_op rhs\n code_str = \n join src_lines g = torch torch builtins exec code_str g g f