Owner s module inductor contextlib os unittest unittest skipUnless numpy np sympy torch torch nn functional F torch nn torch _dynamo testing rand_strided torch _dynamo utils same torch _inductor config inductor_config ir metrics torch _inductor codegen triton TritonScheduling torch _inductor graph GraphLowering torch _inductor scheduler SchedulerNode torch _inductor test_case run_tests TestCase torch _inductor test_operators realize torch _inductor utils is_big_gpu run_and_get_code sympy_index_symbol torch _inductor virtualized ops V torch testing FileCheck torch testing _internal common_cuda PLATFORM_SUPPORTS_FP torch testing _internal common_utils instantiate_parametrized_tests parametrize torch testing _internal inductor_utils GPU_TYPE HAS_GPU torch utils _ordered_set OrderedSet torch utils _pytree tree_map torch utils _sympy functions FloorDiv ModularIndexing set so metrics appear torch _logging set_logs inductor_metrics=True DO_PERF_TEST = os environ get DO_PERF_TEST == HAS_GPU torch set_default_device GPU_TYPE MockScheduler available_buffer_names = staticmethod get_backend cls args TritonScheduling cls can_buffer_be_removed_through_fusion args kwargs False MockSchedulerTest TestCase _exit_stack = None classmethod setUpClass cls super setUpClass gm = torch fx symbolic_trace lambda graph = GraphLowering gm graph scheduler = MockScheduler cls _exit_stack = contextlib ExitStack cls _exit_stack enter_context V set_graph_handler graph classmethod tearDownClass cls super tearDownClass cls _exit_stack close inductor_config patch loop_ordering_after_fusion=True ImplDetailTest MockSchedulerTest staticmethod _get_snode_body_sym_prefix snode body = snode _body prefix = var body var_ranges prefix = str var break assert prefix prefix staticmethod _create_computed_buffer_ax sizes= strides=None Create ComputedBuffer x strides None strides = ir FlexibleLayout contiguous_strides sizes box_a = ir TensorBox create ir Buffer name= layout=ir FixedLayout torch device GPU_TYPE dtype=torch float size=sizes stride=strides box_a_loader = box_a make_loader inner_fn index box_a_loader index buf = ir Pointwise create device=box_a get_device dtype=box_a get_dtype inner_fn=inner_fn ranges=box_a get_size buf realize computed_buf = buf data data computed_buf decide_layout computed_buf test_reorder_twice This may happen practice we pick order when fusing A B Then we pick another order AB when we fusion C into E g happens BertForMaskedLM buf = _create_computed_buffer_ax snode = SchedulerNode V graph scheduler buf snode apply_new_loop_order prefix = _get_snode_body_sym_prefix snode assertTrue prefix == p snode apply_new_loop_order prefix = _get_snode_body_sym_prefix snode assertTrue prefix == p test_reorder_and_merge_loops sizes = strides = buf = _create_computed_buffer_ax sizes strides old_sizes old_body = buf simplify_and_reorder Make sure loop reordering happens here assertTrue tuple old_sizes == tuple reversed sizes f old_sizes= new_body = old_body merge_loops new_sizes = new_body sizes assertTrue tuple new_sizes == np prod sizes f new_sizes= test_merge_loops_invalidate_pw_dep_cache sizes = strides = buf = _create_computed_buffer_ax sizes strides snode = SchedulerNode V graph scheduler buf old_var_ranges = snode pointwise_read_writes var_ranges assertTrue len old_var_ranges == dimension merged snode merge_loops new_var_ranges = snode pointwise_read_writes var_ranges we cache pointwise_read_writes result scheduler node make sure new_var_ranges refreshed invalidating cache assertTrue len new_var_ranges == dimensions get merged test_reorder_modular_indexing There bug we wrongly map i dimension size when reordering loop cause ModularIndexing get optimized away no-op _create_computed_buffer inner_fn index i _ i i = index ops load primal i + i + ModularIndexing i buf = ir Pointwise create device=torch device GPU_TYPE dtype=torch float inner_fn=inner_fn ranges= buf realize cbuf = buf data data cbuf decide_layout cbuf buf = _create_computed_buffer _ body = buf simplify_and_reorder new_body = body reorder_iter_loops z z z z = sympy_index_symbol f p i i range assertEqual body var_ranges z z z z assertEqual body indexing_exprs index z + z + ModularIndexing z assertEqual new_body var_ranges z z z z assertEqual new_body indexing_exprs index z + z + ModularIndexing z inductor_config patch benchmark_kernel True loop_ordering_after_fusion True triton unique_kernel_names True LoopOrderingTest TestCase device = GPU_TYPE do_acc_test f args cast_fp =True expect = f args actual = torch compile f args cast_fp _cast x isinstance x torch Tensor x dtype torch float _e m torch float _e m fn x torch float x Workaround issue call allclose fp tensor triggers error RuntimeError mul_cuda implemented Float _e m fn expect = tree_map _cast expect actual = tree_map _cast actual assertTrue same expect actual tol= e- setUp super setUp metrics reset test_for_reordering_reindex ComputedBuffer iter_reoredering_reindex can cause some fusion opportunitiies being skipped In test case Inductor generates triton kernels before By removing ComputedBuffer iter_reoredering_reindex we can fuse those two kernels into single one f x y Add matmul since inductor may force layout output x sum dim=- + y A B = Make first dimension able merge purpose so ComputedBuffer iter_reoredering_reindex will updated x = rand_strided A A B B B A + device=GPU_TYPE y = torch randn A A do_acc_test f x y assertEqual metrics generated_kernel_count expected_num_bytes = expected_num_bytes += A A B + A A fused reduction expected_num_bytes += A A matmul expected_num_bytes = x itemsize assertEqual expected_num_bytes metrics num_bytes_accessed test_apbt_realize M = N = f x y There will kernels being generated without loop ordering after fusion https gist github com shunting df f de c c ac ed x = realize x y = realize y x + y x = torch randn M N y = torch randn N M t do_acc_test f x y assertEqual metrics generated_kernel_count test_sum_and_t N = f x x sum dim=- x t contiguous x = torch randn N N do_acc_test f x assertEqual metrics generated_kernel_count test_pw_outer_red f x x = realize x + x sum dim= make first dimension small so we don t split reduction x = torch randn do_acc_test f x assertEqual metrics generated_kernel_count test_pw_outer_red_ The pointwise kernel fused kernel f x x = realize x + x = realize x - x = realize x x sum dim= make first dimension small so we don t split reduction x = torch randn do_acc_test f x assertEqual metrics generated_kernel_count inductor_config patch split_reductions=False test_different_reduction_order We should reorder loops case Since reordering loops does help f x x sum dim= x sum dim= x = torch randn do_acc_test f x assertEqual metrics generated_kernel_count assertEqual metrics num_loop_reordering test_keep_fake_dep In model there fake dependencies StarDep between Scatter following mutation kernel computes gradients embedding tables When we do loop reordering mutation kernel we re-analyze node s dependencies But analysis result does contains those fake dependencies Have add them back manually V = hidden_size = max_seqlen = batch_size = Model nn Module __init__ super __init__ word_embeddings = nn Embedding V hidden_size position_embeddings = nn Embedding max_seqlen hidden_size layer_norm = nn LayerNorm hidden_size forward input_ids labels position_ids emb = word_embeddings input_ids + position_embeddings position_ids layer_norm emb m = Model torch compile f args m args sum backward input_ids = torch randint V batch_size max_seqlen labels = torch randint V batch_size max_seqlen position_ids = torch arange max_seqlen None Make sure line does raise exceptions If we miss fake dependencies after loop reordering we may get exception some buffer used before being defined f input_ids labels position_ids test_different_broadcast_shapes f x y c x + c y + c x = torch randn y = torch randn c = torch randn do_acc_test f x y c The two kernels fused due c broadcasted assertEqual metrics generated_kernel_count test_view Passing test relies we compare normalized MemoryDep Normlaization here means merging contiguous loops To make loop reordering work we don t merge loops when creating SchedulerNode Thus we need explicitly normalize MemoryDep when we check two MemeoryDep matches f x y = x sin x = realize x view x y x = torch randn do_acc_test f x assertEqual metrics generated_kernel_count unittest skipIf PLATFORM_SUPPORTS_FP FP requires H + MI + test_fp _cast_and_t This test repros able fuses issue https github com pytorch pytorch issues fp cast transpose f x scale x = x scale x = x clamp - E M _MAX_POS E M _MAX_POS x = x torch float _e m fn x_t = x t contiguous t x x_t x = torch randn dtype=torch bfloat scale = torch Tensor GPU_TYPE E M _MAX_POS = torch finfo torch float _e m fn max do_acc_test f x scale assertEqual metrics generated_kernel_count unittest skipIf PLATFORM_SUPPORTS_FP FP requires H + MI + test_fp _pattern_ This test repros fp fusion relation issue here https github com pytorch pytorch issues ref_dtype = torch bfloat M K = input_tensor = torch randn M K device=GPU_TYPE dtype=ref_dtype requires_grad=False scale = torch Tensor GPU_TYPE E M _MAX_POS = torch finfo torch float _e m fn max test_pattern tensor_x_inp scale_x tensor_x = tensor_x_inp scale_x tensor_x = tensor_x clamp min=- E M _MAX_POS max=E M _MAX_POS tensor_fp = tensor_x torch float _e m fn tensor_x_t = tensor_x_inp scale_x t tensor_x_t = tensor_x_t clamp min=- E M _MAX_POS max=E M _MAX_POS tensor_fp _t = tensor_x_t torch float _e m fn tensor_fp _t = tensor_fp _t contiguous t tensor_fp tensor_fp _t test_pattern = torch compile test_pattern tensor_fp tensor_fp _t = test_pattern input_tensor scale assertEqual metrics generated_kernel_count expected_numbytes = scale nbytes scalar expected_numbytes += input_tensor nbytes input expected_numbytes += tensor_fp nbytes + tensor_fp _t nbytes output assertEqual expected_numbytes metrics num_bytes_accessed test_outer_dimension_softmax This test repros able fuse problem outer dimension softmax reported here https github com pytorch pytorch issues Perf data h - without loop ordering after fusion ms - loop ordering after fusion ms This x speedup x = torch randn device=GPU_TYPE f x F softmax x dim= do_acc_test f x assertEqual metrics generated_kernel_count test_outer_dimension_sum_fuse_with_pw Test fusion outer dimension sum followed pointwise Perf data h - without loop ordering after fusion ms - loop ordering after fusion ms This x speedup x = torch randn device=GPU_TYPE f x x sum dim= keepdim=True + x do_acc_test f x assertEqual metrics generated_kernel_count DO_PERF_TEST triton testing do_bench optf = torch compile f print f ms= do_bench lambda optf x Disable split reduction make easier calculate expected number bytes accessed In case split reduction does help perf much inductor_config patch split_reductions=False test_fuse_reduction_with_tiled_pw f x y = torch sum torch sum x dim=- z = x z_t = z t contiguous t y z z_t use input sizes test perf DO_PERF_TEST M N = M N = x = torch randn M N device=GPU_TYPE actual = f x opt_f = torch compile f expected = opt_f x assertTrue same actual expected tol= e- We should fuse first sum two pointwise Overall we read x once all these three kernels write out buffers same size x This should sort optimal workload expected_numbytes = x nbytes A small amount extra memory access - store output first reduction - load input second reduction - store output second reduction expected_numbytes += M + x itemsize print expected_numbytes assertEqual expected_numbytes metrics num_bytes_accessed DO_PERF_TEST triton testing do_bench ms = do_bench lambda opt_f x print f ms= f inductor_config patch max_autotune True max_autotune_gemm_backends TRITON test_configs max_mm_configs skipUnless HAS_GPU is_big_gpu Need big gpu max-autotune test_interaction_with_triton_template Make sure dependency prefix TritonTempalate its prologue match torch compile f x y x expand y shape + y x = torch randn device=GPU_TYPE y = torch randn device=GPU_TYPE out code = run_and_get_code f x y well when benchmark_kernel flag we have one more run call benchmarking code FileCheck check call check_count run + int inductor_config benchmark_kernel exactly=True run code inductor_config patch max_autotune True max_autotune_gemm_backends TRITON test_configs max_mm_configs skipUnless HAS_GPU is_big_gpu Need big gpu max-autotune test_interaction_with_multi_template Skip MultiTemplateBuffer during loop reordering torch compile f x y x y x + N = x = torch randn N N device=GPU_TYPE dtype=torch bfloat y = torch randn N N device=GPU_TYPE dtype=torch bfloat out code = run_and_get_code f x y didn t fuse due small savings FileCheck check_count triton jit exactly=True run code test_fuse_with_scalar_shared_memory Make sure we can fuse two nodes sharing scalar before we can still do LOAF applied This really big deal But some tests rely less number kernels has some small benefits torch compile f x torch mean x x = torch randn device=GPU_TYPE out code = run_and_get_code f x FileCheck check_count triton jit exactly=True run code test_ dred_pw_ d_outer_red Test pattern follows We have d contiguous tensor m n k input do reduction k dimension get m n tensor do pointwise operation m n tensor realize computation do outer reduction output step m dimension Each these step generate kernel before fusion Without any loop reorder kernel kernel will get fused And kernel will separeate But we reorder loop kernel then kernel will get fused kernel And fused kernel- - can fused kernel The older version LOAF algorithm will do reorder case But there no real benefits There even some slight downsides original fusion without loop reordering more natural fusion kernel kernel may help precision when output kernel low precision By fusion kernel kernel pointwise operation will operate fp precision thanks fusion M N K = f x x = x sum dim=- x = x + can more complex like sigmoid other ops x x sum dim= x = torch randn M N K device=GPU_TYPE do_acc_test f x assertEqual metrics num_loop_reordering inductor_config patch triton unique_kernel_names True loop_ordering_after_fusion True triton max_tiles triton coalesce_tiling_analysis True instantiate_parametrized_tests MemoryCoalescingTest MockSchedulerTest Tests memory coalescing analysis specific tensor sizes device = GPU_TYPE _exit_stack = None setUp super setUp metrics reset _create_buffer name sizes Create buffer specified sizes strides = ir FlexibleLayout contiguous_strides sizes box = ir TensorBox create ir Buffer name=name layout=ir FixedLayout torch device device dtype=torch float size=sizes stride=strides box_loader = box make_loader inner_fn index box_loader index buf = ir Pointwise create device=box get_device dtype=box get_dtype inner_fn=inner_fn ranges=box get_size buf realize computed_buf = buf data data computed_buf decide_layout computed_buf _create_scheduler_node buf s = SchedulerNode V graph scheduler buf s min_order = s max_order = s parametrize inps test_inferred_splits inps Test memory coalescing analysis specified tensor sizes Using direct SchedulerNode creation sizes s s expected_size = inps Create buffers specified sizes buf = _create_buffer buffer s buf = _create_buffer buffer s Create scheduler nodes snode = _create_scheduler_node buf snode = _create_scheduler_node buf Create fused node fused_node = torch _inductor scheduler FusedSchedulerNode fuse snode snode torch _inductor tiling_utils fused_norm_read_writes = tiling_utils extract_normalized_read_writes fused_node var_ranges = fused_norm_read_writes var_ranges assertEqual list var_ranges values list expected_size test_remapped_reads torch _inductor tiling_utils fn nodes assert len nodes == fused_norm_read_writes = tiling_utils extract_normalized_read_writes nodes assertTrue len fused_norm_read_writes var_ranges == both reads remapped correctly FileCheck check n + n run repr fused_norm_read_writes reads keys FileCheck check n + n run repr fused_norm_read_writes reads keys nodes torch _inductor config patch _post_fusion_custom_pass=fn torch compile foo x y x + y foo torch rand device=GPU_TYPE torch rand device=GPU_TYPE T test_remapped_reads_split torch _inductor tiling_utils fn nodes assertTrue len nodes == fused_norm_read_writes = tiling_utils extract_normalized_read_writes nodes inp_node_reads = nodes get_nodes _body get_read_exprs node_ranges = nodes get_nodes _body var_ranges assertTrue len node_ranges == assertTrue next iter node_ranges values == var = next iter node_ranges keys r = FloorDiv var + ModularIndexing var assertTrue r inp_node_reads mapped reads assertTrue list fused_norm_read_writes var_ranges values == n n = list fused_norm_read_writes var_ranges keys translation above n + n assertTrue n + n fused_norm_read_writes reads keys nodes torch _inductor config patch _post_fusion_custom_pass=fn torch compile foo x y x + y contiguous flatten + torch ops _inductor_test realize y T + flatten foo torch rand device=GPU_TYPE torch rand device=GPU_TYPE T test_reduction_pointwise test one pw var one red var torch _inductor tiling_utils fn nodes assertTrue len nodes == fused_rw = tiling_utils extract_normalized_read_writes nodes i_vars r_vars = fused_rw index_vars fused_rw reduce_vars assertTrue len i_vars == assertTrue len r_vars == single write index var assertTrue fused_rw index_vars == next iter fused_rw writes keys write fused intermediary node should removed assertTrue len fused_rw writes == single read assertTrue len fused_rw reads == applied two bufs assertTrue len next iter fused_rw reads values == read should terms index + reduce var even though node pointwise assertTrue i_vars + r_vars fused_rw reads nodes torch _inductor config patch _post_fusion_custom_pass=fn torch no_grad torch compile foo x y out = torch ops _inductor_test realize x + y out sum dim= foo torch rand device=GPU_TYPE torch rand device=GPU_TYPE test_reduction_no_pointwise test one pw var one red var torch _inductor tiling_utils fn nodes assertTrue len nodes == fused_rw = tiling_utils extract_normalized_read_writes nodes i_vars r_vars = fused_rw index_vars fused_rw reduce_vars assertTrue len i_vars == assertTrue len r_vars == nodes torch _inductor config patch _post_fusion_custom_pass=fn torch no_grad torch compile foo x x sum foo torch rand device=GPU_TYPE test_coalescing torch _inductor tiling_utils Define symbolic variables i j n m = sympy symbols i j n m integer=True Test cases expression var_ranges expected_result test_cases = Simple direct case i + j i j i Floor division case i + FloorDiv j i j i Modular indexing i + ModularIndexing j i j j Case no coalescing variable i + j i j None Division case i i None More complex floor division j + FloorDiv i i j j Addition inside modular indexing ModularIndexing i + i j i expr var_ranges expected test_cases Test function result = tiling_utils find_coalesced_var expr var_ranges assertEqual result expected parametrize downcast_transposed_v False True test_tiled_coalesce_analysis downcast_transposed_v test one pw var one red var torch _inductor tiling_utils fn nodes assertTrue len nodes == coalesce_analysis = tiling_utils analyze_memory_coalescing nodes i_vars = coalesce_analysis norm_read_writes index_vars because output contiguous second dimension should coalesce twice many bytes first dimension downcasted downcasted should equal bc larger dtype size we also weight writes x cont_reads = coalesce_analysis coalesced_by_var i_vars t_reads = coalesce_analysis coalesced_by_var i_vars downcast_transposed_v assertEqual cont_reads t_reads assertEqual cont_reads t_reads nodes torch _inductor config patch _post_fusion_custom_pass=fn torch no_grad torch compile foo x y x + y x dtype y_dtype = torch float downcast_transposed_v torch float foo torch rand device=GPU_TYPE torch rand device=GPU_TYPE dtype=y_dtype T test_solve_for_zero torch _inductor tiling_utils x y = sympy symbols x y integer=True Test cases expression expected_result test_cases = Simple linear expressions x + - x - Constant expressions should None sympy Integer None sympy Integer None FloorDiv cases should None per function FloorDiv x None FloorDiv x + None ModularIndexing cases ModularIndexing x ModularIndexing x Expressions no constant solution x + None No real solution expr expected test_cases result = tiling_utils solve_for_zero expr assertEqual result expected test_solve_for_tiling torch _inductor tiling_utils x = sympy Symbol x integer=True test_cases = Simple linear cases coalesce x None Expression no free symbols sympy Integer None x FloorDiv x ModularIndexing expressions ModularIndexing FloorDiv x x + ModularIndexing x None x None Non-linear diff constant ModularIndexing x + FloorDiv x ModularIndexing x + FloorDiv x expr expected test_cases result = tiling_utils solve_for_tiling expr assertEqual result expected test_induced_fused_tiling torch _inductor tiling_utils fn nodes assertTrue len nodes == coalesce_analysis = tiling_utils analyze_memory_coalescing nodes assertEqual coalesce_analysis suggested_split tiling_factor nodes torch _inductor config patch _post_fusion_custom_pass=fn torch no_grad forward permute clone = torch ops aten clone default permute memory_format=torch contiguous_format view_ = torch ops aten view default clone - amax_ = torch ops aten amax default view_ amax_ XDIM = YDIM = arg _ = torch randn XDIM YDIM device=GPU_TYPE dtype=torch bfloat permute = torch ops aten permute default arg _ out code = run_and_get_code torch compile forward permute assertEqual out forward permute FileCheck check YBLOCK check XBLOCK run code layouts = cont NHWC T inductor_config patch triton unique_kernel_names True loop_ordering_after_fusion True triton coalesce_tiling_analysis True instantiate_parametrized_tests TestTiling TestCase T layout str SIZE_A = SIZE_B = SIZE_C = layout == cont torch rand SIZE_A SIZE_B SIZE_C device=GPU_TYPE unsqueeze layout == T torch rand SIZE_A SIZE_B SIZE_C device=GPU_TYPE transpose contiguous transpose unsqueeze assert layout == NHWC torch rand SIZE_A SIZE_B SIZE_C device=GPU_TYPE memory_format=torch channels_last parametrize layouts parametrize b layouts test_pointwise b foo x y x + y x y = T T b res code = run_and_get_code torch compile foo x y = b FileCheck check ynumel run code FileCheck check_not ynumel run code assertEqual res foo x y test_tiled_reduction f b b sum dim=- N = inps = torch randn N N N device=GPU_TYPE permute torch randn N N N device=GPU_TYPE permute f_c = torch compile f out code = run_and_get_code f_c inps FileCheck check_dag xnumel = check_dag ynumel = check_dag rnumel run code assertEqual out f inps atol= rtol= test_ d_pointwise inps = T cont T T T NHWC f x y z x + y + z f_c = torch compile f out code = run_and_get_code f_c inps FileCheck check_dag znumel check_dag ynumel check_dag xnumel run code assertEqual out f inps test_cat test unwrapping Identity f x y torch cat x y + x = T cont y = T T inps = x y f_c = torch compile f out code = run_and_get_code f_c inps FileCheck check_dag ynumel check_dag xnumel run code assertEqual out f inps test_penalized_small_dim x = torch rand device=GPU_TYPE y = torch rand device=GPU_TYPE T don t tile when doesn t affect total coalesced mem accesses much f x y x + y inps = x y f_c = torch compile f out code = run_and_get_code f_c inps FileCheck check_not ynumel check_dag xnumel run code assertEqual out f inps test_mutation_deps f x x add_ x = T cont torch _inductor tiling_utils fn nodes assertTrue len nodes == coalesce_analysis = tiling_utils analyze_memory_coalescing nodes assert coalesce_analysis None reads = coalesce_analysis norm_read_writes reads writes = coalesce_analysis norm_read_writes writes assertTrue len reads == len writes == assertEqual list coalesce_analysis norm_read_writes reads values OrderedSet arg _ assertEqual list coalesce_analysis norm_read_writes writes values OrderedSet buf nodes torch _inductor config patch _post_fusion_custom_pass=fn torch no_grad torch compile f x __name__ == __main__ HAS_GPU run_tests