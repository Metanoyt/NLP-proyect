mypy allow-untyped-defs contextlib contextlib nullcontext dataclasses dataclass field typing Any Optional TYPE_CHECKING Union torch torch utils _pytree pytree torch _C DispatchKey torch _dispatch python suspend_functionalization torch _higher_order_ops utils _from_fun _maybe_reenter_make_fx _set_compilation_env clone_outputs_aliasing_inputs FunctionalizeCtxWrapper get_dummy_aot_autograd_config HopInstance prepare_fw_with_masks reenter_make_fx register_fake save_tensors_and_symints_for_backward saved_tensors_and_symints torch _ops HigherOrderOperator torch _subclasses functional_tensor disable_functional_mode torch fx experimental proxy_tensor _temp_remove_metadata_torch_function_mode _temp_remove_pre_dispatch_torch_function_mode disable_proxy_modes_tracing ProxyTorchDispatchMode track_tensor_tree torch fx graph_module GraphModule torch fx passes runtime_assert insert_deferred_runtime_asserts TYPE_CHECKING collections abc Callable invoke_subgraph_counter = During tracing joint graph we construct information This used filter out grad_outs tangents ` backward ` method InvokeSubgraphAutogradOp dataclass OutputMetadata num_fw_outs Optional int = None indexes_with_symint set int = field default_factory=set indexes_with_no_grad set int = field default_factory=set InvokeSubgraphHOP HigherOrderOperator __init__ - None Invoke subgraph does have any state just wrapper over subgraph so we can safely cache HOP super __init__ invoke_subgraph cacheable=True This used fake tensor cache key validator extract subgraph iterate over nodes find all nodes fake tensor cacheable subgraph_indexes = identifier setup upper part stack This helps us identifying two invoke_subgraph calls have same subgraph __call__ subgraph Union GraphModule FunctionalizeCtxWrapper identifier Optional str operands assert identifier None isinstance identifier str identifier must None string assert all isinstance o torch Tensor int torch SymInt torch Generator o operands f invoke_subgraph operands must list tensors ints SymInts Generator operands super __call__ subgraph identifier operands pyrefly ignore bad-override gen_schema subgraph identifier operands torch _higher_order_ops schema HopSchemaGenerator torch _higher_order_ops utils check_input_alias_and_mutation_return_outputs materialize_as_graph gm torch fx GraphModule = materialize_as_graph subgraph operands schema_gen = HopSchemaGenerator schema_gen add_arg subgraph gm schema_gen add_arg identifier identifier _ _ _ mutated_inputs outputs = check_input_alias_and_mutation_return_outputs gm idx arg enumerate operands schema_gen add_arg f arg idx arg is_mutated=idx mutated_inputs out outputs schema_gen add_output out schema_gen gen_schema invoke_subgraph = InvokeSubgraphHOP invoke_subgraph_placeholder func args kwargs torch compiler is_dynamo_compiling This just placeholder Dynamo replace invoke_subgraph raise RuntimeError invoke_subgraph should called directly Dynamo torch compiler is_compiling For non-strict export tracing we still want go through Dynamo torch _dynamo backends debugging make_eager_backend_with_torch_function_mode _invoke_subgraph_placeholder_wrapper func args invoke_subgraph_placeholder func args _set_compilation_env torch _dynamo utils disable_cache_limit _temp_remove_pre_dispatch_torch_function_mode _temp_remove_metadata_torch_function_mode metadata_mode metadata_mode backend Union str Callable Any = make_eager_backend_with_torch_function_mode metadata_mode backend = eager torch compile _invoke_subgraph_placeholder_wrapper backend=backend fullgraph=True func args func args kwargs mark_compile_region fn=None This wrapper instructs torch compile compile wrapped region once reuse compiled artifact instead usual way aggressively inlining function Under hood tells TorchDynamo use InvokeSubgraph HOP region For PyTorch eager no-op wrap func inner args kwargs Get innermost function avoid nested compile regions inner_func = func while hasattr inner_func __marked_compile_region_fn__ inner_func = inner_func __marked_compile_region_fn__ invoke_subgraph_placeholder inner_func args kwargs inner __marked_compile_region_fn__ = func type ignore attr-defined inner fn wrap fn wrap get_invoke_subgraph_cache cache = None tracing_ctx = torch _guards TracingContext try_get cache = tracing_ctx hop_dispatch_set_cache get_cache invoke_subgraph cache TODO anijain - Delete function when base_hop uses invoke_subgraph infra trace_joint_graph fn fw_inputs fw_outputs Naively trace out joint graph This simplifies reconstruction joint graph min-cut partitioner later torch _functorch aot_autograd create_joint dummy_aot_config = get_dummy_aot_autograd_config This joint_fn inserted backward graph This simplifies min-cut partitioner work later Input signature - primals tangents Output signature - grads fw_outs The output signature deliberately kept grads first fw_outs second Having grads first makes min-cut partitioner HOP graph stitching easier joint_fn primals_and_tangents primals = primals_and_tangents len fw_inputs tangents = primals_and_tangents len fw_inputs fw_outs grads = create_joint prepare_fw_with_masks fn aot_config=dummy_aot_config primals tangents maybe_clone = clone_outputs_aliasing_inputs primals_and_tangents signature deliberately kept grads fw_outs This simplifies partitioning work later pytree tree_map maybe_clone tuple grads + list fw_outs primals = list fw_inputs This assumes tangent strides match fw_outputs strides Check InvokeSubgraphAutogradOp backward op contiguous call tangents = _from_fun out out fw_outputs joint_operands = primals + tangents _maybe_reenter_make_fx joint_fn joint_operands TODO anijain - Delete function when base_hop uses invoke_subgraph infra create_fw_bw_graph subgraph operands grad_outputs=None suspend_functionalization disable_functional_mode disable_proxy_modes_tracing args functional tensors generate some example tensors fw_inputs = pytree tree_map _from_fun operands torch _guards detect_fake_mode fake_mode = detect_fake_mode fw_inputs context = nullcontext fake_mode None fake_mode shape_env None fake_mode shape_env ignore_fresh_unbacked_symbols context fw_outs = pytree tree_map _from_fun subgraph fw_inputs num_fw_outs = len fw_outs Collect indexes none output check grad None corresponding index backward This check performed autograd Function - InvokeSubgraphAutogradOp Also collect indexes no_grad output filter out grad_outs ` backward ` method output_metadata = OutputMetadata output_metadata num_fw_outs = num_fw_outs idx fw_out enumerate fw_outs isinstance fw_out torch SymInt output_metadata indexes_with_symint add idx fw_out requires_grad output_metadata indexes_with_no_grad add idx grad_outputs None Infer grad_outputs same properties fw_outputs they re passed Although fw_outs equivalent grad_outputs tracing purposes we have carefully handle None fw_out do have require_grad At those indexes we will have None backward graph grad_outputs = fw_outs grad_outputs = grad grad grad_outputs grad None grad_outputs = grad grad grad_outputs grad requires_grad Force grad_out contiguous This because runtime grad_out could have different strides than fw_outs So we force grad_outs contiguous both tracing runtime grad_outputs = grad contiguous grad grad_outputs any isinstance out torch Tensor out grad_outputs out None raise RuntimeError Expect outputs invoke_subgraph only contains tensors None f Got types type out out grad_outputs Trace forward subgraph fw_graph = _maybe_reenter_make_fx subgraph fw_inputs Trace joint graph assign bwd graph bw_graph = trace_joint_graph subgraph fw_inputs grad_outputs fw_graph bw_graph output_metadata get_output_metadata subgraph operands suspend_functionalization disable_functional_mode disable_proxy_modes_tracing args functional tensors generate some example tensors fw_inputs = pytree tree_map _from_fun operands torch _guards detect_fake_mode fake_mode = detect_fake_mode fw_inputs context = nullcontext fake_mode None fake_mode shape_env None fake_mode shape_env ignore_fresh_unbacked_symbols context fw_outs = pytree tree_map _from_fun subgraph fw_inputs num_fw_outs = len fw_outs Collect indexes none output check grad None corresponding index backward This check performed autograd Function - InvokeSubgraphAutogradOp Also collect indexes no_grad output filter out grad_outs ` backward ` method output_metadata = OutputMetadata output_metadata num_fw_outs = num_fw_outs idx fw_out enumerate fw_outs isinstance fw_out torch SymInt output_metadata indexes_with_symint add idx fw_out requires_grad output_metadata indexes_with_no_grad add idx output_metadata trace_joint_graph_as_bwd subgraph num_primals joint_operands include_key_set exclude_key_set Naively trace out joint graph This simplifies reconstruction joint graph min-cut partitioner later torch _functorch aot_autograd create_joint dummy_aot_config = get_dummy_aot_autograd_config isinstance subgraph torch fx GraphModule graph_with_interpreter args Running graph interpreter needed propagating stack_trace torch fx traceback preserve_node_meta torch fx Interpreter subgraph run args fn = graph_with_interpreter fn = subgraph This joint_fn inserted backward graph This simplifies min-cut partitioner work later Input signature - primals tangents Output signature - grads fw_outs The output signature deliberately kept grads first fw_outs second Having grads first makes min-cut partitioner HOP graph stitching easier joint_fn primals_and_tangents primals = primals_and_tangents num_primals tangents = primals_and_tangents num_primals fw_outs grads = create_joint prepare_fw_with_masks fn aot_config=dummy_aot_config primals tangents maybe_clone = clone_outputs_aliasing_inputs primals_and_tangents signature deliberately kept grads fw_outs This simplifies partitioning work later pytree tree_map maybe_clone tuple grads + list fw_outs suspend_functionalization disable_functional_mode disable_proxy_modes_tracing joint_operands = _from_fun arg arg joint_operands contextlib ExitStack stack stack enter_context torch _C _ForceDispatchKeyGuard include_key_set exclude_key_set torch enable_grad _maybe_reenter_make_fx joint_fn joint_operands InvokeSubgraphAutogradOp torch autograd Function Saves subgraph i e original callable forward method And then traces out joint graph backward This delaying tracing backward also called lazy backward ensures assumptions about grad_out strides tensor-subclass-ness already accounted staticmethod pyrefly ignore bad-override forward ctx subgraph identifier output_metadata operands We want delay backward graph construction until backward So forward we just run fw callable And save all information necessary construct backward graph ctx ctx _subgraph = subgraph ctx _identifier = identifier ctx _output_metadata = output_metadata We snapshot dispatch keys forward materializing bw_graph backward ctx _fw_include_key_set = torch _C _dispatch_tls_local_include_set ctx _fw_exclude_key_set = torch _C _dispatch_tls_local_exclude_set save_tensors_and_symints_for_backward ctx operands torch _C _AutoDispatchBelowAutograd out = invoke_subgraph subgraph f fw_ identifier operands Check int coming symint expected indexes idx o enumerate out isinstance o int assert idx output_metadata indexes_with_symint out staticmethod backward ctx grad_outs torch _dynamo utils dynamo_timed subgraph = ctx _subgraph identifier = ctx _identifier output_metadata = ctx _output_metadata primals = saved_tensors_and_symints ctx Filter out grads None do require_grad This assumption we made during tracing joint_graph filtered_grad_outs = idx o enumerate grad_outs o None assert idx output_metadata indexes_with_symint idx output_metadata indexes_with_no_grad Deliberately skip over grad_outs which we know should None because corresponding fwd_out does require_grad pass filtered_grad_outs append o filtered_grad_outs = tuple filtered_grad_outs Important note - Even though forward graph can same different invoke_subgraphs backward graph can different because tangent strides can different So here we cache tangent_metadata addition identifier torch _guards detect_fake_mode torch _subclasses _fake_tensor_utils _CacheKeyState torch _subclasses fake_tensor extract_tensor_metadata fake_mode = detect_fake_mode primals + filtered_grad_outs assert fake_mode None fake_mode should enabled HOPs state = _CacheKeyState fake_mode shape_env tangent_metadata list object = tangent filtered_grad_outs metadata = extract_tensor_metadata tangent metadata _flatten_into tangent_metadata fake_mode state pyrefly ignore bad-assignment tangent_metadata = tuple tangent_metadata bw_graph joint graph signature primals_and_tangents returns grads_and_fw_outs To get grads we use num_fw_outs extract grads primals_and_tangents = primals + filtered_grad_outs Check we have already traced bwd subgraph bw_graph = None suffix = None invoke_subgraph_cache = get_invoke_subgraph_cache cache_hit = False invoke_subgraph_cache bw_graph suffix = invoke_subgraph_cache get_lazy_bwd_entry identifier tangent_metadata cache_hit = bw_graph None bw_graph None assert suffix None dynamo_timed invoke_subgraph_trace_joint_graph log_pt _compile_event=True bw_graph = trace_joint_graph_as_bwd subgraph len primals primals_and_tangents ctx _fw_include_key_set ctx _fw_exclude_key_set invoke_subgraph_cache cache_hit suffix = invoke_subgraph_cache add_lazy_bwd_entry identifier tangent_metadata bw_graph grads = invoke_subgraph bw_graph f bw_ identifier _ suffix primals_and_tangents -output_metadata num_fw_outs None None None grads invoke_subgraph py_autograd_impl _ subgraph identifier operands Check we have already traced subgraph invoke_subgraph_cache = get_invoke_subgraph_cache invoke_subgraph_cache saved_autograd_fn = invoke_subgraph_cache get_autograd_key_entry identifier saved_autograd_fn operands output_metadata = get_output_metadata subgraph operands autograd_fn_callable args InvokeSubgraphAutogradOp apply subgraph identifier output_metadata args Save autograd_fn_callable dispatch set cache invoke_subgraph_cache invoke_subgraph_cache add_autograd_key_entry identifier autograd_fn_callable autograd_fn_callable operands invoke_subgraph py_impl DispatchKey CompositeExplicitAutograd _ subgraph identifier operands torch utils _python_dispatch _get_current_dispatch_mode mode = _get_current_dispatch_mode assert mode None Mode should never enabled CPU CUDA key subgraph operands invoke_subgraph py_functionalize_impl _ ctx subgraph identifier operands torch _higher_order_ops auto_functionalize can_auto_functionalize do_auto_functionalize_v unwrapped_operands = ctx unwrap_tensors operands hop_instance = HopInstance create invoke_subgraph subgraph identifier operands can_auto_functionalize hop_instance NOTE auto_functionalize x invoke_subgraph caching We call auto_functionalized_v support input mutation invoke_subgraph See NOTE Support input mutation hops overall design invoke_subgraph special because its identifier based caching mechanism In invoke_subgraph s functionalization key implementation we create new identifier because subgraph replaced FunctionWithNoFreeVars functional + epilogue form assert isinstance identifier str identifier do_auto_functionalize_v ctx mode hop_instance subgraph auto_functionalized_ + identifier operands ctx redispatch_to_next NB There assumption subgraph does mutate inputs there no aliasing Its Dynamo responsibility prevent formation invoke_subgraph ops input aliasing mutation detected functionalized_subgraph = FunctionalizeCtxWrapper ctx subgraph out = invoke_subgraph functionalized_subgraph identifier unwrapped_operands ctx wrap_tensors out Register hop fake fn This will called fake_tensor _dispatch_impl register_fake invoke_subgraph _ subgraph identifier operands torch _dynamo utils dynamo_timed dynamo_timed invoke_subgraph_fake_tensor log_pt _compile_event=True subgraph operands invoke_subgraph py_impl ProxyTorchDispatchMode _ proxy_mode ProxyTorchDispatchMode subgraph identifier operands Check we have already traced subgraph graph = None invoke_subgraph_cache = get_invoke_subgraph_cache invoke_subgraph_cache graph = invoke_subgraph_cache get_proxy_dispatch_entry identifier graph None torch _dynamo utils dynamo_timed dynamo_timed invoke_subgraph_proxy_tensor log_pt _compile_event=True graph = reenter_make_fx subgraph operands torch _guards detect_fake_mode fake_mode = detect_fake_mode operands assert fake_mode None fake_mode shape_env None insert_deferred_runtime_asserts graph fake_mode shape_env invoke_subgraph_proxy_torch_dispatch_mode export=True graph recompile assert isinstance proxy_mode tracer torch fx Tracer invoke_subgraph_cache invoke_subgraph_cache add_proxy_dispatch_entry identifier graph node_args = graph identifier operands _unwrap_proxy arg isinstance arg torch fx GraphModule NOTE invoke_subgraph proxy_mode x auto_functionalize Previously we assumed ` invoke_subgraph ` would always traced same tracer This allowed us cache modules their identifiers assuming they already registered However assumption no longer holds when we auto-functionalize ` invoke_subgraph ` auto_functionalize functionalizes subgraph wrap ` FunctionWithNoFreeVars ` In proxy mode implementation ` auto_functionalized_v ` we need materialize ` FunctionWithNoFreeVars ` input graph module To do we re-trace ` invoke_subgraph ` hop which starts new sub-tracer see NOTE materialize callable inputs graph When new sub-tracer traces ` invoke_subgraph ` previously cached identifier corresponding graph module might exist submodule new tracer s root Therefore we register submodule below The alternative give new identifier when we re-trace invoke_subgraph will increase compilatoin time which defeats purpose caching registered_before = False _ submod proxy_mode tracer root named_modules type ignore union-attr arg submod registered_before = True registered_before qualname = proxy_mode tracer get_fresh_qualname repeated_subgraph type ignore union-attr proxy_mode tracer root register_module qualname arg type ignore union-attr proxy_mode tracer unwrap_proxy arg type ignore union-attr proxy_args = pytree tree_map _unwrap_proxy node_args type ignore union-attr out_proxy = proxy_mode tracer create_proxy call_function invoke_subgraph proxy_args example_out = invoke_subgraph graph identifier operands track_tensor_tree example_out out_proxy constant=None tracer=proxy_mode tracer