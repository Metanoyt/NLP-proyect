Owner s oncall distributed os sys tempfile torch torch distributed c d torch multiprocessing mp torch testing _internal common_distributed MultiProcessTestCase torch testing _internal common_utils load_tests run_tests Torch distributed nn available windows check errors _torch_dist_nn_available = True try torch distributed nn except ImportError _torch_dist_nn_available = False load_tests common_utils used automatically filter tests sharding sandcastle This line silences flake warnings load_tests = load_tests noqa PLW c d is_available print c d available skipping tests file=sys stderr sys exit AbstractProcessGroupShareTensorTest world_size = _test_multiprocess f shared_tensors init_pg n_output ws = world_size file store will delete test file destruction file = tempfile NamedTemporaryFile delete=False ctx = mp get_context spawn c p = ctx Queue p c = ctx Queue ps = i range ws p = ctx Process target=f args= i file name shared_tensors ws init_pg c p p c p start ps append p _ range ws n_output pid expected result = c p get assertEqual expected result msg=f Expect rank pid receive tensor expected got result _ range ws p c put p ps p join Why classmethod multiprocessing cannot pickle TestCase subclass when spawn mode See https bugs python org issue classmethod _test_broadcast_process cls rank filename shared_tensors world_size init_pg c p p c pg = init_pg rank filename world_size xs = shared_tensors rank pg broadcast xs wait c p put rank torch zeros xs cpu p c get classmethod _test_allreduce_process cls rank filename shared_tensors world_size init_pg c p p c pg = init_pg rank filename world_size xs = shared_tensors rank pg allreduce xs op=c d ReduceOp SUM wait c p put rank torch ones xs cpu p c get classmethod _test_allgather_process cls rank filename shared_tensors world_size init_pg c p p c pg = init_pg rank filename world_size xs = shared_tensors rank ys = torch zeros_like xs i range world_size pg allgather ys xs wait i range world_size c p put rank torch ones i ys i cpu p c get TestDistributedNNFunctions MultiProcessTestCase setUp super setUp _spawn_processes tearDown super tearDown try os remove file_name except OSError pass property op_timeout_sec property world_size _test_broadcast backend store = c d FileStore file_name world_size This required because these functions calls directly dist needs world initialized c d init_process_group store=store rank=self rank world_size=self world_size backend=backend device = torch device f cuda rank x = torch ones device=device + rank x requires_grad = True y = torch distributed nn broadcast x assertEqual y + torch ones z = y sin sum z backward We can t check gradient communications numerically so we have do some calculations rank == assertEqual x grad torch cos x rank == assertEqual x grad torch zeros device=device _test_reduce backend store = c d FileStore file_name world_size This required because these functions calls directly dist needs world initialized c d init_process_group store=store rank=self rank world_size=self world_size backend=backend device = torch device f cuda rank x = torch ones device=device + rank x requires_grad = True y = torch distributed nn reduce x op=c d ReduceOp SUM rank == assertEqual y torch ones device=device z = y sin sum z backward Gradients broadcasted both ranks x_g = torch ones device=device cos assertEqual x grad x_g _test_allreduce backend store = c d FileStore file_name world_size This required because these functions calls directly dist needs world initialized c d init_process_group store=store rank=self rank world_size=self world_size backend=backend device = torch device f cuda rank x = torch ones device=device + rank x requires_grad = True y = torch distributed nn all_reduce x op=c d ReduceOp SUM assertEqual y torch ones device=device z = y sin sum z backward x_g = torch ones device=device cos assertEqual x grad x_g _test_all_gather backend store = c d FileStore file_name world_size This required because these functions calls directly dist needs world initialized c d init_process_group store=store rank=self rank world_size=self world_size backend=backend device = torch device f cuda rank x = torch ones device=device + rank x requires_grad = True tensors = torch distributed nn all_gather x i t enumerate tensors assertEqual t torch ones device=device + i y = torch sum torch stack tensors axis= z = y sin sum z backward x_s = torch ones device=device cos assertEqual x grad x_s _test_all_to_all backend store = c d FileStore file_name world_size This required because these functions calls directly dist needs world initialized c d init_process_group store=store rank=self rank world_size=self world_size backend=backend device = torch device f cuda rank x = torch ones device=device + rank x = torch ones device=device + rank x requires_grad = True x requires_grad = True y = torch empty_like x y = torch empty_like x tensors = torch distributed nn all_to_all y y x x i t enumerate tensors assertEqual t torch ones device=device + i y = torch sum torch stack tensors axis= z = y sin sum z backward x_s = torch ones device=device cos assertEqual x grad x_s assertEqual x grad x_s _test_all_to_all_single backend store = c d FileStore file_name world_size This required because these functions calls directly dist needs world initialized c d init_process_group store=store rank=self rank world_size=self world_size backend=backend device = torch device f cuda rank row = world_size rank + world_size + x = torch ones int row device=device rank + x requires_grad = True y = torch empty_like x split_sizes = i + rank + i range world_size y = torch distributed nn all_to_all_single y x output_split_sizes=split_sizes input_split_sizes=split_sizes expected = idx tensor enumerate torch split x split_sizes expected append torch full_like tensor idx + expected = torch cat expected assertEqual y expected z = y sin sum z backward x_s = rank + torch ones int row device=device cos assertEqual x grad x_s __name__ == __main__ run_tests