mypy allow-untyped-defs Copyright c Meta Platforms Inc affiliates collections abc Callable Sequence functools partial typing Union torch torch _ops OpOverload torch distributed tensor DTensor torch distributed tensor _op_schema OpSchema OpStrategy PlacementList RuntimeSchemaInfo StrategyType TupleStrategy torch distributed tensor _ops utils expand_to_full_mesh_op_strategy __all__ = register_sharding register_sharding op Union OpOverload list OpOverload meth ` register_sharding ` experimental API allows users register sharding strategies operator when tensor inputs outputs DTensor It can useful when there doesn t exist default sharding strategy ` ` op ` ` e g when ` ` op ` ` custom operator supported ` DTensor ` when users would like overwrite default sharding strategies existing operators Args op Union OpOverload List OpOverload An op list ops register customized sharding function Returns A function decorator which can used wrap function defines sharding strategy operator specified ` ` op ` ` The defined sharding strategy will registered DTensor will override default sharding strategy DTensor has already implemented operator The customized sharding function takes same inputs original op except arg ` torch Tensor ` will replaced tensor-like object DTensor uses internally The function should sequence -tuples each specifying acceptable output placements its corresponding input placements Example xdoctest +SKIP distributed register_sharding aten _softmax default custom_softmax_sharding x dim half_to_float softmax_dim = dim dim = dim + x ndim acceptable_shardings = all_replicate = Replicate Replicate None None acceptable_shardings append all_replicate sharding_dim range x ndim sharding_dim = softmax_dim all_sharded = Shard sharding_dim Shard sharding_dim None None acceptable_shardings append all_sharded acceptable_shardings note This API currently experimental subject change custom_strategy custom_sharding_fn Callable Sequence tuple PlacementList PlacementList op_schema OpSchema - StrategyType strategy_to_spec strategy object - object isinstance strategy OpStrategy take output spec first strategy strategy strategies output_spec isinstance strategy TupleStrategy tuple strategy_to_spec s s strategy children strategy mesh = op_schema get_mesh_from_args args_schema = tuple strategy_to_spec i i op_schema args_schema kwargs_schema = k strategy_to_spec v k v op_schema kwargs_schema items acceptable_shardings = custom_sharding_fn args_schema kwargs_schema single_mesh_dim_strategies list PlacementList = output_specs input_specs acceptable_shardings single_mesh_dim_strategies append output_specs + input_specs TODO handle out variant ops expand_to_full_mesh_op_strategy mesh op_schema single_mesh_dim_strategies input_index=len op_schema op _schema returns inplace_op=op_schema is_inplace_op wrapper custom_sharding_fn derive_schema_info op NOTE without user directly providing RuntimeSchemaInfo now we create conservative fashion follows let static_argnum first int argument let static_kwargkey include all int type kwargs always set needs_pytree=True static_argnum = static_kwargkey list str = i arg enumerate op _schema arguments isinstance arg type torch IntType isinstance arg type torch OptionalType isinstance arg type getElementType torch IntType static_argnum = min i static_argnum arg kwarg_only static_kwargkey append arg name RuntimeSchemaInfo static_argnum static_kwargkey None needs_pytree=True overloads = op isinstance op list op overload overloads DTensor _op_dispatcher sharding_propagator register_op_strategy overload partial custom_strategy custom_sharding_fn derive_schema_info overload custom_sharding_fn wrapper