Owner s module inductor itertools logging os pathlib subprocess sys tempfile unittest zipfile unittest skip unittest mock patch torch torch _export torch _inductor torch _inductor config torch ao quantization quantizer x _inductor_quantizer xiq torch nn nn torch _dynamo config dynamo_config torch _dynamo device_interface get_interface_for_device torch _dynamo testing rand_strided same torch _dynamo utils counters torch _inductor config torch _inductor codecache WritableTempFile torch _inductor cpp_builder normalize_path_separator torch _inductor package package_aoti torch _inductor runtime runtime_utils cache_dir torch _inductor test_case TestCase torch _inductor utils is_big_gpu maybe_aoti_standalone_config run_and_get_cpp_code torch _library capture_triton torch _utils_internal full_aoti_runtime_assert torch ao quantization quantize_pt e convert_pt e prepare_pt e torch ao quantization quantizer x _inductor_quantizer X InductorQuantizer torch export Dim export torch export pt _archive _package load_pt torch testing FileCheck torch testing _internal common_utils torch testing _internal common_cuda _get_torch_cuda_version CDNA OrLater IS_SM PLATFORM_SUPPORTS_FLASH_ATTENTION PLATFORM_SUPPORTS_FP PLATFORM_SUPPORTS_MEM_EFF_ATTENTION SM OrLater tf _on_and_off torch testing _internal common_device_type _has_sufficient_memory e m _type skipCUDAIf torch testing _internal common_quantization _group_quantize_tensor skip_if_no_torchvision skipIfNoFBGEMM torch testing _internal common_utils DeterministicGuard IS_CI IS_FBCODE IS_MACOS IS_WINDOWS MACOS_VERSION MI _ARCH parametrize runOnRocm skipIfMPS skipIfRocm skipIfRocmArch skipIfWindows skipIfWindowsXPU skipIfXpu TEST_MPS TEST_WITH_ROCM torch testing _internal custom_tensor CustomTensorPlainOut torch testing _internal inductor_utils GPU_TYPE HAS_GPU HAS_XPU_AND_TRITON IS_BIG_GPU torch testing _internal logging_utils LoggingTestCase make_logging_test torch testing _internal triton_utils requires_gpu torch utils _pytree pytree torch utils _triton has_triton_experimental_host_tma has_triton_tensor_descriptor_host_tma HAS_GPU triton manual triton language tl torch testing _internal triton_utils add_kernel add_kernel_ d_autotuned add_kernel_autotuned add_kernel_autotuned_weird_param_order add_kernel_on_device_tma_new_api add_kernel_on_device_tma_old_api add_kernel_with_boolean_param add_kernel_with_none_param_and_equal_to_ _arg add_kernel_with_optional_param add_kernel_with_scaling add_kernel_with_tma_ d_new_api add_kernel_with_tma_ d_old_api add_kernel_with_tma_ d_new_api add_kernel_with_tma_ d_old_api create_tensor_descriptor_shim mul _inplace_kernel strange_config_matmul_kernel sub_kernel_autotuned IS_WINDOWS IS_CI sys stderr write Windows CI does have necessary dependencies test_torchinductor yet\n __name__ == __main__ sys exit raise unittest SkipTest requires sympy functorch filelock try try test_aot_inductor_utils AOTIRunnerUtil check_model check_model_with_multiple_inputs code_check_count test_control_flow CondModels prepend_counters prepend_predicates WhileLoopModels test_torchinductor copy_tests requires_multigpu TestFailure except ImportError test_aot_inductor_utils manual=fbcode caffe test inductor aot_inductor_utils-library AOTIRunnerUtil check_model check_model_with_multiple_inputs code_check_count test_control_flow manual=fbcode caffe test inductor control_flow-library CondModels prepend_counters prepend_predicates WhileLoopModels test_torchinductor manual=fbcode caffe test inductor test_inductor-library copy_tests requires_multigpu TestFailure except unittest SkipTest ImportError __name__ == __main__ sys exit raise get_module_ext_type IS_WINDOWS pyd so AOTInductorTestsTemplate Temporarily skipping test pytorch cpuinfo able retrieve cache size AMD EPYC F -Core Processor CPU gfx VM Runners common_utils parametrize embed_kernel_binary False True common_utils parametrize max_autotune False True skipIfRocmArch MI _ARCH test_simple embed_kernel_binary max_autotune device == cpu IS_MACOS max_autotune raise unittest SkipTest max_autotune supported macos Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y x + linear y example_inputs = torch randn device=self device torch randn device=self device model = Model config patch aot_inductor embed_kernel_binary embed_kernel_binary max_autotune max_autotune check_model model example_inputs _ code = run_and_get_cpp_code AOTIRunnerUtil compile model example_inputs device == mps FileCheck check aoti_torch_mps_get_kernel_function run code device == GPU_TYPE FileCheck check launchKernel run code config aot_inductor embed_kernel_binary Not expect see launchKernel CUBIN_FILE_NAME FileCheck check_not launchKernel run code use_minimal_arrayref_interface code_check_count model example_inputs AOTInductorModelRunMinimalArrayrefInterface test_triton_kernel_bool_param device = GPU_TYPE device == mps raise unittest SkipTest requires GPU Model torch nn Module forward x out = torch zeros_like x add_kernel_with_boolean_param in_ptr =x in_ptr =x out_ptr=out n_elements=x numel add_xy=True BLOCK_SIZE= out inputs = torch randn device=self device check_model Model inputs unittest skipIf IS_FBCODE toolchain doesn t support ptx fatbin skipIfMPS skipIfRocm Skip embed_kernel_binary == True now shows random failure CI common_utils parametrize embed_kernel_binary False unittest skipIf _get_torch_cuda_version Test only supported CUDA + test_simple_multi_arch embed_kernel_binary device = GPU_TYPE raise unittest SkipTest requires GPU_TYPE Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y x + linear y example_inputs = torch randn device=self device torch randn device=self device model = Model config patch aot_inductor embed_kernel_binary embed_kernel_binary aot_inductor emit_multi_arch_kernel True check_model model example_inputs embed_kernel_binary _ code = run_and_get_cpp_code AOTIRunnerUtil compile model example_inputs file_extension = spv device == xpu fatbin FileCheck check file_extension run code test_small_constant Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x example_inputs = torch randn device=self device config patch always_keep_tensor_constants True check_model Model device example_inputs test_output_path_ Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y x + linear y example_inputs = torch randn device=self device torch randn device=self device config patch aot_inductor output_path tmp_output_ check_model Model example_inputs test_output_path_ Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y x + linear y model = Model device=self device example_inputs = torch randn device=self device torch randn device=self device expected_path = normalize_path_separator os path join tempfile mkdtemp dir=cache_dir f model get_module_ext_type actual_path = AOTIRunnerUtil legacy_compile model example_inputs options= aot_inductor output_path expected_path assertTrue actual_path == expected_path unittest skipIf config triton native_matmul different input output constants native matmul test_empty_constant_folding Model torch nn Module __init__ device super __init__ w = torch randn device=device b = torch randn device=device forward x torch matmul x w + b model = Model device example_inputs = torch randn device=self device config patch aot_inductor use_runtime_constant_folding True so_path code = run_and_get_cpp_code AOTIRunnerUtil legacy_compile model example_inputs We should have input output constants model FileCheck check_count AOTInductorModelBase check_next check_next run code test_constant_folding Model torch nn Module __init__ device super __init__ w_pre = torch randn device=device b = torch randn device=device forward x w_transpose = torch transpose w_pre w_relu = torch nn functional relu w_transpose w = w_relu + b torch matmul x w example_inputs = torch randn device=self device config patch aot_inductor use_runtime_constant_folding True check_model Model device example_inputs test_constant_folding_with_update Model torch nn Module __init__ device super __init__ w_pre = torch randn device=device b = torch randn device=device forward x w_transpose = torch transpose w_pre w_relu = torch nn functional relu w_transpose w = w_relu + b torch matmul x w example_inputs = torch randn device=self device torch no_grad config patch always_keep_tensor_constants True aot_inductor use_runtime_constant_folding True model = Model device so_path = AOTIRunnerUtil legacy_compile model=model example_inputs=example_inputs runner = AOTIRunnerUtil legacy_load_runner device so_path runner_call args kwargs torch fx _pytree fx_pytree call_spec = runner get_call_spec in_spec = pytree treespec_loads call_spec out_spec = pytree treespec_loads call_spec flat_inputs = fx_pytree tree_flatten_spec args kwargs in_spec flat_inputs = x x flat_inputs isinstance x torch Tensor flat_outputs = runner run flat_inputs pytree tree_unflatten flat_outputs out_spec test_inputs = torch randn device=self device expected = model test_inputs output = runner_call test_inputs assertEqual expected output Update new weights active buffer new_weights = L__self___b torch randn device=self device L__self___w_pre torch randn device=self device model w_pre = new_weights L__self___w_pre model b = new_weights L__self___b expected = model test_inputs runner update_constant_buffer new_weights False False output = runner_call test_inputs assertEqual expected output Update new weights inactive buffer new_weights = L__self___b torch randn device=self device L__self___w_pre torch randn device=self device model w_pre = new_weights L__self___w_pre model b = new_weights L__self___b expected = model test_inputs runner update_constant_buffer new_weights True False new_output = runner_call test_inputs We have yet swapped buffer new_output should same old one assertEqual output new_output Swap buffer should get correct result now runner swap_constant_buffer new_output = runner_call test_inputs assertEqual expected new_output requires_gpu test_duplicate_constant_folding Model torch nn Module __init__ device super __init__ w = torch randn device=device w = torch randn device=device w = torch randn device=device w = torch randn device=device forward x w_concat = torch cat w w w w torch cat x w_concat example_inputs = torch randn device=self device config patch aot_inductor use_runtime_constant_folding True check_model Model device example_inputs test_autotune_with_constant_folding Model torch nn Module __init__ device - None super __init__ x = torch randn dtype=torch float device=device _quantize input torch abs input forward y abs_weight = _quantize x abs_y = _quantize y abs_weight abs_y input = torch rand dtype=torch float device=self device model = Model device device _ = model input ep = torch export export model input dynamic_shapes=None strict=False torch _inductor aoti_compile_and_package ep inductor_configs= aot_inductor use_runtime_constant_folding True unittest skipIf TEST_MPS MACOS_VERSION Compilation error test_aot_inductor_consts_cpp_build Model torch nn Module __init__ device - None super __init__ x = torch randn dtype=torch float device=device _quantize input torch abs input forward y abs_weight = _quantize x abs_y = _quantize y abs_weight abs_y input = torch rand dtype=torch float device=self device model = Model device device _ = model input ep = torch export export model input dynamic_shapes=None strict=False torch _inductor aoti_compile_and_package ep inductor_configs= aot_inductor use_runtime_constant_folding True aot_inductor use_consts_asm_build False common_utils parametrize dynamic False True common_utils parametrize tma_version new old test_triton_kernel_on_device_tma dynamic tma_version device = GPU_TYPE raise unittest SkipTest requires GPU tma_version == new has_triton_tensor_descriptor_host_tma skipTest requires triton tools tensor_descriptor TMA support tma_version == old has_triton_experimental_host_tma skipTest requires triton tools experimental_descriptor TMA support kernel = add_kernel_on_device_tma_new_api tma_version == new add_kernel_on_device_tma_old_api Model torch nn Module __init__ - None super __init__ forward b BLOCK_SIZE = out = torch zeros_like m n = out size Allocate workspace on-device TMA descriptors Need bytes per descriptor descriptors total tma_version == old workspace = torch zeros dtype=torch uint device=a device workspace = None grid = triton cdiv m BLOCK_SIZE triton cdiv n BLOCK_SIZE kernel grid b out m n workspace BLOCK_SIZE=BLOCK_SIZE out = torch randn device=self device b = torch randn device=self device example_inputs = b triton set_allocator lambda size align stream torch empty size dtype=torch int device=GPU_TYPE dynamic_shapes = None dynamic dim = Dim s min= max= dim = Dim s min= max= dynamic_shapes = dim None b dim None check_model Model example_inputs=example_inputs dynamic_shapes=dynamic_shapes requires_gpu test_multi_device device == cpu GPU_TYPE == xpu raise unittest SkipTest In scenario test case will run XPU code AOTIModelContainerRunnerCpu which reasonable See issue Model torch nn Module forward x x = x + x = x cpu x = x + x = x GPU_TYPE x example_inputs = torch randn device=self device check_model Model example_inputs unittest skip install_free_tensors leads OOM - https github com pytorch pytorch issues test_large_weight Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y x + linear y example_inputs = torch randn device=self device torch randn device=self device We only test compilation since we often get OOM running CI model = Model model = model device AOTIRunnerUtil compile model example_inputs test_constant_type_propagation Model torch nn Module __init__ device super __init__ w_pre = torch randn device=device b = torch randn device=device forward x w_transpose = torch transpose w_pre w_relu = torch nn functional relu w_transpose w = w_relu + b torch matmul x w model = Model device example_inputs = torch randn device=self device config patch aot_inductor use_runtime_constant_folding True so_path code = run_and_get_cpp_code AOTIRunnerUtil legacy_compile model example_inputs FileCheck check_not torch aot_inductor ConstantType Unknown run code test_subclasses device_to_init = device Foo torch nn Module __init__ super __init__ p = torch nn Parameter torch ones device=device_to_init p = torch nn Parameter CustomTensorPlainOut torch ones device=device_to_init torch ones device=device_to_init forward x = p + p sum x + m = Foo ref_x = torch randn device=device_to_init torch no_grad result = AOTIRunnerUtil run m ref_x actual = m ref_x assertTrue same result actual test_large_mmaped_weights Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y x + linear y example_inputs = torch randn device=self device torch randn device=self device config patch aot_inductor force_mmap_weights True check_model Model example_inputs test_large_mmaped_weights_on_disk Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y x + linear y example_inputs = torch randn device=self device torch randn device=self device config patch aot_inductor package_constants_on_disk_format binary_blob check_model Model example_inputs test_with_offset Model torch nn Module __init__ device super __init__ orig_tensor = torch randn device=device tensor = orig_tensor forward x y x + torch nn functional linear y orig_tensor + tensor example_inputs = torch randn device=self device torch randn device=self device check_model Model device example_inputs unittest skipIf IS_FBCODE Not yet runnable fbcode when model so newly generated while older PyTorch used test_freezing Model torch nn Module __init__ device super __init__ weight = torch randn device=device padding = torch randn device=device forward x y padded_weight = torch cat weight padding dim= x + torch nn functional linear y padded_weight example_inputs = torch randn device=self device torch randn device=self device config patch freezing True check_model Model device example_inputs unittest skipIf IS_FBCODE Not yet runnable fbcode when model so newly generated while older PyTorch used test_conv_freezing dtypes = torch bfloat torch float SM OrLater torch float dtype groups itertools product dtypes iC = oC = Model torch nn Module __init__ device super __init__ weight = torch randn oC groups iC device=device dtype forward y torch nn functional conv d y weight groups=groups example_inputs = torch randn iC groups device=self device dtype config patch freezing True check_model Model device example_inputs unittest skipIf IS_FBCODE Not yet runnable fbcode when model so newly generated while older PyTorch used tf _on_and_off test_deconv_freezing dtypes = torch float torch _C _has_mkldnn torch ops mkldnn _is_mkldnn_bf _supported dtypes append torch bfloat dtype groups itertools product dtypes iC = oC = Model torch nn Module __init__ device super __init__ weight = torch randn iC oC groups device=device dtype forward y torch nn functional conv_transpose d y weight groups=groups example_inputs = torch randn iC device=self device dtype config patch freezing True check_model Model device example_inputs unittest skipIf IS_FBCODE Not yet runnable fbcode when model so newly generated while older PyTorch used test_linear_freezing dtypes = torch bfloat torch float SM OrLater torch float dtype dtypes LinearModel torch nn Module __init__ device super __init__ weight = torch randn device=device dtype bias = torch randn device=device dtype forward y torch nn functional linear y weight bias example_inputs = torch randn device=self device dtype config patch freezing True model = LinearModel device=self device check_model model example_inputs test_same_backing torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor Tensor b - Tensor tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo CompositeExplicitAutograd lib=lib foo_impl torch Tensor b torch Tensor - torch Tensor + b M torch nn Module forward b x = shape y = b shape = torch cat = torch ops mylib foo = x b = torch cat b b b = torch ops mylib foo b b b = b y b inp = torch ones device=self device torch ones device=self device check_model M inp unittest skipIf TEST_MPS MACOS_VERSION MPS BFloat only supported MacOS + test_empty_cat_dtype_promotion Foo torch nn Module forward x y z = torch cat x y dim= z = z dtype=torch bfloat z model = Foo inps = torch randn dtype=torch bfloat torch randn check_model model inps unittest skipIf IS_BIG_GPU Skipping triton backend only since big GPU enough SM test_linear_dynamic_maxautotune device == cpu raise unittest SkipTest using triton backend only supported CPU Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x model = Model device=self device compile_inputs = torch randn device=self device dim _x = Dim dim _x min= max= dynamic_shapes = x dim _x ep = torch export export model compile_inputs dynamic_shapes=dynamic_shapes strict=True optimized = torch _inductor aoti_load_package torch _inductor aoti_compile_and_package ep inductor_configs= max_autotune True max_autotune_gemm_backends TRITON runtime_input = torch randn device=self device assertTrue same optimized runtime_input model runtime_input runtime_input = torch randn device=self device assertTrue same optimized runtime_input model runtime_input runtime_input = torch randn device=self device assertTrue same optimized runtime_input model runtime_input torch _inductor config patch pre_grad_fusion_options= normalization_pass remove_split_with_size_one_pass merge_getitem_cat_pass merge_stack_tahn_unbind_pass merge_splits_pass mutate_cat_pass split_cat_pass unbind_stack_pass post_grad_fusion_options= test_simple_split Model torch nn Module __init__ - None super __init__ forward x torch cat tensors=torch split x dim= dim=- example_inputs = torch randn device=self device counters clear model = Model device=self device actual = AOTIRunnerUtil legacy_run device model example_inputs assertTrue same model example_inputs actual assertEqual counters inductor scmerge_split_removed assertEqual counters inductor scmerge_cat_removed assertEqual counters inductor scmerge_split_sections_removed test_amp_fallback_random fn x w torch functional F linear x w example_inputs = torch randn device=self device torch randn device=self device config patch fallback_random True torch amp autocast device_type=self device check_model fn example_inputs test_missing_output Model torch nn Module __init__ - None super __init__ forward x y = torch sin x b = torch mm y c = torch cos b c example_inputs = torch randn device=self device torch randn device=self device check_model Model example_inputs test_output_misaligned Model torch nn Module __init__ - None super __init__ forward x y x_unsqueeze = torch unsqueeze x dim= y_unsqueeze = torch unsqueeze y dim= cat = torch cat x_unsqueeze y_unsqueeze dim= x_getitem = cat y_getitem = cat x_sigmoid = torch sigmoid x_getitem x_sigmoid y_getitem example_inputs = torch randn device=self device torch randn device=self device config patch aot_inductor use_runtime_constant_folding True check_model Model example_inputs unittest skipIf IS_BIG_GPU Skipping triton backend only since big GPU enough SM skip Test marked expected failure does fail always anymore test_dynamic_smem_above_default_limit device == cpu raise unittest SkipTest using triton backend only supported CPU Model torch nn Module forward x y x y model = Model device A generated Triton kernel MM requires bytes dynamic SMEM which above A s default dynamic SMEM limit bytes example_inputs = torch randn device=self device torch randn device=self device check_model model example_inputs options= max_autotune True max_autotune_gemm_backends TRITON unittest skipIf IS_FBCODE Not yet runnable fbcode test_seq layernorm = torch nn LayerNorm net = torch nn Sequential layernorm torch nn ReLU layernorm torch nn ReLU example_inputs = torch randn device=self device check_model net eval example_inputs test_addmm Model torch nn Module __init__ n k device super __init__ weight = torch randn n k device=device bias = torch randn n device=device forward torch nn functional linear weight bias M = N = K = model = Model N K device batch = = torch randn batch M K device=self device We should able call check_model here torch export export constants non-parameter non-buffer doesn t work today example_inputs = check_model model example_inputs test_aliased_buffer_reuse Model torch nn Module __init__ - None super __init__ forward x y x = x y = y c = torch cat x y dim=- d = + c m = torch mm d d m + x example_inputs = torch randn device=self device torch randn device=self device check_model Model example_inputs test_buffer_reuse Model torch nn Module __init__ - None super __init__ forward x y = torch sin x b = torch cos y c = torch mm b d = torch relu c e = torch sigmoid d f = torch mm x y g = e + f g example_inputs = torch randn device=self device torch randn device=self device check_model Model example_inputs test_duplicated_params Model torch nn Module __init__ - None super __init__ p = torch nn Parameter torch rand q = p forward x p x + q example_inputs = torch rand device=self device check_model Model example_inputs unittest skip Skip test only local test SIGABRT produced test_inf Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y x + linear y x = torch randn device=self device x = float Inf example_inputs = x torch randn device=self device check_model Model device example_inputs options= debug_check_inf_and_nan True unittest skip Skip test only local test SIGABRT produced test_nan Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y x + linear y x = torch randn device=self device x = float nan example_inputs = x torch randn device=self device check_model Model device example_inputs options= debug_check_inf_and_nan True skipIfWindowsXPU msg= crash Windows XPU test_assert_async device = GPU_TYPE raise unittest SkipTest requires GPU_TYPE Model torch nn Module __init__ - None super __init__ forward x u = x item torch _check u torch ones u x = torch tensor device=self device example_inputs = x check_model Model example_inputs test_simple_dynamic Model torch nn Module __init__ - None super __init__ forward x y add_ = x + y torch nn functional relu input=add_ inplace=False x = torch randn device=self device y = torch randn device=self device dim _x = Dim dim _x min= max= dynamic_shapes = x dim _x y dim _x example_inputs = x y check_model Model example_inputs dynamic_shapes=dynamic_shapes skipIfWindows msg= TODO xuhancn confirm Crash access violation test_large_dynamic_dim Model torch nn Module __init__ - None super __init__ forward x y add_ = x + y torch nn functional relu input=add_ inplace=False x = torch randn device=self device y = torch randn device=self device Use dimension exceeds maximum value C long long ^ - dim _x = Dim dim _x min= max= dynamic_shapes = x dim _x y dim _x example_inputs = x y check_model Model example_inputs dynamic_shapes=dynamic_shapes unittest skipIf PLATFORM_SUPPORTS_FP FP only supported H + SM MI + devices skipIfXpu test_fp cuda only device = cuda Model torch nn Module __init__ dtype super __init__ out_dtype = dtype forward x weight bias scale_a scale_b weight = weight e m _type output = torch _scaled_mm x weight bias=input_bias out_dtype=self out_dtype scale_a=scale_a scale_b=scale_b output dtype = torch float a_scale = torch Tensor device=GPU_TYPE b_scale = torch Tensor device=GPU_TYPE input_bias = torch rand device=GPU_TYPE dtype=dtype weight_shape = weight = torch rand weight_shape device=GPU_TYPE dtype=dtype T a_inverse_scale = a_scale b_inverse_scale = b_scale x_shape = x = torch rand x_shape device=GPU_TYPE dtype=dtype e m _type dim _x = Dim dim _x min= max= dynamic_shapes = dim _x None None None None check_model Model dtype x weight input_bias a_inverse_scale b_inverse_scale dynamic_shapes=dynamic_shapes unittest skipIf TEST_WITH_ROCM IS_SM scaled_grouped_mm only supported SM skipIfXpu test_scaled_grouped_mm Test torch _scaled_grouped_mm AOTI lowering cuda only device = cuda raise unittest SkipTest requires CUDA Model torch nn Module __init__ super __init__ forward x weight scale_a scale_b offsets x num_groups batch in_features - FP inputs weight total_out_features in_features - FP weights transposed scale_a num_groups - input scales scale_b num_groups - weight scales offsets num_groups - cumulative output sizes output = torch _scaled_grouped_mm x weight t scale_a=scale_a scale_b=scale_b offs=offsets use_fast_accum=True output half dtype = torch float num_groups = batch_size = in_features = out_features_list = Different output sizes each group device = GPU_TYPE Calculate offsets cumulative output sizes offsets = torch cumsum torch tensor out_features_list dim= device dtype=torch int total_out_features = sum out_features_list Create FP input tensors - stacked all groups x_fp = torch randn num_groups batch_size in_features dtype=dtype device=device x_fp = x_fp torch float _e m fn Create FP weight tensor - concatenated transposed weight_fp = torch randn total_out_features in_features dtype=dtype device=device weight_fp = weight_fp torch float _e m fn Create scales scale_a = torch ones num_groups batch_size device=device dtype=torch float scale_b = torch ones total_out_features device=device dtype=torch float check_model Model x_fp weight_fp scale_a scale_b offsets unittest skipIf PLATFORM_SUPPORTS_FP FP only supported H + SM MI + devices skipIfXpu test_fp _view_of_param cuda only device = GPU_TYPE Model torch nn Module __init__ dtype weight super __init__ out_dtype = dtype weight = weight forward x bias scale_a scale_b test do view inside graph AOTI needs materialize view before passing into scaled_mm extern kernel weight = weight T output = torch _scaled_mm x weight bias=input_bias out_dtype=self out_dtype scale_a=scale_a scale_b=scale_b output dtype = torch float a_scale = torch Tensor device=self device b_scale = torch Tensor device=self device input_bias = torch rand device=self device dtype=dtype weight_shape = weight = torch rand weight_shape device=self device dtype=dtype e m _type a_inverse_scale = a_scale b_inverse_scale = b_scale x_shape = x = torch rand x_shape device=self device dtype=dtype e m _type dim _x = Dim dim _x min= max= dynamic_shapes = dim _x None None None check_model Model dtype weight x input_bias a_inverse_scale b_inverse_scale dynamic_shapes=dynamic_shapes test_poi_multiple_dynamic Model torch nn Module __init__ - None super __init__ forward x y add_ = x + y torch nn functional relu input=add_ inplace=False x = torch randn device=self device y = torch randn device=self device dim _x = Dim dim _x min= max= dynamic_shapes = x dim _x y dim _x list_example_inputs = x y list_example_inputs append torch randn device=self device torch randn device=self device list_example_inputs append torch randn device=self device torch randn device=self device check_model_with_multiple_inputs Model list_example_inputs dynamic_shapes=dynamic_shapes unittest skipIf IS_BIG_GPU Skipping triton backend only since big GPU enough SM test_addmm_multiple_dynamic device == cpu raise unittest SkipTest using triton backend only supported CPU Model torch nn Module __init__ n k device super __init__ weight = torch randn n k device=device bias = torch randn n device=device forward torch nn functional linear weight bias M = N = K = model = Model N K device batch = = torch randn batch M K device=self device dim _a = Dim dim _a min= max= dynamic_shapes = dim _a list_example_inputs = batch = list_example_inputs append torch randn batch M K device=self device batch = list_example_inputs append torch randn batch M K device=self device check_model_with_multiple_inputs model list_example_inputs dynamic_shapes=dynamic_shapes options= max_autotune True max_autotune_gemm_backends TRITON unittest skipIf IS_BIG_GPU Skipping triton backend only since big GPU enough SM test_bmm_multiple_dynamic device == cpu raise unittest SkipTest using triton backend only supported CPU Model torch nn Module __init__ - None super __init__ forward b torch bmm b M = N = K = model = Model batch = = torch randn batch M K device=self device b = torch randn batch K N device=self device dim _a = Dim dim _a min= max= dynamic_shapes = dim _a b dim _a list_example_inputs = b batch = list_example_inputs append torch randn batch M K device=self device torch randn batch K N device=self device batch = list_example_inputs append torch randn batch M K device=self device torch randn batch K N device=self device check_model_with_multiple_inputs model list_example_inputs options= max_autotune True max_autotune_gemm_backends TRITON dynamic_shapes=dynamic_shapes skipIfWindows msg= TODO xuhancn confirm Crash access violation test_foreach_multiple_dynamic Model torch nn Module __init__ - None super __init__ forward x y x_unsqueeze = torch unsqueeze x dim= y_unsqueeze = torch unsqueeze y dim= cat = torch cat x_unsqueeze y_unsqueeze dim= cat model = Model x = torch randn device=self device y = torch randn device=self device dim _x = Dim dim _x min= max= dynamic_shapes = x dim _x y dim _x list_example_inputs = x y list_example_inputs append torch randn device=self device torch randn device=self device list_example_inputs append torch randn device=self device torch randn device=self device check_model_with_multiple_inputs model list_example_inputs dynamic_shapes=dynamic_shapes scaled_dot_product_flash_attention unittest skipIf HAS_XPU_AND_TRITON SM OrLater bfloat only supported sm + test_sdpa Model torch nn Module __init__ - None super __init__ forward q k v torch nn functional scaled_dot_product_attention q k v example_inputs = torch randn dtype=torch bfloat device=self device torch randn dtype=torch bfloat device=self device torch randn dtype=torch bfloat device=self device check_model Model example_inputs unittest skipIf SM OrLater bfloat only supported sm + unittest skipIf archs where isn t lowered flash attention math backend will used doesn t work bfloat PLATFORM_SUPPORTS_FLASH_ATTENTION Some archs don t support SDPA bfloat test_sdpa_ Model torch nn Module __init__ - None super __init__ forward q k v x t = torch nn functional scaled_dot_product_attention q k v is_causal=True x + t example_inputs = torch randn dtype=torch bfloat device=self device torch randn dtype=torch bfloat device=self device torch randn dtype=torch bfloat device=self device torch randn dtype=torch bfloat device=self device check_model Model example_inputs skipIfNoFBGEMM test_quantized_linear Model torch nn Module __init__ device super __init__ weight = torch randn device=device bias = torch randn device=device forward x torch ops quantized linear_dynamic_fp _unpacked_weight x weight bias example_inputs = torch randn device=self device config patch aot_inductor use_runtime_constant_folding True check_model Model device example_inputs skipIfNoFBGEMM test_quantized_linear_bias_none Model torch nn Module __init__ device super __init__ weight = torch randn device=device forward x torch ops quantized linear_dynamic_fp _unpacked_weight x weight None example_inputs = torch randn device=self device config patch aot_inductor use_runtime_constant_folding True check_model Model device example_inputs skipIfNoFBGEMM test_quanatized_int _linear Model torch nn Module __init__ device super __init__ weight = torch randn device=device bias = torch randn device=device input_scale = torch tensor input_zero_point = torch tensor weight_scale = torch tensor weight_zero_point = torch tensor output_scale = torch tensor output_zero_point = torch tensor out_channel = forward x torch ops _quantized wrapped_quantized_linear x input_scale input_zero_point weight weight_scale weight_zero_point bias output_scale output_zero_point out_channel example_inputs = torch randn device=self device config patch aot_inductor use_runtime_constant_folding True check_model Model device example_inputs test_zero_grid_with_unbacked_symbols Repro torch nn Module __init__ - None super __init__ forward x y nz = torch nonzero x b = torch ones_like nz dtype=torch float c = torch zeros_like nz dtype=torch float d = b + c y d sum example_inputs = torch tensor device=self device torch randn dtype=torch float device=self device check_model Repro example_inputs skipIfMPS config patch unbacked_symint_fallback parametrize shift_k parametrize use_static_size True False test_unbacked_expr_replacements shift_k use_static_size Test parameters - shift_k Validates torch _check assertion order doesn t affect results shifting order torch _checks - use_static_size Tests torch _check compatibility between unbacked symbolic expressions static shapes device = GPU_TYPE raise unittest SkipTest Need triton user-defined triton kernel realize_out_tensor_with_size size STATIC_DIM = large enough hit IMA w o compute-sanitizer tensor = torch ones size STATIC_DIM device=self device Realize tensor intermediate buffer nrows ncols = tensor shape numel = tensor numel add_kernel nrows in_ptr =tensor in_ptr =tensor out_ptr=tensor n_elements=numel BLOCK_SIZE=ncols tensor Repro torch nn Module forward x y lst STATIC_SIZE = s s = x shape s s = y shape u u u u u = lst tolist expr = s + u expr = s + u expr = s s + u u make one lil complicated expr = STATIC_SIZE use_static_size u t = realize_out_tensor_with_size expr t = realize_out_tensor_with_size expr t = realize_out_tensor_with_size expr t = realize_out_tensor_with_size expr shift tensors change up torch _check order tensors = t t t t shifted_tensors = tensors shift_k + tensors shift_k torch cat implicitly runs torch _check lhs == rhs cat = torch cat shifted_tensors dim= cat cat Disable cuda caching allocator check IMA torch cuda caching_allocator_enable False model = Repro example_inputs = s s torch randn device=self device s s torch randn device=self device u u u u u torch tensor device=self device dtype=torch int spec = x Dim DYNAMIC Dim DYNAMIC y Dim DYNAMIC Dim DYNAMIC lst Dim STATIC check_model model example_inputs dynamic_shapes=spec torch cuda caching_allocator_enable True skipIfMPS config patch unbacked_symint_fallback config patch triton autotune_at_compile_time None test_replace_unbacked_symbol_with_backed_expr This will test how autotune_at_compile_time generates sample inputs when user torch _checks s + s == u We may fail IMA generated input sizes aren t correct device = GPU_TYPE raise unittest SkipTest requires triton force_realize tensor Realize tensor intermediate buffer nrows ncols = tensor shape numel = tensor numel add_kernel nrows in_ptr =tensor in_ptr =tensor out_ptr=tensor n_elements=numel BLOCK_SIZE=ncols INNER_DIM = Repro torch nn Module forward x y lengths Realize intermediate buffer backed shape s + s relevant_embeddings = torch cat x y dim= force_realize relevant_embeddings Realize intermediate buffer unbacked shape u num_relevant_embeddings = lengths nonzero size ones = torch ones num_relevant_embeddings INNER_DIM device=x device force_realize ones Add deferred runtime assertion s + s == u torch _check relevant_embeddings size == ones size relevant_embeddings += ones relevant_embeddings relevant_embeddings torch cuda caching_allocator_enable False model = Repro example_inputs = torch randn INNER_DIM device=self device torch randn INNER_DIM device=self device torch ones spec = x Dim DYNAMIC Dim STATIC y Dim DYNAMIC Dim STATIC lengths Dim DYNAMIC check_model model example_inputs dynamic_shapes=spec torch cuda caching_allocator_enable True config patch triton autotune_at_compile_time None test_stride_with_unbacked_expr Repro torch nn Module forward x y u = x item torch _check u = s = y size expr = u s sevens = torch empty_strided size= expr stride= expr device=x device fill_ sevens example_inputs = torch scalar_tensor dtype=torch int device=self device torch ones device=self device check_model Repro example_inputs unittest skipIf TEST_MPS MACOS_VERSION bfloat only supported MacOS + test_size_with_unbacked_add_expr Tests AOTI autotuning make sure correct input tensor sizes generated sizes include expr such s + u Repro torch nn Module forward values repeats mask embeddings x z scalar repeat_interleave = torch repeat_interleave values repeats index = torch clamp repeat_interleave min= max= int index_select = torch index_select embeddings index backed = z size unbacked = scalar item unbacked_add_expr = backed + unbacked repeated = x repeat unbacked_add_expr torch cat repeated index_select dim= example_inputs = torch ones dtype=torch int device=self device torch ones dtype=torch int device=self device torch ones dtype=torch int device=self device bool torch randn dtype=torch bfloat device=self device torch randn dtype=torch bfloat device=self device torch ones dtype=torch int device=self device torch scalar_tensor dtype=torch int device=self device spec = values Dim DYNAMIC repeats Dim DYNAMIC mask Dim DYNAMIC embeddings Dim DYNAMIC Dim STATIC x Dim STATIC Dim STATIC z Dim DYNAMIC Dim STATIC scalar check_model Repro example_inputs dynamic_shapes=spec skipIfWindowsXPU msg= crash Windows XPU test_size_with_unbacked_add_expr_transitive Edge case torch _check expr expr + torch _check expr unbacked When generating example input sizes autotuning should coalesce expr expr unbacked into single size device = GPU_TYPE raise unittest SkipTest requires GPU Repro torch nn Module forward values repeats mask embeddings x y z lst index = torch repeat_interleave values repeats index_select = torch index_select embeddings index u u = lst tolist backed backed = z size z size repeated = y repeat backed + u repeated = x repeat backed + u out = torch empty_like repeated add_kernel out numel repeated repeated out out numel BLOCK_SIZE= Implicitly add torch _check expr unbacked cat = torch cat out index_select dim= add = repeated + repeated Explicitly add torch _check expr expr torch _check repeated size == out size cat add example_inputs = torch ones dtype=torch int device=self device torch ones dtype=torch int device=self device torch ones dtype=torch int device=self device bool torch randn dtype=torch bfloat device=self device torch randn dtype=torch bfloat device=self device torch randn dtype=torch bfloat device=self device torch ones dtype=torch int device=self device torch tensor dtype=torch int device=self device spec = values Dim DYNAMIC repeats Dim DYNAMIC mask Dim DYNAMIC embeddings Dim DYNAMIC Dim STATIC x Dim DYNAMIC Dim STATIC y Dim DYNAMIC Dim STATIC z Dim DYNAMIC Dim DYNAMIC lst Dim STATIC check_model Repro example_inputs dynamic_shapes=spec config patch unbacked_symint_fallback test_size_with_unbacked_add_and_mul_expr Edge case torch _check add_expr mul_expr When generating example input sizes autotuning make sure they coalesce into single size device = GPU_TYPE raise unittest SkipTest requires GPU Repro torch nn Module forward values repeats mask embeddings x y z lst u u u = lst tolist backed = z size backed = z size unbacked_add_expr = backed + u unbacked_mul_expr = backed + u u repeated = x repeat unbacked_add_expr repeated = y repeat unbacked_mul_expr out = torch empty_like repeated out = torch empty_like repeated add_kernel out numel repeated repeated out out numel BLOCK_SIZE= add_kernel out numel repeated repeated out out numel BLOCK_SIZE= torch cat out out dim= example_inputs = torch ones dtype=torch int device=self device torch ones dtype=torch int device=self device torch ones dtype=torch int device=self device bool torch randn dtype=torch bfloat device=self device torch randn dtype=torch bfloat device=self device torch randn dtype=torch bfloat device=self device torch ones dtype=torch int device=self device torch tensor dtype=torch int device=self device spec = values Dim DYNAMIC repeats Dim DYNAMIC mask Dim DYNAMIC embeddings Dim DYNAMIC Dim STATIC x Dim DYNAMIC Dim STATIC y Dim DYNAMIC Dim STATIC z Dim DYNAMIC Dim DYNAMIC lst Dim STATIC check_model Repro example_inputs dynamic_shapes=spec skipIfXpu msg= _scaled_dot_product_flash_attention supported XPU yet unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Some archs don t support flash SDPA test_fallback_kernel_with_symexpr_output device = GPU_TYPE raise unittest SkipTest requires GPU Module torch nn Module forward q k v q = q reshape q shape q shape q shape q shape k = k reshape k shape k shape k shape k shape v = v reshape v shape v shape v shape v shape res = torch ops aten _scaled_dot_product_flash_attention default q k v res m = Module device=self device tensor_shape = inputs = torch randn tensor_shape dtype=torch float device=self device torch randn tensor_shape dtype=torch float device=self device torch randn tensor_shape dtype=torch float device=self device dynamic_shapes = q Dim DYNAMIC Dim DYNAMIC k Dim DYNAMIC Dim DYNAMIC v Dim DYNAMIC Dim DYNAMIC ep = torch export export m inputs dynamic_shapes=dynamic_shapes strict=False path = torch _inductor aot_compile ep module inputs aot_model = torch _export aot_load path device=self device torch testing assert_close m inputs aot_model inputs test_aoti_constant_tensor Foo torch nn Module __init__ device super __init__ = torch ones device=device b = torch ones device=device forward x torch ops aten linear default x b example_inputs = torch ones device=self device check_model Foo device example_inputs test_aoti_constant_tensor_name_collision SubModule torch nn Module __init__ device super __init__ register_buffer _tensor_constant torch ones device=device dtype=torch float persistent=True forward x linear x Foo torch nn Module __init__ user_float_feature_idx device super __init__ user_float_feature_idx = user_float_feature_idx register_buffer _tensor_constant torch ones device=device dtype=torch float persistent=True register_buffer _tensor_constant torch ones device=device dtype=torch float persistent=True sub_mod = SubModule device forward x _tensor_constant = torch index_select x torch tensor user_float_feature_idx device=x device _tensor_constant _tensor_constant sub_mod _tensor_constant example_inputs = torch ones device=self device user_float_feature_idx = we have have run_decomposition first trigger name collision ep = torch export export Foo user_float_feature_idx device example_inputs strict=False run_decompositions gm = ep module check_model gm device example_inputs test_large_grid device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module __init__ - None super __init__ forward primals_ view = torch ops aten reshape default primals_ - primals_ = None permute = torch ops aten permute default view clone = torch ops aten clone default permute memory_format=torch contiguous_format clone let y_grid = s = s = example_inputs = torch rand s s device=self device check_model Model example_inputs test_cond_simple inputs = torch randn device=self device torch randn device=self device dim _ab = Dim s min= max= dynamic_shapes = p dim _ab None b dim _ab None check_model_with_multiple_inputs CondModels Simple prepend_predicates inputs dynamic_shapes=dynamic_shapes test_cond_nested inputs = torch randn device=self device torch randn device=self device torch randn device=self device dim _abc = Dim s min= max= dynamic_shapes = p p p dim _abc None b dim _abc None c dim _abc None check_model_with_multiple_inputs CondModels Nested prepend_predicates inputs num_predicates= dynamic_shapes=dynamic_shapes test_cond_with_parameters inputs = torch randn device=self device dim _abc = Dim s min= max= dynamic_shapes = p dim _abc None check_model_with_multiple_inputs CondModels Parameters device prepend_predicates inputs dynamic_shapes=dynamic_shapes test_cond_with_reinterpret_view_inputs_outputs inputs = torch randn device=self device torch randn device=self device TODO min value need because body_fn we re slicing over z since output size dim _ab- when we extract tensor metadata out output we call guard_size_oblivious which assumes dim _ab- = So we have set minimum now We need relax restriction either writing less constrained shape checking fake impl cond dim _ab = Dim s min= max= dynamic_shapes = p dim _ab None b dim _ab None check_model_with_multiple_inputs CondModels ReinterpretView prepend_predicates inputs dynamic_shapes=dynamic_shapes test_cond_with_multiple_outputs inputs = torch randn device=self device torch randn device=self device torch randn device=self device dim _ab = Dim s min= max= dim _c = Dim s min= max= dynamic_shapes = p dim _ab None b dim _ab None c dim _c None check_model_with_multiple_inputs CondModels MultipleOutputs prepend_predicates inputs dynamic_shapes=dynamic_shapes test_cond_with_outer_code_before_after inputs = torch randn device=self device torch randn device=self device dim _ab = Dim s min= max= dynamic_shapes = p dim _ab None b dim _ab None check_model_with_multiple_inputs CondModels OuterCode prepend_predicates inputs dynamic_shapes=dynamic_shapes test_cond_use_buffers_from_outer_scope inputs = torch randn device=self device torch randn device=self device torch randn device=self device dim _abc = Dim s min= max= dynamic_shapes = p dim _abc None b dim _abc None c dim _abc None check_model_with_multiple_inputs CondModels OuterBuffers prepend_predicates inputs dynamic_shapes=dynamic_shapes common_utils parametrize dynamic False True test_cond_non_tensor_predicates dynamic inputs = torch randn device=self device torch randn device=self device inputs = torch randn device=self device torch randn device=self device inputs = inputs dynamic_shapes = None dynamic inputs = inputs inputs dim _a = Dim s min= max= dim _b = Dim s min= max= dynamic_shapes = dim _a None b dim _b None check_model_with_multiple_inputs CondModels WithNonTensorPredicate inputs dynamic_shapes=dynamic_shapes common_utils parametrize dynamic False True test_cond_unbacked_symint_closure dynamic inputs = torch randn device=self device torch randn device=self device torch randn device=self device dynamic_shapes = None dynamic dim _a = Dim s min= max= dim _b = Dim s min= max= dynamic_shapes = p x dim _a None y dim _b None z dim _a None check_model_with_multiple_inputs CondModels UnbackedSymIntClosure prepend_predicates inputs dynamic_shapes=dynamic_shapes skipIfWindows msg= TODO xuhancn confirm Crash access violation common_utils parametrize dynamic False True test_cond_mismatched_branch_output dynamic inputs = torch randn device=self device torch randn device=self device torch randn device=self device dynamic_shapes = None dynamic Note minimum has because model slicing over first dim first dim slicing will specialized causing constraint violation error dim _a = Dim s min= max= dim _b = Dim s min= max= dynamic_shapes = p x dim _a None y dim _b None z dim _a None check_model_with_multiple_inputs CondModels MismatchedOutputSize prepend_predicates inputs dynamic_shapes=dynamic_shapes test_cond_symint_input M torch nn Module forward x y z = y shape b = z shape true_fn x x + false_fn x x + b z torch cond x shape true_fn false_fn x input = torch ones device=self device torch ones device=self device torch ones device=self device input = torch ones device=self device torch ones device=self device torch ones device=self device inputs = input input dynamic_shapes = x Dim d y Dim d z Dim d check_model_with_multiple_inputs M inputs dynamic_shapes=dynamic_shapes test_cond_symint_input_disable_one_pass M torch nn Module forward x y z = y shape b = z shape true_fn x x + false_fn x x + b z torch cond x shape true_fn false_fn x input = torch ones device=self device torch ones device=self device torch ones device=self device input = torch ones device=self device torch ones device=self device torch ones device=self device inputs = input input dynamic_shapes = x Dim d y Dim d z Dim d torch _inductor config patch triton autotune_at_compile_time False check_model_with_multiple_inputs M inputs dynamic_shapes=dynamic_shapes test_while_loop_simple inputs = torch randn device=self device torch randn device=self device dim _ab = Dim s min= max= dynamic_shapes = ci dim _ab None b dim _ab None check_model_with_multiple_inputs WhileLoopModels Simple prepend_counters inputs dynamic_shapes=dynamic_shapes test_while_loop_nested inputs = torch randn device=self device torch randn device=self device dim _ab = Dim s min= max= dynamic_shapes = ci cj dim _ab None b dim _ab None check_model_with_multiple_inputs WhileLoopModels Nested prepend_counters inputs num_counters= dynamic_shapes=dynamic_shapes test_while_loop_with_outer_code inputs = torch randn device=self device torch randn device=self device dim _ab = Dim s min= max= dynamic_shapes = c dim _ab None b dim _ab None check_model_with_multiple_inputs WhileLoopModels OuterCode prepend_counters inputs dynamic_shapes=dynamic_shapes mps doesn t support float skipIfMPS unittest skipIf config triton native_matmul FIXME cannot do get_size FakeTensor during lowering test_while_loop_with_parameters inputs = torch randn dtype=torch float device=self device dim _a = Dim s min= max= dynamic_shapes = c dim _a None check_model_with_multiple_inputs WhileLoopModels Parameters device prepend_counters inputs dynamic_shapes=dynamic_shapes test_while_loop_with_outer_buffers inputs = torch randn device=self device torch randn device=self device dynamic shapes don t work now due https github com pytorch pytorch issues dim _ab = Dim s min= max= dynamic_shapes = c dim _ab None b dim _ab None dynamic_shapes = None check_model_with_multiple_inputs WhileLoopModels OuterBuffers prepend_counters inputs dynamic_shapes=dynamic_shapes test_while_loop_with_pytree_inputs inputs = torch tensor device=self device torch randn device=self device x torch randn device=self device y torch randn device=self device check_model_with_multiple_inputs WhileLoopModels PytreeCarry inputs dynamic_shapes=None common_utils parametrize dynamic False True test_while_loop_with_unbacked_symint_closure dynamic inputs = torch randn device=self device torch randn device=self device dim _ab = Dim s min= max= dynamic_shapes = None dynamic dynamic_shapes = c dim _ab None b dim _ab None check_model_with_multiple_inputs WhileLoopModels UnbackedSymIntClosure prepend_counters inputs dynamic_shapes=dynamic_shapes common_utils parametrize dynamic False True test_while_loop_with_mixed_device dynamic inputs = torch randn device=self device torch randn device=self device dim _ab = Dim s min= max= dynamic_shapes = None dynamic dynamic_shapes = c dim _ab None b dim _ab None check_model_with_multiple_inputs WhileLoopModels MixedDevice prepend_counters inputs dynamic_shapes=dynamic_shapes common_utils parametrize dynamic False True test_while_loop_with_sym_expr_cond dynamic inputs = torch randn device=self device torch randn device=self device dim _ab = Dim s min= max= dynamic_shapes = None dynamic dynamic_shapes = c dim _ab None b dim _ab None check_model_with_multiple_inputs WhileLoopModels SymExprCond prepend_counters inputs dynamic_shapes=dynamic_shapes common_utils parametrize dynamic False True test_while_loop_with_conv dynamic inputs = torch randn device=self device dtype=torch float dim _ab = Dim s min= max= dynamic_shapes = None dynamic dynamic_shapes = c x dim _ab None check_model_with_multiple_inputs WhileLoopModels Conv device prepend_counters inputs dynamic_shapes=dynamic_shapes config patch is_predispatch True test_constant M torch nn Module __init__ device super __init__ device = device forward x t = torch tensor x size - device=self device dtype=torch float t = torch sqrt t x t check_model M device torch randn device=self device unittest skipIf IS_MACOS no CUDA Mac test_zero_grid_with_backed_symbols device = GPU_TYPE raise unittest SkipTest requires GPU Repro torch nn Module __init__ - None super __init__ forward x b x + b example_inputs = torch randn device=self device torch randn device=self device dynamic_shapes = x Dim dx Dim STATIC b None Compile run model where dynamic dim size package_path str = AOTIRunnerUtil compile Repro example_inputs dynamic_shapes=dynamic_shapes aot_inductor_module = torch _inductor aoti_load_package package_path aot_inductor_module example_inputs Re-run where dynamic dim size example_inputs = torch randn device=self device torch randn device=self device actual = aot_inductor_module example_inputs expected = Repro example_inputs torch testing assert_close actual expected test_repeat_interleave Repro torch nn Module __init__ - None super __init__ forward x torch ops aten repeat_interleave Tensor x output_size= example_inputs = torch ones dtype=torch int device=self device check_model Repro example_inputs test_dynamic_cat Model torch nn Module __init__ - None super __init__ forward b torch cat b dim= = torch randn device=self device b = torch randn device=self device dim _a = Dim dim _a min= max= dim _b = Dim dim _b min= max= dynamic_shapes = dim _a b dim _b example_inputs = b check_model Model example_inputs dynamic_shapes=dynamic_shapes test_buffer_mutation_ Model torch nn Module __init__ device super __init__ foo = torch nn Buffer torch randn device=device forward x foo add_ foo + x example_inputs = torch rand device=self device check_model Model device example_inputs test_non_tensor_input Model torch nn Module forward b alpha= torch add b alpha=alpha = torch randn device=self device b = torch randn device=self device simdlen None torch _inductor config patch cpp simdlen simdlen so_path = torch _export aot_compile torch ops aten add args= b kwargs= alpha kernel_runner = AOTIRunnerUtil legacy_load_runner device so_path res = kernel_runner run b assertTrue isinstance res list assertTrue len res == assertEqual Model b alpha= res test_buffer_mutation_ Model torch nn Module __init__ device super __init__ foo = torch nn Buffer torch arange device=device bar = torch nn Buffer torch arange device=device forward x bar mul_ foo = bar x + bar x foo example_inputs = torch randn device=self device check_model Model device example_inputs skipIfWindows msg= OpenMP crashed application windows TODO xuhancn need root cause fix test_buffer_mutation_ KVCache torch nn Module __init__ max_batch_size max_seq_length n_heads head_dim dtype=torch float super __init__ cache_shape = max_batch_size n_heads max_seq_length head_dim k_cache = torch nn Buffer torch zeros cache_shape dtype=dtype v_cache = torch nn Buffer torch zeros cache_shape dtype=dtype update input_pos k_val v_val input_pos S k_val B H S D k_out = k_cache v_out = v_cache k_out input_pos = k_val v_out input_pos = v_val k_out v_out Model torch nn Module __init__ device super __init__ kv_cache = KVCache forward inp_pos k v kv_cache update inp_pos k v kv_cache k_cache + kv_cache v_cache example_inputs = torch tensor device=self device torch randn device=self device torch randn device=self device model = Model device check_model model example_inputs code_check_count model example_inputs empty_strided test_buffer_mutation_ device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module __init__ - None super __init__ register_buffer _tensor_constant torch randint size= dtype=torch int device= cpu forward x x + _tensor_constant torch device type=GPU_TYPE index= example_inputs = torch randint size= dtype=torch int device=GPU_TYPE torch _export aot_compile Model example_inputs skipCUDAIf True Test x backend unittest skipIf IS_FBCODE Need newer ideep test_buffer_mutation_and_force_mmap_weights Model nn Module __init__ super __init__ linear = torch nn Linear linear = torch nn Linear forward x x = linear x out = linear x out example_inputs = torch randn model = Model eval config patch freezing True aot_inductor force_mmap_weights True torch no_grad exported_model = export model example_inputs strict=True module quantizer = X InductorQuantizer quantizer set_global xiq get_default_x _inductor_quantization_config reduce_range=True prepared_model = prepare_pt e exported_model quantizer prepared_model example_inputs converted_model = convert_pt e prepared_model torch ao quantization move_exported_model_to_eval converted_model check_model converted_model example_inputs skipIfMPS test_fallback_mem_leak_fix device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module __init__ - None super __init__ forward x y idx tmp = x + y w = torch ops aten as_strided tmp x shape x stride out = torch ops aten index Tensor w idx w out example_inputs = torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch randn device=GPU_TYPE dim = Dim dim min= max= dynamic_shapes = x dim y dim idx dim package_path str = AOTIRunnerUtil compile Model example_inputs dynamic_shapes=dynamic_shapes aot_inductor_module = torch _inductor aoti_load_package package_path device_interface = get_interface_for_device GPU_TYPE device int = device_interface current_device mem_before = device_interface memory_allocated device aot_inductor_module example_inputs mem_after = device_interface memory_allocated device assertEqual mem_before mem_after actual = aot_inductor_module example_inputs expected = Model example_inputs torch testing assert_close actual expected requires_multigpu skipIfMPS test_replicate_on_devices device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module __init__ w w super __init__ w = w w = w forward x y = x w b = y w + b w = torch randn w = torch randn inputs = torch randn torch randn result_cpu = Model w w inputs Compile model AOTInductor device_interface = get_interface_for_device GPU_TYPE device_interface device package_path = AOTIRunnerUtil compile model=Model w torch device GPU_TYPE w torch device GPU_TYPE example_inputs=tuple t torch device GPU_TYPE t inputs Run model gpu N i range device_interface device_count device_interface device i example_inputs = tuple t torch device GPU_TYPE i t inputs optimized = torch _inductor aoti_load_package package_path result_gpu = optimized example_inputs assertTrue same result_cpu result_gpu cpu requires_multigpu skipIfMPS test_on_gpu_device device = GPU_TYPE raise unittest SkipTest requires GPU device_interface = get_interface_for_device GPU_TYPE try device_interface get_device_properties except AssertionError raise unittest SkipTest GPU device available None Model torch nn Module __init__ super __init__ fc = torch nn Linear relu = torch nn ReLU fc = torch nn Linear sigmoid = torch nn Sigmoid forward x x = fc x x = relu x x = fc x x = sigmoid x x device = f GPU_TYPE model = Model device example_inputs = torch randn device=device expected = model example_inputs so_path = AOTIRunnerUtil legacy_compile model example_inputs optimized = AOTIRunnerUtil legacy_load device so_path actual = optimized example_inputs torch testing assert_close actual expected test_pytree_inputs M torch nn Module __init__ - None super __init__ forward x dict str torch Tensor device = next iter x values device add_ = torch zeros device=device mul_ = torch ones device=device v x values add_ += v mul_ = v add_ mul_ check_model M x torch ones device=self device y torch ones device=self device requires_multigpu test_non_default_gpu_device device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module __init__ weight super __init__ weight = weight forward x y x + torch nn functional linear y weight weight = torch randn inputs = torch randn torch randn result_cpu = Model weight inputs device_interface = get_interface_for_device GPU_TYPE device_interface device torch no_grad result_gpu_ = AOTIRunnerUtil run Model weight torch device GPU_TYPE tuple t torch device GPU_TYPE t inputs device_interface device torch no_grad result_gpu_ = AOTIRunnerUtil run Model weight torch device GPU_TYPE tuple t torch device GPU_TYPE t inputs assertTrue same result_cpu result_gpu_ cpu assertTrue same result_cpu result_gpu_ cpu requires_multigpu test_load_package_multiple_gpus device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module __init__ weight super __init__ weight = weight forward x y x + torch nn functional linear y weight weight = torch randn device=self device inputs = torch randn device=self device torch randn device=self device model = Model weight device=self device result_ref = model inputs package_path = AOTIRunnerUtil compile model inputs Load AOT package gpu N device_interface = get_interface_for_device GPU_TYPE i range device_interface device_count device = torch device GPU_TYPE i device_interface device i torch no_grad model_package = torch _inductor aoti_load_package package_path device_index=i inputs_on_device = input device=device input inputs result_package = model_package inputs_on_device assertTrue same result_ref cpu result_package cpu unittest skipIf config triton native_matmul sin mm fused native matmul test_reuse_kernel Model torch nn Module __init__ - None super __init__ forward x y = torch sin x b = torch mm y c = torch sin b d = torch mm b c d example_inputs = torch randn device=self device torch randn device=self device model = Model e- tol value used pytorch torch _dynamo utils py check_model model example_inputs atol= e- rtol= e- device == mps code_check_count model example_inputs aoti_torch_mps_get_kernel_function device == GPU_TYPE code_check_count model example_inputs triton_poi_fused_sin_ = loadKernel test_reuse_kernel_dynamic Model torch nn Module __init__ device super __init__ cst = torch randn device=device dtype=torch float weights = torch randn device=device dtype=torch float cst_ = torch randn device=device dtype=torch float weights_ = torch randn device=device dtype=torch float forward x y z dim = x size add_ = z + z expand_ = add_ expand - - s mul_ = add_ expand_ s permute_ = torch permute mul_ s bmm_ = torch bmm permute_ weights add_ = bmm_ + cst reshape_ = torch reshape add_ dim s permute_ = torch permute reshape_ mul_ = permute_ reshape_ = torch reshape y dim s permute_ = torch permute reshape_ bmm_ = torch bmm mul_ permute_ add_ _ = z + z expand_ _ = add_ _ expand - - s mul_ _ = add_ _ expand_ _ s permute_ _ = torch permute mul_ _ s bmm_ _ = torch bmm permute_ _ weights_ add_ _ = bmm_ _ + cst_ reshape_ _ = torch reshape add_ _ dim s permute_ _ = torch permute reshape_ _ mul_ _ = permute_ _ reshape_ _ = torch reshape y dim s permute_ _ = torch permute reshape_ _ bmm_ _ = torch bmm mul_ _ permute_ _ bmm_ + bmm_ _ x = torch randn device=self device dtype=torch float y = torch randn device=self device dtype=torch float z = torch randn device=self device dtype=torch float dim = Dim dim min= max= dynamic_shapes = x dim y dim z dim example_inputs = x y z model = Model device dtype=torch float check_model model example_inputs dynamic_shapes=dynamic_shapes atol= e- rtol= e- test_fake_tensor_device_validation device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module __init__ - None super __init__ forward x y x + y example_inputs = torch randn torch randn Export CPU exported_program = export Model example_inputs strict=True Compile exported model GPU gm = exported_program graph_module device assertRaisesRegex ValueError Device mismatch between fake input torch _inductor aot_compile gm tuple i device i example_inputs test_fx_gm_return_tuple_validation torch fx experimental proxy_tensor make_fx Model torch nn Module __init__ - None super __init__ forward x y x + y example_inputs = torch randn torch randn gm = make_fx Model tracing_mode= symbolic example_inputs assertRaisesRegex AssertionError r Graph output must tuple\ \ This so we can avoid pytree processing outputs torch _inductor aot_compile gm example_inputs test_consecutive_compiles Test compilation behaves correctly cache hits TestModule torch nn Module __init__ - None super __init__ forward x x + mod = TestModule inp = torch rand mod inp mod = torch fx symbolic_trace mod concrete_args= inp so = torch _export aot_compile mod inp assert so None compile nd time cache hit so = torch _export aot_compile mod inp assert so None test_normal_functional Model torch nn Module __init__ - None super __init__ forward x torch ops aten normal_functional default x check_model Model torch empty device=self device test_empty_graph Model torch nn Module __init__ - None super __init__ forward x x example_inputs = torch randn device=self device check_model Model example_inputs patch torch _dynamo utils CompileEventLogger log_instant_event test_backward_no_op_logging mock_log_instant_event Model torch nn Module __init__ - None super __init__ forward x x model = Model dummy_input = torch randn torch _dynamo utils CompileEventLogLevel torch _inductor compile_fx graph_module = torch fx symbolic_trace model compile_fx _compile_fx_inner graph_module dummy_input mock_log_instant_event assert_called_once_with backward no-op metadata= compile_id None log_level=CompileEventLogLevel PT _COMPILE unittest skipIf IS_FBCODE Not runnable fbcode test_dup_unbacked_sym_decl Model torch nn Module __init__ - None super __init__ forward x abs_ = torch ops aten abs default x lt = torch ops aten lt Scalar abs_ eq = torch ops aten eq Scalar lt index_ = torch ops aten index Tensor x eq sin = torch ops aten sin default index_ index_ = torch ops aten index Tensor x eq div_ = torch ops aten div Tensor sin index_ div_ example_inputs = torch randn device check_model Model example_inputs This exercises _eliminate_unbacked path ShapeEnv unittest skipIf IS_FBCODE Not runnable fbcode test_dup_unbacked_sym_decl_with_refinement Model torch nn Module __init__ - None super __init__ forward x abs_ = torch ops aten abs default x lt = torch ops aten lt Scalar abs_ eq = torch ops aten eq Scalar lt index_ = torch ops aten index Tensor x eq torch _check index_ size == sin = torch ops aten sin default index_ index_ = torch ops aten index Tensor x eq div_ = torch ops aten div Tensor sin index_ div_ example_inputs = torch ones device check_model Model example_inputs test_run_with_grad_enabled Model torch nn Module forward x weight bias torch ops aten addmm bias weight x m = Model device=self device x = torch rand device=self device requires_grad=True weight = torch rand device=self device requires_grad=True bias = torch rand device=self device requires_grad=True example_inputs = x weight bias expected = m example_inputs expected = pytree tree_leaves expected compiler under no_grad torch no_grad package_path = AOTIRunnerUtil compile m example_inputs run under grad enabled assertTrue torch is_grad_enabled optimized = torch _inductor aoti_load_package package_path actual = optimized example_inputs actual = pytree tree_leaves actual assertTrue same actual expected test_return_constant Model torch nn Module __init__ device super __init__ cst = torch randn device=device forward x = cst clone x x = torch randn device=self device check_model Model device x test_return_view_constant Model torch nn Module __init__ device super __init__ cst = torch randn device=device forward x = torch transpose cst x x = torch randn device=self device check_model Model device x test_profile_benchmark_harness batch_size = seq_length = hidden_size = create_test_fn test_fn inp = torch randn batch_size seq_length hidden_size device=self device weight = torch randn hidden_size hidden_size device=self device matmul_output = inp weight torch nn LayerNorm hidden_size device=self device matmul_output True test_fn fn = torch compile options= profile_bandwidth_output foo benchmark_harness False create_test_fn fn test_with_profiler Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y x + linear y example_inputs = torch randn device=self device torch randn device=self device config patch profile_bandwidth profile_bandwidth_regex check_model Model example_inputs test_with_no_triton_profiler Model torch nn Module __init__ - None super __init__ forward x torch permute x example_inputs = torch randn device=self device config patch profile_bandwidth profile_bandwidth_regex check_model Model example_inputs test_repeat_output Model torch nn Module __init__ - None super __init__ forward x y = torch sin x y y example_inputs = torch randn device=self device check_model Model example_inputs test_repeated_calling device = cuda raise unittest SkipTest requires CUDA Model torch nn Module __init__ - None super __init__ forward x torch sin x example_inputs = torch randn device=self device optimized = torch _inductor aoti_load_package torch _inductor aoti_compile_and_package torch export export Model example_inputs strict=True try torch cuda memory empty_cache torch cuda memory _record_memory_history context=None _ range optimized example_inputs finally torch cuda memory _record_memory_history False segments = torch cuda memory _snapshot segments assertEqual segments requested_size test_view_outputs Model torch nn Module forward x y = torch sin x y_same_size = y view y shape y_diff_size = y view y shape y y_same_size y_diff_size example_inputs = torch randn device=self device check_model Model example_inputs skip_if_no_torchvision test_missing_cubin torchvision models resnet Bottleneck ResNet Model ResNet __init__ - None super __init__ block=Bottleneck layers= replace_stride_with_dilation= False False True norm_layer=None forward x x = conv x x = bn x x = relu x f = x x = maxpool x x = layer x f = x x = layer x f = x x = layer x x = layer x f = x f f f f Call eval here so batch_norm won t update running stats Use float avoid numeric difference failure dtype = torch float device == mps torch float model = Model device=self device dtype=dtype eval example_inputs = torch randn device=self device dtype=dtype check_model model example_inputs test_triton_next_power_of_ device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module forward b lengths n_elements = numel out = torch empty_like max_len = int lengths max scaling_factor = triton next_power_of_ max_len add_kernel_with_scaling n_elements b out n_elements scaling_factor BLOCK_SIZE= out example_inputs = torch randn device=self device torch randn device=self device torch arange end= device=self device check_model Model example_inputs common_utils parametrize minmax min max skipIfWindowsXPU msg= crash Windows XPU test_sympy_cpp_printer_min_max minmax device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module forward b ranks n_elements = numel out = torch empty_like backed = size unbacked = int ranks max scaling_factor = minmax backed unbacked add_kernel_with_scaling n_elements b out n_elements scaling_factor BLOCK_SIZE= out example_inputs = torch randn device=self device torch randn device=self device torch arange end= device=self device dtype=torch int torch _dynamo mark_dynamic example_inputs torch _dynamo mark_dynamic example_inputs check_model Model example_inputs skipIfMPS common_utils parametrize grid_type common_utils parametrize num_dims common_utils parametrize dynamic False True common_utils parametrize autotune False True test_triton_kernel grid_type num_dims dynamic autotune device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module __init__ - None super __init__ forward x y output = torch zeros_like x autotune num_dims == x_elements = output size y_elements = output size n_elements = output numel Select grid autotune num_dims == grid_type == grid = x_elements y_elements grid_type == grid = lambda meta noqa E triton cdiv x_elements meta BLOCK_SIZE_X triton cdiv y_elements meta BLOCK_SIZE_Y grid_fn meta triton cdiv x_elements meta BLOCK_SIZE_X triton cdiv y_elements meta BLOCK_SIZE_Y grid = grid_fn grid_type == grid = n_elements grid_type == grid = lambda meta noqa E triton cdiv n_elements meta BLOCK_SIZE grid_fn meta triton cdiv n_elements meta BLOCK_SIZE grid = grid_fn Select kernel autotune num_dims == add_kernel_autotuned grid x y output n_elements add_kernel_ d_autotuned grid x y output x_elements y_elements add_kernel grid x y output n_elements BLOCK_SIZE= output dims = num_dims x = torch randn dims device=self device y = torch randn dims device=self device dynamic_shapes = dynamic dim _x = Dim dim _x min= max= dim _y = Dim dim _y min= max= dynamic_shapes = x dim _x y dim _y check_model Model x y dynamic_shapes=dynamic_shapes test_triton_kernel_dynamic_shape_with_div device = GPU_TYPE raise unittest SkipTest requires GPU triton jit pass_kernel x num pass Model torch nn Module __init__ - None super __init__ forward x num = x numel grid = lambda meta triton cdiv num noqa E pass_kernel grid x num x x = torch randn device=self device dim _x = Dim dim _x min= max= dynamic_shapes = x dim _x check_model Model x dynamic_shapes=dynamic_shapes test_triton_kernel_reinterpret_view device = GPU_TYPE raise unittest SkipTest requires GPU triton jit pass_kernel x y pass Model torch nn Module __init__ - None super __init__ forward x out = torch zeros_like x slicing below creates two ReinterpretView instances offset= offset= add_kernel in_ptr =x - in_ptr =x out_ptr=out n_elements= BLOCK_SIZE= out example_inputs = torch randn device=self device check_model Model example_inputs common_utils parametrize dynamic False True common_utils parametrize tma_version new old test_triton_kernel_tma_descriptor_ d dynamic tma_version device = GPU_TYPE raise unittest SkipTest requires GPU tma_version == new has_triton_tensor_descriptor_host_tma skipTest requires triton tools tensor_descriptor TMA support tma_version == old has_triton_experimental_host_tma skipTest requires triton tools experimental_descriptor TMA support kernel = add_kernel_with_tma_ d_new_api tma_version == new add_kernel_with_tma_ d_old_api Model torch nn Module __init__ - None super __init__ forward b BLOCK_SIZE = out = torch zeros_like n_elements = out numel desc_a desc_b desc_out = create_tensor_descriptor_shim t BLOCK_SIZE new_api= tma_version == new t b out grid = lambda meta noqa E triton cdiv n_elements meta BLOCK_SIZE kernel grid desc_a desc_b desc_out BLOCK_SIZE=BLOCK_SIZE out = torch randn device=self device b = torch randn device=self device example_inputs = b dynamic_shapes = None dynamic dim _ab = Dim s min= max= dynamic_shapes = dim _ab None b dim _ab None check_model Model example_inputs=example_inputs dynamic_shapes=dynamic_shapes common_utils parametrize dynamic False True common_utils parametrize tma_version new old test_triton_kernel_tma_descriptor_ d dynamic tma_version device = GPU_TYPE raise unittest SkipTest requires GPU tma_version == new has_triton_tensor_descriptor_host_tma skipTest requires triton tools tensor_descriptor TMA support tma_version == old has_triton_experimental_host_tma skipTest requires triton tools experimental_descriptor TMA support kernel = add_kernel_with_tma_ d_new_api tma_version == new add_kernel_with_tma_ d_old_api Model torch nn Module __init__ - None super __init__ forward b BLOCK_SIZE_X = BLOCK_SIZE_Y = out = torch zeros_like x_size y_size = out size desc_a desc_b desc_out = create_tensor_descriptor_shim t BLOCK_SIZE_X BLOCK_SIZE_Y new_api= tma_version == new t b out grid = lambda meta noqa E triton cdiv x_size meta BLOCK_SIZE_X triton cdiv y_size meta BLOCK_SIZE_Y kernel grid desc_a desc_b desc_out BLOCK_SIZE_X=BLOCK_SIZE_X BLOCK_SIZE_Y=BLOCK_SIZE_Y out = torch randn device=self device b = torch randn device=self device example_inputs = b dynamic_shapes = None dynamic dim _ab = Dim s min= max= dynamic_shapes = dim _ab None b dim _ab None check_model Model example_inputs=example_inputs dynamic_shapes=dynamic_shapes test_triton_kernel_sympy_expr_arg device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module forward x e sympy_expr = max e item out = torch zeros_like x add_kernel in_ptr =x in_ptr =x out_ptr=out n_elements=sympy_expr BLOCK_SIZE= out NUMEL = inputs = torch randn NUMEL device=self device torch tensor NUMEL device=self device check_model Model inputs test_triton_kernel_sympy_fn_like_arg This test should hit sympy expand sqrt which crashes AttributeError function object has no attribute expand device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module forward x out = torch zeros_like x add_kernel_with_optional_param in_ptr =x in_ptr =x out_ptr=out n_elements=x numel BLOCK_SIZE= ARGS_PASSED= sqrt sqrt valid sympy fn out inputs = torch randn device=self device check_model Model inputs test_triton_kernel_with_none_input device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module __init__ - None super __init__ forward x y n_elements = x size BLOCK_SIZE = output_wo_y = torch empty_like x output_with_y = torch empty_like x add_kernel_with_optional_param x None output_wo_y n_elements ARGS_PASSED= one BLOCK_SIZE=BLOCK_SIZE add_kernel_with_optional_param x y output_with_y n_elements ARGS_PASSED= two BLOCK_SIZE=BLOCK_SIZE output_wo_y + output_with_y example_inputs = torch randn device=self device torch randn device=self device check_model Model example_inputs test_triton_kernel_equal_to_ _arg device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module forward x y out = torch empty_like x n_elements = x numel add_kernel n_elements x y out n_elements BLOCK_SIZE= out example_inputs = torch randn device=self device torch randn device=self device check_model Model example_inputs test_triton_kernel_with_none_inputs_and_equal_to_ _arg device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module __init__ - None super __init__ forward x n_elements = x size BLOCK_SIZE = out = torch empty_like x out = torch empty_like x Run same kernel multiple times test optimization removing None arguments then update indices equal_to_ arguments The None arguments need before equal_to_ arguments add_kernel_with_none_param_and_equal_to_ _arg x None out n_elements x stride equal ARGS_PASSED= one BLOCK_SIZE=BLOCK_SIZE add_kernel_with_none_param_and_equal_to_ _arg out None out n_elements x stride equal ARGS_PASSED= one BLOCK_SIZE=BLOCK_SIZE out example_inputs = torch randn device=self device check_model Model example_inputs common_utils parametrize dynamic False True test_triton_kernel_equal_to_ _float_arg dynamic device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module forward x y out = torch empty_like x n_elements = x numel scaling_factor = n_elements add_kernel_with_scaling n_elements x y out n_elements scaling_factor BLOCK_SIZE= out dynamic_shapes = None dynamic dim _xy = Dim s min= max= dynamic_shapes = x dim _xy y dim _xy example_inputs = torch randn device=self device torch randn device=self device check_model Model example_inputs dynamic_shapes=dynamic_shapes test_triton_kernel_weird_param_order device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module __init__ - None super __init__ forward x out = torch empty_like x add_kernel_autotuned_weird_param_order in_ptr =x in_ptr =x n_elements=x numel out_ptr=out out x = torch randn device=self device check_model Model x skipIfWindowsXPU msg= crash Windows XPU test_triton_kernel_dynamic_grid device = GPU_TYPE raise unittest SkipTest requires GPU math Model torch nn Module __init__ - None super __init__ forward x y n_elements_tensor output = torch zeros_like x n_elements_symint = n_elements_tensor item n_elements = x numel grid meta n_elements_complicated = n_elements_symint math trunc n_elements_complicated meta BLOCK_SIZE add_kernel_autotuned grid x y output n_elements output x = torch randn device=self device y = torch randn device=self device n_elem = torch tensor dim _x = Dim dim _x min= max= dim _y = Dim dim _y min= max= dynamic_shapes = x dim _x y dim _y n_elements_tensor check_model Model x y n_elem dynamic_shapes=dynamic_shapes test_shifted_constraint_ranges Model torch nn Module __init__ - None super __init__ forward x torch Tensor y torch Tensor torch _check y size == x size + x sum + y sum = torch randn device=self device b = torch randn device=self device dim _x = Dim dim _x min= max= dim _y = dim _x + dynamic_shapes = x dim _x y dim _y check_model Model b dynamic_shapes=dynamic_shapes test_scatter_fallback Model torch nn Module __init__ - None super __init__ forward inp torch Tensor index torch Tensor src torch Tensor torch scatter inp index src inputs = torch ones device=self device dtype=torch int torch tensor device=self device dtype=torch int torch zeros device=self device dtype=torch int check_model Model inputs test_scatter_reduce_fallback Model torch nn Module __init__ - None super __init__ forward inp torch Tensor index torch Tensor src torch Tensor torch scatter_reduce inp index src reduce= sum inputs = torch tensor device=self device dtype=torch int torch tensor device=self device dtype=torch int torch tensor device=self device dtype=torch int check_model Model inputs test_index_put_fallback index_put falls back deterministic mode DeterministicGuard True Model torch nn Module __init__ - None super __init__ forward self_tensor torch Tensor indices tuple torch Tensor values torch Tensor torch index_put self_tensor indices values accumulate=True inputs = torch ones device=self device dtype=torch int torch tensor device=self device dtype=torch bool torch ones device=self device dtype=torch int check_model Model inputs test_narrow_fallback Model torch nn Module __init__ - None super __init__ forward inp torch Tensor dim int start int length int torch ops aten narrow inp dim start length inputs = torch rand device=self device check_model Model inputs test_pad_fallback Model torch nn Module __init__ - None super __init__ forward inp torch Tensor pad tuple int torch ops aten pad inp pad inputs = torch rand device=self device check_model Model inputs test_fill__fallback Model torch nn Module __init__ - None super __init__ forward inp torch Tensor scalar float torch ops aten fill_ inp scalar inp inputs = torch rand device=self device check_model Model inputs common_utils parametrize embed_kernel_binary False True test_repeated_user_defined_triton_kernel embed_kernel_binary device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module __init__ - None super __init__ forward x _ range mul _inplace_kernel x n_elements= BLOCK_SIZE= x inputs = torch randn device=self device config patch aot_inductor embed_kernel_binary embed_kernel_binary model = Model check_model model inputs _ code = run_and_get_cpp_code AOTIRunnerUtil compile model inputs FileCheck check launchKernel run code config aot_inductor embed_kernel_binary Not expect see launchKernel CUBIN_FILE_NAME FileCheck check_not launchKernel run code unittest skipIf IS_BIG_GPU Skipping triton backend only since big GPU enough SM test_convolution device == cpu raise unittest SkipTest using triton backend only supported CPU Model torch nn Module __init__ - None super __init__ forward x w b torch ops aten convolution x w b True example_inputs = torch randn device=self device torch randn device=self device torch randn device=self device config patch max_autotune True max_autotune_gemm_backends Triton check_model Model example_inputs test_zero_size_weight Model torch nn Module __init__ channel r= super __init__ pool = torch nn AdaptiveAvgPool d net = torch nn Sequential torch nn Linear channel channel r bias=False torch nn ReLU inplace=True torch nn Linear channel r channel bias=False torch nn Sigmoid forward inp b c _ _ = inp shape x = pool inp view b c x = net x view b c x = inp x x inputs = torch rand device=self device check_model Model inputs test_zero_size_buffer Model torch nn Module __init__ device super __init__ foo = torch nn Buffer torch zeros device=device forward x x + foo example_inputs = torch rand device=self device check_model Model device example_inputs test_no_args Model torch nn Module __init__ m n super __init__ weight = torch nn Parameter torch randn m n alpha = torch nn Parameter torch randn m n forward weight alpha check_model Model test_dynamic_scalar Model torch nn Module __init__ - None super __init__ criterion_ce = torch nn CrossEntropyLoss reduction= none forward inputs targets split_index=None statistics = total_loss = criterion_ce inputs targets sum statistics dl = total_loss item total_loss statistics inputs = torch rand device=self device torch rand device=self device check_model Model inputs test_symint_item Model torch nn Module forward tensor tensor item inputs = torch tensor dtype=torch int device=self device check_model Model inputs test_symbool_item Model torch nn Module forward tensor tensor item inputs = torch tensor dtype=torch bool device=self device check_model Model inputs test_symfloat_item Model torch nn Module forward tensor tensor item inputs = torch tensor dtype=torch float device=self device check_model Model inputs test_constant_original_fqn_and_dtype FooBarModule torch nn Module __init__ - None super __init__ register_parameter torch nn Parameter torch randn test_buf = torch nn Buffer torch randn register_parameter test_param torch nn Parameter torch randn forward x x + test_buf getattr test_param TestModule torch nn Module __init__ - None super __init__ foo_bar = FooBarModule register_parameter test_param torch nn Parameter torch randn test_buf = torch nn Buffer torch randn forward x foo_bar x + test_param test_buf torch no_grad so_path = AOTIRunnerUtil legacy_compile model=TestModule device=self device example_inputs= torch rand device=self device runner = AOTIRunnerUtil legacy_load_runner device so_path expected_original_fqns = L__self___test_param test_param L__self___test_buf test_buf L__self___foo_bar_ foo_bar L__self___foo_bar_test_param foo_bar test_param L__self___foo_bar_test_buf foo_bar test_buf assertEqual expected_original_fqns runner get_constant_names_to_original_fqns expected_dtypes = L__self___test_param L__self___test_buf L__self___foo_bar_ L__self___foo_bar_test_param L__self___foo_bar_test_buf assertEqual expected_dtypes runner get_constant_names_to_dtypes test_masked_select_dynamic M torch nn Module __init__ - None super __init__ forward x torch Tensor - torch Tensor mask = x ge torch masked_select x mask example_args = torch randn device=self device dim _x_max dim _x_max = dynamic_shapes = x Dim dim _x max=dim _x_max Dim dim _x_max max=dim _x_max m = M check_model m example_args dynamic_shapes=dynamic_shapes test_proxy_executor_permute M torch nn Module __init__ - None super __init__ forward x torch ops aten permute default x example_args = torch randn dtype=torch complex m = M check_model m example_args test_proxy_executor_abs M torch nn Module __init__ - None super __init__ forward x torch ops aten abs default x example_args = torch randn dtype=torch complex m = M check_model m example_args test_proxy_executor_squeeze M torch nn Module __init__ - None super __init__ forward x torch ops aten squeeze dim x example_args = torch randn dtype=torch complex m = M check_model m example_args test_proxy_executor_hann M torch nn Module __init__ - None super __init__ forward torch ops aten hann_window default example_args = m = M check_model m example_args test_fqn NestedChild torch nn Module __init__ - None super __init__ nestedchild buffer = torch nn Buffer torch ones forward x x nestedchild buffer Child torch nn Module __init__ - None super __init__ nested = NestedChild register_parameter child param torch nn Parameter torch ones forward x x = nested x x + child param Child torch nn Module __init__ - None super __init__ child buffer = torch nn Buffer torch ones forward x x - child buffer MyModule torch nn Module __init__ - None super __init__ foo = Child bar = Child register_parameter rootparam torch nn Parameter torch ones forward x x = x rootparam x = foo x x = bar x x check_model MyModule torch randn device=self device test_model_modified_weights Model torch nn Module __init__ n k device super __init__ weight = torch randn n k device=device bias = torch randn n device=device forward torch nn functional linear weight bias M = N = K = example_inputs = torch randn M K device=self device model = Model N K device check_model model example_inputs Update model weights after AOTInductor should re-generate model so weights stored model so model weight += check_model model example_inputs skipIfWindowsXPU msg= crash Windows XPU test_triton_kernel_extern_kernel_arg device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module forward x y out = torch zeros_like x torch mm ExternKernelOut add_kernel x torch mm x y out out example_inputs = torch randn device=GPU_TYPE torch randn device=GPU_TYPE check_model Model example_inputs test_triton_kernel_multi_output_arg device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module forward x y out = torch zeros_like x torch sort creates fallback kernel hence MultiOutput add_kernel x torch sort y values out out example_inputs = torch randn device=GPU_TYPE torch randn device=GPU_TYPE check_model Model example_inputs test_triton_kernel_reinterpret_view_mem_leak Check memory leak when using user-defined Triton Kernel + AOTI device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module __init__ - None super __init__ forward x y out = torch zeros_like x yy = y y reshape creates ReinterpretView add_kernel x yy reshape_as x out out example_inputs = torch randn device=GPU_TYPE torch randn device=GPU_TYPE package_path str = AOTIRunnerUtil compile Model example_inputs aot_inductor_module = torch _inductor aoti_load_package package_path Don t assign outputs variable b c will allocate GPU memory device_interface = get_interface_for_device GPU_TYPE device int = device_interface current_device mem_before = device_interface memory_allocated device aot_inductor_module example_inputs aot_inductor_module example_inputs mem_after = device_interface memory_allocated device assertEqual mem_before mem_after actual = aot_inductor_module example_inputs expected = Model example_inputs torch testing assert_close actual expected skipIfMPS torch _dynamo config patch capture_scalar_outputs=True common_utils parametrize dynamic False True common_utils parametrize autotuning False True test_triton_kernel_unbacked_symint_in_grid dynamic autotuning device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module forward x y n_elements_tensor output = torch zeros_like x n_elements_symint = n_elements_tensor item n_elements = x numel grid meta triton cdiv n_elements_symint meta BLOCK_SIZE autotuning add_kernel_autotuned grid x y output n_elements add_kernel grid x y output n_elements BLOCK_SIZE= output example_inputs = torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch tensor dynamic_shapes = None dynamic dim = Dim s min= max= dynamic_shapes = x dim y dim n_elements_tensor check_model Model example_inputs dynamic_shapes=dynamic_shapes unittest skipIf PLATFORM_SUPPORTS_MEM_EFF_ATTENTION Some archs don t support mem eff SDPA test_scaled_dot_product_efficient_attention device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module forward q k v attn_bias torch ops aten _scaled_dot_product_efficient_attention q k v attn_bias False example_inputs = torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch randn device=GPU_TYPE check_model Model example_inputs test_aoti_runtime_asserts torch export _draft_export draft_export FailureType torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor Tensor b - Tensor tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib foo torch Tensor b torch Tensor - torch Tensor b item torch library register_fake mylib foo lib=lib foo_fake_impl b ctx = torch library get_ctx u = ctx new_dynamic_size torch empty u M torch nn Module forward b res = torch ops mylib foo b s = res shape torch _check s torch _check s shape s - example_inputs = torch randn torch tensor ep = draft_export M example_inputs report = ep _report need_config_patch = any f xfail f failure_type == FailureType MISMATCHED_FAKE_KERNEL f report failures m = ep module This should no longer needed after torch _functorch config functorch_config functorch_config patch generate_fake_kernels_from_real_mismatches need_config_patch pt _file = torch _inductor aoti_compile_and_package ep optimized = torch _inductor aoti_load_package pt _file assertTrue same optimized example_inputs m example_inputs assertRaisesRegex Exception run_func_ API call failed optimized torch randn torch tensor patch dict os environ TORCHINDUCTOR_SCALAR_ASSERTS_FULL test_aoti_runtime_asserts_backed_symint full_aoti_runtime_assert raise unittest SkipTest full runtime assert turned Model torch nn Module forward x y = x reshape - clone y = y + y model = Model device input = torch rand device=self device input = torch rand device=self device dynamic_shapes = x torch export Dim DYNAMIC package_path = AOTIRunnerUtil compile model input dynamic_shapes=dynamic_shapes optimized = torch _inductor aoti_load_package package_path assertEqual model input optimized input assertRaisesRegex Exception run_func_ API call failed optimized input skipIfWindows msg= TODO xuhancn confirm Crash access violation test_index_put_with_none_index index_put falls back deterministic mode DeterministicGuard True Model torch nn Module forward x i i y torch ops aten index_put x None None i i transpose y accumulate=True example_inputs = torch rand device=self device torch zeros dtype=torch int device=self device torch ones dtype=torch int device=self device torch randn device=self device check_model Model example_inputs patch dict os environ AOTI_RUNTIME_CHECK_INPUTS test_runtime_checks Model torch nn Module forward inputs list inputs values inputs = dtypes = torch float torch float torch bool torch int torch int torch int torch int torch uint TEST_MPS dtypes append torch float SM OrLater dtypes append torch bfloat dtype dtypes inputs f x_ str dtype = torch ones dtype=dtype device=self device dim = Dim s min= max= dim = Dim s min= max= dim = Dim s min= max= dynamic_shapes = x_torch float dim x_torch float dim x_torch bool dim x_torch int dim x_torch int x_torch int dim x_torch int dim x_torch uint dim TEST_MPS dynamic_shapes x_torch float = dim SM OrLater dynamic_shapes x_torch bfloat = dim m = Model inputs = inputs dynamic_shapes = dynamic_shapes torch no_grad so_path = AOTIRunnerUtil legacy_compile m inputs dynamic_shapes=dynamic_shapes Expected results following checks unmatched dtype unmatched dim value dim value too unmatched stride value SM OrLater dynamic dims expected_results = TEST_MPS dynamic dims expected_results = dynamic dims expected_results = open os path splitext so_path + cpp cpp src_code = cpp read FileCheck check_count unmatched dtype expected_results exactly=True run src_code FileCheck check_count unmatched dim value expected_results exactly=True run src_code FileCheck check_count dim value too expected_results exactly=True run src_code FileCheck check_count unmatched stride value expected_results exactly=True run src_code check_model m inputs unittest skipIf TEST_WITH_ROCM FP supported ROCM unittest skipIf PLATFORM_SUPPORTS_FP FP only supported H + SM MI + devices patch dict os environ AOTI_RUNTIME_CHECK_INPUTS test_runtime_checks_fp cuda only device = cuda Model torch nn Module __init__ - None super __init__ forward x x t = x torch float + x torch float t inputs = dtype torch float _e m fn torch float _e m FP funz AMD see https github com pytorch pytorch issues torch float _e m fnuz torch float _e m fnuz inputs append torch ones dtype=dtype device=self device dim = Dim s min= max= dynamic_shapes = x dim x dim torch no_grad check_model Model tuple inputs dynamic_shapes=dynamic_shapes skipIfXpu msg= Total size kernel arguments exceeds driver limit XPU test_runtime_checks_large Model torch nn Module __init__ - None super __init__ forward inputs result = inputs i range len inputs result = result + inputs i result inputs = _ range inputs append torch ones dtype=torch float device=self device inputs = tuple inputs model = Model torch no_grad AOTIRunnerUtil compile model inputs test_runtime_checks_complex Model torch nn Module __init__ - None super __init__ forward x x x x x x inputs = x = torch tensor - dtype=torch complex device=self device x = torch tensor + j - + j - + j - j j - dtype=torch complex device=self device x = torch tensor dtype=torch complex device=self device inputs append x inputs append x inputs append x dim = Dim s min= max= dynamic_shapes = x dim x x torch no_grad check_model Model tuple inputs dynamic_shapes=dynamic_shapes unittest skipIf IS_FBCODE Not yet runnable fbcode patch dict os environ AOTI_RUNTIME_CHECK_INPUTS test_runtime_checks_dtype_failed Model torch nn Module __init__ - None super __init__ forward x y = x type torch float y x = torch randn dtype=torch float device=self device model = Model torch no_grad package_path str = AOTIRunnerUtil compile model x aot_inductor_module = torch _inductor aoti_load_package package_path x_casted = x float assertRaisesRegex Exception aot_inductor_module x_casted patch dict os environ AOTI_RUNTIME_CHECK_INPUTS test_runtime_checks_device_type_failed device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module __init__ - None super __init__ forward x x + x = torch randn dtype=torch float device= cpu model = Model torch no_grad package_path str = AOTIRunnerUtil compile model x aot_inductor_module = torch _inductor aoti_load_package package_path aot_inductor_module x x_casted = x GPU_TYPE assertRaisesRegex Exception aot_inductor_module x_casted test_non_contiguous_output_alias Test x x contiguous where x non-contiguous Model torch nn Module forward x squared = x x transposed = squared t non-contiguous contig = transposed contiguous transposed contig x = torch randn dtype=torch float device=self device model = Model torch no_grad result = AOTIRunnerUtil run model x actual = model x assertTrue same result actual contiguous should create new tensor assertTrue result data_ptr = result data_ptr test_multiple_output_alias Test when multiple outputs alias same tensor Model torch nn Module forward x squared = x x contig = squared contiguous alias reshaped = squared reshape squared shape alias cubed = squared x squared contig reshaped cubed x = torch randn dtype=torch float device=self device model = Model torch no_grad result = AOTIRunnerUtil run model x actual = model x assertTrue same result actual squared contig reshaped alias same tensor assertTrue result data_ptr == result data_ptr assertTrue result data_ptr == result data_ptr cubed shouldn t alias assertTrue result data_ptr = result data_ptr patch dict os environ AOTI_RUNTIME_CHECK_INPUTS test_runtime_checks_shape_failed Model torch nn Module __init__ - None super __init__ forward x x x = torch randn dtype=torch float device=self device y = torch randn dtype=torch float device=self device y = torch randn dtype=torch float device=self device y = rand_strided dtype=torch float device=self device batch size outside range y = torch randn dtype=torch float device=self device y = torch randn dtype=torch float device=self device dim = Dim s min= max= dynamic_shapes = x dim model = Model torch no_grad package_path str = AOTIRunnerUtil compile model x dynamic_shapes=dynamic_shapes aot_inductor_module = torch _inductor aoti_load_package package_path dynamic dim works fine _ = aot_inductor_module y assertRaisesRegex Exception aot_inductor_module y assertRaisesRegex Exception aot_inductor_module y assertRaisesRegex Exception aot_inductor_module y assertRaisesRegex Exception aot_inductor_module y test_add_complex Model torch nn Module forward b torch add b x = torch tensor + j - + j - + j - j j - device=self device y = torch tensor + j - + j - + j - j j - device=self device check_model Model x y test_embedding_bag Model torch nn Module forward w i o torch ops aten _embedding_bag w i o False False None example_inputs = torch randn device=self device torch randint device=self device torch tensor device=self device check_model Model example_inputs unittest skipIf TEST_MPS MACOS_VERSION FFT operations only supported MacOS + test_fft_c c Model torch nn Module forward x torch fft fftn x torch fft fftn x real example_inputs = torch randn device=self device check_model Model example_inputs test_bool_input Specialize whichever branch example input b Model torch nn Module forward x b b x x x + x example_inputs = torch randn device=self device True check_model Model example_inputs test_int_list_input Model torch nn Module forward x i x i i example_inputs = torch randn device=self device check_model Model example_inputs test_nested_tensor_from_jagged Model nn Module __init__ - None super __init__ mlp = nn Sequential nn Linear nn ReLU nn Linear nn Sigmoid forward values offsets nt = torch nested nested_tensor_from_jagged values offsets res = mlp nt res values model = Model device=self device example_inputs_ = torch randn device=self device torch tensor device=self device same NT batch size different actual amount data example_inputs_ = torch randn device=self device torch tensor device=self device same actual amount data different NT batch size example_inputs_ = torch randn device=self device torch tensor device=self device different NT batch size example_inputs_ = torch randn device=self device torch tensor device=self device dim _values = Dim dim _values min= max= dim _offsets = Dim dim _offsets min= max= dynamic_shapes = values dim _values offsets dim _offsets example_inputs_list = example_inputs_ example_inputs_ example_inputs_ example_inputs_ example_input example_inputs_list actual = AOTIRunnerUtil legacy_run device model example_input dynamic_shapes=dynamic_shapes assertTrue same model example_input actual Temporarily skipping test pytorch cpuinfo able retrieve cache size AMD EPYC F -Core Processor CPU gfx VM Runners common_utils parametrize max_autotune True False skipIfRocmArch MI _ARCH test_misc_ max_autotune device == cpu IS_MACOS max_autotune raise unittest SkipTest max_autotune supported macos Model nn Module __init__ - None super __init__ mlp = nn Sequential nn Linear nn ReLU nn Linear nn Sigmoid emb = nn EmbeddingBag num_embeddings= embedding_dim= over_arch = nn Sequential nn Linear nn ReLU nn Linear nn Sigmoid forward x y mlp_output = mlp x emb_output = emb y over_arch torch concat mlp_output emb_output dim= example_inputs = torch randn device=self device torch randint device=self device check_model Model example_inputs options=dict max_autotune=max_autotune skip_if_no_torchvision test_torchvision_transforms_functional_tensor_resize torchvision https fb workplace com groups permalink A torch nn Module forward image torch Tensor target_size torch Tensor target_h target_w = target_size tolist torch _check target_h torch _check target_w torch _check target_h = torch _check target_w = torchvision transforms _functional_tensor resize image size= target_h target_w interpolation= bilinear antialias=False model = A example_inputs = torch ones device=self device torch tensor device=self device dynamic_shapes = image torch export Dim height min= max= torch export Dim width min= max= target_size None check_model model example_inputs dynamic_shapes=dynamic_shapes unittest skipIf config triton native_matmul matmul generated test_aoti_debug_printer_codegen basic addmm model test codegen aoti intermediate debug printer Model torch nn Module __init__ n k device super __init__ weight = torch randn n k device=device bias = torch randn n device=device forward torch nn functional linear weight bias M = N = K = model = Model N K device batch = = torch randn batch M K device=self device example_inputs = device == mps kernel_calls = aoti_torch_mps_addmm_out device == GPU_TYPE kernel_calls = triton_poi_fused_ f aoti_torch_ GPU_TYPE _addmm_out kernel_calls = aoti_torch_cpu_addmm_out test default debug printing all tensor values codegen config patch aot_inductor debug_intermediate_value_printer result code = run_and_get_cpp_code AOTIRunnerUtil legacy_compile model example_inputs check c shim print_tensor_handle call triggered config injected cpp output code expected assertEqual aoti_torch_print_tensor_handle code True check codegen debug printing around actual kernel call expected kernel_call count kernel_calls FileCheck check_count f before_launch - kernel_call count run code FileCheck check_count f after_launch - kernel_call count run code test printing selected kernel s tensor values codegen filtered_kernel_name = f aoti_torch_ device _addmm_out config patch aot_inductor debug_intermediate_value_printer aot_inductor filtered_kernel_names filtered_kernel_name result code = run_and_get_cpp_code AOTIRunnerUtil legacy_compile model example_inputs filtered_kernel_calls = filtered_kernel_name kernel_call count filtered_kernel_calls FileCheck check_count f before_launch - kernel_call count run code FileCheck check_count f after_launch - kernel_call count run code kernel_calls_not_to_print = kernel_call kernel_call kernel_calls kernel_call = filtered_kernel_name kernel_name _ kernel_calls_not_to_print FileCheck check_not f before_launch - kernel_name run code FileCheck check_not f after_launch - kernel_name run code unittest skipIf config triton native_matmul different kernel name when native matmul common_utils parametrize enable_kernel_profile True False test_aoti_profiler enable_kernel_profile basic addmm model Model torch nn Module __init__ n k device super __init__ weight = torch randn n k device=device bias = torch randn n device=device forward torch nn functional linear weight bias sys platform linux win raise unittest SkipTest enable_kernel_profile only supported linux win M = N = K = model = Model N K device batch = = torch randn batch M K device=self device example_inputs = kernel_calls = f aoti_torch_ GPU_TYPE _addmm_out device == GPU_TYPE aoti_torch_cpu_addmm_out config patch cpp enable_kernel_profile enable_kernel_profile _ code = run_and_get_cpp_code AOTIRunnerUtil compile model example_inputs shim_fn_codes = f RAIIAtenRecordFunctionHandle \\ kernel_calls enable_kernel_profile FileCheck check_regex shim_fn_codes run code FileCheck check_not RAIIAtenRecordFunctionHandle run code check_model Model N K device example_inputs test_aoti_user_defined_triton_kernel_profiling device = GPU_TYPE device == mps raise unittest SkipTest requires GPU Model torch nn Module __init__ - None super __init__ forward x y out = torch zeros_like x add_kernel x y out n_elements= BLOCK_SIZE= out example_inputs = torch randn device=self device torch randn device=self device config patch cpp enable_kernel_profile True torch profiler profile record_shapes=True activities= torch profiler ProfilerActivity CPU getattr torch profiler ProfilerActivity GPU_TYPE upper prof check_model Model example_inputs common_utils TemporaryFileName mode= w+ fname prof export_chrome_trace fname open fname f json j = json load f op_events = e e j traceEvents e get name == kernels_ add_kernel_ assertEqual len op_events assertEqual op_events args get Input Args in_ptr in_ptr out_ptr n_elements test_aoti_debug_printer_user_defined_triton_kernel device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module __init__ - None super __init__ forward x y out = torch zeros_like x add_kernel x y out n_elements= BLOCK_SIZE= out example_inputs = torch randn device=self device torch randn device=self device kernel_calls = add_kernel_ config patch aot_inductor debug_intermediate_value_printer result code = run_and_get_cpp_code AOTIRunnerUtil compile Model example_inputs check c shim print_tensor_handle call triggered config injected cpp output code expected assertEqual aoti_torch_print_tensor_handle code True check codegen debug printing around actual kernel call expected kernel_call count kernel_calls FileCheck check_count f before_launch - kernel_call count run code FileCheck check_count f after_launch - kernel_call count run code test_aoti_debug_printer_cpp_kernel device = cpu raise unittest SkipTest cpu test case only simple cpp kernel test case testing debug printer codegen cpp kernel cpu device Model torch nn Module __init__ - None super __init__ forward x t = torch tensor x size - device= cpu dtype=torch float t = torch sqrt t x t example_inputs = torch randn device= cpu kernel_calls = cpp_fused_mul_sqrt_ config patch aot_inductor debug_intermediate_value_printer result code = run_and_get_cpp_code AOTIRunnerUtil compile Model example_inputs check c shim print_tensor_handle call triggered config injected cpp output code expected assertEqual aoti_torch_print_tensor_handle code True check codegen debug printing around actual kernel call expected kernel_call count kernel_calls FileCheck check_count f before_launch - kernel_call count run code FileCheck check_count f after_launch - kernel_call count run code test_aoti_debug_printer_sym_inputs device = GPU_TYPE raise unittest SkipTest requires GPU torch testing _internal triton_utils add_kernel Model torch nn Module __init__ super __init__ forward x maxlen = max x item = torch ones maxlen device=GPU_TYPE b = torch ones maxlen device=GPU_TYPE out = torch zeros_like unbacked symint grid add_kernel maxlen b out maxlen out example_inputs = torch randint high= size= device=self device expected_scalar_args = triton_poi_fused_zeros_like_ _xnumel triton_poi_fused_ones_ _xnumel std max static_cast int _t L static_cast int _t u config patch aot_inductor debug_intermediate_value_printer result code = run_and_get_cpp_code AOTIRunnerUtil compile Model example_inputs assertEqual aoti_torch_print_tensor_handle code True scalar expected_scalar_args FileCheck check_count f scalar run code unittest skipIf PLATFORM_SUPPORTS_FP FP only supported H + SM MI + devices skipIfXpu test_aoti_debug_printer_fp _dtype device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module __init__ dtype super __init__ out_dtype = dtype forward x weight bias scale_a scale_b weight = weight e m _type output = torch _scaled_mm x weight bias=input_bias out_dtype=self out_dtype scale_a=scale_a scale_b=scale_b output dtype = torch float a_scale = torch Tensor device=GPU_TYPE b_scale = torch Tensor device=GPU_TYPE input_bias = torch rand device=GPU_TYPE dtype=dtype weight_shape = weight = torch rand weight_shape device=GPU_TYPE dtype=dtype T a_inverse_scale = a_scale b_inverse_scale = b_scale x_shape = x = torch rand x_shape device=GPU_TYPE dtype=dtype e m _type kernel_calls = f aoti_torch_ GPU_TYPE __scaled_mm_out test default debug printing all tensor values codegen config patch aot_inductor debug_intermediate_value_printer result code = run_and_get_cpp_code AOTIRunnerUtil legacy_compile Model dtype x weight input_bias a_inverse_scale b_inverse_scale check c shim print_tensor_handle call triggered config injected cpp output code expected assertEqual aoti_torch_print_tensor_handle code True check codegen debug printing around actual kernel call expected float dtype printed expected kernel_call count kernel_calls FileCheck check_count f before_launch - kernel_call count run code FileCheck check_count f after_launch - kernel_call count run code test_aoti_debug_printing_model_inputs_codegen device cuda xpu raise unittest SkipTest requires CUDA XPU Model torch nn Module __init__ super __init__ forward b c x = y = torch addmm c x b z = torch nn functional gelu y z example_inputs = torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch randn device=GPU_TYPE model = Model kernel_calls = aoti_model_inputs config patch aot_inductor debug_intermediate_value_printer result code = run_and_get_cpp_code AOTIRunnerUtil compile model example_inputs assertEqual aoti_torch_print_tensor_handle code True check triton kernel printed comment assertEqual triton_ code True check codegen debug printing around aoti model inputs expected kernel_call count kernel_calls FileCheck check_count f kernel_call count run code test_size_from_multi_output Model torch nn Module __init__ super __init__ relu = torch nn ReLU forward x _x _i = torch unique x sorted=True return_inverse=True _x = _x detach clone relu _x _i example_inputs = torch randn device=self device check_model Model example_inputs dynamo_config patch capture_scalar_outputs True test_sym_i _input_codegen device = GPU_TYPE raise unittest SkipTest requires GPU torch testing _internal triton_utils add_kernel Model torch nn Module __init__ - None super __init__ forward x x_symint = x item = torch ones x_symint device=GPU_TYPE b = torch ones x_symint device=GPU_TYPE out = torch zeros_like unbacked symint grid add_kernel x_symint b out x_symint out example_inputs = torch randint high= size= device=self device dtype=torch int This simple unit test case model generates two triton kernels triton_poi_fused_ones_ triton_meta= signature out_ptr fp xnumel i add_kernel triton_meta= signature in_ptr fp in_ptr fp out_ptr fp n_elements i input u defined int _t initially verify every kernel var args downstream gets explicitly declared using its data types cpp wrapper codegen code expected_scalar_args = buf u buf u buf buf buf u full_aoti_runtime_assert we ll have one more assertion expected_scalar_args = buf u buf u buf buf buf u check new behavior codegen expected result code = run_and_get_cpp_code AOTIRunnerUtil compile Model example_inputs scalar_line expected_scalar_args FileCheck check_count scalar_line run code check_model Model example_inputs test_input_codegen_with_sympy_expr device = GPU_TYPE raise unittest SkipTest requires GPU MyModel torch nn Module forward getitem_ getitem_ getitem_ values_ offsets bitwise_or = torch bitwise_or getitem_ getitem_ combined = torch cat getitem_ values_ dim= add = combined + bitwise_or sliced = values_ - + offsets add sliced inps = torch randint device=GPU_TYPE dtype=torch uint torch randint device=GPU_TYPE dtype=torch uint torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch randint device=GPU_TYPE dtype=torch uint dim = torch export Dim dimensionality derived_dim = dim spec = getitem_ Dim AUTO s + s + getitem_ Dim AUTO s + s + getitem_ derived_dim s values_ Dim AUTO s + offsets Dim AUTO s check_model MyModel inps dynamic_shapes=spec common_utils parametrize mark_unbacked True False test_unbacked_equals_input_size_runtime_assertion mark_unbacked bool This test checks unbacked symint runtime assertions following cases A unbacked symint equals unbacked symint mark_unbacked=True B unbacked symint equals backed symint mark_unbacked=False Model torch nn Module forward b c nz = torch nonzero ones = new_ones nz size b size torch _check ones size = equals = torch add ones c equals model = Model example_inputs = torch ones device=self device b = torch randn device=self device c = torch randn device=self device mark_unbacked torch _dynamo decorators mark_unbacked c torch _dynamo mark_dynamic c Check runtime assertion codegen ed so_path code = run_and_get_cpp_code AOTIRunnerUtil legacy_compile model example_inputs lowerbound_check = u = mark_unbacked u = FileCheck check_count lowerbound_check run code compiled = AOTIRunnerUtil legacy_load device so_path compiled example_inputs Check runtime assertion assertRaisesRegex Exception unexpected_inputs = torch ones device=self device b c compiled unexpected_inputs Try again without runtime assertions config patch scalar_asserts False AOTIRunnerUtil run_multiple model example_inputs unexpected_inputs test_none_args_aot_codegen device = GPU_TYPE raise unittest SkipTest requires GPU triton autotune configs= triton Config BLOCK_SIZE num_stages= num_warps= triton Config BLOCK_SIZE num_stages= num_warps= key= n_elements triton jit sin_kernel in_ptr out_ptr We want include arg known compile time This because we remove None args arg list changing eq_ constexpr arg indices We want make sure we recompute these correctly EQ_ _ARG n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements in_ptr None x = tl load in_ptr + offsets mask=mask x = output = tl sin x + EQ_ _ARG tl store out_ptr + offsets output mask=mask sin_triton x out n_elements = out numel sin_kernel n_elements x out n_elements out x = torch randn device=self device out = torch empty_like x not_none_inputs = x out none_inputs = None out AOTI compilation specializes either None non-None inputs So we have check twice here check_model sin_triton none_inputs check_model sin_triton not_none_inputs skipIfRocm RoCM does support config block size test suite test_autotune_int _user_defined_triton_kernel device = GPU_TYPE raise unittest SkipTest requires GPU triton jit add_kernel in_ptr in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= tl int block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = x + y tl store out_ptr + offsets output mask=mask torch library triton_op mylib add mutates_args= custom_add x torch Tensor y torch Tensor - torch Tensor output = torch empty_like x n_elements = output numel grid meta triton cdiv n_elements meta BLOCK_SIZE capture_triton add_kernel grid x y output n_elements output Model torch nn Module forward x x = custom_add x x split_with_sizes_ = torch ops aten split_with_sizes default x getitem_ = split_with_sizes_ getitem_ n = try buf = torch randint n dtype=torch int device=self device example_inputs = buf check_model Model example_inputs dynamic_shapes= x Dim x max= Dim STATIC options= max_autotune True except torch OutOfMemoryError CI can OOM because test uses too much memory raise unittest SkipTest OOM Test too large None skipIfWindows msg= OpenMP crashed application windows TODO xuhancn need root cause fix test_issue_ Model torch nn Module __init__ super __init__ mlp = torch nn Sequential torch nn Linear torch nn ReLU torch nn Linear norm = torch nn LayerNorm attn = torch nn functional scaled_dot_product_attention forward x x = x transpose _ range x = forward_block x x forward_block x x B H W C B = x shape H W C = shortcut = x x = norm x x = x reshape B H W C B H W C x = attn x x x x = x reshape B H W - x = x transpose reshape B H W - x = shortcut + x x = x + mlp norm x x bs = torch export Dim bs max= example_inputs = torch randn device=self device check_model Model example_inputs dynamic_shapes= x bs requires_gpu test_d h_copy device copy host should always have same stride cuda device raise unittest SkipTest This test only CUDA ToCpuModel nn Module forward x predictions = x permute predictions = torch nan_to_num predictions nan= posinf= neginf= predictions = predictions cpu non_blocking=True p = predictions ones = p new_ones p ones model = ToCpuModel GPU_TYPE input_tensor = torch randn device=GPU_TYPE dtype=torch float ep = torch export export model input_tensor package_path = torch _inductor aoti_compile_and_package ep aoti_model = torch _inductor aoti_load_package package_path expect_res = model input_tensor torch profiler profile activities= torch profiler ProfilerActivity CPU torch profiler ProfilerActivity CUDA prof true_res = aoti_model input_tensor assertEqual expect_res true_res all_ops = event key event prof key_averages assertTrue any aten contiguous op op all_ops test_so_without_weight Model torch nn Module __init__ n k device super __init__ weight = torch randn n k device=device bias = torch randn n device=device forward torch nn functional linear weight bias M N K = model = Model N K device = torch randn M K device=self device example_inputs = torch no_grad config patch always_keep_tensor_constants True aot_inductor package_constants_in_so True so_path = AOTIRunnerUtil legacy_compile model=model example_inputs=example_inputs torch no_grad config patch always_keep_tensor_constants True aot_inductor package_constants_in_so False so_path_weightless = AOTIRunnerUtil legacy_compile model=model example_inputs=example_inputs assertTrue os path getsize so_path _ _ assertTrue os path getsize so_path_weightless _ _ runner = AOTIRunnerUtil legacy_load_runner device so_path_weightless Let s check whether model has correct constant name mapping expected_original_fqns = L__self___weight L__self___weight L__self___bias L__self___bias assertEqual expected_original_fqns runner get_constant_names_to_original_fqns runner_call args kwargs torch fx _pytree fx_pytree call_spec = runner get_call_spec in_spec = pytree treespec_loads call_spec out_spec = pytree treespec_loads call_spec flat_inputs = fx_pytree tree_flatten_spec args kwargs in_spec flat_inputs = x x flat_inputs isinstance x torch Tensor flat_outputs = runner run flat_inputs pytree tree_unflatten flat_outputs out_spec test_inputs = torch randn M K device=self device attach_weights = L__self___weight model weight L__self___bias model bias runner update_constant_buffer attach_weights False False expected = model test_inputs output = runner_call test_inputs atol rtol = e- e- assertEqual expected output atol=atol rtol=rtol test_weight_on_disk_legacy Model torch nn Module __init__ n k device super __init__ weight = torch randn n k device=device bias = torch randn n device=device forward torch nn functional linear weight bias M N K = model = Model N K device = torch randn M K device=self device example_inputs = torch no_grad config patch always_keep_tensor_constants True aot_inductor package_constants_in_so False aot_inductor package_constants_on_disk_format pickle_weights aot_inductor package True aoti_files = AOTIRunnerUtil legacy_compile model=model example_inputs=example_inputs WritableTempFile suffix= pt f package_path = package_aoti f name model aoti_files pt _contents = load_pt package_path load_weights_from_disk=True loaded = pt _contents aoti_runners model atol rtol = e- e- assertEqual loaded model atol=atol rtol=rtol test_extract_constants_map Model torch nn Module __init__ n k device super __init__ weight = torch randn n k device=device bias = torch randn n device=device forward torch nn functional linear weight bias M N K = model = Model N K device = torch randn M K device=self device example_inputs = torch no_grad config patch always_keep_tensor_constants True so_path = AOTIRunnerUtil legacy_compile model=model example_inputs=example_inputs runner = AOTIRunnerUtil legacy_load_runner device so_path runner_call args kwargs torch fx _pytree fx_pytree call_spec = runner get_call_spec in_spec = pytree treespec_loads call_spec out_spec = pytree treespec_loads call_spec flat_inputs = fx_pytree tree_flatten_spec args kwargs in_spec flat_inputs = x x flat_inputs isinstance x torch Tensor flat_outputs = runner run flat_inputs pytree tree_unflatten flat_outputs out_spec test_inputs = torch randn M K device=self device expected = model test_inputs output = runner_call test_inputs assertEqual expected output original_weights = L__self___weight model weight L__self___bias model bias new_weights = L__self___weight torch randn N K device=self device L__self___bias torch randn N device=self device Extract weights use_inactive = False should current weight extracted_original_weights = runner extract_constants_map False assertEqual original_weights extracted_original_weights update inactive weights new_weights extract inactive weights runner update_constant_buffer new_weights True False extracted_new_weights = runner extract_constants_map True assertEqual new_weights extracted_new_weights Swap constant buffer should give us opposite weights runner swap_constant_buffer extracted_inactive_weights = runner extract_constants_map True extracted_active_weights = runner extract_constants_map False assertEqual original_weights extracted_inactive_weights assertEqual new_weights extracted_active_weights test_update_constant_buffer Model torch nn Module __init__ n k device super __init__ weight = torch randn n k device=device bias = torch randn n device=device forward torch nn functional linear weight bias M N K = model = Model N K device = torch randn M K device=self device example_inputs = Attribute naming has changed new export API so still use legacy API here torch no_grad config patch always_keep_tensor_constants True so_path = AOTIRunnerUtil legacy_compile model=model example_inputs=example_inputs runner = AOTIRunnerUtil legacy_load_runner device so_path Let s check whether model has correct constant name mapping expected_original_fqns = L__self___weight L__self___weight L__self___bias L__self___bias assertEqual expected_original_fqns runner get_constant_names_to_original_fqns runner_call args kwargs torch fx _pytree fx_pytree call_spec = runner get_call_spec in_spec = pytree treespec_loads call_spec out_spec = pytree treespec_loads call_spec flat_inputs = fx_pytree tree_flatten_spec args kwargs in_spec flat_inputs = x x flat_inputs isinstance x torch Tensor flat_outputs = runner run flat_inputs pytree tree_unflatten flat_outputs out_spec test_inputs = torch randn M K device=self device expected = model test_inputs output = runner_call test_inputs assertEqual expected output new_weights = L__self___weight torch randn N K device=self device L__self___bias torch randn N device=self device runner update_constant_buffer new_weights False False new_output = runner_call test_inputs new_expected = torch nn functional linear test_inputs new_weights L__self___weight new_weights L__self___bias assertEqual new_expected new_output test_update_constant_buffer_simple Model torch nn Module __init__ device super __init__ weight = torch randn device=device forward + weight model = Model device = torch randn device=self device example_inputs = torch no_grad config patch always_keep_tensor_constants True so_path = AOTIRunnerUtil legacy_compile model=model example_inputs=example_inputs runner = AOTIRunnerUtil legacy_load_runner device so_path Let s check whether model has correct constant name mapping expected_original_fqns = L__self___weight L__self___weight assertEqual expected_original_fqns runner get_constant_names_to_original_fqns test_inputs = torch randn device=self device new_weight = torch randn device=self device model weight = new_weight attach_weights = L__self___weight new_weight runner update_constant_buffer attach_weights False False False expected = model test_inputs runner_call args kwargs call_spec = runner get_call_spec type ignore attr-defined out_spec = pytree treespec_loads call_spec flat_inputs = pytree tree_flatten args kwargs flat_inputs = x x flat_inputs isinstance x torch Tensor flat_outputs = runner run flat_inputs type ignore attr-defined pytree tree_unflatten flat_outputs out_spec output = runner_call test_inputs assertEqual expected output test_update_inactive_constant_buffer Model torch nn Module __init__ n k device super __init__ weight = torch randn n k device=device bias = torch randn n device=device forward torch nn functional linear weight bias M N K = model = Model N K device = torch randn M K device=self device example_inputs = torch no_grad config patch always_keep_tensor_constants True so_path = AOTIRunnerUtil legacy_compile model=model example_inputs=example_inputs runner = AOTIRunnerUtil legacy_load_runner device so_path runner_call args kwargs torch fx _pytree fx_pytree call_spec = runner get_call_spec in_spec = pytree treespec_loads call_spec out_spec = pytree treespec_loads call_spec flat_inputs = fx_pytree tree_flatten_spec args kwargs in_spec flat_inputs = x x flat_inputs isinstance x torch Tensor flat_outputs = runner run flat_inputs pytree tree_unflatten flat_outputs out_spec test_inputs = torch randn M K device=self device expected = model test_inputs output = runner_call test_inputs assertEqual expected output atol= e- rtol= e- new_weights = L__self___weight torch randn N K device=self device L__self___bias torch randn N device=self device new_expected = torch nn functional linear test_inputs new_weights L__self___weight new_weights L__self___bias runner update_constant_buffer new_weights True False output_before_swap = runner_call test_inputs runner swap_constant_buffer output_after_swap = runner_call test_inputs assertEqual expected output_before_swap assertEqual new_expected output_after_swap test_free_inactive_buffer device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module __init__ n k device super __init__ weight = torch randn n k device=device bias = torch randn n device=device forward torch nn functional linear weight bias M N K = model = Model N K device = torch randn M K device=self device example_inputs = torch no_grad config patch always_keep_tensor_constants True so_path = AOTIRunnerUtil legacy_compile model=model example_inputs=example_inputs runner = AOTIRunnerUtil legacy_load_runner device so_path runner_call args kwargs torch fx _pytree fx_pytree call_spec = runner get_call_spec in_spec = pytree treespec_loads call_spec out_spec = pytree treespec_loads call_spec flat_inputs = fx_pytree tree_flatten_spec args kwargs in_spec flat_inputs = x x flat_inputs isinstance x torch Tensor flat_outputs = runner run flat_inputs pytree tree_unflatten flat_outputs out_spec test_inputs = torch randn M K device=self device expected = model test_inputs output = runner_call test_inputs Check outputs make sure model correct here assertEqual expected output new_weights = L__self___weight torch randn N K device=self device L__self___bias torch randn N device=self device new_expected = torch nn functional linear test_inputs new_weights L__self___weight new_weights L__self___bias runner update_constant_buffer new_weights True False Make sure we have swapped buffer runner swap_constant_buffer output_after_swap = runner_call test_inputs assertEqual new_expected output_after_swap Free secondary buffer runner free_inactive_constant_buffer Create new set weights refill into already freed buffer new_weights_ = L__self___weight torch randn N K device=self device L__self___bias torch randn N device=self device new_expected_ = torch nn functional linear test_inputs new_weights L__self___weight new_weights L__self___bias runner update_constant_buffer new_weights_ True False output_after_swap_ = runner_call test_inputs assertEqual new_expected_ output_after_swap_ runner free_inactive_constant_buffer test_update_user_managed_buffer device cuda xpu raise unittest SkipTest requires CUDA XPU Model torch nn Module __init__ n k device super __init__ weight = torch randn n k device=device bias = torch randn n device=device forward torch nn functional linear weight bias M N K = model = Model N K device = torch randn M K device=self device example_inputs = Attribute naming has changed new export API so still use legacy API here torch no_grad config patch always_keep_tensor_constants True so_path = AOTIRunnerUtil legacy_compile model=model example_inputs=example_inputs runner = AOTIRunnerUtil legacy_load_runner device so_path runner_call args kwargs torch fx _pytree fx_pytree call_spec = runner get_call_spec in_spec = pytree treespec_loads call_spec out_spec = pytree treespec_loads call_spec flat_inputs = fx_pytree tree_flatten_spec args kwargs in_spec flat_inputs = x x flat_inputs isinstance x torch Tensor flat_outputs = runner run flat_inputs pytree tree_unflatten flat_outputs out_spec test_inputs = torch randn M K device=self device expected = model test_inputs output = runner_call test_inputs assertEqual expected output atol= e- rtol= e- new_weights = L__self___weight torch randn N K device=self device L__self___bias torch randn N device=self device mem_before _ = getattr torch GPU_TYPE mem_get_info device Do use user managed_buffer should have less free memory runner update_constant_buffer new_weights True False False mem_after _ = getattr torch GPU_TYPE mem_get_info device assertGreater mem_before mem_after runner swap_constant_buffer new_output = runner_call test_inputs new_expected = torch nn functional linear test_inputs new_weights L__self___weight new_weights L__self___bias assertEqual new_expected new_output atol= e- rtol= e- Inplace substitube tensor without user managed buffer result should different new_weights L__self___weight add_ new_weights L__self___bias add_ new_output = runner_call test_inputs Same previous result assertEqual new_expected new_output atol= e- rtol= e- new_expected = torch nn functional linear test_inputs new_weights L__self___weight new_weights L__self___bias Differ latest result assertNotEqual new_expected new_output Clear out all buffers runner free_inactive_constant_buffer runner swap_constant_buffer runner free_inactive_constant_buffer new_weights = L__self___weight torch randn N K device=self device L__self___bias torch randn N device=self device mem_before _ = getattr torch GPU_TYPE mem_get_info device Try user managed_buffer should have same free memory runner update_constant_buffer new_weights True False True mem_after _ = getattr torch GPU_TYPE mem_get_info device assertEqual mem_before mem_after atol= e- rtol= e- runner swap_constant_buffer new_output = runner_call test_inputs new_expected = torch nn functional linear test_inputs new_weights L__self___weight new_weights L__self___bias assertEqual new_expected new_output atol= e- rtol= e- Inplace substitube tensor user managed buffer result should same new_weights L__self___weight add_ new_weights L__self___bias add_ new_output = runner_call test_inputs new_expected = torch nn functional linear test_inputs new_weights L__self___weight new_weights L__self___bias assertEqual new_expected new_output atol= e- rtol= e- new_weights = L__self___weight torch randn N K device=self device L__self___bias torch randn N device=self device runner update_constant_buffer new_weights True False True runner swap_constant_buffer model weight = torch nn Parameter new_weights L__self___weight model bias = torch nn Parameter new_weights L__self___bias updated_state_dict = weight torch ones_like model weight bias torch zeros_like model bias model load_state_dict updated_state_dict new_output = runner_call test_inputs expected_output = model test_inputs torch testing assert_close new_output expected_output atol= e- rtol= e- assertRaises AssertionError torch testing assert_close new_expected new_output atol= e- rtol= e- test_cond_share_predicte Model torch nn Module forward predicate x y = torch cond predicate lambda x + lambda x + z = torch cond predicate lambda y + lambda y + z example_inputs = torch tensor True device torch tensor device check_model Model example_inputs unittest skipIf IS_FBCODE To enable after C shim FC window ends test_misaligned_input_ device cuda xpu raise unittest SkipTest CUDA XPU test only Model torch nn Module forward x x sin + x cos N = + arg = torch randn N device=self device example_inputs = arg model = Model expected = model example_inputs package_path = AOTIRunnerUtil compile model example_inputs optimized = torch _inductor aoti_load_package package_path If model compiled aligned inputs generated code will check inputs alignment runtime code_check_count model example_inputs aoti_torch_clone_preserve_strides misaligned_arg = torch zeros N + device=self device misaligned_arg = misaligned_arg misaligned_arg copy_ arg actual = optimized misaligned_arg torch testing assert_close actual expected test_misaligned_input_ device = GPU_TYPE raise unittest SkipTest GPU test only Model torch nn Module forward x x sin + x cos N = + arg = torch randn N device=self device misaligned_arg = torch zeros N + device=self device misaligned_arg = misaligned_arg misaligned_arg copy_ arg example_inputs = misaligned_arg model = Model check_model model example_inputs If model already compiled misaligned input generated code should NOT contain alignment check input code_check_count model example_inputs aoti_torch_clone_preserve_strides test_autotuning_args_reuse device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module forward x y x_out = torch empty_strided x size x size x size device=GPU_TYPE x_out = torch permute x_out add_kernel_autotuned x x x_out y_out = torch empty_strided y size y size y size device=GPU_TYPE y_out = torch permute y_out add_kernel_autotuned y y y_out sub_kernel_autotuned x x x_out x_out y_out example_inputs = torch randn device=GPU_TYPE torch randn device=GPU_TYPE dim _x = Dim dim _x min= max= dim _y = Dim dim _y min= max= dynamic_shapes = x dim _x y dim _y check_model Model example_inputs dynamic_shapes=dynamic_shapes options= max_autotune True unittest skipIf IS_FBCODE Not runnable fbcode unittest skipIf TEST_MPS MACOS_VERSION FFT operations only supported MacOS + test_stft N_FFT = HOP_LENGTH = Model torch nn Module forward x window = torch hann_window N_FFT device=x device stft = torch stft x N_FFT HOP_LENGTH window=window return_complex=True magnitudes = stft - abs magnitudes model = Model example_inputs = torch randn device=self device check_model model example_inputs skipIfXpu test_conv d device = GPU_TYPE is_big_gpu raise unittest SkipTest requires modern GPU run max-autotune _has_sufficient_memory device raise unittest SkipTest insufficient memory Model torch nn Module __init__ - None super __init__ forward convert_element_type_ convert_element_type_ convert_element_type_ torch ops aten convolution default convert_element_type_ convert_element_type_ convert_element_type_ False example_inputs = torch randn device=self device torch randn device=self device torch randn device=self device dynamic_shapes = convert_element_type_ torch export Dim DYNAMIC torch export Dim DYNAMIC convert_element_type_ None convert_element_type_ None config patch max_autotune True max_autotune_conv_backends TRITON check_model Model example_inputs atol= rtol= e- dynamic_shapes=dynamic_shapes test__int_mm Model torch nn Module __init__ - None super __init__ forward x y torch _int_mm x y example_inputs = torch randint - device=self device dtype=torch int torch randint - device=self device dtype=torch int check_model Model example_inputs skipIfMPS skipIfXpu msg= aten convert_weight_to_int pack currently implemented XPU parametrize m parametrize n parametrize q_group parametrize num_groups test__weight_int pack_mm m n q_group num_groups device = GPU_TYPE raise unittest SkipTest requires GPU TEST_WITH_ROCM CDNA OrLater skipTest _int _mm supported only CDNA later Model torch nn Module __init__ weight scale_and_zeros - None super __init__ weight = weight scale_and_zeros = scale_and_zeros forward torch _weight_int pack_mm weight q_group scale_and_zeros convert_weight_to_int pack b b_int b_scales_and_zeros = _group_quantize_tensor b n_bit= q_group_size=q_group b_int pack = torch _convert_weight_to_int pack b_int innerKTiles= b_int pack b_scales_and_zeros k = q_group num_groups = torch rand m k device=self device dtype=torch bfloat b = torch rand k n device=self device dtype=torch bfloat b_int pack b_scales_and_zeros_f = convert_weight_to_int pack b model = Model b_int pack b_scales_and_zeros_f check_model model parametrize m parametrize n parametrize q_group parametrize num_groups test__weight_int pack_mm_with_scales_and_zeros m n q_group num_groups xpu device raise unittest SkipTest requires Intel GPU TEST_WITH_ROCM CDNA OrLater skipTest _int _mm supported only CDNA later Model torch nn Module __init__ weight scale zeros - None super __init__ weight = weight scale = scale zeros = zeros forward torch _weight_int pack_mm_with_scales_and_zeros weight q_group scale zeros _group_quantize_tensor_xpu w n_bit= q_group_size= w k n = assert w dim == w n k = w = w transpose contiguous assert q_group_size assert w shape - q_group_size == to_quant n k group_size group_size to_quant = w reshape - q_group_size assert torch isnan to_quant sum == max_val = to_quant amax dim= keepdim=True min_val = to_quant amin dim= keepdim=True max_int = n_bit - min_int = scales = max_val - min_val clamp min= e- max_int assert torch isnan scales sum == zeros = min_int - min_val div scales round zeros = torch clamp zeros min_int max_int zeros = zeros torch int assert torch isnan zeros sum == out = to_quant div scales add zeros round clamp_ min_int max_int assert torch isnan out sum == n k out = out dtype=torch int reshape w shape out device = torch device cpu out = out &#124; out torch uint Scales zeros same q-group should contiguous so we can load -bit word scales = scales view w shape - transpose contiguous zeros = zeros view w shape - transpose contiguous out scales zeros convert_weight_to_int pack b b_uint n k b_uint scales zeros = _group_quantize_tensor_xpu b n_bit= q_group_size=q_group b_int pack k n b_int pack = torch _convert_weight_to_int pack b_uint innerKTiles= b_int pack scales zeros k = q_group num_groups = torch rand m k device=self device dtype=torch bfloat b = torch rand k n device=self device dtype=torch bfloat b_int pack b_scales zeros_int = convert_weight_to_int pack b model = Model b_int pack b_scales zeros_int check_model model test_assert_tensor_meta Module torch nn Module forward x torch ops aten _assert_tensor_metadata default x dtype=torch int x + example_inputs = torch tensor dtype=torch int config patch implicit_fallbacks False check_model Module example_inputs atol= rtol= e- runOnRocm test_rocm_triton_autotuning device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module forward x y m _M K = x shape K N = y shape M = torch abs m out = torch empty _M N device=x device dtype=torch float grid = lambda META noqa E triton cdiv META BLOCK_SIZE_M META BLOCK_SIZE_N strange_config_matmul_kernel grid x y out M N K out x = torch randn device=self device y = torch randn device=self device m = torch tensor dtype=torch int device=self device torch no_grad config patch triton autotune_with_sample_inputs True aot_inductor allow_stack_allocation allow_stack_allocation aot_inductor use_minimal_arrayref_interface use_minimal_arrayref_interface torch _export aot_compile Model x y m skipIfRocm RoCM does support config block size test suite test_triton_autotuning device = GPU_TYPE raise unittest SkipTest requires GPU Model torch nn Module forward x y m _M K = x shape K N = y shape M = torch abs m out = torch empty _M N device=x device dtype=torch float grid = lambda META noqa E triton cdiv META BLOCK_SIZE_M META BLOCK_SIZE_N strange_config_matmul_kernel grid x y out M N K out x = torch randn device=self device y = torch randn device=self device m = torch tensor dtype=torch int device=self device config patch triton autotune_with_sample_inputs True The tuned best config XPU different CUDA grid_ = GPU_TYPE == xpu code_check_count Model x y m f uint _t grid_ = grid_ L skipIfRocm RoCM does support config block size test suite test_triton_mutated_autotuning device = GPU_TYPE raise unittest SkipTest requires GPU triton jit add_one_kernel X Y N pid = tl program_id axis= block_start = pid offsets = block_start + tl arange x = tl load X + offsets mask=offsets N y = x + tl store Y + offsets y mask=offsets N Model torch nn Module forward x y m _M K = x shape K N = y shape M = torch empty device=x device dtype=torch int add_one_kernel m M out = torch empty _M N device=x device dtype=torch float grid = lambda META noqa E triton cdiv META BLOCK_SIZE_M META BLOCK_SIZE_N strange_config_matmul_kernel grid x y out M N K out x = torch randn device=self device y = torch randn device=self device m = torch tensor dtype=torch int device=self device config patch triton autotune_with_sample_inputs True The tuned best config XPU different CUDA grid_ = GPU_TYPE == xpu code_check_count Model x y m f uint _t grid_ = grid_ L skipIfRocm patch dict os environ TRITON_DEBUG test_triton_dynamic_launcher_grid device = GPU_TYPE raise unittest SkipTest requires GPU triton autotune configs= triton Config BLOCK_SIZE num_stages= num_warps= triton Config BLOCK_SIZE num_stages= num_warps= key= numel triton jit add_one_kernel X Y numel BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE tl device_assert block_start numel offsets = block_start + tl arange BLOCK_SIZE x = tl load X + offsets y = x + tl store Y + offsets y Model torch nn Module forward x value numel = value item out = torch zeros_like x dtype=torch float grid = lambda META noqa E triton cdiv numel META BLOCK_SIZE add_one_kernel grid x out numel out example_inputs = torch randn device=self device torch tensor dtype=torch int device=self device config patch triton autotune_with_sample_inputs True dim _x = Dim dim _x min= max= dynamic_shapes = x dim _x value Dim AUTO check_model Model example_inputs dynamic_shapes=dynamic_shapes skipIfRocm patch dict os environ TRITON_DEBUG test_triton_dynamic_launcher_grid_infer_from_tensor device = GPU_TYPE raise unittest SkipTest requires GPU triton autotune configs= triton Config BLOCK_SIZE num_stages= num_warps= triton Config BLOCK_SIZE num_stages= num_warps= key= numel triton jit add_one_kernel X Y numel BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE tl device_assert block_start numel offsets = block_start + tl arange BLOCK_SIZE x = tl load X + offsets y = x + tl store Y + offsets y Model torch nn Module forward x dim_D numel = x shape dim_D item x = x repeat dim_D out = torch zeros_like x dtype=torch float grid = lambda META noqa E triton cdiv numel META BLOCK_SIZE add_one_kernel grid x out numel out example_inputs = torch randn device=self device torch tensor dtype=torch int device=self device config patch triton autotune_with_sample_inputs True dim _x = Dim dim _x min= max= dynamic_shapes = x Dim AUTO dim _x dim_D Dim AUTO check_model Model example_inputs dynamic_shapes=dynamic_shapes test_composed_dynamic_size Model torch nn Module forward x x + example_inputs = torch randn device=self device dim = torch export Dim dim_ dim_even = dim dynamic_shapes = x dim_even check_model Model example_inputs dynamic_shapes=dynamic_shapes test_boolean_indexing Model torch nn Module forward x y z x z = x y = x y b = torch cat z dim= b = torch cat z dim= b b x = torch randn device=self device y = torch tensor dtype=torch bool device=self device z = torch randn device=self device x = torch randn device=self device z = torch randn device=self device example_inputs = x y z x z s = Dim s min= max= s = Dim s min= max= s = Dim s min= max= s = Dim s min= max= dynamic_shapes = x s s y s z s s x s s z s s check_model Model example_inputs dynamic_shapes=dynamic_shapes test_sym_expr_indexing device cuda xpu raise unittest SkipTest requires CUDA XPU Repro torch nn Module __init__ - None super __init__ forward episode_builder_position_encoding_observations_weight add_ add_ add_ add_ add_ arange_ = torch ops aten arange start device=torch device type=GPU_TYPE index= pin_memory=False add_ = torch ops aten add Tensor arange_ arange_ = None stack_ = torch ops aten stack default add_ add_ add_ add_ add_ add_ add_ = add_ = add_ = add_ = add_ = add_ = None select_ = torch ops aten select int stack_ stack_ = None embedding_ = torch ops aten embedding default episode_builder_position_encoding_observations_weight select_ episode_builder_position_encoding_observations_weight = select_ = None embedding_ Embedding weight vocab_size x emb_dim episode_builder_position_encoding_observations_weight = torch randn device=self device These six must all -D shape same dtype use Long embedding indices add_ = torch tensor dtype=torch long device=self device one used index add_ = torch tensor dtype=torch long device=self device add_ = torch tensor dtype=torch long device=self device add_ = torch tensor dtype=torch long device=self device add_ = torch tensor dtype=torch long device=self device Instantiate run m = Repro device example_inputs = episode_builder_position_encoding_observations_weight add_ add_ add_ add_ add_ check_model m example_inputs test_with_cudagraphs device = cuda raise unittest SkipTest requires CUDA define CUDAGraph handling wrapper only works kwargs simplicity cudagraph f _graphs = f_ kwargs key = hash tuple tuple kwargs shape sorted kwargs keys isinstance kwargs torch Tensor key _graphs wrapped _ = _graphs key wrapped kwargs g = torch cuda CUDAGraph in_tensors = k v clone isinstance v torch Tensor v k v kwargs items f in_tensors stream warmup torch cuda graph g out_tensors = f in_tensors wrapped kwargs key kwargs in_tensors key copy_ kwargs key g replay isinstance out_tensors torch Tensor out_tensors clone isinstance out_tensors list tuple type out_tensors o clone o out_tensors raise ValueError unsupported output type encountered _graphs key = wrapped g in_tensors out_tensors wrapped kwargs f_ define simple model model = torch nn Linear device=self device export + AOTI model_kwargs = input torch randn device=self device ep = torch export export model args= kwargs=model_kwargs strict=True optimized = torch _inductor aoti_load_package torch _inductor aoti_compile_and_package ep inductor_configs= max_autotune True NB flag avoids CUDAGraph + AOTI runtime multi-threading conflict Error operation permitted when stream capturing run_single_threaded=True enable CUDAGraphs optimized = cudagraph optimized warmup - run CUDAGraphs _ range optimized model_kwargs compare against eager assertEqual optimized model_kwargs model model_kwargs test_custom_op_in_subgraph torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo_add Tensor - Tensor tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo_add CompositeExplicitAutograd lib=lib torch library register_fake mylib foo_add lib=lib foo_add _impl torch Tensor - torch Tensor + torch library define mylib foo_add Tensor - Tensor tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo_add CompositeExplicitAutograd lib=lib torch library register_fake mylib foo_add lib=lib foo_add _impl torch Tensor - torch Tensor + M torch nn Module forward x torch cond x shape torch ops mylib foo_add torch ops mylib foo_add x list_example_inputs = torch ones device=self device torch ones device=self device check_model_with_multiple_inputs M list_example_inputs dynamic_shapes= Dim DYNAMIC test_clamp_decomposition Model torch nn Module forward x x clamp min= Model torch nn Module forward x x clamp min= x = torch randint output should have float type int check_model Model x output should have int type check_model Model x test_upper_bound_i Model torch nn Module forward x y x + y inp = torch randint device=self device dtype=torch int torch tensor device=self device dtype=torch int ep = torch export export Model inp dynamic_shapes= Dim d min= max= Dim STATIC so_path = torch _inductor aot_compile ep module inp m = torch _export aot_load so_path device assertEqual Model inp m inp del inp inp = torch randint device=self device dtype=torch int torch tensor device=self device dtype=torch int don t check accuracy result reduce memory usage test mostly checking ensure there s no IMA m inp test_using_model_name_for_files Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y x + linear y example_inputs = torch randn device=self device torch randn device=self device model = Model device torch no_grad package_path str = AOTIRunnerUtil compile model example_inputs inductor_configs= aot_inductor model_name_for_generated_files test_model zipfile ZipFile package_path r zip_ref all_files = zip_ref namelist base_dir = test_model wrapper data aotinductor model test_model ext_type = get_module_ext_type assertTrue f base_dir wrapper cpp all_files assertTrue f base_dir kernel cpp all_files assertTrue f base_dir wrapper ext_type all_files aot_inductor_module = torch _inductor aoti_load_package package_path assertEqual aot_inductor_module example_inputs model example_inputs test_copy_non_blocking_is_pinned device == cpu device == mps raise unittest SkipTest only matters device-to-cpu copy Model torch nn Module __init__ super __init__ forward b a_cpu = device= cpu non_blocking=True b_cpu = b device= cpu non_blocking=True a_to_cpu_event = torch Event a_to_cpu_event record a_to_cpu_event synchronize torch cat a_cpu b_cpu model = Model = torch randn device=self device b = torch randn device=self device example_inputs = b outputs = model example_inputs package_path code = run_and_get_cpp_code AOTIRunnerUtil compile model example_inputs FileCheck check pinned run code model_aoti = torch _inductor aoti_load_package package_path outputs_aoti = model_aoti example_inputs assertEqual outputs outputs_aoti unittest skipIf config triton native_matmul different code generated test_pad_non_zero_memory_leak device = GPU_TYPE raise unittest SkipTest test only GPU_TYPE Model torch nn Module forward x x = x + x = torch ops aten constant_pad_nd x x x model = Model example_inputs = torch randn device=self device package_path code = run_and_get_cpp_code AOTIRunnerUtil compile model example_inputs outputs = model example_inputs model_aoti = torch _inductor aoti_load_package package_path outputs_aoti = model_aoti example_inputs assertEqual outputs outputs_aoti atol= e- rtol= e- FileCheck check_regex r aoti_torch_as_strided\ buf _handle buf _handle_restrided\ check wrap_with_raii_handle_if_needed buf _handle check RAIIAtenTensorHandle buf buf _handle_restrided run code unittest skipIf IS_MACOS might have no readelf Mac test_libtorch_free_so Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y x + linear y example_inputs = torch randn device=self device torch randn device=self device model = Model device ep = torch export export model example_inputs package_path = torch _inductor aoti_compile_and_package ep inductor_configs= aot_inductor link_libtorch False torch_libs = libtorch so libc so libtorch_cuda so libc _cuda so libtorch_cpu so libtorch_xpu so libc _xpu so tempfile TemporaryDirectory tmpdir Unpack zipfile ZipFile package_path r zf zf extractall tmpdir so_files = list pathlib Path tmpdir rglob so assertTrue len so_files so_file so_files so_copy = pathlib Path tmpdir f so_file name checkcopy so_copy write_bytes so_file read_bytes result = subprocess run readelf -d str so_copy check=True capture_output=True text=True line result stdout splitlines NEEDED line lib torch_libs assertTrue lib line test_unbounded_expr_substitutions Model torch nn Module forward x y b u s = item b item u_max = max u construct equality rule Max u == s Max u torch _check u_max == s u_max size x - Max u x = x expand u_max x shape clone x y model = Model example_inputs = torch randn dtype=torch bfloat device=self device torch randn dtype=torch bfloat device=self device torch tensor device=self device torch tensor device=self device torch _dynamo mark_dynamic example_inputs - so_path code = run_and_get_cpp_code AOTIRunnerUtil legacy_compile model example_inputs compiled = AOTIRunnerUtil legacy_load device so_path compiled_outputs = compiled example_inputs eager_outputs = model example_inputs torch testing assert_close eager_outputs compiled_outputs AOTInductorLoggingTest LoggingTestCase make_logging_test dynamic=logging DEBUG test_shape_env_reuse records make sure ShapeEnv only created once reused afterwards Foo torch nn Module forward x x + inputs = torch randn dynamic_shapes = x Dim AUTO Dim AUTO ep = export Foo inputs dynamic_shapes=dynamic_shapes strict=False torch no_grad torch _inductor aot_compile ep module inputs assertEqual r msg == create_env r records count True make_logging_test dynamic=logging DEBUG test_shape_env_reuse_zero_consts_use_consts_asm_false records make sure ShapeEnv only created once reused afterwards Foo torch nn Module forward x x + inputs = torch randn dynamic_shapes = x Dim AUTO Dim AUTO ep = export Foo inputs dynamic_shapes=dynamic_shapes strict=False torch no_grad config patch aot_inductor use_consts_asm_build False torch _inductor aot_compile ep module inputs assertEqual r msg == create_env r records count True TestAOTInductorConfig TestCase test_no_compile_standalone config patch aot_inductor_mode compile_standalone False result = maybe_aoti_standalone_config assertEqual result test_compile_standalone_sets_package_cpp result = maybe_aoti_standalone_config aot_inductor_mode compile_standalone True assertEqual result aot_inductor package_cpp_only True assertEqual result aot_inductor_mode compile_standalone True assertEqual result aot_inductor embed_kernel_binary True assertEqual result aot_inductor emit_multi_arch_kernel torch version hip assertEqual result aot_inductor model_name_for_generated_files aoti_model assertEqual result aot_inductor dynamic_linkage False test_compile_standalone_explicit_set patches = aot_inductor_mode compile_standalone True aot_inductor package_cpp_only True aot_inductor embed_kernel_binary True aot_inductor dynamic_linkage False aot_inductor link_libtorch False aot_inductor emit_multi_arch_kernel torch version hip aot_inductor model_name_for_generated_files aoti_model result = maybe_aoti_standalone_config patches assertEqual result patches test_compile_standalone_package_cpp_false_raises patches = aot_inductor_mode compile_standalone True aot_inductor package_cpp_only False assertRaises RuntimeError maybe_aoti_standalone_config patches config patch aot_inductor package_cpp_only False patches = aot_inductor_mode compile_standalone True assertRaises RuntimeError maybe_aoti_standalone_config patches test_compile_standalone_cross_compile_windows_package_format patches = aot_inductor cross_target_platform windows aot_inductor package_constants_in_so True assertRaises RuntimeError maybe_aoti_standalone_config patches common_utils instantiate_parametrized_tests AOTInductorTestsTemplate fail_cpu is_skip=False TestFailure cpu is_skip=is_skip fail_mps is_skip=False TestFailure mps is_skip=is_skip fail_gpu suffixes tuple str is_skip=False TestFailure suffixes is_skip=is_skip test_failures xfail default set is_skip=True skip CPU_TEST_FAILURES = TODO failed internally test_multiple_output_alias fail_cpu is_skip=True test_failures xfail default set is_skip=True skip GPU_TEST_FAILURES = quantized unsupported GPU test_quantized_linear fail_gpu cuda xpu test_quanatized_int _linear fail_gpu cuda xpu test_quantized_linear_bias_none fail_gpu cuda xpu No scaled_dot_product_efficient_attention implementation XPU yet test_scaled_dot_product_efficient_attention fail_gpu xpu MPS_TEST_FAILURES = aten _scaled_dot_product_efficient_attention currently implemented MPS device test_scaled_dot_product_efficient_attention fail_mps aten _int_mm implemented MPS backend test__int_mm fail_mps MPS doesn t support float test_while_loop_with_conv_dynamic_True fail_mps test_while_loop_with_conv_dynamic_False fail_mps MPS doesn t support float test_fp fail_mps test_fp _view_of_param fail_mps cannot initialize parameter type double rvalue type std nullptr_t test_fallback_kernel_with_symexpr_output fail_mps correctness issue test_index_put_with_none_index fail_mps Error device may nil test_zero_size_weight fail_mps is_skip=True MPSGraph does support tensor dims INT_MAX test_upper_bound_i fail_mps is_skip=True MPS doesn t support triton test_autotuning_args_reuse fail_mps test_triton_autotuning fail_mps test_triton_dynamic_launcher_grid fail_mps test_triton_dynamic_launcher_grid_infer_from_tensor fail_mps test_triton_kernel_on_device_tma_dynamic_False_tma_version_new fail_mps test_triton_kernel_on_device_tma_dynamic_False_tma_version_old fail_mps test_triton_kernel_on_device_tma_dynamic_True_tma_version_new fail_mps test_triton_kernel_on_device_tma_dynamic_True_tma_version_old fail_mps test_size_with_unbacked_add_expr_transitive fail_mps test_size_with_unbacked_add_and_mul_expr fail_mps test_triton_next_power_of_ fail_mps test_sympy_cpp_printer_min_max_minmax fail_mps test_sympy_cpp_printer_min_max_minmax fail_mps test_triton_kernel_dynamic_shape_with_div fail_mps test_triton_kernel_reinterpret_view fail_mps test_triton_kernel_tma_descriptor_ d_dynamic_False_tma_version_new_mps fail_mps test_triton_kernel_tma_descriptor_ d_dynamic_False_tma_version_old_mps fail_mps test_triton_kernel_tma_descriptor_ d_dynamic_True_tma_version_new_mps fail_mps test_triton_kernel_tma_descriptor_ d_dynamic_True_tma_version_old_mps fail_mps test_triton_kernel_tma_descriptor_ d_dynamic_False_tma_version_new_mps fail_mps test_triton_kernel_tma_descriptor_ d_dynamic_False_tma_version_old_mps fail_mps test_triton_kernel_tma_descriptor_ d_dynamic_True_tma_version_new_mps fail_mps test_triton_kernel_tma_descriptor_ d_dynamic_True_tma_version_old_mps fail_mps test_triton_kernel_sympy_expr_arg fail_mps test_triton_kernel_sympy_fn_like_arg fail_mps test_triton_kernel_with_none_input fail_mps test_triton_kernel_equal_to_ _arg fail_mps test_triton_kernel_with_none_inputs_and_equal_to_ _arg fail_mps test_triton_kernel_equal_to_ _float_arg_dynamic_True fail_mps test_triton_kernel_equal_to_ _float_arg_dynamic_False fail_mps test_triton_kernel_weird_param_order fail_mps test_triton_kernel_dynamic_grid fail_mps test_repeated_user_defined_triton_kernel_embed_kernel_binary_False fail_mps test_repeated_user_defined_triton_kernel_embed_kernel_binary_True fail_mps test_triton_kernel_extern_kernel_arg fail_mps test_triton_kernel_multi_output_arg fail_mps test_triton_kernel_reinterpret_view_mem_leak fail_mps test_triton_mutated_autotuning fail_mps test_sym_i _input_codegen fail_mps test_none_args_aot_codegen fail_mps test_aoti_debug_printer_sym_inputs fail_mps test_aoti_debug_printer_user_defined_triton_kernel fail_mps test_autotune_int _user_defined_triton_kernel fail_mps AOTInductorTestABICompatibleCpu TestCase device = cpu device_type = cpu check_model = check_model check_model_with_multiple_inputs = check_model_with_multiple_inputs code_check_count = code_check_count allow_stack_allocation = False use_minimal_arrayref_interface = False copy_tests AOTInductorTestsTemplate AOTInductorTestABICompatibleCpu cpu CPU_TEST_FAILURES unittest skipIf sys platform == darwin No CUDA MacOS AOTInductorTestABICompatibleGpu TestCase device = GPU_TYPE device_type = GPU_TYPE check_model = check_model check_model_with_multiple_inputs = check_model_with_multiple_inputs code_check_count = code_check_count allow_stack_allocation = False use_minimal_arrayref_interface = False copy_tests AOTInductorTestsTemplate AOTInductorTestABICompatibleGpu GPU_TYPE GPU_TEST_FAILURES unittest skipIf torch backends mps is_available No MPS backend available AOTInductorTestABICompatibleMps TestCase device = mps device_type = mps check_model = check_model check_model_with_multiple_inputs = check_model_with_multiple_inputs code_check_count = code_check_count allow_stack_allocation = False use_minimal_arrayref_interface = False copy_tests AOTInductorTestsTemplate AOTInductorTestABICompatibleMps mps MPS_TEST_FAILURES __name__ == __main__ torch _inductor test_case run_tests cpp_extension N A fbcode HAS_GPU sys platform == darwin run_tests needs= filelock