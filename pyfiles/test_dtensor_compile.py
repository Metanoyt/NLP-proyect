Copyright c Meta Platforms Inc affiliates Owner s oncall distributed contextlib copy functools unittest unittest mock patch torch torch _dynamo torch _dynamo testing torch distributed dist torch nn nn torch _C FileCheck torch _inductor utils run_and_get_triton_code torch distributed algorithms _checkpoint checkpoint_wrapper checkpoint_wrapper CheckpointImpl torch distributed device_mesh init_device_mesh torch distributed fsdp FullyShardedDataParallel FSDP torch distributed tensor DeviceMesh distribute_module distribute_tensor DTensor Partial Replicate Shard torch distributed tensor _dtensor_spec DTensorSpec TensorMeta torch distributed tensor parallel ColwiseParallel loss_parallel parallelize_module PrepareModuleInput PrepareModuleOutput RowwiseParallel torch distributed tensor placement_types _StridedShard torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp get_devtype torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests skipIfHpu skipIfTorchDynamo TEST_CUDA TEST_HPU torch testing _internal distributed _tensor common_dtensor DTensorTestBase MLPModule with_comms torch testing _internal distributed fake_pg FakeStore torch testing _internal inductor_utils HAS_GPU torch testing _internal two_tensor TwoTensor torch utils checkpoint checkpoint dev_type = torch device get_devtype SimpleModel nn Module __init__ device super __init__ mlp_ = MLPModule device mlp_ = MLPModule device forward input mlp_ mlp_ input extract_graph fx_g _ graph_cell graph_cell = fx_g code fx_g Make custom compiler runs aot autograd extracts fw graph fw_graph_cell = None bw_graph_cell = None fw_compiler = functools partial extract_graph graph_cell=fw_graph_cell bw_compiler = functools partial extract_graph graph_cell=bw_graph_cell functorch compile min_cut_rematerialization_partition torch _dynamo backends common aot_autograd aot_eager_graph = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler partition_fn=min_cut_rematerialization_partition _apply_sharding mod nn Module shard_dim int device_mesh DeviceMesh Shards given dimension possible replicate Args mod nn Module Module shard replicate shard_dim int Dimension shard possible device_mesh DeviceMesh D Device Mesh Returns Sharded DTensor shard_module_params name module device_mesh name param module named_parameters placement = Replicate shard_dim len param size placement = Shard shard_dim dist_param = torch nn Parameter distribute_tensor param device_mesh placement name = name split - module register_parameter name dist_param sharded_mod = distribute_module mod device_mesh shard_module_params sharded_mod TestDTensorCompile torch _dynamo test_case TestCase setUp super type setUp use explicit params compiled autograd test wrapping fake_store = FakeStore dist init_process_group fake store=fake_store rank= world_size=self world_size tearDown super type tearDown use explicit params compiled autograd test wrapping dist destroy_process_group property device_type - str cuda TEST_CUDA hpu TEST_HPU cpu property world_size - int test_dtensor_basic mesh = DeviceMesh device_type torch arange world_size torch compile backend= aot_eager fullgraph=True fn x x x + param = torch randn requires_grad=True x = DTensor from_local param mesh Shard run_check=False res = fn x res to_local sum backward unittest skipIf TEST_CUDA CUDA available test_dtensor_basic_export mesh = DeviceMesh cuda torch arange world_size param = torch randn param_x = DTensor from_local param mesh Shard run_check=False Foo torch nn Module __init__ super __init__ buffer = torch nn Buffer param_x forward x inter = buffer + DTensor from_local x mesh Shard run_check=False inter to_local torch utils _pytree register_constant torch distributed tensor _dtensor_spec DTensorSpec torch utils _pytree register_constant DeviceMesh ep = torch export export Foo torch randn dtype=torch float strict=False assertExpectedInline str ep graph_module code strip \ forward b_buffer x _assert_tensor_metadata_default = torch ops aten _assert_tensor_metadata default x dtype = torch float device = device type= cpu layout = torch strided _assert_tensor_metadata_default = None = torch ops aten dtype_layout x dtype = torch float layout = torch strided device = device type= cuda x = None view_as = torch ops aten view_as default = None dtensor___init__ = dtensor___init__ dtensor_const_func_spec = dtensor_const_func_spec flat_apply = torch ops higher_order flat_apply dtensor_const_func_spec dtensor___init__ view_as False dtensor_const_func_spec = dtensor___init__ = view_as = None add = torch ops aten add Tensor b_buffer flat_apply b_buffer = flat_apply = None access_subclass_inner_tensor_default_ = torch ops export access_subclass_inner_tensor default add _local_tensor add = None view_as_ = torch ops aten view_as default access_subclass_inner_tensor_default_ access_subclass_inner_tensor_default_ access_subclass_inner_tensor_default_ = None view_as_ noqa B During tracing sharding propagation cache skipped so extra dry run add performed _propagate_tensor_meta_non_cached hence add_ instead add assertExpectedInline str ep run_decompositions graph_module code strip \ forward b_parametrizations_buffer_original x _assert_tensor_metadata = torch ops aten _assert_tensor_metadata default x None None torch float device = device type= cpu layout = torch strided _assert_tensor_metadata = None _to_copy = torch ops aten _to_copy default x dtype = torch float layout = torch strided device = device type= cuda index= x = None view = torch ops aten view default _to_copy _to_copy = None add = torch ops aten add Tensor b_parametrizations_buffer_original view b_parametrizations_buffer_original = view = None view_ = torch ops aten view default add add = None view_ noqa B test_placement_compile fn x = x is_replicate += x is_shard += x dim raise RuntimeError dim x is_shard += x is_shard dim= += x is_shard dim=None += x is_partial += compiled_fn = torch compile backend= aot_eager fullgraph=True fn split_factors = x Shard Replicate Partial + _StridedShard split_factor=s s split_factors opt_fn = fn x compiled_out = compiled_fn x assertEqual opt_fn compiled_out test_device_mesh_compile fn x DeviceMesh test size = x size b = x size c = x size mesh_dim= size = + b + c test get_coordinate coord = x get_coordinate test get_group group = x get_group group = x get_group mesh_dim= size coord group group Can t fullgraph=True because ProcessGroup reconstructible dynamo compiled_fn = torch compile backend= aot_eager fn mesh = DeviceMesh device_type torch arange world_size unsqueeze opt_fn = fn mesh compiled_out = compiled_fn mesh assertEqual opt_fn compiled_out test_get_local_rank_compile mesh = init_device_mesh device_type world_size mesh_dim_names= dp fn_with_str_arg x local_rank = x device_mesh get_local_rank dp x local_rank x = DTensor from_local torch rand mesh Shard run_check=False ref = fn_with_str_arg x opt_fn = torch compile fn_with_str_arg backend= aot_eager fullgraph=True res = opt_fn x assertEqual res ref fn_with_int_arg x local_rank = x device_mesh get_local_rank x local_rank ref = fn_with_int_arg x opt_fn = torch compile fn_with_int_arg backend= aot_eager fullgraph=True res = opt_fn x assertEqual res ref fn_without_arg x will fail device_mesh ndim local_rank = x device_mesh get_local_rank x + local_rank ref = fn_without_arg x opt_fn = torch compile fn_without_arg backend= aot_eager fullgraph=True res = opt_fn x assertEqual res ref test_fakify_dtensor mesh = DeviceMesh device_type torch arange world_size pass DTensor inputs outputs function fn x x x = DTensor from_local torch rand mesh Shard run_check=False ref = fn x opt_fn = torch compile fn backend= aot_eager fullgraph=True res = opt_fn x assertEqual res ref test_dynamo_dtensor mesh = DeviceMesh device_type torch arange world_size test passing DTensor inputs outputs run some tensor computation fn x x x + x = DTensor from_local torch rand mesh Shard run_check=False ref = fn x opt_fn = torch compile fn backend= aot_eager fullgraph=True res = opt_fn x assertEqual res ref skipIfHpu test_dtensor_dynamic mesh = DeviceMesh device_type torch arange world_size test passing DTensor inputs outputs run some tensor computation fn x torch mul x x redistribute device_mesh=x device_mesh placements= Replicate to_local x = DTensor from_local torch rand requires_grad=True mesh Shard run_check=False torch _dynamo mark_dynamic x ref = fn x opt_fn = torch compile fn backend= aot_eager fullgraph=True res = opt_fn x assertEqual res ref skipIfHpu unittest skip DTensor + dynamic fails - s + tracked proxy proxy_tensor PythonKeyTracer test_dtensor_dynamic_slice mesh = DeviceMesh device_type torch arange world_size test passing DTensor inputs outputs run some tensor computation fn x t redistribute device_mesh=x device_mesh placements= Replicate to_local t torch tensor_split x x = DTensor from_local torch rand requires_grad=True mesh Shard run_check=False ref = fn x opt_fn = torch compile fn backend= aot_eager fullgraph=True dynamic=True res = opt_fn x assertEqual res ref skipIfHpu test_dtensor_dynamic_loss_parallel_log_softmax mesh = DeviceMesh device_type torch arange world_size fn x t = torch nn functional log_softmax x x ndim - dtype=torch float t redistribute device_mesh=x device_mesh placements= Replicate to_local loss_parallel x = DTensor from_local torch rand mesh Shard run_check=False ref = fn x opt_fn = torch compile fn backend= aot_eager fullgraph=True dynamic=True res = opt_fn x assertEqual res ref unittest skip DTensor + dynamic fails - s + tracked proxy proxy_tensor PythonKeyTracer test_dtensor_dynamic_cat mesh = DeviceMesh device_type torch arange world_size test passing tuple DTensors fn x y torch cat x y dim= redistribute device_mesh=x device_mesh placements= Replicate to_local x = DTensor from_local torch rand requires_grad=True mesh Shard run_check=False y = DTensor from_local torch rand requires_grad=True mesh Shard run_check=False torch _dynamo mark_dynamic x ref = fn x y opt_fn = torch compile fn backend= aot_eager fullgraph=True res = opt_fn x y assertEqual res ref test_dtensor_dynamic_recompiles cnt = torch _dynamo testing CompileCounterWithBackend aot_eager mesh = DeviceMesh device_type torch arange world_size inp shape param = torch randn shape requires_grad=True x = DTensor from_local param mesh Shard run_check=False torch _dynamo mark_dynamic x torch _dynamo mark_dynamic x x run func shape res = func inp shape res sum backward torch compile backend=cnt fullgraph=True f x y = x x y to_local run f run f run f assertEqual cnt frame_count sanity check shape guard recompiles still handled torch compile backend=cnt fullgraph=True g x x size = y = x x y = x + x y to_local cnt clear run g run g assertEqual cnt frame_count run g assertEqual cnt frame_count test_dtensor_attribute_access_on_intermediate mesh = DeviceMesh device_type torch arange world_size fn x tmp = x tmp placements is_shard tmp _local_tensor + tmp _local_tensor + x = DTensor from_local torch ones mesh Shard run_check=False ref = fn x opt_fn = torch compile fn backend= aot_eager fullgraph=True res = opt_fn x assertEqual res ref test_dtensor_constructor_w_graph_break mesh = DeviceMesh device_type torch arange world_size x = torch randn requires_grad=True spec = DTensorSpec mesh Replicate Shard tensor_meta=TensorMeta shape=torch Size stride= dtype=x dtype test passing DTensor inputs outputs run some tensor computation fn x print graph break DTensor x spec requires_grad=x requires_grad fn x torch compile fn backend= eager x test_dtensor_constructor_w_dynamo_disable mesh = DeviceMesh device_type torch arange world_size x = torch randn requires_grad=True spec = DTensorSpec mesh Replicate tensor_meta=TensorMeta shape=torch Size stride= dtype=x dtype torch _dynamo disable recursive=False fn x print foo DTensor x spec requires_grad=x requires_grad out = fn x out = torch compile fn backend= eager x assertEqual out out test_dtensor_noncontiguous_output mesh = DeviceMesh device_type torch arange world_size test passing DTensor inputs outputs run some tensor computation fn x y z x_transposed = x permute contiguous tmp = torch _C _nn linear x_transposed y z tmp permute x_inner = torch randn requires_grad=True y_inner = torch randn requires_grad=True z_inner = torch randn requires_grad=True x = DTensor from_local x_inner mesh Shard run_check=False y = DTensor from_local y_inner mesh Shard run_check=False z = DTensor from_local z_inner mesh Replicate run_check=False out = torch compile fn backend= aot_eager fullgraph=True x y z out contiguous sum backward test_dynamo_dtensor_from_local mesh = DeviceMesh device_type torch arange world_size create DTensor inside fn run some compute fn x dt = DTensor from_local x mesh Replicate run_check=False dt to_local + below op approach reference torch distributed _tensor api _FromTorchTensor from_local_tensor x _FromTorchTensor apply x mesh Replicate False _dt_lib_def = torch library Library dtensor DEF _dt_lib_def define from_local Tensor - Tensor _dt_lib_impl = torch library Library dtensor IMPL _dt_lib_impl impl from_local from_local_tensor Autograd x = torch ones requires_grad=True ref = fn x cnt = torch _dynamo testing CompileCounterWithBackend aot_eager opt_fn = torch compile fn backend=cnt fullgraph=True res = opt_fn x backward should work well res sum backward assertEqual res ref assertEqual cnt frame_count test user calls from_local mesh placements kwargs should still work from_local_kwargs_fn x dt = DTensor from_local x device_mesh=mesh placements= Replicate run_check=False dt to_local + ref = from_local_kwargs_fn x opt_kwargs_fn = torch compile from_local_kwargs_fn backend=cnt fullgraph=True res = opt_kwargs_fn x assertEqual res ref assertEqual cnt frame_count test_dynamo_dtensor_from_local_dynamic_shapes mesh = DeviceMesh device_type torch arange world_size Case all dims dynamic fn x dt = DTensor from_local x mesh Replicate run_check=False shape=x shape stride=x stride dt to_local + inp = torch randn requires_grad=True ref = fn inp cnt = torch _dynamo testing CompileCounterWithBackend aot_eager res = torch compile fn backend=cnt fullgraph=True dynamic=True inp res sum backward assertEqual res ref assertEqual cnt frame_count Case only sizes dynamic strides static fn x dt = DTensor from_local x mesh Replicate run_check=False shape=x shape stride= dt to_local + inp = torch randn requires_grad=True torch _dynamo mark_dynamic inp ref = fn inp cnt = torch _dynamo testing CompileCounterWithBackend aot_eager res = torch compile fn backend=cnt fullgraph=True inp res sum backward assertEqual res ref assertEqual cnt frame_count Case both sizes strides have mix dynamic static dims fn x dt = DTensor from_local x mesh Replicate run_check=False shape= x shape x shape stride= x stride dt to_local + inp = torch randn requires_grad=True torch _dynamo mark_dynamic inp torch _dynamo mark_dynamic inp ref = fn inp cnt = torch _dynamo testing CompileCounterWithBackend aot_eager res = torch compile fn backend=cnt fullgraph=True inp res sum backward assertEqual res ref assertEqual cnt frame_count test_dynamo_dtensor_recompile mesh = DeviceMesh device_type torch arange world_size test passing DTensor inputs outputs run some tensor computation fn x torch mul x x x = DTensor from_local torch rand mesh Shard run_check=False x = DTensor from_local torch rand mesh Shard run_check=False x = DTensor from_local torch rand mesh Shard run_check=False cnt = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnt fullgraph=True dynamic=False assertEqual fn x opt_fn x assertEqual cnt frame_count assertEqual fn x opt_fn x assertEqual cnt frame_count assertEqual fn x opt_fn x assertEqual cnt frame_count skipIfHpu test_dtensor_partial_placement_redistribute_unbalanced_correct_strides Partial - Shard unbalanced tensor results - A contiguous DTensor - where inner _local_tensor noncontiguous placement = Shard fn x out = x redistribute mesh placement out Temporarily ignore setUp use rank graphs during tracing dist destroy_process_group fake_store = FakeStore dist init_process_group fake store=fake_store rank= world_size= mesh = DeviceMesh device_type x = torch randn requires_grad=True x_dt = DTensor from_local x mesh Partial run_check=False shape= stride= tmp_dt has inner non-contiguous tensor autograd non-leaf tmp_dt = fn x_dt fake_mode = torch _subclasses FakeTensorMode tmp_dt_fake = fake_mode from_tensor tmp_dt assertEqual tmp_dt shape tmp_dt_fake shape assertEqual tmp_dt stride tmp_dt_fake stride assertEqual tmp_dt _local_tensor shape tmp_dt_fake _local_tensor shape assertEqual tmp_dt _local_tensor stride tmp_dt_fake _local_tensor stride unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_dtensor_contiguous_dtensor_noncontiguous_local_as_tangent Partial - Shard unbalanced tensor results - A contiguous DTensor - where inner _local_tensor noncontiguous When tensor fwd graph output AOTAutograd needs make sure we trace backward contiguous tangent placement = Shard fn x out = x redistribute mesh placement out Temporarily ignore setUp use rank graphs during tracing dist destroy_process_group fake_store = FakeStore dist init_process_group fake store=fake_store rank= world_size= mesh = DeviceMesh device_type x = torch randn requires_grad=True x_dt = DTensor from_local x mesh Partial run_check=False shape= stride= out_dt = torch compile fn x_dt If we don t properly contiguify our traced tangents fails inductor stride assert out_dt to_local sum backward test_dynamo_to_local_kwargs mesh = DeviceMesh device_type torch arange world_size fn x dt to_local grad_placements= Shard + fn_opt = torch compile fn backend= aot_eager fullgraph=True x = torch ones dt = DTensor from_local x mesh Replicate run_check=False out_ref = fn dt out_test = fn_opt dt assertEqual out_ref out_test test_dynamo_to_local_kwargs_forward_hook mesh = DeviceMesh device_type torch arange world_size fw_hook module inp out tmp = out to_local grad_placements=out placements + DTensor from_local tmp mesh out placements run_check=False mod = torch nn Linear mod register_forward_hook fw_hook mod = torch nn Linear mod register_forward_hook fw_hook mod weight = torch nn Parameter DTensor from_local mod weight mesh Replicate run_check=False mod bias = torch nn Parameter DTensor from_local mod bias mesh Replicate run_check=False opt_mod = torch compile mod backend= aot_eager fullgraph=True x = torch ones dt = DTensor from_local x mesh Replicate run_check=False out_ref = mod dt out_test = opt_mod dt assertEqual out_ref out_test unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_dtensor_different_gradient_placement mesh = DeviceMesh device_type torch arange world_size fn x y z permute = x permute permute = permute contiguous layer_norm = torch nn functional layer_norm permute y z e- out = layer_norm permute out x = torch randn requires_grad=True device= cuda x_dt = DTensor from_local x mesh Shard run_check=False y = torch randn requires_grad=True device= cuda y_dt = DTensor from_local y mesh Replicate run_check=False z = torch randn requires_grad=True device= cuda z_dt = DTensor from_local z mesh Replicate run_check=False opt_fn = torch compile fn backend= inductor fullgraph=True tmp_dt = opt_fn x_dt y_dt z_dt out_dt = torch matmul tmp_dt x_dt permute out_dt sum backward test_dynamo_dtensor_from_local_redistribute mesh = DeviceMesh device_type torch arange world_size pass tensor inputs outputs create DTensor run redistribute allgather collective inside fn fn x dt = DTensor from_local x mesh Shard run_check=False dt redistribute mesh Replicate to_local + x = torch ones ref = fn x cnt = torch _dynamo testing CompileCounterWithBackend aot_eager opt_fn = torch compile fn backend=cnt fullgraph=True res = opt_fn x assertEqual res ref redistribute_kwargs_fn x dt = DTensor from_local x mesh Shard run_check=False dt redistribute device_mesh=mesh placements= Replicate to_local + x = torch ones ref = redistribute_kwargs_fn x opt_kwargs_fn = torch compile redistribute_kwargs_fn backend=cnt fullgraph=True res = opt_kwargs_fn x assertEqual res ref skipIfHpu test_dynamo_dtensor_from_local_redistribute_async mesh = DeviceMesh device_type torch arange world_size torch distributed _functional_collectives AsyncCollectiveTensor pass tensor inputs outputs create DTensor run redistribute allgather collective inside fn fn x dt = DTensor from_local x mesh Shard run_check=False out = dt redistribute mesh Replicate async_op=True to_local isinstance out AsyncCollectiveTensor out wait out x = torch ones ref = fn x cnt = torch _dynamo testing CompileCounterWithBackend aot_eager opt_fn = torch compile fn backend=cnt fullgraph=True res = opt_fn x assertEqual res ref test_dtensor_dont_recompile_on_same_placement_devicemesh cnt = torch _dynamo testing CompileCounterWithBackend inductor torch compile backend=cnt fn x DTensor from_local x mesh placement run_check=False x = torch ones requires_grad=True mesh = DeviceMesh device_type torch arange world_size placement = Shard fn x mesh = DeviceMesh device_type torch arange world_size placement = Shard no recompile placement unchanged fn x mesh = DeviceMesh device_type torch arange world_size placement = Partial recompile since placement different fn x mesh = DeviceMesh device_type torch arange world_size placement = Partial no recompile placement unchanged fn x total frames one Partial one Shard assertEqual cnt frame_count test_dtensor_dynamo_device_mesh_attrs mesh = DeviceMesh device_type torch arange world_size pass tensor inputs outputs create DTensor run redistribute allgather collective inside fn fn x_dt x_dt device_mesh device_type == cuda x_dt + x_dt + x = torch ones x_dt = DTensor from_local x mesh Shard run_check=False ref = fn x_dt opt_fn = torch compile fn backend= eager fullgraph=True res = opt_fn x_dt assertEqual ref res skipIfHpu test_graph_input_is_async torch distributed _functional_collectives AsyncCollectiveTensor mesh = DeviceMesh device_type torch arange world_size fn x x sin sin opt_fn = torch compile fn backend=aot_eager_graph fullgraph=True x = torch randn requires_grad=True x_dt = DTensor from_local x mesh Shard run_check=False x = x_dt redistribute mesh Replicate async_op=True x = x to_local assertTrue isinstance x AsyncCollectiveTensor opt_fn x The important part we get wait_tensor graph At runtime input graph AsyncCollectiveTensor inside graph we need issue wait synchronize assertExpectedInline str fw_graph_cell strip \ forward primals_ wait_tensor = torch ops _c d_functional wait_tensor default primals_ sin = torch ops aten sin default wait_tensor sin_ = torch ops aten sin default sin sin = None sin_ primals_ wait_tensor skipIfTorchDynamo test_unwrap_async_collective_tensor_tangent torch distributed _functional_collectives AsyncCollectiveTensor fn x x clone ref_x = TwoTensor torch randn requires_grad=True torch randn requires_grad=True ref_y = fn ref_x ref_y backward gradient=TwoTensor torch randn torch randn fn_comp = torch compile fn fullgraph=True x = TwoTensor torch randn requires_grad=True torch randn requires_grad=True y = fn_comp x y backward gradient=TwoTensor torch randn torch randn x = TwoTensor torch randn requires_grad=True torch randn requires_grad=True y = fn_comp x y backward gradient=TwoTensor AsyncCollectiveTensor torch randn AsyncCollectiveTensor torch randn unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_dtensor_partial_placement_graph_output mesh = DeviceMesh device_type torch arange world_size fn x x + x x = torch randn requires_grad=True x_dt = DTensor from_local x mesh Partial run_check=False y = torch randn requires_grad=True y_dt = DTensor from_local y mesh Replicate run_check=False opt_fn = torch compile fn backend= inductor fullgraph=True tmp_dt = opt_fn x_dt out_dt = torch matmul tmp_dt y_dt out_dt sum backward unittest skipIf torch _inductor config triton native_matmul Matmul now generated _test_tp_compile_comm_reordering FakeAttention nn Module __init__ - None super __init__ wq = nn Linear wk = nn Linear wv = nn Linear wo = nn Linear forward x xq = wq x xk = wk x xv = wv x fake attention xo = xq + xk + xv wo xo FakeTransformerBlock nn Module __init__ - None super __init__ attn = FakeAttention forward x attn x FakeTransformer nn Module __init__ - None super __init__ block = FakeTransformerBlock forward input block input model = FakeTransformer device_type tp_mesh = init_device_mesh cuda mesh_dim_names= tp apply sequence parallel parallel_plan = attn PrepareModuleInput input_layouts=Shard desired_input_layouts=Replicate attn wq ColwiseParallel attn wk ColwiseParallel attn wv ColwiseParallel attn wo RowwiseParallel output_layouts=Shard parallelize_module module=model block device_mesh=tp_mesh parallelize_plan=parallel_plan cnt = torch _dynamo testing CompileCounterWithBackend inductor compiled_model = torch compile model backend=cnt fullgraph=True inp = torch rand device_type out = compiled_model inp out sum backward assertEqual cnt frame_count code = run_and_get_triton_code compiled_model inp FileCheck check buf = torch ops _c d_functional all_gather_into_tensor default primal check torch ops _c d_functional wait_tensor default buf check extern_kernels mm buf run code unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu TODO somehow inductor bg compile threads causing hangs exit distributed work dtor patch object torch _inductor config compile_threads patch object torch _inductor config reorder_for_compute_comm_overlap True test_tp_compile_comm_reordering _test_tp_compile_comm_reordering unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu TODO somehow inductor bg compile threads causing hangs exit distributed work dtor patch object torch _inductor config compile_threads patch object torch _inductor config reorder_for_compute_comm_overlap True torch _inductor config patch graph_partition True test_tp_compile_comm_reordering_graph_partition _test_tp_compile_comm_reordering instantiate_parametrized_tests TestDTensorCompileE E DTensorTestBase property world_size multiprocess relies pickling source code so compiled autograd tests can t dynamically wrap _bwd_ctx use_ca use_ca contextlib nullcontext torch _dynamo compiled_autograd _enable torch compile with_comms parametrize is_seq_parallel True False parametrize use_ca True False test_tp_compile_fullgraph is_seq_parallel use_ca mesh = DeviceMesh device_type torch arange world_size model = SimpleModel device_type colwise_style = ColwiseParallel input_layouts=Shard is_seq_parallel ColwiseParallel rowwise_style = RowwiseParallel output_layouts=Shard is_seq_parallel RowwiseParallel is_seq_parallel use input preparation test out compile prepare_module_input = PrepareModuleInput input_layouts=Shard desired_input_layouts=Replicate prepare_module_out = PrepareModuleOutput output_layouts=Replicate desired_output_layouts=Shard plan = mlp_ prepare_module_input mlp_ net ColwiseParallel mlp_ net rowwise_style mlp_ net colwise_style mlp_ net RowwiseParallel mlp_ prepare_module_out plan = mlp_ net colwise_style mlp_ net rowwise_style mlp_ net colwise_style mlp_ net rowwise_style model = parallelize_module model mesh parallelize_plan=plan rng_seed = rank is_seq_parallel torch manual_seed rng_seed inp = torch rand device=self device_type out = model inp cnt = torch _dynamo testing CompileCounterWithBackend aot_eager compiled_mod = torch compile model backend=cnt fullgraph=True compiled_out = compiled_mod inp _bwd_ctx use_ca compiled_out sum backward assertEqual compiled_out out assertEqual cnt frame_count with_comms skip_if_lt_x_gpu parametrize use_ca True False test_ d_fsdp_tp_compile use_ca data_parallel_size = model = SimpleModel device_type model_copy = copy deepcopy model -D mesh dp tp twod_mesh = init_device_mesh device_type data_parallel_size world_size data_parallel_size mesh_dim_names= dp tp inp = torch rand device=self device_type parallelize_plan = mlp_ net ColwiseParallel mlp_ net RowwiseParallel mlp_ net ColwiseParallel mlp_ net RowwiseParallel tp_model = parallelize_module model twod_mesh tp parallelize_plan eager_ d = FSDP tp_model device_id=dev_type type use_orig_params=True device_mesh=twod_mesh dp out = eager_ d inp tp_model = parallelize_module model_copy twod_mesh tp parallelize_plan fsdp_ d = FSDP tp_model device_id=dev_type type use_orig_params=True device_mesh=twod_mesh dp TODO once aot autograd support ready we can just use default backend cnt = torch _dynamo testing CompileCounterWithBackend aot_eager compiled_ d = torch compile fsdp_ d backend=cnt compiled_output = compiled_ d inp _bwd_ctx use_ca compiled_output sum backward assertEqual out compiled_output assertEqual cnt frame_count with_comms skip_if_lt_x_gpu parametrize use_ca True False test_ d_fsdp_tp_ac_compile use_ca dp_degree = tp_degree = world_size dp_degree model = SimpleModel device_type model_copy = copy deepcopy model -D mesh dp tp mesh_ d = init_device_mesh device_type mesh_shape= dp_degree tp_degree mesh_dim_names= dp tp inp = torch rand device=self device_type parallelize_plan = mlp_ net ColwiseParallel mlp_ net RowwiseParallel mlp_ net ColwiseParallel mlp_ net RowwiseParallel tp_model = parallelize_module model mesh_ d tp parallelize_plan tp_model = checkpoint_wrapper tp_model checkpoint_impl=CheckpointImpl NO_REENTRANT checkpoint_fn=checkpoint use_reentrant=False eager_ d = FSDP tp_model device_mesh=mesh_ d dp use_orig_params=True tp_model = parallelize_module model_copy mesh_ d tp parallelize_plan fsdp_ d = FSDP tp_model device_mesh=mesh_ d dp use_orig_params=True TODO once aot autograd support ready we can just use default backend compiled_ d = torch compile fsdp_ d backend= aot_eager forward pass out = eager_ d inp compiled_output = compiled_ d inp assertEqual out compiled_output backward pass out sum backward _bwd_ctx use_ca compiled_output sum backward compare gradients n p zip fsdp_ d parameters compiled_ d parameters assertEqual n grad p grad with_comms skip_if_lt_x_gpu parametrize use_ca True False test_compile_dtensor_redistribute_backward use_ca mesh = DeviceMesh device_type=self device_type mesh=torch arange world_size fn x y dt = DTensor from_local x reshape mesh Shard run_check=False dt = DTensor from_local y reshape mesh Shard run_check=False dt_out = torch matmul dt dt dt_out_redistribute = dt_out redistribute mesh Replicate dt_out_redistribute to_local opt_fn = torch compile fn backend=aot_eager_graph fullgraph=True x_ref = torch arange requires_grad=True dtype=torch float y_ref = torch arange requires_grad=True dtype=torch float ref = fn x_ref y_ref x = torch arange requires_grad=True dtype=torch float y = torch arange requires_grad=True dtype=torch float res = opt_fn x y assertEqual res ref Now run assert backward + gradients ref sum backward _bwd_ctx use_ca res sum backward assertEqual x_ref grad x grad assertEqual y_ref grad y grad with_comms test_compile_embedding_redistribute mesh = build_device_mesh Network nn Module __init__ embedding mesh super __init__ mesh = mesh embedding = _apply_sharding embedding mesh forward x x = embedding x x = x redistribute mesh Shard x embedding = torch nn Embedding device=self device_type inp = torch randint device=self device_type ref_out = embedding inp sharded_net = torch compile Network embedding mesh replicated_inp = DTensor from_local inp mesh Replicate run_check=False output = sharded_net replicated_inp assertEqual output full_tensor ref_out __name__ == __main__ run_tests