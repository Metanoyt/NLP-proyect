mypy allow-untyped-defs Gradient interface torch torch nn modules utils _pair _single _triple conv d_input input_size weight grad_output stride= padding= dilation= groups= r Compute gradient conv d respect input convolution This same D transposed convolution operator under hood requires shape gradient w r t input specified explicitly Args input_size Shape input gradient tensor weight weight tensor out_channels x in_channels groups x kW grad_output output gradient tensor minibatch x out_channels x oW stride int tuple optional Stride convolution Default padding int tuple optional Zero-padding added both sides input Default dilation int tuple optional Spacing between kernel elements Default groups int optional Number blocked connections input channels output channels Default Examples input = torch randn requires_grad=True weight = torch randn requires_grad=True output = F conv d input weight grad_output = torch randn output shape grad_input = torch autograd grad output input grad_output F grad conv d_input input shape weight grad_output input = grad_output new_empty expand input_size torch ops aten convolution_backward grad_output input weight None _single stride _single padding _single dilation False groups True False False conv d_weight input weight_size grad_output stride= padding= dilation= groups= r Compute gradient conv d respect weight convolution Args input input tensor shape minibatch x in_channels x iW weight_size Shape weight gradient tensor grad_output output gradient tensor minibatch x out_channels x oW stride int tuple optional Stride convolution Default padding int tuple optional Zero-padding added both sides input Default dilation int tuple optional Spacing between kernel elements Default groups int optional Number blocked connections input channels output channels Default Examples input = torch randn requires_grad=True weight = torch randn requires_grad=True output = F conv d input weight grad_output = torch randn output shape xdoctest +SKIP grad_weight = torch autograd grad output filter grad_output F grad conv d_weight input weight shape grad_output weight = grad_output new_empty expand weight_size torch ops aten convolution_backward grad_output input weight None _single stride _single padding _single dilation False groups False True False conv d_input input_size weight grad_output stride= padding= dilation= groups= r Compute gradient conv d respect input convolution This same D transposed convolution operator under hood requires shape gradient w r t input specified explicitly Args input_size Shape input gradient tensor weight weight tensor out_channels x in_channels groups x kH x kW grad_output output gradient tensor minibatch x out_channels x oH x oW stride int tuple optional Stride convolution Default padding int tuple optional Zero-padding added both sides input Default dilation int tuple optional Spacing between kernel elements Default groups int optional Number blocked connections input channels output channels Default Examples input = torch randn requires_grad=True weight = torch randn requires_grad=True output = F conv d input weight grad_output = torch randn output shape grad_input = torch autograd grad output input grad_output F grad conv d_input input shape weight grad_output input = grad_output new_empty expand input_size torch ops aten convolution_backward grad_output input weight None _pair stride _pair padding _pair dilation False groups True False False conv d_weight input weight_size grad_output stride= padding= dilation= groups= r Compute gradient conv d respect weight convolution Args input input tensor shape minibatch x in_channels x iH x iW weight_size Shape weight gradient tensor grad_output output gradient tensor minibatch x out_channels x oH x oW stride int tuple optional Stride convolution Default padding int tuple optional Zero-padding added both sides input Default dilation int tuple optional Spacing between kernel elements Default groups int optional Number blocked connections input channels output channels Default Examples input = torch randn requires_grad=True weight = torch randn requires_grad=True output = F conv d input weight grad_output = torch randn output shape xdoctest +SKIP grad_weight = torch autograd grad output filter grad_output F grad conv d_weight input weight shape grad_output weight = grad_output new_empty expand weight_size torch ops aten convolution_backward grad_output input weight None _pair stride _pair padding _pair dilation False groups False True False conv d_input input_size weight grad_output stride= padding= dilation= groups= r Compute gradient conv d respect input convolution This same D transposed convolution operator under hood requires shape gradient w r t input specified explicitly Args input_size Shape input gradient tensor weight weights tensor out_channels x in_channels groups x kT x kH x kW grad_output output gradient tensor minibatch x out_channels x oT x oH x oW stride int tuple optional Stride convolution Default padding int tuple optional Zero-padding added both sides input Default dilation int tuple optional Spacing between kernel elements Default groups int optional Number blocked connections input channels output channels Default Examples input = torch randn requires_grad=True weight = torch randn requires_grad=True output = F conv d input weight grad_output = torch randn output shape grad_input = torch autograd grad output input grad_output F grad conv d_input input shape weight grad_output input = grad_output new_empty expand input_size torch ops aten convolution_backward grad_output input weight None _triple stride _triple padding _triple dilation False groups True False False conv d_weight input weight_size grad_output stride= padding= dilation= groups= r Compute gradient conv d respect weight convolution Args input input tensor shape minibatch x in_channels x iT x iH x iW weight_size Shape weight gradient tensor grad_output output gradient tensor minibatch x out_channels x oT x oH x oW stride int tuple optional Stride convolution Default padding int tuple optional Zero-padding added both sides input Default dilation int tuple optional Spacing between kernel elements Default groups int optional Number blocked connections input channels output channels Default Examples input = torch randn requires_grad=True weight = torch randn requires_grad=True output = F conv d input weight grad_output = torch randn output shape grad_weight = torch autograd grad output weight grad_output F grad conv d_weight input weight shape grad_output weight = grad_output new_empty expand weight_size torch ops aten convolution_backward grad_output input weight None _triple stride _triple padding _triple dilation False groups False True False