Owner s oncall distributed contextlib itertools os sys functools partial itertools product typing Any torch torch cuda nccl nccl torch nn nn torch nn functional F torch distributed dist torch distributed fsdp BackwardPrefetch CPUOffload FullyShardedDataParallel FSDP MixedPrecision ShardingStrategy torch distributed fsdp sharded_grad_scaler ShardedGradScaler torch distributed fsdp wrap ModuleWrapPolicy size_based_auto_wrap_policy torch nn modules batchnorm _BatchNorm torch optim swa_utils AveragedModel torch testing _internal common_distributed SaveForwardInputsModel skip_if_lt_x_gpu torch testing _internal common_fsdp DEVICEInitMode FSDPInitMode FSDPTest subtest_name TransformerWithSharedParams torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN try torchvision HAS_TORCHVISION = True except ImportError HAS_TORCHVISION = False skipIfNoTorchVision = skip_but_pass_in_sandcastle_if HAS_TORCHVISION no torchvision dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit Various mixed precision configs test under default_mp = MixedPrecision param_dtype=torch float buffer_dtype=torch float reduce_dtype=torch float Params buffers cast comm only happens reduced precision mp_only_reduce = MixedPrecision reduce_dtype=torch float Only parameters cast thus comm should happen param_dtype precision mp_only_param_and_buf = MixedPrecision param_dtype=torch float buffer_dtype=torch float Nothing cast thus param comm grad buffer should full precision mp_no_mixed_precision = MixedPrecision nccl_supports_bf = dist is_nccl_available nccl version = mp_configs = default_mp mp_only_reduce mp_only_param_and_buf mp_no_mixed_precision nccl_supports_bf mp_diff_buffer_and_reduce = MixedPrecision param_dtype=torch float buffer_dtype=torch bfloat reduce_dtype=torch float mp_configs extend mp_diff_buffer_and_reduce Buffer original dtype which can differ model params _BUFFER_ORIG_DTYPE = torch float params = mp_config cpu_offload full_precision_param_dtype enable_sharded_grad_scaler cpu_offload_config = CPUOffload offload_params=True CPUOffload offload_params=False full_precision_param_dtype_config = torch float torch float enable_sharded_grad_scaler = enable_sharded_grad_scaler None configs = list product mp_configs cpu_offload_config full_precision_param_dtype_config enable_sharded_grad_scaler test_name_mapping = str CPUOffload offload_params=True offload_true str CPUOffload offload_params=False offload_false str default_mp mp_fp str mp_only_reduce mp_only_reduce str mp_only_param_and_buf mp_only_param_and_buf str mp_no_mixed_precision mp_no_mp str torch float fp str torch float fp enable_sharded_grad_scaler enable_sharded_grad_scaler nccl_supports_bf test_name_mapping update str mp_diff_buffer_and_reduce mp_diff_buffer_reduce subtest_name = partial subtest_name test_name_mapping _CURRENT_FULL_PRECISION_PARAM_DTYPE = None contextlib contextmanager patch_reduce_scatter new_reduce_scatter full_precision_param_dtype Patches ` ` dist reduce_scatter_tensor ` ` ` ` new_reduce_scatter ` ` restores upon exiting Used validation mixed precision orig_reduce_scatter = dist reduce_scatter_tensor dist reduce_scatter_tensor = new_reduce_scatter global _CURRENT_FULL_PRECISION_PARAM_DTYPE _CURRENT_FULL_PRECISION_PARAM_DTYPE = full_precision_param_dtype try yield finally dist reduce_scatter_tensor = orig_reduce_scatter _CURRENT_FULL_PRECISION_PARAM_DTYPE = None LinearMixedPrecision nn Module A linear module extra checks mixed precision training __init__ param_dtype buffer_name= buffer run_checks=True super __init__ lin = nn Linear bias=False param_dtype Use configurable buffer name avoid all submodules sharing same buffer name which may hide prefixed vs unprefixed name bugs buffer_name = buffer_name register_buffer buffer_name torch randn dtype=_BUFFER_ORIG_DTYPE _orig_param_type = param_dtype _orig_buffer_dtype = _BUFFER_ORIG_DTYPE run_checks = run_checks forward tup inp cls fsdp mp_config full_precision_param_dtype = tup run_checks Param input should mixed precision type expected_param_type = mp_config param_dtype mp_config param_dtype None _orig_param_type expected_buffer_type = mp_config buffer_dtype mp_config buffer_dtype None _orig_buffer_dtype cls assertEqual inp dtype expected_param_type Buffer should specified precision well cls assertEqual getattr buffer_name dtype expected_buffer_type In FSDP params should point right type num_active_fsdp = fsdp_module FSDP fsdp_modules fsdp fsdp_managed_params = fsdp_module params Single param assumption cls assertEqual len fsdp_managed_params param fsdp_managed_params FSDP unit currently active using param local shard This supports both FULL_SHARD SHARD_GRAD_OP cases In FULL_SHARD we have additional property param _full_param_padded has been freed param_is_sharded = fsdp_module sharding_strategy = ShardingStrategy NO_SHARD fsdp_module world_size is_fsdp_unit_active = param_is_sharded param data data_ptr = param _local_shard data_ptr is_fsdp_unit_active num_active_fsdp += This FSDP unit active verify param points mixed cls assertEqual param dtype expected_param_type _unshard should have also freed fp shard Shard never allocated param_dtype mixed precision enabled mp_config param_dtype None cls assertEqual param _mp_shard untyped_storage size cls assertFalse hasattr param _mp_shard param_is_sharded This FSDP unit active full param has been freed yet allocated Ensure param points full precision param cls assertEqual param dtype full_precision_param_dtype We should have gotten least one active FSDP unit sharded world size cases For cases where param sharded ie world_size == bit hard check FSDP unit active we d always point local shard so we rely forward pass lin inp working well inp being reduced precision implicitly validate param indeed reduced precision cls world_size cls assertGreater num_active_fsdp lin inp cls fsdp mp_config full_precision_param_dtype TestFSDPMixedPrecision FSDPTest property world_size raise ValueError To implemented child classes _get_simple_nested_model param_dtype run_checks fsdp_args fsdp_kwargs model = FSDP nn Sequential FSDP LinearMixedPrecision param_dtype buffer_name= buffer run_checks=run_checks cuda fsdp_args fsdp_kwargs LinearMixedPrecision param_dtype buffer_name= buffer run_checks=run_checks cuda fsdp_args fsdp_kwargs model _get_simple_model param_dtype fsdp_args fsdp_kwargs model = FSDP LinearMixedPrecision param_dtype cuda fsdp_args fsdp_kwargs model _validate_no_mp_shard fsdp_model Validates there no mixed precision _mp_shard allocated when expected fsdp_units = FSDP fsdp_modules fsdp_model fsdp fsdp_units param fsdp params assertFalse hasattr param _mp_shard _validate_mp_shard_freed fsdp_model Ensures mixed precision shard greed all FSDP units fsdp_units = FSDP fsdp_modules fsdp_model fsdp fsdp_units param fsdp params assertEqual param _mp_shard untyped_storage size _reduce_scatter_validate_mp orig_reduce_scatter mp_config should_run_low_prec args kwargs Runs reduce-scatter verifies mixed precision settings before This test mixed precision working expected during backward pass In particular ensures gradients cast right type comm going happen right type tensors = x args isinstance x torch Tensor tensors append x x kwargs values isinstance x torch Tensor tensors append x reduce_dtype has higher priority than param_dtype because mixed_precision supports overriding param_dtype reduce_dtype control reduction precision In case where reduce_dtype == param_dtype tests gradients expected precision well If reduce_dtype specified None we comm param_dtype specified otherwise full precision dtype should_run_low_prec expected_dtype = mp_config reduce_dtype mp_config reduce_dtype None mp_config param_dtype mp_config param_dtype None _CURRENT_FULL_PRECISION_PARAM_DTYPE expected_dtype = _CURRENT_FULL_PRECISION_PARAM_DTYPE t tensors assertEqual expected_dtype t dtype f Expected reduce expected_dtype got tensors t dtype orig_reduce_scatter args kwargs _test_grads_reduced_precision offload_params bool use_orig_params bool MyModel nn Module __init__ - None super __init__ lin = nn Linear lin = nn Linear forward x lin lin x m = MyModel cuda mp = MixedPrecision param_dtype=torch float reduce_dtype=torch float buffer_dtype=torch float keep_low_precision_grads=True fsdp_kwargs = mixed_precision mp cpu_offload CPUOffload offload_params=offload_params use_orig_params use_orig_params m lin = FSDP m lin fsdp_kwargs m = FSDP m fsdp_kwargs _ range inp = torch ones m inp sum backward param m parameters param grad None assertEqual torch float param grad dtype dist barrier _run_test_mixed_precision_e e mp_config cpu_offload backward_prefetch forward_prefetch full_precision_param_dtype sharding_strategy enable_sharded_grad_scaler torch cuda set_device rank fsdp_models = _get_simple_model param_dtype=full_precision_param_dtype sharding_strategy=sharding_strategy cpu_offload=cpu_offload mixed_precision=mp_config backward_prefetch=backward_prefetch forward_prefetch=forward_prefetch _get_simple_nested_model param_dtype=full_precision_param_dtype run_checks=True sharding_strategy=sharding_strategy cpu_offload=cpu_offload mixed_precision=mp_config backward_prefetch=backward_prefetch forward_prefetch=forward_prefetch model fsdp_models cpu_offload offload_params model cuda Patch reduce_scatter add validation mixed precision types orig_reduce_scatter = dist reduce_scatter_tensor test_reduce_scatter = partial _reduce_scatter_validate_mp orig_reduce_scatter mp_config True patch_reduce_scatter test_reduce_scatter full_precision_param_dtype scaler = ShardedGradScaler enabled=enable_sharded_grad_scaler optim = torch optim Adam model parameters _ range inp = torch randn device= cuda dtype=full_precision_param_dtype Forward pass LinearMixedPrecision check casting inputs params buffers act _ = model inp model mp_config full_precision_param_dtype Buffers should casted buf model buffers mp_config buffer_dtype None assertEqual buf dtype mp_config buffer_dtype assertEqual buf dtype _BUFFER_ORIG_DTYPE p _mp_shard should freed mp_config param_dtype None _validate_mp_shard_freed model We never should have allocated _mp_shard _validate_no_mp_shard model loss = act sum loss = scaler scale loss mp_config param_dtype None assertEqual loss dtype mp_config param_dtype assertEqual loss dtype full_precision_param_dtype Will run patched reduce scatter validates mixed_precision types backward loss backward Buffers stay casted even after backwards buf model buffers mp_config buffer_dtype None assertEqual buf dtype mp_config buffer_dtype assertEqual buf dtype _BUFFER_ORIG_DTYPE p _mp_shard should freed mp_config param_dtype None _validate_mp_shard_freed model _validate_no_mp_shard model Ensure params grads full precision after fwd backward we maintain full precision shards param model parameters assertEqual param dtype full_precision_param_dtype param grad None assertEqual param grad dtype full_precision_param_dtype Unscale gradients step scaler step optim Update scale factor scaler update Summon full params should full precision model summon_full_params model It expected summon_full_params allocate mixed precision shard mp_config param_dtype None _validate_mp_shard_freed model _validate_no_mp_shard model params = list model parameters p params assertEqual p dtype full_precision_param_dtype Note buffers cast only once only restored original buffer dtype state_dict so summon_full_params expected restore buffer types their original named_buffers = dict model named_buffers v named_buffers values mp_config buffer_dtype None assertEqual v dtype mp_config buffer_dtype assertEqual v dtype _BUFFER_ORIG_DTYPE state_dict should full precision state_dict = k v clone k v model state_dict items name tensor state_dict items Parameters buffers checkpointed their original dtypes which may different name named_buffers keys assertEqual tensor dtype _BUFFER_ORIG_DTYPE assertEqual tensor dtype full_precision_param_dtype f name tensor dtype vs full_precision_param_dtype After state_dict buffer s dtype should have been restored mixed precision one buf model buffers mp_config buffer_dtype None assertEqual buf dtype mp_config buffer_dtype assertEqual buf dtype _BUFFER_ORIG_DTYPE TestFSDPMixedPrecisionSharded TestFSDPMixedPrecision property world_size _get_subtest_config - dict str list Any Returns subtest configuration subtests prefetching settings together forward_prefetch False True backward_prefetch None BackwardPrefetch BACKWARD_PRE BackwardPrefetch BACKWARD_POST skip_if_lt_x_gpu test_mixed_precision_no_reshard_after_forward Note we don t exercise all possible different configs so increase test TTS too much mp = default_mp nccl_supports_bf mp_diff_buffer_and_reduce _run_test_mixed_precision_e e mp_config=mp cpu_offload=CPUOffload offload_params=True backward_prefetch=None forward_prefetch=False full_precision_param_dtype=torch float sharding_strategy=ShardingStrategy SHARD_GRAD_OP enable_sharded_grad_scaler=False skip_if_lt_x_gpu parametrize params configs subtest_name test_mixed_precision_e e_full_shard mp_config cpu_offload full_precision_param_dtype enable_sharded_grad_scaler run_subtests _get_subtest_config _run_test_mixed_precision_e e mp_config=mp_config cpu_offload=cpu_offload full_precision_param_dtype=full_precision_param_dtype sharding_strategy=ShardingStrategy FULL_SHARD enable_sharded_grad_scaler=enable_sharded_grad_scaler _test_mixed_precision_embedding_table mp_config Basic test ensure int inputs casted which would break modules such embedding tables param_dtype = mp_config param_dtype torch float orig_reduce_scatter = dist reduce_scatter_tensor test_reduce_scatter = partial _reduce_scatter_validate_mp orig_reduce_scatter mp_config True patch_reduce_scatter test_reduce_scatter param_dtype TODO ` test_mp_embedding_reduce ` fails we do wrap entire ` TransformerWithSharedParams ` single top-level FSDP model = TransformerWithSharedParams init process_group FSDPInitMode NO_FSDP DEVICEInitMode DEVICE_BEFORE mixed_precision mp_config fsdp_model = FSDP model mixed_precision=mp_config optim = torch optim SGD fsdp_model parameters lr= _ range inp = fsdp_model module get_input torch device cuda This would fail we casted integer module inputs such embedding tables output = fsdp_model inp loss = fsdp_model module get_loss inp output cuda assertEqual loss dtype param_dtype fsdp_model module run_backward loss optim step skip_if_lt_x_gpu test_mp_embedding_reduce _test_mixed_precision_embedding_table mp_config=MixedPrecision reduce_dtype=torch float skip_if_lt_x_gpu test_mp_embedding_only_params_and_bufs _test_mixed_precision_embedding_table mp_config=MixedPrecision param_dtype=torch float buffer_dtype=torch float skip_if_lt_x_gpu test_mp_embedding_default default_mp_config = MixedPrecision param_dtype=torch float buffer_dtype=torch float reduce_dtype=torch float _test_mixed_precision_embedding_table mp_config=default_mp_config skip_if_lt_x_gpu test_mp_embedding_params_and_reduce_diff params_and_reduce_different = MixedPrecision param_dtype=torch float reduce_dtype=torch float buffer_dtype=torch float _test_mixed_precision_embedding_table mp_config=params_and_reduce_different skip_if_lt_x_gpu skipIfNoTorchVision test_mixed_precision_resnet End end test ensure mixed precision + auto_wrap works ResNet model resnet_model = torchvision models resnet cuda resnet_model = nn SyncBatchNorm convert_sync_batchnorm resnet_model process_group=dist distributed_c d _get_default_group n_bn = sum isinstance x _BatchNorm x resnet_model modules inp = torch ones device= cuda mp_config = MixedPrecision param_dtype=torch float reduce_dtype=torch float buffer_dtype=torch float fsdp = FSDP resnet_model auto_wrap_policy=size_based_auto_wrap_policy mixed_precision=mp_config Batchnorm units should wrapped individually Validate ensuring there equal no FSDP units BN BN units original resnet model fsdp_bn = module fsdp fsdp_modules fsdp wrapped_module = module module isinstance wrapped_module _BatchNorm fsdp_bn += assertEqual fsdp_bn n_bn Would throw type mismatch issue without mixed precision autowrapping loss = fsdp inp sum loss backward skip_if_lt_x_gpu test_grads_reduced_precision run_subtests offload_params False True use_orig_params False True _test_grads_reduced_precision skip_if_lt_x_gpu parametrize convert_sync_bn True False test_mp_batchnorm convert_sync_bn BatchNormNet nn Module __init__ affine=True super __init__ fc = nn Linear bias=False bn = nn BatchNorm d affine=affine fc = nn Linear bias=False ln = nn LayerNorm fc = nn Linear bias=False forward x x = torch reshape fc x - x = bn x x = torch reshape x - x = fc x x = ln x x = fc x F softmax x dim= never_wrap_policy args kwargs False net = BatchNormNet cuda convert_sync_bn net = nn SyncBatchNorm convert_sync_batchnorm net FSDP detects mixed precision + batchnorm will cause issues thus wrap batchnorm distinct FSDP unit does use mixed precision mp_config = MixedPrecision param_dtype=torch float reduce_dtype=torch float buffer_dtype=torch float _module_classes_to_ignore= _BatchNorm nn LayerNorm assertWarnsRegex expected_warning=UserWarning expected_regex= These modules will wrapped separate FSDP model = FSDP net mixed_precision=mp_config auto_wrap_policy=never_wrap_policy no_mp = MixedPrecision mod model ln model bn assertTrue isinstance mod FSDP assertEqual no_mp mod mixed_precision policy should have wrapped any other submodules mod model fc model fc model fc assertFalse isinstance mod FSDP Overall mixed precision still enabled assertEqual mp_config model mixed_precision inp = torch randn device= cuda Without FSDP BN mixed precision fix would result RuntimeError Expected counts have type Half got Float syncBN model inp sum backward skip_if_lt_x_gpu test_eval_root_cast_inputs In case where root module does manage FSDP parameters ensure we don t cast forward inputs which could potentially cause dtype mismatch Check FSDP_USE_FULL_PREC_IN_EVAL controls low_prec_dtype = torch float MyModel torch nn Module __init__ - None super __init__ = nn Linear forward x expect_use_full_prec_in_eval expect_use_full_prec_in_eval assert x dtype == torch float f Expected fp got x dtype assert x dtype == low_prec_dtype f Expected low_prec_dtype got x dtype x mp_config = MixedPrecision param_dtype=low_prec_dtype reduce_dtype=low_prec_dtype buffer_dtype=low_prec_dtype use_full_prec_in_eval True False os environ FSDP_USE_FULL_PREC_IN_EVAL = use_full_prec_in_eval m = MyModel cuda m = FSDP m mixed_precision=mp_config model = FSDP m mixed_precision=mp_config model eval inp = torch randn model inp use_full_prec_in_eval sum backward skip_if_lt_x_gpu test_full_precision_in_eval Tests eval runs full precision FSDP_USE_FULL_PREC_IN_EVAL set cast_forward_inputs use_full_prec_in_eval itertools product True False True False mp_config = MixedPrecision param_dtype=torch float reduce_dtype=torch float buffer_dtype=torch float cast_forward_inputs=cast_forward_inputs os environ FSDP_USE_FULL_PREC_IN_EVAL = use_full_prec_in_eval model = TransformerWithSharedParams init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE mixed_precision mp_config inp = model get_input torch device cuda output = model inp loss = model get_loss inp output cuda Loss should fp assertEqual torch float loss dtype model run_backward loss Grads should fp we upcast them p model parameters p grad None assertEqual torch float p grad dtype Now eval mode loss should fp use_full_prec_in_eval set model eval inp = model get_input torch device cuda output = model inp loss = model get_loss inp output cuda expected_dtype = torch float use_full_prec_in_eval torch float assertEqual expected_dtype loss dtype skip_if_lt_x_gpu test_full_precision_in_eval_buffers Tests when model eval FSDP_USE_FULL_PREC_IN_EVAL set buffers full precision cast_forward_inputs use_full_prec_in_eval itertools product True False True False os environ FSDP_USE_FULL_PREC_IN_EVAL = use_full_prec_in_eval mp_config = MixedPrecision param_dtype=torch float reduce_dtype=torch float buffer_dtype=torch float cast_forward_inputs=cast_forward_inputs model_getter = _get_simple_nested_model fsdp_model = model_getter param_dtype=torch float run_checks=False mixed_precision=mp_config inp = torch randn device= cuda fsdp_model inp fsdp_model mp_config torch float buf fsdp_model buffers assertEqual torch float buf dtype model eval + forward pass should make buffers full prec again Add pre-forward hooks verify_eval_buffer_dtype module input expected_dtype = _BUFFER_ORIG_DTYPE use_full_prec_in_eval torch float buf module buffers assertEqual expected_dtype buf dtype _get_underlying_module m m module isinstance m FSDP m hook_handles = hook_handles append _get_underlying_module fsdp_model register_forward_pre_hook verify_eval_buffer_dtype hook_handles append _get_underlying_module fsdp_model register_forward_pre_hook verify_eval_buffer_dtype fsdp_model eval fsdp_model inp fsdp_model mp_config torch float hook_handle hook_handles hook_handle remove expected_dtype = _BUFFER_ORIG_DTYPE use_full_prec_in_eval torch float buf fsdp_model buffers assertEqual expected_dtype buf dtype model train + forward again should make buffers fp fsdp_model train fsdp_model inp fsdp_model mp_config torch float buf fsdp_model buffers assertEqual torch float buf dtype skip_if_lt_x_gpu test_full_precision_in_eval_comm cast_forward_inputs use_full_prec_in_eval itertools product True False True False os environ FSDP_USE_FULL_PREC_IN_EVAL = use_full_prec_in_eval mp_config = MixedPrecision param_dtype=torch float reduce_dtype=torch float buffer_dtype=torch float cast_forward_inputs=cast_forward_inputs cast reduction batchnorm also just test make validation easier _module_classes_to_ignore= model = TransformerWithSharedParams init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE mixed_precision mp_config Patch reduce_scatter add validation mixed precision types orig_reduce_scatter = dist reduce_scatter_tensor test_reduce_scatter = partial _reduce_scatter_validate_mp orig_reduce_scatter mp_config use_full_prec_in_eval model eval patch_reduce_scatter test_reduce_scatter torch float inp = model get_input torch device cuda output = model inp loss = model get_loss inp output cuda model run_backward loss skip_if_lt_x_gpu test_input_grads_with_param_mixed_precision Tests input tensors require gradients do get their gradients even after being cast low precision when parameter mixed precision enabled run_subtests sharding_strategy ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP ShardingStrategy NO_SHARD use_orig_params False True _test_input_grads_with_param_mixed_precision _test_input_grads_with_param_mixed_precision sharding_strategy ShardingStrategy use_orig_params bool model = nn Linear bias=False mixed_precision = MixedPrecision param_dtype=torch float reduce_dtype=torch float buffer_dtype=torch float fsdp_model = FSDP model sharding_strategy=sharding_strategy mixed_precision=mixed_precision device_id=torch cuda current_device use_orig_params=use_orig_params Use input dtype equal mixed precision ` param_dtype ` so gets cast x_float = torch randn device= cuda dtype=torch float requires_grad=True fsdp_model x_float sum backward assertTrue x_float grad None Check ` x_float ` preserves its dtype meaning gradient propagated via ` ToCopyBackward ` assertEqual x_float grad dtype torch float skip_if_lt_x_gpu test_buffer_dtype_no_root_handle NonLearnableConv nn Module __init__ kernel in_channels int super __init__ padding = len kernel - kernel = torch tensor kernel dtype=torch float kernel = kernel kernel sum kernel = kernel outer kernel None None repeat in_channels register_buffer kernel kernel forward x torch Tensor - torch Tensor nn functional conv d x kernel groups=self kernel shape stride= padding=self padding model = nn Sequential nn Sequential nn Conv d padding= nn Sequential NonLearnableConv nn Sequential nn Conv d padding= nn Sequential NonLearnableConv cuda dtype = torch float model = FSDP module=model device_id=self rank use_orig_params=True limit_all_gathers=True auto_wrap_policy=ModuleWrapPolicy nn Sequential mixed_precision=MixedPrecision param_dtype=dtype buffer_dtype=dtype reduce_dtype=dtype Check we can run forward backward without dtype errors x = torch randn device= cuda out = model x out mean backward TestFSDPMixedPrecisionUnsharded TestFSDPMixedPrecision Smaller test suite unshared param i e world_size == case property world_size skip_if_lt_x_gpu test_grads_reduced_precision run_subtests offload_params False True use_orig_params False True _test_grads_reduced_precision skip_if_lt_x_gpu test_mixed_precision_no_reshard_after_forward Note we don t exercise all possible different configs so increase test TTS too much mp = default_mp nccl_supports_bf mp_diff_buffer_and_reduce _run_test_mixed_precision_e e mp_config=mp cpu_offload=CPUOffload offload_params=True backward_prefetch=None forward_prefetch=False full_precision_param_dtype=torch float sharding_strategy=ShardingStrategy SHARD_GRAD_OP enable_sharded_grad_scaler=False skip_if_lt_x_gpu test_mixed_precision_e e_full_shard mp = default_mp nccl_supports_bf mp_diff_buffer_and_reduce _run_test_mixed_precision_e e mp_config=mp cpu_offload=CPUOffload offload_params=True backward_prefetch=None forward_prefetch=False full_precision_param_dtype=torch float sharding_strategy=ShardingStrategy FULL_SHARD enable_sharded_grad_scaler=False instantiate_parametrized_tests TestFSDPMixedPrecisionSharded IgnoredModule nn Module __init__ - None super __init__ l = nn Linear forward x l x ModelWithIgnoredModule nn Module __init__ - None super __init__ l = nn Linear ignored = IgnoredModule l = nn Linear forward x l ignored l x TestFSDPMixedPrecisionIgnoredModules FSDPTest property world_size skip_if_lt_x_gpu test_mixed_precision_with_ignored_module model = ModelWithIgnoredModule cuda float = MixedPrecision param_dtype=torch float model = FSDP model ignored_modules= model ignored mixed_precision=float x = torch ones device=torch cuda current_device assertRaisesRegex RuntimeError must have same dtype model x sum backward TestFSDPDifferentSubmodulePrecision FSDPTest property world_size skip_if_lt_x_gpu test_float _on_one_submodule forward_inputs dict str nn Module = float = MixedPrecision param_dtype=torch float cast_forward_inputs=True model = SaveForwardInputsModel forward_inputs cast_forward_inputs=False cuda c c = model c model c x = torch zeros device= cuda float one submodule float everything model c = FSDP model c mixed_precision=float fsdp = FSDP model fsdp x sum backward assertEqual forward_inputs model dtype torch float assertEqual forward_inputs c dtype torch float assertEqual forward_inputs c dtype torch float skip_if_lt_x_gpu test_float _on_one_submodule_skip_inputs forward_inputs dict nn Module torch Tensor = float = MixedPrecision param_dtype=torch float cast_forward_inputs=False model = SaveForwardInputsModel forward_inputs=forward_inputs cast_forward_inputs=True cuda c c = model c model c x = torch zeros device= cuda float one submodule float everything model c = FSDP model c mixed_precision=float fsdp = FSDP model fsdp x sum backward assertEqual forward_inputs model dtype torch float assertEqual forward_inputs c dtype torch float assertEqual forward_inputs c dtype torch float skip_if_lt_x_gpu test_float _on_one_submodule_skip_inputs_error forward_inputs dict nn Module torch Tensor = float = MixedPrecision param_dtype=torch float cast_forward_inputs=False model = SaveForwardInputsModel forward_inputs=forward_inputs cast_forward_inputs=False cuda x = torch zeros device= cuda float one submodule float everything model c = FSDP model c mixed_precision=float fsdp = FSDP model assertRaisesRegex RuntimeError mat mat must have same dtype fsdp x sum backward skip_if_lt_x_gpu test_submodules_with_different_precisions_error forward_inputs dict nn Module torch Tensor = float = MixedPrecision param_dtype=torch float cast_forward_inputs=True float = MixedPrecision param_dtype=torch float cast_forward_inputs=True model = SaveForwardInputsModel forward_inputs=forward_inputs cast_forward_inputs=False cuda x = torch zeros device= cuda For submodules different precisions right now current design does support case when root FSDP instance wraps submodule first one executed Because submodule its inputs previous submodule s outputs have no way casted instead root module s inputs casted upfront before entering root module s forward model c = FSDP model c mixed_precision=float fsdp = FSDP model mixed_precision=float assertRaisesRegex RuntimeError mat mat must have same dtype fsdp x sum backward skip_if_lt_x_gpu test_submodules_with_different_precisions forward_inputs dict nn Module torch Tensor = float = MixedPrecision param_dtype=torch float cast_forward_inputs=True float = MixedPrecision param_dtype=torch float cast_forward_inputs=True model = SaveForwardInputsModel forward_inputs=forward_inputs cast_forward_inputs=False cuda c c = model c model c x = torch zeros device= cuda model c = FSDP model c mixed_precision=float fsdp = FSDP model mixed_precision=float fsdp x sum backward assertEqual forward_inputs model dtype torch float assertEqual forward_inputs c dtype torch float assertEqual forward_inputs c dtype torch float skip_if_lt_x_gpu test_submodules_with_external_inputs ToyModule nn Module __init__ forward_inputs dict str torch Tensor - None super __init__ l = nn Linear forward_inputs = forward_inputs forward x torch Tensor y torch Tensor - torch Tensor forward_inputs l _input_x = x forward_inputs l _input_y = y l x ToyModel nn Module __init__ forward_inputs dict str torch Tensor - None super __init__ l = nn Linear l = ToyModule forward_inputs forward_inputs = forward_inputs forward x torch Tensor - torch Tensor forward_inputs model_input_x = x y = torch ones device= cuda dtype=torch float l l x y forward_inputs dict str torch Tensor = float = MixedPrecision param_dtype=torch float model = ToyModel forward_inputs cuda x = torch zeros device= cuda dtype=torch float model l = FSDP model l mixed_precision=float fsdp = FSDP model mixed_precision=float fsdp x sum backward Inputs casted root module default inputs submodules explicitly casted so external inputs ` ` y ` ` module ` ` l ` ` casted assertEqual forward_inputs model_input_x dtype torch float assertEqual forward_inputs l _input_x dtype torch float assertEqual forward_inputs l _input_y dtype torch float TestFSDPTrainEval FSDPTest property world_size skip_if_lt_x_gpu test_train_ema_eval_flow Tests train - EMA update - eval flow mixed precision enabled run_subtests sharding_strategy We mainly want test ` SHARD_GRAD_OP ` since surfaced original bug using right EMA parameters eval we also test others completeness ShardingStrategy SHARD_GRAD_OP ShardingStrategy FULL_SHARD ShardingStrategy NO_SHARD _test_train_ema_eval_flow _test_train_ema_eval_flow sharding_strategy ShardingStrategy TransformerWithEMA nn Module __init__ device torch device super __init__ module = nn Transformer device=device ema_module = AveragedModel nn Transformer device=device multi_avg_fn=torch optim swa_utils get_ema_multi_avg_fn use_buffers=True forward args kwargs Use main copy training EMA copy eval training module args kwargs ema_module args kwargs device = torch device cuda model = TransformerWithEMA device=device policy = ModuleWrapPolicy nn Transformer nn TransformerEncoderLayer nn TransformerDecoderLayer mixed_precision = MixedPrecision param_dtype=torch float fsdp_model = FSDP model auto_wrap_policy=policy mixed_precision=mixed_precision sharding_strategy=sharding_strategy optim = torch optim Adam fsdp_model module parameters lr= e- rank == print fsdp_model torch manual_seed + rank eval_src = torch randn device=device eval_tgt = torch randn device=device eval_out_sums list torch Tensor = An iteration consists training forward backward optimizer updating EMA copy main copy eval forward _ range fsdp_model train train_src = torch randn device=device train_tgt = torch randn device=device train_out = fsdp_model train_src train_tgt train_out sum backward optim step optim zero_grad FSDP summon_full_params fsdp_model fsdp_model ema_module update_parameters fsdp_model module fsdp_model eval torch no_grad eval_out = fsdp_model eval_src eval_tgt eval_out_sums append eval_out sum Check eval outputs differ iteration iteration proxy eval using correct EMA parameters i range len eval_out_sums - assertNotEqual eval_out_sums i eval_out_sums i + assertNotEqual eval_out_sums eval_out_sums - __name__ == __main__ run_tests