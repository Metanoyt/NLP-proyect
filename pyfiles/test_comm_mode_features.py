Copyright c Meta Platforms Inc affiliates Owner s oncall distributed typing Any torch torch distributed tensor DeviceMesh distribute_tensor DTensor torch distributed tensor debug CommDebugMode torch distributed tensor parallel ColwiseParallel parallelize_module RowwiseParallel torch testing _internal common_utils run_tests skipIfHpu TEST_XPU xfailIf torch testing _internal distributed _tensor common_dtensor DTensorTestBase MLPModule MLPStacked ModelArgs NUM_DEVICES skip_unless_torch_gpu Transformer with_comms c d_functional = torch ops c d_functional TestCommModeFeatures DTensorTestBase checks parameter sharding info same ground truth check_same_set_of_keys dict dict Used ensure comm_mode parameter sharding dictionaries contain same information produced ground truth dict _keys = dict _keys = key dict nested_key dict key dict _keys append key nested_key key dict nested_key dict key dict _keys append key nested_key assertEqual len dict _keys len dict _keys i range len dict _keys assertEqual dict _keys i dict _keys i generates ground truth parameter sharding info ground_truth model Used generate ground-truth parameter sharding info given distributed model verify comm_mode correctness module_parameters_dict dict str Any = module_sharding_dict dict str Any = name parameters model named_parameters splits name into module name create FQN parameter name module_name = model __class__ __name__ + + name rsplit parameter_name = name rsplit module_name module_parameters_dict module_parameters_dict module_name = module_parameters_dict module_name parameter_name = parameters data isinstance parameters data DTensor key_name = module_name + + parameter_name module_sharding_dict key_name = parameters data placements module_parameters_dict module_sharding_dict with_comms test_MLP_distributed_sharding_display tests parameters sharding module level device_mesh = DeviceMesh device_type torch arange NUM_DEVICES inp_size = torch manual_seed inp = torch rand inp_size device=self device_type model = MLPModule device_type parallelize_plan = net ColwiseParallel net RowwiseParallel model = parallelize_module model device_mesh parallelize_plan comm_mode = CommDebugMode comm_mode output_tp = model inp output_tp sum backward module_parameters_dict module_sharding_dict = ground_truth model checks parameter sharding info same ground truth check_same_set_of_keys module_parameters_dict comm_mode get_parameter_info check_same_set_of_keys module_sharding_dict comm_mode get_sharding_info skipIfHpu with_comms test_MLPStacked_distributed_sharding_display tests model nested modules makes sure comm_mode correctly resets parameter sharding information device_mesh = DeviceMesh device_type torch arange NUM_DEVICES inp_size = torch manual_seed inp = torch rand inp_size device=self device_type model = MLPModule device_type parallelize_plan = net ColwiseParallel net RowwiseParallel model = parallelize_module model device_mesh parallelize_plan comm_mode = CommDebugMode comm_mode output_tp = model inp output_tp sum backward model = MLPStacked device_type parallelize_plan = layers net ColwiseParallel layers net RowwiseParallel layers net ColwiseParallel layers net RowwiseParallel model = parallelize_module model device_mesh parallelize_plan comm_mode ensures comm_mode resetting properly assertEqual comm_mode get_parameter_info assertEqual comm_mode get_sharding_info output_tp = model inp module_parameters_dict module_sharding_dict = ground_truth model check_same_set_of_keys module_parameters_dict comm_mode get_parameter_info check_same_set_of_keys module_sharding_dict comm_mode get_sharding_info assertEqual len comm_mode get_sharding_info with_comms test_MLP_module_tracing tests module-level tracing MLP module device_mesh = DeviceMesh device_type torch arange NUM_DEVICES inp_size = torch manual_seed inp = torch rand inp_size device=self device_type model = MLPModule device_type parallelize_plan = net ColwiseParallel net RowwiseParallel model = parallelize_module model device_mesh parallelize_plan comm_mode = CommDebugMode comm_mode output_tp = model inp output_tp sum backward checks see all sub-modules make into module_depth_dictionary assertEqual len comm_mode advanced_module_tracker module_helper_dict checks see all collectives correctly traced module-level assertEqual comm_mode comm_module_counts Global forward c d_functional all_reduce assertEqual comm_mode comm_module_counts MLPModule forward c d_functional all_reduce assertEqual comm_mode comm_module_counts MLPModule net forward c d_functional all_reduce skipIfHpu skip_unless_torch_gpu xfailIf TEST_XPU https github com intel torch-xpu-ops issues with_comms test_transformer_module_tracing is_seq_parallel=False tests module-level tracing more complicated transformer module ensures comm_module depth tracing dictionaries correctly reset device_mesh = DeviceMesh device_type torch arange NUM_DEVICES inp_size = torch manual_seed inp = torch rand inp_size device=self device_type model = MLPModule device_type parallelize_plan = net ColwiseParallel net RowwiseParallel model = parallelize_module model device_mesh parallelize_plan comm_mode = CommDebugMode comm_mode assertEqual len comm_mode advanced_module_tracker module_helper_dict assertEqual comm_mode comm_module_counts Global forward backward model inp model_args = ModelArgs dropout_p= model = Transformer model_args device=self device_type model = Transformer parallelize model device_mesh is_seq_parallel inp_size = inp = torch randint model_args vocab_size inp_size device=self device_type inp = distribute_tensor inp device_mesh=device_mesh comm_mode = CommDebugMode comm_mode model inp checks see all collectives correctly traced module-level assertEqual comm_mode comm_module_counts Global forward c d_functional all_reduce assertEqual comm_mode comm_module_counts Global forward c d_functional all_gather_into_tensor assertEqual comm_mode comm_module_counts Transformer forward c d_functional all_reduce assertEqual comm_mode comm_module_counts Transformer forward c d_functional all_gather_into_tensor assertEqual comm_mode comm_module_counts Transformer tok_embeddings forward c d_functional all_reduce assertEqual comm_mode comm_module_counts Transformer pos_embeddings forward c d_functional all_reduce assertEqual comm_mode comm_module_counts Transformer layers forward c d_functional all_reduce assertEqual comm_mode comm_module_counts Transformer layers attention forward c d_functional all_reduce assertEqual comm_mode comm_module_counts Transformer layers attention wo forward c d_functional all_reduce assertEqual comm_mode comm_module_counts Transformer layers feed_forward forward c d_functional all_reduce assertEqual comm_mode comm_module_counts Transformer layers feed_forward w forward c d_functional all_reduce assertEqual comm_mode comm_module_counts Transformer layers forward c d_functional all_reduce assertEqual comm_mode comm_module_counts Transformer layers attention forward c d_functional all_reduce assertEqual comm_mode comm_module_counts Transformer layers attention wo forward c d_functional all_reduce assertEqual comm_mode comm_module_counts Transformer layers feed_forward forward c d_functional all_reduce assertEqual comm_mode comm_module_counts Transformer layers feed_forward w forward c d_functional all_reduce assertEqual comm_mode comm_module_counts Transformer output forward c d_functional all_gather_into_tensor __name__ == __main__ run_tests