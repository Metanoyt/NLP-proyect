mypy allow-untyped-defs Copyright c Meta Platforms Inc affiliates collections abc Sequence Sized typing cast Optional torch torch _prims_common IntLike torch distributed tensor _dtensor_spec DTensorSpec torch distributed tensor _op_schema OpSchema OpSpec OpStrategy OutputSharding PlacementList RuntimeSchemaInfo StrategyType TupleStrategy torch distributed tensor _ops _common_rules pointwise_rule torch distributed tensor _ops _embedding_ops MaskPartial torch distributed tensor _ops utils expand_to_full_mesh_op_strategy generate_redistribute_costs is_tensor_dim_sharded is_tensor_evenly_shardable is_tensor_partial normalize_dim register_op_strategy register_prop_rule shift_shard_dims_after_insert shift_shard_dims_after_remove torch distributed tensor placement_types Partial Placement Replicate Shard aten = torch ops aten propagate_single_input_strategy op_schema OpSchema - StrategyType For ops single tensor input we perform mapping such each strategy input supports we create corresponding strategy Note may complete waste work because should equivalent ` first_input_strategy ` unless creating deep copy important some reason len s s op_schema args_schema isinstance s OpStrategy = raise AssertionError propagate_single_input_strategy only works single-tensor-input ops first_input_strategy = op_schema args_schema isinstance first_input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type first_input_strategy OpStrategy OpSpec output_specs=DTensorSpec mesh=first_input_strategy mesh placements=strategy output_spec placements tensor_meta=strategy output_spec tensor_meta input_specs= DTensorSpec mesh=first_input_strategy mesh placements=strategy output_spec placements tensor_meta=strategy output_spec tensor_meta redistribute_cost= generate_redistribute_costs first_input_strategy strategy output_spec strategy first_input_strategy strategies register_op_strategy aten clone default aten contiguous default aten detach default aten fill_ Scalar aten view dtype aten zero_ default propagate_single_input_strategy register_op_strategy aten _to_copy default schema_info=RuntimeSchemaInfo static_kwargkey= dtype propagate_single_input_strategy register_op_strategy aten equal default aten is_same_size default equal_strategy op_schema OpSchema - StrategyType equal_strategy deals ops comparing two tensor we need make sure sharding layout same two operands we choose follow arg max num shards still keep is_same_size here completeness they share same strategy theory mesh = op_schema get_mesh_from_args self_strategy other_strategy = op_schema args_schema isinstance self_strategy OpStrategy raise AssertionError f Expected OpStrategy got type self_strategy isinstance other_strategy OpStrategy raise AssertionError f Expected OpStrategy got type other_strategy select_strategy = self_strategy self_strategy max_num_shards = other_strategy max_num_shards other_strategy equal_strategy = OpStrategy arg_strategy select_strategy strategies arg_spec = arg_strategy output_spec is_tensor_partial arg_spec arg_spec have partial reshard replicate otherwise local shard tensor comparison would invalid output_spec = DTensorSpec mesh=mesh placements=tuple Replicate isinstance p Partial p p arg_spec placements equal_strategy strategies append OpSpec output_specs=output_spec equal_strategy strategies append OpSpec arg_spec equal_strategy register_op_strategy aten empty_like default aten ones_like default aten rand_like default aten randn_like default aten zeros_like default schema_info=RuntimeSchemaInfo dtype register_op_strategy aten full_like default schema_info=RuntimeSchemaInfo dtype register_op_strategy aten randint_like default aten randint_like low_dtype aten randint_like low_dtype_out schema_info=RuntimeSchemaInfo dtype create_like_strategy op_schema OpSchema - StrategyType create_like_strategy deals ops creating tensors same shape input specific content does depend input we can propagate sharding we have make sure we move partial replicated select_strategy = op_schema args_schema create_like_strategy = OpStrategy isinstance select_strategy OpStrategy raise AssertionError f Expected OpStrategy got type select_strategy arg_strategy select_strategy strategies arg_spec = arg_strategy output_spec output_spec = DTensorSpec mesh=select_strategy mesh placements=tuple Replicate isinstance p Partial p p arg_spec placements create_like_strategy strategies append OpSpec output_specs=output_spec input_specs= arg_spec create_like_strategy register_op_strategy aten new_empty default aten new_full default aten new_ones default aten new_zeros default aten new_empty_strided default schema_info=RuntimeSchemaInfo dtype new_factory_strategy op_schema OpSchema - StrategyType Currently there two strategies let output replicated let output follow input input output have same shape input_strategy = op_schema args_schema isinstance input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type input_strategy mesh = input_strategy mesh input_shape = input_strategy shape output_shape = op_schema args_schema isinstance output_shape list raise AssertionError f Expected list got type output_shape new_factory_strategy = OpStrategy arg_strategy input_strategy strategies input_spec = arg_strategy output_spec replica_spec = DTensorSpec mesh tuple Replicate mesh ndim new_factory_strategy strategies append OpSpec output_specs=replica_spec input_specs= input_spec redistribute_cost= len input_strategy strategies tuple input_shape == tuple output_shape input_spec is_sharded NOTE new_empty_strided currently non-replicate sharding supported only when shape evenly shardable op_schema op == aten new_empty_strided default is_tensor_evenly_shardable input_shape input_spec continue new_factory_strategy strategies append OpSpec output_specs=input_spec input_specs= input_spec encouraging new tensor placement same input redistribute_cost= - len input_strategy strategies new_factory_strategy register_op_strategy aten bucketize Tensor gen_bucketize_strategy op_schema OpSchema - StrategyType Just propagate input sharding expect replicated boundaries input mesh = op_schema get_mesh_from_args input_strategy boundaries_strategy = op_schema args_schema bucketize_strategy = OpStrategy isinstance input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type input_strategy isinstance boundaries_strategy OpStrategy raise AssertionError f Expected OpStrategy got type boundaries_strategy arg_strategy input_strategy strategies arg_spec = DTensorSpec mesh arg_strategy output_spec placements arg_strategy output_spec tensor_meta replica_spec = DTensorSpec mesh tuple Replicate mesh ndim boundaries_strategy strategies output_spec tensor_meta bucketize_strategy strategies append OpSpec output_specs=arg_spec input_specs= arg_spec replica_spec redistribute_cost= generate_redistribute_costs input_strategy arg_spec generate_redistribute_costs boundaries_strategy replica_spec bucketize_strategy register_op_strategy aten select int schema_info=RuntimeSchemaInfo select_int_strategy op_schema OpSchema - StrategyType In select op first determine input specs then determine output specs - Input specs - If input sharded selected dim unshard change replicate - Otherwise keep original input specs - Output specs - It checks input specs following cases - Case shard_dim == selected_dim possible input already unsharded - Case shard_dim selected_dim keep input specs - Case shard_dim selected_dim shard_dim -= input_strategy = op_schema args_schema isinstance input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type input_strategy len op_schema args_schema = raise AssertionError f Expected args got len op_schema args_schema selected_dim index = cast int op_schema args_schema cast int op_schema args_schema input_shape = input_strategy shape input_ndim = input_strategy ndim selected_dim = normalize_dim selected_dim input_ndim index = normalize_dim index input_shape selected_dim select_strategy = OpStrategy arg_strategy input_strategy strategies arg_spec = arg_strategy output_spec determine input spec input_specs = arg_spec is_tensor_dim_sharded arg_spec dim=selected_dim input sharded selected dim need unshard change replicate arg_target_placements = unshard_tensor_dim arg_spec placements dim=selected_dim input_specs = DTensorSpec arg_spec mesh arg_target_placements R determine output spec output_specs = input_specs input_specs is_sharded handle cases sharded_dim = selected_dim output_placements = shift_shard_dims_after_remove input_specs placements selected_dim output_specs = DTensorSpec arg_spec mesh placements=tuple output_placements select_strategy strategies append OpSpec output_specs=output_specs input_specs= input_specs select_strategy register_op_strategy aten select_backward default schema_info=RuntimeSchemaInfo select_backward_strategy op_schema OpSchema - OpStrategy func select_backward Tensor grad_output SymInt input_sizes int dim SymInt index - Tensor args_schema = op_schema args_schema input_strategy dim = args_schema args_schema isinstance input_strategy OpStrategy raise AssertionError f Expected OpStrategy got input_strategy isinstance dim int raise AssertionError f Expected int got type dim output_strategies list OpSpec = placement_strategy input_strategy strategies input_spec = placement_strategy output_spec NOTE shard_dim guaranteed exist because grad_input has one more dim than grad_output output_placements = shift_shard_dims_after_insert input_spec placements dim output_specs = DTensorSpec input_spec mesh tuple output_placements output_strategies append OpSpec output_specs=output_specs input_specs= input_spec OpStrategy output_strategies register_op_strategy aten slice Tensor schema_info=RuntimeSchemaInfo gen_slice_strategy op_schema OpSchema - StrategyType Forward all shardings except slice dimension defaults = None None None input_strategy dim start end step = op_schema args_schema + defaults len op_schema args_schema isinstance input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type input_strategy mesh = input_strategy mesh input_shape = input_strategy shape input_ndim = input_strategy ndim isinstance dim int raise AssertionError f Expected int got type dim start None start = end None end input_shape dim end = input_shape dim isinstance start IntLike raise AssertionError f Expected IntLike got type start isinstance end IntLike raise AssertionError f Expected IntLike got type end isinstance step IntLike raise AssertionError f Expected IntLike got type step normalize args slice_dim = normalize_dim dim input_ndim type ignore arg-type start = normalize_dim start input_shape dim type ignore arg-type end = normalize_dim end input_shape dim type ignore arg-type redundant_slice = start == end == input_shape dim step == slice_strategy = OpStrategy arg_strategy input_strategy strategies arg_spec = arg_strategy output_spec is_tensor_dim_sharded arg_spec dim=slice_dim redundant_slice only add strategy slice dim sharded out_spec = DTensorSpec mesh arg_spec placements slice_strategy strategies append OpSpec output_specs=out_spec input_specs= arg_spec redistribute_cost= len input_strategy strategies slice_strategy strategies all strategies filtered out unsharding all specs slice dim input strategy use op strategy arg_strategy input_strategy strategies arg_spec = arg_strategy output_spec unshard_spec = DTensorSpec mesh unshard_tensor_dim arg_spec placements dim=slice_dim slice_strategy strategies append OpSpec output_specs=unshard_spec redistribute_cost= generate_redistribute_costs input_strategy unshard_spec slice_strategy register_op_strategy aten slice_backward default schema_info=RuntimeSchemaInfo slice_backward_rules op_schema OpSchema - OpStrategy func slice_backward Tensor grad_output SymInt input_sizes int dim SymInt start SymInt end SymInt step - Tensor args_schema = op_schema args_schema input_strategy dim = args_schema args_schema isinstance input_strategy OpStrategy raise AssertionError f Expected OpStrategy got input_strategy output_strategies list OpSpec = placement_strategy input_strategy strategies output_spec = placement_strategy output_spec new_placements list Placement = placement output_spec placements Redistribute replicate only dim sharded matches slice dim isinstance placement Shard placement dim == dim new_placements append Replicate new_placements append placement new_spec = DTensorSpec output_spec mesh tuple new_placements redistribute_cost = generate_redistribute_costs input_strategy new_spec new_strategy = OpSpec output_specs=new_spec redistribute_cost=redistribute_cost output_strategies append new_strategy OpStrategy output_strategies unshard_tensor_dim placements Sequence Placement dim int - tuple Placement Disallow given tensor dimension sharded tuple p isinstance p Shard p dim = dim Replicate p placements replicate_tensor_dim placements Sequence Placement dim int - tuple Placement Force given tensor dimension replicated Not using p is_shard avoid mypy complain about Placement having attribute dim tuple Replicate p is_partial isinstance p Shard p dim == dim p p placements register_op_strategy aten slice_scatter default schema_info=RuntimeSchemaInfo gen_slice_scatter_strategy op_schema OpSchema - StrategyType number dimensions input src need match number elements all non-dim need match between input src number elements src dim need match slice size Given above - We suggest src follow sharding input except scatter dimension where our best bet now make them replicated fall-back TODO Ideally we d like make sure output re-sharded afterwards keep input sharding mesh = op_schema get_mesh_from_args input_strategy = op_schema args_schema src_strategy = op_schema args_schema isinstance input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type input_strategy isinstance src_strategy OpStrategy raise AssertionError f Expected OpStrategy got type src_strategy input_ndim = input_strategy ndim slice_dim = cast int op_schema args_schema len op_schema args_schema slice_dim = normalize_dim slice_dim input_ndim slice_scatter_strategy = OpStrategy default follow input strategy both input src arg_strategy input_strategy strategies arg_spec = arg_strategy output_spec is_tensor_dim_sharded arg_spec dim=slice_dim is_tensor_partial arg_spec input_spec = DTensorSpec mesh arg_spec placements arg_spec tensor_meta TODO need relax constraint src src_spec = DTensorSpec mesh arg_spec placements only add strategy slice_scatter dim sharded partial slice_scatter_strategy strategies append OpSpec output_specs=arg_spec input_specs= input_spec src_spec redistribute_cost= generate_redistribute_costs input_strategy input_spec generate_redistribute_costs src_strategy src_spec slice_scatter_strategy strategies all strategies filtered out replicating all specs slice_scatter dim input strategy use op strategy arg_strategy input_strategy strategies arg_spec = arg_strategy output_spec new_placement = replicate_tensor_dim arg_spec placements dim=slice_dim input_spec = DTensorSpec mesh new_placement src_spec = DTensorSpec mesh new_placement slice_scatter_strategy strategies append OpSpec output_specs=input_spec input_specs= input_spec src_spec redistribute_cost= generate_redistribute_costs input_strategy input_spec generate_redistribute_costs src_strategy src_spec slice_scatter_strategy register_op_strategy aten _local_scalar_dense default replica_only_strategy op_schema OpSchema - StrategyType Only allow replication input output input_strategy = op_schema args_schema isinstance input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type input_strategy mesh = input_strategy mesh replicate_spec = DTensorSpec mesh tuple Replicate mesh ndim OpStrategy OpSpec replicate_spec register_op_strategy aten scatter_ value aten scatter value aten scatter_ src aten scatter src schema_info=RuntimeSchemaInfo scatter_strategy op_schema OpSchema - StrategyType mesh = op_schema get_mesh_from_args single_mesh_dim_strategies = placement list stores placements output input index src first we always have replicate all inputs output len op_schema args_strategy scatter_ src scatter src src float number instead tensor all_replicate PlacementList = Replicate all_replicate = Replicate single_mesh_dim_strategies append all_replicate TODO see we can support input sharding pattern op_strategy = expand_to_full_mesh_op_strategy mesh op_schema single_mesh_dim_strategies inplace_op=op_schema is_inplace_op op_strategy register_op_strategy aten scatter_add default schema_info=RuntimeSchemaInfo scatter_add_strategy op_schema OpSchema - StrategyType input_strategy = op_schema args_schema dim = op_schema args_schema index_strategy = op_schema args_schema isinstance input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type input_strategy isinstance index_strategy OpStrategy raise AssertionError f Expected OpStrategy got type index_strategy isinstance dim int raise AssertionError f Expected int got type dim dim = normalize_dim dim input_strategy ndim mesh = input_strategy mesh input_shape = input_strategy shape index_shape = index_strategy shape single_mesh_dim_strategies = placement list stores placements output input index src first we always have replicate all inputs output all_replicate PlacementList = Replicate single_mesh_dim_strategies append all_replicate len input_shape == len index_shape d range len input_shape d = dim input_shape d == index_shape d sharding PlacementList = Shard d Shard d Shard d Shard d single_mesh_dim_strategies append sharding expand_to_full_mesh_op_strategy mesh op_schema single_mesh_dim_strategies input_index= register_op_strategy aten gather default schema_info=RuntimeSchemaInfo gather_strategy op_schema OpSchema - StrategyType mesh = op_schema get_mesh_from_args input_strategy = cast OpStrategy op_schema args_schema dim = cast int op_schema args_schema dim = normalize_dim dim input_strategy ndim index_strategy = cast OpStrategy op_schema args_schema input_shape = input_strategy shape index_shape = index_strategy shape single_mesh_dim_strategies = placement list stores placements output input index first we always have replicate all inputs output all_replicate PlacementList = Replicate single_mesh_dim_strategies append all_replicate input sharding input sharded index accepts mask partial output follows index only works when input sharded gather dimension index has size gather dimension dim len index_shape index_shape dim == index_partial_placement = MaskPartial offset_shape=input_shape offset_dim=dim input_sharding PlacementList = index_partial_placement Shard dim index_partial_placement single_mesh_dim_strategies append input_sharding index sharding input replicated index sharded output follows index only works when sharding dimension gather dimension index_sharding PlacementList = Shard dim Replicate Shard dim single_mesh_dim_strategies append index_sharding len input_shape == len index_shape d range len input_shape d = dim sharding PlacementList = Shard d Shard d Shard d single_mesh_dim_strategies append sharding expand_to_full_mesh_op_strategy mesh op_schema single_mesh_dim_strategies input_index= _derive_follow_placements_from_tuple_strategy op torch _ops OpOverload tuple_strategy TupleStrategy - Sequence Placement derive placements follow tuple strategy mainly used aten stack aten cat where each operand have same shape correspondingly expecting same sharding merge_placement cur_placement Placement new_placement Placement - Placement semantic we already have follow placement we check each placement current arg placement see we want merge adjust placement follow priority Partial - Shard - Replicate cur_placement == new_placement cur_placement cur_placement is_partial new_placement is_shard follow new placement new_placement new_placement is_partial different partial types we can t merge have replicate all here Replicate follow partial cur_placement cur_placement is_shard new_placement is_shard cur new placement different sharding i e different shard dim currently fallback replicate all args Replicate partial replicate follow current shard placement cur_placement current replicate just follow new placement new_placement follow_placements Optional list Placement = None mesh = tuple_strategy child_mesh arg_strategy tuple_strategy children isinstance arg_strategy OpStrategy raise AssertionError f Expected OpStrategy got type arg_strategy arg_strategy mesh = mesh raise ValueError f All operands op must have same mesh f got arg_strategy mesh mesh placement_strategy arg_strategy strategies arg_placements = placement_strategy output_spec placements follow_placements None follow_placements = list arg_placements continue follow_placements None raise AssertionError follow_placements should None point mesh_idx range mesh ndim merge placements priority follow_placements mesh_idx = merge_placement follow_placements mesh_idx arg_placements mesh_idx follow_placements None raise AssertionError follow placements should None follow_placements register_op_strategy aten stack default RuntimeSchemaInfo needs_pytree=True stack_strategy op_schema OpSchema - StrategyType args_schema = op_schema args_schema input_tuple_strategy = args_schema isinstance input_tuple_strategy TupleStrategy raise AssertionError f Expected TupleStrategy got input_tuple_strategy first_input_strategy = input_tuple_strategy children isinstance first_input_strategy OpStrategy raise AssertionError f Expected OpStrategy got first_input_strategy common_input_ndim = first_input_strategy ndim dim = cast int args_schema len args_schema normalize dim within common input ndim dim = normalize_dim dim common_input_ndim mesh = first_input_strategy mesh follow_placements = _derive_follow_placements_from_tuple_strategy op_schema op input_tuple_strategy create op strategy base follow placements op_strategy = OpStrategy input_specs = tuple DTensorSpec mesh tuple follow_placements _ range len input_tuple_strategy children stack op would insert new dim so all sharded dim = inserted dim need normalized new Shard placement follow_placements = shift_shard_dims_after_insert follow_placements dim strategy input_tuple_strategy children isinstance strategy OpStrategy raise AssertionError f Expected OpStrategy got type strategy output_spec = DTensorSpec mesh tuple follow_placements redistribute_cost = input_spec input_specs cost = generate_redistribute_costs strategy input_spec redistribute_cost append cost op_strategy strategies append OpSpec output_specs=output_spec input_specs=input_specs redistribute_cost=redistribute_cost op_strategy register_op_strategy aten cat default RuntimeSchemaInfo needs_pytree=True cat_strategy op_schema OpSchema - StrategyType args_schema = op_schema args_schema input_tuple_strategy = args_schema isinstance input_tuple_strategy TupleStrategy raise AssertionError f Expected TupleStrategy got input_tuple_strategy num_input_tensor = len input_tuple_strategy children first_input_strategy = input_tuple_strategy children isinstance first_input_strategy OpStrategy raise AssertionError f Expected OpStrategy got first_input_strategy common_input_ndim = first_input_strategy ndim dim = cast int args_schema len args_schema normalize dim within common input ndim dim = normalize_dim dim common_input_ndim mesh = first_input_strategy mesh op_strategy = OpStrategy use set deduplicate strategies same placement strategies_placement_pool = set this_strategy input_tuple_strategy children check strategy each tensor concatenated isinstance this_strategy OpStrategy raise AssertionError f Expected OpStrategy got type this_strategy this_strategy mesh = mesh raise AssertionError cat op doesn t support cross mesh concatenation op_spec this_strategy strategies Check each OpSpec tensor placement OpSpec used exemplar strategy other tensors output tensor should follow We also need deduplicate output strategy same placement isinstance op_spec OpSpec raise AssertionError f Expected OpSpec got type op_spec exemplar OpSpec follow exemplar_spec = op_spec output_spec check tensor sharded concat dim is_tensor_dim_sharded exemplar_spec dim tensor sharded concat dim we need unshard first exemplar_placement = unshard_tensor_dim exemplar_spec placements dim exemplar_placement = exemplar_spec placements exemplar_placement strategies_placement_pool strategies_placement_pool add exemplar_placement assert isinstance exemplar_placement Tuple redistribute_costs = input_specs = idx range num_input_tensor extract strategy idx tensors build tensor_metadata redistribute_cost that_tensor_strategy = input_tuple_strategy children idx isinstance that_tensor_strategy OpStrategy raise AssertionError f Expected OpStrategy got type that_tensor_strategy input_spec = DTensorSpec mesh exemplar_placement tensor_meta=that_tensor_strategy strategies output_spec tensor_meta input_specs append input_spec redistribute_costs append generate_redistribute_costs that_tensor_strategy input_spec op_strategy strategies append OpSpec output_specs=DTensorSpec mesh exemplar_placement input_specs=tuple input_specs redistribute_cost=redistribute_costs op_strategy register_prop_rule aten index_select default schema_info=RuntimeSchemaInfo prop_index_select op_schema OpSchema - OutputSharding values_spec dim indices_spec = op_schema args_schema isinstance values_spec DTensorSpec raise AssertionError f Expected DTensorSpec got type values_spec isinstance dim int raise AssertionError f Expected int got type dim isinstance indices_spec DTensorSpec raise AssertionError f Expected DTensorSpec got type indices_spec all_indices_spec list Optional DTensorSpec = indices_spec dim == i None i range values_spec ndim result = prop_index OpSchema op=op_schema op args_schema= values_spec all_indices_spec kwargs_schema=op_schema kwargs_schema result redistribute_schema schema_suggestion = result redistribute_schema result redistribute_schema = OpSchema op=op_schema op args_schema= schema_suggestion args_schema dim schema_suggestion args_schema dim type ignore index kwargs_schema=op_schema kwargs_schema result register_op_strategy aten index_put default aten _index_put_impl_ default schema_info=RuntimeSchemaInfo needs_pytree=True prop_index_put op_schema OpSchema - StrategyType We have DTensor spec argument ` ` ` indices ` ` values ` accordingly in_spec indices_spec values_spec _ = op_schema args_schema isinstance in_spec OpStrategy raise AssertionError f Expected OpStrategy got type in_spec ` indices ` ` tuple scalar LongTensor so we use TupleStrategy isinstance indices_spec TupleStrategy raise AssertionError f Expected TupleStrategy got type indices_spec isinstance values_spec OpStrategy raise AssertionError f Expected OpStrategy got type values_spec mesh = values_spec mesh op_strategy = OpStrategy ` indices ` should all replicated first indices_redistribute_costs = new_indices_spec list Optional DTensorSpec = indices_spec_child indices_spec children isinstance indices_spec_child OpStrategy raise AssertionError f Expected OpStrategy got type indices_spec_child replicated_spec = DTensorSpec mesh=mesh placements=tuple Replicate mesh ndim tensor_meta=indices_spec_child strategies output_spec tensor_meta new_indices_spec append replicated_spec child_costs = generate_redistribute_costs indices_spec_child replicated_spec indices_redistribute_costs append child_costs For placement rule ` values ` ` ` assume ` values ` shape = b c d e f ` ` shape = d e f Then ` values ` s b c selected dim must replicated d e f nonselected dim both ` values ` ` ` should follow same sharding replicate shard partial size_offset = in_spec strategies output_spec ndim - values_spec strategies output_spec ndim We can either let ` values ` follow ` ` s placements reverse exemplar_spec in_spec values_spec use exemplar_spec target spec strategy exemplar_spec strategies in_spec_new_placements list Placement = values_spec_new_placements list Placement = placements = strategy output_spec placements placement placements placement is_shard isinstance placement Shard raise AssertionError f Expected Shard got type placement exemplar_spec in_spec let ` values_spce ` follow ` in_spec ` placement dim size_offset sharded selected dim need change replicate in_spec_new_placements append Replicate values_spec_new_placements append Replicate in_spec_new_placements append placement values_spec_new_placements append Shard placement dim - size_offset let ` in_spec ` follow ` values_spec ` in_spec_new_placements append Shard placement dim + size_offset values_spec_new_placements append placement in_spec_new_placements append Replicate values_spec_new_placements append Replicate new_in_spec = DTensorSpec mesh=mesh placements=tuple in_spec_new_placements tensor_meta=in_spec strategies output_spec tensor_meta new_values_spec = DTensorSpec mesh=mesh placements=tuple values_spec_new_placements tensor_meta=values_spec strategies output_spec tensor_meta output_spec = DTensorSpec mesh=mesh placements=tuple in_spec_new_placements tensor_meta=in_spec strategies output_spec tensor_meta cost_in_spec = generate_redistribute_costs in_spec new_in_spec cost_values_spec = generate_redistribute_costs values_spec new_values_spec op_strategy strategies append OpSpec input_specs= new_in_spec new_indices_spec type ignore arg-type new_values_spec output_specs=output_spec redistribute_cost= cost_in_spec indices_redistribute_costs cost_values_spec op_strategy register_prop_rule aten index Tensor schema_info=RuntimeSchemaInfo needs_pytree=True prop_index op_schema OpSchema - OutputSharding Expect replicated first input _mostly_ pointwise second input TODO exception when dtype second input bool then torch nonzero needs triggered first Current sharding constraints For values We currently require dimension values_spec replicated partial they being indexed Other dimensions values_spec can remain sharded they so For indices Indices can either sharded replicated All index tensors need sharded compatible way following pointwise rule including resolving Partial into either sharded replicated values_spec multi_indices_spec = op_schema args_schema isinstance values_spec DTensorSpec raise AssertionError f Expected DTensorSpec got type values_spec isinstance multi_indices_spec list raise AssertionError f Expected list got type multi_indices_spec multi_indices_spec = cast list Optional DTensorSpec multi_indices_spec valid_indices_spec list tuple int DTensorSpec = i i enumerate multi_indices_spec None All indices have sharded equally Moreover indices can broadcast Here we piggyback pointwise sharding rule indices indices_out = pointwise_rule OpSchema op=op_schema op args_schema=tuple v v valid_indices_spec kwargs_schema= need_reshard_on_indices = indices_out output_spec None need_reshard_on_indices means our inputs already sharded properly we will use our indices_spec isinstance indices_out output_spec DTensorSpec raise AssertionError f Expected DTensorSpec got type indices_out output_spec indices_spec DTensorSpec = indices_out output_spec indices_out redistribute_schema None raise AssertionError redistribute_schema should None valid_indices_suggestion = indices_out redistribute_schema i v enumerate valid_indices_suggestion args_spec multi_indices_spec valid_indices_spec i = v we ll need call pointwise_rule again see what s our ideal indices_spec then use compute our ideal values_spec indices_output_spec = pointwise_rule valid_indices_suggestion output_spec isinstance indices_output_spec DTensorSpec raise AssertionError f Expected DTensorSpec got type indices_output_spec indices_spec = indices_output_spec lookup_dims = v v valid_indices_spec need_reshard_on_values = tuple isinstance vp Shard vp dim lookup_dims isinstance ip Shard vp ip zip values_spec placements indices_spec placements need_reshard_on_indices any need_reshard_on_values value_placements = values_spec placements all_dims_consecutive = all b - == b zip valid_indices_spec valid_indices_spec - all_dims_consecutive all index vectors consecutives insert dimension first index insert_dim int = valid_indices_spec insert first dimension insert_dim = place vp Placement ip Placement - Placement isinstance vp Shard Shard vp dim vp dim insert_dim accounts offset output dimensions vp dim + indices_spec ndim - sum vp dim v v valid_indices_spec isinstance ip Shard Shard ip dim + insert_dim Partial Replicated vp value_placements = tuple place vp ip vp ip zip values_spec placements indices_spec placements result = OutputSharding output_spec=DTensorSpec mesh=values_spec mesh placements=value_placements result result = OutputSharding output_spec=None redistribute_schema=OpSchema op=op_schema op args_schema= DTensorSpec mesh=values_spec mesh placements=tuple Replicate need_reshard_on_values i v i v enumerate values_spec placements tensor_meta=values_spec tensor_meta multi_indices_spec kwargs_schema=op_schema kwargs_schema result register_op_strategy aten split Tensor aten split_with_sizes default aten split_with_sizes_copy default RuntimeSchemaInfo split_strategy op_schema OpSchema - OpStrategy input_strategy = op_schema args_schema split_size_or_sections = op_schema args_schema isinstance input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type input_strategy input_ndim = input_strategy ndim split_dim = cast int op_schema args_schema len op_schema args_schema dim = normalize_dim split_dim input_ndim size_split N i - list Last chunk will smaller tensor size N along given dimension dim divisible i i raise AssertionError f Split size must positive got i i N i + N i N i = output_size_list = size_split input_strategy shape dim split_size_or_sections isinstance split_size_or_sections int split_size_or_sections isinstance output_size_list Sized raise AssertionError f Expected Sized got type output_size_list all_strategies = strategy input_strategy strategies spec = strategy output_spec placements = spec placements is_tensor_dim_sharded spec dim=dim input sharded split dim we need unshard placements = unshard_tensor_dim spec placements dim=dim input_spec = DTensorSpec spec device_mesh placements spec tensor_meta output_specs = tuple DTensorSpec spec device_mesh placements _ range len output_size_list all_strategies append OpSpec output_specs=output_specs input_specs= input_spec redistribute_cost= generate_redistribute_costs input_strategy input_spec OpStrategy all_strategies TODO fix remaining failures xfail unbind test_dtensor_ops py remove xfail item register_op_strategy aten unbind int schema_info=RuntimeSchemaInfo gen_unbind_strategy op_schema OpSchema - StrategyType Forward all shardings except unbind dimension input_strategy = op_schema args_schema isinstance input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type input_strategy input_ndim = input_strategy ndim input_shape = input_strategy shape unbind_dim = cast int op_schema args_schema len op_schema args_schema unbind_dim = normalize_dim unbind_dim input_ndim mesh = input_strategy mesh unbind_strategy = OpStrategy arg_strategy input_strategy strategies arg_spec = arg_strategy output_spec is_tensor_dim_sharded arg_spec dim=unbind_dim raise RuntimeError f Attempted unbind along sharded dimension unbind_dim It cannot performed without redistribution which disallowed current operator only add strategy unbind dim sharded output_placements = shift_shard_dims_after_remove arg_spec placements unbind_dim output_specs = tuple DTensorSpec mesh tuple output_placements _ range input_shape unbind_dim unbind_strategy strategies append OpSpec output_specs=output_specs input_specs= arg_spec redistribute_cost= len input_strategy strategies unbind_strategy