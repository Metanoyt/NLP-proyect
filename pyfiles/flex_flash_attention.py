mypy allow-untyped-defs Call into flash-attention flexattention functools importlib contextlib contextmanager typing Any Callable Optional Sequence sympy sympy Expr Integer torch torch fx GraphModule torch utils _sympy functions Identity ir FixedLayout ShapeAsConstantBuffer Subgraph TensorBox lowering empty_strided common infer_dense_strides load_flex_template SubgraphResults aten = torch ops aten prims = torch ops prims functools lru_cache maxsize= ensure_flash_available - bool Check flash-attn importable cache result reuse Call ensure_flash_available cache_clear after installing flash-attn same interpreter retry try importlib util find_spec flash_attn cute None type ignore attr-defined except ImportError False codegen cutedsl cutedsl_template CuteDSLTemplate flash_attention_cutedsl_template = CuteDSLTemplate name= flash_attention_cutedsl source=load_flex_template flash_attention _fixed_indexer_cute size Sequence int stride Optional Sequence int = None offset Expr = Integer - Callable Sequence Expr Expr Colexicographic indexer CuteDSL - matches CuTe s coordinate interpretation CuTe interprets linear indices colexicographic column-major order whereas Inductor s default _fixed_indexer uses lexicographic row-major order For size= index= b q_idx - Lexicographic b + q_idx - Colexicographic b + q_idx CuTe then applies tensor s actual memory strides get correct offset indexer index Sequence Expr - Expr assert offset == Integer Offset supported colexicographic indexing index Integer result = index runner = size idx sz zip index size strict=True result = result + runner Identity idx runner = runner sz result indexer contextmanager patch_fixed_layout_indexer_for_cutedsl Temporarily swap FixedLayout make_indexer so CuteDSL sees colexicographic indexing Note CuteDSL indexer patch Flex flash attention only supports limited set IR ops pointwise reads no stores so temporarily changing indexing order safe kernels we emit today TODO dynamic shapes Reconfirm once flex flash attention supports dynamic shapes original_make_indexer = FixedLayout make_indexer cutedsl_make_indexer _fixed_indexer_cute size stride offset FixedLayout make_indexer = cutedsl_make_indexer type ignore assignment try yield finally FixedLayout make_indexer = original_make_indexer type ignore assignment input_buffers_require_grads graph_module num_score_mod_placeholders int Check any input buffers beyond score mod placeholders require gradients inputs = node graph_module graph nodes node op == placeholder inputs append node len inputs = num_score_mod_placeholders False requires_grad n tensor_meta = n meta get tensor_meta tensor_meta requires_grad tensor_meta None False any requires_grad n n inputs num_score_mod_placeholders is_trivial_mask_graph graph_module GraphModule - bool Mask graph trivial when only gates via default full op graph = graph_module graph nodes = list graph nodes placeholders = n n nodes n op == placeholder output = n n nodes n op == output assert len output == Got graph w multiple outputs output_val = output args mask mod graph empty we have inputs full_default output len placeholders == output_val target torch ops aten full default functools lru_cache maxsize= _supports_nontrivial_mask_graphs - bool Currently only supported Hopper SM GPUs torch cuda get_device_capability == _can_use_flex_flash_attention subgraph Subgraph mask_graph Subgraph num_score_mod_placeholders int - tuple bool str Check flex flash attention can used given inputs Returns tuple can_use reason where reason explains why can t used can_use False ensure_flash_available False CUTE flash attention library available input_buffers_require_grads subgraph graph_module num_score_mod_placeholders False Input buffers require gradients supported flash attention mask_trivial = is_trivial_mask_graph mask_graph graph_module mask_trivial True _supports_nontrivial_mask_graphs False NYI Non-trivial mask graphs only supported Hopper SM flash attention True _use_flex_flash_attention subgraph Subgraph mask_graph Subgraph kernel_options dict str Any num_score_mod_placeholders int - bool Determine we should use flex flash attention given inputs force_flash = kernel_options get force_flash False can_use reason = _can_use_flex_flash_attention subgraph mask_graph num_score_mod_placeholders force_flash can_use raise RuntimeError f force_flash=True flash attention cannot used reason force_flash can_use create_flex_flash_attention_kernel query TensorBox key TensorBox value TensorBox block_mask tuple Any scale float kernel_options dict str Any subgraph_buffer SubgraphResults mask_graph_buffer SubgraphResults score_mod_other_buffers list TensorBox mask_mod_other_buffers list TensorBox kv_num_blocks TensorBox &#124; None kv_indices TensorBox &#124; None full_kv_num_blocks TensorBox &#124; None full_kv_indices TensorBox &#124; None mask_graph Subgraph subgraph Subgraph &#124; None = None - tuple TensorBox &#124; ShapeAsConstantBuffer TensorBox &#124; ShapeAsConstantBuffer Create flex flash attention kernel using CuteDSL template ensure_flash_available raise RuntimeError CUTE flash attention available Get dimensions batch_size num_heads seq_len_q head_dim = query get_size v_head_dim = value get_size - device = query get_device dtype = query get_dtype assert device None Device must specified Match stride pattern query tensor q_strides = query get_stride out_size = batch_size num_heads seq_len_q v_head_dim out_strides = infer_dense_strides out_size q_strides output = empty_strided size=out_size stride=out_strides dtype=dtype device=device lse = empty_strided size= batch_size num_heads seq_len_q stride=None LSE can contiguous dtype=torch float LSE always fp device=device Create layout primary output output_layout = FixedLayout device=device dtype=dtype size= batch_size num_heads seq_len_q v_head_dim stride= sympy sympify s s output get_stride Used check we can skip block sparse impl mask_graph_is_trivial = is_trivial_mask_graph mask_graph graph_module needs_block_mask = mask_graph_is_trivial has_full_blocks = full_kv_num_blocks None choices list Any = assert flash_attention_cutedsl_template None input_nodes = query key value lse has_full_blocks input_nodes extend kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices needs_block_mask has_full_blocks raise NotImplementedError Flash attention block mask without full blocks supported yet error = flash_attention_cutedsl_template maybe_append_choice choices input_nodes=input_nodes layout=output_layout mutated_inputs= lse subgraphs= subgraph_buffer mask_graph_buffer SM_SCALE=scale NEEDS_BLOCK_MASK=needs_block_mask wrap_choice_render choice See Note CuteDSL indexer patch original_make_kernel_render = choice make_kernel_render make_kernel_render_with_patch args kwargs render_kernel render = original_make_kernel_render args kwargs Let template construct its closures then scope indexer patch actual render call emits kernel render_with_patch = patch_fixed_layout_indexer_for_cutedsl render render_kernel render_with_patch choice make_kernel_render = make_kernel_render_with_patch choice choices wrap_choice_render choice error choices Fallback original implementation raise RuntimeError f CuteDSL template failed error No autotune now template_output = choices output_node template_output lse