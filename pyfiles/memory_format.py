__future__ annotations typing TypeVar torch _M = TypeVar _M bound= torch nn Module convert_conv d_weight_memory_format module _M memory_format torch memory_format - _M r Convert ` ` memory_format ` ` ` ` nn Conv d weight ` ` ` ` memory_format ` ` The conversion recursively applies nested ` ` nn Module ` ` including ` ` module ` ` Note only changes memory_format semantics each dimensions This function used facilitate computation adopt NHWC kernels which provides considerable speed up fp data CUDA devices compute capability = note Calling ` ` model memory_format=torch channels_last ` ` more aggressive than utility function ` ` convert_conv d_weight_memory_format ` ` Any layer d weight will affected ` ` model ` ` which does necessarily benefit conversion specified ` ` memory_format ` ` One place we confident NHWC channels_last conversion convolution cuDNN beneficial run convolution NHWC even cases where we have apply permutation input tensors Hence our strategy here convert only weight convolution channels_last This ensures Fast convolution kernels will used benefit which could outweigh overhead permutation input same format No unnecessary permutations applied layers do benefit memory_format conversion The optimal case layers between convolution layers channels last compatible Input tensor would permuted channels last when encounters first convolution layer stay memory format Hence following convolutions will need permute its input tensor In case where channels last incompatible layer between convolution layers we need permute input tensor back contiguous format layer The input tensor will go through remaining layers contiguous format permuted channels last when encounters another convolution layer There s no point propagating permutation earlier layer most layers quite agnostic ` ` memory_format ` ` This claim might change when PyTorch supports fusion permutation there might have been better spot fuse permutation other than immediately before convolution Args module nn Module ` ` nn Conv d ` ` ` ` nn ConvTranspose d ` ` container ` ` nn Module ` ` memory_format user specified ` ` memory_format ` ` e g ` ` torch channels_last ` ` ` ` torch contiguous_format ` ` Returns The original module updated ` ` nn Conv d ` ` Example xdoctest +REQUIRES env TORCH_DOCTEST_CUDA xdoctest +REQUIRES env CUBLAS_WORKSPACE_CONFIG input = torch randint dtype=torch float device= cuda model = nn Sequential nn Conv d cuda half This identical nn utils convert_conv d_weight_memory_format model torch channels_last model = nn utils convert_conv d_weight_memory_format model torch channels_last out = model input TODO expand ` _ConvNd ` when channels_last support extended beyond only d tensors isinstance module torch nn Conv d torch nn ConvTranspose d weight_data = module weight detach clone memory_format=memory_format module weight data = weight_data resize_ weight_data size memory_format=memory_format child module children convert_conv d_weight_memory_format child memory_format pyrefly ignore bad-return module convert_conv d_weight_memory_format module _M memory_format torch memory_format - _M r Convert ` ` memory_format ` ` ` ` nn Conv d weight ` ` ` ` memory_format ` ` The conversion recursively applies nested ` ` nn Module ` ` including ` ` module ` ` Note only changes memory_format semantics each dimensions This function used facilitate computation adopt NHWC kernels which provides considerable speed up fp data CUDA devices compute capability = note Calling ` ` model memory_format=torch channels_last_ d ` ` more aggressive than utility function ` ` convert_conv d_weight_memory_format ` ` Any layer d weight will affected ` ` model ` ` which does necessarily benefit conversion specified ` ` memory_format ` ` One place we confident NDHWC channels_last_ d conversion convolution cuDNN beneficial run convolution NDHWC even cases where we have apply permutation input tensors Hence our strategy here convert only weight convolution channels_last_ d This ensures Fast convolution kernels will used benefit which could outweigh overhead permutation input same format No unnecessary permutations applied layers do benefit memory_format conversion The optimal case layers between convolution layers channels last compatible Input tensor would permuted channels last when encounters first convolution layer stay memory format Hence following convolutions will need permute its input tensor In case where channels last incompatible layer between convolution layers we need permute input tensor back contiguous format layer The input tensor will go through remaining layers contiguous format permuted channels last when encounters another convolution layer There s no point propagating permutation earlier layer most layers quite agnostic ` ` memory_format ` ` This claim might change when PyTorch supports fusion permutation there might have been better spot fuse permutation other than immediately before convolution Args module nn Module ` ` nn Conv d ` ` ` ` nn ConvTranspose d ` ` container ` ` nn Module ` ` memory_format user specified ` ` memory_format ` ` e g ` ` torch channels_last ` ` ` ` torch contiguous_format ` ` Returns The original module updated ` ` nn Conv d ` ` Example xdoctest +REQUIRES env TORCH_DOCTEST_CUDA xdoctest +REQUIRES env CUBLAS_WORKSPACE_CONFIG input = torch randint dtype=torch float device= cuda model = nn Sequential nn Conv d cuda half This identical nn utils convert_conv d_weight_memory_format model torch channels_last_ d model = nn utils convert_conv d_weight_memory_format model torch channels_last_ d out = model input TODO expand ` _ConvNd ` when channels_last support extended beyond only d tensors isinstance module torch nn Conv d torch nn ConvTranspose d weight_data = module weight detach clone memory_format=memory_format module weight data = weight_data resize_ weight_data size memory_format=memory_format child module children convert_conv d_weight_memory_format child memory_format pyrefly ignore bad-return module __all__ = convert_conv d_weight_memory_format convert_conv d_weight_memory_format