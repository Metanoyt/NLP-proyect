threading collections abc Generator contextlib contextmanager typing Any torch See Note Metadata mutation proxy tracing why sacrificial parameter mutates metadata during proxy tracing we should remove sacrificial parameter logic doc = This used when dynamo traces torch nn Parameter which normally would trace properly AOTAutograd We instead create placeholder torch nn Parameter before graph which becomes graph arg has no storage backing At point graph where parameter actually should created we mutate sacrificial placeholder into This allows gradients flow into parameter input graph which only thing we allowed compute gradients strip TracableCreateParameter torch autograd Function staticmethod pyrefly ignore bad-override forward ctx Any tensor Any placeholder Any - torch nn Parameter assert tensor requires_grad placeholder set_ tensor staticmethod backward ctx Any grad_outputs torch Tensor - tuple None torch Tensor grad = grad_outputs None grad grad flows placeholder tracable_create_parameter tensor torch Tensor placeholder torch nn Parameter - torch nn Parameter torch set_grad_enabled placeholder requires_grad out = TracableCreateParameter apply tensor placeholder out new_parameter_placeholder size tuple int dtype torch dtype device torch device requires_grad bool - torch nn Parameter Create placeholder passed above functions result = torch nn Parameter torch empty size dtype=dtype device=device requires_grad=requires_grad TODO jansel alloc followed free inefficient need way allocate unbacked tensor Allocating zero tensor would causes assert failures autograd result untyped_storage resize_ result _TLS = threading local contextmanager do_not_convert_to_tracable_parameter - Generator bool None None old_flag = getattr _TLS convert_tracable_parameter True _TLS convert_tracable_parameter = False try yield False finally _TLS convert_tracable_parameter = old_flag can_convert_to_tracable_parameter - bool getattr _TLS convert_tracable_parameter True