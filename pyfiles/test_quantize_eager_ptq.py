Owner s oncall quantization ruff noqa F hypothesis given strategies st torch torch ao nn quantized nnq torch nn nn torch testing _internal hypothesis_utils hu torch ao quantization convert default_dynamic_qconfig default_dynamic_quant_observer default_qconfig default_weight_observer DeQuantStub FixedQParamsObserver float _dynamic_qconfig float_qparams_weight_only_qconfig float_qparams_weight_only_qconfig_ bit per_channel_dynamic_qconfig PerChannelMinMaxObserver prepare prepare_qat QConfig quantize quantize_dynamic QuantStub QuantWrapper torch nn utils rnn PackedSequence annotated models torch testing _internal common_quantization ActivationsTestModel AnnotatedCustomConfigNestedModel AnnotatedNestedModel AnnotatedSingleLayerLinearModel AnnotatedSkipQuantModel AnnotatedSubNestedModel AnnotatedTwoLayerLinearModel convert_dynamic EmbeddingBagModule EmbeddingModule EmbeddingWithStaticLinear LinearReluLinearModel ModelWithFunctionals NestedModel NormalizationTestModel prepare_dynamic QuantizationTestCase QuantStubModel ResNetBase RNNCellDynamicModel RNNDynamicModel SingleLayerLinearDynamicModel skipIfNoFBGEMM test_only_eval_fn TwoLayerLinearModel torch testing _internal common_quantized override_qengines override_quantized_engine supported_qengines hu assert_deadline_disabled Standard library numpy np TestQuantizeEagerOps QuantizationTestCase override_qengines _test_reference_module_impl float_module_class quantized_module_class extra_module_kwargs input_size M torch nn Module __init__ - None super __init__ conv = float_module_class extra_module_kwargs quant = QuantStub dequant = DeQuantStub forward x x = quant x x = conv x x = dequant x x RefM torch nn Module __init__ - None super __init__ conv = float_module_class extra_module_kwargs quant = QuantStub dequant = DeQuantStub quant = QuantStub dequant = DeQuantStub forward x x = quant x x = dequant x x = conv x x = quant x x = dequant x x qengine = torch backends quantized engine qengine supported_qengines qengine == qnnpack qnnpack does support nnq ConvTranspose d data = torch randn input_size dtype=torch float original_m = M original_ref_m = RefM original_ref_m conv weight = torch nn Parameter original_m conv weight detach original_ref_m conv bias = torch nn Parameter original_m conv bias detach original_m qconfig = torch ao quantization default_qconfig m = prepare original_m calibration m data m = convert m check module properly quantized assertEqual type m quant nnq Quantize assertEqual type m conv quantized_module_class assertEqual type m dequant nnq DeQuantize res = m data quantize reference model original_ref_m eval original_ref_m qconfig = torch ao quantization default_qconfig ref_m = prepare original_ref_m ref_m data ref_m = convert ref_m is_reference=True ref_res = ref_m data assertEqual res ref_res test_conv_ d _test_reference_module_impl nn Conv d nnq Conv d in_channels out_channels kernel_size test_conv_ d _test_reference_module_impl nn Conv d nnq Conv d in_channels out_channels kernel_size test_conv_ d _test_reference_module_impl nn Conv d nnq Conv d in_channels out_channels kernel_size test_conv_transpose_ d _test_reference_module_impl nn ConvTranspose d nnq ConvTranspose d in_channels out_channels kernel_size test_conv_transpose_ d _test_reference_module_impl nn ConvTranspose d nnq ConvTranspose d in_channels out_channels kernel_size test_conv_transpose_ d _test_reference_module_impl nn ConvTranspose d nnq ConvTranspose d in_channels out_channels kernel_size test_linear _test_reference_module_impl nn Linear nnq Linear in_features out_features override_qengines test_int _reference_module RefM torch nn Module __init__ - None super __init__ conv = nn ConvTranspose d quant = QuantStub dequant = DeQuantStub quant = QuantStub dequant = DeQuantStub forward x x = quant x x = dequant x x = conv x x = quant x x = dequant x x input_size = data = torch randn input_size dtype=torch float original_ref_m = RefM rand_w = torch randn_like original_ref_m conv weight rand_b = torch randn_like original_ref_m conv bias original_ref_m conv weight = torch nn Parameter rand_w requires_grad=False original_ref_m conv bias = torch nn Parameter rand_b requires_grad=False qengine = torch backends quantized engine qengine supported_qengines torch ao quantization observer MovingAverageMinMaxObserver weight_obs = MovingAverageMinMaxObserver with_args dtype=torch qint set qmin qmax represent qint quant_min=- quant_max= - qscheme=torch per_tensor_symmetric act_obs = MovingAverageMinMaxObserver with_args dtype=torch qint quant_min=- quant_max= - custom_qconfig = QConfig activation=act_obs weight=weight_obs quantize reference model original_ref_m eval original_ref_m qconfig = custom_qconfig ref_m = prepare original_ref_m calibration ref_m torch randn input_size dtype=torch float ref_m = convert ref_m is_reference=True myobs = MovingAverageMinMaxObserver averaging_constant= dtype=torch qint set qmin qmax represent qint quant_min=- quant_max= - qscheme=torch per_tensor_symmetric result = myobs rand_w qparams = myobs calculate_qparams assertEqual ref_m conv weight_scale qparams _test_activation_op_impl float_module_class quantized_module_class extra_module_kwargs Implementation testing common activation ops like leaky relu Args extra_module_kwargs keyword args instantiate float module M torch nn Module __init__ - None super __init__ activation_op = float_module_class extra_module_kwargs quant = QuantStub dequant = DeQuantStub forward x x = quant x x = activation_op x x = dequant x x m = M eval m qconfig = default_qconfig m = prepare m checkObservers m m = convert m assertEqual type m activation_op quantized_module_class test_leaky_relu _test_activation_op_impl nn LeakyReLU nnq LeakyReLU negative_slope inplace False test_relu _test_activation_op_impl nn ReLU nn ReLU inplace False Histogram Observers slow so have no-deadline ensure test doesn t time out given train_mode=st booleans test_functional_module train_mode model = ModelWithFunctionals x = torch rand dtype=torch float xq = torch quantize_per_tensor x torch quint checkScriptable model x check_save_load=True train_mode model qconfig = torch ao quantization get_default_qat_qconfig fbgemm model = prepare_qat model model qconfig = torch ao quantization get_default_qconfig qnnpack model = prepare model Check observers quant dequant nodes inserted checkNoPrepModules model checkObservers model Calibrate model xq dequantize model = convert model checkQuantized model checkNoPrepModules model assertEqual type model myadd torch ao nn quantized QFunctional assertEqual type model mycat torch ao nn quantized QFunctional assertEqual type model myadd_relu torch ao nn quantized QFunctional assertEqual type model mymatmul torch ao nn quantized QFunctional checkNoQconfig model checkQuantized model checkScriptable model xq check_save_load=True TestQuantizeEagerPTQStatic QuantizationTestCase test_single_layer r Quantize SingleLayerLinearModel which has one Linear module make sure swapped nnq Linear which quantized version module qengine supported_qengines override_quantized_engine qengine qconfig = torch ao quantization get_default_qconfig qengine model = AnnotatedSingleLayerLinearModel qengine model qconfig = qconfig model = prepare model Check observers quant dequant nodes inserted checkNoPrepModules model checkHasPrepModules model fc checkObservers model test_only_eval_fn model calib_data model = convert model checkQuantized model checkNoPrepModules model checkHasPrepModules model fc checkWrappedQuantizedLinear model fc test_only_eval_fn model calib_data checkScriptable model calib_data checkNoQconfig model checkQuantized model test one line API - out place version base = AnnotatedSingleLayerLinearModel qengine base qconfig = qconfig keys_before = set base state_dict keys model = quantize base test_only_eval_fn calib_data checkQuantized model keys_after = set base state_dict keys assertEqual keys_before keys_after simple check nothing changed in-place version model = AnnotatedSingleLayerLinearModel qengine model qconfig = qconfig quantize model test_only_eval_fn calib_data inplace=True checkQuantized model skipIfNoFBGEMM test_two_layers r TwoLayerLinearModel has two Linear modules we only quantize second one ` fc ` ` fc ` quantized override_quantized_engine fbgemm model = AnnotatedTwoLayerLinearModel model = prepare model checkNoPrepModules model checkObservers model checkNoPrepModules model fc checkHasPrepModules model fc test_only_eval_fn model calib_data model = convert model checkQuantized model checkNoPrepModules model checkNoPrepModules model fc checkHasPrepModules model fc assertEqual type model fc torch nn Linear checkWrappedQuantizedLinear model fc test_only_eval_fn model calib_data checkScriptable model calib_data checkNoQconfig model checkQuantized model test one line API model = quantize AnnotatedTwoLayerLinearModel test_only_eval_fn calib_data checkQuantized model test_nested r Test quantization nested model top level fc fc submodule sub sub fc quantized qengine supported_qengines override_quantized_engine qengine model = AnnotatedNestedModel qengine checkPrepModules model before_calib=False before_calib checkObservers model checkNoPrepModules model checkNoPrepModules model sub checkNoPrepModules model sub fc checkNoPrepModules model sub relu checkNoPrepModules model sub checkHasPrepModules model sub fc checkNoPrepModules model sub fc checkHasPrepModules model fc model = prepare model checkPrepModules model True test_only_eval_fn model calib_data model = convert model checkQuantized model checkPrepModules model checkLinear model sub fc checkWrappedQuantizedLinear model fc checkWrappedQuantizedLinear model sub fc checkLinear model sub fc test_only_eval_fn model calib_data checkScriptable model calib_data checkNoQconfig model checkQuantized model test one line API model = quantize AnnotatedNestedModel qengine test_only_eval_fn calib_data checkQuantized model skipIfNoFBGEMM test_nested model = AnnotatedSubNestedModel model = prepare model checkPrepModules model before_calib=False before_calib checkObservers model checkNoPrepModules model checkNoPrepModules model sub checkNoPrepModules model sub fc checkNoPrepModules model sub relu checkHasPrepModules model sub checkNoPrepModules model sub module fc checkNoPrepModules model sub module fc checkHasPrepModules model fc checkPrepModules model True test_only_eval_fn model calib_data model = convert model checkQuantized model checkPrepModules model checkLinear model sub fc assertEqual type model sub relu torch nn ReLU checkQuantizedLinear model sub module fc checkQuantizedLinear model sub module fc checkWrappedQuantizedLinear model fc test_only_eval_fn model calib_data checkScriptable model calib_data checkNoQconfig model checkQuantized model test one line API model = quantize AnnotatedSubNestedModel test_only_eval_fn calib_data checkQuantized model test_nested r More complicated nested test case child qconfig overrides parent qconfig qengine supported_qengines override_quantized_engine qengine model = AnnotatedCustomConfigNestedModel model = prepare model checkPrepModules model before_calib=False before_calib checkObservers model checkNoPrepModules model checkNoPrepModules model sub checkNoPrepModules model sub fc checkNoPrepModules model sub relu checkNoPrepModules model sub checkHasPrepModules model sub fc checkHasPrepModules model sub fc checkHasPrepModules model fc checkPrepModules model True test_only_eval_fn model calib_data model = convert model checkQuantized model checkPrepModules model checkWrappedQuantizedLinear model sub fc checkWrappedQuantizedLinear model sub fc checkWrappedQuantizedLinear model fc test_only_eval_fn model calib_data checkScriptable model calib_data checkNoQconfig model checkQuantized model test one line API model = quantize AnnotatedCustomConfigNestedModel test_only_eval_fn calib_data checkQuantized model test_skip_quant r The case when we want skip quantizing some layers qengine supported_qengines override_quantized_engine qengine model = AnnotatedSkipQuantModel qengine model = prepare model checkObservers model test_only_eval_fn model calib_data model = convert model checkQuantized model checkLinear model fc checkQuantDequant model sub checkQuantizedLinear model sub module fc checkQuantizedLinear model sub module fc assertEqual type model sub module relu nn ReLU assertEqual type model sub module relu nn ReLU checkScriptable model calib_data checkNoQconfig model checkQuantized model test one line API model = quantize AnnotatedSkipQuantModel qengine test_only_eval_fn calib_data checkQuantized model skipIfNoFBGEMM test_manual r User inserts QuantStub DeQuantStub model code call quantization utility functions model = QuantStubModel propagate qconfig parents children model changed inplace model = prepare model checkObservers model test_only_eval_fn model calib_data model = convert model checkQuantized model assertEqual type model fc nnq Linear test_only_eval_fn model calib_data checkScriptable model calib_data checkNoQconfig model checkQuantized model test one line API model = quantize QuantStubModel test_only_eval_fn calib_data checkQuantized model test_resnet_base r Test quantization bottleneck topology used resnet resnext add coverage conversion average pool float functional qengine supported_qengines override_quantized_engine qengine qconfig = torch ao quantization get_default_qconfig qengine model = ResNetBase float eval model fuse_model model = QuantWrapper model model qconfig = qconfig model = prepare model checkObservers model test_only_eval_fn model img_data_ d model = convert model checkQuantized model assertEqual type model module conv nn intrinsic quantized ConvReLU d assertEqual type model module myop nn quantized QFunctional assertEqual type model module avgpool nn AdaptiveAvgPool d assertEqual type model module fc nnq Linear test_only_eval_fn model img_data_ d checkNoQconfig model checkQuantized model skipIfNoFBGEMM test_normalization r Test quantization normalization layers model = NormalizationTestModel model qconfig = torch ao quantization get_default_qconfig fbgemm prepare model inplace=True checkObservers model test_only_eval_fn model calib_data model = convert model checkQuantized model checkNoPrepModules model layer_norm checkNoPrepModules model group_norm checkNoPrepModules model instance_norm d checkNoPrepModules model instance_norm d checkNoPrepModules model instance_norm d assertEqual type model layer_norm nnq LayerNorm assertEqual type model group_norm nnq GroupNorm assertEqual type model instance_norm d nnq InstanceNorm d assertEqual type model instance_norm d nnq InstanceNorm d assertEqual type model instance_norm d nnq InstanceNorm d test_only_eval_fn model calib_data checkScriptable model calib_data checkNoQconfig model checkQuantized model model_oneline = quantize NormalizationTestModel test_only_eval_fn calib_data checkQuantized model test_save_load_state_dict r Test PTQ flow creating model quantizing saving quantized state_dict Load quantized state_dict eval compare results against original model qengine supported_qengines override_quantized_engine qengine model = TwoLayerLinearModel model = torch ao quantization QuantWrapper model model qconfig = torch ao quantization get_default_qconfig qengine model = prepare model calibrate test_only_eval_fn model calib_data model = convert model x = torch rand dtype=torch float ref = model x quant_state_dict = model state_dict Create model again eval model = TwoLayerLinearModel model = torch ao quantization QuantWrapper model model qconfig = torch ao quantization get_default_qconfig qengine model = prepare model model = convert model new_state_dict = model state_dict Check make sure state dict keys match original model after convert assertEqual set new_state_dict keys set quant_state_dict keys model load_state_dict quant_state_dict out = model x assertEqual ref out skipIfNoFBGEMM test_activations r Test quantization activations model = ActivationsTestModel model qconfig = torch ao quantization get_default_qconfig fbgemm prepare model inplace=True checkObservers model test_only_eval_fn model calib_data model = convert model checkQuantized model checkNoPrepModules model hardswish assertEqual type model hardswish nnq Hardswish assertEqual type model elu nnq ELU test_only_eval_fn model calib_data checkScriptable model calib_data checkNoQconfig model checkQuantized model test one line API model_oneline = quantize ActivationsTestModel test_only_eval_fn calib_data checkQuantized model_oneline override_qengines test_forward_hooks_preserved r Test post-training static quantization preserving pre forward post forward hooks original model qengine = torch backends quantized engine model = QuantStubModel counter = pre_forwards forwards fw_pre_hook h_module input counter pre_forwards += fw_hook h_module input output counter forwards += model fc register_forward_pre_hook fw_pre_hook model fc register_forward_hook fw_hook model qconfig = torch ao quantization get_default_qconfig qengine model = prepare model checkHooksIsPresent model before_convert=True num_fwd_hooks = before_convert assertEqual len model quant _forward_hooks values Quantization observer hook has disappeared num_fwd_hooks = assertObjectIn fw_pre_hook model fc _forward_pre_hooks values assertObjectIn fw_hook model fc _forward_hooks values assertEqual len model fc _forward_pre_hooks values Extra pre forward hooks have appeared layer During static quantization non stub layers provided quantization observer hook too assertEqual len model fc _forward_hooks values num_fwd_hooks Extra post forward hooks have appeared layer Implicitly check fw_hook goes after _observer_forward_hook assertEqual list model fc _forward_hooks values - fw_hook _observer_forward_hook first entry hooks list checkHooksIsPresent model True test_only_eval_fn model calib_data torch ao quantization convert model inplace=True checkHooksIsPresent model False skipIfNoFBGEMM test_quantized_embedding r Test post-training quantization flow serialization scripting embedding modules qconfig float_qparams_weight_only_qconfig float_qparams_weight_only_qconfig_ bit model = EmbeddingModule eval indices = torch tensor weights = torch randn dtype=torch float model qconfig = qconfig prepare model inplace=True convert model inplace=True assertTrue QuantizedEmbedding str model assertEqual type model emb torch ao nn quantized Embedding checkScriptable model indices check_save_load=True idx = torch LongTensor offsets = torch LongTensor x = torch randn model = EmbeddingWithStaticLinear eval prepare model inplace=True convert model inplace=True assertTrue QuantizedEmbedding str model assertTrue QuantizedLinear str model checkQuantizedLinear model fc model idx offsets x skipIfNoFBGEMM test_dequant_stub m = QuantStubModel eval prepare m inplace=True checkObservers m convert m inplace=True assertEqual type m quant nnq Quantize assertEqual type m fc nnq Linear assertEqual type m dequant nnq DeQuantize check DeQuantStub swapped when doesn t have qconfig m = QuantStubModel eval m dequant qconfig = None prepare m inplace=True checkObservers m convert m inplace=True assertEqual type m quant nnq Quantize assertEqual type m fc nnq Linear assertEqual type m dequant DeQuantStub test_quantized_embedding_bag r Test post-training quantization flow serialization scripting embedding_bag modules indices = torch tensor offsets = torch tensor weights = torch randn dtype=torch float dtype torch quint torch quint x model = EmbeddingBagModule eval float_qparams_observer = PerChannelMinMaxObserver with_args dtype=dtype qscheme=torch per_channel_affine_float_qparams ch_axis= float_qparams_qconfig = QConfig activation=default_dynamic_quant_observer weight=float_qparams_observer model qconfig = float_qparams_qconfig prepare model inplace=True quantized_model = convert model per_sample_weights = torch from_numpy np random uniform low= high= size= len indices astype np float Test make sure module quantized correctly assertTrue QuantizedEmbeddingBag str quantized_model checkDynamicQuantizedModule quantized_model emb torch ao nn quantized EmbeddingBag torch quint checkScriptable quantized_model indices offsets per_sample_weights check_save_load=True EmbeddingBagWithLinear torch nn Module __init__ - None super __init__ emb = torch nn EmbeddingBag num_embeddings= embedding_dim= include_last_offset=True scale_grad_by_freq=False mode= sum fc = torch nn Linear forward indices offsets per_sample_weights linear_in emb indices offsets per_sample_weights fc linear_in Test quantization embedding_bag layer only model = EmbeddingBagWithLinear eval model emb qconfig = float_qparams_qconfig prepare model inplace=True quantized_model = convert model assertTrue QuantizedEmbeddingBag str quantized_model checkLinear model fc checkDynamicQuantizedModule quantized_model emb torch ao nn quantized EmbeddingBag torch quint skipIfNoFBGEMM test_custom_module_class CustomModule torch nn Module __init__ - None super __init__ conv = torch nn Conv d forward x conv x ObservedCustomModule torch nn Module __init__ conv super __init__ conv = conv forward x conv x classmethod from_float cls float_module assert hasattr float_module qconfig observed = cls float_module conv observed qconfig = float_module qconfig observed QuantizedCustomModule torch nn Module __init__ conv super __init__ conv = conv forward x conv x classmethod from_observed cls observed_module assert hasattr observed_module qconfig assert hasattr observed_module activation_post_process observed_module conv activation_post_process = observed_module activation_post_process quantized = cls nnq Conv d from_float observed_module conv quantized Sub torch nn Module __init__ - None super __init__ custom = CustomModule forward x custom x M torch nn Module __init__ - None super __init__ quant = QuantStub conv = torch nn Conv d sub = Sub dequant = DeQuantStub forward x x = quant x x = conv x x = sub x x = dequant x x RefM torch nn Module __init__ - None super __init__ quant = QuantStub conv = torch nn Conv d conv = torch nn Conv d dequant = DeQuantStub forward x x = quant x x = conv x x = conv x x = dequant x x data = torch randn instantiate M RefM align parameters original_m = M original_ref_m = RefM original_ref_m conv weight = torch nn Parameter original_m conv weight detach original_ref_m conv bias = torch nn Parameter original_m conv bias detach original_ref_m conv weight = torch nn Parameter original_m sub custom conv weight detach original_ref_m conv bias = torch nn Parameter original_m sub custom conv bias detach original_m qconfig = default_qconfig prepare_custom_config_dict = float_to_observed_custom_module_class CustomModule ObservedCustomModule convert_custom_config_dict = observed_to_quantized_custom_module_class ObservedCustomModule QuantizedCustomModule m = prepare original_m prepare_custom_config_dict=prepare_custom_config_dict checkObservers m None prepare_custom_config_dict calibration m data all activation observers inserted top level module check converted quantized model m = convert m convert_custom_config_dict=convert_custom_config_dict check module properly quantized assertEqual type m quant nnq Quantize assertEqual type m conv nnq Conv d assertEqual type m sub Sub assertEqual type m sub custom QuantizedCustomModule assertEqual type m sub custom conv nnq Conv d assertEqual type m dequant nnq DeQuantize res = m data quantize reference model original_ref_m eval original_ref_m qconfig = default_qconfig ref_m = prepare original_ref_m ref_m data ref_m = convert ref_m ref_res = ref_m data assertEqual res ref_res skipIfNoFBGEMM test_convtranspose_per_channel_fails_early r Verifies attempting quantize ConvTranspose module per-Channel weight observers fails prepare step opposed convert step m = torch nn Sequential torch nn ConvTranspose d m qconfig = torch ao quantization get_default_qconfig fbgemm assertRaises AssertionError context mp = torch ao quantization prepare m assertTrue str context exception == Per channel weight observer supported yet ConvTranspose n d skipIfNoFBGEMM test_convtranspose_per_channel_qconfig_none r Verifies having qconfig==None conv transpose does crash m = torch nn Sequential torch nn ConvTranspose d m qconfig = torch ao quantization get_default_qconfig fbgemm m qconfig = None mp = torch ao quantization prepare m skipIfNoFBGEMM test_quantwrapper_attaches_qconfig_to_dequant qconfig = torch ao quantization default_qconfig m = nn Sequential nn Conv d eval i range len m m i qconfig = qconfig m i = torch ao quantization QuantWrapper m i mp = torch ao quantization prepare m mq = torch ao quantization convert mp assertTrue isinstance mq dequant nnq DeQuantize test_activations_in_non_leaf_module_list Ensure activations like ` nn Sigmoid ` ` nn Tanh ` properly handled ` non_leaf_module_list ` MyModel torch nn Module __init__ - None super __init__ quant = QuantStub sigmoid = torch nn Sigmoid hardsigmoid = torch nn Hardsigmoid softmax = torch nn Softmax tanh = torch nn Tanh dequant = DeQuantStub forward x x = quant x x = sigmoid x x = hardsigmoid x x = softmax x x = tanh x x = dequant x x qconfig = QConfig activation=FixedQParamsObserver with_args scale= zero_point= weight=default_weight_observer m = MyModel m qconfig = qconfig m = prepare m observer_non_leaf_module_list= torch nn Sigmoid torch nn Hardsigmoid torch nn Softmax torch nn Tanh Should use observer specified QConfig instead default FixedQParamsFakeQuantize assertTrue isinstance m sigmoid activation_post_process FixedQParamsObserver assertTrue isinstance m hardsigmoid activation_post_process FixedQParamsObserver assertTrue isinstance m softmax activation_post_process FixedQParamsObserver assertTrue isinstance m tanh activation_post_process FixedQParamsObserver skipIfNoFBGEMM test_mha_batch_first_attr_is_copied_in_prepare TransformerDecoderLayer nn Module __init__ d_model nhead batch_first super __init__ self_attn = nn MultiheadAttention d_model nhead dropout= batch_first=batch_first qengine = torch backends quantized engine batch_first True False model = TransformerDecoderLayer batch_first quantization_config = torch ao quantization get_default_qconfig qengine model qconfig = quantization_config prepared_model = torch ao quantization prepare model inplace=False assertTrue prepared_model self_attn batch_first == model self_attn batch_first skipIfNoFBGEMM TestQuantizeEagerPTQDynamic QuantizationTestCase test_single_layer r Dynamic Quantize SingleLayerLinearDynamicModel which has one Linear module make sure swapped nnqd Linear which quantized version module dtype torch qint torch float model = SingleLayerLinearDynamicModel eval qconfig = float _dynamic_qconfig dtype == torch float default_dynamic_qconfig qconfig_dict = fc qconfig prepare_dynamic model qconfig_dict convert_dynamic model checkQuantized model checkDynamicQuantizedLinear model fc dtype checkScriptable model calib_data check_save_load=True checkNoQconfig model checkQuantized model test one line API - out place version base = SingleLayerLinearDynamicModel keys_before = set base state_dict keys model = quantize_dynamic base qconfig_dict checkQuantized model keys_after = set base state_dict keys assertEqual keys_before keys_after simple check nothing changed in-place version model = SingleLayerLinearDynamicModel quantize_dynamic model qconfig_dict inplace=True checkQuantized model Test set qconfig model = SingleLayerLinearDynamicModel quantize_dynamic model nn Linear inplace=True dtype=dtype checkQuantized model test_two_layers r TwoLayerLinearModel has two Linear modules we only quantize second one ` fc ` ` fc ` quantized dtype torch qint torch float model = TwoLayerLinearModel eval qconfig = float _dynamic_qconfig dtype == torch float default_dynamic_qconfig qconfig_dict = fc qconfig prepare_dynamic model qconfig_dict convert_dynamic model checkQuantized model assertEqual type model fc torch nn Linear checkDynamicQuantizedLinear model fc dtype=dtype checkScriptable model calib_data check_save_load=True checkNoQconfig model checkQuantized model test one line API model = quantize_dynamic TwoLayerLinearModel eval qconfig_dict checkQuantized model Test set API model = quantize_dynamic TwoLayerLinearModel eval fc dtype=dtype checkQuantized model test_nested r Test quantization nested model top level fc fc submodule sub sub fc quantized dtype torch qint torch float model = NestedModel eval qconfig = float _dynamic_qconfig dtype == torch float default_dynamic_qconfig qconfig_dict = fc qconfig sub fc qconfig prepare_dynamic model qconfig_dict convert_dynamic model checkQuantized model checkLinear model sub fc checkDynamicQuantizedLinear model fc dtype=dtype checkDynamicQuantizedLinear model sub fc dtype=dtype checkLinear model sub fc checkScriptable model calib_data check_save_load=True checkNoQconfig model checkQuantized model test one line API model = quantize_dynamic NestedModel eval qconfig_dict checkQuantized model model = quantize_dynamic NestedModel eval fc sub fc dtype=dtype checkQuantized model test_nested r Another test case quantized we will quantize all submodules submodule sub dtype torch qint torch float model = NestedModel eval qconfig = float _dynamic_qconfig dtype == torch float default_dynamic_qconfig qconfig_dict = fc qconfig sub qconfig prepare_dynamic model qconfig_dict convert_dynamic model checkQuantized model checkLinear model sub fc assertEqual type model sub relu torch nn ReLU checkDynamicQuantizedLinear model sub fc dtype=dtype checkDynamicQuantizedLinear model sub fc dtype=dtype checkDynamicQuantizedLinear model fc dtype=dtype checkScriptable model calib_data check_save_load=True checkNoQconfig model checkQuantized model test one line API model = quantize_dynamic NestedModel eval qconfig_dict dtype=dtype checkQuantized model Test set API model = quantize_dynamic NestedModel eval fc sub dtype=dtype checkQuantized model test_nested r More complicated nested test case child qconfig overrides parent qconfig dtype torch qint torch float model = NestedModel eval qconfig = float _dynamic_qconfig dtype == torch float default_dynamic_qconfig qconfig_dynamic_dict = fc qconfig sub qconfig sub fc qconfig prepare_dynamic model qconfig_dynamic_dict convert_dynamic model checkQuantized model checkDynamicQuantizedLinear model sub fc dtype=dtype checkDynamicQuantizedLinear model sub fc dtype=dtype checkDynamicQuantizedLinear model fc dtype=dtype checkScriptable model calib_data check_save_load=True checkNoQconfig model checkQuantized model test one line API model = quantize_dynamic NestedModel eval qconfig_dynamic_dict checkQuantized model Test set API model = quantize_dynamic NestedModel eval fc sub sub fc dtype=dtype checkQuantized model test_type_match_rule r Test quantization nested model top level fc fc submodule sub All torch nn Linear modules quantized dtype torch qint torch float model = NestedModel eval qconfig = float _dynamic_qconfig dtype == torch float default_dynamic_qconfig qconfig_dict = fc None sub fc None torch nn Linear qconfig prepare_dynamic model qconfig_dict test_only_eval_fn model calib_data convert_dynamic model checkQuantized model checkDynamicQuantizedLinear model sub fc dtype=dtype checkLinear model fc checkLinear model sub fc checkDynamicQuantizedLinear model sub fc dtype=dtype test_only_eval_fn model calib_data checkScriptable model calib_data check_save_load=True checkNoQconfig model checkQuantized model test one line API model = quantize_dynamic NestedModel eval qconfig_dict dtype=dtype checkQuantized model test_per_channel_linear_quantize r Test quantization per_channel dynamic quantization model = NestedModel eval qconfig_dict = torch nn Linear per_channel_dynamic_qconfig prepare_dynamic model qconfig_dict test_only_eval_fn model calib_data convert_dynamic model checkQuantized model checkDynamicQuantizedLinear model sub fc dtype=torch qint checkDynamicQuantizedLinear model fc dtype=torch qint checkDynamicQuantizedLinear model sub fc dtype=torch qint checkDynamicQuantizedLinear model sub fc dtype=torch qint test_only_eval_fn model calib_data checkScriptable model calib_data check_save_load=True checkNoQconfig model checkQuantized model test one line API model = quantize_dynamic NestedModel eval qconfig_dict checkQuantized model test_linear_relu_fusion dtype = torch qint model = LinearReluLinearModel eval qconfig = default_dynamic_qconfig qconfig_dict = qconfig torch ao quantization fuse_modules model fc relu inplace=True prepare_dynamic model qconfig_dict convert_dynamic model checkQuantized model checkDynamicQuantizedLinearRelu model fc dtype checkDynamicQuantizedLinear model fc dtype checkScriptable model calib_data check_save_load=True checkNoQconfig model checkQuantized model given qconfig=st sampled_from per_channel_dynamic_qconfig default_dynamic_qconfig dtype=st sampled_from torch qint torch float test_quantized_rnn qconfig dtype r Test dynamic quantization scriptability serialization dynamic quantized lstm modules int fp niter = x = torch tensor - - - dtype=torch float unsqueeze repeat niter qconfig_dict = torch nn LSTM qconfig torch nn GRU qconfig checkQuantized model module_type mod_type_map = LSTM torch ao nn quantized dynamic LSTM GRU torch ao nn quantized dynamic GRU mod_repr_map = LSTM DynamicQuantizedLSTM GRU DynamicQuantizedGRU assertTrue mod_repr_map module_type str model_quantized checkDynamicQuantizedModule model_quantized mod mod_type_map module_type dtype module_type LSTM GRU model = RNNDynamicModel module_type eval dtype == torch float model_quantized = quantize_dynamic model=model dtype=dtype model_quantized = quantize_dynamic model=model qconfig_spec=qconfig_dict dtype=dtype checkQuantized model_quantized module_type checkScriptable model_quantized x check_save_load=True ScriptWrapperPackedLSTM torch nn Module __init__ cell super __init__ cell = cell forward x PackedSequence - tuple PackedSequence tuple torch Tensor torch Tensor cell x ScriptWrapperPackedGRU torch nn Module __init__ cell super __init__ cell = cell forward x PackedSequence - tuple PackedSequence torch Tensor cell x script_wrapper_map = LSTM ScriptWrapperPackedLSTM GRU ScriptWrapperPackedGRU packed_input = torch nn utils rnn pack_padded_sequence x torch tensor model_with_packed_input = script_wrapper_map module_type model_quantized mod model_with_packed_input packed_input scripted = torch jit script model_with_packed_input scripted packed_input We cannot trace input dtype being packed sequence _checkScriptable model_with_packed_input scripted packed_input True given qconfig=st sampled_from per_channel_dynamic_qconfig default_dynamic_qconfig dtype=st sampled_from torch qint torch float test_quantized_rnn_cell qconfig dtype r Test dynamic quantization scriptability serialization dynamic quantized rnn cell modules int fp qconfig_dict = torch nn LSTMCell qconfig torch nn GRUCell qconfig torch nn RNNCell qconfig module_type LSTMCell GRUCell RNNTanh RNNReLU model = RNNCellDynamicModel module_type eval x = torch tensor - - - dtype=torch float torch backends quantized engine == qnnpack dtype == torch float continue fp dynamic quant supported qnnpack dtype == torch float model_quantized = quantize_dynamic model=model dtype=dtype model_quantized = quantize_dynamic model=model qconfig_spec=qconfig_dict dtype=dtype checkQuantized model module_type mod_type_map = LSTMCell torch ao nn quantized dynamic LSTMCell GRUCell torch ao nn quantized dynamic GRUCell RNNTanh torch ao nn quantized dynamic RNNCell RNNReLU torch ao nn quantized dynamic RNNCell mod_repr_map = LSTMCell DynamicQuantizedLSTMCell GRUCell DynamicQuantizedGRUCell RNNTanh DynamicQuantizedRNNCell RNNReLU DynamicQuantizedRNNCell assertTrue mod_repr_map module_type str model_quantized checkDynamicQuantizedModule model_quantized mod mod_type_map module_type dtype checkNoQconfig model Smoke test extra reprs checkQuantized model_quantized module_type checkScriptable model_quantized x check_save_load=True test_forward_hooks_preserved r Test post-training dynamic quantization preserving pre forward post forward hooks original model dtype torch qint torch float model = SingleLayerLinearDynamicModel eval qconfig = float _dynamic_qconfig dtype == torch float default_dynamic_qconfig qconfig_dict = fc qconfig convert_dynamic model counter = pre_forwards forwards fw_pre_hook h_module input counter pre_forwards += fw_hook h_module input output counter forwards += model fc register_forward_pre_hook fw_pre_hook model fc register_forward_hook fw_hook prepare_dynamic model qconfig_dict checkHooksIsPresent model assertObjectIn fw_pre_hook model fc _forward_pre_hooks values assertObjectIn fw_hook model fc _forward_hooks values assertEqual len model fc _forward_pre_hooks values Extra pre forward hooks have appeared layer assertEqual len model fc _forward_hooks values Extra post forward hooks have appeared layer checkHooksIsPresent model test_only_eval_fn model calib_data convert_dynamic model checkHooksIsPresent model skipIfNoFBGEMM test_embedding_bag_dynamic EmbeddingBagWithLinear torch nn Module __init__ - None super __init__ emb = torch nn EmbeddingBag num_embeddings= embedding_dim= include_last_offset=True scale_grad_by_freq=False mode= sum fc = torch nn Linear forward indices offsets linear_in emb indices offsets fc linear_in model = EmbeddingBagWithLinear eval qconfig_dict = torch nn EmbeddingBag float_qparams_weight_only_qconfig torch nn Linear default_dynamic_qconfig indices = torch tensor offsets = torch tensor q_model = quantize_dynamic model qconfig_dict q_model indices offsets torch randn assertTrue QuantizedEmbeddingBag str q_model emb assertTrue DynamicQuantizedLinear str q_model fc skipIfNoFBGEMM test_embedding_ops_dynamic EmbeddingWithLinear torch nn Module __init__ - None super __init__ emb = torch nn Embedding num_embeddings= embedding_dim= scale_grad_by_freq=False fc = torch nn Linear forward indices linear_in emb indices fc linear_in model = EmbeddingWithLinear eval qconfig_dict = torch nn Embedding float_qparams_weight_only_qconfig torch nn Linear default_dynamic_qconfig indices = torch tensor q_model = quantize_dynamic model qconfig_dict assertTrue QuantizedEmbedding str q_model emb assertTrue DynamicQuantizedLinear str q_model fc q_model indices torch randn __name__ == __main__ raise RuntimeError This test file meant run directly use \n\n \tpython test test_quantization py TESTNAME\n\n instead