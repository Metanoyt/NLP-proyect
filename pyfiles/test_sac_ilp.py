Owner s oncall distributed copy unittest torch torch _subclasses fake_tensor FakeTensorMode torch distributed _tools ilp_utils aggregate_stats get_peak_memory_runtime_baseline ModuleInfo parse_module_info torch distributed _tools mem_tracker _ModState MemTracker torch distributed _tools runtime_estimator RuntimeEstimator torch distributed _tools sac_estimator SACEstimator SACStats torch distributed _tools sac_ilp get_optimal_checkpointing_policy_per_module sac_milp torch testing _internal common_cuda TEST_CUDA torch testing _internal common_utils MI _ARCH run_tests skipIfRocmArch skipIfTorchDynamo TestCase torch testing _internal distributed _tensor common_dtensor ModelArgs Transformer TestSACILP TestCase setUp super setUp device = torch cuda current_device estimate_mode = operator-level-cost-model _init_model_input_optimizer - tuple torch nn Module torch optim Optimizer torch Tensor bsz = model_args = ModelArgs n_layers= n_heads= vocab_size= max_seq_len= dim= dropout_p= torch device device model = Transformer model_args optimizer = torch optim Adam model parameters lr= e- foreach=True inp = torch randint model_args vocab_size bsz model_args max_seq_len device=self device model optimizer inp _run_and_get_memTracker model torch nn Module optimizer torch optim Optimizer inp torch Tensor - MemTracker mem_tracker = MemTracker mem_tracker track_external model optimizer mem_tracker mt iter_idx range running twice initialize optimizer output = model inp output sum backward iter_idx == last_snapshot = mt get_tracker_snapshot current optimizer step optimizer zero_grad iter_idx == mt reset_mod_stats assert last_snapshot None mod_stats mem_tracker memory_tracking values postprocessing due fact ModTracker post backward hook being called modules whose inputs don t require gradients TODO fix ModTracker ensure does lead any perf regression _ModState POST_BW mod_stats snapshots keys mod_stats snapshots setdefault _ModState POST_BW append copy deepcopy last_snapshot mem_tracker _run_and_get_runtime_estimator model torch nn Module optimizer torch optim Optimizer inp torch Tensor - RuntimeEstimator _run_one_step - None output = model inp output sum backward optimizer step optimizer zero_grad Initializing optimizer states warm-up _run_one_step runtime_estimator = RuntimeEstimator runtime_estimator estimate_mode_type=self estimate_mode _run_one_step We use only one iteration estimation runtime_estimator _run_and_get_sac_estimator model torch nn Module inp torch Tensor - SACEstimator sac_estimator = SACEstimator sac_estimator estimate_mode_type=self estimate_mode loss = model inp sum loss backward sac_estimator _collect_module_info_with_fake_tensor_mode - ModuleInfo FakeTensorMode model optimizer inp = _init_model_input_optimizer mem_tracker = _run_and_get_memTracker model optimizer inp runtime_estimator = _run_and_get_runtime_estimator model optimizer inp sac_estimator = _run_and_get_sac_estimator model inp mod_info = aggregate_stats model mem_tracker runtime_estimator sac_estimator torch device device mod_info skipIfTorchDynamo https github com pytorch pytorch issues unittest skipIf TEST_CUDA CUDA available skipIfRocmArch MI _ARCH test_sac_ilp_case This case where memory budget either binding too tight meaning some AC model can fit into GPU memory mod_info = _collect_module_info_with_fake_tensor_mode g = parse_module_info mod_info peak_mem compute_time = get_peak_memory_runtime_baseline g assertAlmostEqual peak_mem delta= ac_decisions recomputation_time _ = sac_milp g memory_budget= world_size= The solution should AC all four transformer layers On A machine percentage activation memory discard three layers fourth layer Due symmetry layer has can any first three layers On CI due machine variance difference flops results can different -- e g ratios four transformer layers test linux-jammy-cuda -py -gcc test distributed lf linux xlarge nvidia gpu recomputation_time = compute_time = modules_to_ac = set ac_decisions keys sorted_discard_ratio = sorted ac_decisions values assertEqual modules_to_ac Transformer layers + str i i range n_layers= assertAlmostEqual sorted_discard_ratio delta= assertAlmostEqual sorted_discard_ratio delta= assertAlmostEqual sorted_discard_ratio delta= assertAlmostEqual sum sorted_discard_ratio delta= assertAlmostEqual ac_decisions Transformer layers delta= On A machine recomputation_time ms compute_time ms Since runtime device_flops dependent so we only check ratio assertAlmostEqual recomputation_time compute_time delta= skipIfTorchDynamo https github com pytorch pytorch issues unittest skipIf TEST_CUDA CUDA available test_sac_ilp_case This case where memory budget binding meaning no AC needed fit model into memory mod_info = _collect_module_info_with_fake_tensor_mode g = parse_module_info mod_info ac_decisions recomputation_time peak_mem = sac_milp g memory_budget= world_size= assertDictEqual ac_decisions assertEqual recomputation_time assertGreater peak_mem skipIfTorchDynamo https github com pytorch pytorch issues unittest skipIf TEST_CUDA CUDA available test_sac_ilp_case This case where memory budget too tight meaning even aggressive AC model cannot fit into memory mod_info = _collect_module_info_with_fake_tensor_mode g = parse_module_info mod_info ac_decisions recomputation_time peak_mem = sac_milp g memory_budget= world_size= assertEqual ac_decisions assertEqual recomputation_time assertEqual peak_mem - TestOptimalCheckpointingPolicy TestCase tests adapted tests xformers https github com facebookresearch xformers blob c c ac f b bc c ed f f tests test_checkpoint py#L setUp super setUp data = aten copy_ aten add aten div aten mm aten native_dropout aten linear aten t aten relu_ sac_stats = SACStats func_names= x x data runtimes= x x data memory= x x data view_like_ops= rand_ops= saved_autograd_ops= needed SAC decisions inplace_ops= force_store_random=False skipIfTorchDynamo https github com pytorch pytorch issues unittest skipIf TEST_CUDA CUDA available test_get_optimial_checkpointing_policy_per_module memory_budget optimal_soln soln = get_optimal_checkpointing_policy_per_module sac_stats=self sac_stats memory_budget=memory_budget assertEqual optimal_soln soln __name__ == __main__ run_tests