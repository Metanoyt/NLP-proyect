usr bin env python encoding UTF- os shutil subprocess check_call check_output list_dir path str - list str Helper getting paths Python check_output ls - path decode split \n replace_tag filename - None open filename f lines = f readlines i line enumerate lines line startswith Tag lines i = line replace -linux_ -manylinux_ _ _ print f Updated tag line lines i break open filename w f f writelines lines patch_library_rpath folder str lib_name str use_nvidia_pypi_libs bool = False desired_cuda str = - None Apply patchelf set RPATH library torch lib lib_path = f folder tmp torch lib lib_name use_nvidia_pypi_libs For PyPI NVIDIA libraries construct CUDA RPATH cuda_rpaths = $ ORIGIN nvidia cudnn lib $ ORIGIN nvidia nvshmem lib $ ORIGIN nvidia nccl lib $ ORIGIN nvidia cusparselt lib desired_cuda cuda_rpaths append $ ORIGIN nvidia cu lib cuda_rpaths extend $ ORIGIN nvidia cublas lib $ ORIGIN nvidia cuda_cupti lib $ ORIGIN nvidia cuda_nvrtc lib $ ORIGIN nvidia cuda_runtime lib $ ORIGIN nvidia cufft lib $ ORIGIN nvidia curand lib $ ORIGIN nvidia cusolver lib $ ORIGIN nvidia cusparse lib $ ORIGIN nvidia nvtx lib $ ORIGIN nvidia cufile lib Add $ ORIGIN local torch libs rpath = join cuda_rpaths + $ ORIGIN For bundled libraries just use $ ORIGIN rpath = $ ORIGIN os path exists lib_path os system f cd folder tmp torch lib f patchelf -- set-rpath rpath -- force-rpath lib_name copy_and_patch_library src_path str folder str use_nvidia_pypi_libs bool = False desired_cuda str = - None Copy library torch lib patch its RPATH os path exists src_path lib_name = os path basename src_path shutil copy src_path f folder tmp torch lib lib_name patch_library_rpath folder lib_name use_nvidia_pypi_libs desired_cuda package_cuda_wheel wheel_path desired_cuda - None Package cuda wheel libraries folder = os path dirname wheel_path os mkdir f folder tmp os system f unzip wheel_path -d folder tmp Delete original wheel since will repackaged os system f rm wheel_path Check we should use PyPI NVIDIA libraries bundle system libraries use_nvidia_pypi_libs = os getenv USE_NVIDIA_PYPI_LIBS == use_nvidia_pypi_libs print Using nvidia libs pypi - skipping CUDA library bundling For PyPI approach we don t bundle CUDA libraries - they come PyPI packages We only need bundle non-NVIDIA libraries minimal_libs_to_copy = lib libgomp so usr lib libgfortran so acl build libarm_compute so acl build libarm_compute_graph so usr local lib libnvpl_lapack_lp _gomp so usr local lib libnvpl_blas_lp _gomp so usr local lib libnvpl_lapack_core so usr local lib libnvpl_blas_core so Copy minimal libraries unzipped_folder torch lib lib_path minimal_libs_to_copy copy_and_patch_library lib_path folder use_nvidia_pypi_libs desired_cuda Patch torch libraries used searching libraries torch_libs_to_patch = libtorch so libtorch_cpu so libtorch_cuda so libtorch_cuda_linalg so libtorch_global_deps so libtorch_python so libtorch_nvshmem so libc so libc _cuda so libcaffe _nvrtc so libshm so lib_name torch_libs_to_patch patch_library_rpath folder lib_name use_nvidia_pypi_libs desired_cuda print Bundling CUDA libraries wheel Original logic bundling system CUDA libraries Common libraries all CUDA versions common_libs = Non-NVIDIA system libraries lib libgomp so usr lib libgfortran so acl build libarm_compute so acl build libarm_compute_graph so Common CUDA libraries same all versions usr local lib libnvpl_lapack_lp _gomp so usr local lib libnvpl_blas_lp _gomp so usr local lib libnvpl_lapack_core so usr local lib libnvpl_blas_core so usr local cuda extras CUPTI lib libnvperf_host so usr local cuda lib libcudnn so usr local cuda lib libcusparseLt so usr local cuda lib libcurand so usr local cuda lib libnccl so usr local cuda lib libnvshmem_host so usr local cuda lib libcudnn_adv so usr local cuda lib libcudnn_cnn so usr local cuda lib libcudnn_graph so usr local cuda lib libcudnn_ops so usr local cuda lib libcudnn_engines_runtime_compiled so usr local cuda lib libcudnn_engines_precompiled so usr local cuda lib libcudnn_heuristic so usr local cuda lib libcufile so usr local cuda lib libcufile_rdma so usr local cuda lib libcusparse so CUDA version-specific libraries desired_cuda minor_version = desired_cuda - version_specific_libs = usr local cuda extras CUPTI lib libcupti so usr local cuda lib libcublas so usr local cuda lib libcublasLt so usr local cuda lib libcudart so usr local cuda lib libcufft so usr local cuda lib libcusolver so usr local cuda lib libnvJitLink so usr local cuda lib libnvrtc so f usr local cuda lib libnvrtc-builtins so minor_version desired_cuda Get last character libnvrtc-builtins version e g - minor_version = desired_cuda - version_specific_libs = usr local cuda extras CUPTI lib libcupti so usr local cuda lib libcublas so usr local cuda lib libcublasLt so usr local cuda lib libcudart so usr local cuda lib libcufft so usr local cuda lib libcusolver so usr local cuda lib libnvJitLink so usr local cuda lib libnvrtc so f usr local cuda lib libnvrtc-builtins so minor_version raise ValueError f Unsupported CUDA version desired_cuda Combine all libraries libs_to_copy = common_libs + version_specific_libs Copy libraries unzipped_folder torch lib lib_path libs_to_copy copy_and_patch_library lib_path folder use_nvidia_pypi_libs desired_cuda Make sure wheel tagged manylinux_ _ f os scandir f folder tmp f is_dir f name endswith dist-info replace_tag f f path WHEEL break os system f wheel pack folder tmp -d folder os system f rm -rf folder tmp complete_wheel folder str - str Complete wheel build put artifact location wheel_name = list_dir f folder dist Please note cuda we don t run auditwheel since we use custom script package cuda dependencies wheel file using update_wheel method However we need make sure filename reflects correct Manylinux platform pytorch folder enable_cuda print Repairing Wheel AuditWheel check_call auditwheel repair f dist wheel_name cwd=folder repaired_wheel_name = list_dir f folder wheelhouse print f Moving repaired_wheel_name wheel folder dist os rename f folder wheelhouse repaired_wheel_name f folder dist repaired_wheel_name repaired_wheel_name = list_dir f folder dist print f Copying repaired_wheel_name artifacts shutil copy f folder dist repaired_wheel_name f artifacts repaired_wheel_name repaired_wheel_name parse_arguments Parse inline arguments argparse ArgumentParser parser = ArgumentParser AARCH wheels python CD parser add_argument -- debug action= store_true parser add_argument -- build-only action= store_true parser add_argument -- test-only type=str parser add_argument -- enable-mkldnn action= store_true parser add_argument -- enable-cuda action= store_true parser parse_args __name__ == __main__ Entry Point args = parse_arguments enable_mkldnn = args enable_mkldnn enable_cuda = args enable_cuda branch = check_output git rev-parse -- abbrev-ref HEAD cwd= pytorch decode print Building PyTorch wheel build_vars = MAX_JOB= required CPU backend see commit d b enable_cuda build_vars += MAX_JOBS= Handle PyPI NVIDIA libraries vs bundled libraries use_nvidia_pypi_libs = os getenv USE_NVIDIA_PYPI_LIBS == use_nvidia_pypi_libs print Configuring build PyPI NVIDIA libraries Configure dynamic linking matching x logic build_vars += ATEN_STATIC_CUDA= USE_CUDA_STATIC_LINK= USE_CUPTI_SO= print Configuring build bundled NVIDIA libraries Keep existing static linking approach - already configured above override_package_version = os getenv OVERRIDE_PACKAGE_VERSION desired_cuda = os getenv DESIRED_CUDA override_package_version None version = override_package_version build_vars += f BUILD_TEST= PYTORCH_BUILD_VERSION= version PYTORCH_BUILD_NUMBER= branch nightly main build_date = check_output git log -- pretty=format cs - cwd= pytorch decode replace - version = check_output cat version txt cwd= pytorch decode strip - enable_cuda build_vars += f BUILD_TEST= PYTORCH_BUILD_VERSION= version dev build_date + desired_cuda PYTORCH_BUILD_NUMBER= build_vars += f BUILD_TEST= PYTORCH_BUILD_VERSION= version dev build_date PYTORCH_BUILD_NUMBER= branch startswith v v build_vars += f BUILD_TEST= PYTORCH_BUILD_VERSION= branch branch find - PYTORCH_BUILD_NUMBER= enable_mkldnn print build pytorch mkldnn+acl backend build_vars += USE_MKLDNN=ON USE_MKLDNN_ACL=ON build_vars += ACL_ROOT_DIR= acl enable_cuda build_vars += BLAS=NVPL build_vars += BLAS=OpenBLAS OpenBLAS_HOME= opt OpenBLAS print build pytorch without mkldnn backend os system f cd pytorch build_vars python -m build -- wheel -- no-isolation enable_cuda print Updating Cuda Dependency filename = os listdir pytorch dist wheel_path = f pytorch dist filename package_cuda_wheel wheel_path desired_cuda pytorch_wheel_name = complete_wheel pytorch print f Build Complete Created pytorch_wheel_name