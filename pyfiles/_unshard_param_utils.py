mypy allow-untyped-defs contextlib warnings collections abc Generator typing cast torch torch distributed fsdp _traversal_utils traversal_utils torch nn nn torch distributed fsdp _common_utils _FSDPState _get_module_fsdp_state _has_fsdp_params _module_handle HandleTrainingState TrainingState torch distributed fsdp _runtime_utils _lazy_init _reset_flat_param_grad_info_if_needed _reshard _reshard_grads _unshard _unshard_grads torch distributed utils _p_assert _flat_param FlatParamHandle FLAT_PARAM = _flat_param torch no_grad _writeback_to_local_shard handle FlatParamHandle writeback_grad bool For handle writes back rank s shard unsharded flattened parameter sharded flattened parameter If ` ` writeback_grad=True ` ` then writes back sharded gradient well Precondition The handle s ` ` FlatParameter ` ` s data points padded unsharded flattened parameter _get_shard flat_param_or_grad torch Tensor - torch Tensor handle uses_sharded_strategy For sharded strategies get unpadded shard instead padded shard persist user changes padding though FSDP does explicitly support shard _ = FlatParamHandle _get_unpadded_shard flat_param_or_grad handle rank handle world_size shard For ` NO_SHARD ` ` flat_param ` its gradient may modified so we write back directly flat_param_or_grad param_shard = _get_shard handle flat_param handle flat_param _local_shard param_shard numel copy_ param_shard type ignore attr-defined writeback_grad existing_grad = handle sharded_grad existing_grad None handle flat_param grad None raise AssertionError Expected handle flat_param grad None grad_shard = _get_shard handle flat_param grad existing_grad grad_shard numel copy_ grad_shard _deregister_flat_param state _FSDPState module nn Module - None De-registers flattened parameter wrapped module hiding ` ` nn Module ` ` methods We do use ` ` del ` ` because we want ` ` FLAT_PARAM ` ` always attribute dynamically change whether visible ` ` nn Module ` ` methods _has_fsdp_params state module TODO figure out case composable APIs cast nn Module module module _parameters pop FLAT_PARAM None _register_flat_param state _FSDPState module nn Module - None Registers flattened parameter wrapped module making visible ` ` nn Module ` ` methods We do use meth ` nn Module register_parameter ` because we want ` ` FLAT_PARAM ` ` always attribute dynamically change whether visible ` ` nn Module ` ` methods handle = _module_handle state module _has_fsdp_params state module TODO figure out case composable APIs cast nn Module module module _parameters FLAT_PARAM = handle flat_param contextlib contextmanager _unflatten_as_params state _FSDPState module nn Module - Generator Assumes flattened parameter unsharded When context de-registers flattened parameter unflattens original parameters ` ` nn Parameter ` ` views into flattened parameter After context re-registers flattened parameter restores original parameters ` ` Tensor ` ` views into flattened parameter handle = _module_handle state module handle yield _deregister_flat_param state module try handle unflatten_as_params yield finally handle _use_orig_params _register_flat_param state module _validate_unshard_params_args state _FSDPState writeback bool rank _only bool offload_to_cpu bool with_grads bool - None with_grads offload_to_cpu state _use_orig_params raise NotImplementedError f with_grads= with_grads f use_orig_params= state _use_orig_params f offload_to_cpu= offload_to_cpu f supported yet offload_to_cpu state _handle state _handle uses_sharded_strategy raise NotImplementedError offload_to_cpu=True NO_SHARD supported yet writeback rank _only TODO Rank can broadcast ` FlatParameter ` allow all ranks persist changes raise NotImplementedError writeback=True rank _only=True supported yet offload_to_cpu rank _only warnings warn offload_to_cpu=True rank _only=False may result unsharded parameters being redundantly copied CPU memory GPUs sharing same CPU memory which risks CPU OOM We recommend using offload_to_cpu=True rank _only=True stacklevel= contextlib contextmanager _unshard_fsdp_state_params module nn Module state _FSDPState writeback bool rank _only bool offload_to_cpu bool with_grads bool This unshards parameters single FSDP state ` ` state ` ` corresponds ` ` module ` ` _validate_unshard_params_args state writeback rank _only offload_to_cpu with_grads state _device_handle synchronize If handles shared other module s handle may already unsharded maybe_handle = _module_handle state module handle = None maybe_handle maybe_handle _training_state = HandleTrainingState SUMMON_FULL_PARAMS handle = maybe_handle handle yield handle _training_state = HandleTrainingState IDLE raise AssertionError f Expects handle training IDLE got handle _training_state handle _training_state = HandleTrainingState SUMMON_FULL_PARAMS _reset_flat_param_grad_info_if_needed handle free_unsharded_flat_param = handle needs_unshard No need call ` wait_stream ` since we unshard computation stream directly computation_stream = state _device_handle current_stream _unshard state handle computation_stream computation_stream with_grads _unshard_grads handle rank _only state rank = Free unsharded flattened parameter early _reshard state handle free_unsharded_flat_param with_grads _reshard_grads handle try yield finally handle _training_state = HandleTrainingState IDLE Unflatten unsharded flattened parameters contextlib ExitStack stack Invariant rank == rank _only offload_to_cpu handle uses_sharded_strategy stack enter_context handle to_cpu NOTE Since PyTorch enforces parameter its gradients need match metadata e g device we must move gradients CPU after we move parameters NOTE This assumes ` FlatParameter ` state _use_orig_params stack enter_context _unflatten_as_params state module try yield finally stack close writeback _writeback_to_local_shard handle with_grads _reshard state handle free_unsharded_flat_param with_grads _reshard_grads handle handle _training_state = HandleTrainingState IDLE contextlib contextmanager _unshard_params_for_summon module nn Module state _FSDPState writeback bool rank _only bool offload_to_cpu bool with_grads bool _validate_unshard_params_args state writeback rank _only offload_to_cpu with_grads _lazy_init state module state training_state == TrainingState FORWARD_BACKWARD raise AssertionError Cannot manually unshard parameters during forward backward state training_state == TrainingState SUMMON_FULL_PARAMS raise AssertionError Cannot manually unshard parameters when already unsharding parameters _unshard_fsdp_state_params module=module state=state writeback=writeback rank _only=rank _only offload_to_cpu=offload_to_cpu with_grads=with_grads try state training_state = TrainingState SUMMON_FULL_PARAMS yield finally state training_state = TrainingState IDLE contextlib contextmanager _unshard_params module nn Module recurse bool writeback bool rank _only bool offload_to_cpu bool with_grads bool This unshards FSDP-managed parameters all modules FSDP applied module tree rooted ` ` module ` ` recurse optional_state = _get_module_fsdp_state module optional_state None contextlib nullcontext yield states_and_modules = optional_state module states_and_modules = traversal_utils _get_fsdp_states_with_modules module contextlib ExitStack stack state module zip states_and_modules stack enter_context _unshard_params_for_summon module=module state=state writeback=writeback rank _only=rank _only offload_to_cpu=offload_to_cpu with_grads=with_grads yield _deregister_orig_params state _FSDPState module nn Module - None Deregisters original parameters registers ` ` FlatParameter ` ` handle = _module_handle state module handle _p_assert handle _use_orig_params f Inconsistent ` _use_orig_params ` -- FSDP state _use_orig_params f handle handle _use_orig_params handle _deregister_orig_params _register_flat_param state module _register_orig_params state _FSDPState module nn Module - None Deregisters ` ` FlatParameter ` ` registers original parameters handle = _module_handle state module handle _deregister_flat_param state module handle is_sharded handle flat_param handle _use_sharded_views handle _use_sharded_grad_views handle _use_unsharded_views as_params=True