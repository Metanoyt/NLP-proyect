torchaudio_models models utils check_for_functorch extract_weights GetterReturnType load_weights torch torch nn Tensor has_functorch = check_for_functorch get_wav letter device torch device - GetterReturnType N = input_frames = vocab_size = model = models Wav Letter num_classes=vocab_size criterion = torch nn NLLLoss model device params names = extract_weights model inputs = torch rand N input_frames device=device labels = torch rand N device=device mul vocab_size long forward new_params Tensor - Tensor load_weights model names new_params out = model inputs loss = criterion out labels loss forward params get_deepspeech device torch device - GetterReturnType sample_rate = window_size = window = hamming audio_conf = dict sample_rate=sample_rate window_size=window_size window=window noise_dir=None N = num_classes = spectrogram_size = Commented original sizes code seq_length = target_length = labels = torch rand num_classes device=device inputs = torch rand N spectrogram_size seq_length device=device Sequence length each input inputs_sizes = torch rand N device=device mul seq_length add seq_length targets = torch rand N target_length device=device targets_sizes = torch full N target_length dtype=torch int device=device model = models DeepSpeech rnn_type=nn LSTM labels=labels rnn_hidden_size= nb_layers= audio_conf=audio_conf bidirectional=True has_functorch functorch experimental replace_all_batch_norm_modules_ replace_all_batch_norm_modules_ model model = model device criterion = nn CTCLoss params names = extract_weights model forward new_params Tensor - Tensor load_weights model names new_params out out_sizes = model inputs inputs_sizes out = out transpose For ctc loss loss = criterion out targets out_sizes targets_sizes loss forward params get_transformer device torch device - GetterReturnType For most SOTA research you would like have embed nhead bsz tgt_len src_len N = seq_length = ntoken = model = models TransformerModel ntoken=ntoken ninp= nhead= nhid= nlayers= model device has_functorch disable dropout consistency checking model eval criterion = nn NLLLoss params names = extract_weights model data = torch rand N seq_length + device=device mul ntoken long inputs = data narrow seq_length targets = data narrow seq_length forward new_params Tensor - Tensor load_weights model names new_params out = model inputs loss = criterion out reshape N seq_length ntoken targets reshape N seq_length loss forward params get_multiheadattn device torch device - GetterReturnType From https github com pytorch text blob master test data test_modules py#L embed_dim nhead tgt_len src_len bsz = Build torchtext MultiheadAttention module in_proj = models InProjContainer torch nn Linear embed_dim embed_dim bias=False torch nn Linear embed_dim embed_dim bias=False torch nn Linear embed_dim embed_dim bias=False model = models MultiheadAttentionContainer nhead in_proj models ScaledDotProduct torch nn Linear embed_dim embed_dim bias=False model device params names = extract_weights model query = torch rand tgt_len bsz embed_dim device=device key = value = torch rand src_len bsz embed_dim device=device attn_mask_ D = torch randint tgt_len src_len device=device torch bool bias_k = bias_v = torch rand embed_dim device=device attn_mask = torch stack attn_mask_ D bsz nhead bias_k = bias_k repeat bsz reshape bsz nhead - bias_v = bias_v repeat bsz reshape bsz nhead - forward new_params Tensor - Tensor load_weights model names new_params mha_output attn_weights = model query key value attn_mask=attn_mask bias_k=bias_k bias_v=bias_v Don t test any specific loss just backprop ones both outputs loss = mha_output sum + attn_weights sum loss forward params