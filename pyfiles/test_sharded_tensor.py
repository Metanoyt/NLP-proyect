Owner s oncall distributed copy io itertools math pickle sys torch torch distributed dist torch distributed distributed_c d rpc torch distributed _shard sharded_tensor torch distributed _shard api _collect_local_shard _reshard_output _shard_tensor load_with_process_group shard_parameter torch distributed _shard sharded_tensor custom_sharded_op_impl pre_load_state_dict_hook Shard ShardedTensor ShardedTensorBase ShardedTensorMetadata state_dict_hook torch distributed _shard sharded_tensor api _create_tensor_from_params TensorProperties torch distributed _shard sharded_tensor utils _parse_and_validate_remote_device torch distributed _shard sharding_spec ChunkShardingSpec EnumerableShardingSpec ShardMetadata torch distributed remote_device _remote_device torch testing _internal common_distributed requires_nccl skip_if_lt_x_gpu spawn_threads_and_init_comms tp_transports torch testing _internal common_utils run_tests skip_but_pass_in_sandcastle_if skipIfRocm TEST_CUDA TEST_WITH_DEV_DBG_ASAN TestCase torch testing _internal distributed _shard sharded_tensor ShardedTensorTestBase with_comms torch testing _internal distributed _shard sharded_tensor _test_st_common _chunk_sharding_specs_list_for_test MyShardedModel TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit TestShardedTensorMetadata TestCase test_serialize_and_deserialize shard_metadatas = ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda dtypes = torch float torch double torch cfloat torch cdouble torch half torch bfloat torch uint torch int torch short torch int torch long torch bool layouts = torch strided torch sparse_coo requires_grads = True False memory_formats = torch contiguous_format torch channels_last torch preserve_format pin_memories = True False tensor_properties_input itertools product dtypes layouts requires_grads memory_formats pin_memories dtype layout requires_grad memory_format pin_memory = tensor_properties_input expected_st_metadata = sharded_tensor ShardedTensorMetadata shard_metadatas TensorProperties dtype layout requires_grad memory_format pin_memory pickled_obj = pickle dumps expected_st_metadata st_metadata = pickle loads pickled_obj assertEqual expected_st_metadata st_metadata TestCreateTensorFromParams TestCase skip_but_pass_in_sandcastle_if TEST_CUDA CUDA GPU needed test_empty expected_dtype = torch double tensor_properties = TensorProperties dtype=expected_dtype layout=torch strided requires_grad=False pin_memory=False memory_format=torch contiguous_format local_device = torch device cuda local_tensor = _create_tensor_from_params local_device=local_device tensor_properties=tensor_properties assertEqual local_device local_tensor device assertEqual expected_dtype local_tensor dtype assertEqual torch strided local_tensor layout assertEqual False local_tensor requires_grad TestShardParameter ShardedTensorTestBase with_comms init_rpc=False skip_if_lt_x_gpu requires_nccl test_shard_parameter spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda fc = torch nn Linear cuda rank weight_og = fc weight clone shard_parameter fc weight spec Verify assertTrue isinstance fc weight ShardedTensor local_shards = fc weight local_shards assertEqual len local_shards assertEqual torch Size local_shards tensor size assertEqual local_shards tensor size assertEqual local_shards tensor size assertEqual torch narrow weight_og rank local_shards tensor with_comms init_rpc=False skip_if_lt_x_gpu requires_nccl test_shard_parameter_errors spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda fc = torch nn Linear cuda rank assertRaisesRegex ValueError does match src_rank shard_parameter fc weight spec src_rank=self rank assertRaisesRegex AttributeError has no attribute shard_parameter fc foo spec assertRaisesRegex ValueError Expected Linear bias Tensor found str del fc bias fc bias = foo shard_parameter fc bias spec assertRaisesRegex ValueError contiguous Tensor fc bias = torch rand cuda rank t shard_parameter fc bias spec spec = ChunkShardingSpec dim= placements= f rank rank cuda rank cuda rank cuda rank cuda assertRaisesRegex ValueError does match sharding_spec shard_parameter fc weight spec spec = EnumerableShardingSpec ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda assertRaisesRegex NotImplementedError implemented yet shard_parameter fc weight spec TestShardTensor ShardedTensorTestBase with_comms init_rpc=False skip_if_lt_x_gpu requires_nccl test_shard_tensor spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda tensor = torch rand cuda rank st = _shard_tensor tensor spec Verify assertTrue isinstance st sharded_tensor ShardedTensor local_shard = st local_tensor assertEqual len st local_shards assertEqual torch Size local_shard size assertEqual torch narrow tensor rank local_shard with_comms init_rpc=False skip_if_lt_x_gpu requires_nccl test_shard_tensor_with_empty_shard spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda tensor = torch rand cuda rank st = _shard_tensor tensor spec Verify assertTrue isinstance st sharded_tensor ShardedTensor sms = st metadata shards_metadata assertEqual len sms sm sms assertTrue sm shard_offsets + sm shard_sizes = tensor size local_shard = st local_tensor assertEqual len st local_shards dist get_rank assertEqual torch Size local_shard size assertEqual torch narrow tensor rank local_shard assertEqual torch Size local_shard size with_comms init_rpc=False skip_if_lt_x_gpu requires_nccl test_shard_tensor_errors spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda tensor = torch rand cuda rank assertRaisesRegex ValueError does match src_rank _shard_tensor tensor spec src_rank=self rank assertRaisesRegex ValueError contiguous Tensor tensor_t = torch rand cuda rank t _shard_tensor tensor_t spec spec = ChunkShardingSpec dim= placements= f rank rank cuda rank cuda rank cuda rank cuda assertRaisesRegex ValueError does match sharding_spec _shard_tensor tensor spec spec = EnumerableShardingSpec ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda assertRaisesRegex NotImplementedError implemented yet _shard_tensor tensor spec TestModuleHookApi ShardedTensorTestBase DummyNNModule torch nn Module __init__ spec tensor_size super __init__ st = sharded_tensor rand spec tensor_size forward st with_comms init_rpc=False skip_if_lt_x_gpu requires_nccl test_reshard_output specs = _chunk_sharding_specs_list_for_test seed= spec reshard_spec = specs specs test_module = DummyNNModule spec st = test_module local_shard = st local_tensor pg = dist distributed_c d _get_default_group st_compare = ShardedTensor _init_from_local_shards copy deepcopy st local_shards st size process_group=pg st_compare _sharding_spec = copy deepcopy spec st_compare reshard reshard_spec test_module = _reshard_output test_module reshard_spec st = test_module local_shard = st local_tensor local_shard_compare = st_compare local_tensor assertEqual local_shard local_shard_compare assertEqual local_shard size assertEqual local_shard size with_comms init_rpc=False skip_if_lt_x_gpu requires_nccl test_collect_local_shard specs = _chunk_sharding_specs_list_for_test seed= spec = specs test_module = DummyNNModule spec st = test_module local_shard = st local_tensor test_module = _collect_local_shard test_module output = test_module assertTrue isinstance output torch Tensor assertEqual local_shard output TestLocalTensor ShardedTensorTestBase with_comms init_rpc=False skip_if_lt_x_gpu requires_nccl test_local_tensor spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda st = sharded_tensor rand spec local_shard = st local_tensor assertEqual torch Size local_shard size assertEqual st local_tensor local_shard with_comms init_rpc=False skip_if_lt_x_gpu requires_nccl test_local_tensor_error spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda rank cuda rank cuda rank cuda rank cuda rank cuda rank cuda st = sharded_tensor rand spec assertRaisesRegex NotImplementedError Only single local shard supported st local_tensor TestShardedTensorChunked ShardedTensorTestBase with_comms skip_if_lt_x_gpu requires_nccl test_sharded_tensor_metadata spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda st = sharded_tensor empty spec init_rrefs=True st_metadata = st metadata assertEqual torch Size st_metadata size assertEqual torch Size st size assertEqual torch float st dtype assertEqual torch strided st layout assertEqual False st requires_grad assertTrue st is_contiguous assertFalse st is_pinned st = sharded_tensor empty spec requires_grad=True init_rrefs=True assertEqual True st requires_grad st = sharded_tensor empty spec dtype=torch double init_rrefs=True assertEqual torch double st dtype Need CPU pin_memory spec = ChunkShardingSpec dim= placements= rank cpu rank cpu rank cpu rank cpu st = sharded_tensor empty spec pin_memory=True init_rrefs=True assertEqual True st is_pinned test read only properties they re read only we can t simply change global metadata without changing underlying shard s properties assertRaisesRegex RuntimeError torch function __set__ st requires_grad = True skipIfRocm with_comms skip_if_lt_x_gpu requires_nccl test_complete_world_size dim - spec = ChunkShardingSpec dim=dim placements= rank cuda rank cuda rank cuda rank cuda st = sharded_tensor empty spec init_rrefs=True Validate local shard local_shards = st local_shards assertEqual len local_shards local_shard = local_shards tensor assertEqual torch device f cuda rank local_shard device rank == assertEqual local_shard size assertEqual local_shard size Validate global metadata st_metadata = st metadata shards_metadata = st_metadata shards_metadata assertEqual len shards_metadata rank shard_metadata enumerate shards_metadata assertEqual rank shard_metadata shard_offsets rank == assertEqual shard_metadata shard_sizes assertEqual shard_metadata shard_sizes assertEqual f rank rank cuda rank str shard_metadata placement Validate remote shards remote_shards = st remote_shards assertEqual len remote_shards rpc_rank shards remote_shards items assertEqual len shards remote_shard shards assertEqual rpc_rank remote_shard owner id shard = remote_shard to_here assertEqual f rank rpc_rank cuda rpc_rank str shard metadata placement rpc_rank == assertEqual shard tensor size assertEqual shard tensor size with_comms skip_if_lt_x_gpu requires_nccl test_create_sharded_tensor_with_ones Test sharded_tensor ones spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda h w = st = sharded_tensor ones spec h w Validate local shard initialized torch ones local_shards = st local_shards assertEqual len local_shards local_shard = local_shards tensor assertEqual torch device f cuda rank local_shard device The split rank = ceil h = rank= expected_h = rank == math ceil h assertEqual expected_h w local_shard size assertEqual local_shard torch ones expected_h w with_comms skip_if_lt_x_gpu requires_nccl test_gather_even - None Test _sharded_tensor gather evenly distributed _shards spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda h w = st = sharded_tensor ones spec h w full_tensor = None dst = rank == dst full_tensor = torch zeros h w device=torch device f cuda dst st gather dst full_tensor rank == dst assertEqual full_tensor torch ones h w assertIsNone full_tensor with_comms skip_if_lt_x_gpu requires_nccl test_gather_uneven - None Test _sharded_tensor gather unevenly distributed _shards spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda rank cuda h w = st = sharded_tensor ones spec h w full_tensor = None dst = rank == dst full_tensor = torch zeros h w device=torch device f cuda dst st gather dst full_tensor rank == dst assertEqual full_tensor torch ones h w assertIsNone full_tensor with_comms skip_if_lt_x_gpu requires_nccl test_create_sharded_tensor_with_zeros Test sharded_tensor zeros spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda h w = st = sharded_tensor zeros spec h w Validate local shard initialized torch zeros local_shards = st local_shards assertEqual len local_shards local_shard = local_shards tensor assertEqual torch device f cuda rank local_shard device The split rank = ceil h = rank= expected_h = rank == math ceil h assertEqual expected_h w local_shard size assertEqual local_shard torch zeros expected_h w with_comms skip_if_lt_x_gpu requires_nccl test_create_sharded_tensor_with_rand Test sharded_tensor rand randn spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda h w = seed = expected_h = expected_device = torch device f cuda rank dtype = torch double torch manual_seed seed Test sharded_tensor rand creation expected = torch rand expected_h w device=expected_device dtype=dtype reset seed ensure same random numbers generated torch manual_seed seed st = sharded_tensor rand spec h w dtype=dtype Validate local shard initialized torch rand local_shards = st local_shards assertEqual len local_shards local_shard = local_shards tensor assertEqual expected_device local_shard device assertEqual expected_h w local_shard size assertEqual expected local_shard Test sharded_tensor randn creation torch manual_seed seed expected_randn = torch randn expected_h w device=expected_device dtype=dtype reset seed ensure same random numbers generated torch manual_seed seed st_randn = sharded_tensor randn spec h w dtype=dtype Validate local shard initialized torch randn local_shards = st_randn local_shards assertEqual len local_shards local_shard = local_shards tensor assertEqual expected_device local_shard device assertEqual expected_h w local_shard size assertEqual expected_randn local_shard with_comms skip_if_lt_x_gpu requires_nccl test_create_sharded_tensor_with_full Test sharded_tensor full spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda h w = fill_value = st = sharded_tensor full spec size= h w fill_value=fill_value dtype=torch int Validate local shard initialized torch full local_shards = st local_shards assertEqual len local_shards local_shard = local_shards tensor assertEqual torch device f cuda rank local_shard device The split rank = ceil h = rank= expected_h = rank == math ceil h assertEqual expected_h w local_shard size assertEqual local_shard torch full size= expected_h w fill_value=fill_value dtype=torch int with_comms skip_if_lt_x_gpu requires_nccl test_create_sharded_tensor_like Test tensor like methods i e torch zeros_like torch full_like etc spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda h w = expected_h = seed = dtype = torch double expected_device = torch device f cuda rank st = sharded_tensor rand spec h w dtype=dtype tensor_like_ops = torch zeros_like torch zeros torch ones_like torch ones torch rand_like torch rand torch randn_like torch randn torch empty_like torch empty torch full_like torch full op expect_local_op tensor_like_ops items op == torch full_like special handle full full_like needs have additional fill_value arg expect_tensor = expect_local_op expected_h w device=expected_device dtype=dtype new_op_st = op st dtype=dtype assertEqual new_op_st local_tensor expect_tensor op == torch empty_like empty empty_like we only compare shape expect_tensor = expect_local_op expected_h w device=expected_device dtype=dtype new_op_st = op st dtype=dtype assertEqual new_op_st local_tensor shape expect_tensor shape torch manual_seed seed expect_tensor = expect_local_op expected_h w device=expected_device dtype=dtype torch manual_seed seed new_op_st = op st dtype=dtype assertEqual new_op_st local_tensor expect_tensor skipIfRocm with_comms skip_if_lt_x_gpu requires_nccl test_partial_world_size spec = ChunkShardingSpec dim= placements= rank cuda rank cuda st = sharded_tensor empty spec init_rrefs=True Validate local shard local_shards = st local_shards rank = assertEqual len local_shards local_shard = local_shards tensor assertEqual torch device f cuda rank local_shard device assertEqual local_shard size assertEqual len local_shards Validate global metadata st_metadata = st metadata shards_metadata = st_metadata shards_metadata assertEqual len shards_metadata shard_rank shard_metadata enumerate shards_metadata assertEqual shard_rank shard_metadata shard_offsets assertEqual shard_metadata shard_sizes assertEqual f rank shard_rank + cuda shard_rank + str shard_metadata placement Validate remote shards remote_shards = st remote_shards rank = assertEqual len remote_shards assertEqual len remote_shards rpc_rank shards remote_shards items assertEqual len shards remote_shard shards assertEqual rpc_rank remote_shard owner id shard = remote_shard to_here assertEqual f rank rpc_rank cuda rpc_rank str shard metadata placement assertEqual shard tensor size skipIfRocm with_comms skip_if_lt_x_gpu requires_nccl test_new_group spec = ChunkShardingSpec dim= placements= rank cuda rank cuda pg = dist new_group ranks= st = sharded_tensor empty spec process_group=pg init_rrefs=True Validate local shard local_shards = st local_shards rank = assertEqual len local_shards local_shard = local_shards tensor assertEqual torch device f cuda rank local_shard device assertEqual local_shard size assertEqual len local_shards Validate global metadata st_metadata = st metadata shards_metadata = st_metadata shards_metadata assertEqual len shards_metadata shard_rank shard_metadata enumerate shards_metadata assertEqual shard_rank shard_metadata shard_offsets assertEqual shard_metadata shard_sizes assertEqual f rank shard_rank + cuda shard_rank + str shard_metadata placement Validate remote shards remote_shards = st remote_shards rank = assertEqual len remote_shards assertEqual len remote_shards rpc_rank shards remote_shards items assertEqual len shards remote_shard shards shard = remote_shard to_here assertEqual rpc_rank remote_shard owner id assertEqual f rank rpc_rank cuda rpc_rank str shard metadata placement assertEqual shard tensor size skipIfRocm with_comms skip_if_lt_x_gpu requires_nccl test_multiple_local_shards spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda rank cuda rank cuda rank cuda rank cuda st = sharded_tensor empty spec init_rrefs=True Validate local shards local_shards = st local_shards assertEqual len local_shards local_shard local_shards assertEqual torch device f cuda rank local_shard tensor device assertEqual local_shard tensor size Validate global metadata st_metadata = st metadata shards_metadata = st_metadata shards_metadata assertEqual len shards_metadata shard_idx shard_metadata enumerate shards_metadata assertEqual shard_idx shard_metadata shard_offsets assertEqual shard_metadata shard_sizes assertEqual f rank shard_idx cuda shard_idx str shard_metadata placement Validate remote shards remote_shards = st remote_shards assertEqual len remote_shards rpc_rank shards remote_shards items assertEqual len shards remote_shard shards shard = remote_shard to_here assertEqual shard tensor size assertEqual rpc_rank remote_shard owner id skip_if_lt_x_gpu requires_nccl test_sharding_columns init_pg dim - spec = ChunkShardingSpec dim=dim placements= rank cuda rank cuda rank cuda rank cuda st = sharded_tensor empty spec Validate local shard local_shards = st local_shards assertEqual len local_shards local_shard = local_shards tensor assertEqual torch device f cuda rank local_shard device assertEqual local_shard size Validate global metadata st_metadata = st metadata shards_metadata = st_metadata shards_metadata assertEqual len shards_metadata rank shard_metadata enumerate shards_metadata assertEqual rank shard_metadata shard_offsets assertEqual shard_metadata shard_sizes assertEqual f rank rank cuda rank str shard_metadata placement skip_if_lt_x_gpu requires_nccl test_invalid_sharding init_pg assertRaisesRegex NotImplementedError does support named dimension spec = ChunkShardingSpec dim= H placements= rank cuda sharded_tensor empty spec dim - - - spec = ChunkShardingSpec dim=dim placements= rank cuda assertRaisesRegex ValueError Invalid sharding dim sharded_tensor empty spec spec = ChunkShardingSpec dim= placements= rank cuda assertRaisesRegex ValueError Global rank does exist input process group sharded_tensor empty spec spec = ChunkShardingSpec dim= placements= rank cuda st = sharded_tensor empty spec tensor = torch empty assertRaisesRegex RuntimeError r supported ShardedTensor $ torch add st tensor spec = ChunkShardingSpec dim= placements= rank cuda assertRaisesRegex ValueError Only torch strided layout currently supported sharded_tensor empty spec layout=torch sparse_coo spec = ChunkShardingSpec dim= placements= rank cuda assertRaisesRegex ValueError Only torch contiguous_format memory_format currently supported sharded_tensor empty spec memory_format=torch channels_last spec = ChunkShardingSpec dim= placements= worker cuda assertRaisesRegex RuntimeError RPC framework needs initialized sharded_tensor empty spec spec = ChunkShardingSpec dim= placements= rank cuda assertRaisesRegex RuntimeError RPC Framework needs initialized st = sharded_tensor empty spec init_rrefs=True assertRaisesRegex RuntimeError ShardedTensor created init_rrefs=False st = sharded_tensor empty spec st remote_shards init_rpc spec = ChunkShardingSpec dim= placements= workerfoo cuda assertRaisesRegex ValueError Invalid worker name sharded_tensor empty spec init_rrefs=True skip_if_lt_x_gpu requires_nccl test_invalid_pg_rpc_ranks init_pg Init RPC different ranks rpc_backend_options = rpc TensorPipeRpcBackendOptions _transports=tp_transports rpc_backend_options init_method = f file file_name rank = rank + world_size rpc init_rpc name=f worker rank rank=rank world_size=self world_size rpc_backend_options=rpc_backend_options spec = ChunkShardingSpec dim= placements= rank cuda assertRaisesRegex ValueError Default ProcessGroup RPC ranks must same sharded_tensor empty spec init_rrefs=True skip_if_lt_x_gpu requires_nccl test_insufficient_sharding_dims init_pg spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda st = sharded_tensor empty spec Validate local shard local_shards = st local_shards rank = assertEqual len local_shards local_shard = local_shards tensor assertEqual torch device f cuda rank local_shard device assertEqual local_shard size assertEqual len local_shards local_shard = local_shards tensor assertEqual torch device f cuda rank local_shard device assertEqual local_shard numel Validate global metadata st_metadata = st metadata shards_metadata = st_metadata shards_metadata assertEqual len shards_metadata shard_rank shard_metadata enumerate shards_metadata assertEqual shard_rank shard_metadata shard_offsets assertEqual f rank shard_rank cuda shard_rank str shard_metadata placement shard_rank = assertEqual shard_metadata shard_sizes assertEqual shard_metadata shard_sizes with_comms skip_if_lt_x_gpu requires_nccl test_sharded_tensor_sizes spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda Test args st = sharded_tensor empty spec init_rrefs=True assertEqual torch Size st size Test single args st = sharded_tensor empty spec init_rrefs=True assertEqual torch Size st size Test list st = sharded_tensor empty spec init_rrefs=True assertEqual torch Size st size Test tuple st = sharded_tensor empty spec init_rrefs=True assertEqual torch Size st size Test row size st = sharded_tensor empty spec init_rrefs=True assertEqual st size Test col size st = sharded_tensor empty spec init_rrefs=True assertEqual st size Test negative indexed size st = sharded_tensor empty spec init_rrefs=True assertEqual st size - Test dim ndim assertEqual st dim assertEqual st ndim Test invalid input st = sharded_tensor empty spec init_rrefs=True assertRaisesRegex IndexError Dimension out range st size - assertRaisesRegex IndexError Dimension out range st size assertRaises TypeError st = sharded_tensor empty spec foo with_comms skip_if_lt_x_gpu requires_nccl test_state_dict spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda m = MyShardedModel spec Test save m _register_state_dict_hook state_dict_hook buffer = io BytesIO mod_state_dict = m state_dict mod_state_keys = mod_state_dict keys assertTrue sharded_tensor mod_state_keys assertTrue submodule sharded_tensor mod_state_keys torch save mod_state_dict buffer Test load module_load = MyShardedModel module_load _register_load_state_dict_pre_hook pre_load_state_dict_hook True buffer seek weights_only=False ShardedTensor weights_only already tested TestFSDPStateDict test_torch_save_load state_dict_deser = torch load buffer weights_only=False module_load load_state_dict state_dict_deser strict=False module_load _register_state_dict_hook state_dict_hook loaded_dict_keys = module_load state_dict keys assertTrue sharded_tensor loaded_dict_keys assertTrue submodule sharded_tensor loaded_dict_keys Verify after load assertTrue torch equal m sharded_tensor module_load sharded_tensor assertTrue torch equal m submodule sharded_tensor module_load submodule sharded_tensor with_comms skip_if_lt_x_gpu requires_nccl test_state_dict_new_group spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda pg = dist new_group m = MyShardedModel spec pg Test save m _register_state_dict_hook state_dict_hook buffer = io BytesIO torch save m state_dict buffer Test load module_load = MyShardedModel spec=None group=pg module_load _register_load_state_dict_pre_hook pre_load_state_dict_hook True buffer seek load_with_process_group pg ShardedTensor weights_only already tested TestFSDPStateDict test_torch_save_load state_dict_deser = torch load buffer weights_only=False module_load load_state_dict state_dict_deser strict=False Verify after load assertTrue torch equal m sharded_tensor module_load sharded_tensor assertTrue torch equal m submodule sharded_tensor module_load submodule sharded_tensor with_comms skip_if_lt_x_gpu requires_nccl test_state_dict_no_sharded_tensors Verify hooks don t affect modules no ShardedTensors m = torch nn Linear Test save state_dict_before = m state_dict m _register_state_dict_hook state_dict_hook buffer = io BytesIO torch save m state_dict buffer assertEqual state_dict_before m state_dict Test load module_load = torch nn Linear module_load _register_load_state_dict_pre_hook pre_load_state_dict_hook True buffer seek state_dict_deser = torch load buffer module_load load_state_dict state_dict_deser strict=False Verify after load assertEqual m weight module_load weight assertEqual m bias module_load bias skip_if_lt_x_gpu requires_nccl test_load_state_dict_errors init_rpc dist init_process_group backend= nccl world_size=self world_size rank=self rank init_method=f file file_name spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda m = MyShardedModel spec Test save m _register_state_dict_hook state_dict_hook buffer = io BytesIO torch save m state_dict buffer pg = dist new_group ranks= buffer seek rank = assertRaisesRegex RuntimeError Local rank save time load_with_process_group pg ShardedTensor weights_only already tested TestFSDPStateDict test_torch_save_load torch load buffer weights_only=False assertRaisesRegex RuntimeError Local world size save time load_with_process_group pg ShardedTensor weights_only already tested TestFSDPStateDict test_torch_save_load torch load buffer weights_only=False dist destroy_process_group buffer seek assertRaisesRegex RuntimeError Need initialize default process group ShardedTensor weights_only already tested TestFSDPStateDict test_torch_save_load torch load buffer weights_only=False rpc shutdown with_comms skip_if_lt_x_gpu requires_nccl test_cleanup create_tensors spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda sharded_tensor empty spec init_rrefs=True sharded_tensor empty spec create_tensors assertEqual len sharded_tensor api _sharded_tensor_map TestShardedTensorEnumerable ShardedTensorTestBase with_comms skip_if_lt_x_gpu requires_nccl test_sharded_tensor_metadata spec = EnumerableShardingSpec ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda st = sharded_tensor empty spec init_rrefs=True st_metadata = st metadata assertEqual torch Size st_metadata size assertEqual torch float st dtype assertEqual torch strided st layout assertEqual False st requires_grad assertTrue st is_contiguous assertFalse st is_pinned st = sharded_tensor empty spec requires_grad=True init_rrefs=True assertEqual True st requires_grad st = sharded_tensor empty spec dtype=torch double init_rrefs=True assertEqual torch double st dtype Need CPU pin_memory spec = EnumerableShardingSpec ShardMetadata shard_offsets= shard_sizes= placement= rank cpu ShardMetadata shard_offsets= shard_sizes= placement= rank cpu ShardMetadata shard_offsets= shard_sizes= placement= rank cpu ShardMetadata shard_offsets= shard_sizes= placement= rank cpu st = sharded_tensor empty spec pin_memory=True init_rrefs=True assertTrue st is_pinned skipIfRocm with_comms skip_if_lt_x_gpu requires_nccl test_grid_sharding spec = EnumerableShardingSpec ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda st = sharded_tensor empty spec init_rrefs=True assertEqual st size assertEqual len st local_shards Verify local shard local_shard = st local_shards assertEqual torch device f cuda rank local_shard tensor device assertEqual local_shard tensor size Verify local shard metadata assertEqual rank rank local_shard metadata shard_offsets assertEqual local_shard metadata shard_sizes assertEqual f rank rank cuda rank str local_shard metadata placement Verify global metadata st_metadata = st metadata shards_metadata = st_metadata shards_metadata assertEqual len shards_metadata rank shard_metadata enumerate shards_metadata assertEqual rank rank shard_metadata shard_offsets assertEqual shard_metadata shard_sizes assertEqual f rank rank cuda rank str shard_metadata placement Validate remote shards remote_shards = st remote_shards assertEqual len remote_shards rpc_rank shards remote_shards items assertEqual len shards remote_shard shards assertEqual rpc_rank remote_shard owner id shard = remote_shard to_here assertEqual shard tensor size with_comms skip_if_lt_x_gpu requires_nccl test_create_sharded_tensor_with_ones Test sharded_tensor ones spec = EnumerableShardingSpec ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda st = sharded_tensor ones spec init_rrefs=True assertEqual st size assertEqual len st local_shards Verify local shard initialized torch ones local_shard = st local_shards assertEqual torch device f cuda rank local_shard tensor device assertEqual local_shard tensor size assertEqual local_shard tensor torch ones with_comms skip_if_lt_x_gpu requires_nccl test_gather_even - None Test _sharded_tensor gather evenly distributed _shards spec = EnumerableShardingSpec ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda h w = st = sharded_tensor ones spec h w init_rrefs=True full_tensor = None dst = rank == dst full_tensor = torch zeros h w device=torch device f cuda dst st gather dst full_tensor rank == dst assertEqual full_tensor torch ones h w assertIsNone full_tensor with_comms skip_if_lt_x_gpu requires_nccl test_gather_uneven - None Test _sharded_tensor gather unevenly distributed _shards spec = EnumerableShardingSpec ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda h w = st = sharded_tensor ones spec h w init_rrefs=True full_tensor = None dst = rank == dst full_tensor = torch zeros h w device=torch device f cuda dst st gather dst full_tensor rank == dst assertEqual full_tensor torch ones h w assertIsNone full_tensor with_comms skip_if_lt_x_gpu requires_nccl test_sharded_tensor_to_cpu cpu_spec = ChunkShardingSpec dim= placements= rank cpu rank cpu rank cpu rank cpu spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda h w = gloo_pg = dist new_group backend= gloo CPU sharded tensor should same instance no copy st_cpu = sharded_tensor zeros cpu_spec h w process_group=gloo_pg new_st_cpu = st_cpu cpu assertTrue st_cpu new_st_cpu GPU sharded tensor cpu st = sharded_tensor zeros spec h w test ability move st CPU spec_before_move = st sharding_spec new_st = st cpu process_group=gloo_pg copy original st assertFalse st new_st check spec still ChunkShardingSpec spec_after_move = new_st sharding_spec assertIsInstance spec_after_move ChunkShardingSpec assertIsInstance new_st _process_group distributed_c d ProcessGroup test specs before after move almost same except placement device assertEqual spec_before_move dim spec_after_move dim assertEqual len spec_before_move placements len spec_after_move placements i remote_device_after enumerate spec_after_move placements remote_device_before = spec_before_move placements i assertEqual remote_device_before rank remote_device_after rank assertEqual str remote_device_after device cpu ensure metadata also get changed CPU metas = new_st metadata shards_metadata meta metas assertEqual str meta placement device cpu Test mixed sharded tensor ShardedTensor different devices cpu mixed_spec = ChunkShardingSpec dim= placements= rank cpu rank cpu rank cuda rank cuda st = sharded_tensor zeros mixed_spec h w process_group=gloo_pg new_st = st cpu copy original st assertFalse st new_st check spec still ChunkShardingSpec spec_after_move = new_st sharding_spec assertIsInstance spec_after_move ChunkShardingSpec test specs before after move almost same except placement device assertEqual mixed_spec dim spec_after_move dim assertEqual len mixed_spec placements len spec_after_move placements i remote_device_after enumerate spec_after_move placements remote_device_before = mixed_spec placements i assertEqual remote_device_before rank remote_device_after rank assertEqual str remote_device_after device cpu ensure metadata also get changed CPU metas = new_st metadata shards_metadata meta metas assertEqual str meta placement device cpu with_comms skip_if_lt_x_gpu requires_nccl test_sharded_tensor_to_cuda cpu_spec = ChunkShardingSpec dim= placements= rank cpu rank cpu rank cpu rank cpu spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda h w = CUDA sharded tensor should new ShardedTensor same local shards no movements st_cuda = sharded_tensor zeros spec h w new_st_cuda = st_cuda cuda assertTrue st_cuda new_st_cuda assertTrue st_cuda local_tensor new_st_cuda local_tensor gloo_pg = dist new_group backend= gloo CPU sharded tensor GPU st_cpu = sharded_tensor zeros cpu_spec h w process_group=gloo_pg test ability move st GPU spec_before_move = st_cpu sharding_spec new_st_gpu = st_cpu cuda check spec still ChunkShardingSpec spec_after_move = new_st_gpu sharding_spec assertIsInstance spec_after_move ChunkShardingSpec test specs before after move almost same except placement device assertEqual spec_before_move dim spec_after_move dim assertEqual len spec_before_move placements len spec_after_move placements i remote_device_after enumerate spec_after_move placements remote_device_before = spec_before_move placements i assertEqual remote_device_before rank remote_device_after rank assertEqual str remote_device_before device type cpu assertEqual str remote_device_after device type cuda ensure metadata also get changed GPU metas = new_st_gpu metadata shards_metadata meta metas assertEqual str meta placement device type cuda with_comms skip_if_lt_x_gpu requires_nccl test_sharded_tensor_to_test spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda h w = CUDA sharded tensor should new ShardedTensor same local shards no movements st = sharded_tensor zeros spec h w test same dtype device itself st_self = st dtype=st dtype device= cuda assertTrue st_self st test dtype st_ = st torch float assertFalse st_ st assertEqual st_ dtype torch float test device st_cpu = st device=torch device cpu assertFalse st_cpu st assertEqual st_cpu local_tensor device type cpu st_cuda = st_cpu device=torch device cuda assertEqual st_cuda local_tensor device type cuda non-kwarg device st_cuda = st_cpu torch device cuda assertEqual st_cuda local_tensor device type cuda st_cpu = st_cuda torch device cpu assertEqual st_cpu local_tensor device type cpu string like device conversion st_cpu = st_cuda cpu assertEqual st_cpu local_tensor device type cpu st_cuda = st_cpu cuda assertEqual st_cuda local_tensor device type cuda int like device conversion st_cpu = st_cuda cpu assertEqual st_cpu local_tensor device type cpu st_cuda = st_cpu rank assertEqual st_cuda local_tensor device type cuda test tensor cuda_tensor = torch randn dtype=torch float device= cuda st_cuda = st cuda_tensor assertFalse st_cuda st assertEqual st_cuda dtype torch float cuda_tensor = torch randn dtype=torch float device= cuda st_cuda = st cuda_tensor assertEqual st_cuda dtype torch float test dtype device together st_cpu_ = st cpu torch float assertEqual st_cpu_ dtype torch float assertEqual st_cpu_ local_tensor device type cpu st_cuda_ = st_cpu_ cuda torch float assertEqual st_cuda_ dtype torch float assertEqual st_cuda_ local_tensor device type cuda test pass additional process group gloo_pg = dist new_group backend= gloo st_gloo = st device= cpu process_group=gloo_pg assertFalse st_gloo st assertEqual st_gloo local_tensor device type cpu assertEqual st_gloo _process_group gloo_pg with_comms skip_if_lt_x_gpu requires_nccl test_sharded_tensor_device spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda h w = CUDA sharded tensor should new ShardedTensor same local shards no movements st = sharded_tensor zeros spec h w current_device = torch device torch cuda current_device assertEqual current_device st device test after cpu device get changed cpu_device = torch device cpu st_cpu = st device=cpu_device assertEqual st_cpu device cpu_device skip_if_lt_x_gpu requires_nccl test_uneven_shards init_pg spec = EnumerableShardingSpec ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda st = sharded_tensor empty spec assertEqual st size assertEqual len st local_shards verify_size rank tensor_dims rank == assertEqual tensor_dims rank == assertEqual tensor_dims rank == assertEqual tensor_dims rank == assertEqual tensor_dims verify_offsets rank offsets rank == assertEqual offsets rank == assertEqual offsets rank == assertEqual offsets rank == assertEqual offsets Verify local shard local_shard = st local_shards assertEqual torch device f cuda rank local_shard tensor device verify_size rank local_shard tensor size Verify local shard metadata verify_offsets rank local_shard metadata shard_offsets verify_size rank local_shard metadata shard_sizes assertEqual f rank rank cuda rank str local_shard metadata placement Verify global metadata st_metadata = st metadata shards_metadata = st_metadata shards_metadata assertEqual len shards_metadata rank shard_metadata enumerate shards_metadata verify_offsets rank shard_metadata shard_offsets verify_size rank shard_metadata shard_sizes assertEqual f rank rank cuda rank str shard_metadata placement skipIfRocm with_comms skip_if_lt_x_gpu requires_nccl test_partial_world_size spec = EnumerableShardingSpec ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda st = sharded_tensor empty spec init_rrefs=True assertEqual st size rank = assertEqual len st local_shards assertEqual len st local_shards rank = Verify local shard local_shard = st local_shards assertEqual torch device f cuda rank local_shard tensor device assertEqual local_shard tensor size Verify local shard metadata assertEqual rank local_shard metadata shard_offsets assertEqual local_shard metadata shard_sizes assertEqual f rank rank cuda rank str local_shard metadata placement Verify global metadata st_metadata = st metadata shards_metadata = st_metadata shards_metadata assertEqual len shards_metadata rank shard_metadata enumerate shards_metadata assertEqual rank shard_metadata shard_offsets assertEqual shard_metadata shard_sizes assertEqual f rank rank cuda rank str shard_metadata placement Validate remote shards remote_shards = st remote_shards rank = assertEqual len remote_shards assertEqual len remote_shards rpc_rank shards remote_shards items assertEqual len shards remote_shard shards assertEqual rpc_rank remote_shard owner id shard = remote_shard to_here assertEqual shard tensor size skipIfRocm with_comms skip_if_lt_x_gpu requires_nccl test_new_group spec = EnumerableShardingSpec ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda pg = dist new_group ranks= st = sharded_tensor empty spec process_group=pg init_rrefs=True assertEqual st size rank == rank == Verify local shard local_shard = st local_shards assertEqual torch device f cuda rank local_shard tensor device assertEqual local_shard tensor size Verify local shard metadata assertEqual rank local_shard metadata shard_offsets assertEqual local_shard metadata shard_sizes assertEqual f rank rank cuda rank str local_shard metadata placement Verify global metadata st_metadata = st metadata shards_metadata = st_metadata shards_metadata assertEqual len shards_metadata rank shard_metadata enumerate shards_metadata assertEqual rank shard_metadata shard_offsets assertEqual shard_metadata shard_sizes assertEqual f rank rank + cuda rank + str shard_metadata placement Validate remote shards remote_shards = st remote_shards rank == rank == assertEqual len remote_shards assertEqual len remote_shards rpc_rank shards remote_shards items assertEqual len shards remote_shard shards assertEqual rpc_rank remote_shard owner id shard = remote_shard to_here assertEqual shard tensor size skipIfRocm with_comms skip_if_lt_x_gpu requires_nccl test_multiple_local_shards spec = EnumerableShardingSpec ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda st = sharded_tensor empty spec init_rrefs=True assertEqual st size rank = assertEqual len st local_shards Verify local shards idx local_shard enumerate st local_shards assertEqual torch device f cuda rank local_shard tensor device assertEqual local_shard tensor size Verify local shard metadata assertEqual idx rank local_shard metadata shard_offsets assertEqual local_shard metadata shard_sizes assertEqual f rank rank cuda rank str local_shard metadata placement assertEqual len st local_shards Verify global metadata st_metadata = st metadata shards_metadata = st_metadata shards_metadata assertEqual len shards_metadata shard_rank shard_metadata enumerate shards_metadata assertEqual shard_rank shard_rank shard_metadata shard_offsets assertEqual shard_metadata shard_sizes assertEqual f rank shard_rank cuda shard_rank str shard_metadata placement Validate remote shards remote_shards = st remote_shards rank = assertEqual len remote_shards assertEqual len remote_shards rpc_rank shards remote_shards items assertEqual len shards remote_shard shards assertEqual rpc_rank remote_shard owner id shard = remote_shard to_here assertEqual shard tensor size skipIfRocm with_comms skip_if_lt_x_gpu requires_nccl test_with_rpc_names spec = EnumerableShardingSpec ShardMetadata shard_offsets= shard_sizes= placement= worker cuda ShardMetadata shard_offsets= shard_sizes= placement= worker cuda ShardMetadata shard_offsets= shard_sizes= placement= worker cuda ShardMetadata shard_offsets= shard_sizes= placement= worker cuda st = sharded_tensor empty spec init_rrefs=True assertEqual st size assertEqual len st local_shards Verify local shard local_shard = st local_shards assertEqual torch device f cuda rank local_shard tensor device assertEqual local_shard tensor size Verify local shard metadata assertEqual rank rank local_shard metadata shard_offsets assertEqual local_shard metadata shard_sizes assertEqual f worker rank cuda rank str local_shard metadata placement Verify global metadata st_metadata = st metadata shards_metadata = st_metadata shards_metadata assertEqual len shards_metadata rank shard_metadata enumerate shards_metadata assertEqual rank rank shard_metadata shard_offsets assertEqual shard_metadata shard_sizes assertEqual f worker rank cuda rank str shard_metadata placement Validate remote shards remote_shards = st remote_shards assertEqual len remote_shards rpc_rank shards remote_shards items assertEqual len shards remote_shard shards assertEqual rpc_rank remote_shard owner id shard = remote_shard to_here assertEqual shard tensor size TestShardedTensorFromLocalTensor ShardedTensorTestBase _generate_st_from_chunk_local_tensor st_size sharding_spec tensor_meta = sharding_spec build_metadata st_size TensorProperties pg = dist distributed_c d _get_default_group local_tensor = None local_shard_metadata = None rank_to_metadata = shard_metadata tensor_meta shards_metadata rank device = _parse_and_validate_remote_device pg shard_metadata placement rank_to_metadata rank = shard_metadata rank == rank local_tensor = torch rand shard_metadata shard_sizes cuda device local_shard_metadata = shard_metadata TODO figure out what API should behave when some rank have no shard see https github com pytorch pytorch issues assert local_tensor None st = ShardedTensor _init_from_local_tensor local_tensor sharding_spec st_size init_rrefs=True assertEqual tuple st_size st size assertEqual len st local_shards Verify local shard local_shard = st local_shards assertEqual st local_tensor local_tensor assertEqual torch device f cuda rank local_shard tensor device Verify local shard metadata assertEqual local_shard_metadata shard_offsets local_shard metadata shard_offsets assertEqual local_shard_metadata shard_sizes local_shard metadata shard_sizes assertEqual local_shard_metadata placement local_shard metadata placement Verify global metadata st_shards_metadata = st metadata shards_metadata assertEqual world_size len st_shards_metadata assertEqual tensor_meta shards_metadata st_shards_metadata Validate remote shards remote_shards = st remote_shards assertEqual world_size - len remote_shards rpc_rank shards remote_shards items assertEqual len shards remote_shard shards assertEqual rpc_rank remote_shard owner id If remote shard does exist to_here will throw exception tensor_meta shards_metadata rpc_rank shard = remote_shard to_here assertEqual rank_to_metadata rpc_rank shard_sizes shard tensor size skipIfRocm with_comms skip_if_lt_x_gpu requires_nccl test_init_from_local_tensor chunk_specs = _chunk_sharding_specs_list_for_test seed= spec chunk_specs _generate_st_from_chunk_local_tensor spec _generate_st_from_chunk_local_tensor spec _generate_st_from_chunk_local_tensor spec _generate_st_from_chunk_local_tensor spec with_comms skip_if_lt_x_gpu requires_nccl test_init_from_local_tensor_errors enumerable_sharding_spec = EnumerableShardingSpec ShardMetadata shard_offsets= shard_sizes= placement= rank cuda ShardMetadata shard_offsets= shard_sizes= placement= rank cuda st_size = local_tensor = torch rand st_size cuda rank assertRaisesRegex ValueError do cover entire tensor ShardedTensor _init_from_local_tensor local_tensor enumerable_sharding_spec st_size chunk_specs = _chunk_sharding_specs_list_for_test seed= assertRaisesRegex ValueError local_tensor contiguous Tensor ShardedTensor _init_from_local_tensor local_tensor t chunk_specs st_size TestShardedTensorFromLocalShards ShardedTensorTestBase with_comms init_rpc=False skip_if_lt_x_gpu requires_nccl test_local_shards shard_offsets = rank rank local_shard_metadata = ShardMetadata shard_offsets=shard_offsets shard_sizes= placement=f rank rank cuda rank local_tensor = torch randn device=f cuda rank local_shard = sharded_tensor Shard local_tensor local_shard_metadata local_shard_from_offsets = sharded_tensor Shard from_tensor_and_offsets local_tensor shard_offsets=shard_offsets rank=self rank assertEqual local_shard metadata local_shard_from_offsets metadata wrong_local_shard_metadata = ShardMetadata shard_offsets=shard_offsets shard_sizes= placement=f rank rank cuda rank assertRaisesRegex ValueError Shard tensor size does match sharded_tensor Shard local_tensor metadata=wrong_local_shard_metadata skipIfRocm with_comms skip_if_lt_x_gpu requires_nccl test_init_from_local_shards local_shard_metadata = ShardMetadata shard_offsets= rank rank shard_sizes= placement=f rank rank cuda rank local_shards = sharded_tensor Shard torch randn device=f cuda rank local_shard_metadata st = sharded_tensor init_from_local_shards local_shards init_rrefs=True assertEqual st size assertEqual len st local_shards Verify local shard local_shard = st local_shards assertEqual torch device f cuda rank local_shard tensor device assertEqual local_shard tensor size Verify local shard metadata assertEqual rank rank local_shard metadata shard_offsets assertEqual local_shard metadata shard_sizes assertEqual f rank rank cuda rank str local_shard metadata placement Verify global metadata shards_metadata = st metadata shards_metadata assertEqual len shards_metadata rank shard_metadata enumerate shards_metadata assertEqual rank rank shard_metadata shard_offsets assertEqual shard_metadata shard_sizes assertEqual f rank rank cuda rank str shard_metadata placement Validate remote shards remote_shards = st remote_shards assertEqual len remote_shards rpc_rank shards remote_shards items assertEqual len shards remote_shard shards assertEqual rpc_rank remote_shard owner id shard = remote_shard to_here assertEqual shard tensor size skipIfRocm with_comms init_rpc=False skip_if_lt_x_gpu requires_nccl test_recalc_for_metadata shard_sizes = test different shard sizes shard_size shard_sizes local_shard_metadata = ShardMetadata shard_offsets= shard_sizes= shard_size shard_size placement=f rank rank cuda rank local_shards = sharded_tensor Shard torch randn shard_size shard_size device=f cuda rank local_shard_metadata st = sharded_tensor init_from_local_shards local_shards None None assertEqual shard_size shard_size st size assertEqual len st local_shards Verify local shard local_shard = st local_shards assertEqual torch device f cuda rank local_shard tensor device assertEqual shard_size shard_size local_shard tensor size Verify local shard metadata assertEqual rank shard_size local_shard metadata shard_offsets assertEqual shard_size shard_size local_shard metadata shard_sizes assertEqual f rank rank cuda rank str local_shard metadata placement Verify global metadata shards_metadata = st metadata shards_metadata assertEqual len shards_metadata rank shard_metadata enumerate shards_metadata assertEqual rank shard_size shard_metadata shard_offsets assertEqual shard_size shard_size shard_metadata shard_sizes assertEqual f rank rank cuda rank str shard_metadata placement assertRaises ValueError st = sharded_tensor init_from_local_shards local_shards skipIfRocm with_comms init_rpc=False skip_if_lt_x_gpu requires_nccl test_init_from_local_shards_with_different_glb_size wrong_offset_local_shard_metadata = ShardMetadata shard_offsets= shard_sizes= placement=f rank rank cuda rank wrong_offset_local_shards = sharded_tensor Shard torch randn device=f cuda rank wrong_offset_local_shard_metadata assertRaises ValueError sharded_tensor init_from_local_shards wrong_offset_local_shards local_shard_metadata = ShardMetadata shard_offsets= rank shard_sizes= placement=f rank rank cuda rank local_shards = sharded_tensor Shard torch randn device=f cuda rank local_shard_metadata assertRaises ValueError sharded_tensor init_from_local_shards local_shards skipIfRocm with_comms init_rpc=False skip_if_lt_x_gpu requires_nccl test_non_rw_sharded_recalc_for_metadata local_shard_metadata = ShardMetadata shard_offsets= rank rank shard_sizes= placement=f rank rank cuda rank local_shards = sharded_tensor Shard torch randn device=f cuda rank local_shard_metadata st = sharded_tensor init_from_local_shards local_shards None rank == assertEqual st local_shards metadata shard_offsets local_shard_metadata shard_offsets assertNotEqual st local_shards metadata shard_offsets local_shard_metadata shard_offsets assertEqual st local_shards metadata shard_sizes local_shard_metadata shard_sizes assertEqual st local_shards metadata placement local_shard_metadata placement skip_if_lt_x_gpu test_st_base_init_from_local_shards_and_global_metadata world_size = shards_metadata = shards = rank range world_size local_shard_metadata = ShardMetadata shard_offsets= rank rank shard_sizes= placement=f rank rank cuda rank shards_metadata append local_shard_metadata shards append sharded_tensor Shard torch randn device=f cuda rank local_shard_metadata tensor_properties = TensorProperties dtype=torch get_default_dtype layout=torch strided requires_grad=False memory_format=torch contiguous_format pin_memory=False sharded_tensor_metadata = sharded_tensor ShardedTensorMetadata shards_metadata=shards_metadata size=torch Size tensor_properties=tensor_properties st_base = sharded_tensor ShardedTensorBase _init_from_local_shards_and_global_metadata shards sharded_tensor_metadata=sharded_tensor_metadata assertEqual len st_base local_shards Verify local shard st_base local_shard = st_base local_shards assertEqual torch device cuda local_shard tensor device assertEqual local_shard tensor size Verify local shard metadata assertEqual local_shard metadata shard_offsets assertEqual local_shard metadata shard_sizes assertEqual rank cuda str local_shard metadata placement Verify global metadata shards_metadata = st_base metadata shards_metadata assertEqual len shards_metadata rank shard_metadata enumerate shards_metadata assertEqual rank rank shard_metadata shard_offsets assertEqual shard_metadata shard_sizes assertEqual f rank rank cuda rank str shard_metadata placement skipIfRocm with_comms init_rpc=False skip_if_lt_x_gpu requires_nccl test_init_from_local_shards_and_global_metadata_with_all_zeros local_shard_metadata = ShardMetadata shard_offsets= shard_sizes= placement=f rank rank cuda rank shards_metadata = r range world_size r == rank shards_metadata append local_shard_metadata shards_metadata append ShardMetadata shard_offsets= shard_sizes= placement=f rank r cuda r local_shards = sharded_tensor Shard torch randn device=f cuda rank local_shard_metadata tensor_properties = TensorProperties dtype=torch get_default_dtype layout=torch strided requires_grad=False memory_format=torch contiguous_format pin_memory=False sharded_tensor_metadata = sharded_tensor ShardedTensorMetadata shards_metadata=shards_metadata size=torch Size tensor_properties=tensor_properties st = ShardedTensor _init_from_local_shards_and_global_metadata local_shards sharded_tensor_metadata assertEqual st size assertEqual len st local_shards Verify local shard local_shard = st local_shards assertEqual torch device f cuda rank local_shard tensor device assertEqual local_shard tensor size Verify local shard metadata assertEqual local_shard metadata shard_offsets assertEqual local_shard metadata shard_sizes assertEqual f rank rank cuda rank str local_shard metadata placement Verify global metadata shards_metadata = st metadata shards_metadata assertEqual len shards_metadata rank shard_metadata enumerate shards_metadata assertEqual shard_metadata shard_offsets assertEqual shard_metadata shard_sizes assertEqual f rank rank cuda rank str shard_metadata placement skipIfRocm with_comms init_rpc=False skip_if_lt_x_gpu requires_nccl test_init_from_local_shards_and_global_metadata_with_local_view testing cases where we create ST local view meaning we initialize other rank s metadata s shard_offsets = valid invalid shard_offset shard_offsets local_shard_metadata = ShardMetadata shard_offsets= shard_offset shard_sizes= placement=f rank rank cuda rank shards_metadata = r range world_size r == rank shards_metadata append local_shard_metadata shards_metadata append ShardMetadata shard_offsets= r rank shard_sizes= placement=f rank r cuda r local_shards = sharded_tensor Shard torch randn device=f cuda rank local_shard_metadata tensor_properties = TensorProperties dtype=torch get_default_dtype layout=torch strided requires_grad=False memory_format=torch contiguous_format pin_memory=False sharded_tensor_metadata = sharded_tensor ShardedTensorMetadata shards_metadata=shards_metadata size=torch Size tensor_properties=tensor_properties shard_offset == valid case st = ShardedTensor _init_from_local_shards_and_global_metadata local_shards sharded_tensor_metadata invalid case assertRaises ValueError ShardedTensor _init_from_local_shards_and_global_metadata local_shards sharded_tensor_metadata assertEqual st size assertEqual len st local_shards Verify local shard local_shard = st local_shards assertEqual torch device f cuda rank local_shard tensor device assertEqual local_shard tensor size Verify local shard metadata assertEqual local_shard metadata shard_offsets assertEqual local_shard metadata shard_sizes assertEqual f rank rank cuda rank str local_shard metadata placement Verify global metadata shards_metadata = st metadata shards_metadata assertEqual len shards_metadata rank shard_metadata enumerate shards_metadata assertEqual rank = rank shard_metadata shard_offsets rank == rank assertEqual shard_metadata shard_sizes assertEqual shard_metadata shard_sizes assertEqual f rank rank cuda rank str shard_metadata placement skipIfRocm with_comms skip_if_lt_x_gpu requires_nccl test_init_from_local_shards_and_global_metadata local_shard_metadata = ShardMetadata shard_offsets= rank rank shard_sizes= placement=f rank rank cuda rank shards_metadata = r range world_size r == rank shards_metadata append local_shard_metadata shards_metadata append ShardMetadata shard_offsets= r r shard_sizes= placement=f rank r cuda r local_shards = sharded_tensor Shard torch randn device=f cuda rank local_shard_metadata tensor_properties = TensorProperties dtype=torch get_default_dtype layout=torch strided requires_grad=False memory_format=torch contiguous_format pin_memory=False sharded_tensor_metadata = sharded_tensor ShardedTensorMetadata shards_metadata=shards_metadata size=torch Size tensor_properties=tensor_properties st = ShardedTensor _init_from_local_shards_and_global_metadata local_shards sharded_tensor_metadata init_rrefs=True assertEqual st size assertEqual len st local_shards Verify local shard local_shard = st local_shards assertEqual torch device f cuda rank local_shard tensor device assertEqual local_shard tensor size Verify local shard metadata assertEqual rank rank local_shard metadata shard_offsets assertEqual local_shard metadata shard_sizes assertEqual f rank rank cuda rank str local_shard metadata placement Verify global metadata shards_metadata = st metadata shards_metadata assertEqual len shards_metadata rank shard_metadata enumerate shards_metadata assertEqual rank rank shard_metadata shard_offsets assertEqual shard_metadata shard_sizes assertEqual f rank rank cuda rank str shard_metadata placement Validate remote shards remote_shards = st remote_shards assertEqual len remote_shards rpc_rank shards remote_shards items assertEqual len shards remote_shard shards assertEqual rpc_rank remote_shard owner id shard = remote_shard to_here assertEqual shard tensor size with_comms skip_if_lt_x_gpu requires_nccl test_init_from_local_shards_new_group new_pg = dist new_group ranks= rank = local_shard_metadata = ShardMetadata shard_offsets= rank - shard_sizes= placement=f rank rank cuda rank local_shards = sharded_tensor Shard torch randn device=f cuda rank local_shard_metadata st = sharded_tensor init_from_local_shards local_shards process_group=new_pg Verify local shard local_shard = st local_shards assertEqual torch device f cuda rank local_shard tensor device assertEqual local_shard tensor size Verify local shard metadata assertEqual rank - local_shard metadata shard_offsets assertEqual local_shard metadata shard_sizes assertEqual f rank rank cuda rank str local_shard metadata placement Verify global metadata st_metadata = st metadata shards_metadata = st_metadata shards_metadata assertEqual len shards_metadata rank shard_metadata enumerate shards_metadata assertEqual rank shard_metadata shard_offsets assertEqual shard_metadata shard_sizes assertEqual f rank rank + cuda rank + str shard_metadata placement with_comms skip_if_lt_x_gpu requires_nccl test_init_from_local_shards_invalid_local_shards local_shard_metadata = ShardMetadata shard_offsets= rank rank shard_sizes= placement=f rank rank cuda rank indices = values = sparse_tensor = torch sparse_coo_tensor indices values device=f cuda rank empty_local_shards = assertRaisesRegex ValueError have no local shards all ranks sharded_tensor init_from_local_shards empty_local_shards init_rrefs=True wrong_layout_shards = sharded_tensor Shard sparse_tensor local_shard_metadata assertRaisesRegex ValueError Only torch strided layout currently supported sharded_tensor init_from_local_shards wrong_layout_shards init_rrefs=True wrong_memory_format_shards = sharded_tensor Shard torch randn device=f cuda rank t local_shard_metadata assertRaisesRegex ValueError Only torch contiguous_format memory_format currently supported sharded_tensor init_from_local_shards wrong_memory_format_shards init_rrefs=True assertRaisesRegex ValueError Shard tensor size does match sharded_tensor Shard torch randn device=f cuda rank local_shard_metadata assertRaisesRegex ValueError Local shard tensor device does match sharded_tensor Shard torch randn local_shard_metadata with_comms skip_if_lt_x_gpu requires_nccl test_init_from_local_shards_invalid_property_cross_ranks local_shard_metadata = ShardMetadata shard_offsets= rank rank shard_sizes= placement=f rank rank cuda rank tensor_overall_size = rank == wrong_dtype_shards = sharded_tensor Shard torch ones device=f cuda rank local_shard_metadata assertRaisesRegex ValueError ShardedTensor global_size property does match different ranks sharded_tensor init_from_local_shards wrong_dtype_shards tensor_overall_size init_rrefs=True tensor_dtype = torch int rank == torch float wrong_dtype_shards = sharded_tensor Shard torch ones device=f cuda rank dtype=tensor_dtype local_shard_metadata assertRaisesRegex ValueError ShardedTensor dtype property does match different ranks sharded_tensor init_from_local_shards wrong_dtype_shards init_rrefs=True tensor_requires_grad = rank == wrong_requires_grad_shards = sharded_tensor Shard torch randn device=f cuda rank requires_grad=tensor_requires_grad local_shard_metadata assertRaisesRegex ValueError ShardedTensor requires_grad property does match different ranks sharded_tensor init_from_local_shards wrong_requires_grad_shards init_rrefs=True local_shard_metadata = ShardMetadata shard_offsets= rank rank shard_sizes= placement=f rank rank cpu with_comms init_rpc=False backend= gloo skip_if_lt_x_gpu test_init_from_local_shards_invalid_pin_memory pin memory can only dense cpu local_shard_metadata = ShardMetadata shard_offsets= rank rank shard_sizes= placement=f rank rank cpu wrong_pin_memory_local_shards = sharded_tensor Shard torch randn pin_memory=True local_shard_metadata sharded_tensor Shard torch randn pin_memory=False local_shard_metadata assertRaisesRegex ValueError Local shards tensor pin_memory property need same sharded_tensor init_from_local_shards wrong_pin_memory_local_shards init_rrefs=True tensor_pin_memory = rank == wrong_pin_memory_shards_cross_ranks = sharded_tensor Shard torch randn pin_memory=tensor_pin_memory local_shard_metadata assertRaisesRegex ValueError ShardedTensor pin_memory property does match different ranks sharded_tensor init_from_local_shards wrong_pin_memory_shards_cross_ranks init_rrefs=True with_comms skip_if_lt_x_gpu requires_nccl test_init_from_local_shards_invalid_shards_overlap local_shard_size = rank = local_shard_metadata = ShardMetadata shard_offsets= rank rank shard_sizes=local_shard_size placement=f rank rank cuda rank local_shards = sharded_tensor Shard torch randn local_shard_size device=f cuda rank local_shard_metadata assertRaisesRegex ValueError overlap sharded_tensor init_from_local_shards local_shards init_rrefs=True with_comms skip_if_lt_x_gpu requires_nccl test_init_from_local_shards_invalid_shards_gaps local_shard_size = rank = local_shard_metadata = ShardMetadata shard_offsets= rank rank shard_sizes=local_shard_size placement=f rank rank cuda rank local_shards = sharded_tensor Shard torch randn local_shard_size device=f cuda rank local_shard_metadata assertRaisesRegex ValueError does match tensor volume sharded_tensor init_from_local_shards local_shards init_rrefs=True with_comms skip_if_lt_x_gpu requires_nccl test_init_from_local_shards_and_global_metadata_invalid_shards local_shard_metadata = ShardMetadata shard_offsets= rank rank shard_sizes= placement=f rank rank cuda rank shards_metadata = r range world_size r == rank shards_metadata append local_shard_metadata shards_metadata append ShardMetadata shard_offsets= r r shard_sizes= placement=f rank r cuda r tensor_properties = TensorProperties dtype=torch get_default_dtype layout=torch strided requires_grad=False memory_format=torch contiguous_format pin_memory=False sharded_tensor_metadata = sharded_tensor ShardedTensorMetadata shards_metadata=shards_metadata size=torch Size tensor_properties=tensor_properties empty_local_shards = assertRaisesRegex RuntimeError does match number local shards metadata ShardedTensor _init_from_local_shards_and_global_metadata empty_local_shards sharded_tensor_metadata wrong_num_shards = sharded_tensor Shard torch randn device=f cuda rank local_shard_metadata sharded_tensor Shard torch randn device=f cuda rank local_shard_metadata assertRaisesRegex RuntimeError does match number local shards metadata ShardedTensor _init_from_local_shards_and_global_metadata wrong_num_shards sharded_tensor_metadata assertRaisesRegex ValueError Shard tensor size does match metadata shard_lengths sharded_tensor Shard torch randn device=f cuda rank local_shard_metadata assertRaisesRegex ValueError Local shard tensor device does match local Shard s placement sharded_tensor Shard torch randn local_shard_metadata wrong_dtype_shards = sharded_tensor Shard torch ones device=f cuda rank dtype=torch int local_shard_metadata assertRaisesRegex ValueError Local shards tensor dtype property incompatible ShardedTensor _init_from_local_shards_and_global_metadata wrong_dtype_shards sharded_tensor_metadata indices = values = sparse_tensor = torch sparse_coo_tensor indices values device=f cuda rank wrong_layout_shards = sharded_tensor Shard sparse_tensor local_shard_metadata assertRaisesRegex ValueError Local shards tensor layout property incompatible ShardedTensor _init_from_local_shards_and_global_metadata wrong_layout_shards sharded_tensor_metadata wrong_requires_grad_shards = sharded_tensor Shard torch randn device=f cuda rank requires_grad=True local_shard_metadata assertRaisesRegex ValueError Local shards tensor requires_grad property incompatible ShardedTensor _init_from_local_shards_and_global_metadata wrong_requires_grad_shards sharded_tensor_metadata wrong_memory_format_shards = sharded_tensor Shard torch randn device=f cuda rank t local_shard_metadata assertRaisesRegex ValueError Only torch contiguous_format memory_format currently supported ShardedTensor _init_from_local_shards_and_global_metadata wrong_memory_format_shards sharded_tensor_metadata pin_memory can only CPU local_shard_metadata placement = _remote_device f rank rank cpu wrong_pin_memory_shards = sharded_tensor Shard torch randn pin_memory=True local_shard_metadata assertRaisesRegex ValueError Local shards tensor pin_memory property incompatible ShardedTensor _init_from_local_shards_and_global_metadata wrong_pin_memory_shards sharded_tensor_metadata TestShardedTensorCustomOps ShardedTensorTestBase with_comms skip_if_lt_x_gpu requires_nccl test_custom_op custom_sharded_op_impl torch asin my_sharded_asin types args kwargs process_group torch asin args local_shards tensor spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda st = sharded_tensor rand spec res = torch asin st assertEqual res torch asin st local_shards tensor with_comms skip_if_lt_x_gpu requires_nccl test_custom_op_override t = torch rand cuda rank torch distributed _shard sharding_spec api custom_sharding_spec_op custom_sharding_spec_op ChunkShardingSpec torch nn functional linear my_sharded_linear types args kwargs process_group t spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rank cuda rank cuda m = torch nn Linear cuda rank shard_parameter m weight spec result = m torch rand cuda rank assertEqual t result with_comms skip_if_lt_x_gpu requires_nccl test_custom_op_errors assertRaisesRegex TypeError expects signature custom_sharded_op_impl torch nn functional linear my_op types args kwargs process_group random_param pass assertRaisesRegex TypeError expects signature custom_sharded_op_impl torch nn functional linear my_op types pass TestShardMetadata ShardedTensorTestBase with_comms requires_nccl test_shard_metadata_init pg = dist distributed_c d _get_default_group md = ShardMetadata assertIsNone md placement assertRaisesRegex ValueError remote device None _parse_and_validate_remote_device pg md placement String placement gets converted ctor md = ShardMetadata rank cpu assertEqual md placement _remote_device rank cpu rank device = _parse_and_validate_remote_device pg md placement assertEqual rank assertEqual device torch device cpu with_comms requires_nccl test_create_shard_with_no_placement md = ShardMetadata shard = Shard torch zeros md assertIsNone shard metadata placement TestShardedTensorSubGroupInit TestCase spawn_threads_and_init_comms world_size= test_sub_process_group_sharded_tensor_init world_pg = dist GroupMember WORLD rank = dist get_rank sub_group_sz = sub_pg_ranks = r r range r sub_group_sz == rank sub_group_sz sub_pg = dist new_group sub_pg_ranks backend=dist get_backend world_pg use_local_synchronization=True dist barrier sub_pg ShardedTensor _init_from_local_shards Shard tensor=torch tensor device= meta metadata=ShardMetadata shard_offsets= rank sub_group_sz shard_sizes= placement=f rank rank meta process_group=sub_pg spawn_threads_and_init_comms world_size= test_sub_process_group_placement_validation world_pg = dist GroupMember WORLD assertIsNotNone world_pg rank = dist get_rank sub_group_sz = sub_pg_ranks = r r range r sub_group_sz == rank sub_group_sz sub_pg = dist new_group sub_pg_ranks backend=dist get_backend world_pg use_local_synchronization=True dist barrier sub_pg r sub_pg_ranks _parse_and_validate_remote_device sub_pg _remote_device f rank r cuda r sub_group_sz TestCreateTensorNoProcessGroupMode TestCase test_init_from_local_shards_and_global_metadata st_metadata ShardedTensorMetadata = ShardedTensorMetadata shards_metadata= ShardMetadata shard_offsets= shard_sizes= placement= rank cpu ShardMetadata shard_offsets= shard_sizes= placement= rank cpu size=torch Size st_local_shards list Shard = shard_metadata st_metadata shards_metadata st_local_shards append Shard tensor=torch zeros shard_metadata shard_sizes device=shard_metadata placement device metadata=shard_metadata ShardedTensorBase _init_from_local_shards_and_global_metadata local_shards=st_local_shards sharded_tensor_metadata=st_metadata test_non_contiguous_local_shards st_metadata ShardedTensorMetadata = ShardedTensorMetadata shards_metadata= ShardMetadata shard_offsets= shard_sizes= placement= rank cpu ShardMetadata shard_offsets= shard_sizes= placement= rank cpu size=torch Size st_local_shards list Shard = src = torch randn shard_metadata st_metadata shards_metadata offsets = shard_metadata shard_offsets sizes = shard_metadata shard_sizes st_local_shards append Shard tensor=src offsets offsets + sizes offsets offsets + sizes metadata=shard_metadata ShardedTensorBase _init_from_local_shards_and_global_metadata local_shards=st_local_shards sharded_tensor_metadata=st_metadata __name__ == __main__ run_tests