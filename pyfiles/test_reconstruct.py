Owner s module dynamo contextlib dis unittest torch torch _dynamo test_case torch testing _internal common_utils IS_FBCODE torch testing _internal inductor_utils GPU_TYPE requires_triton torch utils _triton has_triton_experimental_host_tma has_triton_tensor_descriptor_host_tma _filter_instructions instructions opname list filter lambda x x opname == opname instructions ReconstructTest torch _dynamo test_case TestCase contextlib contextmanager register_bytecode_hook fn hook code out_code fn list dis get_instructions out_code None torch _dynamo reset handle = torch _dynamo convert_frame register_bytecode_hook hook try yield finally handle remove test_ConstDict_optimize_reconstruct Emit code reconstruct only key changed hook instructions list dis Instruction build_map = _filter_instructions instructions BUILD_MAP assertEqual len build_map reconstruct only d assertEqual build_map argval f d t d = t + t = torch randn d = t d_opt = d copy f d t register_bytecode_hook hook opt_f = torch compile f backend= eager fullgraph=True opt_f d_opt t assertEqual d d_opt test_ConstDict_pop_reconstruct If something pop ed dict we reconstruct everything hook instructions list dis Instruction build_map = _filter_instructions instructions BUILD_MAP assertEqual len build_map reconstruct everything assertEqual build_map argval f d t d pop d = t + t = torch randn d = t t + d_opt = d copy f d t register_bytecode_hook hook opt_f = torch compile f backend= eager fullgraph=True opt_f d_opt t assertEqual d d_opt test_ConstDict_popitem_reconstruct If something pop ed dict we reconstruct everything hook instructions list dis Instruction build_map = _filter_instructions instructions BUILD_MAP assertEqual len build_map reconstruct everything assertEqual build_map argval f d t d popitem t = torch randn d = t t + d_opt = d copy f d t register_bytecode_hook hook opt_f = torch compile f backend= eager fullgraph=True opt_f d_opt t assertEqual d d_opt test_ConstDict_popitem_reconstruct_graph_break If something pop ed dict we reconstruct everything Calling dict popitem will graph break f d t d popitem t = torch randn d = t t + d_opt = d copy f d t opt_f = torch compile backend= eager f opt_f d_opt t assertEqual d d_opt test_ConstDict_del_reconstruct If something deleted dict we reconstruct everything hook instructions list dis Instruction build_map = _filter_instructions instructions BUILD_MAP assertEqual len build_map reconstruct everything assertEqual build_map argval f d t del d d = t + t = torch randn d = t t + d_opt = d copy f d t register_bytecode_hook hook opt_f = torch compile f backend= eager fullgraph=True opt_f d_opt t assertEqual d d_opt test_ConstDict_get_reconstruct dict get shouldn t affect anything hook instructions list dis Instruction build_map = _filter_instructions instructions BUILD_MAP assertEqual len build_map assertEqual build_map argval load_const = _filter_instructions instructions LOAD_CONST assertNotIn load_const f d t d = d get + t t = torch randn d = t t + d_opt = d copy f d t register_bytecode_hook hook opt_f = torch compile f backend= eager fullgraph=True opt_f d_opt t assertEqual d d_opt test_ConstDict_clear_reconstruct If dict clear used we reconstruct everything hook instructions list dis Instruction build_map = _filter_instructions instructions BUILD_MAP assertEqual len build_map reconstruct everything assertEqual build_map argval f d t d clear d = t + t = torch randn d = t t + d_opt = d copy f d t register_bytecode_hook hook opt_f = torch compile f backend= eager fullgraph=True opt_f d_opt t assertEqual d d_opt test_create_dict_reconstruct If dict created inside function everything needs reconstructed hook instructions list dis Instruction build_map = _filter_instructions instructions BUILD_MAP assertEqual len build_map reconstruct everything assertEqual build_map argval f t t t + t = torch randn d = f t register_bytecode_hook hook opt_f = torch compile f backend= eager fullgraph=True d_opt = opt_f t assertEqual d d_opt unittest skipIf IS_FBCODE capturing functional_call enabled default FB_CODE test_functional_call_reconstruct PyTorch shouldn t codegen any key value when functional_call used hook instructions list dis Instruction build_map = _filter_instructions instructions BUILD_MAP don t reconstruct anything assertEqual len build_map m = torch nn Linear new_bias = torch randn new_weight = torch randn fn new_weight new_bias x torch func functional_call m weight new_weight bias new_bias x x = torch randn expected = torch nn functional linear x new_weight new_bias register_bytecode_hook hook opt_fn = torch compile fn backend= eager fullgraph=True got = opt_fn new_weight new_bias x assertEqual expected got unittest skipIf IS_FBCODE capturing functional_call enabled default FB_CODE test_functional_call_reconstruct_ PyTorch shouldn t codegen any key value when functional_call used hook instructions list dis Instruction build_map = _filter_instructions instructions BUILD_MAP don t reconstruct anything assertEqual len build_map DummyModule torch nn Module __init__ super __init__ = torch nn ModuleDict b torch nn ModuleDict c torch nn ModuleDict d torch nn ModuleDict e torch nn Linear bias=False forward x b c d e x model = DummyModule fn model states x torch func functional_call model states x x = torch randn states = model state_dict x = torch randn expected = fn model states x register_bytecode_hook hook opt_fn = torch compile fn backend= eager fullgraph=True got = opt_fn model states x assertEqual expected got test_graph_break_in_wrapped_user_function fn x x = x + torch _dynamo graph_break assert torch compiler is_compiling assert torch is_grad_enabled x + torch compile backend= eager gn x x = torch no_grad fn x reconstruction failure would cause skipped frame assert torch compiler is_compiling assert torch is_grad_enabled x inp = torch randn assertEqual gn inp inp + test_graph_break_in_wrapped_user_method Foo __init__ = b = fn x x = x + torch _dynamo graph_break assert torch compiler is_compiling assert torch is_grad_enabled x + b obj = Foo torch compile backend= eager gn x obj fn = torch no_grad obj fn x = obj fn x reconstruction failure would cause skipped frame assert torch compiler is_compiling assert torch is_grad_enabled x inp = torch randn assertEqual gn inp inp + test_graph_break_in_wrapped_nested_function torch compile backend= eager gn x = b = torch no_grad fn x x = x + torch _dynamo graph_break assert torch compiler is_compiling assert torch is_grad_enabled x + b x = fn x reconstruction failure would cause skipped frame assert torch compiler is_compiling assert torch is_grad_enabled x inp = torch randn assertEqual gn inp inp + test_graph_break_in_wrapped_skipped_function torch _dynamo trace_rules torch _dynamo testing _skipped_function_for_test_reconstruct torch _dynamo variables SkipFunctionVariable assertIs trace_rules lookup _skipped_function_for_test_reconstruct SkipFunctionVariable fn x x = x + torch _dynamo graph_break assert torch compiler is_compiling assert torch is_grad_enabled x + torch compile backend= eager gn x x = torch no_grad _skipped_function_for_test_reconstruct fn x reconstruction failure would cause skipped frame assert torch compiler is_compiling assert torch is_grad_enabled x inp = torch randn assertEqual gn inp inp + requires_triton unittest skipIf has_triton_experimental_host_tma Test requires triton tools experimental_descriptor API test_tma_experimental_reconstruct triton create_tma tensor tma = triton tools experimental_descriptor create_ d_tma_descriptor tensor data_ptr tensor size tensor size tensor element_size tensor + tma x = torch randn device=GPU_TYPE ref = create_tma x res = torch compile create_tma backend= eager x assertEqual ref desc res desc requires_triton unittest skipIf has_triton_tensor_descriptor_host_tma Test requires triton tools tensor_descriptor API test_tma_stable_reconstruct triton create_tma tensor tma = triton tools tensor_descriptor TensorDescriptor from_tensor tensor tensor + tma x = torch randn device=GPU_TYPE ref = create_tma x res = torch compile create_tma backend= eager x assertEqual ref res __name__ == __main__ torch _dynamo test_case run_tests run_tests