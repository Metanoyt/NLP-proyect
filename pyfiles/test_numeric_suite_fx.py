Owner s oncall quantization ruff noqa F copy math operator unittest torch torch nn nn torch nn functional F torch ao quantization default_dynamic_qconfig QConfigMapping get_default_qconfig_mapping torch ao nn quantized nnq toq = torch ops quantized torch ao quantization quantize_fx convert_fx convert_to_reference_fx prepare_fx prepare_qat_fx torch testing _internal common_quantization ConvBnModel ConvBnReLUModel ConvModel QuantizationTestCase skipIfNoFBGEMM skipIfNoQNNPACK withQNNPACKBackend SingleLayerLinearDynamicModel SingleLayerLinearModel LSTMwithHiddenDynamicModel SparseNNModel skip_if_no_torchvision TwoLayerLinearModel torch testing _internal common_utils raise_on_run_directly skipIfTorchDynamo torch ao quantization quantization_mappings get_default_static_quant_module_mappings get_default_dynamic_quant_module_mappings get_default_float_to_quantized_operator_mappings torch testing _internal common_cuda TEST_CUDA torch testing _internal common_quantization NodeSpec ns torch ao quantization fx pattern_utils get_default_quant_patterns torch ao quantization fx quantize_handler qh torch ao ns fx pattern_utils get_type_a_related_to_b torch ao ns fx graph_matcher get_matching_subgraph_pairs GraphMatchingException torch ao ns fx utils compute_sqnr compute_normalized_l _error compute_cosine_similarity torch ao ns fx mappings get_node_type_to_io_type_map get_unmatchable_types_map get_base_name_to_sets_of_related_ops get_base_name_for_op add_op_to_sets_of_related_ops torch ao ns fx weight_utils get_op_to_type_to_weight_extraction_fn torch ao ns _numeric_suite_fx extract_weights _extract_weights_impl add_loggers _add_loggers_impl OutputLogger add_shadow_loggers _add_shadow_loggers_impl extract_logger_info extract_shadow_logger_info extend_logger_results_with_comparison prepare_n_shadows_model convert_n_shadows_model extract_results_n_shadows_model OutputComparisonLogger print_comparisons_n_shadows_model loggers_set_enabled loggers_set_save_activations _prepare_n_shadows_add_loggers_model _n_shadows_compare_weights torch ao ns fx qconfig_multi_mapping QConfigMultiMapping torch ao quantization backend_config get_native_backend_config torch ao quantization fx quantize_handler _get_pattern_to_quantize_handlers Note these models use outside file While s good reuse code we also need able iterate tests quickly when debugging If test model has large number callsites across various different files speed debugging individual test cases decreases LinearReluFunctional nn Module __init__ - None super __init__ w = nn Parameter torch empty b = nn Parameter torch zeros torch nn init kaiming_uniform_ w a=math sqrt forward x x = F linear x w b x = F relu x x LinearFunctional nn Module __init__ - None super __init__ w = nn Parameter torch empty b = nn Parameter torch zeros torch nn init kaiming_uniform_ w a=math sqrt forward x x = F linear x w b x LinearReluLinearFunctional nn Module __init__ - None super __init__ w = nn Parameter torch Tensor b = nn Parameter torch zeros torch nn init kaiming_uniform_ w a=math sqrt forward x x = F linear x w b x = F relu x x = F linear x w b x AddMulFunctional nn Module forward x y x = x + x = x x = + x x = x x = x + y x = x y x AllConvAndLinearFusionModules torch nn Module __init__ - None super __init__ conv d conv d_ = nn Conv d conv d - relu conv d_ = nn Conv d relu_ = nn ReLU conv d - bn qat only conv d_ = nn Conv d bn d_ = nn BatchNorm d conv d - bn - relu qat only conv d_ = nn Conv d bn d_ = nn BatchNorm d relu_ = nn ReLU conv d conv d_ = nn Conv d conv d - relu conv d_ = nn Conv d relu_ = nn ReLU conv d - bn qat only conv d_ = nn Conv d bn d_ = nn BatchNorm d conv d - bn - relu qat only conv d_ = nn Conv d bn d_ = nn BatchNorm d relu_ = nn ReLU conv d conv d_ = nn Conv d conv d - relu conv d_ = nn Conv d relu_ = nn ReLU conv d - bn qat only conv d_ = nn Conv d bn d_ = nn BatchNorm d conv d - bn - relu qat only conv d_ = nn Conv d bn d_ = nn BatchNorm d relu_ = nn ReLU linear linear_ = nn Linear linear - relu linear_ = nn Linear relu_ = nn ReLU forward x conv d x = conv d_ x x = conv d_ x x = relu_ x x = conv d_ x x = bn d_ x x = conv d_ x x = bn d_ x x = relu_ x conv d x = x reshape x = conv d_ x x = conv d_ x x = relu_ x x = conv d_ x x = bn d_ x x = conv d_ x x = bn d_ x x = relu_ x conv d x = x reshape x = conv d_ x x = conv d_ x x = relu_ x x = conv d_ x x = bn d_ x x = conv d_ x x = bn d_ x x = relu_ x linear x = x reshape x = linear_ x x = linear_ x x = relu_ x x AllConvFunctional torch nn Module __init__ weight d weight d weight d bias d bias d bias d super __init__ weight d = torch nn Parameter weight d weight d = torch nn Parameter weight d weight d = torch nn Parameter weight d bias d = torch nn Parameter bias d bias d = torch nn Parameter bias d bias d = torch nn Parameter bias d stride d = padding d = dilation d = stride d = padding d = dilation d = groups = stride d = padding d = dilation d = forward x x = F conv d x weight d bias d stride d padding d dilation d groups x = F conv d x weight d bias d stride d padding d dilation d groups x = F relu x x = F conv d x weight d bias d stride d padding d dilation d groups x = F conv d x weight d bias d stride d padding d dilation d groups x = F relu x x = F conv d x weight d bias d stride d padding d dilation d groups x = F conv d x weight d bias d stride d padding d dilation d groups x = F relu x x torch fx wrap _wrapped_hardswish x F hardswish x torch fx wrap _wrapped_hardswish_fp x x = x dequantize x = F hardswish x x = x torch float x torch fx wrap _wrapped_sigmoid x F sigmoid x torch fx wrap _wrapped_linear x w b F linear x w b get_all_quant_patterns we process migrate frontend fx graph mode quant use backend_config_dict so some patterns moved backend_config_dict function will include these patterns so we can still have all patterns TODO we can remove call get all patterns backend_config_dict future when frontend refactor done fx graph mode quantization all_quant_patterns = get_default_quant_patterns some patterns moved native backend_config_dict so we need add them back here pattern quantize_handler _get_pattern_to_quantize_handlers get_native_backend_config items all_quant_patterns pattern = quantize_handler all_quant_patterns TestFXGraphMatcher QuantizationTestCase skipIfNoFBGEMM test_simple_mod m = nn Sequential nn Conv d eval mp = prepare_fx m torch ao quantization default_qconfig example_inputs= torch randn mp_copy = copy deepcopy mp mq = convert_fx mp_copy results = get_matching_subgraph_pairs mp mq base_name_to_sets_of_related_ops = get_base_name_to_sets_of_related_ops conv_name_ = base_op_ + get_base_name_for_op base_name_to_sets_of_related_ops nn Conv d + _ expected_types = conv_name_ nn Conv d torch ao quantization MinMaxObserver nnq Conv d nnq Conv d assert_types_for_matched_subgraph_pairs results expected_types mp mq skipIfNoFBGEMM test_simple_fun M nn Module __init__ - None super __init__ w = nn Parameter torch empty b = nn Parameter torch zeros torch nn init kaiming_uniform_ w a=math sqrt forward x F linear x w b m = M eval mp = prepare_fx m torch ao quantization default_qconfig example_inputs= torch randn mp_copy = copy deepcopy mp mq = convert_fx mp_copy results = get_matching_subgraph_pairs mp mq base_name_to_sets_of_related_ops = get_base_name_to_sets_of_related_ops linear_name_ = base_op_ + get_base_name_for_op base_name_to_sets_of_related_ops F linear + _ expected_types = linear_name_ F linear torch ao quantization MinMaxObserver toq linear toq linear assert_types_for_matched_subgraph_pairs results expected_types mp mq skipIfNoFBGEMM test_simple_fusion m = LinearReluFunctional eval mp = prepare_fx m torch ao quantization default_qconfig example_inputs= torch randn mp_copy = copy deepcopy mp mq = convert_fx mp_copy results = get_matching_subgraph_pairs mp mq base_name_to_sets_of_related_ops = get_base_name_to_sets_of_related_ops linear_name_ = base_op_ + get_base_name_for_op base_name_to_sets_of_related_ops F linear + _ expected_types = linear_name_ F linear torch ao quantization MinMaxObserver toq linear_relu toq linear_relu assert_types_for_matched_subgraph_pairs results expected_types mp mq skipIfNoFBGEMM test_simple_mod_multi m = nn Sequential nn Sequential nn Conv d nn Conv d eval mp = prepare_fx m torch ao quantization default_qconfig example_inputs= torch randn mp_copy = copy deepcopy mp mq = convert_fx mp_copy assume success no exceptions results = get_matching_subgraph_pairs mp mq skipIfNoFBGEMM test_simple_tensor_ops M nn Module forward x y z = x + y z m = M eval example_inputs = torch randn torch randn mp = prepare_fx m torch ao quantization default_qconfig example_inputs=example_inputs mp_copy = copy deepcopy mp mq = convert_fx mp_copy assume success no exceptions results = get_matching_subgraph_pairs mp mq skipIfNoFBGEMM test_matching_failure_node_count verify matching graphs matching node types different counts matchable nodes fails m = nn Sequential nn Conv d eval m = nn Sequential nn Conv d nn Conv d eval example_inputs = torch randn mp = prepare_fx m torch ao quantization default_qconfig example_inputs=example_inputs mp = prepare_fx m torch ao quantization default_qconfig example_inputs=example_inputs assertRaises GraphMatchingException ex results = get_matching_subgraph_pairs mp mp skipIfNoFBGEMM test_matching_failure_node_type verify matching graphs non-matching node types fails m = nn Sequential nn Conv d eval m = nn Sequential nn Linear eval example_inputs = torch randn mp = prepare_fx m torch ao quantization default_qconfig example_inputs=example_inputs example_inputs = torch randn mp = prepare_fx m torch ao quantization default_qconfig example_inputs=example_inputs assertRaises GraphMatchingException ex results = get_matching_subgraph_pairs mp mp skipIfNoFBGEMM test_nodes_before_cat verify nodes before cat get matched M nn Module forward x x = torch add x y = torch add x x = torch cat x y x m = M eval example_inputs = torch randn mp = prepare_fx m torch ao quantization default_qconfig example_inputs=example_inputs mp_copy = copy deepcopy mp mq = convert_fx mp_copy results = get_matching_subgraph_pairs mp mq base_name_to_sets_of_related_ops = get_base_name_to_sets_of_related_ops cat_name_ = base_op_ + get_base_name_for_op base_name_to_sets_of_related_ops torch cat + _ add_name_ = base_op_ + get_base_name_for_op base_name_to_sets_of_related_ops torch add + _ add_name_ = base_op_ + get_base_name_for_op base_name_to_sets_of_related_ops torch add + _ expected_types = cat_name_ torch cat torch cat torch cat torch cat add_name_ torch add torch ao quantization MinMaxObserver toq add toq add add_name_ torch add torch ao quantization MinMaxObserver toq add toq add assert_types_for_matched_subgraph_pairs results expected_types mp mq skipIfNoFBGEMM test_dict_return_type verify we can traverse up nodes which dictionaries M nn Module forward x x = torch add x y = torch add x z = torch add x = x x y y z key z m = M eval example_inputs = torch randn mp = prepare_fx m torch ao quantization default_qconfig example_inputs=example_inputs mp_copy = copy deepcopy mp mq = convert_fx mp_copy results = get_matching_subgraph_pairs mp mq base_name_to_sets_of_related_ops = get_base_name_to_sets_of_related_ops add_name_ = base_op_ + get_base_name_for_op base_name_to_sets_of_related_ops torch add + _ add_name_ = base_op_ + get_base_name_for_op base_name_to_sets_of_related_ops torch add + _ add_name_ = base_op_ + get_base_name_for_op base_name_to_sets_of_related_ops torch add + _ expected_types = add_name_ torch add torch ao quantization MinMaxObserver toq add toq add add_name_ torch add torch ao quantization MinMaxObserver toq add toq add add_name_ torch add torch ao quantization MinMaxObserver toq add toq add assert_types_for_matched_subgraph_pairs results expected_types mp mq skipIfNoFBGEMM test_nodes_with_equal_types_get_matched M nn Module __init__ - None super __init__ conv = nn Conv d conv = nn Conv d forward x x = conv x x = conv x x = torch mul x x x = torch sigmoid x x = F relu x x m = M eval prevent conv getting quantized so we can test modules equal types qconfig_mapping = torch ao quantization get_default_qconfig_mapping set_module_name conv None example_inputs = torch randn mp = prepare_fx m qconfig_mapping example_inputs=example_inputs mp_copy = copy deepcopy mp mq = convert_fx mp_copy results = get_matching_subgraph_pairs mp mq base_name_to_sets_of_related_ops = get_base_name_to_sets_of_related_ops conv_name_ = base_op_ + get_base_name_for_op base_name_to_sets_of_related_ops nn Conv d + _ conv_name_ = base_op_ + get_base_name_for_op base_name_to_sets_of_related_ops nn Conv d + _ mul_name_ = base_op_ + get_base_name_for_op base_name_to_sets_of_related_ops torch mul + _ relu_name_ = base_op_ + get_base_name_for_op base_name_to_sets_of_related_ops torch relu + _ sigmoid_name_ = base_op_ + get_base_name_for_op base_name_to_sets_of_related_ops torch sigmoid + _ all these should matched expected_types = conv_name_ nn Conv d torch ao quantization HistogramObserver nnq Conv d nnq Conv d conv_name_ nn Conv d torch ao quantization HistogramObserver nn Conv d nn Conv d mul_name_ torch mul torch ao quantization HistogramObserver toq mul toq mul relu_name_ F relu torch ao quantization FixedQParamsObserver F relu F relu sigmoid_name_ torch sigmoid torch ao quantization FixedQParamsObserver torch sigmoid torch sigmoid assert_types_for_matched_subgraph_pairs results expected_types mp mq test_methods Verify graph matching works methods M nn Module forward x x = x sigmoid x m = M eval m = M eval qconfig_mapping = torch ao quantization get_default_qconfig_mapping example_inputs = torch randn m p = prepare_fx m qconfig_mapping example_inputs=example_inputs m p = prepare_fx m qconfig_mapping example_inputs=example_inputs results = get_matching_subgraph_pairs m p m p base_name_to_sets_of_related_ops = get_base_name_to_sets_of_related_ops sigmoid_name_ = base_op_ + get_base_name_for_op base_name_to_sets_of_related_ops torch sigmoid + _ expected_types = sigmoid_name_ sigmoid torch ao quantization FixedQParamsObserver sigmoid torch ao quantization FixedQParamsObserver assert_types_for_matched_subgraph_pairs results expected_types m p m p test_op_relationship_mapping Tests mapping op relationships complete base_name_to_sets_of_related_ops = get_base_name_to_sets_of_related_ops type_a_related_to_b = \ get_type_a_related_to_b base_name_to_sets_of_related_ops check static quant module mappings static_quant_mod_mappings = get_default_static_quant_module_mappings fp _type int _type static_quant_mod_mappings items skip quants dequants purposes Numerical Suite types_to_skip = torch ao quantization QuantStub torch ao quantization DeQuantStub nnq FloatFunctional ConvTranspose d swap implemented FX Graph mode quantization yet nn ConvTranspose d GroupNorm swap implemented FX Graph mode quantization yet nn GroupNorm nnq ReLU no longer swapped because nn ReLU can take quantized inputs nn ReLU fp _type types_to_skip continue verify relatedness in_type_a_related_to_b = \ fp _type int _type type_a_related_to_b assertTrue in_type_a_related_to_b f fp _type int _type need relationship mapping check static quant op mappings static_quant_fun_mappings = get_default_float_to_quantized_operator_mappings fp _type int _type static_quant_fun_mappings items verify relatedness in_type_a_related_to_b = \ fp _type int _type type_a_related_to_b assertTrue in_type_a_related_to_b f fp _type int _type need relationship mapping check dynamic quant mappings dynamic_quant_mappings = get_default_dynamic_quant_module_mappings fp _type int _type dynamic_quant_mappings items TODO future PR enable correct weight extraction these remove list types_to_skip = nn GRUCell nn GRU nn LSTMCell nn RNNCell fp _type types_to_skip continue verify relatedness in_type_a_related_to_b = \ fp _type int _type type_a_related_to_b assertTrue in_type_a_related_to_b f fp _type int _type need relationship mapping go through ops mapped each QuantizeHandler type verify correctness _op_in_base_sets_of_related_ops op ops base_name_to_sets_of_related_ops values op ops True False unmatchable_types_map = get_unmatchable_types_map FUNS_UNMATCHABLE = unmatchable_types_map funs_unmatchable MODS_UNMATCHABLE = unmatchable_types_map mods_unmatchable METHS_UNMATCHABLE = unmatchable_types_map meths_unmatchable _op_is_unmatchable op op FUNS_UNMATCHABLE op MODS_UNMATCHABLE op METHS_UNMATCHABLE default_quant_patterns = get_all_quant_patterns pattern qhandler_cls default_quant_patterns items base_op = None isinstance pattern tuple base_op = pattern - isinstance pattern str base_op = pattern base_op = pattern qhandler_cls_all_ops_quantizeable = qh CatQuantizeHandler qh ConvReluQuantizeHandler qh LinearReLUQuantizeHandler qh BatchNormQuantizeHandler qh EmbeddingQuantizeHandler qh RNNDynamicQuantizeHandler qhandler_cls_quant_op_same_signature = qh FixedQParamsOpQuantizeHandler qh CopyNodeQuantizeHandler qh GeneralTensorShapeOpQuantizeHandler qhandler_cls == qh BinaryOpQuantizeHandler these ops do have quantized equivalents ops_to_skip = torch bmm torch div torch sub operator truediv operator sub base_op ops_to_skip continue assertTrue _op_in_base_sets_of_related_ops base_op f base_op sets related ops qhandler_cls == qh RNNDynamicQuantizeHandler TODO future PR add support all classes RNNDynamicQuantizeHandler pass qhandler_cls == qh DefaultNodeQuantizeHandler assertTrue _op_in_base_sets_of_related_ops base_op f base_op sets related ops qhandler_cls qhandler_cls_quant_op_same_signature these ops use same op signature fp quantized tensors assertTrue _op_in_base_sets_of_related_ops base_op _op_is_unmatchable base_op f base_op sets related ops unmatchable qhandler_cls qhandler_cls_all_ops_quantizeable assertTrue _op_in_base_sets_of_related_ops base_op f base_op sets related ops torch sum does have quantized equivalents base_op torch sum nn GRUCell nn GRU nn LSTMCell nn RNNCell continue isinstance base_op tuple skip fusion patterns continue didn t match explicit quantize handler we can check operator related op set directly _op_in_base_sets_of_related_ops base_op _op_is_unmatchable base_op raise AssertionError f handling qhandler_cls op base_op implemented skipIfNoFBGEMM test_user_defined_function Verify graph matching works user defined functions M nn Module forward x x = F hardswish x x M nn Module forward x x = _wrapped_hardswish x x qconfig_mapping = torch ao quantization get_default_qconfig_mapping example_inputs = torch randn m = prepare_fx M eval qconfig_mapping example_inputs=example_inputs m = prepare_fx M eval qconfig_mapping example_inputs=example_inputs base_name_to_sets_of_related_ops = get_base_name_to_sets_of_related_ops add_op_to_sets_of_related_ops base_name_to_sets_of_related_ops _wrapped_hardswish F hardswish results = get_matching_subgraph_pairs m m base_name_to_sets_of_related_ops=base_name_to_sets_of_related_ops hardswish_name_ = base_op_ + get_base_name_for_op base_name_to_sets_of_related_ops F hardswish + _ expected_types = hardswish_name_ F hardswish torch ao quantization HistogramObserver _wrapped_hardswish _wrapped_hardswish assert_types_for_matched_subgraph_pairs results expected_types m m skipIfNoFBGEMM test_results_order m = nn Sequential nn Conv d nn Linear eval example_inputs = torch randn mp = prepare_fx m torch ao quantization default_qconfig example_inputs=example_inputs mp_copy = copy deepcopy mp mq = convert_fx mp_copy results = get_matching_subgraph_pairs mp mq assertTrue len results == results_iter = iter results items _ subgraph_a_ subgraph_b_ = next results_iter assertTrue subgraph_a_ start_node name == _ subgraph_b_ start_node name == _ _ subgraph_a_ subgraph_b_ = next results_iter assertTrue subgraph_a_ start_node name == _ subgraph_b_ start_node name == _ TestFXGraphMatcherModels QuantizationTestCase skipIfTorchDynamo too slow skipIfNoFBGEMM skip_if_no_torchvision test_mobilenet_v verify mobilenetv graph able matched torchvision m = torchvision models __dict__ mobilenet_v pretrained=False eval float example_inputs = torch randn mp = prepare_fx copy deepcopy m torch ao quantization default_qconfig example_inputs=example_inputs assume success no exceptions results_m_mp = get_matching_subgraph_pairs torch fx symbolic_trace m mp mp_copy = copy deepcopy mp mq = convert_fx mp_copy assume success no exceptions results_mp_mq = get_matching_subgraph_pairs mp mq skipIfNoFBGEMM skip_if_no_torchvision test_mobilenet_v _qat verify mobilenetv graph able matched torchvision m = torchvision models __dict__ mobilenet_v pretrained=False float example_inputs = torch randn mp = prepare_qat_fx copy deepcopy m torch ao quantization get_default_qat_qconfig fbgemm example_inputs=example_inputs assume success no exceptions results_m_mp = get_matching_subgraph_pairs torch fx symbolic_trace m mp mp_copy = copy deepcopy mp mq = convert_fx mp_copy assume success no exceptions results_mp_mq = get_matching_subgraph_pairs mp mq FXNumericSuiteQuantizationTestCase QuantizationTestCase _test_extract_weights m example_inputs results_len= qconfig_dict=None prepare_fn=prepare_fx m = torch fx symbolic_trace m qconfig_dict None qconfig_dict = torch ao quantization default_qconfig mp = prepare_fn copy deepcopy m qconfig_dict example_inputs=example_inputs mp_copy = copy deepcopy mp mq = convert_fx mp_copy test both public API well internal GraphModule API extract_weights_fun extract_weights _extract_weights_impl test both m vs mp mp vs mq m m m mp mp mq results = extract_weights_fun m b m assertTrue len results == results_len f expected len results_len got len len results assert_ns_compare_dict_valid results extend_logger_results_with_comparison results b compute_sqnr sqnr extend_logger_results_with_comparison results b compute_normalized_l _error l _error extend_logger_results_with_comparison results b compute_cosine_similarity cosine_similarity _test_match_activations m data prepared_expected_node_occurrence=None results_len= should_log_inputs=False qconfig_dict=None skip_scripting=False prepare_fn=prepare_fx qconfig_dict None qconfig_dict = torch ao quantization get_default_qconfig_mapping prepare_fn == prepare_fx m eval m train mp = prepare_fn copy deepcopy m qconfig_dict example_inputs=data mp data mp_copy = copy deepcopy mp mq = convert_fx mp_copy m_ns mp_ns = add_loggers m b copy deepcopy mp OutputLogger should_log_inputs=should_log_inputs mp_ns mq_ns = add_loggers mp b mq OutputLogger should_log_inputs=should_log_inputs prepared_expected_node_occurrence checkGraphModuleNodes m_ns expected_node_occurrence=prepared_expected_node_occurrence checkGraphModuleNodes mp_ns expected_node_occurrence=prepared_expected_node_occurrence checkGraphModuleNodes mp_ns expected_node_occurrence=prepared_expected_node_occurrence checkGraphModuleNodes mq_ns expected_node_occurrence=prepared_expected_node_occurrence skip_scripting m_ns = torch jit script m_ns mp_ns = torch jit script mp_ns mq_ns = torch jit script mq_ns calibrate m_ns data mp_ns data mp_ns data mq_ns data check activation result correctness results = m m m_ns mp_ns mp_ns mq_ns act_compare_dict = extract_logger_info m m OutputLogger b assertTrue len act_compare_dict == results_len f expected len results_len got len len act_compare_dict assert_ns_compare_dict_valid act_compare_dict extend_logger_results_with_comparison act_compare_dict b compute_sqnr sqnr extend_logger_results_with_comparison act_compare_dict b compute_normalized_l _error l _error extend_logger_results_with_comparison act_compare_dict b compute_cosine_similarity cosine_similarity results append act_compare_dict results _test_match_shadow_activations m data prepared_expected_node_occurrence=None results_len=None should_log_inputs=False qconfig_dict=None skip_scripting=False prepare_fn=prepare_fx compare_fp _vs_fp _prepared=True qconfig_dict None qconfig_dict = torch ao quantization get_default_qconfig_mapping prepare_fn == prepare_fx m eval m train print qconfig_dict qconfig_dict mp = prepare_fn copy deepcopy m qconfig_dict example_inputs=data print prepared mp mp data mp_copy = copy deepcopy mp mq = convert_fx mp_copy print quantized mq compare_fp _vs_fp _prepared m_shadows_mp = add_shadow_loggers copy deepcopy m b copy deepcopy mp OutputLogger should_log_inputs=should_log_inputs mp_shadows_mq = add_shadow_loggers mp b mq OutputLogger should_log_inputs=should_log_inputs prepared_expected_node_occurrence compare_fp _vs_fp _prepared checkGraphModuleNodes m_shadows_mp expected_node_occurrence=prepared_expected_node_occurrence checkGraphModuleNodes mp_shadows_mq expected_node_occurrence=prepared_expected_node_occurrence skip_scripting compare_fp _vs_fp _prepared m_shadows_mp = torch jit script m_shadows_mp mp_shadows_mq = torch jit script mp_shadows_mq calibrate compare_fp _vs_fp _prepared m_shadows_mp data mp_shadows_mq data check activation result correctness results = models = m_shadows_mp mp_shadows_mq \ compare_fp _vs_fp _prepared mp_shadows_mq model models act_compare_dict = extract_shadow_logger_info model OutputLogger b results_len None assertTrue len act_compare_dict == results_len f expected len results_len got len len act_compare_dict assert_ns_compare_dict_valid act_compare_dict extend_logger_results_with_comparison act_compare_dict b compute_sqnr sqnr extend_logger_results_with_comparison act_compare_dict b compute_normalized_l _error l _error extend_logger_results_with_comparison act_compare_dict b compute_cosine_similarity cosine_similarity results append act_compare_dict results TestFXNumericSuiteCoreAPIs FXNumericSuiteQuantizationTestCase skipIfNoFBGEMM test_extract_weights_mod_ptq m = AllConvAndLinearFusionModules eval example_inputs = torch randn _test_extract_weights m example_inputs results_len= skipIfNoFBGEMM test_extract_weights_mod_qat m = AllConvAndLinearFusionModules train qconfig_dict = torch ao quantization get_default_qat_qconfig fbgemm example_inputs = torch randn _test_extract_weights m example_inputs results_len= qconfig_dict=qconfig_dict prepare_fn=prepare_qat_fx skipIfNoFBGEMM test_extract_weights_linear_fun_ptq m = LinearReluLinearFunctional eval example_inputs = torch randn _test_extract_weights m example_inputs results_len= skipIfNoFBGEMM test_extract_weights_linear_fun_qat m = LinearReluLinearFunctional train qconfig_dict = torch ao quantization get_default_qat_qconfig fbgemm example_inputs = torch randn _test_extract_weights m example_inputs results_len= qconfig_dict=qconfig_dict prepare_fn=prepare_qat_fx skipIfNoFBGEMM test_extract_weights_conv_fun_ptq w d = torch randn w d = torch randn w d = torch randn b d = torch randn b d = torch randn b d = torch randn m = AllConvFunctional w d w d w d b d b d b d eval example_inputs = torch randn _test_extract_weights m example_inputs results_len= skipIfNoFBGEMM test_extract_weights_conv_fun_qat w d = torch randn w d = torch randn w d = torch randn b d = torch randn b d = torch randn b d = torch randn m = AllConvFunctional w d w d w d b d b d b d train qconfig_dict = torch ao quantization get_default_qat_qconfig fbgemm example_inputs = torch randn _test_extract_weights m example_inputs results_len= qconfig_dict=qconfig_dict prepare_fn=prepare_qat_fx skipIfNoFBGEMM test_extract_weights_dynamic TODO future PR add Linear-ReLU after fixed m = nn Sequential nn Linear eval qconfig_dict = object_type nn Linear default_dynamic_qconfig example_inputs = torch randn _test_extract_weights m example_inputs results_len= qconfig_dict=qconfig_dict skipIfNoFBGEMM test_extract_weights_fqn m = nn Sequential nn Sequential nn Conv d nn Conv d eval qconfig_dict = torch ao quantization default_qconfig example_inputs = torch randn mp = prepare_fx m qconfig_dict example_inputs=example_inputs mq = convert_fx copy deepcopy mp results = extract_weights mp b mq fqn_a_ = results _ _ weight fqn fqn_b_ = results _ _ weight b fqn assertTrue fqn_a_ == fqn_a_ == fqn_b_ fqn_a_ = results _ weight fqn fqn_b_ = results _ weight b fqn assertTrue fqn_a_ == fqn_a_ == fqn_b_ _test_match_activations_mod_impl prepare_fn=prepare_fx m = nn Sequential torch ao quantization QuantStub nn Conv d nn Conv d eval qconfig_dict = None prepare_fn == prepare_qat_fx qconfig_dict = torch ao quantization get_default_qat_qconfig fbgemm expected_occurrence = ns call_module OutputLogger _test_match_activations m torch randn prepared_expected_node_occurrence=expected_occurrence results_len= qconfig_dict=qconfig_dict prepare_fn=prepare_fn skipIfNoFBGEMM test_match_activations_mod_ptq _test_match_activations_mod_impl prepare_fn=prepare_fx skipIfNoFBGEMM test_match_activations_mod_qat _test_match_activations_mod_impl prepare_fn=prepare_qat_fx _test_match_activations_fun_impl prepare_fn=prepare_fx m = LinearReluLinearFunctional eval qconfig_dict = None prepare_fn == prepare_qat_fx qconfig_dict = torch ao quantization get_default_qat_qconfig fbgemm expected_occurrence = ns call_module OutputLogger _test_match_activations m torch randn prepared_expected_node_occurrence=expected_occurrence results_len= prepare_fn=prepare_fn qconfig_dict=qconfig_dict skipIfNoFBGEMM test_match_activations_fun_ptq _test_match_activations_fun_impl prepare_fn=prepare_fx skipIfNoFBGEMM test_match_activations_fun_qat _test_match_activations_fun_impl prepare_fn=prepare_qat_fx skipIfNoFBGEMM test_match_activations_meth_ptq Verify add_loggers works methods M nn Module forward x x = x sigmoid x m = M eval res = _test_match_activations m torch randn results_len= skipIfNoFBGEMM test_match_activations_fqn m = nn Sequential nn Sequential nn Conv d nn Conv d eval qconfig_dict = torch ao quantization default_qconfig example_inputs = torch randn mp = prepare_fx m qconfig_dict example_inputs=example_inputs mq = convert_fx copy deepcopy mp mp_ns mq_ns = add_loggers mp b mq OutputLogger datum = torch randn mp_ns datum mq_ns datum results = extract_logger_info mp_ns mq_ns OutputLogger b fqn_a_ = results _ _ node_output fqn fqn_b_ = results _ _ node_output b fqn assertTrue fqn_a_ == fqn_a_ == fqn_b_ fqn_a_ = results _ node_output fqn fqn_b_ = results _ node_output b fqn assertTrue fqn_a_ == fqn_a_ == fqn_b_ _test_add_shadow_loggers_mod_impl prepare_fn=prepare_fx m = nn Sequential nn Conv d nn Conv d eval qconfig_dict = None prepare_fn == prepare_qat_fx qconfig_dict = torch ao quantization get_default_qat_qconfig fbgemm res = _test_match_shadow_activations m torch randn results_len= prepare_fn=prepare_fn qconfig_dict=qconfig_dict skipIfNoFBGEMM test_add_shadow_loggers_mod_ptq _test_add_shadow_loggers_mod_impl prepare_fn=prepare_fx skipIfNoFBGEMM test_add_shadow_loggers_mod_qat _test_add_shadow_loggers_mod_impl prepare_fn=prepare_qat_fx _test_add_shadow_loggers_fun_impl prepare_fn=prepare_fx m = LinearReluLinearFunctional qconfig_dict = None prepare_fn == prepare_qat_fx qconfig_dict = torch ao quantization get_default_qat_qconfig fbgemm res = _test_match_shadow_activations m torch randn results_len= prepare_fn=prepare_fn qconfig_dict=qconfig_dict skipIfNoFBGEMM test_add_shadow_loggers_fun_ptq _test_add_shadow_loggers_fun_impl prepare_fn=prepare_fx skipIfNoFBGEMM test_add_shadow_loggers_fun_qat _test_add_shadow_loggers_fun_impl prepare_fn=prepare_qat_fx skipIfNoFBGEMM test_add_shadow_loggers_meth_ptq Verify add_loggers works methods M nn Module forward x x = x sigmoid x m = M eval res = _test_match_shadow_activations m torch randn For now sigmoid supported shadowing because dtype inference implemented yet So just testing shadowing models method calls does crash results_len= skipIfNoFBGEMM test_shadow_activations_fqn m = nn Sequential nn Sequential nn Conv d nn Conv d eval qconfig_mapping = torch ao quantization get_default_qconfig_mapping example_inputs = torch randn mp = prepare_fx m qconfig_mapping example_inputs=example_inputs mq = convert_fx copy deepcopy mp mp_shadows_mq = add_shadow_loggers mp b mq OutputLogger datum = torch randn mp_shadows_mq datum results = extract_shadow_logger_info mp_shadows_mq OutputLogger b fqn_a_ = results _ _ node_output fqn fqn_b_ = results _ _ node_output b fqn assertTrue fqn_a_ == fqn_a_ == fqn_b_ fqn_a_ = results _ node_output fqn fqn_b_ = results _ node_output b fqn assertTrue fqn_a_ == fqn_a_ == fqn_b_ skipIfNoFBGEMM test_logging_inputs Verifies logging inputs works correctly M nn Module __init__ - None super __init__ conv = nn Conv d forward x x = conv x x = torch cat x x dim= x m = M eval _test_match_shadow_activations m torch randn results_len= should_log_inputs=True skipIfNoFBGEMM test_ops_with_same_fp _and_int _signature Verifies we can match pairs ops which have same aten signature fp int tensors M nn Module __init__ - None super __init__ max_pool_ d = nn MaxPool d forward x x = max_pool_ d x x = F relu x x m = M eval _test_match_activations m torch randn results_len= skipIfNoFBGEMM test_add_mul_inputs_activations m = AddMulFunctional eval res = _test_match_activations m torch randn torch randn results_len= should_log_inputs=True skipIfNoFBGEMM test_linear_fp _weights qconfig_dict = torch ao quantization float _static_qconfig m = LinearReluFunctional eval example_inputs = torch randn _test_extract_weights m example_inputs results_len= qconfig_dict=qconfig_dict skipIfNoFBGEMM test_linear_fp _activations should_log_inputs True False qconfig_dict = torch ao quantization float _static_qconfig m = LinearReluFunctional eval num_loggers = should_log_inputs expected_occurrence = ns call_module OutputLogger num_loggers res = _test_match_activations m torch randn prepared_expected_node_occurrence=expected_occurrence results_len= qconfig_dict=qconfig_dict should_log_inputs=should_log_inputs skipIfNoFBGEMM test_linear_fp _shadow_activations should_log_inputs True False qconfig_dict = torch ao quantization float _static_qconfig m = LinearReluFunctional eval num_loggers = should_log_inputs expected_occurrence = ns call_module OutputLogger num_loggers res = _test_match_shadow_activations m torch randn prepared_expected_node_occurrence=expected_occurrence results_len= qconfig_dict=qconfig_dict should_log_inputs=should_log_inputs skipIfNoFBGEMM test_linear_fp _vs_linear_fp _shadow_activations m = LinearFunctional eval qconfig_dict = torch ao quantization float _static_qconfig example_inputs = torch randn mp = prepare_fx m qconfig_dict example_inputs=example_inputs mq = convert_fx copy deepcopy mp mq = convert_fx copy deepcopy mp mq _shadows_mq = _add_shadow_loggers_impl mq b mq OutputLogger should_log_inputs=False mq _shadows_mq torch randn act_compare_dict = extract_shadow_logger_info mq _shadows_mq OutputLogger b assertTrue len act_compare_dict == assert_ns_compare_dict_valid act_compare_dict skipIfNoFBGEMM test_op_with_either_fp _or_int _input Verify shadowing works ops which accept either fp int inputs M nn Module __init__ - None super __init__ relu = nn ReLU forward x x = relu x x = F relu x x m = M res = _test_match_shadow_activations m torch randn Note shadowing relu itself currently supported test just testing does crash results_len= _test_int _shadows_int _impl m Verify shadowing works where both modules int qconfig_dict = torch ao quantization default_qconfig example_inputs = torch randn mp = prepare_fx m qconfig_dict example_inputs=example_inputs mp example_inputs mq = convert_fx copy deepcopy mp mq = convert_fx mp mq _shadows_mq = add_shadow_loggers mq b mq OutputLogger mq _shadows_mq torch randn act_compare_dict = extract_shadow_logger_info mq _shadows_mq OutputLogger b assertTrue len act_compare_dict == assert_ns_compare_dict_valid act_compare_dict skipIfNoFBGEMM test_int _shadows_int _mod m = nn Sequential nn Conv d eval _test_int _shadows_int _impl m skipIfNoFBGEMM test_int _shadows_int _fun m = LinearFunctional eval _test_int _shadows_int _impl m skipIfNoFBGEMM test_user_module_scriptable Logging output supported because neither tensor RNN type M nn Module forward x x = x x = x x x M nn Module __init__ - None super __init__ m = M forward x x x = m x x x m = M eval qconfig_dict = torch ao quantization default_qconfig prepare_custom_config_dict = non_traceable_module_class M example_inputs = torch randn mp = prepare_fx m qconfig_dict example_inputs=example_inputs prepare_custom_config=prepare_custom_config_dict mp = copy deepcopy mp unmatchable_types_map = get_unmatchable_types_map unmatchable_types_map mods_unmatchable add M mp _ns mp _ns = _add_loggers_impl mp b mp OutputLogger should_log_inputs=False unmatchable_types_map=unmatchable_types_map Scripting model loggers should succeed If fails because incorrect dtypes we can blocklist associated types being instrumented mp _ns_scripted = torch jit script mp _ns mp _ns_scripted = torch jit script mp _ns skipIfNoFBGEMM test_user_module For user defined modules weight extraction should crash unshadowed activations should only have loggers known types shadowed activations should only have loggers known types known dtypes UserModule nn Module forward x x M nn Module __init__ - None super __init__ linear = nn Linear user_module = UserModule forward x x = linear x x = user_module x x m = M eval quantize without tracing through UserModule qconfig_dict = torch ao quantization default_qconfig prepare_custom_config_dict = non_traceable_module_name user_module example_inputs = torch randn mp = prepare_fx m qconfig_dict example_inputs=example_inputs prepare_custom_config=prepare_custom_config_dict mp example_inputs mq = convert_fx copy deepcopy mp weight extraction should crash weights = _extract_weights_impl fp _prepared mp int mq unshadowed activations should have loggers add loggers without retracing note converting again because we cannot copy quantized linear mp_ns mq_ns = _add_loggers_impl fp _prepared copy deepcopy mp int convert_fx copy deepcopy mp OutputLogger should_log_inputs=True both fp int models should have loggers each I O linear I O user_module unshadowed_expected_occurrence = ns call_module OutputLogger checkGraphModuleNodes mp_ns expected_node_occurrence=unshadowed_expected_occurrence checkGraphModuleNodes mq_ns expected_node_occurrence=unshadowed_expected_occurrence shadowed activations should only have loggers nodes where types known we can do dtype cast add shadow loggers without retracing mp_shadows_mq_ns = _add_shadow_loggers_impl fp _prepared mp int mq OutputLogger should_log_inputs=True loggers I O linear loggers I O user_module shadowed_expected_occurrence = ns call_module OutputLogger checkGraphModuleNodes mp_shadows_mq_ns expected_node_occurrence=shadowed_expected_occurrence test_op_io_dtype_coverage Tests all ops quantization cares about have input output dtypes defined base_name_to_sets_of_related_ops = get_base_name_to_sets_of_related_ops type_a_related_to_b = \ get_type_a_related_to_b base_name_to_sets_of_related_ops TODO future PR clean up node_type_to_io_type_map = get_node_type_to_io_type_map FUNS_IO_TYPE_FP = node_type_to_io_type_map funs_io_type_fp FUNS_IO_TYPE_INT = node_type_to_io_type_map funs_io_type_int FUNS_IO_TYPE_FP _OR_INT = node_type_to_io_type_map funs_io_type_fp _or_int MODS_IO_TYPE_FP = node_type_to_io_type_map mods_io_type_fp MODS_IO_TYPE_INT = node_type_to_io_type_map mods_io_type_int MODS_IO_TYPE_FP _OR_INT = node_type_to_io_type_map mods_io_type_fp _or_int METHS_IO_TYPE_FP _OR_INT = node_type_to_io_type_map meths_io_type_fp _or_int unmatchable_types_map = get_unmatchable_types_map FUNS_UNMATCHABLE = unmatchable_types_map funs_unmatchable MODS_UNMATCHABLE = unmatchable_types_map mods_unmatchable METHS_UNMATCHABLE = unmatchable_types_map meths_unmatchable check static quant module mappings static_quant_mod_mappings = get_default_static_quant_module_mappings fp _type int _type static_quant_mod_mappings items types_to_skip = torch ao quantization QuantStub torch ao quantization DeQuantStub nnq FloatFunctional TODO future PR look into whether shadowing embeddings makes sense nn Embedding nn EmbeddingBag ConvTranspose d swap implemented FX Graph mode quantization yet nn ConvTranspose d GroupNorm swap implemented FX Graph mode quantization yet nn GroupNorm nnq ReLU no longer swapped because nn ReLU can take quantized inputs nn ReLU fp _type types_to_skip continue assertTrue fp _type MODS_IO_TYPE_FP f missing IO type handling f fp _type assertTrue int _type MODS_IO_TYPE_INT f missing IO type handling f int _type check static quant op mappings static_quant_fun_mappings = get_default_float_to_quantized_operator_mappings fp _type int _type static_quant_fun_mappings items assertTrue fp _type FUNS_IO_TYPE_FP f missing IO type handling f fp _type assertTrue int _type FUNS_IO_TYPE_INT f missing IO type handling f int _type check dynamic quant mappings dynamic_quant_mappings = get_default_dynamic_quant_module_mappings fp _type fp _type dynamic_quant_mappings items TODO future PR verify correct I O these remove list types_to_skip = nn GRUCell nn GRU nn LSTMCell nn RNNCell TODO future PR look into whether shadowing embeddings makes sense nn Embedding nn EmbeddingBag fp _type types_to_skip continue assertTrue fp _type MODS_IO_TYPE_FP f missing IO type handling f fp _type assertTrue fp _type MODS_IO_TYPE_FP f missing IO type handling f fp _type go through ops mapped each QuantizeHandler type verify correctness default_quant_patterns = get_all_quant_patterns pattern qhandler_cls default_quant_patterns items base_op = None isinstance pattern tuple base_op = pattern - isinstance pattern str base_op = pattern base_op = pattern qhandler_cls qh BinaryOpQuantizeHandler qh RNNDynamicQuantizeHandler TODO future PR implement shadowing binary ops TODO future PR implement shadowing RNN ops continue qhandler_cls == qh CatQuantizeHandler assertTrue base_op FUNS_IO_TYPE_FP _OR_INT f missing IO type handling base_op qhandler_cls qh ConvReluQuantizeHandler qh LinearReLUQuantizeHandler qh BatchNormQuantizeHandler qh DefaultNodeQuantizeHandler assertTrue base_op FUNS_IO_TYPE_FP base_op MODS_IO_TYPE_FP f missing IO type handling base_op qhandler_cls qh FixedQParamsOpQuantizeHandler qh CopyNodeQuantizeHandler qh GeneralTensorShapeOpQuantizeHandler base_op FUNS_UNMATCHABLE base_op MODS_UNMATCHABLE base_op METHS_UNMATCHABLE continue assertTrue base_op FUNS_IO_TYPE_FP _OR_INT base_op MODS_IO_TYPE_FP _OR_INT base_op METHS_IO_TYPE_FP _OR_INT Softmax has different signature quantized version so does fit into cases above base_op torch nn Softmax f missing IO type handling base_op qhandler_cls == qh EmbeddingQuantizeHandler embedding shadowing implemented now continue base_op FUNS_UNMATCHABLE base_op MODS_UNMATCHABLE base_op METHS_UNMATCHABLE continue qhandler_cls None is_general_tensor_value_op assertTrue base_op FUNS_IO_TYPE_FP _OR_INT base_op MODS_IO_TYPE_FP _OR_INT base_op METHS_IO_TYPE_FP _OR_INT f missing IO type handling base_op using qhandler_cls assertTrue base_op FUNS_IO_TYPE_FP _OR_INT base_op MODS_IO_TYPE_FP _OR_INT base_op METHS_IO_TYPE_FP _OR_INT base_op FUNS_IO_TYPE_FP base_op MODS_IO_TYPE_FP f missing IO type handling base_op using qhandler_cls skipIfNoFBGEMM test_user_defined_function Verify NS APIs work user defined functions M nn Module __init__ - None super __init__ w = nn Parameter torch empty b = nn Parameter torch zeros torch nn init kaiming_uniform_ w a=math sqrt forward x x = F hardswish x x = x sigmoid x = F linear x w b x M nn Module __init__ - None super __init__ w = nn Parameter torch empty b = nn Parameter torch zeros torch nn init kaiming_uniform_ w a=math sqrt forward x x = _wrapped_hardswish x x = _wrapped_sigmoid x x = _wrapped_linear x w b x qconfig_mapping = torch ao quantization get_default_qconfig_mapping example_inputs = torch randn m = prepare_fx M eval qconfig_mapping example_inputs=example_inputs m = prepare_fx M eval qconfig_mapping example_inputs=example_inputs data = torch randn base_name_to_sets_of_related_ops = get_base_name_to_sets_of_related_ops add_op_to_sets_of_related_ops base_name_to_sets_of_related_ops _wrapped_hardswish F hardswish add_op_to_sets_of_related_ops base_name_to_sets_of_related_ops _wrapped_sigmoid F sigmoid add_op_to_sets_of_related_ops base_name_to_sets_of_related_ops _wrapped_linear F linear op_to_type_to_weight_extraction_fn = \ get_op_to_type_to_weight_extraction_fn op_to_type_to_weight_extraction_fn call_function _wrapped_linear = \ torch ao ns fx weight_utils get_linear_fun_weight test compare weights results = extract_weights m b m base_name_to_sets_of_related_ops=base_name_to_sets_of_related_ops op_to_type_to_weight_extraction_fn=op_to_type_to_weight_extraction_fn assertTrue len results == assertTrue len results _wrapped_linear weight == test unshadowed activations m _ns m _ns = _add_loggers_impl copy deepcopy m b copy deepcopy m OutputLogger should_log_inputs=False base_name_to_sets_of_related_ops=base_name_to_sets_of_related_ops calibrate m _ns data m _ns data check activation result correctness act_compare_dict = extract_logger_info m _ns m _ns OutputLogger b assertTrue len act_compare_dict == assert_ns_compare_dict_valid act_compare_dict test shadowed activations node_type_to_io_type_map = get_node_type_to_io_type_map node_type_to_io_type_map funs_io_type_fp add _wrapped_hardswish node_type_to_io_type_map funs_io_type_fp add _wrapped_sigmoid m _shadows_m _ns = _add_shadow_loggers_impl m b m OutputLogger should_log_inputs=False base_name_to_sets_of_related_ops=base_name_to_sets_of_related_ops node_type_to_io_type_map=node_type_to_io_type_map calibrate m _shadows_m _ns data check activation result correctness act_compare_dict = extract_shadow_logger_info m _shadows_m _ns OutputLogger b assertTrue len act_compare_dict == assert_ns_compare_dict_valid act_compare_dict skipIfNoFBGEMM test_layer_names m = nn Sequential nn Conv d nn Conv d nn Sigmoid eval qconfig_mapping = torch ao quantization get_default_qconfig_mapping fbgemm example_inputs = torch randn mp = torch ao quantization quantize_fx prepare_fx m qconfig_mapping example_inputs=example_inputs mq = torch ao quantization quantize_fx convert_fx copy deepcopy mp extract weights results = extract_weights fp mp int mq mq_node_names = node name node mq graph nodes layer_name results keys assertTrue layer_name mq_node_names match activations mq = torch ao quantization quantize_fx convert_fx copy deepcopy mp mp_ns mq_ns = add_loggers fp copy deepcopy mp int mq OutputLogger data = torch randn mp_ns data mq_ns data results = extract_logger_info mp_ns mq_ns OutputLogger int mq_node_names = node name node mq_ns graph nodes layer_name results keys assertTrue layer_name mq_node_names match shadow activations mq = torch ao quantization quantize_fx convert_fx copy deepcopy mp mp_shadows_mq = add_shadow_loggers fp mp int mq OutputLogger mp_shadows_mq data results = extract_shadow_logger_info mp_shadows_mq OutputLogger int mq_node_names = node name node mp_shadows_mq graph nodes layer_name results keys assertTrue layer_name mq_node_names skipIfNoFBGEMM test_extend_logger_results_with_comparison m = nn Sequential nn Conv d nn Conv d eval qconfig_dict = torch ao quantization default_qconfig example_inputs = torch randn mp = torch ao quantization quantize_fx prepare_fx m qconfig_dict example_inputs=example_inputs mq = torch ao quantization quantize_fx convert_fx copy deepcopy mp extract weights results = extract_weights fp mp int mq extend_logger_results_with_comparison results fp int compute_sqnr sqnr_int _vs_fp extend_logger_results_with_comparison results fp int compute_normalized_l _error l _error_int _vs_fp extend_logger_results_with_comparison results fp int compute_cosine_similarity cosine_similarity_int _vs_fp layer_results results values assert sqnr_int _vs_fp \ layer_results weight int keys assert l _error_int _vs_fp \ layer_results weight int keys assert cosine_similarity_int _vs_fp \ layer_results weight int keys skipIfNoFBGEMM test_int _shadows_fp _simple m = nn Sequential nn Conv d nn Conv d nn ReLU eval qconfig_dict = torch ao quantization default_qconfig example_inputs = torch randn mp = torch ao quantization quantize_fx prepare_fx m qconfig_dict example_inputs=example_inputs mp torch randn mq = torch ao quantization quantize_fx convert_fx copy deepcopy mp mq_ref = torch ao quantization quantize_fx convert_fx copy deepcopy mp mp_shadows_mq = add_shadow_loggers int mq fp mp OutputLogger verify scale zp extracted correctly first op scale+zp live attributes module scale_ = mp_shadows_mq _ _input_scale_ scale_ _ref = getattr mq_ref _input_scale_ assertEqual scale_ scale_ _ref zp_ = mp_shadows_mq _ _input_zero_point_ zp_ _ref = getattr mq_ref _input_zero_point_ assertEqual zp_ zp_ _ref second op scale zp input second op must equal scale zp output first op scale_ = mp_shadows_mq _ _input_scale_ scale_ _ref = getattr mq_ref scale assertEqual scale_ scale_ _ref zp_ = mp_shadows_mq _ _input_zero_point_ zp_ _ref = getattr mq_ref zero_point assertEqual zp_ zp_ _ref verify running data works mp_shadows_mq torch randn act_compare_dict = extract_shadow_logger_info mp_shadows_mq OutputLogger fp assertTrue len act_compare_dict == assert_ns_compare_dict_valid act_compare_dict skipIfNoFBGEMM test_int _shadows_fp _coverage M torch nn Module __init__ - None super __init__ adaptive_avg_pool = nn AdaptiveAvgPool d conv = nn Conv d forward x x = adaptive_avg_pool x input qparams conv will input qparams adaptive_avg_pool x = conv x x = torch mul x x x = conv x x = torch add x x x = F relu x x = conv x x m = M eval qconfig_dict = torch ao quantization default_qconfig example_inputs = torch randn mp = prepare_fx m qconfig_dict example_inputs=example_inputs mp example_inputs mq = torch ao quantization quantize_fx convert_fx copy deepcopy mp mq_ref = torch ao quantization quantize_fx convert_fx copy deepcopy mp mp_shadows_mq = add_shadow_loggers int mq fp mp OutputLogger mp_shadows_mq torch randn act_compare_dict = extract_shadow_logger_info mp_shadows_mq OutputLogger fp assertTrue len act_compare_dict == assert_ns_compare_dict_valid act_compare_dict skipIfNoFBGEMM test_loggers_preserve_qat_numerics m = nn Sequential nn Conv d nn Conv d qconfig_dict = torch ao quantization get_default_qat_qconfig fbgemm example_inputs = torch randn mp = prepare_qat_fx m qconfig_dict example_inputs=example_inputs mp example_inputs mc = convert_fx copy deepcopy mp mp apply torch ao quantization disable_observer ref_fp = mp example_inputs ref_int = mc example_inputs mp_ns mc_ns = add_loggers fp mp int mc OutputLogger ref_fp _ns = mp_ns example_inputs ref_int _ns = mc_ns example_inputs assertEqual ref_fp ref_fp _ns assertEqual ref_int ref_int _ns skipIfNoFBGEMM test_shadow_loggers_preserve_qat_numerics m = nn Sequential nn Conv d nn Conv d qconfig_dict = torch ao quantization get_default_qat_qconfig fbgemm example_inputs = torch randn mp = prepare_qat_fx m qconfig_dict example_inputs=example_inputs mp example_inputs mc = convert_fx copy deepcopy mp mp apply torch ao quantization disable_observer ref_fp = mp example_inputs ref_int = mc example_inputs mc_shadows_mp = add_shadow_loggers int mc fp mp OutputLogger ref_shadow = mc_shadows_mp example_inputs assertEqual ref_fp ref_shadow unittest skipIf TEST_CUDA CUDA unavailable test_extract_weights_cuda Note using quantization because quantized kernels do work cuda yet m = nn Sequential nn Conv d cuda m = nn Sequential nn Conv d cuda results = extract_weights m b m extend_logger_results_with_comparison results b compute_sqnr sqnr assert_ns_compare_dict_valid results unittest skipIf TEST_CUDA CUDA unavailable test_add_loggers_cuda Note using quantization because quantized kernels do work cuda yet m = nn Sequential nn Conv d cuda m = nn Sequential nn Conv d cuda m _ns m _ns = add_loggers m b m OutputLogger datum = torch randn datum = datum cuda m _ns datum m _ns datum act_compare_dict = extract_logger_info m _ns m _ns OutputLogger b extend_logger_results_with_comparison act_compare_dict b compute_sqnr sqnr unittest skipIf TEST_CUDA CUDA unavailable test_add_shadow_loggers_cuda Note using quantization because quantized kernels do work cuda yet m = nn Sequential nn Conv d cuda m = nn Sequential nn Conv d cuda m _shadows_m = add_shadow_loggers m b m OutputLogger datum = torch randn datum = datum cuda m _shadows_m datum act_compare_dict = extract_shadow_logger_info m _shadows_m OutputLogger b extend_logger_results_with_comparison act_compare_dict b compute_sqnr sqnr test_fp _shadows_fp m = LinearReluFunctional eval example_inputs = torch randn qconfig_dict = torch ao quantization float _static_qconfig mp = prepare_fx copy deepcopy m qconfig_dict example_inputs=example_inputs mq = convert_to_reference_fx mp mq_shadows_m = add_shadow_loggers mq b m OutputLogger test_mul_add_cat_stack_skips_shadowing M nn Module forward x x = x x x = torch mul x x x = x + x x = torch add x x x = torch cat x x = torch stack x x m = M eval _test_match_shadow_activations m torch randn results_len= test_op_with_only_kwargs_skips_shadowing M nn Module forward x x = torch cat tensors= x x = torch stack tensors= x x m = M eval _test_match_shadow_activations m torch randn results_len= test_unsupported_op_copy_skips_shadowing Copying ` call_function ` node implemented test does crash shadowing instead skips node M nn Module forward x second argument leads attempting copy call_function node x = F layer_norm x x shape x m = M eval _test_match_shadow_activations m torch randn results_len= test_linear_kwargs_shadow M nn Module __init__ - None super __init__ w = nn Parameter torch empty b = nn Parameter torch zeros torch nn init kaiming_uniform_ w a=math sqrt forward x x = F linear input=x weight=self w bias=self b x note FX graph mode quantization does have good support kwargs-only right now so we pass two unquantized models m = M eval mt = torch fx symbolic_trace m mt_copy = copy deepcopy mt mt_shadows_mt_copy = add_shadow_loggers mt b mt_copy OutputLogger mt_shadows_mt_copy torch randn act_compare_dict = extract_shadow_logger_info mt_shadows_mt_copy OutputLogger b assertTrue len act_compare_dict == skipIfNoQNNPACK TestFXNumericSuiteNShadows FXNumericSuiteQuantizationTestCase Tests n shadows workflow _test_impl m example_input qconfig_mappings backend_config = get_native_backend_config test input valid _ = m example_input msp = prepare_n_shadows_model m example_input qconfig_mappings backend_config print msp msp _ range msp example_input msq = convert_n_shadows_model msp loggers_set_enabled msq True msq example_input results = extract_results_n_shadows_model msq print_comparisons_n_shadows_model results msq withQNNPACKBackend test_linear_mod M nn Module __init__ - None super __init__ fc = nn Linear forward x x = fc x x m = M eval example_input = torch randn qconfig_mappings = \ QConfigMultiMapping set_global torch ao quantization default_qconfig _test_impl m example_input qconfig_mappings withQNNPACKBackend test_linear_relu_mod M nn Module __init__ - None super __init__ fc = nn Linear fc = nn Linear relu = nn ReLU forward x x = fc x x = fc x x = relu x x m = M eval example_input = torch randn qconfig_mappings = QConfigMultiMapping set_global torch ao quantization default_qconfig torch ao quantization default_dynamic_qconfig _test_impl m example_input qconfig_mappings withQNNPACKBackend test_conv_bn_relu_mod M nn Module __init__ - None super __init__ conv = nn Conv d bn = nn BatchNorm d relu = nn ReLU forward x x = conv x x = bn x x = relu x x m = M eval example_input = torch randn qconfig_mappings = QConfigMultiMapping \ set_global torch ao quantization default_qconfig torch ao quantization default_per_channel_qconfig _test_impl m example_input qconfig_mappings withQNNPACKBackend test_functions M nn Module __init__ - None super __init__ w = nn Parameter torch randn b = nn Parameter torch zeros torch nn init kaiming_uniform_ w a=math sqrt forward x x = F sigmoid x x = F linear x w b x = F linear x w b x = F relu x x = x + x x = torch cat x x = torch cat x x = torch cat tensors= x TODO future PR enable layernorm blocked FX graph mode quant inserting observer second arg second arg module input x = F layer_norm x x shape x = F layer_norm x x shape x = x reshape - x = F layer_norm x reshape - x shape x = torch matmul x x reshape x = torch matmul x reshape x reshape TODO future PR enable below after FX graph mode quantization handles currently supported x = F linear input=x weight=self w bias=self b x m = M eval example_input = torch randn qconfig_mappings = QConfigMultiMapping \ set_global torch ao quantization default_qconfig _test_impl m example_input qconfig_mappings withQNNPACKBackend test_partial_qconfig_mapping M nn Module __init__ - None super __init__ fc = nn Linear w = nn Parameter torch randn b = nn Parameter torch randn torch nn init kaiming_uniform_ w a=math sqrt forward x x = fc x x = F linear x w b x = F relu x x = x + x x m = M eval example_input = torch randn qconfig = torch ao quantization default_qconfig qconfig_mappings = QConfigMultiMapping \ set_object_type F linear qconfig \ set_object_type F relu qconfig _test_impl m example_input qconfig_mappings withQNNPACKBackend test_logger_enabled_and_save_activations_flags m = nn Sequential nn Linear eval example_input = torch randn qconfig_mappings = QConfigMultiMapping \ set_global torch ao quantization default_qconfig backend_config = get_native_backend_config msp = prepare_n_shadows_model m example_input qconfig_mappings backend_config _ range msp example_input _check_logger_count model exp_count_stats exp_count_comparisons mod model modules isinstance mod OutputLogger assertTrue len mod stats == exp_count_stats f stats expected len mod stats equal exp_count_stats isinstance mod OutputComparisonLogger assertTrue len mod comparisons == exp_count_comparisons f comparisons expected len mod comparisons equal exp_count_comparisons check behavior save_activations enabled msq = convert_n_shadows_model copy deepcopy msp loggers_set_enabled msq True loggers_set_save_activations msq True after prepare calibration before convert calibration loggers should have anything saved _check_logger_count msq msq example_input loggers should save each item after calibration _check_logger_count msq check behavior save_activations disabled msq = convert_n_shadows_model copy deepcopy msp loggers_set_enabled msq True loggers_set_save_activations msq False after prepare calibration before convert calibration loggers should have anything saved _check_logger_count msq msq example_input stats should empty comparisons should there _check_logger_count msq skipIfTorchDynamo too slow skip_if_no_torchvision withQNNPACKBackend test_mobilenet_v torchvision m = torchvision models quantization mobilenet_v pretrained=False quantize=False eval example_input = torch randn qconfig_mappings = QConfigMultiMapping \ set_global torch ao quantization default_qconfig torch ao quantization default_dynamic_qconfig _test_impl m example_input qconfig_mappings withQNNPACKBackend test_qconfig_multi_mapping_deduplication check insertion deduplicates qconfigs qconfig_multi_mapping = QConfigMultiMapping set_global torch ao quantization default_qconfig torch ao quantization default_qconfig assertEqual len qconfig_multi_mapping qconfig_mappings_list withQNNPACKBackend test_qconfig_multi_mapping_insert_padding test inserting higher priority qconfig style fewer elements than lower priority qconfig will result adding None extra QConfigMappings same style+key qconfig_multi_mapping = QConfigMultiMapping set_global torch ao quantization default_qconfig torch ao quantization default_dynamic_qconfig set_object_type torch nn Linear torch ao quantization default_qconfig set_module_name_regex fc torch ao quantization default_qconfig set_module_name fc torch ao quantization default_qconfig set_module_name_object_type_order nn Linear torch ao quantization default_qconfig assertEqual qconfig_multi_mapping qconfig_mappings_list object_type_qconfigs torch nn Linear None assertEqual qconfig_multi_mapping qconfig_mappings_list module_name_regex_qconfigs fc None assertEqual qconfig_multi_mapping qconfig_mappings_list module_name_qconfigs fc None assertEqual qconfig_multi_mapping qconfig_mappings_list module_name_object_type_order_qconfigs nn Linear None withQNNPACKBackend test_qconfig_multi_mapping_retroactive_padding test inserting lower priority qconfig style more elements thhan lower priority qconfig styles will result new QConfigMapping having None all previously existing styles+keys qconfig_multi_mapping = QConfigMultiMapping set_object_type torch nn Linear torch ao quantization default_qconfig set_module_name_regex fc torch ao quantization default_qconfig set_module_name fc torch ao quantization default_qconfig set_module_name_object_type_order nn Linear torch ao quantization default_qconfig set_global torch ao quantization default_qconfig torch ao quantization default_dynamic_qconfig assertEqual qconfig_multi_mapping qconfig_mappings_list object_type_qconfigs torch nn Linear None assertEqual qconfig_multi_mapping qconfig_mappings_list module_name_regex_qconfigs fc None assertEqual qconfig_multi_mapping qconfig_mappings_list module_name_qconfigs fc None assertEqual qconfig_multi_mapping qconfig_mappings_list module_name_object_type_order_qconfigs nn Linear None withQNNPACKBackend test_qconfig_multi_mapping_end_to_end test prepare convert_n_shadows_model works expected qconfig_multi_mapping avoids unwanted matches m = TwoLayerLinearModel eval example_input = m get_example_inputs qconfig_multi_mapping = QConfigMultiMapping set_global torch ao quantization default_qconfig torch ao quantization default_dynamic_qconfig set_module_name fc None torch ao quantization default_qconfig assertEqual qconfig_multi_mapping qconfig_mappings_list module_name_qconfigs fc None msq = _test_impl m example_input qconfig_multi_mapping checkQuantizedLinear msq shadow_wrapper_ _ mod_ checkDynamicQuantizedLinear msq shadow_wrapper_ _ mod_ torch qint checkQuantizedLinear msq shadow_wrapper_ _ mod_ assertRaisesRegex AttributeError lambda msq shadow_wrapper_ _ withQNNPACKBackend test_qconfig_multi_mapping_from_list test QConfigMultiMapping from_list_qconfig_mapping works expected m = TwoLayerLinearModel eval example_input = m get_example_inputs qconfig_mappings_list = QConfigMapping set_global torch ao quantization default_qconfig QConfigMapping set_global torch ao quantization default_dynamic_qconfig set_module_name fc torch ao quantization default_qconfig qconfig_multi_mapping = QConfigMultiMapping from_list_qconfig_mapping qconfig_mappings_list assertEqual qconfig_multi_mapping qconfig_mappings_list module_name_qconfigs fc None msq = _test_impl m example_input qconfig_multi_mapping checkQuantizedLinear msq shadow_wrapper_ _ mod_ checkDynamicQuantizedLinear msq shadow_wrapper_ _ mod_ torch qint checkQuantizedLinear msq shadow_wrapper_ _ mod_ assertRaisesRegex AttributeError lambda msq shadow_wrapper_ _ withQNNPACKBackend test_qconfig_multi_mapping_ordering test module ordering ignores None m = TwoLayerLinearModel eval example_input = m get_example_inputs qconfig_multi_mapping = QConfigMultiMapping set_global torch ao quantization default_qconfig torch ao quantization default_dynamic_qconfig set_module_name fc None torch ao quantization default_dynamic_qconfig torch ao quantization default_qat_qconfig_v assertEqual len qconfig_multi_mapping qconfig_mappings_list msq = _test_impl m example_input qconfig_multi_mapping checkQuantizedLinear msq shadow_wrapper_ _ mod_ checkDynamicQuantizedLinear msq shadow_wrapper_ _ mod_ torch qint checkDynamicQuantizedLinear msq shadow_wrapper_ _ mod_ torch qint checkQuantizedLinear msq shadow_wrapper_ _ mod_ withQNNPACKBackend test_qconfig_multi_mapping_repr qconfig_multi_mapping = QConfigMultiMapping set_global torch ao quantization default_qconfig torch ao quantization default_dynamic_qconfig set_module_name fc None torch ao quantization default_dynamic_qconfig torch ao quantization default_qat_qconfig_v assertTrue isinstance qconfig_multi_mapping __repr__ str withQNNPACKBackend test_custom_functions_and_tracer M nn Module __init__ - None super __init__ fc = nn Linear fc = nn Linear forward x x = fc x x = fc x x m = M eval example_inputs = torch randn qconfig_mappings = QConfigMultiMapping set_global torch ao quantization default_qat_qconfig custom_tracer = torch ao quantization quantize_fx QuantizationTracer fc custom_prepare_fn = torch ao quantization quantize_fx prepare_qat_fx custom_convert_fn module to_print print to_print mod = torch ao quantization quantize_fx convert_fx module mod backend_config = get_native_backend_config test input valid _ = m example_inputs kwargs = to_print working msp = prepare_n_shadows_model m example_inputs qconfig_mappings backend_config custom_prepare_fn=custom_prepare_fn custom_prepare_kwargs=None custom_tracer=custom_tracer _ range msp example_inputs msq = convert_n_shadows_model msp custom_convert_fn=custom_convert_fn custom_convert_kwargs=kwargs print msq loggers_set_enabled msq True msq example_inputs results = extract_results_n_shadows_model msq print_comparisons_n_shadows_model results _test_extract_weights_impl m example_input qconfig_mapping backend_config = get_native_backend_config results = _n_shadows_compare_weights m example_input qconfig_mapping backend_config print_comparisons_n_shadows_model results withQNNPACKBackend test_extract_weights_linear M nn Module __init__ - None super __init__ w = nn Parameter torch randn b = nn Parameter torch randn torch nn init kaiming_uniform_ w a=math sqrt w = nn Parameter torch randn b = nn Parameter torch randn torch nn init kaiming_uniform_ w a=math sqrt w = nn Parameter torch randn b = nn Parameter torch randn torch nn init kaiming_uniform_ w a=math sqrt w = nn Parameter torch randn b = nn Parameter torch randn torch nn init kaiming_uniform_ w a=math sqrt forward x x = F linear x w b x = F linear x w b x = F relu x x = F linear x w b x = F linear x w b x per_tensor_qconfig = torch ao quantization default_qconfig m = M eval example_input = torch randn qconfig_mapping = get_default_qconfig_mapping test unquantized qconfig_mapping set_module_name_object_type_order F linear None test per-tensor qconfig_mapping set_module_name_object_type_order F linear per_tensor_qconfig _test_extract_weights_impl m example_input qconfig_mapping _test_add_loggers_impl m example_input qconfig_mapping backend_config = get_native_backend_config m_copy = copy deepcopy m test input valid _ = m example_input msp = _prepare_n_shadows_add_loggers_model m example_input qconfig_mapping backend_config print msp msp msp example_input msq = convert_n_shadows_model msp print msq msq loggers_set_enabled msq True output_fp = msq example_input results = extract_results_n_shadows_model msq print results print_comparisons_n_shadows_model results get last quantized output results inner_results = results model node_output last_subgraph = list inner_results keys - output_shadow = inner_results last_subgraph values - verify both fp quantized output matches reference output_fp _ref = m_copy example_input mp_ref = prepare_fx m_copy qconfig_mapping example_input _ range mp_ref example_input mq_ref = convert_fx mp_ref output_shadow_ref = mq_ref example_input assertTrue torch allclose output_fp output_fp _ref f fp comparison output_fp close output_fp _ref print shadow output_shadow shape output_shadow print shadow_ref output_shadow_ref shape output_shadow_ref assertTrue torch allclose output_shadow output_shadow_ref f shadow comparison output_shadow close output_shadow_ref msq withQNNPACKBackend test_add_loggers_linear_mod_quant_quant m = nn Sequential nn Linear nn Linear example_input = torch randn qconfig_mapping = get_default_qconfig_mapping _test_add_loggers_impl m example_input qconfig_mapping withQNNPACKBackend test_add_loggers_linear_mod_fp _quant m = nn Sequential nn Linear nn Linear example_input = torch randn qconfig_mapping = get_default_qconfig_mapping qconfig_mapping set_module_name None _test_add_loggers_impl m example_input qconfig_mapping withQNNPACKBackend test_add_loggers_linear_mod_quant_fp m = nn Sequential nn Linear nn Linear example_input = torch randn qconfig_mapping = get_default_qconfig_mapping qconfig_mapping set_module_name None _test_add_loggers_impl m example_input qconfig_mapping withQNNPACKBackend test_add_loggers_linear_mod_fp _fp m = nn Sequential nn Linear nn Linear example_input = torch randn qconfig_mapping = get_default_qconfig_mapping qconfig_mapping set_module_name None qconfig_mapping set_module_name None _test_add_loggers_impl m example_input qconfig_mapping withQNNPACKBackend test_add_loggers_conv_bn_relu_fusion_quant m = nn Sequential nn Conv d nn BatchNorm d nn ReLU m eval example_input = torch randn qconfig_mapping = get_default_qconfig_mapping _test_add_loggers_impl m example_input qconfig_mapping withQNNPACKBackend test_add_loggers_conv_bn_relu_fusion_fp m = nn Sequential nn Conv d nn BatchNorm d nn ReLU m eval example_input = torch randn qconfig_mapping = get_default_qconfig_mapping qconfig_mapping set_module_name None qconfig_mapping set_module_name None qconfig_mapping set_module_name None _test_add_loggers_impl m example_input qconfig_mapping withQNNPACKBackend test_add_loggers_functions M nn Module __init__ - None super __init__ w = nn Parameter torch randn b = nn Parameter torch randn torch nn init kaiming_uniform_ w a=math sqrt forward x x = F linear x w b x = F relu x x = x + x x = x + TODO future PR support first arg being scalar x = + x x = torch cat x x x = torch cat x x x = torch cat tensors= x x function matchable quantization x = torch nn functional rrelu x x = F linear x w b x m = M eval example_input = torch randn qconfig_mapping get_default_qconfig_mapping QConfigMapping _test_add_loggers_impl m example_input qconfig_mapping skipIfTorchDynamo too slow skip_if_no_torchvision withQNNPACKBackend test_add_loggers_mobilenet_v torchvision m = torchvision models quantization mobilenet_v pretrained=False quantize=False eval example_input = torch randn qconfig_mapping = get_default_qconfig_mapping _test_add_loggers_impl m example_input qconfig_mapping TestFXNumericSuiteCoreAPIsModels FXNumericSuiteQuantizationTestCase Tests numeric suite core APIs non-toy models skipIfNoFBGEMM test_compare_weights_conv test_cases = ConvModel ConvBnModel ConvBnReLUModel m test_cases m eval example_inputs = torch randn _test_extract_weights m example_inputs results_len= skipIfNoFBGEMM test_compare_weights_linear test_cases = SingleLayerLinearModel None SingleLayerLinearDynamicModel object_type nn Linear default_dynamic_qconfig m qconfig_dict test_cases m eval example_inputs = torch randn res = _test_extract_weights m example_inputs results_len= qconfig_dict=qconfig_dict skipIfNoFBGEMM test_compare_weights_lstm_dynamic qconfig_dict = object_type nn LSTM default_dynamic_qconfig lstm_input = torch rand lstm_hidden = torch rand torch rand example_inputs = lstm_input lstm_hidden m = LSTMwithHiddenDynamicModel eval res = _test_extract_weights m example_inputs results_len= qconfig_dict=qconfig_dict skipIfNoFBGEMM test_compare_activations_conv test_cases = ConvModel ConvBnModel ConvBnReLUModel m test_cases m eval res = _test_match_activations m torch randn results_len= skipIfNoFBGEMM test_compare_activations_linear test_cases = SingleLayerLinearModel None SingleLayerLinearDynamicModel object_type nn Linear default_dynamic_qconfig m qconfig_dict test_cases m eval res = _test_match_activations m torch randn results_len= qconfig_dict=qconfig_dict skipIfNoFBGEMM test_compare_activations_lstm_dynamic qconfig_dict = object_type nn LSTM default_dynamic_qconfig m = LSTMwithHiddenDynamicModel eval lstm_input = torch rand lstm_hidden = torch rand torch rand TODO future PR enable scripting quant prepared LSTM scriptable res = _test_match_activations m lstm_input lstm_hidden results_len= qconfig_dict=qconfig_dict skip_scripting=True skipIfNoFBGEMM test_compare_shadow_activations_conv test_cases = ConvModel ConvBnModel ConvBnReLUModel m test_cases m eval res = _test_match_shadow_activations m torch randn results_len= skipIfNoFBGEMM test_compare_shadow_activations_linear test_cases = SingleLayerLinearModel None SingleLayerLinearDynamicModel object_type nn Linear default_dynamic_qconfig m qconfig_dict test_cases m eval res = _test_match_shadow_activations m torch randn results_len= qconfig_dict=qconfig_dict skipIfNoFBGEMM test_compare_shadow_activations_lstm_dynamic qconfig_dict = object_type nn LSTM default_dynamic_qconfig m = LSTMwithHiddenDynamicModel eval lstm_input = torch rand lstm_hidden = torch rand torch rand TODO future PR enable scripting quant prepared LSTM scriptable res = _test_match_shadow_activations m lstm_input lstm_hidden results_len= qconfig_dict=qconfig_dict skip_scripting=True skipIfNoFBGEMM test_sparsenn_compare_activations should_log_inputs True False sparse_nn = SparseNNModel eval idx = torch LongTensor offsets = torch LongTensor x = torch randn _test_match_activations sparse_nn idx offsets x results_len= should_log_inputs=should_log_inputs skipIfNoFBGEMM test_sparsenn_shadow should_log_inputs True False sparse_nn = SparseNNModel eval idx = torch LongTensor offsets = torch LongTensor x = torch randn _test_match_shadow_activations sparse_nn idx offsets x results_len= should_log_inputs=should_log_inputs skipIfTorchDynamo too slow skip_if_no_torchvision skipIfNoFBGEMM test_resnet torchvision m = torchvision models quantization resnet pretrained=False quantize=False eval qconfig_dict = torch ao quantization default_qconfig _test_match_shadow_activations m torch randn qconfig_dict=qconfig_dict should_log_inputs=False skipIfTorchDynamo too slow skip_if_no_torchvision skipIfNoFBGEMM test_mobilenet_v torchvision m = torchvision models quantization mobilenet_v pretrained=False quantize=False eval qconfig_dict = torch ao quantization default_qconfig _test_match_shadow_activations m torch randn qconfig_dict=qconfig_dict should_log_inputs=False __name__ == __main__ raise_on_run_directly test test_quantization py