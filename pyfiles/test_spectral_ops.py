Owner s module fft ruff noqa F torch unittest math contextlib contextmanager itertools product itertools doctest inspect torch testing _internal common_utils \ TestCase run_tests TEST_NUMPY TEST_LIBROSA TEST_MKL first_sample TEST_WITH_ROCM make_tensor skipIfTorchDynamo torch testing _internal common_device_type \ instantiate_device_type_tests ops dtypes onlyNativeDeviceTypes skipCPUIfNoFFT deviceCountAtLeast onlyCUDA OpDTypes skipIf toleranceOverride tol torch testing _internal common_methods_invocations spectral_funcs SpectralFuncType torch testing _internal common_cuda SM OrLater torch _prims_common corresponding_complex_dtype typing Optional packaging version TEST_NUMPY numpy np TEST_LIBROSA librosa has_scipy_fft = False try scipy fft has_scipy_fft = True except ModuleNotFoundError pass REFERENCE_NORM_MODES = None forward backward ortho version parse np __version__ = version parse has_scipy_fft version parse scipy __version__ = version parse None ortho _complex_stft x args kwargs Transform real imaginary components separably stft_real = torch stft x real args kwargs return_complex=True onesided=False stft_imag = torch stft x imag args kwargs return_complex=True onesided=False stft_real + j stft_imag _hermitian_conj x dim Returns hermitian conjugate along single dimension H x i = conj x -i out = torch empty_like x mid = x size dim - idx = tuple slice None out dim out idx = x idx idx_neg = list idx idx_neg dim = slice -mid None idx_neg = tuple idx_neg idx_pos = list idx idx_pos dim = slice mid + idx_pos = tuple idx_pos out idx_pos = x idx_neg flip dim out idx_neg = x idx_pos flip dim mid + x size dim idx = list idx idx dim = mid + idx = tuple idx out idx = x idx out conj _complex_istft x args kwargs Decompose into Hermitian FFT real anti-Hermitian FFT imaginary n_fft = x size - slc = Ellipsis slice None n_fft + slice None hconj = _hermitian_conj x dim=- x_hermitian = x + hconj x_antihermitian = x - hconj istft_real = torch istft x_hermitian slc args kwargs onesided=True istft_imag = torch istft - j x_antihermitian slc args kwargs onesided=True torch complex istft_real istft_imag _stft_reference x hop_length window r Reference stft implementation This doesn t implement all torch stft only STFT definition math X m \omega = \sum_n x n w n - m e^ -jn\omega n_fft = window numel X = torch empty n_fft x numel - n_fft + hop_length hop_length device=x device dtype=torch cdouble m range X size start = m hop_length start + n_fft x numel slc = torch empty n_fft device=x device dtype=x dtype tmp = x start slc tmp numel = tmp slc = x start start + n_fft X m = torch fft fft slc window X skip_helper_for_fft device dtype device_type = torch device device type dtype torch half torch complex device_type == cpu raise unittest SkipTest half complex supported CPU SM OrLater raise unittest SkipTest half complex only supported CUDA device SM Tests functions related Fourier analysis torch fft namespace TestFFT TestCase exact_dtype = True onlyNativeDeviceTypes ops op op spectral_funcs op ndimensional == SpectralFuncType OneD allowed_dtypes= torch float torch cfloat test_reference_ d device dtype op op ref None raise unittest SkipTest No reference implementation norm_modes = REFERENCE_NORM_MODES test_args = product input torch randn device=device dtype=dtype torch randn device=device dtype=dtype torch randn device=device dtype=dtype torch randn device=device dtype=dtype n None dim - norm norm_modes Test transforming middle dimensions multi-dim tensor product torch randn device=device dtype=dtype None - norm_modes iargs test_args args = list iargs input = args args = args expected = op ref input cpu numpy args exact_dtype = dtype torch double torch complex actual = op input args assertEqual actual expected exact_dtype=exact_dtype skipCPUIfNoFFT onlyNativeDeviceTypes toleranceOverride torch half tol e- e- torch chalf tol e- e- dtypes torch half torch float torch double torch complex torch complex torch complex test_fft_round_trip device dtype skip_helper_for_fft device dtype Test round trip through ifft fft x identity dtype torch half torch complex test_args = list product input torch randn device=device dtype=dtype torch randn device=device dtype=dtype torch randn device=device dtype=dtype torch randn device=device dtype=dtype dim - norm None forward backward ortho cuFFT supports powers half complex half precision test_args = list product input torch randn device=device dtype=dtype torch randn device=device dtype=dtype torch randn device=device dtype=dtype torch randn device=device dtype=dtype dim - norm None forward backward ortho fft_functions = torch fft fft torch fft ifft Real-only functions dtype is_complex NOTE Using ihfft forward transform avoid needing generate true half-complex input fft_functions += torch fft rfft torch fft irfft torch fft ihfft torch fft hfft forward backward fft_functions x dim norm test_args kwargs = n x size dim dim dim norm norm y = backward forward x kwargs kwargs x dtype torch half y dtype torch complex Since type promotion currently doesn t work complex manually promote ` x ` complex x = x torch complex For real input ifft fft x will convert complex assertEqual x y exact_dtype= forward = torch fft fft x is_complex Note NumPy will throw ValueError empty input onlyNativeDeviceTypes ops spectral_funcs allowed_dtypes= torch half torch float torch complex torch cfloat test_empty_fft device dtype op t = torch empty device=device dtype=dtype match = r Invalid number data points \ -\d \ specified assertRaisesRegex RuntimeError match op t onlyNativeDeviceTypes test_empty_ifft device t = torch empty device=device dtype=torch complex match = r Invalid number data points \ -\d \ specified f torch fft irfft torch fft irfft torch fft irfftn torch fft hfft torch fft hfft torch fft hfftn assertRaisesRegex RuntimeError match f t onlyNativeDeviceTypes test_fft_invalid_dtypes device t = torch randn device=device dtype=torch complex assertRaisesRegex RuntimeError rfft expects real input tensor torch fft rfft t assertRaisesRegex RuntimeError rfftn expects real-valued input tensor torch fft rfftn t assertRaisesRegex RuntimeError ihfft expects real input tensor torch fft ihfft t skipCPUIfNoFFT onlyNativeDeviceTypes dtypes torch int torch half torch float torch double torch complex torch complex torch complex test_fft_type_promotion device dtype skip_helper_for_fft device dtype dtype is_complex dtype is_floating_point t = torch randn device=device dtype=dtype t = torch randint - device=device dtype=dtype PROMOTION_MAP = torch int torch complex torch half torch complex torch float torch complex torch double torch complex torch complex torch complex torch complex torch complex torch complex torch complex T = torch fft fft t assertEqual T dtype PROMOTION_MAP dtype PROMOTION_MAP_C R = torch int torch float torch half torch half torch float torch float torch double torch double torch complex torch half torch complex torch float torch complex torch double dtype torch half torch complex cuFFT supports powers half complex half precision NOTE With hfft default args where output_size n= input_size - we make sure logical fft size power two x = torch randn device=device dtype=dtype R = torch fft hfft x R = torch fft hfft t assertEqual R dtype PROMOTION_MAP_C R dtype dtype is_complex PROMOTION_MAP_R C = torch int torch complex torch half torch complex torch float torch complex torch double torch complex C = torch fft rfft t assertEqual C dtype PROMOTION_MAP_R C dtype onlyNativeDeviceTypes ops spectral_funcs dtypes=OpDTypes unsupported allowed_dtypes= torch half torch bfloat test_fft_half_and_bfloat _errors device dtype op TODO Remove torch half error when complex fully implemented sample = first_sample op sample_inputs device dtype device_type = torch device device type default_msg = Unsupported dtype dtype torch half device_type == cuda TEST_WITH_ROCM err_msg = default_msg dtype torch half device_type == cuda SM OrLater err_msg = cuFFT doesn t support signals half type compute capability less than SM_ err_msg = default_msg assertRaisesRegex RuntimeError err_msg op sample input sample args sample kwargs onlyNativeDeviceTypes ops spectral_funcs allowed_dtypes= torch half torch chalf test_fft_half_and_chalf_not_power_of_two_error device dtype op t = make_tensor device=device dtype=dtype err_msg = cuFFT only supports dimensions whose sizes powers two assertRaisesRegex RuntimeError err_msg op t op ndimensional SpectralFuncType ND SpectralFuncType TwoD kwargs = s kwargs = n assertRaisesRegex RuntimeError err_msg op t kwargs nd-fft tests onlyNativeDeviceTypes unittest skipIf TEST_NUMPY NumPy found ops op op spectral_funcs op ndimensional == SpectralFuncType ND allowed_dtypes= torch cfloat torch cdouble test_reference_nd device dtype op op ref None raise unittest SkipTest No reference implementation norm_modes = REFERENCE_NORM_MODES input_ndim s dim transform_desc = product range None None - product range None None None None None None None None input_ndim s dim transform_desc shape = itertools islice itertools cycle range input_ndim input = torch randn shape device=device dtype=dtype norm norm_modes expected = op ref input cpu numpy s dim norm exact_dtype = dtype torch double torch complex actual = op input s dim norm assertEqual actual expected exact_dtype=exact_dtype skipCPUIfNoFFT onlyNativeDeviceTypes toleranceOverride torch half tol e- e- torch chalf tol e- e- dtypes torch half torch float torch double torch complex torch complex torch complex test_fftn_round_trip device dtype skip_helper_for_fft device dtype norm_modes = None forward backward ortho input_ndim dim transform_desc = product range None - None fft_functions = torch fft fftn torch fft ifftn Real-only functions dtype is_complex NOTE Using ihfftn forward transform avoid needing generate true half-complex input fft_functions += torch fft rfftn torch fft irfftn torch fft ihfftn torch fft hfftn input_ndim dim transform_desc dtype torch half torch complex cuFFT supports powers half complex half precision shape = itertools islice itertools cycle input_ndim shape = itertools islice itertools cycle range input_ndim x = torch randn shape device=device dtype=dtype forward backward norm product fft_functions norm_modes isinstance dim tuple s = x size d d dim s = x size dim None x size dim kwargs = s s dim dim norm norm y = backward forward x kwargs kwargs For real input ifftn fftn x will convert complex x dtype torch half y dtype torch chalf Since type promotion currently doesn t work complex manually promote ` x ` complex assertEqual x torch chalf y assertEqual x y exact_dtype= forward = torch fft fftn x is_complex onlyNativeDeviceTypes ops op op spectral_funcs op ndimensional == SpectralFuncType ND allowed_dtypes= torch float torch cfloat test_fftn_invalid device dtype op = torch rand device=device dtype=dtype FIXME https github com pytorch pytorch issues errMsg = dims must unique assertRaisesRegex RuntimeError errMsg op dim= assertRaisesRegex RuntimeError errMsg op dim= - assertRaisesRegex RuntimeError dim shape same length op s= dim= assertRaisesRegex IndexError Dimension out range op dim= assertRaisesRegex RuntimeError tensor only has dimensions op s= skipCPUIfNoFFT onlyNativeDeviceTypes dtypes torch half torch float torch double torch cfloat torch cdouble test_fftn_noop_transform device dtype skip_helper_for_fft device dtype RESULT_TYPE = torch half torch chalf torch float torch cfloat torch double torch cdouble op torch fft fftn torch fft ifftn torch fft fft torch fft ifft inp = make_tensor device=device dtype=dtype out = op inp dim= expect_dtype = RESULT_TYPE get inp dtype inp dtype expect = inp expect_dtype assertEqual expect out skipCPUIfNoFFT onlyNativeDeviceTypes toleranceOverride torch half tol e- e- dtypes torch half torch float torch double test_hfftn device dtype skip_helper_for_fft device dtype input_ndim dim transform_desc = product range None - None input_ndim dim transform_desc actual_dims = list range input_ndim dim None dim dtype torch half shape = tuple itertools islice itertools cycle input_ndim shape = tuple itertools islice itertools cycle range input_ndim expect = torch randn shape device=device dtype=dtype input = torch fft ifftn expect dim=dim norm= ortho lastdim = actual_dims - lastdim_size = input size lastdim + idx = slice None input_ndim idx lastdim = slice lastdim_size idx = tuple idx input = input idx s = shape dim dim actual_dims actual = torch fft hfftn input s=s dim=dim norm= ortho assertEqual expect actual skipCPUIfNoFFT onlyNativeDeviceTypes toleranceOverride torch half tol e- e- dtypes torch half torch float torch double test_ihfftn device dtype skip_helper_for_fft device dtype input_ndim dim transform_desc = product range None - None input_ndim dim transform_desc dtype torch half shape = tuple itertools islice itertools cycle input_ndim shape = tuple itertools islice itertools cycle range input_ndim input = torch randn shape device=device dtype=dtype expect = torch fft ifftn input dim=dim norm= ortho Slice off half-symmetric component lastdim = - dim None dim - lastdim_size = expect size lastdim + idx = slice None input_ndim idx lastdim = slice lastdim_size idx = tuple idx expect = expect idx actual = torch fft ihfftn input dim=dim norm= ortho assertEqual expect actual d-fft tests NOTE d transforms only thin wrappers over n-dim transforms so don t require exhaustive testing skipCPUIfNoFFT onlyNativeDeviceTypes dtypes torch double torch complex test_fft _numpy device dtype norm_modes = REFERENCE_NORM_MODES input_ndim s transform_desc = product range None fft_functions = fft ifft irfft hfft dtype is_floating_point fft_functions += rfft ihfft input_ndim s transform_desc shape = itertools islice itertools cycle range input_ndim input = torch randn shape device=device dtype=dtype fname norm product fft_functions norm_modes torch_fn = getattr torch fft fname hfft fname has_scipy_fft continue Requires scipy compare against numpy_fn = getattr scipy fft fname numpy_fn = getattr np fft fname fn t torch Tensor s Optional list int dim list int = - - norm Optional str = None torch_fn t s dim norm torch_fns = torch_fn torch jit script fn Once dim defaulted input_np = input cpu numpy expected = numpy_fn input_np s norm=norm fn torch_fns actual = fn input s norm=norm assertEqual actual expected Once explicit dims dim = expected = numpy_fn input_np s dim norm fn torch_fns actual = fn input s dim norm assertEqual actual expected skipCPUIfNoFFT onlyNativeDeviceTypes dtypes torch float torch complex test_fft _fftn_equivalence device dtype norm_modes = None forward backward ortho input_ndim s dim transform_desc = product range None None None fft_functions = fft ifft irfft hfft Real-only functions dtype is_floating_point fft_functions += rfft ihfft input_ndim s dim transform_desc shape = itertools islice itertools cycle range input_ndim x = torch randn shape device=device dtype=dtype func norm product fft_functions norm_modes f d = getattr torch fft func + fnd = getattr torch fft func + n kwargs = s s norm norm dim None kwargs dim = dim expect = fnd x kwargs expect = fnd x dim= - - kwargs actual = f d x kwargs assertEqual actual expect skipCPUIfNoFFT onlyNativeDeviceTypes test_fft _invalid device = torch rand device=device fft_funcs = torch fft fft torch fft ifft torch fft rfft torch fft irfft func fft_funcs assertRaisesRegex RuntimeError dims must unique func dim= assertRaisesRegex RuntimeError dims must unique func dim= - assertRaisesRegex RuntimeError dim shape same length func s= assertRaisesRegex IndexError Dimension out range func dim= c = torch complex assertRaisesRegex RuntimeError rfftn expects real-valued input torch fft rfft c Helper functions skipCPUIfNoFFT onlyNativeDeviceTypes unittest skipIf TEST_NUMPY NumPy found dtypes torch float torch double test_fftfreq_numpy device dtype test_args = product n range d None functions = fftfreq rfftfreq fname functions torch_fn = getattr torch fft fname numpy_fn = getattr np fft fname n d test_args args = n d None n d expected = numpy_fn args actual = torch_fn args device=device dtype=dtype assertEqual actual expected exact_dtype=False skipCPUIfNoFFT onlyNativeDeviceTypes dtypes torch float torch double test_fftfreq_out device dtype func torch fft fftfreq torch fft rfftfreq expect = func n= d= device=device dtype=dtype actual = torch empty device=device dtype=dtype assertWarnsRegex UserWarning out tensor will resized func n= d= out=actual assertEqual actual expect skipCPUIfNoFFT onlyNativeDeviceTypes unittest skipIf TEST_NUMPY NumPy found dtypes torch float torch double torch complex torch complex test_fftshift_numpy device dtype test_args = shape dim product None - product None - product None functions = fftshift ifftshift shape dim test_args input = torch rand shape device=device dtype=dtype input_np = input cpu numpy fname functions torch_fn = getattr torch fft fname numpy_fn = getattr np fft fname expected = numpy_fn input_np axes=dim actual = torch_fn input dim=dim assertEqual actual expected skipCPUIfNoFFT onlyNativeDeviceTypes unittest skipIf TEST_NUMPY NumPy found dtypes torch float torch double test_fftshift_frequencies device dtype n range sorted_fft_freqs = torch arange - n n - n device=device dtype=dtype x = torch fft fftfreq n d= n device=device dtype=dtype Test fftshift sorts fftfreq output shifted = torch fft fftshift x assertEqual shifted shifted sort values assertEqual sorted_fft_freqs shifted And ifftshift inverse assertEqual x torch fft ifftshift shifted Legacy fft tests _test_fft_ifft_rfft_irfft device dtype complex_dtype = corresponding_complex_dtype dtype _test_complex sizes signal_ndim prepro_fn=lambda x x x = prepro_fn torch randn sizes dtype=complex_dtype device=device dim = tuple range -signal_ndim norm ortho None res = torch fft fftn x dim=dim norm=norm rec = torch fft ifftn res dim=dim norm=norm assertEqual x rec atol= e- rtol= msg= fft ifft res = torch fft ifftn x dim=dim norm=norm rec = torch fft fftn res dim=dim norm=norm assertEqual x rec atol= e- rtol= msg= ifft fft _test_real sizes signal_ndim prepro_fn=lambda x x x = prepro_fn torch randn sizes dtype=dtype device=device signal_numel = signal_sizes = x size -signal_ndim dim = tuple range -signal_ndim norm None ortho res = torch fft rfftn x dim=dim norm=norm rec = torch fft irfftn res s=signal_sizes dim=dim norm=norm assertEqual x rec atol= e- rtol= msg= rfft irfft res = torch fft fftn x dim=dim norm=norm rec = torch fft ifftn res dim=dim norm=norm x_complex = torch complex x torch zeros_like x assertEqual x_complex rec atol= e- rtol= msg= fft ifft real contiguous case _test_real _test_real _test_real _test_real _test_real _test_real _test_complex _test_complex _test_complex _test_complex _test_complex _test_complex non-contiguous case _test_real lambda x x narrow input aligned complex type _test_real lambda x x _test_real lambda x x t _test_real lambda x x view _test_real lambda x x _test_real lambda x x transpose transpose _test_complex lambda x x expand _test_complex lambda x x narrow _test_complex lambda x x transpose select _test_complex lambda x x skipCPUIfNoFFT onlyNativeDeviceTypes dtypes torch double test_fft_ifft_rfft_irfft device dtype _test_fft_ifft_rfft_irfft device dtype deviceCountAtLeast onlyCUDA dtypes torch double test_cufft_plan_cache devices dtype contextmanager plan_cache_max_size device n device None plan_cache = torch backends cuda cufft_plan_cache plan_cache = torch backends cuda cufft_plan_cache device original = plan_cache max_size plan_cache max_size = n try yield finally plan_cache max_size = original plan_cache_max_size devices max torch backends cuda cufft_plan_cache size - _test_fft_ifft_rfft_irfft devices dtype plan_cache_max_size devices _test_fft_ifft_rfft_irfft devices dtype torch backends cuda cufft_plan_cache clear check stll works after clearing cache plan_cache_max_size devices _test_fft_ifft_rfft_irfft devices dtype assertRaisesRegex RuntimeError r must non-negative torch backends cuda cufft_plan_cache max_size = - assertRaisesRegex RuntimeError r read-only property torch backends cuda cufft_plan_cache size = - assertRaisesRegex RuntimeError r got device index torch backends cuda cufft_plan_cache torch cuda device_count + Multigpu tests len devices Test different GPU has different cache x = torch randn device=devices x = x devices assertEqual torch fft rfftn x dim= - - torch fft rfftn x dim= - - If plan used across different devices following line assert above would trigger illegal memory access Other ways trigger error include setting CUDA_LAUNCH_BLOCKING= pytorch pytorch# printing device tensor x copy_ x Test un-indexed ` torch backends cuda cufft_plan_cache ` uses current device plan_cache_max_size devices plan_cache_max_size devices assertEqual torch backends cuda cufft_plan_cache max_size assertEqual torch backends cuda cufft_plan_cache max_size assertEqual torch backends cuda cufft_plan_cache max_size default cuda torch cuda device devices assertEqual torch backends cuda cufft_plan_cache max_size default cuda torch cuda device devices assertEqual torch backends cuda cufft_plan_cache max_size default cuda assertEqual torch backends cuda cufft_plan_cache max_size torch cuda device devices plan_cache_max_size None default cuda assertEqual torch backends cuda cufft_plan_cache max_size assertEqual torch backends cuda cufft_plan_cache max_size assertEqual torch backends cuda cufft_plan_cache max_size default cuda torch cuda device devices assertEqual torch backends cuda cufft_plan_cache max_size default cuda assertEqual torch backends cuda cufft_plan_cache max_size default cuda onlyCUDA dtypes torch cfloat torch cdouble test_cufft_context device dtype Regression test https github com pytorch pytorch issues x = torch randn dtype=dtype device=device requires_grad=True dout = torch zeros dtype=dtype device=device compute iFFT FFT x out = torch fft ifft torch fft fft x out backward dout retain_graph=True dx = torch fft fft torch fft ifft dout assertTrue x grad - dx abs max == assertFalse x grad - x abs max == passes ROCm w python fails w python skipIfTorchDynamo cannot set WRITEABLE flag True array skipCPUIfNoFFT onlyNativeDeviceTypes dtypes torch double test_stft device dtype TEST_LIBROSA raise unittest SkipTest librosa found librosa_stft x n_fft hop_length win_length window center window None window = np ones n_fft win_length None win_length window = window cpu numpy input_ d = x dim == input_ d x = x view - NOTE librosa changed default pad_mode constant zero padding however we use pre- default reflect pad_mode = reflect result = xi x ri = librosa stft xi cpu numpy n_fft=n_fft hop_length=hop_length win_length=win_length window=window center=center pad_mode=pad_mode result append torch from_numpy np stack ri real ri imag - result = torch stack result input_ d result = result result _test sizes n_fft hop_length=None win_length=None win_sizes=None center=True expected_error=None x = torch randn sizes dtype=dtype device=device win_sizes None window = torch randn win_sizes dtype=dtype device=device window = None expected_error None result = x stft n_fft hop_length win_length window center=center return_complex=False NB librosa defaults np complex output no matter what input dtype ref_result = librosa_stft x n_fft hop_length win_length window center assertEqual result ref_result atol= e- rtol= msg= stft comparison against librosa exact_dtype=False With return_complex=True result same viewed complex instead real result_complex = x stft n_fft hop_length win_length window center=center return_complex=True assertEqual result_complex torch view_as_complex result assertRaises expected_error lambda x stft n_fft hop_length win_length window center=center center True False _test center=center _test center=center _test center=center _test center=center _test win_sizes= center=center _test win_sizes= center=center spectral oversample _test win_length= center=center _test win_length= center=center _test expected_error=RuntimeError _test center=False expected_error=RuntimeError _test - expected_error=RuntimeError _test win_length= expected_error=RuntimeError _test win_sizes= expected_error=RuntimeError _test win_sizes= expected_error=RuntimeError skipIfTorchDynamo double skipCPUIfNoFFT onlyNativeDeviceTypes dtypes torch double test_istft_against_librosa device dtype TEST_LIBROSA raise unittest SkipTest librosa found librosa_istft x n_fft hop_length win_length window length center window None window = np ones n_fft win_length None win_length window = window cpu numpy librosa istft x cpu numpy n_fft=n_fft hop_length=hop_length win_length=win_length length=length window=window center=center _test size n_fft hop_length=None win_length=None win_sizes=None length=None center=True x = torch randn size dtype=dtype device=device win_sizes None window = torch randn win_sizes dtype=dtype device=device window = None x_stft = x stft n_fft hop_length win_length window center=center onesided=True return_complex=True ref_result = librosa_istft x_stft n_fft hop_length win_length window length center result = x_stft istft n_fft hop_length win_length window length=length center=center assertEqual result ref_result center True False _test center=center _test center=center _test center=center length= _test center=center _test center=center _test center=center length= _test win_sizes= center=center _test win_sizes= center=center _test win_sizes= center=center length= onlyNativeDeviceTypes skipCPUIfNoFFT dtypes torch double torch cdouble test_complex_stft_roundtrip device dtype test_args = list product input torch randn device=device dtype=dtype torch randn device=device dtype=dtype torch randn device=device dtype=dtype n_fft hop_length None center True pad_mode constant reflect circular normalized True False onesided True False dtype is_complex False args test_args x n_fft hop_length center pad_mode normalized onesided = args common_kwargs = n_fft n_fft hop_length hop_length center center normalized normalized onesided onesided Functional interface x_stft = torch stft x pad_mode=pad_mode return_complex=True common_kwargs x_roundtrip = torch istft x_stft return_complex=dtype is_complex length=x size - common_kwargs assertEqual x_roundtrip x Tensor method interface x_stft = x stft pad_mode=pad_mode return_complex=True common_kwargs x_roundtrip = torch istft x_stft return_complex=dtype is_complex length=x size - common_kwargs assertEqual x_roundtrip x onlyNativeDeviceTypes skipCPUIfNoFFT dtypes torch double torch cdouble test_stft_roundtrip_complex_window device dtype test_args = list product input torch randn device=device dtype=dtype torch randn device=device dtype=dtype torch randn device=device dtype=dtype n_fft hop_length None pad_mode constant reflect replicate circular normalized True False args test_args x n_fft hop_length pad_mode normalized = args window = torch rand n_fft device=device dtype=torch cdouble x_stft = torch stft x n_fft=n_fft hop_length=hop_length window=window center=True pad_mode=pad_mode normalized=normalized assertEqual x_stft dtype torch cdouble assertEqual x_stft size - n_fft Not onesided x_roundtrip = torch istft x_stft n_fft=n_fft hop_length=hop_length window=window center=True normalized=normalized length=x size - return_complex=True assertEqual x_stft dtype torch cdouble dtype is_complex assertEqual x_roundtrip imag torch zeros_like x_roundtrip imag atol= e- rtol= assertEqual x_roundtrip real x assertEqual x_roundtrip x skipCPUIfNoFFT dtypes torch cdouble test_complex_stft_definition device dtype test_args = list product input torch randn device=device dtype=dtype torch randn device=device dtype=dtype n_fft hop_length args test_args window = torch randn args device=device dtype=dtype expected = _stft_reference args args window actual = torch stft args window=window center=False assertEqual actual expected onlyNativeDeviceTypes skipCPUIfNoFFT dtypes torch cdouble test_complex_stft_real_equiv device dtype test_args = list product input torch rand device=device dtype=dtype torch rand device=device dtype=dtype torch rand device=device dtype=dtype torch rand device=device dtype=dtype n_fft hop_length None win_length None center False True pad_mode constant reflect circular normalized True False args test_args x n_fft hop_length win_length center pad_mode normalized = args expected = _complex_stft x n_fft hop_length=hop_length win_length=win_length pad_mode=pad_mode center=center normalized=normalized actual = torch stft x n_fft hop_length=hop_length win_length=win_length pad_mode=pad_mode center=center normalized=normalized assertEqual expected actual skipCPUIfNoFFT dtypes torch cdouble test_complex_istft_real_equiv device dtype test_args = list product input torch rand device=device dtype=dtype torch rand device=device dtype=dtype torch rand device=device dtype=dtype hop_length None center False True normalized True False args test_args x hop_length center normalized = args n_fft = x size - expected = _complex_istft x n_fft hop_length=hop_length center=center normalized=normalized actual = torch istft x n_fft hop_length=hop_length center=center normalized=normalized return_complex=True assertEqual expected actual skipCPUIfNoFFT test_complex_stft_onesided device stft complex input cannot onesided x_dtype window_dtype product torch double torch cdouble repeat= x = torch rand device=device dtype=x_dtype window = torch rand device=device dtype=window_dtype x_dtype is_complex window_dtype is_complex assertRaisesRegex RuntimeError complex x stft window=window pad_mode= constant onesided=True y = x stft window=window pad_mode= constant onesided=True return_complex=True assertEqual y dtype torch cdouble assertEqual y size x = torch rand device=device dtype=torch cdouble assertRaisesRegex RuntimeError complex x stft pad_mode= constant onesided=True stft currently warning requires return-complex while upgrader written onlyNativeDeviceTypes skipCPUIfNoFFT test_stft_requires_complex device x = torch rand assertRaisesRegex RuntimeError stft requires return_complex parameter y = x stft pad_mode= constant onlyNativeDeviceTypes skipCPUIfNoFFT test_stft_align_to_window_only_requires_non_center device x = torch rand align_to_window True False assertRaisesRegex RuntimeError stft align_to_window should only set when center = false y = x stft center=True return_complex=True align_to_window=align_to_window stft istft currently warning window provided onlyNativeDeviceTypes skipCPUIfNoFFT test_stft_requires_window device x = torch rand assertWarnsOnceRegex UserWarning A window provided y = x stft pad_mode= constant return_complex=True onlyNativeDeviceTypes skipCPUIfNoFFT test_istft_requires_window device stft = torch rand dtype=torch cdouble = n_fft + = number frames assertWarnsOnceRegex UserWarning A window provided x = torch istft stft n_fft= length= skipCPUIfNoFFT test_fft_input_modification device FFT functions should modify their input gh- signal = torch ones device=device signal_copy = signal clone spectrum = torch fft fftn signal dim= - - assertEqual signal signal_copy spectrum_copy = spectrum clone _ = torch fft ifftn spectrum dim= - - assertEqual spectrum spectrum_copy half_spectrum = torch fft rfftn signal dim= - - assertEqual signal signal_copy half_spectrum_copy = half_spectrum clone _ = torch fft irfftn half_spectrum_copy s= dim= - - assertEqual half_spectrum half_spectrum_copy onlyNativeDeviceTypes skipCPUIfNoFFT test_fft_plan_repeatable device Regression test gh- gh- n = torch randn n device=device dtype=torch complex res = torch fft fftn res = torch fft fftn clone assertEqual res res = torch randn n device=device dtype=torch float res = torch fft rfft res = torch fft rfft clone assertEqual res res onlyNativeDeviceTypes skipCPUIfNoFFT dtypes torch double test_istft_round_trip_simple_cases device dtype stft - istft should recover original signale _test input n_fft length stft = torch stft input n_fft=n_fft return_complex=True inverse = torch istft stft n_fft=n_fft length=length assertEqual input inverse exact_dtype=True _test torch ones dtype=dtype device=device _test torch zeros dtype=dtype device=device onlyNativeDeviceTypes skipCPUIfNoFFT dtypes torch double test_istft_round_trip_various_params device dtype stft - istft should recover original signale _test_istft_is_inverse_of_stft stft_kwargs generates random sound signal each tril then does stft istft operation check whether we can reconstruct signal data_sizes = num_trials = istft_kwargs = stft_kwargs copy del istft_kwargs pad_mode sizes data_sizes _ range num_trials original = torch randn sizes dtype=dtype device=device stft = torch stft original return_complex=True stft_kwargs inversed = torch istft stft length=original size istft_kwargs assertEqual inversed original msg= istft comparison against original atol= e- rtol= exact_dtype=True patterns = hann_window centered normalized onesided n_fft hop_length win_length window torch hann_window dtype=dtype device=device center True pad_mode reflect normalized True onesided True hann_window centered normalized onesided n_fft hop_length win_length window torch hann_window dtype=dtype device=device center True pad_mode reflect normalized False onesided False hamming_window centered normalized onesided n_fft hop_length win_length window torch hamming_window dtype=dtype device=device center True pad_mode constant normalized True onesided False hamming_window centered normalized onesided window same size n_fft n_fft hop_length win_length window torch hamming_window dtype=dtype device=device center True pad_mode constant normalized False onesided True pattern patterns _test_istft_is_inverse_of_stft pattern onlyNativeDeviceTypes skipCPUIfNoFFT dtypes torch double test_istft_round_trip_with_padding device dtype long hop_length centered may cause length mismatch inversed signal _test_istft_is_inverse_of_stft_with_padding stft_kwargs generates random sound signal each tril then does stft istft operation check whether we can reconstruct signal num_trials = sizes = stft_kwargs size del stft_kwargs size istft_kwargs = stft_kwargs copy del istft_kwargs pad_mode _ range num_trials original = torch randn sizes dtype=dtype device=device stft = torch stft original return_complex=True stft_kwargs assertWarnsOnceRegex UserWarning The length signal shorter than length parameter inversed = torch istft stft length=original size - istft_kwargs n_frames = stft size - stft_kwargs center True len_expected = stft_kwargs n_fft + stft_kwargs hop_length n_frames - len_expected = stft_kwargs n_fft + stft_kwargs hop_length n_frames - trim original case when constructed signal shorter than original padding = inversed len_expected inversed = inversed len_expected original = original len_expected test padding points inversed signal all zeros zeros = torch zeros_like padding device=padding device assertEqual padding zeros msg= istft padding values against zeros atol= e- rtol= exact_dtype=True assertEqual inversed original msg= istft comparison against original atol= e- rtol= exact_dtype=True patterns = hamming_window centered normalized onesided window same size n_fft size n_fft hop_length win_length window torch hamming_window dtype=dtype device=device center False pad_mode reflect normalized False onesided False hamming_window centered normalized onesided long hop_length window same size n_fft size n_fft hop_length win_length window torch hamming_window dtype=dtype device=device center True pad_mode constant normalized False onesided True pattern patterns _test_istft_is_inverse_of_stft_with_padding pattern onlyNativeDeviceTypes test_istft_throws device istft should throw exception invalid parameters stft = torch zeros device=device window size hops so there gap which throw error assertRaises RuntimeError torch istft stft n_fft= hop_length= win_length= window=torch ones A window zeros does meet NOLA invalid_window = torch zeros device=device assertRaises RuntimeError torch istft stft n_fft= win_length= window=invalid_window Input cannot empty assertRaises RuntimeError torch istft torch zeros assertRaises RuntimeError torch istft torch zeros skipIfTorchDynamo Failed running call_function onlyNativeDeviceTypes skipCPUIfNoFFT dtypes torch double test_istft_of_sine device dtype complex_dtype = corresponding_complex_dtype dtype _test amplitude L n stft amplitude sin pi L n x hop length window size equaling L x = torch arange L + device=device dtype=dtype original = amplitude torch sin math pi L x n stft = torch stft original L hop_length=L win_length=L window=torch ones L center=False normalized=False stft = torch zeros L + device=device dtype=complex_dtype stft_largest_val = amplitude L n stft size stft n imag = torch tensor -stft_largest_val dtype=dtype = L - n stft size symmetric about L stft L - n imag = torch tensor stft_largest_val dtype=dtype inverse = torch istft stft L hop_length=L win_length=L window=torch ones L device=device dtype=dtype center=False normalized=False There larger error due scaling amplitude original = original inverse size - assertEqual inverse original atol= e- rtol= _test amplitude= L= n= _test amplitude= L= n= _test amplitude= L= n= _test amplitude= L= n= _test amplitude= L= n= _test amplitude= L= n= _test amplitude= L= n= onlyNativeDeviceTypes skipCPUIfNoFFT dtypes torch double test_istft_linearity device dtype num_trials = complex_dtype = corresponding_complex_dtype dtype _test data_size kwargs _ range num_trials tensor = torch randn data_size device=device dtype=complex_dtype tensor = torch randn data_size device=device dtype=complex_dtype b = torch rand dtype=dtype device=device Also compare method vs functional call signature istft = tensor istft kwargs istft = tensor istft kwargs istft = istft + b istft estimate = torch istft tensor + b tensor kwargs assertEqual istft estimate atol= e- rtol= patterns = hann_window centered normalized onesided n_fft window torch hann_window device=device dtype=dtype center True normalized True onesided True hann_window centered normalized onesided n_fft window torch hann_window device=device dtype=dtype center True normalized False onesided False hamming_window centered normalized onesided n_fft window torch hamming_window device=device dtype=dtype center True normalized True onesided False hamming_window centered normalized onesided n_fft window torch hamming_window device=device dtype=dtype center False normalized False onesided True data_size kwargs patterns _test data_size kwargs onlyNativeDeviceTypes skipCPUIfNoFFT test_batch_istft device original = torch tensor device=device dtype=torch complex single = original repeat multi = original repeat i_original = torch istft original n_fft= length= i_single = torch istft single n_fft= length= i_multi = torch istft multi n_fft= length= assertEqual i_original repeat i_single atol= e- rtol= exact_dtype=True assertEqual i_original repeat i_multi atol= e- rtol= exact_dtype=True onlyCUDA skipIf TEST_MKL Test requires MKL test_stft_window_device device Test i stft window must same device input x = torch randn dtype=torch complex window = torch randn dtype=torch complex assertRaisesRegex RuntimeError stft input window must same device torch stft x n_fft= window=window device assertRaisesRegex RuntimeError stft input window must same device torch stft x device n_fft= window=window X = torch stft x n_fft= window=window assertRaisesRegex RuntimeError istft input window must same device torch istft X n_fft= window=window device assertRaisesRegex RuntimeError istft input window must same device torch istft x device n_fft= window=window FFTDocTestFinder The default doctest finder doesn t like function __module__ doesn t match torch fft It assumes functions leaked imports __init__ - None parser = doctest DocTestParser find obj name=None module=None globs=None extraglobs=None doctests = modname = name name None obj __name__ globs = globs None globs fname obj __all__ func = getattr obj fname inspect isroutine func qualname = modname + + fname docstring = inspect getdoc func docstring None continue examples = parser get_doctest docstring globs=globs name=fname filename=None lineno=None doctests append examples doctests TestFFTDocExamples TestCase pass generate_doc_test doc_test test device assertEqual device cpu runner = doctest DocTestRunner runner run doc_test runner failures = runner summarize fail Doctest failed setattr TestFFTDocExamples test_ + doc_test name skipCPUIfNoFFT test doc_test FFTDocTestFinder find torch fft globs=dict torch=torch generate_doc_test doc_test instantiate_device_type_tests TestFFT globals instantiate_device_type_tests TestFFTDocExamples globals only_for= cpu __name__ == __main__ run_tests