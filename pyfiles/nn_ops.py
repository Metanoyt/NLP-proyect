torch torch nn nn torch nn functional F https pytorch org docs stable nn html NNConvolutionModule torch nn Module __init__ - None super __init__ input d = torch randn input d = torch randn input d = torch randn module d = nn ModuleList nn Conv d nn ConvTranspose d nn Fold output_size= kernel_size= module d = nn ModuleList nn Conv d nn ConvTranspose d nn Unfold kernel_size= module d = nn ModuleList nn Conv d nn ConvTranspose d forward len module input d i module enumerate module d module input d i module enumerate module d module input d i module enumerate module d NNPoolingModule torch nn Module __init__ - None super __init__ input d = torch randn module d = nn ModuleList nn MaxPool d stride= nn AvgPool d stride= nn LPPool d stride= nn AdaptiveMaxPool d nn AdaptiveAvgPool d input d = torch randn module d = nn ModuleList nn MaxPool d stride= nn AvgPool d stride= nn FractionalMaxPool d output_ratio= nn LPPool d stride= nn AdaptiveMaxPool d nn AdaptiveAvgPool d input d = torch randn module d = nn ModuleList nn MaxPool d nn AvgPool d nn FractionalMaxPool d output_ratio= nn AdaptiveMaxPool d nn AdaptiveAvgPool d TODO max_unpool forward len module input d i module enumerate module d module input d i module enumerate module d module input d i module enumerate module d NNPaddingModule torch nn Module __init__ - None super __init__ input d = torch randn module d = nn ModuleList nn ReflectionPad d nn ReplicationPad d nn ConstantPad d input d = torch randn module d = nn ModuleList nn ReflectionPad d nn ReplicationPad d nn ZeroPad d nn ConstantPad d input d = torch randn module d = nn ModuleList nn ReflectionPad d nn ReplicationPad d nn ConstantPad d forward len module input d i module enumerate module d module input d i module enumerate module d module input d i module enumerate module d NNNormalizationModule torch nn Module __init__ - None super __init__ input d = torch randn module d = nn ModuleList nn BatchNorm d nn InstanceNorm d input d = torch randn module d = nn ModuleList nn BatchNorm d nn GroupNorm nn InstanceNorm d nn LayerNorm nn LocalResponseNorm input d = torch randn module d = nn ModuleList nn BatchNorm d nn InstanceNorm d nn ChannelShuffle forward len module input d i module enumerate module d module input d i module enumerate module d module input d i module enumerate module d NNActivationModule torch nn Module __init__ - None super __init__ activations = nn ModuleList nn ELU nn Hardshrink nn Hardsigmoid nn Hardtanh nn Hardswish nn LeakyReLU nn LogSigmoid nn MultiheadAttention nn PReLU nn ReLU nn ReLU nn RReLU nn SELU nn CELU nn GELU nn Sigmoid nn SiLU nn Mish nn Softplus nn Softshrink nn Softsign nn Tanh nn Tanhshrink nn Threshold nn GLU nn Softmin nn Softmax nn Softmax d nn LogSoftmax nn AdaptiveLogSoftmaxWithLoss forward input = torch randn len module input i module enumerate activations NNRecurrentModule torch nn Module __init__ - None super __init__ rnn = nn ModuleList nn RNN nn RNNCell gru = nn ModuleList nn GRU nn GRUCell lstm = nn ModuleList nn LSTM nn LSTMCell forward input = torch randn h = torch randn c = torch randn r = rnn input h r = rnn input h r = gru input h r = gru input h r = lstm input h c r = lstm input h c len r NNTransformerModule torch nn Module __init__ - None super __init__ transformers = nn ModuleList nn Transformer d_model= nhead= num_encoder_layers= num_decoder_layers= nn TransformerEncoder nn TransformerEncoderLayer d_model= nhead= num_layers= nn TransformerDecoder nn TransformerDecoderLayer d_model= nhead= num_layers= forward input = torch rand tgt = torch rand r = transformers input tgt r = transformers input r = transformers input tgt len r NNLinearModule torch nn Module __init__ - None super __init__ linears = nn ModuleList nn Identity nn Linear nn Bilinear nn LazyLinear forward input = torch randn r = linears input r = linears input r = linears input input len r NNDropoutModule torch nn Module forward = torch randn b = torch randn c = torch randn len F dropout F dropout d b F dropout d c F alpha_dropout F feature_alpha_dropout c NNSparseModule torch nn Module forward input = torch tensor input = torch tensor embedding_matrix = torch rand offsets = torch tensor len F embedding input embedding_matrix F embedding_bag input embedding_matrix offsets F one_hot torch arange num_classes= NNDistanceModule torch nn Module forward = torch randn b = torch randn len F pairwise_distance b F cosine_similarity b F pdist NNLossFunctionModule torch nn Module __init__ - None super __init__ x = torch FloatTensor y = torch LongTensor - forward = torch randn b = torch rand c = torch rand log_probs = torch randn log_softmax detach targets = torch randint dtype=torch long input_lengths = torch full dtype=torch long target_lengths = torch randint dtype=torch long len F binary_cross_entropy torch sigmoid b F binary_cross_entropy_with_logits torch sigmoid b F poisson_nll_loss b F cosine_embedding_loss b c F cross_entropy b F ctc_loss log_probs targets input_lengths target_lengths F gaussian_nll_loss b torch ones ENTER supported mobile module F hinge_embedding_loss b F kl_div b F l _loss b F mse_loss b F margin_ranking_loss c c c F multilabel_margin_loss x y F multilabel_soft_margin_loss x y F multi_margin_loss x torch tensor F nll_loss torch tensor F huber_loss b F smooth_l _loss b F soft_margin_loss b F triplet_margin_loss b -b F triplet_margin_with_distance_loss b -b can t take variable number arguments NNVisionModule torch nn Module __init__ - None super __init__ input = torch randn vision_modules = nn ModuleList nn PixelShuffle nn PixelUnshuffle nn Upsample scale_factor= mode= nearest nn Upsample scale_factor= mode= bilinear nn Upsample scale_factor= mode= bicubic nn UpsamplingNearest d scale_factor= nn UpsamplingBilinear d scale_factor= linear_sample = nn Upsample scale_factor= mode= linear trilinear_sample = nn Upsample scale_factor= mode= trilinear forward input = torch randn module vision_modules r = module input len r linear_sample torch randn trilinear_sample torch randn F grid_sample input torch ones NNShuffleModule torch nn Module __init__ - None super __init__ shuffle = nn ChannelShuffle forward len shuffle torch randn NNUtilsModule torch nn Module __init__ - None super __init__ flatten = nn Sequential nn Linear nn Unflatten forward = torch tensor torch tensor b = nn utils rnn pad_sequence batch_first=True c = nn utils rnn pack_padded_sequence b batch_first=True lengths=torch tensor input = torch randn len flatten input b