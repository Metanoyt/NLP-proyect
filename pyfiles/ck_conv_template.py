mypy allow-untyped-defs copy logging random typing Any typing_extensions override torch _inductor virtualized V rocm_template ArgInfo try ck inductor type ignore except ImportError ck inductor = None ck inductor None ck inductor grouped_conv_fwd gen_instances type ignore gen_conv_ops_library ck inductor grouped_conv_fwd op type ignore noqa TCH CKGroupedConvFwdOp gen_conv_ops_library torch _inductor config torch _inductor codegen rocm ck_template CKTemplate torch _inductor codegen rocm rocm_kernel ROCmTemplateKernel torch _inductor utils IndentedBuffer log = logging getLogger __name__ torch_layout_to_ck_layouts torch_layout logically torch tensors always NCHW channels-last memory layout visible strides V graph sizevars statically_known_equals torch_layout stride - when input output NCHW NB torch conv d result always NCHW NGCHW GKCYX NGKHW V graph sizevars statically_known_equals torch_layout stride - when input output weight channels-last NHWGC GKYXC NHWGK None torch_layout_to_ck_input_layout torch_layout V graph sizevars statically_known_equals torch_layout stride - NGCHW V graph sizevars statically_known_equals torch_layout stride - NHWGC None torch_layout_to_ck_weight_layout torch_layout V graph sizevars statically_known_equals torch_layout stride - GKCYX V graph sizevars statically_known_equals torch_layout stride - GKYXC None torch_layout_to_ck_output_layout torch_layout V graph sizevars statically_known_equals torch_layout stride - NGKHW V graph sizevars statically_known_equals torch_layout stride - NHWGK None CKGroupedConvFwdTemplate CKTemplate conv_template = r headers globals instance_definition extern C PT_EXPORT kernel_definition auto conv = instance_type auto invoker = conv MakeInvoker using ck index_t constexpr index_t NumDTensor = n_d_tensors constexpr index_t NDimSpatial = n_dim_spatial const std vector index_t FilterSize = FilterSize_ FilterSize_ const std vector index_t InputSize = InputSize_ InputSize_ const std vector index_t ConvolutionStrides = ConvolutionStrides_ ConvolutionStrides_ const std vector index_t Dilations = Dilations_ Dilations_ const std vector index_t LeftPads = LeftPads_ LeftPads_ const std vector index_t RightPads = RightPads_ RightPads_ auto conv_param = ck utils conv ConvParam NDimSpatial GroupCount NBatch NOutChannels NInChannels FilterSize InputSize ConvolutionStrides Dilations LeftPads RightPads using InLayout = ck tensor_layout convolution input_layout using WeiLayout = ck tensor_layout convolution weight_layout using OutLayout = ck tensor_layout convolution output_layout const auto in_g_n_c_wis_desc = ck utils conv make_input_host_tensor_descriptor_g_n_c_wis_packed InLayout conv_param const auto wei_g_k_c_xs_desc = ck utils conv make_weight_host_tensor_descriptor_g_k_c_xs_packed WeiLayout conv_param const auto out_g_n_k_wos_desc = ck utils conv make_output_host_tensor_descriptor_g_n_k_wos_packed OutLayout conv_param const void p_a = input const void p_b = weight const std array const void NumDTensor p_ds void p_e = output std array index_t NDimSpatial + a_g_n_c_wis_lengths std array index_t NDimSpatial + a_g_n_c_wis_strides std array index_t NDimSpatial + b_g_k_c_xs_lengths std array index_t NDimSpatial + b_g_k_c_xs_strides std array std array index_t NDimSpatial + NumDTensor ds_g_n_k_wos_lengths std array std array index_t NDimSpatial + NumDTensor ds_g_n_k_wos_strides std array index_t NDimSpatial + e_g_n_k_wos_lengths std array index_t NDimSpatial + e_g_n_k_wos_strides std array index_t NDimSpatial conv_filter_strides std array index_t NDimSpatial conv_filter_dilations std array index_t NDimSpatial input_left_pads std array index_t NDimSpatial input_right_pads const auto a_element_op = PassThrough const auto b_element_op = PassThrough const auto cde_element_op = PassThrough auto copy = auto x auto y ck ranges copy x y begin copy in_g_n_c_wis_desc GetLengths a_g_n_c_wis_lengths copy in_g_n_c_wis_desc GetStrides a_g_n_c_wis_strides copy wei_g_k_c_xs_desc GetLengths b_g_k_c_xs_lengths copy wei_g_k_c_xs_desc GetStrides b_g_k_c_xs_strides copy out_g_n_k_wos_desc GetLengths e_g_n_k_wos_lengths copy out_g_n_k_wos_desc GetStrides e_g_n_k_wos_strides copy conv_param conv_filter_strides_ conv_filter_strides copy conv_param conv_filter_dilations_ conv_filter_dilations copy conv_param input_left_pads_ input_left_pads copy conv_param input_right_pads_ input_right_pads auto argument = conv MakeArgument p_a p_b p_ds p_e a_g_n_c_wis_lengths a_g_n_c_wis_strides b_g_k_c_xs_lengths b_g_k_c_xs_strides ds_g_n_k_wos_lengths ds_g_n_k_wos_strides e_g_n_k_wos_lengths e_g_n_k_wos_strides conv_filter_strides conv_filter_dilations input_left_pads input_right_pads a_element_op b_element_op cde_element_op conv IsSupportedArgument argument we do our best statically avoid case ` filter_op ` std cerr invalid argument conv instance conv GetTypeString std endl argument Print - workspace_size workspace_size = conv GetWorkSpaceSize argument p_a == nullptr std cerr p_a nullptr std endl - p_b == nullptr std cerr p_b nullptr std endl - p_e == nullptr std cerr p_e nullptr std endl - when debugging do time kernel serialize launches auto stream_config = StreamConfig stream time kernel false log level workspace = nullptr conv SetWorkSpacePointer argument workspace stream_config run kernel float elapsed_time = invoker Run argument stream_config kernel definition extern C #ifdef GENERATE_CK_STANDALONE_RUNNER int main int argc char argv void argc void argv #endif GENERATE_CK_STANDALONE_RUNNER globals - IndentedBuffer res = super globals res splice CK conv globals using NWC = ck tensor_layout convolution NWC using NHWC = ck tensor_layout convolution NHWC using NDHWC = ck tensor_layout convolution NDHWC using KXC = ck tensor_layout convolution KXC using KYXC = ck tensor_layout convolution KYXC using KZYXC = ck tensor_layout convolution KZYXC using NWK = ck tensor_layout convolution NWK using NHWK = ck tensor_layout convolution NHWK using NDHWK = ck tensor_layout convolution NDHWK using GNWC = ck tensor_layout convolution GNWC using GNHWC = ck tensor_layout convolution GNHWC using GNDHWC = ck tensor_layout convolution GNDHWC using GKXC = ck tensor_layout convolution GKXC using GKYXC = ck tensor_layout convolution GKYXC using GKZYXC = ck tensor_layout convolution GKZYXC using GKCX = ck tensor_layout convolution GKCX using GKCYX = ck tensor_layout convolution GKCYX using GKCZYX = ck tensor_layout convolution GKCZYX using GNWK = ck tensor_layout convolution GNWK using GNHWK = ck tensor_layout convolution GNHWK using GNDHWK = ck tensor_layout convolution GNDHWK using NGKW = ck tensor_layout convolution NGKW using NGKHW = ck tensor_layout convolution NGKHW using NGKDHW = ck tensor_layout convolution NGKDHW using NWGC = ck tensor_layout convolution NWGC using NHWGC = ck tensor_layout convolution NHWGC using NDHWGC = ck tensor_layout convolution NDHWGC using KXGC = ck tensor_layout convolution KXGC using KYXGC = ck tensor_layout convolution KYXGC using KZYXGC = ck tensor_layout convolution KZYXGC using NWGK = ck tensor_layout convolution NWGK using NHWGK = ck tensor_layout convolution NHWGK using NDHWGK = ck tensor_layout convolution NDHWGK using NGCW = ck tensor_layout convolution NGCW using NGCHW = ck tensor_layout convolution NGCHW using NGCDHW = ck tensor_layout convolution NGCDHW using G_K = ck tensor_layout convolution G_K using BlockGemmPipelineScheduler = ck BlockGemmPipelineScheduler using GemmSpecialization = ck tensor_operation device GemmSpecialization using BlockGemmPipelineVersion = ck BlockGemmPipelineVersion using ConvolutionForwardSpecialization = ck tensor_operation device ConvolutionForwardSpecialization using OutElementOp = PassThrough namespace ck namespace utils namespace conv ConvParam ConvParam ck index_t n_dim ck index_t group_count ck index_t n_batch ck index_t n_out_channels ck index_t n_in_channels const std vector ck index_t filters_len const std vector ck index_t input_len const std vector ck index_t strides const std vector ck index_t dilations const std vector ck index_t left_pads const std vector ck index_t right_pads num_dim_spatial_ static_cast ck long_index_t n_dim G_ static_cast ck long_index_t group_count N_ static_cast ck long_index_t n_batch K_ static_cast ck long_index_t n_out_channels C_ static_cast ck long_index_t n_in_channels filter_spatial_lengths_ num_dim_spatial_ input_spatial_lengths_ num_dim_spatial_ output_spatial_lengths_ num_dim_spatial_ conv_filter_strides_ num_dim_spatial_ conv_filter_dilations_ num_dim_spatial_ input_left_pads_ num_dim_spatial_ input_right_pads_ num_dim_spatial_ static_cast ck index_t filter_spatial_lengths_ size = num_dim_spatial_ &#124; &#124; static_cast ck index_t input_spatial_lengths_ size = num_dim_spatial_ &#124; &#124; static_cast ck index_t conv_filter_strides_ size = num_dim_spatial_ &#124; &#124; static_cast ck index_t conv_filter_dilations_ size = num_dim_spatial_ &#124; &#124; static_cast ck index_t input_left_pads_ size = num_dim_spatial_ &#124; &#124; static_cast ck index_t input_right_pads_ size = num_dim_spatial_ throw std runtime_error ConvParam ConvParam parameter size different number declared dimensions ck index_t i = i num_dim_spatial_ ++i filter_spatial_lengths_ i = static_cast ck long_index_t filters_len i input_spatial_lengths_ i = static_cast ck long_index_t input_len i conv_filter_strides_ i = static_cast ck long_index_t strides i conv_filter_dilations_ i = static_cast ck long_index_t dilations i input_left_pads_ i = static_cast ck long_index_t left_pads i input_right_pads_ i = static_cast ck long_index_t right_pads i XEff = X - conv_dilation_w + Wo = Wi + in_left_pad_w + in_right_pad_w - XEff conv_stride_w + const ck long_index_t x_eff = filter_spatial_lengths_ i - conv_filter_dilations_ i + output_spatial_lengths_ i = input_spatial_lengths_ i + input_left_pads_ i + input_right_pads_ i - x_eff conv_filter_strides_ i + namespace conv namespace utils namespace ck const std vector std size_t HostTensorDescriptor GetLengths const mLens const std vector std size_t HostTensorDescriptor GetStrides const mStrides std size_t HostTensorDescriptor GetNumOfDimension const mLens size void HostTensorDescriptor CalculateStrides mStrides clear mStrides resize mLens size mStrides empty mStrides back = std partial_sum mLens rbegin mLens rend - mStrides rbegin + std multiplies std size_t res header - IndentedBuffer res = super header res splice CK conv headers #include ck tensor_operation gpu device impl device_grouped_conv_fwd_multiple_abd_xdl_cshuffle_v hpp #include ck tensor_operation gpu device convolution_forward_specialization hpp #include ck tensor_operation gpu device gemm_specialization hpp #include ck library utility convolution_parameter hpp #include ck library utility convolution_host_tensor_descriptor_helper hpp res staticmethod add_ck_conv_choices choices layout input_nodes stride padding dilation groups n_spatial_dimensions template = CKGroupedConvFwdTemplate input_nodes layout stride=stride padding=padding dilation=dilation groups=groups n_spatial_dimensions=n_spatial_dimensions ops = template gen_ops op ops template maybe_append_choice choices op=op __init__ input_nodes layout stride padding dilation groups n_spatial_dimensions super __init__ ck_conv_template input_nodes layout stride = stride padding = padding dilation = dilation groups = groups n_spatial_dimensions = n_spatial_dimensions filter_op op CKGroupedConvFwdOp type ignore name-defined metas = T get_layout T input_nodes output_node T None X_meta = metas W_meta = metas Y_meta = metas - disable instance dtypes don t match op a_element_dtype = _TORCH_DTYPE_TO_CK X_meta dtype None op b_element_dtype = _TORCH_DTYPE_TO_CK W_meta dtype None op e_element_dtype = _TORCH_DTYPE_TO_CK Y_meta dtype None disable instance layouts don t match op a_layout = torch_layout_to_ck_input_layout X_meta None op b_layout = torch_layout_to_ck_weight_layout W_meta None op e_layout = torch_layout_to_ck_output_layout Y_meta None disable instance number spatial dimensions doesn t match op n_dim_spatial = n_spatial_dimensions None disable x odd-channels conv specializations now Default op conv_forward_specialization None op gen_ops unfiltered_instances = gen_conv_ops_library filtered_instances = list filter lambda op filter_op op unfiltered_instances NB when using fixed list order most likely we will pick subset instances which very similar each other Randomizing choice seems solve random seed - chosen_instances = random sample filtered_instances min len filtered_instances config rocm ck_max_profiling_configs config rocm ck_max_profiling_configs filtered_instances log debug generated d ck instances after filter s len chosen_instances chosen_instances chosen_instances emit_ck_instance op CKGroupedConvFwdOp - tuple str str type ignore name-defined The Jinja template generating C++ type alias definition Universal GEMM instance template_definition = r Gemm operator operation_name using Operation_ operation_name = ck tensor_operation device DeviceGroupedConvFwdMultipleABD_Xdl_CShuffle_V template_params The Jinja template generating C++ type alias usage Universal GEMM instance template_type = r Operation_ operation_name template_params = field_name field_value op dict_items isinstance field_value tuple tuple_elements = join map str iter field_value ds field_name element type layout bias arg = f field_name Tuple tuple_elements tile shape arg = f field_name S tuple_elements pyrefly ignore bad-argument-type template_params append arg field_value None pyrefly ignore bad-argument-type template_params append f field_name field_value _template_from_string template_definition render operation_name=op name template_params= \n + join template_params _template_from_string template_type render operation_name=op name render type ignore override kernel ROCmTemplateKernel op CKGroupedConvFwdOp type ignore name-defined kwargs - str template_buffer_node = kwargs get template_buffer_node template_buffer_node None output_node = template_buffer_node X W = input_nodes input_nodes Y = output_node Bias = input_nodes == len input_nodes None op = copy deepcopy op instance_definition instance_type = emit_ck_instance op size_arg_strs = GroupCount NBatch NOutChannels NInChannels FilterSize_ FilterSize_ InputSize_ InputSize_ ConvolutionStrides_ ConvolutionStrides_ Dilations_ Dilations_ LeftPads_ LeftPads_ RightPads_ RightPads_ _template_from_string conv_template render headers=self header getvalue globals=self globals getvalue instance_definition=instance_definition instance_type=instance_type kernel_definition=kernel def_kernel inputs= X W Bias Bias None X W outputs= Y names_str= input weight bias output Bias None input weight output size_args= f int _t arg arg size_arg_strs n_d_tensors= Bias None n_dim_spatial=self n_spatial_dimensions input_layout=op a_layout weight_layout=op b_layout output_layout=op e_layout size_args x w = input_nodes input_nodes y = output_node group_count = groups n_batch = x shape type ignore index n_out_channels = y shape type ignore index n_in_channels = x shape type ignore index filter_size_ filter_size_ = w shape type ignore index input_size_ input_size_ = x shape type ignore index convolution_strides_ convolution_strides_ = stride dilations_ dilations_ = dilation left_pads_ left_pads_ = padding right_pads_ right_pads_ = padding group_count n_batch n_out_channels n_in_channels filter_size_ filter_size_ input_size_ input_size_ convolution_strides_ convolution_strides_ dilations_ dilations_ left_pads_ left_pads_ right_pads_ right_pads_ override get_runtime_arg_info - list ArgInfo override get_runtime_arg_values kwargs Any - list Any Helper method retrieve runtime args generate kwargs