Owner s module dynamo dataclasses importlib inspect math types unittest warnings typing Any torch torch _dynamo config config torch _dynamo test_case torch _functorch deprecated deprecated_func torch _dynamo trace_rules LEGACY_MOD_INLINELIST load_object lookup_inner manual_torch_name_rule_map MOD_INLINELIST torch_c_binding_in_graph_functions torch_non_c_binding_in_graph_functions torch _dynamo utils hashable is_safe_constant istype torch _dynamo variables SkipFunctionVariable TorchInGraphFunctionVariable UserFunctionVariable torch testing _internal common_utils skipIfWindows try utils create_dummy_module_and_function except ImportError utils create_dummy_module_and_function ignored_c_binding_in_graph_function_names = Ignored because they have manual rules defined ` trace_rules manual_torch_name_rule_map ` torch _nested_tensor_from_mask torch _nested_from_padded torch sparse_compressed_tensor torch sparse_bsc_tensor torch sparse_bsr_tensor torch sparse_coo_tensor torch sparse_csc_tensor torch sparse_csr_tensor torch cuda _get_device_properties Ignored go through rules defined ` trace_rules check ` torch _functionalize_are_all_mutations_under_no_grad_or_inference_mode torch _cslt_sparse_mm_search torch _C _abort torch _C _mps_is_on_macos_or_newer torch _C _swap_tensor_impl torch _C _unsafe_reset_storage torch _dynamo eval_frame reset_code torch _C autocast_decrement_nesting torch _C autocast_increment_nesting torch _C clear_autocast_cache torch _C set_anomaly_enabled torch _C set_autocast_cache_enabled torch _C set_autocast_cpu_dtype torch _C set_autocast_cpu_enabled torch _C set_autocast_enabled torch _C set_autocast_gpu_dtype torch _C set_autocast_ipu_dtype torch _C set_autocast_ipu_enabled torch _C set_autocast_xla_dtype torch _C set_autocast_xla_enabled torch resize_as_ torch resize_as_sparse_ torch _C _data_address torch _C _is_cow_tensor torch _lazy_clone torch _test_parallel_materialize torch _C _storage_address torch _C _pickle_save torch _validate_sparse_compressed_tensor_args torch _validate_sparse_csr_tensor_args torch _validate_sparse_bsr_tensor_args torch _validate_sparse_csc_tensor_args torch _validate_sparse_coo_tensor_args torch _validate_sparse_bsc_tensor_args torch _validate_compressed_sparse_indices torch _C _llvm_enabled ignored_c_binding_in_graph_function_names &#124; = torch _C _te set_llvm_aot_workflow torch _C _te set_llvm_target_cpu torch _C _te set_llvm_target_attrs torch _C _te set_llvm_target_triple Helper function dump torch name rule map generated based heuristic defined gen_allowed_objs_and_ids dump_allowed_torch_name_rule_map - None m = gen_allowed_objs_and_ids record=True c_binding_only=False name_rule_map k v m items print f k v __name__ dataclasses dataclass AllowedObjects Track objects object id - name pairs name - dynamo wrapping rule pairs heuristic defined ` gen_allowed_objs_and_ids ` object_ids dict int str c_binding_in_graph_functions set Any non_c_binding_in_graph_functions set Any name_rule_map dict str Any gen_allowed_objs_and_ids record=False c_binding_only=True - AllowedObjects Walk torch get ids all stuff warnings filterwarnings ignore category=UserWarning module= torch distributed torch_object_ids = c_binding_in_graph_functions = set non_c_binding_in_graph_functions = set torch_name_rule_map = In some platforms these functions loaded classes instead functions To mitigate these weird cases we need special check is_special_functions obj hashable obj obj torch _C _cuda_isCurrentStreamCapturing torch _C _graph_pool_handle Add obj c_binding_in_graph_functions set non_c_binding_in_graph_functions set s torch function method This used generate graph function list based heuristic heuristic_record_if_in_graph_function obj module name try hasattr obj __wrapped__ obj = obj __wrapped__ except Exception pass isinstance obj types FunctionType types BuiltinFunctionType types MethodDescriptorType types WrapperDescriptorType is_special_functions obj torch_name_rule_map f module __name__ name = TorchInGraphFunctionVariable c_binding_only hasattr obj __code__ c_binding_in_graph_functions add obj hasattr obj __code__ non_c_binding_in_graph_functions add obj c_binding_in_graph_functions add obj _is_allowed_module_prefix obj allowed_modules = torch math torch nn modules rnn disallowed because these modules internally flatten their parameters This flattening process will call Tensor set_ Storage Storages cannot traced AOTAutograd so we need graph-break To ensure we inline these functions rather than keep them opaque-ly graph disallowed_modules = torch optim torch nn modules rnn torch _dynamo torch _C _dynamo torch _inductor torch _C inductor torch fx torch _C _autograd torch _C _cudart torch _C _distributed_autograd torch _C _distributed_c d torch _C _distributed_rpc torch _C _functorch torch _C _monitor torch _C _nvtx torch _C _lazy torch _C _profiler torch __config__ torch _custom_op torch _decomp torch _dispatch torch _export torch _functorch make_functional torch _functorch compile_utils torch _functorch partitioners torch _functorch aot_autograd torch _functorch compilers torch _functorch fx_minifier torch autograd profiler_util torch autograd profiler torch _jit_internal torch _library torch _lobpcg torch _logging torch _meta_registrations torch _namedtensor_internals torch _numpy torch _sources torch _subclasses torch _tensor torch _tensor_str torch _utils torch _utils_internal torch _vmap_internals torch compiler torch distributed torch export torch hub torch jit torch library torch masked maskedtensor torch nn init torch nn modules module torch nn parallel torch nn utils torch multiprocessing torch onnx torch overrides torch package torch profiler torch serialization torch storage torch utils torch distributed allowed_modules_dot = tuple x + x allowed_modules module = inspect getmodule obj module None False mod_name = module __name__ any mod_name startswith m m disallowed_modules False mod_name allowed_modules mod_name startswith allowed_modules_dot _find_torch_objects module any module __name__ startswith mod_name mod_name config allowed_functions_module_string_ignorelist torch_object_ids id module = module __name__ name obj list module __dict__ items id obj torch_object_ids Dynamo allows all builtins into graph does attempt introspect into them We don t want allow instances HigherOrderOperator into graph all time Dynamo needs introspect body functions these HigherOrderOperator first decide they safe then allow them into graph So we exclude HigherOrderOperator being builtin torch _ops isinstance obj torch _ops HigherOrderOperator continue We want trace through ` grad ` ` vmap ` obj torch func grad deprecated_func grad torch func vmap deprecated_func vmap torch nn functional triplet_margin_with_distance_loss torch cond continue isinstance obj types ModuleType obj __name__ startswith torch _is_allowed_module_prefix obj torch_object_ids id obj = f module __name__ name _find_torch_objects obj _is_allowed_module_prefix obj record heuristic_record_if_in_graph_function obj module name torch_object_ids id obj = f module __name__ name inspect getmodule obj None is_safe_constant obj record heuristic_record_if_in_graph_function obj module name torch_object_ids id obj = f module __name__ name _find_torch_objects torch _find_torch_objects math AllowedObjects torch_object_ids c_binding_in_graph_functions non_c_binding_in_graph_functions torch_name_rule_map TraceRuleTests torch _dynamo test_case TestCase _check_set_equality generated used rule_map ignored_set x = generated - used y = used - generated msg = f New torch objects x f added ` trace_rules rule_map ` ` test_trace_rules ignored_set ` Refer instruction ` torch _dynamo trace_rules py ` more details msg = f Existing torch objects y removed f Please remove them ` trace_rules rule_map ` ` test_trace_rules ignored_set ` Refer instruction ` torch _dynamo trace_rules py ` more details assertTrue len x == msg assertTrue len y == msg We using python function module string names these inlinelist unit test make sure functions modules can correctly imported loaded case there typo strings test_skipfiles_inlinelist m LEGACY_MOD_INLINELIST union MOD_INLINELIST try mod = importlib import_module m except ImportError continue assertTrue isinstance mod types ModuleType f m trace_rules MOD_INLINELIST LEGACY_MOD_INLINELIST python module please check correct unittest skip This test keeps getting broken our disable infra handling well see test_torch_name_rule_map_updated Generate allowed objects based heuristic defined ` allowed_functions py ` objs = gen_allowed_objs_and_ids record=True c_binding_only=True Test C binding graph functions updated torch_name_rule_map generated = objs c_binding_in_graph_functions used = set x set torch_c_binding_in_graph_functions keys &#124; ignored_c_binding_in_graph_function_names obj = load_object x obj None used add obj _check_set_equality generated used torch_c_binding_in_graph_functions ignored_c_binding_in_graph_function_names For non C binding graph functions we only test they can loaded successfully f torch_non_c_binding_in_graph_functions assertTrue isinstance load_object f types FunctionType types BuiltinFunctionType types MethodDescriptorType types WrapperDescriptorType test_force_inline_torch_function ` torch _dynamo utils istype ` skipped default fn x istype x torch Tensor x + x - _manual_torch_name_rule_map = manual_torch_name_rule_map copy Force inline ` torch _dynamo utils istype ` setting trace rule _manual_torch_name_rule_map torch _dynamo utils istype = UserFunctionVariable _torch_name_rule_map = _manual_torch_name_rule_map torch_c_binding_in_graph_functions torch_non_c_binding_in_graph_functions assertTrue torch _dynamo torch _dynamo trace_rules LEGACY_MOD_INLINELIST assertTrue torch _dynamo torch _dynamo trace_rules MOD_INLINELIST unittest mock patch torch _dynamo trace_rules torch_name_rule_map _torch_name_rule_map unittest mock patch torch _dynamo trace_rules get_torch_obj_rule_map torch _dynamo trace_rules get_torch_obj_rule_map __wrapped__ bypass functools lru_cache x = torch rand opt_fn = torch compile backend= eager fullgraph=True fn ref = fn x res = opt_fn x assertEqual ref res test_force_inline_custom_function mod func = create_dummy_module_and_function fn x func x _manual_torch_name_rule_map = manual_torch_name_rule_map copy Force inline ` mod func ` setting trace rule _manual_torch_name_rule_map f mod __name__ func __name__ = UserFunctionVariable _torch_name_rule_map = _manual_torch_name_rule_map torch_c_binding_in_graph_functions torch_non_c_binding_in_graph_functions unittest mock patch torch _dynamo trace_rules torch_name_rule_map _torch_name_rule_map unittest mock patch torch _dynamo trace_rules get_torch_obj_rule_map torch _dynamo trace_rules get_torch_obj_rule_map __wrapped__ First adding module SKIP_DIRS so will skipped default torch _dynamo trace_rules add mod __name__ x = torch rand opt_fn = torch compile backend= eager fullgraph=True fn ref = fn x res = opt_fn x assertEqual ref res test_no_special_handlers_for_torch_non_c_bindings handlers = TorchInGraphFunctionVariable _get_handlers These handlers manually audited safe safe_handlers = handle_tracing_state_functions No global state constant handle_radians No global state constant handle_is_tensor No global state handle_torch_compile No global state constant handle_ntuple No global state handle_is_grad_enabled Safely implemented handle_use_deterministic_algorithms Guarded variable handle_are_deterministic_algorithms_enabled Guarded constant handle_device_interface_stream No global state handle_cudnn_is_acceptable No global state handle_assert No global state constant handle_nested_tensor No global state handle_current_stream Safely implemented fn handlers isinstance fn staticmethod inspect ismethod fn fn_name = f fn __module__ fn __name__ fn_name = f fn __module__ fn __name__ handlers fn __name__ safe_handlers continue assertFalse fn_name torch_non_c_binding_in_graph_functions f torch function fn_name has special handler handlers fn __name__ \n We expected all functions ` torch_non_c_binding_in_graph_functions ` safe cache \n Functions special handlers may safe cache since they can close over global state \n If your handler function safe cache please add list safe handlers above \n Otherwise add ` manual_torch_name_rule_map ` instead test_almost_impossible_missing_name weird noqa UP __getattribute__ name name == __name__ raise AttributeError test w = weird o = set assertRaises AttributeError w __name__ assertEqual lookup_inner w name=None reasons=o SkipFunctionVariable TestModuleSurviveSkipFiles torch _dynamo test_case TestCase unittest skipIf torch distributed is_available need MLP module distributed skipIfWindows msg= AssertionError False true MLP did survive skip files test_module_survive_skip_files torch testing _internal common_fsdp MLP model = MLP inp = torch randn frame_count_before = torch _dynamo convert_frame FRAME_COUNTER model compile backend= eager model inp frame_count_after = torch _dynamo convert_frame FRAME_COUNTER assertTrue frame_count_after frame_count_before MLP did survive skip files __name__ == __main__ torch _dynamo test_case run_tests run_tests