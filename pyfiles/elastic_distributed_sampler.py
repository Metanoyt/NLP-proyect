usr bin env python Copyright c Facebook Inc its affiliates All rights reserved This source code licensed under BSD-style license found LICENSE file root directory source tree math collections abc Iterator Sized typing cast Optional TypeVar torch torch utils data Dataset torch utils data distributed DistributedSampler T = TypeVar T __all__ = ElasticDistributedSampler ElasticDistributedSampler DistributedSampler T Sampler restricts data loading subset dataset elastic training It especially useful conjunction ` torch nn parallel DistributedDataParallel ` In such case each process can pass DistributedSampler instance DataLoader sampler load subset original dataset exclusive note Dataset assumed constant size Args dataset Dataset used sampling num_replicas optional Number processes participating distributed training rank optional Rank current process within num_replicas start_index optional Which index dataset start sampling __init__ dataset Dataset T num_replicas Optional int = None rank Optional int = None start_index int = super __init__ dataset=dataset num_replicas=num_replicas rank=rank isinstance dataset Sized raise TypeError Dataset must instance collections abc Sized Cast Sized mypy pyrefly ignore redundant-cast sized_dataset = cast Sized dataset start_index = len sized_dataset raise ValueError f Start index start_index should less than dataset size len sized_dataset start_index = start_index sized_dataset = cast Sized dataset num_samples = math ceil float len sized_dataset - start_index num_replicas total_size = num_samples num_replicas __iter__ - Iterator T deterministically shuffle based epoch g = torch Generator g manual_seed epoch sized_dataset = cast Sized dataset indices = torch randperm len sized_dataset - start_index generator=g add start_index tolist add extra samples make evenly divisible indices += indices total_size - len indices assert len indices == total_size subsample indices = indices rank total_size num_replicas assert len indices == num_samples iter indices __len__ - int num_samples