mypy allow-untyped-defs functools inspect itertools warnings collections OrderedDict collections abc Callable typing Any Concatenate Optional TypeVar typing_extensions deprecated ParamSpec torch torch _C _C torch _functorch _functorch torch utils hooks hooks torch _C _functions torch _functorch autograd_function custom_function_call __all__ = FunctionCtx BackwardCFunction FunctionMeta Function once_differentiable InplaceFunction NestedIOFunction Unique id provider each inheriting Function This incremented FunctionMeta during definition AUTOGRAD_FUNCTION_COUNTER = itertools count _T = TypeVar _T _R = TypeVar _R _P = ParamSpec _P Formerly known _ContextMethodMixin FunctionCtx save_for_backward tensors torch Tensor r Save given tensors future call func ` ~Function backward ` ` ` save_for_backward ` ` should called most once either func ` setup_context ` func ` forward ` methods only tensors All tensors intended used backward pass should saved ` ` save_for_backward ` ` opposed directly ` ` ctx ` ` prevent incorrect gradients memory leaks enable application saved tensor hooks See ` torch autograd graph saved_tensors_hooks ` See ref ` extending-autograd ` more details Note intermediary tensors tensors neither inputs nor outputs func ` forward ` saved backward your custom Function may support double backward Custom Functions do support double backward should decorate their func ` backward ` method ` ` once_differentiable ` ` so performing double backward raises error If you d like support double backward you can either recompute intermediaries based inputs during backward intermediaries outputs custom Function See ` double backward tutorial https pytorch org tutorials intermediate custom_function_double_backward_tutorial html ` _ more details In func ` backward ` saved tensors can accessed through attr ` saved_tensors ` attribute Before returning them user check made ensure they weren t used any in-place operation modified their content Arguments can also ` ` None ` ` This no-op See ref ` extending-autograd ` more details how use method Example xdoctest +REQUIRES env TORCH_DOCTEST_AUTOGRAD Func Function staticmethod forward ctx x torch Tensor y torch Tensor z int w = x z out = x y + y z + w y ctx save_for_backward x y w out ctx z = z z tensor out staticmethod once_differentiable backward ctx grad_out x y w out = ctx saved_tensors z = ctx z gx = grad_out y + y z gy = grad_out x + z + w gz = None gx gy gz = torch tensor requires_grad=True dtype=torch double b = torch tensor requires_grad=True dtype=torch double c = d = Func apply b c to_save = tensors save_for_forward tensors torch Tensor r Save given tensors future call func ` ~Function jvp ` ` ` save_for_forward ` ` should called most once either func ` setup_context ` func ` forward ` methods all arguments should tensors In func ` jvp ` saved objects can accessed through attr ` saved_tensors ` attribute Arguments can also ` ` None ` ` This no-op See ref ` extending-autograd ` more details how use method Example xdoctest +SKIP Func torch autograd Function staticmethod forward ctx x torch Tensor y torch Tensor z int ctx save_for_backward x y ctx save_for_forward x y ctx z = z x y z staticmethod jvp ctx x_t y_t _ x y = ctx saved_tensors z = ctx z z y x_t + x y_t staticmethod vjp ctx grad_out x y = ctx saved_tensors z = ctx z z grad_out y z grad_out x None = torch tensor requires_grad=True dtype=torch double t = torch tensor dtype=torch double b = torch tensor requires_grad=True dtype=torch double c = fwAD dual_level a_dual = fwAD make_dual t d = Func apply a_dual b c tensor tensors isinstance tensor torch Tensor tensor None raise AssertionError save_for_forward expects all arguments tensors you should save non-tensors attributes ctx saved_for_forward = tensors mark_dirty args torch Tensor r Mark given tensors modified in-place operation This should called most once either func ` setup_context ` func ` forward ` methods all arguments should inputs Every tensor s been modified in-place call func ` forward ` should given function ensure correctness our checks It doesn t matter whether function called before after modification Examples xdoctest +REQUIRES env TORCH_DOCTEST_AUTOGRAD Inplace Function staticmethod forward ctx x x_npy = x numpy x_npy shares storage x x_npy += ctx mark_dirty x x staticmethod once_differentiable backward ctx grad_output grad_output = torch tensor requires_grad=True dtype=torch double clone b = Inplace apply This would lead wrong gradients engine would know unless we mark_dirty xdoctest +SKIP b backward RuntimeError one variables needed gradient computation has been modified inplace operation dirty_tensors = args deprecated ` mark_shared_storage ` deprecated Tensors shared storages automatically tracked Note calls ` set_ ` tracked category=FutureWarning mark_shared_storage pairs pass mark_non_differentiable args torch Tensor r Mark outputs non-differentiable This should called most once either func ` setup_context ` func ` forward ` methods all arguments should tensor outputs This will mark outputs requiring gradients increasing efficiency backward computation You still need accept gradient each output meth ` ~Function backward ` s always going zero tensor same shape shape corresponding output This used e g indices returned sort See example Func Function staticmethod forward ctx x sorted idx = x sort ctx mark_non_differentiable idx ctx save_for_backward x idx sorted idx staticmethod once_differentiable backward ctx g g still need accept g x idx = ctx saved_tensors grad_input = torch zeros_like x grad_input index_add_ idx g grad_input non_differentiable = args set_materialize_grads value bool r Set whether materialize grad tensors Default ` ` True ` ` This should called only either func ` setup_context ` func ` forward ` methods If ` ` True ` ` undefined grad tensors will expanded tensors full zeros prior calling func ` backward ` func ` jvp ` methods Example xdoctest +REQUIRES env TORCH_DOCTEST_AUTOGRAD SimpleFunc Function staticmethod forward ctx x x clone x clone staticmethod once_differentiable backward ctx g g g + g No check None necessary We modify SimpleFunc handle non-materialized grad outputs Func Function staticmethod forward ctx x ctx set_materialize_grads False ctx save_for_backward x x clone x clone staticmethod once_differentiable backward ctx g g x = ctx saved_tensors grad_input = torch zeros_like x g None We must check None now grad_input += g g None grad_input += g grad_input = torch tensor requires_grad=True b _ = Func apply induces g undefined materialize_grads = value DO NOT USE This only defined able load old serialized models _ContextMethodMixin = FunctionCtx _HookMixin staticmethod _register_hook backward_hooks hook backward_hooks None backward_hooks = OrderedDict handle = hooks RemovableHandle backward_hooks backward_hooks handle id = hook backward_hooks handle BackwardCFunction _C _FunctionBase FunctionCtx _HookMixin r This used internal autograd work Do use apply args r Apply method used when executing Node during backward _forward_cls defined derived The user should define either backward vjp never both backward_fn = _forward_cls backward type ignore attr-defined vjp_fn = _forward_cls vjp type ignore attr-defined backward_fn Function backward vjp_fn Function vjp raise RuntimeError Implementing both backward vjp custom Function allowed You should only implement one them user_fn = vjp_fn vjp_fn Function vjp backward_fn user_fn args apply_jvp args r Apply method used when executing forward mode AD during forward _forward_cls defined derived _forward_cls jvp args type ignore attr-defined _compiled_autograd_key _forward_cls _compiled_autograd_key type ignore attr-defined FunctionMeta type Function metaclass This metaclass sets up following properties _backward_cls The Function corresponding differentiated version function which generated fly metaclass __init__ cls name bases attrs backward_fn = type name + Backward BackwardCFunction _forward_cls cls backward_fn _autograd_function_id = next AUTOGRAD_FUNCTION_COUNTER type ignore attr-defined cls _backward_cls = backward_fn super __init__ name bases attrs _SingleLevelFunction _C _FunctionBase FunctionCtx _HookMixin metaclass=FunctionMeta staticmethod forward args Any kwargs Any - Any r Define forward custom autograd Function This function overridden all subclasses There two ways define forward Usage Combined forward ctx staticmethod forward ctx Any args Any kwargs Any - Any pass - It must accept context ctx first argument followed any number arguments tensors other types - See ref ` combining-forward-context ` more details Usage Separate forward ctx staticmethod forward args Any kwargs Any - Any pass staticmethod setup_context ctx Any inputs Tuple Any output Any - None pass - The forward no longer accepts ctx argument - Instead you must also override meth ` torch autograd Function setup_context ` staticmethod handle setting up ` ` ctx ` ` object ` ` output ` ` output forward ` ` inputs ` ` Tuple inputs forward - See ref ` extending-autograd ` more details The context can used store arbitrary data can then retrieved during backward pass Tensors should stored directly ` ctx ` though currently enforced backward compatibility Instead tensors should saved either func ` ctx save_for_backward ` they intended used ` ` backward ` ` equivalently ` ` vjp ` ` func ` ctx save_for_forward ` they intended used ` ` jvp ` ` raise NotImplementedError You must implement forward function custom autograd Function staticmethod setup_context ctx Any inputs tuple Any output Any - Any r There two ways define forward pass autograd Function Either Override forward signature ` ` forward ctx args kwargs ` ` ` ` setup_context ` ` overridden Setting up ctx backward happens inside ` ` forward ` ` Override forward signature ` ` forward args kwargs ` ` override ` ` setup_context ` ` Setting up ctx backward happens inside ` ` setup_context ` ` opposed inside ` ` forward ` ` See meth ` torch autograd Function forward ` ref ` extending-autograd ` more details raise NotImplementedError setup_context implemented staticmethod backward ctx Any grad_outputs Any - Any r Define formula differentiating operation backward mode automatic differentiation This function overridden all subclasses Defining function equivalent defining ` ` vjp ` ` function It must accept context attr ` ctx ` first argument followed many outputs func ` forward ` returned None will passed non tensor outputs forward function should many tensors there inputs func ` forward ` Each argument gradient w r t given output each returned value should gradient w r t corresponding input If input Tensor Tensor requiring grads you can just pass None gradient input The context can used retrieve tensors saved during forward pass It also has attribute attr ` ctx needs_input_grad ` tuple booleans representing whether each input needs gradient E g func ` backward ` will have ` ` ctx needs_input_grad = True ` ` first input func ` forward ` needs gradient computed w r t output raise NotImplementedError You must implement either backward vjp method your custom autograd Function use backward mode AD vjp backward alias each other vjp = backward staticmethod jvp ctx Any grad_inputs Any - Any r Define formula differentiating operation forward mode automatic differentiation This function overridden all subclasses It must accept context attr ` ctx ` first argument followed many inputs func ` forward ` got None will passed non tensor inputs forward function should many tensors there outputs func ` forward ` Each argument gradient w r t given input each returned value should gradient w r t corresponding output If output Tensor function differentiable respect output you can just pass None gradient input You can use attr ` ctx ` object pass any value forward functions raise NotImplementedError You must implement jvp function custom autograd Function use forward mode AD Function _SingleLevelFunction r Base create custom ` autograd Function ` To create custom ` autograd Function ` subclass implement meth ` forward ` meth ` backward ` static methods Then use your custom op forward pass call method ` ` apply ` ` Do call meth ` forward ` directly To ensure correctness best performance make sure you calling correct methods ` ` ctx ` ` validating your backward function using func ` torch autograd gradcheck ` See ref ` extending-autograd ` more details how use Examples xdoctest +REQUIRES env TORCH_DOCTEST_AUTOGRAD Exp Function staticmethod forward ctx i result = i exp ctx save_for_backward result result staticmethod backward ctx grad_output result = ctx saved_tensors grad_output result Use calling apply method xdoctest +SKIP output = Exp apply input __init__ args kwargs warnings warn f __class__ should instantiated Methods autograd functions all static so you should invoke them itself Instantiating autograd function will raise error future version PyTorch DeprecationWarning stacklevel= __call__ args kwargs raise RuntimeError Legacy autograd function non-static forward method deprecated Please use new-style autograd function static forward method Example https pytorch org docs stable autograd html#torch autograd Function Bool specifies PyTorch should attempt autogenerate func ` torch vmap ` support autograd Function You may set True only autograd Function s forward backward jvp they exist written using PyTorch operations otherwise please override meth ` torch autograd Function vmap ` add support func ` torch vmap ` Please see ref ` func-autograd-function ` more details generate_vmap_rule = False staticmethod vmap info in_dims args r Define behavior autograd Function underneath func ` torch vmap ` For func ` torch autograd Function ` support func ` torch vmap ` you must either override static method set ` ` generate_vmap_rule ` ` ` ` True ` ` you may do both If you choose override staticmethod must accept - ` ` info ` ` object first argument ` ` info batch_size ` ` specifies size dimension being vmapped over while ` ` info randomness ` ` randomness option passed func ` torch vmap ` - ` ` in_dims ` ` tuple second argument For each arg ` ` args ` ` ` ` in_dims ` ` has corresponding ` ` Optional int ` ` It ` ` None ` ` arg Tensor arg being vmapped over otherwise integer specifying what dimension Tensor being vmapped over - ` ` args ` ` which same args meth ` ~Function forward ` The vmap staticmethod tuple ` ` output out_dims ` ` Similar ` ` in_dims ` ` ` ` out_dims ` ` should same structure ` ` output ` ` contain one ` ` out_dim ` ` per output specifies output has vmapped dimension what index Please see ref ` func-autograd-function ` more details raise NotImplementedError To use autograd Function vmap you must either override vmap staticmethod set generate_vmap_rule=True classmethod apply cls args kwargs bind_default_args func args kwargs signature = inspect signature func bound_args = signature bind args kwargs bound_args apply_defaults bound_args args is_setup_ctx_defined = _is_setup_context_defined cls setup_context is_setup_ctx_defined args = bind_default_args cls forward args kwargs torch _C _are_functorch_transforms_active See NOTE functorch vjp autograd interaction args = _functorch utils unwrap_dead_wrappers args super apply args kwargs type ignore misc is_setup_ctx_defined raise RuntimeError In order use autograd Function functorch transforms vmap grad jvp jacrev must override setup_context staticmethod For more details please see https pytorch org docs main notes extending func html custom_function_call cls args kwargs staticmethod _compiled_autograd_key ctx ctx _autograd_function_id _is_setup_context_defined fn fn = _SingleLevelFunction setup_context once_differentiable fn Callable Concatenate _T _P _R - Callable Concatenate _T _P _R functools wraps fn wrapper ctx _T args _P args kwargs _P kwargs - _R torch no_grad outputs = fn ctx args kwargs torch is_grad_enabled outputs If any inputs have requires_grad=True we force outputs have requires_grad=True point grad_fn which throws error message during double back-propagation XXX only approximation requires_grad - there s no way figure out fn didn t use ctx saved_tensors result some Tensors might require grad even no args do Unfortunately leads unexpected error messages no nodes require computing gradients I don t have better idea These functions would raise error backward anyway requires_grad = any isinstance arg torch Tensor arg requires_grad arg args requires_grad outputs isinstance outputs tuple outputs_ = outputs outputs_ = outputs err_fn = _functions DelayedError b trying differentiate twice function marked b once_differentiable len outputs_ Create aliases each output has requires_grad=True We need least one inputs err_fn require grad so output will have grad_fn fake_requires_grad var var None var = var detach var requires_grad = True var err_fn fake_requires_grad v v outputs_ type ignore return-value wrapper InplaceFunction Function r This here only backward compatibility reasons Use ` Function ` instead any new use case __init__ inplace=False super __init__ inplace = inplace _nested_map condition fn condition_msg=None _map obj condition obj fn obj obj None None isinstance obj list tuple mapped = _map x x obj hasattr obj _fields obj namedtuple type obj mapped type obj mapped isinstance obj dict x _map obj x x obj raise ValueError Auto nesting doesn t know how process input object type + torch typename obj + Accepted types + condition_msg + lists tuples them condition_msg _map _jit_unwrap_structured obj hasattr obj _jit_unwrap obj _jit_unwrap obj _iter_filter condition allow_unknown=False condition_msg=None conversion=None _iter obj conversion None obj = conversion obj condition obj yield obj obj None isinstance obj list tuple o obj yield _iter o isinstance obj dict We only accept primitive key types so we needn t inspect them o obj values yield _iter o allow_unknown yield obj raise ValueError Auto nesting doesn t know how process input object type + torch typename obj + Accepted types + condition_msg + lists tuples them condition_msg _iter _unflatten input proto unflatten list tuple input into nested list tuple structure specified proto unflatten_helper input proto res list Optional torch Tensor = hasattr proto _jit_wrap proto _jit_wrap input isinstance proto list tuple input input e proto e None res append e res_e input = unflatten_helper input e res append res_e type proto res input unflatten_helper input proto _iter_jit_values = _iter_filter lambda o o None isinstance o torch _C Value condition_msg= jit s Values None _iter_tensors = _iter_filter lambda x isinstance x torch Tensor condition_msg= Tensors conversion=_jit_unwrap_structured _iter_tensors_permissive = _iter_filter lambda x isinstance x torch Tensor allow_unknown=True condition_msg= Tensors permissive _iter_None_tensors = _iter_filter lambda o o None isinstance o torch Tensor condition_msg= Tensors None _map_tensor_data = _nested_map lambda x isinstance x torch Tensor lambda o o data condition_msg= Tensors NestedIOFunction Function r This here only backward compatibility reasons Use ` Function ` instead any new use case The type ignore statements needed here because these functions declared staticmethod superclass Function instance methods here which mypy reports incompatible _do_forward input _nested_input = input flat_input = tuple _iter_tensors input flat_output = super _do_forward flat_input type ignore misc nested_tensors = _unflatten flat_output _nested_output nested_tensors _do_backward gradients retain_variables retain_variables = retain_variables result = super _do_backward gradients retain_variables type ignore misc retain_variables del _nested_output del _to_save_nested result backward gradients Any - Any type ignore override r Shared backward utility nested_gradients = _unflatten gradients _nested_output result = backward_extended nested_gradients type ignore func-returns-value tuple _iter_None_tensors result __call__ = _do_forward forward args Any - Any type ignore override r Shared forward utility nested_tensors = _map_tensor_data _nested_input result = forward_extended nested_tensors type ignore func-returns-value del _nested_input _nested_output = result tuple _iter_tensors result save_for_backward args Any - None r See meth ` Function save_for_backward ` to_save = tuple _iter_tensors args _to_save_nested = args property saved_tensors type ignore override r See meth ` Function saved_tensors ` flat_tensors = super saved_tensors type ignore misc _unflatten flat_tensors _to_save_nested mark_dirty args Any kwargs Any - None r See meth ` Function mark_dirty ` dirty_tensors = tuple _iter_tensors args kwargs mark_non_differentiable args Any kwargs Any - None r See meth ` Function mark_non_differentiable ` non_differentiable = tuple _iter_tensors args kwargs forward_extended input Any - None r User defined forward raise NotImplementedError backward_extended grad_output Any - None r User defined backward raise NotImplementedError