mypy allow-untyped-defs math typing Optional torch torch _refs _unsqueeze_multiple torch ao quantization utils determine_qparams validate_qmin_qmax torch library impl Library Note decomposed means decomposed quantized tensor using decomposed so name too long quantized_decomposed_lib = Library quantized_decomposed DEF _INTEGER_DTYPES = torch uint torch int torch uint torch int torch int _FLOAT_DTYPES = torch float _e m torch float _e m fn _DTYPE_TO_QVALUE_BOUNDS = k torch iinfo k min torch iinfo k max k _INTEGER_DTYPES _DTYPE_TO_QVALUE_BOUNDS update k int torch finfo k min int torch finfo k max k _FLOAT_DTYPES Helper check passed quant min max valid dtype _quant_min_max_bounds_check quant_min quant_max dtype dtype _DTYPE_TO_QVALUE_BOUNDS raise ValueError f Unsupported dtype dtype quant_min_lower_bound quant_max_upper_bound = _DTYPE_TO_QVALUE_BOUNDS dtype quant_min quant_min_lower_bound raise AssertionError quant_min out bound dtype f quant_min_lower_bound quant_min_lower_bound quant_min quant_min quant_max quant_max_upper_bound raise AssertionError quant_max out bound dtype f quant_max_upper_bound quant_max_upper_bound quant_max quant_max quantized_decomposed_lib define quantize_per_tensor Tensor input float scale int zero_point int quant_min int quant_max ScalarType dtype - Tensor impl quantized_decomposed_lib quantize_per_tensor CompositeExplicitAutograd quantize_per_tensor input torch Tensor scale float zero_point int quant_min int quant_max int dtype torch dtype - torch Tensor Affine quantization Tensor using same quantization parameters map floating point quantized values Args input torch Tensor original float bfloat Tensor scale float quantization parameter affine quantization zero_point int quantization parameter affine quantization quant_min int minimum quantized value output Tensor quant_max int maximum quantized value output Tensor dtype torch dtype requested dtype e g torch uint output Tensor Returns Tensor requested dtype e g torch uint note quantization parameters stored Tensor we storing them function arguments instead input dtype torch float torch bfloat input = input torch float input dtype = torch float raise AssertionError f Expecting input have dtype torch float got dtype input dtype _quant_min_max_bounds_check quant_min quant_max dtype inv_scale = scale torch clamp torch round input inv_scale + zero_point quant_min quant_max dtype impl quantized_decomposed_lib quantize_per_tensor Meta quantize_per_tensor_meta input torch Tensor scale float zero_point int quant_min int quant_max int dtype torch dtype - torch Tensor input dtype torch float torch bfloat input = input torch float input dtype = torch float raise AssertionError f Expecting input have dtype torch float got dtype input dtype torch empty_like input dtype=dtype quantized_decomposed_lib define quantize_per_tensor tensor Tensor input Tensor scale Tensor zero_point int quant_min int quant_max ScalarType dtype - Tensor impl quantized_decomposed_lib quantize_per_tensor tensor CompositeExplicitAutograd quantize_per_tensor_tensor input torch Tensor scale torch Tensor zero_point torch Tensor quant_min int quant_max int dtype torch dtype - torch Tensor Affine quantization Tensor using same quantization parameters map floating point quantized values Same ` quantize_per_tensor ` scale zero_point Scalar Tensor instead scalar values zero_point numel = raise AssertionError f Expecting zero_point tensor one element received zero_point numel scale numel = raise AssertionError f Expecting scale tensor one element received scale numel quantize_per_tensor input scale item zero_point item type ignore arg-type quant_min type ignore arg-type quant_max type ignore arg-type dtype impl quantized_decomposed_lib quantize_per_tensor tensor Meta quantize_per_tensor_tensor_meta input torch Tensor scale torch Tensor zero_point torch Tensor quant_min int quant_max int dtype torch dtype - torch Tensor input dtype torch float torch bfloat input = input torch float zero_point numel = raise AssertionError f Expecting zero_point tensor one element received zero_point numel scale numel = raise AssertionError f Expecting scale tensor one element received scale numel input dtype = torch float raise AssertionError f Expecting input have dtype torch float got dtype input dtype torch empty_like input dtype=dtype TODO remove other variants keep one quantized_decomposed_lib define quantize_per_tensor tensor Tensor input Tensor scale Tensor zero_point Tensor quant_min Tensor quant_max ScalarType dtype - Tensor impl quantized_decomposed_lib quantize_per_tensor tensor CompositeExplicitAutograd quantize_per_tensor_tensor input torch Tensor scale torch Tensor zero_point torch Tensor quant_min torch Tensor quant_max torch Tensor dtype torch dtype - torch Tensor Affine quantization Tensor using same quantization parameters map floating point quantized values Same ` quantize_per_tensor ` scale zero_point Scalar Tensor instead scalar values zero_point numel = raise AssertionError f Expecting zero_point tensor one element received zero_point numel scale numel = raise AssertionError f Expecting scale tensor one element received scale numel quantize_per_tensor input scale item zero_point item type ignore arg-type quant_min item type ignore arg-type quant_max item type ignore arg-type dtype impl quantized_decomposed_lib quantize_per_tensor tensor Meta quantize_per_tensor_tensor _meta input torch Tensor scale torch Tensor zero_point torch Tensor quant_min torch Tensor quant_max torch Tensor dtype torch dtype - torch Tensor quantize_per_tensor_tensor_meta input scale zero_point type ignore arg-type quant_min type ignore arg-type quant_max type ignore arg-type dtype Note quant_min quant_max dtype used operator now s kept signature metadata input Tensor might useful pattern matching future We will revisit later we found there no use cases quantized_decomposed_lib define dequantize_per_tensor Tensor input float scale int zero_point int quant_min int quant_max ScalarType dtype ScalarType out_dtype=None - Tensor impl quantized_decomposed_lib dequantize_per_tensor CompositeExplicitAutograd dequantize_per_tensor input torch Tensor scale float zero_point int quant_min int quant_max int dtype torch dtype out_dtype Optional torch dtype = None - torch Tensor Affine dequantization Tensor using same quantization parameters map quantized values floating point values Args input torch Tensor Tensor dtype matching ` dtype ` argument e g ` torch uint ` per tensor quantized Tensor combined quantization parameters argument function scale zero_point scale float quantization parameter affine quantization zero_point int quantization parameter affine quantization quant_min int minimum quantized value input Tensor used computation reserved pattern matching quant_max int maximum quantized value input Tensor used computation reserved pattern matching dtype torch dtype dtype input Tensor used computation reserved pattern matching out_dtype torch dtype optional dtype output Tensor Returns dequantized float Tensor input dtype = dtype raise AssertionError f Expecting input have dtype dtype got input dtype out_dtype None out_dtype = torch float dtype _DTYPE_TO_QVALUE_BOUNDS TODO investigate why input - zero_point torch float scale failed test input out_dtype - zero_point scale raise ValueError f Unsupported dtype dequantize_per_tensor dtype impl quantized_decomposed_lib dequantize_per_tensor Meta dequantize_per_tensor_meta input torch Tensor scale torch Tensor zero_point torch Tensor quant_min int quant_max int dtype torch dtype out_dtype Optional torch dtype = None - torch Tensor out_dtype None out_dtype = torch float torch empty_like input dtype=out_dtype quantized_decomposed_lib define dequantize_per_tensor tensor Tensor input Tensor scale Tensor zero_point int quant_min int quant_max ScalarType dtype ScalarType out_dtype=None - Tensor impl quantized_decomposed_lib dequantize_per_tensor tensor CompositeExplicitAutograd dequantize_per_tensor_tensor input torch Tensor scale torch Tensor zero_point torch Tensor quant_min int quant_max int dtype torch dtype out_dtype Optional torch dtype = None - torch Tensor Affine dequantization Tensor using same quantization parameters map quantized values floating point values Same ` dequantize_per_tensor ` scale zero_point Scalar Tensor instead scalar values zero_point numel = raise AssertionError f Expecting zero_point tensor one element received zero_point numel scale numel = raise AssertionError f Expecting scale tensor one element received scale numel dequantize_per_tensor input scale item zero_point item type ignore arg-type quant_min quant_max dtype out_dtype=out_dtype impl quantized_decomposed_lib dequantize_per_tensor tensor Meta dequantize_per_tensor_tensor_meta input torch Tensor scale torch Tensor zero_point torch Tensor quant_min int quant_max int dtype torch dtype out_dtype Optional torch dtype = None - torch Tensor out_dtype None out_dtype = torch float zero_point numel = raise AssertionError f Expecting zero_point tensor one element received zero_point numel scale numel = raise AssertionError f Expecting scale tensor one element received scale numel input dtype = dtype raise AssertionError f Expecting input have dtype dtype got input dtype dtype _DTYPE_TO_QVALUE_BOUNDS torch empty_like input dtype=out_dtype raise ValueError f Unsupported dtype dequantize_per_tensor dtype TODO remove other variants keep one quantized_decomposed_lib define dequantize_per_tensor tensor Tensor input Tensor scale Tensor zero_point Tensor quant_min Tensor quant_max ScalarType dtype ScalarType out_dtype=None - Tensor impl quantized_decomposed_lib dequantize_per_tensor tensor CompositeExplicitAutograd dequantize_per_tensor_tensor input torch Tensor scale torch Tensor zero_point torch Tensor quant_min torch Tensor quant_max torch Tensor dtype torch dtype out_dtype Optional torch dtype = None - torch Tensor Affine dequantization Tensor using same quantization parameters map quantized values floating point values Same ` dequantize_per_tensor ` scale zero_point Scalar Tensor instead scalar values zero_point numel = raise AssertionError f Expecting zero_point tensor one element received zero_point numel scale numel = raise AssertionError f Expecting scale tensor one element received scale numel dequantize_per_tensor input scale item zero_point item type ignore arg-type quant_min item type ignore arg-type quant_max item type ignore arg-type dtype out_dtype=out_dtype impl quantized_decomposed_lib dequantize_per_tensor tensor Meta dequantize_per_tensor_tensor _meta input scale zero_point quant_min quant_max dtype out_dtype Optional torch dtype = None - torch Tensor dequantize_per_tensor_tensor_meta input scale zero_point quant_min quant_max dtype out_dtype=out_dtype quantized_decomposed_lib define choose_qparams tensor Tensor input int quant_min int quant_max float eps ScalarType dtype - Tensor Tensor impl quantized_decomposed_lib choose_qparams tensor CompositeExplicitAutograd choose_qparams_tensor input torch Tensor qmin int qmax int eps float dtype torch dtype - tuple torch Tensor torch Tensor Given input Tensor derive per tensor affine quantization parameter scale zero_point target quantized Tensor Tensor Args input torch Tensor floating point input Tensor quant_min int minimum quantized value target quantized Tensor quant_max int maximum quantized value target quantized Tensor dtype torch dtype dtype target quantized Tensor Returns scale float quantization parameter target quantized Tensor zero_point int quantization parameter target quantized Tensor input dtype torch float torch float torch bfloat raise AssertionError f Expecting input have dtype torch float b got dtype input dtype dtype _DTYPE_TO_QVALUE_BOUNDS raise AssertionError f Expecting target dtype one _DTYPE_TO_QVALUE_BOUNDS keys got dtype validate_qmin_qmax qmin qmax min_val max_val = torch aminmax input determine_qparams min_val max_val qmin qmax dtype torch Tensor eps has_customized_qrange=False quantized_decomposed_lib define choose_qparams_symmetric tensor Tensor input int quant_min int quant_max float eps ScalarType dtype - Tensor Tensor impl quantized_decomposed_lib choose_qparams_symmetric tensor CompositeExplicitAutograd choose_qparams_symmetric_tensor input torch Tensor qmin int qmax int eps float dtype torch dtype - tuple torch Tensor torch Tensor Given input Tensor derive per tensor affine quantization parameter scale zero_point target quantized Tensor Tensor Args input torch Tensor floating point input Tensor quant_min int minimum quantized value target quantized Tensor quant_max int maximum quantized value target quantized Tensor dtype torch dtype dtype target quantized Tensor Returns scale float quantization parameter target quantized Tensor zero_point int quantization parameter target quantized Tensor input dtype torch float torch float torch bfloat raise AssertionError f Expecting input have dtype torch float b got dtype input dtype dtype _DTYPE_TO_QVALUE_BOUNDS raise AssertionError f Expecting target dtype one _DTYPE_TO_QVALUE_BOUNDS keys got dtype validate_qmin_qmax qmin qmax min_val max_val = torch aminmax input determine_qparams min_val max_val qmin qmax dtype torch Tensor eps has_customized_qrange=False qscheme=torch per_tensor_symmetric impl quantized_decomposed_lib choose_qparams tensor Meta choose_qparams_tensor_meta input torch Tensor quant_min int quant_max int eps float dtype torch dtype - tuple torch Tensor torch Tensor input dtype torch float torch float torch bfloat raise AssertionError f Expecting input have dtype torch float b got dtype input dtype quant_min = quant_max raise AssertionError f Expecting quant_min smaller than quant_max received min quant_min max quant_max torch empty dtype=torch double device=input device torch empty dtype=torch int device=input device impl quantized_decomposed_lib choose_qparams_symmetric tensor Meta choose_qparams_symmetric_tensor_meta input torch Tensor quant_min int quant_max int eps float dtype torch dtype - tuple torch Tensor torch Tensor torch empty dtype=torch double device=input device torch empty dtype=torch int device=input device Helper function used implement per-channel quantization against any axis _permute_to_axis_zero x axis new_axis_list = list range x dim new_axis_list axis = new_axis_list = axis y = x permute tuple new_axis_list y new_axis_list quantized_decomposed_lib define quantize_per_channel Tensor input Tensor scales Tensor zero_points int axis int quant_min int quant_max ScalarType dtype - Tensor impl quantized_decomposed_lib quantize_per_channel CompositeExplicitAutograd quantize_per_channel input torch Tensor scales torch Tensor zero_points torch Tensor axis int quant_min int quant_max int dtype torch dtype - torch Tensor Affine per channel quantization Tensor using same quantization parameters each channel axis map floating point quantized values Args input torch Tensor original float bfloat Tensor scales torch Tensor list scale quantization parameter affine quantization one per channel zero_point torch Tensor list zero_point quantization parameter affine quantization one per channel quant_min int minimum quantized value output Tensor quant_max int maximum quantized value output Tensor dtype torch dtype requested dtype e g torch uint output Tensor Returns Tensor requested dtype e g torch uint note quantization parameters stored Tensor we storing them function arguments instead input dtype torch float torch bfloat input = input torch float input dtype = torch float raise AssertionError f Expecting input have dtype torch float got dtype input dtype axis = input dim raise AssertionError f Expecting axis input dim _quant_min_max_bounds_check quant_min quant_max dtype input permute_axis_list = _permute_to_axis_zero input axis new_shape = input dim new_shape = scales shape scales = scales view new_shape zero_points = zero_points view new_shape res = torch clamp torch round input scales + zero_points quant_min quant_max out = res permute tuple permute_axis_list out dtype impl quantized_decomposed_lib quantize_per_channel Meta quantize_per_channel_meta input torch Tensor scales torch Tensor zero_points torch Tensor axis int quant_min int quant_max int dtype torch dtype - torch Tensor input dtype torch float torch bfloat input = input torch float input dtype = torch float raise AssertionError f Expecting input have dtype torch float got dtype input dtype axis = input dim raise AssertionError f Expecting axis input dim _quant_min_max_bounds_check quant_min quant_max dtype torch empty_like input dtype=dtype Note quant_min quant_max dtype used operator now s kept signature metadata input Tensor might useful pattern matching future We will revisit later we found there no use cases quantized_decomposed_lib define dequantize_per_channel Tensor input Tensor scales Tensor zero_points int axis int quant_min int quant_max ScalarType dtype ScalarType out_dtype=None - Tensor impl quantized_decomposed_lib dequantize_per_channel CompositeExplicitAutograd dequantize_per_channel input torch Tensor scales torch Tensor zero_points Optional torch Tensor axis int quant_min int quant_max int dtype torch dtype out_dtype Optional torch dtype = None - torch Tensor Affine per channel dequantization Tensor using same quantization parameters each channel axis map quantized values floating point values Args input torch Tensor Tensor dtype matching ` dtype ` argument e g ` torch uint ` per channel quantized Tensor combined quantization parameter argument function scales zero_points axis scales torch Tensor list scale quantization parameter affine quantization one per channel zero_points torch Tensor list zero_point quantization parameter affine quantization one per channel quant_min int minimum quantized value output Tensor used computation reserved pattern matching quant_max int maximum quantized value output Tensor used computation reserved pattern matching dtype torch dtype requested dtype output Tensor used computation reserved pattern matching out_dtype torch dtype optional dtype output Tensor Returns dequantized float Tensor input dtype = dtype raise AssertionError f Expecting input have dtype dtype got dtype input dtype out_dtype None out_dtype = torch float axis = input dim raise AssertionError f Expecting axis input dim _quant_min_max_bounds_check quant_min quant_max dtype input permute_axis_list = _permute_to_axis_zero input axis new_shape = input dim new_shape = scales shape scales = scales view new_shape zero_points None res = input - zero_points view new_shape scales res = input scales res = res out_dtype out = res permute tuple permute_axis_list out impl quantized_decomposed_lib dequantize_per_channel Meta dequantize_per_channel_meta input torch Tensor scales torch Tensor zero_points Optional torch Tensor axis int quant_min int quant_max int dtype torch dtype out_dtype Optional torch dtype = None - torch Tensor input dtype = dtype raise AssertionError f Expecting input have dtype dtype got dtype input dtype out_dtype None out_dtype = torch float axis = input dim raise AssertionError f Expecting axis input dim _quant_min_max_bounds_check quant_min quant_max dtype torch empty_like input dtype=out_dtype quantized_decomposed_lib define choose_qparams_per_token Tensor input ScalarType dtype - Tensor Tensor impl quantized_decomposed_lib choose_qparams_per_token CompositeExplicitAutograd choose_qparams_per_token input torch Tensor dtype torch dtype - tuple torch Tensor torch Tensor Choose quantization parameters per token quantization This means N dimension Tensor M M Mn N we calculate scales zero_points each N elements quantize every N elements same quantization parameter The dimension scales zero_points will M M Mn Args input torch Tensor original float float Tensor dtype torch dtype dtype e g torch uint input Tensor Returns scales zero_points both float Tensors scales = input abs amax dim=- keepdim=True scales dtype == torch float scales = scales float want float scales avoid overflows fp bf has wide enough range dtype == torch int n_bits = quant_max = n_bits - - raise Exception noqa TRY f unsupported dtype choose_qparams_per_token dtype scales = scales clamp min= e- div quant_max zero_points = torch zeros_like scales scales zero_points impl quantized_decomposed_lib choose_qparams_per_token Meta choose_qparams_per_token_meta input torch Tensor dtype torch dtype - tuple torch Tensor torch Tensor size = list input shape - + torch empty size dtype=torch double device=input device torch empty size dtype=torch int device=input device quantized_decomposed_lib define _choose_qparams_per_token_asymmetric_impl Tensor input ScalarType dtype - Tensor Tensor impl quantized_decomposed_lib _choose_qparams_per_token_asymmetric_impl CompositeImplicitAutograd _choose_qparams_per_token_asymmetric_impl input torch Tensor dtype torch dtype - tuple torch Tensor torch Tensor Choose quantization parameters per token quantization This means N dimension Tensor M M Mn N we calculate scales zero_points each N elements quantize every N elements same quantization parameter The dimension scales zero_points will M M Mn Args input torch Tensor original float float Tensor dtype torch dtype dtype e g torch uint input Tensor Returns scales zero_points both float Tensors Based https github com google XNNPACK blob df f cf db cc eeb f ffc src xnnpack quantization h#L qmin qmax = - min_val = torch amin input dim=- keepdim=True max_val = torch amax input dim=- keepdim=True min_val_neg = torch min min_val torch zeros_like min_val max_val_pos = torch max max_val torch zeros_like max_val eps = torch finfo torch float eps use xnnpack eps scale scale = max_val_pos - min_val_neg float qmax - qmin scale = scale clamp min=eps zero point descaled_min = min_val_neg scale descaled_max = max_val_pos scale zero_point_from_min_error = qmin + descaled_min zero_point_from_max_error = qmax + descaled_max zero_point = torch where zero_point_from_min_error + zero_point_from_max_error qmin - descaled_min qmax - descaled_max zero_point = torch clamp zero_point qmin qmax round scale torch float zero_point torch int quantized_decomposed_lib define choose_qparams_per_token_asymmetric Tensor input ScalarType dtype - Tensor Tensor impl quantized_decomposed_lib choose_qparams_per_token_asymmetric CompositeExplicitAutograd choose_qparams_per_token_asymmetric input torch Tensor dtype torch dtype - tuple torch Tensor torch Tensor _choose_qparams_per_token_asymmetric_impl input dtype impl quantized_decomposed_lib choose_qparams_per_token_asymmetric Meta choose_qparams_per_token_asymmetric_meta input torch Tensor dtype torch dtype - tuple torch Tensor torch Tensor size = list input shape - + torch empty size dtype=torch double device=input device torch empty size dtype=torch int device=input device _per_token_quant_qparam_dim_check input scales zero_points num_tokens = math prod list input size - num_tokens = scales numel raise AssertionError f num_tokens num_tokens scales scales size num_tokens = zero_points numel raise AssertionError f num_tokens num_tokens zero_points zero_points size quantized_decomposed_lib define quantize_per_token Tensor input Tensor scales Tensor zero_points int quant_min int quant_max ScalarType dtype - Tensor impl quantized_decomposed_lib quantize_per_token CompositeExplicitAutograd quantize_per_token input torch Tensor scales torch Tensor zero_points torch Tensor quant_min int quant_max int dtype torch dtype Per token quantization Tensor using quantization parameters map floating point quantized values This means N dimension Tensor M M Mn N we calculate scales zero_points each N elements quantize every N elements same quantization parameter The dimension scales zero_points will M M Mn Args input torch Tensor original float bfloat Tensor scales float torch Tensor quantization parameter per token affine quantization zero_points int torch Tensor quantization parameter per token affine quantization quant_min int minimum quantized value output Tensor quant_max int maximum quantized value output Tensor dtype torch dtype requested dtype e g torch uint output Tensor Returns Tensor requested dtype e g torch uint note quantization parameters stored Tensor we storing them function arguments instead _quant_min_max_bounds_check quant_min quant_max dtype _per_token_quant_qparam_dim_check input scales zero_points input = input mul scales add zero_points round clamp quant_min quant_max dtype input impl quantized_decomposed_lib quantize_per_token Meta quantize_per_token_meta input torch Tensor scales torch Tensor zero_points torch Tensor quant_min int quant_max int dtype torch dtype _quant_min_max_bounds_check quant_min quant_max dtype torch empty_like input dtype=dtype quantized_decomposed_lib define dequantize_per_token Tensor input Tensor scales Tensor zero_points int quant_min int quant_max ScalarType dtype ScalarType output_dtype - Tensor impl quantized_decomposed_lib dequantize_per_token CompositeExplicitAutograd dequantize_per_token input torch Tensor scales torch Tensor zero_points torch Tensor quant_min int quant_max int dtype torch dtype output_dtype torch dtype = torch float Per token dequantization Tensor using quantization parameters map floating point quantized values This means N dimension Tensor M M Mn N we calculate scales zero_points each N elements quantize every N elements same quantization parameter The dimension scales zero_points will M M Mn Args input torch Tensor quantized Tensor uint int etc scales float torch Tensor quantization parameter per token affine quantization zero_points int torch Tensor quantization parameter per token affine quantization quant_min int minimum quantized value input Tensor quant_max int maximum quantized value input Tensor dtype torch dtype dtype e g torch uint input Tensor output_dtype torch dtype dtype e g torch float output Tensor Returns dequantized Tensor dtype ` output_dtype ` input = input - zero_points input = input scales Since scales float type we need cast output dtype requested input output_dtype impl quantized_decomposed_lib dequantize_per_token Meta dequantize_per_token_meta input torch Tensor scales torch Tensor zero_points torch Tensor quant_min int quant_max int dtype torch dtype output_dtype torch dtype = torch float _quant_min_max_bounds_check quant_min quant_max dtype TODO support fp torch empty_like input dtype=output_dtype quantized_decomposed_lib define quantize_per_channel_group Tensor input Tensor scales Tensor zero_points int quant_min int quant_max ScalarType dtype int group_size - Tensor TODO dtype ignored now impl quantized_decomposed_lib quantize_per_channel_group CompositeExplicitAutograd quantize_per_channel_group input torch Tensor scales torch Tensor zero_points torch Tensor quant_min int quant_max int dtype torch dtype group_size= group_size = raise AssertionError group_size must needed GPTQ single column quantize group_size input shape - scales shape - == group_size = input shape - input shape - group_size = raise AssertionError input shape - must divisible group_size input dim = raise AssertionError input must -dimensional TODO check dtype currently we can t express torch int so s omitted to_quant = input reshape - group_size torch isnan to_quant sum = raise AssertionError to_quant must contain NaNs scales = scales reshape - zero_points = zero_points reshape - input_int = to_quant mul scales add zero_points round clamp_ quant_min quant_max dtype reshape_as input input_int impl quantized_decomposed_lib quantize_per_channel_group Meta quantize_per_channel_group_meta input torch Tensor scales torch Tensor zero_points torch Tensor quant_min int quant_max int dtype torch dtype group_size= Groupwise quantization within each channel -d Tensor using quantization parameters map floating point quantized values This means each row -d Tensor M N we calculate scales zero_points each ` group_size ` elements quantize every ` group_size ` elements same quantization parameter The dimension scales zero_points will M ceil N group_size Args input torch Tensor original float bfloat Tensor scales float torch Tensor quantization parameter per channel group affine quantization zero_points int torch Tensor quantization parameter per channel group affine quantization quant_min int minimum quantized value output Tensor quant_max int maximum quantized value output Tensor dtype torch dtype requested dtype e g torch uint output Tensor Returns Tensor requested dtype e g torch uint note quantization parameters stored Tensor we storing them function arguments instead group_size = raise AssertionError group_size must needed GPTQ single column quantize group_size input shape - scales shape - == group_size = input shape - input shape - group_size = raise AssertionError input shape - must divisible group_size input dim = raise AssertionError input must -dimensional torch empty_like input dtype=dtype quantized_decomposed_lib define dequantize_per_channel_group Tensor input Tensor scales Tensor zero_points int quant_min int quant_max ScalarType dtype int group_size ScalarType output_dtype - Tensor impl quantized_decomposed_lib dequantize_per_channel_group CompositeExplicitAutograd dequantize_per_channel_group w_int torch Tensor scales torch Tensor zero_points Optional torch Tensor quant_min int quant_max int dtype torch dtype group_size int = output_dtype torch dtype = torch float Groupwise dequantization within each channel -d Tensor using quantization parameters map floating point quantized values This means each row -d Tensor M N we calculate scales zero_points each ` group_size ` elements quantize every ` group_size ` elements same quantization parameter The dimension scales zero_points will M ceil N group_size Args input torch Tensor quantized Tensor uint int etc scales float torch Tensor quantization parameter per channel group affine quantization zero_points int torch Tensor quantization parameter per channel group affine quantization quant_min int minimum quantized value input Tensor quant_max int maximum quantized value input Tensor dtype torch dtype dtype e g torch uint input Tensor output_dtype torch dtype dtype e g torch float output Tensor Returns dequantized Tensor dtype ` output_dtype ` group_size = raise AssertionError group_size must needed GPTQ single column dequantize group_size w_int shape - scales shape - == group_size = w_int shape - w_int shape - group_size = raise AssertionError w_int shape - must divisible group_size w_int dim = raise AssertionError w_int must -dimensional w_int _grouped = w_int reshape - group_size scales = scales reshape - zero_points None zp = zero_points reshape - zp = torch zeros dtype=torch int device=scales device w_dq = w_int _grouped sub zp mul scales reshape_as w_int output_dtype w_dq quantized_decomposed_lib define fake_quant_per_channel Tensor input Tensor scales Tensor zero_points int axis int quant_min int quant_max - Tensor FakeQuantPerChannel torch autograd Function staticmethod pyrefly ignore bad-override forward ctx input scales zero_points axis quant_min quant_max scales dtype = torch float scales = scales torch float zero_points dtype = torch int zero_points = zero_points torch int input dtype = torch float raise AssertionError f Expecting input have dtype torch float got dtype input dtype axis = input dim raise AssertionError f Expecting axis input dim broadcast_dims = list range axis + list range axis + input ndim unsqueeze_scales = _unsqueeze_multiple scales broadcast_dims unsqueeze_zero_points = _unsqueeze_multiple zero_points broadcast_dims temp = torch round input unsqueeze_scales + unsqueeze_zero_points out = torch clamp temp quant_min quant_max - unsqueeze_zero_points unsqueeze_scales mask = torch logical_and temp = quant_min temp = quant_max ctx save_for_backward mask out staticmethod pyrefly ignore bad-override backward ctx gy mask = ctx saved_tensors gy mask None None None None None impl quantized_decomposed_lib fake_quant_per_channel Autograd fake_quant_per_channel input torch Tensor scales torch Tensor zero_points torch Tensor axis int quant_min int quant_max int - torch Tensor FakeQuantPerChannel apply input scales zero_points axis quant_min quant_max impl quantized_decomposed_lib fake_quant_per_channel Meta fake_quant_per_channel_meta input torch Tensor scales torch Tensor zero_points torch Tensor axis int quant_min int quant_max int - torch Tensor torch empty_like input quantized_decomposed_lib define convert_element_type no_fuse Tensor input ScalarType dtype - Tensor impl quantized_decomposed_lib convert_element_type no_fuse CompositeExplicitAutograd convert_element_type input torch Tensor dtype torch dtype - torch Tensor torch ops prims convert_element_type default input dtype impl quantized_decomposed_lib convert_element_type no_fuse Meta convert_element_type_meta input torch Tensor dtype torch dtype - torch Tensor torch empty_like input dtype=dtype