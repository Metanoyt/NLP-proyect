Owner s module inductor math unittest torch torch _dynamo utils counters torch _inductor config torch _inductor pattern_matcher PatternMatcherPass torch _inductor test_case run_tests TestCase torch testing _internal inductor_utils HAS_CPU HAS_TRITON dummy_fn x torch sigmoid x + math pi DummyModule torch nn Module forward x dummy_fn x TestInductorConfig TestCase classmethod setUpClass cls super setUpClass cls _saved_config = config save_config tearDown super tearDown config load_config _saved_config test_set config max_fusion_size = assertEqual config max_fusion_size assertEqual config get_config_copy max_fusion_size config max_fusion_size = assertEqual config max_fusion_size nested config prior = config triton cudagraphs config triton cudagraphs = prior assertEqual config triton cudagraphs prior assertEqual config get_config_copy triton cudagraphs prior test_save_load config max_fusion_size = config triton cudagraphs = True saved = config save_config config max_fusion_size = config triton cudagraphs = False saved = config save_config assertEqual config max_fusion_size assertEqual config triton cudagraphs False config load_config saved assertEqual config max_fusion_size assertEqual config triton cudagraphs True config load_config saved assertEqual config max_fusion_size assertEqual config triton cudagraphs False test_hasattr assertTrue hasattr config max_fusion_size assertFalse hasattr config missing_name test_invalid_names assertRaises AttributeError lambda config does_not_exist assertRaises AttributeError lambda config triton does_not_exist store config does_not_exist = True store config triton does_not_exist = True assertRaises AttributeError store assertRaises AttributeError store test_patch config patch max_fusion_size= assertEqual config max_fusion_size config patch max_fusion_size= assertEqual config max_fusion_size assertEqual config max_fusion_size config patch cpp threads max_fusion_size assertEqual config cpp threads assertEqual config max_fusion_size config patch cpp threads assertEqual config cpp threads assertEqual config cpp threads unittest skipIf HAS_CPU requires C++ compiler test_compile_api these mostly checking config processing doesn t blow up exceptions x = torch randn y = dummy_fn x checks = mode default mode reduce-overhead mode max-autotune options max-fusion-size unroll_reductions_threshold triton cudagraphs False dynamic True fullgraph True backend inductor disable True kwargs checks torch _dynamo reset opt_fn = torch compile dummy_fn kwargs torch testing assert_close opt_fn x y msg=f torch compile kwargs r failed test_get_compiler_config torch _inductor config inductor_default_config default_cudagraphs = inductor_default_config triton cudagraphs nn Module should update default config new value model = DummyModule optimized_module = torch compile model options= triton cudagraphs default_cudagraphs compiler_config = optimized_module get_compiler_config assertEqual compiler_config triton cudagraphs default_cudagraphs nn Module keep default config model = DummyModule optimized_module = torch compile model compiler_config = optimized_module get_compiler_config assertEqual compiler_config triton cudagraphs default_cudagraphs compile user func should update default config new value optimized_module = torch compile dummy_fn options= triton cudagraphs default_cudagraphs compiler_config = optimized_module get_compiler_config assertEqual compiler_config triton cudagraphs default_cudagraphs compile user func keep default config optimized_module = torch compile dummy_fn compiler_config = optimized_module get_compiler_config assertEqual compiler_config triton cudagraphs default_cudagraphs backend=eager expect None optimized_module = torch compile dummy_fn backend= eager compiler_config = optimized_module get_compiler_config assertTrue compiler_config None test_compile_api_passes_config ensure configs actually passed down inductor assertRaises torch _dynamo exc BackendCompilerFailed lambda torch compile dummy_fn options= _raise_error_for_testing True torch randn test_api_options reduce_overhead_opts = torch _inductor list_mode_options reduce-overhead assertEqual reduce_overhead_opts triton cudagraphs True assertEqual reduce_overhead_opts get max_autotune False False max_autotune_opts = torch _inductor list_mode_options max-autotune assertEqual max_autotune_opts max_autotune True assertEqual max_autotune_opts triton cudagraphs True max_autotune_opts = torch _inductor list_mode_options max-autotune dynamic=True assertEqual max_autotune_opts max_autotune True assertEqual max_autotune_opts triton cudagraphs True max_autotune_no_cudagraphs_opts = torch _inductor list_mode_options max-autotune-no-cudagraphs assertEqual max_autotune_no_cudagraphs_opts max_autotune True assertEqual max_autotune_no_cudagraphs_opts get triton cudagraphs False False test_invalid_backend assertRaises torch _dynamo exc InvalidBackend lambda torch compile dummy_fn backend= does_not_exist torch randn test_non_inductor_backend assert_options expected_mode=None expected_options=None backend gm _ mode=None options=None nonlocal call_count assertEqual mode expected_mode assertEqual options expected_options call_count += gm backend inp = torch randn fn x x + mode options None None fast-mode None None foo bar call_count = torch compile fn backend=assert_options mode options mode=mode options=options inp torch _dynamo reset assertEqual call_count test_codegen_skips_custom_passes _CustomPass PatternMatcherPass __init__ - None super __init__ __call__ g torch fx Graph apply g g = _CustomPass torch _inductor config patch post_grad_custom_post_pass=g post_grad_custom_pre_pass=g code = torch _inductor config codegen_config assertNotIn post_grad_custom code test_select_decomp_table_fallback_embedding_bag_byte_unpack Test select_decomp_table removes embedding_bag_byte_unpack when fallback enabled torch _inductor decomposition select_decomp_table Test fallback_embedding_bag_byte_unpack = False default config patch fallback_embedding_bag_byte_unpack=False decomp_table = select_decomp_table The operation should decompositions when fallback False Note We check s fast_random_decomps decompositions table assertTrue torch ops quantized embedding_bag_byte_unpack default decomp_table len decomp_table fast_random_decomps used when fallback False Test fallback_embedding_bag_byte_unpack = True config patch fallback_embedding_bag_byte_unpack=True decomp_table = select_decomp_table The operation should NOT decompositions when fallback True assertNotIn torch ops quantized embedding_bag_byte_unpack default decomp_table unittest skipIf HAS_TRITON requires triton test_options_do_something Verify we can populate load functions cache counters clear fn x y yy = y y x + yy view fn x y yy = y y x + yy view a_orig = torch rand dtype=torch float device= cpu b_orig = torch rand dtype=torch float device= cpu compiled_fn = torch compile fn options= fx_graph_cache True fx_graph_remote_cache False bundle_triton_into_fx_graph_cache True = a_orig clone b = b_orig clone = a_orig clone b = b_orig clone A first call should miss cache eager_result = fn b compiled_result = compiled_fn b assertEqual eager_result compiled_result assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file counters clear compiled_fn = torch compile fn options= fx_graph_cache False fx_graph_remote_cache False bundle_triton_into_fx_graph_cache False A first call should do nothing since cache disabled eager_result = fn b compiled_result = compiled_fn b assertEqual eager_result compiled_result assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file __name__ == __main__ run_tests