mypy ignore-errors collections copy dis enum inspect logging operator sys traceback collections OrderedDict collections abc Callable Iterator dataclasses fields is_dataclass typing Any Optional torch torch fx traceback fx_traceback torch _C _fx_map_aggregate map_aggregate _fx_map_arg map_arg torch _logging getArtifactLogger torch utils _traceback CapturedTraceback _compatibility compatibility graph Graph magic_methods reflectable_magic_methods immutable_collections immutable_dict immutable_list node Argument base_types Node Target operator_schemas check_for_mutable_operation __all__ = TracerBase GraphAppendingTracer TraceError Proxy MetaProxy Attribute ParameterProxy Scope ScopeContextManager log = logging getLogger __name__ annotation_log = getArtifactLogger __name__ annotation compatibility is_backward_compatible=False Scope Scope object records module path module type module Scope used track information module contains Node Graph GraphModule For example Sub torch nn Module forward x This will call_method Node GraphModule scope would module_path= sub module_type=Sub x transpose M torch nn Module __init__ - None sub = Sub forward x This will call_method Node well scope would module_path= None x = x transpose x = sub x x __init__ module_path str module_type Any super __init__ module_path = module_path module_type = module_type compatibility is_backward_compatible=False ScopeContextManager A context manager track Scope Node during symbolic tracing When entering forward function Module we ll update scope information current module when we exit we ll restore previous scope information __init__ scope Scope current_scope Scope super __init__ Keep copy prev scope restore exit _prev_scope = copy copy scope Update scope current scope scope module_path = current_scope module_path scope module_type = current_scope module_type Save reference so we can restore _scope = scope __enter__ _scope __exit__ args _scope module_path = _prev_scope module_path _scope module_type = _prev_scope module_type _COPY_META_FIELDS = nn_module_stack torch_fn source_fn_stack original_aten recompute ac_graph_id has_backward_hook from_node quantization_tag TODO deprecated _numeric_debug_handle TODO deprecated custom partitioner_tag compatibility is_backward_compatible=True TracerBase graph Graph record_stack_traces bool = False When record_stack_traces True only reocrd stack traces forward function names This helps when we want stack trace back model code _record_forward_stack_traces_only bool = False Feature flag mutable schema checking Enableby default check_mutable_operations bool = False Feature flag assert tracing trace_asserts bool = False Feature flag proxying accesses buffer values proxy_buffer_attributes bool = False Name function traced It will only used when ` ` root ` ` instance ` ` nn Module ` ` traced_func_name str = forward Maps containing module s name operator name scope Scope Records module call stack module_stack OrderedDict str tuple str Any Mapping node name module scope node_name_to_scope dict str tuple str type compatibility is_backward_compatible=True create_node kind str target Target args tuple Argument kwargs dict str Argument name Optional str = None type_expr Optional Any = None - Node Inserts graph node given target args kwargs name This method can overridden do extra checking validation modification values used node creation For example one might want disallow in-place operations being recorded kind == call_function check_mutable_operations check_for_mutable_operation target args kwargs node = graph create_node kind target args kwargs name type_expr TODO node_name_to_scope will depreciated favor node meta nn_module_stack node_name_to_scope node name = scope module_path scope module_type Optionally set stack trace created Node debugging purposes fx_traceback has_preserved_node_meta current_meta dict str Any = fx_traceback get_current_meta stack_trace = current_meta get stack_trace stack_trace node stack_trace = stack_trace Explicitly set stack_trace nn_module_stack source_fn node meta If other meta fields needed they can added here field _COPY_META_FIELDS field current_meta node meta field = copy copy current_meta field Here we decrement account sequence_nr having just been incremented while tracing lowered aten op new_seq_nr = torch autograd _get_sequence_nr - The sequence_nr increments every time new autograd Node created During FWD pass we store sequence_nr corresponding last autograd Node created fx node s meta A single aten op can create multiple autograd nodes case in-place foreach ops During BWD pass we retrieve sequence_nr stored current executing autograd Node See NOTE Sequence Number current_meta get in_grad_fn annotation_log debug seq_nr current_meta new_seq_nr = current_meta grad_fn_seq_nr - See Note Functionalization View Replay Annotation Overriding some node meta original node meta regenerated node replay_node Node = fx_traceback get_current_replay_node replay_node None node meta is_functional_regenerated = True seq_nr replay_node meta annotation_log debug seq_nr replay_node new_seq_nr = replay_node meta seq_nr custom replay_node meta node meta custom = replay_node meta get custom stack_trace replay_node meta node stack_trace = replay_node meta get stack_trace annotation_log debug Assigning new_seq_nr s s new_seq_nr node name node meta seq_nr = new_seq_nr module_stack node meta nn_module_stack = copy copy module_stack record_stack_traces node stack_trace user_stack_summary = CapturedTraceback extract summary user_stack_summary user_stack_summary = _filter_traceback_frames user_stack_summary user_stack_summary node stack_trace = join user_stack_summary format strip log debug create_node s node node _filter_traceback_frames user_stack_summary traceback StackSummary - traceback StackSummary This method can overridden customize frame filtering logic recorded stack trace user_frames = _record_forward_stack_traces_only user_frames = frame frame user_stack_summary frame name == forward frame filename endswith torch __init__ py first_forward = - i frame enumerate user_stack_summary frame name == forward user_frames = user_stack_summary i first_forward = i break Not having forward call stacktrace implies stacktrace will probably irrelevant first_forward == - user_frames = torch fx experimental symbolic_shapes uninteresting_files user_frames = frame frame user_frames frame filename uninteresting_files traceback StackSummary from_list user_frames compatibility is_backward_compatible=True proxy node Node - Proxy Proxy node compatibility is_backward_compatible=True create_proxy kind str target Target args tuple Any kwargs dict str Any name Optional str = None type_expr Optional Any = None fix noqa when updating bc tests proxy_factory_fn Callable Node Proxy = None noqa RUF Create Node given arguments then Node wrapped Proxy object If kind = placeholder then we re creating Node represents parameter function If we need encode default parameter we use ` ` args ` ` tuple ` ` args ` ` otherwise empty ` ` placeholder ` ` Nodes args_ = create_arg args kwargs_ = create_arg kwargs assert isinstance args_ tuple assert isinstance kwargs_ dict node = create_node kind target args_ kwargs_ name type_expr proxy_factory_fn proxy = proxy node proxy = proxy_factory_fn node proxy _find_user_frame Find Python stack frame executing user code during symbolic tracing We have do little dance here Basically walk up callstack record first frame pytorch source This frame executing user code during tracing frame = inspect currentframe pt_files = torch fx proxy py torch fx _symbolic_trace py torch fx experimental proxy_tensor py torch _ops py torch _tensor py torch utils _python_dispatch py torch _prims_common wrappers py torch _refs __init__ py torch _refs nn functional __init__ py torch utils _stats py while frame frame = frame f_back frame all frame f_code co_filename endswith file file pt_files break frame None frame compatibility is_backward_compatible=True create_arg Any - Argument A method lowers objects seen arguments during symbolic evaluation into Argument types can stored IR Can override support more trace-specific types IMPORTANT Are you here because you trying proxy new type into graph Please Please Please contact someone PyTorch Compiler team considerations subtle When you add new type all downstream consumers pass writers need handle new type torch fx intended easy write passes so we will push back against new types In torch compile s IR there only specific operations go into graph In particular Tensor operations should go into graph non-Tensor operations shouldn t What means constructors new types SHOULD NOT become nodes FX graph handler = _create_arg_bypass get type handler None just performance optimization can removed needed common types we have fast path avoid isinstance overhead doesn t remove checks below since we need handle subclasses handler isinstance Proxy node most common arg type goes first hasattr __fx_create_arg__ __fx_create_arg__ aggregates isinstance tuple hasattr _fields NamedTuple constructors don t seem like getting generator expression argument their constructor so build intermediate tuple unpack into NamedTuple constructor args = create_arg elem elem type args type ignore arg-type type create_arg elem elem isinstance list create_arg elem elem isinstance dict _create_arg_dict isinstance slice slice create_arg start create_arg stop create_arg step isinstance range range create_arg start create_arg stop create_arg step isinstance torch _ops OpOverload torch _ops HigherOrderOperator is_dataclass kwargs = field name create_arg getattr field name field fields create_node call_function __class__ kwargs isinstance base_types enum Enum None raise NotImplementedError f argument type type compatibility is_backward_compatible=True to_bool obj Proxy - bool Called when proxy object being converted boolean such when used control flow Normally we don t know what do because we don t know value proxy custom tracer can attach more information graph node using create_node can choose value raise TraceError symbolically traced variables cannot used inputs control flow compatibility is_backward_compatible=True iter obj Proxy - Iterator Called when proxy object being iterated over such when used control flow Normally we don t know what do because we don t know value proxy custom tracer can attach more information graph node using create_node can choose iterator raise TraceError Proxy object cannot iterated This can attempted when Proxy used loop args kwargs function argument See torch fx docs pytorch org more detailed explanation what types control flow can traced check out Proxy docstring help troubleshooting Proxy iteration errors compatibility is_backward_compatible=True keys obj Proxy - Any Called when proxy object has keys method called This what happens when called proxy This should iterator suppose work your custom tracer Attribute obj keys used Proxy object when just appending graph while tracing compatibility is_backward_compatible=True GraphAppendingTracer TracerBase __init__ graph Graph super __init__ graph = graph scope = Scope None module_stack = collections OrderedDict node_name_to_scope = compatibility is_backward_compatible=False assert_fn x assert x compatibility is_backward_compatible=True TraceError ValueError pass compatibility is_backward_compatible=True Proxy ` ` Proxy ` ` objects ` ` Node ` ` wrappers flow through program during symbolic tracing record all operations ` ` torch ` ` function calls method calls operators they touch into growing FX Graph If you re doing graph transforms you can wrap your own ` ` Proxy ` ` method around raw ` ` Node ` ` so you can use overloaded operators add additional things ` ` Graph ` ` ` ` Proxy ` ` objects cannot iterated In other words symbolic tracer will throw error ` ` Proxy ` ` used loop ` ` args ` ` ` ` kwargs ` ` function argument There two main ways around Factor out untraceable logic into top-level function use ` ` fx wrap ` ` If control flow static i e loop trip count based some hyperparameter code can kept its original position refactored into something like i range some_hyperparameter indexed_item = proxied_value i For more detailed description into Proxy internals check out Proxy section ` torch fx README md ` compatibility is_backward_compatible=True __init__ node Node tracer Optional TracerBase = None tracer None This allows you create Proxy object around raw Node tracer = GraphAppendingTracer node graph tracer = tracer node = node __repr__ - str f Proxy node name __getattr__ k - Attribute note added graph yet method call we peephole optimize method invocation Attribute k __getstate__ - dict __dict__ __deepcopy__ memo - dict We have explicitly override method because otherwise deepcopy will go __getattr__ __deepcopy__ Attribute __deepcopy__ may go into infinite loop some cases copy new_dict = k v __dict__ items try new_obj = copy deepcopy v memo except Exception log warning Shallow copy s Proxy because cannot deepcopied Proxy created node s k node name new_obj = copy copy v new_dict k = new_obj assert node new_dict assert tracer new_dict new_proxy = Proxy new_dict node new_dict tracer k v new_dict items new_proxy __dict__ k = v new_proxy __setstate__ d This called when being unpickled loaded __dict__ = d __call__ args kwargs - Proxy tracer create_proxy call_method __call__ + args kwargs __iter__ - Iterator Proxy frame = inspect currentframe assert frame None calling_frame = frame f_back assert calling_frame None inst_list = list dis get_instructions calling_frame f_code sys version_info = bisect bisect_left inst_idx = bisect_left inst_list calling_frame f_lasti key=lambda x x offset inst_idx = calling_frame f_lasti inst = inst_list inst_idx inst opname == UNPACK_SEQUENCE i i range inst argval type ignore index tracer iter __abs__ tracer create_proxy call_function operator abs __bool__ - bool tracer trace_asserts check boolean used assertion bytecode pattern assertions pretty stable Python -- frame = inspect currentframe assert frame None calling_frame = frame f_back assert calling_frame None insts = list dis get_instructions calling_frame f_code sys version_info = bisect bisect_left cur = bisect_left insts calling_frame f_lasti key=lambda x x offset cur = calling_frame f_lasti inst = insts cur inst opname == POP_JUMP_IF_TRUE first = insts cur + assert inst arg None last = insts inst arg - starts_with_assert = first opname == LOAD_GLOBAL first argval == AssertionError first opname == LOAD_ASSERTION_ERROR starts_with_assert last opname == RAISE_VARARGS tracer create_proxy call_function assert_fn True tracer to_bool compatibility is_backward_compatible=True keys tracer keys __len__ raise RuntimeError len supported symbolic tracing default If you want call recorded please call torch fx wrap len module scope classmethod __torch_function__ cls orig_method types args=None kwargs=None args = args args kwargs = kwargs kwargs tracers dict Any None = find_tracer isinstance cls tracers tracer = None map_aggregate args find_tracer map_aggregate kwargs find_tracer len tracers raise RuntimeError f Found multiple different tracers list tracers keys while f trying trace operations orig_method tracer = next iter tracers keys isinstance orig_method torch _C ScriptMethod args = orig_method owner + args tracer create_proxy call_method orig_method name args kwargs torch overrides is_tensor_method_or_property orig_method tracer create_proxy call_method orig_method __name__ args kwargs isinstance orig_method torch _ops HigherOrderOperator TODO Define how symbolically trace HigherOrderOperators raise RuntimeError Unable symbolically trace HigherOrderOperators tracer create_proxy call_function orig_method args kwargs name=tracer graph _target_to_str orig_method __name__ compatibility is_backward_compatible=False MetaProxy Proxy A Proxy subclass propagates metadata meta val during graph tracing __init__ node Node tracer Optional TracerBase = None fake_mode=None super __init__ node tracer fake_mode = fake_mode __repr__ - str f MetaProxy node name classmethod __torch_function__ cls orig_method types args=None kwargs=None args = args args kwargs = kwargs kwargs meta_proxy = None arg args isinstance arg MetaProxy meta_proxy = arg break assert meta_proxy None No MetaProxy found arguments one expected proxy = super __torch_function__ orig_method types args kwargs meta_proxy fake_mode proxy node meta val = orig_method node meta val isinstance Proxy args kwargs MetaProxy proxy node proxy tracer meta_proxy fake_mode compatibility is_backward_compatible=True Attribute Proxy compatibility is_backward_compatible=True __init__ root Proxy attr str root = root attr = attr tracer = root tracer _node Optional Node = None property node node attributes added lazily since most will just method calls which do rely getitem call _node None _node = tracer create_proxy call_function getattr root attr node _node __call__ args kwargs tracer create_proxy call_method attr root + args kwargs compatibility is_backward_compatible=False ParameterProxy Proxy A special proxy which lets shape size dim few other attribute accesses pass through underlying module parameter object so conditional tests these attributes will throw exception during tracing __init__ tracer TracerBase node Node name param super __init__ node tracer assert isinstance param torch nn Parameter param = param name = name __repr__ - str f ParameterProxy name property shape param shape size param size dim param dim property ndim param ndim numel param numel nelement param nelement method magic_methods _scope method impl args kwargs tracer = args tracer target = getattr operator method tracer create_proxy call_function target args kwargs impl __name__ = method as_magic = f __ method strip _ __ setattr Proxy as_magic impl _scope method _define_reflectable orig_method_name method_name = f __r orig_method_name strip _ __ impl rhs target = getattr operator orig_method_name tracer create_proxy call_function target rhs impl __name__ = method_name impl __qualname__ = method_name setattr Proxy method_name impl orig_method_name reflectable_magic_methods _define_reflectable orig_method_name _no_nodes_error arg raise RuntimeError Keys dictionaries used argument cannot contain f Node Got key arg _create_arg_dict r = k v items isinstance k str Check invalid dict keys We do want Proxy appear anywhere within key Since keys can collection types we iterate through key map_arg k = create_arg k map_arg k _no_nodes_error r k = create_arg v r _create_arg_bypass = t lambda t base_types type None type torch _ops OpOverload torch _ops HigherOrderOperator _create_arg_bypass Proxy = lambda node _create_arg_bypass tuple = lambda tuple create_arg elem elem _create_arg_bypass list = lambda create_arg elem elem _create_arg_bypass dict = _create_arg_dict _create_arg_bypass immutable_list = _create_arg_bypass list _create_arg_bypass immutable_dict = _create_arg_bypass dict