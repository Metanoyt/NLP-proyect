mypy allow-untyped-defs itertools collections abc Iterable Iterator Sequence Sized typing Generic Optional TypeVar Union torch Note For benchmarking changes samplers see benchmarks data samplers_bench py This benchmark compares performance different sampler implementations can used evaluate impact optimizations __all__ = BatchSampler RandomSampler Sampler SequentialSampler SubsetRandomSampler WeightedRandomSampler _T_co = TypeVar _T_co covariant=True Sampler Generic _T_co r Base all Samplers Every Sampler subclass has provide meth ` __iter__ ` method providing way iterate over indices lists indices batches dataset elements may provide meth ` __len__ ` method returns length returned iterators Example xdoctest +SKIP AccedingSequenceLengthSampler Sampler int __init__ data List str - None data = data __len__ - int len data __iter__ - Iterator int sizes = torch tensor len x x data yield torch argsort sizes tolist AccedingSequenceLengthBatchSampler Sampler List int __init__ data List str batch_size int - None data = data batch_size = batch_size __len__ - int len data + batch_size - batch_size __iter__ - Iterator List int sizes = torch tensor len x x data batch torch chunk torch argsort sizes len yield batch tolist note The meth ` __len__ ` method isn t strictly required ` ~torch utils data DataLoader ` expected any calculation involving length ` ~torch utils data DataLoader ` __iter__ - Iterator _T_co raise NotImplementedError NOTE Lack Default ` __len__ ` Python Abstract Base Classes Many times we have abstract representing collection iterable data e g ` torch utils data Sampler ` its subclasses optionally implementing ` __len__ ` method In such cases we must make sure provide default implementation because both straightforward default implementations have their issues + ` NotImplemented ` Calling ` len subclass_instance ` raises TypeError NotImplementedType object cannot interpreted integer + ` raise NotImplementedError ` This prevents triggering some fallback behavior E g built-in ` list X ` tries call ` len X ` first executes different code path method found ` NotImplemented ` returned while raising ` NotImplementedError ` will propagate make call fail where could have used ` __iter__ ` complete call Thus only two sensible things do + provide default ` __len__ ` + raise ` TypeError ` instead which what Python uses when users call method defined object ssnl verifies works least Python SequentialSampler Sampler int r Samples elements sequentially always same order Args data_source Sized data source sample Must implement __len__ data_source Sized __init__ data_source Sized - None data_source = data_source __iter__ - Iterator int iter range len data_source __len__ - int len data_source RandomSampler Sampler int r Samples elements randomly If without replacement then sample shuffled dataset If replacement then user can specify attr ` num_samples ` draw Args data_source Sized data source sample Must implement __len__ replacement bool samples drawn on-demand replacement ` ` True ` ` default= ` ` False ` ` num_samples int number samples draw default= ` len dataset ` generator Generator Generator used sampling data_source Sized replacement bool __init__ data_source Sized replacement bool = False num_samples Optional int = None generator=None - None data_source = data_source replacement = replacement _num_samples = num_samples generator = generator isinstance replacement bool raise TypeError f replacement should boolean value got replacement= replacement isinstance num_samples int num_samples = raise ValueError f num_samples should positive integer value got num_samples= num_samples property num_samples - int dataset size might change runtime _num_samples None len data_source _num_samples __iter__ - Iterator int n = len data_source generator None seed = int torch empty dtype=torch int random_ item generator = torch Generator generator manual_seed seed generator = generator replacement _ range num_samples yield torch randint high=n size= dtype=torch int generator=generator tolist yield torch randint high=n size= num_samples dtype=torch int generator=generator tolist _ range num_samples n yield torch randperm n generator=generator tolist yield torch randperm n generator=generator tolist num_samples n __len__ - int num_samples SubsetRandomSampler Sampler int r Samples elements randomly given list indices without replacement Args indices sequence sequence indices generator Generator Generator used sampling indices Sequence int __init__ indices Sequence int generator=None - None indices = indices generator = generator __iter__ - Iterator int i torch randperm len indices generator=self generator tolist yield indices i __len__ - int len indices WeightedRandomSampler Sampler int r Samples elements ` ` len weights - ` ` given probabilities weights Args weights sequence sequence weights necessary summing up one num_samples int number samples draw replacement bool ` ` True ` ` samples drawn replacement If they drawn without replacement which means when sample index drawn row cannot drawn again row generator Generator Generator used sampling Example xdoctest +IGNORE_WANT non-deterministic list WeightedRandomSampler replacement=True list WeightedRandomSampler replacement=False weights torch Tensor num_samples int replacement bool __init__ weights Sequence float num_samples int replacement bool = True generator=None - None isinstance num_samples int isinstance num_samples bool num_samples = raise ValueError f num_samples should positive integer value got num_samples= num_samples isinstance replacement bool raise ValueError f replacement should boolean value got replacement= replacement weights_tensor = torch as_tensor weights dtype=torch double len weights_tensor shape = raise ValueError weights should d sequence given f weights have shape tuple weights_tensor shape weights = weights_tensor num_samples = num_samples replacement = replacement generator = generator __iter__ - Iterator int rand_tensor = torch multinomial weights num_samples replacement generator=self generator yield iter rand_tensor tolist __len__ - int num_samples BatchSampler Sampler list int r Wraps another sampler yield mini-batch indices Args sampler Sampler Iterable Base sampler Can any iterable object batch_size int Size mini-batch drop_last bool If ` ` True ` ` sampler will drop last batch its size would less than ` ` batch_size ` ` Example list BatchSampler SequentialSampler range batch_size= drop_last=False list BatchSampler SequentialSampler range batch_size= drop_last=True __init__ sampler Union Sampler int Iterable int batch_size int drop_last bool - None Since collections abc Iterable does check ` __getitem__ ` which one way object iterable we don t do ` isinstance ` check here isinstance batch_size int isinstance batch_size bool batch_size = raise ValueError f batch_size should positive integer value got batch_size= batch_size isinstance drop_last bool raise ValueError f drop_last should boolean value got drop_last= drop_last sampler = sampler batch_size = batch_size drop_last = drop_last __iter__ - Iterator list int sampler_iter = iter sampler drop_last Create multiple references same iterator args = sampler_iter batch_size batch_droplast zip args strict=False yield batch_droplast batch = itertools islice sampler_iter batch_size while batch yield batch batch = itertools islice sampler_iter batch_size __len__ - int Can only called sampler has __len__ implemented We cannot enforce condition so we turn off typechecking implementation below Somewhat related see NOTE Lack Default ` __len__ ` Python Abstract Base Classes drop_last len sampler batch_size type ignore arg-type len sampler + batch_size - batch_size type ignore arg-type