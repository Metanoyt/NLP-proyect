mypy allow-untyped-defs itertools typing collections abc Callable dataclasses dataclass typing NamedTuple Optional torch torch nn functional F torch _subclasses FakeTensor torch ao quantization fx utils get_new_attr_name_with_prefix torch ao quantization pt e export_utils _WrapperModule torch ao quantization pt e utils _get_aten_graph_module_for_pattern _is_conv_node _is_conv_transpose_node torch ao quantization quantizer QuantizationAnnotation QuantizationSpec SharedQuantizationSpec torch ao quantization quantizer utils _annotate_input_qspec_map _annotate_output_qspec torch fx Node torch fx passes utils matcher_with_name_node_map_utils SubgraphMatcherWithNameNodeMap torch fx passes utils source_matcher_utils get_source_partitions __all__ = OperatorConfig OperatorPatternType QuantizationConfig get_input_act_qspec get_output_act_qspec get_weight_qspec get_bias_qspec OP_TO_ANNOTATOR propagate_annotation In absence better name just winging QuantizationConfig dataclass eq=True frozen=True QuantizationConfig input_activation Optional QuantizationSpec output_activation Optional QuantizationSpec weight Optional QuantizationSpec bias Optional QuantizationSpec TODO remove since we can use observer_or_fake_quant_ctr express is_qat bool = False Use Annotated because list Callable __module__ read-only OperatorPatternType = typing Annotated list Callable None OperatorPatternType __module__ = torch ao quantization quantizer xnnpack_quantizer_utils AnnotatorType = Callable torch fx GraphModule Optional QuantizationConfig Optional Callable Node bool Optional list list Node OP_TO_ANNOTATOR dict str AnnotatorType = register_annotator op str - Callable AnnotatorType None decorator annotator AnnotatorType - None OP_TO_ANNOTATOR op = annotator decorator OperatorConfig NamedTuple fix List str List List Union nn Module FunctionType BuiltinFunctionType Basically we mapping quantization config some list patterns pattern defined list nn module function builtin function names e g nn Conv d torch relu torch add We have resolved whether fusion can considered internal details quantizer hence does need communication user Note pattern really informative since does really tell us graph structure resulting list ops config QuantizationConfig operators list OperatorPatternType _is_annotated nodes list Node Given list nodes represents operator pattern check any node annotated True any node annotated otherwise False annotated = False node nodes annotated = annotated quantization_annotation node meta node meta quantization_annotation _annotated annotated _mark_nodes_as_annotated nodes list Node node nodes node None quantization_annotation node meta node meta quantization_annotation = QuantizationAnnotation node meta quantization_annotation _annotated = True get_input_act_qspec quantization_config Optional QuantizationConfig quantization_config None None quantization_config input_activation None None quantization_spec QuantizationSpec = quantization_config input_activation assert quantization_spec qscheme torch per_tensor_affine torch per_tensor_symmetric quantization_spec get_output_act_qspec quantization_config Optional QuantizationConfig quantization_config None None quantization_config output_activation None None quantization_spec QuantizationSpec = quantization_config output_activation assert quantization_spec qscheme torch per_tensor_affine torch per_tensor_symmetric quantization_spec get_weight_qspec quantization_config Optional QuantizationConfig quantization_config None None assert quantization_config None quantization_config weight None None quantization_spec QuantizationSpec = quantization_config weight quantization_spec qscheme torch per_tensor_symmetric torch per_channel_symmetric None raise ValueError f Unsupported quantization_spec quantization_spec weight quantization_spec get_bias_qspec quantization_config Optional QuantizationConfig quantization_config None None assert quantization_config None quantization_config bias None None quantization_spec QuantizationSpec = quantization_config bias assert quantization_spec dtype == torch float Only float dtype bias supported bias right now quantization_spec register_annotator linear _annotate_linear gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional Callable Node bool = None - Optional list list Node annotated_partitions = input_act_qspec = get_input_act_qspec quantization_config output_act_qspec = get_output_act_qspec quantization_config weight_qspec = get_weight_qspec quantization_config bias_qspec = get_bias_qspec quantization_config node gm graph nodes node op = call_function node target = torch ops aten linear default continue filter_fn filter_fn node continue act_node = node args weight_node = node args bias_node = None len node args bias_node = node args _is_annotated node False type ignore list-item _annotate_input_qspec_map node act_node input_act_qspec _annotate_input_qspec_map node weight_node weight_qspec nodes_to_mark_annotated = node weight_node bias_node _annotate_input_qspec_map node bias_node bias_qspec nodes_to_mark_annotated append bias_node _annotate_output_qspec node output_act_qspec _mark_nodes_as_annotated nodes_to_mark_annotated annotated_partitions append nodes_to_mark_annotated annotated_partitions register_annotator linear_relu _annotate_linear_relu gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional Callable Node bool = None - Optional list list Node annotated_partitions = input_act_qspec = get_input_act_qspec quantization_config output_act_qspec = get_output_act_qspec quantization_config weight_qspec = get_weight_qspec quantization_config bias_qspec = get_bias_qspec quantization_config node gm graph nodes node op = call_function node target torch ops aten relu default torch ops aten relu_ default continue relu_node = node maybe_linear_node = node args isinstance maybe_linear_node Node maybe_linear_node op = call_function maybe_linear_node target = torch ops aten linear default continue linear_node = maybe_linear_node len linear_node users linear node has multiple users then can t fused relu continue input_qspec_map = input_act = linear_node args assert isinstance input_act Node input_qspec_map input_act = input_act_qspec weight = linear_node args assert isinstance weight Node input_qspec_map weight = weight_qspec adding weight node partition well partition = relu_node linear_node weight bias = linear_node args len linear_node args None isinstance bias Node input_qspec_map bias = bias_qspec partition append bias _is_annotated partition continue filter_fn any filter_fn n n partition continue linear_node meta quantization_annotation = QuantizationAnnotation input_qspec_map=input_qspec_map _annotated=True relu_node meta quantization_annotation = QuantizationAnnotation output_qspec=output_act_qspec _annotated=True _mark_nodes_as_annotated partition annotated_partitions append partition annotated_partitions register_annotator conv _annotate_conv gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional Callable Node bool = None - Optional list list Node annotated_partitions = n gm graph nodes n op = call_function n target torch ops aten conv d default torch ops aten conv d default continue conv_node = n input_qspec_map = input_act = conv_node args assert isinstance input_act Node input_qspec_map input_act = get_input_act_qspec quantization_config weight = conv_node args assert isinstance weight Node input_qspec_map weight = get_weight_qspec quantization_config adding weight node partition well partition = conv_node conv_node args bias = conv_node args len conv_node args None isinstance bias Node input_qspec_map bias = get_bias_qspec quantization_config partition append bias _is_annotated partition continue filter_fn any filter_fn n n partition continue conv_node meta quantization_annotation = QuantizationAnnotation input_qspec_map=input_qspec_map output_qspec=get_output_act_qspec quantization_config _annotated=True _mark_nodes_as_annotated partition annotated_partitions append partition annotated_partitions _do_annotate_conv_relu gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional Callable Node bool = None is_conv_transpose bool = False annotated_partitions = n gm graph nodes n op = call_function n target torch ops aten relu default torch ops aten relu_ default continue relu_node = n maybe_conv_node = n args is_conv_node = _is_conv_transpose_node is_conv_transpose _is_conv_node isinstance maybe_conv_node Node is_conv_node maybe_conv_node continue conv_node = maybe_conv_node len conv_node users relu shouldn t fuseable conv there other users convolution continue input_qspec_map = input_act = conv_node args assert isinstance input_act Node input_qspec_map input_act = get_input_act_qspec quantization_config weight = conv_node args assert isinstance weight Node input_qspec_map weight = get_weight_qspec quantization_config adding weight node partition well partition = relu_node conv_node conv_node args bias = conv_node args len conv_node args None isinstance bias Node input_qspec_map bias = get_bias_qspec quantization_config partition append bias pyrefly ignore bad-argument-type _is_annotated partition continue pyrefly ignore bad-argument-type filter_fn any filter_fn n n partition continue conv_node meta quantization_annotation = QuantizationAnnotation input_qspec_map=input_qspec_map _annotated=True relu_node meta quantization_annotation = QuantizationAnnotation output_qspec=get_output_act_qspec quantization_config type ignore arg-type _annotated=True pyrefly ignore bad-argument-type _mark_nodes_as_annotated partition annotated_partitions append partition annotated_partitions register_annotator conv_relu _annotate_conv_relu gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional Callable Node bool = None - Optional list list Node _do_annotate_conv_relu gm quantization_config filter_fn is_conv_transpose=False register_annotator conv_transpose_relu _annotate_conv_transpose_relu gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional Callable Node bool = None - Optional list list Node _do_annotate_conv_relu gm quantization_config filter_fn is_conv_transpose=True register_annotator conv_bn _annotate_conv_bn gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional Callable Node bool = None - Optional list list Node Find conv + batchnorm partitions Note This only used QAT In PTQ batchnorm should already fused into conv _do_annotate_conv_bn gm quantization_config filter_fn has_relu=False register_annotator conv_bn_relu _annotate_conv_bn_relu gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional Callable Node bool = None - Optional list list Node Find conv + batchnorm + relu partitions Note This only used QAT In PTQ batchnorm should already fused into conv _do_annotate_conv_bn gm quantization_config filter_fn has_relu=True register_annotator conv_transpose_bn _annotate_conv_transpose_bn gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional Callable Node bool = None - Optional list list Node Find conv_transpose + batchnorm partitions Note This only used QAT In PTQ batchnorm should already fused into conv _do_annotate_conv_bn gm quantization_config filter_fn has_relu=False is_conv_transpose=True register_annotator conv_transpose_bn_relu _annotate_conv_transpose_bn_relu gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional Callable Node bool = None - Optional list list Node Find conv_transpose + batchnorm + relu partitions Note This only used QAT In PTQ batchnorm should already fused into conv _do_annotate_conv_bn gm quantization_config filter_fn has_relu=True is_conv_transpose=True _do_annotate_conv_bn gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional Callable Node bool has_relu bool is_conv_transpose bool = False - list list Node Given function takes ` conv_fn ` returns conv-bn -relu pattern list annotated partitions The output pattern must include dictionary string name node following names input conv weight bias output Example inputs conv-bn d patterns _conv d_bn_example_inputs = torch randn x torch randn conv_weight torch randn conv_bias torch randn bn_weight torch randn bn_bias torch randn bn_running_mean torch randn bn_running_var Example inputs conv-bn d patterns _conv d_bn_example_inputs = torch randn x torch randn conv_weight torch randn conv_bias torch randn bn_weight torch randn bn_bias torch randn bn_running_mean torch randn bn_running_var get_pattern conv_fn Callable relu_is_inplace bool _conv_bn x conv_weight conv_bias bn_weight bn_bias bn_rm bn_rv conv = conv_fn x conv_weight conv_bias bn = F batch_norm conv bn_rm bn_rv bn_weight bn_bias training=True has_relu output = F relu_ bn relu_is_inplace F relu bn output = bn output input x conv conv weight conv_weight bias conv_bias output output _WrapperModule _conv_bn Needed matching otherwise matches gets filtered out due unused nodes returned batch norm gm graph eliminate_dead_code gm recompile matches = is_conv_transpose combinations = F conv_transpose d _conv d_bn_example_inputs F conv_transpose d _conv d_bn_example_inputs combinations = F conv d _conv d_bn_example_inputs type ignore list-item F conv d _conv d_bn_example_inputs type ignore list-item Add ` is_cuda ` ` relu_is_inplace ` dimensions combinations = itertools product type ignore assignment combinations True False torch cuda is_available False is_cuda True False has_relu False relu_is_inplace Match against all conv dimensions cuda variants conv_fn example_inputs is_cuda relu_is_inplace combinations type ignore misc pattern = get_pattern conv_fn relu_is_inplace type ignore has-type pattern = _get_aten_graph_module_for_pattern pattern example_inputs is_cuda type ignore has-type pattern graph eliminate_dead_code pattern recompile matcher = SubgraphMatcherWithNameNodeMap pattern ignore_literals=True matches extend matcher match gm graph Annotate nodes returned matches annotated_partitions = match matches name_node_map = match name_node_map input_node = name_node_map input conv_node = name_node_map conv weight_node = name_node_map weight bias_node = name_node_map bias output_node = name_node_map output TODO annotate uses input weight bias separately instead assuming they come single conv node This possible today because input may have multiple users we can t rely conv node always being first user This case models skip connections like resnet Validate conv args conv_node args input_node raise ValueError Conv arg did contain input node input_node conv_node args weight_node raise ValueError Conv arg did contain weight node weight_node len conv_node args conv_node args bias_node raise ValueError Conv arg did contain bias node bias_node Skip partition already annotated filtered out user partition = conv_node weight_node bias_node None partition append bias_node _is_annotated partition continue filter_fn any filter_fn n n partition continue Annotate conv inputs pattern output input_qspec_map = input_qspec_map input_node = get_input_act_qspec quantization_config input_qspec_map weight_node = get_weight_qspec quantization_config bias_node None input_qspec_map bias_node = get_bias_qspec quantization_config conv_node meta quantization_annotation = QuantizationAnnotation input_qspec_map=input_qspec_map _annotated=True output_node meta quantization_annotation = QuantizationAnnotation output_qspec=get_output_act_qspec quantization_config type ignore arg-type _annotated=True _mark_nodes_as_annotated partition annotated_partitions append partition annotated_partitions register_annotator gru_io_only _annotate_gru_io_only gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional Callable Node bool = None - Optional list list Node gru_partitions = get_source_partitions gm graph torch nn GRU filter_fn gru_partitions = list itertools chain from_iterable gru_partitions values annotated_partitions = gru_partition gru_partitions annotated_partitions append gru_partition nodes output_nodes = gru_partition output_nodes input_nodes = gru_partition input_nodes skip annotation already annotated _is_annotated input_nodes + output_nodes continue inside each GRU partition we should able annotate each linear subgraph input_act = input_nodes input_act_user = next iter input_act users keys assert isinstance input_act Node assert isinstance input_act_user Node input_act_user meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act get_input_act_qspec quantization_config _annotated=True hidden_state = input_nodes hidden_state_user = next iter hidden_state users keys assert isinstance hidden_state Node assert isinstance hidden_state_user Node hidden_state_user meta quantization_annotation = QuantizationAnnotation input_qspec_map= hidden_state get_input_act_qspec quantization_config _annotated=True assert len output_nodes == expecting GRU have two outputs output output_nodes output meta quantization_annotation = QuantizationAnnotation output_qspec=get_output_act_qspec quantization_config _annotated=True nodes_to_mark_annotated = list gru_partition nodes _mark_nodes_as_annotated nodes_to_mark_annotated annotated_partitions register_annotator adaptive_avg_pool d _annotate_adaptive_avg_pool d gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional Callable Node bool = None - Optional list list Node Always annotate adaptive_avg_pool d op module_partitions = get_source_partitions gm graph torch nn AdaptiveAvgPool d F adaptive_avg_pool d filter_fn partitions = list itertools chain from_iterable module_partitions values annotated_partitions = partition partitions pool_node = partition output_nodes pool_node op = call_function pool_node target = torch ops aten adaptive_avg_pool d default raise ValueError f pool_node aten adaptive_avg_pool d operator _is_annotated pool_node continue annotated_partitions append partition nodes input_act = pool_node args assert isinstance input_act Node only annotate input output sharing operator when output input node annotated quantization_annotation input_act meta input_act meta quantization_annotation _annotated input_act meta quantization_annotation output_qspec None input_act_qspec = get_input_act_qspec quantization_config input_act_qspec = SharedQuantizationSpec input_act output sharing input output_act_qspec = SharedQuantizationSpec input_act pool_node pool_node meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act input_act_qspec output_qspec=output_act_qspec _annotated=True annotated_partitions _is_input_large_scalar node Node gm torch fx GraphModule Check input large scalar value So we can skip quantization node since histc op HistogramObserver only works values up certain upper bound node op == get_attr qualified_name = str node target module_path _ name = qualified_name rpartition submod = gm get_submodule module_path tensor = getattr submod name torch histc works until upper bound HISTC_UPPER_BOUND = e tensor numel == abs tensor item HISTC_UPPER_BOUND False _is_input_non_float_tensor node Node Check input float tensor so we can skip quantization node since observers only works float Tensors val node meta isinstance node meta val FakeTensor True node meta val dtype = torch float register_annotator add_relu _annotate_add_relu gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional Callable Node bool = None - Optional list list Node annotated_partitions = node gm graph nodes node op = call_function node target torch ops aten relu default torch ops aten relu_ default continue relu_node = node maybe_add = node args isinstance maybe_add Node maybe_add op = call_function maybe_add target torch ops aten add Tensor torch ops aten add_ Tensor continue add_node = maybe_add len add_node users add can t fused ReLU result add being used where graph continue partition = relu_node add_node _is_annotated partition continue filter_fn any filter_fn n n partition continue input_act_qspec = get_input_act_qspec quantization_config output_act_qspec = get_output_act_qspec quantization_config input_qspec_map = input_act = add_node args isinstance input_act Node _is_input_large_scalar input_act gm continue _is_input_non_float_tensor input_act continue partition append input_act input_qspec_map input_act = input_act_qspec input_act = add_node args isinstance input_act Node _is_input_large_scalar input_act gm continue _is_input_non_float_tensor input_act continue partition append input_act input_qspec_map input_act = input_act_qspec add_node meta quantization_annotation = QuantizationAnnotation input_qspec_map=input_qspec_map _annotated=True relu_node meta quantization_annotation = QuantizationAnnotation output_qspec=output_act_qspec _annotated=True annotated_partitions append partition annotated_partitions register_annotator add _annotate_add gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional Callable Node bool = None - Optional list list Node annotated_partitions = node gm graph nodes node op = call_function node target torch ops aten add Tensor torch ops aten add_ Tensor continue add_node = node partition = add_node _is_annotated partition continue filter_fn any filter_fn n n partition continue input_act_qspec = get_input_act_qspec quantization_config output_act_qspec = get_output_act_qspec quantization_config input_qspec_map = input_act = add_node args isinstance input_act Node _is_input_large_scalar input_act gm continue _is_input_non_float_tensor input_act continue input_qspec_map input_act = input_act_qspec partition append input_act input_act = add_node args isinstance input_act Node _is_input_large_scalar input_act gm continue _is_input_non_float_tensor input_act continue input_qspec_map input_act = input_act_qspec partition append input_act add_node meta quantization_annotation = QuantizationAnnotation input_qspec_map=input_qspec_map output_qspec=output_act_qspec _annotated=True annotated_partitions append partition annotated_partitions register_annotator mul_relu _annotate_mul_relu gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional Callable Node bool = None - Optional list list Node annotated_partitions = node gm graph nodes node op = call_function node target torch ops aten relu default torch ops aten relu_ default continue relu_node = node maybe_mul = node args isinstance maybe_mul Node maybe_mul op = call_function maybe_mul target torch ops aten mul Tensor torch ops aten mul_ Tensor continue mul_node = maybe_mul len mul_node users mul can t fused ReLU result mul being used where graph continue partition = relu_node mul_node _is_annotated partition continue filter_fn any filter_fn n n partition continue input_act_qspec = get_input_act_qspec quantization_config output_act_qspec = get_output_act_qspec quantization_config input_qspec_map = input_act = mul_node args isinstance input_act Node _is_input_large_scalar input_act gm continue _is_input_non_float_tensor input_act continue partition append input_act input_qspec_map input_act = input_act_qspec input_act = mul_node args isinstance input_act Node _is_input_large_scalar input_act gm continue _is_input_non_float_tensor input_act continue partition append input_act input_qspec_map input_act = input_act_qspec mul_node meta quantization_annotation = QuantizationAnnotation input_qspec_map=input_qspec_map _annotated=True relu_node meta quantization_annotation = QuantizationAnnotation output_qspec=output_act_qspec _annotated=True annotated_partitions append partition annotated_partitions register_annotator mul _annotate_mul gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional Callable Node bool = None - Optional list list Node annotated_partitions = node gm graph nodes node op = call_function node target torch ops aten mul Tensor torch ops aten mul_ Tensor continue mul_node = node partition = mul_node _is_annotated partition continue filter_fn any filter_fn n n partition continue input_act_qspec = get_input_act_qspec quantization_config output_act_qspec = get_output_act_qspec quantization_config input_qspec_map = input_act = mul_node args isinstance input_act Node _is_input_large_scalar input_act gm continue _is_input_non_float_tensor input_act continue input_qspec_map input_act = input_act_qspec partition append input_act input_act = mul_node args isinstance input_act Node _is_input_large_scalar input_act gm continue _is_input_non_float_tensor input_act continue input_qspec_map input_act = input_act_qspec partition append input_act mul_node meta quantization_annotation = QuantizationAnnotation input_qspec_map=input_qspec_map output_qspec=output_act_qspec _annotated=True annotated_partitions append partition annotated_partitions TODO remove Optional type fix annotated_partitions logic register_annotator cat _annotate_cat gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional Callable Node bool = None - Optional list list Node cat_partitions = get_source_partitions gm graph torch cat filter_fn cat_partitions = list itertools chain from_iterable cat_partitions values annotated_partitions = cat_partition cat_partitions cat_node = cat_partition output_nodes _is_annotated cat_node continue cat_node target = torch ops aten cat default TODO change AnnotationException raise Exception noqa TRY f Expected cat node torch ops aten cat default found cat_node target please check you calling correct capture API annotated_partitions append cat_partition nodes input_act_qspec = get_input_act_qspec quantization_config inputs = cat_node args input_qspec_map = input_act = inputs type ignore index isinstance input_act Node input_qspec_map input_act = input_act_qspec shared_with_input _qspec = SharedQuantizationSpec input_act cat_node type ignore arg-type input_act inputs type ignore index union-attr input_act input_qspec_map input_qspec_map input_act = shared_with_input _qspec type ignore index output_act_qspec = shared_with_input _qspec cat_node meta quantization_annotation = QuantizationAnnotation input_qspec_map=input_qspec_map output_qspec=output_act_qspec _annotated=True annotated_partitions _is_share_obs_or_fq_op op Callable - bool op torch ops aten relu default torch ops aten hardtanh default torch ops aten hardtanh_ default torch ops aten max_pool d default torch ops aten mean default torch ops aten mean dim torch ops aten permute default torch ops aten permute_copy default torch ops aten squeeze dim torch ops aten squeeze_copy dim TODO remove torch ops aten adaptive_avg_pool d default torch ops aten view_copy default torch ops aten view default torch ops aten slice_copy Tensor torch ops aten flatten using_ints propagate_annotation model torch fx GraphModule - None n model graph nodes n op = call_function _is_share_obs_or_fq_op n target continue prev_node = n args isinstance prev_node Node continue quantization_annotation = prev_node meta get quantization_annotation None quantization_annotation continue output_qspec = quantization_annotation output_qspec output_qspec continue make sure current node annotated quantization_annotation n meta n meta quantization_annotation _annotated continue shared_qspec = SharedQuantizationSpec prev_node propagate previous output_qspec current node n meta quantization_annotation = QuantizationAnnotation input_qspec_map= prev_node shared_qspec output_qspec=shared_qspec _annotated=True TODO make list ops customizable _convert_scalars_to_attrs model torch fx GraphModule - torch fx GraphModule n model graph nodes n op = call_function n target torch ops aten add Tensor torch ops aten mul Tensor continue args = list n args new_args = i range len args isinstance args i torch fx Node new_args append args i continue prefix = _tensor_constant_ get_new_attr_name = get_new_attr_name_with_prefix prefix tensor_constant_name = get_new_attr_name model float_tensor = torch tensor float args i model register_buffer tensor_constant_name float_tensor fake_mode = n meta val fake_mode model graph inserting_before n get_attr_node = model graph create_node get_attr tensor_constant_name get_attr_node meta val = fake_mode from_tensor float_tensor static_shapes=True new_args append get_attr_node n args = tuple new_args model recompile model