copy enum pprint unittest enum Enum Importing these files make modifications op_db we need test_ops noqa F test_vmap noqa F functorch_additional_op_db additional_op_db torch torch _functorch top_operators_github_usage top_ops torch testing _internal common_device_type toleranceOverride torch testing _internal common_methods_invocations op_db all_overridable = list torch overrides get_testing_overrides keys public_docs = torch nn functional torch nn functional docs source nn functional rst torch fft torch fft docs source fft rst torch special torch special docs source special rst torch linalg torch linalg docs source linalg rst torch torch docs source torch rst torch Tensor torch Tensor docs source tensors rst torch abs Tensor abs Tensor abs_ all considered different get_public_overridable_apis pytorch_root= raid rzou pt debug-cpu results = all_overridable_apis = set torch overrides get_testing_overrides keys module module_name src public_docs open f pytorch_root src f lines = f readlines APIs either begin spaces autofunction api_lines = line strip line lines line startswith api_lines = line strip len autofunction line lines line startswith autofunction lines = api_lines + api_lines lines = line removeprefix Tensor line lines lines = line line lines hasattr module line line lines api = getattr module line api all_overridable_apis results f module_name line = api results denylist = torch Tensor data_ptr torch Tensor dim torch Tensor element_size torch Tensor backward torch Tensor as_strided torch Tensor register_hook torch Tensor record_stream torch Tensor qscheme torch Tensor ndimension torch Tensor smm torch Tensor sspaddmm torch Tensor retain_grad torch Tensor sparse_mask torch Tensor sparse_dim torch Tensor dense_dim torch Tensor values torch Tensor indices torch Tensor numel torch Tensor size torch Tensor nelement torch Tensor q_scale torch Tensor q_zero_point torch Tensor q_per_channel_scales torch Tensor q_per_channel_zero_points torch Tensor q_per_channel_axis torch Tensor int_repr torch Tensor to_sparse torch Tensor is_inference torch Tensor storage torch Tensor storage_type get_method_only_ops_we_care_about apis = get_public_overridable_apis result = key apis keys key startswith torch Tensor continue key denylist continue api = key split filter out in-place api endswith _ continue f torch api apis keys result append api result Deduplicates torch abs Tensor abs get_public_overridable_ops results = get_public_overridable_apis cpy = copy deepcopy results key cpy keys key startswith torch Tensor continue api = key split f torch api results keys del results key results get_public_overridable_outplace_ops results = get_public_overridable_ops cpy = copy deepcopy results key cpy keys NB there no dunder methods bcs we don t document those key endswith _ del results key results get_public_overridable_outplace_we_care_about results = get_public_overridable_outplace_ops cpy = copy deepcopy results key cpy keys quantization quant key q_ key del results key is_cpu etc It doesn t make sense have OpInfos these is_ key del results key key denylist key results del results key results e g nn functional softmax get_op dotted_name names = dotted_name split mod = torch name names hasattr mod name None mod = getattr mod name mod Maps function - OpInfo get_ops_covered_by_opinfos ops = safe_append dct key val key dct dct key append val dct key = val opinfo op_db func_op = get_op opinfo name func_op safe_append ops func_op opinfo opinfo method_variant safe_append ops opinfo method_variant opinfo opinfo inplace_variant safe_append ops opinfo inplace_variant opinfo alias opinfo aliases safe_append ops alias op opinfo ops factory_fns = tensor zeros ones randn arange rand empty randperm linspace logspace hann_window full eye blackman_window bartlett_window randint range get_top_ops torch_threshold nn_fn_threshold with_counts=False denylist = set These either real operators factory functions trivially work not-documented ops load no_grad save from_numpy manual_seed set_grad_enabled set_default_tensor_type set_num_threads set_printoptions numel set_default_dtype sparse_coo_tensor set_rng_state get_rng_state get_default_dtype initial_seed get_num_threads quantize_per_tensor hann_window is_tensor as_tensor equal enable_grad seed is_storage is_floating_point nn functional torch set_flush_denormal set_num_interop_threads dequantize get_num_interop_threads nn functional math nn functional threshold_ nn functional selu_ nn functional elu_ nn functional rrelu_ nn functional leaky_relu_ nn functional hardtanh_ nn functional has_torch_function nn functional has_torch_function_unary nn functional has_torch_function_variadic nn functional handle_torch_function nn functional adaptive_max_pool d_with_indices nn functional adaptive_max_pool d_with_indices nn functional adaptive_max_pool d_with_indices nn functional fractional_max_pool d_with_indices nn functional fractional_max_pool d_with_indices is_complex grad quantize_per_channel nn functional max_pool d_with_indices nn functional max_pool d_with_indices nn functional max_pool d_with_indices nn functional celu_ nn functional grad nn functional relu_ nn functional boolean_dispatch nn functional assert_int_or_pair fft namespace torch_ops = top_ops top_torch nn_fn_ops = top_ops get_nn_functional_top_list torch_ops = op op torch_ops op denylist nn_fn_ops = op op nn_fn_ops op denylist ops = torch_ops torch_threshold + nn_fn_ops nn_fn_threshold Now sort priority ops sort reverse=True key=lambda op op with_counts ops = op op ops ops get_ops_percentage torch_threshold nn_fn_threshold data = top_ops top_torch + top_ops get_nn_functional_top_list get_num_usages opname Ignore heavily inflated opname == t result = op op data op == opname assert len result == result get all operators denylist all_ops = get_top_ops total_op_usages = sum get_num_usages op op all_ops get subset all operators subset_ops = get_top_ops torch_threshold nn_fn_threshold subset_op_usages = sum get_num_usages op op subset_ops subset_op_usages total_op_usages get_top_ops_not_covered_by_opinfo torch_threshold= nn_fn_threshold= ops = get_top_ops torch_threshold nn_fn_threshold ops_with_opinfo = op op_db ops_with_opinfo append op name ops_with_opinfo extend op name op op aliases ops_with_opinfo = set ops_with_opinfo result = op op ops op ops_with_opinfo result = op op result op denylist result = op op result op factory_fns result get_covered_ops ops_list invert=False ops_covered_by_opinfo = get_ops_covered_by_opinfos overridable_outplace_ops = ops_list results = key op overridable_outplace_ops items cond = op ops_covered_by_opinfo invert cond = cond cond results key = op results Status Enum Correct = Fast = tests = test_vmap_exhaustive test_op_has_batch_rule test_vjp test_vmapvjp test_vmapvjp_has_batch_rule test_jvp test_vmapjvp is_decorateinfo_skip_or_xfail decorateinfo assert len decorateinfo decorators == actual_decorator = decorateinfo decorators isinstance actual_decorator toleranceOverride False actual_decorator == unittest expectedFailure True Assume rest skips True get_all_tested_ops overridable_outplace_we_care_about = get_public_overridable_outplace_we_care_about op_to_opinfo = get_ops_covered_by_opinfos result = set op get_covered_ops overridable_outplace_we_care_about values opinfos = op_to_opinfo op result update opinfo name opinfo opinfos result get_skipped_or_xfailed_ops_for test_name overridable_outplace_we_care_about = get_public_overridable_outplace_we_care_about op_to_opinfo = get_ops_covered_by_opinfos result = set op get_covered_ops overridable_outplace_we_care_about values opinfos = op_to_opinfo op opinfo opinfos decorator opinfo decorators hasattr decorator test_name continue decorator test_name = test_name continue is_decorateinfo_skip_or_xfail decorator result add opinfo name result get_statuses for_subset=None invert=False overridable_outplace_we_care_about = get_public_overridable_outplace_we_care_about for_subset None overridable_outplace_we_care_about = k v k v overridable_outplace_we_care_about items Removes torch k for_subset op_to_opinfo = get_ops_covered_by_opinfos result = _ = get_covered_ops overridable_outplace_we_care_about get_covered_tests op opinfos = op_to_opinfo op result = copy deepcopy tests opinfo opinfos decorator opinfo decorators hasattr decorator test_name continue decorator test_name tests decorator test_name result result remove decorator test_name result name op get_covered_ops overridable_outplace_we_care_about items successful_tests = get_covered_tests op failed_tests = tests - successful_tests result name = failed_tests invert successful_tests result transpose_statuses for_subset=None invert=False statuses = get_statuses for_subset invert=invert result = test tests result test = set op supported statuses items test supported result test add op result overridable_apis = get_public_overridable_apis overridable_ops = get_public_overridable_ops overridable_outplace_ops = get_public_overridable_outplace_ops overridable_outplace_we_care_about = get_public_overridable_outplace_we_care_about tested_overridable_outplace_ops = get_covered_ops overridable_outplace_we_care_about untested_overridable_outplace_ops = get_covered_ops overridable_outplace_we_care_about invert=True print List OpInfos we need key untested_overridable_outplace_ops keys print key print - print print f Overridable public APIs len overridable_apis print f Overridable public ops len overridable_ops print f Overridable public outplace ops len overridable_outplace_ops print f Overridable public outplace ops we care about len overridable_outplace_we_care_about print f OpInfo-tested overridable public outplace ops len tested_overridable_outplace_ops remove_torch name assert name == torch name get_list_of_all_tests all_tests = list tested_overridable_outplace_ops keys remove_torch test test all_tests mytest = test_vmap_exhaustive test_op_has_batch_rule test_vjp test_vmapvjp test_vmapvjp_has_batch_rule print all_tests = get_list_of_all_tests test mytest result = get_skipped_or_xfailed_ops_for test diff = len all_tests - result print f test diff get_jvp_coverage subset=None - number support autograd - number support forward_ad pytorch core - number support functorch jvp op_to_opinfo = get_ops_covered_by_opinfos ops_dct = tested_overridable_outplace_ops subset None ops_dct = name op name op ops_dct items remove_torch name subset supports_autograd_ops_dct = name op_to_opinfo fn name fn ops_dct items op_to_opinfo fn supports_autograd supports_forwardad_ops_dct = name op_to_opinfo fn name fn ops_dct items op_to_opinfo fn supports_forward_ad ops = remove_torch test test list ops_dct keys supports_autograd = remove_torch test test list supports_autograd_ops_dct keys supports_forward_ad = remove_torch test test list supports_forwardad_ops_dct keys assert supports_forward_ad issubset supports_autograd assert supports_autograd issubset ops failed_ops = get_skipped_or_xfailed_ops_for test_jvp coverage = len supports_forward_ad - failed_ops no_forward_ad = len supports_autograd - len supports_forward_ad print f test_jvp coverage no_forward_ad len ops get_jvp_coverage get_jvp_coverage get_top_ops op get_top_ops print op print result = get_skipped_or_xfailed_ops_for test_vmap_exhaustive result = get_skipped_or_xfailed_ops_for test_op_has_batch_rule result = get_skipped_or_xfailed_ops_for test_vjp result = get_skipped_or_xfailed_ops_for test_vmapvjp result = get_skipped_or_xfailed_ops_for test_vmapvjp_has_batch_rule pdb pdb set_trace statuses = transpose_statuses test tests print f test coverage len statuses test method_only_ops = get_method_only_ops_we_care_about op method_only_ops print f op top_ops_not_covered_by_opinfo = get_top_ops_not_covered_by_opinfo print = op top_ops_not_covered_by_opinfo print f op top_ops usage_count op print top ops covered opinfo top_ops_not_covered_by_opinfo = get_top_ops_not_covered_by_opinfo op top_ops_not_covered_by_opinfo print f op top_ops usage_count op print top ops covered opinfo top_ops_not_covered_by_opinfo = get_top_ops_not_covered_by_opinfo op top_ops_not_covered_by_opinfo print f op top_ops usage_count op print top ops covered opinfo top_ops_not_covered_by_opinfo = get_top_ops_not_covered_by_opinfo op top_ops_not_covered_by_opinfo print f op top_ops usage_count op remove_from_set parent to_remove to_remove_elt to_remove to_remove_elt parent parent remove to_remove_elt print_coverage_info th= nn= print = print f top th nn coverage statuses = transpose_statuses get_top_ops th nn invert=True top_ops_not_covered_by_opinfo = get_top_ops_not_covered_by_opinfo th nn testing problems exemptions = torch nn functional dropout randomness Allowed exemptions vmap_exemptions = torch randn_like randomness torch rand_like randomness torch allclose number output torch unique dynamic torch nonzero dynamic torch masked_select dynamic torch prod dynamic backward torch norm norm nuc commonly used we support other cases torch svd There isn t bug just nondeterministic so we can t test torch nn functional embedding We support everything except sparse option remove_from_set statuses test_vmap_exhaustive vmap_exemptions remove_from_set statuses test_vmapvjp vmap_exemptions remove_from_set statuses test_vmapvjp_has_batch_rule vmap_exemptions remove_from_set statuses test_op_has_batch_rule vmap_exemptions remove_from_set statuses test_vmapjvp vmap_exemptions test tests remove_from_set statuses test exemptions print f total ops set th + nn print f tested OpInfo th + nn - len top_ops_not_covered_by_opinfo test tests test test_jvp test_vmapjvp continue print f test failing coverage len statuses test We don t care about these yet del statuses test_jvp del statuses test_vmapjvp pprint pprint statuses get_name_to_opinfo_map dct = op op_db + additional_op_db add name op name dct dct name = dct name append op add op name op alias op aliases add alias name op dct NAME_TO_OPINFO = get_name_to_opinfo_map Support enum Enum NO = YES = UNKNOWN = FACTORY_FNS = tensor zeros ones randn arange rand empty range full randperm eye randint linspace logspace VJP_EXEMPTIONS = nn functional dropout actually problem randomness testing artifact nn functional dropout d actually problem randomness testing artifact nn functional rrelu actually problem randomness testing artifact bernoulli actually problem randomness testing artifact normal actually problem randomness testing artifact VMAP_EXEMPTIONS = randn_like randomness rand_like randomness allclose number output unique dynamic nonzero dynamic masked_select dynamic prod dynamic backward norm norm nuc commonly used we support other cases svd There isn t bug just nondeterministic so we can t test nn functional embedding We support everything except sparse option nn functional dropout randomness nn functional dropout d randomness bernoulli randomness multinomial randomness normal randomness JVP_EXEMPTIONS = nn functional dropout actually problem randomness testing artifact nn functional dropout d actually problem randomness testing artifact nn functional rrelu actually problem randomness testing artifact normal actually problem randomness testing artifact bernoulli actually problem randomness testing artifact Operator __init__ name name = name opinfos = NAME_TO_OPINFO get name None assert opinfos None len opinfos has_opinfo opinfos None __repr__ f Operator name __hash__ hash name no_opinfos_skip_test test_name Returns NO any opinfos have skip xfail test has_opinfo Support UNKNOWN opinfo opinfos decorator opinfo decorators hasattr decorator test_name continue decorator test_name = test_name continue is_decorateinfo_skip_or_xfail decorator Support NO Support YES any_opinfo_attr attr has_opinfo raise RuntimeError any getattr opinfo attr opinfo opinfos all_opinfo_attr attr has_opinfo raise RuntimeError all getattr opinfo attr opinfo opinfos supports_vjp name FACTORY_FNS Support YES name VJP_EXEMPTIONS Support YES no_opinfos_skip_test test_vjp supports_vmap name FACTORY_FNS Support YES name VMAP_EXEMPTIONS Support YES no_opinfos_skip_test test_vmap_exhaustive supports_fast_vmap name FACTORY_FNS Support YES name VMAP_EXEMPTIONS Support YES no_opinfos_skip_test test_op_has_batch_rule supports_vmapvjp name FACTORY_FNS Support YES name VMAP_EXEMPTIONS Support YES no_opinfos_skip_test test_vmapvjp supports_fast_vmapvjp name FACTORY_FNS Support YES name VMAP_EXEMPTIONS Support YES no_opinfos_skip_test test_vmapvjp_has_batch_rule supports_jvp name FACTORY_FNS Support YES name JVP_EXEMPTIONS Support YES has_opinfo Support UNKNOWN any_opinfo_attr supports_autograd all_opinfo_attr supports_forward_ad Support NO no_opinfos_skip_test test_jvp supports_jvpvjp name FACTORY_FNS Support YES exemptions = we have support see OpInfo testing artifact nn functional dropout d nn functional dropout exception we dont even support double backward nn functional hardswish bernoulli isn t differentiable normal differentiable name exemptions Support YES no_opinfos_skip_test test_jvpvjp _supports_vmapjvp_base test name FACTORY_FNS Support YES VMAPJVP_EXEMPTIONS = prod dynamic backward nn functional batch_norm testing problem normal actually problem randomness testing artifact bernoulli actually problem randomness testing artifact nn functional dropout d actually problem randomness testing artifact nn functional dropout actually problem randomness testing artifact Not problem It s just max_norm testing mutates inputs we have our own functorch variant OpInfo without max_norm nn functional embedding name VMAPJVP_EXEMPTIONS Support YES has_opinfo Support UNKNOWN any_opinfo_attr supports_autograd all_opinfo_attr supports_forward_ad Support NO no_opinfos_skip_test test supports_vmapjvp _supports_vmapjvp_base test_vmapjvpall supports_fast_vmapjvp _supports_vmapjvp_base test_vmapjvpall_has_batch_rule OperatorSet __init__ operators data = set operators classmethod from_names cls names OperatorSet Operator name name names classmethod from_top_ops_threshold cls torch_threshold nn_fn_threshold names = get_top_ops torch_threshold nn_fn_threshold cls from_names names classmethod from_top cls cls from_top_ops_threshold classmethod from_top cls cls from_top_ops_threshold classmethod all cls dct = get_public_overridable_outplace_we_care_about names = dct keys names_sanitized = n names torch_tensor = torch Tensor torch_dot = torch n startswith torch_tensor names_sanitized append n len torch_tensor n startswith torch_dot names_sanitized append n len torch_dot raise AssertionError cls from_names names_sanitized query operator_method filter= Support NO Support YES Support UNKNOWN result = key filter result key = set op data support_status = operator_method op support_status filter result support_status add op result summary checks = supports_vjp supports_vmap supports_fast_vmap supports_vmapvjp supports_fast_vmapvjp supports_jvp supports_vmapjvp supports_fast_vmapjvp supports_jvpvjp result = test yes no unknown check checks accessor = getattr Operator check all_results = query accessor yes_amt = len all_results Support YES no_amt = len all_results Support NO unknown_amt = len all_results Support UNKNOWN result append f check yes_amt no_amt unknown_amt \n join result opset = OperatorSet all has_no_opinfo = opset query Operator has_opinfo False print = + Summary + = print f usages github get_ops_percentage print opset summary sanity checks result = opset query Operator supports_vjp Support NO Support UNKNOWN pprint pprint result print = + Top Summary + = print f usages github get_ops_percentage opset = OperatorSet from_top_ops_threshold result = opset query Operator supports_vmapjvp Support NO Support UNKNOWN pprint pprint result result = opset query Operator supports_jvp Support NO Support UNKNOWN pprint pprint result kresult = opset query Operator supports_jvpvjp Support NO Support UNKNOWN kpprint pprint result result = opset query Operator supports_vmapjvp Support NO Support UNKNOWN pprint pprint result result = opset query Operator supports_fast_vmapjvp Support NO Support UNKNOWN pprint pprint result pprint pprint result print opset summary print = + Top Summary + = print f usages github get_ops_percentage opset = OperatorSet from_top result = opset query Operator supports_vmap Support NO Support UNKNOWN pprint pprint result result = opset query Operator supports_jvpvjp Support NO Support UNKNOWN pprint pprint result print supports_vjp result = opset query Operator supports_vjp Support NO Support UNKNOWN pprint pprint result print supports_jvp result = opset query Operator supports_jvp Support NO Support UNKNOWN pprint pprint result print supports_vmapjvp result = opset query Operator supports_vmapjvp Support NO Support UNKNOWN pprint pprint result print supports_jvpvjp result = opset query Operator supports_jvpvjp Support NO Support UNKNOWN pprint pprint result result = opset query Operator supports_fast_vmapjvp Support NO Support UNKNOWN pprint pprint result pprint pprint result print opset summary print = + Top Summary + = opset = OperatorSet from_top result = opset query Operator supports_jvpvjp Support NO Support UNKNOWN pprint pprint result print opset summary Print list everything order all_ops = get_top_ops with_counts=True op count all_ops print f op count