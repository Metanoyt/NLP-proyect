mypy allow-untyped-defs Unlike rest PyTorch file must python compliant This script outputs relevant system environment info Run ` python collect_env py ` ` python -m torch utils collect_env ` datetime json locale os re subprocess sys collections namedtuple typing cast _cast try torch TORCH_AVAILABLE = True except ImportError NameError AttributeError OSError TORCH_AVAILABLE = False System Environment Information SystemEnv = namedtuple SystemEnv torch_version is_debug_build cuda_compiled_version gcc_version clang_version cmake_version os libc_version python_version python_platform is_cuda_available cuda_runtime_version cuda_module_loading nvidia_driver_version nvidia_gpu_models cudnn_version is_xpu_available pip_version pip pip pip_packages conda_packages hip_compiled_version hip_runtime_version miopen_runtime_version caching_allocator_config is_xnnpack_available cpu_info COMMON_PATTERNS = torch numpy triton optree NVIDIA_PATTERNS = cuda-cudart cuda-cupti cuda-libraries cuda-opencl cuda-nvrtc cuda-runtime cublas cudnn cufft curand cusolver cusparse nccl nvjitlink nvtx ONEAPI_PATTERNS = dpcpp-cpp-rt intel-cmplr-lib-rt intel-cmplr-lib-ur intel-cmplr-lic-rt intel-opencl-rt intel-sycl-rt mkl onemkl-sycl-blas onemkl-sycl-dft onemkl-sycl-lapack onemkl-sycl-rng onemkl-sycl-sparse intel-openmp tbb impi-rt impi-devel oneccl oneccl-devel intel-pti umf tcmlib CONDA_PATTERNS = cudatoolkit soumith mkl magma PIP_PATTERNS = mypy flake onnx run command Return return-code stdout stderr shell = type command str p = subprocess Popen command stdout=subprocess PIPE stderr=subprocess PIPE shell=shell raw_output raw_err = p communicate rc = p returncode get_platform == win enc = oem enc = locale getpreferredencoding output = raw_output decode enc err = raw_err decode enc rc output strip err strip run_and_read_all run_lambda command Run command using run_lambda reads returns entire output rc rc out _ = run_lambda command rc = None out run_and_parse_first_match run_lambda command regex Run command using run_lambda returns first regex match exists rc out _ = run_lambda command rc = None match = re search regex out match None None match group run_and_return_first_line run_lambda command Run command using run_lambda returns first line output empty rc out _ = run_lambda command rc = None out split \n get_conda_packages run_lambda patterns=None patterns None patterns = CONDA_PATTERNS + COMMON_PATTERNS + NVIDIA_PATTERNS + ONEAPI_PATTERNS conda = os environ get CONDA_EXE conda out = run_and_read_all run_lambda list format conda out None out \n join line line out splitlines line startswith any name line name patterns get_gcc_version run_lambda run_and_parse_first_match run_lambda gcc -- version r gcc get_clang_version run_lambda run_and_parse_first_match run_lambda clang -- version r clang version get_cmake_version run_lambda run_and_parse_first_match run_lambda cmake -- version r cmake get_nvidia_driver_version run_lambda get_platform == darwin cmd = kextstat &#124; grep -i cuda run_and_parse_first_match run_lambda cmd r com nvidia CUDA smi = get_nvidia_smi run_and_parse_first_match run_lambda smi r Driver Version get_gpu_info run_lambda get_platform == darwin TORCH_AVAILABLE hasattr torch version hip torch version hip None TORCH_AVAILABLE torch cuda is_available torch version hip None prop = torch cuda get_device_properties hasattr prop gcnArchName gcnArch = format prop gcnArchName gcnArch = NoGCNArchNameOnOldPyTorch gcnArch = torch cuda get_device_name None + gcnArch None smi = get_nvidia_smi uuid_regex = re compile r \ UUID + \ rc out _ = run_lambda smi + -L rc = None Anonymize GPUs removing their UUID re sub uuid_regex out get_running_cuda_version run_lambda run_and_parse_first_match run_lambda nvcc -- version r release + V get_cudnn_version run_lambda Return list libcudnn so s hard tell which one being used get_platform == win system_root = os environ get SYSTEMROOT C \\Windows cuda_path = os environ get CUDA_PATH CUDA_PATH where_cmd = os path join system_root System where cudnn_cmd = R \\bin cudnn dll format where_cmd cuda_path get_platform == darwin CUDA libraries drivers can found usr local cuda See https docs nvidia com cuda archive cuda-installation-guide-mac-os-x index html#installation https docs nvidia com deeplearning cudnn installation latest Use CUDNN_LIBRARY when cudnn library installed elsewhere cudnn_cmd = ls usr local cuda lib libcudnn cudnn_cmd = ldconfig -p &#124; grep libcudnn &#124; rev &#124; cut -d -f &#124; rev rc out _ = run_lambda cudnn_cmd find will there permission errors found len out == rc = rc = l = os environ get CUDNN_LIBRARY l None os path isfile l os path realpath l None files_set = set fn out split \n fn = os path realpath fn eliminate symbolic links os path isfile fn files_set add fn files_set None Alphabetize result because order non-deterministic otherwise files = sorted files_set len files == files result = \n join files Probably one following \n format result get_nvidia_smi Note nvidia-smi currently available only Windows Linux smi = nvidia-smi get_platform == win system_root = os environ get SYSTEMROOT C \\Windows program_files_root = os environ get PROGRAMFILES C \\Program Files legacy_path = os path join program_files_root NVIDIA Corporation NVSMI smi new_path = os path join system_root System smi smis = new_path legacy_path candidate_smi smis os path exists candidate_smi smi = format candidate_smi break smi _detect_linux_pkg_manager get_platform = linux N A mgr_name dpkg dnf yum zypper rc _ _ = run f which mgr_name rc == mgr_name N A get_linux_pkg_version run_lambda pkg_name pkg_mgr = _detect_linux_pkg_manager pkg_mgr == N A N A grep_version = dpkg field_index command dpkg -l &#124; grep dnf field_index command dnf list &#124; grep yum field_index command yum list &#124; grep zypper field_index command zypper info &#124; grep Version field_index int = int _cast int grep_version pkg_mgr field_index cmd str = str grep_version pkg_mgr command cmd = cmd format pkg_name ret = run_and_read_all run_lambda cmd ret None ret == N A lst = re sub + ret split len lst = field_index N A lst field_index get_intel_gpu_driver_version run_lambda lst = platform = get_platform platform == linux pkgs = type ignore var-annotated dpkg intel-opencl-icd libze level-zero dnf intel-opencl level-zero yum intel-opencl level-zero zypper intel-opencl level-zero get _detect_linux_pkg_manager pkg pkgs ver = get_linux_pkg_version run_lambda pkg ver = N A lst append f pkg \t ver platform win cygwin txt = run_and_read_all run_lambda powershell exe gwmi -Class Win _PnpSignedDriver &#124; where $ _ DeviceClass -eq \\ DISPLAY\\ \ -and $ _ Manufacturer -match \\ Intel\\ &#124; Select-Object -Property DeviceName DriverVersion DriverDate\ &#124; ConvertTo-Json try obj = json loads txt type obj list o obj lst append f o DeviceName o DriverVersion o DriverDate lst append f obj DriverVersion obj DriverDate except ValueError e lst append txt lst append str e \n join lst get_intel_gpu_onboard run_lambda lst list str = platform = get_platform platform == linux txt = run_and_read_all run_lambda xpu-smi discovery -j txt try obj = json loads txt device_list = obj get device_list isinstance device_list list device_list lst extend f device device_name device device_list lst append N A except ValueError TypeError e lst append txt lst append str e lst append N A platform win cygwin txt = run_and_read_all run_lambda powershell exe gwmi -Class Win _PnpSignedDriver &#124; where $ _ DeviceClass -eq \\ DISPLAY\\ \ -and $ _ Manufacturer -match \\ Intel\\ &#124; Select-Object -Property DeviceName &#124; ConvertTo-Json txt try obj = json loads txt isinstance obj list obj lst extend f device DeviceName device obj lst append f obj get DeviceName N A except ValueError e lst append txt lst append str e lst append N A \n join lst get_intel_gpu_detected run_lambda TORCH_AVAILABLE hasattr torch xpu N A device_count = torch xpu device_count device_count == N A devices = f i torch xpu get_device_properties i i range device_count \n join devices example outputs CPU infos linux Architecture x _ CPU op-mode s -bit -bit Address sizes bits physical bits virtual Byte Order Little Endian CPU s On-line CPU s list - Vendor ID GenuineIntel Model name Intel R Xeon R Platinum C CPU GHz CPU family Model Thread s per core Core s per socket Socket s Stepping BogoMIPS Flags fpu vme de pse tsc msr pae mce cx apic sep mtrr pge mca cmov pat pse clflush mmx fxsr sse sse ss ht syscall nx pdpe gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse fma cx pcid sse _ sse _ x apic movbe popcnt tsc_deadline_timer aes xsave avx f c rdrand hypervisor lahf_lm abm dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi avx smep bmi erms invpcid avx f avx dq rdseed adx smap avx ifma clflushopt clwb avx cd sha_ni avx bw avx vl xsaveopt xsavec xgetbv xsaves wbnoinvd ida arat avx vbmi pku ospke avx _vbmi gfni vaes vpclmulqdq avx _vnni avx _bitalg tme avx _vpopcntdq rdpid md_clear flush_l d arch_capabilities Virtualization features Hypervisor vendor KVM Virtualization type full Caches sum all L d MiB instances L i MiB instances L MiB instances L MiB instances NUMA NUMA node s NUMA node CPU s - - NUMA node CPU s - - Vulnerabilities Itlb multihit Not affected L tf Not affected Mds Not affected Meltdown Not affected Mmio stale data Vulnerable Clear CPU buffers attempted no microcode SMT Host state unknown Retbleed Not affected Spec store bypass Mitigation Speculative Store Bypass disabled via prctl seccomp Spectre v Mitigation usercopy swapgs barriers __user pointer sanitization Spectre v Mitigation Enhanced IBRS IBPB conditional RSB filling PBRSB-eIBRS SW sequence Srbds Not affected Tsx async abort Not affected win Architecture= CurrentClockSpeed= DeviceID=CPU Family= L CacheSize= L CacheSpeed= Manufacturer=GenuineIntel MaxClockSpeed= Name=Intel R Xeon R Platinum C CPU GHz ProcessorType= Revision= Architecture= CurrentClockSpeed= DeviceID=CPU Family= L CacheSize= L CacheSpeed= Manufacturer=GenuineIntel MaxClockSpeed= Name=Intel R Xeon R Platinum C CPU GHz ProcessorType= Revision= get_cpu_info run_lambda rc out err = get_platform == linux rc out err = run_lambda lscpu get_platform == win rc out err = run_lambda powershell exe gwmi -Class Win _Processor &#124; Select-Object -Property Name Manufacturer Family \ Architecture ProcessorType DeviceID CurrentClockSpeed MaxClockSpeed L CacheSize L CacheSpeed Revision\ &#124; ConvertTo-Json rc == lst = try obj = json loads out type obj list o obj lst append ---------------------- lst extend f k v k v o items lst extend f k v k v obj items except ValueError e lst append out lst append str e out = \n join lst get_platform == darwin rc out err = run_lambda sysctl -n machdep cpu brand_string cpu_info = None rc == cpu_info = out cpu_info = err cpu_info get_platform sys platform startswith linux linux sys platform startswith win win sys platform startswith cygwin cygwin sys platform startswith darwin darwin sys platform get_mac_version run_lambda run_and_parse_first_match run_lambda sw_vers -productVersion r get_windows_version run_lambda ret = run_and_read_all run_lambda powershell exe gwmi -Class Win _OperatingSystem &#124; Select-Object -Property Caption \ OSArchitecture Version &#124; ConvertTo-Json try obj = json loads ret ret = f obj Caption obj Version obj OSArchitecture except ValueError e ret += f \n str e ret get_lsb_version run_lambda run_and_parse_first_match run_lambda lsb_release -a r Description \t check_release_file run_lambda run_and_parse_first_match run_lambda cat etc -release r PRETTY_NAME= get_os run_lambda platform machine platform = get_platform platform win cygwin get_windows_version run_lambda platform == darwin version = get_mac_version run_lambda version None None macOS format version machine platform == linux Ubuntu Debian based desc = get_lsb_version run_lambda desc None format desc machine Try reading etc -release desc = check_release_file run_lambda desc None format desc machine format platform machine Unknown platform platform get_python_platform platform platform platform get_libc_version platform get_platform = linux N A - join platform libc_ver get_pip_packages run_lambda patterns=None Return ` pip list ` output Note will also find conda-installed pytorch numpy packages patterns None patterns = PIP_PATTERNS + COMMON_PATTERNS + NVIDIA_PATTERNS + ONEAPI_PATTERNS pip_version = pip sys version_info major == pip os environ PIP_DISABLE_PIP_VERSION_CHECK = People generally have pip ` pip ` ` pip ` But here invoked ` python -mpip ` out = run_and_read_all run_lambda sys executable -mpip list -- format=freeze out None pip_version out filtered_out = \n join line line out splitlines any name line name patterns pip_version filtered_out get_cachingallocator_config ca_config = os environ get PYTORCH_CUDA_ALLOC_CONF ca_config ca_config = os environ get PYTORCH_HIP_ALLOC_CONF ca_config get_cuda_module_loading_config TORCH_AVAILABLE torch cuda is_available torch cuda init config = os environ get CUDA_MODULE_LOADING config N A is_xnnpack_available TORCH_AVAILABLE torch backends xnnpack str torch backends xnnpack enabled type ignore attr-defined N A get_env_info Collects environment information aid debugging The returned environment information contains details torch version debug build cuda compiled version gcc version clang version cmake version operating system libc version python version python platform CUDA availability CUDA runtime version CUDA module loading config GPU model configuration Nvidia driver version cuDNN version pip version versions relevant pip conda packages HIP runtime version MIOpen runtime version Caching allocator config XNNPACK availability CPU information Returns SystemEnv namedtuple A tuple containing various environment details system information run_lambda = run pip_version pip_list_output = get_pip_packages run_lambda TORCH_AVAILABLE version_str = torch __version__ debug_mode_str = str torch version debug cuda_available_str = str torch cuda is_available cuda_version_str = torch version cuda xpu_available_str = str torch xpu is_available torch xpu is_available xpu_available_str = f xpu_available_str \n + f XPU used build PyTorch torch version xpu \n + f Intel GPU driver version \n get_intel_gpu_driver_version run_lambda \n + f Intel GPU models onboard \n get_intel_gpu_onboard run_lambda \n + f Intel GPU models detected \n get_intel_gpu_detected run_lambda hasattr torch version hip torch version hip None cuda version hip_compiled_version = hip_runtime_version = miopen_runtime_version = N A HIP version get_version_or_na cfg prefix _lst = s rsplit None - s cfg prefix s _lst _lst N A cfg = torch _C _show_config split \n hip_runtime_version = get_version_or_na cfg HIP Runtime miopen_runtime_version = get_version_or_na cfg MIOpen cuda_version_str = N A hip_compiled_version = torch version hip version_str = debug_mode_str = cuda_available_str = cuda_version_str = xpu_available_str = N A type ignore assignment hip_compiled_version = hip_runtime_version = miopen_runtime_version = N A sys_version = sys version replace \n conda_packages = get_conda_packages run_lambda SystemEnv torch_version=version_str is_debug_build=debug_mode_str python_version= -bit runtime format sys_version sys maxsize bit_length + python_platform=get_python_platform is_cuda_available=cuda_available_str cuda_compiled_version=cuda_version_str cuda_runtime_version=get_running_cuda_version run_lambda cuda_module_loading=get_cuda_module_loading_config nvidia_gpu_models=get_gpu_info run_lambda nvidia_driver_version=get_nvidia_driver_version run_lambda cudnn_version=get_cudnn_version run_lambda is_xpu_available=xpu_available_str hip_compiled_version=hip_compiled_version hip_runtime_version=hip_runtime_version miopen_runtime_version=miopen_runtime_version pip_version=pip_version pip_packages=pip_list_output conda_packages=conda_packages os=get_os run_lambda libc_version=get_libc_version gcc_version=get_gcc_version run_lambda clang_version=get_clang_version run_lambda cmake_version=get_cmake_version run_lambda caching_allocator_config=get_cachingallocator_config is_xnnpack_available=is_xnnpack_available cpu_info=get_cpu_info run_lambda env_info_fmt = PyTorch version torch_version Is debug build is_debug_build CUDA used build PyTorch cuda_compiled_version ROCM used build PyTorch hip_compiled_version OS os GCC version gcc_version Clang version clang_version CMake version cmake_version Libc version libc_version Python version python_version Python platform python_platform Is CUDA available is_cuda_available CUDA runtime version cuda_runtime_version CUDA_MODULE_LOADING set cuda_module_loading GPU models configuration nvidia_gpu_models Nvidia driver version nvidia_driver_version cuDNN version cudnn_version Is XPU available is_xpu_available HIP runtime version hip_runtime_version MIOpen runtime version miopen_runtime_version Is XNNPACK available is_xnnpack_available CPU cpu_info Versions relevant libraries pip_packages conda_packages strip pretty_str envinfo replace_nones dct replacement= Could collect key dct keys dct key None continue dct key = replacement dct replace_bools dct true= Yes false= No key dct keys dct key True dct key = true dct key False dct key = false dct prepend text tag= prepend lines = text split \n updated_lines = tag + line line lines \n join updated_lines replace_if_empty text replacement= No relevant packages text None len text == replacement text maybe_start_on_next_line string If ` string ` multiline prepend \n string None len string split \n \n \n format string string mutable_dict = envinfo _asdict If nvidia_gpu_models multiline start next line mutable_dict nvidia_gpu_models = maybe_start_on_next_line envinfo nvidia_gpu_models If machine doesn t have CUDA report some fields No CUDA dynamic_cuda_fields = cuda_runtime_version nvidia_gpu_models nvidia_driver_version all_cuda_fields = dynamic_cuda_fields + cudnn_version all_dynamic_cuda_fields_missing = all mutable_dict field None field dynamic_cuda_fields TORCH_AVAILABLE torch cuda is_available all_dynamic_cuda_fields_missing field all_cuda_fields mutable_dict field = No CUDA envinfo cuda_compiled_version None mutable_dict cuda_compiled_version = None Replace True Yes False No mutable_dict = replace_bools mutable_dict Replace all None objects Could collect mutable_dict = replace_nones mutable_dict If either these replace No relevant packages mutable_dict pip_packages = replace_if_empty mutable_dict pip_packages mutable_dict conda_packages = replace_if_empty mutable_dict conda_packages Tag conda pip packages prefix If they previously None they ll show up ie conda Could collect mutable_dict pip_packages mutable_dict pip_packages = prepend mutable_dict pip_packages format envinfo pip_version mutable_dict conda_packages mutable_dict conda_packages = prepend mutable_dict conda_packages conda mutable_dict cpu_info = envinfo cpu_info env_info_fmt format mutable_dict get_pretty_env_info Returns pretty string environment information This function retrieves environment information calling ` get_env_info ` function then formats information into human-readable string The retrieved environment information listed document ` get_env_info ` This function used ` python collect_env py ` should executed when reporting bug Returns str A pretty string environment information pretty_str get_env_info main print Collecting environment information output = get_pretty_env_info print output TORCH_AVAILABLE hasattr torch utils hasattr torch utils _crash_handler minidump_dir = torch utils _crash_handler DEFAULT_MINIDUMP_DIR sys platform == linux os path exists minidump_dir dumps = os path join minidump_dir dump dump os listdir minidump_dir latest = max dumps key=os path getctime ctime = os path getctime latest creation_time = datetime datetime fromtimestamp ctime strftime Y- m- d H M S msg = \n Detected minidump created format latest creation_time + related your bug please include when you file report print msg file=sys stderr __name__ == __main__ main