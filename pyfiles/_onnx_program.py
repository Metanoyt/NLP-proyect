mypy allow-untyped-defs mypy disable-error-code= attr-defined name-defined __future__ annotations __all__ = ONNXProgram contextlib copy gc logging os tempfile textwrap warnings collections abc Callable Sequence typing Any TYPE_CHECKING torch torch onnx _internal _lazy_import onnx onnxscript_apis onnxscript_ir ir torch onnx _internal exporter _dynamic_shapes _ir_passes torch utils _pytree NOTE DO NOT module torch onnx _internal module global scope because ONNXProgram exposed public API TYPE_CHECKING numpy np onnxruntime ort _LARGE_MODEL_THRESHOLD = MB _NP_UNSUPPORTED_DTYPES_ BIT = frozenset torch float _e m fn torch float _e m fnuz torch float _e m torch float _e m fnuz logger = logging getLogger __name__ _ort_session_initializer model str &#124; bytes - ort InferenceSession Initialize ONNX Runtime inference session specified model onnxruntime ort session_options = ort SessionOptions session_options log_severity_level = Error possible_providers = CUDAExecutionProvider CPUExecutionProvider available_providers = set ort get_available_providers providers = provider provider possible_providers provider available_providers ort InferenceSession model providers=providers sess_options=session_options _count_initializer_size graph ir Graph - int Count total size initializers bytes sum v const_value nbytes v graph initializers values v const_value None contextlib contextmanager _set_graph_outputs graph ir Graph outputs list ir Value Temporarily set outputs graph Args graph The graph set outputs outputs The outputs set original_outputs = list graph outputs graph outputs clear graph outputs extend outputs try yield finally graph outputs clear graph outputs extend original_outputs _create_value_mapping graph ir Graph - dict str ir Value Return dictionary mapping names values graph The mapping does include values subgraphs Args graph The graph extract mapping Returns A dictionary mapping names values values dict str ir Value = values update graph initializers The names values can None which we need exclude input graph inputs input name continue values input name = input node graph value node outputs value name continue values value name = value values _to_numpy_array input torch Tensor &#124; int &#124; float &#124; str &#124; bool - np ndarray isinstance input int float str bool ir tensor input numpy torch onnx _internal exporter _core _core TorchTensor input numpy _from_numpy_array array np ndarray - torch Tensor Convert NumPy array PyTorch tensor ml_dtypes type ignore import-not-found numpy np array dtype == ml_dtypes bfloat torch from_numpy array view np uint view torch bfloat array dtype == ml_dtypes float _e m fn torch from_numpy array view np uint view torch float _e m fn array dtype == ml_dtypes float _e m fnuz torch from_numpy array view np uint view torch float _e m fnuz array dtype == ml_dtypes float _e m torch from_numpy array view np uint view torch float _e m array dtype == ml_dtypes float _e m fnuz torch from_numpy array view np uint view torch float _e m fnuz torch from_numpy array _to_ort_value input torch Tensor &#124; int &#124; float &#124; str &#124; bool - ort OrtValue Convert PyTorch tensor ONNX Runtime OrtValue numpy np onnxruntime ort torch onnx _internal exporter _core isinstance input int float str bool Convert scalar values OrtValue dtype_mapping = int np int float np float pyrefly ignore no-matching-overload dtype = dtype_mapping get type input None ort OrtValue ortvalue_from_numpy np array input dtype=dtype input dtype == torch bfloat input dtype _NP_UNSUPPORTED_DTYPES_ BIT hasattr ort OrtValue ortvalue_from_numpy_with_onnx_type This requires ONNX Runtime newer input dtype == torch bfloat uint_type = torch uint uint_type = torch uint onnx_type = _core torch_dtype_to_onnx_dtype input dtype Make tensor contiguous ensure view works input = input contiguous ort OrtValue ortvalue_from_numpy_with_onnx_type input view uint_type numpy force=True onnx_element_type=onnx_type raise RuntimeError f Failed convert tensor type input dtype OrtValue Please ensure ONNX Runtime built DLPack support latest version TODO Use dlpack when ORT properly supports ort OrtValue ortvalue_from_numpy input numpy force=True _from_ort_value value ort OrtValue - torch Tensor value element_type ir DataType BFLOAT ir DataType FLOAT E M FN ir DataType FLOAT E M FNUZ ir DataType FLOAT E M ir DataType FLOAT E M FNUZ This requires ONNX Runtime newer try torch from_dlpack value _get_c_value except Exception e raise RuntimeError Failed convert OrtValue torch Tensor Please ensure ONNX Runtime built DLPack support latest version e torch from_numpy value numpy ONNXProgram A represent ONNX program callable torch tensors Attributes model The ONNX model ONNX IR model object exported_program The exported program produced ONNX model __init__ model ir Model exported_program torch export ExportedProgram &#124; None Initialize ONNX program specified model exported program Args model The ONNX model exported_program The exported program produced ONNX model Optional model ir Model = model exported_program = exported_program _inference_session ort InferenceSession &#124; None = None _tempdir tempfile TemporaryDirectory &#124; None = None Strategy used capture exported program _capture_strategy str &#124; None = None __repr__ - str f \ ONNXProgram model= textwrap indent str model exported_program= textwrap indent str exported_program __call__ args kwargs - Sequence torch Tensor Run ONNX model same arguments you would provide GraphModule onnxruntime ort flatten_args = _process_args args kwargs _inference_session None initialize_inference_session assert _inference_session None ort_input = k name _to_ort_value v k v zip model graph inputs flatten_args run_options = ort RunOptions run_options log_severity_level = Error logger debug Running inference session s arguments len ort_input pyrefly ignore missing-attribute outputs = _inference_session run_with_ort_values None ort_input run_options=run_options logger debug Inference session run completed tuple _from_ort_value output output outputs call_reference args kwargs - Sequence torch Tensor Run ONNX model using reference backend onnx reference evaluator = onnx reference ReferenceEvaluator model_proto flatten_args = _process_args args kwargs ref_input = k name _to_numpy_array v k v zip model graph inputs flatten_args outputs = evaluator run None ref_input type ignore arg-type assert isinstance outputs Sequence tuple _from_numpy_array output output outputs compute_values value_names Sequence str args= kwargs=None - Sequence torch Tensor Compute values specified names ONNX model This method used compute values specified names ONNX model The values returned dictionary mapping names tensors Args value_names The names values compute Returns A dictionary mapping names tensors kwargs None kwargs = release values = _create_value_mapping model graph name value_names name values raise ValueError f Value name found model Please provide valid value name temporary_outputs = values name name value_names _set_graph_outputs model graph temporary_outputs try result = args kwargs finally release result property model_proto - onnx ModelProto Return ONNX ` ` ModelProto ` ` object ir serde serialize_model model optimize - None Optimize ONNX model This method optimizes ONNX model performing constant folding eliminating redundancies graph The optimization done in-place model = onnxscript_apis optimize model save destination str &#124; os PathLike include_initializers bool = True keep_initializers_as_inputs bool = False external_data bool &#124; None = None Save ONNX model specified destination When ` ` external_data ` ` ` ` True ` ` model larger than GB weights saved external data separate file Initializer model weights serialization behaviors ` ` include_initializers=True ` ` ` ` keep_initializers_as_inputs=False ` ` default The initializers included saved model ` ` include_initializers=True ` ` ` ` keep_initializers_as_inputs=True ` ` The initializers included saved model kept model inputs Choose option you want ability override model weights during inference ` ` include_initializers=False ` ` ` ` keep_initializers_as_inputs=False ` ` The initializers included saved model listed model inputs Choose option you want attach initializers ONNX model separate post-processing step ` ` include_initializers=False ` ` ` ` keep_initializers_as_inputs=True ` ` The initializers included saved model listed model inputs Choose option you want supply initializers during inference want minimize size saved model Args destination The path save ONNX model include_initializers Whether include initializers saved model keep_initializers_as_inputs Whether keep initializers inputs saved model If ` True ` initializers added inputs model which means they can overwritten providing initializers model inputs external_data Whether save weights external data separate file Raises TypeError If ` ` external_data ` ` ` ` True ` ` ` ` destination ` ` file path original_initializers = copy copy model graph initializers original_inputs = copy copy model graph inputs Adjust model based options include_initializers model graph initializers clear keep_initializers_as_inputs model graph inputs extend original_initializers values type ignore arg-type try Save model disk external_data _count_initializer_size model graph _LARGE_MODEL_THRESHOLD onnxscript_apis save_model_with_external_data model destination ir save model destination finally Revert changes model include_initializers model graph initializers update original_initializers keep_initializers_as_inputs model graph inputs clear model graph inputs extend original_inputs apply_weights state_dict dict str torch Tensor - None Apply weights specified state dict ONNX model Use method replace FakeTensors other weights Args state_dict The state dict containing weights apply ONNX model torch onnx _internal exporter _core name tensor state_dict items name model graph initializers model graph initializers name const_value = _core TorchTensor tensor name warnings warn f Weight name found model Skipped applying category=torch onnx errors OnnxExporterWarning stacklevel= initialize_inference_session initializer Callable str &#124; bytes ort InferenceSession = _ort_session_initializer - None Initialize ONNX Runtime inference session Args initializer The function initialize ONNX Runtime inference session specified model By default uses func ` _ort_session_initializer ` function TODO justinchuby Allow different inference options logger debug Initializing inference session byte_size = _count_initializer_size model graph _LARGE_MODEL_THRESHOLD logger debug The model initializers larger than GB s byte_size Save model temporary file too large _tempdir = tempfile TemporaryDirectory ignore_cleanup_errors=True model_path = os path join _tempdir name model onnx save model_path external_data=True model = model_path model = model_proto SerializeToString type ignore assignment _inference_session = initializer model logger debug Inference session initialized release - None Release inference session You may call method release resources used inference session Release inference session first so model file can deleted _inference_session None _inference_session = None gc collect _tempdir None _tempdir cleanup _tempdir = None _rename_dynamic_axes dynamic_shapes dict str Any &#124; tuple Any &#124; list Any - None Rename dynamic axes model according specified dynamic_axes names rename_mapping = _dynamic_shapes create_rename_mapping model graph inputs dynamic_shapes _ir_passes rename_axis model rename_mapping _process_args args kwargs - tuple torch Tensor Process input arguments ONNX model args = _flatten_inputs args kwargs args = _remove_none_from_inputs args args = _convert_complex_to_real_representation args args _flatten_inputs model_args model_kwargs flattened_args _ = _pytree tree_flatten model_args model_kwargs flattened_args _remove_none_from_inputs model_args tuple arg arg model_args arg None _convert_complex_to_real_representation model_args Convert complex dtype tensors real representation tensors ONNX does support complex dtype tensors Thus we convert complex dtype tensors real representation tensors i e float dtype tensors extra dimension representing real imaginary parts complex number tuple torch view_as_real arg resolve_conj isinstance arg torch Tensor arg is_complex arg arg model_args