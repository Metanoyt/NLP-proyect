Owner s module higher order operators flake noqa B contextlib logging unittest torch torch _dynamo torch _functorch torch _inductor torch _inductor decomposition torch _higher_order_ops InvokeQuant torch _inductor config torch _inductor pattern_matcher Arg CallFunction Ignored Match PatternMatcherPass register_graph_pattern torch _inductor utils is_big_gpu run_and_get_code torch testing FileCheck torch testing _internal common_utils run_tests skipIfTorchDynamo skipIfXpu TestCase torch testing _internal inductor_utils GPU_TYPE requires_gpu invoke_quant_tracer = InvokeQuant skipIfTorchDynamo Not torch _dynamo test TestInvokeQuant TestCase backend = test_simple gn x y torch mul x y + y fn x y invoke_quant_tracer gn x y scheme= nf quant_options=invoke_quant_tracer x = torch randn requires_grad=False y = torch randn requires_grad=False ref = gn x y x_clone = x clone detach requires_grad_ False y_clone = y clone detach requires_grad_ False res = torch compile fn backend=self backend x_clone y_clone assertEqual ref res test_construct_inline gn x y torch mul x y + y fn x y InvokeQuant codegen_low_precision=False gn x y scheme= nf x = torch randn requires_grad=False y = torch randn requires_grad=False ref = gn x y x_clone = x clone detach requires_grad_ False y_clone = y clone detach requires_grad_ False res = torch compile fn backend=self backend x_clone y_clone assertEqual ref res test_inline gn x y torch mul x y + y fn x y InvokeQuant gn x y scheme= nf x = torch randn requires_grad=False y = torch randn requires_grad=False ref = gn x y x_clone = x clone detach requires_grad_ False y_clone = y clone detach requires_grad_ False res = torch compile fn backend=self backend x_clone y_clone assertEqual ref res test_multiple torch _logging set_logs post_grad_graphs=True gn x y torch mul x y + y fn x y z o = invoke_quant_tracer gn x y scheme= nf o = invoke_quant_tracer gn y z scheme= nf o + o x = torch randn requires_grad=False y = torch randn requires_grad=False z = torch randn requires_grad=False ref = fn x y z log_context = contextlib nullcontext backend = inductor assertLogs logger= torch _inductor level=logging DEBUG log_context log res = torch compile fn backend=self backend x y z assertEqual ref res backend == inductor logs = \n join r getMessage r log records f = FileCheck f check AFTER POST GRAD f check subgraph check subgraph _ range f check torch ops higher_order invoke_quant check_same nf f run logs TestInvokeQuantEager TestInvokeQuant backend = eager TestInvokeQuantAotEager TestInvokeQuant backend = aot_eager TestInvokeQuantInductor TestInvokeQuant backend = inductor test_pattern_matching counter = test_pass = PatternMatcherPass my_pass g test_pass apply g gn x y torch mul x y + y fn x y z invoke_quant_tracer gn x y scheme= nf z fn_no_match x y z invoke_quant_tracer gn x y z x = torch randn requires_grad=False y = torch randn requires_grad=False z = torch randn requires_grad=False register_graph_pattern CallFunction torch ops aten mm CallFunction torch ops higher_order invoke_quant Ignored Ignored Ignored scheme= nf Arg pass_dict=test_pass quant_matching match Match args kwargs nonlocal counter counter += unittest mock patch torch _inductor config post_grad_custom_pre_pass my_pass torch compile fn x y z assertTrue counter == torch compile fn_no_match x y z assertTrue counter == skipIfXpu msg= MM Triton template fusion XPU work because fusion can speedup unskip until fixed requires_gpu config patch prologue_fusion=True test_prologue is_big_gpu raise unittest SkipTest requires large gpu max-autotune gn x y torch mul x y + y - fn x y z invoke_quant_tracer gn x y scheme= nf quant_options=invoke_quant_tracer z x = torch randn requires_grad=False device=GPU_TYPE dtype=torch float make no-op ensure equivalent numerics y = torch randn requires_grad=False device=GPU_TYPE dtype=torch float fill_ z = torch randn requires_grad=False device=GPU_TYPE dtype=torch float ref = gn x y z x_clone = x clone detach requires_grad_ False y_clone = y clone detach requires_grad_ False z_clone = z clone detach requires_grad_ False torch _dynamo reset torch no_grad config patch max_autotune_gemm_backends= TRITON fn_c = torch compile fn mode= max-autotune-no-cudagraphs res code = run_and_get_code fn_c x_clone y_clone z_clone FileCheck check k_idx range check_not tl float check tl dot run code assertEqual ref res del TestInvokeQuant __name__ == __main__ run_tests