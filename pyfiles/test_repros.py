PYTEST_DONT_REWRITE prevents pytest rewriting assertions which interferes test_rewrite_assert_with_msg test_rewrite_assert_without_msg Owner s module dynamo collections contextlib copy dataclasses functools gc importlib inspect itertools logging os random sys types typing unittest warnings weakref abc ABC collections defaultdict namedtuple collections abc Iterator copy deepcopy enum Enum IntEnum functools wraps typing Any Literal TypedDict unittest mock numpy np torch torch _dynamo test_case torch _dynamo testing torch _dynamo utils torch _functorch config torch distributed dist torch library torch utils _pytree pytree torch nn torch _dynamo backends debugging ExplainWithBackend torch _dynamo debug_utils same_two_models torch _dynamo testing CompileCounter CompileCounterWithBackend EagerAndRecordGraphs rand_strided same skipIfNotPy skipIfPy torch _inductor utils fresh_cache torch nn functional F torch nn attention flex_attention create_block_mask flex_attention torch profiler profile ProfilerActivity torch testing _internal common_cuda PLATFORM_SUPPORTS_FLASH_ATTENTION PLATFORM_SUPPORTS_FP SM OrLater TEST_CUDA torch testing _internal common_device_type E M _MAX_POS e m _type instantiate_device_type_tests torch testing _internal common_utils instantiate_parametrized_tests parametrize serialTest skipIfHpu skipIfWindows TEST_WITH_ROCM torch testing _internal logging_utils LoggingTestCase make_logging_test torch testing _internal two_tensor TwoTensor torch utils _python_dispatch TorchDispatchMode _orig_module_call = torch nn Module __call__ Custom operator only supports CPU Meta lib = torch library Library test_sample DEF noqa TOR lib define foo Tensor - Tensor lib impl foo torch sin CPU requires_cuda = unittest skipUnless torch cuda is_available requires cuda _GLOBAL_CPU_TENSOR = torch randn HAS_MSGSPEC = importlib util find_spec msgspec HAS_MSGSPEC msgspec HAS_OMEGACONG = importlib util find_spec omegaconf HAS_OMEGACONG omegaconf OmegaConf exists val val None maybe fn wraps fn inner x args kwargs exists x x fn x args kwargs inner is_fx_tracing_test - bool Copied hpc trainer codebase torch nn Module __call__ _orig_module_call has_detectron try detectron layers mask_ops _paste_masks_tensor_shape _paste_masks_tensor_shape None except ImportError False _do_paste_mask masks boxes img_h int img_w int skip_empty bool = True detectron mask_ops py device = masks device skip_empty torch jit is_scripting x _int y _int = torch clamp boxes min dim= values floor - min= dtype=torch int x _int = torch clamp boxes max ceil + max=img_w dtype=torch int y _int = torch clamp boxes max ceil + max=img_h dtype=torch int x _int y _int = x _int y _int = img_w img_h x y x y = torch split boxes dim= each Nx N = masks shape img_y = torch arange y _int y _int device=device dtype=torch float + img_x = torch arange x _int x _int device=device dtype=torch float + img_y = img_y - y y - y - img_x = img_x - x x - x - img_x img_y have shapes N w N h gx = img_x None expand N img_y size img_x size gy = img_y None expand N img_y size img_x size grid = torch stack gx gy dim= torch jit is_scripting masks dtype is_floating_point masks = masks float img_masks = F grid_sample masks grid masks dtype align_corners=False skip_empty torch jit is_scripting img_masks slice y _int y _int slice x _int x _int img_masks global_fn x torch sin x cat tensors dim= detectron wrappers py assert isinstance tensors list tuple len tensors == tensors torch cat tensors dim shapes_to_tensor x device=None detectron wrappers py torch jit is_scripting torch as_tensor x device=device torch jit is_tracing assert all isinstance t torch Tensor t x Shape should tensor during tracing as_tensor should used tracing because records constant ret = torch stack x ret device = device avoid recording hard-coded device necessary ret = ret device=device ret torch as_tensor x device=device fw_graph = None bw_graph = None aot_graph_capture_backend gm args functorch compile min_cut_rematerialization_partition torch _functorch aot_autograd aot_module_simplified fw_compiler gm _ fw_graph = gm gm bw_compiler gm _ bw_graph = gm gm aot_module_simplified gm args fw_compiler bw_compiler partition_fn=min_cut_rematerialization_partition keep_inference_input_mutations=True Boxes detectron poolers py __init__ tensor torch Tensor Args tensor Tensor float Nx matrix Each row x y x y device = tensor device isinstance tensor torch Tensor torch device cpu tensor = torch as_tensor tensor dtype=torch float device=device tensor numel == Use reshape so we don t end up creating new tensor does depend inputs consequently confuses jit tensor = tensor reshape - dtype=torch float device=device assert tensor dim == tensor size - == tensor size tensor = tensor __len__ - int tensor shape property device tensor device convert_boxes_to_pooler_format box_lists detectron structures py boxes = torch cat x tensor x box_lists dim= __len__ returns Tensor tracing sizes = shapes_to_tensor x __len__ x box_lists device=boxes device indices = torch repeat_interleave torch arange len box_lists dtype=boxes dtype device=boxes device sizes cat indices None boxes dim= ReformerBackwardOutput = namedtuple ReformerBackwardOutput attn_output hidden_states grad_attn_output grad_hidden_states ReformerEncoderOutput = namedtuple ReformerEncoderOutput hidden_states all_hidden_states all_attentions past_buckets_states _ReversibleFunction torch autograd Function taken modeling_reformer py huggingface staticmethod forward ctx hidden_states layers attention_mask head_mask num_hashes all_hidden_states all_attentions past_buckets_states use_cache orig_sequence_length output_hidden_states output_attentions all_buckets = split duplicated tensor hidden_states attn_output = torch chunk hidden_states dim=- layer layers output_hidden_states True all_hidden_states append hidden_states attn_output = layer attn_output all_buckets = all_buckets + attn_output Add last layer output_hidden_states True all_hidden_states append hidden_states attach params ctx backward ctx save_for_backward attn_output detach hidden_states detach ctx layers = layers ctx all_buckets = all_buckets ctx head_mask = head_mask ctx attention_mask = attention_mask Concatenate RevNet outputs torch cat attn_output hidden_states dim=- staticmethod backward ctx grad_hidden_states grad_attn_output grad_hidden_states = torch chunk grad_hidden_states dim=- free memory del grad_attn_output num vars has match num forward args gradient hidden_states arg None other args grad_hidden_states None None None None None None None None None None None ReformerEncoder torch nn Module __init__ - None super __init__ dropout = layer_norm = torch nn LayerNorm eps= e- layers = torch nn Linear forward hidden_states attention_mask=None head_mask= None num_hashes=None use_cache=False orig_sequence_length= output_hidden_states=False output_attentions=False hidden_states attention lists filled wished all_hidden_states = all_attentions = past_buckets_states = None None i range len layers concat same tensor reversible ResNet hidden_states = torch cat hidden_states hidden_states dim=- hidden_states = _ReversibleFunction apply hidden_states layers attention_mask head_mask num_hashes all_hidden_states all_attentions past_buckets_states use_cache orig_sequence_length output_hidden_states output_attentions Apply layer norm concatenated hidden states hidden_states = layer_norm hidden_states Apply dropout hidden_states = torch nn functional dropout hidden_states p=self dropout training=self training ReformerEncoderOutput hidden_states=hidden_states all_hidden_states=all_hidden_states all_attentions=all_attentions past_buckets_states=past_buckets_states ListConfig ValueNode __init__ value value = value _dereference_node _is_missing False _value value Based example omegaconfig listconfig ListIterator Iterator Any __init__ lst Any resolve bool - None resolve = resolve iterator = iter lst __dict__ _content index = __next__ - Any x = next iterator resolve x = x _dereference_node x _is_missing raise AssertionError index = index + isinstance x ListConfig ValueNode x _value raise AssertionError __iter__ _iter_ex True _iter_ex resolve bool - Iterator Any try ListConfig ListIterator resolve except Exception raise AssertionError None __init__ - None _content = ListConfig ValueNode ListConfig ValueNode ListConfig ValueNode torch tensor longformer_chunk hidden_states window_overlap= convert into overlapping chunks Chunk size = w overlap size = w non-overlapping chunks size = w hidden_states = hidden_states view hidden_states size hidden_states size window_overlap window_overlap hidden_states size use ` as_strided ` make chunks overlap overlap size = window_overlap chunk_size = list hidden_states size chunk_size = chunk_size - chunk_stride = list hidden_states stride chunk_stride = chunk_stride hidden_states as_strided size=chunk_size stride=chunk_stride PartialT torch nn Module Highly simplified T Attention prefix __init__ - None super __init__ q = torch nn Linear k = torch nn Linear v = torch nn Linear forward hidden_states key_value_states=None past_key_value=None query_length=None batch_size seq_length = hidden_states shape real_seq_length = seq_length past_key_value None assert len past_key_value == f past_key_value should have past states keys values Got len past_key_value past states real_seq_length += past_key_value shape query_length None query_length shape states projection states view batch_size - transpose project hidden_states proj_layer key_value_states past_key_value projects hidden states correctly key query states key_value_states None self-attn batch_size n_heads seq_length dim_per_head hidden_states = shape proj_layer hidden_states past_key_value None cross-attn batch_size n_heads seq_length dim_per_head hidden_states = shape proj_layer key_value_states past_key_value None key_value_states None self-attn batch_size n_heads key_length dim_per_head hidden_states = torch cat past_key_value hidden_states dim= cross-attn hidden_states = past_key_value hidden_states get query states query_states = shape q hidden_states batch_size n_heads seq_length dim_per_head get key value states key_states = project hidden_states k key_value_states past_key_value past_key_value None None value_states = project hidden_states v key_value_states past_key_value past_key_value None None compute scores scores = torch matmul query_states key_states transpose truncated here scores value_states ChunkReformerFeedForward torch nn Module simplified HF modeling_reformer py __init__ - None super __init__ layer_norm = torch nn LayerNorm eps= e- dense = torch nn Linear output = torch nn Linear forward attention_output apply_chunking_to_forward forward_chunk attention_output + forward_chunk hidden_states hidden_states = layer_norm hidden_states hidden_states = dense hidden_states output hidden_states apply_chunking_to_forward forward_fn input_tensors simplified HF model_utils py assert len input_tensors tensor_shape = input_tensors shape assert all input_tensor shape == tensor_shape input_tensor input_tensors num_args_in_forward_chunk_fn = len inspect signature forward_fn parameters num_args_in_forward_chunk_fn = len input_tensors raise ValueError forward_fn input_tensors _validate_model_kwargs fn model_kwargs simplified transformers generation utils _validate_model_kwargs unused_model_args = model_args = set inspect signature fn parameters key value model_kwargs items value None key model_args unused_model_args append key unused_model_args raise ValueError f The following ` model_kwargs ` used model unused_model_args note typos generate arguments will also show up list FakeMamlInner torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x ignored=None bn_training=False linear x view x shape - PartialMaml torch nn Module Highly simplified version maml meta Meta finetuning __init__ - None super __init__ net = FakeMamlInner update_step_test = update_lr = forward x_spt y_spt x_qry y_qry querysz = x_qry size corrects = _ range update_step_test + order ruin state running_mean variance bn_weight bias we finetuning copied model instead net net = deepcopy net run i-th task compute loss k= logits = net x_spt loss = F cross_entropy logits y_spt grad = torch autograd grad loss net parameters fast_weights = p - update_lr p p zip grad net parameters loss accuracy before first update torch no_grad setsz nway logits_q = net x_qry net parameters bn_training=True setsz pred_q = F softmax logits_q dim= argmax dim= scalar correct = torch eq pred_q y_qry sum item corrects = corrects + correct loss accuracy after first update torch no_grad setsz nway logits_q = net x_qry fast_weights bn_training=True setsz pred_q = F softmax logits_q dim= argmax dim= scalar correct = torch eq pred_q y_qry sum item corrects = corrects + correct del net accs = torch tensor corrects querysz accs softmax_backward_data parent grad_output output dim torch _softmax_backward_data _softmax_backward_data grad_output output parent dim dtype XSoftmax torch autograd Function transformers models deberta modeling_deberta XSoftmax staticmethod forward input mask dim dim = dim rmask = ~ mask torch bool output = input masked_fill rmask torch tensor torch finfo input dtype min output = torch softmax output dim output masked_fill_ rmask save_for_backward output rmask output staticmethod backward grad_output output _ = saved_tensors inputGrad = softmax_backward_data grad_output output dim output inputGrad None None ModelOutput collections OrderedDict based file_utils py HuggingFace __getitem__ k isinstance k str inner_dict = dict items inner_dict k to_tuple k __setattr__ name value name keys value None Don t call __setitem__ avoid recursion errors super __setitem__ name value super __setattr__ name value __setitem__ key value Will raise KeyException needed super __setitem__ key value Don t call __setattr__ avoid recursion errors super __setattr__ key value to_tuple tuple k k keys create_rand_mask_from_inputs from_blocked_mask to_blocked_mask rand_attn num_attention_heads num_rand_blocks batch_size from_seq_length from_block_size taken HF modeling_big_bird py num_windows = from_seq_length from_block_size - rand_mask = torch stack p i flatten p i zip to_blocked_mask rand_attn rand_mask = rand_mask view batch_size num_attention_heads num_windows num_rand_blocks from_block_size rand_mask = torch einsum blq bhlk- bhlqk from_blocked_mask - rand_mask rand_mask SequentialAppendList torch nn Sequential timm models vovnet py forward x torch Tensor concat_list list torch Tensor - torch Tensor i module enumerate i == concat_list append module x concat_list append module concat_list - x = torch cat concat_list dim= x concat_list BatchNormAct d torch nn BatchNorm d Taken timm __init__ num_features eps= e- momentum= affine=True track_running_stats=True act_layer=torch nn ReLU inplace=True super __init__ num_features eps=eps momentum=momentum affine=affine track_running_stats=track_running_stats act = act_layer inplace=inplace torch jit ignore _forward_python x super forward x forward x torch jit is_scripting x = _forward_jit x x = _forward_python x x = act x x get_parameter_dtype parameter huggingface model_utils py try next parameter parameters dtype except StopIteration For nn DataParallel compatibility PyTorch find_tensor_attributes module tuples = k v k v module __dict__ items torch is_tensor v tuples gen = parameter _named_members get_members_fn=find_tensor_attributes first_tuple = next gen first_tuple dtype DummyConfig attn_layers = local lsh local lsh local lsh lsh_attn_chunk_length = local_attn_chunk_length = _get_min_chunk_len config hf_Reformer attn_types = config attn_layers attn_types_set = set attn_types len attn_types_set == attn_types == lsh config lsh_attn_chunk_length len attn_types_set == attn_types == local config local_attn_chunk_length len attn_types_set == attn_types_set == lsh local min config lsh_attn_chunk_length config local_attn_chunk_length raise NotImplementedError f Only attn layer types lsh local exist ` config attn_layers ` config attn_layers Select attn layer types lsh local only _stable_argsort vector dim hf_Reformer function scales vector so torch argsort stable torch argsort stable its own scale_offset = torch arange vector shape dim device=vector device view - scale_offset = scale_offset expand vector shape scaled_vector = vector shape dim vector + scale_offset vector shape dim torch argsort scaled_vector dim=dim _get_sorted_bucket_idx_and_undo_sorted_bucket_idx buckets hf_Reformer no gradients needed torch no_grad hash-based sort sorted_bucket_idx = _stable_argsort buckets dim=- create simple indices scatter have undo sort indices = torch arange sorted_bucket_idx shape - device=buckets device view - expand sorted_bucket_idx shape get undo sort undo_sorted_bucket_idx = sorted_bucket_idx new sorted_bucket_idx size undo_sorted_bucket_idx scatter_ - sorted_bucket_idx indices sorted_bucket_idx undo_sorted_bucket_idx CustomList list __call__ x processor x = processor x x clear pass prevents RestrictedListSubclassVariable kicking CustomList list __call__ x processor x = processor x x length_times_ len append_twice x extend x x _merge_criteria_processor_list default_list custom_list simplified transformers generation utils py len custom_list == default_list default default_list custom custom_list type custom type default raise ValueError default_list extend custom_list default_list FeedForwardLayer nn Module __init__ d_model dim_feedforward activation dropout - None super __init__ linear = nn Linear d_model dim_feedforward activation = activation dropout = nn Dropout dropout linear = nn Linear dim_feedforward d_model dropout = nn Dropout dropout forward x dropout linear dropout activation linear x TransformerEncoderLayer nn Module __init__ d_model nhead dim_feedforward= dropout= activation=nn ReLU layer_norm_eps= e- super __init__ self_attn = nn MultiheadAttention d_model nhead dropout=dropout norm = nn LayerNorm d_model eps=layer_norm_eps norm = nn LayerNorm d_model eps=layer_norm_eps dropout = nn Dropout dropout ff_block = FeedForwardLayer d_model dim_feedforward activation dropout forward src src_mask=None src_key_padding_mask=None x = src x = norm x + _sa_block x src_mask src_key_padding_mask x = norm x + _ff_block x x self-attention block _sa_block x attn_mask key_padding_mask x = self_attn x x x attn_mask=attn_mask key_padding_mask=key_padding_mask need_weights=False dropout x feed forward block _ff_block x ff_block x MockModule torch nn Module inner_fn left right tuple left == tuple right fn tensor type tensor int False torch add tensor tensor inner_fn tensor shape IncByOne __init__ x x = x + IncByTwo __init__ x x = x + LRUCacheWarningTests LoggingTestCase requires_cuda make_logging_test dynamo=logging DEBUG test_lru_cache_warning_issued_during_tracing records torch set_default_device cuda torch compile backend= eager f x torch get_device_module x = x cos sin x result = f torch randn assertIsInstance result torch Tensor record records call lru_cache wrapped function record getMessage fail lru_cache warning incorrectly logged ReproTests torch _dynamo test_case TestCase setUp - None try utils install_guard_manager_testing_hook except ImportError utils install_guard_manager_testing_hook exit_stack = contextlib ExitStack exit_stack enter_context install_guard_manager_testing_hook guard_manager_clone_hook_fn super setUp tearDown - None exit_stack close super tearDown test_compiled_module_truthiness Test empty ModuleList original_empty = nn ModuleList compiled_empty = torch compile original_empty assertEqual bool original_empty bool compiled_empty assertFalse bool compiled_empty Test non-empty ModuleList original_filled = nn ModuleList nn Linear compiled_filled = torch compile original_filled assertEqual bool original_filled bool compiled_filled assertTrue bool compiled_filled guard_manager_clone_hook_fn guard_manager_wrapper f_locals builder root = guard_manager_wrapper root cloned_root = root clone_manager lambda x True cloned_wrapper = torch _dynamo guards GuardManagerWrapper cloned_root assertEqual str guard_manager_wrapper str cloned_wrapper assertTrue cloned_root check f_locals guard_manager_wrapper diff_guard_root assertTrue guard_manager_wrapper diff_guard_root check f_locals test_do_paste_mask torch _dynamo utils counters clear cnt = torch _dynamo testing CompileCounter opt__do_paste_mask = torch compile _do_paste_mask backend=cnt opt__do_paste_mask torch randn torch tensor True opt__do_paste_mask torch randn torch tensor True opt__do_paste_mask torch randn torch tensor True opt__do_paste_mask torch randn torch tensor True opt__do_paste_mask torch randn torch tensor False dynamic shapes static shapes assertIn cnt frame_count assertIn cnt op_count test_convert_boxes_to_pooler_format boxes = Boxes torch arange reshape Boxes torch arange reshape boxes = Boxes torch arange reshape Boxes torch arange reshape correct = convert_boxes_to_pooler_format boxes correct = convert_boxes_to_pooler_format boxes fn = convert_boxes_to_pooler_format cnt = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnt assertTrue same opt_fn boxes correct assertTrue same opt_fn boxes correct repeat_interleave dynamic shape operator we do execute In future we could reduce frame_count down guarding exact values ` Tensor repeats ` arg torch _dynamo config assume_static_by_default assertExpectedInline cnt frame_count assertExpectedInline cnt op_count assertExpectedInline cnt frame_count assertExpectedInline cnt op_count test_boxes_len fn boxes len boxes + boxes __len__ + boxes tensor boxes = Boxes torch arange reshape cnt = torch _dynamo testing CompileCounter opt_fn = torch _dynamo optimize_assert cnt fn assertTrue same opt_fn boxes boxes tensor + torch _dynamo config assume_static_by_default assertExpectedInline cnt frame_count assertExpectedInline cnt op_count assertExpectedInline cnt frame_count assertExpectedInline cnt op_count _reformer nopython input = torch randn model = ReformerEncoder torch manual_seed correct = copy deepcopy model input cnt = torch _dynamo testing CompileCounter torch manual_seed opt_model = torch compile model backend=cnt fullgraph=nopython assertTrue same opt_model input correct cnt https github com pytorch pytorch issues test_out_overload_non_contiguous f x y torch abs x out=y T f_compiled = torch compile f backend= aot_eager x_ref = torch arange dtype=torch float reshape y_ref = torch arange dtype=torch float reshape x_test = torch arange dtype=torch float reshape y_test = torch arange dtype=torch float reshape out_ref = f x_ref y_ref out_test = f_compiled x_test y_test assertEqual out_ref out_test assertEqual y_ref y_test https github com pytorch pytorch issues test_view_dtype_overload f x x view torch int f_compiled = torch compile f backend= aot_eager x = torch ones requires_grad=True out_ref = f x out_test = f_compiled x assertEqual out_ref out_test x = torch ones requires_grad=False out_ref = f x out_test = f_compiled x assertEqual out_ref out_test https github com pytorch pytorch issues test_intermediate_leaf_requires_grad f x leaf = torch ones requires_grad=True leaf leaf f_compiled = torch compile f backend= aot_eager x = torch arange dtype=torch float reshape leaf out = f x leaf_test out_test = f_compiled x out sum backward out_test sum backward assertEqual leaf grad leaf_test grad https github com pytorch pytorch issues test_unpack_hooks_dont_run_during_tracing f x y x y f_compiled = torch compile f backend= aot_eager pack_count = unpack_count = pack_hook x nonlocal pack_count pack_count += x unpack hook shouldn t run during compilation while we trace forward unpack_hook x nonlocal unpack_count unpack_count += x x = torch ones requires_grad=True y = torch ones requires_grad=False torch autograd graph saved_tensors_hooks pack_hook unpack_hook out_test = f_compiled x y assertEqual pack_count assertEqual unpack_count out_test sum backward assertEqual pack_count assertEqual unpack_count https github com pytorch pytorch issues test_unpack_hooks_can_be_disabled f x y x y f_compiled = torch compile f backend= aot_eager x = torch ones requires_grad=True y = torch ones requires_grad=False torch autograd graph disable_saved_tensors_hooks hooks disabled out_test = f_compiled x y out_test sum backward https github com pytorch pytorch issues test_disabling_unpack_hooks_within_compiled_region g z torch autograd graph disable_saved_tensors_hooks hooks disabled z + f x y z = x y g z f_compiled = torch compile f backend= aot_eager x = torch ones requires_grad=True y = torch ones requires_grad=False out_test = f_compiled x y out_test sum backward See https github com pytorch pytorch issues test_gan_repro_trying_to_backward_through_the_graph_a_second_time f b c = torch ones d = torch ones e = torch matmul c g_loss = torch abs e - d mean g_loss backward fake_d_pred = torch matmul b e detach d_loss = fake_d_pred mean d_loss backward a_ref = torch randn requires_grad=True b_ref = torch randn requires_grad=True out_ref = f a_ref b_ref a_test = a_ref detach clone requires_grad_ True b_test = b_ref detach clone requires_grad_ True out_test = torch compile f backend= aot_eager a_test b_test assertEqual out_ref out_test assertEqual a_ref grad a_test grad assertEqual b_ref grad b_test grad https github com pytorch pytorch issues test_tuple_enum_as_key_dict MyEnum Enum A = SomeModel torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x - torch Tensor linear x MyEnum A x = MyEnum A torch rand model_pytorch = SomeModel model = torch compile model_pytorch Executing twice works model x y = model x assertEqual y model_pytorch x test_embedding_backward_broadcasting_decomp f grad_output indices num_weights = padding_idx = scale_grad_by_freq = True torch ops aten embedding_dense_backward grad_output indices num_weights padding_idx scale_grad_by_freq f_compiled = torch compile f backend= aot_eager grad_output = torch ones dtype=torch float indices = torch ones dtype=torch int out_ref = f grad_output indices out_test = f_compiled grad_output indices assertEqual out_ref out_test test_reformer_eval torch no_grad cnt = _reformer nopython=True assertEqual cnt frame_count assertEqual cnt op_count test_reformer_train torch enable_grad cnt = _reformer nopython=False expected_op_count = torch _dynamo config inline_inbuilt_nn_modules assertExpectedInline cnt frame_count assertExpectedInline cnt op_count expected_op_count test_longformer_chunk input = torch randn input = torch randn correct = longformer_chunk input correct = longformer_chunk input fn = longformer_chunk cnt = torch _dynamo testing CompileCounter opt_fn = torch _dynamo optimize_assert cnt fn assertTrue same opt_fn input correct assertTrue same opt_fn input correct assertTrue same opt_fn input correct assertTrue same opt_fn input correct torch _dynamo config assume_static_by_default torch _dynamo config automatic_dynamic_shapes assertExpectedInline cnt frame_count assertExpectedInline cnt op_count assertExpectedInline cnt frame_count assertExpectedInline cnt op_count assertExpectedInline cnt frame_count assertExpectedInline cnt op_count test_hf_t _forward input = torch randn model = PartialT correct = model input cnt = torch _dynamo testing CompileCounter opt_model = torch _dynamo optimize_assert cnt model assertTrue same opt_model input correct torch _dynamo config assume_static_by_default assertExpectedInline cnt frame_count assertExpectedInline cnt op_count assertExpectedInline cnt frame_count assertExpectedInline cnt op_count test_module_in_skipfiles model = nn Linear cnt = torch _dynamo testing CompileCounter torch compile model backend=cnt fullgraph=True torch randn assertEqual cnt frame_count assertEqual cnt op_count test_function_in_skipfiles cnt = torch _dynamo testing CompileCounter torch compile torch sin backend=cnt fullgraph=True torch randn assertEqual cnt frame_count assertEqual cnt op_count test_slicing_dynamic_shape fn y x = torch ones idx = y out = x idx out + counter = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=counter out = opt_fn torch ones dtype=torch long idx should - slicing off elem tensor assertEqual list out shape assertEqual counter op_count assertEqual counter frame_count assertEqual list opt_fn torch tensor shape test_slicing_dynamic_shape_setitem fn input_lengths torch Tensor new_ones_ getitem_ = input_lengths new_ones_ slice getitem_ None None = setitem_ = new_ones_ setitem_ x = torch randn dtype=torch int y = torch randn ref = fn x y opt_fn = torch compile fn backend= aot_eager res = opt_fn x y assertTrue same ref res torch _dynamo config patch error_on_recompile=True torch fx experimental _config patch use_duck_shape=False test_dynamic_shape_disable_duck_size noqa F TestModel nn Module __init__ super __init__ forward x torch Tensor val int - torch Tensor x + val main_model = TestModel memory_format=torch channels_last opt_model = torch compile main_model backend= eager dynamic=True x = torch rand memory_format=torch channels_last x = torch rand memory_format=torch channels_last main_model x opt_model x main_model x opt_model x test_chunk_reformer_ff input = torch randn model = ChunkReformerFeedForward correct = model input cnt = torch _dynamo testing CompileCounter opt_model = torch _dynamo optimize_assert cnt model assertTrue same opt_model input correct assertEqual cnt frame_count assertLessEqual cnt op_count see https github com pytorch pytorch issues NB When you remove expectedFailure don t forget uncomment adjust assertEqual below unittest expectedFailure torch _dynamo config patch fake_tensor_propagation=True capture_scalar_outputs=True test_maml_item_capture = torch randn b = torch zeros dtype=torch int c = torch randn d = torch zeros dtype=torch int model = PartialMaml correct = model b c d cnt = torch _dynamo testing CompileCounter opt_model = torch compile model backend=cnt _ range assertTrue same opt_model b c d correct torch _dynamo config assume_static_by_default assertExpectedInline cnt frame_count assertExpectedInline cnt frame_count TODO jansel figure out why op count depends imports assertIn cnt op_count see https github com pytorch pytorch issues torch _dynamo config patch capture_scalar_outputs=False test_maml_no_item_capture = torch randn b = torch zeros dtype=torch int c = torch randn d = torch zeros dtype=torch int model = PartialMaml correct = model b c d cnt = torch _dynamo testing CompileCounter opt_model = torch compile model backend=cnt _ range assertTrue same opt_model b c d correct torch _dynamo config assume_static_by_default assertExpectedInline cnt frame_count assertExpectedInline cnt frame_count test_hf_model_output ex = ModelOutput a=torch randn b=torch randn c=torch randn fn x x + fn x x + fn x x to_tuple + fn x x + cnt = torch _dynamo testing CompileCounter fn fn fn fn fn cnt clear opt_fn = torch _dynamo optimize_assert cnt fn assertTrue same opt_fn ex ex + assertEqual cnt frame_count assertEqual cnt op_count test_create_rand_mask_from_inputs args = torch randn torch randn torch zeros dtype=torch int correct = create_rand_mask_from_inputs args fn = create_rand_mask_from_inputs cnt = torch _dynamo testing CompileCounter opt_fn = torch _dynamo optimize_assert cnt fn assertTrue same opt_fn args correct torch _dynamo config assume_static_by_default assertExpectedInline cnt frame_count assertExpectedInline cnt op_count assertExpectedInline cnt frame_count assertExpectedInline cnt op_count test_rng_state fn state = torch get_rng_state before = torch rand torch set_rng_state state after = torch rand before after cnt = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnt before after = opt_fn assertTrue same before after assertEqual cnt frame_count assertEqual cnt op_count rand rand try _ _ = torch _dynamo export fn See https github com pytorch pytorch pull fail unexpected export success except torch _dynamo exc Unsupported pass test_threading_local threading foo = threading local foo x = torch rand f x torch cat x foo x cnt = torch _dynamo testing CompileCounter opt_f = torch compile f backend=cnt fullgraph=True inp = torch ones out = f inp opt_out = opt_f inp assertEqual opt_out out assertEqual cnt frame_count test_seq_append_list x = torch randn model = SequentialAppendList torch nn Linear torch nn ReLU torch nn Linear torch nn ReLU one tricky because mutates list provided input l = x l = x correct _ = model x l cnt = torch _dynamo testing CompileCounter opt_model = torch _dynamo optimize_assert cnt model result l = opt_model x l assertTrue same result correct assertTrue same l l assertIs l l assertEqual cnt frame_count assertEqual cnt op_count test_batch_norm_act = torch randn model = BatchNormAct d eval correct = model cnt = torch _dynamo testing CompileCounter torch _dynamo config specialize_int _local_scalar_dense causes graph break w -dim tensor opt_model = torch compile model backend=cnt assertTrue same opt_model correct opt_model = torch _dynamo optimize_assert cnt model assertTrue same opt_model correct assertEqual cnt frame_count assertEqual cnt op_count test_get_parameter_dtype model = SequentialAppendList torch nn Linear torch nn ReLU fn model x x + torch randn dtype=get_parameter_dtype model cnt = torch _dynamo testing CompileCounter opt_fn = torch _dynamo optimize_assert cnt fn assertEqual opt_fn model torch randn dtype torch float assertEqual cnt frame_count assertEqual cnt op_count test_nn_parameter test_fn = torch nn Parameter torch randn Checks TensorVariable stores type information correctly assertTrue isinstance torch nn Parameter cnt = torch _dynamo testing CompileCounter opt_test_fn = torch compile test_fn backend=cnt out = opt_test_fn assertTrue isinstance out torch nn Parameter test_Size test_fn = torch randn x = torch Size Checks SizeVariable torch Size object assert isinstance x torch Size Causes graph breaks checks reconstruction SizeVariable object assertIsInstance x torch Size cnt = torch _dynamo testing CompileCounter opt_test_fn = torch compile test_fn backend=cnt opt_test_fn See https github com pytorch pytorch issues test_copy_weird_strides This test requires inductor s copy decomp preserve strides properly test_fn b = torch zeros b = c = b view d = c transpose d add_ d sh st dt dev rg = torch float cpu True = rand_strided sh st dt dev requires_grad_ rg compiled_f = torch compile test_fn backend= aot_eager_decomp_partition out = test_fn out = compiled_f assertEqual out out test_indexing_with_list test_fn run_test tensor idx npt = tensor numpy assert npt idx shape == tensor idx shape x = torch arange cases = None None None case cases run_test x case torch randn cnt = torch _dynamo testing CompileCounter opt_test_fn = torch compile test_fn backend=cnt opt_test_fn test_foreach_decomp_arg_names https github com pytorch pytorch issues torch compile fullgraph=True foreach_pow kwargs torch _foreach_pow kwargs foreach_pow self= torch ones device= cpu exponent= torch compile fullgraph=True foreach_lerp_ kwargs torch _foreach_lerp_ kwargs foreach_lerp_ self= torch ones device= cpu tensors = torch ones device= cpu weights= torch ones device= cpu test_reformer_min_chunk_len fn cfg t = torch empty t fill_ _get_min_chunk_len cfg t cfg = DummyConfig cnt = torch _dynamo testing CompileCounter opt_fn = torch _dynamo optimize_assert cnt fn assertEqual opt_fn cfg With unspec int maximum computation preserved assertExpectedInline cnt frame_count torch _dynamo config automatic_dynamic_shapes torch _dynamo config assume_static_by_default assertExpectedInline cnt op_count assertExpectedInline cnt op_count assertExpectedInline cnt op_count test_reformer_sorting x = torch zeros dtype=torch int correct = _get_sorted_bucket_idx_and_undo_sorted_bucket_idx x fn = _get_sorted_bucket_idx_and_undo_sorted_bucket_idx cnt = torch _dynamo testing CompileCounter opt_fn = torch _dynamo optimize_assert cnt fn assertTrue same opt_fn x correct torch _dynamo config assume_static_by_default assertExpectedInline cnt frame_count assertExpectedInline cnt op_count assertExpectedInline cnt frame_count assertExpectedInline cnt op_count test_recursive_map https github com pytorch torchdynamo issues _recursive_map struct batch_dim= k v struct items v None isinstance v dict _recursive_map v struct k = v toy_example b v x = torch abs + v None _recursive_map v x b cnt = torch _dynamo testing CompileCounter opt_toy_example = torch compile toy_example backend=cnt opt_toy_example torch randn torch randn layer memory_keys torch randn assertEqual cnt frame_count assertEqual cnt op_count test_issue device = torch device cpu fcnn in_dim out_dim hidden_dim activation=torch nn GELU layers = torch nn Linear in_dim hidden_dim device=device activation torch nn Linear hidden_dim out_dim device=device torch nn Sequential layers testmodel torch nn Module __init__ - None super __init__ interaction_networks = torch nn ModuleList fcnn _ range interact x cycle interaction_networks cycle x model = testmodel forward_aot = torch compile model interact fullgraph=True dynamic=True backend= eager x = torch rand device=device forward_aot x previously failed test_issue n_heads = d_model = model = TransformerEncoderLayer d_model n_heads inp = torch randn d_model cnt = torch _dynamo testing CompileCounter opt_model = torch compile model backend=cnt fullgraph=True opt_model inp opt_model inp assertEqual cnt frame_count assertEqual cnt op_count test_exec_import fn exec math fn try math sqrt False except NameError True fn fn fn assertTrue fn opt_fn = torch compile fn backend= eager assertTrue opt_fn test_exec_wildcard_import Test globals carried over frame frame fn exec torch fn x = torch zeros i range x = x + i x fn fn fn ref = fn opt_fn = torch compile fn backend= eager res = opt_fn assertTrue same ref res test_with_on_graph_break_inst reversible x print Hello world Cause graph break so inline fails torch sin torch cos x fn x torch enable_grad = torch sin x b = reversible c = torch sigmoid b c sum backward x grad x = torch randn requires_grad=True x grad = None torch no_grad ref = fn x x grad = None opt_fn = torch compile fn backend= eager torch no_grad res = opt_fn x assertTrue same ref res test_with_on_graph_break_nested reversible x torch _dynamo graph_break Cause graph break so inline fails torch sin torch cos x fn x nested context manager failed previously torch no_grad torch enable_grad = torch sin x b = reversible c = torch sigmoid b c sum backward x grad x = torch randn requires_grad=True x grad = None torch no_grad ref = fn x x grad = None opt_fn = torch compile fn backend= eager torch no_grad res = opt_fn x assertTrue same ref res https github com pytorch torchdynamo issues test_grad_mode_carrying_correct_state_after_graph_break fn x torch no_grad y = x print Break z = x + y z x = torch randn requires_grad=True opt_fn = torch compile fn backend= eager y z = opt_fn x assertFalse y requires_grad assertFalse z requires_grad test_abc_setattr tests we correctly bail out __setattr__ calls TODO does ensure ABC classes correctly inferred ClassVariables doesn t test fix super BaseModule torch nn Module ABC blah x x + Derived BaseModule __setattr__ name value - None super __setattr__ name value forward x expect graph break __setattr__ foo = blah x blah x super blah x x = torch randn requires_grad=True mod = Derived opt_mod = torch compile mod backend= eager opt_mod x Not sure what test testing It earlier graph breaking __dict__ so counter = With __dict__ support there no graph break assertGreaterEqual torch _dynamo utils counters frames ok assertGreaterEqual torch _dynamo utils counters frames total torch _dynamo config patch suppress_errors True test_guard_fail_tensor_bool torch _dynamo disable recursive=False fn condition_shape = dtypes = torch bool shapes = tensors = torch empty shape dtype=dtype fill_ shape dtype itertools product shapes dtypes x_vals = tensors y_vals = tensors torch _dynamo disable get_expected condition x y x_np = x cpu numpy isinstance x torch Tensor x y_np = y cpu numpy isinstance y torch Tensor y torch from_numpy np where condition cpu numpy x_np y_np common_dtype x y zip x_vals y_vals condition = torch empty condition_shape dtype=torch bool bernoulli_ common_dtype = torch result_type x y check_equal condition x y NumPy aggressively promotes double hence cast output correct dtype expected = get_expected condition x y result = torch where condition x y assert torch allclose expected result check_equal condition x y check_equal condition y x fn opt_fn = torch compile fn backend= eager opt_fn test_guard_fail_nested_tuple fn args torch ones args This adds tensor check args args args = torch ones torch ones torch ones args = torch ones torch ones opt_fn = torch compile fn backend= eager ref = opt_fn args res = opt_fn args assertTrue same ref res test_nullcontext torch compile fullgraph=True backend= eager fn x ctx x = x sin ctx x = x cos x = x sin x y = torch randn assertTrue same fn y contextlib nullcontext y sin cos sin test_nullcontext torch compile fullgraph=True backend= eager fn x ctx x = x sin ctx x = x cos x = x sin x y = torch randn assertTrue same fn y contextlib nullcontext y sin cos sin test_no_grad_inline torch no_grad x x sin torch compile backend= eager fullgraph=True b x x cos y = torch randn assertTrue same b y y sin cos skipIfWindows msg= torch _dynamo exc TorchRuntimeError Failed running call_function torch LongTensor FakeTensor size= dtype=torch int noqa B test_longtensor_list partition torch _dynamo disable rand_gen rand_vals = random randint _ range List tensors mixed np arrays list np array rand_vals partition + torch tensor val val rand_vals partition fn x random_list = rand_gen z = torch LongTensor random_list x z x = torch ones random seed ref = fn x ref = fn x opt_fn = torch compile fn backend= eager Especially internal usage there many calls random functions first compile e g various library initializations Run once get out way before resetting seed opt_fn x random seed res = opt_fn x res = opt_fn x assertTrue same ref res assertTrue same ref res test_primtorch torch compile backend= eager fn x torch _refs abs x fn torch randn unittest expectedFailure inline_call inline skipfiles bind python inspect py test_primtorch_no_graph_break torch compile backend= eager fullgraph=True fn x torch _refs abs x fn torch randn test_torch_tensor_ops_no_graph_break torch compile backend= eager fullgraph=True fn x torch Tensor abs_ x fn torch randn unittest skipIf isinstance torch ops aten abs torch _ops OpOverloadPacket old pt doesn t work test_torch_ops_aten Picked op doesn t show up default list torch compile backend= eager fullgraph=True fn x torch ops aten absolute x fn torch randn test_hf_gelu_inline GELUActivation nn Module __init__ - None super __init__ act = nn functional gelu forward input act input torch compile backend= eager fullgraph=True fn x GELUActivation x y = torch randn assertTrue same fn y nn functional gelu y torch compile backend= eager fullgraph=True fn_returns x GELUActivation x + act _ = fn_returns y assertIsInstance act GELUActivation assertIs act act nn functional gelu assertTrue hasattr act _buffers check __init__ got called test_dropout_inline torch compile backend= eager fn x torch nn Dropout x y = torch randn torch manual_seed ref = nn functional dropout y torch manual_seed res = fn y assertTrue same ref res test_setitem_boolean_mask_diff fn x b y x = x clone x b = y x opt_fn = torch compile fn backend= aot_eager x = torch randn requires_grad=True b = torch tensor True False True False y = torch randn requires_grad=True opt_fn x b y test_setitem_tuple_boolean_mask_diff fn x b y x = x clone x b = y x opt_fn = torch compile fn backend= aot_eager x = torch randn requires_grad=True b = torch tensor True False True False y = torch randn requires_grad=True opt_fn x b y test_torch_tensor_ops fn x torch Tensor abs_ x x = torch randn opt_fn = torch compile fn backend= eager fullgraph=True y = fn x y_ = opt_fn x assertTrue same y y_ test_guard_ordering_shape_fail If function which takes tensor has inner function which compiled generates guard its shape they evaluated wrong order So subsequent call int passed instead tensor guard evaluation will crash no attribute shape error m = MockModule opt_m = torch compile m backend= eager opt_m fn torch ones opt_m fn - test_tensor_isinstance_tuple torch compile backend= eager fn t = torch ones isinstance t int torch Tensor msg = str format instance type t int torch Tensor raise ValueError msg True fn test_isinstance_dtype torch compile backend= eager fullgraph=True fn x isinstance torch bfloat torch dtype x fn torch randn test_isinstance_storage torch compile backend= eager fn x f = bytearray x x x x x x x x bools = torch BoolStorage from_buffer f big assert isinstance bools torch BoolStorage x fn torch randn test_issue torch compile backend= eager fullgraph=True f x y x + y A = assertEqual f torch zeros A torch full del A graph break missing attr assertRaises torch _dynamo exc Unsupported f torch zeros A test_sort_out dtype = torch float device = cpu fn tensor = torch randn dtype=dtype device=device values = torch tensor dtype=dtype device=device indices = torch tensor dtype=torch long device=device torch sort tensor out= values indices assertEqual values stride assertEqual indices stride fn opt_fn = torch compile fn backend= eager opt_fn test_sort_out MyModule torch nn Module __init__ - None super __init__ sorted = torch nn Buffer torch ones indices = torch nn Buffer torch ones dtype=torch long forward x torch sort x out= sorted indices x + sorted indices x = torch randn m = MyModule ref = m x opt_m = torch compile m backend= eager res = opt_m x assertTrue same ref res test_sigmoid_out dtype = torch float device = cpu fn inp = torch randn dtype=dtype device=device out = torch tensor dtype=dtype device=device torch sigmoid inp out=out assertEqual out numel fn opt_fn = torch compile fn backend= eager opt_fn test_sigmoid_out MyModule torch nn Module __init__ - None super __init__ base = torch nn Buffer torch ones forward x torch sigmoid x out=self base x + base x = torch randn m = MyModule ref = m x opt_m = torch compile m backend= eager res = opt_m x assertTrue same ref res test_out_root_cell_shape_change torch compile backend= eager fn out = torch empty run x = torch zeros torch sigmoid x out=out out size run res = fn assertEqual res test_out_nested_cell_shape_change torch compile backend= eager fn run x = torch zeros out = torch empty capture out Force ` out ` nested cell torch sigmoid x out=out out size run res = fn assertEqual res test_out_root_cell_tuple_shape_change torch compile backend= eager fn out = torch empty out = torch empty dtype=torch long run x = torch zeros torch sort x out= out out out size out size run res = fn assertEqual res test_out_nested_cell_tuple_shape_change torch compile backend= eager fn run x = torch zeros out = torch empty out = torch empty dtype=torch long capture Force ` out ` ` out ` nested cells out out torch sort x out= out out out size out size run res = fn assertEqual res test_slice_into_list_mutable Mod torch nn Module forward listy x = listy _ range z = torch abs torch randn + x = z x m = Mod listy = torch randn cnt = torch _dynamo testing CompileCounter opt_m = torch compile m backend=cnt fullgraph=True opt_m forward listy assertEqual cnt frame_count torch _dynamo config patch capture_scalar_outputs=True test_issue cnt = CompileCounter torch compile backend=cnt dynamic=True fn x x = x + y = x item y x x x = torch tensor fn x assertEqual cnt frame_count assertEqual cnt op_count torch _dynamo reset fn = torch compile fn fullgraph=True backend= eager assertRaises torch _dynamo exc UserError fn x test_vdd_duplicate_error fn dt keys = list dt _jt_dict keys p = torch cos dt _jt_dict keys _value q = torch sin r = torch sigmoid dt _jt_dict keys _value p + q + r Value __init__ - None _value = torch randn Sample __init__ - None _jt_dict = _jt_dict POSITION_ID = Value = torch randn sample = Sample ref = fn sample optimized_fn = torch compile fn backend= eager fullgraph=True res = optimized_fn sample assertTrue same ref res test_specialized_stride f e = torch empty x = e x stride assertEqual f torch compile f backend= eager test_out_none https github com pytorch pytorch issues fn input torch nn functional normalize input dim= out=None x = torch rand assertEqual fn x torch compile fn backend= eager x test_multi_import has_detectron raise unittest SkipTest requires detectron torch compile backend= eager fullgraph=True to_bitmasks boxes detectron layers mask_ops _paste_masks_tensor_shape paste_masks_in_image paste_masks_in_image None _paste_masks_tensor_shape None boxes + assertTrue to_bitmasks torch zeros == torch ones all test_multi_dot_import fn x torch sin x fn x torch fx _ = torch fx symbolic_trace fn x x = torch randn fn x cnt = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnt opt_fn x assertEqual cnt frame_count test_relative_import try utils _ noqa F fn x utils tensor_for_import_testing x tensor_for_import_testing except ImportError fn x utils tensor_for_import_testing x tensor_for_import_testing x = torch randn fn x cnt = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnt fullgraph=True opt_fn x assertEqual cnt frame_count test_relative_import_no_modulename try utils _ noqa F fn x utils x utils tensor_for_import_testing except ImportError fn x utils x utils tensor_for_import_testing x = torch randn fn x cnt = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnt fullgraph=True opt_fn x assertEqual cnt frame_count test_bigbird_unsqueeze_inplace fn reshape_ view_ = reshape_ clone view_ unsqueeze_ cat_ = torch cat view_ dim= view_ = cat_ view - view_ x = torch randn requires_grad=True ref = fn x opt_fn = torch compile fn backend= aot_eager res = opt_fn x assertTrue same ref res test_issue _size_aot_autograd fn x do tensor op size compute y = x x_size = x size trigger graph break print arf use tensor op size compute z = y view x_size + z x = torch randn requires_grad=True ref = fn x opt_fn = torch compile fn backend= aot_eager res = opt_fn x assertTrue same ref res test_ellipsis Repro torch nn Module __init__ - None super __init__ lnorm = torch nn LayerNorm eps= e- elementwise_affine=True linear = torch nn Linear in_features= out_features= bias=True forward cat_ lnorm = lnorm cat_ getitem_ = lnorm slice None None None slice None Ellipsis linear = linear getitem_ linear args = torch randn mod = Repro opt_mod = torch compile mod backend= eager fullgraph=True assertTrue same mod args opt_mod args test_reinplacing MockModule torch nn Module __init__ - None super __init__ self_layoutlm_embeddings_x_position_embeddings = torch nn Embedding self_layoutlm_embeddings_y_position_embeddings = torch nn Embedding forward getitem_ getitem_ add self_layoutlm_embeddings_x_position_embeddings = self_layoutlm_embeddings_x_position_embeddings getitem_ self_layoutlm_embeddings_y_position_embeddings = self_layoutlm_embeddings_y_position_embeddings getitem_ add_ = add + self_layoutlm_embeddings_x_position_embeddings add_ = add_ + self_layoutlm_embeddings_y_position_embeddings add_ mod = MockModule opt_mod = torch compile mod backend= aot_eager_decomp_partition args = torch int cpu False torch int cpu False torch float cpu True args = rand_strided sh st dt dev requires_grad_ rg sh st dt dev rg args assertTrue same_two_models mod opt_mod args test_optimized_deepcopy See https github com pytorch pytorch pull Foo torch nn Module __init__ - None super __init__ fc = torch nn Linear in_features= out_features= bias=True forward x fc x mod = Foo opt_mod = torch compile mod backend= eager args = torch randn assertTrue same_two_models mod opt_mod args test_class_member Foo torch nn Module = b = torch ones __init__ - None super __init__ c = forward x x cos + + b + c mod = Foo opt_mod = torch compile mod backend= eager fullgraph=True args = torch randn assertTrue same mod args opt_mod args test_named_buffers Foo torch nn Module __init__ - None super __init__ x = torch nn Buffer torch ones y = torch nn Buffer torch ones forward inp res = _ buffer named_buffers res += buffer sum inp cos + res mod = Foo opt_mod = torch compile mod backend= eager fullgraph=True args = torch randn assertTrue same mod args opt_mod args test_requires_grad_guards_with_grad_mode f x x requires_grad x + x + x = torch ones requires_grad=True f_compiled = torch compile f torch no_grad compile inference graph f_compiled x Test we should fail guards recompile even though s still inference graph out_ref = f x detach out = f_compiled x detach assertEqual out_ref out assertEqual out_ref requires_grad out requires_grad test_requires_grad_guards_with_grad_mode x = torch ones requires_grad=True x_ref = x detach clone requires_grad_ True m = torch nn Linear m_compiled = torch compile m torch no_grad compile inference graph m_compiled x Test we should fail guards recompile training graph out_ref = m x_ref out = m_compiled x assertEqual out_ref out assertEqual out_ref requires_grad out requires_grad test_is_symbolic_tracing Ensure no graph break here fn x is_fx_tracing_test x x = torch randn ref = fn opt_fn = torch compile fn backend= eager fullgraph=True res = opt_fn assertTrue same ref res test_tokenization collections UserDict BatchEncoding UserDict Copied tokenization __init__ data super __init__ data __getattr__ item str try data item except KeyError e raise AttributeError e tokenization x encoding = BatchEncoding key x encoding key opt_fn = torch compile tokenization backend= eager x = torch rand ref = tokenization x res = opt_fn x assertTrue same ref res test_modules Foo torch nn Module __init__ - None super __init__ fc = torch nn Linear forward inp res = torch zeros _ modules res += fc inp res mod = Foo args = torch ones cnt = torch _dynamo testing CompileCounter opt_mod = torch compile mod backend=cnt fullgraph=True assertTrue same mod args opt_mod args assertEqual cnt op_count assertEqual cnt frame_count test_omegaconf_listconfig_iter obj = ListConfig x = torch zeros fn y = x i obj y += i y expected = fn actual = torch compile fn fullgraph=True backend= eager assertEqual actual expected test_user_defined_iter MyIter __init__ - None i = __iter__ __next__ i i += i raise StopIteration torch compile backend= eager fullgraph=True fn x i MyIter x += i x assertEqual fn torch zeros torch full test_stop_iteration_reconstruct torch compile backend= eager fullgraph=True fn x x sin StopIteration _ res = fn torch ones assertEqual str res str StopIteration test_tensor_data_kwarg https github com pytorch pytorch issues f torch tensor data= - cnt = torch _dynamo testing CompileCounter opt_fn = torch compile f backend=cnt fullgraph=True assertTrue same f opt_fn assertEqual cnt frame_count test_for_loop_graph_break inner x torch sin x fn x _ range inner x torch _dynamo graph_break x cnt = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnt x = torch randn opt_fn x assertEqual cnt frame_count assertEqual cnt op_count test_for_loop_graph_break_before Checks backedge calculated correctly inner x torch sin x fn x torch _dynamo graph_break _ range inner x x cnt = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnt x = torch randn opt_fn x assertEqual cnt frame_count assertEqual cnt op_count test_avoid_dupe_specialization f x y x + y opt_f = torch compile f backend= aot_eager b True False x = torch randn requires_grad=b y = torch randn requires_grad=b assertEqual f x x opt_f x x assertEqual f x y opt_f x y test_validate_model_kwargs cnt = CompileCounter f b torch sin + torch cos b torch compile backend=cnt fullgraph=True f kwargs _validate_model_kwargs f kwargs f kwargs x = torch randn y = torch randn assertEqual f a=x b=y f x y assertEqual cnt frame_count assertEqual cnt op_count test_swin_base_tensor_attr Foo torch nn Module __init__ - None super __init__ NB parameter buffer t = torch randn forward x x + torch cat t t mod = Foo opt_mod = torch compile mod backend= eager args = torch randn assertTrue same_two_models mod opt_mod args opt_mod args test_pointless_graph_removal cnt = torch _dynamo testing CompileCounter torch compile backend=cnt fn x torch no_grad torch _dynamo graph_break x + fn torch randn assertEqual cnt frame_count assertEqual cnt op_count test_output_aliases_intermediate f x intermediate = x mul intermediate view - intermediate opt_f = torch compile f backend= aot_eager b True False x = torch randn requires_grad=b out = f x out_test = opt_f x assertEqual out out_test assertEqual out out_test assertEqual out requires_grad out_test requires_grad assertEqual out requires_grad out_test requires_grad test aliasing relationship outputs preserved out mul_ out_test mul_ assertEqual out out_test assertEqual out out_test test_while_loop_graph_break Repro tacotron cache_size_recompilation inner x torch sin x fn x i = while i x = inner x i -= torch _dynamo graph_break x cnt = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnt x = torch randn opt_fn x assertEqual cnt frame_count assertEqual cnt op_count test_nested_while_loop_graph_break inner_loop x i = while i i -= x += torch _dynamo graph_break x inner x inner_loop x torch sin x fn x i = while i x = inner x i -= torch _dynamo graph_break x cnt = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnt x = torch randn opt_fn x assertEqual cnt frame_count assertEqual cnt op_count test_while_loop_graph_break_inside_call_function Repro huggingface graph break inside loop ` get_parameter_dtype ` Skip only inner frame has loop contains graph break inner x _ range x += torch _dynamo graph_break x fn x x += inner x x += x cnt = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnt x = torch randn opt_fn x assertEqual cnt frame_count assertEqual cnt op_count test_exception_in_dynamo_handling hit_handler = False See https github com pytorch pytorch pull contextlib contextmanager ctx try yield except RuntimeError nonlocal hit_handler hit_handler = True torch compile backend= eager f ctx h h raise RuntimeError boof Should error f assertTrue hit_handler test_generator_dealloc See https github com pytorch pytorch pull NB yes intentional list containing generator generator_box = x x counter = torch _dynamo testing CompileCounter g x x + TODO This test pretty delicate To test s actually doing anything rebuild eval_frame c #define TORCHDYNAMO_DEBUG then look logs TRACE _custom_eval_frame begin genexpr test_repros py - TRACE _custom_eval_frame throw genexpr This means we re actually hitting relevant codepath NB Make sure we don t actually Dynamo frame we do Dynamo frame Dynamo actually DOES understand list clear will arrange generator deallocation happen when eval frame handler disabled which will prevent bug happening we specifically want trigger generator deallocation WHILE dynamo eval frame handler active will cause generator become exhausted trigger throw_flag == TRUE case torch _dynamo disable recursive=False f x generator_box clear g x assertNoUnraisable lambda torch compile f backend=counter torch randn Make sure x + captured previous incorrect implementation fix would have disabled eval frame callback which means g wouldn t get traced assertEqual counter op_count test_error_return_without_exception_set https github com pytorch pytorch issues torch compile f _generator_type = type _ _ assertNoUnraisable f common_merge_criteria_processor_list list_cls fullgraph cnt = CompileCounter torch compile backend=cnt fullgraph=fullgraph f x left right combined = _merge_criteria_processor_list left right combined x l = list_cls torch nn ReLU torch nn Sigmoid l = list_cls input = torch randn result = f input l l assertEqual result l input assertEqual cnt frame_count assertEqual cnt op_count cnt clear l = list_cls torch nn SiLU expected = l l input result = f input l l assertEqual len l assertEqual result expected assertEqual cnt frame_count assertEqual cnt op_count test_merge_criteria_processor_list common_merge_criteria_processor_list CustomList False test_merge_criteria_processor_list common_merge_criteria_processor_list CustomList True test_restricted_list_subclass cnt = CompileCounter torch compile backend=cnt fullgraph=True fn b l = CustomList l extend True l append l extend b l pop l append l length_times_ sum l x = torch randn y = torch randn assertEqual fn x y x + y + assertEqual cnt op_count test_restricted_list_subclass cnt = CompileCounter torch compile backend=cnt fullgraph=True fn b l = CustomList + l = CustomList b + l extend l l x = torch randn y = torch randn z = fn x y assertEqual type z CustomList assertEqual len z assertEqual z length_times_ assertEqual list z x + y + test_restricted_list_subclass cnt = CompileCounter torch compile backend=cnt fullgraph=True fn CustomList b CustomList extend b append_twice b + append b + b x = torch randn y = torch randn l = CustomList x y assertIs fn l l l assertEqual len l assertIs l x assertIs l y assertIs l x assertIs l y assertEqual l x + assertIs l l assertEqual l y + test_rewrite_assert_with_msg f x b = x sin assert x == First dim need x cos + b args = torch Tensor cnt = torch _dynamo testing CompileCounter opt_f = torch compile f backend=cnt fullgraph=True assertTrue same f args opt_f args assertEqual cnt op_count assertEqual cnt frame_count exported _ = torch _dynamo export f torch Tensor assertTrue same exported args f args test_list_aliasing cnt = CompileCounter torch compile backend=cnt fullgraph=True fn append torch sin x = torch randn l = x assertIs fn l l assertEqual len l assertIs l x assertEqual l torch sin x assertEqual cnt frame_count assertEqual cnt op_count test_not_rewrite_assert_for_other_errors f x b = x sin x sum = raise ValueError input sum needs x cos + b args = torch Tensor opt_fn = torch compile f backend= eager assertRaisesRegex ValueError input sum needs opt_fn args test_rewrite_assert_dont_change_bytecode fn x torch no_grad assert x max f invalid max x max x = torch sin x x x = torch ones opt_fn = torch compile fn backend= eager assertTrue same fn x opt_fn x test_rewrite_assert_without_msg f x b = x sin assert x == x cos + b args = torch Tensor exported _ = torch _dynamo export f torch Tensor assertTrue same exported args f args assertRaisesRegex RuntimeError assertion error exported torch Tensor test_rewrite_assert_with_non_string_msg f x b = x sin assert x == f Error x x size x cos + b torch _dynamo utils counters clear args = torch Tensor opt_f = torch compile f backend= eager assertRaisesRegex AssertionError torch Size opt_f args gb cnt torch _dynamo utils counters graph_break items assert non-string message gb assertEqual cnt break graph break found assertTrue False test_rewrite_assert_noop f x b = x sin assert True assert x dtype == torch float x cos + b args = torch Tensor exported _ = torch _dynamo export f torch Tensor assertTrue same exported args f args cnt = torch _dynamo testing CompileCounter opt_f = torch compile f backend=cnt fullgraph=True assertTrue same f args opt_f args torch _assert shouldn t graph assertEqual cnt op_count assertEqual cnt frame_count exported _ = torch _dynamo export f torch Tensor assertTrue same exported args f args test_size_typematch f x y isinstance x torch Size y + y + y = torch zeros x = torch Size x = cnt = torch _dynamo testing CompileCounter opt_f = torch compile f backend=cnt fullgraph=True assertTrue same f x y opt_f x y assertTrue same f x y opt_f x y assertEqual cnt frame_count test_hf_classinstantier hf activations py ClassInstantier collections OrderedDict __getitem__ key content = super __getitem__ key cls kwargs = content isinstance content tuple content cls kwargs ACT CLS = ClassInstantier relu nn ReLU inplace False tanh nn Tanh torch compile fullgraph=True backend= eager f x act ACT CLS act x y = torch randn assertTrue same f y tanh torch tanh y assertTrue same f y relu torch relu y test_ephemeral_module hf activations py ReLUSquaredActivation nn Module forward input relu_applied = torch nn functional relu input squared = torch square relu_applied squared torch compile fullgraph=True backend= eager f x x = x + x = ReLUSquaredActivation x x = x + x y = torch randn assertTrue same f y ReLUSquaredActivation y + + test_inplace_unsqueeze_input backend gm example_inputs assertEqual example_inputs - size torch Size gm torch compile backend=backend fn x x unsqueeze_ x + inputs = torch randn assertEqual fn inputs size torch Size assertEqual inputs size torch Size test_batchnorm_e e Repro torch nn Module __init__ - None super __init__ bn = torch nn BatchNorm d eps= e- momentum= affine=True track_running_stats=True conv = torch nn Conv d kernel_size= stride= padding= bias=False forward x x = bn x x = conv x out = torch nn functional relu x out torch manual_seed m_ref = Repro m_test = deepcopy m_ref torch compile backend= aot_eager_decomp_partition compiled_fn x m_test x x_ref = torch randn requires_grad=True x_test = x_ref clone Loop multiple times each iteration running_mean var batchnorm will update which changes output next iteration _ range ref = m_ref x_ref res = compiled_fn x_test assertTrue same ref res r ref r requires_grad r sum backward r res r requires_grad r sum backward param_ref param_test zip m_ref parameters m_test parameters assertTrue same param_ref param_test Assert running_mean var buffer_ref buffer_test zip m_ref buffers m_test buffers assertTrue same buffer_ref buffer_test torch _dynamo config patch assume_static_by_default False test_dynamic_shapes_right_side f x torch ones x shape inp = torch randn gm _ = torch _dynamo export f aten_graph=True torch randn assertEqual gm inp shape f inp shape torch _dynamo config patch specialize_int False test_maybe_multiply_symint https github com pytorch pytorch issues torch _functorch aot_autograd aot_module_simplified my_aot_compiler gm example_inputs my_compiler gm example_inputs gm forward Invoke AOTAutograd aot_module_simplified gm example_inputs fw_compiler=my_compiler my_example t t d out = torch add t t alpha=d out compiled_fn = torch compile backend=my_aot_compiler dynamic=True my_example t = torch arange dtype=torch float requires_grad_ True t = torch arange dtype=torch float requires_grad_ True ra = compiled_fn t t assertEqual ra torch tensor ra = compiled_fn t t assertEqual ra torch tensor test_build_map_unpack_with_call forward_with_cond_scale x t cond_scale self_cond other other x sin + t + cond_scale + self_cond + other + other torch compile backend= eager fullgraph=True fn x d = dict other = d = dict other = text_cond = d d forward_with_cond_scale x cond_scale= self_cond= text_cond assertTrue same fn torch ones torch ones sin + torch _dynamo config patch verbose=True test_graph_break_unsupported_fake counter = torch _dynamo testing CompileCounter torch compile backend=counter f x torch ops test_sample foo x + + f torch randn assertEqual counter op_count assertEqual counter frame_count test_delattr MyObj __init__ b = b = b torch compile backend= eager fullgraph=True fn x obj del obj obj c = x + del obj c tmp = MyObj x + x + del tmp b hasattr obj x + tmp x = torch zeros obj = MyObj x x obj = fn x obj assertFalse hasattr obj assertFalse hasattr obj c assertFalse hasattr obj b assertEqual obj b item assertEqual obj item test_delattr_return MyObject __init__ val val = val deletion_attempted = False __delattr__ attr attr == val deletion_attempted = True super __delattr__ attr torch compile fullgraph=True backend= eager test_delattr input_tensor instance_a = MyObject instance_b = MyObject del instance_a val del instance_b val exists_a = hasattr instance_a val exists_b = hasattr instance_b val deletion_attempted_a = instance_a deletion_attempted deletion_attempted_b = instance_b deletion_attempted input_tensor + exists_a exists_b deletion_attempted_a deletion_attempted_b result = test_delattr torch ones assertEqual result torch tensor assertEqual result True True True True test_delattr_raises MyObj __init__ b = b = b torch compile backend= eager fn x obj del obj x = x + obj will raise x x = torch zeros obj = MyObj x x assertRaises AttributeError lambda fn x obj test_delsubscr torch compile backend= eager fn x del x y = x b + y x = torch tensor b torch tensor result = fn x assertFalse hasattr x assertEqual result item test_delsubscr_raises torch compile backend= eager fn x del x y = x + should raise KeyError y x = torch tensor b torch tensor assertRaises KeyError lambda fn x test_attached_attribute_in_dir MyModule torch nn Module __init__ - None super __init__ linear = torch nn Linear relu = torch nn ReLU forward x relu linear x mod = torch compile MyModule backend= eager mod is_compiled = True assertTrue is_compiled dir mod torch _dynamo config patch automatic_dynamic_shapes False test_dynamic_shapes_implicit_guard f x y = x x size x shape torch sum y y shape y cnt = torch _dynamo testing CompileCounter opt_fn = torch compile f backend=cnt fullgraph=True opt_fn torch randn assertEqual cnt frame_count test_dalle _maybe normalize x x cos torch compile backend= eager fullgraph=True fn x normalize_img lowres_cond_img = x sin lowres_cond_img = maybe normalize_img lowres_cond_img lowres_cond_img assertEqual fn torch ones normalize torch ones sin cos test_functools_wraps cool_name x x sin torch compile backend= eager fullgraph=True fn x y = x cos functools wraps cool_name uncool_name cool_name y uncool_name result = fn torch ones assertEqual result __name__ cool_name assertEqual result torch ones cos sin test_dynamic_shapes_float_guard f x torch nn functional dropout x x shape cnt = torch _dynamo testing CompileCounter opt_fn = torch compile f backend=cnt fullgraph=True opt_fn torch randn assertEqual cnt frame_count torch _dynamo config patch capture_scalar_outputs=True test_tensor_item f x y val = y item x sum + val gm _ = torch _dynamo export f aten_graph=True torch zeros torch tensor assertEqual f torch zeros torch tensor gm torch zeros torch tensor assertEqual f torch zeros torch tensor gm torch zeros torch tensor test_dataclass_init_with_default_factory_with_inputs dataclasses dataclass DClass sharding_contexts Any = dataclasses field default_factory=list int = fn x inp_list d = DClass inp_list d sharding_contexts append x sin + d d x = torch randn inp_list = inp_list = inp_list = ref = fn x inp_list ref = fn x inp_list ref = fn x inp_list opt_fn = torch compile fn fullgraph=True opt_ret = opt_fn x inp_list opt_ret = opt_fn x inp_list opt_ret = opt_fn x inp_list assertEqual ref sharding_contexts opt_ret sharding_contexts assertEqual ref sharding_contexts opt_ret sharding_contexts assertEqual ref sharding_contexts opt_ret sharding_contexts test_list_index i list_type enumerate list tuple torch Size collections deque namedtuple FourElems one two three four defaults= torch _dynamo reset index f t i == namedtuple xs = list_type xs = list_type res = xs index index t + res res = torch compile f backend= eager fullgraph=True torch zeros assertEqual res torch tensor test_list_index_not_found f t xs = bar foo baz buzz res = xs index non-existent t + res Raising ValueError item found unsupported assertRaises torch _dynamo exc Unsupported torch compile f backend= eager fullgraph=True torch zeros test_list_index_tensor_unsupported index f t xs = torch tensor i i range res = xs index torch tensor index t + res assertRaisesRegex torch _dynamo exc Unsupported Data-dependent branching torch compile f backend= eager fullgraph=True torch zeros test_hf_xsoftmax_inference fn input mask XSoftmax apply input + mask + fn_opt = torch compile fn backend= eager fullgraph=True inputs = torch randn torch randn expected = fn inputs actual = fn_opt inputs assertTrue same actual expected mock patch torch _dynamo config guard_nn_modules True test_hf_xsoftmax_training torch _dynamo utils counters counters clear fn input mask XSoftmax apply input mask cnt = torch _dynamo testing CompileCounter fn_opt = torch compile fn backend=cnt fullgraph=False torch manual_seed inputs = torch randn requires_grad=True torch randn torch manual_seed inputs = torch randn requires_grad=True torch randn expected = fn inputs actual = fn_opt inputs assertTrue same actual expected assertEqual cnt op_count assertEqual cnt frame_count cnt clear counters clear expected sum backward actual sum backward assertTrue same inputs grad inputs grad currently we don t capture backwards frame assertEqual cnt frame_count assertEqual cnt op_count assertEqual dict counters frames assertEqual dict counters graph_break test_autograd_function_graph_break MySin torch autograd Function staticmethod forward ctx x torch _dynamo graph_break ctx save_for_backward x x sin staticmethod backward ctx gx x = ctx saved_tensors gx x cos x = torch randn requires_grad=True torch compile backend= eager fn x MySin apply x y = fn x assertEqual y x sin gx = torch autograd grad y x assertEqual gx x cos test_jit_trace_errors torch compile backend= eager dynamic=True f x x + assertRaises RuntimeError torch jit trace f torch randn torch _dynamo config patch assume_static_by_default False test_tensor_split f x torch split x x shape dim= gm _ = torch _dynamo export f aten_graph=True torch zeros assertEqual f torch ones gm torch ones skipIfWindows msg= TODO xuhancn fix AssertionError tensor test_optim_state_references_cleared model = torch nn Linear bias=False x = torch ones state_ref = optimizer = torch optim Adadelta model parameters lr= opt_step optimizer step compiled_opt_step = torch compile opt_step backend= eager compiled_model_step x optimizer zero_grad y = model x torch sum y backward compiled_opt_step compiled_model_step x Picked square_avg arbitrarily check optimizer state tensors deallocated state_ref = weakref ref optimizer state optimizer param_groups params square_avg optimizer = None assertIsNone state_ref test_grad_references_cleared model = torch nn Linear bias=False x = torch ones optimizer = torch optim Adadelta model parameters lr= opt_step optimizer step compiled_opt_step = torch compile opt_step backend= eager compiled_model_step x optimizer zero_grad True y = model x torch sum y backward compiled_opt_step compiled_model_step x param_grad_ref = weakref ref next iter model parameters grad optimizer zero_grad True assertIsNone param_grad_ref test_batch_encoding_clone_inputs BatchEncoding dict Copied test_tokenization __init__ data super __init__ data __getattr__ item str try data item except KeyError e raise AttributeError e encoding = BatchEncoding key torch rand cloned_encoding = torch _dynamo utils clone_inputs encoding assertTrue type cloned_encoding dict test_iadd_graph_break fn x = x = torch sin x += x x = torch randn ref = fn x opt_fn = torch compile fn backend= eager fullgraph=True res = opt_fn x assertTrue same ref res test_odict_get_item_index_name d = float torch float np float torch float torch compile backend= eager f x y y torch zeros dtype=d y torch zeros dtype=d y f torch zeros float np float test_dedup_global torch compile f _GLOBAL_CPU_TENSOR + _GLOBAL_CPU_TENSOR assertEqual f _GLOBAL_CPU_TENSOR + _GLOBAL_CPU_TENSOR test_randint_out_dynamic randint_fn high size out torch randint high size out=out opt_model = torch compile randint_fn out = torch empty dtype=torch int opt_model out out = torch empty dtype=torch int opt_model out requires_cuda serialTest test_mem_leak_guards gn x x x x MyMod torch nn Module __init__ super __init__ torch _dynamo disable recursive=False forward running_x This line creates temp tensor which should leaked running_x = torch sin running_x x = running_x This creates TENSOR_ALIASING guard x = gn running_x running_x This creates NO_TENSOR_ALIASING guard which leaking memory x = gn running_x x x mod = MyMod cuda fn = torch compile mod backend= eager x = torch randn device= cuda torch cuda reset_peak_memory_stats fn x peak_mem = torch cuda max_memory_allocated _ range fn x peak_mem = torch cuda max_memory_allocated assertTrue peak_mem == peak_mem requires_cuda test_guard_default_device try torch set_default_device cuda counter = torch _dynamo testing CompileCounter torch compile backend=counter f x = torch randn x assertEqual f device type cuda assertEqual counter frame_count torch set_default_device cpu assertEqual f device type cpu assertEqual counter frame_count finally torch set_default_device None test_list_self_reference Issue - https github com pytorch pytorch issues root = root = root root None None torch compile fullgraph=False backend= eager test_bug root test_bug test_hf_bigbird_unsqueeze torch_bmm_nd inp_ inp_ ndim=None torch _dynamo graph_break torch bmm inp inp fn inp inp inp inp c = torch_bmm_nd inp inp unsqueeze_ = b = torch_bmm_nd inp inp b unsqueeze_ l = + b out = torch cat b c dim= out l inp = torch rand inp = torch rand inp = torch rand inp = torch rand c = torch rand cnt = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnt opt_fn inp inp inp inp c assertEqual cnt frame_count test_torch_variable_type torchvision check_type obj types_or_checks type_or_check types_or_checks isinstance obj type_or_check isinstance type_or_check type type_or_check obj True False opt_check_type = torch compile check_type backend= eager ref = check_type torch randn torch Tensor res = opt_check_type torch randn torch Tensor assertEqual ref res Test https github com pytorch pytorch issues torch _dynamo config patch assume_static_by_default False test_inference_mode_dynamic_shapes Repro torch nn Module __init__ - None super __init__ forward param z = torch matmul param param z model = Repro Need d tensor actually cause error we go down path C++ matmul decomp calls sizes inp = torch randn requires_grad=True model = torch compile model backend= aot_eager dynamic=True torch inference_mode model inp test_kwargs_out_list_variable Repro torch nn Module __init__ - None super __init__ forward param z = torch frexp param z model = Repro params = input torch tensor params out = torch empty dtype=torch float mantissa torch empty dtype=torch int exponent model = torch compile model backend= eager mantissa exponent = model params ref_mantissa = torch tensor ref_exponent = torch tensor dtype=torch int assertEqual ref_mantissa mantissa assertEqual ref_exponent exponent torch _dynamo config patch capture_scalar_outputs=True test_split_with_sizes_aot_autograd fn result split_sizes rs = torch ops aten split_with_sizes result split_sizes tolist rs example_inputs = torch randn requires_grad=True torch tensor actual = torch compile fn fullgraph=True backend= aot_eager example_inputs expected = fn example_inputs assertEqual actual expected test_unspecialized_nn_module_with_torch_variable_attribute In case fn = something should TorchVariable When s TorchVariable dynamo tries trace through fails This makes sure fn handled TorchVariable UserModule torch nn Module torchdynamo_force_dynamic = True forced UnspecializedNNModule __init__ fn super __init__ fn = fn forward inp fn inp inputs = input torch randn uniform_ target torch randn uniform_ reduction mean mod = UserModule torch nn functional binary_cross_entropy ref = mod inputs res = torch compile mod backend= eager fullgraph=True inputs assertEqual ref res test_string_format s = temp i torch compile backend= eager fullgraph=True fn x s format i= == temp torch sin x torch cos x x = torch randn assertEqual fn x torch sin x unittest skip Fails incorrect result fullgraph constraints test_int_format fn num int format num b opt_fn = torch compile fn backend= eager fullgraph=True dynamic=False assertEqual fn opt_fn Repro torch _dynamo exc InternalTorchDynamoError NoneType object has no attribute guards due bad empty list handling test_empty_list_contains_with_jump fn x l x l x cos x sin counter = CompileCounter torch compile fn backend=counter torch randn assertEqual counter frame_count test_get_type_hints Foo pass fn x typing get_type_hints Foo include_extras=True torch sin x x = torch randn ref = fn x opt_fn = torch compile fn backend= eager fullgraph=True res = opt_fn x assertEqual ref res test_graph_break_on_jit_isinstance torch compile backend= eager fn x torch jit isinstance x typing List str noqa UP x x opt_fn = torch compile fn backend= eager x = torch rand assertTrue same fn x opt_fn x test_graph_break_on_jit_isinstance_pep torch compile backend= eager fn x torch jit isinstance x list str x x opt_fn = torch compile fn backend= eager x = torch rand assertTrue same fn x opt_fn x test_add_sub_alpha_out inp = torch randn other = alpha = op torch add torch sub out = torch zeros compile_out = torch zeros op inp other alpha=alpha out=out compiled_fn = torch compile op dynamic=True compiled_fn inp other alpha=alpha out=compile_out assertTrue same out compile_out test_negative_shape_guard fn x x size = x cos x sin counter = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=counter dynamic=True x = torch ones x = torch ones assertEqual fn x opt_fn x assertEqual fn x opt_fn x assertEqual counter frame_count torch _dynamo config patch capture_scalar_outputs=True test_deferred_runtime_asserts torch compile fullgraph=True f x y = x item torch _check y = y = x x f torch tensor assertRaises RuntimeError lambda f torch tensor - test_addr_alpha_beta_out inp = torch randn vec = torch randn vec = torch randn alpha = beta = out = torch zeros compile_out = torch zeros torch addr inp vec vec alpha=alpha beta=beta out=out compiled_fn = torch compile torch addr dynamic=True compiled_fn inp vec vec alpha=alpha beta=beta out=compile_out assertTrue same out compile_out test_setattr_requires_grad_graph_breaks fn x z = x + x requires_grad = True y = x z y backend count eager aot_eager backend == count backend = CompileCounter opt_fn = torch compile fn backend=backend eager = torch zeros compiled = eager clone out_eager = fn eager out_opt = opt_fn compiled assertEqual out_eager out_opt out_eager sum backward out_opt sum backward assertEqual eager compiled isinstance backend CompileCounter assertEqual backend frame_count graph breaks test_dynamic_shapes_double_not_equal https github com pytorch pytorch issues fn x x size = x cos x sin opt_fn = torch compile fn backend= eager x = torch ones x = torch ones assertEqual fn x opt_fn x assertEqual fn x opt_fn x test_inductor_no_recursionerror_on_for_loops forward x _ range x = x x assertTrue same torch compile forward torch tensor torch tensor test_user_defined_object_callable https github com pytorch pytorch issues MyCallable __call__ x x + fn x Create graph - will have source MyCallable x fn_opt = torch compile fn backend= eager fullgraph=True assertEqual fn_opt torch zeros fn torch zeros torch _dynamo config patch log_compilation_metrics=True test_many_views_with_mutation When symbolic storage offsets added tensors_definitely_do_not_overlap began adding shape guards - quadratic amount relative number inputs Test configuration test reasonable number guards added Note when dynamic shapes turned test fails we still get quadratic guards fn x x relu_ torch cat x sum AMT = src = torch rand AMT + x = src as_strided + i i range AMT torch _dynamo reset torch _dynamo utils clear_compilation_metrics torch compile fn backend= aot_eager x all_metrics = torch _dynamo utils get_compilation_metrics total_guards = sum metric guard_count metric all_metrics assertLess total_guards AMT total_shape_env_guards = sum metric shape_env_guard_count metric all_metrics assertLess total_shape_env_guards AMT https github com pytorch pytorch issues test_subclass_graph_output_repro torch _dynamo allow_in_graph to_subclass x TwoTensor x clone x clone f x tmp_subclass = to_subclass x tmp_subclass view - x = torch ones out_ref = f x out_test = torch compile f backend= aot_eager x assertEqual out_ref out_test test_numpy_tobytes_no_error fn x x += z = x tobytes x += z cnt = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnt opt_arg arg = np array np array assertEqual opt_fn opt_arg fn arg assertEqual cnt frame_count test_numpy_not_ndarray_recompiles torch fn x=None x None x = np ones isinstance x int x = np ones isinstance x str x = np ones x cnt = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnt x = np zeros assertEqual opt_fn x fn x assertEqual cnt frame_count assertEqual opt_fn fn assertEqual cnt frame_count assertEqual opt_fn fn assertEqual cnt frame_count assertEqual opt_fn fn assertEqual cnt frame_count parametrize backend eager aot_eager inductor parametrize func_name func func func test_tensor_set_data backend func_name https github com pytorch pytorch issues func x y x data = y x add_ x func x y x data = y y data = torch zeros x func x y z = x x data = y y data = torch zeros torch tensor x z funcs = func func func func func func func = funcs func_name backend = eager func func add_ working w aot_autograd torch _dynamo reset cnt = torch _dynamo testing CompileCounterWithBackend backend compiled_fn = torch compile func backend=cnt fullgraph=True requires_grad = func func _ range Inputs eager_a = torch ones requires_grad=requires_grad compiled_a = torch ones requires_grad=requires_grad eager_b = torch ones requires_grad=requires_grad compiled_b = torch ones requires_grad=requires_grad Eager out_eager = func eager_a eager_b Compiled out_compiled = compiled_fn compiled_a compiled_b assertEqual eager_a compiled_a assertEqual eager_b compiled_b assertTrue torch equal out_eager out_compiled func hits leaf Variable requires grad being used in-place operation requires_grad bwd_inp_eager = torch randn bwd_inp_compiled = torch clone bwd_inp_eager eager_a backward bwd_inp_eager compiled_a backward bwd_inp_compiled assertEqual eager_a grad compiled_a grad Prove guarding works - we run compiled_fn times frame_count should stay assertEqual cnt frame_count test_tensor_set_data_mismatched_dtype func x y x data = y dtype=torch bfloat x = torch tensor dtype=torch float x = torch tensor dtype=torch float y = torch tensor dtype=torch float y = torch tensor dtype=torch float func x y torch compile func backend= eager x y assertEqual x x assertEqual x data x data assertEqual y y test_user_ctor_ctx_manager UserCtxManager __enter__ __exit__ exc_type exc_val exc_tb pass fn x y ucm = UserCtxManager noqa F x x cnt = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnt fullgraph=True x = torch rand opt_fn x x assertExpectedInline cnt frame_count torch _dynamo config patch capture_scalar_outputs=True test_unbacked_arange_in_bounds see https github com pytorch pytorch issues PaddingNet nn Module __init__ - None super __init__ forward lengths max_seq_len = lengths max item row_vector = torch arange max_seq_len matrix = torch unsqueeze lengths dim=- mask = row_vector matrix mask = mask type torch float mask_ d_btd = mask None mask_ d_btd model = PaddingNet lengths = torch tensor dtype=torch int cnt = torch _dynamo testing CompileCounter opt_fn = torch compile model backend=cnt fullgraph=True opt_fn lengths assertEqual cnt frame_count test_overlapping_inputs_with_dynamic_shapes_error torch compile backend= aot_eager fn b c d e f mul_ b mul_ c mul_ d mul_ e mul_ f mul_ base = torch ones = base b = base c = base d = base e = base f = base f = base fn b c d e f assertRaisesRegex AssertionError being compiled dynamic shapes fn b c d e f test_user_ctor_ctx_manager_custom_init UserCtxManager __init__ x x = __enter__ __exit__ exc_type exc_val exc_tb pass fn x y ucm = UserCtxManager y noqa F x y cnt = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnt fullgraph=True x = torch rand assertEqual opt_fn x fn x assertExpectedInline cnt frame_count test_user_ctor_ctx_manager_custom_init_graph_break counter = UserCtxManager __init__ k k += __enter__ __exit__ exc_type exc_val exc_tb pass fn x counter x = x x ucm = UserCtxManager counter noqa F x x cnt = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnt x = torch rand assertEqual opt_fn x counter fn x counter assertEqual counter _ range opt_fn x counter assertEqual counter torch _dynamo config assume_static_by_default assertExpectedInline cnt frame_count assertExpectedInline cnt frame_count test_many_overlapping_inputs_does_not_explode_guards torch _dynamo backends common aot_autograd Before num_shape_guards = None num_aot_guards = None num_compiles = guard_count_backend gm args nonlocal num_shape_guards nonlocal num_aot_guards nonlocal num_compiles num_shape_guards = len torch _guards TracingContext try_get fake_mode shape_env guards num_aot_guards = len torch _guards TracingContext try_get guards_context aotautograd_guards num_compiles += gm aot_guard_counter = aot_autograd fw_compiler=guard_count_backend torch compile backend=aot_guard_counter dynamic=True f args args add_ x = torch ones requires_grad=True args = x split torch no_grad f args In example there guards roughly tensors ^ because every pair aliased inputs needs guard assertTrue num_aot_guards But there no dynamic shape guards assertEqual num_shape_guards don t recompile torch no_grad f args assertEqual num_compiles test_issue BoundingBox DIndex IntEnum _X = _Y = _HEADING = _LENGTH = _WIDTH = classmethod size cls classmethod property X cls cls _X classmethod property Y cls cls _Y classmethod property HEADING cls cls _HEADING classmethod property LENGTH cls cls _LENGTH classmethod property WIDTH cls cls _WIDTH classmethod property POINT cls assumes X Y have subsequent indices slice cls _X cls _Y + classmethod property STATE_SE cls assumes X Y HEADING have subsequent indices slice cls _X cls _HEADING + SimpleModel nn Module __init__ super __init__ _mlp_states = nn Sequential nn Linear nn ReLU nn Linear BoundingBox DIndex size forward x agent_states = _mlp_states x agent_states BoundingBox DIndex POINT = agent_states BoundingBox DIndex POINT tanh agent_states BoundingBox DIndex HEADING = agent_states BoundingBox DIndex HEADING tanh torch pi agent_states model = SimpleModel eval input_tensor = torch randn dtype=torch float opt = torch compile model eval backend= eager fullgraph=True actual = opt input_tensor try expected = model input_tensor except Exception e raise unittest SkipTest eager failed requires Python = e assertEqual actual expected test_invalid_seq_unpack myfn arg b = arg noqa F fn myfn try torch compile fn except ValueError pass fail expected exception test_udf_classes_reconstruction fn x o = T o x + x opt_fn = torch compile fn backend= eager T = IncByOne x = torch randn assertEqual fn x opt_fn x This should recompile T = IncByTwo assertEqual fn x opt_fn x test_contains_range_constprop fn x dynamo should const prop False range x + x + opt_fn = torch compile fn backend= eager x = torch zeros assertEqual fn x opt_fn x https github com pytorch pytorch issues test_as_strided_on_base_with_mutation_works foo f = as_strided f add_ = torch randn a_ref = clone out_ref = foo a_ref f_compiled = torch compile foo backend= aot_eager out = f_compiled assertEqual out_ref out assertEqual a_ref https github com pytorch pytorch issues test_as_strided_on_existing_view_banned foo e = diagonal f = e as_strided f add_ = torch randn a_ref = clone foo a_ref f_compiled = torch compile foo backend= aot_eager assertRaisesRegex RuntimeError encountered mutation view chain length where view as_strided f_compiled See https github com pytorch pytorch issues test_preserve_stride_with_clone - None A = torch rand device= cuda torch cuda is_available cpu B = torch rand device= cuda torch cuda is_available cpu fn src torch Tensor count torch Tensor - tuple tuple int tuple int Q R = torch linalg qr src rhs = torch ones Q shape device=src device = torch linalg solve_triangular R Q T rhs upper=True cloned = clone memory_format=torch preserve_format stride cloned stride a_stride cloned_stride = fn A torch zeros assertEqual a_stride cloned_stride f Strides should match eager a_stride against cloned_stride compiled_a_stride compiled_cloned_stride = torch compile fn backend= eager B torch zeros assertEqual compiled_a_stride compiled_cloned_stride f Strides should match eager compiled_a_stride against compiled_cloned_stride Extension https github com pytorch pytorch issues non memory dense case test_clone_not_memory_dense foo - torch Tensor x = torch randn t y = x clone y y = foo assertEqual y stride Reference eager implementation should have stride y = torch compile foo backend= eager assertEqual y stride Compile eager backend should have stride y = torch compile foo backend= aot_eager assertEqual y stride Compile aot_eager backend should have stride y = torch compile foo backend= inductor assertEqual y stride Compile inductor backend should have stride https github com pytorch pytorch issues unittest expectedFailure test_lru_cache_tracing functools lru_cache counter = lru_cache cached_fn x nonlocal counter counter += x + compiled_fn = torch compile cached_fn backend= eager t = torch randn result = compiled_fn t assertEqual counter result = compiled_fn t assertEqual counter assertEqual result result test_dont_aggressively_write_assert record_graph = torch _dynamo testing EagerAndRecordGraphs torch compile dynamic=True backend=record_graph f x assert x shape assert x sum assert x shape = assert x shape - x shape = x cos f torch ones graph = record_graph graphs It bit annoying we generate useless statements shape guards DCE should able remove them since t there no backed assert them The reason ok because dynamo will only skip assert statement instructions before assertExpectedInline str graph code strip \ forward s torch SymInt s torch SymInt L_x_ torch Tensor l_x_ = L_x_ getitem_ = l_x_ sum_ = getitem_ sum getitem_ = None gt_ = sum_ sum_ = None _assert_async = torch _assert_async gt_ assertion error gt_ = _assert_async = None cos = l_x_ cos l_x_ = None cos node graph graph nodes example_value node meta isinstance node meta example_value torch _subclasses fake_tensor FakeTensor shape_env = node meta example_value fake_mode shape_env lower_ranges = val lower val shape_env var_to_range values assertTrue lower_ranges == torch compile dynamic=True backend=record_graph f_fail x assert x shape We graph-break here so failure should eager assertRaisesRegex AssertionError f_fail torch ones test_detectron _instances_cat Instances __init__ image_size tuple int int kwargs Any _image_size = image_size _fields dict str Any = k v kwargs items set k v property image_size - tuple int int _image_size __setattr__ name str val Any - None name startswith _ super __setattr__ name val set name val __getattr__ name str - Any name == _fields name _fields raise AttributeError f Cannot find field name given Instances _fields name __len__ - int v _fields values use __len__ because len has int friendly tracing v __len__ raise NotImplementedError Empty Instances does support __len__ set name str value Any - None warnings catch_warnings record=True data_len = len value len _fields assert len == data_len f Adding field length data_len Instances length len _fields name = value get name str - Any _fields name staticmethod cat instance_lists list Instances - Instances assert all isinstance i Instances i instance_lists assert len instance_lists len instance_lists == instance_lists image_size = instance_lists image_size isinstance image_size torch Tensor could tensor tracing i instance_lists assert i image_size == image_size ret = Instances image_size k instance_lists _fields keys values = i get k i instance_lists v = values isinstance v torch Tensor values = torch cat values dim= isinstance v list values = list itertools chain values hasattr type v cat values = type v cat values raise ValueError f Unsupported type type v concatenation ret set k values ret instances = Instances a=torch randn b=torch randn _ range torch compile backend= eager fullgraph=True fn instances instances cat instances actual = fn instances expected = instances cat instances assertEqual type actual type expected assertEqual actual __dict__ expected __dict__ test_weakref_construction fn x y x_weak = weakref ref x x_weak y x = torch randn y = torch randn ref = fn x y opt_fn = torch compile fn backend= eager fullgraph=True res = opt_fn x y assertEqual ref res test_weakref fn x_weak weight y x_weak None x_weak weight torch sin y torch cos y weight = torch randn y = torch randn x_weak = weakref ref weight ref = fn x_weak weight y opt_fn = torch compile fn backend= eager fullgraph=True res = opt_fn x_weak weight y assertEqual ref res https github com pytorch pytorch issues test_weakref_proxy DummyTrainer __init__ x foo = x DummyModel nn Module __init__ super __init__ trainer = None foo trainer foo x = torch randn model = DummyModel trainer = DummyTrainer x model trainer = weakref proxy trainer compiled_foo = torch compile model foo backend= eager fullgraph=True assertEqual compiled_foo x test_weakref_reconstruct fn x_weak weight y y = torch sin y referent = x_weak torch _dynamo graph_break referent weight torch sin y torch cos y weight = torch randn y = torch randn x_weak = weakref ref weight ref = fn x_weak weight y cnt = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnt res = opt_fn x_weak weight y assertEqual ref res assertEqual cnt frame_count test_return_weakref f t t = t wr = weakref ref t wr t ref_t = torch randn requires_grad=True ref_y = f ref_t t = ref_t detach clone requires_grad_ y = torch compile f backend= eager fullgraph=True t assertEqual ref_y y test_weakref_del fn x_weak y x = x_weak x None torch sin y torch cos y weight = torch randn x_weak = weakref ref weight y = torch randn opt_fn = torch compile fn backend= eager fullgraph=True ref = fn x_weak y res = opt_fn x_weak y assertEqual ref res del weight gc collect ref = fn x_weak y res = opt_fn x_weak y assertEqual ref res The programming model around weak references we DO NOT guarantee any behavior depends deallocation order We do guarantee eventual consistency after torch compile d function finished running including any graph breaks refcount semantics will match eager s skipIfWindows msg= TODO xuhancn fix AssertionError False true test_weakref_callback called = False callback ref nonlocal called called = True torch compiler is_compiling raise RuntimeError callback expected compiled weakref callbacks should called compiled region will compiled But exact place compiled code callback made undefined torch compile backend= eager fn x y = x + ref = weakref ref y callback torch _dynamo graph_break ref fn torch ones assertTrue called called = False callback ref nonlocal called called = True torch compiler is_compiling raise RuntimeError callback expected compiled weakref callbacks fire outside compiled region work torch compile backend= eager gn x y = x + ref = weakref ref y callback torch _dynamo graph_break y ref y _ = gn torch ones del y assertTrue called callback ref raise RuntimeError callback should called The callback will NOT called both weakref referrent deleted same compiled region graph breaks act like memory sync thus make things tricky - callback actually expected called This test does NOT mean behavior part weak ref programming model rather reminds us intentionally allowed weakref-Dynamo behavior torch compile backend= eager hn x y = x + _ = weakref ref y callback hn torch ones torch _functorch config patch recompute_views=True test_storage_resize_forward_full_graph TestModule torch nn Module __init__ - None super __init__ param = torch nn Parameter torch randn forward x param untyped_storage resize_ param numel param itemsize torch no_grad torch _foreach_copy_ param x out = torch matmul param param param untyped_storage resize_ out post_accumulate_grad_hook param param untyped_storage resize_ Beginning backward resize put data into param pre_backward_hook module grad - None module param untyped_storage resize_ param numel param itemsize torch no_grad simulates loading data into param allgather module param fill_ post_forward_hook module args output output register_hook functools partial pre_backward_hook module x = torch randn mod_ref = TestModule mod_test = deepcopy mod_ref Start param off zero storage size mimic fsdp mod_ref param untyped_storage resize_ mod_test param untyped_storage resize_ Resize storage beginning backward Free storage end backward mod_ref register_forward_hook post_forward_hook prepend=False mod_ref param register_post_accumulate_grad_hook post_accumulate_grad_hook mod_test register_forward_hook post_forward_hook prepend=False mod_test param register_post_accumulate_grad_hook post_accumulate_grad_hook mod_test = torch compile mod_test backend=aot_graph_capture_backend out_ref = mod_ref x out_test = mod_test x assertExpectedInline str fw_graph code strip \ forward primals_ primals_ _foreach_copy = torch ops aten _foreach_copy default primals_ primals_ primals_ = primals_ = None getitem = _foreach_copy _foreach_copy = None mm = torch ops aten mm default getitem getitem mm getitem assertEqual out_ref out_test test_super_in_staticmethod A staticmethod foo super __init__ fn obj obj foo obj = A try fn obj except Exception e orig_str = str e assertIn no arguments orig_str try torch compile backend= eager fn obj except Exception e compiled_str = str e assertEqual orig_str compiled_str test_super_staticmethod Parent staticmethod greet Child Parent staticmethod greet x x super Child Child greet child = Child fn x child greet x opt_fn = torch compile fn backend= eager fullgraph=True x = torch ones ref = fn x res = opt_fn x assertEqual ref res test_super_classmethod Parent classmethod greet cls cls == Parent cls == Child cls == GrandChild Child Parent greet x x super greet GrandChild Child pass grand_child = GrandChild fn x grand_child greet x opt_fn = torch compile fn backend= eager fullgraph=True x = torch ones ref = fn x res = opt_fn x assertEqual ref res test_super_classmethod_inheritance GrandParent classmethod greet cls x cls A x Parent GrandParent classmethod greet cls x super greet x Child Parent A = classmethod greet cls x super greet x child = Child fn x child greet x opt_fn = torch compile fn backend= eager fullgraph=True x = torch ones ref = fn x res = opt_fn x assertEqual ref res test_super_diamond A __init__ super __init__ = Nothing pass B Nothing A __init__ super __init__ b = run x b x fn x b = B b run x opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn ref = fn x res = opt_fn x assertEqual ref res test_vc_bumped_in_inference_graph torch compile f x x mul_ x = torch randn vc_before = x _version f x vc_after = x _version assertTrue vc_after vc_before test_nn_module_callable M nn Module forward x x sin f m callable m res = torch compile f fullgraph=True M assertTrue res test_stk_sdd_is_transposed _is_transposed x x is_contiguous x stride == x stride == x size SDD torch autograd Function staticmethod forward ctx lhs rhs ctx save_for_backward lhs rhs out = torch full_like lhs dtype=lhs dtype device=lhs device out staticmethod backward ctx dy saved_tensors = ctx saved_tensors lhs rhs = saved_tensors trans_a = _is_transposed lhs trans_b = _is_transposed rhs dlhs = None ctx needs_input_grad dlhs = torch full_like lhs trans_a drhs = None ctx needs_input_grad drhs = torch full_like rhs trans_b dlhs drhs None None x = torch randn requires_grad=True y = torch randn transpose requires_grad_ True x = torch randn requires_grad=True y = torch randn transpose requires_grad_ True SDD apply x y sum backward torch compile backend= eager fullgraph=True fn SDD apply x y fn sum backward assertEqual x grad x grad assertEqual y grad y grad test_partially_initialized_module_property Matrix torch nn Module __init__ data super __init__ _data = data foo = blocking property data _data property blocking data shape torch compile backend= eager fullgraph=True fn Matrix torch randn v = fn assertEqual v foo assertEqual v data shape assertEqual type v Matrix test_classmethod_with_slots Mock __slots__ = _a __init__ _a = classmethod _m cls run x torch sin x _a _m fn x mock = Mock mock run x opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn assertEqual fn x opt_fn x test_nn_parametrize Module nn Module __init__ - None super __init__ param = torch nn Parameter torch randn forward x param x Parametrization torch nn Module forward x torch sin x m = Module torch nn utils parametrize register_parametrization m param Parametrization sin_found = False backend gm _ nonlocal sin_found node gm graph nodes node target torch sin sin_found = True gm opt_m = torch compile m backend=backend fullgraph=True inp = torch randn assertEqual m inp opt_m inp assertTrue sin_found torch nn utils parametrize remove_parametrizations m param sin_found = False assertEqual m inp opt_m inp assertFalse sin_found test_nn_module_property_closure x = torch randn Mod torch nn Module property y torch ones + x forward x x y mod = Mod fn x mod x opt_fn = torch compile fn backend= eager fullgraph=True inp = torch randn assertEqual fn inp opt_fn inp test_global_fn_mutation foo x y global_fn x + y x = torch ones y = torch ones opt = torch compile foo fullgraph=True backend= eager assertEqual opt x y foo x y Change global_fn global global_fn new_fn x torch cos x global_fn = new_fn assertEqual opt x y foo x y ref https github com pytorch pytorch issues test_list_reverse ladder x trail = x size - assert trail weights = s trail trail - trail - weights append torch ones s s - w weights x = x w weights reverse w weights x = x w t x data = torch randn opt_ladder = torch compile ladder fullgraph=True backend= eager assertEqual opt_ladder data ladder data test_trace_functional_tensor_with torch _subclasses fake_tensor FakeTensorMode torch _subclasses functional_tensor FunctionalTensor FunctionalTensorMode f tmp a_view = view - torch no_grad set_ tmp a_view mul_ + tmp fake_mode = FakeTensorMode FunctionalTensorMode inp = torch ones requires_grad=True inp = fake_mode from_tensor inp static_shapes=True inp = FunctionalTensor to_functional inp tmp = torch ones requires_grad=True tmp = fake_mode from_tensor tmp static_shapes=True tmp = FunctionalTensor to_functional tmp opt_f = torch compile f backend= eager assertRaisesRegex RuntimeError cannot mutate tensors frozen storage opt_f inp tmp test_const_dict_keyerror d = fn x try y = d except KeyError y = x + y opt_fn = torch compile fn backend= eager inp = torch randn assertEqual fn inp opt_fn inp test_nonconst_issubclass fn x issubclass x __class__ np ndarray opt_fn = torch compile fn backend= eager opt_fn np ones test_issue fn x = torch randn y = torch randn torch mm x y sum fn x = torch randn y = torch randn torch mm x y sum fresh_cache torch compile fn torch compile fn test_jit_script_defaults torch jit script fast_cos x c float = torch cos x c Mod torch nn Module __init__ - None super __init__ fast_cos = fast_cos forward x fast_cos x mod = Mod opt_mod = torch compile mod backend= eager fullgraph=True x = torch randn assertEqual mod x opt_mod x test_enum ExplicitEnum str Enum classmethod _missing_ cls value raise ValueError f value valid cls __name__ please select one list cls _value member_map_ keys PaddingStrategy ExplicitEnum LONGEST = longest MAX_LENGTH = max_length DO_NOT_PAD = do_not_pad fn x = PaddingStrategy longest == PaddingStrategy LONGEST torch sin x torch cos x x = torch randn opt_fn = torch compile fn backend= eager fullgraph=True assertEqual fn x opt_fn x test_hasattr_builtin MyClass foo int = func x m getattr type m foo x + MyClass foo x opt_func = torch compile func backend= eager fullgraph=True m = MyClass x = torch zeros assertEqual func x m opt_func x m assertEqual func x opt_func x test_grad Write ` grad ` ` _grad ` should reflective reading other should codegen-ed fn x y x _grad = y + y grad = x + x grad data y _grad data x = torch randn requires_grad=True y = torch randn requires_grad=True x = x clone y = y clone opt_fn = torch compile fn backend= eager assertEqual fn x y opt_fn x y assertEqual x grad x grad assertEqual y grad y grad test_nn_module_stack_bc torch _dynamo mutation_guard GenerationTracker compiler gm args module_stacks = node meta get nn_module_stack None node gm graph nodes module_stacks _ = pytree tree_flatten module_stacks module_stacks = x x module_stacks isinstance x str stack module_stacks assertTrue _module stack gm forward SubMod torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x Mod torch nn Module __init__ - None super __init__ submod = SubMod submod = SubMod forward x submod x + submod x mod = Mod opt_mod = torch compile mod backend=compiler opt_mod torch randn torch _dynamo config patch inline_inbuilt_nn_modules=True mod = Mod opt_mod = torch compile mod backend=compiler opt_mod torch randn example similar Pippy usecase mod = Mod GenerationTracker tag mod submod GenerationTracker mark_class_dynamic type mod submod mod = Mod opt_mod = torch compile mod backend=compiler opt_mod torch randn test_is_make_fx_tracing torch compile backend= eager fullgraph=True fn x torch nn modules activation _is_make_fx_tracing torch sin x fn torch rand test_export_vs_dynamo_for_multiheadattention More details https github com pytorch pytorch issues Ensure both dynamo export do take fast path torch no_grad inp = torch randn mha = nn MultiheadAttention dropout= batch_first=True mha eval backend = EagerAndRecordGraphs mha_compile = torch compile mha backend=backend fullgraph=True mha_compile inp inp inp torch compiler reset mha_export = torch _dynamo export mha inp inp inp compile_nodes = backend graphs graph find_nodes op= call_function target=torch _native_multi_head_attention export_nodes = mha_export graph_module graph find_nodes op= call_function target=torch _native_multi_head_attention assertEqual len compile_nodes assertEqual len export_nodes test_negative_floor_div_solve CompiledClass nn Module __init__ - None super __init__ nums = torch tensor t = forward num = nums t t += num m = CompiledClass m = torch compile m backend= eager first call works m second call causes failure m https github com pytorch pytorch issues test_tensor_random random_op tensor args kwargs res = tensor random_ args kwargs res random_op = torch compile random_op tensor = torch randn random_op tensor - random_op tensor - random_op tensor - https github com pytorch pytorch issues test_tensor_uniform uniform_op tensor args kwargs res = tensor uniform_ args kwargs res uniform_op = torch compile uniform_op tensor = torch randn uniform_op tensor - uniform_op tensor - uniform_op tensor - test_data_attr_mutation_after_saved_for_bw f x out = x sin x data mul_ out x = torch randn requires_grad=True x_test = x detach clone requires_grad_ True out = f x out_test = torch compile f backend= aot_eager x_test assertEqual out out_test out sum backward out_test sum backward assertEqual x grad x_test grad https github com pytorch pytorch issues test_map_with_multiple_args f b b + b gen_inps len_x len_y x = torch randn _ range len_x y = torch randn _ range len_y x y g x y map f x y opt_g = torch compile g fullgraph=True backend= eager inps = gen_inps assertEqual type g inps type opt_g inps assertEqual tuple g inps tuple opt_g inps inps = gen_inps assertEqual type g inps type opt_g inps assertEqual tuple g inps tuple opt_g inps test_staticmethod_allow_in_graph MyClass i = staticmethod foo_inner x torch mul x MyClass i dynamo inlines fullgraph will error verify dynamo doesn t inline staticmethod torch _dynamo allow_in_graph foo x torch _dynamo graph_break MyClass foo_inner x torch compile backend= eager fullgraph=True f_bad x MyClass foo x f_bad torch ones test_guard_with_tuple_mutation Foo __init__ - None x = foo = Foo d = b foo fn x d x d d b x opt_fn = torch compile fn backend= eager inp = torch randn assertEqual fn inp d opt_fn inp d d b x = assertEqual fn inp d opt_fn inp d test_compile_complex_conj f x torch mul x j x_ref = torch randn requires_grad=True x_test = x_ref detach clone requires_grad_ True out_ref = f torch view_as_complex x_ref out_test = torch compile f backend= aot_eager torch view_as_complex x_test assertEqual out_ref out_test torch view_as_real out_ref sum backward torch view_as_real out_test sum backward assertEqual x_ref grad x_test grad unittest skipIf SM OrLater Triton only supports devices CUDA capability = test_add_complex_conj f x x + x conj x = torch randn dtype=torch complex requires_grad=True out = torch compile f x expected_complex = x real dtype=out dtype assertTrue out dtype == torch complex assertEqual out expected_complex https github com pytorch pytorch issues test_partitioner_cse_respects_mutation_boundaries set_available = hasattr torch ops fsdp hasattr torch ops fsdp set_ set_available torch compile backend= aot_eager_decomp_partition f x l z z can CSEd z = x sin z = x sin y = x + torch ops fsdp copy_ default x y z z can CSEd each other z z they cross mutation boundary z = x sin z = x sin z z z z l x = torch randn x_clone = x clone l = torch randn requires_grad=True z z z z _ = f x l partitioner runs CSE We expect sin ops above - first CSE d - last CSE d - set_ op middle mutation barrier preventing CSE assertEqual z x_clone sin assertEqual z x_clone sin assertEqual z x_clone + sin assertEqual z x_clone + sin https github com pytorch pytorch issues test_fsdp_set_input_mutation_applied_when_input_gets_no_gradients set_available = hasattr torch ops fsdp hasattr torch ops fsdp set_ set_available torch compile backend= aot_eager_decomp_partition f x l z = x sin noqa F y = x + graph input has its storage mutated torch ops fsdp copy_ default x y z = x sin z l x = torch randn x_test = x clone l = torch randn requires_grad=True result _ = f x l result_test _ = torch compile f backend= aot_eager_decomp_partition x_test l assertEqual result result_test assertEqual x x_test test_aot_autograd_runtime_wrapper_prologue_profiled Names prologue profiling event prologue_name = AOTDispatcher Runtime Wrapper Prologue Simple linear op compile mod = torch nn Linear opt_mod = torch compile mod x = torch randn Run test grad no-grad test both boolean cases trace_joint c contextlib nullcontext torch no_grad Run compiled op profiling c warmup before profiling opt_mod x profile activities= ProfilerActivity CPU prof opt_mod x Make sure events populated then find prologue event last start time events = prof events assertTrue events None prologue_event = None last_start_time = event events hasattr event name prologue_name event name prologue_event = event event time_range start last_start_time last_start_time = event time_range start Make sure prologue event exist assertTrue prologue_event None Make sure there least one other event compiled function starts after prologue starts assertLess prologue_event time_range end last_start_time test_changing_stride cnt = torch _dynamo testing CompileCounter torch compile backend=cnt fn x y x y i range x = torch randn i create view i i == x = x x = x y = torch randn print x shape y shape fn x y assertTrue cnt frame_count = test_unsqueeze_mul_strides This case where we had input marked unbacked size= u stride= which bad We want actually size= u stride= u See more issue below https github com pytorch pytorch issues torch compile backend= eager fullgraph=True fn aot _sub_ aot _mul_ aot _unsqueeze_ = torch ops aten unsqueeze default aot _mul_ torch ops aten mul Tensor aot _sub_ aot _unsqueeze_ aot _sub_ = torch randn torch _dynamo decorators mark_unbacked aot _sub_ aot _mul_ = torch randn No assert necessary since used crash fn aot _sub_ aot _mul_ torch _dynamo config patch guard_nn_modules=False torch _dynamo config patch inline_inbuilt_nn_modules=False test_inlining_cornercase nn Modules can mapped either NNModuleVariable UnspecializedNNModuleVariable For NNModuleVariable tensor attributes become part Dynamo graph For unspecialized they lifted inputs But there cornercase Suppose you have NNModuleVariable submodule UnspecializedNNModuleVariable Today Dynamo will still consider submodule specialized courtesy guard source is_nn_module In retrospect mistake there dependencies export also cudagraphs which make harder fix corner case right away The long term solution inline_inbuilt_nn_modules anyways so we might have live cornercase short term We starting annotate source each nn module more precisely - NNModuleVariable attribute marked NNModuleSource UnspecilaizedNNModuleVariable attribute marked UnspecializedNNModuleSource But changes behavior cornercase And fails some tests which have unfortunately relied behavior To solve we tag source only when inline_inbuilt_nn_module flag turned In test we purposely turn flag off testing tagging disabled SubMod torch nn Module __init__ super __init__ linear = torch nn Linear = torch randn counter = multipliers = forward x counter += linear x multipliers multipliers Mod torch nn Module __init__ super __init__ submod = SubMod forward x submod x mod = Mod opt_mod = torch compile mod backend= eager x = torch randn ref = mod x noqa F res = opt_mod x noqa F mod submod multipliers = Since guard_nn_modules False will recompile torch _dynamo config patch error_on_recompile=True ref = mod x noqa F res = opt_mod x noqa F test_optimized_module_training mod = torch nn Linear mod eval opt_mod = torch compile mod backend= eager assertFalse opt_mod training opt_mod train assertTrue opt_mod training assertTrue mod training mod eval assertFalse opt_mod training test_optimized_module_patched_init A regression test pattern acame deepspeed MyModule torch nn Module __init__ super __init__ forward x x mul patch_init init functools wraps init wrapper module args kwargs hasattr module _ds_child_entered child s __init__ called since parents all see same object they can now skip post_init module _ds_child_entered = True init module args kwargs wrapper patch_init_for_class cls __init__ cls __dict__ cls _old_init = cls __init__ cls __init__ = patch_init cls __init__ patch_init_for_class MyModule mod = MyModule opt_mod = torch compile mod x = torch rand ref = mod x res = opt_mod x assertEqual ref res test_os_fspath torch compile backend= eager fullgraph=True fn x os fspath torch sin x fn torch randn requires_cuda test involves custom ops unbacked symints torch _dynamo config patch capture_dynamic_output_shape_ops=True test requires activation memory budget code think j banned recompute torch _functorch config patch activation_memory_budget= test_partitioner_activation_memory_budget_with_unbacked_symints torch library custom_op test_partitioner f mutates_args= f x torch Tensor - torch Tensor x new_zeros f register_fake _ x torch Tensor - torch Tensor ctx = torch library get_ctx s = ctx new_dynamic_size torch empty s device=x device dtype=x dtype torch library custom_op test_partitioner g mutates_args= g x torch Tensor - torch Tensor torch cat x x unsqueeze - g register_fake _ x torch Tensor - torch Tensor torch cat x x unsqueeze - torch library custom_op test_partitioner i mutates_args= i x torch Tensor sz int - torch Tensor torch ones sz dtype=x dtype device=x device i register_fake _ x torch Tensor sz int - torch Tensor torch empty sz dtype=x dtype device=x device torch library custom_op test_partitioner j mutates_args= j x torch Tensor y torch Tensor - torch Tensor x + j register_fake _ x torch Tensor y torch Tensor - torch Tensor sz = x shape - sz = y numel torch _check sz == sz make reduction so partitioner bans recompute x sum f x param y = torch ops test_partitioner f x z = torch ops test_partitioner g y z = torch ops test_partitioner i x z shape - z = torch ops test_partitioner j z z torch matmul x param sin z sum x = torch randn device= cuda param = torch randn device= cuda requires_grad=True out_ref = f x param out_test = torch compile f backend= aot_eager_decomp_partition x param assertEqual out_ref out_test requires_cuda This test will fail flip combination particular input lengths produces weird results This under investigations https github com pytorch pytorch issues unittest skip Skip flip test moment It under investigation test_flip_bad_accuracy torch torch _dynamo config torch _functorch config torch _inductor config torch _inductor inductor_prims torch fx experimental _config Repro torch nn Module __init__ super __init__ forward arg _ rev = torch ops prims rev default arg _ arg _ = None slice_ = torch ops aten slice Tensor rev - slice_ = torch ops aten slice Tensor rev add_ = torch ops aten add Tensor slice_ slice_ slice_ = slice_ = None slice_ = torch ops aten slice Tensor add_ - slice_ = torch ops aten slice Tensor add_ add_ = torch ops aten add Tensor slice_ slice_ slice_ = slice_ = None slice_ = torch ops aten slice Tensor add_ - slice_ = torch ops aten slice Tensor add_ add_ = torch ops aten add Tensor slice_ slice_ slice_ = slice_ = None slice_ = torch ops aten slice Tensor add_ add_ = None unsqueeze = torch ops aten unsqueeze default slice_ slice_ = None unsqueeze_ = torch ops aten unsqueeze default add_ add_ = None cat = torch ops aten cat default unsqueeze unsqueeze_ unsqueeze = unsqueeze_ = None view = torch ops aten view default cat cat = None slice_ = torch ops aten slice Tensor view - slice_ = torch ops aten slice Tensor add_ add_ = torch ops aten add Tensor slice_ slice_ slice_ = slice_ = None slice_ = torch ops aten slice Tensor add_ add_ = None cat_ = torch ops aten cat default slice_ add_ slice_ = add_ = None unsqueeze_ = torch ops aten unsqueeze default cat_ cat_ = None unsqueeze_ = torch ops aten unsqueeze default view view = None cat_ = torch ops aten cat default unsqueeze_ unsqueeze_ unsqueeze_ = unsqueeze_ = None view_ = torch ops aten view default cat_ cat_ = None slice_ = torch ops aten slice Tensor rev add_ = torch ops aten add Tensor view_ slice_ slice_ = None slice_ = torch ops aten slice Tensor rev rev = None cat_ = torch ops aten cat default slice_ add_ slice_ = add_ = None constant_pad_nd = torch ops aten constant_pad_nd default view_ view_ = None unsqueeze_ = torch ops aten unsqueeze default cat_ cat_ = None unsqueeze_ = torch ops aten unsqueeze default constant_pad_nd constant_pad_nd = None cat_ = torch ops aten cat default unsqueeze_ unsqueeze_ unsqueeze_ = unsqueeze_ = None view_ = torch ops aten view default cat_ cat_ = None slice_ = torch ops aten slice Tensor view_ view_ = None rev_ = torch ops prims rev default slice_ slice_ = None rev_ mod = Repro x = torch arange device=torch device cuda torch compile f x mod x out = f x assertEqual torch flip torch cumsum torch flip x out https github com pytorch pytorch issues test_return_value_duplication_tensor - None fn val torch Tensor - tuple torch Tensor torch Tensor val val x = torch randn requires_grad=True expect = fn x assertNotEqual expect untyped_storage data_ptr expect untyped_storage data_ptr actual = torch compile fn backend= aot_eager x assertNotEqual actual untyped_storage data_ptr actual untyped_storage data_ptr https github com pytorch pytorch issues test_return_value_duplication_mixed_grad - None fn val torch Tensor - tuple torch Tensor torch Tensor torch no_grad out = val + out = val + out out x = torch randn requires_grad=True torch enable_grad expect = fn x actual = torch compile fn backend= aot_eager x assertEqual expect requires_grad actual requires_grad assertEqual expect requires_grad actual requires_grad https github com pytorch pytorch pull #discussion_r test_return_value_duplication_scalar - None fn val torch Tensor - tuple torch Tensor torch Tensor x y = val val x y x = torch randn requires_grad=True expect = fn x assertNotEqual expect untyped_storage data_ptr expect untyped_storage data_ptr actual = torch compile fn backend= aot_eager x assertNotEqual actual untyped_storage data_ptr actual untyped_storage data_ptr test_torch_compile_in_compile_frame gn x c=None c None c = c x outer_func x torch compile gn backend= eager x compile_outer = torch compile outer_func backend= eager fullgraph=True x = torch randn ref = outer_func x res = compile_outer x assertEqual ref res https github com pytorch pytorch issues test_inductor_dynamic_shapes_broadcasting - None fn x y x_view = x view - y_view = y view - x_view y_view x = torch randn y = torch randn out_ref = fn x y out_test = torch compile fn dynamic=True x y assertEqual out_ref out_test https github com pytorch pytorch issues test_inductor_rng_default_dtype - None torch compile fn tmp = torch randn dtype=torch bfloat tmp try old = torch get_default_dtype torch set_default_dtype torch bfloat out = fn finally torch set_default_dtype old output dtype should float assertEqual out dtype torch bfloat unittest skipIf HAS_MSGSPEC missing msgspec package test_c_defined_metaclass User msgspec Struct A new type describing User name str value int fn x u = User alice x u value x = torch randn opt_fn = torch compile fn backend= eager assertEqual fn x opt_fn x unittest skipIf HAS_OMEGACONG missing omegaconf package test_omegaconf_dictconfig fn cfg x = cfg foo x b = cfg bar b cfg __dict__ baz = b cfg baz config = OmegaConf create foo bar b x = torch randn opt_fn = torch compile fn backend= eager fullgraph=True fn config x cloned_config = copy deepcopy config opt_fn cloned_config x assertEqual fn config x opt_fn config x assertEqual cloned_config baz unittest skipIf HAS_OMEGACONG missing omegaconf package test_omegaconf_listconfig_contains fn cfg x cfg torch sin x torch cos x config = OmegaConf create key value x = torch randn opt_fn = torch compile fn backend= eager fullgraph=True assertEqual fn config x opt_fn config x https github com pytorch pytorch issues test_overwriting_params M torch nn Module __init__ super __init__ fc = torch nn Linear fc = torch nn Linear forward x x = fc x x = fc x x ZeROOrderedDict collections OrderedDict __init__ parent_module=None args kwargs A replacement ` ` collections OrderedDict ` ` detect external ZeRO params Args parent_module ` ` collections OrderedDict ` ` collection replace super __init__ args kwargs _parent_module = parent_module __getitem__ key param = super __getitem__ key Params can registered None e g bias param None param do something here param inject_parameters module cls module module modules noqa B cls == ZeROOrderedDict new_param = cls parent_module=module new_param = cls key param module _parameters items new_param key = param module _parameters = new_param model = M inject_parameters model ZeROOrderedDict model = torch compile model backend= eager fullgraph=True x = torch ones torch no_grad model x test_typed_dict LlavaImagePixelInputs TypedDict type Literal pixel_values data torch Tensor Shape ` batch_size num_channels height width ` fn x y obj = LlavaImagePixelInputs type=int data=y out = x obj data obj data = out obj data x y = torch randn torch randn ref = fn x y opt_fn = torch compile fn backend= eager fullgraph=True res = opt_fn x y assertEqual ref res test_typed_dict_total LlavaImagePixelInputs TypedDict type Literal pixel_values data torch Tensor Shape ` batch_size num_channels height width ` fn x y obj = LlavaImagePixelInputs data=y total=False x obj data x y = torch randn torch randn ref = fn x y opt_fn = torch compile fn backend= eager fullgraph=True res = opt_fn x y assertEqual ref res skipIfPy listcomp bytecode optimized skipIfWindows msg= TODO xuhancn fix AssertionError Scalars equal test_listcomp Module torch nn Module __init__ super __init__ _num = torch _dynamo disable recursive=False forward x values = i torch cos x i range _num sum values mod = Module fn x mod x cnt = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnt x = torch randn ref = fn x res = opt_fn x assertEqual ref res assertEqual cnt frame_count Ensure listcomp fully compiled assertEqual cnt op_count https github com pytorch pytorch issues test_distributions_subclass torch torch distributions Categorical SubCateg Categorical pass torch compile backend= eager fullgraph=True make_dist_and_execute t d categ = d logits=t = categ log_prob categ sample + categ probs + categ logits _ range make_dist_and_execute torch randn SubCateg test_bitwise_print_precedence math torch compile fullgraph=True dynamic=True f x torch _check math floor x size &#124; == x sin f torch randn test_tensor_split_within_device_cm torch compile fullgraph=True split x x split x = torch zeros res = split x torch device cpu assertEqual res split x test_method_overriding DilateConv torch nn Module __init__ dilate_func=None super __init__ dilate_func = dilate_func forward x dilate_func torch sin x MainModule torch nn Module __init__ super __init__ mod = DilateConv dilate_func = dilate_func forward x mod x mod = MainModule opt_mod = torch compile mod backend= eager fullgraph=True x = torch randn ref = mod x res = opt_mod x assertEqual ref res test_symnode_is_op torch compile backend= eager fullgraph=True dynamic=True f x xs x size xs x + x t = torch randn res = f t assertEqual t res test_compile_copy__int_overload torch compile backend= aot_eager fullgraph=True f x x copy_ t = torch zeros res = f t assertEqual torch ones_like t res test_symnode_is_not_op torch compile backend= eager fullgraph=True dynamic=True f x xs x size xs x + x t = torch randn res = f t assertEqual t + res test_symint_bitwise fn x z = x shape z &#124; = z z &#124; = z z = z &#124; z y = z &#124; z = test composition non-bitwise ops z = z &#124; z y z opt_fn = torch compile fn backend= eager dynamic=True fullgraph=True inp = torch randn assertEqual fn inp opt_fn inp test_bitwise_op_guard attempt evaluating guard BitwiseFn_bitwise_ fn x x shape &#124; x shape x = x + x shape x shape x + x - opt_fn = torch compile fn backend= eager dynamic=True fullgraph=True inp = torch randn assertEqual fn inp opt_fn inp test_ones_out_dynamic ones_fn size out torch ones size out=out opt_model = torch compile ones_fn out = torch empty opt_model out out = torch empty opt_model out test_zeros_out_dynamic zeros_fn size out torch zeros size out=out opt_model = torch compile zeros_fn out = torch empty opt_model out out = torch empty opt_model out test_empty_out_dynamic empty_fn size out torch empty size out=out opt_model = torch compile empty_fn out = torch empty opt_model out out = torch empty opt_model out test_dataclass_in_module dataclasses dataclass MyData value float MyModel nn Module __init__ super __init__ my_data = MyData value= forward x Make sure use scalar value correctly tensor operations value_tensor = torch tensor my_data value x + value_tensor model = MyModel inputs = torch randn expected = model inputs compiled_model = torch compile model actual = compiled_model inputs assertEqual actual expected test_no_tracing_into_eval_frame test dynamo doesn t trace into nested calls eval_frame torch compile backend= eager fullgraph=True fn x x + orig_fn = torch _dynamo eval_frame _maybe_set_eval_frame bad args kwargs torch _dynamo graph_break orig_fn args kwargs mock patch torch _dynamo eval_frame _maybe_set_eval_frame bad fn torch ones torch _dynamo config patch raise_on_ctx_manager_usage=False test_no_tracing_into_eval_frame_ctx_manager Test dynamo doesn t trace into nested calls eval_frame when using context manager Even though we don t officially support Dynamo context managers we still have tests use them so we should still make sure eval_frame callback set correct places these cases fn x x + orig_fn = torch _dynamo eval_frame _maybe_set_eval_frame bad args kwargs torch _dynamo graph_break orig_fn args kwargs mock patch torch _dynamo eval_frame _maybe_set_eval_frame bad torch _dynamo optimize_assert eager fn torch ones torch _dynamo config patch allow_empty_graphs=True parametrize fullgraph True False test_empty_graph_nested_calls fullgraph k x x g x k x f x g x TODO clear all tests torch _dynamo eval_frame clear_dynamo_tls opt_f = torch compile f backend= eager fullgraph=fullgraph dynamic=False opt_f torch randn we should compiling g h top-level functions assertEqual len torch _dynamo eval_frame dynamo_tls traced_frame_infos no recompilation opt_f torch randn assertEqual len torch _dynamo eval_frame dynamo_tls traced_frame_infos recompilation opt_f torch randn assertEqual len torch _dynamo eval_frame dynamo_tls traced_frame_infos test_torchname fn obj torch typename obj opt_fn = torch compile fn backend= eager assertEqual fn typing Any opt_fn typing Any unittest skipIf TEST_CUDA test requires CUDA unittest skipIf dist is_available test requires distributed TODO Remoe skip once nccl issue fixed unittest skip Failing ncc update https github com pytorch pytorch issues test_ddp_checkpoint https github com pytorch pytorch issues DIM = SEQ_LEN = torch compile backend= eager fullgraph=True mlp_forward x w w b b y = F linear x w b y = F relu y y = F linear y w b y MLP nn Module __init__ in_features int hidden_features int out_features int super __init__ w_in = nn Parameter torch randn hidden_features in_features w_out = nn Parameter torch randn out_features hidden_features b_in = nn Parameter torch randn hidden_features b_out = nn Parameter torch randn out_features forward x result = torch utils checkpoint checkpoint mlp_forward x w_in w_out b_in b_out use_reentrant=False assert isinstance result torch Tensor result x = torch randn SEQ_LEN DIM y = torch zeros dataset = torch utils data TensorDataset x y dataloader = torch utils data DataLoader dataset batch_size= model = MLP DIM DIM DIM try required DDP wrapper initialization prior_master_addr = os environ get MASTER_ADDR None prior_master_port = os environ get MASTER_PORT None os environ MASTER_ADDR = localhost os environ MASTER_PORT = dist init_process_group backend= nccl world_size= rank= model = model cuda model = nn parallel DistributedDataParallel model batch dataloader x y = batch x = x cuda output = model x loss = output sum loss backward finally dist destroy_process_group prior_master_addr os environ MASTER_ADDR = prior_master_addr del os environ MASTER_ADDR prior_master_port os environ MASTER_PORT = prior_master_port del os environ MASTER_PORT torch _dynamo config patch recompile_limit= fail_on_recompile_limit_hit=True test_compilation_metrics_on_error torch _dynamo utils clear_compilation_metrics torch compile backend= eager fn x force recompile way friendly test_dynamic_shapes x numel == x sum x numel == x sum x = torch randn y = torch randn metrics = torch _dynamo utils _compilation_metrics assertEqual len metrics fn x assertTrue metrics torch _dynamo utils _compilation_metrics assertEqual len metrics latest_metrics = metrics - assertTrue latest_metrics dynamo_config None assertTrue latest_metrics recompile_reason None assertRaises torch _dynamo exc FailOnRecompileLimitHit fn y assertTrue metrics torch _dynamo utils _compilation_metrics assertEqual len metrics latest_metrics = metrics - assertTrue latest_metrics dynamo_config None assertTrue latest_metrics recompile_reason None torch _dynamo utils clear_compilation_metrics https github com pytorch pytorch issues serialTest test_dont_dce_rand https github com pytorch pytorch issues f image_latent B = num_ref = num_tar = x = torch rand B indices = torch argsort torch rand x shape dim=- num_ref + num_tar image_latent torch arange B unsqueeze - indices num_ref torch manual_seed torch cuda manual_seed_all expected = f torch randn sum https github com pytorch pytorch issues torch _inductor config fallback_random = True backend eager aot_eager torch manual_seed torch cuda manual_seed_all actual = torch compile backend=backend fullgraph=True f torch randn sum assertEqual actual expected test_incompatible_configs torch _dynamo config patch suppress_errors=False fail_on_recompile_limit_hit=False torch compile lambda None torch _dynamo config patch suppress_errors=True fail_on_recompile_limit_hit=False torch compile lambda None torch _dynamo config patch suppress_errors=False fail_on_recompile_limit_hit=True torch compile lambda None torch _dynamo config patch suppress_errors=True fail_on_recompile_limit_hit=True assertRaises AssertionError torch compile lambda None test_str_isalnum f x c str isalnum c x sin opt_f = torch compile f backend= eager fullgraph=True x = torch randn c = foobar assertEqual f x c opt_f x c test_nn_param_freevar_codegen Model nn Module __init__ - None super __init__ conv = nn Conv d in_channels= out_channels= kernel_size= batchnorm = nn BatchNorm d num_features= conv_weight = torch randn conv_bias = torch randn forward x conv weight = nn Parameter conv_weight conv bias = nn Parameter conv_bias requires_grad=False conv eval x = conv x x = batchnorm x x = F relu x x input_tensor = torch randn func = Model cpu functools wraps func wrapper args kwargs func args kwargs torch no_grad func train False v = func input_tensor jit_func = torch compile wrapper backend= eager fullgraph=True v = jit_func input_tensor assertEqual v v test_amp_foreach_fake_impl inv_scale = torch full found_inf = torch full grads = torch ones torch ones f res = torch _amp_foreach_non_finite_check_and_unscale_ grads found_inf inv_scale res ref = f res = torch compile f backend= aot_eager assertEqual ref res test_deleted_compile_wrapper_segfault fn x x + opt_fn = torch compile fn backend= eager This calls cached_backend clear which removes any strong references callback torch _dynamo reset opt_fn torch randn opt_fn = torch compile fn backend= eager opt_fn torch randn possible segfault due first opt_fn deletion test_delete_local_error torch compile backend= eager fullgraph=True fn x y = x + del y z = y + noqa F z assertRaises torch _dynamo exc Unsupported fn torch ones test_nanmean_out f x out torch nanmean x out=out x = torch randn out_ref = torch tensor out_res = torch tensor f x out_ref torch compile f backend= eager fullgraph=True x out_res assertEqual out_ref out_res skipIfNotPy test_sys_monitoring found_dynamo = False found_compiled_graph = False compiled_graph = None backend gm _ nonlocal compiled_graph compiled_graph = gm gm callback code offset nonlocal found_dynamo nonlocal found_compiled_graph torch _dynamo graph_break code torch _dynamo symbolic_convert InstructionTranslator run __code__ found_dynamo = True compiled_graph code compiled_graph __call__ __code__ found_compiled_graph = True sys monitoring use_tool_id test old_callback = sys monitoring register_callback sys monitoring events PY_START callback sys monitoring set_events sys monitoring events PY_START try torch compile backend=backend fullgraph=True fn x x + fn torch ones sys monitoring should still run Python dynamo assertTrue found_dynamo sys monitoring should still run compiled graph assertTrue found_compiled_graph finally sys monitoring register_callback sys monitoring events PY_START old_callback test_ _local_cell_overlap keys = range allowed = fn x x = x + torch _dynamo graph_break key = key key keys key allowed inner nonlocal key x + key assertEqual fn torch ones torch compile fn backend= eager torch ones test_cells_unsupported_step_exception This error happened because - we generating cells into list stack - we encountered unsupported step resulting step graph break - we encounter exception which pops stack until reaches certain length presence list cells then messes things up cell = torch compile backend= eager fn x x = x + + torch _dynamo step_unsupported contextlib nullcontext print cell raise AssertionError assertRaises AssertionError fn torch ones test_unbind_copy_out f eye out torch unbind_copy eye out=out eye = torch eye out_ref = torch zeros torch zeros torch zeros out_res = torch zeros torch zeros torch zeros f eye out_ref torch compile f backend= eager fullgraph=True eye out_res assertEqual out_ref out_res test_setitem_tensor_prop Using composite implicit forward would incorrect MyFn torch autograd Function staticmethod forward ctx x torch matmul x x t staticmethod backward ctx grad_out grad_out fn x y x = y MyFn apply x inputs torch manual_seed x = torch randn y = torch randn requires_grad=True x y x y = inputs fn x y sum backward assertTrue x requires_grad x y = inputs torch compile fn backend= eager x y sum backward assertTrue x requires_grad assertEqual y grad y grad test_nn_parameter_ctor_graph_breaks fn param = torch nn Parameter torch ones param maxDiff = None eb = ExplainWithBackend eager optimized_fn = torch compile fn backend=eb _ = optimized_fn explain_output = eb output assertEqual explain_output graph_break_count expected_msg = Attempted use ` torch nn Parameter ` constructor Dynamo\n Explanation Dynamo does support this\n Hint Try construct ` torch nn Parameter ` outside compiled region \n Hint If possible turn ` graph_break_on_nn_param_ctor ` off\n Hint It may possible write Dynamo tracing rules code Please report issue PyTorch you encounter graph break often causing performance issues \n\n Developer debug context \n\n For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html assertEqual explain_output break_reasons reason expected_msg parametrize backend eager inductor test_issue backend str backend == inductor torch _dynamo config dynamic_shapes raise unittest SkipTest Skip only dynamic-shapes wrapper known issue MixedFakeModeModel nn Module __init__ dim= super __init__ dim = dim lin = torch nn Linear forward x batch_size seq_len _ = x shape Process input first - creates fake tensors export s fake mode processed = lin x Create some computation depends processed tensor intermediate = processed sum dim=- detach Shape batch seq_len dynamic_mask_function batch_idx head_idx q_idx kv_idx threshold = intermediate batch_idx q_idx seq_len Access captured tensor kv_idx = q_idx threshold block_mask = create_block_mask mask_mod=dynamic_mask_function B=batch_size H=None Q_LEN=seq_len KV_LEN=seq_len device=x device _compile=False q = processed view batch_size seq_len dim k = processed view batch_size seq_len dim v = processed view batch_size seq_len dim out = torch compile flex_attention q k v block_mask=block_mask out = flex_attention q k v block_mask=block_mask out backend_counter = CompileCounterWithBackend backend model = MixedFakeModeModel compiled = torch compile model backend=backend_counter fullgraph=True backend == inductor A known InductorError Issue https github com pytorch pytorch issues assertRaises RuntimeError compiled torch randn compiled torch randn One graph so no graph breaks assertEqual backend_counter frame_count assertEqual len backend_counter graphs https github com pytorch pytorch issues test_guard_same_frame_fail_message torch _dynamo guards g deterministically fail check same frame verify error message correctness other example fail might datetime now until patched - see issue compile_check_fn = g CheckFunctionManager compile_check_fn wrapper builder sorted_guards guard_fail_fn compile_check_fn builder sorted_guards guard_fail_fn check x False guard_manager check = check mock patch object g CheckFunctionManager compile_check_fn new=wrapper Model nn Module forward x x + model = Model x = torch randn assertRaises AssertionError e torch compile model x msg = str e exception assertIn Guard failed same frame created This bug - please create issue Guard fail reason msg ReproTestsDevice torch _dynamo test_case TestCase test_sub_alpha_scalar_repro device torch compile backend= aot_eager f x x sub alpha= f torch ones device=device dtype=torch float requires_cuda test_norm_dtype device foo _stack getitem = _stack slice None None None - _stack = None normalize = torch nn functional normalize getitem p= dim= getitem = None normalize args = torch float device False args = rand_strided sh st dt dev requires_grad_ rg sh st dt dev rg args torch compile foo backend= aot_eager_decomp_partition torch cuda amp autocast enabled=True ref = foo args res = foo args assertEqual ref dtype res dtype assertTrue same res ref test_guard_default_device device try torch set_default_device device counter = torch _dynamo testing CompileCounter torch _dynamo optimize counter f x = torch randn x assertEqual f device type + device assertEqual counter frame_count torch set_default_device cpu assertEqual f device type cpu assertEqual counter frame_count finally torch set_default_device None skipIfHpu unittest skipIf TEST_WITH_ROCM PLATFORM_SUPPORTS_FLASH_ATTENTION flash attention supported test_flash_attn_backward_mixed_strides device repro grad_out value transposed tensors key value contiguous gen_inputs device torch randn dtype=torch float device=device transpose torch randn dtype=torch float device=device torch randn dtype=torch float device=device torch randn dtype=torch float device=device transpose torch randn dtype=torch float device=device torch randn device=device None None False torch tensor dtype=torch int torch tensor dtype=torch int inps_device = gen_inputs device inps_meta = gen_inputs meta out _ref out _ref out _ref = torch ops aten _scaled_dot_product_flash_attention_backward inps_device scale= torch _meta_registrations meta__scaled_dot_product_flash_backward out _test out _test out _test = meta__scaled_dot_product_flash_backward inps_meta scale= assertEqual out _ref shape out _test shape assertEqual out _ref stride out _test stride assertEqual out _ref shape out _test shape assertEqual out _ref stride out _test stride assertEqual out _ref shape out _test shape assertEqual out _ref stride out _test stride test_megablocks_moe device try megablocks layers moe megablocks layers arguments Arguments except ImportError e raise unittest SkipTest requires megablocks e bs sl hs num_experts top_k = args = Arguments hidden_size=hs ffn_hidden_size=hs moe_num_experts=num_experts moe_capacity_factor= moe_top_k=top_k moe_mlp = moe MoE args moe_mlp cuda torch cuda current_device half moe_mlp device torch device current_device half x = torch randn sl bs hs device half out _ = moe_mlp x out _ = torch compile moe_mlp backend= eager x assertEqual out out test_tensor_size_hasattr fn x hasattr x size x = x hasattr x stride x = x x x = torch ones opt_fn = torch compile fn backend= eager fullgraph=True assertEqual fn x opt_fn x requires_cuda test_memleak_when_graph_input_has_tensor_attr device torch compile backend= eager f x x add_ mem_before = torch cuda memory_allocated x = torch ones device=device x foo = torch zeros device=device f x del x foo del x mem_after = torch cuda memory_allocated assertEqual mem_before mem_after check when non-tensor data structure attribute contains tensor torch compile backend= eager f x x add_ mem_before = torch cuda memory_allocated x = torch ones device=device x foo = torch zeros device=device _ range f x del x foo del x mem_after = torch cuda memory_allocated assertEqual mem_before mem_after check tensor refcycle torch compile backend= eager g x y x + y mem_before = torch cuda memory_allocated x = torch ones device=device y = torch zeros device=device x foo = y y foo = x g x y del x foo del y foo del x del y mem_after = torch cuda memory_allocated assertEqual mem_before mem_after test_udf_class_source Foo pass fn x foo = Foo bar = type foo noqa F torch cos x opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn assertEqual fn x opt_fn x test_truthiness_of_symints_no_recompiles device f x numel = x numel numel x + x + cnt = torch _dynamo testing CompileCounter f_compiled = torch compile f backend=cnt dynamic=True x = torch randn _ = f_compiled x x = torch randn _ = f_compiled x assertEqual cnt frame_count requires_cuda test_sdpa_dynamic_shapes device f x s s s q = x view s s s torch _C _nn scaled_dot_product_attention q q q attn_mask=None dropout_p= is_causal=True x = torch randn dtype=torch bfloat device=device x_ref = x clone detach requires_grad_ s = s = s = f_compiled = torch compile f dynamic=True torch _dynamo config patch assume_static_by_default=False out_ref = f x_ref s s s out = f_compiled x s s s assertEqual out_ref out unittest skipIf PLATFORM_SUPPORTS_FP requires gpu fp support requires_cuda test_partitioner_saves_weights_for_bw mul_tiled bs b bs = unflatten b shape - unflatten - b shape - - = b None None = flatten end_dim= flatten start_dim=- scale t amax_t max_v = E M _MAX_POS scale_t = torch clamp amax_t float min= e- max_v t_fp = mul_tiled t scale_t reciprocal e m _type t_fp scale_t matmul first amax_first second_t amax_second_t bias first_fp scale_first = scale first amax_first second_t_fp scale_second_t = scale second_t amax_second_t post_scales = post_bias = None post_scales = scale_first scale_second_t t scale_first = scale_first new_ones scale_second_t = scale_second_t t new_ones post_bias bias = bias None res = torch _scaled_mm first_fp second_t_fp t scale_a=scale_first scale_b=scale_second_t t bias=bias out_dtype=torch bfloat use_fast_accum=False res = mul_tiled res post_scales torch bfloat post_bias None res += post_bias res torch compiler allow_in_graph Fp LinearFn torch autograd Function staticmethod forward ctx b_t bias amax_a = abs unflatten - - amax dim=- amax_b_t = b_t abs unflatten - - amax dim=- out = matmul amax_a b_t amax_b_t bias ctx a_requires_grad = requires_grad ctx b_requires_grad = b_t requires_grad ctx bias_requires_grad = bias requires_grad bias None False ctx save_for_backward b_t amax_b_t out staticmethod backward ctx grad_out b_t amax_b_t = ctx saved_tensors Workaround https github com pytorch pytorch issues The partitioner would pre-compute transposed scaling weight forward s most efficient actually uses too much memory We prevent making scaling depend gradient way has no effect will optimized away later Care needed support tensor parallelism circumvent bugs b_t = b_t + grad_out None squeeze ctx a_requires_grad b = b_t t contiguous amax_grad_out = grad_out abs unflatten - - amax dim=- amax_b = amax_b_t t unflatten - - amax dim=- amax_b = amax_b repeat_interleave b shape amax_b shape dim= output_size=b shape grad_a = matmul grad_out amax_grad_out b amax_b None grad_a = None ctx b_requires_grad grad_b = grad_out t grad_b = None ctx bias_requires_grad grad_bias = grad_out sum dim= grad_bias = None grad_a grad_b grad_bias Mod torch nn Module __init__ super __init__ = torch nn Parameter torch randn dtype=torch bfloat device= cuda requires_grad=True b = torch nn Parameter torch randn dtype=torch bfloat device= cuda requires_grad=True bias = torch nn Parameter torch randn dtype=torch bfloat device= cuda requires_grad=True CustomLinear torch nn Linear forward input torch Tensor - torch Tensor out = Fp LinearFn apply input flatten end_dim=- weight bias out = out unflatten input shape - out m = CustomLinear dtype=torch bfloat device= cuda m = torch compile m backend= aot_eager simple mode track how many collective ops we saw backward TrackingMode TorchDispatchMode __init__ super __init__ ops_counter = defaultdict int __torch_dispatch__ func types args= kwargs=None kwargs None kwargs = rs = func args kwargs ops_counter func += rs = torch randn dtype=torch bfloat device= cuda requires_grad=True out = m TrackingMode mode out sum backward If you print out AOT fw bw graphs main thing look both weights primals_ primals_ saved backward become back inputs The easier-to-test thing I m checking here recompute primals_ happens backward With recompute there _to_copy ops backward Without there aka you set torch _functorch config treat_parameters_as_free_to_save = False assertEqual mode ops_counter torch ops aten _to_copy default test_getattr_return _WrapperDescriptor = type type __call__ _MethodWrapper = type all __call__ _ClassMethodWrapper = type int __dict__ from_bytes _NonUserDefinedCallables = _WrapperDescriptor _MethodWrapper _ClassMethodWrapper types BuiltinFunctionType _signature_get_user_defined_method cls method_name try meth = getattr cls method_name except AttributeError isinstance meth _NonUserDefinedCallables Once __signature__ will added C -level callables check won t necessary meth fn x s = _signature_get_user_defined_method type torch nn Linear __call__ s None torch cos x torch sin x opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn assertEqual fn x opt_fn x test_data_dependent_error_log_no_print This regression test case https github com pytorch pytorch pull io StringIO capturedOutput = StringIO sys stderr = capturedOutput torch compile fullgraph=True func sum + + = torch rand try func except Exception pass sys stderr = sys __stderr__ Make sure we don t _print_ out graph module output = capturedOutput getvalue assertNotIn GraphModule output test_deepcopy_constant_tensor_in_aot_bwd Fn torch autograd Function staticmethod forward ctx x x + staticmethod backward ctx grad_out grad_out torch tensor grad_out shape f x Fn apply x x = torch randn requires_grad=True out = f x should raise c_out = torch compile f backend= aot_eager dynamic=True x expected = torch autograd grad out sum inputs= x actual = torch autograd grad c_out sum inputs= x assertEqual expected actual test_module_attribute_error torch compile backend= eager f x torch _bar x torch compile backend= eager f x try torch _bar x except AttributeError x + assertRaises AttributeError f torch ones assertEqual f torch ones torch ones + test_torch_cuda_is_initialized torch compile fullgraph=True backend= eager f x torch cuda is_initialized x + x + inp = torch randn assertEqual f inp inp + mock patch torch cuda is_initialized lambda False assertEqual f inp inp + test_named_tuple_vt_clone https github com pytorch pytorch issues SVDCompressor nn Module __init__ k= super __init__ k = k forward x U S = torch linalg svd x reduced = U k torch diag_embed S k reduced input = torch randn model = SVDCompressor k= out = model input clone out = torch compile model backend= eager input clone assertEqual out out requires_cuda test_zero_dim_param_mixed_device_grad cpu -dim params cuda grads https github com pytorch pytorch issues RegressionModel torch nn Module __init__ a= b= super __init__ = torch nn Parameter torch tensor float b = torch nn Parameter torch tensor b float forward x x + b model = RegressionModel model forward = torch compile model forward backend= aot_eager fullgraph=True inputs = torch randn cuda out = model inputs out sum backward assertIsNotNone model grad assertIsNotNone model b grad assertEqual model grad device torch device cpu assertEqual model b grad device torch device cpu unittest skipIf TEST_CUDA test requires CUDA test_cuda_sync fn x y = x + torch cuda synchronize y x = torch ones device= cuda cnt = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnt assertEqual fn x opt_fn x assertEqual cnt frame_count test_filter_warnings x = torch ones requires_grad=True call_foobar x warnings warn foobar torch compile backend= eager f x call_foobar x call_foobar x call_foobar x call_foobar x call_foobar x warnings catch_warnings record=True w f x assertEqual len w assertEqual str w message foobar test_filter_safe_grad_warning x = torch ones requires_grad=True y = x non-leaf grad should warn torch _subclasses meta_utils safe_grad y filters out warning unsafe_grad y y grad warnings catch_warnings record=True w unsafe_grad y should still warn different callsite assertEqual len w assertTrue The grad attribute Tensor str w message unsafe_grad y should warn assertEqual len w test_filter_user_warnings x = torch ones requires_grad=True y = x non-leaf grad should warn torch _dynamo eval_frame TorchPatcher suppress_torch_distributed_warnings mute_warn y y grad mute_warn y filters out warning unsafe_grad y y grad warnings catch_warnings record=True w unsafe_grad y should still warn different callsite assertEqual len w assertTrue The grad attribute Tensor str w message unsafe_grad y should warn assertEqual len w torch _dynamo config patch install_free_tensors=True test_partial_export Foo torch nn Module __init__ super __init__ parallelize fn = _call_impl wrapped_fn fn args kwargs new_args_ = args torch bfloat new_args_ = args torch bfloat fn new_args_ new_args_ fn = functools partial wrapped_fn fn _call_impl = fn forward b + b torch _dynamo functional_export _dynamo_graph_capture_for_export foo = Foo foo parallelize x = torch randn dtype=torch float y = torch randn dtype=torch float ref = foo x y gm = _dynamo_graph_capture_for_export foo x y res = gm x y assertEqual res ref test_current_accelerator torch compile backend= eager fullgraph=True fn x torch accelerator current_accelerator x + assertEqual fn torch ones torch ones + instantiate_parametrized_tests ReproTests devices = cuda hpu instantiate_device_type_tests ReproTestsDevice globals only_for=devices __name__ == __main__ torch _dynamo test_case run_tests run_tests