mypy allow-untyped-defs logging contextlib contextmanager torch torch _C DispatchKey manual torch _functorch _aot_autograd utils KNOWN_TYPES torch _higher_order_ops utils autograd_not_implemented torch _library fake_class_registry _is_script_object _ns_and_class_name FakeScriptObject torch _ops HigherOrderOperator torch _subclasses fake_tensor FakeTensorMode torch fx experimental proxy_tensor ProxyTorchDispatchMode track_tensor_tree torch fx node has_side_effect torch utils _pytree pytree log = logging getLogger __name__ The call_torchbind operator represents method invocation torchbind object The calling convention call_torchbind ScriptObject method_name str method_args method_kwargs We do expect users write operator directly Instead will emitted Dynamo when tracing encounters torchbind object CallTorchBind HigherOrderOperator __init__ super __init__ call_torchbind __call__ obj method args kwargs super __call__ obj method args kwargs staticmethod schema obj method - torch FunctionSchema Returns schema ` ` CallTorchbind __call__ ` ` assert isinstance obj torch _inductor ir TorchBindObject val = obj get_real_obj schema = val _get_method method schema schema_str = str schema new_schema_str = f call_torchbind str schema arguments real_type schema arguments name first_comma_index = schema_str find first_comma_index == - If no comma found find last closing parenthesis first_comma_index = schema_str rfind - new_schema_str = new_schema_str + str method + schema_str first_comma_index new_schema = torch _C parse_schema new_schema_str new_schema call_torchbind = CallTorchBind Register operator side-effectful FX TODO really sufficient While passes hopefully check Node is_impure make good decisions we also assume we can execute graph many times we want without changing behavior which NOT true ops mutate torchbind object state has_side_effect call_torchbind _orig_scriptmethod_call = torch ScriptMethod __call__ torchbind_method_redispatch args kwargs _is_script_object raw_owner call_torchbind raw_owner name args kwargs _orig_scriptmethod_call args kwargs contextmanager enable_torchbind_tracing Context manager acts feature flag enable torchbind tracing behavior Once torchbind tracing has been stabilized we can remove turn always try KNOWN_TYPES append torch ScriptObject torch ScriptMethod __call__ = torchbind_method_redispatch type ignore method-assign yield finally assert KNOWN_TYPES pop torch ScriptObject Someone messed KNOWN_TYPES during tracing exploding torch ScriptMethod __call__ = _orig_scriptmethod_call type ignore method-assign call_torchbind py_impl DispatchKey CompositeExplicitAutograd call_torchbind_impl obj method args kwargs isinstance obj torch ScriptObject _orig_scriptmethod_call getattr obj method args kwargs isinstance obj FakeScriptObject getattr obj wrapped_obj method args kwargs raise RuntimeError f Unsupported first arg type type obj call_torchbind call_torchbind py_impl ProxyTorchDispatchMode inner mode args kwargs proxy_args = pytree tree_map mode tracer unwrap_proxy args proxy_kwargs = pytree tree_map mode tracer unwrap_proxy kwargs out_proxy = mode tracer create_proxy call_function call_torchbind proxy_args proxy_kwargs out = call_torchbind args kwargs obj method _rest_args = args isinstance obj torch ScriptObject ns class_name = _ns_and_class_name obj _type qualified_name type ignore attr-defined log warning Tracing torchbind method s s real ScriptObject This may cause original object being mutated If intended You can register fake torch _library register_fake_class s s class_name method ns class_name ret = track_tensor_tree out out_proxy constant=None tracer=mode tracer val out_proxy node meta assert out None isinstance out int float bool Currently only these constant dtypes supported returned torchbind methods out_proxy node meta val = out ret When tracing fake script object call_torchbind op will fake tensor When tracing real script object call_torchbind op may real tensor we need convert fake tensor manually Dynamic shape supported call_torchbind py_impl FakeTensorMode call_torchbind_fake mode args kwargs mode out = call_torchbind_impl args kwargs pytree tree_map_only torch Tensor lambda x mode from_tensor x static_shapes=True isinstance x torch _subclasses fake_tensor FakeTensor x out call_torchbind py_autograd_impl autograd_not_implemented call_torchbind deferred_error=True call_torchbind py_functionalize_impl call_torchbind_func ctx args kwargs torch _higher_order_ops effects handle_effects handle_effects ctx mode _allow_token_discovery ctx mode _tokens call_torchbind args kwargs