glob gzip os time zipfile functools lru_cache pathlib Path typing Any REPO_ROOT = Path __file__ resolve parent parent parent LAST_UPDATED = LOG_BUCKET_PREFIX = temp_logs lru_cache maxsize= get_s _resource - Any boto type ignore boto client s zip_artifact file_name str paths list str - None Zip files paths listed into file_name The paths will used glob should relative REPO_ROOT zipfile ZipFile file_name w f path paths file glob glob f REPO_ROOT path recursive=True f write file os path relpath file REPO_ROOT concated_logs - str Concatenate all logs test-reports directory into single string logs = log_file glob glob f REPO_ROOT test test-reports log recursive=True pyrefly ignore bad-argument-type logs append f === log_file === open log_file f For every line prefix fake timestamp log classifier line f line = line rstrip \n Remove any trailing newline pyrefly ignore bad-argument-type logs append f - - T Z line \n join logs upload_to_s _artifacts failed bool - None Upload file S workflow_id = os environ get GITHUB_RUN_ID workflow_run_attempt = os environ get GITHUB_RUN_ATTEMPT file_suffix = os environ get ARTIFACTS_FILE_SUFFIX job_id = os environ get JOB_ID workflow_id workflow_run_attempt file_suffix print GITHUB_RUN_ID GITHUB_RUN_ATTEMPT ARTIFACTS_FILE_SUFFIX set uploading test_reports_zip_path = f REPO_ROOT test-reports- file_suffix zip zip_artifact test_reports_zip_path test test-reports xml test test-reports csv test_logs_zip_path = f REPO_ROOT logs- file_suffix zip zip_artifact test_logs_zip_path test test-reports log jsons_zip_path = f REPO_ROOT test-jsons- file_suffix zip zip_artifact jsons_zip_path test test-reports json s _prefix = f pytorch pytorch workflow_id workflow_run_attempt artifact get_s _resource upload_file test_reports_zip_path gha-artifacts f s _prefix Path test_reports_zip_path name get_s _resource upload_file test_logs_zip_path gha-artifacts f s _prefix Path test_logs_zip_path name get_s _resource upload_file test_logs_zip_path gha-artifacts f s _prefix Path jsons_zip_path name get_s _resource put_object Body=b Bucket= gha-artifacts Key=f workflows_failing_pending_upload workflow_id txt job_id failed logs = concated_logs Put logs into bucket so log classifier can access them We cannot get actual GH logs so will have proxy print f Uploading logs job_id S get_s _resource put_object Body=gzip compress logs encode utf- Bucket= gha-artifacts Key=f LOG_BUCKET_PREFIX job_id ContentType= text plain ContentEncoding= gzip zip_and_upload_artifacts failed bool - None thread safe correctness LAST_UPDATED var doesn t really matter Upload test failed every minutes global LAST_UPDATED failed time time - LAST_UPDATED start = time time try upload_to_s _artifacts failed=failed LAST_UPDATED = time time except Exception e print f Failed upload artifacts e print f Uploading artifacts took time time - start f seconds trigger_upload_test_stats_intermediate_workflow - None requests The GITHUB_TOKEN cannot trigger workflow so isn t used now print Triggering upload_test_stats_intermediate workflow x = requests post https api github com repos pytorch pytorch actions workflows upload_test_stats_intermediate yml dispatches noqa B lint-ignore headers= Accept application vnd github v +json Authorization f Bearer os environ get GITHUB_TOKEN json= ref main inputs workflow_run_id os environ get GITHUB_RUN_ID workflow_run_attempt os environ get GITHUB_RUN_ATTEMPT print x text