Owner s oncall quantization copy operator unittest typing Any Optional torch torch ao quantization default_fake_quant FusedMovingAvgObsFakeQuantize MovingAverageMinMaxObserver MovingAveragePerChannelMinMaxObserver QConfigMapping torch ao quantization backend_config get_qnnpack_backend_config torch ao quantization qconfig default_per_channel_symmetric_qnnpack_qat_qconfig default_symmetric_qnnpack_qat_qconfig torch ao quantization quantize_fx prepare_qat_fx torch ao quantization quantize_pt e _convert_to_reference_decomposed_fx convert_pt e prepare_pt e prepare_qat_pt e torch ao quantization quantizer DerivedQuantizationSpec QuantizationAnnotation QuantizationSpec Quantizer torch ao quantization quantizer xnnpack_quantizer get_symmetric_quantization_config XNNPACKQuantizer torch export export torch testing _internal common_cuda TEST_CUDA torch testing _internal common_quantization NodeSpec ns QuantizationTestCase skip_if_no_torchvision skipIfNoQNNPACK torch testing _internal common_quantized override_quantized_engine torch testing _internal common_utils raise_on_run_directly PT EQATTestCase QuantizationTestCase Base QuantizationTestCase PT E QAT some helper methods _BaseConvBnModel torch nn Module __init__ conv_class type torch nn Module bn_class type torch nn Module has_conv_bias bool has_bn bool has_relu bool conv_kwargs super __init__ conv_kwargs setdefault in_channels conv_kwargs setdefault out_channels conv_kwargs setdefault kernel_size conv_kwargs setdefault bias has_conv_bias conv = conv_class conv_kwargs bn = bn_class conv_kwargs out_channels has_bn None relu = torch nn ReLU has_relu None forward x x = conv x bn None x = bn x relu None x = relu x x _get_conv_bn_model has_conv_bias bool = True has_bn bool = True has_relu bool = False transpose bool = False conv_kwargs Return instance simple test model containing conv -bn -relu pattern By default returns conv-bn model conv bias _BaseConvBnModel conv_transpose_class transpose conv_class bn_class has_conv_bias has_bn has_relu conv_kwargs _verify_symmetric_xnnpack_qat_numerics model torch nn Module example_inputs tuple Any _verify_symmetric_xnnpack_qat_numerics_helper model example_inputs is_per_channel=True _verify_symmetric_xnnpack_qat_numerics_helper model example_inputs is_per_channel=False _verify_symmetric_xnnpack_qat_numerics_helper model torch nn Module example_inputs tuple Any is_per_channel bool verify_convert bool = True Helper method verify QAT numerics PT E quantization match those FX graph mode quantization symmetric qnnpack resetting dynamo cache torch _dynamo reset MANUAL_SEED = PT export model_pt e = copy deepcopy model quantizer = XNNPACKQuantizer quantizer set_global get_symmetric_quantization_config is_per_channel=is_per_channel is_qat=True model_pt e = export model_pt e example_inputs strict=True module model_pt e = prepare_qat_pt e model_pt e quantizer torch manual_seed MANUAL_SEED after_prepare_result_pt e = model_pt e example_inputs model_fx = copy deepcopy model is_per_channel default_qconfig = default_per_channel_symmetric_qnnpack_qat_qconfig default_qconfig = default_symmetric_qnnpack_qat_qconfig qconfig_mapping = QConfigMapping set_global default_qconfig backend_config = get_qnnpack_backend_config model_fx = prepare_qat_fx model_fx qconfig_mapping example_inputs backend_config=backend_config torch manual_seed MANUAL_SEED after_prepare_result_fx = model_fx example_inputs Verify numerics match assertEqual after_prepare_result_pt e after_prepare_result_fx verify_convert We don t want impose any ordering requirements between move_exported_model_to_eval convert_pt e torch ao quantization move_exported_model_to_eval model_pt e model_pt e = convert_pt e model_pt e quant_result_pt e = model_pt e example_inputs model_fx eval model_fx = _convert_to_reference_decomposed_fx model_fx backend_config=backend_config quant_result_fx = model_fx example_inputs assertEqual quant_result_pt e quant_result_fx _verify_symmetric_xnnpack_qat_graph m torch fx GraphModule example_inputs tuple Any has_relu bool has_bias bool = True is_cuda bool = False expected_conv_literal_args Optional tuple Any = None TODO set true default verify_convert bool = False _verify_symmetric_xnnpack_qat_graph_helper m example_inputs is_per_channel=True has_relu=has_relu has_bias=has_bias is_cuda=is_cuda expected_conv_literal_args=expected_conv_literal_args verify_convert=verify_convert _verify_symmetric_xnnpack_qat_graph_helper m example_inputs is_per_channel=False has_relu=has_relu has_bias=has_bias is_cuda=is_cuda expected_conv_literal_args=expected_conv_literal_args verify_convert=verify_convert _verify_symmetric_xnnpack_qat_graph_helper m torch fx GraphModule example_inputs tuple Any is_per_channel bool has_relu bool has_bias bool = True is_cuda bool = False expected_conv_literal_args Optional tuple Any = None verify_convert bool = False Verify graph module matches fused QAT conv - bn - relu pattern fake quantizes inserted into correct places TODO also verify metadata copied over new nodes m = copy deepcopy m quantizer = XNNPACKQuantizer quantizer set_global get_symmetric_quantization_config is_per_channel is_qat=True m = export m example_inputs strict=True module m = prepare_qat_pt e m quantizer m example_inputs Verify getitem output activation fake quantize output_node = list m graph nodes - output_fq_node = output_node args assertTrue output_fq_node target startswith activation_post_process_ output_fq_mod = getattr m output_fq_node target assertEqual type output_fq_mod FusedMovingAvgObsFakeQuantize assertEqual type output_fq_mod activation_post_process MovingAverageMinMaxObserver assertEqual output_fq_mod dtype torch int assertEqual output_fq_mod quant_min - assertEqual output_fq_mod quant_max Verify getitem bn relu getitem bn has_relu relu_node = output_fq_node args bn_node = relu_node args assertEqual relu_node target torch ops aten relu default relu_node = None bn_node = output_fq_node args The relu node takes output bn See NOTE training ir has no getitem bn node assertEqual bn_node target torch ops aten batch_norm default Verify conv scale_factor reshape + bias reshape has_bias add_bias_node = bn_node args div_scale_factor_node bias_reshape_node = add_bias_node args assertEqual add_bias_node target torch ops aten add Tensor assertEqual bias_reshape_node target torch ops aten reshape default div_scale_factor_node = bn_node args conv_node scale_factor_reshape_node = div_scale_factor_node args conv_op = conv_node target assertEqual div_scale_factor_node target torch ops aten div Tensor assertTrue _is_conv_node conv_node assertEqual scale_factor_reshape_node target torch ops aten reshape default Verify conv literal args expected_conv_literal_args None assert len expected_conv_literal_args == wrong num conv args bad test setup i range i + len conv_node args assertEqual conv_node args i + expected_conv_literal_args i Verify conv input activation fake quantize conv_input_fq_node = conv_node args conv_input_node = conv_input_fq_node args assertTrue conv_input_fq_node target startswith activation_post_process_ conv_input_fq_mod = getattr m conv_input_fq_node target assertEqual type conv_input_fq_mod FusedMovingAvgObsFakeQuantize assertEqual type conv_input_fq_mod activation_post_process MovingAverageMinMaxObserver assertEqual conv_input_fq_mod dtype torch int assertEqual conv_input_fq_mod quant_min - assertEqual conv_input_fq_mod quant_max assertTrue conv_input_node op placeholder Verify conv weight fake quantize conv_weight_fq_node = conv_node args assertTrue conv_weight_fq_node target startswith activation_post_process_ conv_weight_fq_mod = getattr m conv_weight_fq_node target is_per_channel expected_weight_observer_type = MovingAveragePerChannelMinMaxObserver expected_weight_observer_type = MovingAverageMinMaxObserver assertEqual type conv_weight_fq_mod FusedMovingAvgObsFakeQuantize assertEqual type conv_weight_fq_mod activation_post_process expected_weight_observer_type assertEqual conv_weight_fq_mod dtype torch int assertEqual conv_weight_fq_mod quant_min - assertEqual conv_weight_fq_mod quant_max Verify conv fq input fq weight scale_factor reshape zero_bias zero_bias_node = conv_node args len conv_node args None mul_weight_scale_factor_node = conv_weight_fq_node args conv_weight_fq_node scale_factor_reshape_node = mul_weight_scale_factor_node args has_bias assertEqual zero_bias_node target torch ops aten zeros_like default assertTrue zero_bias_node None assertEqual mul_weight_scale_factor_node target torch ops aten mul Tensor assertEqual scale_factor_reshape_node target torch ops aten reshape default Verify scale_factor = bn_weight sqrt bn_running_var + eps scale_factor_node = scale_factor_reshape_node args bn_weight_node sqrt_node = scale_factor_node args bn_running_var_add_node = sqrt_node args bn_running_var_node eps = bn_running_var_add_node args assertEqual scale_factor_node target torch ops aten div Tensor assertTrue bn weight bn_weight_node target assertTrue bn running_var bn_running_var_node target assertEqual sqrt_node target torch ops aten sqrt default assertEqual bn_running_var_add_node target torch ops aten add Tensor assertEqual eps e- Optionally check converted graph verify_convert m = convert_pt e m m example_inputs is_per_channel conv_weight_dq_op = torch ops quantized_decomposed dequantize_per_channel default node_occurrence = ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_channel default conv_weight_dq_op = torch ops quantized_decomposed dequantize_per_tensor default node_occurrence = ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default node_list = ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function conv_weight_dq_op ns call_function conv_op ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default checkGraphModuleNodes m expected_node_list=node_list expected_node_occurrence=node_occurrence TestQuantizePT EQAT_ConvBn_Base PT EQATTestCase Base TestCase used all conv-bn -relu fusion patterns TODO how can we avoid adding every new test dynamo expected_test_failures Otherwise fails following error torch _dynamo exc InternalTorchDynamoError QuantizationConfig object has no attribute __bool__ setUp NB Skip test base handle test discovery logic buck which finds runs all tests here including base which we don t want run id _Base id skipTest Skipping test running base test_qat_conv_no_bias m = _get_conv_bn_model has_conv_bias=False has_bn=False has_relu=True m = _get_conv_bn_model has_conv_bias=False has_bn=False has_relu=False _verify_symmetric_xnnpack_qat_numerics m example_inputs _verify_symmetric_xnnpack_qat_numerics m example_inputs test_qat_conv_bn_fusion m = _get_conv_bn_model _verify_symmetric_xnnpack_qat_graph m example_inputs has_relu=False _verify_symmetric_xnnpack_qat_numerics m example_inputs unittest skipIf TEST_CUDA CUDA unavailable test_qat_conv_bn_fusion_cuda m = _get_conv_bn_model cuda example_inputs = example_inputs cuda _verify_symmetric_xnnpack_qat_graph m example_inputs has_relu=False is_cuda=True _verify_symmetric_xnnpack_qat_numerics m example_inputs test_qat_conv_bn_fusion_literal_args M torch nn Module __init__ conv_class bn_class super __init__ conv = conv_class stride= padding= bn = bn_class forward x x = conv x x = bn x x assert dim dim == stride padding dilation transposed output_padding groups conv_args = False example_inputs = torch randn stride padding dilation transposed output_padding groups conv_args = False example_inputs = torch randn m = M conv_class bn_class _verify_symmetric_xnnpack_qat_graph m example_inputs has_relu=False expected_conv_literal_args=conv_args _verify_symmetric_xnnpack_qat_numerics m example_inputs test_qat_conv_bn_fusion_no_conv_bias M torch nn Module Mixed conv + BN without conv bias __init__ conv_class bn_class super __init__ conv = conv_class bias=False bn = bn_class conv = conv_class bias=True bn = bn_class forward x x = conv x x = bn x x = conv x x = bn x x m = _get_conv_bn_model has_conv_bias=False m = M conv_class bn_class assert dim dim == example_inputs = torch randn example_inputs = torch randn _verify_symmetric_xnnpack_qat_graph m example_inputs has_relu=False has_bias=False _verify_symmetric_xnnpack_qat_numerics m example_inputs _verify_symmetric_xnnpack_qat_numerics m example_inputs test_qat_conv_bn_relu_fusion m = _get_conv_bn_model has_relu=True _verify_symmetric_xnnpack_qat_graph m example_inputs has_relu=True _verify_symmetric_xnnpack_qat_numerics m example_inputs unittest skipIf TEST_CUDA CUDA unavailable test_qat_conv_bn_relu_fusion_cuda m = _get_conv_bn_model has_relu=True cuda example_inputs = example_inputs cuda _verify_symmetric_xnnpack_qat_graph m example_inputs has_relu=True is_cuda=True _verify_symmetric_xnnpack_qat_numerics m example_inputs test_qat_conv_bn_relu_fusion_no_conv_bias m = _get_conv_bn_model has_conv_bias=False has_relu=True _verify_symmetric_xnnpack_qat_graph m example_inputs has_relu=True has_bias=False _verify_symmetric_xnnpack_qat_numerics m example_inputs test_qat_inplace_add_relu M torch nn Module __init__ conv_class super __init__ conv = conv_class relu = torch nn ReLU inplace=True forward x x = x x = conv x x += x x = relu x x assert dim dim == example_inputs = torch randn example_inputs = torch randn m = M conv_class _verify_symmetric_xnnpack_qat_numerics m example_inputs test_qat_update_shared_qspec Test case where nodes used SharedQuantizationSpec replaced during QAT subgraph rewriting M torch nn Module __init__ conv_class bn_class super __init__ conv = conv_class bn = bn_class hardtanh = torch nn Hardtanh forward x x = conv x x = bn x x = hardtanh x x m = M conv_class bn_class _verify_symmetric_xnnpack_qat_numerics m example_inputs test_qat_preserve_source_fn_stack Test whether ` source_fn_stack ` preserved after QAT fusion M torch nn Module __init__ conv_class bn_class backbone super __init__ conv = conv_class bn = bn_class relu = torch nn ReLU backbone = backbone forward x x = conv x x = bn x x = relu x x = backbone x x assert dim dim == example_inputs = torch randn example_inputs = torch randn QAT prepare + convert backbone = _get_conv_bn_model has_relu=True m = M conv_class bn_class backbone quantizer = XNNPACKQuantizer quantizer set_global get_symmetric_quantization_config is_qat=True m = export m example_inputs strict=True module m = prepare_qat_pt e m quantizer m example_inputs m = convert_pt e m Extract conv relu nodes bn folded into conv first_conv first_relu second_conv second_relu = None None None None n m graph nodes n target == torch ops aten relu default first_relu None assert first_conv None bad test setup first_relu = n first_conv = n args assert second_conv None bad test setup second_relu = n second_conv = n args Extract conv weight bias nodes get_conv_weight_and_bias conv_node torch fx Node weight_dq_node = conv_node args qweight_node = weight_dq_node args bias_node = conv_node args assert isinstance qweight_node torch fx Node assert isinstance bias_node torch fx Node qweight_node bias_node _ first_conv_bias = get_conv_weight_and_bias first_conv _ second_conv_bias = get_conv_weight_and_bias second_conv Assert each set conv conv weight conv bias same partition get_source_fn node torch fx Node E g l__self___backbone _conv torch nn modules conv Conv d node meta source_fn_stack we don t preserve quantized weight currently since s folded user can attach quantization_tag node will preserved assertEqual get_source_fn first_conv get_source_fn first_conv_qweight assertEqual get_source_fn second_conv get_source_fn second_conv_qweight assertEqual get_source_fn first_conv get_source_fn first_conv_bias assertEqual get_source_fn second_conv get_source_fn second_conv_bias Assert different sets convs relus have different partitions assertNotEqual get_source_fn first_conv get_source_fn first_relu assertNotEqual get_source_fn first_conv get_source_fn second_conv assertNotEqual get_source_fn second_conv get_source_fn second_relu assertNotEqual get_source_fn first_relu get_source_fn second_relu test_qat_conv_bn_bias_derived_qspec m = _get_conv_bn_model example_inputs = example_inputs m = export m example_inputs strict=True module quantizer = ConvBnDerivedBiasQuantizer m = prepare_qat_pt e m quantizer m example_inputs m = convert_pt e m m example_inputs Assert both weight bias quantized conv_node _ _ = _get_conv_bn_getitem_nodes m weight_dq = conv_node args bias_dq = conv_node args assertEqual weight_dq target torch ops quantized_decomposed dequantize_per_tensor default assertEqual bias_dq target torch ops quantized_decomposed dequantize_per_tensor default weight_getattr = weight_dq args bias_getattr = bias_dq args assertEqual weight_getattr op get_attr assertEqual bias_getattr op get_attr Assert bias scale = weight scale input scale input_dq = conv_node args input_scale = input_dq args bias_scale = bias_dq args weight_scale = weight_dq args assertEqual bias_scale input_scale weight_scale Assert args bias quantize dequantize ops copied correctly after subgraph rewriting bias_qmin bias_qmax bias_dtype = bias_dq args assertEqual bias_qmin - assertEqual bias_qmax - assertEqual bias_dtype torch int test_qat_per_channel_weight_custom_dtype m = _get_conv_bn_model example_inputs = example_inputs m = export m example_inputs strict=True module quantizer = ConvBnInt WeightQuantizer m = prepare_qat_pt e m quantizer m example_inputs m = convert_pt e m m example_inputs Assert conv weight quantized per channel conv_node _ _ = _get_conv_bn_getitem_nodes m weight_dq = conv_node args assertEqual weight_dq target torch ops quantized_decomposed dequantize_per_channel default weight_getattr = weight_dq args assertEqual weight_getattr op get_attr Assert args weight s dequantize ops copied correctly after subgraph rewriting dq_axis dq_qmin dq_qmax dq_dtype = weight_dq args assertEqual dq_axis assertEqual dq_qmin assertEqual dq_qmax - assertEqual dq_dtype torch int _do_test_qat_conv_transpose_bn has_relu bool Use different out channel sizes test conv weight properly transposed QAT pattern m = _get_conv_bn_model has_relu=has_relu transpose=True in_channels= out_channels= kernel_size= _verify_symmetric_xnnpack_qat_graph m example_inputs has_relu=has_relu verify_convert=True test_qat_conv_transpose_bn _do_test_qat_conv_transpose_bn has_relu=False test_qat_conv_transpose_bn_relu _do_test_qat_conv_transpose_bn has_relu=True test_qat_conv_bn_per_channel_weight_bias m = _get_conv_bn_model example_inputs = example_inputs m = export m example_inputs strict=True module quantizer = ConvBnDerivedBiasQuantizer is_per_channel=True m = prepare_qat_pt e m quantizer m example_inputs m = convert_pt e m m example_inputs Expected graph x - q_tensor - dq_tensor - conv - q_tensor - dq_tensor - output weight - q_channel - dq_channel bias - q_channel - dq_channel conv_node _ _ = _get_conv_bn_getitem_nodes m conv_op = conv_node target conv_weight_dq_op = torch ops quantized_decomposed dequantize_per_channel default node_occurrence = ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_channel default node_list = ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function conv_weight_dq_op ns call_function conv_weight_dq_op ns call_function conv_op ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default checkGraphModuleNodes m expected_node_list=node_list expected_node_occurrence=node_occurrence test_fold_bn_erases_bn_node Ensure BN node erased graph after folding into conv ` convert_pt e ` even train mode m = _get_conv_bn_model has_conv_bias=False has_bn=True has_relu=False m = export m example_inputs strict=True module quantizer = XNNPACKQuantizer quantizer set_global get_symmetric_quantization_config is_per_channel=False is_qat=True m = prepare_qat_pt e m quantizer m = convert_pt e m conv_node bn_node _ = _get_conv_bn_getitem_nodes m assertTrue conv_node None assertTrue bn_node None test_fold_bn_erases_add_node Test batch norm stat tracking which results add_ tensor removed when folding batch norm m = _get_conv_bn_model has_conv_bias=False has_bn=True has_relu=False m = export m example_inputs strict=True module _has_add_ graph node graph nodes node target == torch ops aten add_ Tensor True False Verify add_ tensor exists exported model tracking batch norm stats has_add_tensor_before = _has_add_ m graph assertTrue has_add_tensor_before Expected find add_ tensor exported model quantizer = XNNPACKQuantizer quantizer set_global get_symmetric_quantization_config is_per_channel=False is_qat=True m = prepare_qat_pt e m quantizer m = convert_pt e m Verify add_ tensor removed quantized model has_add_tensor_after = _has_add_ m graph assertFalse has_add_tensor_after Expected add_ tensor removed quantized model skipIfNoQNNPACK TestQuantizePT EQAT_ConvBn d TestQuantizePT EQAT_ConvBn_Base dim = example_inputs = torch randn conv_class = torch nn Conv d conv_transpose_class = torch nn ConvTranspose d bn_class = torch nn BatchNorm d skipIfNoQNNPACK TestQuantizePT EQAT_ConvBn d TestQuantizePT EQAT_ConvBn_Base dim = example_inputs = torch randn conv_class = torch nn Conv d conv_transpose_class = torch nn ConvTranspose d bn_class = torch nn BatchNorm d _is_conv_node n torch fx Node n op == call_function n target torch ops aten conv d default torch ops aten conv d default torch ops aten conv_transpose d torch ops aten conv_transpose d default torch ops aten conv_transpose d torch ops aten conv_transpose d input _get_conv_bn_getitem_nodes model torch fx GraphModule Return -tuple conv bn getitem nodes graph model graph eliminate_dead_code model recompile conv_node = None bn_node = None getitem_node = None n model graph nodes _is_conv_node n conv_node = n n target torch ops aten _native_batch_norm_legit default torch ops aten batch_norm default bn_node = n n target == operator getitem getitem_node = n assert conv_node None bad test setup conv_node bn_node getitem_node ConvBnInt WeightQuantizer Quantizer Dummy quantizer annotates conv bn such way weights quantized per channel int annotate model torch fx GraphModule - torch fx GraphModule conv_node bn_node getitem_node = _get_conv_bn_getitem_nodes model act_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine observer_or_fake_quant_ctr=default_fake_quant weight_qspec = QuantizationSpec dtype=torch int quant_min= quant_max= - qscheme=torch per_channel_affine observer_or_fake_quant_ctr=FusedMovingAvgObsFakeQuantize with_args observer=MovingAveragePerChannelMinMaxObserver conv_node meta quantization_annotation = QuantizationAnnotation input_qspec_map= conv_node args act_qspec conv_node args weight_qspec _annotated=True See NOTE training ir has no getitem bn node bn_node meta quantization_annotation = QuantizationAnnotation output_qspec=act_qspec _annotated=True model validate model torch fx GraphModule pass ConvBnDerivedBiasQuantizer Quantizer Dummy quantizer annotates conv bn such way bias qparams derived conv input activation weight qparams __init__ is_per_channel bool = False super __init__ is_per_channel = is_per_channel _derive_bias_qparams_from_act_and_weight_qparams obs_or_fqs act_scale _ = obs_or_fqs calculate_qparams weight_scale _ = obs_or_fqs calculate_qparams is_per_channel bias_scale = act_scale weight_scale bias_zero_point = torch zeros_like bias_scale dtype=torch int bias_scale = torch tensor act_scale weight_scale dtype=torch float bias_zero_point = torch tensor dtype=torch int bias_scale bias_zero_point annotate model torch fx GraphModule - torch fx GraphModule is_per_channel weight_qscheme = torch per_channel_symmetric weight_fq = FusedMovingAvgObsFakeQuantize with_args observer=MovingAveragePerChannelMinMaxObserver weight_qscheme = torch per_tensor_affine weight_fq = default_fake_quant conv_node bn_node getitem_node = _get_conv_bn_getitem_nodes model act_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine observer_or_fake_quant_ctr=default_fake_quant weight_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=weight_qscheme observer_or_fake_quant_ctr=weight_fq bias_qspec = DerivedQuantizationSpec derived_from= conv_node args conv_node conv_node args conv_node derive_qparams_fn=self _derive_bias_qparams_from_act_and_weight_qparams dtype=torch int quant_min=- quant_max= - qscheme=weight_qscheme ch_axis= is_per_channel None conv_node meta quantization_annotation = QuantizationAnnotation input_qspec_map= conv_node args act_qspec conv_node args weight_qspec conv_node args bias_qspec _annotated=True NOTE training ir has no getitem bn node getitem None when we use training IR It outputs aten batch_norm default which do need any getitem node In case we need annotate batch norm node geteitem node should only None we using training IR bn_node meta quantization_annotation = QuantizationAnnotation output_qspec=act_qspec _annotated=True model validate model torch fx GraphModule pass skipIfNoQNNPACK TestQuantizePT EQATModels PT EQATTestCase skip_if_no_torchvision skipIfNoQNNPACK test_qat_resnet torchvision override_quantized_engine qnnpack example_inputs = torch randn m = torchvision models resnet _verify_symmetric_xnnpack_qat_numerics m example_inputs skip_if_no_torchvision skipIfNoQNNPACK test_qat_mobilenet_v torchvision override_quantized_engine qnnpack example_inputs = torch randn m = torchvision models mobilenet_v _verify_symmetric_xnnpack_qat_numerics m example_inputs TestQuantizeMixQATAndPTQ QuantizationTestCase TwoLinear torch nn Module __init__ - None super __init__ linear = torch nn Linear bias=False linear = torch nn Linear forward x linear linear x QATPTQTestModule torch nn Module __init__ - None super __init__ conv = torch nn Conv d linears = TestQuantizeMixQATAndPTQ TwoLinear my_linear = torch nn Linear forward x conv_out = conv x permute_out = torch permute conv_out linear_out = linears permute_out my_linear_out = my_linear linear_out Hardtanh doesnt get quantized via xnnpack quantizer test because relies propagation rules Need fix torch nn functional hardtanh my_linear_out _prepare_qat_linears model name child model named_children isinstance child torch nn Linear TestQuantizeMixQATAndPTQ TwoLinear isinstance child torch nn Linear in_channels = child weight size in_channels = child linear weight size example_input = torch rand in_channels traced_child = export child example_input strict=True module quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True is_qat=True quantizer set_global quantization_config traced_child_prepared = prepare_qat_pt e traced_child quantizer setattr model name traced_child_prepared _prepare_qat_linears child _convert_qat_linears model name child model named_children isinstance child torch fx GraphModule torch ao quantization move_exported_model_to_eval child converted_child = convert_pt e child setattr model name converted_child _convert_qat_linears child test_mixing_qat_ptq example_inputs = torch randn model = TestQuantizeMixQATAndPTQ QATPTQTestModule _prepare_qat_linears model model example_inputs must fixed model eval _convert_qat_linears model model example_inputs model_pt e = export model example_inputs strict=True module quantizer = XNNPACKQuantizer quantizer set_module_type torch nn Linear None quantization_config = get_symmetric_quantization_config quantizer set_global quantization_config model_pt e = prepare_pt e model_pt e quantizer after_prepare_result_pt e = model_pt e example_inputs noqa F model_pt e = convert_pt e model_pt e quant_result_pt e = model_pt e example_inputs noqa F exported_model = torch export export model_pt e example_inputs strict=True node_occurrence = conv d act weight output x linear act output ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_channel default There needs one hardtanh checkGraphModuleNodes exported_model graph_module expected_node_occurrence=node_occurrence __name__ == __main__ raise_on_run_directly test test_quantization py