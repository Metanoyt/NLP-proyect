Owner s module functorch Copyright c Facebook Inc its affiliates All rights reserved This source code licensed under BSD-style license found LICENSE file root directory source tree copy math os subprocess sys unittest warnings functools partial wraps NB numpy testing dependency numpy np common_utils expectedFailureIf functorch torch torch autograd forward_ad fwAD torch nn nn torch nn functional F functorch combine_state_for_ensemble grad grad_and_value hessian jacfwd jacrev jvp make_functional make_functional_with_buffers make_fx vjp vmap functorch experimental functionalize replace_all_batch_norm_modules_ torch _C _ExcludeDispatchKeyGuard DispatchKey DispatchKeySet torch _dynamo allow_in_graph torch _functorch eager_transforms _slice_argnums torch _functorch make_functional functional_init functional_init_with_buffers torch _functorch utils enable_single_level_autograd_function torch _ops HigherOrderOperator torch _subclasses fake_tensor FakeTensorMode torch func functional_call linearize stack_module_state torch testing make_tensor torch testing _internal common_cuda SM OrLater TEST_CUDA tf _on_and_off with_tf _off torch testing _internal common_device_type dtypes instantiate_device_type_tests onlyCPU onlyCUDA torch testing _internal common_dtype get_all_fp_dtypes torch testing _internal common_utils freeze_rng_state instantiate_parametrized_tests IS_FBCODE IS_WINDOWS markDynamoStrictTest parametrize run_tests skipIfTorchDynamo subtest TEST_CUDA_MEM_LEAK_CHECK TEST_WITH_TORCHDYNAMO TestCase xfailIfTorchDynamo torch utils _pytree tree_flatten tree_map tree_unflatten USE_TORCHVISION = False try torchvision noqa F USE_TORCHVISION = True except ImportError warnings warn Couldn t torchvision Some our tests use try install commands pytorch org post-fixed ` -- no-deps ` avoid overwriting pytorch installation UserWarning TestCase _slice_argnums important helper function VmapTearDownMixin tearDown Ensure case test failure next test won t fail because previous call _vmap_increment_nesting wasn t undone i e test_vmap_free_tensor fails when PYTORCH_TEST_WITH_DYNAMO= call increment nesting undone TEST_WITH_TORCHDYNAMO warn = False while ci = torch _C _functorch peek_interpreter_stack ci key == torch _C _functorch TransformType Vmap warn = True torch _C _functorch _vmap_decrement_nesting break warn msg = Interpreter stack empty Test should have called torch _C _functorch _vmap_decrement_nesting warnings warn msg markDynamoStrictTest TestSliceArgnums TestCase test_invalid_argnum_type x = torch randn args = x assertRaisesRegex RuntimeError int Tuple _slice_argnums args assertRaisesRegex RuntimeError int Tuple _slice_argnums args assertRaisesRegex RuntimeError must int _slice_argnums args args = assertRaisesRegex RuntimeError must int _slice_argnums args test_out_of_bounds_argnum_values x = torch randn args = x assertRaisesRegex RuntimeError positional inputs _slice_argnums args assertRaisesRegex RuntimeError positional inputs _slice_argnums args - assertRaisesRegex RuntimeError positional inputs _slice_argnums args - test_not_enough_argnums x = torch randn args = x assertRaisesRegex RuntimeError must non-empty _slice_argnums args test_duplicate_argnums x = torch randn args = x x assertRaisesRegex RuntimeError must unique _slice_argnums args assertRaisesRegex RuntimeError must unique _slice_argnums args - test_flat_args_with_positive_int_argnum args = res = _slice_argnums args assertEqual res res = _slice_argnums args assertEqual res test_flat_args_with_negative_int_argnum args = res = _slice_argnums args - assertEqual res res = _slice_argnums args - assertEqual res test_flat_args_with_tuple_argnum args = res = _slice_argnums args assertEqual res args res = _slice_argnums args - assertEqual res test_pytree_args args = res = _slice_argnums args assertEqual res args res = _slice_argnums args assertEqual res args res = _slice_argnums args - assertEqual res args - res = _slice_argnums args - assertEqual res args test_argnums_reorders args = res = _slice_argnums args assertEqual res args args _get_weights_and_functional_call net mechanism mechanism == make_functional make_functional net assert mechanism == functional_call makes so function make_functional call have same signature net_func weights data functional_call net weights data net_func dict net named_parameters _get_weights_and_functional_call_with_buffers net mechanism mechanism == make_functional make_functional_with_buffers net assert mechanism == functional_call makes so function make_functional call have same signature net_func weights buffers data functional_call net weights buffers data net_func dict net named_parameters dict net named_buffers markDynamoStrictTest TestGradTransform TestCase test_primitive device x = torch randn device=device result = grad torch sin x assertEqual result torch cos x test_composite_simple device x = torch randn device=device result = grad lambda x torch flatten x sum x assertEqual result torch ones_like x test_fn_with_kwargs device foo x y x y sum x = torch randn device=device y = torch randn device=device expected = grad foo x y result = grad foo x y=y assertEqual result expected test_composite_complicated device x = torch randn device=device y = torch randn device=device foo x y result = x y result sum result = grad foo x y x requires_grad_ out = foo x y expected = torch autograd grad out x assertEqual result expected test_composite_two_ops device N C = y = torch randn N C device=device targets = torch randint C N device=device foo y targets F cross_entropy y targets result = grad foo y targets y requires_grad_ expected = torch autograd grad foo y targets y assertEqual result expected _test_attributes get_attr_lambda device x = torch randn dtype=torch double device=device expected = get_attr_lambda x foo x assertEqual get_attr_lambda x expected x sum grad foo x test_shape device _test_attributes lambda x x shape device test_dtype device _test_attributes lambda x x dtype device test_is_cuda device _test_attributes lambda x x is_cuda device test_numel device _test_attributes lambda x x numel device test_layout_sparse device indices = torch tensor device=device values = torch tensor device=device sparse_x = torch sparse_coo_tensor indices values device=device Verify input sparse assertEqual sparse_x layout torch sparse_coo foo x assert GradTrackingTensor still reports sparse layout assertEqual x layout torch sparse_coo x coalesce _values sum result = grad foo sparse_x The gradient should also sparse assertEqual result layout torch sparse_coo test_inplace device x = torch randn device=device foo x x clone sin_ result = grad foo x assertEqual result x cos test_inplace_on_view device x = torch randn device=device foo x y = x clone y = y y sin_ y sum result = grad foo x x requires_grad_ out = foo x expected = torch autograd grad out x assertEqual result expected test_inplace_on_view_base device x = torch randn device=device foo x y = x clone y = y y sin_ y result = grad foo x x requires_grad_ out = foo x expected = torch autograd grad out x assertEqual result expected test_inplace_on_captures device x = torch tensor device=device captured = torch randn device=device foo x captured copy_ x x captured sum assertRaisesRegex RuntimeError mutate captured Tensor grad foo x test_nesting_simple device x = torch randn device=device result = grad grad torch sin x assertEqual result -torch sin x skipIfTorchDynamo Ref https github com pytorch pytorch issues test_escaped_wrappers_are_marked_as_dead device x = torch randn device=device escaped = foo x y = x sin escaped append y y grad foo x assertEqual torch _C _functorch dlevel escaped - skipIfTorchDynamo Ref https github com pytorch pytorch issues test_escaped_wrappers_are_ignored device x = torch randn device=device escaped = foo x y = x sin escaped append y y grad foo x something = escaped sum assertEqual torch _C _functorch dlevel something assertEqual something x sin sum test_manual_seed_inside_grad device x = torch randn device=device f x torch manual_seed x torch randn_like x freeze_rng_state result = grad f x x requires_grad_ expected = torch autograd grad f x x assertEqual result expected test_vjp device x = torch randn device=device out vjp_fn = vjp torch sin x assertEqual out x sin v = torch randn device=device result = vjp_fn v assertEqual result v x cos test_vjp_two_outputs device f x x x result vjp_fn = vjp f torch tensor vjp_fn result test_conj_bit x = torch tensor + j foo x assert x is_conj y = x conj assert y is_conj y abs res = grad foo x torch no_grad assertEqual res torch ones_like res torch sgn x test_composed_with_autograd device x = torch randn requires_grad=True device=device y = grad torch sin x result = torch autograd grad y x assertEqual result -x sin test_grad_of_vjp_composition device x = torch randn device=device y = torch randn device=device foo x y out vjp_fn = vjp torch sin x grad lambda y vjp_fn y y result = foo x y expected = x cos assertEqual result expected test_vjp_of_grad_composition device x = torch randn device=device y = torch randn device=device foo x y out vjp_fn = vjp grad torch sin x vjp_fn y result = foo x y expected = -y x sin assertEqual result expected test_grad_of_vjp_of_grad_composition device x = torch randn device=device y = torch randn device=device foo x y df vjp_fn = vjp grad lambda x -torch cos x x grad lambda y vjp_fn y y result = foo x y expected = x cos assertEqual result expected test_views device x = torch randn requires_grad=True device=device y = torch randn requires_grad=True device=device silly_sin x x = x view x = x sin x foo x y z = grad silly_sin x z = torch cos y z + z result = foo x y grads = torch autograd grad result x y assertEqual grads -x sin assertEqual grads -y sin test_view_inplace_simple device foo x x = x clone x view sin_ x x = torch randn requires_grad=True device=device result = grad foo x assertEqual result x cos test_invalid_argnums device x = torch randn y = torch randn assertRaisesRegex RuntimeError only grad torch mul argnums=- x y assertRaisesRegex RuntimeError only grad torch mul argnums= x y assertRaisesRegex RuntimeError int Tuple grad torch mul argnums= x y assertRaisesRegex RuntimeError must int grad torch mul argnums= x y assertRaisesRegex RuntimeError must unique grad torch mul argnums= x y assertRaisesRegex RuntimeError must unique grad torch mul argnums= - x y test_argnums device x = torch randn y = torch randn gx = grad torch mul argnums= x y assertEqual gx y gy = grad torch mul argnums= x y assertEqual gy x gx = grad torch mul argnums= x y assertEqual gx y gx gy = grad torch mul argnums= x y assertEqual gx y assertEqual gy x test_out_of_order_argnums device x = torch randn y = torch randn gy gx = grad torch mul argnums= x y assertEqual gx y assertEqual gy x test_negative_argnums device x = torch randn y = torch randn gx = grad torch mul argnums=- x y assertEqual gx y gy = grad torch mul argnums=- x y assertEqual gy x gx = grad torch mul argnums= - x y assertEqual gx y gx gy = grad torch mul argnums= - - x y assertEqual gx y assertEqual gy x test_grad_pytree_inputs device x = torch randn device=device f b x y = x + y + b foo args = x x foo x gx gy = grad f args assertEqual gx torch tensor device=device assertEqual gy torch tensor device=device gx gy = grad f argnums= args assertEqual gx torch tensor device=device assertEqual gy torch tensor device=device gx gy gz = grad f argnums= args assertEqual gx torch tensor device=device assertEqual gy torch tensor device=device assertEqual gz foo torch tensor device=device test_grad_aux_tensor device x = torch randn device=device assertRaisesRegex RuntimeError r grad_and_value\ f\ \ \ args\ output function f should tuple grad lambda t t t has_aux=True x assertRaisesRegex RuntimeError r grad_and_value\ f\ \ \ args\ output function f should tuple grad lambda t t t + t + has_aux=True x f t y = t sin y sum t cos out aux = grad f has_aux=True x assertEqual aux x cos assertEqual out x cos test_grad_aux_pytree device f x y = x sin y sum x cos b x tan x = torch randn device=device out aux = grad f has_aux=True x _ expected_aux = f x assertEqual aux expected_aux assertEqual out x cos aux abc assertRaisesRegex RuntimeError r Expected tensors got unsupported type _ = grad lambda x x sum aux has_aux=True x assertRaisesRegex RuntimeError r Expected tensors got unsupported type _ = grad lambda x x sum x aux has_aux=True x test_zero_grad device f x x sum inps = torch randn device=device + b torch randn device=device grads = grad f inps assertNotEqual grads sum assertEqual grads b sum test_unrelated_grad device x = torch tensor device=device y = torch tensor device=device unrelated x y result = grad unrelated x assertEqual result torch zeros_like x test_unrelated_vjp device x = torch tensor device=device y = torch tensor device=device v = torch tensor device=device unrelated x y out vjp_fn = vjp unrelated x result = vjp_fn v expected = torch zeros_like x assertEqual result expected test_unrelated_vjp_multiple_inputs_outputs device w = torch tensor device=device x = torch tensor device=device y = torch tensor device=device v = torch tensor device=device unrelated w x y y x out vjp_fn = vjp unrelated w x result = vjp_fn v v v expected = torch zeros_like x torch ones_like x assertEqual result expected TODO https github com pytorch functorch issues onlyCPU test_unrelated_hessian device N = M = W = torch randn N M device=device f x W x x = torch randn M result = jacrev jacrev f x expected = torch zeros N M M device=device assertEqual result expected test_vjp_pytree_input device f x x x x = torch randn device=device v = torch randn device=device out vjp_fn = vjp f x x x assertEqual out x x result = vjp_fn v assertEqual result x v x v test_vjp_pytree_output device f x x x x x = torch randn device=device v = torch randn device=device v = torch randn device=device v = torch randn device=device _ vjp_fn = vjp f x result = vjp_fn v v v assertEqual result v + v + v test_vjp_outputs_can_any_pytree device x = torch randn device=device t = torch randn device=device output None assertRaisesRegex RuntimeError r vjp\ f \ primals\ Expected f function has non-empty output _ vjp_fn = vjp lambda _ output x vjp_fn t output True abc assertRaisesRegex RuntimeError r vjp\ f \ primals\ expected f\ \ primals\ only tensors _ vjp_fn = vjp lambda _ output x vjp_fn t Check list output output vjp_fn = vjp lambda x x x sum x vjp_out = vjp_fn t t sum assert isinstance output list len output == assert isinstance vjp_out torch Tensor Check dict output output vjp_fn = vjp lambda x x x xsum x sum x vjp_out = vjp_fn x t xsum t sum assert isinstance output dict len output == xsum output assert isinstance vjp_out torch Tensor composite_output x out = x sum out x out x out output vjp_fn = vjp composite_output x vjp_out = vjp_fn t sum t out t t sum assert isinstance output list assert isinstance output tuple isinstance output dict assert isinstance vjp_out torch Tensor test_vjp_pytree_error device f x x x x x = torch randn device=device v = torch randn device=device v = torch randn device=device v = torch randn device=device _ vjp_fn = vjp f x assertRaisesRegex RuntimeError Expected pytree structure result = vjp_fn v v v test_vjp_aux_tensor device x = torch randn device=device assertRaisesRegex RuntimeError r vjp\ f \ primals\ output function f should tuple vjp lambda t t t x has_aux=True assertRaisesRegex RuntimeError r vjp\ f \ primals\ output function f should tuple vjp lambda t t t + t + x has_aux=True f t y = t sin y t cos out vjp_fn aux = vjp f x has_aux=True assertEqual aux x cos assertEqual out x sin v = torch randn device=device grad_x = vjp_fn v assertEqual grad_x v x cos test_vjp_aux_pytree device f x y = x sin y x cos b x tan x = torch randn device=device out vjp_fn aux = vjp f x has_aux=True expected_out expected_aux = f x assertEqual out expected_out assertEqual aux expected_aux v = torch randn device=device grad_x = vjp_fn v assertEqual grad_x v x cos aux abc assertRaisesRegex RuntimeError r Expected tensors got unsupported type _ = vjp lambda x x aux x has_aux=True assertRaisesRegex RuntimeError r Expected tensors got unsupported type _ = vjp lambda x x x aux x has_aux=True test_functional_init device MLPClassifier nn Module __init__ hidden_dim= n_classes= super __init__ hidden_dim = hidden_dim n_classes = n_classes fc = nn Linear hidden_dim fc = nn Linear hidden_dim n_classes forward x x = fc x x = F relu x x = fc x x = F log_softmax x - x B = weights fn _ = functional_init MLPClassifier B device=device inputs = torch randn B device=device vmap fn weights inputs test_functional_init_with_buffers device MLPClassifier nn Module __init__ hidden_dim= n_classes= super __init__ hidden_dim = hidden_dim n_classes = n_classes fc = nn Linear hidden_dim bn = nn BatchNorm d hidden_dim affine=True fc = nn Linear hidden_dim n_classes forward x x = fc x x = F relu x x = bn x x = fc x x = F log_softmax x - x B = weights buffers fn _ _ = functional_init_with_buffers MLPClassifier B device=device inputs = torch randn B device=device vmap fn weights buffers inputs test_advanced_indexing device f value log_prob = torch ones device=device val = torch zeros log_prob val = value result = grad f torch randn device=device assertEqual result torch ones_like result f value value = value clone value value = value sum x = torch randn device=device result = grad f x assertEqual result x = type_as x test_tensor_ctor_inside_grad device foo x x torch tensor device=device x = torch tensor device=device functorch grad foo x parametrize op_list_data subtest vmap name= vmap subtest vmap vmap name= vmap_vmap subtest grad name= grad subtest grad grad name= grad_grad subtest vmap grad name= vmap_grad test_tensor_print device op_list_data op_list shapes = op_list_data dt get_all_fp_dtypes data = torch randn s dtype=dt device=device s shapes x data buf = None foo t nonlocal buf buf = repr t t mean fn = foo bdim = op reversed op_list op == vmap fn = op fn in_dims=bdim bdim += fn = op fn expected = f repr x level = op op_list level += noqa SIM op == grad expected = f GradTrackingTensor lvl= level value= expected op == vmap bdim -= expected = f BatchedTensor lvl= level bdim= bdim value= expected fn x buf = buf replace \n replace expected = expected replace \n replace assertEqual expected buf test_print_captured_tensor_inside_transform device x = torch tensor device=device out = None f y nonlocal out out = repr x y vjp f torch randn device=device assertEqual out repr x test_no_grad_outside device x = torch randn device=device requires_grad=True torch no_grad y = grad torch sin x assertEqual y x cos assertFalse y requires_grad test_no_grad_inside device f x torch no_grad shift = x x - shift x = torch randn device=device y = grad f x assertEqual y x y = grad grad f x assertEqual y x = torch randn device=device requires_grad=True y = grad f x z = torch autograd grad y x assertEqual z test_no_grad_mixed device f x torch no_grad shift = x x - shift x = torch randn device=device requires_grad=True torch no_grad y = grad f x assertEqual y x assertFalse y requires_grad test_no_grad_nested_simple device h x torch no_grad shift = grad lambda x x x x - shift x = torch tensor device=device requires_grad=True y = grad h x assertEqual y x z = torch autograd grad y x assertEqual z x test_no_grad_nested_complicated device f x torch no_grad shift = x x - shift g x r = grad f x torch no_grad shift = grad f x r - shift x = torch randn requires_grad=True device=device y = grad g x The only differential part g x assertEqual y x z = torch autograd grad y x assertEqual z test_no_grad_value device h x torch no_grad gvalue value = grad_and_value lambda x x x x - value x = torch tensor device=device requires_grad=True y = grad h x assertEqual y x z = torch autograd grad y x assertEqual z x test_no_grad_outside_vjp device h x x x = torch tensor requires_grad=True device=device torch no_grad out vjp_fn = vjp h x y = vjp_fn torch tensor device=device assertEqual y x assertFalse y requires_grad assertFalse out requires_grad test_no_grad_outside_vjp_fn device h x x x = torch tensor requires_grad=True device=device out vjp_fn = vjp h x torch no_grad y = vjp_fn torch tensor device=device assertEqual y x assertFalse y requires_grad assertTrue out requires_grad z = torch autograd grad out x assertEqual z x test_no_grad_outside_vjp_only device h x x x = torch tensor requires_grad=True device=device torch no_grad out vjp_fn = vjp h x y = vjp_fn torch tensor device=device assertEqual y x assertFalse out requires_grad This one little weird assertTrue y requires_grad z = torch autograd grad y x assertEqual z markDynamoStrictTest TestAutogradFunction TestCase test_set_materialize_grads device A torch autograd Function staticmethod forward x y x y staticmethod setup_context ctx inputs output ctx set_materialize_grads False staticmethod backward ctx gx gy assertIsNotNone gx assertIsNone gy gx gy f y x x y = A apply x y x x = torch tensor device=device y = torch tensor device=device grad differentiates w r t arg default grad f y x grad grad f y x parametrize inner_requires_grad True False parametrize save_for jvp vjp parametrize save_tensors input output neither parametrize mark_dirty True False test_function_returns_input device inner_requires_grad save_for save_tensors mark_dirty A torch autograd Function staticmethod forward x x staticmethod setup_context ctx inputs output save_for == jvp save_fn = ctx save_for_forward save_fn = ctx save_for_backward mark_dirty ctx mark_dirty inputs save_tensors == input save_fn inputs save_tensors == output save_fn output save_tensors == neither pass staticmethod backward ctx grad_output grad_output staticmethod jvp ctx x_t NB logic check ctx save_for_forward happens before we reach mark_dirty ret = x_t add_ ret = x_t view_as x_t ret fn x A apply x clone err_msg = A input has been returned as-is = torch tensor device=device requires_grad=inner_requires_grad a_t = torch tensor device=device requires_grad=inner_requires_grad save_tensors input output mark_dirty assertRaisesRegex RuntimeError err_msg grad fn assertRaisesRegex RuntimeError err_msg jvp fn a_t grad fn jvp fn a_t = torch tensor device=device requires_grad=inner_requires_grad clone a_t = torch tensor device=device requires_grad=inner_requires_grad clone save_tensors input output mark_dirty assertRaisesRegex RuntimeError err_msg A apply assertRaisesRegex RuntimeError err_msg fwAD dual_level A apply fwAD make_dual a_t b = A apply mark_dirty assertTrue b mark_dirty save_for == vjp save_tensors input output TODO soulitzer https github com pytorch pytorch issues fwAD dual_level a_dual = fwAD make_dual a_t b_dual = A apply a_dual mark_dirty assertTrue a_dual b_dual test_needs_input_grads device A torch autograd Function staticmethod forward x y x y staticmethod setup_context ctx inputs output staticmethod backward ctx grad_output assertTrue ctx needs_input_grad assertFalse ctx needs_input_grad None None x = torch tensor device=device y = torch tensor device=device grad differentiates w r t arg default grad A apply x y grad grad A apply x y _get_NumpyCubeNotComposable NumpyCubeNotComposable torch autograd Function staticmethod forward input input_np = input cpu numpy torch tensor input_np device=input device input_np staticmethod setup_context ctx inputs output ctx input_np = output ctx device = inputs device staticmethod torch autograd function once_differentiable backward ctx grad_output grad_saved result_np = ctx input_np torch tensor result_np device=ctx device NumpyCubeNotComposable test_once_differentiable_autograd_vjp device NumpyCubeNotComposable = _get_NumpyCubeNotComposable f x y _ = NumpyCubeNotComposable apply x y regular autograd x vjp x = torch randn requires_grad=True device=device grad_y = torch randn_like x requires_grad=True _ vjp_fn = vjp f x gx = vjp_fn grad_y assertRaisesRegex RuntimeError marked once_differentiable gx backward TODO support torch autograd function once_differentiable impossible figure out how raise nice error https github com pytorch pytorch issues unittest expectedFailure test_once_differentiable_grad_vjp device grad x vjp x = torch randn device=device grad_y = torch randn_like x h x grad_y _ vjp_fn = vjp f x noqa F gx = vjp_fn grad_y gx grad h argnums= x grad_y test_grad_fn_name device names = FooBar torch autograd Function staticmethod forward x x clone staticmethod setup_context ctx inputs output staticmethod backward ctx grad_output grad_output f x y = FooBar apply x names append type y grad_fn __name__ y x = torch tensor grad f x assertEqual names FooBarGeneratedBackward markDynamoStrictTest TestAutogradFunctionVmapAPI TestCase test_no_vmap_staticmethod_and_no_generate_vmap_rule device NumpyCube torch autograd Function staticmethod forward input input_np = to_numpy input noqa F dinput = torch tensor input_np device=input device torch tensor input_np device=input device dinput staticmethod setup_context ctx inputs output ctx save_for_backward inputs output staticmethod backward ctx grad_output grad_saved raise RuntimeError foobar x = torch randn device=device assertRaisesRegex RuntimeError does have vmap support vmap NumpyCube apply x test_has_vmap_staticmethod_and_has_generate_vmap_rule device NumpyCube torch autograd Function generate_vmap_rule = True staticmethod forward input input_np = to_numpy input noqa F dinput = torch tensor input_np device=input device torch tensor input_np device=input device dinput staticmethod setup_context ctx outputs input ctx save_for_backward input outputs staticmethod backward ctx grad_output grad_saved raise RuntimeError foobar staticmethod vmap infos in_dims x raise RuntimeError foobar x = torch randn device=device assertRaisesRegex RuntimeError generate_vmap_rule=True vmap NumpyCube apply x test_info_object device batch_size = Id torch autograd Function staticmethod forward input pass staticmethod setup_context ctx inputs output pass staticmethod backward ctx grad_output grad_saved pass staticmethod vmap info in_dims input assertEqual info batch_size batch_size assertEqual info randomness randomness input in_dims x = torch randn batch_size device=device randomness error different same vmap Id apply randomness=randomness x test_in_dims_single_input device Id torch autograd Function staticmethod forward input pass staticmethod setup_context ctx inputs output pass staticmethod backward ctx grad_output grad_saved pass staticmethod vmap info in_dims input assertEqual in_dims input in_dims B = x = torch randn B device=device vmap Id apply in_dims= x vmap Id apply in_dims= x test_in_dims_multiple_inputs device Id torch autograd Function staticmethod forward x y pass staticmethod setup_context ctx inputs output pass staticmethod backward ctx grad_output grad_saved pass staticmethod vmap info in_dims x y assertEqual in_dims assertTrue isinstance in_dims tuple assertTrue isinstance in_dims list x y in_dims x = torch randn device=device vmap Id apply x x x test_skips_empty_layer device Id torch autograd Function staticmethod forward input input staticmethod setup_context ctx inputs output pass staticmethod backward ctx grad_output grad_saved pass staticmethod vmap info in_dims input raise RuntimeError expected called f x y = torch tensor y = Id apply y x x = torch randn vmap f x test_none_returns device Zeros torch autograd Function staticmethod forward input torch zeros input shape device=input device staticmethod setup_context ctx inputs output pass staticmethod vmap info in_dims input assert in_dims == torch zeros input shape device=input device None B = x = torch randn B y = vmap Zeros apply x assertEqual y torch zeros_like x TwoZeros torch autograd Function staticmethod forward input r = torch zeros input shape device=input device r r staticmethod setup_context ctx inputs output pass staticmethod vmap info in_dims input assert in_dims == r = torch zeros input shape device=input device r r None B = x = torch randn B result = vmap TwoZeros apply x assertTrue isinstance result tuple y z = result assertEqual y torch zeros_like x assertEqual z torch zeros_like x test_should_have_two_returns device Zeros torch autograd Function staticmethod forward input r = torch zeros input shape device=input device r staticmethod setup_context ctx inputs output pass staticmethod vmap info in_dims input r = torch zeros input shape device=input device r B = x = torch randn B assertRaisesRegex RuntimeError have two returns vmap Zeros apply x TwoZeros torch autograd Function staticmethod forward input r = torch zeros input shape device=input device r r staticmethod setup_context ctx inputs output pass staticmethod vmap info in_dims input r = torch zeros input shape device=input device r r B = x = torch randn B assertRaisesRegex RuntimeError have two returns vmap Zeros apply x test_incompatible_out_dims_error_msg device Zeros torch autograd Function staticmethod forward input r = torch zeros input shape device=input device r staticmethod setup_context ctx inputs output pass staticmethod vmap info in_dims input r = torch zeros input shape device=input device r None B = x = torch randn B assertRaisesRegex RuntimeError returned incompatible vmap Zeros apply x Zeros torch autograd Function staticmethod forward input r = torch zeros input shape device=input device r staticmethod setup_context ctx inputs output pass staticmethod vmap info in_dims input r = torch zeros input shape device=input device r None B = x = torch randn B assertRaisesRegex RuntimeError returned incompatible vmap Zeros apply x test_kwarg_only_tensors device assertRaisesRegex NotImplementedError kwarg-only Tensor args MyClass torch autograd Function staticmethod forward x y x + y staticmethod setup_context ctx inputs output pass staticmethod vmap info in_dims x y assert in_dims == x + y x = torch randn y = torch randn vmap MyClass apply x y=y markDynamoStrictTest TestVmapOfGrad TestCase test_per_sample_grads_inplace_view device compute_loss weight x t x = x mm weight y = x squeeze_ y - t sum weight = torch randn device=device x = torch randn device=device t = torch randn device=device result = vmap partial grad compute_loss weight x t expected = grad compute_loss weight x i t i i range expected = torch stack expected TODO Check rtol problem assertEqual result expected atol= rtol= e- test_new_zeros_materializes_tensor device N = C = foo y x result = x new_zeros C result copy_ y result sum x = torch randn N device=device y = torch randn N C device=device result = vmap grad foo y x assertEqual result torch ones_like y test_new_empty_materializes_tensor device N = C = foo y x result = x new_empty C result copy_ y result sum x = torch randn N device=device y = torch randn N C device=device result = vmap grad foo y x assertEqual result torch ones_like y test_per_sample_grads_simple device compute_loss weight x t y = x weight y - t sum weight = torch randn device=device x = torch randn device=device t = torch randn device=device result = vmap partial grad compute_loss weight x t expected = grad compute_loss weight x i t i i range expected = torch stack expected TODO Check rtol problem assertEqual result expected atol= rtol= e- _compare_expected_and_result expected result mechanism mechanism == make_functional expected = zip expected expected = tuple torch stack shards shards expected r e zip result expected assertEqual r e atol= rtol= e- assert mechanism == functional_call expected = k tuple d k d expected k v expected items expected = k torch stack shards k shards expected items key result assertEqual result key expected key atol= rtol= e- tf _on_and_off parametrize mechanism make_functional functional_call test_per_sample_grads_embeddingnet device mechanism SampleNet nn Module __init__ vocab_size int super __init__ emb = nn Embedding vocab_size fc = nn Linear fc = nn Linear forward x x = emb x x = torch transpose x - - x = torch mean x - x = fc x x = F relu x x = fc x x name SampleNet Create our inputs vocab_size = batch_shape = words_per_sentence = data = torch randint vocab_size batch_shape words_per_sentence device=device targets = torch randint batch_shape device=device Construct our module net = SampleNet vocab_size device=device criterion = nn CrossEntropyLoss net_func weights = _get_weights_and_functional_call net mechanism compute_loss weights data target output = net_func weights data result = criterion output target result expected = grad compute_loss weights data i targets i i range result = vmap partial grad compute_loss weights data targets _compare_expected_and_result expected result mechanism test_log_softmax device x = torch randn device=device v = torch randn device=device foo x v _ vjp_fn = vjp partial torch log_softmax dim=- x vjp_fn v result = vmap foo None x v v = v expand_as x x requires_grad_ output = torch log_softmax x dim=- output backward v assertEqual result x grad jacrev_and_jacfwd = parametrize jacapi subtest jacrev name= jacrev subtest jacfwd name= jacfwd FIXME_jacrev_only = parametrize jacapi subtest jacrev name= jacrev markDynamoStrictTest TestJac VmapTearDownMixin TestCase jacrev_and_jacfwd test_simple device jacapi x = torch randn device=device y = jacapi torch sin x expected = torch diagflat x cos assert torch allclose y expected jacrev_and_jacfwd test_simple_not_flat device jacapi x = torch randn device=device y = jacapi torch sin x expected = torch diagflat x view - cos expected = expected view assert torch allclose y expected jacrev_and_jacfwd test_take device jacapi x = torch rand func x y = torch ones dtype=torch long z = torch take x y z assertEqual jacrev func x torch autograd functional jacobian func x jacrev_and_jacfwd test_diff_numel device jacapi x = torch randn device=device Tensor - Tensor f x x unsqueeze - y = jacapi f x assertEqual y shape expected = x new_zeros expected = expected = expected = assertEqual y expected jacrev_and_jacfwd test_vmap_on_jac_simple device jacapi x = torch randn device=device y = vmap jacapi torch sin x expected = torch stack torch diagflat x i cos i range assert torch allclose y expected jacrev_and_jacfwd test_nested_jac_simple device jacapi foo x x sin sum x = torch randn device=device y = jacapi jacapi foo x expected = torch diagflat -x sin assert torch allclose y expected jacrev_and_jacfwd test_multiple_args device jacapi x = torch randn device=device y = torch randn device=device z = jacapi torch multiply argnums= x y expected = torch diagflat x assert torch allclose z expected jacrev_and_jacfwd test_multiple_outputs_multiple_argnums device jacapi f x y x + y x + y x = torch randn device=device y = torch randn device=device z = jacapi f argnums= x y expected_out _x = torch diagflat torch full_like x expected_out _y = torch diagflat torch full_like y expected_out _x = torch diagflat torch full_like x expected_out _y = torch diagflat torch full_like y assertEqual len z assertTrue isinstance z tuple assertEqual len z assertTrue isinstance z tuple assertEqual z expected_out _x assertEqual z expected_out _y assertEqual z expected_out _x assertEqual z expected_out _y jacrev_and_jacfwd test_multiple_outputs_single_argnums device jacapi f x y x + y x + y x = torch randn device=device y = torch randn device=device expected_out _x = torch diagflat torch full_like x expected_out _x = torch diagflat torch full_like x z = jacapi f argnums= x y assertEqual len z assertTrue isinstance z tuple assertEqual z expected_out _x expected_out _x z = jacapi f argnums= x y assertEqual len z assertTrue isinstance z tuple assertTrue isinstance z tuple assertEqual z expected_out _x expected_out _x jacrev_and_jacfwd test_multiple_outputs_pytree device jacapi f x y left x + y right x + y x = torch randn device=device y = torch randn device=device z = jacapi f argnums= x y expected_left_x = torch diagflat torch full_like x expected_left_y = torch diagflat torch full_like y expected_right_x = torch diagflat torch full_like x expected_right_y = torch diagflat torch full_like y expected = left expected_left_x expected_left_y right expected_right_x expected_right_y assertTrue isinstance z dict assertTrue isinstance z left tuple assertTrue isinstance z right tuple assertEqual z expected jacrev_and_jacfwd test_multiple_inputs_pytree device jacapi f b c = + + b + c x = torch randn device=device args = x x x x result = jacapi f argnums= args expected = torch tensor device=device torch tensor device=device torch tensor device=device torch tensor device=device assertEqual result expected result = jacapi f argnums= args expected = torch tensor device=device torch tensor device=device assertEqual result expected result = jacapi f args expected = torch tensor device=device torch tensor device=device assertEqual result expected jacrev_and_jacfwd test_dimensionality device jacapi f x x x = torch randn device=device result = jacapi f x assertEqual result dim assertEqual result torch ones_like x x = torch randn device=device result = jacapi f x assertEqual result dim assertEqual result x new_ones jacrev_and_jacfwd test_aux_tensor device jacapi f x y = x clone y y cos x = torch randn device=device result aux = jacapi f has_aux=True x assertEqual result torch eye device=device assertEqual aux x cos jacrev_and_jacfwd test_aux_pytree device jacapi f x y = x clone y y cos b y tan x = torch randn device=device result aux = jacapi f has_aux=True x assertEqual result torch eye device=device _ expected_aux = f x assertEqual aux expected_aux aux abc assertRaisesRegex RuntimeError r Expected tensors got unsupported type _ = jacapi lambda x x aux has_aux=True x assertRaisesRegex RuntimeError r Expected tensors got unsupported type _ = jacapi lambda x x x aux has_aux=True x jacrev_and_jacfwd test_outputs_can_any_pytree device jacapi x = torch randn device=device output None assertRaisesRegex RuntimeError r vjp &#124; jvp + Expected f function has non-empty output jacapi lambda _ output x output True abc assertRaisesRegex RuntimeError r vjp &#124; jvp + expected f\ \ primals\ only tensors jacapi lambda _ output x Check list output out = jacapi lambda x x x sum x assert isinstance out list len out == Check dict output out = jacapi lambda x x x xsum x sum x assert isinstance out dict len out == xsum out composite_output x out = x sum out x out x out out = jacapi composite_output x assert isinstance out list assert isinstance out tuple isinstance out dict jacrev_and_jacfwd test_multiple_inputs_outputs_pytree device jacapi f b c = + foo b + c x = torch randn device=device zero = torch zeros device=device args = x x x x result = jacapi f args expected = torch tensor device=device torch tensor device=device foo zero zero assertEqual result expected result = jacapi f argnums= args expected = torch tensor device=device torch tensor device=device foo zero zero assertEqual result expected result = jacapi f argnums= args expected = torch tensor device=device torch tensor device=device zero foo zero zero torch tensor device=device assertEqual result expected jacrev_and_jacfwd test_multiple_inputs_outputs_pytree_multidim device jacapi f dct = dct b = dct b c sin d b cos x = torch randn device=device args = x b x result = jacapi f args expected = c x cos diagflat b x new_zeros d x new_zeros b -x sin diagflat assertEqual result expected jacrev_and_jacfwd test_unrelated_input device jacapi f x y x x = torch randn device=device y = torch randn device=device result = jacapi f argnums= x y expected = torch eye device=device view expected = y new_zeros expected = expected expected assertTrue isinstance result tuple assertEqual result expected jacrev_and_jacfwd test_unrelated_output device jacapi y = torch randn device=device f x y x = torch randn device=device result = jacapi f x expected = x new_zeros assertEqual result expected jacrev_and_jacfwd test_empty_output device jacapi x = torch randn device=device y = torch randn device=device f x y assertRaisesRegex RuntimeError xpected jacapi f x y jacrev_and_jacfwd test_argnums_tuple device jacapi x = torch randn device=device y = torch randn device=device z = jacapi torch multiply argnums= x y expected = torch diagflat y expected = torch diagflat x assert len z == assert torch allclose z expected assert torch allclose z expected jacrev_and_jacfwd test_argnums_effect_on_return device jacapi x = torch randn device=device y = torch randn device=device z = jacapi torch multiply argnums= x y expected = torch diagflat y assert isinstance z tuple assert len z == assert torch allclose z expected x = torch randn device=device y = torch randn device=device z = jacapi torch multiply argnums= x y expected = torch diagflat y assert isinstance z torch Tensor assert torch allclose z expected jacrev_and_jacfwd test_argnums_defaults_to_zero device jacapi f x y x + y x = torch randn device=device y = torch randn device=device z = jacapi f x y expected = torch diagflat torch full_like x assertEqual z expected jacrev_and_jacfwd test_empty_argnums device jacapi x = torch randn device=device assertRaisesRegex RuntimeError must non-empty jacapi torch sin argnums= x jacrev_and_jacfwd test_out_of_bounds_argnums device jacapi x = torch randn device=device assertRaisesRegex RuntimeError only positional inputs jacapi torch sin argnums= x jacrev_and_jacfwd test_negative_argnums device jacapi x = torch randn device=device assertRaisesRegex RuntimeError only positional inputs jacapi torch sin argnums=- x jacrev_and_jacfwd test_repeated_argnums device jacapi x = torch randn device=device assertRaisesRegex RuntimeError must unique jacapi torch sin argnums= x jacrev_and_jacfwd test_float_argnums device jacapi x = torch randn device=device assertRaisesRegex RuntimeError must int Tuple jacapi torch sin argnums= x assertRaisesRegex RuntimeError must int jacapi torch multiply argnums= x x test_hessian_simple device f x x sin x = torch randn device=device hessian f x _test_against_reference f inputs jacapi foo inputs f inputs expected = torch autograd functional jacobian f inputs result = jacapi foo inputs assertEqual result expected jacrev_and_jacfwd test_against_reference_simple device jacapi f x x x = torch randn device=device _test_against_reference f x jacapi jacrev_and_jacfwd test_against_reference_multi_input device jacapi f x y x cos x y sin x = torch randn device=device y = torch randn device=device _test_against_reference f x y jacapi jacrev_and_jacfwd test_against_reference_multi_input_multi_output device jacapi f x y x x y x x sum y y sum x = torch randn device=device y = torch randn device=device _test_against_reference f x y jacapi jacrev_and_jacfwd test_against_reference_unrelated_outputs device jacapi f x y x y x y x = torch randn device=device y = torch randn device=device _test_against_reference f x y jacapi jacrev_and_jacfwd test_against_reference_zero_dim device jacapi zero-dim output f x y x sum y sum x y x = torch randn device=device y = torch randn device=device _test_against_reference f x y jacapi zero-dim input g x torch stack x x x x = torch randn device=device _test_against_reference g x jacapi Mixed zero-dim input zero-dim output h x y y sum x y x = torch randn device=device y = torch randn device=device _test_against_reference h x y jacapi jacrev_and_jacfwd test_against_reference_correctness_different_devices device jacapi f x y x y x y device=device x = torch randn y = torch randn _test_against_reference f x y jacapi jacrev_and_jacfwd test_against_reference_default_arg device jacapi f x y z= x y z x = torch randn device=device y = torch randn device=device _test_against_reference f x y jacapi jacrev_and_jacfwd test_inplace device jacapi f x y y copy_ x y out = jacapi f argnums= x differentiable x y = torch randn device=device torch randn device=device assertEqual out x y torch eye y shape testing tuple argnums example raised issue originally g x y z x = y torch vstack x sum z sum out = jacapi g argnums= x y z = torch randn device=device torch randn device=device torch randn device=device expected_out = torch zeros device=device torch zeros device=device expected_out = y top left corner expected_out = z bottom right corner out_val = out x y z assertEqual out_val expected_out parametrize _preallocate_and_copy True False test_chunk_jacrev device _preallocate_and_copy x = torch randn device=device y = torch randn device=device f x y x sin x + y x + x sum chunk_size expected = jacrev f argnums= x y actual = jacrev f argnums= chunk_size=chunk_size _preallocate_and_copy=_preallocate_and_copy x y assertEqual actual expected err_msg = jacrev ` chunk_size ` should greater than assertRaisesRegex ValueError err_msg jacrev f argnums= chunk_size= x y assertRaisesRegex ValueError err_msg jacrev f argnums= chunk_size=- x y parametrize _preallocate_and_copy True False test_chunk_jacrev_composition device _preallocate_and_copy x = torch randn device=device chunk_size = f x x sin x x + x sum expected = vmap jacrev jacrev f x actual = vmap jacrev jacrev f chunk_size=chunk_size _preallocate_and_copy=_preallocate_and_copy chunk_size=chunk_size x assertEqual actual expected https github com pytorch pytorch issues xfailIfTorchDynamo parametrize _preallocate_and_copy True False test_chunk_jacrev_chunksize_one device _preallocate_and_copy With chunk_size= we shouldn t ` vmap ` hence limited s constraints x = torch randn device=device Function Dynamic Op Backward This should cause jacrev vmap vjp fail IdentityWithDynamicBackwardOp torch autograd Function staticmethod forward input input staticmethod setup_context ctx inputs output pass staticmethod backward ctx grad_output dynamic op backward pass grad_output nonzero grad_output f x IdentityWithDynamicBackwardOp apply x With ` chunk_size= ` we don t use vmap So following should work jacfn = jacrev f chunk_size= _preallocate_and_copy=_preallocate_and_copy actual = jacfn x expected = torch autograd functional jacobian f x vectorize=False assertEqual actual expected Should fail ` chunk_size= ` msg = r vmap We do support batching operators can output dynamic shape assertRaisesRegex RuntimeError msg jacrev f chunk_size= _preallocate_and_copy=_preallocate_and_copy x test_complex_error device Verify complex input raises error C - C fn x x conj x = torch randn device=device dtype=torch cfloat assertRaisesRegex RuntimeError jacrev Expected all inputs jacrev fn x assertRaisesRegex RuntimeError jacfwd Expected all inputs jacfwd fn x Verify complex output raises error R - C fn x torch conj x j x = torch randn device=device dtype=torch float assertRaisesRegex RuntimeError jacrev Expected all outputs jacrev fn x assertRaisesRegex RuntimeError jacfwd Expected all outputs jacfwd fn x jacrev_and_jacfwd test_jac_with_non_tensor_args device jacapi f t int_x t + int_x t = torch randn device=device actual = jacapi f t expected = torch autograd functional jacobian partial f int_x= t assertEqual actual expected markDynamoStrictTest TestHessian TestCase _test_against_reference f inputs foo inputs f inputs expected = torch autograd functional hessian f inputs result = hessian foo inputs assertEqual result expected test_hessian_vectorize_correctness_simple device f x x sum x = torch randn device=device _test_against_reference f x test_hessian_vectorize_correctness_multi_input device f x y z x relu x y sin z sum x = torch randn device=device y = torch randn device=device z = torch randn device=device _test_against_reference f x y z test_hessian_vectorize_correctness_unrelated_outputs device output unrelated one input f x y x sum x = torch randn device=device y = torch randn device=device _test_against_reference f x y output unrelated all inputs f x y torch ones x = torch randn device=device y = torch randn device=device _test_against_reference f x y test_jacfwd_different_levels device Test case https github com pytorch functorch issues b = n = d = x = torch randn b n d device=device x = x A = torch randn b d d device=device loss A x x x _hat = A x T T res = x - x _hat res_sqr = res res_sqr sum hess = vmap jacrev jacrev loss A x x hess = vmap hessian loss A x x assertEqual hess hess markDynamoStrictTest TestJvp TestCase test_inplace_on_captures device x = torch tensor device=device captured = torch randn device=device foo x captured copy_ x x captured sum assertRaisesRegex RuntimeError mutate captured Tensor grad foo x test_simple device x = torch randn device=device t = torch randn device=device result = jvp torch sin x t expected = x sin x cos t assertTrue isinstance result tuple assertEqual result expected test_multiple_inputs device x = torch randn device=device y = torch randn device=device tx = torch randn device=device ty = torch randn device=device f x y x y result = jvp f x y tx ty expected = x y y tx + x ty assertTrue isinstance result tuple assertEqual result expected test_pytree_inputs device f x y z b = x + b + y + z one = torch tensor device=device primal_outs tangent_outs = jvp f one one one one one one one one assertEqual primal_outs one assertEqual tangent_outs one test_pytree_inputs_error_cases device f x x one = torch tensor device=device assertRaisesRegex RuntimeError Expected primals tuple jvp f one one assertRaisesRegex RuntimeError same python structure jvp f one one one one one assertRaisesRegex RuntimeError only contain Tensors jvp f one one one one one assertRaisesRegex RuntimeError only contain Tensors jvp f one one one one assertRaisesRegex RuntimeError least one Tensor jvp f test_unrelated_input device f x y x x = torch randn device=device y = torch randn device=device tx = torch randn device=device ty = torch randn device=device result = jvp f x y tx ty expected = x tx assertTrue isinstance result tuple assertEqual result expected test_unrelated_output device y = torch randn device=device f x y x = torch randn device=device tx = torch randn device=device result = jvp f x tx expected = y torch zeros_like y assertTrue isinstance result tuple assertEqual result expected test_strict_mode device y = torch randn device=device f x x y x = torch randn device=device tx = torch randn device=device assertRaisesRegex RuntimeError strict jvp f x tx strict=True test_multiple_outputs device x = torch randn device=device t = torch randn device=device f x torch sin x torch cos x result = jvp f x t expected = f x x cos t -x sin t assertTrue isinstance result tuple assertEqual result expected test_multiple_inputs_outputs device x = torch randn device=device y = torch randn device=device tx = torch randn device=device ty = torch randn device=device f x y x + y x + y result = jvp f x y tx ty expected = f x y f tx ty assertTrue isinstance result tuple assertEqual result expected test_jvp_new_tensor f x y = x new_tensor x + y x = torch rand tangents = torch zeros_like x actual = jvp f x tangents expected = f x torch zeros_like x assertEqual actual expected test_primals_tangents_length_mismatch device x = torch randn device=device t = torch randn device=device msg = same python structure assertRaisesRegex RuntimeError msg jvp torch sin x t t assertRaisesRegex RuntimeError msg jvp torch sin x x t t t test_nonempty_primals_and_tangents device assertRaisesRegex RuntimeError least one Tensor jvp torch sin test_inputs_are_tuples_of_tensors device x = torch randn device=device t = torch randn device=device assertRaisesRegex RuntimeError tuple jvp torch sin x t assertRaisesRegex RuntimeError same python structure jvp torch sin x t assertRaisesRegex RuntimeError same python structure jvp torch sin x t assertRaisesRegex RuntimeError only contain Tensors jvp torch sin t assertRaisesRegex RuntimeError only contain Tensors jvp torch sin x test_outputs_can_any_pytree device x = torch randn device=device t = torch randn device=device output None assertRaisesRegex RuntimeError r jvp\ f primals tangents\ Expected f function has non-empty output jvp lambda _ output x t output True abc assertRaisesRegex RuntimeError r jvp\ f primals tangents\ expected f\ \ primals\ only tensors jvp lambda _ output x t Check list output out = jvp lambda x x x sum x t i range assert isinstance out i list len out i == Check dict output out = jvp lambda x x x xsum x sum x t i range assert isinstance out i dict len out i == xsum out i composite_output x out = x sum out x out x out out = jvp composite_output x t i range assert isinstance out i list assert isinstance out i tuple isinstance out i dict test_aux_tensor device x = torch randn device=device t = torch randn device=device assertRaisesRegex RuntimeError r jvp\ f primals tangents\ output function f should tuple jvp lambda t t t x t has_aux=True assertRaisesRegex RuntimeError r jvp\ f primals tangents\ output function f should tuple jvp lambda t t t + t + x t has_aux=True f z y = z sin y z cos out jvp_out aux = jvp f x t has_aux=True assertEqual aux x cos assertEqual out x sin assertEqual jvp_out t x cos test_aux_pytree device f x y = x sin y x cos b x tan x = torch randn device=device t = torch randn device=device out jvp_out aux = jvp f x t has_aux=True expected_out expected_aux = f x assertEqual out expected_out assertEqual aux expected_aux assertEqual jvp_out t x cos aux abc assertRaisesRegex RuntimeError r Expected tensors got unsupported type _ = jvp lambda x x aux x t has_aux=True assertRaisesRegex RuntimeError r Expected tensors got unsupported type _ = jvp lambda x x x aux x t has_aux=True test_autograd_function_disables_fwd_grad device Sanity check We don t really assume anywhere so s fine breaks one day MySquare torch autograd Function staticmethod forward ctx x enabled = fwAD _is_fwd_grad_enabled assertFalse enabled x x staticmethod backward ctx gx gx x = torch randn requires_grad=True MySquare apply x test_disable_fwd_grad_outside device x = torch randn device=device t = torch ones_like x fwAD _set_fwd_grad_enabled False _ y = jvp torch sin x t assertEqual y x cos test_disable_fwd_grad_inside device f x fwAD _set_fwd_grad_enabled False shift = x x - shift x = torch randn device=device t = torch ones_like x _ y = jvp f x t assertEqual y x _ y = jvp lambda x jvp f x t x t assertEqual y test_disable_fwd_grad_mixed device f x fwAD _set_fwd_grad_enabled False shift = x x - shift x = torch randn device=device t = torch ones_like x fwAD _set_fwd_grad_enabled True _ y = jvp f x t assertEqual y x test_jvp_inside_autograd_function device MySin torch autograd Function staticmethod forward ctx x t = torch ones_like x _ neg_sin_x = jvp torch cos x t ctx save_for_backward x -neg_sin_x staticmethod backward ctx gx x = ctx saved_tensors t = torch ones_like x _ cos_x = jvp torch sin x t gx cos_x x = torch randn device=device requires_grad=True y = MySin apply x assertEqual y x sin gx = torch autograd grad y x assertEqual gx x cos test_zerotensor_vmapjvp_interaction device dummy = torch ones x = torch randn x_tangent = torch randn push_jvp dummy x result = jvp torch cov x x_tangent result Should error vmap vmap push_jvp None dummy x markDynamoStrictTest TestLinearize TestCase dtypes torch float test_linearize_basic device dtype x_p = make_tensor device=device dtype=dtype x_t = make_tensor device=device dtype=dtype fn x x cos actual_output jvp_fn = linearize fn x_p actual_jvp = jvp_fn x_t expected_output expected_jvp = jvp fn x_p x_t assertEqual actual_output expected_output assertEqual actual_jvp expected_jvp dtypes torch float unittest skipIf TEST_CUDA_MEM_LEAK_CHECK Leaking memory see https github com pytorch pytorch pull example test_linearize_return device dtype x_p = make_tensor device=device dtype=dtype x_t = make_tensor device=device dtype=dtype fn x x cos x sum actual_output jvp_fn = linearize fn x_p actual_jvp = jvp_fn x_t expected_output expected_jvp = jvp fn x_p x_t assertEqual actual_output expected_output assertEqual actual_jvp expected_jvp dtypes torch float unittest skipIf TEST_CUDA_MEM_LEAK_CHECK Leaking memory see https github com pytorch pytorch pull example test_linearize_composition_vmap device dtype x_p = make_tensor device=device dtype=dtype x_t = make_tensor device=device dtype=dtype fn x x cos x sum _ jvp_fn = linearize fn x_p actual_batched_jvp = vmap jvp_fn x_t jvp_fn x_t jvp fn x_p x_t expected_batched_jvp = vmap jvp_fn x_t assertEqual actual_batched_jvp expected_batched_jvp dtypes torch float unittest skipIf TEST_CUDA_MEM_LEAK_CHECK Leaking memory see https github com pytorch pytorch pull example test_linearize_composition_grad device dtype x_p = make_tensor device=device dtype=dtype x_t = make_tensor device=device dtype=dtype fn x z = torch ones device=device dtype=dtype grad lambda x z x x _ jvp_fn = linearize fn x_p actual_batched_jvp = jvp_fn x_t jvp_fn x_t jvp fn x_p x_t expected_batched_jvp = jvp_fn x_t assertEqual actual_batched_jvp expected_batched_jvp dtypes torch float unittest skipIf TEST_CUDA_MEM_LEAK_CHECK Leaking memory see https github com pytorch pytorch pull example test_linearize_nested_input_nested_output device dtype x_p = make_tensor device=device dtype=dtype x_t = make_tensor device=device dtype=dtype y_p = make_tensor device=device dtype=dtype y_t = make_tensor device=device dtype=dtype z_p = make_tensor device=device dtype=dtype z_t = make_tensor device=device dtype=dtype fn arg x = arg x y = arg yz z = arg yz x sum b c y + z d x z y exp inp_p = x x_p yz y_p z_p inp_t = x x_t yz y_t z_t actual_output jvp_fn = linearize fn inp_p actual_jvp = jvp_fn inp_t expected_output expected_jvp = jvp fn inp_p inp_t assertEqual actual_output expected_output assertEqual actual_jvp expected_jvp onlyCUDA test_linearize_errors dtype = torch float device = torch device cpu x_p = make_tensor device=device dtype=dtype x_t = make_tensor device=device dtype=dtype fn x x sin _ jvp_fn = linearize fn x_p assertRaisesRegex RuntimeError have same argspec primals jvp_fn x_t x_t assertRaisesRegex RuntimeError flattened pytree doesn t match shape jvp_fn x_t unsqueeze assertRaisesRegex RuntimeError flattened pytree doesn t match dtype jvp_fn x_t torch double assertRaisesRegex RuntimeError flattened pytree doesn t match device jvp_fn x_t torch device cuda The tests here follow cases Forward Grad View inplace https github com pytorch pytorch blob master torch csrc autograd autograd_meta cpp#L -L markDynamoStrictTest TestVmapJvpInplaceView TestCase Case Forward Grad View inplace test_all_dual_no_view device B = push_jvp f inner x xt y yt jvp f x y xt yt inner f x y x copy_ y x x = torch randn B device=device xt = torch randn B device=device y = torch randn B device=device yt = torch randn B device=device out out_tangent = vmap push_jvp f in_dims= x xt y yt assertEqual out x movedim assertEqual out_tangent yt movedim x = torch randn B device=device xt = torch randn B device=device y = torch randn device=device yt = torch randn device=device out out_tangent = vmap push_jvp f in_dims= None None x xt y yt assertEqual out x movedim assertEqual out_tangent yt expand B Case Forward Grad View inplace test_all_dual_base_view_inplace device B = push_jvp f inner x xt y yt jvp f x y xt yt inner view propagate view base f x y view = x view copy_ y view x orig_x = torch randn B device=device orig_xt = torch randn B device=device x = orig_x clone xt = orig_xt clone y = torch randn B device=device yt = torch randn B device=device out out_tangent = vmap push_jvp f in_dims= x xt y yt expected_out = vmap f in_dims= orig_x clone y assertEqual out expected_out assertEqual out expected_out assertEqual out_tangent yt movedim expected_x_tangent = orig_xt movedim - clone expected_x_tangent copy_ yt movedim assertEqual out_tangent expected_x_tangent expected = orig_x movedim clone expected = y movedim assertEqual x movedim expected Case Forward Grad View inplace test_all_dual_base_inplace device B = push_jvp f inner x xt y yt jvp f x y xt yt inner Case view propagate base view f x y view = x x copy_ y x view x = torch randn B device=device xt = torch randn B device=device y = torch randn B device=device yt = torch randn B device=device out out_tangent = vmap push_jvp f in_dims= x clone xt y yt expected_out = vmap f in_dims= x clone y assertEqual out expected_out assertEqual out expected_out assertEqual out_tangent yt movedim assertEqual out_tangent yt movedim Case Forward Grad View inplace test_right_dual_view_prop device B = Changes view must propagate its base Also - x regular Tensor - y dual tensor f x y x = x clone view = x view copy_ y view x push_jvp x y yt jvp partial f x y yt x = torch randn B device=device y = torch randn B device=device yt = torch randn B device=device outs tangents = vmap push_jvp in_dims= x y yt expected_out = vmap f in_dims= x clone y assertEqual outs expected_out assertEqual outs expected_out assertEqual tangents yt movedim expected_tangent_ = torch zeros_like x movedim expected_tangent_ copy_ yt movedim assertEqual tangents expected_tangent_ Case Forward Grad View inplace test_right_dual_base_prop device B = Changes base must propagate all its views Also - x regular Tensor - y dual tensor f x y x = x clone view = x x copy_ y view x push_jvp x y yt jvp partial f x y yt x = torch randn B y = torch randn B yt = torch randn B outs tangents = vmap push_jvp in_dims= x y yt expected_out = vmap f in_dims= x y assertEqual outs expected_out assertEqual outs expected_out assertEqual tangents yt movedim assertEqual tangents yt movedim Use testing miscellaneous helper functions markDynamoStrictTest TestHelpers TestCase test_CtxWithSavedTensors_error_if_name_collision device x = torch randn device=device requires_grad=True y = torch randn device=device requires_grad=True A torch autograd Function staticmethod forward ctx x ctx _pt_inner_ctx = ctx save_for_backward x x staticmethod backward ctx gy wrapped = torch _functorch autograd_function CtxWithSavedTensors noqa F ctx y gy B torch autograd Function staticmethod forward ctx x ctx _pt_new_saved_tensors = ctx save_for_backward x x staticmethod backward ctx gy wrapped = torch _functorch autograd_function CtxWithSavedTensors noqa F ctx y gy out = A apply x assertRaisesRegex RuntimeError name collision out backward out = B apply x assertRaisesRegex RuntimeError name collision out backward test_CtxWithSavedTensors_nesting device CtxWithSavedTensors = torch _functorch autograd_function CtxWithSavedTensors x = torch randn device=device requires_grad=True y = torch randn device=device z = torch randn device=device A torch autograd Function staticmethod forward ctx x ctx save_for_backward x x staticmethod backward ctx gy ctx_y = CtxWithSavedTensors ctx y Can t use assertEqual because relies TLS available multithread autograd assert len ctx_y saved_tensors == assert torch allclose ctx_y saved_tensors y wrapped = CtxWithSavedTensors ctx_y z assert len wrapped saved_tensors == assert torch allclose wrapped saved_tensors z assert len ctx_y saved_tensors == assert torch allclose ctx_y saved_tensors y gy wrapped saved_tensors out = A apply x out backward assertEqual x grad z test_CtxWithSavedTensors_overrides_saved_tensors device x = torch randn device=device requires_grad=True A torch autograd Function staticmethod forward ctx x ctx save_for_backward x x staticmethod backward ctx gy The override can literally anything override = wrapped = torch _functorch autograd_function CtxWithSavedTensors ctx override assert wrapped saved_tensors == override gy out = A apply x out backward test_CtxWithSavedTensors_passthrough device x = torch randn device=device requires_grad=True y = torch randn device=device A torch autograd Function staticmethod forward ctx x y ctx save_for_backward x y x y staticmethod backward ctx gz The override can literally anything override = wrapped = torch _functorch autograd_function CtxWithSavedTensors ctx override assert wrapped needs_input_grad == ctx needs_input_grad assert wrapped needs_input_grad == ctx needs_input_grad wrapped foo = bar assert wrapped foo == bar assert ctx foo == bar gz gz out = A apply x y out backward test_debug_unwrap stuff = f x stuff append torch func debug_unwrap x x sin x = torch randn _ = vmap vmap f x assertEqual stuff x assertTrue stuff x test_reductify_leaf device reductify_leaf = torch _functorch autograd_function reductify_leaf B = grad_input None case output = reductify_leaf None None B assertIsNone output output = reductify_leaf None None None B assertIsNone output grad_input has bdim input does have bdim grad_input = torch randn B device=device output = reductify_leaf grad_input None B assertEqual output grad_input sum grad_input = torch randn B device=device output = reductify_leaf grad_input None B assertEqual output grad_input sum grad_input does have bdim input has bdim This can happen user returns fresh Tensor backward pass unrelated input grad_input = torch randn device=device output = reductify_leaf grad_input None B assertEqual output grad_input view expand B grad_input = torch randn device=device output = reductify_leaf grad_input None B assertEqual output grad_input view expand B sum grad_input has bdim input has bdim grad_input = torch randn B device=device output = reductify_leaf grad_input B assertEqual output grad_input movedim grad_input = torch randn B device=device output = reductify_leaf grad_input B assertEqual output grad_input movedim - sum sum markDynamoStrictTest TestComposability TestCase test_deprecation_vmap device functorch version API deprecated assertWarnsRegex FutureWarning Please use ` torch vmap ` vmap torch sin non-functorch version deprecated warnings catch_warnings warnings simplefilter error torch vmap torch sin Some these pass some these don t parametrize transform grad jacrev jacfwd grad_and_value hessian functionalize test_deprecation_transforms device transform api = getattr functorch transform new_api = getattr torch func transform functorch version API deprecated assertWarnsRegex FutureWarning f Please use ` torch func transform ` api torch sin non-functorch version deprecated warnings catch_warnings warnings simplefilter error new_api torch sin test_grad_grad device x = torch randn device=device y = grad grad torch sin x assertEqual y -x sin test_grad_vmap device foo x y = vmap torch sin x y sum x = torch randn device=device y = grad foo x assertEqual y x cos test_grad_vjp device x = torch randn device=device foo x _ vjp_fn = vjp torch sin x vjp_fn x sum y = grad foo x expected = grad lambda x x x cos sum x assertEqual y expected test_vmap_grad device x = torch randn device=device y = vmap grad torch sin x assertEqual y x cos test_vmap_vmap device x = torch randn device=device y = vmap vmap torch sin x assertEqual y x sin test_vmap_vjp device x = torch randn device=device _ vjp_fn = vjp torch sin x foo x _ vjp_fn = vjp torch sin x vjp_fn x y = vmap foo x assertEqual y vjp_fn x TODO there s very interesting error message when following CPU xs = torch randn device=device expected = torch stack vjp_fn x x xs result = vmap lambda x vjp_fn x xs assertEqual result expected test_vjp_grad device x = torch randn device=device y vjp_fn = vjp grad torch sin x assertEqual y x cos v = torch randn assertEqual vjp_fn v -x sin v test_vjp_vmap device x = torch randn device=device y vjp_fn = vjp vmap torch sin x assertEqual y x sin v = torch randn device=device assertEqual vjp_fn v x cos v test_vjp_vjp device x = torch randn device=device y vjp_fn = vjp torch sin x assertEqual y x sin y vjp_fn = vjp lambda x vjp_fn x x assertEqual y x x cos y = vjp_fn x Honestly IDK what result here least runs test_make_fx_vmap device f x torch sin x inp = torch randn f = vmap f fx_f = make_fx f inp new_inp = torch randn assertEqual fx_f new_inp f new_inp test_make_fx_jacrev device f x x sin sum inp = torch randn f = jacrev jacrev f fx_f = make_fx f inp new_inp = torch randn assertEqual fx_f new_inp f new_inp test_make_fx_vjp device f x torch sin x sum primals = torch randn _ vjp_fn = vjp f primals cotangent = torch randn fx_f = make_fx vjp_fn cotangent True True new_cotangent = torch randn assertEqual fx_f new_cotangent True True vjp_fn new_cotangent FIXME test fails Windows unittest skipIf IS_WINDOWS fails Windows needs investigation unittest skipIf IS_FBCODE can t subprocess fbcode redundant run test twice machine has GPUs onlyCPU test_no_warning_on_import_functorch device out = subprocess check_output sys executable -W always -c functorch stderr=subprocess STDOUT cwd=os path dirname os path realpath __file__ decode utf- assertEqual out test_requires_grad_inside_transform device f x x requires_grad_ x sin sum x = torch randn assertRaisesRegex RuntimeError Tensor requires_grad_ vmap f x assertRaisesRegex RuntimeError Tensor requires_grad_ grad f x assertRaisesRegex RuntimeError Tensor requires_grad_ vmap grad f x x = torch randn assertRaisesRegex RuntimeError Tensor requires_grad_ grad grad f x test_retain_grad_inside_transform device f x y = x sin y retain_grad y sum x = torch randn assertRaisesRegex RuntimeError Tensor retain_grad grad f x test_autograd_functional_jacrev_inside_transform device f x y = torch autograd functional jacobian lambda x x sin sum x y B = x = torch randn B assertRaisesRegex RuntimeError torch autograd functional vmap f x x = torch randn assertRaisesRegex RuntimeError torch autograd functional grad f x test_autograd_functional_vjp_inside_transform device f x y = torch autograd functional vjp lambda x x sin sum x y B = x = torch randn B assertRaisesRegex RuntimeError torch autograd functional vmap f x x = torch randn assertRaisesRegex RuntimeError torch autograd functional grad f x test_autograd_functional_jvp_inside_transform device f x t = torch ones_like x y = torch autograd functional jvp lambda x x sin sum x t y B = x = torch randn B assertRaisesRegex RuntimeError torch autograd functional vmap f x x = torch randn assertRaisesRegex RuntimeError torch autograd functional grad f x test_autograd_functional_jacfwd_inside_transform device f x y = torch autograd functional jacobian lambda x x sin sum x strategy= forward-mode vectorize=True y B = x = torch randn B assertRaisesRegex RuntimeError Batching rule implemented aten _make_dual vmap f x parametrize transform vmap grad jacrev jacfwd grad_and_value hessian functionalize test_autograd_function_no_setup_context device transform MySin torch autograd Function staticmethod forward ctx x ctx save_for_backward x x sin staticmethod backward ctx gy x = ctx saved_tensors gy x cos x = torch randn device=device transform = getattr functorch transform assertRaisesRegex RuntimeError must override setup_context transform MySin apply x Some these pass some these don t parametrize transform grad jacrev grad_and_value hessian test_transforms_dont_support_saved_tensor_hooks device transform f x torch sin x sum g x torch autograd graph save_on_cpu f x x = torch randn device=device transform == functionalize transform = functorch experimental functionalize transform = getattr functorch transform assertRaisesRegex RuntimeError saved tensor hooks torch autograd graph save_on_cpu transform f x assertRaisesRegex RuntimeError saved tensor hooks transform g x test_vjp_doesnt_support_saved_tensor_hooks device f x torch sin x sum g x torch autograd graph save_on_cpu f x x = torch randn device=device assertRaisesRegex RuntimeError saved tensor hooks torch autograd graph save_on_cpu vjp f x assertRaisesRegex RuntimeError saved tensor hooks vjp g x test_jvp_supports_saved_tensor_hooks device f x torch sin x sum g x torch autograd graph save_on_cpu f x x = torch randn device=device t = torch randn device=device smoke tests torch autograd graph save_on_cpu jvp f x t smoke tests jvp g x t test_can_use_functionalize_when_key_is_excluded device f x y = x clone y sin_ y x = torch randn device=device expected = f x _ExcludeDispatchKeyGuard DispatchKeySet DispatchKey Functionalize gm = make_fx functorch functionalize f x assertTrue sin_ gm code assertEqual gm x expected local_exclude_set = torch _C _dispatch_tls_local_exclude_set assertTrue local_exclude_set has DispatchKey Functionalize test_can_use_vmap_when_key_is_excluded device f x x sum x = torch randn device=device expected = vmap f x _ExcludeDispatchKeyGuard DispatchKeySet DispatchKey FuncTorchBatched result = vmap f x assertEqual result expected local_exclude_set = torch _C _dispatch_tls_local_exclude_set assertTrue local_exclude_set has DispatchKey FuncTorchBatched test_can_use_grad_when_key_is_excluded device f x x sin x = torch randn device=device expected = grad f x _ExcludeDispatchKeyGuard DispatchKeySet DispatchKey Autograd result = grad f x assertEqual result expected local_exclude_set = torch _C _dispatch_tls_local_exclude_set assertTrue local_exclude_set has DispatchKey Autograd markDynamoStrictTest TestMakeFunctional TestCase parametrize disable_autograd_tracking True False test_disable_autograd_tracking disable_autograd_tracking Foo nn Module __init__ - None super __init__ linear = nn Linear forward x x = linear x x mod = Foo _ params = make_functional mod disable_autograd_tracking=disable_autograd_tracking assertEqual len params param params assertEqual param requires_grad disable_autograd_tracking test_parameter_tying Foo nn Module __init__ - None super __init__ bias = nn Parameter torch randn linear = nn Linear linear bias = bias linear_tied = linear forward x x = linear x x = linear_tied x x = x + bias x torch manual_seed mod = Foo func _ = make_functional mod torch manual_seed mod = Foo _ params = make_functional mod assertEqual len params x = torch randn result = func params x expected = mod x assertEqual result expected test_buffer_tying Foo nn Module __init__ - None super __init__ bias = nn Parameter torch randn linear = nn Linear buffer = nn Buffer torch randn buffer_tied = buffer forward x x = linear x x = x + bias x = x + buffer x = x + buffer_tied x torch manual_seed mod = Foo func _ _ = make_functional_with_buffers mod torch manual_seed mod = Foo _ params buffers = make_functional_with_buffers mod assertEqual len params assertEqual len buffers x = torch randn result = func params buffers x expected = mod x assertEqual result expected parametrize disable_autograd_tracking True False test_with_buffers_disable_autograd_tracking disable_autograd_tracking Foo nn Module __init__ - None super __init__ linear = nn Linear buffer = nn Buffer torch randn forward x x = linear x x = x + buffer x mod = Foo _ params buffers = make_functional_with_buffers mod disable_autograd_tracking=disable_autograd_tracking assertEqual len params assertEqual len buffers param params assertEqual param requires_grad disable_autograd_tracking parametrize detach_params True False test_using_detach_functional_call detach_params Foo nn Module __init__ - None super __init__ linear = nn Linear buffer = nn Buffer torch randn forward x x = linear x x = x + buffer x params_dict mod named_params = mod named_parameters k v detach k v named_params detach_params dict named_params mod = Foo x = torch randn d = params_dict mod dict mod named_buffers out = functional_call mod d x assertEqual out grad_fn None detach_params test_parameter_tying_grad Foo nn Module __init__ - None super __init__ linear = nn Linear weight = linear weight bias = linear bias forward x x = linear x x = F linear x weight bias x x = torch randn torch manual_seed mod = Foo loss = mod x sum expected = torch autograd grad loss mod parameters mod = Foo fmod _ _ = make_functional_with_buffers mod torch manual_seed mod = Foo _ params buffers = make_functional_with_buffers mod compute_loss params buffers x fmod params buffers x sum result = grad compute_loss params buffers x assertEqual result expected test_parameter_tying_ensemble Foo nn Module __init__ - None super __init__ linear = nn Linear weight = linear weight bias = linear bias buffer = nn Buffer torch randn buffer_tied = buffer forward x x = linear x x = F linear x weight bias x = x + buffer x = x + buffer_tied x num_models = xs = torch randn num_models models = Foo _ range num_models fmodel _ _ = combine_state_for_ensemble models torch manual_seed models = Foo _ range num_models _ params buffers = combine_state_for_ensemble models result = vmap fmodel params buffers xs torch manual_seed models = Foo _ range num_models expected = torch stack model x model x zip models xs assertEqual result expected parametrize mechanism make_functional functional_call test_correctness_mnist mechanism Net nn Module __init__ - None super __init__ conv = nn Conv d kernel_size= conv = nn Conv d kernel_size= conv _drop = nn Dropout d fc = nn Linear fc = nn Linear forward x x = F relu F max_pool d conv x x = F relu F max_pool d conv _drop conv x x = x view - x = F relu fc x x = F dropout x training=self training x = fc x F log_softmax x x = torch randn torch manual_seed fnet _ = _get_weights_and_functional_call Net mechanism torch manual_seed _ params = _get_weights_and_functional_call Net mechanism result = fnet params x torch manual_seed net = Net expected = net x assertEqual result expected test_combine_state_for_ensemble_error in_features = out_features = models = assertRaisesRegex RuntimeError Expected least one model _ = combine_state_for_ensemble models num_models = models = torch nn Linear in_features out_features i range num_models models eval assertRaisesRegex RuntimeError same training eval mode _ = combine_state_for_ensemble models models = torch nn Linear in_features out_features i range num_models models = torch nn Conv d assertRaisesRegex RuntimeError models same _ = combine_state_for_ensemble models test_combine_state_for_ensemble_smoke in_features = out_features = num_models = models = torch nn Linear in_features out_features i range num_models _ = combine_state_for_ensemble models test_stack_module_state_smoke in_features = out_features = num_models = models = torch nn Linear in_features out_features i range num_models _ = stack_module_state models test_stack_module_state_leaf in_features = out_features = num_models = models = torch nn Linear in_features out_features i range num_models params buffers = stack_module_state models param params values assertTrue param requires_grad assertTrue param is_leaf test_stack_module_state_mismatch_error in_features = out_features = num_models = models = torch nn Linear in_features out_features i range num_models models weight requires_grad_ False assertRaisesRegex RuntimeError same requires_grad params buffers = stack_module_state models test_stack_module_state_error in_features = out_features = models = assertRaisesRegex RuntimeError stack_module_state Expected least one model _ = stack_module_state models num_models = models = torch nn Linear in_features out_features i range num_models models eval assertRaisesRegex RuntimeError stack_module_state same training eval mode _ = stack_module_state models models = torch nn Linear in_features out_features i range num_models models = torch nn Conv d assertRaisesRegex RuntimeError stack_module_state models same _ = stack_module_state models parametrize mechanism make_functional functional_call test_make_functional_state_correctly_returned_after_forward mechanism Net nn Module __init__ - None super __init__ linear = nn Linear forward x x = linear x x get_module_info mod mechanism == make_functional make_functional mod assert mechanism == functional_call mod dict mod named_parameters mod = Net func_mod params = get_module_info mod state func names_map mod = func_mod stateless_model mechanism == make_functional func_mod old_state_linear_weight = mod linear weight old_state_linear_bias = mod linear bias assertIsNotNone old_state_linear_weight assertIsNotNone old_state_linear_bias x = torch randn mechanism == make_functional func_mod params x assert mechanism == functional_call functional_call func_mod params x mod = func_mod stateless_model mechanism == make_functional func_mod new_state_linear_weight = mod linear weight new_state_linear_bias = mod linear bias assertIsNotNone new_state_linear_weight assertIsNotNone new_state_linear_bias assertEqual old_state_linear_weight new_state_linear_weight assertEqual old_state_linear_bias new_state_linear_bias markDynamoStrictTest TestExamplesCorrectness TestCase _update_params params grads alpha mechanism mechanism == make_functional params i - alpha grads i i range len params assert mechanism == functional_call k params k - alpha grads k k params parametrize mechanism make_functional functional_call test_maml_regression device mechanism ThreeLayerNet nn Module __init__ - None super __init__ fc = nn Linear relu = nn ReLU fc = nn Linear relu = nn ReLU fc = nn Linear forward x x = fc x x = relu x x = fc x x = relu x x = fc x x TODO should replace F mse_loss mse_loss x y torch mean x - y net params = _get_weights_and_functional_call ThreeLayerNet device mechanism K = num_tasks = alpha = sample_tasks outer_batch_size inner_batch_size Select amplitude phase task As = phases = _ range outer_batch_size As append np random uniform low= high= phases append np random uniform low= high=np pi get_batch xs ys = A phase zip As phases x = np random uniform low=- high= size= inner_batch_size y = A np sin x + phase xs append x ys append y torch tensor xs dtype=torch float device=device torch tensor ys dtype=torch float device=device x y = get_batch x y = get_batch x y x y get_loss_for_task use_transform x y x y inner_loss params x y f = net params x loss = mse_loss f y loss use_transform grads = grad inner_loss params x y loss = inner_loss params x y grad_params spec = tree_flatten params grads = torch autograd grad loss grad_params create_graph=True grads = tree_unflatten grads spec new_params = _update_params params grads alpha mechanism v_f = net new_params x mse_loss v_f y task = sample_tasks num_tasks K list_params = params mechanism == make_functional list params values Compute vmap+grad inner_losses = vmap partial get_loss_for_task True task task task task loss = sum inner_losses len inner_losses result_grads = torch autograd grad loss list_params Compute without vmap+grad inner_losses = get_loss_for_task False task i task i task i task i i range num_tasks loss = sum inner_losses len inner_losses expected_grads = torch autograd grad loss list_params assertEqual result_grads expected_grads parametrize mechanism make_functional functional_call test_maml_omniglot device mechanism TODO there appears precision issues float dtype = torch double TODO We don t support inplace relu inplace_relu = False n_way = n_inner_iter = num_tasks = real example uses batch norm s numerically unstable first iteration when near won t produce same gradients Uses group norm instead net = nn Sequential nn Conv d nn GroupNorm affine=True nn ReLU inplace=inplace_relu nn MaxPool d nn Conv d nn GroupNorm affine=True nn ReLU inplace=inplace_relu nn MaxPool d nn Conv d nn GroupNorm affine=True nn ReLU inplace=inplace_relu nn MaxPool d nn Flatten nn Linear n_way device dtype fnet params buffers = _get_weights_and_functional_call_with_buffers net mechanism net = params buffers fnet loss_for_task net n_inner_iter use_transform x_spt y_spt x_qry y_qry params buffers fnet = net querysz = x_qry size compute_loss new_params buffers x y logits = fnet new_params buffers x loss = F cross_entropy logits y loss new_params = params _ range n_inner_iter use_transform grads = grad compute_loss new_params buffers x_spt y_spt res = compute_loss new_params buffers x_spt y_spt grad_params spec = tree_flatten new_params grads = torch autograd grad res grad_params create_graph=True grads = tree_unflatten grads spec new_params = _update_params new_params grads e- mechanism qry_logits = fnet new_params buffers x_qry qry_loss = F cross_entropy qry_logits y_qry qry_acc = qry_logits argmax dim= == y_qry sum querysz qry_loss qry_acc Get some sample inputs x_spt = torch randn num_tasks dtype=dtype device=device y_spt = torch randint num_tasks device=device x_qry = torch randn num_tasks dtype=dtype device=device y_qry = torch randint num_tasks device=device compute vmap + grad compute_loss = partial loss_for_task net n_inner_iter True qry_losses _ = vmap compute_loss x_spt y_spt x_qry y_qry list_params = params mechanism == make_functional list params values result_grads = torch autograd grad qry_losses sum list_params compute without vmap + grad compute_loss = partial loss_for_task net n_inner_iter False losses = compute_loss x_spt i y_spt i x_qry i y_qry i i range num_tasks expected_grads = torch autograd grad sum losses list_params assertEqual result_grads expected_grads parametrize mechanism make_functional functional_call parametrize originally_track_running_stats True False test_update_batch_norm device originally_track_running_stats mechanism dtype = torch double inplace_relu = False classes = num_batches = net = nn Sequential nn Conv d nn BatchNorm d affine=True track_running_stats=originally_track_running_stats nn ReLU inplace=inplace_relu nn Flatten nn Linear classes device dtype replace_all_batch_norm_modules_ net transformed_net = net fnet params buffers = _get_weights_and_functional_call_with_buffers transformed_net mechanism criterion = nn CrossEntropyLoss compute_loss x y params buffers criterion fnet params buffers x y Get some sample inputs x = torch randn num_batches device=device dtype=dtype y = torch randint classes num_batches device=device compute some per sample grads vmap + grad result_grads = vmap grad compute_loss argnums= in_dims= None None x y params buffers compute some per sample grads without vmap + grad fnet params buffers = _get_weights_and_functional_call_with_buffers transformed_net mechanism flat_params spec = tree_flatten params expected_grads = torch autograd grad compute_loss x i y i params buffers flat_params i range num_batches expected_grads = torch stack shards shards zip expected_grads expected_grads = tree_unflatten expected_grads spec assertEqual result_grads expected_grads parametrize jac jacfwd jacrev test_lennard_jones_batched_jac device jac sigma = epsilon = jac = getattr functorch jac lennard_jones r epsilon sigma r - sigma r lennard_jones_force r Get magnitude LJ force -epsilon - sigma r + sigma r r = torch linspace sigma steps= requires_grad=True device=device drs = torch outer r torch tensor device=device norms = torch norm drs dim= reshape - training_energies = torch stack list map lennard_jones norms reshape - training_forces = torch stack force dr force dr zip map lennard_jones_force norms drs model = nn Sequential nn Linear nn Tanh nn Linear nn Tanh nn Linear nn Tanh nn Linear nn Tanh nn Linear device make_prediction model drs use_functorch norms = torch norm drs dim= reshape - energies = model norms use_functorch network_derivs = vmap jac model norms squeeze - forces = -network_derivs drs norms forces = r dr zip norms drs network_deriv = torch autograd functional jacobian model r create_graph=True force = -network_deriv dr r forces append force forces = torch cat forces energies forces loss_fn energies forces predicted_energies predicted_forces F mse_loss energies predicted_energies + F mse_loss forces predicted_forces energies forces = make_prediction model drs use_functorch=True loss = loss_fn training_energies training_forces energies forces result = torch autograd grad loss model parameters energies forces = make_prediction model drs use_functorch=False loss = loss_fn training_energies training_forces energies forces expected = torch autograd grad loss model parameters assertEqual result expected parametrize mechanism make_functional functional_call test_ensemble_regression device mechanism make_spirals n_samples noise_std= rotations= ts = torch linspace n_samples rs = ts thetas = rs rotations math pi signs = torch randint n_samples - labels = signs torch long xs = rs signs torch cos thetas + torch randn n_samples noise_std ys = rs signs torch sin thetas + torch randn n_samples noise_std points = torch stack xs ys dim= points device labels device points labels = make_spirals noise_std= MLPClassifier nn Module __init__ hidden_dim= n_classes= super __init__ hidden_dim = hidden_dim n_classes = n_classes fc = nn Linear hidden_dim fc = nn Linear hidden_dim n_classes forward x x = fc x x = F relu x x = fc x x = F log_softmax x - x loss_fn = nn NLLLoss func_model weights = _get_weights_and_functional_call MLPClassifier device mechanism train_step_fn use_transform weights batch targets lr= compute_loss weights batch targets output = func_model weights batch loss = loss_fn output targets loss use_transform grad_weights loss = grad_and_value compute_loss weights batch targets loss = compute_loss weights batch targets flat_weights spec = tree_flatten weights flat_grad_weights = torch autograd grad loss flat_weights grad_weights = tree_unflatten flat_grad_weights spec new_weights = _update_params weights grad_weights lr mechanism loss new_weights unpack train_result train_result train_result init_fn num_models models = tuple MLPClassifier device _ range num_models mechanism == make_functional combine_state_for_ensemble models stack_module_state models slice_weights batched_weights index tree_map lambda weight weight index detach requires_grad_ batched_weights batched_weights = init_fn num_models= parallel_train_step_fn = vmap partial train_step_fn True in_dims= None None result_loss result_weights = unpack parallel_train_step_fn batched_weights points labels loss weights = unpack train_step_fn False slice_weights batched_weights points labels loss weights = unpack train_step_fn False slice_weights batched_weights points labels expected_loss = torch stack loss loss weights spec = tree_flatten weights weights spec = tree_flatten weights assert spec == spec expected_weights = tuple torch stack w w w w zip weights weights expected_weights = tree_unflatten expected_weights spec assertEqual result_loss expected_loss assertEqual result_weights expected_weights parametrize dropout_layer subtest nn Dropout Dropout subtest nn AlphaDropout AlphaDropout subtest nn FeatureAlphaDropout FeatureAlphaDropout parametrize mechanism make_functional functional_call test_find_learning_rate_ensembling device dropout_layer mechanism This example mimics what user might do when trying find optimal learning rate They would want run bunch models same behavior including same dropout have them each run different learning rates Specifically example using same randomness vmap points labels = torch randn device=device torch randint device=device MLPClassifier nn Module __init__ hidden_dim= n_classes= super __init__ hidden_dim = hidden_dim n_classes = n_classes dropout = dropout_layer fc = nn Linear hidden_dim fc = nn Linear hidden_dim n_classes forward x x = dropout x x = torch flatten x start_dim= x = fc x x = F relu x x = fc x x = F log_softmax x - x loss_fn = nn NLLLoss func_model weights = _get_weights_and_functional_call MLPClassifier device mechanism train_step_fn weights batch targets lr compute_loss weights batch targets output = func_model weights batch loss = loss_fn output targets loss grad_weights loss = grad_and_value compute_loss weights batch targets new_weights = _update_params weights grad_weights lr mechanism mechanism = make_functional new_weights = list new_weights values NB looks weird because torch vmap must Tensors loss new_weights unpack train_result train_result train_result init_fn num_models og_model = MLPClassifier device models = tuple copy deepcopy og_model _ range num_models have same initialization mechanism == make_functional combine_state_for_ensemble models stack_module_state models batched_weights = init_fn num_models= parallel_train_step_fn = vmap train_step_fn in_dims= None None randomness= same lrs = torch tensor device=device result_loss result_weights = unpack parallel_train_step_fn batched_weights points labels lrs assertEqual result_loss result_loss assertNotEqual tuple weight weight result_weights tuple weight weight result_weights with_tf _off https github com pytorch pytorch issues unittest skipIf USE_TORCHVISION test requires torchvision parametrize mechanism make_functional functional_call test_resnet _per_sample_grads device mechanism torchvision models model = models __dict__ resnet pretrained=False norm_layer= lambda c nn GroupNorm min c c device criterion = nn CrossEntropyLoss reduction= sum avoid cross batch reductions loop comparison func_model weights = _get_weights_and_functional_call model mechanism compute_loss weights image target image = image unsqueeze target = target unsqueeze output = func_model weights image loss = criterion output target loss batch_size = images = torch randn batch_size device=device targets = torch randint batch_size device=device result_grads = vmap grad compute_loss in_dims= None weights images targets flat_weights spec = tree_flatten weights expected_grads = torch autograd grad compute_loss weights images i targets i flat_weights i range batch_size expected_grads = torch stack shards shards zip expected_grads expected_grads = tree_unflatten expected_grads spec assertEqual result_grads expected_grads atol= e- rtol= normalize_devices fx_g node fx_g graph nodes args = list node args idx arg enumerate args isinstance arg torch device args idx = cpu node args = tuple args new_kwargs = k v node kwargs items isinstance v torch device v = cpu new_kwargs k = v node kwargs = new_kwargs fx_g recompile fx_g markDynamoStrictTest TestFunctionalize TestCase _check_functionalize_correctness f inpt skip_vmap=False inpt = inpt clone inpt = inpt clone inpt = inpt clone expected_outputs = f inpt skip_vmap actual_outputs = functionalize f inpt actual_outputs = vmap functionalize f inpt unsqueeze squeeze Right now flavor functionalize also removes view ops isn t being used vmap That s because view _copy ops don t have batching rules yet although we should probably fix actual_outputs_view_copy = functionalize f remove= mutations_and_views inpt Check outputs same assertEqual actual_outputs expected_outputs assertEqual actual_outputs_view_copy expected_outputs Inputs might have been mutated f check they mutated properly assertEqual inpt inpt assertEqual inpt inpt test_simple_view device f x torch Tensor - torch Tensor tmp = torch ones device=device y = x view y add_ tmp x _check_functionalize_correctness f torch zeros device=device test_multioutput_view device f x torch Tensor - torch Tensor tmp = torch ones device=device y y = x split y _view = y diagonal y _view add_ tmp x _check_functionalize_correctness f torch zeros device=device test_inplace_view device f x torch Tensor - torch Tensor tmp = torch ones device=device y = x + x y = y transpose z = y z add_ tmp y _check_functionalize_correctness f torch zeros device=device skip_vmap=True See https github com pytorch functorch issues test_linear device f x y z - torch Tensor torch _C _nn linear x y z x = torch randn device=device y = torch randn device=device z = torch randn device=device out_expected = f x y z out_actual = functionalize f x y z assertEqual out_expected out_actual test_multioutput_inplace_slice_view device f x torch Tensor - torch Tensor tmp = torch ones device=device y = x view z = y reshape z = z transpose z unsqueeze_ z squeeze_ z z = z split z add_ tmp x See Note Fix vmap slice_scatter _check_functionalize_correctness f torch zeros device=device skip_vmap=True Ensure functionalize works List Optional Tensor arguments See fix discussion https github com pytorch pytorch pull test_functionalize_opt_tensor_list device f x torch Tensor indices torch Tensor - torch Tensor x indices inpta = torch ones device=device inptb = torch arange device=device out = f inpta inptb out = functionalize f inpta inptb assertEqual out out out = make_fx functionalize f inpta inptb assertExpectedInline out code \ forward x_ indices_ - torch Tensor index = torch ops aten index Tensor x_ indices_ x_ = indices_ = None index Ensure grad functionalize f works test_functionalize_grad device f x torch Tensor - torch Tensor tmp = torch ones device=device y = x + x z = y view y add_ tmp z sum inpt = torch ones device=device inpt = torch ones device=device out = grad f inpt out = grad functionalize f inpt assertEqual out out assertEqual inpt inpt unittest skipIf IS_FBCODE fails fbcode test_vmap_functionalize_jvp device f x torch Tensor - torch Tensor y = x + x z = y view - y add_ z jvp_wrapper x t jvp f x t x = torch randn device=device t = torch randn device=device out = vmap jvp_wrapper x t out = vmap functionalize jvp_wrapper x t assertEqual out out TODO move test into test_fake_tensor py once functionalize can used core tests test_functionalize_fake_tensors device f x torch Tensor - torch Tensor y = x detach y + y FakeTensorMode x = torch ones device=device requires_grad=True functionalize f x assertEqual x size test_functionalize_fx_simple device f x torch Tensor - torch Tensor tmp = torch ones device=device y = x view y add_ tmp x There s copy_ graph because input x mutated To preserve semantics functionalize needs propagate mutation fn = make_fx functionalize f remove= mutations_and_views out = fn torch zeros device=device out = normalize_devices out assertExpectedInline out code \ forward x_ - torch Tensor ones = torch ops aten ones default device = cpu pin_memory = False view_copy = torch ops aten view_copy default x_ add = torch ops aten add Tensor view_copy ones view_copy = ones = None view_copy_ = torch ops aten view_copy default add add = None view_copy_ = torch ops aten view_copy default view_copy_ view_copy_ = None copy_ = torch ops aten copy_ default x_ view_copy_ x_ = copy_ = None view_copy_ test_functionalize_fx_transpose_simple device f x torch Tensor - torch Tensor x transpose fn = make_fx functionalize f remove= mutations_and_views out = fn torch zeros device=device out = normalize_devices out assertExpectedInline out code \ forward x_ - torch Tensor transpose_copy = torch ops aten transpose_copy int x_ x_ = None transpose_copy test_functionalize_fx_out_op device f inpt torch Tensor - torch Tensor out = torch empty dtype=torch float torch add inpt inpt out=out out_view = out view out_view add_ out fn = make_fx functionalize f remove= mutations_and_views out = fn torch arange device=device dtype=torch float out = normalize_devices out assertExpectedInline out code \ forward inpt_ - torch Tensor empty = torch ops aten empty memory_format dtype = torch float device = cpu pin_memory = False empty = None add = torch ops aten add Tensor inpt_ inpt_ inpt_ = None view_copy = torch ops aten view_copy default add view_copy = None view_copy_ = torch ops aten view_copy default add add = None add_ = torch ops aten add Tensor view_copy_ view_copy_ = None view_copy_ = torch ops aten view_copy default add_ add_ = None view_copy_ = torch ops aten view_copy default view_copy_ view_copy_ = None view_copy_ test_functionalize_fx_multi_out_op device f inpt torch Tensor - torch Tensor mins = torch empty dtype=torch float maxs = torch empty dtype=torch float maxs_view = maxs view inpt_view = inpt view torch aminmax inpt_view dim= out= mins maxs_view maxs mins fn = make_fx functionalize f remove= mutations_and_views out = fn torch arange device=device dtype=torch float out = normalize_devices out assertExpectedInline out code \ forward inpt_ - torch Tensor empty = torch ops aten empty memory_format dtype = torch float device = cpu pin_memory = False empty = None empty_ = torch ops aten empty memory_format dtype = torch float device = cpu pin_memory = False view_copy = torch ops aten view_copy default empty_ empty_ = view_copy = None view_copy_ = torch ops aten view_copy default inpt_ inpt_ = None aminmax = torch ops aten aminmax default view_copy_ dim = view_copy_ = None getitem = aminmax getitem_ = aminmax aminmax = None view_copy_ = torch ops aten view_copy default getitem_ getitem_ = None view_copy_ = torch ops aten view_copy default view_copy_ view_copy_ = None view_copy_ getitem test_functionalize_fx_reapply_views_simple device f x torch Tensor - torch Tensor tmp = torch ones device=device y = x view y add_ tmp x out = make_fx functionalize f torch zeros device=device out = normalize_devices out assertExpectedInline out code \ forward x_ - torch Tensor ones = torch ops aten ones default device = cpu pin_memory = False view = torch ops aten view default x_ add = torch ops aten add Tensor view ones view = ones = None view_ = torch ops aten view default add add = None view_ = torch ops aten view default view_ view_ = None copy_ = torch ops aten copy_ default x_ view_ x_ = copy_ = None view_ test_functionalize_nonfunctional_output device global_out = torch ones device=device f - torch Tensor global_out out = make_fx functionalize f out = normalize_devices out assertExpectedInline out code \ forward - torch Tensor _tensor_constant = _tensor_constant _tensor_constant test_functionalize_optional_tensorlist device f b - torch Tensor index has OptionalTensorList arguments test here b = torch arange reshape b = torch ones dtype=torch long out = make_fx functionalize f b out = normalize_devices out assertExpectedInline out code \ forward a_ b_ - torch Tensor index = torch ops aten index Tensor a_ b_ a_ = b_ = None index unittest skipIf IS_FBCODE fails fbcode test_functionalize_optional_tensorlist device f b - torch Tensor See https github com pytorch pytorch pull torch ops aten index b = torch arange reshape b = torch ones dtype=torch long out = make_fx functionalize f b assertExpectedInline out code \ forward a_ b_ - torch Tensor unbind = torch ops aten unbind int b_ b_ = None getitem = unbind getitem_ = unbind unbind = None index = torch ops aten index Tensor a_ getitem getitem_ a_ = getitem = getitem_ = None index test_resize_program_inputs device f x x resize_ x fill_ fn = make_fx functionalize f out = fn torch zeros device=device out = normalize_devices out assertExpectedInline out code \ forward x_ resize = torch ops aten resize default x_ fill = torch ops aten fill Scalar resize resize = None resize_ = torch ops aten resize_ default x_ x_ = None copy_ = torch ops aten copy_ default resize_ fill resize_ = fill = copy_ = None None construct_sum_pyop MySum HigherOrderOperator __init__ super __init__ mysum __call__ args kwargs super __call__ args kwargs mysum = MySum mysum py_impl torch _C _functorch TransformType Vmap mysum_batch_rule interpreter x dim torch _C _functorch is_batchedtensor x interpreter lower x = x view_as x unnecessary just here test dispatch mysum x dim bdim = torch _C _functorch maybe_get_bdim x value = torch _C _functorch get_unwrapped x interpreter lower value = value movedim bdim result = mysum value dim + torch _C _functorch _add_batch_dim result interpreter level mysum py_impl torch _C _functorch TransformType Grad mysum_grad_rule interpreter x dim level = interpreter level MySum torch autograd function _SingleLevelFunction staticmethod forward ctx x dim ctx x_shape = x shape ctx dim = dim x = torch _C _functorch _unwrap_for_grad x level torch enable_grad interpreter lower x = x view_as x unnecessary just here test dispatch y = mysum x dim y = torch _C _functorch _wrap_for_grad y level y staticmethod backward ctx gy gy unsqueeze ctx dim expand ctx x_shape None enable_single_level_autograd_function MySum apply x dim mysum py_impl torch _C DispatchKey AutogradCPU mysum_autograd_cpu x dim torch sum x dim mysum py_impl torch _C DispatchKey AutogradCUDA mysum_autograd_cuda x dim torch sum x dim mysum sum_pyop = construct_sum_pyop markDynamoStrictTest TestHigherOrderOperatorInteraction TestCase test_basic_sum device x = torch randn device=device result = sum_pyop x assertEqual result torch sum x test_vmap_sum device x = torch randn device=device result = vmap sum_pyop None x assertEqual result torch sum x result = vmap vmap sum_pyop None None x assertEqual result torch sum x test_grad_sum device x = torch randn device=device gx = grad sum_pyop x assertEqual gx torch ones_like x test_grad_grad_sum device x = torch randn requires_grad=True device=device f x higher order grad Requires non-linearity sum_pyop x sin grad_f_sum x grad f x sum ggx = grad grad_f_sum x assertEqual ggx -x sin test_vmap_grad_sum device x = torch randn device=device gx = vmap grad sum_pyop None x assertEqual gx torch ones_like x test_no_grad_outside_grad device x = torch randn device=device requires_grad=True torch no_grad y = grad sum_pyop x assertEqual y torch ones_like x assertFalse y requires_grad test_no_grad_inside_grad device f x torch no_grad shift = sum_pyop x sum_pyop x - shift x = torch randn device=device y = grad f x assertEqual y x y = grad lambda x grad f x sum x assertEqual y torch full_like x x = torch randn device=device requires_grad=True y = grad f x z = torch autograd grad y sum x assertEqual z torch full_like x test_grad_name_wrapping device my_fn x x sum grad_fn = grad my_fn assertEqual grad_fn __name__ my_fn test_functional_call_multiple_dicts mod = nn Linear x = torch randn params = weight torch zeros bias torch ones functional_call mod params x traceable f f = allow_in_graph f wraps f wrapper args kwargs f args kwargs wrapper markDynamoStrictTest TestCompileTransforms TestCase torch compile supported Windows CUDA Triton only supports GPU SM later expectedFailureIf IS_WINDOWS TEST_CUDA TEST_CUDA SM OrLater unittest skipIf TEST_CUDA_MEM_LEAK_CHECK Leaking memory see https github com pytorch pytorch pull example test_compile_vmap_hessian device The model inputs smaller version code benchmark repo https github com pytorch benchmark blob main userbenchmark functorch vmap_hessian_fc py D = B = x = torch randn B D device=device model = nn Sequential nn Linear D D nn ReLU device params_and_buffers = dict model named_parameters dict model named_buffers predict params_and_buffers x out = torch func functional_call model params_and_buffers x out out fn = vmap jacfwd jacrev predict argnums= has_aux=True argnums= has_aux=True in_dims= None expected = fn params_and_buffers x opt_fn = torch compile traceable fn actual = opt_fn params_and_buffers x assertEqual actual expected torch compile supported Windows torch _dynamo config patch suppress_errors=False test_grad_deprecated_api device x = torch randn device=device y = torch randn device=device wrapper_fn x y functorch grad torch mul x y actual = wrapper_fn x y expected = torch compile wrapper_fn backend= eager fullgraph=True x y torch compile wrapper_fn backend= eager fullgraph=True assertEqual actual expected wrapper_fn x y functorch grad torch mul argnums= x y actual = wrapper_fn x y expected = torch compile wrapper_fn backend= eager fullgraph=True x y assertEqual actual expected TestGradTrackingTensorToList TestCase Tests tolist method GradTrackingTensor functorch tensors test_tolist_with_grad Test see tolist works inside grad transformation f x inside grad x GradTrackingTensor result = x tolist tolist should python list fail assertIsInstance result list assertEqual result x sum x = torch tensor requires_grad=True grad_f = torch func grad f result = grad_f x assertIsInstance result torch Tensor gradients should still computed correctly assertEqual result test_tolist_nested_grad Test ` tolist ` nested grad transformations f x g y y gradTrackingTensor lvl= inner_list = y tolist assertIsInstance inner_list list y sum x gradTrackingTensor lvl= outer_list = x tolist assertIsInstance outer_list list grad_g = torch func grad g grad_g x sum x = torch tensor requires_grad=True grad_f = torch func grad f result = grad_f x should compute second derivate assertIsInstance result torch Tensor grad_f should derivate g y which x sum assertEqual result test_tolist_multidimensional_grad Test tolist multi-dimensional tensors grad f x result = x tolist assertIsInstance result list assertEqual len result assertEqual result x sum x = torch tensor requires_grad=True grad_f = torch func grad f result = grad_f x assertIsInstance result torch Tensor assertEqual result test_tolist_conj_neg_grad Test tolist method conjugate negative tensors grad context f x test conjugate view x_conj = x conj result_conj = x_conj tolist assertIsInstance result_conj list x x conj real sum x = torch tensor + j + j requires_grad=True grad_f = torch func grad f result = grad_f x assertIsInstance result torch Tensor assertEqual result + j + j only_for = cpu cuda instantiate_device_type_tests TestGradTransform globals only_for=only_for instantiate_device_type_tests TestVmapOfGrad globals only_for=only_for instantiate_device_type_tests TestJac globals only_for=only_for instantiate_device_type_tests TestJvp globals only_for=only_for instantiate_device_type_tests TestLinearize globals only_for=only_for instantiate_device_type_tests TestVmapJvpInplaceView globals only_for=only_for instantiate_device_type_tests TestHessian globals only_for=only_for instantiate_device_type_tests TestComposability globals only_for=only_for instantiate_device_type_tests TestExamplesCorrectness globals only_for=only_for instantiate_device_type_tests TestHigherOrderOperatorInteraction globals only_for=only_for instantiate_device_type_tests TestFunctionalize globals only_for=only_for instantiate_device_type_tests TestAutogradFunction globals only_for=only_for instantiate_device_type_tests TestAutogradFunctionVmapAPI globals only_for=only_for instantiate_device_type_tests TestHelpers globals only_for=only_for instantiate_parametrized_tests TestMakeFunctional instantiate_device_type_tests TestCompileTransforms globals only_for=only_for instantiate_device_type_tests TestGradTrackingTensorToList globals only_for=only_for __name__ == __main__ run_tests