mypy allow-untyped-defs Copyright c Meta Platforms Inc affiliates abc ABC abstractmethod functools partial typing Any Optional Union torch torch nn nn torch distributed tensor DeviceMesh distribute_module distribute_tensor DTensor Replicate Shard torch distributed tensor placement_types Placement __all__ = ParallelStyle RowwiseParallel SequenceParallel ColwiseParallel PrepareModuleInput PrepareModuleInputOutput PrepareModuleOutput ParallelStyle ABC The parallel style contract defines how module submodule should parallelized It only defines ` ` apply ` ` method ` ` parallelize_module ` ` use allows maximum flexibility different kind style implementations src_data_rank Optional int = abstractmethod _apply module nn Module device_mesh DeviceMesh - nn Module ColwiseParallel ParallelStyle Partition compatible nn Module column-wise fashion Currently supports nn Linear nn Embedding Users can compose together RowwiseParallel achieve sharding more complicated modules i e MLP Attention Keyword Args input_layouts Placement optional The DTensor layout input tensor nn Module used annotate input tensor become DTensor If specified we assume input tensor replicated output_layouts Placement optional The DTensor layout output nn Module used ensure output nn Module user desired layout If specified output tensor sharded last dimension use_local_output bool optional Whether use local ` torch Tensor ` instead ` DTensor ` module output default True Returns A ` ParallelStyle ` object represents Colwise sharding nn Module Example xdoctest +SKIP failing torch distributed tensor parallel parallelize_module ColwiseParallel torch distributed device_mesh init_device_mesh m = Model m nn Module contains w nn Linear submodule tp_mesh = init_device_mesh cuda By default input w Linear will converted Replicated DTensor output w will ` torch Tensor ` shards last dim sharded_mod = parallelize_module m tp_mesh w ColwiseParallel note By default ` ` ColwiseParallel ` ` output sharded last dimension ` ` output_layouts ` ` specified there re operators require specific tensor shape i e before paired ` ` RowwiseParallel ` ` keep mind output sharded operator might need adjusted sharded size __init__ input_layouts Optional Placement = None output_layouts Optional Placement = None use_local_output bool = True super __init__ input_layouts = input_layouts Replicate output_layouts = output_layouts Shard - colwise linear runtime sharding desired sharding requires replicate input shard output last dim desired_input_layouts = Replicate use_local_output = use_local_output staticmethod _prepare_input_fn input_layouts desired_input_layouts mod inputs device_mesh TODO figure out dynamo support instance method switch instance method annotate module input placements sharding input_layouts input_tensor = inputs isinstance input_tensor DTensor input_tensor = DTensor from_local input_tensor device_mesh input_layouts run_check=False transform input layouts desired layouts ColwiseParallel input_layouts = desired_input_layouts input_tensor = input_tensor redistribute placements=desired_input_layouts async_op=True input_tensor _partition_linear_fn name module device_mesh colwise shard weight bias Shard weight Shard means Colwise Linear input weight^T + bias where weight would become Shard name param module named_parameters dist_param = nn Parameter distribute_tensor param device_mesh Shard src_data_rank=self src_data_rank module register_parameter name dist_param _partition_embedding_fn name module device_mesh colwise shard embedding weight straight forward Shard name param module named_parameters dist_param = nn Parameter distribute_tensor param device_mesh Shard src_data_rank=self src_data_rank module register_parameter name dist_param staticmethod _prepare_output_fn output_layouts use_local_output mod outputs device_mesh outputs shard last dimension DTensor i e Shard - outputs placements = output_layouts outputs = outputs redistribute placements=output_layouts async_op=True back local tensor outputs to_local use_local_output outputs _apply module nn Module device_mesh DeviceMesh - nn Module isinstance module nn Linear partition_fn = _partition_linear_fn isinstance module nn Embedding partition_fn = _partition_embedding_fn raise NotImplementedError ColwiseParallel currently only support nn Linear nn Embedding distribute_module module device_mesh partition_fn partial _prepare_input_fn input_layouts desired_input_layouts partial _prepare_output_fn output_layouts use_local_output __repr__ - str tmpstr = __class__ __name__ + tmpstr += f input_layouts= input_layouts tmpstr += f output_layouts= output_layouts tmpstr += f use_local_output= use_local_output tmpstr += tmpstr RowwiseParallel ParallelStyle Partition compatible nn Module row-wise fashion Currently supports nn Linear nn Embedding Users can compose ColwiseParallel achieve sharding more complicated modules i e MLP Attention Keyword Args input_layouts Placement optional The DTensor layout input tensor nn Module used annotate input tensor become DTensor If specified we assume input tensor sharded last dimension output_layouts Placement optional The DTensor layout output nn Module used ensure output nn Module user desired layout If specified output tensor replicated use_local_output bool optional Whether use local ` torch Tensor ` instead ` DTensor ` module output default True Returns A ` ParallelStyle ` object represents Rowwise sharding nn Module Example xdoctest +SKIP failing torch distributed tensor parallel parallelize_module RowwiseParallel torch distributed device_mesh init_device_mesh m = Model m nn Module contains w nn Linear submodule tp_mesh = init_device_mesh cuda By default input w Linear will converted DTensor shards last dim output w will replicated ` torch Tensor ` sharded_mod = parallelize_module m tp_mesh w RowwiseParallel __init__ input_layouts Optional Placement = None output_layouts Optional Placement = None use_local_output bool = True super __init__ input_layouts = input_layouts Shard - output_layouts = output_layouts Replicate use_local_output = use_local_output staticmethod _prepare_input_fn input_layouts desired_input_layouts mod inputs device_mesh input_tensor = inputs isinstance input_tensor DTensor input_tensor = DTensor from_local input_tensor device_mesh input_layouts run_check=False input_layouts = desired_input_layouts input_tensor = input_tensor redistribute placements=desired_input_layouts async_op=True input_tensor _partition_linear_fn name module device_mesh Rowwise shard weight Shard bias Replicate weight Shard means Rowwise nn Linear input weight^T + bias where weight would become Shard module register_parameter weight nn Parameter distribute_tensor module weight device_mesh Shard src_data_rank=self src_data_rank getattr module bias None None The Linear module has bias module register_parameter bias nn Parameter distribute_tensor module bias device_mesh Replicate src_data_rank=self src_data_rank _partition_embedding_fn name module device_mesh rowwise shard embedding weight Shard name param module named_parameters dist_param = nn Parameter distribute_tensor param device_mesh Shard src_data_rank=self src_data_rank module register_parameter name dist_param staticmethod _prepare_output_fn output_layouts use_local_output mod outputs device_mesh Rowwise sharding produces partial output depending output layouts replicate - allreduce shard - reduce_scatter outputs placements = output_layouts outputs = outputs redistribute placements=output_layouts async_op=True back local tensor use_local_output True outputs to_local use_local_output outputs _apply module nn Module device_mesh DeviceMesh - nn Module isinstance module nn Linear partition_fn = _partition_linear_fn rowwise linear runtime sharding requires input tensor shard last dim desired_input_layouts tuple Placement = Shard - isinstance module nn Embedding partition_fn = _partition_embedding_fn rowwise embedding runtime sharding requires input tensor replicated desired_input_layouts = Replicate raise NotImplementedError RowwiseParallel currently only support nn Linear nn Embedding distribute_module module device_mesh partition_fn partial _prepare_input_fn input_layouts desired_input_layouts partial _prepare_output_fn output_layouts use_local_output __repr__ - str tmpstr = __class__ __name__ + tmpstr += f input_layouts= input_layouts tmpstr += f output_layouts= output_layouts tmpstr += f use_local_output= use_local_output tmpstr += tmpstr SequenceParallel ParallelStyle SequenceParallel replicates compatible ` ` nn Module ` ` parameters runs sharded computation input sharded sequence dimension This currently supports ` ` nn LayerNorm ` ` ` ` nn Dropout ` ` ` RMSNorm python implementation https github com facebookresearch llama blob main llama model py#L ` __ This style implements operation described paper ` Reducing Activation Recomputation Large Transformer Models https arxiv org abs ` __ If input passed ` ` nn Module ` ` ` torch Tensor ` assumes input already sharded sequence dimension converts input ` DTensor ` sharded sequence dimension If input passed ` ` nn Module ` ` already ` DTensor ` sharded sequence dimension would redistribute input sharded sequence dimension The output ` ` nn Module ` ` will sharded sequence dimension Keyword Args sequence_dim int optional The sequence dimension input tensor ` ` nn Module ` ` used annotate input tensor become DTensor sharded sequence dimension default use_local_output bool optional Whether use local ` torch Tensor ` instead ` DTensor ` module output default False Returns A ` ParallelStyle ` object represents Sequence Parallel ` ` nn Module ` ` Example xdoctest +SKIP failing torch distributed tensor parallel parallelize_module SequenceParallel torch distributed device_mesh init_device_mesh m = Model m nn Module contains norm nn LayerNorm submodule tp_mesh = init_device_mesh cuda By default input norm will converted DTensor shards sequence dim output norm will sharded sequence dimension ` DTensor ` sharded_mod = parallelize_module m tp_mesh norm SequenceParallel note SequenceParallel style assumes ones initialization there weights nn Module i e ` ` nn LayerNorm ` ` ` ` RMSNorm ` ` they default have ones initialization If you have custom inits weights those modules you need broadcast weights before after parallelizing ensure they replicated __init__ sequence_dim int = use_local_output bool = False super __init__ sequence_sharding = Shard sequence_dim use_local_output = use_local_output _replicate_module_fn name str module nn Module device_mesh DeviceMesh p_name param module named_parameters simple replication fixed ones_ init LayerNorm RMSNorm which allow us simply just use from_local replicated_param = torch nn Parameter DTensor from_local param device_mesh Replicate run_check=False module register_parameter p_name replicated_param staticmethod _prepare_input_fn sequence_sharding mod inputs device_mesh input_tensor = inputs isinstance input_tensor DTensor passed input DTensor sharded sequence dim we need redistribute input_tensor placements = sequence_sharding input_tensor = input_tensor redistribute placements=sequence_sharding async_op=True input_tensor isinstance input_tensor torch Tensor assume input passed already sharded sequence dim create DTensor DTensor from_local input_tensor device_mesh sequence_sharding run_check=False raise ValueError f expecting input mod torch Tensor DTensor got input_tensor staticmethod _prepare_output_fn use_local_output mod outputs device_mesh outputs to_local use_local_output outputs _apply module nn Module device_mesh DeviceMesh - nn Module distribute_module module device_mesh _replicate_module_fn partial _prepare_input_fn sequence_sharding partial _prepare_output_fn use_local_output __repr__ - str tmpstr = __class__ __name__ + len sequence_sharding == tmpstr += f sequence_dim= sequence_sharding dim tmpstr += f use_local_output= use_local_output tmpstr += tmpstr PrepareModuleInput ParallelStyle Configure nn Module s inputs convert input tensors nn Module DTensors runtime according ` ` input_layouts ` ` perform layout redistribution according ` ` desired_input_layouts ` ` Keyword Args input_layouts Union Placement Tuple Optional Placement The DTensor layouts input tensors nn Module used convert input tensors DTensors If some inputs torch Tensor no need convert DTensors ` ` None ` ` need specified placeholder default None desired_input_layouts Union Placement Tuple Optional Placement The desired DTensor layout input tensors nn Module used ensure inputs nn Module have desired DTensor layouts This argument needs have same length ` ` input_layouts ` ` default None input_kwarg_layouts Dict str Placement The DTensor layouts input kwargs nn Module used convert input kwarg tensors DTensors default None desired_input_kwarg_layouts Dict str Placement The desired DTensor layout input kwargs nn Module used ensure inputs nn Module have desired DTensor layouts default None use_local_output bool optional Whether use local ` torch Tensor ` instead ` DTensor ` module inputs default False Returns A ` ParallelStyle ` object prepares sharding layouts nn Module s inputs Example xdoctest +SKIP failing torch distributed tensor parallel parallelize_module PrepareModuleInput torch distributed device_mesh init_device_mesh block = TransformerBlock block nn Module contains attn Attention submodule tp_mesh = init_device_mesh cuda According style specified below first input attn will annotated Sharded DTensor then redistributed Replicated DTensor parallelize_module block can submodule module tp_mesh parallelize_plan= attn PrepareModuleInput input_layouts= Shard None None desired_input_layouts= Replicate None None __init__ input_layouts Optional Union Placement tuple Optional Placement = None desired_input_layouts Optional Union Placement tuple Optional Placement = None input_kwarg_layouts Optional dict str Placement = None desired_input_kwarg_layouts Optional dict str Placement = None use_local_output bool = False input_layouts = input_layouts isinstance input_layouts Placement input_layouts desired_input_layouts = desired_input_layouts isinstance desired_input_layouts Placement desired_input_layouts use_local_output = use_local_output input_layouts None assert desired_input_layouts None desired module inputs should None assert len input_layouts == len desired_input_layouts input_layouts desired_input_layouts should have same length with_kwargs = input_kwarg_layouts None input_kwarg_layouts = input_kwarg_layouts desired_input_kwarg_layouts = desired_input_kwarg_layouts with_kwargs assert len input_kwarg_layouts == len desired_input_kwarg_layouts input_kwarg_layouts desired_input_kwarg_layouts should have same length _prepare_input_arg input Any mesh DeviceMesh input_layout Optional Placement desired_layout Optional Placement input_layout None isinstance input DTensor TODO re-enable check once we fix compile path assert inp placements == input_layout dt_inp = input assert isinstance input torch Tensor expecting input torch Tensor dt_inp = DTensor from_local input mesh input_layout run_check=False desired_layout None input_layout = desired_layout dt_inp = dt_inp redistribute placements= desired_layout dt_inp to_local use_local_output dt_inp input _prepare_input_fn inputs device_mesh input_layouts None inputs prepared_inputs = isinstance inputs tuple inputs = inputs len inputs = len input_layouts raise ValueError module inputs input_layouts should have same length assert desired_input_layouts None desired module inputs should None inp input_layout desired_layout zip inputs input_layouts desired_input_layouts prepared_inputs append _prepare_input_arg inp device_mesh input_layout desired_layout tuple prepared_inputs _prepare_input_kwarg_fn inputs kwarg_inputs device_mesh prepared_arg_inputs = _prepare_input_fn inputs device_mesh prepared_kwarg_inputs = kwarg_key kwarg_inputs keys kwarg_val = kwarg_inputs kwarg_key input_layout = input_kwarg_layouts get kwarg_key desired_input_layout = desired_input_kwarg_layouts get kwarg_key prepared_kwarg_inputs kwarg_key = _prepare_input_arg kwarg_val device_mesh input_layout desired_input_layout prepared_arg_inputs prepared_kwarg_inputs _apply module nn Module device_mesh DeviceMesh - nn Module with_kwargs module register_forward_pre_hook lambda _ inputs kwargs _prepare_input_kwarg_fn inputs kwargs device_mesh with_kwargs=True type ignore misc module register_forward_pre_hook lambda _ inputs _prepare_input_fn inputs device_mesh type ignore misc call-arg module __repr__ - str tmpstr = __class__ __name__ + tmpstr += f input_layouts= input_layouts tmpstr += f desired_input_layouts= desired_input_layouts tmpstr += f input_kwarg_layouts= input_kwarg_layouts tmpstr += f desired_input_kwarg_layouts= desired_input_kwarg_layouts tmpstr += f use_local_output= use_local_output tmpstr += tmpstr PrepareModuleOutput ParallelStyle Configure nn Module s outputs convert output tensors nn Module DTensors runtime according ` ` output_layouts ` ` perform layout redistribution according ` ` desired_output_layouts ` ` Keyword Args output_layouts Union Placement Tuple Placement The DTensor layouts output tensors nn Module used convert output tensors DTensors they ` torch Tensor ` If some outputs torch Tensor no need convert DTensors ` ` None ` ` need specified placeholder desired_output_layouts Union Placement Tuple Placement The desired DTensor layouts output tensors nn Module used ensure outputs nn Module have desired DTensor layouts use_local_output bool optional Whether use local ` torch Tensor ` instead ` DTensor ` module outputs default True Returns A ParallelStyle object prepares sharding layouts nn Module s outputs Example xdoctest +SKIP failing torch distributed tensor parallel parallelize_module PrepareModuleOutput torch distributed device_mesh init_device_mesh block = TransformerBlock block nn Module contains attn Attention submodule tp_mesh = init_device_mesh cuda According style specified below output TransformerBlock will converted Replicated DTensor then redistributed Sharded DTensor parallelize_module block can submodule module tp_mesh parallelize_plan = PrepareModuleOutput output_layouts=Replicate desired_output_layouts=Shard __init__ output_layouts Union Placement tuple Optional Placement desired_output_layouts Union Placement tuple Placement use_local_output bool = True output_layouts = output_layouts isinstance output_layouts Placement output_layouts desired_output_layouts = desired_output_layouts isinstance desired_output_layouts Placement desired_output_layouts use_local_output = use_local_output assert len output_layouts == len desired_output_layouts output_layouts desired_output_layouts should have same length _prepare_out_fn outputs device_mesh prepared_outputs = isinstance outputs tuple outputs = outputs len outputs = len output_layouts raise ValueError module outputs output_layouts should have same length out out_layout desired_out_layout zip outputs output_layouts desired_output_layouts out_layout None isinstance out DTensor TODO re-enable check once we fix compile path assert out placements == out_layout dt_out = out dt_out = DTensor from_local out device_mesh out_layout run_check=False out_layout = desired_out_layout dt_out = dt_out redistribute placements= desired_out_layout prepared_outputs append dt_out to_local use_local_output dt_out prepared_outputs append out len prepared_outputs == prepared_outputs tuple prepared_outputs _apply module nn Module device_mesh DeviceMesh - nn Module module register_forward_hook lambda _ inputs outputs _prepare_out_fn outputs device_mesh type ignore misc call-arg module __repr__ - str tmpstr = __class__ __name__ + tmpstr += f output_layouts= output_layouts tmpstr += f desired_output_layouts= desired_output_layouts tmpstr += f use_local_output= use_local_output tmpstr += tmpstr PrepareModuleInputOutput ParallelStyle Configure nn Module s inputs outputs convert input tensors output tensors respectively nn Module DTensors runtime according ` ` input_layouts ` ` output_layouts respectively perform layout redistribution according ` ` desired_input_layouts ` ` ` ` desired_output_layouts ` ` respectively This combination ` PrepareModuleInput ` ` PrepareModuleOutput ` Keyword Args input_layouts Union Placement Tuple Optional Placement The DTensor layouts input tensors nn Module used convert input tensors DTensors If some inputs torch Tensor no need convert DTensors ` ` None ` ` need specified placeholder default None desired_input_layouts Union Placement Tuple Optional Placement The desired DTensor layout input tensors nn Module used ensure inputs nn Module have desired DTensor layouts This argument needs have same length ` ` input_layouts ` ` default None input_kwarg_layouts Dict str Placement The DTensor layouts input kwargs nn Module used convert input kwarg tensors DTensors default None desired_input_kwarg_layouts Dict str Placement The desired DTensor layout input kwargs nn Module used ensure inputs nn Module have desired DTensor layouts default None use_local_input bool optional Whether use local ` torch Tensor ` instead ` DTensor ` module inputs default False output_layouts Union Placement Tuple Placement The DTensor layouts output tensors nn Module used convert output tensors DTensors they ` torch Tensor ` If some outputs torch Tensor no need convert DTensors ` ` None ` ` need specified placeholder desired_output_layouts Union Placement Tuple Placement The desired DTensor layouts output tensors nn Module used ensure outputs nn Module have desired DTensor layouts use_local_output bool optional Whether use local ` torch Tensor ` instead ` DTensor ` module outputs default True Returns A ` ParallelStyle ` object prepares sharding layouts nn Module s inputs outputs Example xdoctest +SKIP failing torch distributed tensor parallel parallelize_module PrepareModuleInputOutput torch distributed device_mesh init_device_mesh block = TransformerBlock block nn Module contains attn Attention submodule tp_mesh = init_device_mesh cuda According style specified below first input attn will annotated Sharded DTensor then redistributed Replicated DTensor output TransformerBlock will annotated Replicated DTensor then redistributed Sharded DTensor parallelize_module block can submodule module tp_mesh parallelize_plan= attn PrepareModuleInputOutput input_layouts= Shard None None desired_input_layouts= Replicate None None output_layouts=Replicate desired_output_layouts=Shard __init__ input_layouts Optional Union Placement tuple Optional Placement = None desired_input_layouts Optional Union Placement tuple Optional Placement = None input_kwarg_layouts Optional dict str Placement = None desired_input_kwarg_layouts Optional dict str Placement = None use_local_input bool = False output_layouts Union Placement tuple Optional Placement desired_output_layouts Union Placement tuple Placement use_local_output bool = True prepare_module_input = PrepareModuleInput input_layouts=input_layouts desired_input_layouts=desired_input_layouts input_kwarg_layouts=input_kwarg_layouts desired_input_kwarg_layouts=desired_input_kwarg_layouts use_local_output=use_local_input prepare_module_output = PrepareModuleOutput output_layouts=output_layouts desired_output_layouts=desired_output_layouts use_local_output=use_local_output _apply module nn Module device_mesh DeviceMesh - nn Module prepare_module_input _apply module device_mesh prepare_module_output _apply module device_mesh module __repr__ - str tmpstr = __class__ __name__ + tmpstr += f input_layouts= prepare_module_input input_layouts tmpstr += f desired_input_layouts= prepare_module_input desired_input_layouts tmpstr += f input_kwarg_layouts= prepare_module_input input_kwarg_layouts tmpstr += f desired_input_kwarg_layouts= prepare_module_input desired_input_kwarg_layouts tmpstr += f use_local_input= prepare_module_input use_local_output tmpstr += f output_layouts= prepare_module_output output_layouts tmpstr += f desired_output_layouts= prepare_module_output desired_output_layouts tmpstr += f use_local_output= prepare_module_output use_local_output tmpstr += tmpstr