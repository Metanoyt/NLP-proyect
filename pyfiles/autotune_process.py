mypy allow-untyped-defs __future__ annotations atexit ctypes dataclasses functools logging os pickle queue selectors subprocess sys time warnings collections abc Iterable Sequence concurrent futures ThreadPoolExecutor ctypes byref c_size_t c_void_p CDLL typing Any Callable IO Optional TYPE_CHECKING Union torch torch _inductor async_compile noqa F required warm up AsyncCompile pools torch _dynamo device_interface get_interface_for_device torch _dynamo testing rand_strided torch _inductor ir torch _inductor codecache CppCodeCache CUDACodeCache DLLWrapper get_hash PyCodeCache torch _inductor utils get_gpu_type get_ld_library_path is_gpu python_subprocess_env torch _logging getArtifactLogger torch utils _ordered_set OrderedSet TYPE_CHECKING types ModuleType torch _inductor select_algorithm PartialRender TritonTemplateCaller config runtime benchmarking benchmarker virtualized V CUDA_VISIBLE_DEVICES = CUDA_VISIBLE_DEVICES autotuning_log = getArtifactLogger __name__ autotuning NonzeroWorkspaceNotSupportedError Exception pass TuningProcess Class launch interact benchmarking subprocess staticmethod process_main read_pipe IO bytes write_pipe IO bytes - None Entry point child process autotuning_log debug Started autotune subprocess s Visible devices s os getpid os environ get CUDA_VISIBLE_DEVICES workloop while True job = TuningProcess recv read_pipe job None None sentinel child shut down break try result = job except Exception e result = e TuningProcess send result write_pipe try workloop except EOFError The parent closed pipe pass staticmethod send obj Any write_pipe IO bytes - None pickle dump obj write_pipe write_pipe flush staticmethod recv read_pipe IO bytes - Any pickle load read_pipe __init__ device Optional int device = device start start Start benchmarking subprocess entry = os path join os path dirname __file__ __autotune_main__ py subproc_read_fd write_fd = os pipe read_fd subproc_write_fd = os pipe write_pipe = os fdopen write_fd wb read_pipe = os fdopen read_fd rb selector = selectors DefaultSelector selector register read_pipe selectors EVENT_READ cmd = sys executable entry f -- parent= os getpid f -- read-fd= str subproc_read_fd f -- write-fd= str subproc_write_fd env = python_subprocess_env We shouldn t using Triton async compile subprocess pool precaution set env var disables its creation TORCH_WARM_POOL Some internal usages need modified LD_LIBRARY_PATH LD_LIBRARY_PATH get_ld_library_path This will cause subprocs profile using profiler TORCHINDUCTOR_PROFILE_WITH_DO_BENCH_USING_PROFILING config profile_bandwidth_with_do_bench_using_profiling device None env CUDA_VISIBLE_DEVICES = str device process = subprocess Popen cmd env=env pass_fds= subproc_read_fd subproc_write_fd os close subproc_read_fd os close subproc_write_fd running = True alive - bool True subprocess still running running process poll None put req Any - None Push work item child process alive start TuningProcess send req write_pipe get timeout float = - Any Get response child process Raises TimeoutError timeout raises EOFError subprocess crashes try selector select timeout raise TimeoutError f Timeout autotune subprocess process pid result = TuningProcess recv read_pipe except TimeoutError kill raise except EOFError The subprocess crashed close raise except Exception autotuning_log exception Unexpected exception autotune subprocess s process pid kill raise isinstance result Exception raise result result shutdown wait bool = True - None Signal child process shut down gracefully alive TuningProcess send None write_pipe wait wait wait - None Wait child process exit alive process wait close close - None Close resources selector close read_pipe close write_pipe close running = False kill - None Send SIGKILL child process alive autotuning_log error Sending SIGKILL autotune subprocess d process pid process kill close restart - None Gracefully restarts child process shutdown wait=True start TuningProcessPool Maintains pool TuningProcesses benchmark kernels parallel across devices By default we create one TuningProcess per device set sub-process environment make only device visible __init__ - None Start child processes devices = get_device_list autotuning_log debug Sub-process autotune device list s devices Launch child processes processes = TuningProcess device=device device devices process_queue queue Queue TuningProcess = queue Queue p processes process_queue put p Use thread pool manage distributing work subprocesses Threads block available process so makes sense match number threads number devices executor = ThreadPoolExecutor max_workers=len devices staticmethod get_device_list - Sequence Optional int Gather list devices used pool config autotune_multi_device Don t use multiple devices None gpu_type = get_gpu_type device_interface = get_interface_for_device gpu_type count = device_interface device_count If user specified visible devices env use those CUDA_VISIBLE_DEVICES os environ devices = int d d os environ CUDA_VISIBLE_DEVICES split assert len devices = count devices list range count shutdown - None Signal all child processes exit executor shutdown p processes p shutdown wait=False p processes p wait target choice TritonTemplateCaller - float Entry point thread-pool helper threads Wait open TuningProcess remove queue execute benchmark subprocess TuningProcess queue assert choice bmreq None process = process_queue get process put choice bmreq benchmark try process get config max_autotune_subproc_result_timeout_seconds except TimeoutError warnings warn f Timed out benchmarking choice choice It will ignored Please debug root cause case choice can bring perf gains Set INF so choice will ignored float inf except Exception process_exception warnings warn f Failed benchmark choice choice It will ignored Please debug root cause case choice can bring perf gains An unspecified launch failure cudaErrorLaunchFailure corrupts CUDA context making unrecoverable All subsequent CUDA calls will fail well The process must restarted restore CUDA functionality cudaErrorLaunchFailure str process_exception process restart Set INF so choice will ignored float inf finally process_queue put process benchmark choices list TritonTemplateCaller - dict TritonTemplateCaller float Benchmark each choice separate process Use ThreadExecutorPool spread work across subprocesses grab subprocesses soon they re free results = dict zip choices executor map target choices results LayoutOrBuffer = Union ir Layout ir Buffer dataclasses dataclass TensorMeta device torch device dtype torch dtype sizes torch _prims_common ShapeType strides torch _prims_common StrideType offset int name Optional str = None classmethod from_irnodes cls irnodes Union LayoutOrBuffer Sequence LayoutOrBuffer - Union TensorMeta list TensorMeta isinstance irnodes Sequence result list Any = cls from_irnodes x x irnodes assert all isinstance x TensorMeta x result result node = irnodes isinstance node ir Layout node = ir Buffer name= fake layout=node dtype = node get_dtype assert dtype None device = node get_device assert device None TensorMeta device=device dtype=dtype sizes=V graph sizevars size_hints node get_size fallback=config unbacked_symint_fallback strides=V graph sizevars size_hints node get_stride fallback=config unbacked_symint_fallback offset=V graph sizevars size_hint node get_layout offset fallback=config unbacked_symint_fallback name=node get_name to_tensor - torch Tensor rand_strided sizes strides device=self device dtype=self dtype extra_size=self offset dataclasses dataclass BenchmarkRequest Only handle triton template benchmark now The extern kernel benchmark can done inside same process since they usually don t cause crash Important Instances subclasses have serializable across process boundaries Do put CUDA Tensors here __init__ kernel_name str input_tensor_meta Union TensorMeta list TensorMeta output_tensor_meta Union TensorMeta list TensorMeta extra_args Iterable Any - None kernel name defined module kernel_name = kernel_name isinstance input_tensor_meta TensorMeta input_tensor_meta = input_tensor_meta input_tensor_meta = input_tensor_meta isinstance output_tensor_meta tuple list len output_tensor_meta Each output same meta Grouped GEMM assert all getattr output_tensor_meta attr == getattr x attr x output_tensor_meta attr device dtype sizes strides offset output_tensor_meta = output_tensor_meta output_tensor_meta = output_tensor_meta extra_args = extra_args make_run_fn input_tensors torch Tensor out torch Tensor - Callable None raise NotImplementedError cleanup_run_fn - None pass do_bench fn input_tensors torch Tensor out Optional torch Tensor = None - float raise NotImplementedError benchmark input_tensors torch Tensor out Optional torch Tensor = None - float debug = autotuning_log isEnabledFor logging DEBUG debug start_ts = time time create args out tensor out None assert len input_tensors == input_tensors = tuple x to_tensor x input_tensor_meta out = output_tensor_meta to_tensor debug create_tensor_elapse = time time - start_ts type ignore possibly-undefined start_ts = time time try fn = make_run_fn input_tensors out=out except NonzeroWorkspaceNotSupportedError Skipping all ops nonzero workspace requirements autotuning_log info Skipping op due nonzero workspace requirement float inf debug load_elapse = time time - start_ts type ignore possibly-undefined start_ts = time time res = do_bench fn input_tensors out debug bench_elapse = time time - start_ts type ignore possibly-undefined autotuning_log debug InChildProcess s load f create tensor f bench f str load_elapse type ignore possibly-undefined create_tensor_elapse type ignore possibly-undefined bench_elapse cleanup_run_fn res _TestBenchmarkRequest BenchmarkRequest Supports unit testing Defined file instead test file so TuningProcess sub-process can unpickle these objects __init__ result float = device Optional int = None sleep Optional float = None exc Optional Exception = None crash bool = False result = result device = device sleep = sleep exc = exc crash = crash benchmark input_tensors torch Tensor out Optional torch Tensor = None - float device None assert os environ get CUDA_VISIBLE_DEVICES None == str device sleep time sleep sleep exc raise exc crash sys exit result GPUDeviceBenchmarkMixin do_bench fn input_tensors torch Tensor out Optional torch Tensor = None - float device_idx_set = OrderedSet tensor device index tensor input_tensors out isinstance tensor torch Tensor is_gpu tensor device type tensor device index None assert len device_idx_set = f Can mix devices device_idx_set device_type = next tensor device type tensor input_tensors is_gpu tensor device type cuda device_interface = get_interface_for_device device_type len device_idx_set == device_idx = next iter device_idx_set device_idx = device_interface current_device device_interface device device_idx type ignore attr-defined res = benchmarker benchmark_gpu fn device_interface synchronize shake out any CUDA errors res CPUDeviceBenchmarkMixin do_bench fn input_tensors torch Tensor out Optional torch Tensor = None - float benchmarker benchmark_cpu fn TritonBenchmarkRequest BenchmarkRequest Important Instances have serializable across process boundaries Do put CUDA Tensors here __init__ kernel_name str input_tensor_meta Union TensorMeta list TensorMeta output_tensor_meta Union TensorMeta list TensorMeta extra_args Iterable Any module_path str path module defining triton kernel module_cache_key str num_stages int num_warps int num_consumer_groups int = num_buffers_warp_spec int = matrix_instr_nonkdim int = only used hip choose shape mfma instruction waves_per_eu int = only used hip schedule waves per execution unit kpack int = ROCm specific gemm parameter - None super __init__ kernel_name input_tensor_meta output_tensor_meta extra_args module_path = module_path module_cache_key = module_cache_key num_stages = num_stages num_warps = num_warps num_consumer_groups = num_consumer_groups num_buffers_warp_spec = num_buffers_warp_spec matrix_instr_nonkdim = matrix_instr_nonkdim waves_per_eu = waves_per_eu kpack = kpack make_run_fn input_tensors torch Tensor out torch Tensor - Callable None mod = PyCodeCache load_by_key_path module_cache_key module_path autotuning_log debug benchmark module key s path s module_cache_key module_path run_method = getattr mod kernel_name run extra_args = list extra_args run_method __self__ with_bandwidth_info = False Newer version triton add warmup argument JITFunction run This code handles backward-compatibility warmup_arg = inspect warmup inspect signature run_method parameters warmup_arg warmup = False out device type == cpu stream = device_type = out device type device_interface = get_interface_for_device device_type stream = device_interface get_raw_stream output_tensor_meta device index isinstance getattr mod kernel_name torch _inductor runtime triton_heuristics DebugAutotuner functools partial run_method input_tensors out extra_args warmup_arg stream=stream functools partial run_method input_tensors out extra_args warmup_arg stream=stream benchmark_run=True precompile mod = PyCodeCache load_by_key_path module_cache_key module_path getattr mod kernel_name precompile __str__ - str f kernel_name= module_path= module_cache_key= TritonGPUBenchmarkRequest GPUDeviceBenchmarkMixin TritonBenchmarkRequest pass TritonCPUBenchmarkRequest CPUDeviceBenchmarkMixin TritonBenchmarkRequest pass CUDABenchmarkRequest GPUDeviceBenchmarkMixin BenchmarkRequest A handle CUDA CUTLASS benchmark requests This managing lifecycle CUDA kernel benchmark including compiling source code managing workspace memory executing kernel Important Instances have serializable across process boundaries Do put CUDA Tensors here __init__ kernel_name str input_tensor_meta Union TensorMeta list TensorMeta output_tensor_meta Union TensorMeta list TensorMeta extra_args Iterable Any source_code str - None super __init__ kernel_name input_tensor_meta output_tensor_meta extra_args source_code = source_code workspace_size int = workspace Optional torch Tensor = None DLL Optional DLLWrapper = None _workspace_size_updated = False hash_key str = source_file str = hash_key source_file = CUDACodeCache write source_code so precompile Precompile CUDA source code populate CUDACodeCache This may happen separate thread pool autotuning_log debug Precompiling s CUDACodeCache compile source_code so autotuning_log debug Done precompiling s make_run_fn input_tensors torch Tensor out torch Tensor - Callable None Create function run CUDA kernel given input output tensors ensure_dll_loaded update_workspace_size args = c_void_p tensor data_ptr tensor list input_tensors + out autotuning_log debug make_run_fn kernel_name= s source_file= s hash_key= s DLL= s args= s extra_args= s kernel_name source_file hash_key DLL args extra_args stream_ptr = c_void_p torch cuda current_stream cuda_stream run_method = getattr DLL kernel_name workspace_ptr = c_void_p workspace_size workspace = torch zeros workspace_size + dtype=torch float device=out device workspace_ptr = c_void_p workspace data_ptr Generate partial function ret = functools partial run_method args extra_args None null workspace size ptr workspace_ptr set workspace ptr stream_ptr sanity check make sure we cleanup run fn properly try ret except RuntimeError e err_msg = str e raise_runtime_error raise RuntimeError err_msg cleanup_run_fn raise_runtime_error ret update_workspace_size - None _workspace_size_updated ensure_dll_loaded unique_input_count = len dict fromkeys meta name meta input_tensor_meta args = c_void_p None _ range unique_input_count + stream_ptr = c_void_p torch cuda current_stream cuda_stream run_method = getattr DLL kernel_name Retrieve workspace_size initialize workspace c_workspace_size = c_size_t run_method args input ptrs output ptrs extra_args byref c_workspace_size set workspace size ptr retrieve workspace size None null workspace ptr stream_ptr torch cuda synchronize shake out any CUDA errors workspace_size = c_workspace_size value autotuning_log debug update_workspace_size called new workspace size= d kernel_name= s source_file= s hash_key= s DLL= s args= s extra_args= s noqa B workspace_size kernel_name source_file hash_key DLL args extra_args _workspace_size_updated = True ensure_dll_loaded DLL None DLL hash_key source_file = CUDACodeCache load source_code so cleanup_run_fn - None DLL None DLL close DLL = None workspace = None __str__ - str f kernel_name= source_file= hash_key= CppBenchmarkRequest CPUDeviceBenchmarkMixin BenchmarkRequest Important Instances have serializable across process boundaries Do put Tensors here __init__ kernel_name str input_tensor_meta Union TensorMeta list TensorMeta output_tensor_meta Union TensorMeta list TensorMeta extra_args Iterable Any source_code str - None super __init__ kernel_name input_tensor_meta output_tensor_meta extra_args source_code = source_code hash_key = get_hash source_code DLL Optional Union CDLL ModuleType = None precompile Prepopulate CppCodeCache may happen separate Threadpool autotuning_log debug Precompiling s CppCodeCache load source_code device_type= cpu autotuning_log debug Done precompiling s make_run_fn input_tensors torch Tensor out torch Tensor - Callable None TODO jgong use CppPythonBindingsCodeCache better binding perf DLL = CppCodeCache load source_code device_type= cpu args = tensor data_ptr tensor list input_tensors + out autotuning_log debug make_run_fn kernel_name= s DLL= s args= s extra_args= s kernel_name DLL args extra_args run_method = getattr DLL kernel_name Assume only size type ctypes c_ulonglong extra_args assert all isinstance arg ctypes c_ulonglong arg extra_args run_method argtypes = ctypes c_ulonglong len args + len list extra_args Generate partial function functools partial run_method args extra_args cleanup_run_fn - None DLL None Check close attr due crash Windows hasattr DLL close DLL close __str__ - str f kernel_name= CuteDSLBenchmarkRequest GPUDeviceBenchmarkMixin BenchmarkRequest Benchmark request CuteDSL CUTLASS Python DSL kernels __init__ kernel_name str input_tensor_meta Union TensorMeta list TensorMeta output_tensor_meta Union TensorMeta list TensorMeta extra_args tuple Any source_code PartialRender - None super __init__ kernel_name input_tensor_meta output_tensor_meta extra_args finalized_code = source_code finalize_all module_cache_key module_path = PyCodeCache write finalized_code make_run_fn input_tensors torch Tensor out torch Tensor - Callable None Create function run CuteDSL kernel given input output tensors Similar TritonBenchmarkRequest make_run_fn CuteDSL kernels mod = PyCodeCache load_by_key_path module_cache_key module_path Logic replicated async_compile codegen cutedsl cutedsl_kernel MAIN_SUFFIX main_func_name = f kernel_name _ MAIN_SUFFIX hasattr mod main_func_name available = name name dir mod callable getattr mod name raise RuntimeError f Could find CuteDSL main kernel function main_func_name Available callables available kernel_func = getattr mod main_func_name run_kernel device_interface = get_interface_for_device cuda stream = device_interface get_raw_stream out device index kernel_func input_tensors out stream=stream run_kernel cleanup_run_fn - None Clean up any resources used kernel functools cache get_tuning_process_pool - TuningProcessPool pool = TuningProcessPool atexit register pool shutdown pool benchmark_in_sub_process choices list TritonTemplateCaller - dict TritonTemplateCaller float Do benchmarking subprocess perf number latency get_tuning_process_pool benchmark choices