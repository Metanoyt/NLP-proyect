Owner s module cuda unittest torch torch testing _internal common_utils common torch testing _internal common_cuda TEST_CUDA TEST_MULTIGPU TEST_NUMBA_CUDA torch testing _internal common_utils TEST_NUMPY TEST_NUMPY numpy TEST_NUMBA_CUDA numba cuda TestNumbaIntegration common TestCase unittest skipIf TEST_NUMPY No numpy unittest skipIf TEST_CUDA No cuda test_cuda_array_interface torch Tensor exposes __cuda_array_interface__ cuda tensors An object t considered cuda-tensor hasattr t __cuda_array_interface__ A cuda-tensor provides tensor description dict shape integer Tensor shape strides integer Tensor strides bytes typestr str A numpy-style typestr data int boolean A data_ptr read-only tuple version int Version See https numba pydata org numba-doc dev cuda cuda_array_interface html types = torch DoubleTensor torch FloatTensor torch HalfTensor torch LongTensor torch IntTensor torch ShortTensor torch CharTensor torch ByteTensor dtypes = numpy float numpy float numpy float numpy int numpy int numpy int numpy int numpy uint tp npt zip types dtypes CPU tensors do implement interface cput = tp assertFalse hasattr cput __cuda_array_interface__ assertRaises AttributeError lambda cput __cuda_array_interface__ Sparse CPU CUDA tensors do implement interface tp torch HalfTensor indices_t = torch empty cput size dtype=torch long clamp_ min= sparse_t = torch sparse_coo_tensor indices_t cput assertFalse hasattr sparse_t __cuda_array_interface__ assertRaises AttributeError lambda sparse_t __cuda_array_interface__ sparse_cuda_t = torch sparse_coo_tensor indices_t cput cuda assertFalse hasattr sparse_cuda_t __cuda_array_interface__ assertRaises AttributeError lambda sparse_cuda_t __cuda_array_interface__ CUDA tensors have attribute v interface cudat = tp cuda assertTrue hasattr cudat __cuda_array_interface__ ar_dict = cudat __cuda_array_interface__ assertEqual set ar_dict keys shape strides typestr data version assertEqual ar_dict shape assertIs ar_dict strides None typestr numpy cuda-native little-endian assertEqual ar_dict typestr numpy dtype npt newbyteorder str assertEqual ar_dict data cudat data_ptr False assertEqual ar_dict version unittest skipIf TEST_CUDA No cuda unittest skipIf TEST_NUMBA_CUDA No numba cuda test_array_adaptor Torch __cuda_array_adaptor__ exposes tensor data numba cuda torch_dtypes = torch complex torch complex torch float torch float torch float torch uint torch int torch uint torch int torch uint torch int torch uint torch int torch bool dt torch_dtypes CPU tensors all types do register cuda arrays attempts convert raise type error cput = torch arange dt npt = cput numpy assertTrue numba cuda is_cuda_array cput assertRaises TypeError numba cuda as_cuda_array cput Any cuda tensor cuda array cudat = cput device= cuda assertTrue numba cuda is_cuda_array cudat numba_view = numba cuda as_cuda_array cudat assertIsInstance numba_view numba cuda devicearray DeviceNDArray The reported type cuda array matches numpy type cpu tensor assertEqual numba_view dtype npt dtype assertEqual numba_view strides npt strides assertEqual numba_view shape cudat shape Pass back cuda host all equality checks below needed float comparisons which aren t supported cpu-side The data identical view assertEqual cudat torch tensor numba_view copy_to_host cuda Writes torch Tensor reflected numba array cudat = assertEqual cudat torch tensor numba_view copy_to_host cuda Strided tensors supported strided_cudat = cudat strided_npt = cput numpy strided_numba_view = numba cuda as_cuda_array strided_cudat assertEqual strided_numba_view dtype strided_npt dtype assertEqual strided_numba_view strides strided_npt strides assertEqual strided_numba_view shape strided_cudat shape As numba support strided views limited Cannot verify correctness strided view operations unittest skipIf TEST_CUDA No cuda unittest skipIf TEST_NUMBA_CUDA No numba cuda test_conversion_errors Numba properly detects array interface tensor Tensor variants CPU tensors cuda arrays cput = torch arange assertFalse numba cuda is_cuda_array cput assertRaises TypeError numba cuda as_cuda_array cput Sparse tensors cuda arrays regardless device sparset = torch sparse_coo_tensor cput None cput assertFalse numba cuda is_cuda_array sparset assertRaises TypeError numba cuda as_cuda_array sparset sparset cuda assertFalse numba cuda is_cuda_array sparset assertRaises TypeError numba cuda as_cuda_array sparset Device-status overrides gradient status CPU+gradient isn t cuda array cpu_gradt = torch zeros requires_grad_ True assertFalse numba cuda is_cuda_array cpu_gradt assertRaises TypeError numba cuda as_cuda_array cpu_gradt CUDA+gradient raises RuntimeError check conversion Use hasattr interface detection causes interface change python swallows all exceptions just AttributeError cuda_gradt = torch zeros requires_grad_ True cuda conversion raises RuntimeError assertRaises RuntimeError numba cuda is_cuda_array cuda_gradt assertRaises RuntimeError numba cuda as_cuda_array cuda_gradt unittest skipIf TEST_CUDA No cuda unittest skipIf TEST_NUMBA_CUDA No numba cuda unittest skipIf TEST_MULTIGPU No multigpu test_active_device as_cuda_array tensor device must match active numba context Both torch numba default device can interop freely cudat = torch arange device= cuda assertEqual cudat device index assertIsInstance numba cuda as_cuda_array cudat numba cuda devicearray DeviceNDArray Tensors non-default device raise api error converted cudat = torch arange device=torch device cuda assertRaises numba cuda driver CudaAPIError numba cuda as_cuda_array cudat can converted when switching device s context numba cuda devices gpus cudat device index assertIsInstance numba cuda as_cuda_array cudat numba cuda devicearray DeviceNDArray unittest skip Test temporary disabled see https github com pytorch pytorch issues unittest skipIf TEST_NUMPY No numpy unittest skipIf TEST_CUDA No cuda unittest skipIf TEST_NUMBA_CUDA No numba cuda test_from_cuda_array_interface torch as_tensor torch tensor supports __cuda_array_interface__ protocol If object exposes __cuda_array_interface__ as_tensor tensor will use exposed device memory See https numba pydata org numba-doc dev cuda cuda_array_interface html dtypes = numpy complex numpy complex numpy float numpy float numpy int numpy int numpy int numpy int numpy uint dtype dtypes numpy_arys = numpy ones dtype=dtype numpy arange reshape astype dtype numpy arange reshape astype dtype View offset should ignored numpy arange reshape astype dtype None change strides still contiguous Zero-copy when using ` torch as_tensor ` numpy_ary numpy_arys numba_ary = numba cuda to_device numpy_ary torch_ary = torch as_tensor numba_ary device= cuda assertEqual numba_ary __cuda_array_interface__ torch_ary __cuda_array_interface__ assertEqual torch_ary cpu data numpy numpy asarray numba_ary dtype=dtype Check ` torch_ary ` ` numba_ary ` points same device memory torch_ary += assertEqual torch_ary cpu data numpy numpy asarray numba_ary dtype=dtype Implicit-copy because ` torch_ary ` CPU array numpy_ary numpy_arys numba_ary = numba cuda to_device numpy_ary torch_ary = torch as_tensor numba_ary device= cpu assertEqual torch_ary data numpy numpy asarray numba_ary dtype=dtype Check ` torch_ary ` ` numba_ary ` points different memory torch_ary += assertEqual torch_ary data numpy numpy asarray numba_ary dtype=dtype + Explicit-copy when using ` torch tensor ` numpy_ary numpy_arys numba_ary = numba cuda to_device numpy_ary torch_ary = torch tensor numba_ary device= cuda assertEqual torch_ary cpu data numpy numpy asarray numba_ary dtype=dtype Check ` torch_ary ` ` numba_ary ` points different memory torch_ary += assertEqual torch_ary cpu data numpy numpy asarray numba_ary dtype=dtype + unittest skipIf TEST_NUMPY No numpy unittest skipIf TEST_CUDA No cuda unittest skipIf TEST_NUMBA_CUDA No numba cuda test_from_cuda_array_interface_inferred_strides torch as_tensor numba_ary should have correct inferred contiguous strides This could theory combined test_from_cuda_array_interface test overly strict checks exported protocols exactly same which cannot handle differing exported protocol versions dtypes = numpy float numpy float numpy int numpy int numpy int numpy int numpy uint dtype dtypes numpy_ary = numpy arange reshape astype dtype numba_ary = numba cuda to_device numpy_ary assertTrue numba_ary is_c_contiguous torch_ary = torch as_tensor numba_ary device= cuda assertTrue torch_ary is_contiguous unittest skip Test temporary disabled see https github com pytorch pytorch issues unittest skipIf TEST_NUMPY No numpy unittest skipIf TEST_CUDA No cuda unittest skipIf TEST_NUMBA_CUDA No numba cuda test_from_cuda_array_interface_lifetime torch as_tensor obj tensor grabs reference obj so lifetime obj exceeds tensor numba_ary = numba cuda to_device numpy arange torch_ary = torch as_tensor numba_ary device= cuda assertEqual torch_ary __cuda_array_interface__ numba_ary __cuda_array_interface__ No copy del numba_ary assertEqual torch_ary cpu data numpy numpy arange ` torch_ary ` still alive unittest skip Test temporary disabled see https github com pytorch pytorch issues unittest skipIf TEST_NUMPY No numpy unittest skipIf TEST_CUDA No cuda unittest skipIf TEST_NUMBA_CUDA No numba cuda unittest skipIf TEST_MULTIGPU No multigpu test_from_cuda_array_interface_active_device torch as_tensor tensor device must match active numba context Zero-copy both torch numba default device can interop freely numba_ary = numba cuda to_device numpy arange torch_ary = torch as_tensor numba_ary device= cuda assertEqual torch_ary cpu data numpy numpy asarray numba_ary assertEqual torch_ary __cuda_array_interface__ numba_ary __cuda_array_interface__ Implicit-copy when Numba Torch device differ numba_ary = numba cuda to_device numpy arange torch_ary = torch as_tensor numba_ary device=torch device cuda assertEqual torch_ary get_device assertEqual torch_ary cpu data numpy numpy asarray numba_ary = torch_ary __cuda_array_interface__ = numba_ary __cuda_array_interface__ assertNotEqual data data del data del data assertEqual __name__ == __main__ common run_tests