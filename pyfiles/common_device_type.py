mypy ignore-errors copy gc inspect os runpy sys threading unittest collections namedtuple collections abc Callable Iterable Sequence enum Enum functools partial wraps typing Any ClassVar Optional TypeVar Union typing_extensions ParamSpec torch torch _inductor utils GPU_TYPES torch testing _internal common_cuda _get_torch_cuda_version _get_torch_rocm_version TEST_CUSPARSE_GENERIC TEST_HIPSPARSE_GENERIC torch testing _internal common_dtype get_all_dtypes torch testing _internal common_utils _TestParametrizer clear_tracked_input compose_parametrize_fns dtype_name get_tracked_input IS_FBCODE IS_MACOS is_privateuse _backend_available IS_REMOTE_GPU IS_S X IS_SANDCASTLE IS_WINDOWS NATIVE_DEVICES PRINT_REPRO_ON_FAILURE skipCUDANonDefaultStreamIf skipIfTorchDynamo TEST_HPU TEST_MKL TEST_MPS TEST_WITH_ASAN TEST_WITH_MIOPEN_SUGGEST_NHWC TEST_WITH_MTIA TEST_WITH_ROCM TEST_WITH_TORCHINDUCTOR TEST_WITH_TSAN TEST_WITH_UBSAN TEST_XPU TestCase _T = TypeVar _T _P = ParamSpec _P try psutil type ignore HAS_PSUTIL = True except ModuleNotFoundError HAS_PSUTIL = False psutil = None Note Writing Test Templates ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ This note written shortly after PyTorch release If you notice s out-of-date think could improved then please file issue PyTorch has its own framework instantiating test templates That taking test classes look similar unittest pytest compatible test classes optionally doing following - instantiating version test each available device type often CPU CUDA META device types - further instantiating version each test s always specialized test s device type optionally specialized further datatypes operators This functionality similar pytest s parametrize functionality see https docs pytest org en x parametrize html considerable additional logic specializes instantiated test classes their device types see CPUTestBase CUDATestBase below supports variety composable decorators allow test filtering setting tolerances allows tests parametrized operators instantiate only subset device type x dtype operator supports This framework built make easier write tests run multiple device types multiple datatypes dtypes multiple operators It s also useful controlling which tests run For example only tests use CUDA device can run platforms CUDA Let s dive example get idea how works -------------------------------------------------------- A template looks like regular unittest TestCase TestClassFoo TestCase A template test can specialized device NOTE test case runnable unittest pytest because accepts extra positional argument device they do understand test_bar device pass Function instantiates template its tests instantiate_device_type_tests TestCommon globals -------------------------------------------------------- In above code example we see template single test template can instantiated device The function instantiate_device_type_tests called file scope instantiates new test classes one per available device type new tests those classes these templates It actually does removing TestClassFoo replacing classes like TestClassFooCPU TestClassFooCUDA instantiated test classes inherit CPUTestBase CUDATestBase respectively Additional device types like XLA see https github com pytorch xla can further extend set instantiated test classes create classes like TestClassFooXLA The test template test_bar also instantiated In case template only specialized device so depending available device types might become test_bar_cpu TestClassFooCPU test_bar_cuda TestClassFooCUDA We can think instantiated test classes looking like -------------------------------------------------------- An instantiated test CPU device type TestClassFooCPU CPUTestBase An instantiated test calls template string representation device test s device type test_bar_cpu test_bar cpu An instantiated test CUDA device type TestClassFooCUDA CUDATestBase An instantiated test calls template string representation device test s device type test_bar_cuda test_bar cuda -------------------------------------------------------- These instantiated test classes ARE discoverable runnable both unittest pytest One thing may confusing however attempting run test_bar will work despite appearing original template code This because test_bar no longer discoverable after instantiate_device_type_tests runs above snippet shows Instead test_bar_cpu test_bar_cuda may run directly both can run option -k test_bar Removing template adding instantiated classes requires passing globals instantiate_device_type_tests because edits file s Python objects As mentioned tests can additionally parametrized dtypes operators Datatype parametrization uses dtypes decorator require test template like -------------------------------------------------------- A template test can specialized device datatype dtype dtypes torch float torch int test_car device dtype pass -------------------------------------------------------- If CPU CUDA device types available test would instantiated tests cover cross-product two dtypes two device types - test_car_cpu_float - test_car_cpu_int - test_car_cuda_float - test_car_cuda_int The dtype passed torch dtype object Tests parametrized operators actually OpInfos more moment use ops decorator require test template like -------------------------------------------------------- A template test can specialized device dtype OpInfo ops op_db test_car device dtype op pass -------------------------------------------------------- See documentation ops decorator below additional details how use see note OpInfos common_methods_invocations py more details OpInfos A test parametrized over entire op_db which contains hundreds OpInfos will likely have hundreds thousands instantiations The test will instantiated cross-product device types operators dtypes operator supports device type The instantiated tests will have names like - test_car_add_cpu_float - test_car_sub_cuda_int The first instantiated test calls original test_car OpInfo torch add its op argument string cpu its device argument dtype torch float dtype argument The second instantiated test calls test_car OpInfo torch sub CUDA device string like cuda cuda its device argument dtype torch int its dtype argument In addition parametrizing over device dtype ops via OpInfos parametrize decorator supported arbitrary parametrizations -------------------------------------------------------- A template test can specialized device dtype value x parametrize x range test_car device dtype x pass -------------------------------------------------------- See documentation parametrize common_utils py additional details Note instantiate_device_type_tests function will handle such parametrizations there no need additionally call instantiate_parametrized_tests Clever test filtering can very useful when working parametrized tests -k test_car would run every instantiated variant test_car test template -k test_car_add runs every variant instantiated torch add It important use passed device dtype appropriate Use helper functions like make_tensor require explicitly specifying device dtype so they re forgotten Test templates can use variety composable decorators specify additional options requirements some listed here - deviceCountAtLeast minimum number devices run test Passes list strings representing all available devices test s device type test template s device argument If there fewer devices than value passed decorator test skipped - dtypes list tuples dtypes In addition accepting multiple dtypes dtypes decorator can accept sequence tuple pairs dtypes The test template will called each tuple its dtype argument - onlyNativeDeviceTypes Skips test device native device type currently CPU CUDA Meta - onlyCPU Skips test device CPU device - onlyCUDA Skips test device CUDA device - onlyMPS Skips test device MPS device - skipCPUIfNoLapack Skips test device CPU device LAPACK installed - skipCPUIfNoMkl Skips test device CPU device MKL installed - skipCUDAIfNoMagma Skips test device CUDA device MAGMA installed - skipCUDAIfRocm Skips test device CUDA device ROCm being used Note Adding Device Type ~~~~~~~~~~~~~~~~~~~~~~~~~~~ To add device type Create new TestBase extending DeviceTypeTestBase See CPUTestBase CUDATestBase below Define device_type attribute base appropriate string Add logic file appends your base device_type_test_bases when your device type available Optional Write setUpClass tearDownClass methods instantiate dependencies see MAGMA CUDATestBase Optional Override instantiate_test method total control over how your creates tests setUpClass called AFTER tests have been created BEFORE ONLY IF they run This makes useful initializing devices dependencies _dtype_test_suffix dtypes Returns test suffix dtype sequence dtypes None isinstance dtypes list tuple len dtypes == _ + _ join dtype_name d d dtypes dtypes f _ dtype_name dtypes _update_param_kwargs param_kwargs name value Adds kwarg specified name value param_kwargs dict Make name plural e g devices dtypes value composite plural_name = f name s Clear out old entries arg any name param_kwargs del param_kwargs name plural_name param_kwargs del param_kwargs plural_name isinstance value list tuple param_kwargs plural_name = value value None param_kwargs name = value Leave param_kwargs as-is when value None DeviceTypeTestBase TestCase device_type str = generic_device_type Flag disable test suite early due unrecoverable error such CUDA error _stop_test_suite = False Precision thread-local setting since may overridden per test _tls = threading local _tls precision = TestCase _precision _tls rel_tol = TestCase _rel_tol property precision _tls precision precision setter precision prec _tls precision = prec property rel_tol _tls rel_tol rel_tol setter rel_tol prec _tls rel_tol = prec Returns string representing device single device tests should use Note single device tests use device exclusively classmethod get_primary_device cls cls device_type classmethod _init_and_get_primary_device cls try cls get_primary_device except Exception For CUDATestBase XPUTestBase XLATestBase possibly others primary device won t available until setUpClass sets Call manually here needed hasattr cls setUpClass cls setUpClass cls get_primary_device Returns list strings representing all available devices device type The primary device must first string list list must contain no duplicates Note UNSTABLE API Will replaced once PyTorch has device generic mechanism acquiring all available devices classmethod get_all_devices cls cls get_primary_device Returns dtypes test has requested Prefers device-specific dtype specifications over generic ones classmethod _get_dtypes cls test hasattr test dtypes None default_dtypes = test dtypes get all msg = f dtypes mandatory when using dtypesIf however test __name__ didn t specify assert default_dtypes None msg test dtypes get cls device_type default_dtypes _get_precision_override test dtype hasattr test precision_overrides precision test precision_overrides get dtype precision _get_tolerance_override test dtype hasattr test tolerance_overrides precision rel_tol test tolerance_overrides get dtype tol precision rel_tol _apply_precision_override_for_test test param_kwargs dtype = param_kwargs get dtype dtype = param_kwargs get dtypes dtype dtype precision = _get_precision_override test dtype precision rel_tol = _get_tolerance_override test dtype Creates device-specific tests classmethod instantiate_test cls name test generic_cls=None instantiate_test_helper cls name test param_kwargs=None decorator_fn=lambda _ Add device param kwarg test needs device devices param_kwargs = param_kwargs None param_kwargs test_sig_params = inspect signature test parameters device test_sig_params devices test_sig_params device_arg str = cls _init_and_get_primary_device hasattr test num_required_devices device_arg = cls get_all_devices _update_param_kwargs param_kwargs device device_arg Apply decorators based param kwargs decorator decorator_fn param_kwargs test = decorator test Constructs test wraps test instantiated_test param_kwargs=param_kwargs Sets precision runs test Note precision reset after test run guard_precision = precision guard_rel_tol = rel_tol try _apply_precision_override_for_test test param_kwargs result = test param_kwargs except RuntimeError rte check rte should stop entire test suite _stop_test_suite = _should_stop_test_suite Check test has been decorated ` expectedFailure ` Using ` __unittest_expecting_failure__ ` attribute see https github com python cpython blob ffa b Lib unittest case py#L In case make fail unexpected success suppressing exception getattr test __unittest_expecting_failure__ False _stop_test_suite sys print Suppressing fatal exception trigger unexpected success file=sys stderr raise runtime error test suite record raise rte finally precision = guard_precision rel_tol = guard_rel_tol result assert hasattr cls name f Redefinition test name setattr cls name instantiated_test default_parametrize_fn test generic_cls device_cls By default no parametrization needed yield test lambda _ Parametrization decorators set parametrize_fn attribute test parametrize_fn = getattr test parametrize_fn default_parametrize_fn If one dtypes decorators present also parametrize over dtypes set dtypes = cls _get_dtypes test dtypes None dtype_parametrize_fn test generic_cls device_cls dtypes=dtypes dtype dtypes param_kwargs dict str Any = _update_param_kwargs param_kwargs dtype dtype Note empty test suffix set here so dtype can appended later after device yield test param_kwargs lambda _ parametrize_fn = compose_parametrize_fns dtype_parametrize_fn parametrize_fn Instantiate parametrized tests test noqa B test_suffix param_kwargs decorator_fn parametrize_fn test generic_cls cls test_suffix = test_suffix == _ + test_suffix cls_device_type = cls device_type cls device_type = privateuse torch _C _get_privateuse _backend_name device_suffix = _ + cls_device_type Note device dtype suffix placement Special handling here place dtype s after device according test name convention dtype_kwarg = None dtype param_kwargs dtypes param_kwargs dtype_kwarg = param_kwargs dtypes dtypes param_kwargs param_kwargs dtype test_name = f name test_suffix device_suffix _dtype_test_suffix dtype_kwarg instantiate_test_helper cls=cls name=test_name test=test param_kwargs=param_kwargs decorator_fn=decorator_fn run result=None super run result=result Early terminate test _stop_test_suite set _stop_test_suite result stop CPUTestBase DeviceTypeTestBase device_type = cpu No critical error should stop CPU test suite _should_stop_test_suite False CUDATestBase DeviceTypeTestBase device_type = cuda _do_cuda_memory_leak_check = True _do_cuda_non_default_stream = True primary_device ClassVar str cudnn_version ClassVar Any no_magma ClassVar bool no_cudnn ClassVar bool has_cudnn no_cudnn classmethod get_primary_device cls cls primary_device classmethod get_all_devices cls primary_device_idx = int cls get_primary_device split num_devices = torch cuda device_count prim_device = cls get_primary_device cuda_str = cuda non_primary_devices = cuda_str format idx idx range num_devices idx = primary_device_idx prim_device + non_primary_devices classmethod setUpClass cls has_magma shows up after cuda initialized t = torch ones cuda cls no_magma = torch cuda has_magma Determines cuDNN available its version cls no_cudnn = torch backends cudnn is_acceptable t cls cudnn_version = None cls no_cudnn torch backends cudnn version Acquires current device primary test device cls primary_device = f cuda torch cuda current_device See Note Lazy Tensor tests device agnostic testing lazy_ts_backend_init = False LazyTestBase DeviceTypeTestBase device_type = lazy _should_stop_test_suite False classmethod setUpClass cls torch _lazy torch _lazy metrics torch _lazy ts_backend global lazy_ts_backend_init lazy_ts_backend_init Need connect TS backend lazy key before running tests torch _lazy ts_backend init lazy_ts_backend_init = True MPSTestBase DeviceTypeTestBase device_type = mps primary_device ClassVar str classmethod get_primary_device cls cls primary_device classmethod get_all_devices cls currently only one device supported MPS backend prim_device = cls get_primary_device prim_device classmethod setUpClass cls cls primary_device = mps _should_stop_test_suite False XPUTestBase DeviceTypeTestBase device_type = xpu primary_device ClassVar str classmethod get_primary_device cls cls primary_device classmethod get_all_devices cls currently only one device supported MPS backend primary_device_idx = int cls get_primary_device split num_devices = torch xpu device_count prim_device = cls get_primary_device xpu_str = xpu non_primary_devices = xpu_str format idx idx range num_devices idx = primary_device_idx prim_device + non_primary_devices classmethod setUpClass cls cls primary_device = f xpu torch xpu current_device _should_stop_test_suite False HPUTestBase DeviceTypeTestBase device_type = hpu primary_device ClassVar str classmethod get_primary_device cls cls primary_device classmethod setUpClass cls cls primary_device = hpu PrivateUse TestBase DeviceTypeTestBase primary_device ClassVar str device_mod = None device_type = privateuse classmethod get_primary_device cls cls primary_device classmethod get_all_devices cls primary_device_idx = int cls get_primary_device split num_devices = cls device_mod device_count prim_device = cls get_primary_device device_str = f cls device_type non_primary_devices = device_str format idx idx range num_devices idx = primary_device_idx prim_device + non_primary_devices classmethod setUpClass cls cls device_type = torch _C _get_privateuse _backend_name cls device_mod = getattr torch cls device_type None assert cls device_mod None f torch has no module ` cls device_type ` you should register module ` torch _register_device_module ` cls primary_device = f cls device_type cls device_mod current_device Adds available device-type-specific test base classes get_device_type_test_bases set type List Any due mypy list-of-union issue https github com python mypy issues test_bases list Any = IS_SANDCASTLE IS_FBCODE IS_REMOTE_GPU Skip sanitizer enabled we re MTIA machines TEST_WITH_ASAN TEST_WITH_TSAN TEST_WITH_UBSAN TEST_WITH_MTIA test_bases append CUDATestBase test_bases append CPUTestBase test_bases append CPUTestBase torch cuda is_available test_bases append CUDATestBase is_privateuse _backend_available test_bases append PrivateUse TestBase Disable MPS testing generic device testing temporarily while we re ramping up support torch backends mps is_available test_bases append MPSTestBase test_bases device_type_test_bases = get_device_type_test_bases filter_desired_device_types device_type_test_bases except_for=None only_for=None device type cannot appear both except_for only_for intersect = set except_for except_for set only_for only_for assert intersect f device intersect appeared both except_for only_for Replace your privateuse backend name privateuse is_privateuse _backend_available privateuse _backend_name = torch _C _get_privateuse _backend_name func_replace x str x replace privateuse _backend_name privateuse except_for = func_replace x x except_for except_for None None isinstance except_for str func_replace except_for only_for = func_replace x x only_for only_for None None isinstance only_for str func_replace only_for except_for device_type_test_bases = filter lambda x x device_type except_for device_type_test_bases only_for device_type_test_bases = filter lambda x x device_type only_for device_type_test_bases list device_type_test_bases Note How extend DeviceTypeTestBase add new test device The following logic optionally allows downstream projects like pytorch xla add more test devices Instructions - Add python file e g pytorch xla test pytorch_test_base py downstream project - Inside file one should inherit ` DeviceTypeTestBase ` define new DeviceTypeTest e g ` XLATestBase ` proper implementation ` instantiate_test ` method - DO NOT common_device_type inside file ` runpy run_path ` ` globals ` already properly setup context so ` DeviceTypeTestBase ` already available - Set top-level variable ` TEST_CLASS ` equal your new E g TEST_CLASS = XLATensorBase - To run tests new device type set ` TORCH_TEST_DEVICE ` env variable path file Multiple paths can separated ` ` See pytorch xla test pytorch_test_base py more detailed example _TORCH_TEST_DEVICES = os environ get TORCH_TEST_DEVICES None _TORCH_TEST_DEVICES path _TORCH_TEST_DEVICES split runpy stdlib module lacks annotations mod = runpy run_path path init_globals=globals type ignore func-returns-value device_type_test_bases append mod TEST_CLASS PYTORCH_CUDA_MEMCHECK = os getenv PYTORCH_CUDA_MEMCHECK == PYTORCH_TESTING_DEVICE_ONLY_FOR_KEY = PYTORCH_TESTING_DEVICE_ONLY_FOR PYTORCH_TESTING_DEVICE_EXCEPT_FOR_KEY = PYTORCH_TESTING_DEVICE_EXCEPT_FOR PYTORCH_TESTING_DEVICE_FOR_CUSTOM_KEY = PYTORCH_TESTING_DEVICE_FOR_CUSTOM get_desired_device_type_test_bases except_for=None only_for=None include_lazy=False allow_mps=False allow_xpu=False allow callers specifically opt tests into being tested MPS similar ` include_lazy ` test_bases = device_type_test_bases copy allow_mps TEST_MPS MPSTestBase test_bases test_bases append MPSTestBase allow_xpu TEST_XPU XPUTestBase test_bases test_bases append XPUTestBase TEST_HPU HPUTestBase test_bases test_bases append HPUTestBase Filter out device types based user inputs desired_device_type_test_bases = filter_desired_device_types test_bases except_for only_for include_lazy Note Lazy Tensor tests device agnostic testing Right now test_view_ops py runs LazyTensor We don t want opt every device-agnostic test into using lazy device because many them will fail So instead only way opt specific device-agnostic test file into lazy tensor testing include_lazy=True IS_FBCODE print TorchScript backend yet supported FBCODE OVRSOURCE builds file=sys stderr desired_device_type_test_bases append LazyTestBase split_if_not_empty x str x split x run some cuda testcases other devices available Usage export PYTORCH_TESTING_DEVICE_FOR_CUSTOM=privateuse env_custom_only_for = split_if_not_empty os getenv PYTORCH_TESTING_DEVICE_FOR_CUSTOM_KEY env_custom_only_for desired_device_type_test_bases += filter lambda x x device_type env_custom_only_for test_bases desired_device_type_test_bases = list set desired_device_type_test_bases Filter out device types based environment variables available Usage export PYTORCH_TESTING_DEVICE_ONLY_FOR=cuda cpu export PYTORCH_TESTING_DEVICE_EXCEPT_FOR=xla env_only_for = split_if_not_empty os getenv PYTORCH_TESTING_DEVICE_ONLY_FOR_KEY env_except_for = split_if_not_empty os getenv PYTORCH_TESTING_DEVICE_EXCEPT_FOR_KEY filter_desired_device_types desired_device_type_test_bases env_except_for env_only_for Adds instantiated device-specific test cases given scope The tests these test cases derived generic tests generic_test_class This function should used instead instantiate_parametrized_tests test contains device-specific tests NB supports additional parametrize usage See note Writing Test Templates TODO remove allow_xpu option after Interl GPU support all test case instantiate function instantiate_device_type_tests generic_test_class scope except_for=None only_for=None include_lazy=False allow_mps=False allow_xpu=False Removes generic test its enclosing scope so its tests discoverable del scope generic_test_class __name__ generic_members = set generic_test_class __dict__ keys generic_tests = x x generic_members x startswith test Creates device-specific test cases base get_desired_device_type_test_bases except_for only_for include_lazy allow_mps allow_xpu class_name = generic_test_class __name__ + base device_type upper type set Any suppressed due unsupported runtime https github com python mypy wiki Unsupported-Python-Features device_type_test_class Any = type class_name base generic_test_class Arrange setUpClass tearDownClass methods defined both test template generic base called This allows device-parameterized test classes support setup teardown NB This should done before instantiate_test called invokes setup classmethod _setUpClass cls This should always called whether test invokes super setUpClass set primary device base setUpClass We want call classmethod defined generic base pass device-specific object cls hence __func__ call generic_test_class setUpClass __func__ cls classmethod _tearDownClass cls We want call classmethod defined generic base pass device-specific object cls hence __func__ call generic_test_class tearDownClass __func__ cls base tearDownClass device_type_test_class setUpClass = _setUpClass device_type_test_class tearDownClass = _tearDownClass name generic_members name generic_tests Instantiates test member test = getattr generic_test_class name XLA-compat shim XLA s instantiate_test takes doesn t take generic_cls sig = inspect signature device_type_test_class instantiate_test len sig parameters == Instantiates device-specific tests device_type_test_class instantiate_test name copy deepcopy test generic_cls=generic_test_class device_type_test_class instantiate_test name copy deepcopy test Ports non-test member Setup teardown have already been handled above name device_type_test_class __dict__ nontest = getattr generic_test_class name setattr device_type_test_class name nontest Mimics defining instantiated caller s file setting its module given s adding module given scope This lets instantiated discovered unittest device_type_test_class __module__ = generic_test_class __module__ scope class_name = device_type_test_class Delete generic form test functions e g TestFoo test_bar so they re discoverable This mutates original TestFoo which removed scope above At point device-specific tests e g TestFooCUDA test_bar_cuda have already been created generic forms no longer needed name generic_tests delattr generic_test_class name Category dtypes run OpInfo-based test Example use ops dtype=OpDTypes supported There categories - supported Every dtype supported operator Use exhaustive testing all dtypes - unsupported Run tests dtypes supported operator e g testing operator raises error doesn t crash - supported_backward Every dtype supported operator s backward pass - unsupported_backward Run tests dtypes supported operator s backward pass - any_one Runs test one dtype operator supports Prioritizes dtypes operator supports both forward backward - none Useful tests dtype-specific No dtype will passed test when selected - any_common_cpu_cuda_one Pick dtype supports both CPU CUDA OpDTypes Enum supported = Test all supported dtypes default unsupported = Test only unsupported dtypes supported_backward = Test all supported backward dtypes unsupported_backward = Test only unsupported backward dtypes any_one = Test precisely one supported dtype none = Instantiate no dtype variants no dtype kwarg needed any_common_cpu_cuda_one = Test precisely one supported dtype common both cuda cpu Arbitrary order ANY_DTYPE_ORDER = torch float torch float torch complex torch complex torch float torch bfloat torch long torch int torch int torch int torch uint torch bool torch float _e m fn torch float _e m _serialize_sample sample_input NB For OpInfos SampleInput summary prints cleaner way getattr sample_input summary None None sample_input summary str sample_input Decorator defines OpInfos test template should instantiated Example usage ops unary_ufuncs test_numerics device dtype op test_code This will instantiate variants test_numerics each given OpInfo each device OpInfo s operator supports every dtype supported operator There few caveats dtype rule explained below The ops decorator can accept two additional arguments dtypes allowed_dtypes If dtypes specified then test variants instantiated those dtypes regardless what operator supports If given allowed_dtypes then test variants instantiated only intersection allowed_dtypes dtypes they would otherwise instantiated That allowed_dtypes composes options listed above below The dtypes argument can also accept additional values see OpDTypes above OpDTypes supported - test instantiated all dtypes operator supports OpDTypes unsupported - test instantiated all dtypes operator doesn t support OpDTypes supported_backward - test instantiated all dtypes operator s gradient formula supports OpDTypes unsupported_backward - test instantiated all dtypes operator s gradient formula doesn t support OpDTypes any_one - test instantiated one dtype operator supports The dtype supports forward backward possible OpDTypes none - test instantiated without any dtype The test signature should include dtype kwarg case OpDTypes any_common_cpu_cuda_one - test instantiated dtype supports both CPU CUDA These options allow tests have considerable control over dtypes they re instantiated ops _TestParametrizer __init__ op_list dtypes Union OpDTypes Sequence torch dtype = OpDTypes supported allowed_dtypes Optional Sequence torch dtype = None skip_if_dynamo=True op_list = list op_list opinfo_dtypes = dtypes allowed_dtypes = set allowed_dtypes allowed_dtypes None None skip_if_dynamo = skip_if_dynamo _parametrize_test test generic_cls device_cls Parameterizes given test function across each op its associated dtypes device_cls None raise RuntimeError The ops decorator only intended used device-specific context use instantiate_device_type_tests instead instantiate_parametrized_tests op = check_exhausted_iterator = object op op_list Determine set dtypes use dtypes Union set torch dtype set None isinstance opinfo_dtypes Sequence dtypes = set opinfo_dtypes opinfo_dtypes == OpDTypes unsupported_backward dtypes = set get_all_dtypes difference op supported_backward_dtypes device_cls device_type opinfo_dtypes == OpDTypes supported_backward dtypes = op supported_backward_dtypes device_cls device_type opinfo_dtypes == OpDTypes unsupported dtypes = set get_all_dtypes difference op supported_dtypes device_cls device_type opinfo_dtypes == OpDTypes supported dtypes = set op supported_dtypes device_cls device_type opinfo_dtypes == OpDTypes any_one Tries pick dtype supports both forward backward supported = op supported_dtypes device_cls device_type supported_backward = op supported_backward_dtypes device_cls device_type supported_both = supported intersection supported_backward dtype_set = supported_both len supported_both supported dtype ANY_DTYPE_ORDER dtype dtype_set dtypes = dtype break dtypes = opinfo_dtypes == OpDTypes any_common_cpu_cuda_one Tries pick dtype supports both CPU CUDA supported = set op dtypes intersection op dtypesIfCUDA supported dtypes = next dtype dtype ANY_DTYPE_ORDER dtype supported dtypes = opinfo_dtypes == OpDTypes none dtypes = None raise RuntimeError f Unknown OpDType opinfo_dtypes allowed_dtypes None dtypes = dtypes intersection allowed_dtypes Construct test name device dtype parts handled outside See Note device dtype suffix placement test_name = op formatted_name Filter sample skips xfails only those apply OpInfo These defined test function via decorators sample_skips_and_xfails = getattr test sample_skips_and_xfails None sample_skips_and_xfails None sample_skips_and_xfails = rule rule sample_skips_and_xfails rule op_match_fn device_cls device_type op dtype dtypes Construct parameter kwargs pass test param_kwargs = op op _update_param_kwargs param_kwargs dtype dtype NOTE test_wrapper exists because we don t want apply op-specific decorators original test Test-specific decorators applied original test however try wraps test test_wrapper args kwargs try test args kwargs except unittest SkipTest e raise e except Exception e tracked_input = get_tracked_input PRINT_REPRO_ON_FAILURE tracked_input None e_tracked = Exception noqa TRY f str e \n\nCaused tracked_input type_desc f index tracked_input index f _serialize_sample tracked_input val e_tracked _tracked_input = tracked_input type ignore attr raise e_tracked e raise e finally clear_tracked_input skip_if_dynamo TEST_WITH_TORCHINDUCTOR test_wrapper = skipIfTorchDynamo Policy we don t run OpInfo tests w Dynamo test_wrapper Initialize info last input seen This useful tracking down which inputs caused test failure Note TrackedInputIter responsible managing test tracked_input = None decorator_fn = partial op get_decorators generic_cls __name__ test __name__ device_cls device_type dtype sample_skips_and_xfails None test_wrapper sample_skips_and_xfails = sample_skips_and_xfails yield test_wrapper test_name param_kwargs decorator_fn except Exception ex Provides error message debugging before rethrowing exception print f Failed instantiate test_name op op name raise ex op check_exhausted_iterator raise ValueError An empty op_list passed ops Note may result reuse generator Decorator skips test given condition true Notes Skip conditions stack Skip conditions can bools strings If string test base must have defined corresponding attribute False test run If you want use string argument you should probably define new decorator instead see below Prefer existing decorators defining device_type kwarg skipIf __init__ dep reason device_type=None dep = dep reason = reason device_type = device_type __call__ fn wraps fn dep_fn slf args kwargs device_type None device_type == slf device_type isinstance device_type Iterable slf device_type device_type isinstance dep str getattr slf dep True isinstance dep bool dep raise unittest SkipTest reason fn slf args kwargs dep_fn Skips test CPU condition true skipCPUIf skipIf __init__ dep reason super __init__ dep reason device_type= cpu Skips test CUDA condition true skipCUDAIf skipIf __init__ dep reason super __init__ dep reason device_type= cuda Skips test XPU condition true skipXPUIf skipIf __init__ dep reason super __init__ dep reason device_type= xpu Skips test XPU CUDA condition true skipGPUIf skipIf __init__ dep reason super __init__ dep reason device_type=GPU_TYPES Skips test Lazy condition true skipLazyIf skipIf __init__ dep reason super __init__ dep reason device_type= lazy Skips test Meta condition true skipMetaIf skipIf __init__ dep reason super __init__ dep reason device_type= meta Skips test MPS condition true skipMPSIf skipIf __init__ dep reason super __init__ dep reason device_type= mps skipHPUIf skipIf __init__ dep reason super __init__ dep reason device_type= hpu Skips test XLA condition true skipXLAIf skipIf __init__ dep reason super __init__ dep reason device_type= xla skipPRIVATEUSE If skipIf __init__ dep reason device_type = torch _C _get_privateuse _backend_name super __init__ dep reason device_type=device_type _has_sufficient_memory device size device_ = torch device device device_type = device_ type device_type cuda xpu acc = torch accelerator current_accelerator Case no accelerator found acc False Case accelerator found matching device type acc type = device_type True Case accelerator found matching device type available torch accelerator is_available False Case accelerator found matching device type available gc collect torch accelerator empty_cache device_ index None device_ = torch device device_type device_type == cuda torch cuda memory mem_get_info device_ torch cuda memory get_per_process_memory_fraction device_ = size device_type == xpu torch xpu memory mem_get_info device_ = size device_type == xla raise unittest SkipTest TODO Memory availability checks XLA device_type = cpu raise unittest SkipTest Unknown device type CPU HAS_PSUTIL raise unittest SkipTest Need psutil determine memory sufficient The sanitizers have significant memory overheads TEST_WITH_ASAN TEST_WITH_TSAN TEST_WITH_UBSAN effective_size = size effective_size = size don t try using all RAM s x leave some service processes IS_S X effective_size = effective_size psutil virtual_memory available effective_size gc collect psutil virtual_memory available = effective_size largeTensorTest size device=None inductor=TEST_WITH_TORCHINDUCTOR Skip test device has insufficient memory run test size may number bytes string form N GB callable If test device generic test available memory primary device will checked It can also overridden optional ` device= ` argument In other tests ` device= ` argument needs specified isinstance size str assert size endswith GB gb only bytes GB supported size = int size - inner fn wraps fn dep_fn args kwargs size_bytes int = size args kwargs callable size size _device = device _device None hasattr get_primary_device _device = get_primary_device _device = device If running GPU cpp_wrapper autotuning step will generate additional array same size input inductor torch _inductor config cpp_wrapper _device = cpu size_bytes = _has_sufficient_memory _device size_bytes raise unittest SkipTest f Insufficient _device memory fn args kwargs dep_fn inner expectedFailure __init__ device_type dtype=None device_type = device_type dtype = dtype __call__ fn wraps fn efail_fn slf args kwargs hasattr slf device_type hasattr slf device isinstance slf device str target_device_type = slf device target_device_type = slf device_type target_dtype = kwargs get dtype getattr slf dtype None device_matches = device_type None device_type == target_device_type dtype_matches = dtype None dtype == target_dtype device_matches dtype_matches try fn slf args kwargs except Exception slf fail expected test fail passed fn slf args kwargs efail_fn onlyOn __init__ device_type Union str list device_type = device_type __call__ fn wraps fn only_fn slf args kwargs slf device_type device_type reason = f Only runs device_type raise unittest SkipTest reason fn slf args kwargs only_fn Decorator provides all available devices device type test list strings instead providing single device string Skips test number available devices variant s device type less than num_required_devices arg deviceCountAtLeast __init__ num_required_devices num_required_devices = num_required_devices __call__ fn assert hasattr fn num_required_devices f deviceCountAtLeast redefinition fn __name__ fn num_required_devices = num_required_devices wraps fn multi_fn slf devices args kwargs len devices num_required_devices reason = f fewer than num_required_devices devices detected raise unittest SkipTest reason fn slf devices args kwargs multi_fn Only runs test native device type currently CPU CUDA Meta PRIVATEUSE onlyNativeDeviceTypes fn Callable _P _T - Callable _P _T wraps fn only_fn args _P args kwargs _P kwargs - _T device_type NATIVE_DEVICES reason = f onlyNativeDeviceTypes doesn t run device_type raise unittest SkipTest reason fn args kwargs only_fn Only runs test native device types devices specified devices list onlyNativeDeviceTypesAnd devices=None decorator fn wraps fn only_fn args kwargs device_type NATIVE_DEVICES device_type devices reason = f onlyNativeDeviceTypesAnd devices doesn t run device_type raise unittest SkipTest reason fn args kwargs only_fn decorator Specifies per-dtype precision overrides Ex precisionOverride torch half e- torch float e- dtypes torch half torch float torch double test_X device dtype When test instantiated its s precision will set corresponding override exists precision can accessed directly also controls behavior functions like assertEqual Note precision scalar value so you require multiple precisions working multiple dtypes they should specified explicitly computed using precision e g precision max precision precisionOverride __init__ d assert isinstance d dict precisionOverride given dtype precision dict dtype d keys assert isinstance dtype torch dtype f precisionOverride given unknown dtype dtype d = d __call__ fn fn precision_overrides = d fn Specifies per-dtype tolerance overrides tol atol rtol It has priority over precisionOverride Ex toleranceOverride torch float tol atol= e- rtol= e- torch double tol atol= e- rtol = dtypes torch half torch float torch double test_X device dtype When test instantiated its s tolerance will set corresponding override exists rtol precision can accessed directly they also control behavior functions like assertEqual The above example sets atol = e- rtol = e- torch float atol = e- rtol = torch double tol = namedtuple tol atol rtol toleranceOverride __init__ d assert isinstance d dict toleranceOverride given dtype tol dict dtype prec d items assert isinstance dtype torch dtype f toleranceOverride given unknown dtype dtype assert isinstance prec tol toleranceOverride given dtype tol dict d = d __call__ fn fn tolerance_overrides = d fn Decorator instantiates variant test each given dtype Notes Tests accept dtype argument MUST use decorator Can overridden CPU CUDA respectively using dtypesIfCPU dtypesIfCUDA Can accept iterable dtypes iterable tuples dtypes Examples dtypes torch float torch float dtypes torch long torch float torch int torch float dtypes __init__ args device_type= all len args isinstance args list tuple arg args assert isinstance arg list tuple When one dtype variant tuple list all dtype variants must f Received non-list non-tuple dtype str arg assert all isinstance dtype torch dtype dtype arg f Unknown dtype str arg assert all isinstance arg torch dtype arg args f Unknown dtype str args args = args device_type = device_type __call__ fn d = getattr fn dtypes assert device_type d f dtypes redefinition device_type d device_type = args fn dtypes = d fn Overrides specified dtypes CPU dtypesIfCPU dtypes __init__ args super __init__ args device_type= cpu Overrides specified dtypes CUDA dtypesIfCUDA dtypes __init__ args super __init__ args device_type= cuda Overrides specified dtypes Intel GPU dtypesIfXPU dtypes __init__ args super __init__ args device_type= xpu dtypesIfMPS dtypes __init__ args super __init__ args device_type= mps dtypesIfHPU dtypes __init__ args super __init__ args device_type= hpu dtypesIfPRIVATEUSE dtypes __init__ args super __init__ args device_type=torch _C _get_privateuse _backend_name onlyCPU fn onlyOn cpu fn onlyCUDA fn onlyOn cuda fn onlyMPS fn onlyOn mps fn onlyXPU fn onlyOn xpu fn onlyHPU fn onlyOn hpu fn onlyPRIVATEUSE fn device_type = torch _C _get_privateuse _backend_name device_mod = getattr torch device_type None device_mod None reason = f Skip torch has no module device_type unittest skip reason fn onlyOn device_type fn onlyCUDAAndPRIVATEUSE fn wraps fn only_fn args kwargs device_type cuda torch _C _get_privateuse _backend_name reason = f onlyCUDAAndPRIVATEUSE doesn t run device_type raise unittest SkipTest reason fn args kwargs only_fn disablecuDNN fn wraps fn disable_cudnn args kwargs device_type == cuda has_cudnn torch backends cudnn flags enabled=False fn args kwargs fn args kwargs disable_cudnn disableMkldnn fn wraps fn disable_mkldnn args kwargs torch backends mkldnn is_available torch backends mkldnn flags enabled=False fn args kwargs fn args kwargs disable_mkldnn expectedFailureCPU fn expectedFailure cpu fn expectedFailureCUDA fn expectedFailure cuda fn expectedFailureXPU fn expectedFailure xpu fn expectedFailureMeta fn skipIfTorchDynamo expectedFailure meta fn expectedFailureXLA fn expectedFailure xla fn expectedFailureHPU fn expectedFailure hpu fn expectedFailureMPS fn expectedFailure mps fn expectedFailureMPSComplex fn expectedFailure mps torch complex fn expectedFailureMPSPre fn platform version = float join platform mac_ver split - version version cpu other unsupported device fn version expectedFailure mps fn fn expectedFailureMPSPre fn platform version = float join platform mac_ver split - version version cpu other unsupported device fn version expectedFailure mps fn fn Skips test CPU LAPACK available skipCPUIfNoLapack fn skipCPUIf torch _C has_lapack PyTorch compiled without Lapack fn Skips test CPU FFT available skipCPUIfNoFFT fn skipCPUIf torch _C has_spectral PyTorch built without FFT support fn Skips test CPU MKL available skipCPUIfNoMkl fn skipCPUIf TEST_MKL PyTorch built without MKL support fn Skips test CPU MKL Sparse available s linked Windows skipCPUIfNoMklSparse fn skipCPUIf IS_WINDOWS TEST_MKL PyTorch built without MKL support fn Skips test CPU mkldnn available skipCPUIfNoMkldnn fn skipCPUIf torch backends mkldnn is_available PyTorch built without mkldnn support fn Skips test CUDA MAGMA available skipCUDAIfNoMagma fn skipCUDAIf no_magma no MAGMA library detected skipCUDANonDefaultStreamIf True fn has_cusolver TEST_WITH_ROCM has_hipsolver rocm_version = _get_torch_rocm_version hipSOLVER disabled ROCM rocm_version = Skips test CUDA ROCM cuSOLVER hipSOLVER available skipCUDAIfNoCusolver fn skipCUDAIf has_cusolver has_hipsolver cuSOLVER available fn Skips test both cuSOLVER MAGMA available skipCUDAIfNoMagmaAndNoCusolver fn has_cusolver fn cuSolver disabled cuda tests depend MAGMA skipCUDAIfNoMagma fn Skips test both cuSOLVER hipSOLVER MAGMA available skipCUDAIfNoMagmaAndNoLinalgsolver fn has_cusolver has_hipsolver fn cuSolver disabled cuda tests depend MAGMA skipCUDAIfNoMagma fn Skips test CUDA when using ROCm skipCUDAIfRocm func=None msg= test doesn t currently work ROCm stack dec_fn fn reason = f skipCUDAIfRocm msg skipCUDAIf TEST_WITH_ROCM reason=reason fn func dec_fn func dec_fn Skips test CUDA when using ROCm skipCUDAIfNotRocm fn skipCUDAIf TEST_WITH_ROCM test doesn t currently work CUDA stack fn Skips test CUDA ROCm unavailable its version lower than requested skipCUDAIfRocmVersionLessThan version=None dec_fn fn wraps fn wrap_fn args kwargs device_type == cuda TEST_WITH_ROCM reason = ROCm available raise unittest SkipTest reason rocm_version_tuple = _get_torch_rocm_version rocm_version_tuple None version None rocm_version_tuple tuple version reason = f ROCm rocm_version_tuple available version required raise unittest SkipTest reason fn args kwargs wrap_fn dec_fn Skips test CUDA when using ROCm skipCUDAIfNotMiopenSuggestNHWC fn skipCUDAIf TEST_WITH_MIOPEN_SUGGEST_NHWC test doesn t currently work without MIOpen NHWC activation fn Skips test specified CUDA versions given form list major minor s skipCUDAVersionIn versions Optional list tuple int int = None dec_fn fn wraps fn wrap_fn args kwargs version = _get_torch_cuda_version version == cpu rocm fn args kwargs version versions reason = f test skipped CUDA version version raise unittest SkipTest reason fn args kwargs wrap_fn dec_fn Skips test CUDA versions less than specified given form major minor skipCUDAIfVersionLessThan versions Optional tuple int int = None dec_fn fn wraps fn wrap_fn args kwargs version = _get_torch_cuda_version version == cpu rocm fn args kwargs version versions reason = f test skipped CUDA versions version raise unittest SkipTest reason fn args kwargs wrap_fn dec_fn Skips test CUDA cuDNN unavailable its version lower than requested skipCUDAIfCudnnVersionLessThan version= dec_fn fn wraps fn wrap_fn args kwargs device_type == cuda no_cudnn reason = cuDNN available raise unittest SkipTest reason cudnn_version None cudnn_version version reason = f cuDNN version cudnn_version available version required raise unittest SkipTest reason fn args kwargs wrap_fn dec_fn Skips test CUDA cuSparse generic API available skipCUDAIfNoCusparseGeneric fn skipCUDAIf TEST_CUSPARSE_GENERIC cuSparse Generic API available fn skipCUDAIfNoHipsparseGeneric fn skipCUDAIf TEST_HIPSPARSE_GENERIC hipSparse Generic API available fn skipCUDAIfNoSparseGeneric fn skipCUDAIf TEST_CUSPARSE_GENERIC TEST_HIPSPARSE_GENERIC Sparse Generic API available fn skipCUDAIfNoCudnn fn skipCUDAIfCudnnVersionLessThan fn skipCUDAIfMiopen fn skipCUDAIf torch version hip None Marked skipped MIOpen fn skipCUDAIfNoMiopen fn skipCUDAIf torch version hip None MIOpen available skipCUDAIfNoCudnn fn skipLazy fn skipLazyIf True test doesn t work lazy tensors fn skipMeta fn skipMetaIf True test doesn t work meta tensors fn skipXLA fn skipXLAIf True Marked skipped XLA fn skipMPS fn skipMPSIf True test doesn t work MPS backend fn skipHPU fn skipHPUIf True test doesn t work HPU backend fn skipXPU fn skipXPUIf True test doesn t work XPU backend fn skipPRIVATEUSE fn skipPRIVATEUSE If True test doesn t work privateuse backend fn TODO all name isn t true anymore quite some time we have also have example XLA MPS now This should probably enumerate all available device type test base classes get_all_device_types - list str cpu torch cuda is_available cpu cuda skip since currently flex attention requires least ` avx ` support CPU IS_FLEX_ATTENTION_CPU_PLATFORM_SUPPORTED = torch xpu is_available torch cuda is_available IS_MACOS torch cpu _is_avx _supported os getenv ATEN_CPU_CAPABILITY = default IS_FLEX_ATTENTION_XPU_PLATFORM_SUPPORTED = torch xpu is_available torch utils _triton has_triton flex_attention_supported_platform = unittest skipUnless IS_FLEX_ATTENTION_XPU_PLATFORM_SUPPORTED IS_FLEX_ATTENTION_CPU_PLATFORM_SUPPORTED torch cuda is_available torch utils _triton has_triton torch cuda get_device_capability = Requires CUDA Triton Intel GPU triton CPU avx later torch version hip gfx torch cuda get_device_properties gcnArchName e m _type = torch float _e m fnuz e m _type = torch float _e m fnuz E M _MAX_POS = torch finfo torch float _e m fnuz max E M _MAX_POS = torch finfo torch float _e m fnuz max e m _type = torch float _e m fn e m _type = torch float _e m E M _MAX_POS = torch finfo torch float _e m fn max E M _MAX_POS = torch finfo torch float _e m max