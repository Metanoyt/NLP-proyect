contextlib itertools logging types abc ABC abstractmethod collections abc Callable Generator Mapping Sequence dataclasses dataclass enum auto Enum functools partial typing Any cast Optional Protocol TypeAlias torch torch distributed dist torch distributed _functional_collectives ft_c torch distributed distributed_c d c d torch nn nn torch nn functional F torch distributed device_mesh DeviceMesh torch distributed tensor distribute_tensor DTensor Shard torch distributed tensor parallel ParallelStyle torch nn attention flex_attention _mask_mod_signature BlockMask create_block_mask torch utils _pytree tree_flatten tree_unflatten _cp_custom_ops flex_cp_allgather _load_balancer _create_default_load_balancer _LoadBalancer __all__ = _CausalBehavior _context_parallel_shard _ContextParallel _cp_options _disable_context_parallel_dispatcher _enable_context_parallel_dispatcher _is_causal_behavior _RotateMethod context_parallel context_parallel_unshard set_rotate_method _CausalBehavior Enum SKIP = None NOT_IS_CAUSAL = False IS_CAUSAL = True _RotateMethod Enum ALL_TO_ALL = auto ALL_GATHER = auto aten = torch ops aten logger = logging getLogger __name__ _DispatchMode Enum MONKEY_PATCH = auto MODULE_WRAPPER = auto _dispatch_mode _DispatchMode = _DispatchMode MONKEY_PATCH dataclass _ContextParallelOptions Whether upcast parameters gradients float avoid accumulation errors It likely always True we currently keep variable experimental purposes convert_to_f bool = True enable_load_balance bool = True rotate_method _RotateMethod = _RotateMethod ALL_GATHER _cp_options = _ContextParallelOptions _is_causal_behavior rank int world_size int i int is_causal bool - _CausalBehavior Calculate is_causal behavior each KV block The attention can either calculated full all causal mask applied is_causal _CausalBehavior NOT_IS_CAUSAL i == _CausalBehavior IS_CAUSAL source_rank = rank - i world_size source_rank rank _cp_options enable_load_balance _CausalBehavior NOT_IS_CAUSAL _CausalBehavior SKIP _maybe_wait tensor torch Tensor - torch Tensor When tracing code result tensor AsyncCollectiveTensor so we cannot call ` ` wait ` ` isinstance tensor ft_c AsyncCollectiveTensor tensor wait tensor _partial_update original torch Tensor new torch Tensor dim int n_chunks int idx int add bool - torch Tensor This API partially updates chunk ` ` original ` ` tensor The ` ` original ` ` tensor will first chunked along ` ` dim ` ` dimension then ` ` idx ` ` chunk will updated ` ` new ` ` If ` ` add ` ` True chunk will added ` ` new ` ` otherwise chunk will replaced ` ` new ` ` The result tensor same size ` ` original ` ` chunks = list original chunk n_chunks dim=dim assert chunks idx shape == new shape original shape new shape idx add chunks idx += new chunks idx = new torch cat chunks dim=dim _SDPAMerger A help merge local SDPA result __init__ convert_to_f bool seq_dim int _seq_dim = seq_dim _out Optional torch Tensor = None _lse Optional torch Tensor = None _should_lse_squeeze = False _convert_to_f = convert_to_f _out_dtype = torch float _lse_dtype = torch float _merge_one block_out torch Tensor block_lse torch Tensor partial bool - None The cuDNN backend preserves last dimension LSE Apply unsqueeze only input does already have required dimensionality len block_lse shape len block_out shape block_lse = block_lse unsqueeze dim=- _should_lse_squeeze = True assert len block_lse shape == len block_out shape _lse None _lse = block_lse _out = block_out ROUND_ROBIN_CYCLE = assert _lse None assert _out None lse = _lse chunk ROUND_ROBIN_CYCLE dim=self _seq_dim partial _lse out = _out chunk ROUND_ROBIN_CYCLE dim=self _seq_dim partial _out The algorithm github com zhuzilin ring-flash-attention pull #issuecomment- gives relatively stable result out = out - F sigmoid block_lse - lse out - block_out lse = lse - F logsigmoid lse - block_lse partial _lse = _partial_update _lse lse dim=self _seq_dim n_chunks=ROUND_ROBIN_CYCLE idx= add=False _out = _partial_update _out out dim=self _seq_dim n_chunks=ROUND_ROBIN_CYCLE idx= add=False _lse = lse _out = out step out torch Tensor lse torch Tensor partial bool - None _out_dtype = out dtype _lse_dtype = lse dtype _convert_to_f out = out torch float lse = lse torch float _merge_one out lse partial results - tuple torch Tensor torch Tensor assert _out None assert _lse None out = _out _out_dtype _should_lse_squeeze lse = _lse squeeze - _lse_dtype lse = _lse _lse_dtype out lse _AttentionOp Protocol __call__ query torch Tensor key torch Tensor value torch Tensor kwargs object - tuple torch Tensor _RingRotater ABC abstractmethod __init__ pg dist ProcessGroup seq_dim int - None abstractmethod exchange_buffers curr_buffer torch Tensor - None abstractmethod next_buffer - torch Tensor _AllToAllRotater _RingRotater Use all_to_all send kv next rank __init__ pg dist ProcessGroup seq_dim int - None _pg = pg _seq_dim = seq_dim _buffer Optional torch Tensor = None exchange_buffers curr_buffer torch Tensor - None curr_buffer = curr_buffer contiguous size = dist get_world_size _pg dsts = list range size + _buffer = ft_c permute_tensor curr_buffer dsts _pg next_buffer - torch Tensor assert _buffer None _maybe_wait _buffer _AllGatherRotater _RingRotater Allgather kv only required kv Only one communication will done __init__ pg dist ProcessGroup seq_dim int - None _pg = pg _seq_dim = seq_dim _aggregated_buffer Optional torch Tensor = None _idx = exchange_buffers curr_buffer torch Tensor - None We only need perform allgather once _idx += _aggregated_buffer None _aggregated_buffer = ft_c all_gather_tensor curr_buffer contiguous gather_dim= group=self _pg next_buffer - torch Tensor rank = dist get_rank _pg idx = rank - _idx assert _aggregated_buffer None _aggregated_buffer = _maybe_wait _aggregated_buffer _aggregated_buffer chunk dist get_world_size _pg idx _create_rotater pg dist ProcessGroup seq_dim int method Optional _RotateMethod = None - _RingRotater method None method = _cp_options rotate_method method == _RotateMethod ALL_TO_ALL _AllToAllRotater pg seq_dim method == _RotateMethod ALL_GATHER _AllGatherRotater pg seq_dim raise NotImplementedError f Unknown method method _templated_ring_attention group dist ProcessGroup seq_dim int op _AttentionOp query torch Tensor key torch Tensor value torch Tensor is_causal bool = False kwargs object - tuple torch Tensor A generalized ring attention implementation can support multiple attention ops Note Context parallelism load balance algorithm causal masking ===================== This explanation uses example illustrate CP algorithm causal masking Consider scenario where sequence length q k v e g q = q q q q there two ranks For simplicity we will discuss only q k v follows same pattern k The diagram below represents complete QK^T operation without parallelism The ` ` entries indicate result required due causal masking e g q k marked ` ` + ---- + ------------------------ + &#124; &#124; k k k k &#124; + ---- + ------------------------ + &#124; q &#124; q k &#124; &#124; q &#124; q k q k &#124; &#124; q &#124; q k q k q k &#124; &#124; q &#124; q k q k q k q k &#124; + ---- + ------------------------ + ### No Load Balance In scenario each rank owns local chunk q k v each chunk containing two elements Rank responsible managing q q k k while rank manages q q k k First Iteration Both rank rank perform SDPA their local qkv pairs Causal masking enabled some results required e g q k Second Iteration Local queries remain same local kv pairs exchanged Rank now has q q k k rank has q q k k Rank performs no computation while rank computes locally without causal masking since all results q k q k q k q k needed ### Round-robin Load Balance In setup each rank owns two local chunks q k v each chunk containing one element Rank manages q q k k Rank manages q q k k Although local chunks consecutive they concatenated enable SDPA performed single call each step Consequently chunk function may required prepare correct q k v configurations First Iteration Both ranks perform SDPA their local qkv pairs similar no-load-balance case This iteration corresponds ` ` ` ` ` ` ` implementation Second Iteration Rank now has q q k k rank has q q k k For rank no computation needed q However computations q k q k required so only q used SDPA This corresponds ` ` ` ` ` ` ` ` implementation For rank k needed q q so only k used SDPA This corresponds ` ` ` ` ` ` ` ` implementation Parameters ---------- op The attention op use args additional args passed op kwargs additional kwargs passed op Returns ------- out The merged attention output softmax_lse The logsumexp merged attention output is_causal query size = key size raise NotImplementedError is_causal requires same query context sequence lengths is_causal _cp_options enable_load_balance raise RuntimeError Load balancing requires ` is_causal=True ` assert isinstance group dist ProcessGroup process group must single dimension rank = dist get_rank group size = dist get_world_size group next_kv = None Without making key value contiguous loss curve bad TODO fegin figure out why requirement since SDPA does have requirement key = key contiguous value = value contiguous sdpa_merger = _SDPAMerger _cp_options convert_to_f seq_dim=seq_dim rest list Any out torch Tensor logsumexp torch Tensor rotater = _create_rotater group i range size i Wait kv cp_rank - rank next_kv = rotater next_buffer key = next_kv key numel reshape key shape value = next_kv key numel reshape value shape i size - Send k v next rank next_kv = torch cat key flatten value flatten next_kv = rotater exchange_buffers next_kv is_causal_behavior = _is_causal_behavior rank=rank world_size=size i=i is_causal=is_causal For detailed understanding load balancing algorithm see Note Context parallelism load balance algorithm causal masking is_causal_behavior == _CausalBehavior SKIP If i rank load balancing turned continue i == _cp_options enable_load_balance is_causal When local balance enabled we still need do SDPA both local chunks q k v first iteration q k v partial = query key value False i = rank Round-robin load balancing case i = rank We need do SDPA only first local chunk k v Note q k v each contains two local chunks ROUND_ROBIN_CYCLE = q k v partial = query key chunk ROUND_ROBIN_CYCLE dim= value chunk ROUND_ROBIN_CYCLE dim= False Round-robin load balancing case i rank We need do SDPA only second half q update only second part logsumexp So partial True Note q k v each contains two chunks q k v partial = query chunk dim= key value True See https github com pytorch pytorch blob release aten src ATen native native_functions yaml#L SDPA kernel definitions out logsumexp rest = op q k v is_causal=is_causal_behavior value kwargs sdpa_merger step out logsumexp partial pyrefly ignore unbound-name sdpa_merger results rest _templated_ring_attention_backward group dist ProcessGroup seq_dim int op _AttentionOp grad_out torch Tensor grad_out_name str query torch Tensor key torch Tensor value torch Tensor out torch Tensor logsumexp torch Tensor is_causal bool kwargs Any - tuple torch Tensor This API implements backward pass ring attention is_causal _cp_options enable_load_balance raise RuntimeError Load balancing requires ` is_causal=True ` rank = dist get_rank group size = dist get_world_size group next_kv = None next_grad_kv = None rest list Any grad_query_ grad_key_ grad_value_ = None None None accum_dtype = torch float _cp_options convert_to_f query dtype grad_query = torch zeros_like query dtype=accum_dtype grad_key = torch zeros_like key dtype=accum_dtype grad_value = torch zeros_like value dtype=accum_dtype key = key contiguous value = value contiguous kv_rotater = _create_rotater group dkv_rotater = _create_rotater group method=_RotateMethod ALL_TO_ALL i range size i Wait kv cp_rank - rank buffer = kv_rotater next_buffer pointer = key = buffer pointer pointer + key numel reshape key shape pointer += key numel value = buffer pointer pointer + value numel reshape value shape pointer += value numel i = size - Send kv next rank next_kv = torch cat key flatten value flatten kv_rotater exchange_buffers next_kv is_causal_behavior = _is_causal_behavior rank=rank world_size=size i=i is_causal=is_causal is_causal_behavior = _CausalBehavior SKIP i == _cp_options enable_load_balance is_causal We need do SDPA full local q k v q k v out_ dout lse = query key value out grad_out logsumexp i = rank Round-robin load balancing case i = rank We need do SDPA only first half k v Note q k v each contains two chunks q k v out_ dout lse = query key chunk dim=seq_dim value chunk dim=seq_dim out grad_out logsumexp Round-robin load balancing case i rank We need do SDPA only second half q Note q k v each contains two chunks q k v out_ dout lse = query chunk dim=seq_dim key value out chunk dim=seq_dim grad_out chunk dim=seq_dim Need make logsumexp contiguous otherwise there will numerical error logsumexp chunk dim=seq_dim contiguous kwargs grad_out_name = dout See https github com pytorch pytorch blob release aten src ATen native native_functions yaml#L SDPA kernel definitions grad_query_ grad_key_ grad_value_ rest = op query=q key=k value=v out=out_ logsumexp=lse is_causal=is_causal_behavior value kwargs grad_query_ = torch zeros_like query dtype=accum_dtype grad_key_ = torch zeros_like key dtype=accum_dtype grad_value_ = torch zeros_like value dtype=accum_dtype ROUND_ROBIN_CYCLE = i == grad_key += grad_key_ grad_value += grad_value_ pointer = Wait kv gradient cp_rank - rank next_grad_kv = dkv_rotater next_buffer grad_key = next_grad_kv pointer pointer + grad_key numel reshape grad_key shape pointer += grad_key numel grad_value = next_grad_kv pointer pointer + grad_value numel reshape grad_value shape i = rank _cp_options enable_load_balance grad_key = _partial_update grad_key grad_key_ dim=seq_dim n_chunks=ROUND_ROBIN_CYCLE idx= add=True grad_value = _partial_update grad_value grad_value_ dim=seq_dim n_chunks=ROUND_ROBIN_CYCLE idx= add=True grad_key += grad_key_ grad_value += grad_value_ next_grad_kv = torch cat grad_key flatten grad_value flatten Send grad key grad value next rank dkv_rotater exchange_buffers next_grad_kv i = rank _cp_options enable_load_balance grad_query += grad_query_ grad_query = _partial_update grad_query grad_query_ dim=seq_dim n_chunks=ROUND_ROBIN_CYCLE idx= add=True assert grad_key_ None assert grad_value_ None grad_query = grad_query query dtype next_grad_kv = dkv_rotater next_buffer key dtype grad_key = next_grad_kv grad_key numel reshape grad_key shape grad_value = next_grad_kv grad_key numel reshape grad_value shape grad_query grad_key grad_value pyrefly ignore unbound-name rest _scaled_dot_product_ring_flash_attention mesh DeviceMesh query torch Tensor key torch Tensor value torch Tensor dropout_p float = is_causal bool = False return_debug_mask bool = False scale Optional float = None - tuple torch Tensor return_debug_mask raise NotImplementedError return_debug_mask supported yet TODO remove hardcoding seq_dim = group = mesh get_group _templated_ring_attention group seq_dim aten _scaled_dot_product_flash_attention query=query key=key value=value is_causal=is_causal dropout_p=dropout_p scale=scale _scaled_dot_product_ring_efficient_attention mesh DeviceMesh query torch Tensor key torch Tensor value torch Tensor attn_bias Optional torch Tensor = None compute_log_sumexp bool = True dropout_p float = is_causal bool = False scale Optional float = None - tuple torch Tensor attn_bias None raise NotImplementedError attn_bias supported yet compute_log_sumexp CP requires compute_log_sumexp True because always merges LSE compute_log_sumexp = True TODO remove hardcoding seq_dim = group = mesh get_group _templated_ring_attention group seq_dim aten _scaled_dot_product_efficient_attention query=query key=key value=value is_causal=is_causal attn_bias=attn_bias dropout_p=dropout_p scale=scale compute_log_sumexp=compute_log_sumexp _scaled_dot_product_ring_cudnn_attention mesh DeviceMesh query torch Tensor key torch Tensor value torch Tensor attn_bias Optional torch Tensor = None compute_log_sumexp bool = True dropout_p float = is_causal bool = False return_debug_mask bool = False scale Optional float = None - tuple torch Tensor attn_bias None raise NotImplementedError attn_bias supported yet compute_log_sumexp CP requires compute_log_sumexp True because always merges LSE compute_log_sumexp = True TODO remove hardcoding seq_dim = group = mesh get_group _templated_ring_attention group seq_dim aten _scaled_dot_product_cudnn_attention query=query key=key value=value attn_bias=attn_bias compute_log_sumexp=compute_log_sumexp dropout_p=dropout_p is_causal=is_causal return_debug_mask=return_debug_mask scale=scale _scaled_dot_product_ring_flash_attention_backward mesh DeviceMesh grad_out torch Tensor query torch Tensor key torch Tensor value torch Tensor out torch Tensor logsumexp torch Tensor cum_seq_q torch Tensor cum_seq_k torch Tensor max_q int max_k int dropout_p float is_causal bool philox_seed torch Tensor philox_offset torch Tensor scale Optional float = None - tuple torch Tensor TODO remove hardcoding seq_dim = group = mesh get_group _templated_ring_attention_backward group seq_dim aten _scaled_dot_product_flash_attention_backward default grad_out=grad_out grad_out_name= grad_out query=query key=key value=value out=out logsumexp=logsumexp is_causal=is_causal cum_seq_q=cum_seq_q cum_seq_k=cum_seq_k max_q=max_q max_k=max_k dropout_p=dropout_p philox_seed=philox_seed philox_offset=philox_offset scale=scale _scaled_dot_product_ring_efficient_attention_backward mesh DeviceMesh grad_out torch Tensor query torch Tensor key torch Tensor value torch Tensor bias torch Tensor out torch Tensor logsumexp torch Tensor philox_seed torch Tensor philox_offset torch Tensor dropout_p float grad_input_mask tuple bool is_causal bool = False scale Optional float = None - tuple torch Tensor TODO remove hardcoding seq_dim = group = mesh get_group _templated_ring_attention_backward group seq_dim aten _scaled_dot_product_efficient_attention_backward default grad_out=grad_out grad_out_name= grad_out_ query=query key=key value=value attn_bias=bias out=out logsumexp=logsumexp philox_seed=philox_seed philox_offset=philox_offset dropout_p=dropout_p grad_input_mask=grad_input_mask is_causal=is_causal scale=scale _scaled_dot_product_ring_cudnn_attention_backward mesh DeviceMesh grad_out torch Tensor query torch Tensor key torch Tensor value torch Tensor out torch Tensor logsumexp torch Tensor philox_seed torch Tensor philox_offset torch Tensor attn_bias torch Tensor cum_seq_q torch Tensor cum_seq_k torch Tensor max_q int max_k int dropout_p float is_causal bool scale Optional float = None - tuple torch Tensor TODO remove hardcoding seq_dim = group = mesh get_group _templated_ring_attention_backward group seq_dim aten _scaled_dot_product_cudnn_attention_backward default grad_out=grad_out grad_out_name= grad_out query=query key=key value=value out=out logsumexp=logsumexp philox_seed=philox_seed philox_offset=philox_offset attn_bias=attn_bias cum_seq_q=cum_seq_q cum_seq_k=cum_seq_k max_q=max_q max_k=max_k dropout_p=dropout_p is_causal=is_causal scale=scale _sdpa_handler op_call torch _ops OpOverload args tuple object kwargs dict str object - object extract local tensor sharding infos OpInfo op_info = DTensor _op_dispatcher unwrap_to_op_info op_call args kwargs logger debug Dispatching op_call s op_info schema sharding propagation TODO remove context parallel strategy default propagation rule Either figure out how dynamically enable just don t call propagate DTensor _op_dispatcher sharding_propagator propagate op_info output_sharding = op_info output_sharding assert output_sharding None output sharding should None assert output_sharding needs_redistribute inputs need redistributed call_maps dict torch _ops OpOverload Callable = aten _scaled_dot_product_flash_attention default _scaled_dot_product_ring_flash_attention aten _scaled_dot_product_efficient_attention default _scaled_dot_product_ring_efficient_attention aten _scaled_dot_product_cudnn_attention default _scaled_dot_product_ring_cudnn_attention aten _scaled_dot_product_flash_attention_backward default _scaled_dot_product_ring_flash_attention_backward aten _scaled_dot_product_efficient_attention_backward default _scaled_dot_product_ring_efficient_attention_backward aten _scaled_dot_product_cudnn_attention_backward default _scaled_dot_product_ring_cudnn_attention_backward op_call call_maps local_results = call_maps op_call op_info compute_mesh op_info local_args type ignore arg-type op_info local_kwargs type ignore arg-type raise NotImplementedError CP only supports flash attention memory efficient attention now DTensor _op_dispatcher wrap local_results output_sharding output_spec custom_ops = aten _scaled_dot_product_flash_attention default _sdpa_handler aten _scaled_dot_product_flash_attention_backward default _sdpa_handler aten _scaled_dot_product_efficient_attention default _sdpa_handler aten _scaled_dot_product_efficient_attention_backward default _sdpa_handler aten _scaled_dot_product_cudnn_attention default _sdpa_handler aten _scaled_dot_product_cudnn_attention_backward default _sdpa_handler exitsing_custom_ops = DTensor _op_dispatcher _custom_op_handlers ArgsType = tuple Any KwargsType = dict str Any InputFnType = Callable Optional nn Module ArgsType KwargsType DeviceMesh Any OutputFnType = Callable Optional nn Module Any Any DeviceMesh Any _replaced_functions dict Callable tuple str Callable = _distribute_function fn Callable fn_module types ModuleType device_mesh DeviceMesh input_fn InputFnType output_fn OutputFnType - None A helper function replace function distributed version using monkey patching approach This function CP internal usage only wrapper target_fn Callable input_fn InputFnType output_fn OutputFnType - Callable inner_fn args ArgsType kwargs KwargsType - Any args kwargs = input_fn None args kwargs device_mesh outputs = target_fn args kwargs output_fn None args kwargs outputs device_mesh inner_fn global _replaced_functions fn _replaced_functions wrapper_fn = wrapper fn input_fn output_fn setattr fn_module fn __name__ wrapper_fn _replaced_functions wrapper_fn = fn __name__ fn _restore_function fn Callable fn_module types ModuleType - None Restore function replaced _distribute_function fn _replaced_functions original_name original_fn = _replaced_functions fn setattr fn_module original_name original_fn _enable_cp_dtensor_dispatcher - None Enables DTensor dispatcher dispatch SDPA CP DTensor _op_dispatcher _custom_op_handlers = exitsing_custom_ops custom_ops _disable_cp_dtensor_dispatcher - None Disables DTensor dispatcher dispatch SDPA CP DTensor _op_dispatcher _custom_op_handlers = exitsing_custom_ops _enable_context_parallel_dispatcher_impl seq_dim int mesh DeviceMesh - None sdpa_cp = _ContextParallel seq_dim=seq_dim attention_type=_ContextParallel AttentionType SDPA _dispatch_mode == _DispatchMode MONKEY_PATCH _distribute_function F scaled_dot_product_attention F mesh sdpa_cp sdpa_input_fn sdpa_cp sdpa_output_fn _enable_cp_dtensor_dispatcher _dispatch_mode == _DispatchMode MODULE_WRAPPER _enable_cp_dtensor_dispatcher raise ValueError f Unknown dispatch mode _dispatch_mode _disable_context_parallel_dispatcher_impl - None _dispatch_mode == _DispatchMode MONKEY_PATCH _restore_function F scaled_dot_product_attention F _dispatch_mode == _DispatchMode MODULE_WRAPPER pass raise NotImplementedError f Unknown dispatch mode _dispatch_mode _disable_cp_dtensor_dispatcher _compiled_create_block_mask = torch compile create_block_mask dynamic=False fullgraph=True _context_parallel_buffers mesh DeviceMesh buffers list torch Tensor &#124; BlockMask buffer_seq_dims list int load_balancer Optional _LoadBalancer = None - list torch Tensor &#124; BlockMask Shard buffers along sequence dimensions according CP rules Args mesh ` DeviceMesh ` device mesh context parallelism buffers List torch Tensor buffers sharded seq_dims List int sequence dimensions ` ` buffers ` ` This list must have same length ` ` buffers ` ` load_balancer Optional ` _LoadBalancer ` optional ` _LoadBalancer ` object If argument ` None ` means ` buffers ` need no rearrangement before being sharded If argument ` _LoadBalancer ` object call its ` _generate_indices restore=False ` generate rearrangement indices such each shard ` buffer rearrange_idx ` well-balanced i e having close sparsities Returns List torch Tensor sharded buffers Note For ` _context_parallel_shard ` we require non-None ` load_balancer ` object explicitly passed load-balancing needed generate index tensor rearranging buffer load-balance available load_balance_indices = load_balancer _generate_indices load_balancer None assert load_balance_indices None load_balance_indices ndim == load balance index expects shape seq_len B seq_len f got load_balance_indices shape new_buffers = sharded_buffer torch Tensor &#124; BlockMask buffer seq_dim zip buffers buffer_seq_dims isinstance buffer torch Tensor TODO load balance doesn t perform error handling NOTE assuming batch dim load_balance_indices None TODO we should expclitly ask users unsqueeze batch dim But BC breaking ask However what we have done today also very safe idx_batch_size = load_balance_indices size data_batch_size = buffer size seq_dim idx_batch_size = idx_batch_size = data_batch_size raise ValueError Cannot rearrange buffer f load_balance_indices has shape load_balance_indices shape f buffer has shape buffer shape seq_dim == buffer = torch index_select buffer dim= index=load_balance_indices indices = load_balance_indices idx_batch_size == size = data_batch_size + list indices size indices = indices expand size i range data_batch_size buffer i = torch index_select buffer i dim=seq_dim - index=indices i use DTensor shard buffer sequence dimension retain local tensor sharded_buffer = distribute_tensor buffer mesh Shard seq_dim src_data_rank=None to_local isinstance buffer BlockMask sharded_buffer = _create_cp_block_mask mask_mod=buffer mask_mod B=buffer kv_num_blocks shape H=buffer kv_num_blocks shape Q_LEN=buffer seq_lengths KV_LEN=buffer seq_lengths device_mesh=mesh load_balancer=load_balancer raise ValueError f Unknown buffer type type buffer new_buffers append sharded_buffer new_buffers _create_cp_block_mask mask_mod _mask_mod_signature B int H int Q_LEN int KV_LEN int device_mesh DeviceMesh load_balancer Optional _LoadBalancer = None - BlockMask Creates specialized BlockMask Context Parallel FlexAttention This function creates BlockMask enables computation attention results sharded Q attending global KV The mask appropriately handles query index offset required when each rank operates shard query sequence while accessing full key-value sequence The function internally rewrites provided mask_mod function translate local query indices global query indices ensuring masking logic applied correctly across distributed computation Args mask_mod Callable Mask function operates global attention indices B int Batch size H int Number query heads Q_LEN int Global sequence length query KV_LEN int Global sequence length key value device_mesh DeviceMesh Device mesh used context parallelism load_balancer Optional ` _LoadBalancer ` The load-balancer used rearrange QKV before sharding This will used modify block_mask generated Returns BlockMask A block mask configured local query shard can used flex_attention given cp_mesh Raises NotImplementedError If Q_LEN divisible CP world size BLOCK_SIZE Warning Currently requires Q_LEN divisible CP mesh world size BLOCK_SIZE BLOCK_SIZE defaults This constraint exists because BlockMask must handle both padding offsets correctly For example Q_LEN CP world size BLOCK_SIZE local Q_LEN would In such cases both rank rank would have paddings their local BlockMasks Support padding scenario planned future work torch nn attention flex_attention _DEFAULT_SPARSE_BLOCK_SIZE Q_LEN device_mesh size _DEFAULT_SPARSE_BLOCK_SIZE = raise NotImplementedError f Q_LEN Q_LEN divisible CP mesh world size device_mesh size f BLOCK_SIZE _DEFAULT_SPARSE_BLOCK_SIZE This supported yet compiled_create_block_mask = torch compile create_block_mask dynamic=False fullgraph=True _rewrite_mask_mod mask_mod _mask_mod_signature rank int block_size int local_q_size int qkv_rearrange_indices Optional torch Tensor = None - _mask_mod_signature assert qkv_rearrange_indices None qkv_rearrange_indices ndim == load balance index expects shape seq_len B seq_len f got qkv_rearrange_indices shape qkv_idx_restore b torch Tensor idx_post_rearrange torch Tensor - torch Tensor qkv_rearrange_indices None qkv_rearrange_indices size == identical load-balance batch idx_pre_rearrange = qkv_rearrange_indices idx_post_rearrange idx_pre_rearrange = qkv_rearrange_indices b idx_post_rearrange idx_pre_rearrange = idx_post_rearrange idx_pre_rearrange local_q_idx_to_q_idx local_q_idx torch Tensor - torch Tensor calculate local block_idx block_offset local_blk_idx local_blk_offset = local_q_idx block_size local_q_idx block_size NOTE load balancing used local_num_blocks = local_q_size block_size blk_idx = local_num_blocks rank + local_blk_idx blk_idx block_size + local_blk_offset lambda b h q_idx kv_idx mask_mod b h qkv_idx_restore b local_q_idx_to_q_idx q_idx qkv_idx_restore b kv_idx cp_rank = device_mesh get_local_rank cp_group_size = device_mesh size load_balancer = load_balancer _create_default_load_balancer Q_LEN cp_group_size device_mesh device_type Q_SHARD_LEN = Q_LEN cp_group_size block_size = _DEFAULT_SPARSE_BLOCK_SIZE rearrange_indices = load_balancer _generate_indices restore=False load_balancer None block_mask = compiled_create_block_mask _rewrite_mask_mod mask_mod cp_rank block_size Q_SHARD_LEN qkv_rearrange_indices=rearrange_indices B H Q_SHARD_LEN KV_LEN device=device_mesh device_type BLOCK_SIZE= block_size block_size block_mask ##################### Experimental APIs ##################### _ContextParallel ParallelStyle AttentionType Enum FLEX = flex_attention SDPA = scaled_dot_product_attention __init__ seq_dim int attention_type AttentionType - None super __init__ seq_dim = seq_dim attention_type = attention_type _apply module nn Module mesh DeviceMesh - nn Module attention_type == AttentionType FLEX module register_forward_pre_hook partial flex_input_fn mesh=mesh with_kwargs=True module attention_type == AttentionType SDPA module register_forward_pre_hook partial sdpa_input_fn mesh=mesh with_kwargs=True module register_forward_hook partial sdpa_output_fn mesh=mesh module raise ValueError f Unknown attention type attention_type flex_input_fn module Optional nn Module args Any kwargs Any mesh DeviceMesh - Any args_list = list args idx name enumerate query key value score_mod block_mask idx = len args args_list append kwargs pop name None query key value score_mod block_mask = args_list assert isinstance query torch Tensor assert isinstance key torch Tensor assert isinstance value torch Tensor assert isinstance block_mask BlockMask &#124; tuple key = key contiguous value = value contiguous global_key global_value = flex_cp_allgather key value seq_dim c d _get_process_group_name mesh get_group args_list = global_key args_list = global_value tuple args_list kwargs sdpa_input_fn module Optional nn Module args tuple Any kwargs dict str Any mesh DeviceMesh - tuple tuple Any dict str Any placement = Shard seq_dim all_args = arg itertools chain args kwargs values isinstance arg torch Tensor isinstance arg DTensor assert arg _spec placements == placement arg = DTensor from_local arg mesh placement run_check=False all_args append arg new_args = tuple all_args len args new_kwargs = dict zip kwargs keys all_args len args new_args new_kwargs sdpa_output_fn module Optional nn Module inputs Any outputs Any mesh DeviceMesh - Any new_outputs = output outputs isinstance outputs torch Tensor outputs output = output to_local isinstance output DTensor output new_outputs append output isinstance outputs torch Tensor new_outputs tuple new_outputs CPBuffer TypeAlias = torch Tensor &#124; BlockMask CPBufferContainer TypeAlias = Sequence CPBuffer &#124; Mapping str CPBuffer CPBufferSeqDims TypeAlias = Sequence int &#124; Mapping str int _context_parallel_shard mesh DeviceMesh buffers CPBufferContainer seq_dims CPBufferSeqDims load_balancer Optional _LoadBalancer = None - list torch Tensor &#124; BlockMask Shard buffers along specified sequence dimensions ` seq_dims ` so each rank retains only its corresponding shard according provided ` mesh ` If ` load_balancer ` provided buffers will rearranged load balancer before sharding improve load balance Buffers can either tensors ` BlockMask ` objects If buffer ` BlockMask ` its sharding dimension determined ` BlockMask ` implementation corresponding ` seq_dim ` ignored Note For ` _context_parallel_shard ` non-None ` load_balancer ` must explicitly passed load balancing required Args mesh DeviceMesh The device mesh used context parallelism buffers List torch Tensor &#124; BlockMask Buffers whose usage depends sequence dimension Examples include input batches labels positional embedding buffers These buffers must sharded along sequence dimension ensure correctness seq_dims List int The sequence dimensions each buffer ` buffers ` Must have same length ` buffers ` load_balancer Optional _LoadBalancer An optional load balancer object If provided rearranges buffers before sharding achieve better load balance If provided no rearrangement performed Returns List torch Tensor &#124; BlockMask The sharded buffers each corresponding local shard current rank TODO these global variables going bite us someday We will have remove them soon For new API we only support module wrapper mode global _dispatch_mode _dispatch_mode = _DispatchMode MODULE_WRAPPER global _cp_options load_balancer None _cp_options enable_load_balance = True _cp_options enable_load_balance = False len buffers = len seq_dims raise ValueError ` seq_dims ` must have same number elements ` buffers ` flat_buffers spec = tree_flatten buffers flat_seq_dims _ = tree_flatten seq_dims len flat_buffers = len flat_seq_dims raise ValueError ` seq_dims ` must have pytree structure ` buffers ` isinstance flat_buffers torch Tensor device = flat_buffers device device = flat_buffers kv_num_blocks device buffer flat_buffers isinstance buffer torch Tensor assert device == buffer device All buffers must same device assert device == buffer kv_num_blocks device All buffers must same device flat_sharded_buffers = _context_parallel_buffers mesh flat_buffers flat_seq_dims load_balancer tree_unflatten flat_sharded_buffers spec _enable_context_parallel_dispatcher - None Enable context parallel dispatcher This API experimental subject change _enable_cp_dtensor_dispatcher _disable_context_parallel_dispatcher - None Disable context parallel dispatcher This API experimental subject change _disable_cp_dtensor_dispatcher ##################################################### Current public APIs also subject change ##################################################### contextlib contextmanager torch no_grad context_parallel mesh DeviceMesh buffers Optional list torch Tensor = None buffer_seq_dims Optional list int = None no_restore_buffers Optional set torch Tensor = None - Generator None None None ` ` context_parallel ` ` experimental API enable context parallelism CP This API performs two actions patch SDPA ` ` torch nn functional scaled_dot_product_attention ` ` CP-enabled one shard ` ` buffers ` ` along sequence dimension each rank will preserve corresponding shard according ` ` mesh ` ` Args mesh ` DeviceMesh ` device mesh context parallelism buffers Optional List torch Tensor buffers usage depend sequence dimension Examples input batch labels positional embedding buffers These buffers must sharded along sequence dimension ensure accuracy The sharding will happen in-place buffer s shape will change within context The buffers will restored after context finishes ` ` no_restore_buffers ` ` can used specify which buffers don t need restored Note ` ` buffers ` ` should contain any nn Parameter buffer_seq_dims Optional List int sequence dimensions ` ` buffers ` ` no_restore_buffers Optional Set torch Tensor buffers these set won t restored after context exits This set must subset ` ` buffers ` ` If buffers won t used after context exits these buffers can put list avoid extra restore time warning ` torch distributed tensor experimental context_parallel ` prototype feature PyTorch The API subject change For legacy API we only support monkey-patch mode We will deprecate API once new API widely used global _dispatch_mode _dispatch_mode = _DispatchMode MONKEY_PATCH buffers = buffers None buffers buffer_seq_dims = buffer_seq_dims None buffer_seq_dims no_restore_buffers = set no_restore_buffers None no_restore_buffers len buffers = len buffer_seq_dims raise ValueError ` seq_dims ` must have same number elements ` buffers ` buffer no_restore_buffers Cannot use ` buffer buffers ` which will incur tensor comparison any b buffer b buffers raise ValueError ` no_restore_buffers ` must subset ` buffers ` original_buffers = None b no_restore_buffers b clone b buffers device = buffers device seq_length = buffers shape buffer_seq_dims cp_world_size = mesh size If ` enable_load_balance ` True default Head-tail load balancer ` _HeadTailLoadBalancer ` used rearrange buffers before sharding Otherwise we don t do any load-balance rearrange passing ` None ` ` _context_parallel_shard ` load_balancer = _create_default_load_balancer seq_length cp_world_size device shards = _context_parallel_buffers mesh cast list torch Tensor &#124; BlockMask buffers buffer_seq_dims load_balancer buffer shard zip buffers shards assert isinstance shard torch Tensor ContextParallel only supports Tensor shard = shard clone buffer resize_ shard shape buffer copy_ shard _enable_context_parallel_dispatcher_impl seq_dim= mesh=mesh yield _disable_context_parallel_dispatcher_impl buffer original_buffer zip buffers original_buffers original_buffer None buffer resize_ original_buffer shape buffer copy_ original_buffer torch no_grad context_parallel_unshard mesh DeviceMesh buffers list torch Tensor seq_dims list int load_balancer Optional _LoadBalancer = None - list torch Tensor Unshard tensors e g output sharded due context parallelism Args mesh ` DeviceMesh ` device mesh context parallelism buffers List torch Tensor buffers unsharded seq_dims List int sequence dimensions ` ` buffers ` ` This list must have same length ` ` buffers ` ` load_balancer Optional ` _Loadbalancer ` optional ` _LoadBalancer ` object If argument ` None ` means ` buffers ` rearranged when being sharded there s no need put back order after unsharding If argument ` _LoadBalancer ` object call its ` _generate_indices restore=True ` generate restore indices such ` unsharded restore_idx ` original buffer Returns List torch Tensor unsharded buffers Note For ` context_parallel_unshard ` we require not-None ` load_balancer ` object explicitly passed flex_attention used load-balancing needed This different case SDPA though we strongly suggest users follow same convention device = buffers device cp_world_size = mesh size seq_length = buffers shape seq_dims cp_world_size If users don t pass ` load_balancer ` - ` enable_load_balance ` True we use default round-robin load balancer - ` enable_load_balance ` False we don t do any load balancing passing ` None ` ` restore_indices ` load_balancer = load_balancer _create_default_load_balancer seq_length cp_world_size device restore_indices = load_balancer _generate_indices restore=True load_balancer None assert restore_indices None restore_indices ndim == load balance restore index expects shape seq_len B seq_len f got restore_indices shape unsharded_buffers = b dim zip buffers seq_dims b = b contiguous unsharded_b = _maybe_wait ft_c all_gather_tensor b dim mesh restore_indices None NOTE assuming batch dim idx_batch_size = restore_indices size data_batch_size = unsharded_b size idx_batch_size = idx_batch_size = data_batch_size raise ValueError Cannot restore buffer f restore_indices has shape restore_indices shape f unsharded_b has shape unsharded_b shape i range data_batch_size index = restore_indices identical load-balance batch idx_batch_size == restore_indices i unsharded_b_batch_i = torch index_select unsharded_b i dim=dim - index=index unsharded_b i = unsharded_b_batch_i unsharded_buffers append unsharded_b unsharded_buffers set_rotate_method rotate_method str - None Context Parallel SDPA requires rotation kv shards Users can call API specify which rotation method use alltoall shuffles kv shards using all-to-all collective While allgather gathers kv shards using all-gather collective after first sub-SDPA computation If API has been called default rotate method allgather Args rotate_method str rotate method use Currently only supports allgather alltoall If different string other than these two passed function will raise error Returns None logger info Note FlexAttention CP doesn t support alltoall yet rotate_method == allgather _cp_options rotate_method = _RotateMethod ALL_GATHER rotate_method == alltoall _cp_options rotate_method = _RotateMethod ALL_TO_ALL raise NotImplementedError Context Parallel does support f using rotate_method kv shards rotation