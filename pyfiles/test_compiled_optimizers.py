Owner s module inductor random sys types unittest weakref contextlib ExitStack copy deepcopy typing NamedTuple expecttest assert_expected_inline torch torch _inductor torch _inductor cudagraph_trees torch optim lr_scheduler torch _higher_order_ops foreach_map torch _inductor config torch _inductor test_case TestCase torch optim Adadelta Adagrad Adam Adamax AdamW ASGD NAdam RAdam RMSprop Rprop SGD SparseAdam torch optim lr_scheduler ChainedScheduler ConstantLR CosineAnnealingLR CosineAnnealingWarmRestarts CyclicLR ExponentialLR LambdaLR LinearLR MultiplicativeLR MultiStepLR OneCycleLR PolynomialLR ReduceLROnPlateau StepLR torch testing _internal common_device_type instantiate_device_type_tests skipCUDAIf skipXPUIf torch testing _internal common_optimizers _get_optim_inputs_including_global_cliquey_kwargs optim_db optims torch testing _internal common_utils parametrize skipIfWindows torch testing _internal inductor_utils GPU_TYPE HAS_CPU HAS_GPU has_triton torch testing _internal triton_utils requires_cuda_and_triton requires_gpu get_inputs optim steps = params = grads = exp_avgs = exp_avg_sqs = group optim param_groups p group params params append p grads append p grad state = optim state p exp_avgs append state exp_avg exp_avg_sqs append state exp_avg_sq steps append state step steps params exp_avgs exp_avg_sqs update_exp_avg_sq exp_avg_sq grad beta exp_avg_sq mul beta addcmul grad grad value= - beta update_param param step exp_avg exp_avg_sq beta beta lr eps bias_correction = - torch pow beta step bias_correction = - torch pow beta step sqrt step_size = lr bias_correction neg denom = exp_avg_sq sqrt bias_correction step_size add eps step_size torch add param torch div exp_avg denom foreach_map_adam steps params exp_avgs exp_avg_sqs weight_decay= beta = beta = lr= e- eps= e- torch no_grad grads = param grad param params update step updated_steps = foreach_map lambda x x + steps torch _foreach_copy_ steps updated_steps weight_decay = foreach_map torch add grads alpha=weight_decay HOPS cannot have multiple outputs moment need call foreach_map once each output exp_avgs_updated = foreach_map torch lerp exp_avgs grads - beta exp_avgs_sq_updated = foreach_map update_exp_avg_sq exp_avg_sqs grads beta params_updated = foreach_map update_param params steps exp_avgs_updated exp_avgs_sq_updated beta beta lr eps No input mutation HOPS torch _foreach_copy_ exp_avgs exp_avgs_updated torch _foreach_copy_ exp_avg_sqs exp_avgs_sq_updated torch _foreach_copy_ params params_updated Note we use atypical values amplify error LR_SCHEDULER_TO_KWARGS = LambdaLR lr_lambda lambda x MultiplicativeLR lr_lambda lambda x StepLR step_size gamma MultiStepLR milestones gamma ExponentialLR gamma CosineAnnealingLR T_max These schedulers have memory leaks eager https github com pytorch pytorch issues SequentialLR schedulers None milestones ChainedScheduler schedulers None CyclicLR base_lr max_lr cycle_momentum False CosineAnnealingWarmRestarts T_ OneCycleLR max_lr cycle_momentum False steps_per_epoch epochs ConstantLR factor LinearLR ReduceLROnPlateau factor patience PolynomialLR create_scheduler scheduler optim kwargs = LR_SCHEDULER_TO_KWARGS scheduler schedulers kwargs kwargs schedulers = create_scheduler torch optim lr_scheduler ConstantLR optim _ range + create_scheduler torch optim lr_scheduler LambdaLR optim scheduler == ChainedScheduler scheduler kwargs scheduler optim kwargs KernelCounts NamedTuple multitensor int singletensor int With different settings certain tests you can get different kernel counts This maps test name expected kernel count fmt off expecttest got error after PYFMT add line break triple quotes KERNEL_COUNT_OVERRIDES = test_rmsprop_foreach_weight_decay_cpu lambda x assert_expected_inline x test_nadam_foreach_weight_decay_momentum_decay_cpu lambda x assert_expected_inline x test_adamw_amsgrad_capturable_foreach_cuda lambda x assert_expected_inline x test_adamw_amsgrad_capturable_foreach_xpu lambda x assert_expected_inline x test_adamw_amsgrad_capturable_cuda lambda x assert_expected_inline x test_adamw_amsgrad_capturable_xpu lambda x assert_expected_inline x test_adamw_tensor_lr_tensor_betas_amsgrad_capturable_cuda lambda x assert_expected_inline x test_adamw_tensor_lr_tensor_betas_amsgrad_capturable_xpu lambda x assert_expected_inline x test_adamw_tensor_lr_tensor_betas_capturable_cuda lambda x assert_expected_inline x test_adamw_tensor_lr_tensor_betas_capturable_xpu lambda x assert_expected_inline x test_adamw_tensor_lr_amsgrad_capturable_cuda lambda x assert_expected_inline x test_adamw_tensor_lr_amsgrad_capturable_xpu lambda x assert_expected_inline x test_adam_tensor_lr_amsgrad_capturable_cuda lambda x assert_expected_inline x test_adam_tensor_lr_amsgrad_capturable_xpu lambda x assert_expected_inline x test_adam_tensor_lr_tensor_betas_amsgrad_capturable_cuda lambda x assert_expected_inline x test_adam_tensor_lr_tensor_betas_amsgrad_capturable_xpu lambda x assert_expected_inline x test_adam_tensor_lr_tensor_betas_capturable_cuda lambda x assert_expected_inline x test_adam_tensor_lr_tensor_betas_capturable_xpu lambda x assert_expected_inline x test_adam_amsgrad_capturable_cuda lambda x assert_expected_inline x test_adam_amsgrad_capturable_xpu lambda x assert_expected_inline x test_adadelta_tensor_lr_capturable_cuda lambda x assert_expected_inline x test_adadelta_tensor_lr_capturable_xpu lambda x assert_expected_inline x test_rmsprop_tensor_lr_capturable_cuda lambda x assert_expected_inline x test_rmsprop_tensor_lr_capturable_xpu lambda x assert_expected_inline x test_adadelta_foreach_weight_decay_maximize_cpu lambda x assert_expected_inline x test_adadelta_foreach_rho_weight_decay_cpu lambda x assert_expected_inline x test_adadelta_foreach_weight_decay_cpu lambda x assert_expected_inline x test_sgd_foreach_momentum_weight_decay_cpu lambda x assert_expected_inline x test_sgd_foreach_momentum_nesterov_weight_decay_cpu lambda x assert_expected_inline x test_sgd_momentum_dampening_foreach_cuda lambda x assert_expected_inline x test_sgd_momentum_dampening_foreach_xpu lambda x assert_expected_inline x test_sgd_momentum_foreach_cuda lambda x assert_expected_inline x test_sgd_momentum_foreach_xpu lambda x assert_expected_inline x test_sgd_weight_decay_maximize_cuda lambda x assert_expected_inline x test_sgd_weight_decay_maximize_xpu lambda x assert_expected_inline x test_sgd_weight_decay_maximize_cpu lambda x assert_expected_inline x test_sgd_weight_decay_cpu lambda x assert_expected_inline x test_sgd_weight_decay_cuda lambda x assert_expected_inline x test_sgd_weight_decay_xpu lambda x assert_expected_inline x test_sgd_momentum_weight_decay_foreach_cuda lambda x assert_expected_inline x test_sgd_momentum_weight_decay_foreach_xpu lambda x assert_expected_inline x test_sgd_momentum_nesterov_weight_decay_foreach_cuda lambda x assert_expected_inline x test_sgd_momentum_nesterov_weight_decay_foreach_xpu lambda x assert_expected_inline x test_sgd_cuda lambda x assert_expected_inline x test_sgd_cpu lambda x assert_expected_inline x test_sgd_xpu lambda x assert_expected_inline x test_adagrad_initial_accumulator_value_weight_decay_foreach_xpu lambda x assert_expected_inline x test_adagrad_lr_decay_weight_decay_foreach_xpu lambda x assert_expected_inline x test_adagrad_weight_decay_foreach_xpu lambda x assert_expected_inline x test_adagrad_weight_decay_maximize_foreach_xpu lambda x assert_expected_inline x test_adagrad_tensor_lr_cpu lambda x assert_expected_inline x test_adagrad_tensor_lr_cuda lambda x assert_expected_inline x test_adagrad_tensor_lr_xpu lambda x assert_expected_inline x test_adamax_tensor_lr_weight_decay_capturable_cuda lambda x assert_expected_inline x test_adamax_tensor_lr_weight_decay_capturable_xpu lambda x assert_expected_inline x test_asgd_tensor_lr_weight_decay_maximize_capturable_cuda lambda x assert_expected_inline x test_asgd_tensor_lr_weight_decay_maximize_capturable_xpu lambda x assert_expected_inline x test_nadam_tensor_lr_weight_decay_momentum_decay_decoupled_weight_decay_capturable_cuda lambda x assert_expected_inline x noqa B test_nadam_tensor_lr_weight_decay_momentum_decay_decoupled_weight_decay_capturable_xpu lambda x assert_expected_inline x noqa B test_radam_tensor_lr_capturable_weight_decay_decoupled_weight_decay_cuda lambda x assert_expected_inline x test_radam_tensor_lr_capturable_weight_decay_decoupled_weight_decay_xpu lambda x assert_expected_inline x test_sgd_tensor_lr_cpu lambda x assert_expected_inline x test_sgd_tensor_lr_cuda lambda x assert_expected_inline x test_sgd_tensor_lr_xpu lambda x assert_expected_inline x fmt also tracks currently supported optimizers KERNEL_COUNTS = Adam KernelCounts multitensor= singletensor= AdamW KernelCounts multitensor= singletensor= NAdam KernelCounts multitensor= singletensor= Rprop KernelCounts multitensor= singletensor= RMSprop KernelCounts multitensor= singletensor= Adadelta KernelCounts multitensor= singletensor= Adagrad KernelCounts multitensor= singletensor= SGD KernelCounts multitensor= singletensor= ASGD KernelCounts multitensor= singletensor= RAdam KernelCounts multitensor= singletensor= Adamax KernelCounts multitensor= singletensor= build_opt_kwarg_db compiled_opt_db = optim_info optim_db optim_info optim_cls KERNEL_COUNTS continue device cpu GPU_TYPE optim_inputs _get_optim_inputs_including_global_cliquey_kwargs device None optim_info skip= differentiable fused kwargs = dict optim_inputs kwargs name = f test_ optim_info optim_cls __name__ lower has_tensor_lr = False key val kwargs items key = lr key = betas isinstance val bool isinstance val bool val name += _ + key key == lr isinstance kwargs lr torch Tensor has_tensor_lr = True name += _tensor_lr key == betas isinstance kwargs betas torch Tensor name += _tensor_betas name += f _ device kwargs device = device name KERNEL_COUNT_OVERRIDES kwargs kernel_count = KERNEL_COUNT_OVERRIDES name kwargs kernel_count = KERNEL_COUNTS optim_info optim_cls multitensor kwargs get foreach False device == GPU_TYPE KERNEL_COUNTS optim_info optim_cls singletensor kwargs kernel_count None kwargs get fused False continue has_tensor_lr scheduler_cls LR_SCHEDULER_TO_KWARGS keys name_w_scheduler = name + f _ scheduler_cls __name__ lower compiled_opt_db append optim_info optim_cls name_w_scheduler kwargs scheduler_cls compiled_opt_db append optim_info optim_cls name kwargs None compiled_opt_db COMPILED_OPT_KWARG_DB = build_opt_kwarg_db aten = torch ops aten try try test_torchinductor check_model check_model_gpu except ImportError test_torchinductor manual=fbcode caffe test inductor test_inductor-library check_model check_model_gpu except unittest SkipTest ImportError e sys stderr write f type e e \n __name__ == __main__ sys exit raise call_scheduler scheduler isinstance scheduler torch optim lr_scheduler ReduceLROnPlateau scheduler step we won t reduce metric over two iters anyway scheduler step compile_opt opt_compiled closure=None fullgraph=True run patcher so step has expected structure torch _dynamo eval_frame TorchPatcher patch unwrap step TWICE avoid deliberate graph break due limitation functionalization no_grad detection see Note graph break optimizer py This ignores outer _use_grad_if_differentiable wrapper instead manually disables grad before calling step which fine now dynamo does support differentiable optimizers anyway step_fn = opt_compiled step __wrapped__ __wrapped__ This ensures we don t receive spam warnings LR Scheduler opt_compiled _opt_called = True closure None fn step_fn opt_compiled closure fn step_fn opt_compiled torch compile fn backend= inductor fullgraph=fullgraph check_optim optim_cls params_eager params_compiled state_eager state_compiled atol=None rtol=None params_eager = list params_eager params_compiled = list params_compiled Note tolerances test_correctness_Adadelta_cuda_float Mismatched elements Greatest absolute difference e- index up e- allowed Greatest relative difference index up e- allowed This due floating point ordering error + usage sqrt rtol = None atol = None optim_cls Adadelta rtol = e- atol = e- inductor test_compiled_optimizers py CompiledOptimizerTests test_nadam_tensor_lr_weight_decay_momentum_decay_decoupled_weight_decay_capturable_foreach_cuda_lambdalr Mismatched elements Greatest absolute difference e- index up e- allowed Greatest relative difference e- index up e- allowed optim_cls NAdam atol = e- rtol = e- assertEqual list params_eager list params_compiled atol=atol rtol=rtol p_eager p_compiled zip params_eager params_compiled assertEqual state_eager p_eager state_compiled p_compiled atol=atol rtol=rtol make_test optim_cls closure=None scheduler_cls=None kernel_count= device=GPU_TYPE kwargs config patch score_fusion_memory_threshold test_fn stack = ExitStack try https github com pytorch pytorch issues capturable Adagrad support https github com pytorch pytorch issues capturable SGD support run_cudagraphs = device == cuda optim_cls Adagrad SGD run_cudagraphs stack enter_context config patch triton cudagraphs True kwargs_compiled = deepcopy kwargs isinstance kwargs get lr torch Tensor kwargs lr = kwargs lr device kwargs_compiled lr = kwargs_compiled lr device betas kwargs isinstance kwargs betas torch Tensor kwargs betas = kwargs betas device kwargs betas device kwargs_compiled betas = kwargs_compiled betas device kwargs_compiled betas device torch _dynamo reset torch _inductor metrics reset input = torch ones device=device model_eager = torch nn Sequential torch nn Linear device=device _ range model_eager input sum backward input = torch ones device=device model_compiled = deepcopy model_eager model_compiled input sum backward opt_eager = optim_cls model_eager parameters kwargs opt_compiled = optim_cls model_compiled parameters kwargs_compiled compiled_step = compile_opt opt_compiled closure=closure scheduler_cls scheduler_compiled = create_scheduler scheduler_cls opt_compiled scheduler_eager = create_scheduler scheduler_cls opt_eager some schedulers only change after least epoch has passed scheduler_compiled last_epoch = scheduler_eager last_epoch = torch set_grad_enabled False _ range compiled_step opt_eager step scheduler_cls call_scheduler scheduler_eager call_scheduler scheduler_compiled check_optim optim_cls model_eager parameters model_compiled parameters opt_eager state opt_compiled state run_cudagraphs check_cudagraphs_ran check_kernel_count currently we compile step rest computation separately because step single element tensor hence usual kernel count isinstance kernel_count types LambdaType kernel_count str torch _inductor metrics generated_kernel_count assertEqual torch _inductor metrics generated_kernel_count kernel_count finally stack close device == GPU_TYPE test_fn = requires_gpu test_fn test_fn make_recompile_test optim_cls closure=None kernel_count= kwargs config patch score_fusion_memory_threshold requires_gpu test_fn torch _dynamo reset torch _inductor metrics reset input = torch ones device=GPU_TYPE model = torch nn Sequential torch nn Linear device=GPU_TYPE _ range model input sum backward opt_compiled = optim_cls model parameters kwargs compiled_step = compile_opt opt_compiled check no recompile here torch set_grad_enabled False _ range compiled_step perturb state force recompile Adagrad doesn t reinitialize state each step SGD has empty state optim_cls Adagrad SGD opt_compiled param_groups lr = optim_cls Adam ensure we guarding data_ptr states state_tensor = opt_compiled state opt_compiled param_groups params exp_avg opt_compiled state opt_compiled param_groups params exp_avg = torch zeros_like state_tensor opt_compiled state clear compiled_step check_kernel_count currently we compile step rest computation separately because step single element tensor hence usual kernel count multiply account recompile multiplier = assertEqual torch _inductor metrics generated_kernel_count multiplier kernel_count test_fn CompiledOptimizerParityTests TestCase skipCUDAIf has_triton torch compile cuda requires triton skipXPUIf has_triton torch compile xpu requires triton optims optim_db dtypes= torch float parametrize use_closure True False test_correctness device dtype optim_info use_closure torch cuda manual_seed_all torch manual_seed random seed optim_cls = optim_info optim_cls all_optim_inputs = _get_optim_inputs_including_global_cliquey_kwargs device dtype optim_info skip= differentiable optim_info step_requires_closure use_closure optim_input all_optim_inputs kwargs = optim_input kwargs use_scheduler = isinstance kwargs get lr None torch Tensor scheduler_classes = list LR_SCHEDULER_TO_KWARGS keys use_scheduler None scheduler_cls scheduler_classes torch _dynamo reset torch _inductor metrics reset input = torch ones device=device model_eager = torch nn Sequential torch nn Linear device=device bias=False _ range model_eager input sum backward model_compiled = deepcopy model_eager model_compiled input sum backward optim_cls SparseAdam param model_eager parameters param grad = param grad to_sparse param model_compiled parameters param grad = param grad to_sparse opt_compiled = optim_cls model_compiled parameters deepcopy kwargs opt_eager = optim_cls model_eager parameters deepcopy kwargs scheduler_cls scheduler_compiled = create_scheduler scheduler_cls opt_compiled scheduler_eager = create_scheduler scheduler_cls opt_eager some schedulers only change after least epoch has passed scheduler_compiled last_epoch = scheduler_eager last_epoch = num_steps = use_closure torch compile fn closure loss = model_compiled input sum loss backward optim_info only_supports_sparse_grads param model_compiled parameters param grad = param grad to_sparse loss opt_compiled step closure scheduler_cls call_scheduler scheduler_compiled closure_eager loss = model_eager input sum loss backward optim_info only_supports_sparse_grads param model_eager parameters param grad = param grad to_sparse loss _ range num_steps opt_eager step closure_eager scheduler_cls call_scheduler scheduler_eager torch compile fn opt_compiled step scheduler_cls call_scheduler scheduler_compiled _ range num_steps opt_eager step scheduler_cls call_scheduler scheduler_eager _ range num_steps fn check_optim optim_cls model_eager parameters model_compiled parameters opt_eager state opt_compiled state CompiledOptimizerTests TestCase check_model_gpu = check_model_gpu check_model_cpu = check_model check_kernel_count = True setUp super setUp torch _dynamo reset torch _inductor metrics reset tearDown super tearDown torch _dynamo reset torch _inductor metrics reset check_cudagraphs_ran We run zeroth device currently manager = torch _inductor cudagraph_trees get_container tree_manager assertIsNotNone manager assertEqual manager new_graph_id id test_adam_recompile = make_recompile_test Adam lr= test_adamw_recompile = make_recompile_test AdamW lr= test_adamax_recompile = make_recompile_test Adamax lr= test_nadam_recompile = make_recompile_test NAdam lr= test_rprop_recompile = make_recompile_test Rprop lr= kernel_count= test_rmsprop_recompile = make_recompile_test RMSprop lr= test_adadelta_recompile = make_recompile_test Adadelta lr= test_adagrad_recompile = make_recompile_test Adagrad lr= test_asgd_recompile_default = make_recompile_test ASGD lr= test_asgd_recompile_single = make_recompile_test ASGD kernel_count= lr= foreach=False test_asgd_recompile_foreach = make_recompile_test ASGD lr= foreach=True test_sgd_recompile_single = make_recompile_test SGD kernel_count= lr= foreach=False test_sgd_recompile_foreach = make_recompile_test SGD kernel_count= lr= foreach=True skipIfWindows requires_gpu test_static_address_finalizer gc gc disable p_ref = None fn nonlocal p_ref mod = torch nn Linear device=GPU_TYPE bias=False p mod parameters p grad = torch rand_like p opt = torch optim Adam mod parameters lr= fn opt step torch set_grad_enabled False step_fn_compiled = torch compile fn step_fn_compiled p_ref = weakref ref p assertTrue p_ref None fn assertTrue p_ref None gc enable test_guard_on_none_grads training_loop input = torch tensor reshape model = torch nn Sequential torch nn Linear torch nn Sigmoid torch nn Linear torch nn Sigmoid params = list model parameters optimizer = torch optim Adam params step_list = i range optimizer zero_grad Test step behaves expected no-op when grads set None i = output = model input loss = output sum loss backward optimizer step step_list append optimizer state params step step_list compiled_training_loop = torch compile training_loop backend= eager actual_steps = compiled_training_loop expected_steps = training_loop assertEqual actual_steps expected_steps Basic shampoo test verify we support compiling various ops without error requires_gpu test_basic_shampoo param_buf = torch rand param_buf_c = param_buf detach clone params_c = param_buf_c t param_buf_c t params = param_buf t param_buf t p p_c zip params params_c p grad = torch rand_like p p_c grad = p grad detach clone note skips root inverse because has lot internal dependencies we also don t compile regardless torch no_grad shampoo_functional_basic params step = weight_decay = grads = p grad p params beta = beta = epsilon = e- preconditioners = torch zeros_like p p params lr = pt region weight decay torch _foreach_add_ grads params alpha=weight_decay update preconditioners torch _foreach_addcmul_ preconditioners grads grads value= torch _foreach_mul_ grads beta torch _foreach_add_ grads grads alpha= - beta bias_correction = - beta step grad_list = torch _foreach_div grads bias_correction pt region precondition shampoo branch no grafting bias_correction = - beta step bias_corrected_preconditioner_list = torch _foreach_div preconditioners bias_correction torch _foreach_sqrt_ bias_corrected_preconditioner_list torch _foreach_add_ bias_corrected_preconditioner_list epsilon search_directions = torch _foreach_div grad_list bias_corrected_preconditioner_list torch _foreach_add_ search_directions params alpha=weight_decay torch _foreach_mul_ search_directions -lr pt region update params torch _foreach_add_ params search_directions params preconditioners grads compiled_fn = torch compile shampoo_functional_basic assertEqual compiled_fn params_c shampoo_functional_basic params requires_gpu test_closure_graph_break param = torch rand dtype=torch float device=GPU_TYPE requires_grad=True param_c = param detach clone requires_grad_ True closure param grad = torch ones_like param param grad closure_c param_c grad = torch ones_like param_c param_c grad optimizer = torch optim AdamW param optimizer_c = torch optim AdamW param_c loop opt c opt step c compiled_loop = torch compile loop backend= eager compiled_loop optimizer closure loop optimizer_c closure_c assertEqual param param_c test_get_value_on_static_address torch _dynamo decorators mark_static_address torch optim optimizer _get_value compiled = torch compile _get_value x = torch ones mark_static_address x guard=True ret_val = compiled x assertEqual ret_val x compile large foreach op verify time taken within expected range requires_gpu test_compile_time_smoketest time xs = torch ones device=GPU_TYPE _ range ys = torch ones device=GPU_TYPE _ range torch compile fn xs ys torch _foreach_add xs ys start = time perf_counter fn xs ys end = time perf_counter assertLess end - start requires_cuda_and_triton test_S Just verify we can compile function without error try s _repro except ImportError s _repro manual forward = s _repro forward torch _dynamo torch _inductor torch _dynamo debug_utils aot_graph_input_parser torch _inductor utils fresh_cache fresh_cache kwargs = aot_graph_input_parser forward torch compile forward kwargs requires_cuda_and_triton test_foreach_map_adam params = torch rand dtype=torch float device=GPU_TYPE requires_grad=True _ range param params param grad = torch rand_like param params_ref = p detach clone requires_grad_ True p params param param_ref zip params params_ref param_ref grad = param grad detach clone optimizer = torch optim Adam params capturable=True foreach=True optimizer_ref = torch optim Adam params_ref capturable=True foreach=True warm up optimizer state optimizer step optimizer_ref step inps = get_inputs optimizer torch compile foreach_map_adam_step foreach_map_adam inps loop foreach_map_adam_step optimizer_ref step loop assertEqual torch _inductor metrics generated_kernel_count param param_ref zip params params_ref assertEqual param param_ref optim_cls name kwargs scheduler_cls COMPILED_OPT_KWARG_DB setattr CompiledOptimizerTests name make_test optim_cls scheduler_cls=scheduler_cls kwargs instantiate_device_type_tests CompiledOptimizerParityTests globals allow_xpu=True except_for= cpu __name__ == __main__ torch _inductor test_case run_tests HAS_CPU HAS_GPU run_tests needs= filelock