Owner s module pt -dispatcher torch functorch compile min_cut_rematerialization_partition torch _C FileCheck torch _inductor custom_graph_pass CustomPartitionerFn get_hash_for_files torch _inductor test_case TestCase torch _inductor utils run_fw_bw_and_get_code torch testing _internal inductor_utils GPU_TYPE HAS_GPU MyCustomPartitionerFn CustomPartitionerFn A custom partitioner function static_lifetime_input_indices overwrites __init__ super __init__ called = False __call__ gm joint_inputs kwargs called = True kwargs static_lifetime_input_indices = min_cut_rematerialization_partition gm joint_inputs kwargs uuid get_hash_for_files __file__ TestCustomPartitionerFn TestCase test_custom_partitioner_fn For function f b partitioner compile_fx stack addition ` a+b ` equivalently ` buf ` saved backward With custom partitioner function we indicate ` ` ` b ` equivalently ` primals_ ` ` primals_ ` do take additional memory thus they saved backward initialization torch compile f b + b cos cos = torch randn requires_grad=True device=GPU_TYPE b = torch randn requires_grad=True device=GPU_TYPE CASE -- default addition ` + b ` i e ` buf ` saved backward code_og = run_fw_bw_and_get_code lambda f b fwd_code_og = code_og FileCheck check buf buf run fwd_code_og CASE -- custom partitioner function ` ` ` b ` i e ` primals_ ` ` primals_ ` saved backward custom_partitioner_fn = MyCustomPartitionerFn assertFalse custom_partitioner_fn called assertIsNotNone custom_partitioner_fn uuid torch _inductor config patch custom_partitioner_fn=custom_partitioner_fn code_cp = run_fw_bw_and_get_code lambda f b fwd_code_cp = code_cp FileCheck check buf primals_ primals_ run fwd_code_cp make sure custom partitioner function indeed invoked assertTrue custom_partitioner_fn called __name__ == __main__ torch _inductor test_case run_tests HAS_GPU run_tests