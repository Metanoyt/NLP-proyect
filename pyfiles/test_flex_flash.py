Owner s module inductor unittest contextlib contextmanager torch torch _inductor kernel flex flex_flash_attention ensure_flash_available torch _inductor test_case TestCase InductorTestCase torch nn attention flex_attention create_block_mask flex_attention torch profiler profile ProfilerActivity torch testing _internal common_device_type dtypes instantiate_device_type_tests torch testing _internal common_utils parametrize _times_two score _b _h _m _n score _causal score _b _h token_q token_kv torch where token_q = token_kv score float -inf _rel_bias score _b _h token_q token_kv score + token_q - token_kv create_alibi_learned num_heads= dtype=torch float ALiBi learned per-head slopes tests tensor loading slopes = torch exp -torch linspace num_heads device= cuda dtype=dtype alibi_score_mod score b h q_idx kv_idx bias = kv_idx - q_idx slopes h score + bias alibi_score_mod create_pos_bias_table seq_len= dtype=torch float Relative position bias table tests computed indexing max_len = seq_len table = torch randn max_len - device= cuda dtype=dtype pos_bias_mod score b h q_idx kv_idx rel_pos = kv_idx - q_idx + max_len - bias = table rel_pos score + bias pos_bias_mod create_head_scale num_heads= dtype=torch float Per-head scaling factors tests multiplication tensor loading scales = torch rand num_heads device= cuda dtype=dtype + head_scale_mod score b h q_idx kv_idx score scales h head_scale_mod create_batch_bias batch_size= dtype=torch float Per-batch bias tests batch indexing bias = torch randn batch_size device= cuda dtype=dtype batch_bias_mod score b h q_idx kv_idx score + bias b batch_bias_mod create_batch_head_bias batch_size= num_heads= dtype=torch float Per-batch-head bias matrix tests D indexing batch + head bias_matrix = torch randn batch_size num_heads device= cuda dtype=dtype batch_head_mod score b h q_idx kv_idx bias = bias_matrix b h score + bias batch_head_mod create_dual_buffer_bias num_heads= seq_len= dtype=torch float Dual buffer loading tests loading separate tensors head_bias = torch randn num_heads device= cuda dtype=dtype pos_scale = torch arange seq_len device= cuda dtype=dtype dual_buffer_mod score b h q_idx kv_idx head_component = head_bias h pos_component = pos_scale q_idx score + head_component + pos_component dual_buffer_mod create_test_tensors batch_size= num_heads= seq_len= dim= dtype=torch float device= cuda shape = batch_size num_heads seq_len dim q = torch randn shape device=device dtype=dtype requires_grad=False k = torch randn shape device=device dtype=dtype requires_grad=False v = torch randn shape device=device dtype=dtype requires_grad=False q k v contextmanager cuda_kernel_profiler kernel_pattern= flash_attncute Context manager profiling CUDA kernels result = found False kernel_names profile activities= ProfilerActivity CUDA prof yield result kernel_names = evt name evt prof events evt device_type == torch autograd DeviceType CUDA evt name result kernel_names = kernel_names result found = any kernel_pattern name name kernel_names flash_vs_triton q k v score_mod=None block_mask=None rtol= compiled_fn = torch compile flex_attention out_ref_fp = flex_attention q torch float k torch float v torch float score_mod=score_mod block_mask=block_mask q dtype out_flash = compiled_fn q k v score_mod=score_mod block_mask=block_mask kernel_options= force_flash True out_triton = compiled_fn q k v score_mod=score_mod block_mask=block_mask kernel_options= force_flash False assert out_flash shape == out_ref_fp shape == out_triton shape assert torch isnan out_flash any assert torch isnan out_triton any assert torch isnan out_ref_fp any assert torch isfinite out_flash all assert torch isfinite out_triton all assert torch isfinite out_ref_fp all fwd_atol = out_ref_fp + - - out_ref_fp abs max item triton_error = out_triton - out_ref_fp abs max item flash_error = out_flash - out_ref_fp abs max item assert flash_error = rtol triton_error + fwd_atol f Flash error flash_error e exceeds rtol x Triton error triton_error e + fwd_atol e out_flash out_triton out_ref_fp name_fn score_mod score_mod __name__ lstrip _ unittest skipIf ensure_flash_available Flash attention CUTE library available TestFlexFlash InductorTestCase dtypes torch float torch bfloat test_flash_attention_basic device dtype q k v = create_test_tensors dtype=dtype device=device flash_vs_triton q k v dtypes torch float torch bfloat parametrize score_mod _times_two _causal _rel_bias name_fn=name_fn test_flash_attention_with_score_mod device dtype score_mod q k v = create_test_tensors dtype=dtype device=device flash_vs_triton q k v score_mod=score_mod dtypes torch float torch bfloat parametrize seq_len test_flash_attention_unfriendly_seqlen_with_causal device dtype seq_len Test flash attention unfriendly sequence lengths causal masking q k v = create_test_tensors seq_len=seq_len dtype=dtype device=device flash_vs_triton q k v score_mod=_causal dtypes torch float torch bfloat test_flash_attention_kernel_called device dtype Test flash attention kernel actually called when force_flash=True q k v = create_test_tensors dtype=dtype device=device compiled_fn = torch compile flex_attention Test flash kernel called force_flash=True cuda_kernel_profiler flash_attncute prof_result compiled_fn q k v score_mod=_causal kernel_options= force_flash True assertTrue prof_result found f Flash attention kernel found Available kernels prof_result kernel_names Test flash kernel NOT called force_flash=False cuda_kernel_profiler flash_attncute prof_result compiled_fn q k v score_mod=_causal kernel_options= force_flash False assertFalse prof_result found f Flash attention kernel unexpectedly found when force_flash=False Kernels prof_result kernel_names dtypes torch float torch bfloat test_flash_attention_with_alibi_learned device dtype Test flash attention ALiBi learned slopes tensor loading q k v = create_test_tensors dtype=dtype device=device score_mod = create_alibi_learned num_heads= dtype=dtype flash_vs_triton q k v score_mod=score_mod dtypes torch float torch bfloat test_flash_attention_with_pos_bias_table device dtype Test flash attention position bias table tensor loading q k v = create_test_tensors dtype=dtype device=device score_mod = create_pos_bias_table seq_len= dtype=dtype flash_vs_triton q k v score_mod=score_mod dtypes torch float torch bfloat test_flash_attention_with_head_scale device dtype Test flash attention head scaling tensor loading q k v = create_test_tensors dtype=dtype device=device score_mod = create_head_scale num_heads= dtype=dtype flash_vs_triton q k v score_mod=score_mod dtypes torch float torch bfloat test_flash_attention_with_batch_bias device dtype Test flash attention batch bias tensor loading q k v = create_test_tensors dtype=dtype device=device score_mod = create_batch_bias batch_size= dtype=dtype flash_vs_triton q k v score_mod=score_mod dtypes torch float torch bfloat test_flash_attention_with_batch_head_bias device dtype Test flash attention batch-head bias matrix tensor loading q k v = create_test_tensors dtype=dtype device=device score_mod = create_batch_head_bias batch_size= num_heads= dtype=dtype flash_vs_triton q k v score_mod=score_mod dtypes torch float torch bfloat test_flash_attention_with_dual_buffer_bias device dtype Test flash attention dual buffer loading tensor loading q k v = create_test_tensors dtype=dtype device=device score_mod = create_dual_buffer_bias num_heads= seq_len= dtype=dtype flash_vs_triton q k v score_mod=score_mod dtypes torch float torch bfloat test_flash_attention_with_score_view_buffer device dtype Score modifier should load non-contiguous view num_heads = q k v = create_test_tensors num_heads=num_heads dtype=dtype device=device base_scales = torch rand num_heads device=device dtype=dtype + scales_view = base_scales assert scales_view is_contiguous score_view_mod score b h q_idx kv_idx score + scales_view h flash_vs_triton q k v score_mod=score_view_mod dtypes torch float torch bfloat test_force_flash_error_with_requires_grad device dtype Test force_flash=True raises error when tensor requires gradients q k v = create_test_tensors dtype=dtype device=device bias = torch randn device=device dtype=dtype requires_grad=True score_mod_with_grad score b h q_idx kv_idx score + bias h compiled_fn = torch compile flex_attention assertRaisesRegex RuntimeError r force_flash=True flash attention cannot used require gradients compiled_fn q k v score_mod=score_mod_with_grad kernel_options= force_flash True dtypes torch float torch bfloat test_flash_attention_with_block_mask device dtype Test flash attention block mask mask_mod q k v = create_test_tensors dtype=dtype device=device causal_mask b h q_idx kv_idx q_idx = kv_idx block_mask = create_block_mask causal_mask device=device flash_vs_triton q k v block_mask=block_mask dtypes torch float torch bfloat test_flash_attention_block_mask_with_score_mod device dtype Test flash attention both block mask score_mod q k v = create_test_tensors dtype=dtype device=device causal_mask b h q_idx kv_idx q_idx = kv_idx block_mask = create_block_mask causal_mask device=device flash_vs_triton q k v score_mod=_times_two block_mask=block_mask dtypes torch float torch bfloat test_flash_attention_with_mask_mod_buffer device dtype Test flash attention mask_mod loads buffer q k v = create_test_tensors batch_size= num_heads= dtype=dtype device=device mask_bias = torch randn device=device dtype=dtype custom_mask b h q_idx kv_idx bias_value = mask_bias h q_idx = kv_idx &#124; bias_value block_mask = create_block_mask custom_mask device=device flash_vs_triton q k v block_mask=block_mask dtypes torch float torch bfloat test_flash_attention_with_doc_mask device dtype Test flash attention document-aware mask_mod Use shorter sequences make document layout explicit seq_len = q k v = create_test_tensors batch_size= num_heads= seq_len=seq_len dtype=dtype device=device lengths_per_batch = batch batch uses different document arrangement document_ids = lengths lengths_per_batch assert sum lengths == seq_len doc_tokens = doc_id length enumerate lengths doc_tokens extend doc_id length document_ids append doc_tokens document_ids = torch tensor document_ids device=device dtype=torch long document_mask b _h q_idx kv_idx doc_id_q = document_ids b q_idx doc_id_kv = document_ids b kv_idx doc_id_q == doc_id_kv block_mask = create_block_mask document_mask seq_len seq_len device=device flash_vs_triton q k v block_mask=block_mask dtypes torch float torch bfloat test_flash_attention_mask_mod_with_view_buffer device dtype Mask modifier should support buffers non-contiguous views batch_size num_heads seq_len = q k v = create_test_tensors batch_size=batch_size num_heads=num_heads dtype=dtype device=device base_bias = torch randn num_heads device=device dtype=dtype mask_bias_view = base_bias assert mask_bias_view is_contiguous mask_with_view_buffer b h q_idx kv_idx bias_value = mask_bias_view h double_bias = bias_value q_idx = kv_idx &#124; double_bias block_mask = create_block_mask mask_with_view_buffer batch_size num_heads seq_len seq_len device=device flash_vs_triton q k v block_mask=block_mask dtypes torch float torch bfloat test_flash_attention_mask_mod_with_dual_buffers device dtype Mask modifier should support multiple captured buffers batch_size num_heads seq_len = q k v = create_test_tensors batch_size=batch_size num_heads=num_heads dtype=dtype device=device head_bias = torch randn num_heads device=device dtype=dtype batch_bias = torch randn batch_size device=device dtype=dtype dual_buffer_mask b h q_idx kv_idx head_term = head_bias h batch_term = batch_bias b causal = q_idx = kv_idx bias_cond = head_term + batch_term torch float causal &#124; bias_cond block_mask = create_block_mask dual_buffer_mask batch_size num_heads seq_len seq_len device=device flash_vs_triton q k v block_mask=block_mask dtypes torch float torch bfloat test_flash_attention_score_mod_with_many_buffer_indexing device dtype batch_size num_heads seq_len = q k v = create_test_tensors batch_size=batch_size num_heads=num_heads dtype=dtype device=device head_bias = torch randn num_heads device=device dtype=dtype query_scale = torch randn seq_len device=device dtype=dtype kv_scale = torch randn seq_len device=device dtype=dtype batch_bias = torch randn batch_size device=device dtype=dtype complex_score score b h q_idx kv_idx head_term = head_bias h query_term = query_scale q_idx kv_term = kv_scale kv_idx batch_term = batch_bias b score + head_term + query_term - kv_term + batch_term flash_vs_triton q k v score_mod=complex_score dtypes torch float torch bfloat test_flash_attention_with_score_and_mask_buffers device dtype Test flash attention both score_mod mask_mod using buffers q k v = create_test_tensors batch_size= num_heads= dtype=dtype device=device score_bias = torch randn device=device dtype=dtype mask_bias = torch randn device=device dtype=dtype score_with_buffer score b h q_idx kv_idx score + score_bias h mask_with_buffer b h q_idx kv_idx bias_value = mask_bias h q_idx = kv_idx &#124; bias_value block_mask = create_block_mask mask_with_buffer device=device flash_vs_triton q k v score_mod=score_with_buffer block_mask=block_mask instantiate_device_type_tests TestFlexFlash globals only_for= cuda __name__ == __main__ torch _inductor test_case run_tests run_tests