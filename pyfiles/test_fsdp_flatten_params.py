Owner s oncall distributed sys torch torch nn nn torch distributed dist torch distributed fsdp FullyShardedDataParallel FSDP torch distributed fsdp _flat_param FlatParamHandle FlatParamShardMetadata HandleShardingStrategy torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp FSDPTest torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests TEST_WITH_DEV_DBG_ASAN dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit TestFlattenParams FSDPTest Tests parameter flattening shard metadata logic property world_size - int Clamp world size since these unit tests either exercise only flattening logic check sharding subroutines directly without requiring multiple ranks _get_default_config device_type = acc type acc = torch accelerator current_accelerator cpu device torch device device_type sharding_strategy HandleShardingStrategy FULL_SHARD offload_params False mp_param_dtype None mp_reduce_dtype None keep_low_precision_grads False process_group process_group use_orig_params False fsdp_extension None _get_transformer seed= torch manual_seed seed keep everything deterministic module = torch nn Transformer d_model= num_encoder_layers= num_decoder_layers= dim_feedforward= dropout= module dummy_buffer = nn Buffer torch tensor get_input device dtype torch manual_seed keep everything deterministic src = torch rand device=device dtype=dtype T x B x C tgt = torch rand device=device dtype=dtype T x B x C src tgt module get_input = get_input module _get_shared_params_transformer seed= module = _get_transformer seed=seed share FFNs enc_layer dec_layer zip module encoder layers module decoder layers dec_layer linear weight = enc_layer linear weight dec_layer linear weight = enc_layer linear weight module skip_if_lt_x_gpu test_partial_flattening Tests flattening some submodules others run_subtests half False True _test_partial_flattening _test_partial_flattening half bool module = _get_transformer half module = module half numel = sum p numel p module parameters encoder_ _params = list module encoder layers parameters decoder_ _params = list module decoder layers parameters params_to_flatten = encoder_ _params + decoder_ _params num_params = len encoder_ _params len decoder_ _params numel_to_flatten = sum p numel p params_to_flatten module encoder layers = FSDP module encoder layers module decoder layers = FSDP module decoder layers flat_params = module encoder layers _flat_param module decoder layers _flat_param assertEqual sum fp numel fp flat_params numel_to_flatten assertEqual sum p numel p module parameters numel Check flattened parameters have been replaced single ` FlatParameter ` assertEqual len list module encoder layers parameters assertEqual len list module decoder layers parameters Check non-flattened parameters remain assertEqual len list module encoder layers parameters num_params assertEqual len list module decoder layers parameters num_params Check calling ` module ` affects ` FlatParameter ` s orig_dtype = params_to_flatten dtype new_dtype = torch float orig_dtype == torch float torch float flat_param flat_params assertEqual flat_param dtype orig_dtype assertTrue all p dtype == orig_dtype p module encoder layers parameters module = module dtype=new_dtype flat_param flat_params assertEqual flat_param dtype new_dtype assertTrue all p dtype == new_dtype p module encoder layers parameters test_flatten_nothing Tests constructing ` ` FlatParamHandle ` ` no parameters raises error run_subtests half False True _test_flatten_nothing _test_flatten_nothing half bool module = _get_transformer half module = module half assertRaisesRegex ValueError Cannot construct FlatParamHandle empty parameter list FlatParamHandle module _get_default_config skip_if_lt_x_gpu test_empty_module Tests flattening empty module i e one without any parameters module = _get_empty_module in_data = torch rand ref_out = module in_data fsdp_module = FSDP module assertEqual len list fsdp_module parameters assertIsNone fsdp_module _flat_param fsdp_out = fsdp_module in_data assertEqual ref_out fsdp_out _get_empty_module Returns module no parameters torch manual_seed keep everything deterministic EmptyModule torch nn Module forward x x + get_input device dtype torch manual_seed keep everything deterministic torch rand device=device dtype=dtype EmptyModule test_numel_without_shared_params Tests numel preserved after flattening when there no shared parameters module run_subtests half False True _test_numel_without_shared_params _test_numel_without_shared_params half bool module = _get_transformer half module = module half _test_numel module test_numel_with_shared_params Tests numel preserved after flattening when there shared parameters module run_subtests half False True _test_numel_with_shared_params _test_numel_with_shared_params half bool module = _get_shared_params_transformer half module = module half _test_numel module _test_numel module ref_numel = sum p numel p module parameters params_to_flatten = list module parameters flat_param_handle = FlatParamHandle params_to_flatten module _get_default_config assertEqual ref_numel flat_param_handle flat_param numel skip_if_lt_x_gpu test_output_without_shared_params Tests forward pass after flattening when there no shared parameters module run_subtests half False True _test_output_without_shared_params _test_output_without_shared_params half bool module = _get_transformer half module = module half _test_output module skip_if_lt_x_gpu test_output_with_shared_params Tests forward pass after flattening when there shared parameters module run_subtests half False True _test_output_with_shared_params _test_output_with_shared_params half bool module = _get_shared_params_transformer half module = module half _test_output module _test_output module nn Module module = module rank ref_output = _get_output module fsdp_module = FSDP module fsdp_output = _get_output fsdp_module assertEqual ref_output fsdp_output _get_output module device = next module parameters device dtype = next module parameters dtype input = module get_input device dtype module input skip_if_lt_x_gpu test_pnorm_after_step_with_shared_params Tests parameter Frobenius norm parity after optimizer step when there shared parameters module If parameter sharing handled incorrectly then optimizer step should reveal run_subtests half False True _test_pnorm_after_step_with_shared_params _test_pnorm_after_step_with_shared_params half bool module = _get_shared_params_transformer rank half module = module half ref_pnorm_after_step = _get_pnorm_after_step module module = _get_shared_params_transformer rank recreate half module = module half fsdp_module = FSDP module fsdp_pnorm_after_step = _get_pnorm_after_step fsdp_module assertEqual ref_pnorm_after_step fsdp_pnorm_after_step _get_pnorm_after_step module optim = torch optim SGD module parameters lr= loss = _get_output module sum loss backward optim step torch norm torch stack p detach norm p module parameters test_flat_param_shard_metadata_unaligned Tests ` ` FlatParameter ` ` shard metadata computed expected without any explicit alignment padding module = torch nn Sequential torch nn Linear bias=False nn ReLU torch nn Linear bias=False nn ReLU torch nn Linear bias=False nn ReLU params_to_flatten = list module parameters handle = FlatParamHandle params_to_flatten module _get_default_config _test_flat_param_shard_metadata handle start= end= expected=FlatParamShardMetadata param_names= weight param_shapes= param_strides= param_contiguities= True param_numels= param_offsets= _test_flat_param_shard_metadata handle start= end= expected=FlatParamShardMetadata param_names= weight param_shapes= param_strides= param_contiguities= True param_numels= param_offsets= _test_flat_param_shard_metadata handle start= end= expected=FlatParamShardMetadata param_names= weight param_shapes= param_strides= param_contiguities= True param_numels= param_offsets= _test_flat_param_shard_metadata handle start= end= expected=FlatParamShardMetadata param_names= weight weight param_shapes= param_strides= param_contiguities= True True param_numels= param_offsets= _test_flat_param_shard_metadata handle start= end= expected=FlatParamShardMetadata param_names= weight weight param_shapes= param_strides= param_contiguities= True True param_numels= param_offsets= _test_flat_param_shard_metadata handle start= end= expected=FlatParamShardMetadata param_names= weight weight param_shapes= param_strides= param_contiguities= True True param_numels= param_offsets= _test_flat_param_shard_metadata handle start= end= expected=FlatParamShardMetadata param_names= weight param_shapes= param_strides= param_contiguities= True param_numels= param_offsets= _test_flat_param_shard_metadata handle start= end= expected=FlatParamShardMetadata param_names= weight weight param_shapes= param_strides= param_contiguities= True True param_numels= param_offsets= _test_flat_param_shard_metadata handle start= end= expected=FlatParamShardMetadata param_names= weight weight param_shapes= param_strides= param_contiguities= True True param_numels= param_offsets= _test_flat_param_shard_metadata handle start= end= expected=FlatParamShardMetadata param_names= weight param_shapes= param_strides= param_contiguities= True param_numels= param_offsets= test_flat_param_shard_metadata_aligned_full_precision Tests ` ` FlatParameter ` ` shard metadata computed expected alignment padding parameter full precision module = torch nn Sequential torch nn Linear bias=False weight torch nn Linear bias=False weight torch nn Linear bias=False weight params_to_flatten = list module parameters handle_kwargs = _get_default_config handle_kwargs use_orig_params = True handle = FlatParamHandle params_to_flatten module handle_kwargs For -bit full precision FSDP pads up numel after each original parameter achieve mod numel i e mod bytes Thus unsharded ` FlatParameter ` layout looks like + + + + where x means x numel padding This gives total numel The ` FlatParamShardMetadata ` do include alignment padding do account them _test_flat_param_shard_metadata handle Emulate rank ranks start= end= expected=FlatParamShardMetadata param_names= weight weight param_shapes= param_strides= param_contiguities= True True param_numels= + + = param_offsets= _test_flat_param_shard_metadata handle Emulate rank ranks start= end= expected=FlatParamShardMetadata param_names= weight weight param_shapes= param_strides= param_contiguities= True True param_numels= + + = param_offsets= test_flat_param_shard_metadata_aligned_mixed_precision Tests ` ` FlatParameter ` ` shard metadata computed expected alignment padding parameter mixed precision module = torch nn Sequential torch nn Linear bias=False weight torch nn Linear bias=False weight torch nn Linear bias=False weight params_to_flatten = list module parameters handle_kwargs = _get_default_config handle_kwargs use_orig_params = True handle_kwargs mp_param_dtype = torch float handle = FlatParamHandle params_to_flatten module handle_kwargs For -bit mixed precision FSDP pads up numel after each original parameter achieve mod numel i e mod bytes Thus unsharded ` FlatParameter ` layout looks like + + + + where x means x numel padding This gives total numel The ` FlatParamShardMetadata ` do include alignment padding do account them _test_flat_param_shard_metadata handle Emulate rank ranks start= end= expected=FlatParamShardMetadata param_names= weight weight param_shapes= param_strides= param_contiguities= True True param_numels= + + = param_offsets= _test_flat_param_shard_metadata handle Emulate rank ranks start= end= expected=FlatParamShardMetadata param_names= weight weight param_shapes= param_strides= param_contiguities= True True param_numels= + + = param_offsets= _test_flat_param_shard_metadata handle FlatParamHandle start int end int expected FlatParamShardMetadata Tests subroutine ` ` _get_shard_metadata ` ` computes shard metadata based start end indices unsharded flat parameter where both indices inclusive We manually set relevant attributes flat parameter able check effect ` ` _get_shard_metadata ` ` via ` ` shard_metadata ` ` since normally attributes set ` ` _init_shard_metadata ` ` start end indices fixed based rank world size flat_param = handle flat_param flat_param _shard_param_infos = handle _get_shard_metadata start end shard_metadata = handle shard_metadata assertEqual shard_metadata expected msg=f handle shard_metadata expected parametrize memory_format torch contiguous_format torch channels_last test_flat_param_shard_metadata_with_memory_format memory_format Tests ` ` FlatParameter ` ` shard metadata computed expected alignment padding parameter full precision module = torch nn Sequential torch nn Conv d bias=False weight params torch nn Conv d bias=False weight params torch nn Conv d bias=False weight params memory_format=memory_format params_to_flatten = list module parameters handle_kwargs = _get_default_config handle_kwargs use_orig_params = True handle = FlatParamHandle params_to_flatten module handle_kwargs contiguous_tensors = memory_format == torch contiguous_format _test_flat_param_shard_metadata handle Emulate rank ranks start= end= expected=FlatParamShardMetadata param_names= weight weight param_shapes= param_strides= contiguous_tensors param_contiguities= contiguous_tensors contiguous_tensors param_numels= param_offsets= _test_flat_param_shard_metadata handle Emulate rank ranks start= end= expected=FlatParamShardMetadata param_names= weight weight param_shapes= param_strides= contiguous_tensors param_contiguities= contiguous_tensors contiguous_tensors param_numels= param_offsets= skip_if_lt_x_gpu test_writeback_orig_params_no_shard EmbeddingModel nn Module __init__ - None super __init__ emb = nn Embedding forward x torch Tensor - torch Tensor emb x sum model = EmbeddingModel half rank fsdp_model = FSDP model sharding_strategy=HandleShardingStrategy NO_SHARD use_orig_params=True Copied https github com huggingface accelerate blob main src accelerate accelerator py#L - fsdp_module FSDP fsdp_modules fsdp_model fsdp_module _has_params continue param = fsdp_module _flat_param param data = param data float fsdp_module _handle _orig_param_dtype = torch float x = torch randint device=self rank torch no_grad out = fsdp_model x assertEqual out shape torch Size instantiate_parametrized_tests TestFlattenParams __name__ == __main__ run_tests