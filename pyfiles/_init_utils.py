mypy allow-untyped-defs collections itertools os warnings collections abc Callable Generator Iterable Iterator typing Any no_type_check Optional TYPE_CHECKING Union torch torch distributed dist torch distributed fsdp _exec_order_utils exec_order_utils torch distributed fsdp _traversal_utils traversal_utils torch distributed fsdp fully_sharded_data_parallel fsdp_file torch nn nn torch distributed algorithms _comm_hooks default_hooks torch distributed device_mesh DeviceMesh torch distributed distributed_c d _get_default_group torch distributed fsdp _common_utils _FSDPDeviceHandle _FSDPState _get_module_fsdp_state _is_fsdp_flattened _named_parameters_with_duplicates clean_tensor_name TrainingState torch distributed fsdp _flat_param _FSDP_USE_FULL_PREC_IN_EVAL FlatParameter FlatParamHandle HandleShardingStrategy torch distributed fsdp _limiter_utils _FreeEventQueue torch distributed fsdp api BackwardPrefetch CPUOffload FullOptimStateDictConfig FullStateDictConfig MixedPrecision ShardingStrategy StateDictConfig StateDictType torch distributed fsdp wrap _Policy torch distributed tensor parallel fsdp DTensorExtensions torch distributed utils _sync_params_and_buffers torch utils _python_dispatch is_traceable_wrapper_subclass TYPE_CHECKING torch utils hooks RemovableHandle _TORCHDISTX_AVAIL = True try torchdistx deferred_init fake type ignore except ImportError _TORCHDISTX_AVAIL = False PARAM_BROADCAST_BUCKET_SIZE = FSDP_SYNCED = _fsdp_synced Specification process groups hybrid sharding strategies HybridShardProcessGroupType = tuple dist ProcessGroup dist ProcessGroup Overall specification process group ProcessGroupType = Optional Union dist ProcessGroup HybridShardProcessGroupType TODO awgu Refactor later SHARDING_STRATEGY_MAP = ShardingStrategy NO_SHARD HandleShardingStrategy NO_SHARD ShardingStrategy FULL_SHARD HandleShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP HandleShardingStrategy SHARD_GRAD_OP ShardingStrategy HYBRID_SHARD HandleShardingStrategy HYBRID_SHARD ShardingStrategy _HYBRID_SHARD_ZERO HandleShardingStrategy _HYBRID_SHARD_ZERO HYBRID_SHARDING_STRATEGIES = ShardingStrategy HYBRID_SHARD ShardingStrategy _HYBRID_SHARD_ZERO NO_RESHARD_AFTER_FORWARD_STRATEGIES = ShardingStrategy SHARD_GRAD_OP ShardingStrategy _HYBRID_SHARD_ZERO NOTE Since non-self attributes cannot type annotated several attributes ` state ` defined first local variables before being assigned no_type_check _init_process_group_state state _FSDPState process_group ProcessGroupType sharding_strategy ShardingStrategy policy Optional _Policy device_mesh Optional DeviceMesh = None - _FSDPState process_group None device_mesh None raise ValueError Cannot pass both process_group device_mesh same time Please just pass only one them is_hybrid_strategy = sharding_strategy HYBRID_SHARDING_STRATEGIES is_hybrid_strategy process_group None policy None device_mesh None Raise error here since manual wrapping no process group passed there no way ensure all wrapped FSDP instances use same process groups raise ValueError f Manual wrapping sharding_strategy requires explicit specification process group device_mesh state = _init_process_group_state_for_hybrid_shard state process_group device_mesh device_mesh state _device_mesh = device_mesh state process_group = device_mesh get_group mesh_dim= state process_group = process_group process_group None _get_default_group state rank = state process_group rank state world_size = state process_group size data_parallel_world_size = state world_size is_hybrid_strategy data_parallel_world_size = state _inter_node_pg size state _gradient_predivide_factor = default_hooks DefaultState _get_gradient_predivide_factor data_parallel_world_size state _gradient_postdivide_factor = data_parallel_world_size state _gradient_predivide_factor state no_type_check _init_process_group_state_for_hybrid_shard state _FSDPState process_group ProcessGroupType device_mesh DeviceMesh - _FSDPState device_mesh _is_valid_hybrid_shard_device_mesh device_mesh state _device_mesh = device_mesh We currently only allow _inter_node_pg outermost dimension process_group intra_node innermost dimension state _inter_node_pg = device_mesh get_group mesh_dim= state process_group = device_mesh get_group mesh_dim= raise ValueError f Expected device_mesh have ndim= got device_mesh ndim process_group None default_group = _get_default_group intra_node_group inter_node_group = _init_intra_and_inter_node_groups default_group state _device_handle device_count we shard across intra-node state process_group = intra_node_group save _inter_node_pg allreduce across state _inter_node_pg = inter_node_group Check type assign state process_group state _inter_node_pg _is_valid_hybrid_shard_pg_type process_group Assuming user passed intra node group inter node group documented state process_group state _inter_node_pg = process_group raise ValueError Expected process_group passed either None f Tuple dist ProcessGroup dist ProcessGroup got type process_group Create state allreduce state _inter_node_state = _get_default_comm_hook_state process_group=state _inter_node_pg state no_type_check _is_valid_hybrid_shard_pg_type process_group Any - bool isinstance process_group tuple len process_group == all isinstance pg dist ProcessGroup pg process_group no_type_check _is_valid_hybrid_shard_device_mesh device_mesh DeviceMesh - bool isinstance device_mesh DeviceMesh device_mesh ndim == no_type_check _init_intra_node_process_group num_devices_per_node int - dist ProcessGroup Return process group across current node For example given each row distinct node This API would intra-node subgroup across depending process s rank For example rank would get intra_node_subgroup _ = dist new_subgroups num_devices_per_node intra_node_subgroup no_type_check _init_inter_node_process_group global_process_group dist ProcessGroup num_devices_per_node int - dist ProcessGroup Return inter-node process group where each contained rank has same local rank For example given each row distinct node This API would inter-node process group so forth depending process s rank For example rank would get rank would get inter-node pg returned inter_node_pg = None sharding_backend = dist get_backend global_process_group world_size = dist get_world_size global_process_group Assuming fully homogeneous setup num_nodes = world_size num_devices_per_node my_local_rank = dist get_rank global_process_group num_devices_per_node local_rank range num_devices_per_node ranks_for_inter_group = local_rank + i num_devices_per_node i range num_nodes every rank always needs call dist new_group grp = dist new_group ranks=ranks_for_inter_group backend=sharding_backend local_rank == my_local_rank inter_node_pg = grp inter_node_pg None raise AssertionError f my_local_rank expected assign inter-node pg did inter_node_pg _init_intra_and_inter_node_groups global_process_group dist ProcessGroup num_devices_per_node int - tuple dist ProcessGroup dist ProcessGroup Initialize intra inter-node process groups ones corresponding process s rank This function can used initialize process groups ` ` HYBRID_SHARD ` ` ` ` _HYBRID_SHARD_ZERO ` ` FSDP This function assumes each node has equal number CUDA-enabled devices Returns Tuple dist ProcessGroup dist ProcessGroup Intra inter-node process group _init_intra_node_process_group num_devices_per_node _init_inter_node_process_group global_process_group num_devices_per_node no_type_check _init_ignored_module_states state _FSDPState module nn Module ignored_modules Optional Iterable torch nn Module ignored_states Union Optional Iterable torch nn Parameter Optional Iterable torch nn Module = None - _FSDPState ignored_modules None ignored_states None raise ValueError Cannot pass both ignored_modules ignored_states same time Please just pass ignored_states ignored_parameters = None passed_as_ignored_states = ignored_states None passed_as_ignored_states ignored_states_list = list ignored_states _check_ignored_states ignored_states_list True ignored_states_list = _check_ignored_states list ignored_modules ignored_modules None False len ignored_states_list isinstance ignored_states_list nn Parameter ignored_parameters = ignored_states_list ignored_modules = ignored_states_list state _ignored_modules = _get_ignored_modules module ignored_modules state _ignored_params = _get_ignored_params module state _ignored_modules ignored_parameters state _ignored_buffer_names = _get_ignored_buffer_names module state _ignored_modules TODO FSDP s contract buffers well-defined They implicitly ignored most functionality since they sharded however FSDP still imposes some semantics buffers e g buffer mixed precision We should formalize contract decide we need compute store ` _ignored_buffers ` state _check_ignored_states ignored_states list Any passed_as_ignored_states bool - None Check ignored states uniformly parameters uniformly modules We may remove check future we permit mixing len ignored_states == passed_as_ignored_states all_params = all isinstance state nn Parameter state ignored_states all_modules = all isinstance state nn Module state ignored_states all_params all_modules Sort consistent ordering unit test regex matching sorted_types = sorted type state state ignored_states key=repr raise ValueError ignored_states expects all nn Parameter all nn Module list f elements got types sorted_types all isinstance state nn Module state ignored_states sorted_types = sorted type state state ignored_states key=repr raise ValueError ignored_modules expects nn Module list elements got f types sorted_types no_type_check _init_device_handle state _FSDPState module nn Module ignored_params set nn Parameter device_id Optional Union int torch device - _FSDPState Determine device handle used initializing FSDP If device specified ` ` device_id ` ` then returns device handle corresponds device type Otherwise If module already non-CPU device then device type non-CPU device type If module CPU meta then device type current accelerator device See ref ` Accelerators accelerators ` details This method will called once ignored parameters determined device handle maybe needed other initialization determined_device = None device_id None determined_device = device_id isinstance device_id torch device torch device device_id determined_device None param _get_orig_params module ignored_params param device type cpu meta continue determined_device None determined_device = param device param device type = determined_device type raise RuntimeError f FSDP does support modules different device types f got params determined_device type param device type determined_device = determined_device torch _C _get_accelerator determined_device type == cpu raise RuntimeError FSDP needs non-CPU accelerator device no accelerator device detected state _device_handle = _FSDPDeviceHandle from_device determined_device state no_type_check _init_buffer_state state _FSDPState module nn Module - _FSDPState state _buffer_names = _get_buffer_names module Save mapping clean fully-qualified buffer name starting ` module ` its original dtype restoring dtype during model checkpointing when buffer mixed precision enabled The names should clean since casting happens ` summon_full_params ` context _buffer_name_to_orig_dtype dict str torch dtype = buffer_name buffer module named_buffers buffer_name = clean_tensor_name buffer_name _buffer_name_to_orig_dtype buffer_name = buffer dtype state _buffer_name_to_orig_dtype = _buffer_name_to_orig_dtype state no_type_check _init_core_state state _FSDPState sharding_strategy Optional ShardingStrategy mixed_precision Optional MixedPrecision cpu_offload Optional CPUOffload limit_all_gathers bool use_orig_params bool backward_prefetch_limit int forward_prefetch_limit int - _FSDPState We clamp strategy ` NO_SHARD ` world size since they currently functionally equivalent This may change when we integrate FSDP MoE state world_size == sharding_strategy = ShardingStrategy NO_SHARD warnings warn FSDP switching use ` NO_SHARD ` instead f sharding_strategy ShardingStrategy FULL_SHARD since world size stacklevel= sharding_strategy = ShardingStrategy NO_SHARD sharding_strategy == ShardingStrategy NO_SHARD warnings warn The ` NO_SHARD ` sharding strategy deprecated If having issues please use ` DistributedDataParallel ` instead FutureWarning Level here level ` FullyShardedDataParallel ` level true caller stacklevel= state sharding_strategy = sharding_strategy ShardingStrategy FULL_SHARD state mixed_precision = mixed_precision MixedPrecision mixed_precision None torch _C _log_api_usage_once f torch distributed fsdp mixed_precision str state mixed_precision state _use_full_prec_in_eval = os environ get _FSDP_USE_FULL_PREC_IN_EVAL == state cpu_offload = cpu_offload CPUOffload state limit_all_gathers = limit_all_gathers state _use_orig_params = use_orig_params state training_state = TrainingState IDLE state _is_root = None state _free_event_queue = _FreeEventQueue state _debug_level = dist get_debug_level state _exec_order_data = exec_order_utils _ExecOrderData state _debug_level backward_prefetch_limit forward_prefetch_limit state _unshard_event = None Mapping fully sharded module handles responsible unshard reshard see Note Fully Sharded Module _fully_sharded_module_to_handle dict nn Module FlatParamHandle = state _fully_sharded_module_to_handle = _fully_sharded_module_to_handle Invariant ` state params ` contains exactly ` FlatParameter ` s handles ` state _handle ` _handle Optional FlatParamHandle = None state _handle = _handle params list FlatParameter = state params = params state no_type_check _init_runtime_state state _FSDPState - _FSDPState _root_pre_forward_handles list RemovableHandle = state _root_pre_forward_handles = _root_pre_forward_handles _pre_forward_handles list RemovableHandle = state _pre_forward_handles = _pre_forward_handles _post_forward_handles list RemovableHandle = state _post_forward_handles = _post_forward_handles state _sync_gradients = True state _comm_hook = None state _comm_hook_state = None Used prevent running pre-backward hook multiple times state no_type_check _init_prefetching_state state _FSDPState backward_prefetch BackwardPrefetch forward_prefetch bool - _FSDPState state backward_prefetch = backward_prefetch state forward_prefetch = forward_prefetch The data structures use tuples handles generalize over case where module s forward involves multiple handles state no_type_check pyrefly ignore bad-function-definition _init_extension state _FSDPState device_mesh DeviceMesh = None - _FSDPState TODO we need add additional check once we support FSDP + PiPPy This check currently sufficient since we only support FSDP + TP root_mesh = device_mesh _get_root_mesh device_mesh None None root mesh same device_mesh meaning device_mesh sliced out root mesh device_mesh root_mesh = state _device_mesh state _fsdp_extension = DTensorExtensions state _device_handle We need explicitly set _fsdp_extension None Otherwise we will run into infinite recursion when getting attribute state _fsdp_extension = None state no_type_check _init_state_dict_state state _FSDPState - _FSDPState state _state_dict_type = StateDictType FULL_STATE_DICT state_dict_config StateDictConfig = FullStateDictConfig state _optim_state_dict_config = FullOptimStateDictConfig state _state_dict_config = state_dict_config unshard_params_ctx dict nn Module Generator = state _unshard_params_ctx = unshard_params_ctx state _verify_managed_params module nn Module params list nn Parameter - None Verify parameters accepted FSDP The only restriction now parameter cannot scalar tensor param shape == param params len param shape == param_name = name param_ module named_parameters param param_ param_name = name break param_name raise AssertionError Expected param_name set raise ValueError FSDP doesn t support scalar parameters f Change param_name D tensor numel equal no_type_check _init_param_handle_from_module state _FSDPState fully_sharded_module nn Module device_id Optional Union int torch device param_init_fn Optional Callable nn Module None sync_module_states bool - _FSDPState Initialize ` ` FlatParamHandle ` ` module ` ` fully_sharded_module ` ` _check_single_device_module fully_sharded_module state _ignored_params device_id device_from_device_id = _get_device_from_device_id device_id state rank state _device_handle is_meta_module is_torchdistX_deferred_init = _need_to_materialize_module fully_sharded_module state _ignored_params state _ignored_modules Materialize module needed is_meta_module is_torchdistX_deferred_init param_init_fn None _materialize_with_param_init_fn fully_sharded_module param_init_fn state _ignored_modules is_meta_module _materialize_meta_module fully_sharded_module device_id state _ignored_modules state _device_handle is_torchdistX_deferred_init deferred_init materialize_module fully_sharded_module check_fn=lambda submodule _get_module_fsdp_state submodule None submodule state _ignored_modules ignored_buffers = buffer ignored_module state _ignored_modules buffer ignored_module buffers _move_module_to_device fully_sharded_module state _ignored_params ignored_buffers device_from_device_id state compute_device = _get_compute_device fully_sharded_module state _ignored_params device_from_device_id state rank state _device_handle managed_params = list _get_orig_params fully_sharded_module state _ignored_params _verify_managed_params fully_sharded_module managed_params sync_module_states _sync_module_params_and_buffers fully_sharded_module managed_params state process_group state sharding_strategy HYBRID_SHARDING_STRATEGIES _sync_module_params_and_buffers fully_sharded_module managed_params state _inter_node_pg _init_param_handle_from_params state managed_params fully_sharded_module state no_type_check _init_param_handle_from_params state _FSDPState params list nn Parameter fully_sharded_module nn Module len params == handle = FlatParamHandle params fully_sharded_module state compute_device SHARDING_STRATEGY_MAP state sharding_strategy state cpu_offload offload_params state mixed_precision param_dtype state mixed_precision reduce_dtype state mixed_precision keep_low_precision_grads state process_group state _use_orig_params fsdp_extension=state _fsdp_extension handle shard state _handle raise AssertionError Expected state _handle None state params append handle flat_param state _handle = handle state _fully_sharded_module_to_handle handle _fully_sharded_module = handle cpu_device = torch device cpu state cpu_offload offload_params handle flat_param device = cpu_device handle flat_param_to cpu_device _get_ignored_modules root_module nn Module _ignored_modules Optional Iterable torch nn Module - set nn Module Check ` ` _ignored_modules ` ` iterable ` ` nn Module ` ` s without any FSDP instances Return modules contained their module subtrees ` set ` Nested FSDP instances excluded their already-computed ignored modules included ` ` _ignored_modules ` ` represents argument passed user FSDP msg_prefix = ` ignored_modules ` should iterable ` torch nn Module ` s try ignored_root_modules = set _ignored_modules _ignored_modules None set except TypeError e raise TypeError msg_prefix + f got type _ignored_modules e module ignored_root_modules isinstance module torch nn Module raise TypeError msg_prefix + f got iterable type module _get_module_fsdp_state module TODO We may relax taking FSDP instance s wrapped module provide more flexibility user raise ValueError ` ignored_modules ` should include FSDP modules Treat modules cannot compose ` fully_shard ` ignored modules meaning their subtrees ignored module root_module modules traversal_utils _composable module ignored_root_modules add module NOTE Even ` ignored_root_modules ` empty do early so FSDP instance can get any ignored modules its children Include child modules exclude nested FSDP modules themselves ignored_modules = child module ignored_root_modules child module modules isinstance child fsdp_file FullyShardedDataParallel root_module ignored_modules warnings warn Trying ignore top-level module passed into FSDP constructor itself will result all parameters being f ignored well-supported module stacklevel= Include nested FSDP modules ignored modules submodule root_module modules optional_fsdp_state = _get_module_fsdp_state submodule optional_fsdp_state None hasattr optional_fsdp_state _ignored_modules raise AssertionError Expected optional_fsdp_state have _ignored_modules attribute ignored_modules update optional_fsdp_state _ignored_modules ignored_modules _get_ignored_params root_module torch nn Module ignored_modules set torch nn Module ignored_parameters Optional Iterable torch nn Parameter = None - set torch nn Parameter Return parameters modules ` ` ignored_modules ` ` parameters ` ` ignored_parameters ` ` ` FlatParameter ` s excluded result all_ignored_params set torch nn Parameter = set params_in_ignored_modules = p m ignored_modules p m parameters _is_fsdp_flattened p all_ignored_params update params_in_ignored_modules ignored_parameters None params_in_ignored_parameters = p p ignored_parameters _is_fsdp_flattened p all_ignored_params update params_in_ignored_parameters Always include nested FSDP modules ignored parameters submodule root_module modules optional_fsdp_state = _get_module_fsdp_state submodule optional_fsdp_state None hasattr optional_fsdp_state _ignored_params raise AssertionError Expected optional_fsdp_state have _ignored_params attribute all_ignored_params update optional_fsdp_state _ignored_params all_ignored_params _get_ignored_buffer_names root_module torch nn Module ignored_modules set torch nn Module - set str Return cleaned buffer FQNs ` ` ignored_modules ` ` all_ignored_buffer_names set str = set buffers_in_ignored_modules = buffer m ignored_modules buffer m buffers all_ignored_buffer_names update clean_tensor_name buffer_name buffer_name buffer root_module named_buffers buffer buffers_in_ignored_modules Always include nested FSDP modules ignored buffer names submodule root_module modules optional_fsdp_state = _get_module_fsdp_state submodule optional_fsdp_state None hasattr optional_fsdp_state _ignored_buffer_names raise AssertionError Expected optional_fsdp_state have _ignored_buffer_names attribute all_ignored_buffer_names update optional_fsdp_state _ignored_buffer_names all_ignored_buffer_names _get_buffer_names root_module nn Module - set str Return fully prefixed names all buffers module hierarchy rooted ` ` root_module ` ` ` set ` clean_tensor_name buffer_name buffer_name _ root_module named_buffers _check_single_device_module module nn Module ignored_params set nn Parameter device_id Optional Union int torch device - None Raise error ` ` module ` ` has original parameters multiple devices ignoring parameters ` ` ignored_params ` ` Thus after method module must either fully CPU fully non-CPU device devices = param device param _get_orig_params module ignored_params We allow module partially CPU partially GPU device_id None since device_id arg will result CPU portion being moved GPU This useful cases where part module may parallelized another algorithm may already GPU We d like enforce device_id None otherwise we d flatten parameters mixed module which supported len devices == torch device cpu devices device_id None raise RuntimeError To support module both CPU GPU params please pass device_id argument len devices raise RuntimeError f FSDP only supports single device modules got params devices _get_device_from_device_id device_id Optional Union int torch device rank int device_handle _FSDPDeviceHandle - Optional torch device Return ` ` torch device ` ` specified ` ` device_id ` ` Processes ` ` device_id ` ` returns either corresponding device ` ` None ` ` ` ` device_id ` ` ` ` None ` ` device_id None None device = device_id isinstance device_id torch device torch device device_id device type = cpu device index None warnings warn f FSDP got argument ` device_id ` device_id rank f rank which does have explicit index f FSDP will use current device device_handle current_device f If incorrect please explicitly call ` torch device type set_device ` before FSDP initialization pass explicit device index ` device_id ` argument stacklevel= device = torch device device_handle current_device device _need_to_materialize_module module nn Module ignored_params set nn Parameter ignored_modules set nn Module - tuple bool bool Return ` ` module ` ` has parameters meta device ` ` module ` ` using torchdistX deferred initialization At most returned bools can ` ` True ` ` If either ` ` True ` ` then ` ` module ` ` needs materialized managed_params = list _get_orig_params module ignored_params is_meta_module = any param is_meta param managed_params TODO We need establish contract FSDP buffers For now we skip checking meta buffers ignored modules We should consider refactoring initialization holistically avoid so many traversals submodule module modules submodule ignored_modules continue buf submodule buffers recurse=False is_meta_module &#124; = buf is_meta is_torchdistX_deferred_init = is_meta_module _TORCHDISTX_AVAIL any fake is_fake param param managed_params is_meta_module is_torchdistX_deferred_init _materialize_with_param_init_fn root_module nn Module param_init_fn Callable nn Module None ignored_modules set nn Module - None callable param_init_fn raise ValueError f Expected param_init_fn callable got type param_init_fn modules_to_materialize = _get_modules_to_materialize root_module ignored_modules module modules_to_materialize param_init_fn module _materialize_meta_module root_module nn Module device_from_device_id Optional torch device ignored_modules set nn Module device_handle _FSDPDeviceHandle Run default meta device initialization materialization_device = device_from_device_id torch device device_handle current_device modules_to_materialize = _get_modules_to_materialize root_module ignored_modules module = None try Assume each module s ` reset_parameters ` only initializes its own parameters those its children torch no_grad module modules_to_materialize As contract user only call ` reset_parameters ` module has directly managed parameters buffers module_state_iter = itertools chain module parameters recurse=False pyrefly ignore bad-argument-type module buffers recurse=False has_module_states = len list module_state_iter has_module_states module to_empty device=materialization_device recurse=False module reset_parameters type ignore operator except BaseException e warnings warn Unable call ` reset_parameters ` module meta f device error str e Please ensure your module f type type module implements ` reset_parameters ` method stacklevel= type ignore possibly-undefined raise e _get_modules_to_materialize root_module nn Module ignored_modules set nn Module - list nn Module Run BFS collect modules materialize via ` reset_parameters ` stopping any module FSDP already applied ignored modules modules_to_materialize list nn Module = queue = collections deque root_module visited_modules set nn Module = root_module while queue module = queue popleft modules_to_materialize append module child_module module children child_module visited_modules _get_module_fsdp_state child_module None child_module ignored_modules visited_modules add child_module queue append child_module modules_to_materialize _move_module_to_device module nn Module ignored_params set nn Parameter ignored_buffers set torch Tensor device_from_device_id Optional torch device - None Move ` ` module ` ` depending ` ` device_from_device_id ` ` its current device This includes moving ignored modules parameters - If ` ` device_from_device_id ` ` ` ` None ` ` then moves ` ` module ` ` device - If ` ` device_from_device_id ` ` ` ` None ` ` then does move ` ` module ` ` warns user CPU Precondition ` ` _check_single_device_module ` ` cpu_device = torch device cpu device_from_device_id None BFS ` module ` without traversing any nested FSDP instances collect parameters buffers have yet been managed queue collections deque nn Module = collections deque queue append module params list nn Parameter = buffers list torch Tensor = while queue curr_module = queue popleft NOTE We include check only move parameters buffers CPU device If they CUDA device different one specified ` device_id ` then does NOT move them This so we can raise error ` _get_compute_device ` params extend param param curr_module parameters recurse=False param device == cpu_device buffers extend buffer buffer curr_module buffers recurse=False buffer device == cpu_device submodule curr_module children isinstance submodule fsdp_file FullyShardedDataParallel queue append submodule params_to_move = p p params p ignored_params bufs_to_move = p p buffers p ignored_buffers _move_states_to_device params_to_move bufs_to_move device_from_device_id param = next _get_orig_params module ignored_params None param None param device == cpu_device _warn_cpu_init _move_states_to_device params list nn Parameter buffers list torch Tensor device_from_device_id Optional torch device - None Move states specified device Precondition ` ` _check_single_device_module ` ` module s parameters buffers have been materialized needed len params == len buffers == len params current_device = params device len buffers current_device = buffers device cpu_device = torch device cpu device_from_device_id None Move parameters buffers like ` data ` code path ` nn Module _apply ` which underlies ` nn Module ` param params torch no_grad param data = param device_from_device_id param grad None param grad data = param grad device_from_device_id buffer buffers buffer data = buffer device_from_device_id current_device == cpu_device type ignore possibly-undefined _warn_cpu_init _warn_cpu_init warnings warn The passed-in ` module ` CPU will thus have FSDP s sharding initialization run CPU which may slower than GPU We recommend passing ` device_id ` argument FSDP move ` module ` GPU sharding initialization ` module ` must also GPU device work ` sync_module_states=True ` flag since requires GPU communication stacklevel= _get_compute_device module nn Module ignored_params set nn Parameter device_from_device_id Optional torch device rank int device_handle _FSDPDeviceHandle - torch device Determine FSDP instance s compute device If module already non-CPU device then compute device non-CPU device If module CPU then compute device current device Since method should called after materializing module any non-CPU device should meta device For now compute device always CUDA CUDA-like device its explicit index Precondition ` ` _check_single_device_module ` ` ` ` _move_module_to_device ` ` param = next _get_orig_params module ignored_params None param None param device type = cpu compute_device = param device Determined model param placement compute_device = torch device device_handle current_device device_from_device_id None compute_device = device_from_device_id raise ValueError f Inconsistent compute device ` device_id ` rank rank f compute_device vs device_from_device_id compute_device TODO See how deprecate _sync_module_params_and_buffers module nn Module params list nn Parameter process_group dist ProcessGroup - None Synchronize module states i e parameters ` ` params ` ` all not-yet-synced buffers broadcasting rank all ranks Precondition ` ` sync_module_states == True ` ` ` ` process_group ` ` has been set module_states list torch Tensor = buffer module buffers Avoid re-synchronizing buffers case nested wrapping getattr buffer FSDP_SYNCED False setattr buffer FSDP_SYNCED True detached_buffer = buffer detach is_traceable_wrapper_subclass detached_buffer NOTE Here we assume no nested subclasses most one level subclass both model s buffers params attrs _ = detached_buffer __tensor_flatten__ type ignore attr-defined inner_buffers = getattr detached_buffer attr attr attrs module_states extend inner_buffers module_states append detached_buffer param params detached_param = param detach is_traceable_wrapper_subclass detached_param attrs _ = detached_param __tensor_flatten__ type ignore attr-defined inner_params = getattr detached_param attr attr attrs module_states extend inner_params module_states append detached_param _check_module_states_for_sync_module_states module_states _sync_params_and_buffers process_group module_states PARAM_BROADCAST_BUCKET_SIZE src= _check_module_states_for_sync_module_states module_states list torch Tensor - None module_states any tensor device == torch device cpu tensor module_states raise ValueError The module has CPU parameters buffers when ` sync_module_states=True ` which requires them GPU Please specify ` device_id ` argument move module GPU before passing FSDP _get_orig_params module nn Module ignored_params set nn Parameter - Iterator nn Parameter Return iterator over original parameters ` ` module ` ` The iterator does parameters ` ` ignored_params ` ` any ` ` FlatParameter ` ` s which may present due nested FSDP wrapping any original parameters already flattened only relevant when ` ` use_orig_params=True ` ` param_gen = module parameters try while True param = next param_gen param ignored_params _is_fsdp_flattened param yield param except StopIteration pass _check_orig_params_flattened fsdp_module ignored_params set nn Parameter - None Check original parameters ` ` fsdp_module ` ` have been flattened The flattened parameters made invisible ` ` named_parameters ` ` module hierarchy rooted ` ` fsdp_module ` ` This should called sanity check after flattening wrapped module s parameters param_name param _named_parameters_with_duplicates fsdp_module param ignored_params _is_fsdp_flattened param raise RuntimeError f Found unflattened parameter param_name f param size param __class__ _get_default_comm_hook sharding_strategy ShardingStrategy default_hooks allreduce_hook sharding_strategy == ShardingStrategy NO_SHARD default_hooks reduce_scatter_hook _get_default_comm_hook_state process_group dist ProcessGroup - default_hooks DefaultState default_hooks DefaultState process_group=process_group