Owner s oncall distributed io itertools sys contextlib nullcontext copy deepcopy functools partial typing Any torch torch nn nn torch distributed dist torch distributed _shard sharded_tensor init_from_local_shards Shard ShardedTensor torch distributed _shard sharded_tensor metadata MEM_FORMAT_ENCODING ShardedTensorMetadata TensorProperties torch distributed _shard sharding_spec ChunkShardingSpec ShardMetadata torch distributed _state_dict_utils _all_gather_sharded_tensor _gather_state_dict torch distributed algorithms _checkpoint checkpoint_wrapper apply_activation_checkpointing checkpoint_wrapper CheckpointImpl torch distributed fsdp CPUOffload FullStateDictConfig FullyShardedDataParallel FSDP LocalStateDictConfig MixedPrecision ShardedStateDictConfig StateDictType torch distributed fsdp _common_utils FSDP_PREFIX torch distributed fsdp _unshard_param_utils FLAT_PARAM torch distributed fsdp wrap enable_wrap ModuleWrapPolicy wrap torch distributed remote_device _remote_device torch nn Linear Module TransformerDecoderLayer TransformerEncoderLayer torch nn parallel DistributedDataParallel torch optim SGD torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp _assert_module_states _broadcast_state_dict _get_state_dict _zero_model DEVICEInitMode FSDPInitMode FSDPTest get_full_params SkipModel TransformerWithSharedParams torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests TEST_WITH_DEV_DBG_ASAN dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit INNER_SHAPE = OUTER_SHAPE = BUFFER_SHAPE = NON_ROOT_FSDP_PREFIX = non_fsdp_lin _UNFLATTENED_STATE_DICT_IMPLS = state_dict sharded_state_dict _FLATTENED_STATE_DICT_IMPLS = local_state_dict _SUPPORTED_STATE_DICT_IMPLS = _UNFLATTENED_STATE_DICT_IMPLS + _FLATTENED_STATE_DICT_IMPLS STATE_DICT_MAPPING = state_dict StateDictType FULL_STATE_DICT local_state_dict StateDictType LOCAL_STATE_DICT sharded_state_dict StateDictType SHARDED_STATE_DICT device_type = acc type acc = torch accelerator current_accelerator cpu Model Module __init__ wrap_fsdp register_buffers=False ignore_inner=False mixed_precision=False process_group=None super __init__ inner = Linear INNER_SHAPE register_buffers inner buffer = nn Buffer torch randn BUFFER_SHAPE inner register_buffer non_persistent_buffer torch randn BUFFER_SHAPE persistent=False wrap_fsdp inner = FSDP inner ignored_modules= inner ignore_inner mixed_precision=MixedPrecision param_dtype=torch float reduce_dtype=torch float buffer_dtype=torch float mixed_precision None process_group=process_group outer = Linear OUTER_SHAPE register_buffers outer buffer = nn Buffer torch randn BUFFER_SHAPE outer register_buffer non_persistent_buffer torch randn BUFFER_SHAPE persistent=False forward x Forward twice i = inner x j = inner x outer i + j TestDummyModel torch nn Module __init__ - None super __init__ torch manual_seed net = nn Sequential nn Linear nn ReLU net = nn Sequential nn Linear nn ReLU net = net random_parameter = nn Parameter torch Tensor shared_parameter = random_parameter forward x net net net x get_input torch rand device=device_type TestFSDPStateDict FSDPTest property world_size min torch accelerator device_count _broadcast_state_dict state_dict _broadcast_state_dict rank state_dict _state_compare model model_new assert_fn state_generator= parameters state_base = list getattr model state_generator state_new = list getattr model_new state_generator Regardless ` assert_fn ` number parameters should same assertEqual len state_base len state_new assert_fn state_base state_new _compare_models model model_new assert_fn check_fp =False check_buffers=True assert assert_fn assertEqual assertNotEqual FSDP summon_full_params model FSDP summon_full_params model_new _state_compare model model_new assert_fn check_buffers has_buffers = any len list m buffers m model model_new has_buffers _state_compare model model_new assert_fn state_generator= buffers check_fp tensor model_new parameters assertEqual tensor dtype torch float _get_simple_nested_model fsdp_args wrap=True checkpoint_wrap=False fsdp_kwargs wrap lin = nn Linear bias=False device_type lin = nn Linear bias=False device_type checkpoint_wrap lin = checkpoint_wrapper lin lin = checkpoint_wrapper lin seq = nn Sequential FSDP lin fsdp_args fsdp_kwargs lin checkpoint_wrap seq = checkpoint_wrapper seq model = FSDP seq fsdp_args fsdp_kwargs model = nn Sequential nn Linear bias=False device_type nn Linear bias=False device_type model _get_simple_model fsdp_args checkpoint_wrap=False fsdp_kwargs lin = nn Linear bias=False device_type checkpoint_wrap lin = checkpoint_wrapper lin model = FSDP lin fsdp_args fsdp_kwargs model _get_multibuffer_nested_model fsdp_args wrap=True checkpoint_wrap=False fsdp_kwargs full_p = torch float lin_mp = fsdp_kwargs pop mixed_precision None bn_mp = MixedPrecision param_dtype=full_p reduce_dtype=full_p buffer_dtype=full_p lin_mp None wrap lin = nn Linear bias=False device_type bn = nn BatchNorm d device_type lin = nn Linear bias=False device_type checkpoint_wrap lin = checkpoint_wrapper lin bn = checkpoint_wrapper bn lin = checkpoint_wrapper lin seq = nn Sequential FSDP lin fsdp_args mixed_precision=lin_mp fsdp_kwargs FSDP bn fsdp_args mixed_precision=bn_mp fsdp_kwargs lin checkpoint_wrap seq = checkpoint_wrapper seq model = FSDP seq fsdp_args fsdp_kwargs model = nn Sequential nn Linear bias=False device_type nn BatchNorm d device_type nn Linear bias=False device_type model _get_non_fsdp_root_module fsdp_args wrap=True fsdp_kwargs FSDPContainer nn Module __init__ fsdp_ fsdp_ super __init__ non_fsdp_lin = nn Linear bias=False device_type fsdp_ = fsdp_ fsdp_ = fsdp_ forward x x = non_fsdp_lin x x = fsdp_ x x = fsdp_ x x FSDPContainer _get_simple_nested_model fsdp_args wrap=wrap fsdp_kwargs _get_simple_nested_model fsdp_args wrap=wrap fsdp_kwargs _get_state_dict_mgr model nn Module state_dict_type str state_dict_rank _and_offload bool _state_dict_type = STATE_DICT_MAPPING state_dict_type state_dict_type == state_dict config = FullStateDictConfig rank _only=state_dict_rank _and_offload offload_to_cpu=state_dict_rank _and_offload state_dict_type == local_state_dict config = LocalStateDictConfig offload_to_cpu=state_dict_rank _and_offload state_dict_type == sharded_state_dict config = ShardedStateDictConfig offload_to_cpu=state_dict_rank _and_offload raise ValueError Unsupported state_dict_type FSDP state_dict_type model _state_dict_type config _validate_state_dict_contents model fsdp_state_dict state_dict_rank _and_offload ignore_keys=None state_dict_rank _and_offload rank == assertNotEqual fsdp_state_dict key tensor fsdp_state_dict items ignore_keys key ignore_keys continue assertEqual tensor device torch device cpu f key unexpectedly device tensor device For non-FSDP roots non FSDP portion can still have parameters rank so bypass check now isinstance model FSDP assertEqual fsdp_state_dict f Expected empty state_dict got fsdp_state_dict rank dist get_rank skip_if_lt_x_gpu parametrize state_dict_type _UNFLATTENED_STATE_DICT_IMPLS parametrize checkpoint_wrap source dest both source_after_wrap both_after_wrap parametrize rank _only_and_offload False True test_fsdp_state_dict_with_activation_checkpoint state_dict_type checkpoint_wrap rank _only_and_offload Tests saving state dict zeroing target model s parameters loading state dict where source target models may have checkpoint wrapper apply_ac_to_linears model - None non_reentrant_wrapper = partial checkpoint_wrapper offload_to_cpu=False checkpoint_impl=CheckpointImpl NO_REENTRANT apply_activation_checkpointing model checkpoint_wrapper_fn=non_reentrant_wrapper check_fn=lambda submodule isinstance submodule nn Linear model_call partial _get_simple_model partial _get_simple_nested_model model = model_call checkpoint_wrap= checkpoint_wrap source both checkpoint_wrap source_after_wrap both_after_wrap apply_ac_to_linears model _get_state_dict_mgr model state_dict_type rank _only_and_offload state_dict = _gather_state_dict _get_state_dict model False False Possibly wrap new model activation checkpoint wrapper test save load wrapper model_new = model_call checkpoint_wrap= checkpoint_wrap dest both checkpoint_wrap == both_after_wrap apply_ac_to_linears model_new _zero_model model_new _compare_models model model_new assertNotEqual rank _only_and_offload state_dict = _broadcast_state_dict state_dict Would fail checkpoint_wrapper did correctly implement state_dict pre post hooks model_new load_state_dict state_dict strict=True _compare_models model model_new assertEqual skip_if_lt_x_gpu parametrize state_dict_type _UNFLATTENED_STATE_DICT_IMPLS parametrize rank _only_and_offload False True test_state_dict_with_manual_ac_wrapper state_dict_type str rank _only_and_offload bool Tests saving loading state dict model manually wrapped ` ` FSDP CheckpointWrapper module ` ` where ` ` CheckpointWrapper ` ` wrapped before FSDP TODO Investigate why test above does cover everything test de-duplicate afterwards state_dict_type == sharded_state_dict rank _only_and_offload supported model_ac = TransformerWithSharedParams init process_group FSDPInitMode NO_FSDP DEVICEInitMode DEVICE_BEFORE Manually wrap FSDP without AC model_no_ac = deepcopy model_ac i layer enumerate model_no_ac transformer encoder layers model_no_ac transformer encoder layers i = FSDP layer i layer enumerate model_no_ac transformer decoder layers model_no_ac transformer decoder layers i = FSDP layer model_no_ac transformer = FSDP model_no_ac transformer Manually wrap FSDP AC ` FSDP CheckpointWrapper module ` i layer enumerate model_ac transformer encoder layers layer = checkpoint_wrapper layer model_ac transformer encoder layers i = FSDP layer i layer enumerate model_ac transformer decoder layers layer = checkpoint_wrapper layer model_ac transformer decoder layers i = FSDP layer model_ac transformer = FSDP model_ac transformer Save load compare two models _get_state_dict_mgr model_no_ac state_dict_type rank _only_and_offload state_dict_no_ac = model_no_ac state_dict _get_state_dict_mgr model_ac state_dict_type rank _only_and_offload state_dict_ac = model_ac state_dict assertEqual state_dict_ac keys state_dict_no_ac keys rank _only_and_offload state_dict_no_ac = _broadcast_state_dict state_dict_no_ac state_dict_ac = _broadcast_state_dict state_dict_ac _get_state_dict_mgr model_no_ac state_dict_type rank _only_and_offload model_no_ac load_state_dict state_dict_no_ac _get_state_dict_mgr model_ac state_dict_type rank _only_and_offload model_ac load_state_dict state_dict_ac _compare_models model_ac model_no_ac assertEqual skip_if_lt_x_gpu parametrize state_dict_type _SUPPORTED_STATE_DICT_IMPLS test_state_dict_with_shared_parameters state_dict_type auto_wrap_policy = ModuleWrapPolicy TransformerEncoderLayer TransformerDecoderLayer model_creator = partial TransformerWithSharedParams init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE auto_wrap_policy auto_wrap_policy fsdp_model = model_creator _get_state_dict_mgr fsdp_model state_dict_type False state_dict = fsdp_model state_dict new_model = model_creator _zero_model new_model zero_buffers=True _get_state_dict_mgr new_model state_dict_type False new_model load_state_dict state_dict skip_if_lt_x_gpu parametrize use_orig_params False True test_state_dict_rank _offload_save_load_flow use_orig_params bool Tests saving model checkpoint only rank loading only rank ` ` sync_module_states=True ` ` emulate workflow avoid redundant CPU memory usage auto_wrap_policy = ModuleWrapPolicy TransformerEncoderLayer TransformerDecoderLayer fsdp_kwargs = auto_wrap_policy auto_wrap_policy use_orig_params use_orig_params fsdp_model = TransformerWithSharedParams init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE fsdp_kwargs Force model parameters buffers nonzero FSDP summon_full_params fsdp_model tensor itertools chain fsdp_model parameters fsdp_model buffers torch count_nonzero tensor == torch no_grad tensor add_ torch ones_like tensor _get_state_dict_mgr fsdp_model state_dict True state_dict = deepcopy _get_state_dict fsdp_model Initialize non-wrapped model all ranks new_model = TransformerWithSharedParams init process_group FSDPInitMode NO_FSDP DEVICEInitMode DEVICE_BEFORE _zero_model new_model zero_buffers=True Only load checkpoint rank rank == new_model load_state_dict state_dict strict=True _assert_module_states new_model process_group=self process_group assert_fn=self assertNotEqual Broadcast module states rank ` sync_module_states=True ` new_fsdp_model = FSDP new_model device_id=torch accelerator current_device_index auto_wrap_policy=auto_wrap_policy sync_module_states=True Check FSDP models equal across ranks FSDP summon_full_params new_fsdp_model _assert_module_states new_fsdp_model process_group=self process_group assert_fn=self assertEqual Check FSDP models correctly loaded checkpoint FSDP summon_full_params fsdp_model FSDP summon_full_params new_fsdp_model params = list fsdp_model parameters params_new = list new_fsdp_model parameters assertEqual params params_new skip_if_lt_x_gpu parametrize state_dict_type _SUPPORTED_STATE_DICT_IMPLS parametrize cpu_offload CPUOffload offload_params=True CPUOffload offload_params=False parametrize fp True False parametrize state_dict_rank _and_offload True False parametrize use_orig_params True False test_basic_save_and_load_state_dict state_dict_type str cpu_offload bool fp bool state_dict_rank _and_offload bool use_orig_params bool Tests we can save state_dict load into blank model various configs such fp cpu offload parameters match expected state_dict_rank _and_offload state_dict_type = state_dict use_orig_params state_dict_type _UNFLATTENED_STATE_DICT_IMPLS supported device = torch device rank model_call partial _get_non_fsdp_root_module cpu_offload=cpu_offload use_orig_params=use_orig_params partial _get_simple_nested_model cpu_offload=cpu_offload use_orig_params=use_orig_params partial _get_simple_model cpu_offload=cpu_offload use_orig_params=use_orig_params model = model_call fp model half Run forward backward compute gradients test case where there gradients populated inp = torch randn device=device fp inp = inp half model inp sum backward ctx = _get_state_dict_mgr model state_dict_type state_dict_rank _and_offload ctx fsdp_state_dict = _get_state_dict model cpu_offload offload_params fp ignore_keys = k k fsdp_state_dict keys NON_ROOT_FSDP_PREFIX k _validate_state_dict_contents model fsdp_state_dict state_dict_rank _and_offload ignore_keys=ignore_keys fp Verify fp type tensor fsdp_state_dict values assertEqual tensor dtype torch float model_new = model_call cpu_offload offload_params model_new = model_new device_type fp model_new half Run forward backward compute gradients test case where there gradients populated inp = torch randn device=device fp inp = inp half model_new inp sum backward zero model ensure parameters different _zero_model model_new zero_buffers=True _compare_models model model_new assertNotEqual Verify parameters same new model state_dict_rank _and_offload fsdp_state_dict = _broadcast_state_dict fsdp_state_dict FSDP state_dict_type model_new STATE_DICT_MAPPING state_dict_type model_new load_state_dict fsdp_state_dict strict=True _compare_models model model_new assertEqual check_fp =fp skip_if_lt_x_gpu parametrize state_dict_type _SUPPORTED_STATE_DICT_IMPLS parametrize cpu_offload CPUOffload offload_params=True CPUOffload offload_params=False parametrize mixed_precision True False parametrize state_dict_rank _and_offload True False parametrize use_orig_params True False test_buffers_save_and_load_state_dict state_dict_type str cpu_offload bool mixed_precision bool state_dict_rank _and_offload bool use_orig_params bool Tests we can save state_dict load modules persistent buffers including context non-default mixed precision different ` ` state_dict_type ` ` s CPU offloading state_dict_rank _and_offload state_dict_type = state_dict use_orig_params state_dict_type _UNFLATTENED_STATE_DICT_IMPLS supported mixed_precision = MixedPrecision param_dtype=torch float reduce_dtype=torch float buffer_dtype=torch float mixed_precision None model_call = partial _get_multibuffer_nested_model cpu_offload=cpu_offload use_orig_params=use_orig_params mixed_precision=mixed_precision model = model_call ctx = _get_state_dict_mgr model state_dict_type state_dict_rank _and_offload ctx fsdp_state_dict = _get_state_dict model cpu_offload offload_params False _validate_state_dict_contents model fsdp_state_dict state_dict_rank _and_offload model_new = model_call cpu_offload offload_params model_new = model_new device_type zero model ensure parameters different _zero_model model_new zero_buffers=True _compare_models model model_new assertNotEqual Verify parameters same new model state_dict_rank _and_offload fsdp_state_dict = _broadcast_state_dict fsdp_state_dict FSDP state_dict_type model_new STATE_DICT_MAPPING state_dict_type model_new load_state_dict fsdp_state_dict strict=True _compare_models model model_new assertEqual skip_if_lt_x_gpu parametrize state_dict_type _SUPPORTED_STATE_DICT_IMPLS parametrize mixed_precision True False parametrize state_dict_rank _and_offload True False test_save_and_load_after_forward_state_dict state_dict_type mixed_precision state_dict_rank _and_offload Test saving after some training results params being updated expected state_dict_rank _and_offload state_dict_type = state_dict torch accelerator set_device_index rank mixed_precision = MixedPrecision param_dtype=torch float reduce_dtype=torch float buffer_dtype=torch float mixed_precision None model = _get_simple_nested_model mixed_precision=mixed_precision optim = torch optim SGD model parameters lr= initial_params = get_full_params model _ range inp = torch randn device=torch accelerator current_device_index output = model inp loss = output sum expected_dtype = torch float mixed_precision None torch float assertEqual expected_dtype loss dtype loss backward optim step trained_params = get_full_params model Ensure some training occurred assertNotEqual initial_params trained_params Save copy state_dict fsd_mgr = _get_state_dict_mgr model state_dict_type state_dict_rank _and_offload fsd_mgr state_dict = model state_dict state_dict_type == state_dict state_dict = k v clone k v state_dict items sharded_tensor state_dict values shard = sharded_tensor _local_shards shard tensor = shard tensor clone detach_ _validate_state_dict_contents model state_dict state_dict_rank _and_offload _zero_model model Ensure checkpointed params have full param dtype tensor state_dict values assertEqual tensor dtype torch float Load state_dict into zeroed model state_dict_rank _and_offload state_dict = _broadcast_state_dict state_dict FSDP state_dict_type model STATE_DICT_MAPPING state_dict_type model load_state_dict state_dict strict=True loaded_params = get_full_params model assertEqual loaded_params trained_params _initialize_model wrap_fsdp bool wrap_ddp bool = True register_buffers bool = False keep everything deterministic input data torch manual_seed model = Model wrap_fsdp register_buffers=register_buffers device_type wrap_fsdp model = FSDP model wrap_ddp model = DistributedDataParallel model device_ids= rank model staticmethod _state_dict model Module state_dict_type str try enum_val = STATE_DICT_MAPPING state_dict_type except KeyError e raise ValueError f No state_dict type state_dict_type e FSDP state_dict_type model enum_val model state_dict staticmethod _load_state_dict model Module state_dict_type str state_dict dict str Any try enum_val = STATE_DICT_MAPPING state_dict_type except KeyError e raise ValueError f No state_dict state_dict_type e FSDP state_dict_type model enum_val model load_state_dict state_dict strict=True _dist_train wrap_fsdp bool state_dict_type str = move_to_cpu bool = False TODO Move test common_fsdp model = _initialize_model wrap_fsdp optim = SGD model parameters lr= in_data = torch rand requires_grad=True device=torch device device_type _ range out = model in_data out sum backward optim step optim zero_grad wrap_fsdp blank_model = FSDP Model True device_type _zero_model blank_model state_dict = _state_dict model state_dict_type move_to_cpu key list state_dict keys tensor = state_dict key isinstance tensor torch Tensor state_dict key = tensor cpu shards = tensor local_shards shards shards tensor = shards tensor cpu _load_state_dict blank_model state_dict_type state_dict get_full_params blank_model list model parameters skip_if_lt_x_gpu parametrize state_dict_type _SUPPORTED_STATE_DICT_IMPLS test_state_dict_save_load_flow state_dict_type run_subtests move_to_cpu True False _test_state_dict_save_load_flow state_dict_type=state_dict_type _test_state_dict_save_load_flow state_dict_type move_to_cpu fsdp_params = _dist_train wrap_fsdp=True state_dict_type=state_dict_type move_to_cpu=move_to_cpu ddp_params = _dist_train wrap_fsdp=False assertEqual ddp_params fsdp_params skip_if_lt_x_gpu parametrize state_dict_type _SUPPORTED_STATE_DICT_IMPLS test_fsdp_state_dict_keys state_dict_type state_dict = _state_dict _initialize_model True state_dict_type state_dict_type == local_state_dict assertEqual FLAT_PARAM f inner FLAT_PARAM state_dict keys state_dict_type state_dict sharded_state_dict Keys should match local model local_model = _initialize_model wrap_fsdp=False wrap_ddp=False local_keys = local_model state_dict keys assertEqual state_dict keys local_keys raise NotImplementedError f No test state_dict_type skip_if_lt_x_gpu parametrize state_dict_type _UNFLATTENED_STATE_DICT_IMPLS parametrize state_dict_rank _and_offload True False parametrize fsdp_root True False test_state_dict_load_into_local_module state_dict_type state_dict_rank _and_offload fsdp_root Tests FSDP s state_dict can loaded into local model state_dict_rank _and_offload state_dict_type = state_dict fsdp_root model = _get_non_fsdp_root_module model = _initialize_model wrap_fsdp=True register_buffers=True optim = SGD model parameters lr= fsdp_root in_data = torch randn requires_grad=True device=torch device device_type in_data = torch rand requires_grad=True device=torch device device_type _ range out = model in_data out sum backward optim step optim zero_grad FSDP summon_full_params model fsdp_params = deepcopy list model parameters get FSDP state_dict Note default we full_state_dict sd_mgr = _get_state_dict_mgr model state_dict_type state_dict_rank _and_offload sd_mgr fsdp_state_dict = model state_dict ignore_keys = k k fsdp_state_dict keys NON_ROOT_FSDP_PREFIX k _validate_state_dict_contents model fsdp_state_dict state_dict_rank _and_offload ignore_keys=ignore_keys Create zeroed local model fsdp_root blank_local_model = _get_non_fsdp_root_module wrap=False blank_local_model = _initialize_model wrap_fsdp=False wrap_ddp=False register_buffers=True Nothing should FSDP mod blank_local_model modules assertFalse isinstance mod FSDP param blank_local_model parameters torch no_grad param zero_ fsdp_state_dict = _gather_state_dict fsdp_state_dict Load fsdp s full state dict into local verify params expected state_dict_rank _and_offload fsdp_state_dict = _broadcast_state_dict fsdp_state_dict blank_local_model load_state_dict fsdp_state_dict strict=True local_params = list blank_local_model parameters fsdp_param local_param zip fsdp_params local_params assertEqual fsdp_param local_param skip_if_lt_x_gpu parametrize state_dict_type _SUPPORTED_STATE_DICT_IMPLS parametrize double_nest True test_state_dict_skip_module state_dict_type double_nest torch accelerator set_device_index rank _create_module wrap_fsdp=True LINEAR_SKIP = linear_skip ctx = enable_wrap wrapper_cls=FSDP wrap_fsdp nullcontext ctx module = SkipModel double_nest=double_nest Full name linear_skip param tensors SkipModel would stored checkpoint linear_skip_tensor_names = k k dict module named_parameters keys LINEAR_SKIP k skip SkipModule linear_skip = getattr module LINEAR_SKIP delattr module LINEAR_SKIP Wrap FSDP fsdp = wrap module reattach setattr module LINEAR_SKIP linear_skip fsdp linear_skip_tensor_names fsdp _ = _create_module Run forward pass inp = torch randn device=torch accelerator current_device_index loss = fsdp inp loss sum backward FSDP state_dict_type fsdp STATE_DICT_MAPPING state_dict_type state_dict = fsdp state_dict rank == state_dict_type = local_state_dict sd_keys = list state_dict keys expected = list SkipModel double_nest=False state_dict keys assertEqual sorted sd_keys sorted expected TODO parameters linear_skip_tensor_names should handled FSDP state_dict Have check once implemented FSDP state_dict Check can loaded into FSDP new_fsdp _ = _create_module _zero_model new_fsdp p p zip fsdp parameters new_fsdp parameters assertNotEqual p p FSDP state_dict_type new_fsdp STATE_DICT_MAPPING state_dict_type state_dict_type = local_state_dict FlatParameter has supported deepcopy yet state_dict = deepcopy state_dict new_fsdp load_state_dict state_dict strict=True p p zip fsdp parameters new_fsdp parameters assertEqual p p Test checkpoint can loaded into local model local _ = _create_module wrap_fsdp=False param local parameters torch no_grad param zero_ fsdp summon_full_params fsdp p p zip fsdp parameters local parameters assertNotEqual p p state_dict_type == local_state_dict state_dict = _gather_state_dict state_dict fsdp summon_full_params fsdp rank == local load_state_dict state_dict strict=True p p zip fsdp parameters local parameters assertEqual p p skip_if_lt_x_gpu test_wrong_state_dict_config model = FSDP Model wrap_fsdp=True device_type assertRaisesRegex RuntimeError Expected state_dict_config type model state_dict_type model StateDictType FULL_STATE_DICT LocalStateDictConfig pass skip_if_lt_x_gpu parametrize state_dict_type _UNFLATTENED_STATE_DICT_IMPLS parametrize prefix True False parametrize ignore_inner True False parametrize mixed_precision True False test_state_dict_with_ignored_modules state_dict_type prefix ignore_inner mixed_precision Initialize FSDP-wrapped model ignored module includes both parameters buffer model = Model wrap_fsdp=True register_buffers=True ignore_inner=ignore_inner mixed_precision=mixed_precision device_type ignored_modules = model outer ignored_tensor_to_tensor_name = model outer bias outer bias model outer weight outer weight ignore_inner ignored_tensor_to_tensor_name = ignored_tensor_to_tensor_name model inner bias inner bias model inner weight inner weight Note when model inner ignored test also ensures non-ignored buffers cloned buffer_to_buffer_name = model inner buffer inner buffer model outer buffer outer buffer expect fp model inner buffer mixed_precisions expect fp sd inner buffer after restoring original precision so skip AssertEqual mixed_precision ignore_inner buffer_to_buffer_name pop model inner buffer fsdp_model = FSDP model ignored_modules=ignored_modules mixed_precision=MixedPrecision param_dtype=torch float reduce_dtype=torch float buffer_dtype=torch float mixed_precision None prefix_str = foo prefix FSDP state_dict_type fsdp_model STATE_DICT_MAPPING state_dict_type sd = _gather_state_dict fsdp_model state_dict prefix=prefix_str FSDP summon_full_params fsdp_model fsdp_params = deepcopy list fsdp_model parameters Check ignored parameters all buffers cloned tensor tensor_name ignored_tensor_to_tensor_name buffer_to_buffer_name items prefixed_tensor_name = f prefix_str tensor_name assertTrue prefixed_tensor_name sd assertEqual tensor data_ptr sd prefixed_tensor_name data_ptr f prefixed_tensor_name should apply mixed_precision ignored buffers buffer_name buffer_to_buffer_name values prefixed_buffer_name = f prefix_str buffer_name assertTrue prefixed_buffer_name sd assertEqual sd prefixed_buffer_name dtype torch float Check state dict can loaded into non-wrapped version model nonwrapped_model = Model wrap_fsdp=False register_buffers=True device_type param nonwrapped_model parameters torch no_grad param zero_ to_load = k len prefix_str v k v sd items nonwrapped_model load_state_dict to_load strict=True local_params = list nonwrapped_model parameters fsdp_param local_param zip fsdp_params local_params assertEqual fsdp_param local_param Check we save state dict again ignored parameters buffer still have same data pointer FSDP state_dict_type fsdp_model STATE_DICT_MAPPING state_dict_type sd = fsdp_model state_dict prefix=prefix_str tensor tensor_name ignored_tensor_to_tensor_name buffer_to_buffer_name items prefixed_tensor_name = f prefix_str tensor_name assertTrue prefixed_tensor_name sd assertEqual tensor data_ptr sd prefixed_tensor_name data_ptr assertEqual sd prefixed_tensor_name data_ptr sd prefixed_tensor_name data_ptr skip_if_lt_x_gpu test_state_dict_type module = SkipModel double_nest=True enable_wrap wrapper_cls=FSDP fsdp = wrap module FSDP state_dict_type fsdp StateDictType LOCAL_STATE_DICT pass module FSDP fsdp_modules fsdp assertEqual module _state_dict_type StateDictType FULL_STATE_DICT skip_if_lt_x_gpu test_local_state_dict_with_empty_ranks Model Module __init__ - None super __init__ my_tensor = torch full my_parameter = nn Parameter my_tensor forward x my_parameter model = FSDP Model device_type FSDP state_dict_type model StateDictType LOCAL_STATE_DICT out = model None out backward state_dict = deepcopy model state_dict torch no_grad FSDP summon_full_params model assertEqual model my_parameter item model my_parameter copy_ torch full device_type assertEqual model my_parameter item model load_state_dict state_dict FSDP summon_full_params model assertEqual model my_parameter item skip_if_lt_x_gpu test_torch_save_load model = Model wrap_fsdp=True device_type FSDP state_dict_type model StateDictType LOCAL_STATE_DICT state_dict = model state_dict checkpoint = io BytesIO torch save state_dict checkpoint checkpoint seek torch serialization safe_globals Shard ShardMetadata ShardedTensor ShardedTensorMetadata TensorProperties MEM_FORMAT_ENCODING _remote_device getattr ShardedTensor ProcessGroupState ChunkShardingSpec state_dict_saved = torch load checkpoint k v state_dict_saved items isinstance v ShardedTensor assertEqual v _local_shards tensor state_dict k _local_shards tensor assertEqual v state_dict k skip_if_lt_x_gpu test_shared_module_and_shared_parameter model = FSDP TestDummyModel device_type FSDP state_dict_type model StateDictType FULL_STATE_DICT state_dict = model state_dict assertEqual state_dict random_parameter state_dict shared_parameter assertEqual state_dict net bias state_dict net bias assertEqual state_dict net weight state_dict net weight skip_if_lt_x_gpu test_full_state_dict_missing_unexpected_keys_cleaned model = _get_simple_nested_model sd = model state_dict Create missing key sd pop next iter sd keys Create unexpected key sd unexpected = torch ones missing unexpected = model load_state_dict sd strict=False assert len missing == assert len unexpected == assertTrue FSDP_PREFIX missing assertTrue FSDP_PREFIX unexpected skip_if_lt_x_gpu test_sharded_load_multi_backend_pg auto_wrap_policy = ModuleWrapPolicy TransformerEncoderLayer TransformerDecoderLayer fsdp_kwargs = auto_wrap_policy auto_wrap_policy use_orig_params True load_cpu True False subTest load_cpu=load_cpu backend = torch distributed get_default_backend_for_device device_type pg = dist new_group backend=f cpu gloo device_type backend fsdp_model = TransformerWithSharedParams init pg FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE fsdp_kwargs FSDP set_state_dict_type fsdp_model StateDictType SHARDED_STATE_DICT sharded = fsdp_model state_dict param_copy = t clone detach_ t fsdp_model parameters torch no_grad p fsdp_model parameters p zero_ load_cpu Offload CPU simulate CPU state_dict load k v sharded items sharded k = v cpu fsdp_model load_state_dict sharded p p zip param_copy fsdp_model parameters assertEqual p p f equal p sum vs p sum skip_if_lt_x_gpu test_world_size_one my_pg = None i range world_size pg = dist new_group ranks= i i == rank my_pg = pg model = TransformerWithSharedParams init my_pg FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE FSDP state_dict_type model StateDictType SHARDED_STATE_DICT state_dict = model state_dict model load_state_dict state_dict dist barrier TestFSDPStateDict GPUs FSDPTest property world_size torch accelerator device_count skip_if_lt_x_gpu test_local_state_dict_reshard This test demonstrates ability do resharding when using local_state_dict Although we do recommend users use local_state_dict there still some corner cases using local_state_dict better solution model = FSDP Model wrap_fsdp=True device_type optim = torch optim SGD model parameters lr= batch = torch randn device=torch accelerator current_device_index output = model batch loss = output sum loss backward optim step FSDP state_dict_type model StateDictType LOCAL_STATE_DICT state_dict = model state_dict rank = dist get_rank new_pg = dist new_group ranks= resharded_state_dict = Mimic resharding GPUs GPUs key value state_dict items isinstance value ShardedTensor full_flat_param = _all_gather_sharded_tensor value rank full_numel = full_flat_param size chunks = full_flat_param chunk flat_param = chunks rank shard_offset = rank == chunks numel local_shards = Shard from_tensor_and_offsets flat_param shard_offset rank sharded_tensor = init_from_local_shards local_shards full_numel process_group=new_pg resharded_state_dict key = sharded_tensor rank resharded_state_dict key = value rank model = FSDP Model wrap_fsdp=True process_group=new_pg process_group=new_pg device_type FSDP state_dict_type model StateDictType LOCAL_STATE_DICT model load_state_dict resharded_state_dict FSDP state_dict_type model StateDictType FULL_STATE_DICT full_state_dict = model state_dict rank FSDP state_dict_type model StateDictType FULL_STATE_DICT full_state_dict = model state_dict assertEqual full_state_dict full_state_dict instantiate_parametrized_tests TestFSDPStateDict __name__ == __main__ run_tests