mypy ignore-errors torch unittest copy deepcopy enum Enum functools wraps partial itertools chain product itertools math torch nn functional F torch nn utils rnn pack_padded_sequence torch testing make_tensor torch testing _internal common_cuda TEST_CUDNN torch testing _internal common_dtype floating_types floating_and_complex_types_and get_all_fp_dtypes torch testing _internal common_device_type _TestParametrizer _update_param_kwargs expectedFailureMPS toleranceOverride tol precisionOverride skipMeta skipMPS torch testing _internal common_methods_invocations DecorateInfo torch testing _internal common_nn cosineembeddingloss_reference cross_entropy_loss_reference ctcloss_reference hingeembeddingloss_reference huberloss_reference kldivloss_reference marginrankingloss_reference multimarginloss_reference multilabelmarginloss_reference nllloss_reference nlllossNd_reference smoothl loss_reference softmarginloss_reference get_reduction torch testing _internal common_utils freeze_rng_state skipIfMPS GRADCHECK_NONDET_TOL TEST_WITH_ROCM IS_WINDOWS skipIfTorchDynamo types ModuleType operator List all namespaces containing modules test MODULE_NAMESPACES list ModuleType = torch nn modules torch ao nn qat modules torch ao nn quantizable modules torch ao nn quantized modules torch ao nn quantized modules Modules shouldn t tested one reason another MODULES_TO_SKIP set type = torch nn Module abstract base torch nn Container deprecated torch nn NLLLoss d deprecated torch ao nn quantized MaxPool d aliases nn MaxPool d torch ao nn quantized MaxPool d aliases nn MaxPool d List all module classes test MODULE_CLASSES list type = chain from_iterable getattr namespace module_name module_name namespace __all__ type ignore attr-defined namespace MODULE_NAMESPACES MODULE_CLASSES = cls cls MODULE_CLASSES cls MODULES_TO_SKIP Dict module - common name Useful making test names more intuitive Example torch nn modules linear Linear - nn Linear MODULE_CLASS_NAMES dict type str = namespace MODULE_NAMESPACES module_name namespace __all__ type ignore attr-defined module_cls = getattr namespace module_name namespace_name = namespace __name__ replace torch replace modules Deal any aliases preferring earlier names module_cls MODULE_CLASS_NAMES MODULE_CLASS_NAMES module_cls = f namespace_name module_name Specifies modes i e train eval test over TrainEvalMode = Enum TrainEvalMode train_only eval_only train_and_eval modules _TestParametrizer PROTOTYPE Decorator specifying list modules over which run test __init__ module_info_iterable allowed_dtypes=None train_eval_mode=TrainEvalMode train_and_eval skip_if_dynamo=True module_info_list = list module_info_iterable allowed_dtypes = set allowed_dtypes allowed_dtypes None None train_eval_mode = train_eval_mode skip_if_dynamo = skip_if_dynamo _get_training_flags module_info training_flags = train_eval_mode == TrainEvalMode train_only train_eval_mode == TrainEvalMode train_and_eval training_flags append True train_eval_mode == TrainEvalMode eval_only train_eval_mode == TrainEvalMode train_and_eval training_flags append False If train eval modes don t differ module don t bother using more than one module_info train_and_eval_differ training_flags = training_flags training_flags _parametrize_test test generic_cls device_cls device_cls None raise RuntimeError The modules decorator only intended used device-specific context use instantiate_device_type_tests instead instantiate_parametrized_tests module_info module_info_list dtypes = set module_info supported_dtypes device_cls device_type allowed_dtypes None dtypes = dtypes intersection allowed_dtypes training_flags = _get_training_flags module_info training dtype product training_flags dtypes Construct test name device dtype parts handled outside See Note device dtype suffix placement test_name = module_info formatted_name len training_flags test_name += f _ train_mode training eval_mode Construct parameter kwargs pass test param_kwargs = module_info module_info _update_param_kwargs param_kwargs dtype dtype _update_param_kwargs param_kwargs training training try wraps test test_wrapper args kwargs test args kwargs skip_if_dynamo torch testing _internal common_utils TEST_WITH_TORCHINDUCTOR test_wrapper = skipIfTorchDynamo Policy we don t run ModuleInfo tests w Dynamo test_wrapper decorator_fn = partial module_info get_decorators generic_cls __name__ test __name__ device_cls device_type dtype yield test_wrapper test_name param_kwargs decorator_fn except Exception ex Provides error message debugging before rethrowing exception print f Failed instantiate test_name module module_info name raise ex get_module_common_name module_cls module_cls MODULE_CLASS_NAMES Example nn Linear MODULE_CLASS_NAMES module_cls module_cls __name__ FunctionInput Contains args kwargs pass input function __slots__ = args kwargs __init__ args kwargs args = args kwargs = kwargs ModuleInput Contains args kwargs module instantiation + forward pass __slots__ = constructor_input forward_input desc reference_fn __init__ constructor_input forward_input=None desc= reference_fn=None constructor_input = constructor_input Inputs pass during construction forward_input = forward_input Inputs pass forward desc = desc Description set inputs reference_fn = reference_fn Reference signature reference_fn module parameters args kwargs reference_fn None wraps reference_fn copy_reference_fn m args kwargs Copy inputs avoid undesired side effects calling reference args kwargs = deepcopy args deepcopy kwargs Note module parameters passed convenience reference_fn m list m parameters args kwargs reference_fn = copy_reference_fn ModuleErrorEnum Enum Enumerates when error raised when testing modules CONSTRUCTION_ERROR = FORWARD_ERROR = ErrorModuleInput A ModuleInput will cause operation throw error plus information about resulting error __slots__ = module_error_input error_on error_type error_regex __init__ module_error_input error_on=ModuleErrorEnum CONSTRUCTION_ERROR error_type=RuntimeError error_regex module_error_input = module_error_input error_on = error_on error_type = error_type error_regex = error_regex ModuleInfo Module information used testing __init__ module_cls Class object module under test module_inputs_func Function generate module inputs skips= Indicates which tests skip decorators=None Additional decorators apply generated tests dtypes=floating_types dtypes function expected work dtypesIfMPS= torch float torch float dtypes function expected work MPS dtypesIfHpu= torch bfloat torch float supports_gradgrad=True whether op supports second order gradients gradcheck_nondet_tol= tolerance nondeterminism while performing gradcheck module_memformat_affects_out=False whether converting module channels last will generate channels last output train_and_eval_differ=False whether module has differing behavior between train eval module_error_inputs_func=None Function generate module inputs error gradcheck_fast_mode=None Whether use fast implementation gradcheck gradgradcheck When set None defers default value provided wrapper function around gradcheck testing _internal common_utils gradcheck module_cls = module_cls module_inputs_func = module_inputs_func decorators = decorators decorators skips skips dtypes = dtypes dtypesIfMPS = dtypesIfMPS dtypesIfHpu = dtypesIfHpu supports_gradgrad = supports_gradgrad gradcheck_nondet_tol = gradcheck_nondet_tol module_memformat_affects_out = module_memformat_affects_out train_and_eval_differ = train_and_eval_differ module_error_inputs_func = module_error_inputs_func gradcheck_fast_mode = gradcheck_fast_mode is_lazy = issubclass module_cls torch nn modules lazy LazyModuleMixin get_decorators test_class test_name device dtype param_kwargs result = decorator decorators isinstance decorator DecorateInfo decorator is_active test_class test_name device dtype param_kwargs result extend decorator decorators result append decorator result supported_dtypes device_type device_type == mps dtypesIfMPS device_type == hpu dtypesIfHpu dtypes property name get_module_common_name module_cls property formatted_name name replace _ Start module inputs functions module_inputs_torch_nn_Linear module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad module_inputs = ModuleInput constructor_input=FunctionInput forward_input=FunctionInput input=make_input reference_fn=lambda m p input torch mm input p t + p view - expand ModuleInput constructor_input=FunctionInput bias=False forward_input=FunctionInput make_input desc= no_bias reference_fn=lambda m p i torch mm i p t ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= no_batch_dim reference_fn=lambda m p i torch mm i view - p t view - + p module_inputs module_inputs_torch_nn_Bilinear module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad bilinear_reference_fn m p x x bias=True result = torch einsum bn anm bm- ba x p x bias x shape == result = result view - + p result = result + p view - expand x shape p shape result module_inputs = ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input make_input reference_fn=bilinear_reference_fn ModuleInput constructor_input=FunctionInput bias=False forward_input=FunctionInput make_input make_input desc= no_bias reference_fn=lambda m p x x bilinear_reference_fn m p x x bias=False ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input make_input desc= no_batch_dim reference_fn=lambda m p x x bilinear_reference_fn m p x view - x view - module_inputs module_inputs_torch_nn_KLDivLoss module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad cases list tuple str dict = reduction_sum reduction sum reduction_batchmean reduction batchmean reduction_none reduction none log_target log_target True module_inputs = desc constructor_kwargs cases reference_fn m p i t constructor_kwargs=constructor_kwargs kldivloss_reference i t constructor_kwargs input = make_input log target = make_input kwargs get log_target False make_input log module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput input target desc=desc reference_fn=reference_fn scalar_input = make_input log FIXME rec scalar_target unused perhaps should argument FunctionInput scalar_target = noqa F make_input kwargs get log_target False make_input log module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput scalar_input scalar_input desc= scalar_ + desc reference_fn=reference_fn module_inputs module_inputs_torch_nn_NLLLoss module_info device dtype requires_grad training kwargs make_input shape device=device dtype=dtype requires_grad=requires_grad make_tensor shape device=device dtype=dtype requires_grad=False log_softmax dim= requires_grad_ requires_grad make_weight = partial make_tensor device=device dtype=dtype requires_grad=False cases list tuple str dict = reduction_sum reduction sum reduction_none reduction none ignore_index ignore_index weights weight make_weight abs weights_ignore_index weight make_weight abs ignore_index weights_ignore_index_neg weight make_weight abs ignore_index - TODO Uncomment when negative weights supported negative_weight = make_weight negative_weight = - cases append weights_negative weight negative_weight module_inputs = desc constructor_kwargs cases reference_fn m p i t constructor_kwargs=constructor_kwargs nllloss_reference i t constructor_kwargs module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input torch empty device=device uniform_ mul floor long desc=desc reference_fn=reference_fn nd_reference_fn m p i t constructor_kwargs=constructor_kwargs nlllossNd_reference i t constructor_kwargs module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input torch empty device=device uniform_ mul floor long desc=f nd_ desc reference_fn=nd_reference_fn module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input torch empty device=device uniform_ mul floor long desc=f higher_dim_ desc reference_fn=nd_reference_fn module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input torch empty device=device uniform_ mul floor long desc=f d_ desc reference_fn=nd_reference_fn module_inputs module_inputs_torch_nn_GaussianNLLLoss module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_target = partial make_tensor device=device dtype=dtype requires_grad=False cases list tuple str dict = reduction_sum reduction sum reduction_mean reduction mean reduction_none reduction none homoscedastic homoscedastic True module_inputs = desc constructor_kwargs cases homoscedastic = constructor_kwargs pop homoscedastic False var_input = make_input abs homoscedastic make_input abs module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input make_target var_input desc=desc reference_fn=no_batch_dim_reference_fn module_inputs module_inputs_torch_nn_PoissonNLLLoss module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_target = partial make_tensor device=device dtype=dtype requires_grad=False cases list tuple str dict = reduction_sum reduction sum reduction_mean reduction mean reduction_none reduction none full full True no_log_input log_input False full_no_log_input full True log_input False poissonnllloss_reference_fn i t log_input=True full=False reduction= mean eps= e- log_input result = i exp - t mul i result = i - t mul i + eps log full result += t mul t log - t + math pi t log masked_fill t = reduction == none result reduction == mean result sum i numel result sum module_inputs = desc constructor_kwargs cases reference_fn m p i t constructor_kwargs=constructor_kwargs poissonnllloss_reference_fn i t constructor_kwargs log_input = constructor_kwargs get log_input True input = make_input log_input make_input abs add module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput input make_target floor_ abs_ desc=desc reference_fn=reference_fn module_inputs module_inputs_torch_nn_MSELoss module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_target = partial make_tensor device=device dtype=dtype requires_grad=False cases list tuple str dict = reduction_sum reduction sum reduction_mean reduction mean reduction_none reduction none mse_loss_reference_fn m p i t reduction= mean reduction == none i - t pow reduction == mean i - t pow sum i numel i - t pow sum module_inputs = desc constructor_kwargs cases module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input make_target desc=desc reference_fn=partial mse_loss_reference_fn constructor_kwargs module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input make_target desc=f desc _scalar reference_fn=partial mse_loss_reference_fn constructor_kwargs module_inputs no_batch_dim_reference_fn m p args kwargs Reference function modules supporting no batch dimensions Unbatched inputs unsqueezed form single batch input before passing them module The output squeezed compare output unbatched input module Currently only supports modules which single Tensor output You can bind following kwargs Kwargs batch_first bool If True all Tensors ` args ` while unsqueezed dim ` ` output will squeezed dim ` ` dim ` ` both kwargs_to_batchify dict Dictionary specifying name argument dimension unsqueeze Useful there few arguments whose batch dimension different ones selected ` batch_first ` is_criterion bool Specify module criterion handle reduction output accordingly get_and_pop key default v = kwargs get key default key kwargs kwargs pop key v batch_dim = get_and_pop batch_first True kwargs_to_batchify = get_and_pop kwargs_to_batchify None is_criterion = get_and_pop is_criterion False kwargs_to_batchify None assert isinstance kwargs_to_batchify dict k v kwargs items k kwargs_to_batchify v None bdim = kwargs_to_batchify k kwargs k = v unsqueeze bdim single_batch_input_args = input unsqueeze batch_dim input args freeze_rng_state output = m single_batch_input_args kwargs squeeze batch_dim is_criterion reduction = get_reduction m reduction == none output squeeze output no_batch_dim_reference_mha m p args kwargs Reference function MultiheadAttention supporting no batch dimensions Unbatched inputs unsqueezed form single batch input before passing them module The output squeezed compare output unbatched input module batch_dim = kwargs get batch_first True batch_first kwargs kwargs pop batch_first key_padding_mask kwargs kwargs key_padding_mask None kwargs key_padding_mask = kwargs key_padding_mask unsqueeze single_batch_input_args = input unsqueeze batch_dim input args freeze_rng_state output = m single_batch_input_args kwargs output squeeze batch_dim output squeeze no_batch_dim_reference_rnn_gru m p args kwargs Reference function RNN GRU supporting no batch dimensions Unbatched inputs unsqueezed form single batch input before passing them module The output squeezed compare output unbatched input module len args == inp = args h = None len args == inp h = args h = h unsqueeze batch_dim = kwargs batch_first kwargs pop batch_first inp = inp unsqueeze batch_dim single_batch_input_args = inp h freeze_rng_state output = m single_batch_input_args kwargs output squeeze batch_dim output squeeze no_batch_dim_reference_lstm m p args kwargs Reference function LSTM supporting no batch dimensions Unbatched inputs unsqueezed form single batch input before passing them module The output squeezed compare output unbatched input module len args == inp = args h = None len args == inp h = args h = h unsqueeze h unsqueeze batch_dim = kwargs batch_first kwargs pop batch_first inp = inp unsqueeze batch_dim single_batch_input_args = inp h freeze_rng_state output = m single_batch_input_args kwargs output squeeze batch_dim output squeeze output squeeze no_batch_dim_reference_lstmcell m p args kwargs Reference function LSTMCell supporting no batch dimensions The module passed input target batched form single item The output squeezed compare no-batch input inp h c = args single_batch_input_args = inp unsqueeze h unsqueeze c unsqueeze freeze_rng_state output = m single_batch_input_args kwargs output squeeze output squeeze generate_regression_criterion_inputs make_input ModuleInput constructor_input=FunctionInput reduction=reduction forward_input=FunctionInput make_input make_input reference_fn=partial no_batch_dim_reference_fn is_criterion=True desc=f no_batch_dim_ reduction reduction none mean sum module_inputs_torch_nn_AvgPool d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput kernel_size= forward_input=FunctionInput make_input desc= no_batch_dim reference_fn=no_batch_dim_reference_fn ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= stride ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= stride_pad module_inputs_torch_nn_AvgPool d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= no_batch_dim reference_fn=no_batch_dim_reference_fn ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= stride ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= stride_pad ModuleInput constructor_input=FunctionInput divisor_override= forward_input=FunctionInput make_input desc= divisor ModuleInput constructor_input=FunctionInput divisor_override= forward_input=FunctionInput make_input desc= divisor_stride ModuleInput constructor_input=FunctionInput divisor_override= forward_input=FunctionInput make_input desc= divisor_stride_pad module_inputs_torch_nn_AvgPool d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= no_batch_dim reference_fn=no_batch_dim_reference_fn ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= stride ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= stride_pad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= stride_pad_gpu_fixedkw_output ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= stride_pad_gpu_general_output ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= stride _pad _gpu_input ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= stride_pad_gpu_input_nooverlap ModuleInput constructor_input=FunctionInput divisor_override= forward_input=FunctionInput make_input desc= divisor ModuleInput constructor_input=FunctionInput divisor_override= forward_input=FunctionInput make_input desc= divisor_stride ModuleInput constructor_input=FunctionInput divisor_override= forward_input=FunctionInput make_input desc= divisor_stride_pad ModuleInput constructor_input=FunctionInput divisor_override= forward_input=FunctionInput make_input desc= divisor_stride_pad_gpu_fixedkw_output ModuleInput constructor_input=FunctionInput divisor_override= forward_input=FunctionInput make_input desc= divisor_stride_pad_gpu_general_output ModuleInput constructor_input=FunctionInput divisor_override= forward_input=FunctionInput make_input desc= divisor_stride _pad _gpu_input ModuleInput constructor_input=FunctionInput divisor_override= forward_input=FunctionInput make_input desc= divisor_stride_pad_gpu_input_nooverlap module_inputs_torch_nn_AdaptiveAvgPool d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= single ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= one_output module_inputs_torch_nn_AdaptiveAvgPool d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= single ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= single_ x output ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= tuple ModuleInput constructor_input=FunctionInput None forward_input=FunctionInput make_input desc= tuple_none module_inputs_torch_nn_AdaptiveAvgPool d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= single ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= tuple ModuleInput constructor_input=FunctionInput None forward_input=FunctionInput make_input desc= tuple_none ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= last_dim module_inputs_torch_nn_AdaptiveMaxPool d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= single ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim module_inputs_torch_nn_AdaptiveMaxPool d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= single ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= tuple ModuleInput constructor_input=FunctionInput None forward_input=FunctionInput make_input desc= tuple_none module_inputs_torch_nn_AdaptiveMaxPool d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= single ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= tuple ModuleInput constructor_input=FunctionInput None forward_input=FunctionInput make_input desc= tuple_none ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= single_nonatomic ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= tuple_nonatomic module_inputs_torch_nn_BatchNorm d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= affine ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= d_input ModuleInput constructor_input=FunctionInput e- None forward_input=FunctionInput make_input desc= affine_simple_average ModuleInput constructor_input=FunctionInput e- False forward_input=FunctionInput make_input desc= not_affine ModuleInput constructor_input=FunctionInput e- True False forward_input=FunctionInput make_input desc= not_tracking_stats ModuleInput constructor_input=FunctionInput e- False forward_input=FunctionInput make_input desc= d_input_not_affine ModuleInput constructor_input=FunctionInput e- False forward_input=FunctionInput make_input desc= zero_batch module_inputs_torch_nn_BatchNorm d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input ModuleInput constructor_input=FunctionInput e- None forward_input=FunctionInput make_input desc= d_simple_average ModuleInput constructor_input=FunctionInput e- forward_input=FunctionInput make_input desc= momentum ModuleInput constructor_input=FunctionInput e- False forward_input=FunctionInput make_input desc= not_affine ModuleInput constructor_input=FunctionInput e- True False forward_input=FunctionInput make_input desc= not_tracking_stats ModuleInput constructor_input=FunctionInput e- False forward_input=FunctionInput make_input desc= zero_batch module_inputs_torch_nn_BatchNorm d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input ModuleInput constructor_input=FunctionInput e- None forward_input=FunctionInput make_input desc= d_simple_average ModuleInput constructor_input=FunctionInput e- forward_input=FunctionInput make_input desc= momentum ModuleInput constructor_input=FunctionInput e- False forward_input=FunctionInput make_input desc= not_affine ModuleInput constructor_input=FunctionInput e- True False forward_input=FunctionInput make_input desc= not_tracking_stats ModuleInput constructor_input=FunctionInput e- False forward_input=FunctionInput make_input desc= zero_batch module_inputs_torch_nn_ConvNd module_info device dtype requires_grad training kwargs N = kwargs N lazy = kwargs get lazy False transposed = kwargs get transposed False make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad conv_kwargs_list = transposed padding same kernel_size C_in C_out = input_no_batch_shape = C_in + tuple i + i range N input_batch_shape = + input_no_batch_shape ModuleInput constructor_input= FunctionInput C_out kernel_size conv_kwargs lazy FunctionInput C_in C_out kernel_size conv_kwargs forward_input=FunctionInput make_input input_batch_shape with_batch input_no_batch_shape desc= with_batch no_batch_dim reference_fn= None with_batch no_batch_dim_reference_fn with_batch conv_kwargs itertools product True False conv_kwargs_list module_inputs_torch_nn_CosineEmbeddingLoss module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_target = partial make_tensor device=device dtype=dtype requires_grad=False cases list tuple str dict = reduction_sum reduction sum reduction_mean reduction mean reduction_none reduction none margin margin module_inputs = desc constructor_kwargs cases reference_fn m p i i t constructor_kwargs=constructor_kwargs cosineembeddingloss_reference i i t constructor_kwargs module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input make_input make_target sign desc=desc reference_fn=reference_fn module_inputs module_inputs_torch_nn_ELU module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput alpha= forward_input=FunctionInput make_input reference_fn=lambda m p i torch where i = i i exp - ModuleInput constructor_input=FunctionInput alpha= forward_input=FunctionInput make_input desc= scalar ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= no_batch_dim reference_fn=no_batch_dim_reference_fn ModuleInput constructor_input=FunctionInput alpha= forward_input=FunctionInput make_input desc= d_input module_inputs_torch_nn_CELU module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput alpha= forward_input=FunctionInput make_input reference_fn=lambda m p i torch where i = i i exp - ModuleInput constructor_input=FunctionInput alpha= forward_input=FunctionInput make_input reference_fn=lambda m p i torch where i = i i exp - desc= scalar ModuleInput constructor_input=FunctionInput alpha= forward_input=FunctionInput make_input desc= no_batch_dim reference_fn=no_batch_dim_reference_fn module_inputs_torch_nn_GLU module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= dim ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= no_batch_dim reference_fn=no_batch_dim_reference_fn module_inputs_torch_nn_GELU module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput none forward_input=FunctionInput make_input reference_fn=lambda m p x _ x + torch erf x math sqrt desc= scalar ModuleInput constructor_input=FunctionInput none forward_input=FunctionInput make_input reference_fn=lambda m p x _ x + torch erf x math sqrt ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= no_batch_dim reference_fn=no_batch_dim_reference_fn module_inputs_torch_nn_ReLU module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= scalar ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= channels_last_mem_format ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= channels_last_ d_mem_format module_inputs_torch_nn_ReLU module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= scalar ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= channels_last_mem_format ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= channels_last_ d_mem_format module_inputs_torch_nn_LeakyReLU module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= with_negval ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= with_zero_negval ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= with_negval_scalar module_inputs_torch_nn_PReLU module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= scalar ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i torch clamp i min= + torch clamp i max= p desc= d ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i torch clamp i min= + torch clamp i max= p desc= d_multiparam ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i torch clamp i min= + torch clamp i max= p desc= d ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i torch clamp i min= + torch clamp i max= p desc= d_multiparam ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i torch clamp i min= + torch clamp i max= p desc= d ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i torch clamp i min= + torch clamp i max= p desc= d_multiparam module_inputs_torch_nn_SELU module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= scalar module_inputs_torch_nn_SiLU module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p x _ x torch sigmoid x desc= scalar ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p x _ x torch sigmoid x module_inputs_torch_nn_Softmax module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i torch exp i div torch exp i sum True expand ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i torch exp i div torch exp i sum True desc= scalar ModuleInput constructor_input=FunctionInput - forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim module_inputs_torch_nn_Softmax d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i torch exp i div torch exp i sum False ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim module_inputs_torch_nn_LogSoftmax module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i torch exp i div_ torch exp i sum True expand log_ ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i torch exp i div_ torch exp i sum False log_ desc= multiparam ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i torch exp i div_ torch exp i sum False log_ desc= multiparam_scalar ModuleInput constructor_input=FunctionInput - forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim module_inputs_torch_nn_Softmin module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= multidim ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= scalar ModuleInput constructor_input=FunctionInput - forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim module_inputs_torch_nn_Softplus module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i torch log p torch exp i ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i torch log p torch exp i desc= beta ModuleInput constructor_input=FunctionInput - forward_input=FunctionInput make_input reference_fn= lambda m p i i - type_as i i + i = - type_as i torch log p torch exp i desc= beta_threshold ModuleInput constructor_input=FunctionInput - forward_input=FunctionInput make_input reference_fn= lambda m p i i - type_as i i + i = - type_as i torch log p torch exp i desc= beta_threshold_scalar ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim module_inputs_torch_nn_Softshrink module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= lambda ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= lambda_scalar ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim module_inputs_torch_nn_Softsign module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i i div + torch abs i ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i i div + torch abs i desc= scalar ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim module_inputs_torch_nn_Tanh module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= scalar ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim module_inputs_torch_nn_Tanhshrink module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= scalar ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim module_inputs_torch_nn_Threshold module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= threshold_value ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= large_value ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= threshold_value_scalar ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim module_inputs_torch_nn_Mish module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i i torch tanh F softplus i ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i i torch tanh F softplus i desc= scalar ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim module_inputs_torch_nn_L Loss module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input make_input reference_fn=lambda m p i t i numel sum - b abs sum b zip i t strict=True ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input make_input reference_fn=lambda m p i t i numel i - t abs sum desc= scalar + generate_regression_criterion_inputs make_input module_inputs_torch_nn_SmoothL Loss module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad cases list tuple str dict = reduction_sum reduction sum reduction_mean reduction mean reduction_none reduction none module_inputs = desc constructor_kwargs cases reference_fn m p i t constructor_kwargs=constructor_kwargs smoothl loss_reference i t constructor_kwargs module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input make_input desc=desc reference_fn=reference_fn module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input make_input desc=f scalar_ desc reference_fn=reference_fn module_inputs module_inputs_torch_nn_BCELoss module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_target = partial make_tensor device=device dtype=dtype requires_grad=False make_weight = partial make_tensor device=device dtype=dtype requires_grad=False cases list tuple str dict = reduction_sum reduction sum reduction_mean reduction mean reduction_none reduction none weights weight make_weight bce_loss_reference_fn m p i t reduction= mean weight=None result = - t i log + - t - i log weight None result = result weight reduction == none result reduction == mean result sum i numel result sum module_inputs = desc constructor_kwargs cases module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input low= e- high= - e- make_target gt dtype desc=desc reference_fn=partial bce_loss_reference_fn constructor_kwargs scalar_weight = make_weight module_inputs append ModuleInput constructor_input=FunctionInput weight=scalar_weight forward_input=FunctionInput make_input low= e- high= - e- make_target gt dtype desc= scalar_weight reference_fn=partial bce_loss_reference_fn weight=scalar_weight module_inputs module_inputs_torch_nn_BCEWithLogitsLoss module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_target = partial make_tensor device=device dtype=dtype requires_grad=False make_weight = partial make_tensor device=device dtype=dtype requires_grad=False cases list tuple str dict = reduction_sum reduction sum reduction_mean reduction mean reduction_none reduction none weights weight make_weight scalar_weights weight make_weight bce_withlogitsloss_reference_fn m p i t reduction= mean weight=None TODO add pos_weight definition here corresponding SampleInputs max_val = -i clamp min= result = - t mul_ i add_ max_val add_ -max_val exp_ add_ -i - max_val exp_ log_ weight None result = result weight reduction == none result reduction == mean result sum i numel result sum module_inputs = desc constructor_kwargs cases module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input low= e- high= - e- make_target gt dtype desc=desc reference_fn=partial bce_withlogitsloss_reference_fn constructor_kwargs module_inputs module_inputs_torch_nn_CrossEntropyLoss module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_target = partial make_tensor device=device dtype=torch long requires_grad=False make_weight = partial make_tensor device=device dtype=dtype requires_grad=False reductions list str = mean sum none cases list tuple str dict = weights weight make_weight ignore_index ignore_index label_smoothing label_smoothing ignore_index_label_smoothing ignore_index label_smoothing module_inputs = reduction desc constructor_kwargs product reductions cases reference_fn m p i t reduction=reduction constructor_kwargs=constructor_kwargs cross_entropy_loss_reference i t reduction=reduction constructor_kwargs module_inputs append ModuleInput constructor_input=FunctionInput reduction=reduction constructor_kwargs forward_input=FunctionInput make_input make_target low= high= desc=f d_ desc _ reduction reference_fn=reference_fn module_inputs append ModuleInput constructor_input=FunctionInput reduction=reduction constructor_kwargs forward_input=FunctionInput make_input make_target low= high= desc=f d_ desc _ reduction reference_fn=reference_fn module_inputs append ModuleInput constructor_input=FunctionInput reduction=reduction constructor_kwargs forward_input=FunctionInput make_input make_target low= high= desc=f d_ desc _ reduction reference_fn=reference_fn module_inputs append ModuleInput constructor_input=FunctionInput reduction=reduction constructor_kwargs forward_input=FunctionInput make_input make_target low= high= desc=f higher_dim_ desc _ reduction reference_fn=reference_fn constructor_kwargs get ignore_index None None module_inputs append ModuleInput constructor_input=FunctionInput reduction=reduction constructor_kwargs forward_input=FunctionInput make_input make_input softmax dim= desc=f d_prob_target_ desc _ reduction reference_fn=reference_fn module_inputs append ModuleInput constructor_input=FunctionInput reduction=reduction constructor_kwargs forward_input=FunctionInput make_input make_input softmax dim= desc=f d_prob_target_ desc _ reduction reference_fn=reference_fn module_inputs append ModuleInput constructor_input=FunctionInput reduction=reduction constructor_kwargs forward_input=FunctionInput make_input make_input softmax dim= desc=f d_prob_target_ desc _ reduction reference_fn=reference_fn module_inputs append ModuleInput constructor_input=FunctionInput reduction=reduction constructor_kwargs forward_input=FunctionInput make_input make_input softmax dim= desc=f higher_dim_prob_target_ desc _ reduction reference_fn=reference_fn module_inputs append ModuleInput constructor_input=FunctionInput reduction=reduction constructor_kwargs forward_input=FunctionInput make_input make_target low= high= desc=f no_batch_dim_ desc _ reduction reference_fn=partial no_batch_dim_reference_fn is_criterion=True module_inputs module_inputs_torch_nn_CTCLoss module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_target = partial make_tensor device=device requires_grad=False cases list tuple str dict = reduction_sum reduction sum reduction_mean reduction mean reduction_none reduction none blank blank target_dtypes = torch int torch long module_inputs = target_dtype desc constructor_kwargs product target_dtypes cases reference_fn m p i t il tl constructor_kwargs=constructor_kwargs ctcloss_reference i t il tl constructor_kwargs blank = constructor_kwargs get blank low = blank == high = blank == module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input log_softmax make_target dtype=target_dtype low=low high=high desc=f desc _lengths_intlists reference_fn=reference_fn module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input log_softmax make_target dtype=target_dtype low=low high=high torch tensor device=device torch tensor device=device desc=f desc _lengths_tensors reference_fn=reference_fn module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input log_softmax make_target + + dtype=target_dtype low=low high=high desc=f desc _ d_target_lengths_intlists reference_fn=reference_fn module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input log_softmax make_target + + dtype=target_dtype low=low high=high torch tensor device=device torch tensor device=device desc=f desc _ d_target_lengths_tensors reference_fn=reference_fn module_inputs module_inputs_torch_nn_GroupNorm module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput e- forward_input=FunctionInput make_input desc= d_affine ModuleInput constructor_input=FunctionInput e- forward_input=FunctionInput make_input desc= d_affine_GN ModuleInput constructor_input=FunctionInput e- forward_input=FunctionInput make_input desc= d_affine_large_batch ModuleInput constructor_input=FunctionInput e- False forward_input=FunctionInput make_input desc= d_no_affine_IN ModuleInput constructor_input=FunctionInput e- False forward_input=FunctionInput make_input desc= d_no_affine_LN ModuleInput constructor_input=FunctionInput e- forward_input=FunctionInput make_input desc= d_affine ModuleInput constructor_input=FunctionInput e- False forward_input=FunctionInput make_input desc= d_no_affine_IN ModuleInput constructor_input=FunctionInput e- False forward_input=FunctionInput make_input desc= d_no_affine_LN module_inputs_torch_nn_Hardshrink module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= scalar ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim module_inputs_torch_nn_Hardswish module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= d_input module_inputs_torch_nn_Hardtanh module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i i clamp - ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i i clamp - desc= scalar ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim module_inputs_torch_nn_HingeEmbeddingLoss module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_target = partial make_tensor device=device dtype=dtype requires_grad=False cases list tuple str dict = reduction_sum reduction sum reduction_mean reduction mean reduction_none reduction none margin margin module_inputs = desc constructor_kwargs cases reference_fn m p i t constructor_kwargs=constructor_kwargs hingeembeddingloss_reference i t constructor_kwargs module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input make_target gt dtype mul_ sub_ desc=desc reference_fn=reference_fn module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input make_target gt dtype mul_ sub_ desc=f scalar_ desc reference_fn=reference_fn module_inputs module_inputs_torch_nn_HuberLoss module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad cases list tuple str dict = reduction_sum reduction sum reduction_mean reduction mean reduction_none reduction none module_inputs = desc constructor_kwargs cases reference_fn m p i t constructor_kwargs=constructor_kwargs huberloss_reference i t constructor_kwargs module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input make_input desc=desc reference_fn=reference_fn module_inputs module_inputs_torch_nn_InstanceNormNd module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad lazy = kwargs get lazy False N = kwargs N num_features eps momentum affine track_running_stats = e- False True input_no_batch_shape_dict = input_no_batch_shape = input_no_batch_shape_dict N input_batch_shape = + input_no_batch_shape ModuleInput constructor_input= FunctionInput eps momentum lazy FunctionInput num_features eps momentum forward_input=FunctionInput make_input input_batch_shape ModuleInput constructor_input= FunctionInput eps momentum affine track_running_stats lazy FunctionInput num_features eps momentum affine track_running_stats forward_input=FunctionInput make_input input_batch_shape desc= tracking_stats ModuleInput constructor_input= FunctionInput eps momentum lazy FunctionInput num_features eps momentum forward_input=FunctionInput make_input input_no_batch_shape reference_fn=no_batch_dim_reference_fn desc= tracking_stats_no_batch_dim ModuleInput constructor_input= FunctionInput eps momentum affine track_running_stats lazy FunctionInput num_features eps momentum affine track_running_stats forward_input=FunctionInput make_input input_no_batch_shape reference_fn=no_batch_dim_reference_fn desc= no_batch_dim module_inputs_torch_nn_LayerNorm module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput e- forward_input=FunctionInput make_input desc= d_elementwise_affine ModuleInput constructor_input=FunctionInput e- forward_input=FunctionInput make_input desc= d_elementwise_affine_large_batch ModuleInput constructor_input=FunctionInput e- False forward_input=FunctionInput make_input desc= d_no_elementwise_affine ModuleInput constructor_input=FunctionInput e- forward_input=FunctionInput make_input desc= d_elementwise_affine ModuleInput constructor_input=FunctionInput e- False forward_input=FunctionInput make_input desc= d_no_elementwise_affine ModuleInput constructor_input=FunctionInput e- forward_input=FunctionInput make_input desc= d_empty_elementwise_affine ModuleInput constructor_input=FunctionInput e- elementwise_affine=True bias=False forward_input=FunctionInput make_input desc= d_elementwise_affine_no_bias module_inputs_torch_nn_RMSNorm module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad rms_norm_reference_fn m p i eps = m eps eps None eps = torch finfo i dtype eps ndim = i ndim normalized_shape = m normalized_shape weight = m weight dims = ndim - i - i range len normalized_shape upcasted_i = i float result = upcasted_i torch rsqrt upcasted_i pow mean dim=dims keepdim=True + m eps weight None result = weight result type_as i ModuleInput constructor_input=FunctionInput e- forward_input=FunctionInput make_input desc= d_elementwise_affine reference_fn=rms_norm_reference_fn ModuleInput constructor_input=FunctionInput e- forward_input=FunctionInput make_input desc= d_elementwise_affine_large_batch reference_fn=rms_norm_reference_fn ModuleInput constructor_input=FunctionInput e- False forward_input=FunctionInput make_input desc= d_no_elementwise_affine reference_fn=rms_norm_reference_fn ModuleInput constructor_input=FunctionInput e- forward_input=FunctionInput make_input desc= d_elementwise_affine reference_fn=rms_norm_reference_fn ModuleInput constructor_input=FunctionInput e- False forward_input=FunctionInput make_input desc= d_no_elementwise_affine reference_fn=rms_norm_reference_fn ModuleInput constructor_input=FunctionInput e- forward_input=FunctionInput make_input desc= d_empty_elementwise_affine reference_fn=rms_norm_reference_fn module_inputs_torch_nn_LocalResponseNorm module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= d ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= d_uneven_pad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= d_custom_params module_inputs_torch_nn_LPPool d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= norm ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim module_inputs_torch_nn_LPPool d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= norm module_inputs_torch_nn_LPPool d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= norm module_inputs_torch_nn_MaxPool d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= d_input ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= stride ModuleInput constructor_input=FunctionInput return_indices=True forward_input=FunctionInput make_input desc= return_indices module_inputs_torch_nn_MaxPool d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= d_input ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= d_input ModuleInput constructor_input=FunctionInput return_indices=True forward_input=FunctionInput make_input desc= return_indices module_inputs_torch_nn_MaxPool d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= stride ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= stride_padding ModuleInput constructor_input=FunctionInput return_indices=True forward_input=FunctionInput make_input desc= return_indices module_inputs_torch_nn_FractionalMaxPool d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_random_samples torch empty dtype=torch double device=device uniform_ ModuleInput constructor_input=FunctionInput output_ratio= _random_samples=make_random_samples forward_input=FunctionInput make_input desc= ratio ModuleInput constructor_input=FunctionInput output_size= _random_samples=make_random_samples forward_input=FunctionInput make_input desc= size ModuleInput constructor_input=FunctionInput output_ratio= _random_samples=make_random_samples return_indices=True forward_input=FunctionInput make_input desc= ratio_return_indices ModuleInput constructor_input=FunctionInput output_ratio= _random_samples=make_random_samples forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= ratio_no_batch_dim ModuleInput constructor_input=FunctionInput output_size= _random_samples=make_random_samples forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= size_no_batch_dim module_inputs_torch_nn_FractionalMaxPool d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_random_samples torch empty dtype=torch double device=device uniform_ ModuleInput constructor_input=FunctionInput output_ratio= _random_samples=make_random_samples forward_input=FunctionInput make_input desc= ratio ModuleInput constructor_input=FunctionInput output_size= _random_samples=make_random_samples forward_input=FunctionInput make_input desc= size ModuleInput constructor_input=FunctionInput output_size= _random_samples=make_random_samples forward_input=FunctionInput make_input desc= asymsize ModuleInput constructor_input=FunctionInput output_ratio= _random_samples=make_random_samples return_indices=True forward_input=FunctionInput make_input desc= ratio_return_indices ModuleInput constructor_input=FunctionInput output_ratio= _random_samples=make_random_samples forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= ratio_no_batch_dim ModuleInput constructor_input=FunctionInput output_size= _random_samples=make_random_samples forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= size_no_batch_dim module_inputs_torch_nn_Sigmoid module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= scalar ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= channels_last_mem_format ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= channels_last_ d_mem_format module_inputs_torch_nn_LogSigmoid module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i i sigmoid log desc= scalar ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i i sigmoid log ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn desc= no_batch_dim module_inputs_torch_nn_MarginRankingLoss module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_target = partial make_tensor device=device dtype=torch long requires_grad=False cases list tuple str dict = reduction_sum reduction sum reduction_mean reduction mean reduction_none reduction none margin margin module_inputs = desc constructor_kwargs cases reference_fn m p i i t constructor_kwargs=constructor_kwargs marginrankingloss_reference i i t constructor_kwargs module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input make_input make_target sign desc=desc reference_fn=reference_fn module_inputs module_inputs_torch_nn_MultiLabelMarginLoss module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_target = partial make_tensor device=device dtype=torch long requires_grad=False cases list tuple str dict = reduction_sum reduction sum reduction_mean reduction mean reduction_none reduction none module_inputs = desc constructor_kwargs cases reference_fn m p i t constructor_kwargs=constructor_kwargs multilabelmarginloss_reference i t constructor_kwargs module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input make_target low= high= desc=f d_ desc reference_fn=reference_fn module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input make_target low= high= desc=desc reference_fn=reference_fn module_inputs module_inputs_torch_nn_MultiMarginLoss module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_target = partial make_tensor device=device dtype=torch long requires_grad=False make_weight = partial make_tensor device=device dtype=dtype requires_grad=False cases list tuple str dict = reduction_sum reduction sum reduction_mean reduction mean reduction_none reduction none p p margin margin weights weight make_weight module_inputs = desc constructor_kwargs cases reference_fn m p i t constructor_kwargs=constructor_kwargs multimarginloss_reference i t constructor_kwargs module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input make_target low= high= desc=desc reference_fn=reference_fn module_inputs module_inputs_torch_nn_MultiLabelSoftMarginLoss module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_target = partial make_tensor device=device dtype=torch long requires_grad=False make_weight = partial make_tensor device=device dtype=dtype requires_grad=False cases list tuple str dict = reduction_sum reduction sum reduction_mean reduction mean reduction_none reduction none weight weight make_weight multilabelsoftmargin_loss_reference_fn m p i t reduction= mean weight=None result = t i sigmoid log + - t -i sigmoid log weight None result = weight result = -result sum i dim - i size - reduction == none result reduction == mean result mean result sum module_inputs = desc constructor_kwargs cases module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input make_target low= high= desc=desc reference_fn=partial multilabelsoftmargin_loss_reference_fn constructor_kwargs module_inputs module_inputs_torch_nn_SoftMarginLoss module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad make_target = partial make_tensor device=device dtype=dtype requires_grad=False cases list tuple str dict = reduction_sum reduction sum reduction_mean reduction mean reduction_none reduction none module_inputs = desc constructor_kwargs cases reference_fn m p i t constructor_kwargs=constructor_kwargs softmarginloss_reference i t constructor_kwargs module_inputs append ModuleInput constructor_input=FunctionInput constructor_kwargs forward_input=FunctionInput make_input make_target sign desc=desc reference_fn=reference_fn module_inputs module_inputs_torch_nn_TransformerEncoder module_info device dtype requires_grad training kwargs Reuse TransformerEncoderLayer samples since forward args nearly same samples = layer_module_input module_inputs_torch_nn_TransformerEncoderLayer None device dtype requires_grad training Construct TransformerEncoderLayer object pass TransformerEncoder l_args l_kwargs = layer_module_input constructor_input args layer_module_input constructor_input kwargs l_kwargs device = device l_kwargs dtype = dtype encoder_layer = torch nn TransformerEncoderLayer l_args l_kwargs num_layers = Note TransformerEncoderLayer takes src_mask while TransformerEncoder takes mask rename kwarg appropriately forward_input = layer_module_input forward_input src_mask forward_input kwargs forward_input kwargs mask = forward_input kwargs src_mask del forward_input kwargs src_mask samples append ModuleInput constructor_input=FunctionInput encoder_layer num_layers forward_input=forward_input desc=layer_module_input desc samples module_inputs_torch_nn_TransformerEncoderLayer module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad samples = ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input desc= relu_activation ModuleInput constructor_input=FunctionInput F gelu forward_input=FunctionInput make_input desc= gelu_activation ModuleInput constructor_input=FunctionInput bias=False forward_input=FunctionInput make_input desc= no_bias Samples below validating no-batch-dim support key_padding_masks = None torch tensor False False True device=device dtype=torch bool attn_masks = None torch tensor False False True device=device dtype=torch bool expand src_mask src_key_padding_mask norm_first batch_first bias \ itertools product attn_masks key_padding_masks True False True False True False samples append ModuleInput constructor_input=FunctionInput d_model= nhead= dim_feedforward= dropout= batch_first=batch_first norm_first=norm_first bias=bias forward_input=FunctionInput make_input src_mask=src_mask src_key_padding_mask=src_key_padding_mask reference_fn=partial no_batch_dim_reference_fn batch_first=batch_first kwargs_to_batchify= src_key_padding_mask desc=f no_batch_dim_batch_first_ batch_first Samples below where we pass reference_fn validating fast path since fast path requires no_grad mode we run fast path eval no_grad reference_fn verify against results train mode fast_path_reference_fn module parameters args kwargs assert module training module train False torch no_grad output = module args kwargs module train True output training norm_first bias itertools product True False True False samples append ModuleInput constructor_input=FunctionInput dropout= batch_first=True norm_first=norm_first bias=bias forward_input=FunctionInput make_input fastpath doesn t run when bias=False reference_fn=fast_path_reference_fn bias None desc=f fastpath_ bias _norm_first_ norm_first samples module_inputs_torch_nn_TransformerDecoderLayer module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad samples = ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input make_input desc= relu_activation ModuleInput constructor_input=FunctionInput F gelu forward_input=FunctionInput make_input make_input desc= gelu_activation ModuleInput constructor_input=FunctionInput bias=False forward_input=FunctionInput make_input make_input desc= no_bias key_padding_masks = None torch tensor False False True device=device dtype=torch bool attn_masks = None torch tensor False False True device=device dtype=torch bool expand tgt_mask tgt_key_padding_mask norm_first bias batch_first \ itertools product attn_masks key_padding_masks True False True False True False Using same mask tgt memory memory_mask = tgt_mask memory_key_padding_mask = tgt_key_padding_mask samples append ModuleInput constructor_input=FunctionInput d_model= nhead= dim_feedforward= dropout= batch_first=batch_first norm_first=norm_first bias=bias forward_input=FunctionInput make_input make_input tgt_mask=tgt_mask memory_mask=memory_mask tgt_key_padding_mask=tgt_key_padding_mask memory_key_padding_mask=memory_key_padding_mask reference_fn=partial no_batch_dim_reference_fn batch_first=batch_first kwargs_to_batchify= tgt_key_padding_mask memory_key_padding_mask desc=f no_batch_dim_batch_first_ batch_first src tgt = make_input make_input batch_first src tgt = src transpose tgt transpose tgt_key_padding_mask None memory_key_padding_mask tgt_key_padding_mask = tgt_key_padding_mask expand samples append ModuleInput constructor_input=FunctionInput d_model= nhead= dim_feedforward= dropout= batch_first=batch_first norm_first=norm_first bias=bias forward_input=FunctionInput src tgt tgt_mask=tgt_mask memory_mask=memory_mask tgt_key_padding_mask=tgt_key_padding_mask memory_key_padding_mask=memory_key_padding_mask desc=f norm_first_ norm_first _batch_first_ batch_first _bias_ bias samples module_inputs_torch_nn_Transformer module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad samples = Samples below validating no-batch-dim support key_padding_masks = None torch tensor False False True device=device dtype=torch bool attn_masks = None torch tensor False False True device=device dtype=torch bool expand mask key_padding_mask norm_first bias batch_first \ itertools product attn_masks key_padding_masks True False True False True False Using same mask tgt memory src_mask tgt_mask = mask src_key_padding_mask tgt_key_padding_mask = key_padding_mask samples append ModuleInput constructor_input=FunctionInput d_model= nhead= dim_feedforward= num_encoder_layers= num_decoder_layers= dropout= batch_first=batch_first norm_first=norm_first bias=bias forward_input=FunctionInput make_input make_input tgt_mask=tgt_mask src_mask=src_mask tgt_key_padding_mask=tgt_key_padding_mask src_key_padding_mask=src_key_padding_mask reference_fn=partial no_batch_dim_reference_fn batch_first=batch_first kwargs_to_batchify= tgt_key_padding_mask src_key_padding_mask desc=f no_batch_dim_batch_first_ batch_first src tgt = make_input make_input batch_first src = src transpose tgt = tgt transpose key_padding_mask None src_key_padding_mask tgt_key_padding_mask = key_padding_mask expand samples append ModuleInput constructor_input=FunctionInput d_model= nhead= dim_feedforward= num_encoder_layers= num_decoder_layers= dropout= batch_first=batch_first norm_first=norm_first bias=bias forward_input=FunctionInput src tgt tgt_mask=tgt_mask src_mask=src_mask tgt_key_padding_mask=tgt_key_padding_mask src_key_padding_mask=src_key_padding_mask samples module_inputs_torch_nn_Embedding module_info device dtype requires_grad training kwargs make_empty = partial torch empty device=device dtype=torch long requires_grad=False ModuleInput constructor_input=FunctionInput num_embeddings= embedding_dim= forward_input=FunctionInput make_empty random_ ModuleInput constructor_input=FunctionInput num_embeddings= embedding_dim= forward_input=FunctionInput make_empty random_ expand desc= discontiguous module_inputs_torch_nn_MultiheadAttention module_info device dtype requires_grad training kwargs Currently all samples below validating no-batch-dim support make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad samples = bool_vals = True False key_padding_masks = None torch tensor False False True device=device dtype=torch bool attn_masks = None torch tensor False False True device=device dtype=torch bool expand products = itertools product bool_vals bool_vals bool_vals key_padding_masks attn_masks bias add_bias_kv add_zero_attn key_padding_mask attn_mask products samples append ModuleInput constructor_input=FunctionInput embed_dim= num_heads= batch_first=True bias=bias add_bias_kv=add_bias_kv add_zero_attn=add_zero_attn forward_input=FunctionInput make_input make_input make_input key_padding_mask=key_padding_mask attn_mask=attn_mask reference_fn=no_batch_dim_reference_mha samples append ModuleInput constructor_input=FunctionInput embed_dim= num_heads= batch_first=False bias=bias add_bias_kv=add_bias_kv add_zero_attn=add_zero_attn forward_input=FunctionInput make_input make_input make_input key_padding_mask=key_padding_mask attn_mask=attn_mask reference_fn=partial no_batch_dim_reference_mha batch_first=False samples module_inputs_torch_nn_RNN_GRU_Cell module_info device dtype requires_grad training kwargs Currently all samples below validating no-batch-dim support make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad samples = ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input make_input reference_fn=no_batch_dim_reference_fn ModuleInput constructor_input=FunctionInput bias=True forward_input=FunctionInput make_input make_input reference_fn=no_batch_dim_reference_fn is_rnn = kwargs get is_rnn False is_rnn RNN also supports ` nonlinearity ` argument ` tanh ` default so we check ` relu ` samples append ModuleInput constructor_input=FunctionInput bias=True nonlinearity= relu forward_input=FunctionInput make_input make_input reference_fn=no_batch_dim_reference_fn samples module_inputs_torch_nn_LSTMCell module_info device dtype requires_grad training kwargs Currently all samples below validating no-batch-dim support make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad samples = ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input make_input make_input reference_fn=no_batch_dim_reference_lstmcell ModuleInput constructor_input=FunctionInput bias=True forward_input=FunctionInput make_input make_input make_input reference_fn=no_batch_dim_reference_lstmcell samples make_packed_sequence inp batch_sizes required_grad = inp requires_grad inp requires_grad_ False user won t have access inp so won t able get its grads seq = pack_padded_sequence inp batch_sizes seq data requires_grad_ required_grad seq module_inputs_torch_nn_RNN_GRU module_info device dtype requires_grad training with_packed_sequence=False kwargs Currently all samples below validating no-batch-dim support make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad is_rnn = kwargs is_rnn nonlinearity = relu tanh bias = False True batch_first = False True bidirectional = False True samples = is_rnn prod_gen = product nonlinearity bias batch_first bidirectional prod_gen = product bias batch_first bidirectional args prod_gen is_rnn nl b b_f bidir = args b b_f bidir = args cons_args = input_size hidden_size num_layers batch_first b_f bias b bidirectional bidir cons_args_hidden = input_size hidden_size num_layers batch_first b_f bias b bidirectional bidir is_rnn cons_args nonlinearity = nl cons_args_hidden nonlinearity = nl samples append ModuleInput constructor_input=FunctionInput cons_args forward_input=FunctionInput make_input reference_fn=partial no_batch_dim_reference_rnn_gru batch_first=b_f samples append ModuleInput constructor_input=FunctionInput cons_args_hidden forward_input=FunctionInput make_input make_input bidir reference_fn=partial no_batch_dim_reference_rnn_gru batch_first=b_f with_packed_sequence samples append ModuleInput constructor_input=FunctionInput cons_args forward_input=FunctionInput make_packed_sequence make_input torch tensor reference_fn=partial no_batch_dim_reference_rnn_gru batch_first=b_f samples append ModuleInput constructor_input=FunctionInput cons_args forward_input=FunctionInput make_packed_sequence make_input torch tensor reference_fn=partial no_batch_dim_reference_rnn_gru batch_first=b_f samples module_inputs_torch_nn_LSTM module_info device dtype requires_grad training kwargs Currently all samples below validating no-batch-dim support make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad bias = False True batch_first = False True bidirectional = False True proj_sizes = samples = prod_gen = product bias batch_first bidirectional proj_sizes args prod_gen b b_f bidir proj_size = args hidden_size = cons_args = input_size hidden_size hidden_size num_layers proj_size proj_size batch_first b_f bias b bidirectional bidir cons_args_hidden = input_size hidden_size hidden_size num_layers proj_size proj_size batch_first b_f bias b bidirectional bidir samples append ModuleInput constructor_input=FunctionInput cons_args forward_input=FunctionInput make_input reference_fn=partial no_batch_dim_reference_lstm batch_first=b_f h_out = proj_size proj_size hidden_size hx = make_input bidir h_out make_input bidir hidden_size samples append ModuleInput constructor_input=FunctionInput cons_args_hidden forward_input=FunctionInput make_input hx reference_fn=partial no_batch_dim_reference_lstm batch_first=b_f samples module_inputs_torch_nn_ReflectionPad d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input module_inputs_torch_nn_ReflectionPad d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input module_inputs_torch_nn_ReflectionPad d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input module_inputs_torch_nn_ReplicationPad d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input module_inputs_torch_nn_ReplicationPad d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input module_inputs_torch_nn_ReplicationPad d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input module_inputs_torch_nn_ZeroPad d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input module_inputs_torch_nn_ZeroPad d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input module_inputs_torch_nn_ZeroPad d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input module_inputs_torch_nn_ConstantPad d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input module_inputs_torch_nn_ConstantPad d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input module_inputs_torch_nn_ConstantPad d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input module_inputs_torch_nn_CircularPad d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad padding d_circular_ref inp pad r input pad output torch cat inp -pad inp inp pad dim= ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i padding d_circular_ref i m padding ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i padding d_circular_ref i m padding ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i padding d_circular_ref i m padding module_inputs_torch_nn_CircularPad d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad padding d_circular_ref inp pad r input pad output inp = torch cat inp -pad inp inp pad dim= torch cat inp -pad inp inp pad dim= ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i padding d_circular_ref i m padding ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i padding d_circular_ref i m padding ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i padding d_circular_ref i m padding module_inputs_torch_nn_CircularPad d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad padding d_circular_ref inp pad r input pad output inp = torch cat inp -pad inp inp pad dim= inp = torch cat inp -pad inp inp pad dim= torch cat inp -pad inp inp pad dim= ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=no_batch_dim_reference_fn ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i padding d_circular_ref i m padding ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i padding d_circular_ref i m padding ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input reference_fn=lambda m p i padding d_circular_ref i m padding All these operators share similar issues cuDNN MIOpen rnn_gru_lstm_module_info_decorators = RuntimeError Batching rule implemented aten _cudnn_rnn_backward We could generate fallback DecorateInfo unittest expectedFailure TestModule test_grad active_if= TEST_CUDNN TEST_WITH_ROCM device_type= cuda NotImplementedError derivative _cudnn_rnn_backward implemented Double backwards supported CuDNN RNNs due limitations CuDNN API DecorateInfo unittest expectedFailure TestModule test_gradgrad active_if= TEST_CUDNN TEST_WITH_ROCM device_type= cuda CUDNN GRU doesn t accept non-contiguous hx DecorateInfo unittest expectedFailure TestModule test_non_contiguous_tensors active_if= TEST_CUDNN TEST_WITH_ROCM device_type= cuda MIOPEN GRU doesn t accept non-contiguous hx dispatched miopen only float DecorateInfo unittest expectedFailure TestModule test_non_contiguous_tensors active_if= TEST_CUDNN TEST_WITH_ROCM dtypes= torch float device_type= cuda Start module error inputs functions module_error_inputs_torch_nn_RNN_GRU_Cell module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad samples = ErrorModuleInput ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input make_input error_on=ModuleErrorEnum FORWARD_ERROR error_type=RuntimeError error_regex= input has inconsistent input_size got expected ErrorModuleInput ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input make_input error_on=ModuleErrorEnum FORWARD_ERROR error_type=RuntimeError error_regex= hidden has inconsistent hidden_size got expected ErrorModuleInput ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input make_input error_on=ModuleErrorEnum FORWARD_ERROR error_type=RuntimeError error_regex= Input batch size doesn t match hidden batch size ErrorModuleInput ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input make_input error_on=ModuleErrorEnum FORWARD_ERROR error_type=ValueError error_regex= Expected hidden D D got D instead ErrorModuleInput ModuleInput constructor_input=FunctionInput relu forward_input=FunctionInput make_input make_input error_on=ModuleErrorEnum FORWARD_ERROR error_type=RuntimeError error_regex= hidden has inconsistent hidden_size got expected ErrorModuleInput ModuleInput constructor_input=FunctionInput tanh forward_input=FunctionInput make_input make_input error_on=ModuleErrorEnum FORWARD_ERROR error_type=RuntimeError error_regex= hidden has inconsistent hidden_size got expected samples module_error_inputs_torch_nn_LSTMCell module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad samples = ErrorModuleInput ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input make_input make_input error_on=ModuleErrorEnum FORWARD_ERROR error_type=RuntimeError error_regex= input has inconsistent input_size got expected ErrorModuleInput ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input make_input make_input error_on=ModuleErrorEnum FORWARD_ERROR error_type=RuntimeError error_regex= hidden has inconsistent hidden_size got expected ErrorModuleInput ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input make_input make_input error_on=ModuleErrorEnum FORWARD_ERROR error_type=RuntimeError error_regex= Input batch size doesn t match hidden batch size ErrorModuleInput ModuleInput constructor_input=FunctionInput forward_input=FunctionInput make_input make_input make_input error_on=ModuleErrorEnum FORWARD_ERROR error_type=ValueError error_regex= Expected hx\\ \\ D D got D instead samples module_error_inputs_torch_nn_RNN_GRU module_info device dtype requires_grad training kwargs samples = ErrorModuleInput ModuleInput constructor_input=FunctionInput error_on=ModuleErrorEnum CONSTRUCTION_ERROR error_type=ValueError error_regex= hidden_size must greater than zero ErrorModuleInput ModuleInput constructor_input=FunctionInput error_on=ModuleErrorEnum CONSTRUCTION_ERROR error_type=ValueError error_regex= num_layers must greater than zero samples module_error_inputs_torch_nn_Pad d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad is_constant = kwargs get is_constant False ErrorModuleInput ModuleInput constructor_input=FunctionInput is_constant FunctionInput forward_input=FunctionInput make_input error_on=ModuleErrorEnum FORWARD_ERROR error_type=ValueError error_regex=r expected D D input \ got D input\ module_error_inputs_torch_nn_Pad d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad is_constant = kwargs get is_constant False ErrorModuleInput ModuleInput constructor_input=FunctionInput is_constant FunctionInput forward_input=FunctionInput make_input error_on=ModuleErrorEnum FORWARD_ERROR error_type=ValueError error_regex=r expected D D input \ got D input\ module_error_inputs_torch_nn_Pad d module_info device dtype requires_grad training kwargs make_input = partial make_tensor device=device dtype=dtype requires_grad=requires_grad is_constant = kwargs get is_constant False ErrorModuleInput ModuleInput constructor_input=FunctionInput is_constant FunctionInput forward_input=FunctionInput make_input error_on=ModuleErrorEnum FORWARD_ERROR error_type=ValueError error_regex=r expected D D input \ got D input\ _macos _or_newer = torch backends mps is_available torch backends mps is_macos_or_newer Database ModuleInfo entries alphabetical order module_db list ModuleInfo = ModuleInfo torch nn AdaptiveAvgPool d module_inputs_func=module_inputs_torch_nn_AdaptiveAvgPool d skips= Fails MPS backend input output sizes divisible DecorateInfo skipMPS ModuleInfo torch nn AdaptiveAvgPool d gradcheck_nondet_tol=GRADCHECK_NONDET_TOL module_inputs_func=module_inputs_torch_nn_AdaptiveAvgPool d skips= Fails MPS backend input output sizes divisible DecorateInfo skipMPS Fails backward check output size x DecorateInfo unittest expectedFailure TestModule test_memory_format active_if=operator itemgetter training ModuleInfo torch nn AdaptiveAvgPool d gradcheck_nondet_tol=GRADCHECK_NONDET_TOL module_inputs_func=module_inputs_torch_nn_AdaptiveAvgPool d skips= DecorateInfo unittest skip Skipped TestModule test_memory_format supported MPS backend DecorateInfo skipMPS ModuleInfo torch nn AdaptiveMaxPool d module_inputs_func=module_inputs_torch_nn_AdaptiveMaxPool d ModuleInfo torch nn AdaptiveMaxPool d gradcheck_nondet_tol=GRADCHECK_NONDET_TOL module_inputs_func=module_inputs_torch_nn_AdaptiveMaxPool d ModuleInfo torch nn AdaptiveMaxPool d gradcheck_nondet_tol=GRADCHECK_NONDET_TOL module_inputs_func=module_inputs_torch_nn_AdaptiveMaxPool d skips= DecorateInfo unittest skip Skipped TestModule test_memory_format supported MPS backend DecorateInfo skipMPS ModuleInfo torch nn AvgPool d module_inputs_func=module_inputs_torch_nn_AvgPool d ModuleInfo torch nn AvgPool d module_inputs_func=module_inputs_torch_nn_AvgPool d skips= The difference between channels last backward channels first backward AvgPool d CUDA too large See https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestModule test_memory_format active_if=operator itemgetter training device_type= cuda ModuleInfo torch nn AvgPool d module_inputs_func=module_inputs_torch_nn_AvgPool d gradcheck_nondet_tol=GRADCHECK_NONDET_TOL skips= No channels_last support AvgPool d does take D inputs DecorateInfo unittest skip Skipped TestModule test_memory_format backward supported MPS backend DecorateInfo skipMPS TestModule test_non_contiguous_tensors ModuleInfo torch nn BatchNorm d train_and_eval_differ=True module_inputs_func=module_inputs_torch_nn_BatchNorm d skips= tracking here rather than list test_aotdispatch py eval mode passes RuntimeError tried get Double out SymInt DecorateInfo unittest expectedFailure TestEagerFusionModuleInfo test_aot_autograd_symbolic_module_exhaustive active_if=operator itemgetter training torch _subclasses fake_tensor DataDependentOutputException aten _local_scalar_dense default DecorateInfo unittest expectedFailure TestEagerFusionModuleInfo test_aot_autograd_module_exhaustive active_if=operator itemgetter training ModuleInfo torch nn BatchNorm d train_and_eval_differ=True module_inputs_func=module_inputs_torch_nn_BatchNorm d skips= See https github com pytorch pytorch issues DecorateInfo expectedFailureMPS TestModule test_memory_format active_if=operator itemgetter training tracking here rather than list test_aotdispatch py eval mode passes RuntimeError tried get Double out SymInt DecorateInfo unittest expectedFailure TestEagerFusionModuleInfo test_aot_autograd_symbolic_module_exhaustive active_if=operator itemgetter training torch _subclasses fake_tensor DataDependentOutputException aten _local_scalar_dense default DecorateInfo unittest expectedFailure TestEagerFusionModuleInfo test_aot_autograd_module_exhaustive active_if=operator itemgetter training ModuleInfo torch nn BatchNorm d train_and_eval_differ=True module_inputs_func=module_inputs_torch_nn_BatchNorm d skips= supported MPS backend DecorateInfo skipMPS tracking here rather than list test_aotdispatch py eval mode passes RuntimeError tried get Double out SymInt DecorateInfo unittest expectedFailure TestEagerFusionModuleInfo test_aot_autograd_symbolic_module_exhaustive active_if=operator itemgetter training torch _subclasses fake_tensor DataDependentOutputException aten _local_scalar_dense default DecorateInfo unittest expectedFailure TestEagerFusionModuleInfo test_aot_autograd_module_exhaustive active_if=operator itemgetter training ModuleInfo torch nn CELU module_inputs_func=module_inputs_torch_nn_CELU MPS specific will xfailed all devices next PR skips= DecorateInfo unittest expectedFailure TestModule test_check_inplace device_type= mps dtypes= torch float ModuleInfo torch nn Conv d module_inputs_func=partial module_inputs_torch_nn_ConvNd N= lazy=False gradcheck_nondet_tol=GRADCHECK_NONDET_TOL module_memformat_affects_out=True decorators= DecorateInfo precisionOverride torch float e- TestModule test_memory_format ModuleInfo torch nn Conv d module_inputs_func=partial module_inputs_torch_nn_ConvNd N= lazy=False gradcheck_nondet_tol=GRADCHECK_NONDET_TOL module_memformat_affects_out=True skips= This wrongly being skipped before needs investigation See https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestModule test_memory_format device_type= cuda dtypes= torch float decorators= DecorateInfo precisionOverride torch float e- TestModule test_memory_format ModuleInfo torch nn Conv d module_inputs_func=partial module_inputs_torch_nn_ConvNd N= lazy=False gradcheck_nondet_tol=GRADCHECK_NONDET_TOL module_memformat_affects_out=True skips= Conv d supported MPS backend DecorateInfo skipMPS device_type= mps This wrongly being skipped before needs investigation See https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestModule test_memory_format decorators= DecorateInfo precisionOverride torch float e- TestModule test_memory_format ModuleInfo torch nn ConvTranspose d module_inputs_func=partial module_inputs_torch_nn_ConvNd N= lazy=False transposed=True gradcheck_nondet_tol=GRADCHECK_NONDET_TOL module_memformat_affects_out=True dtypes=floating_and_complex_types_and torch chalf skips= Not implemented chalf CPU DecorateInfo unittest expectedFailure TestModule test_cpu_gpu_parity dtypes= torch chalf device_type= cuda decorators= DecorateInfo precisionOverride torch float e- TestModule test_memory_format DecorateInfo precisionOverride torch chalf e- TestModule test_memory_format ModuleInfo torch nn ConvTranspose d module_inputs_func=partial module_inputs_torch_nn_ConvNd N= lazy=False transposed=True gradcheck_nondet_tol=GRADCHECK_NONDET_TOL module_memformat_affects_out=True dtypes=floating_and_complex_types_and torch chalf skips= Fails backward check because ViewAsRealBackward apply contiguous grad DecorateInfo unittest expectedFailure TestModule test_memory_format dtypes= torch complex torch complex torch complex This wrongly being skipped before needs investigation See https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestModule test_memory_format device_type= cuda dtypes= torch float torch complex Not implemented chalf CPU DecorateInfo unittest expectedFailure TestModule test_cpu_gpu_parity dtypes= torch chalf device_type= cuda decorators= DecorateInfo precisionOverride torch float e- TestModule test_memory_format DecorateInfo precisionOverride torch chalf e- TestModule test_memory_format ModuleInfo torch nn ConvTranspose d module_inputs_func=partial module_inputs_torch_nn_ConvNd N= lazy=False transposed=True dtypes=floating_and_complex_types_and torch chalf gradcheck_nondet_tol=GRADCHECK_NONDET_TOL module_memformat_affects_out=True skips= ConvTranspose d supported MPS backend DecorateInfo skipMPS This wrongly being skipped before needs investigation See https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestModule test_memory_format Not implemented chalf CPU DecorateInfo unittest expectedFailure TestModule test_cpu_gpu_parity dtypes= torch chalf device_type= cuda decorators= DecorateInfo precisionOverride torch float e- TestModule test_memory_format DecorateInfo precisionOverride torch complex e- TestModule test_cpu_gpu_parity DecorateInfo precisionOverride torch chalf e- TestModule test_memory_format ModuleInfo torch nn CosineEmbeddingLoss module_inputs_func=module_inputs_torch_nn_CosineEmbeddingLoss skips= No channels_last support loss functions DecorateInfo unittest skip Skipped TestModule test_memory_format ModuleInfo torch nn ELU module_inputs_func=module_inputs_torch_nn_ELU MPS specific will xfailed all devices next PR skips= DecorateInfo unittest expectedFailure TestModule test_check_inplace device_type= mps dtypes= torch float ModuleInfo torch nn FractionalMaxPool d module_inputs_func=module_inputs_torch_nn_FractionalMaxPool d gradcheck_nondet_tol=GRADCHECK_NONDET_TOL skips= supported MPS backend DecorateInfo skipMPS DecorateInfo unittest skip Skipped TestModule test_memory_format ModuleInfo torch nn FractionalMaxPool d module_inputs_func=module_inputs_torch_nn_FractionalMaxPool d gradcheck_nondet_tol=GRADCHECK_NONDET_TOL skips= supported MPS backend DecorateInfo skipMPS DecorateInfo unittest skip Skipped TestModule test_memory_format ModuleInfo torch nn L Loss module_inputs_func=module_inputs_torch_nn_L Loss skips= No channels_last support loss functions DecorateInfo unittest skip Skipped TestModule test_memory_format ModuleInfo torch nn SmoothL Loss module_inputs_func=module_inputs_torch_nn_SmoothL Loss skips= No channels_last support loss functions DecorateInfo unittest skip Skipped TestModule test_memory_format See input types tensor f tensor x xf broadcast compatible NS Still fails MacOS DecorateInfo skipIfMPS TestModule test_non_contiguous_tensors dtypes= torch float device_type= mps ModuleInfo torch nn LazyConv d module_inputs_func=partial module_inputs_torch_nn_ConvNd N= lazy=True gradcheck_nondet_tol=GRADCHECK_NONDET_TOL module_memformat_affects_out=True skips= Lazy modules don t currently play well ModuleInfo tests meta device See https github com pytorch pytorch issues more info DecorateInfo skipMeta decorators= DecorateInfo precisionOverride torch float e- TestModule test_memory_format ModuleInfo torch nn LazyConv d module_inputs_func=partial module_inputs_torch_nn_ConvNd N= lazy=True gradcheck_nondet_tol=GRADCHECK_NONDET_TOL module_memformat_affects_out=True skips= Lazy modules don t currently play well ModuleInfo tests meta device See https github com pytorch pytorch issues more info DecorateInfo skipMeta This wrongly being skipped before needs investigation See https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestModule test_memory_format device_type= cuda dtypes= torch float decorators= DecorateInfo precisionOverride torch float e- TestModule test_memory_format ModuleInfo torch nn LazyConv d module_inputs_func=partial module_inputs_torch_nn_ConvNd N= lazy=True gradcheck_nondet_tol=GRADCHECK_NONDET_TOL module_memformat_affects_out=True skips= Lazy modules don t currently play well ModuleInfo tests meta device See https github com pytorch pytorch issues more info DecorateInfo skipMeta LazyConv d supported MPS backend DecorateInfo skipMPS This wrongly being skipped before needs investigation See https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestModule test_memory_format decorators= DecorateInfo precisionOverride torch float e- TestModule test_memory_format ModuleInfo torch nn LazyConvTranspose d module_inputs_func=partial module_inputs_torch_nn_ConvNd N= lazy=True transposed=True gradcheck_nondet_tol=GRADCHECK_NONDET_TOL module_memformat_affects_out=True skips= Lazy modules don t currently play well ModuleInfo tests meta device See https github com pytorch pytorch issues more info DecorateInfo skipMeta decorators= DecorateInfo precisionOverride torch float e- TestModule test_memory_format ModuleInfo torch nn LazyConvTranspose d module_inputs_func=partial module_inputs_torch_nn_ConvNd N= lazy=True transposed=True gradcheck_nondet_tol=GRADCHECK_NONDET_TOL module_memformat_affects_out=True skips= Lazy modules don t currently play well ModuleInfo tests meta device See https github com pytorch pytorch issues more info DecorateInfo skipMeta This wrongly being skipped before needs investigation See https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestModule test_memory_format device_type= cuda dtypes= torch float decorators= DecorateInfo precisionOverride torch float e- TestModule test_memory_format ModuleInfo torch nn LazyConvTranspose d module_inputs_func=partial module_inputs_torch_nn_ConvNd N= lazy=True transposed=True gradcheck_nondet_tol=GRADCHECK_NONDET_TOL module_memformat_affects_out=True skips= Lazy modules don t currently play well ModuleInfo tests meta device See https github com pytorch pytorch issues more info DecorateInfo skipMeta LazyConvTranspose d supported MPS backend DecorateInfo skipMPS This wrongly being skipped before needs investigation See https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestModule test_memory_format decorators= DecorateInfo precisionOverride torch float e- TestModule test_memory_format ModuleInfo torch nn Linear module_inputs_func=module_inputs_torch_nn_Linear skips= No channels_last support Linear currently DecorateInfo unittest skip Skipped TestModule test_memory_format ModuleInfo torch nn Bilinear module_inputs_func=module_inputs_torch_nn_Bilinear decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- torch float tol atol= e- rtol= e- TestModule test_forward device_type= cpu skips= No channels_last support Bilinear currently DecorateInfo unittest skip Skipped TestModule test_memory_format See tolerance issue DecorateInfo unittest expectedFailure TestModule test_forward device_type= mps dtypes= torch float ModuleInfo torch nn LPPool d module_inputs_func=module_inputs_torch_nn_LPPool d skips= DecorateInfo unittest skip Skipped TestModule test_grad DecorateInfo unittest skip Skipped TestModule test_gradgrad ModuleInfo torch nn LPPool d module_inputs_func=module_inputs_torch_nn_LPPool d skips= DecorateInfo unittest skip Skipped TestModule test_grad DecorateInfo unittest skip Skipped TestModule test_gradgrad Fails backward check MPS See https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestModule test_memory_format active_if=operator itemgetter training _macos _or_newer device_type= mps ModuleInfo torch nn LPPool d module_inputs_func=module_inputs_torch_nn_LPPool d skips= DecorateInfo unittest skip Skipped TestModule test_grad DecorateInfo unittest skip Skipped TestModule test_gradgrad DecorateInfo unittest skip Skipped TestModule test_memory_format DecorateInfo skipIfMPS device_type= mps ModuleInfo torch nn MaxPool d module_inputs_func=module_inputs_torch_nn_MaxPool d ModuleInfo torch nn MaxPool d module_inputs_func=module_inputs_torch_nn_MaxPool d ModuleInfo torch nn MaxPool d module_inputs_func=module_inputs_torch_nn_MaxPool d gradcheck_nondet_tol=GRADCHECK_NONDET_TOL ModuleInfo torch nn KLDivLoss module_inputs_func=module_inputs_torch_nn_KLDivLoss skips= No channels_last support loss functions DecorateInfo unittest skip Skipped TestModule test_memory_format https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestModule test_cpu_gpu_parity DecorateInfo unittest skip Skipped TestModule test_grad DecorateInfo unittest skip Skipped TestModule test_gradgrad ModuleInfo torch nn MSELoss module_inputs_func=module_inputs_torch_nn_MSELoss skips= No channels_last support loss functions DecorateInfo unittest skip Skipped TestModule test_memory_format See tolerance issue DecorateInfo unittest expectedFailure TestModule test_forward device_type= mps dtypes= torch float ModuleInfo torch nn MarginRankingLoss module_inputs_func=module_inputs_torch_nn_MarginRankingLoss skips= No channels_last support loss functions DecorateInfo unittest skip Skipped TestModule test_memory_format ModuleInfo torch nn MultiLabelMarginLoss module_inputs_func=module_inputs_torch_nn_MultiLabelMarginLoss skips= No channels_last support loss functions DecorateInfo unittest skip Skipped TestModule test_memory_format aten multilabel_margin_loss_forward currently implemented MPS device DecorateInfo skipIfMPS TestModule device_type= mps derivative aten multilabel_margin_loss_backward implemented DecorateInfo unittest skip Skipped TestModule test_gradgrad ModuleInfo torch nn MultiMarginLoss module_inputs_func=module_inputs_torch_nn_MultiMarginLoss skips= No channels_last support loss functions DecorateInfo unittest skip Skipped TestModule test_memory_format aten multi_margin_loss currently implemented MPS device DecorateInfo skipIfMPS TestModule device_type= mps RuntimeError derivative aten multi_margin_loss_backward implemented DecorateInfo unittest skip Skipped TestModule test_gradgrad ModuleInfo torch nn SoftMarginLoss module_inputs_func=module_inputs_torch_nn_SoftMarginLoss skips= No channels_last support loss functions DecorateInfo unittest skip Skipped TestModule test_memory_format See tolerance issue DecorateInfo unittest expectedFailure TestModule test_forward device_type= mps dtypes= torch float ModuleInfo torch nn MultiLabelSoftMarginLoss module_inputs_func=module_inputs_torch_nn_MultiLabelSoftMarginLoss skips= No channels_last support loss functions DecorateInfo unittest skip Skipped TestModule test_memory_format ModuleInfo torch nn NLLLoss module_inputs_func=module_inputs_torch_nn_NLLLoss skips= No channels_last support loss functions DecorateInfo unittest skip Skipped TestModule test_memory_format See tolerance issue DecorateInfo unittest expectedFailure TestModule test_forward device_type= mps dtypes= torch float ModuleInfo torch nn GaussianNLLLoss module_inputs_func=module_inputs_torch_nn_GaussianNLLLoss skips= No channels_last support loss functions DecorateInfo unittest skip Skipped TestModule test_memory_format ModuleInfo torch nn PoissonNLLLoss module_inputs_func=module_inputs_torch_nn_PoissonNLLLoss skips= No channels_last support loss functions DecorateInfo unittest skip Skipped TestModule test_memory_format ModuleInfo torch nn HingeEmbeddingLoss module_inputs_func=module_inputs_torch_nn_HingeEmbeddingLoss skips= No channels_last support loss functions DecorateInfo unittest skip Skipped TestModule test_memory_format ModuleInfo torch nn HuberLoss module_inputs_func=module_inputs_torch_nn_HuberLoss skips= No channels_last support loss functions DecorateInfo unittest skip Skipped TestModule test_memory_format See seemingly incorrect output dtype DecorateInfo unittest expectedFailure TestModule test_forward device_type= mps dtypes= torch float ModuleInfo torch nn BCELoss module_inputs_func=module_inputs_torch_nn_BCELoss skips= No channels_last support loss functions DecorateInfo unittest skip Skipped TestModule test_memory_format error input types tensor f tensor x xf broadcast compatible DecorateInfo skipIfMPS TestModule dtypes= torch float device_type= mps ModuleInfo torch nn BCEWithLogitsLoss module_inputs_func=module_inputs_torch_nn_BCEWithLogitsLoss skips= No channels_last support loss functions DecorateInfo unittest skip Skipped TestModule test_memory_format see tolerance issue DecorateInfo skipIfMPS TestModule dtypes= torch float device_type= mps ModuleInfo torch nn CrossEntropyLoss module_inputs_func=module_inputs_torch_nn_CrossEntropyLoss dtypes=get_all_fp_dtypes include_half=True include_bfloat =False decorators= No channels_last support loss functions DecorateInfo unittest expectedFailure TestModule test_memory_format DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestModule test_forward dtypes= torch float device_type= cpu DecorateInfo unittest expectedFailure TestModule test_cpu_gpu_parity dtypes= torch float device_type= cuda ModuleInfo torch nn CTCLoss module_inputs_func=module_inputs_torch_nn_CTCLoss skips= No channels_last support loss functions DecorateInfo unittest skip Skipped TestModule test_memory_format The operator aten _ctc_loss currently implemented MPS device DecorateInfo skipIfMPS TestModule device_type= mps derivative aten _ctc_loss_backward implemented DecorateInfo unittest skip Skipped TestModule test_grad DecorateInfo unittest skip Skipped TestModule test_gradgrad https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestModule test_non_contiguous_tensors ModuleInfo torch nn GELU module_inputs_func=module_inputs_torch_nn_GELU skips= See tolerance issue DecorateInfo unittest expectedFailure TestModule test_forward device_type= mps dtypes= torch float ModuleInfo torch nn GLU module_inputs_func=module_inputs_torch_nn_GLU ModuleInfo torch nn GroupNorm module_inputs_func=module_inputs_torch_nn_GroupNorm dtypes=get_all_fp_dtypes include_bfloat =True include_half=True skips= Tracking https github com pytorch pytorch issues DecorateInfo unittest skip Skipped TestModule test_cpu_gpu_parity DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestModule test_memory_format device_type= cpu No channels_last support GroupNorm currently DecorateInfo unittest skip Skipped TestModule test_memory_format device_type= cuda DecorateInfo unittest skip Skipped TestModule test_memory_format device_type= mps DecorateInfo unittest skip Skipped TestModule test_grad active_if=TEST_WITH_ROCM device_type= cuda ModuleInfo torch nn Hardshrink module_inputs_func=module_inputs_torch_nn_Hardshrink ModuleInfo torch nn Hardswish module_inputs_func=module_inputs_torch_nn_Hardswish supports_gradgrad=False ModuleInfo torch nn Hardtanh module_inputs_func=module_inputs_torch_nn_Hardtanh ModuleInfo torch nn InstanceNorm d module_inputs_func=partial module_inputs_torch_nn_InstanceNormNd N= train_and_eval_differ=True skips= No channels_last support InstanceNorm d currently DecorateInfo unittest skip Skipped TestModule test_memory_format ModuleInfo torch nn InstanceNorm d module_inputs_func=partial module_inputs_torch_nn_InstanceNormNd N= train_and_eval_differ=True skips= No channels_last support InstanceNorm d currently DecorateInfo unittest skip Skipped TestModule test_memory_format ModuleInfo torch nn InstanceNorm d module_inputs_func=partial module_inputs_torch_nn_InstanceNormNd N= train_and_eval_differ=True skips= supported MPS backend DecorateInfo expectedFailureMPS TestModuleMPS test_memory_format DecorateInfo expectedFailureMPS TestModuleMPS test_non_contiguous_tensors DecorateInfo expectedFailureMPS TestModuleMPS test_forward DecorateInfo expectedFailureMPS TestModuleMPS test_non_contiguous DecorateInfo expectedFailureMPS TestModuleMPS test_save_load No channels_last support InstanceNorm d currently DecorateInfo unittest skip Skipped TestModule test_memory_format ModuleInfo torch nn LocalResponseNorm module_inputs_func=module_inputs_torch_nn_LocalResponseNorm ModuleInfo torch nn LayerNorm module_inputs_func=module_inputs_torch_nn_LayerNorm skips= No channels_last support LayerNorm currently DecorateInfo unittest skip Skipped TestModule test_memory_format ModuleInfo torch nn RMSNorm module_inputs_func=module_inputs_torch_nn_RMSNorm TransformerEncoder takes same inputs TransformerEncoderLayer ModuleInfo torch nn TransformerEncoder train_and_eval_differ=True module_inputs_func=module_inputs_torch_nn_TransformerEncoder decorators= Not implemented SDPA backward derivative DecorateInfo unittest skip Skipped TestModule test_gradgrad device_type= cpu skips= No channels_last support TransformerEncoderLayer currently DecorateInfo unittest skip Skipped TestModule test_memory_format Doesn t support device dtype kwargs directly because just container TransformerEncoderLayers DecorateInfo unittest expectedFailure TestModule test_factory_kwargs ModuleInfo torch nn TransformerEncoderLayer train_and_eval_differ=True module_inputs_func=module_inputs_torch_nn_TransformerEncoderLayer decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestModule test_non_contiguous_tensors device_type= cpu active_if=IS_WINDOWS DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestModule test_forward device_type= mps Not implemented SDPA backward derivative DecorateInfo unittest skip Skipped TestModule test_gradgrad device_type= cpu skips= No channels_last support TransformerEncoderLayer currently DecorateInfo unittest skip Skipped TestModule test_memory_format ModuleInfo torch nn TransformerDecoderLayer module_inputs_func=module_inputs_torch_nn_TransformerDecoderLayer decorators= Not implemented SDPA backward derivative DecorateInfo unittest skip Skipped TestModule test_gradgrad device_type= cpu skips= No channels_last support TransformerDecoderLayer currently DecorateInfo unittest skip Skipped TestModule test_memory_format ModuleInfo torch nn Transformer module_inputs_func=module_inputs_torch_nn_Transformer Inputs too large run slow gradcheck https github com pytorch pytorch issues gradcheck_fast_mode=True decorators= Not implemented SDPA backward derivative DecorateInfo unittest skip Skipped TestModule test_gradgrad device_type= cpu skips= No channels_last support Transformer currently DecorateInfo unittest skip Skipped TestModule test_memory_format ModuleInfo torch nn MultiheadAttention train_and_eval_differ=True module_inputs_func=module_inputs_torch_nn_MultiheadAttention skips= No channels_last support MultiheadAttention currently DecorateInfo unittest skip Skipped TestModule test_memory_format ModuleInfo torch nn Embedding module_inputs_func=module_inputs_torch_nn_Embedding decorators= DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestModule test_non_contiguous_tensors device_type= mps skips= DecorateInfo unittest skip Skipped TestModule test_memory_format ModuleInfo torch nn ReLU module_inputs_func=module_inputs_torch_nn_ReLU skips=None _macos _or_newer Fails backward check MPS See https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestModule test_memory_format active_if=operator itemgetter training device_type= mps ModuleInfo torch nn LeakyReLU module_inputs_func=module_inputs_torch_nn_LeakyReLU ModuleInfo torch nn ReLU module_inputs_func=module_inputs_torch_nn_ReLU skips= test fails MPS backend being investigated See https github com pytorch pytorch issues DecorateInfo skipMPS ModuleInfo torch nn PReLU module_inputs_func=module_inputs_torch_nn_PReLU skips= test fails MPS backend being investigated See https github com pytorch pytorch issues DecorateInfo skipMPS ModuleInfo torch nn RNNCell module_inputs_func=partial module_inputs_torch_nn_RNN_GRU_Cell is_rnn=True module_error_inputs_func=module_error_inputs_torch_nn_RNN_GRU_Cell ModuleInfo torch nn GRUCell module_inputs_func=module_inputs_torch_nn_RNN_GRU_Cell module_error_inputs_func=module_error_inputs_torch_nn_RNN_GRU_Cell ModuleInfo torch nn LSTMCell module_inputs_func=module_inputs_torch_nn_LSTMCell module_error_inputs_func=module_error_inputs_torch_nn_LSTMCell ModuleInfo torch nn Sigmoid module_inputs_func=module_inputs_torch_nn_Sigmoid skips=None _macos _or_newer Fails backward check MPS See https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestModule test_memory_format active_if=operator itemgetter training device_type= mps ModuleInfo torch nn LogSigmoid module_inputs_func=module_inputs_torch_nn_LogSigmoid skips= See tolerance issue DecorateInfo unittest expectedFailure TestModule test_forward device_type= mps dtypes= torch float ModuleInfo torch nn SiLU module_inputs_func=module_inputs_torch_nn_SiLU ModuleInfo torch nn Softmax module_inputs_func=module_inputs_torch_nn_Softmax ModuleInfo torch nn Softmax d module_inputs_func=module_inputs_torch_nn_Softmax d skips= no channels last support Softmax d currently DecorateInfo unittest skip Skipped TestModule test_memory_format See tolerance issue DecorateInfo unittest expectedFailure TestModule test_forward device_type= mps dtypes= torch float ModuleInfo torch nn LogSoftmax module_inputs_func=module_inputs_torch_nn_LogSoftmax skips= no channels last support LogSoftmax currently DecorateInfo unittest skip Skipped TestModule test_memory_format See inf nan error DecorateInfo unittest expectedFailure TestModule test_forward device_type= mps dtypes= torch float ModuleInfo torch nn Softmin module_inputs_func=module_inputs_torch_nn_Softmin skips= no channels last support Softmin currently DecorateInfo unittest skip Skipped TestModule test_memory_format ModuleInfo torch nn Softplus module_inputs_func=module_inputs_torch_nn_Softplus skips= test fails MPS backend being investigated See https github com pytorch pytorch issues DecorateInfo skipMPS ModuleInfo torch nn Softshrink module_inputs_func=module_inputs_torch_nn_Softshrink skips= supported MPS backend DecorateInfo skipMPS ModuleInfo torch nn Softsign module_inputs_func=module_inputs_torch_nn_Softsign ModuleInfo torch nn Tanh module_inputs_func=module_inputs_torch_nn_Tanh skips=None _macos _or_newer Fails backward check MPS See https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestModule test_memory_format active_if=operator itemgetter training device_type= mps ModuleInfo torch nn Tanhshrink module_inputs_func=module_inputs_torch_nn_Tanhshrink skips=None _macos _or_newer Fails backward check MPS See https github com pytorch pytorch issues DecorateInfo unittest expectedFailure TestModule test_memory_format active_if=operator itemgetter training device_type= mps ModuleInfo torch nn Threshold module_inputs_func=module_inputs_torch_nn_Threshold skips= test fails MPS backend being investigated See https github com pytorch pytorch issues DecorateInfo skipMPS ModuleInfo torch nn Mish module_inputs_func=module_inputs_torch_nn_Mish skips= supported MPS backend DecorateInfo skipMPS ModuleInfo torch nn RNN train_and_eval_differ=True module_inputs_func=partial module_inputs_torch_nn_RNN_GRU is_rnn=True module_error_inputs_func=module_error_inputs_torch_nn_RNN_GRU decorators=rnn_gru_lstm_module_info_decorators ModuleInfo torch nn GRU train_and_eval_differ=True module_inputs_func=partial module_inputs_torch_nn_RNN_GRU is_rnn=False module_error_inputs_func=module_error_inputs_torch_nn_RNN_GRU decorators=rnn_gru_lstm_module_info_decorators ModuleInfo torch nn LSTM train_and_eval_differ=True module_inputs_func=module_inputs_torch_nn_LSTM module_error_inputs_func=module_error_inputs_torch_nn_RNN_GRU skips= LSTM projections currently supported MPS DecorateInfo skipMPS decorators=rnn_gru_lstm_module_info_decorators ModuleInfo torch nn ReflectionPad d module_inputs_func=module_inputs_torch_nn_ReflectionPad d ModuleInfo torch nn ReflectionPad d module_inputs_func=module_inputs_torch_nn_ReflectionPad d gradcheck_nondet_tol=GRADCHECK_NONDET_TOL skips= DecorateInfo unittest skip Skipped TestModule test_memory_format device_type= cuda DecorateInfo unittest skip Skipped TestModule test_memory_format device_type= mps ModuleInfo torch nn ReflectionPad d module_inputs_func=module_inputs_torch_nn_ReflectionPad d gradcheck_nondet_tol=GRADCHECK_NONDET_TOL skips= DecorateInfo unittest skip Skipped TestModule test_memory_format device_type= cuda DecorateInfo unittest skip Skipped TestModule test_memory_format device_type= mps ModuleInfo torch nn ReplicationPad d module_inputs_func=module_inputs_torch_nn_ReplicationPad d ModuleInfo torch nn ReplicationPad d module_inputs_func=module_inputs_torch_nn_ReplicationPad d gradcheck_nondet_tol=GRADCHECK_NONDET_TOL skips= DecorateInfo unittest skip Skipped TestModule test_memory_format device_type= cuda DecorateInfo unittest skip Skipped TestModule test_memory_format device_type= mps ModuleInfo torch nn ReplicationPad d module_inputs_func=module_inputs_torch_nn_ReplicationPad d gradcheck_nondet_tol=GRADCHECK_NONDET_TOL skips= DecorateInfo unittest skip Skipped TestModule test_memory_format device_type= cuda DecorateInfo unittest skip Skipped TestModule test_memory_format device_type= mps ModuleInfo torch nn SELU module_inputs_func=module_inputs_torch_nn_SELU skips= test fails MPS backend being investigated See https github com pytorch pytorch issues DecorateInfo skipMPS ModuleInfo torch nn ZeroPad d module_inputs_func=module_inputs_torch_nn_ZeroPad d ModuleInfo torch nn ZeroPad d module_inputs_func=module_inputs_torch_nn_ZeroPad d skips= Fails channels last test MPS backend DecorateInfo unittest expectedFailure TestModule test_memory_format device_type= mps ModuleInfo torch nn ZeroPad d module_inputs_func=module_inputs_torch_nn_ZeroPad d skips= Fails channels last test MPS backend DecorateInfo unittest expectedFailure TestModule test_memory_format device_type= mps ModuleInfo torch nn CircularPad d module_inputs_func=module_inputs_torch_nn_CircularPad d module_error_inputs_func=module_error_inputs_torch_nn_Pad d ModuleInfo torch nn CircularPad d module_inputs_func=module_inputs_torch_nn_CircularPad d module_error_inputs_func=module_error_inputs_torch_nn_Pad d ModuleInfo torch nn CircularPad d module_inputs_func=module_inputs_torch_nn_CircularPad d module_error_inputs_func=module_error_inputs_torch_nn_Pad d skips= Fails channels last test MPS backend DecorateInfo unittest expectedFailure TestModule test_memory_format ModuleInfo torch nn ConstantPad d module_inputs_func=module_inputs_torch_nn_ConstantPad d ModuleInfo torch nn ConstantPad d module_inputs_func=module_inputs_torch_nn_ConstantPad d skips= Fails channels last test MPS backend DecorateInfo unittest expectedFailure TestModule test_memory_format device_type= mps ModuleInfo torch nn ConstantPad d module_inputs_func=module_inputs_torch_nn_ConstantPad d skips= Fails channels last test MPS backend DecorateInfo unittest expectedFailure TestModule test_memory_format device_type= mps