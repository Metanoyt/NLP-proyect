mypy allow-untyped-defs r Implementation NAdam algorithm typing cast Optional Union torch torch Tensor optimizer _capturable_doc _default_to_fused_or_foreach _differentiable_doc _disable_dynamo_if_unsupported _foreach_doc _get_capturable_supported_devices _get_scalar_dtype _get_value _maximize_doc _params_doc _stack_if_compiling _to_scalar _use_grad_for_differentiable _view_as_real Optimizer ParamsT __all__ = NAdam nadam NAdam Optimizer noqa D __init__ params ParamsT lr Union float Tensor = e- betas tuple float float = eps float = e- weight_decay float = momentum_decay float = e- decoupled_weight_decay bool = False foreach Optional bool = None maximize bool = False capturable bool = False differentiable bool = False noqa D isinstance lr Tensor lr numel = raise ValueError Tensor lr must -element = lr raise ValueError f Invalid learning rate lr = eps raise ValueError f Invalid epsilon value eps = betas raise ValueError f Invalid beta parameter index betas = betas raise ValueError f Invalid beta parameter index betas = weight_decay raise ValueError f Invalid weight_decay value weight_decay = momentum_decay raise ValueError f Invalid momentum_decay value momentum_decay defaults = lr lr betas betas eps eps weight_decay weight_decay momentum_decay momentum_decay decoupled_weight_decay decoupled_weight_decay maximize maximize foreach foreach capturable capturable differentiable differentiable super __init__ params defaults __setstate__ state noqa D super __setstate__ state group param_groups group setdefault maximize False group setdefault foreach None group setdefault capturable False group setdefault differentiable False group setdefault decoupled_weight_decay False p group params p_state = state get p len p_state = torch is_tensor p_state step step_val = float p_state step p_state step = torch tensor step_val dtype=_get_scalar_dtype device=p device group capturable torch tensor step_val dtype=_get_scalar_dtype torch is_tensor p_state mu_product mu_prod_val = p_state mu_product p_state mu_product = torch tensor mu_prod_val dtype=_get_scalar_dtype device=p device group capturable torch tensor mu_prod_val dtype=_get_scalar_dtype _init_group group params_with_grad grads exp_avgs exp_avg_sqs mu_products state_steps has_complex = False p group params p grad None has_complex &#124; = torch is_complex p params_with_grad append p p grad is_sparse raise RuntimeError NAdam does support sparse gradients grads append p grad state = state p Lazy state initialization len state == note crcrpar special device hosting step Deliberately host ` step ` ` mu_product ` CPU capturable False This because kernel launches costly CUDA XLA state step = torch zeros dtype=_get_scalar_dtype device=p device group capturable torch tensor dtype=_get_scalar_dtype state mu_product = torch ones dtype=_get_scalar_dtype device=p device group capturable torch tensor dtype=_get_scalar_dtype Exponential moving average gradient values state exp_avg = torch zeros_like p memory_format=torch preserve_format Exponential moving average squared gradient values state exp_avg_sq = torch zeros_like p memory_format=torch preserve_format exp_avgs append state exp_avg exp_avg_sqs append state exp_avg_sq mu_products append state mu_product state_steps append state step has_complex _use_grad_for_differentiable step closure=None Perform single optimization step Args closure Callable optional A closure reevaluates model returns loss _cuda_graph_capture_health_check loss = None closure None torch enable_grad loss = closure group param_groups params_with_grad list Tensor = grads list Tensor = exp_avgs list Tensor = exp_avg_sqs list Tensor = mu_products list Tensor = state_steps list Tensor = beta beta = cast tuple float float group betas has_complex = _init_group group params_with_grad grads exp_avgs exp_avg_sqs mu_products state_steps nadam params_with_grad grads exp_avgs exp_avg_sqs mu_products state_steps beta =beta beta =beta lr=group lr weight_decay=group weight_decay momentum_decay=group momentum_decay eps=group eps maximize=group maximize decoupled_weight_decay=group decoupled_weight_decay foreach=group foreach capturable=group capturable differentiable=group differentiable has_complex=has_complex loss NAdam __doc__ = r Implements NAdam algorithm math \begin aligned \rule mm pt \\ \textbf input \gamma_t \text lr \ \beta_ \beta_ \text betas \ \theta_ \text params \ f \theta \text objective \\ \hspace mm \ \lambda \text weight decay \ \psi \text momentum decay \\ \hspace mm \ \textit decoupled\_weight\_decay \ \textit maximize \\ \textbf initialize m_ \leftarrow \text first moment v_ \leftarrow \text second moment \\ - ex \rule mm pt \\ \textbf \ t= \ \textbf \ \ldots \ \textbf do \\ \hspace mm \textbf \ \textit maximize \\ \hspace mm g_t \leftarrow -\nabla_ \theta f_t \theta_ t- \\ \hspace mm \textbf \\ \hspace mm g_t \leftarrow \nabla_ \theta f_t \theta_ t- \\ \hspace mm \theta_t \leftarrow \theta_ t- \\ \hspace mm \textbf \ \lambda \neq \\ \hspace mm \textbf \ \textit decoupled\_weight\_decay \\ \hspace mm \theta_t \leftarrow \theta_ t- - \gamma \lambda \theta_ t- \\ \hspace mm \textbf \\ \hspace mm g_t \leftarrow g_t + \lambda \theta_ t- \\ \hspace mm \mu_t \leftarrow \beta_ \big - \frac ^ t \psi \big \\ \hspace mm \mu_ t+ \leftarrow \beta_ \big - \frac ^ t+ \psi \big \\ \hspace mm m_t \leftarrow \beta_ m_ t- + - \beta_ g_t \\ \hspace mm v_t \leftarrow \beta_ v_ t- + -\beta_ g^ _t \\ \hspace mm \widehat m_t \leftarrow \mu_ t+ m_t -\prod_ i= ^ t+ \mu_i \\ - ex \hspace mm + -\mu_t g_t -\prod_ i= ^ t \mu_ i \\ \hspace mm \widehat v_t \leftarrow v_t \big -\beta_ ^t \big \\ \hspace mm \theta_t \leftarrow \theta_t - \gamma \widehat m_t \big \sqrt \widehat v_t + \epsilon \big \\ \rule mm pt \\ - ex \bf \ \theta_t \\ - ex \rule mm pt \\ - ex \end aligned For further details regarding algorithm we refer ` Incorporating Nesterov Momentum into Adam ` _ + rf Args _params_doc lr float Tensor optional learning rate default e- betas Tuple float float optional coefficients used computing running averages gradient its square default eps float optional term added denominator improve numerical stability default e- weight_decay float optional weight decay L penalty default momentum_decay float optional momentum momentum_decay default e- decoupled_weight_decay bool optional whether decouple weight decay AdamW obtain NAdamW If True algorithm does accumulate weight decay momentum nor variance default False _foreach_doc _maximize_doc _capturable_doc _differentiable_doc _Incorporating Nesterov Momentum into Adam https openreview net forum id=OM jvwB jIp ZJjtNEZ _Decoupled Weight Decay Regularization https arxiv org abs _single_tensor_nadam params list Tensor grads list Tensor exp_avgs list Tensor exp_avg_sqs list Tensor mu_products list Tensor state_steps list Tensor beta float beta float lr float weight_decay float momentum_decay float eps float decoupled_weight_decay bool maximize bool capturable bool differentiable bool has_complex bool torch jit is_scripting lr = _to_scalar lr i param enumerate params grad = grads i maximize -grads i exp_avg = exp_avgs i exp_avg_sq = exp_avg_sqs i mu_product = mu_products i step_t = state_steps i torch is_complex param param = torch view_as_real param grad = torch view_as_real grad exp_avg = torch view_as_real exp_avg exp_avg_sq = torch view_as_real exp_avg_sq If compiling compiler will handle cudagraph checks see note torch compile x capturable torch compiler is_compiling capturable capturable_supported_devices = _get_capturable_supported_devices param device type == mu_product device type == step_t device type param device type capturable_supported_devices raise AssertionError f If capturable=True params mu_products state_steps must f supported devices capturable_supported_devices update step step_t += capturable step = step_t step = _get_value step_t bias_correction = - beta step weight_decay = decoupled_weight_decay Perform stepweight decay param mul_ - lr weight_decay grad = grad add param alpha=weight_decay calculate momentum cache \mu^ t \mu^ t+ mu = beta - step momentum_decay mu_next = beta - step + momentum_decay update mu_product mu_product = mu decay first second moment running average coefficient exp_avg lerp_ grad - beta exp_avg_sq mul_ beta addcmul_ grad grad value= - beta denom = exp_avg_sq div bias_correction sqrt differentiable capturable denom = denom add eps Make autograd track operations updating grad exp_avg directly using scalar value argument addcdiv mu_product_next = mu_product mu_next grad = grad -lr - mu - mu_product exp_avg = exp_avg -lr mu_next - mu_product_next param addcdiv_ grad denom param addcdiv_ exp_avg denom mu_product_next = _get_value mu_product mu_next denom add_ eps param addcdiv_ grad denom value= -lr - mu - _get_value mu_product param addcdiv_ exp_avg denom value=cast float -lr mu_next - mu_product_next _multi_tensor_nadam params list Tensor grads list Tensor exp_avgs list Tensor exp_avg_sqs list Tensor mu_products list Tensor state_steps list Tensor beta float beta float lr float weight_decay float momentum_decay float eps float decoupled_weight_decay bool maximize bool capturable bool differentiable bool has_complex bool len params == differentiable raise AssertionError _foreach ops don t support autograd If compiling compiler will handle cudagraph checks see note torch compile x capturable torch compiler is_compiling capturable capturable_supported_devices = _get_capturable_supported_devices supports_xla=False all p device type == mp device type == step device type p device type capturable_supported_devices p mp step zip params mu_products state_steps strict=True raise AssertionError If capturable=True params mu_products state_steps must supported devices f capturable_supported_devices lr = _to_scalar lr grouped_tensors = Optimizer _group_tensors_by_device_and_dtype params grads exp_avgs exp_avg_sqs mu_products state_steps type ignore list-item grouped_params_ grouped_grads_ grouped_exp_avgs_ grouped_exp_avg_sqs_ grouped_mu_products_ grouped_state_steps_ _ grouped_tensors values grouped_params = cast list Tensor grouped_params_ grouped_grads = cast list Tensor grouped_grads_ grouped_exp_avgs = cast list Tensor grouped_exp_avgs_ grouped_exp_avg_sqs = cast list Tensor grouped_exp_avg_sqs_ grouped_mu_products = cast list Tensor grouped_mu_products_ grouped_state_steps = cast list Tensor grouped_state_steps_ handle complex has_complex _view_as_real grouped_params grouped_grads grouped_exp_avgs grouped_exp_avg_sqs maximize grouped_grads = torch _foreach_neg grouped_grads type ignore assignment Update steps If steps CPU foreach will fall back slow path which for-loop calling t add over over will then wrapped into Tensor over over again which slower than we just wrapped once now The alpha required assure we go right overload torch compiler is_compiling grouped_state_steps is_cpu torch _foreach_add_ grouped_state_steps torch tensor device= cpu alpha= torch _foreach_add_ grouped_state_steps weight_decay = decoupled_weight_decay Perform stepweight decay torch _foreach_mul_ grouped_params - lr weight_decay Reuse intermediate memory grouped_grads already allocated maximize maximize torch _foreach_add_ grouped_grads grouped_params alpha=weight_decay grouped_grads = torch _foreach_add type ignore assignment grouped_grads grouped_params alpha=weight_decay Decay first second moment running average coefficient torch _foreach_lerp_ grouped_exp_avgs grouped_grads - beta torch _foreach_mul_ grouped_exp_avg_sqs beta torch _foreach_addcmul_ grouped_exp_avg_sqs grouped_grads grouped_grads - beta exp_avg_sq_sqrt = torch _foreach_sqrt grouped_exp_avg_sqs bias_correction_sqrt Union tuple Tensor list Tensor mus Union tuple Tensor list Tensor mu_nexts Union tuple Tensor list Tensor capturable mus will beta - step momentum_decay exponent = torch _foreach_mul grouped_state_steps momentum_decay mus = torch _foreach_pow exponent torch _foreach_mul_ mus - torch _foreach_add_ mus torch _foreach_mul_ mus beta mu_nexts will beta - step + momentum_decay torch _foreach_add_ exponent momentum_decay mu_nexts = torch _foreach_pow exponent torch _foreach_mul_ mu_nexts - torch _foreach_add_ mu_nexts torch _foreach_mul_ mu_nexts beta save peak memory we don t need exponent anymore del exponent bias_correction_sqrt = torch _foreach_pow beta grouped_state_steps foreach_sub doesn t allow scalar first arg torch _foreach_sub_ bias_correction_sqrt torch _foreach_neg_ bias_correction_sqrt torch _foreach_sqrt_ bias_correction_sqrt bias_correction_sqrt = - beta _get_value step step grouped_state_steps mus = beta - _get_value step momentum_decay step grouped_state_steps mu_nexts = beta - _get_value step + momentum_decay step grouped_state_steps update mu_products torch _foreach_mul_ grouped_mu_products mus torch _foreach_div_ exp_avg_sq_sqrt bias_correction_sqrt torch _foreach_add_ exp_avg_sq_sqrt eps explicitly delete bias_correction refs save memory del bias_correction_sqrt capturable Build up step_size multiplier grad reusing mus memory torch _foreach_sub_ mus torch _foreach_mul_ mus lr foreach_sub doesn t allow scalar first arg denom = torch _foreach_sub grouped_mu_products torch _foreach_neg_ denom torch _foreach_div_ mus denom - lr - mu - mu_product step_size_grads = mus explicitly delete denom save memory del denom Build up step_size multiplier exp_avg reusing mu_nexts memory denom = torch _foreach_mul grouped_mu_products mu_nexts torch _foreach_mul_ mu_nexts lr foreach_sub doesn t allow scalar first arg s okay because we need negative here anyway torch _foreach_sub_ denom torch _foreach_div_ mu_nexts denom - lr mu_next - mu_product mu_next step_size_expavg = mu_nexts explicitly delete denom save memory del denom we cannot inplace into step_size_grads cuz list ScalarTensors mul ing grouped_grads will result list bigger Tensors numerator = torch _foreach_mul step_size_grads grouped_grads torch _foreach_addcmul_ numerator step_size_expavg grouped_exp_avgs finally update params torch _foreach_addcdiv_ grouped_params numerator exp_avg_sq_sqrt step_size_grads = _stack_if_compiling _get_value lr - mu - _get_value mu_product - mu_product mu zip grouped_mu_products mus strict=True step_size_expavg = _stack_if_compiling _get_value lr mu_next - _get_value mu_product mu_next - mu_product mu_next zip grouped_mu_products mu_nexts strict=True torch _foreach_addcdiv_ grouped_params grouped_grads exp_avg_sq_sqrt step_size_grads type ignore arg-type torch _foreach_addcdiv_ grouped_params grouped_exp_avgs exp_avg_sq_sqrt step_size_expavg type ignore arg-type _disable_dynamo_if_unsupported single_tensor_fn=_single_tensor_nadam nadam params list Tensor grads list Tensor exp_avgs list Tensor exp_avg_sqs list Tensor mu_products list Tensor state_steps list Tensor kwonly args defaults supported functions compiled torchscript issue setting kwarg now functional API compiled torch distributed optim decoupled_weight_decay bool = False foreach Optional bool = None capturable bool = False differentiable bool = False has_complex bool = False maximize bool = False beta float beta float lr float weight_decay float momentum_decay float eps float r Functional API performs NAdam algorithm computation See ` ~torch optim NAdam ` details all isinstance t torch Tensor t state_steps raise RuntimeError API has changed ` state_steps ` argument must contain list singleton tensors all isinstance t torch Tensor t mu_products raise RuntimeError API has changed ` mu_products ` argument must contain list singleton tensors foreach None _ foreach = _default_to_fused_or_foreach params differentiable use_fused=False foreach torch jit is_scripting raise RuntimeError torch jit script supported foreach optimizers foreach torch jit is_scripting func = _multi_tensor_nadam func = _single_tensor_nadam func params grads exp_avgs exp_avg_sqs mu_products state_steps beta =beta beta =beta lr=lr weight_decay=weight_decay momentum_decay=momentum_decay maximize=maximize decoupled_weight_decay=decoupled_weight_decay eps=eps capturable=capturable differentiable=differentiable has_complex=has_complex