__future__ annotations dataclasses dataclass typing TYPE_CHECKING torchgen api cpp torchgen api types Binding CppSignature CppSignatureGroup torchgen gen pythonify_default torchgen model Argument BaseTy BaseType FunctionSchema ListType NativeFunction OptionalType Return Type Variant TYPE_CHECKING collections abc Iterable Sequence ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Data Models ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Notes python binding codegen The Python binding codegen produces code takes input list PyObjects finds matching ATen C++ function using PythonArgParser converts PyObjects into C++ types calls ATen C++ function + -------- + parsing + ------------------------ + binding + ----------------------- + &#124; PyObjs &#124; --------- &#124; PythonArgParser Output &#124; --------- &#124; Cpp Function Dispatch &#124; + -------- + + ------------------------ + + ----------------------- + The following examples demonstrate data models Python binding codegen needs deal tasks needs accomplish It helps understand purpose new data types we introduced below - Function Schema source truth aten empty names int size Dimname names ScalarType dtype=None Layout layout=None Device device=None bool pin_memory=None MemoryFormat memory_format=None - Tensor - Python Signature It s used generate input schema string PythonArgParser Note TensorOptions fields reordered additional requires_grad field added empty IntArrayRef size DimnameList names MemoryFormat memory_format=None ScalarType dtype=None Layout layout=torch strided Device device=None bool pin_memory=False bool requires_grad=False - C++ Signature It s used generate C++ lambda formals dispatch call Note scattered TensorOptions fields packed into options auto dispatch_empty = IntArrayRef size std optional DimnameList names const TensorOptions options std optional MemoryFormat memory_format - Tensor pybind gil_scoped_release no_gil torch empty size names options memory_format - Binding between Python Arguments C++ Arguments Given set Python Arguments scope we need produce binding expressions translate Python API into C++ API Python Args Cpp Args Binding Exprs ----------------------------------------------------------------- size size _r intlist names names names special init memory_format ------- + dtype ----- +- &#124; -- options options special packing layout &#124; device + -- memory_format _r memoryformatOptional pin_memory requires_grad -+ So full dispatch expression would look like dispatch_empty _r intlist names options _r memoryformatOptional Where does names come It involves special local init auto __names = _r toDimnameListOptional std optional DimnameList names = __names std make_optional DimnameList __names value std nullopt Where does options come It involves special local init TensorOptions Note Python side has additional requires_grad field const auto options = TensorOptions dtype _r scalartype device _r device layout _r layoutOptional requires_grad _r toBool pinned_memory _r toBool In some other cases one Python Argument can map multiple C++ Arguments For example aten max names_dim Tensor Dimname dim bool keepdim=False - Tensor values Tensor indices Python Args Cpp Args Binding Exprs --------------------------------------------------------------------- + ---- max out ----- max_values out input _r tensor dim dim _r dimname keepdim keepdim _r toBool out ----- + local init out _r tensorlist_n As demonstrated above binding can involve reordering packing unpacking special local inits Let s look concrete example static PythonArgParser parser abs Tensor input Tensor out=None ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^ + --- Python Schema represented PythonSignature PythonArgument traceable= true ParsedArgs parsed_args auto _r = parser parse nullptr args kwargs parsed_args _r isNone ~~~~~~~~~~~~ --- Scattered PythonArgParser output arg name = out represented PythonArgParserOutputExpr aten abs Tensor - Tensor ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^ + --- NativeFunction schema base version auto dispatch_abs = const Tensor - Tensor ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^ + --- dispatch_lambda_args dispatch_lambda_return_str generated NativeFunction CppSignature deprecated PythonSignature special arguments represented DispatchLambdaArgument pybind gil_scoped_release no_gil abs ~~~~~~~~~~~ --- cpp_dispatch_target cpp_dispatch_exprs generated NativeFunction CppSignature wrap dispatch_abs _r tensor ~~~~~~~~~~~~~ ^ + --- dispatch_lambda_exprs binding PythonArgParserOutputExpr python args DispatchLambdaArgument c++ args aten abs out Tensor Tensor out - Tensor ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^ + --- NativeFunction schema out-variant auto dispatch_abs_out = Tensor out const Tensor - Tensor pybind gil_scoped_release no_gil abs_out out wrap dispatch_abs_out _r tensor _r tensor Notes python interface codegen The python dataclasses below used used generate both python binding code pyi type hint signatures In theory these two should look very similar there number differences how pyi signatures vs python_arg_parser signatures generated These differences have been encapsulated signature_str vs signature_str_pyi display full signatures argument_str vs argument_str_pyi display arguments For examples only pyi signatures include types format_function_signature name str arguments Iterable str = return_type str &#124; None = None - str isinstance arguments list tuple arguments = tuple arguments return_type = f - return_type return_type None sig = f name join arguments return_type len sig = len arguments == tuple arguments == sig lines = f name f arg arg arguments f return_type sig = \n join lines all len line = line lines sig ruff format bug compound statements https github com astral-sh ruff issues use ` skip ` instead ` ` + ` off ` sig removesuffix + fmt skip\n dataclass frozen=True PythonReturns returns tuple Return dataclass frozen=True PythonArgument name str type Type default str &#124; None Used generate default init expr some PythonArgParser outputs e g _r layoutWithDefault layout_from_backend options backend ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^ + --- default_init str default_init str &#124; None Compute argument formal python argument parsing Needs consistent torch csrc utils python_arg_parser h argument_str method bool = False symint bool = True - str type_str = argument_type_str type symint=symint replace const replace name = name s input outside method bindings old codegen TODO remove doesn t rename codegen s just parse string name == type_str Tensor Number method name = input add default default None default = nullptr None std nullopt None std nullopt None None get default default f type_str name = default f type_str name argument_str_pyi method bool = False deprecated bool = False - str type_str = argument_type_str_pyi type name = name s input outside method bindings old codegen TODO remove doesn t rename codegen s just parse string name == type_str == Tensor method deprecated name = input name == Python keyword name += _ pyi merges _out functional variants into same signature optional out arg name == out type_str == Tensor deprecated type_str = f type_str &#124; None replace &#124; None &#124; None &#124; None pyi deprecated signatures don t get defaults their out arg treat_as_no_default = deprecated isinstance PythonOutArgument default == None add default default None treat_as_no_default isinstance type ListType type elem == BaseType BaseTy int default startswith default endswith default = + join map str strip default - split + default = nullptr None std nullopt None std nullopt None None c MemoryFormat Contiguous contiguous_format QScheme PER_TENSOR_AFFINE per_tensor_affine get default default f name type_str = default f name type_str dataclass frozen=True PythonOutArgument PythonArgument In Python signature multiple output fields packed into one out argument When binding C++ s first binded local out variable auto out = _r tensorlist_n then binded scattered C++ output arguments out out etc TODO maybe don t need keep scattered out fields python signature outputs tuple PythonArgument staticmethod from_outputs outputs tuple PythonArgument - PythonOutArgument &#124; None outputs None size = len outputs size == PythonOutArgument name=outputs name type=outputs type default= None default_init=None outputs=outputs size any type is_tensor_like outputs raise RuntimeError f Unsupported output type outputs PythonOutArgument name= out TODO shouldn t OptionalType ListType since defaults None type=ListType BaseType BaseTy Tensor size default= None default_init=None outputs=outputs raise AssertionError r Unexpected PythonOutArgument size dataclass frozen=True PythonSignature Base operator name without inplace outplace suffix name str Positional arguments TODO create dedicated SelfArgument type input_args tuple PythonArgument Keyword arguments excluding out argument scattered kwargs belonging TensorOptions dtype layout device pin_memory requires_grad etc input_kwargs tuple PythonArgument output_args PythonOutArgument &#124; None Return types which only used pyi returns PythonReturns These scattered kwargs arguments belonging TensorOptions When binding C++ they packed into TensorOptions object options It s possible C++ signature doesn t take TensorOptions object e g out variant which case they will used scattered fields without being packed into options TODO maybe create PythonTensorOptionsArgument tensor_options_args tuple PythonArgument method function signature method bool property deprecated - bool False arguments skip_outputs bool = False skip_tensor_options bool = False - tuple PythonArgument &#124; PythonOutArgument result list PythonArgument &#124; PythonOutArgument = result extend input_args result extend input_kwargs output_args None skip_outputs result append output_args skip_tensor_options result extend tensor_options_args tuple result arguments_count - int len arguments output_idx - int len input_args + len input_kwargs old codegen Compute Python function signature argument parsing specified torch csrc utils python_arg_parser h WARNING NOT same type signature specified PEP understood mypy our format independently developed has some quirks make more suitable specifically error parsing For translation mypy-valid type signatures see signature_str_pyi signature_str skip_outputs bool = False symint bool = True - str args = arguments skip_outputs=skip_outputs schema_formals list str = argument_str method=self method symint=symint args positional_argc = len input_args len schema_formals positional_argc schema_formals insert positional_argc f name join schema_formals signature_str_pyi skip_outputs bool = False - str args = arguments skip_outputs=skip_outputs schema_formals list str = argument_str_pyi method=self method args positional_argc = len input_args len schema_formals positional_argc schema_formals insert positional_argc only pyi signatures include returns returns_str = returns_str_pyi pyi also includes no typing defaults methods method schema_formals insert format_function_signature name schema_formals returns_str signature_str_pyi_vararg skip_outputs bool = False - str &#124; None only pyi uses vararg signatures args = arguments skip_outputs=skip_outputs schema_formals list str = argument_str_pyi method=self method args vararg only applies pyi signatures vararg variants generated all signatures num_args = arguments_count num_args == None num_positionalargs = len input_args vararg_type = args type isinstance vararg_type ListType str vararg_type elem int SymInt num_positionalargs == None Below major changes vararg vs regular pyi signatures vararg signatures also omit asterix assert isinstance vararg_type ListType schema_formals = + args name + + argument_type_str_pyi vararg_type elem returns_str = returns_str_pyi pyi also includes no typing defaults methods method schema_formals insert format_function_signature name schema_formals returns_str The deprecated python signature involves some special logic so create dedicated data model store these extra properties dataclass frozen=True PythonSignatureDeprecated PythonSignature Schema deprecated function deprecated_schema FunctionSchema The deprecated signature might miss some arguments corresponding C++ signature expects We need store constant default values pass For example deprecate signature addmm Scalar beta Tensor Tensor mat Tensor mat func schema aten addmm Tensor Tensor mat Tensor mat Scalar beta= Scalar alpha= - Tensor func call addmm mat mat beta We store mat mat beta case deprecated_args_exprs tuple str property deprecated - bool True signature_str skip_outputs bool = False symint bool = True - str PythonSignature signature_str skip_outputs=skip_outputs symint=symint + &#124; deprecated signature_str_pyi skip_outputs bool = False - str args = arguments skip_outputs=skip_outputs schema_formals list str = argument_str_pyi method=self method deprecated=True args positional_argc = len input_args len schema_formals positional_argc schema_formals insert positional_argc returns_str = returns_str_pyi format_function_signature name schema_formals returns_str signature_str_pyi_vararg skip_outputs bool = False - str &#124; None codegen doesn t include vararg variants deprecated signatures None This struct used hold PythonSignature its corresponding NativeFunction BEFORE grouping base out-variant functions Why store NativeFunction PythonSignature construct PythonSignature NativeFunction Because they - mapped One native function could have both deprecated non-deprecated python signatures - NativeFunction doesn t contain information construct deprecated python signature One python signature used handle both base out-variant function - see PythonSignatureGroup dataclass frozen=True PythonSignatureNativeFunctionPair signature PythonSignature function NativeFunction We merge pairs functions signatures equivalent mod output arguments use single entry python_arg_parser sig list both output arguments become optional dataclass frozen=True PythonSignatureGroup The signature used Python argument parsing The outplace signature preferred exists because can used parse inputs both out-place variant base version output omitted signature PythonSignature The regular ATen declaration e g conv d base NativeFunction The out variant e g conv d_out outplace NativeFunction &#124; None classmethod from_pairs cls functional PythonSignatureNativeFunctionPair out PythonSignatureNativeFunctionPair &#124; None - PythonSignatureGroup out None PythonSignatureGroup signature=functional signature base=functional function outplace=None prefer signature optional out= arguments because s superset can used parse input both base outplace signature_kwargs = out signature __dict__ copy Out overloads C++ don t have TensorOptions arguments so take these functional variant signature_kwargs tensor_options_args = functional signature tensor_options_args PythonSignatureGroup signature=type out signature signature_kwargs base=functional function outplace=out function C++ function dispatch wrapped lambda function The lambda function has almost same signature C++ function only some small variants - see details below This data model used represent arguments lambda function signature dataclass frozen=True DispatchLambdaArgument name str type_str str is_out_arg bool To pass PyObjects arguments C++ function via lambda wrapper we need first convert PyObjects into simple C++ objects This work done PythonArgParser This data model used represent output PythonArgParser It has - mapping PythonArgument PythonSignature dataclass frozen=True PythonArgParserOutputExpr argument name name str RHS expression reference PythonArgParser output expr str In some special cases we need create different expr e g _r isNone instead _r tensor index int The python argument maps argument PythonArgument property is_none_expr - str f _r isNone index To pass PythonArgParser output lambda wrapper we need bind PythonArgParserOutputExpr DispatchLambdaArgument They always - mapped e g scattered TensorOptions fields need packed into TensorOptions object which argument lambda function wrapper takes dataclass frozen=True DispatchLambdaArgumentExprs The exprs provide binding lambda arguments e g - _r tensor min - out min_indices - out options - options It has - mapping DispatchLambdaArgument exprs Sequence str Special local inits which might introduce new variables exprs above reference e g auto out = _r tensorlist_n inits Sequence str ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Helper Functions ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ _cpp_signature f NativeFunction method bool = False - CppSignature CppSignatureGroup from_native_function f method=method signature has_tensor_options f NativeFunction - bool f func arguments tensor_options None ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Python Signature ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ simple_type introduced old codegen which slightly different python schema type e g doesn t have suffix optional Tensor TensorList doesn t have size suffix list type argument_type_str t Type simple_type bool = False symint bool = True - str isinstance t BaseType t name == BaseTy int int _t t name == BaseTy float double t name == BaseTy str c string_view t name BaseTy Tensor BaseTy bool BaseTy QScheme BaseTy Scalar BaseTy ScalarType BaseTy Generator BaseTy Storage BaseTy Layout BaseTy Device BaseTy DeviceIndex BaseTy MemoryFormat BaseTy Dimname BaseTy Stream BaseTy SymInt These python schema type names line up their function schema names t name name isinstance t OptionalType elem = argument_type_str t elem simple_type=simple_type symint=symint f elem isinstance t ListType size = t size simple_type None str t elem == bool assert t size None f std array bool t size str t elem == int f IntArrayRef size size None IntArrayRef str t elem == SymInt symint f SymIntArrayRef size size None SymIntArrayRef f IntArrayRef size size None IntArrayRef str t elem == Tensor f TensorList size size None TensorList str t elem == Scalar f ScalarList size size None ScalarList str t elem == Tensor simple_type c List std optional Tensor const c List std optional Tensor str t elem == Dimname f DimnameList size size None DimnameList elem = argument_type_str t elem simple_type=simple_type symint=symint f ArrayRef elem raise RuntimeError f unrecognized type repr t argument_type_size t Type - int &#124; None l = t is_list_like l None str l elem = bool l size None argument Argument - PythonArgument PythonArgument name=a name type=a type TODO directly translate default python default default= str pythonify_default cpp default_expr default type symint=False default None None default_init=None Generates PythonSignature can used either pyi PythonArgParser codegen signature f NativeFunction method bool = False pyi bool = False - PythonSignature signature_from_schema f func category_override=f category_override method=method pyi=pyi signature_from_schema func FunctionSchema category_override str &#124; None method bool = False pyi bool = False - PythonSignature args list Argument = args extend func arguments pre_self_positional Skip SelfArgument method method func arguments self_arg None args append func arguments self_arg argument args extend func arguments post_self_positional args extend func arguments pre_tensor_options_kwarg_only Skip TensorOptionsArguments Python side TensorOptions arguments created based different rules - see below args extend func arguments post_tensor_options_kwarg_only args extend func arguments out input_arg_set = name func arguments flat_positional kwarg_only_set = name func arguments flat_kwarg_only out_arg_set = name func arguments out input_args = tuple map argument filter lambda name input_arg_set args input_kwargs = tuple map argument filter lambda name kwarg_only_set args outputs = tuple map argument filter lambda name out_arg_set args Reintroduce scattered fields TensorOptions Python Compared cpp counterpart python arguments have new property default_init new argument requires_grad which require some special handlings old codegen TODO because these aren t guaranteed faithful original versions yaml recreation potential source drift between eager JIT Pull logic out shared place has_tensor_input_arg = any type is_tensor_like func arguments flat_non_out any name == requires_grad func schema_order_arguments raise ValueError argument named requires_grad reserved should explicitly add schema old codegen probably won t work one returns tensor will produce compile-time error obvious has_tensor_return = any r type is_tensor_like r func returns name str = cpp name func is_factory_function = category_override == factory has_tensor_return has_tensor_input_arg is_like_or_new_function = category_override new like name startswith new_ name endswith _like is_dummy_function = category_override == dummy tensor_options_args list PythonArgument = is_factory_function is_like_or_new_function is_dummy_function topt_default_init name str - str &#124; None topt_args = func arguments tensor_options topt_args None None = getattr topt_args name default None default == None None cpp default_expr default type symint=False tensor_options_args append PythonArgument name= dtype type=OptionalType BaseType BaseTy ScalarType default= None default_init= None is_like_or_new_function topt_default_init dtype tensor_options_args append PythonArgument name= layout type=OptionalType BaseType BaseTy Layout default= None default_init= None is_like_or_new_function topt_default_init layout tensor_options_args append PythonArgument name= device type=OptionalType BaseType BaseTy Device default= None default_init= None is_like_or_new_function topt_default_init device torch tensors get_default_device tensor_options_args append PythonArgument name= pin_memory type=OptionalType BaseType BaseTy bool default= False default_init=None tensor_options_args append PythonArgument name= requires_grad type=OptionalType BaseType BaseTy bool default= False default_init=None returns = PythonReturns returns=func returns PythonSignature name=str func name name input_args=input_args input_kwargs=input_kwargs output_args=PythonOutArgument from_outputs outputs tensor_options_args=tuple tensor_options_args returns=returns method=method ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Python Interface ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ structseq_fieldnames returns tuple Return - list str len returns = all r name None r returns any r name None r returns When building Windows ` PyStructSequence_UnnamedField ` could resolved linker some reason which cause error building python_nn_functions cpp obj error LNK unresolved external symbol PyStructSequence_UnnamedField Thus point time we do support unnamed fields structseq you must either name all fields none them raise ValueError Unnamed field supported codegen str r name r returns argument_type_str_pyi t Type - str add_optional = False isinstance t OptionalType t = t elem add_optional = True ret = isinstance t BaseType t name BaseTy int BaseTy DeviceIndex ret = _int t name == BaseTy SymInt ret = _int &#124; SymInt t name == BaseTy float ret = _float t name == BaseTy str ret = str t name == BaseTy Scalar ret = Number &#124; _complex t name == BaseTy ScalarType ret = _dtype t name == BaseTy bool ret = _bool t name == BaseTy QScheme ret = _qscheme t name == BaseTy Layout ret = _layout t name == BaseTy Device ret = DeviceLikeType &#124; None t name == BaseTy MemoryFormat ret = memory_format t name == BaseTy Dimname ret = str &#124; EllipsisType &#124; None t name == BaseTy Storage ret = Storage &#124; UntypedStorage t name BaseTy Tensor BaseTy Generator BaseTy Stream These python schema type names line up their function schema names ret = t name name isinstance t ListType str t elem == int ret = _int &#124; _size t size None _size t is_tensor_like TODO doesn t seem right Tensor currently translates tuple Tensor &#124; list Tensor &#124; None It should probably translate tuple Tensor &#124; None &#124; list Tensor &#124; None add_optional = True ret = Tensor &#124; tuple Tensor &#124; list Tensor t size None tuple Tensor &#124; list Tensor str t elem == float ret = Sequence _float str t elem == SymInt t size None elem = argument_type_str_pyi t elem ret = f elem &#124; Sequence elem elem = argument_type_str_pyi t elem ret = f Sequence elem raise RuntimeError f unrecognized type repr t add_optional ret = f ret &#124; None replace &#124; None &#124; None &#124; None ret return_type_str_pyi t Type - str Where arguments open accepting Union types should concrete types isinstance t OptionalType inner = return_type_str_pyi t elem f inner &#124; None replace &#124; None &#124; None &#124; None isinstance t BaseType t name == BaseTy Device _device t name == BaseTy Dimname str &#124; None argument_type_str_pyi t isinstance t ListType inner = return_type_str_pyi t elem f tuple inner argument_type_str_pyi t returns_structseq_pyi signature PythonSignature - tuple str str &#124; None python_returns = return_type_str_pyi r type r signature returns returns structseq_name = signature name field_names = structseq_fieldnames signature returns returns field_names These types structseq objects which act like named NamedTuples constructor acts like constructor tuple Using typing NamedTuple does allow us override __init__ seq_type = f tuple join python_returns structseq_def_lines = f structseq_name seq_type fmt skip name ret_type zip field_names python_returns structseq_def_lines extend property f name - ret_type structseq_def_lines extend __new__ cls f sequence seq_type - Self fmt skip f n_fields Final _int = len field_names f n_sequence_fields Final _int = len field_names n_unnamed_fields Final _int = __init_subclass__ cls - NoReturn prohibit subclassing add extra newline structseq_def = \n join structseq_def_lines Example structseq_def = max tuple Tensor Tensor fmt skip\n property\n values - Tensor \n property\n indices - Tensor \n __new__ \n cls \n sequence tuple Tensor Tensor \n - Self fmt skip\n \n n_fields Final _int = n_sequence_fields Final _int = n_unnamed_fields Final _int = __init_subclass__ cls - NoReturn prohibit subclassing structseq_name structseq_def None returns_str_pyi signature PythonSignature - str field_names = structseq_fieldnames signature returns returns field_names f torch return_types signature name python_returns = return_type_str_pyi r type r signature returns returns len python_returns tuple + join python_returns + len python_returns == python_returns None ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ C++ Function Dispatch ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ This section provides APIs generate code does C++ function dispatch The C++ function call wrapped lambda function For example aten selu_ Tensor - Tensor auto dispatch_selu_ = Tensor - Tensor pybind gil_scoped_release no_gil selu_ The lambda function s signature follows C++ signature common cases e g aten add Tensor Tensor Tensor other Scalar alpha= - Tensor const Tensor const Tensor other Scalar alpha - Tensor For out variant out argument s type changed Tensor Tensor It s because when calling lambda passes PythonArgParser output _r tensor which stack allocated object needs pass value Also see comments dispatch_lambda_return_str aten add out Tensor Tensor other Scalar alpha= Tensor out - Tensor Tensor out const Tensor const Tensor other Scalar alpha - Tensor For multi-output case can keep using reference type because PythonArgParser output has been unpacked local variables e g aten max names_dim_max Tensor Dimname dim bool keepdim=False Tensor max Tensor b max_values - Tensor values Tensor b indices Tensor max Tensor max_values const Tensor Dimname dim bool keepdim - std tuple Tensor Tensor For deprecated python signature should follow deprecated python arg order TODO This keep same byte-for-byte result old codegen - maybe unnecessary dispatch_lambda_args ps PythonSignature f NativeFunction symint bool = True - tuple DispatchLambdaArgument isinstance ps PythonSignatureDeprecated schema = ps deprecated_schema schema = f func Start cpp arguments - dispatch lambda signature always include cpp_args = cpp arguments arguments=schema arguments faithful=False symint=symint method=False cpp_no_default_args=f cpp_no_default_args out_args set str = name schema arguments out Convert cpp argument lambda argument dispatch_lambda_arg cpp_arg Binding - DispatchLambdaArgument type_str = cpp_arg type is_out_arg = cpp_arg name out_args ps method cpp_arg name == For method s we can use const Tensor simply ignore mutability type_str = const Tensor For other cases we need prevent dangling refs temps unless s unpacked scattered output The reason explained comments above dispatch_lambda_return_str TODO avoid special handling ensure_temp_safe = len out_args = is_out_arg ensure_temp_safe type_str = Tensor Tensor get type_str type_str DispatchLambdaArgument name=cpp_arg name type_str=type_str is_out_arg=is_out_arg tuple map dispatch_lambda_arg cpp_args old codegen XXX you got here because assertion failure doesn t mean s enough just extend list here Before you do make sure add appropriate wrap overload torch csrc autograd utils wrap_outputs h SUPPORTED_RETURN_TYPES = Tensor std tuple Tensor Tensor std tuple Tensor Tensor Tensor std tuple Tensor Tensor Tensor Tensor std tuple Tensor Tensor Tensor Tensor Tensor std tuple Tensor Tensor Tensor Tensor Tensor Tensor std tuple Tensor Tensor Tensor int _t std tuple Tensor Tensor double int _t std tuple Tensor Tensor Tensor Tensor int _t std tuple Tensor Tensor double Tensor int _t std tuple double int _t std tuple Tensor std vector Tensor std vector Tensor Needed flash attention forw backward std tuple Tensor Tensor Tensor Tensor c SymInt c SymInt Tensor Tensor Tensor Scalar bool int _t void void QScheme double IntArrayRef ScalarType Stream dispatch_lambda_return_str f NativeFunction - str old codegen Remove type annotation e g Tensor rather than Tensor because dispatch lambdas take mutable arguments value reference If you then reference such argument you will now have pointer dangling stack entry Not good You want auto dispatch_selu_ = Tensor - Tensor selu_ ^^^^^^ auto dispatch_selu_ = Tensor - Tensor selu_ ^^^^^^^ NB We can t make dispatch_selu_ take Tensor because enclosing codegen looks like dispatch_selu_ _r tensor you can t take mutable reference temporary Maybe we could assign variable itself returns_without_annotation = tuple Return r name r type None r f func returns return_str = cpp returns_type returns_without_annotation symint=True cpp_type return_str SUPPORTED_RETURN_TYPES raise RuntimeError f f func name returns unsupported type return_str return_str cpp_dispatch_target f NativeFunction - str symint = f func has_symint name = cpp name f func symint_overload=symint Variant method f variants f name Variant function f variants has_tensor_options f f func name name base endswith _like namespace = torch namespace = f namespace name raise RuntimeError f could dispatch neither function nor method f func cpp_dispatch_exprs f NativeFunction python_signature PythonSignature &#124; None = None - tuple str cpp_args Sequence Binding = _cpp_signature f method=False arguments exprs tuple str = isinstance python_signature PythonSignatureDeprecated By default exprs consistent C++ signature exprs = tuple name cpp_args For deprecated python signature we may need fill some constants exprs = tuple filter lambda n n = out f func is_out_fn python_signature deprecated_args_exprs Variant method f variants exprs = tuple filter __ne__ exprs exprs ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Python C++ Args Binding ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ We explicitly enumerate PythonArgParser unpacking methods all supported types This might more verbose than necessary partially because irregularity unpacking method naming partially because we want mimic old codegen behavior - reject unexpected unsupported cases which old codegen rejects For certain cases intentionally more restrictive than necessary e g doesn t accepts doublelist definite size arg_parser_unpack_method t Type default str &#124; None default_init str &#124; None symint bool = True - str has_default_init = default_init None has_default_init str t ScalarType ScalarType Device Device Layout Layout bool bool raise RuntimeError f type t does supported unpacking default isinstance t BaseType t name BaseTy Tensor BaseTy Stream BaseTy Storage BaseTy Scalar BaseTy Dimname These unpack methods line up their schema names t name name lower t name == BaseTy ScalarType scalartypeWithDefault has_default_init scalartype t name == BaseTy Device deviceWithDefault has_default_init device t name == BaseTy DeviceIndex toInt t name == BaseTy int toInt t name == BaseTy SymInt toSymInt symint toInt t name == BaseTy bool toBoolWithDefault has_default_init toBool t name == BaseTy float toDouble t name == BaseTy str stringView t name == BaseTy Layout layoutWithDefault has_default_init layout t name == BaseTy MemoryFormat memoryformat isinstance t OptionalType str t elem == Tensor optionalTensor str t elem == Generator generator str t elem == Dimname toDimnameListOptional has_default_init default None None std nullopt std nullopt If default None append Optional elem s unpacking method arg_parser_unpack_method t elem None None symint=symint + Optional Otherwise load underlying type default arg_parser_unpack_method t elem default default_init symint=symint isinstance t ListType str t elem == Tensor accept use definite size f tensorlist_n t size t size None tensorlist str t elem == Tensor list_of_optional_tensors str t elem == Dimname accept definite size dimnamelist str t elem == int accept definite size intlist str t elem == float doublelist str t elem == SymInt accept definite size symintlist symint intlist str t elem == Scalar scalarlist raise RuntimeError f type t supported PythonArgParser Return RHS expression python argument using PythonArgParser output e g arg name foo arg type bool arg_index = returns _r toBool arg_parser_output_expr arg_index int PythonArgument symint bool = True - PythonArgParserOutputExpr has_default = default_init None unpack_method = arg_parser_unpack_method t=a type default=a default default_init=a default_init symint=symint default = f default_init has_default expr = f _r unpack_method arg_index default PythonArgParserOutputExpr name=a name expr=expr index=arg_index argument=a Returns map key = arg_name value = PythonArgParserOutputExpr arg_parser_output_exprs ps PythonSignature f NativeFunction symint bool = True - dict str PythonArgParserOutputExpr e name e i enumerate ps arguments e arg_parser_output_expr i symint=symint argument name type scattered tensor options fields TENSOR_OPTIONS_FIELDS = dtype ScalarType device Device layout Layout pin_memory bool requires_grad bool bind arg parser outputs python args dispatch lambda arguments c++ args dispatch_lambda_exprs ps PythonSignature f NativeFunction symint bool = True - DispatchLambdaArgumentExprs This method bind arg_parser_outputs lambda_args producing inits lambda_args_exprs each lambda argument using arg parser outputs arg_parser_outputs = arg_parser_output_exprs ps f symint=symint lambda_args = dispatch_lambda_args ps f symint=symint inits list str = lambda_args_exprs dict str str = has_toptions = has_tensor_options f special inits unpacking provide binding exprs lambda arguments ps arguments skip_tensor_options=True name = name arg_parser_expr = arg_parser_outputs name expr has_toptions name == TODO why needs special case inits extend f auto = arg_parser_expr lambda_args_exprs name = name isinstance PythonOutArgument len outputs f func is_out_fn inits extend f auto out = arg_parser_expr i out_arg enumerate outputs lambda_args_exprs out_arg name = f out i str type == Dimname old codegen TODO make part something more general get rid optional ArrayRef T special The PythonArgParser returns optional vector T which cannot implicitly converted optional ArrayRef T One needs unwrap optional rewrap inits extend f auto __ name = arg_parser_expr f std optional DimnameList name = __ name std make_optional DimnameList __ name value std nullopt noqa B lambda_args_exprs name = name default case - directly using PythonArgParser output expr lambda_args_exprs name = arg_parser_expr method s passed directly python binding rather than parsed ps method lambda_args_exprs = special packing checking TensorOptions tensor_options_args_names = name ps tensor_options_args has_toptions f func is_out_fn raise RuntimeError f f func tensor options output arg ps tensor_options_args name TENSOR_OPTIONS_FIELDS raise RuntimeError f f func unrecognized tensor options field name python binding arguments str type = TENSOR_OPTIONS_FIELDS get name raise RuntimeError f f func unrecognized type str type tensor options field name all tensor_options_args_names TENSOR_OPTIONS_FIELDS raise RuntimeError f f func incomplete tensor options args tensor_options_args_names inits append f \ const auto options = TensorOptions dtype arg_parser_outputs dtype expr device arg_parser_outputs device expr layout arg_parser_outputs layout expr requires_grad arg_parser_outputs requires_grad expr pinned_memory arg_parser_outputs pin_memory expr torch utils maybe_initialize_device options lambda_args_exprs options = options special case - access scattered TensorOptions fields without packing TODO maybe move generator side s related binding has_toptions tensor_options_args_names dtype tensor_options_args_names we re output-arg variant check these args against output tensor f func is_out_fn raise RuntimeError f f func dtype tensor_options_args without output arg ps ps arguments all tensor_options_args_names layout device raise RuntimeError f f func incomplete tensor options output check inits append f \ check_out_type_matches arg_parser_outputs out expr arg_parser_outputs dtype expr arg_parser_outputs dtype is_none_expr arg_parser_outputs layout expr arg_parser_outputs device expr arg_parser_outputs device is_none_expr we ll set requires_grad outgoing tensor requires_grad tensor_options_args_names raise RuntimeError f f func expected requires_grad tensor_options_args absent found tensor_options_args_names DispatchLambdaArgumentExprs exprs=tuple lambda_args_exprs name lambda_args inits=inits