__future__ annotations dataclasses dataclass typing TYPE_CHECKING torchgen api ufunc ufunc torchgen api translate translate torchgen api types BaseCType Binding CType Expr NamedCType opmath_t scalar_t StructuredImplSignature VectorizedCType torchgen context with_native_function torchgen model Argument BaseTy BaseType DispatchKey NativeFunctionsGroup ScalarType UfuncKey torchgen utils OrderedSet TYPE_CHECKING collections abc Sequence torchgen api ufunc UfunctorBindings ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CUDA STUFF ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ NB bothering generate dispatch stub forward declaration header we can just paste wherever necessary TODO use BackendIndex dispatch_key DispatchKey only CPU CUDA right now Represents functors implementing CUDA ufuncs Functors templated scalar_t because when USERS instantiate functors they templated A functor looks something like template typename scalar_t struct CUDAFunctorOnSelf_add using opmath_t = opmath_type scalar_t opmath_t other_ opmath_t alpha_ CUDAFunctorOnSelf_add opmath_t other opmath_t alpha other_ other alpha_ alpha __device__ scalar_t operator scalar_t ufunc add static_cast opmath_t other_ alpha_ dataclass frozen=True UfunctorSignature g NativeFunctionsGroup scalar_tensor_idx int &#124; None name str arguments - UfunctorBindings ufunc ufunctor_arguments g scalar_tensor_idx=self scalar_tensor_idx scalar_t=scalar_t fields - list Binding fields renamed have trailing underscore conventional b rename f b name _ b arguments ctor returns_type - CType TODO don t hardcode type will inferred based tags native function BaseCType scalar_t decl_fields - str \n join f f type f name f fields inline_defn_ctor - str args_str = join decl arguments ctor NB hypothetically could do translate transition here very regular init_str = join f name _ name arguments ctor f name args_str init_str decl_apply - str args_str = join decl arguments apply f returns_type cpp_type operator args_str const dataclass frozen=True UfuncSignature g NativeFunctionsGroup name str compute_t CType arguments - list Binding ufunc ufunc_arguments g compute_t=self compute_t call ctx Sequence Binding &#124; Expr - str f name join expr translate ctx arguments steps take functional signature use api ufunc convert template signature establishes type template function use api ufunc II generate split struct operator signature establish context which we call template signature StructuredImplSignature context ~ functor constructor sig Functor constructor context ~ functor fields sig Functor apply context functor fields + functor apply sig ~ template sig eligible_for_binary_scalar_specialization g NativeFunctionsGroup - bool num_tensors = sum g functional func arguments flat_non_out type is_tensor_like num_tensors == compute_ufunc_cuda_functors g NativeFunctionsGroup - tuple dict ScalarType dict UfuncKey UfunctorSignature str First build functors ufunctor_sigs dict ScalarType dict UfuncKey UfunctorSignature = ufunctors list str = loops = g out ufunc_inner_loop scalar_tensor_idx_lookup = UfuncKey CUDAFunctorOnSelf UfuncKey CUDAFunctorOnOther UfuncKey CUDAFunctor None eligible_for_binary_scalar_specialization g keys = UfuncKey CUDAFunctorOnSelf UfuncKey CUDAFunctorOnOther UfuncKey CUDAFunctor keys = UfuncKey CUDAFunctor k UfuncKey CUDAFunctorOnSelf UfuncKey CUDAFunctorOnOther assert k loops f cannot use k non-binary function k keys If key directly defined skip functor codegen we assume user already done us k loops ufunctor_sig = UfunctorSignature g scalar_tensor_idx=scalar_tensor_idx_lookup k name=loops k name dtype loops k supported_dtypes ufunctor_sigs setdefault dtype k = ufunctor_sig continue Note ScalarOnly Generic must match names CUDA ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Otherwise look ANY generic entries For simplicity codegen both ScalarOnly Generic defined ufunc name must match they didn t match we d have generate distinct functors per dtype which awful so we re going do unless someone really forces us ufunc_name = None supported_dtypes OrderedSet ScalarType = OrderedSet lk UfuncKey ScalarOnly UfuncKey Generic lk loops continue ufunc_name None ufunc_name = loops lk name See Note ScalarOnly Generic must match names CUDA assert ufunc_name == loops lk name ScalarOnly Generic must have same ufunc name supported_dtypes &#124; = loops lk supported_dtypes assert ufunc_name None name = f k _ ufunc_name ufunctor_sig = UfunctorSignature g scalar_tensor_idx=scalar_tensor_idx_lookup k name=name dtype supported_dtypes ufunctor_sigs setdefault dtype k = ufunctor_sig ufunc_sig = UfuncSignature g name=f ufunc ufunc_name compute_t=BaseCType opmath_t apply_ctx = ufunctor_sig fields + ufunctor_sig arguments apply ufunctors append f template typename scalar_t struct ufunctor_sig name using opmath_t = opmath_type scalar_t ufunctor_sig decl_fields ufunctor_sig inline_defn_ctor __device__ ufunctor_sig decl_apply ufunc_sig call apply_ctx ufunctor_sigs \n join ufunctors dataclass frozen=True BinaryScalarSpecializationConfig scalar_idx int ctor_tensor str ufunc_key UfuncKey BinaryScalarSpecializationConfigs = BinaryScalarSpecializationConfig scalar_idx= ctor_tensor= ufunc_key=UfuncKey CUDAFunctorOnOther BinaryScalarSpecializationConfig scalar_idx= ctor_tensor= other ufunc_key=UfuncKey CUDAFunctorOnSelf compute_ufunc_cuda_dtype_body g NativeFunctionsGroup dtype ScalarType inner_loops dict UfuncKey UfunctorSignature parent_ctx Sequence Binding - str body = using opmath_t = opmath_type scalar_t body += false \n ease codegen config BinaryScalarSpecializationConfigs config ufunc_key inner_loops continue ufunctor_sig = inner_loops config ufunc_key scalar_idx = config scalar_idx + Make copy same time widen type permissible without copy we don t want mutate input argument anyway ctx list Expr &#124; Binding = list parent_ctx ctx append Expr expr=f iter scalar_value opmath_t scalar_idx type=NamedCType config ctor_tensor BaseCType opmath_t ufunctor_ctor_exprs_str = join expr translate ctx ufunctor_sig arguments ctor NB ufunctor must allocated before iter remove_operand called relies iter body += f \ iter is_cpu_scalar scalar_idx ufunctor_sig name scalar_t ufunctor ufunctor_ctor_exprs_str iter remove_operand scalar_idx gpu_kernel iter ufunctor ufunctor_sig = inner_loops UfuncKey CUDAFunctor ufunctor_ctor_exprs_str = join expr translate parent_ctx ufunctor_sig arguments ctor body += f gpu_kernel iter ufunctor_sig name scalar_t ufunctor_ctor_exprs_str body with_native_function compute_ufunc_cuda g NativeFunctionsGroup - str First build functors indexing them dtype ufunctor_sigs ufunctors = compute_ufunc_cuda_functors g Next build conditionals sig = StructuredImplSignature g ufunc kernel_name g DispatchKey CUDA dtype_cases = dtype inner_ufunc_sigs ufunctor_sigs items dtype_cases append f AT_DISPATCH_CASE ScalarType dtype compute_ufunc_cuda_dtype_body g dtype inner_ufunc_sigs sig arguments dtype_cases_str = \n join dtype_cases stub_sig = StubSignature g f ufunctors stub_sig type_defn stub_sig dispatch_decl stub_sig kernel_defn AT_DISPATCH_SWITCH iter common_dtype sig name dtype_cases_str REGISTER_DISPATCH stub_sig name stub_sig kernel_name sig defn stub_sig direct_call sig arguments ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CPU STUFF ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ dataclass frozen=True StubSignature g NativeFunctionsGroup property name - str f str g functional func name name _stub property kernel_name - str f str g functional func name name _kernel property type_name - str f str g functional func name name _fn arguments - list Binding ufunc stub_arguments g type - str cpp_args = arguments f void TensorIteratorBase join type cpp_args dispatch_decl - str f DECLARE_DISPATCH type_name name dispatch_defn - str f DEFINE_DISPATCH name kernel_defn - str f void kernel_name TensorIteratorBase iter join defn arguments type_defn - str f using type_name = type must called context where TensorIteratorBase call ctx Sequence Binding - str f name device_type join expr translate ctx arguments used CUDA skip unnecessary dynamic dispatch direct_call ctx Sequence Binding - str f kernel_name join expr translate ctx arguments with_native_function compute_ufunc_cpu g NativeFunctionsGroup - str stub_sig = StubSignature g sig = StructuredImplSignature g ufunc kernel_name g DispatchKey CPU f stub_sig type_defn stub_sig dispatch_decl stub_sig dispatch_defn sig defn stub_sig call sig arguments compute_ufunc_cpu_dtype_body g NativeFunctionsGroup dtype ScalarType inner_loops dict UfuncKey UfuncSignature parent_ctx Sequence Binding - str assert UfuncKey CPUScalar inner_loops f dtype inner_loops keys assert inner_loops keys = UfuncKey CPUScalar UfuncKey CPUVector scalar_loop = inner_loops UfuncKey CPUScalar vec_loop = None UfuncKey CPUVector inner_loops vec_loop = inner_loops UfuncKey CPUVector NB We DON T use translate here because translate incapable CSE ing scalar accesses case also used Vectorized also unpacking here very simple only affects Scalar everything implicitly captured lambda Setup scalar scope body = ctx = b parent_ctx isinstance b argument Argument b argument type = BaseType BaseTy Scalar continue body append f auto _s_ b name = b name scalar_t ctx append Expr f _s_ b name NamedCType b nctype name BaseCType scalar_t vec_loop None b parent_ctx isinstance b argument Argument b argument type = BaseType BaseTy Scalar continue body append f auto _v_ b name = vec Vectorized scalar_t _s_ b name ctx append Expr f _v_ b name NamedCType b nctype name VectorizedCType BaseCType scalar_t Setup lambda signature NB simplified version ufunctor_arguments scalar_bindings = vec_bindings = g functional func arguments flat_non_out type is_tensor_like continue assert type == BaseType BaseTy Tensor scalar_bindings append Binding name=a name nctype=NamedCType name BaseCType scalar_t argument=a vec_loop None vec_bindings append Binding name=a name nctype=NamedCType name VectorizedCType BaseCType scalar_t argument=a with_ctx b Sequence Binding - list Expr &#124; Binding r list Expr &#124; Binding = r extend ctx r extend b r body_str = \n join body vec_loop None f body_str cpu_kernel_vec iter = join b decl b scalar_bindings scalar_loop call with_ctx scalar_bindings = join b decl b vec_bindings vec_loop call with_ctx vec_bindings f body_str cpu_kernel iter = join b decl b scalar_bindings scalar_loop call with_ctx scalar_bindings with_native_function compute_ufunc_cpu_kernel g NativeFunctionsGroup - str stub_sig = StubSignature g Reindex ufunc dtypes processing generic scalaronly well loops = g out ufunc_inner_loop ufunc_sigs dict ScalarType dict UfuncKey UfuncSignature = k UfuncKey CPUScalar UfuncKey CPUVector lks = ORDER MATTERS specifies overriding precedence k loops should happen rarely lks append k UfuncKey ScalarOnly loops k UfuncKey CPUScalar lks append UfuncKey ScalarOnly UfuncKey Generic loops lks append UfuncKey Generic TODO don t hardcode ufunc namespace here should centralized smh lk lks dtype loops lk supported_dtypes compute_t CType k UfuncKey CPUScalar compute_t = BaseCType scalar_t k UfuncKey CPUVector compute_t = VectorizedCType BaseCType scalar_t raise AssertionError inner_ufunc_sigs = ufunc_sigs setdefault dtype k inner_ufunc_sigs inner_ufunc_sigs k = UfuncSignature g name=f ufunc loops lk name compute_t=compute_t Build conditionals dtype_cases = dtype inner_ufunc_sigs ufunc_sigs items dtype_cases append f AT_DISPATCH_CASE ScalarType dtype compute_ufunc_cpu_dtype_body g dtype inner_ufunc_sigs stub_sig arguments dtype_cases_str = \n join dtype_cases f namespace stub_sig kernel_defn AT_DISPATCH_SWITCH iter common_dtype stub_sig name dtype_cases_str anonymous namespace stub_sig type_defn stub_sig dispatch_decl REGISTER_DISPATCH stub_sig name stub_sig kernel_name