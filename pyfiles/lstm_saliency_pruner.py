typing Any cast torch torch nn base_structured_sparsifier BaseStructuredSparsifier parametrization FakeStructuredSparsity LSTMSaliencyPruner BaseStructuredSparsifier Prune packed LSTM weights based saliency For each layer k inside LSTM we have two packed weight matrices - weight_ih_l k - weight_hh_l k These tensors pack weights linear layers together efficiency W_ii &#124; W_if &#124; W_ig &#124; W_io Pruning tensor directly will lead weights being misassigned when unpacked To ensure each packed linear layer pruned same amount We split packed weight into constituent linear parts Update mask each individual piece using saliency individually This applies both weight_ih_l k weight_hh_l k update_mask module nn Module tensor_name str kwargs Any - None weights = getattr module tensor_name p getattr module parametrizations tensor_name isinstance p FakeStructuredSparsity mask = cast torch Tensor p mask select weights based magnitude weights dim = raise Exception noqa TRY Structured pruning can only applied +dim weight tensor take norm over all first dim dims = tuple range weights dim saliency = weights norm dim=dims p= handle weights groups split_size = len mask masks = torch split mask split_size saliencies = torch split saliency split_size keep_mask sal zip masks saliencies mask smallest k values removed k = int len keep_mask kwargs sparsity_level prune = sal topk k largest=False sorted=False indices keep_mask data prune = False modifies underlying p mask directly