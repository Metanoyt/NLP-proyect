mypy allow-untyped-defs math traceback dataclasses dataclass enum auto Enum typing Any Optional torch torch distributed dist torch nn nn torch distributed _composable contract _get_registry torch distributed tensor DeviceMesh DTensor torch distributed tensor _dtensor_spec DTensorSpec _compiled_autograd_enabled bool = False detect_compiled_autograd torch compiler is_compiling raise AssertionError ` detect_compiled_autograd ` designed called eager mode global _compiled_autograd_enabled torch _dynamo compiled_autograd ca _compiled_autograd_enabled = ca compiled_autograd_enabled ca compiled_autograd_enabled_force_eager ca in_compiled_autograd_region compiled_autograd_enabled global _compiled_autograd_enabled _compiled_autograd_enabled dataclass DataParallelMeshInfo mesh DeviceMesh shard_mesh_dim Optional int = None replicate_mesh_dim Optional int = None __post_init__ shard_mesh_dim None replicate_mesh_dim None raise AssertionError At least one shard_mesh_dim replicate_mesh_dim must None dataclass FSDPMeshInfo DataParallelMeshInfo __post_init__ super __post_init__ shard_mesh_dim None raise AssertionError Expects non-None shard_mesh_dim shard_mesh_size int = mesh size shard_mesh_dim shard_process_group = mesh get_group shard_mesh_dim shard_mesh_rank int = shard_process_group rank dataclass DDPMeshInfo DataParallelMeshInfo __post_init__ super __post_init__ replicate_mesh_dim None raise AssertionError Expects non-None replicate_mesh_dim replicate_mesh_size int = mesh size replicate_mesh_dim replicate_process_group = mesh get_group replicate_mesh_dim replicate_mesh_rank int = replicate_process_group rank dataclass HSDPMeshInfo FSDPMeshInfo DDPMeshInfo __post_init__ Calls ` FSDPMeshInfo ` - ` DDPMeshInfo ` - ` DataParallelMeshInfo ` super __post_init__ TrainingState Enum Describes training state one FSDP state parameter group Transition forward starting pre-forward until post-forward FORWARD = auto Transition pre-backward when unsharding backward PRE_BACKWARD = auto Transition post-backward when resharding reducing gradients POST_BACKWARD = auto Idle before after forward before pre-backward after post-backward IDLE = auto _raise_assert_with_print args Any kwargs Any print f Rank dist get_rank end= print args kwargs traceback print_stack raise AssertionError args kwargs _is_composable_with_fsdp module nn Module - bool registry = _get_registry module registry None True Registry keys function name replicate registry _get_dim _padded_size tensor_size torch Size dim _factor int - torch Size padded_dim = math ceil tensor_size dim _factor dim _factor torch Size padded_dim + tensor_size _chunk_with_empty tensor torch Tensor num_chunks int dim int - list torch Tensor chunks = list torch chunk tensor num_chunks dim=dim while len chunks num_chunks chunks append chunks new_empty chunks _get_dim_chunked_size chunk torch Tensor unchunked_size torch Size dim int - torch Size chunk numel chunk size For numel we need preserve nonzero-sized dims DTensor APIs unchunked_size dim + torch Size + unchunked_size dim + _from_local_no_grad local_tensor torch Tensor sharding_spec DTensorSpec - DTensor This method similar ` ` DTensor from_local ` ` except eager mode avoids some CPU overhead avoiding default args being differentiable compiled_autograd_enabled pyrefly ignore bad-argument-type DTensor Use local tensor directly instead constructing new tensor variable e g ` view_as ` since differentiable pyrefly ignore bad-argument-count local_tensor sharding_spec pyrefly ignore unexpected-keyword requires_grad=local_tensor requires_grad DTensor from_local local_tensor sharding_spec mesh sharding_spec placements shape=sharding_spec shape stride=sharding_spec stride _to_dtype_if_needed tensor torch Tensor dtype Optional torch dtype - torch Tensor dtype None tensor dtype = dtype tensor dtype tensor _cast_fp_tensor dtype torch dtype x torch Tensor - torch Tensor isinstance x torch Tensor torch is_floating_point x x dtype == dtype x x dtype is_bw - bool torch _C _current_graph_task_id = -