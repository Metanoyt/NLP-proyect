To run example use following command torchrun -- standalone -- nnodes= -- nproc-per-node= flex_attention_cp py os functools lru_cache typing Optional torch torch distributed dist torch nn functional F torch distributed device_mesh init_device_mesh torch distributed tensor distribute_tensor DTensor Partial Shard torch nn attention flex_attention _mask_mod_signature BlockMask create_block_mask flex_attention get_device_type - str cuda lru_cache create_block_mask_cached score_mod _mask_mod_signature B Optional int H Optional int M int N int device str = cuda - BlockMask block_mask = create_block_mask score_mod B H M N device=device block_mask flex_attn_example world_size int rank int - None device_type = get_device_type device_handle = getattr torch device_type None assert device_handle None f Unsupported device type device_type num_devices_per_host = device_handle device_count device_handle set_device rank num_devices_per_host torch _dynamo config cache_size_limit = init device mesh device_mesh = init_device_mesh device_type=device_type mesh_shape= world_size mesh_dim_names= cp causal_mask b int h int q_idx int kv_idx int - bool q_idx = kv_idx Compile flex_attention function compiled_flex_attention = torch compile flex_attention dynamic=False init input torch manual_seed dtype = torch float B = H = S = world_size D = qkv = torch rand B H S D device=device_type dtype=dtype requires_grad=True _ range input distribution seq_dim = qkv_dist = distribute_tensor t detach clone requires_grad_ device_mesh Shard seq_dim t qkv local forward pass block_mask = create_block_mask_cached causal_mask B= H= M=S N=S device=device_type q k v = qkv out = compiled_flex_attention q k v score_mod=None block_mask=block_mask assert isinstance out torch Tensor expect_out = F scaled_dot_product_attention q k v is_causal=True torch testing assert_close out expect_out atol= e- rtol= e- context parallel forward pass rewrite_mask_mod_for_cp mask_mod _mask_mod_signature rank int shard_size int - _mask_mod_signature since we re sharding ` seq_dim ` global q_idx mapped q_idx shard_size each rank which means q_idx = q_idx_on_rank + shard_size rank lambda b h q_idx kv_idx mask_mod b h q_idx + rank shard_size kv_idx manually do context parallel attention input hook Context Parallel q_local = qkv_dist to_local kv all-gather NOTE we don t consider load-balance now NOTE wait immediately called all_gather_tensor when gather_dim = k_full v_full = t full_tensor grad_placements= Partial t qkv_dist rewrite ` block_mask ` mask_mod _mask_mod_signature = block_mask mask_mod shard_size = S world_size cp_mask_mod = rewrite_mask_mod_for_cp mask_mod rank shard_size cp_block_mask = create_block_mask_cached cp_mask_mod B= H= M=shard_size N=S device=device_type TODO doesn t address return_lse=True case cp_out = compiled_flex_attention q_local k_full v_full score_mod=None block_mask=cp_block_mask assert isinstance cp_out torch Tensor wrap local output into DTensor cp_out_dist = DTensor from_local cp_out device_mesh Shard seq_dim compare flex_attention output torch testing assert_close cp_out_dist full_tensor out atol= e- rtol= e- local backward pass grad_out = torch randn B H S D device=device_type dtype=dtype grad_out_dist = distribute_tensor grad_out detach clone requires_grad_ device_mesh Shard seq_dim out backward grad_out grad = t grad t qkv t qkv t grad = None expect_out backward grad_out grad = t grad t qkv t qkv t grad = None flex_grad expect_grad zip grad grad torch testing assert_close flex_grad expect_grad atol= e- rtol= e- context parallel backward pass cp_out backward grad_out_dist to_local cp_flex_grad_dist expect_grad zip t grad t qkv_dist grad assert isinstance cp_flex_grad_dist DTensor torch testing assert_close cp_flex_grad_dist full_tensor expect_grad atol= e- rtol= e- __name__ == __main__ script launched via torchrun which automatically manages ProcessGroup rank = int os environ RANK world_size = int os environ WORLD_SIZE assert world_size == our example uses worker ranks try flex_attn_example world_size rank finally dist barrier dist destroy_process_group