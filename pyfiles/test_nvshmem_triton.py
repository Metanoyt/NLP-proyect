Owner s oncall distributed To run python test distributed test_nvshmem_triton py sys triton language tl torch torch distributed dist torch distributed _symmetric_memory symm_mem torch distributed _symmetric_memory _nvshmem_triton nvshmem torch _inductor runtime triton_compat triton torch distributed _symmetric_memory _nvshmem_triton requires_nvshmem torch testing _internal common_cuda SM OrLater torch testing _internal common_distributed MultiProcContinuousTest torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests skip_but_pass_in_sandcastle_if skipIfRocm torch testing _internal inductor_utils IS_H requires_triton symm_mem is_nvshmem_available print NVSHMEM available skipping tests sys exit requires_h skip_but_pass_in_sandcastle_if IS_H NVSHMEM requires H Skipping test non-H GPU So tests written device-agnostic way device_type = cuda device_module = torch get_device_module device_type Shared Triton JIT kernels requires_nvshmem triton jit my_put_kernel dest src nelems pe nvshmem put dest src nelems pe requires_nvshmem triton jit my_get_kernel dest src nelems pe nbi tl constexpr use nonblocking interface True nbi nvshmem get_nbi dest src nelems pe nvshmem quiet nvshmem get dest src nelems pe requires_nvshmem triton jit my_putmem_signal_block_kernel dst src size_bytes signal sig_val sig_op peer nvshmem putmem_signal_block dst src size_bytes signal sig_val sig_op peer requires_nvshmem triton jit my_signal_wait_until_kernel signal cmp_op cmp_val nvshmem signal_wait_until signal cmp_op cmp_val requires_nvshmem triton jit my_signal_op_kernel sig_addr signal sig_op peer nvshmem signal_op sig_addr signal sig_op peer requires_nvshmem triton jit my_wait_until_kernel ivar cmp_op cmp_val nvshmem wait_until ivar cmp_op cmp_val requires_nvshmem triton jit my_fence_kernel nvshmem fence requires_nvshmem triton jit my_put_with_fence_kernel dst src dst src flag_dst flag_src nelems peer First put nvshmem put dst src nelems peer Ensure first put ordered before next nvshmem fence Second put nvshmem put dst src nelems peer Order second put before flag update nvshmem fence Write flag single int signal completion nvshmem put flag_dst flag_src peer requires_nvshmem triton jit my_put_with_quiet_kernel dst src flag_dst flag_src nelems peer Put data nvshmem put dst src nelems peer Call quiet ensure put complete nvshmem quiet Only after quiet set completion flag This ensures data put complete before flag set nvshmem put flag_dst flag_src peer requires_nvshmem triton jit my_barrier_test_kernel dst src nelems Testing barrier_all requires coordinated operations across PEs within same kernel execution Unlike other kernels just wrap NVSHMEM primitives one implements full test logic properly verify device-side barrier synchronization my_pe = nvshmem my_pe n_pes = nvshmem n_pes Rank broadcasts its value all other ranks my_pe == Write initial value p_src = src tl pointer_type tl int tl store p_src Put all other ranks i = while i n_pes nvshmem put dst src nelems i i += Synchronize all PEs nvshmem barrier_all Non-zero ranks increment received value my_pe = p_dst = dst tl pointer_type tl int received = tl load p_dst tl store p_dst received + requires_nvshmem triton jit my_barrier_all_kernel nvshmem barrier_all requires_nvshmem triton jit my_sync_test_kernel local_data remote_data nelems my_pe = nvshmem my_pe n_pes = nvshmem n_pes Each PE writes unique value its local memory p_local = local_data tl pointer_type tl int unique_value = my_pe + PE writes PE writes etc tl store p_local unique_value sync_all ensures local stores visible other PEs doesn t guarantee completion any remote operations nvshmem sync_all Now each PE reads next PE s memory verify visibility PE reads PE PE reads PE PE n- reads PE next_pe = my_pe + n_pes nvshmem get remote_data local_data nelems next_pe The get should now see value next PE wrote locally because sync_all made those local stores visible requires_nvshmem triton jit my_alltoall_kernel team_handle dst src nelems_per_pe nvshmem alltoall team_handle dst src nelems_per_pe requires_nvshmem triton jit my_broadcast_kernel team_handle dst src nelems pe_root nvshmem broadcast team_handle dst src nelems pe_root requires_nvshmem triton jit my_reduce_kernel team_handle dest_tensor source_tensor nreduce operation tl constexpr nvshmem reduce team_handle dest_tensor source_tensor nreduce operation skip_but_pass_in_sandcastle_if SM OrLater Skipping all NVSHMEM Triton tests due https github com pytorch pytorch issues instantiate_parametrized_tests NVSHMEMTritonTest MultiProcContinuousTest _init_device - None TODO relieve seems hang without device_module set_device device Set NVSHMEM SymmMem backend symm_mem set_backend NVSHMEM property device - torch device torch device device_type rank skipIfRocm requires_triton requires_h test_triton_put - None torch manual_seed + rank _init_device group_name = dist distributed_c d _get_default_group group_name symm_mem enable_symm_mem_for_group group_name rank = rank Configuration nelems = number elements transfer dtype = torch int val = + rank Each rank has different data Create symmetric tensors src = symm_mem empty nelems dtype=dtype device=self device dst = symm_mem empty nelems dtype=dtype device=self device fill_ - Fill source tensor rank-specific pattern i range nelems src i = val + i Rank Rank Rendezvous symm_mem rendezvous src group=group_name symm_mem rendezvous dst group=group_name Synchronize before operation dist barrier peer = - rank rank == Rank puts its data Rank my_put_kernel dst src nelems peer Synchronize after operation dist barrier rank == Verify rank received rank s data expected = + i i range nelems torch testing assert_close dst torch tensor expected device=self device dtype=dtype skipIfRocm requires_triton requires_h parametrize nbi False True Test both blocking nonblocking interfaces test_triton_get nbi bool - None torch manual_seed + rank _init_device group_name = dist distributed_c d _get_default_group group_name symm_mem enable_symm_mem_for_group group_name rank = rank Configuration numel = dtype = torch int val = Create symmetric tensors inp = symm_mem empty numel dtype=dtype device=self device fill_ val rank == - out = symm_mem empty numel dtype=dtype device=self device fill_ - symm_mem rendezvous inp group=group_name symm_mem rendezvous out group=group_name dist barrier peer = - rank rank == Rank gets data rank using tensor-aware API my_get_kernel out inp numel peer nbi=nbi rank == torch testing assert_close out val torch ones numel dtype=dtype device=self device skipIfRocm requires_triton requires_h test_triton_get_ring - None torch manual_seed + rank _init_device group_name = dist distributed_c d _get_default_group group_name symm_mem enable_symm_mem_for_group group_name rank = rank world_size = dist get_world_size Configuration numel = dtype = torch int Each rank fills its input buffer its own rank value inp = symm_mem empty numel dtype=dtype device=self device fill_ rank out = symm_mem empty numel dtype=dtype device=self device fill_ - symm_mem rendezvous inp group=group_name symm_mem rendezvous out group=group_name dist barrier Ring topology each rank gets data rank its left rank gets rank world_size- rank gets rank etc peer = rank - world_size All ranks execute get operation using tensor-aware API my_get_kernel out inp numel peer nbi=False expected_value = peer torch testing assert_close out expected_value torch ones numel dtype=dtype device=self device skipIfRocm requires_triton requires_h test_triton_put_signal_set - None torch manual_seed + rank _init_device group_name = dist distributed_c d _get_default_group group_name symm_mem enable_symm_mem_for_group group_name rank = rank msg_size_bytes = dtype = torch int numel = msg_size_bytes dtype itemsize Data buffers val = inp = symm_mem empty numel dtype=dtype device=self device fill_ val out = symm_mem empty numel dtype=dtype device=self device fill_ - symm_mem rendezvous inp group=group_name out_hdl = symm_mem rendezvous out group=group_name Use signal pad attached output symmetric memory handle flag buffer signaling completion flag = out_hdl get_signal_pad rank dtype=torch int fill_ peer = - rank NVSHMEM_SIGNAL_SET = value defined NVSHMEM atomic set SIGNAL_VAL = Signal completion value NVSHMEM_CMP_EQ = compare equal signal wait until rank == Rank puts into Rank my_putmem_signal_block_kernel out inp size_bytes=msg_size_bytes signal=flag sig_val=SIGNAL_VAL sig_op=NVSHMEM_SIGNAL_SET peer=peer rank == Wait until signal flag set Rank my_signal_wait_until_kernel flag cmp_op=NVSHMEM_CMP_EQ cmp_val=SIGNAL_VAL After wait completes verify data flag contents torch testing assert_close out val torch ones numel dtype=dtype device=self device torch testing assert_close flag torch tensor SIGNAL_VAL dtype=torch int device=self device skipIfRocm requires_triton requires_h test_triton_put_signal_add - None torch manual_seed + rank _init_device group_name = dist distributed_c d _get_default_group group_name symm_mem enable_symm_mem_for_group group_name rank = rank msg_size_bytes = dtype = torch int numel = msg_size_bytes dtype itemsize Data buffers val = inp = symm_mem empty numel dtype=dtype device=self device fill_ val out = symm_mem empty numel dtype=dtype device=self device fill_ - symm_mem rendezvous inp group=group_name out_hdl = symm_mem rendezvous out group=group_name Use signal pad attached output symmetric memory handle flag buffer signaling completion flag = out_hdl get_signal_pad rank dtype=torch int fill_ peer = - rank NVSHMEM_SIGNAL_ADD = atomic add operation SIGNAL_VAL = val + NVSHMEM_SIGNAL_ADD NVSHMEM_CMP_EQ = rank == Rank puts into Rank my_putmem_signal_block_kernel out inp size_bytes=msg_size_bytes signal=flag sig_val=SIGNAL_VAL sig_op=NVSHMEM_SIGNAL_ADD peer=peer rank == my_signal_wait_until_kernel flag cmp_op=NVSHMEM_CMP_EQ cmp_val=SIGNAL_VAL torch testing assert_close out val torch ones numel dtype=dtype device=self device torch testing assert_close flag torch tensor SIGNAL_VAL dtype=torch int device=self device skipIfRocm requires_triton requires_h test_triton_wait_until - None torch manual_seed + rank _init_device group_name = dist distributed_c d _get_default_group group_name symm_mem enable_symm_mem_for_group group_name rank = rank peer = - rank NVSHMEM_CMP_EQ = equal comparison FLAG_INITIAL_VALUE = FLAG_FINAL_VALUE = Use single int symmetric tensor our synchronization flag flag = symm_mem empty dtype=torch int device=self device fill_ FLAG_INITIAL_VALUE symm_mem rendezvous flag group=group_name expected_flag = torch tensor FLAG_FINAL_VALUE dtype=torch int device=self device rank == Rank waiter my_wait_until_kernel flag cmp_op=NVSHMEM_CMP_EQ cmp_val=FLAG_FINAL_VALUE Verification torch testing assert_close flag expected_flag rank == Rank signaler Launch kernel put value Rank s flag tensor my_put_kernel flag Destination symmetric tensor remote PE expected_flag Source data tensor local Number elements peer The target PE Rank skipIfRocm requires_triton requires_h test_triton_signal_wait_until - None _init_device group_name = dist distributed_c d _get_default_group group_name symm_mem enable_symm_mem_for_group group_name rank = rank peer = - rank NVSHMEM constants documentation NVSHMEM_CMP_EQ = equal comparison NVSHMEM_SIGNAL_SET = atomic set operation Message configuration msg_size_bytes = dtype = torch int numel = msg_size_bytes dtype itemsize val_to_put = arbitrary test value COMPLETION_FLAG_VAL = Producer rank prepares data send inp = symm_mem empty numel dtype=dtype device=self device fill_ val_to_put symm_mem rendezvous inp group=group_name Consumer rank prepares destination buffer out = symm_mem empty numel dtype=dtype device=self device fill_ - out_hdl = symm_mem rendezvous out group=group_name Use signal pad synchronization previous tests flag_dtype = torch int flag = out_hdl get_signal_pad rank dtype=flag_dtype fill_ rank == Producer rank Puts data into rank s ` out ` buffer then sets flag my_putmem_signal_block_kernel out inp size_bytes=msg_size_bytes signal=flag sig_val=COMPLETION_FLAG_VAL sig_op=NVSHMEM_SIGNAL_SET peer=peer rank == Consumer rank Waits signal variable using ` signal_wait_until ` my_signal_wait_until_kernel flag cmp_op=NVSHMEM_CMP_EQ cmp_val=COMPLETION_FLAG_VAL After wait returns verify data flag torch testing assert_close out val_to_put torch ones numel dtype=dtype device=self device torch testing assert_close flag torch tensor COMPLETION_FLAG_VAL dtype=flag_dtype device=self device skipIfRocm requires_triton requires_h test_triton_fence - None Rank performs two put operations into Rank s buffers fence between them followed another fence flag update Rank waits flag then verifies both destination buffers contain expected values The flag transferred after final fence so its arrival implies both preceding puts have been delivered order torch manual_seed + rank _init_device group_name = dist distributed_c d _get_default_group group_name symm_mem enable_symm_mem_for_group group_name rank = rank peer = - rank Message configuration dtype = torch int numel = val = val = flag_val = Symmetric buffers inp = symm_mem empty numel dtype=dtype device=self device fill_ val inp = symm_mem empty numel dtype=dtype device=self device fill_ val out = symm_mem empty numel dtype=dtype device=self device fill_ - out = symm_mem empty numel dtype=dtype device=self device fill_ - symm_mem rendezvous inp group=group_name symm_mem rendezvous inp group=group_name symm_mem rendezvous out group=group_name symm_mem rendezvous out group=group_name Use regular symmetric memory tensor flag flag = symm_mem empty dtype=torch int device=self device fill_ symm_mem rendezvous flag group=group_name flag_update_val = torch tensor flag_val dtype=torch int device=self device NVSHMEM_CMP_EQ = compare equal rank == my_put_with_fence_kernel out inp out inp flag flag_update_val nelems=numel peer=peer rank == Wait until flag set Rank my_wait_until_kernel flag cmp_op=NVSHMEM_CMP_EQ cmp_val=flag_val Verify ordered data arrival torch testing assert_close out val torch ones numel dtype=dtype device=self device torch testing assert_close out val torch ones numel dtype=dtype device=self device torch testing assert_close flag torch tensor flag_val dtype=torch int device=self device skipIfRocm requires_triton requires_h test_triton_quiet - None torch manual_seed + rank _init_device group_name = dist distributed_c d _get_default_group group_name symm_mem enable_symm_mem_for_group group_name rank = rank peer = - rank dtype = torch int numel = val = flag_val = inp = symm_mem empty numel dtype=dtype device=self device fill_ val out = symm_mem empty numel dtype=dtype device=self device fill_ - flag = symm_mem empty dtype=torch int device=self device fill_ flag_update_val = torch tensor flag_val dtype=torch int device=self device symm_mem rendezvous inp group=group_name symm_mem rendezvous out group=group_name symm_mem rendezvous flag group=group_name NVSHMEM_CMP_EQ = dist barrier rank == my_put_with_quiet_kernel out inp flag flag_update_val nelems=numel peer=peer rank == my_wait_until_kernel flag cmp_op=NVSHMEM_CMP_EQ cmp_val=flag_val torch testing assert_close out val torch ones numel dtype=dtype device=self device dist barrier skipIfRocm requires_triton requires_h test_triton_barrier - None torch manual_seed + rank _init_device group_name = dist distributed_c d _get_default_group group_name symm_mem enable_symm_mem_for_group group_name rank = rank numel = dtype = torch int src = symm_mem empty numel dtype=dtype device=self device fill_ dst = symm_mem empty numel dtype=dtype device=self device fill_ symm_mem rendezvous src group=group_name symm_mem rendezvous dst group=group_name my_barrier_test_kernel dst src nelems=numel launch_cooperative_grid=True num_ctas= dist barrier rank == torch testing assert_close src torch tensor device=self device dtype=dtype torch testing assert_close dst torch tensor device=self device dtype=dtype skipIfRocm requires_triton requires_h test_triton_sync - None torch manual_seed + rank _init_device group_name = dist distributed_c d _get_default_group group_name symm_mem enable_symm_mem_for_group group_name rank = rank numel = dtype = torch int Create symmetric buffers local_data = symm_mem empty numel dtype=dtype device=self device fill_ remote_data = symm_mem empty numel dtype=dtype device=self device fill_ symm_mem rendezvous local_data group=group_name symm_mem rendezvous remote_data group=group_name Launch kernel cooperative grid my_sync_test_kernel local_data remote_data nelems=numel launch_cooperative_grid=True num_ctas= Verify results Each PE should have written rank + its local_data expected_local = rank + torch testing assert_close local_data torch tensor expected_local device=self device dtype=dtype Each PE should have read next_rank + into its remote_data PE reads PE PE reads PE PE n- reads PE next_rank = rank + world_size expected_remote = next_rank + torch testing assert_close remote_data torch tensor expected_remote device=self device dtype=dtype skipIfRocm requires_triton requires_h test_triton_alltoall - None torch manual_seed + rank _init_device group_name = dist distributed_c d _get_default_group group_name symm_mem enable_symm_mem_for_group group_name world_size = dist get_world_size rank = rank Each PE will send int elements every other PE nelems_per_pe = dtype = torch int Source buffer contains data all PEs Layout data_for_pe data_for_pe src_size = nelems_per_pe world_size src = symm_mem empty src_size dtype=dtype device=self device Fill source rank-specific data Formula rank + destination_pe i range world_size value = rank + i src i nelems_per_pe i + nelems_per_pe = value Destination buffer dst = symm_mem empty src_size dtype=dtype device=self device fill_ - symm_mem rendezvous src group=group_name symm_mem rendezvous dst group=group_name Synchronize before alltoall dist barrier team_handle = NVSHMEM_TEAM_WORLD handle Launch kernel using new tensor-aware API my_alltoall_kernel team_handle dst src nelems_per_pe launch_cooperative_grid=True Synchronize after alltoall dist barrier Verify results i range world_size After alltoall we should receive data PE i intended us PE i sends i + rank us expected = i + rank actual = dst i nelems_per_pe i + nelems_per_pe torch testing assert_close actual torch full_like actual expected skipIfRocm requires_triton requires_h test_triton_broadcast - None torch manual_seed + rank _init_device group_name = dist distributed_c d _get_default_group group_name symm_mem enable_symm_mem_for_group group_name rank = rank Configuration nelems = number elements dtype = torch int Source buffer - only root will have meaningful data pe_root = PE will root src = symm_mem empty nelems dtype=dtype device=self device Destination buffer dst = symm_mem empty nelems dtype=dtype device=self device fill_ - rank == pe_root Root fills specific pattern i range nelems src i = + i Non-root PEs have dummy data src fill_ - symm_mem rendezvous src group=group_name symm_mem rendezvous dst group=group_name Synchronize before broadcast dist barrier Execute broadcast team_handle = NVSHMEM_TEAM_WORLD my_broadcast_kernel team_handle dst src nelems pe_root launch_cooperative_grid=True Synchronize after broadcast dist barrier Verify results - all ranks should have root s data expected = + i i range nelems torch testing assert_close dst torch tensor expected device=self device dtype=dtype skipIfRocm requires_triton requires_h parametrize dtype torch int torch int torch int torch int torch uint torch float torch float torch float Tensor-likes close torch bfloat test_triton_sum_reduce dtype - None torch manual_seed + rank _init_device group_name = dist distributed_c d _get_default_group group_name symm_mem enable_symm_mem_for_group group_name world_size = dist get_world_size rank = rank Configuration nreduce = number separate reductions Source buffer - each rank contributes different values src = symm_mem empty nreduce dtype=dtype device=self device i range nreduce src i = rank + i + Rank Rank etc Destination buffer dst = symm_mem empty nreduce dtype=dtype device=self device fill_ - symm_mem rendezvous src group=group_name symm_mem rendezvous dst group=group_name Calculate expected results expected = i range nreduce Sum across all ranks sum rank+ i+ rank range world_size total = sum r + i + r range world_size expected append total Synchronize before reduction dist barrier Execute sum reduction across all ranks team_handle = NVSHMEM_TEAM_WORLD my_reduce_kernel team_handle dst src nreduce operation= sum launch_cooperative_grid=True Synchronize after reduction dist barrier Verify results torch testing assert_close dst torch tensor expected device=self device dtype=dtype skipIfRocm requires_triton requires_h parametrize dtype torch int torch int torch int torch int torch float torch float torch float torch bfloat test_triton_minmax_reduce dtype - None torch manual_seed + rank _init_device group_name = dist distributed_c d _get_default_group group_name symm_mem enable_symm_mem_for_group group_name world_size = dist get_world_size rank = rank Configuration nreduce = number values reduce Source buffers min max src_min = symm_mem empty nreduce dtype=dtype device=self device src_max = symm_mem empty nreduce dtype=dtype device=self device Each rank contributes different values For min rank rank etc For max same values i range nreduce i == src_min i = + rank src_max i = + rank src_min i = - rank - src_max i = - rank Destination buffers dst_min = symm_mem empty nreduce dtype=dtype device=self device fill_ - dst_max = symm_mem empty nreduce dtype=dtype device=self device fill_ - symm_mem rendezvous src_min group=group_name symm_mem rendezvous src_max group=group_name symm_mem rendezvous dst_min group=group_name symm_mem rendezvous dst_max group=group_name Calculate expected results all_values = i range nreduce values = r range world_size i == values append + r values append - r all_values append values expected_min = min vals vals all_values expected_max = max vals vals all_values dist barrier Execute MIN reduction team_handle = my_reduce_kernel team_handle dst_min src_min nreduce operation= min launch_cooperative_grid=True Execute MAX reduction my_reduce_kernel team_handle dst_max src_max nreduce operation= max launch_cooperative_grid=True dist barrier Verify results torch testing assert_close dst_min torch tensor expected_min device=self device dtype=dtype torch testing assert_close dst_max torch tensor expected_max device=self device dtype=dtype skipIfRocm requires_triton requires_h parametrize dtype torch int torch int torch int torch int torch float torch float torch float Tensor-likes close torch bfloat test_triton_prod_reduce dtype - None torch manual_seed + rank _init_device group_name = dist distributed_c d _get_default_group group_name symm_mem enable_symm_mem_for_group group_name world_size = dist get_world_size rank = rank Configuration nreduce = number separate reductions Source buffer - each rank contributes different values Use very small values avoid overflow especially small integer types src = symm_mem empty nreduce dtype=dtype device=self device i range nreduce Use values won t overflow even int all values i == For first element rank gets rank gets src i = rank == i == For second element all get no multiplication effect src i = For third element rank get rank get etc groups src i = rank == Destination buffer dst = symm_mem empty nreduce dtype=dtype device=self device fill_ - symm_mem rendezvous src group=group_name symm_mem rendezvous dst group=group_name Calculate expected results vals = torch empty nreduce world_size dtype=dtype vals = vals = vals = rank range world_size vals rank = rank == expected = vals prod - tolist Synchronize before reduction dist barrier Execute product reduction across all ranks team_handle = NVSHMEM_TEAM_WORLD my_reduce_kernel team_handle dst src nreduce operation= prod launch_cooperative_grid=True Synchronize after reduction dist barrier Verify results torch testing assert_close dst torch tensor expected device=self device dtype=dtype __name__ == __main__ run_tests