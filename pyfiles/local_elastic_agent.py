usr bin env python mypy allow-untyped-defs Copyright c Facebook Inc its affiliates All rights reserved This source code licensed under BSD-style license found LICENSE file root directory source tree json os signal socket time uuid string Template typing Any Optional TYPE_CHECKING torch distributed elastic timer timer torch distributed elastic events torch distributed elastic agent server api RunResult SimpleElasticAgent WorkerGroup WorkerSpec WorkerState torch distributed elastic agent server health_check_server create_healthcheck_server HealthCheckServer torch distributed elastic metrics api prof torch distributed elastic multiprocessing LogsSpecs PContext start_processes torch distributed elastic utils macros torch distributed elastic utils logging get_logger TYPE_CHECKING torch distributed elastic events api EventMetadataValue logger = get_logger __name__ __all__ = LocalElasticAgent TORCHELASTIC_ENABLE_FILE_TIMER TORCHELASTIC_TIMER_FILE TORCHELASTIC_HEALTH_CHECK_PORT TORCHELASTIC_ENABLE_FILE_TIMER = TORCHELASTIC_ENABLE_FILE_TIMER TORCHELASTIC_HEALTH_CHECK_PORT = TORCHELASTIC_HEALTH_CHECK_PORT TORCHELASTIC_TIMER_FILE = TORCHELASTIC_TIMER_FILE LocalElasticAgent SimpleElasticAgent An implementation py ` torchelastic agent server ElasticAgent ` handles host-local workers This agent deployed per host configured spawn ` ` n ` ` workers When using GPUs ` ` n ` ` maps number GPUs available host The local agent does communicate other local agents deployed other hosts even workers may communicate inter-host The worker id interpreted local process The agent starts stops all worker processes single unit The worker function argument passed worker function must python multiprocessing compatible To pass multiprocessing data structures workers you may create data structure same multiprocessing context specified ` ` start_method ` ` pass function argument The ` ` exit_barrier_timeout ` ` specifies amount time seconds wait other agents finish This acts safety net handle cases where workers finish different times prevent agents viewing workers finished early scale-down event It strongly advised user code deal ensuring workers terminated synchronous manner rather than relying exit_barrier_timeout A named pipe based watchdog can enabled ` ` ` LocalElasticAgent ` ` ` environment variable ` ` TORCHELASTIC_ENABLE_FILE_TIMER ` ` value has been defined ` ` ` LocalElasticAgent ` ` ` process Optionally another environment variable ` ` ` TORCHELASTIC_TIMER_FILE ` ` ` can set unique file name named pipe If environment variable ` ` ` TORCHELASTIC_TIMER_FILE ` ` ` set ` ` ` LocalElasticAgent ` ` ` will internally create unique file name set environment variable ` ` ` TORCHELASTIC_TIMER_FILE ` ` ` environment variable will propagated worker processes allow them connect same named pipe ` ` ` LocalElasticAgent ` ` ` uses Logs written specified log directory Each log line will default prefixed ` ` $ role_name $ local_rank ` ` e g ` ` trainer foobar ` ` Log prefixes can customized passing ` template string https docs python org library string html#template-strings ` _ ` ` log_line_prefix_template ` ` argument The following macros identifiers substituted runtime ` ` $ role_name $ local_rank $ rank ` ` For example prefix each log line global rank instead local rank set ` ` log_line_prefix_template = $ rank ` ` Example launching function trainer args - str do train main start_method= spawn shared_queue= multiprocessing get_context start_method Queue spec = WorkerSpec role= trainer local_world_size=nproc_per_process entrypoint=trainer args= foobar OTHER_PARAMS agent = LocalElasticAgent spec start_method results = agent run results is_failed print trainer failed print f rank value results return_values prints - rank value do train Example launching binary main spec = WorkerSpec role= trainer local_world_size=nproc_per_process entrypoint= usr local bin trainer args= -- trainer-args foobar OTHER_PARAMS agent = LocalElasticAgent spec results = agent run results is_failed print binary launches do have values __init__ spec WorkerSpec logs_specs LogsSpecs start_method= spawn exit_barrier_timeout float = log_line_prefix_template Optional str = None super __init__ spec exit_barrier_timeout _start_method = start_method _pcontext Optional PContext = None _rdzv_handler = spec rdzv_handler _log_line_prefix_template = log_line_prefix_template _worker_watchdog Optional timer FileTimerServer = None _logs_specs = logs_specs _health_check_server Optional HealthCheckServer = None _setup_local_watchdog envs dict int dict str str - None enable_watchdog_env_name = TORCHELASTIC_ENABLE_FILE_TIMER watchdog_enabled = os getenv enable_watchdog_env_name watchdog_file_env_name = TORCHELASTIC_TIMER_FILE watchdog_file_path = os getenv watchdog_file_env_name watchdog_enabled None str watchdog_enabled == watchdog_file_path None watchdog_file_path = tmp watchdog_timer_ + str uuid uuid logger info Starting FileTimerServer s watchdog_file_path envs logger warning Empty envs variables using empty run_id FileTimerServer run_id = run_id = envs TORCHELASTIC_RUN_ID _worker_watchdog = timer FileTimerServer file_path=watchdog_file_path run_id=run_id max_interval= daemon=True log_event=self _log_watchdog_event _worker_watchdog start logger info FileTimerServer started logger info Environment variable s found Do start FileTimerServer enable_watchdog_env_name Propagate watchdog file env worker processes watchdog_file_path None worker_env envs values worker_env watchdog_file_env_name = watchdog_file_path staticmethod _get_current_time_secs - int int time time _setup_healthcheck - None healthcheck_port_env_name = TORCHELASTIC_HEALTH_CHECK_PORT healthcheck_port = os getenv healthcheck_port_env_name healthcheck_port None logger info Found healthcheck port s s healthcheck_port_env_name healthcheck_port _worker_watchdog None logger info FileTimerServer doesn t exist using current time dummy callback alive_callback = LocalElasticAgent _get_current_time_secs alive_callback = _worker_watchdog get_last_progress_time try healthcheck_port_as_int = int healthcheck_port _health_check_server = create_healthcheck_server alive_callback=alive_callback port=healthcheck_port_as_int timeout= _health_check_server start except ValueError logger info Invalid healthcheck port value s expecting integer Not starting healthcheck server healthcheck_port logger info Environment variable s found Do start health check healthcheck_port_env_name _get_fq_hostname - str socket getfqdn socket gethostname _log_watchdog_event name str request Optional timer FileTimerRequest - None wg = _worker_group spec = wg spec md = watchdog_event name request None md worker_pid = str request worker_pid md scope_id = request scope_id md expiration_time = str request expiration_time md signal = str request signal md_str = json dumps md state = RUNNING metadata dict str EventMetadataValue = run_id spec rdzv_handler get_run_id global_rank None group_rank wg group_rank worker_id None role spec role hostname _get_fq_hostname state state total_run_time _total_execution_time rdzv_backend spec rdzv_handler get_backend raw_error None metadata md_str agent_restarts spec max_restarts - _remaining_restarts Note The metadata field Event converted TorchelasticStatusLogEntry later The name field Event NOT used TorchelasticStatusLogEntry event = events Event name=name source=events EventSource AGENT metadata=metadata events record event _worker_group spec event_log_handler pyre-fixme Pyre able infer type decorator ` torch distributed elastic metrics prof ` prof _stop_workers worker_group WorkerGroup - None _shutdown pyre-fixme Pyre able infer type decorator ` torch distributed elastic metrics prof ` prof _start_workers worker_group WorkerGroup - dict int Any spec = worker_group spec store = worker_group store assert store None restart_count = spec max_restarts - _remaining_restarts use_agent_store bool = spec rdzv_handler use_agent_store logger info use_agent_store s use_agent_store args dict int tuple = envs dict int dict str str = log_line_prefixes Optional dict int str = _log_line_prefix_template None worker worker_group workers local_rank = worker local_rank worker_env = LOCAL_RANK str local_rank RANK str worker global_rank GROUP_RANK str worker_group group_rank ROLE_RANK str worker role_rank ROLE_NAME spec role LOCAL_WORLD_SIZE str spec local_world_size WORLD_SIZE str worker world_size GROUP_WORLD_SIZE str worker_group group_world_size ROLE_WORLD_SIZE str worker role_world_size MASTER_ADDR worker_group master_addr MASTER_PORT str worker_group master_port TORCHELASTIC_RESTART_COUNT str restart_count TORCHELASTIC_MAX_RESTARTS str spec max_restarts TORCHELASTIC_RUN_ID spec rdzv_handler get_run_id TORCHELASTIC_USE_AGENT_STORE str use_agent_store TORCH_NCCL_ASYNC_ERROR_HANDLING os getenv TORCH_NCCL_ASYNC_ERROR_HANDLING str OMP_NUM_THREADS os environ worker_env OMP_NUM_THREADS = os environ OMP_NUM_THREADS _log_line_prefix_template log_line_prefix = Template _log_line_prefix_template safe_substitute role_name=spec role rank=worker global_rank local_rank=local_rank pyrefly ignore unsupported-operation log_line_prefixes local_rank = log_line_prefix pyrefly ignore unsupported-operation envs local_rank = worker_env worker_args = list spec args worker_args = macros substitute worker_args str local_rank args local_rank = tuple worker_args _setup_local_watchdog envs=envs _setup_healthcheck assert spec entrypoint None assert _logs_specs None _pcontext = start_processes name=spec role entrypoint=spec entrypoint args=args envs=envs logs_specs=self _logs_specs log_line_prefixes=log_line_prefixes start_method=self _start_method numa_options=spec numa_options duplicate_stdout_filters=spec duplicate_stdout_filters duplicate_stderr_filters=spec duplicate_stderr_filters _pcontext pids _shutdown death_sig signal Signals = signal SIGTERM - None _worker_watchdog None _worker_watchdog stop _worker_watchdog = None _health_check_server None _health_check_server stop _health_check_server = None _pcontext _pcontext close death_sig pyre-fixme Pyre able infer type decorator ` torch distributed elastic metrics prof ` prof _monitor_workers worker_group WorkerGroup - RunResult role = worker_group spec role worker_pids = w id w worker_group workers assert _pcontext None pc_pids = set _pcontext pids values worker_pids = pc_pids logger error s worker pids do match process_context pids Expected s actual s role worker_pids pc_pids RunResult state=WorkerState UNKNOWN result = _pcontext wait result result is_failed map local rank failure global rank worker_failures = local_rank failure result failures items worker = worker_group workers local_rank worker_failures worker global_rank = failure RunResult state=WorkerState FAILED failures=worker_failures copy ret_val_queue into map global ranks workers_ret_vals = local_rank ret_val result return_values items worker = worker_group workers local_rank workers_ret_vals worker global_rank = ret_val RunResult state=WorkerState SUCCEEDED return_values=workers_ret_vals RunResult state=WorkerState HEALTHY