usr bin env python Copyright c Facebook Inc its affiliates Licensed under Apache License Version License you may use file except compliance License You may obtain copy License http www apache org licenses LICENSE- Unless required applicable law agreed writing software distributed under License distributed AS IS BASIS WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND either express implied See License specific language governing permissions limitations under License This example shows how use higher do Model Agnostic Meta Learning MAML few-shot Omniglot classification For more details see original MAML paper https arxiv org abs This code has been modified Jackie Loong s PyTorch MAML implementation https github com dragen MAML-Pytorch blob master omniglot_train py Our MAML++ fork experiments available https github com bamos HowToTrainYourMAMLPytorch argparse time matplotlib mpl matplotlib pyplot plt numpy np pandas pd support omniglot_loaders OmniglotNShot torch torch nn functional F torch optim optim functorch make_functional_with_buffers torch nn mpl use Agg plt style use bmh main argparser = argparse ArgumentParser argparser add_argument -- n-way -- n_way type=int help= n way default= argparser add_argument -- k-spt -- k_spt type=int help= k shot support set default= argparser add_argument -- k-qry -- k_qry type=int help= k shot query set default= argparser add_argument -- device type=str help= device default= cuda argparser add_argument -- task-num -- task_num type=int help= meta batch size namely task num default= argparser add_argument -- seed type=int help= random seed default= args = argparser parse_args torch manual_seed args seed torch cuda is_available torch cuda manual_seed_all args seed np random seed args seed Set up Omniglot loader device = args device db = OmniglotNShot tmp omniglot-data batchsz=args task_num n_way=args n_way k_shot=args k_spt k_query=args k_qry imgsz= device=device Create vanilla PyTorch neural network will automatically monkey-patched higher later Before higher models could created like parameters needed manually updated copied updates net = nn Sequential nn Conv d nn BatchNorm d momentum= affine=True nn ReLU inplace=True nn MaxPool d nn Conv d nn BatchNorm d momentum= affine=True nn ReLU inplace=True nn MaxPool d nn Conv d nn BatchNorm d momentum= affine=True nn ReLU inplace=True nn MaxPool d Flatten nn Linear args n_way device net train fnet params buffers = make_functional_with_buffers net We will use Adam meta- optimize initial parameters adapted meta_opt = optim Adam params lr= e- log = epoch range train db params buffers fnet device meta_opt epoch log test db params buffers fnet device epoch log plot log train db net device meta_opt epoch log params buffers fnet = net n_train_iter = db x_train shape db batchsz batch_idx range n_train_iter start_time = time time Sample batch support query images labels x_spt y_spt x_qry y_qry = db next task_num setsz c_ h w = x_spt size querysz = x_qry size TODO Maybe pull out into separate module so doesn t have duplicated between ` train ` ` test ` Initialize inner optimizer adapt parameters support set n_inner_iter = inner_opt = torch optim SGD net parameters lr= e- qry_losses = qry_accs = meta_opt zero_grad i range task_num Optimize likelihood support set taking gradient steps w r t model s parameters This adapts model s meta-parameters task new_params = params _ range n_inner_iter spt_logits = fnet new_params buffers x_spt i spt_loss = F cross_entropy spt_logits y_spt i grads = torch autograd grad spt_loss new_params create_graph=True new_params = p - g e- p g zip new_params grads The final set adapted parameters will induce some final loss accuracy query dataset These will used update model s meta-parameters qry_logits = fnet new_params buffers x_qry i qry_loss = F cross_entropy qry_logits y_qry i qry_losses append qry_loss detach qry_acc = qry_logits argmax dim= == y_qry i sum item querysz qry_accs append qry_acc Update model s meta-parameters optimize query losses across all tasks sampled batch This unrolls through gradient steps qry_loss backward meta_opt step qry_losses = sum qry_losses task_num qry_accs = sum qry_accs task_num i = epoch + float batch_idx n_train_iter iter_time = time time - start_time batch_idx == print f Epoch i f Train Loss qry_losses f &#124; Acc qry_accs f &#124; Time iter_time f log append epoch i loss qry_losses acc qry_accs mode train time time time test db net device epoch log Crucially our testing procedure here we do fine-tune model during testing simplicity Most research papers using MAML task do extra stage fine-tuning here should added you adapting code research params buffers fnet = net n_test_iter = db x_test shape db batchsz qry_losses = qry_accs = batch_idx range n_test_iter x_spt y_spt x_qry y_qry = db next test task_num setsz c_ h w = x_spt size TODO Maybe pull out into separate module so doesn t have duplicated between ` train ` ` test ` n_inner_iter = i range task_num new_params = params _ range n_inner_iter spt_logits = fnet new_params buffers x_spt i spt_loss = F cross_entropy spt_logits y_spt i grads = torch autograd grad spt_loss new_params new_params = p - g e- p g zip new_params grads The query loss acc induced these parameters qry_logits = fnet new_params buffers x_qry i detach qry_loss = F cross_entropy qry_logits y_qry i reduction= none qry_losses append qry_loss detach qry_accs append qry_logits argmax dim= == y_qry i detach qry_losses = torch cat qry_losses mean item qry_accs = torch cat qry_accs float mean item print f Epoch epoch + f Test Loss qry_losses f &#124; Acc qry_accs f log append epoch epoch + loss qry_losses acc qry_accs mode test time time time plot log Generally you should pull your plotting code out your training script we doing here brevity df = pd DataFrame log fig ax = plt subplots figsize= train_df = df df mode == train test_df = df df mode == test ax plot train_df epoch train_df acc label= Train ax plot test_df epoch test_df acc label= Test ax set_xlabel Epoch ax set_ylabel Accuracy ax set_ylim fig legend ncol= loc= lower right fig tight_layout fname = maml-accs png print f --- Plotting accuracy fname fig savefig fname plt close fig Won t need after PR merged https github com pytorch pytorch pull Flatten nn Module forward input input view input size - __name__ == __main__ main