mypy allow-untyped-defs typing Optional Union torch torch Tensor optimizer _to_scalar Optimizer ParamsT __all__ = LBFGS _cubic_interpolate x f g x f g bounds=None ported https github com torch optim blob master polyinterp lua Compute bounds interpolation area bounds None xmin_bound xmax_bound = bounds xmin_bound xmax_bound = x x x = x x x Code most common case cubic interpolation points w function derivative values both Solution case where x farthest point d = g + g - f -f x -x d = sqrt d ^ - g g min_pos = x - x - x g + d - d g - g + d t_new = min max min_pos xmin_bound xmax_bound d = g + g - f - f x - x d _square = d - g g d _square = d = d _square sqrt x = x min_pos = x - x - x g + d - d g - g + d min_pos = x - x - x g + d - d g - g + d min max min_pos xmin_bound xmax_bound xmin_bound + xmax_bound _strong_wolfe obj_func x t d f g gtd c = e- c = tolerance_change= e- max_ls= ported https github com torch optim blob master lswolfe lua d_norm = d abs max g = g clone memory_format=torch contiguous_format evaluate objective gradient using initial step f_new g_new = obj_func x t d ls_func_evals = gtd_new = g_new dot d bracket interval containing point satisfying Wolfe criteria t_prev f_prev g_prev gtd_prev = f g gtd done = False ls_iter = while ls_iter max_ls check conditions f_new f + c t gtd ls_iter f_new = f_prev bracket = t_prev t bracket_f = f_prev f_new bracket_g = g_prev g_new clone memory_format=torch contiguous_format bracket_gtd = gtd_prev gtd_new break abs gtd_new = -c gtd bracket = t bracket_f = f_new bracket_g = g_new done = True break gtd_new = bracket = t_prev t bracket_f = f_prev f_new bracket_g = g_prev g_new clone memory_format=torch contiguous_format bracket_gtd = gtd_prev gtd_new break interpolate min_step = t + t - t_prev max_step = t tmp = t t = _cubic_interpolate t_prev f_prev gtd_prev t f_new gtd_new bounds= min_step max_step next step t_prev = tmp f_prev = f_new g_prev = g_new clone memory_format=torch contiguous_format gtd_prev = gtd_new f_new g_new = obj_func x t d ls_func_evals += gtd_new = g_new dot d ls_iter += reached max number iterations ls_iter == max_ls bracket = t bracket_f = f f_new bracket_g = g g_new zoom phase we now have point satisfying criteria bracket around We refine bracket until we find exact point satisfying criteria insuf_progress = False find high low points bracket low_pos high_pos = bracket_f = bracket_f - type ignore possibly-undefined while done ls_iter max_ls line-search bracket so small abs bracket - bracket d_norm tolerance_change type ignore possibly-undefined break compute new trial value t = _cubic_interpolate pyrefly ignore index-error bracket pyrefly ignore unbound-name bracket_f bracket_gtd type ignore possibly-undefined pyrefly ignore index-error bracket pyrefly ignore unbound-name bracket_f pyrefly ignore unbound-name bracket_gtd test we making sufficient progress case ` t ` so close boundary we mark we making insufficient progress + we have made insufficient progress last step + ` t ` one boundary we will move ` t ` position which ` len bracket ` away nearest boundary point pyrefly ignore unbound-name eps = max bracket - min bracket pyrefly ignore unbound-name min max bracket - t t - min bracket eps interpolation close boundary pyrefly ignore unbound-name insuf_progress t = max bracket t = min bracket evaluate away boundary pyrefly ignore unbound-name abs t - max bracket abs t - min bracket pyrefly ignore unbound-name t = max bracket - eps pyrefly ignore unbound-name t = min bracket + eps insuf_progress = False insuf_progress = True insuf_progress = False Evaluate new point f_new g_new = obj_func x t d ls_func_evals += gtd_new = g_new dot d ls_iter += pyrefly ignore unbound-name f_new f + c t gtd f_new = bracket_f low_pos Armijo condition satisfied lower than lowest point pyrefly ignore unsupported-operation bracket high_pos = t pyrefly ignore unbound-name bracket_f high_pos = f_new bracket_g high_pos = g_new clone memory_format=torch contiguous_format type ignore possibly-undefined pyrefly ignore unbound-name bracket_gtd high_pos = gtd_new pyrefly ignore unbound-name low_pos high_pos = bracket_f = bracket_f abs gtd_new = -c gtd Wolfe conditions satisfied done = True pyrefly ignore index-error gtd_new bracket high_pos - bracket low_pos = old high becomes new low pyrefly ignore unsupported-operation bracket high_pos = bracket low_pos pyrefly ignore unbound-name bracket_f high_pos = bracket_f low_pos bracket_g high_pos = bracket_g low_pos type ignore possibly-undefined pyrefly ignore unbound-name bracket_gtd high_pos = bracket_gtd low_pos new point becomes new low pyrefly ignore unsupported-operation bracket low_pos = t pyrefly ignore unbound-name bracket_f low_pos = f_new bracket_g low_pos = g_new clone memory_format=torch contiguous_format type ignore possibly-undefined pyrefly ignore unbound-name bracket_gtd low_pos = gtd_new stuff t = bracket low_pos type ignore possibly-undefined pyrefly ignore unbound-name f_new = bracket_f low_pos g_new = bracket_g low_pos type ignore possibly-undefined f_new g_new t ls_func_evals LBFGS Optimizer Implements L-BFGS algorithm Heavily inspired ` minFunc https www cs ubc ca ~schmidtm Software minFunc html ` _ warning This optimizer doesn t support per-parameter options parameter groups there can only one warning Right now all parameters have single device This will improved future note This very memory intensive optimizer requires additional ` ` param_bytes history_size + ` ` bytes If doesn t fit memory try reducing history size use different algorithm Args params iterable iterable parameters optimize Parameters must real lr float optional learning rate default max_iter int optional maximal number iterations per optimization step default max_eval int optional maximal number function evaluations per optimization step default max_iter tolerance_grad float optional termination tolerance first order optimality default e- tolerance_change float optional termination tolerance function value parameter changes default e- history_size int optional update history size default line_search_fn str optional either strong_wolfe None default None __init__ params ParamsT lr Union float Tensor = max_iter int = max_eval Optional int = None tolerance_grad float = e- tolerance_change float = e- history_size int = line_search_fn Optional str = None isinstance lr Tensor lr numel = raise ValueError Tensor lr must -element = lr raise ValueError f Invalid learning rate lr max_eval None max_eval = max_iter defaults = lr lr max_iter max_iter max_eval max_eval tolerance_grad tolerance_grad tolerance_change tolerance_change history_size history_size line_search_fn line_search_fn super __init__ params defaults len param_groups = raise ValueError LBFGS doesn t support per-parameter options parameter groups _params = param_groups params _numel_cache = None _numel _numel_cache None pyrefly ignore bad-assignment _numel_cache = sum p numel torch is_complex p p numel p _params _numel_cache _gather_flat_grad views = p _params p grad None view = p new p numel zero_ p grad is_sparse view = p grad to_dense view - view = p grad view - torch is_complex view view = torch view_as_real view view - views append view torch cat views _add_grad step_size update offset = p _params torch is_complex p p = torch view_as_real p numel = p numel view avoid deprecated pointwise semantics p add_ update offset offset + numel view_as p alpha=step_size offset += numel offset = _numel raise AssertionError f Expected offset offset equal _numel _clone_param p clone memory_format=torch contiguous_format p _params _set_param params_data p pdata zip _params params_data strict=True p copy_ pdata _directional_evaluate closure x t d _add_grad t d loss = float closure flat_grad = _gather_flat_grad _set_param x loss flat_grad torch no_grad step closure type ignore override Perform single optimization step Args closure Callable A closure reevaluates model returns loss len param_groups = raise AssertionError f Expected exactly one param_group got len param_groups Make sure closure always called grad enabled closure = torch enable_grad closure group = param_groups lr = _to_scalar group lr max_iter = group max_iter max_eval = group max_eval tolerance_grad = group tolerance_grad tolerance_change = group tolerance_change line_search_fn = group line_search_fn history_size = group history_size NOTE LBFGS has only global state we register state first param because helps casting load_state_dict state = state _params state setdefault func_evals state setdefault n_iter evaluate initial f x df dx orig_loss = closure loss = float orig_loss current_evals = state func_evals += flat_grad = _gather_flat_grad opt_cond = flat_grad abs max = tolerance_grad optimal condition opt_cond orig_loss tensors cached state tracing d = state get d t = state get t old_dirs = state get old_dirs old_stps = state get old_stps ro = state get ro H_diag = state get H_diag prev_flat_grad = state get prev_flat_grad prev_loss = state get prev_loss n_iter = optimize max max_iter iterations while n_iter max_iter keep track nb iterations n_iter += state n_iter += ############################################################ compute gradient descent direction ############################################################ state n_iter == d = flat_grad neg old_dirs = old_stps = ro = H_diag = do lbfgs update update memory y = flat_grad sub prev_flat_grad s = d mul t ys = y dot s y s ys e- updating memory len old_dirs == history_size shift history one limited-memory old_dirs pop old_stps pop ro pop store new direction step old_dirs append y old_stps append s ro append ys update scale initial Hessian approximation H_diag = ys y dot y y y compute approximate L-BFGS inverse Hessian multiplied gradient num_old = len old_dirs al state state al = None history_size al = state al iteration L-BFGS loop collapsed use just one buffer q = flat_grad neg i range num_old - - - al i = old_stps i dot q ro i q add_ old_dirs i alpha=-al i multiply initial Hessian r d final direction d = r = torch mul q H_diag i range num_old be_i = old_dirs i dot r ro i r add_ old_stps i alpha=al i - be_i prev_flat_grad None prev_flat_grad = flat_grad clone memory_format=torch contiguous_format prev_flat_grad copy_ flat_grad prev_loss = loss ############################################################ compute step length ############################################################ reset initial guess step size state n_iter == t = min flat_grad abs sum lr t = lr directional derivative gtd = flat_grad dot d g d directional derivative below tolerance gtd -tolerance_change break optional line search user function ls_func_evals = line_search_fn None perform line search using user function line_search_fn = strong_wolfe raise RuntimeError only strong_wolfe supported x_init = _clone_param obj_func x t d _directional_evaluate closure x t d loss flat_grad t ls_func_evals = _strong_wolfe obj_func x_init t d loss flat_grad gtd max_ls=max_eval - current_evals _add_grad t d opt_cond = flat_grad abs max = tolerance_grad no line search simply move fixed-step _add_grad t d n_iter = max_iter re-evaluate function only last iteration reason we do stochastic setting no use re-evaluate function here torch enable_grad loss = closure loss = float loss flat_grad = _gather_flat_grad opt_cond = flat_grad abs max = tolerance_grad ls_func_evals = update func eval current_evals += ls_func_evals state func_evals += ls_func_evals ############################################################ check conditions ############################################################ n_iter == max_iter break current_evals = max_eval break optimal condition opt_cond break lack progress d mul t abs max = tolerance_change break abs loss - prev_loss tolerance_change break state d = d state t = t state old_dirs = old_dirs state old_stps = old_stps state ro = ro state H_diag = H_diag state prev_flat_grad = prev_flat_grad state prev_loss = prev_loss orig_loss