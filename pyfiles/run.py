usr bin env python mypy allow-untyped-defs Copyright c Facebook Inc its affiliates All rights reserved This source code licensed under BSD-style license found LICENSE file root directory source tree Module ` ` torch distributed run ` ` ` ` torch distributed run ` ` module spawns up multiple distributed training processes each training nodes ` ` torchrun ` ` python ` console script https packaging python org en latest specifications entry-points #use-for-scripts ` _ main module ` torch distributed run https github com pytorch pytorch blob master torch distributed run py ` _ declared ` ` entry_points ` ` configuration ` setup py https github com pytorch pytorch blob master setup py ` _ It equivalent invoking ` ` python -m torch distributed run ` ` ` ` torchrun ` ` can used single-node distributed training which one more processes per node will spawned It can used either CPU training GPU training If used GPU training each distributed process will operating single GPU This can achieve well-improved single-node training performance ` ` torchrun ` ` can also used multi-node distributed training spawning up multiple processes each node well-improved multi-node distributed training performance well This will especially beneficial systems multiple Infiniband interfaces have direct-GPU support since all them can utilized aggregated communication bandwidth In both cases single-node distributed training multi-node distributed training ` ` torchrun ` ` will launch given number processes per node ` ` -- nproc-per-node ` ` If used GPU training number needs less equal number GPUs current system ` ` nproc_per_node ` ` each process will operating single GPU GPU GPU nproc_per_node - versionchanged ` ` torchrun ` ` will pass ` ` -- local-rank= rank ` ` argument your script From PyTorch onwards dashed ` ` -- local-rank ` ` preferred over previously used underscored ` ` -- local_rank ` ` For backward compatibility may necessary users handle both cases their argument parsing code This means including both ` ` -- local-rank ` ` ` ` -- local_rank ` ` argument parser If only ` ` -- local_rank ` ` provided ` ` torchrun ` ` will trigger error error unrecognized arguments -- local-rank= rank For training code only supports PyTorch + including ` ` -- local-rank ` ` should sufficient xdoctest +SKIP argparse parser = argparse ArgumentParser parser add_argument -- local-rank -- local_rank type=int args = parser parse_args Usage ----- Single-node multi-worker ++++++++++++++++++++++++ torchrun -- standalone -- nnodes= -- nproc-per-node=$ NUM_TRAINERS YOUR_TRAINING_SCRIPT py -- arg train script args note ` ` -- nproc-per-node ` ` may ` ` gpu ` ` spawn one process per GPU ` ` cpu ` ` spawn one process per CPU ` ` xpu ` ` spawn one process per XPU ` ` auto ` ` equivalent ` ` gpu ` ` CUDA available equivalent ` ` xpu ` ` XPU available equivalent ` ` cpu ` ` integer specifying number processes See ` torch distributed run determine_local_world_size https github com pytorch pytorch blob bb ed cc d d b e torch distributed run py#L -L ` _ more details Stacked single-node multi-worker ++++++++++++++++++++++++++++++++ To run multiple instances separate jobs single-node multi-worker same host we need make sure each instance job setup different ports avoid port conflicts worse two jobs being merged single job To do you have run ` ` -- rdzv-backend=c d ` ` specify different port setting ` ` -- rdzv-endpoint=localhost $ PORT_k ` ` For ` ` -- nodes= ` ` its often convenient let ` ` torchrun ` ` pick free random port automatically instead manually assigning different ports each run torchrun -- rdzv-backend=c d -- rdzv-endpoint=localhost -- nnodes= -- nproc-per-node=$ NUM_TRAINERS YOUR_TRAINING_SCRIPT py -- arg train script args Fault tolerant fixed sized number workers no elasticity tolerates failures +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ torchrun -- nnodes=$ NUM_NODES -- nproc-per-node=$ NUM_TRAINERS -- max-restarts= -- rdzv-id=$ JOB_ID -- rdzv-backend=c d -- rdzv-endpoint=$ HOST_NODE_ADDR YOUR_TRAINING_SCRIPT py -- arg train script args ` ` HOST_NODE_ADDR ` ` form host port e g node example com specifies node port which C d rendezvous backend should instantiated hosted It can any node your training cluster ideally you should pick node has high bandwidth note If no port number specified ` ` HOST_NODE_ADDR ` ` defaults Elastic ` ` min= ` ` ` ` max= ` ` tolerates up membership changes failures ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ torchrun -- nnodes= -- nproc-per-node=$ NUM_TRAINERS -- max-restarts= -- rdzv-id=$ JOB_ID -- rdzv-backend=c d -- rdzv-endpoint=$ HOST_NODE_ADDR YOUR_TRAINING_SCRIPT py -- arg train script args ` ` HOST_NODE_ADDR ` ` form host port e g node example com specifies node port which C d rendezvous backend should instantiated hosted It can any node your training cluster ideally you should pick node has high bandwidth note If no port number specified ` ` HOST_NODE_ADDR ` ` defaults Note rendezvous backend -------------------------- For multi-node training you need specify ` ` -- rdzv-id ` ` A unique job id shared all nodes participating job ` ` -- rdzv-backend ` ` An implementation py ` torch distributed elastic rendezvous RendezvousHandler ` ` ` -- rdzv-endpoint ` ` The endpoint where rendezvous backend running usually form ` ` host port ` ` Currently ` ` c d ` ` recommended ` ` etcd-v ` ` ` ` etcd ` ` legacy rendezvous backends supported out box To use ` ` etcd-v ` ` ` ` etcd ` ` setup etcd server ` ` v ` ` api enabled e g ` ` -- enable-v ` ` warning ` ` etcd-v ` ` ` ` etcd ` ` rendezvous use etcd API v You MUST enable v API etcd server Our tests use etcd v warning For etcd-based rendezvous we recommend using ` ` etcd-v ` ` over ` ` etcd ` ` which functionally equivalent uses revised implementation ` ` etcd ` ` maintenance mode will removed future version Definitions ----------- ` ` Node ` ` - A physical instance container maps unit job manager works ` ` Worker ` ` - A worker context distributed training ` ` WorkerGroup ` ` - The set workers execute same function e g trainers ` ` LocalWorkerGroup ` ` - A subset workers worker group running same node ` ` RANK ` ` - The rank worker within worker group ` ` WORLD_SIZE ` ` - The total number workers worker group ` ` LOCAL_RANK ` ` - The rank worker within local worker group ` ` LOCAL_WORLD_SIZE ` ` - The size local worker group ` ` rdzv_id ` ` - A user-defined id uniquely identifies worker group job This id used each node join member particular worker group ` ` rdzv_backend ` ` - The backend rendezvous e g ` ` c d ` ` This typically strongly consistent key-value store ` ` rdzv_endpoint ` ` - The rendezvous backend endpoint usually form ` ` host port ` ` A ` ` Node ` ` runs ` ` LOCAL_WORLD_SIZE ` ` workers which comprise ` ` LocalWorkerGroup ` ` The union all ` ` LocalWorkerGroups ` ` nodes job comprise ` ` WorkerGroup ` ` Environment Variables --------------------- The following environment variables made available you your script ` ` LOCAL_RANK ` ` - The local rank ` ` RANK ` ` - The global rank ` ` GROUP_RANK ` ` - The rank worker group A number between ` ` max_nnodes ` ` When running single worker group per node rank node ` ` ROLE_RANK ` ` - The rank worker across all workers have same role The role worker specified ` ` WorkerSpec ` ` ` ` LOCAL_WORLD_SIZE ` ` - The local world size e g number workers running locally equals ` ` -- nproc-per-node ` ` specified ` ` torchrun ` ` ` ` WORLD_SIZE ` ` - The world size total number workers job ` ` ROLE_WORLD_SIZE ` ` - The total number workers launched same role specified ` ` WorkerSpec ` ` ` ` MASTER_ADDR ` ` - The FQDN host running worker rank used initialize Torch Distributed backend ` ` MASTER_PORT ` ` - The port ` ` MASTER_ADDR ` ` can used host C d TCP store ` ` TORCHELASTIC_RESTART_COUNT ` ` - The number worker group restarts so far ` ` TORCHELASTIC_MAX_RESTARTS ` ` - The configured maximum number restarts ` ` TORCHELASTIC_RUN_ID ` ` - Equal rendezvous ` ` run_id ` ` e g unique job id ` ` PYTHON_EXEC ` ` - System executable override If provided python user script will use value ` ` PYTHON_EXEC ` ` executable The ` sys executable ` used default Deployment ---------- Not needed C d backend Start rendezvous backend server get endpoint passed ` ` -- rdzv-endpoint ` ` ` ` torchrun ` ` Single-node multi-worker Start ` ` torchrun ` ` host start agent process which creates monitors local worker group Multi-node multi-worker Start ` ` torchrun ` ` same arguments all nodes participating training When using job cluster manager entry point command multi-node job should ` ` torchrun ` ` Failure Modes ------------- Worker failure For training job ` ` n ` ` workers ` ` k =n ` ` workers fail all workers stopped restarted up ` ` max_restarts ` ` Agent failure An agent failure results local worker group failure It up job manager fail entire job gang semantics attempt replace node Both behaviors supported agent Node failure Same agent failure Membership Changes ------------------ Node departure scale-down The agent notified departure all existing workers stopped new ` ` WorkerGroup ` ` formed all workers started new ` ` RANK ` ` ` ` WORLD_SIZE ` ` Node arrival scale-up The new node admitted job all existing workers stopped new ` ` WorkerGroup ` ` formed all workers started new ` ` RANK ` ` ` ` WORLD_SIZE ` ` Important Notices ----------------- This utility multi-process distributed single-node multi-node GPU training currently only achieves best performance using NCCL distributed backend Thus NCCL backend recommended backend use GPU training The environment variables necessary initialize Torch process group provided you module no need you pass ` ` RANK ` ` manually To initialize process group your training script simply run xdoctest +SKIP stub torch distributed dist dist init_process_group backend= gloo &#124; nccl In your training program you can either use regular distributed functions use func ` torch nn parallel DistributedDataParallel ` module If your training program uses GPUs training you would like use func ` torch nn parallel DistributedDataParallel ` module here how configure local_rank = int os environ LOCAL_RANK model = torch nn parallel DistributedDataParallel model device_ids= local_rank output_device=local_rank Please ensure ` ` device_ids ` ` argument set only GPU device id your code will operating This generally local rank process In other words ` ` device_ids ` ` needs ` ` int os environ LOCAL_RANK ` ` ` ` output_device ` ` needs ` ` int os environ LOCAL_RANK ` ` order use utility On failures membership changes ALL surviving workers killed immediately Make sure checkpoint your progress The frequency checkpoints should depend your job s tolerance lost work This module only supports homogeneous ` ` LOCAL_WORLD_SIZE ` ` That assumed all nodes run same number local workers per role ` ` RANK ` ` NOT stable Between restarts local workers node can assigned different range ranks than before NEVER hard code any assumptions about stable-ness ranks some correlation between ` ` RANK ` ` ` ` LOCAL_RANK ` ` When using elasticity ` ` min_size =max_size ` ` DO NOT hard code assumptions about ` ` WORLD_SIZE ` ` world size can change nodes allowed leave join It recommended your script have following structure main load_checkpoint checkpoint_path initialize train train batch iter dataset train_step batch should_checkpoint save_checkpoint checkpoint_path Recommended On worker errors tool will summarize details error e g time rank host pid traceback etc On each node first error timestamp heuristically reported Root Cause error To get tracebacks part error summary print out you must decorate your main entrypoint function your training script shown example below If decorated then summary will include traceback exception will only contain exitcode For details torchelastic error handling see https pytorch org docs stable elastic errors html torch distributed elastic multiprocessing errors record record main do train pass __name__ == __main__ main noqa E os sys uuid argparse ArgumentParser REMAINDER collections abc Callable importlib metadata typing Optional Union torch torch distributed argparse_util check_env env torch distributed elastic multiprocessing DefaultLogsSpecs LogsSpecs Std torch distributed elastic multiprocessing errors record torch distributed elastic rendezvous utils _parse_rendezvous_config torch distributed elastic utils macros torch distributed elastic utils logging get_logger torch distributed launcher api elastic_launch LaunchConfig torch numa binding AffinityMode _AffinityMode Signify private _ NumaOptions _NumaOptions torch utils backend_registration _get_custom_mod_func logger = get_logger __name__ get_args_parser - ArgumentParser Parse command line options parser = ArgumentParser description= Torch Distributed Elastic Training Launcher comma_separated_list value placeholder = COMMA_PLACEHOLDER value = value replace placeholder items = value split items = item replace placeholder item items items Worker node size related arguments parser add_argument -- nnodes action=env type=str default= help= Number nodes range nodes form minimum_nodes maximum_nodes parser add_argument -- nproc-per-node -- nproc_per_node action=env type=str default= help= Number workers per node supported values auto cpu gpu xpu int Rendezvous related arguments parser add_argument -- rdzv-backend -- rdzv_backend action=env type=str default= static help= Rendezvous backend parser add_argument -- rdzv-endpoint -- rdzv_endpoint action=env type=str default= help= Rendezvous backend endpoint usually form host port parser add_argument -- rdzv-id -- rdzv_id action=env type=str default= none help= User-defined group id parser add_argument -- rdzv-conf -- rdzv_conf action=env type=str default= help= Additional rendezvous configuration key = value key = value parser add_argument -- standalone action=check_env help= Start local standalone rendezvous backend represented C d TCP store free port Useful when launching single-node multi-worker job If specified -- rdzv-backend -- rdzv-endpoint -- rdzv-id auto-assigned any explicitly set values ignored User-code launch related arguments parser add_argument -- max-restarts -- max_restarts action=env type=int default= help= Maximum number worker group restarts before failing parser add_argument -- monitor-interval -- monitor_interval action=env type=float default= help= Interval seconds monitor state workers parser add_argument -- start-method -- start_method action=env type=str default= spawn choices= spawn fork forkserver help= Multiprocessing start method use when creating workers parser add_argument -- event-log-handler -- event_log_handler action=env type=str default= null help= name registered event logging handler see https docs pytorch org docs stable elastic events html parser add_argument -- role action=env type=str default= default help= User-defined role workers parser add_argument -m -- module action=check_env help= Change each process interpret launch script Python module executing same behavior python -m parser add_argument -- no-python -- no_python action=check_env help= Skip prepending training script python - just execute directly Useful when script Python script parser add_argument -- run-path -- run_path action=check_env help= Run training script runpy run_path same interpreter Script must provided abs path e g abs path script py Takes precedence over -- no-python parser add_argument -- log-dir -- log_dir action=env type=str default=None help= Base directory use log files e g var log torch elastic The same directory reused multiple runs unique job-level sub-directory created rdzv_id prefix parser add_argument -r -- redirects action=env type=str default= help= Redirect std streams into log file log directory e g -r redirects both stdout+stderr all workers -r redirects stdout local rank stderr local rank parser add_argument -t -- tee action=env type=str default= help= Tee std streams into log file also console see -- redirects format parser add_argument -- local-ranks-filter -- local_ranks_filter action=env type=str default= help= Only show logs specified ranks console e g -- local_ranks_filter= will only show logs rank This will only apply stdout stderr log files saved via -- redirect -- tee parser add_argument -- duplicate-stdout-filters -- duplicate_stdout_filters action=env type=comma_separated_list default= help= Duplicates logs streamed stdout another specified file list filters e g -- duplicate_stdout_filters apple orange will duplicate log lines matching apple OR orange An empty filters list won t duplicate any lines Use double comma escape comma parser add_argument -- duplicate-stderr-filters -- duplicate_stderr_filters action=env type=comma_separated_list default= help= Duplicates logs streamed stderr another specified file list filters e g -- duplicate_stdout_filters apple orange will duplicate log lines matching apple OR orange An empty filters list won t duplicate any lines Use double comma escape comma Backwards compatible parameters caffe distributed launch parser add_argument -- node-rank -- node_rank type=int action=env default= help= Rank node multi-node distributed training parser add_argument -- master-addr -- master_addr default= type=str action=env help= Address master node rank only used static rendezvous It should either IP address hostname rank For single node multi-proc training -- master-addr can simply IPv should have pattern ` ` parser add_argument -- master-port -- master_port default= type=int action=env help= Port master node rank used communication during distributed training It only used static rendezvous parser add_argument -- local-addr -- local_addr default=None type=str action=env help= Address local node If specified will use given address connection Else will look up local node address instead Else will default local machine s FQDN parser add_argument -- logs-specs -- logs_specs default=None type=str help= torchrun logs_specs group entrypoint name value must type LogsSpecs Can used override custom logging behavior parser add_argument -- numa-binding -- numa_binding type=str choices= mode value mode _AffinityMode default=None help= If provided we will affinitize worker processes based NUMA nodes better performance E g preferring allocate memory locally run CPUs same NUMA node NOTE This currently only supported GPUs we assume LOCAL_RANK process corresponds GPU index LOCAL_RANK If accurate your workload feature may pessimization Available options - node Processes bound cpu cores within NUMA node This good starting point other options may perform even slightly better some cases - socket Processes bound cpu cores within socket - exclusive Processes bound exclusive sets cpu cores within NUMA node - core-complex Processes bound cpu cores core-complex NOTE The core-complex option might achieve optimal performance architectures featuring single L cache per socket parser add_argument -- signals-to-handle -- signals_to_handle action=env type=str default= SIGTERM SIGINT SIGHUP SIGQUIT help= Comma-separated list signals handle forward subprocesses Default SIGTERM SIGINT SIGHUP SIGQUIT Common additional signals SIGUSR SIGUSR used SLURM environments Positional arguments parser add_argument training_script type=str help= Full path single GPU training program script launched parallel followed all arguments training script Rest training program parser add_argument training_script_args nargs=REMAINDER parser parse_args args parser = get_args_parser parser parse_args args parse_min_max_nnodes nnodes str arr = nnodes split len arr == min_nodes = max_nodes = int arr len arr == min_nodes = int arr max_nodes = int arr raise RuntimeError f nnodes= nnodes MIN MAX format noqa E min_nodes max_nodes determine_local_world_size nproc_per_node str try logger info Using nproc_per_node= s nproc_per_node int nproc_per_node except ValueError e nproc_per_node == cpu num_proc = os cpu_count device_type = cpu nproc_per_node == gpu torch cuda is_available raise ValueError Cuda available e device_type = gpu num_proc = torch cuda device_count nproc_per_node == xpu torch xpu is_available raise ValueError Xpu available e device_type = xpu num_proc = torch xpu device_count nproc_per_node == torch _C _get_privateuse _backend_name _get_custom_mod_func is_available raise ValueError f nproc_per_node available e device_type = nproc_per_node num_proc = _get_custom_mod_func device_count nproc_per_node == auto torch accelerator is_available num_proc = torch accelerator device_count device_type = torch accelerator current_accelerator type type ignore union-attr num_proc = os cpu_count device_type = cpu raise ValueError f Unsupported nproc_per_node value nproc_per_node e logger info Using nproc_per_node= s setting nproc_per_node s since instance has s s nproc_per_node num_proc num_proc device_type num_proc get_rdzv_endpoint args args rdzv_backend == static args rdzv_endpoint f args master_addr args master_port noqa E args rdzv_endpoint get_use_env args - bool Retrieve ` ` use_env ` ` args ` ` use_env ` ` legacy argument ` ` use_env ` ` False ` ` -- node-rank ` ` argument will transferred all worker processes ` ` use_env ` ` only used ` ` torch distributed launch ` ` will deprecated future releases hasattr args use_env True args use_env _get_logs_specs_class logs_specs_name Optional str - type LogsSpecs Attempts load ` torchrun logs_spec ` entrypoint key ` logs_specs_name ` param Provides plugin mechanism provide custom implementation LogsSpecs Returns ` DefaultLogsSpecs ` when logs_spec_name None Raises ValueError when entrypoint ` logs_spec_name ` can t found entrypoints logs_specs_cls = None logs_specs_name None eps = metadata entry_points group = eps select group= torchrun logs_specs group select name=logs_specs_name logs_specs_cls = group logs_specs_name load logs_specs_cls None raise ValueError f Could find entrypoint under torchrun logs_specs logs_specs_name key logger info Using logs_spec s mapped s logs_specs_name str logs_specs_cls logs_specs_cls = DefaultLogsSpecs logs_specs_cls config_from_args args - tuple LaunchConfig Union Callable str list str If ` ` args ` ` passed defaults ` ` sys argv ` ` min_nodes max_nodes = parse_min_max_nnodes args nnodes assert min_nodes = max_nodes assert args max_restarts = hasattr args master_addr args rdzv_backend = static args rdzv_endpoint logger warning master_addr only used static rdzv_backend when rdzv_endpoint specified nproc_per_node = determine_local_world_size args nproc_per_node OMP_NUM_THREADS os environ nproc_per_node omp_num_threads = logger warning \n \n Setting OMP_NUM_THREADS environment variable each process s default avoid your system being overloaded please further tune variable optimal performance your application needed \n omp_num_threads This env variable will passed down subprocesses os environ OMP_NUM_THREADS = str omp_num_threads log_line_prefix_template = os getenv TORCHELASTIC_LOG_LINE_PREFIX_TEMPLATE rdzv_configs = _parse_rendezvous_config args rdzv_conf args rdzv_backend == static rdzv_configs rank = args node_rank rdzv_endpoint = get_rdzv_endpoint args ranks Optional set int = None args local_ranks_filter try ranks = set map int args local_ranks_filter split assert ranks except Exception e raise ValueError -- local_ranks_filter must comma-separated list integers e g -- local_ranks_filter= e logs_specs_cls type LogsSpecs = _get_logs_specs_class args logs_specs pyrefly ignore bad-instantiation logs_specs = logs_specs_cls log_dir=args log_dir redirects=Std from_str args redirects tee=Std from_str args tee local_ranks_filter=ranks numa_options = None args numa_binding None _NumaOptions affinity_mode=_AffinityMode args numa_binding config = LaunchConfig min_nodes=min_nodes max_nodes=max_nodes nproc_per_node=nproc_per_node run_id=args rdzv_id role=args role rdzv_endpoint=rdzv_endpoint rdzv_backend=args rdzv_backend rdzv_configs=rdzv_configs max_restarts=args max_restarts monitor_interval=args monitor_interval start_method=args start_method log_line_prefix_template=log_line_prefix_template local_addr=args local_addr logs_specs=logs_specs event_log_handler=args event_log_handler numa_options=numa_options signals_to_handle=args signals_to_handle duplicate_stdout_filters=args duplicate_stdout_filters duplicate_stderr_filters=args duplicate_stderr_filters with_python = args no_python cmd Union Callable str cmd_args = use_env = get_use_env args args run_path cmd = run_script_path cmd_args append args training_script with_python cmd = os getenv PYTHON_EXEC sys executable cmd_args append -u args module cmd_args append -m cmd_args append args training_script args module raise ValueError Don t use both -- no-python flag -- module flag same time cmd = args training_script use_env cmd_args append f -- local-rank= macros local_rank cmd_args extend args training_script_args config cmd cmd_args run_script_path training_script str training_script_args str Run provided ` training_script ` within interpreter Usage ` script_as_function abs path script py -- arg val ` runpy sys sys argv = training_script + training_script_args runpy run_path sys argv run_name= __main__ run args torch multiprocessing _set_thread_name pt_elastic args standalone args rdzv_backend = c d args rdzv_endpoint = localhost args rdzv_id = str uuid uuid logger info \n \n Rendezvous info \n -- rdzv-backend= s -- rdzv-endpoint= s -- rdzv-id= s\n \n args rdzv_backend args rdzv_endpoint args rdzv_id config cmd cmd_args = config_from_args args elastic_launch config=config entrypoint=cmd cmd_args record main args=None args = parse_args args run args __name__ == __main__ main