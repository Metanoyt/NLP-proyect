Owner s oncall jit ruff noqa F io os sys itertools product typing Union hypothesis strategies st hypothesis example given settings torch Make helper files test importable pytorch_test_dir = os path dirname os path dirname os path realpath __file__ sys path append pytorch_test_dir torch jit mobile _load_for_lite_interpreter torch testing _internal common_utils raise_on_run_directly torch testing _internal jit_utils JitTestCase TestSaveLoadForOpVersion JitTestCase Helper returns module after saving loading _save_load_module m scripted_module = torch jit script m buffer = io BytesIO torch jit save scripted_module buffer buffer seek torch jit load buffer _save_load_mobile_module m scripted_module = torch jit script m buffer = io BytesIO scripted_module _save_to_buffer_for_lite_interpreter buffer seek _load_for_lite_interpreter buffer Helper which returns result function exception function threw _try_fn fn args kwargs try fn args kwargs except Exception e e _verify_no kind m _verify_count kind m _verify_count kind m count node_count = sum str n count kind n m graph nodes assertEqual node_count count Tests verify Torchscript remaps aten div _ versions - call either aten true_divide _ input float type truncated aten divide _ otherwise NOTE currently compares against current div behavior too since div behavior has yet been updated settings max_examples= deadline= A total examples will generated given sample_input=st tuples st integers min_value= max_value= st floats min_value= max_value= Generate pair integer float example Ensure example will covered test_versioned_div_tensor sample_input historic_div other is_floating_point other is_floating_point true_divide other divide other rounding_mode= trunc Tensor x Tensor MyModule torch nn Module forward b result_ = b result_ = torch div b result_ = div b result_ result_ result_ Loads historic module try v _mobile_module = _load_for_lite_interpreter pytorch_test_dir + cpp jit upgrader_models test_versioned_div_tensor_v ptl except Exception e skipTest Failed load fixture current_mobile_module = _save_load_mobile_module MyModule val_a val_b product sample_input sample_input = torch tensor val_a b = torch tensor val_b _helper m fn m_results = _try_fn m b fn_result = _try_fn fn b isinstance m_results Exception assertTrue isinstance fn_result Exception result m_results assertEqual result fn_result _helper v _mobile_module historic_div _helper current_mobile_module torch div settings max_examples= deadline= A total examples will generated given sample_input=st tuples st integers min_value= max_value= st floats min_value= max_value= Generate pair integer float example Ensure example will covered test_versioned_div_tensor_inplace sample_input historic_div_ other is_floating_point other is_floating_point true_divide_ other divide_ other rounding_mode= trunc MyModule torch nn Module forward b = b try v _mobile_module = _load_for_lite_interpreter pytorch_test_dir + cpp jit upgrader_models test_versioned_div_tensor_inplace_v ptl except Exception e skipTest Failed load fixture current_mobile_module = _save_load_mobile_module MyModule val_a val_b product sample_input sample_input = torch tensor val_a b = torch tensor val_b _helper m fn fn_result = _try_fn fn clone b m_result = _try_fn m b isinstance m_result Exception assertTrue fn_result Exception assertEqual m_result fn_result assertEqual m_result _helper v _mobile_module historic_div_ Recreates since modified place = torch tensor val_a _helper current_mobile_module torch Tensor div_ settings max_examples= deadline= A total examples will generated given sample_input=st tuples st integers min_value= max_value= st floats min_value= max_value= Generate pair integer float example Ensure example will covered test_versioned_div_tensor_out sample_input historic_div_out other out is_floating_point other is_floating_point out is_floating_point torch true_divide other out=out torch divide other out=out rounding_mode= trunc MyModule torch nn Module forward b out div b out=out try v _mobile_module = _load_for_lite_interpreter pytorch_test_dir + cpp jit upgrader_models test_versioned_div_tensor_out_v ptl except Exception e skipTest Failed load fixture current_mobile_module = _save_load_mobile_module MyModule val_a val_b product sample_input sample_input = torch tensor val_a b = torch tensor val_b out torch empty torch empty dtype=torch long _helper m fn fn_result = None fn torch div fn_result = _try_fn fn b out=out clone fn_result = _try_fn fn b out clone m_result = _try_fn m b out isinstance m_result Exception assertTrue fn_result Exception assertEqual m_result fn_result assertEqual m_result out _helper v _mobile_module historic_div_out _helper current_mobile_module torch div settings max_examples= deadline= A total examples will generated given sample_input=st tuples st integers min_value= max_value= st floats min_value= max_value= Generate pair integer float example Ensure example will covered test_versioned_div_scalar sample_input historic_div_scalar_float other float torch true_divide other historic_div_scalar_int other int is_floating_point torch true_divide other torch divide other rounding_mode= trunc MyModuleFloat torch nn Module forward b float b MyModuleInt torch nn Module forward b int b try v _mobile_module_float = _load_for_lite_interpreter pytorch_test_dir + jit fixtures test_versioned_div_scalar_float_v ptl v _mobile_module_int = _load_for_lite_interpreter pytorch_test_dir + cpp jit upgrader_models test_versioned_div_scalar_int_v ptl except Exception e skipTest Failed load fixture current_mobile_module_float = _save_load_mobile_module MyModuleFloat current_mobile_module_int = _save_load_mobile_module MyModuleInt val_a val_b product sample_input sample_input = torch tensor val_a b = val_b _helper m fn m_result = _try_fn m b fn_result = _try_fn fn b isinstance m_result Exception assertTrue fn_result Exception assertEqual m_result fn_result isinstance b float _helper v _mobile_module_float current_mobile_module_float _helper current_mobile_module_float torch div _helper v _mobile_module_int historic_div_scalar_int _helper current_mobile_module_int torch div settings max_examples= deadline= A total examples will generated given sample_input=st tuples st integers min_value= max_value= st floats min_value= max_value= Generate pair integer float example Ensure example will covered test_versioned_div_scalar_reciprocal sample_input historic_div_scalar_float_reciprocal other float other historic_div_scalar_int_reciprocal other int is_floating_point other torch divide other rounding_mode= trunc MyModuleFloat torch nn Module forward b float b MyModuleInt torch nn Module forward b int b try v _mobile_module_float = _load_for_lite_interpreter pytorch_test_dir + cpp jit upgrader_models test_versioned_div_scalar_reciprocal_float_v ptl v _mobile_module_int = _load_for_lite_interpreter pytorch_test_dir + cpp jit upgrader_models test_versioned_div_scalar_reciprocal_int_v ptl except Exception e skipTest Failed load fixture current_mobile_module_float = _save_load_mobile_module MyModuleFloat current_mobile_module_int = _save_load_mobile_module MyModuleInt val_a val_b product sample_input sample_input = torch tensor val_a b = val_b _helper m fn m_result = _try_fn m b fn_result = None Reverses argument order torch div fn torch div fn_result = _try_fn torch div b fn_result = _try_fn fn b isinstance m_result Exception assertTrue isinstance fn_result Exception fn torch div is_floating_point assertEqual m_result fn_result Skip when fn torch div integral because historic_div_scalar_int performs floored division pass isinstance b float _helper v _mobile_module_float current_mobile_module_float _helper current_mobile_module_float torch div _helper v _mobile_module_int current_mobile_module_int _helper current_mobile_module_int torch div settings max_examples= deadline= A total examples will generated given sample_input=st tuples st integers min_value= max_value= st floats min_value= max_value= Generate pair integer float example Ensure example will covered test_versioned_div_scalar_inplace sample_input historic_div_scalar_float_inplace other float true_divide_ other historic_div_scalar_int_inplace other int is_floating_point true_divide_ other divide_ other rounding_mode= trunc MyModuleFloat torch nn Module forward b float = b MyModuleInt torch nn Module forward b int = b try v _mobile_module_float = _load_for_lite_interpreter pytorch_test_dir + cpp jit upgrader_models test_versioned_div_scalar_inplace_float_v ptl v _mobile_module_int = _load_for_lite_interpreter pytorch_test_dir + cpp jit upgrader_models test_versioned_div_scalar_inplace_int_v ptl except Exception e skipTest Failed load fixture current_mobile_module_float = _save_load_module MyModuleFloat current_mobile_module_int = _save_load_module MyModuleInt val_a val_b product sample_input sample_input = torch tensor val_a b = val_b _helper m fn m_result = _try_fn m b fn_result = _try_fn fn b isinstance m_result Exception assertTrue fn_result Exception assertEqual m_result fn_result isinstance b float _helper current_mobile_module_float torch Tensor div_ _helper current_mobile_module_int torch Tensor div_ NOTE Scalar division already true division op version so test verifies behavior unchanged test_versioned_div_scalar_scalar MyModule torch nn Module forward float b int c float d int result_ = b result_ = c result_ = b c result_ = b d result_ result_ result_ result_ try v _mobile_module = _load_for_lite_interpreter pytorch_test_dir + cpp jit upgrader_models test_versioned_div_scalar_scalar_v ptl except Exception e skipTest Failed load fixture current_mobile_module = _save_load_mobile_module MyModule _helper m fn vals = m_result = m vals fn_result = fn vals mr hr zip m_result fn_result assertEqual mr hr _helper v _mobile_module current_mobile_module test_versioned_linspace Module torch nn Module forward Union int float complex b Union int float complex c = torch linspace b steps= d = torch linspace b steps= c d scripted_module = torch jit load pytorch_test_dir + jit fixtures test_versioned_linspace_v ptl buffer = io BytesIO scripted_module _save_to_buffer_for_lite_interpreter buffer seek v _mobile_module = _load_for_lite_interpreter buffer current_mobile_module = _save_load_mobile_module Module sample_inputs = - + j + j b sample_inputs output_with_step output_without_step = v _mobile_module b current_with_step current_without_step = current_mobile_module b when no step given should have used assertTrue output_without_step size dim= == assertTrue output_with_step size dim= == outputs should equal newest version assertEqual output_with_step current_with_step assertEqual output_without_step current_without_step test_versioned_linspace_out Module torch nn Module forward Union int float complex b Union int float complex out torch Tensor torch linspace b steps= out=out model_path = pytorch_test_dir + jit fixtures test_versioned_linspace_out_v ptl loaded_model = torch jit load model_path buffer = io BytesIO loaded_model _save_to_buffer_for_lite_interpreter buffer seek v _mobile_module = _load_for_lite_interpreter buffer current_mobile_module = _save_load_mobile_module Module sample_inputs = torch empty dtype=torch int torch empty dtype=torch int - torch empty dtype=torch int torch empty dtype=torch int torch empty dtype=torch float torch empty dtype=torch float + j + j torch empty dtype=torch complex torch empty dtype=torch complex start end out_for_old out_for_new sample_inputs output = v _mobile_module start end out_for_old output_current = current_mobile_module start end out_for_new when no step given should have used assertTrue output size dim= == Upgraded model should match new version output assertEqual output output_current test_versioned_logspace Module torch nn Module forward Union int float complex b Union int float complex c = torch logspace b steps= d = torch logspace b steps= c d scripted_module = torch jit load pytorch_test_dir + jit fixtures test_versioned_logspace_v ptl buffer = io BytesIO scripted_module _save_to_buffer_for_lite_interpreter buffer seek v _mobile_module = _load_for_lite_interpreter buffer current_mobile_module = _save_load_mobile_module Module sample_inputs = - + j + j b sample_inputs output_with_step output_without_step = v _mobile_module b current_with_step current_without_step = current_mobile_module b when no step given should have used assertTrue output_without_step size dim= == assertTrue output_with_step size dim= == outputs should equal newest version assertEqual output_with_step current_with_step assertEqual output_without_step current_without_step test_versioned_logspace_out Module torch nn Module forward Union int float complex b Union int float complex out torch Tensor torch logspace b steps= out=out model_path = pytorch_test_dir + jit fixtures test_versioned_logspace_out_v ptl loaded_model = torch jit load model_path buffer = io BytesIO loaded_model _save_to_buffer_for_lite_interpreter buffer seek v _mobile_module = _load_for_lite_interpreter buffer current_mobile_module = _save_load_mobile_module Module sample_inputs = torch empty dtype=torch int torch empty dtype=torch int - torch empty dtype=torch int torch empty dtype=torch int torch empty dtype=torch float torch empty dtype=torch float + j + j torch empty dtype=torch complex torch empty dtype=torch complex start end out_for_old out_for_new sample_inputs output = v _mobile_module start end out_for_old output_current = current_mobile_module start end out_for_new when no step given should have used assertTrue output size dim= == Upgraded model should match new version output assertEqual output output_current __name__ == __main__ raise_on_run_directly test test_jit py