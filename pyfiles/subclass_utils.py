mypy allow-untyped-defs This file contains utilities tracing through __torch_dispatch__ based tensor subclasses modes AOTAutograd s responsibility trace through all pytorch capabilities live pytorch dispatcher includes tensor subclasses implement __torch_dispatch__ collections typing collections abc Callable Iterable typing Any Optional TypeGuard TypeVar Union torch torch utils _pytree pytree torch SymInt Tensor torch _subclasses fake_tensor get_plain_tensors torch types IntLikeType torch utils _python_dispatch is_traceable_wrapper_subclass descriptors AOTInput AOTOutput DummyAOTInput SubclassGetAttrAOTInput SubclassGetAttrAOTOutput SubclassSizeAOTInput SubclassSizeAOTOutput SubclassStrideAOTInput SubclassStrideAOTOutput schemas FxValue MutationType PlainTensorMeta SubclassCreationMeta ViewAndMutationMeta utils strict_zip zip = strict_zip T = TypeVar T bound=torch Tensor requires_subclass_dispatch args fw_metadata ViewAndMutationMeta - bool args_flattened = pytree arg_tree_leaves args any_subclass_args = any is_traceable_wrapper_subclass x x args_flattened isinstance x Tensor torch _functorch _aot_autograd schemas SubclassCreationMeta any_subclass_outputs = any type x SubclassCreationMeta x fw_metadata subclass_fw_graph_out_meta This tells us whether we need perform any unwrapping wrapping tensor subclasses runtime any_subclass_args any_subclass_outputs schemas MemoryFormatMeta maybe_suggest_memory_format t with_memory_format bool - Optional MemoryFormatMeta with_memory_format None MemoryFormatMeta from_tensor t get_subclass_typing_container tensor_subclass torch Tensor - dict type torch Tensor list type torch Tensor Given subclass returns recursive dictionary mapping each inner tensors its subclass types _get_types_for_subclass tensor_subclass torch Tensor - None is_traceable_wrapper_subclass tensor_subclass tracker type tensor_subclass append tensor_subclass inner_keys _ = tensor_subclass __tensor_flatten__ key inner_keys inner_tensor = getattr tensor_subclass key _get_types_for_subclass inner_tensor tracker dict Any list Any = collections defaultdict list _get_types_for_subclass tensor_subclass tracker create_subclass_metadata Any start_idx int count_symints bool with_memory_format bool = False is_traceable_wrapper_subclass idx = start_idx + PlainTensorMeta idx memory_format=maybe_suggest_memory_format with_memory_format idx inner_keys metadata = __tensor_flatten__ new_start_idx = start_idx attrs = key inner_keys new_subclass_meta new_start_idx = create_subclass_metadata getattr key new_start_idx count_symints=count_symints with_memory_format=with_memory_format attrs key = new_subclass_meta It must because is_traceable_wrapper_subclass - mypy smart assert isinstance Tensor new_start_idx = new_start_idx + count_symints len enumerate_filter_symints size + count_symints len enumerate_filter_symints stride SubclassCreationMeta flat_tensor_start_idx=start_idx arg_count=new_start_idx - start_idx included_subclass_symints=count_symints attrs=attrs meta=metadata outer_size=a size type ignore attr-defined arg-type outer_stride=a stride type ignore arg-type original_subclass=a memory_format=maybe_suggest_memory_format with_memory_format new_start_idx Given flat list arguments some which may tensor subclasses computes metadata about how reconstruct current list subclasses we given their flattened dense tensors instead create_subclass_meta curr_args Union list Any tuple Any count_symints bool = True with_memory_format bool = False - list Union PlainTensorMeta SubclassCreationMeta idx = infos list Union PlainTensorMeta SubclassCreationMeta = curr_args is_traceable_wrapper_subclass assert isinstance Tensor start_idx = idx subclass_meta _ = create_subclass_metadata start_idx count_symints=count_symints with_memory_format=with_memory_format infos append subclass_meta cnt = subclass_meta arg_count infos append PlainTensorMeta idx memory_format=maybe_suggest_memory_format with_memory_format cnt = idx += cnt infos enumerate_filter_symints lst Iterable IntLikeType - list tuple int SymInt Capture all SymInts iterable symint_check s IntLikeType - TypeGuard SymInt isinstance s SymInt s node is_nested_int i s i s enumerate lst symint_check s compute_symint_placeholders lst Iterable Union None int SymInt - list bool Non-nested symints replaced None ` make_runtime_safe ` s None s lst Intended make easier define function either AOTInput - AOTInput AOTOutput - AOTOutput other combos AOTDescriptor = TypeVar AOTDescriptor AOTInput AOTOutput This function takes pytree arguments unwraps any tensor subclasses NOTE The reason append_symints At compile time we append extra symint args when unwrapping primals tangents because they should always share symints primals We also append extra symints when unwrapping subclass outputs traced function so we can them extra outputs At runtime we similarly append subclass sizes when we unwrap subclass primals tangents entry forward See runtime version function below unwrap_tensor_subclasses wrapped_args list FxValue wrapped_args_descs list AOTDescriptor append_symints bool - tuple list FxValue list AOTDescriptor flatten_subclass t FxValue desc AOTDescriptor out tuple list FxValue list AOTDescriptor unwrap subclass into plain tensors their size stride append_symint True is_traceable_wrapper_subclass t out append t out append desc attrs _ = t __tensor_flatten__ attr attrs inner_tensor = getattr t attr n_desc Any = SubclassGetAttrAOTInput desc attr isinstance desc AOTInput pyrefly ignore bad-argument-type SubclassGetAttrAOTOutput desc attr flatten_subclass inner_tensor n_desc out=out append_symints sizes = enumerate_filter_symints t size strides = enumerate_filter_symints t stride out extend s _ s sizes out extend s _ s strides isinstance desc AOTInput out extend SubclassSizeAOTInput desc i i _ sizes type ignore misc out extend SubclassStrideAOTInput desc i i _ strides type ignore misc out extend SubclassSizeAOTOutput desc i i _ sizes type ignore misc out extend SubclassStrideAOTOutput desc i i _ strides type ignore misc xs_inner list FxValue = descs_inner list AOTDescriptor = x desc zip wrapped_args wrapped_args_descs pyrefly ignore bad-argument-type flatten_subclass typing cast Tensor x desc out= xs_inner descs_inner xs_inner descs_inner subclass_metas needed runtime compute which indices symints outer_size outer_stride runtime_unwrap_tensor_subclasses wrapped_args list Union Tensor int append_symints bool subclass_metas Optional list Union PlainTensorMeta SubclassCreationMeta = None flatten_subclass x Tensor meta Optional SubclassCreationMeta out is_traceable_wrapper_subclass x out append x out assert isinstance x Tensor attrs _ = x __tensor_flatten__ attr attrs inner_tensor = getattr x attr pyrefly ignore missing-attribute inner_meta = meta attrs get attr flatten_subclass inner_tensor inner_meta out=out append_symints assert isinstance meta SubclassCreationMeta outer_size size = x size symint_placeholders = compute_symint_placeholders meta outer_size assert len size == len symint_placeholders out extend r r is_symint zip size symint_placeholders is_symint outer_stride stride = x stride symint_placeholders = compute_symint_placeholders meta outer_stride assert len stride == len symint_placeholders out extend r r is_symint zip stride symint_placeholders is_symint out xs_inner list Union int Tensor SymInt = append_symints assert subclass_metas None idx x enumerate wrapped_args is_traceable_wrapper_subclass x xs_inner append x continue subclass_metas None get_plain_tensors typing cast Tensor x out=xs_inner meta = subclass_metas idx assert isinstance meta SubclassCreationMeta flatten_subclass typing cast Tensor x meta out=xs_inner xs_inner unwrap_tensor_subclasses_with_indices_to_original wrapped_args ret_unwrapped = ret_indices_to_original = i enumerate wrapped_args a_unwrapped _ = unwrap_tensor_subclasses DummyAOTInput append_symints=False ret_unwrapped extend a_unwrapped n = len a_unwrapped ret_indices_to_original extend i n ret_unwrapped ret_indices_to_original remap_unwrapped_subclass_arg_indices wrapped_args static_input_indices static_input_indices = set static_input_indices new_ind = remapped_static_indices = i arg enumerate wrapped_args num_indices = is_traceable_wrapper_subclass arg num_indices = len get_plain_tensors typing cast Tensor arg out= + len enumerate_filter_symints arg size + len enumerate_filter_symints arg stride _ range num_indices i static_input_indices remapped_static_indices append new_ind new_ind += remapped_static_indices Turns flattened list tensor arguments into maybe subclass tensors This function used both trace time runtime so we have is_runtime flag telling us which context we re wrap_tensor_subclasses unwrapped_args Union tuple Any list Any subclass_metas list Union PlainTensorMeta SubclassCreationMeta num_fw_outs_saved_for_bw Optional int = None included_subclass_symints bool = False is_runtime bool = False make_subclass_override Optional Callable = None - tuple Any wrapped_args = num_args_tallied = subclass_meta subclass_metas isinstance subclass_meta PlainTensorMeta wrapped_args append unwrapped_args subclass_meta unwrapped_idx num_args_tallied += assert isinstance subclass_meta SubclassCreationMeta assert subclass_meta included_subclass_symints == included_subclass_symints make_subclass_override wrapped_args append make_subclass_override subclass_meta is_runtime unwrapped_args wrapped_args append subclass_meta creation_fn unwrapped_args is_runtime=is_runtime num_args_tallied += subclass_meta arg_count Note Partitioner handling Subclasses Part At beginning AOTAutograd we collect metadata inputs outputs user fw figure out which inputs outputs subclasses how reconstruct subclasses after flattening them When function called runtime forward we have been passed list flattened dense-tensor fw-outs need reconstruct any subclass fw outs One reasonable question you should ask when should dense_tensor - subclass_tensor wrapping happen Answer we do inside our compiled autograd Function This seems like morally right place autograd happens above subclass desugaring so autograd should see actual tensor subclasses runtime flattened dense tensors This causes tricky interaction though when we run min-cut partitioner divvy up joint graph into forward backward graph we end up some activations show up extra outputs compiled forward graph user outputs These activations visible user so there s no need us wrap them back into subclasses On top when we first computed subclass metadata ` run_functionalized_fw_and_collect_metadata ` we computed subclass metadata every forward output did include activations created partitioner result ` unwrapped_args ` here will correspond unwrapped_user_fw_outs activations ` subclass_metas ` will only correspond subclass metadata ` user_fw_outs ` We then need make sure we wrapped_user_fw_outs activations num_fw_outs_saved_for_bw None assert len unwrapped_args == num_args_tallied + num_fw_outs_saved_for_bw f Expected number actual unwrapped-subclass outputs len unwrapped_args equal f number args calculated subclasses num_args_tallied plus number f additional activations saved backward pass num_fw_outs_saved_for_bw activations = unwrapped_args num_args_tallied isinstance wrapped_args tuple isinstance activations tuple wrapped_args + activations tuple list wrapped_args + list activations assert len unwrapped_args == num_args_tallied f Expected len unwrapped_args == num_args_tallied tuple wrapped_args Given bunch dense tensor arguments function potentially wraps them into tensor subclasses This function carefully handles inference vs joint cases - when is_joint_structure True args primals tangents - when is_joint_structure False args primals wrap_tensor_subclasses_maybe_joint unwrapped_args is_joint_structure bool meta ViewAndMutationMeta - Union tuple Any list Any Since function reused both inference joint graphs is_joint_structure assert isinstance unwrapped_args tuple len unwrapped_args == assert isinstance unwrapped_args tuple list isinstance unwrapped_args tuple list primals tangents = unwrapped_args unwrapped_args wrapped_primals = wrap_tensor_subclasses primals subclass_metas=meta subclass_inp_meta included_subclass_symints=True wrapped_tangents = wrap_tensor_subclasses tangents subclass_metas=meta subclass_tangent_meta included_subclass_symints=False wrapped_primals wrapped_tangents wrapped_args = wrap_tensor_subclasses unwrapped_args subclass_metas=meta subclass_inp_meta included_subclass_symints=True wrapped_args compute_inner_mutated_inp_indices_from_subclass_meta fw_metadata ViewAndMutationMeta inner_metadata ViewAndMutationMeta - list int Note Recomputing subclass mutation handling Generally subclass requires grad its components will require grad But purposes tracking returned tensors we should treat those component tensors they require grad For example subclass tensor requires grad will mutated way requires us handle mutation outside graph we need forward graph The inner_meta data won t consider component tensors they need returned because they don t require grad really we should handle those tensors same way we handle subclass tensor itself i e we d include subclass tensor part outputs then we should also include component tensors To do we patch num_mutated_inp_runtime_indices below expanding inputs outer subclass tensors propagating updated_input_info = inner_idx = fw_metadata subclass_inp_meta Sometimes we don t have subclass info e g synthetic_base codepaths inner_metadata mutated_inp_runtime_indices assert len fw_metadata subclass_inp_meta == len fw_metadata input_info outer_idx inp_meta enumerate fw_metadata subclass_inp_meta isinstance inp_meta PlainTensorMeta assert outer_idx len fw_metadata input_info inner_metadata None assert inner_idx len inner_metadata input_info assert inner_metadata input_info inner_idx == fw_metadata input_info outer_idx updated_input_info append fw_metadata input_info outer_idx inner_idx += assert inp_meta original_subclass None _ range inp_meta arg_count updated_input_info append fw_metadata input_info outer_idx inner_idx += inner_metadata None assert len inner_metadata input_info == len updated_input_info i i inp enumerate updated_input_info inp mutation_type == MutationType MUTATED_OUT_GRAPH