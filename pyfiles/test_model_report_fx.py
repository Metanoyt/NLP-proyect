Owner s oncall quantization ruff noqa F torch torch nn nn torch ao quantization quantize_fx quantize_fx torch nn functional F torch ao quantization QConfig QConfigMapping torch ao quantization fx _model_report detector DynamicStaticDetector InputWeightEqualizationDetector PerChannelDetector OutlierDetector torch ao quantization fx _model_report model_report_observer ModelReportObserver torch ao quantization fx _model_report model_report_visualizer ModelReportVisualizer torch ao quantization fx _model_report model_report ModelReport torch ao quantization observer HistogramObserver default_per_channel_weight_observer default_observer torch ao nn intrinsic modules fused ConvReLU d LinearReLU torch testing _internal common_quantization ConvModel QuantizationTestCase SingleLayerLinearModel TwoLayerLinearModel skipIfNoFBGEMM skipIfNoQNNPACK override_quantized_engine torch testing _internal common_utils raise_on_run_directly Partition input domain Model contains conv linear both conv linear Model contains ConvTransposeNd supported per_channel Model post training quantization model quantization aware training model Model composed nn Sequential composed structure QConfig utilizes per_channel weight observer backend uses non per_channel weight observer QConfig_dict uses only one default qconfig Qconfig dict uses unique qconfigs Partition output domain There possible changes suggestions there no changes suggestions Default output string no optimizations possible DEFAULT_NO_OPTIMS_ANSWER_STRING = Further Optimizations backend \nNo further per_channel optimizations possible Example Sequential Model multiple Conv Linear nesting involved NESTED_CONV_LINEAR_EXAMPLE = torch nn Sequential torch nn Conv d torch nn Sequential torch nn Linear torch nn ReLU torch nn Linear torch nn ReLU torch nn Conv d Example Sequential Model Conv sub-class example LAZY_CONV_LINEAR_EXAMPLE = torch nn Sequential torch nn LazyConv d torch nn Sequential torch nn Linear torch nn ReLU torch nn ReLU torch nn Linear torch nn ReLU torch nn LazyConv d Example Sequential Model Fusion directly built into model FUSION_CONV_LINEAR_EXAMPLE = torch nn Sequential ConvReLU d torch nn Conv d torch nn ReLU torch nn Sequential LinearReLU torch nn Linear torch nn ReLU LinearReLU torch nn Linear torch nn ReLU torch nn Conv d Test example model use tests ThreeOps nn Module __init__ - None super __init__ linear = nn Linear bn = nn BatchNorm d relu = nn ReLU forward x x = linear x x = bn x x = relu x x get_example_inputs torch randn TwoThreeOps nn Module __init__ - None super __init__ block = ThreeOps block = ThreeOps forward x x = block x y = block x z = x + y z = F relu z z get_example_inputs torch randn TestFxModelReportDetector QuantizationTestCase Prepares calibrate model _prepare_model_and_run_input model q_config_mapping input model_prep = torch ao quantization quantize_fx prepare_fx model q_config_mapping input prep model model_prep input sum calibrate model model_prep Case includes one conv linear post training quantization composed module qconfig uses per_channel weight observer Only qconfig qconfig dict Output has no changes suggestions skipIfNoFBGEMM test_simple_conv override_quantized_engine fbgemm torch backends quantized engine = fbgemm q_config_mapping = QConfigMapping q_config_mapping set_global torch ao quantization get_default_qconfig torch backends quantized engine input = torch randn prepared_model = _prepare_model_and_run_input ConvModel q_config_mapping input run detector per_channel_detector = PerChannelDetector torch backends quantized engine optims_str per_channel_info = per_channel_detector generate_detector_report prepared_model no optims possible there should nothing per_channel_status assertEqual optims_str DEFAULT_NO_OPTIMS_ANSWER_STRING format torch backends quantized engine there should only one conv there model assertEqual per_channel_info conv backend torch backends quantized engine assertEqual len per_channel_info assertEqual next iter per_channel_info conv assertEqual per_channel_info conv per_channel_quantization_supported True assertEqual per_channel_info conv per_channel_quantization_used True Case includes Multiple conv linear post training quantization composed module qconfig doesn t use per_channel weight observer Only qconfig qconfig dict Output has possible changes suggestions skipIfNoQNNPACK test_multi_linear_model_without_per_channel override_quantized_engine qnnpack torch backends quantized engine = qnnpack q_config_mapping = QConfigMapping q_config_mapping set_global torch ao quantization get_default_qconfig torch backends quantized engine prepared_model = _prepare_model_and_run_input TwoLayerLinearModel q_config_mapping TwoLayerLinearModel get_example_inputs run detector per_channel_detector = PerChannelDetector torch backends quantized engine optims_str per_channel_info = per_channel_detector generate_detector_report prepared_model there should optims possible assertNotEqual optims_str DEFAULT_NO_OPTIMS_ANSWER_STRING format torch backends quantized engine pick random key look rand_key str = next iter per_channel_info keys assertEqual per_channel_info rand_key backend torch backends quantized engine assertEqual len per_channel_info each linear layer should supported used linear_key per_channel_info keys module_entry = per_channel_info linear_key assertEqual module_entry per_channel_quantization_supported True assertEqual module_entry per_channel_quantization_used False Case includes Multiple conv linear post training quantization composed Module qconfig doesn t use per_channel weight observer More than qconfig qconfig dict Output has possible changes suggestions skipIfNoQNNPACK test_multiple_q_config_options override_quantized_engine qnnpack torch backends quantized engine = qnnpack qconfig support per_channel quantization per_channel_qconfig = QConfig activation=HistogramObserver with_args reduce_range=True weight=default_per_channel_weight_observer we need design model ConvLinearModel torch nn Module __init__ - None super __init__ conv = torch nn Conv d fc = torch nn Linear relu = torch nn ReLU fc = torch nn Linear conv = torch nn Conv d forward x x = conv x x = fc x x = relu x x = fc x x = conv x x q_config_mapping = QConfigMapping q_config_mapping set_global torch ao quantization get_default_qconfig torch backends quantized engine set_object_type torch nn Conv d per_channel_qconfig prepared_model = _prepare_model_and_run_input ConvLinearModel q_config_mapping torch randn run detector per_channel_detector = PerChannelDetector torch backends quantized engine optims_str per_channel_info = per_channel_detector generate_detector_report prepared_model only suggestions should linear layers there should optims possible assertNotEqual optims_str DEFAULT_NO_OPTIMS_ANSWER_STRING format torch backends quantized engine ensure got into nested layer assertEqual len per_channel_info each layer should supported used key per_channel_info keys module_entry = per_channel_info key assertEqual module_entry per_channel_quantization_supported True linear False conv d true cuz uses different config fc key assertEqual module_entry per_channel_quantization_used False conv key assertEqual module_entry per_channel_quantization_used True raise ValueError Should only contain conv linear layers key values Case includes Multiple conv linear post training quantization composed sequential qconfig doesn t use per_channel weight observer Only qconfig qconfig dict Output has possible changes suggestions skipIfNoQNNPACK test_sequential_model_format override_quantized_engine qnnpack torch backends quantized engine = qnnpack q_config_mapping = QConfigMapping q_config_mapping set_global torch ao quantization get_default_qconfig torch backends quantized engine prepared_model = _prepare_model_and_run_input NESTED_CONV_LINEAR_EXAMPLE q_config_mapping torch randn run detector per_channel_detector = PerChannelDetector torch backends quantized engine optims_str per_channel_info = per_channel_detector generate_detector_report prepared_model there should optims possible assertNotEqual optims_str DEFAULT_NO_OPTIMS_ANSWER_STRING format torch backends quantized engine ensure got into nested layer assertEqual len per_channel_info each layer should supported used key per_channel_info keys module_entry = per_channel_info key assertEqual module_entry per_channel_quantization_supported True assertEqual module_entry per_channel_quantization_used False Case includes Multiple conv linear post training quantization composed sequential qconfig doesn t use per_channel weight observer Only qconfig qconfig dict Output has possible changes suggestions skipIfNoQNNPACK test_conv_sub_class_considered override_quantized_engine qnnpack torch backends quantized engine = qnnpack q_config_mapping = QConfigMapping q_config_mapping set_global torch ao quantization get_default_qconfig torch backends quantized engine prepared_model = _prepare_model_and_run_input LAZY_CONV_LINEAR_EXAMPLE q_config_mapping torch randn run detector per_channel_detector = PerChannelDetector torch backends quantized engine optims_str per_channel_info = per_channel_detector generate_detector_report prepared_model there should optims possible assertNotEqual optims_str DEFAULT_NO_OPTIMS_ANSWER_STRING format torch backends quantized engine ensure got into nested layer considered lazyConv d assertEqual len per_channel_info each layer should supported used key per_channel_info keys module_entry = per_channel_info key assertEqual module_entry per_channel_quantization_supported True assertEqual module_entry per_channel_quantization_used False Case includes Multiple conv linear post training quantization composed sequential qconfig uses per_channel weight observer Only qconfig qconfig dict Output has no possible changes suggestions skipIfNoFBGEMM test_fusion_layer_in_sequential override_quantized_engine fbgemm torch backends quantized engine = fbgemm q_config_mapping = QConfigMapping q_config_mapping set_global torch ao quantization get_default_qconfig torch backends quantized engine prepared_model = _prepare_model_and_run_input FUSION_CONV_LINEAR_EXAMPLE q_config_mapping torch randn run detector per_channel_detector = PerChannelDetector torch backends quantized engine optims_str per_channel_info = per_channel_detector generate_detector_report prepared_model no optims possible there should nothing per_channel_status assertEqual optims_str DEFAULT_NO_OPTIMS_ANSWER_STRING format torch backends quantized engine ensure got into nested layer considered all nested fusion components assertEqual len per_channel_info each layer should supported used key per_channel_info keys module_entry = per_channel_info key assertEqual module_entry per_channel_quantization_supported True assertEqual module_entry per_channel_quantization_used True Case includes Multiple conv linear quantitative aware training composed model qconfig does use per_channel weight observer Only qconfig qconfig dict Output has possible changes suggestions skipIfNoQNNPACK test_qat_aware_model_example first we want QAT model QATConvLinearReluModel torch nn Module __init__ - None super __init__ QuantStub converts tensors floating point quantized quant = torch ao quantization QuantStub conv = torch nn Conv d bn = torch nn BatchNorm d relu = torch nn ReLU DeQuantStub converts tensors quantized floating point dequant = torch ao quantization DeQuantStub forward x x = quant x x = conv x x = bn x x = relu x x = dequant x x override_quantized_engine qnnpack create model instance model_fp = QATConvLinearReluModel model_fp qconfig = torch ao quantization get_default_qat_qconfig qnnpack model must eval mode fusion model_fp eval model_fp _fused = torch ao quantization fuse_modules model_fp conv bn relu model must set train mode QAT logic work model_fp _fused train prepare model QAT different than post training quantization model_fp _prepared = torch ao quantization prepare_qat model_fp _fused run detector per_channel_detector = PerChannelDetector torch backends quantized engine optims_str per_channel_info = per_channel_detector generate_detector_report model_fp _prepared there should optims possible assertNotEqual optims_str DEFAULT_NO_OPTIMS_ANSWER_STRING format torch backends quantized engine make sure able find single conv fused model assertEqual len per_channel_info one conv should still give advice use different qconfig key per_channel_info keys module_entry = per_channel_info key assertEqual module_entry per_channel_quantization_supported True assertEqual module_entry per_channel_quantization_used False Partition Domain Things Test - All zero tensor - Multiple tensor dimensions - All outward facing functions - Epoch min max correctly updating - Batch range correctly averaging expected - Reset each epoch correctly resetting values Partition Output - calcuation ratio occurring correctly TestFxModelReportObserver QuantizationTestCase NestedModifiedSingleLayerLinear torch nn Module __init__ - None super __init__ obs = ModelReportObserver mod = SingleLayerLinearModel obs = ModelReportObserver fc = torch nn Linear dtype=torch float relu = torch nn ReLU forward x x = obs x x = mod x x = obs x x = fc x x = relu x x run_model_and_common_checks model ex_input num_epochs batch_size split up data into batches split_up_data = torch split ex_input batch_size _epoch range num_epochs reset all model report obs model apply lambda module module reset_batch_and_epoch_values isinstance module ModelReportObserver None quick check reset occurred assertEqual model obs average_batch_activation_range torch tensor float assertEqual model obs epoch_activation_min torch tensor float inf assertEqual model obs epoch_activation_max torch tensor float -inf loop through batches run through index batch enumerate split_up_data num_tracked_so_far = model obs num_batches_tracked assertEqual num_tracked_so_far index get general info about batch model use later batch_min batch_max = torch aminmax batch current_average_range = model obs average_batch_activation_range current_epoch_min = model obs epoch_activation_min current_epoch_max = model obs epoch_activation_max run input through model ex_input check average batch activation range updated correctly correct_updated_value = current_average_range num_tracked_so_far + batch_max - batch_min num_tracked_so_far + assertEqual model obs average_batch_activation_range correct_updated_value current_epoch_max - current_epoch_min assertEqual model obs get_batch_to_epoch_ratio correct_updated_value current_epoch_max - current_epoch_min Case includes all zero tensor dim size = run epoch run batch tests input data observer test_zero_tensor_errors initialize model model = NestedModifiedSingleLayerLinear generate desired input ex_input = torch zeros run through model do general tests run_model_and_common_checks model ex_input make sure final values all assertEqual model obs epoch_activation_min assertEqual model obs epoch_activation_max assertEqual model obs average_batch_activation_range we should get error we try calculate ratio assertRaises ValueError ratio_val = model obs get_batch_to_epoch_ratio Case includes non-zero tensor dim size = run epoch run batch tests input data observer test_single_batch_of_ones initialize model model = NestedModifiedSingleLayerLinear generate desired input ex_input = torch ones run through model do general tests run_model_and_common_checks model ex_input make sure final values all except range assertEqual model obs epoch_activation_min assertEqual model obs epoch_activation_max assertEqual model obs average_batch_activation_range we should get error we try calculate ratio assertRaises ValueError ratio_val = model obs get_batch_to_epoch_ratio Case includes non-zero tensor dim size = run epoch run batch tests non input data observer test_observer_after_relu model specific test NestedModifiedObserverAfterRelu torch nn Module __init__ - None super __init__ obs = ModelReportObserver mod = SingleLayerLinearModel obs = ModelReportObserver fc = torch nn Linear dtype=torch float relu = torch nn ReLU forward x x = obs x x = mod x x = fc x x = relu x x = obs x x initialize model model = NestedModifiedObserverAfterRelu generate desired input ex_input = torch randn run through model do general tests run_model_and_common_checks model ex_input Case includes non-zero tensor dim size = run multiple epoch run multiple batch tests input data observer test_random_epochs_and_batches set up basic model TinyNestModule torch nn Module __init__ - None super __init__ obs = ModelReportObserver fc = torch nn Linear dtype=torch float relu = torch nn ReLU obs = ModelReportObserver forward x x = obs x x = fc x x = relu x x = obs x x LargerIncludeNestModel torch nn Module __init__ - None super __init__ obs = ModelReportObserver nested = TinyNestModule fc = SingleLayerLinearModel relu = torch nn ReLU forward x x = obs x x = nested x x = fc x x = relu x x ModifiedThreeOps torch nn Module __init__ batch_norm_dim super __init__ obs = ModelReportObserver linear = torch nn Linear obs = ModelReportObserver batch_norm_dim == bn = torch nn BatchNorm d batch_norm_dim == bn = torch nn BatchNorm d raise ValueError Dim should only relu = torch nn ReLU forward x x = obs x x = linear x x = obs x x = bn x x = relu x x HighDimensionNet torch nn Module __init__ - None super __init__ obs = ModelReportObserver fc = torch nn Linear block = ModifiedThreeOps fc = torch nn Linear block = ModifiedThreeOps fc = torch nn Linear forward x x = obs x x = fc x x = block x x = fc x y = block x y = fc y z = x + y z = F relu z z purpose test give observers variety data examples initialize model models = NestedModifiedSingleLayerLinear LargerIncludeNestModel ModifiedThreeOps HighDimensionNet get some number epochs batches num_epochs = num_batches = input_shapes = generate desired inputs inputs = shape input_shapes ex_input = torch randn num_batches shape inputs append ex_input run through model do general tests index model enumerate models run_model_and_common_checks model inputs index num_epochs num_batches Partition domain things test There only single test case now This will more thoroughly tested implementation full end end tool coming soon TestFxModelReportDetectDynamicStatic QuantizationTestCase skipIfNoFBGEMM test_nested_detection_case SingleLinear torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x x = linear x x TwoBlockNet torch nn Module __init__ - None super __init__ block = SingleLinear block = SingleLinear forward x x = block x y = block x z = x + y z = F relu z z override_quantized_engine fbgemm create model example input qconfig mapping torch backends quantized engine = fbgemm model = TwoBlockNet example_input = torch randint - example_input = example_input torch float q_config_mapping = QConfigMapping q_config_mapping set_global torch ao quantization get_default_qconfig fbgemm prep model select observer model_prep = quantize_fx prepare_fx model q_config_mapping example_input obs_ctr = ModelReportObserver find layer attach store linear_fqn = block linear fqn target linear target_linear = None node model_prep graph nodes node target == linear_fqn target_linear = node break insert into both module graph pre post set up insert before target_linear pre_observer model_prep graph inserting_before target_linear obs_to_insert = obs_ctr pre_obs_fqn = linear_fqn + model_report_pre_observer model_prep add_submodule pre_obs_fqn obs_to_insert model_prep graph create_node op= call_module target=pre_obs_fqn args=target_linear args set up insert after target_linear post_observer model_prep graph inserting_after target_linear obs_to_insert = obs_ctr post_obs_fqn = linear_fqn + model_report_post_observer model_prep add_submodule post_obs_fqn obs_to_insert model_prep graph create_node op= call_module target=post_obs_fqn args= target_linear need recompile module after submodule added pass input through model_prep recompile num_iterations = i range num_iterations i == example_input = torch randint - torch float example_input = torch randint torch float model_prep example_input run through dynamic vs static detector dynamic_vs_static_detector = DynamicStaticDetector dynam_vs_stat_str dynam_vs_stat_dict = dynamic_vs_static_detector generate_detector_report model_prep one stats should stationary other non-stationary result dynamic should recommended data_dist_info = dynam_vs_stat_dict linear_fqn DynamicStaticDetector PRE_OBS_DATA_DIST_KEY dynam_vs_stat_dict linear_fqn DynamicStaticDetector POST_OBS_DATA_DIST_KEY assertTrue stationary data_dist_info assertTrue non-stationary data_dist_info assertTrue dynam_vs_stat_dict linear_fqn dynamic_recommended TestFxModelReportClass QuantizationTestCase skipIfNoFBGEMM test_constructor Tests constructor ModelReport Specifically looks - The desired reports - Ensures observers interest properly initialized override_quantized_engine fbgemm set backend test torch backends quantized engine = fbgemm backend = torch backends quantized engine create model model = ThreeOps q_config_mapping = QConfigMapping q_config_mapping set_global torch ao quantization get_default_qconfig torch backends quantized engine model_prep = quantize_fx prepare_fx model q_config_mapping model get_example_inputs make example set detectors test_detector_set = DynamicStaticDetector PerChannelDetector backend initialize empty detector model_report = ModelReport model_prep test_detector_set make sure internal valid reports matches detector_name_set = detector get_detector_name detector test_detector_set assertEqual model_report get_desired_reports_names detector_name_set now attempt no valid reports should raise error assertRaises ValueError model_report = ModelReport model set number expected obs interest entries num_expected_entries = len test_detector_set assertEqual len model_report get_observers_of_interest num_expected_entries value model_report get_observers_of_interest values assertEqual len value skipIfNoFBGEMM test_prepare_model_callibration Tests model_report prepare_detailed_calibration prepares model callibration Specifically looks - Whether observers properly inserted into regular nn Module - Whether target arguments observers proper - Whether internal representation observers interest updated override_quantized_engine fbgemm create model report object create model model = TwoThreeOps make example set detectors torch backends quantized engine = fbgemm backend = torch backends quantized engine test_detector_set = DynamicStaticDetector PerChannelDetector backend initialize empty detector prepare model example_input = model get_example_inputs current_backend = torch backends quantized engine q_config_mapping = QConfigMapping q_config_mapping set_global torch ao quantization get_default_qconfig torch backends quantized engine model_prep = quantize_fx prepare_fx model q_config_mapping example_input model_report = ModelReport model_prep test_detector_set prepare model calibration prepared_for_callibrate_model = model_report prepare_detailed_calibration see whether observers properly regular nn Module there should observers present case modules_observer_cnt = module prepared_for_callibrate_model modules isinstance module ModelReportObserver modules_observer_cnt += assertEqual modules_observer_cnt model_report_str_check = model_report also make sure arguments observers graph proper node prepared_for_callibrate_model graph nodes all node targets strings so check isinstance node target str model_report_str_check node target pre-observer has same args linear next node pre_observer node target assertEqual node args node next args post-observer args target linear previous node post_observer node target assertEqual node args node prev ensure model_report observers interest updated there should two entries assertEqual len model_report get_observers_of_interest detector test_detector_set assertTrue detector get_detector_name model_report get_observers_of_interest keys get number entries detector detector_obs_of_interest_fqns = model_report get_observers_of_interest detector get_detector_name assert per channel detector has dynamic static has isinstance detector PerChannelDetector assertEqual len detector_obs_of_interest_fqns isinstance detector DynamicStaticDetector assertEqual len detector_obs_of_interest_fqns ensure we can prepare calibration only once assertRaises ValueError prepared_for_callibrate_model = model_report prepare_detailed_calibration get_module_and_graph_cnts callibrated_fx_module r Calculates number ModelReportObserver modules model well graph structure Returns tuple two elements int The number ModelReportObservers found model int The number model_report nodes found graph get number observers stored modules modules_observer_cnt = module callibrated_fx_module modules isinstance module ModelReportObserver modules_observer_cnt += get number observers graph model_report_str_check = model_report graph_observer_cnt = also make sure arguments observers graph proper node callibrated_fx_module graph nodes all node targets strings so check isinstance node target str model_report_str_check node target increment we found graph observer graph_observer_cnt += modules_observer_cnt graph_observer_cnt skipIfNoFBGEMM test_generate_report Tests model_report generate_model_report ensure report generation Specifically looks - Whether correct number reports being generated - Whether observers being properly removed specified - Whether correct blocking generating report twice obs removed override_quantized_engine fbgemm set backend test torch backends quantized engine = fbgemm check whether correct number reports being generated filled_detector_set = DynamicStaticDetector PerChannelDetector torch backends quantized engine single_detector_set = DynamicStaticDetector create our models model_full = TwoThreeOps model_single = TwoThreeOps prepare calibrate two different instances same model prepare model example_input = model_full get_example_inputs current_backend = torch backends quantized engine q_config_mapping = QConfigMapping q_config_mapping set_global torch ao quantization get_default_qconfig torch backends quantized engine model_prep_full = quantize_fx prepare_fx model_full q_config_mapping example_input model_prep_single = quantize_fx prepare_fx model_single q_config_mapping example_input initialize one filled detector model_report_full = ModelReport model_prep_full filled_detector_set initialize another single detector set model_report_single = ModelReport model_prep_single single_detector_set prepare models calibration prepared_for_callibrate_model_full = model_report_full prepare_detailed_calibration prepared_for_callibrate_model_single = model_report_single prepare_detailed_calibration now calibrate two models num_iterations = _ range num_iterations example_input = torch tensor torch randint dtype=torch float prepared_for_callibrate_model_full example_input prepared_for_callibrate_model_single example_input now generate reports model_full_report = model_report_full generate_model_report True model_single_report = model_report_single generate_model_report False check sizes appropriate assertEqual len model_full_report len filled_detector_set assertEqual len model_single_report len single_detector_set make sure observers being properly removed full report since we put flag modules_observer_cnt graph_observer_cnt = get_module_and_graph_cnts prepared_for_callibrate_model_full assertEqual modules_observer_cnt assert no more observer modules assertEqual graph_observer_cnt assert no more observer nodes graph make sure observers aren t being removed single report since specified modules_observer_cnt graph_observer_cnt = get_module_and_graph_cnts prepared_for_callibrate_model_single assertNotEqual modules_observer_cnt assertNotEqual graph_observer_cnt make sure error when try rerun report generation full report single report assertRaises Exception model_full_report = model_report_full generate_model_report prepared_for_callibrate_model_full False make sure we don t run into error single report model_single_report = model_report_single generate_model_report False skipIfNoFBGEMM test_generate_visualizer Tests ModelReport can properly create ModelReportVisualizer instance Checks - Correct number modules represented - Modules sorted - Correct number features each module override_quantized_engine fbgemm set backend test torch backends quantized engine = fbgemm test multiple detectors detector_set = set detector_set add OutlierDetector reference_percentile= detector_set add InputWeightEqualizationDetector model = TwoThreeOps get tst model calibrate prepared_for_callibrate_model mod_report = _get_prepped_for_calibration_model_helper model detector_set model get_example_inputs now we actually calibrate model example_input = model get_example_inputs example_input = example_input torch float prepared_for_callibrate_model example_input try visualize without generating report should throw error assertRaises Exception mod_rep_visualizaiton = mod_report generate_visualizer now get report running through ModelReport instance generated_report = mod_report generate_model_report remove_inserted_observers=False now we get visualizer should error mod_rep_visualizer ModelReportVisualizer = mod_report generate_visualizer since we tested outlier detector which looks every base level module should six entries ordered dict mod_fqns_to_features = mod_rep_visualizer generated_reports assertEqual len mod_fqns_to_features outlier detector has feature per module input-weight has features per module there common data point so should + - = unique features per common modules all linears will common module_fqn mod_fqns_to_features linear module_fqn linear_info = mod_fqns_to_features module_fqn assertEqual len linear_info skipIfNoFBGEMM test_qconfig_mapping_generation Tests generation qconfigs ModelReport API - Tests qconfigmapping generated - Tests mappings include information relavent modules override_quantized_engine fbgemm set backend test torch backends quantized engine = fbgemm test multiple detectors detector_set = set detector_set add PerChannelDetector detector_set add DynamicStaticDetector model = TwoThreeOps get tst model calibrate prepared_for_callibrate_model mod_report = _get_prepped_for_calibration_model_helper model detector_set model get_example_inputs now we actually calibrate models example_input = model get_example_inputs example_input = example_input torch float prepared_for_callibrate_model example_input get mapping without error qconfig_mapping = mod_report generate_qconfig_mapping now get report running through ModelReport instance generated_report = mod_report generate_model_report remove_inserted_observers=False get visualizer so we can get access reformatted reports module fqn mod_reports_by_fqn = mod_report generate_visualizer generated_reports compare entries mapping those report we should have same number entries assertEqual len qconfig_mapping module_name_qconfigs len mod_reports_by_fqn non_empty one we should have because we have only applicable linears so should have suggestions each module named assertEqual len qconfig_mapping module_name_qconfigs only two linears make sure per channel min max weight since fbgemm also static distribution since simple single calibration key qconfig_mapping module_name_qconfigs config = qconfig_mapping module_name_qconfigs key assertEqual config weight default_per_channel_weight_observer assertEqual config activation default_observer make sure these can actually used prepare model prepared = quantize_fx prepare_fx TwoThreeOps qconfig_mapping example_input now convert model ensure no errors conversion converted = quantize_fx convert_fx prepared skipIfNoFBGEMM test_equalization_mapping_generation Tests generation qconfigs ModelReport API - Tests equalization config generated when input-weight equalization detector used - Tests mappings include information relavent modules override_quantized_engine fbgemm set backend test torch backends quantized engine = fbgemm test multiple detectors detector_set = set detector_set add InputWeightEqualizationDetector model = TwoThreeOps get tst model calibrate prepared_for_callibrate_model mod_report = _get_prepped_for_calibration_model_helper model detector_set model get_example_inputs now we actually calibrate models example_input = model get_example_inputs example_input = example_input torch float prepared_for_callibrate_model example_input get mapping without error qconfig_mapping = mod_report generate_qconfig_mapping equalization_mapping = mod_report generate_equalization_mapping tests lot more simple equalization mapping shouldn t have any equalization suggestions case assertEqual len qconfig_mapping module_name_qconfigs make sure these can actually used prepare model prepared = quantize_fx prepare_fx TwoThreeOps qconfig_mapping example_input _equalization_config=equalization_mapping now convert model ensure no errors conversion converted = quantize_fx convert_fx prepared TestFxDetectInputWeightEqualization QuantizationTestCase SimpleConv torch nn Module __init__ con_dims super __init__ relu = torch nn ReLU conv = torch nn Conv d con_dims con_dims kernel_size= stride= padding= bias=False forward x x = conv x x = relu x x TwoBlockComplexNet torch nn Module __init__ - None super __init__ block = TestFxDetectInputWeightEqualization SimpleConv block = TestFxDetectInputWeightEqualization SimpleConv conv = torch nn Conv d kernel_size= stride= padding= bias=False linear = torch nn Linear relu = torch nn ReLU forward x x = block x x = conv x y = block x y = y repeat z = x + y z = z flatten start_dim= z = linear z z = relu z z get_fusion_modules conv relu get_example_inputs torch randn ReluOnly torch nn Module __init__ - None super __init__ relu = torch nn ReLU forward x x = relu x x get_example_inputs torch arange reshape _get_prepped_for_calibration_model model detector_set fused=False r Returns model has been prepared callibration corresponding model_report pass necessary inputs helper example_input = model get_example_inputs _get_prepped_for_calibration_model_helper model detector_set example_input fused skipIfNoFBGEMM test_input_weight_equalization_determine_points use fbgemm create our model instance then create model report instance detector override_quantized_engine fbgemm detector_set = InputWeightEqualizationDetector get tst model calibrate non_fused = _get_prepped_for_calibration_model TwoBlockComplexNet detector_set fused = _get_prepped_for_calibration_model TwoBlockComplexNet detector_set fused=True reporter should still give same counts even fused model prepared_for_callibrate_model _mod_report non_fused fused supported modules check mods_to_check = nn Linear nn Conv d get set all nodes graph their fqns node_fqns = node target node prepared_for_callibrate_model graph nodes there should node fqns have observer inserted correct_number_of_obs_inserted = number_of_obs_found = obs_name_to_find = InputWeightEqualizationDetector DEFAULT_PRE_OBSERVER_NAME node prepared_for_callibrate_model graph nodes obs name inside target we found observer obs_name_to_find str node target number_of_obs_found += assertEqual number_of_obs_found correct_number_of_obs_inserted assert each desired modules have observers inserted module prepared_for_callibrate_model modules check module supported module is_in_include_list = sum isinstance module x x mods_to_check is_in_include_list make sure has observer attribute assertTrue hasattr module InputWeightEqualizationDetector DEFAULT_PRE_OBSERVER_NAME s supported type shouldn t have observer attached assertTrue hasattr module InputWeightEqualizationDetector DEFAULT_PRE_OBSERVER_NAME skipIfNoFBGEMM test_input_weight_equalization_report_gen use fbgemm create our model instance then create model report instance detector override_quantized_engine fbgemm test_input_weight_detector = InputWeightEqualizationDetector detector_set = test_input_weight_detector model = TwoBlockComplexNet prepare model calibration prepared_for_callibrate_model model_report = _get_prepped_for_calibration_model model detector_set now we actually calibrate model example_input = model get_example_inputs example_input = example_input torch float prepared_for_callibrate_model example_input now get report running through ModelReport instance generated_report = model_report generate_model_report True check sizes appropriate only detector assertEqual len generated_report get specific report input weight equalization input_weight_str input_weight_dict = generated_report test_input_weight_detector get_detector_name we should have layers looked since conv linear layers assertEqual len input_weight_dict we can validate max min values detector recorded properly first one because no data has been processed yet so should values original input example_input = example_input reshape reshape input module_fqn input_weight_dict look first linear block linear module_fqn block_ _lin_recs = input_weight_dict module_fqn get input range info channel axis ch_axis = block_ _lin_recs InputWeightEqualizationDetector CHANNEL_KEY ensure min max values extracted match properly example_min example_max = torch aminmax example_input dim=ch_axis dimension_min = torch amin example_min dim=ch_axis dimension_max = torch amax example_max dim=ch_axis make sure per channel min max expected min_per_key = InputWeightEqualizationDetector ACTIVATION_PREFIX min_per_key += InputWeightEqualizationDetector PER_CHANNEL_MIN_KEY max_per_key = InputWeightEqualizationDetector ACTIVATION_PREFIX max_per_key += InputWeightEqualizationDetector PER_CHANNEL_MAX_KEY per_channel_min = block_ _lin_recs min_per_key per_channel_max = block_ _lin_recs max_per_key assertEqual per_channel_min dimension_min assertEqual per_channel_max dimension_max make sure per channel min max expected min_key = InputWeightEqualizationDetector ACTIVATION_PREFIX min_key += InputWeightEqualizationDetector GLOBAL_MIN_KEY max_key = InputWeightEqualizationDetector ACTIVATION_PREFIX max_key += InputWeightEqualizationDetector GLOBAL_MAX_KEY make sure global min max correctly recorded presented global_min = block_ _lin_recs min_key global_max = block_ _lin_recs max_key assertEqual global_min min dimension_min assertEqual global_max max dimension_max input_ratio = torch sqrt per_channel_max - per_channel_min global_max - global_min ensure comparison stat passed back sqrt range ratios need get weight ratios first make sure per channel min max expected min_per_key = InputWeightEqualizationDetector WEIGHT_PREFIX min_per_key += InputWeightEqualizationDetector PER_CHANNEL_MIN_KEY max_per_key = InputWeightEqualizationDetector WEIGHT_PREFIX max_per_key += InputWeightEqualizationDetector PER_CHANNEL_MAX_KEY get weight per channel global info per_channel_min = block_ _lin_recs min_per_key per_channel_max = block_ _lin_recs max_per_key make sure per channel min max expected min_key = InputWeightEqualizationDetector WEIGHT_PREFIX min_key += InputWeightEqualizationDetector GLOBAL_MIN_KEY max_key = InputWeightEqualizationDetector WEIGHT_PREFIX max_key += InputWeightEqualizationDetector GLOBAL_MAX_KEY global_min = block_ _lin_recs min_key global_max = block_ _lin_recs max_key weight_ratio = torch sqrt per_channel_max - per_channel_min global_max - global_min also get comp stat specific layer comp_stat = block_ _lin_recs InputWeightEqualizationDetector COMP_METRIC_KEY weight_to_input_ratio = weight_ratio input_ratio assertEqual comp_stat weight_to_input_ratio only looking first example so can break break skipIfNoFBGEMM test_input_weight_equalization_report_gen_empty tests report gen model doesn t have any layers use fbgemm create our model instance then create model report instance detector override_quantized_engine fbgemm test_input_weight_detector = InputWeightEqualizationDetector detector_set = test_input_weight_detector model = ReluOnly prepare model calibration prepared_for_callibrate_model model_report = _get_prepped_for_calibration_model model detector_set now we actually calibrate model example_input = model get_example_inputs example_input = example_input torch float prepared_for_callibrate_model example_input now get report running through ModelReport instance generated_report = model_report generate_model_report True check sizes appropriate only detector assertEqual len generated_report get specific report input weight equalization input_weight_str input_weight_dict = generated_report test_input_weight_detector get_detector_name we should have layers since there only Relu assertEqual len input_weight_dict make sure string only has two lines should no suggestions assertEqual input_weight_str count \n TestFxDetectOutliers QuantizationTestCase LargeBatchModel torch nn Module __init__ param_size super __init__ param_size = param_size linear = torch nn Linear param_size param_size relu_ = torch nn ReLU conv = torch nn Conv d param_size param_size relu_ = torch nn ReLU forward x x = linear x x = relu_ x x = conv x x = relu_ x x get_example_inputs param_size = param_size torch randn param_size param_size param_size get_outlier_inputs param_size = param_size random_vals = torch randn param_size param_size param_size change one some them massive value random_vals param_size = torch tensor e random_vals _get_prepped_for_calibration_model model detector_set use_outlier_data=False r Returns model has been prepared callibration corresponding model_report call general helper function calibrate example_input = model get_example_inputs we specifically want test data outliers replace input use_outlier_data example_input = model get_outlier_inputs _get_prepped_for_calibration_model_helper model detector_set example_input skipIfNoFBGEMM test_outlier_detection_determine_points use fbgemm create our model instance then create model report instance detector similar test InputWeightEqualization key differences made refactoring viable explicitly testing fusion because fx workflow automatically override_quantized_engine fbgemm detector_set = OutlierDetector reference_percentile= get tst model calibrate prepared_for_callibrate_model mod_report = _get_prepped_for_calibration_model LargeBatchModel param_size= detector_set supported modules check mods_to_check = nn Linear nn Conv d nn ReLU there should node fqns have observer inserted correct_number_of_obs_inserted = number_of_obs_found = obs_name_to_find = InputWeightEqualizationDetector DEFAULT_PRE_OBSERVER_NAME number_of_obs_found = sum obs_name_to_find str node target node prepared_for_callibrate_model graph nodes assertEqual number_of_obs_found correct_number_of_obs_inserted assert each desired modules have observers inserted module prepared_for_callibrate_model modules check module supported module is_in_include_list = isinstance module tuple mods_to_check is_in_include_list make sure has observer attribute assertTrue hasattr module InputWeightEqualizationDetector DEFAULT_PRE_OBSERVER_NAME s supported type shouldn t have observer attached assertTrue hasattr module InputWeightEqualizationDetector DEFAULT_PRE_OBSERVER_NAME skipIfNoFBGEMM test_no_outlier_report_gen use fbgemm create our model instance then create model report instance detector override_quantized_engine fbgemm test multiple detectors outlier_detector = OutlierDetector reference_percentile= dynamic_static_detector = DynamicStaticDetector tolerance= param_size int = detector_set = outlier_detector dynamic_static_detector model = LargeBatchModel param_size=param_size get tst model calibrate prepared_for_callibrate_model mod_report = _get_prepped_for_calibration_model model detector_set now we actually calibrate model example_input = model get_example_inputs example_input = example_input torch float prepared_for_callibrate_model example_input now get report running through ModelReport instance generated_report = mod_report generate_model_report True check sizes appropriate only detectors assertEqual len generated_report get specific report input weight equalization outlier_str outlier_dict = generated_report outlier_detector get_detector_name we should have layers looked since conv + linear + relu assertEqual len outlier_dict assert following true all modules module_fqn outlier_dict get info specific module module_dict = outlier_dict module_fqn there really should any outliers since we used normal distribution perform calculation outlier_info = module_dict OutlierDetector OUTLIER_KEY assertEqual sum outlier_info ensure number ratios batches counted same number params assertEqual len module_dict OutlierDetector COMP_METRIC_KEY param_size assertEqual len module_dict OutlierDetector NUM_BATCHES_KEY param_size skipIfNoFBGEMM test_all_outlier_report_gen make percentile ratio then see everything outlier according use fbgemm create our model instance then create model report instance detector override_quantized_engine fbgemm create detector interest outlier_detector = OutlierDetector ratio_threshold= reference_percentile= param_size int = detector_set = outlier_detector model = LargeBatchModel param_size=param_size get tst model calibrate prepared_for_callibrate_model mod_report = _get_prepped_for_calibration_model model detector_set now we actually calibrate model example_input = model get_example_inputs example_input = example_input torch float prepared_for_callibrate_model example_input now get report running through ModelReport instance generated_report = mod_report generate_model_report True check sizes appropriate only detector assertEqual len generated_report get specific report input weight equalization outlier_str outlier_dict = generated_report outlier_detector get_detector_name we should have layers looked since conv + linear + relu assertEqual len outlier_dict assert following true all modules module_fqn outlier_dict get info specific module module_dict = outlier_dict module_fqn everything should outlier because we said max should equal min all them however we will just test say most should case we have several channel values outlier_info = module_dict OutlierDetector OUTLIER_KEY assert sum outlier_info = len outlier_info ensure number ratios batches counted same number params assertEqual len module_dict OutlierDetector COMP_METRIC_KEY param_size assertEqual len module_dict OutlierDetector NUM_BATCHES_KEY param_size skipIfNoFBGEMM test_multiple_run_consistent_spike_outlier_report_gen specifically make row really high consistently number batches you testing try generate report after just run after many runs make sure above minimum threshold there override_quantized_engine fbgemm detector interest outlier_detector = OutlierDetector reference_percentile= param_size int = detector_set = outlier_detector model = LargeBatchModel param_size=param_size get tst model calibrate prepared_for_callibrate_model mod_report = _get_prepped_for_calibration_model model detector_set use_outlier_data=True now we actually calibrate model example_input = model get_outlier_inputs example_input = example_input torch float now calibrate minimum times make above minimum threshold i range example_input = model get_outlier_inputs example_input = example_input torch float make batches have zero channel i == make one channel constant example_input = torch zeros_like example_input prepared_for_callibrate_model example_input now get report running through ModelReport instance generated_report = mod_report generate_model_report True check sizes appropriate only detector assertEqual len generated_report get specific report input weight equalization outlier_str outlier_dict = generated_report outlier_detector get_detector_name we should have layers looked since conv + linear + relu assertEqual len outlier_dict assert following true all modules module_fqn outlier_dict get info specific module module_dict = outlier_dict module_fqn because we ran times we should have least couple significant could less because some channels could possibly all sufficient_batches_info = module_dict OutlierDetector IS_SUFFICIENT_BATCHES_KEY assert sum sufficient_batches_info = len sufficient_batches_info half them should outliers because we set really high value every channels outlier_info = module_dict OutlierDetector OUTLIER_KEY assertEqual sum outlier_info len outlier_info ensure number ratios batches counted same number params assertEqual len module_dict OutlierDetector COMP_METRIC_KEY param_size assertEqual len module_dict OutlierDetector NUM_BATCHES_KEY param_size first one ensure per channel max values what we set module_fqn == linear check non-zero channel count least should there first module counts_info = module_dict OutlierDetector CONSTANT_COUNTS_KEY assert sum counts_info = half recorded max values should what we set matched_max = sum val == e val module_dict OutlierDetector MAX_VALS_KEY assertEqual matched_max param_size TestFxModelReportVisualizer QuantizationTestCase _callibrate_and_generate_visualizer model prepared_for_callibrate_model mod_report r Callibrates passed model generates report returns visualizer now we actually calibrate model example_input = model get_example_inputs example_input = example_input torch float prepared_for_callibrate_model example_input now get report running through ModelReport instance generated_report = mod_report generate_model_report remove_inserted_observers=False now we get visualizer should error mod_rep_visualizer ModelReportVisualizer = mod_report generate_visualizer mod_rep_visualizer skipIfNoFBGEMM test_get_modules_and_features Tests get_all_unique_module_fqns get_all_unique_feature_names methods ModelReportVisualizer Checks whether returned sets proper size filtered properly override_quantized_engine fbgemm set backend test torch backends quantized engine = fbgemm test multiple detectors detector_set = set detector_set add OutlierDetector reference_percentile= detector_set add InputWeightEqualizationDetector model = TwoThreeOps get tst model calibrate prepared_for_callibrate_model mod_report = _get_prepped_for_calibration_model_helper model detector_set model get_example_inputs mod_rep_visualizer ModelReportVisualizer = _callibrate_and_generate_visualizer model prepared_for_callibrate_model mod_report ensure module fqns match ones given get_all_unique_feature_names method actual_model_fqns = set mod_rep_visualizer generated_reports keys returned_model_fqns = mod_rep_visualizer get_all_unique_module_fqns assertEqual returned_model_fqns actual_model_fqns now ensure features all properly returned all linears have all features two detectors can use those check method working reliably b_ _linear_features = mod_rep_visualizer generated_reports block linear first test all features returned_all_feats = mod_rep_visualizer get_all_unique_feature_names False assertEqual returned_all_feats set b_ _linear_features keys now test plottable features plottable_set = set feature_name b_ _linear_features type b_ _linear_features feature_name torch Tensor plottable_set add feature_name returned_plottable_feats = mod_rep_visualizer get_all_unique_feature_names assertEqual returned_plottable_feats plottable_set _prep_visualizer_helper r Returns mod rep visualizer we test various ways set backend test torch backends quantized engine = fbgemm test multiple detectors detector_set = set detector_set add OutlierDetector reference_percentile= detector_set add InputWeightEqualizationDetector model = TwoThreeOps get tst model calibrate prepared_for_callibrate_model mod_report = _get_prepped_for_calibration_model_helper model detector_set model get_example_inputs mod_rep_visualizer ModelReportVisualizer = _callibrate_and_generate_visualizer model prepared_for_callibrate_model mod_report mod_rep_visualizer skipIfNoFBGEMM test_generate_tables_match_with_report Tests generate_table_view ModelReportVisualizer Checks whether generated dict has proper information Visual check tables look correct performed during testing override_quantized_engine fbgemm get visualizer mod_rep_visualizer = _prep_visualizer_helper table_dict = mod_rep_visualizer generate_filtered_tables test primarily dict since has same info str tensor_headers tensor_table = table_dict ModelReportVisualizer TABLE_TENSOR_KEY channel_headers channel_table = table_dict ModelReportVisualizer TABLE_CHANNEL_KEY these two together should same generated report info terms keys tensor_info_modules = row row tensor_table channel_info_modules = row row channel_table combined_modules set = tensor_info_modules union channel_info_modules generated_report_keys set = set mod_rep_visualizer generated_reports keys assertEqual combined_modules generated_report_keys skipIfNoFBGEMM test_generate_tables_no_match Tests generate_table_view ModelReportVisualizer Checks whether generated dict has proper information Visual check tables look correct performed during testing override_quantized_engine fbgemm get visualizer mod_rep_visualizer = _prep_visualizer_helper try random filter make sure there no rows either table empty_tables_dict = mod_rep_visualizer generate_filtered_tables module_fqn_filter= random there module test primarily dict since has same info str tensor_headers tensor_table = empty_tables_dict ModelReportVisualizer TABLE_TENSOR_KEY channel_headers channel_table = empty_tables_dict ModelReportVisualizer TABLE_CHANNEL_KEY tensor_info_modules = row row tensor_table channel_info_modules = row row channel_table combined_modules set = tensor_info_modules union channel_info_modules assertEqual len combined_modules should no matching modules skipIfNoFBGEMM test_generate_tables_single_feat_match Tests generate_table_view ModelReportVisualizer Checks whether generated dict has proper information Visual check tables look correct performed during testing override_quantized_engine fbgemm get visualizer mod_rep_visualizer = _prep_visualizer_helper try matching filter feature make sure only those features show up we filter very specific feature name should only have additional column each table row single_feat_dict = mod_rep_visualizer generate_filtered_tables feature_filter=OutlierDetector MAX_VALS_KEY test primarily dict since has same info str tensor_headers tensor_table = single_feat_dict ModelReportVisualizer TABLE_TENSOR_KEY channel_headers channel_table = single_feat_dict ModelReportVisualizer TABLE_CHANNEL_KEY get number features each these tensor_info_features = len tensor_headers channel_info_features = len channel_headers - ModelReportVisualizer NUM_NON_FEATURE_CHANNEL_HEADERS make sure there no tensor features there one channel level feature assertEqual tensor_info_features assertEqual channel_info_features _get_prepped_for_calibration_model_helper model detector_set example_input fused bool = False r Returns model has been prepared callibration corresponding model_report set backend test torch backends quantized engine = fbgemm create model instance prepare example_input = example_input torch float q_config_mapping = torch ao quantization get_default_qconfig_mapping they passed fusion parameter make sure test fused model = torch ao quantization fuse_modules model model get_fusion_modules model_prep = quantize_fx prepare_fx model q_config_mapping example_input model_report = ModelReport model_prep detector_set prepare model calibration prepared_for_callibrate_model = model_report prepare_detailed_calibration prepared_for_callibrate_model model_report __name__ == __main__ raise_on_run_directly test test_quantization py