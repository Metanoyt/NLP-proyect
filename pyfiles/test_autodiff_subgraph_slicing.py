Owner s oncall jit os sys unittest torch torch testing _internal common_jit check_against_reference torch testing _internal common_utils enable_profiling_mode_for_profiling_tests GRAPH_EXECUTOR num_profiled_runs ProfilingMode Make helper files test importable pytorch_test_dir = os path dirname os path dirname os path realpath __file__ sys path append pytorch_test_dir typing List Optional Tuple torch testing FileCheck torch testing _internal common_utils raise_on_run_directly torch testing _internal jit_utils disable_autodiff_subgraph_inlining JitTestCase assert GRAPH_EXECUTOR None unittest skipIf GRAPH_EXECUTOR == ProfilingMode SIMPLE Simple Executor doesn t support gradients TestAutodiffSubgraphSlicing JitTestCase TODO It better we can test directly graphs instead current end-to-end fashion _perform_ad_subgraph_slicing fn input_sizes disable_autodiff_subgraph_inlining enable_profiling_mode_for_profiling_tests ge = torch jit script fn inputs = torch randn size requires_grad=True size input_sizes ge inputs profile_and_replay=True ge graph_for inputs assertGraphSize graph size nodes = list filter lambda n n kind = prim BailOut n kind = prim BailoutTemplate n kind = prim TypeCheck n kind = prim RequiresGradCheck graph nodes assertEqual len list nodes size test_chunk_constant_script_ad torch jit script func x x x = torch chunk x x x input = torch rand requires_grad_ disable_autodiff_subgraph_inlining enable_profiling_mode_for_profiling_tests func input profile_and_replay=True FileCheck check_not prim DifferentiableGraph run func graph_for input unittest skipIf GRAPH_EXECUTOR = ProfilingMode PROFILING This threshold only valid Profiling Executor test_diff_graph_inline_threshold enable_profiling_mode_for_profiling_tests NUM_RUNS = num_profiled_runs NUM_RUNS torch jit script foo x two nodes should fused see https github com pytorch pytorch blob master torch csrc jit runtime graph_executor_impl h#L torch sigmoid torch sigmoid x torch jit script bar x two nodes should NOT fused torch sigmoid x input = torch rand requires_grad=True foo input foo input bar input bar input assertGraphContainsExactly foo graph_for input prim DifferentiableGraph assertGraphContainsExactly bar graph_for input prim DifferentiableGraph test_bias_as_module_attr enable_profiling_mode_for_profiling_tests M torch nn Module __init__ has_bias super __init__ ll = torch nn Linear has_bias forward x y ll x + y x + y x = torch rand requires_grad=True no_bias = M False scripted_no_bias = torch jit script no_bias scripted_no_bias x x scripted_no_bias x x scripted_no_bias x x has_bias = M True check_against_reference scripted_no_bias no_bias lambda x x x x check_types=False scripted_has_bias = torch jit script has_bias scripted_has_bias x x scripted_has_bias x x scripted_has_bias x x check_against_reference scripted_has_bias has_bias lambda x x x x check_types=False test_constructed_bias enable_profiling_mode_for_profiling_tests method x weight b b bias = b b torch nn functional linear x weight bias N = x = torch rand N N requires_grad=True weight = torch rand N N requires_grad=True b = torch rand N N requires_grad=True b = torch rand N N requires_grad=True scripted = checkScript method x weight b b check_types requires last_graph scripted set so we just skip check_against_reference scripted method lambda x x x weight b b check_types=False test_bias_as_arg enable_profiling_mode_for_profiling_tests method x weight bias Optional torch Tensor torch nn functional linear x weight bias relu + N = x = torch rand N N requires_grad=True weight = torch rand N N requires_grad=True bias = None scripted = checkScript method x weight bias check_types requires last_graph scripted set so we just skip check_against_reference scripted method lambda x x x weight bias check_types=False bias = torch rand N N requires_grad=True scripted = checkScript method x weight bias check_types requires last_graph scripted set so we just skip check_against_reference scripted method lambda x x x weight bias check_types=False test_requires_grad_for_tensor_list enable_profiling_mode_for_profiling_tests output var_list should have requires_grad set True func input torch Tensor input torch Tensor - Tuple torch Tensor List torch Tensor var_list = input input var = torch cat var_list output = var + output var_list jit_f = torch jit script func input = torch randn requires_grad=True input = torch randn output_ref = func input input _ range output = jit_f input input assert output_ref requires_grad == output requires_grad assert output_ref requires_grad == output requires_grad assert output_ref requires_grad == output requires_grad unittest skip disable until we property handle tensor lists undefined gradients test_differentiable_graph_ops_requires_grad x = torch randn dtype=torch float requires_grad_ y = torch randn dtype=torch float t x torch Tensor y torch Tensor flag bool o = x + o = torch relu o o = y + o = torch relu o o = o + o flag o = o + oo = torch relu o o = o + oo = torch relu o oo = oo + oo o = o oo = torch relu o o = o oo = torch relu o oo = oo + oo o o o oo oo oo enable_profiling_mode_for_profiling_tests t_jit = torch jit script t jit_o = t_jit x y False jit_o = t_jit x y False o = t x y False FileCheck check prim DifferentiableGraph run t_jit graph_for x y False validate differentiableGraphOps marking proper requires_grad oo jit_oo zip o jit_o assertEqual oo requires_grad jit_oo requires_grad assertEqual oo jit_oo one more runs trigger fusion jit_o = t_jit x y False oo jit_oo zip o jit_o assertEqual oo dtype jit_oo dtype assertEqual oo requires_grad jit_oo requires_grad assertEqual oo jit_oo unittest skipIf GRAPH_EXECUTOR == ProfilingMode PROFILING Simple Executor doesn t support gradients test_prune_grad torch jit script t input bias torch nn functional relu input + bias input = torch randn requires_grad=True bias = torch randn requires_grad=False bias does NOT require grad NUM_PROFILED_RUNS = num_profiled_runs NUM_PROFILED_RUNS WARMUP = runs reach backward + optimize _ range WARMUP o = t input bias o sum backward fwd_plan = list t get_debug_state execution_plans values bwd_graph = list fwd_plan code grad_executor_states execution_plans values graph tup = next bwd_graph outputs assertEqual len list tup node inputs test_simple_merge o -- o fn x y z = x y b = z b graph = _perform_ad_subgraph_slicing fn assertGraphSize graph assertGraphContainsExactly graph prim DifferentiableGraph test_simple_no_merge o autodiff supported x autodiff supported o -- x fn x y z = x y b = torch zeros abs int y b graph = _perform_ad_subgraph_slicing fn g_str = str graph FileCheck check aten Int check aten zeros check_not aten mul run g_str g_str find assertGraphContainsExactly graph prim DifferentiableGraph test_does_not_merge_unrelated o o fn w x y z = x y b = w z b graph = _perform_ad_subgraph_slicing fn assertGraphSize graph assertGraphContainsExactly graph prim DifferentiableGraph test_merges_without_cycles o -- o -- o &#124; ^ \_________ fn w x y = w x b = y c = b c graph = _perform_ad_subgraph_slicing fn assertGraphSize graph assertGraphContainsExactly graph prim DifferentiableGraph test_merges_dense o o &#124; \ &#124; &#124; \ &#124; &#124; \ &#124; vv vv o o fn x y b = x chunk c d = y chunk + c b + d graph = _perform_ad_subgraph_slicing fn assertGraphSize graph assertGraphContainsExactly graph prim DifferentiableGraph test_does_not_create_cycles o -- x -- o &#124; ^ \_________ fn w x y = w x b = torch zeros abs int c = b c graph = _perform_ad_subgraph_slicing fn assertGraphContainsExactly graph prim DifferentiableGraph test_merges_up o -- x o &#124; ^ \_________ fn w x y z = w x b = torch zeros abs int y c = z b c graph = _perform_ad_subgraph_slicing fn g_str = str graph FileCheck check_not aten add run g_str g_str find assertGraphContainsExactly graph prim DifferentiableGraph test_merges_down o x -- o &#124; ^ \_________ fn v w x y = v w b = torch ones int y c = b c graph = _perform_ad_subgraph_slicing fn add moved down g_str = str graph FileCheck check_not aten add run g_str g_str find assertGraphContainsExactly graph prim DifferentiableGraph test_respects_lexical_scoping fn x k y = x bool k k = k + y z = y k z k graph = _perform_ad_subgraph_slicing fn We should have combined two multiplications into same group they should each separate DiffGraph assertGraphContainsExactly graph prim DifferentiableGraph test_merge_respects_aliasing fn x k cond y = x y = y k y = y bool cond z = y z = y z add_ out = z + k + out = out out out graph = _perform_ad_subgraph_slicing fn z did did get merged into subgraph FileCheck check prim If check aten select check_next aten select check_next aten add_ check Differentiable run graph assertGraphContainsExactly graph prim DifferentiableGraph test_aliased_outputs enable_profiling_mode_for_profiling_tests Case aliasing between relu t within DifferentiableGraph It should valid merge both split_with_sizes relu one graph input_str = graph Tensor b Tensor = aten relu Tensor = aten t b graph = torch _C parse_ir input_str torch _C _jit_pass_create_autodiff_subgraphs graph FileCheck check prim DifferentiableGraph check aten relu check aten t run graph Case aliasing between relu split_with_sizes both outputs Diff graph It should invalid merge both split_with_sizes relu one graph i e relu split_with_sizes should different differentiable graphs input_str = graph Tensor b Tensor = aten relu int = prim Constant value= int = prim Constant value= Tensor = aten split_with_sizes b Tensor Tensor = prim TupleConstruct b graph = torch _C parse_ir input_str torch _C _jit_pass_create_autodiff_subgraphs graph FileCheck check Tensor = prim DifferentiableGraph check prim DifferentiableGraph check Tensor = aten relu check_not aten split_with_sizes run graph Case two aliased nodes graph Both ` split_with_sizes ` should unfused input_str = graph Tensor b Tensor = aten relu s int = prim Constant value= s int = prim Constant value= int = prim Constant value= Tensor = aten split_with_sizes b s Tensor = aten split_with_sizes b s Tensor Tensor = prim TupleConstruct b graph = torch _C parse_ir input_str torch _C _jit_pass_create_autodiff_subgraphs graph FileCheck check Tensor = prim DifferentiableGraph check prim DifferentiableGraph check Tensor = aten relu check_not aten split_with_sizes run graph Case aliased output has descendant Both should unfused Note comes before test we unfuse reverse topo order input_str = graph Tensor b Tensor = aten relu int = prim Constant value= int = prim Constant value= Tensor = aten t b Tensor = aten relu Tensor Tensor Tensor = prim TupleConstruct b graph = torch _C parse_ir input_str torch _C _jit_pass_create_autodiff_subgraphs graph FileCheck check Tensor = prim DifferentiableGraph check prim DifferentiableGraph check Tensor = aten relu check_not aten t run graph Case multiple aliased groups Both should unfused Note comes before test we unfuse reverse topo order input_str = graph Tensor b Tensor = aten relu c Tensor = aten abs int = prim Constant value= int = prim Constant value= d Tensor = aten t c Tensor = aten t b Tensor = aten relu Tensor Tensor Tensor = prim TupleConstruct d b c b graph = torch _C parse_ir input_str torch _C _jit_pass_create_autodiff_subgraphs graph FileCheck check Tensor = prim DifferentiableGraph check prim DifferentiableGraph check Tensor = aten relu check_not aten t run graph test_has_profiled_info_aliasing_outputs The expectation CallFunction will prevent final profile node getting merged into DifferentiableGraph create_autodiff_subgraphs will instead add type ir = graph Tensor Tensor = prim profile profiled_type=Float requires_grad= Tensor = aten relu Tensor = prim profile profiled_type=Float requires_grad= Tensor = aten relu Tensor = prim CallFunction Tensor = prim profile profiled_type=Float requires_grad= graph = torch _C parse_ir ir torch _C _jit_pass_create_autodiff_subgraphs graph n graph nodes n kind == prim DifferentiableGraph diff_graph = n g Subgraph outputs = list diff_graph outputs assertEqual len outputs output = outputs assertEqual False output requiresGrad FileCheck check = prim DifferentiableGraph check prim DifferentiableGraph check = aten relu check requires_grad= check aten relu run graph __name__ == __main__ raise_on_run_directly test test_jit py