Owner s oncall distributed copy functools itertools typing Union torch torch distributed dist torch nn nn torch nn functional F torch distributed _composable checkpoint replicate torch distributed fsdp fully_shard torch distributed fsdp _fully_shard _fsdp_param_group RegisterPostBackwardFunction torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp check_sharded_parity FSDPTest get_devtype MLP patch_reduce_scatter patch_register_post_backward_hook_backward reduce_scatter_with_assert torch testing _internal common_utils run_tests device_type = torch device get_devtype TestFullyShardFrozen FSDPTest property world_size - int min torch get_device_module device_type device_count skip_if_lt_x_gpu test_train_mixed_requires_grad_per_group Tests training parity DDP when mixing frozen non-frozen parameters same FSDP communication group This checks reduce-scatters reduce expected numel they called via custom autograd function backward i e they delayed until end backward run_subtests reshard_after_forward False True use_activation_checkpointing False True freeze_after_init False True _test_train_mixed_requires_grad_per_group _test_train_mixed_requires_grad_per_group reshard_after_forward Union bool int use_activation_checkpointing bool freeze_after_init bool torch manual_seed num_mlps lin_dim = model = nn Sequential MLP lin_dim torch device cpu _ range num_mlps Train biases only e g like BitFit freeze_after_init param_name param model named_parameters bias param_name param requires_grad_ False ref_model = replicate copy deepcopy model device_type device_ids= rank find_unused_parameters=freeze_after_init ref_optim = torch optim Adam ref_model parameters lr= e- mlp model use_activation_checkpointing checkpoint mlp fully_shard mlp reshard_after_forward=reshard_after_forward fully_shard model reshard_after_forward=reshard_after_forward optim = torch optim Adam model parameters lr= e- orig_reduce_scatter = dist reduce_scatter_tensor freeze_after_init param_name param itertools chain model named_parameters ref_model named_parameters bias param_name param requires_grad_ False mlp model assert isinstance mlp MLP The reduce-scatter numel check assumes model consists f only same MLP got type mlp expected_numel = sum p _local_tensor numel n p model named_parameters bias n assert_fn output torch Tensor assertEqual output numel expected_numel reduce_scatter = functools partial reduce_scatter_with_assert orig_reduce_scatter assert_fn orig_backward = RegisterPostBackwardFunction backward backward_count = backward_with_count args kwargs nonlocal backward_count backward_count += orig_backward args kwargs torch manual_seed + rank + device = device_type patch_reduce_scatter reduce_scatter patch_register_post_backward_hook_backward backward_with_count iter_idx range inp = torch randn lin_dim device=device losses list torch Tensor = _model _optim ref_model ref_optim model optim _optim zero_grad set_to_none= iter_idx == losses append _model inp sum losses - backward _optim step check_sharded_parity ref_model model assertEqual losses losses Check post-backward hooks ran through autograd backward final callback except possibly first MLP which does have input requires grad assertTrue backward_count = num_mlps - skip_if_lt_x_gpu test_train_mixed_requires_grad_across_groups Tests training parity DDP when mixing frozen non-frozen parameters across different FSDP communication groups including possibly unfreezing parameters run_subtests reshard_after_forward False True unfreeze_params False True _test_train_mixed_requires_grad_across_groups _test_train_mixed_requires_grad_across_groups reshard_after_forward Union bool int unfreeze_params bool torch manual_seed num_linears lin_dim = modules list nn Module = _ range num_linears modules += nn Linear lin_dim lin_dim nn ReLU model = nn Sequential modules ref_model = replicate copy deepcopy model device_type device_ids= rank find_unused_parameters=True module model modules isinstance module nn Linear fully_shard module reshard_after_forward=reshard_after_forward ref_optim = torch optim Adam ref_model parameters lr= e- optim = torch optim Adam model parameters lr= e- orig_backward = RegisterPostBackwardFunction backward backward_count = _set_requires_grad seq nn Module requires_grad bool i range num_linears Interleave frozen - non-frozen - linears i == param seq i parameters param requires_grad_ requires_grad backward_with_count args kwargs nonlocal backward_count backward_count += orig_backward args kwargs _set_requires_grad model False _set_requires_grad ref_model False num_iters no_grad_iter_idx = torch manual_seed + rank inp = torch randn lin_dim device=device_type patch_register_post_backward_hook_backward backward_with_count iter_idx range num_iters losses list torch Tensor = _model _optim ref_model ref_optim model optim Unfreeze parameters last step emulate some kinds fine-tuning unfreeze_params iter_idx == num_iters - _set_requires_grad model True iter_idx == no_grad_iter_idx torch no_grad losses append _model inp sum losses append _model inp sum losses - backward _optim step _optim zero_grad set_to_none= iter_idx == assertEqual losses losses Check post-backward hooks ran through autograd backward final callback except possibly first linear which does have input requires grad assertTrue backward_count = num_linears - skip_if_lt_x_gpu test_multi_forward_mixed_requires_grad Tests training parity DDP when having trainable frozen modules participate multiple times forward run_subtests reshard_after_forward True False _test_multi_forward_mixed_requires_grad _test_multi_forward_mixed_requires_grad reshard_after_forward Union bool int MultiForwardModule nn Module __init__ device torch device super __init__ layer_ = nn Linear device=device layer_no_grad = nn Linear device=device layer_with_grad = nn Linear device=device layer_no_grad requires_grad_ False forward x torch Tensor - torch Tensor x = layer_ x _ range x = layer_no_grad F relu layer_with_grad x Make sure calling same layer multiple times works regardless whether gradient enabled torch no_grad x += F relu layer_with_grad x x torch manual_seed model = MultiForwardModule torch device cpu ref_model = replicate copy deepcopy model device_type device_ids= rank ref_optim = torch optim Adam ref_model parameters lr= e- module model modules isinstance module nn Linear fully_shard module reshard_after_forward=reshard_after_forward fully_shard model reshard_after_forward=reshard_after_forward optim = torch optim Adam model parameters lr= e- iter_idx range inp = torch randn device=device_type losses list torch Tensor = _model _optim ref_model ref_optim model optim _optim zero_grad set_to_none= iter_idx == losses append _model inp sum losses - backward _optim step assertEqual losses losses __name__ == __main__ run_tests