__future__ annotations dataclasses dataclass typing Optional TYPE_CHECKING torchgen api cpp dispatcher functionalization torchgen api translate translate torchgen api types BaseCType Binding CType DispatcherSignature iTensorListRefT NativeSignature OptionalCType optionalSymIntArrayRefT symIntArrayRefT SymIntT tensorListT tensorT VectorCType ViewInverseSignature torchgen context method_with_native_function native_function_manager with_native_function with_native_function_and torchgen model Argument BackendIndex BaseTy BaseType FunctionSchema ListType NativeFunction NativeFunctionsGroup NativeFunctionsViewGroup Return SchemaKind SelfArgument TensorOptionsArguments torchgen native_function_generation INPLACE_OPS_THAT_DONT_GET_GROUPED_PROPERLY MUTABLE_OPS_THAT_CANNOT_GET_AN_OUT_VARIANT OUT_OPS_THAT_DONT_GET_GROUPED_PROPERLY torchgen utils concatMap dataclass_repr FileManager TYPE_CHECKING collections abc Callable torchgen selective_build selector SelectiveBuilder Note Mutable Ops Not Using Functionalization Ops list currently do work functionalization should fixed MUTABLE_OPS_NOT_USING_FUNCTIONALIZATION = OUT_OPS_THAT_DONT_GET_GROUPED_PROPERLY + MUTABLE_OPS_THAT_CANNOT_GET_AN_OUT_VARIANT + INPLACE_OPS_THAT_DONT_GET_GROUPED_PROPERLY + It will BC-breaking we should fix their schemas should inplace record_stream See Note resize_ Functionalization resize_ resize_as_ This function used testing purposes only _fill_mem_eff_dropout_mask_ This file contains codegen relates functionalization pass It includes - gen_functionalization_definition Generates dispatcher kernel definitions functionalization pass - gen_functionalization_registration Generates dispatcher kernel registrations functionalization pass - gen_functionalization_view_inverse_declaration Generates declaration inverse view every view op needed functionalization We manually implement their definitions - gen_composite_view_copy_kernel Generates view_copy composite kernels all view_copy operators Generates body default composite C++ kernel view _copy NativeFunction See Note view_copy NativeFunctions dataclass frozen=True GenCompositeViewCopyKernel backend_index BackendIndex method_with_native_function __call__ g NativeFunctionsViewGroup - str &#124; None g view_copy None None g view_copy func name name base = f g view func name name _copy If view_copy doesn t match standard naming scheme op _copy assume already exists doesn t need generated Example slice_inverse copy variant named slice_scatter instead slice_inverse_copy None metadata = backend_index get_kernel g view_copy assert metadata None We can make view_copy work more cases using reshape when normal view call would ordinarily fail This also makes LTC more efficient because they don t need include clone calls their graph which normally needed reshape str g view_copy func name == view_copy assert metadata kernel == view_copy_symint \ Tensor view_copy_symint const Tensor SymIntArrayRef size c SymDimVector shape = infer_size_dv size sym_numel detail computeStride sym_sizes sym_strides shape has_value reshape_symint size auto output = _ops view call size output clone memory_format= MemoryFormat Contiguous view_copy native signature since we re generating native kernel Functionalization always operates symints though view_copy_sig = NativeSignature g view_copy func symint=metadata supports_symint view dispatcher signature since we re calling into _ops API view_sig = DispatcherSignature g view func view_api_name = g view func name unambiguous_name exprs = join e expr e translate view_copy_sig arguments view_sig arguments view ops today always either Tensor list Tensors assert len g view func returns == assert g view func returns type == BaseType BaseTy Tensor g view func returns type == ListType BaseType BaseTy Tensor None g view func returns type == BaseType BaseTy Tensor return_cloned_output = \ output clone memory_format= MemoryFormat Contiguous If type list we need clone each tensor list return_cloned_output = f \ view_copy_sig returns_type cpp_type out_clone const auto i c irange output size out_clone push_back output i clone memory_format= MemoryFormat Contiguous out_clone The default generated composite kernel view _copy operators just clones input tensor runs underlying view clone f view_copy_sig defn name=metadata kernel auto output = _ops view_api_name call exprs return_cloned_output return_str rets tuple Return names list str - str assert len rets == len names len rets == len rets == f names f dispatcher returns_type rets cpp_type join names modifies_arguments f NativeFunction - bool any annotation None annotation is_write f func arguments flat_all wrapper_name func FunctionSchema - str func name overload_name f cpp name func _ func name overload_name cpp name func is_tensor_like Argument &#124; TensorOptionsArguments &#124; SelfArgument - bool isinstance SelfArgument isinstance Argument type is_tensor_like We need wrap unwrap various arguments op functionalization kernels Some op schemas include non-owning types though like TensorList when we unwrap them we expect get out owning type We also lambda tells you how convert non-owning type argument into owning type get_owning_type t CType - tuple CType Callable str str t == BaseCType tensorListT VectorCType BaseCType tensorT lambda x f x vec t == BaseCType iTensorListRefT VectorCType BaseCType tensorT lambda x f x begin x end There technically other non-owning types out there like IntArrayRef functionalization only actually cares about ones involving tensors t lambda x x unwraps all tensor-like arguments returning string containing all logic does unwrapping context used translate all relevant bindings unwrap_tensor_args sig DispatcherSignature is_view_op bool - tuple str list Binding context list Binding = unwrapped_tensor_args list str = arg sig arguments is_tensor_like arg argument tensor inputs we want unwrap them before passing them into redispatch calls unwrapped_name = f arg name _ For most ops functionalization needs sync any pending updates input tensors before calling operator since otherwise operator will act stale data For view ops though we can continue defer syncing until tensor used non-view operator maybe_sync_input = is_view_op f functionalization impl sync arg name unwrapped_type conversion_fn = get_owning_type arg nctype remove_const_ref type unwrapped_tensor_args append f unwrapped_type cpp_type unwrapped_name functionalization impl isFunctionalTensor arg name maybe_sync_input unwrapped_name = functionalization impl from_functional_tensor arg name unwrapped_name = conversion_fn arg name context append arg with_name unwrapped_name non-tensor inputs we want pass them directly into redispatch calls context append arg unwrap_tensor_args_str = \n join unwrapped_tensor_args unwrap_tensor_args_str context converts all tensor-like arguments meta tensors which used compute stride info Returns string containing all logic does conversions context used translate all relevant bindings convert_to_meta_tensors sig DispatcherSignature - tuple str list Binding context list Binding = unwrapped_tensor_args list str = arg sig arguments is_tensor_like arg argument tensor inputs we want unwrap them before passing them into redispatch calls a_ = arg name unwrapped_name = f arg name _meta unwrapped_tensor_args append f auto unwrapped_name = to_meta a_ context append arg with_name unwrapped_name non-tensor inputs we want pass them directly into redispatch calls context append arg unwrap_tensor_args_str = \n join unwrapped_tensor_args unwrap_tensor_args_str context The functionalization codegen currently expects view op schemas have form foo Tensor - Tensor e g transpose foo Tensor - Tensor e g transpose_ assert_view_op_properties func FunctionSchema - None is_alias Argument - bool annotation None args = func arguments flat_non_out The first argument tensor alias semantics annotations assert len args args type == BaseType BaseTy Tensor f In functionalization codegen we expect first argument every view operator tensor found argument type str args type operator str func name No other arguments have aliasing semantics assert is_alias args any is_alias args In functionalization codegen we expect first argument every view operator alias output View operators multiple aliasing inputs aren t supported yet Found operator doesn t satisfy constraint One-liner expression checking expression expr type type has any symbolic values emit_expr_has_symbolic_values expr str type CType - str type == BaseCType SymIntT f expr is_symbolic isinstance type OptionalCType innerexpr = f expr f expr has_value emit_expr_has_symbolic_values innerexpr type elem false type == BaseCType optionalSymIntArrayRefT emit_expr_has_symbolic_values expr OptionalCType BaseCType symIntArrayRefT type BaseCType symIntArrayRefT VectorCType BaseCType SymIntT argname = arg lambda_check = emit_expr_has_symbolic_values argname BaseCType SymIntT std any_of f expr begin expr end f = auto argname lambda_check raise ValueError unsupported type has_symbolic_values check It should SymInt collection those f Got type cpp_type Detects whether any SymInt arguments fact symbolic values This used constructor ViewMeta emit_has_symbolic_inputs sig DispatcherSignature - tuple str str name = has_symbolic_inputs statements = f name = name &#124; emit_expr_has_symbolic_values binding name binding nctype type binding sig arguments isinstance binding argument Argument binding argument type is_symint_like body = \n join statements name f bool name = false body Generates Functionalization kernel - ops create aliases e g transpose - ops views AND mutations e g transpose_ emit_view_functionalization_body g NativeFunctionsViewGroup view_inplace bool - str view_inplace This op both inplace op AND view op See Note Functionalization Pass - Inplace View Ops details I currently have view meta call into out-of-place variant view avoid having define extra ~ inplace view _inverse_ functions Most view ops don t have NativeFunctionGroup s both because we don t define out= variants view ops I m assuming every inplace-view op has corresponding out-of-place view op same name trailing underscore removed This currently asserted parse time gen py see error_check_native_functions assert g view_inplace None f = g view_inplace f = g view assert g view_copy None native_function_manager f call_sig = DispatcherSignature from_schema g view_copy func spec = ViewMetaSpecialization g f=f view_copy op name functionalization kernels need call api_name = g view_copy func name unambiguous_name Sometimes functionalization pass needs no-op e g passed non-functional tensors no-op ing context just redispatching original op noop_api_name = f func name unambiguous_name dispatcher_sig = DispatcherSignature from_schema f func assert_view_op_properties f func view_tensor_name = dispatcher_sig arguments name return_type = dispatcher_sig returns_type remove_const_ref cpp_type unwrap_tensor_args_str unwrapped_args_ctx = unwrap_tensor_args dispatcher_sig is_view_op=True view_redispatch_args = e expr e translate unwrapped_args_ctx call_sig arguments method=False The meta API call should use same arguments convert all tensors meta tensors first meta_conversion_str meta_call_ctx = convert_to_meta_tensors dispatcher_sig meta_call_args = e expr e translate meta_call_ctx call_sig arguments method=False symbolic_inputs_varname symbolic_inputs_check = emit_has_symbolic_inputs call_sig inplace_view f tags See Note Functionalization Pass - Inplace View Ops more details f dispatcher_sig defn name=wrapper_name f func is_redispatching_fn=True functionalization impl isFunctionalTensor view_tensor_name functionalization re-entrant will no-op wasn t passed FunctionalTensorWrapper unwrap_tensor_args_str AutoDispatchSkipFunctionalize guard _ops noop_api_name call join view_redispatch_args auto reapply_views = functionalization impl getFunctionalizationReapplyViewsTLS auto inverse_return_mode = reapply_views functionalization InverseReturnMode ViewOrScatterInverse functionalization InverseReturnMode NeverView symbolic_inputs_check auto view_meta = spec new auto compute_reference_meta = view_tensor_name key_set has_backend c BackendComponent XLABit &#124; &#124; view_tensor_name key_set has_backend c BackendComponent LazyBit return_type reference_tensor_output compute_reference_meta disable_meta_reference meta_conversion_str AutoDispatchSkipFunctionalize func_guard c impl ExcludeDispatchKeyGuard guard exclude_keys_for_meta_dispatch reference_tensor_output = _ops noop_api_name call join meta_call_args This function adds above view meta current tensor replays them off base mutating size stride info current FunctionalTensorWrapper Because we need make sure run reference shape function above BEFORE doing otherwise we ll end up running reference function using wrong sizes strides functionalization impl mutate_view_meta view_tensor_name view_meta See Note Propagating strides functionalization pass XLA LTC don t implement logic propagate strides correctly so we need rely reference implementation here instead relying output forward lambda having correct stride info compute_reference_meta disable_meta_reference functionalization impl set_sizes_strides_offset view_tensor_name reference_tensor_output view_tensor_name f dispatcher_sig defn name=wrapper_name f func is_redispatching_fn=True unwrap_tensor_args_str functionalization impl isFunctionalTensor view_tensor_name functionalization re-entrant will no-op wasn t passed FunctionalTensorWrapper AutoDispatchSkipFunctionalize guard _ops noop_api_name call join view_redispatch_args auto reapply_views = functionalization impl getFunctionalizationReapplyViewsTLS auto inverse_return_mode = reapply_views functionalization InverseReturnMode ViewOrScatterInverse functionalization InverseReturnMode NeverView auto compute_reference_meta = view_tensor_name key_set has_backend c BackendComponent XLABit &#124; &#124; view_tensor_name key_set has_backend c BackendComponent LazyBit return_type reference_tensor_output compute_reference_meta disable_meta_reference meta_conversion_str AutoDispatchSkipFunctionalize func_guard c impl ExcludeDispatchKeyGuard guard exclude_keys_for_meta_dispatch reference_tensor_output = _ops noop_api_name call join meta_call_args return_type tmp_output AutoDispatchSkipFunctionalize guard reapply_views tmp_output = _ops noop_api_name call join view_redispatch_args tmp_output = _ops api_name call join view_redispatch_args symbolic_inputs_check auto view_meta = spec new auto out = functionalization impl create_functional_tensor_with_view_meta tmp_output view_tensor_name view_meta See Note Propagating strides functionalization pass compute_reference_meta disable_meta_reference functionalization impl set_sizes_strides_offset out reference_tensor_output out maybe_create_output f NativeFunction var_name str - str len f func returns == return_type = dispatcher returns_type f func returns remove_const_ref cpp_type f return_type var_name = Given NativeFunction variable name corresponding output redispatching function returns two lists names consisting - names returns corresponding original mutable inputs outer function - names returns corresponding immutable outputs inner redispatched function get_mutable_redispatch_return_names f NativeFunction inner_return_var str - tuple list str list str aliased_returns = non_aliased_returns = i name enumerate f func aliased_return_names name None aliased_returns append name non_aliased_returns append inner_return_var len f func returns == f std get i inner_return_var aliased_returns non_aliased_returns When functionalization no-op s redispatches mutable operator we need take care so - For fresh outputs we result redispatch without wrapping outputs - For outputs aliased inputs we inputs directly since some them might have been wrapped return_from_mutable_noop_redispatch f NativeFunction inner_return_var str - str aliased non_aliased = get_mutable_redispatch_return_names f inner_return_var Just get all names immediately them return_str f func returns aliased + non_aliased wrap_propagate_mutations_and_return f NativeFunction functional_op NativeFunction inner_return_var str - str mutable_arg_names = f func arguments mutable_arg_names aliased_outer_rets non_aliased_outer_rets = get_mutable_redispatch_return_names f inner_return_var _ non_aliased_inner_rets = get_mutable_redispatch_return_names functional_op inner_return_var The outer function may have mix aliased non-aliased outputs But inner functional op we re transforming should only have non-aliased outputs assert len mutable_arg_names + len non_aliased_outer_rets == len non_aliased_inner_rets First take all newly created outputs inner call wrap them into functional tensors updates = non_aliased_wrapped_ret_names = i inner_ret enumerate non_aliased_inner_rets len non_aliased_outer_rets ret_name = f output_ i updates append f \ auto output_ i = functionalization impl to_functional_tensor inner_ret non_aliased_wrapped_ret_names append ret_name Next take all mutated outputs inner call corresponding mutated inputs propagate mutations outer_arg inner_ret zip mutable_arg_names non_aliased_inner_rets len non_aliased_outer_rets updates append f \ auto outer_arg _inner = functionalization impl from_functional_tensor outer_arg functionalization impl replace_ outer_arg inner_ret functionalization impl commit_update outer_arg functionalization impl sync outer_arg auto outer_arg _inner_updated = functionalization impl from_functional_tensor outer_arg functionalization impl propagate_xla_data_direct outer_arg _inner outer_arg _inner_updated Finally we - Any mutable arguments also returns - Any immutable returns created wrapping output inner call returns_str = return_str f func returns aliased_outer_rets + non_aliased_wrapped_ret_names updates_str = \n join updates f \ updates_str returns_str Generates Functionalization kernel - mutation ops inplace out= ops with_native_function_and emit_inplace_functionalization_body f NativeFunction g NativeFunctionsGroup - str mutation case assert modifies_arguments f dispatcher_sig = DispatcherSignature from_schema f func unwrap_tensor_args_str unwrapped_args_ctx = unwrap_tensor_args dispatcher_sig is_view_op=False mutated_names = name f func arguments flat_all type is_tensor_like annotation None non_mutated_names = name f func arguments flat_all type is_tensor_like annotation None non_mutated_tensor_names = name f func arguments flat_all type == BaseType BaseTy Tensor annotation None all mutable inputs must functional tensors order participate functionalization check_all_mutated_args_are_functional = join true + f functionalization impl isFunctionalTensor mutated_names check_any_non_mutated_args_are_functional = &#124; &#124; join false + f functionalization impl isFunctionalTensor non_mutated_names check_any_non_mutated_tensors_are_xla = &#124; &#124; join false + f device type == c DeviceType XLA non_mutated_tensor_names These used cases where we don t functionalize redispatch inplace op case we hit inplace op doesn t have out-of-place equivalent case we hit inplace ops our inputs functional tensors which case our kernel just no-ops inplace_exprs = e expr e translate unwrapped_args_ctx dispatcher_sig arguments method=False call out-of-place variant op return_type = dispatcher returns_type g functional func returns remove_const_ref cpp_type functional_sig = DispatcherSignature from_schema g functional func functional_exprs = e expr e translate unwrapped_args_ctx functional_sig arguments method=False meta_conversion_str meta_call_ctx = convert_to_meta_tensors dispatcher_sig We don t want run inplace meta func ops like set_ because they re unnecessary inplace meta checks only useful ops like add_ where broadcasting will work out-of-place case should fail inplace call They ll also fail without adding extra infra we d need convert input storage argument into meta storage any_storage_args = any type == BaseType BaseTy Storage f func arguments flat_all f dispatcher_sig defn name=wrapper_name f func is_redispatching_fn=True str any_storage_args f func kind == SchemaKind inplace lower disable_meta_reference Before converting mutable op its functional variant run meta tensors through original op This will help us catch shape errors apply inplace ops wouldn t apply their functional variants We can only do inplace ops today though because they technically all support meta tensors meta_conversion_str AutoDispatchSkipFunctionalize func_guard c impl ExcludeDispatchKeyGuard guard exclude_keys_for_meta_dispatch _ops f func name unambiguous_name call join name meta_call_ctx unwrap_tensor_args_str check_all_mutated_args_are_functional We want disable check there any XLA tensors cpu_tensor copy_ xla_tensor valid code check_any_non_mutated_tensors_are_xla check_any_non_mutated_args_are_functional case trying mutate non functional tensor functional tensor error TORCH_INTERNAL_ASSERT false mutating non-functional tensor functional tensor allowed Please ensure all your inputs wrapped inside functionalize call case arguments functional tensors so we no-op redispatch AutoDispatchSkipFunctionalize guard maybe_create_output f tmp_output _ops f func name unambiguous_name call join inplace_exprs return_from_mutable_noop_redispatch f tmp_output return_type tmp_output AutoDispatchSkipFunctionalize guard tmp_output = _ops g functional func name unambiguous_name call join functional_exprs wrap_propagate_mutations_and_return f g functional tmp_output The below functions generate RegisterFunctionalization cpp These files provide kernels run functionalization pass which can opted into per backend e g XLA Vulkan composable transform functionalize functorch See Note Functionalization Pass View Inverses gen_functionalization_view_inverse_declaration selector SelectiveBuilder g NativeFunctionsViewGroup - str &#124; None For every non-composite view op we need corresponding inverse view function This generates declarations so we get good compiler error when someone adds new view with_native_function emit_decl_helper g NativeFunctionsViewGroup - str &#124; None g view has_composite_implicit_autograd_kernel None view_inverse_sig = ViewInverseSignature g view_inverse_sig decl emit_decl_helper g Helper generating ` ViewMeta ` specializations dataclass ViewMetaSpecialization g NativeFunctionsViewGroup f NativeFunction property is_multi_output - bool functionalization is_multi_output f func property is_as_strided - bool str f func name == as_strided property out_index - str is_multi_output functionalization out_index_binding name property classname - str functionalization classname f func decl - list str base_ctor_arguments = functionalization base_ctor_arguments f func extra_ctor_arguments = functionalization extra_ctor_arguments f func attributes = functionalization attributes f func List types declaring ` SerializableTuple ` type serializable_tuple_args = \n join f binding type binding name binding base_ctor_arguments + attributes Arguments used forwarding tuple elements constructor destructure_tuple_args = join f std get i tpl i range len base_ctor_arguments + len extra_ctor_arguments List constructor parameters ctor_parameters = join binding decl binding base_ctor_arguments + extra_ctor_arguments Call base ` ViewMeta ` constructor Both ` is_multi_output ` ` is_as_strided ` known values given operation schema is_multi_output_str = str is_multi_output lower is_as_strided_str = str is_as_strided lower base_ctor_bindings = join ` has_symbolic_inputs ` always taken parameter functionalization has_symbolic_inputs_binding name f is_multi_output= is_multi_output_str f is_as_strided= is_as_strided_str ` out_index ` know operation returns only one value Otherwise we also take parameter f out_index= out_index Assignments ` extra_ctor_arguments ` their corresponding fields These extra fields to-be-declared specialization We need set ` allow_expensive_conversions ` since we storing owned versions non-owning arguments ctor_assignments = \n join f e type name e expr e translate extra_ctor_arguments attributes method=False allow_expensive_conversions=True List arguments constructing ` SerializableTuple ` instance tuple_arguments = join binding name binding base_ctor_arguments + attributes List field declarations attr_declarations = \n join f binding decl binding attributes Override ` to_out_index ` operation returns more than value to_out_index_decl = is_multi_output to_out_index_decl = std shared_ptr ViewMeta to_out_index int _t out_idx override f struct TORCH_API classname public ViewMeta FUNCTIONALIZATION_VIEWMETA_NAME classname FUNCTIONALIZATION_VIEWMETA_SERIALIZABLE_TUPLE \n serializable_tuple_args classname const SerializableTuple tpl classname destructure_tuple_args classname ctor_parameters functionalization ViewMeta base_ctor_bindings ctor_assignments Tensor forward const Tensor base override Tensor reverse const Tensor base const Tensor mutated_view override to_out_index_decl SerializableTuple to_serializable_tuple std make_tuple tuple_arguments attr_declarations Generate call actual operation opcall is_reverse bool reapply_views bool - str opname = functionalization name g is_reverse=is_reverse include_namespace=True reapply_views=reapply_views Expected arguments operation assert g view_copy None op_arguments = functionalization op_arguments g view_copy func is_reverse The context composed constructor arguments which also field variables stored instance ` base ` tensor context = functionalization base_binding context += functionalization base_ctor_arguments f func context += functionalization attributes f func If we generating call reverse function we also have access ` mutated_view ` argument is_reverse context append functionalization mutated_view_binding arguments = join e expr e translate context op_arguments method=False Index result operation returns multiple values maybe_index = is_reverse is_multi_output maybe_index = f out_index f opname arguments maybe_index impl - list str functions = f Tensor classname forward const Tensor base reapply_views opcall is_reverse=False reapply_views=True opcall is_reverse=False reapply_views=False f Tensor classname reverse const Tensor base const Tensor mutated_view opcall is_reverse=True reapply_views=True If operation returns multiple values also generate ` to_out_index ` implementation is_multi_output functions append f std shared_ptr functionalization ViewMeta classname to_out_index int _t out_index new out_index functions Create Python binding specialized binding - list str name = functionalization classname f func with_namespace=True f create_binding_with_pickle name functionalization Generate instantiation specialized new out_index str = - str name = functionalization classname f func with_namespace=True ctor_arguments = functionalization base_ctor_arguments f func + functionalization extra_ctor_arguments f func Replace ` out_index ` parameter given ` out_index ` arguments = join binding name binding name = out_index out_index binding ctor_arguments f std make_shared name arguments Run function ` run ` both ` view ` ` view_inplace ` functions staticmethod map g NativeFunctionsViewGroup run Callable ViewMetaSpecialization list str - list str maybe_run f Optional NativeFunction - list str f None native_function_manager f run ViewMetaSpecialization g f list concatMap maybe_run g view g view_inplace gen_functionalization_view_meta_classes_base selector SelectiveBuilder g NativeFunctionsViewGroup run Callable ViewMetaSpecialization list str - list str selector include_all_operators g composite ViewMetaSpecialization map g run gen_functionalization_view_meta_classes_decl selector SelectiveBuilder g NativeFunctionsViewGroup - list str gen_functionalization_view_meta_classes_base selector g ViewMetaSpecialization decl gen_functionalization_view_meta_classes_impl selector SelectiveBuilder g NativeFunctionsViewGroup - list str gen_functionalization_view_meta_classes_base selector g ViewMetaSpecialization impl gen_functionalization_view_meta_classes_binding selector SelectiveBuilder g NativeFunctionsViewGroup - list str gen_functionalization_view_meta_classes_base selector g ViewMetaSpecialization binding Generates Python bindings ` ViewMeta ` specialized classes gen_functionalization_view_meta_classes native_functions_path str tags_path str selector SelectiveBuilder install_dir str template_dir str - None torchgen gen get_grouped_by_view_native_functions parse_native_yaml Parse native_functions yaml Then group them into ` NativeFunctionsViewGroup ` This same steps we do gen py ATen codegen native_functions = parse_native_yaml native_functions_path tags_path native_functions native_functions_with_view_groups = get_grouped_by_view_native_functions native_functions view_groups = g g native_functions_with_view_groups isinstance g NativeFunctionsViewGroup fm = FileManager install_dir=install_dir template_dir=template_dir dry_run=False fm write ViewMetaClassesPythonBinding cpp lambda view_meta_bindings list concatMap lambda g gen_functionalization_view_meta_classes_binding selector g view_groups gen_functionalization_registration selector SelectiveBuilder g NativeFunction &#124; NativeFunctionsGroup &#124; NativeFunctionsViewGroup composite_implicit_autograd_index BackendIndex - list str with_native_function emit_registration_helper f NativeFunction - str assert f has_composite_implicit_autograd_kernel registration_str = f TORCH_FN functionalization wrapper_name f func f m impl f func name registration_str Don t generate kernels mobile build selector include_all_operators isinstance g NativeFunctionsViewGroup functionalization needs register kernels view + view_inplace ops See Note Functionalization torch Tensor constructor str g view func name == lift_fresh view_str = g view has_composite_implicit_autograd_kernel view_str append emit_registration_helper g view g view_inplace None g view_inplace has_composite_implicit_autograd_kernel assert g view_inplace is_view_op view_str append emit_registration_helper g view_inplace view_str isinstance g NativeFunctionsGroup Gets hand-written functionalization kernel g inplace None str g inplace func name == set_ source_Tensor fns = fns = list g functions str g func name MUTABLE_OPS_NOT_USING_FUNCTIONALIZATION fns = g registrations = f fns f has_composite_implicit_autograd_kernel continue str f func name == lift See Note Functionalization torch Tensor constructor str f func name == resize_ See Note resize_ Functionalization str f func name name = set_ assert f is_view_op functionalization needs generate register kernels inplace ops We also need directly register CompositeImplicitAUtograd kernels so they decompose properly before functioanlization modifies_arguments f registrations append emit_registration_helper f registrations gen_functionalization_definition selector SelectiveBuilder Note Ideally code should never have look NativeFunction instead only need operate grouped NativeFunctions The only reason currently because we need emit direct dispatch registrations For CompositeImplicitAutograd operators which potentially ungrouped g NativeFunction &#124; NativeFunctionsGroup &#124; NativeFunctionsViewGroup - list str Don t generate kernels mobile build selector include_all_operators isinstance g NativeFunctionsViewGroup Case emit view - view_copy kernels functionalization pass view_defs = g composite invariant NativeFunctionsViewGroup s always have view_copy operator view composite implicit autograd assert g view_copy None dataclass_repr g indent= view_defs append emit_view_functionalization_body g view_inplace=False g view_inplace None view_defs append emit_view_functionalization_body g view_inplace=True view_defs isinstance g NativeFunction Invariant all mutable operators we need handle functionalization should have been properly grouped up TODO The below ops all have problematic schemas prevent them getting functionalized Instead bending over backwards get things work I think we should either fix their schemas BC-breaking hand-write their functionalization kernels str g func name MUTABLE_OPS_NOT_USING_FUNCTIONALIZATION str g func name name MUTABLE_OPS_NOT_USING_FUNCTIONALIZATION assert g has_composite_implicit_autograd_kernel modifies_arguments g Case emit inplace - out-of-place kernels functionalization pass mutation_defs = mutation_defs append emit_inplace_functionalization_body g out g g inplace None mutation_defs append emit_inplace_functionalization_body g inplace g g mutable None mutation_defs append emit_inplace_functionalization_body g mutable g mutation_defs