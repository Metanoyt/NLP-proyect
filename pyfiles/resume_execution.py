This module provides functionality resuming Python execution specific points code primarily used PyTorch Dynamo control flow handling optimization It implements bytecode transformation execution state management enable - Resuming execution arbitrary points Python bytecode - Managing context managers their state across execution boundaries - Transforming generating new code objects preserved execution state - Supporting Python + exception handling block management - Restoring torch function mode stacks other execution context The module critical PyTorch Dynamo s ability optimize code while preserving Python semantics execution state copy dataclasses sys types collections abc Callable Iterable contextlib AbstractContextManager typing Any cast Optional bytecode_transformation add_push_null bytecode_from_template create_binary_subscr create_call_function create_call_function_ex create_instruction create_jump_absolute create_load_const Instruction overwrite_instruction transform_code_object unique_id utils ExactWeakKeyDictionary taken code h cpython CO_OPTIMIZED = x CO_NEWLOCALS = x CO_VARARGS = x CO_VARKEYWORDS = x CO_NESTED = x CO_GENERATOR = x CO_NOFREE = x CO_COROUTINE = x CO_ITERABLE_COROUTINE = x CO_ASYNC_GENERATOR = x trace_rules py constant consistency TORCH_DYNAMO_RESUME_IN_PREFIX = torch_dynamo_resume_in IS_TRACING_RESUME_PROLOGUE_VARNAME = __is_tracing_resume_prologue If is_resume - codegen resume function _initial_push_null insts list Instruction - None sys version_info = insts append create_instruction PUSH_NULL sys version_info insts append create_instruction SWAP arg= Generates bytecode template splits code where LOAD_FAST dummy present _bytecode_from_template_with_split template Callable Any stack_index int varname_map Optional dict str Any = None - tuple list Instruction list Instruction template_code = bytecode_from_template template varname_map=varname_map template_code append create_instruction POP_TOP adjust exception table entry depth inst template_code inst exn_tab_entry inst exn_tab_entry depth += stack_index search LOAD_FAST dummy replace NOPs we can break up bytecode between them dummy_idx dummy_inst = next i inst i inst enumerate template_code inst opname LOAD_FAST LOAD_FAST_BORROW inst argval == dummy None None assert dummy_idx None dummy_inst None replace LOAD_FAST dummy first NOP marking exception area overwrite_instruction dummy_inst create_instruction NOP POP_TOP follows LOAD_FAST dummy - replace NOP marking end exception area assert template_code dummy_idx + opname == POP_TOP overwrite_instruction template_code dummy_idx + create_instruction NOP template_code dummy_idx + template_code dummy_idx + _try_except_tf_mode_template dummy Any stack_var_name Any - None NOTE Make sure name matches what generated symbolic_convert import_source torch _dynamo utils pyrefly ignore unknown-name global __import_torch_dot__dynamo_dot_utils try dummy except noqa E B __import_torch_dot__dynamo_dot_utils set_torch_function_mode_stack type ignore name-defined stack_var_name raise dataclasses dataclass frozen=True ReenterWith stack_index int target_values Optional tuple Any = None try_except_torch_function_mode code_options dict str Any cleanup list Instruction - list Instruction Codegen based off try rest except restore previous tf mode stack raise variables torch_function get_prev_stack_var_name setup_try_except epilogue = _bytecode_from_template_with_split _try_except_tf_mode_template stack_index varname_map= stack_var_name get_prev_stack_var_name cleanup = epilogue + cleanup setup_try_except If we do want destroy stack we can do same thing ` SETUP_WITH ` block only we store context manager local_symbol try_finally code_options dict str Any cleanup list Instruction - list Instruction Codegen based off load args enter context try rest finally exit context NOTE we assume TOS context manager CLASS load_args = target_values load_args = create_load_const val val target_values ctx_name = unique_id f ___context_manager_ stack_index ctx_name code_options co_varnames code_options co_varnames += ctx_name name __enter__ __exit__ name code_options co_names code_options co_names += name create_ctx list Instruction = _initial_push_null create_ctx create_ctx extend load_args create_call_function len load_args False create_instruction STORE_FAST argval=ctx_name _template ctx AbstractContextManager Any dummy Any - None ctx __enter__ try dummy finally ctx __exit__ None None None setup_try_finally epilogue = _bytecode_from_template_with_split _template stack_index varname_map= ctx ctx_name cleanup = epilogue + cleanup create_ctx + setup_try_finally __call__ code_options dict str Any cleanup list Instruction - tuple list Instruction Optional Instruction Codegen based off ctx args rest NOTE we assume TOS context manager CLASS load_args = target_values load_args = create_load_const val val target_values create_ctx list Instruction = Do push NULL Python + since NULL should symbolic stack sys version_info _initial_push_null create_ctx create_ctx extend load_args create_call_function len load_args False _template ctx AbstractContextManager Any dummy Any - None ctx dummy setup_with epilogue = _bytecode_from_template_with_split _template stack_index cleanup = epilogue + cleanup load_fast_ctx_inst = next inst inst setup_with inst opname LOAD_FAST LOAD_FAST_BORROW inst argval == ctx None assert load_fast_ctx_inst None ctx already loaded stack before template - no need LOAD_FAST overwrite_instruction load_fast_ctx_inst create_instruction NOP + only push_exc_info_gen = inst inst epilogue inst opname == PUSH_EXC_INFO push_exc_info_inst = next push_exc_info_gen None expect only PUSH_EXC_INFO epilogue assert next push_exc_info_gen None None create_ctx + setup_with push_exc_info_inst dataclasses dataclass ResumeFunctionMetadata code types CodeType instructions list Instruction = dataclasses field default_factory=list Python + fields NOTE Python removed blocks our purposes block consists instructions all exception table entries have same target map PUSH_EXC_INFO s prefix original block target offset prefix_block_target_offset_remap list int = dataclasses field default_factory=list per-offset map new block target offsets original block target offsets block_target_offset_remap dict tuple int int dict int int = dataclasses field default_factory=dict _filter_iter l Iterable Any l Iterable Any cond Callable Any Any bool - list Any Two-pointer conditional filter e g _filter_iter insts sorted_offsets lambda i o i offset == o returns instructions offsets sorted_offsets = iter l res list Instruction = try cur = next val l cond val cur res append val cur = next except StopIteration pass res _load_tuple_and_call tup tuple Any - list Instruction insts list Instruction = _initial_push_null insts insts extend create_load_const val val tup insts extend create_call_function len tup False insts ContinueExecutionCache cache = ExactWeakKeyDictionary generated_code_metadata = ExactWeakKeyDictionary classmethod lookup cls code types CodeType lineno int init_offset int key Any - types CodeType code cls cache cls cache code = key = tuple key key cls cache code cls cache code key = cls generate code lineno init_offset key cls cache code key classmethod generate cls code types CodeType lineno int init_offset int resume_offset int setup_fn_target_offsets tuple int only used Python + nstack int argnames tuple str argnames_null tuple str setup_fns tuple ReenterWith handle_inactive_ctx bool stack_ctx_vars tuple tuple int tuple Any argnames_ctx_vars tuple tuple str tuple Any null_idxes tuple int mainly used ensure distinct code objects per stack trace which prevents excessive recompilation inner frames nested_code_objs tuple types CodeType - types CodeType assert resume_offset None assert code co_flags CO_GENERATOR &#124; CO_COROUTINE &#124; CO_ITERABLE_COROUTINE &#124; CO_ASYNC_GENERATOR assert code co_flags CO_OPTIMIZED code ContinueExecutionCache generated_code_metadata cls generate_based_on_original_code_object code lineno init_offset resume_offset setup_fn_target_offsets nstack argnames argnames_null setup_fns handle_inactive_ctx stack_ctx_vars argnames_ctx_vars null_idxes nested_code_objs is_py _plus = sys version_info = meta = ResumeFunctionMetadata code update instructions list Instruction code_options dict str Any - None meta instructions = copy deepcopy instructions args = __nested_resume_fns __nested_frame_values args += f ___stack i i range nstack args extend v v argnames v args freevars = tuple code_options co_cellvars + tuple code_options co_freevars freevars = tuple sorted freevars code_options co_name = f TORCH_DYNAMO_RESUME_IN_PREFIX _ code_options co_name _at_ lineno is_py _plus qualified_path = code_options co_qualname rsplit maxsplit= len qualified_path == code_options co_qualname = code_options co_name assert len qualified_path == module_name co_name = qualified_path code_options co_qualname = f module_name TORCH_DYNAMO_RESUME_IN_PREFIX _ co_name _at_ lineno code_options co_firstlineno = lineno code_options co_cellvars = code_options co_freevars = freevars code_options co_argcount = len args code_options co_posonlyargcount = code_options co_kwonlyargcount = code_options co_varnames = tuple args + v v argnames_null v args + v v code_options co_varnames v args + IS_TRACING_RESUME_PROLOGUE_VARNAME code_options co_flags = code_options co_flags ~ CO_VARARGS &#124; CO_VARKEYWORDS target = next i i instructions i offset == resume_offset prefix = is_py _plus freevars prefix append create_instruction COPY_FREE_VARS arg=len freevars prefix append create_instruction RESUME arg= Set is_tracing_resume_prologue prevent graph breaks This doesn t really do anything runtime dynamo will trace will know we re resume function prologue prefix extend create_instruction LOAD_CONST argval=True create_instruction STORE_FAST argval=IS_TRACING_RESUME_PROLOGUE_VARNAME cleanup list Instruction = hooks = fn stack_index fn fn setup_fns hook_target_offsets = fn stack_index setup_fn_target_offsets i i fn enumerate setup_fns offset_to_inst = inst offset inst inst instructions map old hook targets new targets generated hook old_hook_target_remap = stack_i = null_i = stack_ctx_vars_d = dict stack_ctx_vars type ignore var-annotated arg-type i range nstack + len null_idxes null_i len null_idxes null_idxes null_i == i prefix append create_instruction PUSH_NULL null_i += prefix append create_instruction LOAD_FAST argval=f ___stack stack_i handle_inactive_ctx stack_i stack_ctx_vars_d NOTE we assume current stack var context manager CLASS Load args context variable construct prefix extend _load_tuple_and_call stack_ctx_vars_d stack_i stack_i += i hooks hook = hooks pop i hook_insts exn_target = hook code_options cleanup prefix extend hook_insts is_py _plus hook_target_offset = hook_target_offsets pop i old_hook_target = offset_to_inst hook_target_offset meta prefix_block_target_offset_remap append hook_target_offset old_hook_target_remap old_hook_target = exn_target is_py _plus reverse mapping since targets later nested contexts inserted into mapping later show up earlier prefix meta prefix_block_target_offset_remap = list reversed meta prefix_block_target_offset_remap assert hooks NOTE we assume local var context manager CLASS initialize inactive context vars argnames handle_inactive_ctx name vals argnames_ctx_vars prefix append create_instruction LOAD_FAST argval=name prefix extend _load_tuple_and_call vals prefix append create_instruction STORE_FAST argval=name + store NULL into variables NULL argnames_null assert sys version_info = v argnames_null assert v args prefix extend create_instruction PUSH_NULL create_instruction STORE_FAST argval=v Call nested resume function nested_code_objs prefix extend set up __nested_resume_fns - call add_push_null create_instruction LOAD_FAST argval= __nested_resume_fns create_instruction LOAD_CONST argval=- create_binary_subscr del __nested_resume_fns - create_instruction LOAD_FAST argval= __nested_resume_fns create_instruction LOAD_CONST argval=- create_instruction DELETE_SUBSCR load __nested_resume_fns __nested_frame_values create_instruction LOAD_FAST argval= __nested_resume_fns create_instruction LOAD_FAST argval= __nested_frame_values create_instruction BUILD_LIST arg= load __nested_frame_values - create_instruction LOAD_FAST argval= __nested_frame_values create_instruction LOAD_CONST argval=- create_binary_subscr create __nested_resume_fns __nested_frame_values __nested_frame_values - create_instruction LIST_EXTEND arg= del __nested_frame_values - create_instruction LOAD_FAST argval= __nested_frame_values create_instruction LOAD_CONST argval=- create_instruction DELETE_SUBSCR delete __nested values create_instruction DELETE_FAST argval= __nested_resume_fns create_instruction DELETE_FAST argval= __nested_frame_values Set is_tracing_resume_prologue back allow graph breaks nested resume create_instruction LOAD_CONST argval=False create_instruction STORE_FAST argval=IS_TRACING_RESUME_PROLOGUE_VARNAME finish call create_call_function_ex False False Set is_tracing_resume_prologue back allow graph breaks after jump prefix extend create_instruction LOAD_CONST argval=False create_instruction STORE_FAST argval=IS_TRACING_RESUME_PROLOGUE_VARNAME prefix append create_jump_absolute target because line number table monotonically increases co_firstlineno remove starts_line any instructions before graph break instruction will ensure instructions after break have correct line numbers inst instructions inst offset == target offset break inst starts_line = None sys version_info = inst positions = None cleanup prefix extend cleanup prefix extend cls unreachable_codes code_options remap original instructions exception table entries old_hook_target_remap pyrefly ignore unbound-name assert is_py _plus inst instructions inst exn_tab_entry inst exn_tab_entry target old_hook_target_remap inst exn_tab_entry target = old_hook_target_remap type ignore assignment inst exn_tab_entry target TODO jansel add dead code elimination here instructions = prefix + instructions new_code _ = transform_code_object code update ContinueExecutionCache generated_code_metadata new_code = meta new_code staticmethod unreachable_codes code_options dict str Any - list Instruction Codegen ` raise None ` make analysis work unreachable code create_load_const None create_instruction RAISE_VARARGS arg= classmethod generate_based_on_original_code_object cls code types CodeType lineno int init_offset int resume_offset int setup_fn_target_offsets tuple int args Any - types CodeType This handles case generating resume into code generated resume something We want always generate starting original code object so control flow paths converge we only generated resume function rather than ^n resume functions meta ResumeFunctionMetadata = ContinueExecutionCache generated_code_metadata code find_orig_offset cur_offset int - int orig_offset = - find_orig_offset_transform instructions list Instruction code_options dict str Any - None nonlocal orig_offset target = i i instructions i offset == cur_offset match functions starting last instruction we have added prefix new_target_tuple = tuple i i i zip reversed instructions reversed meta instructions i target new_target_tuple Instruction cur_offset instructions found original code - orig_offset left - Caller expected handle case assert len new_target_tuple == new_target = new_target_tuple assert target opcode == new_target opcode assert new_target offset None orig_offset = new_target offset transform_code_object code find_orig_offset_transform orig_offset orig_init_offset = find_orig_offset init_offset It fine initial instruction found original code means we graph broke prefix which only happens nested graph breaks We should running into ambiguous graph break issues here orig_resume_offset = find_orig_offset resume_offset assert orig_resume_offset - resume instruction found original code - bug sys version_info = setup_fn_target_offsets currently contains target offset each setup_fn based ` code ` When we codegen resume function based original code object ` meta code ` offsets setup_fn_target_offsets must based ` meta code ` instead offset_key = orig_init_offset orig_resume_offset NOTE we key offset_key since same resume function may graph break multiple places we need different block_target_offset_remap s each graph break location Keying orig_resume_offset may enough graph breaks different initial offsets resume same instruction although rare tested anywhere offset_key meta block_target_offset_remap block_target_offset_remap = meta block_target_offset_remap offset_key = remap_block_offsets instructions list Instruction code_options dict str Any - None NOTE each prefix block generates exactly one PUSH_EXC_INFO so we can tell which block prefix PUSH_EXC_INFO belongs counting Then we can use meta prefix_block_target_offset_remap determine where original code PUSH_EXC_INFO offset replaced prefix_blocks list Instruction = inst instructions NOTE meta prefix_block_target_offset_remap based off how we codegen d context managers prefix prologue resume function It same every graph break same resume function so we do need recompute each graph break unlike meta block_target_offset_remap len prefix_blocks == len meta prefix_block_target_offset_remap break inst opname == PUSH_EXC_INFO prefix_blocks append inst remap block target offsets blocks generated resume prefix inst o zip prefix_blocks meta prefix_block_target_offset_remap block_target_offset_remap cast int inst offset = o current bytecode targets after prefix PUSH_EXC_INFO s cur_start_offset = cast int prefix_blocks - offset prefix_blocks - get remaining block target offsets current bytecode cur_inst_offsets = sorted n n setup_fn_target_offsets n cur_start_offset targets = _filter_iter instructions cur_inst_offsets lambda inst o inst offset == o The original code resume code should have matching suffixes Match post-prefix block target offsets current resume code original code orig_targets = reversed _filter_iter zip reversed instructions reversed meta instructions reversed targets lambda v v v v orig cur zip orig_targets targets block_target_offset_remap cur offset = orig offset transform_code_object code remap_block_offsets offset_key offset setup_fn_target_offsets error needs fixed setup_fn_target_offsets = tuple meta block_target_offset_remap offset_key n n setup_fn_target_offsets ContinueExecutionCache lookup meta code lineno orig_init_offset orig_resume_offset setup_fn_target_offsets args