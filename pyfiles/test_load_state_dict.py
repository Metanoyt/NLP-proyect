Owner s module nn re unittest copy deepcopy itertools product torch torch nn nn torch testing _internal common_nn NNTestCase torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests skipIfCrossRef skipIfTorchDynamo swap TEST_NUMPY TestCase torch utils _pytree tree_map TEST_NUMPY numpy np TestLoadStateDict NNTestCase _do_cuda_memory_leak_check = True _do_cuda_non_default_stream = True unittest skipIf TEST_NUMPY numpy found swap True False test_load_state_dict_invalid m = torch nn Linear bias=False state_dict = weight np random randn assertRaisesRegex RuntimeError expected torch Tensor Tensor-like object checkpoint received m load_state_dict state_dict state_dict = weight assertRaisesRegex RuntimeError expected torch Tensor Tensor-like object checkpoint received m load_state_dict state_dict swap True False test_load_state_dict_type m = nn Module assertRaisesRegex TypeError Expected state_dict dict-like got m load_state_dict assertRaisesRegex TypeError Expected state_dict dict-like got m load_state_dict swap True False skipIfTorchDynamo dynamo installs weakrefs some params test_load_state_dict l = nn Linear block = nn Module block conv = nn Conv d bias=True block conv = nn Conv d bias=False net = nn Module net linear = l net linear = l net bn = nn BatchNorm d net block = block net add_module empty None conv _bias_dtype = block conv bias dtype state_dict = net state_dict state_dict update linear weight torch ones block conv bias torch arange dtype=conv _bias_dtype bn running_mean torch randn Also test DDP state_dict can loaded local model ddp_state_dict = net state_dict ddp_state_dict update module linear weight torch ones module block conv bias torch arange dtype=conv _bias_dtype module bn running_mean torch randn torch nn modules utils consume_prefix_in_state_dict_if_present ddp_state_dict module sd state_dict ddp_state_dict incompatible_keys = net load_state_dict sd assertEqual len incompatible_keys missing_keys assertEqual len incompatible_keys unexpected_keys assertNotIn Incompatible str incompatible_keys assertEqual net linear weight sd linear weight assertEqual net block conv bias sd block conv bias assertEqual net bn running_mean sd bn running_mean state_dict = net state_dict state_dict update extra torch ones assertRaises RuntimeError lambda net load_state_dict state_dict incompatible_keys = net load_state_dict state_dict strict=False assertEqual len incompatible_keys missing_keys assertEqual len incompatible_keys unexpected_keys assertIn extra incompatible_keys unexpected_keys assertIn Incompatible str incompatible_keys state_dict = net state_dict state_dict update extra param torch ones assertRaises RuntimeError lambda net load_state_dict state_dict incompatible_keys = net load_state_dict state_dict strict=False assertEqual len incompatible_keys missing_keys assertEqual len incompatible_keys unexpected_keys assertIn extra param incompatible_keys unexpected_keys state_dict = net state_dict del state_dict linear weight assertRaises RuntimeError lambda net load_state_dict state_dict incompatible_keys = net load_state_dict state_dict strict=False assertEqual len incompatible_keys missing_keys assertEqual len incompatible_keys unexpected_keys assertIn linear weight incompatible_keys missing_keys state_dict update extra param torch ones assertRaises RuntimeError lambda net load_state_dict state_dict incompatible_keys = net load_state_dict state_dict strict=False assertEqual len incompatible_keys missing_keys assertEqual len incompatible_keys unexpected_keys assertIn linear weight incompatible_keys missing_keys assertIn extra param incompatible_keys unexpected_keys state_dict = net state_dict state_dict update bn running_mean torch rand wrong size assertRaises RuntimeError lambda net load_state_dict state_dict assertRaises RuntimeError lambda net load_state_dict state_dict strict=False state_dict = net state_dict old_state_dict = deepcopy state_dict state_dict = linear weight torch ones block conv bias torch arange dtype=conv _bias_dtype bn running_mean torch randn nonexistent_key torch rand net load_state_dict state_dict strict=False assertEqual net linear weight state_dict linear weight assertEqual net block conv bias state_dict block conv bias assertEqual net bn running_mean state_dict bn running_mean new_state_dict = net state_dict del old_state_dict linear weight del old_state_dict block conv bias del old_state_dict bn running_mean k v old_state_dict items assertTrue v equal new_state_dict k swap True False test_load_state_dict_BC BatchNormNd Added num_batches_tracked buffer version For state dict earlier versions no versions should provide default value bn = nn BatchNorm d state_dict = bn state_dict del state_dict num_batches_tracked state_dict _metadata version = version bn load_state_dict state_dict assertEqual bn num_batches_tracked dtype torch long assertEqual bn num_batches_tracked item del state_dict _metadata version no version bn load_state_dict state_dict assertEqual bn num_batches_tracked dtype torch long assertEqual bn num_batches_tracked item swap True False test_load_state_dict_child base_module = nn Linear model = base_module _ range model = nn Sequential deepcopy model _ range hook_fn module state_dict prefix local_metadata strict missing_keys unexpected_keys error_msgs module_state_dict = module state_dict assertEqual len module_state_dict keys len state_dict keys model register_load_state_dict_pre_hook hook_fn model load_state_dict model state_dict strict=True fails swapping LSTM installs weak references parameters swap False skipIfTorchDynamo TorchDynamo fails here unknown reasons test_load_state_dict_ref_cycle load_state_dict shouldn t cause reference cycle involving Tensors gc m = torch nn LSTM bidirectional=True gc collect m load_state_dict deepcopy m state_dict refcycles = gc collect assertEqual refcycles swap True False test_load_state_dict_custom CustomState nn Module __init__ - None super __init__ param = torch nn Parameter torch ones sub = torch nn Linear _save_to_state_dict destination prefix keep_vars destination prefix + serialized = param data + _load_from_state_dict state_dict prefix local_metadata strict missing_keys unexpected_keys error_msgs skip some error handling param data copy_ state_dict prefix + serialized - use sequential verify nesting m = nn Sequential CustomState torch no_grad m param = m sub weight = state_dict = m state_dict assertEqual state_dict serialized item assertIn sub weight state_dict assertNotIn param state_dict del m mm = nn Sequential CustomState assertEqual mm param item mm load_state_dict state_dict assertEqual mm param item assertEqual mm sub weight item swap True False parametrize keep_vars True False test_load_state_dict_assign_meta keep_vars MyModule torch nn Module __init__ - None super __init__ fc = nn Linear bn = nn BatchNorm d x = nn Parameter torch rand requires_grad=False forward input x + bn fc input swap = torch __future__ get_swap_module_params_on_conversion net = MyModule state_dict = net state_dict keep_vars=keep_vars v state_dict values v requires_grad_ False torch device meta net_meta = MyModule net_meta_state_dict_old = net_meta state_dict keep_vars=True net_meta load_state_dict state_dict assign=True Make sure parameters persistent buffers assigned net_meta_state_dict = net_meta state_dict keep_vars=True key state_dict keys key net_meta _parameters keep_vars swap state_dict key nn Parameter assertTrue state_dict key net_meta_state_dict key swap assertTrue net_meta_state_dict key net_meta_state_dict_old key state_dict key nn Parameter so will detached when wrapping Parameter assertTrue net_meta_state_dict key net_meta_state_dict_old key assertEqual net_meta_state_dict_old key requires_grad net_meta_state_dict key requires_grad assertEqual net_meta_state_dict_old key requires_grad net_meta_state_dict key requires_grad assertEqual state_dict key net_meta_state_dict key key net_meta _buffers key net_meta _non_persistent_buffers_set assertTrue state_dict key net_meta_state_dict key assertEqual state_dict key net_meta_state_dict key Make sure ordering parameters buffers preserved net_named_parameters = net named_parameters net_named_buffers = net named_buffers net_meta_named_parameters = net_meta named_parameters net_meta_named_buffers = net_meta named_buffers n _ n _ zip net_named_parameters net_meta_named_parameters assertEqual n n n _ n _ zip net_named_buffers net_meta_named_buffers assertEqual n n Make sure outputs same t = torch randn out_net = net t out_net_meta = net_meta t clone assertEqual out_net out_net_meta swap True False test_load_state_dict_assign_with_optimizer MyModule torch nn Module __init__ - None super __init__ fc = nn Linear bn = nn BatchNorm d forward input bn fc input net = MyModule opt = torch optim Adam net parameters lr= x = torch randn num_iters = _ range num_iters opt zero_grad out = net x out sum backward opt step opt_state_dict = deepcopy opt state_dict net_state_dict = deepcopy net state_dict torch device meta net_meta = MyModule net_meta load_state_dict net_state_dict assign=True must create optimizer only after loading state_dict when assign=True opt = torch optim Adam net_meta parameters lr= opt load_state_dict opt_state_dict y = x clone _ range num_iters opt zero_grad out = net x out sum backward opt step opt zero_grad out = net_meta y out sum backward opt step assertEqual opt state_dict opt state_dict assertEqual net state_dict net_meta state_dict swap True False test_load_state_dict_assign_shape_stride Assigned tensor allowed have different properties than initial tensor except shape MyModule torch nn Module __init__ - None super __init__ fc = nn Linear bn = nn BatchNorm d forward input bn fc input net = MyModule state_dict = net state_dict loading should ok stride different state_dict fc weight = torch randn transpose net = MyModule net load_state_dict state_dict strict=False assign=True state_dict fc weight = torch randn assertRaisesRegex RuntimeError size mismatch fc weight copying param shape net load_state_dict state_dict strict=False assign=True swap True False test_load_state_dict_warn_assign torch device meta m = torch nn Linear state_dict = m state_dict state_dict weight = torch empty_like state_dict weight device= cpu assertWarnsRegex UserWarning weight copying non-meta parameter checkpoint meta m load_state_dict state_dict swap True False test_load_state_dict_with_unexpected_key MyModule torch nn Module __init__ - None super __init__ fc = torch nn Linear m = MyModule Unexpected key strict = True assertRaisesRegex RuntimeError Unexpected key state_dict = m state_dict state_dict fc bad_suffix = torch randn m load_state_dict state_dict Unexpected key strict = False state_dict = m load_state_dict state_dict strict=False assertIn fc bad_suffix state_dict unexpected_keys Unexpected key whose prefix matches valid key strict = True assertRaisesRegex RuntimeError Unexpected key state_dict = m state_dict state_dict fc weight bad_suffix = torch randn m load_state_dict state_dict Unexpected key whose prefix matches valid key strict = False state_dict = m load_state_dict state_dict strict=False assertIn fc weight bad_suffix state_dict unexpected_keys load_torch_function_handler cls func types args= kwargs=None kwargs = kwargs None kwargs module_load dest src assign=False isinstance dest cls assign src detach type src torch Tensor cls src type src cls src detach isinstance src MyWrapperLoadTensor cls src _data cls src assert isinstance src cls f Expected isinstance src cls got type src assert type dest torch Tensor type dest torch nn Parameter issubclass cls type dest assign src detach isinstance src MyWrapperLoadTensor type dest torch Tensor torch nn Parameter type dest src _data src _data detach torch Tensor src func torch Tensor module_load module_load args kwargs torch _C DisableTorchFunctionSubclass detach must instance same subclass nn Parameter func == torch Tensor detach ret = func args kwargs isinstance ret cls cls ret ret func args kwargs MyLoadTensor torch Tensor classmethod __torch_function__ cls func types args= kwargs=None load_torch_function_handler cls func types args kwargs We use MyLoadTensor test tensor subclass wrapper tensor subclass where neither inherits each other MyLoadTensor torch Tensor classmethod __torch_function__ cls func types args= kwargs=None load_torch_function_handler cls func types args kwargs MyBrokenLoadTensor torch Tensor classmethod __torch_function__ cls func types args= kwargs=None kwargs = kwargs None kwargs func torch Tensor module_load wrong doesn t detach args torch _C DisableTorchFunctionSubclass detach must instance same subclass nn Parameter func == torch Tensor detach cls func args kwargs func args kwargs MyWrapperLoadTensor MyLoadTensor staticmethod __new__ cls data torch Tensor t = torch Tensor _make_wrapper_subclass cls data size dtype=data dtype layout=data layout device=data device requires_grad=data requires_grad strides=data stride storage_offset=data storage_offset t __init__ data torch Tensor _data = data __repr__ f MyWrapperLoadTensor _data __repr__ classmethod __torch_dispatch__ cls func types args= kwargs=None unwrap t t _data isinstance t MyWrapperLoadTensor t wrap t MyWrapperLoadTensor t isinstance t torch Tensor t kwargs = kwargs None kwargs out = func tree_map unwrap args tree_map unwrap kwargs tree_map wrap out TestLoadStateDictSwap TestCase skipIfCrossRef skipIfTorchDynamo Can t swap dynamo dynamo installs weakrefs swap True parametrize assign True False test_swap_subclass assign _create_model subclass=None m = torch nn Linear bias=False m buf = torch nn Buffer torch randn subclass None m weight = torch nn Parameter subclass m weight m buf = subclass m buf m _test m_subclass=None sd_subclass=None m = _create_model m_subclass sd = _create_model sd_subclass state_dict m load_state_dict sd assign=assign assertEqual m weight sd weight assertEqual m buf sd buf assertTrue isinstance m weight torch nn Parameter assertTrue isinstance m buf torch nn Parameter weight_type buf_type = torch nn Parameter torch Tensor assign sd_subclass None weight_type buf_type = sd_subclass sd_subclass m_subclass None weight_type buf_type = m_subclass m_subclass assertTrue type m weight weight_type assertTrue type m buf buf_type MyLoadTensor MyWrapperLoadTensor tests behavior superclass subclass subclasses = None MyLoadTensor MyLoadTensor MyWrapperLoadTensor m_s sd_s product subclasses subclasses _test m_s sd_s MyBrokenLoadTensor should error since its module_load doesn t call detach assertRaisesRegex RuntimeError re escape Error s loading state_dict Linear _test None MyBrokenLoadTensor instantiate_parametrized_tests TestLoadStateDict instantiate_parametrized_tests TestLoadStateDictSwap __name__ == __main__ TestCase _default_dtype_check_enabled = True run_tests