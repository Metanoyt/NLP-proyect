mypy allow-untyped-defs copy logging operator time collections defaultdict collections abc Iterable enum Enum typing Any cast Optional torch torch fx fx torch nn nn torch nn functional F torch utils mkldnn th_mkldnn torch fx node Argument Target torch fx passes shape_prop ShapeProp torch nn utils fusion fuse_conv_bn_eval fuse_linear_bn_eval __all__ = matches_module_pattern replace_node_module fuse remove_dropout extract_subgraph modules_to_mkldnn reset_modules MklSubgraph gen_mkl_autotuner use_mkl_length UnionFind optimize_for_inference _parent_name target str - tuple str str Splits qualname into parent path last atom For example ` foo bar baz ` - ` foo bar ` ` baz ` parent name = target rsplit parent parent name Works length patterns modules matches_module_pattern pattern Iterable type node fx Node modules dict str Any len node args == False nodes tuple Any fx Node = node args node expected_type current_node zip pattern nodes isinstance current_node fx Node False current_node op = call_module False isinstance current_node target str False current_node target modules False type modules current_node target expected_type False True replace_node_module node fx Node modules dict str Any new_module torch nn Module assert isinstance node target str parent_name name = _parent_name node target modules node target = new_module setattr modules parent_name name new_module fuse model torch nn Module inplace=False no_trace=False - torch nn Module Fuses convolution BN linear BN layers inference purposes Will deepcopy your model default can modify model inplace well patterns = nn Conv d nn BatchNorm d nn Conv d nn BatchNorm d nn Conv d nn BatchNorm d nn Linear nn BatchNorm d inplace model = copy deepcopy model no_trace isinstance model torch fx GraphModule fx_model = fx symbolic_trace model fx_model = model modules = dict fx_model named_modules new_graph = copy deepcopy fx_model graph pattern patterns node new_graph nodes matches_module_pattern pattern node modules len node args users Output conv linear used other nodes continue first_layer = modules node args target bn = modules node target bn track_running_stats continue pattern nn Conv d nn Conv d nn Conv d fused_layer = fuse_conv_bn_eval first_layer bn nn Linear fused_layer = fuse_linear_bn_eval first_layer bn replace_node_module node args modules fused_layer node replace_all_uses_with node args new_graph erase_node node fx GraphModule fx_model new_graph remove_dropout model nn Module - nn Module Removes all dropout layers module fx_model = fx symbolic_trace model DropoutRemover torch fx Transformer call_module target Target args tuple Argument kwargs dict str Any - Any isinstance submodules target nn Dropout assert len args == args super call_module target args kwargs DropoutRemover fx_model transform extract_subgraph orig_module nn Module nodes list fx Node inputs list fx Node outputs list fx Node Given lists nodes existing graph represent subgraph returns submodule executes subgraph new_graph = fx Graph env dict fx Node fx Node = input inputs new_node = new_graph placeholder input name env input = new_node node nodes new_node = new_graph node_copy node lambda x env x env node = new_node new_graph output env output output outputs new_graph lint fx GraphModule orig_module new_graph mkldnn_supported = nn Conv d nn Linear nn BatchNorm d nn ReLU nn MaxPool d nn AvgPool d nn AdaptiveAvgPool d torch relu torch transpose torch sigmoid F relu F avg_pool d F adaptive_avg_pool d These operators may convertible into MKLDNN ops e g args scalar values Thus we only include them subgraph their arguments already MKLDNN TODO Determine whether can removed after type inference mkldnn_supported_unknown = operator add operator mul mkldnn_map = nn Conv d th_mkldnn MkldnnConv d nn Linear th_mkldnn MkldnnLinear nn BatchNorm d lambda _ th_mkldnn MkldnnBatchNorm modules_to_mkldnn nodes list fx Node modules dict str nn Module For each node s module can preconverted into MKLDNN then we do so create mapping allow us convert MKLDNN version module original old_modules dict nn Module nn Module = node nodes node op == call_module assert isinstance node target str cur_module = modules node target type cur_module mkldnn_map pyrefly ignore index-error new_module = mkldnn_map type cur_module cur_module torch float assert isinstance new_module nn Module old_modules new_module = copy deepcopy cur_module replace_node_module node modules new_module old_modules reset_modules nodes list fx Node modules dict str nn Module old_modules dict nn Module nn Module Maps each module s been changed ` modules_to_mkldnn ` back its original node nodes node op == call_module assert isinstance node target str cur_module = modules node target cur_module old_modules replace_node_module node modules old_modules cur_module MklSubgraph __init__ fx_graph fx Graph fx_graph = fx_graph nodes list fx Node = start_nodes list fx Node = end_nodes list fx Node = gen_mkl_autotuner example_inputs iters= warmup= This generates heuristic can passed into ` optimize_for_inference ` determines whether subgraph should run MKL running example_inputs Example usage heuristic = gen_mkl_autotuner example_inputs iters= fast_model = optimization optimize_for_inference model heuristic fx_model = None old_modules = None use_mkl_heuristic graph MklSubgraph - bool nonlocal fx_model old_modules input_nodes = graph start_nodes fx_model None fx_model = graph fx_graph owning_module old_modules = graph fx_graph old_modules type ignore attr-defined ShapeProp fx_model propagate example_inputs sample_inputs = torch randn node shape node input_nodes type ignore attr-defined output_args = cast list fx Node node args node graph end_nodes submodule = extract_subgraph fx_model graph nodes input_nodes output_args benchmark f _ range warmup f begin = time time _ range iters f time time - begin mkl_time = benchmark lambda i to_dense i submodule i to_mkldnn i sample_inputs reset_modules submodule graph nodes dict submodule named_modules pyrefly ignore bad-argument-type old_modules no_mkl_time = benchmark lambda submodule sample_inputs mkl_time no_mkl_time use_mkl_heuristic use_mkl_length graph MklSubgraph - bool This heuristic can passed into ` optimize_for_inference ` determines whether subgraph should run MKL checking there more than nodes len graph nodes UnionFind __init__ n parent list Optional int = None n size list int = n make_set v int parent v = v size v = find v int - int par = parent v v == par v assert par None parent v = find par cast int parent v join int b int b = find find b == b size size b b = b parent b = size += size b optimize_for_inference model torch nn Module pass_config Optional dict str Any = None tracer type fx Tracer = fx Tracer - torch nn Module Performs set optimization passes optimize model purposes inference Specifically passes run Conv BN fusion Dropout removal MKL layout optimizations The third optimization takes function ` use_mkl_heuristic ` s used determine whether subgraph should explicitly run MKL layout Note As FX does currently handle aliasing pass currently assumes nothing aliases If isn t true use your own risk default_pass_config = conv_bn_fuse True remove_dropout True mkldnn_layout_optimize heuristic use_mkl_length pass_config None pass_config = default_pass_config update pass_config default_pass_config conv_bn_fuse model = fuse model default_pass_config remove_dropout model = remove_dropout model default_pass_config mkldnn_layout_optimize False model isinstance default_pass_config mkldnn_layout_optimize dict raise RuntimeError mkldnn_layout_optimize config dict heuristic default_pass_config mkldnn_layout_optimize raise RuntimeError Heuristic found mkldnn_layout_optimize config use_mkl_heuristic = default_pass_config mkldnn_layout_optimize heuristic cur_tracer = tracer fx_graph = cur_tracer trace copy deepcopy model fx GraphModule cur_tracer root fx_graph modules dict str nn Module = dict model named_modules MklSupport Enum NO = YES = UNKNOWN = Inserts to_mkldnn to_dense around every node we want MKLDNN node If op ` mkldnn_supported ` then we always treat MKLDNN node However s ` mkldnn_supported_unknown ` then we only treat MKLDNN node its inputs MKLDNN nodes node list fx_graph nodes supports_mkldnn = MklSupport NO node op == call_module cur_module = modules node target type cur_module mkldnn_supported supports_mkldnn = MklSupport YES sample_parameter = next cur_module parameters None sample_parameter None assert sample_parameter dtype == torch float pass only torch float modules assert sample_parameter device == torch device cpu pass only CPU modules node op == call_function node target mkldnn_supported supports_mkldnn = MklSupport YES node target mkldnn_supported_unknown supports_mkldnn = MklSupport UNKNOWN supports_mkldnn = MklSupport NO supports_mkldnn == MklSupport UNKNOWN any arg target == to_dense arg node args continue fx_graph inserting_before node mkldnn_args = fx map_arg node args lambda n fx_graph call_method to_mkldnn n node args = cast tuple fx node Argument mkldnn_args fx_graph inserting_after node dense_x = fx_graph create_node call_method to_dense node node replace_all_uses_with dense_x dense_x args = node Does pre-conversion all modules into MKLDNN when possible old_modules = modules_to_mkldnn list fx_graph nodes modules fx_graph old_modules = old_modules type ignore attr-defined optimizes all - to_dense - to_mkldnn - b patterns into - b node fx_graph nodes node op == call_method node target == to_dense prv_node = node args users = list node users user users user op == call_method user target == to_mkldnn user replace_all_uses_with prv_node fx_graph erase_node user len node users == fx_graph erase_node node num_nodes = len fx_graph nodes uf = UnionFind num_nodes get_color n hasattr n color Current node part MKL subgraph uf find n color hasattr n start_color Current node input MKL subgraph uf find n start_color None This code find each MKLDNN subgraph Each MKLDNN subgraph consists input nodes which only ` to_mkldnn ` calls output nodes ` to_dense ` calls intermediate nodes which run entirely MKLDNN layout tensors Specifically code does flood fill directed acyclic graph DAG starting each possible start node i e ` to_mkldnn ` nodes If every node only had one input would sufficient However case node has multiple inputs coming different start nodes i e colors we need join these colors into That s done using Disjoint Set Union cur_idx node enumerate fx_graph nodes node op == call_method node target == to_mkldnn node start_color = cur_idx uf make_set cur_idx node op == call_method node target == to_dense assert get_color node args None node end_color = get_color node args cur_colors = get_color i i node all_input_nodes isinstance i fx Node get_color i None len cur_colors == continue assert any i None i cur_colors cur_colors = sorted cur_colors node color = cur_colors other_color cur_colors uf join cur_colors other_color mkldnn_graphs dict int MklSubgraph = defaultdict lambda MklSubgraph fx_graph node fx_graph nodes hasattr node color mkldnn_graphs uf find node color nodes append node hasattr node start_color mkldnn_graphs uf find node start_color start_nodes append node hasattr node end_color mkldnn_graphs uf find node end_color end_nodes append node Now we have all subgraphs we need decide which MKLDNN subgraphs we actually want keep MKLDNN graph mkldnn_graphs values use_mkl_heuristic graph node graph start_nodes + graph end_nodes prv = node args node replace_all_uses_with prv type ignore arg-type fx_graph erase_node node reset_modules graph nodes modules old_modules mkldnn_conversions = node fx_graph nodes node target == to_mkldnn node target == to_dense mkldnn_conversions += logging getLogger __name__ info mkldnn conversions s mkldnn_conversions fx_graph lint result = fx GraphModule model fx_graph result