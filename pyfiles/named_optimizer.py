logging warnings collections abc Callable Collection Mapping copy deepcopy typing Any Optional overload Union torch torch nn nn torch optim torch distributed _shard sharded_tensor ShardedTensor torch distributed fsdp FullyShardedDataParallel FSDP __all__ list str = logger = logging getLogger __name__ _NamedOptimizer optim Optimizer ` ` _NamedOptimizer ` ` takes dict parameters exposes ` ` state_dict ` ` parameter key We replace original key number optim fully qualified name FQN string User can initialize optim they initialize PyTorch optim only difference they also need pass FQN each parameters Args named_parameters Mapping str Union torch Tensor ShardedTensor Mapping FQN parameter optimizer_class optim Optimizer The optimizer instantiate param_groups Collection Mapping str Any ` param_groups ` pass optimizer specified The key inner map needs FQNs Default None module nn Module module whose parameters updated optimizer args arguments pass optimizer constructor kwargs arguments pass optimizer constructor Example xdoctest +SKIP distributed torch optim torch distributed optim _NamedOptimizer Define named optimizer m = Model named_optim = _NamedOptimizer m named_parameters optim SGD Forward pass + backward pass named_optim step Call state_dict named optimizer returns FQN state_dict named_optim state_dict Warning This API still development subject change TODO Add tutorial _NamedOptimizer TODO Add documentation docstring public attributes like param_groups named_parameters __init__ named_parameters Mapping str Union torch Tensor ShardedTensor optimizer_class optim Optimizer param_groups Optional Collection Mapping str Any = None module Optional nn Module = None args tuple Any kwargs dict str Any - None torch _C _log_api_usage_once torch distributed optim _NamedOptimizer param_groups Collection Mapping str Any = param_groups type ignore assignment _param_groups_check named_parameters = dict named_parameters params_for_optimizer = named_parameters values param_groups None param_groups _optimizer = optimizer_class type ignore operator params_for_optimizer args kwargs module = module param_groups None ordered_param_keys = list named_parameters keys warnings warn Since we pass param_groups we will use param_groups initialize optimizer all parameters module stacklevel= param_to_key = param key key param named_parameters items type ignore misc has-type ordered_param_keys = group param_groups param group params param param_to_key raise ValueError f Expect param name param found param group missing ordered_param_keys append param_to_key param ordered_param_keys = ordered_param_keys Update param_groups optimizer param_groups = _optimizer param_groups _param_groups_check - None param_groups None param_group param_groups assert isinstance param_group dict param group must dict assert params param_group param group must contain key params params = param_group params isinstance params torch Tensor params = params params = list params param params isinstance param torch Tensor raise TypeError optimizer can only optimize Tensors one params + torch typename param param_group params = params state_dict - dict str Any Return ` ` state_dict ` ` optimizer Instead using number index parameters we will use module fully qualified name FQN key state_dict = _optimizer state_dict param_groups = state_dict param_groups ret_state = ordered_param_keys st_key state_val st_key state_val state_dict state items ret_groups = group param_groups param_keys = ordered_param_keys param param group params ret_group = params sorted param_keys k v group items k = params ret_group k = deepcopy v ret_groups append ret_group _post_state_dict state ret_state param_groups ret_groups overload step closure None = None - None overload step closure Callable float - float step closure Optional Callable float = None - Optional float Perform single optimization step This will call meth ` torch optim Optimizer step ` wrapped optimizer _optimizer step closure=closure property state - Mapping torch Tensor Any type ignore override _optimizer state load_state_dict state_dict dict str Any - None Define default behavior load state_dict ` ` _NamedOptimizer ` ` Sample Code ` ` ` my_model = MyModule optimizer = _NamedOptimizer my_model named_parameters Adagrad optim_state_dict = optimizer state_dict optimizer load_state_dict optim_state_dict ` ` ` Args state_dict dict str Any A ` ` state_dict ` ` load into optimizer Note state dict update performed place note PyTorch using lazy init initialize optim states So possible there no optim state when user call ` ` load_state_dict ` ` ` ` _NamedOptimizer ` ` we make stricter users can only call ` ` load_state_dict ` ` after state initialized By doing we can validate optim ` ` state_dict ` ` loaded new_state_dict = _optimizer state_dict state_dict = _pre_load_state_dict state_dict state = state_dict state new_state = new_state_dict state len new_state == raise ValueError Expects optim initialized before load found initialized idx param_key enumerate ordered_param_keys When conditional training performed all parameters updated optim param_key state keys continue len state param_key = len new_state idx raise ValueError f Expects equal length len new_state idx parameter param_key found len state param_key Iterate through all optimizer states state_key state_val new_state idx items state_key state param_key raise ValueError f Expects state state_key parameter param_key found src_state_val = state param_key state_key isinstance state_val ShardedTensor assert isinstance src_state_val ShardedTensor num_shards = len state_val local_shards num_new_shards = len src_state_val local_shards num_shards = num_new_shards raise ValueError f Expects equal number shards num_new_shards found num_shards param_key state_key shard src_shard zip state_val local_shards src_state_val local_shards shard tensor detach copy_ src_shard tensor isinstance state_val torch Tensor assert isinstance src_state_val torch Tensor state_val detach copy_ src_state_val new_state idx state_key = deepcopy src_state_val Load param_groups state_dict src_param_groups = state_dict param_groups new_param_groups = new_state_dict param_groups src_group_map = group src_param_groups param_keys = list group params src_group_map _gen_param_group_key param_keys = group new_group_map = new_group new_param_groups param_keys = param_key new_group params param_keys append ordered_param_keys param_key type ignore call-overload new_group_map _gen_param_group_key param_keys = new_group group_key new_group new_group_map items When all parameters used training receive gradient aka all parameters would param_group Thus we skip group_key here group_key src_group_map continue src_group = src_group_map group_key len src_group = len new_group raise ValueError f Expects equal param_group size len new_group group group_key found len src_group k src_group k new_group raise ValueError f Expects group key k group group_key ` state_dict ` missing k = params new_group k = deepcopy src_group k _optimizer load_state_dict new_state_dict add_param_group param_group Mapping str Any - None Add param group ` _NamedOptimizer ` s ` param_groups ` Warning This API still development subject change assert isinstance param_group dict param group must dict params = param_group params isinstance params torch Tensor param_group params = params param_group params = list params param_to_key = param key key param named_parameters items type ignore misc has-type param param_group params param param_to_key raise ValueError some parameters module ordered_param_keys append param_to_key param _optimizer add_param_group param_group Update param_groups optimizer param_groups = _optimizer param_groups init_state - None Run dummy optimizer step which allows initialize optimizer state because we do lazy init most optimizers This allows doing in-place loading optimizer state checkpoint param named_parameters values param requires_grad t = torch zeros_like param param grad = torch autograd Variable t Calling ` ` step ` ` will load initial state optimizer states step closure=None _pre_load_state_dict state_dict dict str Any - dict str Any TODO chienchin This API should FSDP agnostic should support general user hooks isinstance module FSDP FSDP optim_state_dict_to_load module _optimizer state_dict is_named_optimizer=True state_dict _post_state_dict state_dict dict str Any - dict str Any TODO chienchin This API should FSDP agnostic should support general user hooks isinstance module FSDP FSDP optim_state_dict module _optimizer state_dict state_dict _gen_param_group_key param_keys list str - str Concatenate all param keys unique identifier one param group join sorted param_keys