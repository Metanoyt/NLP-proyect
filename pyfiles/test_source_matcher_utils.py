Owner s module fx os sys unittest torch pytorch_test_dir = os path dirname os path dirname os path realpath __file__ sys path append pytorch_test_dir torch _dynamo eval_frame is_dynamo_supported torch fx passes tools_common legalize_graph torch fx passes utils source_matcher_utils check_subgraphs_connected get_source_partitions torch testing _internal common_utils instantiate_parametrized_tests parametrize raise_on_run_directly skipIfTorchDynamo torch testing _internal jit_utils JitTestCase TestSourceMatcher JitTestCase unittest skipIf is_dynamo_supported Dynamo supported test_module_partitioner_linear_relu_linear M torch nn Module __init__ - None super __init__ linear = torch nn Linear relu = torch nn ReLU linear = torch nn Linear forward x x = linear x x = linear x x = relu x x = linear x x inputs = torch randn gm _ = torch _dynamo export M aten_graph=True inputs gm graph eliminate_dead_code module_partitions = get_source_partitions gm graph torch nn Linear torch nn ReLU assertEqual len module_partitions assertEqual len module_partitions torch nn Linear assertEqual len module_partitions torch nn ReLU assertFalse check_subgraphs_connected module_partitions torch nn Linear module_partitions torch nn ReLU assertTrue check_subgraphs_connected module_partitions torch nn Linear module_partitions torch nn ReLU assertFalse check_subgraphs_connected module_partitions torch nn Linear module_partitions torch nn ReLU unittest skipIf is_dynamo_supported Dynamo supported test_module_partitioner_conv_relu_maxpool M torch nn Module __init__ constant_tensor torch Tensor - None super __init__ constant_tensor = constant_tensor conv = torch nn Conv d in_channels= out_channels= kernel_size= padding= conv = torch nn Conv d in_channels= out_channels= kernel_size= padding= conv = torch nn Conv d in_channels= out_channels= kernel_size= padding= relu = torch nn ReLU maxpool = torch nn MaxPool d kernel_size= forward x torch Tensor - torch Tensor = conv x b = conv c = + constant_tensor z = conv b + c maxpool relu z inputs = torch randn gm _ = torch _dynamo export M torch ones aten_graph=True inputs gm graph eliminate_dead_code module_partitions = get_source_partitions gm graph torch nn Conv d torch nn ReLU torch nn MaxPool d assertEqual len module_partitions assertEqual len module_partitions torch nn Conv d assertEqual len module_partitions torch nn ReLU assertEqual len module_partitions torch nn MaxPool d assertFalse check_subgraphs_connected module_partitions torch nn Conv d module_partitions torch nn ReLU assertFalse check_subgraphs_connected module_partitions torch nn Conv d module_partitions torch nn ReLU assertTrue check_subgraphs_connected module_partitions torch nn Conv d module_partitions torch nn ReLU assertFalse check_subgraphs_connected module_partitions torch nn MaxPool d module_partitions torch nn ReLU assertTrue check_subgraphs_connected module_partitions torch nn ReLU module_partitions torch nn MaxPool d unittest skipIf is_dynamo_supported Dynamo supported test_module_partitioner_functional_conv_relu_conv FunctionalConv d torch nn Module __init__ - None super __init__ stride = padding = dilation = groups = forward x weight bias torch nn functional conv d x weight bias stride padding dilation groups M torch nn Module __init__ - None super __init__ conv = FunctionalConv d conv = FunctionalConv d forward x weight bias x = conv x weight bias x = torch nn functional relu x x = conv x weight bias x inputs = torch randn torch rand torch rand gm _ = torch _dynamo export M aten_graph=True inputs gm graph eliminate_dead_code module_partitions = get_source_partitions gm graph torch nn functional conv d assertEqual len module_partitions assertEqual len module_partitions torch nn functional conv d unittest skipIf is_dynamo_supported Dynamo supported test_module_partitioner_functional_linear_relu_linear M torch nn Module __init__ - None super __init__ forward x weight bias x = torch nn functional linear x weight bias x = torch nn functional linear x weight bias x = torch nn functional relu x x = torch nn functional linear x weight bias x = torch nn functional linear x weight bias x = torch nn functional relu x x inputs = torch randn torch rand torch zeros gm _ = torch _dynamo export M aten_graph=True inputs gm graph eliminate_dead_code module_partitions = get_source_partitions gm graph torch nn functional linear torch nn functional relu assertEqual len module_partitions assertEqual len module_partitions torch nn functional linear assertEqual len module_partitions torch nn functional relu skipIfTorchDynamo unexplained failure weakref inlining raises dynamic shape error only unittest skipIf is_dynamo_supported Dynamo supported test_legalize_slice M torch nn Module forward x y b = x item torch _check b = torch _check b + y size y b + ep = torch export export M torch tensor torch randn strict=True fake_inputs = node meta val node ep graph nodes node op == placeholder gm = ep module fake_inputs fake_mode torch fx Interpreter gm run fake_inputs legalized_gm = legalize_graph gm fake_inputs fake_mode torch fx Interpreter legalized_gm run fake_inputs unittest skipIf is_dynamo_supported Dynamo supported parametrize strict True False test_module_partitioner_linear_relu_linear_torch_fn_export strict bool M torch nn Module __init__ - None super __init__ linear = torch nn Linear relu = torch nn ReLU linear = torch nn Linear forward x x = linear x x = linear x x = relu x x = linear x x inputs = torch randn gm = torch export export M inputs strict=strict module gm graph eliminate_dead_code Remove source_fn_stack meta let partitioner use torch_fn only TODO remove after we fix torch_fn T node gm graph nodes node meta source_fn_stack = None module_partitions = get_source_partitions gm graph linear relu assertEqual len module_partitions assertEqual len module_partitions linear assertEqual len module_partitions relu assertFalse check_subgraphs_connected module_partitions linear module_partitions relu assertTrue check_subgraphs_connected module_partitions linear module_partitions relu assertFalse check_subgraphs_connected module_partitions linear module_partitions relu unittest skipIf is_dynamo_supported Dynamo supported parametrize strict True False test_module_partitioner_conv_relu_maxpool_torch_fn_export strict bool M torch nn Module __init__ constant_tensor torch Tensor - None super __init__ constant_tensor = constant_tensor conv = torch nn Conv d in_channels= out_channels= kernel_size= padding= conv = torch nn Conv d in_channels= out_channels= kernel_size= padding= conv = torch nn Conv d in_channels= out_channels= kernel_size= padding= relu = torch nn ReLU maxpool = torch nn MaxPool d kernel_size= forward x torch Tensor - torch Tensor = conv x b = conv c = + constant_tensor z = conv b + c maxpool relu z inputs = torch randn gm = torch export export M torch ones inputs strict=strict module gm graph eliminate_dead_code Remove source_fn_stack meta let partitioner use torch_fn only TODO remove after we fix torch_fn T node gm graph nodes node meta source_fn_stack = None module_partitions = get_source_partitions gm graph conv d relu max_pool d assertEqual len module_partitions assertEqual len module_partitions conv d assertEqual len module_partitions relu assertEqual len module_partitions max_pool d assertFalse check_subgraphs_connected module_partitions conv d module_partitions relu assertFalse check_subgraphs_connected module_partitions conv d module_partitions relu assertTrue check_subgraphs_connected module_partitions conv d module_partitions relu assertFalse check_subgraphs_connected module_partitions max_pool d module_partitions relu assertTrue check_subgraphs_connected module_partitions relu module_partitions max_pool d unittest skipIf is_dynamo_supported Dynamo supported parametrize strict True False test_module_partitioner_functional_conv_relu_conv_torch_fn_export strict bool FunctionalConv d torch nn Module __init__ - None super __init__ stride = padding = dilation = groups = forward x weight bias torch nn functional conv d x weight bias stride padding dilation groups M torch nn Module __init__ - None super __init__ conv = FunctionalConv d conv = FunctionalConv d forward x weight bias x = conv x weight bias x = torch nn functional relu x x = conv x weight bias x inputs = torch randn torch rand torch rand gm = torch export export M inputs strict=strict module gm graph eliminate_dead_code Remove source_fn_stack meta let partitioner use torch_fn only TODO remove after we fix torch_fn T node gm graph nodes node meta source_fn_stack = None module_partitions = get_source_partitions gm graph conv d assertEqual len module_partitions assertEqual len module_partitions conv d unittest skipIf is_dynamo_supported Dynamo supported parametrize strict True False test_module_partitioner_functional_linear_relu_linear_torch_fn_export strict bool M torch nn Module __init__ - None super __init__ forward x weight bias x = torch nn functional linear x weight bias x = torch nn functional linear x weight bias x = torch nn functional relu x x = torch nn functional linear x weight bias x = torch nn functional linear x weight bias x = torch nn functional relu x x inputs = torch randn torch rand torch zeros gm = torch export export M inputs strict=strict module gm graph eliminate_dead_code Remove source_fn_stack meta let partitioner use torch_fn only TODO remove after we fix torch_fn T node gm graph nodes node meta source_fn_stack = None module_partitions = get_source_partitions gm graph linear relu assertEqual len module_partitions assertEqual len module_partitions linear assertEqual len module_partitions relu unittest skipIf is_dynamo_supported Dynamo supported parametrize strict True False test_module_partitioner_weight_tied strict bool real-world example https github com pytorch pytorch issues M torch nn Module __init__ input_size output_size super __init__ Define linear layer linear = torch nn Linear input_size output_size tied_weight = linear weight forward x Forward pass through linear layer b = tied_weight + linear x b inputs = torch randn gm = torch export export M input_size= output_size= inputs strict=strict module gm graph eliminate_dead_code k = torch nn Linear strict linear module_partitions = get_source_partitions gm graph k assertEqual len module_partitions assertEqual len module_partitions k assertEqual len module_partitions k output_nodes assertEqual module_partitions k output_nodes name linear input_node_names = node name node module_partitions k input_nodes assertEqual input_node_names x instantiate_parametrized_tests TestSourceMatcher __name__ == __main__ raise_on_run_directly test test_fx py