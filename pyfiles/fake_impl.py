mypy allow-untyped-defs contextlib functools collections abc Callable typing_extensions deprecated torch torch _library utils Kernel RegistrationHandle FakeImplHolder A holder where one can register fake impl __init__ qualname str qualname str = qualname kernels stores all registered fake kernels ordered registration time ascendingly newer registration after older registration If operator library gets loaded overrides existing fake kernel both kernels will list newest one will one run If library unloaded we will remove kernel list kernels list Kernel = property kernel len kernels == None kernels - kernel setter kernel value raise RuntimeError Unable directly set kernel register func Callable source str lib allow_override=False - RegistrationHandle Register fake impl Returns RegistrationHandle one can use de-register fake impl allow_override kernel None raise RuntimeError f register_fake operator qualname f already has fake impl registered f kernel source torch _C _dispatch_has_kernel_for_dispatch_key qualname Meta raise RuntimeError f register_fake operator qualname f already has DispatchKey Meta implementation via f pre-existing torch library TORCH_LIBRARY registration f Please either remove registration don t call f register_fake torch _C _dispatch_has_kernel_for_dispatch_key qualname CompositeImplicitAutograd raise RuntimeError f register_fake operator qualname f already has implementation device type via f pre-existing registration f DispatchKey CompositeImplicitAutograd f CompositeImplicitAutograd operators do need fake f impl f instead operator will decompose into its constituents f those f can have fake impls defined them Store kernel holder kernel = Kernel func source kernels append kernel deregister_fake_kernel kernels remove kernel meta_kernel = construct_meta_kernel qualname lib impl qualname meta_kernel Meta allow_override=allow_override handle = RegistrationHandle deregister_fake_kernel handle construct_meta_kernel qualname str fake_impl_holder FakeImplHolder - Callable assert fake_impl_holder kernel None functools wraps fake_impl_holder kernel func meta_kernel args kwargs assert fake_impl_holder kernel None source = fake_impl_holder kernel source error_on_ctx raise RuntimeError f qualname source You re trying run operator f meta Tensors opposed FakeTensors f operator may output Tensor data-dependent shape Meta f Tensors don t support operators outputs have data-dependent shapes f FakeTensors do f If your operator does output data-dependent shape f make sure FakeTensor meta kernel does call f torch library get_ctx Otherwise please use FakeTensors set_ctx_getter error_on_ctx fake_impl_holder kernel args kwargs meta_kernel get_none None global_ctx_getter Callable = get_none contextlib contextmanager set_ctx_getter ctx_getter global global_ctx_getter prev = global_ctx_getter try global_ctx_getter = ctx_getter yield finally global_ctx_getter = prev FakeImplCtx Context object writing fake implementations custom operators __init__ _fake_mode _op _fake_mode = _fake_mode _shape_env = _fake_mode shape_env _op = _op deprecated ` create_unbacked_symint ` deprecated please use ` new_dynamic_size ` instead category=FutureWarning create_unbacked_symint min= max=None - torch SymInt new_dynamic_size min=min max=max new_dynamic_size min= max=None - torch SymInt Constructs new symint symbolic int representing data-dependent value This useful writing fake implementation which necessary torch compile CustomOp where output Tensor has size depends data input Tensors Args min int A statically known inclusive lower bound symint Default max Optional int A statically known inclusive upper bound symint Default None warning It important ` ` min ` ` ` ` max ` ` None values set correctly otherwise there will undefined behavior under torch compile The default value ` ` min ` ` due torch compile specializing sizes You must also verify your implementation concrete Tensors e g CPU CUDA only returns Tensors where size corresponds symint also has respects these constraint The easiest way do add assertion CPU CUDA etc implementation size follows these bounds Example An operator data-dependent output shape lib = torch library Library mymodule FRAGMENT lib define mymodule custom_nonzero Tensor x - Tensor torch library register_fake mymodule custom_nonzero _ x Number nonzero-elements data-dependent Since we cannot peek data fake impl we use ctx object construct new symint represents data-dependent size ctx = torch library get_ctx nnz = ctx new_dynamic_size shape = nnz x dim result = x new_empty shape dtype=torch int result torch library impl lib custom_nonzero CPU _ x x_np = x numpy res = np stack np nonzero x_np axis= torch tensor res device=x device _shape_env None _shape_env allow_dynamic_output_shape_ops raise torch _subclasses fake_tensor DynamicOutputShapeException _op isinstance min torch SymInt isinstance max torch SymInt raise ValueError f ctx new_dynamic_size min= min max= max expected f min max statically known ints got SymInt f This supported min raise ValueError f ctx new_dynamic_size min= min expected min f greater than equal API can only create f non-negative sizes allocate_size _shape_env min max allocate_size shape_env min_val= max_val=None result = shape_env create_unbacked_symint torch fx experimental symbolic_shapes _constrain_range_for_size result min=min_val max=max_val result