mypy allow-untyped-defs __future__ annotations typing Any Optional TYPE_CHECKING Union scheduler BaseSchedulerNode BaseScheduling FusedSchedulerNode Scheduler SchedulerNode cuda cuda_cpp_scheduling CUDACPPScheduling cutedsl cutedsl_scheduling CuteDSLScheduling rocm rocm_cpp_scheduling ROCmCPPScheduling triton TritonScheduling TYPE_CHECKING collections abc Sequence typing_extensions TypeAlias sympy Expr torch torch utils _ordered_set OrderedSet common BackendFeature _IntLike TypeAlias = Union int Expr CUDACombinedScheduling BaseScheduling Scheduler CUDA Kernels which delegates calls appropriate CUDA-C++ Triton Schedulers which both work CUDA devices use unified-wrapper codegen If Scheduling code needs specialized case mixed Triton CUDA C++ code would also place do __init__ scheduler Optional Scheduler - None super __init__ scheduler _triton_scheduling = TritonScheduling scheduler _cuda_cpp_scheduling = CUDACPPScheduling scheduler _rocm_cpp_scheduling = ROCmCPPScheduling scheduler _cutedsl_scheduling = CuteDSLScheduling scheduler get_backend_features device torch device - OrderedSet BackendFeature _triton_scheduling get_backend_features device choose_node_backend node BaseSchedulerNode - BaseScheduling _cuda_cpp_scheduling is_cuda_cpp_template node _cuda_cpp_scheduling _rocm_cpp_scheduling is_rocm_cpp_template node _rocm_cpp_scheduling _cutedsl_scheduling is_cutedsl_template node _cutedsl_scheduling _triton_scheduling can_fuse_vertical node BaseSchedulerNode node BaseSchedulerNode - bool _cuda_cpp_scheduling can_fuse_vertical node node True _cuda_cpp_scheduling is_cuda_cpp_template node _cuda_cpp_scheduling is_cuda_cpp_template node False CuteDSL doesn t support vertical fusion currently _cutedsl_scheduling is_cutedsl_template node _cutedsl_scheduling is_cutedsl_template node False _triton_scheduling can_fuse_vertical node node can_fuse_horizontal node BaseSchedulerNode node BaseSchedulerNode - bool node node node _cuda_cpp_scheduling is_cuda_cpp_template node _cuda_cpp_scheduling can_fuse_horizontal node node always False moment _cutedsl_scheduling is_cutedsl_template node _cutedsl_scheduling can_fuse_horizontal node node always False moment _triton_scheduling can_fuse_horizontal node node group_fn sizes Sequence Sequence _IntLike - tuple tuple _IntLike _triton_scheduling group_fn sizes codegen_template template_node BaseSchedulerNode epilogue_nodes Sequence BaseSchedulerNode prologue_nodes Sequence BaseSchedulerNode - Optional str _cuda_cpp_scheduling is_cuda_cpp_template template_node assert prologue_nodes _cuda_cpp_scheduling codegen_template template_node epilogue_nodes prologue_nodes _rocm_cpp_scheduling is_rocm_cpp_template template_node assert epilogue_nodes assert prologue_nodes _rocm_cpp_scheduling codegen_template template_node epilogue_nodes prologue_nodes _cutedsl_scheduling is_cutedsl_template template_node TODO remove when we add epilogue support assert epilogue_nodes assert prologue_nodes _cutedsl_scheduling codegen_template template_node epilogue_nodes prologue_nodes _triton_scheduling codegen_template template_node epilogue_nodes prologue_nodes codegen_mix_order_reduction node _triton_scheduling codegen_mix_order_reduction node codegen_node node Union FusedSchedulerNode SchedulerNode - None _triton_scheduling codegen_node node codegen_sync - None _triton_scheduling codegen_sync flush - None _triton_scheduling flush codegen_combo_kernel args Any kwargs Any - None _triton_scheduling codegen_combo_kernel args kwargs benchmark_fused_nodes nodes Sequence BaseSchedulerNode - tuple float str _triton_scheduling benchmark_fused_nodes nodes benchmark_codegened_module module _triton_scheduling benchmark_codegened_module module generate_kernel_code_from_nodes nodes Sequence Any benchmark_kernel bool = False hint_override Optional int = None - str _triton_scheduling generate_kernel_code_from_nodes nodes benchmark_kernel hint_override=hint_override benchmark_combo_kernel node_list Sequence BaseSchedulerNode - tuple float float list Optional str _triton_scheduling benchmark_combo_kernel node_list