__future__ annotations base copyreg dataclasses functools hashlib importlib importlib resources io itertools json logging os pickle pkgutil platform re shlex shutil struct subprocess sys tempfile textwrap threading warnings bisect bisect_right copy copy ctypes c_void_p CDLL cdll datetime timedelta functools lru_cache partial pathlib Path tempfile _TemporaryFileWrapper time time time_ns types ModuleType typing Any Callable cast Generic NoReturn TYPE_CHECKING TypeVar Union typing_extensions override Self torch torch distributed dist torch SymInt Tensor torch _dynamo device_interface get_interface_for_device torch _dynamo exc SkipFrame torch _dynamo utils CompileEventLogger counters dynamo_timed get_metrics_context torch _inductor config exc metrics torch _inductor codegen common custom_backend_codegen_configs custom_backend_passes init_backend_registration torch _inductor codegen cuda cuda_env torch _inductor codegen rocm compile_command rocm_compile_command rocm_compiler torch _inductor compile_worker utils in_toplevel_process torch _inductor cpp_builder _LINKER_SCRIPT _set_gpu_runtime_env _TORCH_PATH _transform_cuda_paths convert_cubin_to_obj CppBuilder CppOptions CppTorchDeviceOptions get_compiler_version_info get_ld_and_objcopy get_name_and_dir_from_output_file_path normalize_path_separator run_asm_build_object torch _inductor cpu_vec_isa pick_vec_isa torch _inductor custom_graph_pass CustomGraphModulePass CustomGraphPass CustomGraphPassType CustomPartitionerFn CustomPartitionerFnType torch _inductor freezing_utils has_frozen_params is_frozen_param torch _inductor runtime compile_tasks _reload_python_module torch _inductor runtime runtime_utils cache_dir default_cache_dir torch _inductor utils ALIGN_BYTES clear_on_fresh_cache determine_aoti_mmap_flags is_linux is_windows torch _logging trace_structured torch _subclasses fake_tensor extract_tensor_metadata FakeTensor TensorMetadata torch _utils_internal log_cache_bypass torch compiler config cconfig torch compiler _cache CacheArtifact CacheArtifactFactory CacheArtifactManager torch export pt _archive _package_weights TensorProperties Weights torch export pt _archive constants CUSTOM_OBJ_FILENAME_PREFIX torch fx experimental symbolic_shapes has_hint hint_int ShapeEnv torch utils _ordered_set OrderedSet output_code CompiledFxGraph remote_cache create_cache runtime autotune_cache runtime autotune_cache AutotuneCacheBundler triton_bundler TritonBundler virtualized V config is_fbcode triton fb build build_paths T = TypeVar T TYPE_CHECKING collections abc Generator KeysView Sequence concurrent futures Future compile_fx _CompileFxKwargs cpp_builder BuildOptionsBase graph GraphLowering ir ChoiceCaller output_code CompiledFxGraphConstants OutputCode remote_cache JsonDataTy RemoteCache runtime hints HalideInputSpec HalideMeta runtime triton_heuristics CachingAutotuner utils InputType _IS_WINDOWS = sys platform == win LOCK_TIMEOUT = config file_lock_timeout output_code_log = torch _logging getArtifactLogger __name__ output_code autotuning_log = torch _logging getArtifactLogger __name__ autotuning log = logging getLogger __name__ use_re_build - bool Use CUTLASS compilation only right now config is_fbcode cuda_env nvcc_exist _cuda_compiler triton fb re_build_helper should_build_locally should_build_locally False get_cpp_wrapper_cubin_path_name - str cubin_path torch version hip None hsaco_path get_kernel_bin_format device str - str device == cuda cubin torch version hip None hsaco device == xpu spv get_device_information device_type str - dict str str Gets all current device information used compile so metadata dict str str = AOTI_PLATFORM sys platform AOTI_MACHINE platform machine AOTI_CPU_ISA str torch _inductor cpu_vec_isa pick_vec_isa upper AOTI_COMPUTE_CAPABILITY str get_interface_for_device device_type get_compute_capability metadata CacheBase staticmethod functools cache get_system - dict str Any torch _inductor runtime triton_compat HAS_TRITON triton_key HAS_TRITON Use triton_key instead triton __version__ version updated each code change triton_version = triton_key triton_version = None try system dict str Any = device name None version triton triton_version device_properties = torch cuda get_device_properties torch cuda current_device torch version cuda None system device name = device_properties name system version cuda = torch version cuda system device name = device_properties gcnArchName system version hip = torch version hip except AssertionError RuntimeError If cuda installed none above config relevant system = system hash = hashlib sha json dumps system sort_keys=True encode utf- hexdigest system staticmethod clear_on_fresh_cache functools cache get_local_cache_path - Path Path os path join cache_dir cache CacheBase get_system hash __init__ - None system = CacheBase get_system get_local_cache - dict str Any local_cache_path = get_local_cache_path local_cache_path is_file open local_cache_path local_cache_fp local_cache = json load local_cache_fp local_cache cache update_local_cache local_cache dict str Any - None local_cache_path = get_local_cache_path write_atomic str local_cache_path json dumps system system cache local_cache indent= make_dirs=True LocalCache CacheBase lookup keys str - dict str Any &#124; None cache = get_local_cache sub_cache = cache key keys key cache sub_cache = cache key None sub_cache set_value keys str value Any - None cache = get_local_cache sub_cache = cache key keys - sub_cache setdefault key sub_cache = sub_cache key sub_cache keys - = value update_local_cache cache PersistentCache CacheBase lookup choices list ChoiceCaller op str inputs str benchmark Callable Any dict ChoiceCaller float &#124; None hint_override int &#124; None = None - dict ChoiceCaller float Check see we have benchmarked given choice callers For each choice caller Check local_cache op inputs choice precision benchmark cached If benchmark None ` max_autotune_gemm=True ` benchmark choice update local_cache op inputs choice benchmark b ` max_autotune_gemm=False ` don t benchmark choice nothing precision = torch get_float _matmul_precision cache_key = f inputs _ hint_override hint_override None inputs timings = check_cache cache dict str Any - bool Check ` cache ` contains data all choices hit = True choice choices choice_hash = choice hash_key choice_hash cache get op get cache_key get precision cache hit timings choice = cache op cache_key precision choice_hash cache miss hit = False break hit local_cache = get_local_cache config autotune_local_cache check_cache local_cache benchmark None re-benchmark everything try get consistent numbers same machine timings = benchmark choices assert all choice timings choice choices local_cache setdefault op local_cache op setdefault cache_key setdefault precision choice timing timings items local_cache op cache_key precision choice hash_key = timing update_local_cache local_cache timings get_lock_dir - str lock_dir = os path join cache_dir locks os path exists lock_dir os makedirs lock_dir exist_ok=True lock_dir sha _hash data bytes - str strip off Q==== suffix common every hash value base b encode hashlib sha data digest decode utf- lower code_hash code str &#124; bytes extra str &#124; bytes = - str hashing_str = code isinstance code bytes code encode utf- extra extra_b = extra isinstance extra bytes extra encode utf- hashing_str = hashing_str + b &#124; &#124; + extra_b c + sha _hash hashing_str get_path basename str extension str specified_dir str = - tuple str str str specified_dir os path isabs specified_dir subdir = specified_dir subdir = os path join cache_dir specified_dir subdir = os path join cache_dir basename path = os path join subdir f basename extension basename subdir path get_hash content str &#124; bytes extra str = hash_type str = code - str hash_type amdgcn code ptx spv code_hash content extra hash_type cubin hsaco spv code_hash repr content raise AssertionError f Unknown hash type hash_type WritableTempFile Avoid Permission denied error Windows tempfile NamedTemporaryFile w suffix= gv temp_file Not writable Windows https docs python org library tempfile html#tempfile NamedTemporaryFile Example WritableTempFile w suffix= gv temp_file tree to_dotfile temp_file name __init__ mode str = w encoding Any = None suffix Any = None - None mode = mode encoding = encoding suffix = suffix __enter__ - _TemporaryFileWrapper Any temp_file = tempfile NamedTemporaryFile mode encoding=self encoding suffix=self suffix delete=False temp_file __exit__ exc_type Any exc_val Any exc_tb Any - None temp_file close try os unlink temp_file name except OSError e _IS_WINDOWS On Windows some case temp file opened fail unlink Need ignore pass raise e write content str &#124; bytes extension str extra str = hash_type str = code specified_dir str = key str &#124; None = None - tuple str str key None use striped content compute hash so we don t end up different hashes just because content begins ends different number spaces key = get_hash content strip extra hash_type basename _subdir path = get_path key extension specified_dir os path exists path write_atomic path content make_dirs=True basename path write_text text str - str Write ` text ` file path computed based hash write text txt write_atomic path_ str content str &#124; bytes make_dirs bool = False encode_utf_ bool = False - None Write into temporary file first avoid conflicts between threads Avoid using named temporary file those have restricted permissions assert isinstance content str bytes Only strings byte arrays can saved cache path = Path path_ make_dirs path parent mkdir parents=True exist_ok=True tmp_path = path parent f os getpid threading get_ident tmp write_mode = w isinstance content str wb tmp_path open write_mode encoding= utf- encode_utf_ None f f write content try tmp_path rename target=path except FileExistsError _IS_WINDOWS raise On Windows file exist expected https docs python org library pathlib html#pathlib Path rename Below two lines code equal ` tmp_path rename path ` non-Windows OS Copy tmp_file Target Dst file shutil copy src=tmp_path dst=path Delete tmp_file os remove tmp_path dataclasses dataclass TensorMetadataAndValues TensorMetadata plus elements list raw values Used hashing inlined constants tensor_metadata TensorMetadata values list Any _ident x T - T x extract_tensor_metadata_for_cache_key t Tensor - TensorMetadata Extracts tensor metadata removes fields TensorMetadata needed caching meta = extract_tensor_metadata t hasattr t _is_inductor_static meta = dataclasses replace meta storage_offset= storage_bytes=None meta FxGraphCachePickler pickle Pickler Custom pickler customize pickling some objects Tensors only purpose computing hash keying into FxGraphCache Tensors contain objects don t pickle vary between runs we want capture data allow us compute stable safe hash __init__ gm torch fx GraphModule has_user_defined_triton_kernels bool = False - None Create FX graph pickler If include_non_inlined=True then pickling will include _values_ all Tensors Note any tensors constants attached attributes GraphModule Otherwise pickling will include only metadata these tensors _stream = io BytesIO super __init__ _stream dispatch_table = copyreg dispatch_table copy dispatch_table update FakeTensor functools partial _reduce_fake_tensor torch Tensor functools partial _reduce_tensor torch nn parameter Parameter functools partial _reduce_tensor torch SymInt functools partial _reduce_symint torch fx experimental _backward_state BackwardState functools partial _reduce_unsupported has_user_defined_triton_kernels Need use runtime type GraphModule generates singleton __new__ function dispatch_table gm __class__ = functools partial _reduce_graph_module Run pickler fast so doesn t intern strings making hash result more predictable TODO pickler fast technically deprecated Will work new python versions fast = True _reduce_fake_tensor t Tensor - tuple Callable T T tuple TensorMetadata Custom reducer pickle FakeTensors metadata = extract_tensor_metadata_for_cache_key t _ident metadata _reduce_tensor t Tensor - tuple Callable T T tuple TensorMetadata &#124; TensorMetadataAndValues Custom reducer pickle Tensors If we see tensors we know they re constants stored attributes GraphModule graph GraphLowering t is_mkldnn TODO These tensors don t currently pickle so we can t cache compiled graph containing them Just fail now If mkldnn tensors get pickling support we can remove raise BypassFxGraphCache mkldnn tensors unpickleable metadata = extract_tensor_metadata_for_cache_key t If non-inlined frozen parameter we consider metadata only is_frozen_param t GraphLowering can_inline_constant t _ident metadata Very large tensors will expensive copy cpu hash Let s least report any slowness start = time values = t tolist elapsed = time - start elapsed warnings warn f FX graph cache copying large constant took elapsed s Please file issue _ident TensorMetadataAndValues metadata values _reduce_symint s SymInt - tuple Callable T T tuple str Custom reducer pickle SymInts For hashing purposes we only care about name symbol backed value We evaluate guards stored cached graph ensure cached entity SymInt args safe reuse _ident str s _reduce_unsupported s Any - NoReturn Custom reducer handle any objects we don t support therefore raise bypass caching raise BypassFxGraphCache Reduce unsupported _reduce_graph_module gm torch fx GraphModule - tuple Any tuple dict str Any str Custom reducer graph module handle irrelevant data user defined triton kernels Essentially what we doing here huge hack where user defined triton kernel contain dynamo time side table arguments call_function indices into side table These arguments hashing purposes since we included source code into cache key numbers prone give false negatives due ordering fn data imports = gm __reduce__ code = data _code code = re sub r kernel_idx = \d+ code code = re sub r constant_args_idx = \d+ code data _code = code fn data imports dumps obj Any - bytes Pickle object byte string try dump obj _stream getvalue except TypeError AttributeError e Some configs options may pickle log warning Failed pickle cache key exc_info=True raise BypassFxGraphCache Failed pickle cache key e finally Reset our stream next dump _stream seek _stream truncate get_hash obj Any - str Serialize object hash bytes serialized_data = dumps obj sha _hash serialized_data debug_lines inp FxGraphHashDetails - list str Get printable string describing more detail all attributes comprising object Useful debugging when one graph hashes different value than another get_str obj Any - str isinstance obj torch Tensor str extract_tensor_metadata_for_cache_key obj isinstance obj bytes val = obj decode utf- errors= replace val len val = val + type obj dispatch_table Run reducer object str dispatch_table type obj obj str obj lines = attr obj vars inp items isinstance obj list ii range len obj h = get_hash obj ii lines append f h attr ii get_str obj ii isinstance obj dict k v obj items h = get_hash v lines append f h attr k get_str v h = get_hash obj lines append f h attr get_str obj lines build_code_hash roots list str &#124; None prefix str hasher hashlib _Hash - None lib sorted pkgutil iter_modules roots prefix key=lambda x x name spec = lib module_finder find_spec lib name None assert spec None module = spec origin assert module None open module rb f hasher update spec name encode utf- hasher update f read lib ispkg need also hash submodules build_code_hash spec submodule_search_locations f spec name hasher torch_key_cache func Callable bytes - Callable bytes This function reimplementation functools lru_cache set function allows prepopulating cache Use list reference semantics _cache list bytes = wrapper - bytes len _cache == _cache append func _cache set_val val bytes - None assert len _cache == _cache append val clear - None _cache clear wrapper set = set_val type ignore attr-defined wrapper clear = clear type ignore attr-defined wrapper torch_key_cache torch_key - bytes Compute key contains relevant information about torch source files dynamo_timed inductor_codecache_torch_key log_pt _compile_event=False config is_fbcode get_code_hash root str - bytes This function isn t meant used outside torch_key just helper clarity Instead use torch_key directly when you need hash representing state source code extra_files = codegen aoti_runtime interface cpp script ld inductor_root = os path dirname __file__ extra_files = os path join inductor_root x x extra_files hasher = hashlib sha hasher update torch __version__ encode utf- build_code_hash root hasher path extra_files os path exists path open path rb f hasher update f read hasher digest get_code_hash _TORCH_PATH libfb py parutil parutil get_file_contents torch src_hash txt rstrip encode ascii get_inductor_root - str os path dirname __file__ dataclasses dataclass OrderedSetHolder See FxGraphHashDetails Holds sorted list support stable hashing set kwargs items list Any BypassFxGraphCache Exception Exception indicate FxGraphCache should bypassed FxGraphHashDetails Object capture all details compiled FX graph relevant computing safe stable cache key Excluded kwargs param stable between runs EXCLUDED_KWARGS = graph_id __init__ gm torch fx GraphModule example_inputs Sequence InputType fx_kwargs _CompileFxKwargs inputs_to_check Sequence int - None gm = gm example_inputs = example_inputs cache_key_tag = cconfig cache_key_tag Order kwargs so hashing stable changes kwarg order Although s technically _CompileFxKwargs we don t actually need typed such since we re just using generate hash fx_kwargs dict str object = k v sorted fx_kwargs items k EXCLUDED_KWARGS type v set OrderedSet noqa set_linter Special case handle set params Python sets can t ordered so sort elements store them proxy fx_kwargs k = OrderedSetHolder sorted v type ignore call-overload fx_kwargs k = v torch _higher_order_ops triton_kernel_wrap kernel_side_table triton_kernel_wrapper_functional triton_kernel_wrapper_mutation torch _inductor codegen wrapper user_defined_triton_kernel_transitive_closure_source_code Node meta will part gm s reduce function so lets remember kernel source code separately user_defined_triton_source list Any = gm None module gm modules isinstance module torch fx GraphModule continue node itertools chain module graph find_nodes op= call_function target=triton_kernel_wrapper_functional module graph find_nodes op= call_function target=triton_kernel_wrapper_mutation triton runtime autotuner Autotuner kernel = kernel_side_table get_kernel node kwargs kernel_idx configs = None isinstance kernel Autotuner kernel configs configs = str sorted sorted str kv kv c all_kwargs items c kernel configs kernel = kernel fn kernel_source = user_defined_triton_kernel_transitive_closure_source_code kernel constant_args = kernel_side_table get_constant_args node kwargs constant_args_idx user_defined_triton_source append kernel_source constant_args configs Alignment checks inputs_to_check = inputs_to_check no_tensor_inputs = any isinstance x torch Tensor x example_inputs This device index usually already encoded device inputs fx graphs don t necessarily have tensor inputs If there aren t any we need guard device index case we allocate cuda tensors no_tensor_inputs torch accelerator is_available default_cuda_device_index = torch accelerator current_device_index Deterministic algorithms can affect codegen via lowering cuda kernels deterministic_algorithms_settings = torch are_deterministic_algorithms_enabled torch is_deterministic_algorithms_warn_only_enabled torch utils deterministic fill_uninitialized_memory type ignore attr-defined Global settings affecting matmul codegen cuda_matmul_settings = torch backends cuda matmul fp _precision torch backends cuda matmul allow_fp _reduced_precision_reduction torch backends cuda matmul allow_bf _reduced_precision_reduction Also hash various system info including triton compiler version torch_version = torch_key system_info = CacheBase get_system inductor_config = config save_config_portable ignore_private_configs=False Custom post grad passes should provide ID hash post_grad_custom_pre_pass = _get_custom_pass_detail config post_grad_custom_pre_pass TODO change more holistic config rather than bundled_autograd_cache precompile_enabled = torch _functorch config bundled_autograd_cache post_grad_custom_post_pass = _get_custom_pass_detail config post_grad_custom_post_pass joint_custom_pre_pass = _get_custom_pass_detail config joint_custom_pre_pass joint_custom_post_pass = _get_custom_pass_detail config joint_custom_post_pass _pre_fusion_custom_pass = _get_custom_pass_detail_unsafe config _pre_fusion_custom_pass _fuse_ddp_communication_passes = _get_custom_pass_detail_unsafe config _fuse_ddp_communication_passes Register indcutor backends custom passes get their UUIDs init_backend_registration custom_backend_passes = tuple map _get_custom_pass_detail custom_backend_passes values Save custom inductor codegen configs custom_backend_codegen_configs = device custom_config save_config_portable ignore_private_configs=False device custom_config custom_backend_codegen_configs items custom_config None Register custom partitioner function _custom_partitioner_fn = _get_custom_partitioner_fn_detail config custom_partitioner_fn This mainly added handle these two inductor configs which unfortunately sometimes cache safe - _pre_fusion_custom_pass - _fuse_ddp_communication_passes Their types can found ` torch _inductor config py ` - they string names we can cache them safely one default - any them set custom callables we will need cache miss Future work someone find any places where these functions used force them type CustomGraphPass so we can guarantee serialization _get_custom_pass_detail_unsafe custom_pass Any - Any &#124; None custom_pass None isinstance custom_pass list _get_custom_pass_detail_unsafe x x custom_pass isinstance custom_pass str custom_pass isinstance custom_pass CustomGraphPass custom_pass uuid callable custom_pass Returning None safe here because we raise explicit bypass error later we detect these passes set callables None raise AssertionError f unknown config type str type custom_pass _get_custom_pass_detail custom_pass CustomGraphPassType &#124; CustomGraphModulePass - Any &#124; None custom_pass None assert isinstance custom_pass CustomGraphPass CustomGraphModulePass custom_pass uuid _get_custom_partitioner_fn_detail custom_partitioner_fn CustomPartitionerFnType - Any &#124; None custom_partitioner_fn None assert isinstance custom_partitioner_fn CustomPartitionerFn custom_partitioner_fn uuid compiled_fx_graph_hash gm torch fx GraphModule example_inputs Sequence InputType fx_kwargs _CompileFxKwargs inputs_to_check Sequence int - tuple str list str Generate unique hash FX graph caching details = FxGraphHashDetails gm example_inputs fx_kwargs inputs_to_check has_user_defined_triton_kernels = len details user_defined_triton_source = pickler = FxGraphCachePickler gm has_user_defined_triton_kernels The prefix distinguishes among other kinds objects we cache module key = f + pickler get_hash details debug_lines = pickler debug_lines details debug_str = \n join debug_lines log debug f FX graph cache hash details key key \n debug_str noqa G key debug_lines add_ephemeral_timeout_increase_for_distributed time_saved_ns int - int Ephemerally increases NCCL timeout when compiling distributed job Returns amount seconds increased torch distributed is_available torch distributed is_initialized increased_timeout_sec = int time_saved_ns e convert seconds config is_fbcode fudge_factor = torch _utils_internal justknobs_getval_int pytorch remote_cache ephemeral_timeout_fudge_factor_percentage log info Ephemeral NCCL timeout increase fudge factor d original increase value d fudge_factor increased_timeout_sec increased_timeout_sec += int increased_timeout_sec fudge_factor log info Increasing NCCL timeout d increased_timeout_sec dist distributed_c d _add_ephemeral_timeout_for_all_pgs timedelta seconds=increased_timeout_sec increased_timeout_sec GuardedCache Generic T Mixin caches have guards associated their entries classmethod _get_tmp_dir_for_key cls type GuardedCache T _key str - str raise NotImplementedError Implement _get_tmp_dir_for_key parent classmethod iterate_over_candidates cls type GuardedCache T local bool remote_cache RemoteCache JsonDataTy &#124; None key str - Generator tuple T bytes None None local subdir = cls _get_tmp_dir_for_key key os path exists subdir path sorted os listdir subdir try open os path join subdir path rb f content = f read yield pickle loads content content except Exception log warning fx graph cache unable load compiled graph exc_info=True remote_cache try cache_data = remote_cache get key None assert isinstance cache_data dict data = cache_data data assert isinstance data str bytes content = base b decode data yield pickle loads content content except Exception log warning s unable load compiled graph cls __name__ exc_info=True classmethod find_guarded_entry cls type GuardedCache T key str local bool remote_cache RemoteCache JsonDataTy &#124; None evaluate_guards Callable str list int &#124; list torch SymInt bool hints list int - tuple T &#124; None bytes &#124; None dict str str Find first cache entry iterate_over_candidates passes ` evaluate_guards ` Args key The cache key look up local Whether check local cache remote_cache The remote cache check any evaluate_guards Function evaluates whether guard passes check given list hint values guard expression hints List symint hints paired evaluate_guards Returns A tuple graph pickled_content found None None found graph = None pickled_content = None result_status = full_miss sample_guards_expr = None Iterate over any entries subdir key evaluate guards determine whether there s hit candidate content cls iterate_over_candidates local remote_cache key assert hasattr candidate guards_expr candidate guards_expr type ignore attr-defined No guards evaluate so hit graph = candidate pickled_content = content result_status = hit break Evaluate guard expression current context If there s cache hit we don t want evaluation affect current env e g cause creation new guards so we evaluate hints instead symbols hit = bool evaluate_guards candidate guards_expr hints type ignore attr-defined hit graph = candidate pickled_content = content result_status = hit sample_guards_expr = candidate guards_expr break At least one guard missed log result_status = guard_miss sample_guards_expr = candidate guards_expr info = cache_status_detailed result_status sample_guards_expr None info cache_status_guard_expr = sample_guards_expr graph pickled_content info classmethod _filter_backed_symints cls type GuardedCache T inputs Sequence InputType - list torch SymInt Get backed SymInt objects input list Note we can never have guards depend unbacked symint s s inputs isinstance s torch SymInt has_hint s classmethod _get_shape_env cls type GuardedCache T - ShapeEnv &#124; None Helper get shape env tracing context ctx = torch _guards TracingContext try_get ctx ctx fake_mode None ctx fake_mode shape_env CacheArtifactFactory register InductorCacheArtifact CacheArtifact override populate_cache - None FxGraphCache _write_to_local_cache key content override staticmethod type - str inductor FxGraphCache GuardedCache CompiledFxGraph Supports caching reusing compiled Fx graphs The overall strategy follows - This cache stores entries disk When saving entry we can t serialize callables could C++ Triton etc so we serialize their own disk cache location We then recreate compiled artifact after fetching disk - For indexing cache we gather fields relevant identifying FxGraph graph module graph inputs system settings etc into FxGraphCacheDetails object pickle compute hash key See FxGraphCachePickler - Among metadata we store we also include guards expression s appropriate validating any symbols Tensor arguments have symbolic bounds On cache lookup then we evaluate those guards current context validate cached entry can served - A given graph could have multiple compiled versions corresponding different sets guards Therefore we store cache entries form temp dir fx graph hash serialized metadata - On lookup we compute key graph details iterate over all leaf files corresponding subdirectory deserialize entry evaluate its guards expression If evaluation succeeds we have cache hit If fails we compile graph store new entry - Finally cache hit we need make sure any guards would have been created during compilation added current context TODO masnesral Investigate whether s beneficial store compiled graphs in-memory cache after loading disk staticmethod _get_tmp_dir - str Get toplevel temporary directory storing compiled graphs os path join cache_dir fxgraph classmethod _get_tmp_dir_for_key cls type FxGraphCache key str - str Return disk location given cache key os path join FxGraphCache _get_tmp_dir key key staticmethod cache_hit_post_compile graph CompiledFxGraph cache_info dict str Any constants CompiledFxGraphConstants - tuple CompiledFxGraph &#124; None dict str Any Cache specific post compile steps need run we find graph cache This includes putting bundled triton artifacts right place reloading PyCodeCache artifact etc These don t always happen i e cache miss so they separate function CompiledFxGraph post_compile bundle = graph _triton_bundle triton_bundler_meta = TritonBundler read_and_emit bundle meta = triton_bundler_meta None cache_info triton_bundler_meta = str meta CompileEventLogger try_add_pt _compile inductor_compile cached_kernel_names=meta cached_kernel_names CompileEventLogger try_add_pt _compile AOTAutogradCache inductor_load cached_kernel_names=meta cached_kernel_names len meta cached_kernel_names CompileEventLogger try_ CompileEventLogger increment_toplevel num_triton_bundles try artifact_path = graph after_deserialization constants graph GraphLowering This used tests check output specific details GraphLowering save_output_code None GraphLowering save_output_code graph source_code except OSError Not expected case PyCodeCache entry removed underneath us treat cache miss recompile None cache_info inductor_meta = autotune_cache inductor_meta_from_config code = graph source_code AutotuneCacheBundler begin_compile inductor_meta code=code Increment cached metrics counters amounts recorded when FX graph compiled cache entry Pretending these counters incremented normally useful testing cache enabled metrics CachedMetricsHelper apply_deltas graph metrics_deltas counters inductor += graph counter_deltas output_code_log debug Output code \n s code output_code_log debug Output code written s artifact_path On cache hit use artifact path filename trace_structured artifact metadata_fn=lambda name fx_graph_runnable encoding string payload_fn=lambda graph runnable_graph_str trace_structured inductor_post_grad_graph payload_fn=lambda graph inductor_post_grad_graph_str trace_structured inductor_output_code lambda filename artifact_path file_path os path abspath artifact_path payload_fn=lambda code trace_structured artifact metadata_fn=lambda name inductor_provenance_tracking_node_mappings encoding json payload_fn=lambda graph inductor_provenance_mapping_str trace_structured artifact metadata_fn=lambda name inductor_provenance_tracking_kernel_stack_traces encoding json payload_fn=lambda graph inductor_provenance_stack_traces_str get_metrics_context in_progress graph inductor_provenance_stack_traces_str get_metrics_context add_to_set inductor_provenance graph inductor_provenance_stack_traces_str graph cache_info staticmethod _lookup_graph key str example_inputs Sequence InputType local bool remote_cache RemoteCache JsonDataTy &#124; None constants CompiledFxGraphConstants evaluate_guards Callable str list int &#124; list torch SymInt bool &#124; None = None - tuple CompiledFxGraph &#124; None dict str Any Lookup compiled graph cache key On hit deserialized CompiledFxGraph object On miss None ` constants ` tracks list constants way obtain list constants associated given cache entry ` evaluate_guards ` allows AOTAutogradCache other callers customize what constitutes guard success Normally guard hit happens ` shape_env evaluate_guards_expression ` returns True shape_env = FxGraphCache _get_shape_env assert shape_env None symints = FxGraphCache _filter_backed_symints example_inputs hints = hint_int s s symints If config turned everything guard hit we check nothing config unsafe_skip_cache_dynamic_shape_guards This also makes so we don t add anything dynamic shape environment evaluate_guards = lambda x y True noqa E evaluate_guards None evaluate_guards = shape_env evaluate_guards_expression cache_info dict str Any = dict Use find_graph_for_key method find graph given key graph pickled_content guard_info = FxGraphCache find_guarded_entry key local remote_cache evaluate_guards hints cache_info update guard_info graph None None cache_info pickled_content None CacheArtifactManager record_artifact InductorCacheArtifact type key pickled_content Now re-evaluate symints add any guards current env graph guards_expr check = bool evaluate_guards graph guards_expr symints assert check True log debug fx graph cache key s post-load guards s key shape_env guards FxGraphCache cache_hit_post_compile graph cache_info constants staticmethod _write_to_local_cache key str content bytes - None subdir = FxGraphCache _get_tmp_dir_for_key key os path exists subdir os makedirs subdir exist_ok=True Use hash serialized CompiledFxGraph get unique file name The specific name doesn t matter since lookup involves iterating over all entries parent subdir path = os path join subdir sha _hash content write_atomic path content make_dirs=True staticmethod _save_graph key str compiled_graph OutputCode example_inputs Sequence InputType local bool remote_cache RemoteCache JsonDataTy &#124; None - None Store serialized CompiledFxGraph disk compile_fx CompiledFxGraph assert isinstance compiled_graph CompiledFxGraph f serialization type compiled_graph NYI Before serializing compute guard expression will used ensure CompiledFxGraph valid when loaded cache It s sufficient consider only SymInt args fx graph since Tensor shapes already captured hash cache key Any Tensor arg symbolic shape will have SymInt arg graph shape_env = FxGraphCache _get_shape_env assert shape_env None symints = FxGraphCache _filter_backed_symints example_inputs guards = shape_env get_pruned_guards symints compiled_graph guards_expr = shape_env produce_guards_expression placeholders=symints guards=guards disk_compiled_graph = copy compiled_graph disk_compiled_graph prepare_for_serialization try content = pickle dumps disk_compiled_graph except Exception log warning fx graph cache unable serialize compiled graph exc_info=True counters inductor fxgraph_cache_pickle_error += try CacheArtifactManager record_artifact InductorCacheArtifact type key content local FxGraphCache _write_to_local_cache key content remote_cache time_taken_ms = int disk_compiled_graph _time_taken_ns e cache_data JsonDataTy = data base b encode content decode ascii time_taken_ms time_taken_ms remote_cache put key cache_data except Exception log warning fx graph unable write cache exc_info=True counters inductor fxgraph_cache_write_error += staticmethod _check_for_hop gm torch fx GraphModule - None module gm modules isinstance module torch fx GraphModule continue node module graph nodes isinstance node target torch _ops HigherOrderOperator node target cacheable raise BypassFxGraphCache f Can t cache HigherOrderOperator node target name node op == getattr isinstance getattr gm node target torch _C ScriptObject raise BypassFxGraphCache Can t cache torchbind objects staticmethod _check_can_cache gm torch fx GraphModule - None Check some conditions would preclude caching raise BypassFxGraphCache bypass case caching possible Post grad custom passes must implement CustomGraphPass we don t know how include them cache key calculation p config post_grad_custom_pre_pass config post_grad_custom_post_pass p isinstance p CustomGraphPass p uuid raise BypassFxGraphCache Unsupported post grad custom pass Same joint custom passes p config joint_custom_pre_pass config joint_custom_post_pass p isinstance p CustomGraphPass p uuid raise BypassFxGraphCache Unsupported joint custom pass We should find any users _pre_fusion_custom_pass _fuse_ddp_communication_passes ensure they passing us raw callables config _pre_fusion_custom_pass None isinstance config _pre_fusion_custom_pass CustomGraphPass raise BypassFxGraphCache Unsupported _pre_fusion_custom_pass p config _fuse_ddp_communication_passes callable p isinstance p CustomGraphPass raise BypassFxGraphCache Unsupported _fuse_ddp_communication_pass Freezing can embed constants wouldn t static across runs has_frozen_params gm torch _utils_internal justknobs_check pytorch inductor allow_freezing_with_caching raise BypassFxGraphCache Skipping graph frozen constants config aot_inductor use_runtime_constant_folding raise BypassFxGraphCache Runtime constant folding can introduce constants aren t static across runs torch _inductor compiler_bisector CompilerBisector CompilerBisector bisection_enabled log debug dont cache graph when bisect enabled raise BypassFxGraphCache The treatment guards caching implementation requires we have shape env FxGraphCache _get_shape_env None log debug fx graph cache no shape env raise BypassFxGraphCache No shape env We skip caching there any HOPs torchbind objects FxGraphCache _check_for_hop gm staticmethod prepare_key gm torch fx GraphModule example_inputs Sequence InputType fx_kwargs _CompileFxKwargs inputs_to_check Sequence int remote bool - tuple tuple str list str &#124; None dict str Any Checks inductor input cacheable then computes returns cache key input Returns key_info cache_info where - key_info hash_key debug_lines - cache_info will contain debug info event BypassFxGraphCache NB It possible have function union instead But I personally believe more annoying difficult read format try FxGraphCache _check_can_cache gm key debug_lines = compiled_fx_graph_hash gm example_inputs fx_kwargs inputs_to_check except BypassFxGraphCache e counters inductor fxgraph_cache_bypass += log info Bypassing FX Graph Cache because s e noqa G remote log_cache_bypass bypass_fx_graph str e cache_info = cache_state bypass cache_bypass_reason str e cache_event_time time_ns None cache_info If key exists then cache_info will come load_with_key key debug_lines staticmethod get_remote_cache - RemoteCache JsonDataTy &#124; None Attempts load remote cache returns None error cache_id = fx-graph-v create_cache cache_id config is_fbcode FbRemoteFxGraphCache RemoteFxGraphCache staticmethod load_with_key key str debug_lines list str example_inputs Sequence InputType local bool remote_cache RemoteCache JsonDataTy &#124; None is_backward bool constants CompiledFxGraphConstants evaluate_guards Callable str list int &#124; list torch SymInt bool &#124; None = None - tuple CompiledFxGraph &#124; None dict str Any Lookup graph given key results metadata Doesn t do any logging its own because AOTAutograd handles cache miss differently FXGraphCache compiled_graph cache_info = FxGraphCache _lookup_graph key example_inputs local remote_cache constants evaluate_guards cache_info = cache_info key key components debug_lines cache_event_time time_ns compiled_graph None log info fx graph cache hit key s key counters inductor fxgraph_cache_hit += cache_info cache_state = hit remote_cache Count remote cache hit stats CompileEventLogger try_ CompileEventLogger increment_toplevel inductor_fx_remote_cache_hit_count CompileEventLogger try_ CompileEventLogger add_to_set_toplevel inductor_fx_remote_cache_hit_keys key time_saved_ns = compiled_graph _time_taken_ns None cache_info time_saved_ns = time_saved_ns CompileEventLogger try_ CompileEventLogger increment_toplevel distributed_ephemeral_timeout_us time_saved_ns ephemeral_increase = add_ephemeral_timeout_increase_for_distributed time_saved_ns = cache_info ephemeral_timeout_increase = ephemeral_increase remote_cache Count remote cache miss stats CompileEventLogger try_ CompileEventLogger increment_toplevel inductor_fx_remote_cache_miss_count CompileEventLogger try_ CompileEventLogger add_to_set_toplevel inductor_fx_remote_cache_miss_keys key log info fx graph cache miss key s key counters inductor fxgraph_cache_miss += cache_info cache_state = miss compiled_graph cache_info staticmethod clear - None Clear out on-disk cache try shutil rmtree FxGraphCache _get_tmp_dir except FileNotFoundError pass functools cache split_aot_inductor_output_path path str - tuple str str get_module_ext_type - str _IS_WINDOWS pyd so Returns path where AOT Inductor compiled kernels stored path endswith get_module_ext_type os path split path path endswith pt os path split path path clear_on_fresh_cache CudaKernelParamCache cache dict str dict str Any = cache_clear = staticmethod cache clear classmethod set cls key str params dict str str &#124; None cubin str bin_type str asm str &#124; None = None asm_type str &#124; None = None - None basename = None config aot_inductor package_cpp_only assert config triton unique_kernel_names package_cpp_only requires triton kernel names unique assert params mangled_name Missing kernel name basename = params mangled_name _ bin_path = write cubin bin_type hash_type=bin_type specified_dir=split_aot_inductor_output_path config aot_inductor output_path key=basename Retrieve basename again case generated hashcode basename _ = get_name_and_dir_from_output_file_path bin_path config aot_inductor emit_multi_arch_kernel bin_type_to_ext = cubin fatbin spv spv assert bin_type bin_type_to_ext keys multi_arch_kernel_binary only supported CUDA XPU base_path _ = os path splitext bin_path bin_path = base_path + bin_type_to_ext bin_type asm_path str = config aot_inductor emit_multi_arch_kernel config aot_inductor package_cpp_only assert asm Missing kernel assembly code assert asm_type Missing kernel assembly type _ asm_path = write asm asm_type hash_type=asm_type specified_dir=split_aot_inductor_output_path config aot_inductor output_path make sure asm file has same basename key=basename params get_cpp_wrapper_cubin_path_name = bin_path params asm = asm_path cls cache key = params classmethod get cls key str - dict str Any &#124; None cls cache get key None classmethod get_keys cls - KeysView str cls cache keys AotCodeCompiler Compile AOT Inductor generated code classmethod compile cls graph GraphLowering wrapper_code str kernel_code str serialized_extern_kernel_nodes str &#124; None device_type str additional_files list str - list Union str Weights &#124; str Returns so path returns list files generated config aot_inductor package=True generated_files list str &#124; Weights = additional_files type ignore assignment _set_gpu_runtime_env cpp_extension consults env picked_vec_isa = pick_vec_isa vec_isa_cmd_gen = CppBuilder name= o sources= i BuildOption=CppTorchDeviceOptions vec_isa=picked_vec_isa device_type=device_type aot_mode=graph aot_mode write function will calc source_code hash same source code different ISA level should generate different hash So we need get command_line which contains isa related parameter part hash key And then pass command_line below write function extra parameter guarantee source code hash contains ISA difference cpp_command = repr vec_isa_cmd_gen get_command_line Meta internal AOTInductor CPU use_relative_path = config is_fbcode device_type == cpu graph aot_mode specified_output_path specified_artifact_name = split_aot_inductor_output_path config aot_inductor output_path TODO benjaminglass CMake packaging path doesn t support linking files built different flags Until s implemented append kernel code wrapper build everything max optimization config aot_inductor package_cpp_only wrapper_code = \n join wrapper_code kernel_code kernel_code = wrapper_key wrapper_path = write wrapper_code wrapper cpp extra=cpp_command specified_dir=specified_output_path key=config aot_inductor model_name_for_generated_files kernel_code = f Triton kernels embedded comments wrapper_path \n + kernel_code _ kernel_path = write kernel_code kernel cpp extra=cpp_command specified_dir=specified_output_path key=config aot_inductor model_name_for_generated_files header_code = header_path = config aot_inductor dynamic_linkage link statically we also need header file open os path join os path dirname os path dirname __file__ csrc inductor aoti_runtime model h f model_name_for_generated_files guaranteed non-empty when compile_standalone model_class_name = config aot_inductor model_name_for_generated_files class_name = f AOTInductorModel model_class_name header_code = f read we replace like avoid replacing AOTInductorModelBase AOTInductorModelKernelsBase header_code = header_code replace AOTInductorModel f class_name replace AOTInductorModel f class_name replace AOTInductorModel f class_name _ header_path = write header_code h specified_dir=specified_output_path key=model_class_name Log AOTInductor wrapper kernel code needed WritableTempFile w+ t Avoid Permission denied error Windows tempfile NamedTemporaryFile w suffix= gv temp_file Not writable Windows https docs python org library tempfile html#tempfile NamedTemporaryFile Example WritableTempFile w suffix= gv temp_file tree to_dotfile temp_file name t writelines wrapper_code \n kernel_code \n t flush V debug output_code t name extension= cpp config aot_inductor package generated_files append wrapper_path config aot_inductor package_cpp_only generated_files append kernel_path config aot_inductor dynamic_linkage generated_files append header_path output_code_log info Wrapper code written s wrapper_path output_code_log info Kernel code written s kernel_path trace_structured graph_dump lambda name inductor_aot_wrapper_code type cpp filename wrapper_path payload_fn=lambda wrapper_code trace_structured graph_dump lambda name inductor_aot_kernel_code type cpp filename kernel_path payload_fn=lambda kernel_code config aot_inductor dynamic_linkage output_code_log info Header code written s header_path trace_structured graph_dump lambda name inductor_aot_header_code type cpp filename header_path payload_fn=lambda header_code We use file lock below protect FS operations The lock file scoped key so make sure consts_s protected same lock wrapper_path_operator = Path wrapper_path kernel_path_operator = Path kernel_path specified_sub_dir = wrapper_path_operator parent wrapper_key specified_sub_dir exists specified_sub_dir mkdir exist_ok=True cmake_path = str Path specified_sub_dir CMakeLists txt _compile_consts consts bytes platform str - str Load aot_inductor update value demand use_asm_build bool = config aot_inductor use_consts_asm_build platform == linux graph mutated_buffers OrderedSet graph constants keys data section between text bss When size data large during linking relocation text against bss may overflow Rename ldata so won t between text bss section len consts _ _ _ raise ValueError Models buffer mutation included doesn t support constants greater than GB section_attr = ldata aw section_attr = lrodata symbol_prefix = platform == darwin section_attr = __DATA __data symbol_prefix = _ platform == win symbol_prefix = ASM build supported Windows force use CPP build use_asm_build = False raise RuntimeError f Unsupported platform platform Intel compiler failed compile manually constructed assembly file Switch XPU use consts cpp build device_type == xpu use_asm_build = False is_large_consts = len consts is_zero_size_consts = len consts == format_consts_to_gnu_asm consts bytes align_bytes int symbol_prefix str is_large_consts bool - tuple str str consts_asm = f \t section\t section_attr \n consts_asm += f \t balign align_bytes \n consts_asm += f \t globl\t symbol_prefix _binary_constants_bin_start\n consts_asm += f symbol_prefix _binary_constants_bin_start \n is_large_consts c consts consts_asm += f \t byte c \n Add one element even constants empty Otherwise assembler will put them data section consts consts_asm += \t space \n consts_asm += \t quad x abcdef\n consts_asm += f \t space len consts - \n consts_asm += f globl\t symbol_prefix _binary_constants_bin_end\n consts_asm += f symbol_prefix _binary_constants_bin_end \n consts_asm weights S Use c++ convert consts object file can support more compilers such msvc icx format_consts_to_cpp consts bytes align_bytes int symbol_prefix str - tuple str str consts_size = len consts asan_attr = #if defined __clang__ &#124; &#124; defined __GNUC__ \t\n\ #define ATTRIBUTE_NO_SANITIZE_ADDRESS __attribute__ no_sanitize address \t\n\ #else\t\n\ #define ATTRIBUTE_NO_SANITIZE_ADDRESS\t\n\ #endif\t\n\ \t\n\ ATTRIBUTE_NO_SANITIZE_ADDRESS\t\n const_cpp = asan_attr const_cpp += f alignas align_bytes extern const_cpp += f unsigned char symbol_prefix _binary_constants_bin_start consts_size = \t\n count_bytes = c consts const_cpp += f c count_bytes = count_bytes + count_bytes == const_cpp += \t\n const_cpp += \t\n const_cpp += f alignas align_bytes extern unsigned char symbol_prefix _binary_constants_bin_end \t\n const_cpp weights cpp get_zero_consts_asm_code align_bytes int symbol_prefix str - tuple str str This function handles zero-sized constants because C++ standard prohibits zero-length arrays https stackoverflow com questions what-happens-if-i-define-a- -size-array-in-c-c On Windows MSVC The compiler reports error C zero-sized arrays https learn microsoft com en-us cpp error-messages compiler-errors- compiler-error-c Solution Use assembly compilation handle case Why use Win assembly all paths ml only supports alignment up bytes which isn t optimal performance Cross-platform implementation Linux Added -pedantic disable zero-sized arrays C++ compiler Windows MSVC naturally rejects zero-sized arrays default _IS_WINDOWS Windows ml max support align no effect zero size data asm_code = option casemap none data _binary_constants_bin_start PAEA align _binary_constants_bin_end PAEA align public _binary_constants_bin_start PAEA public _binary_constants_bin_end PAEA end asm_ext = asm asm_code = f \t section\t section_attr \n asm_code += f \t balign align_bytes \n asm_code += f \t globl\t symbol_prefix _binary_constants_bin_start\n asm_code += f symbol_prefix _binary_constants_bin_start \n asm_code += f globl\t symbol_prefix _binary_constants_bin_end\n asm_code += f symbol_prefix _binary_constants_bin_end \n asm_ext = S asm_code asm_ext use_asm_build consts_code code_ext = format_consts_to_gnu_asm consts ALIGN_BYTES symbol_prefix is_large_consts is_zero_size_consts consts_code code_ext = get_zero_consts_asm_code ALIGN_BYTES symbol_prefix consts_code code_ext = format_consts_to_cpp consts ALIGN_BYTES symbol_prefix _ consts_s = write consts_code code_ext specified_dir=str specified_sub_dir key=config aot_inductor model_name_for_generated_files consts_s = Path consts_s object_build_options = CppTorchDeviceOptions device_type=device_type aot_mode=graph aot_mode compile_only=True use_relative_path=use_relative_path object_builder = CppBuilder name=str consts_s stem sources=str consts_s output_dir=str consts_s parent BuildOption=object_build_options consts_o = object_builder get_target_file_path use_asm_build False is_zero_size_consts run_asm_build_object str consts_s consts_o str consts_s parent object_builder build is_large_consts use_asm_build open consts_o r+b f f seek hdr = f read Search magic number write actual data over start_idx = hdr find b \xef\xcd\xab\x \x \x \x \x sys byteorder == little hdr find b \x \x \x \x \x \xab\xcd\xef assert start_idx = - f seek start_idx pos = while pos len consts rc = f write consts pos pos += rc Remove S file save space os remove consts_s consts_o torch utils _filelock FileLock lock_dir = get_lock_dir lock = FileLock os path join lock_dir wrapper_key + lock timeout=LOCK_TIMEOUT lock serialized_extern_kernel_nodes extern_kernel_nodes_json = str wrapper_path_operator with_suffix json open extern_kernel_nodes_json w f f write serialized_extern_kernel_nodes config aot_inductor package generated_files append extern_kernel_nodes_json metadata = config aot_inductor metadata metadata AOTI_DEVICE_KEY = device_type Add environment information ensure so compatibility metadata update get_device_information device_type Save user provided metadata meta_json = str wrapper_path_operator with_name f wrapper_path_operator stem _metadata json k v config aot_inductor metadata items assert isinstance k str isinstance v str Metadata must only contain strings open meta_json w f f write json dumps config aot_inductor metadata kernel_meta_json = str kernel_path_operator with_name f kernel_path_operator stem _metadata json shutil copy meta_json kernel_meta_json config aot_inductor package generated_files append meta_json config aot_inductor package_cpp_only generated_files append kernel_meta_json output_so = config aot_inductor output_path specified_artifact_name str wrapper_path_operator with_suffix so all_cuda = all graph get_original_value_of_constant name is_cuda name graph constants keys name graph folded_constants _to_bytes t torch Tensor all_cuda bool - bytes _pad_to_alignment raw_bytes bytes - bytes padded_bytes = raw_bytes ljust len raw_bytes + ALIGN_BYTES - ALIGN_BYTES ALIGN_BYTES b \x padded_bytes This serializes tensor s untyped_storage bytes accessing raw data underlying structure ctypes t numel == b t is_mkldnn data_ptr = torch ops mkldnn data_ptr t nbytes = torch ops mkldnn _nbytes t t_cpu = t untyped_storage cpu data_ptr = t_cpu data_ptr nbytes = t_cpu nbytes raw_array = ctypes cast data_ptr ctypes POINTER ctypes c_ubyte nbytes pyrefly ignore missing-attribute raw_bytes = bytes raw_array contents raw_bytes all_cuda _pad_to_alignment raw_bytes config aot_inductor package_constants_in_so config aot_inductor package_constants_on_disk_format == binary_blob serialized_weights = b join _to_bytes graph get_original_value_of_constant name all_cuda name graph constants keys name graph folded_constants serialized_weights = b config aot_inductor package_constants_on_disk_format == pickle_weights We need storage key here because original value tensor might clone weights_dict = Weights graph allocated_constant_name name graph get_original_value_of_constant name TensorProperties graph constants name name graph constants keys name graph folded_constants generated_files append weights_dict consts_size = len serialized_weights use_external_weights use_mmap_weights = determine_aoti_mmap_flags consts_size use_external_weights use_mmap_weights Should never reach here just check sanity raise RuntimeError use_external_weights use_mmap_weights cannot both True external_weights_path = None use_external_weights external_weights_filename = f wrapper_path_operator stem _weights blob external_weights_path = str wrapper_path_operator with_name external_weights_filename compile_command dict str Any = aot_mode graph aot_mode device_type device_type use_mmap_weights use_mmap_weights use_mmap_weights_external use_external_weights use_relative_path use_relative_path vec_isa picked_vec_isa If we re packaging via CMake we build whole code max optimization wrapper_build_options = CppTorchDeviceOptions compile_only=True min_optimize=not config aot_inductor package_cpp_only compile_command kernel_build_options = CppTorchDeviceOptions compile_only=True compile_command potentially precompile AOT header device config aot_inductor precompile_headers _IS_WINDOWS header_file = _get_cpp_wrapper_header device_type aot_mode=graph aot_mode wrapper_build_options precompiled_header = _precompile_header header_file cpp_command min_optimize=not config aot_inductor package_cpp_only compile_command cpp_prefix = _get_cpp_prefix_header device_type kernel_build_options precompiled_header = _precompile_header cpp_prefix cpp_command compile_command wrapper_builder = CppBuilder name=str wrapper_path_operator stem sources=wrapper_path output_dir=str wrapper_path_operator parent BuildOption=wrapper_build_options wrapper_compile_cmd = wrapper_builder get_command_line wrapper_o = wrapper_builder get_target_file_path kernel_builder = CppBuilder name=str kernel_path_operator stem sources=kernel_path output_dir=str wrapper_path_operator parent BuildOption=kernel_build_options kernel_compile_cmd = kernel_builder get_command_line kernel_o = kernel_builder get_target_file_path log debug aot wrapper compilation command s wrapper_compile_cmd log debug aot kernel compilation command s kernel_compile_cmd config aot_inductor package_cpp_only Not doing actual compilation here compile_flags = str wrapper_path_operator with_name f wrapper_path_operator stem _compile_flags json wrapper_build_options save_flags_to_json compile_flags generated_files append compile_flags wrapper_builder save_compile_cmd_to_cmake cmake_path device_type wrapper_builder save_src_to_cmake cmake_path wrapper_path generated_files append cmake_path try wrapper_builder build except exc CppCompileError SkipFrame e too big optimize str e raise RuntimeError Please use torch _inductor config aot_inductor compile_wrapper_opt_level = O flag e raise e kernel_builder build use_mmap_weights aot_constants = serialized_weights magic_number = use_external_weights aot_constants = struct pack q consts_size assert external_weights_path None For external weights write weights separate file embed minimal placeholder open external_weights_path wb f_weights f_weights write serialized_weights generated_files append external_weights_path we ll append weights binary end so file mmap when loading magic_number = cast int torch randint torch iinfo torch int max item aot_constants = struct pack qq consts_size + magic_number consts_o = _compile_consts aot_constants sys platform custom_obj_idx = Note custom_objs_config json file different model_constants_config json file produced package_sigmoid The keys custom_objs_config json directly correspond arg name extern nodes json The key model_constants_config json produced package_sigmoid attribute name user model code qual_name_to_id = Map constant name its name constants folder custom_obj_idx name constant enumerate graph torchbind_constants items isinstance constant torch _library fake_class_registry FakeScriptObject constant = constant real_obj assert isinstance constant torch _C ScriptObject custom_obj_name = f CUSTOM_OBJ_FILENAME_PREFIX custom_obj_idx log debug saving script object s s name custom_obj_name qual_name_to_id name = custom_obj_name custom_obj_bytes = torch _C _pickle_save constant custom_obj_path = os path join wrapper_path_operator parent custom_obj_name write_atomic custom_obj_path custom_obj_bytes True generated_files append custom_obj_path qual_name_to_id constants_config_json = os path join wrapper_path_operator parent custom_objs_config json open constants_config_json w f f write json dumps qual_name_to_id generated_files append constants_config_json gpu_codecache ROCmCodeCache &#124; CUDACodeCache = ROCmCodeCache torch version hip CUDACodeCache gpu_kernels_o = gpu_codecache aot_kernels_o copy clear list aot kernels after each linking gpu_codecache aot_kernels_o clear gpu_kernels_o assert config aot_inductor emit_multi_arch_kernel TODO add emit_multi_arch_kernel support cutlass kernels cubins_o = asm_files = _IS_WINDOWS ld objcopy = get_ld_and_objcopy use_relative_path kernels = getattr V graph wrapper_code _kernel_name_to_body kernel_name value CudaKernelParamCache cache items kernel_name kernels It possible CudaKernelParamCache contains more Triton kernels than what current graph uses continue asm_file = value asm asm_files append asm_file cubin_file = value get_cpp_wrapper_cubin_path_name config aot_inductor emit_multi_arch_kernel device_type == cuda current_arch = _nvcc_arch_as_compile_option cmd = pyrefly ignore unbound-name f _cuda_compiler -fatbin asm_file -o cubin_file Triton only allows generating PTX version same current arch f -gencode arch=compute_ current_arch code=compute_ current_arch Include SASS current specific arch f -gencode arch=compute_ current_arch code=sm_ current_arch try subprocess run cmd split capture_output=True text=True check=True except subprocess CalledProcessError e print f cmd failed \nstdout \n e stdout \nstderr \n e stderr file=sys stderr raise config aot_inductor embed_kernel_binary Embed cubin files into model so using objcopy cubins_o append convert_cubin_to_obj cubin_file kernel_name ld objcopy output_name output_dir = get_name_and_dir_from_output_file_path output_so so_build_options = CppTorchDeviceOptions vec_isa=picked_vec_isa device_type=device_type aot_mode=graph aot_mode use_relative_path=use_relative_path obj_srcs = wrapper_o kernel_o consts_o gpu_kernels_o cubins_o so_builder = CppBuilder name=output_name sources=obj_srcs output_dir=output_dir BuildOption=so_build_options link_cmd = so_builder get_command_line output_so = so_builder get_target_file_path log debug aot linkage command s link_cmd Append cmds end codegen-ed wrapper file open wrapper_path f f write \n f write f Compile cmd\n wrapper_compile_cmd \n f write f Link cmd\n link_cmd \n open kernel_path f f write \n f write f Compile cmd\n kernel_compile_cmd \n f write f Link cmd\n link_cmd \n config aot_inductor package_cpp_only linker_flags = str wrapper_path_operator with_name f wrapper_path_operator stem _linker_flags json so_build_options save_flags_to_json linker_flags generated_files append linker_flags generated_files append _LINKER_SCRIPT If we only want package cpp then we need save weights separately into bin we also need prevent compiling so use_mmap_weights weight_file = str wrapper_path_operator with_name f wrapper_path_operator stem _serialized_weights bin open weight_file wb f_weights f_weights write serialized_weights f_weights write struct pack q magic_number generated_files append weight_file TODO unify always use mmap_weights generated_files append consts_o so_builder save_src_to_cmake cmake_path consts_o config aot_inductor emit_multi_arch_kernel so_builder save_kernel_asm_to_cmake cmake_path asm_files generated_files extend asm_files obj_srcs = gpu_kernels_o cubins_o generated_files extend obj_srcs obj obj_srcs so_builder save_src_to_cmake cmake_path obj so_builder save_link_cmd_to_cmake cmake_path so_builder build o_file obj_srcs o_file gpu_kernels_o continue Remove these they needed anymore os remove o_file use_mmap_weights config aot_inductor cross_target_platform == windows raise RuntimeError when cross_target_platform windows use_mmap_weights should true get_page_size - int Don t use resource getpagesize Windows Unix specific package seen https docs python org library resource html _IS_WINDOWS ctypes type ignore attr-defined byref Structure windll ctypes wintypes DWORD LPVOID WORD SYSTEM_INFO Structure _fields_ = wProcessorArchitecture WORD wReserved WORD dwPageSize DWORD lpMinimumApplicationAddress LPVOID lpMaximumApplicationAddress LPVOID dwActiveProcessorMask DWORD dwNumberOfProcessors DWORD dwProcessorType DWORD dwAllocationGranularity DWORD wProcessorLevel WORD wProcessorRevision WORD si = SYSTEM_INFO windll kernel GetSystemInfo byref si sys_page_size = si dwPageSize resource sys_page_size = resource getpagesize sys_page_size page_size_ = get_page_size page_size = max page_size_ open output_so a+b f_so so_size = f_so tell Page align weights f_so write b page_size - so_size page_size f_so write serialized_weights f_so write struct pack q magic_number config aot_inductor package generated_files append output_so config trace provenance_tracking_level = kernel_info = torch _inductor debug create_kernel_information_json kernel_info_json = os path join wrapper_path_operator parent kernel_information json open kernel_info_json w f f write json dumps kernel_info indent= generated_files append kernel_info_json config aot_inductor package We want directory contains all AOTI generated files just so os path split output_so generated_files output_so _libgomp CDLL &#124; None = None custom_op_wrapper op str args Any - list c_void_p &#124; c_void_p &#124; None This function will called generated cpp wrapper code JIT mode Because tensors will passed AtenTensorHandle we need explicitly convert them convert_arg arg Any - Any str type arg == PyCapsule No easy way do isinstance check PyCapsule torch _C _aoti alloc_tensor_by_stealing_from_void_ptr arg isinstance arg list tuple type arg convert_arg arg arg converted_args = convert_arg arg arg args assert op startswith torch ops op + can called through custom_op_wrapper func = None i s enumerate op split i == func = importlib import_module s func = getattr func s assert callable func op + can loaded through custom_op_wrapper convert any kwarg-only arguments kwargs kwargs = dict pyrefly ignore missing-attribute func_arg conv_arg zip func _schema arguments converted_args func_arg kwarg_only kwargs func_arg name = conv_arg kwargs del converted_args -len kwargs result = func converted_args kwargs result None None isinstance result list tuple unsafe_alloc_void_ptrs_from_tensors expects result contains tensor only result = torch tensor r None r r result r result assert isinstance r torch Tensor op + returns list non-tensors torch _C _aoti unsafe_alloc_void_ptrs_from_tensors result type ignore arg-type assert isinstance result torch Tensor op + returns non-tensor torch _C _aoti unsafe_alloc_void_ptr_from_tensor result Precompiled headers persistent past program runtime associated one specific compiler version set flags We explicitly use default_cache_dir here because these headers need global rather than ignored fresh_cache _HEADER_DIR = os path join default_cache_dir precompiled_headers _HEADER_LOCK_DIR = os path join _HEADER_DIR locks functools cache _precompile_header header str hashable_cmd_line str compile_command Any - str assert _IS_WINDOWS CppBuilder does currently support precompiling Windows Get preprocessed output header file precompiled This allows us properly invalidate file cache when any header dependency changes This thread-safe each thread will get its own temporary directory N B we can t use NamedTemporaryFile here because Windows errors out attempts read file open write handle tempfile TemporaryDirectory preprocessing_dir preprocessing_header = Path preprocessing_dir header hpp preprocessing_header write_text f #include header \n preprocessor = CppBuilder name=str preprocessing_header - strip off hpp extension sources=str preprocessing_header BuildOption=CppTorchDeviceOptions compile_command preprocessing=True preprocessor build _get_file_checksum filename str - str Reading whole preprocessed header hashing very expensive calling fast hashing utility subprocess cheap If Windows support needs added here use certutil -hashfile cmd_output = subprocess run openssl sha filename capture_output=True text=True cmd_output stdout split - preprocessor_hash = _get_file_checksum preprocessor get_target_file_path header_build_option = CppTorchDeviceOptions compile_command precompiling=True header_hash header_full_path = write content=f #include header \n extension= h extra= hashable_cmd_line + preprocessor_hash + get_compiler_version_info header_build_option get_compiler specified_dir=_HEADER_DIR cpp_builder = CppBuilder name=header_full_path sources=header_full_path BuildOption=header_build_option _worker_compile_cpp will automatically ignore any compilation whose result already exists so always safe os makedirs _HEADER_LOCK_DIR exist_ok=True _worker_compile_cpp os path join _HEADER_LOCK_DIR f header_hash lock cpp_builder header_full_path _get_cpp_prefix_header device str - str &#124; None device startswith cpu torch csrc inductor cpp_prefix h None _get_cpp_wrapper_header device str aot_mode bool = False - str Given device type optionally whether we re AOT Inductor mode returns path cpp_wrapper header file precompiled base_device = device split maxsplit= is_array_ref = config aot_inductor allow_stack_allocation base_device == cpu torch csrc inductor f aoti_include aot_mode cpp_wrapper f array_ref is_array_ref base_device h clear_on_fresh_cache CppCodeCache Compiles caches C++ libraries Users supply source code compiled while compilation flags set CppBuilder cache dict str Callable CDLL &#124; ModuleType = cache_clear = staticmethod cache clear cpp_compile_command_flags dict str Any = staticmethod _load_library_inner path str key str - CDLL &#124; ModuleType cdll LoadLibrary path classmethod _load_library cls path str key str - CDLL &#124; ModuleType try result = cls _load_library_inner path key result key = key type ignore union-attr result except ImportError OSError e gomp str e os path exists usr lib libgomp so hacky workaround fbcode buck global _libgomp _libgomp = cdll LoadLibrary usr lib libgomp so result = cls _load_library_inner path key result key = key type ignore union-attr result failed map segment shared object str e raise OSError f e The most common reason may occur tempfile gettempdir folder mounted noexec e g default Docker mounts tmp file systems f noexec Please remount tempfile gettempdir exec enabled set another temporary directory TORCHINDUCTOR_CACHE_DIR environment variable e raise classmethod _get_uncompiled_header cls device str - str &#124; None Given device type returns path CPP header file precompiled None classmethod load_async cls main_code str device_type str = cpu submit_fn Any = None extra_flags Sequence str = optimized_code str &#124; None = None - Any Compile load C++ library Returns callable returns loaded library compile_command = cls cpp_compile_command_flags device_type device_type extra_flags extra_flags use_relative_path config is_fbcode vec_isa pick_vec_isa _set_gpu_runtime_env cpp_extension consults env Note distinction between two booleans We do minimal optimization optimized_code argument present all since s how user function opts we do compilation linking one step optimized_code argument empty micro-optimization main_build_option = CppTorchDeviceOptions compile_only=bool optimized_code min_optimize=optimized_code None pyrefly ignore bad-argument-type compile_command optimized_build_option = CppTorchDeviceOptions pyrefly ignore bad-argument-type compile_only=True pyrefly ignore bad-argument-type compile_command get_hashable_command_line build_option BuildOptionsBase - str Writing code file will calculate hash which we need vary command line flags change This implements mostly-generic way validating CppBuilder name= o sources= i BuildOption=build_option get_command_line main_cmd_line = get_hashable_command_line main_build_option optimized_cmd_line = get_hashable_command_line optimized_build_option key main_path = write main_code main cpp extra=f optimized_code main_cmd_line Don t bother writing argument empty optimized_code _ optimized_path = write optimized_code optimized cpp extra=optimized_cmd_line Unused makes type checkers happy optimized_path = os devnull key cls cache torch utils _filelock FileLock lock_path = os path join get_lock_dir key + lock future Future Any &#124; None = None lib = None requested pre-compile any headers config cpp_cache_precompile_headers _IS_WINDOWS header = cls _get_uncompiled_header device_type main_build_option precompiled_header = _precompile_header header main_cmd_line min_optimize=optimized_code None compile_command Currently optimized_code field only used cpp kernel code so go ahead precompile relevant header here Revisit decision ever changes optimized_code header = _get_cpp_prefix_header device_type optimized_build_option precompiled_header = _precompile_header pyrefly ignore unbound-name header optimized_cmd_line compile_command main_name output_dir = get_name_and_dir_from_output_file_path main_path main_builder = CppBuilder name=main_name sources=main_path BuildOption=main_build_option output_dir=output_dir optimized_code optimized_name _ = get_name_and_dir_from_output_file_path optimized_path optimized_builder = CppBuilder name=optimized_name sources=optimized_path BuildOption=optimized_build_option output_dir=output_dir linker = CppBuilder name=main_name sources= main_builder get_target_file_path optimized_builder get_target_file_path pyrefly ignore bad-argument-type BuildOption=CppTorchDeviceOptions compile_command output_dir=output_dir worker_fn = functools partial _worker_compile_cpp lock_path main_builder optimized_builder linker binary_path = normalize_path_separator linker get_target_file_path worker_fn = functools partial _worker_compile_cpp lock_path main_builder binary_path = normalize_path_separator main_builder get_target_file_path load_fn - Any nonlocal lib lib None future None future result result = worker_fn assert result None lib = cls _load_library binary_path key assert lib None lib submit_fn None FileLock lock_path timeout=LOCK_TIMEOUT os path exists binary_path future = submit_fn worker_fn cls cache key = load_fn cls cache key classmethod load cls args Any kwargs Any - Any cls load_async args kwargs _worker_compile_cpp lock_path str cpp_builders Sequence CppBuilder - None torch utils _filelock FileLock FileLock lock_path timeout=LOCK_TIMEOUT builder cpp_builders os path exists builder get_target_file_path builder build Customized Python binding cpp kernels clear_on_fresh_cache CppPythonBindingsCodeCache CppCodeCache cache dict str Callable CDLL &#124; ModuleType = cache_clear = staticmethod cache clear cpp_compile_command_flags = kernels have no dependency libtorch include_pytorch False shared True entry_function = kernel call_entry_function = kernel Py_RETURN_NONE extra_parse_arg = suffix_template = textwrap dedent Python bindings call entry_func #define PY_SSIZE_T_CLEAN #include Python h #include sstream #include cstdlib #ifndef _MSC_VER #if __cplusplus L C++ earlier code https en cppreference com w cpp language attributes likely #define likely x __builtin_expect x #define unlikely x __builtin_expect x #endif #else #define likely x x #define unlikely x x #endif This defined guards cpp so we don t need PyTorch headers slooow We manually link below workaround issues fbcode build static void _torchinductor_pyobject_tensor_data_ptr PyObject obj template typename T static inline T parse_arg PyObject args size_t n static_assert std is_pointer_v T arg type must pointer long static_cast T _torchinductor_pyobject_tensor_data_ptr PyTuple_GET_ITEM args n template inline int _t parse_arg int _t PyObject args size_t n auto result = PyLong_AsSsize_t PyTuple_GET_ITEM args n unlikely result == - PyErr_Occurred throw std runtime_error expected int arg result template inline uintptr_t parse_arg uintptr_t PyObject args size_t n auto result = PyLong_AsVoidPtr PyTuple_GET_ITEM args n unlikely result == reinterpret_cast void - PyErr_Occurred throw std runtime_error expected int arg reinterpret_cast uintptr_t result extra_parse_arg static PyObject entry_func _py PyObject PyObject args try unlikely PyTuple_CheckExact args throw std runtime_error tuple args required unlikely PyTuple_GET_SIZE args = arg_len throw std runtime_error requires arg_len args call_entry_func catch std exception const e PyErr_SetString PyExc_RuntimeError e what nullptr catch PyErr_SetString PyExc_RuntimeError unhandled error nullptr static PyMethodDef py_methods = entry_func entry_func _py METH_VARARGS NULL NULL NULL static struct PyModuleDef py_module = PyModuleDef_HEAD_INIT entry_func NULL - py_methods PyMODINIT_FUNC PyInit_ entry_func void const char str_addr = std getenv _TORCHINDUCTOR_PYOBJECT_TENSOR_DATA_PTR str_addr PyErr_SetString PyExc_RuntimeError _TORCHINDUCTOR_PYOBJECT_TENSOR_DATA_PTR must set nullptr std istringstream iss str_addr uintptr_t addr = iss addr _torchinductor_pyobject_tensor_data_ptr = reinterpret_cast decltype _torchinductor_pyobject_tensor_data_ptr addr PyObject module = PyModule_Create py_module module == NULL NULL #ifdef Py_GIL_DISABLED PyUnstable_Module_SetGIL module Py_MOD_GIL_NOT_USED #endif module classmethod pyrefly ignore bad-override _load_library_inner cls path str key str - ModuleType os environ _TORCHINDUCTOR_PYOBJECT_TENSOR_DATA_PTR = str torch _C _dynamo guards _torchinductor_pyobject_tensor_data_ptr type ignore attr-defined module_name = f key cls entry_function try sys modules module_name except KeyError pass spec = importlib util spec_from_file_location module_name path assert spec None module = importlib util module_from_spec spec sys modules module_name = module assert spec loader None spec loader exec_module module module classmethod _get_uncompiled_header cls device str - str &#124; None _get_cpp_prefix_header device classmethod load_pybinding_async cls argtypes Sequence str main_code str device_type str = cpu num_outputs int = - submit_fn Any = None extra_flags Sequence str = kernel_code str &#124; None = None - Any Wrap C++ function fast Python bindings Args argtypes The types args ENTRY_FUNCTION e g float long main_code C++ source code containing ENTRY_FUNCTION Will built -O kernel_code None maximize performance any kernels present -O otherwise minimize compile time kernel_code If present C++ source code will built -O linked main_code Returns A python version ENTRY_FUNCTION parseargs = join f parse_arg argtype replace const args n n argtype enumerate argtypes suffix = cls suffix_template format arg_len=len argtypes call_entry_func=cls call_entry_function format parseargs entry_func=cls entry_function extra_parse_arg=cls extra_parse_arg format array_len=num_outputs get_result = cls load_async main_code + suffix device_type submit_fn=submit_fn extra_flags=extra_flags optimized_code=kernel_code result = None future - Any nonlocal result result None result = get_result assert isinstance result ModuleType getattr result cls entry_function future classmethod load_pybinding cls args Any kwargs Any - Any cls load_pybinding_async args kwargs clear_on_fresh_cache CppWrapperCodeCache CppPythonBindingsCodeCache cache dict str Callable CDLL &#124; ModuleType = cache_clear = staticmethod cache clear cpp_compile_command_flags = include_pytorch True shared True entry_function = inductor_entry_cpp call_entry_function = inductor_entry_cpp extra_parse_arg = textwrap dedent #include torch csrc inductor aoti_torch c shim h static inline std vector AtenTensorHandle unpack_tensor_handle_list PyObject pyvec std vector AtenTensorHandle result size_t result_len = PyList_GET_SIZE pyvec result reserve result_len size_t i = i result_len i++ AtenTensorHandle essentially pointer void elem = PyCapsule_GetPointer PyList_GET_ITEM pyvec i NULL result push_back reinterpret_cast AtenTensorHandle elem result static inline PyObject pack_tensor_handle_list const std array AtenTensorHandle array_len arr PyObject result = PyList_New array_len size_t i = i array_len i++ PyObject elem = arr i == nullptr Py_None Store AtenTensorHandle PyCapsulate PyCapsule_New reinterpret_cast void arr i NULL NULL PyList_SET_ITEM result i elem result template inline std vector AtenTensorHandle parse_arg std vector AtenTensorHandle PyObject args size_t n unpack_tensor_handle_list PyTuple_GET_ITEM args n PyObject inductor_entry_cpp std vector AtenTensorHandle input_handles For outputs we only allocate array hold returned tensor handles actual output tensor storage std array AtenTensorHandle array_len output_handles try inductor_entry_impl input_handles data output_handles data PyErr_Occurred nullptr pack_tensor_handle_list output_handles catch std exception const e PyErr_SetString PyExc_RuntimeError e what nullptr catch PyErr_SetString PyExc_RuntimeError unhandled error nullptr classmethod _get_uncompiled_header cls device str - str &#124; None _get_cpp_wrapper_header device clear_on_fresh_cache HalideCodeCache CppPythonBindingsCodeCache cache dict str Callable ModuleType &#124; CDLL = cache_clear = staticmethod cache clear _standalone_runtime_path str &#124; None = None prefix = textwrap dedent #include halideruntime_h #include headerfile #include stdexcept #include cmath namespace c inline long div_floor_integer long long b = b const auto quot = b const auto rem = b rem quot - quot b glue_template_cpp = prefix + textwrap dedent void kernel argdefs buffers int err = halide_kernel buffer_names err = throw std runtime_error halide_kernel failed glue_template_cuda = prefix + textwrap dedent #include cuda h static const halide_device_interface_t cuda_interface = halide_cuda_device_interface void kernel argdefs uintptr_t stream buffers int err = halide_kernel reinterpret_cast void stream buffer_names err = throw std runtime_error halide_kernel failed standalone_runtime_cuda_init = textwrap dedent #include #include cuda h static int acquire_context void user_context void cuda_context_out bool create cuCtxGetCurrent reinterpret_cast CUcontext cuda_context_out static int release_context void user_context static int get_stream void user_context void cuda_context void stream_out stream_out = user_context static int register_halide_hooks halide_set_cuda_acquire_context acquire_context halide_set_cuda_release_context release_context halide_set_cuda_get_stream get_stream int inductor_register_halide_hooks_result = register_halide_hooks classmethod _codegen_buffer cls name str arg HalideInputSpec cuda bool - list str assert arg shape None assert arg stride None len arg shape == len arg stride assert arg offset None data_ptr = f arg alias_of arg name + arg offset cuda device = f reinterpret_cast uint _t data_ptr device_interface = cuda_interface host = nullptr flags = halide_buffer_flag_device_dirty device = device_interface = nullptr host = f reinterpret_cast uint _t data_ptr flags = halide_buffer_flag_host_dirty dims = size stride zip arg shape arg stride dims append f halide_dimension_t size stride f halide_buffer_t name f halide_dimension_t name _dims = join dims len dims f halide_dimension_t name _dims = nullptr f name device = device f name device_interface = device_interface f name host = host f name flags = flags f name type = arg halide_type f name dimensions = len dims f name dim = name _dims f name padding = nullptr classmethod _codegen_glue cls meta HalideMeta headerfile object - str is_cuda = meta is_cuda assert is_cuda user_context meta target assert no_runtime meta target buffers = buffer_names = i arg enumerate meta argtypes arg is_buffer pyrefly ignore bad-argument-type buffer_names append f hl_buf_ i buffers extend cls _codegen_buffer f hl_buf_ i arg is_cuda assert arg ctype pyrefly ignore bad-argument-type buffer_names append arg name buffers = \n join f line line buffers lstrip glue_template = cls glue_template_cuda is_cuda cls glue_template_cpp glue_code = glue_template format halideruntime_h=cls find_header HalideRuntimeCuda h is_cuda HalideRuntime h headerfile=headerfile argdefs= join f bindings_type name meta argtypes alias_of None buffers=buffers buffer_names= join buffer_names glue_code classmethod functools cache config_hash cls - str command_gen = CppBuilder name= O sources= I BuildOption=CppOptions command_line = command_gen get_command_line sha _hash \n join cls glue_template_cpp cls glue_template_cuda cls standalone_runtime_cuda_init command_line encode utf- staticmethod _search_for_file suffix str errmsg str - str spec = importlib machinery PathFinder find_spec halide spec None spec submodule_search_locations raise RuntimeError halide python bindings installed try search = spec submodule_search_locations file os listdir search file endswith so try out = subprocess check_output ldd os path join search file except subprocess SubprocessError continue m = re search r libHalide so out decode utf- m path = os path join os path abspath m group suffix os path exists path os path abspath path except Exception e raise RuntimeError errmsg e raise RuntimeError errmsg staticmethod functools cache find_libautoschedule name str - str sofile = f libautoschedule_ name lower so HALIDE_LIB os environ path = os path join os environ HALIDE_LIB sofile os path exists path path errmsg = f Can t find sofile set env HALIDE_LIB directory containing HalideCodeCache _search_for_file sofile errmsg staticmethod functools cache find_header name str - str HALIDE_INCLUDE os environ path = os path join os environ HALIDE_INCLUDE name os path exists path path HALIDE_LIB os environ path = os path abspath os path join os environ HALIDE_LIB f include name os path exists path path errmsg = f Can t find name set env HALIDE_INCLUDE directory containing HalideCodeCache _search_for_file f include name errmsg classmethod generate_halide_async cls meta HalideMeta source_code str submit_fn Any = None - Callable Any dirpath = Path get_path code_hash source_code extra=repr cls config_hash meta halide os makedirs dirpath exist_ok=True wait_for_compile = None genfile = str dirpath generate_kernel py libfile = str dirpath halide_kernel headerfile = str dirpath halide_kernel h donefile = str dirpath done lockfile = str dirpath lock need_compile = os path exists donefile jobs list Any = need_compile write_atomic genfile source_code cmd = sys executable genfile -g kernel -o f dirpath -f halide_kernel -e static_library h schedule meta scheduler cmd extend -p cls find_libautoschedule meta scheduler cmd extend meta args jobs append functools partial subprocess check_call cmd binding_types = arg bindings_type arg meta argtypes arg alias_of None meta is_cuda binding_types append uintptr_t stream bindings_future = cls load_pybinding_async binding_types cls _codegen_glue meta headerfile extra_flags= libfile cls build_standalone_runtime submit_fn=jobs append need_compile None device_type= cuda meta is_cuda cpu need_compile jobs append functools partial touch donefile task = functools partial _worker_task_halide lockfile jobs submit_fn wait_for_compile = submit_fn task result task load - Callable Any wait_for_compile wait_for_compile bindings_future load classmethod generate_halide cls args Any kwargs Any - Callable Any cls generate_halide_async args kwargs classmethod build_standalone_runtime cls - str cls _standalone_runtime_path os path exists cls _standalone_runtime_path cls _standalone_runtime_path device_type = cuda torch cuda is_available cpu libname = libStandaloneHalideRuntime so target = host-cuda device_type == cuda host cls _standalone_runtime_path assert os path exists cls _standalone_runtime_path We hit case unittests when we run fresh_cache Generating fresh runtime over over causes errors because we initialize cuda hundreds times same process run out file descriptors Workaround jail breaking current fresh_cache base = default_cache_dir base = cache_dir dirpath = Path base f halide-runtime- target - cls config_hash os makedirs dirpath exist_ok=True done_file = str dirpath done lock_file = str dirpath lock hook_file = str dirpath hooks cpp a_file = str dirpath standalone_halide_runtime so_file = str dirpath libname os path exists done_file halide hl type ignore import-untyped import-not-found torch utils _filelock FileLock FileLock lock_file LOCK_TIMEOUT os path exists done_file open hook_file w f device_type == cuda f write cls standalone_runtime_cuda_init format cls find_header HalideRuntimeCuda h hl compile_standalone_runtime a_file hl Target target name output_dir = get_name_and_dir_from_output_file_path so_file halide_cmd_gen = CppBuilder name=name sources= hook_file a_file output_dir=output_dir BuildOption=CppTorchDeviceOptions device_type=device_type subprocess check_call shlex split halide_cmd_gen get_command_line touch done_file assert os path exists so_file cls _standalone_runtime_path = so_file so_file classmethod _get_uncompiled_header cls device str - str &#124; None Header precompiling currently disabled halide None _worker_task_halide lockfile str jobs list partial Any - None torch utils _filelock FileLock try FileLock lockfile LOCK_TIMEOUT job jobs job except subprocess SubprocessError e os environ get HALIDE_REPRO == cmd list Any python script cmd = getattr e cmd os path basename python startswith python code = open script read main = hl main assert code count main == Out __repr__ - str out ci = cmd index -o assert isinstance ci int pyrefly ignore unsupported-operation cmd ci + = Out repl = textwrap indent textwrap dedent f \ sys tempfile tempfile TemporaryDirectory out sys argv = repro py cmd r hl main code = code replace main repl open repro py w fd fd write code lstrip raise RuntimeError f wrote repro py e e raise touch filename str - None open filename close clear_on_fresh_cache PyCodeCache Track loaded modules so we can remove on-disk artifacts when clearing cache Note also we may load same path more than once attach different attributes i e due different constant values modules list ModuleType = Modules loaded without extra attributes stored here those do need re-loaded modules_no_attr dict str ModuleType = linemaps dict str list tuple Any = classmethod write cls source_code str extra str = - tuple str str write source_code py extra=extra classmethod load cls source_code str extra str = - ModuleType key path = write source_code py extra=extra cls load_by_key_path key path classmethod load_by_key_path cls key str path str linemap list tuple int str &#124; None = None attrs dict str Any &#124; None = None - ModuleType linemap None linemap = we only cache when attrs None attrs None path cls modules_no_attr cls modules_no_attr path in_toplevel = in_toplevel_process mod = _reload_python_module key path set_sys_modules=in_toplevel unzip into separate lines nodes lists in_toplevel cls linemaps path = list zip linemap attrs None k v attrs items setattr mod k v in_toplevel we only cache when attrs None attrs None cls modules_no_attr path = mod cls modules append mod mod classmethod cache_clear cls purge bool = False - None Clear in-memory module cache If purge=True also delete all corresponding on-disk source files purge mod cls modules try assert mod __file__ os remove mod __file__ except FileNotFoundError pass cls modules clear cls modules_no_attr clear classmethod functools cache stack_frames_for_code cls path str lineno int - list dict str Any &#124; None path cls linemaps None len cls linemaps path == None starting_line fx node lines nodes = cls linemaps path p = bisect_right lines lineno p == None entry = nodes p - entry None parse_stack_trace stack_trace str - list dict str Any ideally fx stores stack traces data rather than string along performance critical path regex = r File + line \d+ + \n matches = re findall regex stack_trace filename f line int l name n f l n reversed matches parse_stack_trace entry _load_triton_kernel_from_source kernel_name str source_code str - CachingAutotuner getattr PyCodeCache load source_code kernel_name _cuda_compiler - str &#124; None cuda_env nvcc_exist config cuda cuda_cxx config cuda cuda_cxx config is_fbcode os path join build_paths sdk_home bin nvcc cuda_env nvcc_exist os getenv CUDACXX os getenv CUDACXX cuda_env nvcc_exist os getenv CUDA_HOME os path realpath os path join os getenv CUDA_HOME bin nvcc nvcc _cutlass_path - str config is_fbcode libfb py parutil parutil get_dir_path cutlass- -headers config cuda cutlass_dir _cutlass_paths - list str include tools library include tools library src tools util include _clone_cutlass_paths build_root str - list str paths = _cutlass_paths cutlass_root = _cutlass_path path _cutlass_paths old_path = os path join cutlass_root path new_path = os path join build_root path shutil copytree old_path new_path dirs_exist_ok=True paths _cutlass_include_paths - list str cutlass_path = _cutlass_path Use realpath get canonical absolute paths order mess up cache keys os path realpath os path join cutlass_path path path _cutlass_paths torch_key_cache cutlass_key - bytes Compute key representing state CUTLASS library Note OSS fbcode will have different keys config is_fbcode importlib resources path cutlass_library src_hash txt resource_path open resource_path resource_file resource_file read encode combined_hash = hashlib sha build_code_hash config cuda cutlass_dir combined_hash combined_hash digest _cuda_lib_options - list str Util function CUTLASS backend find correct CUDA libraries _set_gpu_runtime_env cpp_extension consults env torch utils cpp_extension lpaths = cpp_extension library_paths device_type= cuda use_re_build lpaths += build_paths sdk_lib os path join build_paths sdk_lib stubs extra_ldflags list str = is_linux _transform_cuda_paths lpaths path lpaths torch lib path don t want depend pytorch continue extra_ldflags append f -L path -rpath ensures DLL can find its dependencies when loaded even library path non-standard But do add stubs folder rpath driver expected found runtime os path basename path = stubs extra_ldflags extend -Xlinker f -rpath= path extra_ldflags append -lcuda extra_ldflags append -lcudart raise NotImplementedError Unsupported env failed find cuda libs Currently only Linux supported extra_ldflags _nvcc_host_compiler_options - list str -fPIC -fno-strict-aliasing -fvisibility=hidden -Wconversion _nvcc_arch_as_compile_option - str arch = cuda_env get_cuda_arch arch == Required cutlass compilation arch == arch _nvcc_compiler_options - list str arch = _nvcc_arch_as_compile_option code = f sm_ arch f compute_ arch config cuda enable_cuda_lto code += f lto_ arch options = -t= -DCUTLASS_ENABLE_TENSOR_CORE_MMA= -DCUTLASS_ENABLE_SM _EXTENDED_MMA_SHAPES= -DCUTE_SM _EXTENDED_MMA_SHAPES_ENABLED -w f -gencode=arch=compute_ arch code= join code config cuda compile_opt_level -std=c++ -- expt-relaxed-constexpr -DNDEBUG config is_fbcode options extend -ccbin os path dirname build_paths gcc config cuda enable_debug_info options extend -lineinfo -g -DCUTLASS_DEBUG_TRACE_LEVEL= config cuda enable_ptxas_info options extend -- keep Keep intermediate files debugging including ptx sass cubin etc -- ptxas-options= -- warn-on-local-memory-usage warn us local memory used CUDA Kernels -- ptxas-options= -- warn-on-spills warn us register spilling happens CUDA Kernels -- resource-usage Report CUDA resource usage shared mem registers etc -- source-in-ptx Annotate ptx file source information config cuda use_fast_math options extend -- use_fast_math -DCUTLASS_USE_TANH_FOR_SIGMOID= options cuda_compile_command src_files list str dst_file str dst_file_ext str extra_args list str &#124; None = None - str extra_args None extra_args = use_re_build build_path = os path dirname dst_file include_paths = _clone_cutlass_paths build_path src_files = os path basename src_file src_file src_files dst_file = os path basename dst_file include_paths = _cutlass_include_paths cuda_lib_options = _cuda_lib_options nvcc_host_compiler_options = _nvcc_host_compiler_options nvcc_compiler_options = _nvcc_compiler_options options = nvcc_compiler_options + extra_args + f -Xcompiler opt = opt f -Xcompiler= opt opt nvcc_host_compiler_options + -I + path path include_paths + cuda_lib_options src_file = join src_files res = dst_file_ext == o res = f _cuda_compiler join options -c -o dst_file src_file dst_file_ext == so options append -shared res = f _cuda_compiler join options -o dst_file src_file dst_file_ext == exe res = f _cuda_compiler join options -o dst_file src_file raise NotImplementedError f Unsupported output file suffix dst_file_ext log isEnabledFor logging DEBUG log debug CUDA command s res autotuning_log debug CUDA command s res res DLLWrapper A wrapper dynamic library __init__ lib_path str - None lib_path = lib_path is_open = False DLL = cdll LoadLibrary lib_path is_open = True close - None is_open _dlclose is_open = False _dlclose - None f_dlclose = None is_linux syms = CDLL None hasattr syms dlclose Apline Linux syms = CDLL libc so hasattr syms dlclose f_dlclose = syms dlclose is_windows ctypes kernel = ctypes CDLL kernel use_last_error=True f_dlclose = kernel FreeLibrary raise NotImplementedError Unsupported env failed do dlclose f_dlclose None is_linux f_dlclose argtypes = c_void_p f_dlclose DLL _handle is_windows ctypes ctypes wintypes f_dlclose argtypes = wintypes HMODULE f_dlclose DLL _handle log warning dll unloading function found library may unloaded properly __getattr__ name str - Callable None is_open raise RuntimeError f Cannot use closed DLL library lib_path method = getattr DLL name _wrapped_func args Any - None err = method args err raise RuntimeError f Error function method __name__ _wrapped_func __enter__ - Self __exit__ args Any - None close __del__ - None close lru_cache binary_error_path output_path str - str standard format error path output_path + error clear_on_fresh_cache CUDACodeCache A cache managing compilation loading CUDA source code specifically CUTLASS This handles writing source code files compiling them into shared objects caching results avoid redundant compilations It also manages error handling logging compilation process dataclasses dataclass CacheEntry input_path str output_path str error_json str &#124; None = None cache dict str CacheEntry = aot_kernels_o list str = _SOURCE_CODE_SUFFIX = cu staticmethod cache_clear - None CUDACodeCache cache clear CUDACodeCache aot_kernels_o clear staticmethod lru_cache maxsize= get_kernel_binary_remote_cache caching_enabled bool caching_available bool - Any &#124; None Get create instance CUTLASSKernelBinaryRemoteCache Args caching_enabled Whether binary remote caching enabled caching_available Whether we re fbcode environment Returns CUTLASSKernelBinaryRemoteCache The instance kernel binary remote cache caching_enabled log debug CUTLASSKernelBinaryRemoteCache requested skipping None caching_available None try torch _inductor fb kernel_binary_remote_cache CUTLASSKernelBinaryRemoteCache CUTLASSKernelBinaryRemoteCache except ImportError log debug CUTLASSKernelBinaryRemoteCache available remote caching disabled None classmethod lru_cache None write cls source_code str dst_file_ext str - tuple str str Writes source code into file dst_file_ext file extension Returns hash key source code path file config cuda cutlass_hash_with_compile_cmd cuda_command = repr cuda_compile_command dummy_input dummy_output dst_file_ext extra = cuda_command extra = repr nvcc cuda hash _cuda_compiler cutlass flags gcc hash _nvcc_compiler_options flags _nvcc_host_compiler_options cutlass key cutlass_key hack deal AOTI o compilation key input_path = write source_code cls _SOURCE_CODE_SUFFIX extra=extra key input_path classmethod compile cls source_code str dst_file_ext str extra_args list str &#124; None = None - tuple str str str Compiles CUDA source_code into file dst_file_ext extension If dst_file_ext so first compiles o then links so Returns tuple dst_file_path hash_key source_code_path dst_file_ext == so Two-step compilation first compile o then link so obj_path _ _ = cls compile source_code o extra_args key input_path = cls write source_code dst_file_ext src_files operation_name = obj_path Linking Regular compilation non- so files key input_path = cls write source_code dst_file_ext src_files operation_name = input_path Compilation key_with_ext = key + dst_file_ext key_with_ext cls cache torch utils _filelock FileLock lock_dir = get_lock_dir lock = FileLock os path join lock_dir key + lock timeout=LOCK_TIMEOUT lock output_path = input_path -len cls _SOURCE_CODE_SUFFIX + dst_file_ext error_path = binary_error_path output_path binary_remote_cache = cls get_kernel_binary_remote_cache caching_enabled=config cuda use_binary_remote_cache config force_disable_caches caching_available=config is_fbcode binary_remote_cache None The remote cache implementation will only download file does already exist locally binary_remote_cache get output_path error_path os path exists error_path open error_path encoding= utf- fh error_json = fh read cmd_parts error_output = json loads error_json binary_remote_cache None config cuda upload_to_binary_remote_cache This ensures local error uploaded remote cache we make no assumptions about remote cache having same information local cache binary_remote_cache put error_path config cuda binary_remote_cache_force_write cls cache key_with_ext = CUDACodeCache CacheEntry input_path output_path error_json raise exc CUDACompileError cmd_parts error_output os path exists output_path cmd = cuda_compile_command src_files output_path dst_file_ext extra_args open input_path f f write \n f write f CUDA operation_name cmd\n cmd \n start_time = time log debug CUDA s s operation_name cmd cmd_parts = cmd split try use_re_build triton fb re_build_helper run_build_command run_build_command cmd_parts os path dirname input_path os path basename output_path subprocess check_output cmd_parts stderr=subprocess STDOUT env=os environ except subprocess CalledProcessError error cls _record_cuda_compile_error error output decode utf- key_with_ext cmd_parts input_path output_path binary_remote_cache raise exc CUDACompileError cmd_parts error output error except Exception error COMPILE FAILED WITH str error cls _record_cuda_compile_error str error key_with_ext cmd_parts input_path output_path binary_remote_cache raise exc CUDACompileError cmd_parts str error error raise error end_time = time log_duration_msg = f CUDA operation_name took end_time - start_time seconds Command cmd log info log_duration_msg log debug CUDA s skipped s since output already exists operation_name output_path Upload remote cache enabled binary_remote_cache None config cuda upload_to_binary_remote_cache will log errors fail out binary_remote_cache put output_path config cuda binary_remote_cache_force_write cls cache key_with_ext = CUDACodeCache CacheEntry input_path output_path None cache_entry CUDACodeCache CacheEntry = cls cache key_with_ext cache_entry error_json None Restore cached Exception raise we had compiled cmd_parts error_output = json loads cache_entry error_json raise exc CUDACompileError cmd_parts error_output encode utf- cls cache key_with_ext output_path key input_path classmethod load cls source_code str dst_file_ext str - tuple DLLWrapper str str Compiles source code loads generated so file Returns tuple DLLWrapper hash_key source_code_path dst_file_ext = so raise RuntimeError f Only support loading so file now f Requested file extension dst_file_ext Source code source_code dst_file_path hash_key source_code_path = cls compile source_code dst_file_ext DLLWrapper dst_file_path hash_key source_code_path classmethod _record_cuda_compile_error cls error_str str key_with_ext str cmd_parts list str input_path str output_path str Any here type will only work fbcode TODO Make typing hint strong here binary_remote_cache Any = None - None error_json = json dumps cmd_parts error_str cls cache key_with_ext = CUDACodeCache CacheEntry input_path output_path error_json error_path = binary_error_path output_path open error_path w encoding= utf- fh fh write error_json Upload remote cache directly memory enabled binary_remote_cache None config cuda upload_to_binary_remote_cache binary_remote_cache put error_path config cuda binary_remote_cache_force_write clear_on_fresh_cache ROCmCodeCache dataclasses dataclass CacheEntry input_path str output_path str cache dict str CacheEntry = aot_kernels_o list str = _SOURCE_CODE_SUFFIX = cpp _logged_compiler_version = False staticmethod cache_clear - None ROCmCodeCache cache clear ROCmCodeCache aot_kernels_o clear classmethod write cls source_code str dst_file_ext str - tuple str str Writes source code into file dst_file_ext file extension Returns hash key source code path file cuda_command = repr rocm_compile_command dummy_input dummy_output dst_file_ext key input_path = write source_code cls _SOURCE_CODE_SUFFIX extra=cuda_command key input_path classmethod compile cls source_code str dst_file_ext str extra_args list str &#124; None = None - tuple str str str Compiles source_code into file dst_file_ext extension using compile command specific ROCm platform Returns tuple dst_file_path hash_key source_code_path cls _logged_compiler_version cls _logged_compiler_version = True log debug get_compiler_version_info str rocm_compiler key input_path = cls write source_code dst_file_ext key cls cache torch utils _filelock FileLock lock_dir = get_lock_dir lock = FileLock os path join lock_dir key + lock timeout=LOCK_TIMEOUT lock output_path = input_path -len cls _SOURCE_CODE_SUFFIX + dst_file_ext os path exists output_path cmd = rocm_compile_command input_path output_path dst_file_ext extra_args start_time = time cmd_parts = cmd split try output = subprocess check_output cmd_parts stderr=subprocess STDOUT text=True env=os environ log debug Compilation output s output except subprocess CalledProcessError error raise exc CUDACompileError cmd_parts error output error end_time = time log_duration_msg = f Compilation took end_time - start_time seconds Compile command cmd log info log_duration_msg log debug Skip compiling s output s already exists input_path output_path cls cache key = ROCmCodeCache CacheEntry input_path output_path cls cache key output_path key input_path classmethod load cls source_code str dst_file_ext str - tuple DLLWrapper str str Compiles source code loads generated so file Returns tuple DLLWrapper hash_key source_code_path dst_file_ext = so raise RuntimeError f Only support loading so file now f Requested file extension dst_file_ext Source code source_code dst_file_path hash_key source_code_path = cls compile source_code dst_file_ext DLLWrapper dst_file_path hash_key source_code_path CodeCacheFuture result - Callable Any raise NotImplementedError LambdaFuture CodeCacheFuture __init__ result_fn Callable Any future Future Any &#124; None = None - None result_fn = result_fn future = future result - Callable Any result_fn StaticAutotunerFuture CodeCacheFuture A statically launchable CachingAutotuner loaded TritonBundler __init__ static_autotuner CachingAutotuner - None Pickled version CachingAutotuner static_autotuner = static_autotuner This needs set AsyncCompile triton case we need reload CachingAutotuner its source code We don t store source code CachingAutotuner itself since can very large reload_kernel_from_src Callable Any &#124; None = None result - CachingAutotuner assert reload_kernel_from_src None dynamo_timed StaticAutotunerFuture warm_precompile static_autotuner recheck_autotune_cache reload_kernel_from_src=self reload_kernel_from_src static_autotuner precompile type ignore union-attr warm_cache_only=False reload_kernel=self reload_kernel_from_src static_triton_bundle_key=None no need save again static_autotuner