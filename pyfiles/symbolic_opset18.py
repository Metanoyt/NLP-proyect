mypy allow-untyped-defs This file exports ONNX ops opset Note ONNX Operators added updated opset ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ https github com onnx onnx blob main docs Changelog md#version- -of-the-default-onnx-operator-set New operators BitwiseAnd CenterCropPad Col Im Mish OptionalGetElement OptionalHasElement Pad Resize ScatterElements ScatterND Split functools collections abc Sequence typing Optional torch torch _C torch onnx _internal torchscript_exporter _type_utils jit_utils registration symbolic_helper symbolic_opset opset EDITING THIS FILE READ THIS FIRST see Note Edit Symbolic Files symbolic_helper py __all__ = col im _onnx_symbolic = functools partial registration onnx_symbolic opset= _onnx_symbolic aten __and_ _onnx_symbolic aten bitwise_and __and_ g jit_utils GraphContext other do type promotion scalars don t seem apply args = other type promotion doesn t happen torch bitwise_and tensor scalar prom_args = arg arg args symbolic_helper _get_tensor_rank arg len prom_args == prom_args = args promotion_jit_type = symbolic_helper _type_promote_from_values prom_args = symbolic_helper _maybe_cast_to_type g promotion_jit_type other = symbolic_helper _maybe_cast_to_type g other promotion_jit_type promotion_jit_type == _type_utils JitScalarType BOOL g op And other g op BitwiseAnd other _onnx_symbolic aten col im symbolic_helper parse_args v v v col im g input _C Value output_size _C Value kernel_size _C Value dilation Sequence int padding Sequence int stride Sequence int convert i i into i i i i adjusted_padding list int = pad padding adjusted_padding extend pad _ range num_dimensional_axis = symbolic_helper _get_tensor_sizes output_size adjusted_padding adjusted_padding = num_dimensional_axis dilation dilation = num_dimensional_axis stride stride = num_dimensional_axis g op Col Im input output_size kernel_size dilations_i=dilation pads_i=adjusted_padding strides_i=stride _onnx_symbolic aten mean decorate= symbolic_helper _apply_params ReduceMean mean _onnx_symbolic aten prod decorate= symbolic_helper _apply_params ReduceProd prod allow_multi_dim_support=False _reduce_with_dtype onnx_op str name str allow_multi_dim_support bool = True symbolic_helper _reduce_with_dtype_helper onnx_op name allow_multi_dim_support _onnx_symbolic aten native_layer_norm symbolic_helper quantized_args True False False False symbolic_helper parse_args v v v f _native_layer_norm g jit_utils GraphContext input _C Value normalized_shape Sequence int weight _C Value bias _C Value eps float - tuple _C Value _C Value _C Value opset native_layer_norm g input normalized_shape weight bias eps _onnx_symbolic aten glu symbolic_helper parse_args v i _glu g jit_utils GraphContext input dim dim_size = symbolic_helper _get_tensor_dim_size input dim dim_size None assert dim_size == first second = g op Split input axis_i=dim num_outputs_i= outputs= g op Mul first g op Sigmoid second _onnx_symbolic aten max torch max same torch min actually has two interfaces smashed together torch max x dim keepdim torch max x y TODO justinchuby Support multiple quantized args output max g jit_utils GraphContext dim_or_y=None keepdim=None symbolic_helper _max_helper g dim_or_y keepdim _onnx_symbolic aten maximum symbolic_helper quantized_args True True maximum g jit_utils GraphContext input other pyrefly ignore no-matching-overload max g input dim_or_y=other _onnx_symbolic aten min TODO justinchuby Support multiple quantized args output min g jit_utils GraphContext dim_or_y=None keepdim=None symbolic_helper _min_helper g dim_or_y keepdim _onnx_symbolic aten minimum symbolic_helper quantized_args True True minimum g jit_utils GraphContext input other pyrefly ignore no-matching-overload min g input dim_or_y=other _onnx_symbolic aten amax symbolic_helper quantized_args True symbolic_helper parse_args v i amax g jit_utils GraphContext dim keepdim axes = g op Constant value_t=torch tensor dim dtype=torch long g op ReduceMax axes keepdims_i=keepdim _onnx_symbolic aten amin symbolic_helper quantized_args True symbolic_helper parse_args v i amin g jit_utils GraphContext dim keepdim axes = g op Constant value_t=torch tensor dim dtype=torch long g op ReduceMin axes keepdims_i=keepdim _onnx_symbolic aten aminmax symbolic_helper quantized_args True symbolic_helper parse_args v v i aminmax g jit_utils GraphContext dim keepdim symbolic_helper _is_none dim dim = symbolic_helper _get_const dim i dim axes = g op Constant value_t=torch tensor dim dtype=torch long g op ReduceMin axes keepdims_i=keepdim g op ReduceMax axes keepdims_i=keepdim g op ReduceMin keepdims_i=keepdim g op ReduceMax keepdims_i=keepdim _onnx_symbolic aten var_mean _var_mean g jit_utils GraphContext input args len args == symbolic_helper _var_mean_helper g input None args None symbolic_helper _var_mean_helper g input args _onnx_symbolic aten logsumexp symbolic_helper parse_args v i _logsumexp g jit_utils GraphContext input dim keepdim dim None g op ReduceLogSumExp input keepdims_i= axes = g op Constant value_t=torch tensor dim dtype=torch long g op ReduceLogSumExp input axes keepdims_i=keepdim _onnx_symbolic aten linalg_matrix_norm symbolic_helper parse_args v v b v _linalg_matrix_norm g jit_utils GraphContext torch _C Value ord torch _C Value dim list int keepdim bool dtype torch _C Value opset linalg_matrix_norm g ord dim keepdim dtype _onnx_symbolic aten embedding_bag symbolic_helper parse_args v v v i i i v i i embedding_bag g jit_utils GraphContext embedding_matrix indices offsets scale_grad_by_freq mode sparse per_sample_weights include_last_offset padding_idx symbolic_helper _embedding_bag_helper g embedding_matrix indices offsets scale_grad_by_freq mode sparse per_sample_weights include_last_offset padding_idx _onnx_symbolic aten linalg_vector_norm symbolic_helper parse_args v f b v linalg_vector_norm g jit_utils GraphContext torch _C Value ord float dim Optional Sequence int keepdim bool dtype torch _C Value symbolic_helper _linalg_vector_norm_helper g ord dim keepdim dtype