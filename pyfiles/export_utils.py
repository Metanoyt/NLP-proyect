mypy allow-untyped-defs types torch torch nn functional F torch ao quantization utils _assert_and_get_unique_device __all__ = model_is_exported _EXPORTED_TRAINING_ATTR = _exported_training _WrapperModule torch nn Module Class wrap callable ` torch nn Module ` Use you trying export callable __init__ fn super __init__ fn = fn forward args kwargs Simple forward just calls ` ` fn ` ` provided meth ` WrapperModule __init__ ` fn args kwargs model_is_exported m torch nn Module - bool Return True ` torch nn Module ` exported False otherwise e g model FX symbolically traced traced all isinstance m torch fx GraphModule any val n meta n m graph nodes _replace_dropout m torch fx GraphModule train_to_eval bool Switch dropout patterns model between train eval modes Dropout has different behavior train vs eval mode For exported models however calling ` model train ` ` model eval ` does automatically switch dropout behavior between two modes so here we need rewrite aten dropout patterns manually achieve same effect See https github com pytorch pytorch issues Avoid circular dependencies utils _get_aten_graph_module_for_pattern Needed ensure subgraph matches self-contained m graph eliminate_dead_code m recompile inplace False True dropout_train x F dropout x p= training=True inplace=inplace dropout_eval x F dropout x p= training=False inplace=inplace example_inputs = torch randn train_to_eval match_pattern = _get_aten_graph_module_for_pattern _WrapperModule dropout_train example_inputs replacement_pattern = _get_aten_graph_module_for_pattern _WrapperModule dropout_eval example_inputs match_pattern = _get_aten_graph_module_for_pattern _WrapperModule dropout_eval example_inputs replacement_pattern = _get_aten_graph_module_for_pattern _WrapperModule dropout_train example_inputs torch fx subgraph_rewriter replace_pattern_with_filters replace_pattern_with_filters m match_pattern replacement_pattern match_filters= ignore_literals=True m recompile _replace_batchnorm m torch fx GraphModule train_to_eval bool Switch batchnorm patterns model between train eval modes Batchnorm has different behavior train vs eval mode For exported models however calling ` model train ` ` model eval ` does automatically switch batchnorm behavior between two modes so here we need rewrite aten batchnorm patterns manually achieve same effect TODO Leslie This function still fails support custom momentum eps value Enable support future updates Avoid circular dependencies utils _get_aten_graph_module_for_pattern Needed ensure subgraph matches self-contained m graph eliminate_dead_code m recompile bn_train x torch Tensor bn_weight torch Tensor bn_bias torch Tensor bn_running_mean torch Tensor bn_running_var torch Tensor F batch_norm x bn_running_mean bn_running_var bn_weight bn_bias training=True bn_eval x torch Tensor bn_weight torch Tensor bn_bias torch Tensor bn_running_mean torch Tensor bn_running_var torch Tensor F batch_norm x bn_running_mean bn_running_var bn_weight bn_bias training=False example_inputs = torch randn x torch randn bn_weight torch randn bn_bias torch randn bn_running_mean torch randn bn_running_var device = _assert_and_get_unique_device m is_cuda = device None device type == cuda bn_train_aten = _get_aten_graph_module_for_pattern _WrapperModule bn_train example_inputs is_cuda bn_eval_aten = _get_aten_graph_module_for_pattern _WrapperModule bn_eval example_inputs is_cuda train_to_eval match_pattern = bn_train_aten replacement_pattern = bn_eval_aten match_pattern = bn_eval_aten replacement_pattern = bn_train_aten torch fx subgraph_rewriter replace_pattern_with_filters replace_pattern_with_filters m match_pattern replacement_pattern match_filters= ignore_literals=True m recompile TODO expose these under namespace _move_exported_model_to_eval model torch fx GraphModule Move exported GraphModule eval mode This equivalent model eval only certain special ops like dropout batchnorm QAT users should call before performing inference model This call idempotent model already eval mode nothing will happen is_training = getattr model _EXPORTED_TRAINING_ATTR True is_training model setattr model _EXPORTED_TRAINING_ATTR False _replace_dropout model train_to_eval=True _replace_batchnorm model train_to_eval=True model _move_exported_model_to_train model torch fx GraphModule Move exported GraphModule train mode This equivalent model train only certain special ops like dropout batchnorm QAT users should call before performing training model This call idempotent model already train mode nothing will happen is_training = getattr model _EXPORTED_TRAINING_ATTR False is_training model setattr model _EXPORTED_TRAINING_ATTR True _replace_dropout model train_to_eval=False _replace_batchnorm model train_to_eval=False model _allow_exported_model_train_eval model torch fx GraphModule Allow users call ` model train ` ` model eval ` exported model effect changing behavior between two modes limited special ops only which currently dropout batchnorm Note This does achieve same effect what ` model train ` ` model eval ` does eager models only provides approximation In particular user code branching ` training ` flag will function correctly general because branch already specialized export time Additionally other ops beyond dropout batchnorm have different train eval behavior will also converted properly _train mode bool = True mode _move_exported_model_to_train _move_exported_model_to_eval _eval _move_exported_model_to_eval model train = types MethodType _train model type ignore method-assign model eval = types MethodType _eval model type ignore method-assign model