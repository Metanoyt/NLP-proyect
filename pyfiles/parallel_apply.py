threading collections abc Sequence typing Any cast Optional Union torch torch _utils ExceptionWrapper torch cuda _utils _get_device_index torch nn modules Module __all__ = get_a_var parallel_apply get_a_var obj Union torch Tensor list Any tuple Any dict Any Any - Optional torch Tensor isinstance obj torch Tensor obj isinstance obj list tuple result map get_a_var obj isinstance result torch Tensor result isinstance obj dict result map get_a_var obj items isinstance result torch Tensor result None parallel_apply modules Sequence Module inputs Sequence Any kwargs_tup Optional Sequence dict str Any = None devices Optional Sequence Optional Union int torch device = None - list Any r Apply each ` module ` attr ` modules ` parallel each attr ` devices ` Args modules Module modules parallelized inputs tensor inputs modules devices list int torch device CUDA devices attr ` modules ` attr ` inputs ` attr ` kwargs_tup ` given attr ` devices ` given should all have same length Moreover each element attr ` inputs ` can either single object only argument module collection positional arguments assert len modules == len inputs f The number modules len modules equal number inputs len inputs kwargs_tup None assert len modules == len kwargs_tup kwargs_tup = cast dict str Any len modules devices None assert len modules == len devices devices = None len modules devices = _get_device_index x True x devices streams = torch accelerator current_stream x x devices assert torch accelerator is_available No available accelerator found device_type = torch accelerator current_accelerator type type ignore union-attr lock = threading Lock results = grad_enabled autocast_enabled = torch is_grad_enabled torch is_autocast_enabled _worker i int module Module input Any kwargs dict str Any device Optional Union int torch device = None stream Optional torch Stream = None - None torch set_grad_enabled grad_enabled device None t = get_a_var input t None lock results i = ExceptionWrapper where=f replica i no device provided no tensor input found device cannot resolved device = t get_device isinstance device torch device device = device index stream None stream = torch accelerator current_stream device try torch accelerator device_index device stream torch amp autocast device_type enabled=autocast_enabled also avoids accidental slicing ` input ` Tensor isinstance input list tuple input = input output = module input kwargs lock results i = output except Exception lock results i = ExceptionWrapper where=f replica i device device len modules threads = threading Thread target=_worker args= i module input kwargs device stream i module input kwargs device stream enumerate zip modules inputs kwargs_tup devices streams strict=True thread threads thread start thread threads thread join _worker modules inputs kwargs_tup devices streams outputs = i range len inputs output = results i isinstance output ExceptionWrapper output reraise outputs append output outputs