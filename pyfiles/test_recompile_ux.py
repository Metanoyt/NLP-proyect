Owner s module dynamo unittest weakref torch torch _dynamo torch _dynamo config torch _dynamo test_case torch _dynamo testing torch _logging torch _dynamo exc FailOnRecompileLimitHit torch testing _internal logging_utils kwargs_to_settings log_settings device_type = acc type acc = torch accelerator current_accelerator True cpu RecompileUxTests torch _dynamo test_case TestCase TODO whc dynamo actually recompiles one more time than cache limit cache_limit = classmethod setUpClass cls super setUpClass cls _exit_stack enter_context torch _dynamo config patch recompile_limit cls cache_limit test_drop_cache_on_skip model x i x + i attached = False triggered = False trigger nonlocal triggered triggered = True compiler gm input nonlocal attached f = gm forward assert attached NB making weakref ref causes cycle no longer promptly GC ed weakref finalize f trigger attached = True f x = torch randn i range opt_model = torch compile model backend=compiler opt_model x i assertTrue triggered test_loop_torture loop_torture input iters out = input randint itself causes one graph break _ range iters out += input out compile_counter = torch _dynamo testing CompileCounter _ range x = torch randn iters = torch randint low= high= size= opt_loop_torture = torch compile loop_torture backend=compile_counter opt_loop_torture x iters Currently we recompile each time We d probably like bail out quickly warn TODO whc these checks fail py Why assertEqual counters frames total + cache_limit assertEqual counters frames ok + cache_limit compile_counter only sees frames fed backend compiler which subset counters frames ok -- probably because counters frames ok includes frames containing torch ops assertEqual compile_counter frame_count cache_limit torch _dynamo config patch automatic_dynamic_shapes False test_dynamic_input model input input + input expected_recompiles = compile_counter = torch _dynamo testing CompileCounter torch _dynamo config patch recompile_limit expected_recompiles assertLogs logger= torch _dynamo level= WARNING logs _ range bsz = torch randint low= high= size= x = torch randn bsz opt_model = torch compile model backend=compile_counter opt_model x assertEqual compile_counter frame_count expected_recompiles assertEqual len logs records print logs records assertTrue logs records getMessage startswith torch _dynamo hit config recompile_limit unittest skipIf torch cuda is_available torch xpu is_available requires cuda xpu test_nvfuser_guards we may want model dynamo s guards sufficiently after nvfuser s ProfilingExecutor guards such we ensure dynamo charge all recompilations top level we could thus simplify underlying torchscript executor func b c + b c = torch rand device=device_type b = torch rand device=device_type b_v = torch rand device=device_type view b_p = torch rand device=device_type permute c = torch rand device=device_type compile_counter = torch _dynamo testing CompileCounter torch _dynamo config patch recompile_limit opt_func = torch compile func backend=compile_counter opt_func b c warmup assertEqual compile_counter frame_count opt_func b c no guard fail recompile assertEqual compile_counter frame_count opt_func b_v c view should cause nvfuser recompile assertEqual compile_counter frame_count opt_func b_p c permutation should cause recompile assertEqual compile_counter frame_count assert_single_log_contains logs contains_str assertEqual len logs records assertTrue logs records getMessage find contains_str msg=f Expected find contains_str log logs records getMessage test_verbose_tensor_check func Warning choose function here whose meta implementation lives entirely C++ If you do Python one Dynamo will dive into torch _refs which OK will muddy up warnings torch add cache_fail_test cached_input missed_input expected_failure TODO whc maybe its hacky have test within test seemed convenient torch _dynamo reset torch _dynamo utils counters clear opt_func = torch compile func backend= eager warmup opt_func cached_input assertLogs logger= torch _dynamo level= WARNING logs opt_func = torch compile func backend= eager opt_func missed_input assert_single_log_contains logs expected_failure = torch rand cache_fail_test tensor size mismatch index expected actual cache_fail_test clone as_strided stride= tensor stride mismatch index expected actual cache_fail_test tensor rank mismatch expected actual cache_fail_test meta tensor dispatch key set mismatch cache_fail_test torch float tensor dtype mismatch expected Float actual Half a_grad = clone a_grad requires_grad = True cache_fail_test a_grad tensor requires_grad mismatch expected requires_grad= test_mismatched_type = torch rand b = torch rand func b + b opt_func = torch compile func backend= eager warmup opt_func b assertLogs logger= torch _dynamo level= WARNING logs opt_func = torch compile func backend= eager opt_func assert_single_log_contains logs expected type b tensor type found int torch _dynamo config patch recompile_limit= fail_on_recompile_limit_hit=True test_fail_on_recompile_limit_hit torch compile backend= eager func b b b + func torch randn True assertRaises FailOnRecompileLimitHit func torch randn False torch _dynamo config patch recompile_limit test_multiple_guard_fails failure_reasons = guard_fail_fn failure failure_reasons append failure f x torch relu x opt_f = torch _dynamo optimize backend= eager guard_fail_fn=guard_fail_fn dynamic=False f i range failure_reasons clear opt_f torch randn + i failure_str = \n join failure_reasons line tensor x size mismatch index expected actual tensor x size mismatch index expected actual tensor x size mismatch index expected actual tensor x size mismatch index expected actual assertIn line failure_str torch _dynamo config patch recompile_limit test_multiple_guard_fails_report_all log_settings kwargs_to_settings recompiles_verbose=True failure_reasons = guard_fail_fn failure failure_reasons append failure f x torch ones len x x - opt_f = torch _dynamo optimize backend= eager guard_fail_fn=guard_fail_fn dynamic=False f opt_f filter_reasons \n join line line \n join failure_reasons splitlines line startswith ___check_type_id failure_reasons clear opt_f line len x == assertIn line filter_reasons failure_reasons clear opt_f line len x == len x == assertIn line filter_reasons torch _dynamo config patch recompile_limit= test_recompile_child_run_only f x n torch compiler is_compiling x = x + x = g x h x + n g x torch compiler is_compiling x + x h x torch compiler is_compiling x + x torch compile g backend= eager torch randn inp = torch randn opt_f = torch compile f backend= eager opt_f inp expect f run eager g compiled previous invocatino h eager res = opt_f inp assertEqual res inp + __name__ == __main__ torch _dynamo test_case run_tests run_tests