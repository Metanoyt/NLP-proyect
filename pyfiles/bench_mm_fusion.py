flake noqa B prettytable PrettyTable torch torch _dynamo torch _inductor config torch _inductor runtime benchmarking benchmarker torch _inductor config debug = True torch _inductor config triton dense_indexing = True torch manual_seed The flag below controls whether allow TF matmul torch backends cuda matmul allow_tf = True Func mm torch _dynamo optimize inductor mm b bias y = torch mm b y mm+bias torch _dynamo optimize inductor mm_add b bias y = torch mm b y + bias relu mm torch _dynamo optimize inductor mm_relu b bias y = torch mm b torch relu y relu mm+bias torch _dynamo optimize inductor mm_add_relu b bias y = torch mm b y += bias torch relu y bench shape layer_id p fusion_types=None torch _logging set_logs inductor_metrics=True fusion_types None fusion_types = dtype = torch float M K = shape _ N = shape torch manual_seed allocate inputs = torch randn shape device= cuda dtype=dtype b = torch randn shape device= cuda dtype=dtype tflops ms M K N ms e- row = layer_id fusion_type fusion_types fusion_type == fn_mm = Func mm fn_mm = getattr Func f mm_ fusion_type add fusion_type bias = torch randn M N dtype=dtype device= cuda bias = None args = b bias fn fn_mm args torch _inductor config triton mm = aten torch_mm_ms _ _ = benchmarker benchmark_gpu fn torch _inductor config triton mm = triton reset force code gen new python code torch _dynamo reset torch _inductor metrics reset triton_mm_ms _ _ = benchmarker benchmark_gpu fn assert torch _inductor metrics generated_kernel_count == codegen #kernel = row extend tflops torch_mm_ms tflops triton_mm_ms p add_row row torch _logging set_logs fusion_types = add relu add_relu shapes = alexnet BERT hf_GPT p = PrettyTable field_names = layer fusion_type fusion_types fusion_type == field_names append torch mm field_names append triton mm field_names append f torch mm+ fusion_type field_names append f triton mm+ fusion_type p field_names = field_names p float_format = id shape enumerate shapes bench shape id p fusion_types print p