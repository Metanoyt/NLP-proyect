mypy ignore-errors collections defaultdict collections abc Iterable numpy np torch hypothesis functools reduce importlib metadata version hypothesis assume hypothesis settings hypothesis strategies st hypothesis extra numpy stnp hypothesis strategies SearchStrategy torch testing _internal common_quantized _calculate_dynamic_qparams _calculate_dynamic_per_channel_qparams Setup hypothesis tests The tuples torch_quantized_dtype zero_point_enforce where last element enforced zero_point If None any zero_point point within range data type OK Tuple all quantized data types _ALL_QINT_TYPES = torch quint torch qint torch qint Enforced zero point every quantized data type If None any zero_point point within range data type OK _ENFORCED_ZERO_POINT = defaultdict lambda None torch quint None torch qint None torch qint _get_valid_min_max qparams scale zero_point _quantized_type = qparams adjustment = + torch finfo torch float eps _long_type_info = torch iinfo torch long long_min long_max = _long_type_info min adjustment _long_type_info max adjustment make sure intermediate results within range long min_value = max long_min - zero_point scale long_min scale + zero_point max_value = min long_max - zero_point scale long_max scale + zero_point np float min_value np float max_value This wrapper wraps around ` st floats ` checks version ` hypothesis ` too old removes ` width ` parameter which introduced _floats_wrapper args kwargs width kwargs hypothesis version __version_info__ As long nan inf min max specified reimplement width parameter older versions hypothesis no_nan_and_inf = allow_nan kwargs kwargs allow_nan allow_nan kwargs allow_infinity kwargs kwargs allow_infinity allow_infinity kwargs min_and_max_not_specified = len args == min_value kwargs max_value kwargs no_nan_and_inf min_and_max_not_specified kwargs width == kwargs min_value = torch finfo torch float min kwargs max_value = torch finfo torch float max kwargs width == kwargs min_value = torch finfo torch float min kwargs max_value = torch finfo torch float max kwargs width == kwargs min_value = torch finfo torch float min kwargs max_value = torch finfo torch float max kwargs pop width st floats args kwargs floats args kwargs width kwargs kwargs width = _floats_wrapper args kwargs Hypothesis filter avoid overflows quantized tensors Args tensor Tensor floats filter qparams Quantization parameters returned ` qparams ` Returns True Raises hypothesis UnsatisfiedAssumption Note This filter slow Use only when filtering test cases absolutely necessary assume_not_overflowing tensor qparams min_value max_value = _get_valid_min_max qparams assume tensor min = min_value assume tensor max = max_value True Strategy generating quantization parameters Args dtypes quantized data types sample scale_min scale_max Min max scales If None set e- e zero_point_min zero_point_max Min max zero point If None set minimum maximum quantized data type Note The min max only valid zero_point enforced data type itself Generates scale Sampled scale zero_point Sampled zero point quantized_type Sampled quantized type st composite qparams draw dtypes=None scale_min=None scale_max=None zero_point_min=None zero_point_max=None dtypes None dtypes = _ALL_QINT_TYPES isinstance dtypes list tuple dtypes = dtypes quantized_type = draw st sampled_from dtypes _type_info = torch iinfo quantized_type qmin qmax = _type_info min _type_info max TODO Maybe embed enforced zero_point ` torch iinfo ` _zp_enforced = _ENFORCED_ZERO_POINT quantized_type _zp_enforced None zero_point = _zp_enforced _zp_min = qmin zero_point_min None zero_point_min _zp_max = qmax zero_point_max None zero_point_max zero_point = draw st integers min_value=_zp_min max_value=_zp_max scale_min None scale_min = torch finfo torch float eps scale_max None scale_max = torch finfo torch float max scale = draw floats min_value=scale_min max_value=scale_max width= scale zero_point quantized_type Strategy create different shapes Args min_dims max_dims minimum maximum rank min_side max_side minimum maximum dimensions per rank Generates Possible shapes tensor constrained rank dimensionality Example Generates D D tensors given Q = qtensor shapes=array_shapes min_dims= max_dims= some_test Q st composite array_shapes draw min_dims= max_dims=None min_side= max_side=None max_numel=None Return strategy array shapes tuples int = assert min_dims max_dims None max_dims = min min_dims + assert max_dims max_side None max_side = min_side + candidate = st lists st integers min_side max_side min_size=min_dims max_size=max_dims max_numel None candidate = candidate filter lambda x reduce int __mul__ x = max_numel draw candidate map tuple Strategy generating test cases tensors The resulting tensor float format Args shapes Shapes under test tensor Could either hypothesis strategy iterable different shapes sample elements Elements generate returned data type If None strategy resolves float within range - e e qparams Instance qparams strategy This used filter tensor such overflow would happen Generates X Tensor type float Note NaN + -inf included qparams If ` qparams ` arg set Quantization parameters X The returned parameters ` scale zero_point quantization_type ` If ` qparams ` arg None returns None st composite tensor draw shapes=None elements=None qparams=None dtype=np float isinstance shapes SearchStrategy _shape = draw shapes _shape = draw st sampled_from shapes qparams None elements None elements = floats - e e allow_nan=False width= X = draw stnp arrays dtype=dtype elements=elements shape=_shape assume np isnan X any np isinf X any X None qparams = draw qparams elements None min_value max_value = _get_valid_min_max qparams elements = floats min_value max_value allow_infinity=False allow_nan=False width= X = draw stnp arrays dtype=dtype elements=elements shape=_shape Recompute scale zero_points according X statistics scale zp = _calculate_dynamic_qparams X qparams enforced_zp = _ENFORCED_ZERO_POINT get qparams None enforced_zp None zp = enforced_zp X scale zp qparams st composite per_channel_tensor draw shapes=None elements=None qparams=None isinstance shapes SearchStrategy _shape = draw shapes _shape = draw st sampled_from shapes qparams None elements None elements = floats - e e allow_nan=False width= X = draw stnp arrays dtype=np float elements=elements shape=_shape assume np isnan X any np isinf X any X None qparams = draw qparams elements None min_value max_value = _get_valid_min_max qparams elements = floats min_value max_value allow_infinity=False allow_nan=False width= X = draw stnp arrays dtype=np float elements=elements shape=_shape Recompute scale zero_points according X statistics scale zp = _calculate_dynamic_per_channel_qparams X qparams enforced_zp = _ENFORCED_ZERO_POINT get qparams None enforced_zp None zp = enforced_zp Permute model quantization along axis axis = int np random randint X ndim permute_axes = np arange X ndim permute_axes = axis permute_axes axis = X = np transpose X permute_axes X scale zp axis qparams Strategy generating test cases tensors used Conv The resulting tensors float format Args spatial_dim Spatial Dim feature maps If given iterable randomly picks one pool make spatial dimension batch_size_range Range generate ` batch_size ` Must tuple ` min max ` input_channels_per_group_range Range generate ` input_channels_per_group ` Must tuple ` min max ` output_channels_per_group_range Range generate ` output_channels_per_group ` Must tuple ` min max ` feature_map_range Range generate feature map size each spatial_dim Must tuple ` min max ` kernel_range Range generate kernel size each spatial_dim Must tuple ` min max ` max_groups Maximum number groups generate elements Elements generate returned data type If None strategy resolves float within range - e e qparams Strategy quantization parameters X w b Could either single strategy used all list three strategies X w b Generates X W b g Tensors type ` float ` following drawen shapes X ` batch_size input_channels H W ` W ` output_channels input_channels_per_group + kernel_shape b ` output_channels ` groups Number groups input divided into Note X W b tuples Tensor qparams where qparams could either None scale zero_point quantized_type Example given tensor_conv spatial_dim= batch_size_range= input_channels_per_group_range= output_channels_per_group_range= feature_map_range= kernel_range= max_groups= elements=st floats - qparams=qparams st composite tensor_conv draw spatial_dim= batch_size_range= input_channels_per_group_range= output_channels_per_group_range= feature_map_range= kernel_range= max_groups= can_be_transposed=False elements=None qparams=None Resolve minibatch in_channels out_channels iH iW iK iW batch_size = draw st integers batch_size_range input_channels_per_group = draw st integers input_channels_per_group_range output_channels_per_group = draw st integers output_channels_per_group_range groups = draw st integers max_groups input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups isinstance spatial_dim Iterable spatial_dim = draw st sampled_from spatial_dim feature_map_shape = draw st integers feature_map_range _ range spatial_dim kernels = draw st integers kernel_range _ range spatial_dim tr = False weight_shape = output_channels input_channels_per_group + tuple kernels bias_shape = output_channels can_be_transposed tr = draw st booleans tr weight_shape = input_channels output_channels_per_group + tuple kernels bias_shape = output_channels Resolve tensors qparams None isinstance qparams list tuple assert len qparams == Need qparams X w b qparams = qparams X = draw tensor shapes= batch_size input_channels + tuple feature_map_shape elements=elements qparams=qparams W = draw tensor shapes= weight_shape elements=elements qparams=qparams b = draw tensor shapes= bias_shape elements=elements qparams=qparams X W b groups tr We set deadline currently loaded profile Creating loading separate profile overrides any settings user already specified hypothesis_version = tuple map int version hypothesis split = hypothesis_version Hypothesis â†’ use ` timeout ` instead ` deadline ` settings register_profile no_deadline timeout=hypothesis unlimited Hypothesis = use ` deadline=None ` settings register_profile no_deadline deadline=None Activate profile settings load_profile no_deadline assert_deadline_disabled Check deadlines effectively disabled across Hypothesis versions hypothesis_version warnings warning_message = Your version hypothesis outdated To avoid ` DeadlineExceeded ` errors please update f Current hypothesis version hypothesis __version__ warnings warn warning_message stacklevel= assert settings deadline None