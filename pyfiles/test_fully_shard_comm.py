Owner s oncall distributed copy functools itertools os tempfile unittest collections abc Callable typing Optional Union unittest mock MagicMock torch torch distributed dist torch nn nn torch nn functional F torch distributed _composable checkpoint replicate torch distributed algorithms _checkpoint checkpoint_wrapper apply_activation_checkpointing torch distributed device_mesh DeviceMesh init_device_mesh torch distributed fsdp FSDPModule fully_shard MixedPrecisionPolicy OffloadPolicy torch distributed fsdp _fully_shard _fsdp_api AllGather torch distributed fsdp _fully_shard _fsdp_collectives _div_if_needed _get_gradient_divide_factors DefaultAllGather DefaultReduceScatter foreach_all_gather foreach_all_gather_copy_out foreach_reduce torch distributed fsdp _fully_shard _fsdp_common FSDPMeshInfo TrainingState torch distributed fsdp _fully_shard _fsdp_init _get_post_forward_mesh_info _init_default_fully_shard_mesh torch distributed fsdp _fully_shard _fsdp_param ShardedState torch distributed fsdp _fully_shard _fsdp_param_group FSDPParamGroup torch distributed tensor DTensor torch distributed tensor debug CommDebugMode torch distributed tensor experimental implicit_replication torch testing _internal common_distributed requires_multicast_support skip_if_lt_x_gpu torch testing _internal common_fsdp check_sharded_parity DoubleLinear FSDPTest FSDPTestMultiThread MLP patch_post_backward patch_reshard patch_unshard torch testing _internal common_utils run_tests TEST_XPU xfailIf torch testing _internal distributed _tensor common_dtensor FeedForward ModelArgs Transformer TransformerBlock c d_ops = torch ops c d For recording FSDP events like unshard post-backward EventType = tuple str str TrainingState torch testing _internal common_fsdp get_devtype device_type = torch device get_devtype device_module = torch get_device_module device_type TestFullyShardCollectiveOps FSDPTestMultiThread property world_size - int property device - torch device torch device device_type type _get_param_sizes - list torch Size For world size fp all-gather reduce-scatter testing requires ~ GB torch Size torch Size torch Size torch Size torch Size torch Size torch Size torch Size _init_params param_sizes list torch Size - list nn Parameter torch manual_seed orig_params = nn Parameter torch randn size device=self device size param_sizes Since seed per process per thread we broadcast ensure same original parameters across ranks orig_param orig_params dist broadcast orig_param src= orig_params _init_fsdp_param_group params list nn Parameter reshard_after_forward Union bool int module = nn ParameterList param detach clone param params mesh_info = FSDPMeshInfo _init_default_fully_shard_mesh shard_mesh_dim= post_forward_mesh_info = _get_post_forward_mesh_info reshard_after_forward mesh_info fsdp_param_group = FSDPParamGroup list module parameters module mesh_info post_forward_mesh_info device None shard_placement_fn MixedPrecisionPolicy OffloadPolicy fsdp_param_group lazy_init fsdp_param_group skip_if_lt_x_gpu test_all_gather_fp param_sizes = _get_param_sizes default_stream = device_module current_stream stream stream = device_module Stream device_module Stream async_op streams reshard_after_forward itertools product False True default_stream default_stream stream stream True all_gather_copy_in_stream all_gather_stream = streams Save test time only testing reshard after forward int non-async non-default streams like pre-backward type reshard_after_forward int async_op all_gather_stream default_stream continue _test_all_gather param_sizes reshard_after_forward=reshard_after_forward async_op=async_op all_gather_copy_in_stream=all_gather_copy_in_stream all_gather_stream=all_gather_stream _test_all_gather param_sizes list torch Size reshard_after_forward Union bool int async_op bool all_gather_copy_in_stream all_gather_stream all_gather fsdp_param_group FSDPParamGroup group dist ProcessGroup all_gather_comm = DefaultAllGather all_gather_result = foreach_all_gather fsdp_param_group fsdp_params group async_op=async_op all_gather_copy_in_stream=all_gather_copy_in_stream all_gather_stream=all_gather_stream device=self device all_gather_comm=all_gather_comm foreach_all_gather_copy_out all_gather_result fsdp_params group Transition unsharded state register unsharded parameters fsdp_param fsdp_param_group fsdp_params fsdp_param init_unsharded_param fsdp_param_group _to_unsharded check_all_gathered_params orig_params list nn Parameter module nn Module orig_param param zip orig_params module parameters assertIsInstance param torch Tensor assertIsInstance param nn Parameter assertEqual param orig_param param dtype Set up reference parameters construct FSDP group orig_params = _init_params param_sizes fsdp_param_group = _init_fsdp_param_group orig_params reshard_after_forward fsdp_params = fsdp_param_group fsdp_params module = fsdp_param_group modules Sanity check parameter sharding expected orig_param param zip orig_params module parameters assertTrue isinstance param DTensor assertEqual param full_tensor orig_param Run foreach all-gather including copy-in copy-out all_gather fsdp_param_group fsdp_param_group mesh_info shard_process_group Check all-gather correctness check_all_gathered_params orig_params module For reshard after after forward int further test emulating pre-backward all-gather type reshard_after_forward int fsdp_param_group _to_sharded_post_forward all_gather fsdp_param_group fsdp_param_group post_forward_mesh_info shard_process_group check_all_gathered_params orig_params module skip_if_lt_x_gpu test_reduce_scatter_fp param_sizes = _get_param_sizes default_stream = device_module current_stream stream = device_module Stream reduce_scatter_stream default_stream stream _test_reduce_scatter param_sizes reduce_scatter_stream=reduce_scatter_stream reduce_scatter_dtype=torch float skip_if_lt_x_gpu test_reduce_scatter_fp param_sizes = _get_param_sizes default_stream = torch get_device_module device_type current_stream stream = device_module Stream reduce_scatter_stream default_stream stream _test_reduce_scatter param_sizes reduce_scatter_stream=reduce_scatter_stream reduce_scatter_dtype=torch float _test_reduce_scatter param_sizes list torch Size reduce_scatter_stream reduce_scatter_dtype torch dtype Set up reference parameters construct FSDP group orig_params = _init_params param_sizes fsdp_param_group = _init_fsdp_param_group orig_params True fsdp_params = fsdp_param_group fsdp_params fsdp_param_group comm_ctx lazy_init device Run one unshard initialize metadata fsdp_param_group unshard fsdp_param_group wait_for_unshard fsdp_param_group reshard Run foreach reduce-scatter including copy-in view-out torch manual_seed unsharded_grads = torch ones_like param rank param orig_params group = fsdp_param_group mesh_info shard_process_group assertEqual group size world_size all_reduce_stream = device_module Stream comm = DefaultReduceScatter _ _ post_reduce_event _ _ _ = foreach_reduce fsdp_params unsharded_grads group reduce_scatter_stream comm orig_dtype=orig_params dtype reduce_dtype=reduce_scatter_dtype device=self device gradient_divide_factor=None all_reduce_group=None all_reduce_stream=all_reduce_stream all_reduce_hook=None all_reduce_grads=True partial_reduce_output=None torch get_device_module device_type current_stream wait_event post_reduce_event Check reduce-scatter correctness predivide_factor postdivide_factor _ all_reduce_op = _get_gradient_divide_factors group None reduce_scatter_dtype reduced_grads = grad detach clone grad unsharded_grads grad reduced_grads _div_if_needed grad predivide_factor dist all_reduce grad group=group op=all_reduce_op _div_if_needed grad postdivide_factor fsdp_param reduced_grad zip fsdp_params reduced_grads sharded_grad = fsdp_param sharded_param grad assertIsInstance sharded_grad DTensor assertEqual sharded_grad full_tensor reduced_grad TestFullyShardCommunication FSDPTest property world_size - int min torch get_device_module device_type device_count skip_if_lt_x_gpu test_fully_shard_communication_count Tests FSDP issues expected number all-gathers reduce-scatters during forward backward run_subtests reshard_after_forward True False None _test_communication_count _test_communication_count reshard_after_forward Union bool int None torch manual_seed model_args = ModelArgs model = Transformer model_args fully_shard_fn = functools partial fully_shard reshard_after_forward=reshard_after_forward num_blocks = module model modules isinstance module TransformerBlock fully_shard_fn module num_blocks += fully_shard_fn model We construct ` num_blocks ` plus FSDP states communication groups torch manual_seed + rank inp = torch randint model_args vocab_size device=device_type type CommDebugMode fwd_comm_mode loss = model inp fwd_comm_counts = fwd_comm_mode get_comm_counts assertEqual len fwd_comm_counts assertEqual fwd_comm_counts c d_ops _allgather_base_ num_blocks + CommDebugMode bwd_comm_mode loss sum backward bwd_comm_counts = bwd_comm_mode get_comm_counts reshard_after_forward None means two types collectives all-gather reduce-scatter assertEqual len bwd_comm_counts do reshard root model assertEqual bwd_comm_counts c d_ops _allgather_base_ num_blocks reshard_after_forward assertEqual len bwd_comm_counts assertEqual bwd_comm_counts c d_ops _allgather_base_ num_blocks + assertEqual len bwd_comm_counts assertEqual bwd_comm_counts c d_ops _reduce_scatter_base_ num_blocks + skip_if_lt_x_gpu test_manual_reshard_with_reshard_after_forward_false Tests we can manually call ` ` reshard ` ` FSDP modules initialized ` ` reshard_after_forward=False ` ` still run unshard torch manual_seed model_args = ModelArgs model = Transformer model_args module model modules isinstance module TransformerBlock fully_shard module reshard_after_forward=False model = fully_shard model reshard_after_forward=False num_fsdp_modules = sum isinstance module FSDPModule module model modules torch manual_seed + rank inp = torch randint model_args vocab_size device=device_type type CommDebugMode fwd_comm_mode loss = model inp fwd_comm_counts = fwd_comm_mode get_comm_counts assertEqual len fwd_comm_counts assertEqual fwd_comm_counts c d_ops _allgather_base_ num_fsdp_modules module model modules isinstance module FSDPModule module reshard CommDebugMode bwd_comm_mode loss sum backward bwd_comm_counts = bwd_comm_mode get_comm_counts assertEqual len bwd_comm_counts assertEqual bwd_comm_counts c d_ops _allgather_base_ num_fsdp_modules assertEqual bwd_comm_counts c d_ops _reduce_scatter_base_ num_fsdp_modules skip_if_lt_x_gpu xfailIf TEST_XPU https github com intel torch-xpu-ops issues test_set_reduce_scatter_divide_factor run_subtests divide_factor world_size world_size _test_set_reduce_scatter_divide_factor run_subtests divide_factor world_size _test_set_reduce_scatter_divide_factor_mixed_prevision _test_set_reduce_scatter_divide_factor divide_factor float torch manual_seed model_args = ModelArgs dropout_p= weight_tying=False model = Transformer model_args ref_model = copy deepcopy model device_type ref_optim = torch optim AdamW ref_model parameters lr= e- module model modules isinstance module TransformerBlock fully_shard module reshard_after_forward=False model = fully_shard model reshard_after_forward=False optim = torch optim AdamW model parameters lr= e- model set_reduce_scatter_divide_factor divide_factor torch manual_seed + rank inp = torch randint model_args vocab_size device=device_type type _ range ref_loss = ref_model inp sum ref_loss backward param ref_model parameters param grad mul_ divide_factor dist all_reduce param grad loss = model inp sum loss backward ref_optim step optim step ref_optim zero_grad optim zero_grad assertEqual ref_loss loss check_sharded_parity ref_model model _test_set_reduce_scatter_divide_factor_mixed_prevision divide_factor float torch manual_seed param_dtype = torch bfloat reduce_dtype = torch float mp_policy = MixedPrecisionPolicy param_dtype=param_dtype reduce_dtype=reduce_dtype model = nn Sequential MLP _ range ref_model = copy deepcopy model device_type ref_model_bf = copy deepcopy ref_model param_dtype ref_optim = torch optim AdamW ref_model parameters lr= e- mlp model fully_shard mlp mp_policy=mp_policy model = fully_shard model mp_policy=mp_policy optim = torch optim AdamW model parameters lr= e- model set_reduce_scatter_divide_factor divide_factor torch manual_seed + rank inp = torch randn device=device_type type dtype=param_dtype _ range loss = model inp sum loss backward optim step optim zero_grad ref_loss = ref_model_bf inp param_dtype sum ref_loss backward param ref_model_bf parameters param grad data = param grad torch float param grad mul_ divide_factor dist all_reduce param grad param_fp param_bf zip ref_model parameters ref_model_bf parameters param_fp grad = param_bf grad param_bf grad = None ref_optim step param_fp param_bf zip ref_model parameters ref_model_bf parameters param_bf detach copy_ param_fp ref_optim zero_grad assertEqual ref_loss loss check_sharded_parity ref_model model skip_if_lt_x_gpu test_set_reshard_after_forward Tests FSDP issues expected number all-gathers reduce-scatters during train step when setting reshard_after_forward comm_count should perform same test_fully_shard_communication_count run_subtests set_reshard_after_forward True False None recurse True False _test_set_reshard_after_forward_by_communication_count _test_set_reshard_after_forward_by_communication_count set_reshard_after_forward Union bool None recurse bool torch manual_seed model_args = ModelArgs model = Transformer model_args device_type set_reshard_after_forward None fully_shard_fn = fully_shard fully_shard_fn = functools partial fully_shard reshard_after_forward=not set_reshard_after_forward num_blocks = module model modules isinstance module TransformerBlock fully_shard_fn module num_blocks += fully_shard_fn model num_fsdp_modules = sum isinstance module FSDPModule module model modules set_reshard_after_forward None model set_reshard_after_forward reshard_after_forward=set_reshard_after_forward recurse=recurse torch manual_seed + rank inp = torch randint model_args vocab_size device=device_type type CommDebugMode fwd_comm_mode loss = model inp fwd_comm_counts = fwd_comm_mode get_comm_counts assertEqual len fwd_comm_counts assertEqual fwd_comm_counts c d_ops _allgather_base_ num_fsdp_modules CommDebugMode bwd_comm_mode loss sum backward bwd_comm_counts = bwd_comm_mode get_comm_counts If recurse False set_reshard_after_forward only affects root module set_reshard_after_forward None assertEqual len bwd_comm_counts assertEqual bwd_comm_counts c d_ops _allgather_base_ num_blocks set_reshard_after_forward assertEqual len bwd_comm_counts assertEqual bwd_comm_counts c d_ops _allgather_base_ num_blocks + recurse recurse assertEqual len bwd_comm_counts assertEqual len bwd_comm_counts assertEqual bwd_comm_counts c d_ops _allgather_base_ num_blocks assertEqual bwd_comm_counts c d_ops _reduce_scatter_base_ num_blocks + TestFullyShardPrefetch FSDPTest property world_size - int min torch get_device_module device_type device_count skip_if_lt_x_gpu test_fully_shard_backward_prefetch Activation checkpointing should affect expected FSDP events run_subtests reshard_after_forward True False None checkpoint_impl None utils composable _test_backward_prefetch_forward_backward run_subtests reshard_after_forward True False None checkpoint_impl None utils composable _test_backward_prefetch_multi_forward _test_backward_prefetch_unused_in_backward True _test_backward_prefetch_forward_backward reshard_after_forward Union bool int None checkpoint_impl Optional str n_layers = model optim inp = _init_transformer n_layers reshard_after_forward checkpoint_impl events list EventType = unshard_with_record = _get_unshard_with_record FSDPParamGroup unshard events post_backward_with_record = _get_post_backward_with_record FSDPParamGroup post_backward events Check order normal forward backward optimizer step patch_unshard unshard_with_record patch_post_backward post_backward_with_record iter_idx range loss = model inp expected_events = unshard TrainingState FORWARD root unshard layers TrainingState FORWARD unshard layers TrainingState FORWARD unshard layers TrainingState FORWARD assertEqual events expected_events events clear loss sum backward expected_events = Root does reshard after forward so there no unshard event backward reshard_after_forward None expected_events append unshard TrainingState PRE_BACKWARD expected_events extend unshard layers TrainingState PRE_BACKWARD Explicit backward prefetching moves unshards early one module note how swapping each unshard down one event would give natural event order unshard layers TrainingState PRE_BACKWARD post_backward layers TrainingState POST_BACKWARD unshard layers TrainingState PRE_BACKWARD post_backward layers TrainingState POST_BACKWARD post_backward layers TrainingState POST_BACKWARD post_backward TrainingState POST_BACKWARD reshard_after_forward False No reshard after forward means no backward unshards expected_events = e e expected_events e = unshard assertEqual events expected_events events clear optim step optim zero_grad set_to_none= iter_idx == _test_backward_prefetch_multi_forward reshard_after_forward Union bool int checkpoint_impl Optional str n_layers = model _ inp = _init_transformer n_layers reshard_after_forward checkpoint_impl events list EventType = unshard_with_record = _get_unshard_with_record FSDPParamGroup unshard events post_backward_with_record = _get_post_backward_with_record FSDPParamGroup post_backward events Check order multiple forwards before backward patch_unshard unshard_with_record patch_post_backward post_backward_with_record loss = model inp loss = model inp expected_events = unshard TrainingState FORWARD root unshard layers TrainingState FORWARD unshard layers TrainingState FORWARD unshard layers TrainingState FORWARD reshard_after_forward None expected_events append unshard TrainingState FORWARD expected_events extend unshard layers TrainingState FORWARD unshard layers TrainingState FORWARD unshard layers TrainingState FORWARD reshard_after_forward False No reshard after forward means no second set unshards expected_events = expected_events - assertEqual events expected_events events clear loss + loss sum backward expected_events = reshard_after_forward None expected_events append unshard TrainingState PRE_BACKWARD expected_events extend Same single forward backward case except root s post-backward does run until end backward final callback since input requiring gradient means we do have tensor which hook post-backward unshard layers TrainingState PRE_BACKWARD unshard layers TrainingState PRE_BACKWARD post_backward layers TrainingState POST_BACKWARD unshard layers TrainingState PRE_BACKWARD post_backward layers TrainingState POST_BACKWARD post_backward layers TrainingState POST_BACKWARD reshard_after_forward False No reshard after forward means no backward unshards expected_events = e e expected_events e = unshard However post-backward reshards so second set unshards will run real ops expected_events += Repeat same pattern except root s post-backward end since final callback runs unshard layers TrainingState PRE_BACKWARD unshard layers TrainingState PRE_BACKWARD post_backward layers TrainingState POST_BACKWARD unshard layers TrainingState PRE_BACKWARD post_backward layers TrainingState POST_BACKWARD post_backward layers TrainingState POST_BACKWARD post_backward TrainingState POST_BACKWARD assertEqual events expected_events events clear _test_backward_prefetch_unused_in_backward reshard_after_forward Union bool int None Test model linear module then split into two linear modules where we run backward through one path first before other meaning only one linear two split used per backward initial shared linear used both backwards dim = model = nn Sequential nn Linear dim dim DoubleLinear dim fully_shard model reshard_after_forward=reshard_after_forward fully_shard model lin reshard_after_forward=reshard_after_forward fully_shard model lin reshard_after_forward=reshard_after_forward fully_shard model reshard_after_forward=reshard_after_forward inp = torch randn dim device=device_type type events list EventType = unshard_with_record = _get_unshard_with_record FSDPParamGroup unshard events post_backward_with_record = _get_post_backward_with_record FSDPParamGroup post_backward events patch_unshard unshard_with_record patch_post_backward post_backward_with_record loss loss = model inp expected_events = Root has no parameters so does have unshard unshard TrainingState FORWARD unshard lin TrainingState FORWARD unshard lin TrainingState FORWARD assertEqual events expected_events events clear model set_is_last_backward False loss sum backward retain_graph=True expected_events = unshard lin TrainingState PRE_BACKWARD NOTE This ` lin ` unshard mistargeted prefetch unshard lin TrainingState PRE_BACKWARD post_backward lin TrainingState POST_BACKWARD unshard TrainingState PRE_BACKWARD post_backward TrainingState POST_BACKWARD ` lin ` post-backward hook runs no-op post_backward lin TrainingState POST_BACKWARD assertEqual events expected_events events clear model set_is_last_backward True loss sum backward expected_events = NOTE ` lin ` already unsharded mistargeted prefetch first backward Prefetch ` ` unshard TrainingState PRE_BACKWARD post_backward lin TrainingState POST_BACKWARD post_backward TrainingState POST_BACKWARD ` lin ` post-backward hook runs no-op post_backward lin TrainingState POST_BACKWARD assertEqual events expected_events events clear skip_if_lt_x_gpu test_set_modules_to_forward_prefetch n_layers = reshard_after_forward = True checkpoint_impl = utils model _ inp = _init_transformer n_layers reshard_after_forward checkpoint_impl set_forward_prefetch model Transformer num_to_prefetch int - None Use model-specific knowledge configure forward prefetching each transformer block layer prefetches next few i layer enumerate model layers i = len model layers - num_to_prefetch break layers_to_prefetch = model layers i + j j range num_to_prefetch + layer set_modules_to_forward_prefetch layers_to_prefetch events list EventType = unshard_with_record = _get_unshard_with_record FSDPParamGroup unshard events reshard_with_record = _get_reshard_with_record FSDPParamGroup reshard events post_backward_with_record = _get_post_backward_with_record FSDPParamGroup post_backward events expected_backward_events = Default backward prefetching unshard TrainingState PRE_BACKWARD unshard layers TrainingState PRE_BACKWARD unshard layers TrainingState PRE_BACKWARD reshard layers TrainingState POST_BACKWARD post_backward layers TrainingState POST_BACKWARD unshard layers TrainingState PRE_BACKWARD reshard layers TrainingState POST_BACKWARD post_backward layers TrainingState POST_BACKWARD unshard layers TrainingState PRE_BACKWARD reshard layers TrainingState POST_BACKWARD post_backward layers TrainingState POST_BACKWARD reshard layers TrainingState POST_BACKWARD post_backward layers TrainingState POST_BACKWARD reshard TrainingState POST_BACKWARD post_backward TrainingState POST_BACKWARD patch_unshard unshard_with_record patch_reshard reshard_with_record patch_post_backward post_backward_with_record set_forward_prefetch model num_to_prefetch= loss = model inp expected_forward_events = unshard TrainingState FORWARD ` layers i ` prefetches ` layers i+ ` unshard layers TrainingState FORWARD unshard layers TrainingState FORWARD reshard layers TrainingState FORWARD unshard layers TrainingState FORWARD reshard layers TrainingState FORWARD unshard layers TrainingState FORWARD reshard layers TrainingState FORWARD reshard layers TrainingState FORWARD reshard TrainingState FORWARD assertEqual events expected_forward_events events clear loss sum backward assertEqual events expected_backward_events events clear set_forward_prefetch model num_to_prefetch= loss = model inp expected_forward_events = unshard TrainingState FORWARD ` layers i ` prefetches ` layers i+ ` ` layers i+ ` unshard layers TrainingState FORWARD unshard layers TrainingState FORWARD unshard layers TrainingState FORWARD reshard layers TrainingState FORWARD unshard layers TrainingState FORWARD reshard layers TrainingState FORWARD reshard layers TrainingState FORWARD reshard layers TrainingState FORWARD reshard TrainingState FORWARD assertEqual events expected_forward_events events clear loss sum backward assertEqual events expected_backward_events events clear skip_if_lt_x_gpu test_set_modules_to_backward_prefetch n_layers = reshard_after_forward = True checkpoint_impl = utils model _ inp = _init_transformer n_layers reshard_after_forward checkpoint_impl set_backward_prefetch model Transformer num_to_prefetch int - None Use model-specific knowledge configure backward prefetching each transformer block layer prefetches previous few i layer enumerate model layers i num_to_prefetch continue layers_to_prefetch = model layers i - j j range num_to_prefetch + layer set_modules_to_backward_prefetch layers_to_prefetch events list EventType = unshard_with_record = _get_unshard_with_record FSDPParamGroup unshard events reshard_with_record = _get_reshard_with_record FSDPParamGroup reshard events post_backward_with_record = _get_post_backward_with_record FSDPParamGroup post_backward events expected_forward_events = Default forward prefetching unshard TrainingState FORWARD root unshard layers TrainingState FORWARD reshard layers TrainingState FORWARD unshard layers TrainingState FORWARD reshard layers TrainingState FORWARD unshard layers TrainingState FORWARD reshard layers TrainingState FORWARD unshard layers TrainingState FORWARD reshard layers TrainingState FORWARD reshard TrainingState FORWARD patch_unshard unshard_with_record patch_reshard reshard_with_record patch_post_backward post_backward_with_record set_backward_prefetch model num_to_prefetch= loss = model inp assertEqual events expected_forward_events events clear loss sum backward expected_backward_events = unshard TrainingState PRE_BACKWARD Root prefetches ` layers ` per default unshard layers TrainingState PRE_BACKWARD ` layers i ` prefetches ` layers i- ` same default unshard layers TrainingState PRE_BACKWARD reshard layers TrainingState POST_BACKWARD post_backward layers TrainingState POST_BACKWARD unshard layers TrainingState PRE_BACKWARD reshard layers TrainingState POST_BACKWARD post_backward layers TrainingState POST_BACKWARD unshard layers TrainingState PRE_BACKWARD reshard layers TrainingState POST_BACKWARD post_backward layers TrainingState POST_BACKWARD reshard layers TrainingState POST_BACKWARD post_backward layers TrainingState POST_BACKWARD reshard TrainingState POST_BACKWARD post_backward TrainingState POST_BACKWARD assertEqual events expected_backward_events events clear set_backward_prefetch model num_to_prefetch= loss = model inp assertEqual events expected_forward_events events clear loss sum backward expected_backward_events = unshard TrainingState PRE_BACKWARD Root prefetches ` layers ` per default unshard layers TrainingState PRE_BACKWARD ` layers i ` prefetches ` layers i- ` ` layers i- ` unshard layers TrainingState PRE_BACKWARD unshard layers TrainingState PRE_BACKWARD reshard layers TrainingState POST_BACKWARD post_backward layers TrainingState POST_BACKWARD unshard layers TrainingState PRE_BACKWARD reshard layers TrainingState POST_BACKWARD post_backward layers TrainingState POST_BACKWARD reshard layers TrainingState POST_BACKWARD post_backward layers TrainingState POST_BACKWARD reshard layers TrainingState POST_BACKWARD post_backward layers TrainingState POST_BACKWARD reshard TrainingState POST_BACKWARD post_backward TrainingState POST_BACKWARD assertEqual events expected_backward_events events clear skip_if_lt_x_gpu test_set_modules_to_backward_prefetch_inside_ac n_layers = reshard_after_forward = True use checkpoint wrapper instead torch utils model_args = ModelArgs n_layers=n_layers checkpoint_activations=False model = Transformer model_args apply_activation_checkpointing model check_fn=lambda m isinstance m TransformerBlock apply_activation_checkpointing model check_fn=lambda m isinstance m FeedForward fully_shard model tok_embeddings model pos_embeddings layer model layers mimic fully_shard layer moe experts fully_shard layer feed_forward w reshard_after_forward=reshard_after_forward fully_shard layer reshard_after_forward=reshard_after_forward fully_shard model norm model output reshard_after_forward=reshard_after_forward fully_shard model reshard_after_forward=reshard_after_forward inp = torch randint model_args vocab_size model_args max_seq_len device=device_type type set_backward_prefetch model Transformer - None tell pyre model set_modules_to_backward_prefetch available assert isinstance model FSDPModule assert isinstance model output FSDPModule mimic deepseek MOE prefetch layer - its feedforward before cpu sync during reversed_transformer_blocks = list reversed model layers prev_transformer_blocks = reversed_transformer_blocks + None model norm None model output None len model layers assert isinstance reversed_transformer_blocks FSDPModule model output set_modules_to_backward_prefetch reversed_transformer_blocks transformer_block prev_transformer_block zip reversed_transformer_blocks prev_transformer_blocks assert isinstance transformer_block FSDPModule prev_transformer_block None assert isinstance prev_transformer_block FSDPModule assert hasattr prev_transformer_block feed_forward w assert isinstance prev_transformer_block feed_forward w FSDPModule transformer_block set_modules_to_backward_prefetch prev_transformer_block prev_transformer_block feed_forward w model tok_embeddings None assert isinstance model tok_embeddings FSDPModule transformer_block set_modules_to_backward_prefetch model tok_embeddings events list EventType = unshard_with_record = _get_unshard_with_record FSDPParamGroup unshard events reshard_with_record = _get_reshard_with_record FSDPParamGroup reshard events patch_unshard unshard_with_record patch_reshard reshard_with_record loss = model inp events clear loss sum backward expected_backward_events = unshard norm output TrainingState PRE_BACKWARD unshard layers TrainingState PRE_BACKWARD reshard norm output TrainingState POST_BACKWARD layers prefetch w unshard layers _checkpoint_wrapped_module feed_forward _checkpoint_wrapped_module w TrainingState PRE_BACKWARD layers w prefetch layers unshard layers TrainingState PRE_BACKWARD reshard layers _checkpoint_wrapped_module feed_forward _checkpoint_wrapped_module w TrainingState POST_BACKWARD reshard layers TrainingState POST_BACKWARD unshard layers _checkpoint_wrapped_module feed_forward _checkpoint_wrapped_module w TrainingState PRE_BACKWARD unshard layers TrainingState PRE_BACKWARD reshard layers _checkpoint_wrapped_module feed_forward _checkpoint_wrapped_module w TrainingState POST_BACKWARD reshard layers TrainingState POST_BACKWARD unshard layers _checkpoint_wrapped_module feed_forward _checkpoint_wrapped_module w TrainingState PRE_BACKWARD unshard tok_embeddings pos_embeddings TrainingState PRE_BACKWARD reshard layers _checkpoint_wrapped_module feed_forward _checkpoint_wrapped_module w TrainingState POST_BACKWARD reshard layers TrainingState POST_BACKWARD reshard tok_embeddings pos_embeddings TrainingState POST_BACKWARD reshard tok_embeddings pos_embeddings TrainingState POST_BACKWARD reshard norm output TrainingState POST_BACKWARD assertEqual events expected_backward_events events clear set_backward_prefetch model loss = model inp events clear loss sum backward expected_backward_events = unshard norm output TrainingState PRE_BACKWARD root explicit prefetch layers unshard layers TrainingState PRE_BACKWARD reshard norm output TrainingState POST_BACKWARD layers prefetch layers feed_forward unshard layers TrainingState PRE_BACKWARD unshard layers _checkpoint_wrapped_module feed_forward _checkpoint_wrapped_module w TrainingState PRE_BACKWARD AC recompute_fn unshard layers _checkpoint_wrapped_module feed_forward _checkpoint_wrapped_module w TrainingState FORWARD reshard layers _checkpoint_wrapped_module feed_forward _checkpoint_wrapped_module w TrainingState POST_BACKWARD reshard layers TrainingState POST_BACKWARD layers prefetch layers unshard layers TrainingState PRE_BACKWARD unshard layers _checkpoint_wrapped_module feed_forward _checkpoint_wrapped_module w TrainingState PRE_BACKWARD reshard layers _checkpoint_wrapped_module feed_forward _checkpoint_wrapped_module w TrainingState POST_BACKWARD reshard layers TrainingState POST_BACKWARD layers prefetch embeddings unshard tok_embeddings pos_embeddings TrainingState PRE_BACKWARD reshard layers _checkpoint_wrapped_module feed_forward _checkpoint_wrapped_module w TrainingState POST_BACKWARD reshard layers TrainingState POST_BACKWARD reshard tok_embeddings pos_embeddings TrainingState POST_BACKWARD reshard tok_embeddings pos_embeddings TrainingState POST_BACKWARD reshard norm output TrainingState POST_BACKWARD assertEqual events expected_backward_events events clear skip_if_lt_x_gpu test_fully_shard_multi_module_backward_prefetch n_layers = model_args = ModelArgs n_layers=n_layers checkpoint_activations=True model = Transformer model_args i range n_layers i == fully_shard model layers i i == fully_shard model layers i model layers i + fully_shard model tok_embeddings model pos_embeddings fully_shard model norm model output reshard_after_forward=False fully_shard model optim = torch optim AdamW model parameters lr= e- events list EventType = unshard_with_record = _get_unshard_with_record FSDPParamGroup unshard events post_backward_with_record = _get_post_backward_with_record FSDPParamGroup post_backward events inp = torch randint model_args vocab_size model_args max_seq_len device=device_type type patch_unshard unshard_with_record patch_post_backward post_backward_with_record _ range loss = model inp expected_events = unshard tok_embeddings pos_embeddings TrainingState FORWARD unshard layers TrainingState FORWARD unshard layers layers TrainingState FORWARD unshard layers layers TrainingState FORWARD unshard norm output TrainingState FORWARD assertEqual events expected_events events clear loss sum backward expected_events = norm output does reshard after forward so there no unshard begin backward unshard layers layers TrainingState PRE_BACKWARD post_backward norm output TrainingState POST_BACKWARD unshard layers layers TrainingState PRE_BACKWARD post_backward layers layers TrainingState POST_BACKWARD unshard layers TrainingState PRE_BACKWARD post_backward layers layers TrainingState POST_BACKWARD unshard tok_embeddings pos_embeddings TrainingState PRE_BACKWARD post_backward layers TrainingState POST_BACKWARD post_backward tok_embeddings pos_embeddings TrainingState POST_BACKWARD events clear optim step optim zero_grad skip_if_lt_x_gpu test_fully_shard_multi_module_unused_module ModuleWithUnusedLinear nn Module __init__ - None super __init__ unused_lin = nn Linear lin = nn Linear forward x torch Tensor - torch Tensor nn functional relu lin x model = nn Sequential ModuleWithUnusedLinear ModuleWithUnusedLinear nn Linear fully_shard model unused_lin model lin reshard_after_forward=True fully_shard model unused_lin model lin reshard_after_forward=True fully_shard model optim = torch optim AdamW model parameters lr= e- events list EventType = unshard_with_record = _get_unshard_with_record FSDPParamGroup unshard events post_backward_with_record = _get_post_backward_with_record FSDPParamGroup post_backward events inp = torch randn device=device_type type patch_unshard unshard_with_record patch_post_backward post_backward_with_record _ range loss = model inp expected_events = unshard TrainingState FORWARD unshard unused_lin lin TrainingState FORWARD unshard unused_lin lin TrainingState FORWARD assertEqual events expected_events events clear loss sum backward expected_events = Since both ` model ` ` model ` have unused modules never ran forward they do reshard after forward despite setting ` True ` Check there no unshards backward post_backward unused_lin lin TrainingState POST_BACKWARD post_backward unused_lin lin TrainingState POST_BACKWARD post_backward TrainingState POST_BACKWARD events clear optim step optim zero_grad skip_if_lt_x_gpu test_backward_misprefetch torch manual_seed model = MLP dim= device=device_type ref_model = copy deepcopy model ref_optim = torch optim Adam ref_model parameters lr= e- fully_shard model in_proj fully_shard model out_proj fully_shard model optim = torch optim Adam model parameters lr= e- Backward should run through ` out_proj ` - ` in_proj ` so ` in_proj ` prefetches ` out_proj ` then misprefetch ` out_proj ` should needed anymore backward model in_proj set_modules_to_backward_prefetch model out_proj torch manual_seed rank + inp = torch randn device=device_type type _ range ref_optim zero_grad ref_loss = ref_model inp sum ref_loss backward param ref_model parameters dist all_reduce param grad op=dist ReduceOp AVG ref_optim step optim zero_grad loss = model inp sum loss backward optim step assertEqual ref_loss loss _init_transformer n_layers int reshard_after_forward Union bool int None checkpoint_impl Optional str model_args = ModelArgs n_layers=n_layers checkpoint_activations= checkpoint_impl == utils model = Transformer model_args module model modules isinstance module TransformerBlock checkpoint_impl == composable checkpoint module fully_shard module reshard_after_forward=reshard_after_forward fully_shard model reshard_after_forward=reshard_after_forward optim = torch optim Adam model parameters lr= e- inp = torch randint model_args vocab_size model_args max_seq_len device=device_type type model optim inp _get_unshard_with_record orig_unshard Callable events list EventType - Callable unshard_with_record args kwargs nonlocal events _all_gather_result None _sharded_state = ShardedState UNSHARDED skip no-ops events append unshard _module_fqn _training_state orig_unshard args kwargs unshard_with_record _get_reshard_with_record orig_reshard Callable events list EventType - Callable reshard_with_record args kwargs nonlocal events _training_state == TrainingState FORWARD _reshard_after_forward skip no-ops events append reshard _module_fqn _training_state orig_reshard args kwargs reshard_with_record _get_post_backward_with_record orig_post_backward Callable events list EventType - Callable post_backward_with_record args kwargs nonlocal events ret = orig_post_backward args kwargs Use training state after running post-backward check state transitioned ` POST_BACKWARD ` expected events append post_backward _module_fqn _training_state ret post_backward_with_record TestFullyShardUnshardMultiProcess FSDPTest property world_size - int min torch get_device_module device_type device_count skip_if_lt_x_gpu test_unshard_async ReduceModule nn Module __init__ dim int mesh DeviceMesh super __init__ mesh = mesh weight = nn Parameter torch randn dim dim forward x torch Tensor y = F relu x weight NOTE This all-reduce differentiable included exercise overlap work = dist all_reduce y group=self mesh get_group async_op=True y work MLPs nn Module __init__ dim int super __init__ mlp = MLP dim mlp = MLP dim mlp = MLP dim forward ys list torch Tensor works list dist Work y y y work work work = ys works work wait z = mlp y work wait z = mlp y work wait z = mlp y z + z + z ReduceModel nn Module __init__ dim int mesh DeviceMesh super __init__ reduce_module = ReduceModule dim mesh reduce_module = ReduceModule dim mesh reduce_module = ReduceModule dim mesh mlps = MLPs dim forward x torch Tensor y work = reduce_module x isinstance mlps mlp FSDPModule mlps mlp unshard async_op=True y work = reduce_module x isinstance mlps mlp FSDPModule mlps mlp unshard async_op=True y work = reduce_module x isinstance mlps mlp FSDPModule mlps mlp unshard async_op=True mlps y y y work work work mesh = init_device_mesh device_type type world_size batch_size dim = torch manual_seed ref_model = replicate ReduceModel dim mesh device_type ref_optim = torch optim Adam ref_model parameters lr= e- torch manual_seed model = ReduceModel dim mesh fully_shard model mlps mlp reshard_after_forward=False fully_shard model mlps mlp reshard_after_forward=False fully_shard model mlps mlp reshard_after_forward=False fully_shard model mlps replicate model device_type optim = torch optim Adam model parameters lr= e- foreach=True torch manual_seed + rank + inp = torch randn batch_size dim device=device_type type _ range losses list torch Tensor = _model _optim ref_model ref_optim model optim losses append _model inp sum losses - backward implicit_replication _optim step _optim zero_grad assertEqual losses losses TestFullyShardUnshardMultiThread FSDPTestMultiThread property world_size - int skip_if_lt_x_gpu test_unshard_no_param_group Check we can call ` unshard ` module no parameter group no managed parameters without erroring model = nn Sequential nn Linear nn Linear lin model fully_shard lin fully_shard model handle = model unshard async_op=True handle wait skip_if_lt_x_gpu test_unshard_without_lazy_init torch manual_seed model = MLP param model parameters dist broadcast param src= ref_model = copy deepcopy model fully_shard model model unshard no lazy init yet ref_param param zip ref_model parameters model parameters assertEqual ref_param param TestFullyShardAllocFromPG FSDPTest The messages might change when we move different NCCL version Please update test starts failing MEMORY_REGISTER_RE = NCCL INFO register comm x - a-f + buffer x - a-f + size - + classmethod _run cls args kwargs cls nccl_log_dir = tempfile TemporaryDirectory os environ NCCL_DEBUG = INFO os environ NCCL_DEBUG_SUBSYS = INIT ENV REG os environ NCCL_DEBUG_FILE = cls nccl_log_dir name + nccl_log super _run args kwargs skip_if_lt_x_gpu The NCCL PG refuses allocate tensors multicast unavailable see https github com pytorch pytorch blob d b af dbd ca c torch csrc distributed c d ProcessGroupNCCL cpp#L requires_multicast_support test_fully_shard_alloc_from_pg torch manual_seed model_args = ModelArgs model = Transformer model_args module model modules isinstance module TransformerBlock fully_shard module fully_shard model torch manual_seed + rank inp = torch randint model_args vocab_size device= cuda loss = model inp loss sum backward torch distributed barrier torch cuda synchronize open nccl_log_dir name + nccl_log f assertNotRegex f read MEMORY_REGISTER_RE module model modules isinstance module TransformerBlock module set_allocate_memory_from_process_group_for_comm True model set_allocate_memory_from_process_group_for_comm True loss = model inp loss sum backward torch distributed barrier torch cuda synchronize open nccl_log_dir name + nccl_log f assertRegex f read MEMORY_REGISTER_RE skip_if_lt_x_gpu test_exception_when_used_together_with_comm_hooks model = nn Linear model = fully_shard model ok model set_allocate_memory_from_process_group_for_comm True setting custom hook after also ok overrides set_allocate_memory_from_process_group_for_comm mock_all_gather = MagicMock spec=AllGather model set_custom_all_gather mock_all_gather setting after custom comm used ko assertRaises AssertionError model set_allocate_memory_from_process_group_for_comm True TestFullyShardForceSumReduction FSDPTest The messages might change when we move different NCCL version Please update test starts failing COLLECTIVE_RE = NCCL INFO coll opCount - a-f + sendbuff x - a-f + recvbuff x - a-f + count count datatype - + op reduce_op root - + comm x - a-f + See here numerical values each reduction op https github com NVIDIA nccl blob d d ae abd e c d dbf src nccl h in#L -L SUM_REDUCTION = AVG_REDUCTION = classmethod _run cls args kwargs cls nccl_log_dir = tempfile TemporaryDirectory os environ NCCL_DEBUG = INFO os environ NCCL_DEBUG_SUBSYS = COLL os environ NCCL_DEBUG_FILE = cls nccl_log_dir name + nccl_log super _run args kwargs Test reduce-scatter only plain FSDP GPUs skip_if_lt_x_gpu unittest skipIf TEST_XPU Related environment variable supported XCCL test_fully_shard_force_sum_reduce_scatter torch manual_seed model_args = ModelArgs model = Transformer model_args module model modules isinstance module TransformerBlock fully_shard module fully_shard model We target specific count so we don t pick up barrier ops layer_numel = sum w numel w model layers parameters comms_size = layer_numel world_size reduce_scatter_avg_re = COLLECTIVE_RE format coll= ReduceScatter count=comms_size reduce_op=self AVG_REDUCTION reduce_scatter_sum_re = COLLECTIVE_RE format coll= ReduceScatter count=comms_size reduce_op=self SUM_REDUCTION torch manual_seed + rank inp = torch randint model_args vocab_size device= cuda loss = model inp loss sum backward torch distributed barrier torch cuda synchronize open nccl_log_dir name + nccl_log f logs = f read At stage we should have only AVG no SUM assertRegex logs reduce_scatter_avg_re assertNotRegex logs reduce_scatter_sum_re module model modules isinstance module TransformerBlock module set_force_sum_reduction_for_comms True model set_force_sum_reduction_for_comms True loss = model inp loss sum backward torch distributed barrier torch cuda synchronize open nccl_log_dir name + nccl_log f logs = f read Now we should also have SUM assertRegex logs reduce_scatter_sum_re Test both reduce-scatter all-reduce HSDP DDP+FSDP GPUs skip_if_lt_x_gpu unittest skipIf TEST_XPU Related environment variable supported XCCL test_fully_shard_force_sum_both_reductions mesh = init_device_mesh device_type type world_size mesh_dim_names= ddp fsdp torch manual_seed model_args = ModelArgs model = Transformer model_args module model modules isinstance module TransformerBlock fully_shard module mesh=mesh fully_shard model mesh=mesh We target specific count so we don t pick up barrier ops layer_numel = sum w numel w model layers parameters comms_size = layer_numel world_size reduce_scatter_avg_re = COLLECTIVE_RE format coll= ReduceScatter count=comms_size reduce_op=self AVG_REDUCTION reduce_scatter_sum_re = COLLECTIVE_RE format coll= ReduceScatter count=comms_size reduce_op=self SUM_REDUCTION all_reduce_avg_re = COLLECTIVE_RE format coll= AllReduce count=comms_size reduce_op=self AVG_REDUCTION all_reduce_sum_re = COLLECTIVE_RE format coll= AllReduce count=comms_size reduce_op=self SUM_REDUCTION torch manual_seed + rank inp = torch randint model_args vocab_size device= cuda loss = model inp loss sum backward torch distributed barrier torch cuda synchronize open nccl_log_dir name + nccl_log f logs = f read At stage we should have only AVG no SUM assertRegex logs reduce_scatter_avg_re assertRegex logs all_reduce_avg_re assertNotRegex logs reduce_scatter_sum_re assertNotRegex logs all_reduce_sum_re module model modules isinstance module TransformerBlock module set_force_sum_reduction_for_comms True model set_force_sum_reduction_for_comms True loss = model inp loss sum backward torch distributed barrier torch cuda synchronize open nccl_log_dir name + nccl_log f logs = f read Now we should also have SUM assertRegex logs reduce_scatter_sum_re assertRegex logs all_reduce_sum_re TestFullyShardReduceOpWorldSize FSDPTest property world_size - int test_size _reduceop torch distributed distributed_c d ReduceOp model = nn Linear ref_model = copy deepcopy model device_type ref_optim = torch optim Adam ref_model parameters fully_shard model mesh=init_device_mesh device_type type reshard_after_forward=False optim = torch optim Adam model parameters inp = torch randn device=device_type type _ range ref_optim zero_grad ref_loss = ref_model inp sum ref_loss backward param ref_model parameters dist all_reduce param grad op=dist ReduceOp SUM ref_optim step optim zero_grad loss = model inp sum loss backward optim step assertEqual loss ref_loss assertEqual model bias grad _local_tensor ref_model bias grad state = model _get_fsdp_state fsdp_param_group = state _fsdp_param_group group = fsdp_param_group mesh_info shard_process_group _ _ _ all_reduce_op = _get_gradient_divide_factors group None torch float assertEqual all_reduce_op ReduceOp SUM __name__ == __main__ run_tests