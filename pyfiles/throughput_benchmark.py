mypy allow-untyped-defs torch _C format_time time_us=None time_ms=None time_s=None Define time formatting sum time_us None time_ms None time_s None = raise AssertionError Expected only one time_us time_ms time_s given US_IN_SECOND = e US_IN_MS = e time_us None time_ms None time_us = time_ms US_IN_MS time_s None time_us = time_s US_IN_SECOND raise AssertionError Shouldn t reach here time_us = US_IN_SECOND f time_us US_IN_SECOND f s time_us = US_IN_MS f time_us US_IN_MS f ms f time_us f us ExecutionStats __init__ c_stats benchmark_config _c_stats = c_stats benchmark_config = benchmark_config property latency_avg_ms _c_stats latency_avg_ms property num_iters _c_stats num_iters property iters_per_second Return total number iterations per second across all calling threads num_iters total_time_seconds property total_time_seconds num_iters latency_avg_ms benchmark_config num_calling_threads __str__ \n join Average latency per example + format_time time_ms=self latency_avg_ms f Total number iterations num_iters f Total number iterations per second across all threads iters_per_second f Total time + format_time time_s=self total_time_seconds ThroughputBenchmark This wrapper around c++ component throughput_benchmark ThroughputBenchmark This wrapper throughput_benchmark ThroughputBenchmark component responsible executing PyTorch module nn Module ScriptModule under inference server like load It can emulate multiple calling threads single module provided In future we plan enhance component support inter intra-op parallelism well multiple models running single process Please note even though nn Module supported might incur overhead need hold GIL every time we execute Python code pass around inputs Python objects As soon you have ScriptModule version your model inference deployment better switch using benchmark Example xdoctest +SKIP undefined vars torch utils ThroughputBenchmark bench = ThroughputBenchmark my_module Pre-populate benchmark s data set inputs input inputs Both args kwargs work same any PyTorch Module ScriptModule bench add_input input x =input Inputs supplied above randomly used during execution stats = bench benchmark num_calling_threads= num_warmup_iters = num_iters = print Avg latency ms format stats latency_avg_ms print Number iterations format stats num_iters __init__ module isinstance module torch jit ScriptModule _benchmark = torch _C ThroughputBenchmark module _c _benchmark = torch _C ThroughputBenchmark module run_once args kwargs Given input id input_idx run benchmark once prediction This useful testing benchmark actually runs module you want run input_idx here index into inputs array populated calling add_input method _benchmark run_once args kwargs add_input args kwargs Store single input module into benchmark memory keep there During benchmark execution every thread going pick up random input all inputs ever supplied benchmark via function _benchmark add_input args kwargs benchmark num_calling_threads= num_warmup_iters= num_iters= profiler_output_path= Run benchmark module Args num_warmup_iters int Warmup iters used make sure we run module few times before actually measuring things This way we avoid cold caches any other similar problems This number warmup iterations each thread separate num_iters int Number iterations benchmark should run This number separate warmup iterations Also number shared across all threads Once num_iters iterations across all threads reached we will stop execution Though total number iterations might slightly larger Which reported stats num_iters where stats result function profiler_output_path str Location save Autograd Profiler trace If empty Autograd Profiler will enabled main benchmark execution warmup phase The full trace will saved into file path provided argument This function returns BenchmarkExecutionStats object which defined via pybind It currently has two fields - num_iters - number actual iterations benchmark have made - avg_latency_ms - average time took infer one input example milliseconds config = torch _C BenchmarkConfig config num_calling_threads = num_calling_threads config num_warmup_iters = num_warmup_iters config num_iters = num_iters config profiler_output_path = profiler_output_path c_stats = _benchmark benchmark config ExecutionStats c_stats config