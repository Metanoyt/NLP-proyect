mypy allow-untyped-defs This module responsible transforming functions traced into form easier downstream infra e g Autograd FX AOTAutograd analysis handle It does so functionalization including RNG functionalzation creating joint graph when required transforming mutations into extra outputs dispatching subclasses warnings collections abc Callable contextlib AbstractContextManager contextmanager ExitStack nullcontext dataclasses dataclass typing Any Optional TypeVar Union unittest mock patch torch torch fx traceback fx_traceback torch utils _pytree pytree torch Tensor torch _decomp decompositions_for_rng PhiloxStateTracker torch _guards detect_fake_mode torch _prims_common CUDARngStateHelper torch fx experimental proxy_tensor _proxy_tensor_disable_update_tensor_tracker maybe_disable_thunkify maybe_enable_thunkify torch fx experimental symbolic_shapes guard_or_true PropagateUnbackedSymInts sym_eq torch nn utils stateless torch utils _python_dispatch is_traceable_wrapper_subclass torch utils _pytree TreeSpec config collect_metadata_analysis run_functionalized_fw_and_collect_metadata descriptors AOTInput AOTOutput BackwardTokenAOTOutput ForwardTokenAOTInput ForwardTokenAOTOutput GradAOTOutput InputMutationAOTOutput IntermediateBaseAOTOutput PhiloxBackwardBaseOffsetAOTInput PhiloxBackwardSeedAOTInput PhiloxForwardBaseOffsetAOTInput PhiloxForwardSeedAOTInput PhiloxUpdatedBackwardOffsetAOTOutput PhiloxUpdatedForwardOffsetAOTOutput functional_utils _check_if_mutation_can_be_in_graph are_all_mutations_hidden_from_autograd are_all_mutations_under_no_grad_or_inference_mode from_fun has_data_mutation has_metadata_mutation is_fun sync_functional_tensor to_fun was_inductor_storage_resized logging_utils setup_stacktrace_preservation_hooks schemas AOTConfig FxValue JointTraceFn MutationType OutputType PreppedForAutogradTraceFn SubclassMeta SubclassTracingInfo TraceFn ViewAndMutationMeta subclass_utils create_subclass_meta remap_unwrapped_subclass_arg_indices requires_subclass_dispatch unwrap_tensor_subclasses wrap_tensor_subclasses_maybe_joint utils call_and_expect_output_descs maybe_to_fresh_input simple_wraps without_output_descs This function returns new function returns mutated inputs outputs keep_data_input_mutations set then we assume data-only mutations will left graph we only metadata-mutated inputs outputs fn_input_mutations_to_outputs fn Callable args_descs list AOTInput meta ViewAndMutationMeta keep_data_input_mutations bool - Any simple_wraps fn inner_fn args outs outs_descs = call_and_expect_output_descs fn args assert len meta output_info == len outs The compiled fw will mutated input tensors including metadata-only mutation However keep_data_input_mutations set compiled fw only needs metadata-mutated inputs because data-only input mutations handled directly compiled graph mutated_input_pairs = x InputMutationAOTOutput src i x src enumerate zip args args_descs i meta mutated_inp_runtime_indices mutated_input_pairs mutated_inputs_to_return mutated_inputs_to_return_descs = zip mutated_input_pairs mutated_inputs_to_return mutated_inputs_to_return_descs = mutated_inputs_to_return outs mutated_inputs_to_return_descs outs_descs inner_fn contextmanager disable_autocast ExitStack stack autocast_enabled_devices = torch _C _autocast_supported_devices device_type autocast_enabled_devices hasattr torch device_type stack enter_context torch amp autocast device_type enabled=False yield This function takes fn external aliasing mutation returns new fn no external aliasing mutation needed autograd The main transformations - Return mutated inputs extra outputs - Clone mutated inputs require gradients because autograd will require us pass pre-mutated inputs into autograd grad - Return intermediate bases outputs additional outputs needed appease autograd Function The new function returns The updated outputs A boolean mask len new_fn_outputs can used tell autograd grad which outputs should get tangents we trace backward fn_prepped_for_autograd fn TraceFn args_descs list AOTInput meta ViewAndMutationMeta aot_config AOTConfig - PreppedForAutogradTraceFn simple_wraps fn inner_fn args args_maybe_cloned = maybe_to_fresh_input i t meta i t enumerate args outs outs_descs = call_and_expect_output_descs fn args_maybe_cloned assert isinstance outs tuple list outs = list outs assert len meta output_info == len outs mutated_input_pairs = x InputMutationAOTOutput src i x src enumerate zip args_maybe_cloned args_descs i meta mutated_inp_runtime_indices mutated_input_pairs mutated_inputs_to_return mutated_inputs_to_return_descs = zip mutated_input_pairs mutated_inputs_to_return mutated_inputs_to_return_descs = intermediate_bases = intermediate_bases_descs = o info o_desc zip outs meta output_info outs_descs info output_type == OutputType alias_of_intermediate_save_as_output assert isinstance o torch Tensor f Expected tensor intermediate base got type o intermediate_bases append o _base intermediate_bases_descs append IntermediateBaseAOTOutput o_desc assert meta num_intermediate_bases == len intermediate_bases compiled forward should mutated_inputs user_outs intermediate_bases fw_outs_to_return = mutated_inputs_to_return outs intermediate_bases fw_outs_to_return_descs = mutated_inputs_to_return_descs outs_descs intermediate_bases_descs Also boolean mask specifying which outputs function will used tangents mutated_inputs_grad_mask = meta input_info meta mutated_inp_runtime_indices i mutates_data meta input_info meta mutated_inp_runtime_indices i requires_grad i x enumerate mutated_inputs_to_return Pass any non-aliased outputs tangents since they ll returned outputs fw For outputs aliases intermediates we will have returned output s _base output graph instead which we should send grad output_grad_mask = meta output_info i output_type OutputType non_alias OutputType unsafe_view_alias OutputType custom_function_view Also only tensor outputs should participate backward particular Symint outputs forward graph shouldn t get tangents issubclass meta output_info i raw_type Tensor meta output_info i requires_grad i x enumerate outs intermediate_base_grad_mask = True _ range len intermediate_bases out_grad_mask = mutated_inputs_grad_mask + output_grad_mask + intermediate_base_grad_mask assert len out_grad_mask == len fw_outs_to_return Take care grab sync updated inputs primals_after_cloning inputs we actually mutate primals preserved inputs pre-mutation we pass grad This annoying our joint function needs aware functionalization syncing mutated inputs before calling autograd grad In theory we could make autograd engine do automatically although probably isn t any cleaner aot_config disable_functionalization arg args_maybe_cloned isinstance arg Tensor continue sync_functional_tensor arg fw_outs_to_return out_grad_mask fw_outs_to_return_descs out_grad_mask inner_fn dataclass JointFnHandle post_forward Optional Callable = None Given fn computes joint NOTE fn expects following behavior fn needs tuple outs mask where ` mask ` tells us which outputs meant have tangents we don t know info automatically because we don t actually want blindly compute tangents every output requires grad Specifically outputs alias inputs won t participate backward get tangents fn cannot mutate any inputs require gradient otherwise when we compute autograd grad we will take those input mutations into account way handled we ensure any inputs normally get mutated cloned first create_joint fn Any PreppedForAutogradTraceFn primals_descs Optional list AOTInput = None aot_config AOTConfig - Any JointTraceFn joint_fn_handle = JointFnHandle post_forward NB type inaccurate when primals_descs None simple_wraps fn inner_fn primals list FxValue tangents list FxValue - tuple tuple list FxValue list Optional Tensor tuple list AOTOutput list Optional AOTOutput outs_descs = None primals_descs None outs tangent_mask = fn primals assert pytree tree_any lambda x isinstance x AOTOutput tangent_mask outs tangent_mask outs_descs _ = call_and_expect_output_descs fn primals TODO I think hook can also eliminated now joint_fn_handle joint_fn_handle post_forward joint_fn_handle post_forward primals assert len tangent_mask == len outs outs_to_grad = o needs_tangent o zip tangent_mask outs needs_tangent assert len outs_to_grad == len tangents Get inputs need gradients grad_primals list torch Tensor = inputs_needs_grads = Note we re using primals here being carefully pass any mutated inputs into autograd grad p primals isinstance p Tensor p requires_grad inputs_needs_grads append True assert isinstance p torch Tensor Help mypy understand type grad_primals append p inputs_needs_grads append False Get outputs need gradients needed_outs = needed_tangents = out tangent zip outs_to_grad tangents isinstance out Tensor out requires_grad A bit sketchy fixes e g test_aot_autograd_exhaustive_matmul_cpu_float The issue we sensitive decomps don t accurately maintain their output s _base shape compared eager mode helps mitigate bit The guard_or_true also sketchy unbacked symints involved we re just going assume decomps setup base shape correctly Return out result out shape==tangent shape unknown known true otherwise its known false out view tangent shape tangent should also tensor since corresponds tensor output assert isinstance tangent torch Tensor f Expected tensor tangent got type tangent needed_outs append out guard_or_true sym_eq out shape tangent shape out view tangent shape needed_tangents append tangent setup_stacktrace_preservation_hooks out grad_fn out needed_outs config functionalize_rng_ops PhiloxStateTracker mark_beginning_of_backward backward_out tuple Tensor = Call backwards pass grad_primals functional_tensor_mode = torch utils _python_dispatch _detect_infra_mode torch _C _TorchDispatchModeKey FUNCTIONAL functional_tensor_mode None Side-Effect Tokens We want have independent chains tokens forward backward functional_tensor_mode _tokens used both We memoize result tokens forward functional_tensor_mode _tokens_forward_output them joint graph outputs We clean functional_tensor_mode _tokens before backward prevent reuse forward tokens backward Joint graph tracing allows tokens discovery So all tokens backward will created added graph inputs during tracing functional_tensor_mode _tokens_forward_output = functional_tensor_mode _tokens functional_tensor_mode _tokens = set_partitioner_tag_is_backward fx_traceback preserve_node_meta ExitStack stack backward_pass_autocast = torch _functorch config backward_pass_autocast backward_pass_autocast == same_as_forward Use ambient autocast mode s pass backward_pass_autocast == off stack enter_context disable_autocast Disable autocast then enable anything ` backward_pass_autocast ` stack enter_context disable_autocast assert isinstance backward_pass_autocast list kwargs backward_pass_autocast assert isinstance kwargs dict stack enter_context torch amp autocast kwargs full graph export we always export joint graph where we assume no tangents needed aot_config no_tangents assert len needed_tangents == needed_tangents numel == backward_out = torch autograd grad needed_outs grad_primals allow_unused=True backward_out = torch autograd grad needed_outs grad_primals grad_outputs=needed_tangents allow_unused=True backward_out_iter = iter backward_out final_outs = outs next backward_out_iter i None i inputs_needs_grads primals_descs None final_outs type ignore return-value assert outs_descs None final_outs outs_descs TODO ideally we do know DifferentiableAOTInput quite involved refactor GradAOTOutput desc i None type ignore arg-type i desc zip inputs_needs_grads primals_descs simple_wraps inner_fn inner_fn_with_anomaly primals list FxValue tangents list FxValue - tuple tuple list FxValue list Optional Tensor tuple list AOTOutput list Optional AOTOutput fx_traceback preserve_node_meta warnings catch_warnings warnings filterwarnings ignore Anomaly Detection has been enabled torch autograd detect_anomaly check_nan=False inner_fn primals tangents joint_helper primals tangents inner_fn_with_anomaly primals tangents joint_helper handle = joint_fn_handle type ignore attr-defined joint_helper create_functionalized_rng_ops_wrapper func args args_descs trace_joint=True - Any Functionalization rng ops changes calling convention joint graph It goes primals tangents seed offset primals tangents At runtime we pass current seed offset This hidden user fake_mode_det = detect_fake_mode fake_mode AbstractContextManager Any = nullcontext fake_mode_det None fake_mode = fake_mode_det override_get_rng_state device Union int str torch device = cuda out = PhiloxStateTracker get_state_as_tensor out override_set_rng_state x device Union int str torch device = cuda PhiloxStateTracker set_state_from_tensor x append_rng_offsets outs outs_descs trace_joint outs signature before Tuple fwd_outputs Tuple bwd_outputs outs signature after Tuple fwd_outputs new_fwd_rng_offset Tuple bwd_offset new_bwd_rng_offset outs PhiloxStateTracker get_updated_fwd_offset outs PhiloxStateTracker get_updated_bwd_offset outs_descs PhiloxUpdatedForwardOffsetAOTOutput outs_descs PhiloxUpdatedBackwardOffsetAOTOutput outs signature before Tuple fwd_outputs outs signature after Tuple fwd_outputs new_fwd_rng_offset outs PhiloxStateTracker get_updated_fwd_offset outs_descs PhiloxUpdatedForwardOffsetAOTOutput traced_joint primals tangents fwd_seed fwd_base_offset bwd_seed bwd_base_offset patch torch cuda get_rng_state override_get_rng_state patch torch cuda set_rng_state override_set_rng_state append_rng_offsets func primals tangents traced_forward primals_fwd_seed_fwd_base_offset The signature primals seed offset patch torch cuda get_rng_state override_get_rng_state patch torch cuda set_rng_state override_set_rng_state append_rng_offsets func primals_fwd_seed_fwd_base_offset - trace_joint Get current seed offset setup tracing fwd_seed fwd_base_offset = CUDARngStateHelper get_torch_state_as_tuple fake_mode bwd_seed bwd_base_offset = CUDARngStateHelper get_torch_state_as_tuple fake_mode PhiloxStateTracker record_state fwd_seed fwd_base_offset forward PhiloxStateTracker record_state bwd_seed bwd_base_offset backward traced_joint args fwd_seed fwd_base_offset bwd_seed bwd_base_offset args_descs PhiloxForwardSeedAOTInput PhiloxForwardBaseOffsetAOTInput PhiloxBackwardSeedAOTInput PhiloxBackwardBaseOffsetAOTInput Get current seed offset setup tracing fwd_seed fwd_base_offset = CUDARngStateHelper get_torch_state_as_tuple fake_mode PhiloxStateTracker record_state fwd_seed fwd_base_offset forward traced_forward args fwd_seed fwd_base_offset args_descs PhiloxForwardSeedAOTInput PhiloxForwardBaseOffsetAOTInput contextmanager set_partitioner_tag tag str meta_key = partitioner_tag assert fx_traceback has_preserved_node_meta original_val = fx_traceback current_meta get meta_key None fx_traceback current_meta meta_key = tag try yield finally fx_traceback current_meta meta_key = original_val set_partitioner_tag_is_backward set_partitioner_tag is_backward set_partitioner_tag_must_be_in_backward set_partitioner_tag must_be_in_backward set_partitioner_tag_must_be_in_forward set_partitioner_tag must_be_in_forward dataclass MutationCounters mc_data int mc_storage int mc_inductor_storage_resized int T = TypeVar T sc_visit t fn Callable Tensor T reduce_fn Callable T T T accum_init T - T is_traceable_wrapper_subclass t fn t accum = accum_init visit e is_traceable_wrapper_subclass e nonlocal accum accum = reduce_fn accum fn e e __tensor_flatten__ visit getattr e visit t accum _get_mutation_counter t - int sc_visit t lambda t torch _functionalize_mutation_counter t elem type ignore attr-defined lambda l r max l r - _get_storage_changed_counter t - int sc_visit t lambda t torch _functionalize_storage_changed_counter t elem type ignore attr-defined lambda l r max l r - _get_inductor_storage_resized_counter t - int sc_visit t lambda t torch _functionalize_inductor_storage_resized_counter t elem type ignore attr-defined lambda l r max l r - _get_mutation_counters t - MutationCounters MutationCounters _get_mutation_counter t _get_storage_changed_counter t _get_inductor_storage_resized_counter t apply_in_graph_mutations input_info inpt_old inpt_new f_inpt input_idx mcs Optional MutationCounters = None applied_mcs Optional MutationCounters = None assert input_info mutation_type == MutationType MUTATED_IN_GRAPH See Note set_ Input Mutations AOTAutograd all mutations input must under no_grad so safe put graph Here we re saying input experienced set call inp set_ other then we can effectively have worry about whether its data mutated There cases We mutate inp after set_ call other graph intermediate In case we re really mutating input storage inp we re mutating storage intermdiate value other slamming storage into input tensor So no data mutation necessary We mutate inp after set_ call other graph input In case data mutation will properly handled runtime epilogue during processing other We mutate inp before set_ call This case currently handled input_info mutates_storage_metadata mcs None mcs mc_storage applied_mcs mc_storage type ignore union-attr torch no_grad inpt_old set_ inpt_new Note Ordering resize_ set_ Importantly common usage FSDP we have dummy parameter sees set_ Then resize_ We must put those mutations into graph same order Since running them opposite order will have different behavior We fully ban resize_ followed set_ now although principal we could support input_info mutation_inductor_storage_resize mcs None mcs mc_inductor_storage_resized applied_mcs mc_inductor_storage_resized type ignore union-attr resizing supported subclasses we error earlier happens torch _subclasses functional_tensor FunctionalTensor assert isinstance f_inpt FunctionalTensor old_storage_size = torch _functionalize_get_storage_size type ignore attr-defined f_inpt elem before=True new_storage_size = torch _functionalize_get_storage_size type ignore attr-defined f_inpt elem before=False old_storage_size = new_storage_size assert old_storage_size == new_storage_size == f \ Encosize during tracing input input_idx Old nbytes= old_storage_size new nbytes= new_storage_size We oresizing graph inputs long input either starts ends storage size thee FSDP torch ops inductor resize_storage_bytes_ inpt_old new_storage_size new_storage_size == Even we marked input having data mutation thus needing copy_ We should ignore our input has no storage can happen e g we temporarily resize our input copy data into resize back down zero Optimization copy_ no-op then don t include graph In theory inductor could optimize away however fsdp we end up param copy_ param where param zero-storage-size tensor running op eager mode using aot_eager backend will result segfault So we may well optimize away here inpt_old inpt_new This check needs done after putting resize_ graph since resize_ doesn t actually change FunctionalTensor s inner tensor We found input had data-only mutation Since keep_input_mutations set we need faithfully apply copy_ so compiler will see input mutation graph input_info mutates_data mcs None mcs mc_data = applied_mcs mc_data type ignore union-attr input_info mutations_hidden_from_autograd Hidden autograd = run under no_grad don t bump VC although tensor created inference mode has no VC inpt_old is_inference maybe_preserve_vc = nullcontext maybe_preserve_vc = torch autograd _unsafe_preserve_version_counter inpt_old type ignore assignment torch no_grad maybe_preserve_vc inpt_old copy_ inpt_new input_info mutations_under_no_grad_or_inference_mode Under no_grad = run under no_grad we still bump VC though inference_mode will also bump VC long tensor question created outside inference_mode torch no_grad inpt_old copy_ inpt_new inpt_old copy_ inpt_new This creates final function we want trace using make_fx both aot_dispatch_autograd aot_dispatch_base Preconditions - fn corresponds user s fw function - fn arguments have been flattened duplicate arguments have been handled - In returned function primals arguments includes synthetic bases This function does work functionalizing input function performing copy_ calls end function ` keep_input_mutations ` set The function returned has signature either traced_fn primals List Any trace_joint False traced_fn primals List Any tangents List Any trace_joint True Returns new functionalized function updated arguments call create_functionalized_fn fn args args_descs meta ViewAndMutationMeta aot_config AOTConfig trace_joint bool joint_fn_handle Optional JointFnHandle = None - Any primals_after_forward = None f_args_after_forward = None f_args_mutation_counters_after_forward Optional list MutationCounters = None inputs_mutated_in_graph = info mutation_type == MutationType MUTATED_IN_GRAPH info meta input_info has_input_mutated_in_graph = any inputs_mutated_in_graph simple_wraps fn _functionalized_f_helper args list FxValue - tuple tuple list FxValue list Tensor list Optional AOTOutput maybe_enable_thunkify See Note Disabling Functionalize TLS Above Python Functionalization disable_above = torch _C _ExcludeDispatchKeyGuard torch _C DispatchKeySet torch _C DispatchKey Functionalize disable_above The functionalization code here can potentially trigger traces into graph we d prefer NOT do because we trace them now we will end up FX nodes don t have module stack annotations which makes unflattener unhappy Wrap inputs into functional wrappers f_args = pytree tree_map to_fun args trace_joint has_input_mutated_in_graph joint_fn_handle TODO ivankobzarev Support fw bw mutations subclasses _post_forward primals nonlocal primals_after_forward primals_after_forward = pytree tree_map from_fun primals nonlocal f_args_after_forward f_args_after_forward = f_args nonlocal f_args_mutation_counters_after_forward f_args_mutation_counters_after_forward = MutationCounters - - - inputs_mutated_in_graph i _get_mutation_counters f_arg i f_arg enumerate f_args_after_forward joint_fn_handle post_forward = _post_forward Run joint f_outs f_outs_descs = call_and_expect_output_descs fn f_args trace_joint We support limited amount mutation graph inputs during backward pass This used e g Float which needs update buffers during backward pass Here we perform extra checks primals mutated backward We re doing checks here instead doing them rest input mutation handling because - We need detect inputs mutated backward separately mutations happened during forward because handling different some input mutations forward can only handled fw-only runtime epilogue theory we wanted handle those same types mutations backward we would need bw-only runtime epilogue - We could theory have our analysis pass differentiate mutations fw mutations bw running our analysis first fw-only graph then joint graph This would require extra round tracing though so s more efficient do in-line here assert isinstance args tuple len args == isinstance args list tuple Only look mutations happened forward inputs e g fw buffers saved bw primals_before = args primals_after = pytree tree_map from_fun f_args idx f_inpt before after inpt_info enumerate zip f_args primals_before primals_after meta input_info Store information about mutations joint backward analysis joint_mutates_data = has_data_mutation f_inpt joint_mutates_metadata = has_metadata_mutation f_inpt before check_only_storage_mutation=False Ban metadata mutations fw inputs during bw inpt_info mutates_metadata assert joint_mutates_metadata Found graph input had its metadata mutated backward This supported Ban storage resizing fw inputs during bw inpt_info mutation_inductor_storage_resize assert was_inductor_storage_resized f_inpt Found graph input had storage resizing backward This supported Allow data mutations fw inputs during bw only they do require grad So we can guarantee we can keep mutations graph joint_mutates_data inpt_info mutates_data inpt_info mutates_storage_metadata Not banning here mutations inpt_info requires_grad - we ll check runtime fail only when backward under torch is_grad_enabled create_graph Add node meta copy_ partitioner node should backward graph torch fx traceback preserve_node_meta set_partitioner_tag_must_be_in_backward before after should tensors we re calling copy_ them assert isinstance before torch Tensor isinstance after torch Tensor before copy_ after meta indices_of_inputs_that_requires_grad_with_mutations_in_bw append idx Now we covered mutations forward inputs during backward we also need cover mutations backward-only inputs during backward e g mutation grad_out Today we will just error all cases happening unless someone needs us support tangents_before = args tangents_after = pytree tree_map from_fun f_args f_inpt before after zip f_args tangents_before tangents_after assert has_metadata_mutation f_inpt before check_only_storage_mutation=False Found input backward had metadata mutated during backward pass This supported has_data_mutation f_inpt can_be_in_graph = _check_if_mutation_can_be_in_graph keep_input_mutations=True mutates_data=True mutates_metadata=False mutations_hidden_from_autograd=are_all_mutations_hidden_from_autograd f_inpt mutations_under_no_grad_or_inference_mode=are_all_mutations_under_no_grad_or_inference_mode f_inpt mutates_storage_metadata=False mutation_inductor_storage_resize=was_inductor_storage_resized f_inpt requires_grad=f_inpt requires_grad assert can_be_in_graph backward input had data mutated autograd-aware way This supported Perform input mutation torch fx traceback preserve_node_meta before after should tensors we re calling copy_ them assert isinstance before torch Tensor isinstance after torch Tensor before copy_ after aot_config keep_inference_input_mutations Note This bit annoying There s layering issue here where functionalization needs operate synthetic base inputs before unpacking them into real inputs For keep_input_mutations we support tracing call copy_ directly mutated inputs However we only want support inputs have data-only no metadata mutations because inductor backends generally would prefer see these e g as_strided_ resize_ This makes pretty difficult logic operate synthetic bases In addition there cases where s significantly cheaper perform copy individual unpacked input aliases instead synthetic base Example case where could important f x y x mul_ y mul_ x y = torch ones x y = out It would much better add copy_ calls into graph two tiny slices instead materializing giant updated synthetic base copying into s entire storage For now we pessimistically performing optimization we will materialize updated synthetic base copy back synthetic input base This allows us factor aot autograd much more nicely since only one area code needs worry about synthetic bases Apply graph forward mutations only joint case Note Mutations primals forward AND backward If we have mutations same input forward backward we can fuse them into one copy_ node As case partitioner will put either forward backward This will lead incorrect state after forward before backward We have emit two copy_ nodes marking additional meta each node must forward backward We memorize mutation counter inputs after forward Based after joint graph we check backward also mutated input We emit copy_ only end joint tracing provide invariant joint graph passes our graph functional except only some number copy_ nodes end mcs_applied list MutationCounters = MutationCounters len meta input_info f_args_mutation_counters_after_forward None primals_before = args idx f_inpt before after inpt_info enumerate zip f_args_after_forward type ignore arg-type primals_before type ignore arg-type primals_after_forward type ignore arg-type meta input_info inpt_info mutation_type = MutationType MUTATED_IN_GRAPH continue mcs_after_forward = f_args_mutation_counters_after_forward idx torch fx traceback preserve_node_meta set_partitioner_tag_must_be_in_forward _proxy_tensor_disable_update_tensor_tracker apply_in_graph_mutations inpt_info before after f_inpt idx mcs_after_forward mcs_applied idx mcs_applied idx = mcs_after_forward idx inpt_old f_inpt enumerate zip args f_args trace_joint zip args f_args type ignore arg-type isinstance f_inpt torch Tensor continue assert is_fun f_inpt inpt_new = from_fun f_inpt meta input_info idx mutation_type = MutationType MUTATED_IN_GRAPH continue mcs Optional MutationCounters = None f_args_mutation_counters_after_forward None This could happen subclasses tracing Subclasses support mutations fw bw TBD mcs = _get_mutation_counters f_inpt mcs == mcs_applied idx No mutation backward mutation already applied continue torch fx traceback preserve_node_meta set_partitioner_tag_must_be_in_backward apply_in_graph_mutations meta input_info idx inpt_old inpt_new f_inpt idx mcs mcs_applied idx When output tensor functionalized mutated input we able move mutation graph then we can mutated input directly This prevents duplicating tensors contents flat_outs outs_spec = pytree tree_flatten f_outs flat_outs = from_fun o o flat_outs num_outs = len meta output_info i range num_outs info = meta output_info i info output_type = OutputType is_input continue assert info base_idx None meta input_info info base_idx mutation_type == MutationType MUTATED_IN_GRAPH fw_args = args trace_joint args flat_outs i = fw_args info base_idx pytree tree_unflatten flat_outs outs_spec f_outs_descs pytree tree_map from_fun f_outs f_outs_descs Kinda annoying needed make sure fx graph we trace out has primals tangents its input names which special-cased partitioner TODO tmanlaibaatar revisit we ever need turn non-strict joint graph export joint_helper primals tangents _functionalized_f_helper primals tangents helper = joint_helper trace_joint _functionalized_f_helper config functionalize_rng_ops Setup wrapper functionalization rng ops helper args args_descs = create_functionalized_rng_ops_wrapper helper args args_descs trace_joint helper args args_descs handle_effect_tokens_fn fn args args_descs list AOTInput meta ViewAndMutationMeta trace_joint bool - Any num_tokens = len meta tokens simple_wraps fn inner_fn args See Note Disabling Functionalize TLS Above Python Functionalization disable_above = torch _C _ExcludeDispatchKeyGuard torch _C DispatchKeySet torch _C DispatchKey Functionalize disable_above See Note Side-Effectful Tokens AOTAutograd trace_joint assert isinstance args tuple isinstance args list tuple tokens = args num_tokens assert all token numel == token tokens args = args num_tokens args tokens = args num_tokens assert all token numel == token tokens args = args num_tokens Populate current FunctionalTensorMode tokens per operator See Note FunctionalTensorMode Stateful functional_tensor_mode = torch utils _python_dispatch _detect_infra_mode torch _C _TorchDispatchModeKey FUNCTIONAL assert functional_tensor_mode None f_tokens = pytree tree_map to_fun tokens i k enumerate meta tokens keys functional_tensor_mode _tokens k = f_tokens i Run joint outs outs_descs = call_and_expect_output_descs fn args Return both tokens outputs See Note Side-Effectful Tokens AOTAutograd trace_joint assert len outs == assert len functional_tensor_mode _tokens_forward_output == num_tokens fwd_out_tokens = functional_tensor_mode _tokens_forward_output values bwd_out_tokens = functional_tensor_mode _tokens values f_fwd_out_tokens = from_fun t t fwd_out_tokens f_bwd_out_tokens = from_fun t t bwd_out_tokens f_fwd_out_tokens_descs = ForwardTokenAOTOutput i i range len fwd_out_tokens f_bwd_out_tokens_descs = BackwardTokenAOTOutput i i range len bwd_out_tokens meta num_backward_tokens = len bwd_out_tokens f_fwd_out_tokens outs outs f_bwd_out_tokens f_fwd_out_tokens_descs outs_descs outs_descs f_bwd_out_tokens_descs out_tokens = from_fun t t functional_tensor_mode _tokens values TODO can probably do little more resolution here out_tokens_descs = ForwardTokenAOTOutput i i range len functional_tensor_mode _tokens values out_tokens outs out_tokens_descs outs_descs Additionally pass tokens inputs See Note Side-Effectful Tokens AOTAutograd additional_fwd_token_inputs = torch tensor num_tokens additional_fwd_token_inputs_descs = ForwardTokenAOTInput i i range num_tokens trace_joint args = additional_fwd_token_inputs args args args_descs = type ignore assignment additional_fwd_token_inputs_descs args_descs type ignore misc args_descs args = additional_fwd_token_inputs args args_descs = additional_fwd_token_inputs_descs args_descs inner_fn args args_descs Given function operating Subclass - Subclass returns function operates Tensor - Tensor Also returns - new set arguments pass into function now tensor subclasses have been eliminated - updated ViewAndMutationMeta dense - dense function The other important arguments - flat_fn_maybe_joint when is_joint_structure=True joint fw-bw function when is_joint_structure=False just forward function - fw_only always forward-only function Why do we need We need collect updated ViewAndMutationMeta our new dense - dense functions In particular we need tell partitioner how many dense forward outputs there aot_dispatch_subclass flat_fn_maybe_joint Union JointTraceFn TraceFn args Union list FxValue tuple list FxValue list FxValue args_descs Union list AOTInput tuple list AOTInput list AOTInput is_joint_structure bool meta ViewAndMutationMeta fw_only Callable - SubclassTracingInfo Skip logic we don t need trace through any subclasses req_subclass_dispatch = requires_subclass_dispatch args meta req_subclass_dispatch SubclassTracingInfo plain_tensor_trace_fn=flat_fn_maybe_joint plain_tensor_args=args plain_tensor_args_descs=args_descs maybe_subclass_meta=None TODO add subclass guards later PR What s going here We need compute subclass metadata about outputs joint grad_inputs Annoying we don t know grad input metas until we re middle tracing joint so we set later while we re tracing joint see inner_fn below Another option would run our run_functionalized_fw_and_collect_metadata function directly joint would hurt compile time adding yet another pass through joint subclass_meta = SubclassMeta NB doesn t take descs going NEW flat_args subclasses we don t need do bookkeeping here inner_fn fn args use_trace_joint bool Step wrap tensor inputs into subclasses necessary all_args = wrap_tensor_subclasses_maybe_joint args is_joint_structure=use_trace_joint meta=meta Step call inner function our maybe subclass inputs wrapped_outs wrapped_outs_descs = call_and_expect_output_descs fn all_args use_trace_joint See Note Computing Subclass Metadata about grad_inputs We also stash subclass info our grad_inputs we re tracing joint nonlocal subclass_meta assert isinstance wrapped_outs tuple len wrapped_outs == wrapped_outs wrapped_outs_descs Don t need fw outs since we already have subclass metadata them grad_inputs = wrapped_outs subclass_meta grad_input_metas = create_subclass_meta grad_inputs Add extra symints outputs forward backward graphs ignore nested ints here forward_outs forward_outs_descs = unwrap_tensor_subclasses wrapped_outs wrapped_outs_descs append_symints=True ignore nested ints here backward_outs backward_outs_descs = unwrap_tensor_subclasses wrapped_outs wrapped_outs_descs append_symints=True forward_outs backward_outs forward_outs_descs backward_outs_descs Step Unwrap any subclass outputs back into dense tensors unwrap_tensor_subclasses wrapped_outs wrapped_outs_descs append_symints=True joint_fn primals list FxValue tangents list FxValue - tuple tuple list FxValue list FxValue tuple list AOTOutput list AOTOutput maybe_enable_thunkify inner_fn flat_fn_maybe_joint primals tangents use_trace_joint=True fw_fn primals FxValue - tuple list FxValue list AOTOutput maybe_enable_thunkify inner_fn flat_fn_maybe_joint primals use_trace_joint=False metadata_fn primals FxValue - tuple list FxValue list AOTOutput simple_wraps fw_only inner_fw_only args call_and_expect_output_descs fw_only args inner_fn inner_fw_only primals use_trace_joint=False is_joint_structure Add extra symints size strides input forward graph primals_unwrapped_pair = unwrap_tensor_subclasses args type ignore arg-type args_descs type ignore arg-type append_symints=True We pass append_symints=False here because partitioner will capture add any extra argument tangents_unwrapped_pair = unwrap_tensor_subclasses args type ignore arg-type args_descs type ignore arg-type append_symints=False args_unwrapped = primals_unwrapped_pair tangents_unwrapped_pair args_descs_unwrapped = primals_unwrapped_pair tangents_unwrapped_pair args_unwrapped args_descs_unwrapped = unwrap_tensor_subclasses type ignore assignment args type ignore arg-type args_descs type ignore arg-type append_symints=True remapped_static_indices = remap_unwrapped_subclass_arg_indices args meta static_input_indices is_joint_structure primals_unwrapped = args_unwrapped type ignore assignment primals_unwrapped_descs = args_descs_unwrapped type ignore assignment fn_to_trace = joint_fn type ignore assignment primals_unwrapped = args_unwrapped type ignore assignment primals_unwrapped_descs = args_descs_unwrapped type ignore assignment fn_to_trace = fw_fn type ignore assignment Note Partitioner handling Subclasses Part The way partitioner works we pass single graph containing joint fw bw where graph outputs corresponds fw_outputs + grad_inputs The partitioner accepts arguments num_fwd_outputs assumes first num_fwd_outputs graph outputs correspond outputs forward graph How do tensor subclasses enter picture num_fwd_outputs final graph actually non-trivial compute because can influenced input mutations intermediate bases So we compute inspecting current ViewAndMutationMeta object However original ViewAndMutationMeta we computed created subclass - subclass graph which can have different number outputs than dense - dense graph That s why we created fresh metadata object dense - dense function here plumb back up partitioner See Note Partitioner handling Subclasses Part more info meta_updated = run_functionalized_fw_and_collect_metadata without_output_descs metadata_fn pyrefly ignore bad-argument-type flat_args_descs=primals_unwrapped_descs static_input_indices=remapped_static_indices keep_input_mutations=meta keep_input_mutations is_train=meta is_train pyrefly ignore not-iterable primals_unwrapped subclass_meta fw_metadata = meta_updated SubclassTracingInfo plain_tensor_trace_fn=fn_to_trace plain_tensor_args=args_unwrapped plain_tensor_args_descs=args_descs_unwrapped maybe_subclass_meta=subclass_meta create_functional_call mod params_spec params_len store_orig_mod=False strict_out_tuple=True Redundant dynamo worth having case gets invoked elsewhere https github com pytorch pytorch issues simple_wraps mod functional_call args kwargs flat_params = args params_len isinstance params_spec TreeSpec params = pytree tree_unflatten flat_params params_spec assert isinstance params_spec list params = dict zip params_spec flat_params stateless _reparametrize_module mod params maybe_disable_thunkify isinstance mod torch fx GraphModule kwargs Handle kwargs FX only natively supports positional arguments through placeholders arg_list = list args params_len arg_list extend list kwargs values args = tuple arg_list args = args params_len fx_traceback preserve_node_meta warnings catch_warnings warnings filterwarnings ignore Anomaly Detection has been enabled torch autograd detect_anomaly check_nan=False fake_mode = detect_fake_mode assert fake_mode None fake_mode epoch += out = PropagateUnbackedSymInts mod run args out = mod args params_len kwargs strict_out_tuple isinstance out tuple list raise RuntimeError Graph output must This so we can avoid pytree processing outputs Please change module have tuple outputs use aot_module instead out Note Preserving nn module stack metadata during export non-strict mode This path currently only used non-strict export flow where we cannot rely dynamo preserve nn stack metadata our captured graph Instead we stash original user nn module here rely ` make_fx ` grab stashed module use track nn module stack metadata store_orig_mod hasattr functional_call _orig_mod functional_call _orig_mod = mod type ignore attr-defined functional_call