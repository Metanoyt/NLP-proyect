mypy allow-untyped-defs contextlib functools traceback typing Any Callable Optional TYPE_CHECKING torch torch _subclasses fake_tensor FakeTensor FakeTensorMode torch utils _dtype_abbrs dtype_abbrs torch utils _python_dispatch _get_current_dispatch_mode _get_current_dispatch_mode_stack TorchDispatchMode torch utils _pytree tree_all tree_map torch utils _traceback CapturedTraceback TYPE_CHECKING torch distributed _tools mod_tracker ModTracker __all__ = DebugMode get_active_debug_mode REDISTRIBUTE_FUNC = redistribute_input _DISPATCH_RECORD_HOOKS list Callable = _DISPATCH_LOG_HOOKS list Callable = _stringify_shape shape - str f join str x x shape _stringify_device_mesh mesh - str f DM join str s s mesh shape _stringify_placement placement - str f join str p p placement _stringify_attributes tensor attributes - str pairs = attr attributes hasattr tensor attr pairs attr = getattr tensor attr len pairs == f join f k = v k v pairs items _stringify_dtensor_spec spec - str torch distributed tensor _dtensor_spec DTensorSpec DTensorSpec format_shard_order_str spec placements spec shard_order _tensor_debug_string tensor attributes - str Convert tensor debug string representation isinstance tensor torch Tensor tensor_debug_str = f dtype_abbrs tensor dtype _stringify_shape tensor shape _stringify_attributes tensor attributes isinstance tensor torch distributed tensor DTensor omitted device mesh f dt tensor_debug_str &#124; _stringify_dtensor_spec tensor _spec isinstance tensor FakeTensor f ft tensor_debug_str f t tensor_debug_str raise RuntimeError f Unsupported tensor type type tensor _arg_to_str arg attributes - str torch distributed tensor _dtensor_spec DTensorSpec to_str x isinstance x torch Tensor _tensor_debug_string x attributes isinstance x DTensorSpec _stringify_dtensor_spec x x arg = tree_map to_str arg str arg default_hash_fn t torch Tensor use_scalar bool = False - torch Tensor Observer Computes hash tensor converting float needed making contiguous replacing NaN inf values fixed numbers then computing L norm float complex This used generate deterministic summary value tensor comparison t is_floating_point t is_complex t = t float t = t contiguous Clean tensor handle NaN inf values then compute norm t_clean = torch nan_to_num t nan= posinf= neginf=- dtype = torch complex t is_complex torch float out = t_clean norm p= dtype=dtype use_scalar out item out _get_stack_trace - str torch fx experimental symbolic_shapes uninteresting_files summary = CapturedTraceback extract summary summary = summary - filter out DebugMode frames summary = frame frame summary frame filename uninteresting_files summary = traceback StackSummary from_list summary join summary format _DebugCall Base tracking operator calls DebugMode __init__ call_depth int record Optional dict str Any = None log Optional dict str Any = None stack bool = False call_depth = call_depth stack stack_trace = _get_stack_trace results dispatch hooks record = record log = log stringify_args attributes list str - None To reduce memory consumption method stringifies args kwargs stores result deletes original args kwargs raise NotImplementedError Subclasses must implement stringify_args even no-op render attributes list str - str raise NotImplementedError Subclasses must implement string render __repr__ - str render _OpCall _DebugCall Normal operator call __init__ op args tuple kwargs dict call_depth int stack bool = False super __init__ call_depth stack=stack op = op args = args kwargs = kwargs args_str Optional str = None kwargs_str Optional str = None stringify_args attributes list str - None args_str = join _arg_to_str arg attributes arg args kwargs kwargs_str = + join f k = _arg_to_str v attributes k v kwargs items kwargs_str = del args del kwargs render attributes list str - str args_str None args_str = args_str args_str = join _arg_to_str arg attributes arg args kwargs_str None kwargs_str = kwargs_str kwargs kwargs_str = + join f k = _arg_to_str v attributes k v kwargs items kwargs_str = isinstance op torch _ops OpOverload op_name = op __qualname__ hasattr op __module__ hasattr op __name__ op_name = f op __module__ op __name__ op_name = str op base_str = f op_name args_str kwargs_str log base_str += f log base_str __iter__ BC tuple returns op args kwargs call_depth args_str None yield op args_str kwargs_str call_depth yield op args kwargs call_depth _RedistributeCall _DebugCall Redistribute call DTensor dispatch __init__ arg src_placement dst_placement transform_info_str call_depth stack=False super __init__ call_depth stack=stack arg = arg src_placement = src_placement dst_placement = dst_placement transform_info_str = transform_info_str arg_str Optional str = None stringify_args attributes list str - None arg_str = f _arg_to_str arg attributes del arg render attributes list str - str arg_str None arg_str = arg_str arg_str = f _arg_to_str arg attributes transform_info_str None prioritize over src dst placements placement_str = f trace transform_info_str src_placement_str = _arg_to_str src_placement attributes dst_placement_str = _arg_to_str dst_placement attributes placement_str = f src_placement_str - dst_placement_str f REDISTRIBUTE_FUNC arg_str placement_str __iter__ BC tuple returns op placement info kwargs call_depth arg_str None arg = arg_str arg = arg yield REDISTRIBUTE_FUNC transform_info_str yield arg transform_info_str yield arg src_placement dst_placement yield yield call_depth _NNModuleCall _DebugCall Designates entering nn Module s forward method __init__ module_name str call_depth int stack bool = False super __init__ call_depth stack=stack module_name = module_name stringify_args attributes list str - None pass nothing stringify render attributes list str - str f nn Mod module_name __iter__ yield f nn Mod module_name call_depth _run_hook hook args out = hook args assert out None isinstance out dict out _run_dispatch_hooks call _DebugCall func types args kwargs result - None global _DISPATCH_RECORD_HOOKS _DISPATCH_LOG_HOOKS _DISPATCH_RECORD_HOOKS record = hook _DISPATCH_RECORD_HOOKS hook_out = _run_hook hook func types args kwargs result hook_out None record update hook_out record call record = record _DISPATCH_LOG_HOOKS log = hook _DISPATCH_LOG_HOOKS hook_out = _run_hook hook func types args kwargs result hook_out None log update hook_out log call log = log DebugMode TorchDispatchMode __init__ record_torchfunction=False record_faketensor=False record_realtensor=True record_tensor_attributes=None record_nn_module=False store_original_args=False record_stack_trace=False super __init__ torch distributed tensor noqa F supports_higher_order_operators = True Pushes DebugMode onto torchfunction stack records __torch_function__ calls well WARNING currently incompatible torch compile due dynamo guard failures record_torchfunction = record_torchfunction Records __torch_dispatch__ calls FakeTensors record_faketensor = record_faketensor Records __torch_dispatch__ calls real tensors record_realtensor = record_realtensor Optional list str tensor attributes annotated string dump record_tensor_attributes = record_tensor_attributes Uses ModTracker record nn Module entrances _NNModuleCall entries This flag currently has no effect torch compiled-regions record_nn_module = record_nn_module module_tracker Optional ModTracker = None record_nn_module module_tracker_setup If True stores call args kwargs logs without immediately stringifying Defaults False memory concerns store_original_args = store_original_args For stack trace recording stores log call stack traces stack_trace record_stack_trace = record_stack_trace operators = call_depth = Without override running torch compile under DebugMode will force torch compile always use “ eager ” backend With DebugMode will take effect torch compile classmethod ignore_compile_internals cls True _record_call call store_original_args call stringify_args record_tensor_attributes operators append call __torch_function__ func types args= kwargs=None kwargs None kwargs = _record_call _OpCall func args kwargs call_depth stack=self record_stack_trace try call_depth += func args kwargs finally call_depth -= __torch_dispatch__ func types args= kwargs=None kwargs None kwargs = Record operation its call depth call = None torch distributed tensor DTensor types call = _OpCall func args kwargs call_depth stack=self record_stack_trace _record_call call NotImplemented FakeTensor types isinstance _get_current_dispatch_mode FakeTensorMode record_faketensor func = torch ops prim device default call = _OpCall func args kwargs call_depth + stack=self record_stack_trace _record_call call len types == record_realtensor call = _OpCall func args kwargs call_depth + stack=self record_stack_trace _record_call call result = func args kwargs call _run_dispatch_hooks call func types args kwargs result result __enter__ operators = call_depth = record_torchfunction torch _C _push_on_torch_function_stack super __enter__ record_nn_module module_tracker __enter__ type ignore attribute union-attr pyrefly ignore bad-override __exit__ args super __exit__ args record_nn_module module_tracker __exit__ type ignore attribute union-attr record_torchfunction torch _C _pop_torch_function_stack module_tracker_setup torch distributed _tools mod_tracker ModTracker module_tracker = ModTracker module pre-fw hook record module call pre_fw_hook module input fqn = module_tracker _get_mod_name module type ignore attribute union-attr operators append _NNModuleCall fqn call_depth + call_depth += module post-fw hook decrement call depth post_fw_hook module input output call_depth -= module_tracker register_user_hooks pre_fw_hook post_fw_hook contextlib contextmanager record_redistribute_calls arg src_placement dst_placement transform_info_str Optional str = None try _record_call _RedistributeCall arg src_placement=src_placement dst_placement=dst_placement transform_info_str=transform_info_str call_depth=self call_depth + stack=self record_stack_trace call_depth += yield finally call_depth -= debug_string - str torch _C DisableTorchFunction result = result += \n join + op call_depth + op render record_tensor_attributes op operators result staticmethod contextlib contextmanager dispatch_hooks record_hook Optional Callable = None log_hook Optional Callable = None Allows installing post-hooks arguments intercepted __torch_dispatch__ calls hook signatures expected func types args kwargs result i e __torch_dispatch__ args + value Logging hook outputs stored call log annotate calls debug_string while recording hook outputs just stored call record For now hooks expected dictionaries global _DISPATCH_RECORD_HOOKS _DISPATCH_LOG_HOOKS record_hook _DISPATCH_RECORD_HOOKS append record_hook log_hook _DISPATCH_LOG_HOOKS append log_hook try yield finally record_hook _DISPATCH_RECORD_HOOKS pop log_hook _DISPATCH_LOG_HOOKS pop staticmethod contextlib contextmanager record_outputs Hook storing cloned output tensors record output dispatch_hook func types args kwargs result torch _C _DisablePythonDispatcher out = tree_map lambda x x clone isinstance x torch Tensor x result output out DebugMode dispatch_hooks record_hook=dispatch_hook yield staticmethod contextlib contextmanager log_tensor_hashes hash_fn Optional Callable = None hash_inputs bool = False Installs hook tensor hash logging hash_fn optional function custom hashing hash_inputs True also hashes tensors args kwargs storing them input_hash NOTE currently post-hook so e g inplace ops will log output hashes hash_fn None hash_fn = functools partial default_hash_fn use_scalar=True _tree_hash obj torch _C _DisablePythonDispatcher tree_map lambda x hash_fn x isinstance x torch Tensor None obj _dispatch_hash_hook func types args kwargs result empty str func profiler str func None out = out hash = _tree_hash result hash_inputs out input_hash = _tree_hash args kwargs tree_all lambda x x None out values None out DebugMode dispatch_hooks log_hook=_dispatch_hash_hook yield get_active_debug_mode - Optional DebugMode debug_mode = None mode _get_current_dispatch_mode_stack isinstance mode DebugMode debug_mode = mode break debug_mode