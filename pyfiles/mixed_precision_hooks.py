dataclasses dataclass typing Any no_type_check torch torch distributed dist torch autograd Variable torch distributed utils _free_storage dataclass _AllreduceUpcastHookState State manage DDP mixed precision backward gradient communication This contains weakref DDP module access reducer process group stream run parameter gradient upcasts ddp_weakref Any upcast_stream torch Stream wait_for_stream_enqueued bool = False no_type_check _reducer_allreduce_and_upcast_hook hook_state _AllreduceUpcastHookState bucket dist GradBucket - torch futures Future torch Tensor Perform allreduce precision ` ` reduce_dtype ` ` upcast prepare optimizer Performs allreduce reduced precision given DDP s mixed precision reduce_dtype upcasts parameters gradients fp preparation run optimizer ddp_weakref = hook_state ddp_weakref reducer process_group = ddp_weakref reducer ddp_weakref process_group Cast bucket different than param_dtype ddp_weakref mixed_precision param_dtype = ddp_weakref mixed_precision reduce_dtype Cast bucket tensor reduce_dtype bucket set_buffer bucket buffer ddp_weakref mixed_precision reduce_dtype fut = reducer _run_allreduce_hook bucket ret_fut = torch futures Future stream = hook_state upcast_stream stream fut wait bucket buffer div_ process_group size ret_fut set_result bucket buffer Upcast parameters gradients so optimizer step can run fp p bucket parameters p data = p _fp_param free storage mp param will allocated again next forward pass _free_storage p _mp_param p grad data = p grad p data dtype enqueue callback wait stream end backward wait_for_stream_cb torch accelerator current_stream wait_stream stream Remove post-backward hooks since they re-installed next iteration similar FSDP Parameters don t require grad still needed casted since they may participate computation However they would recast hook above they don t have grad hook installed so cast them back here _ p ddp_weakref module named_parameters hasattr p _ddp_mp_hook_state p _ddp_mp_hook_state remove delattr p _ddp_mp_hook_state p requires_grad hasattr p _ddp_ignored p data = p _fp_param reset next backward pass hook_state wait_for_stream_enqueued = False hook_state wait_for_stream_enqueued Variable _execution_engine queue_callback wait_for_stream_cb mark callback enqueued hook_state wait_for_stream_enqueued = True ret_fut