Owner s oncall distributed copy sys torch torch nn nn torch distributed _shard shard_module torch distributed _shard sharded_tensor ShardedTensor torch distributed _shard sharder Sharder torch distributed _shard sharding_plan ShardingPlan torch distributed _shard sharding_spec ChunkShardingSpec torch testing _internal common_distributed requires_nccl skip_if_lt_x_gpu torch testing _internal common_utils run_tests TEST_WITH_DEV_DBG_ASAN torch testing _internal distributed _shard sharded_tensor ShardedTensorTestBase TEST_GPU_NUM with_comms TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit simple collection embedding bag implementation CustomEmbeddingBagCollection nn Module __init__ num_bags num_embeddings_per_bag num_dims super __init__ num_bags = num_bags embedding_bags nn ModuleDict = nn ModuleDict i range num_bags embedding_bags f embedding_bag_ i = nn EmbeddingBag num_embeddings_per_bag num_dims mode= sum forward inputs outputs = bag embedding_bags values outputs append bag inputs torch cat outputs simple sharded version EBC CustomShardedEBC nn Module __init__ ebc split_idx specs super __init__ split_idx = split_idx row_spec col_spec = specs create embedding bags base spec embedding_bags nn ModuleDict = nn ModuleDict assert split_idx ebc num_bags i range ebc num_bags bag_key = f embedding_bag_ i i split_idx shard_module ebc plan=ShardingPlan plan= f embedding_bags bag_key weight row_spec shard_module ebc plan=ShardingPlan plan= f embedding_bags bag_key weight col_spec embedding_bags bag_key = ebc embedding_bags bag_key CustomSharder Sharder __init__ devices split_sharding_idx devices = devices split_sharding_idx = split_sharding_idx rowwise_spec = ChunkShardingSpec dim= placements=devices colwise_spec = ChunkShardingSpec dim= placements=devices shard ebc nn Module - nn Module isinstance ebc CustomEmbeddingBagCollection raise RuntimeError The custom sharder only supports CustomEmbeddingBagCollection CustomShardedEBC ebc split_sharding_idx rowwise_spec colwise_spec TestCustomSharder ShardedTensorTestBase with_comms init_rpc=False skip_if_lt_x_gpu TEST_GPU_NUM requires_nccl test_custom_sharder MyModule nn Module __init__ - None super __init__ ebc = CustomEmbeddingBagCollection forward inputs ebc inputs custom_sharder = CustomSharder devices= f rank i cuda i i range TEST_GPU_NUM split_sharding_idx=TEST_GPU_NUM sharding_plan = ShardingPlan plan= ebc custom_sharder local_model = MyModule cuda rank sharded_model = copy deepcopy local_model shard module provided sharding plan shard_module sharded_model sharding_plan check make sure module already been sharded emb_bags = sharded_model ebc embedding_bags assertTrue isinstance emb_bags embedding_bag_ weight ShardedTensor assertTrue isinstance emb_bags embedding_bag_ weight ShardedTensor assertEqual emb_bags embedding_bag_ weight sharding_spec custom_sharder rowwise_spec assertEqual emb_bags embedding_bag_ weight sharding_spec custom_sharder colwise_spec make sure we can run sharded computation compare outputs local model version input = torch arange reshape cuda rank local_output = local_model input sharded_output = sharded_model input assertEqual local_output sharded_output with_comms init_rpc=False skip_if_lt_x_gpu TEST_GPU_NUM requires_nccl test_custom_sharder_errors custom_sharder = CustomSharder devices= f rank i cuda i i range TEST_GPU_NUM split_sharding_idx=TEST_GPU_NUM sharding_plan = ShardingPlan plan= custom_sharder sharded_model = CustomEmbeddingBagCollection cuda rank assertRaisesRegex KeyError path must empty custom sharder shard module provided sharding plan shard_module sharded_model sharding_plan test conflicted sharding plan spec = ChunkShardingSpec dim= placements= rank cuda rank cuda sharding_plan = ShardingPlan plan= embedding_bags embedding_bag_ weight spec embedding_bags custom_sharder assertRaisesRegex RuntimeError should conflict submodule tree shard module provided sharding plan shard_module sharded_model sharding_plan __name__ == __main__ run_tests