mypy allow-untyped-defs argparse os enum Enum typing cast Optional Union torch torch distributed dist torch distributed _shard _utils narrow_tensor_by_index torch distributed checkpoint FileSystemReader FileSystemWriter torch distributed checkpoint _nested_dict flatten_state_dict torch distributed checkpoint default_planner _EmptyStateDictLoadPlanner DefaultLoadPlanner torch distributed checkpoint metadata Metadata STATE_DICT_TYPE STORAGE_TYPES TensorProperties TensorStorageMetadata torch distributed checkpoint planner LoadItemType LoadPlan LoadPlanner torch distributed checkpoint planner_helpers _create_chunk_list torch distributed checkpoint state_dict_loader _load_state_dict torch distributed checkpoint state_dict_saver _save_state_dict torch distributed checkpoint storage StorageReader torch futures Future __all__ = dcp_to_torch_save torch_save_to_dcp BroadcastingTorchSaveReader DynamicMetaLoadPlanner BroadcastingTorchSaveReader StorageReader StorageReader reading Torch Save file This reader will read entire checkpoint coordinator rank then broadcast shard each tensor all ranks N B Intended used DynamicMetaLoadPlanner warning Current implementation only supports loading Tensors xdoctest +SKIP undefined vars sd = mode model dcp load sd storage_reader=BroadcastingTorchSaveReader planner=DynamicMetaLoadPlanner checkpoint_id= path_to_model pt __init__ checkpoint_id Optional Union str os PathLike = None coordinator_rank int = - None checkpoint_id = checkpoint_id coordinator_rank = coordinator_rank pyrefly ignore bad-override read_metadata - Metadata Extends default StorageReader support building metadata file Metadata built planner set_up_planner since we actually reading metadata disk Metadata state_dict_metadata= read_data plan LoadPlan planner LoadPlanner - Future None Reads torch save data coordinator rank broadcast afterwards incurrs communication cost avoids having load entire checkpoint each rank hopefully preventing OOM issues planner = cast DefaultLoadPlanner planner data read coordinator rank broadcast afterwards incurs communication cost avoids having load entire checkpoint each rank hopefully preventing OOM issues TODO read each host instead only coordinator is_coordinator checkpoint_id None raise AssertionError checkpoint_id must set before reading data torch_state_dict = torch load checkpoint_id map_location= cpu weights_only=False planner flatten_state_dict torch_state_dict _ = flatten_state_dict torch_state_dict torch_state_dict = None req plan items req type == LoadItemType BYTE_IO raise RuntimeError f Non-tensor value identified req storage_index fqn f At time type __name__ only supports loading Tensors Broadcast tensor coordinator rank is_coordinator pg_device = dist distributed_c d _get_pg_default_device pyrefly ignore unsupported-operation tensor = torch_state_dict req storage_index fqn pg_device tensor = torch empty_like planner state_dict req storage_index fqn dist broadcast tensor src=self coordinator_rank async_op=False tensor = narrow_tensor_by_index tensor req storage_offsets req lengths target_tensor = planner resolve_tensor req detach target_tensor size == tensor size raise AssertionError f req req storage_index mismatch sizes f target_tensor size vs tensor size target_tensor copy_ tensor planner commit_tensor req target_tensor fut Future = Future fut set_result None fut pyrefly ignore bad-override set_up_storage_reader metadata Metadata is_coordinator bool - None Implementation StorageReader method is_coordinator = is_coordinator is_coordinator dist get_rank == coordinator_rank raise AssertionError f Coordinator rank mismatch expected coordinator_rank f got dist get_rank checkpoint_id None raise AssertionError checkpoint_id must set before setting up storage reader prepare_local_plan plan LoadPlan - LoadPlan Implementation StorageReader method plan prepare_global_plan global_plan list LoadPlan - list LoadPlan Implementation StorageReader method global_plan reset checkpoint_id Union str os PathLike None = None - None Implementation StorageReader method checkpoint_id = checkpoint_id classmethod validate_checkpoint_id cls checkpoint_id Union str os PathLike - bool Implementation StorageReader method os path isfile checkpoint_id DynamicMetaLoadPlanner DefaultLoadPlanner Extension DefaultLoadPlanner which creates new Metadata object based passed state dict avoiding need read metadata disk This useful when reading formats which don t have metadata file like Torch Save files N B Intended used BroadcastingTorchSaveReader warning Current implementation only supports loading Tensors xdoctest +SKIP undefined vars sd = mode model dcp load sd storage_reader=BroadcastingTorchSaveReader planner=DynamicMetaLoadPlanner checkpoint_id= path_to_model pt set_up_planner state_dict STATE_DICT_TYPE metadata Optional Metadata = None is_coordinator bool = False - None Setups planner extnding default behavior creating Metadata object state dict super set_up_planner state_dict metadata is_coordinator state_dict_metadata dict str STORAGE_TYPES = key tensor state_dict items torch is_tensor tensor raise RuntimeError f Non-tensor value identified key f At time type __name__ only supports loading Tensors state_dict_metadata key = TensorStorageMetadata TensorProperties dtype=tensor dtype tensor size _create_chunk_list tensor metadata = Metadata state_dict_metadata=state_dict_metadata dcp_to_torch_save dcp_checkpoint_dir Union str os PathLike torch_save_path Union str os PathLike Given directory containing DCP checkpoint function will convert into Torch save file Args dcp_checkpoint_dir Directory containing DCP checkpoint torch_save_path Filename store converted Torch save file warning To avoid OOM s recommended only run function single rank sd STATE_DICT_TYPE = _load_state_dict sd storage_reader=FileSystemReader dcp_checkpoint_dir planner=_EmptyStateDictLoadPlanner no_dist=True torch save sd torch_save_path torch_save_to_dcp torch_save_path Union str os PathLike dcp_checkpoint_dir Union str os PathLike Given location torch save file converts into DCP checkpoint Args torch_save_path Filename Torch save file dcp_checkpoint_dir Directory store DCP checkpoint warning To avoid OOM s recommended only run function single rank state_dict = torch load torch_save_path weights_only=False we don t need stateful behavior here because expectation anything loaded torch load would contain stateful objects _save_state_dict state_dict storage_writer=FileSystemWriter dcp_checkpoint_dir no_dist=True __name__ == __main__ FormatMode Enum TORCH_TO_DCP = torch_to_dcp DCP_TO_TORCH = dcp_to_torch Parse command-line arguments parser = argparse ArgumentParser parser add_argument mode type=str help= Conversion mode choices= m value m FormatMode default=FormatMode TORCH_TO_DCP parser add_argument src type=str help= Path source model parser add_argument dst type=str help= Path destination model args = parser parse_args print f Converting checkpoint args src args dst using method args mode checkpoint_missing_warning = f No checkpoint found args src Skipping conversion args mode == FormatMode TORCH_TO_DCP value os path isfile args src torch_save_to_dcp args src args dst print checkpoint_missing_warning args mode == FormatMode DCP_TO_TORCH value os path isdir args src dcp_to_torch_save args src args dst print checkpoint_missing_warning raise ValueError f Unknown conversion mode args mode