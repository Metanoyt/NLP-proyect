mypy allow-untyped-defs contextlib functools logging os warnings collections abc Callable Generator Iterator Sequence enum auto Enum itertools accumulate chain typing Any cast NamedTuple no_type_check Optional Union torch torch distributed dist torch nn nn torch nn functional F torch Tensor torch distributed fsdp _common_utils _FSDPDeviceHandle _named_parameters_with_duplicates _no_dispatch_record_stream _set_fsdp_flattened HandleTrainingState torch distributed utils _alloc_storage _data_ptr_allocated _free_storage _p_assert torch nn parameter _ParameterMeta type ignore attr-defined torch testing _internal distributed fake_pg FakeProcessGroup _fsdp_extensions _ext_post_unflatten_transform _ext_pre_flatten_transform FSDPExtensions __all__ = FlatParameter FlatParamHandle FlatParamShardMetadata ParamInfo SharedParamInfo HandleShardingStrategy logger = logging getLogger __name__ Note Fully Sharded Module We define fully sharded module original ` ` nn Module ` ` owns ` ` FlatParamHandle ` ` It single module logically responsible single unshard reshard pair handle s ` ` FlatParameter ` ` given forward backward pass The fully sharded module should passed ` ` FlatParamHandle ` ` constructor For wrapper code path - The ` ` FullyShardedDataParallel ` ` module wrapping fully sharded module runs unshard reshard behalf fully sharded module overriding ` ` nn Module forward ` ` - The fully sharded module exactly module passed ` ` FullyShardedDataParallel ` ` constructor s ` ` module ` ` argument For non-wrapper code path - Hooks registered fully sharded module run unshard reshard - The fully sharded module may either direct argument ` ` fully_shard ` ` submodule chosen provided wrapping policy Environment variable toggling whether use unsafe ` setattr ` view setting ` _use_sharded_views ` ` _use_unsharded_views ` We should use safe default since respects method overrides special cases such high CPU overhead intentionally bypassing checks overrides we may use unsafe _FSDP_USE_UNSAFE_SETATTR = FSDP_USE_UNSAFE_SETATTR Environment variable toggling whether check parameter gradient writeback case their storages change after FSDP initialization We should check default since prevents silent correctness errors since such changes atypical we may want skip check save CPU overhead especially since check happens pre-forward pre-backward each iteration _FSDP_SKIP_WRITEBACK_CHECK = FSDP_SKIP_WRITEBACK_CHECK Env var toggling whether when model eval mode should we run fp reduced precision _FSDP_USE_FULL_PREC_IN_EVAL = FSDP_USE_FULL_PREC_IN_EVAL Some value set padding tensors debuggability _FLAT_PARAM_PADDING_VALUE = Environment variables disabling all-gather reduce-scatter communication ops ablation studies Note without these communication ops training won t converge you probably need disable correctness checks your model _FSDP_USE_FAKE_ALL_GATHER = FSDP_USE_FAKE_ALL_GATHER _FSDP_USE_FAKE_REDUCE = FSDP_USE_FAKE_REDUCE TODO Define now avoid circular imports See we can remove HandleShardingStrategy Enum FULL_SHARD = auto SHARD_GRAD_OP = auto NO_SHARD = auto HYBRID_SHARD = auto _HYBRID_SHARD_ZERO = auto RESHARD_AFTER_FORWARD_HANDLE_STRATEGIES = HandleShardingStrategy FULL_SHARD HandleShardingStrategy HYBRID_SHARD NO_RESHARD_AFTER_FORWARD_HANDLE_STRATEGIES = HandleShardingStrategy SHARD_GRAD_OP HandleShardingStrategy _HYBRID_SHARD_ZERO ParamInfo NamedTuple Information original parameter param_name str unprefixed module nn Module module_name str SharedParamInfo NamedTuple Additional information shared parameter For each shared parameter we designate one module its parameter variable primary owner determined first one encountered parameter walk These prefixed prim The primary module parameter do have their own ` SharedParamInfo ` instance param_name str unprefixed module nn Module module_name str prim_param_name str unprefixed prim_module nn Module prim_module_name str _ShardParamInfo NamedTuple Shard-related information original parameter in_shard bool Use index into sharded flat parameter e g ` flat_param offset_in_shard offset_in_shard + numel_in_shard ` offset_in_shard Optional int numel_in_shard Optional int Use get part parameter local shard flattened version unsharded parameter e g either ` param flatten intra_param_start_idx intra_param_end_idx + ` ` param as_strided param numel intra_param_start_idx intra_param_end_idx + ` intra_param_start_idx Optional int intra_param_end_idx Optional int inclusive FlatParamShardMetadata NamedTuple This holds metadata specific rank s shard flat parameter Attributes param_names Tuple str Prefixed parameter names rank s shard parameters see ` FlatParameter ` param_shapes Tuple torch Size Parameter shapes rank s shard parameters see ` FlatParameter ` param_strides Tuple torch Size Parameter strides rank s shard parameters see ` FlatParameter ` param_contiguities Tuple bool Parameter ` contiguous ` call results rank s shard parameters see ` FlatParameter ` param_numels Tuple int Parameter numels rank s shard parameters see ` FlatParameter ` param_offsets Tuple Tuple int int start end offsets units numels giving rank s part each flattened original parameter param_names tuple str param_shapes tuple torch Size param_strides tuple tuple int param_contiguities tuple bool param_numels tuple int param_offsets tuple tuple int int _FlatParameterMeta _ParameterMeta Make ` isinstance t FlatParameter ` True custom tensor instances have _is_flat_param flag BC __instancecheck__ instance NB do NOT test super implementation isinstance instance torch Tensor getattr instance _is_flat_param False FlatParameter nn Parameter metaclass=_FlatParameterMeta This flat parameter used ` FullyShardedDataParallel ` It comprised one more original parameters which flattened concatenated construct flat parameter Under current design parameter logically represents both unsharded sharded flat parameter its data changes storages dynamically - In ` FullyShardedDataParallel ` constructor parameter initialized unsharded then sharded in-place - At runtime parameter lazily re -initialized The sharded parameter data saved ` ` _local_shard ` ` new ` ` Tensor ` ` ` ` _full_param_padded ` ` created which all-gather destination owns unsharded parameter storage thereafter See meth ` FlatParamHandle init_flat_param_attributes ` - Throughout runtime parameter data changes storages needed e g sharded flat parameter low precision sharded flat parameter unsharded flat parameter NOTE Since ` ` use_orig_params=True ` ` supports intra- ` ` FlatParameter ` ` padding we have two versions per-parameter numels one includes padding ` ` _numels_with_padding ` ` one does ` ` _numels ` ` The former may have length longer than other data structures while latter has same length number actual original parameters like other per-parameter data structures NOTE This real instead you will always get Parameter back out you try create one these This similar trick we implemented Parameter get work subclasses primarily so FlatParameter supports combination FakeTensor Attributes _unpadded_unsharded_size torch Size Unsharded flat parameter s size without right-hand-side padding divisibility world size For ` ` use_orig_params=True ` ` includes alignment padding _padded_unsharded_size torch Size Unsharded flat parameter s size right-hand-side padding divisibility world size For ` ` use_orig_params=True ` ` includes alignment padding This only set sharded strategies since they require padding all-gather _sharded_size torch Size Sharded flat parameter s size padding This also set ` ` NO_SHARD ` ` which case same unsharded sizes We omit padded because there no analogous unpadded one _num_params int Number original parameters flattened into flat parameter This length per-parameter data structures _param_infos Tuple ParamInfo Each parameter s parameter info entry see ` ParamInfo ` details _shapes Tuple torch Size Each parameter s original shape _strides Tuple torch Size Each parameter s original stride _contiguities Tuple bool Each parameter s ` ` contiguous ` ` call result _fqns Tuple str Each parameter s fully-qualified name FQN prefixed ` ` _fully_sharded_module ` ` The names guaranteed unique subtree rooted module _param_extensions Tuple Optional Any Each parameter s extension i e some per-parameter state used customize pre-flatten post-unflatten behavior ` ` None ` ` This experimental users should depend its existence future _numels_with_padding Tuple int Each parameter s numel including entries padding This used construct views into flat parameter via ` ` torch split ` ` This may have length longer than ` ` _num_params ` ` _numels Tuple int Each parameter s numel excluding entries padding This has length equal ` ` _num_params ` ` _shard_param_infos Tuple _ShardParamInfo Each parameter s shard parameter info see ` _ShardParamInfo ` details _shared_param_infos Tuple SharedParamInfo Shared parameter info entries see ` SharedParamInfo ` details _modules set nn Module Modules contain some original parameter flattened into flat parameter _shard_numel_padded int Numel padded rank s sharded flat parameter _local_shard Tensor Sharded flat parameter padding using sharded strategy If using ` ` NO_SHARD ` ` then unpadded unsharded flat parameter there no notion sharded flat parameter padded unsharded flat parameter _full_param_padded Tensor Unsharded flat parameter padding This defined ` ` NO_SHARD ` ` When using mixed precision parameters has low precision _full_prec_full_param_padded Tensor Full precision unsharded flat parameter padding This used unsharding outside computation when using mixed precision parameters This never defined ` ` NO_SHARD ` ` _post_backward_hook_handle RemovableHandle Flat parameter s post-backward hook handle Compile only _post_backward_hook_state Tuple AccumulateGrad RemovableHandle Flat parameter s ` AccumulateGrad ` object post-backward hook handle Eager only _mp_shard Tensor Low precision sharded flat parameter padding This only defined when parameter mixed precision enabled For ` ` NO_SHARD ` ` used computation _cpu_grad Tensor Sharded gradient padding stored CPU This only defined when offloading parameters enabled _saved_grad_shard Tensor Sharded gradient padding previous iterations gradient accumulation without meth ` no_sync ` _params Optional List nn Parameter If ` ` use_orig_params=True ` ` then each original parameter variable otherwise ` ` None ` ` This does include any padding tensors _shared_params Optional List nn Parameter The original shared parameter variables ` ` use_orig_params=True ` ` ` ` None ` ` otherwise _tensors Optional List Optional Tensor This saves ` ` Tensor ` ` views created forward tracked autograd when ` ` use_orig_params=True ` ` ` ` None ` ` otherwise This preserve those ` ` Tensor ` ` variables backward ensure ` ` FlatParameter ` ` s ` ` AccumulateGrad ` ` object does change which case post-backward hook does run This relevant cases like reentrant activation checkpointing _is_grad_none_mask Optional List bool If ` ` use_orig_params=True ` ` mask over original parameters gradients indicating logically ` ` None ` ` otherwise ` ` None ` ` This does include entries padding This mask needed because only some parameters may have ` ` None ` ` gradient which case flat gradient must non- ` ` None ` ` must use zeros approximate those original ` ` None ` ` gradients This mask informs FSDP set original parameter gradients ` ` None ` ` instead zeros needed _unpadded_unsharded_size torch Size _padded_unsharded_size torch Size _sharded_size torch Size _num_params int _param_infos tuple ParamInfo _shapes tuple torch Size _strides tuple tuple int _contiguities tuple bool _fqns tuple str _param_extensions tuple Optional Any _numels_with_padding tuple int _numels tuple int _shard_param_infos tuple _ShardParamInfo _shared_param_infos tuple SharedParamInfo _modules set nn Module _shard_numel_padded int _local_shard Tensor _full_param_padded Tensor _full_prec_full_param_padded Tensor Eager only _post_backward_hook_state tuple Any Any Compile only _post_backward_hook_handle Any _mp_shard Tensor _cpu_grad Tensor _saved_grad_shard Tensor _params Optional list nn Parameter _shared_params Optional list nn Parameter _tensors Optional list Optional Tensor _is_grad_none_mask Optional list bool _is_padding_mask list bool __new__ cls data=None requires_grad=True cls FlatParameter raise AssertionError subclasses FlatParameter supported r = nn Parameter __new__ nn Parameter data requires_grad type ignore call-arg r _is_flat_param = True type ignore attr-defined r NB This regular method because FlatParameters actually instances see __new__ above So you must indirectly call directly through classmethod classmethod _init_metadata cls param_infos list ParamInfo numels list int shapes list torch Size strides list tuple int contiguities list bool fqns list str shared_param_infos list SharedParamInfo param_extensions list Optional Any params Optional list nn Parameter shared_params Optional list nn Parameter is_padding_mask list bool - None Initialize attributes holding metadata about original parameters comprising flat parameter We expose method separate constructor keep constructor only responsible flat parameter s tensor data This method should only called once per model while constructor may called multiple times e g when reloading checkpoint which case only tensor data needs passed constructor Since meth ` load_state_dict ` implemented via meth ` copy_ ` metadata correctly assumed unchanged Args See Attributes docstring len param_infos = len shapes raise AssertionError f Expected param_infos length len param_infos match shapes length len shapes len param_infos = len strides raise AssertionError f Expected param_infos length len param_infos match strides length len strides len param_infos = len contiguities raise AssertionError f Expected param_infos length len param_infos match contiguities length len contiguities len param_infos = len fqns raise AssertionError f Expected param_infos length len param_infos match fqns length len fqns len param_infos = len param_extensions raise AssertionError f Expected param_infos length len param_infos match param_extensions length len param_extensions _num_params = len param_infos _param_infos = param_infos _shapes = shapes _strides = strides _contiguities = contiguities _fqns = fqns _param_extensions = param_extensions _is_padding_mask = is_padding_mask numels_without_padding list int = numel is_padding zip numels is_padding_mask is_padding numels_without_padding append numel _numels = tuple numels_without_padding _numels_with_padding = tuple numels len _numels = _num_params raise AssertionError f Expected _numels length len _numels equal _num_params _num_params _shared_param_infos = tuple shared_param_infos _modules = pi module pi _param_infos union spi module spi _shared_param_infos params None = shared_params None raise AssertionError Expected params shared_params both None both None params None shared_params None len shared_params = len shared_param_infos raise AssertionError f Expected shared_params None have length len shared_param_infos got shared_params _params = param is_padding zip params is_padding_mask is_padding _params append param shared_params None _shared_params = shared_params _shared_params = Mark original parameters avoid flattening them into another ` FlatParameter ` during recursive construction param chain _params _shared_params _set_fsdp_flattened param _is_grad_none_mask = False _ range _num_params _tensors = None _ range _num_params _params = None _shared_params = None _is_grad_none_mask = None _tensors = None _unpadded_unsharded_size = size _set_fsdp_flattened Tracks whether ` FlatParameter ` s post-backward hook has been called modify behavior post-backward callback _post_backward_called = False FlatParamHandle A handle manages flat parameter ` FlatParameter ` This includes sharding view management Args params Sequence nn Parameter The parameters flatten into flat parameter fully_sharded_module nn Module See Note Fully Sharded Module device torch device The compute communication device which should non-CPU device We refer compute device sharding_strategy ShardingStrategy Sharding strategy apply handle s ` ` FlatParameter ` ` offload_params bool Whether offload handle s ` ` FlatParameter ` ` CPU mp_param_dtype Optional torch dtype Parameter mixed precision setting passed FSDP constructor mp_reduce_dtype Optional torch dtype Gradient reduction mixed precision setting passed FSDP constructor keep_low_precision_grads bool Whether keep gradients low precision use_orig_params bool If ` ` True ` ` then FSDP preserves original parameter variables returns them ` ` named_parameters ` ` e g support different optimizer hyperparameters within one ` FlatParameter ` If ` ` False ` ` then FSDP reconstructs parameters every iteration returns ` FlatParameter ` s ` ` named_parameters ` ` ################## INITIALIZATION ################## __init__ params Sequence Union nn Parameter Tensor fully_sharded_module nn Module device torch device sharding_strategy HandleShardingStrategy offload_params bool mp_param_dtype Optional torch dtype mp_reduce_dtype Optional torch dtype keep_low_precision_grads bool process_group dist ProcessGroup use_orig_params bool fsdp_extension Optional FSDPExtensions = None super __init__ params = list params len params == raise ValueError f Cannot construct __class__ __name__ empty parameter list _init_setattr_fns _skip_writeback_check = os environ get _FSDP_SKIP_WRITEBACK_CHECK == _use_full_prec_in_eval = os environ get _FSDP_USE_FULL_PREC_IN_EVAL == _use_fake_all_gather = os environ get _FSDP_USE_FAKE_ALL_GATHER == _use_fake_reduce = os environ get _FSDP_USE_FAKE_REDUCE == _skip_writeback_check _warn_skip_writeback_check logger f Since _FSDP_SKIP_WRITEBACK_CHECK = FSDP will check parameter gradient writeback Changing parameter gradient storages may lead silent correctness errors _use_fake_all_gather _warn_use_fake_all_gather logger f Since _FSDP_USE_FAKE_ALL_GATHER = FSDP will execute all-gather ops Your training will incorrect can reveal how much time spent all-gather ops _use_fake_reduce _warn_use_fake_reduce logger f Since _FSDP_USE_FAKE_REDUCE = FSDP will execute reduce-scatter ops Your training will incorrect can reveal how much time spent reduce-scatter ops Only align addresses ` use_orig_params=True ` now align_addresses = use_orig_params _init_get_unflat_views_fn align_addresses pyrefly ignore read-only device = device _device_handle = _FSDPDeviceHandle from_device device process_group = process_group _use_fake_all_gather _use_fake_reduce _fake_process_group = FakeProcessGroup _create_internal rank=process_group rank world_size=process_group size rank = process_group rank world_size = process_group size _sharding_strategy = sharding_strategy _offload_params = offload_params _use_orig_params = use_orig_params _keep_low_precision_grads = keep_low_precision_grads _training_state = HandleTrainingState IDLE _debug_level = dist get_debug_level _fully_sharded_module = fully_sharded_module For strategies do free after forward we skip using sharded views after forward since unsharded data exists We still switch ` flat_param ` point sharded flat parameter since what points parameterizes behavior We use following attribute track which tensor data parameters unsharded views into _unsharded_flat_param_for_skipped_views Optional Tensor = None The index state s ` all_handles ` which must same across ranks execution order validation work _handle_index Optional int = None Index handles_to_pre_forward_order _pre_forward_order_index Optional int = None Index ` handles_post_forward_order ` _post_forward_index Optional int = None Used guarding against mistargeted forward prefetches _needs_pre_forward_unshard = False Used guarding against mistargeted backward prefetches _needs_pre_backward_unshard = False Was handle prefetched Set successful _prefetch_handle unshard _prefetched = False Optimistically assume valid input ` params ` set dtype attributes before ` _init_flat_param ` which performs actual validation _orig_param_dtype = params dtype _init_param_reduce_dtypes mp_param_dtype mp_reduce_dtype _fwd_bwd_param_dtype None raise AssertionError Expected _fwd_bwd_param_dtype None mypy _aligned_numel = _get_aligned_numel unsharded_dtype=self _fwd_bwd_param_dtype align_addresses _fsdp_extension = fsdp_extension _init_flat_param_and_metadata params fully_sharded_module _aligned_numel use_orig_params type ignore arg-type _use_unsharded_views as_params=False __repr__ f FlatParamHandle flat_param fqns= flat_param _fqns _init_setattr_fns use_unsafe_setattr = os environ get _FSDP_USE_UNSAFE_SETATTR == _setattr_tensor Callable nn Module str Tensor None _setattr_param Callable nn Module str nn Parameter None use_unsafe_setattr _setattr_tensor = _unsafe_setattr_tensor _setattr_param = _unsafe_setattr_param _setattr_tensor = _safe_setattr_tensor_or_param _setattr_param = _safe_setattr_tensor_or_param _init_get_unflat_views_fn align_addresses bool _get_unflat_views = _get_unflat_views_aligned align_addresses _get_unflat_views_unaligned _init_flat_param_and_metadata params list Union Tensor nn Parameter module nn Module aligned_numel int use_orig_params bool - None Initialize ` ` FlatParameter ` ` its metadata NOTE This should only called once construction time after which ` ` FlatParameter ` ` metadata assumed static NOTE The elements ` ` params ` ` should only ` ` Tensor ` ` s when composing ` ` DTensor ` ` -based tensor parallelism which case elements may ` ` DTensor ` ` local shards len params == raise ValueError Expects non-empty ` params ` aligned_numel raise ValueError f Expects non-negative ` aligned_numel ` got aligned_numel dtype flat_param_requires_grad device = _validate_tensors_to_flatten params params_set = set params For alignment padding only ` numels ` gets strictly non- ` None ` elements all other lists get ` None ` elements padding param_infos list ParamInfo = numels list int = shapes list torch Size = strides list tuple int = contiguities list bool = fqns list str = shared_param_infos list SharedParamInfo = shared_param_memo dict Union Tensor nn Parameter tuple nn Module str str = params_to_flatten list Union Tensor nn Parameter = shared_params list Union Tensor nn Parameter = param_extensions list Any = is_padding_mask list bool = total_numel = total_numel_without_padding = submodule_name submodule module named_modules remove_duplicate=False param_name param _named_parameters_with_duplicates submodule recurse=False param params_set continue param shared_param_memo shared reference prim_module prim_module_name prim_param_name = shared_param_memo param shared_params append param shared_param_infos append SharedParamInfo param_name submodule submodule_name prim_param_name prim_module prim_module_name aligned_numel numel_to_pad = aligned_numel - total_numel aligned_numel numel_to_pad numel_to_pad aligned_numel padding_tensor = _construct_padding_tensor numel_to_pad dtype False device params_to_flatten append padding_tensor is_padding_mask append True numels append numel_to_pad total_numel += numel_to_pad transform_t extension = _ext_pre_flatten_transform param _fsdp_extension param = cast nn Parameter transform_t param_extensions append extension shared_param_memo param = submodule submodule_name param_name params_to_flatten append param is_padding_mask append False param_infos append ParamInfo param_name submodule submodule_name numels append param numel shapes append param shape strides append param stride contiguities append _is_truly_contiguous param fqn = submodule_name + + param_name submodule_name param_name fqns append fqn total_numel += param numel total_numel_without_padding += param numel len params_to_flatten == raise ValueError f ` params ` found ` module ` s tree f params params \nmodule module rank == aligned_numel total_numel = total_numel_without_padding logger debug FSDP FlatParameter address alignment created s numel padding s vs s total_numel - total_numel_without_padding total_numel total_numel_without_padding aligned_numel Pad divisible world size avoid copy post-backward reduce-scatter numel_to_pad = world_size - total_numel world_size numel_to_pad numel_to_pad world_size rank == logger info FSDP FlatParameter world size divisibility created s numel padding numel_to_pad padding_tensor = _construct_padding_tensor numel_to_pad dtype False device params_to_flatten append padding_tensor is_padding_mask append True numels append numel_to_pad total_numel += numel_to_pad Pass ` aligned_numel= ` since we already included padding tensors flat_param FlatParameter = flatten_tensors_into_flat_param params_to_flatten aligned_numel= requires_grad=flat_param_requires_grad FlatParameter _init_metadata flat_param param_infos numels shapes strides contiguities fqns shared_param_infos param_extensions _convert_to_params params_to_flatten use_orig_params None _convert_to_params shared_params use_orig_params None is_padding_mask _validate_tensors_to_flatten tensors list Union Tensor nn Parameter - tuple Validate tensors flatten returns any necessary metadata dtype Optional torch dtype = None Return logical OR over each tensor s value flat_param_requires_grad Optional bool = None device Optional torch device = None For ` use_orig_params=True ` permit non-uniform ` requires_grad ` tensor tensors isinstance tensor FlatParameter raise ValueError Cannot flatten ` FlatParameter ` dtype None tensor is_floating_point raise ValueError Cannot flatten integer dtype tensors dtype None tensor dtype = dtype raise ValueError f Must flatten tensors uniform dtype got dtype f tensor dtype _use_orig_params flat_param_requires_grad None tensor requires_grad = flat_param_requires_grad raise ValueError Must flatten tensors uniform ` requires_grad ` when ` use_orig_params=False ` device None tensor device = device raise ValueError Must flatten tensors same device got both f device tensor device dtype = tensor dtype flat_param_requires_grad = flat_param_requires_grad tensor requires_grad device = tensor device flat_param_requires_grad None raise AssertionError Requires non-empty ` tensors ` list dtype flat_param_requires_grad device flatten_tensors tensors list Tensor aligned_numel int - Tensor Flatten ` ` tensors ` ` into single flat tensor The flattening optionally includes padding ` ` aligned_numel ` ` greater than where ` ` aligned_numel ` ` gives numel required have address alignment NOTE The padding alignment algorithm must kept sync meth ` _init_flat_param_metadata ` We separate two methods because initialization happens once whereas method may called multiple times throughout training e g checkpointing len tensors == raise ValueError Expects non-empty ` tensors ` aligned_numel raise ValueError f Expects non-negative ` aligned_numel ` got aligned_numel dtype _ device = _validate_tensors_to_flatten tensors flat_tensors list Tensor = aligned_numel total_numel = tensor tensors numel_to_pad = aligned_numel - total_numel aligned_numel numel_to_pad numel_to_pad aligned_numel padding_tensor = _construct_padding_tensor numel_to_pad dtype False device flat_tensors append padding_tensor total_numel += numel_to_pad flat_tensors append torch flatten _detach_if_needed tensor _is_truly_contiguous tensor _detach_if_needed tensor as_strided tensor numel total_numel += tensor numel numel_to_pad = world_size - total_numel world_size numel_to_pad numel_to_pad world_size padding_tensor = _construct_padding_tensor numel_to_pad dtype False device flat_tensors append padding_tensor total_numel += numel_to_pad flat_tensors = torch flatten _detach_if_needed tensor _is_truly_contiguous tensor _detach_if_needed tensor as_strided tensor numel tensor tensors torch cat flat_tensors dim= flatten_tensors_into_flat_param tensors list Tensor aligned_numel int requires_grad bool - FlatParameter flat_param_data = flatten_tensors tensors aligned_numel FlatParameter flat_param_data requires_grad=requires_grad _init_param_reduce_dtypes mp_param_dtype Optional torch dtype mp_reduce_dtype Optional torch dtype - None Initialize param reduce dtypes Precondition ` ` flat_param ` ` set This ensures handle s parameters have single dtype Postcondition This sets ` ` _fwd_bwd_param_dtype ` ` ` ` _reduce_dtype ` ` If ` ` mp_param_dtype ` ` ` ` mp_reduce_dtype ` ` ` ` None ` ` then we assume original parameter dtype One special case ` ` mp_param_dtype ` ` ` ` None ` ` ` ` mp_reduce_dtype ` ` ` ` None ` ` which case we assume gradient reduction dtype matches forward backward parameter dtype Save whether these dtypes specified so we permit parameter dtype change up until lazy initialization _low_prec_param_dtype_specified = mp_param_dtype None _low_prec_reduce_dtype_specified = mp_reduce_dtype None _low_prec_param_dtype_specified _low_prec_reduce_dtype_specified Special case infer gradient reduction mixed precision _fwd_bwd_param_dtype = mp_param_dtype _reduce_dtype = _fwd_bwd_param_dtype _fwd_bwd_param_dtype = mp_param_dtype _orig_param_dtype _reduce_dtype = mp_reduce_dtype _orig_param_dtype _fwd_bwd_param_dtype None raise AssertionError Expected _fwd_bwd_param_dtype None _reduce_dtype None raise AssertionError Expected _reduce_dtype None ################################### SHARD INITIALIZATION METADATA ################################### torch no_grad shard Shard handle s ` ` FlatParameter ` ` This allocates new memory sharded flat parameter frees unsharded flat parameter s storage Postcondition ` ` flat_param ` ` sharded flat parameter Shard metadata attributes set all sharding strategies flat_param = flat_param uses_sharded_strategy _init_shard_metadata flat_param numel - _p_assert flat_param storage_offset == The ` FlatParameter ` sole occupant its storage sharded_flat_param numel_padded = FlatParamHandle _get_shard flat_param rank world_size torch distributed _functional_collectives is_torchdynamo_compiling allocated = flat_param _typed_storage _size allocated flat_param _typed_storage _resize_ flat_param set_ sharded_flat_param type ignore call-overload start_idx = sharded_flat_param numel rank end_idx = sharded_flat_param numel rank + - inclusive _init_shard_metadata numel_padded start_idx end_idx _use_orig_params _use_sharded_views _init_shard_metadata numel_padded int unsharded_start_idx int unsharded_end_idx int - None Initialize shard-related metadata rank s shard flat parameter This includes ` ` _sharded_size ` ` ` ` _shard_param_infos ` ` ` ` _shard_numel_padded ` ` Args numel_padded int Numel padded rank s sharded flat parameter unsharded_start_idx int Start index unsharded flat parameter assigned rank unsharded_end_idx int End index inclusive unsharded flat parameter assigned rank Precondition ` ` flat_param ` ` s data sharded flat parameter flat_param = flat_param flat_param _sharded_size = flat_param size type ignore attr-defined sharded_flat_param_numel = flat_param numel includes ` numel_padded ` _p_assert unsharded_start_idx = unsharded_start_idx = unsharded_end_idx f unsharded_start_idx unsharded_start_idx unsharded_end_idx unsharded_end_idx _p_assert numel_padded = sharded_flat_param_numel f numel_padded numel_padded f sharded_flat_param_numel sharded_flat_param_numel shard_param_infos = _get_shard_metadata unsharded_start_idx unsharded_end_idx len shard_param_infos = flat_param _num_params raise AssertionError f Expects length flat_param _num_params got len shard_param_infos flat_param _shard_param_infos = shard_param_infos type ignore attr-defined flat_param _shard_numel_padded = numel_padded type ignore attr-defined _get_shard_metadata unsharded_start_idx int unsharded_end_idx int - tuple _ShardParamInfo Compute shard metadata based ` ` unsharded_start_idx ` ` ` ` unsharded_end_idx ` ` inclusive ` ` unsharded_start_idx ` ` ` ` unsharded_end_idx ` ` give interval unsharded flat parameter specifying shard flat_param_offsets = _get_flat_param_offsets len flat_param_offsets = len flat_param _numels_with_padding raise AssertionError f Expected len flat_param _numels_with_padding got len flat_param_offsets shard_param_infos list _ShardParamInfo = sharded_flat_param_numel = unsharded_end_idx - unsharded_start_idx + ` unsharded_param_start_idx ` ` unsharded_param_end_idx ` indices into unsharded flat parameter inclusive given parameter unsharded_param_start_idx unsharded_param_end_idx is_padding zip flat_param_offsets flat_param _is_padding_mask is_padding continue in_sharded_flat_param = unsharded_start_idx = unsharded_param_end_idx unsharded_end_idx = unsharded_param_start_idx in_sharded_flat_param shard_param_info = _ShardParamInfo False None None None None unsharded_start_idx = unsharded_param_start_idx This branch can only happen once since rank s unsharded start index can only intersect one parameter intra_param_start_idx = offset_in_shard = unsharded_param_start_idx - unsharded_start_idx intra_param_start_idx = unsharded_start_idx - unsharded_param_start_idx offset_in_shard = offset_in_shard = offset_in_shard sharded_flat_param_numel raise AssertionError f Invalid ` offset_in_shard ` offset_in_shard f sharded flat parameter sharded_flat_param_numel numel intra_param_end_idx = min unsharded_param_end_idx unsharded_end_idx - unsharded_param_start_idx numel_in_shard = intra_param_end_idx - intra_param_start_idx + shard_param_info = _ShardParamInfo True offset_in_shard numel_in_shard intra_param_start_idx intra_param_end_idx shard_param_infos append shard_param_info tuple shard_param_infos staticmethod _get_unpadded_shard tensor Tensor rank int world_size int - tuple Tensor int Return unpadded shard ` ` tensor ` ` given ` ` rank ` ` ` ` world_size ` ` The returned value tuple shard ` ` tensor ` ` without any padding numel pad shard If ` ` tensor ` ` already flattened may viewed flattened shape which true expected usage then method does allocate any new tensor memory chunks = torch flatten tensor chunk world_size _is_truly_contiguous tensor tensor as_strided tensor numel chunk world_size len chunks rank + This rank gets empty chunk fully padded zeros since there enough chunks across ranks chunk = chunks new_empty chunk = chunks rank numel_to_pad = chunks numel - chunk numel numel_to_pad raise AssertionError Chunk s size should most first chunk s size chunk numel_to_pad staticmethod _get_shard tensor Tensor rank int world_size int - tuple Tensor int Return shard ` ` tensor ` ` padding given ` ` rank ` ` ` ` world_size ` ` numel padded shard This method allocates new memory via meth ` clone ` since unsharded ` ` tensor ` ` may deallocated after method returns chunk numel_to_pad = FlatParamHandle _get_unpadded_shard tensor rank world_size shard = chunk clone numel_to_pad shard = F pad shard numel_to_pad shard numel_to_pad staticmethod _get_sharded_size tensor Tensor rank int world_size int - torch Size Return shape ` ` tensor ` ` after sharding including padding This requires ` ` tensor ` ` have D shape ensures returned shape D len tensor shape = raise AssertionError f Expected D tensor shape got tensor shape unpadded_sharded_tensor numel_to_pad = FlatParamHandle _get_unpadded_shard tensor rank world_size unpadded_sharded_size = unpadded_sharded_tensor size len unpadded_sharded_size = raise AssertionError f Expected D unpadded_sharded_size got unpadded_sharded_size torch Size unpadded_sharded_size + numel_to_pad _get_flat_param_offsets - list tuple int int Return start end offsets each original parameter s flattened data unsharded flat parameter without padding NOTE The returned list includes elements alignment padding cumulative_sum = list accumulate flat_param _numels_with_padding starts = + cumulative_sum - ends = end - end cumulative_sum inclusive param_offsets = list zip starts ends param_offsets no_type_check shard_metadata - FlatParamShardMetadata Return shard-related metadata specific rank s shard flat parameter NOTE The returned tuple does include elements alignment padding does account padding fqns_list = shapes_list = strides_list = contiguities_list = numels_list = shard_param_offsets = fqn shape stride contiguous numel shard_param_info zip flat_param _fqns flat_param _shapes flat_param _strides flat_param _contiguities flat_param _numels flat_param _shard_param_infos shard_param_info in_shard continue fqns_list append fqn shapes_list append shape strides_list append stride contiguities_list append contiguous numels_list append numel shard_param_offsets append shard_param_info intra_param_start_idx shard_param_info intra_param_end_idx FlatParamShardMetadata tuple fqns_list tuple shapes_list tuple strides_list tuple contiguities_list tuple numels_list tuple shard_param_offsets no_type_check torch no_grad init_flat_param_attributes - None This initializes some attributes handle s ` ` FlatParameter ` ` This should called during lazy initialization since requires parameter compute device offloading CPU we want give users chance move parameter appropriately after FSDP constructor For each tensor attribute ` ` FlatParameter ` ` see unshard reshard methods allocation free pattern flat_param = flat_param flat_param dtype = _orig_param_dtype Entering branch means user changed parameter dtype after FSDP initialization which case we may need refresh some saved dtype attributes dtypes specified part mixed precision take precedence _low_prec_param_dtype_specified _fwd_bwd_param_dtype = flat_param dtype For ` reduce_dtype ` require ` param_dtype ` specified since then we infer ` reduce_dtype ` specified ` param_dtype ` _low_prec_reduce_dtype_specified _low_prec_param_dtype_specified _reduce_dtype = flat_param dtype _orig_param_dtype = flat_param dtype cpu_device = torch device cpu _offload_params _p_assert flat_param device == cpu_device f Expects ` FlatParameter ` CPU when parameter CPU f offloading enabled flat_param device _check_on_compute_device flat_param flat_param _local_shard = flat_param data _offload_params Pin memory faster H D transfer flat_param _local_shard = flat_param _local_shard pin_memory Pre-allocate sharded gradient CPU enable non-blocking D H transfer during backward pass flat_param _cpu_grad = torch zeros_like flat_param _local_shard device=cpu_device pin_memory _uses_param_mixed_precision For parameter mixed precision we maintain low precision sharded tensor compute device all-gathered sharded strategies directly used ` NO_SHARD ` computation flat_param _mp_shard = torch empty_like flat_param _local_shard device=self device dtype=self _fwd_bwd_param_dtype _free_storage flat_param _mp_shard uses_sharded_strategy We maintain padded unsharded tensor serves all-gather destination owns original parameter storages unsharded_param_dtype = _fwd_bwd_param_dtype _uses_param_mixed_precision flat_param dtype use low precision parameter mixed precision enabled padded_unsharded_numel = flat_param numel world_size flat_param _full_param_padded = torch empty padded_unsharded_numel device=self device dtype=unsharded_param_dtype flat_param _padded_unsharded_size = flat_param _full_param_padded size _free_storage flat_param _full_param_padded _uses_param_mixed_precision For parameter mixed precision we maintain full precision padded unsharded tensor when we force full precision flat_param _full_prec_full_param_padded = torch empty padded_unsharded_numel device=self device dtype=flat_param dtype full precision _free_storage flat_param _full_prec_full_param_padded ################### UNSHARD RESHARD ################### pre_unshard - bool Return ` ` False ` ` no-op ` ` True ` ` otherwise Postcondition ` ` flat_param ` ` s data device communication what should all-gathered This means matches dtype expected unsharded parameter _training_state == HandleTrainingState SUMMON_FULL_PARAMS _skipped_use_sharded_views Since path imposes special semantics unsharded flat parameter e g forcing full precision use sharded views reuse existing logic special handling _use_sharded_views ret = False _use_orig_params _skip_writeback_check ret = _writeback_orig_params uses_sharded_strategy _offload_params needs_unshard pass no-op _uses_param_mixed_precision _force_full_precision _use_low_precision_shard ret = True _offload_params flat_param device = device NOTE This creates new tensor distinct any attributes flat_param_to device non_blocking=True ret = True _check_on_compute_device flat_param ret _use_low_precision_shard Allocate compute device switch using low precision sharded flat parameter _check_low_precision_shard flat_param = flat_param _alloc_storage flat_param _mp_shard flat_param _local_shard size type ignore attr-defined ` copy_ ` implicitly casts low precision flat_param _mp_shard copy_ type ignore attr-defined flat_param _local_shard type ignore attr-defined device non_blocking=True Invariant ` _mp_shard ` always compute device flat_param data = flat_param _mp_shard type ignore attr-defined unshard Run unshard logic This includes all-gathering flat parameter switching using unsharded flat parameter If handle does need unsharding then only switches using unsharded flat parameter For ` ` NO_SHARD ` ` no-op If FSDP meth ` summon_full_params ` handle uses parameter mixed precision then parameter forced full precision needs_unshard Even when needing unshard we should switch using unsharded flat parameter unsharded_flat_param = _get_padded_unsharded_flat_param uses_sharded_strategy flat_param _use_unsharded_flat_param unsharded_flat_param unsharded_flat_param = _alloc_padded_unsharded_flat_param padded_unsharded_flat_param = _all_gather_flat_param unsharded_flat_param _use_unsharded_flat_param padded_unsharded_flat_param needs_unshard - bool Return handle s flat parameter needs unsharded uses_sharded_strategy False unsharded_flat_param = _get_padded_unsharded_flat_param already_unsharded = _same_storage_size unsharded_flat_param unsharded_flat_param numel already_unsharded _alloc_padded_unsharded_flat_param Allocate padded unsharded flat parameter The unpadded unsharded flat parameter always view into padded one This padded parameter saved different attribute ` ` FlatParameter ` ` depending we force full precision _check_sharded_strategy flat_param = flat_param unsharded_flat_param = _get_padded_unsharded_flat_param _check_storage_freed unsharded_flat_param _alloc_storage unsharded_flat_param flat_param _padded_unsharded_size type ignore attr-defined unsharded_flat_param _get_padded_unsharded_flat_param - torch Tensor Return reference padded unsharded flat parameter depending calling context This should only called using sharded strategy _check_sharded_strategy flat_param = flat_param _force_full_precision _uses_param_mixed_precision When parameter mixed precision enabled we use different tensor all-gather destination preserve invariant ` _full_param_padded ` low precision unsharded_flat_param = flat_param _full_prec_full_param_padded type ignore attr-defined _p_assert unsharded_flat_param dtype = _fwd_bwd_param_dtype f Expects full precision got _fwd_bwd_param_dtype For no-reshard-after-forward strategies ` _full_param_padded ` may still allocated previous forward As we forcing full precision here full-precision unsharded copy may modified invalidating existing low-precision unsharded copy so we should free here ensure new all-gather next forward backward computation persist modifications flat_param _full_param_padded untyped_storage size _free_storage flat_param _full_param_padded unsharded_flat_param = flat_param _full_param_padded type ignore attr-defined unsharded_flat_param _all_gather_flat_param padded_unsharded_flat_param Tensor - Tensor All-gather handle s flat parameter destination ` ` padded_unsharded_flat_param ` ` Then switch use all-gathered tensor _p_assert hasattr process_group hasattr world_size Expects process group world size have been set via ` shard ` sharded_flat_param = flat_param data expected_numel = sharded_flat_param numel world_size _p_assert padded_unsharded_flat_param numel == expected_numel f Expects expected_numel numel got padded_unsharded_flat_param numel pg = _fake_process_group _use_fake_all_gather process_group HACK should handled C D sharded_flat_param is_cpu type ignore attr-defined tensor_list = list torch chunk padded_unsharded_flat_param dist get_world_size pg type ignore arg-type dist all_gather tensor_list sharded_flat_param group=pg dist all_gather_into_tensor padded_unsharded_flat_param sharded_flat_param pg _offload_params In case offloading ` flat_param data ` i e sharded param created pre-unshard stream We need hand over unshard stream all-gather _no_dispatch_record_stream sharded_flat_param _device_handle current_stream unshard_stream padded_unsharded_flat_param _use_unsharded_flat_param padded_unsharded_flat_param torch Tensor - None Switch use unpadded unsharded flat parameter This view into padded unsharded flat parameter unsharded_size = flat_param _unpadded_unsharded_size flat_param_part = padded_unsharded_flat_param unsharded_size numel slicing visible autograd because data flat_param data = flat_param_part in_forward = _training_state == HandleTrainingState FORWARD in_pre_backward = _training_state == HandleTrainingState BACKWARD_PRE _use_orig_params _skipped_use_sharded_views in_pre_backward This call corresponds complementary pre-backward ` _use_unsharded_views ` skipped pre-forward ` _use_sharded_views ` so we should skip one too We use ` Tensor ` views forward so they tracked autograd We use them pre-backward well support reentrant activation checkpointing which needs views tracked autograd backward pass s recomputed forward _use_unsharded_views as_params= in_forward in_pre_backward in_forward _use_unsharded_views as_params=False post_unshard Run post-unshard logic This includes freeing low precision shard needed _uses_param_mixed_precision uses_sharded_strategy _free_low_precision_sharded_param _check_on_compute_device flat_param _free_low_precision_sharded_param Frees low precision sharded flat parameter _check_low_precision_shard ` _mp_shard ` allocated pre-unshard stream consumed unshard stream sharded strategies consumed both unshard default streams ` NO_SHARD ` For sharded strategies current stream here unshard stream ` NO_SHARD ` default stream For ` NO_SHARD ` only recording default stream suffices since default stream waits unshard stream _no_dispatch_record_stream flat_param _mp_shard _device_handle current_stream type ignore attr-defined _free_storage flat_param _mp_shard type ignore attr-defined torch no_grad unshard_grad Unshard handle s ` ` FlatParameter ` ` s gradient If all ranks have ` ` None ` ` gradient then all original parameters will well This method performs all-reduce all-gather The additional all-reduce tolerable since method meant used computation critical path Postcondition ` ` _saved_grad_shard ` ` defined contains value set ` ` flat_param grad ` ` after gradients resharded uses_sharded_strategy _use_unsharded_grad_views flat_param = flat_param _check_unsharded flat_param Check all ranks have ` None ` gradient num_grad_none = torch zeros dtype=torch int device=self device num_grad_none = flat_param grad None dist all_reduce num_grad_none group=self process_group num_grad_none == world_size flat_param _saved_grad_shard = None type ignore assignment _use_unsharded_grad_views flat_param grad None In case only some ranks have ` None ` gradient we use zeros approximate best effort attempt _debug_level == dist DebugLevel INFO warnings warn f Rank rank Only some all ranks have ` None ` ` FlatParameter ` gradient so FSDP using zeros approximate those ranks sharded gradients being ` None ` stacklevel= flat_param _saved_grad_shard = None type ignore assignment sharded_grad = torch zeros flat_param _sharded_size device=self device type ignore attr-defined _check_sharded flat_param grad flat_param _saved_grad_shard = flat_param grad type ignore attr-defined sharded_grad = flat_param _saved_grad_shard type ignore attr-defined padded_unsharded_grad = torch empty flat_param _padded_unsharded_size type ignore attr-defined device=self device dtype=sharded_grad dtype dist all_gather_into_tensor padded_unsharded_grad sharded_grad process_group unsharded_size = flat_param _unpadded_unsharded_size flat_param grad = padded_unsharded_grad unsharded_size numel view unsharded_size _use_unsharded_grad_views reshard_grad _use_orig_params _use_sharded_grad_views uses_sharded_strategy flat_param grad = flat_param _saved_grad_shard type ignore attr-defined delattr flat_param _saved_grad_shard prepare_gradient_for_backward Prepare gradient backward computation This done saving clearing any existing sharded gradient ` ` grad ` ` enable computing new unsharded gradient _p_assert _training_state HandleTrainingState BACKWARD_PRE HandleTrainingState IDLE Expects ` BACKWARD_PRE ` ` IDLE ` prefetching flat_param = flat_param flat_param grad None flat_param grad size = flat_param _unpadded_unsharded_size flat_param grad device = flat_param device grad CPU _check_on_compute_device flat_param grad_offloaded = flat_param grad device = device _p_assert grad_offloaded _offload_params f Expects sharded gradient device f got flat_param grad device prev_iter_synced_gradients = flat_param grad size == flat_param _local_shard size type ignore attr-defined prev_iter_synced_gradients TODO awgu Gradient accumulation outside ` no_sync ` does work CPU offloading The issue should post-backward hook we cannot do addition between CPU tensor existing sharded gradient GPU tensor new sharded gradient grad_offloaded flat_param _saved_grad_shard = flat_param grad data type ignore attr-defined sharded_grad = flat_param _saved_grad_shard type ignore attr-defined _p_assert hasattr flat_param _cpu_grad ` _cpu_grad ` should defined gradient CPU sharded_grad = flat_param _cpu_grad type ignore attr-defined If user specified keep gradient low precision then gradient may still low precision dtype user did set gradient ` None ` after previous backward which case FSDP should cast back full precision dtype so FSDP can accumulate dtype post-backward hook assign ` grad ` dtype post-backward callback local_shard_dtype = flat_param _local_shard dtype type ignore attr-defined _keep_low_precision_grads sharded_grad dtype = local_shard_dtype sharded_grad data = sharded_grad local_shard_dtype padded_unsharded_size = flat_param _padded_unsharded_size type ignore attr-defined _p_assert flat_param grad size == padded_unsharded_size Expects ` grad ` unsharded gradient f ` no_sync ` size padded_unsharded_size f got size flat_param grad size flat_param grad = None prepare_gradient_for_optim Prepare gradient optimizer computation moving sharded gradient ` ` grad ` ` attribute cast_grad_to_param_dtype_if_needed flat_param TODO rohan-varma test full precision keep_low_precision_grads _force_full_precision _keep_low_precision_grads _p_assert flat_param grad None Unexpected None grad flat_param grad dtype = _fwd_bwd_param_dtype flat_param grad data = flat_param grad _fwd_bwd_param_dtype _use_orig_params _use_sharded_grad_views flat_param = flat_param TODO awgu We should replace these conditional checks encode logical intention more directly hasattr flat_param _cpu_grad NOTE This branch includes ` NO_SHARD ` _check_sharded flat_param _check_on_cpu flat_param flat_param grad = flat_param _cpu_grad type ignore attr-defined cast_grad_to_param_dtype_if_needed flat_param hasattr flat_param _saved_grad_shard _check_sharded flat_param _check_on_compute_device flat_param flat_param _saved_grad_shard None _check_on_compute_device flat_param _saved_grad_shard type ignore attr-defined If no sharded gradient computed iteration then there no need forward ` _saved_grad_shard ` ` grad ` flat_param _post_backward_called type ignore attr-defined flat_param grad = flat_param _saved_grad_shard type ignore attr-defined flat_param grad None cast_grad_to_param_dtype_if_needed flat_param _p_assert uses_sharded_strategy flat_param _post_backward_called type ignore attr-defined All sharded parameters received gradient post-backward should use ` _saved_grad_shard ` Delete ` _saved_grad_shard ` since its existence indicates previous gradient accumulate post-backward hook hasattr flat_param _saved_grad_shard delattr flat_param _saved_grad_shard contextlib contextmanager to_cpu Move unpadded unsharded flat parameter CPU while context moves back previous device upon exit For now assumes ` ` FlatParameter ` ` unpadded unsharded flat parameter since there no reason include padding copy there no use case sharded flat parameter Precondition ` ` flat_param ` ` s data unpadded unsharded flat parameter compute device handle uses sharded strategy Postcondition Same precondition _check_sharded_strategy _p_assert flat_param size == flat_param _unpadded_unsharded_size f Expects size flat_param _unpadded_unsharded_size got flat_param size _check_on_compute_device flat_param Check unpadded unsharded flat parameter view into padded unsharded flat parameter expected NOTE This check strictly needed correctness useful sanity check since tensor should only used internally _p_assert _same_storage flat_param _get_padded_unsharded_flat_param Expects unpadded parameter view into padded parameter flat_param_to torch device cpu _free_unsharded_flat_param try yield finally _p_assert flat_param size == flat_param _unpadded_unsharded_size f Expects size flat_param _unpadded_unsharded_size got flat_param size padded_unsharded_flat_param = _alloc_padded_unsharded_flat_param Copy CPU compute device padded_unsharded_flat_param flat_param numel copy_ flat_param _use_unsharded_flat_param padded_unsharded_flat_param reshard free_unsharded_flat_param bool Run reshard logic This includes freeing unsharded flat parameter ` ` free_unsharded_flat_param ` ` switching using sharded flat parameter Note also implicitly offloads sharded flat parameter CPU offload enabled pointing ` ` _local_shard ` ` attribute which resides CPU Switch sharded ` FlatParameter ` before freeing prevent use-after-free -type bugs external profiling tools where ` use_orig_params=True ` ` param ` does point valid memory when setting ` param data = ` ` _use_sharded_views ` _use_sharded_flat_param free_unsharded_flat_param _free_unsharded_flat_param post_reshard Run post-reshard logic This includes freeing any memory can now freed given ` ` FlatParameter ` ` points full precision sharded flat parameter Precondition ` ` flat_param ` ` s data points full precision sharded flat parameter For ` NO_SHARD ` ` _mp_shard ` freed post-unshard since also low precision unsharded flat parameter Hence we delay free until reshard _uses_param_mixed_precision uses_sharded_strategy _force_full_precision did use low precision shard _free_low_precision_sharded_param _free_unsharded_flat_param Free padded unsharded flat parameter We allow function called even when storage allocated The tensor free depends calling context since unshard may have forced full precision which case different tensor used _check_sharded_strategy unsharded_flat_param = _get_padded_unsharded_flat_param _check_on_compute_device unsharded_flat_param Do free memory until all ops current stream finish _no_dispatch_record_stream unsharded_flat_param _device_handle current_stream _free_storage unsharded_flat_param _use_sharded_flat_param - None Switches using sharded flat parameter flat_param = flat_param _use_orig_params in_forward = _training_state == HandleTrainingState FORWARD skip_use_sharded_views = torch is_grad_enabled in_forward _sharding_strategy NO_RESHARD_AFTER_FORWARD_HANDLE_STRATEGIES Only incur extra ` data ` call needed skip_use_sharded_views unsharded_flat_param = flat_param data _offload_params device = flat_param _local_shard device type ignore attr-defined _p_assert device == torch device cpu f Expects local shard CPU got device flat_param data = flat_param _local_shard type ignore attr-defined _use_orig_params skip_use_sharded_views type ignore possibly-undefined _unsharded_flat_param_for_skipped_views = unsharded_flat_param type ignore possibly-undefined _use_sharded_views For post-forward reshard we may try use sharded gradient views unsharded gradient views gradient accumulated ` no_sync ` post-backward reshard we delay call after reduce-scatter in_forward type ignore possibly-undefined Skip using gradient views skipped using sharded views since exposing unsharded parameters sharded gradients may confusing user _skipped_use_sharded_views TODO Change ` _unpadded_unsharded_size ` we change gradient computed directly padding accumulated_grad_in_no_sync = flat_param grad None uses_sharded_strategy flat_param grad shape == flat_param _unpadded_unsharded_size accumulated_grad_in_no_sync _use_unsharded_grad_views _use_sharded_grad_views ######### VIEWS ######### no_type_check _get_unflat_views_unaligned tensor Optional torch Tensor = None - Iterator Tensor Return unflattened ` ` Tensor ` ` views into ` ` tensor ` ` If ` tensor ` ` ` ` None ` ` ` ` flat_param ` ` used The unflattening based ` ` flat_param ` ` s metadata Examples ` ` tensor ` ` include ` ` flat_param grad ` ` unsharded tensor optimizer state flat_param = flat_param tensor None tensor = flat_param views = _ext_post_unflatten_transform subtensor view shape contiguous subtensor as_strided shape stride param_extension _fsdp_extension subtensor shape stride contiguous param_extension zip torch split tensor flat_param _numels dim= flat_param _shapes flat_param _strides flat_param _contiguities flat_param _param_extensions views no_type_check _get_unflat_views_aligned tensor Optional Tensor = None - list Tensor Return unflattened ` ` Tensor ` ` views into ` ` tensor ` ` handling padding This method has same contract meth ` _get_unflat_views_unaligned ` except checks ` ` None ` ` placeholders representing padding alignment which may incur slightly more CPU overhead flat_param = flat_param tensor None tensor = flat_param splits list Tensor = torch split tensor flat_param _numels_with_padding dim= idx = views list Tensor = split is_padding zip splits flat_param _is_padding_mask is_padding continue views append _ext_post_unflatten_transform split view flat_param _shapes idx flat_param _contiguities idx split as_strided flat_param _shapes idx flat_param _strides idx flat_param _param_extensions idx _fsdp_extension idx += views no_type_check torch enable_grad _use_unsharded_views as_params bool - None Unflatten unsharded flat parameter setting original parameter variables views into Args as_params bool If ` ` True ` ` then registers original parameters ` ` nn Parameter ` ` s ` ` False ` ` then registers original parameters only ` ` Tensor ` ` s ` ` False ` ` should used during forward backward computation when hiding original parameters meth ` nn Module named_parameters ` Note when prefetching next forward current forward may annotated ` torch no_grad ` ` torch enable_grad ` ensures non-empty ` view grad_fn ` otherwise ` _post_backward_hook ` will get called flat_param = flat_param _check_unsharded flat_param views = _get_unflat_views torch distributed tensor DTensor i view param_name module _ enumerate zip views flat_param _param_infos _use_orig_params as_params type view DTensor A ` DTensor ` ` view ` compatible assigning ` param data = view ` so we cannot preserve parameter variable _setattr_param module param_name nn Parameter view requires_grad=flat_param requires_grad continue param = flat_param _params i _setattr_param module param_name param param data = view as_params _setattr_param module param_name nn Parameter view requires_grad=flat_param requires_grad ` as_params=False ` param_var Tensor = view _use_orig_params _training_state == HandleTrainingState FORWARD Save ` Tensor ` pre-backward flat_param _tensors i = view save pre-backward _training_state == HandleTrainingState BACKWARD_PRE Use saved ` Tensor ` variable forward preserve autograd graph so post-backward hook fires e g reentrant AC tensor = flat_param _tensors i tensor data = view param_var = tensor _setattr_tensor module param_name param_var _use_orig_params _training_state == HandleTrainingState FORWARD module _parameters param_name = param_var i param_name module _ prim_param_name prim_module _ enumerate flat_param _shared_param_infos prim_param Union Tensor nn Parameter = getattr prim_module prim_param_name _p_assert as_params isinstance prim_param nn Parameter f as_params= as_params type prim_param = type prim_param _use_orig_params as_params shared_param = flat_param _shared_params i _setattr_param module param_name shared_param shared_param data = prim_param as_params _setattr_param module param_name prim_param _setattr_tensor module param_name prim_param _use_orig_params _training_state == HandleTrainingState FORWARD module _parameters param_name = prim_param no_type_check _use_unsharded_grad_views - None Unflatten unsharded flat parameter s gradient The original parameter variables gradients set views into unsharded flat parameter s gradient Expects gradient ` flat_param grad ` flat_param grad None param chain flat_param _params flat_param _shared_params param grad = None _check_unsharded flat_param grad views = _get_unflat_views flat_param grad i view param_name module _ enumerate zip views flat_param _param_infos _p_assert hasattr module param_name f flat_param _fqns i missing param = getattr module param_name param shape = view shape param dtype = view dtype param device = view device NOTE This hack using ` data ` side step check parameter gradient sizes dtypes devices match From calling ` reshard ` ` param ` has sharded size has full precision dtype CPU offloading enabled CPU Thus one more following cases can hold when ` no_sync ` where ` view ` original parameter s gradient ` view ` can have unsharded size ` view ` can have parameter low precision dtype ` view ` can GPU param grad None param grad = torch empty_like param param grad data = view param grad = view param_name module module_name prim_param_name prim_module _ flat_param _shared_param_infos _p_assert hasattr module param_name f module_name + + param_name module_name param_name missing param = getattr module param_name prim_param = getattr prim_module prim_param_name param shape = prim_param grad shape param dtype = prim_param grad dtype param device = prim_param grad device NOTE This same hack use ` data ` side step size check param grad None param grad = torch empty_like param param grad data = prim_param grad param grad = prim_param grad contextlib contextmanager unflatten_as_params - Generator Unflatten original parameters The function assumes flat parameter unsharded When context unflattens original parameters ` ` nn Parameter ` ` views into flat parameter after context restores original parameters ` ` Tensor ` ` views into flat parameter _use_unsharded_views as_params=True try yield finally _use_unsharded_views as_params=False no_type_check torch no_grad _use_sharded_views - None Set original parameter variables data flattened views into sharded flat parameter The views kept flattened simplify case where parameter sharded across ranks Parameters whose data present sharded flat parameter have their data set size- empty tensor We do delete them ensure preserve expected behaviors like model printability Parameters whose data present must preserve their variables passable optimizer _unsharded_flat_param_for_skipped_views = None uses_sharded_strategy For ` NO_SHARD ` use unflattened unsharded views since we have unsharded parameter _use_unsharded_views as_params=True flat_param = flat_param _check_sharded flat_param Construct once reuse all parameters local shard size_ _empty_tensor = torch empty dtype=self flat_param dtype case ` flat_param ` changed dtype device=self flat_param device requires_grad=False param shard_param_info param_name module _ zip flat_param _params flat_param _shard_param_infos flat_param _param_infos _setattr_param module param_name param shard_param_info in_shard Allow original data freed via garbage collection param data = size_ _empty_tensor offset = shard_param_info offset_in_shard numel_in_shard = shard_param_info numel_in_shard param data = flat_param offset offset + numel_in_shard flat_param _shared_params None raise AssertionError Expected _shared_params None param param_name module _ prim_param_name prim_module _ zip flat_param _shared_params flat_param _shared_param_infos _setattr_param module param_name param prim_param = getattr prim_module prim_param_name param data = prim_param could both empty non-empty _training_state == HandleTrainingState BACKWARD_POST Clear saved ` Tensor ` s since they unneeded now i range len flat_param _tensors flat_param _tensors i = None no_type_check torch no_grad _use_sharded_grad_views - None Set original parameter variables gradients flattened views into sharded flat parameter s gradient This no-op there no gradient Parameters whose data present sharded flat parameter parameters ` ` requires_grad=False ` ` have their gradients set ` ` None ` ` Since gradient variables do need preserved method does manipulate existing ` ` Tensor ` ` data directly creates new ` ` Tensor ` ` variables instead flat_param = flat_param _check_sharded flat_param grad = sharded_grad grad None param chain flat_param _params flat_param _shared_params param grad = None _check_sharded grad param shard_param_info is_grad_none zip flat_param _params flat_param _shard_param_infos flat_param _is_grad_none_mask shard_param_info in_shard param grad = None numel_in_shard = shard_param_info numel_in_shard param requires_grad is_grad_none offset = shard_param_info offset_in_shard _keep_low_precision_grads param dtype = grad dtype NOTE This hack using ` data ` side step check parameter gradient dtypes match Here ` param ` has full precision ` grad ` has low precision param grad None ` grad ` must have same shape ` param ` param grad = torch empty_like param param grad data = grad offset offset + numel_in_shard reshape param shape param grad = grad offset offset + numel_in_shard reshape param shape param grad = None flat_param _shared_params None raise AssertionError Expected _shared_params None param _ _ _ prim_param_name prim_module _ zip flat_param _shared_params flat_param _shared_param_infos in_sharded_flat_param = hasattr prim_module prim_param_name in_sharded_flat_param param requires_grad prim_param = getattr prim_module prim_param_name param grad = prim_param grad share same reference param grad = None no_type_check torch no_grad _writeback_orig_params - bool Write back any parameters changed storage handle s ` ` FlatParameter ` ` Iterates over original parameters writes back any parameters changed storages due non-inplace operator handle s ` ` FlatParameter ` ` This method preserves ` ` FlatParameter ` s device even original parameter s device changes Raises RuntimeError If original parameter gradient changes storages no longer has expected flattened shape Returns ` ` True ` ` some writeback happened ` ` False ` ` otherwise uses_sharded_strategy is_sharded flat_param _skipped_use_sharded_views For ` NO_SHARD ` we may still need writeback False flat_param = flat_param wroteback = False _skipped_use_sharded_views uses_sharded_strategy NOTE We must use unsharded flat parameter which unsharded views computed one current calling context ` _get_padded_unsharded_flat_param ` since may different e g model changed train eval flat_param_tensor = _unsharded_flat_param_for_skipped_views _p_assert _data_ptr_allocated flat_param_tensor If skipped using sharded views unsharded flat parameter should allocated flat_param_tensor = flat_param NOTE Since method called pre-unshard which only called during computation pre-forward pre-backward sharded gradient should guaranteed ` grad ` ` _saved_grad_shard ` flat_param_grad = flat_param grad uses_sharded_strategy _offload_params flat_param _cpu_grad i param in_shard offset_in_shard numel_in_shard _ _ param_name module _ enumerate zip flat_param _params flat_param _shard_param_infos flat_param _param_infos in_shard continue hasattr module param_name Do writeback original parameters deregistered e g during model checkpointing continue Check parameter writeback _skipped_use_sharded_views param = flat_param _tensors i _p_assert param None f Expects have saved tensor flat_param _fqns i param_changed = getattr module param_name param needs_param_writeback = param_changed changed parameter variable itself _same_storage param flat_param_tensor _skipped_use_sharded_views param_changed needs_param_writeback raise AssertionError FSDP does support changing parameters between f forward backward _sharding_strategy param_changed NOTE The gradient preserved after parameter change param = getattr module param_name flat_param _params i = param needs_param_writeback expected_shape = torch Size numel_in_shard src = param uses_sharded_strategy param view - _writeback_tensor src flat_param i expected_shape offset_in_shard True wroteback = True Check gradient writeback _skipped_use_sharded_views Skip writeback check because we do expose gradients when we skipped using sharded views continue param grad None flat_param grad None expected_shape = torch Size numel_in_shard _writeback_tensor None flat_param grad i expected_shape offset_in_shard False param grad None For ` NO_SHARD ` + CPU offloading ` _cpu_grad ` always memory owns gradient storage so will never require gradient writeback uses_sharded_strategy _offload_params Explicitly continue handle case ` no_sync ` where ` param grad ` view into GPU gradient referenced ` flat_param grad ` while ` flat_param_grad ` ` flat_param _cpu_grad ` which CPU continue needs_grad_writeback = flat_param_grad None _same_storage param grad flat_param_grad needs_grad_writeback flat_param_grad None flat_param_grad = torch zeros_like flat_param expected_shape = torch Size numel_in_shard src = param grad uses_sharded_strategy param grad view - _writeback_tensor src flat_param_grad i expected_shape offset_in_shard False flat_param grad = flat_param_grad flat_param_grad = flat_param grad TODO If we want handle shared parameters we need re-generate shared parameter data structures case sharedness changed param_name module _ prim_param_name prim_module _ flat_param _shared_param_infos getattr module param_name getattr prim_module prim_param_name raise NotImplementedError Changing shared parameters supported yet wroteback _writeback_tensor src_tensor Optional Tensor dst_tensor Tensor tensor_index int expected_shape torch Size offset int is_param bool gradient - None Write back ` ` src_tensor ` ` ` ` dst_tensor ` ` offset ` ` offset ` ` where ` ` src_tensor ` ` should have shape ` ` expected_shape ` ` ` ` is_param ` ` indicates tensor parameter ` ` True ` ` gradient ` ` False ` ` If ` ` src_tensor ` ` ` ` None ` ` then effect zeroing instead copying ` ` tensor_index ` ` gives index ` ` src_tensor ` ` metadata structures Raises RuntimeError If ` ` src_tensor ` ` does have expected shape _p_assert len expected_shape == f Expects D expected shape got expected_shape _debug_level == dist DebugLevel INFO rank = rank hasattr rank dist get_rank src_shape = src_tensor shape src_tensor None None src_device = src_tensor device src_tensor None None warnings warn f Rank rank Parameter is_param Gradient needs f writeback _training_state \n f expected shape= expected_shape shape= src_shape f expected device= dst_tensor device device= src_device stacklevel= src_tensor None src_tensor shape = expected_shape NOTE Gradient shape mismatch possible practice since gradient shape enforced match parameter we already check parameter shape mismatch raise RuntimeError f Cannot writeback when parameter is_param gradient f shape changes\nExpects expected_shape got src_tensor shape src_tensor None dst_tensor offset offset + expected_shape numel copy_ src_tensor dst_tensor offset offset + expected_shape numel zero_ flat_param _is_grad_none_mask None raise AssertionError Expected _is_grad_none_mask None flat_param _is_grad_none_mask tensor_index = True _reset_flat_param_grad_info_if_needed Reset ` ` flat_param grad ` ` needed When ` ` use_orig_params=True ` ` sets underlying ` ` flat_param grad ` ` ` ` None ` ` all original parameters ` ` grad ` ` ` ` None ` ` sets ` ` flat_param requires_grad=False ` ` none original parameters require gradient For targeting ` ` optim zero_grad set_to_none=True ` ` which case we want free gradients soon after ` ` zero_grad ` ` call possible _use_orig_params flat_param = flat_param flat_param _params None raise AssertionError Expected _params None mypy all_grad_none = True requires_grad = False param flat_param _params all_grad_none = param grad None requires_grad &#124; = param requires_grad all_grad_none flat_param grad = None As long one parameter requires gradient then flat parameter must require gradient flat_param requires_grad = requires_grad _deregister_orig_params param_info flat_param _param_infos param_name module _ = param_info hasattr module param_name delattr module param_name param_name module _ _ _ _ flat_param _shared_param_infos hasattr module param_name delattr module param_name ########### HELPERS ########### flat_param_to args kwargs Wrap in-place call ` ` ` ` ` ` flat_param ` ` pyrefly ignore not-iterable flat_param data = flat_param args kwargs _use_orig_params Refresh views because their storage may have changed is_sharded flat_param _use_sharded_views _use_unsharded_views as_params=True _get_modules - set nn Module Return ` set ` modules whose parameters included handle s flat parameter pi module pi flat_param _param_infos union spi module spi flat_param _shared_param_infos is_sharded tensor Tensor - bool Return whether ` ` tensor ` ` currently sharded For ` ` NO_SHARD ` ` we choose have always ` ` False ` ` clarity hasattr flat_param _sharded_size uses_sharded_strategy ` _sharded_size ` defined iff ` handle shard ` has been called False sharded_size = flat_param _sharded_size type ignore attr-defined tensor size == sharded_size param_module_names - Iterator tuple str str shared_param_infos = ParamInfo param_name module module_name param_name module module_name _ _ _ flat_param _shared_param_infos param_info chain flat_param _param_infos shared_param_infos param_name _ module_name = param_info type ignore misc yield param_name module_name shared_param_module_names - Iterator tuple str str param_name _ module_name ParamInfo param_name module module_name param_name module module_name _ _ _ flat_param _shared_param_infos yield param_name module_name property _fqns_in_shard - list str Return FQNs parameters present rank s shard fqns_in_shard list str = fqn shard_param_info zip flat_param _fqns flat_param _shard_param_infos type ignore attr-defined shard_param_info in_shard fqns_in_shard append fqn fqns_in_shard property sharded_grad - Optional Tensor Return handle s sharded gradient flat_param = flat_param Priority non- ` None ` ` _cpu_grad ` ` _saved_grad_shard ` ` grad ` - CPU offloading ` _cpu_grad ` - No CPU offloading + sharded strategies ` _saved_grad_shard ` - No CPU offloading + ` NO_SHARD ` ` grad ` grad Optional Tensor hasattr flat_param _cpu_grad grad = flat_param _cpu_grad type ignore attr-defined hasattr flat_param _saved_grad_shard In post-backward hook sharded gradient still ` _saved_grad_shard ` grad = flat_param _saved_grad_shard type ignore attr-defined If IDLE FORWARD states then there may accumulated gradient If accessed IDLE then should due re-registering original parameters e g state dict load _p_assert flat_param grad None uses_sharded_strategy _training_state HandleTrainingState FORWARD HandleTrainingState IDLE Sharded strategies should use ` _cpu_grad ` ` _saved_grad_shard ` unless IDLE FORWARD grad = flat_param grad grad _reset_is_grad_none - None Reset ` ` _is_grad_none_mask ` ` needed This method should only called post-backward after gradient computation which case parameter requires gradient then will surely receive gradient we may reset its mask entry ` ` False ` ` _use_orig_params _p_assert _training_state == HandleTrainingState BACKWARD_POST Expects only called post-backward after gradient computation flat_param = flat_param flat_param _params None raise AssertionError Expected _params None mypy i param enumerate flat_param _params type ignore arg-type As long parameter requires gradient should receive meaningful gradient even gradient happens zeros param requires_grad flat_param _is_grad_none_mask None raise AssertionError Expected _is_grad_none_mask None mypy flat_param _is_grad_none_mask i = False ####################### CHECKS INVARIANTS ####################### _check_sharded_strategy _p_assert uses_sharded_strategy Expects sharded strategy _check_on_compute_device tensor Tensor _p_assert tensor device == device f Expects tensor compute device device tensor device _check_on_cpu tensor Tensor _p_assert tensor device == torch device cpu f Expects tensor CPU got tensor device staticmethod _check_storage_freed tensor Tensor Compile does resize during trace torch distributed _functional_collectives is_torchdynamo_compiling _p_assert _same_storage_size tensor Expects storage freed got storage size staticmethod _check_storage_allocated tensor Tensor _p_assert _storage_size_allocated tensor Expects storage allocated _check_low_precision_shard _p_assert _uses_param_mixed_precision Not using low precision parameters _p_assert getattr flat_param _mp_shard None None Expects ` _mp_shard ` exist device = flat_param _mp_shard device type ignore attr-defined _p_assert device == device f Expects low precision shard device got device _check_unsharded tensor Tensor msg_prefix = Expects tensor unsharded _p_assert tensor None msg_prefix + got ` None ` unsharded_size = flat_param _unpadded_unsharded_size _p_assert tensor size == unsharded_size msg_prefix + f size unsharded_size got tensor size _check_sharded tensor Tensor msg_prefix = Expects tensor sharded _p_assert tensor None msg_prefix + got ` None ` sharded_size = flat_param _sharded_size type ignore attr-defined _p_assert tensor size == sharded_size msg_prefix + f size sharded_size got tensor size ############## PROPERTIES ############## property uses_sharded_strategy - bool _sharding_strategy = HandleShardingStrategy NO_SHARD property _uses_param_mixed_precision - bool _fwd_bwd_param_dtype = _orig_param_dtype property _uses_reduce_mixed_precision - bool _reduce_dtype = _orig_param_dtype property _force_full_precision - bool _uses_param_mixed_precision _uses_reduce_mixed_precision _training_state == HandleTrainingState SUMMON_FULL_PARAMS Also disable mixed precision model eval mode configured _fully_sharded_module training _use_full_prec_in_eval property _skipped_use_sharded_views - bool This property used sharding strategies do free after forward ` ` use_orig_params=True ` ` This returns handle currently state where has skipped using sharded views which case can restore view invariants via ` ` _use_sharded_views ` ` _unsharded_flat_param_for_skipped_views None NOTE These hacks bypass ` nn Module __setattr__ ` checks _unsafe_setattr_param module nn Module param_name str param nn Parameter - None module _parameters param_name = param This bypasses any overrides case ` module ` instance ` nn Module ` subclass super nn Module module __setattr__ param_name param _unsafe_setattr_tensor module nn Module param_name str tensor Tensor - None module _parameters pop param_name None This bypasses any overrides case ` module ` instance ` nn Module ` subclass super nn Module module __setattr__ param_name tensor _safe_setattr_tensor_or_param module nn Module param_name str tensor_or_param Union Tensor nn Parameter Call ` delattr ` ` setattr ` go through ` nn Module ` checks hasattr module param_name delattr module param_name setattr module param_name tensor_or_param _convert_to_params tensors list Union torch Tensor nn Parameter - list nn Parameter t isinstance t nn Parameter nn Parameter t t tensors _is_truly_contiguous x Tensor - bool Special case Pytorch thinks x channels_last convolution weights both contiguous channels_last contiguous same time CuDNN does agree though refuses select faster kernels It reason having extra check here x stride - == x is_contiguous _detach_if_needed param_or_tensor Union nn Parameter Tensor - Tensor param_or_tensor detach isinstance param_or_tensor nn Parameter param_or_tensor _get_aligned_numel unsharded_dtype torch dtype NOTE This alignment constraint comes TorchInductor ALIGNMENT = bytes unsharded_dtype_size = _get_dtype_size unsharded_dtype aligned_numel = ALIGNMENT unsharded_dtype_size aligned_numel functools lru_cache _get_dtype_size dtype torch empty dtype=dtype element_size _construct_padding_tensor padding_numel int dtype torch dtype requires_grad bool device torch device NOTE Set padding value magic number debuggability The value itself should never used any user-facing computation torch ones padding_numel dtype=dtype requires_grad=requires_grad device=device _FLAT_PARAM_PADDING_VALUE Use ` lru_cache ` only log warning once assuming fixed warning message passed functools lru_cache _warn_skip_writeback_check log logging Logger warning str logger warning warning Use ` lru_cache ` only log warning once functools lru_cache _warn_use_fake_all_gather log logging Logger warning str logger warning warning Use ` lru_cache ` only log warning once functools lru_cache _warn_use_fake_reduce log logging Logger warning str logger warning warning _same_storage b Params DTensors backward SHARD_GRAD_OP + TP torch distributed tensor DTensor isinstance DTensor = _local_tensor isinstance b DTensor b = b _local_tensor untyped_storage data_ptr == b untyped_storage data_ptr _same_storage_size torch Tensor b int untyped_storage size element_size == b _storage_size_allocated tensor Tensor storage_size int = tensor untyped_storage size storage_size