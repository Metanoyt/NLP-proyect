mypy allow-untyped-defs mypy disable-error-code=arg-type This file exports ONNX ops opset Note ONNX operators added updated opset ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ New operators HardSwish Trilu Updated operators Reshape Add Sub Mul Div GRU LSTM RNN BatchNorm Cumsum Relu EDITING THIS FILE READ THIS FIRST see Note Edit Symbolic Files README md __future__ annotations functools torch torch onnx _constants torch onnx _internal torchscript_exporter _type_utils jit_utils registration symbolic_helper torch onnx _internal torchscript_exporter _globals GLOBALS __all__ = hardswish tril triu reshape batch_norm quantized_hardswish scaled_dot_product_attention _onnx_symbolic = functools partial registration onnx_symbolic opset= _onnx_symbolic aten hardswish symbolic_helper parse_args v hardswish g jit_utils GraphContext g op HardSwish _onnx_symbolic aten tril tril g jit_utils GraphContext diagonal out=None g op Trilu diagonal upper_i= _onnx_symbolic aten triu triu g jit_utils GraphContext diagonal out=None g op Trilu diagonal upper_i= _onnx_symbolic aten reshape symbolic_helper quantized_args True symbolic_helper parse_args v v reshape g jit_utils GraphContext shape NOTE Due bug ORT https github com microsoft onnxruntime issues Reshape export cannot utilize new allowzero attribute introduced opset symbolic_helper _reshape_helper g shape allowzero= _onnx_symbolic aten batch_norm symbolic_helper parse_args v v v v v i f f i batch_norm g jit_utils GraphContext input weight bias running_mean running_var training momentum eps cudnn_enabled torch is_autocast_enabled symbolic_helper args_have_same_dtype input weight bias running_mean running_var GLOBALS export_onnx_opset_version symbolic_helper _onnx_opset_unsupported_detailed BatchNormalization All input tensors must have same ` dtype ` Turn off Autocast export using opset version input symbolic_helper check_training_mode training batch_norm weight bias running_mean running_var = symbolic_helper _batchnorm_helper g input weight bias running_mean running_var out = g op BatchNormalization input weight bias running_mean running_var epsilon_f=eps momentum_f= - momentum training_mode_i= training outputs= training training out res new_running_mean new_running_var = out new_running_mean setType running_mean type new_running_var setType running_var type res _onnx_symbolic quantized hardswish quantized_hardswish g jit_utils GraphContext x op_scale op_zero_point x _ _ _ = symbolic_helper dequantize_helper g x output = hardswish g x symbolic_helper quantize_helper g output op_scale op_zero_point Ported https github com microsoft onnxscript blob b b b f d c d e d ef d b onnxscript function_libs torch_aten ops nn py#L aten_scaled_dot_product_attention NOTE Need op Trilu _onnx_symbolic aten scaled_dot_product_attention symbolic_helper parse_args v v v v f b v b scaled_dot_product_attention g jit_utils GraphContext query torch _C Value key torch _C Value value torch _C Value attn_mask torch _C Value &#124; None = None dropout_p float = is_causal bool = False scale torch _C Value &#124; None = None enable_gqa bool = False assert is_causal is_causal symbolic_helper _is_none attn_mask is_causal attn_mask cannot set same time assert enable_gqa conversion scaled_dot_product_attention implemented enable_gqa True symbolic_helper _is_none scale scale = _attention_scale g query is_causal attn_mask = _causal_attention_mask g query key Swap last two axes key NOTE onnx-script has different logic here because attribute perms transpose needs list ints key_shape_builtin = symbolic_helper _get_tensor_rank key pyrefly ignore no-matching-overload key_transposed_axes = list range key_shape_builtin key_transposed_axes - key_transposed_axes - = key_transposed_axes - key_transposed_axes - key_transposed = g op Transpose key perm_i=key_transposed_axes https github com pytorch pytorch blob da c b c fda bce aten src ATen native transformers attention cpp#L Scale q k before matmul stability see https tinyurl com sudb s math pyrefly ignore bad-argument-type query_scaled = g op Mul query g op Sqrt scale pyrefly ignore bad-argument-type key_transposed_scaled = g op Mul key_transposed g op Sqrt scale mul_qk = g op MatMul query_scaled key_transposed_scaled symbolic_helper _is_none attn_mask mul_qk_add = mul_qk attn_weight = g op Softmax mul_qk_add axis_i=- _type_utils JitScalarType from_value attn_mask == _type_utils JitScalarType BOOL Turn Boolean mask float attn_mask masked_fill attn_mask -float inf const_zero = g op Constant value_t=torch tensor const_neg_inf = g op Constant value_t=torch tensor -float inf pyrefly ignore bad-argument-type attn_mask = g op Where attn_mask const_zero const_neg_inf mul_qk_add = g op Add mul_qk attn_mask attn_weight = g op Softmax mul_qk_add axis_i=- When using scaled dot product attention boolean mask softmax operation might NaN values due presence -inf entire row padding tokens resulting NaN softmax output This because there s no safe softmax imp ONNX so we need handle NaN values explicitly match behavior PyTorch boolean masks attn_weight = g op Where g op IsNaN attn_weight const_zero attn_weight _type_utils JitScalarType from_value attn_mask _type_utils JitScalarType FLOAT _type_utils JitScalarType HALF _type_utils JitScalarType BFLOAT pyrefly ignore bad-argument-type mul_qk_add = g op Add mul_qk attn_mask attn_weight = g op Softmax mul_qk_add axis_i=- raise ValueError f Unsupported type attn_mask _type_utils JitScalarType from_value attn_mask dropout_p = attn_weight = g op Dropout attn_weight g op Constant value_t=torch tensor dropout_p dtype=torch float g op MatMul attn_weight value _attention_scale g jit_utils GraphContext query torch _C Value - torch _C Value Calculate scale factor attention result Args query Tensor shape L E Returns Scalar scale factor = math sqrt query size - query_shape = g op Shape query query_shape_last = g op Slice query_shape g op Constant value_t=torch tensor - dtype=torch int g op Constant value_t=torch tensor _constants INT _MAX dtype=torch int embedding_size = g op Cast query_shape_last to_i=_type_utils JitScalarType from_value query onnx_type const_one = g op Constant value_t=torch tensor dtype=torch float scale = g op Div const_one g op Sqrt embedding_size Add Cast convert scale back original type scale = g op Cast scale to_i=_type_utils JitScalarType from_value query onnx_type scale _causal_attention_mask g jit_utils GraphContext query torch _C Value key torch _C Value - torch _C Value Create causal mask given query key tensors Equivalent mask = torch ones L S dtype=torch bool tril diagonal= attn_mask = torch zeros L S dtype=torch float attn_mask = attn_mask masked_fill mask -float inf Args query Tensor shape L E key Tensor shape S E Returns Tensor shape L S query_shape = g op Shape query key_shape = g op Shape key last_idx = g op Constant value_t=torch tensor - dtype=torch int second_last_idx = g op Constant value_t=torch tensor - dtype=torch int target_length = g op Slice query_shape second_last_idx last_idx source_length = g op Slice key_shape second_last_idx last_idx attn_mask = torch ones L S = size = g op Concat target_length source_length axis_i= const_one = g op Constant value_t=torch tensor attn_mask = g op Expand const_one size attn_mask = g op Trilu attn_mask upper_i= The causal mask has s lower triangle -inf upper triangle const_zero = g op Constant value_t=torch tensor const_neg_inf = g op Constant value_t=torch tensor -float inf attn_mask = g op Where g op Equal attn_mask const_zero const_neg_inf const_zero attn_mask