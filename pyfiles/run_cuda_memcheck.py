usr bin env python This script runs cuda-memcheck specified unit test Each test case run its isolated process timeout so different test cases won t influence each other case hang script would still finish finite amount time The output will written log file result log Example usage python run_cuda_memcheck py test_torch py Note running cuda-memcheck could very slow argparse asyncio multiprocessing os subprocess sys cuda_memcheck_common cmc tqdm torch ALL_TESTS = GPUS = torch cuda device_count parse arguments parser = argparse ArgumentParser description= Run isolated cuda-memcheck unit tests parser add_argument filename help= python file test such test_torch py parser add_argument timeout type=int help= kill test does terminate certain amount seconds parser add_argument -- strict action= store_true help= Whether show cublas cudnn errors These errors ignored default because cublas cudnn does run error-free under cuda-memcheck ignoring these errors parser add_argument -- nproc type=int default=multiprocessing cpu_count help= Number processes running tests default number cores system parser add_argument -- gpus default= all help= GPU assignments each process could all separated list like parser add_argument -- ci action= store_true help= Whether script executed CI When executed inside CI script fails when error detected Also will show tqdm progress bar directly print error stdout instead parser add_argument -- nohang action= store_true help= Treat timeout success parser add_argument -- split type=int default= help= Split job into pieces parser add_argument -- rank type=int default= help= Which piece process should pick args = parser parse_args Filters ignores cublas cudnn errors TODO zasdfgbnm When can we remove Will cublas cudnn run error-free under cuda-memcheck is_ignored_only output try report = cmc parse output except cmc ParseError case simple parser fails parsing output cuda memcheck then error never ignored False count_ignored_errors = e report errors libcublas join e stack libcudnn join e stack libcufft join e stack count_ignored_errors += count_ignored_errors == report num_errors Set environment PYTORCH_CUDA_MEMCHECK= allow skipping some tests os environ PYTORCH_CUDA_MEMCHECK = Discover tests To get list tests run pytest -- setup-only test test_torch py then parse output proc = subprocess Popen pytest -- setup-only args filename stdout=subprocess PIPE stderr=subprocess PIPE stdout stderr = proc communicate lines = stdout decode strip splitlines line lines fixtures used line line = line strip split line = line line find + line = line replace ALL_TESTS append line Do simple filtering cpu CPU name cuda CUDA name then skip is_cpu_only name name = name lower cpu name cuda name ALL_TESTS = x x ALL_TESTS is_cpu_only x Split all tests into chunks only selected chunk ALL_TESTS sort chunk_size = len ALL_TESTS + args split - args split start = chunk_size args rank end = chunk_size args rank + ALL_TESTS = ALL_TESTS start end Run tests Since running cuda-memcheck PyTorch unit tests very slow these tests must run parallel This done using coroutine feature new Python versions A number coroutines created they create subprocesses awaiting them finish The number running subprocesses could specified user default same number CPUs machine These subprocesses balanced across different GPUs system assigning one devices per process specified user progress = args ci logfile = open result log w progressbar = tqdm tqdm total=len ALL_TESTS logfile = sys stdout create fake progress bar does display anything ProgressbarStub update args progressbar = ProgressbarStub async run coroutine_id global progress args gpus == all gpuid = coroutine_id GPUS gpu_assignments = args gpus split assert args nproc == len gpu_assignments Please specify GPU assignment each process separated gpuid = gpu_assignments coroutine_id while progress len ALL_TESTS test = ALL_TESTS progress progress += cmd = f CUDA_VISIBLE_DEVICES= gpuid cuda-memcheck -- error-exitcode python args filename test proc = await asyncio create_subprocess_shell cmd stdout=asyncio subprocess PIPE stderr=asyncio subprocess PIPE try stdout stderr = await asyncio wait_for proc communicate args timeout except asyncio TimeoutError print Timeout test file=logfile proc kill args ci args nohang sys exit Hang detected cuda-memcheck proc returncode == print Success test file=logfile stdout = stdout decode stderr = stderr decode should_display = args strict is_ignored_only stdout should_display print Fail test file=logfile print stdout file=logfile print stderr file=logfile args ci sys exit Failure detected cuda-memcheck print Ignored test file=logfile del proc progressbar update async main tasks = asyncio ensure_future run i i range args nproc t tasks await t __name__ == __main__ loop = asyncio get_event_loop loop run_until_complete main