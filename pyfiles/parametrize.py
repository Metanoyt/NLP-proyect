mypy allow-untyped-decorators mypy allow-untyped-defs collections copyreg collections abc Sequence contextlib contextmanager copy deepcopy typing Optional Union torch torch Tensor torch __future__ get_swap_module_params_on_conversion torch nn modules container Module ModuleDict ModuleList torch nn parameter Parameter torch utils _python_dispatch is_traceable_wrapper_subclass __all__ = cached ParametrizationList register_parametrization is_parametrized remove_parametrizations type_before_parametrizations transfer_parametrizations_and_params _cache_enabled = _cache dict tuple int str Optional Tensor = contextmanager cached r Context manager enables caching system within parametrizations registered func ` register_parametrization ` The value parametrized objects computed cached first time they required when context manager active The cached values discarded when leaving context manager This useful when using parametrized parameter more than once forward pass An example when parametrizing recurrent kernel RNN when sharing weights The simplest way activate cache wrapping forward pass neural network code-block python torch nn utils parametrize P P cached output = model inputs training evaluation One may also wrap parts modules use several times parametrized tensors For example loop RNN parametrized recurrent kernel code-block python P cached x xs out_rnn = rnn_cell x out_rnn global _cache global _cache_enabled _cache_enabled += try yield finally _cache_enabled -= _cache_enabled _cache = _register_parameter_or_buffer module name X isinstance X Parameter module register_parameter name X module register_buffer name X _maybe_set dest Tensor src Tensor - None should_swap = get_swap_module_params_on_conversion is_traceable_wrapper_subclass dest should_swap isinstance dest Parameter isinstance src Parameter src = Parameter src requires_grad=dest requires_grad torch utils swap_tensors dest src dest set_ src type ignore call-overload ParametrizationList ModuleList r A sequential container holds manages original parameters buffers parametrized ` torch nn Module ` It type ` ` module parametrizations tensor_name ` ` when ` ` module tensor_name ` ` has been parametrized func ` register_parametrization ` If first registered parametrization has ` ` right_inverse ` ` returns one tensor does have ` ` right_inverse ` ` which case we assume ` ` right_inverse ` ` identity will hold tensor under name ` ` original ` ` If has ` ` right_inverse ` ` returns more than one tensor these will registered ` ` original ` ` ` ` original ` ` warning This used internally func ` register_parametrization ` It documented here completeness It shall instantiated user Args modules sequence sequence modules representing parametrizations original Parameter Tensor parameter buffer parametrized unsafe bool boolean flag denotes whether parametrization may change dtype shape tensor Default ` False ` Warning parametrization checked consistency upon registration Enable flag your own risk original Tensor unsafe bool __init__ modules Sequence Module original Union Tensor Parameter unsafe bool = False - None We require because we need treat differently first parametrization This should never throw unless used outside len modules == raise ValueError ParametrizationList requires one more modules super __init__ modules unsafe = unsafe In plain words module weight must keep its dtype shape Furthermore there no right_inverse right_inverse returns tensor should same dtype original tensor We check following invariants hold X = module weight Y = param right_inverse X assert isinstance Y Tensor isinstance Y collections abc Sequence all isinstance t Tensor t Y Z = param Y isinstance Y Tensor param Y Consistency checks assert X dtype == Z dtype X shape == Z shape If has one input allows able use set_ able move data original tensor without changing its id which what optimizer uses track parameters isinstance Y Tensor assert X dtype == Y dtype Below we use original = X new = Y original_shape = original shape original_dtype = original dtype Compute new torch no_grad new = original module reversed type ignore call-overload hasattr module right_inverse try new = module right_inverse new type ignore operator except NotImplementedError pass throws we assume right_inverse identity isinstance new Tensor isinstance new Sequence raise ValueError right_inverse must Tensor Sequence tensors list tuple f Got type new __name__ Set number original tensors is_tensor = isinstance new Tensor ntensors = is_tensor len new Register tensor s is_tensor pyrefly ignore missing-attribute original dtype = new dtype raise ValueError When ` right_inverse ` outputs one tensor may change dtype \n f original dtype original dtype \n pyrefly ignore missing-attribute f right_inverse original dtype new dtype pyrefly ignore missing-attribute original device = new device raise ValueError When ` right_inverse ` outputs one tensor may change device \n f original device original device \n pyrefly ignore missing-attribute f right_inverse original device new device Set original original so user does need re-register parameter manually optimiser torch no_grad pyrefly ignore bad-argument-type _maybe_set original new _register_parameter_or_buffer original original i originali enumerate new isinstance originali Tensor raise ValueError right_inverse must Tensor Sequence tensors list tuple f Got element i sequence type type originali __name__ If original tensor Parameter required grad we expect user add new parameters optimizer after registering parametrization documented isinstance original Parameter originali = Parameter originali original requires_grad originali requires_grad_ original requires_grad _register_parameter_or_buffer f original i originali unsafe Consistency checks Since f A - B right_inverse B - A Z original should live B Z = forward right_inverse original Z = isinstance Z Tensor raise ValueError f A parametrization must tensor Got type Z __name__ Z dtype = original_dtype raise ValueError Registering parametrization may change dtype tensor unless ` unsafe ` flag enabled \n f unparametrized dtype original_dtype \n f parametrized dtype Z dtype Z shape = original_shape raise ValueError Registering parametrization may change shape tensor unless ` unsafe ` flag enabled \n f unparametrized shape original_shape \n f parametrized shape Z shape right_inverse value Tensor - None r Call ` ` right_inverse ` ` methods parametrizations inverse registration order Then stores result ` ` original ` ` ` ` right_inverse ` ` outputs one tensor ` ` original ` ` ` ` original ` ` outputs several Args value Tensor Value which initialize module All exceptions function should almost never throw They could throw example right_inverse function returns different dtype when given different input which should most likely caused bug user s code torch no_grad See https github com pytorch pytorch issues module reversed type ignore call-overload hasattr module right_inverse value = module right_inverse value type ignore operator raise RuntimeError f parametrization type module __name__ does implement right_inverse is_tensor These exceptions should only throw when right_inverse function does same dtype every input which should most likely caused bug isinstance value Tensor raise ValueError f ` right_inverse ` should tensor Got type value __name__ value dtype = original dtype raise ValueError f The tensor returned ` right_inverse ` has dtype value dtype f while ` original ` has dtype original dtype We know result going have same dtype _maybe_set original value isinstance value collections abc Sequence raise ValueError right_inverse must sequence tensors f Got type value __name__ len value = ntensors raise ValueError right_inverse must sequence tensors length f ntensors Got sequence length len value i tensor enumerate value original_i = getattr f original i isinstance tensor Tensor raise ValueError f ` right_inverse ` must sequence tensors f Got element i type type tensor __name__ original_i dtype = tensor dtype raise ValueError f Tensor i returned ` right_inverse ` has dtype tensor dtype f while ` original i ` has dtype original_i dtype _maybe_set original_i tensor forward - Tensor torch jit is_scripting raise RuntimeError Parametrization working scripting Unpack originals first parametrization is_tensor x = original originals = getattr f original i i range ntensors x = originals It s possible call here so we have bit more cryptic Also we want skip all non-integer keys curr_idx = while hasattr str curr_idx x = curr_idx x curr_idx += x _inject_new_class module Module - None r Set up module parametrized This works substituting module extends able inject property Args module nn Module module into which inject property cls = module __class__ default_deepcopy memo Just emulate standard deepcopy procedure when __deepcopy__ doesn t exist current obj = memo get id None obj None obj replica = __new__ __class__ memo id = replica replica __dict__ = deepcopy __dict__ memo Also save all slots they exist slots_to_save = copyreg _slotnames __class__ type ignore attr-defined slot slots_to_save hasattr slot setattr replica slot deepcopy getattr slot memo replica getstate raise RuntimeError Serialization parametrized modules only supported through state_dict See \n https pytorch org tutorials beginner saving_loading_models html #saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training dct = __getstate__ getstate We don t allow serialization parametrized modules should still allow deepcopying Default deepcopy function invokes __deepcopy__ method instead __getstate__ when exists hasattr cls __deepcopy__ dct __deepcopy__ = default_deepcopy type ignore assignment param_cls = type f Parametrized cls __name__ cls dct module __class__ = param_cls _inject_property module Module tensor_name str - None r Injects property into module tensor_name It assumes module has already been modified its original one using _inject_new_class tensor under attr ` tensor_name ` has already been moved out Args module nn Module module into which inject property tensor_name str name name property create We check precondition This should never fire register_parametrization correctly implemented assert hasattr module tensor_name torch jit unused get_cached_parametrization parametrization - Tensor global _cache key = id module tensor_name tensor = _cache get key tensor None tensor = parametrization _cache key = tensor tensor get_parametrized - Tensor torch jit is_scripting raise RuntimeError Parametrization working scripting parametrization = parametrizations tensor_name pyrefly ignore redundant-condition _cache_enabled torch jit is_scripting Scripting raise RuntimeError Caching implemented scripting Either disable caching avoid scripting torch _C _get_tracing_state None Tracing raise RuntimeError Cannot trace model while caching parametrizations get_cached_parametrization parametrization If caching active function just evaluates parametrization parametrization set_original value Tensor - None torch jit is_scripting raise RuntimeError Parametrization working scripting parametrizations tensor_name right_inverse value setattr module __class__ tensor_name property get_parametrized set_original register_parametrization module Module tensor_name str parametrization Module unsafe bool = False - Module r Register parametrization tensor module Assume ` ` tensor_name= weight ` ` simplicity When accessing ` ` module weight ` ` module will parametrized version ` ` parametrization module weight ` ` If original tensor requires gradient backward pass will differentiate through attr ` parametrization ` optimizer will update tensor accordingly The first time module registers parametrization function will add attribute ` ` parametrizations ` ` module type ` ~ParametrizationList ` The list parametrizations tensor ` ` weight ` ` will accessible under ` ` module parametrizations weight ` ` The original tensor will accessible under ` ` module parametrizations weight original ` ` Parametrizations may concatenated registering several parametrizations same attribute The training mode registered parametrization updated registration match training mode host module Parametrized parameters buffers have inbuilt caching system can activated using context manager func ` cached ` A attr ` parametrization ` may optionally implement method signature code-block python right_inverse X Tensor - Union Tensor Sequence Tensor This method called unparametrized tensor when first parametrization registered compute initial value original tensor If method implemented original tensor will just unparametrized tensor If all parametrizations registered tensor implement ` right_inverse ` possible initialize parametrized tensor assigning shown example below It possible first parametrization depend several inputs This may implemented returning tuple tensors ` ` right_inverse ` ` see example implementation ` ` RankOne ` ` parametrization below In case unconstrained tensors also located under ` ` module parametrizations weight ` ` names ` ` original ` ` ` ` original ` ` note If unsafe=False default both forward right_inverse methods will called once perform number consistency checks If unsafe=True then right_inverse will called tensor parametrized nothing will called otherwise note In most situations ` ` right_inverse ` ` will function such ` ` forward right_inverse X == X ` ` see ` right inverse https en wikipedia org wiki Inverse_function#Right_inverses ` _ Sometimes when parametrization surjective may reasonable relax warning If parametrization depends several inputs func ` ~register_parametrization ` will register number new parameters If such parametrization registered after optimizer created these new parameters will need added manually optimizer See meth ` torch Optimizer add_param_group ` Args module nn Module module which register parametrization tensor_name str name parameter buffer which register parametrization parametrization nn Module parametrization register Keyword args unsafe bool boolean flag denotes whether parametrization may change dtype shape tensor Default ` False ` Warning parametrization checked consistency upon registration Enable flag your own risk Raises ValueError module does have parameter buffer named attr ` tensor_name ` Examples xdoctest +REQUIRES env TORCH_DOCTEST_LAPACK torch torch nn nn torch nn utils parametrize P Symmetric nn Module forward X X triu + X triu T Return symmetric matrix right_inverse A A triu m = nn Linear P register_parametrization m weight Symmetric print torch allclose m weight m weight T m weight now symmetric True A = torch rand A = A + A T A now symmetric m weight = A Initialize weight symmetric matrix A print torch allclose m weight A True RankOne nn Module forward x y Form rank matrix multiplying two vectors x unsqueeze - y unsqueeze - right_inverse Z Project Z onto rank matrices U S Vh = torch linalg svd Z full_matrices=False Return rescaled singular vectors s _sqrt = S sqrt unsqueeze - U s _sqrt Vh s _sqrt linear_rank_one = P register_parametrization nn Linear weight RankOne print torch linalg matrix_rank linear_rank_one weight item parametrization train module training is_parametrized module tensor_name Correctness checks If A space tensors shape dtype equal module weight we check parametrization forward parametrization right_inverse functions A A unsafe Y = getattr module tensor_name X = parametrization Y isinstance X Tensor raise ValueError f A parametrization must tensor Got type X __name__ X dtype = Y dtype raise ValueError Registering parametrization may change dtype tensor unless ` unsafe ` flag enabled \n f module tensor_name dtype Y dtype \n f parametrization module tensor_name dtype X dtype X shape = Y shape raise ValueError Registering parametrization may change shape tensor unless ` unsafe ` flag enabled \n f module tensor_name shape Y shape \n f parametrization module tensor_name shape X shape hasattr parametrization right_inverse try Z = parametrization right_inverse X type ignore operator except NotImplementedError pass isinstance Z Tensor raise ValueError f parametrization right_inverse must tensor Got type Z __name__ Z dtype = Y dtype raise ValueError The tensor returned parametrization right_inverse must have same dtype f module tensor_name unless ` unsafe ` flag enabled \n f module tensor_name dtype Y dtype \n f returned dtype Z dtype Z shape = Y shape raise ValueError The tensor returned parametrization right_inverse must have same shape f module tensor_name unless ` unsafe ` flag enabled \n f module tensor_name shape Y shape \n f returned shape Z shape right_inverse assumed identity add new parametrization parametrization list assert isinstance module parametrizations ModuleDict Make mypy happy module parametrizations tensor_name append parametrization type ignore operator If unsafe True previous parametrization keep enabled module parametrizations tensor_name unsafe &#124; = unsafe type ignore index union-attr operator tensor_name module _buffers tensor_name module _parameters Set parametrization mechanism Fetch original buffer parameter original = getattr module tensor_name We create early check possible errors parametrizations = ParametrizationList parametrization original unsafe=unsafe Delete previous parameter buffer delattr module tensor_name If first parametrization registered module we prepare module inject property is_parametrized module Change _inject_new_class module Inject ` ` ModuleDict ` ` into instance under module parametrizations module parametrizations = ModuleDict Add property into _inject_property module tensor_name Add ParametrizationList assert isinstance module parametrizations ModuleDict Make mypy happy module parametrizations tensor_name = parametrizations raise ValueError f Module module does have parameter buffer f parametrized element name tensor_name module is_parametrized module Module tensor_name Optional str = None - bool r Determine module has parametrization Args module nn Module module query tensor_name str optional name parameter module Default ` ` None ` ` Returns ` ` True ` ` attr ` module ` has parametrization parameter named attr ` tensor_name ` has any parametrization when attr ` tensor_name ` ` ` None ` ` otherwise ` ` False ` ` parametrizations = getattr module parametrizations None parametrizations None isinstance parametrizations ModuleDict False tensor_name None Check there least one parametrized buffer Parameter len parametrizations tensor_name parametrizations remove_parametrizations module Module tensor_name str leave_parametrized bool = True - Module r Remove parametrizations tensor module - If ` ` leave_parametrized=True ` ` ` ` module tensor_name ` ` will set its current output In case parametrization shall change ` ` dtype ` ` tensor - If ` ` leave_parametrized=False ` ` ` ` module tensor_name ` ` will set unparametrised tensor ` ` module parametrizations tensor_name original ` ` This only possible when parametrization depends just one tensor Args module nn Module module which remove parametrization tensor_name str name parametrization removed leave_parametrized bool optional leave attribute attr ` tensor_name ` parametrized Default ` ` True ` ` Returns Module module Raises ValueError ` ` module tensor_name ` ` parametrized ValueError ` ` leave_parametrized=False ` ` parametrization depends several tensors is_parametrized module tensor_name raise ValueError f Module module does have parametrization tensor_name Fetch original tensor assert isinstance module parametrizations ModuleDict Make mypy happy parametrizations = module parametrizations tensor_name pyrefly ignore invalid-argument parametrizations is_tensor original = parametrizations original assert isinstance original torch Tensor is_tensor promised us Tensor leave_parametrized torch no_grad t = getattr module tensor_name We know they have same dtype because we have checked when registering parametrizations As such we can use set_ We do so parameter does change id This way user does need update optimizer torch no_grad type original torch Tensor _maybe_set original t try _maybe_set original t except RuntimeError e TODO Fix tensor subclasses parameters RuntimeError set_storage allowed Tensor created data detach raise RuntimeError Calling remove_parametrizations leave_parametrized=True parameter instance tensor subclass requires set_ implemented correctly tensor subclass Alternatively one can opt into swap_tensors path Either set leave_parametrized=False provide working implementation set_ tensor subclass set torch __future__ set_swap_module_params_on_conversion True e leave_parametrized We cannot use no_grad because we need know whether one more original tensors required grad t = getattr module tensor_name We ll have trust user add optimizer original = Parameter t t requires_grad t raise ValueError Cannot leave unparametrized ` leave_parametrized=False ` tensor parametrized terms sequence tensors Delete property manages parametrization delattr module __class__ tensor_name Delete ParametrizationList del module parametrizations tensor_name Restore parameter buffer into main _register_parameter_or_buffer module tensor_name original Roll back parametrized no other buffer parameter currently parametrized is_parametrized module delattr module parametrizations Restore orig_cls = module __class__ __bases__ module __class__ = orig_cls module type_before_parametrizations module Module - type r Return module type before parametrizations applied then returns module type Args module nn Module module get type is_parametrized module module __class__ __bases__ type module transfer_parametrizations_and_params from_module Module to_module Module tensor_name Optional str = None - Module r Transfer parametrizations parameters they parametrize attr ` from_module ` attr ` to_module ` If attr ` tensor_name ` specified only transfers specified parameter otherwise transfers all parametrized parameters If those parameters do exist to_module will create them Does nothing from_module parametrized Args from_module nn Module module transfer to_module nn Module module transfer tensor_name str optional parameter transfer Returns Module to_module is_parametrized from_module assert isinstance from_module parametrizations ModuleDict mypy get list all params single param transfer parameters_to_transfer Union list ModuleDict = from_module parametrizations tensor_name None tensor_name assert hasattr parameters_to_transfer __iter__ mypy parameter_name parameters_to_transfer initialize to-be-transferred param to_module doesn t exist already hasattr to_module parameter_name setattr to_module parameter_name Parameter getattr from_module parameter_name apply params s parametrizations to_module param_func from_module parametrizations type ignore attr-defined parameter_name register_parametrization to_module parameter_name param_func assert isinstance to_module parametrizations ModuleDict mypy make values match original values can stored either original original original need check both cases hasattr from_module parametrizations parameter_name original to_module parametrizations parameter_name original = from_module parametrizations parameter_name original num = orig_num = original + str num loop through each original# until all values have been set while hasattr from_module parametrizations parameter_name orig_num setattr to_module parametrizations parameter_name orig_num getattr from_module parametrizations parameter_name orig_num num = num + orig_num = original + str num to_module