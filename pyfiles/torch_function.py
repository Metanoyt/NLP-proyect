TorchDynamo support __torch_function__ tensor subclasses This module implements support tensor subclasses __torch_function__ overrides A tensor subclass instance represented TensorWithTFOverrideVariable which handles dispatching __torch_function__ attribute accesses method calls torch API calls Unsupported features - Triggering __torch_function__ tensor subclass non-tensor custom attributes - Graph breaking mutating guardable tensor properties within __torch_function__ context can cause excessive recompiles certain cases - Matching exact eager behavior ignoring __torch_function__ objects non-tensor argument positions Torch API calls Supported features - Static method implementations __torch_function__ custom objects triggers torch API calls object any argument - Triggering __torch_function__ torch API calls tensor subclass arguments - __torch_function__ calls base tensor attribute access method calls tensor subclass instances - Matches dispatch ordering behavior eager __torch_function__ subclass object arguments any position See https docs google com document d WBxBSvW NXhRp ncmtokJloMLCtF AYNhJaffvHe Kw edit#heading=h vacn lozd w more information design collections contextlib functools inspect operator types TracebackType typing Any Generator Iterable Optional TYPE_CHECKING torch _C torch utils _pytree pytree torch _guards Source torch overrides _get_overloaded_args get_default_nowrap_functions TorchFunctionMode torch utils _device DeviceContext graph_break_hints exc unimplemented_v guards GuardBuilder install_guard polyfills NoEnterTorchFunctionMode source AttrSource GlobalSource TorchFunctionModeStackSource TypeSource utils class_has_getattribute clear_torch_function_mode_stack get_safe_global_name has_torch_function is_tensor_base_attr_getter set_torch_function_mode_stack base VariableTracker constant ConstantVariable ctx_manager GenericContextWrappingVariable functions UserMethodVariable lazy LazyVariableTracker lists TupleVariable tensor TensorSubclassVariable TensorVariable user_defined UserDefinedObjectVariable TYPE_CHECKING torch _dynamo codegen PyCodegen torch _dynamo symbolic_convert InstructionTranslator bin_ops = operator pow operator mul operator matmul operator floordiv operator truediv operator mod operator add operator lt operator gt operator ge operator le operator ne operator eq operator sub operator ipow operator imul operator imatmul operator ifloordiv operator itruediv operator imod operator iadd operator isub bin_int_ops = operator and_ operator or_ operator xor operator iand operator ixor operator ior un_int_ops = operator invert tensor_and_int_ops = operator lshift operator rshift operator ilshift operator irshift operator getitem un_ops = operator abs operator pos operator neg operator not_ Note has local scalar dense call operator length_hint banned_attrs = fn __self__ __name__ type ignore attr-defined fn get_default_nowrap_functions is_tensor_base_attr_getter fn functools cache get_prev_stack_var_name - str bytecode_transformation unique_id unique_id ___prev_torch_function_mode_stack TorchFunctionModeVariable GenericContextWrappingVariable staticmethod is_supported_torch_function_mode ty type TorchFunctionMode - bool Supported sense means we can support graph breaks under context We able trace custom modes there graph breaks under them they have custom __enter__ __exit__ we don t handle same reason we don t handle generic context managers there may side effects now affected executing function across two frames instead one Today we support enter exit default TorchFunctionMode well DeviceContext which used set_default_device issubclass ty NoEnterTorchFunctionMode DeviceContext class_has_getattribute ty inspect getattr_static ty __enter__ TorchFunctionMode __enter__ inspect getattr_static ty __exit__ TorchFunctionMode __exit__ __init__ value Optional TorchFunctionMode source Optional Source = None kwargs Any value None super __init__ value kwargs value = value cm_obj = value needed BC calling enter CM code source = source type ignore assignment reconstruct codegen PyCodegen - None This shouldn t called unless we have source assert source source reconstruct codegen module_name - str value __module__ fn_name - str type value __name__ python_type - type type value call_torch_function tx InstructionTranslator fn VariableTracker types TupleVariable args Iterable Any kwargs dict str Any - VariableTracker call_torch_function tx get_torch_function_fn tx type ignore arg-type fn types args kwargs enter tx InstructionTranslator - VariableTracker torch TorchInGraphFunctionVariable isinstance value NoEnterTorchFunctionMode ConstantVariable create None TorchInGraphFunctionVariable torch _C _push_on_torch_function_stack call_function tx ConstantVariable create None exit tx InstructionTranslator args Any - VariableTracker torch TorchInGraphFunctionVariable TorchInGraphFunctionVariable torch _C _pop_torch_function_stack call_function tx ConstantVariable create None reconstruct_type codegen PyCodegen - None ty = NoEnterTorchFunctionMode codegen AttrSource codegen tx import_source ty __module__ ty __name__ supports_graph_breaks - bool True exit_on_graph_break - bool False Used clear restore python torch function mode stack temporarily restore needed TorchFunctionModeStackStateManager __init__ - None stack list Any = __enter__ - None stack = torch overrides _get_current_function_mode_stack clear_torch_function_mode_stack __exit__ exc_type Optional type BaseException exc_val Optional BaseException exc_tb Optional TracebackType - None set_torch_function_mode_stack stack stack = contextlib contextmanager temp_restore_stack - Generator None None None prev = torch overrides _get_current_function_mode_stack set_torch_function_mode_stack stack try yield finally set_torch_function_mode_stack prev torch_function_mode_stack_state_mgr = TorchFunctionModeStackStateManager SymbolicTorchFunctionState __init__ py_stack Iterable Any - None This annoyingly complicated because how torch function subclass + mode C API designed There two exposed C knobs here contexts torch _C DisableTorchFunction torch _C DisableTorchFunctionSubclass These their definitions torch _C _is_torch_function_enabled indicates neither above knobs have been entered either entered will False torch _C _is_torch_function_mode_enabled indicates either torch mode stack empty OR torch _C DisableTorchFunction has been entered To disambiguate these keep myself sane I added C API check whether all torch function concepts modes subclasses enabled This only returns true iff we have entered torch _C DisableTorchFunction allows us separate stack length enablement state torch function modes This important because now mode pushed while dynamo tracing we know whether torch function modes enabled whether we should trace torch_function_subclass_enabled = torch _C _is_torch_function_enabled This differs C API same name will only false iff we have entered torch _C DisableTorchFunction does take into account mode stack length while C API bundles these two concepts torch_function_mode_enabled = torch _C _is_torch_function_all_disabled cur_mode = None TorchFunctionModeStackVariable reset mode_stack collections deque TorchFunctionModeVariable = collections deque i val enumerate py_stack mode_stack append LazyVariableTracker create val source=TorchFunctionModeStackSource i type ignore arg-type in_torch_function_mode - bool len mode_stack pop_torch_function_mode - TorchFunctionModeVariable mode_stack pop push_torch_function_mode mode_var TorchFunctionModeVariable - None mode_stack append mode_var call_torch_function_mode tx InstructionTranslator fn VariableTracker types TupleVariable args Iterable Any kwargs dict str Any - Any _pop_mode_for_inlining cur_mode cur_mode call_torch_function tx fn types args kwargs contextlib contextmanager _pop_mode_for_inlining - Generator TorchFunctionModeVariable None None old_mode = cur_mode cur_mode = pop_torch_function_mode type ignore assignment try yield cur_mode type ignore misc finally mode = cur_mode cur_mode = old_mode push_torch_function_mode mode type ignore arg-type TorchFunctionModeStackVariable VariableTracker Fake VT use dummy object indicating presence torch function mode stack mutation singleton value representing global torch function mode stack singleton exists C++ stack_value_singleton = object offset used track we have inserted removed device context which always placed bottom stack device context inserted graph will run mutation so when we want reconstruct any other modes stack their indices should shifted right + Conversely there device context stack graph mutates stack remove context set default device None each indices other modes should shifted left - offset = __init__ source Source symbolic_stack collections deque TorchFunctionModeVariable - None source = source symbolic_stack = symbolic_stack classmethod reset cls - None cls offset = classmethod register_mutation cls tx InstructionTranslator - None cls stack_value_singleton tx output side_effects var = cls source=Source symbolic_stack=tx symbolic_torch_function_state mode_stack tx output side_effects track_mutable cls stack_value_singleton var tx output side_effects mutation var classmethod register_device_context_insertion cls tx InstructionTranslator - None stack = tx symbolic_torch_function_state mode_stack stack cls is_device_context stack cls offset += stack insert TorchFunctionModeVariable None source=TorchFunctionModeStackSource -cls offset classmethod clear_default_device cls tx InstructionTranslator - None stack = tx symbolic_torch_function_state mode_stack stack cls is_device_context stack stack popleft cls offset -= staticmethod is_device_context var TorchFunctionModeVariable - bool isinstance var value DeviceContext var value None classmethod get_mode_index cls ind int - int ind + cls offset _get_all_args args Iterable Any kwargs dict str Any - Iterable VariableTracker _flatten_vts pytree arg_tree_leaves args kwargs _flatten_vts vts Iterable VariableTracker - list VariableTracker collections deque dicts ConstDictVariable lists ListVariable vts = deque vts output = while vts vt = vts popleft vt is_realized vt peek_type dict list tuple type ignore attr-defined vt realize vt is_realized isinstance vt ListVariable vts extend vt items continue isinstance vt ConstDictVariable vts extend vt items values continue output append vt output _get_subclass_type var VariableTracker - type assert isinstance var TensorWithTFOverrideVariable UserDefinedObjectVariable var python_type _get_subclass_type_var tx InstructionTranslator var VariableTracker - VariableTracker isinstance var TensorWithTFOverrideVariable var class_type_var tx isinstance var UserDefinedObjectVariable source = var source TypeSource var source VariableTracker build tx var python_type source raise AssertionError f Unexpected type type var _is_attr_overridden tx InstructionTranslator var VariableTracker name str - bool isinstance var TensorWithTFOverrideVariable UserDefinedObjectVariable False torch overridden = False try attr_val = inspect getattr_static var python_type name overridden &#124; = attr_val = getattr torch Tensor name except AttributeError pass overridden call_torch_function tx InstructionTranslator torch_function_var VariableTracker fn VariableTracker types TupleVariable args Iterable Any kwargs dict str Any - Any This emulates calling __torch_function__ which has signature __torch_function__ cls func types args= kwargs=None Also notice ` cls ` explicitly passed reference implementations https github com pytorch pytorch blob d bc c ee c ef bacf d torch csrc utils python_arg_parser cpp#L -L noqa B https github com pytorch pytorch blob d bc c ee c ef bacf d torch overrides py#L -L tf_args = fn types VariableTracker build tx tuple args VariableTracker build tx kwargs torch_function_var call_function tx tf_args get_torch_function_fn tx InstructionTranslator vt VariableTracker - VariableTracker The underlying function could classmethod staticmethod regular function function C-implementation It doesn t matter long they satisfy calling convention ` call_torch_function ` builtin BuiltinVariable args = vt ConstantVariable __torch_function__ func_vt = BuiltinVariable getattr call_function tx args func_vt can_dispatch_torch_function tx InstructionTranslator args Iterable Any kwargs dict str Any - bool has_overridden_args = any has_torch_function arg arg _get_all_args args kwargs tf_state = tx symbolic_torch_function_state has_overridden_args tf_state torch_function_subclass_enabled tf_state torch_function_mode_enabled tf_state in_torch_function_mode dispatch_torch_function tx InstructionTranslator fn VariableTracker args Iterable Any kwargs dict str Any - Any Gathers all args TensorWithTFOverrideVariable dispatches based ordering _get_overloaded_args all_args = _get_all_args args kwargs overloaded_args = _get_overloaded_args arg arg all_args has_torch_function arg _get_subclass_type types = TupleVariable _get_subclass_type_var tx arg arg overloaded_args tx symbolic_torch_function_state in_torch_function_mode res = tx symbolic_torch_function_state call_torch_function_mode tx fn types args kwargs isinstance res ConstantVariable res value NotImplemented res arg overloaded_args res = arg call_torch_function tx fn types args kwargs isinstance res ConstantVariable res value NotImplemented res unimplemented_v gb_type= All __torch_function__ overrides returned NotImplemented due TypeError user code context=f fn= args= kwargs= explanation=f All __torch_function__ overrides function fn returned NotImplemented hints= graph_break_hints USER_ERROR TensorWithTFOverrideVariable TensorVariable Represents tensor subclass instance __torch_function__ override classmethod from_tensor_var cls tx InstructionTranslator tensor_var VariableTracker class_type type cls_source Source - TensorWithTFOverrideVariable Note __torch_function__ coerce ` tensor_var ` into TensorWithTFOverrideVariable In eager just type change torch This simulates shallow-copying tensor object kwargs = dict tensor_var __dict__ input_tensor_type = kwargs pop class_type assert input_tensor_type torch Tensor torch nn Parameter f invalid type input_tensor_type TensorWithTFOverrideVariable from_tensor_var var = cls class_type=class_type kwargs var install_global tx var install_global tx InstructionTranslator - None stash subclass type rewrap output tensor needed needed because actual type needs available each time compiled artifact run outputs wrapped tensor global_mangled_class_name tx tx output global_scope Safe because global_mangled_class_name figures out tx output install_global_unsafe global_mangled_class_name tx class_type python_type - type class_type class_type_var tx InstructionTranslator - VariableTracker TensorSubclassVariable class_type source=GlobalSource global_mangled_class_name tx global_mangled_class_name tx InstructionTranslator - str get_safe_global_name tx f __subclass_ class_type __name__ class_type var_getattr tx InstructionTranslator name str - VariableTracker Note __torch_function__ We currently only support attributes defined base tensors custom attribute accesses will graph break torch I think only ` _base ` breaking because we aren t modelling view relationship perfectly some scenarios name banned_attrs unimplemented_v gb_type= Unsupported tensor subclass attribute access context=f name explanation= ` torch compile ` currently can t trace hints= f Avoid accessing name tensor subclass torch compile region graph_break_hints SUPPORTABLE Handle non-overridden attributes inherited ` torch Tensor ` attr_is_overridden = _is_attr_overridden tx name hasattr torch Tensor name attr_is_overridden inspect ismethoddescriptor getattr torch Tensor name args = kwargs dict Any Any = can_dispatch_torch_function tx args kwargs get_fn = VariableTracker build tx getattr torch Tensor name __get__ call_torch_function tx get_fn TupleVariable class_type_var tx args kwargs ` TensorVariable var_getattr ` doesn t handle user-defined function attribute well so we explicitly handle them here TODO move logic into ` TensorVariable ` try merge similar logic ` UserDefinedObjectVariable ` try attr = inspect getattr_static class_type name except AttributeError pass types cls_source = GlobalSource global_mangled_class_name tx attr_source = AttrSource cls_source name isinstance attr types FunctionType install_guard attr_source make_guard GuardBuilder CLOSURE_MATCH UserMethodVariable attr isinstance attr property getter_source = AttrSource attr_source fget getter = attr fget getter_var = VariableTracker build tx getter source=getter_source getter_var call_function tx isinstance attr classmethod UserMethodVariable attr __func__ class_type_var tx source=attr_source attr_is_overridden unimplemented_v gb_type= Unsupported tensor subclass overridden attribute access context=f name explanation= ` torch compile ` only support tracing certain types overridden tensor subclass attributes hints= f Avoid accessing name tensor subclass torch compile region f Renaming attribute ` name ` type class_type graph_break_hints SUPPORTABLE super var_getattr tx name call_torch_function tx InstructionTranslator fn VariableTracker types TupleVariable args Iterable Any kwargs dict str Any - Any NOTE assumes ` __torch_function__ ` isn t modified during tracing hasattr torch_function_fn torch_function_fn = get_torch_function_fn tx call_torch_function tx torch_function_fn fn types args kwargs call_method tx InstructionTranslator name str args list VariableTracker kwargs dict str VariableTracker - VariableTracker This code block implements inlining __torch_function__ override ` call_method ` tf_args = + args can_dispatch_torch_function tx tf_args kwargs torch _is_attr_overridden tx name unimplemented_v gb_type= Tensor subclass overridden method call context=f name explanation= ` torch compile ` currently can t trace hints= f Avoid calling name tensor subclass torch compile region f Renaming method ` name ` type class_type graph_break_hints SUPPORTABLE Note __torch_function__ Currently we only support methods defined tensor we will graph break other cases will need bigger overhaul extracting methods comparing them equality We ve established above check method overridden so we guard method same impl defined tensor retrieve source source = AttrSource AttrSource source __class__ name value = inspect getattr_static python_type name source = None value = getattr torch Tensor name func_var = VariableTracker build tx value source dispatch_torch_function tx func_var tf_args kwargs super call_method tx name args kwargs