Experimental staging module PyTorch Distributed Checkpointing This module provides advanced staging capabilities checkpoints including - Asynchronous staging using ThreadPoolExecutor - Pinned memory allocation faster CPU-GPU transfers - Shared memory support multi-process scenarios - Non-blocking CUDA operations stream synchronization - Caching frequently used storages efficient memory management - Automatic resource cleanup memory management Classes CheckpointStager Abstract base defining staging interface StagingOptions Configuration dataclass staging behavior DefaultStager Default implementation comprehensive staging features abc concurrent futures Future ThreadPoolExecutor dataclasses dataclass typing Any TypeVar Union torch torch distributed checkpoint _state_dict_stager StateDictStager types STATE_DICT T = TypeVar T CheckpointStager abc ABC Abstract base checkpoint staging implementations CheckpointStager defines interface all staging implementations must follow Staging process offloading state dictionaries async checkpointing abc abstractmethod stage state_dict STATE_DICT kwargs Any - Union STATE_DICT Future STATE_DICT Stage state dictionary checkpointing Args state_dict The state dictionary stage kwargs Additional staging parameters Returns Either staged state dictionary synchronous Future will resolve staged state dictionary asynchronous abc abstractmethod close - None Clean up all resources used stager dataclass CheckpointStagerConfig Configuration options checkpoint staging behavior Attributes use_pinned_memory bool Enable pinned memory allocation faster CPU-GPU transfers Requires CUDA available Default True use_shared_memory bool Enable shared memory multi-process scenarios Useful when multiple processes need access same staged data Default True use_async_staging bool Enable asynchronous staging using background thread pool Allows overlapping computation staging operations Requires CUDA Default True use_non_blocking_copy bool Use non-blocking device memory copies stream synchronization Improves performance allowing CPU work continue during GPU transfers Default True Note CUDA-dependent features will raise exception CUDA available use_pinned_memory bool = True use_shared_memory bool = True use_async_staging bool = True use_non_blocking_copy bool = True DefaultStager CheckpointStager DefaultStager provides full-featured staging implementation combines multiple optimization techniques efficient checkpoint preparation The staging process works follows State dictionary submitted staging sync async Tensors copied GPU optimized CPU storage CUDA operations synchronized non-blocking copies used Staged state dictionary returned made available via Future NOTE state_dict should deep-copyable object staging will create copy Usage Patterns Synchronous staging stager = DefaultStager CheckpointStagerConfig use_async_staging=False staged_dict = stager stage state_dict stager close Asynchronous staging stager = DefaultStager CheckpointStagerConfig use_async_staging=True future = stager stage state_dict do other work staged_dict = future result stager close Context manager pattern recommended DefaultStager config stager result = stager stage state_dict Automatic cleanup exit Performance Considerations - Async staging provides best performance when model computation can overlap staging operations - Pinned memory improves CPU-GPU transfer speeds uses more memory - Shared memory allows efficient IPC checkpoint process - Non-blocking copies reduce GPU idle time during memory transfers Thread Safety DefaultStager thread-safe Each thread should use its own instance external synchronization should provided __init__ config CheckpointStagerConfig = CheckpointStagerConfig _config = config _state_dict_stager = StateDictStager pin_memory=config use_pinned_memory share_memory=config use_shared_memory _staging_executor = None _staging_stream = None _config use_async_staging pyrefly ignore bad-assignment _staging_executor = ThreadPoolExecutor max_workers= torch accelerator is_available Note stream needs initialized main thread after default cuda stream setup used avoid risk accidentally reusing main compute stream other cases kernels actually launching main thread pyrefly ignore bad-assignment _staging_stream = torch Stream _config use_non_blocking_copy torch accelerator is_available raise AssertionError Non-blocking copy requires current accelerator available stage state_dict STATE_DICT kwargs Any - Union STATE_DICT Future STATE_DICT _config use_async_staging _staging_executor None raise AssertionError Staging executor should initialized async staging _staging_executor submit _stage state_dict kwargs _stage state_dict kwargs _stage state_dict STATE_DICT kwargs Any - STATE_DICT state_dict = _state_dict_stager stage state_dict non_blocking=self _config use_non_blocking_copy kwargs _config use_non_blocking_copy _staging_stream _config use_async_staging raise AssertionError Non-blocking copy background thread async staging needs staging_stream initialized waits enqued copy operations finish _staging_stream synchronize _staging_stream torch accelerator synchronize state_dict close - None Clean up all resources used DefaultStager Shuts down ThreadPoolExecutor used async staging operations cleans up underlying StateDictStager s cached storages Should called when stager no longer needed prevent resource leaks especially long-running applications After calling close stager should used further staging operations state_dict should deep-copyable object Example stager = DefaultStager CheckpointStagerConfig use_async_staging=True do staging operations stager close Clean up all resources _staging_executor _staging_executor shutdown wait=True _state_dict_stager close