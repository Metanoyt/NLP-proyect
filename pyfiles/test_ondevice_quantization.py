Owner s oncall quantization io torch torch _C torch ao quantization default_dynamic_qconfig per_channel_dynamic_qconfig torch ao quantization quantize_jit _prepare_ondevice_dynamic_jit _quantize_ondevice_dynamic_jit convert_dynamic_jit prepare_dynamic_jit torch jit mobile _load_for_lite_interpreter LiteScriptModule torch testing FileCheck torch testing _internal common_quantization get_script_module LinearAddModel torch testing _internal common_utils TestCase torch utils bundled_inputs myMod torch nn Module __init__ weight super __init__ fc = torch nn Linear float fc weight = weight fc = torch nn Linear float forward x fc fc x MyConvLinearModule torch nn Module __init__ - None super __init__ conv = torch nn Conv d weight = torch nn Parameter torch ones weight = torch nn Parameter torch ones mymod = myMod weight forward x conv_output = conv x y = mymod conv_output z = torch nn functional linear y weight z get_example_inputs torch rand OnDevicePTQUtils observer_module_name = MinMaxObserver PerChannelMinMaxObserver staticmethod insert_observers model qconfig_dict inputs = model get_example_inputs scripted_model = get_script_module model False inputs scripted_model = _prepare_ondevice_dynamic_jit scripted_model qconfig_dict scripted_model staticmethod ptq_dynamic_quantize model qconfig_dict inputs = model get_example_inputs m = get_script_module model False inputs m = _quantize_ondevice_dynamic_jit m qconfig_dict forward True m staticmethod find_observer_modules m observer_modules = child_module m children child_module original_name OnDevicePTQUtils observer_module_name observer_modules append child_module observer_modules staticmethod is_value_type_observer value type_name = value type observer_type OnDevicePTQUtils observer_module_name observer_type type_name str True False staticmethod is_calculate_qparam node node kind == prim CallMethod node s name == calculate_qparams True False staticmethod get_linear_packed_param_fp_weight node weight = node inputsAt node weight kind = aten quantize_per_tensor weight kind = aten quantize_per_channel raise ValueError Quantized weight must produced fp_weight = weight inputsAt node assert fp_weight kind == prim GetAttr Weight must attribute module fp_weight_name = fp_weight s name fp_weight_name staticmethod is_per_channel_quantized_packed_param node assert node kind == quantized linear_prepack Node must corresponds linear_prepack weight = node inputsAt node assert weight kind = aten quantize_per_tensor weight kind = aten quantize_per_channel weight kind = aten quantize_per_tensor TestOnDeviceDynamicPTQInsertObservers TestCase _check_num_and_type_of_observers model num_observers qconfig_dict = default_dynamic_qconfig scripted_model = OnDevicePTQUtils insert_observers model qconfig_dict observer_modules = OnDevicePTQUtils find_observer_modules scripted_model assertTrue len observer_modules == num_observers observer observer_modules assertTrue observer original_name == MinMaxObserver qconfig_dict = per_channel_dynamic_qconfig scripted_model = OnDevicePTQUtils insert_observers model qconfig_dict observer_modules = OnDevicePTQUtils find_observer_modules scripted_model assertTrue len observer_modules == num_observers observer observer_modules assertTrue observer original_name == PerChannelMinMaxObserver _check_observer_method model num_observers qconfig_dict = default_dynamic_qconfig inputs = model get_example_inputs orig_scripted_model = get_script_module model False inputs torch _C _jit_pass_inline orig_scripted_model graph orig_forward_graph = orig_scripted_model graph str scripted_model = OnDevicePTQUtils insert_observers model qconfig_dict quant_forward_graph = scripted_model graph str exact graph matching difficult so just resorting lines instead implementing graph matching assertEqual len orig_forward_graph splitlines len quant_forward_graph splitlines observe_method = scripted_model observe_forward graph FileCheck check_count prim CallMethod name= forward _observer num_observers exactly=True run observe_method reset_observers_method = scripted_model reset_observers_forward graph FileCheck check_count prim CallMethod name= reset_min_max_vals _observer num_observers exactly=True run reset_observers_method _observer_is_weight_only node node kind == prim CallMethod node s name == forward OnDevicePTQUtils is_value_type_observer node inputsAt node inputsAt node kind == prim GetAttr False test_num_observers model = LinearAddModel _check_num_and_type_of_observers model model = MyConvLinearModule _check_num_and_type_of_observers model test_observe_method model = MyConvLinearModule _check_observer_method model test_weight_only_observers model = MyConvLinearModule qconfig_dict = default_dynamic_qconfig scripted_model = OnDevicePTQUtils insert_observers model qconfig_dict observe_forward_graph = scripted_model observe_forward graph num_weight_only_observers = node observe_forward_graph nodes _observer_is_weight_only node num_weight_only_observers += assertEqual num_weight_only_observers TestOnDeviceDynamicPTQInsertQuantDequant TestCase _validate_quant_dequant_nodes model num_nodes per_channel= quantize_forward_graph = model quantize_forward graph quantize_per_tensor = quantize_per_channel = n quantize_forward_graph nodes aten quantize_per_tensor n kind quantize_per_tensor += aten quantize_per_channel n kind quantize_per_channel += assertEqual quantize_per_tensor + quantize_per_channel num_nodes _validate_calculate_qparams model num_nodes quantize_forward_graph = model quantize_forward graph num_calculate_qparams = n quantize_forward_graph nodes OnDevicePTQUtils is_calculate_qparam n num_calculate_qparams += assertEqual num_calculate_qparams num_nodes _validate_no_observer_forward model quantize_forward_graph = model quantize_forward graph n quantize_forward_graph nodes n kind == prim CallMethod n s name == forward OnDevicePTQUtils is_value_type_observer n inputsAt False True _check_quant_dequant_and_calc_qparams model num_nodes qconfig_dict = default_dynamic_qconfig m = OnDevicePTQUtils ptq_dynamic_quantize model qconfig_dict _validate_quant_dequant_nodes m num_nodes _validate_calculate_qparams m num_nodes _validate_no_observer_forward m qconfig_dict = per_channel_dynamic_qconfig m = OnDevicePTQUtils ptq_dynamic_quantize model qconfig_dict _validate_quant_dequant_nodes m num_nodes num_nodes _validate_calculate_qparams m num_nodes _validate_no_observer_forward m _check_quantize_forward_runs model inputs = model get_example_inputs qconfig_dict = default_dynamic_qconfig m = OnDevicePTQUtils ptq_dynamic_quantize model qconfig_dict m observe_forward inputs m quantize_forward inputs qconfig_dict = per_channel_dynamic_qconfig m = OnDevicePTQUtils ptq_dynamic_quantize model qconfig_dict First must run observe forward record stats produce correct scales zero points m observe_forward inputs m quantize_forward inputs test_num_quant_dequant_nodes model = LinearAddModel _check_quant_dequant_and_calc_qparams model model = MyConvLinearModule _check_quant_dequant_and_calc_qparams model test_quantize_forward_runs model = LinearAddModel _check_quantize_forward_runs model model = MyConvLinearModule _check_quantize_forward_runs model TestOnDeviceDynamicPTQFinalize TestCase _validate_packed_params model num_nodes per_channel= quantize_forward_graph = model quantize_forward graph quantize_per_tensor = quantize_per_channel = linear_prepack = linear_prepack_uses = n quantize_forward_graph nodes n kind == prim SetAttr maybe_packed_param_value = n inputsAt maybe_packed_param = maybe_packed_param_value node maybe_packed_param kind == quantized linear_prepack linear_prepack += linear_prepack_uses += len maybe_packed_param_value uses OnDevicePTQUtils is_per_channel_quantized_packed_param maybe_packed_param quantize_per_channel += quantize_per_tensor += assertEqual quantize_per_tensor + quantize_per_channel num_nodes assertEqual quantize_per_channel per_channel assertEqual linear_prepack num_nodes assertEqual linear_prepack_uses num_nodes _validate_no_linear_unpack model quantize_forward_graph = model quantize_forward graph n quantize_forward_graph nodes n kind == quantized linear_unpack False True _validate_setattr_fp_weights model num_nodes quantize_forward_graph = model quantize_forward graph fp_weights_setattr = fp_weight_names = n quantize_forward_graph nodes n kind == prim SetAttr maybe_packed_param = n inputsAt node maybe_packed_param kind == quantized linear_prepack weight_name = OnDevicePTQUtils get_linear_packed_param_fp_weight maybe_packed_param fp_weight_names append weight_name n quantize_forward_graph nodes This basically detecting x = prim Constant = prim SetAttr weight_name module_value x Thus making sure original fp weights reset n kind == prim SetAttr weight_name = n s name weight_name fp_weight_names maybe_constant = n inputsAt node maybe_constant kind == prim Constant fp_weights_setattr += assertEqual fp_weights_setattr num_nodes _validate_quantized_forward model num_nodes quantized_forward_graph = model quantized_forward graph quantize_per_tensor = quantize_per_channel = quantized_linear_dynamic = linear_packed_params = num_setattr = n quantized_forward_graph nodes aten quantize_per_tensor n kind quantize_per_tensor += aten quantize_per_channel n kind quantize_per_channel += quantized linear_dynamic n kind quantized_linear_dynamic += n kind == prim GetAttr output = n outputsAt output_type = output type LinearPackedParamsBase output_type str linear_packed_params += n kind == prim SetAttr num_setattr += assertEqual quantize_per_tensor assertEqual quantize_per_channel assertEqual quantized_linear_dynamic num_nodes assertEqual linear_packed_params num_nodes assertEqual num_setattr _check_quantize_forward model num_nodes qconfig_dict = default_dynamic_qconfig m = OnDevicePTQUtils ptq_dynamic_quantize model qconfig_dict _validate_packed_params m num_nodes _validate_no_linear_unpack m _validate_setattr_fp_weights m num_nodes qconfig_dict = per_channel_dynamic_qconfig m = OnDevicePTQUtils ptq_dynamic_quantize model qconfig_dict _validate_packed_params m num_nodes num_nodes _validate_no_linear_unpack m _validate_setattr_fp_weights m num_nodes _check_quantized_forward model num_nodes qconfig_dict = default_dynamic_qconfig m = OnDevicePTQUtils ptq_dynamic_quantize model qconfig_dict _validate_quantized_forward m num_nodes qconfig_dict = per_channel_dynamic_qconfig m = OnDevicePTQUtils ptq_dynamic_quantize model qconfig_dict _validate_quantized_forward m num_nodes _check_against_ref_dynamic_ptq model model eval inputs = model get_example_inputs ref_m = torch jit script model torch _C _jit_pass_inline ref_m graph qconfig_dict = default_dynamic_qconfig ref_m = prepare_dynamic_jit ref_m qconfig_dict ref_m = convert_dynamic_jit ref_m ref_output = ref_m inputs m = OnDevicePTQUtils ptq_dynamic_quantize model qconfig_dict m observe_forward inputs m quantize_forward inputs output = m quantized_forward inputs assertTrue torch allclose ref_output output thrown = False try m inputs except Exception thrown = True assertTrue thrown test per channel quant ref_m = torch jit script model torch _C _jit_pass_inline ref_m graph qconfig_dict = per_channel_dynamic_qconfig ref_m = prepare_dynamic_jit ref_m qconfig_dict ref_m = convert_dynamic_jit ref_m ref_output = ref_m inputs m = OnDevicePTQUtils ptq_dynamic_quantize model qconfig_dict m observe_forward inputs m quantize_forward inputs output = m quantized_forward inputs assertTrue torch allclose ref_output output thrown = False try m inputs except Exception thrown = True assertTrue thrown _check_serdes_and_device_side_api_helper model check_device_side_api=False model eval inputs = model get_example_inputs ref_m = torch jit script model torch _C _jit_pass_inline ref_m graph qconfig_dict = default_dynamic_qconfig ref_m = prepare_dynamic_jit ref_m qconfig_dict ref_m = convert_dynamic_jit ref_m buffer = io BytesIO torch jit save ref_m buffer buffer seek ref_m = torch jit load buffer ref_output = ref_m inputs check_device_side_api m = OnDevicePTQUtils ptq_dynamic_quantize model qconfig_dict buffer = io BytesIO torch jit save m buffer buffer seek m = torch jit load buffer m reset_observers_forward m observe_forward inputs m quantize_forward inputs output = m quantized_forward inputs assertTrue torch allclose ref_output output check lite interpreter m = OnDevicePTQUtils ptq_dynamic_quantize model qconfig_dict first_input = inputs rand_input = bundled_inputs bundle_randn first_input size dtype=first_input dtype m = bundled_inputs bundle_inputs m inputs= rand_input buffer = io BytesIO m _save_to_buffer_for_lite_interpreter buffer seek m = _load_for_lite_interpreter buffer Error here torch _C _quantize_ondevice_ptq_dynamic m _c forward assertFalse m find_method quantized_forward assertFalse m find_method quantize_forward assertFalse m find_method observe_forward assertFalse m find_method reset_observers_forward output = m inputs assertTrue torch allclose ref_output output Now serialize flabuffer load fb check dict_ dict str str = bytes = torch _C _save_mobile_module_to_bytes m _c dict_ m = LiteScriptModule torch _C _load_mobile_module_from_bytes bytes fb_output = m inputs assertTrue torch allclose ref_output fb_output model eval inputs = model get_example_inputs ref_m = torch jit script model torch _C _jit_pass_inline ref_m graph qconfig_dict = per_channel_dynamic_qconfig ref_m = prepare_dynamic_jit ref_m qconfig_dict ref_m = convert_dynamic_jit ref_m buffer = io BytesIO torch jit save ref_m buffer buffer seek ref_m = torch jit load buffer ref_output = ref_m inputs check_device_side_api m = OnDevicePTQUtils ptq_dynamic_quantize model qconfig_dict buffer = io BytesIO torch jit save m buffer buffer seek m = torch jit load buffer m reset_observers_forward m observe_forward inputs m quantize_forward inputs output = m quantized_forward inputs assertTrue torch allclose ref_output output check lite interpreter m = OnDevicePTQUtils ptq_dynamic_quantize model qconfig_dict first_input = inputs rand_input = bundled_inputs bundle_randn first_input size dtype=first_input dtype m = bundled_inputs bundle_inputs m inputs= rand_input buffer = io BytesIO m _save_to_buffer_for_lite_interpreter buffer seek m = _load_for_lite_interpreter buffer Error here torch _C _quantize_ondevice_ptq_dynamic m _c forward assertFalse m find_method quantized_forward assertFalse m find_method quantize_forward assertFalse m find_method observe_forward assertFalse m find_method reset_observers_forward output = m inputs assertTrue torch allclose ref_output output _check_serialization_deserialization model _check_serdes_and_device_side_api_helper model False _check_device_side_api model _check_serdes_and_device_side_api_helper model True test_quantize_forward model = LinearAddModel _check_quantize_forward model model = MyConvLinearModule _check_quantize_forward model test_quantized_forward model = LinearAddModel _check_quantized_forward model model = MyConvLinearModule _check_quantized_forward model test_against_offdevice_dynamic_ptq model = LinearAddModel _check_against_ref_dynamic_ptq model model = MyConvLinearModule _check_against_ref_dynamic_ptq model test_serialization_deserialization model = MyConvLinearModule _check_serialization_deserialization model test_device_side_api model = MyConvLinearModule _check_device_side_api model __name__ == __main__ raise RuntimeError This test currently used should enabled discover_tests py required