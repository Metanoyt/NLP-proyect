mypy allow-untyped-defs DTensor operator schema definitions utilities This module defines core data structures utilities describing managing distributed tensor operations PyTorch s DTensor system It provides foundational schema types used sharding propagation operator strategy selection distributed execution planning Key components - OpSpec Describes acceptable sharding placements operations - OpStrategy Represents possible sharding strategies operator - TupleStrategy Container multiple strategies when ops have tuple list tensors input - OpSchema Describes operator input output schemas DTensorSpecs - OutputSharding Manages output sharding specifications redistribution - RuntimeSchemaInfo Runtime execution metadata operators - OpInfo Complete runtime operator execution information These schema definitions enable DTensor system Propagate tensor sharding information operator outputs Greedily select sharding strategies distributed operations Plan execute tensor redistributions when needed Cache sharding decisions performance optimization collections abc Sequence dataclasses dataclass functools cached_property typing Any Optional Union typing_extensions deprecated torch torch _C _DTensor_OpSchema_post_init _DTensor_OpSchema_recompute_comparison_key torch _ops OpOverload torch distributed device_mesh DeviceMesh torch distributed tensor _dtensor_spec DTensorSpec torch distributed tensor placement_types Placement try torch utils _cxx_pytree register_pytree_node tree_leaves tree_map_only TreeSpec except ImportError torch utils _pytree type ignore no-redef assignment register_pytree_node tree_leaves tree_map_only TreeSpec Common type aliases ArgsType = tuple object KwargsType = dict str object PlacementList = list Optional Placement ATen op schemas could have Tensor Tuple Tensor List Tensor so output type should same set possibilities OutputSpecType = Optional Union DTensorSpec Sequence Optional DTensorSpec _rebuild_tensor_from_dtensor_meta arg - object This used propagate tensor metadata must under fake mode assert arg tensor_meta None DTensorSpec does contain tensor_meta torch empty_strided arg tensor_meta shape arg tensor_meta stride dtype=arg tensor_meta dtype _pretty_print_spec spec object - str spec None None isinstance spec DTensorSpec join str p p spec placements isinstance spec Sequence + join _pretty_print_spec s s spec + raise RuntimeError f Unknown spec type print spec= spec dataclass OpSpec An OpSpec describes acceptable sharding placements operation specified DTensorSpecs both output inputs note when op value single DTensor object output_specs DTensorSpec when value tuple Optional DTensor output_specs tuple Optional DTensorSpec note we MUST produce DTensorSpec every output Tensor None entries only occur non-Tensor outputs e g operators Optional Tensor non-Tensor outputs invariant DeviceMesh all DTensorSpec must same output_specs input_specs related op given these input_specs way output would look output_specs Union DTensorSpec tuple Optional DTensorSpec input_specs Optional Sequence DTensorSpec = None redistribute_cost tells how expensive redistribute given input into placement specified OpSpec outer list one entry list per tensor input op s arg schema inner list one entry cost value per possible sharding spec input Example ------- another_op - tensor_a another_op produces output becomes our first input my_op tensor_a Let s assume OpSpec s input_specs Replicate another_op supports strategies OpSpecs which produce outputs Replicate Shard In example redistribute_costs would look like one row representing my_op s first input tensor_a two entries one each strategies supported another_op cost redistributing tensor_a Replicate K cost redistributing tensor_a Shard redistribute_cost Optional list list float = None cached_property output_spec - DTensorSpec This function requires strategy have exactly one DTensorSpec output spec If output_specs tuple we throw exception isinstance output_specs DTensorSpec output_specs raise ValueError f function output_spec expects single DTensorSpec got output_specs cached_property mesh isinstance output_specs DTensorSpec output_specs mesh isinstance output_specs tuple out_spec = output_specs assert isinstance out_spec DTensorSpec out_spec mesh raise ValueError f function output_spec expects single DTensorSpec tuple DTensorSpec got output_specs input_spec index int = - DTensorSpec assert input_specs None input_specs OpSpec None assert len input_specs index f Invalid index index input_specs length f len input_specs input_specs input_specs index __str__ - str input_specs None input_specs_str = f _pretty_print_spec input_specs - input_specs_str = output_spec_str = _pretty_print_spec output_specs f input_specs_str output_spec_str StrategyType Base type op strategy We have two StrategyType OpStrategy TupleStrategy OpStrategy StrategyType OpStrategy consists list sharding strategies associated op where each strategy OpSpec describes acceptable input output sharding invariant DeviceMesh all OpSpec must same __init__ strategies list OpSpec - None super __init__ strategies list OpSpec = strategies __str__ - str strategy_list_str = join str strategy strategy strategies mesh_shape = mesh_shape f OpStragety strategy_list_str mesh mesh_shape max_num_shards - int Returns max number shards across all OpSpecs max strategy output_spec num_shards strategy strategies property mesh strategies mesh property mesh_shape strategies mesh shape property ndim strategies output_spec ndim property shape strategies output_spec shape TupleStrategy StrategyType TupleStrategy special case operators fundamentally compound batched such some subset inputs outputs completely unrelated some other subset Generally foreach_ ops most common use-case TupleStrategy because they accept lists inputs operate independently each input tuple zipped inputs For example out_a out_b = torch foreach_add b scalar input s sharding only affects out_a s sharding independent b out_b An example operator should NOT use TupleStrategy torch split It produces List Tensor its output sharding decision one output bound together decision each other output common input __init__ children Sequence StrategyType - None super __init__ children Sequence StrategyType = children property deprecated TupleStrategy childs deprecated use TupleStrategy children instead codespell ignore childs category=FutureWarning childs - Sequence StrategyType codespell ignore childs Alias children maintain backward compatibility children child_mesh index int - DeviceMesh op_strategy = children index assert isinstance op_strategy OpStrategy op_strategy mesh __str__ - str child_strategies_str = join f str strat idx strat enumerate children f TupleStrategy child_strategies_str try register_pytree_node TupleStrategy lambda node node children None lambda children _ TupleStrategy tuple children except ValueError already registered TupleStrategy skip pass dataclass RuntimeSchemaInfo RuntimeSchemaInfo stores operator schema related information runtime eager execution This mainly used two ways generate hash args determine whether re-run sharding prop determine we need pytree This static_argnum records static arg starting index ops have non-tensor args kwargs which would affect sharding propagation results All args starting index would hashed our sharding cache Note only few ops need information e g view transpose var dim etc static_argnum int = This static_kwargkey records static kwarg names which would affect sharding prop static_kwargkey Optional list str = None each op can decide wants use pytree flatten unflatten during operator eager execution default we don t need do flatten unflatten only op indicate needs accelerate eager performance needs_pytree bool = False dataclass OpSchema OpSchema data describes operator input schemas includes DTensorSpecs OpStrategies instead DTensor non-tensor args kwargs positional order preserved It mainly used DTensor s dispatching logic perform various actions i e sharding propagation caching sharding decisions redistribute etc NOTE must used read only data TODO make frozen dataclass Args op operator overload we intercepting args_schema contains args except DTensor args have been replaced its DTensorSpec OpStrategy kwargs_schema contains kwargs except DTensor kwargs have been replaced its DTensorSpec OpStrategy op OpOverload args_schema ArgsType kwargs_schema KwargsType schema_info Optional RuntimeSchemaInfo = None _comparison_key Optional tuple object = None property args_spec - tuple DTensorSpec args_spec Tuple DTensorSpec contains clean list args spec list NO non-DTensor positional arguments i e int float tuple etc mainly used sharding propagation propagate output spec args = tree_leaves args_schema schema_info None schema_info needs_pytree args_schema tuple item item args isinstance item DTensorSpec property args_strategy - tuple OpStrategy filter out non-relevant values args schema get clean OpStrategy list separate args_spec ease type annotation TODO see we should merge args_spec args = tree_leaves args_schema schema_info None schema_info needs_pytree args_schema tuple item item args isinstance item OpStrategy __repr__ - str args_schema = join str arg_schema arg_schema args_schema f OpSchema op= op f args_schema= args_schema f kwargs_schema= kwargs_schema __str__ - str args_schema list str = device_mesh = None arg args_schema isinstance arg DTensorSpec args_schema append str arg device_mesh = arg mesh isinstance arg OpStrategy assert len arg strategies == args_schema append _pretty_print_spec arg strategies output_specs device_mesh = arg mesh isinstance arg TupleStrategy first_op_strategy = arg children assert isinstance first_op_strategy OpStrategy device_mesh = first_op_strategy mesh args_schema append str arg args_schema append str arg f op join args_schema device_mesh __post_init__ - None _DTensor_OpSchema_post_init arg_type_tensor_or_tensor_list_like arg object - bool is_tensor = isinstance arg DTensorSpec is_tensor True isinstance arg list False all isinstance e DTensorSpec e None e arg return_type_tuple_tensor_like - bool all dispatch ops could only Tuple Tensor have None ints floats tuple first element must Tensor so check enough return_types = op _schema returns len return_types isinstance return_types type torch TensorType return_type_list_tensor_like - bool returns True type List return_types = op _schema returns len return_types == isinstance return_types type torch ListType return_type_tensor - bool return_types = op _schema returns all dispatch ops only Tensor Tuple Tensor tensor like types so check enough tensor like types isinstance return_types type torch TensorType get_mesh_from_args validate bool = True - DeviceMesh This util can used get mesh OpSchema contains multiple DTensors arguments When ` validate ` True will try validate all arguments have same mesh avoid unexpected cross mesh errors NOTE util currently does handle TupleStrategy when ` validate=True ` because TupleStrategy there could different types checks i e - stack cat like op we need check within TupleStrategy every input same mesh - foreach like ops we need check zipped inputs same mesh each index first_arg = args_schema isinstance first_arg DTensorSpec OpStrategy mesh = first_arg mesh isinstance first_arg list tuple TupleStrategy first_elem = first_arg children isinstance first_arg TupleStrategy first_arg assert isinstance first_elem DTensorSpec OpStrategy mesh = first_elem mesh raise ValueError f Cannot find device mesh args op op validate arg args_schema isinstance arg DTensorSpec OpStrategy arg mesh = mesh raise RuntimeError f DTensor does support cross-mesh operation op f Got meshes mesh arg mesh f Please make sure all arguments have same DeviceMesh mesh is_inplace_op - bool simple analysis function schema determine inplace variant might entirely correct s good enough now op _schema name - == _ is_out_variant_op - bool simple analysis function schema determine out variant might entirely correct s good enough now out op _schema overload_name is_view_op - bool op _schema _is_view_op _recompute_comparison_key - None _DTensor_OpSchema_recompute_comparison_key __hash__ - int hash _comparison_key __eq__ other object - bool early checks isinstance other OpSchema False op = other op False len args_schema = len other args_schema False _comparison_key == other _comparison_key gen_fake_args - ArgsType gen_fake_args generate fake args operator mainly used sharding propagation rules generate fake args operator run local tensor operator get output spec tree_map_only DTensorSpec _rebuild_tensor_from_dtensor_meta args_schema is_leaf=lambda x isinstance x DTensorSpec gen_fake_kwargs - KwargsType gen_fake_kwargs generate fake kwargs operator mainly used sharding propagation rules generate fake kwargs operator run local tensor operator get output spec tree_map_only DTensorSpec _rebuild_tensor_from_dtensor_meta kwargs_schema is_leaf=lambda x isinstance x DTensorSpec _inplace_rewrap_schema_suggestion origin_schema OpSchema - None suggestion_args_spec = args_spec new_arg_schema list object = idx_of_args_spec = origin_schema schema_info None origin_schema schema_info needs_pytree args_schema Sequence Any = tree_leaves origin_schema args_schema args_schema = origin_schema args_schema arg args_schema isinstance arg DTensorSpec new_arg_schema append suggestion_args_spec idx_of_args_spec idx_of_args_spec += new_arg_schema append arg args_schema = tuple new_arg_schema kwargs_schema = origin_schema kwargs_schema _recompute_comparison_key dataclass OutputSharding OutputSharding data used sharding propagation could set output_spec upon successful propagation If needs_redistribute set True redistribute_schema would returned together indicate input arguments needs redistributed before op execution NOTE redistribute_schema generated sharding propagation should exactly same operator OpSchema except DTensorSpecs specifies output sharding pattern output_spec OutputSpecType schema redistribution needed redistribute_schema Optional OpSchema = None flag indicating inputs need redistribution needs_redistribute bool = False flag use values ` redistribute_schema ` use_val_from_redistribute_schema bool = False cached_property mesh isinstance output_spec DTensorSpec output_spec mesh isinstance output_spec tuple out_spec = output_spec isinstance out_spec DTensorSpec out_spec mesh raise ValueError f Unknown output spec type type out_spec raise ValueError f Unknown output spec type type output_spec dataclass OpInfo All Runtime Op execution info packed here The first compute device mesh recorded args NOTE one op could have multiple meshes its args We just record first mesh here check current rank should participate computation compute_mesh DeviceMesh compete runtime operator infos schema OpSchema flat_args_schema list object local_args Sequence object local_kwargs dict str object args_tree_spec Optional TreeSpec = None output sharding info output_sharding Optional OutputSharding = None