Owner s oncall distributed contextlib unittest torch torch distributed dist torch fx traceback fx_traceback torch _dynamo functional_export _dynamo_graph_capture_for_export dynamo_graph_capture_for_export torch _functorch aot_autograd aot_export_joint_with_descriptors torch _functorch partitioners min_cut_rematerialization_partition torch _guards tracing TracingContext torch distributed device_mesh init_device_mesh torch distributed tensor distribute_tensor Partial Replicate Shard torch distributed tensor _api DTensor torch distributed tensor _dtensor_spec DTensorSpec torch distributed tensor parallel ColwiseParallel parallelize_module RowwiseParallel torch nn attention flex_attention BlockMask create_block_mask flex_attention torch testing _internal common_utils instantiate_parametrized_tests parametrize requires_cuda run_tests TestCase torch testing _internal distributed _tensor common_dtensor MLPModule torch testing _internal distributed fake_pg FakeStore torch utils _pytree register_pytree_node SimpleModel torch nn Module __init__ device super __init__ mlp_ = MLPModule device mlp_ = MLPModule device forward input mlp_ mlp_ input EinsumModel torch nn Module Simple model uses einsum DTensor inputs returns DTensor __init__ super __init__ placement = None forward x y z result = torch einsum bsh hd- bsd x y placement = result placements placement_ = y placements placement_ = z placements result SimpleModelDynamicShapes torch nn Module __init__ device super __init__ mlp_ = MLPModule device mlp_ = MLPModule device forward input input shape mlp_ input sin mlp_ input cos SimpleModelAnnotated torch nn Module __init__ device super __init__ mlp_ = MLPModule device mlp_ = MLPModule device forward input fx_traceback annotate pp_stage x = mlp_ input mlp_ x FlexAttentionModel torch nn Module __init__ device super __init__ proj_q = torch nn Linear device=device proj_k = torch nn Linear device=device proj_v = torch nn Linear device=device proj_out = torch nn Linear device=device num_heads = head_dim = forward x block_mask=None batch_size seq_len embed_dim = x shape Project Q K V q = proj_q x k = proj_k x v = proj_v x After colwise parallel q k v sharded last dimension Get actual size after sharding hidden_size = q shape - num_heads_local = hidden_size head_dim Reshape batch num_heads seq_len head_dim q = q view batch_size seq_len num_heads_local head_dim transpose k = k view batch_size seq_len num_heads_local head_dim transpose v = v view batch_size seq_len num_heads_local head_dim transpose Apply flex_attention attn_output_raw = flex_attention q k v block_mask=block_mask Reshape back batch seq_len hidden_size attn_output = attn_output_raw transpose contiguous view batch_size seq_len hidden_size Output projection output = proj_out attn_output output strict_export_and_aot_export_joint_with_descriptors model args kwargs=None kwargs None kwargs = needed stric export torch utils _pytree register_constant DTensorSpec install_free_tensors required dynamo work torch _dynamo config patch install_free_tensors=True inline_inbuilt_nn_modules=True torch _export utils _disable_aten_to_metadata_assertions ep = torch export export model args kwargs strict=True joint_gm produced here missing backward region due incompatiblility between ep module aot_export_joint_with_descriptors Keeping here show issue aot_export_joint_with_descriptors_alone ep module args kwargs graph_capture_and_aot_export_joint_with_descriptors_v model args kwargs=None kwargs None kwargs = gm = dynamo_graph_capture_for_export model args kwargs fake_mode = gm meta get fake_mode None tracing TracingContext fake_mode aot_export_joint_with_descriptors_alone gm args kwargs graph_capture_and_aot_export_joint_with_descriptors model args kwargs=None kwargs None kwargs = torch _dynamo config patch install_free_tensors=True TODO switch use official graph_capture API once ready gm = _dynamo_graph_capture_for_export model args kwargs fake_mode = gm meta get fake_mode None tracing TracingContext fake_mode aot_export_joint_with_descriptors_alone gm args kwargs aot_export_joint_with_descriptors_alone model args kwargs=None kwargs None kwargs = contextlib ExitStack stack joint_with_descriptors = aot_export_joint_with_descriptors stack model args kwargs joint_with_descriptors graph_module _count_op gm target sum node gm graph nodes node target == target register_pytree_node BlockMask BlockMask _flatten BlockMask _unflatten flatten_with_keys_fn=BlockMask _flatten_with_keys serialized_type_name= torch nn attention flex_attention BlockMask requires_cuda DTensorExportTest TestCase tearDown super tearDown dist destroy_process_group setUp super setUp world_size = store = FakeStore dist init_process_group backend= fake rank= world_size=self world_size store=store device_type = cuda _run_test export_fn test_annotation=False dp_degree = tp_degree = world_size dp_degree -D mesh dp tp mesh_ d = init_device_mesh device_type mesh_shape= dp_degree tp_degree mesh_dim_names= dp tp model = None test_annotation model = SimpleModelAnnotated device_type model = SimpleModel device_type parallelize_plan = mlp_ net ColwiseParallel mlp_ net RowwiseParallel mlp_ net ColwiseParallel mlp_ net RowwiseParallel tp_model = parallelize_module model mesh_ d tp parallelize_plan inp = torch rand device=self device_type inputs = distribute_tensor inp mesh_ d tp placements= Replicate joint_gm = export_fn tp_model inputs fw_gm bw_gm = min_cut_rematerialization_partition joint_gm None num_fwd_outputs= assertTrue _count_op joint_gm torch ops _c d_functional all_reduce default assertTrue _count_op fw_gm torch ops _c d_functional all_reduce default assertTrue _count_op bw_gm torch ops _c d_functional all_reduce default test_annotation has_tag node custom node meta node meta custom == pp_stage marked_nodes gm node name node gm graph nodes has_tag node node op == call_function unmarked_nodes gm node name node gm graph nodes has_tag node node op == call_function marked_nodes_fw = t addmm view relu view_ t_ div addmm_ all_reduce wait_tensor view_ t_ unmarked_nodes_fw = view_ t_ addmm_ view_ relu_ view_ t_ div_ addmm_ all_reduce_ wait_tensor_ view_ t_ t_ marked_nodes_bw = mm_ t_ view_ mm_ t_ sum_ view_ t_ detach detach_ threshold_backward_ t_ mm_ t_ sum_ view_ t_ unmarked_nodes_bw = mm t_ view_ mm_ t_ sum_ view_ t_ detach_ detach_ threshold_backward mm_ t_ mm_ t_ sum_ view_ t_ all_reduce_ wait_tensor_ assertEqual marked_nodes fw_gm marked_nodes_fw assertEqual unmarked_nodes fw_gm unmarked_nodes_fw assertEqual marked_nodes bw_gm marked_nodes_bw assertEqual unmarked_nodes bw_gm unmarked_nodes_bw assertEqual set marked_nodes joint_gm set marked_nodes_fw + marked_nodes_bw assertEqual set unmarked_nodes joint_gm set unmarked_nodes_fw + unmarked_nodes_bw parametrize export_fn graph_capture_and_aot_export_joint_with_descriptors_v graph_capture_and_aot_export_joint_with_descriptors aot_export_joint_with_descriptors_alone test_export_parallelize_module_with_dtensor_input export_fn _run_test export_fn aot_export_joint_with_descriptors strict-exported exported_program module producing joint graph backward region missing unittest expectedFailure test_strict_export_parallelize_module_with_dtensor_input _run_test strict_export_and_aot_export_joint_with_descriptors test_annotate_aot_export_joint_with_descriptors_alone _run_test aot_export_joint_with_descriptors_alone True parametrize export_fn_with_answer graph_capture_and_aot_export_joint_with_descriptors_v s s graph_capture_and_aot_export_joint_with_descriptors s s test_dynamic_shapes export_fn_with_answer export_fn answer = export_fn_with_answer dp_degree = tp_degree = world_size dp_degree -D mesh dp tp mesh_ d = init_device_mesh device_type mesh_shape= dp_degree tp_degree mesh_dim_names= dp tp model = SimpleModelDynamicShapes device_type parallelize_plan = mlp_ net ColwiseParallel mlp_ net RowwiseParallel mlp_ net ColwiseParallel mlp_ net RowwiseParallel tp_model = parallelize_module model mesh_ d tp parallelize_plan inp = torch rand device=self device_type inp_dtensor = distribute_tensor inp mesh_ d tp placements= Replicate torch _dynamo mark_dynamic inp_dtensor min= max= inputs = inp_dtensor joint_gm = export_fn tp_model inputs res = node joint_gm graph nodes node op == placeholder assert val node meta fake_val = node meta val isinstance fake_val torch _subclasses fake_tensor FakeTensor res append list fake_val shape assertEqual str res answer parametrize export_fn dynamo_graph_capture_for_export _dynamo_graph_capture_for_export test_einsum_dtensor_export export_fn Test exporting model einsum has DTensor inputs outputs side effects world_size = Create device mesh device_mesh = init_device_mesh device_type mesh_shape= world_size model = EinsumModel x = torch randn x_dtensor = distribute_tensor x device_mesh placements= Shard y replicated y = torch randn z = torch randn y_dtensor = distribute_tensor y device_mesh placements= Replicate z_dtensor = DTensor from_local z device_mesh placements= Partial inputs = x_dtensor y_dtensor z_dtensor Run model verify works output = model inputs torch _dynamo config patch install_free_tensors= export_fn _dynamo_graph_capture_for_export TODO switch use official graph_capture API once ready gm = export_fn model inputs output_gm = gm inputs assertEqual output output_gm parametrize export_fn graph_capture_and_aot_export_joint_with_descriptors_v graph_capture_and_aot_export_joint_with_descriptors test_flex_attention_dtensor_export export_fn device_mesh = init_device_mesh device_type mesh_shape= world_size model = FlexAttentionModel device_type Parallelize model shard head dimension proj_q proj_k proj_v colwise parallel output sharded head dimension proj_out rowwise parallel input sharded output needs reduction parallelize_plan = proj_q ColwiseParallel proj_k ColwiseParallel proj_v ColwiseParallel proj_out RowwiseParallel tp_model = parallelize_module model device_mesh parallelize_plan batch_size = seq_len = embed_dim = num_heads = Input tensor replicated across all devices inp = torch randn batch_size seq_len embed_dim device=self device_type inputs = distribute_tensor inp device_mesh placements= Replicate causal_mask b h q_idx kv_idx q_idx = kv_idx block_mask = create_block_mask causal_mask batch_size num_heads seq_len seq_len device=self device_type flex_kwargs = block_mask block_mask joint_gm = export_fn tp_model inputs flex_kwargs assertTrue _count_op joint_gm torch ops higher_order flex_attention assertTrue _count_op joint_gm torch ops higher_order flex_attention_backward test_union_typed_annotation fn leaf torch Tensor &#124; DTensor nest_fn leaf torch Tensor &#124; DTensor nest_fn leaf Union torch Tensor DTensor works isinstance leaf DTensor leaf = leaf to_local leaf nest_fn leaf + z = torch randn gm = graph_capture_and_aot_export_joint_with_descriptors fn z assertEqual fn z gm z instantiate_parametrized_tests DTensorExportTest __name__ == __main__ run_tests