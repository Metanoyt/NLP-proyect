PYTEST_DONT_REWRITE prevents pytest rewriting assertions which interferes test_sym_bool Owner s oncall export copy io math tempfile unittest zipfile collections namedtuple pathlib Path typing NamedTuple torch testing _internal inductor_utils GPU_TYPE HAS_GPU torch testing _internal triton_utils requires_gpu HAS_GPU triton triton language tl torch library wrap_triton torch utils _triton has_triton torch torch _dynamo torchdynamo torch _export serde schema schema torch export _trace torch utils _pytree pytree torch _export db case ExportCase SupportLevel torch _export db examples all_examples torch _export serde schema ArgumentKind torch _export serde serialize _dict_to_dataclass _to_json_bytes canonicalize deserialize ExportedProgramDeserializer ExportedProgramSerializer GraphModuleSerializer serialize SerializeError torch _higher_order_ops torchbind enable_torchbind_tracing torch _subclasses fake_tensor FakeTensor FakeTensorMode torch export Dim export load save unflatten torch export pt _archive constants ARCHIVE_VERSION_PATH torch fx experimental symbolic_shapes is_concrete_int ValueRanges torch testing _internal common_utils instantiate_parametrized_tests IS_FBCODE IS_MACOS IS_WINDOWS parametrize run_tests TemporaryFileName TestCase torch testing _internal torchbind_impls init_torchbind_implementations get_filtered_export_db_tests name case name case all_examples items case support_level == SupportLevel SUPPORTED unittest skipIf torchdynamo is_dynamo_supported dynamo doesn t support TestSerialize TestCase test_export_with_extension_op_serialization TestModule torch nn Module forward x x + x FooExtensionOp __hash__ __eq__ other type other type __call__ args kwargs torch ops aten add Tensor args kwargs property __name__ foo my_op ExtensionVerifier torch _export verifier Verifier dialect = FOO allowed_op_types super allowed_op_types + FooExtensionOp FooExtensionHandler torch _export serde serialize ExtensionHandler classmethod namespace cls foo classmethod to_op_name cls op my_op classmethod from_op_name cls name str assertEqual name my_op FooExtensionOp classmethod op_schema cls op torch ops aten add Tensor _schema inp = torch ones ep = export TestModule inp strict=True Register custom op handler foo_custom_op = FooExtensionOp torch _export serde serialize register_extension FooExtensionOp FooExtensionHandler new_gm = copy deepcopy ep graph_module Inject custom operator node new_gm graph nodes node name == add node target = foo_custom_op new_ep = ep _update new_gm ep graph_signature verifiers= ExtensionVerifier serialized = serialize new_ep deserialized = deserialize serialized assertEqual len deserialized graph find_nodes op= call_function target=foo_custom_op test_predispatch_export_with_autograd_op Foo torch nn Module __init__ - None super __init__ forward x torch enable_grad x + x inp = torch ones torch no_grad torch export _trace _export ep = _export Foo inp pre_dispatch=True buffer = io BytesIO torch export save ep buffer buffer seek loaded_ep = torch export load buffer exp_out = ep module inp actual_out = loaded_ep module inp assertEqual exp_out actual_out assertEqual exp_out requires_grad actual_out requires_grad test_export_example_inputs_preserved MyModule torch nn Module A test module has multiple args uses kwargs __init__ - None super __init__ p = torch nn Parameter torch ones forward x y use_p=False out = x + y use_p out += p out model = MyModule eval random_inputs = torch rand torch rand exp_program = export model random_inputs use_p True strict=True output_buffer = io BytesIO Tests example inputs preserved when saving loading module torch export save exp_program output_buffer loaded_model = torch export load output_buffer Extract example inputs before after saving orig_args orig_kwargs = exp_program example_inputs loaded_args loaded_kwargs = loaded_model example_inputs Run both modules confirm outputs match orig_out = exp_program module orig_args orig_kwargs loaded_out = loaded_model module loaded_args loaded_kwargs assertEqual orig_out loaded_out test_metadata_run_decomp_serder M torch nn Module forward x x sin exp_program = export M torch randn strict=True output_buffer = io BytesIO Tests example forward arg names preserved when saving loading module torch export save exp_program output_buffer loaded_model = torch export load output_buffer ep = loaded_model run_decompositions We should preserve original module name assertExpectedInline str ep graph_module code strip \ forward x sin = torch ops aten sin default x x = None sin test_metadata_parsing_with_layer_split Tests modules more complicated layer patterns can serialized deserialized correctly MyModule torch nn Module __init__ - None super __init__ layers = torch nn Sequential torch nn SiLU torch nn SiLU torch nn SiLU forward x Splitting layers sequential stack introduces commas parens into metadata trace out_start out_rest = layers layers h = out_start x h = out_rest h h inp = torch ones Module will only able roundtrip metadata can correctly parsed ep = export MyModule inp strict=True buffer = io BytesIO save ep buffer loaded_ep = load buffer Check both modules run confirm load successful exp_out = ep module inp actual_out = loaded_ep module inp assertEqual exp_out actual_out test_nested_layer_split Bar torch nn Module __init__ - None super __init__ layers = torch nn Sequential torch nn SiLU torch nn SiLU torch nn SiLU forward x out_start out_rest = layers layers h = out_start x h = out_rest h + h Foo torch nn Module __init__ - None super __init__ register_module Bar register_module b Bar register_buffer c torch randn forward x out_a out_b = getattr getattr b out_c = getattr c h = out_a x h = out_b h h + out_c inp = torch ones ep = export Foo inp strict=True buffer = io BytesIO save ep buffer loaded_ep = load buffer Check both modules run confirm load successful exp_out = ep module inp actual_out = loaded_ep module inp assertEqual exp_out actual_out test_serialize_param_mutation Foo torch nn Module __init__ super __init__ parameter = torch nn Parameter torch ones forward x torch no_grad parameter div_ x + parameter foo = Foo ep = torch export export foo torch rand run_decompositions buffer = io BytesIO save ep buffer loaded_ep = load buffer val = loaded_ep graph_signature parameters_to_mutate assertEqual div parameter val test_serialize_constant_outputs MyModule torch nn Module __init__ - None super __init__ forward x Along tensor output Nonetype constant Although these outputs aren t very useful they do show up graphs x + None Check module can roundtripped thereby confirming proper deserialization inp = torch ones ep = export MyModule inp strict=True buffer = io BytesIO save ep buffer loaded_ep = load buffer exp_out = ep module inp actual_out = loaded_ep module inp assertEqual exp_out actual_out test_serialize_multiple_returns_from_node - None MyModule torch nn Module __init__ - None super __init__ forward x w b torch nn functional layer_norm x x size weight=w bias=b eps= e- exported_module = export MyModule torch ones requires_grad=True torch ones torch ones strict=True run_decompositions serialized = ExportedProgramSerializer serialize exported_module node = serialized exported_program graph_module graph nodes - assertEqual node target torch ops aten native_layer_norm default aten native_layer_norm returns tensors assertEqual len node outputs check names unique seen = set output node outputs name = output as_tensor name assertNotIn name seen seen add name test_serialize_sym_int - None DynamicShapeSimpleModel torch nn Module __init__ super __init__ forward b c - torch Tensor d = torch matmul b + c d_s = d shape d_s = d shape d_s = d_s d_s e = d view d_s torch cat e e inputs = torch randn torch randn torch randn dim _ac = torch export Dim dim _ac dim _bc = torch export Dim dim _b dynamic_shapes = dim _ac b dim _bc c dim _ac dim _bc exported_module = export DynamicShapeSimpleModel inputs dynamic_shapes=dynamic_shapes strict=True run_decompositions serialized = ExportedProgramSerializer serialize exported_module sym_size_nodes = node node serialized exported_program graph_module graph nodes node target == torch ops aten sym_size int node sym_size_nodes assertEqual node inputs name assertEqual node inputs name dim test_serialize_sym_float - None TODO rec This doesn t seem test anything DynamicFloatSimpleModel torch nn Module __init__ multiplier torch SymFloat super __init__ multiplier = multiplier forward b c - torch Tensor d = torch matmul b + c e = d multiplier e_s = e shape e_s = e shape e_s = e_s e_s f = e view e_s torch cat f f multiplier_sym = torch SymFloat multiplier_sym _model = DynamicFloatSimpleModel multiplier_sym _inputs = torch randn torch randn torch randn _dim _ac = Dim dim _ac _dim _bc = Dim dim _b test_serialize_infinite_sym_int - None DynamicShapeSimpleModel torch nn Module __init__ super __init__ forward b c - torch Tensor d = torch matmul b + c d_s = d shape d_s = d shape d_s = d_s d_s e = d view d_s torch cat e e inputs = torch randn torch randn torch randn dim _ac = torch export Dim dim _ac dim _bc = torch export Dim dim _b dynamic_shapes = dim _ac b dim _bc c dim _ac dim _bc exported_module = export DynamicShapeSimpleModel inputs dynamic_shapes=dynamic_shapes strict=True run_decompositions serialized = ExportedProgramSerializer serialize exported_module v serialized exported_program range_constraints values assertEqual v max_val None test_symint_list This reflects behavior inductor s ExternFallbackNode shape_env = torch fx experimental symbolic_shapes ShapeEnv symint = shape_env create_unbacked_symint serializer = GraphModuleSerializer None None type ignore arg-type res = serializer serialize_inputs torch ops aten ones default symint assertEqual len res assertEqual res arg _type as_sym_ints test_serialize_list_returns - None MyModule torch nn Module __init__ - None super __init__ forward x torch split x input = torch arange reshape exported_module = export MyModule input strict=True run_decompositions serialized = ExportedProgramSerializer serialize exported_module node = serialized exported_program graph_module graph nodes - split Tensor gets decomposed split_with_sizes core ATen decomposition table assertEqual node target torch ops aten split_with_sizes default assertEqual len node outputs Input looks like tensor Output looks like tensor tensor tensor assertEqual len node outputs as_tensors check names unique seen = set output node outputs as_tensors name = output name assertNotIn name seen seen add name test_nonfinite_inputs - None Module torch nn Module forward x x = torch ops aten add Scalar x math inf x = torch ops aten add Scalar x -math inf torch ops aten add Scalar x math nan fn = Module ep = torch export export fn torch randn json_bytes = _to_json_bytes ExportedProgramSerializer serialize ep exported_program json parse_constant x raise RuntimeError f Invalid JSON float x json loads json_bytes parse_constant=parse_constant test_multi_return_some_unused - None Make sure serialized output matches op schema even some arguments never used graph MyModule torch nn Module __init__ - None super __init__ forward x torch ops aten var_mean correction x exported_module = export MyModule torch ones requires_grad=True strict=True run_decompositions serialized = ExportedProgramSerializer serialize exported_module node = serialized exported_program graph_module graph nodes - assertEqual node target torch ops aten var_mean correction assertEqual len node outputs check names unique seen = set output node outputs name = output as_tensor name assertNotIn name seen seen add name test_rational_ranges - None M torch nn Module forward x x + x ep = export M torch randn dynamic_shapes= Dim temp strict=True range_constraints = list ep range_constraints keys assert len range_constraints == symint = range_constraints sympy upper_range = sympy Rational lower_range = sympy Rational ep range_constraints symint = ValueRanges lower=lower_range upper=upper_range serialized = ExportedProgramSerializer serialize ep assertEqual serialized exported_program range_constraints symint name min_val assertEqual serialized exported_program range_constraints symint name max_val unittest skipIf torch cuda is_available has_triton requires cuda triton test_triton_hop - None triton jit add_kernel in_ptr in_ptr out_ptr n_elements fval ival BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = x + y + fval + ival tl store out_ptr + offsets output mask=mask custom_add x torch Tensor y torch Tensor - torch Tensor output = torch empty_like x n_elements = output numel grid meta triton cdiv n_elements meta BLOCK_SIZE wrap_triton add_kernel grid x y output n_elements BLOCK_SIZE= output MyModel torch nn Module forward x y custom_add x y custom_add_autotune x torch Tensor y torch Tensor - torch Tensor output = torch empty_like x n_elements = output numel grid meta triton cdiv n_elements meta BLOCK_SIZE wrap_triton add_kernel grid x y output n_elements BLOCK_SIZE= num_warps= output MyModelAutotune torch nn Module forward x y custom_add_autotune x y device = cuda m MyModel device MyModelAutotune device args = torch randn device=device torch randn device=device ep = torch export export m args=args ep = ep run_decompositions decompose_custom_triton_ops=False assert torch allclose m args ep module args serialized = ExportedProgramSerializer serialize ep node serialized exported_program graph_module graph nodes node target == torch ops higher_order triton_kernel_wrapper_functional triton_node = node assertIsNotNone triton_node args = kwargs = arg triton_node inputs arg kind == ArgumentKind POSITIONAL args append arg arg arg kind == ArgumentKind KEYWORD kwargs arg name = arg arg assertEqual len args Always name grid output_indices num_warps Triton version dependent num_cpu_threads shared_memory_bytes assertTrue len kwargs = i range assertIsNotNone args i as_tensor assertEqual args as_int assertAlmostEqual args as_float places= assertEqual args as_int kernel_name = kwargs name as_string symbol_name = kernel_name rpartition _ assertEqual symbol_name add_kernel assertEqual kwargs grid as_ints assertEqual kwargs output_indices as_ints assertEqual kwargs num_warps as_int isinstance m MyModelAutotune num_cpu_threads kwargs assertEqual kwargs num_cpu_threads as_int shared_memory_bytes kwargs assertEqual kwargs shared_memory_bytes as_int assertEqual len triton_node outputs assertIsNotNone triton_node outputs as_tensors assertEqual len triton_node outputs as_tensors len kwargs output_indices as_ints assertEqual triton_node outputs as_tensors name getitem assertRaisesRegex SerializeError deserialize nyi torch _higher_order_ops triton_kernel_wrap triton_kernel_wrapper_functional ExportedProgramDeserializer deserialize serialized exported_program serialized state_dict serialized constants serialized example_inputs test_kwargs_default - None Tests kwargs default values serialized even they specified Foo torch nn Module forward x torch Tensor - torch Tensor values = torch randn torch searchsorted x values side= right right=True f = Foo x _ = torch sort torch randn exported_module = export f x strict=True run_decompositions serialized = ExportedProgramSerializer serialize exported_module node = serialized exported_program graph_module graph nodes - assertEqual node target torch ops aten searchsorted Tensor assertEqual len node inputs assertEqual node inputs name right assertEqual node inputs arg as_bool True assertEqual node inputs name side assertEqual node inputs arg as_string right test_canonicalize - None Module torch nn Module forward x torch Tensor y torch Tensor - torch Tensor = y + x b = x + y b + ep = export Module torch randn torch randn strict=True s = ExportedProgramSerializer serialize ep c = canonicalize s exported_program g = c graph_module graph assertLess g nodes inputs arg as_tensor name g nodes inputs arg as_tensor name test_int_list - None M torch nn Module forward x torch ops aten sum dim_IntList x ep = torch export export M torch randn strict=True serialized = ExportedProgramSerializer serialize ep node serialized exported_program graph_module graph nodes aten sum dim_IntList node target assertEqual node inputs arg type as_ints test_empty_constant - None M torch nn Module __init__ super __init__ linear = torch nn Linear forward x linear x m = M sample_inputs = torch randn eager_out = m sample_inputs ep = torch export export m sample_inputs buffer = io BytesIO torch export save ep buffer buffer seek loaded_ep = torch export load buffer ep_out = loaded_ep module sample_inputs assertTrue torch allclose eager_out ep_out assertEqual len loaded_ep constants test_empty_state_dict - None M torch nn Module __init__ super __init__ const = torch randn forward x x + const m = M sample_inputs = torch randn eager_out = m sample_inputs ep = torch export export m sample_inputs buffer = io BytesIO torch export save ep buffer buffer seek loaded_ep = torch export load buffer ep_out = loaded_ep module sample_inputs assertTrue torch allclose eager_out ep_out assertEqual len loaded_ep state_dict test_preserve_aliasing - None M torch nn Module __init__ super __init__ linear = torch nn Linear linear = linear alias linear register_buffer buffer torch randn register_buffer buffer torch randn persistent=False const = torch ones const = const diagonal partial view const forward x linear x + linear x + buffer + buffer + const + const m = M sample_inputs = torch randn ep = torch export export m sample_inputs buffer = io BytesIO torch export save ep buffer buffer seek loaded_ep = torch export load buffer eager_out = m sample_inputs epm = loaded_ep module ep_out = epm sample_inputs assertTrue torch allclose eager_out ep_out loaded_ep should preserve aliasing info assertEqual loaded_ep state_dict linear weight untyped_storage loaded_ep state_dict linear weight untyped_storage assertEqual loaded_ep state_dict linear bias untyped_storage loaded_ep state_dict linear bias untyped_storage assertEqual loaded_ep constants const untyped_storage loaded_ep constants const untyped_storage verify const const share same storage loaded_ep constants const = assertEqual loaded_ep constants const loaded_ep constants const - = assertEqual loaded_ep constants const - - unlifted module should also preserve aliasing info epm = loaded_ep module epm_state_dict = epm state_dict assertEqual epm_state_dict linear weight untyped_storage epm_state_dict linear weight untyped_storage assertEqual epm_state_dict linear bias untyped_storage epm_state_dict linear bias untyped_storage assertEqual epm const untyped_storage epm const untyped_storage verify const const share same storage epm const = assertEqual epm const epm const - = assertEqual epm const - - test_storage_offset - None M torch nn Module __init__ super __init__ const = torch arange linear = torch nn Linear forward x linear x + const m = M sample_inputs = torch randn ep = torch export export m sample_inputs buffer = io BytesIO save ep buffer buffer seek loaded_ep = load buffer assertEqual m sample_inputs loaded_ep module sample_inputs test_ D_tensor_slicing - None M torch nn Module __init__ super __init__ const = torch arange forward x x + const m = M sample_inputs = torch randn ep = torch export export m sample_inputs buffer = io BytesIO save ep buffer buffer seek loaded_ep = load buffer assertEqual m sample_inputs loaded_ep module sample_inputs test_ D_tensor_slicing - None M torch nn Module __init__ super __init__ const = torch randn forward x x + const m = M sample_inputs = torch randn ep = torch export export m sample_inputs buffer = io BytesIO save ep buffer buffer seek loaded_ep = load buffer assertEqual m sample_inputs loaded_ep module sample_inputs test_non_float_weight - None M torch nn Module __init__ super __init__ p = torch nn Parameter torch ones dtype=torch int requires_grad=False forward x x + p m = M sample_inputs = torch randn ep = torch export export m sample_inputs buffer = io BytesIO save ep buffer buffer seek loaded_ep = load buffer assertEqual m sample_inputs loaded_ep module sample_inputs requires_gpu test_weight_sharing_gpu - None M torch nn Module __init__ super __init__ c = torch ones device=GPU_TYPE c = c linear = torch nn Linear forward x linear x + c + c m = M GPU_TYPE sample_inputs = torch randn device=GPU_TYPE ep = torch export export m sample_inputs Check c c share same storage assertEqual ep constants c untyped_storage ep constants c untyped_storage buffer = io BytesIO save ep buffer buffer seek loaded_ep = load buffer Check c c share same storage after serdes assertEqual loaded_ep constants c untyped_storage loaded_ep constants c untyped_storage assertEqual m sample_inputs loaded_ep module sample_inputs test_complex_constant - None M torch nn Module forward x s = torch sin x y = + j s z = j s y z m = M sample_inputs = torch randn ep = torch export export m sample_inputs buffer = io BytesIO save ep buffer buffer seek loaded_ep = load buffer assertEqual m sample_inputs loaded_ep module sample_inputs unittest skipIf IS_WINDOWS Windows supported test unittest skipIf torchdynamo is_dynamo_supported dynamo doesn t support TestDeserialize TestCase setUp super setUp init_torchbind_implementations _check_graph_nodes gm gm _check_meta=True TODO The _check_meta flag bypasses checking source_fn nn_module_stack there issue roundtripping source_fn value torch ops map nodes original source_fn functorch experimental _map MapWrapper object x f deserialized source_fn functorch experimental _map map assertEqual len gm graph nodes len gm graph nodes node node zip gm graph nodes gm graph nodes assertEqual node op node op node op == call_function Check val metadata val = node meta get val None val = node meta get val None assertEqual len node args len node args assertEqual set node kwargs keys set node kwargs keys val None val None Either both None assertEqual val val isinstance val FakeTensor isinstance val FakeTensor Or both fake tensors same shape dtype assertEqual len val shape len val shape s s zip val shape val shape is_concrete_int s is_concrete_int s assertEqual s s assertEqual str s str s assertEqual val dtype val dtype isinstance val list tuple isinstance val list tuple Or both fake tensors lists one element same shape dtype v v zip pytree tree_leaves val pytree tree_leaves val isinstance v FakeTensor assertEqual v shape v shape assertEqual v dtype v dtype For expressions like s can only compare through string assertEqual str val str val Check stack_trace metadata assertEqual node meta get stack_trace None node meta get stack_trace None node target == torch ops higher_order cond true_graph = getattr gm node args target true_graph = getattr gm node args target _check_graph_nodes true_graph true_graph false_graph = getattr gm node args target false_graph = getattr gm node args target _check_graph_nodes false_graph false_graph node target == torch ops higher_order map_impl map_graph = getattr gm node args target map_graph = getattr gm node args target _check_graph_nodes map_graph map_graph False _check_meta node op get_attr placeholder output Check nn_module_stack metadata assertEqual node meta get nn_module_stack None node meta get nn_module_stack None Check source_fn_stack metadata assertEqual node meta get source_fn_stack None node meta get source_fn_stack None check_graph fn inputs dynamic_shapes=None _check_meta=True use_pre_dispatch=True strict=True - None Export graph serialize deserialize compare results _deepcopy_inputs inputs copy deepcopy deepcopy can fail tensor inputs have attribute i e __dict__ we remove __dict__ when deepcopying dict_mapping = dict inputs_clone = idx i enumerate inputs isinstance i torch Tensor hasattr inputs __dict__ dict_mapping idx = i __dict__ i __dict__ = inputs_clone += copy deepcopy i Add __dict__ back k v dict_mapping items inputs k __dict__ = v inputs_clone k __dict__ = v inputs_clone _check_graph pre_dispatch pre_dispatch ep = torch export export fn _deepcopy_inputs inputs dynamic_shapes=dynamic_shapes strict=strict We should have branch because PT Inference goes through private export API ep = torch export _trace _export fn _deepcopy_inputs inputs dynamic_shapes=dynamic_shapes strict=strict pre_dispatch=False ep graph eliminate_dead_code serialized_artifact = serialize ep opset_version= aten deserialized_ep = deserialize serialized_artifact expected_opset_version= aten deserialized_ep graph eliminate_dead_code orig_outputs = ep module _deepcopy_inputs inputs loaded_outputs = deserialized_ep module _deepcopy_inputs inputs flat_orig_outputs = pytree tree_leaves orig_outputs flat_loaded_outputs = pytree tree_leaves loaded_outputs orig loaded zip flat_orig_outputs flat_loaded_outputs assertEqual type orig type loaded torch allclose doesn t work float isinstance orig torch Tensor orig dtype torch float _e m fn torch float _e m orig is_meta assertEqual orig loaded assertTrue torch allclose orig loaded assertEqual orig loaded _check_graph_nodes ep graph_module deserialized_ep graph_module _check_meta use_pre_dispatch _check_graph pre_dispatch=True _check_graph pre_dispatch=False _check_graph pre_dispatch=False test_optional_tuple torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor Tensor b Tensor c - Tensor Tensor tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch library register_fake mylib foo foo_impl b c res = None c None res = c + + b + b res M torch nn Module forward b c torch ops mylib foo b c check_graph M torch randn torch randn torch randn test_unbacked_bindings_serialize torch _export utils _get_shape_env_from_gm torch utils _sympy symbol prefix_str symbol_is_type SymT M torch nn Module forward x y x += n = x item n = n + y item n + inps = torch tensor torch tensor _strict True False ep = torch export export M inps strict=_strict run_decompositions check bindings after deserialization buffer = io BytesIO save ep buffer buffer seek loaded_ep = load buffer bound = set old_node new_node zip ep graph nodes loaded_ep graph nodes assertEqual unbacked_bindings old_node meta unbacked_bindings new_node meta bound update new_node meta get unbacked_bindings check ShapeEnv counters shape_env = _get_shape_env_from_gm loaded_ep graph_module next_index = shape_env unbacked_symint_counter shape_env unbacked_symint_counter += symbol bound assertTrue symbol_is_type symbol SymT UNBACKED_INT assertTrue int str symbol len prefix_str SymT UNBACKED_INT next_index test_sym_bool_dynamic_shapes - None MyModule torch nn Module __init__ - None super __init__ forward x y z = x -y shape z inputs = torch ones torch ones dynamic_shapes = x y Dim seqlen max= Compile dynamic_shapes set get operator neg involved check_graph MyModule inputs dynamic_shapes=dynamic_shapes test_auto_functionalize torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x Tensor y Tensor b z SymInt w Tensor n - Tensor tags=torch Tag pt _compliant_tag lib=lib torch library define mylib foo Tensor x Tensor y Tensor b z SymInt w Tensor n - Tensor Tensor tags=torch Tag pt _compliant_tag lib=lib torch library define mylib foo Tensor x Tensor y Tensor b z SymInt w Tensor n - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch library register_fake mylib foo foo _impl x y z w n x add_ y + w z add_ y + n n + n torch library impl mylib foo cpu lib=lib torch library register_fake mylib foo foo _impl x y z w n x add_ y + w z add_ y + n n + n n n torch library impl mylib foo cpu lib=lib torch library register_fake mylib foo foo _impl x y z w n x add_ y + w z add_ y + n M torch nn Module forward x y z n n = torch ops mylib foo x y z n torch ops mylib foo x y z n torch ops mylib foo x y z n x = torch randn y = torch randn torch randn z = torch randn n = torch randn orig_args = x y z n TODO Auto_functionalize supported pre_dispatch IR check_graph M orig_args use_pre_dispatch=False test_hoo_symint_input Mod torch nn Module __init__ super __init__ forward b c num = c item torch cond pred=torch tensor True true_fn=lambda b + b + num false_fn=lambda b - b - num operands= b inp = torch ones torch ones torch tensor check_graph Mod inp use_pre_dispatch=False test_none_input Testing backwards-compatibility breakage where old models do have input spec node name M torch nn Module forward x y z x + z ep = torch export export M torch ones None torch ones serialized_program = ExportedProgramSerializer None serialize ep serialized_program exported_program graph_module signature input_specs = schema InputSpec create user_input=schema UserInputSpec arg=schema Argument create as_none=True ep = ExportedProgramDeserializer None deserialize serialized_program exported_program ep graph_module recompile unflattened = torch export unflatten ep inp = torch rand None torch rand assertEqual unflattened inp M inp test_multi_return - None Test multiple single node ex layer_norm has outputs MyModule torch nn Module __init__ - None super __init__ forward x w b torch nn functional layer_norm x x size weight=w bias=b eps= e- inputs = torch ones requires_grad=True torch ones torch ones check_graph MyModule inputs test_basic - None MyModule torch nn Module __init__ - None super __init__ forward x x = x + x x = x x x = x x x x clone inputs = torch ones requires_grad=True check_graph MyModule inputs test_dynamic - None DynamicShapeSimpleModel torch nn Module __init__ - None super __init__ forward b c - torch Tensor d = torch matmul b + c d_s = d shape d_s = d shape d_s = d_s d_s e = d view d_s torch cat e e inputs = torch randn torch randn torch randn dim _ac = torch export Dim dim _ac dynamic_shapes = dim _ac b None c dim _ac check_graph DynamicShapeSimpleModel inputs dynamic_shapes test_sym_bool Module torch nn Module forward x y assert x size y x + y f = Module check_graph f torch ones torch ones test_sym_bool_torch_check_equal Module torch nn Module forward x y = x nonzero z = y size torch _check z == y check_graph Module torch Tensor test_sym_int_torch_check_equal Module torch nn Module forward x y = x nonzero z = y size torch _check z == torch _check z == y check_graph Module torch Tensor test_shape Foo torch nn Module forward x z y = x size z + y + x z inputs = torch ones dim _x dim _x = torch export dims dim _x dim _x dynamic_shapes = x dim _x dim _x check_graph Foo inputs dynamic_shapes test_module M torch nn Module __init__ - None super __init__ linear = torch nn Linear relu = torch nn ReLU linear = torch nn Linear forward x x = linear x x = linear x x = torch nn functional relu x x = linear x x inputs = torch randn check_graph M inputs test_module_meta M torch nn Module __init__ - None super __init__ p = torch nn Parameter torch ones forward x p + x torch device meta mod = M inputs = torch randn device= meta check_graph mod inputs test_pytree_namedtuple N = namedtuple N b N NamedTuple torch Tensor b torch Tensor M torch nn Module forward x y N x + y x b y b pytree _register_namedtuple N serialized_type_name= test export test_serialize test_pytree_namedtuple N pytree _register_namedtuple N serialized_type_name= test export test_serialize test_pytree_namedtuple N inp = N torch randn torch randn N torch randn torch randn ep = torch export export M inp ep example_inputs = None Can t pickle input since namedtuple global namespace serialized = ExportedProgramSerializer serialize ep assertEqual len serialized exported_program graph_module treespec_namedtuple_fields deserialized = ExportedProgramDeserializer deserialize serialized exported_program serialized state_dict serialized constants assertTrue treespec_namedtuple_fields deserialized graph_module meta assertEqual deserialized graph_module meta treespec_namedtuple_fields test export test_serialize test_pytree_namedtuple N b test export test_serialize test_pytree_namedtuple N b unlifted = deserialized module assertTrue treespec_namedtuple_fields unlifted meta assertEqual len unlifted meta treespec_namedtuple_fields unflattened = unflatten deserialized assertTrue treespec_namedtuple_fields unflattened meta assertEqual len unflattened meta treespec_namedtuple_fields test_cond functorch experimental control_flow cond inputs = torch ones torch zeros M torch nn Module forward x y t x y x + y f x y x - y cond x t f x y check_graph M inputs test_sym_float M torch nn Module forward x b = x item b check_graph M torch tensor test_arg_from M torch nn Module __init__ super __init__ register_buffer compress_weight torch ones register_buffer compress_bias torch ones forward - None compress_weight None compress_bias None torch nn init kaiming_uniform_ compress_weight a=math sqrt fan_in _ = torch nn init _calculate_fan_in_and_fan_out compress_weight bound = math sqrt fan_in fan_in torch nn init uniform_ compress_bias -bound bound torch no_grad check_graph M test_map functorch experimental control_flow f x y x + y Module torch nn Module forward xs y control_flow map f xs y g = Module inputs = torch ones torch ones check_graph g inputs _check_meta=False test_positional_argument_with_default_value MyLinear torch nn Module __init__ - None super __init__ weight = torch randn bias = torch randn forward x bias has default value here should preserved positional argument torch ops aten linear default x weight bias check_graph MyLinear torch randn test_tensor_tensor_list torch library _scoped_library _export FRAGMENT lib lib define _test_tensor_tensor_list_output Tensor x Tensor y - Tensor Tensor tags=torch Tag pt _compliant_tag _test_tensor_tensor_list_output x y y x lib impl _test_tensor_tensor_list_output _test_tensor_tensor_list_output CPU lib impl _test_tensor_tensor_list_output _test_tensor_tensor_list_output Meta M torch nn Module forward x y b = torch ops _export _test_tensor_tensor_list_output default x y + b check_graph M torch rand torch rand test_list_of_optional_tensors - None MyModule torch nn Module __init__ - None super __init__ forward x y z indices = None None torch tensor indexed = torch ops aten index Tensor x + y indices indexed + z inputs = torch rand torch rand torch rand check_graph MyModule inputs test_sym_ite Foo torch nn Module forward x b = x shape == ret = torch sym_ite b x shape x shape ret dynamic_shapes = x Dim dim Dim dim check_graph Foo torch ones dynamic_shapes=dynamic_shapes test_multiple_getitem M torch nn Module forward x b = torch topk x = b ep = torch export export M torch ones strict=True insert another getitem node node ep graph nodes node op == call_function node target == torch ops aten mul Tensor getitem_ = node args ep graph inserting_before getitem_ getitem_copy = ep graph node_copy getitem_ mul_node = ep graph call_function torch ops aten mul Tensor getitem_copy mul_node meta = copy copy getitem_copy meta node args = getitem_ mul_node deserialized_ep = deserialize serialize ep inp = torch randn orig_res = ep module inp res = deserialized_ep module inp assertTrue torch allclose orig_res res assertTrue torch allclose orig_res res The deserialized graph should have deduped getitem calls assertExpectedInline deserialized_ep graph_module code strip \n \ forward x topk_default = torch ops aten topk default x x = None getitem = topk_default getitem_ = topk_default topk_default = None mul_tensor = torch ops aten mul Tensor getitem mul = torch ops aten mul Tensor getitem mul_tensor getitem = mul_tensor = None mul getitem_ parametrize name case get_filtered_export_db_tests name_fn=lambda name case f case_ name test_exportdb_supported name str case ExportCase - None model = case model _check_meta = map name torch _export config patch use_new_tracer_experimental=True check_graph model case example_args _check_meta=_check_meta test_constraints Module torch nn Module forward x y n = x item torch _check n = y sum + torch ones n sum f = Module check_graph f torch tensor torch randn test_get_attr - None Module torch nn Module forward x x + torch tensor f = Module check_graph f torch tensor test_get_attr_list - None Module torch nn Module forward x torch cat x torch tensor f = Module check_graph f torch tensor unittest skipIf torch cuda is_available Requires cuda test_device - None MyModule torch nn Module __init__ - None super __init__ conv = torch nn Conv d stride= bias=True relu = torch nn ReLU forward x conv = conv x relu = relu conv mul = relu mul inp = torch randn dtype=torch float cuda model = MyModule eval cuda check_graph model inp test_custom_obj_tuple_out MyModule torch nn Module __init__ - None super __init__ attr = torch classes _TorchScriptTesting _Foo forward x = torch ops _TorchScriptTesting takes_foo_tuple_return attr x y = + b = torch ops _TorchScriptTesting takes_foo attr y x + b m = MyModule inputs = torch ones check_graph m inputs strict=False test_custom_obj MyModule torch nn Module __init__ - None super __init__ attr = torch classes _TorchScriptTesting _Foo forward x = torch ops _TorchScriptTesting takes_foo attr x b = torch ops _TorchScriptTesting takes_foo attr x + b m = MyModule inputs = torch ones check_graph m inputs strict=False test_custom_obj_list_out MyModule torch nn Module __init__ - None super __init__ attr = torch classes _TorchScriptTesting _Foo forward x = torch ops _TorchScriptTesting takes_foo_list_return attr x y = + + b = torch ops _TorchScriptTesting takes_foo attr y x + b m = MyModule inputs = torch ones check_graph m inputs strict=False test_export_no_inputs M torch nn Module __init__ - None super __init__ p = torch ones forward p p ep = torch export export M strict=True ep _example_inputs = None roundtrip_ep = deserialize serialize ep assertTrue torch allclose ep module roundtrip_ep module test_serialize_float dtype torch float _e m torch float _e m fn MyModule torch nn Module forward x x dtype m = MyModule inputs = torch ones check_graph m inputs strict=False test_forward_compatibility assertEqual schema TensorArgument name= x _dict_to_dataclass schema TensorArgument shiny_new_field hello world name x instantiate_parametrized_tests TestDeserialize unittest skipIf torchdynamo is_dynamo_supported dynamo doesn t support TestSchemaVersioning TestCase test_error Module torch nn Module forward x x + x f = Module ep = export f torch randn strict=True serialized_program = ExportedProgramSerializer serialize ep serialized_program exported_program schema_version major = - assertRaisesRegex SerializeError r Serialized schema version does match our current ExportedProgramDeserializer deserialize serialized_program exported_program serialized_program state_dict serialized_program constants serialized_program example_inputs We didn t set up kwargs input yet unittest expectedFailure TestDeserialize test_exportdb_supported_case_fn_with_kwargs unittest skipIf torchdynamo is_dynamo_supported dynamo doesn t support TestSaveLoad TestCase test_save_buffer inp = torch tensor Module torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x x = x + y = x t y = y relu y = linear y y ep = export Module inp strict=True buffer = io BytesIO save ep buffer buffer seek loaded_ep = load buffer assertTrue torch allclose ep module inp loaded_ep module inp unittest skipIf IS_WINDOWS Cannot modify file windows test_save_file Foo torch nn Module forward x x x f = Foo inp = torch randn ep = export f inp strict=True tempfile NamedTemporaryFile suffix= pt f save ep f name f seek loaded_ep = load f name assertTrue torch allclose ep module inp loaded_ep module inp test_save_path Foo torch nn Module forward x y x + y f = Foo inp = torch tensor torch tensor ep = export f inp strict=True TemporaryFileName suffix= pt fname path = Path fname save ep path loaded_ep = load path assertTrue torch allclose ep module inp loaded_ep module inp test_save_extra inp = torch tensor Foo torch nn Module forward x x x + x f = Foo ep = export f inp strict=True buffer = io BytesIO save ep buffer extra_files= extra txt moo buffer seek extra_files = extra txt loaded_ep = load buffer extra_files=extra_files assertTrue torch allclose ep module inp loaded_ep module inp assertEqual extra_files extra txt moo unittest skipIf IS_FBCODE IS_MACOS IS_WINDOWS The file path different fbcode CI test_version_error Foo torch nn Module forward x x + x f = Foo ep = export f torch randn strict=True assertRaisesRegex ValueError r Saved archive version - does match our current tempfile NamedTemporaryFile suffix= pt f save ep f name f seek file_prefix = f name split split Create new file copy things over modify archive version tempfile NamedTemporaryFile suffix= pt fnew zipfile ZipFile f r zin zipfile ZipFile fnew w zout item zin infolist item filename = f file_prefix ARCHIVE_VERSION_PATH zout writestr item zin read item filename zout writestr f file_prefix ARCHIVE_VERSION_PATH - f seek load fnew name test_save_constants Foo torch nn Module __init__ - None super __init__ = torch tensor forward x list_tensor = torch tensor torch tensor x + + list_tensor + list_tensor ep = export Foo torch tensor strict=True buffer = io BytesIO save ep buffer buffer seek loaded_ep = load buffer inp = torch tensor assertTrue torch allclose ep module inp loaded_ep module inp test_save_load_with_multiple_empty_tensors - None Test scenario where models have multiple empty tensors different data types M torch nn Module __init__ super __init__ register_buffer int_buffer torch zeros dtype=torch uint register_buffer int_buffer torch zeros dtype=torch uint register_buffer float_buffer torch zeros dtype=torch float forward t torch Tensor - torch Tensor t + int_buffer + float_buffer + int_buffer m = M inp = torch rand ep = torch export export m inp buffer = io BytesIO torch export save ep buffer model_bytes = buffer getvalue First two buffers duplicates third one So serialized model there will two physical tensors assertTrue b weight_ model_bytes assertTrue b weight_ model_bytes assertFalse b weight_ model_bytes buffer = io BytesIO model_bytes buffer seek dep = torch export load buffer unf = torch export unflatten dep assertEqual unf int_buffer dtype torch uint assertEqual unf int_buffer dtype torch uint assertEqual unf float_buffer dtype torch float unittest skipIf torchdynamo is_dynamo_supported dynamo doesn t support TestSerializeCustomClass TestCase setUp super setUp init_torchbind_implementations test_custom_class custom_obj = torch classes _TorchScriptTesting _PickleTester Foo torch nn Module forward x x + x f = Foo inputs = torch zeros ep = export f inputs strict=True Replace one values instance our custom node ep graph nodes node op == call_function node target == torch ops aten add Tensor ep graph inserting_before node custom_node = ep graph call_function torch ops _TorchScriptTesting take_an_instance default custom_obj custom_node meta val = torch ones custom_node meta torch_fn = take_an_instance take_an_instance arg _ = node args node args = arg custom_node serialized_vals = serialize ep ep_str = serialized_vals exported_program decode utf- assert class_fqn ep_str assert custom_obj _type qualified_name ep_str deserialized_ep = deserialize serialized_vals node deserialized_ep graph nodes node op == call_function node target == torch ops _TorchScriptTesting take_an_instance default arg = node args assertTrue isinstance arg torch _C ScriptObject assertEqual arg _type custom_obj _type assertEqual arg __getstate__ custom_obj __getstate__ assertEqual arg top test_custom_class_containing_fake_tensor Foo torch nn Module __init__ - None super __init__ custom_obj = torch classes _TorchScriptTesting _ContainsTensor torch rand forward x x + custom_obj get FakeTensorMode f = Foo inputs = torch zeros enable_torchbind_tracing ep = export f inputs strict=False serialized_vals = serialize ep ep = deserialize serialized_vals assertTrue isinstance ep constants custom_obj get FakeTensor test_custom_class_input_to_function Foo torch nn Module __init__ - None super __init__ attr = torch classes _TorchScriptTesting _Foo forward x x + torch ops _TorchScriptTesting takes_foo attr x FakeTensorMode f = Foo inputs = torch zeros enable_torchbind_tracing ep = export f inputs strict=False serialized_vals = serialize ep ep = deserialize serialized_vals assertExpectedInline str ep graph_module code strip \ forward obj_attr x takes_foo = torch ops _TorchScriptTesting takes_foo default obj_attr x obj_attr = None add = torch ops aten add Tensor x takes_foo x = takes_foo = None add assertTrue isinstance ep constants attr torch ScriptObject gm = ep module assertExpectedInline str gm code strip \ forward x x = fx_pytree tree_flatten_spec x _in_spec attr = attr _guards_fn = _guards_fn x _guards_fn = None takes_foo = torch ops _TorchScriptTesting takes_foo default attr x attr = None add = torch ops aten add Tensor x takes_foo x = takes_foo = None pytree tree_unflatten add _out_spec assertTrue isinstance gm attr torch ScriptObject test_custom_tag_metadata_serialization Foo torch nn Module forward x x + x f = Foo inputs = torch zeros ep = export f inputs strict=True new_gm = copy deepcopy ep graph_module new_gm meta custom = new_gm meta custom f = bar node new_gm graph nodes node op == call_function node target == torch ops aten add Tensor node meta custom = node meta custom quantization_tag = foo new_ep = ep _update new_gm ep graph_signature serialized_vals = serialize new_ep new_ep = deserialize serialized_vals assertEqual new_ep graph_module meta custom f bar counter = node new_ep graph nodes node op == call_function node target == torch ops aten add Tensor counter += assertTrue node meta custom quantization_tag == foo assertEqual counter test_custom_tag_metadata_decomp Foo torch nn Module __init__ super __init__ linear = torch nn Linear forward x linear x f = Foo inputs = torch ones ep = export f inputs strict=True new_gm = copy deepcopy ep graph_module new_gm meta custom = new_gm meta custom f = bar counter = node new_gm graph nodes node op == call_function node target == torch ops aten linear default counter += node meta custom = node meta custom quantization_tag = foo assertEqual counter new_ep = ep _update new_gm ep graph_signature new_ep = new_ep run_decompositions assertEqual new_ep graph_module meta custom f bar counter = node new_ep graph nodes node op == call_function counter += assertTrue node meta custom quantization_tag == foo assertTrue counter test_custom_tag_metadata_copy Foo torch nn Module forward x x + x f = Foo inputs = torch zeros ep = export f inputs strict=True new_gm = copy deepcopy ep graph_module new_gm meta custom = new_gm meta custom f = bar node new_gm graph nodes node op == call_function node target == torch ops aten add Tensor node meta custom = node meta custom quantization_tag = foo new_gm = copy deepcopy new_gm assertEqual new_gm meta custom f bar counter = node new_gm graph nodes node op == call_function node target == torch ops aten add Tensor counter += assertTrue node meta custom quantization_tag == foo assertEqual counter test_unbacked_range_serdes Foo torch nn Module forward x y n = x item torch _check n = torch _check n y size torch empty n y n ep = torch export export Foo torch tensor torch randn dynamic_shapes= x None y Dim DYNAMIC buffer = io BytesIO save ep buffer buffer seek loaded_ep = load buffer pre-serialize ep pre_shape_env = torch _guards detect_fake_mode node meta get val node ep graph nodes shape_env post_shape_env = torch _guards detect_fake_mode node meta get val node loaded_ep graph nodes shape_env assertEqual pre_shape_env var_to_range post_shape_env var_to_range test_backed_size_oblivious_serdes Foo torch nn Module forward x y z x + y + z item torch fx experimental _config patch backed_size_oblivious=True ep = torch export export Foo torch randn torch randn torch tensor dynamic_shapes= x Dim DYNAMIC y Dim DYNAMIC z None buffer = io BytesIO save ep buffer buffer seek loaded_ep = load buffer shape_env = torch _guards detect_fake_mode node meta get val node loaded_ep graph nodes shape_env s = next iter ep graph nodes meta val size assertEqual shape_env var_to_range s node expr lower __name__ == __main__ run_tests