Owner s oncall quantization ruff noqa F torch torch testing _internal common_quantization skipIfNoFBGEMM torch testing _internal jit_utils JitTestCase TestDeprecatedJitQuantized JitTestCase skipIfNoFBGEMM test_rnn_cell_quantized d_in d_hid = cell torch nn LSTMCell d_in d_hid float torch nn GRUCell d_in d_hid float torch nn RNNCell d_in d_hid float isinstance cell torch nn LSTMCell num_chunks = isinstance cell torch nn GRUCell num_chunks = isinstance cell torch nn RNNCell num_chunks = Replace parameter values s t range values exactly thus we will have quantization error quantized GEMM call This i s testing purposes Note current implementation does support accumulation values outside range representable bit integer instead resulting saturated value We must take care our test we do end up dot product overflows int range e g + = So we hardcode test values here ensure mix signedness vals = - - - - - - - - vals = vals d_hid num_chunks cell weight_ih = torch nn Parameter torch tensor vals dtype=torch float requires_grad=False cell weight_hh = torch nn Parameter torch tensor vals dtype=torch float requires_grad=False assertRaisesRegex RuntimeError quantize_rnn_cell_modules function no longer supported cell = torch jit quantized quantize_rnn_cell_modules cell skipIfNoFBGEMM test_rnn_quantized d_in d_hid = cell torch nn LSTM d_in d_hid float torch nn GRU d_in d_hid float Replace parameter values s t range values exactly thus we will have quantization error quantized GEMM call This i s testing purposes Note current implementation does support accumulation values outside range representable bit integer instead resulting saturated value We must take care our test we do end up dot product overflows int range e g + = So we hardcode test values here ensure mix signedness vals = - - - - - - - - isinstance cell torch nn LSTM num_chunks = isinstance cell torch nn GRU num_chunks = vals = vals d_hid num_chunks cell weight_ih_l = torch nn Parameter torch tensor vals dtype=torch float requires_grad=False cell weight_hh_l = torch nn Parameter torch tensor vals dtype=torch float requires_grad=False assertRaisesRegex RuntimeError quantize_rnn_modules function no longer supported cell_int = torch jit quantized quantize_rnn_modules cell dtype=torch int assertRaisesRegex RuntimeError quantize_rnn_modules function no longer supported cell_fp = torch jit quantized quantize_rnn_modules cell dtype=torch float fbgemm torch backends quantized supported_engines test_quantization_modules K N = FooBar torch nn Module __init__ - None super __init__ linear = torch nn Linear K N float forward x x = linear x x fb = FooBar fb linear weight = torch nn Parameter torch tensor - - dtype=torch float requires_grad=False fb linear bias = torch nn Parameter torch zeros_like fb linear bias requires_grad=False x = torch rand K float - value = torch tensor - dtype=torch float y_ref = fb value assertRaisesRegex RuntimeError quantize_linear_modules function no longer supported fb_int = torch jit quantized quantize_linear_modules fb assertRaisesRegex RuntimeError quantize_linear_modules function no longer supported fb_fp = torch jit quantized quantize_linear_modules fb torch float skipIfNoFBGEMM test_erase_class_tensor_shapes Linear torch nn Module __init__ in_features out_features super __init__ qweight = torch _empty_affine_quantized out_features in_features scale= zero_point= dtype=torch qint _packed_weight = torch ops quantized linear_prepack qweight torch jit export __getstate__ torch ops quantized linear_unpack _packed_weight training forward _packed_weight torch jit export __setstate__ state _packed_weight = torch ops quantized linear_prepack state training = state property weight torch ops quantized linear_unpack _packed_weight weight setter weight w _packed_weight = torch ops quantized linear_prepack w torch _jit_internal _disable_emit_hooks x = torch jit script Linear torch _C _jit_pass_erase_shape_information x graph __name__ == __main__ raise RuntimeError This test file meant run directly use \n\n \tpython test test_quantization py TESTNAME\n\n instead