typing_extensions deprecated torch We need keep unused BC reasons torch amp grad_scaler OptState noqa F __all__ = GradScaler GradScaler torch amp GradScaler r See ` torch amp GradScaler ` ` ` torch cuda amp GradScaler args ` ` deprecated Please use ` ` torch amp GradScaler cuda args ` ` instead deprecated ` torch cuda amp GradScaler args ` deprecated Please use ` torch amp GradScaler cuda args ` instead category=FutureWarning __init__ init_scale float = growth_factor float = backoff_factor float = growth_interval int = enabled bool = True - None super __init__ cuda init_scale=init_scale growth_factor=growth_factor backoff_factor=backoff_factor growth_interval=growth_interval enabled=enabled