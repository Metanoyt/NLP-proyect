logging weakref dataclasses dataclass typing Any Optional torch _guards CompileId config types DynamoFrameType log logging Logger = logging getLogger __name__ Note cache size limit Background - TorchDynamo cache linked list Each cache entry guard_manager out_code next pointer These stored f_code s co_extra scratch space When frame invoked we walk linked list run guard_manager each cache_entry decide frame needs recompilation If none guard_manager s returns True we recompile add new entry To ensure we don t end up recompiling infinitely we put limits cache size There two limits recompile_limit accumulated_recompile_limit Earlier we used have only limit - maximum number entries cache line which now represented above So why do we need two limits Lets try understand In general we want our cache limit value small number e g even lower This ensures frames cause too many recompilation fall eager quickly However there another problem prevents us lowering value recompile_limit This due ID_MATCH d guards Today we put ID_MATCH guards nn module there graph break This means we will have many recompilations same code object because ID_MATCH guard fails different instances nn module This common pattern how models authored Therefore requires us keep recompile_limit high We resolve introducing these two limits The first limit limits number cache entries have ID_MATCH d guard nn module instance And nd limit becomes safeguard mechanism have maximum compilations code object One important question - what limit code object does have any ID_MATCH guard For such code objects we choose cache size limit Lets take example understand how these limits help Suppose we have instances nn module we ID_MATCH object Further suppose inputs these functions have varying batch size leading one recompilation In total there will recompilations therefore cache entries forward code object In older case when we had only limit our cache size limit must = capture all these recompilations Now suppose there separate function same program which very dynamic unsuitable compilation Such function will need undergo compilations burst cache fallback eager These recompilations too many we want fallback these compilation-unfriendly functions sooner In new scenario we can have recompile_limit = accumulated_recompile_limit = This means each ID_MATCH d object can have maximum two cache entries maximum number cache entries irrespective ID_MATCH obj This covers case forward code object which has recompilations For other function one unsuitable recompilation our limit So we will burst cache just recompilations In manner these limits help us resolve tension mentioned earlier dataclass CacheSizeRelevantForFrame We track number cache entries have same id_match objects given frame TODO janimesh - Consider adding map tuple_of_match_ids count - https github com pytorch pytorch pull #discussion_r - could useful debugging well Total number CacheEntry objects Dynamo linked list num_cache_entries int = Number CacheEntry objects having same ID_MATCH d objects given frame num_cache_entries_with_same_id_matched_objs int = will_compilation_exceed limit int - bool Checks compilation will exceed given limit s why = will_compilation_exceed_accumulated_limit will_compilation_exceed_specific_limit limit will_compilation_exceed_accumulated_limit - bool num_cache_entries = config accumulated_recompile_limit will_compilation_exceed_specific_limit limit int - bool num_cache_entries_with_same_id_matched_objs = limit _get_weakref_from_f_locals frame DynamoFrameType local_name str - Optional weakref ref Any obj = frame f_locals get local_name None weak_id = None try weak_id = weakref ref obj except TypeError pass cannot weakref bool object weak_id _has_same_id_matched_objs frame DynamoFrameType cache_entry Any - bool Checks ID_MATCH d objects saved cache_entry same ones frame f_locals cache_entry False local_name weakref_from_cache_entry cache_entry guard_manager id_matched_objs items weakref_from_cache_entry None weakref_from_frame = _get_weakref_from_f_locals frame local_name weakref_from_frame weakref_from_cache_entry False Also covers case where no ID_MATCH objects saved frame f_locals True compute_cache_size frame DynamoFrameType cache_entry Any - CacheSizeRelevantForFrame Walk linked list calculate cache size num_cache_entries = num_cache_entries_with_same_id_matched_objs = while cache_entry num_cache_entries += Track number cache entries having same ID_MATCH d objects frame f_locals This will used later compare against recompile_limit _has_same_id_matched_objs frame cache_entry num_cache_entries_with_same_id_matched_objs += cache_entry = cache_entry next CacheSizeRelevantForFrame num_cache_entries num_cache_entries_with_same_id_matched_objs is_recompilation cache_size CacheSizeRelevantForFrame - bool If frame earlier parsed compute_cache_size has more than cache entry same ID_MATCH d objects then its recompilation Note you can have multiple entries cache still recompile e g you can have nn module instances each one having ID_MATCH guard each one having just cache entry cache In case we can have entries cache no recompilation because there only one entry each id_matched_obj cache_size will_compilation_exceed exceeds_recompile_limit cache_size CacheSizeRelevantForFrame compile_id CompileId - tuple bool str Checks we exceeding cache size limit cache_size will_compilation_exceed_accumulated_limit True accumulated_recompile_limit cache_size will_compilation_exceed_specific_limit config recompile_limit True recompile_limit NOTE check needed case frame s cache doesn t grow we keep recompiling This can happen guard guard_manager becomes invalidated e g due guarded objects being freed This technically makes will_compilation_exceed_accumulated_limit check unnecessary we will keep check case we have better fix future assert compile_id frame_compile_id None compile_id frame_compile_id = config accumulated_recompile_limit True accumulated_recompile_limit False