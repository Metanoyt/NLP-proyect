Owner s oncall distributed itertools sys typing Union torch torch nn nn torch distributed dist torch distributed fsdp ShardingStrategy torch distributed fsdp fully_sharded_data_parallel CPUOffload FullyShardedDataParallel FSDP MixedPrecision torch distributed fsdp wrap ModuleWrapPolicy torch nn TransformerDecoderLayer TransformerEncoderLayer torch nn parallel DistributedDataParallel DDP torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp DEVICEInitMode FSDPInitMode FSDPTest get_devtype NestedWrappedModule TransformerWithSharedParams torch testing _internal common_utils run_tests TEST_WITH_DEV_DBG_ASAN device_type = torch device get_devtype dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit TestClipGradNorm FSDPTest Tests meth ` FullyShardedDataParallel clip_grad_norm_ ` skip_if_lt_x_gpu test_non_root device Tests calling ` ` clip_grad_norm_ ` ` non-root FSDP instance raises error Model nn Module __init__ - None super __init__ lin = nn Linear lin = nn Linear forward x torch Tensor - torch Tensor lin lin x model = Model device_type type model lin = FSDP model lin fsdp_model = FSDP model fsdp_model torch randn device=torch device device_type sum backward fsdp_model torch randn device=device_type sum backward error_regex = should only called root FSDP instance assertRaisesRegex RuntimeError error_regex fsdp_model lin clip_grad_norm_ max_norm= skip_if_lt_x_gpu test_ddp_parity device Tests FSDP ` ` FullyShardedDataParallel clip_grad_norm_ ` against DDP ` ` torch nn utils clip_grad_norm_ ` when using full precision run_subtests device device max_norm norm_type float inf sharding_strategy ShardingStrategy FULL_SHARD ShardingStrategy NO_SHARD mixed_strategy use_orig_params False True offload_params False True _test_ddp_parity _test_ddp_parity device max_norm Union float int norm_type Union float int sharding_strategy Union ShardingStrategy str use_orig_params bool offload_params bool local_model = TransformerWithSharedParams init process_group FSDPInitMode NO_FSDP DEVICEInitMode DEVICE_BEFORE deterministic=True ddp_model = DDP local_model device_ids= device_type fsdp_kwargs = cpu_offload CPUOffload offload_params=offload_params use_orig_params use_orig_params device_id device_type type sharding_strategy == mixed_strategy fsdp_model = TransformerWithSharedParams init process_group FSDPInitMode NO_FSDP DEVICEInitMode DEVICE_BEFORE deterministic=True Apply ` NO_SHARD ` encoder fsdp_model transformer encoder = FSDP fsdp_model transformer encoder sharding_strategy=ShardingStrategy NO_SHARD fsdp_kwargs Apply ` FULL_SHARD ` decoder fsdp_model transformer decoder = FSDP fsdp_model transformer decoder sharding_strategy=ShardingStrategy FULL_SHARD fsdp_kwargs TODO FSDP s ` clip_grad_norm_ ` static method so we must make root module FSDP instance fsdp_model = FSDP fsdp_model sharding_strategy=ShardingStrategy FULL_SHARD fsdp_kwargs fsdp_kwargs update sharding_strategy sharding_strategy auto_wrap_policy ModuleWrapPolicy TransformerEncoderLayer TransformerDecoderLayer fsdp_model = TransformerWithSharedParams init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE deterministic=True fsdp_kwargs=fsdp_kwargs LR = e- ddp_optim = torch optim Adam ddp_model parameters lr=LR fsdp_optim = torch optim Adam fsdp_model parameters lr=LR device = torch device device_type LARGE_FACTOR = inp = ddp_model module get_input device model ddp_model fsdp_model out = model inp isinstance model DDP FSDP loss = model module get_loss inp out loss = model get_loss inp out loss backward Multiply gradients large factor ensure gradients will actually clipped param itertools chain ddp_model parameters fsdp_model parameters param grad None gradients may ` None ` ` use_orig_params=True ` param grad = LARGE_FACTOR orig_ddp_grads = param grad detach clone param ddp_model parameters orig_fsdp_grads = param grad detach clone param grad None None param fsdp_model parameters ddp_total_norm = torch nn utils clip_grad_norm_ ddp_model parameters max_norm=max_norm norm_type=norm_type fsdp_total_norm = fsdp_model clip_grad_norm_ max_norm=max_norm norm_type=norm_type assertEqual ddp_total_norm fsdp_total_norm Check gradients modified ` clip_grad_norm_ ` param orig_grad zip ddp_model parameters orig_ddp_grads assert torch equal param grad orig_grad param orig_grad zip fsdp_model parameters orig_fsdp_grads param grad None assertEqual param grad orig_grad ` None ` assert torch equal param grad orig_grad Run optimizer step ensure gradients matched after clipping ddp_optim step fsdp_optim step FSDP summon_full_params fsdp_model n p n p zip ddp_model module named_parameters fsdp_model named_parameters assertEqual n n assertEqual p p offload_params TODO Gradient computation CPU GPU differ slightly causing drift unrelated ` clip_grad_norm_ ` https github com pytorch pytorch issues Run few more iterations TODO We cannot run too many iterations there drift https github com pytorch pytorch issues i range set_to_none = i == exercise both ddp_optim zero_grad set_to_none=set_to_none fsdp_optim zero_grad set_to_none=set_to_none inp = ddp_model module get_input device model ddp_model fsdp_model out = model inp out sum backward ddp_total_norm = torch nn utils clip_grad_norm_ ddp_model parameters max_norm=max_norm norm_type=norm_type fsdp_total_norm = fsdp_model clip_grad_norm_ max_norm=max_norm norm_type=norm_type assertEqual ddp_total_norm fsdp_total_norm ddp_optim step fsdp_optim step skip_if_lt_x_gpu test_low_precision_grads device Tests ` ` clip_grad_norm_ ` ` when using low precision gradients run_subtests device device max_norm norm_type float inf sharding_strategy ShardingStrategy FULL_SHARD ShardingStrategy NO_SHARD use_orig_params False True _test_low_precision_grads _test_low_precision_grads device max_norm Union float int norm_type Union float int sharding_strategy ShardingStrategy use_orig_params bool fsdp_kwargs = sharding_strategy sharding_strategy use_orig_params use_orig_params mixed_precision MixedPrecision param_dtype=torch float reduce_dtype=torch float keep_low_precision_grads=True device_id device_type type fsdp_model = FSDP NestedWrappedModule init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE deterministic=True fsdp_kwargs=fsdp_kwargs fsdp_kwargs inp = fsdp_model module get_input torch device device_type out = fsdp_model inp out sum backward param fsdp_model parameters param grad None assertEqual param grad dtype torch float total_norm = fsdp_model clip_grad_norm_ max_norm=max_norm norm_type=norm_type Check total norm FP match gradient dtype assertEqual total_norm dtype torch float As best effort check each gradient has norm most max norm since DDP does support mixed precision natively we cannot directly compare parity param fsdp_model parameters param grad None assertTrue torch linalg vector_norm param grad norm_type item = max_norm skip_if_lt_x_gpu test_no_gradients device Tests calling ` ` clip_grad_norm_ ` ` when FDSP module has no gradients simply returns scalar zero tensor FP without erroring run_subtests device device use_orig_params False True _test_no_gradients _test_no_gradients device use_orig_params bool lin_module = nn Linear mixed_precision_config = MixedPrecision param_dtype=torch float reduce_dtype=torch float buffer_dtype=torch float fsdp_module = FSDP lin_module sharding_strategy=ShardingStrategy SHARD_GRAD_OP mixed_precision=mixed_precision_config device_id=device_type type use_orig_params=use_orig_params inp = torch randn device=self device_type fsdp_module inp assertWarnsRegex expected_warning=UserWarning expected_regex= rank rf rank no gradients -- returning total norm default dtype torch float total_norm = fsdp_module clip_grad_norm_ assertEqual total_norm dtype torch float assertEqual total_norm torch tensor device=self device_type devices = cuda hpu xpu instantiate_device_type_tests TestClipGradNorm globals only_for=devices allow_xpu=True __name__ == __main__ run_tests