mypy allow-untyped-defs typing Optional torch expanded_weights_impl ExpandedWeight is_batch_first expanded_args_and_kwargs batch_first = None pyrefly ignore bad-assignment arg expanded_args_and_kwargs isinstance arg ExpandedWeight continue batch_first batch_first = arg batch_first arg batch_first = batch_first raise RuntimeError Got conflicting batch_first arguments same layer batch_first standard_kwargs kwarg_names expanded_args r Separate args kwargs ` __torch_function__ ` s standardize kwargs Most ` __torch_function__ ` s standardize kwargs they give so will separate args kwargs they pass Functions don t linear convND kwarg_values = expanded_args len expanded_args - len kwarg_names expanded_args_without_kwargs = expanded_args len expanded_args - len kwarg_names expanded_kwargs = dict zip kwarg_names kwarg_values strict=True expanded_args_without_kwargs expanded_kwargs forward_helper func expanded_args expanded_kwargs r Compute forward pass function has expanded weight s passed It will run forward pass where all ExpandedWeights their original weight It runs checks given arguments detaches outputs note First argument attr ` expanded_args ` must input batch dimension first element shape note attr ` func ` must Tensor tuple Tensors Args func The function called expanded_args Arguments passed attr ` func ` Will include arguments need unpacked because they ExpandedWeights expanded_kwargs Keyword arguments passed attr ` func ` Similar attr ` expanded_args ` unexpanded_args unexpanded_kwargs = _check_and_unexpand_args func expanded_args expanded_kwargs func unexpanded_args unexpanded_kwargs _check_and_unexpand_args func expanded_args expanded_kwargs input must first argument passed input = expanded_args isinstance input ExpandedWeight raise RuntimeError Expanded Weights do support inputs also ExpandedWeights f Input must Tensor got type input __name__ function func __name__ isinstance input torch Tensor raise RuntimeError Expanded Weights requires Tensor first input get batch dimension f got type input __name__ function func __name__ len input shape == raise RuntimeError f Expanded Weights requires batch dimension got input size function func __name__ input shape == raise RuntimeError valid batch size Expanded Weights got input tensor f input function func __name__ arg expanded_args + tuple expanded_kwargs values isinstance arg ExpandedWeight continue batch_size = input shape arg batch_first input shape arg allow_smaller_batches batch_size arg batch_size arg allow_smaller_batches arg batch_size = batch_size raise RuntimeError Expected ExpandedWeights have batch size matching input got f input batch size batch_size ExpandedWeight batch size arg batch_size loss_reduction Optional str = None arg expanded_args + tuple expanded_kwargs values isinstance arg ExpandedWeight loss_reduction None loss_reduction = arg loss_reduction loss_reduction = arg loss_reduction raise RuntimeError Expected ExpandedWeights all have same loss_reduction argument got one f loss_reduction one arg loss_reduction unexpanded_args = tuple arg orig_weight isinstance arg ExpandedWeight arg arg expanded_args unexpanded_kwargs = name arg orig_weight isinstance arg ExpandedWeight arg name arg expanded_kwargs items unexpanded_args unexpanded_kwargs maybe_scale_by_batch_size grad_sample expanded_weight expanded_weight loss_reduction == mean grad_sample expanded_weight batch_size grad_sample set_grad_sample_if_exists maybe_expanded_weight per_sample_grad_fn unpacked = unpack_expanded_weight_or_tensor maybe_expanded_weight isinstance maybe_expanded_weight ExpandedWeight grad_sample_contribution = maybe_scale_by_batch_size per_sample_grad_fn unpacked maybe_expanded_weight maybe_expanded_weight batch_size grad_sample_contribution shape only passes other checks arg allows smaller batch sizes intermediate = torch zeros maybe_expanded_weight batch_size grad_sample_contribution shape dtype=grad_sample_contribution dtype device=grad_sample_contribution device intermediate grad_sample_contribution shape = grad_sample_contribution grad_sample_contribution = intermediate hasattr unpacked grad_sample unpacked grad_sample None unpacked grad_sample = unpacked grad_sample + grad_sample_contribution unpacked grad_sample = grad_sample_contribution unpack_expanded_weight_or_tensor maybe_expanded_weight func=lambda x x isinstance maybe_expanded_weight ExpandedWeight orig_weight = maybe_expanded_weight orig_weight func orig_weight isinstance maybe_expanded_weight torch Tensor maybe_expanded_weight requires_grad func maybe_expanded_weight isinstance maybe_expanded_weight torch Tensor raise RuntimeError ExpandedWeights currently does support mixture ExpandedWeight parameters normal Parameters Please file issue pytorch pytorch sum_over_all_but_batch_and_last_n tensor torch Tensor n_dims int - torch Tensor r Calculate sum over all dimensions except first batch dimension excluding last n_dims This function will ignore first dimension will aggregate over last n_dims dimensions Args tensor An input tensor shape ` ` B X n_dims- ` ` n_dims Number dimensions keep Example tensor = torch ones sum_over_all_but_batch_and_last_n tensor n_dims= shape torch Size Returns A tensor shape ` ` B X n_dims- ` ` tensor dim == n_dims + tensor dims = list range tensor dim - n_dims tensor sum dim=dims