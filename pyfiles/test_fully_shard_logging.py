Owner s module fsdp functools os unittest mock torch distributed dist torch _dynamo test_case run_tests torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal logging_utils LoggingTestCase requires_distributed = functools partial unittest skipIf dist is_available requires distributed torch torch testing _internal common_fsdp get_devtype device_type = torch device get_devtype skip_if_lt_x_gpu LoggingTests LoggingTestCase requires_distributed test_fsdp_logging env = dict os environ env TORCH_LOGS = fsdp env RANK = env WORLD_SIZE = env MASTER_PORT = env MASTER_ADDR = localhost _ stderr = run_process_no_exception f \ logging torch torch distributed dist torch nn nn torch distributed fsdp fully_shard logger = logging getLogger torch distributed _composable fsdp logger setLevel logging DEBUG device = device_type type torch manual_seed model = nn Sequential nn Linear device=device bias=False _ range layer model fully_shard layer fully_shard model x = torch randn device=device model x sum backward env=env assertIn FSDP root_pre_forward stderr decode utf- assertIn FSDP pre_forward stderr decode utf- assertIn FSDP pre_forward stderr decode utf- assertIn FSDP post_forward stderr decode utf- assertIn FSDP post_forward stderr decode utf- assertIn FSDP pre_backward stderr decode utf- assertIn FSDP pre_backward stderr decode utf- assertIn FSDP post_backward stderr decode utf- assertIn FSDP post_backward stderr decode utf- assertIn FSDP root_post_backward stderr decode utf- __name__ == __main__ run_tests