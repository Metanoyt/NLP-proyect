Owner s module unknown functools partial textwrap dedent torch torch testing FileCheck torch testing _internal common_device_type instantiate_device_type_tests OpDTypes ops torch testing _internal common_jit check_against_reference JitCommonTestCase torch testing _internal common_methods_invocations op_db torch testing _internal common_utils clone_input_helper first_sample IS_SANDCASTLE run_tests TestCase unMarkDynamoStrictTest torch testing _internal jit_metaprogramming_utils check_alias_annotation create_script_fn create_traced_fn torch testing _internal jit_utils disable_autodiff_subgraph_inlining is_lambda variant testing only done torch float torch cfloat avoid excessive test times maximize signal noise ratio _variant_ops = partial ops dtypes=OpDTypes supported allowed_dtypes= torch float torch cfloat Tests operators consistency between JIT eager also checks correctness JIT specific alias schemas intended autodifferentiation behavior Inherits JitCommonTestCase instead TestCase directly share functionality original test_jit py method operator tests unMarkDynamoStrictTest TestJit JitCommonTestCase exact_dtype = True Tests forward backward passes operations produce same values cross-product op variants function method inplace runtimes eager traced scripted TODO WARNING inplace x traced scripted currently tested _variant_ops op_db test_variant_consistency_jit device dtype op _requires_grad = dtype op supported_backward_dtypes torch device device type include_conjugated_inputs = op test_conjugated_samples dtype is_complex samples = op sample_inputs device dtype requires_grad=_requires_grad include_conjugated_inputs=include_conjugated_inputs Acquires variants test func = op get_op method = op get_method variants = TODO inplace tests currently fail fix add inplace variant function func method method scripting strips torch ops prefix these operators incorrectly don t bother testing case Count testing isinstance func torch _ops OpOverload skipTest variant consistency doesn t work torch ops TODO find better way standardize op registration itself has_fake_function = op name resize_ resize_as_ has_fake_function variants = method getattr torch Tensor op name samples = op sample_inputs device dtype requires_grad=False tested = False sample samples Test traced scripted consistency func_type variant variants items variant None continue scripting check_alias_analysis do work lambdas lambdas typically used way simulate methods without functional variants so rely other variant testing now is_lambda variant continue tested = True try indiv_variant_test_jit device dtype op sample func_type variant has_fake_function except Exception e variant_error_info = dedent f Error testing op name func_type variant dtype dtype inputs sample raise Exception variant_error_info e noqa TRY assert tested JIT Test does execute any logic indiv_variant_test_jit device dtype op sample func_type variant has_fake_function _requires_grad = dtype op supported_backward_dtypes torch device device type support_script = op supports_scripting Create accessor script function variant name = op name + _ func_type == inplace op name run disable_autodiff_subgraph_inlining True test autodiff support Context manager forces graph contain DifferentiableGraph nodes they present disable_autodiff_subgraph_inlining Check scripted forward grad grad grad support_script script_fn = create_script_fn name func_type out_fn output Processes output autograd sample output_process_fn_grad None sample output_process_fn_grad output output get_sample clone_input_helper sample input op name - == _ sample input support_script check_against_reference script_fn op get_op out_fn get_sample + sample args sample kwargs no_grad=not _requires_grad no_gradgrad=not op supports_gradgrad Check traced forward grad grad grad TODO fix tracing here supports_tracing = op supports_tracing has_fake_function op assert_jit_shape_analysis assertTrue supports_tracing supports_tracing traced_fn = create_traced_fn variant check_against_reference traced_fn op get_op out_fn get_sample + sample args sample kwargs no_grad=not _requires_grad no_gradgrad=not op supports_gradgrad Check alias annotation schema correctness make sure inputs aren t supposed modified aren t Note only runs float because schema isn t affected dtype so running all dtypes would excessive dtype == torch float TODO no reason why we can t run tracing graph support_script op name = rsub check_alias_annotation name get_sample + sample args sample kwargs func_type=func_type aten_name=op aten_name TODO use script graph well checked_shape_analysis = False supports_tracing out = variant get_sample sample args sample kwargs right now tuple outputs tensor output supported TODO list tensor outputs tuple_of_tensors = isinstance out tuple all isinstance elem torch Tensor elem out isinstance out torch Tensor tuple_of_tensors tuple_of_tensors sizes = elem size elem out sizes = out size checkShapeAnalysis sizes traced_fn graph op assert_jit_shape_analysis checked_shape_analysis = True op assert_jit_shape_analysis assertTrue checked_shape_analysis Check autodifferentiation nodes traced scripted graphs only need check once per sample dtype torch float Sandcastle doesn t fuse nodes IS_SANDCASTLE fusible nodes expected found FusionGroups DifferentiableGraphs nonfusible_nodes = op autodiff_nonfusible_nodes + op autodiff_fusible_nodes fusible_nodes = nonfusible_nodes = op autodiff_nonfusible_nodes fusible_nodes = op autodiff_fusible_nodes supports_tracing assertAutodiffNode traced_fn last_graph op assert_autodiffed nonfusible_nodes fusible_nodes support_script assertAutodiffNode script_fn last_graph op assert_autodiffed nonfusible_nodes fusible_nodes alias testing only done torch float same reason _alias_ops = partial ops dtypes=OpDTypes supported allowed_dtypes= torch float _alias_ops op op op_db op aliases test_jit_alias_remapping device dtype op NOTE only tests first sample samples = op sample_inputs device dtype requires_grad=True sample = first_sample samples Scripting Data Preparation Prepare data test scripting Below we prepare strings args kwargs without type annotations These strings inserted into function template strings which then torch scripted - args string t corresponding input tensor required op - args_kw value args strings kwargs used call op without type annotations example True tensor - fn t variant t True tensor args = t quote_strs v isinstance v str f v str v args_kw = args + f v v sample args + f k = quote_strs v k v sample kwargs items Prepare data test tracing sample_args_kwargs = len sample args sample_args_kwargs += sample args len sample kwargs sample_args_kwargs += sample kwargs original_name = op aten_name original_name_inplace = original_name + _ expected_dtype = op sample input sample args sample kwargs dtype a_op op aliases inplace = a_op inplace_variant method_or_inplace = a_op inplace_variant a_op method_variant variants = v v a_op op a_op method_variant a_op inplace_variant v None Test scripting variant variants variant_name = variant __name__ op_name = original_name_inplace variant inplace original_name variant method_or_inplace fn_template = _fn t c t alias_name args_kw remove first input tensor script = fn_template format c= len args_kw args_kw= join args_kw alias_name=variant_name fn_template = _fn args variant args_kw script = fn_template format args= join args args_kw= join args_kw Required avoid undefined value tensor error JIT compilation function template script = script replace tensor torch tensor scripted = torch jit CompilationUnit script _fn variant inplace torch can_cast expected_dtype dtype try inp = clone_input_helper sample input scripted inp except Exception continue fail Inplace operation integer tensor should promoted float didn t fail inp = clone_input_helper sample input scripted inp inp = clone_input_helper sample input graph = scripted graph_for inp FileCheck check op aten_name check_not variant_name run graph Test tracing variant variants variant_name = variant __name__ op_name = original_name_inplace variant inplace original_name _fn sample_args sample_kwargs variant sample_args sample_kwargs inp = clone_input_helper sample input + sample_args_kwargs traced = torch jit trace _fn inp inp = clone_input_helper sample input + sample_args_kwargs traced inp inp = clone_input_helper sample input + sample_args_kwargs graph = traced graph_for inp FileCheck check op_name check_not variant_name run graph instantiate_device_type_tests TestJit globals __name__ == __main__ TestCase _default_dtype_check_enabled = True run_tests