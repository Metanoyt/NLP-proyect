mypy allow-untyped-defs warnings collections abc Callable typing Any Optional TYPE_CHECKING TypeAlias TypeVar Union typing_extensions ParamSpec torch torch sym_float Tensor torch _prims_common corresponding_real_dtype torch masked _docs torch masked maskedtensor core is_masked_tensor MaskedTensor torch masked maskedtensor creation as_masked_tensor TYPE_CHECKING torch _prims_common DimsType torch types _dtype DType DimOrDims TypeAlias = Optional DimsType The JIT doesn t understand Union nor torch dtype here DType = int DimOrDims = Optional tuple int __all__ list str = _T = TypeVar _T _P = ParamSpec _P All masked reduction normalization operations have same signatures Here we introduce docstring templates applied docstrings reduction normalization functions via _apply_docstring_templates decorator _apply_docstring_templates func Callable _P _T - Callable _P _T Decorator applies docstring templates function docstring returns function instance doc_string = getattr _docs f func __name__ _docstring None doc_string None warnings warn f No documentation string available func __name__ PyTorch team should run ` python tools update_masked_docs py ` generate missing docstrings stacklevel= func __doc__ = doc_string Expose function public symbol __all__ append func __name__ func _generate_docstring func A utility function called tools update_masked_docs py script update module torch masked _docs py docstring_templates = dict reduction_signature= \ function_name input operation_args operation_kwargs - Tensor reduction_descr= \ Returns operation name all elements attr ` input ` tensor along given dimension s attr ` dim ` while attr ` input ` elements masked out according boolean tensor attr ` mask ` reduction_args= \ If attr ` keepdim ` ` ` True ` ` output tensor same size attr ` input ` except dimension s attr ` dim ` where size Otherwise attr ` dim ` squeezed see func ` torch squeeze ` resulting output tensor having ` ` len dim ` ` fewer dimension s The boolean tensor attr ` mask ` defines validity attr ` input ` tensor elements attr ` mask ` element True then corresponding element attr ` input ` tensor will included operation name computation otherwise element ignored When all elements attr ` input ` along given dimension attr ` dim ` ignored fully masked-out corresponding element output tensor will have undefined value may may correspond identity value operation name operation choice may correspond value leads most efficient storage attr ` output ` tensor The mask output tensor can computed ` ` torch any torch broadcast_to mask input shape dim keepdim=keepdim dtype=torch bool ` ` The shapes attr ` mask ` tensor attr ` input ` tensor don t need match they must ref ` broadcastable broadcasting-semantics ` dimensionality attr ` mask ` tensor must greater than attr ` input ` tensor Args input Tensor input tensor args_declarations Keyword args kwargs_declarations reduction_example= \ Example input = example_input input indent_example_input mask = example_mask mask indent_example_mask full_function_name input example_args mask=mask indent_example_output reduction_identity= \ The identity value operation name operation which used start reduction ` ` identity_int ` ` reduction_identity_dtype= \ The identity value operation name operation which used start reduction depends input dtype For instance float uint int dtypes identity values ` ` identity_float ` ` ` ` identity_uint ` ` ` ` identity_int ` ` respectively normalization_signature= \ function_name input operation_args operation_kwargs - Tensor normalization_descr= \ Returns operation name all slices attr ` input ` tensor along attr ` dim ` while attr ` input ` elements masked out according boolean tensor attr ` mask ` definition normalization_args= \ The boolean tensor attr ` mask ` defines validity attr ` input ` tensor elements attr ` mask ` element True then corresponding element attr ` input ` tensor will included operation name computation otherwise element ignored The values masked-out elements output tensor have undefined value may may set zero nan choice may correspond value leads most efficient storage attr ` output ` tensor The mask operation name output tensor can computed ` ` torch broadcast_to mask input shape ` ` The shapes attr ` mask ` tensor attr ` input ` tensor don t need match they must ref ` broadcastable broadcasting-semantics ` dimensionality attr ` mask ` tensor must greater than attr ` input ` tensor Args input Tensor input tensor args_declarations Keyword args kwargs_declarations normalization_example= \ Example input = example_input input indent_example_input mask = example_mask mask indent_example_mask full_function_name input example_args mask=mask indent_example_output args_and_kwargs = argument name sufficies separated double underscore will removed final documentation string sum dim keepdim=False dtype=None mask=None prod dim keepdim=False dtype=None mask=None cumsum dim__as_int dtype=None mask=None cumprod dim__as_int dtype=None mask=None amin dim keepdim=False dtype=None mask=None amax dim keepdim=False dtype=None mask=None argmin dim__as_int keepdim=False dtype=None mask=None argmax dim__as_int keepdim=False dtype=None mask=None mean dim keepdim=False dtype=None mask=None median dim__as_int keepdim=False dtype=None mask=None norm ord dim keepdim=False dtype=None mask=None var dim unbiased keepdim=False dtype=None mask=None std dim unbiased keepdim=False dtype=None mask=None logsumexp dim keepdim=False dtype=None mask=None softmax dim__as_int dtype=None mask=None log_softmax dim__as_int dtype=None mask=None softmin dim__as_int dtype=None mask=None normalize ord__required dim__as_int eps= e- dtype=None mask=None argument_declarations = dim \ dim int tuple ints optional dimension dimensions reduce Default None equivalent ` ` tuple range input ndim ` ` dim__as_int \ dim int dimension along which operation name computed ord \ ord int float optional order vector norm Default See func ` torch linalg vector_norm ` list supported norms ord__required \ ord int float order vector norm Default See func ` torch linalg vector_norm ` list supported norms unbiased \ unbiased bool when True use Bessel s correction otherwise compute uncorrected sample variance eps \ eps float optional small value avoid division zero Default default keepdim \ keepdim bool optional whether output tensor has attr ` dim ` retained Default default dtype \ dtype ` torch dtype ` optional desired data type returned tensor If specified input tensor casted attr ` dtype ` before operation performed Default default mask \ mask ` torch Tensor ` optional boolean tensor containing binary mask validity input tensor elements Default None equivalent ` ` torch ones input shape dtype=torch bool ` ` definitions = softmax \ Let ` ` x ` ` sequence unmasked elements one-dimensional slice attr ` input ` tensor Softmax i-th element ` ` x ` ` defined ` ` exp x i sum exp x ` ` log_softmax \ Let ` ` x ` ` sequence unmasked elements one-dimensional slice attr ` input ` tensor LogSoftmax i-th element ` ` x ` ` defined ` ` log exp x i sum exp x ` ` softmin \ Let ` ` x ` ` sequence unmasked elements one-dimensional slice attr ` input ` tensor Softmin i-th element ` ` x ` ` defined ` ` exp -x i sum exp -x ` ` normalize \ Let ` ` x ` ` sequence unmasked elements one-dimensional slice attr ` input ` tensor Normalize i-th element ` ` x ` ` defined ` ` x i max norm x p eps ` ` cumsum \ Let ` ` x ` ` sequence unmasked elements one-dimensional slice attr ` input ` tensor Cumsum i-th element ` ` x ` ` defined ` ` sum x i ` ` cumprod \ Let ` ` x ` ` sequence unmasked elements one-dimensional slice attr ` input ` tensor Cumsum i-th element ` ` x ` ` defined ` ` prod x i ` ` reduction_names = sum sum prod product amax maximum amin minimum argmax argmax argmin argmin mean mean median median norm norm var variance std standard_deviation logsumexp logsumexp normalization_names = softmax softmax log_softmax log_softmax softmin softmin normalize normalize cumsum cumulative_sum cumprod cumulative_prod operation_names = operation_names update reduction_names operation_names update normalization_names Default example data example_dim = example_input = torch tensor - - - example_mask = torch tensor True False True False False False example_args tuple Any func __name__ norm normalize example_args = example_dim example_input = example_input dtype=torch float func __name__ var std example_args = example_dim False func __name__ == median example_args = example_dim example_input = example_input dtype=torch float example_args = example_dim operation_args tuple str operation_kwargs tuple str operation_args operation_kwargs = args_and_kwargs func __name__ arg_declarations = \n join argument_declarations get f split __ TBD splitlines operation_args kwarg_declarations = \n join argument_declarations get split = f split __ TBD format default=a split = splitlines operation_kwargs func __name__ reduction_names op_kind = reduction doc_sections = signature descr identity args example func __name__ normalization_names op_kind = normalization doc_sections = signature descr args example example_input = example_input dtype=torch float assert add function name operation names dictionaries example_output = func example_input example_args mask=example_mask template_data = function_name func __name__ full_function_name func __module__ + + func __name__ operation name operation_names func __name__ operation_args join split __ operation_args operation_kwargs join split __ operation_kwargs one-line representation tensor example_input join str example_input split example_args join map str example_args example_mask join str example_mask split multi-line representation tensor indent indent_example_input \n join str example_input splitlines indent_example_mask \n join str example_mask splitlines indent_example_output \n join str example_output splitlines func __name__ reduction_names template_data update identity_uint =_reduction_identity func __name__ torch tensor dtype=torch uint identity_int =_reduction_identity func __name__ torch tensor dtype=torch int identity_float =_reduction_identity func __name__ torch tensor dtype=torch float func __name__ == norm template_data update identity_ord_ninf=_reduction_identity func __name__ torch tensor dtype=torch float float -inf func __name__ normalization_names template_data update definition=definitions func __name__ assert add function name operation names dictionaries template_data update args_declarations= \n join arg_declarations format_map template_data template_data update kwargs_declarations= \n join kwarg_declarations format_map template_data Apply function name info docstring templates templates = k v format_map template_data k v docstring_templates items k startswith op_kind templates update k v format_map template_data isinstance v str v k v template_data items Apply docstring templates function doctring func __doc__ None doc_template = \n\n join f op_kind _ sec sec doc_sections doc_template = func __doc__ doc_template format_map templates _reduction_identity op_name str input Tensor args Return identity value scalar tensor reduction operation given input None identity value cannot uniquely defined given input The identity value operation defined initial value reduction operation has property ` ` op op_identity value == value ` ` any value domain operation Or put another way including excluding identity value list operands will change reduction result See https github com pytorch rfcs pull more information dtype DType = input dtype device = input device op_name = op_name rsplit - lstrip module name when present op_name sum cumsum torch tensor dtype=dtype device=device op_name prod cumprod torch tensor dtype=dtype device=device op_name amax argmax logaddexp torch is_floating_point input torch tensor -torch inf dtype=dtype device=device torch is_signed input dtype == torch uint torch tensor torch iinfo dtype min dtype=dtype device=device op_name == logsumexp torch is_floating_point input torch tensor -torch inf dtype=dtype device=device torch is_complex input torch tensor -torch inf + j dtype=dtype device=device torch is_signed input dtype == torch uint torch tensor torch iinfo dtype min dtype=dtype device=device op_name amin argmin torch is_floating_point input torch tensor torch inf dtype=dtype device=device torch is_signed input dtype == torch uint torch tensor torch iinfo dtype max dtype=dtype device=device op_name == mean Strictly speaking identity value mean operation mean input Since mean value depends dim argument may non-scalar tensor we consider identity value mean operation ambiguous Moreover mean value empty input undefined None op_name == norm ord = args args ord == float -inf assert torch is_floating_point input input dtype torch tensor torch inf dtype=dtype device=device torch tensor dtype=dtype device=device op_name == median We use NaN now because implementation currently using torch nanmedian NaN identity function since gets ignored dtype = input dtype torch is_floating_point input torch float torch tensor torch nan dtype=dtype device=device op_name var std None raise NotImplementedError f identity op_name dtype input _canonical_dim dim DimOrDims ndim int - tuple int Return dim argument tuple sorted dim values dims list int = dim == Currently ` dim= ` reductions operations means reduce over all dimensions while future will read no reduce See https github com pytorch pytorch issues When gh- resolved if-block must deleted dim = None dim None tuple range ndim ndim = max ndim dim_ = dim isinstance dim int torch SymInt dim d dim_ d dims raise RuntimeError f dim= d appears multiple times list dims d = ndim d -ndim raise IndexError f Dimension out range expected range -ndim ndim - got d pyrefly ignore bad-argument-type dims append d ndim tuple sorted dims _sparse_coo_flatten_indices indices Tensor shape tuple Flatted N-D indices -D indices flat_indices = indices new_zeros indices size d sz enumerate shape flat_indices mul_ sz flat_indices add_ indices d flat_indices _any input Tensor dim tuple keepdim bool Support torch any tuple dim argument Workaround https github com pytorch pytorch issues r = input d reversed dim r = r any dim=d keepdim=keepdim r _sparse_coo_where mask Tensor input Tensor fill_value Tensor - Tensor Sparse variant torch where Supports sparse COO hybrid sparse COO tensors _sparse_coo_where implements following invariant _sparse_coo_where mask input fill_value to_dense fill_value == torch where mask to_dense input to_dense torch full input shape fill_value where ` == b ` means ` assertEqual b ` mask boolean sparse tensor ` to_dense fill_value ` like ` to_dense ` except unspecified elements mapped ` fill_value ` rather than ` ` Returns sparse COO tensor following features - all specified elements correspond masked-in elements have values input tensor If there exists masked-in element specified mask specified input result tensor corresponding element has value In dense part sparse tensor masked-out elements replaced fill_value - all unspecified elements correspond masked-out elements assert input layout == torch sparse_coo assert mask layout == input layout assert mask shape == input shape assert mask dense_dim == input dense_dim TODO eliminate restriction input = input coalesce For set operations sparse tensor indices we ll convert multi-dimensional indices -D indices efficiency input_flat_indices = _sparse_coo_flatten_indices input indices input shape input sparse_dim mask_flat_indices = _sparse_coo_flatten_indices mask indices mask shape mask sparse_dim set mask flat indices define masked-in elements mask dense_dim mask_values = _any mask values tuple range input sparse_dim + False mask_values = mask values maskin_flat_indices = mask_flat_indices mask_values nonzero intersection i i union counts = torch cat i i unique return_counts=True union torch where counts gt minus i i union counts = torch cat i i unique return_counts=True intersection union torch where counts eq i _apply obj w = obj w set input flat indices specified masked-in elements maskin_input_flat_indices = _apply intersection maskin_flat_indices input_flat_indices _ w = intersection input_flat_indices maskin_input_flat_indices indices values masked-in elements where_input_indices = input indices slice None + w where_input_values = input values w mask dense_dim apply mask dense part input values _ w = intersection mask_flat_indices maskin_input_flat_indices where_mask_values = mask values w where_input_values = torch where where_mask_values where_input_values fill_value set flat indices unspecified input masked-in elements maskin_zero_flat_indices = _apply minus maskin_flat_indices maskin_input_flat_indices indices masked-in zero elements _ w = intersection mask_flat_indices maskin_zero_flat_indices where_zero_indices = mask indices slice None + w construct result n = where_zero_indices size n == input coalesced hence input_flat_indices ordered result guaranteed coalesced result = torch sparse_coo_tensor where_input_indices where_input_values input shape result _coalesced_ True where_indices = torch cat where_input_indices where_zero_indices dim= where_values = torch cat where_input_values where_input_values new_zeros n + where_input_values shape result = torch sparse_coo_tensor where_indices where_values input shape appending zero elements leads uncoalesced sparse tensor result coalesce _sparse_coo_scatter_reduction_helper op mask_input Tensor dims tuple int keepdim bool dtype Optional DType = None - Tensor reduce = op __name__ valid_reductions = sum prod amax amin reduce valid_reductions raise ValueError f op must one join valid_reductions got reduce instead output_dtype = dtype values indices = mask_input _values mask_input _indices input_dims = mask_input dim num_sparse_dims = mask_input sparse_dim reduced_sparse_dims = retained_sparse_dims = reduced_dense_dims = promote dtype specified values dtype = output_dtype values = values output_dtype keepdim output_shape = tuple i dims si i si enumerate mask_input shape output_shape = tuple si i si enumerate mask_input shape i dims d dims d = input_dims continue d num_sparse_dims reduced_sparse_dims append d reduced_dense_dims append d + - num_sparse_dims Reduce dense dimensions len reduced_dense_dims reduce == sum new_values = values new_values = op new_values dim=reduced_dense_dims keepdim=bool keepdim FIXME Implement reductions dense dimensions ops non-zero reduction identities NotImplemented new_values = values clone Reduce sparse dimensions len reduced_sparse_dims == num_sparse_dims reduce amax amin new_values size == IndexError amax Expected reduction dim have non-zero size sum prod reduction identity when dim has size amax amin do See https github com pytorch pytorch issues new_values = _reduction_identity reduce new_values new_values = op new_values dim= keepdim _ range num_sparse_dims new_values = new_values unsqueeze new_values dtype=output_dtype to_sparse new_indices = indices clone keepdim zero out reduced sparse dimensions keepdim = True ensures call torch unique folds duplicated indices together while preserving dimension new_indices reduced_sparse_dims = remove reduced sparse dimensions keepdim = False len reduced_sparse_dims retained_sparse_dims = i i range num_sparse_dims i set reduced_sparse_dims new_indices = new_indices index_select torch tensor retained_sparse_dims mask_input device Use scatter_reduce reduce items new_values tensor correspond same indices new_indices new_indices numel lexsort indices get index tensor scatter reduction new_indices inverse_indices = torch unique new_indices return_inverse=True dim= out_shape = list new_values shape out_shape = new_indices shape _ range new_values ndim - inverse_indices = inverse_indices unsqueeze - scatter_indices = inverse_indices expand new_values shape FIXME temporary workaround issue bfloat float remove when acctype implemented scatter_reduce output_dtype torch bfloat torch float new_values = new_values torch float out = new_values new_empty out_shape new_values = out scatter_reduce_ scatter_indices new_values reduce=reduce include_self=False new_values = new_values dtype=output_dtype out = new_values new_empty out_shape new_values = out scatter_reduce_ scatter_indices new_values reduce=reduce include_self=False torch sparse_coo_tensor new_indices new_values output_shape dtype=output_dtype device=mask_input device _sparse_csr_segment_reduction_helper op mask_input Tensor dims tuple int keepdim bool dtype Optional DType = None - Tensor Currently while sparse CSR always D no dense dimensions keepdim must True FIXME when dense dimensions implemented CSR tensors assert keepdim reduction operations CSR tensors keepdim=False unsupported reduce = op __name__ valid_reductions = sum prod mean amax amin reduce valid_reductions raise ValueError f op must one join valid_reductions got reduce instead device = mask_input device output_dtype = dtype values crow_indices col_indices = mask_input values mask_input crow_indices mask_input col_indices promote dtype specified values dtype = output_dtype values = values output_dtype len dims == mask_input len dims == dims == new_col_indices scatter_indices = torch unique col_indices return_inverse=True new_nnz = new_col_indices shape new_crow_indices = torch tensor new_nnz new_values = values new_empty new_col_indices shape new_values scatter_reduce_ scatter_indices values reduce include_self=False new_shape = mask_input size assert dims == Sparse CSR tensors D only support reduction along dim all intervals new_crow_indices i - new_crow_indices i- except where crow_indices i == crow_indices i- where interval remains new_crow_indices = torch cat crow_indices new_zeros torch cumsum torch diff crow_indices = new_nnz = new_crow_indices - new_col_indices = col_indices new_zeros new_nnz type ignore call-overload new_values = torch _segment_reduce values reduce offsets=crow_indices type ignore attr-defined new_shape = mask_input size assert len dims == nnz = min values numel nnz == op_kwargs = keepdim True dtype output_dtype amax amin do support dtype kwarg reduce amax amin del op_kwargs dtype new_values = op values op_kwargs new_values = torch empty dtype=output_dtype new_col_indices = col_indices new_zeros nnz new_crow_indices = torch tensor nnz new_shape = nnz torch sparse_csr_tensor new_crow_indices new_col_indices new_values new_shape dtype=output_dtype device=device _sparse_csr_where mask Tensor input Tensor fill_value Tensor - Tensor Sparse variant torch where Supports sparse CSR tensors TODO implement sparse CSR specific where operator efficiency _sparse_coo_where mask to_sparse_coo input to_sparse_coo fill_value to_sparse_csr _where mask Tensor input Tensor fill_value Tensor - Tensor torch where sparse inputs support _where implements following invariant _where mask input fill_value to_dense fill_value == torch where mask to_dense input to_dense torch full input shape fill_value where ` == b ` means ` assertEqual b ` mask boolean sparse tensor ` to_dense fill_value ` like ` to_dense ` except unspecified elements mapped ` fill_value ` rather than ` ` Returns sparse tensor following features - all specified elements correspond masked-in elements have values input tensor If there exists masked-in element specified mask specified input result tensor corresponding element has value In dense part sparse tensor masked-out elements replaced fill_value - all unspecified elements correspond masked-out elements mask layout == torch strided torch where mask input fill_value mask layout == torch sparse_coo _sparse_coo_where mask input fill_value mask layout == torch sparse_csr _sparse_csr_where mask input fill_value raise ValueError f _where expects strided sparse COO sparse CSR tensor got mask layout _input_mask input Union Tensor MaskedTensor args kwargs - Tensor Return canonical input mask A canonical input mask defined boolean mask tensor shape layout matches shape layout input The canonical input mask computed attr ` mask ` tensor content meet following criteria The shape canonical input mask same shape attr ` input ` tensor If mask tensor has smaller shape than shape attr ` input ` broadcasting rules will applied Downcasting mask supported The layout canonical input mask same layout attr ` input ` tensor If mask has different layout will converted expected layout In case sparse COO layout canonical input mask will coalesced The dtype canonical input mask torch bool If mask dtype bool then will converted bool dtype using ` dtype=bool ` method call The elements canonical input mask have boolean values copied content attr ` mask ` tensor after possible broadcasting dtype conversion transforms In general sparsity pattern sparse canonical input mask need same sparsity pattern sparse attr ` input ` tensor input layout torch strided torch sparse_coo torch sparse_csr raise ValueError f _input_mask expects strided sparse COO sparse CSR tensor got input layout mask = kwargs get mask default mask mask None raise ValueError _input_mask requires explicit mask mask shape must match input shape mask shape = input shape mask ndim input ndim raise IndexError _input_mask expected broadcastable mask got mask dimensionality higher than input mask layout == torch strided mask = torch broadcast_to mask clone input shape dtype=torch bool mask layout == torch sparse_coo mask = torch _sparse_broadcast_to mask input shape assert mask layout == torch sparse_csr Broadcasting CSR tensors implemented Working around using COO layout mask = torch _sparse_broadcast_to mask to_sparse input shape to_sparse_csr mask layout must match input layout mask layout = input layout input layout == torch strided mask = mask to_dense input layout == torch sparse_coo mask layout == torch strided mask = mask to_sparse input sparse_dim mask = mask to_sparse assert input layout == torch sparse_csr mask = mask to_sparse_csr sparse mask must coalesced mask layout == torch sparse_coo mask = mask coalesce mask boolean tensor mask = mask dtype=torch bool mask _output_mask op input Tensor args kwargs - Tensor Return output mask masked operation applied given arguments callable op is_reduction = op __name__ sum prod amax amin argmax argmin mean median norm var std logsumexp is_normalization = op __name__ softmax log_softmax softmin normalize cumsum cumprod is_reduction op __name__ == norm args args = args lstrip ord argument dim = args args kwargs get dim outmask = _input_mask input args kwargs keepdim = kwargs get keepdim False dim_ = _canonical_dim dim input ndim _any outmask dim_ bool keepdim is_normalization _input_mask input args kwargs raise ValueError f _output_mask expected masked operation got callable op __module__ op __name__ raise ValueError f _output_mask expected masked operation got type op __name__ object _combine_input_and_mask op input Union MaskedTensor Tensor mask args - Tensor helper input mask mask None input canonical_mask = _input_mask input mask=mask callable op fill_value = _reduction_identity op __name__ input args _where canonical_mask input fill_value raise ValueError f _combine_input_and_mask expected masked operation got type op __name__ object Combine torch autograd Function staticmethod pyrefly ignore bad-override forward ctx input mask Return input masked-out elements eliminated given operations ctx save_for_backward mask mask None ctx mark_non_differentiable mask helper input mask staticmethod pyrefly ignore bad-override backward ctx grad_output mask = ctx saved_tensors grad_data = grad_output get_data is_masked_tensor grad_output grad_output result = as_masked_tensor grad_data mask result None Combine apply input get_data input get_mask type ignore union-attr is_masked_tensor input helper input mask _apply_docstring_templates sum input Union Tensor MaskedTensor dim DimOrDims = None keepdim Optional bool = False dtype Optional DType = None mask Optional Tensor = None - Tensor __doc__ generated _apply_docstring_templates decorator dtype None promote integer types int when output dtype specified input layout == torch sparse_csr input dtype torch uint torch bool torch int torch int torch int csr dtype=torch int implemented so using coo input ensure promoted dtype input = input to_sparse_coo dtype=torch int to_sparse_csr dtype = input dtype dtype = input dtype input dtype torch uint torch bool torch int torch int torch int dtype = torch int dim_ = _canonical_dim dim input ndim mask_input = _combine_input_and_mask sum input mask mask_input layout == torch strided torch sum mask_input dim_ bool keepdim dtype=dtype mask_input layout == torch sparse_coo _sparse_coo_scatter_reduction_helper torch sum mask_input dim_ bool keepdim dtype mask_input layout == torch sparse_csr torch _sparse_csr_sum mask_input dim=list dim_ keepdim=bool keepdim dtype=dtype raise ValueError f masked sum expects strided sparse_coo sparse_csr tensor got mask_input layout tensor _apply_docstring_templates prod input Union Tensor MaskedTensor dim DimOrDims = None keepdim Optional bool = False dtype Optional DType = None mask Optional Tensor = None - Tensor __doc__ generated _apply_docstring_templates decorator dtype None promote integer types int when output dtype specified input layout == torch sparse_csr input dtype torch uint torch bool torch int torch int torch int csr dtype=torch int implemented so using coo input ensure promoted dtype input = input to_sparse_coo dtype=torch int to_sparse_csr dtype = input dtype dtype = input dtype input dtype torch uint torch bool torch int torch int torch int dtype = torch int dim_ = _canonical_dim dim input ndim mask_input = _combine_input_and_mask prod input mask mask_input layout == torch strided Workaround https github com pytorch pytorch issues result = mask_input result = result dtype=dtype d reversed dim_ result = result prod dim=d keepdim=bool keepdim result mask_input layout == torch sparse_coo mask None See comment sparse_csr branch same issue arises sparse_coo tensors raise ValueError masked prod expects explicit mask sparse_coo tensor input _sparse_coo_scatter_reduction_helper torch prod mask_input dim_ bool keepdim dtype mask_input layout == torch sparse_csr mask None mask None corresponds all-True mask The unspecified elements CSR tensor correspond zero values Hence prod reduction result automatically zero unless all elements specified A semi-optimal way take into account use masked_prod csr mask=None == torch _sparse_csr_prod csr all csr nonzero requires implementing ` all ` ` nonzero ` support sparse csr tensors raise ValueError masked prod expects explicit mask sparse_csr tensor input torch _sparse_csr_prod mask_input dim=list dim_ keepdim=bool keepdim dtype=dtype raise ValueError f masked prod expects strided sparse_coo sparse_csr tensor got mask_input layout tensor _apply_docstring_templates cumsum input Tensor dim int dtype Optional DType = None mask Optional Tensor = None - Tensor dtype None dtype = input dtype dim_ = _canonical_dim dim input ndim mask_input = _combine_input_and_mask sum input mask mask_input layout == torch strided torch cumsum mask_input dim_ dtype=dtype dtype=dtype raise ValueError f masked cumsum expects strided tensor got mask_input layout tensor _apply_docstring_templates cumprod input Tensor dim int dtype Optional DType = None mask Optional Tensor = None - Tensor dtype None dtype = input dtype dim_ = _canonical_dim dim input ndim mask_input = _combine_input_and_mask prod input mask mask_input layout == torch strided torch cumprod mask_input dim_ dtype=dtype dtype=dtype raise ValueError f masked cumprod expects strided tensor got mask_input layout tensor _apply_docstring_templates amax input Union Tensor MaskedTensor dim DimOrDims = None keepdim Optional bool = False dtype Optional DType = None mask Optional Tensor = None - Tensor \ reduction_signature reduction_descr reduction_identity_dtype reduction_args reduction_example dtype None dtype = input dtype mask_input = _combine_input_and_mask amax input mask dim_ = _canonical_dim dim mask_input ndim mask_input layout == torch strided torch amax mask_input dim_ bool keepdim dtype=dtype mask_input layout == torch sparse_coo mask None See comment sparse_csr branch prod similar issue arises here where unspecified elements along dimension may need reduced result raise ValueError masked amax expects explicit mask sparse_coo tensor input _sparse_coo_scatter_reduction_helper torch amax mask_input dim_ bool keepdim dtype mask_input layout == torch sparse_csr mask None raise ValueError masked amax expects explicit mask sparse_csr tensor input _sparse_csr_segment_reduction_helper torch amax mask_input dim_ bool keepdim dtype raise ValueError f masked amax expects strided sparse_coo sparse_csr tensor got mask_input layout tensor _apply_docstring_templates amin input Union Tensor MaskedTensor dim DimOrDims = None keepdim Optional bool = False dtype Optional DType = None mask Optional Tensor = None - Tensor \ reduction_signature reduction_descr reduction_identity_dtype reduction_args reduction_example dtype None dtype = input dtype mask_input = _combine_input_and_mask amin input mask dim_ = _canonical_dim dim mask_input ndim mask_input layout == torch strided torch amin mask_input dim_ bool keepdim dtype=dtype mask_input layout == torch sparse_coo mask None See comment sparse_csr branch prod similar issue arises here where unspecified elements along dimension may need reduced result raise ValueError masked amax expects explicit mask sparse_coo tensor input _sparse_coo_scatter_reduction_helper torch amin mask_input dim_ bool keepdim dtype mask_input layout == torch sparse_csr mask None raise ValueError masked amin expects explicit mask sparse_csr tensor input _sparse_csr_segment_reduction_helper torch amin mask_input dim_ bool keepdim dtype raise ValueError f masked amin expects strided sparse_coo sparse_csr tensor got mask_input layout tensor _apply_docstring_templates argmax input Union Tensor MaskedTensor dim Optional int = None keepdim Optional bool = False dtype Optional DType = None mask Optional Tensor = None - Tensor \ reduction_signature reduction_descr reduction_identity_dtype reduction_args reduction_example dtype None dtype = input dtype mask_input = _combine_input_and_mask argmax input mask mask_input layout == torch strided torch argmax mask_input dim bool keepdim dtype=dtype raise ValueError f masked argmax expects strided tensor got mask_input layout tensor _apply_docstring_templates argmin input Union Tensor MaskedTensor dim Optional int = None keepdim Optional bool = False dtype Optional DType = None mask Optional Tensor = None - Tensor \ reduction_signature reduction_descr reduction_identity_dtype reduction_args reduction_example dtype None dtype = input dtype mask_input = _combine_input_and_mask argmin input mask mask_input layout == torch strided torch argmin mask_input dim bool keepdim dtype=dtype raise ValueError f masked argmin expects strided tensor got mask_input layout tensor _apply_docstring_templates mean input Union Tensor MaskedTensor dim DimOrDims = None keepdim Optional bool = False dtype Optional DType = None mask Optional Tensor = None - Tensor \ reduction_signature reduction_descr By definition identity value mean operation mean value tensor If all elements input tensor along given dimension s attr ` dim ` masked-out identity value mean undefined Due ambiguity elements output tensor strided layout correspond fully masked-out elements have ` ` nan ` ` values reduction_args reduction_example dtype_source = Optional dtype None dtype = input dtype dtype_source = Input dtype is_floating_point dtype is_complex raise ValueError f mean Could infer output dtype dtype_source dtype must either f floating point complex dtype Got dtype input layout == torch strided mask None TODO compute count analytically pyrefly ignore no-matching-overload count = sum torch ones input shape dtype=torch int device=input device dim keepdim=keepdim pyrefly ignore no-matching-overload total = sum input dim keepdim=keepdim dtype=dtype inmask = _input_mask input mask=mask count = inmask sum dim=dim keepdim=bool keepdim pyrefly ignore no-matching-overload total = sum input dim keepdim=keepdim dtype=dtype mask=inmask total count input layout == torch sparse_csr mask_input = _combine_input_and_mask mean input mask dim_ = _canonical_dim dim mask_input ndim mask None raise ValueError masked mean expects explicit mask sparse_csr tensor input _sparse_csr_segment_reduction_helper torch mean mask_input dim_ bool keepdim dtype raise ValueError f masked mean expects strided sparse_csr tensor got input layout tensor _apply_docstring_templates median input Union Tensor MaskedTensor dim int = - keepdim bool = False dtype Optional DType = None mask Optional Tensor = None - Tensor \ reduction_signature reduction_descr By definition identity value median operation median value tensor If all elements input tensor along given dimension s attr ` dim ` masked-out identity value median undefined Due ambiguity elements output tensor strided layout correspond fully masked-out elements have ` ` nan ` ` values reduction_args reduction_example dtype None dtype = input dtype dim_ = _canonical_dim dim input ndim is_float = torch is_floating_point input is_float input = input dtype=torch float mask_input = _combine_input_and_mask median input mask mask_input layout == torch strided output = torch nanmedian mask_input dim_ keepdim values is_float output is_float torch isnan output any output dtype=dtype raise ValueError masked median expects no fully masked out rows dtype floating point raise ValueError f masked median expects strided tensor got mask_input layout tensor _apply_docstring_templates logsumexp input Tensor dim DimOrDims = None keepdim bool = False dtype Optional DType = None mask Optional Tensor = None - Tensor dtype None dtype = input dtype dim_ = _canonical_dim dim input ndim mask_input = _combine_input_and_mask logsumexp input mask mask_input layout == torch strided torch logsumexp mask_input dim_ keepdim=keepdim dtype=dtype raise ValueError f masked logsumexp expects strided tensor got mask_input layout tensor Cannot use _apply_docstring_templates only set up reductions normalizations logaddexp input Union Tensor MaskedTensor other Union Tensor MaskedTensor dtype Optional DType = None input_mask Optional Tensor = None other_mask Optional Tensor = None - Tensor logaddexp input other dtype=None input_mask=None other_mask=None - Tensor Returns logaddexp all elements attr ` input ` attr ` other ` tensor The attr ` input ` elements masked out according boolean tensor attr ` input_mask ` attr ` other ` elements masked out according boolean tensor attr ` other_mask ` The shapes mask tensor tensor masked don t need match they must ref ` broadcastable broadcasting-semantics ` dimensionality mask tensor must greater than tensor masked Args input Tensor input tensor other Tensor second input tensor Keyword args dtype ` torch dtype ` optional desired data type returned tensor If specified output tensor casted attr ` dtype ` after operation performed Default None input_mask ` torch Tensor ` optional boolean tensor containing binary mask validity attr ` input ` tensor elements Default None equivalent ` ` torch ones input shape dtype=torch bool ` ` other_mask ` torch Tensor ` optional boolean tensor containing binary mask validity attr ` other ` tensor elements Default None equivalent ` ` torch ones other shape dtype=torch bool ` ` Example input = torch tensor - - - input tensor - - - other = torch tensor - - - other tensor - - - mask = torch tensor True False True mask tensor True False True torch masked _ops logaddexp input other input_mask=mask other_mask=mask tensor - -inf - dtype None dtype = input dtype input layout == torch strided other layout == torch strided mask_input = _combine_input_and_mask logaddexp input input_mask mask_other = _combine_input_and_mask logaddexp other other_mask torch logaddexp mask_input mask_other dtype=dtype raise ValueError f masked logaddexp expects strided tensors got input layout tensor input other layout other _apply_docstring_templates norm input Union Tensor MaskedTensor ord Optional float = dim DimOrDims = None keepdim Optional bool = False dtype Optional DType = None mask Optional Tensor = None - Tensor \ reduction_signature reduction_descr The identity value norm operation which used start reduction ` ` identity_float ` ` except ` ` ord=-inf ` ` ` ` identity_ord_ninf ` ` reduction_args reduction_example dtype None dtype = input dtype mask_input = _combine_input_and_mask norm input mask ord mask_input layout == torch strided dim_ = _canonical_dim dim input ndim torch linalg vector_norm mask_input ord dim_ bool keepdim dtype=dtype raise ValueError f masked norm expects strided tensor got mask_input layout tensor _std_var input Union Tensor MaskedTensor dim DimOrDims unbiased Optional bool correction_opt Optional Union int float keepdim Optional bool dtype Optional DType mask Optional Tensor take_sqrt Optional bool - Tensor assert unbiased None correction_opt None Only one unbiased correction may given correction = unbiased None correction = unbiased correction_opt None correction = sym_float correction_opt dtype None dtype = input dtype dtype is_floating_point dtype is_complex dtype = torch float compute_dtype = dtype compute_dtype is_floating_point compute_dtype is_complex compute_dtype = torch float input layout == torch strided mask None TODO compute count analytically pyrefly ignore no-matching-overload count = sum torch ones input shape dtype=torch int device=input device dim keepdim=True pyrefly ignore no-matching-overload sample_total = sum input dim keepdim=True dtype=dtype inmask = _input_mask input mask=mask count = inmask sum dim=dim keepdim=True pyrefly ignore no-matching-overload sample_total = sum input dim keepdim=True dtype=dtype mask=inmask TODO replace torch subtract divide square maximum masked subtract divide square maximum when these will available sample_mean = torch divide sample_total count x = torch subtract input sample_mean mask None pyrefly ignore no-matching-overload total = sum x x conj dim keepdim=keepdim dtype=compute_dtype total = sum x x conj dim keepdim=keepdim dtype=compute_dtype mask=inmask type ignore possibly-undefined keepdim count = count reshape total shape correction = real_dtype = corresponding_real_dtype compute_dtype compute_dtype is_complex compute_dtype count = count real_dtype count = torch subtract count correction count = torch maximum count count new_zeros output = torch divide total count dtype=dtype take_sqrt output = torch sqrt output output raise ValueError f masked std var expects strided tensor got input layout tensor _apply_docstring_templates var input Union Tensor MaskedTensor dim DimOrDims = None unbiased Optional bool = None correction Optional Union int float = None keepdim Optional bool = False dtype Optional DType = None mask Optional Tensor = None - Tensor \ reduction_signature reduction_descr The identity value sample variance operation undefined The elements output tensor strided layout correspond fully masked-out elements have ` ` nan ` ` values reduction_args reduction_example _std_var input=input dim=dim unbiased=unbiased correction_opt=correction keepdim=keepdim dtype=dtype mask=mask take_sqrt=False _apply_docstring_templates std input Union Tensor MaskedTensor dim DimOrDims = None unbiased Optional bool = None correction Optional int = None keepdim Optional bool = False dtype Optional DType = None mask Optional Tensor = None - Tensor \ reduction_signature reduction_descr The identity value sample standard deviation operation undefined The elements output tensor strided layout correspond fully masked-out elements have ` ` nan ` ` values reduction_args reduction_example _std_var input=input dim=dim unbiased=unbiased correction_opt=correction keepdim=keepdim dtype=dtype mask=mask take_sqrt=True _apply_docstring_templates softmax input Union Tensor MaskedTensor dim int dtype Optional DType = None mask Optional Tensor = None - Tensor dtype None dtype = input dtype dim_ = _canonical_dim dim input ndim mask_input = _combine_input_and_mask amax input mask mask_input layout == torch strided torch nn functional softmax mask_input dim_ dtype=dtype raise ValueError f masked softmax expects strided tensor got mask_input layout tensor _apply_docstring_templates log_softmax input Union Tensor MaskedTensor dim int dtype Optional DType = None mask Optional Tensor = None - Tensor dtype None dtype = input dtype dim_ = _canonical_dim dim input ndim mask_input = _combine_input_and_mask amax input mask mask_input layout == torch strided torch nn functional log_softmax mask_input dim_ dtype=dtype raise ValueError f masked log_softmax expects strided tensor got mask_input layout tensor _apply_docstring_templates softmin input Union Tensor MaskedTensor dim int dtype Optional DType = None mask Optional Tensor = None - Tensor dtype None dtype = input dtype dim_ = _canonical_dim dim input ndim mask_input = _combine_input_and_mask amin input mask mask_input layout == torch strided torch nn functional softmin mask_input dim_ dtype=dtype raise ValueError f masked softmin expects strided tensor got mask_input layout tensor _apply_docstring_templates normalize input Union Tensor MaskedTensor ord float dim int eps float = e- dtype Optional DType = None mask Optional Tensor = None - Tensor dtype None dtype = input dtype TODO eliminate mask_input unnecessary when using masked divide mask_input = _combine_input_and_mask sum input mask mask_input layout == torch strided nrm_ = norm input ord dim keepdim=True dtype=dtype mask=mask TODO replace torch maximum masked maximum when available denom = torch maximum nrm_ nrm_ new_full eps TODO replace torch divide masked divide when available torch divide mask_input denom raise ValueError f masked normalize expects strided tensor got mask_input layout tensor