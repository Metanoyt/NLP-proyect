Owner s module functorch inspect random unittest collections abc Callable torch torch fx fx torch nn nn functorch make_fx functorch compile memory_efficient_fusion torch _functorch compile_utils fx_graph_cse torch nn functional F torch testing _internal common_utils run_tests TestCase HAS_CUDA = torch cuda is_available _num_args fn Callable len inspect signature fn parameters gelu_bias bias y x = bias + y x + torch tanh x + x x swish x x torch sigmoid x mish x x mul torch tanh F softplus x hard_sigmoid x x + clamp min= max= div hard_swish x x x + clamp min= max= div hard_mish x x x + clamp min= max= todo convert these into tests group_std x groups int = eps float = e- flatten bool = False B C H W = x shape x_dtype = x dtype flatten x = x reshape B groups - FIXME simpler shape causing TPU XLA issues std = x float var dim= unbiased=False keepdim=True add eps sqrt x_dtype x = x reshape B groups C groups H W std = x float var dim= unbiased=False keepdim=True add eps sqrt x_dtype std expand x shape reshape B C H W EvoNorm dS nn Module __init__ num_features groups= group_size=None apply_act=True eps= e- _ super __init__ apply_act = apply_act apply activation non-linearity group_size assert num_features group_size == groups = num_features group_size groups = groups eps = eps weight = nn Parameter torch ones num_features bias = nn Parameter torch zeros num_features v = nn Parameter torch ones num_features apply_act None reset_parameters reset_parameters nn init ones_ weight nn init zeros_ bias v None nn init ones_ v forward x x_dtype = x dtype v_shape = - v None v = v view v_shape dtype=x_dtype x = x x v sigmoid group_std x groups eps x weight view v_shape dtype=x_dtype + bias view v_shape dtype=x_dtype device = cuda dtype = torch float evo_norm = EvoNorm dS evo_norm_inp = run_and_compare_activation fn inps torch jit fuser fuser device = cuda dtype = torch float isinstance fn nn Module fn = fn device=device dtype=dtype ref_args = torch randn shape device=device dtype=dtype requires_grad=True shape inps res_args = i detach clone requires_grad_ True i ref_args ref = fn ref_args ref sum backward mem_optimized_fn = memory_efficient_fusion fn _ range i res_args i grad = None res = mem_optimized_fn res_args res sum backward assertEqual ref res ref_arg res_arg zip ref_args res_args assertEqual ref_arg grad res_arg grad unittest skipIf torch cuda is_available CUDA unavailable TestMemoryEfficientOpAuthoring TestCase test_gelu_bias run_and_compare_activation gelu_bias test_mish run_and_compare_activation mish test_swish run_and_compare_activation swish test_hard_sigmoid run_and_compare_activation hard_sigmoid test_hard_swish run_and_compare_activation hard_swish test_layer_norm layer_norm x weight bias dim = - eps = e- mean = torch mean x dim keepdim=True centered = x - mean var = torch sum centered centered dim keepdim=True x size - rvar = torch sqrt var + eps normed = x - mean rvar normed weight + bias bs = ln_size = layer_norm_inps = bs ln_size ln_size ln_size run_and_compare_activation layer_norm layer_norm_inps test_rmsnorm T LayerNorm nn Module __init__ hidden_size eps= e- Construct layernorm module T style No bias no subtraction mean super __init__ weight = nn Parameter torch ones hidden_size variance_epsilon = eps forward hidden_states layer norm should always calculated float variance = hidden_states torch float pow mean - keepdim=True hidden_states = hidden_states torch rsqrt variance + variance_epsilon convert into half-precision necessary weight dtype torch float torch bfloat hidden_states = hidden_states weight dtype weight hidden_states bs = seq = hidden = t _norm = T LayerNorm hidden t _norm_inputs = bs seq hidden run_and_compare_activation t _norm t _norm_inputs TODO - Assertion failure test_hard_mish compiler compilers run_and_compare_activation hard_mish check CSE modified graph f has delta less nodes do reduce number nodes further second pass delta integer = - If delta = - only check new graph has less equal number nodes check f t delta check_val=True graph_input=False graph_input fx_g = f fx_g = make_fx f t new_graph = fx_graph_cse fx_g graph new_g = fx GraphModule fx_g new_graph number nodes decrease stay same old_num_nodes = len fx_g graph nodes new_num_nodes = len new_graph nodes delta == - assert old_num_nodes = new_num_nodes f number nodes increased old_num_nodes new_num_nodes assert old_num_nodes == new_num_nodes + delta f number nodes same old_num_nodes - delta new_num_nodes \n fx_g graph \n new_graph second pass should reduce more nodes pass_ _graph = fx_graph_cse new_graph pass_ _num_nodes = len pass_ _graph nodes assert pass_ _num_nodes == new_num_nodes f second pass graph has less node pass_ _num_nodes new_num_nodes \n new_graph \n pass_ _graph check correctness check_val true_result = fx_g t our_result = new_g t true_result None both None assert our_result None f true result None CSE result our_result results returned same assert torch all true_result == our_result f results different true_result our_result check results same NoChangeTestCase TestCase test_nochange f x = x + b = x + = x d = x + b + d t = torch randn check f t test_empty f x pass t = torch randn check f t test_rand_like f x = torch rand_like x b = torch rand_like x + b t = torch randn check f t check_val=False test_rand_n f x = torch randn b = torch randn + b t = torch randn check f t check_val=False test_hash_with_numbers Test repro issue fx_graph_cse when hash primals_ == hash primals_ torch _dynamo is_compiling skipTest Unsupported test run compiled f inpt osize size = inpt shape - s = size - s = size - scale = s osize - inpt = torch clamp inpt s scale inpt Fetch dynamic graph gms = toy_backend gm _ gms append gm gm forward torch _dynamo reset fn = torch compile backend=toy_backend dynamic=True f t = torch rand _ = fn t assert len gms == gms fx_g = gms check fx_g None check_val=False graph_input=True ReduceTestCase TestCase test_immutable_list_type f x = x sum dim= b = x sum dim= c = x sum d = x sum + b + c + d t = torch randn check f t test_immutable_list_multiple_entries f x = x sum dim= b = x sum dim= c = x sum dim= d = x sum dim= + b + c + d t = torch randn check f t test_simple f x = x cos b = x cos c = + d = b + b c + d t = torch randn check f t test_simple_ f x = x cos sin b = x cos sin c = + d = b + b c + d t = torch randn check f t test_two_args_default f x = x sum dim= b = x sum dim= keepdim=False c = x sum dim= keepdim=False d = x sum dim= + b + c + d t = torch randn check f t test_two_args f x = x sum dim= b = x sum dim= keepdim=True c = x sum dim= keepdim=True d = x sum dim= + b + c + d t = torch randn check f t test_simple_multiple_same_ops f x = x sum b = x sum c = x sum d = x sum + b + c + d t = torch randn check f t test_nested_immutable_list_type f x = torch cat x x b = torch cat x x + b t = torch randn check f t test_kwarg f x = torch ones_like x b = torch ones_like x + b t = torch randn check f t RandomOpTestCase TestCase test_random f x vals = x ops = torch clone torch cos torch tanh torch nn functional gelu _ range new_val = random choice ops random choice vals vals append new_val vals - fx_g = fx symbolic_trace f fx_g graph eliminate_dead_code fx_g recompile t = torch randn _ range check fx_g t - graph_input=True __name__ == __main__ run_tests