mypy allow-untyped-defs copy torch torch distributed dist torch distributed _shard sharding_spec shard_spec torch _C _distributed_c d ProcessGroup torch distributed _shard metadata ShardMetadata torch distributed _shard sharding_spec _internals get_chunked_dim_size get_split_size torch distributed nn functional all_to_all all_to_all_single shard Shard get_idx_from_placements placements current_rank - int Return position current rank given placements Args placements List Union _remote_device str Specifies placement each shard Tensor The size list represents number shards created This could list ` torch distributed _remote_device ` s This list could also contain string which represents remote device accepted ` torch distributed _remote_device ` current_rank int number current device Returns A int which contains position current device placement list idx placement enumerate placements type ignore attr-defined current_rank == placement rank type ignore union-attr idx raise RuntimeError current_rank placement build_reshard_metadata st_size torch Size sharding_spec shard_spec ShardingSpec world_size int - tuple list ShardMetadata list int Based given sharding spec we calculate offset local shard size We then build ShardMetadata top calculation result Args st_size torch Size The size sharded tensor sharding_spec ` torch distributed _shard sharding_spec ShardingSpec ` The specification describing how tensor sharded world_size int number ranks Returns A Tuple followings A List ` ShardMetadata ` which contains metadata shard including offsets lengths device placement A List int which contains ranks order placement shard_dim = int sharding_spec dim type ignore attr-defined shards_metadata = None world_size ranks = offsets = len st_size split_size = get_split_size st_size shard_dim world_size idx placement enumerate sharding_spec placements type ignore attr-defined ranks append placement rank sharded_dim_size = get_chunked_dim_size st_size shard_dim split_size idx local_tensor_size = list st_size local_tensor_size shard_dim = sharded_dim_size shards_metadata placement rank = ShardMetadata type ignore call-overload shard_offsets=copy deepcopy offsets shard_sizes=local_tensor_size placement=placement offsets shard_dim += sharded_dim_size shards_metadata ranks type ignore return-value reshuffle_local_shard local_shard torch Tensor st_size torch Size sharding_spec shard_spec ShardingSpec resharding_spec shard_spec ShardingSpec pg ProcessGroup - tuple list Shard list ShardMetadata Reshuffle local shard directly when reshard dim same original sharding dim Logically we do two step To collect all shards based original sharding spec Reshard tensor based given resharding spec In reality we consolidate two steps into one sending local tensor new shard directly based resharding spec Args local_shard Tensor Local tensor stored current rank st_size torch Size The size sharded tensor sharding_spec ` torch distributed _shard sharding_spec ShardingSpec ` The specification describing how tensor sharded originally resharding_spec ` torch distributed _shard sharding_spec ShardingSpec ` The specification describing how tensor will resharded pg ProcessGroup The process group aggregate Returns A Tuple followings A List ` Shard ` which contains local tensor its metadata A List ` ShardMetadata ` which contains metadata shard including offsets lengths device placement current_rank = dist get_rank pg world_size = dist get_world_size pg Build shards_metadata first shards_metadata ranks = build_reshard_metadata st_size resharding_spec world_size Get input split size all all reshard_dim = int resharding_spec dim type ignore attr-defined split_size = get_split_size st_size reshard_dim world_size input_split_sizes = world_size idx = get_idx_from_placements sharding_spec placements current_rank type ignore attr-defined new_rank = resharding_spec placements idx rank type ignore union-attr attr-defined input_split_sizes new_rank = local_shard size reshard_dim Get output split size all all output_split_sizes = world_size new_idx = ranks index current_rank sharded_dim_size = get_chunked_dim_size st_size reshard_dim split_size new_idx output_split_sizes new_rank = sharded_dim_size Get gathered_input all all local_shard = local_shard transpose reshard_dim contiguous gathered_input_size = list local_shard size gathered_input_size = sharded_dim_size gathered_input = torch empty gathered_input_size device=local_shard device dtype=local_shard dtype all all local_shard = all_to_all_single gathered_input local_shard input_split_sizes=input_split_sizes output_split_sizes=output_split_sizes group=pg local_tensor = local_shard transpose reshard_dim contiguous local_shards = Shard local_tensor shards_metadata current_rank local_shards shards_metadata reshard_local_shard local_tensor torch Tensor st_size torch Size sharding_spec shard_spec ShardingSpec resharding_spec shard_spec ShardingSpec pg ProcessGroup - tuple list Shard list ShardMetadata Reshard sharded tensor given ` ` resharding_spec ` ` When reshard dim different original sharding dim we need do two steps logically To collect all shards based original sharding spec Reshard tensor based given resharding spec In reality we consolidate two steps into one sending each rank new shard based resharding spec Args local_tensor Tensor Local tensor stored current rank st_size torch Size The size sharded tensor sharding_spec ` torch distributed _shard sharding_spec ShardingSpec ` The specification describing how tensor sharded originally resharding_spec ` torch distributed _shard sharding_spec ShardingSpec ` The specification describing how tensor will resharded pg ProcessGroup The process group aggregate Returns A Tuple followings A List ` Shard ` which contains local tensor its metadata A List ` ShardMetadata ` which contains metadata shard including offsets lengths device placement current_rank = dist get_rank pg world_size = dist get_world_size pg current_sharding_dim = int sharding_spec dim type ignore attr-defined reshard_dim = int resharding_spec dim type ignore attr-defined Build shards_metadata first shards_metadata ranks = build_reshard_metadata st_size resharding_spec world_size Compute expected size input_split_sizes = metadata shard_sizes reshard_dim metadata shards_metadata rearrange_input = any ranks i ranks i + i range len ranks - rearrange_input Need re-arrange reshard_dim local_tensor before all all indices list int = metadata shards_metadata offset_start_idx = metadata shard_offsets reshard_dim split_size = metadata shard_sizes reshard_dim indices += range offset_start_idx offset_start_idx + split_size local_tensor = local_tensor index_select reshard_dim torch tensor indices device=local_tensor device Because reshard_dim = original shard_dim We need compute size tensor each rank output_tensor_list = torch tensor world_size split_size = get_split_size st_size current_sharding_dim world_size rearrange_output_list = False indices = idx placement enumerate sharding_spec placements type ignore attr-defined sharded_dim_size = get_chunked_dim_size st_size current_sharding_dim split_size idx output_tensor_size = list st_size output_tensor_size current_sharding_dim = sharded_dim_size output_tensor_size reshard_dim = input_split_sizes current_rank output_tensor_list placement rank = torch empty type ignore union-attr index output_tensor_size device=local_tensor device dtype=local_tensor dtype indices append placement rank type ignore union-attr index arg-type idx = placement rank type ignore union-attr rearrange_output_list = True Perform autograd enabled all all input_tensor_tuple = torch split local_tensor input_split_sizes dim=reshard_dim input_tensor_list = tensor contiguous tensor input_tensor_tuple output_tensor_list = all_to_all output_tensor_list input_tensor_list group=pg rearrange_output_list Need re-arrange original shard_dim output_tensor_list output_tensor_list = output_tensor_list idx idx indices type ignore call-overload local_tensor = torch cat output_tensor_list dim=current_sharding_dim local_shards = Shard local_tensor shards_metadata current_rank local_shards shards_metadata