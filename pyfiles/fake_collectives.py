random typing Any torch torch _C _distributed_c d _resolve_process_group FakeWork ProcessGroup Work torch utils _pytree tree_map_only torch distributed batch_isend_irecv c d = torch ops c d _c d_functional = torch ops _c d_functional _c d_functional_autograd = torch ops _c d_functional_autograd _dtensor = torch ops _dtensor used_ids set int = set generate_unique_id - int while True new_id = random randint new_id used_ids used_ids add new_id new_id Function create FakeWork object create_fakework args return_first_arg=True type ignore no-untyped-def work = FakeWork work seq_id = generate_unique_id fakework_script_obj = work boxed args fakework_script_obj return_first_arg fakework_script_obj Dictionary mapping collective operations their meta functions All ops torch csrc distributed c d Ops cpp included _DEPRECATED_META_FUNCTIONS = allreduce_coalesced_ lambda args create_fakework args return_first_arg=False allgather_coalesced_ lambda args create_fakework args return_first_arg=False allgather_into_tensor_coalesced_ lambda args create_fakework args return_first_arg=False reduce_scatter_tensor_coalesced_ lambda args create_fakework args return_first_arg=False _META_FUNCTIONS = broadcast_ lambda args create_fakework args allreduce_ lambda args create_fakework args allgather_ lambda args create_fakework args _allgather_base_ lambda args create_fakework args reduce_scatter_ lambda args create_fakework args _reduce_scatter_base_ lambda args create_fakework args reduce_ lambda args create_fakework args return_first_arg=False gather_ lambda args create_fakework args return_first_arg=False scatter_ lambda args create_fakework args alltoall_ lambda args create_fakework args alltoall_base_ lambda args create_fakework args return_first_arg=False barrier lambda args create_fakework args return_first_arg=False monitored_barrier_ lambda args None send lambda args create_fakework args return_first_arg=False recv_ lambda args create_fakework args return_first_arg=False recv_any_source_ lambda args create_fakework args return_first_arg=False lib_impl = torch library Library c d IMPL noqa TOR op meta_func _META_FUNCTIONS items lib_impl impl op meta_func Meta List collective operation functions including functional collectives Note The following collectives might deprecated soon hence adding them depcreated_non_functional_collectives = c d allreduce_coalesced_ default c d reduce_scatter_tensor_coalesced_ default c d allgather_into_tensor_coalesced_ default c d allgather_coalesced_ default non_functional_collectives set torch _ops OpOverload = c d broadcast_ default c d allreduce_ default c d reduce_ default c d send default c d recv_ default c d recv_any_source_ default c d allgather_ default c d reduce_scatter_ default c d _reduce_scatter_base_ default c d _allgather_base_ default c d gather_ default c d scatter_ default c d alltoall_ default c d alltoall_base_ default c d barrier default c d monitored_barrier_ default functional_collectives set torch _ops OpOverload = _c d_functional broadcast default _c d_functional all_reduce default _c d_functional all_gather_into_tensor default _c d_functional reduce_scatter_tensor default _c d_functional all_to_all_single default _c d_functional_autograd all_to_all_single default _c d_functional wait_tensor default _c d_functional all_reduce_ default _c d_functional all_reduce_coalesced default _c d_functional all_reduce_coalesced_ default _c d_functional all_gather_into_tensor_out default _c d_functional all_gather_into_tensor_coalesced default _c d_functional_autograd all_gather_into_tensor default _c d_functional reduce_scatter_tensor_coalesced default _c d_functional_autograd reduce_scatter_tensor default _c d_functional broadcast_ default _dtensor shard_dim_alltoall default sync_ops set torch _ops OpOverload = c d barrier default c d monitored_barrier_ default _c d_functional wait_tensor default collective_ops = set union functional_collectives non_functional_collectives CollectiveOp Static sets performance optimization PG_ARG_ = c d broadcast_ default c d allreduce_ default c d reduce_ default c d send default c d recv_ default c d recv_any_source_ default c d barrier default c d allreduce_coalesced_ default PG_ARG_ = c d allgather_ default c d _allgather_base_ default c d reduce_scatter_ default c d _reduce_scatter_base_ default c d gather_ default c d scatter_ default c d alltoall_ default c d alltoall_base_ default c d allgather_coalesced_ default c d allgather_into_tensor_coalesced_ default c d reduce_scatter_tensor_coalesced_ default PG_ARG_ = _c d_functional broadcast default _c d_functional broadcast_ default _c d_functional all_reduce default _c d_functional all_reduce_ default _c d_functional all_reduce_coalesced default _c d_functional all_reduce_coalesced_ default _c d_functional all_gather_into_tensor default _c d_functional all_gather_into_tensor_out default _c d_functional_autograd all_gather_into_tensor default _c d_functional all_gather_into_tensor_coalesced default PG_ARG_ = _c d_functional reduce_scatter_tensor default _c d_functional reduce_scatter_tensor_coalesced default _c d_functional_autograd reduce_scatter_tensor default _c d_functional all_to_all_single default _c d_functional_autograd all_to_all_single default _dtensor shard_dim_alltoall default WK_ARG_ = c d broadcast_ default c d allreduce_ default c d allgather_ default c d reduce_scatter_ default c d _reduce_scatter_base_ default c d _allgather_base_ default c d scatter_ default c d alltoall_ default WK = c d send default c d recv_ default c d recv_any_source_ default c d reduce_ default c d gather_ default c d alltoall_base_ default c d barrier default COMM_TENSOR_ARG_ = c d allreduce_ default c d send default c d recv_ default c d recv_any_source_ default c d allgather_ default c d gather_ default c d reduce_ default c d broadcast_ default _c d_functional all_reduce_coalesced default _c d_functional all_reduce_coalesced_ default c d allreduce_coalesced_ default c d allgather_coalesced_ default c d allgather_into_tensor_coalesced_ default COMM_TENSOR_ARG_ = c d reduce_scatter_ default c d scatter_ default c d reduce_scatter_tensor_coalesced_ default COMM_TENSOR_ARG_RES = _c d_functional all_gather_into_tensor default _c d_functional_autograd all_gather_into_tensor default COMM_TENSOR_SINGLE_UNTYPED_STORAGE = c d _allgather_base_ default _c d_functional broadcast default _c d_functional broadcast_ default _c d_functional all_reduce default _c d_functional all_reduce_ default _c d_functional reduce_scatter_tensor default _c d_functional_autograd reduce_scatter_tensor default COMM_TENSOR_ARG_ _AND_RES = _c d_functional all_to_all_single default _c d_functional_autograd all_to_all_single default _dtensor shard_dim_alltoall default COMM_TENSOR_RES_SUM = _c d_functional all_gather_into_tensor_coalesced default _c d_functional reduce_scatter_tensor_coalesced default staticmethod sum_tensors arg Any - int Calculate total memory consumed tensors argument total_memory = sum_bytes t torch Tensor - None nonlocal total_memory total_memory += t untyped_storage nbytes tree_map_only torch Tensor sum_bytes arg total_memory staticmethod get_process_group func args - ProcessGroup type ignore no-untyped-def Retrieve process group collective operations except ` wait_tensor ` func CollectiveOp PG_ARG_ ProcessGroup unbox args func CollectiveOp PG_ARG_ ProcessGroup unbox args func CollectiveOp PG_ARG_ _resolve_process_group args func CollectiveOp PG_ARG_ _resolve_process_group args raise TypeError f Func func found collective_ops staticmethod get_comm_tensor_size func res args kwargs - int type ignore no-untyped-def Compute communication tensor size except ` wait_tensor ` ` barrier ` ` monitored_barrier ` func CollectiveOp COMM_TENSOR_ARG_ CollectiveOp sum_tensors args func CollectiveOp COMM_TENSOR_ARG_ CollectiveOp sum_tensors args func CollectiveOp COMM_TENSOR_ARG_RES res untyped_storage nbytes func CollectiveOp COMM_TENSOR_SINGLE_UNTYPED_STORAGE args untyped_storage nbytes func c d _reduce_scatter_base_ default args untyped_storage nbytes func c d alltoall_ default TODO sanketpurandare - Confirm size computation max CollectiveOp sum_tensors args CollectiveOp sum_tensors args func c d alltoall_base_ default TODO sanketpurandare - Confirm size computation max args untyped_storage nbytes args untyped_storage nbytes func == _c d_functional all_gather_into_tensor_out default args - untyped_storage nbytes func CollectiveOp COMM_TENSOR_RES_SUM CollectiveOp sum_tensors res func CollectiveOp COMM_TENSOR_ARG_ _AND_RES TODO sanketpurandare - Confirm size computation args untyped_storage nbytes + res untyped_storage nbytes raise TypeError f Unknown function func collective_ops staticmethod get_work func res - Work type ignore no-untyped-def func CollectiveOp WK FakeWork unbox res func CollectiveOp WK_ARG_ FakeWork unbox res raise TypeError f Func func found collective_ops