Owner s oncall distributed time unittest weakref test_c d_common torch torch distributed dist torch nn nn torch _C _distributed_c d _create_work_from_future torch futures Future torch nn parallel DistributedDataParallel DDP torch testing _internal common_cuda TEST_CUDA torch testing _internal common_distributed MultiThreadedTestCase torch testing _internal common_utils run_tests TestCase create_work result future = Future future set_result result _create_work_from_future future MyWork dist _Work __init__ result pg super __init__ result_ = result future_ = torch futures Future future_ set_result result pg_ = weakref ref pg wait timeout pg_ wait_count += True get_future pg_ get_future_count += future_ LonelyRankProcessGroup dist ProcessGroup This PG only supports world_size __init__ rank world use_wrapper super __init__ rank world assert rank == assert world == _rank = rank _world = world wait_count = get_future_count = use_wrapper = use_wrapper _work = broadcast tensor_list opts use_wrapper create_work tensor_list res = MyWork tensor_list _work append res res allgather output_tensors input_tensor opts o i zip output_tensors input_tensor o copy_ i use_wrapper create_work output_tensors res = MyWork output_tensors _work append res res allreduce tensors opts use_wrapper create_work tensors res = MyWork tensors _work append res res getSize _world getBackendName lonely-pg __repr__ f PLG w _world r _rank DummyAttrProcessGroup dist ProcessGroup getRank getSize getBackendName dummy-attr setGroupName name - None _group_name = py + name getGroupName - str _group_name setGroupDesc group_desc - None _group_desc = py + group_desc getGroupDesc - str _group_desc We cannot use parametrize some tests defined base use _get_process_group AbstractDDPSingleRank test_c d_common CommonDistributedDataParallelTest setUp super setUp _spawn_threads property world_size _get_process_group LonelyRankProcessGroup rank world_size use_wrapper test_ddp_invoke_work_object pg = _get_process_group torch manual_seed model = nn Sequential nn Linear nn ReLU wrapped_model = model input_tensor = torch rand model = DDP model process_group=pg model input_tensor sum backward ddp_grad = wrapped_model bias grad clone wrapped_model zero_grad wrapped_model input_tensor sum backward assertEqual wrapped_model bias grad ddp_grad use_wrapper assertTrue pg wait_count assertTrue pg get_future_count test_ddp_with_pypg pg = _get_process_group _test_ddp_with_process_group pg torch device cpu device_ids=None test_ddp_with_pypg_with_grad_views pg = _get_process_group _test_ddp_with_process_group pg torch device cpu device_ids=None gradient_as_bucket_view=True test_ddp_no_init_sync pg = _get_process_group model = nn Sequential nn Linear nn ReLU model = DDP model process_group=pg init_sync=False assertEqual pg wait_count assertEqual pg get_future_count TestDDPWithWorkSubclass AbstractDDPSingleRank MultiThreadedTestCase property use_wrapper False TestDDPWithWorkWrapper AbstractDDPSingleRank MultiThreadedTestCase property use_wrapper True BlockWork dist _Work Dummy work used test blocking current stream __init__ super __init__ future_ = torch futures Future get_future future_ TestPyProcessGroup TestCase test_attr_overrides pg = DummyAttrProcessGroup assertEqual pg name dummy-attr assertEqual pg rank assertEqual pg size pg _set_group_name name assertEqual pg group_name py name pg _set_group_desc desc assertEqual pg group_desc py desc test_abort_shutdown - None verify noops pg = DummyAttrProcessGroup pg abort pg shutdown unittest skipIf TEST_CUDA no cuda xpu test_block_current_stream - None torch cuda synchronize stream = torch cuda Stream stream nothing queue so instantly resolves event = torch cuda Event event record time sleep assertTrue event query work = BlockWork work block_current_stream stream blocked so doesn t resolve event = torch cuda Event event record time sleep assertFalse event query resolve work work get_future set_result None stream synchronize assertTrue event query unittest skipIf TEST_CUDA no cuda xpu test_block_current_stream_use_after_free - None This tests CPU control tensor freed before CUDA kernel executes torch cuda synchronize stream = torch cuda Stream stream = BlockWork block_current_stream b = BlockWork b block_current_stream unblock b first though still blocking b get_future set_result None delete b del b still blocking so doesn t resolve event = torch cuda Event event record time sleep assertFalse event query unblock get_future set_result None stream synchronize assertTrue event query __name__ == __main__ run_tests