Owner s oncall distributed To run python test distributed test_cupy_as_tensor py dataclasses dataclass torch torch multiprocessing reductions reduce_tensor torch testing _internal common_cuda SM OrLater torch testing _internal common_distributed MultiProcContinuousTest torch testing _internal common_utils requires_cuda_p p_access run_tests skip_but_pass_in_sandcastle_if So tests written device-agnostic way device_type = cuda device_module = torch get_device_module device_type dataclass CupyWrapper data_ptr int size_in_bytes int property __cuda_array_interface__ shape size_in_bytes typestr &#124; u data data_ptr False version from_buffer data_ptr int size_in_bytes int device str dtype torch dtype - torch Tensor data = torch as_tensor CupyWrapper data_ptr size_in_bytes device=device view dtype assert data data_ptr == data_ptr data requires_cuda_p p_access CupyAsTensorTest MultiProcContinuousTest classmethod backend_str cls gloo _init_device - None need use vmm api test see https forums developer nvidia com t inconsistent-behavior-of-cudapointergetattributes-between-cudamalloc-ipc-and-vmm-based-ipc noqa B torch cuda memory _set_allocator_settings expandable_segments True init pin process device device_module set_device device torch empty device=self device property device - torch device torch device device_type rank skip_but_pass_in_sandcastle_if SM OrLater Fails ran docker environment without privileged access https github com pytorch pytorch issues test_cupy_as_tensor - None Test torch as_tensor works cupy array interface zero-copy when pointer p p-shared across processes _init_device tensor torch Tensor rank == seems only error rank non-zero will caught test tensor = torch randn device=self device tensor_meta = reduce_tensor tensor torch distributed broadcast_object_list tensor_meta src= recv_list = None torch distributed broadcast_object_list recv_list src= tensor_meta = recv_list func args = tensor_meta args = list args args = rank ipc_tensor = func args tensor = from_buffer ipc_tensor data_ptr ipc_tensor numel ipc_tensor element_size device ipc_tensor dtype torch distributed barrier rank == tensor fill_ device_module synchronize torch distributed barrier assert tensor allclose tensor torch distributed barrier classmethod tearDownClass cls torch cuda memory _set_allocator_settings expandable_segments False super tearDownClass __name__ == __main__ run_tests