Copyright c Meta Platforms Inc affiliates functools partial typing Any Optional torch torch distributed tensor DeviceMesh DTensor Replicate Shard __all__ = input_reshard input_reshard module torch nn Module tp_device_mesh DeviceMesh input_reshard_dim Optional int = None - torch nn Module Register hooks nn Module input resharding enabling sharding restoration during backward computation Register hooks nn Module input resharding so we can shard per given ` tp_device_mesh ` ` input_reshard_dim ` restore input back when recomputing activations backward The reason why we can do Tensor Parallel TP input same across all TP ranks Args module ` nn Module ` Module registered input resharding tp_device_mesh ` DeviceMesh ` Object which describes mesh topology devices Tensor Parallel input_reshard_dim Optional int The dimension where we perform sharding input If set None there no sharding input Default None Return A ` nn Module ` object registered TP input resharding input_reshard_dim None module cx Optional torch autograd graph saved_tensors_hooks = None input_reshard_forward_pre_hook _ torch nn Module _i tuple Any - None saved_tensor_hooks = torch autograd graph saved_tensors_hooks partial _pack_hook_tp tp_device_mesh input_reshard_dim partial _unpack_hook_tp tp_device_mesh input_reshard_dim saved_tensor_hooks __enter__ nonlocal cx cx = saved_tensor_hooks type ignore name-defined input_reshard_backward_hook _ torch nn Module _i tuple Any _o Any - Any nonlocal cx cx __exit__ type ignore name-defined union-attr module register_forward_pre_hook input_reshard_forward_pre_hook module register_forward_hook input_reshard_backward_hook module _pack_hook_tp mesh DeviceMesh input_reshard_dim int x torch Tensor - Any noqa D Hook function called after FWD shard input isinstance x DTensor all p is_replicate p x _spec placements x redistribute device_mesh=mesh placements= Shard input_reshard_dim isinstance x DTensor isinstance x torch Tensor x numel = mesh size DTensor from_local x device_mesh=mesh redistribute device_mesh=mesh placements= Shard input_reshard_dim to_local x _unpack_hook_tp mesh DeviceMesh input_reshard_dim int x Any - torch Tensor noqa D Hook function called before activation recomputing BWD restore input isinstance x DTensor len x _spec placements == x _spec placements is_shard x redistribute device_mesh=mesh placements= Replicate isinstance x DTensor isinstance x torch Tensor x numel = mesh size DTensor from_local x device_mesh=mesh placements= Shard input_reshard_dim redistribute device_mesh=mesh placements= Replicate to_local x