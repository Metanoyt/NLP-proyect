mypy allow-untyped-defs json math os re typing Optional torch torch utils benchmark benchmark torch _C _profiler _EventType _ExtraFields_PyCall _ExtraFields_PyCCall _ExtraFields_TorchOp _ProfilerEvent torch profiler profile torch profiler _utils index_of_first_match traverse_bfs traverse_dfs Pattern Base all patterns subclass implement match define custom patterns In subclass define description skip property __init__ prof profile should_benchmark bool = False - None prof = prof should_benchmark = should_benchmark name = Please specify name pattern description = Please specify description pattern url = assert prof profiler None prof profiler kineto_results None event_tree = prof profiler kineto_results experimental_event_tree tid_root dict int list _ProfilerEvent = event event_tree tid_root setdefault event start_tid append event property skip - bool False report event _ProfilerEvent msg = f description \n Source Code Location source_code_location event msg eventTreeTraversal Traverse event tree yield all events Override method subclass customize traversal yield traverse_dfs event_tree summary events list _ProfilerEvent default_summary = f name len events events matched should_benchmark If benchmark summary empty use benchmark_summary events hasattr benchmark type ignore attr-defined default_summary default_summary benchmark_summary events list _ProfilerEvent - str format_time time_ns int - str unit_lst = ns us ms unit unit_lst time_ns f time_ns f unit time_ns = f time_ns f s assert hasattr benchmark Please implement benchmark shapes_factor_map = benchmark events type ignore attr-defined original_time = sum event duration_time_ns event events new_time = sum shapes_factor_map input_shapes event event duration_time_ns event events f name len events events matched f Total Estimated Speedup format_time original_time - new_time round original_time new_time X match event _ProfilerEvent Return True event matches pattern This method should overridden subclass raise NotImplementedError matched_events skip matched_events = event event eventTreeTraversal match event matched_events root_of event _ProfilerEvent while event parent event = event parent event siblings_of event _ProfilerEvent event parent children = event parent children children = tid_root event start_tid index = children index event children index children index + next_of event _ProfilerEvent _ next_events = siblings_of event next_events next_events None prev_of event _ProfilerEvent prev_events _ = siblings_of event prev_events - prev_events None go_up_until event _ProfilerEvent predicate event None while event parent predicate event event = event parent event Patterns NamePattern Pattern __init__ prof profile name str should_benchmark bool = False - None super __init__ prof should_benchmark description = f Matched Name Event name name = name match event _ProfilerEvent re search name event name None ExtraCUDACopyPattern Pattern This pattern identifies we creates constant tensor CPU immediately moves GPU example torch zeros cuda Pattern built-in method &#124; built-in method &#124; aten aten fill_ aten zero_ &#124; aten _to_copy Algorithm We start node aten go parent events previous events check we have aten fill_ aten zero_ we keep going down tree We always select last child children list when we go down tree If any step we failed match __init__ prof profile should_benchmark bool = False - None super __init__ prof should_benchmark name = Extra CUDA Copy Pattern description = Filled CPU tensor immediately moved GPU Please initialize GPU url = https pytorch org tutorials recipes recipes tuning_guide html#create-tensors-directly-on-the-target-device init_ops = aten fill_ aten zero_ aten normal_ aten uniform_ property skip - bool prof with_stack prof record_shapes match event TODO We should also check tensor identities event name = aten False to_event = event event children False event = event children - event name = aten _to_copy False event children False event = event children - event name = aten copy_ False aten copy_ should have first args dtype same dtypes = input_dtypes event len dtypes False dtypes None dtypes = dtypes False event = to_event Up one level event = event parent event None False Check we have aten fill_ previous leaf event = prev_of event event None False while event children event = event children - aten zero_ special optimization case where fill_ called event name init_ops True event name init_ops TODO Check tensor reused benchmark events list _ProfilerEvent shapes_factor_map = input_shapes event event events shape shapes_factor_map size = shape to_timer = benchmark Timer stmt= torch ones size cuda globals= size size de_timer = benchmark Timer stmt= torch ones size device= cuda globals= size size to_time = to_timer timeit mean de_time = de_timer timeit mean shapes_factor_map shape = de_time to_time shapes_factor_map ForLoopIndexingPattern Pattern This pattern identifies we use loop index tensor can vectorized example tensor = torch empty i range tensor i = i Pattern aten select &#124; &#124; aten select &#124; Repeat Algorithm We start node aten select we check we can find alternating patterns We also keep dictionary avoid duplicate match loop __init__ prof profile should_benchmark bool = False - None super __init__ prof should_benchmark name = For Loop Indexing Pattern description = For loop indexing detected Vectorization recommended visited set int = set eventTreeTraversal We need use BFS traversal order avoid duplicate match yield traverse_bfs event_tree match event _ProfilerEvent event name = aten select False event id visited False repeat_count = _ next = siblings_of event len next = False Custom event list matching same_ops list list - bool len list = len list False op op zip list list strict=True op name = op name False True Record ops between two aten select next_select_idx = index_of_first_match next lambda e e name == aten select next_select_idx None False indexing_ops = event + next next_select_idx next = next len indexing_ops - i range len next len indexing_ops same_ops indexing_ops next i i + len indexing_ops repeat_count += visited add next i id break repeat_count = FP MatMulPattern Pattern __init__ prof profile should_benchmark bool = False - None super __init__ prof should_benchmark name = FP MatMul Pattern description = You currently using GPU supports TF Please enable TF setting torch backends cuda matmul allow_tf = True url = https pytorch org docs stable notes cuda html#tensorfloat- -tf -on-ampere-devices property skip torch version hip None has_tf = False Anything less than sm_ Ampere which doesn t support TF has_tf = all int re sub sm_ &#124; compute_ arch = arch torch cuda get_arch_list has_tf False super skip prof record_shapes match event _ProfilerEvent - bool If we saw pattern once we don t need match again event tag = _EventType TorchOp False assert isinstance event extra_fields _ExtraFields_TorchOp event name == aten mm event extra_fields allow_tf _cublas False True False report event _ProfilerEvent description benchmark events list _ProfilerEvent shapes_factor_map = input_shapes event event events shape shapes_factor_map matrixA = torch randn shape device= cuda dtype=torch float matrixB = torch randn shape device= cuda dtype=torch float fp _timer = benchmark Timer stmt= torch mm matrixA matrixB globals= matrixA matrixA matrixB matrixB tf _timer = benchmark Timer stmt= torch mm matrixA matrixB setup= torch backends cuda matmul allow_tf = True globals= matrixA matrixA matrixB matrixB torch backends cuda matmul allow_tf = False fp _time = fp _timer timeit mean tf _time = tf _timer timeit mean shapes_factor_map shape = tf _time fp _time shapes_factor_map OptimizerSingleTensorPattern Pattern This pattern identifies we using single-tensor version optimizer example optimizer = torch optim SGD model parameters lr= By adding foreach=True enable multi-tensor optimizer we can gain speedup when kernels relatively small Pattern XXXXX _single_tenser_ OPTIMIZER_NAME Algorithm String match __init__ prof profile should_benchmark bool = False - None super __init__ prof should_benchmark name = Optimizer Single Tensor Pattern optimizers_with_foreach = adam sgd adamw description = Detected optimizer running single tensor implementation Please enable multi tensor implementation passing foreach=True into optimizer url = match event _ProfilerEvent - bool optimizer optimizers_with_foreach event name endswith f _single_tensor_ optimizer True False SynchronizedDataLoaderPattern Pattern This pattern identifies we using num_workers= DataLoader example torch utils data DataLoader dataset batch_size=batch_size Add num_workers=N arguments N depends system configuration Pattern dataloader py __iter__ dataloader py _get_iterator NOT dataloader py check_worker_number_rationality Algorithm If we don t see check_worker_number_rationality call dataloader __iter__ It asynchronous dataloader __init__ prof profile should_benchmark bool = False - None super __init__ prof should_benchmark name = Synchronized DataLoader Pattern description = Detected DataLoader running synchronized implementation Please enable asynchronous dataloading setting num_workers when initializing DataLoader url = https pytorch org tutorials recipes recipes tuning_guide html #enable-async-data-loading-and-augmentation match event _ProfilerEvent - bool is_dataloader_function name str function_name str name startswith os path join torch utils data dataloader py name endswith function_name TODO fixme Due lifetime issues function name field might actually point already freed string when even PyCall Just silently skip unblock testing try event name except UnicodeDecodeError False is_dataloader_function event name __iter__ False event children False event = event children is_dataloader_function event name _get_iterator False event children False event = event children is_dataloader_function event name check_worker_number_rationality TODO We should also check loader bottleneck GradNotSetToNonePattern Pattern This pattern identifies we setting grad None zero_grad example optimizer zero_grad By setting set_to_none=True we can gain speedup Pattern XXXXX _zero_grad NOT aten zeros aten zero_ aten zero_ called each parameter model We also want make sure called aten zeros Algorithm String match __init__ prof profile should_benchmark bool = False - None super __init__ prof should_benchmark name = Gradient Set To Zero Instead None Pattern description = Detected gradient set zero instead None Please add set_to_none=True when calling zero_grad url = https pytorch org tutorials recipes recipes tuning_guide html #disable-gradient-calculation-for-validation-or-inference match event _ProfilerEvent - bool event name endswith zero_grad False event children False sub_event traverse_dfs event children sub_event name == aten zero_ sub_event parent name = aten zeros True TODO We should also check optimizer s numerical behavior will change False Conv dBiasFollowedByBatchNorm dPattern Pattern This pattern identifies we enabling bias Conv d which followed BatchNorm d Bias doesn t do anything when followed batchnorm Pattern nn Module Conv d &#124; nn Module BatchNorm d aten conv d AND dtype third argument null The third argument bias Algorithm String match __init__ prof profile should_benchmark bool = False - None super __init__ prof should_benchmark name = Enabling Bias Conv d Followed By BatchNorm Pattern description = Detected bias enabled Conv d followed BatchNorm d Please set bias=False Conv d url = https pytorch org tutorials recipes recipes tuning_guide html #disable-bias-for-convolutions-directly-followed-by-a-batch-norm property skip prof record_shapes False super skip match event _ProfilerEvent event name = aten conv d False len input_dtypes event input_dtypes event None False This means bias=True event = go_up_until event lambda e e name startswith nn Module Conv d event False event = next_of event event False event name startswith nn Module BatchNorm d MatMulDimInFP Pattern Pattern __init__ prof profile should_benchmark bool = False - None super __init__ prof should_benchmark name = Matrix Multiplication Dimension Not Aligned Pattern description = Detected matmul dimension aligned Please use matmul aligned dimension url = https pytorch org tutorials recipes recipes tuning_guide html#use-mixed-precision-and-amp property skip - bool prof with_stack prof record_shapes match event _ProfilerEvent - bool mutiple_of shapes multiple all dim multiple == shape shapes dim shape - event name aten mm aten bmm aten addmm False input_dtypes event False arg_dtype = input_dtypes event arg_dtype torch bfloat torch half mutiple_of input_shapes event True False benchmark events list _ProfilerEvent closest_multiple shapes multiple multiple math ceil shape multiple shape shapes shapes_factor_map = input_shapes event event events shape shapes_factor_map matrixA = torch randn shape device= cuda dtype=torch float matrixB = torch randn shape device= cuda dtype=torch float not_aligned_dim_timer = benchmark Timer stmt= torch mm matrixA matrixB globals= matrixA matrixA matrixB matrixB matrixA = torch randn closest_multiple shape device= cuda dtype=torch float matrixB = torch randn closest_multiple shape device= cuda dtype=torch float aligned_dim_timer = benchmark Timer stmt= torch mm matrixA matrixB globals= matrixA matrixA matrixB matrixB not_aligned_dim_time = not_aligned_dim_timer timeit mean aligned_dim_time = aligned_dim_timer timeit mean shapes_factor_map shape = aligned_dim_time not_aligned_dim_time shapes_factor_map source_code_location event Optional _ProfilerEvent - str while event event tag == _EventType PyCall event tag == _EventType PyCCall assert isinstance event extra_fields _ExtraFields_PyCall _ExtraFields_PyCCall event extra_fields caller file_name startswith torch + os sep f event extra_fields caller file_name event extra_fields caller line_number event = event parent No source code location found input_shapes event _ProfilerEvent assert isinstance event extra_fields _ExtraFields_TorchOp tuple tuple getattr i sizes i event extra_fields inputs input_dtypes event _ProfilerEvent assert isinstance event extra_fields _ExtraFields_TorchOp tuple getattr i dtype None i event extra_fields inputs report_all_anti_patterns prof should_benchmark bool = False print_enable bool = True json_report_dir Optional str = None - None report_dict dict = anti_patterns = ExtraCUDACopyPattern prof should_benchmark ForLoopIndexingPattern prof should_benchmark FP MatMulPattern prof should_benchmark OptimizerSingleTensorPattern prof should_benchmark SynchronizedDataLoaderPattern prof should_benchmark GradNotSetToNonePattern prof should_benchmark Conv dBiasFollowedByBatchNorm dPattern prof should_benchmark MatMulDimInFP Pattern prof should_benchmark reported = set summaries = message_list = f - TorchTidy Report - message_list append Matched Events anti_pattern anti_patterns matched_events = anti_pattern matched_events matched_events continue summaries append anti_pattern summary matched_events event matched_events report_msg = anti_pattern report event report_msg reported message_list append report_msg reported add report_msg src_location line_no = source_code_location event split report_dict setdefault src_location append line_number int line_no name anti_pattern name url anti_pattern url message anti_pattern description json_report_dir None json_report_path = os path join json_report_dir torchtidy_report json os path exists json_report_path open json_report_path f exisiting_report = json load f exisiting_report update report_dict report_dict = exisiting_report open json_report_path w f json dump report_dict f indent= message_list append Summary message_list += summaries message_list append f - TorchTidy Report - print_enable print \n join message_list