Owner s module inductor unittest unittest mock unittest mock MagicMock torch torch _inductor ir Buffer FixedLayout FlexibleLayout torch _inductor lowering register_lowering torch _inductor select_algorithm autotune_select_algorithm torch _inductor test_case run_tests TestCase torch testing _internal common_utils skipIfXpu TEST_WITH_ROCM torch testing _internal inductor_utils GPU_TYPE HAS_CPU HAS_GPU decomposeK b kPartitions m = shape n = b shape k = shape B = k kPartitions a_reshaped = torch permute reshape m B kPartitions b_reshaped = b reshape B kPartitions n result = torch bmm a_reshaped b_reshaped out_dtype=torch float result_fp = result torch float reduced_buf = torch sum result_fp reduced_buf dtype TestSubgraphChoice TestCase setUp super setUp _create_buffer name shape dtype Buffer name=name layout=FixedLayout torch device f GPU_TYPE dtype=dtype size=shape skipIfXpu unittest skipIf TEST_WITH_ROCM decompose_k supported ROCm test_subgraph_decompose_k torch _inductor kernel mm aten_mm torch _inductor kernel mm_common mm_args mat _shape mat _shape = torch library custom_op mylib matmul_decompose mutates_args= matmul_decompose torch Tensor b torch Tensor - torch Tensor b matmul_decompose register_fake _ b b register_lowering torch ops mylib matmul_decompose _ b _ _ _ layout mat mat = mm_args b choices = aten_mm bind mat mat layout kPartitions = decompose_k_subgraph_template = torch _inductor kernel mm DecomposeKSugraphTemplate decompose_k_subgraph_template maybe_append_choice choices k_split=kPartitions input_nodes= mat mat layout=layout Test benchmarking against aten autotune_select_algorithm test_subgraph_choice choices b layout Only decomposeK case codegen choices = choices autotune_select_algorithm test_subgraph_choice choices b layout a_in = torch randn mat _shape dtype=torch float device=torch device f GPU_TYPE b_in = torch randn mat _shape dtype=torch float device=torch device f GPU_TYPE func mat mat torch ops mylib matmul_decompose mat mat compiled_func = torch compile func mode= max-autotune dynamic=False res = compiled_func a_in b_in Check same results compiled result regular torch mm torch testing assert_close res a_in b_in atol= e- rtol= e- skipIfXpu unittest skipIf TEST_WITH_ROCM decompose_k supported ROCm test_subgraph_freeze_layout torch _inductor kernel mm_common mm_args M N K = a_in = torch randn M K dtype=torch bfloat device=torch device f GPU_TYPE b_in = torch randn K N dtype=torch bfloat device=torch device f GPU_TYPE torch library custom_op mylib matmul_decompose_padding mutates_args= matmul_decompose torch Tensor b torch Tensor - torch Tensor b matmul_decompose register_fake _ b b register_lowering torch ops mylib matmul_decompose_padding _ b _ _ _ layout mat mat = mm_args b mat _layout = mat layout assert isinstance mat _layout FlexibleLayout mat _stride = mat _layout stride choices = kPartitions = decompose_k_subgraph_template = torch _inductor kernel mm DecomposeKSugraphTemplate decompose_k_subgraph_template maybe_append_choice choices k_split=kPartitions input_nodes= mat mat layout=layout choice = choices assert isinstance mat layout FixedLayout Creating subgraph choice should have frozen layout We ensure padding so stride should differ assert mat layout stride = mat _stride example_stride layout_stride zip choice example_inputs stride mat layout stride Example inputs should have same stride current layout assert example_stride == layout_stride autotune_select_algorithm test_subgraph_choice choices b layout func mat mat torch ops mylib matmul_decompose_padding mat + mat mock patch torch _inductor ir V get_current_node get_node_mock node_mock = MagicMock node_mock meta = dislike_padding False get_node_mock return_value = node_mock compiled_func = torch compile func mode= max-autotune dynamic=False compiled_func a_in b_in __name__ == __main__ Set env make work CI HAS_GPU HAS_CPU run_tests