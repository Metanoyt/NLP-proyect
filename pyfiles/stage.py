mypy allow-untyped-defs Copyright c Meta Platforms Inc affiliates logging operator abc ABC abstractmethod collections abc Callable typing Any cast Optional Union torch torch distributed dist torch fx fx torch nn nn torch _subclasses fake_tensor FakeTensor torch distributed _composable replicate_with_fsdp replicate ReplicateModule torch distributed fsdp FSDPModule fully_shard torch fx node Argument map_aggregate torch nn parallel DistributedDataParallel torch utils _pytree tree_map_only _backward stage_backward stage_backward_input stage_backward_weight _debug map_debug_info _utils flatten_args PipeInfo validate_tensors_metadata __all__ = PipelineStage build_stage logger = logging getLogger __name__ _normalize_model_output_as_tuple output Any - tuple Any Note pipeline model output type The output model passed pipelining can any type controlled user However there API surfaces complicate outputs intermediate stages passed via Send Recv ops subsequent stages The implicit assumption each element outputs tensor Otherwise Send Recv would supported The exception last layer model which can output anything any which won t communicated via Send Recv outputs last layer model returned user passed loss function The loss function can written any way such its inputs match outputs model It would convenient we could strictly type output signature pipeline stage wrapping model we do want impose unnecessary constraint user provided models Currently we let user provided models either Tensor tuple Tensors each stage Due torch export tracing compiled models may also list instead Tuple which we will normalize back tuple consistency TODO should we stricter about asserting stage modules intermediate output all only Tensor values type output list HACK hacky workaround fact export creates output list format output = tuple output Unify output form tuple easy correspondence ` act_send_info ` output_tuple = output type output tuple output output_tuple _RootArgPlaceholder Placeholder model-level inputs __init__ tensor meta = tensor meta _RecvInfo Represents stage input __init__ input_name str source int buffer torch Tensor Name input input_name = input_name Stage index source input source = source Buffer receive input into buffer = buffer __repr__ f _RecvInfo input= input_name source= source shape= buffer size An input can either received activation model input InputInfo = Union _RecvInfo _RootArgPlaceholder _make_tensor_from_meta example Union torch Tensor FakeTensor device torch device - torch Tensor Create real tensor tensor torch empty example size dtype=example dtype layout=example layout device=device _PipelineStageBase ABC Base pipeline stages Defines implements common methods used ` _PipelineStage ` used tracing frontend ` PipelineStage ` used manual frontend __init__ submodule torch nn Module stage_index int num_stages int device torch device group Optional dist ProcessGroup = None dw_builder Optional Callable Callable None = None Args submodule torch nn Module The module executed stage stage_index int The index stage num_stages int The total number stages pipeline device torch device The device run stage group Optional dist ProcessGroup The process group use communication If ` None ` default process group will used Default ` None ` dw_builder Optional Callable Callable None If provided dw_builder builder function will build new dw_runner function will run parts module backward intentionally skipped during module s actual backward pass The builder must invoked stage after stage runs model backwards stage should save latest dw_runner run during weight pas W If provided dw_runner will generated automatically traversing autograd graph When used schedules only have F B steps fresh dw_runner function will called part I input backwards When used F I W schedules dw_runner function implements W super __init__ stage_index = num_stages raise ValueError f Stage index stage_index out range num_stages submod = submodule stage_index = stage_index num_stages = num_stages pyrefly ignore read-only device = device group = group dw_builder = dw_builder backward state backward_state dict int tuple Any = store dw_runner per microbatch_id dw_runner dict int Callable None = ` group_rank ` rank process group ` group ` group_rank = dist get_rank group group_size = dist get_world_size group group_size num_stages raise RuntimeError f Pipeline group size group_size cannot larger than number stages num_stages Run time states _outputs_meta Optional tuple torch Tensor = None map microbatch ID list forward tensor args fwd_cache dict int tuple Any list torch Tensor = map microbatch ID list backward grad tensor args bwd_cache dict int tuple Optional torch Tensor = Caching chunk outputs final output merge reduction output_chunks list Any = Initialize has_backward false will set true loss function passed pipeline schedule has_backward = False Log prefix log_prefix = f Stage stage_index Forward infra args_recv_info dict int tuple InputInfo = act_send_info dict int list = Backward infra will created lazily grad_recv_info dict = grad_send_info Optional list = None To populated later Schedule chunks Optional int = None stage_index_to_group_rank dict int int = i i group_size i range num_stages property has_backward - bool Returns true stage has backward pass _has_backward has_backward setter has_backward has_backward bool _has_backward = has_backward property is_first Returns true stage first stage pipeline stage_index == property is_last Returns true stage last stage pipeline stage_index == num_stages - _check_chunk_id chunk_id int chunks None raise RuntimeError Attempted access chunk_id before chunks have been configured chunk_id = chunks raise RuntimeError f Chunk id chunk_id out range chunks _configure_outputs_meta outputs_meta tuple torch Tensor Track output shapes dtype stage since they determine send operation s which must match recv operations next stage The next stage _will_ freezing its recv buffers based its initial configuration so s important also freeze validate output side avoid any send recv mismatches which could show up hangs silent corruption other errors assert _outputs_meta None Attempting reconfigure output_meta which supported _outputs_meta = tuple outputs_meta type ignore assignment get_outputs_meta - tuple torch Tensor Get output metadata meta tensors representing outputs stage assert _outputs_meta None Attempted get_outputs_meta without configuring output meta _outputs_meta _create_grad_send_info args_recv_info tuple - list Optional int Create list stage indices send gradients grad_send_info list Optional int = map_recv_to_send Note we send gradients back previous stage long forward received input regardless whether requires grad It up previous stage discard gradient isinstance _RecvInfo grad_send_info append source source grad_send_info append None None map_aggregate args_recv_info map_recv_to_send logger debug s Grad send info s log_prefix grad_send_info grad_send_info abstractmethod _prepare_forward_infra num_microbatches int args tuple Any kwargs Optional dict str Any = None - tuple Any raise NotImplementedError _prepare_backward_infra num_microbatches int TODO needed backward_maybe_with_nosync chunks = num_microbatches mb_index range num_microbatches ` grad_recv_info ` mirror ` act_send_info ` grad_recv_info mb_index = _create_grad_recv_info act_send_info abstractmethod _create_grad_recv_info act_send_info dict - tuple _RecvInfo raise NotImplementedError _get_recv_ops recv_infos tuple InputInfo - list dist P POp Helper function shared ` get_fwd_recv_ops ` ` get_bwd_recv_ops ` Returns list ops correspond recv infos ops list dist P POp = info recv_infos isinstance info _RecvInfo continue peer_rank = stage_index_to_group_rank info source peer_global_rank = peer_rank group None dist get_global_rank group peer_rank ops append dist P POp dist irecv info buffer peer_global_rank group ops Note V-schedule special case V-Schedules have special case where stages adjacent stage_id same rank ex ranks stages forms simple V rank stage stage rank stage stage stage communicate activations using send recv usual stage do need use communication ops Instead they should pass tensor data directly via function call set_local_fwd_input get_local_bwd_output + set_local_bwd_input facilitate optimization should called appropriate time during pipeline schedule after forward backward execution set_local_fwd_input prev_stage_outputs Any mb_index int - None Moves prev_stage_outputs another stage same rank into place inputs stage Avoids copying tensor data using send recv op Detaches original tensor sets requires_grad so tensor can serve leaf autograd gradients can collected during backward recv_infos tuple InputInfo = args_recv_info mb_index See Note pipeline model output type prev_stage_outputs = _normalize_model_output_as_tuple prev_stage_outputs info tensor zip recv_infos prev_stage_outputs assert isinstance tensor torch Tensor f expected tensor values outputs prev stage got type tensor assert isinstance info _RecvInfo set_local_Fwd_input should only called non-first stage which should always have RecvInfo We don t need do data copy here since we can directly pass activation tensor reference one stage next However we do need mark activation leaf tensor since will serve input tensor fresh autograd graph part previous stage s autograd graph TODO confirm do we use activation root backward call previous stage does detach have any affect info buffer = tensor detach requires_grad_ True get_local_bwd_output mb_index Returns input grad tensors stage which correspond stage inputs during forward assert has_backward can t steal_bwd_input stage doesn t have backward assert is_first can t get bwd output stage first _check_chunk_id mb_index bwd_cache pop mb_index set_local_bwd_input next_stage_bwd_outputs tuple Optional torch Tensor mb_index int - None Moves grad input tensors next stage grad_output stage avoiding copy send recv Does detach set _requires_grad assert isinstance next_stage_bwd_outputs tuple f Expected tuple got type next_stage_bwd_outputs assert has_backward can t set bwd input stage doesn t have backward assert is_last can t set bwd input stage last recv_infos = grad_recv_info mb_index info tensor zip recv_infos next_stage_bwd_outputs assert isinstance tensor torch Tensor f expected tensor values outputs prev stage got type tensor assert isinstance info _RecvInfo f Expected recv info got type info info buffer = tensor get_fwd_recv_ops fwd_chunk_id int - list dist P POp Returns list ops needed receive input arguments stage recv_infos tuple InputInfo = args_recv_info fwd_chunk_id _get_recv_ops recv_infos get_bwd_recv_ops bwd_chunk_id int - list dist P POp Returns list ops needed receive gradients stage has_backward is_last recv_infos = grad_recv_info bwd_chunk_id _get_recv_ops recv_infos get_fwd_send_ops fwd_chunk_id int - list dist P POp Get activation send ops current stage s forward output_tuple _ = fwd_cache fwd_chunk_id ops list dist P POp = idx out enumerate output_tuple dst_stages = act_send_info idx dst dst_stages dst None continue logger debug s Sending tensor Stage s s log_prefix dst out size peer_rank = stage_index_to_group_rank dst peer_global_rank = peer_rank group None dist get_global_rank group peer_rank ops append dist P POp dist isend out peer_global_rank group ops get_bwd_send_ops bwd_chunk_id int - list dist P POp Get gradient send ops current stage s backward has_backward is_first _check_chunk_id bwd_chunk_id Create bwd send infra lazily grad_send_info None Send info input grads during backward List destinations corresponding input grads Can None input has no grad ` grad_send_info ` mirror ` args_recv_info ` grad_send_info = _create_grad_send_info args_recv_info ops list dist P POp = grads_input = bwd_cache pop bwd_chunk_id grad grad_recv_stage zip grads_input grad_send_info isinstance grad torch Tensor grad_recv_stage None logger debug s Sending gradient Stage s s log_prefix grad_recv_stage grad size peer_rank = stage_index_to_group_rank grad_recv_stage peer_global_rank = peer_rank group None dist get_global_rank group peer_rank ops append dist P POp dist isend grad peer_global_rank group grad None grad_recv_stage None raise RuntimeError f stage_index chunk bwd_chunk_id has gradients grad f expecting send gradients stage grad_recv_stage ops clear_runtime_states - None Clear runtime states stage map microbatch ID list forward tensor args fwd_cache clear Caching chunk outputs final output merge reduction output_chunks clear Clear grad input buffers between schedule steps This because ` torch autograd backward ` will accumulate gradients into leaf tensors default For gradients pass back previous stages we don t want such accumulation recv_tuple args_recv_info values iterate over all chunks recv_tuple iterate over all input args isinstance _RecvInfo Set None newer recommended way clear grads compared ` zero_ ` See https github com pytorch pytorch pull buffer grad = None _map_tensor_from_recv_info recv_infos tuple InputInfo Map tensors recv infos list get_recv_tensor info isinstance info _RecvInfo info buffer raise AssertionError f Expected _RecvInfo got type info map_aggregate cast Argument recv_infos get_recv_tensor _retrieve_recv_activations fwd_chunk_id int Retrieve activations received current stage during forward recv_infos = args_recv_info fwd_chunk_id activations = _map_tensor_from_recv_info recv_infos activations _retrieve_recv_grads bwd_chunk_id int Retrieve gradients received current stage during backward recv_infos = grad_recv_info bwd_chunk_id grads = _map_tensor_from_recv_info recv_infos grads forward_maybe_with_nosync args kwargs If submod wrapped DDP we use ` no_sync ` context manager avoid gradient all-reduce per microbatch isinstance submod DistributedDataParallel submod no_sync type ignore operator out_val = submod args kwargs out_val = submod args kwargs out_val scale_grads grad_scale_factor int - None Scale gradients model gradients ` grad_scale_factor ` which should specified coordination loss function used pipelining For loss functions which perform mean loss reduction ` grad_scale_factor ` should set num_microbatches For loss functions use ` sum ` reduction ` grad_scale_factor ` should set Should only called once per pipeline schedule step after all backwards passes have completed PP scales only its own contribution microbatches relies DP scale further DP degree grad_scale_factor = p submod parameters p grad None p grad div_ grad_scale_factor backward_maybe_with_nosync backward_type bwd_kwargs dict last_backward bool = False - tuple tuple Optional torch Tensor Optional list dict str Any Whether using PP FSDP DDP replicate there some runtime differences between last backward step other steps Namely we need accumulate gradients previous steps reduce them last step there additional state-variables performance considerations depending data parallelism used This helper should adapt any pipeline parallel schedule work common supported data parallel libraries perform_backward backward_type - Callable tuple tuple Optional torch Tensor Optional list dict str Any backward_type == full lambda stage_backward bwd_kwargs stage_output bwd_kwargs output_grads bwd_kwargs input_values None backward_type == input lambda stage_backward_input bwd_kwargs stage_output bwd_kwargs output_grads bwd_kwargs input_values submod parameters backward_type == weight lambda stage_backward_weight submod parameters bwd_kwargs param_groups None raise RuntimeError f Unknown backward type backward_type If submod wrapped DDP isinstance submod DistributedDataParallel last_backward Last chunk prepare gradient reduction HACK reaching into DDP implementation details here Is there better way submod reducer prepare_for_backward type ignore union-attr operator list torch nn parallel distributed _find_tensors type ignore attr-defined bwd_kwargs stage_output result = perform_backward backward_type submod no_sync type ignore operator result = perform_backward backward_type If submod FSDP replicate module isinstance submod FSDPModule submod set_is_last_backward False submod set_reshard_after_backward False submod set_requires_gradient_sync False result = perform_backward backward_type Non-DP submodule regular backward result = perform_backward backward_type grads param_groups = result grads param_groups forward_one_chunk fwd_chunk_id int args tuple Any kwargs Optional dict str Any = None save_forward_output bool = True Perform forward pass stage one microbatch ` args ` ` kwargs ` inputs external stage As Sept - ` args ` applies first stage only other stages receives args through activation transmission - ` kwargs ` can passed all stages via respective ` step ` calls is_first First stage doesn t need receive anything composite_args = args Receive activations chunk Activations only come args form composite_args = _retrieve_recv_activations fwd_chunk_id composite_kwargs = kwargs _validate_fwd_input args kwargs Compute forward try output = forward_maybe_with_nosync composite_args composite_kwargs except Exception e exc_msg = f log_prefix failed run forward args map_debug_info composite_args kwargs map_debug_info composite_kwargs raise RuntimeError exc_msg e See Note pipeline model output type output_tuple = _normalize_model_output_as_tuple output Prepare final output merge reduction Output chunks only used last stage since we only merge output last stage is_last save_forward_output output_chunks append output Save activations inputs backward flat_args = flatten_args composite_args flat_kwargs = flatten_args composite_kwargs flatten_input_tensors = flat_args + flat_kwargs fwd_cache fwd_chunk_id = output_tuple stage_output flatten_input_tensors input_values logger debug s Forwarded chunk s outputs s log_prefix fwd_chunk_id map_debug_info output _validate_fwd_outputs output_tuple We original user-provided output normalized tuple See Note pipeline model output type output backward_one_chunk bwd_chunk_id int loss=None full_backward bool = True last_backward=False Perform backward pass module This should only called once per microbatch If full_backward True default full backward pass including weight input gradients will run error call ` backward_weight_one_chunk ` bwd_chunk_id If full_backward False optional ` dw_runner ` provided PipelineStage __init__ time subsequent call ` backward_weight_one_chunk ` required invoke dw_runner complete backward last_backward controlled schedule signals synchronization gradients across DP groups after last backward skip backward computation backward enabled has_backward _check_chunk_id bwd_chunk_id stage_output input_values = fwd_cache pop bwd_chunk_id Compute backward is_last Last stage computes gradients loss has no gradients next stage bwd_kwargs = stage_output loss output_grads None input_values input_values Otherwise receive gradients next stage grads_output = _retrieve_recv_grads bwd_chunk_id If input pipeline requires gradient ` torch autograd backward ` will accumulate gradient into ` grad ` field such input bwd_kwargs = stage_output stage_output output_grads grads_output input_values input_values grads_input tuple Optional torch Tensor = Custom backward function dw_builder TODO We may want change our semantics so we allowed ignore dw_builder call full_backward directly when full_backward op grads_input _ = backward_maybe_with_nosync full bwd_kwargs last_backward=last_backward full_backward dw_builder dw_runner bwd_chunk_id = dw_builder full_backward grads_input _ = backward_maybe_with_nosync full bwd_kwargs last_backward=last_backward param_groups list dict str Any &#124; None = None Skip backward first stage since we will perform weight update autograd backward backward_weight_one_chunk is_first isinstance bwd_kwargs stage_output torch Tensor bwd_kwargs stage_output = bwd_kwargs stage_output perform partial backwards inputs custom backward function when stage_ouput loss then tensor otherwise tuple tensors grads_input param_groups = backward_maybe_with_nosync input bwd_kwargs last_backward=last_backward TODO we dont need save add dw_runner backward_state bwd_chunk_id = bwd_kwargs input_values param_groups bwd_kwargs stage_output bwd_kwargs output_grads Save placeholder dw_runner dw_runner bwd_chunk_id = lambda None bwd_cache bwd_chunk_id = grads_input is_last is_first Autograd dependencies rest_of_autograd_graph - stage_output - loss stage_output no longer used last stage backward only needed user merge_output_chunks therefore should detached release autograd graph context free memory earlier t stage_output t _is_view views detachable in-place t detach_ logger debug s Backwarded chunk s log_prefix bwd_chunk_id backward_weight_one_chunk bwd_chunk_id int last_backward=False skip backward computation backward enabled has_backward assert bwd_chunk_id dw_runner f log_prefix Attempted run backward_weight_one_chunk chunk bwd_chunk_id without first calling ` backward_one_chunk full_backward=False ` dw_builder None dw_runner pop bwd_chunk_id input_values param_groups stage_output output_grads = backward_state pop bwd_chunk_id stage_index = bwd_kwargs = stage_output stage_output param_groups param_groups backward_maybe_with_nosync weight bwd_kwargs last_backward=last_backward TODO figure out better way do inputs does require gradient then parameter group will fully captured during stage_backward_input case we need call grad directly parameters To solve make input fn do intersect compute then finish off during W bwd_kwargs = stage_output stage_output output_grads output_grads input_values input_values backward_maybe_with_nosync full bwd_kwargs last_backward=last_backward _validate_fwd_input args kwargs Raises RuntimeError shapes input args kwargs do match shapes configured stage is_first TODO why there separate recv_info each pipeline chunk kwen avoid passing ` fwd_chunk_id ` function we check all chunks against args_recv_info expected_args = args_recv_info We don t check inputs non- stages assuming they don t accept user inputs canonical pipeline scenarios len kwargs TODO- need mapping kwarg position args_recv_info Without we sure how match args expected_args TODO- need mapping kwarg position args_recv_info maybe s impossible tell whether len mismatches because user passed extra arg missed arg b user did pass kwarg which has default value baked into expected_args expected_tensors_meta = e meta isinstance e _RootArgPlaceholder e buffer e expected_args validate_tensors_metadata f Stage stage_index forward inputs expected_tensors_meta args _validate_fwd_outputs outputs tuple torch Tensor Raises RuntimeError stage produces output unexpected shape dtype Most likely could cause either incorrect user specification output shapes because shape inference done original model then runtime model wrapped something like mixed precision which changes output dtype expected_tensors_meta = get_outputs_meta validate_tensors_metadata f Stage stage_index forward outputs expected_tensors_meta outputs _get_init_p p_neighbors_ops - list dist P POp Get operations initialize p p communicators between previous next stages This done so creating dummy tensor sending next stage receiving previous stage ops list dist P POp = next_stage_peer_rank = stage_index_to_group_rank get stage_index + prev_stage_peer_rank = stage_index_to_group_rank get stage_index - recv_tensor = torch zeros device=self device dtype=torch float send_tensor = torch tensor stage_index device=self device dtype=torch float forward is_first ops append dist P POp dist irecv recv_tensor group_peer=prev_stage_peer_rank group=self group is_last ops append dist P POp dist isend send_tensor group_peer=next_stage_peer_rank group=self group backward is_first ops append dist P POp dist isend send_tensor group_peer=prev_stage_peer_rank group=self group is_last ops append dist P POp dist irecv recv_tensor group_peer=next_stage_peer_rank group=self group ops _post_backward grad_scale_factor int Manually call post backward FSDP isinstance submod FSDPModule fsdp_module = submod fsdp_module set_is_last_backward True fsdp_module set_reshard_after_backward True fsdp_module set_requires_gradient_sync True isinstance fsdp_module ReplicateModule distributed_state = replicate state fsdp_module type ignore arg-type distributed_state = fully_shard state fsdp_module type ignore attr-defined state distributed_state _state_ctx all_states state _fsdp_param_group state _fsdp_param_group post_backward would much better pipelining backward invoked backward so autograd hooks worked modules like DDP FSDP behaved expected Working around time being we need call too ensure FSDP syncs its grad reduction ops back default stream distributed_state _root_post_backward_final_callback Call gradient scaling end backward pass NOTE must happen after FSDP post_backward FSDP enabled scale_grads grad_scale_factor _PipelineStage _PipelineStageBase __init__ stage_module torch nn Module stage_index int pipe_info PipeInfo device torch device group Optional dist ProcessGroup = None Create pipeline stage given stage_module wrapped stage ` pipe_info ` describing stage relationship pipeline Args stage_module torch nn Module module wrapped stage stage_index int index stage pipeline pipe_info PipeInfo information about pipeline can retrieved ` pipe info ` device torch device device used stage group Optional dist ProcessGroup process group used stage _PipelineStageBase __init__ stage_module stage_index pipe_info num_stages device group pipe_info = pipe_info Find stage nodes graph submod_nodes = node node pipe_info graph nodes node op == call_module len submod_nodes = num_stages raise AssertionError f Number submodules pipe graph len submod_nodes does match number stages num_stages Find my stage node graph node = submod_nodes stage_index name = node name logger info s Creating PipelineStage s s group_rank stage_index name Create mapping stage name stage index submod_to_stage_index dict str int = i node enumerate submod_nodes submod_to_stage_index setdefault node name i Cast submodule device _move_submod_to_device _move_submod_to_device Move submodule indicated device possible Note we cannot move meta module real devices because meta tensors do support method One needs do in-place tensor swap case has_meta_param = any isinstance p FakeTensor p is_meta p submod parameters has_meta_param logger debug s Found meta parameters log_prefix submod device _prepare_forward_infra num_microbatches int args tuple Any kwargs Optional dict str Any = None - tuple Any Create send recv infrastructures activations during forward TODO whc method should deleted once lazy buffer allocation implemented now ignores args kwargs because should need do shape inference chunk range num_microbatches args_recv_info chunk = _create_act_recv_info Send info during forward each activation act_send_info = _create_act_send_info tuple get_stage_index_of_submod submod_name str Given submodule name stage index submodule submod_name submod_to_stage_index raise AssertionError f Stage id submod_name found submod_to_stage_index submod_name _create_act_recv_info Create tuple ` _RecvInfo ` inputs stage create_recv_tensor placeholder arg_node Create receive buffer placeholder example_value = placeholder meta val arg_node op == placeholder This root level placeholder thus input argument entire model We likely stage hence no need create receive buffer _RootArgPlaceholder example_value Figure out source stage input while arg_node target operator getitem If input getitem we need go deeper arg_node = arg_node args assert arg_node op == call_module f Expecting call_module got arg_node op src_stage = get_stage_index_of_submod arg_node name Create receive buffer placeholder logger debug s Creating recv buffer input s s s log_prefix placeholder name example_value shape example_value dtype buffer = _make_tensor_from_meta example_value device In case there backward pass set requires_grad receive buffers before first forward has_backward buffer requires_grad_ True _RecvInfo arg_node name src_stage buffer args_recv_info list InputInfo = Filter out placeholder nodes ` submod ` GraphModule placeholders = filter type ignore var-annotated lambda node node op == placeholder type ignore arg-type submod graph nodes type ignore arg-type union-attr ` placeholders ` nodes internal submod ` node args ` dependency nodes outer graph The two placeholder arg_node zip placeholders node args Create receive buffer placeholder recv_info = create_recv_tensor placeholder arg_node args_recv_info append recv_info logger debug s Activation recv args info s log_prefix args_recv_info ` args ` Tuple hence we will Tuple InputInfo tuple args_recv_info find_dst_rank user fx Node - Optional int Find destination rank ` user ` node If ` user ` submod ` None ` may returned user op == call_module User stage ` call_module ` get_stage_index_of_submod user name - If user op == output No need send back rank - If user target stage_backward No need send assuming submod output stored locally should re-calculated case activation checkpointing None _create_act_send_info Create dict send info activations The dict form output_index dst_rank_ dst_rank_ where list ` dst_rank ` s covers case where output value may consumed multiple stages Output index List receiver ranks act_send_info dict int list = out_idx = user node users user target operator getitem Recursively find real destination gi_dsts = act_send_info setdefault out_idx gi_user user users dst_rank = find_dst_rank gi_user dst_rank None gi_dsts append dst_rank Next ` getitem ` will point next output index out_idx += In case single output value ` out_idx ` will increase dsts = act_send_info setdefault out_idx dst_rank = find_dst_rank user dst_rank None dsts append dst_rank output_node = _get_output_node output_vals tuple torch Tensor = tuple v meta val v flatten_args output_node args _configure_outputs_meta output_vals logger debug s Send info s log_prefix act_send_info act_send_info _get_output_node output_nodes = node node submod graph nodes node op == output type ignore union-attr assert len output_nodes == output_node = output_nodes output_node _create_grad_recv_info act_send_info dict - tuple _RecvInfo Create tuple ` _RecvInfo ` gradients Dict output_index _RecvInfo grad_recv_info dict int _RecvInfo = output_node = _get_output_node The output node may take multiple args meaning submod having multiple output values output_vals = flatten_args output_node args out_idx dst_list act_send_info items dst_list No actual receiver activation so no grad coming back continue output = output_vals out_idx example_value = output meta val logger debug f log_prefix Creating grad recv buffer output output name noqa G f example_value shape example_value dtype TODO otherwise needs grad accumulation assert len dst_list == Backward skip connections supported yet grad_src = dst_list grad_recv_info out_idx = _RecvInfo f grad_src noqa G grad_src _make_tensor_from_meta example_value device Convert tuple convenience get_ops retrieve tensor grad_recv_info_tuple = tuple grad_recv_info values logger debug s Grad recv info s log_prefix grad_recv_info_tuple grad_recv_info_tuple A helper function create pipeline stage based traced pipeline information build_stage stage_module torch nn Module stage_index int pipe_info PipeInfo device torch device group Optional dist ProcessGroup = None - _PipelineStage Create pipeline stage given stage_module wrapped stage pipeline information Args stage_module torch nn Module module wrapped stage stage_index int index stage pipeline pipe_info PipeInfo information about pipeline can retrieved ` pipe info ` device torch device device used stage group Optional dist ProcessGroup process group used stage Returns _PipelineStage pipeline stage can run ` PipelineSchedules ` _PipelineStage stage_module stage_index pipe_info device group PipelineStage _PipelineStageBase A representing pipeline stage pipeline parallelism setup PipelineStage assumes sequential partitioning model i e model split into chunks where outputs one chunk feed into inputs next chunk no skip connections PipelineStage performs runtime shape dtype inference automatically propagating outputs stage stage so forth linear order To bypass shape inference pass ` input_args ` ` output_args ` each PipelineStage instance Args submodule nn Module The PyTorch module wrapped stage stage_index int The ID stage num_stages int The total number stages device torch device The device where stage located input_args Union torch Tensor Tuple torch tensor optional The input arguments submodule output_args Union torch Tensor Tuple torch tensor optional The output arguments submodule group dist ProcessGroup optional The process group distributed training If None default group dw_builder Optional Callable Callable None If provided dw_builder will build new dw_runner function will W action input weights F I W Fwd Input Weight zero bubble schedules __init__ submodule nn Module stage_index int num_stages int device torch device input_args Optional Union torch Tensor tuple torch Tensor = None output_args Optional Union torch Tensor tuple torch Tensor = None group Optional dist ProcessGroup = None dw_builder Optional Callable Callable None = None super __init__ submodule stage_index num_stages device group dw_builder inputs Optional list torch Tensor = None inputs_meta Optional tuple torch Tensor = None Note inputs submod should ideally meta device We decided assert yet because might breaking existing users input_args None assert output_args None If specifying output_args input_args must also specified Otherwise shape inference will performed runtime inputs_meta = input_args isinstance input_args torch Tensor input_args output_args None logger warning Deprecation warning passing input_args performing init-time shape inference deprecated PipelineStage now supports runtime shape inference using real inputs provided schedule step Either delete ` input_args ` arg ` PipelineStage ` opt-into runtime shape inference additionally pass ` output_args ` ` PipelineStage ` fully override shape inference try torch no_grad output_args = submodule inputs_meta output_args = tree_map_only torch Tensor lambda x x meta output_args except Exception e raise RuntimeError Failed perform pipeline shape inference- your inputs same device your module e assert output_args None If passing input_args also pass output_args override shape inference _configure_outputs_meta output_args isinstance output_args torch Tensor output_args these buffers used backwards send recv they allocated later outputs_grad list torch Tensor = dbg_str = f Finished pipeline stage init stage_index= is_first= noqa G f is_last= num_stages= inputs_meta None dbg_str += f inputs inp shape inp inputs_meta f output output shape output get_outputs_meta dbg_str += running shape-inference runtime logger debug dbg_str _shape_inference args tuple Any kwargs Optional dict str Any = None kwargs None kwargs = assert args None Args may empty tuple None We skip recv communication we re first stage also previous stage same rank can pass its output shapes args instead using send recv is_first first stage then check prev stage same rank stage_index_to_group_rank stage_index - == group_rank logger debug Shape inference stage s skipping recv because shape info passed via ` args ` stage_index args = tree_map_only torch Tensor lambda x x meta args assert len args == Can t supply input args shape inference non-first stage objects = None logger debug Shape inference stage s receiving stage s stage_index stage_index - dist recv_object_list objects src=dist get_global_rank group dist distributed_c d _get_default_group stage_index_to_group_rank stage_index - group=self group device=self device use_batch=True recv_args = objects assert isinstance recv_args tuple type recv_args args = recv_args cache input shapes use during recv buffer allocation inputs_meta = args args = tree_map_only torch Tensor lambda x torch zeros_like x device=self device args set attributes needed forward torch no_grad outputs = submod args kwargs single tensor convert so always list isinstance outputs torch Tensor outputs = outputs communicate meta outputs real outputs two reasons - its faster esp since obj coll pickles tensor data - avoid activating cuda context src rank when unpickling recv end outputs_meta = tuple tree_map_only torch Tensor lambda x x meta outputs logger debug Shape inference stage s inputs s outputs s stage_index inputs_meta outputs_meta _configure_outputs_meta outputs_meta Passing outputs next stage two cases- Usually use send recv communication pass output Special case V-schedules adjacent stages e g stage -stage -rank V pass their shape info via value function args rather than send recv is_last last stage then check next stage same rank stage_index_to_group_rank stage_index + == group_rank Case above pass shape info via value caller passes args next stage s _shape_inference call logger debug Shape inference stage s skipping send next stage stage_index Case send shapes via send operation ensure caller logger debug Shape inference stage s sending stage s stage_index stage_index + dist send_object_list outputs_meta dst=dist get_global_rank group dist distributed_c d _get_default_group stage_index_to_group_rank stage_index + group=self group device=self device use_batch=True outputs_meta = tuple outputs_meta _prepare_forward_infra num_microbatches int args tuple Any kwargs Optional dict str Any = None - tuple Any TODO move device argument step API its input tensors assert num_microbatches None TODO fix num_microbatches outputs tuple Any = tuple inputs_meta None outputs = _shape_inference args kwargs assert inputs_meta None Receive info during forward TODO create args_recv_info lazily same needed PipelineStage chunk_id range num_microbatches is_first We assume we always receive stage - recv_infos = tuple _RecvInfo f recv_for_ stage_index _from_ stage_index - stage_index - _make_tensor_from_meta inp device inp inputs_meta In case there backward pass set requires_grad receive buffers has_backward r recv_infos r buffer requires_grad_ True args_recv_info chunk_id = recv_infos args_recv_info chunk_id = tuple _RootArgPlaceholder i i inputs_meta Send info during forward each activation only need rank being sent act_send_info dict int list = idx range len get_outputs_meta We assume we always send stage + is_last act_send_info idx = stage_index + act_send_info idx = outputs _create_grad_recv_info act_send_info dict - tuple _RecvInfo grad_recv_info tuple _RecvInfo = is_last Receiving gradients multiple sources supported hence we only take first destination grad_recv_info = tuple _RecvInfo f recv_grad_for_ stage_index _from_ dst_list dst_list _make_tensor_from_meta get_outputs_meta idx device idx dst_list act_send_info items grad_recv_info