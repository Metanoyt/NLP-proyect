Be extra careful when you edit file because affects AOTInductor ABI compatibility See https github com pytorch pytorch blob e c e cf e torchgen gen py#L -L details The inductor_fallback_ops list based fallback ops torch _inductor lowering py Generally speaking ok add new op list you need run ` python torchgen gen py -- update-aoti-c-shim ` order regenerate C shim header files But NOT ok remove existing fallback op list since will break some existing AOTInductor-compiled models A fallback op version defaults If you want extend existing fallback op adding new argument default value while fine Python world will BC-breaking when generating C shim Thus you need bump up version number fallback op updating entry inductor_fallback_ops list adding new version number list new arguments then run ` python torchgen gen py -- update-aoti-c-shim ` regenerate inductor_fallback_ops dict str dict str list str = aten _adaptive_avg_pool d_backward default aten _adaptive_avg_pool d default aten _adaptive_avg_pool d_backward default aten _adaptive_avg_pool d default aten _addmm_activation default aten _cdist_backward default aten _cdist_forward default aten _cudnn_rnn default aten _dyn_quant_matmul_ bit default aten _dyn_quant_pack_ bit_weight default aten _efficient_attention_backward default aten _efficient_attention_forward default aten _efficientzerotensor default aten _embedding_bag_dense_backward default aten _embedding_bag_forward_only default aten _embedding_bag_per_sample_weights_backward default aten _embedding_bag default aten _fft_c c default aten _fft_r c default aten _flash_attention_backward default aten _flash_attention_forward default aten _fused_moving_avg_obs_fq_helper_functional default aten _fused_moving_avg_obs_fq_helper default aten _fused_rms_norm default aten _histogramdd_from_bin_cts default aten _int_mm out aten _pdist_backward default aten _pdist_forward default aten _scaled_dot_product_attention_math_for_mps default aten _scaled_dot_product_cudnn_attention_backward default aten _scaled_dot_product_cudnn_attention default aten _scaled_dot_product_efficient_attention_backward default aten _scaled_dot_product_efficient_attention default aten _scaled_dot_product_flash_attention_backward default aten _scaled_dot_product_flash_attention_for_cpu_backward default aten _scaled_dot_product_flash_attention_for_cpu default aten _scaled_dot_product_flash_attention default aten _scaled_dot_product_fused_attention_overrideable_backward default aten _scaled_dot_product_fused_attention_overrideable default aten _scaled_mm default aten _scaled_grouped_mm default aten _scaled_mm out aten _segment_reduce_backward default aten _thnn_fused_lstm_cell default aten _to_sparse default aten _trilinear default aten _weight_int pack_mm default aten _weight_int pack_mm default aten abs default aten adaptive_max_pool d_backward default aten adaptive_max_pool d default aten adaptive_max_pool d_backward default aten adaptive_max_pool d default aten add Scalar aten add Tensor aten addbmm default aten addmm out aten addmv default aten angle default aten avg_pool d_backward default aten avg_pool d default aten avg_pool d_backward default aten avg_pool d default aten baddbmm out aten bernoulli_ float aten bernoulli_ Tensor aten bmm out aten bucketize Tensor aten cat default aten cholesky_inverse default aten cholesky_solve default aten convolution_backward default aten convolution default aten cummax default aten cummin default aten cumprod default aten cumsum default aten exponential default aten fill_ Scalar aten fractional_max_pool d_backward default aten fractional_max_pool d default aten fractional_max_pool d_backward default aten fractional_max_pool d default aten gcd default aten geqrf default aten grid_sampler_ d_backward default aten hann_window default aten histc default aten histogram bin_ct aten index_put default aten index_reduce default aten index Tensor aten kthvalue default aten logcumsumexp default aten lu_unpack default aten masked_scatter_backward default aten masked_scatter default aten masked_select default aten max_pool d_with_indices_backward default aten max_pool d_with_indices default aten max_pool d_with_indices_backward default aten max_pool d_with_indices default aten max_unpool d default aten max_unpool d default aten median default aten mm out aten mode default aten mul Scalar aten mul Tensor aten nanmedian default aten narrow default aten native_dropout default aten nonzero default aten normal_functional default aten ormqr default aten pad default aten permute default aten polar default aten pow Scalar aten pow Tensor_Scalar aten pow Tensor_Tensor aten rand default aten rand generator aten randint default aten randint generator aten randint low_out aten randint low aten randn default aten randn generator aten randperm default aten repeat_interleave Tensor aten replication_pad d_backward default aten replication_pad d_backward default aten reshape default aten resize_ default aten resize_as_ default aten scatter_reduce two_out aten scatter src_out aten scatter value_out aten searchsorted Scalar aten searchsorted Tensor aten segment_reduce default aten set_ source_Tensor aten slice Tensor aten soft_margin_loss_backward default aten sort default aten sort stable aten squeeze dim aten to_sparse default aten topk default aten triangular_solve default aten uniform default aten upsample_bicubic d_backward default aten upsample_linear d_backward default aten upsample_trilinear d_backward default aten view_as_complex default aten view_as_real default aten view dtype aten _weight_int pack_mm_with_scales_and_zeros default ` python torchgen gen py -- update-aoti-c-shim ` will automatically generate c_shim_aten h cpp based list below Operators list intended used torch csrc stable ops h Unlike other c_shims operators file do bypass dispatcher The same BC rules apply inductor_fallback_ops aten_shimified_ops dict str dict str list str = aten fill_ Scalar aten pad default aten narrow default aten amax default aten new_empty default aten new_zeros default