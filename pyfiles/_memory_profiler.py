mypy allow-untyped-defs collections dataclasses enum itertools logging collections abc Iterator typing Any cast Literal Optional Union torch torch _C FunctionSchema torch _C _autograd _ProfilerResult torch _C _profiler _EventType _ExtraFields_Allocation _ExtraFields_TorchOp _ProfilerEvent _TensorMetadata RecordScope torch _utils _element_size torch profiler _utils KeyAndID = tuple Key int TensorAndID = tuple TensorKey int log = logging getLogger __name__ Category enum Enum INPUT = enum auto TEMPORARY = enum auto ACTIVATION = enum auto GRADIENT = enum auto AUTOGRAD_DETAIL = enum auto PARAMETER = enum auto OPTIMIZER_STATE = enum auto _CATEGORY_TO_COLORS = Category PARAMETER darkgreen Category OPTIMIZER_STATE goldenrod Category INPUT black Category TEMPORARY mediumpurple Category ACTIVATION red Category GRADIENT mediumblue Category AUTOGRAD_DETAIL royalblue None grey _CATEGORY_TO_INDEX = c i i c enumerate _CATEGORY_TO_COLORS Action enum Enum PREEXISTING = enum auto CREATE = enum auto INCREMENT_VERSION = enum auto DESTROY = enum auto _ACTION_TO_INDEX = i i value i Action dataclasses dataclass eq=True unsafe_hash=False frozen=True Key device torch device dataclasses dataclass _Storage Bundle storage pointer id All profiling logic should use ` allocation_id ` however useful print storage pointers debugging unit tests sometimes look up values using storage data pointer live Tensor ptr int allocation_id int __repr__ - str f hex ptr allocation_id __eq__ other object - bool isinstance other _Storage allocation_id == other allocation_id __hash__ - int hash allocation_id dataclasses dataclass eq=True unsafe_hash=True frozen=True TensorKey Key Hashable identifier storage which has been assigned ID A detailed description Tensor IDs why they needed given ` torch csrc profiler collection h ` when ` TensorID ` declared To summarize multiple Storage buffers can map same logical Tensor This dataclass used refer concrete in-memory StorageImpl Tensor id int storage _Storage __repr__ - str f id= id repr storage device __lt__ other TensorKey - bool _as_sortable other _as_sortable staticmethod _make tensor_id Optional int storage_ptr Optional int allocation_id Optional int device torch device - Optional TensorKey tensor_id None storage_ptr None allocation_id None TensorKey device tensor_id _Storage storage_ptr allocation_id None classmethod from_allocation cls alloc _ExtraFields_Allocation - Optional TensorKey cls _make alloc id alloc ptr alloc allocation_id alloc device classmethod from_tensor cls t Optional _TensorMetadata - Optional TensorKey t None cls _make t id t storage_data_ptr t allocation_id t device None property _as_sortable - tuple int int str int id storage allocation_id device type device index _extract_parameters_and_gradients node _ProfilerEvent - Iterator tuple Optional TensorKey Optional TensorKey children = node children AccumulateGrad used Autograd engine handle gradient updates There two possible cases This newly created gradient Tensor In case there nothing accumulate so autograd simply detaches Tensor There preexisting gradient Tensor we need add newly computed update This done in-place add aten add_ op The underscore suffix denotes in-place node typed == _EventType TorchOp node typed scope == RecordScope BACKWARD_FUNCTION TODO robieta Move away load bearing names node name == torch autograd AccumulateGrad children children typed == _EventType TorchOp children name aten detach aten add_ children typed inputs isinstance children typed inputs _TensorMetadata yield None TensorKey from_tensor children typed inputs We directly instrument ` torch nn Module ` ` torch optim Optimizer ` NOTE The values captured python tracer cached they can used build up labels do imply Tensor live particular time node typed == _EventType PyCall typed_fields = node typed assert typed_fields module None typed_fields optimizer None typed_fields module None _ p p_grad typed_fields module parameters yield TensorKey from_tensor p TensorKey from_tensor p_grad typed_fields optimizer None p p_grad _ typed_fields optimizer parameters yield TensorKey from_tensor p TensorKey from_tensor p_grad extract_parameters node _ProfilerEvent - Iterator TensorKey p _p_grad _extract_parameters_and_gradients node p None yield p extract_gradients node _ProfilerEvent - Iterator tuple Optional TensorKey TensorKey p p_grad _extract_parameters_and_gradients node p_grad None yield p p_grad get_scopes event Optional _ProfilerEvent - tuple RecordScope scopes = while event event typed == _EventType TorchOp scopes append event typed scope event = event parent tuple scopes SchemaMatcher Lookup operator schema based profiled name When profiling we record operator s name schema However some analysis requires information Fortunately we can look up registered schema recorded name We do however record overload so we must compare profiled arguments all overloads determine viable matches Note Once https github com pytorch pytorch issues completed code will obsolete classmethod inputs_are_mutable cls t _ExtraFields_TorchOp - tuple Optional bool Determine which inputs may have mutated based function schema Note we don t need resolve down single schema perform analysis An input mutable mutable any overload In practice however overwhelmingly common match single overload If we cannot find any valid schema then we must conservative assume all inputs mutable mutable Optional list bool = None schema cls match_schemas t mutable = mutable False _ schema arguments i arg enumerate schema arguments pyrefly ignore unsupported-operation mutable i &#124; = getattr arg alias_info is_write False tuple mutable None _ t inputs classmethod match_schemas cls t _ExtraFields_TorchOp - tuple FunctionSchema signature = tuple Tensor TensorKey from_tensor i isinstance i _TensorMetadata TensorList TensorKey from_tensor j j i isinstance i list Scalar uncaptured inputs i i t inputs matches schema - bool len schema arguments == len signature all cls _types_match observed schema_arg type observed schema_arg zip signature schema arguments strict=True tuple s s cls lookup_schemas t name matches s classmethod _types_match cls observed schema_type - bool isinstance schema_type torch _C OptionalType schema_type = schema_type getElementType observed None cls _types_match observed schema_type isinstance schema_type torch _C AnyType True schema_type isSubtypeOf torch _C ListType ofTensors isinstance observed list all isinstance i TensorKey i observed type_map tuple tuple Any Union type tuple type = torch _C TensorType TensorKey torch _C NoneType type None torch _C BoolType bool torch _C IntType int torch _C FloatType float torch _C ComplexType complex torch _C NumberType bool int float complex jit_type py_types type_map isinstance schema_type jit_type isinstance observed py_types Profiler only records subset possible argument types If we reach point then schema must call type profiler does record Thus schema can only match ` observed ` also None observed None staticmethod lookup_schemas name str - Optional tuple FunctionSchema TODO robieta _jit_get_schemas_for_operator quite expensive ~ us call Consider adding ` functools lru_cache ` becomes issue try Schema lookup will throw ` name ` malformed For example schemas must namespaced schema lookup will fail name does include We simply catch exception ` None ` denote ` name ` cannot operator name Note record_function annotations also go through path so expected some names will correspond PyTorch operators name None tuple torch _C _jit_get_schemas_for_operator name except RuntimeError None OpTree __init__ result _ProfilerResult - None _root_nodes = result experimental_event_tree _sorted_nodes = tuple sorted dfs key=lambda x x start_time_ns dfs args kwargs - Iterator _ProfilerEvent yield _utils traverse_dfs _root_nodes args kwargs property sorted_nodes - tuple _ProfilerEvent _sorted_nodes SizeMap __init__ op_tree OpTree - None _values dict TensorKey int = node op_tree sorted_nodes node typed == _EventType TorchOp t _flat_tensor_inputs node typed _update_values t node typed == _EventType PyCall typed_fields = node typed assert typed_fields module None typed_fields optimizer None typed_fields module None _ p p_grad typed_fields module parameters _update_values p _update_values p_grad typed_fields optimizer None p p_grad state typed_fields optimizer parameters _update_values p _update_values p_grad _ t state _update_values t allocations dict TensorKey int = node op_tree sorted_nodes node typed == _EventType Allocation alloc_fields = node typed key = TensorKey from_allocation alloc_fields key new_size = abs alloc_fields alloc_size prior_size = allocations setdefault key new_size It possible resize Storage PyTorch however we key data pointer so most resizes will treated change storage The one corner case cannot handled ` realloc ` which successfully resizes storage At time writing done anywhere core PyTorch codebase prior_size = new_size delta = f prior_size vs new_size log warning Mismatch between allocation free s delta _values update allocations _update_values t Optional _TensorMetadata - None key = TensorKey from_tensor t key None t None t layout == torch strided Scalars represented zero dim Tensors n = max i i i zip t sizes t strides strict=True num_bytes = n _element_size t dtype assert num_bytes = f num_bytes _values key = max _values get key num_bytes staticmethod _flat_tensor_inputs op _ExtraFields_TorchOp - Iterator _TensorMetadata i op inputs isinstance i _TensorMetadata yield i isinstance i list yield i __getitem__ key TensorKey _values key dataclasses dataclass DataFlowEdge input_version Optional int = None mutated Optional bool = False property is_allocation - bool input_version None property is_deletion - bool mutated None DataFlowNode __init__ event _ProfilerEvent graph DataFlowGraph - None _event = event _graph = graph _edges dict TensorKey DataFlowEdge = _determine_edges key edge _edges items edge mutated edge is_allocation _graph bump key Make sure version bumping behavior matches what we expect versions = k v _graph lookup k k v outputs items assert all i == j i j versions values f versions _edges _determine_edges - dict TensorKey DataFlowEdge subtree = tuple _utils traverse_dfs _event Start populating edges op inputs outputs mutable_by_key dict Optional TensorKey set Optional bool = op i typed i subtree i typed == _EventType TorchOp op_input mutable zip op inputs SchemaMatcher inputs_are_mutable op strict=True Tensor isinstance op_input _TensorMetadata key = TensorKey from_tensor op_input mutable_by_key setdefault key set add mutable TensorList isinstance op_input list op_input_i op_input key = TensorKey from_tensor op_input_i mutable_by_key setdefault key set add mutable edges collections defaultdict Optional TensorKey DataFlowEdge edges = collections defaultdict DataFlowEdge key mutable_set mutable_by_key items key None edges key input_version = _graph lookup key key - We consider op mutated we encounter schema where mutable argument OR ambiguous We never explicitly see any schema mutated = True mutable_set tuple mutable_set == None edges key mutated = mutated Then handle deletions Note deleting Tensor implicitly adds input edge i subtree i typed == _EventType Allocation i typed alloc_size key = TensorKey from_allocation i typed edge = edges key assert key None edge mutated None f Double delete key edge mutated = None edge input_version = _graph lookup key key - And finally handle allocations This step must last because previous two steps optimistically add input edges i subtree i typed == _EventType Allocation i typed alloc_size edges TensorKey from_allocation i typed input_version = None We don t need sort inputs makes debugging unit tests nicer dict sorted k v k v edges items k None property inputs - dict TensorKey tuple bool int MyPy can t see through ` is_allocation ` know ` v input_version ` None k bool v mutated cast int v input_version k v _edges items v is_allocation property outputs - dict TensorKey int k v input_version None v input_version + k v _edges items v is_allocation v is_deletion v mutated property intermediates - tuple TensorKey tuple k k v _edges items v is_allocation v is_deletion property start_time - int _event start_time_ns DataFlowGraph __init__ op_tree OpTree - None _op_tree = op_tree _leaf_events = _extract_leaf_events op_tree _active_version dict TensorKey Optional int = _flow_nodes = DataFlowNode e e leaf_events _flow_nodes sort key=lambda x x start_time validate property flow_nodes - tuple DataFlowNode tuple _flow_nodes validate - None Check each Tensor version pair has unique creation node outputs set tuple TensorKey int = set node flow_nodes node_outputs = set node outputs items duplicates = outputs node_outputs assert duplicates f node _event name node _edges duplicates outputs &#124; = node_outputs And check ` _nodes ` forms valid topologically sorted DAG tensor_versions dict TensorKey int = node flow_nodes key _ version node inputs items expected = tensor_versions get key assert expected == version expected version key version node outputs items prior_version = tensor_versions get key version assert version = prior_version version prior_version tensor_versions key = version property leaf_events - tuple _ProfilerEvent _leaf_events staticmethod _extract_leaf_events op_tree OpTree - tuple _ProfilerEvent Partially traverse op tree extract top level ops Consider following code ` ` ` record_function My annotation x zero_ y zero_ ` ` ` The op tree assuming no Autograd will look like Python context TorchOp My annotation TorchOp zero_ TorchOp fill_ TorchOp zero_ TorchOp fill_ The recursive structure operator calls makes data flow unwieldy In order simplify analysis we would like select highest level ops represent graph In case those ` zero_ ` ops fact ` fill_ ` called implementation detail We also do want group everything under My annotation could create overly coarse bundles lose critical semantics To address issue we walk over graph select topmost torch ops which match least one operator schema These form leaves first pass through op tree As well any allocations frees which do part kernel These events form logical nodes our data flow graph leaf_events list _ProfilerEvent = leaf_op e _ProfilerEvent - bool e typed == _EventType TorchOp e typed scope == RecordScope BACKWARD_FUNCTION bool SchemaMatcher match_schemas e typed children_fn e _ProfilerEvent leaf_op e e tag == _EventType Allocation leaf_events append e e children _ op_tree dfs children_fn=children_fn pass tuple sorted leaf_events key=lambda x x start_time_ns lookup key TensorKey - int version = _active_version setdefault key assert version None version bump key TensorKey - None prior_version = _active_version get key None assert prior_version None _active_version key = prior_version + delete key TensorKey - None assert _active_version setdefault key None _active_version key = None dataclasses dataclass CategoryElement by_id Optional Category = None by_key dict TensorKey Category = dataclasses field default_factory=dict by_version dict TensorAndID Category = dataclasses field default_factory=dict Used unit tests check internals And consequently MemoryProfile lookup This should used any other capacity _by_id_keyset set TensorKey = dataclasses field default_factory=set dataclasses dataclass CategoryDict _values collections defaultdict int CategoryElement = dataclasses field default_factory=lambda collections defaultdict CategoryElement set_by_id key TensorKey category Category - None _values key id by_id = category _values key id _by_id_keyset add key set_by_key key TensorKey category Category - None _values key id by_key key = category set_by_version key TensorKey version int category Category - None _values key id by_version key version = category setdefault_by_version key TensorKey version int category Category - None _values key id by_version setdefault key version category get key Key version int - Optional Category isinstance key Key isinstance key TensorKey None element = _values key id element by_id element by_key get key None element by_version get key version None MemoryProfile __init__ result _ProfilerResult - None _op_tree = OpTree result _data_flow_graph = DataFlowGraph _op_tree _size_map = SizeMap _op_tree _categories = CategoryDict _set_gradients_and_temporaries _set_parameters_using_python_tracer _set_inputs _set_parameters_using_data_flow _set_activations _set_optimizer_state _set_autograd_detail property timeline - tuple tuple int Action KeyAndID int output list tuple int Action KeyAndID int = allocation_times dict tuple TensorKey bool int = live_unknown dict tuple int torch device Literal True = event _op_tree dfs event typed == _EventType Allocation alloc_fields = event typed alloc_size = alloc_fields alloc_size is_allocation = alloc_size t = event start_time_ns tkey = TensorKey from_allocation alloc_fields tkey None allocation_times tkey is_allocation = t key = Key alloc_fields device ptr_and_device = alloc_fields ptr key device is_allocation ptr_and_device live_unknown output append t Action INCREMENT_VERSION key alloc_size live_unknown ptr_and_device = True output append t Action CREATE key alloc_size output append t Action DESTROY key -alloc_size live_unknown pop ptr_and_device False output append - Action PREEXISTING key -alloc_size snapshot = _category_snapshot last_version = dict sorted snapshot keys events list tuple int Action TensorAndID = - Action PREEXISTING key version key version snapshot keys key True allocation_times version == node _data_flow_graph flow_nodes key edge node _edges items edge is_allocation t = allocation_times key True events append t Action CREATE key edge mutated t = node _event start_time_ns version = edge input_version assert version None events append t Action INCREMENT_VERSION key version edge is_deletion t = allocation_times key False events append t Action DESTROY key last_version key output extend time action key version _size_map key time action key version events output sort key=lambda x x x value tuple output _is_gradient args kwargs - bool _categories get args kwargs == Category GRADIENT _category_snapshot - dict TensorAndID Optional Category all_tensor_versions set TensorAndID = set node _data_flow_graph flow_nodes all_tensor_versions update k v k _ v node inputs items all_tensor_versions update key key node intermediates all_tensor_versions update node outputs items i _categories _values values all_tensor_versions update key key i _by_id_keyset key version _categories get key version key version sorted all_tensor_versions _any_version_depends_on_gradient - set int Extract IDs Tensors which depend will depend gradient Note weakened definition depends requires us loop over data flow graph multiple times because allows dependency information flow backward through edges removes guarantee nodes topologically sorted Or indeed even valid topological order exists Put another way we have converted acyclic data flow graph into cyclic graph we attempting partition cycles involving gradient rest graph depends_on_gradient set int = set while True start_size = len depends_on_gradient node _data_flow_graph flow_nodes ids = tuple key id key _ version node inputs items _categories get key version Category GRADIENT Category PARAMETER key id depends_on_gradient ids depends_on_gradient update ids depends_on_gradient update key id key node outputs We guaranteed exit because there finite set TensorAndID pairs In practice we do expect loop more than three times once identify core parameter update loop once fold first step into loop third time where no new elements added len depends_on_gradient == start_size depends_on_gradient _set_gradients_and_temporaries - None Mark Tensors which unambiguous simple reason about Gradients straightforward detect We directly check ` grad ` property Python tracer we can detect any new gradient Tensors ` AccumulateGrad ` ops event _op_tree dfs _ p_grad extract_gradients event _categories set_by_id p_grad Category GRADIENT Similarly temporary Tensors easy identify useful flag since they can make memory use spikier than one would otherwise expect node _data_flow_graph flow_nodes i node intermediates _categories set_by_key i Category TEMPORARY _set_parameters_using_python_tracer - None event _op_tree dfs p extract_parameters event p None _categories set_by_id p Category PARAMETER _set_inputs - None Mark inputs based which Tensors updated using gradients The process differentiating between inputs activations more involved Most Tensors training loop depend least one gradient parameters depend them through updates activations optimizer state depend them transitively through parameters Critically we do need know which Tensors parameters apply method we can simply walk data flow graph build set all values which depend gradient then obtain set inputs conjugate set There however one hiccup The first time we see parameter generally forward pass first step We know inspection data flow graph v Tensor depends gradient provided we profile optimizer step v To address problem we weaken definition depends gradient any version Tensor depends gradient which turn strengthens criteria input set enough filter activations forward pass first step All analysis predicated using least one training step parameters python tracer partition graph Absent we cannot determine which Tensors inputs which ones part model depends_on_gradient = _any_version_depends_on_gradient We only want annotate Tensors which actually contribute model calculation produces_gradient set TensorAndID = set node reversed _data_flow_graph flow_nodes tensors = key version key _ version node inputs items tensors &#124; = node outputs items any _categories get i Category GRADIENT Category PARAMETER i produces_gradient i tensors produces_gradient &#124; = tensors Don t include Tensors created backward pass these generally Autograd implementation details rather than proper inputs input_candidates = produces_gradient copy node _data_flow_graph flow_nodes RecordScope BACKWARD_FUNCTION get_scopes node _event input_candidates -= set node outputs items key version input_candidates key id depends_on_gradient _categories setdefault_by_version key version Category INPUT _set_parameters_using_data_flow - None Deduce which Tensors parameters Consider following code step SGD momentum nesterov=False where ` d_p ` gradient ` param ` ` buf ` momentum buffer ` ` ` buf mul_ momentum add_ d_p alpha= - dampening d_p = buf param add_ d_p alpha=-lr ` ` ` Both ` param ` ` buf ` take gradient perform in-place update The python tracer will inspect calls ` nn Module forward ` ` optim Optimizer step ` extract parameter optimizer state respectively including parameters so generally non-issue However fallback we can also exploit several properties parameters distinguish them other model state First they directly used forward pass At point we haven t established which parts graph correspond forward pass we can deduce enough suffice Some mutable state such batch norm moving averages also contribute forward pass optimizer state does Second parameter definition used compute least one gradient depends least one gradient snapshot = _category_snapshot Determine which Tensors might parameters based forward pass data flow Note these only candidates we filter nodes we know part backward pass doesn t guarantee they part forward pass candidate_parameters set TensorAndID = set candidate_fwd_tensors set TensorAndID = i i category snapshot items category == Category INPUT node _data_flow_graph flow_nodes inputs = key value key _ value node inputs items Don t check nodes backward pass RecordScope BACKWARD_FUNCTION get_scopes node _event any _is_gradient i i inputs any _is_gradient i i node outputs items only check nodes which depend input candidate_fwd_tensors intersection inputs candidate_fwd_tensors &#124; = node outputs items candidate_parameters &#124; = inputs difference candidate_fwd_tensors Require each parameter eventually contributes value gradient used_for_gradient set TensorAndID = set node reversed _data_flow_graph flow_nodes any _is_gradient i i used_for_gradient i node outputs items used_for_gradient update key version key _ version node inputs items candidate_parameters intersection_update used_for_gradient depends gradient parameter_keys = key id key _ candidate_parameters parameter_keys = _any_version_depends_on_gradient key _ snapshot keys key id parameter_keys _categories set_by_id key Category PARAMETER _set_activations - None Flood graph identify activations required = Category INPUT Category ACTIVATION also_allowed = Category PARAMETER Category TEMPORARY node _data_flow_graph flow_nodes inputs = key value key _ value node inputs items input_categories = _categories get i i inputs input_categories required input_categories - required &#124; also_allowed Stop filling when we reach backward pass RecordScope BACKWARD_FUNCTION get_scopes node _event i node outputs items _categories setdefault_by_version i Category ACTIVATION _set_optimizer_state - None event _op_tree dfs event typed == _EventType PyCall event typed optimizer parameters = event typed optimizer parameters _ t chain from_iterable state _ _ state parameters key = TensorKey from_tensor t key None _categories set_by_id key Category OPTIMIZER_STATE _set_autograd_detail - None prior = None Category AUTOGRAD_DETAIL node _data_flow_graph flow_nodes RecordScope BACKWARD_FUNCTION get_scopes node _event key version node outputs items version == _categories get key version - prior _categories setdefault_by_version key version Category AUTOGRAD_DETAIL MemoryProfileTimeline __init__ memory_profile - None The minimum representation memory profile timeline includes memory timeline categories The timeline consists timestamp action TensorKey version numbytes elements denote any actions pre-existing create destroy increment_version occurred specific Tensor chunk memory The categories help map each TensorKey version pair into category timeline = memory_profile timeline categories = memory_profile _categories _coalesce_timeline device_str Convert memory timeline categories into memory plot consisting timestamps their respective sizes category given device Input device Output timestamps sizes category device = torch device device_str times list int = sizes list list int = update key version delta - None category = categories get key version isinstance key TensorKey None index = _CATEGORY_TO_INDEX category + sizes - index += int delta t_min = - t action key version numbytes timeline key device = device continue Convert timestamps ns us match trace events t = - t = int t Save smallest timestamp populate pre-existing allocs t_min == - t t_min t t_min = t Handle timestep len times == times append t sizes append + _ _CATEGORY_TO_INDEX t = times - times append t sizes append sizes - copy Handle memory categories action Action PREEXISTING Action CREATE update key version numbytes action == Action INCREMENT_VERSION update key version -numbytes update key version + numbytes action == Action DESTROY update key version -numbytes raise ValueError f Unknown action action times = t_min t t t times times sizes export_memory_timeline path device_str - None Saves memory timeline times sizes category JSON formatted file given path given device times sizes = _coalesce_timeline device_str TODO Write faster serialize orjson available CI json open path w f json dump times sizes f export_memory_timeline_raw path device_str - None Saves memory timeline raw memory event tuples form timestamp action numbytes category JSON formatted file given path given device device = torch device device_str raw_events list tuple int int int int = get_category_index key version category = categories get key version isinstance key TensorKey None _CATEGORY_TO_INDEX category t action key version numbytes timeline key device = device continue action Action PREEXISTING Action CREATE raw_events append pyrefly ignore bad-argument-type t _ACTION_TO_INDEX action numbytes get_category_index key version action == Action INCREMENT_VERSION raw_events append pyrefly ignore bad-argument-type t _ACTION_TO_INDEX action -numbytes get_category_index key version raw_events append pyrefly ignore bad-argument-type t _ACTION_TO_INDEX action numbytes get_category_index key version + action == Action DESTROY raw_events append pyrefly ignore bad-argument-type t _ACTION_TO_INDEX action -numbytes get_category_index key version raise ValueError f Unknown action action json open path w f json dump raw_events f export_memory_timeline_html path device_str figsize= title=None - None Exports memory timeline HTML file which contains memory timeline plot embedded PNG file Check user has matplotlib installed gracefully importlib util matplotlib_spec = importlib util find_spec matplotlib matplotlib_spec None print export_memory_timeline_html failed because matplotlib found base b encode os remove tempfile NamedTemporaryFile matplotlib pyplot plt numpy np mt = _coalesce_timeline device_str times sizes = np array mt np array mt For timeline start match Chrome traces t_min = min times times -= t_min stacked = np cumsum sizes axis= device = torch device device_str max_memory_allocated = torch cuda max_memory_allocated device max_memory_reserved = torch cuda max_memory_reserved device Plot memory timeline stacked data fig = plt figure figsize=figsize dpi= axes = fig gca category color _CATEGORY_TO_COLORS items i = _CATEGORY_TO_INDEX category axes fill_between times e stacked i stacked i + color=color alpha= fig legend Unknown i None i name i _CATEGORY_TO_COLORS Usually training steps magnitude ms axes set_xlabel Time ms axes set_ylabel Memory GB title = \n\n join title title + f Max memory allocated max_memory_allocated f GiB \n f Max memory reserved max_memory_reserved f GiB axes set_title title Embed memory timeline image into HTML file tmpfile = NamedTemporaryFile wb suffix= png delete=False tmpfile close fig savefig tmpfile name format= png open tmpfile name rb tmp encoded = b encode tmp read decode utf- html = f html head meta charset= utf- title GPU Memory Timeline HTML title head body img src= data image png base encoded body html open path w f f write html remove tmpfile name