functools math operator collections abc Sequence torch torch _C ScriptObject torch _C _distributed_c d FakeWork torch distributed _mesh_layout _MeshLayout torch distributed distributed_c d _get_default_group _resolve_process_group ProcessGroup ReduceOp Work NOTE Most c d collectives often take Tensor Tensor when you would expect Tensor Tensor In fact there will only ever one Tensor case old signature support dispatching collective multiple devices ala DataParallel we don t support API anymore Note we consistent about some more modern collectives like _allgather_base_ got rid unnecessary list When doubt consult code dispatches collective PG distributed_c d py e g work = group allgather tensor_list tensor opts indicates its always list _gcd_list numbers Sequence int - int numbers functools reduce math gcd numbers _indices_to_layout indices list int - tuple tuple int tuple int Base case A single index represents point dimension len indices = The smallest stride likely GCD differences between consecutive indices For sorted unique list all differences will positive diffs = indices i - indices i - i range len indices last_stride = _gcd_list diffs assert last_stride = This case should reached indices unique sorted Cannot determine stride indices may unique Identify starting index each row last dimension An index starts new row preceding index index - stride present indices_set = set indices higher_dim_indices = indices index indices index - last_stride indices_set higher_dim_indices append index From number rows we can deduce shape last dimension assert len indices len higher_dim_indices == Indices do form regular grid f Found len higher_dim_indices subgroups len indices total elements last_shape = len indices len higher_dim_indices Recurse higher-dimensional indices start each row higher_shapes higher_strides = _indices_to_layout higher_dim_indices Combine results recursion current dimension s results final_shapes = higher_shapes + last_shape final_strides = higher_strides + last_stride final_shapes final_strides _prepare_collective_groups process_group_so ScriptObject &#124; ProcessGroup - tuple list int list int int process_group = ProcessGroup unbox process_group_so isinstance process_group_so ScriptObject process_group_so ranks = torch distributed get_process_group_ranks process_group assert ranks TODO We can handle permutations layout inference algorithm will lose permutation so we will have reapply assert ranks == sorted ranks ranks offset = ranks ranks = r - offset r ranks shape strides = _indices_to_layout ranks layout = _MeshLayout shape strides global_pg = _get_default_group group_offsets = layout complement global_pg size all_ranks_from_zero ranks group_offsets offset NB There two flavors collectives regular functional Regular collectives allocate outputs write result accept process group support async ops work object Functional collectives expect implementation allocate outputs accept process group name must resolved do support async ops output _local_functional_all_gather_into_tensor tensor torch Tensor group_size int group_name str - torch Tensor all_gather_into_tensor Tensor input int group_size str group_name - Tensor LocalTensor ranks group_offsets offset = _prepare_collective_groups _resolve_process_group group_name assert isinstance tensor LocalTensor Input tensor must LocalTensor output_local_tensors dict int torch Tensor = group_offset group_offsets group_ranks = group_offset + r r ranks group_tensors = rank group_ranks group_tensors append tensor _local_tensors rank gathered_tensor = torch cat group_tensors dim= rank group_ranks output_local_tensors rank = gathered_tensor clone pyrefly ignore bad-argument-type bad-argument-count output = LocalTensor output_local_tensors output _local_functional_reduce_scatter_tensor tensor torch Tensor reduce_op str group_size int group_name str - torch Tensor reduce_scatter_tensor Tensor input str reduce_op int group_size str group_name - Tensor _zero_sized_like LocalTensor ranks group_offsets offset = _prepare_collective_groups _resolve_process_group group_name assert isinstance tensor LocalTensor Input tensor must LocalTensor output_local_tensors dict int torch Tensor = group_offset group_offsets group_ranks = group_offset + r r ranks group_tensors = rank group_ranks group_tensors append tensor _local_tensors rank reduced_tensor = _local_reduce reduce_op group_tensors scattered_tensor = torch split reduced_tensor reduced_tensor size len group_ranks dim= i rank enumerate group_ranks i len scattered_tensor output_local_tensors rank = scattered_tensor i clone output_local_tensors rank = _zero_sized_like reduced_tensor pyrefly ignore bad-argument-type bad-argument-count output = LocalTensor output_local_tensors output _local_functional_shard_dim_alltoall tensor torch Tensor gather_dim int shard_dim int group_name str - torch Tensor shard_dim_alltoall Tensor input int gather_dim int shard_dim str group_name - Tensor _zero_sized_like LocalTensor ranks group_offsets offset = _prepare_collective_groups _resolve_process_group group_name assert isinstance tensor LocalTensor Input tensor must LocalTensor output_local_tensors dict int torch Tensor = group_offset group_offsets group_ranks = group_offset + r r ranks group_tensors = rank group_ranks group_tensors append tensor _local_tensors rank gathered_tensor = torch cat group_tensors dim=gather_dim split_tensor = torch split gathered_tensor gathered_tensor size shard_dim len group_ranks dim=shard_dim i rank enumerate group_ranks i len split_tensor output_local_tensors rank = split_tensor i clone output_local_tensors rank = _zero_sized_like gathered_tensor shard_dim pyrefly ignore bad-argument-type bad-argument-count output = LocalTensor output_local_tensors output _local_broadcast_ tensors list torch Tensor process_group_so ScriptObject root_rank int root_tensor int async_op bool = True timeout int = - - tuple list torch Tensor ScriptObject broadcast_ Tensor tensors __torch__ torch classes c d ProcessGroup process_group int root_rank int root_tensor bool async_op=True int timeout=- - Tensor __torch__ torch classes c d Work LocalTensor assert len tensors == assert root_tensor == tensor = tensors ranks group_offsets offset = _prepare_collective_groups process_group_so We re going assume SPMD where every rank group root_rank same relative others relative_root_rank = root_rank - offset assert isinstance tensor LocalTensor Input tensor must LocalTensor group_offset group_offsets For tensors group group_offset + r r ranks perform broadcast them group_ranks = group_offset + r r ranks source_rank = group_offset + relative_root_rank source_tensor = tensor _local_tensors source_rank Broadcast source tensor all ranks group rank group_ranks source_rank = rank tensor _local_tensors rank copy_ source_tensor work = FakeWork work_so = Work boxed work tensors work_so _local_reduce reduce_op ReduceOp &#124; str tensors list torch Tensor - torch Tensor reduce_op == ReduceOp SUM reduce_op == sum op = operator add reduce_op == ReduceOp AVG reduce_op == avg op = None reduce_op == ReduceOp PRODUCT reduce_op == product op = operator mul reduce_op == ReduceOp MIN reduce_op == min op = torch minimum reduce_op == ReduceOp MAX reduce_op == max op = torch maximum reduce_op == ReduceOp BAND reduce_op == band op = torch bitwise_and reduce_op == ReduceOp BOR reduce_op == bor op = torch bitwise_or reduce_op == ReduceOp BXOR reduce_op == bxor op = torch bitwise_xor reduce_op == ReduceOp PREMUL_SUM reduce_op == premul_sum raise NotImplementedError PREMUL_SUM need add binding scaling factor raise NotImplementedError f ReduceOp reduce_op implemented reduce_op == ReduceOp AVG reduce_op == avg functools reduce operator add tensors len tensors assert op None functools reduce op tensors _local_all_reduce_ tensors list torch Tensor process_group_so ScriptObject reduce_op_so ScriptObject sparse_indices torch Tensor &#124; None = None async_op bool = True timeout int = - - tuple list torch Tensor ScriptObject allreduce_ Tensor tensors __torch__ torch classes c d ProcessGroup process_group __torch__ torch classes c d ReduceOp reduce_op Tensor sparse_indices bool async_op=True int timeout=- - Tensor __torch__ torch classes c d Work LocalTensor assert len tensors == tensor = tensors reduce_op = reduce_op_so op type ignore attr-defined ranks group_offsets _offset = _prepare_collective_groups process_group_so assert isinstance tensor LocalTensor Input tensor must LocalTensor group_offset group_offsets For tensors group group_offset + r r ranks perform allreduce them group_ranks = group_offset + r r ranks Collect tensors specified ranks group group_tensors = rank group_ranks group_tensors append tensor _local_tensors rank Perform reduction operation reduced_tensor = _local_reduce reduce_op group_tensors Update all tensors group reduced result rank group_ranks tensor _local_tensors rank copy_ reduced_tensor work = FakeWork work_so = Work boxed work tensors work_so _local_allreduce_coalesced_ tensors list torch Tensor process_group_so ScriptObject reduce_op_so ScriptObject async_op bool = True timeout int = - - ScriptObject allreduce_coalesced_ Tensor tensors __torch__ torch classes c d ProcessGroup process_group __torch__ torch classes c d ReduceOp reduce_op bool async_op=True int timeout=- - __torch__ torch classes c d Work LocalTensor reduce_op = reduce_op_so op type ignore attr-defined ranks group_offsets _offset = _prepare_collective_groups process_group_so group_offset group_offsets For tensors group group_offset + r r ranks perform allreduce all tensors together group_ranks = group_offset + r r ranks For each tensor perform reduction operation tensor tensors assert isinstance tensor LocalTensor Input tensor must LocalTensor Collect tensors specified ranks group group_tensors = rank group_ranks group_tensors append tensor _local_tensors rank Perform reduction operation reduced_tensor = _local_reduce reduce_op group_tensors Update all tensors group reduced result rank group_ranks tensor _local_tensors rank copy_ reduced_tensor work = FakeWork work_so = Work boxed work work_so _local_reduce_scatter_tensor_coalesced_ output_tensors list torch Tensor input_tensors list torch Tensor process_group_so ScriptObject reduce_op_so ScriptObject async_op bool = True timeout int = - - ScriptObject reduce_scatter_tensor_coalesced_ Tensor outputs Tensor inputs __torch__ torch classes c d ProcessGroup process_group __torch__ torch classes c d ReduceOp reduce_op bool async_op=True int timeout=- - __torch__ torch classes c d Work LocalTensor reduce_op = reduce_op_so op type ignore attr-defined ranks group_offsets _offset = _prepare_collective_groups process_group_so group_offset group_offsets For tensors group group_offset + r r ranks perform allreduce all tensors together group_ranks = group_offset + r r ranks For each tensor perform reduction operation input_tensor output_tensor zip input_tensors output_tensors assert isinstance input_tensor LocalTensor Input tensor must LocalTensor assert isinstance output_tensor LocalTensor Output tensor must LocalTensor Collect tensors specified ranks group group_inputs = rank group_ranks group_inputs append input_tensor _local_tensors rank Perform reduction operation reduced_input = _local_reduce reduce_op group_inputs reduced_input_splits = torch split reduced_input reduced_input size len group_ranks dim= Update all tensors group reduced result i rank enumerate group_ranks output_tensor _local_tensors rank copy_ reduced_input_splits i work = FakeWork work_so = Work boxed work work_so _local_all_gather_ output_tensors list list torch Tensor input_tensors list torch Tensor process_group_so ScriptObject async_op bool = True timeout int = - - tuple list list torch Tensor ScriptObject allgather_ Tensor output_tensors Tensor input_tensors __torch__ torch classes c d ProcessGroup process_group bool async_op=True int timeout=- - Tensor __torch__ torch classes c d Work LocalTensor assert len output_tensors == assert len input_tensors == input_tensor = input_tensors pyrefly ignore bad-assignment output_tensors = output_tensors ranks group_offsets _offset = _prepare_collective_groups process_group_so i range len output_tensors assert isinstance output_tensors i LocalTensor Output tensor must LocalTensor group_offset group_offsets For tensors group group_offset + r r ranks perform all_gather them group_ranks = group_offset + r r ranks For each rank group gather their input tensor i rank_i enumerate group_ranks allgather object happens create pure tensor so we special case here source_tensor = input_tensor isinstance input_tensor LocalTensor source_tensor = input_tensor _local_tensors rank_i pyrefly ignore missing-attribute output_tensors i copy_ source_tensor work = FakeWork work_so = Work boxed work pyrefly ignore bad-return output_tensors work_so _local_allgather_into_tensor_coalesced_ output_tensors list torch Tensor input_tensors list torch Tensor process_group_so ScriptObject async_op bool = True - ScriptObject allgather_into_tensor_coalesced_ Tensor outputs Tensor inputs __torch__ torch classes c d ProcessGroup process_group bool async_op=True - __torch__ torch classes c d Work LocalTensor ranks group_offsets _offset = _prepare_collective_groups process_group_so Each output tensor should sized hold all gathered inputs outputs i will contain all inputs i all ranks assert len output_tensors == len input_tensors f Number outputs len output_tensors must match number inputs len input_tensors group_offset group_offsets For tensors group group_offset + r r ranks perform allgather_into_tensor them group_ranks = group_offset + r r ranks For each input output pair input_tensor output_tensor zip input_tensors output_tensors assert isinstance input_tensor LocalTensor Input tensor must LocalTensor assert isinstance output_tensor LocalTensor Output tensor must LocalTensor Gather input_tensor all ranks into output_tensor The output should concatenation all inputs along first dimension gathered_tensors = rank group_ranks gathered_tensors append input_tensor _local_tensors rank Concatenate along first dimension copy output gathered_tensors concatenated = torch cat gathered_tensors dim= rank group_ranks output_tensor _local_tensors rank copy_ concatenated work = FakeWork work_so = Work boxed work work_so _local_gather_ output_tensors list list torch Tensor input_tensors list torch Tensor process_group_so ScriptObject root_rank int async_op bool = True timeout int = - - ScriptObject gather_ Tensor output_tensors Tensor input_tensors __torch__ torch classes c d ProcessGroup process_group int root_rank bool async_op=True int timeout=- - __torch__ torch classes c d Work raise NotImplementedError LocalTensor does support MPMD operations like gather only root rank receives data Use SPMD collective operations like allgather instead _local_scatter_ output_tensors list torch Tensor input_tensors list list torch Tensor process_group_so ScriptObject root_rank int async_op bool = True timeout int = - - tuple list torch Tensor ScriptObject scatter_ Tensor output_tensors Tensor input_tensors __torch__ torch classes c d ProcessGroup process_group int root_rank bool async_op=True int timeout=- - Tensor __torch__ torch classes c d Work LocalTensor assert len output_tensors == assert len input_tensors == output_tensor = output_tensors pyrefly ignore bad-assignment input_tensors = input_tensors ranks group_offsets offset = _prepare_collective_groups process_group_so We re going assume SPMD where every rank group root_rank same relative others relative_root_rank = root_rank - offset assert isinstance output_tensor LocalTensor Output tensor must LocalTensor assert len ranks == len input_tensors ranks input_tensors group_offset group_offsets For tensors group group_offset + r r ranks perform scatter them group_ranks = group_offset + r r ranks Root rank scatters its input tensors all ranks group i rank enumerate group_ranks input_tensor = input_tensors i assert isinstance input_tensor LocalTensor Each rank i gets i-th input tensor root source_tensor = input_tensor _local_tensors group_offset + relative_root_rank output_tensor _local_tensors rank copy_ source_tensor work = FakeWork work_so = Work boxed work output_tensors work_so _local_alltoall_ output_tensors list torch Tensor input_tensors list torch Tensor process_group_so ScriptObject async_op bool = True timeout int = - - tuple list torch Tensor ScriptObject alltoall_ Tensor output_tensors Tensor input_tensors __torch__ torch classes c d ProcessGroup process_group bool async_op=True int timeout=- - Tensor __torch__ torch classes c d Work LocalTensor ranks group_offsets _offset = _prepare_collective_groups process_group_so assert len input_tensors == len output_tensors == len ranks f Number input tensors len input_tensors f output tensors len output_tensors ranks len ranks must match group_offset group_offsets For tensors group group_offset + r r ranks perform alltoall them group_ranks = group_offset + r r ranks In alltoall rank i sends input_tensors j rank j receives into output_tensors i rank j i rank_i enumerate group_ranks output_tensor = output_tensors i assert isinstance output_tensor LocalTensor Output tensor must LocalTensor j rank_j enumerate group_ranks input_tensor = input_tensors j assert isinstance input_tensor LocalTensor Input tensor must LocalTensor Rank i s j-th input tensor goes rank j s i-th output tensor source_tensor = input_tensor _local_tensors rank_i output_tensor _local_tensors rank_j copy_ source_tensor work = FakeWork work_so = Work boxed work output_tensors work_so _local_alltoall_base_ output_tensor torch Tensor input_tensor torch Tensor process_group_so ScriptObject output_split_sizes list int input_split_sizes list int async_op bool = True timeout int = - - ScriptObject alltoall_base_ Tensor output Tensor input __torch__ torch classes c d ProcessGroup process_group int output_split_sizes int input_split_sizes bool async_op=True int timeout=- - __torch__ torch classes c d Work LocalTensor ranks group_offsets _offset = _prepare_collective_groups process_group_so assert isinstance input_tensor LocalTensor Input tensor must LocalTensor assert isinstance output_tensor LocalTensor Output tensor must LocalTensor Convert split sizes lists they aren t already output_split_sizes None output_split_sizes = list output_split_sizes input_split_sizes None input_split_sizes = list input_split_sizes group_offset group_offsets For tensors group group_offset + r r ranks perform alltoall_base them group_ranks = group_offset + r r ranks i rank_i enumerate group_ranks Split input tensor rank_i according input_split_sizes rank_tensor = input_tensor _local_tensors rank_i input_split_sizes None len input_split_sizes Split input tensor input_splits = torch split rank_tensor input_split_sizes dim= No split sizes specified split evenly split_size = rank_tensor size len group_ranks input_splits = torch split rank_tensor split_size dim= Send each split corresponding rank j rank_j enumerate group_ranks j len input_splits split_tensor = input_splits j Determine where place split output tensor output_split_sizes None len output_split_sizes Calculate offset based output split sizes output_offset = sum output_split_sizes i i end_offset = output_offset + output_split_sizes i i len output_split_sizes output_tensor _local_tensors rank_j size No output split sizes use even splits split_size = output_tensor _local_tensors rank_j size len group_ranks output_offset = i split_size end_offset = min i + split_size output_tensor _local_tensors rank_j size Copy split appropriate section output tensor output_section = output_tensor _local_tensors rank_j output_offset end_offset output_section numel Reshape split_tensor match output_section necessary split_tensor size = output_section size split_tensor = split_tensor view output_section size output_section copy_ split_tensor work = FakeWork work_so = Work boxed work work_so _local_barrier tensor torch Tensor process_group_so ScriptObject device_ids list int async_op bool = True timeout int = - - ScriptObject barrier Tensor tensor __torch__ torch classes c d ProcessGroup process_group int device_ids bool async_op=True int timeout=- - __torch__ torch classes c d Work LocalTensor Barrier synchronization primitive - local simulation we don t need do any actual work since all ranks same process Just validate tensor LocalTensor assert isinstance tensor LocalTensor In real distributed setting barrier would synchronize all processes In local simulation essentially no-op since all ranks local work = FakeWork work_so = Work boxed work work_so _local_monitored_barrier_ tensor torch Tensor process_group_so ScriptObject device_ids list int timeout int wait_all_ranks bool - None monitored_barrier_ Tensor tensor __torch__ torch classes c d ProcessGroup process_group int device_ids int timeout bool wait_all_ranks - LocalTensor Monitored barrier synchronization primitive monitoring - local simulation we don t need do any actual work since all ranks same process Just validate tensor LocalTensor assert isinstance tensor LocalTensor In real distributed setting monitored barrier would synchronize all processes provide monitoring capabilities In local simulation essentially no-op since all ranks local no actual synchronization needed _local_send tensors list torch Tensor process_group_so ScriptObject dst int tag int - ScriptObject send Tensor tensors __torch__ torch classes c d ProcessGroup process_group int dst int tag - __torch__ torch classes c d Work raise NotImplementedError LocalTensor does support MPMD operations like send Use SPMD collective operations instead _local_recv_ tensors list torch Tensor process_group_so ScriptObject src int tag int - ScriptObject recv_ Tensor tensors __torch__ torch classes c d ProcessGroup process_group int src int tag - __torch__ torch classes c d Work raise NotImplementedError LocalTensor does support MPMD operations like recv Use SPMD collective operations instead _local_recv_any_source_ tensors list torch Tensor process_group_so ScriptObject tag int - ScriptObject recv_any_source_ Tensor tensors __torch__ torch classes c d ProcessGroup process_group int tag - __torch__ torch classes c d Work raise NotImplementedError LocalTensor does support MPMD operations like recv_any_source Use SPMD collective operations instead