Owner s module dynamo copy os shutil unittest unittest mock patch torch torch _dynamo torch _dynamo test_case torch _functorch _aot_autograd torch _dynamo config dynamo_config torch _dynamo utils counters torch _functorch config functorch_config torch _functorch _aot_autograd autograd_cache AOTAutogradCache autograd_cache_key BypassAOTAutogradCache sanitize_gm_for_cache torch _functorch _aot_autograd schemas AOTConfig torch _guards TracingContext torch _inductor config inductor_config torch _inductor runtime runtime_utils cache_dir torch _inductor runtime triton_compat tl triton torch _inductor test_case TestCase InductorTestCase torch _inductor utils fresh_cache torch _subclasses FakeTensorMode torch compiler _cache CacheArtifactManager torch fx experimental symbolic_shapes ShapeEnv torch testing _internal common_cuda SM OrLater TEST_MULTIGPU torch testing _internal common_device_type largeTensorTest torch testing _internal common_utils instantiate_parametrized_tests parametrize skipIfWindows torch testing _internal inductor_utils GPU_TYPE HAS_GPU requires_triton torch testing _internal triton_utils requires_cuda_and_triton torch testing _internal two_tensor TwoTensor aot_eager_regional_inductor Regional inductor backend AOT autograd Uses regional_inductor both forward backward compiler torch _dynamo backends common aot_autograd torch fx passes regional_inductor regional_inductor aot_autograd fw_compiler=regional_inductor bw_compiler=regional_inductor saved_tensors_hooks_to_gm pack_fn unpack_fn pack_cache_hash=None unpack_cache_hash=None symbolic_tracing=True inp_fn=None symbolic_tracing pack_gm = torch fx symbolic_trace pack_fn unpack_gm = torch fx symbolic_trace unpack_fn functorch make_fx inp_fn inp = inp_fn inp = torch randn torch _dynamo mark_dynamic inp torch _dynamo mark_dynamic inp pack_out = pack_fn inp pack_gm = make_fx pack_fn inp unpack_gm = make_fx unpack_fn pack_out set_manual_hash g manual_hash node g nodes node meta node meta get is_wrapped False node meta user_cache_hash = manual_hash pack_cache_hash set_manual_hash pack_gm graph pack_cache_hash unpack_cache_hash set_manual_hash unpack_gm graph unpack_cache_hash pack_gm unpack_gm amax_to_scale amax torch Tensor float _dtype torch dtype round_scales_to_power_of_ bool = False amax = amax torch float res = torch finfo float _dtype max torch clamp amax min= e- res = res torch float res Must module level use fx wrap torch fx wrap _pack_fp _with_scale_wrap x x dtype is_floating_point x amax = torch max torch abs x scale = amax_to_scale amax torch float _e m x_scaled = x torch float scale x_fp = x_scaled torch float _e m x dtype scale x_fp torch fx wrap _unpack_fp _with_scale_wrap x isinstance x torch Tensor x dtype scale x_fp = x y = x_fp torch float scale y dtype instantiate_parametrized_tests AOTAutogradCacheTests InductorTestCase setUp Reset all counters caches before each unit test super setUp counters clear _clear_all_caches _clear_all_caches Clear every cache including AOTAutogradCache FXCache torch _inductor codecache FxGraphCache clear AOTAutogradCache clear CacheArtifactManager clear _clear_dynamo_and_codecache _clear_dynamo_and_codecache Clear unrelated caches like dynamo PyCodeCache torch _dynamo reset torch _inductor codecache PyCodeCache cache_clear purge=True requires_triton functorch_config patch enable_autograd_cache True inductor_config patch fx_graph_cache True fx_graph_remote_cache False autotune_local_cache True parametrize device GPU_TYPE cpu parametrize dtype torch float torch bfloat parametrize dynamic False True test_cache_hot_load device dtype dynamic Verify we can populate hot load functions cache device == GPU_TYPE HAS_GPU raise unittest SkipTest f requires GPU_TYPE device == cuda dtype == torch bfloat SM OrLater raise unittest SkipTest requires SM later fn x y x sin y = torch rand dtype=dtype device=device requires_grad=True b = torch rand dtype=dtype device=device requires_grad=True Record artifacts fresh_cache compiled_fn = torch compile fn dynamic=dynamic A first call should miss cache eager_result = fn b compiled_result = compiled_fn b compiled_result sum backward hasattr _dynamo_weak_dynamic_indices del _dynamo_weak_dynamic_indices assertEqual eager_result compiled_result functorch_config bundled_autograd_cache assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved artifacts = torch compiler save_cache_artifacts assertIsNotNone artifacts artifact_bytes cache_info = artifacts autotune_expect = device == GPU_TYPE functorch_config bundled_autograd_cache assertEqual len cache_info inductor_artifacts assertEqual len cache_info inductor_artifacts assertEqual len cache_info autotune_artifacts autotune_expect assertEqual len cache_info aot_autograd_artifacts assertEqual len cache_info pgo_artifacts _clear_all_caches Clean triton kernels shutil rmtree os path join cache_dir triton ignore_errors=True We did load anything so dont hit yet fresh_cache eager_result = fn b compiled_result = compiled_fn b assertEqual eager_result compiled_result compiled_result sum backward hasattr _dynamo_weak_dynamic_indices del _dynamo_weak_dynamic_indices functorch_config bundled_autograd_cache assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved _clear_all_caches Clean triton kernels shutil rmtree os path join cache_dir triton ignore_errors=True Hot load hit fresh_cache cache_info = torch compiler load_cache_artifacts artifact_bytes functorch_config bundled_autograd_cache assertEqual len cache_info inductor_artifacts assertEqual len cache_info inductor_artifacts assertEqual len cache_info autotune_artifacts autotune_expect assertEqual len cache_info aot_autograd_artifacts assertEqual len cache_info pgo_artifacts eager_result = fn b compiled_result = compiled_fn b compiled_result sum backward hasattr _dynamo_weak_dynamic_indices del _dynamo_weak_dynamic_indices assertEqual eager_result compiled_result functorch_config bundled_autograd_cache assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True test_basic Verify interactions between FXGraphCache AOTAutogradCache fn x y x y y = torch rand b = torch rand compiled_fn = torch compile fn backend= inductor A first call should miss cache assertEqual fn b compiled_fn b assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved A second call should hit First reset so in-memory guards don t prevent compilation _clear_dynamo_and_codecache assertEqual fn b compiled_fn b assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True test_vmap make fn x y f = lambda x y x y + sum dim= noqa E vmapped = torch vmap f x y vmapped sum dim= x = torch randn requires_grad=True y = torch randn requires_grad=True x = x detach clone requires_grad_ True y = y detach clone requires_grad_ True compiled_fn = torch compile fn backend= inductor A first call should miss cache assertEqual fn x y compiled_fn x y fn x y sum backward compiled_fn x y sum backward assertEqual x grad x grad assertEqual y grad y grad assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved Reset all tensors x = torch randn requires_grad=True y = torch randn requires_grad=True x = x detach clone requires_grad_ True y = y detach clone requires_grad_ True A second call should hit First reset so in-memory guards don t prevent compilation _clear_dynamo_and_codecache assertEqual fn x y compiled_fn x y fn x y sum backward compiled_fn x y sum backward assertEqual x grad x grad assertEqual y grad y grad assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True test_multi_graph_specialization Verify multi graph specializations all cache hit fn x x = torch randn = torch randn = torch randn torch _dynamo mark_dynamic specialize_on= lambda x x == lambda x x == compiled_fn = torch compile fn backend= inductor A first call should miss cache compiled_fn compiled_fn compiled_fn assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved _clear_dynamo_and_codecache A second call should hit all graphs compiled_fn compiled_fn compiled_fn assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True test_symbol_specialization Verify symbol specializations don t cause cache miss fn x y z torch randn + x + y z torch randn = torch rand torch _dynamo maybe_mark_dynamic b = torch rand c = torch randn torch _dynamo maybe_mark_dynamic c compiled_fn = torch compile fn backend= inductor A first call should miss cache compiled_fn b c assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved A second call should hit even new dimension marked dynamic later specialized part tracing = torch rand torch _dynamo maybe_mark_dynamic b = torch rand torch _dynamo maybe_mark_dynamic b c = torch randn torch _dynamo maybe_mark_dynamic c _clear_dynamo_and_codecache compiled_fn b c assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved functorch_config patch enable_autograd_cache True test_aot_runtime_trace_joint torch compile backend= inductor f x tmp = x sin s = tmp shape tmp expand s s x_a = torch randn requires_grad=True x = TwoTensor x_a x_a clone out = f x out sum backward _clear_dynamo_and_codecache out = f x out sum backward inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True skipIfWindows msg= Known issue Window can t delete loaded modules so we can t clear module cache test_clear_fx_graph_cache Verify interactions between FXGraphCache AOTAutogradCache fn x y x y y = torch rand b = torch rand compiled_fn = torch compile fn backend= inductor A first call should miss cache assertEqual fn b compiled_fn b assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved Clear FX graph cache second call should also miss _clear_dynamo_and_codecache torch _inductor codecache FxGraphCache clear assertEqual fn b compiled_fn b functorch_config bundled_autograd_cache Bundled AutogradCache doesn t care FxGraphCache cleared assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit We save again into cache assertEqual counters aot_autograd autograd_cache_saved inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True functorch_config patch strict_autograd_cache True unittest skipIf torch cuda is_available CUDA unavailable requires_triton test_non_bundled_to_bundled_config_change functorch_config bundled_autograd_cache raise unittest SkipTest BundledAutogradCache already enabled fn x y x y y = torch rand device=GPU_TYPE b = torch rand device=GPU_TYPE compiled_fn = torch compile fn backend= inductor assertEqual fn b compiled_fn b assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved Now turn bundled autograd cache see we successfully save again functorch_config patch bundled_autograd_cache True torch _dynamo reset assertEqual fn b compiled_fn b assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True view_replay_for_aliased_outputs True test_view_replay fn tmp = detach mul_ tmp torch autograd _force_original_view_tracking True compiled_fn = torch compile fn run_and_check miss hit bypass _clear_dynamo_and_codecache inp = torch rand compiled_inp = inp clone detach torch autograd _force_original_view_tracking True out = fn inp compiled_out = compiled_fn compiled_inp assertEqual out compiled_out assertEqual counters aot_autograd autograd_cache_miss miss assertEqual counters aot_autograd autograd_cache_hit hit assertEqual counters aot_autograd autograd_cache_bypass bypass run_and_check miss= hit= bypass= run_and_check miss= hit= bypass= run_and_check miss= hit= bypass= inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True strict_autograd_cache True test_invoke_subgraph torch _higher_order_ops invoke_subgraph mark_compile_region mark_compile_region gn x y x + y torch compile fn x y gn x y + gn x y = torch randn b = torch randn fn b inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True strict_autograd_cache True parametrize fn_select tag_activation_checkpoint allow_in_graph test_unsafe_mark_cacheable fn_select fn_select == tag_activation_checkpoint torch utils checkpoint checkpoint gn x y z=None = torch matmul x y z None torch matmul z torch compile fn x y z torch cos checkpoint gn x y use_reentrant=False z=z fn_name = torch ops higher_order tag_activation_checkpoint assert fn_select == allow_in_graph torch _dynamo allow_in_graph AllowInGraphFunc torch autograd Function staticmethod forward _ x torch _dynamo graph_break x sin torch compile fn x y z AllowInGraphFunc apply x fn_name = torch _dynamo variables misc trampoline_autograd_apply x = torch randn y = torch randn z = torch randn args = x y z assertRaisesRegex torch _dynamo exc BackendCompilerFailed r BypassAOTAutogradCache Unsupported call_function target fn args assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_bypass _clear_dynamo_and_codecache fn_select == allow_in_graph TODO Fix allow graph raise unittest SkipTest Allow graph produces unserializable cache artifact inductor_config patch unsafe_marked_cacheable_functions fn_name key fn args assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_bypass _clear_dynamo_and_codecache fn args assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_bypass _clear_dynamo_and_codecache inductor_config patch unsafe_marked_cacheable_functions fn_name key fn args assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_bypass _clear_dynamo_and_codecache fn args assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_bypass On second try same key should hit once more inductor_config patch unsafe_marked_cacheable_functions fn_name key _clear_dynamo_and_codecache fn args assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_bypass inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache False functorch_config patch enable_autograd_cache True test_fx_graph_cache_off Should use cache FXGraphCache enabled fn x y x y y = torch rand b = torch rand compiled_fn = torch compile fn backend= inductor A first call should miss cache assertEqual fn b compiled_fn b assertEqual counters aot_autograd autograd_cache_bypass assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved Clear FX graph cache second call should also miss _clear_dynamo_and_codecache assertEqual fn b compiled_fn b assertEqual counters aot_autograd autograd_cache_bypass assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True strict_autograd_cache True dynamo_config patch compiled_autograd True test_compiled_autograd_bypass Need make compiled autograd graph serializable fn b out = cos + b loss = out sum ga gb = torch autograd grad loss inputs= b = torch randn requires_grad=True b = torch randn requires_grad=True compiled_fn = torch compile fn backend= inductor assertRaisesRegex torch _dynamo exc BackendCompilerFailed BypassAOTAutogradCache Unsupported call_function target torch _dynamo compiled_autograd ops validate_outputs compiled_fn b inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True dynamo_config patch compiled_autograd True test_inference_graph_cache_hit_with_compiled_autograd_enabled fn b out = cos + b out sum = torch randn b = torch randn compiled_fn = torch compile fn backend= inductor assertEqual fn b compiled_fn b assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_saved Clear dynamo run again Should cache hit counters clear _clear_dynamo_and_codecache assertEqual fn b compiled_fn b assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved requires_cuda_and_triton inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True functorch_config patch autograd_cache_allow_custom_autograd_functions True test_custom_autograd_function_miss MyAutogradFunction torch autograd Function staticmethod forward ctx x y = x sin ctx save_for_backward y ctx foo = x cos y staticmethod backward ctx grad_output result = ctx saved_tensors grad_output result + ctx foo grad_output fn MyAutogradFunction apply = torch randn device= cuda requires_grad=True = clone detach_ requires_grad_ True compiled_fn = torch compile fn backend= inductor result = compiled_fn result sum backward assertEqual fn result assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_saved MyAutogradFunction torch autograd Function noqa F Change function slightly staticmethod forward ctx x y = x cos ctx save_for_backward y ctx foo = x sin y staticmethod backward ctx grad_output result = ctx saved_tensors grad_output result + ctx foo grad_output Clear dynamo run again Should cache miss counters clear _clear_dynamo_and_codecache result = compiled_fn assertEqual fn result result sum backward assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved requires_cuda_and_triton inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True functorch_config patch autograd_cache_allow_custom_autograd_functions True test_custom_autograd_function MyAutogradFunction torch autograd Function staticmethod forward ctx x y = x sin ctx save_for_backward y ctx foo = x cos y staticmethod backward ctx grad_output result = ctx saved_tensors grad_output result + ctx foo grad_output fn MyAutogradFunction apply = torch randn device= cuda requires_grad=True = clone detach_ requires_grad_ True compiled_fn = torch compile fn backend= inductor result = compiled_fn result sum backward assertEqual fn result assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_saved Clear dynamo run again Should cache hit counters clear _clear_dynamo_and_codecache result = compiled_fn assertEqual fn result result sum backward assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved requires_cuda_and_triton inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True functorch_config patch autograd_cache_allow_custom_autograd_functions True test_custom_autograd_function_with_custom_triton_kernel triton jit my_jit x arg_ = tl load x tl store x arg_ + torch _library triton_op test my_triton_op mutates_args= my_triton_op x torch Tensor - torch Tensor y = x clone detach_ requires_grad_ True torch _library capture_triton my_jit y y MyAutogradFunction torch autograd Function staticmethod forward ctx x y = torch ops test my_triton_op x ctx save_for_backward y ctx foo = x cos y staticmethod backward ctx grad_output result = ctx saved_tensors grad_output result + ctx foo grad_output fn MyAutogradFunction apply = torch randn device=GPU_TYPE requires_grad=True = clone detach_ requires_grad_ True compiled_fn = torch compile fn backend= inductor result = compiled_fn assertEqual fn result result sum backward assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_saved Clear dynamo run again Should cache hit counters clear _clear_dynamo_and_codecache result = compiled_fn assertEqual fn result result sum backward assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved requires_cuda_and_triton inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True functorch_config patch autograd_cache_allow_custom_autograd_functions True test_custom_autograd_function_with_custom_triton_kernel_cache_invalidation triton jit my_jit x arg_ = tl load x tl store x arg_ + torch _library triton_op test my_triton_op mutates_args= my_triton_op x torch Tensor - torch Tensor y = x clone detach_ requires_grad_ True torch _library capture_triton my_jit y y MyAutogradFunction torch autograd Function staticmethod forward ctx x y = torch ops test my_triton_op x ctx save_for_backward y ctx foo = x cos y staticmethod backward ctx grad_output result = ctx saved_tensors grad_output result + ctx foo grad_output fn MyAutogradFunction apply = torch randn device=GPU_TYPE requires_grad=True = clone detach_ requires_grad_ True = clone detach_ requires_grad_ True compiled_fn = torch compile fn backend= inductor result = compiled_fn assertEqual fn result result sum backward assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_saved Clear dynamo run again Should cache hit counters clear _clear_dynamo_and_codecache result = compiled_fn assertEqual fn result result sum backward assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved Now modify source code my_jit redefining triton jit my_jit x noqa F arg_ = tl load x tl store x arg_ + Changed + + torch _library triton_op test my_triton_op mutates_args= my_triton_op x torch Tensor - torch Tensor noqa F y = x clone detach_ requires_grad_ True torch _library capture_triton my_jit y y Clear dynamo run again Should cache miss due modified source code counters clear _clear_dynamo_and_codecache compiled_fn = torch compile fn backend= inductor result = compiled_fn Assert after changing source code cache no longer hits assertEqual counters aot_autograd autograd_cache_miss assertEqual fn result requires_cuda_and_triton inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True test_triton_op_cache_invalidation torch _library capture_triton triton jit my_jit x noqa F arg_ = tl load x tl store x arg_ + torch _library triton_op test my_triton_op mutates_args= my_triton_op x torch Tensor - torch Tensor noqa F y = x clone detach_ requires_grad_ True capture_triton my_jit y y fn torch ops test my_triton_op = torch randn device=GPU_TYPE = clone detach_ compiled_fn = torch compile fn backend= inductor result = compiled_fn assertEqual fn result assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved _clear_dynamo_and_codecache Redefine triton op triton jit my_jit x noqa F arg_ = tl load x tl store x arg_ + torch _library triton_op test my_triton_op mutates_args= my_triton_op x torch Tensor - torch Tensor noqa F y = x clone detach_ requires_grad_ True torch _library capture_triton my_jit y y compiled_fn = torch compile fn backend= inductor result = compiled_fn Second run should still miss assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved assertEqual fn result requires_cuda_and_triton inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True unittest expectedFailure Currently ops call other ops does properly invalidate cache test_triton_op_cache_multiple_ops_invalidation triton jit my_jit x arg_ = tl load x tl store x arg_ + triton jit my_jit x arg_ = tl load x tl store x arg_ + torch _library triton_op test my_triton_op mutates_args= my_triton_op x torch Tensor - torch Tensor y = x clone detach_ requires_grad_ True torch _library capture_triton my_jit y torch _library capture_triton my_jit y y torch _library triton_op test my_triton_op mutates_args= my_triton_op x torch Tensor - torch Tensor y = x clone detach_ requires_grad_ True torch ops test my_triton_op y y fn torch ops test my_triton_op = torch randn device=GPU_TYPE = clone detach_ compiled_fn = torch compile fn backend= inductor result = compiled_fn assertEqual fn result assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved _clear_dynamo_and_codecache Redefine triton op triton jit my_jit x noqa F arg_ = tl load x tl store x arg_ + torch _library triton_op test my_triton_op mutates_args= my_triton_op x torch Tensor - torch Tensor noqa F y = x clone detach_ requires_grad_ True torch _library capture_triton my_jit y torch _library capture_triton my_jit y y torch _library triton_op test my_triton_op mutates_args= my_triton_op x torch Tensor - torch Tensor noqa F y = x clone detach_ requires_grad_ True torch ops test my_triton_op y y compiled_fn = torch compile fn backend= inductor result = compiled_fn Second run should still miss assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved assertEqual fn result inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True functorch_config patch strict_autograd_cache True test_autograd_lazy_backward Lazily compile backward lazily save cache fn b cos + b = torch randn requires_grad=True b = torch randn requires_grad=True = detach clone requires_grad_ True b = b detach clone requires_grad_ True compiled_fn = torch compile fn backend= inductor assertEqual fn b compiled_fn b assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved Clear dynamo run again Should cache miss still because backward hasn t run _clear_dynamo_and_codecache assertEqual fn b compiled_fn b assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved Now let s run backward fn b sum backward compiled_fn b sum backward assertEqual grad grad assertEqual b grad b grad assertEqual counters aot_autograd autograd_cache_saved Clear dynamo rerun everything now there should cache hit _clear_dynamo_and_codecache = torch randn requires_grad=True b = torch randn requires_grad=True = detach clone requires_grad_ True b = b detach clone requires_grad_ True assertEqual fn b compiled_fn b assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved fn b sum backward compiled_fn b sum backward assertEqual grad grad assertEqual b grad b grad inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True functorch_config patch strict_autograd_cache True test_autograd_no_dynamo_trace_backward Test dynamo does trace into backward compiled function even cache hit torch _dynamo eval_frame clear_dynamo_tls torch compile fn x Calls x sum backward during forward execution fn x_grad = torch autograd grad x sum x x_grad = torch randn requires_grad=True device= cpu result = fn assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit Backward ` sum ` will run during execution graph break assertEqual counters aot_autograd autograd_cache_saved traced_frame_infos = copy deepcopy torch _dynamo eval_frame dynamo_tls traced_frame_infos torch _dynamo reset torch _dynamo eval_frame clear_dynamo_tls result = fn assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved new_traced_frame_infos = torch _dynamo eval_frame dynamo_tls traced_frame_infos assertEqual result result Dynamo should trace exactly same frames cache hit assertEqual traced_frame_infos new_traced_frame_infos inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True test_autograd_function Tests autograd cache hits fn b sin + b = torch randn requires_grad=True b = torch randn requires_grad=True = detach clone requires_grad_ True b = b detach clone requires_grad_ True compiled_fn = torch compile fn backend= inductor A first call should miss cache assertEqual fn b compiled_fn b fn b sum backward compiled_fn b sum backward assertEqual grad grad assertEqual b grad b grad assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved Reset all tensors = torch randn requires_grad=True b = torch randn requires_grad=True = detach clone requires_grad_ True b = b detach clone requires_grad_ True A second call should hit First reset so in-memory guards don t prevent compilation _clear_dynamo_and_codecache assertEqual fn b compiled_fn b fn b sum backward compiled_fn b sum backward assertEqual grad grad assertEqual b grad b grad assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved largeTensorTest GB device=GPU_TYPE parametrize device GPU_TYPE parametrize dtype torch float torch bfloat inductor_config patch fx_graph_cache True inductor_config patch fx_graph_remote_cache False functorch_config patch enable_autograd_cache True test_autograd_guard_single_entry device dtype Test caching same graph under conditions introduce guards tensor sizes int See test_codecache TestFxGraphCache test_cache_load_with_guards_int _bounds This test particular tests behavior single entry cache If we ever make AOTAutogradCache support multiple entries under same key test should updated device == GPU_TYPE HAS_GPU raise unittest SkipTest f requires GPU_TYPE device == cuda dtype == torch bfloat SM OrLater raise unittest SkipTest requires CUDA SM later fn x y x + x y + y expect_miss compiled_fn b _clear_dynamo_and_codecache counters clear res = compiled_fn b assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_guard_miss assertEqual counters aot_autograd autograd_cache_hit res expect_hit compiled_fn b _clear_dynamo_and_codecache counters clear res = compiled_fn b assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_guard_miss assertEqual counters aot_autograd autograd_cache_hit res expect_guard_miss compiled_fn b _clear_dynamo_and_codecache counters clear res = compiled_fn b assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_guard_miss assertEqual counters aot_autograd autograd_cache_hit res compiled_fn = torch compile fn dynamic=True a_shape = b_shape = = torch rand a_shape device=device dtype=dtype b = torch rand b_shape device=device dtype=dtype res = expect_miss compiled_fn b Same shape should cache hit = detach clone b = b detach clone res = expect_hit compiled_fn b assertEqual res res By changing shape greatly despite same exact input graph inductor should report guard miss leading cache miss our end a_shape = b_shape = = torch rand a_shape device=device dtype=dtype b = torch rand b_shape device=device dtype=dtype expect_guard_miss compiled_fn b Wobble shape bit enough trigger guard miss since still less than int Should result cache hit a_shape = b_shape = = torch rand a_shape device=device dtype=dtype b = torch rand b_shape device=device dtype=dtype expect_hit compiled_fn b Change shape back original FXGraphCache should hit because stores multiple entries a_shape = b_shape = = torch rand a_shape device=device dtype=dtype b = torch rand b_shape device=device dtype=dtype expect_hit compiled_fn b largeTensorTest GB device=GPU_TYPE parametrize device GPU_TYPE parametrize dtype torch float torch bfloat parametrize requires_grad True False inductor_config patch fx_graph_cache True inductor_config patch fx_graph_remote_cache False functorch_config patch enable_autograd_cache True test_autograd_inductor_guards device dtype requires_grad Test caching same graph under conditions introduce guards tensor sizes int See test_codecache TestFxGraphCache test_cache_load_with_guards_int _bounds device == GPU_TYPE HAS_GPU raise unittest SkipTest f requires GPU_TYPE device == cuda dtype == torch bfloat SM OrLater raise unittest SkipTest requires CUDA SM later fn x y x + x y + y compiled_fn = torch compile fn dynamic=True Iterate over different shapes varying whether total size below above int For each combination we expect different guards around whether symbolic sizes do do exceed int shapes = expected_hits = expected_misses = expected_saves = expected_guard_misses = a_shape b_shape shapes = torch rand a_shape device=device dtype=dtype requires_grad=requires_grad b = torch rand b_shape device=device dtype=dtype requires_grad=requires_grad AVOID dynamo reset here We expect guards have been added will violated new shape We should see recompilation along cache miss res = compiled_fn b A first call should miss cache expected_misses += noqa SIM assertEqual counters aot_autograd autograd_cache_miss expected_misses assertEqual counters aot_autograd autograd_cache_guard_miss expected_guard_misses assertEqual counters aot_autograd autograd_cache_hit expected_hits Because dynamic shapes enabled we expect backwards compiled ahead time So we should see cache save here expected_saves += noqa SIM assertEqual counters aot_autograd autograd_cache_saved expected_saves requires_grad res sum backward No extra saves assertEqual counters aot_autograd autograd_cache_saved expected_saves = detach clone requires_grad_ requires_grad b = b detach clone requires_grad_ requires_grad A second call should hit First reset so in-memory guards don t prevent compilation Now clear dynamo we should see cache hit This should populate guards dynamo s cache so subsequent run different shape will still trigger second call autograd_cache _clear_dynamo_and_codecache res = compiled_fn b expected_hits += noqa SIM assertEqual counters aot_autograd autograd_cache_miss expected_misses assertEqual counters aot_autograd autograd_cache_guard_miss expected_guard_misses First compile regular cache miss subsequent guard misses expected_guard_misses += noqa SIM assertEqual counters aot_autograd autograd_cache_hit expected_hits assertEqual counters aot_autograd autograd_cache_saved expected_saves assertEqual res res requires_grad res sum backward assertEqual grad grad inductor_config patch fx_graph_cache True inductor_config patch fx_graph_remote_cache False functorch_config patch enable_autograd_cache True test_nn_module_with_params_global_constant MyMod torch nn Module CONSTANT = torch tensor __init__ - None super __init__ param = torch nn Parameter torch randn forward x x sin + param + MyMod CONSTANT torch no_grad compiled_fn = torch compile MyMod backend= inductor fullgraph=True res = compiled_fn torch ones assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved _clear_dynamo_and_codecache res = compiled_fn torch ones assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved assertEqual res res Edit constant We ll get cache hit should result different result when run because MyMod CONSTANT input graph MyMod CONSTANT = torch tensor _clear_dynamo_and_codecache res = compiled_fn torch ones assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved assertNotEqual res res assertEqual res res sub torch ones inductor_config patch fx_graph_cache True inductor_config patch fx_graph_remote_cache False functorch_config patch enable_autograd_cache True unittest skipIf TEST_MULTIGPU only one GPU detected test_constant_tensor_device_guards Usually when there example inputs device index inputs sufficient make sure we don t cache hit results different cuda devices When input has no arguments we still need have cuda device index cache key torch compile f y = torch tensor device= cuda y torch cuda _DeviceGuard torch cuda set_device result = f assertEqual result device torch device cuda _clear_dynamo_and_codecache torch cuda _DeviceGuard torch cuda set_device result = f assertEqual result device torch device cuda requires_cuda_and_triton inductor_config patch fx_graph_cache True inductor_config patch fx_graph_remote_cache False functorch_config patch enable_autograd_cache True test_multiple_compile_triton_kernels When we cache hit AOTAutogradCache we need still clear CompiledTritonKernels after compiling kernel torch _inductor async_compile CompiledTritonKernels torch compile f x y x sin + y x = torch randn device= cuda y = torch randn device= cuda torch no_grad result = f x y assertEqual result x sin + y assertEqual counters aot_autograd autograd_cache_miss assertEqual len CompiledTritonKernels _cache _clear_dynamo_and_codecache torch no_grad result = f x y assertEqual result x sin + y assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual len CompiledTritonKernels _cache inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True functorch_config patch strict_autograd_cache True test_dynamic_shapes_different_sizes The forward backward function have different symint inputs same underlying symbols fn x y z = x y torch cat x x dim= z x y = torch randn requires_grad=True torch randn compiled_fn = torch compile fn backend= inductor dynamic=True x_compiled _ = compiled_fn x y x_compiled sum backward assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved _clear_dynamo_and_codecache Run second time see cache hit instead erroring x y = torch randn requires_grad=True torch randn x_compiled _ = compiled_fn x y x_compiled sum backward assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved unittest skipIf torch cuda is_available CUDA unavailable unittest skipIf SM OrLater bfloat float inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True functorch_config patch activation_memory_budget functorch_config patch activation_memory_budget_runtime_estimator testing functorch_config patch saved_tensors_hooks_filtering_mode all test_saved_tensors_hooks_autograd_cache ctx = torch autograd graph saved_tensors_hooks device = torch device cuda pack_cpu x x device= cpu unpack_cpu x x device=device pack_cpu x x device= cpu unpack_cpu x x device=device pack_mul x x unpack_mul x x Can use custom AutogradFunction here Cache bypasses AutogradFunction Ctx usage Can save ctx non floating point dtypes For non-symbolic tracing all dtypes devices burned graph fn x x = x + x = x sin cos x = x relu x = x exp x = x x backend = inductor inp_fn x = torch ones device=device requires_grad=True torch _dynamo mark_dynamic x torch _dynamo mark_dynamic x x x = inp_fn fn_compiled = torch compile fn backend=backend fullgraph=True y = fn_compiled x y sum backward assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_saved ctx saved_tensors_hooks_to_gm pack_cpu unpack_cpu symbolic_tracing=False inp_fn=inp_fn pack_cache_hash= cpu_offload unpack_cache_hash= cpu_offload x = inp_fn y = fn_compiled x y sum backward assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_saved ctx saved_tensors_hooks_to_gm pack_cpu unpack_cpu symbolic_tracing=False inp_fn=inp_fn pack_cache_hash= cpu_offload unpack_cache_hash= cpu_offload x = inp_fn y = fn_compiled x y sum backward assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_saved ctx saved_tensors_hooks_to_gm pack_mul unpack_mul symbolic_tracing=False x = inp_fn y = fn_compiled x y sum backward assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_saved unittest skipIf torch cuda is_available CUDA unavailable unittest skipIf SM OrLater bfloat float inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True test_saved_tensors_hooks_autograd_cache_symbolic pack_fp _with_scale x _pack_fp _with_scale_wrap x unpack_fp _with_scale packed _unpack_fp _with_scale_wrap packed ctx = torch autograd graph saved_tensors_hooks fn x x = x + Relu saves bitmask AutogradContext x = x relu x = x relu x device = torch device cuda backend = inductor inp_fn x = torch ones device=device requires_grad=True torch _dynamo mark_dynamic x torch _dynamo mark_dynamic x x x = inp_fn fn_compiled = torch compile fn backend=backend fullgraph=True y = fn_compiled x y sum backward assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_saved ctx saved_tensors_hooks_to_gm pack_fp _with_scale unpack_fp _with_scale fp _with_scale_dtype_floating_point fp _with_scale_dtype_floating_point x = inp_fn y = fn_compiled x y sum backward assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_saved ctx saved_tensors_hooks_to_gm pack_fp _with_scale unpack_fp _with_scale fp _with_scale_dtype_floating_point fp _with_scale_dtype_floating_point x = inp_fn y = fn_compiled x y sum backward assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_saved ctx saved_tensors_hooks_to_gm pack_fp _with_scale unpack_fp _with_scale fp _with_scale_dtype_floating_point_MISS fp _with_scale_dtype_floating_point_MISS x = inp_fn y = fn_compiled x y sum backward assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_miss functorch_config patch enable_autograd_cache True inductor_config patch fx_graph_cache True fx_graph_remote_cache False autotune_local_cache True test_cache_lazy_backward_for_compiled_autograd device = cpu dtype = torch float dynamic = True Verify we can populate hot load functions cache device == GPU_TYPE HAS_GPU raise unittest SkipTest f requires GPU_TYPE device == cuda dtype == torch bfloat SM OrLater raise unittest SkipTest requires SM later fn x y x sin y = torch rand dtype=dtype device=device requires_grad=True b = torch rand dtype=dtype device=device requires_grad=True Record artifacts fresh_cache compiled_fn = torch compile fn dynamic=dynamic A first call should miss cache eager_result = fn b expected_grads = torch autograd grad eager_result sum inputs= b compiled_result = compiled_fn b torch _dynamo compiled_autograd _enable torch compile dynamic=dynamic actual_grads = torch autograd grad compiled_result sum inputs= b hasattr _dynamo_weak_dynamic_indices del _dynamo_weak_dynamic_indices assertEqual eager_result compiled_result assertEqual expected_grads actual_grads assertEqual expected_grads actual_grads functorch_config bundled_autograd_cache assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved assertEqual counters compiled_autograd captures artifacts = torch compiler save_cache_artifacts assertIsNotNone artifacts artifact_bytes cache_info = artifacts autotune_expect = device == GPU_TYPE functorch_config bundled_autograd_cache assertEqual len cache_info inductor_artifacts assertEqual len cache_info inductor_artifacts assertEqual len cache_info autotune_artifacts autotune_expect assertEqual len cache_info aot_autograd_artifacts assertEqual len cache_info pgo_artifacts _clear_all_caches Clean triton kernels shutil rmtree os path join cache_dir triton ignore_errors=True Hot load hit should recompile fresh_cache cache_info = torch compiler load_cache_artifacts artifact_bytes functorch_config bundled_autograd_cache assertEqual len cache_info inductor_artifacts assertEqual len cache_info inductor_artifacts assertEqual len cache_info autotune_artifacts autotune_expect assertEqual len cache_info aot_autograd_artifacts assertEqual len cache_info pgo_artifacts i range counters clear eager_result = fn b expected_grads = torch autograd grad eager_result sum inputs= b compiled_result = compiled_fn b torch _dynamo compiled_autograd _enable torch compile dynamic=dynamic actual_grads = torch autograd grad compiled_result sum inputs= b hasattr _dynamo_weak_dynamic_indices del _dynamo_weak_dynamic_indices assertEqual eager_result compiled_result assertEqual expected_grads actual_grads assertEqual expected_grads actual_grads i == initial compile functorch_config bundled_autograd_cache assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved assertEqual counters compiled_autograd captures no recompiles assertFalse counters inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True functorch_config patch bundled_autograd_cache True test_regional_inductor_basic Basic test regional inductor bundled autograd cache Tests regional inductor compilation results can cached hit torch fx traceback fx_traceback fn x y sin = torch sin x Mark region compiled inductor fx_traceback annotate compile_with_inductor mul = sin y add = mul + torch sin add x = torch randn device= cpu y = torch randn device= cpu Compile regional inductor backend compiled_fn = torch compile fn backend=aot_eager_regional_inductor fullgraph=True First call should miss cache result = compiled_fn x y assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved Second call should hit after clearing dynamo _clear_dynamo_and_codecache result = compiled_fn x y assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved Results should same assertEqual result result inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True functorch_config patch bundled_autograd_cache True test_regional_inductor_with_backward Test regional inductor backward pass bundled autograd cache Note Regional inductor triggers multiple AOT autograd compilations - One outer graph regional inductor backend - One each marked region via standalone_compile torch fx traceback fx_traceback fn x y sin = torch sin x Mark region compiled inductor fx_traceback annotate compile_with_inductor mul = sin y add = mul + torch sin add x = torch randn requires_grad=True y = torch randn requires_grad=True x = x detach clone requires_grad_ True y = y detach clone requires_grad_ True Compile regional inductor backend compiled_fn = torch compile fn backend=aot_eager_regional_inductor fullgraph=True First call AOT autograd compiles outer graph miss Regional inductor then compiles marked region more miss result = compiled_fn x y result sum backward We expect cache misses outer graph + marked region initial_misses = counters aot_autograd autograd_cache_miss initial_saves = counters aot_autograd autograd_cache_saved assertGreater initial_misses assertGreater initial_saves Second call should hit after clearing dynamo _clear_dynamo_and_codecache result = compiled_fn x y result sum backward Should have cache hits now final_hits = counters aot_autograd autograd_cache_hit assertGreater final_hits Cache misses saves should increase assertEqual counters aot_autograd autograd_cache_miss initial_misses assertEqual counters aot_autograd autograd_cache_saved initial_saves Results gradients should same assertEqual result result assertEqual x grad x grad assertEqual y grad y grad inductor_config patch fx_graph_remote_cache False inductor_config patch fx_graph_cache True functorch_config patch enable_autograd_cache True functorch_config patch bundled_autograd_cache True test_regional_inductor_cache_miss_on_change Test changing function causes cache miss regional inductor Regional inductor creates multiple AOT compilations so we track change cache misses rather than absolute counts torch fx traceback fx_traceback fn x y sin = torch sin x fx_traceback annotate compile_with_inductor mul = sin y add = mul + torch sin add fn x y sin = torch sin x fx_traceback annotate compile_with_inductor mul = sin y add = mul + Changed + + torch sin add x = torch randn y = torch randn Compile first function compiled_fn = torch compile fn backend=aot_eager_regional_inductor fullgraph=True result = compiled_fn x y first_misses = counters aot_autograd autograd_cache_miss first_saves = counters aot_autograd autograd_cache_saved assertGreater first_misses assertEqual counters aot_autograd autograd_cache_hit assertGreater first_saves Compile second function different graph _clear_dynamo_and_codecache compiled_fn = torch compile fn backend=aot_eager_regional_inductor fullgraph=True result = compiled_fn x y Should miss because graph different more misses than before assertGreater counters aot_autograd autograd_cache_miss first_misses assertEqual counters aot_autograd autograd_cache_hit assertGreater counters aot_autograd autograd_cache_saved first_saves Results should different assertNotEqual result result functorch_config patch bundled_autograd_cache True AOTAutogradCacheBundledTests AOTAutogradCacheTests pass inductor_config patch fx_graph_cache True AOTAutogradCachePicklerTests torch _dynamo test_case TestCase property device_type - str cuda torch cuda is_available cpu default_config AOTConfig fw_compiler=None bw_compiler=None inference_compiler=None partition_fn=None decompositions= num_params_buffers= aot_id= keep_inference_input_mutations=False dynamic_shapes=True aot_autograd_arg_pos_to_source=None is_export=False no_tangents=False enable_log=False precompile_backend_id=None _get_dynamo_output fn args kwargs Reset dynamo between runs torch _dynamo reset fx_graph = None example_inputs = None compiler gm inputs kwargs nonlocal fx_graph nonlocal example_inputs fx_graph = gm example_inputs = inputs gm g = torch compile fn backend=compiler fullgraph=True result = g args kwargs result fx_graph example_inputs gen_cache_key f config inputs=None inputs None inputs = torch ones _ fx_g example_inputs = _get_dynamo_output f inputs shape_env = ShapeEnv ctx = TracingContext FakeTensorMode shape_env=shape_env Needs shape env FxGraphCache check_can_cache pass Not needed actual key calculation torch _guards tracing ctx sanitize_gm_for_cache fx_g autograd_cache_key fx_g example_inputs config test_basic_hash_key fn x x sin cos config = default_config Check hash stable multiple runs c = gen_cache_key fn config c = gen_cache_key fn config assertEqual c c test_identical_graphs_and_configs fn x x sin cos fn x noqa F y = x sin z = y cos z Make id different otherwise identical config = default_config config = default_config config aot_id = c = gen_cache_key fn config c = gen_cache_key fn config assertEqual c c test_different_graphs fn x x cos sin fn x x sin cos config = default_config c = gen_cache_key fn config c = gen_cache_key fn config assertNotEqual c c test_different_configs fn x x cos sin config = default_config config = default_config config dynamic_shapes = False c = gen_cache_key fn config c = gen_cache_key fn config assertNotEqual c c test_different_inputs fn x x cos sin config = default_config c = gen_cache_key fn config inputs= torch ones c = gen_cache_key fn config inputs= torch ones assertNotEqual c c test_different_global_configs fn x x cos sin config = default_config c = gen_cache_key fn config c = gen_cache_key fn config assertEqual c c c = gen_cache_key fn config Change functorch config functorch_config patch debug_assert functorch_config debug_assert c = gen_cache_key fn config assertNotEqual c c c = gen_cache_key fn config Change inductor config inductor_config patch debug inductor_config debug c = gen_cache_key fn config assertNotEqual c c c = gen_cache_key fn config Change torch grad enabled torch no_grad c = gen_cache_key fn config assertNotEqual c c test_incompatible_function torch _dynamo allow_in_graph AllowInGraphFunc torch autograd Function staticmethod forward _ x torch _dynamo graph_break x sin fn x AllowInGraphFunc apply x config = default_config assertRaises BypassAOTAutogradCache lambda gen_cache_key fn config test_private_namespace TODO anyone who monkeypatches public function into torch namespace allow_in_graph could still break our sanity check cache something bad But s edge case we ll take risk Monkeypatch some random private function into torch see fails torch _dynamo allow_in_graph my_private_fun x x sin patch torch _my_priv new=my_private_fun create=True fn x torch _my_priv x config = default_config assertRaises BypassAOTAutogradCache lambda gen_cache_key fn config torch _inductor config patch freezing True test_freezing fn x x cos sin config = default_config assertRaises BypassAOTAutogradCache lambda gen_cache_key fn config test_private_builtin _foreach_add private torch function s also builtin_function_or_method so should allowed cached since dynamo allows graph fn x b y = x x torch _foreach_add y b config = default_config r = gen_cache_key fn config inputs= torch ones r = gen_cache_key fn config inputs= torch ones assertNotEqual r r test_nn_module_with_params MyMod torch nn Module __init__ - None super __init__ seq = torch nn Parameter torch ones forward x seq + x config = default_config Different inputs parameters all same size c = gen_cache_key MyMod config inputs= torch ones c = gen_cache_key MyMod config inputs= torch ones assertEqual c c test_normal_torch_function torch _dynamo allow_in_graph fn x y = torch sin x z = torch cos x w = y + z w abs w config = default_config gen_cache_key fn config test_safe_torchfunction fn x = x size b = torch Size c = == b x = torch sym_int y = torch sym_float x z = torch sym_int torch sym_sqrt y result = torch sym_sum x y z c result config = default_config gen_cache_key fn config inputs= torch ones test_sanitize_gm_for_cache fn x y = torch sin x z = torch cos x w = y + z w abs w _ fx_g example_inputs = _get_dynamo_output fn torch ones ctx = TracingContext FakeTensorMode shape_env=ShapeEnv torch _guards tracing ctx fx_g meta = foo bar fx_g compile_subgraph_reason = Blah config = default_config sanitize_gm_for_cache fx_g c = autograd_cache_key fx_g example_inputs config c = autograd_cache_key fx_g example_inputs config fx_g meta = foo baz fx_g compile_subgraph_reason = None sanitize_gm_for_cache fx_g c = autograd_cache_key fx_g example_inputs config c = autograd_cache_key fx_g example_inputs config assertEqual c c assertNotEqual c c __name__ == __main__ torch _dynamo test_case run_tests run_tests