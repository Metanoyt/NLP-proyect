Tests performance torch nn MultiheadAttention s fast path BetterTransformer vs slow path torch nn functional multi_head_attention To run script install these dependencies pip install tqdm pip install prettytable argparse itertools json random warnings collections defaultdict OrderedDict pathlib Path pprint pprint typing Optional numpy np prettytable PrettyTable tqdm tqdm torch warnings filterwarnings ignore error_dict = defaultdict int benchmark_torch_function iters f args kwargs f args kwargs f args kwargs torch cuda synchronize start_event = torch cuda Event enable_timing=True end_event = torch cuda Event enable_timing=True start_event record _ range iters f args kwargs end_event record torch cuda synchronize elapsed_time has resolution microseconds returns milliseconds so we need multiply increase resolution start_event elapsed_time end_event iters f args kwargs run int b int iters int batch_size int sequence_length int embed_dim int num_heads int device str dtype str block_size int seed random seed seed torch manual_seed seed np random seed seed scipy stats beta lengths = beta rvs b size=batch_size sequence_length + block_size - block_size lengths = list map int list lengths lengths = l block_size l lengths lengths = max l block_size l lengths Used enforce no padding lengths = sequence_length batch_size Ensure one row batch ele has max_sequence_length lengths random randint batch_size - = sequence_length q = torch randn l embed_dim device=device dtype=dtype l lengths q = torch nested nested_tensor q device=device dtype=dtype k v = q q qkv = torch nn Linear embed_dim embed_dim device=device dtype=dtype proj = torch nn Linear embed_dim embed_dim device=device dtype=dtype native_mha = torch nn MultiheadAttention embed_dim num_heads batch_first=True device=device dtype=dtype eval native_mha in_proj_weight = qkv weight native_mha in_proj_bias = qkv bias native_mha out_proj weight = proj weight native_mha out_proj bias = proj bias Create query mask q_mask = torch nested to_padded_tensor torch nested nested_tensor torch tensor True length dtype=torch bool length lengths q_mask = q_mask cuda q_mask size == None Benchmark native MHA core torch backends cuda sdp_kernel enable_math=False enable_flash=True torch inference_mode time_native_mha_fast y_native_mha_fast _ = benchmark_torch_function iters native_mha q k v need_weights=False q = q to_padded_tensor k = q v = q Internal Flash Attention time_native_mha_slow y_native_mha_slow _ = benchmark_torch_function iters native_mha q k v key_padding_mask=~q_mask need_weights=False Convert padded comparison y_native_mha_fast is_nested y_native_mha_fast = torch nested to_padded_tensor y_native_mha_fast y_native_mha_fast = y_native_mha_fast q_mask unsqueeze - y_native_mha_slow is_nested y_native_mha_slow = torch nested to_padded_tensor y_native_mha_slow y_native_mha_slow = y_native_mha_slow q_mask unsqueeze - Correctness check entry_name = f batch batch_size _seq_len sequence_length _n_heads num_heads _embed_dim embed_dim try torch testing assert_close y_native_mha_fast y_native_mha_slow atol= e- rtol= e- except AssertionError error_dict entry_name += pprint error_dict Calculate amount padding padding = - q_mask float mean item Calculate speedup flash attention speedup_fast_internal = time_native_mha_slow time_native_mha_fast result_entry = OrderedDict result_entry dtype = dtype result_entry batch_size = batch_size result_entry sequence_length = sequence_length result_entry n_heads = num_heads result_entry embed_dim = embed_dim result_entry time_native_mha_slow \u b s = f time_native_mha_slow f result_entry time_native_mha_fast \u b s = f time_native_mha_fast f result_entry speedup flash_mha v native_mha = f speedup_fast_internal f result_entry padding = f padding f result_entry main save_path Optional Path error_path Optional Path table = PrettyTable entries = defaultdict list print CUDA device torch cuda get_device_name iters = header = None batch_sizes = sequence_lengths = embed_dims = num_heads_list = betas = range batch_size sequence_length embed_dim num_heads block_size b tqdm list itertools product batch_sizes sequence_lengths embed_dims num_heads_list betas seed = Magic number works well higher b values entry = run b iters batch_size sequence_length embed_dim num_heads cuda torch float block_size seed entry None continue header None table field_names = list entry keys header = list entry keys row = k v entry items row append v entries k append v table add_row row Print full table console print table pprint error_dict csv_string = table get_csv_string save_path None open save_path w csvfile csvfile write csv_string print f Total errors sum error_dict values error_path None open error_path w file file write json dumps error_dict __name__ == __main__ parser = argparse ArgumentParser parser add_argument -- save-path -- save_path type=str help= Path save results parser add_argument -- error-save-path -- error_save_path type=str help= Path save errors args = parser parse_args save_path = Path args save_path args save_path None error_path = Path args error_save_path args error_save_path None main save_path error_path