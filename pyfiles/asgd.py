mypy allow-untyped-defs typing cast Optional Union torch torch Tensor optimizer _capturable_doc _default_to_fused_or_foreach _differentiable_doc _disable_dynamo_if_unsupported _foreach_doc _get_capturable_supported_devices _get_scalar_dtype _get_value _maximize_doc _params_doc _to_scalar _use_grad_for_differentiable _view_as_real Optimizer ParamsT __all__ = ASGD asgd ASGD Optimizer __init__ params ParamsT lr Union float Tensor = e- lambd float = e- alpha float = t float = e weight_decay float = foreach Optional bool = None maximize bool = False differentiable bool = False capturable bool = False isinstance lr Tensor lr numel = raise ValueError Tensor lr must -element = lr raise ValueError f Invalid learning rate lr = weight_decay raise ValueError f Invalid weight_decay value weight_decay defaults = lr lr lambd lambd alpha alpha t t weight_decay weight_decay foreach foreach maximize maximize differentiable differentiable capturable capturable super __init__ params defaults __setstate__ state super __setstate__ state group param_groups group setdefault foreach None group setdefault maximize False group setdefault differentiable False group setdefault capturable False p group params p_state = state get p len p_state = torch is_tensor p_state step step_val = float p_state step p_state step = torch tensor step_val dtype=_get_scalar_dtype device=p device torch is_tensor p_state eta p_state eta = torch tensor p_state eta dtype=_get_scalar_dtype device=p device torch is_tensor p_state mu p_state mu = torch tensor p_state mu dtype=_get_scalar_dtype device=p device _init_group group params_with_grad grads mus axs etas state_steps has_complex = False p group params p grad None has_complex &#124; = torch is_complex p params_with_grad append p p grad is_sparse raise RuntimeError ASGD does support sparse gradients grads append p grad state = state p State initialization len state == state step = torch zeros device=p device dtype=_get_scalar_dtype state eta = torch as_tensor _to_scalar group lr device=p device dtype=_get_scalar_dtype clone detach state mu = torch ones device=p device dtype=_get_scalar_dtype state ax = torch zeros_like p memory_format=torch preserve_format mus append state mu axs append state ax etas append state eta state_steps append state step has_complex _use_grad_for_differentiable step closure=None Perform single optimization step Args closure Callable optional A closure reevaluates model returns loss _cuda_graph_capture_health_check loss = None closure None torch enable_grad loss = closure group param_groups params_with_grad list Tensor = grads list Tensor = mus list Tensor = axs list Tensor = etas list Tensor = state_steps list Tensor = has_complex = _init_group group params_with_grad grads mus axs etas state_steps asgd params_with_grad grads axs mus etas state_steps lambd=group lambd lr=group lr t =group t alpha=group alpha weight_decay=group weight_decay foreach=group foreach maximize=group maximize differentiable=group differentiable capturable=group capturable has_complex=has_complex loss ASGD __doc__ = rf Implements Averaged Stochastic Gradient Descent It has been proposed ` Acceleration stochastic approximation averaging ` _ Args _params_doc lr float Tensor optional learning rate default e- lambd float optional decay term default e- alpha float optional power eta update default t float optional point which start averaging default e weight_decay float optional weight decay L penalty default _foreach_doc _maximize_doc _differentiable_doc _capturable_doc _Acceleration stochastic approximation averaging https meyn ece ufl edu wp-content uploads sites archive spm_files Courses ECE - media poljud pdf _single_tensor_asgd params list Tensor grads list Tensor axs list Tensor mus list Tensor etas list Tensor state_steps list Tensor lambd float lr float t float alpha float weight_decay float maximize bool differentiable bool capturable bool has_complex bool torch jit is_scripting lr = _to_scalar lr i param enumerate params grad = grads i grad = grad maximize -grad mu = mus i ax = axs i eta = etas i step_t = state_steps i If compiling compiler will handle cudagraph checks see note torch compile x capturable torch compiler is_compiling capturable capturable_supported_devices = _get_capturable_supported_devices param device type == mu device type == eta device type == step_t device type param device type capturable_supported_devices raise AssertionError f If capturable=True params mus etas state_steps must f supported devices capturable_supported_devices torch is_complex param grad = torch view_as_real grad param = torch view_as_real param ax = torch view_as_real ax update step step_t += weight_decay = grad = grad add param alpha=weight_decay capturable param mul_ - lambd eta param addcmul_ grad eta value=- update parameter eta_value = _get_value eta param mul_ - lambd eta_value decay term param add_ grad alpha=-eta_value update parameter averaging capturable mu item = ax add_ param sub ax mul_ mu ax copy_ param capturable pyrefly ignore unsupported-operation eta copy_ lr + lambd lr step_t alpha mu copy_ torch maximum step_t - t torch ones_like step_t step = _get_value step_t new_eta = torch as_tensor lr + lambd lr step alpha eta copy_ new_eta new_mu = torch as_tensor max step - t mu copy_ new_mu _multi_tensor_asgd params list Tensor grads list Tensor axs list Tensor mus list Tensor etas list Tensor state_steps list Tensor lambd float lr float t float alpha float weight_decay float maximize bool differentiable bool capturable bool has_complex bool len params == differentiable raise AssertionError _foreach ops don t support autograd If compiling compiler will handle cudagraph checks see note torch compile x capturable torch compiler is_compiling capturable capturable_supported_devices = _get_capturable_supported_devices supports_xla=False all p device type == mu device type == eta device type == step device type p device type capturable_supported_devices p mu eta step zip params mus etas state_steps strict=True raise AssertionError f If capturable=True params mus etas state_steps must f supported devices capturable_supported_devices lr = _to_scalar lr grouped_tensors = Optimizer _group_tensors_by_device_and_dtype params grads axs mus etas state_steps type ignore list-item device _ grouped_params_ grouped_grads_ grouped_axs_ grouped_mus_ grouped_etas_ grouped_state_steps_ _ grouped_tensors items grouped_params = cast list Tensor grouped_params_ grouped_grads = cast list Tensor grouped_grads_ grouped_axs = cast list Tensor grouped_axs_ grouped_mus = cast list Tensor grouped_mus_ grouped_etas = cast list Tensor grouped_etas_ grouped_state_steps = cast list Tensor grouped_state_steps_ has_complex _view_as_real grouped_params grouped_grads grouped_axs maximize grouped_grads = torch _foreach_neg grouped_grads type ignore assignment Update steps If steps CPU foreach will fall back slow path which for-loop calling t add over over will then wrapped into Tensor over over again which slower than we just wrapped once now The alpha required assure we go right overload torch compiler is_compiling grouped_state_steps is_cpu torch _foreach_add_ grouped_state_steps torch tensor device= cpu alpha= torch _foreach_add_ grouped_state_steps intermediate = grad + param lambd intermediate Union tuple Tensor list Tensor weight_decay = maximize torch _foreach_add_ grouped_grads grouped_params alpha=weight_decay intermediate = grouped_grads intermediate = torch _foreach_add grouped_grads grouped_params alpha=weight_decay torch _foreach_add_ intermediate grouped_params alpha=lambd intermediate = torch _foreach_add grouped_grads grouped_params alpha=lambd update param param - lambd eta - eta grad = param - param lambd eta - eta grad = param - eta intermediate torch _foreach_addcmul_ grouped_params intermediate grouped_etas value=- del intermediate update grouped_axs averaging ax = ax + mu param - ax Note mlazos We can t use lerp here since requires weight float our grouping code requires dtypes match all tensors group should since we use mus other places all dtypes need match so we could introduce cast loop since only adds one additional kernel launch looks like cleaner faster solution intermediate = torch _foreach_sub grouped_params grouped_axs torch _foreach_addcmul_ grouped_axs intermediate grouped_mus del intermediate new_etas Union tuple Tensor list Tensor new_mus Union tuple Tensor list Tensor capturable update grouped_mus new_mus = torch _foreach_sub grouped_state_steps t torch _foreach_maximum_ new_mus torch _foreach_reciprocal_ new_mus torch _foreach_copy_ grouped_mus new_mus del new_mus update eta = lr + lambd lr step ^alpha new_etas = torch _foreach_mul grouped_state_steps lambd torch _foreach_mul_ new_etas lr torch _foreach_add_ new_etas torch _foreach_pow_ new_etas alpha torch _foreach_reciprocal_ new_etas torch _foreach_mul_ new_etas lr torch _foreach_copy_ grouped_etas new_etas new_etas = torch as_tensor lr + lambd lr step alpha device=device step grouped_state_steps new_mus = torch as_tensor max _get_value step - t device=device step grouped_state_steps torch _foreach_copy_ grouped_etas new_etas torch _foreach_copy_ grouped_mus new_mus _disable_dynamo_if_unsupported single_tensor_fn=_single_tensor_asgd asgd params list Tensor grads list Tensor axs list Tensor mus list Tensor etas list Tensor state_steps list Tensor kwonly args defaults supported functions compiled torchscript issue setting kwarg now functional API compiled torch distributed optim foreach Optional bool = None maximize bool = False differentiable bool = False capturable bool = False has_complex bool = False lambd float lr float t float alpha float weight_decay float r Functional API performs asgd algorithm computation See ` ~torch optim ASGD ` details foreach None _ foreach = _default_to_fused_or_foreach params differentiable use_fused=False foreach torch jit is_scripting raise RuntimeError torch jit script supported foreach optimizers foreach torch jit is_scripting func = _multi_tensor_asgd func = _single_tensor_asgd func params grads axs mus etas state_steps lambd=lambd lr=lr t =t alpha=alpha weight_decay=weight_decay maximize=maximize differentiable=differentiable capturable=capturable has_complex=has_complex