mypy allow-untyped-defs functools inspect itertools warnings weakref collections namedtuple OrderedDict collections abc Callable Iterator Mapping typing Any Optional overload TypeVar Union typing_extensions Self torch torch device dtype Tensor torch _prims_common DeviceLikeType torch nn parameter Buffer Parameter torch utils _python_dispatch is_traceable_wrapper_subclass torch utils hooks BackwardHook RemovableHandle __all__ = register_module_forward_pre_hook register_module_forward_hook register_module_full_backward_pre_hook register_module_backward_hook register_module_full_backward_hook register_module_buffer_registration_hook register_module_module_registration_hook register_module_parameter_registration_hook Module _grad_t = Union tuple Tensor Tensor See https mypy readthedocs io en latest generics html#generic-methods-and-generic-self use ` T ` annotate ` ` Many methods ` Module ` ` ` we want those values type subclass looser type ` Module ` T = TypeVar T bound= Module _IncompatibleKeys pyrefly ignore invalid-inheritance namedtuple IncompatibleKeys missing_keys unexpected_keys __slots__ = __repr__ - str pyrefly ignore missing-attribute missing_keys unexpected_keys All keys matched successfully super __repr__ __str__ = __repr__ _addindent s_ numSpaces s = s_ split \n don t do anything single-line stuff len s == s_ first = s pop s = numSpaces + line line s s = \n join s s = first + \n + s s r This tracks hooks common all modules executed immediately before registering buffer module parameter _global_buffer_registration_hooks dict int Callable = OrderedDict _global_module_registration_hooks dict int Callable = OrderedDict _global_parameter_registration_hooks dict int Callable = OrderedDict _WrappedHook __init__ hook Callable module Optional Module = None - None hook Callable = hook functools update_wrapper hook with_module bool = False module None module weakref ReferenceType Module = weakref ref module with_module = True __call__ args Any kwargs Any - Any with_module module = module module None raise RuntimeError You trying call hook dead Module hook module args kwargs hook args kwargs __getstate__ - dict result = hook hook with_module with_module with_module pyrefly ignore unsupported-operation result module = module result __setstate__ state dict hook = state hook with_module = state with_module with_module state module None raise RuntimeError You trying revive hook dead Module module = weakref ref state module r This tracks hooks common all modules executed before after calling forward backward This global state used debugging profiling purposes _global_backward_pre_hooks dict int Callable = OrderedDict _global_backward_hooks dict int Callable = OrderedDict _global_is_full_backward_hook Optional bool = None _global_forward_pre_hooks dict int Callable = OrderedDict _global_forward_hooks dict int Callable = OrderedDict _global_forward_hooks_always_called dict int bool = OrderedDict _global_forward_hooks_with_kwargs dict int bool = OrderedDict _has_any_global_hook _global_backward_pre_hooks _global_backward_hooks _global_forward_pre_hooks _global_forward_hooks _global_forward_hooks_always_called _global_forward_hooks_with_kwargs _EXTRA_STATE_KEY_SUFFIX = _extra_state register_module_buffer_registration_hook hook Callable None - RemovableHandle r Register buffer registration hook common all modules warning This adds global state ` nn Module ` module The hook will called every time func ` register_buffer ` invoked It should have following signature hook module name buffer - None new buffer The hook can modify input single modified value hook Returns ` torch utils hooks RemovableHandle ` handle can used remove added hook calling ` ` handle remove ` ` handle = RemovableHandle _global_buffer_registration_hooks _global_buffer_registration_hooks handle id = hook handle register_module_module_registration_hook hook Callable None - RemovableHandle r Register module registration hook common all modules warning This adds global state ` nn Module ` module The hook will called every time func ` register_module ` invoked It should have following signature hook module name submodule - None new submodule The hook can modify input single modified value hook Returns ` torch utils hooks RemovableHandle ` handle can used remove added hook calling ` ` handle remove ` ` handle = RemovableHandle _global_module_registration_hooks _global_module_registration_hooks handle id = hook handle register_module_parameter_registration_hook hook Callable None - RemovableHandle r Register parameter registration hook common all modules warning This adds global state ` nn Module ` module The hook will called every time func ` register_parameter ` invoked It should have following signature hook module name param - None new parameter The hook can modify input single modified value hook Returns ` torch utils hooks RemovableHandle ` handle can used remove added hook calling ` ` handle remove ` ` handle = RemovableHandle _global_parameter_registration_hooks _global_parameter_registration_hooks handle id = hook handle register_module_forward_pre_hook hook Callable None - RemovableHandle r Register forward pre-hook common all modules warning This adds global state ` nn module ` module only intended debugging profiling purposes The hook will called every time before func ` forward ` invoked It should have following signature hook module input - None modified input The input contains only positional arguments given module Keyword arguments won t passed hooks only ` ` forward ` ` The hook can modify input User can either tuple single modified value hook We will wrap value into tuple single value returned unless value already tuple This hook has precedence over specific module hooks registered ` ` register_forward_pre_hook ` ` Returns ` torch utils hooks RemovableHandle ` handle can used remove added hook calling ` ` handle remove ` ` handle = RemovableHandle _global_forward_pre_hooks _global_forward_pre_hooks handle id = hook handle register_module_forward_hook hook Callable None with_kwargs bool = False always_call bool = False - RemovableHandle r Register global forward hook all modules warning This adds global state ` nn module ` module only intended debugging profiling purposes The hook will called every time after func ` forward ` has computed output It should have following signature hook module input output - None modified output The input contains only positional arguments given module Keyword arguments won t passed hooks only ` ` forward ` ` You can optionally modify output module returning new value will replace output func ` forward ` function Parameters hook Callable The user defined hook registered always_call bool If ` ` True ` ` ` ` hook ` ` will run regardless whether exception raised while calling Module Default ` ` False ` ` Returns ` torch utils hooks RemovableHandle ` handle can used remove added hook calling ` ` handle remove ` ` This hook will executed before specific module hooks registered ` ` register_forward_hook ` ` handle = RemovableHandle _global_forward_hooks extra_dict=_global_forward_hooks_always_called _global_forward_hooks handle id = hook with_kwargs _global_forward_hooks_with_kwargs handle id = True always_call _global_forward_hooks_always_called handle id = True handle register_module_backward_hook hook Callable Module _grad_t _grad_t Union None _grad_t - RemovableHandle r Register backward hook common all modules This function deprecated favor func ` torch nn modules module register_module_full_backward_hook ` behavior function will change future versions Returns ` torch utils hooks RemovableHandle ` handle can used remove added hook calling ` ` handle remove ` ` global _global_is_full_backward_hook _global_is_full_backward_hook True raise RuntimeError Cannot use both regular backward hooks full backward hooks global Module hook Please use only one them _global_is_full_backward_hook = False handle = RemovableHandle _global_backward_hooks _global_backward_hooks handle id = hook handle register_module_full_backward_pre_hook hook Callable Module _grad_t Union None _grad_t - RemovableHandle r Register backward pre-hook common all modules warning This adds global state ` nn module ` module only intended debugging profiling purposes Hooks registered using function behave same way those registered meth ` torch nn Module register_full_backward_pre_hook ` Refer its documentation more details Hooks registered using function will called before hooks registered using meth ` torch nn Module register_full_backward_pre_hook ` Returns ` torch utils hooks RemovableHandle ` handle can used remove added hook calling ` ` handle remove ` ` handle = RemovableHandle _global_backward_pre_hooks _global_backward_pre_hooks handle id = hook handle register_module_full_backward_hook hook Callable Module _grad_t _grad_t Union None _grad_t - RemovableHandle r Register backward hook common all modules warning This adds global state ` nn module ` module only intended debugging profiling purposes Hooks registered using function behave same way those registered meth ` torch nn Module register_full_backward_hook ` Refer its documentation more details Hooks registered using function will called before hooks registered using meth ` torch nn Module register_full_backward_hook ` Returns ` torch utils hooks RemovableHandle ` handle can used remove added hook calling ` ` handle remove ` ` global _global_is_full_backward_hook _global_is_full_backward_hook False raise RuntimeError Cannot use both regular backward hooks full backward hooks global Module hook Please use only one them _global_is_full_backward_hook = True handle = RemovableHandle _global_backward_hooks _global_backward_hooks handle id = hook handle Trick mypy into applying contravariance rules inputs defining forward value rather than function See also https github com python mypy issues _forward_unimplemented input Any - None r Define computation performed every call Should overridden all subclasses note Although recipe forward pass needs defined within function one should call ` Module ` instance afterwards instead since former takes care running registered hooks while latter silently ignores them raise NotImplementedError f Module type __name__ missing required forward function Module r Base all neural network modules Your models should also subclass Modules can also contain other Modules allowing them nested tree structure You can assign submodules regular attributes torch nn nn torch nn functional F Model nn Module __init__ - None super __init__ conv = nn Conv d conv = nn Conv d forward x x = F relu conv x F relu conv x Submodules assigned way will registered will also have their parameters converted when you call meth ` ` etc note As per example above ` ` __init__ ` ` call parent must made before assignment child ivar training Boolean represents whether module training evaluation mode vartype training bool dump_patches bool = False _version int = r This allows better BC support meth ` load_state_dict ` In meth ` state_dict ` version number will saved attribute ` _metadata ` returned state dict thus pickled ` _metadata ` dictionary keys follow naming convention state dict See ` ` _load_from_state_dict ` ` how use information loading If new parameters buffers added removed module number shall bumped module s ` _load_from_state_dict ` method can compare version number do appropriate changes state dict before change training bool _parameters dict str Optional Parameter _buffers dict str Optional Tensor _non_persistent_buffers_set set str _backward_pre_hooks dict int Callable _backward_hooks dict int Callable _is_full_backward_hook Optional bool _forward_hooks dict int Callable Marks whether corresponding _forward_hooks accept kwargs As JIT does support set int dict used set where all hooks represented dict accept kwargs _forward_hooks_with_kwargs dict int bool forward hooks should always called even exception raised _forward_hooks_always_called dict int bool _forward_pre_hooks dict int Callable Marks whether corresponding _forward_hooks accept kwargs As JIT does support set int dict used set where all hooks represented dict accept kwargs _forward_pre_hooks_with_kwargs dict int bool _state_dict_hooks dict int Callable _load_state_dict_pre_hooks dict int Callable _state_dict_pre_hooks dict int Callable _load_state_dict_post_hooks dict int Callable _modules dict str Optional Module call_super_init bool = False _compiled_call_impl Optional Callable = None __init__ args Any kwargs Any - None Initialize internal Module state shared both nn Module ScriptModule torch _C _log_api_usage_once python nn_module Backward compatibility no args used allowed when call_super_init=False call_super_init False bool kwargs raise TypeError f type __name__ __init__ got unexpected keyword argument next iter kwargs call_super_init False bool args raise TypeError f type __name__ __init__ takes positional argument len args + given Calls super __setattr__ instead typical = avoid Module __setattr__ overhead Module s __setattr__ has special handling parameters submodules buffers simply calls into super __setattr__ all other attributes super __setattr__ training True super __setattr__ _parameters super __setattr__ _buffers super __setattr__ _non_persistent_buffers_set set super __setattr__ _backward_pre_hooks OrderedDict super __setattr__ _backward_hooks OrderedDict super __setattr__ _is_full_backward_hook None super __setattr__ _forward_hooks OrderedDict super __setattr__ _forward_hooks_with_kwargs OrderedDict super __setattr__ _forward_hooks_always_called OrderedDict super __setattr__ _forward_pre_hooks OrderedDict super __setattr__ _forward_pre_hooks_with_kwargs OrderedDict super __setattr__ _state_dict_hooks OrderedDict super __setattr__ _state_dict_pre_hooks OrderedDict super __setattr__ _load_state_dict_pre_hooks OrderedDict super __setattr__ _load_state_dict_post_hooks OrderedDict super __setattr__ _modules call_super_init super __init__ args kwargs forward Callable Any = _forward_unimplemented register_buffer name str tensor Optional Tensor persistent bool = True - None r Add buffer module This typically used register buffer should considered model parameter For example BatchNorm s ` ` running_mean ` ` parameter part module s state Buffers default persistent will saved alongside parameters This behavior can changed setting attr ` persistent ` ` ` False ` ` The only difference between persistent buffer non-persistent buffer latter will part module s attr ` state_dict ` Buffers can accessed attributes using given names Args name str name buffer The buffer can accessed module using given name tensor Tensor None buffer registered If ` ` None ` ` then operations run buffers such attr ` cuda ` ignored If ` ` None ` ` buffer included module s attr ` state_dict ` persistent bool whether buffer part module s attr ` state_dict ` Example xdoctest +SKIP undefined vars register_buffer running_mean torch zeros num_features persistent False isinstance torch jit ScriptModule raise RuntimeError ScriptModule does support non-persistent buffers _buffers __dict__ raise AttributeError cannot assign buffer before Module __init__ call isinstance name str raise TypeError f buffer name should string Got torch typename name name raise KeyError buffer name can\ t contain name == raise KeyError buffer name can\ t empty string hasattr name name _buffers raise KeyError f attribute name already exists tensor None isinstance tensor torch Tensor hasattr tensor __torch_function__ raise TypeError f cannot assign torch typename tensor object buffer name torch Tensor None required hook _global_buffer_registration_hooks values output = hook name tensor output None tensor = output _buffers name = tensor persistent _non_persistent_buffers_set discard name _non_persistent_buffers_set add name register_parameter name str param Optional Parameter - None r Add parameter module The parameter can accessed attribute using given name Args name str name parameter The parameter can accessed module using given name param Parameter None parameter added module If ` ` None ` ` then operations run parameters such attr ` cuda ` ignored If ` ` None ` ` parameter included module s attr ` state_dict ` _parameters __dict__ raise AttributeError cannot assign parameter before Module __init__ call isinstance name str raise TypeError f parameter name should string Got torch typename name name raise KeyError parameter name can\ t contain name == raise KeyError parameter name can\ t empty string hasattr name name _parameters raise KeyError f attribute name already exists param None _parameters name = None isinstance param Parameter raise TypeError f cannot assign torch typename param object parameter name torch nn Parameter None required param grad_fn raise ValueError f Cannot assign non-leaf Tensor parameter name Model f parameters must created explicitly To express name function another Tensor compute value forward method hook _global_parameter_registration_hooks values output = hook name param output None param = output _parameters name = param add_module name str module Optional Module - None r Add child module current module The module can accessed attribute using given name Args name str name child module The child module can accessed module using given name module Module child module added module isinstance module Module module None raise TypeError f torch typename module Module subclass isinstance name str raise TypeError f module name should string Got torch typename name hasattr name name _modules raise KeyError f attribute name already exists name raise KeyError f module name can\ t contain got name name == raise KeyError module name can\ t empty string hook _global_module_registration_hooks values output = hook name module output None module = output _modules name = module register_module name str module Optional Module - None r Alias func ` add_module ` add_module name module get_submodule target str - Module Return submodule given ` ` target ` ` exists otherwise throw error For example let s say you have ` ` nn Module ` ` ` ` A ` ` looks like code-block text A net_b Module net_c Module conv Conv d kernel_size= stride= linear Linear in_features= out_features= bias=True The diagram shows ` ` nn Module ` ` ` ` A ` ` ` ` A ` ` which has nested submodule ` ` net_b ` ` which itself has two submodules ` ` net_c ` ` ` ` linear ` ` ` ` net_c ` ` then has submodule ` ` conv ` ` To check whether we have ` ` linear ` ` submodule we would call ` ` get_submodule net_b linear ` ` To check whether we have ` ` conv ` ` submodule we would call ` ` get_submodule net_b net_c conv ` ` The runtime ` ` get_submodule ` ` bounded degree module nesting ` ` target ` ` A query against ` ` named_modules ` ` achieves same result O N number transitive modules So simple check see some submodule exists ` ` get_submodule ` ` should always used Args target The fully-qualified string name submodule look See above example how specify fully-qualified string Returns torch nn Module The submodule referenced ` ` target ` ` Raises AttributeError If any point along path resulting target string sub path resolves non-existent attribute name object instance ` ` nn Module ` ` target == atoms list str = target split mod torch nn Module = item atoms hasattr mod item raise AttributeError mod _get_name + has no attribute ` + item + ` mod = getattr mod item isinstance mod torch nn Module raise AttributeError ` + item + ` nn Module mod set_submodule target str module Module strict bool = False - None Set submodule given ` ` target ` ` exists otherwise throw error note If ` ` strict ` ` set ` ` False ` ` default method will replace existing submodule create new submodule parent module exists If ` ` strict ` ` set ` ` True ` ` method will only attempt replace existing submodule throw error submodule does exist For example let s say you have ` ` nn Module ` ` ` ` A ` ` looks like code-block text A net_b Module net_c Module conv Conv d linear Linear The diagram shows ` ` nn Module ` ` ` ` A ` ` ` ` A ` ` has nested submodule ` ` net_b ` ` which itself has two submodules ` ` net_c ` ` ` ` linear ` ` ` ` net_c ` ` then has submodule ` ` conv ` ` To override ` ` Conv d ` ` new submodule ` ` Linear ` ` you could call ` ` set_submodule net_b net_c conv nn Linear ` ` where ` ` strict ` ` could ` ` True ` ` ` ` False ` ` To add new submodule ` ` Conv d ` ` existing ` ` net_b ` ` module you would call ` ` set_submodule net_b conv nn Conv d ` ` In above you set ` ` strict=True ` ` call ` ` set_submodule net_b conv nn Conv d strict=True ` ` AttributeError will raised because ` ` net_b ` ` does have submodule named ` ` conv ` ` Args target The fully-qualified string name submodule look See above example how specify fully-qualified string module The module set submodule strict If ` ` False ` ` method will replace existing submodule create new submodule parent module exists If ` ` True ` ` method will only attempt replace existing submodule throw error submodule doesn t already exist Raises ValueError If ` ` target ` ` string empty ` ` module ` ` instance ` ` nn Module ` ` AttributeError If any point along path resulting ` ` target ` ` string sub path resolves non-existent attribute name object instance ` ` nn Module ` ` target == raise ValueError Cannot set submodule without target name atoms list str = target split isinstance module torch nn Module raise ValueError ` + module + f ` nn Module found type module len atoms == parent torch nn Module = parent_key = join atoms - parent = get_submodule parent_key strict hasattr parent atoms - raise AttributeError parent _get_name + has no attribute ` + atoms - + ` hasattr parent atoms - mod = getattr parent atoms - isinstance mod torch nn Module raise AttributeError ` + atoms - + ` nn Module setattr parent atoms - module get_parameter target str - Parameter Return parameter given ` ` target ` ` exists otherwise throw error See docstring ` ` get_submodule ` ` more detailed explanation method s functionality well how correctly specify ` ` target ` ` Args target The fully-qualified string name Parameter look See ` ` get_submodule ` ` how specify fully-qualified string Returns torch nn Parameter The Parameter referenced ` ` target ` ` Raises AttributeError If target string references invalid path resolves something ` ` nn Parameter ` ` module_path _ param_name = target rpartition mod torch nn Module = get_submodule module_path hasattr mod param_name raise AttributeError mod _get_name + has no attribute ` + param_name + ` param torch nn Parameter = getattr mod param_name isinstance param torch nn Parameter raise AttributeError ` + param_name + ` nn Parameter param get_buffer target str - Tensor Return buffer given ` ` target ` ` exists otherwise throw error See docstring ` ` get_submodule ` ` more detailed explanation method s functionality well how correctly specify ` ` target ` ` Args target The fully-qualified string name buffer look See ` ` get_submodule ` ` how specify fully-qualified string Returns torch Tensor The buffer referenced ` ` target ` ` Raises AttributeError If target string references invalid path resolves something buffer module_path _ buffer_name = target rpartition mod torch nn Module = get_submodule module_path hasattr mod buffer_name raise AttributeError mod _get_name + has no attribute ` + buffer_name + ` buffer torch Tensor = getattr mod buffer_name buffer_name mod _buffers raise AttributeError ` + buffer_name + ` buffer buffer get_extra_state - Any Return any extra state include module s state_dict Implement corresponding func ` set_extra_state ` your module you need store extra state This function called when building module s ` state_dict ` Note extra state should picklable ensure working serialization state_dict We only provide backwards compatibility guarantees serializing Tensors other objects may break backwards compatibility their serialized pickled form changes Returns object Any extra state store module s state_dict raise RuntimeError Reached code path Module get_extra_state should never called Please file issue https github com pytorch pytorch issues new template=bug-report yml report bug set_extra_state state Any - None Set extra state contained loaded ` state_dict ` This function called func ` load_state_dict ` handle any extra state found within ` state_dict ` Implement function corresponding func ` get_extra_state ` your module you need store extra state within its ` state_dict ` Args state dict Extra state ` state_dict ` raise RuntimeError Reached code path Module set_extra_state should never called Please file issue https github com pytorch pytorch issues new template=bug-report yml report bug _apply fn recurse=True recurse module children module _apply fn torch _subclasses fake_tensor FakeTensor compute_should_use_set_data tensor tensor_applied - bool torch _has_compatible_shallow_copy_type tensor tensor_applied isinstance tensor_applied FakeTensor If new tensor has compatible tensor type existing tensor current behavior change tensor in-place using ` data = ` future behavior overwrite existing tensor However changing current behavior BC-breaking change we want happen future releases So now we introduce ` torch __future__ get_overwrite_module_params_on_conversion ` global flag let user control whether they want future behavior overwriting existing tensor torch __future__ get_overwrite_module_params_on_conversion False should_use_swap_tensors = torch __future__ get_swap_module_params_on_conversion key param _parameters items param None continue Tensors stored modules graph leaves we don t want track autograd history ` param_applied ` so we have use ` torch no_grad ` torch no_grad param_applied = fn param p_should_use_set_data = compute_should_use_set_data param param_applied subclasses may have multiple child tensors so we need use swap_tensors p_should_use_swap_tensors = should_use_swap_tensors is_traceable_wrapper_subclass param_applied isinstance param FakeTensor param_grad = param grad p_should_use_swap_tensors try param_grad None Accessing param grad makes its Tensor s use_count which will prevent swapping Decrement use count gradient setting None param grad = None param_applied = torch nn Parameter pyrefly ignore bad-argument-type param_applied requires_grad=param requires_grad torch utils swap_tensors param param_applied except Exception e param_grad None param grad = param_grad raise RuntimeError f _apply Couldn t swap _get_name key e out_param = param p_should_use_set_data pyrefly ignore bad-assignment param data = param_applied out_param = param assert isinstance param Parameter assert param is_leaf pyrefly ignore bad-argument-type out_param = Parameter param_applied param requires_grad _parameters key = out_param param_grad None torch no_grad grad_applied = fn param_grad g_should_use_set_data = compute_should_use_set_data param_grad grad_applied p_should_use_swap_tensors grad_applied requires_grad_ param_grad requires_grad try torch utils swap_tensors param_grad grad_applied except Exception e raise RuntimeError f _apply Couldn t swap _get_name key grad e out_param grad = param_grad g_should_use_set_data assert out_param grad None out_param grad data = grad_applied assert param_grad is_leaf out_param grad = grad_applied requires_grad_ param_grad requires_grad key buf _buffers items buf None _buffers key = fn buf apply fn Callable Module None - Self r Apply ` ` fn ` ` recursively every submodule returned ` ` children ` ` well Typical use includes initializing parameters model see also ref ` nn-init-doc ` Args fn ` Module ` - None function applied each submodule Returns Module Example torch no_grad init_weights m print m type m nn Linear m weight fill_ print m weight net = nn Sequential nn Linear nn Linear net apply init_weights Linear in_features= out_features= bias=True Parameter containing tensor requires_grad=True Linear in_features= out_features= bias=True Parameter containing tensor requires_grad=True Sequential Linear in_features= out_features= bias=True Linear in_features= out_features= bias=True module children module apply fn fn cuda device Optional Union int device = None - Self r Move all model parameters buffers GPU This also makes associated parameters buffers different objects So should called before constructing optimizer module will live GPU while being optimized note This method modifies module in-place Args device int optional specified all parameters will copied device Returns Module _apply lambda t t cuda device ipu device Optional Union int device = None - Self r Move all model parameters buffers IPU This also makes associated parameters buffers different objects So should called before constructing optimizer module will live IPU while being optimized note This method modifies module in-place Arguments device int optional specified all parameters will copied device Returns Module _apply lambda t t ipu device xpu device Optional Union int device = None - Self r Move all model parameters buffers XPU This also makes associated parameters buffers different objects So should called before constructing optimizer module will live XPU while being optimized note This method modifies module in-place Arguments device int optional specified all parameters will copied device Returns Module _apply lambda t t xpu device mtia device Optional Union int device = None - Self r Move all model parameters buffers MTIA This also makes associated parameters buffers different objects So should called before constructing optimizer module will live MTIA while being optimized note This method modifies module in-place Arguments device int optional specified all parameters will copied device Returns Module _apply lambda t t mtia device cpu - Self r Move all model parameters buffers CPU note This method modifies module in-place Returns Module _apply lambda t t cpu type dst_type Union dtype str - Self r Casts all parameters buffers attr ` dst_type ` note This method modifies module in-place Args dst_type type string desired type Returns Module _apply lambda t t type dst_type float - Self r Casts all floating point parameters buffers ` ` float ` ` datatype note This method modifies module in-place Returns Module _apply lambda t t float t is_floating_point t double - Self r Casts all floating point parameters buffers ` ` double ` ` datatype note This method modifies module in-place Returns Module _apply lambda t t double t is_floating_point t half - Self r Casts all floating point parameters buffers ` ` half ` ` datatype note This method modifies module in-place Returns Module _apply lambda t t half t is_floating_point t bfloat - Self r Casts all floating point parameters buffers ` ` bfloat ` ` datatype note This method modifies module in-place Returns Module _apply lambda t t bfloat t is_floating_point t to_empty device Optional DeviceLikeType recurse bool = True - Self r Move parameters buffers specified device without copying storage Args device ` torch device ` The desired device parameters buffers module recurse bool Whether parameters buffers submodules should recursively moved specified device Returns Module _apply lambda t torch empty_like t device=device recurse=recurse overload device Optional DeviceLikeType = dtype Optional dtype = non_blocking bool = - Self overload dtype dtype non_blocking bool = - Self overload tensor Tensor non_blocking bool = - Self args kwargs r Move cast parameters buffers This can called function device=None dtype=None non_blocking=False noindex function dtype non_blocking=False noindex function tensor non_blocking=False noindex function memory_format=torch channels_last noindex Its signature similar meth ` torch Tensor ` only accepts floating point complex attr ` dtype ` \ s In addition method will only cast floating point complex parameters buffers attr ` dtype ` given The integral parameters buffers will moved attr ` device ` given dtypes unchanged When attr ` non_blocking ` set tries convert move asynchronously respect host possible e g moving CPU Tensors pinned memory CUDA devices See below examples note This method modifies module in-place Args device ` torch device ` desired device parameters buffers module dtype ` torch dtype ` desired floating point complex dtype parameters buffers module tensor torch Tensor Tensor whose dtype device desired dtype device all parameters buffers module memory_format ` torch memory_format ` desired memory format D parameters buffers module keyword only argument Returns Module Examples xdoctest +IGNORE_WANT non-deterministic linear = nn Linear linear weight Parameter containing tensor - - - linear torch double Linear in_features= out_features= bias=True linear weight Parameter containing tensor - - - dtype=torch float xdoctest +REQUIRES env TORCH_DOCTEST_CUDA gpu = torch device cuda linear gpu dtype=torch half non_blocking=True Linear in_features= out_features= bias=True linear weight Parameter containing tensor - - - dtype=torch float device= cuda cpu = torch device cpu linear cpu Linear in_features= out_features= bias=True linear weight Parameter containing tensor - - - dtype=torch float linear = nn Linear bias=None torch cdouble linear weight Parameter containing tensor + j + j + j - + j dtype=torch complex linear torch ones dtype=torch cdouble tensor + j + j + j + j + j + j dtype=torch complex device dtype non_blocking convert_to_format = torch _C _nn _parse_to pyrefly ignore not-iterable args kwargs dtype None dtype is_floating_point dtype is_complex raise TypeError nn Module only accepts floating point complex f dtypes got desired dtype= dtype dtype is_complex warnings warn Complex modules new feature under active development whose design may change some modules might work expected when using complex tensors parameters buffers Please file issue https github com pytorch pytorch issues new template=bug-report yml complex module does work expected stacklevel= convert t try convert_to_format None t dim t device dtype t is_floating_point t is_complex None non_blocking memory_format=convert_to_format t device dtype t is_floating_point t is_complex None non_blocking except NotImplementedError e str e == Cannot copy out meta tensor no data raise NotImplementedError f e Please use torch nn Module to_empty instead torch nn Module f when moving module meta different device None raise _apply convert register_full_backward_pre_hook hook Callable Module _grad_t Union None _grad_t prepend bool = False - RemovableHandle r Register backward pre-hook module The hook will called every time gradients module computed The hook should have following signature hook module grad_output - tuple Tensor None The attr ` grad_output ` tuple The hook should modify its arguments can optionally new gradient respect output will used place attr ` grad_output ` subsequent computations Entries attr ` grad_output ` will ` ` None ` ` all non-Tensor arguments For technical reasons when hook applied Module its forward function will receive view each Tensor passed Module Similarly caller will receive view each Tensor returned Module s forward function warning Modifying inputs inplace allowed when using backward hooks will raise error Args hook Callable The user-defined hook registered prepend bool If true provided ` ` hook ` ` will fired before all existing ` ` backward_pre ` ` hooks ` torch nn Module ` Otherwise provided ` ` hook ` ` will fired after all existing ` ` backward_pre ` ` hooks ` torch nn Module ` Note global ` ` backward_pre ` ` hooks registered func ` register_module_full_backward_pre_hook ` will fire before all hooks registered method Returns ` torch utils hooks RemovableHandle ` handle can used remove added hook calling ` ` handle remove ` ` handle = RemovableHandle _backward_pre_hooks _backward_pre_hooks handle id = hook prepend _backward_pre_hooks move_to_end handle id last=False type ignore attr-defined handle register_backward_hook hook Callable Module _grad_t _grad_t Union None _grad_t - RemovableHandle r Register backward hook module This function deprecated favor meth ` ~torch nn Module register_full_backward_hook ` behavior function will change future versions Returns ` torch utils hooks RemovableHandle ` handle can used remove added hook calling ` ` handle remove ` ` _is_full_backward_hook True raise RuntimeError Cannot use both regular backward hooks full backward hooks single Module Please use only one them _is_full_backward_hook = False handle = RemovableHandle _backward_hooks _backward_hooks handle id = hook handle register_full_backward_hook hook Callable Module _grad_t _grad_t Union None _grad_t prepend bool = False - RemovableHandle r Register backward hook module The hook will called every time gradients respect module computed its firing rules follows Ordinarily hook fires when gradients computed respect module inputs If none module inputs require gradients hook will fire when gradients computed respect module outputs If none module outputs require gradients then hooks will fire The hook should have following signature hook module grad_input grad_output - tuple Tensor None The attr ` grad_input ` attr ` grad_output ` tuples contain gradients respect inputs outputs respectively The hook should modify its arguments can optionally new gradient respect input will used place attr ` grad_input ` subsequent computations attr ` grad_input ` will only correspond inputs given positional arguments all kwarg arguments ignored Entries attr ` grad_input ` attr ` grad_output ` will ` ` None ` ` all non-Tensor arguments For technical reasons when hook applied Module its forward function will receive view each Tensor passed Module Similarly caller will receive view each Tensor returned Module s forward function warning Modifying inputs outputs inplace allowed when using backward hooks will raise error Args hook Callable The user-defined hook registered prepend bool If true provided ` ` hook ` ` will fired before all existing ` ` backward ` ` hooks ` torch nn Module ` Otherwise provided ` ` hook ` ` will fired after all existing ` ` backward ` ` hooks ` torch nn Module ` Note global ` ` backward ` ` hooks registered func ` register_module_full_backward_hook ` will fire before all hooks registered method Returns ` torch utils hooks RemovableHandle ` handle can used remove added hook calling ` ` handle remove ` ` _is_full_backward_hook False raise RuntimeError Cannot use both regular backward hooks full backward hooks single Module Please use only one them _is_full_backward_hook = True handle = RemovableHandle _backward_hooks _backward_hooks handle id = hook prepend _backward_hooks move_to_end handle id last=False type ignore attr-defined handle _get_backward_hooks r Return backward hooks use call function It returns two lists one full backward hooks one non-full backward hooks full_backward_hooks list Callable = _global_is_full_backward_hook True full_backward_hooks += _global_backward_hooks values _is_full_backward_hook True full_backward_hooks += _backward_hooks values non_full_backward_hooks list Callable = _global_is_full_backward_hook False non_full_backward_hooks += _global_backward_hooks values _is_full_backward_hook False non_full_backward_hooks += _backward_hooks values full_backward_hooks non_full_backward_hooks _get_backward_pre_hooks backward_pre_hooks list Callable = backward_pre_hooks += _global_backward_pre_hooks values backward_pre_hooks += _backward_pre_hooks values backward_pre_hooks _maybe_warn_non_full_backward_hook inputs result grad_fn - None isinstance result torch Tensor isinstance result tuple all isinstance r torch Tensor r result warnings warn Using non-full backward hooks Module does single Tensor tuple Tensors deprecated will removed future versions This hook will missing some grad_output Please use register_full_backward_hook get documented behavior FutureWarning stacklevel= result = result isinstance inputs torch Tensor isinstance inputs tuple all isinstance i torch Tensor i inputs warnings warn Using non-full backward hooks Module does take input single Tensor tuple Tensors deprecated will removed future versions This hook will missing some grad_input Please use register_full_backward_hook get documented behavior FutureWarning stacklevel= inputs = inputs At point we sure inputs result tuple Tensors out_grad_fn = r grad_fn r result r grad_fn None len out_grad_fn == len out_grad_fn == grad_fn out_grad_fn warnings warn Using non-full backward hook when outputs nested python data structure deprecated will removed future versions This hook will missing some grad_output FutureWarning stacklevel= len out_grad_fn warnings warn Using non-full backward hook when outputs generated different autograd Nodes deprecated will removed future versions This hook will missing some grad_output Please use register_full_backward_hook get documented behavior FutureWarning stacklevel= At point grad_output part hook will most likely correct inputs_grad_fn = i grad_fn i inputs i grad_fn None next_functions = n n grad_fn next_functions inputs_grad_fn = next_functions warnings warn Using non-full backward hook when forward contains multiple autograd Nodes deprecated will removed future versions This hook will missing some grad_input Please use register_full_backward_hook get documented behavior FutureWarning stacklevel= register_forward_pre_hook hook Union Callable T tuple Any Optional Any Callable T tuple Any dict str Any Optional tuple Any dict str Any prepend bool = False with_kwargs bool = False - RemovableHandle r Register forward pre-hook module The hook will called every time before func ` forward ` invoked If ` ` with_kwargs ` ` false specified input contains only positional arguments given module Keyword arguments won t passed hooks only ` ` forward ` ` The hook can modify input User can either tuple single modified value hook We will wrap value into tuple single value returned unless value already tuple The hook should have following signature hook module args - None modified input If ` ` with_kwargs ` ` true forward pre-hook will passed kwargs given forward function And hook modifies input both args kwargs should returned The hook should have following signature hook module args kwargs - None tuple modified input kwargs Args hook Callable The user defined hook registered prepend bool If true provided ` ` hook ` ` will fired before all existing ` ` forward_pre ` ` hooks ` torch nn Module ` Otherwise provided ` ` hook ` ` will fired after all existing ` ` forward_pre ` ` hooks ` torch nn Module ` Note global ` ` forward_pre ` ` hooks registered func ` register_module_forward_pre_hook ` will fire before all hooks registered method Default ` ` False ` ` with_kwargs bool If true ` ` hook ` ` will passed kwargs given forward function Default ` ` False ` ` Returns ` torch utils hooks RemovableHandle ` handle can used remove added hook calling ` ` handle remove ` ` handle = RemovableHandle _forward_pre_hooks extra_dict=self _forward_pre_hooks_with_kwargs _forward_pre_hooks handle id = hook with_kwargs _forward_pre_hooks_with_kwargs handle id = True prepend _forward_pre_hooks move_to_end handle id last=False type ignore attr-defined handle register_forward_hook hook Union Callable T tuple Any Any Optional Any Callable T tuple Any dict str Any Any Optional Any prepend bool = False with_kwargs bool = False always_call bool = False - RemovableHandle r Register forward hook module The hook will called every time after func ` forward ` has computed output If ` ` with_kwargs ` ` ` ` False ` ` specified input contains only positional arguments given module Keyword arguments won t passed hooks only ` ` forward ` ` The hook can modify output It can modify input inplace will have effect forward since called after func ` forward ` called The hook should have following signature hook module args output - None modified output If ` ` with_kwargs ` ` ` ` True ` ` forward hook will passed ` ` kwargs ` ` given forward function expected output possibly modified The hook should have following signature hook module args kwargs output - None modified output Args hook Callable The user defined hook registered prepend bool If ` ` True ` ` provided ` ` hook ` ` will fired before all existing ` ` forward ` ` hooks ` torch nn Module ` Otherwise provided ` ` hook ` ` will fired after all existing ` ` forward ` ` hooks ` torch nn Module ` Note global ` ` forward ` ` hooks registered func ` register_module_forward_hook ` will fire before all hooks registered method Default ` ` False ` ` with_kwargs bool If ` ` True ` ` ` ` hook ` ` will passed kwargs given forward function Default ` ` False ` ` always_call bool If ` ` True ` ` ` ` hook ` ` will run regardless whether exception raised while calling Module Default ` ` False ` ` Returns ` torch utils hooks RemovableHandle ` handle can used remove added hook calling ` ` handle remove ` ` handle = RemovableHandle _forward_hooks extra_dict= _forward_hooks_with_kwargs _forward_hooks_always_called _forward_hooks handle id = hook with_kwargs _forward_hooks_with_kwargs handle id = True always_call _forward_hooks_always_called handle id = True prepend _forward_hooks move_to_end handle id last=False type ignore attr-defined handle _slow_forward input kwargs tracing_state = torch _C _get_tracing_state tracing_state isinstance forward torch _C ScriptMethod forward input kwargs recording_scopes = torch jit _trace _trace_module_map None recording_scopes type ignore added because point one knows torch jit _trace _trace_module_map Optional has type Dict Any Any name = torch jit _trace _trace_module_map get None type ignore operator union-attr name tracing_state push_scope name recording_scopes = False try result = forward input kwargs finally recording_scopes tracing_state pop_scope result _wrapped_call_impl args kwargs _compiled_call_impl None _compiled_call_impl args kwargs type ignore misc _call_impl args kwargs torchrec tests code consistency following code fmt off _call_impl args kwargs forward_call = _slow_forward torch _C _get_tracing_state forward If we don t have any hooks we want skip rest logic function just call forward _backward_hooks _backward_pre_hooks _forward_hooks _forward_pre_hooks _global_backward_pre_hooks _global_backward_hooks _global_forward_hooks _global_forward_pre_hooks forward_call args kwargs result = None called_always_called_hooks = set inner nonlocal result args kwargs full_backward_hooks non_full_backward_hooks = backward_pre_hooks = _backward_pre_hooks _global_backward_pre_hooks backward_pre_hooks = _get_backward_pre_hooks _backward_hooks _global_backward_hooks full_backward_hooks non_full_backward_hooks = _get_backward_hooks _global_forward_pre_hooks _forward_pre_hooks hook_id hook _global_forward_pre_hooks items _forward_pre_hooks items hook_id _forward_pre_hooks_with_kwargs args_kwargs_result = hook args kwargs type ignore misc args_kwargs_result None isinstance args_kwargs_result tuple len args_kwargs_result == args kwargs = args_kwargs_result raise RuntimeError forward pre-hook must None tuple f new_args new_kwargs got args_kwargs_result args_result = hook args args_result None isinstance args_result tuple args_result = args_result args = args_result bw_hook = None full_backward_hooks backward_pre_hooks bw_hook = BackwardHook full_backward_hooks backward_pre_hooks args = bw_hook setup_input_hook args result = forward_call args kwargs _global_forward_hooks _forward_hooks hook_id hook _global_forward_hooks items _forward_hooks items mark always called hook run hook_id _forward_hooks_always_called hook_id _global_forward_hooks_always_called called_always_called_hooks add hook_id hook_id _forward_hooks_with_kwargs hook_id _global_forward_hooks_with_kwargs hook_result = hook args kwargs result hook_result = hook args result hook_result None result = hook_result bw_hook isinstance result torch Tensor tuple warnings warn For backward hooks called module output should Tensor tuple Tensors f received type result stacklevel= result = bw_hook setup_output_hook result Handle non-full backward hooks non_full_backward_hooks var = result while isinstance var torch Tensor isinstance var dict var = next v v var values isinstance v torch Tensor var = var grad_fn = var grad_fn grad_fn None hook non_full_backward_hooks grad_fn register_hook _WrappedHook hook _maybe_warn_non_full_backward_hook args result grad_fn result This technically behavior equivalent when compiling s incredibly unlikely we will ever support throwing exception NN module then catching here then reraising then catching again expecting resulting frame compiled The reraise here just gunks up our exception handling no good reason Don t try run always called hooks event exception torch compiler is_compiling inner try inner except Exception run always called hooks they have already been run For now only forward hooks have always_call option perhaps functionality should added full backward hooks well hook_id hook _global_forward_hooks items hook_id _global_forward_hooks_always_called hook_id called_always_called_hooks type ignore possibly-undefined try hook_result = hook args result type ignore possibly-undefined hook_result None result = hook_result except Exception e warnings warn global module forward hook ` ` always_call=True ` ` raised exception f silenced another error raised forward str e stacklevel= continue hook_id hook _forward_hooks items hook_id _forward_hooks_always_called hook_id called_always_called_hooks type ignore possibly-undefined try hook_id _forward_hooks_with_kwargs hook_result = hook args kwargs result type ignore possibly-undefined hook_result = hook args result type ignore possibly-undefined hook_result None result = hook_result except Exception e warnings warn module forward hook ` ` always_call=True ` ` raised exception f silenced another error raised forward str e stacklevel= continue raise exception raised try block raise fmt __call__ Callable Any = _wrapped_call_impl __getstate__ state = __dict__ copy state pop _compiled_call_impl None state __setstate__ state __dict__ update state Support loading old checkpoints don t have following attrs _forward_pre_hooks __dict__ _forward_pre_hooks = OrderedDict _forward_pre_hooks_with_kwargs __dict__ _forward_pre_hooks_with_kwargs = OrderedDict _forward_hooks_with_kwargs __dict__ _forward_hooks_with_kwargs = OrderedDict _forward_hooks_always_called __dict__ _forward_hooks_always_called = OrderedDict _state_dict_hooks __dict__ _state_dict_hooks = OrderedDict _state_dict_pre_hooks __dict__ _state_dict_pre_hooks = OrderedDict _load_state_dict_pre_hooks __dict__ _load_state_dict_pre_hooks = OrderedDict _load_state_dict_post_hooks __dict__ _load_state_dict_post_hooks = OrderedDict _non_persistent_buffers_set __dict__ _non_persistent_buffers_set = set _is_full_backward_hook __dict__ _is_full_backward_hook = None _backward_pre_hooks __dict__ _backward_pre_hooks = OrderedDict It crucial type annotated ` Any ` otherwise type checking ` torch nn Module ` all its subclasses largely disabled result See https github com pytorch pytorch pull __getattr__ name str - Union Tensor Module _parameters __dict__ _parameters = __dict__ _parameters name _parameters _parameters name _buffers __dict__ _buffers = __dict__ _buffers name _buffers _buffers name _modules __dict__ modules = __dict__ _modules name modules modules name raise AttributeError f type __name__ object has no attribute name __setattr__ name str value Union Tensor Module - None remove_from dicts_or_sets - None d dicts_or_sets name d isinstance d dict del d name d discard name params = __dict__ get _parameters isinstance value Parameter params None raise AttributeError cannot assign parameters before Module __init__ call remove_from __dict__ _buffers _modules _non_persistent_buffers_set register_parameter name value params None name params value None raise TypeError f cannot assign torch typename value parameter name torch nn Parameter None expected register_parameter name value modules = __dict__ get _modules isinstance value Module modules None raise AttributeError cannot assign module before Module __init__ call remove_from __dict__ _parameters _buffers _non_persistent_buffers_set hook _global_module_registration_hooks values output = hook name value output None value = output modules name = value modules None name modules value None raise TypeError f cannot assign torch typename value child module name torch nn Module None expected hook _global_module_registration_hooks values output = hook name value output None value = output modules name = value buffers = __dict__ get _buffers isinstance value Buffer buffers None name buffers value None isinstance value torch Tensor hasattr value __torch_function__ raise TypeError f cannot assign torch typename value buffer name torch nn Buffer torch Tensor None expected isinstance value Buffer persistent = value persistent persistent = name _non_persistent_buffers_set === HACK === This whole block below should just register_buffer name value persistent But support subclasses nn Module wrongfully implement register_buffer method doesn t have persistent argument Only pass accepted otherwise assume always true getattr register_buffer __func__ None torch nn Module register_buffer register_buffer name value persistent sign = inspect signature register_buffer persistent sign parameters register_buffer name value persistent persistent raise RuntimeError Registering non-persistent buffer Module subclass implements register_buffer without persistent argument allowed Assume implementation without argument has behavior before argument added persistent=True register_buffer name value === HACK END === super __setattr__ name value __delattr__ name - None name _parameters del _parameters name name _buffers del _buffers name _non_persistent_buffers_set discard name name _modules del _modules name super __delattr__ name _register_state_dict_hook hook r Register post-hook meth ` ~torch nn Module state_dict ` method It should have following signature hook module state_dict prefix local_metadata - None state_dict The registered hooks can modify ` ` state_dict ` ` inplace new one If new ` ` state_dict ` ` returned will only respected root module meth ` ~nn Module state_dict ` called getattr hook _from_public_api False raise RuntimeError Cannot register same function state dict post hook previously registered via register_state_dict_post_hook handle = RemovableHandle _state_dict_hooks _state_dict_hooks handle id = hook handle register_state_dict_post_hook hook r Register post-hook meth ` ~torch nn Module state_dict ` method It should have following signature hook module state_dict prefix local_metadata - None The registered hooks can modify ` ` state_dict ` ` inplace In _register_state_dict_hook there bug described https github com pytorch pytorch issues where value only respected root module child submodules We fix public version only allowing inplace modifications state_dict hook However since hooks registered via both these APIs will added ` _state_dict_hooks ` type ` _state_dict_hooks ` cannot changed due many dependencies we mark hook being registered via public API setting ` _from_public_api ` In implementation ` state_dict ` callable does have flag old behavior respecting value will preserved root module otherwise we ensure hook returns None hook _from_public_api = True handle = RemovableHandle _state_dict_hooks _state_dict_hooks handle id = hook handle register_state_dict_pre_hook hook r Register pre-hook meth ` ~torch nn Module state_dict ` method It should have following signature hook module prefix keep_vars - None The registered hooks can used perform pre-processing before ` ` state_dict ` ` call made handle = RemovableHandle _state_dict_pre_hooks _state_dict_pre_hooks handle id = hook handle _save_to_state_dict destination prefix keep_vars - None r Save module state ` destination ` dictionary The ` destination ` dictionary will contain state module its descendants This called every submodule meth ` ~torch nn Module state_dict ` In rare cases subclasses can achieve class-specific behavior overriding method custom logic Args destination dict dict where state will stored prefix str prefix parameters buffers used module name param _parameters items param None destination prefix + name = param keep_vars param detach name buf _buffers items buf None name _non_persistent_buffers_set destination prefix + name = buf keep_vars buf detach extra_state_key = prefix + _EXTRA_STATE_KEY_SUFFIX getattr __class__ get_extra_state Module get_extra_state Module get_extra_state destination extra_state_key = get_extra_state The user can pass optional arbitrary mappable object ` state_dict ` which case ` state_dict ` returns back same object But they pass nothing ` OrderedDict ` created returned T_destination = TypeVar T_destination bound=dict str Any overload state_dict destination T_destination prefix str = keep_vars bool = - T_destination overload state_dict prefix str = keep_vars bool = - dict str Any TODO Change ` args ` ` ` remove corresponding warning docs when BC allows Also remove logic arg parsing together state_dict args destination=None prefix= keep_vars=False r Return dictionary containing references whole state module Both parameters persistent buffers e g running averages included Keys corresponding parameter buffer names Parameters buffers set ` ` None ` ` included note The returned object shallow copy It contains references module s parameters buffers warning Currently ` ` state_dict ` ` also accepts positional arguments ` ` destination ` ` ` ` prefix ` ` ` ` keep_vars ` ` order However being deprecated keyword arguments will enforced future releases warning Please avoid use argument ` ` destination ` ` designed end-users Args destination dict optional If provided state module will updated into dict same object returned Otherwise ` ` OrderedDict ` ` will created returned Default ` ` None ` ` prefix str optional prefix added parameter buffer names compose keys state_dict Default ` ` ` ` keep_vars bool optional default ` ~torch Tensor ` s returned state dict detached autograd If s set ` ` True ` ` detaching will performed Default ` ` False ` ` Returns dict dictionary containing whole state module Example xdoctest +SKIP undefined vars module state_dict keys bias weight TODO Remove ` args ` parsing logic when BC allows len args DeprecationWarning ignored default warnings warn Positional args being deprecated use kwargs instead Refer https pytorch org docs main generated torch nn Module html#torch nn Module state_dict details FutureWarning stacklevel= destination None destination = args len args prefix == prefix = args len args keep_vars False keep_vars = args destination None destination = OrderedDict pyrefly ignore missing-attribute destination _metadata = OrderedDict local_metadata = dict version=self _version hasattr destination _metadata destination _metadata prefix - = local_metadata hook _state_dict_pre_hooks values hook prefix keep_vars _save_to_state_dict destination prefix keep_vars name module _modules items module None module state_dict destination=destination prefix=prefix + name + keep_vars=keep_vars hook _state_dict_hooks values hook_result = hook destination prefix local_metadata getattr hook _from_public_api False hook_result None destination = hook_result hook_result None raise RuntimeError state_dict post-hook must None destination _register_load_state_dict_pre_hook hook with_module=False r See meth ` ~torch nn Module register_load_state_dict_pre_hook ` details A subtle difference ` ` with_module ` ` set ` ` False ` ` then hook will take ` ` module ` ` first argument whereas meth ` ~torch nn Module register_load_state_dict_pre_hook ` always takes ` ` module ` ` first argument Arguments hook Callable Callable hook will invoked before loading state dict with_module bool optional Whether pass module instance hook first parameter handle = RemovableHandle _load_state_dict_pre_hooks _load_state_dict_pre_hooks handle id = _WrappedHook hook with_module None handle register_load_state_dict_pre_hook hook r Register pre-hook run before module s meth ` ~nn Module load_state_dict ` called It should have following signature hook module state_dict prefix local_metadata strict missing_keys unexpected_keys error_msgs - None noqa B Arguments hook Callable Callable hook will invoked before loading state dict _register_load_state_dict_pre_hook hook with_module=True register_load_state_dict_post_hook hook r Register post-hook run after module s meth ` ~nn Module load_state_dict ` called It should have following signature hook module incompatible_keys - None The ` ` module ` ` argument current module hook registered ` ` incompatible_keys ` ` argument ` ` NamedTuple ` ` consisting attributes ` ` missing_keys ` ` ` ` unexpected_keys ` ` ` ` missing_keys ` ` ` ` list ` ` ` ` str ` ` containing missing keys ` ` unexpected_keys ` ` ` ` list ` ` ` ` str ` ` containing unexpected keys The given incompatible_keys can modified inplace needed Note checks performed when calling func ` load_state_dict ` ` ` strict=True ` ` affected modifications hook makes ` ` missing_keys ` ` ` ` unexpected_keys ` ` expected Additions either set keys will result error being thrown when ` ` strict=True ` ` clearing out both missing unexpected keys will avoid error Returns ` torch utils hooks RemovableHandle ` handle can used remove added hook calling ` ` handle remove ` ` handle = RemovableHandle _load_state_dict_post_hooks _load_state_dict_post_hooks handle id = hook handle _load_from_state_dict state_dict prefix local_metadata strict missing_keys unexpected_keys error_msgs - None r Copy parameters buffers attr ` state_dict ` into only module its descendants This called every submodule meth ` ~torch nn Module load_state_dict ` Metadata saved module input attr ` state_dict ` provided attr ` local_metadata ` For state dicts without metadata attr ` local_metadata ` empty Subclasses can achieve class-specific backward compatible loading using version number ` local_metadata get version None ` Additionally attr ` local_metadata ` can also contain key ` assign_to_params_buffers ` indicates whether keys should assigned their corresponding tensor state_dict note attr ` state_dict ` same object input attr ` state_dict ` meth ` ~torch nn Module load_state_dict ` So can modified Args state_dict dict dict containing parameters persistent buffers prefix str prefix parameters buffers used module local_metadata dict dict containing metadata module See strict bool whether strictly enforce keys attr ` state_dict ` attr ` prefix ` match names parameters buffers module missing_keys list str ` ` strict=True ` ` add missing keys list unexpected_keys list str ` ` strict=True ` ` add unexpected keys list error_msgs list str error messages should added list will reported together meth ` ~torch nn Module load_state_dict ` hook _load_state_dict_pre_hooks values hook state_dict prefix local_metadata strict missing_keys unexpected_keys error_msgs persistent_buffers = k v k v _buffers items k _non_persistent_buffers_set local_name_params = itertools chain _parameters items pyrefly ignore bad-argument-type persistent_buffers items local_state = k v k v local_name_params v None assign_to_params_buffers = local_metadata get assign_to_params_buffers False use_swap_tensors = torch __future__ get_swap_module_params_on_conversion name param local_state items key = prefix + name key state_dict input_param = state_dict key torch overrides is_tensor_like input_param error_msgs append f While copying parameter named key expected torch Tensor Tensor-like object checkpoint f received type input_param continue This used avoid copying uninitialized parameters into non-lazy modules since they dont have hook do checks such case will error when accessing shape attribute is_param_lazy = torch nn parameter is_lazy param Backward compatibility loading -dim tensor version + is_param_lazy len param shape == len input_param shape == input_param = input_param is_param_lazy input_param shape = param shape local shape should match one checkpoint error_msgs append f size mismatch key copying param shape input_param shape checkpoint f shape current model param shape continue param is_meta input_param is_meta assign_to_params_buffers warnings warn f key copying non-meta parameter checkpoint meta parameter current model which no-op Did you mean pass ` assign=True ` assign items state dictionary their corresponding key module instead copying them place stacklevel= try torch no_grad use_swap_tensors new_input_param = param module_load input_param assign=assign_to_params_buffers id new_input_param == id input_param id new_input_param == id param raise RuntimeError module_load returned one other please detach result returning one inputs module_load isinstance param torch nn Parameter isinstance new_input_param torch nn Parameter new_input_param = torch nn Parameter new_input_param requires_grad=param requires_grad new_input_param requires_grad_ param requires_grad torch utils swap_tensors param new_input_param del new_input_param assign_to_params_buffers Shape checks already done above isinstance param torch nn Parameter isinstance input_param torch nn Parameter input_param = torch nn Parameter input_param requires_grad=param requires_grad input_param requires_grad_ param requires_grad setattr name input_param param copy_ input_param except Exception ex action = swapping use_swap_tensors copying error_msgs append f While action parameter named key f whose dimensions model param size f whose dimensions checkpoint input_param size f exception occurred ex args strict missing_keys append key extra_state_key = prefix + _EXTRA_STATE_KEY_SUFFIX getattr __class__ set_extra_state Module set_extra_state Module set_extra_state extra_state_key state_dict set_extra_state state_dict extra_state_key strict missing_keys append extra_state_key strict extra_state_key state_dict unexpected_keys append extra_state_key strict key state_dict keys key startswith prefix key = extra_state_key input_name = key len prefix split Must Module have attributes len input_name input_name _modules unexpected_keys append key input_name local_state unexpected_keys append key load_state_dict state_dict Mapping str Any strict bool = True assign bool = False r Copy parameters buffers attr ` state_dict ` into module its descendants If attr ` strict ` ` ` True ` ` then keys attr ` state_dict ` must exactly match keys returned module s meth ` ~torch nn Module state_dict ` function warning If attr ` assign ` ` ` True ` ` optimizer must created after call attr ` load_state_dict ` unless func ` ~torch __future__ get_swap_module_params_on_conversion ` ` ` True ` ` Args state_dict dict dict containing parameters persistent buffers strict bool optional whether strictly enforce keys attr ` state_dict ` match keys returned module s meth ` ~torch nn Module state_dict ` function Default ` ` True ` ` assign bool optional When set ` ` False ` ` properties tensors current module preserved whereas setting ` ` True ` ` preserves properties Tensors state dict The only exception ` ` requires_grad ` ` field ` ~torch nn Parameter ` which value module preserved Default ` ` False ` ` Returns ` ` NamedTuple ` ` ` ` missing_keys ` ` ` ` unexpected_keys ` ` fields ` ` missing_keys ` ` list str containing any keys expected module missing provided ` ` state_dict ` ` ` ` unexpected_keys ` ` list str containing keys expected module present provided ` ` state_dict ` ` Note If parameter buffer registered ` ` None ` ` its corresponding key exists attr ` state_dict ` meth ` load_state_dict ` will raise ` ` RuntimeError ` ` isinstance state_dict Mapping raise TypeError f Expected state_dict dict-like got type state_dict missing_keys list str = unexpected_keys list str = error_msgs list str = copy state_dict so _load_from_state_dict can modify metadata = getattr state_dict _metadata None state_dict = OrderedDict state_dict metadata None mypy isn t aware _metadata exists state_dict state_dict _metadata = metadata type ignore attr-defined load module local_state_dict prefix= - None local_metadata = metadata None metadata get prefix - assign local_metadata assign_to_params_buffers = assign module _load_from_state_dict local_state_dict prefix local_metadata True missing_keys unexpected_keys error_msgs name child module _modules items child None child_prefix = prefix + name + child_state_dict = k v k v local_state_dict items k startswith child_prefix load child child_state_dict child_prefix noqa F Note hook can modify missing_keys unexpected_keys incompatible_keys = _IncompatibleKeys missing_keys unexpected_keys hook module _load_state_dict_post_hooks values out = hook module incompatible_keys assert out None Hooks registered ` ` register_load_state_dict_post_hook ` ` expected new values incompatible_keys need modified should done inplace load state_dict del load strict len unexpected_keys error_msgs insert Unexpected key s state_dict format join f k k unexpected_keys len missing_keys error_msgs insert Missing key s state_dict format join f k k missing_keys len error_msgs raise RuntimeError Error s loading state_dict \n\t format __class__ __name__ \n\t join error_msgs _IncompatibleKeys missing_keys unexpected_keys _named_members get_members_fn prefix= recurse=True remove_duplicate bool = True r Help yield various names + members modules memo = set modules = named_modules prefix=prefix remove_duplicate=remove_duplicate recurse prefix module_prefix module modules members = get_members_fn module k v members v None v memo continue remove_duplicate memo add v name = module_prefix + module_prefix + k yield name v parameters recurse bool = True - Iterator Parameter r Return iterator over module parameters This typically passed optimizer Args recurse bool True then yields parameters module all submodules Otherwise yields only parameters direct members module Yields Parameter module parameter Example xdoctest +SKIP undefined vars param model parameters print type param param size torch Tensor L torch Tensor L L L L _name param named_parameters recurse=recurse yield param named_parameters prefix str = recurse bool = True remove_duplicate bool = True - Iterator tuple str Parameter r Return iterator over module parameters yielding both name parameter well parameter itself Args prefix str prefix prepend all parameter names recurse bool True then yields parameters module all submodules Otherwise yields only parameters direct members module remove_duplicate bool optional whether remove duplicated parameters result Defaults True Yields str Parameter Tuple containing name parameter Example xdoctest +SKIP undefined vars name param named_parameters name bias print param size gen = _named_members lambda module module _parameters items prefix=prefix recurse=recurse remove_duplicate=remove_duplicate yield gen buffers recurse bool = True - Iterator Tensor r Return iterator over module buffers Args recurse bool True then yields buffers module all submodules Otherwise yields only buffers direct members module Yields torch Tensor module buffer Example xdoctest +SKIP undefined vars buf model buffers print type buf buf size torch Tensor L torch Tensor L L L L _ buf named_buffers recurse=recurse yield buf named_buffers prefix str = recurse bool = True remove_duplicate bool = True - Iterator tuple str Tensor r Return iterator over module buffers yielding both name buffer well buffer itself Args prefix str prefix prepend all buffer names recurse bool optional True then yields buffers module all submodules Otherwise yields only buffers direct members module Defaults True remove_duplicate bool optional whether remove duplicated buffers result Defaults True Yields str torch Tensor Tuple containing name buffer Example xdoctest +SKIP undefined vars name buf named_buffers name running_var print buf size gen = _named_members lambda module module _buffers items prefix=prefix recurse=recurse remove_duplicate=remove_duplicate yield gen children - Iterator Module r Return iterator over immediate children modules Yields Module child module _name module named_children yield module named_children - Iterator tuple str Module r Return iterator over immediate children modules yielding both name module well module itself Yields str Module Tuple containing name child module Example xdoctest +SKIP undefined vars name module model named_children name conv conv print module memo = set name module _modules items module None module memo memo add module yield name module modules - Iterator Module r Return iterator over all modules network Yields Module module network Note Duplicate modules returned only once In following example ` ` l ` ` will returned only once Example l = nn Linear net = nn Sequential l l idx m enumerate net modules print idx - m - Sequential Linear in_features= out_features= bias=True Linear in_features= out_features= bias=True - Linear in_features= out_features= bias=True _ module named_modules yield module named_modules memo Optional set Module = None prefix str = remove_duplicate bool = True r Return iterator over all modules network yielding both name module well module itself Args memo memo store set modules already added result prefix prefix will added name module remove_duplicate whether remove duplicated module instances result Yields str Module Tuple name module Note Duplicate modules returned only once In following example ` ` l ` ` will returned only once Example l = nn Linear net = nn Sequential l l idx m enumerate net named_modules print idx - m - Sequential Linear in_features= out_features= bias=True Linear in_features= out_features= bias=True - Linear in_features= out_features= bias=True memo None memo = set memo remove_duplicate memo add yield prefix name module _modules items module None continue submodule_prefix = prefix + prefix + name yield module named_modules memo submodule_prefix remove_duplicate train mode bool = True - Self r Set module training mode This has effect only certain modules See documentation particular modules details their behaviors training evaluation mode i e whether they affected e g ` Dropout ` ` BatchNorm ` etc Args mode bool whether set training mode ` ` True ` ` evaluation mode ` ` False ` ` Default ` ` True ` ` Returns Module isinstance mode bool raise ValueError training mode expected boolean training = mode module children module train mode eval - Self r Set module evaluation mode This has effect only certain modules See documentation particular modules details their behaviors training evaluation mode i e whether they affected e g ` Dropout ` ` BatchNorm ` etc This equivalent meth ` train False torch nn Module train ` See ref ` locally-disable-grad-doc ` comparison between ` eval ` several similar mechanisms may confused Returns Module train False requires_grad_ requires_grad bool = True - Self r Change autograd should record operations parameters module This method sets parameters attr ` requires_grad ` attributes in-place This method helpful freezing part module finetuning training parts model individually e g GAN training See ref ` locally-disable-grad-doc ` comparison between ` requires_grad_ ` several similar mechanisms may confused Args requires_grad bool whether autograd should record operations parameters module Default ` ` True ` ` Returns Module p parameters p requires_grad_ requires_grad zero_grad set_to_none bool = True - None r Reset gradients all model parameters See similar function under ` torch optim Optimizer ` more context Args set_to_none bool instead setting zero set grads None See meth ` torch optim Optimizer zero_grad ` details getattr _is_replica False warnings warn Calling zero_grad module created nn DataParallel has no effect The parameters copied differentiable manner original module This means they leaf nodes autograd so don t accumulate gradients If you need gradients your forward method consider using autograd grad instead stacklevel= p parameters p grad None set_to_none p grad = None p grad grad_fn None p grad detach_ p grad requires_grad_ False p grad zero_ share_memory - Self r See meth ` torch Tensor share_memory_ ` _apply lambda t t share_memory_ _get_name __class__ __name__ extra_repr - str r Return extra representation module To print customized extra information you should re-implement method your own modules Both single-line multi-line strings acceptable __repr__ - str We treat extra repr like sub-module one item per line extra_lines = extra_repr = extra_repr empty string will split into list extra_repr extra_lines = extra_repr split \n child_lines = key module _modules items mod_str = repr module mod_str = _addindent mod_str child_lines append + key + + mod_str lines = extra_lines + child_lines main_str = _get_name + lines simple one-liner info which most builtin Modules will use len extra_lines == child_lines main_str += extra_lines main_str += \n + \n join lines + \n main_str += main_str __dir__ module_attrs = dir __class__ attrs = list __dict__ keys parameters = list _parameters keys modules = list _modules keys buffers = list _buffers keys keys = module_attrs + attrs + parameters + modules + buffers Eliminate attrs legal Python variable names keys = key key keys key isdigit sorted keys _replicate_for_data_parallel replica = __new__ type replica __dict__ = __dict__ copy replicas do have parameters themselves replicas reference original module replica _parameters = replica _buffers = replica _buffers copy replica _modules = replica _modules copy replica _is_replica = True type ignore assignment replica compile args kwargs Compile Module s forward using func ` torch compile ` This Module s ` __call__ ` method compiled all arguments passed as-is func ` torch compile ` See func ` torch compile ` details arguments function _compiled_call_impl = torch compile _call_impl args kwargs