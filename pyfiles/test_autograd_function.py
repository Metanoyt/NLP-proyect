Owner s module dynamo flake noqa B copy math dataclasses dataclass torch torch _dynamo test_case torch _dynamo testing torch _dynamo utils torch testing _internal triton_utils HAS_GPU requires_gpu device_type = acc type acc = torch accelerator current_accelerator True cpu HAS_GPU triton torch testing _internal triton_utils add_kernel CustomFunc torch autograd Function staticmethod forward ctx foo foo + foo staticmethod backward ctx grad_output grad_output CustomFunc torch autograd Function Test there graph break forward function staticmethod forward ctx foo result = foo + foo torch _dynamo graph_break result = result + foo ctx save_for_backward result result staticmethod backward ctx grad_output result = ctx saved_tensors grad_output math sqrt result numel Module torch nn Module forward foo CustomFunc apply foo Module torch nn Module __init__ - None super __init__ fn = CustomFunc apply forward foo fn foo Module torch nn Module forward foo CustomFunc apply foo Module torch nn Module __init__ - None super __init__ fn = CustomFunc apply forward foo fn foo Module torch nn Module forward foo CustomFunc apply foo Module torch nn Module __init__ - None super __init__ fn = CustomFunc apply forward foo fn foo LinearFunction torch autograd Function Note forward setup_context backward staticmethods staticmethod forward input weight bias output = input mm weight t bias None output += bias unsqueeze expand_as output output staticmethod inputs Tuple all inputs passed forward output output forward setup_context ctx inputs output input weight bias = inputs ctx save_for_backward input weight bias This function has only single output so gets only one gradient staticmethod backward ctx grad_output input weight bias = ctx saved_tensors grad_input = grad_weight = grad_bias = None ctx needs_input_grad grad_input = grad_output mm weight ctx needs_input_grad grad_weight = grad_output t mm input bias None ctx needs_input_grad grad_bias = grad_output sum grad_input grad_weight grad_bias ModuleLinear torch nn Module forward input weight bias=None LinearFunction apply input weight bias MaterializingGradFunction torch autograd Function staticmethod forward ctx x ctx set_materialize_grads False x clone x clone staticmethod backward ctx grad_out grad_out grad_out grad_out MaterializingGradModule torch nn Module forward x MaterializingGradFunction apply x CustomFuncBwdPrintGraphBreak torch autograd Function staticmethod forward ctx foo torch add foo foo staticmethod backward ctx grad_output print graph break grad_output CustomFuncBwdPrintModule torch nn Module forward x CustomFuncBwdPrintGraphBreak apply x CustomFuncStrideBwd torch autograd Function staticmethod forward ctx foo torch add foo foo staticmethod backward ctx grad_output grad_output grad_output stride - CustomFuncStrideModule torch nn Module forward x CustomFuncStrideBwd apply x CustomFuncSaveForBwd torch autograd Function staticmethod forward ctx foo result = foo + foo result = result + foo ctx save_for_backward result result staticmethod backward ctx grad_output result = ctx saved_tensors grad_output math sqrt result numel SaveForBwdModule torch nn Module forward foo CustomFuncSaveForBwd apply foo ContextSaveAndMark torch autograd Function staticmethod forward ctx x torch no_grad ctx save_for_backward x ctx mark_non_differentiable x x staticmethod backward ctx grad_output grad_output ContextMarkAndSave torch autograd Function staticmethod forward ctx x torch no_grad ctx mark_non_differentiable x ctx save_for_backward x x staticmethod backward ctx grad_output grad_output ModuleWithGradFunc torch nn Module __init__ func super __init__ f = func apply forward x f x AutogradFunctionTests torch _dynamo test_case TestCase Sound behaviors tested working capture test_autograd_function_equivalence grad True False i range torch _dynamo reset model = globals f Module i opt_model = torch compile model backend= eager assertTrue torch allclose opt_model torch ones requires_grad=grad torch tensor requires_grad=grad test_autograd_function_has_graph_break grad True False x = torch randn requires_grad=grad model Module Module torch _dynamo reset cnts = torch _dynamo testing CompileCounter opt_model = torch compile model backend=cnts _ range ref = model x res = opt_model x assertTrue torch allclose ref res assertEqual cnts frame_count test_linear_setup_context model = ModuleLinear opt_model = torch compile model backend= eager fullgraph=True input = torch randn dtype=torch double requires_grad=True weight = torch randn dtype=torch double requires_grad=True eager_result = model input weight optim_result = opt_model input weight assertEqual optim_result eager_result test_materialize_grad model = MaterializingGradModule opt_model = torch compile model backend= eager x = torch randn dtype=torch double requires_grad=True optim_result = opt_model x eager_result = model x assertEqual optim_result eager_result test_print_in_bwd model = CustomFuncBwdPrintModule opt_model = torch compile model backend= eager fullgraph=True x = torch randn dtype=torch double requires_grad=True assertRaisesRegex torch _dynamo exc Unsupported Dynamo does know how trace builtin operator ` print ` opt_model x test_stride_in_bwd torch _dynamo utils counters clear cnt = torch _dynamo testing CompileCounter model = CustomFuncStrideModule opt_model = torch compile backend=cnt fullgraph=True model x = torch randn dtype=torch double requires_grad=True x = copy deepcopy x ref = model x ref backward x clone detach res = opt_model x res backward x clone detach assertEqual ref res assertEqual x grad x grad assertEqual cnt frame_count test_enum_arg enum Enum SomeEnum Enum A = B = Foo torch autograd Function staticmethod forward ctx x e e SomeEnum A x sin x cos staticmethod backward ctx g g torch compile backend= eager fullgraph=True f x enum output = Foo apply x enum output x = torch tensor requires_grad=True y = f x SomeEnum A assertEqual y x sin test_save_for_bwd model = SaveForBwdModule opt_model = torch compile model backend= eager fullgraph=True x = torch randn dtype=torch double requires_grad=True opt_model x test_allow_in_graph torch _dynamo utils counters clear cnt = torch _dynamo testing CompileCounter torch _dynamo allow_in_graph AllowInGraphFunc torch autograd Function staticmethod forward ctx x torch _dynamo graph_break ctx x = x size x staticmethod backward ctx grad_out grad_out ctx x torch compile backend=cnt fullgraph=True fn x AllowInGraphFunc apply x x = torch rand requires_grad=True result = fn x assertEqual result AllowInGraphFunc apply x assertEqual cnt frame_count test_once_differentiable torch autograd function once_differentiable torch _dynamo utils counters clear cnt = torch _dynamo testing CompileCounter ScaleGradient torch autograd Function staticmethod forward ctx x x staticmethod once_differentiable backward ctx grad grad torch compile backend=cnt fullgraph=True fn x ScaleGradient apply x x = torch randn requires_grad=True result = fn x assertEqual result ScaleGradient apply x assertEqual cnt frame_count test_classmethod Shake torch autograd Function classmethod forward cls ctx foo foo + foo classmethod backward cls ctx grad_output grad_output f x Shake apply x x = torch randn requires_grad=True opt_m = torch compile backend= eager f opt_m x test_function_context_save_and_mark mod = ModuleWithGradFunc ContextSaveAndMark args kwargs = torch rand before = mod args kwargs torch _dynamo reset compiled_model = torch compile mod backend= eager after = compiled_model args kwargs assertEqual before after test_function_context_mark_and_save mod = ModuleWithGradFunc ContextMarkAndSave args kwargs = torch rand before = mod args kwargs torch _dynamo reset compiled_model = torch compile mod backend= eager after = compiled_model args kwargs assertEqual before after test_multi_output torch _dynamo utils counters clear cnt = torch _dynamo testing CompileCounter Foo torch autograd Function staticmethod forward ctx x x clone x clone staticmethod backward ctx grad grad grad + grad torch compile backend=cnt fullgraph=True f x Foo apply x x = torch randn requires_grad=True result = f x assertEqual result Foo apply x assertEqual cnt frame_count test_data_in_bwd Foo torch autograd Function staticmethod forward ctx input_tensor ctx save_for_backward input_tensor input_tensor staticmethod backward ctx grad_output input_tensor = ctx saved_tensors Modify gradient using data Dangerous Breaks autograd tracking modified_grad = grad_output clone modified_grad data input_tensor data = Zero-out gradients negative inputs modified_grad torch compile backend= aot_eager fullgraph=True fn x Foo apply x x = torch tensor - requires_grad=True res = fn x assertEqual res Foo apply x res sum backward assertEqual x grad torch tensor test_requires_grad_in_bwd Foo torch autograd Function staticmethod forward ctx x ctx save_for_backward x torch sin x + staticmethod backward ctx grad_output x = ctx saved_tensors grad_output requires_grad grad_output torch sin x + Wrong gradient we should never get here grad_output torch cos x + torch compile backend= aot_eager fullgraph=True fn x Foo apply x x = torch tensor requires_grad=True res = fn x assertEqual res Foo apply x res sum backward assertEqual x grad torch cos x + test_amp_custom_fwd_bwd torch _dynamo utils counters clear cnt = torch _dynamo testing CompileCounter MyMM torch autograd Function staticmethod torch amp custom_fwd device_type=device_type forward ctx b ctx save_for_backward b mm b staticmethod torch amp custom_bwd device_type=device_type backward ctx grad b = ctx saved_tensors grad mm b t t mm grad torch compile backend=cnt fullgraph=True fn b MyMM apply b = torch randn dtype=torch float requires_grad=True grad = clone res = fn res backward grad assertEqual res MyMM apply assertEqual cnt frame_count test_set_materialize_grads_no_graph_break MulY torch autograd Function staticmethod forward ctx x ctx set_materialize_grads True x staticmethod backward ctx grad_out grad_out torch compile backend= eager fullgraph=True f x MulY apply x x = torch tensor requires_grad=True result = f x result sum backward assertEqual result MulY apply x assertEqual x grad test_user_defined_object_as_input cnt = torch _dynamo testing CompileCounterWithBackend aot_eager dataclass Weird x int b torch Tensor c torch Tensor Foo torch autograd Function staticmethod forward ctx x torch Tensor weird Weird z torch Tensor ctx save_for_backward weird b weird c weird b weird c x clone staticmethod backward ctx grad b c = ctx saved_tensors grad b c None grad torch compile backend=cnt fullgraph=True f x weird z Foo apply x weird z x = torch tensor requires_grad=True weird = Weird torch tensor requires_grad=True torch tensor z = torch tensor requires_grad=True result = f x weird z result sum backward assertEqual result Foo apply x weird z assertEqual x grad assertEqual z grad assertEqual weird b grad None check Dynamo captured graph correct actual_graph = torch _dynamo testing normalize_gm cnt graphs print_readable print_output=False assertExpectedInline actual_graph \ GraphModule torch nn Module forward L_x_ f L_z_ f L_weird_b f L_weird_c f l_x_ = L_x_ l_z_ = L_z_ l_weird_b = L_weird_b l_weird_c = L_weird_c fwd_body_ = fwd_body_ bwd_body_ = bwd_body_ autograd_function_apply f = torch ops higher_order autograd_function_apply fwd_body_ bwd_body_ l_x_ l_z_ l_weird_b l_weird_c args_tensor_mask = True False True non_differentiable_idx = fwd_body_ = bwd_body_ = l_x_ = l_z_ = l_weird_b = l_weird_c = None autograd_function_apply fwd_body_ torch nn Module forward ctx torch autograd function Function x f z f l_weird_b f l_weird_c f _set_grad_enabled = torch _C _set_grad_enabled False _set_grad_enabled = None mul f = l_weird_b l_weird_c clone f = x clone x = None mul_ f = mul clone mul = clone = None _set_grad_enabled_ = torch _C _set_grad_enabled True _set_grad_enabled_ = None mul_ l_weird_b l_weird_c bwd_body_ torch nn Module forward ctx torch autograd function Function grad f l_weird_b f l_weird_c f _set_grad_enabled = torch _C _set_grad_enabled False _set_grad_enabled = None mul f = grad l_weird_b l_weird_b = None mul_ f = mul l_weird_c mul = l_weird_c = None mul_ f = grad grad = None _set_grad_enabled_ = torch _C _set_grad_enabled True _set_grad_enabled_ = None mul_ mul_ test_tensor_list_as_input Foo torch autograd Function staticmethod forward ctx x tl ctx save_for_backward tl tl x clone tl + tl staticmethod backward ctx grad tl tl = ctx saved_tensors grad tl + tl None torch compile backend= aot_eager fullgraph=True f x tl Foo apply x tl x = torch tensor requires_grad=True tl = torch tensor requires_grad=True torch tensor requires_grad=True result = f x tl result sum backward assertEqual result Foo apply x tl assertEqual x grad assertEqual tl grad None assertEqual tl grad None test_multiple_different_non_tensor_inputs dataclass Weird x int b torch Tensor c torch Tensor Foo torch autograd Function staticmethod forward ctx x weird z tl ctx save_for_backward weird b weird c tl tl x clone weird b weird c tl staticmethod backward ctx grad b c tl _ = ctx saved_tensors grad b c tl None grad None torch compile backend= aot_eager fullgraph=True f x weird z tl Foo apply x weird z tl x = torch tensor requires_grad=True weird = Weird torch tensor requires_grad=True torch tensor requires_grad=True z = torch tensor requires_grad=True tl = torch tensor requires_grad=True torch tensor requires_grad=True result = f x weird z tl result sum backward assertEqual result Foo apply x weird z tl assertEqual x grad assertEqual z grad assertEqual weird b grad None assertEqual weird c grad None assertEqual tl grad None assertEqual tl grad None test_backward_returns_none_for_tensor_input Foo torch autograd Function staticmethod forward ctx x y ctx save_for_backward y x clone y staticmethod backward ctx grad y = ctx saved_tensors grad y None torch compile backend= aot_eager fullgraph=True f x y Foo apply x y x = torch tensor requires_grad=True y = torch tensor requires_grad=True result = f x y result sum backward assertEqual result Foo apply x y assertEqual x grad assertEqual y grad None test_function_with_bound_free_variable LowerBound torch autograd Function staticmethod forward ctx inputs bound ctx save_for_backward inputs inputs new_ones bound inputs clamp min=bound staticmethod backward ctx grad_output inputs bound = ctx saved_tensors inputs = bound grad_output None MyMod torch nn Module __init__ - None super __init__ gamma = torch nn Parameter torch rand forward x gamma = LowerBound apply gamma x + gamma mod = MyMod args kwargs = torch rand before = mod args kwargs compiled_model = torch compile mod backend= eager after = compiled_model args kwargs assertEqual before after test_forward_returns_constant Foo torch autograd Function staticmethod forward ctx x x Tensor list integers staticmethod backward ctx grad_output grad_output grad_output torch compile backend= aot_eager fullgraph=True f x Foo apply x x = torch tensor requires_grad=True result = f x result sum backward assertEqual result Foo apply x I pulled all these test cases test_autograd py In future we should make Dynamo test suite actually run test_autograd py s disabled right now delete these test_smoke_from_test_autograd mult x x prod dim=- prod dim=- Mult torch autograd Function staticmethod forward ctx x y = mult x ctx save_for_backward x y y staticmethod backward ctx grad_output x y = ctx saved_tensors grad_output y None None x mult = Mult apply Double torch autograd Function staticmethod forward ctx x y = x ctx save_for_backward x y y staticmethod backward ctx grad_output x _ = ctx saved_tensors grad_output x equivalent uses output forward backward Double Double staticmethod backward ctx grad_output x y = ctx saved_tensors grad_output y x double = Double apply double = Double apply Identity torch autograd Function staticmethod forward ctx b + b staticmethod backward ctx grad_a grad_b grad_a + grad_b grad_b MyFunc torch autograd Function staticmethod forward ctx inp inp clone staticmethod backward ctx gO torch tensor float nan expand run_fn noqa F out = MyFunc apply out sum MyFn torch autograd Function staticmethod forward ctx inp inp view_as inp staticmethod backward ctx grad grad MyAdder torch autograd Function staticmethod forward ctx b add_ b ctx mark_dirty staticmethod backward ctx grad grad grad InplaceMul torch autograd Function staticmethod forward ctx x result = x mul_ ctx mark_dirty result result staticmethod backward ctx grad_output pass staticmethod jvp ctx x_t jvp_err noqa F x_t x_t mul_ MyFn torch autograd Function staticmethod forward ctx x y x + y x staticmethod vjp ctx gO gO gO + gO gO staticmethod jvp ctx x_t y_t x_t + y_t fn x_t noqa F MyFn torch autograd Function staticmethod forward ctx inp inplace view = inp clone inplace view += view staticmethod backward ctx grad grad None test x = torch ones requires_grad_ mult x x = torch tensor double requires_grad_ double x double x x = torch randn requires_grad=True y = torch randn requires_grad=True Identity apply x y = torch rand b = torch rand requires_grad=True MyFn apply = torch ones requires_grad=True b = torch ones requires_grad=True c = MyAdder apply clone b c sum backward z = torch tensor requires_grad=True x = z clone y = InplaceMul apply x = torch tensor dtype=torch double requires_grad=True b = torch tensor dtype=torch double requires_grad=True c = torch tensor dtype=torch double d = torch tensor dtype=torch double MyFn apply b MyFn apply c d base = torch rand requires_grad=True MyFn apply base False test opt_test = torch compile test backend= eager opt_test test_tensor_subclass_intermediary_input FooTensor torch Tensor staticmethod __new__ cls data config scale = torch Tensor _make_wrapper_subclass cls config strides=config storage_offset=config dtype=config layout=config requires_grad=config device=data device _data = data _config = config _scale = scale __repr__ FooTensor __tensor_flatten__ _data _config _scale staticmethod __tensor_unflatten__ tensors metadatas outer_size outer_stride FooTensor tensors _data metadatas metadatas classmethod __torch_dispatch__ cls func types args kwargs=None handling clone view so dynamo fakefication passes s intended handling user code func == torch ops aten clone default FooTensor args _data clone args _config args _scale func == torch ops aten view default new_data = args _data view args FooTensor new_data args _config args _scale raise NotImplementedError foo_autograd_fn torch autograd Function staticmethod forward ctx x access some data ` x ` where ` x ` tensor subclass x = x _data + create tensor subclass within torch autograd Function x = FooTensor x x _config x _scale x _data staticmethod backward ctx g g x_ref = torch randn requires_grad_ True x = copy deepcopy x_ref scale = torch tensor Weird needed having breaks lot things torch _dynamo allow_in_graph FooTensor foo x scale config = x size x stride x storage_offset x dtype x layout x requires_grad x = FooTensor x config scale x = foo_autograd_fn apply x x y_ref = foo x_ref scale y_ref sum backward foo_opt = torch compile foo backend= eager y = foo_opt x scale y sum backward assertEqual y y_ref assertEqual x grad x_ref grad test_assert_is_contiguous_after_matmul LinearFunction torch autograd Function staticmethod forward ctx x weight ctx save_for_backward x weight y = x matmul weight t y staticmethod backward ctx grad_output x weight = ctx saved_tensors grad_x = grad_output matmul weight assert grad_x is_contiguous grad_weight = grad_output transpose matmul x grad_x grad_weight fn x weight LinearFunction apply x weight x = torch randn requires_grad=True x = copy deepcopy x W = torch randn requires_grad=True W = copy deepcopy W y = fn x W y sum backward cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts y = opt_fn x W y sum backward assertEqual y y assertEqual x grad x grad assertEqual W grad W grad assertEqual cnts frame_count test_assert_is_contiguous_on_grad_output_directly LinearFunction torch autograd Function staticmethod forward ctx x weight ctx save_for_backward x weight y = x matmul weight t y staticmethod backward ctx grad_output assert grad_output is_contiguous x weight = ctx saved_tensors grad_x = grad_output matmul weight grad_weight = grad_output transpose matmul x grad_x grad_weight fn x weight LinearFunction apply x weight x = torch randn requires_grad=True x = copy deepcopy x W = torch randn requires_grad=True W = copy deepcopy W y = fn x W y backward y clone detach requires_grad_ True cnt = torch _dynamo testing CompileCounterWithBackend aot_eager opt_fn = torch compile fn backend=cnt y = opt_fn x W y backward y clone detach requires_grad_ True assertEqual y y assertEqual x grad x grad assertEqual W grad W grad Check inserted contiguous call there actual_graph = torch _dynamo testing normalize_gm cnt graphs print_readable print_output=False assertExpectedInline actual_graph \ GraphModule torch nn Module forward L_x_ f L_weight_ f l_x_ = L_x_ l_weight_ = L_weight_ fwd_body_ = fwd_body_ bwd_body_ = bwd_body_ autograd_function_apply f = torch ops higher_order autograd_function_apply fwd_body_ bwd_body_ l_x_ l_weight_ args_tensor_mask = True True non_differentiable_idx = fwd_body_ = bwd_body_ = l_x_ = l_weight_ = None autograd_function_apply fwd_body_ torch nn Module forward ctx torch autograd function Function x f weight f _set_grad_enabled = torch _C _set_grad_enabled False _set_grad_enabled = None t f = weight t y f = x matmul t t = None _set_grad_enabled_ = torch _C _set_grad_enabled True _set_grad_enabled_ = None y weight x bwd_body_ torch nn Module forward function_ctx torch autograd function Function y f weight f x f _set_grad_enabled = torch _C _set_grad_enabled False _set_grad_enabled = None contiguous f = y contiguous y = None grad_x f = contiguous matmul weight weight = None transpose f = contiguous transpose contiguous = None grad_weight f = transpose matmul x transpose = x = None _set_grad_enabled_ = torch _C _set_grad_enabled True _set_grad_enabled_ = None grad_x grad_weight test_smuggle_symint_issue_ torch autograd Function Foo Function staticmethod forward ctx x ctx x = x size x staticmethod backward ctx grad_out grad_out ctx x cnts = torch _dynamo testing CompileCounter torch compile backend=cnts fullgraph=True dynamic=True foo x Foo apply x foo torch randn requires_grad=True assertEqual cnts frame_count test_needs_input_grad cnt = torch _dynamo testing CompileCounter NeedsInputGradFunc torch autograd Function staticmethod forward ctx foo result = foo + foo ctx save_for_backward result result staticmethod torch compile backend=cnt fullgraph=True backward ctx grad_output result = ctx saved_tensors ctx needs_input_grad grad_output result sin None x = torch randn requires_grad=True NeedsInputGradFunc apply x sum backward assertEqual x grad shape x shape assertEqual cnt frame_count assertEqual cnt op_count test_repeated_save_for_backward_calls torch autograd Function Foo Function staticmethod forward ctx x y ctx save_for_backward x ctx save_for_backward x y x y staticmethod backward ctx grad_out x y = ctx saved_tensors grad_out x grad_out y cnts = torch _dynamo testing CompileCounter foo x y Foo apply x y x_ref = torch randn requires_grad=True y_ref = torch randn requires_grad=True x_test = x_ref detach clone requires_grad_ y_test = y_ref detach clone requires_grad_ out_ref = foo x_ref y_ref out_ref sum backward out_test = torch compile foo backend=cnts x_test y_test out_test sum backward assertEqual cnts frame_count assertEqual out_ref out_test assertEqual x_ref grad x_test grad assertEqual y_ref grad y_test grad test_smuggle_tensor_and_complex_structures torch autograd Function Foo Function staticmethod forward ctx x ctx x = x ctx x = x staticmethod backward ctx grad_out x mul = grad_out ctx x i ctx x x mul = x mul i + x mul x mul cnts = torch _dynamo testing CompileCounter torch compile backend=cnts fullgraph=True dynamic=True foo x Foo apply x foo torch randn requires_grad=True assertEqual cnts frame_count test_mark_non_differentiable cnt = torch _dynamo testing CompileCounterWithBackend aot_eager torch autograd Function MyFunction Function staticmethod forward ctx x y out = x sin out = y ctx mark_non_differentiable out out out staticmethod backward ctx grad grad grad cos grad torch compile backend=cnt fullgraph=True fn x y MyFunction apply x y x = torch tensor requires_grad=True y = torch tensor requires_grad=True ref ref = MyFunction apply x y res res = fn x y assertEqual ref res assertEqual ref res Ensure out requires gradients out does assertTrue ref requires_grad assertTrue res requires_grad assertFalse ref requires_grad assertFalse res requires_grad res sum backward check Dynamo captured graph correct actual_graph = torch _dynamo testing normalize_gm cnt graphs print_readable print_output=False assertExpectedInline actual_graph \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ fwd_body_ = fwd_body_ bwd_body_ = bwd_body_ autograd_function_apply = torch ops higher_order autograd_function_apply fwd_body_ bwd_body_ l_x_ l_y_ args_tensor_mask = True True non_differentiable_idx = fwd_body_ = bwd_body_ = l_x_ = l_y_ = None getitem f = autograd_function_apply getitem_ f = autograd_function_apply autograd_function_apply = None getitem getitem_ fwd_body_ torch nn Module forward ctx torch autograd function Function x f y f _set_grad_enabled = torch _C _set_grad_enabled False _set_grad_enabled = None out f = x sin x = None out f = y y = None _set_grad_enabled_ = torch _C _set_grad_enabled True _set_grad_enabled_ = None out out bwd_body_ torch nn Module forward ctx torch autograd function Function grad f grad f _set_grad_enabled = torch _C _set_grad_enabled False _set_grad_enabled = None cos f = grad cos grad = None mul f = grad grad = None _set_grad_enabled_ = torch _C _set_grad_enabled True _set_grad_enabled_ = None cos mul test_mark_multi_output_non_differentiable torch autograd Function MyFunction Function staticmethod forward ctx x y z out = x sin out = y out = z + ctx mark_non_differentiable out out out out out staticmethod backward ctx grad grad grad grad cos grad grad torch compile backend= aot_eager fullgraph=True fn x y z MyFunction apply x y z x = torch tensor requires_grad=True y = torch tensor requires_grad=True z = torch tensor requires_grad=True ref ref ref = MyFunction apply x y z res res res = fn x y z assertEqual ref res assertEqual ref res assertEqual ref res Ensure out requires gradients out does assertTrue ref requires_grad assertTrue res requires_grad assertFalse ref requires_grad assertFalse res requires_grad assertFalse ref requires_grad assertFalse res requires_grad res sum backward test_default_values torch autograd Function Foo Function staticmethod forward ctx x alpha= x staticmethod backward ctx grad_out grad_out torch compile foo x Foo apply x Make sure guards default values do crash foo torch randn foo torch randn requires_grad=True test_fwd_no_grad autograd Function forward should traced called under no_grad mode torch exp out= arguments don t support automatic differentiation so can t traced called under grad mode throwing RuntimeError therefore unit test ensures fwd under no_grad mode Foo torch autograd Function staticmethod forward ctx inputs torch exp inputs out=inputs inputs staticmethod backward ctx grad_output None torch compile backend= eager fullgraph=True f x Foo apply x x = torch randn requires_grad=True x = x clone assertEqual f x Foo apply x https github com pytorch pytorch issues test_fwd_propogation_correctness MyCube torch autograd Function staticmethod forward ctx x result = x dx = x ctx save_for_backward x dx result dx staticmethod backward ctx grad_output grad_dx x dx = ctx saved_tensors result = grad_output dx + grad_dx x Intentionally wrong value test backward triggered twice Since first MyCube apply returns values w o requires_grad=True backward would only triggered once first MyCube apply call second MyCube apply inlined Dynamo corresponding backward would generated autograd engine result torch compile backend= eager fullgraph=True fn x x _ = MyCube apply x x _ = MyCube apply x x inp = torch ones requires_grad=True out = fn inp out sum backward assertEqual out inp assertEqual inp grad torch tensor test_tuple_arg cnt = torch _dynamo testing CompileCounter TupleArgFunc torch autograd Function staticmethod forward ctx x shape ctx save_for_backward torch randn shape x + staticmethod backward ctx grad_output result = ctx saved_tensors result None torch compile backend=cnt fullgraph=True fn TupleArgFunc apply x shape shape = x = torch randn shape requires_grad=True out = fn out sum backward assertEqual out x + assertEqual x grad shape shape assertEqual cnt frame_count assertEqual cnt op_count requires_gpu test_triton_kernel_basic Add torch autograd Function staticmethod forward ctx x y ctx save_for_backward x y output = torch zeros_like x n_elements = output numel grid = lambda meta noqa E triton cdiv n_elements meta BLOCK_SIZE add_kernel grid x y output n_elements BLOCK_SIZE= output staticmethod backward ctx grad_output x y = ctx saved_tensors x grad_output y grad_output torch compile fullgraph=True backend= inductor f x y z = Add apply x y z x = torch randn device=device_type requires_grad=True y = torch randn device=device_type requires_grad=True z = f x y loss = z sum loss backward assertEqual x + y z requires_gpu test_triton_kernel_multiple_out Add torch autograd Function staticmethod forward ctx x y ctx save_for_backward x y ctx t = x ctx t = y output = torch zeros_like x n_elements = output numel grid = lambda meta noqa E triton cdiv n_elements meta BLOCK_SIZE add_kernel grid x y output n_elements BLOCK_SIZE= output x staticmethod backward ctx grad_output old_x x y = ctx saved_tensors x = ctx t y = ctx t old_x x x grad_output y y grad_output torch compile fullgraph=True backend= inductor f x y z = Add apply x y z x = torch randn device=device_type requires_grad=True y = torch randn device=device_type requires_grad=True z _ = f x y loss = z sum loss backward assertEqual x + y z __name__ == __main__ torch _dynamo test_case run_tests run_tests