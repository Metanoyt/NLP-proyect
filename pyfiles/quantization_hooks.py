mypy allow-untyped-defs torch torch distributed dist torch nn _quantize_per_tensor_backend x scale zero_point y = torch round x scale + zero_point y = torch clamp y torch uint y _dequantize_per_tensor_backend y scale zero_point x = scale y torch float - zero_point x _quantize_per_channel_backend x scale zero_point y = torch zeros x size device=x device i range x size y i = torch round x i scale i + zero_point i y = torch clamp y torch uint y _dequantize_per_channel_backend y scale zero_point y = y torch float y device x = torch zeros_like y device=y device i range x size x i = scale i y i - zero_point i x _get_allgather_out_list all_gather_in_list world_size out_list = torch zeros_like all_gather_in_list device=all_gather_in_list device dtype=all_gather_in_list dtype _ range world_size out_list quantization_pertensor_hook process_group dist ProcessGroup bucket dist GradBucket - torch futures Future torch Tensor Apply ` ` torch quantize_per_tensor ` ` logic DDP using ` ` allgather ` ` protocol Workers first allgather scale zero point their own ` ` GradBucket ` ` prior quantization After all workers have information first ` ` then ` ` callback called ` ` quantize_and_allgather ` ` quantizes worker s own gradient tensor uses ` ` allgather ` ` communicate these across all workers The final ` ` then ` ` callback called ` ` dequantize_and_aggregate ` ` dequantizes aggregates each quantized gradient tensor locally returns mean warning This experimental uses ` ` allgather ` ` protocol which considerably slower than ` ` allreduce ` ` protocol It works only flattened grads Example xdoctest +SKIP ddp_model register_comm_hook process_group quantization_pertensor_hook group_to_use = process_group process_group None dist group WORLD rank = process_group rank process_group None dist get_rank pyrefly ignore missing-attribute world_size = group_to_use size tensor = bucket buffer myObserver = torch ao quantization MinMaxObserver tensor device myObserver tensor s z = myObserver calculate_qparams s_and_z = torch FloatTensor s z tensor device all_ranks_s_and_z = _get_allgather_out_list s_and_z world_size First allgather scale zeros fut = dist all_gather all_ranks_s_and_z s_and_z group=group_to_use async_op=True get_future quantize_and_allgather fut Store scale zeros across all workers all_ranks_s_and_z = fut wait All workers quantize their own ` ` GradBucket ` ` tensors quantized_tensor = _quantize_per_tensor_backend tensor all_ranks_s_and_z rank all_ranks_s_and_z rank Allgather quantized tensors fut = dist all_gather _get_allgather_out_list quantized_tensor world_size quantized_tensor group=group_to_use async_op=True get_future fut wait dequantize_and_aggregate fut all_ranks_quantized_tensor = fut wait aggregated_dequantized_tensor = torch zeros_like all_ranks_quantized_tensor device=tensor device dtype=torch float Using previously allgathered scales zeros dequantize gradient tensors locally then aggregate them r quantized_tensor enumerate all_ranks_quantized_tensor aggregated_dequantized_tensor += _dequantize_per_tensor_backend quantized_tensor all_ranks_s_and_z r all_ranks_s_and_z r aggregated_dequantized_tensor world_size fut then quantize_and_allgather then dequantize_and_aggregate quantization_perchannel_hook process_group dist ProcessGroup bucket dist GradBucket bucket_size= - torch futures Future torch Tensor Apply ` ` torch quantize_per_channel ` ` logic DDP using ` ` allgather ` ` protocol Compared per-tensor main motivation per-channel considerably large tensors such tensor contains million elements quantizing per bucket size elements may significantly increase resolution It first splits ` ` GradBucket ` ` tensor into multiple chunks channels ` ` bucket_size ` ` elements Then workers allgather scales zero points their own ` ` GradBucket ` ` prior quantization After all workers have information first ` ` then ` ` callback called ` ` quantize_and_allgather ` ` quantizes worker s own gradient tensor uses ` ` allgather ` ` communicate these across all workers The final ` ` then ` ` callback called ` ` dequantize_and_aggregate ` ` dequantizes flattens aggregates each quantized gradient tensor locally returns mean warning This experimental uses ` ` allgather ` ` protocol which considerably slower than ` ` allreduce ` ` protocol It works only flattened grads Example xdoctest +SKIP ddp_model register_comm_hook process_group quantization_perchannel_hook group_to_use = process_group process_group None dist group WORLD rank = process_group rank process_group None dist get_rank pyrefly ignore missing-attribute world_size = group_to_use size tensor = bucket buffer tensor_in_channels = nn functional pad input=tensor pad= bucket_size - len tensor bucket_size mode= constant value= view - bucket_size tensor device myPerChannelObserver = torch ao quantization PerChannelMinMaxObserver tensor device myPerChannelObserver tensor_in_channels s_ch z_ch = myPerChannelObserver calculate_qparams s_and_z = torch stack s_ch z_ch tensor device all_ranks_s_and_z = _get_allgather_out_list s_and_z world_size First allgather scale zeros fut = dist all_gather all_ranks_s_and_z s_and_z group=group_to_use async_op=True get_future quantize_and_allgather fut Store scale zeros across all workers all_ranks_s_and_z = fut wait All workers quantize their corresponding ` ` GradBucket ` ` tensors quantized_tensor = _quantize_per_channel_backend tensor_in_channels all_ranks_s_and_z rank all_ranks_s_and_z rank Allgather quantized tensors fut = dist all_gather _get_allgather_out_list quantized_tensor world_size quantized_tensor group=group_to_use async_op=True get_future fut wait dequantize_and_aggregate fut all_ranks_quantized_tensor = fut wait aggregated_dequantized_tensor = torch zeros_like all_ranks_quantized_tensor device=tensor device dtype=torch float Using previously allgathered scales zeros dequantize gradient tensors locally then aggregate them r quantized_tensor enumerate all_ranks_quantized_tensor aggregated_dequantized_tensor += _dequantize_per_channel_backend quantized_tensor all_ranks_s_and_z r all_ranks_s_and_z r torch flatten aggregated_dequantized_tensor tensor device tensor size world_size fut then quantize_and_allgather then dequantize_and_aggregate