Owner s module optimizer __future__ annotations typing Any torch torch nn Tensor torch optim Adadelta Adagrad Adam Adamax AdamW ASGD NAdam Optimizer RAdam RMSprop Rprop SGD torch testing _internal common_utils gradcheck load_tests skipIfTorchDynamo TestCase load_tests common_utils used automatically filter tests sharding sandcastle This line silences flake warnings load_tests = load_tests noqa PLW _diff_fn p grad opt_differentiable_state opt_class kwargs ignored Ignored list values ` opt_differentiable_state ` we do ` gradcheck ` correctly track state tensors function inputs because otherwise can t unpack values ` opt_differentiable_state ` dict p = p clone p grad = grad opt_differentiable_state = k v clone isinstance v torch Tensor v k v opt_differentiable_state items opt = opt_class p kwargs opt state p update opt_differentiable_state opt step p + tuple v v opt state p values isinstance v torch Tensor v requires_grad _multistep_backprop_diff_hyperparams_fn params Tensor grad Tensor opt_differentiable_state dict str Any opt_class type Optimizer kwargs dict str Any ignored Any - tuple Tensor assert kwargs differentiable True Only call test function when differentiable=True params = params clone params grad = grad opt_differentiable_state = k v clone isinstance v torch Tensor v k v opt_differentiable_state items This copy necessary so update line doesn t overwrite original kwargs values kwargs = kwargs copy Have pass beta beta separately so they re passed Tensors tuple recognized gradcheck beta kwargs beta kwargs Prevent just one beta kwarg being passed assert beta kwargs beta kwargs Both betas should defined kwargs kwargs update betas kwargs pop beta kwargs pop beta kwargs update k v clone isinstance v torch Tensor v k v kwargs items differentiable_kwargs = v v kwargs values isinstance v torch Tensor v requires_grad + list kwargs betas betas kwargs criterion = nn MSELoss optimizer = opt_class params kwargs optimizer state params update opt_differentiable_state Simple x y pair x = torch tensor dtype=torch float y = torch tensor dtype=torch float _ range loss = criterion x torch sum params y loss backward inputs= params create_graph=True optimizer step optimizer zero_grad meta_loss = loss meta_loss backward inputs= differentiable_kwargs create_graph=True Extra check make sure test properly computed gradient all kwargs kwarg differentiable_kwargs assert kwarg grad None meta_loss + tuple v v optimizer state params values isinstance v torch Tensor v requires_grad + tuple differentiable_kwargs skipIfTorchDynamo Differentiable optimizers supported TestDifferentiableOptimizer TestCase test_sgd p = torch rand requires_grad=True dtype=torch float grad = torch rand requires_grad=True dtype=torch float mbuff = torch rand requires_grad=True dtype=torch float state = momentum_buffer mbuff gradcheck _diff_fn p grad state SGD lr differentiable True state values test_adam state = p = torch rand requires_grad=True dtype=torch float grad = torch rand requires_grad=True dtype=torch float ` step ` continuous variable even though we define float so shouldn t require gradients state step = torch tensor requires_grad=False dtype=torch float state exp_avg = torch rand requires_grad=True dtype=torch float state exp_avg_sq = torch rand requires_grad=True dtype=torch float state max_exp_avg_sq = torch rand requires_grad=True dtype=torch float gradcheck _diff_fn p grad state Adam lr differentiable True amsgrad True state values test_rmsprop state = p = torch rand requires_grad=True dtype=torch float grad = torch rand requires_grad=True dtype=torch float state step = torch zeros dtype=torch float state square_avg = torch rand requires_grad=True dtype=torch float state momentum_buffer = torch rand requires_grad=True dtype=torch float This can cause issues large values nan due sqrt ops state grad_avg = e- torch rand requires_grad=True dtype=torch float gradcheck _diff_fn p grad state RMSprop lr maximize True momentum differentiable True centered True weight_decay state values test_adadelta state = p = torch rand requires_grad=True dtype=torch float grad = torch rand requires_grad=True dtype=torch float ` step ` continuous variable even though we define float so shouldn t require gradients state step = torch tensor requires_grad=False dtype=torch float state square_avg = torch rand requires_grad=True dtype=torch float state acc_delta = torch rand requires_grad=True dtype=torch float gradcheck _diff_fn p grad state Adadelta lr weight_decay differentiable True state values test_adagrad state = p = torch rand requires_grad=True dtype=torch float grad = torch rand requires_grad=True dtype=torch float ` step ` continuous variable even though we define float so shouldn t require gradients state step = torch tensor requires_grad=False dtype=torch float state sum = torch rand requires_grad=True dtype=torch float gradcheck _diff_fn p grad state Adagrad lr weight_decay differentiable True state values test_adamax state = p = torch rand requires_grad=True dtype=torch float grad = torch rand requires_grad=True dtype=torch float ` step ` continuous variable even though we define float so shouldn t require gradients state step = torch tensor requires_grad=False dtype=torch float state exp_avg = torch rand requires_grad=True dtype=torch float state exp_inf = torch rand requires_grad=True dtype=torch float gradcheck _diff_fn p grad state Adamax lr weight_decay differentiable True state values skipIfTorchDynamo The inplace mu update fails dynamo since only happening when differentiable enabled skipping now test_asgd state = p = torch rand requires_grad=True dtype=torch float grad = torch rand requires_grad=True dtype=torch float ` step ` ` eta ` ` mu ` continuous variables even though we define them floats so they shouldn t require gradients state step = torch tensor requires_grad=False dtype=torch float state eta = torch tensor requires_grad=False dtype=torch float state mu = torch tensor requires_grad=False dtype=torch float state ax = torch rand requires_grad=True dtype=torch float gradcheck _diff_fn p grad state ASGD lr differentiable True state values test_rprop state = p = torch rand requires_grad=True dtype=torch float grad = torch rand requires_grad=True dtype=torch float ` step ` continuous variable even though we define float so shouldn t require gradients state step = torch tensor requires_grad=False dtype=torch float state prev = torch rand requires_grad=True dtype=torch float state step_size = torch rand requires_grad=True dtype=torch float gradcheck _diff_fn p grad state Rprop lr differentiable True state values test_adamw state = p = torch rand requires_grad=True dtype=torch float grad = torch rand requires_grad=True dtype=torch float ` step ` continuous variable even though we define float so shouldn t require gradients state step = torch tensor requires_grad=False dtype=torch float state exp_avg = torch rand requires_grad=True dtype=torch float state exp_avg_sq = torch rand requires_grad=True dtype=torch float state max_exp_avg_sq = torch rand requires_grad=True dtype=torch float gradcheck _diff_fn p grad state AdamW lr differentiable True amsgrad True state values test_nadam state = p = torch rand requires_grad=True dtype=torch float grad = torch rand requires_grad=True dtype=torch float ` step ` continuous variable even though we define float so shouldn t require gradients state step = torch tensor requires_grad=False dtype=torch float state exp_avg = torch rand requires_grad=True dtype=torch float state exp_avg_sq = torch rand requires_grad=True dtype=torch float state mu_product = torch tensor requires_grad=True dtype=torch float gradcheck _diff_fn p grad state NAdam lr differentiable True state values gradcheck _diff_fn p grad state NAdam lr decoupled_weight_decay True differentiable True state values test_radam state = p = torch rand requires_grad=True dtype=torch float grad = torch rand requires_grad=True dtype=torch float ` step ` continuous variable even though we define float so shouldn t require gradients state step = torch tensor requires_grad=False dtype=torch float state exp_avg = torch rand requires_grad=True dtype=torch float state exp_avg_sq = torch rand requires_grad=True dtype=torch float gradcheck _diff_fn p grad state RAdam lr differentiable True state values gradcheck _diff_fn p grad state RAdam lr weight_decay decoupled_weight_decay True differentiable True state values test_adam_differentiable_lr params = torch rand requires_grad=True dtype=torch float grad = torch rand_like params requires_grad=True dtype=torch float lr = torch tensor requires_grad=True dtype=torch float state = state step = torch tensor requires_grad=False dtype=torch float state exp_avg = torch rand requires_grad=True dtype=torch float state exp_avg_sq = torch rand requires_grad=True dtype=torch float state max_exp_avg_sq = torch rand requires_grad=True dtype=torch float kwargs dict str Any = lr lr differentiable True gradcheck _multistep_backprop_diff_hyperparams_fn params grad state Adam kwargs includes lr state values kwargs values test_adam_differentiable_weight_decay params = torch rand requires_grad=True dtype=torch float grad = torch rand_like params requires_grad=True dtype=torch float weight_decay = torch tensor requires_grad=True dtype=torch float state = state step = torch tensor requires_grad=False dtype=torch float state exp_avg = torch rand requires_grad=True dtype=torch float state exp_avg_sq = torch rand requires_grad=True dtype=torch float state max_exp_avg_sq = torch rand requires_grad=True dtype=torch float kwargs dict str Any = weight_decay weight_decay differentiable True gradcheck _multistep_backprop_diff_hyperparams_fn params grad state Adam kwargs includes weight_decay state values kwargs values test_adam_differentiable_betas params = torch rand requires_grad=True dtype=torch float grad = torch rand_like params requires_grad=True dtype=torch float lr = torch tensor requires_grad=True dtype=torch float betas = torch tensor requires_grad=True dtype=torch float torch tensor requires_grad=True dtype=torch float state = state step = torch tensor requires_grad=False dtype=torch float state exp_avg = torch rand requires_grad=True dtype=torch float state exp_avg_sq = torch rand requires_grad=True dtype=torch float state max_exp_avg_sq = torch rand requires_grad=True dtype=torch float Have pass beta beta separately so they re passed Tensors tuple recognized gradcheck In test called kwargs update betas beta beta kwargs dict str Any = beta betas beta betas lr lr differentiable True gradcheck _multistep_backprop_diff_hyperparams_fn params grad state Adam kwargs includes betas state values kwargs values test_adam_differentiable_all_hyperparams params = torch rand requires_grad=True dtype=torch float grad = torch rand_like params requires_grad=True dtype=torch float lr = torch tensor requires_grad=True dtype=torch float weight_decay = torch tensor requires_grad=True dtype=torch float betas = torch tensor requires_grad=True dtype=torch float torch tensor requires_grad=True dtype=torch float state = state step = torch tensor requires_grad=False dtype=torch float state exp_avg = torch rand requires_grad=True dtype=torch float state exp_avg_sq = torch rand requires_grad=True dtype=torch float state max_exp_avg_sq = torch rand requires_grad=True dtype=torch float Have pass beta beta separately so they re passed Tensors tuple recognized gradcheck In test called kwargs update betas beta beta kwargs dict str Any = lr lr weight_decay weight_decay beta betas beta betas differentiable True gradcheck _multistep_backprop_diff_hyperparams_fn params grad state Adam kwargs includes betas state values kwargs values test_adamw_differentiable_lr params = torch rand requires_grad=True dtype=torch float grad = torch rand_like params requires_grad=True dtype=torch float lr = torch tensor requires_grad=True dtype=torch float state = state step = torch tensor requires_grad=False dtype=torch float state exp_avg = torch rand requires_grad=True dtype=torch float state exp_avg_sq = torch rand requires_grad=True dtype=torch float state max_exp_avg_sq = torch rand requires_grad=True dtype=torch float kwargs dict str Any = lr lr differentiable True gradcheck _multistep_backprop_diff_hyperparams_fn params grad state AdamW kwargs includes lr state values kwargs values test_adamw_differentiable_weight_decay params = torch rand requires_grad=True dtype=torch float grad = torch rand_like params requires_grad=True dtype=torch float weight_decay = torch tensor requires_grad=True dtype=torch float state = state step = torch tensor requires_grad=False dtype=torch float state exp_avg = torch rand requires_grad=True dtype=torch float state exp_avg_sq = torch rand requires_grad=True dtype=torch float state max_exp_avg_sq = torch rand requires_grad=True dtype=torch float kwargs dict str Any = weight_decay weight_decay differentiable True gradcheck _multistep_backprop_diff_hyperparams_fn params grad state AdamW kwargs includes weight_decay state values kwargs values test_adamw_differentiable_betas params = torch rand requires_grad=True dtype=torch float grad = torch rand_like params requires_grad=True dtype=torch float betas = torch tensor requires_grad=True dtype=torch float torch tensor requires_grad=True dtype=torch float state = state step = torch tensor requires_grad=False dtype=torch float state exp_avg = torch rand requires_grad=True dtype=torch float state exp_avg_sq = torch rand requires_grad=True dtype=torch float state max_exp_avg_sq = torch rand requires_grad=True dtype=torch float Have pass beta beta separately so they re passed Tensors tuple recognized gradcheck In test called kwargs update betas beta beta kwargs dict str Any = beta betas beta betas differentiable True gradcheck _multistep_backprop_diff_hyperparams_fn params grad state AdamW kwargs includes betas state values kwargs values test_adamw_differentiable_all_hyperparams params = torch rand requires_grad=True dtype=torch float grad = torch rand_like params requires_grad=True dtype=torch float lr = torch tensor requires_grad=True dtype=torch float weight_decay = torch tensor requires_grad=True dtype=torch float betas = torch tensor requires_grad=True dtype=torch float torch tensor requires_grad=True dtype=torch float state = state step = torch tensor requires_grad=False dtype=torch float state exp_avg = torch rand requires_grad=True dtype=torch float state exp_avg_sq = torch rand requires_grad=True dtype=torch float state max_exp_avg_sq = torch rand requires_grad=True dtype=torch float Have pass beta beta separately so they re passed Tensors tuple recognized gradcheck In test called kwargs update betas beta beta kwargs dict str Any = lr lr weight_decay weight_decay beta betas beta betas differentiable True gradcheck _multistep_backprop_diff_hyperparams_fn params grad state AdamW kwargs includes betas state values kwargs values test_differentiable_lr params = torch rand requires_grad=True dtype=torch float grad = torch rand_like params requires_grad=True dtype=torch float lr = torch tensor requires_grad=True dtype=torch float mbuff = torch rand_like params requires_grad=True dtype=torch float state = momentum_buffer mbuff kwargs dict str Any = lr lr differentiable True gradcheck _multistep_backprop_diff_hyperparams_fn params grad state SGD kwargs includes lr state values kwargs values test_differentiable_weight_decay params = torch rand requires_grad=True dtype=torch float grad = torch rand_like params requires_grad=True dtype=torch float weight_decay = torch tensor requires_grad=True dtype=torch float mbuff = torch rand_like params requires_grad=True dtype=torch float state = momentum_buffer mbuff kwargs dict str Any = weight_decay weight_decay differentiable True gradcheck _multistep_backprop_diff_hyperparams_fn params grad state SGD kwargs includes weight_decay state values kwargs values test_differentiable_weight_decay_and_lr params = torch rand requires_grad=True dtype=torch float grad = torch rand_like params requires_grad=True dtype=torch float weight_decay = torch tensor requires_grad=True dtype=torch float lr = torch tensor requires_grad=True dtype=torch float mbuff = torch rand_like params requires_grad=True dtype=torch float state = momentum_buffer mbuff kwargs dict str Any = weight_decay weight_decay lr lr differentiable True gradcheck _multistep_backprop_diff_hyperparams_fn params grad state SGD kwargs includes lr weight_decay state values kwargs values __name__ == __main__ print These tests should run through test test_optim py instead