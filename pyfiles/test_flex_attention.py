Owner s module inductor flake noqa B functools json os random string tempfile unittest warnings collections namedtuple collections abc Callable contextlib contextmanager dataclasses dataclass itertools product typing Optional TypeVar Union unittest expectedFailure skip skipUnless unittest mock patch torch torch nn nn torch _dynamo testing CompileCounterWithBackend normalize_gm torch _inductor config metrics torch _inductor runtime triton_compat HAS_WARP_SPEC torch _inductor test_case TestCase InductorTestCase torch _inductor utils run_and_get_code torch nn attention SDPBackend torch nn attention experimental _paged_attention PagedAttention torch nn attention flex_attention _create_empty_block_mask _DEFAULT_SPARSE_BLOCK_SIZE _identity _mask_mod_signature _score_mod_signature _WARNINGS_SHOWN and_masks AuxOutput AuxRequest BlockMask create_block_mask flex_attention flex_attention_hop noop_mask or_masks torch testing FileCheck torch testing _internal common_utils torch testing _internal common_cuda PLATFORM_SUPPORTS_BF TEST_MULTIGPU torch testing _internal common_device_type dtypes dtypesIfCUDA dtypesIfXPU flex_attention_supported_platform supported_platform instantiate_device_type_tests largeTensorTest skipCPUIf skipCUDAIf skipXPUIf torch testing _internal inductor_utils HAS_GPU torch utils _triton has_triton has_triton_tma_device Use decorator only when hitting Triton bugs H running_on_a _only = skipUnless torch cuda is_available has_triton torch cuda get_device_capability == torch version hip torch xpu is_available has_triton Requires Triton + A Triton + ROCm Triton + Intel GPU Tolerances = namedtuple Tolerances atol rtol torch set_float _matmul_precision high index = torch ops aten index Tensor = torch Tensor T = TypeVar T M = TypeVar M bound=Callable large_tensor_test_class size str device Optional Union torch device str = None - Callable type T type T decorator cls type T - type T name method list cls __dict__ items callable method name startswith test_ setattr cls name largeTensorTest size device method cls decorator contextmanager temp_float _matmul_precision precision str Temporarily set float matmul precision restore after context exited Args precision str The precision set highest high medium set_float _matmul_precision_xpu precision str precision == highest torch _C _set_onednn_allow_tf False precision == high torch _C _set_onednn_allow_tf True original_precision = torch get_float _matmul_precision try torch set_float _matmul_precision precision TEST_ON_XPU set_float _matmul_precision_xpu precision yield finally torch set_float _matmul_precision original_precision TEST_ON_XPU set_float _matmul_precision_xpu original_precision skip_on_cpu test_func Decorator skip tests supported CPU decorated_func = skipCPUIf True Not supported CPU test_func decorated_func skip_on_cuda test_func Decorator skip tests supported CUDA decorated_func = skipCUDAIf True Not supported CUDA test_func decorated_func skip_on_rocm test_func Decorator skip tests supported CUDA IS_ROCM = torch cuda is_available torch version hip decorated_func = skipCUDAIf IS_ROCM Not supported ROCM test_func decorated_func skip_on_xpu test_func Decorator skip tests supported Intel GPU decorated_func = skipXPUIf True Not supported Intel GPU test_func decorated_func rmse ref res Calculate root mean squared error ref = ref torch float res = res torch float torch sqrt torch mean torch square ref - res create_attention score_mod block_mask enable_gqa=False kernel_options=None functools partial flex_attention score_mod=score_mod block_mask=block_mask enable_gqa=enable_gqa kernel_options=kernel_options create_block_mask_test score_mod query key block_mask = create_block_mask score_mod query shape - key shape - query device block_mask dataclass DeviceConfig dtypes list torch dtype dtypes_fast list torch dtype TEST_ON_CUDA = torch cuda is_available torch utils _triton has_triton torch cuda get_device_capability = TEST_ON_XPU = torch xpu is_available torch utils _triton has_triton device_configs = HAS_GPU TEST_ON_CUDA test_device = cuda cpu TEST_ON_XPU torch _C _set_onednn_allow_tf True test_device = xpu test_device = cpu SubstringSet __init__ items items = set items __contains__ item cuda item item = cuda xpu item item = xpu item items DEVICE_SUPPORTS_BACKWARDS = SubstringSet cuda device_configs cuda = DeviceConfig dtypes= torch float torch bfloat torch float PLATFORM_SUPPORTS_BF torch float torch float dtypes_fast= torch float device_configs xpu = DeviceConfig dtypes= torch float torch bfloat torch float dtypes_fast= torch float device_configs cpu = DeviceConfig dtypes= torch float torch bfloat torch float torch backends mkldnn is_available torch ops mkldnn _is_mkldnn_bf _supported torch float dtypes_fast= torch float torch_config_string = torch __config__ show LONG_COMPILATION_ON_CPU = False CLANG torch_config_string upper compiler clang skip UT CPU due long compilation time found CI TODO check reason long compile time LONG_COMPILATION_ON_CPU = True --------- Useful score mod functions testing --------- _causal score Tensor batch Tensor head Tensor token_q Tensor token_kv Tensor - Tensor torch where token_q = token_kv score float -inf _rel_bias score Tensor batch Tensor head Tensor token_q Tensor token_kv Tensor - Tensor score + token_q - token_kv _rel_causal score Tensor batch Tensor head Tensor token_q Tensor token_kv Tensor - Tensor torch where token_q = token_kv score + token_q - token_kv float -inf _generate_alibi_bias num_heads int _alibi_bias score Tensor batch Tensor head Tensor token_q Tensor token_kv Tensor - Tensor scale = torch exp - head + num_heads score + token_kv - token_q scale _alibi_bias _inverse_causal score b h m n torch where m = n score float -inf _times_two score b h m n Joint graph needed correctness score _squared score b h m n Joint graph needed correctness score score _head_offset dtype torch dtype device str Captured Buffer head_offset = torch rand H device=device dtype=dtype score_mod score b h m n score head_offset h score_mod _trig score b h m n Joint graph needed correctness torch sin torch cos score + torch tan b _trig score b h m n Branching joint graph cos_score = torch cos score sin_score = torch sin score z = cos_score sin_score + torch tan b z --------- Useful mask mod functions testing --------- _causal_mask batch Tensor head Tensor token_q Tensor token_kv Tensor - Tensor token_q = token_kv _inverse_causal_mask batch Tensor head Tensor token_q Tensor token_kv Tensor - Tensor token_q = token_kv test_score_mods = _identity _times_two _squared _causal _inverse_causal _rel_bias _rel_causal _generate_alibi_bias test_score_mask_mod_map = _identity noop_mask _times_two noop_mask _squared noop_mask _causal _causal_mask _inverse_causal _inverse_causal_mask _rel_bias noop_mask _rel_causal _causal_mask _generate_alibi_bias noop_mask captured_buffers_map = _head_offset _head_offset B = H = S = D = test_Hq_Hkv = test_Bq_Bkv = test_block_size = test_strides = H S D S D D offset H D D B H D transposed dimensions H S D D H D heads sequence transposed S D + B S D + D + additional buffer one dim D B + H + D additional buffer multiple dim + shared dimension query_key_value_clones query torch Tensor key torch Tensor value torch Tensor dtype Optional torch dtype = None Clones query key value tensors moves them specified dtype dtype None dtype = query dtype query_ref = query detach clone dtype requires_grad_ query requires_grad key_ref = key detach clone dtype requires_grad_ key requires_grad value_ref = value detach clone dtype requires_grad_ value requires_grad query_ref key_ref value_ref batch_reserve paged_attention PagedAttention target_seq_len Tensor B = target_seq_len shape b range B paged_attention reserve torch tensor b target_seq_len b large_tensor_test_class GB device=test_device TestFlexAttention InductorTestCase setUp super setUp skipCPUIf LONG_COMPILATION_ON_CPU skip UT CPU due long compilation time found CI _check_equal golden_out torch Tensor ref_out torch Tensor compiled_out torch Tensor fudge_factor float tensor_name Optional str = None fudge_atol float = compiled_error = golden_out - compiled_out abs mean ref_error = golden_out - ref_out abs mean torch isnan compiled_error any torch isnan ref_error any fail Output Grad NaN name = tensor_name tensor_name None msg = f name Compiled error compiled_error greater than ref error ref_error more than fudge_factor X torch testing assert_close compiled_error ref_error rtol=fudge_factor atol= e- msg=msg _check_out golden_out torch Tensor ref_out torch Tensor compiled_out torch Tensor is_paged_attention bool = False dtype = ref_out dtype torch no_grad Note seems like we really less accurate than float computation likely due online softmax dtype == torch float fudge_factor = is_paged_attention paged attention less accurate since may reorder blocks block mask fudge_factor = fudge_factor = Checkout output _check_equal golden_out ref_out compiled_out fudge_factor Out _check_out_and_grad golden_out torch Tensor ref_out torch Tensor compiled_out torch Tensor q_gold torch Tensor q_ref torch Tensor q torch Tensor k_gold torch Tensor k_ref torch Tensor k torch Tensor v_gold torch Tensor v_ref torch Tensor v torch Tensor dtype = ref_out dtype torch no_grad Note seems like we really less accurate than float computation likely due online softmax dtype == torch float fudge_factor = fudge_factor = Checkout output _check_equal golden_out ref_out compiled_out fudge_factor Out Check gradients q_fudge_factor = fudge_factor _check_equal q_gold grad q_ref grad q grad q_fudge_factor Grad_Query k_fudge_factor = fudge_factor _check_equal k_gold grad k_ref grad k grad k_fudge_factor Grad_Key v_fudge_factor = fudge_factor _check_equal v_gold grad v_ref grad v grad v_fudge_factor Grad_Value run_test score_mod _score_mod_signature dtype torch dtype device str Q_B int = B Q_H int = H Q_S int = S Q_D int = D KV_B Optional int = None KV_H Optional int = None KV_S Optional int = None V_D Optional int = None block_mask Optional BlockMask = None requires_grad = device DEVICE_SUPPORTS_BACKWARDS KV_B None KV_B = Q_B KV_H None KV_H = Q_H KV_S None KV_S = Q_S V_D None V_D = Q_D device == cpu dtype torch float dtype = torch float requires_grad = device DEVICE_SUPPORTS_BACKWARDS q = torch randn Q_B Q_H Q_S Q_D dtype=dtype device=device requires_grad=requires_grad k = torch randn KV_B KV_H KV_S Q_D dtype=dtype device=device requires_grad=requires_grad v = torch randn KV_B KV_H KV_S V_D dtype=dtype device=device requires_grad=requires_grad block_mask None block_mask = create_block_mask noop_mask Q_B Q_H Q_S KV_S device=device q_ref k_ref v_ref = query_key_value_clones q k v q_gold k_gold v_gold = query_key_value_clones q k v torch float sdpa_partial = create_attention score_mod block_mask enable_gqa= Q_H = KV_H compiled_sdpa = torch compile sdpa_partial golden_out = sdpa_partial q_gold k_gold v_gold ref_out = sdpa_partial q_ref k_ref v_ref compiled_out = compiled_sdpa q k v assert isinstance golden_out torch Tensor assert isinstance ref_out torch Tensor assert isinstance compiled_out torch Tensor requires_grad _check_out golden_out ref_out compiled_out is_paged_attention=False backward_grad = torch randn Q_B Q_H Q_S V_D dtype=dtype device=device golden_out backward backward_grad torch float ref_out backward backward_grad compiled_out backward backward_grad _check_out_and_grad golden_out ref_out compiled_out q_gold q_ref q k_gold k_ref k v_gold v_ref v preprocess_paged_attention score_mod Optional Callable q Tensor k Tensor v Tensor block_mask dtype torch dtype device str page_size int = - tuple Tensor Tensor BlockMask _score_mod_signature assert block_mask None Must provide block_mask Q_B Q_H Q_S _ = q shape KV_B KV_H KV_S QK_D = k shape _ _ _ V_D = v shape test different batch size max_batch_size = max Q_B KV_B + n_pages = KV_S + page_size - page_size max_batch_size allocate cache MAX_CACHED_SEQ_LEN = n_pages page_size k_cache = torch zeros KV_H MAX_CACHED_SEQ_LEN QK_D device=device dtype=dtype v_cache = torch zeros KV_H MAX_CACHED_SEQ_LEN V_D device=device dtype=dtype For testing purposes we randomly initialize page table which maps batch_idx logical_block_idx physical_block_idx Specifically PagedAttention maintains stack empty_pages unused physical_block_idx The ` batch_reserve ` function grabs physical_block_idx top empty_pages until there enough pages each batch index i e num pages batch_idx = target_seq_len batch_idx For example first batch_reserve call physical block indices KV_S allocated batch index physical block indices KV_S + KV_S + KV_S allocated batch index etc Thus kv tensors batch index will scattered kv cache simulating real use case paged attention paged_attention = PagedAttention n_pages page_size max_batch_size device=device batch_reserve paged_attention torch tensor KV_S KV_S KV_S KV_S device=device batch_reserve paged_attention torch tensor KV_S KV_S KV_S KV_S device=device batch_reserve paged_attention torch tensor KV_S KV_S KV_S KV_S device=device batch_reserve paged_attention torch tensor KV_S KV_S KV_S KV_S device=device update cache k v input_pos = torch arange KV_S device=device dtype=torch int unsqueeze expand KV_B KV_S batch_idx = torch arange KV_B device=device dtype=torch int paged_attention assign batch_idx input_pos k v k_cache v_cache convert block mask score mod kv_len_tensor = torch full KV_B KV_S device=device dtype=torch int converted_block_mask = paged_attention convert_logical_block_mask block_mask kv_len=kv_len_tensor converted_score_mod = paged_attention get_score_mod score_mod kv_len=kv_len_tensor k_cache v_cache converted_block_mask converted_score_mod run_paged_attention score_mod Optional Callable q Tensor k Tensor v Tensor dtype torch dtype device str block_mask Optional BlockMask = None kernel_options Optional dict = None - tuple Tensor Tensor B Q_H Q_S KV_H KV_S = q shape q shape q shape k shape k shape block_mask None block_mask = create_block_mask noop_mask B Q_S KV_S device=device k_cache v_cache converted_block_mask converted_score_mod = preprocess_paged_attention score_mod q k v block_mask dtype device block_mask BLOCK_SIZE compiled_sdpa = torch compile flex_attention compute return_lse = True requires_grad = device DEVICE_SUPPORTS_BACKWARDS requires_grad compiled_out compiled_lse = compiled_sdpa q k_cache v_cache return_lse=return_lse block_mask=converted_block_mask score_mod=converted_score_mod enable_gqa= Q_H = KV_H kernel_options=kernel_options return_lse = False compiled_lse = None compiled_out = compiled_sdpa q k_cache v_cache return_lse=return_lse block_mask=converted_block_mask score_mod=converted_score_mod enable_gqa= Q_H = KV_H kernel_options=kernel_options compiled_out compiled_lse run_test_with_paged_attention score_mod Optional Callable dtype torch dtype device Q_B int = B Q_H int = H Q_S int = S QK_D int = D KV_B int = B KV_H int = H KV_S int = S V_D int = D block_mask Optional BlockMask = None assert Q_H KV_H == device == cpu dtype torch float dtype = torch float q = torch randn Q_B Q_H Q_S QK_D dtype=dtype device=device requires_grad=False k = torch randn KV_B KV_H KV_S QK_D dtype=dtype device=device requires_grad=False v = torch randn KV_B KV_H KV_S V_D dtype=dtype device=device requires_grad=False q_ref k_ref v_ref = query_key_value_clones q k v q_gold k_gold v_gold = query_key_value_clones q k v torch float block_mask None block_mask = create_block_mask noop_mask Q_B Q_S KV_S device=device sdpa_partial = create_attention score_mod block_mask enable_gqa= Q_H = KV_H golden_out golden_lse = sdpa_partial q_gold k_gold v_gold return_lse=True ref_out ref_lse = sdpa_partial q_ref k_ref v_ref return_lse=True compiled_out compiled_lse = run_paged_attention score_mod q k v dtype device block_mask _check_out golden_out ref_out compiled_out is_paged_attention=True requires_grad = device DEVICE_SUPPORTS_BACKWARDS requires_grad _check_out golden_lse ref_lse compiled_lse is_paged_attention=True run_test_with_call sdpa_call Callable dtype torch dtype device str Q_B int = B Q_H int = H Q_S int = S Q_D int = D KV_B int = B KV_H int = H KV_S int = S V_D int = D device == cpu dtype torch float dtype = torch float requires_grad = device DEVICE_SUPPORTS_BACKWARDS q = torch randn Q_B Q_H Q_S Q_D dtype=dtype device=device requires_grad=requires_grad k = torch randn KV_B KV_H KV_S Q_D dtype=dtype device=device requires_grad=requires_grad v = torch randn KV_B KV_H KV_S V_D dtype=dtype device=device requires_grad=requires_grad q_ref k_ref v_ref = query_key_value_clones q k v q_gold k_gold v_gold = query_key_value_clones q k v torch float compiled_sdpa = torch compile sdpa_call golden_out = sdpa_call q_gold k_gold v_gold ref_out = sdpa_call q_ref k_ref v_ref compiled_out = compiled_sdpa q k v requires_grad _check_out golden_out ref_out compiled_out is_paged_attention=False backward_grad = torch randn Q_B Q_H Q_S V_D dtype=dtype device=device golden_out backward backward_grad torch float ref_out backward backward_grad compiled_out backward backward_grad _check_out_and_grad golden_out ref_out compiled_out q_gold q_ref q k_gold k_ref k v_gold v_ref v run_dynamic_test score_mask_mod tuple Callable Callable dtype torch dtype device B int = B H int = H S int = S D int = D device == cpu dtype torch float dtype = torch float score_mod mask_mod = score_mask_mod First batch original dimensions B H S D block_mask = create_block_mask mask_mod S S device=device sdpa_partial = create_attention score_mod block_mask=block_mask requires_grad = device DEVICE_SUPPORTS_BACKWARDS q = torch randn B H S D dtype=dtype device=device requires_grad=requires_grad k = torch randn B H S D dtype=dtype device=device requires_grad=requires_grad v = torch randn B H S D dtype=dtype device=device requires_grad=requires_grad q _ref k _ref v _ref = query_key_value_clones q k v q _gold k _gold v _gold = query_key_value_clones q k v torch float ref_out = sdpa_partial q _ref k _ref v _ref golden_out = sdpa_partial q _gold k _gold v _gold requires_grad backward_grad = torch randn B H S D dtype=dtype device=device golden_out backward backward_grad torch float ref_out backward backward_grad Second batch modified dimensions B H S D B = int B S = int S block_mask = create_block_mask mask_mod S S device=device sdpa_partial = create_attention score_mod block_mask=block_mask q = torch randn B H S D dtype=dtype device=device requires_grad=requires_grad k = torch randn B H S D dtype=dtype device=device requires_grad=requires_grad v = torch randn B H S D dtype=dtype device=device requires_grad=requires_grad q _ref k _ref v _ref = query_key_value_clones q k v q _gold k _gold v _gold = query_key_value_clones q k v torch float ref_out = sdpa_partial q _ref k _ref v _ref golden_out = sdpa_partial q _gold k _gold v _gold requires_grad backward_grad = torch randn B H S D dtype=dtype device=device golden_out backward backward_grad torch float ref_out backward backward_grad Third batch modified dimensions B H S D S = int S block_mask = create_block_mask mask_mod S S device=device sdpa_partial = create_attention score_mod block_mask=block_mask q = torch randn B H S D dtype=dtype device=device requires_grad=requires_grad k = torch randn B H S D dtype=dtype device=device requires_grad=requires_grad v = torch randn B H S D dtype=dtype device=device requires_grad=requires_grad q _ref k _ref v _ref = query_key_value_clones q k v q _gold k _gold v _gold = query_key_value_clones q k v torch float ref_out = sdpa_partial q _ref k _ref v _ref golden_out = sdpa_partial q _gold k _gold v _gold requires_grad backward_grad = torch randn B H S D dtype=dtype device=device golden_out backward backward_grad torch float ref_out backward backward_grad Clear dynamo counters torch _dynamo reset First compilation original dimensions backend = torch _dynamo testing CompileCounterWithBackend inductor compiled_sdpa = torch compile sdpa_partial backend=backend dynamic=True compiled_out = compiled_sdpa q k v requires_grad compiled_out backward backward_grad _check_out_and_grad golden_out ref_out compiled_out q _gold q _ref q k _gold k _ref k v _gold v _ref v _check_out golden_out ref_out compiled_out assertEqual backend frame_count Second compilation new dimensions compiled_sdpa = torch compile sdpa_partial backend=backend dynamic=True compiled_out = compiled_sdpa q k v requires_grad compiled_out backward backward_grad _check_out_and_grad golden_out ref_out compiled_out q _gold q _ref q k _gold k _ref k v _gold v _ref v _check_out golden_out ref_out compiled_out assertEqual backend frame_count Third compilation new dimensions compiled_sdpa = torch compile sdpa_partial backend=backend dynamic=True compiled_out = compiled_sdpa q k v requires_grad compiled_out backward backward_grad _check_out_and_grad golden_out ref_out compiled_out q _gold q _ref q k _gold k _ref k v _gold v _ref v _check_out golden_out ref_out compiled_out assertEqual backend frame_count run_automatic_dynamic_test score_mod Callable dtype torch dtype device str B int = B H int = H S int = S D int = D device == cpu dtype torch float dtype = torch float block_mask = create_block_mask noop_mask S S device=device sdpa_partial = create_attention score_mod block_mask=block_mask The first eager batch shape B H S D requires_grad = device DEVICE_SUPPORTS_BACKWARDS q = torch randn B H S D dtype=dtype device=device requires_grad=requires_grad k = torch randn B H S D dtype=dtype device=device requires_grad=requires_grad v = torch randn B H S D dtype=dtype device=device requires_grad=requires_grad golden_out = sdpa_partial q torch float k torch float v torch float ref_out = sdpa_partial q k v The second eager batch shape B H S D B = int B S = int S block_mask = create_block_mask noop_mask S S device=device sdpa_partial = create_attention score_mod block_mask=block_mask q = torch randn B H S D dtype=dtype device=device requires_grad=requires_grad k = torch randn B H S D dtype=dtype device=device requires_grad=requires_grad v = torch randn B H S D dtype=dtype device=device requires_grad=requires_grad golden_out = sdpa_partial q torch float k torch float v torch float ref_out = sdpa_partial q k v The third eager batch shape B H S D B = int B S = int S block_mask = create_block_mask noop_mask S S device=device sdpa_partial = create_attention score_mod block_mask=block_mask q = torch randn B H S D dtype=dtype device=device requires_grad=requires_grad k = torch randn B H S D dtype=dtype device=device requires_grad=requires_grad v = torch randn B H S D dtype=dtype device=device requires_grad=requires_grad golden_out = sdpa_partial q torch float k torch float v torch float ref_out = sdpa_partial q k v Need clear dynamo counters since flex attention eager mode also uses dynamo tracing We check dynamo counters frames ok ensure first batch compiled static shape second batch compiled dynamic shape no re-compilation third batch torch _dynamo reset Note seems like we really less accurate than float computation likely due online softmax dtype == torch float fudge_factor = fudge_factor = The first batch backend = torch _dynamo testing CompileCounterWithBackend inductor compiled_out = torch compile sdpa_partial backend=backend fullgraph=True q k v _check_equal golden_out ref_out compiled_out fudge_factor assertEqual backend frame_count The second batch automatic dynamic compiled_out = torch compile sdpa_partial backend=backend fullgraph=True q k v _check_equal golden_out ref_out compiled_out fudge_factor assertEqual backend frame_count The third batch no re-compilation compiled_out = torch compile sdpa_partial backend=backend fullgraph=True q k v _check_equal golden_out ref_out compiled_out fudge_factor assertEqual backend frame_count supported_platform dtypes device_configs cpu dtypes dtypesIfCUDA device_configs cuda dtypes dtypesIfXPU device_configs xpu dtypes common_utils parametrize score_mod test_score_mods test_builtin_score_mods device dtype score_mod Callable run_test score_mod dtype device=device run_test_with_paged_attention score_mod dtype device=device running_on_a _only common_utils parametrize score_mod test_score_mods dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast test_builtin_score_mods_seqlen_lt_default_sparse_block_size device dtype score_mod Callable _DEFAULT_SPARSE_BLOCK_SIZE attention = functools partial flex_attention score_mod=score_mod kernel_options= FORCE_USE_FLEX_ATTENTION True run_test_with_call attention dtype device B H D B H D running_on_a _only dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast common_utils parametrize score_mod test_score_mods test_builtin_score_mods_seqlen_lt_custom_sparse_block_size device dtype torch dtype score_mod Callable causal_mask b h q kv q = kv block_mask = create_block_mask causal_mask BLOCK_SIZE= device=device attention = functools partial flex_attention score_mod=score_mod block_mask=block_mask kernel_options= FORCE_USE_FLEX_ATTENTION True run_test_with_call attention dtype device B H D B H D supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast common_utils parametrize score_mask_mod test_score_mask_mod_map items test_builtin_score_mods_dynamic device dtype torch dtype score_mask_mod tuple Callable Callable run_dynamic_test score_mask_mod dtype S= device=device supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast common_utils parametrize score_mod test_score_mods test_builtin_score_mods_automatic_dynamic device dtype torch dtype score_mod Callable run_automatic_dynamic_test score_mod dtype S= device=device supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast common_utils parametrize score_mod test_score_mods test_builtin_score_mods_different_seqlen device dtype torch dtype score_mod Callable inputs = score_mod dtype device B H S Seqlen Q different seqlen K V D B H S D run_test inputs run_test_with_paged_attention inputs supported_platform dtypes device_configs cpu dtypes dtypesIfCUDA device_configs cuda dtypes dtypesIfXPU device_configs xpu dtypes common_utils parametrize score_mod test_score_mods common_utils parametrize BLOCK_SIZE test_block_size test_builtin_score_mods_different_block_size device dtype torch dtype score_mod Callable BLOCK_SIZE Union int tuple int int block_mask = create_block_mask noop_mask B H S S BLOCK_SIZE=BLOCK_SIZE device=device run_test score_mod dtype block_mask=block_mask device=device run_test_with_paged_attention score_mod dtype block_mask=block_mask device=device supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast common_utils parametrize batch_dims test_Bq_Bkv common_utils parametrize head_dims test_Hq_Hkv common_utils parametrize score_mod test_score_mods test_kv_batch_broadcast device dtype torch dtype batch_dims tuple int int head_dims tuple int int score_mod Callable Hq Hkv = head_dims assert Hq Hkv == Bq Bkv = batch_dims assert Bq Bkv == block_mask = create_block_mask noop_mask Bq S S device=device run_test score_mod dtype device Bq Hq S D Bkv Hkv S D block_mask supported_platform skip_on_cpu test_small_block_mask device compiled_create_block_mask = torch compile create_block_mask create_block_mask_from_seqlens q_batch torch Tensor kv_batch torch Tensor - BlockMask B H = None None Q_LEN = q_batch size KV_LEN = kv_batch size batch_mask_mod b torch Tensor h torch Tensor q_idx torch Tensor kv_idx torch Tensor q_idx_batch = q_batch q_idx kv_idx_batch = kv_batch kv_idx batch_mask = q_idx_batch == kv_idx_batch q_idx_batch = - kv_idx_batch = - batch_mask compiled_create_block_mask batch_mask_mod B=B H=H Q_LEN=Q_LEN KV_LEN=KV_LEN device=device = torch tensor device=device b = torch tensor device=device seqlen b create_block_mask_from_seqlens seqlen seqlen supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast common_utils parametrize batch_dims test_Bq_Bkv common_utils parametrize head_dims test_Hq_Hkv common_utils parametrize score_mod test_score_mods test_kv_batch_broadcast_causal_mask device dtype torch dtype batch_dims tuple int int head_dims tuple int int score_mod Callable Hq Hkv = head_dims assert Hq Hkv == Bq Bkv = batch_dims assert Bq Bkv == mask_mod b h q kv q = kv block_mask = create_block_mask mask_mod Bq S S device=device attention = functools partial flex_attention block_mask=block_mask enable_gqa= Hq = Hkv run_test_with_call attention dtype device Bq Hq S D Bkv Hkv S D supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast common_utils parametrize score_mod test_score_mods skip_on_rocm TODO NaNs ROCM skip_on_xpu TODO NaNs XPU like ROCM need another PR fix test_GQA device dtype torch dtype score_mod Callable inputs = score_mod dtype device B H Hq = Hkv S D B H S D run_test inputs run_test_with_paged_attention inputs supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast common_utils parametrize q_s test_strides TODO fix layout query braodcasting common_utils parametrize k_s v_s test_strides test_strides test_strides test_strides test_strides test_strides test_strides test_strides test_strides test_strides TODO Doesn t work broadcasting reasons i think common_utils parametrize do_s test_strides test_strided_inputs device dtype torch dtype q_s k_s v_s do_s q = torch randn B H S D dtype=dtype device=device k = torch randn B H S D dtype=dtype device=device v = torch randn B H S D dtype=dtype device=device do = torch randn B H S D dtype=dtype device=device q_shape = B H S D k_shape = B H S D v_shape = B H S D do_shape = B H S D requires_grad = device DEVICE_SUPPORTS_BACKWARDS coerce_to_strides val shape strides strides offset = strides val_max = x y - x y zip strides shape assert sum val_max + offset B H S D assert strides - == torch as_strided val shape strides offset requires_grad_ requires_grad q = coerce_to_strides q q_shape q_s k = coerce_to_strides k k_shape k_s v = coerce_to_strides v v_shape v_s do = coerce_to_strides do do_shape do_s kernel_options = USE_TMA True block_mask = _create_empty_block_mask q k score_mod = _generate_alibi_bias sdpa_partial = create_attention score_mod=score_mod block_mask=block_mask kernel_options=kernel_options compiled_sdpa = torch compile sdpa_partial fullgraph=True ref_out = sdpa_partial q k v compiled_out = compiled_sdpa q k v tolerance = Tolerances atol= e- rtol= e- torch testing assert_close ref_out compiled_out atol=tolerance atol rtol=tolerance rtol requires_grad ref_out backward do ref_grads = q grad k grad v grad q grad = None k grad = None v grad = None compiled_out backward do compiled_grads = q grad k grad v grad q grad = None k grad = None v grad = None torch testing assert_close compiled_grads ref_grads atol=tolerance atol rtol=tolerance rtol torch testing assert_close compiled_grads ref_grads atol=tolerance atol rtol=tolerance rtol torch testing assert_close compiled_grads ref_grads atol=tolerance atol rtol=tolerance rtol test paged attention which does support backward q requires_grad k requires_grad v requires_grad = False False False paged_compiled_out _ = run_paged_attention score_mod q k v dtype device=device kernel_options=kernel_options torch testing assert_close ref_out paged_compiled_out atol=tolerance atol rtol=tolerance rtol supported_platform test_doc_mask_sparse device document_id = torch zeros S dtype=torch int device=device i range S document_id i i + = i document_masking_causal score b h q_idx kv_idx causal_mask = q_idx = kv_idx document_mask = document_id q_idx == document_id kv_idx torch where causal_mask document_mask score -float inf run_test document_masking_causal torch float device=device run_test_with_paged_attention document_masking_causal torch float device=device supported_platform test_index_multiple device bias = torch randn B S device=device index_multiple score b h q_idx kv_idx score + bias b q_idx run_test index_multiple torch float device=device run_test_with_paged_attention index_multiple torch float device=device supported_platform test_index_weird device bias = torch randn B H S device=device index_weird score b h q_idx kv_idx score + bias b h q_idx run_test index_weird torch float device=device run_test_with_paged_attention index_weird torch float device=device supported_platform test_index_weird device bias = torch randn B H S device=device which_bias = torch tensor device=device index_weird score b h q_idx kv_idx score + bias b h which_bias q_idx run_test index_weird torch float device=device run_test_with_paged_attention index_weird torch float device=device supported_platform dtypes device_configs cpu dtypes dtypesIfCUDA device_configs cuda dtypes dtypesIfXPU device_configs xpu dtypes test_skip_odd_keys device dtype torch dtype score_mod score b h q kv torch where kv == score float -inf run_test score_mod dtype device=device run_test_with_paged_attention score_mod dtype device=device supported_platform dtypes device_configs cpu dtypes dtypesIfCUDA device_configs cuda dtypes dtypesIfXPU device_configs xpu dtypes test_function_composition device dtype torch dtype score_mod_ score b h m n score + m - n score_mod_ score b h m n torch where m = n score float -inf composed_score_mod score b h m n score_mod_ score_mod_ score b h m n b h m n run_test composed_score_mod dtype device=device run_test_with_paged_attention composed_score_mod dtype device=device supported_platform dtypes device_configs cpu dtypes dtypesIfCUDA device_configs cuda dtypes dtypesIfXPU device_configs xpu dtypes test_captured_buffers_all_dims device dtype torch dtype head_scale = torch randn H device=device batch_scale = torch randn B device=device tok_scale = torch randn S device=device all_bias score batch head token_q token_kv score = score + tok_scale token_q score = score + batch_scale batch score = score + head_scale head score run_test all_bias dtype device=device run_test_with_paged_attention all_bias dtype device=device supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast test_seq_masking device dtype seq_idx = torch zeros S device=device dtype=torch bool seq_idx S = seq_mask_mod score b h q kv torch where seq_idx q == seq_idx kv score float -inf run_test seq_mask_mod dtype device=device run_test_with_paged_attention seq_mask_mod dtype device=device supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast test_load_from_bias_seq_only device dtype bias = torch randn S S device=device dtype=dtype bias_mod score b h q kv score + bias q kv run_test bias_mod dtype device=device run_test_with_paged_attention bias_mod dtype device=device supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast test_load_from_bias_seq_batch device dtype bias = torch randn B S S device=device dtype=dtype bias_mod score b h q kv score + bias b q kv run_test bias_mod dtype device=device run_test_with_paged_attention bias_mod dtype device=device supported_platform skip_on_cpu test_load_from_view_buffer device dtype = torch float W = SimpleAttention torch nn Module __init__ super __init__ rel_pos_h = torch randn H - D device=device dtype=dtype forward q k v q = q view B H H W - score_mod = generate_score_mod q q = q view B H H W - flex_attention q k v score_mod=score_mod generate_score_mod q rel_h = add_decomposed_rel_pos q rel_h = rel_h view B H rel_h size rel_h size rel_h size squeeze - score_mod score batch head q_idx k_idx h_idx = k_idx W score + rel_h batch head q_idx h_idx score_mod torch no_grad add_decomposed_rel_pos q q_coords = torch arange H device=self rel_pos_h device None k_coords = torch arange H device=self rel_pos_h device None relative_coords = q_coords - k_coords + H - Rh = rel_pos_h relative_coords long r_q = q reshape B H H W D rel_h = torch einsum bhwc hkc- bhwk r_q Rh rel_h reshape B H H W H m = SimpleAttention device eval m = torch compile m mode= max-autotune fullgraph=True q = torch randn B H H W D device=device dtype=dtype requires_grad=True k = torch randn B H H W D device=device dtype=dtype requires_grad=True v = torch randn B H H W D device=device dtype=dtype requires_grad=True out = m q k v out sum backward supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast test_load_from_bias_head_seq_batch device dtype bias = torch randn B H S S device=device dtype=dtype bias_mod score b h q kv score + bias b h q kv run_test bias_mod dtype device=device run_test_with_paged_attention bias_mod dtype device=device supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast test_load_rel_bias device dtype rel_bias = torch randn S device=device dtype=dtype bias_mod score b h q kv score + rel_bias q - kv + S run_test bias_mod dtype device=device run_test_with_paged_attention bias_mod dtype device=device supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast test_dependent_causal_bidirectional device dtype num_bidirectional = torch randint S B device=device dtype=torch int bias_mod score b h q kv causal_attention = q = kv cur_num_bidirectional = num_bidirectional b bidirectional_attention_on_video = q = cur_num_bidirectional kv = cur_num_bidirectional torch where bidirectional_attention_on_video &#124; causal_attention score -float inf run_test bias_mod dtype device=device run_test_with_paged_attention bias_mod dtype device=device supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast test_natten_ d device dtype H = W = S H WINDOW = assert W H == S get_x_y idx This should floor divide we don t support properly idx W idx W natten_mask score b h q kv q_x q_y = get_x_y q kv_x kv_y = get_x_y kv torch where q_x - kv_x abs = WINDOW &#124; q_y - kv_y abs = WINDOW score float -inf run_test natten_mask dtype device=device run_test_with_paged_attention natten_mask dtype device=device supported_platform test_subgraph_respect_decompostion device torch _decomp core_aten_decompositions torch fx experimental proxy_tensor make_fx score_mod_func score b h q kv score - q + kv make_tensor = functools partial torch randn device=device dtype=torch float requires_grad=True query key value = make_tensor make_tensor make_tensor floor_div decomposed decomposition_table empty attention = functools partial flex_attention score_mod=score_mod_func gm = make_fx attention decomposition_table= query key value assertExpectedInline gm sdpa_score code strip \ forward arg _ arg _ arg _ arg _ arg _ add = torch ops aten add Tensor arg _ arg _ = None floor_divide = torch ops aten floor_divide default arg _ add arg _ = add = None sub = torch ops aten sub Tensor arg _ floor_divide arg _ = floor_divide = None sub floor_div decomposed core_aten_decompositions gm = make_fx attention decomposition_table=core_aten_decompositions query key value assertExpectedInline gm sdpa_score code strip \ forward arg _ arg _ arg _ arg _ arg _ add = torch ops aten add Tensor arg _ arg _ = None div = torch ops aten div Tensor_mode arg _ add rounding_mode = floor arg _ = add = None sub = torch ops aten sub Tensor arg _ div arg _ = div = None sub supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast test_silu_on_score device dtype silu_score score b h q kv torch nn functional silu score run_test silu_score dtype device=device run_test_with_paged_attention silu_score dtype device=device supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast test_padded_dense_causal device dtype seq_len = torch arange B device=device dtype=torch int + create_padded_dense_wrapper orig_score_mod njt_score_mod qk b h q kv torch where qk = seq_len b orig_score_mod qk b h q kv -float inf njt_score_mod causal_njt = create_padded_dense_wrapper _causal run_test causal_njt dtype device=device supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast test_captured_scale device dtype scale = torch ones device=device dtype=torch int score_mod_scale qk b h q kv qk + scale run_test score_mod_scale dtype device=device run_test_with_paged_attention score_mod_scale dtype device=device supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast test_recompile_changed_score_mod device dtype scale = torch ones device=device dtype=torch int ADD = True score_mod_scale qk b h q kv ADD qk + scale qk scale run_test score_mod_scale dtype device=device run_test_with_paged_attention score_mod_scale dtype device=device ADD = False run_test score_mod_scale dtype device=device run_test_with_paged_attention score_mod_scale dtype device=device supported_platform expectedFailure If we capture tensor then we can perform reduction shouldn t allowed dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast test_captured_reduction device dtype scale = torch randn B device=device score_mod_scale qk b h q kv qk + scale b sum dim=- run_test score_mod_scale dtype device=device supported_platform skip_on_cpu dtypes torch float dtypesIfCUDA torch float test_dynamic_captured_buffer device dtype run_with_head_count compiled_fa head_count head_scale = torch randn head_count device=device dtype=dtype requires_grad=True score_mod score batch head token_q token_kv score head_scale head q = torch randn B head_count S D device=device dtype=dtype requires_grad=True k = torch randn_like q requires_grad=True v = torch randn_like q requires_grad=True block_mask = create_block_mask noop_mask B S S device=device out = compiled_fa q k v score_mod=score_mod block_mask=block_mask loss = out sum loss backward out compiled_fa = torch compile flex_attention fullgraph=True dynamic=True head_counts = head_count head_counts run_with_head_count compiled_fa head_count supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast common_utils parametrize score_mod test_score_mods name_fn=lambda score_mod score_mod __name__ skip_on_cpu test_return_max device dtype score_mod make_tensor = functools partial torch randn device=device dtype=dtype requires_grad=True query key value = make_tensor make_tensor make_tensor out_only = flex_attention query key value score_mod out_max aux_max = flex_attention query key value score_mod return_aux=AuxRequest max_scores=True out_both aux_both = flex_attention query key value score_mod return_aux=AuxRequest lse=True max_scores=True flex_compile = torch compile flex_attention fullgraph=True out_compiled aux_compiled = flex_compile query key value score_mod return_aux=AuxRequest max_scores=True torch testing assert_close out_only out_max atol= e- rtol= e- torch testing assert_close out_only out_both atol= e- rtol= e- torch testing assert_close aux_max max_scores aux_both max_scores atol= e- rtol= e- we calculating slightly different scores so add lil fudge Extra tolerance squared score_mod float due limited dynamic range score_mod __name__ == _squared dtype == torch float atol rtol = e- e- atol rtol = e- e- torch testing assert_close out_max out_compiled atol=atol rtol=rtol torch testing assert_close aux_max max_scores aux_compiled max_scores atol=atol rtol=rtol B H L = query shape assertEqual aux_max max_scores shape B H L max_score_tensors = aux_max max_scores aux_both max_scores aux_compiled max_scores max_tensor max_score_tensors assertFalse max_tensor requires_grad max_scores should require gradients assertEqual max_tensor dtype torch float max_scores should kept fp Test gradient computation both eager compiled versions test_cases = eager out_max eager mode compiled out_compiled compiled mode mode_name output description test_cases loss = output sum grads = torch autograd grad loss query key value Verify gradients computed all inputs input_names = query key value grad input_name zip grads input_names assertIsNotNone grad f input_name should receive gradients description supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast common_utils parametrize score_mod test_score_mods name_fn=lambda score_mod score_mod __name__ skip_on_cpu test_return_aux device dtype score_mod Test new return_aux API AuxRequest Output make_tensor = functools partial torch randn device=device dtype=dtype requires_grad=True query key value = make_tensor make_tensor make_tensor flex_compile = torch compile flex_attention fullgraph=True flex_compile_partial = torch compile flex_attention fullgraph=False Test No auxiliary outputs default behavior out_only = flex_compile query key value score_mod assertIsInstance out_only torch Tensor Test Request only LSE out aux_lse = flex_compile query key value score_mod return_aux=AuxRequest lse=True assertIsInstance aux_lse AuxOutput assertIsInstance aux_lse lse torch Tensor assertIsNone aux_lse max_scores assertEqual aux_lse lse shape assertEqual aux_lse lse dtype torch float Test Request only max_scores out aux_max = flex_compile query key value score_mod return_aux=AuxRequest max_scores=True assertIsInstance aux_max AuxOutput assertIsNone aux_max lse assertIsInstance aux_max max_scores torch Tensor assertEqual aux_max max_scores shape assertEqual aux_max max_scores dtype torch float Test Request both auxiliary outputs out aux_both = flex_compile query key value score_mod return_aux=AuxRequest lse=True max_scores=True assertIsInstance aux_both AuxOutput assertIsInstance aux_both lse torch Tensor assertIsInstance aux_both max_scores torch Tensor assertEqual aux_both lse shape assertEqual aux_both max_scores shape Test Request no auxiliary outputs explicitly out aux_none = flex_compile query key value score_mod return_aux=AuxRequest Default lse=False max_scores=False assertIsInstance aux_none AuxOutput assertIsNone aux_none lse assertIsNone aux_none max_scores Test Verify outputs consistent legacy API can t fullgraph through warnings out_legacy lse_legacy = flex_compile_partial query key value score_mod return_lse=True torch testing assert_close out_only out_legacy atol= e- rtol= e- torch testing assert_close aux_lse lse lse_legacy atol= e- rtol= e- supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast skip_on_cpu test_return_aux_deprecation_warnings device dtype Test deprecation warnings issued legacy parameters warnings make_tensor = functools partial torch randn device=device dtype=dtype query key value = make_tensor make_tensor make_tensor Clear shown warnings ensure we can test them original_shown = _WARNINGS_SHOWN copy _WARNINGS_SHOWN clear try Test deprecation warning return_lse warnings catch_warnings record=True w warnings simplefilter always flex_attention query key value return_lse=True assertTrue any return_lse deprecated str warning message warning w Clear next test _WARNINGS_SHOWN clear Test error when both old new API used assertRaises ValueError cm flex_attention query key value return_lse=True return_aux=AuxRequest lse=True assertIn Cannot specify both return_lse return_aux str cm exception finally Restore original warnings state _WARNINGS_SHOWN clear _WARNINGS_SHOWN update original_shown supported_platform dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast skip_on_cpu test_dynamic_divisibility_guards device dtype Test guards divisible non-divisible shape transitions device == cpu dtype torch float dtype = torch float score_mod qk b h q kv torch where q = kv qk -float inf test_shape S backend Test single shape configuration block_mask = create_block_mask noop_mask S S device=device sdpa_partial = create_attention score_mod block_mask=block_mask tensors = torch randn S dtype=dtype device=device requires_grad=False _ range compiled_sdpa = torch compile sdpa_partial backend=backend out code = run_and_get_code compiled_sdpa tensors Check divisibility flag is_divisible = S == expected_flag = f IS_DIVISIBLE tl constexpr = is_divisible assertIn expected_flag str code f S= S should have expected_flag assertEqual out shape S out code torch _dynamo reset backend = CompileCounterWithBackend inductor Test divisible non-divisible shapes test_shapes = _ = test_shape S backend S test_shapes supported_platform test_multiple_score_mod_calls device query = torch randn dtype=torch float device=device keys = torch randn dtype=torch float device=device _ range values = torch randn dtype=torch float device=device _ range scoremod_ qk b h q kv qk + q - kv scoremod_ qk b h q kv torch where q = kv qk -float inf f q k k v v q = flex_attention q k v score_mod=scoremod_ flex_attention q k v score_mod=scoremod_ out = f query keys values out = torch compile f query keys values tolerance = Tolerances atol= e- rtol= e- torch testing assert_close out out atol=tolerance atol rtol=tolerance rtol supported_platform skip_on_cpu skip_on_rocm TODO Investigate test_multiple_mask_calls device make_tensor = functools partial torch randn dtype=torch float device=device requires_grad=True query key value = make_tensor make_tensor make_tensor window_size = causal_mask b h q_idx kv_idx q_idx = kv_idx causal_mask_slidewindow_mod b h q_idx kv_idx q_idx = kv_idx q_idx = kv_idx + window_size mask = create_block_mask causal_mask None _compile=False device=device mask = create_block_mask causal_mask_slidewindow_mod None _compile=False device=device f q k v out = flex_attention q k v block_mask=mask out = flex_attention q k v block_mask=mask out + out f_compiled = torch compile f fullgraph=True out = f query key value out_compiled = f_compiled query key value grads = torch autograd grad out query key value torch ones_like out grads_compile = torch autograd grad out_compiled query key value torch ones_like out_compiled grad grad_compiled zip grads grads_compile torch testing assert_close grad grad_compiled atol= e- rtol= e- supported_platform test_multiple_score_mod_calls device query = torch randn dtype=torch float device=device keys = torch randn dtype=torch float device=device _ range values = torch randn dtype=torch float device=device _ range scoremod_ qk b h q kv qk + q - kv scoremod_ qk b h q kv torch where q = kv qk -float inf attention = functools partial flex_attention score_mod=scoremod_ f q k k k v v v q = attention q k v q = flex_attention q k v score_mod=scoremod_ flex_attention q k v score_mod=scoremod_ out = f query keys values out = torch compile f fullgraph=True query keys values assertTrue out - out abs mean e- supported_platform test_multiple_score_mod_calls_paged_attention device query = torch randn dtype=torch float device=device keys = torch randn dtype=torch float device=device _ range values = torch randn dtype=torch float device=device _ range scoremod_ qk b h q kv qk + q - kv scoremod_ qk b h q kv torch where q = kv qk -float inf f q k k v v q = flex_attention q k v score_mod=scoremod_ flex_attention q k v score_mod=scoremod_ eager_out = f query keys values block_mask = create_block_mask noop_mask device=device k_cache v_cache converted_block_mask converted_score_mod = preprocess_paged_attention scoremod_ query keys values block_mask torch float device=device k_cache v_cache converted_block_mask converted_score_mod = preprocess_paged_attention scoremod_ query keys values block_mask torch float device=device paged_f q k k v v q = flex_attention q k v score_mod=converted_score_mod block_mask=converted_block_mask flex_attention q k v score_mod=converted_score_mod block_mask=converted_block_mask compiled_out = torch compile paged_f fullgraph=True query k_cache k_cache v_cache v_cache tolerance = Tolerances atol= e- rtol= e- torch testing assert_close eager_out compiled_out atol=tolerance atol rtol=tolerance rtol supported_platform test_multiple_score_mod_calls _paged_attention device query = torch randn dtype=torch float device=device keys = torch randn dtype=torch float device=device _ range values = torch randn dtype=torch float device=device _ range scoremod_ qk b h q kv qk + q - kv scoremod_ qk b h q kv torch where q = kv qk -float inf attention = functools partial flex_attention score_mod=scoremod_ f q k k k v v v q = attention q k v q = flex_attention q k v score_mod=scoremod_ flex_attention q k v score_mod=scoremod_ eager_out = f query keys values block_mask = create_block_mask noop_mask device=device k_cache v_cache converted_block_mask converted_score_mod = preprocess_paged_attention scoremod_ query keys values block_mask torch float device=device k_cache v_cache converted_block_mask converted_score_mod = preprocess_paged_attention scoremod_ query keys values block_mask torch float device=device k_cache v_cache converted_block_mask converted_score_mod = preprocess_paged_attention scoremod_ query keys values block_mask torch float device=device paged_attention = functools partial flex_attention score_mod=converted_score_mod block_mask=converted_block_mask paged_f q k k k v v v q = paged_attention q k v q = flex_attention q k v score_mod=converted_score_mod block_mask=converted_block_mask flex_attention q k v score_mod=converted_score_mod block_mask=converted_block_mask compiled_out = torch compile paged_f fullgraph=True query k_cache k_cache k_cache v_cache v_cache v_cache tolerance = Tolerances atol= e- rtol= e- torch testing assert_close eager_out compiled_out atol=tolerance atol rtol=tolerance rtol supported_platform skip_on_cpu test_inputs_are_realized device f q k v x = torch randn device=device x = x func qk b h q kv qk + x q flex_attention q sin k v score_mod=func cos q k v = torch randn device=device requires_grad=True _ range ref = f q k v out = torch compile f q k v assertTrue ref - out abs mean e- gradOut = torch randn_like q ref_grads = torch autograd grad ref q k v gradOut out_grads = torch autograd grad out q k v gradOut ref out zip ref_grads out_grads assertTrue ref - out abs mean e- supported_platform skip_on_cpu test_make_block_mask device causal_mask b h q_idx kv_idx q_idx = kv_idx block_mask_a = torch compile create_block_mask fullgraph=True causal_mask device=device block_mask_b = create_block_mask causal_mask device=device assertEqual block_mask_a kv_num_blocks block_mask_b kv_num_blocks assertEqual block_mask_a kv_indices block_mask_b kv_indices assertEqual block_mask_a q_num_blocks block_mask_b q_num_blocks supported_platform test_mask_mod_combiners device causal_mask b h q kv q = kv neg_causal_mask b h q kv q kv sliding_window b h q kv q - kv = local_s = block_mask = create_block_mask and_masks causal_mask sliding_window local_s local_s device=device assertExpectedInline block_mask kv_num_blocks sum item attention = functools partial flex_attention block_mask=block_mask run_test_with_call attention Q_S=local_s KV_S=local_s dtype=torch float device=device block_mask = create_block_mask and_masks causal_mask neg_causal_mask local_s local_s device=device assertEqual block_mask kv_num_blocks sum block_mask = create_block_mask or_masks causal_mask neg_causal_mask local_s local_s device=device block_mask = create_block_mask noop_mask local_s local_s device=device assertEqual block_mask sparsity block_mask sparsity supported_platform skip_on_cpu test_epilogue_fused device set so metrics appear torch _logging set_logs inductor_metrics=True torch compile f q k v out = flex_attention q k v out cos q k v = torch randn device=device _ range metrics reset _ code = run_and_get_code f q k v fc = FileCheck fc check triton_tem_fused template call fc check_not poi_fused_cos No cos pointwise operation fc run code accessed_bytes = torch float itemsize num_accesses = q k v reads one output TODO Get rid fudge factor We need fudge factor now we write extraneous logsumexp num_accesses += assertLess metrics num_bytes_accessed accessed_bytes num_accesses torch _logging set_logs supported_platform dtypes device_configs cpu dtypes dtypesIfCUDA device_configs cuda dtypes dtypesIfXPU device_configs xpu dtypes test_njt_causal device dtype offsets = torch tensor + S device=device dtype=torch int seq_idx = torch zeros S device=device dtype=torch int idx range len offsets - seq_idx offsets idx offsets idx + = idx create_njt_wrapper orig_score_mod offsets seq_idx njt_score_mod qk b h q kv q_nested = q - offsets seq_idx q kv_nested = kv - offsets seq_idx kv orig_score_mod qk b h q_nested kv_nested njt_score_mod causal_njt = create_njt_wrapper _causal offsets seq_idx run_test causal_njt dtype device=device run_test_with_paged_attention causal_njt dtype device=device supported_platform test_mixed_dtypes_fails device query = torch randn dtype=torch float device=device key = torch randn dtype=torch float device=device value = torch randn dtype=torch float device=device assertRaisesRegex ValueError Expected query key value have same dtype flex_attention query key value _identity supported_platform patch object torch _inductor config max_autotune True test_max_autotune device score_mod score b h m n score run_test score_mod dtype=torch float device=device run_test_with_paged_attention score_mod dtype=torch float device=device run_test_with_paged_attention score_mod=score_mod dtype=torch bfloat KV_S= device=device supported_platform skip TODO Figure out why erroring patch object torch _inductor config max_autotune True test_max_autotune_with_captured device head_scale = torch randn H device=device batch_scale = torch randn B device=device tok_scale = torch randn S device=device bias_mod score batch head token_q token_kv score = score + tok_scale token_q score = score + batch_scale batch score = score + head_scale head score run_test bias_mod dtype=torch float device=device supported_platform common_utils parametrize score_mod test_score_mods dtypes device_configs cpu dtypes dtypesIfCUDA device_configs cuda dtypes dtypesIfXPU device_configs xpu dtypes common_utils parametrize head_dims D D D D test_non_equal_head_dims device dtype score_mod head_dims qk_d v_d = head_dims run_test score_mod dtype device B H S qk_d B H S V_D=v_d run_test_with_paged_attention score_mod dtype device B H S qk_d B H S V_D=v_d supported_platform skip_on_cpu test_autograd_function_in_score_mod device ApplyMask torch autograd Function generate_vmap_rule = True staticmethod forward mask torch where mask -float inf staticmethod setup_context ctx inputs output _ mask = inputs ctx mark_non_differentiable mask staticmethod backward ctx i i None score_mod score b h q kv ApplyMask apply score q = kv func = torch compile flex_attention fullgraph=True q k v = torch randn device=device requires_grad=True _ range Just checking runs func q k v expectedFailure This doesn t work due vmap + autograd Function + torch compile composing run_test score_mod supported_platform test_causal_block device mask_mod b h q kv q = kv block_mask = create_block_mask mask_mod S S device=device attention = functools partial flex_attention block_mask=block_mask run_test_with_call attention dtype=torch float device=device supported_platform test_causal_block_paged_attention device mask_mod b h q kv q = kv block_mask = create_block_mask mask_mod B S S device=device run_test_with_paged_attention score_mod=_identity dtype=torch float device=device block_mask=block_mask supported_platform test_new_empty_mask_mod device S = q k v = torch randn S device=device _ range attn_mask = torch ones S S dtype=torch bool device=device tril score_mod score b h q_idx kv_idx h_ = h new_zeros h shape score + attn_mask b h_ q_idx kv_idx causal b h q_idx kv_idx h_ = h new_zeros h shape attn_mask b h_ q_idx kv_idx block_mask = create_block_mask causal B= H=None Q_LEN=S KV_LEN=S device=device torch compile flex_attention fullgraph=True q k v score_mod block_mask=block_mask supported_platform common_utils parametrize head_dim dtypes device_configs cpu dtypes_fast dtypesIfCUDA device_configs cuda dtypes_fast dtypesIfXPU device_configs xpu dtypes_fast test_non_pow_ _headdim device dtype head_dim run_test _rel_bias dtype device B H S head_dim B H S head_dim supported_platform test_GQA_causal_mask device mask_mod b h q kv q = kv block_mask = create_block_mask mask_mod B S S device=device attention = functools partial flex_attention block_mask=block_mask enable_gqa=True run_test_with_call attention torch float device B H Hq = Hkv S D B H S D run_test_with_paged_attention _identity dtype=torch float device=device Q_H=H Q_S=S KV_H=H KV_S=S block_mask=block_mask supported_platform test_custom_block_mask_generator device mask_mod b h q kv q = kv auto_mask = create_block_mask mask_mod S S device=device BLOCK_SIZE = causal_constructor S num_blocks = torch arange S BLOCK_SIZE device=device + indices = torch arange S BLOCK_SIZE device=device expand S BLOCK_SIZE S BLOCK_SIZE num_blocks = num_blocks None None indices = indices None None BlockMask from_kv_blocks num_blocks indices BLOCK_SIZE=BLOCK_SIZE mask_mod=mask_mod manual_mask = causal_constructor S assertEqual auto_mask to_dense manual_mask to_dense supported_platform skip_on_cpu dtypes device_configs cpu dtypes dtypesIfCUDA device_configs cuda dtypes dtypesIfXPU device_configs xpu dtypes common_utils parametrize score_mod _identity _causal test_logsumexp_correctness device dtype score_mod make_tensor = functools partial torch randn B H S D dtype=dtype device=device requires_grad=True q k v = make_tensor make_tensor make_tensor torch compile sdpa_hop q k v score_mod flex_attention q k v score_mod return_lse=True torch compile backend= aot_eager eager_sdpa_hop q k v score_mod flex_attention q k v score_mod return_lse=True ref_out ref_lse = eager_sdpa_hop q torch float k torch float v torch float score_mod compiled_out compiled_lse = sdpa_hop q k v score_mod assertTrue ref_lse dtype == torch float assertTrue compiled_lse dtype == torch float tolerance = Tolerances atol= e- rtol= e- torch testing assert_close ref_out dtype=torch float compiled_out dtype=torch float atol=tolerance atol rtol=tolerance rtol torch testing assert_close ref_lse dtype=torch float compiled_lse dtype=torch float atol=tolerance atol rtol=tolerance rtol supported_platform skip_on_cpu test_logsumexp_only_return device make_tensor = functools partial torch randn B H S D dtype=torch float device=device requires_grad=True q k v = make_tensor make_tensor make_tensor torch compile func q k v score_mod _ lse = flex_attention q k v score_mod return_lse=True lse_ = lse lse_ _ code = run_and_get_code func q k v _identity Ensure we re still generating flexattention kernel FileCheck check_count run primals_ primals_ primals_ True run code supported_platform skip_on_cpu common_utils parametrize score_mod _identity _causal _times_two _squared _trig _trig test_aot_eager_gradcheck device score_mod make_tensor = functools partial torch randn device=device dtype=torch float requires_grad=True query key value = make_tensor make_tensor make_tensor func = torch compile flex_attention backend= aot_eager fullgraph=True assertTrue torch autograd gradcheck func query key value score_mod raise_exception=True supported_platform skip_on_cpu test_eager_backward_strides device Repro torch nn Module __init__ super __init__ qkv_proj = torch nn Linear n_head = d_attn = forward x n_batch n_ctx _ = x shape q k v = qkv_proj x split d_attn d_attn d_attn dim= q = q reshape n_batch n_ctx n_head - k = k reshape n_batch n_ctx n_head - v = v reshape n_batch n_ctx n_head - q = q transpose k = k transpose v = v transpose x = torch nn attention flex_attention flex_attention q k v x model = Repro device x = torch randn device=device requires_grad=True out = torch compile model backend= aot_eager fullgraph=True x out backward torch ones_like out supported_platform skip_on_cpu test_differentiable_logsumexp_gradcheck device make_tensor = functools partial torch randn device=device dtype=torch float requires_grad=True query key value = make_tensor make_tensor make_tensor flex_attention_lse_only q k v flex_attention q k v return_lse=True func = torch compile flex_attention_lse_only backend= aot_eager assertTrue torch autograd gradcheck func query key value raise_exception=True supported_platform skip_on_cpu test_differentiable_logsumexp_compiled device make_tensor = functools partial torch randn device=device dtype=torch float requires_grad=True q k v = make_tensor make_tensor make_tensor lse_mask = torch randn device=device out lse = flex_attention q k v return_lse=True out mean + lse lse_mask sum backward q_grad k_grad v_grad = q grad k grad v grad q grad = None k grad = None v grad = None out lse = torch compile flex_attention q k v return_lse=True out mean + lse lse_mask sum backward q_grad k_grad v_grad = q grad k grad v grad tolerance = Tolerances atol= e- rtol= e- torch testing assert_close out out atol=tolerance atol rtol=tolerance rtol torch testing assert_close lse lse atol=tolerance atol rtol=tolerance rtol torch testing assert_close q_grad q_grad atol=tolerance atol rtol=tolerance rtol torch testing assert_close k_grad k_grad atol=tolerance atol rtol=tolerance rtol torch testing assert_close v_grad v_grad atol=tolerance atol rtol=tolerance rtol Use weird mask test reusing block_mask does work well supported_platform skip_on_cpu _test_block_mask_reuse_with_weird_mask device mask b h q kv kv &#124; kv = make_tensor = functools partial torch randn device=device dtype=torch float requires_grad=True block_mask = create_block_mask mask None None device=device Compile st version q k v seqlen= block_mask seqlen= torch compile flex_attention dynamic=True fullgraph=True make_tensor make_tensor make_tensor block_mask=block_mask make_tensor = functools partial torch randn device=device dtype=torch float requires_grad=True q k v = make_tensor make_tensor make_tensor Compile nd version q k v seqlen= block_mask seqlen= The graph includes BlockMask _adjust part out = torch compile flex_attention dynamic=True fullgraph=True q k v block_mask=block_mask out sum backward q_grad k_grad v_grad = q grad k grad v grad q grad = None k grad = None v grad = None block_mask = create_block_mask mask None None device=device Reuse st version q k v seqlen= block_mask seqlen= out = torch compile flex_attention dynamic=True fullgraph=True q k v block_mask=block_mask out sum backward q_grad k_grad v_grad = q grad k grad v grad tolerance = Tolerances atol= e- rtol= e- torch testing assert_close out out atol=tolerance atol rtol=tolerance rtol torch testing assert_close q_grad q_grad atol=tolerance atol rtol=tolerance rtol torch testing assert_close k_grad k_grad atol=tolerance atol rtol=tolerance rtol torch testing assert_close v_grad v_grad atol=tolerance atol rtol=tolerance rtol supported_platform skip_on_cpu test_float _matmul_precision device make_tensor = functools partial torch zeros device=device dtype=torch float requires_grad=False query key value = make_tensor make_tensor make_tensor query fill_ key fill_ value fill_ query requires_grad = True key requires_grad = True value requires_grad = True score_mod score b h q kv score temp_float _matmul_precision highest out_eager = flex_attention query key value score_mod flex_compiled = torch compile flex_attention fullgraph=True out_compiled = flex_compiled query key value score_mod grads_eager = torch autograd grad out_eager sum query key value grads_compile = torch autograd grad out_compiled sum query key value torch testing assert_close grads_eager grads_compile supported_platform skip_on_cpu common_utils parametrize score_mod_name _head_offset common_utils parametrize mode eager aot_eager test_captured_score_mod_aot_eager_gradcheck device score_mod_name str mode str make_tensor = functools partial torch randn device=device dtype=torch float requires_grad=True query key value = make_tensor make_tensor make_tensor func = torch compile flex_attention backend=mode fullgraph=True score_mod = captured_buffers_map score_mod_name torch float device assertTrue torch autograd gradcheck func query key value score_mod raise_exception=True supported_platform skip_on_cpu common_utils parametrize mode eager aot_eager test_document_masking_edge_case device mode requires_grad = device DEVICE_SUPPORTS_BACKWARDS document_masks = torch full dtype=torch int device=device document_masks = mask_mod b h q kv same_doc = document_masks b q == document_masks b kv same_doc make_tensor = functools partial torch randn device=device dtype=torch float requires_grad=requires_grad query key value = make_tensor make_tensor make_tensor func = torch compile flex_attention backend=mode fullgraph=True block_mask = create_block_mask mask_mod device=device out = func query key value block_mask=block_mask requires_grad out sum backward supported_platform skip_on_cpu test_strided_backwards device shape = Q = torch randn shape requires_grad=True device=device K = torch randn shape requires_grad=True device=device V = torch randn shape requires_grad=True device=device func = torch compile flex_attention dynamic=True fullgraph=True K_sliced = K - V_sliced = V - out_eager = flex_attention Q K_sliced V_sliced out_compiled = func Q K_sliced V_sliced grad = torch rand_like out_eager eager_grads = torch autograd grad out_eager Q K V grad compiled_grads = torch autograd grad out_compiled Q K V grad eager compiled zip eager_grads compiled_grads torch testing assert_close eager compiled atol= e- rtol= supported_platform skip_on_cpu common_utils parametrize mode eager inductor paged_attention common_utils parametrize permute_order Default order Reverse order Mixed order Another mixed order Non contiguous last dim common_utils parametrize shape test_flex_attention_stride_ordering device mode permute_order shape torch _inductor ir get_stride_order torch version hip mode == paged_attention raise skipTest TODO figure out why mode_paged_attention_permute_order _shape MI caused mem fault dtype = torch float Setup requires_grad = device DEVICE_SUPPORTS_BACKWARDS make_tensor = functools partial torch randn shape device=device dtype=dtype requires_grad=False mode == paged_attention requires_grad Create permute tensors query key value = make_tensor make_tensor make_tensor query = query permute permute_order key = key permute permute_order value = value permute permute_order mode == inductor func = torch compile flex_attention backend=mode fullgraph=True out = func query key value mode == paged_attention out _ = run_paged_attention _identity query key value dtype device=device func = flex_attention out = func query key value out_stride_order = get_stride_order out stride query_stride_order = get_stride_order query stride assertEqual out_stride_order query_stride_order f Stride order mismatch out out_stride_order query query_stride_order supported_platform skip_on_cpu common_utils parametrize mode eager inductor common_utils parametrize permute_order common_utils parametrize shape test_flex_attention_backward_stride_ordering device mode permute_order shape torch _inductor ir get_stride_order dtype = torch float make_tensor = functools partial torch randn shape device=device dtype=dtype requires_grad=False query key value = make_tensor make_tensor make_tensor query = query permute permute_order key = key permute permute_order value = value permute permute_order query requires_grad_ key requires_grad_ value requires_grad_ func = torch compile flex_attention backend=mode fullgraph=True mode == inductor flex_attention out = func query key value grad_output = torch randn_like out out backward grad_output leaf grad name query query grad query key key grad key value value grad value input_stride_order = get_stride_order grad stride orig_stride_order = get_stride_order leaf stride assertEqual input_stride_order orig_stride_order f Mode mode Stride order mismatch name grad input_stride_order input orig_stride_order supported_platform test_non_contiguous_last_dim device Test flex_attention tensors having non contiguous last dimension B H D = dtype = torch float device DEVICE_SUPPORTS_BACKWARDS torch float S column_major_tensor tensor = torch randn B H S D dtype=dtype device=device Column major last dims tensor transpose - - contiguous transpose - - q = column_major_tensor k = column_major_tensor v = column_major_tensor requires_grad = device DEVICE_SUPPORTS_BACKWARDS requires_grad q requires_grad_ True k requires_grad_ True v requires_grad_ True assertNotEqual q stride - assertNotEqual k stride - assertNotEqual v stride - q_ref k_ref v_ref = query_key_value_clones q k v q_gold k_gold v_gold = query_key_value_clones q k v torch float golden_out = flex_attention q_gold k_gold v_gold ref_out = flex_attention q_ref k_ref v_ref flex_compiled = torch compile flex_attention fullgraph=True dynamic=True compiled_out = flex_compiled q k v _check_out golden_out ref_out compiled_out requires_grad backward_grad = torch randn_like ref_out golden_out backward backward_grad torch float ref_out backward backward_grad compiled_out backward backward_grad _check_out_and_grad golden_out ref_out compiled_out q_gold q_ref q k_gold k_ref k v_gold v_ref v supported_platform common_utils parametrize compile True False test_fully_masked_out_rows_ _check device compile bool Ensure fully masked out rows won t cause NaNs requires_grad = device DEVICE_SUPPORTS_BACKWARDS query = torch randn B H S D dtype=torch float device=device requires_grad=requires_grad key = torch randn B H S D dtype=torch float device=device requires_grad=requires_grad value = torch randn B H S D dtype=torch float device=device requires_grad=requires_grad M = S mask_mod b h q kv q M block_mask = create_block_mask mask_mod B S S device=device flex = torch compile flex_attention dynamic=False compile flex_attention requires_grad out lse = flex query key value block_mask=block_mask return_lse=True assertEqual out M sum assertTrue lse M == -float inf all loss = out sum + lse sum loss backward assertEqual query grad M sum out = flex query key value block_mask=block_mask return_lse=False assertEqual out M sum supported_platform test_fully_masked_out_rows device M = S mask_mod b h q kv q M block_mask = create_block_mask mask_mod B S S device=device noop_mod score b h q_idx kv_idx score run_test noop_mod torch float device B H S D B H S D block_mask supported_platform skip_on_cpu test_kernel_options_argument_is_respected device make_tensor = functools partial torch randn device=device dtype=torch float requires_grad=True q k v = make_tensor make_tensor make_tensor Ensure we respect user s input kernel options _ code = run_and_get_code torch compile flex_attention fullgraph=True q k v kernel_options= BLOCK_M FileCheck check BLOCK_M tl constexpr = run code supported_platform test_block_mask_non_divisible device seq = torch arange device=device mod b h q kv seq q == seq kv block_mask = create_block_mask mod None None device=device torch compile create_block_mask mod None None device=device run_test_with_call lambda q k v flex_attention q k v block_mask=block_mask torch float device Q_S= KV_S= supported_platform test_causal_block_non_divisible device mask_mod b h q kv q = kv block_mask = create_block_mask mask_mod B S - S - device=device attention = functools partial flex_attention block_mask=block_mask run_test_with_call attention torch float device Q_S=S - KV_S=S - supported_platform skip_on_cpu test_modular_indexing device B H N D = dtype = torch bfloat device = torch device device Attention torch nn Module __init__ super __init__ bias = torch randn B N N H device=device dtype=dtype forward q torch Tensor k torch Tensor v torch Tensor - torch Tensor score_mod = generate_score_mod bias o = flex_attention q k v score_mod=score_mod o generate_score_mod bias bias = bias view B H N N contiguous score_mod score batch head q_idx k_idx attn_bias = bias batch head q_idx k_idx score + attn_bias score_mod m = Attention device eval dtype m = torch compile m mode= default fullgraph=False q = torch randn B H N D device=device dtype=dtype k = torch randn B H N D device=device dtype=dtype v = torch randn B H N D device=device dtype=dtype m q k v supported_platform skip_on_cpu test_force_write_lse device dtype = torch float make_tensor = functools partial torch randn device=device dtype=dtype requires_grad=False query key value = make_tensor make_tensor make_tensor out_eager lse_eager = flex_attention query key value return_lse=True flex_compile = torch compile flex_attention out_compiled lse_compiled = flex_compile query key value return_lse=True out_paged lse_paged = run_paged_attention score_mod=_identity q=query k=key v=value dtype=dtype device=device torch testing assert_close lse_eager lse_compiled atol= e- rtol= requires_grad = device DEVICE_SUPPORTS_BACKWARDS requires_grad torch testing assert_close lse_eager lse_paged atol= e- rtol= supported_platform skip_on_cpu common_utils parametrize backend flex_attention flex_decode eager test_lse_masked_output device backend backend == flex_decode kernel_options = FORCE_USE_FLEX_ATTENTION False flex_call = torch compile flex_attention fullgraph=True N_CTX = backend == flex_attention kernel_options = FORCE_USE_FLEX_ATTENTION True flex_call = torch compile flex_attention fullgraph=True N_CTX = kernel_options = flex_call = flex_attention N_CTX = SLIDING_WINDOW = make_tensor = functools partial torch randn N_CTX device=device dtype=torch float requires_grad=True sliding_window_causal b h q_idx kv_idx causal_mask = q_idx = kv_idx window_mask = q_idx - kv_idx = SLIDING_WINDOW causal_mask window_mask global_causal b h q_idx kv_idx causal_mask = q_idx = kv_idx window_mask = q_idx - kv_idx SLIDING_WINDOW causal_mask window_mask sliding_window_causal = torch nn attention flex_attention create_block_mask sliding_window_causal B=None H=None Q_LEN=N_CTX KV_LEN=N_CTX device=device global_causal = torch nn attention flex_attention create_block_mask global_causal B=None H=None Q_LEN=N_CTX KV_LEN=N_CTX device=device local_attn = functools partial flex_call block_mask=sliding_window_causal return_lse=True kernel_options=kernel_options global_attn = functools partial flex_call block_mask=global_causal return_lse=True kernel_options=kernel_options q k v = make_tensor make_tensor make_tensor gradOut = make_tensor requires_grad=False x_local lse_local = local_attn q k v x_global lse_global = global_attn q k v max_lse = torch maximum lse_local lse_global lse_global = lse_global - max_lse lse_local = lse_local - max_lse lse_global = torch exp lse_global lse_local = torch exp lse_local x = x_local lse_local None + x_global lse_global None lse_global None + lse_local None x backward gradOut flex_q_grad flex_k_grad flex_v_grad = q grad k grad v grad q grad = None k grad = None v grad = None out = torch nn functional scaled_dot_product_attention q k v is_causal=True out backward gradOut torch testing assert_close x out atol= e- rtol= e- torch testing assert_close flex_q_grad q grad atol= e- rtol= e- torch testing assert_close flex_k_grad k grad atol= e- rtol= e- torch testing assert_close flex_v_grad v grad atol= e- rtol= e- supported_platform skip_on_cpu test_mixed_device_error_message device Create tensors different devices cpu_tensor = torch randn device= cpu gpu_tensor = torch randn device=device Use different devices query key value query key value = cpu_tensor gpu_tensor cpu_tensor expected_error_message = Expected query key value have same device type f got query device query device key device key device f value device value device instead assertRaisesRegex ValueError expected_error_message flex_attention query key value supported_platform skip_on_cpu test_captured_wrong_device_error_message device means = torch randn device=device length_scales = torch logspace device= cpu euclidean_dist_pos_embed score b h q_idx k_idx q_pos = means q_idx k_pos = means k_idx dist = q_pos - k_pos pow sum - sqrt scale = length_scales h inv_dist = torch exp -dist scale inv_dist score expected_error_message = Buffers cannot created q k v = torch randn device=device _ range assertRaisesRegex RuntimeError expected_error_message torch compile flex_attention q k v score_mod=euclidean_dist_pos_embed supported_platform skip_on_cpu test_cant_lower_error_message device We can t lower -element reduction inside pointwise reduction means = torch randn device=device length_scales = torch logspace device=device euclidean_dist_pos_embed score b h q_idx k_idx q_pos = means q_idx k_pos = means k_idx dist = q_pos - k_pos pow sum - sqrt scale = length_scales h inv_dist = torch exp -dist scale inv_dist score expected_error_message = Buffers cannot created q k v = torch randn device=device _ range assertRaisesRegex RuntimeError expected_error_message torch compile flex_attention q k v score_mod=euclidean_dist_pos_embed supported_platform skip_on_cpu test_reduction_unrolled device We can t lower -element reduction inside pointwise reduction means = torch randn S device=device length_scales = torch logspace H device=device euclidean_dist_pos_embed score b h q_idx k_idx q_pos = means q_idx k_pos = means k_idx dist = q_pos - k_pos pow sum - sqrt scale = length_scales h inv_dist = torch exp -dist scale inv_dist score run_test euclidean_dist_pos_embed torch bfloat device=device supported_platform skip_on_cpu test_invalid_block_size device Create tensors different devices q k v = torch randn device=device _ range expected_error_message = ValueError Q KV block size must divisible BLOCK_M BLOCK_N block_mask = create_block_mask noop_mask BLOCK_SIZE= device=device assertRaisesRegex RuntimeError expected_error_message torch compile flex_attention q k v block_mask=block_mask supported_platform skip_on_cpu test_small_q_kv_len device make_tensor = functools partial torch ones device=device dtype=torch float requires_grad=True query key value = make_tensor make_tensor make_tensor kernel_options = FORCE_USE_FLEX_ATTENTION True out_eager lse_eager = flex_attention query key value return_lse=True kernel_options=kernel_options flex_compile = torch compile flex_attention fullgraph=True out_compiled lse_compiled = flex_compile query key value return_lse=True kernel_options=kernel_options assert torch equal out_eager out_compiled assert torch equal lse_eager lse_compiled grads_eager = torch autograd grad out_eager sum query key value grads_compile = torch autograd grad out_compiled sum query key value torch testing assert_close grads_eager grads_compile supported_platform skip_on_cpu test_dynamic_shapes_bug_dynamic_batch device _flex_attention_mask b h q_idx kv_idx input_lengths padding_condition = q_idx input_lengths b kv_idx input_lengths b padding_condition counter = CompileCounterWithBackend inductor Model torch nn Module __init__ dim= super __init__ subsampler = torch nn Conv d projector = torch nn Linear dim num_heads = forward x input_lengths x = subsampler x transpose - - transpose - - x = projector x transpose head_dim = x size - num_heads x = x view - x size num_heads head_dim x = x permute max_time = x size - mask = torch compile create_block_mask dynamic=True fullgraph=False functools partial _flex_attention_mask input_lengths=input_lengths B=input_lengths size H=None Q_LEN=max_time KV_LEN=max_time device=device x = torch compile flex_attention dynamic=True fullgraph=True backend=counter query=x key=x value=x block_mask=mask x model = Model device B F T = _ range x = torch randn B T F device=device l = torch randint T B device=device model x l assert counter frame_count == f Expected graph got counter frame_count graphs supported_platform skip_on_cpu test_dynamic_shapes_with_custom_kernel_options device make_tensor = functools partial torch ones device=device dtype=torch bfloat query key value = make_tensor make_tensor make_tensor kernel_options = BLOCK_M BLOCK_N out_eager = flex_attention query key value kernel_options=kernel_options flex_compile = torch compile flex_attention fullgraph=True dynamic=True out_compiled = flex_compile query key value kernel_options=kernel_options torch testing assert_close out_eager out_compiled atol= e- rtol= e- supported_platform test_dynamic_shapes_with_max_autotune device make_tensor = functools partial torch ones device=device dtype=torch float device == cpu torch bfloat query key value = make_tensor make_tensor make_tensor block_mask = create_block_mask _causal_mask None None device=device out_eager = flex_attention query key value block_mask=block_mask flex_compile = torch compile flex_attention fullgraph=True dynamic=True mode= max-autotune out_compiled = flex_compile query key value block_mask=block_mask torch testing assert_close out_eager out_compiled atol= e- rtol= e- supported_platform skip_on_cpu test_zero_length_sequence_error device make_tensor = functools partial torch ones Zero sequence dimension device=device dtype=torch bfloat query key value = make_tensor make_tensor make_tensor Test compiled mode - should also raise assertion error flex_compile = torch compile flex_attention fullgraph=True assertRaisesRegex torch _inductor exc InductorError Query length must greater than flex_compile query key value supported_platform test_causal_block_non_divisible_with_captured_buffer device Q_S = S - KV_S = S - offset_q = torch randn Q_S device=device dtype=torch bfloat offset_kv = torch randn KV_S device=device dtype=torch bfloat score_mod score b h q kv score + offset_q q + offset_kv kv mask_mod b h q kv q = kv block_mask = create_block_mask mask_mod B Q_S KV_S device=device attention = functools partial flex_attention block_mask=block_mask run_test_with_call attention Q_S=Q_S KV_S=KV_S dtype=torch bfloat device=device supported_platform test_non_divisible_with_captured_buffer device Q_S = S + KV_S = S + multiplier = torch randn Q_S device=device dtype=torch bfloat apply_multiplicative_bias score b h q_idx kv_idx score multiplier q_idx attention = functools partial flex_attention score_mod=apply_multiplicative_bias run_test_with_call attention Q_S=Q_S KV_S=KV_S dtype=torch bfloat device=device supported_platform test_num_warps_ _error device attention = functools partial flex_attention score_mod=_identity run_test_with_call attention dtype=torch float device=device Q_S= KV_S= Q_D= V_D= supported_platform unittest skipIf TEST_MULTIGPU detected only one GPU test_qkv_and_block_mask_on_the_same_device device make_tensor = functools partial torch ones device= cuda dtype=torch float requires_grad=True query key value = make_tensor make_tensor make_tensor mask_mod b h q kv q = kv block_mask = create_block_mask mask_mod device= cuda assertRaisesRegex RuntimeError Expect q k v block_mask same device torch compile flex_attention query key value block_mask=block_mask supported_platform skip_on_cpu unittest skipIf config triton native_matmul different dynamo counters test_free_symbol_dynamic device batch_flip_causal b h q_idx kv_idx q_idx = kv_idx b == SimpleAttention torch nn Module __init__ dim= n_head= super __init__ qkv = torch nn Linear dim dim n_head = n_head head_dim = dim n_head forward x block_mask=None B T C = x size qkv = qkv x view B T n_head head_dim qkv = qkv permute q k v = qkv y = flex_attention q k v block_mask=block_mask y transpose contiguous view B T C model = SimpleAttention device model compile mode= default dynamic=True sequence_len = Test different batch shapes dense masks torch _dynamo reset batch_shape Create dense mask rand_mask = torch randint batch_shape sequence_len device=device bool block_mask = torch compile create_block_mask dynamic=True B=batch_shape BLOCK_SIZE= mask_mod=lambda b h q_idx kv_idx ~rand_mask b q_idx H=None Q_LEN=sequence_len KV_LEN=sequence_len device=device Run forward pass x = torch randn batch_shape sequence_len device=device model x block_mask=block_mask assertEqual torch _dynamo utils counters aot_autograd ok supported_platform skip_on_cpu test_symbol_closure_in_score_mod device SimpleAttention torch nn Module __init__ dim= n_head= super __init__ qkv = torch nn Linear dim dim n_head = n_head head_dim = dim n_head forward x block_mask=None B T C = x size qkv = qkv x view B T n_head head_dim qkv = qkv permute q k v = qkv flex_attention q k v score_mod=lambda s b h q k s + B block_mask=block_mask model = SimpleAttention device torch _dynamo testing EagerAndRecordGraphs backend = EagerAndRecordGraphs model compile mode= default dynamic=True backend=backend sequence_len = torch _dynamo reset batch_shape x = torch randn batch_shape sequence_len device=device model x assertEqual len backend graphs assertExpectedInline backend graphs score_mod_ code strip \ forward child torch Tensor child_ torch Tensor child_ torch Tensor child_ torch Tensor child_ torch Tensor getitem torch SymInt add = child + getitem child = getitem = None add supported_platform skip_on_cpu test_fw_bw_graph_correctness device cnt = CompileCounterWithBackend aot_eager make_tensor = functools partial torch randn device=device dtype=torch float requires_grad=True query key value = make_tensor make_tensor make_tensor causal_mask b h q_idx kv_idx q_idx = kv_idx block_mask = create_block_mask causal_mask device=device func = torch compile flex_attention backend=cnt fullgraph=True out = func query key value _squared block_mask=block_mask out sum backward assertEqual cnt frame_count assertEqual len cnt graphs graph = cnt graphs norm_graph = normalize_gm graph print_readable print_output=False assertExpectedInline norm_graph \ GraphModule torch nn Module forward L_query_ f L_key_ f L_value_ f L_block_mask_kv_indices i L_block_mask_kv_num_blocks i L_block_mask_full_kv_num_blocks i L_block_mask_full_kv_indices i L_block_mask_q_num_blocks i L_block_mask_q_indices i L_block_mask_full_q_num_blocks i L_block_mask_full_q_indices i l_query_ = L_query_ l_key_ = L_key_ l_value_ = L_value_ l_block_mask_kv_indices = L_block_mask_kv_indices l_block_mask_kv_num_blocks = L_block_mask_kv_num_blocks l_block_mask_full_kv_num_blocks = L_block_mask_full_kv_num_blocks l_block_mask_full_kv_indices = L_block_mask_full_kv_indices l_block_mask_q_num_blocks = L_block_mask_q_num_blocks l_block_mask_q_indices = L_block_mask_q_indices l_block_mask_full_q_num_blocks = L_block_mask_full_q_num_blocks l_block_mask_full_q_indices = L_block_mask_full_q_indices score_mod_ = score_mod_ mask_fn_ = mask_fn_ flex_attention = torch ops higher_order flex_attention l_query_ l_key_ l_value_ score_mod_ l_block_mask_kv_num_blocks l_block_mask_kv_indices l_block_mask_full_kv_num_blocks l_block_mask_full_kv_indices l_block_mask_q_num_blocks l_block_mask_q_indices l_block_mask_full_q_num_blocks l_block_mask_full_q_indices mask_fn_ PRESCALE_QK False ROWS_GUARANTEED_SAFE False BLOCKS_ARE_CONTIGUOUS False WRITE_DQ True OUTPUT_LOGSUMEXP True OUTPUT_MAX False l_query_ = l_key_ = l_value_ = score_mod_ = l_block_mask_kv_num_blocks = l_block_mask_kv_indices = l_block_mask_full_kv_num_blocks = l_block_mask_full_kv_indices = l_block_mask_q_num_blocks = l_block_mask_q_indices = l_block_mask_full_q_num_blocks = l_block_mask_full_q_indices = mask_fn_ = None out f = flex_attention flex_attention = None out score_mod_ torch nn Module forward child f child_ i child_ i child_ i child_ i mul f = child child child = None mul mask_fn_ torch nn Module forward child i child_ i child_ i child_ i ge b = child_ = child_ child_ = child_ = None ge noqa B Save AOT graphs aot_graphs = torch _inductor compile_fx debug_compile_fx_inner graph example_inputs args kwargs aot_graphs append graph graph backend = functools partial compile_fx compile_fx inner_compile=debug_compile_fx_inner func = torch compile func backend=backend fullgraph=True out = func query key value _squared out sum backward joint_graph = normalize_gm aot_graphs print_readable print_output=False assertExpectedInline joint_graph \ GraphModule torch nn Module forward primals_ f primals_ f primals_ f full i full_default i convert_element_type i convert_element_type_ i getitem_ f getitem_ f tangents_ f full_default_ f = torch ops aten full default dtype = torch float layout = torch strided device = device type= GPU_TYPE index= pin_memory = False fw_graph = fw_graph joint_graph = joint_graph mask_graph = mask_graph flex_attention_backward = torch ops higher_order flex_attention_backward primals_ primals_ primals_ getitem_ getitem_ tangents_ full_default_ fw_graph joint_graph full full_default None None convert_element_type convert_element_type_ None None mask_graph PRESCALE_QK False ROWS_GUARANTEED_SAFE False BLOCKS_ARE_CONTIGUOUS False WRITE_DQ True OUTPUT_LOGSUMEXP True OUTPUT_MAX False primals_ = primals_ = primals_ = getitem_ = getitem_ = tangents_ = full_default_ = fw_graph = joint_graph = full = full_default = convert_element_type = convert_element_type_ = mask_graph = None getitem_ f = flex_attention_backward getitem_ f = flex_attention_backward getitem_ f = flex_attention_backward flex_attention_backward = None getitem_ getitem_ getitem_ fw_graph torch nn Module forward arg _ f arg _ i arg _ i arg _ i arg _ i mul f = torch ops aten mul Tensor arg _ arg _ arg _ = None mul joint_graph torch nn Module forward arg _ f arg _ i arg _ i arg _ i arg _ i arg _ f mul_ f = torch ops aten mul Tensor arg _ arg _ mul_ f = torch ops aten mul Tensor arg _ arg _ arg _ = arg _ = None add f = torch ops aten add Tensor mul_ mul_ mul_ = mul_ = None add None None None None mask_graph torch nn Module forward arg _ i arg _ i arg _ i arg _ i full_default b = torch ops aten full default True dtype = torch bool layout = torch strided device = device type= GPU_TYPE index= pin_memory = False full_default replace noqa B GPU_TYPE torch device device type supported_platform test_tensor_subclass_dispatch_order device Test tensor subclasses get proper dispatch priority over modes This test verifies fix allows tensor subclasses pyimpl run before FakeTensorMode FunctionalTensorMode implementations preventing issues where subclasses error as_strided would fail flex_attention torch utils _pytree pytree torch utils _python_dispatch return_and_correct_aliasing AsStridedErrorTensor torch Tensor staticmethod __new__ cls elem assert isinstance elem torch Tensor torch Tensor _make_wrapper_subclass cls elem shape strides=elem stride storage_offset=elem storage_offset dtype=elem dtype layout=elem layout device=elem device requires_grad=elem requires_grad __init__ elem elem = elem __repr__ f AsStridedErrorTensor elem __tensor_flatten__ elem None staticmethod __tensor_unflatten__ inner_tensors meta outer_size outer_stride assert meta None elem = inner_tensors elem AsStridedErrorTensor elem classmethod __torch_dispatch__ cls func types args kwargs=None Error as_strided called func torch ops aten as_strided default raise RuntimeError as_strided called AsStridedErrorTensor kwargs None kwargs = args_elem = pytree tree_map_only AsStridedErrorTensor lambda x x elem args kwargs_elem = pytree tree_map_only AsStridedErrorTensor lambda x x elem kwargs out = func args_elem kwargs_elem wrap_output x isinstance x torch Tensor AsStridedErrorTensor x x out_wrapped = pytree tree_map wrap_output out return_and_correct_aliasing func args kwargs out_wrapped torch _higher_order_ops flex_attention flex_attention flex_attention_hop flex_attention_hop py_impl AsStridedErrorTensor flex_attention_as_strided_error_tensor query torch Tensor key torch Tensor value torch Tensor score_mod block_mask scale kernel_options score_mod_other_buffers= mask_mod_other_buffers= inner_q inner_k inner_v = query elem key elem value elem out lse max_scores = flex_attention_hop inner_q inner_k inner_v score_mod block_mask scale kernel_options score_mod_other_buffers mask_mod_other_buffers AsStridedErrorTensor out AsStridedErrorTensor lse AsStridedErrorTensor max_scores Test setup B H S D = dtype = torch float Create regular tensors query_elem = torch randn B H S D device=device dtype=dtype key_elem = torch randn B H S D device=device dtype=dtype value_elem = torch randn B H S D device=device dtype=dtype Test Verify as_strided raises error when called directly AsStridedErrorTensor test_tensor = AsStridedErrorTensor query_elem assertRaisesRegex RuntimeError as_strided called AsStridedErrorTensor torch as_strided test_tensor size= B H S D stride=test_tensor stride Test Run flex_attention normal tensors first compiled_fn = torch compile flex_attention backend= aot_eager normal_out normal_lse = compiled_fn query_elem key_elem value_elem return_lse=True Test Wrap our subclass query = AsStridedErrorTensor query_elem key = AsStridedErrorTensor key_elem value = AsStridedErrorTensor value_elem This should NOT error as_strided after fix Before fix would error because FakeTensorMode would directly call flex_attention_fake_impl which uses as_strided out lse = compiled_fn query key value return_lse=True Verify we got valid output assertIsInstance out AsStridedErrorTensor assertIsInstance lse AsStridedErrorTensor assertEqual out shape B H S D assertEqual lse shape B H S Test Compare outputs between normal tensors subclassed tensors torch testing assert_close out elem normal_out rtol= e- atol= e- torch testing assert_close lse elem normal_lse rtol= e- atol= e- supported_platform skip_on_cuda test_cpu_error_message_return_lse device make_tensor = functools partial torch randn device= cpu dtype=torch float requires_grad=False query key value = make_tensor make_tensor make_tensor attention = torch compile flex_attention assertRaisesRegex torch _inductor exc InductorError r NotImplementedError torch compile CPU only supports inference ` return_lse ` supported yet attention query key value return_lse=True unittest skipIf TEST_MULTIGPU detected only one GPU test_device_cuda_ device TestModule torch nn Module forward q k v block_mask flex_attention q k v block_mask=block_mask q = torch randn device= cuda dtype=torch bfloat k = torch randn device= cuda dtype=torch bfloat v = torch randn device= cuda dtype=torch bfloat mask = create_block_mask lambda b h q_idx kv_idx q_idx = kv_idx B=None H=None Q_LEN= KV_LEN= device= cuda mod = torch compile TestModule attn_output = mod q k v mask assertEqual attn_output device torch device cuda supported_platform skip_on_cpu test_custom_score_mod_layout_freeze device torch manual_seed FlexAttentionCPB nn Module __init__ N int R int H int = hidden int = super __init__ mlp = nn Sequential nn Linear hidden nn GELU nn Linear hidden H bias=False gamma = nn Parameter torch zeros H H = H _init_tables N R register_buffer r_cutoff torch tensor R dtype=torch long persistent=False _init_tables N int R int - None P = N - R S = int P assert S S == P rng = torch arange - S - S dtype=torch float dY dX = torch meshgrid rng rng indexing= ij rel = torch stack dY max S - dX max S - dim=- reshape - rel_table = torch sign rel torch log p rel abs register_buffer rel_table rel_table persistent=False yy xx = torch arange S torch arange S Y X = torch meshgrid yy xx indexing= ij flat = torch stack Y X flatten d = flat None - flat None d = d permute contiguous d += S - d += S - d = S - l_idx = d sum - torch long idx = torch full N N dtype=torch long idx R R = l_idx register_buffer idx_table idx persistent=False _score_mod mu torch Tensor bt = mlp rel_table idx = idx_table mu_q mu_k = mu unbind gam_sig = torch sigmoid gamma score_mod score b h q kv has_bias = q = r_cutoff kv = r_cutoff l = idx q kv bias = bt l h w_gate = gam_sig h mu_q b h q + mu_k b h kv score + has_bias score dtype w_gate bias score_mod forward q k v mu flex_attention q k v score_mod=self _score_mod mu dtype = torch bfloat PLATFORM_SUPPORTS_BF torch float device_obj = torch device device module = FlexAttentionCPB N= R= device_obj compiled_module = torch compile module backend= inductor dynamic=False q = torch randn device=device_obj dtype=dtype k = torch randn_like q v = torch randn_like q mu = torch randn device=device_obj torch no_grad torch nn attention sdpa_kernel SDPBackend FLASH_ATTENTION eager_out = module q k v mu compiled_out = compiled_module q k v mu assertEqual compiled_out shape eager_out shape torch testing assert_close compiled_out float eager_out float atol= e- rtol= e- supported_platform skip_on_cpu common_utils parametrize ops_to_save torch ops aten mm default flex_attention_hop torch ops aten mm default flex_attention_hop test_selective_ac device ops_to_save FlexAttentionModule nn Module __init__ hidden_size num_heads super __init__ hidden_size = hidden_size num_heads = num_heads head_dim = hidden_size num_heads In-projections query key value q_proj = nn Linear hidden_size hidden_size k_proj = nn Linear hidden_size hidden_size v_proj = nn Linear hidden_size hidden_size Out-projection out_proj = nn Linear hidden_size hidden_size forward x batch_size seq_len _ = x size Project queries keys values q = q_proj x view batch_size seq_len num_heads head_dim transpose k = k_proj x view batch_size seq_len num_heads head_dim transpose v = v_proj x view batch_size seq_len num_heads head_dim transpose Apply flex attention attn_output = flex_attention q k v Reshape output attn_output = attn_output transpose contiguous view batch_size seq_len hidden_size Out projection output = out_proj attn_output output torch utils checkpoint checkpoint create_selective_checkpoint_contexts context_fn = functools partial create_selective_checkpoint_contexts ops_to_save Define model uses FlexAttention selective activation checkpointing SacModule nn Module __init__ hidden_size num_heads context_fn super __init__ flex_attn = FlexAttentionModule hidden_size num_heads context_fn = context_fn forward x flex_attn_fn x flex_attn x output = checkpoint flex_attn_fn x use_reentrant=False context_fn=self context_fn output flex_module = SacModule hidden_size= num_heads= context_fn=context_fn device dtype=torch bfloat x = torch ones device=device dtype=torch bfloat Run without compilation output_module = flex_module x compiled_module = torch compile flex_module output_compiled = compiled_module x torch testing assert_close output_module output_compiled rtol= e- atol= e- Calculate gradients compare them x requires_grad_ True output_module = flex_module x output_compiled = compiled_module x grad_output = torch ones_like output_module grad_module = torch autograd grad outputs=output_module inputs=x grad_outputs=grad_output retain_graph=True grad_compiled = torch autograd grad outputs=output_compiled inputs=x grad_outputs=grad_output torch testing assert_close grad_module grad_compiled rtol= e- atol= e- supported_platform skip_on_cpu test_selective_ac_with_max_autotune_short_query device functools partial torch utils checkpoint checkpoint CheckpointPolicy create_selective_checkpoint_contexts compute_intensive_ops = torch ops aten mm torch ops aten bmm policy_fn ctx op args kwargs op compute_intensive_ops CheckpointPolicy MUST_SAVE CheckpointPolicy PREFER_RECOMPUTE causal_mask b h q_idx kv_idx q_idx = kv_idx DummyAttentionModule nn Module __init__ dim= num_heads= super __init__ dim = dim num_heads = num_heads head_dim = dim num_heads q_proj = nn Linear dim dim k_proj = nn Linear dim dim v_proj = nn Linear dim dim out_proj = nn Linear dim dim _activation_checkpoint_context_fn = partial create_selective_checkpoint_contexts policy_fn _flex_attention = torch compile partial checkpoint flex_attention use_reentrant=False context_fn=self _activation_checkpoint_context_fn mode= max-autotune-no-cudagraphs forward x block_mask batch_size seq_len _ = x shape q = q_proj x k = k_proj x v = v_proj x q = q view batch_size seq_len num_heads head_dim transpose k = k view batch_size seq_len num_heads head_dim transpose v = v view batch_size seq_len num_heads head_dim transpose attn_out = _flex_attention q k v block_mask=block_mask attn_out = attn_out transpose contiguous view batch_size seq_len dim out = out_proj attn_out out batch_size = seq_len = dim = num_heads = model = DummyAttentionModule dim=dim num_heads=num_heads device x = torch randn batch_size seq_len dim device=device requires_grad=True block_mask = create_block_mask causal_mask B=batch_size H=num_heads Q_LEN=seq_len KV_LEN=seq_len device=device out = model x block_mask loss = out sum loss backward assertIsNotNone x grad supported_platform skip_on_cpu test_validate_small_embedding_size_error_message device eager support small embedding size q k v = torch randn device=device _ range flex_attention q k v compiled cpu support small embedding size q k v = torch randn device=device _ range flex_attention q k v compiled gpu kernel does support small embedding size q k v = torch randn device=device _ range compiled_fa = torch compile flex_attention assertRaisesRegex torch _inductor exc InductorError NYI embedding dimension query key value must least got E= Ev= compiled_fa q k v compiled gpu kernel supports large embedding size q k v = torch randn device=device _ range compiled_fa = torch compile flex_attention unittest skipIf has_triton HAS_WARP_SPEC reason= FBCODE Triton required test test_triton_template_warp_specialization device make_tensor torch rand device=device dtype=torch bfloat q k v = make_tensor make_tensor make_tensor flex_compiled = torch compile flex_attention fullgraph=True positional_args = q k v keyword_args = kernel_options num_warps num_consumer_groups num_buffers_warp_spec Check kernel code contains warp specialization parameters _ kernel_code = run_and_get_code flex_compiled positional_args keyword_args assert kernel_code None Failed retrieve compiled kernel code assert num_consumer_groups kernel_code num_consumer_groups missing kernel definition assert num_buffers_warp_spec kernel_code num_buffers_warp_spec missing kernel definition Validate correctness C = flex_compiled q k v C = flex_attention q k v assert torch allclose C C atol= e- rtol= e- Warp specialized kernel result differs reference supported_platform skip_on_cpu skipCUDAIf has_triton_tma_device Requires TMA enabled CUDA device test_tma_with_customer_kernel_options device make_tensor = functools partial torch ones device=device dtype=torch bfloat query key value = make_tensor make_tensor make_tensor kernel_options_ = BLOCK_M BLOCK_N USE_TMA False kernel_options_ = BLOCK_M BLOCK_N USE_TMA True flex_compile = torch compile flex_attention fullgraph=True dynamic=True out_compiled = flex_compile query key value kernel_options=kernel_options_ out_tma_compiled = flex_compile query key value kernel_options=kernel_options_ vanilla compiled vs TMA compiled torch testing assert_close out_tma_compiled out_compiled atol= e- rtol= e- supported_platform skip_on_cpu test_large_batch_heads_grid_dimension device B H S D = make_tensor = functools partial torch randn B H S D device=device dtype=torch float requires_grad=True query key value = make_tensor make_tensor make_tensor flex_compile = torch compile flex_attention fullgraph=True dynamic=True out_compiled = flex_compile query key value assertEqual out_compiled shape B H S D grad_output = torch randn_like out_compiled out_compiled backward grad_output assertIsNotNone query grad assertIsNotNone key grad assertIsNotNone value grad assertEqual query grad shape query shape assertEqual key grad shape key shape assertEqual value grad shape value shape supported_platform test_debug_flag_disables_internal_compilation device Test _FLEX_ATTENTION_DISABLE_COMPILE_DEBUG flag bypasses internal compilation torch nn attention flex_attention fa original_flag = fa _FLEX_ATTENTION_DISABLE_COMPILE_DEBUG original_warnings_shown = fa _WARNINGS_SHOWN copy try B H S D = query = torch randn B H S D device=device dtype=torch float key = torch randn B H S D device=device dtype=torch float value = torch randn B H S D device=device dtype=torch float simple_score_mod score b h q_idx kv_idx score Test debug flag False - should warn fa _FLEX_ATTENTION_DISABLE_COMPILE_DEBUG = False fa _WARNINGS_SHOWN clear assertWarns UserWarning cm out_compiled = fa flex_attention query key value score_mod=simple_score_mod assertIn flex_attention called without torch compile str cm warning Test debug flag True - should NOT warn fa _FLEX_ATTENTION_DISABLE_COMPILE_DEBUG = True Should error warnings catch_warnings warnings simplefilter error out_debug = fa flex_attention query key value score_mod=simple_score_mod torch testing assert_close out_compiled out_debug rtol= e- atol= e- finally fa _FLEX_ATTENTION_DISABLE_COMPILE_DEBUG = original_flag fa _WARNINGS_SHOWN = original_warnings_shown TestBlockMask InductorTestCase setUp super setUp supported_platform test_block_mask_attributes device offset = torch zeros device=device causal_mask b h q kv q + offset b = kv block_mask = create_block_mask causal_mask device=device assertEqual block_mask shape assertEqual block_mask shape assertEqual block_mask shape assertEqual block_mask numel assertEqual block_mask sparsity assertEqual block_mask sparsity assertEqual block_mask sparsity assertEqual block_mask sparsity block_mask sparsity offset = torch arange device=device block_mask = create_block_mask causal_mask device=device assertEqual block_mask sparsity assertTrue block_mask sparsity block_mask sparsity assertTrue block_mask sparsity block_mask sparsity supported_platform common_utils parametrize BLOCK_SIZE test_block_size_changes device BLOCK_SIZE Union int tuple int int B H Q_LEN KV_LEN = isinstance BLOCK_SIZE int Q_BLOCK_SIZE = BLOCK_SIZE KV_BLOCK_SIZE = BLOCK_SIZE Q_BLOCK_SIZE KV_BLOCK_SIZE = BLOCK_SIZE block_mask = create_block_mask noop_mask B H Q_LEN KV_LEN BLOCK_SIZE=BLOCK_SIZE device=device assertEqual block_mask BLOCK_SIZE Q_BLOCK_SIZE KV_BLOCK_SIZE assertEqual block_mask shape B H Q_LEN KV_LEN supported_platform test_getitem device offset = torch zeros device=device causal_mask b h q kv q + offset b = kv block_mask = create_block_mask causal_mask device=device assert block_mask kv_num_blocks shape == assert block_mask kv_indices shape == Index batch dimension new_block_mask = block_mask assert new_block_mask kv_num_blocks shape == assert new_block_mask kv_indices shape == Index batch head dimension new_block_mask = block_mask assert new_block_mask kv_num_blocks shape == assert new_block_mask kv_indices shape == Index batch head dimension - semantics new_block_mask = block_mask - - assert new_block_mask kv_num_blocks shape == assert new_block_mask kv_indices shape == slicing batch head dimension new_block_mask = block_mask assert new_block_mask kv_num_blocks shape == assert new_block_mask kv_indices shape == slicing batch head query dimension new_block_mask = block_mask torch tensor dtype=torch int assert new_block_mask kv_num_blocks shape == assert new_block_mask kv_indices shape == slicing batch head query dimension q_index = torch tensor dtype=torch int new_block_mask = block_mask q_index assertEqual new_block_mask kv_num_blocks ndim assertEqual new_block_mask kv_indices ndim torch testing assert_close new_block_mask kv_num_blocks block_mask kv_num_blocks q_index torch testing assert_close new_block_mask kv_indices block_mask kv_indices q_index block_mask full_kv_num_blocks None assert new_block_mask full_kv_num_blocks None assert new_block_mask full_kv_indices None torch testing assert_close new_block_mask full_kv_num_blocks block_mask full_kv_num_blocks q_index torch testing assert_close new_block_mask full_kv_indices block_mask full_kv_indices q_index supported_platform test_sliced_blockmask_mask_mod_error device Test sliced BlockMask raises helpful error when used flex_attention causal_mask b h q_idx kv_idx q_idx = kv_idx base_mask = create_block_mask causal_mask B= H= Q_LEN= KV_LEN= device=device sliced_mask = base_mask q = torch randn device=device k = torch randn device=device v = torch randn device=device compiled_fa = torch compile flex_attention assertRaisesRegex RuntimeError Cannot use mask_mod sliced BlockMask compiled_fa q k v block_mask=sliced_mask supported_platform test_block_mask_device_change device device = torch device device offset = torch zeros device=device causal_mask b h q kv q + offset b = kv block_mask = create_block_mask causal_mask device=device assert block_mask kv_indices device type == device type assert block_mask kv_num_blocks device type == device type assert block_mask q_indices device type == device type assert block_mask q_num_blocks device type == device type block_mask = block_mask cpu assert block_mask kv_indices is_cpu assert block_mask kv_num_blocks is_cpu assert block_mask q_indices is_cpu assert block_mask q_num_blocks is_cpu block_mask = block_mask device assert block_mask kv_indices device type == device type assert block_mask kv_num_blocks device type == device type assert block_mask q_indices device type == device type assert block_mask q_num_blocks device type == device type supported_platform test_compiling_create_block_mask device seq = torch arange device=device mask_mod b h q kv q = kv seq q == seq kv block_mask = torch compile create_block_mask fullgraph=True mask_mod device=device assertIsInstance block_mask BlockMask assertEqual block_mask kv_num_blocks shape torch Size assertEqual block_mask kv_indices shape torch Size supported_platform test_compiling_create_block_mask_no_recompile device mask_mod b h q kv q = kv torch _dynamo reset block_mask = torch compile create_block_mask mask_mod device=device assertIsInstance block_mask BlockMask assertEqual block_mask kv_num_blocks shape torch Size assertEqual block_mask kv_indices shape torch Size assertEqual torch _dynamo utils counters aot_autograd ok automatic dynamic shapes triggered recompilation block_mask = torch compile create_block_mask mask_mod device=device assertIsInstance block_mask BlockMask assertEqual block_mask kv_num_blocks shape torch Size assertEqual block_mask kv_indices shape torch Size assertEqual torch _dynamo utils counters aot_autograd ok no recompilation block_mask = torch compile create_block_mask mask_mod device=device assertIsInstance block_mask BlockMask assertEqual block_mask kv_num_blocks shape torch Size assertEqual block_mask kv_indices shape torch Size assertEqual torch _dynamo utils counters aot_autograd ok supported_platform test_block_mask_viz device causal_mask b h q kv q = kv block_mask = create_block_mask causal_mask device=device replace_non_printable s replace c c string printable c == s c join replace c c s assertExpectedInline replace_non_printable str block_mask \ BlockMask shape= s s s ssparsity= s s ssssssssssssssssssssssssssssss ssssssssssssssssssssssssssss ssssssssssssssssssssssssss ssssssssssssssssssssssss ssssssssssssssssssssss ssssssssssssssssssss ssssssssssssssssss ssssssssssssssss ssssssssssssss ssssssssssss ssssssssss ssssssss ssssss ssss ss offset = torch arange device=device causal_offset_mask b h q kv q + offset b = kv block_mask = create_block_mask causal_offset_mask device=device str_block_mask = str block_mask assertTrue sparsity= str_block_mask generate_test_inputs full_seq_len bool device full_seq_len kv_num_blocks = torch tensor dtype=torch int device=device view kv_indices = torch tensor - dtype=torch int device=device view full_kv_num_blocks = torch tensor dtype=torch int device=device view full_kv_indices = torch tensor - dtype=torch int device=device view kv_num_blocks = torch tensor dtype=torch int device=device view kv_indices = torch tensor dtype=torch int device=device view full_kv_indices = None full_kv_num_blocks = None kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices supported_platform common_utils parametrize full_indices False True test_from_kv_blocks device full_indices bool kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices = generate_test_inputs full_indices device=device block_mask = BlockMask from_kv_blocks kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices assertIsInstance block_mask BlockMask torch testing assert_close block_mask kv_num_blocks kv_num_blocks torch testing assert_close block_mask kv_indices kv_indices full_indices torch testing assert_close block_mask full_kv_num_blocks full_kv_num_blocks torch testing assert_close block_mask full_kv_indices full_kv_indices torch testing assert_close block_mask q_num_blocks torch tensor dtype=torch int device=device view torch testing assert_close block_mask q_indices torch tensor dtype=torch int device=device view torch testing assert_close block_mask full_q_num_blocks torch tensor dtype=torch int device=device view torch testing assert_close block_mask full_q_indices torch tensor dtype=torch int device=device view torch testing assert_close block_mask q_num_blocks torch tensor dtype=torch int device=device view torch testing assert_close block_mask q_indices torch tensor dtype=torch int device=device view assertIsNone block_mask full_kv_num_blocks assertIsNone block_mask full_kv_indices assertIsNone block_mask full_q_num_blocks assertIsNone block_mask full_q_indices supported_platform test_block_size device kv_num_blocks kv_indices _ _ = generate_test_inputs False device block_mask = BlockMask from_kv_blocks kv_num_blocks kv_indices assertEqual block_mask BLOCK_SIZE _DEFAULT_SPARSE_BLOCK_SIZE _DEFAULT_SPARSE_BLOCK_SIZE custom_block_size = block_mask_custom = BlockMask from_kv_blocks kv_num_blocks kv_indices BLOCK_SIZE=custom_block_size assertEqual block_mask_custom BLOCK_SIZE custom_block_size supported_platform test_upcast_appropriately device q = torch randn dtype=torch float device=device k = torch randn dtype=torch float device=device v = torch randn dtype=torch float device=device mass = torch ones dtype=torch float device=device score_mod score b h q_idx kv_idx score + torch log mass torch compile flex_attention q k v score_mod=score_mod supported_platform test_init_mismatched_full_kv device kv_num_blocks kv_indices full_kv_num_blocks _ = generate_test_inputs True device assertRaises AssertionError BlockMask kv_num_blocks=kv_num_blocks kv_indices=kv_indices full_kv_num_blocks=full_kv_num_blocks full_kv_indices=None Mismatched should raise error q_num_blocks=kv_num_blocks q_indices=kv_indices full_q_num_blocks=None full_q_indices=None BLOCK_SIZE= mask_mod=noop_mask seq_lengths= supported_platform test_init_mismatched_full_q device kv_num_blocks kv_indices _ _ = generate_test_inputs False device assertRaises AssertionError BlockMask kv_num_blocks=kv_num_blocks kv_indices=kv_indices full_kv_num_blocks=None full_kv_indices=None q_num_blocks=kv_num_blocks q_indices=kv_indices full_q_num_blocks=kv_num_blocks full_q_indices=None Mismatched should raise error BLOCK_SIZE= mask_mod=noop_mask seq_lengths= supported_platform test_doc_mask_clamped_repro device _offsets_to_doc_ids_tensor offsets device = offsets device counts = offsets - offsets - torch repeat_interleave torch arange len counts device=device dtype=torch int counts length_to_offsets lengths list int device Union str torch device - Tensor offsets = offsets extend lengths offsets = torch tensor offsets device=device dtype=torch int offsets = torch cumsum offsets dim=- offsets generate_doc_mask_mod offsets Tensor - _mask_mod_signature document_id = _offsets_to_doc_ids_tensor offsets doc_mask_mod b h q_idx kv_idx same_doc = document_id q_idx == document_id kv_idx same_doc doc_mask_mod random seed generate_random_lengths total_length num_documents lengths = num_documents remaining_length = total_length - num_documents _ range remaining_length index = random randint num_documents - lengths index += lengths max_seq_len doc_count = SEQ_LEN = max_seq_len lengths = generate_random_lengths max_seq_len doc_count offsets = length_to_offsets lengths device document_causal_mask = generate_doc_mask_mod offsets block_mask_compiled = torch compile create_block_mask document_causal_mask SEQ_LEN SEQ_LEN device=device block_mask = torch compile create_block_mask document_causal_mask SEQ_LEN SEQ_LEN device=device assertEqual block_mask_compiled kv_indices block_mask kv_indices assertEqual block_mask_compiled full_kv_indices block_mask full_kv_indices i range lengths = generate_random_lengths + i offsets = length_to_offsets lengths device doc_ids = _offsets_to_doc_ids_tensor offsets doc_mask_mod b h q_idx kv_idx doc_ids q_idx clamp doc_ids shape - == doc_ids kv_idx clamp doc_ids shape - q k v = torch randn + i device=device _ range block_mask = create_block_mask doc_mask_mod None None + i + i device=device torch compile flex_attention q k v block_mask=block_mask supported_platform test_eager_tracing_correctness device qk_dims = v_dims = q_heads = kv_heads = seq_len = batch_size = make_tensor = functools partial torch randn device=device dtype=torch float q = make_tensor batch_size q_heads seq_len qk_dims k = make_tensor batch_size kv_heads seq_len qk_dims v = make_tensor batch_size kv_heads seq_len v_dims flex_attention_fn out = flex_attention q k v enable_gqa=True out view batch_size q_heads seq_len Run compilation compiled_fn = torch compile flex_attention_fn fullgraph=True result = compiled_fn Assert expected output shape expected_shape = batch_size q_heads seq_len assertEqual result shape expected_shape f Expected output shape expected_shape got result shape supported_platform skip_on_xpu test_create_is_cuda_graphable device mask_mod b h q kv q = kv g = torch cuda CUDAGraph torch cuda graph g create_block_mask mask_mod None None g replay common_utils parametrize compile False True supported_platform test_block_mask_vs_sequence_lengths device compile compile flex_attention_call = torch compile flex_attention flex_attention_call = flex_attention mask_mod b h q_idx kv_idx q_idx = kv_idx create_inputs S q k v = torch randn S dtype=torch float requires_grad=True device=device _ range q k v block_mask = create_block_mask mask_mod None None device=device flex_attention_call create_inputs block_mask=block_mask assertRaisesRegex ValueError block_mask created flex_attention_call create_inputs block_mask=block_mask block_mask = create_block_mask mask_mod None None device=device assertRaisesRegex ValueError block_mask created flex_attention_call create_inputs block_mask=block_mask supported_platform common_utils parametrize full_indices False True test_from_kv_blocks_without_q_computation device full_indices bool kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices = generate_test_inputs full_indices device=device block_mask = BlockMask from_kv_blocks kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices compute_q_blocks=False assertIsInstance block_mask BlockMask assertEqual block_mask kv_num_blocks kv_num_blocks assertEqual block_mask kv_indices kv_indices assertIsNone block_mask q_num_blocks assertIsNone block_mask q_indices assertIsNone block_mask full_q_num_blocks assertIsNone block_mask full_q_indices full_indices assertEqual block_mask full_kv_num_blocks full_kv_num_blocks assertEqual block_mask full_kv_indices full_kv_indices assertIsNone block_mask full_kv_num_blocks assertIsNone block_mask full_kv_indices supported_platform skip_on_cpu test_backward_error_with_none_q_indices device N_BLOCKS = B H S D = S_KV = N_BLOCKS S kv_num_blocks = torch tensor N_BLOCKS dtype=torch int device=device kv_indices = torch tensor dtype=torch int device=device block_mask = BlockMask from_kv_blocks kv_num_blocks kv_indices compute_q_blocks=False q = torch randn B H S D dtype=torch float device=device requires_grad=True k = torch randn B H S_KV D dtype=torch float device=device requires_grad=True v = torch randn B H S_KV D dtype=torch float device=device requires_grad=True flex_compile = torch compile flex_attention fullgraph=True torch no_grad out_no_grad = flex_compile q k v block_mask=block_mask assertEqual out_no_grad shape B H S D Forward pass grad enabled should error immediately assertRaisesRegex RuntimeError BlockMask q_indices None Backward pass requires q_indices computed Please create BlockMask compute_q_blocks=True flex_compile q k v block_mask=block_mask supported_platform skip_on_cpu test_flex_attention_poisoned_rel_logits device B = H = S = D = q k v = torch randn B H S D requires_grad=True device=device _ range rel_logits = torch randn B H S S device=device rel_logits B = float nan score_mod score b h q kv score + rel_logits b h q kv causal b torch Tensor h torch Tensor q torch Tensor kv torch Tensor - torch Tensor q = kv block_mask = create_block_mask causal B H S S device=device out = torch compile flex_attention q k v score_mod=score_mod block_mask=block_mask out sum backward assert out isfinite all item assert q grad isfinite all item assert k grad isfinite all item assert v grad isfinite all item supported_platform skip_on_cpu test_flex_attention_poison_mod_fwd device Div score should cause our edge case handiling NaN B = H = S = D = q k v = torch randn B H S D requires_grad=True device=device _ range score_mod score b h q kv score causal b torch Tensor h torch Tensor q torch Tensor kv torch Tensor - torch Tensor q = kv block_mask = create_block_mask causal B H S S device=device out = torch compile flex_attention backend= inductor q k v score_mod=score_mod block_mask=block_mask out sum backward assert out isfinite all item assert q grad isfinite all item assert k grad isfinite all item assert v grad isfinite all item supported_platform skip_on_cpu test_flex_attention_poison_mod_bwd device log score should cause our edge case handiling NaN grad score B = H = S = D = q k v = torch randn B H S D requires_grad=True device=device _ range score_mod score b h q kv torch where score torch log score score causal b torch Tensor h torch Tensor q torch Tensor kv torch Tensor - torch Tensor q = kv block_mask = create_block_mask causal B H S S device=device out = torch compile flex_attention backend= inductor q k v score_mod=score_mod block_mask=block_mask out sum backward assert out isfinite all item assert q grad isfinite all item assert k grad isfinite all item assert v grad isfinite all item supported_platform skip_on_cpu test_forward_pass_with_none_q_indices device N_BLOCKS = B H S D = S_KV = N_BLOCKS S kv_num_blocks = torch tensor N_BLOCKS dtype=torch int device=device kv_indices = torch tensor dtype=torch int device=device block_mask = BlockMask from_kv_blocks kv_num_blocks kv_indices compute_q_blocks=False q = torch randn B H S D dtype=torch float device=device k = torch randn B H S_KV D dtype=torch float device=device v = torch randn B H S_KV D dtype=torch float device=device flex_compile = torch compile flex_attention fullgraph=True out = flex_compile q k v block_mask=block_mask assertEqual out shape B H S D assertIsInstance out torch Tensor assertEqual out dtype torch float supported_platform test_block_mask_operations_with_none_q_indices device kv_num_blocks = torch tensor dtype=torch int device=device kv_indices = torch tensor dtype=torch int device=device block_mask = BlockMask from_kv_blocks kv_num_blocks kv_indices compute_q_blocks=False assertEqual block_mask shape assertEqual block_mask BLOCK_SIZE sliced_mask = block_mask assertEqual sliced_mask shape assertIsNone sliced_mask q_indices assertIsNone sliced_mask q_num_blocks Test device movement device = cpu cpu_mask = block_mask cpu assertEqual cpu_mask kv_num_blocks device type cpu assertIsNone cpu_mask q_indices supported_platform skip_on_cpu test_broadcasted_head_block_mask device torch manual_seed causal_mask b h q_idx kv_idx q_idx = kv_idx get_mask_mod_with_offset mask_mod offset_tensor _mask_mod b h q kv mask_mod b h q + offset_tensor kv _mask_mod B T H D current_pos = dtype = torch float q = torch randn B H D device=device dtype=dtype k_cache = torch randn B H T D device=device dtype=dtype v_cache = torch randn B H T D device=device dtype=dtype Keep future tokens tiny avoid numerical issues when using full caches k_cache current_pos + = torch randn_like k_cache current_pos + e- v_cache current_pos + = torch randn_like v_cache current_pos + e- k_cropped = k_cache current_pos + v_cropped = v_cache current_pos + sdpa_output = torch nn functional scaled_dot_product_attention q k_cropped v_cropped attn_mask=None base_mask = create_block_mask causal_mask B=B H=None broadcast across heads Q_LEN=T KV_LEN=T device=device _compile=True q_block_size = base_mask BLOCK_SIZE block_offset = current_pos q_block_size mask_slice = base_mask block_offset offset_tensor = torch tensor current_pos device=device mask_slice mask_mod = get_mask_mod_with_offset base_mask mask_mod offset_tensor mask_slice seq_lengths = mask_slice seq_lengths fa = torch compile flex_attention dynamic=True flex_output = fa q k_cache v_cache block_mask=mask_slice assertEqual flex_output sdpa_output atol= e- rtol= e- supported_platform test_pytree_flatten_unflatten device Test BlockMask can correctly flattened unflattened using methods causal_mask b h q_idx kv_idx q_idx = kv_idx Create BlockMask various attributes set block_mask = create_block_mask causal_mask B= H= Q_LEN= KV_LEN= device=device Flatten unflatten using methods tensors context = block_mask _flatten reconstructed_mask = BlockMask _unflatten tensors context Verify reconstructed mask has same attributes assertEqual reconstructed_mask shape block_mask shape assertEqual reconstructed_mask sparsity block_mask sparsity Verify all tensor attributes equal using _TENSOR_ATTRS attr_name BlockMask _TENSOR_ATTRS original_value = getattr block_mask attr_name reconstructed_value = getattr reconstructed_mask attr_name original_value None assertIsNone reconstructed_value f Tensor attribute attr_name should None got reconstructed_value assertIsInstance original_value torch Tensor f Expected attr_name Tensor assertTrue torch equal original_value reconstructed_value f Tensor attribute attr_name equal after reconstruction Verify all context attributes equal using _CONTEXT_ATTRS attr_name BlockMask _CONTEXT_ATTRS original_value = getattr block_mask attr_name reconstructed_value = getattr reconstructed_mask attr_name assertEqual original_value reconstructed_value f Context attribute attr_name equal after reconstruction supported_platform test_pytree_flatten_with_keys device Test BlockMask _flatten_with_keys works correctly tracing causal_mask b h q_idx kv_idx q_idx = kv_idx block_mask = create_block_mask causal_mask B= H= Q_LEN= KV_LEN= device=device tensors_with_keys context_with_keys = block_mask _flatten_with_keys assertEqual len tensors_with_keys len BlockMask _TENSOR_ATTRS assertEqual len context_with_keys len BlockMask _CONTEXT_ATTRS torch utils _pytree GetAttrKey key tensor tensors_with_keys assertIsInstance key GetAttrKey assertIsNotNone key key value context_with_keys assertIsInstance key GetAttrKey assertIsNotNone key supported_platform test_pytree_preserves_new_attributes device Test BlockMask _TENSOR_ATTRS _CONTEXT_ATTRS correctly defined flatten unflatten preserves all attributes these lists causal_mask b h q_idx kv_idx q_idx = kv_idx block_mask = create_block_mask causal_mask B= H= Q_LEN= KV_LEN= device=device Flatten unflatten using methods tensors context = block_mask _flatten reconstructed_mask = BlockMask _unflatten tensors context Verify number tensors context values matches attribute lists assertEqual len tensors len BlockMask _TENSOR_ATTRS Number tensors should match _TENSOR_ATTRS length assertEqual len context len BlockMask _CONTEXT_ATTRS Number context values should match _CONTEXT_ATTRS length Verify all attributes lists exist equal after reconstruction attr_name BlockMask _TENSOR_ATTRS + BlockMask _CONTEXT_ATTRS assertTrue hasattr reconstructed_mask attr_name f Reconstructed mask missing attribute attr_name original_value = getattr block_mask attr_name reconstructed_value = getattr reconstructed_mask attr_name isinstance original_value torch Tensor assertTrue torch equal original_value reconstructed_value f Tensor attribute attr_name equal after reconstruction original_value None assertIsNone reconstructed_value f Attribute attr_name should None got reconstructed_value assertEqual original_value reconstructed_value f Attribute attr_name equal after reconstruction large_tensor_test_class GB device=test_device TestPagedAttention InductorTestCase setUp super setUp skipCPUIf LONG_COMPILATION_ON_CPU skip UT CPU due long compilation time found CI _check_equal golden_out torch Tensor ref_out torch Tensor compiled_out torch Tensor fudge_factor float tensor_name Optional str = None compiled_error = golden_out - compiled_out abs mean ref_error = golden_out - ref_out abs mean torch isnan compiled_error any torch isnan ref_error any assertTrue False Output Grad NaN compiled_error ref_error fudge_factor name = tensor_name tensor_name None msg = f name Compiled error compiled_error greater than ref error ref_error more than fudge_factor X assertTrue False msg allocate_page_cache n_pages int page_size int device str max_batch_size = paged_cache = PagedAttention n_pages page_size max_batch_size device=device paged_cache cdiv x y x + y - y roundup x y x + y - y y supported_platform test_page_allocation device n_pages page_size = paged_cache = allocate_page_cache n_pages page_size device=device batch_reserve paged_cache torch tensor assertRaisesRegex AssertionError requested pages there only empty pages paged_cache reserve torch tensor device=device torch tensor device=device paged_cache erase torch tensor device=device paged_cache reserve torch tensor device=device torch tensor device=device supported_platform test_allocate device n_pages page_size = paged_cache = allocate_page_cache n_pages page_size device=device target_seq_len = torch tensor batch_reserve paged_cache target_seq_len expected_allocated_pages = cdiv target_seq_len page_size sum assertEqual paged_cache capacity roundup target_seq_len page_size assertEqual len paged_cache empty_pages n_pages - expected_allocated_pages deallocate batch paged_cache erase torch tensor device=device target_seq_len = torch tensor expected_allocated_pages = cdiv target_seq_len page_size sum assertEqual paged_cache capacity roundup target_seq_len page_size assertEqual len paged_cache empty_pages n_pages - expected_allocated_pages re-allocate target_seq_len = torch tensor batch_reserve paged_cache target_seq_len expected_allocated_pages = cdiv target_seq_len page_size sum assertEqual paged_cache capacity roundup target_seq_len page_size assertEqual len paged_cache empty_pages n_pages - expected_allocated_pages deallocate all batches paged_cache erase torch tensor assertEqual paged_cache capacity torch tensor assertEqual len paged_cache empty_pages n_pages supported_platform test_convert_logical_block_mask device n_pages page_size max_batch_size max_seq_len = paged_cache = PagedAttention n_pages page_size max_batch_size device=device batch_reserve paged_cache torch tensor device=device batch_reserve paged_cache torch tensor device=device batch_reserve paged_cache torch tensor device=device batch_reserve paged_cache torch tensor device=device expected_page_table = torch tensor - - - - - - - - device=device assertEqual paged_cache capacity torch tensor device=device assertEqual paged_cache page_table expected_page_table Get block mask causal_mask b h q kv q = kv block_mask = create_block_mask causal_mask max_batch_size max_seq_len max_seq_len device=device kv_len_tensor = torch full max_batch_size max_seq_len device=device dtype=torch int new_block_mask = paged_cache convert_logical_block_mask block_mask kv_len=kv_len_tensor zeros = Check new block mask correct expected_kv_num_blocks = torch tensor device=device dtype=torch int expected_kv_indices = torch tensor zeros zeros zeros zeros zeros zeros zeros zeros device=device dtype=torch int expected_full_kv_num_blocks = torch tensor device=device dtype=torch int expected_full_kv_indices = torch tensor zeros zeros zeros zeros zeros zeros zeros zeros device=device dtype=torch int assertEqual new_block_mask kv_num_blocks expected_kv_num_blocks assertEqual new_block_mask kv_indices expected_kv_indices assertEqual new_block_mask full_kv_num_blocks expected_full_kv_num_blocks assertEqual new_block_mask full_kv_indices expected_full_kv_indices supported_platform test_convert_mask_mod device n_pages page_size max_batch_size = paged_cache = PagedAttention n_pages page_size max_batch_size device=device batch_reserve paged_cache torch tensor device=device batch_reserve paged_cache torch tensor device=device batch_reserve paged_cache torch tensor device=device batch_reserve paged_cache torch tensor device=device expected_page_table = torch tensor - - - - - - - - device=device assertEqual paged_cache capacity torch tensor device=device assertEqual paged_cache page_table expected_page_table expected_physical_to_logical = torch tensor - - - - - - - - device=device assertEqual paged_cache physical_to_logical expected_physical_to_logical Get block mask causal_mask b h q kv q = kv converted_causal_mask = paged_cache get_mask_mod causal_mask Equivalent causal_mask assertEqual converted_causal_mask True Equivalent causal_mask assertEqual converted_causal_mask True Not found corresponding logical block assertEqual converted_causal_mask False Equivalent causal_mask assertEqual converted_causal_mask True supported_platform test_update device dtype = torch float n_pages page_size max_batch_size max_seq_len = paged_cache = PagedAttention n_pages page_size max_batch_size device=device n_heads head_dim = cache_shape = n_heads n_pages page_size head_dim k_cache = torch zeros cache_shape dtype=dtype device=device batch_reserve paged_cache torch tensor device=device batch_reserve paged_cache torch tensor device=device batch_reserve paged_cache torch tensor device=device expected_page_table = torch tensor - - - - - - device=device assertEqual paged_cache page_table expected_page_table batch_idx = torch arange max_batch_size device=device dtype=torch int input_pos = torch arange max_seq_len device=device dtype=torch int unsqueeze expand max_batch_size max_seq_len k = torch arange max_batch_size n_heads max_seq_len head_dim device=device dtype=dtype view max_batch_size n_heads max_seq_len head_dim v = k detach clone v_cache = k_cache detach clone paged_cache assign batch_idx input_pos k v k_cache v_cache expected_cache = torch tensor h = page = page = page = page = page = page = h = page = page = page = page = page = page = device=device dtype=dtype assertEqual k_cache expected_cache supported_platform dtypes device_configs cpu dtypes dtypesIfCUDA device_configs cuda dtypes dtypesIfXPU device_configs xpu dtypes common_utils parametrize score_mod test_score_mods test_paged_builtin_score_mods device dtype torch dtype score_mod Callable n_pages page_size max_batch_size max_seq_len = n_heads head_dim = causal_mask b h q kv q = kv block_mask = create_block_mask causal_mask max_batch_size max_seq_len max_seq_len device=device q = torch randn max_batch_size n_heads max_seq_len head_dim device=device dtype=dtype requires_grad=False k = torch randn max_batch_size n_heads max_seq_len head_dim device=device dtype=dtype requires_grad=False v = torch randn max_batch_size n_heads max_seq_len head_dim device=device dtype=dtype requires_grad=False q_ref k_ref v_ref = query_key_value_clones q k v q_gold k_gold v_gold = query_key_value_clones q k v torch float sdpa_partial = create_attention score_mod block_mask enable_gqa=False golden_out = sdpa_partial q_gold k_gold v_gold ref_out = sdpa_partial q_ref k_ref v_ref MAX_CACHED_SEQ_LEN = n_pages page_size k_cache = torch zeros n_heads MAX_CACHED_SEQ_LEN head_dim device=device dtype=dtype v_cache = torch zeros n_heads MAX_CACHED_SEQ_LEN head_dim device=device dtype=dtype paged_cache = PagedAttention n_pages page_size max_batch_size device=device batch_reserve paged_cache torch tensor device=device batch_reserve paged_cache torch tensor device=device batch_reserve paged_cache torch tensor device=device batch_reserve paged_cache torch tensor device=device batch_reserve paged_cache torch tensor device=device batch_idx = torch arange max_batch_size device=device dtype=torch int input_pos = torch arange max_seq_len device=device dtype=torch int unsqueeze expand max_batch_size max_seq_len paged_cache assign batch_idx input_pos k v k_cache v_cache kv_len_tensor = torch full max_batch_size max_seq_len device=device dtype=torch int new_block_mask = paged_cache convert_logical_block_mask block_mask kv_len=kv_len_tensor compiled_sdpa = torch compile create_attention paged_cache get_score_mod score_mod kv_len=kv_len_tensor block_mask enable_gqa=False paged_out = compiled_sdpa q k_cache v_cache block_mask=new_block_mask torch no_grad dtype = ref_out dtype dtype == torch float fudge_factor = fudge_factor = Checkout output _check_equal golden_out ref_out paged_out fudge_factor Out dataclass Params batch_size int num_heads int seq_length int head_dim int dtype torch dtype config_str Optional str = None __str__ f batch batch_size _head num_heads _seq_len seq_length _headdim head_dim _dtype str dtype split - get_params dtypes list torch dtype - list Params params = seq_lengths = seq_len dtype product seq_lengths dtypes params append Params batch_size= num_heads= seq_length=seq_len head_dim= dtype=dtype params supports_learnable_bias = unittest skipUnless torch cuda is_available has_triton torch cuda get_device_capability = torch version hip Requires Triton + A Triton + ROCm supports_learnable_bias large_tensor_test_class GB device=test_device TestLearnableBiases InductorTestCase setUp super setUp skipCPUIf LONG_COMPILATION_ON_CPU skip UT CPU due long compilation time found CI dtype = torch float atol = e- rtol = e- _init_tensors params Params device str make_tensor = functools partial torch randn params batch_size params num_heads params seq_length params head_dim device=device dtype=params dtype requires_grad=True make_tensor make_tensor make_tensor torch no_grad _gold_check eager compiled gold tensor_name fudge_factor= ref_error = rmse eager gold comp_error = rmse compiled gold Note This has been carefully tested FlexAttention within average error SDPA Do bump tolerance unless you absolutely sure you worsening accuracy FlexAttention eager dtype == torch float fudge_factor = fudge_factor comp_error = comp_error item ref_error = ref_error item fudge_factor tensor_name == out eager dtype == torch float comp_error ref_error skipTest Compiled FlexAttention less accurate than eager fp assertLessEqual comp_error ref_error fudge_factor f \nTensor tensor_name \nCompiled error comp_error f exceeds f reference error ref_error f fudge_factor fudge_factor _check_outputs_and_grads out_eager out_compiled out_gold tensors names=None backwards_grad = torch randn_like out_eager device= cpu out_eager device grads_eager = torch autograd grad out_eager tensors backwards_grad grads_compiled = torch autograd grad out_compiled tensors backwards_grad grads_gold = torch autograd grad out_gold tensors backwards_grad tensor_names = out grad_query grad_key grad_value grad_bias names None names eager_tensors = out_eager grads_eager compiled_tensors = out_compiled grads_compiled gold_tensors = out_gold grads_gold eager compiled gold name zip eager_tensors compiled_tensors gold_tensors tensor_names strict=True _gold_check eager compiled gold name skip_on_cpu common_utils parametrize params get_params device_configs cuda dtypes name_fn=lambda x f x common_utils parametrize mode default max-autotune-no-cudagraphs test_relative_ d_bias device params mode str query key value = _init_tensors params device=device bias = torch randn params seq_length device=device dtype=params dtype requires_grad=True bias_func score b h q_idx kv_idx score + bias torch abs q_idx - kv_idx flex_compiled = torch compile flex_attention mode=mode out_eager = flex_attention query key value score_mod=bias_func out_compiled = flex_compiled query key value score_mod=bias_func out_gold = flex_attention query torch float key torch float value torch float score_mod=bias_func _check_outputs_and_grads out_eager out_compiled out_gold query key value bias skip_on_cpu common_utils parametrize params get_params device_configs cuda dtypes name_fn=lambda x f x test_absolute_ d_bias device params query key value = _init_tensors params device=device bias = torch randn params seq_length params seq_length device=device dtype=params dtype requires_grad=True bias_func score b h q_idx kv_idx score + bias q_idx kv_idx flex_compiled = torch compile flex_attention out_eager = flex_attention query key value score_mod=bias_func out_compiled = flex_compiled query key value score_mod=bias_func out_gold = flex_attention query torch float key torch float value torch float score_mod=bias_func _check_outputs_and_grads out_eager out_compiled out_gold query key value bias skip_on_cpu common_utils parametrize params get_params device_configs cuda dtypes name_fn=lambda x f x test_head_specific_bias device params query key value = _init_tensors params device=device bias = torch randn params num_heads params seq_length params seq_length device=device dtype=params dtype requires_grad=True bias_func score b h q_idx kv_idx score + bias h q_idx kv_idx flex_compiled = torch compile flex_attention out_eager = flex_attention query key value score_mod=bias_func out_compiled = flex_compiled query key value score_mod=bias_func out_gold = flex_attention query torch float key torch float value torch float score_mod=bias_func _check_outputs_and_grads out_eager out_compiled out_gold query key value bias skip_on_cpu common_utils parametrize params get_params device_configs cuda dtypes name_fn=lambda x f x test_batch_head_bias device params query key value = _init_tensors params device=device bias = torch randn params batch_size params num_heads params seq_length params seq_length device=device dtype=params dtype requires_grad=True bias_func score b h q_idx kv_idx score + bias b h q_idx kv_idx flex_compiled = torch compile flex_attention out_eager = flex_attention query key value score_mod=bias_func out_compiled = flex_compiled query key value score_mod=bias_func out_gold = flex_attention query torch float key torch float value torch float score_mod=bias_func _check_outputs_and_grads out_eager out_compiled out_gold query key value bias skip_on_cpu common_utils parametrize params get_params device_configs cuda dtypes name_fn=lambda x f x test_multiplicative_bias device params query key value = _init_tensors params device=device bias = torch randn params seq_length device=device dtype=params dtype requires_grad=True bias_func score b h q_idx kv_idx score bias q_idx flex_compiled = torch compile flex_attention out_eager = flex_attention query key value score_mod=bias_func out_compiled = flex_compiled query key value score_mod=bias_func out_gold = flex_attention query torch float key torch float value torch float score_mod=bias_func _check_outputs_and_grads out_eager out_compiled out_gold query key value bias skip_on_cpu common_utils parametrize params get_params device_configs cuda dtypes name_fn=lambda x f x test_local_window_bias device params query key value = _init_tensors params device=device window_size = bias = torch randn window_size + device=device dtype=torch float requires_grad=True bias_func score b h q_idx kv_idx window_idx = torch clamp q_idx - kv_idx + window_size window_size score + bias window_idx flex_compiled = torch compile flex_attention out_eager = flex_attention query key value score_mod=bias_func out_compiled = flex_compiled query key value score_mod=bias_func out_gold = flex_attention query torch float key torch float value torch float score_mod=bias_func _check_outputs_and_grads out_eager out_compiled out_gold query key value bias skip_on_cpu common_utils parametrize params get_params device_configs cuda dtypes name_fn=lambda x f x test_global_tokens_bias device params query key value = _init_tensors params device=device bias = torch randn params seq_length device=device dtype=torch float requires_grad=True bias_func score b h q_idx kv_idx score + bias kv_idx flex_compiled = torch compile flex_attention out_eager = flex_attention query key value score_mod=bias_func out_compiled = flex_compiled query key value score_mod=bias_func out_gold = flex_attention query torch float key torch float value torch float score_mod=bias_func _check_outputs_and_grads out_eager out_compiled out_gold query key value bias skip_on_cpu common_utils parametrize params get_params device_configs cuda dtypes name_fn=lambda x f x test_weird_bias device params query key value = _init_tensors params device=device bias = torch randn params batch_size params num_heads params seq_length device=device dtype=params dtype requires_grad=True which_bias = torch tensor device=device bias_func score b h q_idx kv_idx score + bias b h which_bias q_idx flex_compiled = torch compile flex_attention out_eager = flex_attention query key value score_mod=bias_func out_compiled = flex_compiled query key value score_mod=bias_func out_gold = flex_attention query torch float key torch float value torch float score_mod=bias_func _check_outputs_and_grads out_eager out_compiled out_gold query key value bias skip_on_cpu common_utils parametrize params get_params device_configs cuda dtypes name_fn=lambda x f x test_indirect_bias device params query key value = _init_tensors params device=device bias = torch randn params seq_length device=device dtype=params dtype requires_grad=True offset = torch randint params seq_length params seq_length device=device bias_func score b h q_idx kv_idx score + bias offset q_idx flex_compiled = torch compile flex_attention out_eager = flex_attention query key value score_mod=bias_func out_compiled = flex_compiled query key value score_mod=bias_func out_gold = flex_attention query torch float key torch float value torch float score_mod=bias_func _check_outputs_and_grads out_eager out_compiled out_gold query key value bias skip_on_cpu common_utils parametrize params get_params device_configs cuda dtypes name_fn=lambda x f x common_utils parametrize mode default max-autotune-no-cudagraphs test_symmetric_bias device params mode str query key value = _init_tensors params device=device bias = torch randn params seq_length device=device dtype=params dtype requires_grad=True bias_func score b h q_idx kv_idx score + bias q_idx + bias kv_idx flex_compiled = torch compile flex_attention mode=mode out_eager = flex_attention query key value score_mod=bias_func out_compiled = flex_compiled query key value score_mod=bias_func out_gold = flex_attention query torch float key torch float value torch float score_mod=bias_func Error backwards assertRaisesRegex torch _inductor exc InductorError Using multiple indexing operations same tensor requires gradients _check_outputs_and_grads out_eager out_compiled out_gold query key value bias skip_on_cpu common_utils parametrize params get_params device_configs cuda dtypes name_fn=lambda x f x test_flipped_indexed_bias device params query key value = _init_tensors params device=device bias = torch randn params seq_length params seq_length device=device dtype=params dtype requires_grad=True bias_func score b h q_idx kv_idx score + bias kv_idx q_idx flex_compiled = torch compile flex_attention out_eager = flex_attention query key value score_mod=bias_func out_compiled = flex_compiled query key value score_mod=bias_func out_gold = flex_attention query torch float key torch float value torch float score_mod=bias_func _check_outputs_and_grads out_eager out_compiled out_gold query key value bias skip_on_cpu common_utils parametrize params get_params device_configs cuda dtypes name_fn=lambda x f x common_utils parametrize mode default max-autotune-no-cudagraphs test_head_specific_gate device params mode str query key value = _init_tensors params device=device gate_score = torch randn params num_heads device=device dtype=torch float requires_grad=True bias_func score b h q_idx kv_idx score torch sigmoid gate_score h flex_compiled = torch compile flex_attention mode=mode out_eager = flex_attention query key value score_mod=bias_func out_compiled = flex_compiled query key value score_mod=bias_func out_gold = flex_attention query torch float key torch float value torch float score_mod=bias_func _check_outputs_and_grads out_eager out_compiled out_gold query key value gate_score skip_on_cpu common_utils parametrize params get_params device_configs cuda dtypes name_fn=lambda x f x test_distinct_biases device params query key value = _init_tensors params device=device Create two separate bias tensors bias = torch randn params seq_length device=device dtype=params dtype requires_grad=True bias = torch randn params seq_length device=device dtype=torch float requires_grad=True bias_func score b h q_idx kv_idx score + bias q_idx + bias kv_idx flex_compiled = torch compile flex_attention out_eager = flex_attention query key value score_mod=bias_func out_compiled = flex_compiled query key value score_mod=bias_func out_gold = flex_attention query torch float key torch float value torch float score_mod=bias_func Include both bias tensors tuple gradient checking _check_outputs_and_grads out_eager out_compiled out_gold query key value bias bias names= out grad_query grad_key grad_value grad_bias grad_bias skip_on_cpu common_utils parametrize params get_params device_configs cuda dtypes name_fn=lambda x f x torch compile test_learnable_bias_global_compiled device params batch_size = num_heads = seq_len = head_dim = d_model = num_heads head_dim query = torch randn batch_size num_heads seq_len head_dim device=device key = torch randn batch_size num_heads seq_len head_dim device=device value = torch randn batch_size num_heads seq_len head_dim device=device out_proj = nn Linear d_model d_model device=device query requires_grad = True key requires_grad = True value requires_grad = True bias = torch randn batch_size num_heads seq_len seq_len device=device requires_grad=True bias_mod score b h q_idx kv_idx score + bias b h q_idx kv_idx out = flex_attention query=query key=key value=value score_mod=bias_mod out = out transpose contiguous view batch_size seq_len d_model attn_output = out_proj out random_target = torch randn batch_size seq_len d_model device=device loss = torch nn functional mse_loss attn_output random_target loss backward assert bias grad No gradient computed bias assert torch any bias grad = Gradient bias skip_on_cpu test_backprop_error_case device torch compile test x y Materialize bias matrix B L device = x shape x shape x device b = torch arange B device=device dtype=torch long view B q_idx = torch arange L device=device dtype=torch long view L kv_idx = torch arange L device=device dtype=torch long view L bias_mat = y b q_idx + y b kv_idx B L L Dummy score_mod retrieving bias values score_mod score b h q_idx kv_idx score + bias_mat b q_idx kv_idx x_ = x None repeat torch _dynamo graph_break flex_attention x_ x_ x_ score_mod=score_mod B L D = x = torch randn B L D device=device requires_grad=True y = torch randn B L device=device requires_grad=True _ = test x y mean backward assert x grad norm assert y grad norm skip_on_cpu common_utils parametrize params get_params device_configs cuda dtypes name_fn=lambda x f x test_relative_ d_bias_only_grad device params query key value = _init_tensors params device=device query = query detach requires_grad_ False key = key detach requires_grad_ False value = value detach requires_grad_ False Only bias requires gradients bias = torch randn params seq_length device=device dtype=params dtype requires_grad=True Only bias needs gradients bias_func score b h q_idx kv_idx score + bias torch abs q_idx - kv_idx flex_compiled = torch compile flex_attention out_eager = flex_attention query key value score_mod=bias_func out_compiled = flex_compiled query key value score_mod=bias_func out_gold = flex_attention query torch float key torch float value torch float score_mod=bias_func For gradient checking we only pass bias tensor since s only one requiring gradients _check_outputs_and_grads out_eager out_compiled out_gold bias names= out bias _test_flex_attention_with_dynamic_max_autotune device query = torch randn device=device key = torch randn device=device value = torch randn device=device query requires_grad = True key requires_grad = True value requires_grad = True shape = B Hq M Hkv N D = shape score_mod = _generate_alibi_bias causal b h m n m = n mask_shape = M N block_mask = torch compile create_block_mask causal mask_shape device=device compiled_sdpa = torch compile flex_attention dynamic=True mode= max-autotune-no-cudagraphs out = compiled_sdpa query=query key=key value=value score_mod=score_mod block_mask=block_mask enable_gqa=True kernel_options=None out sum backward assertEqual out shape query shape f Expected shape query shape got out shape skip_on_cpu test_flex_attention_with_dynamic_max_autotune device _test_flex_attention_with_dynamic_max_autotune device skip_on_cpu torch _inductor config patch graph_partition True test_flex_attention_with_dynamic_max_autotune_graph_partition device _test_flex_attention_with_dynamic_max_autotune device skip_on_cpu test_flex_attention_logging device tempfile TemporaryDirectory tmpdir log_file = os path join tmpdir flex_attention_configs patch dict os environ TORCHINDUCTOR_FLEX_ATTENTION_LOGGING_FILE log_file query = torch randn device=device dtype=torch float requires_grad=True key = torch randn device=device dtype=torch float requires_grad=True value = torch randn device=device dtype=torch float requires_grad=True score_mod score b h q_idx kv_idx score causal_mask b h q_idx kv_idx q_idx = kv_idx block_mask = torch compile create_block_mask causal_mask device=device compiled_flex = torch compile flex_attention mode= max-autotune-no-cudagraphs out = compiled_flex query=query key=key value=value score_mod=score_mod block_mask=block_mask out sum backward json_file = log_file + json assertTrue os path exists json_file f Log file json_file created open json_file f log_data = json load f assertIsInstance log_data list assertEqual len log_data keys_seen = next iter entry keys entry log_data expected_fwd_key = forward expected_bwd_key = backward assertIn expected_fwd_key keys_seen assertIn expected_bwd_key keys_seen entry log_data assertIsInstance entry dict assertEqual len entry dims_key = next iter entry keys choices = entry dims_key kernel_type = eval dims_key assertIsInstance choices list assertGreater len choices i choice enumerate choices assertIn type choice assertIn time choice choice type == triton assertIn num_warps choice assertIn num_stages choice kernel_type == forward assertIn BLOCK_M choice assertIn BLOCK_N choice assertNotIn BLOCK_M choice kernel_type == backward assertIn BLOCK_M choice assertIn BLOCK_N choice assertIn BLOCK_M choice assertIn BLOCK_N choice assertNotIn BLOCK_M choice assertNotIn BLOCK_N choice i assertLessEqual choices time choice time skip_on_cpu test_inspect_bug device https github com pytorch pytorch issues sliding_window b h q_idx kv_idx val q_idx - kv_idx abs val sliding_window = functools partial sliding_window val=torch randn device=device opt_fn = torch compile create_block_mask fullgraph=True create_block_mask sliding_window None None device=device checks compile working opt_fn sliding_window None None device=device supported_platform skip_on_cpu test_head_bias_req_grad device B H S D = bias = torch randn H device=device dtype=torch float requires_grad=True bias_flex = bias detach clone requires_grad_ True head_bias score b h q_idx kv_idx score + bias_flex h bias_sdpa_ref = bias detach clone requires_grad_ True implicit_bias_sdpa_ref = bias_sdpa_ref implicit_bias_sdpa_ref = implicit_bias_sdpa_ref view H expand H S S bias_sdpa_gold = bias detach clone dtype=torch float requires_grad_ True implicit_bias_sdpa_gold = bias_sdpa_gold implicit_bias_sdpa_gold = implicit_bias_sdpa_gold view H expand H S S _test_learnable_bias_inner B H S D head_bias bias_flex implicit_bias_sdpa_ref bias_sdpa_ref implicit_bias_sdpa_gold bias_sdpa_gold device supported_platform skip_on_cpu test_comparison_vs_sdpa_with_learnable_bias device -dimensional bias B H S D = bias = torch randn S device=device dtype=torch float requires_grad=True bias_flex = bias detach clone requires_grad_ True rel_pos_ d score b h q_idx kv_idx score + bias_flex q_idx + kv_idx bias_indices = torch arange S None + torch arange S bias_sdpa_ref = bias detach clone requires_grad_ True implicit_bias_sdpa_ref = bias_sdpa_ref bias_indices bias_sdpa_gold = bias detach clone dtype=torch float requires_grad_ True implicit_bias_sdpa_gold = bias_sdpa_gold bias_indices _test_learnable_bias_inner B H S D rel_pos_ d bias_flex implicit_bias_sdpa_ref bias_sdpa_ref implicit_bias_sdpa_gold bias_sdpa_gold device -dimensional bias B H S D = bias = torch randn S S device=device dtype=torch float requires_grad=True bias_flex = bias detach clone requires_grad_ True rel_pos_ d score b h q_idx kv_idx score + bias_flex q_idx kv_idx bias_sdpa_ref = bias detach clone requires_grad_ True implicit_bias_sdpa_ref = bias_sdpa_ref bias_sdpa_gold = bias detach clone dtype=torch float requires_grad_ True implicit_bias_sdpa_gold = bias_sdpa_gold _test_learnable_bias_inner B H S D rel_pos_ d bias_flex implicit_bias_sdpa_ref bias_sdpa_ref implicit_bias_sdpa_gold bias_sdpa_gold device -dimensional bias + index multiple B H S D = bias = torch randn S S device=device dtype=torch float requires_grad=True bias_flex = bias detach clone requires_grad_ True rel_pos_ d score b h q_idx kv_idx score + bias_flex q_idx kv_idx bias_sdpa_ref = bias detach clone requires_grad_ True implicit_bias_sdpa_ref = bias_sdpa_ref bias_sdpa_gold = bias detach clone dtype=torch float requires_grad_ True implicit_bias_sdpa_gold = bias_sdpa_gold _test_learnable_bias_inner B H S D rel_pos_ d bias_flex implicit_bias_sdpa_ref bias_sdpa_ref implicit_bias_sdpa_gold bias_sdpa_gold device -dimensional bias + transposed B H S D = bias = torch randn S S device=device dtype=torch float requires_grad=True bias_flex = bias detach clone requires_grad_ True rel_pos_ d_transposed score b h q_idx kv_idx score + bias_flex kv_idx q_idx bias_sdpa_ref = bias detach clone requires_grad_ True implicit_bias_sdpa_ref = bias_sdpa_ref transpose - - bias_sdpa_gold = bias detach clone dtype=torch float requires_grad_ True implicit_bias_sdpa_gold = bias_sdpa_gold transpose - - _test_learnable_bias_inner B H S D rel_pos_ d_transposed bias_flex implicit_bias_sdpa_ref bias_sdpa_ref implicit_bias_sdpa_gold bias_sdpa_gold device -dimensional bias + transposed B H S D = bias = torch randn H S S device=device dtype=torch float requires_grad=True bias_flex = bias detach clone requires_grad_ True rel_pos_ d_transposed score b h q_idx kv_idx score + bias_flex h kv_idx q_idx bias_sdpa_ref = bias detach clone requires_grad_ True implicit_bias_sdpa_ref = bias_sdpa_ref transpose - - bias_sdpa_gold = bias detach clone dtype=torch float requires_grad_ True implicit_bias_sdpa_gold = bias_sdpa_gold transpose - - _test_learnable_bias_inner B H S D rel_pos_ d_transposed bias_flex implicit_bias_sdpa_ref bias_sdpa_ref implicit_bias_sdpa_gold bias_sdpa_gold device _test_learnable_bias_inner B H S D score_mod bias_flex implicit_bias_sdpa_ref bias_sdpa_ref implicit_bias_sdpa_gold bias_sdpa_gold device make_tensor = functools partial torch ones B H S D device=device dtype=torch float requires_grad=True q_ref k_ref v_ref = make_tensor make_tensor make_tensor q_gold k_gold v_gold = query_key_value_clones q_ref k_ref v_ref torch float q_flex k_flex v_flex = query_key_value_clones q_ref k_ref v_ref out_ref = torch nn functional scaled_dot_product_attention q_ref k_ref v_ref attn_mask=implicit_bias_sdpa_ref out_ref sum backward out_gold = torch nn functional scaled_dot_product_attention q_gold k_gold v_gold attn_mask=implicit_bias_sdpa_gold out_gold sum backward out_flex = flex_attention q_flex k_flex v_flex score_mod=score_mod out_flex sum backward name = score_mod __name__ ref flex gold out_ref out_flex out_gold q_ref grad q_flex grad q_gold grad k_ref grad k_flex grad k_gold grad v_ref grad v_flex grad v_gold grad bias_sdpa_ref grad bias_flex grad bias_sdpa_gold grad ref_error = rmse ref gold flex_error = rmse flex gold assertTrue ref_error = flex_error f name - Ref error ref_error Flex eager Error flex_error instantiate_device_type_tests TestFlexAttention globals only_for=test_device allow_xpu=True instantiate_device_type_tests TestPagedAttention globals only_for=test_device allow_xpu=True instantiate_device_type_tests TestBlockMask globals only_for= test_device HAS_GPU cuda allow_xpu=True instantiate_device_type_tests TestLearnableBiases globals only_for=test_device allow_xpu=True __name__ == __main__ torch _inductor test_case run_tests run_tests