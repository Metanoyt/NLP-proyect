mypy allow-untyped-defs torch MkldnnLinear torch jit ScriptModule __init__ dense_module dtype super __init__ register_buffer weight dense_module weight to_mkldnn dtype dense_module bias None Bias can fp bf OneDNN bf path good accuracy we use fp dtype register_buffer bias dense_module bias to_mkldnn TODO Remove once ScriptModule supports registering None buffer register_buffer bias torch zeros dense_module weight size dtype=torch float to_mkldnn torch jit script_method __getstate__ weight to_dense bias to_dense training torch jit script_method __setstate__ state weight = state to_mkldnn bias = state to_mkldnn training = state torch jit script_method forward x x_mkldnn = x x is_mkldnn x to_mkldnn y_mkldnn = torch _C _nn mkldnn_linear x_mkldnn weight bias y = y_mkldnn x is_mkldnn y_mkldnn to_dense y _MkldnnConvNd torch jit ScriptModule Common base MkldnnConv d MkldnnConv d __constants__ = stride padding dilation groups __init__ dense_module super __init__ stride = dense_module stride padding = dense_module padding dilation = dense_module dilation groups = dense_module groups dense_module bias None register_buffer bias dense_module bias to_mkldnn Bias can fp bf OneDNN bf path good accuracy we use fp dtype TODO Remove once ScriptModule supports registering None buffer register_buffer bias torch zeros dense_module weight size dtype=torch float to_mkldnn torch jit script_method __getstate__ weight to_dense bias to_dense training torch jit script_method forward x torch mkldnn_convolution x weight bias padding stride dilation groups MkldnnConv d _MkldnnConvNd __init__ dense_module dtype super __init__ dense_module register_buffer weight dense_module weight to_mkldnn dtype torch jit script_method __setstate__ state weight = state to_mkldnn bias = state to_mkldnn training = state MkldnnConv d _MkldnnConvNd __init__ dense_module dtype super __init__ dense_module register_buffer weight torch _C _nn mkldnn_reorder_conv d_weight dense_module weight to_mkldnn dtype padding stride dilation groups torch jit script_method __setstate__ state weight = torch _C _nn mkldnn_reorder_conv d_weight state to_mkldnn padding stride dilation groups bias = state to_mkldnn training = state MkldnnConv d _MkldnnConvNd __init__ dense_module dtype super __init__ dense_module register_buffer weight torch _C _nn mkldnn_reorder_conv d_weight dense_module weight to_mkldnn dtype padding stride dilation groups torch jit script_method __setstate__ state weight = torch _C _nn mkldnn_reorder_conv d_weight state to_mkldnn padding stride dilation groups bias = state to_mkldnn training = state MkldnnBatchNorm torch jit ScriptModule __constants__ = exponential_average_factor eps __init__ dense_module super __init__ dense_module training raise AssertionError Only support eval mode batchnorm mkldnn path now dense_module track_running_stats raise AssertionError Only support track_running_stats=True mkldnn path now dense_module affine raise AssertionError Only support affine=True mkldnn path now dense_module momentum None exponential_average_factor = exponential_average_factor = dense_module momentum eps = dense_module eps register_buffer weight dense_module weight to_mkldnn register_buffer bias dense_module bias to_mkldnn register_buffer running_mean dense_module running_mean to_mkldnn register_buffer running_var dense_module running_var to_mkldnn torch jit script_method __getstate__ weight = weight to_dense bias = bias to_dense running_mean = running_mean to_dense running_var = running_var to_dense weight bias running_mean running_var training torch jit script_method __setstate__ state weight = state to_mkldnn bias = state to_mkldnn running_mean = state to_mkldnn running_var = state to_mkldnn training = state torch jit script_method forward x torch batch_norm x weight bias running_mean running_var False training exponential_average_factor eps False cuda_enabled MkldnnPrelu torch jit ScriptModule __init__ dense_module dtype super __init__ register_buffer weight dense_module weight to_mkldnn dtype torch jit script_method __getstate__ weight to_dense training torch jit script_method __setstate__ state weight = state to_mkldnn training = state torch jit script_method forward x x_mkldnn = x x is_mkldnn x to_mkldnn y_mkldnn = torch prelu x_mkldnn weight y = y_mkldnn x is_mkldnn y_mkldnn to_dense y to_mkldnn module dtype=torch float dtype torch float torch bfloat torch half raise AssertionError MKLDNN only support float bfloat half path now m_fn m d isinstance m torch nn Linear MkldnnLinear m d isinstance m torch nn Conv d MkldnnConv d m d isinstance m torch nn Conv d MkldnnConv d m d isinstance m torch nn Conv d MkldnnConv d m d isinstance m torch nn BatchNorm d torch nn BatchNorm d For batchnorm bf path OneDNN requires weight bias need fp dtype so doesn t need dtype argument MkldnnBatchNorm m isinstance m torch nn PReLU MkldnnPrelu m d m m_fn_rec m d new_m = m_fn m d name sub_m m named_children setattr new_m name m_fn_rec sub_m d new_m m_fn_rec module dtype