mypy allow-untyped-defs dataclasses functools inspect sys typing warnings weakref torch torch _C _C torch _library infer_schema torch library library torch _library infer_schema infer_schema torch library get_ctx torchgen model BaseTy BaseType FunctionSchema ListType OperatorName SchemaKind autograd autograd_kernel_indirection construct_autograd_kernel torch _custom_op deprecated We shipped production-ready version into torch library Please use those APIs instead __all__ = custom_op CustomOp get_ctx SUPPORTED_DEVICE_TYPE_TO_KEY = cpu CPU cuda CUDA We will let users register CustomOps anything could look like PyTorch internals avoid confusion RESERVED_NS = prim prims aten torch pytorch warn_deprecated warnings warn torch _custom_op deprecated will removed PyTorch please use equivalent torch library API instead DeprecationWarning stacklevel= custom_op qualname str manual_schema typing Optional str = None - typing Callable r This API deprecated please use torch library custom_op instead warn_deprecated inner func inspect isfunction func raise ValueError f custom_op func Expected ` func ` Python f function got type func ns name = parse_qualname qualname validate_namespace ns func __name__ = name raise ValueError f custom_op qualname= qualname func expected ` func ` f have name name got func __name__ f Please either change name ` func ` qualname f passed ` custom_op ` schema = infer_schema func mutates_args= manual_schema None manual_schema schema_str = f name schema function_schema = FunctionSchema parse schema_str validate_schema function_schema manual_schema None validate_function_matches_schema function_schema func lib = library Library ns FRAGMENT lib define schema_str ophandle = find_ophandle_or_throw ns function_schema name result = CustomOp lib ns function_schema name ophandle _private_access=True result __name__ = func __name__ pyrefly ignore bad-assignment result __module__ = func __module__ result __doc__ = func __doc__ library impl lib result _opname Autograd autograd_kernel_indirection weakref proxy result torch _C _dispatch_set_report_error_callback ophandle functools partial report_error_callback weakref proxy result result inner Global dictionary holding references all CustomOp objects Yes keeps all CustomOps alive see NOTE CustomOp lifetime Used query CustomOp associated specific C++ dispatcher operator An example usage FakeTensor FakeTensor checks specific operator has implementation registered via CustomOp API Indexed qualname e g aten foo global_registry dict str CustomOp = CustomOp r This API deprecated please use torch library custom_op instead __init__ lib cpp_ns schema operator_name ophandle _private_access=False super __init__ warn_deprecated _private_access raise RuntimeError The CustomOp constructor private we do guarantee BC Please use custom_op create CustomOp object name = f cpp_ns operator_name _schema = schema _cpp_ns = cpp_ns _lib library Library = lib _ophandle _C _DispatchOperatorHandle = ophandle Has name op e g foo We cache here convenience _opname str = operator_name _opname namespace e g custom foo _qualname str = name __name__ = None mypy requires NB Some these impls registered kernels DispatchKeys Modifying _impls dict directly won t do anything case _impls dict str typing Optional FuncAndLocation = See NOTE CustomOp autograd kernel indirection _registered_autograd_kernel_indirection = False global_registry _qualname = _register_autograd_kernel_indirection assert _registered_autograd_kernel_indirection _lib impl _opname autograd_kernel_indirection weakref proxy Autograd _registered_autograd_kernel_indirection = True Records impl source location _impls Note doesn t cause torch library use impl needs done separate _lib impl call _register_impl kind func stacklevel= _has_impl kind func_and_location = _impls kind assert func_and_location None Pacify mypy location = func_and_location location raise RuntimeError f Attempting register kind impl operator _qualname f already has kind impl registered Python f location This supported frame = inspect getframeinfo sys _getframe stacklevel location = f frame filename frame lineno _impls kind = FuncAndLocation func location _get_impl kind _impls kind _has_impl kind kind _impls _destroy NOTE CustomOp lifetime A CustomOp once created lives forever The mechanism global registry holds reference However make testing easier we want able destroy CustomOp objects CustomOp _destroy does job though leaves CustomOp garbage state del _lib opnamespace = getattr torch ops _cpp_ns hasattr opnamespace _opname delattr opnamespace _opname del global_registry _qualname __repr__ f CustomOp op= _qualname __call__ args kwargs Bypass torch ops directly do OperatorHandle callBoxed Using torch ops bit pain can slow has lifetime issues caching operators make testing CustomOp difficult result = _C _dispatch_call_boxed _ophandle args kwargs result impl device_types typing Union str typing Iterable str _stacklevel= - typing Callable r This API deprecated please use torch library custom_op instead isinstance device_types str device_types = device_types device_type device_types validate_device_type device_type inner f device_type set device_types _check_doesnt_have_library_impl device_type _register_impl device_type f stacklevel=_stacklevel dispatch_key = SUPPORTED_DEVICE_TYPE_TO_KEY device_type library impl _lib _opname dispatch_key f f inner _check_doesnt_have_library_impl device_type _has_impl device_type key = SUPPORTED_DEVICE_TYPE_TO_KEY device_type _C _dispatch_has_computed_kernel_for_dispatch_key _qualname key raise RuntimeError f impl device_types= device_type operator _qualname f already has implementation device type via f pre-existing torch library TORCH_LIBRARY registration impl_factory - typing Callable r Register implementation factory function inner f _register_impl factory f library impl _lib _opname BackendSelect f f inner impl_abstract _stacklevel= - typing Callable r This API deprecated please use torch library custom_op instead inner f _check_doesnt_have_library_meta_impl _register_impl abstract f stacklevel=_stacklevel location = _get_impl abstract location qualname = _qualname Handle DispatchKey Meta registration functools wraps f f_with_ctx args kwargs error_on_ctx raise RuntimeError f Attempted call get_ctx meta implementation f qualname f You have presumably called get_ctx because operator f has data-dependent output shape so there no f such meta implementation error correct f behavior Otherwise please remove call get_ctx f implementation registered impl_abstract f location torch _library fake_impl set_ctx_getter error_on_ctx f args kwargs _lib impl _opname f_with_ctx Meta f inner _check_can_register_backward error detail raise RuntimeError f Cannot use torch _custom_ops APIs register backward f formula detail Got operator f _qualname schema schema schema = _schema schema kind = SchemaKind functional error non-functional operator rets = schema returns schema returns error operator no returns assert len rets is_non_mutating_view = any r annotation None r annotation is_write r rets is_non_mutating_view error operator returns views We make assumptions about schema s types allowed_return_types = BaseType BaseTy int int BaseType BaseTy SymInt SymInt BaseType BaseTy bool bool BaseType BaseTy float float BaseType BaseTy Tensor Tensor ListType BaseType BaseTy Tensor None List Tensor ret schema returns ret type allowed_return_types continue error f operator list allowed_return_types values got ret type _check_doesnt_have_library_autograd_impl _registered_autograd_kernel_indirection _C _dispatch_has_kernel_for_dispatch_key _qualname CompositeImplicitAutograd raise RuntimeError f impl_backward impl_save_for_backward operator _qualname f already has implementation device type via f pre-existing registration DispatchKey CompositeImplicitAutograd f CompositeImplicitAutograd operators do need autograd formula f instead operator will decompose into its constituents those f can have autograd formulas defined them We can improve adding all Autograd BACKEND keys realistically people will just using API CPU CUDA now key Autograd AutogradCPU AutogradCUDA _C _dispatch_has_kernel_for_dispatch_key _qualname key raise RuntimeError f impl_backward impl_save_for_backward f operator _qualname already has Autograd kernel f registered DispatchKey key vi pre-existing f torch library TORCH_LIBRARY registration Please either f remove those registrations don t use torch _custom_ops APIs _check_doesnt_have_library_meta_impl _has_impl abstract If user s operator CompositeExplicitAutograd allow them impl_abstract This being pragmatic existing custom ops may have CompositeExplicitAutograd registration don t work Meta kernels so gives them escape hatch _C _dispatch_has_kernel_for_dispatch_key _qualname CompositeExplicitAutograd _C _dispatch_has_kernel_for_dispatch_key _qualname Meta Otherwise user s already has Meta kernel their op CompositeImplicitAutograd some other alias dispatch key raise Special case CompositeImplicitAutograd _C _dispatch_has_kernel_for_dispatch_key _qualname CompositeImplicitAutograd raise RuntimeError f impl_abstract operator _qualname f already has implementation device type via f pre-existing registration DispatchKey CompositeImplicitAutograd f CompositeImplicitAutograd operators do need abstract impl f instead operator will decompose into its constituents those f can have abstract impls defined them _C _dispatch_has_kernel_for_dispatch_key _qualname Meta raise RuntimeError f impl_abstract operator _qualname f already has DispatchKey Meta implementation via f pre-existing torch library TORCH_LIBRARY registration f Please either remove registration don t call impl_abstract NOTE backward save_for_backward autograd As part explicit autograd API user must provide us save_for_backward function backward function When both these have been provided then we automatically construct autograd kernel _register_autograd_kernel assert _has_impl backward assert _has_impl save_for_backward kernel = construct_autograd_kernel _schema _output_differentiability get_op _qualname _get_impl save_for_backward func _get_impl backward func _register_impl autograd kernel impl_save_for_backward _stacklevel= r Register function tells us what save backward Please see impl_backward more details inner f _check_can_register_backward _check_doesnt_have_library_autograd_impl _registered_autograd_kernel_indirection _register_autograd_kernel_indirection _register_impl save_for_backward f stacklevel=_stacklevel _has_impl backward _register_autograd_kernel inner impl_backward output_differentiability=None _stacklevel= r This API deprecated please use torch library custom_op instead output_differentiability None yell raise RuntimeError f impl_backward output_differentiability expected f output_differentiability list bools f length equal number outputs CustomOp f got output_differentiability isinstance output_differentiability list yell diff output_differentiability isinstance diff bool yell len _schema returns = len output_differentiability yell inner f _check_can_register_backward _check_doesnt_have_library_autograd_impl _registered_autograd_kernel_indirection _register_autograd_kernel_indirection _register_impl backward f stacklevel=_stacklevel _output_differentiability = output_differentiability _has_impl save_for_backward _register_autograd_kernel inner dataclasses dataclass FuncAndLocation func typing Callable location str find_ophandle_or_throw cpp_ns str operator_name OperatorName overload_name = operator_name overload_name None operator_name overload_name _C _dispatch_find_schema_or_throw f cpp_ns str operator_name name overload_name validate_namespace ns str - None ns raise ValueError f custom_op ns= ns expected ns contain any f valid variable name ns RESERVED_NS raise ValueError f custom_op ns= ns ns reserved namespace f please choose something validate_schema schema FunctionSchema - None torch _library utils is_functional_schema schema raise ValueError f custom_op only supports functional operators f ops do mutate any inputs do f views inputs has least one f Got following non-functional schema schema For simplicity don t allow arguments schema arguments self_arg None raise ValueError f custom_op does support arguments named Please f rename your argument Got schema parse_qualname qualname str - tuple str str names = qualname split len names = raise ValueError f Expected there namespace qualname i e The f operator name should look something like ns foo names raise ValueError f The torch custom_ops APIs do handle overloads f i e operator names them f Please name your operator something like ns foo f Got qualname names names validate_device_type device_type str - None device_type SUPPORTED_DEVICE_TYPE_TO_KEY raise ValueError f CustomOp impl device_types= device_type we only support device_type f SUPPORTED_DEVICE_TYPE_TO_KEY keys supported_param param inspect Parameter - bool param kind inspect Parameter POSITIONAL_OR_KEYWORD inspect Parameter KEYWORD_ONLY validate_function_matches_schema schema FunctionSchema func typing Callable - None sig = inspect signature func all supported_param p _ p sig parameters items raise ValueError f custom_op manual_schema func positional-only args f varargs kwargs supported Please rewrite ` func ` f have them Got ` func ` signature sig any p annotation inspect Parameter empty _ p sig parameters items sig return_annotation inspect Signature empty raise ValueError f custom_op manual_schema func When passing manual f schema we expect ` func ` have no type annotations avoid f ambiguity Got ` func ` signature sig positional = name param name param sig parameters items param kind == inspect Parameter POSITIONAL_OR_KEYWORD kwargonly = name param name param sig parameters items param kind == inspect Parameter KEYWORD_ONLY error raise ValueError f custom_op manual_schema func When passing manual f schema we expect ` func ` s signature match ` manual_schema ` f aside type annotations f func s signature sig manual_schema schema error_default_args raise ValueError f custom_op manual_schema func f neither func nor manual_schema should have default f arguments Got f func s signature sig manual_schema schema compare sig_args schema_args len sig_args = len schema_args error name param arg zip sig_args schema_args name = arg name error param default inspect Parameter empty arg default None error_default_args compare positional schema arguments flat_positional compare kwargonly schema arguments flat_kwarg_only report_error_callback custom_op typing Any key str - None key == Undefined raise NotImplementedError f custom_op There no Tensor inputs operator f e g you passed empty list Tensors If your operator f factory function takes no Tensors constructs f new one then please use CustomOp impl_factory register f implementation key == Meta raise NotImplementedError f custom_op when running device= Meta tensors there no f abstract impl registered CustomOp Please register one via f CustomOp impl_abstract get CustomOp work Meta tensors key CPU CUDA device = key lower raise NotImplementedError f custom_op when running device= device tensors there no f device impl registered CustomOp Please register one via f CustomOp impl device_type= device raise NotImplementedError f custom_op No implementation dispatch key key It likely f we have added functionality yet please either open f issue you re feeling adventurous use low-level f torch library API custom_op_from_existing op ns = op namespace lib = torch library Library ns FRAGMENT name = op name split - schema_str = str op _schema CustomOp expects schema string without namespace schema_str = schema_str rsplit maxsplit= - schema = FunctionSchema parse schema_str CustomOp lib ns schema name op _private_access=True get_op qualname error_not_found raise ValueError f Could find operator qualname Please make sure you have f already registered operator registered C++ f loaded via torch ops load_library ns name = parse_qualname qualname hasattr torch ops ns error_not_found opnamespace = getattr torch ops ns hasattr opnamespace name error_not_found packet = getattr opnamespace name hasattr packet default error_not_found packet default _find_custom_op qualname also_check_torch_library=False qualname global_registry global_registry qualname also_check_torch_library raise RuntimeError f Could find custom op qualname Did you register via f torch _custom_ops API overload = get_op qualname result = custom_op_from_existing overload result get_abstract_impl qualname qualname torch _custom_op impl global_registry None custom_op = torch _custom_op impl global_registry qualname custom_op None None custom_op _has_impl abstract None custom_op _get_impl abstract func _custom_op_with_schema qualname schema needs_fixed_stride_order=True ns name = qualname split schema_str = f name schema function_schema = FunctionSchema parse schema_str validate_schema function_schema tags = torch _C Tag needs_fixed_stride_order needs_fixed_stride_order lib = library Library ns FRAGMENT lib define schema_str tags=tags ophandle = find_ophandle_or_throw ns function_schema name result = CustomOp lib ns function_schema name ophandle _private_access=True result _register_autograd_kernel_indirection torch _C _dispatch_set_report_error_callback ophandle functools partial report_error_callback weakref proxy result get_op qualname