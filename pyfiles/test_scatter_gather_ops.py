Owner s module scatter gather ops random torch torch testing make_tensor torch testing _internal common_utils \ parametrize run_tests TestCase DeterministicGuard TEST_WITH_ROCM torch testing _internal common_device_type \ instantiate_device_type_tests onlyCPU dtypes dtypesIfCUDA toleranceOverride tol torch testing _internal common_dtype \ get_all_dtypes torch testing _internal common_cuda CDNA OrLater Protects against includes accidentally setting default dtype assert torch get_default_dtype torch float Note test_scatter_gather_ops py This test file tests scatter gather operations like torch scatter torch gather TestScatterGather TestCase Fills index tensor valid indices _fill_indices idx dim dim_size elems_per_row m n o unique_indices=True i range dim == m j range dim == n k range dim == o ii = i j k ii dim = slice idx size dim + unique_indices idx tuple ii = torch randperm dim_size elems_per_row idx tuple ii = torch randint dim_size elems_per_row dtypes torch float torch complex test_gather device dtype m n o = random randint random randint random randint elems_per_row = random randint dim = random randrange src = make_tensor m n o device=device dtype=dtype idx_size = m n o idx_size dim = elems_per_row idx = make_tensor idx_size device=device dtype=torch long _fill_indices idx dim src size dim elems_per_row m n o actual = torch gather src dim idx expected = torch zeros idx_size device=device dtype=dtype i range idx_size j range idx_size k range idx_size ii = i j k ii dim = idx i j k expected i j k = src tuple ii assertEqual actual expected atol= rtol= Guarded because torch max isn t defined complex types dtype is_complex src = make_tensor device=device dtype=dtype expected idx = src max True actual = torch gather src idx assertEqual actual expected atol= rtol= dtypes torch int torch bfloat test_gather_large device dtype test larger shapes check vectorized implementation m n k src = make_tensor m k device=device dtype=dtype alloc = torch empty src nelement device=device dtype=dtype discontig = alloc view m k copy_ src alloc = torch empty src nelement + device=device dtype=dtype misaligned = alloc view m k copy_ src alloc = torch empty m k + device=device dtype=dtype misaligned = alloc - copy_ src num_ind = n dim max_ind = src shape dim ind = torch randint max_ind num_ind device=device ind_discontig = torch empty num_ind device=device dtype=torch int copy_ ind shape_ind = src ndim shape_ind dim = ind shape shape_out = list src shape shape_out dim = ind shape ind = ind view shape_ind expand shape_out ind_discontig = ind_discontig view shape_ind expand shape_out res = torch gather src dim=dim index=ind ref = src ind dim == src ind assertEqual res ref atol= rtol= res device type == cuda ref_cpu = src cpu ind cpu dim == src cpu ind cpu assertEqual res cpu ref_cpu atol= rtol= res = torch gather src dim=dim index=ind_discontig assertEqual res ref atol= rtol= res_ind = src ind_discontig dim == src ind_discontig assertEqual res_ind ref atol= rtol= res_ind_neg = src ind - src shape dim dim == src ind - src shape assertEqual res_ind_neg ref atol= rtol= res = torch gather discontig dim=dim index=ind assertEqual res ref atol= rtol= res_ind = discontig ind dim == discontig ind assertEqual res_ind ref atol= rtol= res = torch gather misaligned dim=dim index=ind assertEqual res ref atol= rtol= res_ind = misaligned ind dim == misaligned ind assertEqual res_ind ref atol= rtol= res_ind = misaligned ind dim == misaligned ind assertEqual res_ind ref atol= rtol= res_gather = torch gather misaligned dim=dim index=ind assertEqual res_gather ref atol= rtol= test gather along st dim can accidentally trigger fast path because due index dimension gather dim being unexpected squashing tensorIterator happens src = make_tensor device=device dtype=dtype ind = torch randint device=device view expand res = torch gather src dim= index=ind res device type == cuda ref_cpu = torch gather src cpu dim= index=ind cpu assertEqual res cpu ref_cpu atol= rtol= dtypes torch bool test_gather_bool device dtype src = torch tensor False True True True device=device dtype=dtype idx = torch tensor device=device dtype=torch long actual = torch gather src idx expected = torch tensor False False True True device=device dtype=dtype assertEqual actual expected atol= rtol= parametrize sparse_grad False True dtypes torch float torch float test_gather_backward_with_empty_index_tensor device dtype sparse_grad dim = - input = torch rand dtype=dtype device=device requires_grad=True index = torch randint dtype=torch int device=device res = torch gather input dim index sparse_grad=sparse_grad res sum backward grad = input grad to_dense sparse_grad input grad expected_grad = torch zeros_like input requires_grad=False assertEqual grad expected_grad atol= rtol= _test_scatter_base fn device dtype is_scalar reduction unique_indices=True include_self=True m n o = random randint random randint random randint elems_per_row = random randint dim = random randrange idx_size = m n o idx_size dim = elems_per_row idx = torch empty tuple idx_size device=device dtype=torch long _fill_indices idx dim m n o dim elems_per_row m n o unique_indices is_scalar src = random random src_size = random randint + s s idx_size src = make_tensor tuple src_size device=device dtype=dtype base = make_tensor m n o device=device dtype=dtype reduction None fn torch Tensor scatter_reduce_ actual = fn base clone dim idx src reduce=reduction include_self=include_self actual = fn base clone dim idx src reduce=reduction actual = fn base clone dim idx src expected = base clone counts = torch zeros base shape dtype=torch long device=device + include_self i range idx_size j range idx_size k range idx_size ii = i j k ii dim = idx i j k fn torch Tensor scatter_add_ expected tuple ii += src i j k method may scatter_ scatter scatter_reduce scatter_reduce_ former two might have reduction argument while latter two always do value = src is_scalar src i j k include_self counts tuple ii == expected tuple ii = value reduction == add reduction == sum expected tuple ii += value reduction == multiply reduction == prod expected tuple ii = value reduction == amax expected tuple ii = max expected tuple ii value reduction == amin expected tuple ii = min expected tuple ii value reduction == mean expected tuple ii += value expected tuple ii = value counts tuple ii += reduction == mean counts masked_fill_ counts == dtype is_floating_point dtype is_complex expected = counts expected div_ counts rounding_mode= floor dtype == torch float dtype == torch bfloat Some CUDA kernels e g indexing_backward_kernel_stride_ called during test use fp internal accumulation improved accuracy When using bit precision types can small differences assertEqual actual expected atol= rtol= When we running opportunistic_fastatomics we will expect some floating point rounding errors order operation guaranteed TEST_WITH_ROCM CDNA OrLater \ torch are_deterministic_algorithms_enabled assertEqual actual expected atol= e- rtol= e- assertEqual actual expected atol= rtol= Tests empty index dst = make_tensor device=device dtype=dtype idx = torch tensor device=device dtype=torch long src = make_tensor device=device dtype=dtype reduction None actual = fn dst idx src reduce=reduction actual = fn dst idx src assertEqual actual dst atol= rtol= dtypes torch float torch float torch complex test_scatter_ device dtype deterministic False True DeterministicGuard deterministic _test_scatter_base torch Tensor scatter_ device=device dtype=dtype is_scalar=False reduction=None dtypes torch float torch float torch complex test_scatter__scalar device dtype _test_scatter_base torch Tensor scatter_ device=device dtype=dtype is_scalar=True reduction=None FIXME RuntimeError cuda_scatter_gather_base_kernel_reduce_multiply implemented ComplexFloat toleranceOverride torch float tol atol= e- rtol= dtypesIfCUDA torch float torch float dtypes torch float torch float torch complex test_scatter__reductions device dtype reduction add multiply _test_scatter_base torch Tensor scatter_ device=device dtype=dtype is_scalar=False reduction=reduction _test_scatter_base torch Tensor scatter_ device=device dtype=dtype is_scalar=True reduction=reduction dtypes torch float torch float torch complex test_scatter_add_ device dtype deterministic False True DeterministicGuard deterministic _test_scatter_base torch Tensor scatter_add_ device=device dtype=dtype is_scalar=False reduction=None dtypes torch float test_scatter_add_mult_index_base device dtype deterministic False True DeterministicGuard deterministic m n = idx = torch zeros m n device=device dtype=torch long src = torch ones m n device=device dtype=dtype res = torch zeros m n device=device dtype=dtype scatter_add_ idx src res = torch zeros m n device=device dtype=dtype scatter_add_ idx src assertEqual res m torch ones n device=device dtype=dtype atol= rtol= assertEqual res n torch ones m device=device dtype=dtype atol= rtol= FIXME discrepancy between bool ReduceAdd CUDA CPU + b CPU buggy b CUDA dtypes get_all_dtypes include_half=True include_bfloat =True include_bool=False test_scatter_reduce_sum device dtype include_self True False deterministic False True DeterministicGuard deterministic _test_scatter_base torch Tensor scatter_reduce_ device=device dtype=dtype is_scalar=False reduction= sum unique_indices=False include_self=include_self dtypes get_all_dtypes include_half=True include_bfloat =True dtypesIfCUDA get_all_dtypes include_half=True include_bfloat =True include_complex=False include_bool=False test_scatter_reduce_prod device dtype include_self True False _test_scatter_base torch Tensor scatter_reduce_ device=device dtype=dtype is_scalar=False reduction= prod unique_indices=False include_self=include_self dtypes get_all_dtypes include_half=True include_bfloat =True include_bool=False dtypesIfCUDA get_all_dtypes include_half=True include_bfloat =True include_complex=False include_bool=False test_scatter_reduce_mean device dtype include_self True False deterministic False True DeterministicGuard deterministic _test_scatter_base torch Tensor scatter_reduce_ device=device dtype=dtype is_scalar=False reduction= mean unique_indices=False include_self=include_self dtypes get_all_dtypes include_half=True include_bfloat =True include_complex=False dtypesIfCUDA get_all_dtypes include_half=True include_bfloat =True include_complex=False include_bool=False test_scatter_reduce_amax device dtype include_self True False _test_scatter_base torch Tensor scatter_reduce_ device=device dtype=dtype is_scalar=False reduction= amax unique_indices=False include_self=include_self simple test nan inf propagation dtype is_floating_point input = torch zeros device=device dtype=dtype src = torch tensor float nan -float inf -float inf float inf device=device dtype=dtype idx = torch tensor device=device input scatter_reduce_ idx src amax include_self=include_self expected_result = torch tensor float nan -float inf float inf device=device dtype=dtype include_self expected_result = assertEqual input expected_result dtypes get_all_dtypes include_half=True include_bfloat =True include_complex=False dtypesIfCUDA get_all_dtypes include_half=True include_bfloat =True include_complex=False include_bool=False test_scatter_reduce_amin device dtype include_self True False _test_scatter_base torch Tensor scatter_reduce_ device=device dtype=dtype is_scalar=False reduction= amin unique_indices=False include_self=include_self simple test nan inf propagation dtype is_floating_point input = torch zeros device=device dtype=dtype src = torch tensor float nan - -float inf float inf float inf device=device dtype=dtype idx = torch tensor device=device input scatter_reduce_ idx src amin include_self=include_self expected_result = torch tensor float nan -float inf float inf device=device dtype=dtype include_self expected_result = assertEqual input expected_result onlyCPU dtypes torch float torch float torch bfloat torch float test_scatter_expanded_index device dtype helper input_size idx_size input = torch randn input_size device=device dtype=dtype input = input clone shape = len input_size shape = idx_size dim_size = input_size idx = torch randint dim_size shape The fast path scatter when index expanded will depend sorted index where collected src indice each row input will mapped rowptrs CSR format Create some empty rows masking mask = idx idx idx mask = expanded_shape = input_size expanded_shape = idx_size idx = idx expand expanded_shape idx = idx contiguous src = torch randn expanded_shape device=device dtype=dtype out = input scatter_add idx src out = input scatter_add idx src assertEqual out out reduce sum prod mean amax amin include_self True False out = input scatter_reduce idx src reduce=reduce include_self=include_self out = input scatter_reduce idx src reduce=reduce include_self=include_self assertEqual out out helper helper helper helper dtypes torch float test_scatter_add_broadcasted_index_deterministic device dtype d inp = torch randn device=device dtype=dtype idx_ d = torch randint device=device src_shape = list inp shape src_shape d = src = torch randn src_shape device=device dtype=dtype idx_view_shape = inp ndim idx_view_shape d = idx = idx_ d view idx_view_shape expand src_shape ref = inp clone scatter_add_ d idx src DeterministicGuard True res = inp clone scatter_add_ d idx src assertEqual res ref onlyCPU dtypes torch float torch float torch bfloat test_gather_expanded_index device dtype Test when index N which would have stride should excluded fast path when index ix expanded input = torch arange view input = input dtype=dtype idx = torch arange view out = torch gather input idx out = torch gather input idx assertEqual out dtype=dtype out helper input_size idx_size input = torch randn input_size device=device dtype=dtype input = input clone shape = len input_size shape = idx_size dim_size = input_size idx = torch randint dim_size shape Test fast path gather when index expanded expanded_shape = input_size expanded_shape = idx_size idx = idx expand expanded_shape idx = idx contiguous out = torch gather input idx out = torch gather input idx assertEqual out out test unsqueezed index expanded_index kernel can handle case size stride == dimension example index size stride see https github com pytorch pytorch issues unsqueeze_helper idx dim dim == idx unsqueeze t unsqueeze_helper idx dim - unsqueeze dim - idx = torch randint dim_size input shape idx = unsqueeze_helper idx len input_size expanded_shape = idx = idx expand expanded_shape idx = idx contiguous out = torch gather input idx out = torch gather input idx assertEqual out out helper helper helper helper Generic Device Test Framework instantiation see https github com pytorch pytorch wiki Running-and-writing-tests details instantiate_device_type_tests TestScatterGather globals __name__ == __main__ run_tests