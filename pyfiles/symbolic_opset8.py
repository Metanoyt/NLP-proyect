mypy allow-untyped-defs Note ONNX operators added updated opset opset ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ New operators Compress ConstantOfShape EyeLike MaxUnpool OneHot Sinh Cosh Asinh Acosh Atanh Shrink IsNaN Sign Erf Scatter Where NonZero TfIdfVectorizer MeanVarianceNormalization Updated operators BatchNormalization removed spatial attribute Greater Less Constant MatMul PRelu Gemm Flatten more data types integers supported Cast more data types string supported Upsample moved scales attribute input Scan functools warnings torch torch _C _onnx _C_onnx torch onnx errors torch onnx _internal torchscript_exporter _type_utils jit_utils registration symbolic_helper symbolic_opset opset _onnx_symbolic = functools partial registration onnx_symbolic opset= block_listed_operators = nonzero where scatter scatter_add erf sign isnan gather arange masked_fill index_fill index_copy repeat_interleave any all block_listed_op block_listed_operators _onnx_symbolic f aten block_listed_op symbolic_helper _block_list_in_opset block_listed_op _onnx_symbolic aten upsample_nearest d decorate= symbolic_helper _apply_params upsample_nearest d nearest _onnx_symbolic aten upsample_nearest d decorate= symbolic_helper _apply_params upsample_nearest d nearest _onnx_symbolic aten upsample_nearest d decorate= symbolic_helper _apply_params upsample_nearest d nearest _onnx_symbolic aten upsample_linear d decorate= symbolic_helper _apply_params upsample_linear d linear _onnx_symbolic aten upsample_bilinear d decorate= symbolic_helper _apply_params upsample_bilinear d linear _onnx_symbolic aten upsample_trilinear d decorate= symbolic_helper _apply_params upsample_trilinear d linear _interpolate name dim interpolate_mode symbolic_fn g input output_size args scales align_corners = symbolic_helper _get_interpolate_attributes g interpolate_mode args symbolic_helper _interpolate_warning interpolate_mode align_corners = symbolic_helper _maybe_get_scalar align_corners align_corners symbolic_helper _unimplemented name align_corners == True input output_size = symbolic_helper _maybe_get_const output_size symbolic_helper _is_value output_size symbolic_helper _unimplemented name torch _C Value output_size indexing scales None scales = i float output_size - dim - i float input type sizes - dim - i i range dim g op Upsample input mode_s=interpolate_mode scales_f=scales symbolic_fn _onnx_symbolic aten __interpolate __interpolate g jit_utils GraphContext input size scale_factor mode align_corners recompute_scale_factor antialias align_corners = symbolic_helper _maybe_get_const align_corners b symbolic_helper _is_none align_corners align_corners symbolic_helper _unimplemented interpolate align_corners == True symbolic_helper _is_none scale_factor symbolic_helper _is_value scale_factor symbolic_helper _unimplemented interpolate dynamic scales opset symbolic_helper _is_none size symbolic_helper _is_value size symbolic_helper _unimplemented interpolate dynamic size opset scales mode = symbolic_helper _interpolate_get_scales_and_mode g input size scale_factor mode align_corners g op Upsample input mode_s=mode scales_f=scales NOTE We should create wrapper kind operation after resolving shape type propagation issue cast operators Some symbolic functions depend shape information input tensor which lost after casting _try_cast_integer_to_float g jit_utils GraphContext args floating_scalar_types = _type_utils JitScalarType HALF _type_utils JitScalarType FLOAT _type_utils JitScalarType DOUBLE old_type = None Cast input tensor Float its scalarType known floating number If casting performed old scalarType otherwise None arg _type = _type_utils JitScalarType from_value args _type_utils JitScalarType UNDEFINED arg _type = _type_utils JitScalarType UNDEFINED old_type = arg _type old_type floating_scalar_types old_type = old_type scalar_name type ignore assignment args = tuple g op Cast arg to_i=_C_onnx TensorProtoDataType FLOAT arg args None + args warnings warn Only floating datatype supported these operators Greater Less MatMul PRelu Gemm Flatten This might cause onnx model incorrect inputs have integer datatypes stacklevel= old_type + args _cast_to_type g jit_utils GraphContext input to_type to_type None input getattr opset f _cast_ to_type g input False _comparison_operator g jit_utils GraphContext input other op_name other = symbolic_helper _maybe_get_scalar other other = symbolic_helper _if_scalar_type_as other input _ input other = _try_cast_integer_to_float g input other g op op_name input other NOTE For symbolics gt lt bmm matmul prelu mm addmm view flatten integer input type supported opset Cast float possible _onnx_symbolic aten gt gt g jit_utils GraphContext input other _comparison_operator g input other Greater _onnx_symbolic aten lt lt g jit_utils GraphContext input other _comparison_operator g input other Less _onnx_symbolic aten bmm bmm g jit_utils GraphContext other symbolic_helper _try_get_scalar_type old_type other = _try_cast_integer_to_float g other _cast_to_type g g op MatMul other old_type g op MatMul other _onnx_symbolic aten matmul matmul g jit_utils GraphContext other bmm g other _onnx_symbolic aten prelu prelu g jit_utils GraphContext weight self_rank = symbolic_helper _get_tensor_rank weight_sizes = symbolic_helper _get_tensor_sizes weight self_rank None self_rank weight = g op Unsqueeze weight axes_i=list range self_rank - self_rank == weight_sizes == weight both scalar weight has rank == squeeze weight weight = symbolic_helper _squeeze_helper g weight symbolic_helper _try_get_scalar_type old_type weight = _try_cast_integer_to_float g weight _cast_to_type g g op PRelu weight old_type g op PRelu weight _onnx_symbolic aten mm mm g jit_utils GraphContext other Create dummy C tensor Only needed API purposes value since beta = scalar_type = symbolic_helper _try_get_scalar_type other scalar_type None raise errors SymbolicValueError mm can only operate tensors known types zero_constant = g op Constant value_t=torch tensor dtype=scalar_type dtype symbolic_helper _try_get_scalar_type old_type other zero_constant = _try_cast_integer_to_float g other zero_constant _cast_to_type g g op Gemm other zero_constant beta_f= alpha_f= old_type g op Gemm other zero_constant beta_f= alpha_f= _onnx_symbolic aten addmm symbolic_helper parse_args v v v t t addmm g jit_utils GraphContext mat mat beta alpha symbolic_helper _try_get_scalar_type old_type mat mat = _try_cast_integer_to_float g mat mat _cast_to_type g g op Gemm mat mat beta_f=symbolic_helper _scalar beta alpha_f=symbolic_helper _scalar alpha old_type g op Gemm mat mat beta_f=symbolic_helper _scalar beta alpha_f=symbolic_helper _scalar alpha _onnx_symbolic aten flatten flatten g jit_utils GraphContext input start_dim end_dim start_dim_i = symbolic_helper _get_const start_dim i start_dim end_dim_i = symbolic_helper _get_const end_dim i end_dim dim = input type dim end_dim_i end_dim_i = dim + end_dim_i use ONNX s Flatten operator cases where output shape D start_dim_i == end_dim_i == dim - symbolic_helper _try_get_scalar_type input old_type input = _try_cast_integer_to_float g input _cast_to_type g g op Flatten input axis_i=start_dim_i old_type g op Flatten input axis_i=start_dim_i start_dim_i == end_dim_i == dim - symbolic_helper _try_get_scalar_type input old_type input = _try_cast_integer_to_float g input _cast_to_type g g op Flatten input axis_i=end_dim_i + old_type g op Flatten input axis_i=end_dim_i + opset flatten g input start_dim end_dim _constant_fill g jit_utils GraphContext sizes dtype int const_value dtype None scalar_type = _type_utils JitScalarType FLOAT scalar_type = _type_utils JitScalarType dtype scalar_type dtype is_floating_point result = g op ConstantFill sizes dtype_i=_type_utils JitScalarType FLOAT onnx_type input_as_shape_i= value_f=const_value g op Cast result to_i=scalar_type onnx_type g op ConstantFill sizes dtype_i=scalar_type onnx_type input_as_shape_i= value_f=const_value _onnx_symbolic aten empty symbolic_helper parse_args v i v v v v empty g jit_utils GraphContext sizes dtype layout device pin_memory=False memory_format=None zeros g sizes dtype layout device pin_memory _onnx_symbolic aten empty_like symbolic_helper parse_args v i v v v v empty_like g jit_utils GraphContext input dtype layout device pin_memory=False memory_format=None zeros_like g input dtype layout device pin_memory _onnx_symbolic aten zeros symbolic_helper parse_args v i v v v zeros g jit_utils GraphContext sizes dtype layout device pin_memory=False NOTE no way set device layout ONNX so we ignore _constant_fill g sizes dtype _onnx_symbolic aten zeros_like symbolic_helper parse_args v i v v v v zeros_like g jit_utils GraphContext input dtype layout device pin_memory=False memory_format=None shape = g op Shape input _constant_fill g shape dtype _onnx_symbolic aten ones symbolic_helper parse_args v i v v v ones g jit_utils GraphContext sizes dtype layout device pin_memory=False _constant_fill g sizes dtype _onnx_symbolic aten ones_like symbolic_helper parse_args v i v v v v ones_like g jit_utils GraphContext input dtype layout device pin_memory=False memory_format=None shape = g op Shape input _constant_fill g shape dtype _onnx_symbolic aten full full g jit_utils GraphContext sizes value dtype layout device pin_memory=False const_value = symbolic_helper _maybe_get_const value t symbolic_helper _is_value const_value tmp = zeros g sizes dtype layout device opset add g tmp value g op Constant value_t=torch tensor dtype = symbolic_helper _get_const dtype i dtype _constant_fill g sizes dtype const_value _onnx_symbolic aten full_like symbolic_helper parse_args v f i v v v v full_like g jit_utils GraphContext input fill_value dtype layout device pin_memory=False memory_format=None shape = g op Shape input _constant_fill g shape dtype fill_value _onnx_symbolic aten repeat repeat g jit_utils GraphContext repeats symbolic_helper _is_value repeats repeats = g op Constant value_t=torch LongTensor repeats symbolic_helper _is_packed_list repeats repeat_size_len = len symbolic_helper _unpack_list repeats const_repeats = symbolic_helper _maybe_get_const repeats repeat_size_len = len const_repeats isCompleteTensor sizes = type sizes diff_dims = repeat_size_len - len sizes diff_dims = opset view g g op Constant value_t=torch tensor diff_dims + sizes g op Tile repeats