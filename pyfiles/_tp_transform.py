mypy allow-untyped-defs copy operator collections abc Sequence typing Any cast Optional torch torch _subclasses fake_tensor FakeTensor torch distributed tensor DeviceMesh distribute_tensor DTensor torch distributed tensor _dtensor_spec DTensorSpec TensorMeta torch distributed tensor _op_schema OpSchema OpSpec OutputSharding OutputSpecType torch distributed tensor _redistribute redistribute_local_tensor torch distributed tensor parallel style ColwiseParallel ParallelStyle torch distributed tensor placement_types Placement Replicate Shard torch export ExportedProgram torch export exported_program ExportGraphSignature torch fx GraphModule torch fx experimental proxy_tensor make_fx torch fx node Node torch fx passes infra pass_base PassBase PassResult torch fx passes shape_prop _extract_tensor_metadata torch utils _pytree pytree __all__ = tensor_parallel_transformation aten = torch ops aten tensor_parallel_transformation exported_program ExportedProgram rank int world_size int device_type str parallel_strategies dict str ParallelStyle - ExportedProgram The entry point function perform graph transformations exported program transform single-device graph into tensor parallel graph warning This API experimental subject change gm = exported_program graph_module sig = copy deepcopy exported_program graph_signature state_dict = copy copy exported_program state_dict gm _set_replace_hook sig get_replace_hook res = _TensorParallelTransformPass rank world_size device_type state_dict exported_program graph_signature parallel_strategies gm assert res None gm = res graph_module exported_program _update gm sig state_dict=state_dict _TensorParallelTransformPass PassBase This pass responsible transforming single-device graph into tensor parallel graph It will mark OpSpec each node graph partition graph into distributed graph then shard parameters buffers accordingly __init__ rank int world_size int device_type str state_dict dict str torch Tensor graph_signature ExportGraphSignature parallel_strategies dict str ParallelStyle - None super __init__ rank = rank mesh = DeviceMesh device_type torch arange world_size state_dict dict str torch Tensor = state_dict graph_signature = graph_signature parallel_strategies = parallel_strategies call graph_module - PassResult gm = copy deepcopy graph_module parameter_placements = _generate_parameter_and_buffer_placements list state_dict keys parallel_strategies placement_strategies = _mark_sharding gm graph_signature mesh parameter_placements _partitioner gm _shard_state_dict state_dict placement_strategies graph_signature mesh PassResult gm True _generate_parameter_and_buffer_placements params_and_buffers list str parallel_strategies dict str ParallelStyle - dict str Placement Build parameter placements based give parallel style linear layers parameter_placements dict str Placement = linear_fqn parallel_style parallel_strategies items weight_fqn = f linear_fqn weight bias_fqn = f linear_fqn bias assert weight_fqn params_and_buffers parameter_placements weight_fqn = Shard parallel_style == ColwiseParallel Shard bias_fqn params_and_buffers parameter_placements bias_fqn = Shard parallel_style == ColwiseParallel Replicate parameter_placements _mark_tensor_parallel_shardings gm GraphModule graph_signature ExportGraphSignature mesh DeviceMesh parameter_placements dict str Placement - dict Node OpSpec Mark placement strategies parameter buffer placeholder nodes placement_strategies dict Node OpSpec = num_params_and_buffers = len graph_signature inputs_to_parameters + len graph_signature inputs_to_buffers placeholder_idx int = node gm graph nodes node op == placeholder placeholder_idx num_params_and_buffers fqn str = _get_input_node_fqn node name graph_signature placement Placement = parameter_placements fqn fqn parameter_placements Replicate placement_strategies node = _create_placement_strategy node mesh placements= placement placeholder_idx += placement_strategies node = _create_placement_strategy node mesh placements= Replicate placement_strategies _get_input_node_fqn input_name str graph_signature ExportGraphSignature - str Return FQN input node input_name graph_signature inputs_to_parameters graph_signature inputs_to_parameters input_name input_name graph_signature inputs_to_buffers graph_signature inputs_to_buffers input_name raise ValueError f input_name found inputs_to_parameters inputs_to_buffers _mark_sharding gm GraphModule graph_signature ExportGraphSignature mesh DeviceMesh parameter_placements dict str Placement - dict Node OpSpec Mark sharding strategy each node graph module placement_strategies dict Node OpSpec = _mark_tensor_parallel_shardings gm graph_signature mesh parameter_placements node gm graph nodes node op == placeholder node placement_strategies placement_strategies node = _create_placement_strategy node mesh placements= Replicate node meta sharding = placement_strategies node node op == call_function node target operator getitem input_nodes = node all_input_nodes assert len input_nodes == f non-compute op only support one input now found node node length inputs len node args arg_strategy = placement_strategies input_nodes placement_strategies node = _create_placement_strategy node mesh placements=arg_strategy output_spec placements input_specs=_get_input_node_specs node placement_strategies node meta sharding = placement_strategies node op_schema = _get_op_schema node placement_strategies get DTensor specs inputs outputs op_schema op DTensor _op_dispatcher sharding_propagator op_strategy_funcs op_schema op DTensor _op_dispatcher sharding_propagator op_to_rules Mark all replicated output_sharding = _generate_default_output_sharding node mesh op_schema output_sharding = DTensor _op_dispatcher sharding_propagator propagate_op_sharding type ignore assignment op_schema placement_strategies node = OpSpec pyrefly ignore bad-argument-type output_specs=_get_output_spec_from_output_sharding output_sharding pyrefly ignore missing-attribute input_specs=output_sharding redistribute_schema args_spec pyrefly ignore missing-attribute output_sharding redistribute_schema None _get_input_node_specs node placement_strategies node meta sharding = placement_strategies node node op == output node meta sharding = None raise RuntimeError f op code node op supported placement_strategies _get_output_spec_from_output_sharding output_sharding OutputSharding - DTensorSpec Util function extract output spec output sharding isinstance output_sharding output_spec DTensorSpec output_sharding output_spec For ops multiple outputs outputs should have same output spec assert isinstance output_sharding output_spec Sequence assert output_sharding output_spec None output_sharding output_spec tensor_meta = None output_sharding output_spec _create_placement_strategy node Node mesh DeviceMesh placements tuple Placement input_specs Optional Sequence DTensorSpec = None - OpSpec Util function construct OpSpec given node placement = OpSpec input_specs=input_specs output_specs=DTensorSpec mesh=mesh placements=placements _populate_tensor_meta node placement output_specs placement _populate_tensor_meta node Node output_spec OutputSpecType - None Util function populate tensor meta output_spec based node metadata isinstance node meta val Sequence assert isinstance output_spec Sequence spec fake_tensor zip output_spec node meta val assert spec None spec tensor_meta = TensorMeta shape=fake_tensor shape stride=fake_tensor stride dtype=fake_tensor dtype assert isinstance output_spec DTensorSpec output_spec tensor_meta = TensorMeta shape=node meta val shape stride=node meta val stride dtype=node meta val dtype _generate_default_output_sharding node Node mesh DeviceMesh op_schema OpSchema - OutputSharding Util function create default output sharding suggests Replicate placement both args outputs update_arg_spec arg_spec DTensorSpec - DTensorSpec DTensorSpec mesh=arg_spec mesh placements= Replicate tensor_meta=arg_spec tensor_meta new_op_schema = OpSchema op=op_schema op args_schema=pytree tree_map_only DTensorSpec update_arg_spec op_schema args_schema kwargs_schema=op_schema kwargs_schema create_output_spec tensor FakeTensor - DTensorSpec DTensorSpec mesh=mesh placements= Replicate tensor_meta=TensorMeta shape=tensor shape stride=tensor stride dtype=tensor dtype OutputSharding output_spec=pytree tree_map_only FakeTensor create_output_spec node meta val redistribute_schema=new_op_schema needs_redistribute=True _partitioner gm torch fx GraphModule - torch fx GraphModule Graph partitioner partitions single device graph distributed graph node gm graph nodes node_sharding = node meta sharding node op == placeholder out_spec = node_sharding output_spec local_val = _partition_val node meta val out_spec update node value node meta val = local_val node op == call_function out_spec = node_sharding output_spec check there s misaligned sharding insert reshard there expected_input_specs = node_sharding input_specs idx input_arg enumerate node all_input_nodes input_arg_sharding = input_arg meta sharding input_arg_spec = input_arg_sharding output_spec desired_spec = out_spec expected_input_specs None expected_input_specs idx input_arg_spec = desired_spec _insert_reshard_gm gm node input_arg input_arg_spec desired_spec convert output val its local component output_val = node meta val node meta val = _partition_val output_val out_spec node op == output input_arg node all_input_nodes input args output should Replicate otherwise redistribution needed input_args_to_check Sequence Node = input_arg isinstance input_arg Sequence input_arg arg input_args_to_check arg_sharding = arg meta sharding arg_spec = arg_sharding output_spec desired_spec = copy copy arg_spec desired_spec placements = Replicate arg_spec = desired_spec _insert_reshard_gm gm node arg arg_spec desired_spec raise RuntimeError f op code node supported _clean_up_graph_metadata gm gm graph lint gm recompile gm _partition_val val Any spec DTensorSpec - Any util function convert full tensor val its local component isinstance val torch Tensor local_shard = val val ndim == If s already scalar tensor already local we don t need do anything local_shard idx placement enumerate spec placements placement is_shard placement = cast Shard placement num_chunks = spec mesh size mesh_dim=idx my_coord = spec mesh get_coordinate assert my_coord None current rank mesh my_coord_on_mesh_dim = my_coord idx local_shard = placement _split_tensor local_shard num_chunks with_padding=False contiguous=True my_coord_on_mesh_dim local_shard isinstance val list tuple val __class__ _partition_val v spec v val raise RuntimeError f val type type val supported _insert_reshard_gm gm torch fx GraphModule node Node input_arg Node input_arg_spec DTensorSpec desired_spec DTensorSpec - None Transform graph tensor redistribution input_arg_spec tensor_meta = input_arg meta tensor_meta desired_spec tensor_meta = input_arg meta tensor_meta input_arg_tensor = input_arg meta val insert reshard operation reshard_fn local_tensor torch Tensor - torch Tensor redistribute_local_tensor local_tensor input_arg_spec desired_spec reshard_gm = make_fx reshard_fn input_arg_tensor reshard_gm_nodes = list reshard_gm graph nodes input_node = reshard_gm_nodes gm graph inserting_before node copy nn_module_stack metadata output all-reduce nodes reshard_node reshard_gm graph nodes reshard_node op placeholder output reshard_node meta nn_module_stack = copy copy input_arg meta nn_module_stack input_arg op = placeholder copy copy node meta nn_module_stack output_node = gm graph graph_copy reshard_gm graph val_map= input_node input_arg node replace_input_with input_arg output_node type ignore arg-type _clean_up_graph_metadata gm torch fx GraphModule - None Clean up graph removing sharding partitioning related metadata node gm graph nodes sharding node meta del node meta sharding val node meta isinstance node meta val torch Tensor local_tensor_meta = _extract_tensor_metadata node meta val node meta tensor_meta = local_tensor_meta _get_input_node_specs node Node placement_strategies dict Node OpSpec - tuple DTensorSpec Get input specs node input_specs_list list DTensorSpec = input_arg node all_input_nodes input_arg placement_strategies output_spec = placement_strategies input_arg output_specs assert isinstance output_spec DTensorSpec input_specs_list append output_spec raise ValueError f input_arg does have output_spec populated tuple input_specs_list _get_op_schema node Node placement_strategies dict Node OpSpec - OpSchema Util function construct operator schema node args_schema_list = pytree tree_map_only Node lambda arg placement_strategies arg output_specs node args op_schema = OpSchema op=cast torch _ops OpOverload node target args_schema=tuple args_schema_list kwargs_schema=cast dict str object node kwargs op_schema _shard_state_dict state_dict dict str torch Tensor placement_strategies dict Node OpSpec graph_signature ExportGraphSignature mesh DeviceMesh - None Inplace partition weights based OpSpec node op_spec placement_strategies items node op = placeholder continue node name graph_signature inputs_to_parameters fqn = graph_signature inputs_to_parameters node name node name graph_signature inputs_to_buffers fqn = graph_signature inputs_to_buffers node name continue assert fqn state_dict f fqn found state dict state_dict keys original_param = state_dict fqn dtensor_param = distribute_tensor original_param mesh op_spec output_spec placements local_param = dtensor_param to_local state_dict fqn = torch nn Parameter local_param isinstance original_param torch nn Parameter local_param