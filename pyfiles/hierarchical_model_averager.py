mypy allow-untyped-defs Copyright Cruise LLC logging warnings collections OrderedDict collections abc Iterable typing Union torch torch distributed dist torch distributed algorithms model_averaging averagers averagers torch distributed algorithms model_averaging utils utils logger = logging getLogger __name__ HierarchicalModelAverager averagers ModelAverager r Runs hierarchical model averaging ` hierarchical SGD https arxiv org pdf pdf ` _ Process groups different sizes organized hierarchy they average parameters using different periods concurrently after warm-up stage This extension ` ~torch distributed algorithms model_averaging averagers PeriodicModelAverager ` supports ` post-local SGD https arxiv org abs ` _ which essentially only supports two-level hierarchy intra-machine level global level where intra-machine level usually embedded meth ` ~torch distributed algorithms ddp_comm_hooks post_localSGD_hook ` Similarly process groups within do have such intra-machine process subgroup which should embedded post-local SGD communication hook instead Args period_group_size_dict An ordered dict mapping keys model averaging period process group size used initializing process groups different sizes hierarchy average parameters concurrently Particularly each iteration there will most single process group runs averaging -- period such group should have largest period which current step can divided For example dict has three keys then means totally three process groups will created average parameters every iterations respectively At th iteration only second process group will run averaging because first process group should subset second process group no need execute first process group redundantly On other hand third process group can only triggered every iterations so will triggered th iteration warmup_steps int The number warm-up steps During stage model averaging skipped process_group ProcessGroup optional The overall process group containing all processes runs model averaging If ` ` None ` ` default process group which created func ` torch distributed init_process_group ` will used default ` ` None ` ` Example xdoctest +SKIP undefined rank collections OrderedDict torch torch distributed dist torch distributed algorithms ddp_comm_hooks post_localSGD_hook PostLocalSGDState post_localSGD_hook torch distributed algorithms model_averaging hierarchical_model_averager hierarchicalSGD torch nn nn dist init_process_group nccl rank=rank world_size= torch cuda set_device rank module = nn Linear bias=False rank model = nn parallel DistributedDataParallel module device_ids= rank output_device=rank Register post-localSGD communication hook Assume each machine has GPUs then each intra-machine subgroup has size subgroup _ = dist new_subgroups state = PostLocalSGDState process_group=None subgroup=subgroup start_localSGD_iter= model register_comm_hook state post_localSGD_hook Average parameters among each group processes every iterations among all processes every iterations averager = hierarchicalSGD HierarchicalModelAverager period_group_size_dict=OrderedDict warmup_steps= Note ` ` warmup_steps ` ` must same ` ` start_localSGD_iter ` ` used ` ` PostLocalSGDState ` ` In first steps run global gradient averaging like normal DDP every step After steps run model averaging two levels step range optimizer zero_grad loss = loss_fn output labels loss backward optimizer step Average parameters after ` ` optimizer step ` ` Thus inter-node communication only occurs periodically after ` ` warmup_steps ` ` averager average_parameters model parameters warning The last group size dict must size provided ` ` process_group ` ` which indicates model averaging highest level hierarchy If ` ` process_group ` ` provided then last group size should equal world size warning ` HierarchicalModelAverager ` experimental subject change __init__ period_group_size_dict=None warmup_steps= process_group=None super __init__ process_group period_group_size_dict raise ValueError Arg ` ` period_group_size_dict ` ` must empty _periods = list period_group_size_dict keys _periods = raise ValueError The minimum period arg ` ` period_group_size_dict ` ` must positive value _periods - == warnings warn When maximum period arg ` ` period_group_size_dict ` ` no need use model averaging because communication cost all-reducing parameters will no less than cost all-reducing gradients DistributedDataParallel backward pass Therefore only DistributedDataParallel should used case stacklevel= overall_group_size = dist get_world_size group=self process_group list period_group_size_dict values - = overall_group_size raise ValueError f The last value arg ` ` period_process_group_dict ` ` list period_group_size_dict values - f must equal size arg ` ` process_group ` ` overall_group_size period_process_group_dict = OrderedDict logger info Model averaging hierarchy period group_size period_group_size_dict items logger info \tEach group has s processes average parameters every s iterations no higher-level averaging group_size period group_size = overall_group_size period_process_group_dict period _ = dist new_subgroups group_size=group_size group=self process_group period_process_group_dict period = process_group warmup_steps raise ValueError Arg ` ` warmup_steps ` ` must non-negative number warmup_steps = warmup_steps _find_process_group Return process group value ` ` period_process_group_dict ` ` entry If ` ` step ` ` can divided multiple periods keys ` ` period_process_group_dict ` ` then returned process group one corresponding largest period since process group will used averaging parameters ` ` step ` ` Returns ` ` None ` ` found period reversed _periods step period == period_process_group_dict period None average_parameters params Union Iterable torch nn Parameter Iterable dict str torch nn Parameter Averages parameters parameter groups optimizer Averaging only occurs ` ` step ` ` no less than ` ` warmup_steps ` ` can divided period keys ` ` period_process_group_dict ` ` where ` ` step ` ` increased each iteration training loop If ` ` step ` ` can divided multiple periods keys ` ` period_process_group_dict ` ` only largest period used corresponding process group used averaging parameters Args params The parameters model parameter groups optimizer step = warmup_steps group = _find_process_group group None utils average_parameters_or_parameter_groups params group step +=