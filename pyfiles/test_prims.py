Owner s module decompositions functools partial itertools product unittest torch torch testing make_tensor torch testing _internal common_utils parametrize run_tests TestCase TEST_SCIPY set_default_dtype torch testing _internal common_device_type instantiate_device_type_tests onlyCUDA dtypes OpDTypes torch testing _internal common_methods_invocations op_db torch testing _internal common_device_type ops torch testing _internal logging_tensor LoggingTensor capture_logs log_input torch _prims prims torch _prims_common CUDARngStateHelper torch _prims executor make_traced torch _refs refs TEST_SCIPY scipy special NVPRIM_ATEN_FALLBACK_WARNING = fallback aten executor GET_ISOLATED_GRAPHMODULE_ERROR = get_isolated_graphmodule failed decomposition TestPrims TestCase onlyCUDA dtypes torch float test_broadcast_in_dim device dtype _wrapper b broadcast_dimensions prims broadcast_in_dim b shape broadcast_dimensions traced = make_traced _wrapper make_arg = partial make_tensor device=device dtype=dtype executor aten fn = partial traced executor=executor Same shape shape = = make_arg shape b = make_arg shape low= high= result = fn b assertEqual result shape shape assertTrue result is_contiguous assertEqual result Error input reordering dims assertRaises Exception result = fn b Adding outermost dimensions = make_arg b = make_arg low= high= result = fn b assertEqual result shape b shape assertEqual broadcast_to b shape result Expands = make_arg b = make_arg low= high= result = fn b assertEqual result shape b shape assertEqual expand_as result result Unsqueezes = make_arg b = make_arg low= high= result = fn b assertEqual result shape b shape assertEqual unsqueeze result onlyCUDA dtypes torch float test_broadcast_in_dim_sum device dtype _wrapper a_sum = prims sum a_bc = prims broadcast_in_dim a_sum a_bc traced = make_traced _wrapper make_arg = partial make_tensor device=device dtype=dtype executor aten fn = partial traced executor=executor shape = = make_arg shape result = fn assertEqual result shape assertTrue result is_contiguous assertEqual _wrapper result unittest skipIf TEST_SCIPY SciPy found dtypes torch float torch long test_cbrt_prim device dtype make_arg = partial make_tensor device=device dtype=dtype batches = shapes = Sets default dtype NumPy s default dtype double set_default_dtype torch double Tested here OP currently exposed tested ATen b s product batches shapes x = make_arg b + s y = prims cbrt x x_np = x cpu numpy y_np = scipy special cbrt x_np assertEqual y y_np exact_device=False dtypes torch float test_collapse device dtype t = torch rand dim_ranges = expected_shapes = start end shape zip dim_ranges expected_shapes expect = t reshape shape copy = prims collapse t start end assertEqual copy expect assertFalse copy _is_view view = prims collapse_view t start end assertEqual view expect assertTrue view _is_view t_discontig = t transpose assertRaises RuntimeError msg= Attempting view collapsed tensor no such view exists view = prims collapse_view t_discontig copy = prims collapse t_discontig assertEqual copy t_discontig reshape error_dims = - - start end error_dims fn prims collapse prims collapse_view assertRaises AssertionError fn t start end test_aten_overload_to_prims device This test ensure torch ops aten calls replaced refs torch fx experimental proxy_tensor make_fx torch _prims context TorchRefsMode = torch randn device=device func torch ops aten sigmoid default torch ops aten digamma default TorchRefsMode gm = make_fx func Check all call_function nodes prims call_function_nodes = list filter lambda n n op == call_function gm graph nodes all_prims_namespace = all node target name startswith prims node call_function_nodes assertTrue all_prims_namespace onlyCUDA dtypes torch float parametrize correction test_var device dtype correction _wrapper prims var correction=correction traced = make_traced _wrapper make_arg = partial make_tensor device=device dtype=dtype executor aten fn = partial traced executor=executor shape = = make_arg shape result = fn assertEqual result shape assertTrue result is_contiguous assertEqual _wrapper result dtypes torch float test_memory_format_strides device dtype shapes = channels_last_shapes = channels_last_ d_shapes = pairs = shapes torch contiguous_format channels_last_shapes torch contiguous_format channels_last_ d_shapes torch contiguous_format channels_last_shapes torch channels_last channels_last_ d_shapes torch channels_last_ d shapes memory_format pairs shape shapes tests empty expected = torch empty shape device=device dtype=dtype memory_format=memory_format actual = refs empty shape device=device dtype=dtype memory_format=memory_format assertEqual expected stride actual stride tests clone = torch testing make_tensor shape device=device dtype=dtype expected = torch clone memory_format=memory_format actual = torch clone memory_format=memory_format assertEqual expected stride actual stride tests contiguous = torch testing make_tensor shape device=device dtype=dtype noncontiguous=True expected = contiguous memory_format=memory_format actual = refs contiguous memory_format=memory_format assertEqual expected stride actual stride dtypes torch float test_reshape_view_method device dtype make_arg = partial make_tensor device=device dtype=dtype = make_arg new_shape = result_eager = reshape new_shape result_refs = refs reshape new_shape assertEqual result_eager result_refs result_eager = view new_shape result_refs = refs view new_shape assertEqual result_eager result_refs onlyCUDA dtypes torch float test_philox_rand device dtype sizes = offsets repeats = Checks multiple rand calls results multiple philox_rand calls size sizes torch cuda manual_seed references = results = rng_states = _ range repeats rng_states append CUDARngStateHelper get_torch_state_as_tuple references append torch rand size device=device dtype=dtype torch cuda manual_seed idx range repeats seed offset = rng_states idx result _ = torch ops rngprims philox_rand size seed=seed offset=offset stride=None device=device dtype=dtype results append result b zip references results assertEqual b dtypes torch float test_functional_rng_wrappers device dtype torch manual_seed ref = torch rand device=device dtype=dtype ref = torch rand device=device dtype=dtype torch manual_seed rng_state res = torch _prims rng_prims run_and_save_rng_state torch rand device=device dtype=dtype rng_state res = torch _prims rng_prims run_and_save_rng_state torch rand device=device dtype=dtype res = torch _prims rng_prims run_with_rng_state rng_state torch rand device=device dtype=dtype res = torch _prims rng_prims run_with_rng_state rng_state torch rand device=device dtype=dtype assertEqual ref res assertEqual ref res assertEqual ref res assertEqual ref res TestPrimsBasic TestCase test_torch_ops r = make_tensor device= cpu dtype=torch float assertEqual torch ops prims sin r torch sin r r = LoggingTensor r capture_logs logs log_input input r prims sin r assertExpectedInline \n join logs \ $ f = input input $ f = torch _ops prims sin default $ test_mul_complex prims mul torch randn + j test_clone_complex torch _dispatch python enable_python_dispatcher x = torch randn dtype=torch complex device= meta conj x + test_clone_meta_stride_preservation_dense tensor = torch randn t meta_clone = prims _clone_meta tensor memory_format=torch preserve_format assertEqual tensor stride meta_clone stride test_clone_meta_stride_preservation_sparse tensor = torch arange float view meta_clone = prims _clone_meta tensor memory_format=torch preserve_format assertEqual tensor contiguous stride meta_clone stride test_check_deprecation_warning assertWarnsRegex FutureWarning will removed future torch _prims_common check True lambda message instantiate_device_type_tests TestPrims globals TestRefs TestCase dtypes torch float test_constant_pad_nd_memory_format device dtype Test memory format preserved unambiguous cases mf ndim torch channels_last torch contiguous_format torch channels_last_ d torch contiguous_format = torch zeros ndim memory_format=mf res = refs constant_pad_nd pad= ndim assertTrue res is_contiguous memory_format=mf Ambiguous cases is_channels_last_ is_contiguous_ results channels_last output = torch empty_strided stride= assertTrue is_contiguous memory_format=torch channels_last assertTrue is_contiguous actual = refs constant_pad_nd pad= expect = torch constant_pad_nd pad= assertEqual actual stride expect stride assertTrue actual is_contiguous memory_format=torch channels_last is_channels_last_contiguous_ is_channels_last_ results contiguous output = torch empty_strided stride= assertTrue is_contiguous memory_format=torch channels_last assertTrue is_contiguous actual = refs constant_pad_nd pad= expect = torch constant_pad_nd pad= assertEqual actual stride expect stride assertTrue actual is_contiguous test_unbind If unbind returns empty tuple breaks some assumptions some backward tests test_ops py So can t put test into common_methods_invocations py = torch rand actual = refs unbind expect = torch unbind assertEqual actual expect test_logspace_with_complex_input actual = refs logspace + j steps= expect = torch logspace + j steps= assertEqual actual expect test_linspace_with_complex_input actual = refs linspace + j steps= expect = torch linspace + j steps= assertEqual actual expect From https github com pytorch pytorch issues test_infinite_loop_from_py_dispatcher enables prim decomps torch _dispatch python enable_python_dispatcher x = torch ones x device= meta test_inferred_tags assertEqual torch ops prims normal default tags torch Tag nondeterministic_seeded torch Tag pt _compliant_tag instantiate_device_type_tests TestRefs globals TestDecomp TestCase ops op op op_db op supports_varargs dtypes=OpDTypes any_one test_decomposition_method_vararg device dtype op some ops have vararg variants methods tests we don t have tests varargs OpInfo so we need improvise bit The rule general functions special cases being e g tensor creation functions taking shapes things can vararg method has only one argument sequence type e g permute can called d tensor t t permute well t permute when signature native_functions yaml shows arguments Tensor IntList dims we might need adjust things factory functions have them do their own test torch fx experimental proxy_tensor make_fx torch _prims context TorchRefsMode filter out empty tuple cannot varargs sample_inputs = si si op sample_inputs device dtype requires_grad=False si args - si args si input just run one test we assume there suitable one tests sample_input = next sample_inputs all_args = sample_input input + sample_input args general methods take varargs always function variants exception rule factory functions op is_factory_function fn = op op fn = op method_variant TorchRefsMode gm = make_fx fn all_args - all_args - case we add random factory functions torch manual_seed res = gm all_args - all_args - torch manual_seed expected = fn all_args - all_args - assertEqual res expected instantiate_device_type_tests TestDecomp globals __name__ == __main__ run_tests