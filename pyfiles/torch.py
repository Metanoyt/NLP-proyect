mypy allow-untyped-decorators mypy allow-untyped-defs This module implements variable tracking torch functions operations during Dynamo tracing It provides classes handle different types torch operations TorchInGraphFunctionVariable Handles torch functions should captured FX graph Provides special handling constant folding tensor methods torch function overrides Manages complex cases like out= variants parameter construction TorchCtxManagerClassVariable Handles torch context managers like torch no_grad autocast etc Provides implementations entering exiting these contexts during tracing DispatchKeySetVariable Represents torch DispatchKeySet managing dispatch keys device-specific operations during tracing The module includes special handling - Constant folding pure functions - Tensor method calls - torch nn Parameter construction - __torch_function__ overrides - Context manager state tracking - Device dtype management This core part Dynamo s tracing system translating torch operations into traceable graph nodes while preserving correct semantics handling edge cases functools inspect logging math re collections abc Callable Sequence typing Any Optional TYPE_CHECKING torch _C torch _refs torch fx torch nn torch _guards TracingContext torch _logging warning_once torch utils _python_dispatch is_traceable_wrapper_subclass_type config graph_break_hints polyfills variables codegen PyCodegen create_parameter_op can_convert_to_tracable_parameter new_parameter_placeholder tracable_create_parameter device_interface get_registered_device_interfaces exc raise_observed_exception unimplemented_v guards GuardBuilder install_guard source AttrSource CallFunctionNoArgsSource SyntheticLocalSource TorchSource utils check_unspec_or_constant_args guard_if_dyn has_torch_function hashable is_wrapper_or_member_descriptor product proxy_args_kwargs unwrap_if_wrapper base raise_type_error_exc typestr VariableTracker ctx_manager AutocastModeVariable ProfilerContextVariable TorchFunctionDisableVariable dicts ConstDictVariable distributed DistributedVariable ProcessGroupVariable functions bind_args_cached lists ListVariable TupleVariable torch_function can_dispatch_torch_function dispatch_torch_function TensorWithTFOverrideVariable TorchFunctionModeStackVariable try numpy np except ModuleNotFoundError np = None type ignore assignment try torch distributed fsdp _fully_shard _fsdp_param_group except ModuleNotFoundError _fsdp_param_group = None type ignore assignment TYPE_CHECKING torch _dynamo symbolic_convert InstructionTranslator log = logging getLogger __name__ supported_ctx_manager_classes = dict fromkeys torch profiler profiler profile torch autograd forward_ad _set_fwd_grad_enabled torch autograd forward_ad dual_level torch autograd profiler profile torch autograd profiler record_function torch _C DisableTorchFunctionSubclass torch _C DisableTorchFunction torch _functorch vmap vmap_increment_nesting torch _functorch eager_transforms grad_increment_nesting torch _functorch eager_transforms jvp_increment_nesting torch _functorch eager_transforms enable_inplace_requires_grad torch amp autocast_mode autocast torch autograd grad_mode enable_grad torch autograd grad_mode inference_mode torch autograd grad_mode no_grad torch autograd grad_mode set_grad_enabled torch autograd graph disable_saved_tensors_hooks torch cpu amp autocast_mode autocast torch cuda amp autocast_mode autocast torch fx traceback annotate torch fx traceback annotate __wrapped__ type ignore attr-defined We ll let Dynamo inline into contextlib part these context manager instances all way till invokes wrapped function itself which point we wrap back special context manager VTs This allows us support calling functions decorated these context managers without much extra effort code dup torch nn attention sdpa_kernel __wrapped__ type ignore attr-defined REWRITE_OPS_TO_TENSOR_SIZE_METHOD = dict fromkeys torch _shape_as_tensor constant_fold_functions_need_guards = torch accelerator current_device_index torch accelerator current_accelerator torch cuda current_device torch cuda is_initialized torch xpu current_device torch xpu is_initialized constant_fold_functions = torch _assert torch _utils _get_device_index torch _C _get_cublas_allow_tf torch _C _is_any_autocast_enabled torch accelerator is_available torch cuda get_device_properties torch cuda is_available torch distributed is_available torch get_autocast_dtype torch get_autocast_gpu_dtype torch get_default_dtype torch is_autocast_cache_enabled torch is_autocast_cpu_enabled torch is_autocast_enabled torch is_complex torch is_floating_point torch nn functional _Reduction get_enum type ignore attr-defined torch promote_types torch _C _get_privateuse _backend_name torch autograd _is_checkpoint_valid torch xpu get_device_properties torch xpu is_available + constant_fold_functions_need_guards torch distributed is_available constant_fold_functions extend torch distributed is_initialized torch distributed get_rank torch distributed get_world_size Convert dict O access times constant_fold_functions_need_guards = dict fromkeys constant_fold_functions_need_guards constant_fold_functions = dict fromkeys constant_fold_functions functools cache tracing_state_functions - dict Callable Any Optional bool Defined function avoid circular like torch onnx torch jit is_scripting False torch jit is_tracing False torch _C _get_tracing_state None torch fx _symbolic_trace is_fx_tracing False torch fx _symbolic_trace is_fx_symbolic_tracing False torch onnx is_in_onnx_export False torch _dynamo external_utils is_compiling True torch _utils is_compiling True torch compiler is_compiling True torch compiler is_dynamo_compiling True torch compiler is_exporting True Look into https github com pytorch pytorch pull why turned True Dynamo torch nn modules activation _is_make_fx_tracing True bin_ops = dict fromkeys add sub mul div sqrt dispatch_key_set_functions = torch _C _dispatch_keys torch _C _dispatch_tls_local_include_set torch _C _dispatch_tls_local_exclude_set functools cache get_overridable_functions itertools chain torch overrides get_overridable_functions get_overridable_functions_ funcs = set chain from_iterable get_overridable_functions_ values more set Callable Any = torch ones torch ones_like torch zeros torch zeros_like torch empty torch full funcs update more funcs BaseTorchVariable VariableTracker common base all torch functions classes modules other things classmethod create_with_source cls value source inspect isclass value install_guard source make_guard GuardBuilder CLASS_MATCH inspect ismodule value install_guard source make_guard GuardBuilder MODULE_MATCH inspect isfunction value install_guard source make_guard GuardBuilder CLOSURE_MATCH inspect isbuiltin value isinstance value torch _ops OpOverload torch _ops OpOverloadPacket install_guard source make_guard GuardBuilder BUILTIN_MATCH is_wrapper_or_member_descriptor value isinstance value torch _dynamo compiled_autograd Op Dont need guard wrappers pass install_guard source make_guard GuardBuilder FUNCTION_MATCH cls value source=source __init__ value kwargs - None super __init__ kwargs value = value reconstruct codegen PyCodegen try name = f value __module__ value __name__ except Exception name = f torch_obj_ id value unique_var_name = __ + re sub r ^a-zA-Z - _ + _ name codegen extend_output codegen setup_globally_cached unique_var_name value as_proxy value as_python_constant value call_obj_hasattr tx InstructionTranslator name result = hasattr value name variables ConstantVariable create result can_constant_fold_through value constant_fold_functions True value torch autograd _profiler_enabled config constant_fold_autograd_profiler_enabled The relevant flag enabled only export One might wonder why Actually we would like graph break even case Dynamo But there weird-unsolved bug Kineto + Dynamo when there distributed jobs lead NCCL timeouts This bug rare edege case we have been able root cause yet See https www internalfb com sevmanager view more details So safe export Yes export we do anticipate JIT tracing distributed job training weird edge-case interaction Kineto valid usecase So ok True getattr value __module__ None == math TorchCtxManagerClassVariable BaseTorchVariable Points context manager torch dynamo has implementations __repr__ - str f TorchCtxManagerClassVariable value staticmethod is_matching_cls value Unwrap s functools lru_cache wrapper value = unwrap_if_wrapper value We can t do isinstance value type check because some ctx managers implemented function decorated contextlib contextmanager E g torch _functorch vmap vmap_increment_nesting Context manager type function contextmanager callable callable value hashable value accesses value __hash__ value supported_ctx_manager_classes call_function tx InstructionTranslator args Sequence VariableTracker kwargs dict str VariableTracker - VariableTracker DisabledSavedTensorsHooksVariable DualLevelContextManager FSDPParamGroupUseTrainingStateVariable FxTracebackAnnotateVariable GradIncrementNestingCtxManagerVariable GradInplaceRequiresGradCtxManagerVariable GradModeVariable InferenceModeVariable JvpIncrementNestingCtxManagerVariable SDPAKernelVariable SetFwdGradEnabledContextManager StreamVariable VmapIncrementNestingCtxManagerVariable value torch no_grad len args == isinstance args variables functions BaseUserFunctionVariable ctx = GradModeVariable create tx False ctx call_function tx args kwargs GradModeVariable create tx False value torch enable_grad len args == isinstance args variables functions BaseUserFunctionVariable ctx = GradModeVariable create tx True ctx call_function tx args kwargs GradModeVariable create tx True value torch set_grad_enabled len args == GradModeVariable create tx args as_python_constant initialized=True value torch inference_mode assert len args = len kwargs == inf_mode = args as_python_constant len args == True InferenceModeVariable create tx inf_mode value torch fx traceback annotate torch fx traceback annotate __wrapped__ type ignore attr-defined assert len args = len kwargs == FxTracebackAnnotateVariable args as_python_constant source=self source inspect isclass value issubclass value torch Stream torch _dynamo variables builder wrap_fx_proxy_cls wrap_fx_proxy_cls StreamVariable tx tx output create_proxy call_function value value torch amp autocast_mode autocast torch cuda amp autocast torch cpu amp autocast AutocastModeVariable create value args kwargs value NOTE any added here must align semantic requirements ` ProfilerContextVariable ` torch profiler profile torch profiler record_function torch autograd profiler profile torch autograd profiler record_function warning_once log Profiler function s will ignored value ProfilerContextVariable value torch _C DisableTorchFunctionSubclass value torch _C DisableTorchFunction assert args kwargs TorchFunctionDisableVariable create tx only_subclass=self value torch _C DisableTorchFunctionSubclass value torch _functorch vmap vmap_increment_nesting assert len args == VmapIncrementNestingCtxManagerVariable create tx args value torch _functorch eager_transforms jvp_increment_nesting assert len args == JvpIncrementNestingCtxManagerVariable create tx value torch autograd forward_ad _set_fwd_grad_enabled assert len args == SetFwdGradEnabledContextManager create tx guard_if_dyn x x args value torch autograd forward_ad dual_level assert len args == DualLevelContextManager create tx value torch _functorch eager_transforms grad_increment_nesting assert len args == GradIncrementNestingCtxManagerVariable create tx value torch _functorch eager_transforms enable_inplace_requires_grad assert len args == GradInplaceRequiresGradCtxManagerVariable create tx guard_if_dyn x x args value torch autograd graph disable_saved_tensors_hooks assert len args == DisabledSavedTensorsHooksVariable create tx args as_python_constant _fsdp_param_group None value _fsdp_param_group FSDPParamGroup use_training_state assert len args == FSDPParamGroupUseTrainingStateVariable create tx args args as_python_constant value torch nn attention sdpa_kernel __wrapped__ type ignore attr-defined name_to_arg_map = bind_args_cached value tx source args kwargs backends = name_to_arg_map backends as_python_constant set_priority = name_to_arg_map set_priority as_python_constant SDPAKernelVariable create tx backends set_priority super call_function tx args kwargs TorchInGraphFunctionVariable BaseTorchVariable Points torch function method should put FX graph __init__ value nonstrict_traceable=None kwargs - None super __init__ value kwargs trace_rules is_nonstrict_trace_callable nonstrict_traceable None nonstrict_traceable = is_nonstrict_trace_callable value nonstrict_traceable = nonstrict_traceable __repr__ - str f TorchInGraphFunctionVariable value nonstrict_traceable= nonstrict_traceable get_function value staticmethod functools cache _get_handlers Build dict function - method handle so we O terms number function special handling handlers = register fns _register handler fn fns assert fn handlers fn handlers fn = handler handler assert callable fns _register torch backends cuda SDPAParams ConstantVariable DeterministicAlgorithmsVariable GradModeVariable StreamContextVariable SymNodeVariable TensorVariable UserDefinedObjectVariable builder wrap_fx_proxy wrap_fx_proxy_cls register tracing_state_functions handle_tracing_state_functions tx InstructionTranslator args kwargs assert args kwargs See https github com pytorch pytorch issues value torch _utils is_compiling torch _dynamo external_utils is_compiling torch compiler is_compiling torch compiler is_dynamo_compiling torch compiler is_exporting tx mark_inconsistent_side_effects ConstantVariable create tracing_state_functions value register dispatch_key_set_functions handle_dispatch_key_set_functions tx InstructionTranslator args kwargs assert kwargs value torch _C _dispatch_keys assert len args == assert isinstance args variables TensorVariable example_value = args proxy node meta example_value dks = value example_value Remove Python PythonTLSSnapshot dispatch key set they originate FakeTensor propagation This should only done example_value FakeTensor However tensor subclasses present reasonable Python remain dispatch key set isinstance example_value torch _subclasses FakeTensor dks = dks - torch _C DispatchKeySet torch _C DispatchKey Python - torch _C DispatchKeySet torch _C DispatchKey PythonTLSSnapshot DispatchKeySetVariable create dks assert args DispatchKeySetVariable create value register torch overrides get_default_nowrap_functions __wrapped__ handle_get_default_nowrap_functions tx InstructionTranslator args kwargs Note __torch_function__ we empty here because we restrict set functions we trace __torch_function__ functions outside actual set Implementing properly will require implementing some variable types track compare tensor getset descriptors VariableTracker build tx torch overrides get_default_nowrap_functions register torch ops inductor accumulate_grad_ default handle_accumulate_grad_ tx InstructionTranslator args kwargs tx inline_user_function_return VariableTracker build tx polyfills accumulate_grad args kwargs register math radians handle_radians tx InstructionTranslator args kwargs check_unspec_or_constant_args args kwargs Use polyfill convert math radians x into math pi x tx inline_user_function_return VariableTracker build tx polyfills radians args kwargs register torch is_inference_mode_enabled handle_is_inference_mode_enabled tx InstructionTranslator unimplemented_v gb_type= Encountered torch is_inference_mode_enabled during tracing context= explanation= torch is_inference_mode_enabled supported hints= graph_break_hints FUNDAMENTAL graph_break_hints INFERENCE_MODE register torch is_tensor torch overrides is_tensor_like handle_is_tensor tx InstructionTranslator arg isinstance arg TensorVariable value torch overrides is_tensor_like isinstance arg UserDefinedObjectVariable hasattr arg value __torch_function__ ConstantVariable create True ConstantVariable create False register torch is_floating_point torch is_complex handle_is_floating_point tx InstructionTranslator input input_arg = input isinstance input_arg TensorVariable input_arg dtype None value torch is_floating_point ConstantVariable create input_arg dtype is_floating_point value torch is_complex ConstantVariable create input_arg dtype is_complex raise AssertionError f calling value register torch numel handle_numel tx InstructionTranslator input isinstance input TensorVariable input valid_size ConstantVariable create product input size isinstance input TensorVariable Workaround dynamic shapes issue input call_method tx numel register torch compile handle_torch_compile tx InstructionTranslator args kwargs len args == torch compile no-op dynamo args unimplemented_v gb_type= torch compile call args context=f args= args kwargs= kwargs explanation= Attempted call ` torch compile ` args Dynamo does support hints= Remove torch compile call its additional args graph_break_hints SUPPORTABLE register REWRITE_OPS_TO_TENSOR_SIZE_METHOD handle_tensor_size_rewrites tx InstructionTranslator input assert isinstance input TensorVariable input call_method tx size register torch nn modules utils _single torch nn modules utils _pair torch nn modules utils _triple torch nn modules utils _quadruple torch nn modules utils _ntuple handle_ntuple tx InstructionTranslator args kwargs _call_ntuple tx args kwargs register torch is_grad_enabled handle_is_grad_enabled tx install_guard GradModeVariable _guards_singleton ConstantVariable create torch is_grad_enabled register torch use_deterministic_algorithms handle_use_deterministic_algorithms tx InstructionTranslator mode warn_only=False pyrefly ignore missing-attribute warn_only warn_only as_python_constant unimplemented_v gb_type= Attempted use torch use_deterministic_algorithms warn_only=True context=f mode= mode warn_only= warn_only explanation= Dynamo does support hints= Remove param warn_only function call torch use_deterministic_algorithms graph_break_hints SUPPORTABLE DeterministicAlgorithmsVariable create tx mode as_python_constant register torch are_deterministic_algorithms_enabled handle_are_deterministic_algorithms_enabled tx install_guard DeterministicAlgorithmsVariable _guards_singleton ConstantVariable create torch are_deterministic_algorithms_enabled register torch _C _is_torch_function_enabled handle_is_torch_function_enabled tx install_guard TorchFunctionDisableVariable _guards_singleton see comment SymbolicTorchFunctionState why bug ConstantVariable create tx symbolic_torch_function_state torch_function_subclass_enabled register torch _C _is_torch_function_all_disabled handle_is_torch_function_all_disabled tx install_guard TorchFunctionDisableVariable _guards_singleton ConstantVariable create tx symbolic_torch_function_state torch_function_mode_enabled register torch overrides has_torch_function torch overrides has_torch_function_variadic torch overrides has_torch_function_unary handle_has_torch_function tx InstructionTranslator args elems = args unpack_var_sequence tx len args == isinstance args TupleVariable args ConstantVariable create any has_torch_function x x elems register dict fromkeys remove duplicates device_interface stream _ device_interface get_registered_device_interfaces handle_device_interface_stream tx InstructionTranslator stream StreamContextVariable create tx stream register torch from_numpy handle_from_numpy tx InstructionTranslator args config trace_numpy unimplemented_v gb_type= call ` torch from_numpy ` ` torch _dynamo config trace_numpy=False ` context=f trace_numpy= config trace_numpy explanation= Attempted call ` torch from_numpy ` config ` torch _dynamo config trace_numpy ` set ` False ` hints= Change ` torch _dynamo config trace_numpy ` ` True ` np unimplemented_v gb_type= ` torch from_numpy ` NumPy unavailable context= explanation= Attempted call ` torch numpy ` NumPy could imported hints= Check NumPy version installation your environment graph_break_hints USER_ERROR wrap_fx_proxy_cls target_cls=TensorVariable tx=tx proxy=tx output create_proxy call_function torch as_tensor proxy_args_kwargs args example_value=None register torch jit annotate handle_jit_annotate tx InstructionTranslator the_type the_value the_value register torch backends cudnn is_acceptable handle_cudnn_is_acceptable tx InstructionTranslator tensor extra is_acceptable tensor returns true tensor dtype device supported cudnn b cudnn available c some initialization has completed technically depends some global state c torch backends cudnn __cudnn_version assert extra Expect input cudnn is_acceptable assert isinstance tensor TensorVariable Expect input cudnn is_acceptable tensor tensor_inp = torch tensor dtype=tensor dtype device=tensor device ConstantVariable create torch backends cudnn is_acceptable tensor_inp register torch utils hooks BackwardHook handle_backward_hook tx InstructionTranslator args kwargs variables BackwardHookVariable create tx args kwargs register torch nn Parameter handle_parameter tx InstructionTranslator args kwargs call_nn_parameter tx args kwargs register torch ops aten sym_size torch ops aten sym_size int handle_sym_size self_ tx dim=None we see when retracing already traced code dim None call_method tx size dim register torch ops aten sym_stride torch ops aten sym_stride int handle_sym_stride self_ tx dim=None dim None call_method tx stride dim register torch addcdiv handle_addcdiv tx InstructionTranslator args kwargs len args == value kwargs len kwargs == decompose addcdiv into constituent ops prevents graph break due converting value scalar result = TorchInGraphFunctionVariable torch div call_function tx args result = TorchInGraphFunctionVariable torch mul call_function tx result kwargs value TorchInGraphFunctionVariable torch add call_function tx args result register torch full handle_full tx size fill_value kwargs isinstance fill_value TensorVariable result = TorchInGraphFunctionVariable torch ops aten _local_scalar_dense call_function tx fill_value TorchInGraphFunctionVariable torch full call_function tx size result kwargs register torch _foreach_lerp_ handle_inplace_foreach_lerp_scalar _ tx InstructionTranslator args kwargs len args == isinstance args ListVariable kwargs tx inline_user_function_return VariableTracker build tx polyfills foreach_lerp_inplace args kwargs register torch _foreach_pow handle_foreach_pow_scalar _ tx InstructionTranslator args kwargs In eager s more performant call item within C op implementation compile s more performant graph break len args == isinstance args TensorVariable kwargs tx inline_user_function_return VariableTracker build tx polyfills foreach_pow_scalar args kwargs register torch _assert handle_assert tx InstructionTranslator condition message condition is_python_constant condition as_python_constant isinstance condition variables SymNodeVariable condition evaluate_expr ConstantVariable None register SDPAParams handle_sdpa_params tx InstructionTranslator args kwargs wrap_fx_proxy tx proxy=tx output create_proxy call_function torch _C _SDPAParams proxy_args_kwargs args kwargs param_vars=args DistributedVariable is_available torch distributed distributed_c d _get_group_size_by_name _get_group_tag _rank_not_in_group _resolve_group_name_by_ranks_and_tag get_process_group_ranks torch distributed tensor DTensor register _get_group_size_by_name _get_group_tag _rank_not_in_group get_process_group_ranks _resolve_group_name_by_ranks_and_tag handle_constant_processgroup_functions tx InstructionTranslator args because input ProcessGroupVariable we ll guarding its ID_MATCH based how constructed We desugar trace-time into ranks directly calling util bake result into trace len args == group group name assert isinstance args ProcessGroupVariable ConstantVariable len args == ranks + tag assert isinstance args ListVariable isinstance args ConstantVariable raise AssertionError f Invalid group value args constant pg f function value args_as_value = arg as_python_constant arg args invocation_result = value args_as_value Note - while we could cook up sources around invocations like FunctionSource space invoking functions middle guard chain very iffy As such guard propagation via options best we can do VariableTracker build tx invocation_result register DTensor from_local handle_from_local tx InstructionTranslator args kwargs rewrite non-primitive args kwargs included on-the-fly prim function rewrite args have only proxyable args then insert call_function args_as_value = x as_python_constant x args kwargs_as_value = k v as_python_constant k v kwargs items k shape stride kwargs_to_be_proxied = k kwargs k k shape stride k kwargs fn_with_prim_types x shape=None stride=None value x args_as_value kwargs_as_value shape=shape stride=stride attach same function name better debugging fn_with_prim_types __name__ = prim + value __name__ wrap_fx_proxy tx=tx proxy=tx output create_proxy call_function fn_with_prim_types proxy_args_kwargs args kwargs_to_be_proxied register torch nested nested_tensor handle_nested_tensor tx InstructionTranslator tensor_list=None args layout=None kwargs lists BaseListVariable layout layout as_python_constant == torch strided unimplemented_v gb_type= Attempted use strided NestedTensor context=f layout= layout explanation= Dynamo does support hints= Change layout=torch jagged graph_break_hints SUPPORTABLE isinstance tensor_list BaseListVariable unimplemented_v gb_type= Attempted use ` nested_tensor ` non-list input context=f tensor_list= tensor_list explanation= Dynamo does support hints= Change ` nested_tensor ` list input graph_break_hints USER_ERROR register torch nn functional one_hot handle_one_hot tx InstructionTranslator args kwargs len args + len kwargs == len args == args is_python_constant args as_python_constant == - unimplemented_v gb_type= Attempted use ` torch nn functional one_hot ` data-dependent output shape context=f args= args kwargs= kwargs explanation= Dynamo does support hints= Explicitly set ` num_classes ` param function call ` torch nn functional one_hot ` something other than - register torch fx experimental symbolic_shapes guard_size_oblivious handle_guard_size_oblivious tx InstructionTranslator expr isinstance expr SymNodeVariable TODO probably should folded somewhere I m sure where TODO some other symbolic_shapes special tools can also get treatment too variables ConstantVariable create torch fx experimental symbolic_shapes guard_size_oblivious expr sym_num isinstance expr ConstantVariable expr register torch fx experimental symbolic_shapes guard_or_true handle_guard_or_true tx InstructionTranslator expr isinstance expr SymNodeVariable TODO probably should folded somewhere I m sure where TODO some other symbolic_shapes special tools can also get treatment too variables ConstantVariable create torch fx experimental symbolic_shapes guard_or_true expr sym_num isinstance expr ConstantVariable expr register torch fx experimental symbolic_shapes guard_or_false handle_guard_or_false tx InstructionTranslator expr isinstance expr SymNodeVariable TODO probably should folded somewhere I m sure where TODO some other symbolic_shapes special tools can also get treatment too variables ConstantVariable create torch fx experimental symbolic_shapes guard_or_false expr sym_num isinstance expr ConstantVariable expr register torch fx experimental symbolic_shapes statically_known_false handle_statically_known_false tx InstructionTranslator expr isinstance expr SymNodeVariable variables ConstantVariable create torch fx experimental symbolic_shapes statically_known_false expr sym_num isinstance expr ConstantVariable expr register torch fx experimental symbolic_shapes guard_scalar guard_scalar tx InstructionTranslator expr isinstance expr SymNodeVariable val = expr sym_num isinstance expr ConstantVariable val = expr value raise torch _dynamo exc Unsupported branch supported variables ConstantVariable create pyrefly ignore bad-argument-type torch fx experimental symbolic_shapes guard_scalar val register torch fx experimental symbolic_shapes statically_known_true handle_statically_known_true tx InstructionTranslator expr isinstance expr SymNodeVariable variables ConstantVariable create torch fx experimental symbolic_shapes statically_known_true expr sym_num isinstance expr ConstantVariable expr register torch fx experimental symbolic_shapes sym_and handle_sym_and tx InstructionTranslator terms all isinstance x SymNodeVariable x terms SymNodeVariable create tx torch fx experimental symbolic_shapes sym_and x as_proxy x terms sym_num=None register torch fx experimental symbolic_shapes sym_or handle_sym_or tx InstructionTranslator terms all isinstance x SymNodeVariable x terms SymNodeVariable create tx torch fx experimental symbolic_shapes sym_or x as_proxy x terms sym_num=None register torch fx experimental symbolic_shapes has_static_value handle_has_static_value tx InstructionTranslator expr isinstance expr SymNodeVariable val = expr sym_num isinstance expr ConstantVariable val = expr value variables ConstantVariable create pyrefly ignore bad-argument-type torch fx experimental symbolic_shapes has_static_value val register torch _C _autograd _unsafe_set_version_counter handle_unsafe_set_version_counter tx InstructionTranslator args kwargs tensor_version_op _unsafe_set_version_counter TorchInGraphFunctionVariable _unsafe_set_version_counter call_function tx args kwargs register torch _C _functorch peek_interpreter_stack handle_functorch_peek_interpreter_stack tx InstructionTranslator args kwargs Wrap C++ interpreter torch _C _functorch CInterpreter UserDefinedObjectVariable Python interpreter torch _functorch pyfunctorch FuncTorchInterpreter FuncTorchInterpreterVariable UserDefinedObjectVariable torch _C _functorch peek_interpreter_stack register torch _functorch pyfunctorch coerce_cinterpreter handle_functorch_pyfunctorch_coerce_cinterpreter tx InstructionTranslator args kwargs cinterpreter = args value FuncTorchInterpreterVariable torch _functorch pyfunctorch coerce_cinterpreter cinterpreter register torch tensor handle_torch_tensor tx InstructionTranslator args kwargs check_any_unspec x NB This includes UnspecializedPythonVariable isinstance x TensorVariable SymNodeVariable True isinstance x ListVariable TupleVariable any check_any_unspec y y x items TODO there maybe other recursive structures you need check False data_arg = None args data_arg = args data kwargs data_arg = kwargs data NB OK pass torch tensor tensor will trace fine isinstance data_arg TensorVariable check_any_unspec data_arg This slower less canonical so only use we have TorchInGraphFunctionVariable torch _refs tensor call_function tx args kwargs register torch _C _pop_torch_function_stack handle_pop_torch_function tx InstructionTranslator args kwargs assert args kwargs tx symbolic_torch_function_state mode_stack unimplemented_v gb_type= Attempted pop empty torch function mode stack context= explanation= Called ` torch _C _pop_torch_function_stack ` when torch function mode stack empty hints= Do pop empty torch function mode stack graph_break_hints USER_ERROR TorchFunctionModeStackVariable register_mutation tx tx symbolic_torch_function_state pop_torch_function_mode register torch _C _push_on_torch_function_stack handle_push_torch_function tx InstructionTranslator args kwargs len args = kwargs raise_type_error_exc tx f push_torch_function takes exactly one argument len args given TorchFunctionModeStackVariable register_mutation tx tx symbolic_torch_function_state push_torch_function_mode args ConstantVariable create None register torch _C _len_torch_function_stack handle_len_torch_function tx InstructionTranslator args kwargs args kwargs raise_type_error_exc tx len_torch_function_stack takes no arguments ConstantVariable create len tx symbolic_torch_function_state mode_stack register torch _C _get_function_stack_at handle_get_stack_at tx InstructionTranslator args kwargs len args = kwargs raise_type_error_exc tx f get_function_stack_at takes exactly one argument len args given ind = args as_python_constant assert ind = ind len tx symbolic_torch_function_state mode_stack tx symbolic_torch_function_state mode_stack ind register torch get_device_module __wrapped__ handle_get_device_module tx args kwargs len args + len kwargs kwargs device kwargs unimplemented_v gb_type= improper torch get_device_module arguments context=f args= args kwargs= kwargs explanation= torch get_device_module accepts optional argument ` device ` hints= graph_break_hints USER_ERROR try kwargs device = kwargs device as_python_constant args device = args as_python_constant device = None module = torch get_device_module device except Exception e unimplemented_v gb_type= bad device argument torch get_device_module context=f args= args kwargs= kwargs explanation= Expected valid string torch device argument cpu cuda etc hints= graph_break_hints USER_ERROR from_exc=e need guard only no-arg get_device_module pyrefly ignore unbound-name device None source = CallFunctionNoArgsSource source install_guard source make_guard GuardBuilder ID_MATCH assumes ` module ` form ` torch xyz ` new_source = AttrSource TorchSource pyrefly ignore unbound-name module __name__ rsplit maxsplit= - pyrefly ignore unbound-name VariableTracker build tx module new_source register torch accelerator current_stream handle_current_stream tx InstructionTranslator args kwargs len args + len kwargs kwargs device kwargs unimplemented_v gb_type= unsupported arguments torch accelerator current_stream context=f args= args kwargs= kwargs explanation= torch accelerator current_stream accepts one optional argument ` device ` hints= graph_break_hints USER_ERROR try kwargs device = torch device kwargs device as_python_constant args device = torch device args as_python_constant device = None tx symbolic_stream_state cur_stream device except Exception e unimplemented_v gb_type= bad device argument torch accelerator current_stream context=f args= args kwargs= kwargs explanation= Expected valid string torch device argument cpu cuda etc hints= graph_break_hints USER_ERROR from_exc=e register torch set_default_device handle_set_default_device tx InstructionTranslator args kwargs Today inserted graph once TF mode handling complete we can trace device context like any other TF mode remove special handling Insert TF mode representing device context bottom stack match eager semantics Running graph will ensure DeviceContext mode correct position stack TorchFunctionModeStackVariable register_mutation tx args is_python_constant args as_python_constant None TorchFunctionModeStackVariable clear_default_device tx TorchFunctionModeStackVariable register_device_context_insertion tx ConstantVariable create None handlers call_function tx InstructionTranslator args Sequence VariableTracker kwargs dict str VariableTracker - VariableTracker ConstantVariable SymNodeVariable TensorVariable builder wrap_fx_proxy nonstrict_traceable torch _higher_order_ops flat_apply flat_apply torch _higher_order_ops flat_apply func_to_graphable is_graphable_type torch _subclasses fake_tensor fake_tensor_tls torch utils _pytree tree_flatten base AsPythonConstantNotImplementedError Convert ` args kwargs ` into pytree-flattened proxy forms Rather than reconstructing ` args kwargs ` into python objects then tree_flatten them we just let Dynamo symbolically interpret ` tree_flatten args kwargs ` This saves us having worry about reconstruction logic side effects guards packed_input_vt = TupleVariable build tx TupleVariable build tx args ConstDictVariable build tx kwargs out_vt = variables UserFunctionVariable tree_flatten call_function tx packed_input_vt assert isinstance out_vt TupleVariable len out_vt items == flat_args_vts input_spec_vt = out_vt items assert isinstance flat_args_vts ListVariable Handle case when input contains non-graphable type flat_arg_vt flat_args_vts items arg_type = flat_arg_vt python_type is_graphable_type arg_type type_name = flat_arg_vt python_type __qualname__ unimplemented_v gb_type= Invalid input type nonstrict_trace-ed function context=f Encountered input type type_name explanation= For ` nonstrict_trace ` -ed functions only basic types e g torch Tensor int float pytree containers those allowed inputs The provided argument contains unsupported type hints= Use one following register type pytree \n ` torch utils _pytree register_constant ` \n ` torch utils _pytree register_dataclass ` \n ` torch utils _pytree register_pytree_node ` Since we checked ` is_graphable ` above ` as_proxy ` flat_arg VT should always work proxified_flat_args = flat_arg_vt as_proxy flat_arg_vt flat_args_vts items The downstream ` flat_apply ` call requires input spec however spec graphable type so we still have reconstruct into python object store constant attribute fx graph try input_spec = input_spec_vt as_python_constant except AsPythonConstantNotImplementedError e typ = e vt python_type type_name = typ __qualname__ torch utils _pytree pytree pytree is_constant_class typ unimplemented_v gb_type= Input marked ` pytree register_constant ` constructed ` torch compile ` region context=f Input= input_spec_vt offending type type_name explanation= Calling ` nonstrict_trace ` -ed function input contains object f type type_name which marked ` pytree register_constant ` However object constructed _inside_ ` torch compile ` region This supported hints= Construct object _outside_ ` torch compile ` region submit issue GitHub graph_break_hints SUPPORTABLE from_exc=e unimplemented_v gb_type= Invalid use pytree_flatten nonstrict_trace-ed function context=f Input= input_spec_vt offending type type_name explanation= Calling ` nonstrict_trace ` -ed function where one inputs has been registered f ` pytree_flatten ` places object type type_name into context hints= Modifying ` pytree_flatten ` avoid placing object into context f Apply one following type_name \n ` torch utils _pytree register_constant ` \n ` torch utils _pytree register_dataclass ` \n ` torch utils _pytree register_pytree_node ` graph_break_hints SUPPORTABLE from_exc=e fn = value patched_fn args kwargs This enables reads global captured tensors we ll just treat them constants graph Note after AOTDispatcher logic would disappear old_val = fake_tensor_tls allow_non_fake_inputs_override fake_tensor_tls allow_non_fake_inputs_override = True try res = fn args kwargs finally reset even when ` fn ` raises fake_tensor_tls allow_non_fake_inputs_override = old_val res ` flat_apply ` wants TreeSpec function input _ f_spec = func_to_graphable patched_fn TreeSpec isn t graphable so we register function input specs attributes graph module f_spec_proxy = tx output register_static_attr_and_return_proxy f fn __name__ _spec f_spec input_spec_proxy = tx output register_static_attr_and_return_proxy fn __name__ + _input_spec pyrefly ignore unbound-name input_spec f_spec_proxy node type = type f_spec pyrefly ignore unbound-name input_spec_proxy node type = type input_spec all_args = f_spec_proxy input_spec_proxy proxified_flat_args Create proxy call ` flat_apply ` then fake-tensor propagate call wrap output into VariableTracker proxy = tx output create_proxy call_function flat_apply all_args try TODO support more output types once ` flat_apply ` supports pytree-able output types We can have Dynamo trace through unflatten call just like we traced through flatten above rebuild actual output VT out_vt = wrap_fx_proxy tx proxy except From ` handle_traced_output ` torch _dynamo exc Unsupported From ` flat_apply ` assert output type torch _dynamo exc TorchRuntimeError unimplemented_v gb_type= Unsupported output type nonstrict_trace-ed function context=f Function fn __name__ explanation= For ` nonstrict_trace ` -ed functions only basic types e g torch Tensor int list allowed output The result call contains unsupported type hints= graph_break_hints SUPPORTABLE out_vt torch_function_override_enabled tx args kwargs dispatch_torch_function tx args kwargs can_constant_fold_through check_unspec_or_constant_args args kwargs constant fold functions need guarded value constant_fold_functions_need_guards assert source None source = CallFunctionNoArgsSource source install_guard source make_guard GuardBuilder EQUALS_MATCH constant fold try ConstantVariable create as_python_constant x as_python_constant x args k v as_python_constant k v kwargs items except OverflowError TypeError ValueError exc raise_observed_exception type exc tx args=list map ConstantVariable create exc args is_tensor_method name = value __name__ Guard against inplace view op input tensor supported args isinstance args variables TensorVariable tensor_var = args Check input tensor inplace_view op specifically tensor_var source None hasattr torch ops aten name fn = getattr torch ops aten name hasattr fn overloads hasattr fn fn overloads torch Tag inplace_view getattr fn fn overloads tags unimplemented_v gb_type= Inplace op input tensor context= explanation=f Attempted trace inplace view op input tensor typestr value hints= graph_break_hints SUPPORTABLE Ensure you do modify input tensor place call_tensor_method tx args kwargs special_handler = _get_handlers get value special_handler result = special_handler tx args kwargs result result any_symints_or_symfloats = any isinstance x SymNodeVariable x args all_ints_or_floats = all isinstance x variables ConstantVariable variables SymNodeVariable x args getattr value __module__ == torch value __name__ bin_ops any_symints_or_symfloats all_ints_or_floats msg = f \ Calling str value only torch SymInt arguments yet supported To support behavior we need allow const-propping tensors store symint data For now dynamo will explicitly graph break when encounters user code behavior log warning msg unimplemented_v gb_type= Attempted call torch in-graph function only torch SymInt arguments context=f fn= value args= args kwargs= kwargs explanation= f Attempted call str value should put FX graph only torch SymInt arguments Dynamo does support hints= graph_break_hints SUPPORTABLE TODO voz Replace w dynamic shape rewrite table Ideally we would able do ctor time alas we need combination value + args determine fn_ = value any_symints_or_symfloats torch_sym_op = f _sym_ value __name__ getattr value __module__ None == math hasattr torch torch_sym_op fn_ = getattr torch torch_sym_op TODO each following check ` out= ` ` requires_grad= ` variant torch ops original function could come user defined ` allow_in_graph ` function well which doesn t have same semantics torch ops Calling fake tensor propagation can mutate out= tensor tx output tracked_fakes tracked_fakes used apply symbolic_shape guards Mutating them destroys information prior tracing which essential creating right guards So save shape now check later has changed If has graph break saved_out_shapes = None out_kwarg_vt = None out kwargs out_kwarg_vt = kwargs out e g out= t t isinstance out_kwarg_vt TupleVariable ListVariable saved_out_shapes = vt out_kwarg_vt items isinstance vt variables TensorVariable shape = vt proxy node meta example_value shape shape = None saved_out_shapes append shape e g out=output_tensor isinstance out_kwarg_vt variables TensorVariable saved_out_shapes = out_kwarg_vt proxy node meta example_value shape tensor_variable = wrap_fx_proxy tx=tx proxy=tx output create_proxy call_function fn_ proxy_args_kwargs args kwargs Handle e g ` torch ones requires_grad=True ` isinstance tensor_variable TensorVariable requires_grad kwargs kwargs requires_grad as_python_constant unimplemented_v gb_type= Attempted use tensor creation function requires_grad=True context=f fn= value args= args kwargs= kwargs explanation= Dynamo does support hints= Create tensor outside compiled region Do set ` requires_grad=True ` graph_break_hints SUPPORTABLE Handle e g ` torch add b out=result ` saved_out_shapes None out variants torch operators like torch sort torch sigmoid mutate tensors out field However s non-trivial update all references old ` TensorVariable ` new one returned ` result_var ` so we take conservative approach graph break size changes assume other cases can fall through soundly Note although these tensor variables would hold different proxies in-place mutation semantics preserved FX graph so we won t have correctness issues isinstance saved_out_shapes list out_tensor_vt saved_out_shape zip out_kwarg_vt items type ignore union-attr saved_out_shapes saved_out_shape None This should extremely rare s kept now until we invest enforcing ` out= ` kwarg only torch methods continue assert isinstance out_tensor_vt TensorVariable fake_out = out_tensor_vt proxy node meta example_value saved_out_shape = fake_out shape It s hard get out variants resizing graph inputs work properly across dynamo aot inductor just fall back unimplemented_v gb_type= Shape mismatch out= list tensor variants context=f fn= value args= args kwargs= kwargs explanation= f Shape mismatch when calling value ` out= ` f Provided ` out= ` shape saved_out_shape Actual shape fake_out shape hints= graph_break_hints SUPPORTABLE torch _prims_common is_contiguous fake_out It s difficult handle strides correctly functionalization when calling out= op non-contiguous out argument unimplemented_v gb_type= Attempted call op non-contiguous ` out= ` list tensors context=f value= value args= args kwargs= kwargs explanation= Dynamo does support hints= graph_break_hints SUPPORTABLE assert isinstance out_kwarg_vt TensorVariable assert example_value out_kwarg_vt proxy node meta fake_out = out_kwarg_vt proxy node meta example_value saved_out_shapes = fake_out shape It s hard get out variants resizing graph inputs work properly across dynamo aot inductor just fall back unimplemented_v gb_type= Shape mismatch out= tensor variant context=f fn= value args= args kwargs= kwargs explanation= f Shape mismatch when calling value ` out= ` f Provided ` out= ` shape saved_out_shapes Actual shape fake_out shape hints= graph_break_hints SUPPORTABLE torch _prims_common is_contiguous fake_out It s difficult handle strides correctly functionalization when calling out= op non-contiguous out argument unimplemented_v gb_type= Attempted call op non-contiguous ` out= ` tensor context=f value= value args= args kwargs= kwargs explanation= Dynamo does support hints= graph_break_hints SUPPORTABLE tensor_variable _call_ntuple tx InstructionTranslator args kwargs inline behavior torch nn modules utils _ntuple value torch nn modules utils _ntuple count = args as_python_constant count = value __closure__ cell_contents assert isinstance count int assert kwargs handle_ntuple value value has_unpack_var_sequence tx variables TupleVariable list value unpack_var_sequence tx value is_python_constant constant prop through variables ConstantVariable create torch nn modules utils _ntuple count value as_python_constant unimplemented_v gb_type= Attempted use ` torch nn modules utils _ntuple ` unsupported argument type context=f value= value explanation= Dynamo does support hints= Change use _ntuple argument constant tensor value torch nn modules utils _ntuple variables LambdaVariable handle_ntuple handle_ntuple args classmethod call_nn_parameter cls tx data=None requires_grad=True A call torch nn Parameter gets lifted before graph tx export unimplemented_v gb_type= Attempted use ` torch nn Parameter ` export context= explanation= Dynamo does support hints= Do use ` torch nn Parameter ` export graph_break_hints SUPPORTABLE isinstance requires_grad variables VariableTracker try requires_grad = requires_grad as_python_constant except NotImplementedError unimplemented_v gb_type= non-constant ` requires_grad ` argument ` torch nn Parameter ` context=f requires_grad= requires_grad explanation= Dynamo does support hints= Change ` requires_grad ` bool graph_break_hints USER_ERROR isinstance data variables TensorVariable unimplemented_v gb_type= ` torch nn Parameter ` unsupported data type context=f data= data explanation= Called ` torch nn Parameter ` non-Tensor argument hints= Ensure argument ` torch nn Parameter ` ` torch Tensor ` graph_break_hints USER_ERROR results cleaner graphs only works inputs pyrefly ignore missing-attribute data source cls _nn_param_via_prefix_insert tx data requires_grad config graph_break_on_nn_param_ctor Need user manually move since we cannot unimplemented_v gb_type= Attempted use ` torch nn Parameter ` constructor Dynamo context= explanation= Dynamo does support hints= Try construct ` torch nn Parameter ` outside compiled region If possible turn ` graph_break_on_nn_param_ctor ` off graph_break_hints SUPPORTABLE TODO lucaskabela Remove behavior below since deprecated isinstance data TensorWithTFOverrideVariable pyrefly ignore missing-attribute is_traceable_wrapper_subclass_type data class_type unimplemented_v gb_type= Attempted use torch nn Parameter constructor tensor subclass context=str data explanation= Dynamo does support hints= graph_break_hints SUPPORTABLE can_convert_to_tracable_parameter unimplemented_v gb_type= ` torch nn Parameter ` cannot convert traceable tracable context= explanation= convert_tracable_parameter set False hints= Check usage context manager do_not_convert_to_tracable_parameter graph_break_hints DIFFICULT try pyrefly ignore missing-attribute shape = tuple data var_getattr tx shape as_python_constant pyrefly ignore missing-attribute dtype = data var_getattr tx dtype as_python_constant pyrefly ignore missing-attribute device = data var_getattr tx device as_python_constant except NotImplementedError e unimplemented_v gb_type= ` torch nn Parameter ` non-constant Tensor attributes context=f data= data explanation= Dynamo does support hints= Ensure Tensor argument s shape dtype device correct graph_break_hints USER_ERROR from_exc=e placeholder = tx output synthetic_graph_input new_parameter_placeholder pyrefly ignore unbound-name shape dtype device requires_grad pyrefly ignore missing-attribute data requires_grad pyrefly ignore missing-attribute data = data call_method tx detach builder wrap_fx_proxy result = wrap_fx_proxy tx tx output create_proxy call_function tracable_create_parameter pyrefly ignore missing-attribute data as_proxy placeholder as_proxy In reconstruct we should use original parameter The one returned graph will alias source=placeholder source assert isinstance result variables TensorVariable result class_type = torch nn Parameter TODO jansel bdhirsh - There some issue tracable_create_parameter It does seem use right grad_enabled Since parameter we can just override has_grad_fn field False workaround issue result has_grad_fn = False TODO jansel new param falls out scope currently won t get freed until end graph We should fix result staticmethod _nn_param_via_prefix_insert tx InstructionTranslator data requires_grad Alternate version we have source varname = tx output new_var construct nn Parameter before graph save varname assert tx output root_tx None cg = PyCodegen tx output root_tx cg add_push_null lambda cg load_import_from torch nn Parameter cg data source cg variables ConstantVariable requires_grad cg call_function False cg store varname tx output pregraph_bytecode extend cg get_instructions data_node = data as_proxy node data_node op placeholder get_attr unimplemented_v gb_type= Unexpected type data placeholder op parameter construction context=f data_node op= data_node op explanation= Data node op should placeholder get_attr hints= graph_break_hints DIFFICULT add newly constructed nn Parameter graph input source = SyntheticLocalSource varname example_value = torch nn Parameter tx output example_value_from_input_node data as_proxy node requires_grad=requires_grad result = VariableTracker build tx example_value source Realize VT because we will delete guards next line result = result realize No need guard since we already guarded ` data ` These guards would fail since varname doesn t exist until after function starts TracingContext get guards_context dynamo_guards remove_guards_with_source source result call_tensor_method tx args kwargs args call_method tx get_function __name__ args kwargs is_tensor_method trace_rules get_tensor_method inspect ismethoddescriptor get_function hasattr get_function __objclass__ get_function __objclass__ == torch _C TensorBase get_function get_tensor_method torch_function_override_enabled tx args kwargs get_function get_overridable_functions isinstance get_function torch _ops OpOverload torch _ops OpOverloadPacket can_dispatch_torch_function tx args kwargs DispatchKeySetVariable BaseTorchVariable represents torch DispatchKeySet staticmethod create value kwargs DispatchKeySetVariable value kwargs classmethod create_with_source cls value source install_guard source make_guard GuardBuilder DISPATCH_KEY_SET_MATCH cls value source=source is_constant_fold_method name name == has call_method tx name args list VariableTracker kwargs dict str VariableTracker - VariableTracker is_constant_fold_method name check_unspec_or_constant_args args kwargs method = getattr value name variables ConstantVariable create method x as_python_constant x args k v as_python_constant k v kwargs items name == highestPriorityTypeId variables EnumVariable value highestPriorityTypeId super call_method tx name args kwargs FuncTorchInterpreterVariable BaseTorchVariable represents torch _functorch pyfunctorch FuncTorchInterpreter classmethod create_with_source cls value source install_guard source make_guard GuardBuilder ID_MATCH cls value source=source call_method tx name args list VariableTracker kwargs dict str VariableTracker - VariableTracker name == key variables EnumVariable value key name == process tx inline_user_function_return VariableTracker build tx value process __func__ + args kwargs name level batch_size randomness variables ConstantVariable create getattr value name name == lower assert args kwargs variables TemporarilyPopInterpreterStackCtxManagerVariable create tx None super call_method tx name args kwargs