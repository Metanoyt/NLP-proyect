flake noqa E C B dataclasses dataclass typing Optional torch torch nn nn torch Tensor torch nn functional F find_multiple n int k int - int n k == n n + k - n k dataclass ModelArgs block_size int = vocab_size int = n_layer int = n_head int = dim int = intermediate_size int = None n_local_heads int = - head_dim int = rope_base float = norm_eps float = e- num_experts int = num_activated_experts int = __post_init__ n_local_heads == - n_local_heads = n_head intermediate_size None hidden_dim = dim n_hidden = int hidden_dim intermediate_size = find_multiple n_hidden head_dim = dim n_head classmethod from_name cls name str name transformer_configs cls transformer_configs name fuzzy search config = config config transformer_configs config str name upper config str name assert len config == name cls transformer_configs config transformer_configs = Mixtral- x B-v dict block_size= n_layer= n_head= n_local_heads= dim= intermediate_size= rope_base= num_experts= num_activated_experts= KVCache nn Module __init__ max_batch_size max_seq_length n_heads head_dim dtype=torch bfloat super __init__ cache_shape = max_batch_size n_heads max_seq_length head_dim register_buffer k_cache torch zeros cache_shape dtype=dtype register_buffer v_cache torch zeros cache_shape dtype=dtype update input_pos k_val v_val input_pos S k_val B H S D assert input_pos shape == k_val shape k_out = k_cache v_out = v_cache k_out input_pos = k_val v_out input_pos = v_val k_out v_out Transformer nn Module __init__ config ModelArgs - None super __init__ config = config tok_embeddings = nn Embedding config vocab_size config dim layers = nn ModuleList TransformerBlock config _ range config n_layer norm = RMSNorm config dim eps=config norm_eps output = nn Linear config dim config vocab_size bias=False freqs_cis Optional Tensor = None mask_cache Optional Tensor = None max_batch_size = - max_seq_length = - setup_caches max_batch_size max_seq_length max_seq_length = max_seq_length max_batch_size = max_batch_size head_dim = config dim config n_head max_seq_length = find_multiple max_seq_length max_seq_length = max_seq_length max_batch_size = max_batch_size b layers b attention kv_cache = KVCache max_batch_size max_seq_length config n_local_heads head_dim freqs_cis = precompute_freqs_cis config block_size config dim config n_head config rope_base causal_mask = torch tril torch ones max_seq_length max_seq_length dtype=torch bool forward idx Tensor input_pos Optional Tensor = None - Tensor assert freqs_cis None Caches must initialized first mask = causal_mask None None input_pos freqs_cis = freqs_cis input_pos x = tok_embeddings idx i layer enumerate layers x = layer x input_pos freqs_cis mask x = norm x logits = output x logits classmethod from_name cls name str cls ModelArgs from_name name TransformerBlock nn Module __init__ config ModelArgs - None super __init__ attention = Attention config block_sparse_moe = MOEFeedForward config ffn_norm = RMSNorm config dim config norm_eps attention_norm = RMSNorm config dim config norm_eps forward x Tensor input_pos Tensor freqs_cis Tensor mask Tensor - Tensor h = x + attention attention_norm x freqs_cis mask input_pos out = h + block_sparse_moe ffn_norm h out Attention nn Module __init__ config ModelArgs super __init__ assert config dim config n_head == total_head_dim = config n_head + config n_local_heads config head_dim key query value projections all heads batch wqkv = nn Linear config dim total_head_dim bias=False wo = nn Linear config dim config dim bias=False kv_cache = None n_head = config n_head head_dim = config head_dim n_local_heads = config n_local_heads dim = config dim _register_load_state_dict_pre_hook load_hook load_hook state_dict prefix args prefix + wq weight state_dict wq = state_dict pop prefix + wq weight wk = state_dict pop prefix + wk weight wv = state_dict pop prefix + wv weight state_dict prefix + wqkv weight = torch cat wq wk wv forward x Tensor freqs_cis Tensor mask Tensor input_pos Optional Tensor = None - Tensor bsz seqlen _ = x shape kv_size = n_local_heads head_dim q k v = wqkv x split dim kv_size kv_size dim=- q = q view bsz seqlen n_head head_dim k = k view bsz seqlen n_local_heads head_dim v = v view bsz seqlen n_local_heads head_dim q = apply_rotary_emb q freqs_cis k = apply_rotary_emb k freqs_cis q k v = map lambda x x transpose q k v kv_cache None k v = kv_cache update input_pos k v k = k repeat_interleave n_head n_local_heads dim= v = v repeat_interleave n_head n_local_heads dim= y = F scaled_dot_product_attention q k v attn_mask=mask dropout_p= y = y transpose contiguous view bsz seqlen dim y = wo y y ConditionalFeedForward nn Module __init__ config super __init__ w = nn Parameter torch empty config num_experts config intermediate_size config dim w = nn Parameter torch empty config num_experts config dim config intermediate_size w = nn Parameter torch empty config num_experts config intermediate_size config dim forward x Tensor expert_indices Tensor - Tensor w _weights = w expert_indices T A D D w _weights = w expert_indices T A D D w _weights = w expert_indices T A D D x = F silu torch einsum ti taoi - tao x w _weights x = torch einsum ti taoi - tao x w _weights expert_outs = torch einsum tao taio - tai x x w _weights expert_outs MOEFeedForward nn Module __init__ config - None super __init__ gate = nn Linear config dim config num_experts bias=False cond_ffn = ConditionalFeedForward config dim = config dim num_activated_experts = config num_activated_experts forward x Tensor - Tensor x = x view - dim T = num_tokens E = num_experts D = hidden dim A = activated experts x T D scores = gate x T E expert_weights = F softmax scores dim=- expert_weights expert_indices = torch topk expert_weights num_activated_experts dim=- T A T A expert_weights = expert_weights sum dim=- keepdim=True T A expert_outs = cond_ffn x expert_indices torch einsum tai ta - ti expert_outs expert_weights RMSNorm nn Module __init__ dim int eps float = e- super __init__ eps = eps weight = nn Parameter torch ones dim _norm x x torch rsqrt torch mean x x dim=- keepdim=True + eps forward x Tensor - Tensor output = _norm x float type_as x output weight precompute_freqs_cis seq_len int n_elem int base int = - Tensor freqs = base torch arange n_elem n_elem float n_elem t = torch arange seq_len device=freqs device freqs = torch outer t freqs freqs_cis = torch polar torch ones_like freqs freqs cache = torch stack freqs_cis real freqs_cis imag dim=- cache dtype=torch bfloat apply_rotary_emb x Tensor freqs_cis Tensor - Tensor xshaped = x float reshape x shape - - freqs_cis = freqs_cis view xshaped size xshaped size x_out = torch stack xshaped freqs_cis - xshaped freqs_cis xshaped freqs_cis + xshaped freqs_cis - x_out = x_out flatten x_out type_as x