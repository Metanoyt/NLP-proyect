Owner s module dynamo contextlib sys unittest contextlib contextmanager torch torch _dynamo test_case torch _dynamo testing torch _dynamo exc InternalTorchDynamoError torch _dynamo testing EagerAndRecordGraphs normalize_gm same torch _dynamo utils counters torch nn functional F torch testing _internal common_cuda PLATFORM_SUPPORTS_FLASH_ATTENTION torch testing _internal common_utils instantiate_parametrized_tests parametrize try test_functions except ImportError test_functions _variable = _variable = z_glb = k_glb = contextlib contextmanager set_default_dtype dtype old_dtype = torch get_default_dtype try torch set_default_dtype dtype yield finally torch set_default_dtype old_dtype CustomizedCtxManager __init__ mode prev = torch is_grad_enabled mode = mode __enter__ torch _C _set_grad_enabled mode __exit__ exc_type exc_value traceback torch _C _set_grad_enabled prev contextlib contextmanager customized_ctx_manager mode prev = torch is_grad_enabled try yield torch _C _set_grad_enabled mode finally torch _C _set_grad_enabled prev CustomizedCtxManagerWithGraphBreak CustomizedCtxManager __enter__ torch _dynamo graph_break super __enter__ contextlib contextmanager customized_ctx_manager_with_graph_break mode prev = torch is_grad_enabled try torch _dynamo graph_break yield torch _C _set_grad_enabled mode finally torch _C _set_grad_enabled prev CtxManagerTests torch _dynamo test_case TestCaseWithNestedGraphBreaks test_no_grad fn b x = + redundant no_grad should get ignored torch no_grad x = x + b x = x + x fn b x = + torch set_grad_enabled False x = x + b x = x + x fn b x = + torch enable_grad x = x + b x = x + x fn b x = + torch set_grad_enabled True torch is_grad_enabled x = x + b x = x + x torch no_grad torch _dynamo testing standard_test fn=fn nargs= expected_ops= coalesced noop torch _dynamo testing standard_test fn=fn nargs= expected_ops= coalesced noop torch _dynamo testing standard_test fn=fn nargs= expected_ops= torch _dynamo testing standard_test fn=fn nargs= expected_ops= torch enable_grad torch _dynamo testing standard_test fn=fn nargs= expected_ops= torch _dynamo testing standard_test fn=fn nargs= expected_ops= torch _dynamo testing standard_test fn=fn nargs= expected_ops= coalesced noop torch _dynamo testing standard_test fn=fn nargs= expected_ops= coalesced noop test_grad_mode_guard fn b prev_grad = torch is_grad_enabled torch set_grad_enabled False = + tolist graph break ret = + b torch set_grad_enabled prev_grad ret = torch randn b = torch randn cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts _ range opt_fn b assertEqual cnts frame_count test_nested_grad_mode_graph_break fn x before = torch is_grad_enabled torch set_grad_enabled False torch _dynamo graph_break torch set_grad_enabled True x = torch mul x torch _dynamo graph_break x = torch sqrt x assert torch is_grad_enabled assert torch is_grad_enabled assert torch is_grad_enabled == before x = torch randn cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts _ range opt_fn assertEqual cnts frame_count test_torch_profiler wrap torch profiler NullContextVariable do nothing fn x y = x torch profiler profile y = y + torch profiler record_function my_function z = y z tolist graph break z = z + z x = torch randn requires_grad=True ref = fn x cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts res = opt_fn x assertTrue same ref res assertEqual cnts frame_count test_autograd_profiler wrap torch autograd profiler NullContextVariable do nothing fn x y = x torch autograd profiler profile y = y + torch autograd profiler record_function my_function z = y z tolist graph break z = z + z x = torch randn requires_grad=True ref = fn x cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts res = opt_fn x assertTrue same ref res assertEqual cnts frame_count unittest skipIf torch cuda is_available requires cuda test_cuda_stream_context_manager fn x s = torch cuda Stream x = torch mul x x = torch add x current_stream = torch cuda current_stream s wait_stream current_stream torch cuda stream s x = torch relu x current_stream wait_stream s x = torch add x x = torch cos x x x = torch randn device= cuda ref = fn x cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts fullgraph=True res = opt_fn x assertEqual ref res assertEqual cnts frame_count assertExpectedInline str cnts op_count unittest expectedFailure https github com pytorch pytorch issues unittest skipIf torch cuda is_available requires cuda test_cuda_stream_across_graph_break fn x s = torch cuda Stream x = torch mul x x = torch add x print foo tcs = torch cuda stream s current_stream = torch cuda current_stream s wait_stream current_stream tcs x = torch relu x current_stream wait_stream s x = torch add x x = torch cos x x x = torch randn device= cuda ref = fn x cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts res = opt_fn x assertEqual ref res assertEqual cnts frame_count assertEqual cnts op_count unittest expectedFailure https github com pytorch pytorch issues unittest skipIf torch cuda is_available requires cuda test_cuda_stream_context_manager fn x s x = torch mul x x = torch add x current_stream = torch cuda current_stream s wait_stream current_stream torch cuda stream s x = torch relu x current_stream wait_stream s torch cuda stream current_stream x = torch relu x s = torch cuda Stream s wait_stream current_stream torch cuda stream s x = torch relu x current_stream wait_stream s x = torch add x x = torch cos x x x = torch randn device= cuda s = torch cuda Stream ref = fn x s cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts fullgraph=True res = opt_fn x s assertEqual ref res assertEqual cnts frame_count assertEqual cnts op_count unittest skipIf torch cuda is_available requires cuda test_cuda_stream_method fn x x = torch mul x x = torch add x new_stream = torch cuda Stream cur_stream = torch cuda current_stream new_stream wait_stream cur_stream torch cuda stream new_stream x = torch sin x x = torch add x cur_stream wait_stream new_stream x = torch add x cur_stream query cur_stream synchronize torch cuda stream new_stream x = torch add x new_stream synchronize x = torch relu x x = torch cos x x x = torch randn device= cuda ref = fn x cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts fullgraph=True res = opt_fn x assertEqual ref res assertEqual cnts frame_count assertExpectedInline str cnts op_count unittest skipIf torch cuda is_available requires cuda test_cuda_stream_compared_with_constant fn x x = torch mul x x = torch add x cur_stream = torch cuda current_stream cur_stream None x + x - fn x x = torch mul x x = torch add x cur_stream = torch cuda current_stream cur_stream = const_str x + x - x = torch randn device= cuda ref = fn x cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts fullgraph=True opt_fn = torch compile fn backend=cnts fullgraph=True res = opt_fn x res = opt_fn x assertEqual ref res assertEqual ref res unittest skipIf torch cuda is_available requires cuda test_cuda_stream_compared_with_stream fn x s s s == s x + x - s = torch cuda Stream s = torch cuda Stream x = torch randn cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts fullgraph=True ref = fn x s s res = opt_fn x s s assertEqual cnts frame_count assertEqual ref res ref = fn x s s res = opt_fn x s s We have re-compilation because changing inputs assertEqual cnts frame_count assertEqual ref res torch _dynamo reset cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts fullgraph=True ref = fn x s s res = opt_fn x s s assertEqual cnts frame_count assertEqual ref res ref = fn x s s res = opt_fn x s s We have re-compilation because changing inputs assertEqual cnts frame_count assertEqual ref res unittest skipIf torch cuda is_available requires cuda test_cuda_event_reconstruct fn x e = torch cuda Event x = torch mul x x = torch add x x e x = torch randn device= cuda ref = fn x cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts res = opt_fn x assertEqual ref res assertEqual cnts frame_count assertEqual cnts op_count unittest skipIf torch cuda is_available requires cuda test_cuda_event_across_graph_break fn x e = torch cuda Event e record x = torch mul x x = torch add x print foo torch cuda current_stream wait_event e x = torch add x x = torch cos x x e x = torch randn device= cuda ref = fn x cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts res = opt_fn x assertEqual ref res assertEqual cnts frame_count assertEqual cnts op_count unittest skipIf torch cuda is_available requires cuda test_cuda_event_created_outside_of_graph user_stream = torch cuda Stream event = torch cuda Event foo = torch empty device= cuda func foo event wait foo + event x = torch randn device= cuda cnts = torch _dynamo testing CompileCounter run_iters fn compile=False compile fn = torch compile fn backend=cnts _ range torch cuda stream user_stream torch mm x x out=foo event record out = fn foo let ` fn ` finish reading ` foo ` before writing next iteration ` run_iters ` call torch cuda current_stream synchronize out ref = run_iters func compile=False res = run_iters func compile=True assertEqual ref res assertEqual cnts frame_count assertEqual cnts op_count unittest skipIf torch cuda is_available requires cuda test_cuda_event_method_create_stream_outside_of_compile fn x cur_stream new_stream x = torch mul x x = torch add x x = torch add x event = cur_stream record_event event query new_stream wait_event event torch cuda stream new_stream x = torch add x new_event = torch cuda Event new_event record new_stream new_event wait cur_stream x = torch add x use new event sync new_event synchronize x = torch relu x x = torch cos x x x = torch randn device= cuda cur_stream = torch cuda current_stream new_stream = torch cuda Stream ref = fn x cur_stream new_stream cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts fullgraph=True res = opt_fn x cur_stream new_stream assertEqual ref res assertEqual cnts frame_count assertExpectedInline str cnts op_count unittest skipIf torch cuda is_available requires cuda test_cuda_event_method fn x x = torch mul x x = torch add x cur_stream = torch cuda current_stream new_stream = torch cuda Stream x = torch add x event = cur_stream record_event event query new_stream wait_event event torch cuda stream new_stream x = torch add x new_event = torch Event new_event record new_stream new_event wait cur_stream x = torch add x use new event sync new_event synchronize x = torch relu x x = torch cos x x x = torch randn device= cuda ref = fn x cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts fullgraph=True res = opt_fn x assertEqual ref res assertEqual cnts frame_count assertExpectedInline str cnts op_count unittest skipIf torch cuda is_available requires cuda test_cuda_device fn x torch cuda device x device index - x = torch sin x + x x = torch randn device= cuda ref = fn x opt_fn = torch compile backend= eager fullgraph=True fn res = opt_fn x assertEqual ref res test_autograd_profiler_enabled fn x torch autograd _profiler_enabled x + x - x = torch randn requires_grad=True cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts torch autograd _profiler_enabled torch autograd _disable_profiler assert torch autograd _profiler_enabled ref = fn x res = opt_fn x assertTrue same ref res torch autograd profiler profile assert torch autograd _profiler_enabled ref = fn x res = opt_fn x assertTrue same ref res unittest skipIf torch cuda is_available requires cuda test_autocast torch cuda is_bf _supported raise unittest SkipTest requires bf MyModule torch nn Module forward x a_float = torch rand device= cuda b_float = torch rand device= cuda d_float = torch rand device= cuda torch autocast device_type= cuda dtype=torch bfloat e_float = torch mm a_float b_float f_float = torch mm d_float e_float f_float module = MyModule real = module torch tensor real_device = real device real_dtype = real dtype graph _ = torch _dynamo export module torch tensor exported = graph torch tensor assertEqual exported device real_device assertEqual exported dtype real_dtype assertEqual exported device type cuda assertEqual exported device index assertEqual exported dtype torch bfloat unittest skipIf torch cuda is_available requires cuda test_cuda_amp_autocast MyModule torch nn Module forward x a_float = torch rand device= cuda b_float = torch rand device= cuda torch autocast device_type= cuda dtype=torch float c_float = torch mm a_float b_float c_float module = MyModule real = module torch tensor real_device = real device real_dtype = real dtype graph _ = torch _dynamo export module torch tensor exported = graph torch tensor assertEqual exported device real_device assertEqual exported dtype real_dtype assertEqual exported device type cuda assertEqual exported device index assertEqual exported dtype torch float test_is_autocast_cpu_enabled fn a_float b_float torch autocast device_type= cpu dtype=torch bfloat c_float = torch mm a_float b_float torch is_autocast_cpu_enabled c_float = c_float + c_float = torch rand b = torch rand ref = fn b opt_fn = torch compile fn backend= eager fullgraph=True res = opt_fn b assertTrue same ref res unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Can t run fused SDPA platform test_autocast_sdpa MyModule torch nn Module forward query key value torch autocast cpu torch autocast cuda dtype=torch float out = F scaled_dot_product_attention query key value None True out dtype = torch float seq_len_q = seq_len_k = head_dim = query = torch ones seq_len_q head_dim device= cuda dtype=dtype requires_grad=True key = torch ones seq_len_k head_dim device= cuda dtype=dtype requires_grad=True value = torch ones seq_len_k head_dim device= cuda dtype=dtype requires_grad=True module = MyModule real = module query key value real_device = real device real_dtype = real dtype opt_mod = torch compile module backend= inductor compiled = opt_mod query key value assertEqual compiled device real_device assertEqual compiled dtype real_dtype assertEqual compiled device type cuda assertEqual compiled device index assertEqual compiled dtype torch float test_autocast_cpu MyModule torch nn Module forward x a_float = torch rand device= cpu b_float = torch rand device= cpu d_float = torch rand device= cpu torch autocast device_type= cpu dtype=torch bfloat e_float = torch mm a_float b_float f_float = torch mm d_float e_float f_float module = MyModule real = module torch tensor real_device = real device real_dtype = real dtype graph _ = torch _dynamo export module torch tensor exported = graph torch tensor assertEqual exported device real_device assertEqual exported dtype real_dtype assertEqual exported device type cpu assertEqual exported dtype torch bfloat test_autocast_cpu_graph_break MyModule torch nn Module forward x a_float = torch rand device= cpu b_float = torch rand device= cpu torch _dynamo graph_break d_float = torch rand device= cpu torch autocast device_type= cpu dtype=torch bfloat e_float = torch mm a_float b_float torch _dynamo graph_break f_float = torch mm d_float e_float f_float module = MyModule real = module torch tensor real_device = real device real_dtype = real dtype opt = torch compile module backend= eager res = opt torch tensor assertEqual res device real_device assertEqual res dtype real_dtype assertEqual res device type cpu assertEqual res dtype torch bfloat test_autocast_cpu_graph_break_ Regression https github com pytorch pytorch issues fn x torch autocast device_type= cpu dtype=torch bfloat x = torch mm x x torch _dynamo graph_break x = torch relu x x x = torch rand assertEqual x dtype torch float res = fn x opt_fn = torch compile fn backend= eager opt_res = opt_fn x assertTrue torch allclose res opt_res assertEqual res dtype torch bfloat assertEqual opt_res dtype torch bfloat test_autocast_cpu_graph_break_inner_fn MyModule torch nn Module staticmethod mm_breaks x y torch _dynamo graph_break torch mm x y forward x a_float = torch rand device= cpu b_float = torch rand device= cpu torch autocast device_type= cpu dtype=torch bfloat torch _dynamo graph_break torch autocast device_type= cpu dtype=torch bfloat enabled=False torch _dynamo graph_break g_float = torch mm a_float b_float torch autocast device_type= cpu dtype=torch bfloat Check nested non-inlineable function graph break torch _dynamo graph_break f_float _ = mm_breaks a_float b_float We remember exit inner autocast correctly outer even after graph breaks f_float = mm_breaks a_float b_float assert f_float dtype == f_float _ dtype f_float g_float module = MyModule real_ real_ = module torch tensor real_device_ = real_ device real_dtype_ = real_ dtype real_device_ = real_ device real_dtype_ = real_ dtype graph = torch compile module backend= eager out_ out_ = graph torch tensor assertEqual out_ device real_device_ assertEqual out_ dtype real_dtype_ assertEqual out_ device real_device_ assertEqual out_ dtype real_dtype_ assertEqual out_ device type cpu assertEqual out_ dtype torch bfloat assertEqual out_ device type cpu assertEqual out_ dtype torch float test_autocast_graph_break_method MyModule torch nn Module __init__ bias super __init__ bias = bias mm_not_break x y torch mm x y + bias mm_breaks x y torch _dynamo graph_break torch mm x y + bias forward x a_float = torch rand device= cpu b_float = torch rand device= cpu torch autocast device_type= cpu dtype=torch bfloat torch autocast device_type= cpu dtype=torch bfloat enabled=False g_float = torch mm a_float b_float f_float = mm_breaks a_float b_float assert f_float == mm_not_break a_float b_float f_float g_float module = MyModule bias=torch rand device= cpu dtype=torch bfloat torch autocast device_type= cpu dtype=torch bfloat Autocast doesn t work addition so we need bias ` bfloat ` res = torch rand device= cpu dtype=torch float + torch rand device= cpu dtype=torch bfloat assertEqual res dtype torch float real_ real_ = module torch tensor real_device_ = real_ device real_dtype_ = real_ dtype real_device_ = real_ device real_dtype_ = real_ dtype graph = torch compile module backend= eager out_ out_ = graph torch tensor assertEqual out_ device real_device_ assertEqual out_ dtype real_dtype_ assertEqual out_ device real_device_ assertEqual out_ dtype real_dtype_ assertEqual out_ device type cpu assertEqual out_ dtype torch bfloat assertEqual out_ device type cpu assertEqual out_ dtype torch float unittest skipIf torch cuda is_available requires cuda test_autocast_float MyModule torch nn Module forward x a_float = torch rand device= cuda b_float = torch rand device= cuda d_float = torch rand device= cuda torch autocast device_type= cuda dtype=torch float e_float = torch mm a_float b_float f_float = torch mm d_float e_float f_float module = MyModule real = module torch tensor real_device = real device real_dtype = real dtype graph _ = torch _dynamo export module torch tensor exported = graph torch tensor assertEqual exported device real_device assertEqual exported dtype real_dtype assertEqual exported device index assertEqual exported dtype torch float unittest skipIf torch cuda is_available requires cuda test_autocast_device MyModule torch nn Module forward x a_float = torch rand device= cuda b_float = torch rand device= cuda d_float = torch rand device= cuda torch autocast cuda e_float = torch mm a_float b_float f_float = torch mm d_float e_float f_float module = MyModule real = module torch tensor real_device = real device real_dtype = real dtype graph _ = torch _dynamo export module torch tensor exported = graph torch tensor assertEqual exported device real_device assertEqual exported dtype real_dtype assertEqual exported device index assertEqual exported dtype torch float unittest skipIf torch cuda is_available requires cuda test_autocast_arguments_binding f x torch autocast device_type= cuda enabled=False x = torch sin x + x f x torch autocast device_type= cpu enabled=False x = torch cos x + x x = torch rand ref = f x ref = f x opt_f = torch compile backend= eager f opt_f = torch compile backend= eager f res = opt_f x res = opt_f x assertTrue same ref res assertTrue same ref res unittest skipIf torch cuda is_available requires cuda test_autocast_decorator autocast_func orig_func torch amp autocast device_type= cuda dtype=torch float new_fwd args kwargs orig_func args kwargs new_fwd autocast_func_cuda orig_func torch autocast device_type= cuda dtype=torch float new_fwd args kwargs orig_func args kwargs new_fwd autocast_func_cpu orig_func torch autocast device_type= cpu dtype=torch float new_fwd args kwargs orig_func args kwargs new_fwd mm b torch mm b mm_float = autocast_func mm mm_float _cuda = autocast_func_cuda mm mm_float _cpu = autocast_func_cpu mm fn b mm_float b mm_float _cuda b mm_float _cpu b a_float = torch rand device= cuda b_float = torch rand device= cuda ref = fn a_float b_float opt_fn = torch compile backend= eager fullgraph=True fn res = opt_fn a_float b_float assertTrue same ref res assertTrue res dtype == torch float assertTrue res dtype == torch float parametrize Ctx CustomizedCtxManagerWithGraphBreak customized_ctx_manager_with_graph_break name_fn=lambda x x __name__ test_generic_ctx_manager_with_graph_break Ctx fn x Ctx False body runs eager torch is_grad_enabled z = x + y = x z = y sin + z assertTrue torch is_grad_enabled x = torch randn requires_grad=True expected = fn x got = torch compile backend= eager fullgraph=False fn x assertEqual expected got assertTrue torch is_grad_enabled assertFalse got requires_grad since run under torch no_grad test_return_context_manager torch compile backend= eager fullgraph=True f x cm = CustomizedCtxManager False cm pass cm x = torch randn cm = f x assertFalse cm mode test_return_context_manager_with_graph_break torch compile backend= eager fullgraph=False f x cm = CustomizedCtxManager False torch _dynamo graph_break cm pass cm x = torch randn cm = f x assertFalse cm mode torch _dynamo config patch enable_trace_contextlib=True parametrize Ctx CustomizedCtxManager customized_ctx_manager name_fn=lambda x x __name__ test_generic_context_manager Ctx fn x Ctx True x = x + torch is_grad_enabled x = x x = torch relu x x - x = torch rand cnts = torch _dynamo testing CompileCounter opt_fn = torch compile backend=cnts fullgraph=True fn torch no_grad ref = fn x res = opt_fn x assertTrue same ref res assertEqual cnts frame_count assertEqual cnts op_count torch enable_grad ref = fn x res = opt_fn x assertTrue same ref res assertEqual cnts frame_count assertEqual cnts op_count torch _dynamo config patch enable_trace_contextlib=True parametrize Ctx CustomizedCtxManager customized_ctx_manager name_fn=lambda x x __name__ test_nested_generic_context_manager Ctx fn x Ctx True x = x + torch is_grad_enabled x = x Ctx False torch is_grad_enabled x = x - x = x x = torch relu x x - x = torch rand cnts = torch _dynamo testing CompileCounter opt_fn = torch compile backend=cnts fullgraph=True fn torch no_grad ref = fn x res = opt_fn x assertTrue same ref res assertEqual cnts frame_count assertEqual cnts op_count torch enable_grad ref = fn x res = opt_fn x assertTrue same ref res assertEqual cnts frame_count assertEqual cnts op_count torch _dynamo config patch enable_trace_contextlib=True parametrize Ctx CustomizedCtxManager customized_ctx_manager name_fn=lambda x x __name__ test_generic_context_manager_with_graph_break Ctx fn x Ctx True x = x + torch is_grad_enabled x = x torch _dynamo graph_break x = torch relu x x - x = torch rand cnts = torch _dynamo testing CompileCounter opt_fn = torch compile backend=cnts fullgraph=False fn torch no_grad ref = fn x res = opt_fn x assertTrue same ref res Ctx CustomizedCtxManager assertEqual cnts frame_count assertEqual cnts op_count torch enable_grad ref = fn x res = opt_fn x assertTrue same ref res Ctx CustomizedCtxManager assertEqual cnts frame_count assertEqual cnts op_count torch _dynamo config patch enable_trace_contextlib=True parametrize Ctx CustomizedCtxManager customized_ctx_manager name_fn=lambda x x __name__ test_nested_generic_context_manager_with_graph_break Ctx fn x Ctx True x = x + torch is_grad_enabled x = x Ctx False torch is_grad_enabled x = x - torch _dynamo graph_break x = x x = torch relu x x - x = torch rand cnts = torch _dynamo testing CompileCounter opt_fn = torch compile backend=cnts fullgraph=False fn torch no_grad ref = fn x res = opt_fn x assertTrue same ref res Ctx CustomizedCtxManager assertEqual cnts frame_count assertEqual cnts op_count torch _dynamo reset cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts fullgraph=False torch enable_grad ref = fn x res = opt_fn x assertTrue same ref res Ctx CustomizedCtxManager assertEqual cnts frame_count assertEqual cnts op_count test_graph_break_inlining_grad gn z torch no_grad torch _dynamo graph_break torch sin z fn x y z = torch mm x y z = gn z torch _dynamo reset cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts fullgraph=False x = torch randn requires_grad=True y = torch randn requires_grad=True z = torch randn opt_fn x y z sum backward assertEqual cnts frame_count _graph_break_inlining_autocast_test_helper device gn x y torch autocast device_type=device dtype=torch bfloat z = torch mm x y torch _dynamo graph_break torch sin z fn x y z = torch mm x y z = z + gn x y z x = torch rand device y = torch rand device opt_fn = torch compile backend= eager fn ref = fn x y res = opt_fn x y assertEqual ref res test_graph_break_inlining_autocast device cuda cpu device == cuda torch cuda is_available torch cuda is_bf _supported continue _graph_break_inlining_autocast_test_helper device test_disable_saved_tensors_hooks fn z torch autograd graph disable_saved_tensors_hooks This supported f x y x + y x y = torch ones torch zeros f x y eager = EagerAndRecordGraphs torch compile fn backend=eager fullgraph=True torch randn graph = eager graphs actual = normalize_gm graph print_readable False assertExpectedInline actual \ GraphModule torch nn Module forward _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable This supported _saved_tensors_hooks_disable = None x f = torch ones y f = torch zeros add f = x + y x = y = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None add NOQA B test_disable_saved_tensors_hooks_prev_disabled fn z torch autograd graph disable_saved_tensors_hooks This supported f x y x + y x y = torch ones torch zeros f x y eager = EagerAndRecordGraphs torch autograd graph disable_saved_tensors_hooks Previously disabled message torch compile fn backend=eager fullgraph=True torch randn graph = eager graphs actual = normalize_gm graph print_readable False assertExpectedInline actual \ GraphModule torch nn Module forward _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable This supported _saved_tensors_hooks_disable = None x f = torch ones y f = torch zeros add f = x + y x = y = None _saved_tensors_hooks_disable_ = torch _C _autograd _saved_tensors_hooks_disable Previously disabled message _saved_tensors_hooks_disable_ = None add NOQA B test_disable_saved_tensors_hooks_prev_disabled_nested fn z torch autograd graph disable_saved_tensors_hooks This supported f x y torch autograd graph disable_saved_tensors_hooks This supported inner inner_fn x y x + y inner_fn x y + x x y = torch ones torch zeros f x y eager = EagerAndRecordGraphs torch autograd graph disable_saved_tensors_hooks Previously disabled message torch compile fn backend=eager fullgraph=True torch randn graph = eager graphs actual = normalize_gm graph print_readable False assertExpectedInline actual \ GraphModule torch nn Module forward _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable This supported _saved_tensors_hooks_disable = None x f = torch ones y f = torch zeros _saved_tensors_hooks_disable_ = torch _C _autograd _saved_tensors_hooks_disable This supported inner _saved_tensors_hooks_disable_ = None add f = x + y y = None _saved_tensors_hooks_disable_ = torch _C _autograd _saved_tensors_hooks_disable This supported _saved_tensors_hooks_disable_ = None add_ f = add + x add = x = None _saved_tensors_hooks_disable_ = torch _C _autograd _saved_tensors_hooks_disable Previously disabled message _saved_tensors_hooks_disable_ = None add_ NOQA B test_disable_saved_tensors_hooks_graph_break fn x torch autograd graph disable_saved_tensors_hooks This supported y = x + torch _dynamo graph_break y eager = EagerAndRecordGraphs torch compile fn backend=eager fullgraph=False torch randn check_graph actual expected noqa F assertExpectedInline actual expected graph = eager graphs actual = normalize_gm graph print_readable False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable This supported _saved_tensors_hooks_disable = None y f = l_x_ + l_x_ = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None y NOQA B graph = eager graphs actual = normalize_gm graph print_readable False assertExpectedInline actual \ GraphModule torch nn Module forward L_y_ f l_y_ = L_y_ _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable This supported _saved_tensors_hooks_disable = None mul f = l_y_ l_y_ = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None mul NOQA B test_context_wrapping_grad_mode_decorator ctx_wrappers = torch enable_grad True torch no_grad False call True False i range torch _dynamo reset ctx_wrapper _ = ctx_wrappers i ctx_wrapper_inverse mode_inverse = ctx_wrappers i + fn x inner_func x x sin ctx_wrapper_inverse call inner_func = ctx_wrapper inner_func inner_func = ctx_wrapper inner_func Calling no_grad enabled_grad should mutate global state assert torch is_grad_enabled == mode_inverse ctx_wrapper_inverse inner_func x x = torch zeros requires_grad=True opt_fn = torch compile fn backend= eager fullgraph=True assertEqual fn x opt_fn x assertEqual fn x requires_grad opt_fn x requires_grad test_context_wrapping_grad_mode_nested_function_decorator ctx_wrappers = torch enable_grad True torch no_grad False call True False i range torch _dynamo reset ctx_wrapper _ = ctx_wrappers i ctx_wrapper_inverse mode_inverse = ctx_wrappers i + fn x ctx_wrapper_inverse call ctx_wrapper inner_func x x sin ctx_wrapper inner_func x x sin Calling no_grad enabled_grad should mutate global state assert torch is_grad_enabled == mode_inverse ctx_wrapper_inverse inner_func x x = torch zeros requires_grad=True opt_fn = torch compile fn backend= eager fullgraph=True assertEqual fn x opt_fn x assertEqual fn x requires_grad opt_fn x requires_grad test_context_wrapping_set_grad_enabled_nested_function modes = True False decorator True False i range torch _dynamo reset mode = modes i mode_inverse = modes i + fn x torch set_grad_enabled mode_inverse decorator torch set_grad_enabled mode inner_func x x sin inner_func x x sin inner_func = torch set_grad_enabled mode inner_func Consuming set_grad_enabled calling function should mutate global state assert torch is_grad_enabled == mode_inverse torch set_grad_enabled mode_inverse inner_func x x = torch zeros requires_grad=True opt_fn = torch compile fn backend= eager fullgraph=True assertEqual fn x opt_fn x assertEqual fn x requires_grad opt_fn x requires_grad test_inactive_context_graph_break_local fn x x = x + ctx = torch set_grad_enabled True torch _dynamo graph_break ctx x = x + x x = torch zeros requires_grad=False cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts assertEqual fn x opt_fn x assertEqual fn x requires_grad opt_fn x requires_grad assertEqual cnts frame_count test_inactive_context_graph_break_local_nullctx contextlib test context manager results None target_values fn x x = x + ctx = contextlib nullcontext torch _dynamo graph_break ctx x = x + x x = torch zeros requires_grad=False cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts assertEqual fn x opt_fn x assertEqual fn x requires_grad opt_fn x requires_grad assertEqual cnts frame_count test_inactive_context_graph_break_local_nullctx contextlib test nullcontext where graph break happens inlined function returns something gn torch _dynamo graph_break fn x x = x + ctx = contextlib nullcontext lst = gn ctx x = x + lst x x = torch zeros requires_grad=False cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts assertEqual fn x opt_fn x assertEqual fn x requires_grad opt_fn x requires_grad assertEqual cnts frame_count test_inactive_context_graph_break_stack gn ctx torch _dynamo graph_break ctx fn x x = x + ctx = gn torch set_grad_enabled True we expect graph break next line well ctx x = x + x x = torch zeros requires_grad=False cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts assertEqual fn x opt_fn x assertEqual fn x requires_grad opt_fn x requires_grad test_inactive_context_graph_break_stack gn x ctx y z dummy ctx x y z fn x x = x + x = gn x torch set_grad_enabled True torch _dynamo graph_break x x = torch zeros requires_grad=False cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts assertEqual fn x opt_fn x assertEqual fn x requires_grad opt_fn x requires_grad assertEqual cnts frame_count test_sdpa_kernel_ctx_manager modified_backend_state = torch nn attention SDPBackend MATH torch _dynamo allow_in_graph check_backend_state_is_modified assertEqual torch nn attention _cur_sdpa_kernel_backends modified_backend_state f x torch nn attention sdpa_kernel pyre-fixme Module ` torch nn attention ` has no attribute ` SDPBackend ` torch nn attention SDPBackend MATH output = torch nn functional scaled_dot_product_attention x x x torch float check_backend_state_is_modified output opt_f = torch compile f backend= eager fullgraph=True opt_f torch randn dtype=torch float test_sdpa_kernel_ctx_manager original_backend_state = set torch nn attention _cur_sdpa_kernel_backends modified_backend_state = torch nn attention SDPBackend MATH torch _dynamo allow_in_graph check_backend_state_is_original assertEqual set torch nn attention _cur_sdpa_kernel_backends original_backend_state torch _dynamo allow_in_graph check_backend_state_is_modified assertEqual torch nn attention _cur_sdpa_kernel_backends modified_backend_state g x torch _dynamo graph_break output = torch nn functional scaled_dot_product_attention x x x torch float check_backend_state_is_modified output f x check_backend_state_is_original torch nn attention sdpa_kernel pyre-fixme Module ` torch nn attention ` has no attribute ` SDPBackend ` torch nn attention SDPBackend MATH output = torch nn functional scaled_dot_product_attention x x x torch float check_backend_state_is_modified graph break output = g x output = torch nn functional scaled_dot_product_attention x x x torch float check_backend_state_is_modified check_backend_state_is_original output + output + output cnts = torch _dynamo testing CompileCounter opt_f = torch compile f backend=cnts opt_f torch randn dtype=torch float assertEqual cnts frame_count test sdpa_kernel graph break arguments test_sdpa_kernel_ctx_manager modified_backend_state = torch nn attention SDPBackend MATH torch nn attention SDPBackend FLASH_ATTENTION torch _dynamo allow_in_graph check_backend_state_is_modified assertEqual set torch nn attention _cur_sdpa_kernel_backends modified_backend_state f x torch nn attention sdpa_kernel pyre-fixme Module ` torch nn attention ` has no attribute ` SDPBackend ` torch nn attention SDPBackend MATH torch nn attention SDPBackend FLASH_ATTENTION FLASH_ATTENTION may supported we re actually doing any sdpa x = x + torch _dynamo graph_break check_backend_state_is_modified x = x + x opt_f = torch compile f backend= eager opt_f torch randn Regression test make sure dynamo won t crash these kwargs test_sdpa_kernel_ctx_manager_kwargs backends = torch nn attention SDPBackend MATH torch _dynamo allow_in_graph check_backend_state_is_modified assertEqual set torch nn attention _cur_sdpa_kernel_backends set backends f x torch nn attention sdpa_kernel backends=backends set_priority=True x = x + check_backend_state_is_modified x = x + x opt_f = torch compile f backend= eager opt_f torch randn Regression test make sure dynamo won t graph break calling functions decorated special context manager test_sdpa_kernel_ctx_manager_as_decorator SDPA_BACKEND_PRIORITY = torch nn attention SDPBackend MATH torch nn attention SDPBackend EFFICIENT_ATTENTION torch nn attention SDPBackend FLASH_ATTENTION torch nn attention sdpa_kernel backends=SDPA_BACKEND_PRIORITY set_priority=True scaled_dot_product_attention q k v args kwargs torch nn functional scaled_dot_product_attention q k v args kwargs f x scaled_dot_product_attention x x x opt_f = torch compile f backend= eager fullgraph=True x = torch rand dtype=torch float ref = f x res = opt_f x assertEqual ref res Regression test make sure value set_priority used correctly test_sdpa_kernel_ctx_manager_set_priority backends = torch nn attention SDPBackend MATH default_priority = torch _C _get_sdp_priority_order torch _dynamo allow_in_graph check_backend_priority changed bool assertEqual changed torch _C _get_sdp_priority_order = default_priority f x torch nn attention sdpa_kernel backends=backends set_priority=True x = x + check_backend_priority changed=True x = x + torch nn attention sdpa_kernel backends=backends set_priority=False x = x + check_backend_priority changed=False x = x + x opt_f = torch compile f backend= eager opt_f torch randn test_torch_profiler_use_after_with_block counters clear fn x torch profiler profile p pass p profiler kineto_results experimental_event_tree x + opt_fn = torch compile fn backend= eager x = torch ones ref = fn x res = opt_fn x assertEqual ref res assertEqual len counters graph_break test_ _resume_block_keyerror https github com pytorch pytorch issues flag = True fn x x = x + torch _dynamo graph_break x = x + flag torch no_grad torch _dynamo graph_break x = x + torch no_grad torch _dynamo graph_break x = x + x + inp = torch ones opt_fn = torch compile fn backend= eager assertEqual fn inp opt_fn inp flag = False assertEqual fn inp opt_fn inp test_ _resume_block_keyerror https github com pytorch pytorch issues fn x torch _dynamo graph_break torch no_grad torch no_grad torch _dynamo graph_break x + inp = torch ones opt_fn = torch compile fn backend= eager assertEqual fn inp opt_fn inp test_store_attr_graph_break_key_error STORE_ATTR dummy should result graph break dummy pass fn x x = x + torch no_grad dummy attr = x x + inp = torch ones opt_fn = torch compile fn backend= eager assertEqual fn inp opt_fn inp assertGreater len counters graph_break ContextlibContextManagerTests torch _dynamo test_case TestCaseWithNestedGraphBreaks setUp super setUp _prev = torch _dynamo config enable_trace_contextlib _u_prev = torch _dynamo config enable_trace_unittest torch _dynamo config enable_trace_contextlib = True torch _dynamo config enable_trace_unittest = True tearDown super tearDown torch _dynamo config enable_trace_contextlib = _prev torch _dynamo config enable_trace_unittest = _u_prev test_ctx_basic contextlib contextmanager set_default_dtype dtype old_dtype = torch get_default_dtype try torch set_default_dtype dtype yield finally torch set_default_dtype old_dtype eager = EagerAndRecordGraphs torch compile backend=eager fullgraph=True fn set_default_dtype torch float x = torch tensor + j x y = fn assertEqual y dtype torch complex graph = eager graphs actual = normalize_gm graph print_readable False assertExpectedInline actual \ GraphModule torch nn Module forward set_default_dtype = torch set_default_dtype torch float set_default_dtype = None x c = torch tensor + j set_default_dtype_ = torch set_default_dtype torch float set_default_dtype_ = None x test_ctx_basic contextlib contextmanager compute_sin x try yield x sin finally pass torch compile backend= eager fullgraph=True fn x compute_sin x y y cos x = torch tensor y = fn x assertEqual y x sin cos test_change_parent_nonlocal_ test nonlocal actually gets propagated z = k = create_ctx contextmanager ctx x nonlocal z nonlocal k try k = yield x sin finally pass ctx run_ctx ctx x nonlocal z ctx x y z = k y cos torch compile backend= eager fullgraph=True fn x ctx = create_ctx run_ctx ctx x x = torch tensor y = fn x assertEqual y x sin cos assertEqual z assertEqual k test_change_parent_nonlocal_ test finally executed reading correct variable z = k = create_ctx contextmanager ctx x nonlocal z nonlocal k try yield x sin finally k = z ctx run_ctx ctx x nonlocal z z = ctx x y y cos torch compile backend= eager fullgraph=True fn x ctx = create_ctx run_ctx ctx x x = torch tensor y = fn x assertEqual y x sin cos assertEqual z assertEqual k test_globals_change_in_other_file contextmanager update_global_ctx global _variable _variable try _variable += _variable += yield finally pass torch compile backend= eager fullgraph=True fn x update_global_ctx pass test_functions update_global_ctx x Ensure updated global values read test_functions constant x _variable + _variable + test_functions _variable res = fn torch ones assertEqual _variable assertEqual _variable Ensure reconstructed bytecode updates global value other file assertEqual test_functions _variable assertEqual res torch ones test_change_parent_global_ test global actually gets propagated global z_glb k_glb z_glb k_glb = create_ctx contextmanager ctx x global k_glb try k_glb = yield x sin finally pass ctx run_ctx ctx x global z_glb ctx x y z_glb = k_glb y cos torch compile backend= eager fullgraph=True fn x ctx = create_ctx run_ctx ctx x x = torch tensor y = fn x assertEqual y x sin cos assertEqual z_glb assertEqual k_glb test_change_parent_global_ test finally executed reading correct variable global z_glb k_glb z_glb k_glb = create_ctx contextmanager ctx x global z_glb k_glb try yield x sin finally k_glb = z_glb ctx run_ctx ctx x global z_glb z_glb = ctx x y y cos torch compile backend= eager fullgraph=True fn x ctx = create_ctx run_ctx ctx x x = torch tensor y = fn x assertEqual y x sin cos assertEqual z_glb assertEqual k_glb test_change_parent_ create_ctx contextlib contextmanager ctx x try yield x sin finally pass ctx run_ctx ctx x ctx x y y cos torch compile backend= eager fullgraph=True fn x ctx = create_ctx run_ctx ctx x x = torch tensor y = fn x assertEqual y x sin cos test_change_parent_ create_ctx x contextlib contextmanager ctx try yield x sin finally pass ctx run_ctx ctx ctx y y cos torch compile backend= eager fullgraph=True fn x ctx = create_ctx x run_ctx ctx x = torch tensor y = fn x assertEqual y x sin cos test_graph_break_inside_ctx contextlib contextmanager whoo x y = x tan try torch _dynamo graph_break yield y finally pass f x y = x sin whoo x z y += z neg y += x cos y x = torch randn expected = f x eager = EagerAndRecordGraphs out = torch compile backend=eager fullgraph=False f x assertEqual expected out no graph will generated we will skip all frames due graph break assertEqual len eager graphs test_graph_break_inside_ctx_with_side_effects L = contextlib contextmanager whoo x y = x tan try L append x sin torch _dynamo graph_break yield y finally L append x cos f x y = x sin whoo x z y += z neg y += x cos y x = torch randn eager = EagerAndRecordGraphs y = torch compile backend=eager fullgraph=False f x assertEqual y x sin + x tan neg + x cos assertEqual L x sin x cos no graph will generated we will skip all frames due graph break assertEqual len eager graphs test_graph_break_inside_ctx_ contextlib contextmanager whoo x y = x tan try torch _dynamo graph_break yield y finally pass bar x whoo x z z neg f x x sin + bar x + x cos x = torch randn expected = f x eager = EagerAndRecordGraphs out = torch compile backend=eager fullgraph=False f x assertEqual expected out assertEqual len eager graphs assertExpectedInline normalize_gm eager graphs print_readable False \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ sin f = l_x_ sin l_x_ = None sin assertExpectedInline normalize_gm eager graphs print_readable False \ GraphModule torch nn Module forward L_stack _ f L_stack _ f L_x_ f l_stack _ = L_stack _ l_stack _ = L_stack _ l_x_ = L_x_ add f = l_stack _ + l_stack _ l_stack _ = l_stack _ = None cos f = l_x_ cos l_x_ = None add_ f = add + cos add = cos = None add_ test_graph_break_inside_ctx_ contextlib contextmanager whoo x try torch _dynamo graph_break yield x cos finally pass g x x neg + x acos f x y = x sin whoo x z y += g z y += y tan y x = torch randn expected = f x eager = EagerAndRecordGraphs out = torch compile backend=eager fullgraph=False f x assertEqual expected out assertEqual len eager graphs test_graph_break_before___enter__ contextlib contextmanager whoo x try yield x + finally pass fn x ctx = whoo x torch _dynamo graph_break y = ctx __enter__ ctx __exit__ None None None y x = torch tensor assertRaises InternalTorchDynamoError torch compile fn backend= eager fullgraph=False x test_graph_break_in_finally z = contextlib contextmanager whoo x nonlocal z try z append x yield x sin finally torch _dynamo graph_break z append x cos fn x ctx = whoo x y = ctx __enter__ ctx __exit__ None None None y x = torch tensor eager = EagerAndRecordGraphs out = torch compile backend=eager fullgraph=False fn x assertEqual out x sin assertEqual z x x cos assertEqual len eager graphs test_graph_break_inside___enter__ contextlib contextmanager whoo x try torch _dynamo graph_break yield x + finally pass fn x ctx = whoo x y = ctx __enter__ ctx __exit__ None None None y x = torch tensor expected = fn x eager = EagerAndRecordGraphs out = torch compile backend=eager fullgraph=False fn x assertEqual expected out assertEqual len eager graphs test_graph_break_after___enter__ contextlib contextmanager whoo x try yield x + finally pass fn x ctx = whoo x try y = ctx __enter__ torch _dynamo graph_break finally ctx __exit__ None None None y x = torch tensor expected = fn x eager = EagerAndRecordGraphs out = torch compile backend=eager fullgraph=False fn x assertEqual expected out assertEqual len eager graphs test_graph_break_before_and_after___enter__ contextlib contextmanager whoo x try yield x + finally pass fn x ctx = whoo x try torch _dynamo graph_break y = ctx __enter__ torch _dynamo graph_break finally ctx __exit__ None None None y x = torch tensor expected = fn x eager = EagerAndRecordGraphs out = torch compile backend=eager fullgraph=False fn x assertEqual expected out assertEqual len eager graphs test_graph_break_before___enter___and_disable___exit__ contextlib contextmanager whoo x try yield x + finally pass fn x ctx = whoo x try torch _dynamo graph_break y = ctx __enter__ finally torch _dynamo disable g ctx __exit__ None None None g y x = torch tensor expected = fn x eager = EagerAndRecordGraphs out = torch compile backend=eager fullgraph=False fn x assertEqual expected out assertEqual len eager graphs test_disable___enter__ h x x cos contextlib contextmanager whoo x try yield h x + finally pass fn x ctx = whoo x torch _dynamo disable g ctx __enter__ y = g ctx __exit__ None None None y x = torch tensor assertRaises InternalTorchDynamoError torch compile fn backend= eager fullgraph=False x test_disable___exit__ h x x cos contextlib contextmanager whoo x try yield h x + finally pass fn x ctx = whoo x y = ctx __enter__ torch _dynamo disable g ctx __exit__ None None None g y x = torch tensor assertRaises InternalTorchDynamoError torch compile fn backend= eager fullgraph=False x test_contextmanager_as_argument h x x cos contextlib contextmanager whoo x try yield h x + finally pass fn x ctx y = ctx __enter__ ctx __exit__ None None None x + y x = torch tensor expected = fn x whoo x eager = EagerAndRecordGraphs out = torch compile backend=eager fullgraph=False fn x whoo x assertEqual expected out assertEqual len eager graphs test_return_new_contextmanager L = h x x cos contextlib contextmanager whoo x try L append x sin yield h x + finally L append x cos fn x ctx = whoo x x + ctx x = torch tensor assertRaises InternalTorchDynamoError torch compile fn backend= eager fullgraph=False x test_return_advanced_contextmanager L = h x x cos contextlib contextmanager whoo x try L append x sin yield h x + finally L append x cos fn x ctx = whoo x y = ctx __enter__ x + y ctx x = torch tensor assertRaises InternalTorchDynamoError torch compile fn backend= eager fullgraph=False x test_contextmanager_as_argument_only___enter__ L = h x x cos contextlib contextmanager whoo x try L append x sin yield h x + finally L append x cos fn x ctx y = ctx __enter__ x + y x = torch tensor ctx = whoo x eager = EagerAndRecordGraphs y = torch compile backend=eager fullgraph=False fn x ctx assertEqual y x + x cos + assertEqual L x sin we should only have one item L ctx __exit__ None None None assertEqual L x sin x cos Two items now assertEqual len eager graphs test_contextmanager_as_argument_only___exit__ L = h x x cos contextlib contextmanager whoo x try L append x sin yield h x + finally L append x cos fn x ctx ctx __exit__ None None None x sin x = torch tensor ctx = whoo x ctx __enter__ assertEqual L x sin eager = EagerAndRecordGraphs y = torch compile backend=eager fullgraph=False fn x ctx assertEqual y x sin assertEqual L x sin x cos assertEqual len eager graphs test_advanced_contextmanager_as_argument h x x cos contextlib contextmanager whoo x try yield h x + finally pass fn x ctx ctx __exit__ None None None x + x = torch tensor ctx = whoo x y = ctx __enter__ assertEqual y x cos + z = torch compile backend= eager fullgraph=False fn x ctx assertEqual z x + test_advanced_contextmanager_as_argument_error h x x cos contextlib contextmanager whoo x try yield h x + finally pass fn x ctx y = ctx __enter__ ctx __exit__ None None None y x = torch tensor ctx = whoo x y = ctx __enter__ assertEqual y x cos + assertRaisesRegex AttributeError args torch compile backend= eager fullgraph=False fn x ctx test_disable_ctx_manager contextlib contextmanager whoo x try yield x + finally pass torch _dynamo disable g x whoo x y y fn x g x x = torch tensor expected = fn x eager = EagerAndRecordGraphs out = torch compile backend=eager fullgraph=False fn x assertEqual expected out assertEqual len eager graphs test_graph_break_and_disable___enter__ contextlib contextmanager whoo x try yield x + finally pass fn x ctx = whoo x try torch _dynamo graph_break torch _dynamo disable g ctx __enter__ y = g finally ctx __exit__ None None None y x = torch tensor expected = fn x eager = EagerAndRecordGraphs out = torch compile backend=eager fullgraph=False fn x assertEqual expected out assertEqual len eager graphs test_dynamo_disable_ctx contextlib contextmanager whoo x try yield x + finally pass torch _dynamo disable g x whoo x y y fn x g x x = torch tensor expected = fn x eager = EagerAndRecordGraphs out = torch compile backend=eager fullgraph=False fn x assertEqual expected out assertEqual len eager graphs torch _dynamo config patch enable_trace_contextlib=False test_disable_trace_contextmanager contextlib contextmanager whoo x try yield x cos finally pass g x x neg + x acos f x y = x sin whoo x z y += g z y += y tan y x = torch randn expected = f x eager = EagerAndRecordGraphs out = torch compile backend=eager fullgraph=False dynamic=False f x assertEqual expected out assertEqual len eager graphs parametrize name suppress stdout stderr test_contextlib_suppress name counters clear eager = EagerAndRecordGraphs fn t y = t sin ensure we graph break suppress call below name == suppress ctx = contextlib suppress ValueError name == stdout ctx = contextlib redirect_stdout sys stderr ctx = contextlib redirect_stderr sys stdout ctx y += t cos y tan t = torch randn expected = fn t got = torch compile backend=eager fullgraph=False fn t assertEqual expected got assertEqual len counters graph_break name = f redirect_ name name stdout stderr name assertRegex next iter counters graph_break f contextlib name supported test_contextlib_nullcontext counters clear torch compile backend= eager fullgraph=True fn t contextlib nullcontext t sin t = torch randn y = fn t nullcontext correctly handled dynamo assertEqual len counters graph_break assertEqual y t sin unittest skipIf sys version_info Python + test_WITH_EXCEPT_START contextmanager ctx try yield finally pass torch compile backend= eager fullgraph=True fn t try ctx raise ValueError except ValueError t sin t = torch randn y = fn t assertEqual y t sin instantiate_parametrized_tests CtxManagerTests instantiate_parametrized_tests ContextlibContextManagerTests __name__ == __main__ torch _dynamo test_case run_tests run_tests