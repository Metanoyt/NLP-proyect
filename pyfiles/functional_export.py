inspect logging traceback collections namedtuple collections abc Callable dataclasses dataclass typing Any Optional TYPE_CHECKING Union sympy torch torch fx torch utils _pytree pytree torch _dynamo convert_frame CaptureOutput fullgraph_capture get_traced_fn torch _dynamo eval_frame argument_names check_user_input_output torch _dynamo exc UserErrorType torch _dynamo utils dynamo_timed get_metrics_context torch _export utils _compiling_state_context torch export dynamic_shapes _RelaxedConstraint Constraint torch fx Node torch fx experimental proxy_tensor make_fx torch fx experimental symbolic_shapes ConstraintViolationError DimDynamic ShapeEnv StatelessSymbolicContext torch fx graph _ExportCodeGen _PyTreeCodeGen _PyTreeInfo torch utils _pytree TreeSpec TYPE_CHECKING torch _subclasses fake_tensor FakeTensorMode log = logging getLogger __name__ post_process_error_msg constraint_violation_error ConstraintViolationError func Callable Any args Any kwargs Any Because we trace different callable sources all messed up Manually patch them so error message looks correct torch export _unlift _get_input_paths _replace_sources orig_sig = inspect signature func flat_input_paths = _get_input_paths args kwargs orig_sig constraint_violation_error args = _replace_sources constraint_violation_error args flat_input_paths constraint_violation_error EXPORT_ROOT_REPLACEMENTS = __export_root_ _ _export_root _export_root clean_export_root_string text str - str Generic utility clean export_root patterns strings result = text pattern replacement EXPORT_ROOT_REPLACEMENTS result = result replace pattern replacement result clean_nn_module_stack_and_source_fn graph_module torch fx GraphModule is_inline_builtin=False - torch fx GraphModule Clean up nn_module_stack metadata removing export_root references Removes _export_root module references nn_module_stack metadata graph nodes which artifacts export process Fixes two patterns Keys Removes __export_root_ __modules _export_root _ prefixes - Normal case L__self____export_root_child - L__self__child - inline_builtin case Uses numeric ID strings like Values Removes _export_root _modules _export_root child names e g L _export_root child - L child e g L _modules _export_root child - L child Also removes root export entry L__self____export_root entirely Args graph_module The GraphModule clean up is_inline_builtin If True keys numeric ID strings references L filtered out Returns The cleaned GraphModule modified in-place _process_nn_module_stack nn_module_stack L__self____export_root nn_module_stack del nn_module_stack L__self____export_root Clean up remaining entries cleaned_stack = key child_name child_class nn_module_stack items Clean key removing export_root patterns clean_key = clean_export_root_string key Clean child_name removing export_root patterns clean_name = clean_export_root_string child_name Skip reference inline builtin case is_inline_builtin clean_name == L continue cleaned_stack clean_key = clean_name child_class cleaned_stack _process_source_fn source_fn_stack cleaned_stack = item source_fn_stack isinstance item tuple len item == name cls = item isinstance name str clean_name = clean_export_root_string name cleaned_stack append clean_name cls cleaned_stack append item cleaned_stack append item cleaned_stack node graph_module graph nodes nn_module_stack node meta node meta nn_module_stack = _process_nn_module_stack node meta nn_module_stack copy source_fn_stack = node meta get source_fn_stack None source_fn_stack node meta source_fn_stack = _process_source_fn source_fn_stack copy dynamo_flat_name_to_original_fqn graph_module meta Clean up flat name original fqn mapping clean_name_to_original_fqn = flat_name original_fqn graph_module meta dynamo_flat_name_to_original_fqn items clean_name_to_original_fqn clean_export_root_string flat_name = clean_export_root_string original_fqn graph_module meta dynamo_flat_name_to_original_fqn = clean_name_to_original_fqn graph_module clean_export_root graph_module torch fx GraphModule - None Remove export_root artifacts FX graph in-place Unlike getattr node call_module can invoked multiple times In those cases we should fix all invocations call_module clean_named_module_map dict str str = Update get_attr nodes in-place node graph_module graph nodes node op == get_attr old_target = node target new_target = clean_export_root_string old_target new_target = old_target node target = new_target assert hasattr graph_module old_target Move parameter new name param = torch fx graph_module _get_attr graph_module old_target torch fx graph_module _assign_attr param graph_module new_target torch fx graph_module _del_attr graph_module old_target Dynamo will only have one nested level node op == call_module old_target = node target assert isinstance old_target str new_target = clean_export_root_string old_target assert isinstance new_target str new_name = clean_export_root_string node name new_target == old_target continue module has already been cleaned before just lookup map old_target clean_named_module_map node target = clean_named_module_map old_target node name = new_name continue target = graph_module get_submodule old_target graph_module delete_submodule old_target graph_module add_submodule new_target target node target = new_target node name = new_name clean_named_module_map old_target = new_target ModuleToTrace torch nn Module __init__ foo Any in_spec Any - None super __init__ _export_root = foo in_spec = in_spec forward flat_args Any - ExportTracerOutput args kwargs = pytree tree_unflatten flat_args in_spec res = _export_root args kwargs out_flat out_spec = pytree tree_flatten res ExportTracerOutput out_flat out_spec ExportTracerOutput = namedtuple ExportTracerOutput flat_args out_spec mypy disable-error-code= no-untyped-def var-annotated assignment index operator DynamoGraphTransformer torch fx Transformer Graph transformer dynamo export flattens inputs outputs without complex matching __init__ module torch fx GraphModule flat_inputs list Any flat_args_dynamic_dims list set int graph_input_order dict int int graph_output_map dict int tuple str Any fake_mode Optional Any = None - None super __init__ module assert len flat_args_dynamic_dims == len flat_inputs flat_inputs = flat_inputs flat_args_dynamic_dims = flat_args_dynamic_dims graph_input_order = graph_input_order graph_output_map = graph_output_map fake_mode = fake_mode Get original placeholders output placeholders = n n module graph nodes n op == placeholder output_node = next n n module graph nodes n op == output Create new flattened input placeholders new_input_nodes dict int torch fx Node = _create_flattened_inputs Iterator replacing old placeholders old_to_new_mapping = _create_placeholder_mapping _create_flattened_inputs - None Create new placeholder nodes flattened inputs proper fake tensors i range len flat_inputs placeholder = super placeholder f arg_ i Check user input index i maps graph placeholder i graph_input_order graph_input_order i gives us which graph placeholder user input corresponds graph_placeholder_idx = graph_input_order i graph_placeholder_idx len placeholders orig_placeholder = placeholders graph_placeholder_idx Copy other metadata val yet key value orig_placeholder meta items key = val placeholder node meta key = value Always ensure we have proper val metadata fake tensor fake_mode None isinstance flat_inputs i torch Tensor placeholder node meta val = fake_mode from_tensor flat_inputs i symbolic_context=StatelessSymbolicContext dynamic_sizes= DimDynamic DYNAMIC d flat_args_dynamic_dims i DimDynamic STATIC d range len flat_inputs i shape constraint_sizes= None len flat_inputs i shape hasattr flat_inputs i val _IntWrapper case placeholder node meta val = flat_inputs i val placeholder node meta val = flat_inputs i pyrefly ignore unsupported-operation new_input_nodes i = placeholder _create_placeholder_mapping - None Create mapping old placeholders new ones graph_input_order maps user_input_index - graph_placeholder_index We need create old_graph_placeholder - new_user_input_placeholder user_input_idx graph_placeholder_idx graph_input_order items graph_placeholder_idx len placeholders old_placeholder = placeholders graph_placeholder_idx new_placeholder = new_input_nodes user_input_idx old_to_new_mapping old_placeholder = new_placeholder placeholder target args kwargs - Any Replace old placeholders new flattened ones Return corresponding new placeholder current_node old_to_new_mapping new_arg = old_to_new_mapping current_node Copy over additional metadata current node don t overwrite val key tensor_dict example_value unbacked_bindings key current_node meta new_arg node meta key = current_node meta key Only copy val we don t already have good one val current_node meta val new_arg node meta new_arg node meta val = current_node meta val new_arg Shouldn t happen mapping correct fallback super placeholder target args kwargs output target args kwargs - Any Transform output according graph_output_map original_outputs = args Build new output list based graph_output_map new_outputs = i sorted graph_output_map keys output_type val = graph_output_map i output_type == graph_out new_outputs append original_outputs val output_type == input input_idx = val index new_outputs append new_input_nodes input_idx output_type == constant new_outputs append val super output target tuple new_outputs run_node node Node - Any Run node transformation preserve metadata current_node = node result = super run_node node Copy important metadata hasattr result node result node node key val example_value unbacked_bindings key node meta result node meta key = node meta key Preserve node names except output node op = output hasattr node name result node _rename node name result transform - torch fx GraphModule Perform graph transformation copy module metadata result_gm = super transform Copy module metadata like original implementation hasattr module meta pyrefly ignore unsupported-operation dynamo_flat_name_to_original_fqn module meta pyrefly ignore index-error result_gm meta dynamo_flat_name_to_original_fqn = module meta pyrefly ignore index-error dynamo_flat_name_to_original_fqn pyrefly ignore unsupported-operation dynamo_compile_id module meta pyrefly ignore index-error result_gm meta dynamo_compile_id = module meta pyrefly ignore index-error dynamo_compile_id result_gm _suggest_or_raise_constraint_violation module_to_trace torch nn Module orig_callable Callable type ignore type-arg fake_mode Optional FakeTensorMode graph_capture_output CaptureOutput args Any kwargs Any dynamic_shapes Optional Union dict str Any tuple Any list Any constraint_violation_error = None try Check we have any constraint violations fn _ = get_traced_fn module_to_trace graph_capture_output graph_capture_output build_guards fn __code__ except ConstraintViolationError e constraint_violation_error = e shape_env = getattr fake_mode shape_env None None dim_constraints = shape_env dim_constraints None isinstance module_to_trace forward torch _ops OpOverloadPacket &#124; torch _ops OpOverload dim_constraints solve forced_specializations = dim_constraints forced_specializations msg = dim_constraints prettify_results inspect signature orig_callable type ignore attr-defined dynamic_shapes constraint_violation_error forced_specializations constraint_violation_error constraint_violation_error args = constraint_violation_error args + msg forced_specializations constraint_violation_error = ConstraintViolationError msg log info Summary dimension constraints s msg Error we have any constraints static values k shape_env var_to_range keys isinstance k sympy Integer constraint_violation_error = ConstraintViolationError f join traceback format_list shape_env var_to_stack k \n It appears you re trying set constraint f value which we evaluated have static value k Set TORCH_LOGS= +export more information constraint_violation_error constraint_violation_error = post_process_error_msg constraint_violation_error orig_callable args kwargs raise constraint_violation_error dataclass frozen=True PyTreeifyOutput graph_module torch fx GraphModule in_spec TreeSpec in_shuffle_graph torch fx GraphModule num_flat_args int out_spec TreeSpec out_shuffle_graph torch fx GraphModule root Optional torch nn Module = None pytreeify out CaptureOutput mod Any args tuple Any kwargs dict str Any - PyTreeifyOutput Given dynamo capture output callable graph module contain following information input output pytree spec input output shuffle functions Input shuffle functions converters taking pytree falttened inputs reorder them calling convention dynamo raw graph module Output shuffle functions converters taking outputs dynamo raw graph module convert them pytree format This function will replay any side effects happened during bytecode so important check against side effects before calling function assert out backend_input None backend_input = out backend_input backend = out backend_input graph_module root = None isinstance mod torch nn Module args = mod + args root = mod inspect ismethod mod args = mod __self__ + args root = mod __self__ flat_real_args in_spec = pytree tree_flatten args kwargs Yield Exception pass InShuffle torch nn Module __init__ super __init__ mod = mod num_inputs = len flat_real_args gm_inputs = None forward flat_proxy_args args kwargs = pytree tree_unflatten flat_proxy_args i i range num_inputs in_spec backend_dummy example_inputs gm_inputs = example_inputs raise Yield backend_input graph_module = backend_dummy type ignore assignment try out forward_callable args kwargs except Yield assert gm_inputs None gm_inputs finally backend_input graph_module = backend raise RuntimeError fake_mode = torch _dynamo utils detect_fake_mode flat_real_args fake_mode fake_mode shape_env None fake_mode shape_env = ShapeEnv in_shuffle_graph = make_fx InShuffle tracing_mode= symbolic proxy_module_inputs=True flat_real_args output_node = next iter reversed backend_input graph_module graph nodes OutShuffle torch nn Module __init__ super __init__ num_inputs = len flat_real_args num_outputs = len output_node args out_spec Optional TreeSpec = None forward flat_proxy_args args kwargs = pytree tree_unflatten flat_proxy_args i i range num_inputs in_spec backend_dummy example_inputs flat_proxy_args num_inputs + i i range num_outputs backend_input graph_module = backend_dummy type ignore assignment try results = out forward_callable args kwargs finally backend_input graph_module = backend ret out_spec = pytree tree_flatten results ret out_shuffle = OutShuffle flat_out_shuffle_args = flat_real_args pytree tree_map_only torch fx Node lambda x fake_mode from_tensor x meta example_value fake_mode x meta example_value output_node args fake_mode = torch _dynamo utils detect_fake_mode flat_out_shuffle_args fake_mode fake_mode shape_env None fake_mode shape_env = ShapeEnv out_shuffle_graph = make_fx out_shuffle tracing_mode= symbolic proxy_module_inputs=True flat_out_shuffle_args assert out_shuffle out_spec None PyTreeifyOutput backend_input graph_module in_spec in_shuffle_graph len flat_real_args out_shuffle out_spec out_shuffle_graph root=root type ignore arg-type normalize_graph_module gm node gm graph nodes node op == placeholder node meta val = node meta example_value dynamo_graph_capture_for_export mod Callable Any constraints Optional list Constraint = None - Callable Any inner args Any kwargs Any - Any assert torch _dynamo config install_free_tensors get_metrics_context dynamo_timed fullgraph_capture out = fullgraph_capture mod args kwargs constraints=constraints TODO filter out side effects pyt = pytreeify out mod args kwargs graph_module = pyt graph_module tree_leaf_names = graph_module graph _graph_namespace create_name f _tree_leaf_ i None i range pyt num_flat_args graph_module graph _codegen = _ExportCodeGen _PyTreeInfo TODO we should able use names dynamo graph directly argument_names inspect signature mod args kwargs pyt in_spec pyt out_spec pyt in_shuffle_graph pyt out_shuffle_graph tree_leaf_names pyt root type ignore attr-defined normalize_graph_module graph_module pyt root None graph_module _parameters = pyt root _parameters copy graph_module _buffers = pyt root _buffers copy assert all hasattr graph_module m m pyt root _modules graph_module _modules update pyt root _modules graph_module _non_persistent_buffers_set = pyt root _non_persistent_buffers_set copy graph_module _in_spec = pyt in_spec graph_module _out_spec = pyt out_spec assert hasattr graph_module _in_shuffle_graph assert hasattr graph_module _out_shuffle_graph graph_module _in_shuffle_graph = pyt in_shuffle_graph graph_module _out_shuffle_graph = pyt out_shuffle_graph delattr graph_module _param_name_to_source graph_module recompile graph_module meta module_call_specs = out graph_capture_output output_graph export_metadata module_call_spec assert out backend_input None graph_module meta fake_mode = out backend_input fake_mode type ignore attr-defined graph_module inner _dynamo_graph_capture_for_export mod Callable Any constraints Optional list Constraint = None dynamic_shapes Optional Union dict str Any tuple Any list Any = None - Callable torch fx GraphModule Improved dynamo graph capture using transformer approach proper fake tensor handling This function creates capture instance handles PyTree flattening unflattening proper input ordering Dynamo graph capture export-specific context FX graph transformation export compatibility Proper fake tensor metadata preservation Dynamic dimension constraint handling Notable improvements over manual approach - Uses FX Transformer cleaner graph manipulation - Properly handles fake tensor metadata dynamic dimensions - Preserves all necessary metadata export - More robust error handling edge case management TODO Are we actually gonna run bytecode Need attach guards _dynamic_shapes = dynamic_shapes _constraints = constraints inner args Any kwargs Any - torch fx GraphModule This sets is_exporting flag when building guards _compiling_state_context flat_inputs in_spec = pytree tree_flatten args kwargs check_user_input_output flat_inputs UserErrorType INVALID_INPUT module_to_trace = ModuleToTrace mod in_spec orig_callable = mod forward isinstance mod torch nn Module mod constraints Optional list Constraint = _constraints dynamic_shapes Optional Union dict str Any tuple Any list Any = _dynamic_shapes reset type ignore attr-defined reset dynamo_config_ctx = torch _dynamo config patch specialize_int=True specialize_float=True assume_static_by_default=True automatic_dynamic_shapes=False capture_dynamic_output_shape_ops=True capture_scalar_outputs=True constant_fold_autograd_profiler_enabled=True log_graph_in_out_metadata=True install_free_tensors ensures params buffers still added graph attributes makes Dynamo emits graphs follow export pytree-able input requirements In future we fully rely bytecode runtime we can turn flag off install_free_tensors=torch _dynamo config install_free_tensors_for_export get_metrics_context dynamo_timed fullgraph_capture dynamo_config_ctx out = fullgraph_capture module_to_trace tuple flat_inputs constraints=_constraints _is_export_deprecated_do_not_use=True assert out graph_capture_output output_graph None example_inputs list Any = out backend_input None graph = out backend_input graph_module fake_mode = out backend_input fake_mode example_inputs = out backend_input example_inputs graph = torch fx GraphModule torch nn Module torch fx Graph graph graph output None graph recompile fake_mode = None _suggest_or_raise_constraint_violation module_to_trace orig_callable fake_mode out args kwargs dynamic_shapes Extract export metadata new location export_metadata = out graph_capture_output output_graph export_metadata graph_inputs = export_metadata graph_input_idx_to_local_source graph_output_map = export_metadata output_return_type out_spec = export_metadata out_spec module_call_spec = export_metadata module_call_spec Compute dynamic dimensions each input based constraints flat_args_dynamic_dims = c dim c constraints c t_id == id x isinstance c _RelaxedConstraint c constraint_range vr lower = c constraint_range vr upper x flat_inputs Create input order mapping dynamo s internal order user order graph_input_order dict int int = inp graph_inputs source = graph_inputs inp assert isinstance source torch _dynamo source GetItemSource graph_input_order source index = len graph_input_order real_idx graph_idx graph_input_order items flat_inputs real_idx = example_inputs graph_idx Use FX transformer rebuild graph cleanly transformed_graph = DynamoGraphTransformer graph flat_inputs flat_args_dynamic_dims graph_input_order graph_output_map fake_mode transform Set up PyTree codegen proper input output handling transformed_graph graph _codegen = _PyTreeCodeGen _PyTreeInfo argument_names inspect signature orig_callable args kwargs type ignore attr-defined arg-type in_spec out_spec transformed_graph recompile clean_nn_module_stack_and_source_fn transformed_graph torch _dynamo config inline_inbuilt_nn_modules clean_export_root transformed_graph transformed_graph meta module_call_specs = module_call_spec transformed_graph meta fake_mode = fake_mode transformed_graph inner