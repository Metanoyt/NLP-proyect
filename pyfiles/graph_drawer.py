mypy allow-untyped-defs hashlib itertools chain types ModuleType typing Any Optional TYPE_CHECKING torch torch fx torch fx _compatibility compatibility torch fx graph _parse_stack_trace torch fx node _format_arg _get_qualified_name torch fx operator_schemas normalize_function torch fx passes shape_prop TensorMetadata TYPE_CHECKING pydot HAS_PYDOT = True pydot Optional ModuleType try pydot HAS_PYDOT = True except ModuleNotFoundError HAS_PYDOT = False pydot = None __all__ = FxGraphDrawer _COLOR_MAP = placeholder AliceBlue call_module LemonChiffon get_param Yellow get_attr LightGrey output PowderBlue _HASH_COLOR_MAP = CadetBlue Coral DarkOliveGreen DarkSeaGreen GhostWhite Khaki LavenderBlush LightSkyBlue MistyRose MistyRose PaleTurquoise PeachPuff Salmon Thistle Thistle Wheat _WEIGHT_TEMPLATE = fillcolor Salmon style filled rounded fontcolor HAS_PYDOT compatibility is_backward_compatible=False FxGraphDrawer Visualize torch fx Graph graphviz Basic usage g = FxGraphDrawer symbolic_traced resnet g get_dot_graph write_svg svg __init__ graph_module torch fx GraphModule name str ignore_getattr bool = False ignore_parameters_and_buffers bool = False skip_node_names_in_args bool = True parse_stack_trace bool = False dot_graph_shape Optional str = None normalize_args bool = False _name = name dot_graph_shape = dot_graph_shape dot_graph_shape None record normalize_args = normalize_args _WEIGHT_TEMPLATE shape = dot_graph_shape _dot_graphs = name _to_dot graph_module name ignore_getattr ignore_parameters_and_buffers skip_node_names_in_args parse_stack_trace node graph_module graph nodes node op = call_module continue leaf_node = _get_leaf_node graph_module node isinstance leaf_node torch fx GraphModule continue _dot_graphs f name _ node target = _to_dot leaf_node f name _ node target ignore_getattr ignore_parameters_and_buffers skip_node_names_in_args parse_stack_trace get_dot_graph submod_name=None - pydot Dot Visualize torch fx Graph graphviz Example xdoctest +REQUIRES module pydot xdoctest +REQUIRES module ubelt define module MyModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x clamp min= max= module = MyModule trace module symbolic_traced = torch fx symbolic_trace module setup output file ubelt ub dpath = ub Path appdir torch tests FxGraphDrawer ensuredir fpath = dpath linear svg draw graph g = FxGraphDrawer symbolic_traced linear g get_dot_graph write_svg fpath submod_name None get_main_dot_graph get_submod_dot_graph submod_name get_main_dot_graph - pydot Dot _dot_graphs _name get_submod_dot_graph submod_name - pydot Dot _dot_graphs f _name _ submod_name get_all_dot_graphs - dict str pydot Dot _dot_graphs _get_node_style node torch fx Node - dict str str template = shape dot_graph_shape fillcolor #CAFFE style filled rounded fontcolor node op _COLOR_MAP template fillcolor = _COLOR_MAP node op Use random color each node based its name so s stable target_name = node _pretty_print_target node target target_hash = int hashlib md target_name encode usedforsecurity=False hexdigest template fillcolor = _HASH_COLOR_MAP target_hash len _HASH_COLOR_MAP template _get_leaf_node module torch nn Module node torch fx Node - torch nn Module py_obj = module assert isinstance node target str atoms = node target split atom atoms hasattr py_obj atom raise RuntimeError str py_obj + does have attribute + atom + py_obj = getattr py_obj atom py_obj _typename target Any - str isinstance target torch nn Module ret = torch typename target isinstance target str ret = target ret = _get_qualified_name target Escape prevent dot files like https gist github com SungMinCho aab c d c d c aabc which triggers ` Error bad label format ` dot ret replace r \ replace r \ shorten path avoid drawing long boxes full path = home weif pytorch test py short path = pytorch test py _shorten_file_name full_file_name str truncate_to_last_n int = splits = full_file_name split len splits = truncate_to_last_n join splits -truncate_to_last_n full_file_name _get_node_label module torch fx GraphModule node torch fx Node skip_node_names_in_args bool parse_stack_trace bool - str _get_str_for_args_kwargs arg isinstance arg tuple prefix suffix = r &#124; args= \l r \n \l arg_strs_list = _format_arg max_list_len= arg isinstance arg dict prefix suffix = r &#124; kwargs= \l r \n \l arg_strs_list = f k _format_arg v max_list_len= k v arg items Fall back nothing unexpected case Strip out node names requested skip_node_names_in_args arg_strs_list = arg_strs_list len arg_strs_list == arg_strs = prefix + r \n join arg_strs_list + suffix len arg_strs_list == arg_strs = arg_strs replace r \l replace r \n arg_strs replace r \ replace r \ label = + f name= node name &#124; op_code= node op \n node op == call_module leaf_module = _get_leaf_node module node label += r \n + _typename leaf_module + r \n &#124; extra = hasattr leaf_module __constants__ extra = r \n join f c getattr leaf_module c c leaf_module __constants__ type ignore union-attr type ignore union-attr label += extra + r \n label += f &#124; target= _typename node target + r \n normalize_args try args kwargs = normalize_function type ignore misc node target type ignore arg-type node args type ignore arg-type node kwargs normalize_to_only_use_kwargs=True except Exception Fallback normalizing there s exception Some functions need overloads specified normalize args kwargs = node args node kwargs args kwargs = node args node kwargs len args label += _get_str_for_args_kwargs args len kwargs label += _get_str_for_args_kwargs kwargs label += f &#124; num_users= len node users + r \n tensor_meta = node meta get tensor_meta label += _tensor_meta_to_label tensor_meta original fx graph print buf=buf n_origin= buf_meta = node meta get buf_meta None buf_meta None label += f &#124; buf= buf_meta name + r \n label += f &#124; n_origin= buf_meta n_origin + r \n original fx graph print file lineno code parse_stack_trace node stack_trace None parsed_stack_trace = _parse_stack_trace node stack_trace fname = _shorten_file_name parsed_stack_trace file label += f &#124; file= fname parsed_stack_trace lineno parsed_stack_trace code + r \n label + _tensor_meta_to_label tm - str tm None isinstance tm TensorMetadata _stringify_tensor_meta tm isinstance tm list result = item tm result += _tensor_meta_to_label item result isinstance tm dict result = v tm values result += _tensor_meta_to_label v result isinstance tm tuple result = item tm result += _tensor_meta_to_label item result raise RuntimeError f Unsupported tensor meta type type tm _stringify_tensor_meta tm TensorMetadata - str result = hasattr tm dtype print tm tm result += &#124; + dtype + = + str tm dtype + r \n result += &#124; + shape + = + str tuple tm shape + r \n result += &#124; + requires_grad + = + str tm requires_grad + r \n result += &#124; + stride + = + str tm stride + r \n tm is_quantized assert tm qparams None assert qscheme tm qparams qscheme = tm qparams qscheme qscheme torch per_tensor_affine torch per_tensor_symmetric result += &#124; + q_scale + = + str tm qparams scale + r \n result += &#124; + q_zero_point + = + str tm qparams zero_point + r \n qscheme torch per_channel_affine torch per_channel_symmetric torch per_channel_affine_float_qparams result += &#124; + q_per_channel_scale + = + str tm qparams scale + r \n result += &#124; + q_per_channel_zero_point + = + str tm qparams zero_point + r \n result += &#124; + q_per_channel_axis + = + str tm qparams axis + r \n raise RuntimeError f Unsupported qscheme qscheme result += &#124; + qscheme + = + str tm qparams qscheme + r \n result _get_tensor_label t torch Tensor - str str t dtype + str list t shape + r \n when parse_stack_trace=True print file lineno code _to_dot graph_module torch fx GraphModule name str ignore_getattr bool ignore_parameters_and_buffers bool skip_node_names_in_args bool parse_stack_trace bool - pydot Dot Actual interface visualize fx Graph Note takes GraphModule instead Graph If ignore_parameters_and_buffers True parameters buffers created module will added nodes edges TB means top-to-bottom rank direction layout dot_graph = pydot Dot name rankdir= TB buf_name_to_subgraph = node graph_module graph nodes ignore_getattr node op == get_attr continue style = _get_node_style node dot_node = pydot Node node name label=self _get_node_label graph_module node skip_node_names_in_args parse_stack_trace style type ignore arg-type current_graph = dot_graph buf_meta = node meta get buf_meta None buf_meta None buf_meta n_origin buf_name = buf_meta name buf_name buf_name_to_subgraph buf_name_to_subgraph buf_name = pydot Cluster buf_name label=buf_name current_graph = buf_name_to_subgraph get buf_name type ignore assignment pyrefly ignore missing-attribute current_graph add_node dot_node get_module_params_or_buffers pname ptensor chain leaf_module named_parameters pyrefly ignore bad-argument-type leaf_module named_buffers pname = node name + + pname label = pname + &#124; op_code=get_ + parameter isinstance ptensor torch nn Parameter buffer + r \l dot_w_node = pydot Node pname label= + label + _get_tensor_label ptensor + _WEIGHT_TEMPLATE type ignore arg-type dot_graph add_node dot_w_node dot_graph add_edge pydot Edge pname node name node op == call_module leaf_module = _get_leaf_node graph_module node ignore_parameters_and_buffers isinstance leaf_module torch fx GraphModule get_module_params_or_buffers subgraph buf_name_to_subgraph values subgraph set color royalblue subgraph set penwidth dot_graph add_subgraph subgraph type ignore arg-type node graph_module graph nodes ignore_getattr node op == get_attr continue user node users dot_graph add_edge pydot Edge node name user name dot_graph TYPE_CHECKING compatibility is_backward_compatible=False FxGraphDrawer __init__ graph_module torch fx GraphModule name str ignore_getattr bool = False ignore_parameters_and_buffers bool = False skip_node_names_in_args bool = True parse_stack_trace bool = False dot_graph_shape Optional str = None normalize_args bool = False raise RuntimeError FXGraphDrawer requires pydot package installed Please install pydot through your favorite Python package manager