Owner s oncall distributed copy functools unittest collections abc Callable torch torch distributed dist torch nn nn torch distributed fsdp fully_shard torch distributed tensor experimental implicit_replication torch testing _internal common_distributed skip_if_lt_x_gpu skip_if_rocm_arch_multiprocess torch testing _internal common_fsdp FSDPTest get_devtype patch_all_gather patch_reduce_scatter torch testing _internal common_utils get_cycles_per_ms MI _ARCH run_tests TEST_HPU device_type = torch device get_devtype device_module = torch get_device_module device_type TestFullyShardOverlap FSDPTest NOTE Testing stream overlap PyTorch CI tricky One approach use CUDA sleeps emulate kernels each stream however ` ` torch cuda _sleep ` ` requires inputs units cycles The ` ` get_cycles_per_ms ` ` function convert ms cycles computed once cached thereafter which means there variation later cached value may accurate This leads flakiness CI To address we relax tests simple sanity checks overlapped times less than non-overlapped baseline we do test overlapped time less than precisely calculated time property world_size - int min torch get_device_module device_type device_count skip_if_rocm_arch_multiprocess MI _ARCH skip_if_lt_x_gpu unittest skipIf TEST_HPU Sleep supported HPU test_fully_shard_training_overlap torch manual_seed Use non-trivial comm time still shorter than compute time dim num_linears compute_sleep_ms comm_sleep_ms = model = nn Sequential LinearWithSleep dim compute_sleep_ms _ range num_linears ref_model = copy deepcopy model device_type lin model assert len list lin parameters == Expects only one weight fully_shard lin reshard_after_forward=True fully_shard model reshard_after_forward=True orig_all_gather_into_tensor = dist all_gather_into_tensor orig_reduce_scatter_tensor = dist reduce_scatter_tensor comm_stream = torch get_device_module device_type Stream delay_collective Share stream so all-gather reduce-scatter block each other like ` ProcessGroupNCCL ` comm_stream wait_stream torch get_device_module device_type current_stream torch get_device_module device_type stream comm_stream torch get_device_module device_type _sleep int comm_sleep_ms get_cycles_per_ms torch get_device_module device_type current_stream wait_stream comm_stream delayed_all_gather args kwargs delay_collective orig_all_gather_into_tensor args kwargs delayed_reduce_scatter args kwargs delay_collective orig_reduce_scatter_tensor args kwargs inp = torch randn dim device=device_type type loss = model inp sum warmup CUDA allocator loss backward ref_fwd patch_all_gather delayed_all_gather Run dummy all-gathers per weight which one FSDP group lin ref_model dummy_ag_output = torch empty_like lin weight dummy_ag_input = torch chunk dummy_ag_output world_size rank dist all_gather_into_tensor dummy_ag_output dummy_ag_input ref_model inp fwd patch_all_gather delayed_all_gather model inp ref_fwd_time = _time_fn ref_fwd fwd_time = _time_fn fwd Forward only st all-gather exposed NOTE Do enforce expected forward time due flakiness CI expected_fwd_time = comm_sleep_ms + num_linears compute_sleep_ms + buffer_ms assertLessEqual fwd_time ref_fwd_time ref_fwd_bwd patch_all_gather delayed_all_gather Run dummy all-gathers per weight which one FSDP group lin ref_model dummy_ag_output = torch empty_like lin weight dummy_ag_input = torch chunk dummy_ag_output world_size rank dist all_gather_into_tensor dummy_ag_output dummy_ag_input loss = ref_model inp sum Run dummy all-gathers per weight again since we resharding after forward lin ref_model dummy_ag_output = torch empty_like lin weight dummy_ag_input = torch chunk dummy_ag_output world_size rank dist all_gather_into_tensor dummy_ag_output dummy_ag_input loss backward Run dummy reduce-scatters per weight lin ref_model dummy_rs_input = torch empty_like lin weight dummy_rs_output = torch chunk dummy_rs_input world_size rank dist reduce_scatter_tensor dummy_rs_output dummy_rs_input fwd_bwd patch_all_gather delayed_all_gather patch_reduce_scatter delayed_reduce_scatter loss = model inp sum loss backward ref_fwd_bwd_time = _time_fn ref_fwd_bwd fwd_bwd_time = _time_fn fwd_bwd Backward only st all-gather last reduce-scatter exposed double backward compute since computing two gradients per layer NOTE Do enforce expected forward-backward time due flakiness CI expected_bwd_time = comm_sleep_ms + num_linears compute_sleep_ms + buffer_ms assertLessEqual fwd_bwd_time ref_fwd_bwd_time skip_if_lt_x_gpu unittest skipIf TEST_HPU Sleep supported HPU test_fully_shard_post_optim_event_overlap torch manual_seed Use non-trivial comm time still shorter than compute time dim compute_sleep_ms comm_sleep_ms = Define model have high-compute linear followed low-compute linear where only low-compute linear uses FSDP model = nn Sequential LinearWithSleep dim compute_sleep_ms nn Linear dim dim device_type fully_shard model reshard_after_forward=False optim = torch optim AdamW model parameters lr= e- orig_all_gather_into_tensor = dist all_gather_into_tensor delayed_all_gather args kwargs torch get_device_module device_type _sleep int comm_sleep_ms get_cycles_per_ms orig_all_gather_into_tensor args kwargs inp = torch randn dim device=device_type run_train_steps num_iters int use_post_optim_event bool _ range num_iters optim zero_grad patch_all_gather delayed_all_gather loss = model inp sum loss backward implicit_replication optim step use_post_optim_event post_optim_event = torch get_device_module device_type current_stream record_event model set_post_optim_event post_optim_event run_train_steps False warmup CUDA allocator num_iters = baseline_time = _time_fn functools partial run_train_steps num_iters False test_time = _time_fn functools partial run_train_steps num_iters True buffer_ms = CPU delays copies Baseline FSDP all-gather exposed since FSDP module waits current stream hence high-compute linear assertLessEqual baseline_time num_iters compute_sleep_ms + comm_sleep_ms + buffer_ms Test FSDP all-gather overlapped high-compute linear since FSDP module only waits post-optim event except st iteration when no event has been recorded expected_test_time = num_iters compute_sleep_ms + buffer_ms + comm_sleep_ms assertLessEqual test_time expected_test_time Since ` get_cycles_per_ms ` uses lru cache there may some variance between initially determined cycles vs current cycles per ms so we relax baseline check just greater than test time rather than expected test time assertGreater baseline_time test_time _time_fn fn Callable start_event = device_module Event enable_timing=True end_event = device_module Event enable_timing=True dist barrier device_module synchronize start_event record fn end_event record device_module synchronize elapsed_time = start_event elapsed_time end_event elapsed_time Matmul torch autograd Function Use CUDA sleeps emulate compute time staticmethod forward ctx input torch Tensor weight torch Tensor sleep_ms int ctx save_for_backward input weight ctx sleep_ms = sleep_ms torch get_device_module device_type _sleep int sleep_ms get_cycles_per_ms input weight staticmethod backward ctx grad_output torch Tensor input weight = ctx saved_tensors torch get_device_module device_type _sleep int ctx sleep_ms get_cycles_per_ms grad_input = grad_output weight T grad_weight = input T grad_output grad_input grad_weight None LinearWithSleep nn Module __init__ dim int sleep_ms int super __init__ weight = nn Parameter torch randn dim dim sleep_ms = sleep_ms forward x torch Tensor - torch Tensor nn functional relu Matmul apply x weight sleep_ms __name__ == __main__ run_tests