Python implementation ` ` __torch_function__ ` ` While most torch API handling ` ` __torch_function__ ` ` happens C++ level some torch API written Python so we need python-level handling ` ` __torch_function__ ` ` overrides well The main developer-facing functionality file handle_torch_function has_torch_function See torch functional py test test_overrides py usage examples Note ---- heavily inspired NumPy s ` ` __array_function__ ` ` see https github com pytorch pytorch issues https www numpy org neps nep- -array-function-protocol html If changing file way can affect ` ` __torch_function__ ` ` overhead please report benchmarks ` ` benchmarks overrides_benchmark ` ` See instructions ` ` README md ` ` directory __future__ noqa F collections contextlib functools types warnings collections abc Callable Iterable functools wraps typing Any Optional TypeVar typing_extensions ParamSpec torch torch _C _add_docstr _get_function_stack_at _has_torch_function _has_torch_function_unary _has_torch_function_variadic _is_torch_function_mode_enabled _len_torch_function_stack _pop_torch_function_stack _push_on_torch_function_stack __all__ = get_ignored_functions get_overridable_functions get_testing_overrides handle_torch_function has_torch_function resolve_name is_tensor_like is_tensor_method_or_property wrap_torch_function enable_reentrant_dispatch _P = ParamSpec _P _R = TypeVar _R _disable_user_warnings func Callable _P _R regex str = deprecated please use module str = torch - Callable _P _R Decorator temporarily disables ` ` UserWarning ` ` s given ` ` module ` ` warning message matches given ` ` regex ` ` pattern Arguments --------- func function Function disable warnings regex str A regex pattern compilable ` ` re compile ` ` This used match ` ` UserWarning ` ` message module str The python module which filtering should restricted Returns ------- function The wrapped function wraps func wrapper args _P args kwargs _P kwargs - _R warnings catch_warnings warnings filterwarnings ignore category=UserWarning message=regex module=module func args kwargs wrapper functools cache _disable_user_warnings get_ignored_functions - set Callable Return public functions cannot overridden ` ` __torch_function__ ` ` Returns ------- set Callable A tuple functions publicly available torch API cannot overridden ` ` __torch_function__ ` ` Mostly because none arguments these functions tensors tensor-likes Examples -------- torch Tensor as_subclass torch overrides get_ignored_functions True torch add torch overrides get_ignored_functions False Tensor = torch Tensor torch typename torch is_tensor torch is_storage torch set_default_tensor_type torch set_default_device torch get_default_device torch set_rng_state torch get_rng_state torch manual_seed torch initial_seed torch seed torch save torch load torch set_printoptions torch fork torch get_default_dtype torch get_num_interop_threads torch get_num_threads torch init_num_threads torch import_ir_module torch import_ir_module_from_buffer torch is_anomaly_enabled torch is_anomaly_check_nan_enabled torch is_grad_enabled torch merge_type_from_type_comment torch parse_ir torch parse_schema torch parse_type_comment torch set_anomaly_enabled torch set_flush_denormal torch set_num_interop_threads torch set_num_threads torch wait torch as_tensor torch from_numpy torch tensor torch default_generator torch has_cuda torch has_cudnn torch has_lapack torch device torch dtype torch finfo torch has_mkl torch has_mps torch has_mkldnn torch has_openmp torch iinfo torch memory_format torch qscheme torch set_grad_enabled torch no_grad torch enable_grad torch inference_mode torch is_inference_mode_enabled torch layout torch align_tensors torch arange torch as_strided torch bartlett_window torch blackman_window torch broadcast_shapes torch can_cast torch compile torch cudnn_affine_grid_generator torch cudnn_batch_norm torch cudnn_convolution torch cudnn_convolution_transpose torch cudnn_convolution_relu torch cudnn_convolution_add_relu torch cudnn_grid_sampler torch cudnn_is_acceptable torch empty torch empty_permuted torch empty_strided torch empty_quantized torch export export torch export load torch export register_dataclass torch export save torch eye torch fft fftfreq torch fft rfftfreq torch from_file torch full torch fill torch hamming_window torch hann_window torch kaiser_window torch linspace torch logspace torch mkldnn_adaptive_avg_pool d torch mkldnn_convolution torch mkldnn_max_pool d torch mkldnn_max_pool d torch mkldnn_linear_backward_weights torch mkldnn_rnn_layer torch normal torch ones torch promote_types torch rand torch randn torch randint torch randperm torch range torch result_type torch scalar_tensor torch sparse_coo_tensor torch sparse_compressed_tensor torch sparse_csr_tensor torch sparse_csc_tensor torch sparse_bsr_tensor torch sparse_bsc_tensor torch sym_constrain_range torch sym_constrain_range_for_size torch sym_fresh_size torch tril_indices torch triu_indices torch vander torch zeros torch _jit_internal boolean_dispatch torch nn functional assert_int_or_pair torch nn functional upsample torch nn functional upsample_bilinear torch nn functional upsample_nearest torch nn functional has_torch_function torch nn functional has_torch_function_unary torch nn functional has_torch_function_variadic torch nn functional handle_torch_function torch nn functional scaled_grouped_mm torch nn functional scaled_mm torch nn functional sigmoid torch nn functional hardsigmoid torch nn functional tanh torch nn functional _canonical_mask torch nn functional _none_or_dtype Doesn t actually take tensor arguments torch nn init calculate_gain These deprecated don t test them torch nn init uniform torch nn init normal torch nn init constant torch nn init eye torch nn init dirac torch nn init xavier_uniform torch nn init xavier_normal torch nn init kaiming_uniform torch nn init kaiming_normal torch nn init orthogonal torch nn init sparse torch nested to_padded_tensor has_torch_function handle_torch_function torch set_autocast_enabled torch is_autocast_enabled torch set_autocast_dtype torch get_autocast_dtype torch clear_autocast_cache torch set_autocast_cpu_enabled torch is_autocast_cpu_enabled torch set_autocast_xla_enabled torch is_autocast_xla_enabled torch set_autocast_ipu_enabled torch is_autocast_ipu_enabled torch set_autocast_cpu_dtype torch get_autocast_cpu_dtype torch set_autocast_ipu_dtype torch get_autocast_ipu_dtype torch get_autocast_gpu_dtype torch set_autocast_gpu_dtype torch get_autocast_xla_dtype torch set_autocast_xla_dtype torch autocast_increment_nesting torch autocast_decrement_nesting torch is_autocast_cache_enabled torch set_autocast_cache_enabled torch nn functional hardswish torch is_vulkan_available torch are_deterministic_algorithms_enabled torch use_deterministic_algorithms torch is_deterministic_algorithms_warn_only_enabled torch set_deterministic_debug_mode torch get_device_module torch get_deterministic_debug_mode torch set_float _matmul_precision torch get_float _matmul_precision torch unify_type_list torch is_warn_always_enabled torch set_warn_always torch vitals_enabled torch set_vital torch read_vitals torch vmap torch cond torch frombuffer torch asarray torch _functional_sym_constrain_range torch _make_dep_token Tensor __delitem__ Tensor __dir__ Tensor __getattribute__ Tensor __init__ Tensor __iter__ Tensor __init_subclass__ Tensor __delattr__ Tensor __setattr__ Tensor __torch_function__ Tensor __torch_dispatch__ Tensor __new__ Tensor __class__ Tensor __subclasshook__ Tensor __hash__ Tensor as_subclass Tensor eig Tensor lstsq Tensor reinforce Tensor new Tensor new_tensor Tensor new_empty Tensor new_empty_strided Tensor new_zeros Tensor new_ones Tensor new_full Tensor _make_subclass Tensor solve Tensor symeig Tensor stride Tensor unflatten Tensor to_sparse_coo Tensor to_sparse_csr Tensor to_sparse_csc Tensor to_sparse_bsr Tensor to_sparse_bsc Tensor _to_sparse Tensor _to_sparse_csr Tensor _to_sparse_csc Tensor _to_sparse_bsr Tensor _to_sparse_bsc Tensor _typed_storage Tensor _reduce_ex_internal Tensor _fix_weakref Tensor _view_func Tensor _view_func_unsafe Tensor _rev_view_func_unsafe Tensor _dtensor__new__ Tensor _make_wrapper_subclass Tensor _python_dispatch __get__ Tensor _has_symbolic_sizes_strides __get__ Tensor _conj Tensor _conj_physical Tensor _lazy_clone Tensor _neg_view Tensor _is_zerotensor Tensor _is_all_true Tensor _is_any_true Tensor _addmm_activation Tensor to_padded_tensor Tensor _use_count functools cache get_default_nowrap_functions - set Callable Return public functions do wrap subclass when invoked default ` ` Tensor __torch_function__ ` ` preserves subclasses Typically these functions represent field accesses i e retrieving Tensor stored somewhere Tensor opposed computation Users these functions expect object identity preserved over multiple accesses e g ` ` grad grad ` ` which cannot upheld we re wrapping fly every time furthermore tensor stored here might already subclass which case wrapping really ought happen Not ALL property accessors have property example ` ` Tensor T ` ` actually just creates new transposed tensor fly so we SHOULD interpose these calls you need check implementation function see case Additionally property accessor doesn t Tensor doesn t have list though harmless Tensor = torch Tensor Tensor _base __get__ Tensor grad __get__ Tensor _grad __get__ functools cache _disable_user_warnings get_testing_overrides - dict Callable Callable Return dict containing dummy overrides all overridable functions Returns ------- Dict Callable Callable A dictionary maps overridable functions PyTorch API lambda functions have same signature real function unconditionally - These lambda functions useful testing API coverage type defines ` ` __torch_function__ ` ` Examples -------- inspect my_add = torch overrides get_testing_overrides torch add inspect signature my_add Signature input other out=None Every function PyTorchAPI can overridden needs entry dict Optimally we would use inspect get function signature define lambda function procedurally blocked generating function signatures native kernels can consumed inspect See Issue Tensor = torch Tensor ret dict Callable Callable = torch abs lambda input out=None - torch absolute lambda input out=None - torch adaptive_avg_pool d lambda input output_size - torch adaptive_max_pool d lambda inputs output_size - torch acos lambda input out=None - torch adjoint lambda input - torch arccos lambda input out=None - torch acosh lambda input out=None - torch arccosh lambda input out=None - torch add lambda input other out=None - torch addbmm lambda input batch batch alpha= beta= out=None - torch addcdiv lambda input tensor tensor value= out=None - torch addcmul lambda input tensor tensor value= out=None - torch addmm lambda input mat mat beta= alpha= out=None - torch addmv lambda input mat vec beta= alpha= out=None - torch addr lambda input vec vec beta= alpha= out=None - torch affine_grid_generator lambda theta size align_corners - torch all lambda input dim=None - torch allclose lambda input other trol= e- atol= e- equal_nan=False - torch alpha_dropout lambda input p train inplace=False - torch amax lambda input dim=None - torch amin lambda input dim=None - torch aminmax lambda input dim=None keepdim=False out=None - torch angle lambda input out=None - torch any lambda input dim=None keepdim=False out=None - torch argmax lambda input - torch argmin lambda input - torch argsort lambda input dim=None - torch asin lambda input out=None - torch _assert_async lambda input msg - torch arcsin lambda input out=None - torch asinh lambda input out=None - torch arcsinh lambda input out=None - torch atan lambda input out=None - torch arctan lambda input out=None - torch atan lambda input other out=None - torch arctan lambda input other out=None - torch atanh lambda input out=None - torch arctanh lambda input out=None - torch atleast_ d lambda tensors - torch atleast_ d lambda tensors - torch atleast_ d lambda tensors - torch avg_pool d lambda input kernel_size stride=None padding= ceil_mode=False count_include_pad=True - torch baddbmm lambda input batch batch alpha= beta= out=None - torch batch_norm lambda input weight bias running_mean running_var training momentum eps cudnn_enabled - torch batch_norm_backward_elemt lambda grad_out input mean invstd weight sum_dy sum_dy_xmu count_tensor - torch batch_norm_backward_reduce lambda grad_out input mean invstd weight input_g weight_g bias_g - torch batch_norm_elemt lambda input weight bias mean invstd eps - torch batch_norm_gather_stats lambda input mean invstd running_mean running_var momentum eps count - torch batch_norm_gather_stats_with_counts lambda input mean invstd running_mean running_var momentum eps count - torch batch_norm_stats lambda input eps - torch batch_norm_update_stats lambda input running_mean running_var momentum - torch bernoulli lambda input generator=None out=None - torch bilinear lambda input input weight bias - torch binary_cross_entropy_with_logits lambda input target weight=None size_average=None reduce=None reduction= mean pos_weight=None - torch bincount lambda input weights=None minlength= - torch binomial lambda count prob generator=None - torch bitwise_and lambda input other out=None - torch bitwise_not lambda input out=None - torch bitwise_or lambda input other out=None - torch bitwise_xor lambda input other out=None - torch bitwise_left_shift lambda input other out=None - torch bitwise_right_shift lambda input other out=None - torch block_diag lambda tensors - torch bmm lambda input mat out_dtype=None out=None - torch broadcast_tensors lambda tensors - torch broadcast_to lambda size - torch bucketize lambda input boundaries out_int =False right=False out=None - torch cartesian_prod lambda tensors - torch cat lambda tensors dim= out=None - torch concat lambda tensors dim= out=None - alias torch cat torch concatenate lambda tensors dim= out=None - alias torch concatenate torch cdist lambda x x p= compute_mode= use_mm_for_euclid_dist_if_necessary - torch ceil lambda input out=None - torch celu lambda input alpha= inplace=False - torch chain_matmul lambda matrices out=None - torch channel_shuffle lambda input groups - torch cholesky lambda input upper=False out=None - torch linalg cholesky lambda input out=None - torch linalg cholesky_ex lambda input check_errors=False out=None - torch cholesky_inverse lambda input upper=False out=None - torch cholesky_solve lambda input input upper=False out=None - torch choose_qparams_optimized lambda input numel n_bins ratio bit_width - torch chunk lambda input chunks dim= - torch clamp lambda input min=None max=None out=None - torch clip lambda input min=None max=None out=None - torch clamp_min lambda input min out=None - torch clamp_max lambda input max out=None - torch column_stack lambda tensors out=None - torch cov lambda input correction= fweights=None aweights=None - torch clone lambda input - torch combinations lambda input r= with_replacement=False - torch complex lambda real imag - torch copysign lambda input other out=None - torch polar lambda abs ang - torch linalg cond lambda input ord=None - torch conj lambda input out=None - torch conj_physical lambda input out=None - torch resolve_conj lambda input out=None - torch resolve_neg lambda input out=None - torch constant_pad_nd lambda input pad value= - torch conv d lambda input weight bias=None stride= padding= dilation= groups= - torch conv d lambda input weight bias=None stride= padding= dilation= groups= - torch conv d lambda input weight bias=None stride= padding= dilation= groups= - torch convolution lambda input weight bias stride padding dilation transposed output_adding groups - torch conv_tbc lambda input weight bias pad= - torch conv_transpose d lambda input weight bias=None stride= padding= output_padding= groups= dilation= - torch conv_transpose d lambda input weight bias=None stride= padding= output_padding= groups= dilation= - torch conv_transpose d lambda input weight bias=None stride= padding= output_padding= groups= dilation= - torch corrcoef lambda input - torch cos lambda input out=None - torch cosine_embedding_loss lambda input input target margin= size_average=None reduce=None reduction= mean - torch cosh lambda input out=None - torch cosine_similarity lambda x x dim= eps= e- - torch count_nonzero lambda input - torch cross lambda input other dim=None out=None - torch linalg cross lambda input other dim=- out=None - torch ctc_loss lambda log_probs targets input_lengths target_lengths blank= reduction= mean zero_infinity=False - torch cummax lambda input dim out=None - torch cummin lambda input dim out=None - torch cumprod lambda input dim out=None dtype=None - torch cumsum lambda input dim out=None dtype=None - torch cumulative_trapezoid lambda y x=None dim=- - torch logcumsumexp lambda input dim out=None - torch deg rad lambda input out=None - torch dequantize lambda input - torch det lambda input - torch linalg det lambda input - alias torch det type ignore attr-defined torch detach lambda input - torch diag lambda input diagonal= out=None - torch diag_embed lambda input diagonal= out=None - torch diagflat lambda input offset= - torch diff lambda input n= dim=- prepend=None append=None out=None - torch diagonal lambda input offset= dim = dim = - torch linalg diagonal lambda input offset= dim =- dim =- - torch diagonal_scatter lambda input src offset= dim = dim = - torch as_strided_scatter lambda src size stride storage_offset=None - torch digamma lambda input out=None - torch dist lambda input other p= - torch div lambda input other rounding_mode=None out=None - torch divide lambda input other rounding_mode=None out=None - torch dot lambda input other out=None - torch dropout lambda input p train inplace=False - torch dsmm lambda input mat out_dtype=None - torch hsmm lambda mat mat - torch dsplit lambda input indices_or_sections - torch dstack lambda tensors out=None - torch linalg eig lambda input out=None - torch linalg eigvals lambda input out=None - torch linalg eigh lambda input UPLO= L out=None - torch linalg eigvalsh lambda input UPLO= L out=None - torch einsum lambda equation operands - torch embedding lambda input weight padding_idx=None max_norm=None norm_type= scale_grad_by_freq=False sparse=False - noqa B torch embedding_bag lambda input weight offsets max_norm=None norm_type= scale_grad_by_freq=False mode= mean sparse=False per_sample_weights=None padding_idx=None - noqa B torch empty_like lambda input dtype=None layout=None device=None requires_grad=False - torch eq lambda input other out=None - torch equal lambda input other - torch erf lambda input out=None - torch erfc lambda input out=None - torch erfinv lambda input out=None - torch exp lambda input out=None - torch exp lambda input out=None - torch expm lambda input out=None - torch fake_quantize_per_channel_affine lambda input scale zero_point axis quant_min quant_max - torch fake_quantize_per_tensor_affine lambda input scale zero_point quant_min quant_max - torch fused_moving_avg_obs_fake_quant lambda x observer_on fake_quant_on averaging_const running_min running_max scale zero_point quant_min quant_max ch_axis per_row_fake_quant=False symmetric_quant=False - noqa B torch fbgemm_linear_fp _weight lambda input packed_weight bias output - torch fbgemm_linear_fp _weight_fp _activation lambda input packed_weight bias output - torch fbgemm_linear_int _weight lambda input weight packed col_offsets weight_scale weight_zero_point bias - noqa B torch fbgemm_linear_int _weight_fp _activation lambda input weight packed col_offsets weight_scale weight_zero_point bias - torch fbgemm_linear_quantize_weight lambda input - torch fbgemm_pack_gemm_matrix_fp lambda input - torch fbgemm_pack_quantized_matrix lambda input b - torch feature_alpha_dropout lambda input p train - torch feature_dropout lambda input p train - torch fft ifft lambda input n=None dim=- norm=None - torch fft rfft lambda input n=None dim=- norm=None - torch fft irfft lambda input n=None dim=- norm=None - torch fft hfft lambda input n=None dim=- norm=None - torch fft ihfft lambda input n=None dim=- norm=None - torch fft hfft lambda input s=None dim= - - norm=None - torch fft ihfft lambda input s=None dim= - - norm=None - torch fft hfftn lambda input s=None dim=- norm=None - torch fft ihfftn lambda input s=None dim=- norm=None - torch fft fftn lambda input s=None dim=None norm=None - torch fft ifftn lambda input s=None dim=None norm=None - torch fft rfftn lambda input s=None dim=None norm=None - torch fft irfftn lambda input s=None dim=None norm=None - torch fft fft lambda input s=None dim= - - norm=None - torch fft ifft lambda input s=None dim= - - norm=None - torch fft rfft lambda input s=None dim= - - norm=None - torch fft irfft lambda input s=None dim= - - norm=None - torch fft fftshift lambda input dim=None - torch fft ifftshift lambda input dim=None - torch fft fft lambda input n=None dim=- norm=None - torch fix lambda input out=None - torch flatten lambda input start_dim= end_dim=- - torch flip lambda input dims - torch fliplr lambda input - torch flipud lambda input - torch frobenius_norm lambda input dim=None keepdim=False out=None - torch floor lambda input out=None - torch floor_divide lambda input other - torch float_power lambda input exponent out=None - torch fmod lambda input other out=None - torch frac lambda input out=None - torch frexp lambda input out=None - torch full_like lambda input fill_value out=None dtype=None layout=torch strided device=None requires_grad=False - noqa B torch _functional_assert_async lambda input msg dep_token - torch lu_unpack lambda LU_data LU_pivots unpack_data=True unpack_pivots=True - torch gather lambda input dim index out=None sparse_grad=False - torch gcd lambda input other out=None - torch ge lambda input other out=None - torch get_device lambda input - torch greater_equal lambda input other out=None - torch geqrf lambda input out=None - torch i lambda input out=None - torch inner lambda input other out=None - torch outer lambda input vec out=None - torch ger lambda input vec out=None - alias torch outer torch gradient lambda input spacing=None dim=None edge_order= - torch grid_sampler lambda input grid interpolation_mode padding_mode align_corners - torch grid_sampler_ d lambda input grid interpolation_mode padding_mode align_corners - torch grid_sampler_ d lambda input grid interpolation_mode padding_mode align_corners - torch group_norm lambda input num_groups weight=None bias=None eps= e- cudnn_enabled=True - torch gru lambda input hx params has_biases num_layers dropout train bidirectional batch_first - torch gru_cell lambda input hx w_ih w_hh b_ih=None b_hh=None - torch gt lambda input other out=None - torch greater lambda input other out=None - torch hardshrink lambda input lambd= - torch hash_tensor lambda input dim=None keepdim=False mode= out=None - torch heaviside lambda input values out=None - torch hinge_embedding_loss lambda input target margin= size_average=None reduce=None reduction= mean - noqa B torch histc lambda input bins= min= max= out=None - torch histogram lambda input bins= min=None max=None weight=None density=False out=None - torch histogramdd lambda input bins range=None weight=None density=False - torch linalg householder_product lambda input tau - torch hspmm lambda mat mat out=None - torch hsplit lambda input indices_or_sections - torch hstack lambda tensors out=None - torch hypot lambda input other out=None - torch igamma lambda input other out=None - torch igammac lambda input other out=None - torch imag lambda input out=None - torch index_add lambda input dim index source - torch index_copy lambda input dim index source - torch index_put lambda input indices values accumulate=False - torch index_select lambda input dim index out=None - torch index_fill lambda input dim index value - torch index_reduce lambda input dim index source reduce include_input=True - torch isfinite lambda tensor - torch isin lambda e te assume_unique=False invert=False - torch isinf lambda tensor - torch isreal lambda tensor - torch isposinf lambda input out=None - torch isneginf lambda input out=None - torch instance_norm lambda input running_mean running_var weight bias use_input_stats momentum eps cudnn_enabled - torch int_repr lambda input - torch inverse lambda input out=None - torch linalg inv lambda input out=None - torch linalg inv_ex lambda input check_errors=False out=None - torch is_complex lambda input - torch is_conj lambda input - torch is_neg lambda input - torch is_distributed lambda input - torch is_inference lambda input - torch is_floating_point lambda input - torch is_nonzero lambda input - torch is_same_size lambda input other - torch is_signed lambda input - torch isclose lambda input other rtol= e- atol= e- equal_nan=False - torch isnan lambda input - torch istft lambda input n_fft hop_length=None win_length=None window=None center=True normalized=False onesided=None length=None return_complex=False - noqa B torch kl_div lambda input target size_average=None reduce=None reduction= mean log_target=False - torch kron lambda input other - torch kthvalue lambda input k dim=None keepdim=False out=None - torch linalg ldl_factor_ex lambda input hermitian=False check_errors=False out=None - torch linalg ldl_factor lambda input hermitian=False out=None - torch linalg ldl_solve lambda LD pivots B hermitian=False out=None - torch layer_norm lambda input normalized_shape weight=None bias=None esp= e- cudnn_enabled=True - torch lcm lambda input other out=None - torch ldexp lambda input other out=None - torch le lambda input other out=None - torch less_equal lambda input other out=None - torch lerp lambda input end weight out=None - torch lgamma lambda input out=None - torch lobpcg lambda input k=None B=None X=None n=None iK=None niter=None tol=None largest=None method=None tracker=None ortho_iparams=None ortho_fparams=None ortho_bparams=None - noqa B torch log lambda input out=None - torch log_softmax lambda input dim dtype=None - torch log lambda input out=None - torch log p lambda input out=None - torch log lambda input out=None - torch logaddexp lambda input other out=None - torch logaddexp lambda input other out=None - torch logdet lambda input - torch xlogy lambda x y out=None - torch logical_and lambda input other out=None - torch logical_not lambda input out=None - torch logical_or lambda input other out=None - torch logical_xor lambda input other out=None - torch logit lambda input eps=None - torch logsumexp lambda input names keepdim=False out=None - torch lstm lambda data batch_sizes hx params has_biases num_layers dropout train bidirectional - torch lstm_cell lambda input hx w_ih w_hh b_ih=None b_hh=None - torch lt lambda input other out=None - torch less lambda input other out=None - torch lu lambda A pivot=True get_infos=False out=None - torch lu_solve lambda b LU_data LU_pivots out=None - torch margin_ranking_loss lambda input input target margin= size_average=None reduce=None reduction= mean - type ignore attr-defined noqa B torch masked_fill lambda input mask value - torch masked_scatter lambda input mask source - torch masked_select lambda input mask out=None - torch matmul lambda input other out=None - torch linalg lu lambda input pivot=True out=None - torch linalg lu_factor lambda input pivot=True out=None - torch linalg lu_factor_ex lambda input pivot=True check_errors=False out=None - torch linalg lu_solve lambda LU pivots B left=True adjoint=False out=None - torch linalg matmul lambda input other out=None - alias torch matmul torch matrix_power lambda input n - torch linalg matrix_power lambda input n out=None - torch linalg matrix_rank lambda input tol=None hermitian=False - torch linalg multi_dot lambda tensors out=None - torch matrix_exp lambda input - torch linalg matrix_exp lambda input - torch max lambda input out=None - torch maximum lambda input other out=None - torch fmax lambda input other out=None - torch max_pool d lambda input kernel_size stride=None padding= dilation= ceil_mode=False - torch max_pool d lambda input kernel_size stride=None padding= dilation= ceil_mode=False - torch max_pool d lambda input kernel_size stride=None padding= dilation= ceil_mode=False - torch max_pool d_with_indices lambda input kernel_size stride=None padding= dilation= return_indices=False ceil_mode=False - torch mean lambda input dim=None - torch nanmean lambda input dim=None keepdim=False dtype=None out=None - torch median lambda input dim=None - torch nanmedian lambda input dim=None - torch meshgrid lambda tensors kwargs - torch min lambda input out=None - torch minimum lambda input other out=None - torch fmin lambda input other out=None - torch miopen_batch_norm lambda input weight bias running_mean running_var training exponential_average_factor epsilon - torch miopen_convolution lambda input weight bias padding stride dilation groups benchmark deterministic - noqa B torch miopen_convolution_add_relu lambda input weight z alpha bias stride padding dilation groups - torch miopen_convolution_relu lambda input weight bias stride padding dilation groups - torch miopen_convolution_transpose lambda input weight bias padding output_padding stride dilation groups benchmark deterministic - torch miopen_depthwise_convolution lambda input weight bias padding stride dilation groups benchmark deterministic - torch miopen_rnn lambda input weight weight_stride hx cx mode hidden_size num_layers batch_first dropout train bidirectional batch_sizes dropout_state - noqa B torch mm lambda input mat out_dtype=None out=None - torch mode lambda input dim=- keepdim=False out=None - torch movedim lambda input source destination - torch moveaxis lambda input source destination - torch msort lambda input descending=False out=None - torch mul lambda input other out=None - torch multiply lambda input other out=None - torch multinomial lambda input num_samples replacement=False out=None - torch mv lambda input vec out=None - torch mvlgamma lambda input p - torch narrow lambda input dim start length - torch nan_to_num lambda input nan= posinf=None neginf=None out=None - torch native_batch_norm lambda input weight bias running_mean running_var training momentum eps - torch _native_batch_norm_legit lambda input weight bias training momentum eps - torch native_dropout lambda input p train - torch native_layer_norm lambda input normalized_shape weight=None bias=None eps= e- - torch _fused_rms_norm lambda input normalized_shape weight=None eps= e- - torch native_group_norm lambda input weight bias N C HxW group eps - torch native_norm lambda input p= dim=None keepdim=False dtype=None - torch native_channel_shuffle lambda input groups - torch ne lambda input other out=None - torch not_equal lambda input other out=None - torch neg lambda input out=None - torch negative lambda input out=None - torch nextafter lambda input other out=None - torch nn functional adaptive_avg_pool d lambda input output_size - torch nn functional adaptive_avg_pool d lambda input output_size - torch nn functional adaptive_max_pool d lambda input output_size return_indices=False - torch nn functional adaptive_max_pool d_with_indices lambda input output_size return_indices=False - torch nn functional adaptive_max_pool d lambda input output_size return_indices=False - torch nn functional adaptive_max_pool d_with_indices lambda input output_size return_indices=False - torch nn functional adaptive_max_pool d lambda input output_size return_indices=False - torch nn functional adaptive_max_pool d_with_indices lambda input output_size return_indices=False - torch nn functional affine_grid lambda theta size align_corners=None - torch nn functional alpha_dropout lambda input p= training=False inplace=False - torch nn functional avg_pool d lambda input kernel_size stride=None padding= ceil_mode=False count_include_pad=True divisor_override=None - noqa B torch nn functional avg_pool d lambda input kernel_size stride=None padding= ceil_mode=False count_include_pad=True divisor_override=None - noqa B torch nn functional batch_norm lambda input running_mean running_var weight=None bias=None training=False momentum= eps= e- - torch nn functional bilinear lambda input input weight bias=None - torch nn functional binary_cross_entropy lambda input target weight=None size_average=None reduce=None reduction= mean - torch nn functional binary_cross_entropy_with_logits lambda input target weight=None size_average=None reduce=None reduction= mean pos_weight=None - torch nn functional celu lambda input alpha= inplace=False - torch nn functional cosine_embedding_loss lambda input input target margin= size_average=None reduce=None reduction= mean - torch nn functional cross_entropy lambda input target weight=None size_average=None ignore_index=- reduce=None reduction= mean label_smoothing= - noqa B torch nn functional ctc_loss lambda log_probs targets input_lengths target_lengths blank= reduction= mean zero_infinity=False - torch nn functional dropout lambda input p= training=True inplace=False - torch nn functional dropout d lambda input p= training=True inplace=False - torch nn functional dropout d lambda input p= training=True inplace=False - torch nn functional dropout d lambda input p= training=True inplace=False - torch nn functional elu lambda input alpha= inplace=False - torch nn functional embedding lambda input weight padding_idx=None max_norm=None norm_type= scale_grad_by_freq=False sparse=False - noqa B torch nn functional embedding_bag lambda input weight offsets=None max_norm=None norm_type= scale_grad_by_freq=False mode= mean sparse=False per_sample_weights=None include_last_offset=False padding_idx=None - noqa B torch nn functional feature_alpha_dropout lambda input p= training=False inplace=False - torch nn functional fold lambda input output_size kernel_size dilation= padding= stride= - torch nn functional fractional_max_pool d lambda input kernel_size output_size=None output_ratio=None return_indices=False _random_samples=None - noqa B torch nn functional fractional_max_pool d_with_indices lambda input kernel_size output_size=None output_ratio=None return_indices=False _random_samples=None - noqa B torch nn functional fractional_max_pool d lambda input kernel_size output_size=None output_ratio=None return_indices=False _random_samples=None - noqa B torch nn functional fractional_max_pool d_with_indices lambda input kernel_size output_size=None output_ratio=None return_indices=False _random_samples=None - noqa B torch nn functional gaussian_nll_loss lambda input target var full=False eps= e- reduction= mean - torch nn functional gelu lambda input approximate= none - torch nn functional glu lambda input dim=- - torch nn functional grid_sample lambda input grid mode= bilinear padding_mode= zeros align_corners=None - noqa B torch nn functional group_norm lambda input num_groups weight=None bias=None eps= e- - torch nn functional gumbel_softmax lambda logits tau= hard=False eps= e- dim=- - torch nn functional hardshrink lambda input lambd= - torch nn functional hardtanh lambda input min_val=- max_val= inplace=False - torch nn functional hinge_embedding_loss lambda input target margin= size_average=None reduce=None reduction= mean - torch nn functional instance_norm lambda input running_mean=None running_var=None weight=None bias=None use_input_stats=True momentum= eps= e- - noqa B torch nn functional interpolate lambda input size=None scale_factor=None mode= nearest align_corners=None recompute_scale_factor=None antialias=False - noqa B torch nn functional kl_div lambda input target size_average=None reduce=None reduction= mean log_target=False - noqa B torch nn functional l _loss lambda input target size_average=None reduce=None reduction= mean weight=None - torch nn functional layer_norm lambda input normalized_shape weight=None bias=None eps= e- - torch nn functional leaky_relu lambda input negative_slope= inplace=False - torch nn functional linear lambda input weight bias=None - torch nn functional local_response_norm lambda input size alpha= beta= k= - torch nn functional log_softmax lambda input dim=None _stacklevel= dtype=None - torch nn functional logsigmoid lambda input - torch nn functional lp_pool d lambda input norm_type kernel_size stride=None ceil_mode=False - torch nn functional lp_pool d lambda input norm_type kernel_size stride=None ceil_mode=False - torch nn functional lp_pool d lambda input norm_type kernel_size stride=None ceil_mode=False - torch nn functional margin_ranking_loss lambda input input target margin= size_average=None reduce=None reduction= mean - torch nn functional max_pool d lambda input kernel_size stride=None padding= dilation= ceil_mode=False return_indices=False - torch nn functional max_pool d_with_indices lambda input kernel_size stride=None padding= dilation= return_indices=False ceil_mode=False - torch nn functional max_pool d lambda input kernel_size stride=None padding= dilation= ceil_mode=False return_indices=False - torch nn functional max_pool d_with_indices lambda input kernel_size stride=None padding= dilation= return_indices=False ceil_mode=False - torch nn functional max_pool d lambda input kernel_size stride=None padding= dilation= return_indices=False ceil_mode=False - torch nn functional max_pool d_with_indices lambda input kernel_size stride=None padding= dilation= return_indices=False ceil_mode=False - torch nn functional max_unpool d lambda input indices kernel_size stride=None padding= output_size=None - noqa B torch nn functional max_unpool d lambda input indices kernel_size stride=None padding= output_size=None - noqa B torch nn functional max_unpool d lambda input indices kernel_size stride=None padding= output_size=None - noqa B torch nn functional mse_loss lambda input target size_average=None reduce=None reduction= mean weight=None - torch nn functional multi_head_attention_forward lambda query key value embed_dim_to_check num_heads in_proj_weight in_proj_bias bias_k bias_v add_zero_attn dropout_p out_proj_weight out_proj_bias training=True key_padding_mask=None need_weights=True attn_mask=None use_separate_proj_weight=False q_proj_weight=None k_proj_weight=None v_proj_weight=None static_k=None static_v=None average_attn_weights=None is_causal=False - noqa B torch nn functional multi_margin_loss lambda input target p= margin= weight=None size_average=None reduce=None reduction= mean - torch nn functional multilabel_margin_loss lambda input target size_average=None reduce=None reduction= mean - torch nn functional multilabel_soft_margin_loss lambda input target weight=None size_average=None reduce=None reduction= mean - torch nn functional nll_loss lambda input target weight=None size_average=None ignore_index=- reduce=None reduction= mean - torch nn functional normalize lambda input p= dim= eps= e- out=None - torch nn functional one_hot lambda tensor num_classes=- - torch nn functional pad lambda input pad mode= constant value= - torch nn functional pairwise_distance lambda x x p= eps= e- keepdim=False - torch nn functional poisson_nll_loss lambda input target log_input=True full=False size_average=None eps= e- reduce=None reduction= mean - noqa B torch nn functional prelu lambda input weight - torch nn functional relu lambda input inplace=False - torch nn functional relu lambda input inplace=False - torch nn functional rms_norm lambda input normalized_shape weight=None eps= e- - torch nn functional rrelu lambda input lower= upper= training=False inplace=False - noqa B torch nn functional selu lambda input inplace=False - torch nn functional silu lambda input inplace=False - torch nn functional mish lambda input inplace=False - torch nn functional scaled_dot_product_attention lambda query key value attn_mask=None dropout_p= - torch nn functional smooth_l _loss lambda input target size_average=None reduce=None reduction= mean beta= - noqa B torch nn functional huber_loss lambda input target reduction= mean delta= weight=None - torch nn functional soft_margin_loss lambda input target size_average=None reduce=None reduction= mean - noqa B torch nn functional softmax lambda input dim=None _stacklevel= dtype=None - torch nn functional softmin lambda input dim=None _stacklevel= dtype=None - torch nn functional softplus lambda input beta= threshold= - torch nn functional softshrink lambda input lambd= - torch nn functional softsign lambda input - torch nn functional tanhshrink lambda input - torch nn functional threshold lambda input threshold value inplace=False - torch nn functional triplet_margin_loss lambda anchor positive negative margin= p= eps= e- swap=False size_average=None reduce=None reduction= mean - noqa B torch nn functional triplet_margin_with_distance_loss lambda anchor positive negative distance_function=None margin= swap=False reduction= mean - torch nn functional unfold lambda input kernel_size dilation= padding= stride= - torch nn init uniform_ lambda tensor a= b= generator=None - torch nn init normal_ lambda tensor mean= std= generator=None - torch nn init constant_ lambda tensor val - torch nn init kaiming_uniform_ lambda tensor a= mode= fan_in nonlinearity= leaky_relu generator=None - noqa B torch nonzero lambda input as_tuple=False - torch nonzero_static lambda input size fill_value=- - torch argwhere lambda input - torch norm lambda input p= fro dim=None keepdim=False out=None dtype=None - torch linalg norm lambda input ord=None dim=None keepdim=False out=None dtype=None - torch linalg vector_norm lambda input ord= dim=None keepdim=False out=None dtype=None - torch linalg matrix_norm lambda input ord= fro dim= - - keepdim=False out=None dtype=None - torch norm_except_dim lambda v pow= dim= - torch nuclear_norm lambda input p= fro dim=None keepdim=False out=None dtype=None - torch numel lambda input - torch orgqr lambda input tau - torch ormqr lambda input input input left=True transpose=False - torch pairwise_distance lambda x x p= eps= e- keepdim=False - torch permute lambda dim - torch pca_lowrank lambda input q=None center=True niter= - torch pdist lambda input p= - torch pinverse lambda input rcond= e- - torch linalg pinv lambda input rcond= e- hermitian=False - torch pixel_shuffle lambda input upscale_factor - torch pixel_unshuffle lambda input downscale_factor - torch poisson lambda input generator=None - torch poisson_nll_loss lambda input target log_input full eps reduction - torch polygamma lambda input n out=None - torch positive lambda input out=None - torch prelu lambda input weight - torch ones_like lambda input dtype=None layout=None device=None requires_grad=False - torch pow lambda input exponent out=None - torch prod lambda input dtype=None - torch put lambda input index source accumulate=False - torch q_per_channel_axis lambda input - torch q_per_channel_scales lambda input - torch q_per_channel_zero_points lambda input - torch q_scale lambda input - torch q_zero_point lambda input - torch qr lambda input some=True out=None - torch linalg qr lambda input mode= reduced out=None - torch quantile lambda input q dim=None keepdim=False interpolation= linear out=None - torch nanquantile lambda input q dim=None keepdim=False interpolation= linear out=None - torch quantize_per_channel lambda input scales zero_points axis dtype - torch quantize_per_tensor lambda input scale zero_point dtype - torch quantize_per_tensor_dynamic lambda input dtype reduce_range - torch quantized_batch_norm lambda input weight bias mean var eps output_scale output_zero_point - torch quantized_gru_cell lambda input hx w_ih w_hh b_ih b_hh packed_ih packed_hh col_offsets_ih col_offsets_hh scale_ih scale_hh zero_point_ih zero_point_hh - noqa B torch quantized_lstm_cell lambda input hx w_ih w_hh b_ih b_hh packed_ih packed_hh col_offsets_ih col_offsets_hh scale_ih scale_hh zero_point_ih zero_point_hh - noqa B torch quantized_max_pool d lambda input kernel_size stride= padding= dilation= ceil_mode=False - torch quantized_max_pool d lambda input kernel_size stride= padding= dilation= ceil_mode=False - torch quantized_max_pool d lambda input kernel_size stride= padding= dilation= ceil_mode=False - torch quantized_rnn_relu_cell lambda input hx w_ih w_hh b_ih b_hh packed_ih packed_hh col_offsets_ih col_offsets_hh scale_ih scale_hh zero_point_ih zero_point_hh - noqa B torch quantized_rnn_tanh_cell lambda input hx w_ih w_hh b_ih b_hh packed_ih packed_hh col_offsets_ih col_offsets_hh scale_ih scale_hh zero_point_ih zero_point_hh - noqa B torch rad deg lambda input out=None - torch rand_like lambda input dtype=None layout=None device=None requires_grad=False - torch randint_like lambda input high dtype=None layout=torch strided device=None requires_grad=False - torch randn_like lambda input dtype=None layout=None device=None requires_grad=False - torch ravel lambda input - torch real lambda input out=None - torch vdot lambda input other out=None - torch linalg vecdot lambda input other dim=- out=None - torch view_as_real lambda input - torch view_as_complex lambda input - torch reciprocal lambda input out=None - torch relu lambda input inplace=False - torch remainder lambda input other out=None - torch renorm lambda input p dim maxnorm out=None - torch repeat_interleave lambda input dim=None - torch reshape lambda input shape - torch rms_norm lambda input normalized_shape weight=None eps= e- - torch rnn_relu lambda input hx params has_biases num_layers dropout train bidirectional batch_first - noqa B torch rnn_relu_cell lambda input hx w_ih w_hh b_ih=None b_hh=None - torch rnn_tanh lambda input hx params has_biases num_layers dropout train bidirectional batch_first - noqa B torch rnn_tanh_cell lambda input hx w_ih w_hh b_ih=None b_hh=None - torch roll lambda input shifts dims=None - torch rot lambda input k= dims= - torch round lambda input out=None - torch row_stack lambda tensors out=None - alias torch vstack torch _rowwise_prune lambda weight mask compressed_indices_dtype - torch rrelu lambda input lower= upper= training=False inplace=False - torch rsqrt lambda input out=None - torch rsub lambda input other alpha= - torch saddmm lambda input mat mat beta= alpha= out=None - torch scatter lambda input dim index src - torch scatter_add lambda input dim index src - torch scatter_reduce lambda input dim index src reduce include_self=True - torch searchsorted lambda sorted_sequence input out_int =False right=False out=None - torch _segment_reduce lambda data reduce= max lengths=None indices=None offsets=None axis= unsafe=False - noqa B torch select lambda input dim index - torch select_scatter lambda input src dim index - torch slice_inverse lambda input src dim= start=None end=None step= - torch slice_scatter lambda input src dim= start=None end=None step= - torch selu lambda input inplace=False - torch sigmoid lambda input out=None - torch sign lambda input out=None - torch signbit lambda input out=None - torch sgn lambda input out=None - torch sin lambda input out=None - torch sinc lambda input out=None - torch sinh lambda input out=None - torch slogdet lambda input - torch linalg slogdet lambda input - torch smm lambda input mat out_dtype=None - torch spmm lambda input mat out_dtype=None - torch softmax lambda input dim dtype=None - torch linalg solve lambda A B left=True out=None - torch linalg solve_ex lambda A B left=True check_errors=False out=None - torch sort lambda input dim=- descending=False stable=False out=None - torch split lambda tensor split_size_or_sections dim= - torch split_with_sizes lambda tensor split_size_or_sections dim= - torch sqrt lambda input out=None - torch square lambda input out=None - torch squeeze lambda input dim=None out=None - torch sspaddmm lambda input mat mat beta= alpha= out=None - torch stack lambda tensors dim= out=None - torch std lambda input dim=None - torch std_mean lambda input dim=None - torch stft lambda input n_fft hop_length=None win_length=None window=None center=True pad_mode= reflect normalized=False onesided=True return_complex=None align_to_window=None - noqa B torch sub lambda input other out=None - torch subtract lambda input other out=None - torch sum lambda input dim=None - torch sym_float lambda input - torch sym_int lambda input - torch sym_max lambda b - torch sym_min lambda b - torch sym_not lambda input - torch sym_ite lambda b c - torch sym_sum lambda args - torch _sym_sqrt lambda input - torch _sym_cos lambda input - torch _sym_cosh lambda input - torch _sym_sin lambda input - torch _sym_sinh lambda input - torch _sym_tan lambda input - torch _sym_tanh lambda input - torch _sym_asin lambda input - torch _sym_acos lambda input - torch _sym_atan lambda input - torch nansum lambda input dim=None - torch svd lambda input some=True compute_uv=True out=None - torch svd_lowrank lambda input q= niter= M=None - torch linalg svd lambda input full_matrices=True out=None - torch linalg svdvals lambda input out=None - torch swapaxes lambda input dim dim - torch swapdims lambda input axis axis - torch special airy_ai lambda input - torch special bessel_j lambda input - torch special bessel_j lambda input - torch special bessel_y lambda input - torch special bessel_y lambda input - torch special chebyshev_polynomial_t lambda input n out=None - torch special chebyshev_polynomial_u lambda input n out=None - torch special chebyshev_polynomial_v lambda input n out=None - torch special chebyshev_polynomial_w lambda input n out=None - torch special digamma lambda input - torch special entr lambda input - torch special erf lambda input - torch special erfc lambda input - torch special erfcx lambda input - torch special erfinv lambda input - torch special exp lambda input - torch special expit lambda input - torch special expm lambda input - torch special gammainc lambda input other out=None - torch special gammaincc lambda input other out=None - torch special gammaln lambda input - torch special hermite_polynomial_h lambda input n out=None - torch special hermite_polynomial_he lambda input n out=None - torch special i lambda input - torch special i e lambda input - torch special i lambda input - torch special i e lambda input - torch special laguerre_polynomial_l lambda input n out=None - torch special legendre_polynomial_p lambda input n out=None - torch special log p lambda input - torch special log_ndtr lambda input - torch special log_softmax lambda input dim dtype=None - torch special logit lambda input - torch special logsumexp lambda input dim keepdim=False out=None - torch special modified_bessel_i lambda input - torch special modified_bessel_i lambda input - torch special modified_bessel_k lambda input - torch special modified_bessel_k lambda input - torch special multigammaln lambda input p - torch special ndtr lambda input - torch special ndtri lambda input - torch special polygamma lambda input n out=None - torch special psi lambda input - torch special round lambda input - torch special scaled_modified_bessel_k lambda input - torch special scaled_modified_bessel_k lambda input - torch special shifted_chebyshev_polynomial_t lambda input n out=None - torch special shifted_chebyshev_polynomial_u lambda input n out=None - torch special shifted_chebyshev_polynomial_v lambda input n out=None - torch special shifted_chebyshev_polynomial_w lambda input n out=None - torch special sinc lambda input - torch special softmax lambda input dim dtype=None - torch special spherical_bessel_j lambda input - torch special xlog py lambda input other out=None - torch special xlogy lambda input other out=None - torch special zeta lambda other out=None - torch t lambda input - torch take lambda input index - torch take_along_dim lambda input indices dim=None out=None - torch tan lambda input out=None - torch tanh lambda input out=None - torch linalg tensorinv lambda ind= - torch linalg tensorsolve lambda b dims=None - torch tensordot lambda b dims= out=None - torch tensor_split lambda input indices_or_sections dim= - torch threshold lambda input threshold value inplace=False - torch tile lambda input dims - torch topk lambda input k dim=- descending=False out=None - torch trace lambda input - torch transpose lambda input dim dim - torch trapz lambda y x=None dim=- - torch trapezoid lambda y x=None dim=- - torch triangular_solve lambda input A upper=True transpose=False unitriangular=False - torch linalg solve_triangular lambda input B upper left=True unitriangular=False - torch tril lambda input diagonal= out=None - torch triplet_margin_loss lambda anchor positive negative margin= p= eps= e- swap=False size_average=None reduce=None reduction= mean - noqa B torch triu lambda input diagonal= out=None - torch true_divide lambda input other - torch trunc lambda input out=None - torch unbind lambda input dim= - torch unflatten lambda input dim sizes names - torch unique lambda input sorted=True return_inverse=False return_counts=False dim=None - torch unique_consecutive lambda input return_inverse=False return_counts=False dim=None - torch unravel_index lambda indices shape - torch unsafe_chunk lambda input chunks dim= - torch unsafe_split lambda tensor split_size_or_sections dim= - torch unsafe_split_with_sizes lambda tensor split_size_or_sections dim= - torch unsqueeze lambda input dim out=None - torch linalg vander lambda x N=None - torch var lambda input dim=None - torch var_mean lambda input dim=None - torch vsplit lambda input indices_or_sections - torch vstack lambda tensors out=None - torch where lambda condition x=None y=None - torch _wrapped_linear_prepack lambda weight weight_scale weight_zero_point bias - torch _wrapped_quantized_linear_prepacked lambda input input_scale input_zero_point prepacked out_scale out_zero_point out_channel - noqa B torch zeros_like lambda input dtype=None layout=None device=None requires_grad=False - torch _fw_primal_copy lambda level - torch _make_dual_copy lambda primal tangent level - torch view_as_real_copy lambda - torch view_as_complex_copy lambda - torch _conj_copy lambda - torch _neg_view_copy lambda - torch as_strided_copy lambda size stride storage_offset=None - torch _sparse_broadcast_to_copy lambda size - torch diagonal_copy lambda offset= dim = dim = - torch expand_copy lambda size implicit=False - torch narrow_copy lambda dim start length - torch permute_copy lambda dims - torch _reshape_alias_copy lambda size stride - torch select_copy lambda dim index - torch detach_copy lambda - torch slice_copy lambda dim= start=None end=None step= - torch split_copy lambda split_size dim= - torch split_with_sizes_copy lambda split_sizes dim= - torch squeeze_copy lambda dim - torch t_copy lambda - torch transpose_copy lambda dim dim - torch unsqueeze_copy lambda dim - torch _indices_copy lambda - torch _values_copy lambda - torch indices_copy lambda - torch values_copy lambda - torch crow_indices_copy lambda - torch col_indices_copy lambda - torch ccol_indices_copy lambda - torch row_indices_copy lambda - torch unbind_copy lambda dim= - torch view_copy lambda dtype - torch unfold_copy lambda dimension size step - torch alias_copy lambda - Tensor __floordiv__ lambda other - Tensor __rfloordiv__ lambda other - Tensor __ifloordiv__ lambda other - Tensor __truediv__ lambda other - Tensor __rtruediv__ lambda other - Tensor __itruediv__ lambda other - Tensor __lshift__ lambda other - Tensor __rlshift__ lambda other - Tensor __ilshift__ lambda other - Tensor __rshift__ lambda other - Tensor __rrshift__ lambda other - Tensor __irshift__ lambda other - Tensor __and__ lambda other - Tensor __or__ lambda other - Tensor __xor__ lambda other - Tensor __float__ lambda - Tensor __complex__ lambda - Tensor __array__ lambda dtype - Tensor __bool__ lambda - Tensor __contains__ lambda other - Tensor __neg__ lambda - Tensor __invert__ lambda - Tensor __mod__ lambda other - Tensor __rmod__ lambda other - Tensor __imod__ lambda other - Tensor __array_wrap__ lambda array - Tensor __getitem__ lambda idx - Tensor __deepcopy__ lambda memo - Tensor __int__ lambda - Tensor __long__ lambda - Tensor __index__ lambda - Tensor __len__ lambda - Tensor __format__ lambda format_spec - Tensor __reduce_ex__ lambda proto - Tensor __reversed__ lambda - Tensor __repr__ lambda tensor_contents=None - Tensor __setitem__ lambda k v - Tensor __setstate__ lambda d - Tensor T __get__ lambda - Tensor H __get__ lambda - Tensor mT __get__ lambda - Tensor mH __get__ lambda - Tensor _backward_hooks __get__ lambda - Tensor _post_accumulate_grad_hooks __get__ lambda - Tensor _base __get__ lambda - Tensor _cdata __get__ lambda - Tensor grad __get__ lambda - Tensor _grad __get__ lambda - Tensor _grad_fn __get__ lambda - Tensor grad_fn __get__ lambda - Tensor grad_dtype __get__ lambda - Tensor _version __get__ lambda - Tensor _autocast_to_reduced_precision lambda cuda_enabled cpu_enabled cuda_dtype cpu_dtype - Tensor _autocast_to_full_precision lambda cuda_enabled cpu_enabled - Tensor _clear_non_serializable_cached_data lambda - Tensor data __get__ lambda - Tensor device __get__ lambda - Tensor dtype __get__ lambda - Tensor is_cuda __get__ lambda - Tensor is_cpu __get__ lambda - Tensor is_xla __get__ lambda - Tensor is_xpu __get__ lambda - Tensor is_ipu __get__ lambda - Tensor is_leaf __get__ lambda - Tensor retains_grad __get__ lambda - Tensor is_meta __get__ lambda - Tensor is_mps __get__ lambda - Tensor is_mtia __get__ lambda - Tensor is_nested __get__ lambda - Tensor is_maia __get__ lambda - Tensor is_mkldnn __get__ lambda - Tensor is_quantized __get__ lambda - Tensor is_sparse __get__ lambda - Tensor is_sparse_csr __get__ lambda - Tensor is_vulkan __get__ lambda - Tensor itemsize __get__ lambda - Tensor layout __get__ lambda - Tensor name __get__ lambda - Tensor names __get__ lambda - Tensor nbytes __get__ lambda - Tensor ndim __get__ lambda - Tensor output_nr __get__ lambda - Tensor requires_grad __get__ lambda - Tensor shape __get__ lambda - Tensor volatile __get__ lambda - Tensor real __get__ lambda - Tensor imag __get__ lambda - Tensor __cuda_array_interface__ __get__ lambda - Tensor type lambda dtype=None non_blocking=False kwargs - Tensor _dimI lambda - Tensor _dimV lambda - Tensor _indices lambda - Tensor _is_view lambda - Tensor _nnz lambda - Tensor crow_indices lambda - Tensor col_indices lambda - Tensor ccol_indices lambda - Tensor row_indices lambda - Tensor _update_names lambda names inplace - Tensor _values lambda - Tensor adjoint lambda - Tensor align_as lambda other - Tensor align_to lambda order ellipsis_idx - Tensor apply_ lambda callable - Tensor as_strided lambda size stride - Tensor as_strided_ lambda size stride - Tensor backward lambda gradient=None retain_graph=None create_graph=False inputs=None - Tensor bfloat lambda memory_format=torch preserve_format - Tensor bool lambda memory_format=torch preserve_format - Tensor byte lambda memory_format=torch preserve_format - Tensor char lambda memory_format=torch preserve_format - Tensor cauchy_ lambda median= sigma= generator=None - Tensor coalesce lambda - Tensor _coalesced_ lambda coalesced - Tensor contiguous lambda memory_format=torch contiguous_format - Tensor copy_ lambda src non_blocking=False - Tensor cpu lambda memory_format=torch preserve_format - Tensor cuda lambda memory_format=torch preserve_format - Tensor mtia lambda memory_format=torch preserve_format - Tensor xpu lambda memory_format=torch preserve_format - Tensor ipu lambda memory_format=torch preserve_format - Tensor data_ptr lambda - Tensor dense_dim lambda - Tensor diagonal_scatter lambda src offset= dim = dim = - Tensor dim lambda - Tensor dim_order lambda ambiguity_check=False - Tensor double lambda memory_format=torch preserve_format - Tensor cdouble lambda memory_format=torch preserve_format - Tensor element_size lambda - Tensor expand lambda size - Tensor expand_as lambda other - Tensor exponential_ lambda lambd= generator=None - Tensor fill_ lambda value - Tensor fill_diagonal_ lambda value - Tensor float lambda memory_format=torch preserve_format - Tensor cfloat lambda memory_format=torch preserve_format - Tensor geometric_ lambda p generator=None - Tensor get_device lambda - Tensor half lambda memory_format=torch preserve_format - Tensor chalf lambda memory_format=torch preserve_format - Tensor has_names lambda - Tensor indices lambda - Tensor int lambda memory_format=torch preserve_format - Tensor is_coalesced lambda - Tensor is_contiguous lambda - Tensor is_inference lambda - Tensor is_pinned lambda - Tensor is_set_to lambda tensor - Tensor is_shared lambda - Tensor item lambda - Tensor log_normal_ lambda mean= std= generator=None - Tensor log_softmax lambda dim - Tensor long lambda memory_format=torch preserve_format - Tensor map_ lambda tensor callable - Tensor map _ lambda x y callable - Tensor mm lambda mat out_dtype=None - Tensor module_load lambda other assign=False - Tensor narrow_copy lambda dimension start length - Tensor ndimension lambda - Tensor nelement lambda - Tensor _nested_tensor_size lambda - Tensor _nested_tensor_storage_offsets lambda - Tensor _nested_tensor_strides lambda - Tensor normal_ lambda - Tensor numpy lambda - Tensor permute lambda dim - Tensor pin_memory lambda - Tensor put_ lambda indices tensor accumulate=False - Tensor qscheme lambda - Tensor random_ lambda from_= to=None generator=None - Tensor record_stream lambda stream - Tensor refine_names lambda names - Tensor register_hook lambda hook - Tensor register_post_accumulate_grad_hook lambda hook - Tensor rename lambda name - Tensor repeat lambda size - Tensor requires_grad_ lambda requires_grad=True - Tensor reshape_as lambda other - Tensor resize lambda size - Tensor resize_ lambda size - Tensor resize_as lambda other - Tensor resize_as_sparse_ lambda other - Tensor retain_grad lambda - Tensor set_ lambda source=None storage_offset= size=None stride=None - Tensor select_scatter lambda src dim index - Tensor share_memory_ lambda - Tensor short lambda memory_format=torch preserve_format - Tensor size lambda - Tensor slice_scatter lambda src dim= start=None end=None step= - Tensor sparse_dim lambda - Tensor sparse_mask lambda mask - Tensor _sparse_mask_projection lambda mask accumulate_matches=False - Tensor sparse_resize_ lambda size size dense_dim - Tensor sparse_resize_and_clear_ lambda size size dense_dim - Tensor sspaddmm lambda mat mat beta= alpha= out=None - Tensor storage lambda - Tensor untyped_storage lambda - Tensor storage_offset lambda - Tensor storage_type lambda - Tensor sum_to_size lambda size - Tensor tile lambda reps - Tensor lambda dtype non_blocking=False copy=False memory_format=torch preserve_format - Tensor to_dense lambda dtype=None masked_grad=None - Tensor _to_dense lambda dtype=None masked_grad=None - Tensor to_sparse lambda - Tensor tolist lambda - Tensor to_mkldnn lambda - Tensor type_as lambda other - Tensor unfold lambda dimension size step - Tensor uniform_ lambda from_= to= - Tensor values lambda - Tensor view lambda shape - Tensor view_as lambda other - Tensor zero_ lambda - Tensor __dlpack__ lambda stream=None max_version=None dl_device=None copy=None - Tensor __dlpack_device__ lambda - Tensor index lambda b - torch linalg lstsq lambda b cond=None driver=None - fmt skip privateuse _backend_name = torch utils backend_registration _privateuse _backend_name hasattr Tensor privateuse _backend_name ret getattr Tensor privateuse _backend_name = lambda device=None non_blocking=False kwargs - ret getattr Tensor f is_ privateuse _backend_name __get__ = lambda - ret = ignored = get_ignored_functions k v ret items Generate methods like __add__ add_ default add names = k __name__ Default method k __name__ + _ Inplace variant __ + k __name__ + __ Dunder method __i + k __name__ + __ Inplace dunder method __r + k __name__ + __ Reverse dunder method k __name__ startswith bitwise_ bitwise_ op have dunder methods form __ op __ And so subname = k __name__ len bitwise_ names extend __ + subname + __ __i + subname + __ __r + subname + __ name names func = getattr Tensor name None callable func func ret func ignored ret func = v ret update ret ret wrap_torch_function dispatcher Callable Wraps given function ` ` __torch_function__ ` ` -related functionality Parameters ---------- dispatcher Callable A callable returns iterable Tensor-likes passed into function Note ---- This decorator may reduce performance your code Generally s enough express your code series functions themselves support __torch_function__ If you find yourself rare situation where case e g you re wrapping low-level library you also need work Tensor-likes then function available Examples -------- dispatcher Must have same signature func torch overrides wrap_torch_function dispatcher func This will make func dispatchable __torch_function__ + inner func functools wraps func wrapped args kwargs relevant_args = dispatcher args kwargs has_torch_function relevant_args handle_torch_function wrapped relevant_args args kwargs func args kwargs wrapped inner _get_overloaded_args relevant_args Iterable Any get_type_fn Optional Callable Any type = None - list Any Returns list arguments which call __torch_function__ Checks arguments relevant_args __torch_function__ implementations storing references arguments their types overloaded_args overloaded_types order calling precedence Only distinct types considered If type subclass another type will have higher precedence otherwise precedence order same order arguments relevant_args left-to-right argument list The precedence-determining algorithm implemented function described ` NEP- ` _ See torch append_overloaded_arg equivalent function C++ implementation Parameters ---------- relevant_args iterable array-like Iterable array-like arguments check __torch_function__ methods get_type_fn callable optional Function call each argument relevant_args get its type Returns ------- overloaded_args list Arguments relevant_args which call __torch_function__ methods order which they should called _NEP- https numpy org neps nep- -array-function-protocol html get_type_fn None get_type_fn = type If torch function enabled there no overloaded types torch _C _is_torch_function_enabled Runtime O num_arguments num_unique_types overloaded_types set type = set overloaded_args list Any = arg relevant_args arg_type = get_type_fn arg We only collect arguments they have unique type which ensures reasonable performance even long list possibly overloaded arguments NB Important exclude _disabled_torch_function_impl otherwise https github com pytorch pytorch issues arg_type overloaded_types hasattr arg_type __torch_function__ arg_type __torch_function__ torch _C _disabled_torch_function_impl Create lists explicitly first type usually only one done avoid setting up iterator overloaded_args overloaded_types overloaded_types add arg_type By default insert argument end subclass another argument insert before argument This ensures subclasses before superclasses index = len overloaded_args i old_arg enumerate overloaded_args issubclass arg_type get_type_fn old_arg index = i break overloaded_args insert index arg overloaded_types = arg_type overloaded_args = arg overloaded_args handle_torch_function public_api Callable relevant_args Iterable Any args kwargs - Any Implement function checks ` ` __torch_function__ ` ` overrides See torch autograd handle_torch_function equivalent function C++ implementation Arguments --------- public_api function Function exposed public torch API originally called like ` ` public_api args kwargs ` ` which arguments now being checked relevant_args iterable Iterable arguments check __torch_function__ methods args tuple Arbitrary positional arguments originally passed into ` ` public_api ` ` kwargs tuple Arbitrary keyword arguments originally passed into ` ` public_api ` ` Returns ------- object Result calling ` ` implementation ` ` ` ` __torch_function__ ` ` method appropriate Raises ------ TypeError no implementation found Example ------- func has_torch_function_unary handle_torch_function func + Check __torch_function__ methods overloaded_args = _get_overloaded_args relevant_args overloaded_args already have unique types types = tuple map type overloaded_args Check __torch_function__ mode _is_torch_function_mode_enabled we re here mode must set TorchFunctionStackMode unsets calls directly into TorchFunctionStackMode s torch function _pop_mode_temporarily mode result = mode __torch_function__ public_api types args kwargs result NotImplemented result Call overrides overloaded_arg overloaded_args This call needs become classmethod call future See https github com pytorch pytorch issues torch_func_method = overloaded_arg __torch_function__ hasattr torch_func_method __self__ torch_func_method __self__ overloaded_arg torch_func_method torch _C _disabled_torch_function_impl warnings warn Defining your ` __torch_function__ plain method deprecated will error future please define classmethod DeprecationWarning stacklevel= Use ` public_api ` instead ` implementation ` so __torch_function__ implementations can do equality identity comparisons result = torch_func_method public_api types args kwargs result NotImplemented result func_name = f public_api __module__ public_api __name__ msg = f no implementation found func_name types implement f __torch_function__ type arg arg overloaded_args _is_torch_function_mode_enabled msg += f nor mode _get_current_function_mode raise TypeError msg has_torch_function = _add_docstr _has_torch_function r Check __torch_function__ implementations elements iterable __torch_function__ mode enabled Considers exact ` ` Tensor ` ` s ` ` Parameter ` ` s non-dispatchable Use guard call func ` handle_torch_function ` don t use test something Tensor-like use func ` is_tensor_like ` instead Arguments --------- relevant_args iterable Iterable arguments check __torch_function__ methods Returns ------- bool True any elements relevant_args have __torch_function__ implementations False otherwise See Also ________ torch is_tensor_like Checks something Tensor-like including exact ` ` Tensor ` ` has_torch_function_unary = _add_docstr _has_torch_function_unary r Special case ` has_torch_function ` single inputs Instead ` has_torch_function t ` call ` has_torch_function_unary t ` which skips unnecessary packing unpacking work has_torch_function_variadic = _add_docstr _has_torch_function_variadic r Special case ` has_torch_function ` skips tuple creation This uses METH_FASTCALL protocol introduced Python Instead ` has_torch_function b ` call ` has_torch_function_variadic b ` which skips unnecessary packing unpacking work functools cache _get_overridable_functions - tuple dict Any list Callable dict Callable str overridable_funcs = collections defaultdict list index = tested_namespaces = torch torch torch __all__ torch functional torch functional torch functional __all__ torch nn functional torch nn functional dir torch nn functional torch nn init torch nn init dir torch nn init torch Tensor torch Tensor dir torch Tensor torch linalg torch linalg dir torch linalg torch fft torch fft dir torch fft torch special torch special dir torch special namespace_str namespace ns_funcs tested_namespaces func_name ns_funcs ignore = False ignore private functions functions deleted torch __init__ namespace torch Tensor func_name startswith __ continue func_name startswith _ ignore = True func_name endswith _ ignore = True func_name islower ignore = True func_name == unique_dim continue func = getattr namespace func_name getattr object func_name None == func continue func_name == __weakref__ continue func = getattr namespace func_name namespace torch Tensor getattr object func_name None == func continue ignore re-exported modules isinstance func types ModuleType continue ignore __future__ imports isinstance func __future__ _Feature continue callable func hasattr func __get__ index func __get__ = f namespace_str func_name __get__ index func __set__ = f namespace_str func_name __set__ ignore continue func __get__ get_ignored_functions msg = tuple returned torch _overrides get_ignored_functions still has explicit override assert func __get__ get_testing_overrides msg format namespace func __name__ continue overridable_funcs func append func __get__ continue callable func continue index func = f namespace_str func_name ignore continue cannot overridden __torch_function__ func get_ignored_functions msg = tuple returned torch _overrides get_ignored_functions still has explicit override assert func get_testing_overrides msg format namespace func __name__ continue overridable_funcs namespace append func overridable_funcs index _disable_user_warnings get_overridable_functions - dict Any list Callable List functions overridable via __torch_function__ Returns ------- Dict Any List Callable A dictionary maps namespaces contain overridable functions functions namespace can overridden _get_overridable_functions _disable_user_warnings resolve_name f Get human readable string name function passed __torch_function__ Arguments --------- f Callable Function resolve name Returns ------- str Name function eval ed should give back input function isinstance f torch _ops OpOverload torch _ops OpOverloadPacket str f _get_overridable_functions get f functools cache _get_tensor_methods - set Callable Returns set overridable methods ` ` torch Tensor ` ` overridable_funcs = get_overridable_functions methods = set overridable_funcs torch Tensor methods _disable_user_warnings is_tensor_method_or_property func Callable - bool Returns True function passed handler method property belonging ` ` torch Tensor ` ` passed into ` ` __torch_function__ ` ` note For properties their ` ` __get__ ` ` method must passed This may needed particular following reasons Methods properties sometimes don t contain ` __module__ ` slot They require first passed-in argument instance ` ` torch Tensor ` ` Examples -------- is_tensor_method_or_property torch Tensor add True is_tensor_method_or_property torch add False func _get_tensor_methods func __name__ == __get__ is_tensor_like inp Returns ` ` True ` ` passed-in input Tensor-like Currently occurs whenever there s ` ` __torch_function__ ` ` attribute type input Examples -------- A subclass tensor generally Tensor-like SubTensor torch Tensor is_tensor_like SubTensor True Built-in user types aren t usually Tensor-like is_tensor_like False is_tensor_like None False NotATensor is_tensor_like NotATensor False But they can made Tensor-like implementing __torch_function__ TensorLike classmethod __torch_function__ cls func types args kwargs - is_tensor_like TensorLike True type inp torch Tensor hasattr inp __torch_function__ TorchFunctionMode A ` ` TorchFunctionMode ` ` allows you override meaning all ` ` __torch_function__ ` ` overridable functions within dynamic scope without having actually create tensor subclass manually monkey-patch functions PyTorch API Some common situations where you should use mode You want override meaning factory functions other functions do otherwise take tensor argument these cannot overridden tensor subclasses You want override behavior all functions without needing wrap your inputs tensor subclasses e g you just interested logging intermediate computations You want control order execution various tensor subclasses explicitly rather than implicitly via ` ` NotImplemented ` ` Independent subclasses ` TorchFunctionMode ` compositional modes can pushed onto stack using ` ` MyMode ` ` When you call functions PyTorch API inside your ` ` __torch_function__ ` ` implementation default they will forward next mode mode stack If you want recursively call back into your current ` ` __torch_function__ ` ` implementation either explicitly invoke ` ` __torch_function__ ` ` use context manager ` ` enable_torch_function_mode replace=self inner ` ` make PyTorch API self-referential beware infinite loops case inner TorchFunctionMode Force metaclass generate constructor base hierarchy __init__ - None pass __torch_function__ func types args= kwargs=None raise NotImplementedError __enter__ _push_mode __exit__ exc_type exc_val exc_tb _pop_mode classmethod push cls args kwargs warnings warn ` Mode push ` no longer necessary can replaced just ` Mode ` stacklevel= instance = cls args kwargs instance _get_current_function_mode stack_len = _len_torch_function_stack _get_function_stack_at stack_len - stack_len None _get_current_function_mode_stack stack_len = _len_torch_function_stack _get_function_stack_at i i range stack_len _push_mode mode _push_on_torch_function_stack mode _pop_mode old = _pop_torch_function_stack old contextlib contextmanager _pop_mode_temporarily old = _pop_mode try yield old finally _push_mode old BaseTorchFunctionMode TorchFunctionMode __torch_function__ func types args= kwargs=None kwargs None kwargs = func args kwargs contextlib contextmanager _enable_torch_function old_state = torch _C _get_torch_function_state try torch _C _set_torch_function_state torch _C _TorchFunctionState ENABLED yield finally torch _C _set_torch_function_state old_state contextlib contextmanager enable_reentrant_dispatch NB can t simply ` enable_reentrant_dispatch = torch _C _RestorePythonTLSSnapshot ` because torch _C _RestorePythonTLSSnapshot unavailable when file initially gets imported Probably order thing enable_reentrant_dispatch technically public API assigning object would change __module__ look private torch _C _RestorePythonTLSSnapshot try yield finally pass