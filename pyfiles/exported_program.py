mypy allow-untyped-decorators mypy allow-untyped-defs contextlib copy dataclasses functools operator types warnings collections defaultdict collections abc Callable Iterator contextlib contextmanager typing Any final NamedTuple Optional TYPE_CHECKING Union torch _guards tracing TracingContext torch _higher_order_ops utils autograd_not_implemented torch _library fake_class_registry FakeScriptObject torch _subclasses fake_impls _deregister_op_impl _is_op_registered_to_fake_rule register_op_impl torch _subclasses fake_tensor FakeTensorMode torch fx _symbolic_trace _ConstantAttributeType torch fx _utils first_call_function_nn_module_stack torch fx graph _PyTreeCodeGen _PyTreeInfo torch fx immutable_collections immutable_dict immutable_list torch fx passes runtime_assert insert_deferred_runtime_asserts TYPE_CHECKING Import following modules during type checking enable code intelligence features such auto-completion tools like pylance even when these modules explicitly imported user code sympy torch utils _sympy value_ranges ValueRanges torch torch utils _pytree pytree torch _export utils _build_cache _collect_all_valid_cia_ops _collect_and_set_constant_attrs _collect_param_buffer_metadata _detect_fake_mode_from_gm _fakify_params_buffers _get_decomp_for_cia _is_preservable_cia_op _name_hoo_subgraph_placeholders _override_graph_signature_for_temp_registered_constants _overwrite_signature_for_non_persistent_buffers _populate_param_buffer_metadata_to_new_gm _register_constants_as_buffers _rename_without_collisions _special_op_to_preserve_cia placeholder_naming_pass torch _export verifier Verifier torch _guards detect_fake_mode torch _subclasses fake_tensor unset_fake_temporarily torch export _tree_utils is_equivalent reorder_kwargs torch export decomp_utils CustomDecompTable torch fx _compatibility compatibility torch fx passes infra pass_base PassResult torch fx passes infra pass_manager PassManager graph_signature noqa F ArgumentSpec ConstantArgument CustomObjArgument ExportGraphSignature InputKind InputSpec OutputKind OutputSpec SymBoolArgument SymFloatArgument SymIntArgument TensorArgument TokenArgument __all__ = ExportedProgram ModuleCallEntry ModuleCallSignature default_decompositions PassType = Callable torch fx GraphModule Optional PassResult dataclasses dataclass ModuleCallSignature inputs list ArgumentSpec outputs list ArgumentSpec in_spec pytree TreeSpec out_spec pytree TreeSpec forward_arg_names Optional list str = None replace_all_uses_with original_node new_node i inputs i name == original_node name i name = new_node name o outputs o name == original_node name o name = new_node name dataclasses dataclass ModuleCallEntry fqn str signature Optional ModuleCallSignature = None _disable_prexisiting_fake_mode fn functools wraps fn wrapper args kwargs unset_fake_temporarily fn args kwargs wrapper _fx_collection_equivalence_fn spec _type Optional type spec _context pytree Context spec _type Optional type spec _context pytree Context - bool Treat containers their immutable variants same type Otherwise compare normal spec _type None spec _type None spec _type spec _type spec _context == spec _context issubclass spec _type dict immutable_dict issubclass spec _type dict immutable_dict spec _context == spec _context issubclass spec _type list immutable_list issubclass spec _type list immutable_list spec _context == spec _context spec _type spec _type spec _context == spec _context This list compiled DispatchKey cpp The idea we use these keys override CIA decomp export _AUTOGRAD_ALIAS_BACKEND_KEYS_TO_OVERRIDE = torch _C DispatchKey AutogradCPU torch _C DispatchKey AutogradCUDA torch _C DispatchKey AutogradMeta torch _C DispatchKey AutogradXLA torch _C DispatchKey AutogradLazy torch _C DispatchKey AutogradIPU torch _C DispatchKey AutogradXPU torch _C DispatchKey AutogradMPS torch _C DispatchKey AutogradHPU torch _C DispatchKey AutogradPrivateUse torch _C DispatchKey AutogradPrivateUse torch _C DispatchKey AutogradPrivateUse This list compiled DispatchKey cpp The idea we use these keys add python kernels directly uses default CIA decomp See NOTE Registering old CIA Backend kernel _BACKEND_KEYS_TO_OVERRIDE = torch _C DispatchKey CPU torch _C DispatchKey CUDA torch _C DispatchKey Meta torch _C DispatchKey XLA torch _C DispatchKey Lazy torch _C DispatchKey IPU torch _C DispatchKey XPU torch _C DispatchKey MPS torch _C DispatchKey HPU contextmanager _override_composite_implicit_decomp cia_ops_to_callable This function overrides CompositeImplicitAutograd decomp functional composite ops user specified Ideally we want not-decompose ALL composite ops today s C++ functinalization relies fact working opset after decomp run Hence we can only do functional ops One caveat there some composite ops lie about their schema claimed functional really aka dropout these cases we just decompose saved_tables = patched_ops = set op_overload decomp_callable cia_ops_to_callable items saved_tables op_overload = op_overload py_kernels copy patched_ops add op_overload override_dispatch_key _AUTOGRAD_ALIAS_BACKEND_KEYS_TO_OVERRIDE override_dispatch_key op_overload py_kernels TODO tmanlaibaatar https github com pytorch pytorch issues op_overload py_impl override_dispatch_key autograd_not_implemented op_overload deferred_error=True See NOTE Registering old CIA Backend kernel It important we cache before we override py_kernels orig_cia_callable = _get_decomp_for_cia op_overload torch _C DispatchKey CompositeImplicitAutograd op_overload py_kernels del op_overload py_kernels torch _C DispatchKey CompositeImplicitAutograd op_overload py_impl torch _C DispatchKey CompositeImplicitAutograd decomp_callable NOTE Directly registering fake tensor rule CIA ops The problem we facing here your CIA custom rule says we want preserve op we will NotImplemented Unfortunately will invoke meta device tracing fake tensor resulting divergent behaviour CIA kernels has device based branching one case torch ops aten scaled_dot_product attention To get around issue we register direct fake impl so we run kernel before we actually try decompose op FakeTensorMode Note no-op most cases because In post dispatch tracing CIA would have already decomposed Most CIA impl device agnostic _force_dispatch_to_orig_cia_callable fake_tensor_mode op args kwargs orig_cia_callable = kwargs original_callable del kwargs original_callable fake_tensor_mode orig_cia_callable args kwargs _is_op_registered_to_fake_rule op_overload register_op_impl op_overload functools partial _force_dispatch_to_orig_cia_callable original_callable=orig_cia_callable key _BACKEND_KEYS_TO_OVERRIDE key op_overload py_kernels NOTE Registering old CIA Backend kernel We always register original CIA behavior backend keys kernel The reason when we fake tensor prop-ing executing real kernel we end up calling operator respective backend which python dispatcher will resolve into CIA key see resolve_key torch _ops py As result CIA now will call into custom user defined CIA which can cause problem To make more concrete case we handling there tensor constant we performing constant propagation during tracing we invoke op underneath autograd either because we below autograd we tracing inference mode so one backend keys gets hit op we invoking has CIA impl normally runs eager mode user wants tweak CIA impl during tracing during const-prop we want original CIA run op_overload py_impl key orig_cia_callable try yield finally op patched_ops op py_kernels clear op py_kernels update saved_tables op op _dispatch_cache clear _deregister_op_impl op _split_decomp_table_to_cia_and_python_decomp decomp_table dict torch _ops OperatorBase Callable - tuple dict torch _ops OperatorBase Callable all_preservable_cia_ops = set _collect_all_valid_cia_ops cia_ops_to_callable = op list decomp_table keys TODO we silently allowing non-safe non-functional ops through crack due core aten decomp table having non-functional entries Once we have tighter check around core aten decomp we should warn users about them Tracking issue https github com pytorch pytorch issues valid CIA op we can mess export we check Has been marked decomposed Example decomp_table = decomp_table_to_core_aten del decomp_table aten linear In case user says decompose everything except aten linear Has been marked custom decomp behaviour Example decomp_table = aten linear some_op For we want remove all CIA ops weren t handled user suggests they safe decompose so we should remove preservable_list we just plumb custom decomp AOTDIspatcher In both cases we want remove CIA op decomp_table special handled op all_preservable_cia_ops cia_ops_to_callable op = decomp_table op all_preservable_cia_ops remove op del decomp_table op If custom op we want still preserve do whatever functional CIA The reason we don t remove CIA list because we don t query custom ops _is_preservable_cia_op op op_name = op name assert op_name startswith aten This should custom op cia_ops_to_callable op = decomp_table op If we reached here means user intentionally deleted these CIA ops decomp table k all_preservable_cia_ops cia_ops_to_callable k = _special_op_to_preserve_cia cia_ops_to_callable decomp_table default_decompositions - CustomDecompTable This default decomposition table which contains decomposition all ATEN operators core aten opset Use API together func ` run_decompositions ` CustomDecompTable _decompose_and_get_gm_with_new_signature_constants ep ExportedProgram cia_to_decomp dict torch _ops OperatorBase Callable python_decomp_table dict torch _ops OperatorBase Callable joint_loss_index Optional int decompose_custom_triton_ops torch _export passes lift_constants_pass _materialize_and_lift_constants torch _functorch aot_autograd aot_export_module torch export _trace _disable_custom_triton_op_functional_decomposition _export_to_aten_ir _ignore_backend_decomps _verify_nn_module_stack _verify_placeholder_names _verify_stack_trace torch fx experimental symbolic_shapes ShapeEnv _is_joint_ir_decomp ep joint_loss_index joint_loss_index None ep graph_signature backward_signature None _is_joint_ir_decomp ep joint_loss_index mod = ep module wrapped_params_buffers = dict mod named_parameters remove_duplicate=False dict mod named_buffers remove_duplicate=False torch _functorch _aot_autograd subclass_parametrization unwrap_tensor_subclass_parameters NOTE Unwrapping subclasses AOT In torch compile subclass unwrapping wrapping happen runtime export impossible intended run C++ environment As result we unwrap subclass parameters AOT After ExportedProgram state_dict won t same eager model because eager model could have subclass weights while ExportedProgram will have desugared versions This fine because run_decompositions supposed specialize post-autograd graph where subclass desugaring supposed happen unwrap_tensor_subclass_parameters mod unwrapped_params_buffers = dict mod named_parameters remove_duplicate=False dict mod named_buffers remove_duplicate=False TODO T fake_mode = _detect_fake_mode_from_gm ep graph_module fake_mode None fake_mode = FakeTensorMode shape_env=ShapeEnv export=True Fix graph output signature tuple scalar out_spec = mod _out_spec assert isinstance mod graph _codegen _PyTreeCodeGen orig_arg_names = mod graph _codegen pytree_info orig_args aot_export expect type always tuple assert out_spec None out_spec type list tuple out_spec = pytree treespec_tuple out_spec mod graph _codegen = _PyTreeCodeGen _PyTreeInfo orig_arg_names mod _in_spec out_spec mod recompile exported module will store constants non-persistent buffers such retracing treats them persistent buffers so we inform constants lifting pass overwrite new graph signature using previous program _collect_and_set_constant_attrs ep graph_signature ep constants mod When we have module constant attributes AotDispatcher doesn t actually wrap them functional tensors because dynamo would have already made buffer In non-strict case however AotDispatcher can intercept constants causing functionalize operators operating constant tensors Since dynamo already wraps constants buffers we temporarily register constants buffers undo operation after AOTDispatcher done temp_registered_constants = _register_constants_as_buffers mod ep state_dict ep graph_signature non_persistent_buffers get params buffers after excluding constants fake_params_buffers = _fakify_params_buffers fake_mode mod params_buffers_to_node_meta = _collect_param_buffer_metadata mod TODO tmanlaibaatar Ideally run_decomp should just call _non_strict_export due special handling constants non-persistent buffers make little difficult But we should unify code path together T torch _export non_strict_utils _enable_graph_inputs_of_type_nn_module _fakify_script_objects retracing_args = node mod graph nodes node op == placeholder isinstance node meta val CustomObjArgument real_script_obj = None node meta val fake_val None real_script_obj = ep constants node meta val name real_script_obj = node meta val fake_val real_obj retracing_args append real_script_obj retracing_args append node meta val tx = TracingContext fake_mode fake_mode _override_composite_implicit_decomp cia_to_decomp _enable_graph_inputs_of_type_nn_module ep example_inputs tracing tx retracing_args_unwrapped = pytree tree_unflatten retracing_args mod _in_spec requires empty kwargs pytree flattened format _fakify_script_objects mod retracing_args_unwrapped retracing_args_unwrapped values fake_mode patched_mod new_fake_args new_fake_kwargs new_fake_constant_attrs map_fake_to_real aten_export_artifact = _export_to_aten_ir patched_mod new_fake_args new_fake_kwargs fake_params_buffers new_fake_constant_attrs decomp_table=python_decomp_table _prettify_placeholder_names=False decompose_custom_triton_ops=decompose_custom_triton_ops aten_export_artifact constants contains only fake script objects we need map them back aten_export_artifact constants = fqn map_fake_to_real obj isinstance obj FakeScriptObject obj fqn obj aten_export_artifact constants items gm = aten_export_artifact gm new_graph_signature = aten_export_artifact sig In previous step we assume constants buffers AOTDispatcher functianalize properly so undo here new_graph_signature = _override_graph_signature_for_temp_registered_constants new_graph_signature temp_registered_constants _populate_param_buffer_metadata_to_new_gm params_buffers_to_node_meta gm new_graph_signature overwrite signature non-persistent buffers new_graph_signature = _overwrite_signature_for_non_persistent_buffers ep graph_signature new_graph_signature constants = _materialize_and_lift_constants gm new_graph_signature new_fake_constant_attrs placeholder_naming_pass gm new_graph_signature patched_mod new_fake_args new_fake_kwargs fake_params_buffers constants _verify_nn_module_stack gm _verify_stack_trace gm _verify_placeholder_names gm new_graph_signature gm new_graph_signature = _remove_unnecessary_copy_op_pass gm new_graph_signature When we apply parameterization rule unwrap subclasses state dict will now have different desugared parameters We need manually filter those update ep state_dict Ideally we should just state dict ep module ep module only stores params buffers participate forward If we undo behavior would break some downstream users new_state_dict = ep state_dict name p name p unwrapped_params_buffers items name wrapped_params_buffers name p wrapped_params_buffers items Buffers can persistent non-persistent name new_state_dict assert isinstance p torch nn Parameter name new_state_dict name unwrapped_params_buffers new_state_dict pop name gm new_graph_signature new_state_dict old_placeholders = node node ep graph_module graph nodes node op == placeholder fake_args = node meta val node old_placeholders buffers_to_remove = name name _ ep graph_module named_buffers name buffers_to_remove delattr ep graph_module name TODO zhxhchen Return new graph_signature directly fake_mode_det = detect_fake_mode fake_args fake_mode_ctx = contextlib nullcontext fake_mode_det None fake_mode_det type ignore assignment custom_triton_ops_decomposition_ctx = contextlib nullcontext decompose_custom_triton_ops _disable_custom_triton_op_functional_decomposition _ignore_backend_decomps fake_mode_ctx _override_composite_implicit_decomp cia_to_decomp custom_triton_ops_decomposition_ctx gm graph_signature = aot_export_module ep graph_module fake_args decompositions=python_decomp_table trace_joint=joint_loss_index None output_loss_index= joint_loss_index joint_loss_index None None gm graph eliminate_dead_code Update signatures new placeholder names case they changed when calling aot_export update_arg old_arg new_ph isinstance old_arg ConstantArgument old_arg isinstance old_arg TensorArgument TensorArgument name=new_ph name isinstance old_arg SymIntArgument SymIntArgument name=new_ph name isinstance old_arg SymFloatArgument SymFloatArgument name=new_ph name isinstance old_arg SymBoolArgument SymBoolArgument name=new_ph name raise RuntimeError f Type old_arg supported type old_arg new_placeholders = node node gm graph nodes node op == placeholder new_outputs tuple torch fx Node = tuple gm graph output_node args type ignore arg-type rename placeholders assert len new_placeholders == len old_placeholders old_ph new_ph zip old_placeholders new_placeholders new_ph name = new_ph target = old_ph name handle name collisions newly decomposed graph nodes name_map = find_available dict str int = defaultdict int used_names set str = set ph new_placeholders name_map ph name = ph name _build_cache ph name find_available used_names node gm graph nodes node op == placeholder continue node name = _rename_without_collisions name_map find_available used_names node name node name propagate names higher order op subgraphs _name_hoo_subgraph_placeholders gm Run pass before creating input output specs since size-related CSE DCE might affect output signature Overwrite output specs afterwards torch _export passes _node_metadata_hook _node_metadata_hook _set_node_metadata_hook torch _functorch _aot_autograd input_output_analysis _graph_output_names torch _dynamo config do_not_emit_runtime_asserts stack_trace = File torch fx passes runtime_assert py line insert_deferred_runtime_asserts shape_env = _get_shape_env gm shape_env None _set_node_metadata_hook gm functools partial _node_metadata_hook metadata= stack_trace stack_trace insert_deferred_runtime_asserts gm shape_env f exported program first_call_function_nn_module_stack gm graph export=True update output specs gm recompile output name zip new_outputs _graph_output_names gm name None output name = name To match output target correct input input mutations need find old new placeholder map old_new_placeholder_map = spec arg name new_placeholders i name i spec enumerate ep graph_signature input_specs isinstance spec arg ConstantArgument input_specs = InputSpec spec kind update_arg spec arg new_placeholders i spec target spec persistent i spec enumerate ep graph_signature input_specs output_specs = handle buffer input mutations these appear before loss output gradients ep graph_signature input_specs tells us types inputs graph_signature user_inputs tells us node input names order graph_signature user_inputs_to_mutate tells us buffer input mutations map - input order - input type user_inputs_index = name i i name enumerate graph_signature user_inputs mutation_names = list graph_signature user_inputs_to_mutate keys assert mutation_names == node name node new_outputs len mutation_names output_name input_name graph_signature user_inputs_to_mutate items i = user_inputs_index input_name input_spec = ep graph_signature input_specs i assert input_spec kind InputKind USER_INPUT InputKind BUFFER output_kind = OutputKind BUFFER_MUTATION input_spec kind == InputKind BUFFER OutputKind USER_INPUT_MUTATION target = input_spec target input_spec kind == InputKind BUFFER input_spec arg name output_specs append OutputSpec kind=output_kind arg=TensorArgument name=output_name target=target handle actual user outputs i spec enumerate ep graph_signature output_specs output_specs append OutputSpec OutputKind LOSS_OUTPUT i == joint_loss_index spec kind update_arg spec arg new_outputs len mutation_names + i old_new_placeholder_map get spec target spec target joint_loss_index None assert graph_signature backward_signature None gradients = graph_signature backward_signature gradients_to_user_inputs assert len graph_signature user_inputs == len ep graph_signature input_specs specs = graph_signature user_inputs i spec i spec enumerate ep graph_signature input_specs isinstance spec arg TensorArgument node new_outputs len output_specs source = gradients node name spec = specs source type ignore index spec kind == InputKind PARAMETER kind = OutputKind GRADIENT_TO_PARAMETER target = spec target spec kind == InputKind USER_INPUT kind = OutputKind GRADIENT_TO_USER_INPUT target = source raise AssertionError f Unknown input kind spec kind output_specs append OutputSpec kind TensorArgument name=node name target assert len new_placeholders == len old_placeholders new_graph_signature = ExportGraphSignature input_specs=input_specs output_specs=output_specs NOTE aot_export adds symint metadata placeholders int values since these become specialized we replace such metadata original values Also set param buffer metadata back placeholders old_node new_node zip old_placeholders new_placeholders isinstance old_node meta val torch Tensor new_node meta val = old_node meta val new_node target new_graph_signature inputs_to_parameters new_node target new_graph_signature inputs_to_buffers k v old_node meta items new_node meta k = v gm new_graph_signature ep state_dict _remove_unnecessary_copy_op_pass gm torch fx GraphModule new_graph_signature ExportGraphSignature - tuple torch fx GraphModule ExportGraphSignature Removes redundant copy_ node introduced due mutated buffer gm _set_replace_hook new_graph_signature get_replace_hook node gm graph nodes node op == output args _ = pytree tree_flatten node args out args isinstance out torch fx Node out name new_graph_signature buffers_to_mutate out name new_graph_signature parameters_to_mutate out op == call_function out target torch ops aten copy default out replace_all_uses_with out args type ignore arg-type gm graph erase_node out gm recompile gm new_graph_signature _common_getitem_elimination_pass gm torch fx GraphModule graph_signature module_call_graph gm _set_replace_hook graph_signature get_replace_hook module gm modules isinstance module torch fx GraphModule continue node_id dict torch fx Node str = getitems dict str torch fx Node = node list module graph nodes node op == call_function node target operator getitem source idx = node args new_id = f node_id source idx new_id getitems node replace_all_uses_with getitems new_id entry module_call_graph entry signature None entry signature replace_all_uses_with node getitems new_id module graph erase_node node getitems new_id = node node_id node = new_id node_id node = node name _get_updated_module_call_graph old_gm torch fx GraphModule old_graph_signature ExportGraphSignature gm torch fx GraphModule graph_signature ExportGraphSignature old_module_call_graph list ModuleCallEntry new_module_call_graph = copy deepcopy old_module_call_graph old_nodes = node name node node old_gm graph nodes old_graph_params_buffers = old_graph_signature inputs_to_parameters old_graph_signature inputs_to_buffers new_graph_params_buffers = graph_signature inputs_to_parameters graph_signature inputs_to_buffers use node-level provenance metadata create map old node names new node names provenance dict str str = user_input_counter = old_user_input_names = node target node old_gm graph nodes node op == placeholder old_user_input_names = list filter lambda x x old_graph_params_buffers x old_graph_signature input_tokens old_user_input_names new_user_input_names = node target node gm graph nodes node op == placeholder node gm graph nodes history = node meta get from_node provenance history - name = node name For params buffers we might have applied parameterizaiton rule so names might have changed But user inputs we know we must preserve old name node op == placeholder node target new_graph_params_buffers node target graph_signature input_tokens node target new_user_input_names assert isinstance node name str old_name = old_user_input_names user_input_counter assert isinstance old_name str provenance old_name = node name user_input_counter += For all parameters buffers we first see they result parametrizations they we log them error later old_param_to_desugared = defaultdict list name target new_graph_params_buffers items parameters parametrized naming won t change target startswith parametrizations If we strict mode we can t just reuse param names name old_graph_params_buffers provenance name = name old_target = join target split - old_param_to_desugared old_target append name map old names new names module call signatures entry new_module_call_graph signature = entry signature signature None continue x signature inputs signature outputs We noticed submodule taking subclass input we can t preserve signature here x name old_param_to_desugared raise ValueError f It looks like x name tensor subclass f Preserving submodule takes subclass parameter supported f inference IR because we desugar them resulting more tensors x name provenance x name = provenance x name This can happen when aten called graph boundaries Basically aten post-dispatch level can either copy alias In alias case we will no-op so will disappear graph If we detect such case we should reuse input aten new input submodule Technically can happen other maybe aliasing ops aten probably most common one x name old_nodes old_node = old_nodes x name old_node op == call_function old_node target torch ops aten dtype_layout torch ops aten device torch ops aten dtype old_target = old_node args name old_target provenance raise ValueError f It looks like old_target tensor subclass f Preserving submodule takes subclass parameter supported f inference IR because we desugar them resulting more tensors x name = provenance old_target new_module_call_graph _decompose_exported_program ep cia_to_decomp dict torch _ops OperatorBase Callable python_decomp_table dict torch _ops OperatorBase Callable joint_loss_index Optional int decompose_custom_triton_ops bool gm new_graph_signature state_dict = _decompose_and_get_gm_with_new_signature_constants ep cia_to_decomp=cia_to_decomp python_decomp_table=python_decomp_table joint_loss_index=joint_loss_index decompose_custom_triton_ops=decompose_custom_triton_ops The signatures ep module_call_graph refer input output nodes original graph module However new graph module may have new nodes due decompositions So we need update these signatures decomposed exported program s module_call_graph new_module_call_graph = _get_updated_module_call_graph ep graph_module ep graph_signature gm new_graph_signature ep module_call_graph TODO unfortunately preserving graph-level metadata working well aot_export So we manually copy The node-level meta addressed above gm meta update ep graph_module meta new_range_constraints = _get_updated_range_constraints gm ep range_constraints exported_program = ExportedProgram root=gm graph=gm graph graph_signature=new_graph_signature state_dict=state_dict range_constraints=new_range_constraints module_call_graph=new_module_call_graph example_inputs=ep example_inputs constants=ep constants exported_program ExportedProgram Package program func ` export ` It contains ` torch fx Graph ` represents Tensor computation state_dict containing tensor values all lifted parameters buffers various metadata You can call ExportedProgram like original callable traced func ` export ` same calling convention To perform transformations graph use ` ` module ` ` property access ` torch fx GraphModule ` You can then use ` FX transformation https pytorch org docs stable fx html#writing-transformations ` _ rewrite graph Afterwards you can simply use func ` export ` again construct correct ExportedProgram _graph_module torch fx GraphModule The underlying GraphModule containing exported computation graph _graph_signature ExportGraphSignature The signature containing input output specifications graph _state_dict dict str Any Dictionary containing parameter buffer values original module _range_constraints dict sympy Symbol ValueRanges Symbolic shape constraints dynamic shapes graph _module_call_graph list ModuleCallEntry Call graph information tracking module hierarchy signatures _example_inputs Optional tuple tuple Any dict str Any Example inputs used during export stored args kwargs tuple _constants dict str _ConstantAttributeType Dictionary constant values used graph _verifiers list type Verifier List verifier classes used validate exported program _guards_code list str __init__ root Union torch nn Module dict str Any graph torch fx Graph graph_signature ExportGraphSignature state_dict dict str Union torch Tensor torch nn Parameter range_constraints dict sympy Symbol Any module_call_graph list ModuleCallEntry example_inputs Optional tuple tuple Any dict str Any = None constants Optional dict str _ConstantAttributeType = None verifiers Optional list type Verifier = None Remove codegen related things graph It should just flat graph graph _codegen = torch fx graph CodeGen _graph_module = _create_graph_module_for_export root graph isinstance root torch fx GraphModule _graph_module meta update root meta _common_getitem_elimination_pass _graph_module graph_signature module_call_graph _graph_signature ExportGraphSignature = graph_signature _state_dict dict str Any = state_dict _range_constraints dict sympy Symbol ValueRanges = range_constraints assert module_call_graph None _module_call_graph list ModuleCallEntry = module_call_graph _example_inputs = example_inputs _constants = constants verifiers = verifiers Verifier assert all issubclass v Verifier v verifiers _verifiers = verifiers Validate should always last step constructor validate _guards_code = _convert_guards_to_code _graph_module property compatibility is_backward_compatible=False graph_module _graph_module graph_module setter compatibility is_backward_compatible=False graph_module value raise RuntimeError Unable set ExportedProgram s graph_module attribute property compatibility is_backward_compatible=False graph graph_module graph graph setter compatibility is_backward_compatible=False graph value raise RuntimeError Unable set ExportedProgram s graph attribute property compatibility is_backward_compatible=False graph_signature _graph_signature graph_signature setter compatibility is_backward_compatible=False graph_signature value raise RuntimeError Unable set ExportedProgram s graph_signature attribute property compatibility is_backward_compatible=False state_dict _state_dict state_dict setter compatibility is_backward_compatible=False state_dict value raise RuntimeError Unable set ExportedProgram s state_dict attribute compatibility is_backward_compatible=False parameters - Iterator torch nn Parameter Returns iterator over original module s parameters _ param named_parameters yield param compatibility is_backward_compatible=False named_parameters - Iterator tuple str torch nn Parameter Returns iterator over original module parameters yielding both name parameter well parameter itself param_name graph_signature parameters yield param_name state_dict param_name compatibility is_backward_compatible=False buffers - Iterator torch Tensor Returns iterator over original module buffers _ buf named_buffers yield buf compatibility is_backward_compatible=False named_buffers - Iterator tuple str torch Tensor Returns iterator over original module buffers yielding both name buffer well buffer itself non_persistent_buffers = set graph_signature non_persistent_buffers buffer_name graph_signature buffers buffer_name non_persistent_buffers yield buffer_name constants buffer_name yield buffer_name state_dict buffer_name property compatibility is_backward_compatible=False range_constraints _range_constraints range_constraints setter compatibility is_backward_compatible=False range_constraints value raise RuntimeError Unable set ExportedProgram s range_constraints attribute property compatibility is_backward_compatible=False module_call_graph _module_call_graph module_call_graph setter compatibility is_backward_compatible=False module_call_graph value raise RuntimeError Unable set ExportedProgram s module_call_graph attribute property compatibility is_backward_compatible=False example_inputs _example_inputs example_inputs setter compatibility is_backward_compatible=False example_inputs value This allowed value None _example_inputs = value isinstance value tuple len value == isinstance value tuple isinstance value dict raise ValueError Example inputs should tuple containing example arguments tuple example kwargs dictionary args kwargs = value _unlift _check_inputs_match _check_inputs_match args kwargs call_spec in_spec _example_inputs = value property compatibility is_backward_compatible=False call_spec CallSpec NamedTuple in_spec Optional pytree TreeSpec out_spec Optional pytree TreeSpec len module_call_graph == CallSpec in_spec=None out_spec=None assert module_call_graph fqn == CallSpec in_spec=self module_call_graph signature in_spec out_spec=self module_call_graph signature out_spec call_spec setter compatibility is_backward_compatible=False call_spec value raise RuntimeError Unable set ExportedProgram s call_spec attribute property compatibility is_backward_compatible=False verifier - Any _verifiers verifier setter compatibility is_backward_compatible=False verifier value raise RuntimeError Unable set ExportedProgram s verifier attribute property compatibility is_backward_compatible=False dialect - str assert _verifiers None _verifiers dialect dialect setter compatibility is_backward_compatible=False dialect value raise RuntimeError Unable set ExportedProgram s dialect attribute property compatibility is_backward_compatible=False verifiers _verifiers verifiers setter compatibility is_backward_compatible=False verifiers value raise RuntimeError Unable set ExportedProgram s verifiers attribute property compatibility is_backward_compatible=False tensor_constants _constants tensor_constants setter compatibility is_backward_compatible=False tensor_constants value raise RuntimeError Unable set ExportedProgram s tensor_constants attribute property compatibility is_backward_compatible=False constants _constants constants setter compatibility is_backward_compatible=False constants value raise RuntimeError Unable set ExportedProgram s constants attribute _get_flat_args_with_check args kwargs Flatten args kwargs using pytree then check specs Args args List Any original args passed __call__ kwargs Dict str Any original kwargs passed __call Returns A tuple flat_args received_spec flat_args flattened args kwargs received_spec pytree spec produced while flattening tuple args kwargs in_spec = call_spec in_spec in_spec None kwargs = reorder_kwargs kwargs in_spec flat_args_with_path received_spec = pytree tree_flatten_with_path args kwargs _check_input_constraints flat_args_with_path flat_args = tuple x x flat_args_with_path flat_args received_spec _graph_module_flat_inputs args Any kwargs Any - Any Transform args kwargs __call__ args graph_module graph_module takes stuff state dict inputs The invariant ep ExportedProgram ep args kwargs == ep postprocess ep graph_module ep graph_module_flat_inputs args kwargs in_spec = call_spec in_spec flat_args received_spec = _get_flat_args_with_check args kwargs in_spec None is_equivalent received_spec in_spec _fx_collection_equivalence_fn raise ValueError Trying flatten user inputs exported input tree spec \n f in_spec \n actually got inputs tree spec \n f received_spec additional_inputs = input_ graph_signature input_specs input_ kind == InputKind USER_INPUT continue input_ kind InputKind PARAMETER InputKind BUFFER input_ persistent False This non-persistent buffer grab our constants instead state dict additional_inputs append constants input_ target additional_inputs append state_dict input_ target input_ kind InputKind CONSTANT_TENSOR InputKind CUSTOM_OBJ additional_inputs append constants input_ target additional_inputs = tuple additional_inputs NOTE calling convention first params then buffers then args user supplied them See torch _functorch aot_autograd py#L additional_inputs + flat_args __call__ args Any kwargs Any - Any raise RuntimeError Unable call ExportedProgram directly You should use ` exported_program module ` instead __str__ - str graph_module = graph_module print_readable print_output=False colored=False replace \n \n graph_signature = str graph_signature replace \n \n string = ExportedProgram \n f graph_module \n f Graph signature graph_signature \n f Range constraints range_constraints \n string module check_guards=True - torch fx GraphModule Returns contained GraphModule all parameters buffers inlined - When ` check_guards=True ` default ` _guards_fn ` submodule generated call ` _guards_fn ` submodule inserted right after placeholders graph This module checks guards inputs - When ` check_guards=False ` subset these checks performed forward pre-hook graph module No ` _guards_fn ` submodule generated _unlift _unlift_exported_program_lifted_states module = _unlift_exported_program_lifted_states check_guards=check_guards _train mode bool = True raise NotImplementedError Calling train supported yet _eval mode bool = True raise NotImplementedError Calling eval supported yet module train = types MethodType _train module type ignore method-assign module eval = types MethodType _eval module type ignore method-assign module _num_lifted_params_buffers next i i s enumerate _graph_signature input_specs s kind == InputKind USER_INPUT len _graph_signature input_specs _disable_prexisiting_fake_mode run_decompositions decomp_table Optional dict torch _ops OperatorBase Callable = None decompose_custom_triton_ops bool = False - ExportedProgram Run set decompositions exported program returns new exported program By default we will run Core ATen decompositions get operators ` Core ATen Operator Set https pytorch org docs stable torch compiler_ir html ` _ For now we do decompose joint graphs Args decomp_table An optional argument specifies decomp behaviour Aten ops If None we decompose core aten decompositions If empty we don t decompose any operator Some examples If you don t want decompose anything code-block python ep = torch export export model ep = ep run_decompositions decomp_table= If you want get core aten operator set except certain operator you can do following code-block python ep = torch export export model decomp_table = torch export default_decompositions decomp_table your_op = your_custom_decomp ep = ep run_decompositions decomp_table=decomp_table _decomp_table = default_decompositions decomp_table None dict decomp_table isinstance _decomp_table CustomDecompTable _decomp_table = _decomp_table materialize Note Separating decomp_table into CIA decomps non-CIA decomps At point we have decomp_table contains decomp behaviour both CIA post-autograd ops We need separate op into two categories CIA op These ops we want override CompositeImplicitAutograd decomp For them we need use _override_composite_implicit_decomp context manager plumb through AOTDispatcher Non-CIA op These ops only relevant after AOTDIspatcher runs so just checking they statically functional enough For joint IR case tho we need use old path because we can t register custom decomps way because we can t use context manager installs autograd_error node cia_to_decomp python_decomp_table = _split_decomp_table_to_cia_and_python_decomp _decomp_table _decompose_exported_program cia_to_decomp=cia_to_decomp python_decomp_table=python_decomp_table joint_loss_index=None decompose_custom_triton_ops=decompose_custom_triton_ops _transform_do_not_use passes PassType - ExportedProgram pm = PassManager list passes Since we abstractly run passes we need disable backend decomp here again torch export _trace _ignore_backend_decomps _ignore_backend_decomps res = pm graph_module transformed_gm = res graph_module res None graph_module assert transformed_gm None pyrefly ignore missing-attribute transformed_gm graph_module res modified TODO zhxchen Remove _get_updated_graph_signature old_signature ExportGraphSignature new_gm torch fx GraphModule - ExportGraphSignature Update graph signature s user_input user_outputs new_input_specs = i node enumerate new_gm graph nodes node op = placeholder break assert i len old_signature input_specs Number inputs changed after transformation old_input_spec = old_signature input_specs i arg = old_input_spec arg isinstance old_input_spec arg ConstantArgument CustomObjArgument type old_input_spec arg node name new_input_specs append InputSpec old_input_spec kind arg old_input_spec target old_input_spec persistent output_node = list new_gm graph nodes - assert output_node op == output new_output_specs = i node enumerate output_node args assert i len old_signature output_specs Number outputs changed after transformation old_output_spec = old_signature output_specs i arg = old_output_spec arg isinstance old_output_spec arg ConstantArgument CustomObjArgument type old_output_spec arg node name new_output_specs append OutputSpec old_output_spec kind arg old_output_spec target new_signature = ExportGraphSignature input_specs=new_input_specs output_specs=new_output_specs new_signature transformed_ep = ExportedProgram root=transformed_gm graph=transformed_gm graph graph_signature=_get_updated_graph_signature graph_signature transformed_gm state_dict=self state_dict range_constraints=_get_updated_range_constraints transformed_gm range_constraints module_call_graph=copy deepcopy _module_call_graph example_inputs=self example_inputs constants=self constants verifiers=self verifiers transformed_ep graph_module meta update graph_module meta pyrefly ignore missing-attribute transformed_ep graph_module meta update res graph_module meta transformed_ep _check_input_constraints flat_args_with_path torch _export utils _check_input_constraints_for_graph placeholders = p p graph nodes p op == placeholder input_placeholders = p p s zip placeholders graph_signature input_specs s kind == InputKind USER_INPUT _check_input_constraints_for_graph input_placeholders flat_args_with_path range_constraints compatibility is_backward_compatible=False validate _validate TODO remove final _validate assert len verifiers ExportedProgram must have least one verifier v verifiers v check TODO zhxchen Formalize _update graph_module graph_signature state_dict=None constants=None verifiers=None - ExportedProgram ExportedProgram root=graph_module graph=graph_module graph graph_signature=graph_signature state_dict=state_dict state_dict None state_dict range_constraints=copy deepcopy range_constraints module_call_graph=copy deepcopy _module_call_graph example_inputs=self example_inputs constants=constants constants None constants verifiers=verifiers verifiers None verifiers _get_shape_env gm vals = node meta val node gm graph nodes node meta get val None None torch _guards detect_fake_mode fake_mode = detect_fake_mode vals fake_mode None fake_mode shape_env v vals isinstance v torch SymInt v node shape_env _get_updated_range_constraints gm torch fx GraphModule old_range_constraints Optional dict sympy Symbol Any = None - dict sympy Symbol Any assert old_range_constraints None shape_env = _get_shape_env gm shape_env None range_constraints = copy copy old_range_constraints range_constraints = k v k v range_constraints items k shape_env replacements Only when we have unbacked symint s used constructor inputs runtime_var_to_range will make difference compated var_to_range e g oo - oo k v shape_env var_to_range items k shape_env replacements k range_constraints range_constraints k = v range_constraints _create_graph_module_for_export root graph try gm = torch fx GraphModule root graph except SyntaxError If custom objects stored memory being used graph generated python code will result syntax error custom object since unable parse in-memory object However we can still run graph eagerly through torch fx Interpreter so we will bypass error warnings warn Unable execute generated python source code graph The graph module will no longer directly callable you can still run ExportedProgram needed you can run graph module eagerly using torch fx Interpreter stacklevel= gm = torch fx GraphModule root torch fx Graph gm _graph = graph gm _convert_guards_to_code graph_module shape_env = _get_shape_env graph_module shape_env None local_vars = var var sources shape_env var_to_sources items all isinstance source torch _dynamo source ConstantSource source sources py_printer = torch fx experimental symbolic_shapes ShapeGuardPythonPrinter shape_env var_to_sources lambda s s name shape_env var_to_sources py_printer doprint guard expr guard shape_env guards guard expr free_symbols issubset local_vars