mypy ignore-errors contextlib copy functools math traceback warnings collections abc Callable Generator Iterable Iterator contextlib contextmanager enum auto Enum typing Any Optional Union torch torch distributed dist torch distributed fsdp _traversal_utils traversal_utils torch nn nn torch distributed algorithms _checkpoint checkpoint_wrapper _CHECKPOINT_WRAPPED_MODULE ActivationWrapper torch distributed algorithms _comm_hooks LOW_PRECISION_HOOKS torch distributed fsdp _common_utils _FSDPState _get_param_to_fqns FSDP_PREFIX FSDP_WRAPPED_MODULE HandleTrainingState TrainingState torch distributed fsdp _dynamo_utils _annotate_modules_for_dynamo torch distributed fsdp _init_utils _check_orig_params_flattened _init_buffer_state _init_core_state _init_device_handle _init_extension _init_ignored_module_states _init_param_handle_from_module _init_prefetching_state _init_process_group_state _init_runtime_state _init_state_dict_state HYBRID_SHARDING_STRATEGIES ProcessGroupType torch distributed fsdp _runtime_utils _get_fsdp_root_states _is_fsdp_root _lazy_init _post_forward _post_forward_reshard _pre_forward _pre_forward_unshard _root_pre_forward _unshard _wait_for_computation_stream torch distributed fsdp _wrap_utils _auto_wrap torch distributed fsdp api BackwardPrefetch CPUOffload FullOptimStateDictConfig FullStateDictConfig LocalOptimStateDictConfig LocalStateDictConfig MixedPrecision OptimStateDictConfig ShardedOptimStateDictConfig ShardedStateDictConfig ShardingStrategy StateDictConfig StateDictSettings StateDictType torch distributed tensor DeviceMesh torch distributed utils _p_assert _flat_param FlatParameter FlatParamHandle _optim_utils _flatten_optim_state_dict _get_param_id_to_param_from_optim_input _get_param_key_to_param _get_param_to_param_id_from_optim_input _get_param_to_param_key _optim_state_dict _rekey_sharded_optim_state_dict _set_optim_use_dtensor _state_dict_utils _register_all_state_dict_hooks _unshard_param_utils _deregister_orig_params _register_flat_param _register_orig_params _unshard_params _unshard_params_for_summon wrap CustomPolicy ModuleWrapPolicy __all__ = FullyShardedDataParallel OptimStateKeyType FLAT_PARAM = _flat_param OptimStateKeyType Enum Represents type key optimizer state-dict PARAM_NAME = auto PARAM_ID = auto FullyShardedDataParallel nn Module _FSDPState A wrapper sharding module parameters across data parallel workers This inspired ` Xu et al https arxiv org abs ` _ well ZeRO Stage ` DeepSpeed https www deepspeed ai ` _ FullyShardedDataParallel commonly shortened FSDP Example xdoctest +SKIP undefined variables torch torch distributed fsdp FullyShardedDataParallel FSDP torch cuda set_device device_id sharded_module = FSDP my_module optim = torch optim Adam sharded_module parameters lr= x = sharded_module x y= z=torch Tensor loss = x sum loss backward optim step Using FSDP involves wrapping your module then initializing your optimizer after This required since FSDP changes parameter variables When setting up FSDP you need consider destination CUDA device If device has ID ` ` dev_id ` ` you have three options Place module device Set device using ` ` torch cuda set_device dev_id ` ` Pass ` ` dev_id ` ` into ` ` device_id ` ` constructor argument This ensures FSDP instance s compute device destination device For option FSDP initialization always occurs GPU For option FSDP initialization happens module s current device which may CPU If you re using ` ` sync_module_states=True ` ` flag you need ensure module GPU use ` ` device_id ` ` argument specify CUDA device FSDP will move module FSDP constructor This necessary because ` ` sync_module_states=True ` ` requires GPU communication FSDP also takes care moving input tensors forward method GPU compute device so you don t need manually move them CPU For ` ` use_orig_params=True ` ` ` ` ShardingStrategy SHARD_GRAD_OP ` ` exposes unsharded parameters sharded parameters after forward unlike ` ` ShardingStrategy FULL_SHARD ` ` If you want inspect gradients you can use ` ` summon_full_params ` ` method ` ` with_grads=True ` ` With ` ` limit_all_gathers=True ` ` you may see gap FSDP pre-forward where CPU thread issuing any kernels This intentional shows rate limiter effect Synchronizing CPU thread way prevents over-allocating memory subsequent all-gathers should actually delay GPU kernel execution FSDP replaces managed modules parameters ` ` torch Tensor ` ` views during forward backward computation autograd-related reasons If your module s forward relies saved references parameters instead reacquiring references each iteration then will see FSDP s newly created views autograd will work correctly Finally when using ` ` sharding_strategy=ShardingStrategy HYBRID_SHARD ` ` sharding process group being intra-node replication process group being inter-node setting ` ` NCCL_CROSS_NIC= ` ` can help improve all-reduce times over replication process group some cluster setups Limitations There several limitations aware when using FSDP FSDP currently does support gradient accumulation outside ` ` no_sync ` ` when using CPU offloading This because FSDP uses newly-reduced gradient instead accumulating any existing gradient which can lead incorrect results FSDP does support running forward pass submodule contained FSDP instance This because submodule s parameters will sharded submodule itself FSDP instance so its forward pass will all-gather full parameters appropriately FSDP does work double backwards due way registers backward hooks FSDP has some constraints when freezing parameters For ` ` use_orig_params=False ` ` each FSDP instance must manage parameters all frozen all non-frozen For ` ` use_orig_params=True ` ` FSDP supports mixing frozen non-frozen parameters s recommended avoid doing so prevent higher than expected gradient memory usage As PyTorch FSDP offers limited support shared parameters If enhanced shared parameter support needed your use case please post ` issue https github com pytorch pytorch issues ` __ You should avoid modifying parameters between forward backward without using ` ` summon_full_params ` ` context modifications may persist Args module nn Module This module wrapped FSDP process_group Optional Union ProcessGroup Tuple ProcessGroup ProcessGroup This process group over which model sharded thus one used FSDP s all-gather reduce-scatter collective communications If ` ` None ` ` then FSDP uses default process group For hybrid sharding strategies such ` ` ShardingStrategy HYBRID_SHARD ` ` users can pass tuple process groups representing groups over which shard replicate respectively If ` ` None ` ` then FSDP constructs process groups user shard intra-node replicate inter-node Default ` ` None ` ` sharding_strategy Optional ShardingStrategy This configures sharding strategy which may trade off memory saving communication overhead See ` ShardingStrategy ` details Default ` ` FULL_SHARD ` ` cpu_offload Optional CPUOffload This configures CPU offloading If set ` ` None ` ` then no CPU offloading happens See ` CPUOffload ` details Default ` ` None ` ` auto_wrap_policy Optional Union Callable nn Module bool int bool ModuleWrapPolicy CustomPolicy This specifies policy apply FSDP submodules ` ` module ` ` which needed communication computation overlap thus affects performance If ` ` None ` ` then FSDP only applies ` ` module ` ` users should manually apply FSDP parent modules themselves proceeding bottom-up For convenience accepts ` ` ModuleWrapPolicy ` ` directly which allows users specify module classes wrap e g transformer block Otherwise should callable takes three arguments ` ` module nn Module ` ` ` ` recurse bool ` ` ` ` nonwrapped_numel int ` ` should ` ` bool ` ` specifying whether passed-in ` ` module ` ` should have FSDP applied ` ` recurse=False ` ` traversal should continue into module s subtree ` ` recurse=True ` ` Users may add additional arguments callable The ` ` size_based_auto_wrap_policy ` ` ` ` torch distributed fsdp wrap py ` ` gives example callable applies FSDP module parameters its subtree exceed M numel We recommend printing model after applying FSDP adjusting needed Example custom_auto_wrap_policy module nn Module recurse bool nonwrapped_numel int Additional custom arguments min_num_params int = int e - bool nonwrapped_numel = min_num_params Configure custom ` min_num_params ` my_auto_wrap_policy = functools partial custom_auto_wrap_policy min_num_params=int e backward_prefetch Optional BackwardPrefetch This configures explicit backward prefetching all-gathers If ` ` None ` ` then FSDP does backward prefetch there no communication computation overlap backward pass See ` BackwardPrefetch ` details Default ` ` BACKWARD_PRE ` ` mixed_precision Optional MixedPrecision This configures native mixed precision FSDP If set ` ` None ` ` then no mixed precision used Otherwise parameter buffer gradient reduction dtypes can set See ` MixedPrecision ` details Default ` ` None ` ` ignored_modules Optional Iterable torch nn Module Modules whose own parameters child modules parameters buffers ignored instance None modules directly ` ` ignored_modules ` ` should ` FullyShardedDataParallel ` instances any child modules already-constructed ` FullyShardedDataParallel ` instances will ignored they nested under instance This argument may used avoid sharding specific parameters module granularity when using ` ` auto_wrap_policy ` ` parameters sharding managed FSDP Default ` ` None ` ` param_init_fn Optional Callable nn Module None A ` ` Callable torch nn Module - None ` ` specifies how modules currently meta device should initialized onto actual device As v FSDP detects modules parameters buffers meta device via ` ` is_meta ` ` either applies ` ` param_init_fn ` ` specified calls ` ` nn Module reset_parameters ` ` otherwise For both cases implementation should only initialize parameters buffers module those its submodules This avoid re-initialization In addition FSDP also supports deferred initialization via torchdistX s https github com pytorch torchdistX ` ` deferred_init ` ` API where deferred modules initialized calling ` ` param_init_fn ` ` specified torchdistX s default ` ` materialize_module ` ` otherwise If ` ` param_init_fn ` ` specified then applied all meta-device modules meaning should probably case module type FSDP calls initialization function before parameter flattening sharding Example xdoctest +SKIP undefined variables module = MyModule device= meta my_init_fn module nn Module E g initialize depending module type fsdp_model = FSDP module param_init_fn=my_init_fn auto_wrap_policy=size_based_auto_wrap_policy print next fsdp_model parameters device current CUDA device With torchdistX module = deferred_init deferred_init MyModule device= cuda Will initialize via deferred_init materialize_module fsdp_model = FSDP module auto_wrap_policy=size_based_auto_wrap_policy device_id Optional Union int torch device An ` ` int ` ` ` ` torch device ` ` giving CUDA device which FSDP initialization takes place including module initialization needed parameter sharding This should specified improve initialization speed ` ` module ` ` CPU If default CUDA device set e g via ` ` torch cuda set_device ` ` then user may pass ` ` torch cuda current_device ` ` Default ` ` None ` ` sync_module_states bool If ` ` True ` ` then each FSDP module will broadcast module parameters buffers rank ensure they replicated across ranks adding communication overhead constructor This can help load ` ` state_dict ` ` checkpoints via ` ` load_state_dict ` ` memory efficient way See ` FullStateDictConfig ` example Default ` ` False ` ` forward_prefetch bool If ` ` True ` ` then FSDP explicitly prefetches next forward-pass all-gather before current forward computation This only useful CPU-bound workloads which case issuing next all-gather earlier may improve overlap This should only used static-graph models since prefetching follows first iteration s execution order Default ` ` False ` ` limit_all_gathers bool If ` ` True ` ` then FSDP explicitly synchronizes CPU thread ensure GPU memory usage only two consecutive FSDP instances current instance running computation next instance whose all-gather prefetched If ` ` False ` ` then FSDP allows CPU thread issue all-gathers without any extra synchronization Default ` ` True ` ` We often refer feature rate limiter This flag should only set ` ` False ` ` specific CPU-bound workloads low memory pressure which case CPU thread can aggressively issue all kernels without concern GPU memory usage use_orig_params bool Setting ` ` True ` ` has FSDP use ` ` module ` ` s original parameters FSDP exposes those original parameters user via meth ` nn Module named_parameters ` instead FSDP s internal ` FlatParameter ` s This means optimizer step runs original parameters enabling per-original-parameter hyperparameters FSDP preserves original parameter variables manipulates their data between unsharded sharded forms where they always views into underlying unsharded sharded ` FlatParameter ` respectively With current algorithm sharded form always D losing original tensor structure An original parameter may have all some none its data present given rank In none case its data will like size- empty tensor Users should author programs relying what data present given original parameter its sharded form ` ` True ` ` required use ` ` torch compile ` ` Setting ` ` False ` ` exposes FSDP s internal ` FlatParameter ` s user via meth ` nn Module named_parameters ` Default ` ` False ` ` ignored_states Optional Iterable torch nn Parameter Optional Iterable torch nn Module Ignored parameters modules will managed FSDP instance meaning parameters sharded their gradients reduced across ranks This argument unifies existing ` ` ignored_modules ` ` argument we may deprecate ` ` ignored_modules ` ` soon For backward compatibility we keep both ` ` ignored_states ` ` ` ignored_modules ` ` FSDP only allows one them specified ` ` None ` ` device_mesh Optional DeviceMesh DeviceMesh can used alternative process_group When device_mesh passed FSDP will use underlying process groups all-gather reduce-scatter collective communications Therefore these two args need mutually exclusive For hybrid sharding strategies such ` ` ShardingStrategy HYBRID_SHARD ` ` users can pass D DeviceMesh instead tuple process groups For D FSDP + TP users required pass device_mesh instead process_group For more DeviceMesh info please visit https pytorch org tutorials recipes distributed_device_mesh html __init__ module nn Module process_group ProcessGroupType = None sharding_strategy Optional ShardingStrategy = None cpu_offload Optional CPUOffload = None auto_wrap_policy Optional Union Callable ModuleWrapPolicy CustomPolicy = None backward_prefetch Optional BackwardPrefetch = BackwardPrefetch BACKWARD_PRE mixed_precision Optional MixedPrecision = None ignored_modules Optional Iterable torch nn Module = None param_init_fn Optional Callable nn Module None = None device_id Optional Union int torch device = None sync_module_states bool = False forward_prefetch bool = False limit_all_gathers bool = True use_orig_params bool = False ignored_states Union Optional Iterable torch nn Parameter Optional Iterable torch nn Module = None device_mesh Optional DeviceMesh = None torch _C _log_api_usage_once torch distributed fsdp super __init__ isinstance module nn ModuleList nn ModuleDict warnings warn FSDP will all-gather parameters containers do f implement forward module stacklevel= _init_ignored_module_states module ignored_modules ignored_states _init_device_handle module _ignored_params device_id Add module annotations Dynamo support see function details _annotate_modules_for_dynamo module _ignored_modules use_orig_params Initializes process_group along rank world size This will also set another attribute _inter_node_pg control process group over which sharding occurs sharding_strategy HYBRID_SHARD _HYBRID_SHARD_ZERO Note done before auto_wrapping so child FSDP modules simply pick up same process group state root FSDP module _device_mesh = device_mesh _init_process_group_state process_group sharding_strategy auto_wrap_policy device_mesh auto_wrap_policy None root_kwargs = process_group process_group sharding_strategy sharding_strategy cpu_offload cpu_offload backward_prefetch backward_prefetch mixed_precision mixed_precision param_init_fn param_init_fn device_id device_id sync_module_states sync_module_states forward_prefetch forward_prefetch limit_all_gathers limit_all_gathers use_orig_params use_orig_params ignored_states _ignored_params device_mesh device_mesh sharding_strategy HYBRID_SHARDING_STRATEGIES device_mesh None Share root process groups children maintain invariant all FSDP modules will have same process groups root_kwargs process_group = process_group _inter_node_pg _auto_wrap module auto_wrap_policy _ignored_modules _ignored_params root_kwargs FullyShardedDataParallel backward_prefetch_limit = forward_prefetch_limit = _init_core_state sharding_strategy mixed_precision cpu_offload limit_all_gathers use_orig_params backward_prefetch_limit forward_prefetch_limit _init_runtime_state _init_prefetching_state backward_prefetch forward_prefetch _init_buffer_state module extension needs set before ` _init_param_handle_from_module ` _init_extension device_mesh _init_param_handle_from_module module device_id param_init_fn sync_module_states _fsdp_wrapped_module = module use_orig_params _check_orig_params_flattened _ignored_params _register_flat_param ` _state_dict_type ` controls ` state_dict ` behavior which implemented using post-save pre-load hooks _init_state_dict_state _register_all_state_dict_hooks _zero_scalar = None property module - nn Module Return wrapped module FSDP s ` module ` must refer innermost wrapped module when composing other module wrappers order state dict work isinstance _fsdp_wrapped_module ActivationWrapper getattr _fsdp_wrapped_module _CHECKPOINT_WRAPPED_MODULE _fsdp_wrapped_module property _has_params - bool Returns whether FSDP instance manages any parameters hasattr _handle _handle None property _flat_param - Optional FlatParameter _handle flat_param _handle None __getattr__ name str - Any Forward missing attributes wrapped module try super __getattr__ name defer nn Module s logic except AttributeError getattr _fsdp_wrapped_module name __getitem__ key int - Any Forward indexing calls case module ` ` nn Sequential ` ` hasattr FSDP_WRAPPED_MODULE _fsdp_wrapped_module __getitem__ key type ignore operator super __getitem__ key check_is_root - bool Check instance root FSDP module _is_fsdp_root staticmethod fsdp_modules module nn Module root_only bool = False - list FullyShardedDataParallel Return all nested FSDP instances This possibly includes ` ` module ` ` itself only includes FSDP root modules ` ` root_only=True ` ` Args module torch nn Module Root module which may may ` ` FSDP ` ` module root_only bool Whether only FSDP root modules Default ` ` False ` ` Returns List FullyShardedDataParallel FSDP modules nested input ` ` module ` ` root_only _get_fsdp_root_states module traversal_utils _get_fsdp_states module apply fn Callable nn Module None - FullyShardedDataParallel r Apply ` ` fn ` ` recursively every submodule returned ` ` children ` ` well Typical use includes initializing parameters model see also ref ` nn-init-doc ` Compared ` ` torch nn Module apply ` ` version additionally gathers full parameters before applying ` ` fn ` ` It should called within another ` ` summon_full_params ` ` context Args fn ` Module ` - None function applied each submodule Returns Module uninitialized = _is_root None _assert_state TrainingState IDLE Use ` _unshard_params_for_summon ` ` recurse=False ` instead ` _unshard_fsdp_state_params ` directly perform lazy initialization which needed initialize ` FlatParameter ` parameter attributes required unshard logic _unshard_params_for_summon writeback=True rank _only=False offload_to_cpu=False with_grads=False ret = super apply fn Reset lazy init called ` _unshard_params_for_summon ` since ` apply ` may have been called FSDP instance truly root which case will incorrectly marked one uninitialized _is_root module traversal_utils _get_fsdp_states module _reset_lazy_init ret _mixed_precision_enabled_for_buffers - bool Return whether user explicitly enabled buffer mixed precision NOTE Unlike parameters gradient reduction buffer mixed precision applied FSDP instance level ` ` FlatParameter ` ` level which may different composable code path mixed_precision buffer_dtype None _low_precision_hook_enabled - bool Whether low precision hook registered _comm_hook None _comm_hook LOW_PRECISION_HOOKS _reset_lazy_init - None Reset instance so func ` _lazy_init ` will run next forward _is_root Optional bool = None staticmethod set_state_dict_type module nn Module state_dict_type StateDictType state_dict_config Optional StateDictConfig = None optim_state_dict_config Optional OptimStateDictConfig = None - StateDictSettings Set ` ` state_dict_type ` ` all descendant FSDP modules target module Also takes optional configuration model s optimizer s state dict The target module does have FSDP module If target module FSDP module its ` ` state_dict_type ` ` will also changed note This API should called only top-level root module note This API enables users transparently use conventional ` ` state_dict ` ` API take model checkpoints cases where root FSDP module wrapped another ` ` nn Module ` ` For example following will ensure ` ` state_dict ` ` called all non-FSDP instances while dispatching into ` sharded_state_dict ` implementation FSDP Example xdoctest +SKIP undefined variables model = DDP FSDP FSDP set_state_dict_type model StateDictType SHARDED_STATE_DICT state_dict_config = ShardedStateDictConfig offload_to_cpu=True optim_state_dict_config = OptimStateDictConfig offload_to_cpu=True param_state_dict = model state_dict optim_state_dict = FSDP optim_state_dict model optim Args module torch nn Module Root module state_dict_type StateDictType desired ` ` state_dict_type ` ` set state_dict_config Optional StateDictConfig configuration target ` ` state_dict_type ` ` optim_state_dict_config Optional OptimStateDictConfig configuration optimizer state dict Returns A StateDictSettings include previous state_dict type configuration module warnings warn FSDP state_dict_type FSDP set_state_dict_type being deprecated Please use APIs get_state_dict set_state_dict which can support different parallelisms FSDP FSDP DDP API doc https pytorch org docs stable distributed checkpoint html #torch distributed checkpoint state_dict get_state_dict Tutorial https pytorch org tutorials recipes distributed_checkpoint_recipe html FutureWarning stacklevel= _state_dict_type_to_config = StateDictType FULL_STATE_DICT FullStateDictConfig StateDictType LOCAL_STATE_DICT LocalStateDictConfig StateDictType SHARDED_STATE_DICT ShardedStateDictConfig _optim_state_dict_type_to_config = StateDictType FULL_STATE_DICT FullOptimStateDictConfig StateDictType LOCAL_STATE_DICT LocalOptimStateDictConfig StateDictType SHARDED_STATE_DICT ShardedOptimStateDictConfig Use default config state_dict config set state_dict_config_type = _state_dict_type_to_config state_dict_type optim_state_dict_config_type = _optim_state_dict_type_to_config state_dict_type state_dict_config None state_dict_config = state_dict_config_type optim_state_dict_config None optim_state_dict_config = optim_state_dict_config_type state_dict_config_type type state_dict_config raise RuntimeError f Expected state_dict_config type state_dict_config_type f got type state_dict_config optim_state_dict_config_type type optim_state_dict_config raise RuntimeError f Expected optim_state_dict_config type optim_state_dict_config_type f got type optim_state_dict_config Set state_dict type configurations prev_state_dict_type = None prev_state_dict_config = None prev_optim_state_dict_config = None submodule traversal_utils _get_fsdp_states module prev_state_dict_type None prev_state_dict_type = submodule _state_dict_type prev_state_dict_type = submodule _state_dict_type raise AssertionError All FSDP modules should have same state_dict_type prev_state_dict_config None prev_state_dict_config = submodule _state_dict_config isinstance submodule _state_dict_config type prev_state_dict_config raise AssertionError All FSDP modules must have same type state_dict_config prev_optim_state_dict_config None prev_optim_state_dict_config = submodule _optim_state_dict_config isinstance submodule _optim_state_dict_config type prev_optim_state_dict_config raise AssertionError All FSDP modules must have same type optim_state_dict_config submodule _state_dict_type = state_dict_type submodule _state_dict_config = state_dict_config submodule _optim_state_dict_config = optim_state_dict_config StateDictSettings prev_state_dict_type prev_state_dict_config prev_optim_state_dict_config staticmethod get_state_dict_type module nn Module - StateDictSettings Get state_dict_type corresponding configurations FSDP modules rooted ` ` module ` ` The target module does have FSDP module Returns A ` ` StateDictSettings ` ` containing state_dict_type state_dict optim_state_dict configs currently set Raises ` ` AssertionError ` ` ` ` StateDictSettings ` ` different FSDP submodules differ state_dict_settings Optional StateDictSettings = None submodule FullyShardedDataParallel fsdp_modules module state_dict_settings None state_dict_settings = StateDictSettings state_dict_type=submodule _state_dict_type state_dict_config=submodule _state_dict_config optim_state_dict_config=submodule _optim_state_dict_config _set_optim_use_dtensor submodule state_dict_settings submodule_settings = StateDictSettings submodule _state_dict_type submodule _state_dict_config submodule _optim_state_dict_config state_dict_settings = submodule_settings raise AssertionError All FSDP modules must have same state dict settings f Got submodule_settings state_dict_settings _set_optim_use_dtensor submodule submodule_settings state_dict_settings staticmethod contextlib contextmanager state_dict_type module nn Module state_dict_type StateDictType state_dict_config Optional StateDictConfig = None optim_state_dict_config Optional OptimStateDictConfig = None - Generator Set ` ` state_dict_type ` ` all descendant FSDP modules target module This context manager has same functions meth ` set_state_dict_type ` Read document meth ` set_state_dict_type ` detail Example xdoctest +SKIP undefined variables model = DDP FSDP FSDP state_dict_type model StateDictType SHARDED_STATE_DICT checkpoint = model state_dict Args module torch nn Module Root module state_dict_type StateDictType desired ` ` state_dict_type ` ` set state_dict_config Optional StateDictConfig model ` ` state_dict ` ` configuration target ` ` state_dict_type ` ` optim_state_dict_config Optional OptimStateDictConfig optimizer ` ` state_dict ` ` configuration target ` ` state_dict_type ` ` prev_state_dict_settings = FullyShardedDataParallel set_state_dict_type module state_dict_type state_dict_config optim_state_dict_config yield FullyShardedDataParallel set_state_dict_type module prev_state_dict_settings state_dict_type prev_state_dict_settings state_dict_config prev_state_dict_settings optim_state_dict_config forward args Any kwargs Any - Any Run forward pass wrapped module inserting FSDP-specific pre- post-forward sharding logic handle = _handle torch autograd profiler record_function FullyShardedDataParallel forward args kwargs = _root_pre_forward args kwargs unused = None args kwargs = _pre_forward handle _pre_forward_unshard _fsdp_wrapped_module args kwargs handle _p_assert handle flat_param device == compute_device Expected ` FlatParameter ` compute device f compute_device got handle flat_param device output = _fsdp_wrapped_module args kwargs _post_forward handle _post_forward_reshard unused output staticmethod contextlib contextmanager summon_full_params module nn Module recurse bool = True writeback bool = True rank _only bool = False offload_to_cpu bool = False with_grads bool = False - Generator r Expose full params FSDP instances context manager Can useful after forward backward model get params additional processing checking It can take non-FSDP module will summon full params all contained FSDP modules well their children depending ` ` recurse ` ` argument note This can used inner FSDPs note This can used within forward backward pass Nor can forward backward started within context note Parameters will revert their local shards after context manager exits storage behavior same forward note The full parameters can modified only portion corresponding local param shard will persist after context manager exits unless ` ` writeback=False ` ` which case changes will discarded In case where FSDP does shard parameters currently only when ` ` world_size == ` ` ` ` NO_SHARD ` ` config modification persisted regardless ` ` writeback ` ` note This method works modules which FSDP themselves may contain multiple independent FSDP units In case given arguments will apply all contained FSDP units warning Note ` ` rank _only=True ` ` conjunction ` ` writeback=True ` ` currently supported will raise error This because model parameter shapes would different across ranks within context writing them can lead inconsistency across ranks when context exited warning Note ` ` offload_to_cpu ` ` ` ` rank _only=False ` ` will result full parameters being redundantly copied CPU memory GPUs reside same machine which may incur risk CPU OOM It recommended use ` ` offload_to_cpu ` ` ` ` rank _only=True ` ` Args recurse bool Optional recursively summon all params nested FSDP instances default True writeback bool Optional ` ` False ` ` modifications params discarded after context manager exits disabling can slightly more efficient default True rank _only bool Optional ` ` True ` ` full parameters materialized only global rank This means within context only rank will have full parameters other ranks will have sharded parameters Note setting ` ` rank _only=True ` ` ` ` writeback=True ` ` supported model parameter shapes will different across ranks within context writing them can lead inconsistency across ranks when context exited offload_to_cpu bool Optional If ` ` True ` ` full parameters offloaded CPU Note offloading currently only occurs parameter sharded which only case world_size = ` ` NO_SHARD ` ` config It recommended use ` ` offload_to_cpu ` ` ` ` rank _only=True ` ` avoid redundant copies model parameters being offloaded same CPU memory with_grads bool Optional If ` ` True ` ` gradients also unsharded parameters Currently only supported when passing ` ` use_orig_params=True ` ` FSDP constructor ` ` offload_to_cpu=False ` ` method Default ` ` False ` ` _unshard_params module recurse writeback rank _only offload_to_cpu with_grads yield contextlib contextmanager _deregister_orig_params_ctx Deregister original parameters expose ` FlatParameter ` If ` FlatParameter ` sharded then refreshes sharded views before exiting This method should only called when using original parameters _p_assert _use_orig_params ` _deregister_orig_params_ctx ` should only called when ` _use_orig_params=True ` fsdp_module traversal_utils _get_fsdp_states _deregister_orig_params fsdp_module fsdp_module try yield finally fsdp_module traversal_utils _get_fsdp_states _register_orig_params fsdp_module fsdp_module _apply args kwargs Deregister original parameters expose ` FlatParameter ` s before calling ` ` _apply ` ` When using original parameters Since ` FlatParameter ` s own storage ` _apply ` subroutine underlying most common storage-changing ops like ` ` ` cuda ` we override ` _apply ` have storage change directly performed ` FlatParameter ` s instead applying original parameters then writing back ` FlatParameter ` s context = _deregister_orig_params_ctx _use_orig_params contextlib nullcontext context super _apply args kwargs named_buffers args kwargs - Iterator tuple str torch Tensor Return iterator over module buffers yielding both name buffer buffer itself Intercepts buffer names removes all occurrences FSDP-specific flattened buffer prefix when inside meth ` summon_full_params ` context manager should_clean_name = training_state == TrainingState SUMMON_FULL_PARAMS buffer_name buffer super named_buffers args kwargs should_clean_name Remove any instances FSDP-specific prefix there can multiple case nested FSDP modules buffer_name = buffer_name replace FSDP_PREFIX yield buffer_name buffer named_parameters args kwargs - Iterator tuple str torch nn Parameter Return iterator over module parameters yielding both name parameter parameter itself Intercepts parameter names removes all occurrences FSDP-specific flattened parameter prefix when inside meth ` summon_full_params ` context manager should_clean_name = training_state == TrainingState SUMMON_FULL_PARAMS param_name param super named_parameters args kwargs should_clean_name Remove any instances FSDP-specific prefix there can multiple case nested FSDP modules param_name = param_name replace FSDP_PREFIX yield param_name param _assert_state state Union TrainingState list TrainingState - None Assert we given state Since assert can turned off error checking really important we use explicit error checking raise ValueError needed isinstance state TrainingState state = state training_state state msg = f expected states state current state f training_state In case we failing context autograd hook asserting may generate useful msg So let s print sure rank == print f Asserting FSDP instance print f ERROR msg traceback print_stack raise ValueError msg contextmanager no_sync - Generator Disable gradient synchronizations across FSDP instances Within context gradients will accumulated module variables which will later synchronized first forward-backward pass after exiting context This should only used root FSDP instance will recursively apply all children FSDP instances note This likely results higher memory usage because FSDP will accumulate full model gradients instead gradient shards until eventual sync note When used CPU offloading gradients will offloaded CPU when inside context manager Instead they will only offloaded right after eventual sync _lazy_init _is_root raise RuntimeError ` no_sync ` inner FSDP instances supported Please call ` no_sync ` root FSDP module _assert_state TrainingState IDLE old_flags = m modules isinstance m FullyShardedDataParallel old_flags append m m _sync_gradients m _sync_gradients = False try yield finally m old_flag old_flags m _sync_gradients raise AssertionError ` _sync_gradients ` incorrectly set ` True ` while ` no_sync ` context manager m _sync_gradients = old_flag torch no_grad clip_grad_norm_ max_norm Union float int norm_type Union float int = - torch Tensor Clip gradient norm all parameters The norm computed over all parameters gradients viewed single vector gradients modified in-place Args max_norm float int max norm gradients norm_type float int type used p-norm Can ` ` inf ` ` infinity norm Returns Total norm parameters viewed single vector If every FSDP instance uses ` ` NO_SHARD ` ` meaning no gradients sharded across ranks then you may directly use func ` torch nn utils clip_grad_norm_ ` If least some FSDP instance uses sharded strategy i e one other than ` ` NO_SHARD ` ` then you should use method instead func ` torch nn utils clip_grad_norm_ ` since method handles fact gradients sharded across ranks The total norm returned will have largest dtype across all parameters gradients defined PyTorch s type promotion semantics For example all parameters gradients use low precision dtype then returned norm s dtype will low precision dtype there exists least one parameter gradient using FP then returned norm s dtype will FP warning This needs called all ranks since uses collective communications _lazy_init _is_root raise RuntimeError ` clip_grad_norm_ ` should only called root FSDP instance _zero_scalar None _zero_scalar = torch tensor device=self compute_device _assert_state TrainingState IDLE If every FSDP instance uses ` NO_SHARD ` then we can directly use normal ` nn utils ` one targeting local gradients all_no_shard = all handle uses_sharded_strategy handle _all_handles all_no_shard torch nn utils clip_grad_norm_ parameters max_norm norm_type Otherwise there exists some FSDP instance using sharded strategy where sharded non-sharded parameters must handled separately max_norm = float max_norm norm_type = float norm_type sharded_params_set = set nonsharded_params_set = set ` NO_SHARD ` FSDP-managed Make sure compute local norm using lists deterministic iteration order hence deterministic total norm computation sharded_params = nonsharded_params = grads list torch Tensor = handle _all_handles handle uses_sharded_strategy target_set = sharded_params_set target_list = sharded_params target_set = nonsharded_params_set target_list = nonsharded_params handle _use_orig_params param handle flat_param _params param target_set target_set add param target_list append param param grad None grads append param grad handle flat_param target_set target_set add handle flat_param target_list append handle flat_param handle flat_param grad None grads append handle flat_param grad param parameters not_fsdp_managed = param sharded_params_set param nonsharded_params_set not_fsdp_managed nonsharded_params_set add param nonsharded_params append param param grad None grads append param grad Compute local norms forced FP local_sharded_norm = _get_grad_norm sharded_params norm_type _zero_scalar compute_device local_nonsharded_norm = _get_grad_norm nonsharded_params norm_type _zero_scalar compute_device nonsharded_params None Reconstruct total gradient norm depending norm type norm_type == math inf total_norm = torch maximum local_sharded_norm local_nonsharded_norm local_nonsharded_norm None local_sharded_norm dist all_reduce total_norm op=torch distributed ReduceOp MAX group=self process_group total_norm = local_sharded_norm norm_type dist all_reduce total_norm group=self process_group All-reducing local non-sharded norm would count extra world-size-many times local_nonsharded_norm None total_norm += local_nonsharded_norm norm_type total_norm = total_norm norm_type cpu_offload offload_params total_norm = total_norm cpu clip_coef = max_norm total_norm + e- Multiplying clamped coefficient meaningless when equal avoids host-device sync would result ` clip_coef ` clip_coef_clamped = torch clamp clip_coef max= grad grads grad mul_ clip_coef_clamped grad device grad dtype Use largest dtype type promotion semantics use same dtype we did force local norm computation FP len grads == If rank has no gradients then we must default FP unless we use additional communication which we prefer avoid since ` clip_grad_norm_ ` called training loop warnings warn f Called FSDP clip_grad_norm_ rank rank no gradients -- returning total norm default dtype f total_norm dtype stacklevel= warn since generally unexpected total_norm total_norm_dtype = functools reduce torch promote_types grad dtype grad grads total_norm total_norm_dtype staticmethod _warn_optim_input optim_input stacklevel int = optim_input None warnings warn The ` optim_input ` argument deprecated will removed after PyTorch You may remove your code without changing its functionality FutureWarning stacklevel=stacklevel + staticmethod _is_using_optim_input optim_input optim - bool optim_input None optim None Use default behavior ` optim_input ` ` True optim_input None Use ` optim_input ` code path True Use ` optim ` code path False staticmethod _warn_legacy_optim_state_dict curr str new str stacklevel int = warnings warn f ` ` FullyShardedDataParallel curr ` ` being deprecated f replaced ` ` FullyShardedDataParallel new ` ` f ` ` FullyShardedDataParallel curr ` ` may removed after PyTorch FutureWarning stacklevel=stacklevel + staticmethod _optim_state_dict_impl model torch nn Module optim torch optim Optimizer optim_state_dict dict str Any optim_input Optional Union list dict str Any Iterable torch nn Parameter = None rank _only bool = True full_state_dict bool = True group Optional dist ProcessGroup = None cpu_offload bool = True _stacklevel int = - dict str Any Transform state-dict optimizer corresponding sharded model This internal API used all optim_state_dict implementations Given model optim original optim_state_dict API removes FSDP internal information internal sharding optim_state_dict full_state_dict FullyShardedDataParallel _warn_optim_input optim_input stacklevel=_stacklevel + using_optim_input = FullyShardedDataParallel _is_using_optim_input optim_input optim using_optim_input = False optim_input None rank _only raise AssertionError f Expected optim_input None rank _only False f got optim_input= optim_input rank _only= rank _only use_orig_params = FullyShardedDataParallel fsdp_modules model _use_orig_params all use_orig_params == m _use_orig_params m FullyShardedDataParallel fsdp_modules model raise AssertionError Not all FSDP modules have same _use_orig_params value _optim_state_dict model=model optim=optim optim_state_dict=optim_state_dict optim_input=optim_input rank _only=rank _only shard_state=not full_state_dict group=group using_optim_input=using_optim_input use_orig_params=use_orig_params cpu_offload=cpu_offload staticmethod _optim_state_dict_to_load_impl optim_state_dict dict str Any model torch nn Module optim_input Optional Union list dict str Any Iterable torch nn Parameter = None optim Optional torch optim Optimizer = None full_state_dict bool = True rank _only bool = False is_named_optimizer bool = False group Optional dist ProcessGroup = None - dict str Any Convert optimizer state-dict so can loaded into optimizer associated FSDP model This internal API used all load optim_state_dict implementations Given model optim saved optim_state_dict API adds FSDP internal information internal sharding optim_state_dict full_state_dict FullyShardedDataParallel _warn_optim_input optim_input using_optim_input = FullyShardedDataParallel _is_using_optim_input optim_input optim using_optim_input = False optim_input None rank _only raise AssertionError f Expected optim_input None rank _only False f got optim_input= optim_input rank _only= rank _only use_orig_params = FullyShardedDataParallel fsdp_modules model _use_orig_params all use_orig_params == m _use_orig_params m FullyShardedDataParallel fsdp_modules model raise AssertionError Not all FSDP modules have same _use_orig_params value rank _only dist get_rank group optim_state_dict = sharded_osd = _flatten_optim_state_dict optim_state_dict model=model use_orig_params=use_orig_params optim= optim is_named_optimizer None rank _only=rank _only group=group _rekey_sharded_optim_state_dict sharded_osd model=model optim=optim optim_input=optim_input using_optim_input=using_optim_input is_named_optimizer=is_named_optimizer staticmethod full_optim_state_dict model torch nn Module optim torch optim Optimizer optim_input Optional Union list dict str Any Iterable torch nn Parameter = None rank _only bool = True group Optional dist ProcessGroup = None - dict str Any Return full optimizer state-dict Consolidates full optimizer state rank returns ` dict ` following convention meth ` torch optim Optimizer state_dict ` i e keys ` ` state ` ` ` ` param_groups ` ` The flattened parameters ` ` FSDP ` ` modules contained ` ` model ` ` mapped back their unflattened parameters This needs called all ranks since uses collective communications However ` ` rank _only=True ` ` then state dict only populated rank all other ranks empty ` dict ` Unlike ` ` torch optim Optimizer state_dict ` ` method uses full parameter names keys instead parameter IDs Like meth ` torch optim Optimizer state_dict ` tensors contained optimizer state dict cloned so there may aliasing surprises For best practices consider saving returned optimizer state dict immediately e g using ` ` torch save ` ` Args model torch nn Module Root module which may may ` FullyShardedDataParallel ` instance whose parameters passed into optimizer ` ` optim ` ` optim torch optim Optimizer Optimizer ` ` model ` ` s parameters optim_input Optional Union List Dict str Any Iterable torch nn Parameter Input passed into optimizer ` ` optim ` ` representing either ` list ` parameter groups iterable parameters ` ` None ` ` then method assumes input ` ` model parameters ` ` This argument deprecated there no need pass anymore Default ` ` None ` ` rank _only bool If ` ` True ` ` saves populated ` dict ` only rank ` ` False ` ` saves all ranks Default ` ` True ` ` group dist ProcessGroup Model s process group ` ` None ` ` using default process group Default ` ` None ` ` Returns Dict str Any A ` dict ` containing optimizer state ` ` model ` ` s original unflattened parameters including keys state param_groups following convention meth ` torch optim Optimizer state_dict ` If ` ` rank _only=True ` ` then nonzero ranks empty ` dict ` FullyShardedDataParallel _warn_legacy_optim_state_dict full_optim_state_dict optim_state_dict stacklevel= FullyShardedDataParallel _optim_state_dict_impl model=model optim=optim optim_state_dict=optim state_dict optim_input=optim_input rank _only=rank _only group=group full_state_dict=True _stacklevel= staticmethod sharded_optim_state_dict model torch nn Module optim torch optim Optimizer group Optional dist ProcessGroup = None - dict str Any Return optimizer state-dict its sharded form The API similar meth ` full_optim_state_dict ` API chunks all non-zero-dimension states ` ShardedTensor ` save memory This API should only used when model ` ` state_dict ` ` derived context manager ` ` state_dict_type SHARDED_STATE_DICT ` ` For detailed usage refer meth ` full_optim_state_dict ` warning The returned state dict contains ` ` ShardedTensor ` ` cannot directly used regular ` ` optim load_state_dict ` ` FullyShardedDataParallel _warn_legacy_optim_state_dict sharded_optim_state_dict optim_state_dict stacklevel= FullyShardedDataParallel _optim_state_dict_impl model=model optim=optim optim_state_dict=optim state_dict optim_input=None rank _only=False full_state_dict=False group=group _stacklevel= staticmethod shard_full_optim_state_dict full_optim_state_dict dict str Any model torch nn Module optim_input Optional Union list dict str Any Iterable torch nn Parameter = None optim Optional torch optim Optimizer = None - dict str Any Shard full optimizer state-dict Remaps state ` ` full_optim_state_dict ` ` flattened parameters instead unflattened parameters restricts only rank s part optimizer state The first argument should value meth ` full_optim_state_dict ` Example xdoctest +SKIP undefined variables torch distributed fsdp FullyShardedDataParallel FSDP model optim = full_osd = FSDP full_optim_state_dict model optim torch save full_osd PATH Define new model possibly different world size new_model new_optim = full_osd = torch load PATH sharded_osd = FSDP shard_full_optim_state_dict full_osd new_model new_optim load_state_dict sharded_osd note Both meth ` shard_full_optim_state_dict ` meth ` scatter_full_optim_state_dict ` may used get sharded optimizer state dict load Assuming full optimizer state dict resides CPU memory former requires each rank have full dict CPU memory where each rank individually shards dict without any communication while latter requires only rank have full dict CPU memory where rank moves each shard GPU memory NCCL communicates ranks appropriately Hence former has higher aggregate CPU memory cost while latter has higher communication cost Args full_optim_state_dict Dict str Any Optimizer state dict corresponding unflattened parameters holding full non-sharded optimizer state model torch nn Module Root module which may may ` FullyShardedDataParallel ` instance whose parameters correspond optimizer state ` ` full_optim_state_dict ` ` optim_input Optional Union List Dict str Any Iterable torch nn Parameter Input passed into optimizer representing either ` list ` parameter groups iterable parameters ` ` None ` ` then method assumes input ` ` model parameters ` ` This argument deprecated there no need pass anymore Default ` ` None ` ` optim Optional torch optim Optimizer Optimizer will load state dict returned method This preferred argument use over ` ` optim_input ` ` Default ` ` None ` ` Returns Dict str Any The full optimizer state dict now remapped flattened parameters instead unflattened parameters restricted only include rank s part optimizer state FullyShardedDataParallel _warn_legacy_optim_state_dict shard_full_optim_state_dict optim_state_dict_to_load stacklevel= FullyShardedDataParallel _optim_state_dict_to_load_impl optim_state_dict=full_optim_state_dict model=model optim_input=optim_input optim=optim full_state_dict=True is_named_optimizer=False staticmethod flatten_sharded_optim_state_dict sharded_optim_state_dict dict str Any model torch nn Module optim torch optim Optimizer - dict str Any Flatten sharded optimizer state-dict The API similar meth ` shard_full_optim_state_dict ` The only difference input ` ` sharded_optim_state_dict ` ` should returned meth ` sharded_optim_state_dict ` Therefore there will all-gather calls each rank gather ` ` ShardedTensor ` ` s Args sharded_optim_state_dict Dict str Any Optimizer state dict corresponding unflattened parameters holding sharded optimizer state model torch nn Module Refer meth ` shard_full_optim_state_dict ` optim torch optim Optimizer Optimizer ` ` model ` ` s parameters Returns Refer meth ` shard_full_optim_state_dict ` FullyShardedDataParallel _warn_legacy_optim_state_dict flatten_sharded_optim_state_dict optim_state_dict_to_load stacklevel= FullyShardedDataParallel _optim_state_dict_to_load_impl optim_state_dict=sharded_optim_state_dict model=model optim_input=None optim=optim full_state_dict=False is_named_optimizer=False staticmethod scatter_full_optim_state_dict full_optim_state_dict Optional dict str Any model torch nn Module optim_input Optional Union list dict str Any Iterable torch nn Parameter = None optim Optional torch optim Optimizer = None group Optional Any = None - dict str Any Scatter full optimizer state dict rank all other ranks Returns sharded optimizer state dict each rank The value same meth ` shard_full_optim_state_dict ` rank first argument should value meth ` full_optim_state_dict ` Example xdoctest +SKIP undefined variables torch distributed fsdp FullyShardedDataParallel FSDP model optim = full_osd = FSDP full_optim_state_dict model optim only non-empty rank Define new model possibly different world size new_model new_optim new_group = sharded_osd = FSDP scatter_full_optim_state_dict full_osd new_model group=new_group new_optim load_state_dict sharded_osd note Both meth ` shard_full_optim_state_dict ` meth ` scatter_full_optim_state_dict ` may used get sharded optimizer state dict load Assuming full optimizer state dict resides CPU memory former requires each rank have full dict CPU memory where each rank individually shards dict without any communication while latter requires only rank have full dict CPU memory where rank moves each shard GPU memory NCCL communicates ranks appropriately Hence former has higher aggregate CPU memory cost while latter has higher communication cost Args full_optim_state_dict Optional Dict str Any Optimizer state dict corresponding unflattened parameters holding full non-sharded optimizer state rank argument ignored nonzero ranks model torch nn Module Root module which may may ` FullyShardedDataParallel ` instance whose parameters correspond optimizer state ` ` full_optim_state_dict ` ` optim_input Optional Union List Dict str Any Iterable torch nn Parameter Input passed into optimizer representing either ` list ` parameter groups iterable parameters ` ` None ` ` then method assumes input ` ` model parameters ` ` This argument deprecated there no need pass anymore Default ` ` None ` ` optim Optional torch optim Optimizer Optimizer will load state dict returned method This preferred argument use over ` ` optim_input ` ` Default ` ` None ` ` group dist ProcessGroup Model s process group ` ` None ` ` using default process group Default ` ` None ` ` Returns Dict str Any The full optimizer state dict now remapped flattened parameters instead unflattened parameters restricted only include rank s part optimizer state FullyShardedDataParallel _warn_legacy_optim_state_dict scatter_full_optim_state_dict optim_state_dict_to_load stacklevel= FullyShardedDataParallel _optim_state_dict_to_load_impl optim_state_dict=full_optim_state_dict model=model optim_input=optim_input optim=optim full_state_dict=True rank _only=True is_named_optimizer=False group=group staticmethod rekey_optim_state_dict optim_state_dict dict str Any optim_state_key_type OptimStateKeyType model torch nn Module optim_input Optional Union list dict str Any Iterable torch nn Parameter = None optim Optional torch optim Optimizer = None - dict str Any Re-keys optimizer state dict ` ` optim_state_dict ` ` use key type ` ` optim_state_key_type ` ` This can used achieve compatibility between optimizer state dicts models FSDP instances ones without To re-key FSDP full optimizer state dict i e meth ` full_optim_state_dict ` use parameter IDs loadable non-wrapped model xdoctest +SKIP undefined variables wrapped_model wrapped_optim = full_osd = FSDP full_optim_state_dict wrapped_model wrapped_optim nonwrapped_model nonwrapped_optim = rekeyed_osd = FSDP rekey_optim_state_dict full_osd OptimStateKeyType PARAM_ID nonwrapped_model nonwrapped_optim load_state_dict rekeyed_osd To re-key normal optimizer state dict non-wrapped model loadable wrapped model xdoctest +SKIP undefined variables nonwrapped_model nonwrapped_optim = osd = nonwrapped_optim state_dict rekeyed_osd = FSDP rekey_optim_state_dict osd OptimStateKeyType PARAM_NAME nonwrapped_model wrapped_model wrapped_optim = sharded_osd = FSDP shard_full_optim_state_dict rekeyed_osd wrapped_model wrapped_optim load_state_dict sharded_osd Returns Dict str Any The optimizer state dict re-keyed using parameter keys specified ` ` optim_state_key_type ` ` FullyShardedDataParallel _warn_optim_input optim_input using_optim_input = FullyShardedDataParallel _is_using_optim_input optim_input optim optim_state_key_type OptimStateKeyType PARAM_NAME OptimStateKeyType PARAM_ID raise AssertionError f Expected optim_state_key_type PARAM_NAME PARAM_ID got optim_state_key_type osd = optim_state_dict alias Validate existing parameter keys uniformly typed uses_param_name_mask = type param_key str param_key osd state uses_param_id_mask = type param_key int param_key osd state any uses_param_name_mask all uses_param_name_mask any uses_param_id_mask all uses_param_id_mask error_msg = f Invalid parameter keys osd state keys raise ValueError error_msg Return directly existing key type matches target key type optim_state_key_type == OptimStateKeyType PARAM_NAME all uses_param_name_mask optim_state_key_type == OptimStateKeyType PARAM_ID all uses_param_id_mask osd Otherwise actually perform re-keying new_osd = optim_state_key_type == OptimStateKeyType PARAM_NAME ID - name param_id_to_param = _get_param_id_to_param_from_optim_input model optim_input using_optim_input _get_param_key_to_param optim param_to_param_name = _get_param_to_fqn model param_id_to_param_name list str = param_to_param_name param param param_id_to_param values new_osd state = param_id_to_param_name param_id param_state param_id param_state osd state items new_osd param_groups = copy deepcopy osd param_groups param_group new_osd param_groups param_group params = sorted param_id_to_param_name param_id param_id param_group params new_osd optim_state_key_type == OptimStateKeyType PARAM_ID name - ID param_name_to_param = _get_fqn_to_param model param_to_param_id = _get_param_to_param_id_from_optim_input model optim_input using_optim_input _get_param_to_param_key optim Because all model parameters may passed optimizer input we may need drop some parameters mapping param_name_to_param_id = param_name param_to_param_id param param_name param param_name_to_param items param param_to_param_id new_osd state = param_name_to_param_id param_name param_state param_name param_state osd state items new_osd param_groups = copy deepcopy osd param_groups param_group new_osd param_groups param_group params = sorted param_name_to_param_id param_name param_name param_group params new_osd new_osd should never reach here staticmethod optim_state_dict model torch nn Module optim torch optim Optimizer optim_state_dict Optional dict str Any = None group Optional dist ProcessGroup = None - dict str Any Transform state-dict optimizer corresponding sharded model The given state-dict can transformed one three types full optimizer state_dict sharded optimizer state_dict local optimizer state_dict For full optimizer state_dict all states unflattened sharded Rank only CPU only can specified via meth ` state_dict_type ` avoid OOM For sharded optimizer state_dict all states unflattened sharded CPU only can specified via meth ` state_dict_type ` further save memory For local state_dict no transformation will performed But state will converted nn Tensor ShardedTensor represent its sharding nature supported yet Example xdoctest +SKIP undefined variables torch distributed fsdp FullyShardedDataParallel FSDP torch distributed fsdp StateDictType torch distributed fsdp FullStateDictConfig torch distributed fsdp FullOptimStateDictConfig Save checkpoint model optim = FSDP set_state_dict_type model StateDictType FULL_STATE_DICT FullStateDictConfig rank _only=False FullOptimStateDictConfig rank _only=False state_dict = model state_dict optim_state_dict = FSDP optim_state_dict model optim save_a_checkpoint state_dict optim_state_dict Load checkpoint model optim = state_dict optim_state_dict = load_a_checkpoint FSDP set_state_dict_type model StateDictType FULL_STATE_DICT FullStateDictConfig rank _only=False FullOptimStateDictConfig rank _only=False model load_state_dict state_dict optim_state_dict = FSDP optim_state_dict_to_load model optim optim_state_dict optim load_state_dict optim_state_dict Args model torch nn Module Root module which may may ` FullyShardedDataParallel ` instance whose parameters passed into optimizer ` ` optim ` ` optim torch optim Optimizer Optimizer ` ` model ` ` s parameters optim_state_dict Dict str Any target optimizer state_dict transform If value None optim state_dict will used Default ` ` None ` ` group dist ProcessGroup Model s process group across which parameters sharded ` ` None ` ` using default process group Default ` ` None ` ` Returns Dict str Any A ` dict ` containing optimizer state ` ` model ` ` The sharding optimizer state based ` ` state_dict_type ` ` state_dict_settings = FullyShardedDataParallel get_state_dict_type model optim_state_dict None optim_state_dict = optim state_dict FullyShardedDataParallel _optim_state_dict_impl model=model optim=optim optim_state_dict=optim_state_dict optim_input=None rank _only=getattr state_dict_settings optim_state_dict_config rank _only False full_state_dict=state_dict_settings state_dict_type == StateDictType FULL_STATE_DICT group=group cpu_offload=getattr state_dict_settings optim_state_dict_config offload_to_cpu True _stacklevel= staticmethod optim_state_dict_to_load model torch nn Module optim torch optim Optimizer optim_state_dict dict str Any is_named_optimizer bool = False load_directly bool = False group Optional dist ProcessGroup = None - dict str Any Convert optimizer state-dict so can loaded into optimizer associated FSDP model Given ` ` optim_state_dict ` ` transformed through meth ` optim_state_dict ` gets converted flattened optimizer state_dict can loaded ` ` optim ` ` which optimizer ` ` model ` ` ` ` model ` ` must sharded FullyShardedDataParallel xdoctest +SKIP undefined variables torch distributed fsdp FullyShardedDataParallel FSDP torch distributed fsdp StateDictType torch distributed fsdp FullStateDictConfig torch distributed fsdp FullOptimStateDictConfig Save checkpoint model optim = FSDP set_state_dict_type model StateDictType FULL_STATE_DICT FullStateDictConfig rank _only=False FullOptimStateDictConfig rank _only=False state_dict = model state_dict original_osd = optim state_dict optim_state_dict = FSDP optim_state_dict model optim optim_state_dict=original_osd save_a_checkpoint state_dict optim_state_dict Load checkpoint model optim = state_dict optim_state_dict = load_a_checkpoint FSDP set_state_dict_type model StateDictType FULL_STATE_DICT FullStateDictConfig rank _only=False FullOptimStateDictConfig rank _only=False model load_state_dict state_dict optim_state_dict = FSDP optim_state_dict_to_load model optim optim_state_dict optim load_state_dict optim_state_dict Args model torch nn Module Root module which may may ` FullyShardedDataParallel ` instance whose parameters passed into optimizer ` ` optim ` ` optim torch optim Optimizer Optimizer ` ` model ` ` s parameters optim_state_dict Dict str Any The optimizer states loaded is_named_optimizer bool Is optimizer NamedOptimizer KeyedOptimizer Only set True ` ` optim ` ` TorchRec s KeyedOptimizer torch distributed s NamedOptimizer load_directly bool If set True API will also call optim load_state_dict result before returning result Otherwise users responsible call ` ` optim load_state_dict ` ` Default ` ` False ` ` group dist ProcessGroup Model s process group across which parameters sharded ` ` None ` ` using default process group Default ` ` None ` ` state_dict_settings = FullyShardedDataParallel get_state_dict_type model result = FullyShardedDataParallel _optim_state_dict_to_load_impl optim_state_dict=optim_state_dict model=model optim_input=None optim=optim full_state_dict= state_dict_settings state_dict_type == StateDictType FULL_STATE_DICT rank _only=getattr state_dict_settings optim_state_dict_config rank _only False is_named_optimizer=is_named_optimizer group=group load_directly optim load_state_dict result result register_comm_hook state object hook callable Register communication hook This enhancement provides flexible hook users where they can specify how FSDP aggregates gradients across multiple workers This hook can used implement several algorithms like ` GossipGrad https arxiv org abs ` _ gradient compression which involve different communication strategies parameter syncs while training ` FullyShardedDataParallel ` warning FSDP communication hook should registered before running initial forward pass only once Args state object Passed hook maintain any state information during training process Examples include error feedback gradient compression peers communicate next ` GossipGrad https arxiv org abs ` _ etc It locally stored each worker shared all gradient tensors worker hook Callable Callable which has one following signatures ` ` hook Callable torch Tensor - None ` ` This function takes Python tensor which represents full flattened unsharded gradient respect all variables corresponding model FSDP unit wrapping wrapped other FSDP sub-units It then performs all necessary processing returns ` ` None ` ` ` ` hook Callable torch Tensor torch Tensor - None ` ` This function takes two Python tensors first one represents full flattened unsharded gradient respect all variables corresponding model FSDP unit wrapping wrapped other FSDP sub-units The latter represents pre-sized tensor store chunk sharded gradient after reduction In both cases callable performs all necessary processing returns ` ` None ` ` Callables signature expected handle gradient communication ` NO_SHARD ` case Callables signature expected handle gradient communication sharded cases check_is_root raise AssertionError register_comm_hook can only called root instance fsdp_state traversal_utils _get_fsdp_states fsdp_state sharding_strategy HYBRID_SHARDING_STRATEGIES raise AssertionError f Communication hook supported hybrid strategies fsdp_state sharding_strategy fsdp_state _comm_hook None raise AssertionError A communication hook already registered callable hook raise ValueError f The communication hook must callable got hook fsdp_state _comm_hook = hook fsdp_state _comm_hook_state = state _unshard async_op bool = False UnshardHandle __init__ flat_param_handle Optional FlatParamHandle unshard_event torch Event _flat_param_handle = flat_param_handle _unshard_event = unshard_event wait _flat_param_handle None current_stream = _flat_param_handle _device_handle current_stream current_stream wait_event _unshard_event _flat_param_handle = None _handle _use_training_state TrainingState FORWARD_BACKWARD HandleTrainingState FORWARD _unshard _handle _unshard_stream _pre_unshard_stream _unshard_event = _unshard_stream record_event _handle _prefetched = True unshard_handle = UnshardHandle _handle _unshard_stream async_op unshard_handle unshard_handle wait None _wait_unshard_streams_on_current_stream _wait_for_computation_stream _device_handle current_stream _unshard_stream _pre_unshard_stream contextlib contextmanager _use_training_state training_state TrainingState handle_training_state HandleTrainingState prev_training_state = training_state training_state = training_state _handle prev_handle_training_state = _handle _training_state _handle _training_state = handle_training_state try yield finally training_state = prev_training_state _handle _handle _training_state = prev_handle_training_state _get_grad_norm params Iterable nn Parameter norm_type float zero torch Tensor device torch device - torch Tensor Return gradient norm parameters ` ` param ` ` s where gradients viewed single vector The returned norm FP even parameters gradients low precision This because downstream use value reduction across ranks params_with_grad = param param params param grad None len params_with_grad == Reuse tensor zero avoid GPU sync zero grads = param grad param params_with_grad grad_dtypes = grad dtype grad grads len grad_dtypes = raise ValueError f Requires uniform dtype across all gradients got grad_dtypes Compute gradient norm FP where we treat gradients single vector grad_norm = torch linalg vector_norm torch stack torch linalg vector_norm grad detach norm_type dtype=torch float grad grads norm_type dtype=torch float grad_norm device=device _get_param_to_fqn model torch nn Module - dict torch nn Parameter str Construct mapping parameters their parameter names The ` ` model ` ` should contain any ` FullyShardedDataParallel ` instances which means none parameters should ` ` FlatParameter ` ` s As result compared meth ` _get_param_to_fqns ` mapped values may flattened singleton ` list ` s contained names themselves Args model torch nn Module Root module which should contain any ` FullyShardedDataParallel ` instances param_to_param_names = _get_param_to_fqns model param_names param_to_param_names values len param_names == raise AssertionError ` _get_param_to_fqns ` should construct empty lists len param_names raise RuntimeError Each parameter should only map one parameter name got f len param_names param_names param_to_param_name = param param_names param param_names param_to_param_names items param_to_param_name _get_fqn_to_param model torch nn Module - dict str torch nn Parameter Construct inverse mapping meth ` _get_param_to_fqn ` param_to_param_name = _get_param_to_fqn model dict zip param_to_param_name values param_to_param_name keys