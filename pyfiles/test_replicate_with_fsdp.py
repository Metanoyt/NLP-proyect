Owner s oncall distributed copy functools os copy deepcopy torch torch distributed dist torch nn torch distributed _composable contract _get_registry torch distributed _composable replicate_with_fsdp _get_managed_modules replicate torch distributed device_mesh DeviceMesh init_device_mesh torch distributed fsdp fully_shard torch distributed tensor Replicate Shard torch testing _internal common_distributed MultiProcessTestCase run_subtests skip_if_lt_x_gpu torch testing _internal common_fsdp check_sharded_parity MLPStack torch testing _internal common_utils run_tests torch testing _internal distributed _tensor common_dtensor ModelArgs Transformer Net nn Module __init__ - None super __init__ fc = nn Linear fc = nn Linear fc = nn Linear forward x fc fc fc x ReplicateTest MultiProcessTestCase property world_size - int init_replicate_tp_mesh - DeviceMesh Prefer test = GPUs GPUs use -way TP replicate_size = init_device_mesh cuda replicate_size world_size replicate_size mesh_dim_names= replicate shard tp setUp - None super setUp _spawn_processes tearDown super tearDown try os remove file_name except OSError pass _init_pg Set device explicitly before initializing process group torch cuda set_device rank world_size dist init_process_group backend= nccl rank=self rank world_size=self world_size store=dist FileStore file_name world_size skip_if_lt_x_gpu test_replicate_transformer This tests replicate works transformer model fully_shard replicate layers _init_pg run_subtests sharding_strategy replicate fully_shard _test_replicate_transformer _composable_api_module_check module sharding_strategy sharding_strategy == replicate assertTrue replicate _get_registry module assertTrue fully_shard _get_registry module _test_replicate_transformer sharding_strategy model_args = ModelArgs model = Transformer model_args replicate_model = deepcopy model i layer enumerate replicate_model layers i == replicate layer i == fully_shard layer sharding_strategy == replicate replicate_model = replicate replicate_model replicate_model = fully_shard replicate_model _composable_api_module_check replicate_model sharding_strategy i layer enumerate replicate_model layers i == assertTrue replicate _get_registry layer parameter layer parameters assertEqual parameter placements Replicate Shard dim= i == assertTrue fully_shard _get_registry layer parameter layer parameters assertEqual parameter placements Shard dim= skip_if_lt_x_gpu test_replicate_transformer_managed_modules This tests replicate managed modules works properly In test we use Transformer Module layers which means there submodules We apply replicate first layer fully shard second layer each consisting submodules leaving remaining submodules I have shown below how there many submodules Transformer Module tok_embeddings pos_embeddings dropout layers norm output In layers we have Transformer Blocks Transformer Block attention_norm Attention resid_dropout wq wk wv wo ffn_norm Feed_forward w gelu w resid_dropout _init_pg model_args = ModelArgs model_args n_layers = model = Transformer model_args replicate_model = deepcopy model assertEqual len _get_managed_modules replicate_model i layer enumerate replicate_model layers i == replicate layer i == fully_shard layer replicate_model = replicate replicate_model assertEqual len _get_managed_modules replicate_model skip_if_lt_x_gpu test_replicate_tp_device_mesh This tests user can pass device mesh replicate module _init_pg device = torch device f cuda rank torch cuda device_count model = Net device replicate_model = deepcopy model layers = replicate_model fc replicate_model fc replicate_model fc global_mesh = init_replicate_tp_mesh replicate_mesh = global_mesh replicate shard layer layers replicate layer device_mesh=replicate_mesh parameter layer parameters assertEqual parameter device_mesh shape assertEqual parameter placements Replicate Shard dim= skip_if_lt_x_gpu test_train_replicate_fsdp Tests replicate_model has same behavior original model when training _init_pg device = torch device f cuda rank torch cuda device_count model = Net device replicate_model = deepcopy model layers = replicate_model fc replicate_model fc replicate_model fc layer layers replicate layer replicate_model = replicate replicate_model optim = torch optim Adam model parameters lr= replicate_optim = torch optim Adam replicate_model parameters lr= torch manual_seed + rank + inp = torch randn device=device _ range loss = model inp sum loss backward param model parameters dist all_reduce param grad op=dist ReduceOp SUM replicate_loss = replicate_model inp sum replicate_loss backward optim step replicate_optim step optim zero_grad replicate_optim zero_grad assertEqual replicate_loss loss check_sharded_parity model replicate_model skip_if_lt_x_gpu test_train_parity_ d_mlp Verifies when device mesh passed model has same behavior original model when training _init_pg global_mesh = init_replicate_tp_mesh run_subtests reshard_after_forward False True use_activation_checkpointing False True mlp_dim functools partial _test_train_parity_ d_mlp global_mesh _test_train_parity_ d_mlp global_mesh DeviceMesh reshard_after_forward bool use_activation_checkpointing bool mlp_dim int replicate_shard_mesh tp_mesh = global_mesh replicate shard global_mesh tp replicate_mesh = global_mesh replicate replicate_pg = replicate_mesh get_group used ` replicate ` torch manual_seed model = MLPStack mlp_dim ref_model = copy deepcopy model cuda replicate ref_model device_mesh=replicate_shard_mesh ref_optim = torch optim Adam ref_model parameters lr= e- foreach=False model parallelize tp_mesh replicate_shard_mesh use_activation_checkpointing reshard_after_forward=reshard_after_forward optim = torch optim Adam model parameters lr= e- foreach=False torch manual_seed + replicate_pg rank + device = torch device cuda iter_idx range inp = torch randn mlp_dim device=device losses list torch Tensor = _model _optim ref_model ref_optim model optim _optim zero_grad set_to_none= iter_idx == losses append _model inp sum losses - backward _optim step assertEqual losses losses __name__ == __main__ run_tests