mypy allow-untyped-defs torch distributed dist torch _C _distributed_c d FakeProcessGroup FakeStore dist Store A fake store fake Key-Value store simply initialization usage fake process group one can either use FakeStore HashStore _create_fake_pg common_opts backend_opts A fake process group related FakeTensor process group which doesn t actually do any communication just hallucinates some communication You can run single rank fake process group without needing multiple processes simulates per-rank behavior NOTE This real process group would produce wrong results every collective It should used convenient tool when playing distributed don t care about actual data FakeProcessGroup _create_internal common_opts group_rank common_opts group_size backend_opts dist Backend register_backend fake _create_fake_pg extended_api=True devices= cpu cuda hpu xpu