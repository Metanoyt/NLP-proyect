__future__ annotations contextlib functools itertools logging operator os re sys time collections defaultdict contextlib contextmanager typing Any Callable NoReturn Optional TYPE_CHECKING Union sympy sympy Expr torch torch _logging torch fx torch device Tensor torch _decomp get_decompositions torch _dynamo utils defake dynamo_timed torch _library fake_class_registry FakeScriptObject torch _library utils get_layout_constraint_tag torch _logging LazyString trace_structured torch _prims_common compute_required_storage_length make_channels_last_strides_for torch _subclasses fake_tensor FakeTensor torch _utils_internal full_aoti_runtime_assert torch fx experimental _backward_state BackwardState torch fx experimental sym_node magic_methods method_to_operator torch fx experimental symbolic_shapes _get_placeholder_expr free_unbacked_symbols has_free_symbols resolve_unbacked_bindings RuntimeAssert ShapeEnv SympyBoolean SymTypes torch fx node Node torch fx passes reinplace _is_view_op torch utils _mode_utils no_dispatch torch utils _ordered_set OrderedSet torch utils _sympy numbers int_oo config ir metrics codegen common BackendFeature DeviceOpOverrides FileBackedGraphModule get_backend_features get_device_op_overrides get_wrapper_codegen_for_device init_backend_registration WorkspaceArg exc CppWrapperCodegenError LoweringException MissingOperatorWithDecomp MissingOperatorWithoutDecomp fx_utils count_flops_fx ir assign_origin_node Constant DonatedBuffer FixedLayout get_device_type GraphPartitionSignature InputBuffer Pointwise Reduction ShapeAsConstantBuffer StorageBox TensorBox TorchBindObject lowering constrain_to_fake_tensors constrain_to_fx_strides FALLBACK_ALLOW_LIST fallback_handler fallback_node_due_to_unsupported_type lowerings make_fallback maybe_layout_constraints needs_realized_inputs require_contiguous tag_to_layout_constraint unsupported_output_tensor runtime autotune_cache runtime autotune_cache AutotuneCacheBundler sizevars SizeVarAllocator utils convert_shape_to_inductor gather_origins get_cloned_parameter_buffer_name get_donated_idxs get_sympy_Expr_dtype GraphPartitionMap is_same_tensor maybe_get_suppress_shape_guards_ctx normalize_name should_assume_input_aligned SUPPORTED_MKLDNN_DEVICES ValueWithLineMap virtualized NullHandler V TYPE_CHECKING collections abc Iterable Iterator Sequence types ModuleType torch _higher_order_ops effects _EffectType torch fx GraphModule torch fx graph Graph codegen wrapper PythonWrapperCodegen dependencies Dep scheduler BaseSchedulerNode CompiledModule = Union ModuleType FileBackedGraphModule torch _inductor codecache output_code_log log = logging getLogger __name__ perf_hint_log = torch _logging getArtifactLogger __name__ perf_hints aten = torch ops aten _post_grad_graph_counter = itertools count config is_fbcode torch _inductor fb utils log_module_code log_module_code args Any kwargs Any - None pass may_get_constant_buffer_dtype constant_buffer sympy Expr - Optional torch dtype assert isinstance constant_buffer sympy Symbol sympy Expr sympy core numbers Integer get_constant_buffer_dtype only supports input sympy Symbol sympy Expr sympy core numbers Integer isinstance constant_buffer sympy core numbers Integer torch int isinstance constant_buffer sympy Expr get_sympy_Expr_dtype constant_buffer constant_buffer is_integer torch int constant_buffer is_float torch float None is_magic_method op Any - bool magic_ops = OrderedSet method_to_operator m m magic_methods op magic_ops getattr_recursive obj GraphModule target str - Union Tensor torch _C ScriptObject GraphModule target_atoms = target split attr_itr = obj i atom enumerate target_atoms hasattr attr_itr atom raise RuntimeError f Node referenced nonexistent target join target_atoms i attr_itr = getattr attr_itr atom attr_itr get_user_visible_output_strides g Graph - dict Node tuple int ret dict Node tuple int = output_node = g find_nodes op= output user_visible_output_idxs output_node meta ret isinstance output_node args torch fx Node output_node_args = output_node args output_node_args = output_node args idx node enumerate output_node_args idx output_node meta user_visible_output_idxs ret node = output_node meta original_output_strides idx ret extend_user_visible_output_strides user_visible_outputs dict Node tuple int - dict Node object Extend user_visible_output_strides include view ops lead user-visible outputs result dict Node object = user_visible_outputs queue = result keys visited = OrderedSet queue while queue current = queue pop _is_view_op current target current args isinstance current args torch fx Node base = current args base visited result setdefault base None visited add base queue append base result mark_nodes_dislike_padding g Graph user_visible_output_strides dict Node tuple int - None Nodes like convolution convolution_backward want its input dense If we pad their inputs we result extra calls copy kernels On other hand padding usually helps reduction The pass finds nodes dislike padding These nodes can reached convolution convolution_backward backward direction without going thru reduction config comprehensive_padding extended_user_visible_nodes = extend_user_visible_output_strides user_visible_output_strides ops_dislike_padding = OrderedSet aten convolution aten convolution_backward aten _scaled_mm what s better way collect reduction ops ops_like_padding = OrderedSet aten var_mean aten sum aten mean aten prod aten any aten amin aten amax aten min aten max aten argmin aten argmax aten scatter_reduce _get_overload_packet node torch fx Node - Optional torch _ops OpOverloadPacket node target _overloadpacket node op == call_function hasattr OpOverloadPacket slow do isinstance first isinstance node target torch _ops OpOverload hasattr node target _overloadpacket None cur reversed g nodes isinstance cur target torch _higher_order_ops triton_kernel_wrap TritonKernelWrapperMutation cur meta dislike_padding = True continue isinstance cur target torch _ops OpOverload get_layout_constraint_tag cur target == torch _C Tag needs_exact_strides cur meta dislike_padding = True continue op = _get_overload_packet cur op continue op ops_dislike_padding cur meta dislike_padding = True cur meta get dislike_padding False propagate prior cur all_input_nodes prior_op = _get_overload_packet prior prior_op continue prior_op ops_like_padding prior meta dislike_padding = True We only want mark output nodes So move after above prior nodes process config pad_outputs cur extended_user_visible_nodes cur meta dislike_padding = True GraphLowering torch fx Interpreter graph_outputs list ir IRNode __init__ gm torch fx GraphModule example_inputs Optional Sequence object = None shape_env Optional ShapeEnv = None graph_id Optional int = None cpp_wrapper bool = False aot_mode bool = False layout_opt Optional bool = None extern_node_serializer Optional Callable list ir ExternKernelNode Any = None is_inference bool = False is_backward bool = False is_const_graph bool = False const_output_index Optional dict str int = None const_wrapper_code Optional str = None const_kernel_code Optional str = None const_module Optional GraphLowering = None name Optional str = None inputs_to_check Optional Sequence int = None fx_wrapper bool = False - None super __init__ gm example_inputs = example_inputs layout_opt = layout_opt layout_opt None decide_layout_opt gm is_inference=is_inference num_channels_last_conv = is_inference = is_inference is_backward = is_backward is_const_graph = is_const_graph const_wrapper_code = const_wrapper_code const_kernel_code = const_kernel_code const_module = const_module inputs_to_check = inputs_to_check extra_traceback = False we do our own error wrapping shape_env None shape_env = ShapeEnv reuse_shape_env = False reuse_shape_env = True _shape_env = shape_env We re going mutate ras_by_symbol we finish generating them ras_by_symbol dict Optional sympy Symbol list RuntimeAssert = shape_env deferred_runtime_asserts copy bound_unbacked_symbols = OrderedSet sympy Symbol sizevars = SizeVarAllocator shape_env graph_input_names list str = graph_inputs dict str Union TensorBox TorchBindObject sympy Expr = graph_inputs_original dict str InputBuffer = partition_maps Optional list GraphPartitionMap = None zero_dim_cpu_tensor_list OrderedSet str = OrderedSet device_types OrderedSet str = const_module device_types const_module OrderedSet device_idxs OrderedSet int = const_module device_idxs const_module OrderedSet device_type = cpu additional_buffer_deps dict str OrderedSet str = defaultdict OrderedSet Inplace padding may require Inductor allocate slightly larger tensor padding buffer_to_padded_size dict str list int = buffers list ir Buffer = operations list ir Operation = const_output_index dict str int = const_output_index const_output_index folded_constants OrderedSet str = OrderedSet const_output_index keys const_output_index OrderedSet constants dict str torch Tensor = const_module constants const_module named_buffers dict str torch Tensor = const_module named_buffers const_module named_parameters dict str torch Tensor = const_module named_parameters const_module torchbind_constants dict str Union torch _C ScriptObject FakeScriptObject = seen_subgraphs dict str ir Subgraph = constant_reprs dict str str = removed_operations OrderedSet str = OrderedSet removed_buffers OrderedSet str = OrderedSet removed_inplace_buffers OrderedSet str = OrderedSet mutated_buffers OrderedSet str = OrderedSet never_reuse_buffers OrderedSet str = OrderedSet inplaced_to_remove OrderedSet str = OrderedSet device_ops DeviceOpOverrides = None type ignore assignment wrapper_code PythonWrapperCodegen = None type ignore assignment torch _inductor extern_node_serializer extern_node_json_serializer extern_node_serializer Callable list ir ExternKernelNode Any = extern_node_serializer config is_fbcode extern_node_serializer extern_node_json_serializer current_node torch fx Node = None type ignore assignment lists dict str list str = mutated_inputs OrderedSet str = OrderedSet mutated_input_idxs list int = name_to_buffer dict str ir Buffer = name_to_users defaultdict str list ir IRNode = defaultdict list name_to_op dict str ir Operation = creation_time = time time name = name type ignore assignment cpp_wrapper = cpp_wrapper fx_wrapper = fx_wrapper record multi_kernel choice cpp_wrapper so second pass knows which sub-kernel picked Copy cpp_wrapper another variable since cpp_wrapper flag OrderedSet false first pass codegen record_multi_kernel_choice = cpp_wrapper multi_kernel_to_choice dict str str = aot_mode = aot_mode graph_id = graph_id post_grad_graph_id = next _post_grad_graph_counter scheduler torch _inductor scheduler Scheduler = None type ignore assignment record intermediate results input UsedDefinedTritonKernels This will used autotuning done one pass autotuning_inputs Optional list torch Tensor = None autotuning_mapping Optional dict str dict str int = None autotuning_grids Optional dict str Any = None current_device set only during codegen device-specific kernel graph can have many devices current_device Optional torch device = None nodes_prefer_channels_last = find_nodes_prefer_channels_last layout_opt OrderedSet _warned_fallback = OrderedSet aten convolution_backward user_visible_output_strides = get_user_visible_output_strides gm graph mark_nodes_dislike_padding gm graph user_visible_output_strides cache_key str = This cache key compiled artifact cache_path str = This path filesystem where compiled artifact stored cache_linemap list tuple int str = This linemap used profiler mark custom compiled kernels getting run Used lowering encounters cases where cudagraphs supported disable_cudagraphs_reason Optional str = None only keeping one node per device stack trace purposes device_node_mapping dict torch device torch fx Node = orig_gm torch fx GraphModule = gm __copy__ k v orig_gm named_buffers named_buffers k = v k v orig_gm named_parameters named_parameters k = v dynamo_flat_name_to_original_fqn = module meta get type ignore operator union-attr dynamo_flat_name_to_original_fqn allocated_constant_name dict str str = const_module allocated_constant_name const_module None init_backend_registration get_backend_features = functools lru_cache None get_backend_features effectful_ops dict _EffectType ir Buffer = Track buffers we know unaligned This can either graph input output fallback kernels unaligned_buffers OrderedSet str = OrderedSet no_fuse_buffer_names OrderedSet str = OrderedSet low_precision_codegen_ops OrderedSet str = OrderedSet more aggressive prologue fusion invoke_quant_ops OrderedSet str = OrderedSet Below field related printing debug intermediate tensor values info debugging all_codegen_kernel_names OrderedSet str = OrderedSet state used KernelArgs workspace workspace_id = itertools count track current placeholder index we processing placeholder_idx = - bw_donated_idxs = get_donated_idxs Cache dep size hints avoid expensive recomputation dep_size_hint_cache dict Dep int = freeze_runtime_asserts - None _shape_env freeze_runtime_asserts symbolic_sizes_strides ex torch Tensor - tuple Sequence Union int Expr Sequence Union int Expr Support dynamic shapes dynamic strides assigning variables each dimension We duck-shape tensors so two tensors have same size they get assigned same symbolic variable reuse_shape_env convert_shape_to_inductor ex size convert_shape_to_inductor ex stride torch _dynamo source ConstantSource TODO should needed once lands https github com pytorch pytorch pull #discussion_r TODO make dedicated UnknownSource NB This using legacy default behavior create_symbolic_sizes_strides_storage_offset we hope we can just delete entirely source = ConstantSource f __inductor_unknown_tensor_ len _shape_env var_to_val size stride _ = _shape_env create_symbolic_sizes_strides_storage_offset ex source r_size = i node expr isinstance i torch SymInt i i size r_stride = i node expr isinstance i torch SymInt i i stride r_size r_stride static_sizes_strides ex torch Tensor - tuple list sympy Expr list sympy Expr Primarily used weights size = sympy Integer i i ex size stride = sympy Integer i i ex stride size stride get_allocation_size node Union ir TensorBox ir StorageBox ir Buffer WorkspaceArg ir TorchBindObject - Sequence Expr isinstance node ir TensorBox node = node data type ignore assignment isinstance node ir StorageBox node = node data type ignore assignment isinstance node ir ComputedBuffer node name buffer_to_padded_size pyrefly ignore index-error buffer_to_padded_size node name node get_size get_allocation_storage_size node Union ir Buffer WorkspaceArg ir TorchBindObject - Expr layout = node get_layout size = get_allocation_size node consider inplace padding stride = layout stride offset = layout offset compute_required_storage_length size stride offset type ignore arg-type has_feature device Union torch _inductor ir IRNode device None feature BackendFeature - bool assert isinstance feature BackendFeature feature feature get_backend_features get_device_type device get_dep_size_hint dep Dep - int Get size hint dependency caching avoid expensive recomputation dep dep_size_hint_cache res = try dep has_unbacked_symbols res = dep numbytes_hint except KeyError In least one test test inductor test_torchbind py we create StarDep doesn t exist graph calling ` has_unbacked_symbols ` throws error pass dep_size_hint_cache dep = res dep_size_hint_cache dep get_current_device_or_throw - torch device device = current_device device raise RuntimeError No current device contextlib contextmanager set_current_device device torch device - Iterator None prior = current_device current_device = device try yield finally current_device = prior get_training_phase - str is_inference inference is_backward backward forward staticmethod decide_layout_opt gm GraphModule is_inference bool - bool Decide we should enable layout optimization graph based heuristics config layout_optimization False config force_layout_optimization True conv_nodes = n n gm graph nodes n target torch ops aten convolution default nconv = len conv_nodes nconv == False For cpu backend mkldnn enabled we always use channels_last better performance torch backends mkldnn enabled torch backends mkldnn is_available all n args idx meta val device type SUPPORTED_MKLDNN_DEVICES n conv_nodes idx True Following models skipped due jx_nest_base volo_d _ len list gm graph nodes = nconv log debug Skipped layout opt because only few conv False any has_free_symbols n args idx meta val n conv_nodes idx log debug See perf regression dynamic shape Follow up https github com pytorch pytorch issues False is_grouped n Any - bool meta_val = n args meta val type ignore union-attr operator assert isinstance meta_val torch Tensor n args - meta_val size type ignore union-attr operator is_in_out_channel n torch fx Node - bool n args meta val size = n args meta val size type ignore union-attr operator n args meta val size type ignore union-attr operator is_small_channel n torch fx Node - bool n args meta val size = type ignore union-attr operator n args meta val size = type ignore union-attr operator only grouped convolutions benchmarked slower conv samples inference only is_inference flop_counts dict str float = defaultdict float node conv_nodes counted_flops = count_flops_fx node counted_flops None continue is_grouped node node_type = grouped is_small_channel node node_type = small is_in_out_channel node node_type = in_out node_type = default flop_counts node_type += counted_flops log debug Conv inputs meta found average benchmarked channels last speedup slowdown speedup taken set convolution inputs benchmarks dynamo microbenchmarks operator_inp_logs torchbench_train To regenerate these numbers follow https gist github com eellison d ed f d ac f f df bb GROUPED_MULTIPLIER = DEFAULT_MULTIPLIER = IN_OUT_MULTIPLIER = SMALL_MULTIPLIER = total_flops = sum flop_counts values TODO - get different values per hardware weighted_flops = flop_counts grouped GROUPED_MULTIPLIER + flop_counts small SMALL_MULTIPLIER + flop_counts in_out IN_OUT_MULTIPLIER + flop_counts default DEFAULT_MULTIPLIER do_layout_opt = weighted_flops = total_flops do_layout_opt log debug Skipped layout opt inference because weighted flops indicate slowdown default d channels last d total_flops weighted_flops do_layout_opt Channels last layout can dramatically hurt grouped conv perf E g Conv arguments like input_shape weight_shape stride padding groups slows down x using channels last But lot timm models use depthwise separable convolution which will result grouped convolution in-channel size == For those grouped convolution channels last still helps lot E g Conv arguments input_shape weight_shape stride padding groups get x speedup channels last layout The following heuristics skip using channels-last model contains grouped convolution in-channels any map is_grouped conv_nodes log debug Skip layout opt because found grouped convolution in_channels False For some models contain convolution larger in-channel than out-channel applying channels last hurts performance Following models skipped due - pytorch_unet - phlippe_densenet slightly worse - Background_Matting x - x - pytorch_CycleGAN_and_pix pix x - x any map is_in_out_channel conv_nodes log debug Skip layout opt because some convolutions have smaller out_channel False Following models skipped due - functorch_maml_omniglot all map is_small_channel conv_nodes log debug Skip layout opt because all convolution channels too small False True qualify_name name str - str Prepend given name graph name any name None f name _ name name make_subgraph gm torch fx GraphModule example_inputs list torch Tensor subgraph_name str - SubgraphLowering Make subgraph current graph all inherited parts except graph module ` gm ` ` example_inputs ` The subgraphs lowered separately lifted into separate function parent output wrapper code The subgraph name qualified parent graph s name Note lifting subgraph supported python wrapper only For cpp wrapper we inline subgraphs parent wrapper SubgraphLowering parent=self gm=gm example_inputs=example_inputs shape_env=self _shape_env cpp_wrapper=self cpp_wrapper aot_mode=self aot_mode extern_node_serializer=self extern_node_serializer is_inference=self is_inference is_backward=self is_backward name=self qualify_name subgraph_name find_nodes_prefer_channels_last - OrderedSet Node The rule decide node prefer channels last simple s input output convolution one its user prefers channels last We have rule because cudnn runs faster convolution kernel channels last inputs Rule also important It makes sure indirect inputs convolution also prefers channels last Consider scenario conv - batch-norm - relu - conv Without rule batch-norm output may use contiguous layout That will cause extra copies output batch-norm should channels last initially since its input conv s output Forcing batch-norm s output contiguous results first copy The second conv s input initially contiguous This layout propagated batch-norm s output We need convert channels last layout which results second copy With rule we makes sure all tensors chain uses channels last layout So both copies can saved last_conv = None nodes_cannot_propagate = torch ops aten bmm default output_set = OrderedSet Node n reversed module graph nodes type ignore arg-type union-attr n target torch ops aten convolution default output_set add n last_conv None last_conv = n continue n target nodes_cannot_propagate continue user n users user output_set output_set add n break need second pass add downstream nodes those channel last nodes sets This pass especially needed avoid mix-layout kernel inputs backward pass Let s say conv-batchnorm s output passed relu whose output turn returned fwd graph Without second pass we will force relu s output contiguous Then kernel backward pass contiguous output relu may mix other channels last tensors passed kernel This pass improve yolov training speedup x worse than disabling layout optimization speedup x x It also improves dla training speedup x worse than disabling layout optimization speedup x x This also helps following models - res net _ w_ s - res net _ w_ s - sebotnet ts_ n module graph nodes type ignore union-attr layout propagation ends last conv node which will benefit vison transformers last_conv None n == last_conv break n output_set user n users user target nodes_cannot_propagate continue output_set add user output_set warn_fallback name str - None name _warned_fallback _warned_fallback add name perf_hint_log info Using FallbackKernel s name add_device_info device torch device - None device_types add device type device index None device_idxs add device index V graph current_node device device_node_mapping device_node_mapping device = V graph current_node property fake_mode - torch _subclasses fake_tensor FakeTensorMode V fake_mode try_get_buffer buffer_name str - Optional Union ir TensorBox ir Buffer ir TorchBindObject buffer_name name_to_buffer name_to_buffer buffer_name buffer_name graph_inputs graph_inputs buffer_name buffer_name constants data = V graph constants buffer_name ir ConstantBuffer name=buffer_name layout=ir FixedLayout data device data dtype V graph static_sizes_strides data None add_symbol_graph_input symbol sympy Expr - None raise RuntimeError Should called main graph get_buffer buffer_name str - Union ir TensorBox ir Buffer ir TorchBindObject buf = try_get_buffer buffer_name buf None buf raise RuntimeError f Failed find buffer matching name buffer_name get_dtype buffer_name str - torch dtype buffer_name constants constants buffer_name dtype For mutation op we should dtype buffer being mutated hasattr scheduler mutation_real_name buffer_name scheduler mutation_real_name mutated_buf = scheduler mutation_real_name buffer_name mutated_buf name_to_buffer name_to_buffer mutated_buf get_dtype mutated_buf graph_inputs graph_inputs mutated_buf get_dtype buffer_name name_to_buffer name_to_buffer buffer_name get_dtype buffer_name graph_inputs graph_inputs buffer_name get_dtype m = re match r as_strided &#124; reinterpret_tensor \ a-zA-Z - _ + buffer_name m get_dtype m group raise KeyError f could find buffer_name get_numel buffer_name str - Union int Expr buffer_name constants constants buffer_name numel buffer_name name_to_buffer buf = name_to_buffer buffer_name buf has_tensor_output buf get_numel buffer_name graph_inputs graph_inputs buffer_name get_numel raise KeyError f could find buffer_name run args Any - Any type ignore override dynamo_timed GraphLowering run super run args register_operation op ir Operation - str assert op operation_name None f Operation registered twice op assert isinstance op ir Operation name = qualify_name f op len operations operations append op name_to_op name = op op operation_name = name name register_buffer buffer ir Buffer set_name bool = False - str name = qualify_name f buf len buffers buffers append buffer name_to_buffer name = buffer device = buffer get_device Skip empty CPU tensor so CUDA graphs can succeed see https github com pytorch pytorch pull device None isinstance buffer ir ComputedBuffer buffer is_zero_elements device == torch device cpu add_device_info device set_name buffer name = name name register_operation_list operation_names list str - str name = qualify_name list_ + _ join operation_names lists name = operation_names name register_users_of node_output Union Iterable ir IRNode ir IRNode - None register value Union Iterable ir IRNode ir IRNode - None isinstance value list tuple x value register x isinstance value ir TensorBox read_name value get_read_names name_to_users read_name append value register node_output mark_buffer_mutated name str - None When buffer mutated we need make sure all reads old version realized before mutation happens assert isinstance name str mutated_buffers add name name name_to_users user name_to_users name user realize get_original_value_of_constant name str - torch Tensor In AOTI module buffers may have been mutated during tracing compilation Thus we need read previously stored original buffers make sure generated model so uses correct initial values assert name allocated_constant_name name constants Can find original value + name orig_name = get_cloned_parameter_buffer_name allocated_constant_name name module meta orig_name type ignore index orig_name module meta type ignore operator constants name allocate_non_dup_const_name name Optional str data Union Tensor - str config aot_inductor use_runtime_constant_folding constant_name value constants items is_same_tensor data value constant_name name None name = f constant len constants orig_name = name name isdigit name = f constant_ name name = qualify_name name We may generate var name each constant codegen Let s only keep sane characters prefix = normalize_name name name = prefix cnt = while name constants name = f prefix _ cnt cnt += constants name = data constant_reprs name = f data device r data dtype r f tuple data size r tuple data stride r f hash data x allocated_constant_name name = orig_name type ignore assignment name add_tensor_constant data Tensor name Optional str = None - Union TensorBox ir ShapeAsConstantBuffer new_name = allocate_non_dup_const_name name data TensorBox create ir ConstantBuffer name=new_name layout=FixedLayout data device data dtype static_sizes_strides data constant_name name str device_override Optional torch device - str We AOT copy constants devices they needed If device_override doesn t match constant s device then copy different name constants name device == device_override device_override None name torch utils _python_dispatch _disable_current_modes caller might have OrderedSet fake tensor mode which will create fake tensor when calling so unset modes here allocate_non_dup_const_name f name _ device_override type device_override index constants name device_override pyrefly ignore bad-override placeholder target str type ignore override args tuple object type ignore override kwargs dict str object - Union Expr TensorBox None placeholder_idx += example = super placeholder target args kwargs type ignore arg-type target = qualify_name target isinstance example SymTypes TODO fix partitioning issue re-enable backward https github com pytorch pytorch issues V graph is_backward expr = _get_placeholder_expr example node expr = example node expr graph_inputs target = expr graph_input_names append target expr isinstance example int bool float expr = sympy sympify example graph_inputs target = expr graph_input_names append target expr isinstance example FakeScriptObject obj = TorchBindObject name=target value=example graph_inputs target = obj graph_input_names append target obj example None graph_input_names append target None isinstance example BackwardState Ignored arg must unused Alternately we could filter out AotAutograd graph_input_names append target None See note Note Generator arguments AOTDispatcher isinstance example torch Generator assert len V graph current_node users == next iter V graph current_node users target torch _prims rng_prims graphsafe_run_with_rng_state torch ops higher_order invoke_subgraph gen = ir GeneratorState name=target device=example device graph_inputs target = gen type ignore assignment graph_input_names append target gen assert isinstance example torch Tensor example todo chilli We can remove last check once we turn buffers into static shape tensors That s hack workaround Inductor believing buffer should static us passing fake tensor symbolic shapes example _has_symbolic_sizes_strides first N inputs weights sizes strides = static_sizes_strides example sizes strides = symbolic_sizes_strides example type ignore assignment is_backward bw_donated_idxs placeholder_idx bw_donated_idxs tensor = TensorBox create DonatedBuffer name=target layout=FixedLayout example device example dtype sizes strides TODO jansel handle input aliasing tensor = TensorBox create InputBuffer name=target layout=FixedLayout example device example dtype sizes strides graph_inputs target = tensor graph_input_names append target graph_inputs_original target = tensor data data type ignore union-attr current_node users cudagraphs should work unused CPU input add_device_info example device Note Input Alignment handling Inductor Alignment matters generating efficient code Some operations e g vectorized loads can only performed aligned inputs But we codegen assuming aligned inputs then get unaligned inputs runtime then we forced clone - which bad both perf memory usage One option would guard storage_offset ALIGNMENT then codegen based But storage_offset guards turned out expensive cause recompiles Instead we re generating code based alignment example input without guarding maybe_get_suppress_shape_guards_ctx should_assume_input_aligned example unaligned_buffers add target tensor call_function target Callable args Any kwargs dict str Any - Any type ignore type-arg override target operator getitem isinstance args list tuple dict super call_function target args kwargs hasattr OpOverloadPacket slow check isinstance first isinstance target torch _ops OpOverloadPacket hasattr target _inductor_lowering_function passthrough lowerings pattern_matcher target args kwargs target lowerings assert isinstance target torch _ops OpOverload f target OpOverload base_name = target name split base_name FALLBACK_ALLOW_LIST make_fallback target warn=False override_decomp=True config implicit_fallbacks error = MissingOperatorWithDecomp get_decompositions target MissingOperatorWithoutDecomp log info Creating implicit fallback \n s error operator_str target args kwargs tag Optional torch _C Tag = get_layout_constraint_tag target with_default=False tag None torch _library utils is_builtin target is_backward implicit fallback ATen ops during backward there no layout constraint tag we conservatively require contiguous input since some eager kernels do support non-contiguous inputs Otherwise they may silently cause accuracy problems Check https github com pytorch pytorch issues We only do For ATen ops backward TODO should really switch needs_fixed_stride constraint these identify them one one decided_constraint = require_contiguous type ignore assignment default_tag torch _C Tag = get_layout_constraint_tag target with_default=True decided_constraint = tag_to_layout_constraint default_tag make_fallback target layout_constraint=decided_constraint get_decompositions target There isn t good way dynamically patch since AOT Autograd already ran The error message tells user how fix raise MissingOperatorWithDecomp target args kwargs raise MissingOperatorWithoutDecomp target args kwargs try log debug via s lowerings target type ignore index n = current_node layout_constraints = maybe_layout_constraints target layout_constraints old_args old_kwargs = args kwargs layout_constraints constrain_to_fake_tensors only constrain_to_fake_tensor exists otherwise no constraints all implication operator inserted custom pass so we ll give them freedom eager_input_vals n meta fake_args fake_kwargs = n meta eager_input_vals fake_args fake_kwargs might align args kwargs we need normalize them based schema assert isinstance target torch _ops OpOverload normalize args Any kwargs Any - tuple Any Any result = torch fx operator_schemas normalize_function target args kwargs assert result None result result fake_args fake_kwargs = normalize fake_args fake_kwargs args kwargs = normalize args kwargs old_args old_kwargs = normalize old_args old_kwargs args kwargs = constrain_to_fake_tensors args kwargs fake_args fake_kwargs args kwargs = layout_constraints n args kwargs should_fallback n meta out = fallback_handler target add_to_fallback_set=False args kwargs out = lowerings target args kwargs type ignore index layout_constraints layout_constraints allowed make new copies inputs they do target mutable then we need write new values back into original inputs propagate_mutation n old_args old_kwargs args kwargs type ignore possibly-undefined out except Exception e raise LoweringException e target args kwargs with_traceback e __traceback__ None staticmethod can_inline_constant t torch Tensor - bool True small constant attr will inlined len t shape == t shape = pyrefly ignore bad-override get_attr target str type ignore override args tuple type ignore override kwargs dict str object - Union Constant TensorBox ShapeAsConstantBuffer ir Subgraph TorchBindObject constant value = getattr_recursive module target type ignore arg-type isinstance value torch fx GraphModule Reuse existing subgraph we have seen before already target seen_subgraphs seen_subgraphs target out = ir Subgraph name=target graph_module=value seen_subgraphs target = out out isinstance value torch _C ScriptObject torchbind_constants target = value constant_reprs target = TorchBindObject name=target value=value isinstance value FakeScriptObject torchbind_constants target = value constant_reprs target = TorchBindObject name=target value=value assert isinstance value torch Tensor config aot_inductor use_runtime_constant_folding config always_keep_tensor_constants unsupported_output_tensor value add_tensor_constant value target no_dispatch value shape == Constant value=value item dtype=value dtype device=value device can_inline_constant value log debug Inlining constant s str target tensor lowering has constant inlining logic lowering tensor tensor value tolist dtype=value dtype device=value device add_tensor_constant value target call_module target Any args Any kwargs Any - NoReturn raise AssertionError call_method target Any args Any kwargs Any - NoReturn raise AssertionError pyrefly ignore bad-override output target str type ignore override args tuple object type ignore override kwargs dict str object - None result = super output target args kwargs type ignore arg-type isinstance result tuple list nested subgraphs can have singleton outputs result = result assert isinstance result tuple list type result assert all isinstance x TensorBox ir Constant type None ir ConstantBuffer sympy Expr sympy logic boolalg Boolean int ir EffectfulKernel ir ShapeAsConstantBuffer x result result fx_node_args = V graph current_node args type ignore arg-type isinstance fx_node_args tuple list nested subgraphs can have singleton outputs fx_node_args = fx_node_args result = ir ExternKernel realize_input x x result result_correct_strides = assert len fx_node_args == len result r fx_node zip result fx_node_args isinstance r ir TensorBox ir BaseView result_correct_strides append r isinstance r get_output_spec ir CommBufferLayout Active references persistent comm buffers allowed outside graphs result_correct_strides append ir ExternKernel copy_input r AOT Autograd tries detect stride divergence inductor output metadata Here we try avoid spurious divergence matching insignificant strides such should have already been realized assert torch _inductor ir is_storage_and_layout r meta_strides = s node expr isinstance s torch SymInt s s fx_node meta val stride result_correct_strides append ir try_match_insignificant_strides r meta_strides graph_outputs = result_correct_strides value ir IRNode name value graph_inputs items isinstance value TorchBindObject continue assert isinstance value TensorBox sympy Expr torch _inductor ir GeneratorState f Unsupported inductor graph input type type value isinstance value TensorBox continue value realize assert isinstance value TensorBox value = value data assert isinstance value ir StorageBox value_storage_box = value value = value data isinstance value InputBuffer value get_name = name one our inputs mutated need turn into copy ir MutationLayoutSHOULDREMOVE realize_into value graph_inputs_original name replace output mutated input try ind = graph_outputs index value_storage_box graph_outputs ind = graph_inputs_original name except ValueError pass finalize log debug Force channels last inputs d conv current graph id d num_channels_last_conv graph_id graph_id None - finalize - None buf buffers buf decide_layout contextmanager set_current_node node torch fx Node type ignore no-untyped-def old = current_node try current_node = node yield finally current_node = old contextmanager set_current_wrapper_code - Iterator None old = wrapper_code try yield finally wrapper_code = old propagate_mutation fx_node torch fx Node old_args tuple Any old_kwargs dict str Any new_args tuple Any new_kwargs dict str Any - None Propagate mutations new_args new_kwargs back old_args old_kwargs Assumes we may have cloned old_args old_kwargs into new_args new_kwargs then called fx_node new_args new_kwargs If fx_node mutates any new_args new_kwargs they different old_args old_kwargs then we need update original tensor assert len old_args == len new_args assert len old_kwargs == len new_kwargs fx_node target torch ops higher_order triton_kernel_wrapper_mutation kwargs = fx_node kwargs kwargs assert isinstance kwargs dict mutated = torch _higher_order_ops triton_kernel_wrap get_mutated_tensors old_kwargs kernel_idx old_kwargs constant_args_idx k v meta val isinstance v torch fx Node v k v kwargs items old_kwargs tma_descriptor_metadata name mutated old_arg = old_kwargs kwargs name new_arg = new_kwargs kwargs name old_arg new_arg continue call_function torch ops aten copy_ default old_arg new_arg assert isinstance fx_node target torch _ops OpOverload maybe_propagate schema_arg torch _C Argument old_arg ir IRNode new_arg ir IRNode - None old_arg new_arg schema_arg alias_info None schema_arg alias_info is_write The lowering copy_ smart enough replace old_arg new_arg all future uses so copy_ kernel never gets emitted old_arg new_arg may immutable_list isinstance old_arg ir IRNode old_arg = old_arg type ignore assignment new_arg = new_arg type ignore assignment old_arg_item new_arg_item zip old_arg new_arg type ignore call-overload old_arg_item new_arg_item continue call_function torch ops aten copy_ default old_arg_item new_arg_item schema = fx_node target _schema idx old_arg new_arg enumerate zip old_args new_args schema_arg = schema arguments idx maybe_propagate schema_arg old_arg new_arg schema_kwargs = arg name arg arg schema arguments key old_kwargs keys old_arg = old_kwargs key new_arg = new_kwargs key schema_arg = schema_kwargs key maybe_propagate schema_arg old_arg new_arg run_node n torch fx Node - object debug msg str - None log debug lowering s s LazyString n format_node msg type ignore arg-type torch _inductor compiler_bisector CompilerBisector buffer_watermark = len buffers operation_watermark = len operations origins OrderedSet Union Node ir IRNode = OrderedSet n origins OrderedSet Any = OrderedSet n is_call_function = n op == call_function is_call_function args kwargs = fetch_args_kwargs_from_env n origins &#124; = gather_origins args kwargs ir IRNode current_origins origins set_current_node n V set_current_node n n op == call_function path only built-in operators n target isinstance n target torch _ops OpOverload torch _library utils is_builtin n target fallback_node_due_to_unsupported_type n CompilerBisector disable_subsystem inductor lowerings lambda repr n debug fallback_handler result = fallback_handler n target add_to_fallback_set=False args type ignore possibly-undefined kwargs type ignore possibly-undefined n op == call_function n target torch ops higher_order triton_kernel_wrapper_mutation config triton_kernel_default_layout_constraint = flexible_layout debug user_defined_triton_kernel_layout_constraints config triton_kernel_default_layout_constraint == needs_fixed_stride_order old_args = args type ignore possibly-undefined old_kwargs = kwargs type ignore possibly-undefined eager_input_vals = n meta get eager_input_vals inp_args = eager_input_vals inp_kwargs = eager_input_vals args kwargs = constrain_to_fake_tensors pyrefly ignore unbound-name args pyrefly ignore unbound-name kwargs inp_args inp_kwargs args kwargs = constrain_to_fx_strides n args kwargs type ignore index result = call_function n target args kwargs type ignore arg-type propagate_mutation n old_args old_kwargs args kwargs type ignore possibly-undefined raise RuntimeError f Unknown triton_kernel_default_layout_constraint config triton_kernel_default_layout_constraint is_magic_method n target TODO sus probably should handled lowerings themselves similarly sym_size sym-stride https github com pytorch pytorch issues debug is_magic_method isinstance n meta val torch SymInt torch SymFloat torch SymBool result = n meta val node expr result = super run_node n debug result = super run_node n require same stride order dense outputs user-land view will throw because inductor output different strides than eager long term solution make view always succeed infallible strides as_strided ops we need make sure its input has same size stride eager model align eager behavior as_strided_ops = torch ops aten as_strided default torch ops aten as_strided_ default torch ops aten as_strided_scatter default torch ops aten resize default torch ops aten resize_as default is_output = any user op == output user n users is_user_visible = n user_visible_output_strides is_input_for_as_strided = any user target as_strided_ops user n users n meta get inductor_realize_to_strides False isinstance result TensorBox result realize strides = n meta val stride sym_strides = torch _inductor utils any_is_symbolic strides result maybe_get_stride = strides sym_strides stride_order = ir get_stride_order strides result = ir ExternKernel require_stride_order result stride_order is_output isinstance result TensorBox isinstance result data ir BaseView Realize so outputs correctly aliased result realize is_output is_input_for_as_strided isinstance n meta val torch Tensor is_user_visible strides = user_visible_output_strides get n strides = n meta val stride strides None len strides allow_padding = config pad_outputs is_user_visible is_input_for_as_strided dense = torch _prims_common is_non_overlapping_and_dense n meta val unbacked_symbols_in_strides = len free_unbacked_symbols strides unbacked_symbols_in_strides dense len result get_size == n nodes_prefer_channels_last is_user_visible is_input_for_as_strided strides = ir FlexibleLayout stride_ordered_for_memory_format result get_size torch channels_last unbacked_symbols_in_strides len strides To avoid converting possible view ops copy kernel we use previous require_exact_strides handle views But ultimately s better require right strides tensor definition n meta val _is_view isinstance pyrefly ignore missing-attribute result data ir BaseView result = ir ExternKernel require_stride_order result ir get_stride_order strides allow_padding=allow_padding Fix -d tensors result size empty strides should also empty len result get_size == len strides strides = strides = s node expr isinstance s torch SymInt s s strides result = ir ExternKernel require_exact_strides result strides allow_padding=allow_padding Realize any user need inputs realized there already too many reads rematerializing can bad num_users = len OrderedSet n users num_users isinstance result TensorBox user n users user target needs_realized_inputs result realize_hint This inclusion somewhat controversial discussion between Horace Natalia Elias Currently s very clear why helpful The general idea here even though node may have FlexibleLayout we still often treat contiguous This appears sometimes result suboptimal behavior When we do better job selecting layout we should revisit need_fixed_layout = torch ops aten convolution_backward default torch ops aten mm default torch ops aten _int_mm default need_fixed_channels_last_layout = layout_opt need_fixed_layout append torch ops aten convolution default torch _C _has_mkldnn need_fixed_layout += torch ops mkldnn _linear_pointwise default torch ops mkldnn _linear_pointwise binary torch ops aten mkldnn_rnn_layer default torch ops onednn qlinear_pointwise default torch ops onednn qlinear_pointwise tensor torch ops onednn qlinear_pointwise binary torch ops onednn qlinear_pointwise binary_tensor need_fixed_channels_last_layout += torch ops mkldnn _convolution_pointwise default torch ops mkldnn _convolution_pointwise binary torch ops mkldnn _convolution_pointwise_ binary torch ops mkldnn _convolution_transpose_pointwise default torch ops onednn qconv_pointwise default torch ops onednn qconv d_pointwise binary torch _C has_mkl need_fixed_layout += torch ops mkl _mkl_linear default user target need_fixed_layout result = ir ExternKernel require_stride_order result ir get_stride_order n meta val stride allow_padding=True user target need_fixed_channels_last_layout n user args result = ir ExternKernel require_stride_order result ir get_stride_order make_channels_last_strides_for n meta val shape user op == output pyrefly ignore missing-attribute isinstance result data data Pointwise Reduction result realize TODO jansel introduce store vs inline choice result mark_reuse len n users Realize IRNode already has accumulated lots reads isinstance result TensorBox result has_exceeded_max_reads Prevent excessive accumulation computed buffer when there multiple branches each small number memory reads they converge user result realize_hint Realize Pointwise has too much stuff inlined As may cause RecursionError during Inductor s evaluation isinstance result TensorBox isinstance result data StorageBox curr = result data data isinstance curr Pointwise Use inner fn rough proxy Good enough curr has_large_inner_fn threshold= result realize assign_origin_node result n register_users_of result new_unbacked_defs = OrderedSet sympy Symbol buf buffers buffer_watermark new_unbacked_defs &#124; = buf get_unbacked_symbol_defs op operations operation_watermark new_unbacked_defs &#124; = op get_unbacked_symbol_defs shape_env = V graph sizevars shape_env An input can unbacked symint i e when mark_unbacked used case add new_unbacked_defs n op == placeholder isinstance result sympy Symbol shape_env is_unbacked_symint result new_unbacked_defs add result format_new_defs - str r = f unbacked_symbol_defs= buf get_unbacked_symbol_defs \n buf \n buf buffers buffer_watermark r extend f unbacked_symbol_defs= op get_unbacked_symbol_defs \n op \n op operations operation_watermark \n join r We do skip unbacked symints input backward see note below V graph is_backward n op == placeholder result Note Backwards runtime asserts Backwards poses interesting problem deferred runtime asserts In easy case we may solely close over data dependent sized tensors there no binding sites unbacked SymInts In case we can just drop all runtime asserts floor no non-placeholder bindings no problem However possible fresh runtime assert show up between forwards backwards Right now freezing process happens when we lower forwards means we will freeze runtime asserts then moment backwards lowering process attempts add new deferred runtime assert we will fail Let s say you remove assert Now when we get here we need make sure we actually emit these asserts because we can t emit them forwards we already compiled So we have do something here But we don t want reemit ALL deferred runtime asserts we only want emit NEW ones Therefore needing some sort stratification ShapeEnv This all doable just hasn t been done yet unbacked_bindings = resolve_unbacked_bindings V graph sizevars shape_env n meta get unbacked_bindings assert unbacked_bindings None When we do lowering possible we reallocate unbacked SymInts So we need line up unbacked SymInts when performing test here In principle we could permit lowering introduce MORE unbacked SymInts long all old unbacked ones accounted s fine inductor introduce extra calls item unbacked whatever This actually happens practice when unbacked SymInt gets memoized away naively when Inductor reprocesses kernel doesn t know memo still applies ends up allocating new symbol However generally bad thing we may still end up needing test equalities symbols fresh symbol likely hit lots GuardOnDataDependent errors we already know facts renamed_unbacked_bindings = OrderedSet V fake_mode shape_env unbacked_renamings get s s s unbacked_bindings keys assert new_unbacked_defs = renamed_unbacked_bindings f failed new_unbacked_defs = renamed_unbacked_bindings inductor = fx \n f fx node n format_node \n f new operations \n\n format_new_defs create_deferred_runtime_asserts n new_unbacked_defs result create_deferred_runtime_asserts n torch fx Node new_unbacked_defs OrderedSet sympy Symbol - None NOTE Codegen runtime asserts Inductor We need generate runtime asserts directly Inductor instead just reusing asserts input graphs because we reuse same ShapeEnv before In particular subsequent graph passes we would immediately turn all these assertions into noops because when we evaluated their expressions we would see because we had deferred runtime assert ShapeEnv we know oh course expression True already One example below Model torch nn Module forward b c nz = torch nonzero ones = new_ones nz size b size torch _check ones size = equals = torch add ones c equals torch _dynamo mark_dynamic c When we reuse ShapeEnv Inductor lowering check checks nonzero have same shape would evaluated True after we resolve unbacked bindings using ShapeEnv See test_unbacked_equals_input_size_runtime_assertion test_aot_inductor In addition Inductor generated runtime asserts we also need runtime asserts input graph because some derived runtime asserts backed symints generated Inductor One example ` y = x reshape - clone ` x shape needs multiple See test_aoti_runtime_asserts_backed_symint test_aot_inductor make_assert expr SympyBoolean msg str - None assert_op = ir AssertScalar expr msg register_buffer assert_op set_name=True register_operation assert_op full_aoti_runtime_assert n target torch ops aten _assert_scalar default aot_mode node_args _ = fetch_args_kwargs_from_env n node_args = True noqa E make_assert node_args f node_args True bound_unbacked_symbols tracks symbols created so far we use make sure runtime assertions added after all symbols used them defined bound_unbacked_symbols &#124; = new_unbacked_defs shape_env = V graph sizevars shape_env Emit code runtime asserts can inserted point i new_unbacked_defs ras = ras_by_symbol pop i NB size-like needed we won t retrace vr = shape_env var_to_range i shape_env _default_unspecified_value_range issubset vr is_convertible s Expr - bool s int_oo -int_oo False try int s True except TypeError False is_convertible vr lower make_assert i = vr lower f i = vr lower is_convertible vr upper make_assert i = vr upper f i = vr upper ra ras fvs = free_unbacked_symbols ra expr missing = fvs - bound_unbacked_symbols missing i = min missing key=str ras_by_symbol setdefault i append ra make_assert ra expr f ra expr validate_can_generate_cpp_wrapper - None config disable_cpp_codegen raise CppWrapperCodegenError C++ codegen disabled sys platform linux darwin win raise CppWrapperCodegenError f Unsupported platform sys platform init_wrapper_code is_subgraph bool = False subgraph_name Optional str = None parent_wrapper_code Optional PythonWrapperCodegen = None partition_signatures Optional GraphPartitionSignature = None - None device_types = device_types copy device_types discard cpu device_types discard meta TODO Eikan Only support mixing cpu other device now assert len device_types = Does support mixing format + join device_types only_cpu = len device_types == device_type = cpu only_cpu device_types pop cpp_wrapper validate_can_generate_cpp_wrapper device_ops = get_device_op_overrides device_type wrapper_code_gen_cls = get_wrapper_codegen_for_device device_type cpp_wrapper fx_wrapper assert wrapper_code_gen_cls None f Device device_type supported wrapper_code = wrapper_code_gen_cls create is_subgraph subgraph_name parent_wrapper_code partition_signatures const_module wrapper_code _names_iter = const_module wrapper_code _names_iter extract_autotune_inputs example_inputs list Union int float torch Tensor - None copy cloned_gm = copy deepcopy orig_gm example_inputs = copy deepcopy example_inputs triton_nodes = node cloned_gm graph nodes node op == call_function node target torch ops higher_order triton_kernel_wrapper_mutation triton_nodes append node Store grid related nodes grid_inputs list torch fx Node = visited_grids dict torch fx Node int = Store kwargs related nodes triton_inputs dict str Any = kwargs_inputs list torch fx Node = visited_kwargs dict Any int = node triton_nodes first check whether we have fx node grid settings grid node kwargs grid val grid val visited_grids continue isinstance val torch fx Node visited_grids val = len grid_inputs grid_inputs append val kwargs = node kwargs kwargs identify which args might mutated those should cloned mutated = torch _higher_order_ops triton_kernel_wrap get_mutated_tensors node kwargs kernel_idx node kwargs constant_args_idx k v meta val isinstance v torch fx Node v k v kwargs items node kwargs tma_descriptor_metadata new_kwargs dict str int = cloned_gm graph inserting_before node k v kwargs items k mutated new_node = cloned_gm graph call_function torch clone args= v new_kwargs k = len kwargs_inputs kwargs_inputs append new_node continue v visited_kwargs new_kwargs k = visited_kwargs v continue visited_kwargs v = len kwargs_inputs kwargs_inputs append v new_kwargs k = visited_kwargs v triton_inputs node name = new_kwargs new_outputs = kwargs_inputs + grid_inputs node cloned_gm graph nodes node op == output node args = tuple new_outputs break cloned_gm recompile runner = torch fx Interpreter cloned_gm returned_outputs = runner run example_inputs Extract store grid autotuning len grid_inputs grid_outputs = returned_outputs len kwargs_inputs autotuning_grids = node triton_nodes dynamic_grid = False new_grids list tuple Any = grid node kwargs grid new_grid = val grid isinstance val torch fx Node new_grid append val continue dynamic_grid = True new_grid append grid_outputs visited_grids val pyrefly ignore bad-argument-type new_grids append tuple new_grid dynamic_grid autotuning_grids node name = new_grids Store kwargs input autotuning autotuning_inputs = returned_outputs len kwargs_inputs autotuning_mapping = triton_inputs codegen_with_cpp_wrapper - tuple ValueWithLineMap ValueWithLineMap For GPU Triton kernels autotuned stored cubin files any device device_types device cuda xpu extract_real_inputs - list Union int float torch Tensor materialize x Union torch SymInt torch SymFloat torch Tensor - Union int float torch Tensor x None pyrefly ignore bad-return None isinstance x torch SymInt torch SymFloat Need concrete value run dynamic shapes tune result x node hint isinstance x FakeTensor defake x assert isinstance x torch Tensor Unknown type when creating real inputs + str type x x tracing_context = torch _guards TracingContext try_get tracing_context None isinstance V real_inputs NullHandler tracing_context output_strides tracing_context output_strides clear params_flat = param param tracing_context params_flat type ignore union-attr param None real_inputs = materialize x x itertools chain params_flat V real_inputs In backward pass V real_inputs OrderedSet Generating random inputs based example_inputs sometimes can problematic e g illegal memory access A comprehensive fix autotune separate process real_inputs = materialize x type ignore arg-type x example_inputs type ignore union-attr isinstance V real_inputs NullHandler V real_inputs mutated_inputs compile_fx clone_preserve_strides mutated_input_idxs = idx idx name enumerate graph_inputs name mutated_inputs isinstance real_inputs idx torch Tensor idx mutated_input_idxs clone mutated Tensor inputs avoid mutating them first pass CPP wrapper-based compilation will lead side effect example inputs e g torch compile f x called input-mutating f inputs x will mutated twice process once here again when running compiled model will also lead numerically incorrect output mutated_inp = real_inputs idx assert isinstance mutated_inp torch Tensor real_inputs idx = clone_preserve_strides mutated_inp del mutated_inp real_inputs config triton autotune_at_compile_time If autotune_at_compile_time True we can do codegen one-pass We will construct autotuning values user defined kernel exists config triton autotune_with_sample_inputs user_defined_kernels = False op operations isinstance op ir UserDefinedTritonKernel user_defined_kernels = True break user_defined_kernels real_inputs = extract_real_inputs extract_autotune_inputs real_inputs codegen first pass cpp_wrapper = False compiled = compile_to_module call real_inputs = extract_real_inputs torch utils _python_dispatch _disable_current_modes compiled real_inputs del real_inputs second pass cpp_wrapper = True removed_buffers clear removed_operations clear inplaced_to_remove clear V graph sizevars precomputed_replacements clear V graph sizevars inv_precomputed_replacements clear metrics reset config patch triton autotune_at_compile_time False codegen cpu codegen _update_scheduler - None Re initializes scheduler member When initializing scheduler no CUBIN files should generated avoid biasing any benchmarks pessimizing fusion decisions scheduler Scheduler config patch triton store_cubin False scheduler = Scheduler operations codegen - tuple ValueWithLineMap ValueWithLineMap dynamo_timed GraphLowering codegen log_pt _compile_event=True init_wrapper_code _update_scheduler V debug draw_orig_fx_graph orig_gm scheduler nodes wrapper_code push_codegened_graph scheduler codegen log debug Finished codegen all nodes The list kernel names available s V graph all_codegen_kernel_names result = wrapper_code generate is_inference wrapper_code pop_codegened_graph result codegen_subgraph parent_graph GraphLowering - None This more compact version ` codegen ` above where we codegen graph subgraph some parent graph The parent graph passed argument intention inline codegening subgraph parent graph s wrapper code including generated kernels The wrapper code finalized via ` generate ` call will done parent graph s ` codegen ` dynamo_timed GraphLowering codegen_subgraph log_pt _compile_event=True wrapper_code = parent_graph wrapper_code device_ops = parent_graph device_ops cpp_wrapper = parent_graph cpp_wrapper _update_scheduler scheduler codegen count_bytes - tuple int list tuple BaseSchedulerNode int list tuple BaseSchedulerNode float total_bytes = node_counts = node_runtimes = node scheduler nodes num_bytes = node get_read_write_buffers_sizes total_bytes += num_bytes node_counts append node num_bytes node_runtimes append node node get_estimated_runtime total_bytes node_counts node_runtimes No-op patched unit tests save_output_code Optional Callable str None = None compile_to_module - CompiledModule dynamo_timed GraphLowering compile_to_module phase_name= code_gen log_pt _compile_event=True dynamo_compile_column_us= inductor_code_gen_cumulative_compile_time_us _compile_to_module _compile_to_module - CompiledModule If we re here we don t have worry about kernel code which only returned separately AOTInductor mode wrapper_code _ = codegen_with_cpp_wrapper cpp_wrapper codegen isinstance wrapper_code ValueWithLineMap mod = _compile_to_module_lines wrapper_code isinstance wrapper_code FileBackedGraphModule mod = wrapper_code raise NotImplementedError f Unrecognized wrapper code type type wrapper_code Logged twice per https github com pytorch pytorch pull #discussion_r TODO Revisit once logging API more mature assert mod __file__ None log_module_code mod __file__ log debug Output code written s mod __file__ output_code_log info Output code written s mod __file__ config benchmark_kernel print f Compiled module path mod __file__ file=sys stderr isinstance wrapper_code FileBackedGraphModule V debug output_code mod __file__ V debug copy os path splitext mod __file__ + debug mod _compile_to_module_lines wrapper_code ValueWithLineMap - CompiledModule codecache PyCodeCache config triton autotune_at_compile_time sanitize docstrings kernel defs kernel_autotune_defs = wrapper_code kernel_autotune_defs getvalue kernel_autotune_defs = kernel_autotune_defs replace \\ \\ \\ tuning_code = \n + Compile-time auto-tuning block \n + kernel_autotune_defs + wrapper_code kernel_autotune_calls getvalue + \n wrapper_code value = tuning_code + wrapper_code value GraphLowering save_output_code None GraphLowering save_output_code wrapper_code value output_code_log debug Output code \n s wrapper_code value inductor_meta = autotune_cache inductor_meta_from_config AutotuneCacheBundler begin_compile inductor_meta code=wrapper_code value try linemap = line_no node stack_trace type ignore attr-defined line_no node wrapper_code line_map key path = PyCodeCache write wrapper_code value output_code_log debug Output code written s path V debug output_code path V debug copy os path splitext path + debug except Exception trace_structured inductor_output_code Just omit filename I still want code though payload_fn=lambda wrapper_code value raise trace_structured inductor_output_code lambda filename path file_path os path abspath path payload_fn=lambda wrapper_code value dynamo_timed PyCodeCache load_by_key_path log_pt _compile_event=True mod = PyCodeCache load_by_key_path key path linemap=linemap type ignore arg-type attrs= constants torchbind_constants cache_key = key cache_path = path cache_linemap = linemap type ignore assignment config benchmark_harness config profile_bandwidth_output run inputs code gen get bandwidth info mod benchmark_compiled_module times= repeat= mod _get_output_names graph_outputs list ir IRNode - list str names = shape_counter = itertools count none_counter = itertools count node graph_outputs isinstance node ir NoneAsConstantBuffer names append f name _none next none_counter isinstance node ir ShapeAsConstantBuffer names append f name _shape next shape_counter names append node get_name names get_output_names - list str _get_output_names graph_outputs is_unspec_arg name str - bool dynamo wraps unspec variable d CPU tensor need convert scalar during codegen triton only name graph_inputs keys graph_inputs name get_numel == len graph_inputs name get_size == get_device_type graph_inputs name == cpu name zero_dim_cpu_tensor_list SubgraphLowering GraphLowering Mostly helper subgraph lowering The main goal call init_wrapper_code subgraph related arguments __init__ parent GraphLowering args Any kwargs Any - None parent = parent super __init__ args kwargs init_wrapper_code is_subgraph bool = False subgraph_name Optional str = None parent_wrapper_code Optional PythonWrapperCodegen = None partition_signatures Optional GraphPartitionSignature = None - None super init_wrapper_code is_subgraph=True subgraph_name=self name parent_wrapper_code=self parent wrapper_code