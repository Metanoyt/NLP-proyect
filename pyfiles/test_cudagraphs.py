Owner s module cuda graphs functools unittest torch torch _dynamo torch _dynamo config torch _dynamo test_case torch _dynamo testing torch _dynamo testing same torch testing _internal common_utils TEST_CUDA_GRAPH composed decs deco f dec reversed decs f = dec f f deco assert_aot_autograd_counter ok=True deco f functools wraps f wrap args kwargs torch _dynamo utils counters clear r = f args kwargs c_ok = torch _dynamo utils counters aot_autograd ok c_not_ok = torch _dynamo utils counters aot_autograd not_ok ok assertGreater c_ok assertEqual c_not_ok assertEqual c_ok assertGreater c_not_ok r wrap deco patch_all ok=True composed torch _dynamo config patch verify_correctness=True automatic_dynamic_shapes=True assert_aot_autograd_counter ok N_ITERS = unittest skipIf torch cuda is_available these tests require cuda TestAotCudagraphs torch _dynamo test_case TestCase patch_all test_basic model x y x + y y torch compile backend= cudagraphs fn x y _ range N_ITERS loss = model x y sum loss backward x = torch randn device= cuda requires_grad=True y = torch randn device= cuda fn x y patch_all test_dtoh model x y = x + y b = cpu b torch compile backend= cudagraphs fn x y _ range N_ITERS loss = model x y sum loss backward x = torch randn device= cuda requires_grad=True y = torch randn device= cuda fn x y patch_all test_htod model x y = x + y torch compile backend= cudagraphs fn x y _ range N_ITERS loss = model x y sum loss backward x = torch randn device= cuda requires_grad=True y = torch randn device= cpu fn x y test_mutate_input model x y y add_ x y torch compile backend= cudagraphs fn x y i range N_ITERS subTest i y_orig = y clone loss = model x y sum assertTrue same y y_orig + loss backward x = torch randn device= cuda requires_grad=True y = torch randn device= cuda fn x y patch_all test_mutate_constant model x y c = torch tensor c add_ x y + c torch compile backend= cudagraphs fn x y i range N_ITERS subTest i loss = model x y sum assertTrue same loss torch tensor device= cuda loss backward x = torch randn device= cuda requires_grad=True y = torch randn device= cuda fn x y patch_all test_factory model y x = torch zeros device= cuda x add_ x y torch compile backend= cudagraphs fn y i range N_ITERS subTest i loss = model y sum loss backward y = torch randn device= cuda requires_grad=True fn y patch_all test_mutated_metadata more tortured example https github com pytorch pytorch issues model x x = x clone x resize_ x fill_ x torch compile backend= cudagraphs fn x i range N_ITERS subTest i rx = model x assertTrue same rx torch full device= cuda x = torch empty device= cuda fn x patch_all test_dead_fill model x x = x clone y = x x fill_ y fill_ x y torch compile backend= cudagraphs fn x i range N_ITERS subTest i rx ry = model x assertTrue same rx torch full device= cuda assertTrue same ry torch empty device= cuda x = torch empty device= cuda fn x __name__ == __main__ torch _dynamo test_case run_tests TEST_CUDA_GRAPH __name__ == __main__ sys sys exit raise unittest SkipTest cuda graph test skipped run_tests