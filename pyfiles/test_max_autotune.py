Owner s module inductor contextlib functools inspect json logging math os random re tempfile unittest collections abc Callable typing Optional unittest mock torch torch multiprocessing mp nn torch _dynamo reset torch _dynamo exc BackendCompilerFailed torch _dynamo testing rand_strided reset_rng_state torch _dynamo utils counters same torch _inductor config torch _inductor autotune_process _TestBenchmarkRequest CUDA_VISIBLE_DEVICES TuningProcess TuningProcessPool torch _inductor graph GraphLowering torch _inductor ir Buffer ChoiceCaller FixedLayout FlexibleLayout torch _inductor kernel mm_plus_mm aten_mm_plus_mm torch _inductor select_algorithm add_feedback_saver add_preprocessing_fn AlgorithmSelectorCache clear_feedback_savers clear_preprocessing_fns ExternKernelCaller TritonTemplate TritonTemplateCaller torch _inductor template_heuristics registry override_template_heuristics torch _inductor template_heuristics triton CUDAMMTemplateConfigHeuristic GemmConfig torch testing _internal common_cuda PLATFORM_SUPPORTS_FP torch testing _internal common_utils instantiate_parametrized_tests IS_WINDOWS NAVI_ARCH parametrize skipIfRocmArch TEST_WITH_ROCM torch testing _internal logging_utils multiple_logs_to_string torch utils _triton has_datacenter_blackwell_tma_device has_triton_stable_tma_api has_triton_tma_device aten = torch ops aten torch _inductor mock_cache global_stats PatchCaches Stats torch _inductor test_case run_tests TestCase torch _inductor utils fresh_cache get_k_splits run_and_get_code use_decompose_k_choice torch _inductor virtualized V torch fx experimental proxy_tensor make_fx torch testing FileCheck torch testing _internal common_utils MI _ARCH runOnRocmArch skipIfXpu torch testing _internal inductor_utils get_func_call get_kernel_launch GPU_TYPE HAS_CPU HAS_CUDA_AND_TRITON HAS_GPU torch set_float _matmul_precision high HAS_CUDA_AND_TRITON torch cuda memory _set_allocator_settings expandable_segments False benchmark_choice choice args out expected_out timings result = choice benchmark args out=out expected_out None torch testing assert_close out expected_out timings copy_ torch tensor result FailChoiceCaller ChoiceCaller benchmark args out raise RuntimeError This choice caller will always throw unittest mock patch torch _inductor select_algorithm TritonTemplate test_cache new=True config patch enable_caching_generated_triton_templates=True instantiate_parametrized_tests TestMaxAutotune TestCase parametrize dynamic False True parametrize search_space DEFAULT EXHAUSTIVE test_max_autotune_mm_plus_mm_zero_size_input dynamic search_space Make sure autotuning mm_plus_mm zero-size input works without crashes m n k = mm_plus_mm b c d b + c d = torch randn m k GPU_TYPE b = torch randn k n GPU_TYPE c = torch randn m k GPU_TYPE d = torch randn k n GPU_TYPE config patch max_autotune True max_autotune_gemm_search_space search_space torch compile mm_plus_mm dynamic=dynamic b c d unittest skipIf has_triton_tma_device Need device-side TMA support Triton parametrize a_transposed False True parametrize b_transposed False True parametrize dynamic False True parametrize tma_store False True test_max_autotune_regular_mm_persistent_tma a_transposed bool b_transposed bool dynamic bool tma_store bool mm b TMA requires -byte alignment here we repeat dims factor float -byte All dims repeated due possible transpositions below = repeat b = b repeat a_transposed = T b_transposed b = b T torch mm b M N K = = torch randn K M a_transposed M K torch float GPU_TYPE b = torch randn N K b_transposed K N torch float GPU_TYPE config patch max_autotune True triton enable_persistent_tma_matmul triton native_matmul False triton enable_template_tma_store tma_store test_configs autotune_choice_name_regex mm_persistent_tma c_actual code = run_and_get_code torch compile mm dynamic=dynamic b c_expected = mm b has_triton_stable_tma_api make_desc_api = triton language make_tensor_descriptor read_api = tl load_tensor_descriptor tma_store Note The tma_descriptor generated kernel If code generation process changes could change write_api = tma_descriptor store write_api = tl store make_desc_api = triton language extra cuda experimental_device_tensormap_create d read_api = tl _experimental_descriptor_load TMA store supported experimental API write_api = tl store Verify we using TMA implementation FileCheck check triton_tem_fused_mm check make_desc_api check read_api check write_api run code torch testing assert_close c_actual c_expected atol= e- rtol= e- unittest skipIf has_triton_tma_device Need device-side TMA support Triton parametrize a_transposed False True parametrize b_transposed False True parametrize dynamic False True test_max_autotune_regular_mm_persistent_tma_strided a_transposed bool b_transposed bool dynamic bool mm b TMA requires -byte alignment here we repeat dims factor float -byte All dims repeated due possible transpositions below = repeat b = b repeat a_transposed = T b_transposed b = b T torch mm b next_multiple_ int - int + M N K = a_shape = K M a_transposed M K a_stride = next_multiple_ M a_transposed next_multiple_ K = torch empty_strided a_shape a_stride dtype=torch float GPU_TYPE = torch randn a_shape dtype=torch float = GPU_TYPE b_shape = N K b_transposed K N b_stride = next_multiple_ K a_transposed next_multiple_ N b = torch empty_strided b_shape b_stride dtype=torch float b = torch randn b_shape dtype=torch float b = b GPU_TYPE config patch max_autotune True triton enable_persistent_tma_matmul triton native_matmul False test_configs autotune_choice_name_regex mm_persistent_tma c_actual code = run_and_get_code torch compile mm dynamic=dynamic b c_expected = mm b torch testing assert_close c_actual c_expected atol= e- rtol= e- Verify we using TMA implementation depending whether we re using experimental API we check different string check_str = triton language extra cuda experimental_device_tensormap_create d has_triton_stable_tma_api check_str = triton language make_tensor_descriptor FileCheck check triton_tem_fused_mm check check_str run code unittest skipIf has_datacenter_blackwell_tma_device Need Blackwell device-side TMA support Triton parametrize a_transposed False True parametrize b_transposed False True parametrize dynamic False True parametrize tma_store False True parametrize epilogue_subtile False True test_blackwell_max_autotune_regular_mm_persistent_tma a_transposed bool b_transposed bool dynamic bool tma_store bool epilogue_subtile bool mm b TMA requires -byte alignment here we repeat dims factor float -byte All dims repeated due possible transpositions below = repeat b = b repeat a_transposed = T b_transposed b = b T torch mm b M N K = = torch randn K M a_transposed M K torch float GPU_TYPE b = torch randn N K b_transposed K N torch float GPU_TYPE config patch max_autotune True triton enable_persistent_tma_matmul True triton enable_template_tma_store tma_store triton enable_epilogue_subtiling epilogue_subtile test_configs autotune_choice_name_regex blackwell_ws_persistent_device_tma c_actual code = run_and_get_code torch compile mm dynamic=dynamic b c_expected = mm b torch testing assert_close c_actual c_expected atol= e- rtol= e- write_count = epilogue_subtile tma_store Verify we using TMA implementation Note The tma_descriptor generated kernel If code generation process changes could change write_api = tma_descriptor store write_api = tl store FileCheck check triton_tem_fused_mm check triton language make_tensor_descriptor check tl load_tensor_descriptor check_count write_api write_count run code unittest skipIf has_triton_tma_device Need device-side TMA support Triton skipIfXpu msg= TMA path Intel GPU require check parametrize dynamic False True test_max_autotune_regular_mm_persistent_tma_illegal_alignment dynamic mm b torch mm b M N K = = torch randn M K torch float GPU_TYPE b = torch randn K N torch float GPU_TYPE assertRaises BackendCompilerFailed context config patch max_autotune True triton enable_persistent_tma_matmul triton native_matmul False test_configs autotune_choice_name_regex mm_persistent_tma torch compile mm dynamic=dynamic b Lowering persistent+TMA Triton template should skipped any input inner dims -byte aligned As result given config flags above we should have no choices left assertIn NoValidChoicesError str context exception unittest skipIf has_triton_tma_device Need device-side TMA support Triton parametrize dynamic False True test_max_autotune_regular_mm_persistent_tma_illegal_output_alignment dynamic mm b out torch mm b out=out out M N K = = torch empty_strided M K K dtype=torch float device=GPU_TYPE = torch randn M K dtype=torch float b = torch empty_strided K N K dtype=torch float device=GPU_TYPE b = torch randn K N dtype=torch float allocate output stride divisible so can t satisfy TMA alignment checks out = torch empty_strided M N N dtype=torch float device=GPU_TYPE assertRaises BackendCompilerFailed context config patch max_autotune True triton enable_persistent_tma_matmul triton native_matmul False triton enable_template_tma_store True test_configs autotune_choice_name_regex mm_persistent_tma torch compile mm dynamic=dynamic b out Lowering persistent+TMA Triton template should skipped since output doesn t have stride any dim assertIn NoValidChoicesError str context exception unittest skipIf has_triton_tma_device Need device-side TMA support Triton test_max_autotune_regular_mm_tma_dynamic_outer_dim mm b torch mm b M N K = = torch randn M K torch float GPU_TYPE b = torch randn K N torch float GPU_TYPE TMA requires -byte alignment here we repeat dims factor float -byte All dims repeated due possible transpositions below = repeat b = b repeat torch _dynamo mark_dynamic config patch max_autotune True triton enable_persistent_tma_matmul triton native_matmul False test_configs autotune_choice_name_regex mm_persistent_tma c_actual = torch compile mm b c_expected = mm b torch testing assert_close c_actual c_expected atol= e- rtol= e- parametrize dynamic False True test_max_autotune_regular_mm_zero_size_input dynamic bool Make sure autotuning mm zero-size input works without crashes mm b = torch sin b = torch randn GPU_TYPE b = torch randn GPU_TYPE config patch max_autotune True torch compile mm dynamic=dynamic b NOTE current Inductor template verifies scaling mode either per-tensor per-row TODO support additional scaling modes Blackwell unittest skipIf has_datacenter_blackwell_tma_device Need Blackwell device-side TMA support Triton parametrize dynamic False True parametrize tma_store False True test_blackwell_max_autotune_scaled_mm_per_tensor_persistent_tma dynamic bool tma_store bool scaled_mm b scale_a scale_b NOTE Inductor constrains row_major b col_major torch _scaled_mm b t scale_a scale_b use_fast_accum=True out_dtype=torch float get_scale_per_tensor t scale = torch finfo torch float _e m fn max t abs max scale torch float TMA requires -byte alignment here we repeat dims factor float -byte M N K = = torch randn M K torch float GPU_TYPE repeat b = torch randn N K torch float GPU_TYPE repeat scale_a = get_scale_per_tensor scale_b = get_scale_per_tensor b = torch float _e m fn b = b torch float _e m fn config patch max_autotune True triton enable_persistent_tma_matmul True triton enable_template_tma_store tma_store test_configs autotune_choice_name_regex blackwell_ws_persistent_device_tma c_actual code = run_and_get_code torch compile scaled_mm dynamic=dynamic b scale_a scale_b c_expected = scaled_mm b scale_a scale_b torch testing assert_close c_actual c_expected atol= e- rtol= tma_store Verify we using TMA implementation Note The tma_descriptor generated kernel If code generation process changes could change write_api = tma_descriptor store write_api = tl store FileCheck check triton_tem_fused__scaled_mm check triton language make_tensor_descriptor check tl load_tensor_descriptor check write_api run code unittest skipIf has_datacenter_blackwell_tma_device Need Blackwell device-side TMA support Triton parametrize dynamic False True parametrize tma_store False True test_blackwell_max_autotune_scaled_mm_per_row_persistent_tma dynamic bool tma_store bool scaled_mm b scale_a scale_b NOTE Inductor constrains row_major b col_majo torch _scaled_mm b t scale_a scale_b t use_fast_accum=True out_dtype=torch bfloat get_scale_per_row t scale = torch finfo torch float _e m fn max t abs max dim= keepdim=True values scale torch float TMA requires -byte alignment here we repeat dims factor float -byte M N K = = torch randn M K torch bfloat GPU_TYPE repeat b = torch randn N K torch bfloat GPU_TYPE repeat scale_a = get_scale_per_row scale_b = get_scale_per_row b = torch float _e m fn b = b torch float _e m fn config patch max_autotune True triton enable_persistent_tma_matmul True triton enable_template_tma_store tma_store test_configs autotune_choice_name_regex blackwell_ws_persistent_device_tma c_actual code = run_and_get_code torch compile scaled_mm dynamic=dynamic b scale_a scale_b c_expected = scaled_mm b scale_a scale_b torch testing assert_close c_actual c_expected atol= e- rtol= tma_store Verify we using TMA implementation Note The tma_descriptor generated kernel If code generation process changes could change write_api = tma_descriptor store write_api = tl store FileCheck check triton_tem_fused__scaled_mm check triton language make_tensor_descriptor check tl load_tensor_descriptor check write_api run code unittest skipIf has_triton_tma_device Need device-side TMA support Triton parametrize a_transposed False True parametrize b_transposed False True parametrize dynamic False True parametrize tma_store False True test_max_autotune_addmm_persistent_tma a_transposed bool b_transposed bool dynamic bool tma_store bool addmm x b TMA requires -byte alignment here we repeat dims factor float -byte All dims repeated due possible transpositions below x = x repeat = repeat b = b repeat a_transposed = T b_transposed b = b T torch addmm x b M N K = = torch randn K M a_transposed M K torch float GPU_TYPE b = torch randn N K b_transposed K N torch float GPU_TYPE x = torch randn N torch float GPU_TYPE config patch max_autotune True triton enable_persistent_tma_matmul triton native_matmul False triton enable_template_tma_store tma_store test_configs autotune_choice_name_regex mm_persistent_tma c_actual code = run_and_get_code torch compile addmm dynamic=dynamic x b c_expected = addmm x b has_triton_stable_tma_api make_desc_api = triton language make_tensor_descriptor read_api = tl load_tensor_descriptor tma_store Note The tma_descriptor generated kernel If code generation process changes could change write_api = tma_descriptor store write_api = tl store make_desc_api = triton language extra cuda experimental_device_tensormap_create d read_api = tl _experimental_descriptor_load TMA store supported experimental API write_api = tl store Verify we using TMA implementation FileCheck check triton_tem_fused_addmm check make_desc_api check read_api check write_api run code torch testing assert_close c_actual c_expected atol= e- rtol= e- unittest skipIf has_datacenter_blackwell_tma_device Need Blackwell device-side TMA support Triton parametrize a_transposed False True parametrize b_transposed False True parametrize dynamic False True parametrize tma_store False True parametrize epilogue_subtile False True test_blackwell_max_autotune_addmm_persistent_tma a_transposed bool b_transposed bool dynamic bool tma_store bool epilogue_subtile bool addmm x b TMA requires -byte alignment here we repeat dims factor float -byte All dims repeated due possible transpositions below x = x repeat = repeat b = b repeat a_transposed = T b_transposed b = b T torch addmm x b M N K = = torch randn K M a_transposed M K torch float GPU_TYPE b = torch randn N K b_transposed K N torch float GPU_TYPE x = torch randn N torch float GPU_TYPE config patch max_autotune True triton enable_persistent_tma_matmul True triton enable_template_tma_store tma_store triton enable_epilogue_subtiling epilogue_subtile test_configs autotune_choice_name_regex blackwell_ws_persistent_device_tma c_actual code = run_and_get_code torch compile addmm dynamic=dynamic x b c_expected = addmm x b make_desc_api = triton language make_tensor_descriptor read_api = tl load_tensor_descriptor write_count = epilogue_subtile tma_store Verify we using TMA implementation Note The tma_descriptor generated kernel If code generation process changes could change write_api = tma_descriptor store write_api = tl store Verify we using TMA implementation FileCheck check triton_tem_fused_addmm check make_desc_api check read_api check_count write_api write_count run code torch testing assert_close c_actual c_expected atol= e- rtol= e- unittest skipIf has_triton_tma_device Need device-side TMA support Triton skipIfXpu msg= TMA path Intel GPU require check parametrize dynamic False True test_max_autotune_addmm_persistent_tma_illegal_alignment dynamic addmm x b torch addmm x b M N K = = torch randn M K torch float GPU_TYPE b = torch randn K N torch float GPU_TYPE x = torch randn N torch float GPU_TYPE assertRaises BackendCompilerFailed context config patch max_autotune True triton enable_persistent_tma_matmul triton native_matmul False test_configs autotune_choice_name_regex mm_persistent_tma torch compile addmm dynamic=dynamic x b Lowering persistent+TMA Triton template should skipped any input inner dims -byte aligned As result given config flags above we should have no choices left assertIn NoValidChoicesError str context exception unittest skipIf has_triton_tma_device Need device-side TMA support Triton test_max_autotune_addmm_tma_dynamic_outer_dim addmm x b torch addmm x b M N K = = torch randn M K torch float GPU_TYPE b = torch randn K N torch float GPU_TYPE x = torch randn N torch float GPU_TYPE TMA requires -byte alignment here we repeat dims factor float -byte All dims repeated due possible transpositions below x = x repeat = repeat b = b repeat torch _dynamo mark_dynamic config patch max_autotune True triton enable_persistent_tma_matmul triton native_matmul False test_configs autotune_choice_name_regex mm_persistent_tma c_actual = torch compile addmm x b c_expected = addmm x b torch testing assert_close c_actual c_expected atol= e- rtol= e- fresh_cache skipIfXpu msg= XPU doesn t support sm carveout unittest skipIf TEST_WITH_ROCM ROCm doesn t support sm carveout unittest skipIf IS_WINDOWS Windows doesn t support persistent TMA unittest skipIf has_triton_tma_device Need device-side TMA support Triton unittest skipIf has_datacenter_blackwell_tma_device B doesn t support sm carveout parametrize carveout None parametrize op mm scaled_mm test_honor_sm_carveout_with_triton_tma carveout op str mm_func b torch mm b scaled_mm b scale_a scale_b torch _scaled_mm b scale_a scale_b out_dtype=torch bfloat Create large matrices ensure we use all possible sms size = = torch randn size size device=GPU_TYPE dtype=torch bfloat b = torch randn size size device=GPU_TYPE dtype=torch bfloat transpose contiguous transpose scale_a = torch tensor dtype=torch float device=GPU_TYPE scale_b = torch tensor dtype=torch float device=GPU_TYPE args = torch float _e m fn b torch float _e m fn scale_a scale_b op == scaled_mm b func = scaled_mm op == scaled_mm mm_func Set specified carveout value torch _C _set_sm_carveout_experimental carveout carveout None assertIsNone torch _C _get_sm_carveout_experimental assertEqual torch _C _get_sm_carveout_experimental carveout config patch max_autotune True triton enable_persistent_tma_matmul True triton native_matmul False max_autotune_gemm_backends TRITON test_configs autotune_choice_name_regex tma compiled_mm = torch compile func mode= max-autotune-no-cudagraphs compiled_mm args Warm-up compilation tempfile NamedTemporaryFile f torch profiler profile activities= torch profiler ProfilerActivity CUDA prof Run specified carveout compiled_mm args Export trace analyze results prof export_chrome_trace f name Extract grid sizes trace events TMA kernels kernel_name = triton_tem_fused kernel_events = grid evt get args get grid grid_size math prod evt get args get grid evt json load open f name traceEvents evt get cat == kernel kernel_name evt get name lower We should have exactly kernel event run assertEqual len kernel_events f Expected exactly kernel event got len kernel_events Check grid size matches expected values based carveout expected_grid_size = None max_grid_size = torch cuda get_device_properties cuda multi_processor_count careveout = carveout None carveout expected_grid_size = max_grid_size - careveout assertEqual kernel_events grid_size expected_grid_size f Grid size kernel_events grid_size doesn t match expected_grid_size carveout= carveout parametrize dynamic False True test_max_autotune_addmm_zero_size_input dynamic Make sure autotuning addmm zero-size input works without crashes addmm x b torch addmm x b x = torch randn GPU_TYPE = torch randn GPU_TYPE b = torch randn GPU_TYPE config patch max_autotune True torch compile addmm dynamic=dynamic x b parametrize search_space DEFAULT EXHAUSTIVE test_autotune_conv x search_space Assuming input has channels we want produce channels output conv x = torch nn Conv d in_channels= out_channels= kernel_size= memory_format=torch channels_last GPU_TYPE Example input tensor batch size = channels = height = width = The memory format set ` channels_last ` input_tensor = torch randn contiguous memory_format=torch channels_last GPU_TYPE config patch max_autotune True max_autotune_gemm_backends TRITON max_autotune_gemm_search_space search_space torch compile foo mod x mod x torch no_grad out code = run_and_get_code foo conv x input_tensor FileCheck check_not extern_kernels convolution run code assertEqual conv x input_tensor out atol= e- rtol= fresh_cache config patch max_autotune=True max_fusion_size= test_jit_fusion_matches_aot_fusion In example AOTInductor s JIT-compile will fuse buf buf due proximity we want make sure AOT-compile pass does same AOT could do fuse buf buf instead buf pushed end V graph buffers list because fuse buf buf would have better proximity score than fuse buf buf This scenario possible since finalizing MultiTemplateBuffers needs replace buffers fn x number buf = x + x buf = number item buf = x x buf = x x MultiTemplateBuffer buf = x buf buf buf buf buf inputs = torch rand device=GPU_TYPE torch tensor device=GPU_TYPE torch _export aot_compile fn args=inputs test_cat_addmm fn torch Tensor b torch Tensor c torch Tensor torch cat torch addmm b c torch addmm b c args = torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch randn device=GPU_TYPE config patch max_autotune True max_autotune_gemm_backends Triton expected = fn args actual = torch compile fn args torch testing assert_close actual expected atol= e- rtol= e- config patch benchmark_kernel=True fallback_random=True max_autotune_gemm=True parametrize device cpu GPU_TYPE test_matmul_dropout device fwd b x = b x = torch nn functional dropout x x fn b x = fwd b sum x backward grad N = = torch randn N N device=device requires_grad=True b = torch randn N N device=device opt_fn = torch compile fn reset_rng_state ref = fn b reset_rng_state act = opt_fn b N = print f ref\n ref \nact\n act torch testing assert_close ref act atol= e- rtol= e- config patch max_autotune_gemm=True unittest skipIf getattr torch GPU_TYPE device_count Need least devices test test_autotune_device_guard x = torch randn device=f GPU_TYPE y = torch randn device=f GPU_TYPE f x y x y fresh_cache act = torch compile f x y ref = f x y assertTrue torch allclose act ref atol= e- rtol= e- config patch max_autotune=True parametrize search_space DEFAULT EXHAUSTIVE parametrize kernel_size test_empty_conv_input search_space kernel_size x = torch randn device=GPU_TYPE weight = torch randn kernel_size kernel_size device=GPU_TYPE f x weight torch convolution x weight bias=None stride= padding= dilation= transposed=False output_padding= groups= config patch max_autotune_gemm_search_space search_space opt_f = torch compile f ref = f x weight act = opt_f x weight assertTrue torch allclose ref act atol= e- rtol= e- skipIfXpu msg= Fails Intel XPU see https github com pytorch pytorch issues config patch max_autotune_gemm_backends= TRITON parametrize search_space DEFAULT EXHAUSTIVE test_baddmm search_space M torch nn Module __init__ super __init__ weight = torch nn Parameter torch randn dtype=torch float bias = torch nn Parameter torch randn dtype=torch float forward x torch ops aten baddbmm default bias x weight x = torch randn dtype=torch float requires_grad=False device=GPU_TYPE mod = M GPU_TYPE config patch max_autotune_gemm_search_space search_space m_c = torch compile mode= max-autotune mod out code = run_and_get_code m_c x assertEqual out mod x atol= e- rtol= e- config triton native_matmul FileCheck check triton_tem_fused_baddbmm run code config patch max_autotune=True test_conv x _with_free_symbols Make sure there no exception due free symbols conv = nn Conv d kernel_size= stride= padding= bias=False device=GPU_TYPE torch compile f x y z h = y nonzero size w = z nonzero size x = x h w x = conv x x x = torch randn memory_format=torch channels_last device=GPU_TYPE _ range y = torch randint device=GPU_TYPE z = torch randint device=GPU_TYPE f x y z _test_cat_max_autotune_impl using_triton_mm f x y y = torch cos y x = torch mm x x torch cat x y f_c = torch compile mode= max-autotune-no-cudagraphs f inps = torch randn device=GPU_TYPE torch randn device=GPU_TYPE _ code = run_and_get_code f_c inps inps assertEqual f_c inps f inps atol= rtol= mm kernel cos kernel count = using_triton_mm config triton native_matmul FileCheck check get_func_call check_count get_kernel_launch count exactly=True run code f x y y = torch cos y x = torch mm x x out = torch cat x y out x + f_c = torch compile mode= max-autotune-no-cudagraphs f _ code = run_and_get_code f_c inps inps assertEqual f_c inps f inps atol= rtol= FileCheck check get_func_call check_count get_kernel_launch exactly=True run code f x y y = torch cos y x = torch mm x x torch cat x y torch cat y x f_c = torch compile mode= max-autotune-no-cudagraphs f assertEqual f_c inps f inps atol= rtol= config patch trace enabled True config patch test_configs force_extern_kernel_in_multi_template True config patch triton native_matmul False test_mutation_rename torch _logging set_logs ir_post_fusion=True f x y z other mul = x y diag = torch diagonal mul diag copy_ other x = torch mm mul z y = torch diagonal x add_ torch tensor device=GPU_TYPE y t = functools partial torch randn device=GPU_TYPE inps = t t t t fn = torch compile f mode= max-autotune-no-cudagraphs pre_fusion_tream post_fusion_stream ctx = multiple_logs_to_string torch _inductor debug ir_pre_fusion ir_post_fusion config patch trace debug_dir tempfile mkdtemp assertLogs logging getLogger torch _inductor debug level=logging INFO cm ctx out = fn inps assertEqual f inps out pre_fusion_stream = cm output post_fusion_stream = cm output before after finalizing multi template buffer deps should have same normalization wrt writes FileCheck check MultiTemplateBuffer check unmet check_same buf run pre_fusion_stream FileCheck check ExternKernelSchedulerNode check unmet check_same buf run post_fusion_stream torch _logging set_logs config patch test_configs force_extern_kernel_in_multi_template True test_cat_max_autotune_extern _test_cat_max_autotune_impl using_triton_mm=False skipIfXpu msg= The fusion happened because do speedup XPU see issue config patch max_autotune_gemm_backends TRITON benchmark_epilogue_fusion False test_cat_max_autotune_triton _test_cat_max_autotune_impl using_triton_mm=True parametrize search_space DEFAULT EXHAUSTIVE test_conv_cat search_space ToyModel torch nn Module __init__ super __init__ conv = torch nn Conv d kernel_size= stride= padding= bias=False forward x x = conv x torch cat x x + config patch max_autotune_gemm_search_space search_space torch no_grad m = ToyModel device=GPU_TYPE input_tensor = torch randn device=GPU_TYPE convolution currently plannable m = torch compile m mode= max-autotune-no-cudagraphs out code = run_and_get_code m input_tensor assertEqual out m input_tensor TEST_WITH_ROCM FileCheck check triton_poi_fused_add_cat_ run code parametrize search_space DEFAULT EXHAUSTIVE test_conv d search_space fn = torch nn functional conv d image = torch randn filt = torch randn config patch max_autotune True max_autotune_gemm_search_space search_space expected = fn image filt actual = torch compile fn image filt torch testing assert_close actual expected atol= e- rtol= config patch max_autotune=True max_autotune_conv_backends= layout_optimization=False test_conv_backend m = torch nn Sequential torch nn Conv d GPU_TYPE inp = torch randn GPU_TYPE assertRaises BackendCompilerFailed context torch compile m inp assertIn NoValidChoicesError str context exception skipIfRocmArch NAVI_ARCH test_non_contiguous_input_mm Make sure triton template can work non-contiguous inputs without crash Check https github com pytorch pytorch issues more details x = rand_strided dtype=torch bfloat device=GPU_TYPE y = rand_strided dtype=torch bfloat device=GPU_TYPE torch compile mode= max-autotune f x y x y ref = x y act = f x y torch testing assert_close act ref atol= e- rtol= e- skipIfRocmArch NAVI_ARCH test_non_contiguous_input_addmm b = torch randn dtype=torch bfloat device=GPU_TYPE x = rand_strided dtype=torch bfloat device=GPU_TYPE y = rand_strided dtype=torch bfloat device=GPU_TYPE torch compile mode= max-autotune f x y torch addmm b x y ref = torch addmm b x y act = f x y torch testing assert_close act ref atol= e- rtol= e- skipIfRocmArch NAVI_ARCH test_non_contiguous_input_bmm x = rand_strided dtype=torch bfloat device=GPU_TYPE y = rand_strided dtype=torch bfloat device=GPU_TYPE torch compile mode= max-autotune f x y torch bmm x y ref = torch bmm x y act = f x y torch testing assert_close act ref atol= e- rtol= e- TODO fix accuracy failure triton template XPU enable test case skipIfXpu unittest skipIf config triton native_matmul native matmul Triton template both have accuracy fail test_non_contiguous_input_mm_plus_mm x = rand_strided device=GPU_TYPE y = rand_strided device=GPU_TYPE x = rand_strided device=GPU_TYPE y = rand_strided device=GPU_TYPE torch compile mode= max-autotune f x y x y x y + x y ref = x y + x y act = f x y x y torch testing assert_close act ref atol= e- rtol= e- config patch max_autotune=True max_autotune_gemm_backends= unittest skipIf config triton native_matmul native matmul generates when size = test_no_valid_choices = torch zeros device=GPU_TYPE b = torch zeros device=GPU_TYPE assertRaises BackendCompilerFailed context torch compile lambda b matmul b b assertIn NoValidChoicesError str context exception unittest skipIf config triton native_matmul Only test when template being called parametrize multi_template True False config patch max_autotune=True max_autotune_gemm_backends= TRITON test_inf_timing multi_template unittest mock patch lookup = AlgorithmSelectorCache lookup mock_lookup args kwargs timings = lookup args kwargs choice float inf choice timings keys = torch zeros device=GPU_TYPE b = torch zeros device=GPU_TYPE patch object AlgorithmSelectorCache lookup mock_lookup config patch benchmark_epilogue_fusion=multi_template assertRaises BackendCompilerFailed context torch compile lambda b matmul b b assertIn NoValidChoicesError str context exception unittest skipIf torch cuda is_available torch cuda get_device_properties total_memory e Only GPU has least GB memory safe config patch force_shape_pad=True max_autotune=True test_linear_and_cel Similate GPU without enough SMs Make sure max-autotune still works even when MultiTritonTemplate encapsulates just extern kernels mock_is_big_gpu args kwargs False B T C V = linear = nn Linear C V bfloat device=GPU_TYPE ce = torch nn CrossEntropyLoss f x y x grad = None linear weight grad = None linear bias grad = None loss = ce linear x y loss backward loss x = torch randn B T C requires_grad=True GPU_TYPE bfloat x retain_grad y = torch randint V B T GPU_TYPE torch _inductor utils inductor_utils unittest mock patch object inductor_utils is_big_gpu mock_is_big_gpu opt_f = torch compile f expect = f x y x grad linear weight grad linear bias grad actual = opt_f x y x grad linear weight grad linear bias grad assert same expect actual tol= e- f ref \n expect \nact \n actual skipIfXpu unittest skipIf config cpp_wrapper decompose_k supported cpp_wrapper yet unittest skipIf config triton native_matmul ignore decompose_k when native matmul codegen parametrize dynamic True False parametrize dtype torch float torch bfloat parametrize sizes config patch max_autotune=True max_autotune_gemm_backends= TRITON comprehensive_padding=False shape_padding=False test_max_autotune_decompose_k sizes dtype dynamic fp _red_setting = torch backends cuda matmul allow_fp _reduced_precision_reduction bf _red_setting = torch backends cuda matmul allow_bf _reduced_precision_reduction torch backends cuda matmul allow_fp _reduced_precision_reduction = False torch backends cuda matmul allow_bf _reduced_precision_reduction = False M N K = sizes = torch randn M K dtype=dtype device=GPU_TYPE requires_grad=True b = torch randn K N dtype=dtype device=GPU_TYPE requires_grad=True possible_splits = range min K M K N + divisors = split split possible_splits K split == check_divisors code kernel code decompose_k kernel divisor_found = False divisor divisors f divisor _split kernel divisor_found = True break assertTrue divisor_found f Could find split divisors kernel compiled_func = torch compile lambda b b dynamic=dynamic We assume large k dim relative m n decompose_k will most performant out code = run_and_get_code compiled_func b dynamic torch version hip FileCheck check_not extern_kernels bmm_dtype check_not decompose_k run code FileCheck check extern_kernels bmm_dtype check_regex triton_ _fused_ run check decompose_k run code check_divisors code torch testing assert_close out b atol= e- rtol= e- Test adding epilogue also equivalent eager compiled_func = torch compile lambda b b relu dynamic=dynamic out code = run_and_get_code compiled_func b dynamic torch version hip FileCheck check_not extern_kernels bmm_dtype check_not decompose_k run code FileCheck check extern_kernels bmm_dtype check_regex triton_ _fused_mm_ run check decompose_k run code check_divisors code torch testing assert_close compiled_func b b relu atol= e- rtol= e- Test adding reinterpret view before subgraph = transpose compiled_func = torch compile lambda b transpose b relu dynamic=dynamic out code = run_and_get_code compiled_func b DecomposeK enabled AMD yet dynamic torch version hip FileCheck check_not extern_kernels bmm_dtype check_not decompose_k run code FileCheck check extern_kernels bmm_dtype check_regex triton_ _fused_ _ run check decompose_k run code check_divisors code torch testing assert_close compiled_func b transpose b relu atol= e- rtol= e- torch backends cuda matmul allow_fp _reduced_precision_reduction = fp _red_setting torch backends cuda matmul allow_bf _reduced_precision_reduction = bf _red_setting skipIfXpu unittest skipIf TEST_WITH_ROCM decompose_k supported ROCm unittest skipIf config cpp_wrapper decompose_k supported cpp_wrapper yet unittest skipIf config triton native_matmul ignore decompose_k when native matmul codegen config patch max_autotune=True max_autotune_gemm_backends= TRITON test_max_autotune_decompose_k_dynamic_input f b a_in = torch stack dim= a_in b relu = torch randn dtype=torch bfloat device=GPU_TYPE requires_grad=True b = torch randn dtype=torch bfloat device=GPU_TYPE requires_grad=True torch _dynamo reset torch _dynamo maybe_mark_dynamic compiled_func = torch compile f mock patch torch _inductor kernel mm use_decompose_k_choice decomp_mock decomp_mock side_effect = lambda args kwargs kwargs get threshold_multiple == out code = run_and_get_code compiled_func b FileCheck check extern_kernels bmm_dtype check_regex triton_ _fused_ run check decompose_k check_regex r s - + = s - + check_regex r \ s - + check_regex s - + = run code torch testing assert_close out f b atol= e- rtol= e- skipIfXpu unittest skipIf TEST_WITH_ROCM decompose_k supported ROCm unittest skipIf config cpp_wrapper decompose_k supported cpp_wrapper yet unittest skipIf config triton native_matmul ignore decompose_k when native matmul codegen config patch max_autotune=True max_autotune_gemm_backends= TRITON test_max_autotune_decompose_k_dynamic_input_bwd f b s a_in = torch cat _ range dim= a_in b relu sum = torch randn dtype=torch bfloat device=GPU_TYPE requires_grad=True b = torch randn dtype=torch bfloat device=GPU_TYPE requires_grad=True torch _dynamo reset torch _dynamo maybe_mark_dynamic compiled_func = torch compile f res = compiled_func b res backward mock patch torch _inductor kernel mm use_decompose_k_choice decomp_mock decomp_mock side_effect = lambda args kwargs kwargs get threshold_multiple == out code = run_and_get_code compiled_func b out backward FileCheck check extern_kernels bmm_dtype check_regex triton_ _fused_ run check decompose_k check_regex r s - + = s - + check_regex r \ s - + check_regex s - + = run code case given backwards code skipIfXpu unittest skipIf TEST_WITH_ROCM decompose_k supported ROCm unittest skipIf config cpp_wrapper decompose_k supported cpp_wrapper yet unittest skipIf config triton native_matmul ignore decompose_k when native matmul codegen config patch max_autotune=True max_autotune_gemm_backends= TRITON test_max_autotune_decompose_k_output_stride f b = transpose b = torch randn device=GPU_TYPE dtype=torch bfloat b = torch randn device=GPU_TYPE dtype=torch bfloat b = b Force only decomposeK choice override_template_heuristics device_type=GPU_TYPE template_op_pairs= torch _inductor kernel mm mm_template name mm mock patch torch _inductor kernel mm use_decompose_k_choice decompose_mock decompose_mock return_value = True compiled_f = torch compile f out code = run_and_get_code compiled_f b Output stride equal original gm output stride If output stride correctly checked will which can cause nans assertEqual out stride FileCheck check_not extern_kernels bmm_dtype check decompose_k check empty_strided_cuda torch bfloat run code unittest skipIf torch version hip ROCM only parametrize dtype torch float torch bfloat torch float parametrize sizes config patch max_autotune=True test_max_autotune_contiguous_transform_mm sizes dtype Test contiguous subgraph transform A transpose B pattern This transform makes second matrix contiguous before matmul M N K = sizes mm_transpose b b transpose = torch randn M K dtype=dtype device=GPU_TYPE requires_grad=True b = torch randn N K dtype=dtype device=GPU_TYPE requires_grad=True Compute fp baseline a_fp = torch float b_fp = b torch float expected_fp = mm_transpose a_fp b_fp Force only contiguous choice test transform mock patch torch _inductor template_heuristics contiguous_mm use_contiguous contiguous_mock contiguous_mock return_value = True compiled_func = torch compile mm_transpose out code = run_and_get_code compiled_func b Verify correctness against fp baseline torch testing assert_close out expected_fp dtype atol= e- rtol= e- Check contiguous transform used FileCheck check contiguous_mm run code unittest skipIf torch version hip ROCM only parametrize dtype torch float torch bfloat torch float parametrize sizes config patch max_autotune=True test_max_autotune_contiguous_transform_addmm sizes dtype Test contiguous subgraph transform addmm non-contiguous second matrix M N K = sizes addmm_transpose inp b torch addmm inp b transpose inp = torch randn M N dtype=dtype device=GPU_TYPE requires_grad=True = torch randn M K dtype=dtype device=GPU_TYPE requires_grad=True b = torch randn N K dtype=dtype device=GPU_TYPE requires_grad=True Compute fp baseline inp_fp = inp torch float a_fp = torch float b_fp = b torch float expected_fp = addmm_transpose inp_fp a_fp b_fp Force contiguous choice test transform mock patch torch _inductor template_heuristics contiguous_mm use_contiguous contiguous_mock contiguous_mock return_value = True compiled_func = torch compile addmm_transpose out code = run_and_get_code compiled_func inp b Verify correctness against fp baseline torch testing assert_close out expected_fp dtype atol= e- rtol= e- Check contiguous transform used FileCheck check contiguous_addmm run code unittest skipIf torch version hip ROCM only parametrize dynamic False True test_max_autotune_contiguous_transform_non_contiguous_second_matrix dynamic Test contiguous transform only applied when second matrix non-contiguous M N K = mm b b = torch randn M K dtype=torch float device=GPU_TYPE b_contiguous = torch randn K N dtype=torch float device=GPU_TYPE b_non_contiguous = torch randn N K dtype=torch float device=GPU_TYPE transpose Compute fp baselines without max_autotune since fp doesn t work max_autotune=True a_fp = torch float b_contiguous_fp = b_contiguous torch float b_non_contiguous_fp = b_non_contiguous torch float expected _fp = mm a_fp b_contiguous_fp expected _fp = mm a_fp b_non_contiguous_fp config patch max_autotune=True Test contiguous second matrix - should use contiguous transform compiled_func_contiguous = torch compile mm dynamic=dynamic out code = run_and_get_code compiled_func_contiguous b_contiguous Should contain contiguous transform try FileCheck check contiguous_mm run code fail Contiguous transform should used contiguous matrices except RuntimeError pass Expected - contiguous transform should used Test non-contiguous second matrix - should use contiguous transform mock patch torch _inductor template_heuristics contiguous_mm use_contiguous contiguous_mock contiguous_mock return_value = True compiled_func_non_contiguous = torch compile mm dynamic=dynamic out code = run_and_get_code compiled_func_non_contiguous b_non_contiguous Should contain contiguous transform FileCheck check contiguous_mm run code Verify correctness against fp baselines torch testing assert_close out expected _fp torch float atol= e- rtol= e- torch testing assert_close out expected _fp torch float atol= e- rtol= e- unittest skipIf torch version hip ROCM only config patch max_autotune=True max_autotune_gemm_backends= TRITON test_max_autotune_contiguous_transform_with_epilogue Test contiguous transform epilogue operations like relu M N K = mm_transpose_relu b b transpose relu = torch randn M K dtype=torch float device=GPU_TYPE b = torch randn N K dtype=torch float device=GPU_TYPE Compute fp baseline a_fp = torch float b_fp = b torch float expected_fp = mm_transpose_relu a_fp b_fp Force contiguous transform mock patch torch _inductor template_heuristics contiguous_mm use_contiguous contiguous_mock contiguous_mock return_value = True compiled_func = torch compile mm_transpose_relu out code = run_and_get_code compiled_func b Verify correctness against fp baseline torch testing assert_close out expected_fp torch float atol= e- rtol= e- Check contiguous transform used FileCheck check contiguous_mm run code test_triton_template_generated_code_cache_key generate_and_load_args = len inspect signature torch _inductor select_algorithm TritonTemplate generate_and_load parameters make_key_args = len inspect signature torch _inductor select_algorithm GeneratedCodeCache make_key parameters Make sure all args generate_and_load_args passed make_key_args Except generate_with_caching update function each time new arg added generate_and_load make sure arg added make_key assertEqual generate_and_load_args - make_key_args assertEqual generate_and_load_args fresh_cache config patch max_autotune True test_configs max_mm_configs max_autotune_gemm_backends TRITON unittest skipIf config triton native_matmul only test template-based matmul test_triton_template_generated_code_cache_strategy func_test x y z m = torch matmul x y b = torch matmul z m b = torch rand device=GPU_TYPE b = torch rand device=GPU_TYPE Test testing strategy works overriding input_dependent_preserved_state simulate cache hit unittest mock patch torch _inductor select_algorithm TritonTemplateKernel input_dependent_preserved_state new= lambda same always assertRaisesRegex torch _inductor exc InductorError r Generated code cache results wrong output torch compile func_test dynamic=False b b config patch max_autotune True test_configs max_mm_configs max_autotune_gemm_backends TRITON unittest skipIf config triton native_matmul only test template-based matmul test_triton_template_generated_code_caching reset_counters torch _dynamo utils counters clear hits torch _dynamo utils counters inductor generated_module_cache_hit misses torch _dynamo utils counters inductor generated_module_cache_miss remove white space x remove_white_space x str - str re sub r \s+ x get_cache_key_and_events - tuple str str cache = TritonTemplate all_templates mm _generated_code_cache _cache cache_key = next iter cache events = str cache cache_key events cache_key events func_test x y z m = torch matmul x y b = torch matmul z m b = torch rand device=GPU_TYPE b = torch rand device=GPU_TYPE Valid cache hit fresh_cache reset_counters compile_results = torch compile func_test dynamic=False b b eager_results = func_test b b assertEqual compile_results eager_results atol= rtol= assertEqual hits assertEqual misses cache_key events = get_cache_key_and_events TEST_WITH_ROCM expected = input_nodes torch float device type= cuda index= torch float device type= cuda index= num_stages num_warps prefix_args suffix_args call_sizes layout torch float device type= cuda index= num_consumer_groups num_buffers_warp_spec epilogue_fn_hash identity tma_store False kwargs EVEN_K False USE_FAST_ACCUM False ACC_TYPE tl float BLOCK_M BLOCK_N BLOCK_K GROUP_M ALLOW_TF True hint_override None expected = expected replace cuda GPU_TYPE assertExpectedInline remove_white_space cache_key remove_white_space expected assertEqual remove_white_space events remove_white_space def_kernel A B Test symbolic shapes different symbols Will cache miss due different symbols inputs fresh_cache = torch rand device=GPU_TYPE b = torch rand device=GPU_TYPE c = torch rand device=GPU_TYPE d = torch rand device=GPU_TYPE reset_counters compiled_results = torch compile func_test dynamic=True b c d eager_results = func_test b c d assertEqual compiled_results eager_results atol= rtol= assertEqual hits assertEqual misses cache_key events = get_cache_key_and_events TEST_WITH_ROCM expected = input_nodes s s s torch float device type= cuda index= s s s torch float device type= cuda index= num_stages num_warps prefix_args suffix_args call_sizes s s layout s s s torch float device type= cuda index= num_consumer_groups num_buffers_warp_spec epilogue_fn_hash identity tma_store False kwargs EVEN_K False USE_FAST_ACCUM False ACC_TYPE tl float BLOCK_M BLOCK_N BLOCK_K GROUP_M ALLOW_TF True hint_override None expected = expected replace cuda GPU_TYPE assertExpectedInline remove_white_space cache_key remove_white_space expected assertExpectedInline remove_white_space events remove_white_space def_kernel A B size A size B size A assertExpectedInline remove_white_space events remove_white_space def_kernel A B size A size B size A Test duck typing fresh_cache reset_counters compile_results = torch compile func_test dynamic=True b b eager_results = func_test b b assertEqual compile_results eager_results atol= rtol= assertEqual hits assertEqual misses Test loop test_func x _ range x = torch matmul x x x fresh_cache reset_counters input = torch rand device=GPU_TYPE compile_results = torch compile test_func dynamic=False input eager_results = test_func input assertEqual compile_results eager_results atol= rtol= assertEqual hits assertEqual misses fresh_cache reset_counters input = torch rand device=GPU_TYPE compile_results = torch compile test_func dynamic=True input eager_results = test_func input assertEqual compile_results eager_results atol= rtol= assertEqual hits assertEqual misses No cache hit due symbolic expressions passed i e mm s + s vs mm s reset_counters test_func x y z m l = torch matmul x y b = torch matmul torch cat x z torch cat y m l b fresh_cache = torch rand device=GPU_TYPE b = torch rand device=GPU_TYPE c = torch rand device=GPU_TYPE d = torch rand device=GPU_TYPE e = torch rand device=GPU_TYPE compile_results = torch compile test_func dynamic=True b c d e eager_results = test_func b c d e assertEqual compile_results eager_results atol= rtol= assertEqual hits assertEqual misses config patch max_autotune True test_configs max_mm_configs max_autotune_gemm_backends TRITON unittest skipIf config triton native_matmul only test template-based matmul test_triton_template_generated_code_caching_bmm func_test x y z m = torch bmm x y b = torch bmm z m b = torch rand device=GPU_TYPE b = torch rand device=GPU_TYPE hits torch _dynamo utils counters inductor generated_module_cache_hit misses torch _dynamo utils counters inductor generated_module_cache_miss Valid cache hit fresh_cache torch _dynamo utils counters clear compile_results = torch compile func_test dynamic=False b b eager_results = func_test b b assertEqual compile_results eager_results atol= rtol= assertEqual hits assertEqual misses config patch max_autotune True test_configs max_mm_configs max_autotune_gemm_backends ATEN TRITON unittest skipIf config triton native_matmul only test template-based matmul test_triton_template_generated_code_caching_mm_plus_mm func_test x y z m = torch mm x y b = torch mm z m sum = + b c = torch mm x y d = torch mm z m sum = c + d sum sum = torch rand device=GPU_TYPE b = torch rand device=GPU_TYPE hits torch _dynamo utils counters inductor generated_module_cache_hit misses torch _dynamo utils counters inductor generated_module_cache_miss Valid cache hit fresh_cache torch _dynamo utils counters clear compile_results = torch compile func_test dynamic=False b b eager_results = func_test b b assertEqual compile_results eager_results atol= rtol= assertEqual hits assertEqual misses fresh_cache skipIfXpu unittest skipIf TEST_WITH_ROCM decompose_k supported ROCm unittest skipIf config cpp_wrapper decompose_k supported cpp_wrapper yet unittest skipIf config triton native_matmul ignore decompose_k when native matmul codegen config patch max_autotune=True max_autotune_gemm_backends= TRITON autotune_fallback_to_aten=False parametrize num_decompose_k_splits parametrize decompose_k_threshold test_max_autotune_decompose_k_envvars num_decompose_k_splits decompose_k_threshold shapes = M N K shapes get_k_splits cache_clear use_decompose_k_choice cache_clear = torch randn M K dtype=torch float device=GPU_TYPE b = torch randn K N dtype=torch float device=GPU_TYPE config patch triton num_decompose_k_splits num_decompose_k_splits triton decompose_k_threshold decompose_k_threshold compiled_func = torch compile lambda b b _ code = run_and_get_code compiled_func b decompose_count = codegen code benchmark_decompose_k_mm codegen decompose_count += K M decompose_k_threshold K N decompose_k_threshold num_decompose_k_splits == assertEqual decompose_count assertTrue decompose_count assertTrue decompose_count = num_decompose_k_splits skipIfXpu unittest skipIf TEST_WITH_ROCM exhaustive currently only thoroughly tested NVIDIA unittest skipIf config triton native_matmul native matmul takes different tuning configs config patch max_autotune=True max_autotune_gemm_search_space= EXHAUSTIVE test_max_autotune_exhaustive f b b M N K = = torch randn M K dtype=torch float device=GPU_TYPE requires_grad=True b = torch randn K N dtype=torch float device=GPU_TYPE requires_grad=True mock patch torch _inductor template_heuristics registry get_template_heuristic config_mock config_heuristics = CUDAMMTemplateConfigHeuristic Traditionally would set all possible configs We mock out code path sake unit test config_heuristics exhaustive_configs = GemmConfig config_mock return_value = config_heuristics torch _dynamo utils counters compiled_func = torch compile f compiled_func b Only benchmarks choices aten exhaustive triton config Counter can InductorBenchmarker TritonBenchmarker counter counters inductor benchmark_gpu counter assertEqual counters inductor counter config patch max_autotune True max_autotune_gemm_backends TRITON test_mm_k_ mm x y x y i range torch _dynamo reset = torch randn i device=GPU_TYPE dtype=torch float b = torch randn i device=GPU_TYPE dtype=torch float compiled_f = torch compile mm out code = run_and_get_code compiled_f b torch testing assert_close out mm b atol= e- rtol= e- config patch max_autotune_gemm=True max_autotune_prune_choices_based_on_shared_mem=True test_max_autotune_prune_choices mm x y x y M K N = x = torch rand M K device=GPU_TYPE dtype=torch float y = torch rand K N device=GPU_TYPE dtype=torch float compiled_f = torch compile mm compiled_f x y assertEqual counters inductor select_algorithm_num_precompilation_exceptions parametrize op mm addmm bmm baddbmm mm_plus_mm parametrize max_autotune False True config patch test_configs max_mm_configs max_autotune_gemm_backends ATEN TRITON triton native_matmul False test_autotune_gemm_choice_validation op max_autotune generate_inputs_and_func op_name Base config just x w base_inputs = torch randn device=GPU_TYPE torch randn device=GPU_TYPE func = torch mm op_name == mm default pass op_name == addmm Add bias addmm base_inputs = torch randn device=GPU_TYPE + base_inputs func = torch addmm op_name bmm baddbmm Override batch dimensions base_inputs = torch randn device=GPU_TYPE base_inputs = torch randn device=GPU_TYPE func = torch bmm op_name == baddbmm Add batch bias base_inputs = torch torch randn device=GPU_TYPE + base_inputs func = torch baddbmm op_name == mm_plus_mm Add second matrix pair base_inputs += torch randn device=GPU_TYPE torch randn device=GPU_TYPE mmpmm x w x w torch mm x w + torch mm x w func = mmpmm raise ValueError f Unsupported op op_name base_inputs func choice_types_seen = set choice_validator choices choice choices choice_types_seen add type choice choices inputs fn = generate_inputs_and_func op add_preprocessing_fn choice_validator try config patch max_autotune max_autotune compiled_fn = torch compile fn dynamic=False compiled_fn inputs max_autotune assertIn ExternKernelCaller choice_types_seen assertIn TritonTemplateCaller choice_types_seen assertIn ExternKernelCaller choice_types_seen assertNotIn TritonTemplateCaller choice_types_seen finally clear_preprocessing_fns config patch test_configs max_mm_configs max_autotune_gemm_backends ATEN TRITON parametrize max_autotune_enabled True False test_autotune_layout_optimization max_autotune_enabled Test layouts flexible when every choice ExternKernelChoice we use proxy here bias_addmm max-autotune because enables us see multiple choices both scenarios bias_addmm addmm triton max-autotune only both bias_addmm addmm extern kernel choices layout_checker choices choices expected_layout = FixedLayout max_autotune_enabled FlexibleLayout choice choices assertIsInstance choice layout expected_layout f Expected expected_layout __name__ max_autotune= max_autotune_enabled choices add_preprocessing_fn layout_checker try bias = torch randn device=GPU_TYPE x = torch randn device=GPU_TYPE w = torch randn device=GPU_TYPE config patch max_autotune max_autotune_enabled compiled_fn = torch compile lambda b x w torch addmm b x w _ = compiled_fn bias x w finally clear_preprocessing_fns clear_defaults=False TestMaxAutotunePrecompile TestCase test_precompilation_threads threading typing Any unittest mock Mock patch FakeChoiceCaller ChoiceCaller __init__ - None super __init__ none Mock description= thread_id = None precompile thread_id = threading get_ident call_name - str None to_callable None hash_key - str str hash output_node - TensorBox noqa F None fake_choices = FakeChoiceCaller i range fake_lookup_result = dict fromkeys fake_choices no_lookup choices list ChoiceCaller op str inputs str benchmark Callable Any dict ChoiceCaller float hint_override Optional int = None - Optional dict ChoiceCaller float benchmark None benchmark choices asc = AlgorithmSelectorCache fake_benchmark_fn args kwargs fake_lookup_result main_thread_id = threading get_ident mock_debug_handler = Mock old_debug_handler = V debug try V set_debug_handler mock_debug_handler patch object asc lookup new=no_lookup patch object asc make_benchmark_fn return_value=fake_benchmark_fn config patch autotune_in_subproc False compile_threads len fake_choices asc test_call fake_choices Mock fake_choice fake_choices assert fake_choice thread_id None Expected all ChoiceCaller s precompile method have been called assert fake_choice thread_id = main_thread_id Expected all ChoiceCaller s precompile method have been called separate thread finally V set_debug_handler old_debug_handler test_filled_cache_precompile fn b c = b c b c = t torch float t b c b c fn_c = torch compile mode= max-autotune-no-cudagraphs fn inputs = torch rand device=GPU_TYPE _ range torch _dynamo utils counters assertEqual fn inputs fn_c inputs atol= e- rtol= e- torch _dynamo reset counters clear fn_c = torch compile mode= max-autotune-no-cudagraphs fn assertEqual counters inductor select_algorithm_precompile config patch autotune_local_cache=False autotune_remote_cache=False runOnRocmArch MI _ARCH unittest skipIf config triton native_matmul native matmul has counter test_precompilations fn b c = b c b c = t torch float t b c b c fn_c = torch compile mode= max-autotune-no-cudagraphs fn inputs = torch rand device=GPU_TYPE _ range torch testing assert_close fn_c inputs fn inputs atol= e- rtol= e- torch _dynamo utils counters assertEqual counters inductor select_algorithm_precompile instantiate_parametrized_tests TestMaxAutotuneSubproc TestCase _create_buffer name shape Buffer name=name layout=FixedLayout torch device f GPU_TYPE dtype=torch float size=shape XPU have support multiprocessing reduction torch multiprocessing reductions py skipIfXpu test_benchmark_choice_in_subproc gm = make_fx lambda torch zeros dummy graph construct GraphLowering graph = GraphLowering gm graph handler needed create benchmark example value below V set_graph_handler graph buf = _create_buffer mat buf = _create_buffer mat buf = _create_buffer mat buf = _create_buffer mat layout = FixedLayout torch device f GPU_TYPE torch float mat = AlgorithmSelectorCache benchmark_example_value buf mat = AlgorithmSelectorCache benchmark_example_value buf mat = AlgorithmSelectorCache benchmark_example_value buf mat = AlgorithmSelectorCache benchmark_example_value buf out = AlgorithmSelectorCache benchmark_example_value layout expected_out = mat mat + mat mat expected_out = None choice = aten_mm_plus_mm bind buf buf buf buf layout use tensor since mutation python list sub process synced back parent process timings = torch zeros dtype=torch float ctx = mp get_context spawn child = ctx Process target=benchmark_choice args= choice mat mat mat mat out expected_out timings child start child join assertEqual child exitcode print f timings timings out out expected_out expected_out XPU have support multiprocessing reduction torch multiprocessing reductions py skipIfXpu test_benchmark_choice_fail_in_subproc gm = make_fx lambda torch zeros dummy graph construct GraphLowering graph = GraphLowering gm graph handler needed create benchmark example value below V set_graph_handler graph buf = _create_buffer mat buf = _create_buffer mat buf = _create_buffer mat buf = _create_buffer mat layout = FixedLayout torch device f GPU_TYPE torch float mat = AlgorithmSelectorCache benchmark_example_value buf mat = AlgorithmSelectorCache benchmark_example_value buf mat = AlgorithmSelectorCache benchmark_example_value buf mat = AlgorithmSelectorCache benchmark_example_value buf out = AlgorithmSelectorCache benchmark_example_value layout expected_out = mat mat + mat mat choice = FailChoiceCaller fail_choice_caller None description= use tensor since python list synced back timings = torch zeros dtype=torch float ctx = mp get_context spawn child = ctx Process target=benchmark_choice args= choice mat mat mat mat out expected_out timings child start child join assertNotEqual child exitcode parametrize autotune_in_subproc True False parametrize autotune_multi_device True False test_max_autotune_mm_plus_mm autotune_in_subproc autotune_multi_device This crash previously due triton issue https github com triton-lang triton issues With autotuning subprocess we don t crash anymore m n k = mm_plus_mm b c d b + c d = torch randn m k GPU_TYPE b = torch randn k n GPU_TYPE c = torch randn m k GPU_TYPE d = torch randn k n GPU_TYPE config patch max_autotune True autotune_in_subproc autotune_in_subproc autotune_multi_device autotune_multi_device torch compile mm_plus_mm b c d parametrize dynamic False True test_max_autotune_regular_mm dynamic bool Make sure autotuning mm sub processes work without crashes mm b = torch sin b = torch randn GPU_TYPE b = torch randn GPU_TYPE config patch max_autotune True autotune_in_subproc True torch compile mm dynamic=dynamic b parametrize search_space DEFAULT EXHAUSTIVE parametrize dynamic False True test_max_autotune_addmm search_space dynamic=False Make sure autotuning addmm sub processes work without crashes torch backends cuda matmul allow_fp _reduced_precision_reduction = False addmm x b torch addmm x b x = torch randn GPU_TYPE = torch randn GPU_TYPE b = torch randn GPU_TYPE config patch max_autotune True autotune_in_subproc True max_autotune_gemm_search_space search_space Y_compiled = torch compile addmm dynamic=dynamic x b Y = addmm x b torch testing assert_close Y_compiled Y atol= e- rtol= e- test_triton_template_with_epilogues_and_dynamic_shape fn x torch Tensor w torch Tensor bias torch Tensor mul torch Tensor - torch Tensor torch nn functional relu torch matmul torch transpose x torch transpose w + bias mul M = M = K = N = w = torch rand N K GPU_TYPE half b = torch rand N GPU_TYPE half config patch max_autotune True autotune_in_subproc True max_autotune_gemm_backends Triton compiled_fn = torch compile fn fullgraph=True dynamic=True mode= max-autotune-no-cudagraphs x = torch rand K M GPU_TYPE half mul = torch rand M N GPU_TYPE half y = compiled_fn x w b mul y _expected = fn x w b mul torch testing assert_close y y _expected x = torch rand K M GPU_TYPE half mul = torch rand M N GPU_TYPE half y = compiled_fn x w b mul y _expected = fn x w b mul torch testing assert_close y y _expected instantiate_parametrized_tests TestMaxAutotuneRemoteCache TestCase setUp super setUp PatchCaches setUp tearDown super tearDown PatchCaches tearDown parametrize dynamic False True config patch compile_threads prologue_fusion False Worker processes do register PatchCaches properly test_max_autotune_remote_caching dynamic bool unittest mock patch mm b = torch sin b = torch randn GPU_TYPE b = torch randn GPU_TYPE Model torch nn Module forward x y x + y f x y Model x y x = torch randn GPU_TYPE y = torch randn GPU_TYPE config patch autotune_local_cache False autotune_remote_cache True patch dict os environ PatchCaches os environ pop TRITON_CACHE_MANAGER None config patch max_autotune True _ range fresh_cache torch compile mm dynamic=dynamic b reset torch compiler config patch cache_key_tag test fresh_cache torch compile mm dynamic=dynamic b reset global_stats report assertEqual global_stats autotune_remote Stats global_stats reset _ range fresh_cache torch compile f dynamic=dynamic x y reset torch compiler config patch cache_key_tag test fresh_cache torch compile mm dynamic=dynamic b reset global_stats report assertEqual global_stats autotune_remote Stats _TestTritonTemplateCaller TritonTemplateCaller __init__ bmreq _TestBenchmarkRequest bmreq = bmreq __str__ - str test TestTuningProcess TestCase check_healthy p TuningProcess device Optional int = None result = random random bmreq = _TestBenchmarkRequest result device=device p put bmreq benchmark assertEqual p get result test_tuning_subproc_timeout p = TuningProcess None bmreq = _TestBenchmarkRequest sleep= p put bmreq benchmark assertRaises TimeoutError p get timeout= Make sure TuningProcess still usable after timeout check_healthy p p shutdown test_tuning_subproc_exception p = TuningProcess None bmreq = _TestBenchmarkRequest exc=RuntimeError Fail p put bmreq benchmark assertRaises RuntimeError p get Make sure TuningProcess still usable after exception check_healthy p p shutdown test_tuning_subproc_crash p = TuningProcess None bmreq = _TestBenchmarkRequest crash=True p put bmreq benchmark assertRaises EOFError p get Make sure TuningProcess still usable after crash check_healthy p p shutdown test_tuning_subproc_killed p = TuningProcess None p kill check_healthy p p shutdown test_visible_devices device_list = TuningProcessPool get_device_list device device_list p = TuningProcess device check_healthy p device=device p shutdown TestTuningProcessPool TestCase Use only one device subprocess so we test process restarts usable after crash config patch autotune_multi_device False test_tuning_pool_crash tuning_pool = TuningProcessPool First force tuning process crash bmreq = _TestBenchmarkRequest crash=True choice = _TestTritonTemplateCaller bmreq timings = tuning_pool benchmark choice assertTrue choice timings assertEqual timings choice float inf Then send another request make sure sub-process has restarted operational bmreq = _TestBenchmarkRequest choice = _TestTritonTemplateCaller bmreq timings = tuning_pool benchmark choice assertTrue choice timings assertEqual timings choice bmreq result tuning_pool shutdown config patch autotune_multi_device False test_tuning_pool_timeout tuning_pool = TuningProcessPool First force tuning process timeout bmreq = _TestBenchmarkRequest sleep= choice = _TestTritonTemplateCaller bmreq config patch max_autotune_subproc_result_timeout_seconds timings = tuning_pool benchmark choice assertTrue choice timings assertEqual timings choice float inf Then send another request make sure sub-process has restarted operational bmreq = _TestBenchmarkRequest choice = _TestTritonTemplateCaller bmreq timings = tuning_pool benchmark choice assertTrue choice timings assertEqual timings choice bmreq result tuning_pool shutdown XPU have enable XPU_VISIBLE_DEVICES control devices visibility skipIfXpu config patch autotune_multi_device True test_tuning_pool_multiple_devices Adapt test available devices whether CUDA_VISIBLE_DEVICES already set environment use subset available devices ensure only subset visible sub-processes CUDA_VISIBLE_DEVICES os environ visible_devices = os environ CUDA_VISIBLE_DEVICES split visible_devices = str d d range torch cuda device_count cuda_visible_devices = join visible_devices - unittest mock patch dict os environ CUDA_VISIBLE_DEVICES cuda_visible_devices tuning_pool = TuningProcessPool choice = _TestTritonTemplateCaller _TestBenchmarkRequest choice = _TestTritonTemplateCaller _TestBenchmarkRequest timings = tuning_pool benchmark choice choice assertEqual timings choice choice bmreq result assertEqual timings choice choice bmreq result tuning_pool shutdown test_add_feedback_saver Test add_feedback_saver correctly adds feedback functions torch _inductor select_algorithm get_algorithm_selector_cache Clear any existing feedback savers clear_feedback_savers Create simple feedback saver function feedback_calls = simple_feedback_saver timings name input_nodes choices profiled_time feedback_calls append name name num_choices len choices num_timings len timings has_profiled_time profiled_time None Add feedback saver add_feedback_saver simple_feedback_saver Get global cache verify function added cache = get_algorithm_selector_cache assertEqual len cache feedback_saver_fns assertEqual cache feedback_saver_fns simple_feedback_saver Test we can add multiple feedback savers another_feedback_saver timings name input_nodes choices profiled_time pass add_feedback_saver another_feedback_saver assertEqual len cache feedback_saver_fns Clean up clear_feedback_savers test_clear_feedback_savers Test clear_feedback_savers removes all feedback functions torch _inductor select_algorithm get_algorithm_selector_cache Add some feedback savers first feedback_saver timings name input_nodes choices profiled_time pass feedback_saver timings name input_nodes choices profiled_time pass add_feedback_saver feedback_saver add_feedback_saver feedback_saver Verify they added cache = get_algorithm_selector_cache assertEqual len cache feedback_saver_fns Clear all feedback savers clear_feedback_savers Verify they cleared assertEqual len cache feedback_saver_fns test_feedback_saver_integration Test feedback savers actually called during autotuning Clear any existing feedback savers clear_feedback_savers feedback_calls = test_feedback_saver timings name input_nodes choices profiled_time Store information about call verification feedback_calls append name name num_choices len choices num_timings len timings input_node_count len input_nodes Add our test feedback saver add_feedback_saver test_feedback_saver Create simple matrix multiplication will trigger autotuning mm b b = torch randn device=GPU_TYPE b = torch randn device=GPU_TYPE config patch max_autotune True max_autotune_gemm_backends TRITON triton native_matmul False torch compile mm b Verify our feedback saver called assertGreater len feedback_calls Feedback saver should have been called Verify structure feedback call call = feedback_calls assertIn name call assertIn num_choices call assertIn num_timings call assertIn input_node_count call assertGreater call num_choices assertEqual call input_node_count Two input matrices Clean up clear_feedback_savers instantiate_parametrized_tests TestPrologueFusion TestCase classmethod setUpClass cls super setUpClass cls _stack = contextlib ExitStack cls _stack enter_context config patch max_autotune True prologue_fusion True benchmark_epilogue_fusion False shape_padding False max_autotune_gemm_backends TRITON test_configs max_mm_configs significantly speeds up tests check_code code_str num_kernels num_allocs num_deallocs FileCheck check get_func_call check_count get_kernel_launch num_kernels exactly=True run code_str num_allocs None FileCheck check get_func_call check_count empty_strided num_allocs exactly=True run code_str skip deallocation check when using cpp_wrapper most deallocations happen outside our control via RAIIAtenTensorHandle num_deallocs None config cpp_wrapper FileCheck check get_func_call check_count del num_deallocs exactly=True run code_str skipIfXpu msg= Triton issue exposed new driver will resolved after next triton update parametrize sizes test_upcast sizes M K N = sizes x = torch rand M K dtype=torch float device=GPU_TYPE y = torch rand K N dtype=torch float device=GPU_TYPE foo x y x y dtype y out code = run_and_get_code torch compile foo x y assertEqual out foo x y atol= rtol= check_code code num_kernels= num_allocs= num_deallocs= config triton native_matmul native matmul preserves zero mask - need optimize see codegen triton py FileCheck check = check tl where check tl dot run code upcast preserves zero mask FileCheck check = check_not tl where check tl dot run code unittest skip Triton bug compilation test_gather_fusion M K N = x = torch rand M K dtype=torch float device=GPU_TYPE y = torch rand K N dtype=torch float device=GPU_TYPE index = torch randperm M device=GPU_TYPE foo x y index x index y out code = run_and_get_code torch compile foo x y index assertEqual out foo x y index atol= rtol= check_code code num_kernels= num_allocs= num_deallocs= should done low precision FileCheck check k_idx check_not tl float check dot run code unittest skipIf TEST_WITH_ROCM FP supported ROCM unittest skipIf PLATFORM_SUPPORTS_FP FP only supported H + SM MI + devices config patch triton native_matmul False test_low_precision M = K = N = x = torch rand M K device=GPU_TYPE torch float _e m fn y = torch rand K N dtype=torch bfloat device=GPU_TYPE foo x y x y dtype y out code = run_and_get_code torch compile foo x y assertEqual out foo x y atol= rtol= check_code code num_kernels= num_allocs= num_deallocs= should done low precision no arithmetic FileCheck check k_idx check_not tl float check dot run code foo x y x y dtype + y out code = run_and_get_code torch compile foo x y assertEqual out foo x y atol= rtol= should done low precision two kernels check_code code num_kernels= num_allocs= num_deallocs= unittest skipIf config triton native_matmul generated code different native matmul test_downcast per heuristics dont fuse downcast into mm because would lead more reads inside kernel M K N = x = torch rand M K dtype=torch float device=GPU_TYPE y = torch rand K N dtype=torch float device=GPU_TYPE foo x y x y dtype y out code = run_and_get_code torch compile foo x y assertEqual out foo x y atol= rtol= check_code code num_kernels= num_allocs= num_deallocs= parametrize sizes unittest skipIf config triton native_matmul generated code different native matmul test_multiple_fusions sizes M K N = sizes foo x y x - y + x = torch rand M K dtype=torch float device=GPU_TYPE y = torch rand K N dtype=torch float device=GPU_TYPE out code = run_and_get_code torch compile foo x y assertEqual out foo x y atol= rtol= check_code code num_kernels= num_allocs= num_deallocs= check we do CSE any variables between prologues epilogues FileCheck check triton check_count = exactly=True check tl store run code config patch max_autotune_gemm_backends Triton benchmark_epilogue_fusion True max_epilogue_benchmarked_choices skipIfXpu msg= The fusion happened because do speedup XPU see issue test_pending_fusions_multiple multi_use x y x x T y y T x = torch rand device=GPU_TYPE y = torch rand device=GPU_TYPE out code = run_and_get_code torch compile multi_use x y FileCheck check get_func_call check_count get_kernel_launch exactly=True run code assertEqual out multi_use x y atol= rtol= resolve_pending x x x relu x = torch rand device=GPU_TYPE out code = run_and_get_code torch compile resolve_pending x FileCheck check get_func_call check_count get_kernel_launch exactly=True run code assertEqual out resolve_pending x atol= rtol= config patch max_autotune_gemm_backends Triton benchmark_epilogue_fusion True max_epilogue_benchmarked_choices skipIfXpu msg= The fusion happened because do speedup XPU see issue test_pending_fusion_pro_and_epi test_multiple_fusions x y = x torch float y y relu x = torch rand dtype=torch float device=GPU_TYPE out code = run_and_get_code torch compile test_multiple_fusions x FileCheck check get_func_call check_count get_kernel_launch exactly=True run code assertEqual out test_multiple_fusions x atol= rtol= skipIfXpu msg= Triton issue exposed new driver will resolved after next triton update parametrize sizes test_multiple_inputs sizes M K N = sizes foo x y z x + y torch float z x = torch rand M K dtype=torch float device=GPU_TYPE y = torch rand M K dtype=torch float device=GPU_TYPE z = torch rand K N dtype=torch float device=GPU_TYPE out_eager = foo x y z out code = run_and_get_code torch compile foo x y z assertEqual out out_eager atol= rtol= check_code code num_kernels= num_allocs= num_deallocs= test_storage_offset_prologue foo q = k = torch mm q + k - inp = torch randn device=GPU_TYPE out code = run_and_get_code torch compile foo inp assertEqual out foo inp atol= rtol= check_code code num_kernels= num_allocs= num_deallocs= config patch realize_reads_threshold= realize_opcount_threshold= parametrize sizes unittest skipIf config triton native_matmul generated code different native matmul test_prologue_multiple_nodes sizes M K N = sizes foo x y x - y x = torch rand M K dtype=torch float device=GPU_TYPE y = torch rand K N dtype=torch float device=GPU_TYPE out code = run_and_get_code torch compile foo x y assertEqual out foo x y atol= rtol= check_code code num_kernels= num_allocs= num_deallocs= parametrize K test_broadcast_x K foo x y x expand y shape + y x = torch rand dtype=torch float device=GPU_TYPE y = torch rand K dtype=torch float device=GPU_TYPE out code = run_and_get_code torch compile foo dynamic=True x y assertEqual out foo x y atol= rtol= check_code code num_kernels= num_allocs= num_deallocs= test_broadcast_y foo x y x y M = N = K = x = torch rand M K dtype=torch float device=GPU_TYPE y = torch rand K N dtype=torch float device=GPU_TYPE torch _dynamo mark_dynamic x out code = run_and_get_code torch compile foo dynamic=True x y assertEqual out foo x y atol= rtol= check_code code num_kernels= num_allocs= num_deallocs= unittest skipIf config triton native_matmul generated code different native matmul test_preserves_zero_analysis fns = lambda x x relu False preserves zero lambda x x + True does lambda x torch hypot x x True handled analysis conservatively assume does preserve foo x y fn fn x y fn should_mask fns x = torch rand dtype=torch float device=GPU_TYPE y = torch rand dtype=torch float device=GPU_TYPE out code = run_and_get_code torch compile foo x y fn assertEqual out foo x y fn atol= rtol= check_code code num_kernels= num_allocs= num_deallocs= should_mask f = FileCheck check k_idx check = check_same tl where f = FileCheck check k_idx check = check_not tl where f check tl dot run code config patch realize_reads_threshold= realize_opcount_threshold= parametrize benchmark_fusion True False test_prologue_read_into_both_inputs benchmark_fusion M = K = supported today could typically pointwise nodes would get inlined into separate nodes foo x y = x + y y - config patch benchmark_epilogue_fusion=benchmark_fusion x = torch rand M K dtype=torch float device=GPU_TYPE out code = run_and_get_code torch compile foo x assertEqual out foo x atol= rtol= guaranteed fuse still checking correctness benchmark_fusion check_code code num_kernels= num_allocs=None num_deallocs=None config patch realize_reads_threshold= realize_opcount_threshold= config patch allow_buffer_reuse=False unittest skipIf config triton native_matmul generated code different native matmul test_mismatched_prologue_group foo x y z = x + b = y b z x = torch rand device=GPU_TYPE y = torch rand device=GPU_TYPE z = torch rand device=GPU_TYPE out code = run_and_get_code torch compile foo x y z assertEqual out foo x y z atol= rtol= there s one more dealloc than there should because buffer reuse TODO sure why disabling buffer reuse doesn t stop check_code code num_kernels= num_allocs= num_deallocs= XPU have enabled pad_mm fx_passes so there always one kernel skipIfXpu config patch shape_padding=True config patch force_shape_pad=True parametrize sizes unittest skipIf config triton native_matmul generated code different native matmul test_prologue_masked_load sizes M K N = sizes foo x y x y x = torch rand device=GPU_TYPE y = torch rand device=GPU_TYPE we should attempt prologue fusion turns aligned load into unaligned load out code = run_and_get_code torch compile foo x y assertEqual out foo x y atol= rtol= check_code code num_kernels= num_allocs= num_deallocs= __name__ == __main__ torch _inductor utils is_big_gpu Set env make work CI HAS_GPU HAS_CPU is_big_gpu run_tests