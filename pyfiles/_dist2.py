This experimental new API PyTorch Distributed This actively development subject change deletion entirely This intended proving ground more flexible object oriented distributed APIs collections abc Generator contextlib contextmanager datetime timedelta typing Protocol Union torch torch _C _distributed_c d _current_process_group _set_process_group ProcessGroup ReduceOp Store torch distributed rendezvous rendezvous _BACKENDS dict str ProcessGroupFactory = __all__ = ProcessGroup ReduceOp ProcessGroupFactory register_backend new_group current_process_group process_group ProcessGroupFactory Protocol Protocol process group factories __call__ store Store rank int world_size int timeout timedelta device torch device kwargs object - ProcessGroup register_backend name str func ProcessGroupFactory - None Register new process group backend Args name The name backend func The function create process group name _BACKENDS raise ValueError f Backend name already registered _BACKENDS name = func _gloo_factory store Store rank int world_size int timeout timedelta device torch device kwargs object - ProcessGroup torch distributed ProcessGroupGloo assert len kwargs == Gloo backend received unexpected kwargs backend_class = ProcessGroupGloo store rank world_size timeout backend_class _set_sequence_number_for_group pg = ProcessGroup store rank world_size pg _set_default_backend ProcessGroup BackendType GLOO register devices pg _register_backend device ProcessGroup BackendType GLOO backend_class pg _register_backend torch device cpu ProcessGroup BackendType GLOO backend_class torch cuda is_available pg _register_backend torch device cuda ProcessGroup BackendType GLOO backend_class pg _nccl_factory store Store rank int world_size int timeout timedelta device torch device kwargs object - ProcessGroup torch distributed ProcessGroupNCCL opts = ProcessGroupNCCL Options opts _timeout = timeout k v kwargs items hasattr opts k raise KeyError f Unknown option k setattr opts k v backend_class = ProcessGroupNCCL store rank world_size opts backend_class _set_sequence_number_for_group backend_class eager_connect_single_device device pg = ProcessGroup store rank world_size pg _set_default_backend ProcessGroup BackendType NCCL pg _register_backend device ProcessGroup BackendType NCCL backend_class pg register_backend gloo _gloo_factory register_backend nccl _nccl_factory new_group backend str timeout timedelta device Union str torch device kwargs object - ProcessGroup Create new process group given backend options This group independent will globally registered thus usable via standard torch distributed APIs Args backend The backend use process group timeout The timeout collective operations device The device use process group kwargs All remaining arguments passed backend constructor See backend specific documentation details Returns A new process group backend _BACKENDS raise ValueError f Backend backend registered device = torch device device store rank world_size = next iter rendezvous env store set_timeout timeout _BACKENDS backend store rank world_size timeout device kwargs current_process_group - ProcessGroup Get current process group Thread local method Returns The current process group _current_process_group contextmanager process_group pg ProcessGroup - Generator None None None Context manager process groups Thread local method Args pg The process group use prev_pg = current_process_group _set_process_group pg try yield finally _set_process_group prev_pg