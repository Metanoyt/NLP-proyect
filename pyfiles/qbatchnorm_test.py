operator_benchmark op_bench torch Microbenchmarks quantized batchnorm operator batchnorm_configs_short = op_bench config_list attr_names= M N K attrs= cross_product_configs= device cpu dtype torch qint tags= short QBatchNormBenchmark op_bench TorchBenchmarkBase init M N K device dtype _init M N K device x_scale = x_zero_point = inputs = q_input_one torch quantize_per_tensor input_one scale=x_scale zero_point=x_zero_point dtype=dtype mean torch rand N var torch rand N weight torch rand N bias torch rand N eps e- Y_scale Y_zero_point _init M N K device pass forward pass QBatchNorm dBenchmark QBatchNormBenchmark _init M N K device set_module_name QBatchNorm d input_one = torch rand M N K device=device requires_grad=self auto_set forward q_input_one weight bias mean var eps float Y_scale float Y_zero_point int torch ops quantized batch_norm d q_input_one weight bias mean var eps Y_scale Y_zero_point QBatchNorm dBenchmark QBatchNormBenchmark _init M N K device set_module_name QBatchNorm d Note quantized implementation requires rank which why we add last dimension input_one = torch rand M N K device=device requires_grad=self auto_set forward q_input_one weight bias mean var eps float Y_scale float Y_zero_point int torch ops quantized batch_norm d q_input_one weight bias mean var eps Y_scale Y_zero_point op_bench generate_pt_test batchnorm_configs_short QBatchNorm dBenchmark op_bench generate_pt_test batchnorm_configs_short QBatchNorm dBenchmark __name__ == __main__ op_bench benchmark_runner main