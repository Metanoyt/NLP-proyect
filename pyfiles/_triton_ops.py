mypy allow-untyped-decorators mypy allow-untyped-defs math os weakref functools lru_cache typing Optional torch torch _dynamo utils warn_once torch utils _triton has_triton _triton_ops_meta get_meta TORCH_SPARSE_BSR_SCATTER_MM_LRU_CACHE_SIZE = int os getenv TORCH_SPARSE_BSR_SCATTER_MM_LRU_CACHE_SIZE check cond msg cond raise ValueError msg check_bsr_layout f_name t check t layout == torch sparse_bsr f f_name only BSR sparse format supported sparse argument check_device f_name t device check t device == device t device type == cuda f f_name all inputs expected same GPU device check_mm_compatible_shapes f_name lhs rhs check lhs dim = rhs dim = f f_name all inputs involved matrix product expected least D f got lhs dim == lhs dim rhs dim == rhs dim _m kl = lhs shape - kr _n = rhs shape - check kl == kr f f_name arguments sizes involved matrix product compatible matrix multiplication f got lhs shape - == kl which equal rhs shape - == kr check_dtype f_name t dtype additional_dtypes check t dtype == dtype t dtype torch half torch bfloat torch float + tuple additional_dtypes f f_name all inputs expected same dtype f one half bfloat float additional_dtypes f got dtype == t dtype check_blocksize f_name blocksize assert len blocksize == is_power_of_two v v v - is_compatible_blocksize b res = True blocksize b Triton loads only blocks which least powers res = blocksize = is_power_of_two blocksize res res check is_compatible_blocksize blocksize f f_name sparse inputs blocksize blocksize blocksize should least power each dimension make_triton_contiguous t Return input triton-contiguous tensor A triton-contiguous tensor defined tensor has strides minimal value smaller than equal While triton kernels support triton-non-contiguous tensors all strides being greater than arguments considerable slow-down occurs because tensor data copied element-wise rather than chunk-wise Zero strides assumed have defect min t stride TODO investigate contiguity along other axes than last one can beneficial performance t contiguous t broadcast_batch_dims f_name tensors try torch broadcast_shapes t shape - t tensors except Exception check False f f_name inputs batch dimensions broadcastable slicer dim slice_range tensors t tensors slices = slice None t dim slices dim = slice_range yield t slices multidim_slicer dims slices tensors t tensors s = slice None t dim d d_slice zip dims slices strict=False d None s d = d_slice yield t tuple s ptr_stride_extractor tensors t tensors yield t yield t stride grid_partitioner full_grid grid_blocks tensor_dims_map assert = len full_grid = assert = len grid_blocks = itertools generate_grid_points fg mg zip full_grid grid_blocks strict=False yield range fg mg generate_sliced_tensors slices t t_dims tensor_dims_map items yield next multidim_slicer t_dims slices t grid_point itertools product generate_grid_points grid = min fg - gp mg fg gp mg zip full_grid grid_point grid_blocks strict=False slices = slice gp gp + g gp g zip grid_point grid strict=False grid_points iterated contiguous order i e left dimensions traversed slower than right dimensions This order reversed CUDA grids yield grid - generate_sliced_tensors slices launch_kernel kernel tensor_dims_map full_grid grid_blocks=None cuda_max_grid = - - - cuda_max_grid = - grid_blocks None grid_blocks = cuda_max_grid valid_grid_dim g mg g None mg grid must least no greater than mg max min g mg grid_blocks = tuple valid_grid_dim g mg g mg zip grid_blocks cuda_max_grid strict=False type ignore assignment grid sliced_tensors grid_partitioner full_grid grid_blocks tensor_dims_map kernel grid sliced_tensors prepare_inputs bsr dense_tensors Introduce fake batch dimension present convenience crow_indices = bsr crow_indices unsqueeze col_indices = bsr col_indices unsqueeze values = make_triton_contiguous bsr values unsqueeze tensors = make_triton_contiguous t unsqueeze t dense_tensors Compute broadcasted batch dimension batch_dims_broadcasted = torch broadcast_shapes values shape - t shape - t tensors Broadcast batch dimensions squash The result can either view copy batch_broadcast_and_squash t batch_dims invariant_dims t broadcast_to batch_dims + invariant_dims flatten len batch_dims - crow_indices = batch_broadcast_and_squash crow_indices batch_dims_broadcasted - col_indices = batch_broadcast_and_squash col_indices batch_dims_broadcasted - values = batch_broadcast_and_squash values batch_dims_broadcasted values shape - tensors = batch_broadcast_and_squash t batch_dims_broadcasted t shape - t tensors crow_indices col_indices values tensors broadcast_batch_dims_bsr f_name bsr tensors batch_shape = broadcast_batch_dims f_name bsr tensors crow_indices = bsr crow_indices broadcast_to batch_shape + - col_indices = bsr col_indices broadcast_to batch_shape + - values = bsr values broadcast_to batch_shape + bsr values shape - size = batch_shape + bsr shape - torch sparse_compressed_tensor crow_indices col_indices values size=size layout=bsr layout NOTE function will ALWAYS create view tile_to_blocksize t blocksize rest m n = t shape new_shape = rest + m blocksize blocksize n blocksize blocksize using view instead reshape ensure result indeed view t view new_shape transpose - - Dbatch tensor Return tensor D tensor either prepending new dimensions tensor shape when ` ` tensor ndim ` ` collapsing starting dimensions into first dimension when ` ` tensor ndim ` ` while tensor ndim tensor = tensor unsqueeze tensor ndim tensor = tensor flatten tensor ndim - assert tensor ndim == tensor shape tensor scatter_mm blocks others indices_data accumulators=None Scattered matrix multiplication tensors A scattered matrix multiplication defined series matrix multiplications applied input tensors according input output mappings specified indices data The following indices data formats supported defining scattered matrix multiplication operation attr ` indices_data ` holds name indices data format specified below - ` ` scatter_mm ` ` - matrix multiplications scattered batches tensors If attr ` blocks ` math ` \times M \times K tensor attr ` others ` math ` \times K \times N ` tensor attr ` accumulators ` math ` \times M \times N ` tensor attr ` indices = indices_data indices ` math ` \times ` tensor then operation equivalent following code c_offsets pq = indices_data r range len c_offsets - g range c_offsets r c_offsets r + p q = pq g accumulators r += blocks p others q - ` ` bsr_strided_mm ` ` - matrix multiplications scattered batches tensors tensor If attr ` blocks ` math ` Ms \times Ks tensor attr ` others ` math ` \times K \times N ` tensor attr ` accumulators ` math ` \times M \times N ` tensor then operation equivalent following code c_indices r_offsets p_offsets q_offsets meta = indices_data b range nbatches i r enumerate r_offsets r r = divmod r N acc = accumulators b r r + Ms r r + Ns g range c_indices i c_indices i + p = p_offsets g q q = divmod q_offsets g N acc += blocks p others b q q + Ks q q + Ns where ` ` Ns = N meta SPLIT_N ` ` ` ` M ` ` ` ` K ` ` integer multiples ` ` Ms ` ` ` ` Ks ` ` respectively - ` ` bsr_strided_mm_compressed ` ` - matrix multiplications scattered batches tensors tensor A memory processor efficient version ` ` bsr_strided_mm ` ` format If attr ` blocks ` math ` Ms \times Ks tensor attr ` others ` math ` \times K \times N ` tensor attr ` accumulators ` math ` \times M \times N ` tensor then operation equivalent following code c_indices r_offsets q_offsets meta = indices_data b range nbatches r r_offsets m = r N Ms n = r N Ns r r = divmod r N c c = c_indices m c_indices m + acc = accumulators b r r + Ms r r + Ns i p enumerate range c c q = q_offsets n c + SPLIT_N - n c + i q q = divmod q N acc += blocks p others b q q + Ks q q + Ns where ` ` Ns = N meta SPLIT_N ` ` ` ` M ` ` ` ` K ` ` integer multiples ` ` Ms ` ` ` ` Ks ` ` respectively Notice order ` ` r_offsets ` ` items can arbitrary property enables defining swizzle operators via rearrangements ` ` r_offsets ` ` items Auxiliary functions provided pre-computing attr ` indices_data ` For example func ` bsr_scatter_mm_indices_data ` used define indices data matrix multiplication BSR strided tensors Parameters ---------- blocks Tensor -D tensor first matrices multiplied others Tensor tensor second matrices multiplied If ` ` indices_data == scatter_mm ` ` tensor -D batch tensor second input matrices multiplied Otherwise second input matrices slices attr ` others ` tensor indices_data tuple format data defines inputs outputs scattered matrix multiplications Keyword arguments ----------------- accumulators Tensor optional tensor matrix product accumulators If ` ` indices_data == scatter_mm ` ` tensor -D batch tensor output matrices Otherwise output matrices slices attr ` accumulators ` tensor indices_format = indices_data assert blocks ndim == _P Ms Ks = blocks shape indices_format == scatter_mm c_offsets pq = indices_data assert others ndim == _Q Ks_ Ns = others shape assert Ks == Ks_ accumulators None R = c_offsets shape - accumulators = torch zeros R Ms Ns dtype=blocks dtype device=blocks device R Ms_ Ns_ = accumulators shape assert Ms_ == Ms assert Ns_ == Ns Ms Ks Ns _scatter_mm None r range c_offsets shape - g = c_offsets r g = c_offsets r + g range g g p q = pq g accumulators r += blocks p others q _scatter_mm blocks others c_offsets pq accumulators accumulators indices_format == bsr_strided_mm others_shape = others shape others = Dbatch others B K N = others shape assert K Ks == c_indices r_offsets p_offsets q_offsets meta = indices_data SPLIT_N = meta SPLIT_N accumulators None M = Ms + r_offsets max item + N accumulators = torch zeros others_shape - M N dtype=blocks dtype device=blocks device M N_ = accumulators shape - assert N_ == N accumulators_shape = accumulators shape accumulators = Dbatch accumulators Ns = N SPLIT_N Ms Ks Ns _scatter_mm None accumulators zero_ b range B r range r_offsets shape r_ = r_offsets r item g = c_indices r item g = c_indices r + item r r = divmod r_ N acc = accumulators b r r + Ms r r + Ns g range g g p q = p_offsets g q_offsets g q q = divmod q item N acc += blocks p others b q q + Ks q q + Ns _scatter_mm blocks others c_indices r_offsets p_offsets q_offsets meta accumulators accumulators view accumulators_shape indices_format == bsr_strided_mm_compressed others_shape = others shape others = Dbatch others B K N = others shape assert K Ks == c_indices r_offsets q_offsets meta = indices_data SPLIT_N = meta SPLIT_N accumulators None M = Ms + r_offsets max item + N accumulators = torch zeros others_shape - M N dtype=blocks dtype device=blocks device M N_ = accumulators shape - assert N_ == N accumulators_shape = accumulators shape accumulators = Dbatch accumulators Ns = N SPLIT_N Ms Ks Ns _scatter_mm None b range B j range len r_offsets r r = divmod r_offsets j item N m = r Ms n = r Ns c = c_indices m item c = c_indices m + item acc = accumulators b r r + Ms r r + Ns i p enumerate range c c q = q_offsets n c + SPLIT_N - n c + i item q q = divmod q N acc += blocks p others b q q + Ks q q + Ns p_offsets = torch empty dtype=q_offsets dtype device=q_offsets device _scatter_mm blocks others c_indices r_offsets p_offsets q_offsets meta accumulators accumulators view accumulators_shape raise NotImplementedError indices_format scatter_mm_meta M K N Ms Ks GROUP_SIZE=None TILE_M=None TILE_N=None SPLIT_N=None num_warps=None num_stages=None extra TILE_M TILE_N SPLIT_N num_warps num_stages GROUP_SIZE == None device_name = torch cuda get_device_name meta = get_meta scatter_mm M K N Ms Ks device_name version= torch float meta None meta update extra meta The following parameters optimized performance equilibrium points bsr-dense dense-dense matrix multiplications when using GPU card NVIDIA GeForce RTX SUPER For points far performance equilibrium points well other GPU cards optimal parameters likely different what specified below M K N == Ms Ks == SPLIT_N = TILE_M = TILE_N = GROUP_SIZE = num_stages = num_warps = noqa E E E Ms Ks == SPLIT_N = TILE_M = TILE_N = GROUP_SIZE = num_stages = num_warps = noqa E E E Ms Ks == SPLIT_N = TILE_M = TILE_N = GROUP_SIZE = num_stages = num_warps = noqa E E E Ms Ks == SPLIT_N = TILE_M = TILE_N = GROUP_SIZE = num_stages = num_warps = noqa E E E M K N == Ms Ks == SPLIT_N = TILE_M = TILE_N = GROUP_SIZE = num_stages = num_warps = noqa E E E Ms Ks == SPLIT_N = TILE_M = TILE_N = GROUP_SIZE = num_stages = num_warps = noqa E E E Ms Ks == SPLIT_N = TILE_M = TILE_N = GROUP_SIZE = num_stages = num_warps = noqa E E E Ms Ks == SPLIT_N = TILE_M = TILE_N = GROUP_SIZE = num_stages = num_warps = noqa E E E M K N == Ms Ks == SPLIT_N = TILE_M = TILE_N = GROUP_SIZE = num_stages = num_warps = noqa E E E Ms Ks == SPLIT_N = TILE_M = TILE_N = GROUP_SIZE = num_stages = num_warps = noqa E E E Ms Ks == SPLIT_N = TILE_M = TILE_N = GROUP_SIZE = num_stages = num_warps = noqa E E E Ms Ks == SPLIT_N = TILE_M = TILE_N = GROUP_SIZE = num_stages = num_warps = noqa E E E Ms Ks == SPLIT_N = TILE_M = TILE_N = GROUP_SIZE = num_stages = num_warps = noqa E E E M K N == Ms Ks == SPLIT_N = TILE_M = TILE_N = GROUP_SIZE = num_stages = num_warps = noqa E E E Ms Ks == SPLIT_N = TILE_M = TILE_N = GROUP_SIZE = num_stages = num_warps = noqa E E E Ms Ks == SPLIT_N = TILE_M = TILE_N = GROUP_SIZE = num_stages = num_warps = noqa E E E Ms Ks == SPLIT_N = TILE_M = TILE_N = GROUP_SIZE = num_stages = num_warps = noqa E E E Ms Ks == SPLIT_N = TILE_M = TILE_N = GROUP_SIZE = num_stages = num_warps = noqa E E E M K N == Ms Ks == SPLIT_N = TILE_M = TILE_N = GROUP_SIZE = num_stages = num_warps = noqa E E E Ms Ks == SPLIT_N = TILE_M = TILE_N = GROUP_SIZE = num_stages = num_warps = noqa E E E Ms Ks == SPLIT_N = TILE_M = TILE_N = GROUP_SIZE = num_stages = num_warps = noqa E E E SPLIT_N None Assume NVIDIA GeForce RTX SUPER With probality when N performance will worse more than performance when using optimal value Otherwise when N = using following heuristics may give upto lower performance SPLIT_N = get N Ms = N = SPLIT_N = Ns = N SPLIT_N TILE_M None TILE_M = min Ns Ms TILE_N None TILE_N = min Ns Ns num_stages = num_stages num_warps None min M N num_warps = get Ms min M N == num_warps = get Ms min M N == num_warps = get Ms num_warps = get Ms GROUP_SIZE = GROUP_SIZE assert TILE_M = Ms dict TILE_M=TILE_M Ms=Ms assert TILE_N = Ns dict TILE_N=TILE_N Ns=Ns assert Ms = M dict M=M Ms=Ms assert Ns = N dict N=N Ns=Ns assert Ks = K dict K=K Ks=Ks dict TILE_M=TILE_M TILE_N=TILE_N GROUP_SIZE=GROUP_SIZE num_stages=num_stages num_warps=num_warps SPLIT_N=SPLIT_N extra bsr_dense_addmm_meta M K N Ms Ks beta alpha SPLIT_N=None GROUP_SIZE_ROW=None num_warps=None num_stages=None sparsity=None dtype=None out_dtype=None _version= extra Specifying _version useful situations when one wants discard existing triton kernel tuning results say testing bsr_dense_addmm_meta functionality dtype None dtype = torch float out_dtype None out_dtype = dtype sparsity None sparsity = SPLIT_N num_warps num_stages GROUP_SIZE_ROW == None device_name = torch cuda get_device_name key = M K N Ms Ks beta == beta == alpha == dtype out_dtype version_dtype = dtype version_dtype = dtype out_dtype meta = get_meta bsr_dense_addmm key device_name version= _version version_dtype sparsity meta None sparsity = meta = get_meta bsr_dense_addmm key device_name version= _version version_dtype meta None dtype out_dtype meta = get_meta bsr_dense_addmm key device_name version= _version dtype meta None find approximate meta such N SPLIT_N == matching_meta = get_meta bsr_dense_addmm key key device_name version= _version version_dtype matching_meta None dtype out_dtype matching_meta = get_meta bsr_dense_addmm key key device_name version= _version dtype mkey sorted matching_meta meta_ = matching_meta mkey n = mkey split_n = meta_ SPLIT_N c = n split_n N c == n = N meta = dict meta_ meta SPLIT_N = N c meta None meta update extra meta see Computing optimal kernel parameters _triton_ops_meta py ways avoid warning message warn_once bsr_dense_addmm uses non-optimal triton kernel parameters f M= K= N= Ms= Ks= beta= alpha= dtype= out_dtype= SPLIT_N = SPLIT_N max N Ms GROUP_SIZE_ROW = GROUP_SIZE_ROW num_stages = num_stages num_warps = num_warps dict SPLIT_N=SPLIT_N GROUP_SIZE_ROW=GROUP_SIZE_ROW num_stages=num_stages num_warps=num_warps extra TensorAsKey A light-weight wrapper tensor enables storing tensors keys efficient memory reference based comparison approximation data equality based keys Motivation hash value torch tensor tensor instance based does use data equality makes usage tensors keys less useful For instance result ` ` len crow_indices crow_indices ` ` ` ` although tensor results ` crow_indices ` method call equal fact these share same data storage On other hand efficient caching tensors we want avoid calling torch equal compares tensors item-wise TensorAsKey offers compromise guarantees key equality tensors references data same storage same manner without accessing underlying data However approach does always guarantee correctness For instance complex tensor ` ` x ` ` we have ` ` TensorAsKey x == TensorAsKey x conj ` ` while ` ` torch equal x x conj ` ` would False __init__ obj get_tensor_key obj Warning TensorAsKey does track negative nor conjugate bits its input object because use case wrapping compressed plain indices compressed sparse tensors always integer tensors non-negative items these bits never set However when extending use TensorAsKey float complex tensors values these bits see is_neg is_conj methods must included key well assert obj dtype is_floating_point obj dtype is_complex obj dtype obj data_ptr obj storage_offset obj shape obj stride obj dtype _obj_ref = weakref ref obj obj layout torch strided key = get_tensor_key obj obj layout torch sparse_csr torch sparse_bsr key = get_tensor_key obj crow_indices get_tensor_key obj col_indices obj layout torch sparse_csc torch sparse_bsc key = get_tensor_key obj ccol_indices get_tensor_key obj row_indices raise NotImplementedError obj layout _hash = hash key __hash__ _hash __eq__ other isinstance other TensorAsKey False obj None other obj None dead objects always compare unequal unless these same objects other key == other key property obj Return object alive otherwise None _obj_ref lru_cache maxsize=TORCH_SPARSE_BSR_SCATTER_MM_LRU_CACHE_SIZE _bsr_scatter_mm_indices_data indices_format M K N Ms Ks nbatches SPLIT_N compressed_sparse_tensor_as_key bsr = compressed_sparse_tensor_as_key obj assert bsr None crow_indices col_indices = bsr crow_indices bsr col_indices device = crow_indices device indices_dtype = torch int indices_format == bsr_strided_mm_compressed Ns = N SPLIT_N q_offsets_lst = b = torch arange SPLIT_N dtype=indices_dtype device=device Ns m range M Ms r = crow_indices m item r = crow_indices m + item r == r continue q_offsets_lst append col_indices r r Ks N repeat SPLIT_N + b repeat_interleave r - r q_offsets = torch cat q_offsets_lst crow_indices_diff = crow_indices diff non_zero_row_indices = crow_indices_diff nonzero = non_zero_row_indices Ms N r_offsets = + b view - c_indices = crow_indices swizzle operation mm elements longer sums computed first nnz_per_row = crow_indices_diff non_zero_row_indices repeat_interleave SPLIT_N nnz_per_row indices = nnz_per_row sort descending=True stable=True r_offsets = r_offsets indices indices_format c_indices r_offsets q_offsets indices_format == bsr_strided_mm Ns = N SPLIT_N p_offsets_lst = q_offsets_lst = b = torch arange SPLIT_N dtype=indices_dtype device=device Ns m range M Ms r = crow_indices m item r = crow_indices m + item r == r continue p_offsets_lst append torch arange r r dtype=indices_dtype device=device repeat SPLIT_N q_offsets_lst append col_indices r r Ks N repeat SPLIT_N + b repeat_interleave r - r q_offsets = torch cat q_offsets_lst crow_indices_diff = crow_indices diff non_zero_row_indices = crow_indices_diff nonzero = non_zero_row_indices Ms N r_offsets = + b view - c_indices = torch cat crow_indices torch cumsum crow_indices_diff non_zero_row_indices repeat_interleave SPLIT_N p_offsets = torch cat p_offsets_lst indices_format c_indices r_offsets p_offsets q_offsets indices_format == scatter_mm Ns = Ms c_indices = pq_offsets = todo eliminate inner for-loops efficiency b range nbatches m range M Ms r = crow_indices m item r = crow_indices m + item n range N Ns c_indices append c_indices - + r - r t range r - r p = r + t q = col_indices p item + b K Ks N Ns + n pq_offsets append p q indices_format torch tensor c_indices dtype=indices_dtype device=device torch tensor pq_offsets dtype=indices_dtype device=device raise ValueError f Invalid indices_format= Expected bsr_strided_mm_compressed &#124; bsr_strided_mm &#124; scatter_mm bsr_scatter_mm_indices_data bsr other indices_format= bsr_strided_mm_compressed meta_input Computes indices data func ` scatter_mm ` used BSR strided tensor matrix multiplication assert bsr dense_dim == assert bsr ndim == no batch dims blocksize = bsr values shape - M K = bsr shape Ms Ks = blocksize K_ N = other shape - assert K_ == K nbatches = other shape - numel meta = scatter_mm_meta M K N Ms Ks meta_input allow_tf meta_input meta update allow_tf =bsr dtype torch float torch bfloat SPLIT_N = meta SPLIT_N indices_data = _bsr_scatter_mm_indices_data indices_format M K N Ms Ks nbatches SPLIT_N TensorAsKey bsr indices_format == bsr_strided_mm_compressed meta update is_compressed=True indices_data + meta indices_format == bsr_strided_mm meta update is_compressed=False indices_data + meta indices_data bsr_scatter_mm bsr other indices_data=None out=None BSR strided - strided assert bsr ndim == assert other ndim = Ms Ks Ns = bsr shape - bsr shape - other shape - blocksize = bsr values shape - indices_data None indices_data = bsr_scatter_mm_indices_data bsr other indices_format= bsr_strided_mm_compressed indices_format = indices_data out None out = torch empty other shape - Ms Ns dtype=bsr dtype device=bsr device out_shape = out shape out = Dbatch out bsr _nnz == out zero_ indices_format bsr_strided_mm_compressed bsr_strided_mm out zero_ scatter_mm bsr values other indices_data accumulators=out indices_format == scatter_mm nbatches = other shape - numel accumulators = torch zeros nbatches Ms blocksize Ns blocksize blocksize blocksize dtype=bsr dtype device=bsr device others = Dbatch other transpose - - view nbatches Ns blocksize blocksize Ks blocksize blocksize movedim equivalent transpose - - transpose - - transpose - - flatten scatter_mm bsr values others indices_data accumulators=accumulators out copy_ accumulators unflatten nbatches Ms blocksize Ns blocksize movedim equivalent transpose - - transpose - - transpose - - reshape nbatches Ns Ms transpose - - raise NotImplementedError indices_format out view out_shape _int_bsr_dense_addmm input torch Tensor bsr torch Tensor dense torch Tensor beta= alpha= left_alpha Optional torch Tensor = None right_alpha Optional torch Tensor = None out Optional torch Tensor = None skip_checks bool = False max_grid Optional tuple Optional int Optional int Optional int = None meta Optional dict = None out None dense dtype torch int f_name = _int_bsr_dense_addmm crow_indices = bsr crow_indices batch_ndim = crow_indices dim - M = bsr shape batch_ndim N = dense shape - original_batch_dims_broadcasted = broadcast_batch_dims f_name bsr dense out = torch empty original_batch_dims_broadcasted + M N dtype=torch int device=dense device bsr_dense_addmm input bsr dense beta=beta alpha=alpha left_alpha=left_alpha right_alpha=right_alpha out=out skip_checks=skip_checks max_grid=max_grid meta=meta bsr_dense_addmm input torch Tensor bsr torch Tensor dense torch Tensor beta= alpha= left_alpha Optional torch Tensor = None right_alpha Optional torch Tensor = None out Optional torch Tensor = None skip_checks bool = False max_grid Optional tuple Optional int Optional int Optional int = None meta Optional dict = None Compute out = beta input + left_alpha reshape - alpha bsr dense right_alpha reshape - where left_alpha right_alpha + -D tensors when specified otherwise these treated tensors filled ones f_name = bsr_dense_addmm values = bsr values crow_indices = bsr crow_indices col_indices = bsr col_indices batch_ndim = crow_indices dim - M K = bsr shape batch_ndim batch_ndim + blocksize = values shape batch_ndim + batch_ndim + N = dense shape - todo implement checks original_batch_dims_broadcasted = broadcast_batch_dims f_name bsr dense out None out = dense new_empty original_batch_dims_broadcasted + M N bsr _nnz == alpha == N == M == K == beta == out zero_ out copy_ input beta = out mul_ beta out left_alpha_is_one = False right_alpha_is_one = False left_alpha None left_alpha_is_one = True left_alpha = dense new_empty expand original_batch_dims_broadcasted M N referenced left_alpha = left_alpha view original_batch_dims_broadcasted M expand original_batch_dims_broadcasted M N right_alpha None right_alpha_is_one = True right_alpha = dense new_empty expand original_batch_dims_broadcasted M N referenced right_alpha = right_alpha view original_batch_dims_broadcasted N expand original_batch_dims_broadcasted M N assert left_alpha stride - == assert right_alpha stride - == meta None sparsity = round - bsr _nnz blocksize blocksize M K meta = bsr_dense_addmm_meta M K N blocksize blocksize beta alpha sparsity=sparsity dtype=dense dtype out_dtype=out dtype out_backup = out crow_indices col_indices values input dense left_alpha right_alpha out = prepare_inputs bsr input dense left_alpha right_alpha out BM BK = blocksize SPLIT_N = meta get SPLIT_N N BM BN = N SPLIT_N out_untiled = out out = tile_to_blocksize out BM BN dense = tile_to_blocksize dense BK BN input = tile_to_blocksize input BM BN left_alpha = tile_to_blocksize left_alpha BM BN right_alpha = tile_to_blocksize right_alpha BM BN tl dot supports float float int accumulator types dot_out_dtype = torch float tl float torch bfloat tl float torch float tl float torch float tl float torch int tl int torch int tl int out dtype n_batches = dense size n_block_rows = crow_indices size - - n_block_cols = dense size - full_grid = n_batches n_block_cols n_block_rows max_grid None grid_blocks = tuple max_grid - + None - len max_grid grid_blocks = None tensor_dims_map = values None None crow_indices None - col_indices None None input - - dense - None left_alpha - - right_alpha - - out - - assert alpha = kernel grid sliced_tensors pyrefly ignore unsupported-operation _bsr_strided_addmm_kernel grid ptr_stride_extractor sliced_tensors beta alpha beta_is_one=beta == beta_is_nonzero=beta = alpha_is_one=alpha == left_alpha_is_one=left_alpha_is_one right_alpha_is_one=right_alpha_is_one BLOCKSIZE_ROW=BM BLOCKSIZE_INNER=BK BLOCKSIZE_COL=BN allow_tf =dot_out_dtype == tl float acc_dtype=dot_out_dtype meta launch_kernel kernel tensor_dims_map full_grid grid_blocks out data_ptr = out_backup data_ptr prepare_inputs has made copy out copy its content back out_backup out_backup copy_ out_untiled view out_backup shape out_backup has_triton triton triton language tl triton jit _sampled_addmm_kernel alpha beta IS_BETA_ZERO tl constexpr BLOCKSIZE_ROW tl constexpr BLOCKSIZE_COL tl constexpr k TILE_K tl constexpr values_ptr values_batch_stride values_nnz_stride values_row_block_stride values_col_block_stride crow_indices_ptr crow_indices_batch_stride crow_indices_stride col_indices_ptr col_indices_batch_stride col_indices_stride mat _ptr mat _batch_stride mat _tiled_row_stride mat _tiled_col_stride mat _row_block_stride mat _col_block_stride mat _ptr mat _batch_stride mat _tiled_row_stride mat _tiled_col_stride mat _row_block_stride mat _col_block_stride acc_dtype tl constexpr allow_tf tl constexpr batch_pid = tl program_id axis= row_block_pid = tl program_id axis= crow_indices_offset_ptr = crow_indices_ptr + crow_indices_batch_stride batch_pid + crow_indices_stride row_block_pid nnz_offset = tl load crow_indices_offset_ptr nnz_offset_next = tl load crow_indices_offset_ptr + crow_indices_stride Compute nnz row number row_block_pid If zero skip row row_nnz = nnz_offset_next - nnz_offset row_nnz == row_block_arange = tl arange BLOCKSIZE_ROW col_block_arange = tl arange BLOCKSIZE_COL Pointers set first block current row values_block_ptrs = values_ptr + values_batch_stride batch_pid + values_nnz_stride nnz_offset + values_row_block_stride row_block_arange None + values_col_block_stride col_block_arange None col_index_nnz_ptr = col_indices_ptr + col_indices_batch_stride batch_pid + col_indices_stride nnz_offset Advance mat current tiled row ignore columns mat _block_ptrs = mat _ptr + mat _batch_stride batch_pid + mat _tiled_row_stride row_block_pid + mat _row_block_stride row_block_arange None Advance mat batch block col dimension mat _block_ptrs = mat _ptr + mat _batch_stride batch_pid + mat _col_block_stride col_block_arange None k_tile_arange = tl arange TILE_K _ range row_nnz acc_block = tl zeros BLOCKSIZE_ROW BLOCKSIZE_COL dtype=acc_dtype find column block index col_block = tl load col_index_nnz_ptr k_tile range k TILE_K k_offsets = k_tile + k_tile_arange mask_k = k_offsets k mat _block = tl load mat _block_ptrs + mat _col_block_stride k_offsets None pyrefly ignore index-error mask=mask_k None other= mat _block = tl load mat _block_ptrs + mat _tiled_col_stride col_block + mat _row_block_stride k_offsets None pyrefly ignore index-error mask=mask_k None other= acc_block += tl dot mat _block mat _block allow_tf =allow_tf out_dtype=acc_dtype IS_BETA_ZERO acc_block = alpha acc_block = alpha acc_block + beta tl load values_block_ptrs write result tl store values_block_ptrs acc_block values_ptr dtype element_ty advance val col_index ptrs next block row values_block_ptrs += values_nnz_stride col_index_nnz_ptr += col_indices_stride triton jit _bsr_strided_dense_rowspace_kernel values prologue values_ptr values_batch_stride values_nnz_stride values_row_block_stride values_col_block_stride values epilogue crow_indices prologue crow_indices_ptr crow_indices_batch_stride crow_indices_stride crow_indices epilogue col_indices prologue col_indices_ptr col_indices_batch_stride col_indices_stride col_indices epilogue dense prologue dense_ptr dense_batch_stride dense_tiled_row_stride dense_tiled_col_stride dense_row_block_stride dense_col_block_stride dense epilogue output prologue output_ptr output_batch_stride output_tiled_row_stride output_tiled_col_stride output_row_block_stride output_col_block_stride output epilogue gh- Always keep all constexpr arguments end triton kernel arguments list because triton earlier non-contiguous outputs will corrupt CUDA state due triton bug fixed openai triton# BLOCKSIZE_ROW tl constexpr BLOCKSIZE_COL tl constexpr acc_dtype tl constexpr allow_tf tl constexpr GROUP_SIZE_ROW tl constexpr batch_pid = tl program_id axis= row_block_pid = tl program_id axis= col_block_pid = tl program_id axis= n_block_rows = tl num_programs axis= n_block_cols = tl num_programs axis= row_block_pid col_block_pid = tl swizzle d row_block_pid col_block_pid n_block_rows n_block_cols GROUP_SIZE_ROW crow_indices_offset_ptr = crow_indices_ptr + crow_indices_batch_stride batch_pid + crow_indices_stride row_block_pid nnz_offset = tl load crow_indices_offset_ptr nnz_offset_next = tl load crow_indices_offset_ptr + crow_indices_stride Compute nnz row number row_block_pid If zero skip row row_nnz = nnz_offset_next - nnz_offset row_nnz == row_block_arange = tl arange BLOCKSIZE_ROW col_block_arange = tl arange BLOCKSIZE_COL Pointers set first block current row values_block_ptrs = values_ptr + values_batch_stride batch_pid + values_nnz_stride nnz_offset + values_row_block_stride row_block_arange None + values_col_block_stride col_block_arange None NOTE dense advanced into all dimensions tiled row one That will advanced loop according values col_indices dense_block_ptrs = dense_ptr + dense_batch_stride batch_pid + dense_tiled_col_stride col_block_pid + dense_row_block_stride col_block_arange None + dense_col_block_stride row_block_arange None Pointers set exact write-to locations output_ptrs = output_ptr + output_batch_stride batch_pid + output_tiled_row_stride row_block_pid + output_tiled_col_stride col_block_pid + output_row_block_stride row_block_arange None + output_col_block_stride row_block_arange None Set pointer first nonzero element current row col_index_nnz_ptr = col_indices_ptr + col_indices_batch_stride batch_pid + col_indices_stride nnz_offset output_acc_block = tl zeros BLOCKSIZE_ROW BLOCKSIZE_COL dtype=acc_dtype _ range row_nnz values_block = tl load values_block_ptrs find which row dense needs get loaded multiplication values_block dense_row_idx = tl load col_index_nnz_ptr dense_block = tl load dense_block_ptrs + dense_tiled_row_stride dense_row_idx do block mm output_acc_block += tl dot values_block dense_block allow_tf =allow_tf out_dtype=acc_dtype move val col_index ptrs next block row values_block_ptrs += values_nnz_stride col_index_nnz_ptr += col_indices_stride write back result tl store output_ptrs output_acc_block output_ptr dtype element_ty _run_sampled_addmm_kernel alpha beta is_beta_zero blocksize k tile_k values crow_indices col_indices mat mat max_grid n_batches = values size n_block_rows = crow_indices size - - full_grid = n_batches n_block_rows max_grid None grid_blocks = tuple max_grid - + None - len max_grid grid_blocks = None tensor_dims_map = values None crow_indices - col_indices None mat - mat None values dtype torch half torch bfloat acc_dtype = tl float allow_tf = True acc_dtype = tl float allow_tf = False kernel grid sliced_tensors _sampled_addmm_kernel grid alpha beta is_beta_zero blocksize k tile_k ptr_stride_extractor sliced_tensors acc_dtype=acc_dtype allow_tf =allow_tf num_stages= num_warps= launch_kernel kernel tensor_dims_map full_grid grid_blocks sampled_addmm input torch Tensor mat torch Tensor mat torch Tensor beta= alpha= out Optional torch Tensor = None skip_checks bool = False max_grid Optional tuple Optional int Optional int Optional int = None f_name = sampled_addmm check_bsr_layout f_name input input_broadcasted = broadcast_batch_dims_bsr f_name input mat mat skip_checks check_device f_name mat input device check_device f_name mat input device beta = input dtype torch bool check False f f_name having beta == beta equal boolean mask allowed input dtype torch bool check_dtype f_name mat input dtype check_dtype f_name mat input dtype check_dtype f_name mat mat dtype check_mm_compatible_shapes f_name mat mat out None check_bsr_layout f_name out check_device f_name out mat device check_dtype f_name out input dtype check out shape == input_broadcasted shape out _nnz == input _nnz f f_name Expects ` out ` shape input_broadcasted shape f nnz equal input_broadcasted _nnz f got out shape = out shape out nnz = out _nnz out None out = input_broadcasted mat dtype copy=True out copy_ input_broadcasted out numel == out _nnz == out blocksize = out values shape - k = mat size - NOTE m n == zeros m n alpha == k == out values mul_ beta out prepare inputs reshaping them kernel-compatible out_backup = out crow_indices col_indices values mat mat = prepare_inputs out mat mat mat = tile_to_blocksize mat blocksize k mat = tile_to_blocksize mat k blocksize tile_k = max blocksize _run_sampled_addmm_kernel alpha beta beta == blocksize k tile_k values crow_indices col_indices mat mat max_grid If nnz x block strides same out_backup values values means out_backup values values views each other so we have copy out_backup values stride - = values stride - out_backup values copy_ values reshape out_backup values shape out_backup bsr_dense_mm bsr torch Tensor dense torch Tensor out Optional torch Tensor = None skip_checks bool = False max_grid Optional tuple Optional int Optional int Optional int = None meta Optional dict = None f_name = bsr_dense_mm m _kl = bsr shape - skip_checks check_bsr_layout f_name bsr check_device f_name bsr dense device check_dtype f_name bsr dense dtype torch int check_mm_compatible_shapes f_name bsr dense n = dense size - row_block col_block = bsr values shape - check_blocksize f_name row_block col_block check n f f_name dense size - == n should divisible _kr n = dense shape - original_batch_dims_broadcasted = broadcast_batch_dims f_name bsr dense out None skip_checks expected_out_shape = original_batch_dims_broadcasted + m n check out shape == expected_out_shape bsr_dense_mm ` out ` argument has wrong shape f expected expected_out_shape got out shape check out is_contiguous out transpose - - is_contiguous bsr_dense_mm only row-major col-major ` out ` arguments supported i e out is_contiguous out transpose - - is_contiguous should True Allocate out out None out = dense new_empty original_batch_dims_broadcasted + m n Short circuit lhs zero bsr _nnz == out zero_ beta== addmm ignores input content so we can use out placeholder input because their shapes match bsr_dense_addmm out bsr dense alpha= beta= out=out triton jit _bsr_softmax_kernel crow_indices_ptr crow_indices_batch_stride crow_indices_stride values_ptr values_batch_stride values_row_block_stride values_nnz_col_block_stride row_block col_block MAX_ROW_NNZ tl constexpr TILE tl constexpr batch_pid = tl program_id axis= row_block_offset_pid = tl program_id axis= row_block_pid = tl program_id axis= crow_indices_offset_ptr = crow_indices_ptr + crow_indices_batch_stride batch_pid + crow_indices_stride row_block_pid nnz_offset = tl load crow_indices_offset_ptr nnz_offset_next = tl load crow_indices_offset_ptr + crow_indices_stride Compute nnz row number row_block_pid If zero skip row row_nnz = nnz_offset_next - nnz_offset row_nnz == row_arange = tl arange TILE mask = row_arange row_nnz col_block curr_row_values_ptrs = values_ptr + values_batch_stride batch_pid + values_row_block_stride row_block_offset_pid + nnz_offset col_block find max row row_tile = tl load curr_row_values_ptrs + row_arange mask=mask other=-float inf tl float max_row_value = tl max row_tile axis= _ range TILE MAX_ROW_NNZ TILE row_arange += TILE mask = row_arange row_nnz col_block row_tile = tl load curr_row_values_ptrs + row_arange mask=mask other=-float inf tl float curr_max_row_value = tl max row_tile axis= max_row_value = tl where max_row_value curr_max_row_value max_row_value curr_max_row_value find denominator stable softmax num = tl exp row_tile - max_row_value denom = tl sum num axis= _ range TILE MAX_ROW_NNZ TILE row_arange -= TILE mask = row_arange row_nnz col_block row_tile = tl load curr_row_values_ptrs + row_arange mask=mask other=-float inf tl float num = tl exp row_tile - max_row_value denom += tl sum num axis= populate output tl store curr_row_values_ptrs + row_arange num denom values_ptr dtype element_ty mask=mask _ range TILE MAX_ROW_NNZ TILE row_arange += TILE mask = row_arange row_nnz col_block row_tile = tl load curr_row_values_ptrs + row_arange mask=mask other=-float inf tl float num = tl exp row_tile - max_row_value tl store curr_row_values_ptrs + row_arange num denom values_ptr dtype element_ty mask=mask bsr_softmax input max_row_nnz=None f_name = bsr_softmax check_bsr_layout f_name input check_dtype f_name input input dtype input _nnz == input numel == input clone m n = input shape - nnz = input _nnz row_block col_block = input values shape - max_row_nnz None max_row_nnz = triton next_power_of_ n max_row_nnz = triton next_power_of_ max_row_nnz crow_indices = input crow_indices unsqueeze flatten - reshape values b bn nnz row_block col_block b bn row_block nnz col_block This simplifies batch dim manipulation unlocks possibility access all nnzs any given row input values transpose - - is_contiguous Need clone avoid ` contiguous ` returning view values = input values clone values = input values values = values transpose - - contiguous unsqueeze flatten - reshape - row_block nnz col_block full_grid = values shape row_block m row_block grid_blocks = None tensor_dims_map = We span nnz number blocks nnz + hence crow_indices - crow_indices - None - values None None kernel grid sliced_tensors _bsr_softmax_kernel grid ptr_stride_extractor sliced_tensors row_block col_block max_row_nnz Triton s max numel bounded min max_row_nnz launch_kernel kernel tensor_dims_map full_grid grid_blocks values = values reshape - row_block nnz col_block transpose - - reshape input values shape torch sparse_compressed_tensor input crow_indices clone input col_indices clone values size=input shape layout=input layout _scaled_dot_product_attention query torch Tensor key torch Tensor value torch Tensor attn_mask Optional torch Tensor dropout_p float = is_causal bool = False scale Optional float = None f_name = _scaled_dot_product_attention check is_causal f f_name is_causal == True supported check attn_mask None f f_name attn_mask == None supported assert attn_mask None check attn_mask layout == torch sparse_bsr f f_name f attn_mask layout must torch sparse_bsr got f attn_mask layout == attn_mask layout check_device f_name key query device check_device f_name value query device check_device f_name attn_mask query device check_dtype f_name key query dtype check_dtype f_name value query dtype attn_mask dtype torch bool check_dtype f_name attn_mask query dtype pyrefly ignore not-callable sdpa = sampled_addmm attn_mask query key transpose - - beta= skip_checks=False scale None query size - == scale == check False f f_name current value scale == scale results division zero scale_factor = math sqrt query size - scale None scale sdpa values mul_ scale_factor pyrefly ignore not-callable sdpa = bsr_softmax sdpa torch nn functional dropout sdpa values p=dropout_p inplace=True pyrefly ignore not-callable sdpa = bsr_dense_mm sdpa value sdpa triton jit _scatter_mm _kernel M tl constexpr K tl constexpr N tl constexpr blocks_ptr blocks_stride_P blocks_stride_M blocks_stride_K others_ptr others_stride_Q others_stride_K others_stride_N accumulators_ptr accumulators_stride_R accumulators_stride_M accumulators_stride_N pq_offsets_ptr pq_offsets_stride pq_ptr pq_stride_T pq_stride_ dot_out_dtype tl constexpr TILE_M tl constexpr TILE_N tl constexpr allow_tf tl constexpr Ms = M TILE_M pid_t = tl program_id axis= pid = tl program_id axis= pid_m = pid Ms pid_n = pid Ms rm = pid_m TILE_M + tl arange TILE_M rn = pid_n TILE_N + tl arange TILE_N rk = tl arange K A_ptr = blocks_ptr + rm None blocks_stride_M + rk None blocks_stride_K B_ptr = others_ptr + rk None others_stride_K + rn None others_stride_N g = tl load pq_offsets_ptr + pid_t pq_offsets_stride g = tl load pq_offsets_ptr + pid_t + pq_offsets_stride g == g acc_block = tl zeros TILE_M TILE_N dtype=dot_out_dtype i range g g p = tl load pq_ptr + i pq_stride_T q = tl load pq_ptr + i pq_stride_T + pq_stride_ A = tl load A_ptr + p blocks_stride_P B = tl load B_ptr + q others_stride_Q acc_block += tl dot A B out_dtype=dot_out_dtype allow_tf =allow_tf C_ptr = accumulators_ptr + pid_t accumulators_stride_R + rm None accumulators_stride_M + rn None accumulators_stride_N tl store C_ptr acc_block accumulators_ptr dtype element_ty _scatter_mm blocks torch Tensor others torch Tensor pq_offsets torch Tensor pq_indices torch Tensor accumulators torch Tensor _P M K = blocks shape _Q _ N = others shape meta = dict TILE_M=max M TILE_N=max N num_stages= num_warps= grid META pq_offsets shape - triton cdiv M META TILE_M triton cdiv N META TILE_N dot_out_dtype = torch float tl float torch bfloat tl float torch float tl float torch float tl float accumulators dtype allow_tf meta meta update allow_tf =dot_out_dtype == tl float _scatter_mm _kernel grid M K N blocks blocks stride blocks stride blocks stride others others stride others stride others stride accumulators accumulators stride accumulators stride accumulators stride pq_offsets pq_offsets stride pq_indices pq_indices stride pq_indices stride dot_out_dtype=dot_out_dtype meta triton jit _scatter_mm _kernel nbatches Ms Ks tl constexpr N blocks_ptr blocks_stride_P blocks_stride_M blocks_stride_K others_ptr others_stride_B others_stride_K others_stride_N accumulators_ptr accumulators_stride_B accumulators_stride_M accumulators_stride_N c_indices_ptr r_offsets_ptr p_offsets_ptr q_offsets_ptr is_compressed tl constexpr dot_out_dtype tl constexpr SPLIT_N tl constexpr TILE_M tl constexpr TILE_N tl constexpr GROUP_SIZE tl constexpr allow_tf tl constexpr Ns = N SPLIT_N BLOCKS_M = Ms TILE_M BLOCKS_N = Ns TILE_N pid_t_ = tl program_id axis= pid = tl program_id axis= pid_b = pid_t_ nbatches pid_t = pid_t_ nbatches num_pid_in_group = GROUP_SIZE BLOCKS_N group_id = pid num_pid_in_group first_pid_m = group_id GROUP_SIZE group_size_m = min BLOCKS_M - first_pid_m GROUP_SIZE pid_m = first_pid_m + pid group_size_m pid_n = pid num_pid_in_group group_size_m rm = pid_m TILE_M + tl arange TILE_M rn = pid_n TILE_N + tl arange TILE_N rk = tl arange Ks A_ptr = blocks_ptr + rm None blocks_stride_M + rk None blocks_stride_K B_ptr = others_ptr + pid_b others_stride_B + rk None others_stride_K + rn None others_stride_N When is_compressed True r only variable depends pid_t This property allows sorting r values before calling kernel The sorting r equivalent defining swizzle operator outside kernel r = tl load r_offsets_ptr + pid_t is_compressed m = r N Ms n = r N Ns r = tl load c_indices_ptr + m r = tl load c_indices_ptr + m + g = n r + SPLIT_N - n r nnz = r - r g = tl load c_indices_ptr + pid_t g = tl load c_indices_ptr + pid_t + nnz = g - g q_ptr = q_offsets_ptr + g acc_block = tl zeros TILE_M TILE_N dtype=dot_out_dtype is_compressed A_ptr += r blocks_stride_P type ignore possibly-undefined _ range nnz q = tl load q_ptr B = tl load B_ptr + q A = tl load A_ptr acc_block += tl dot A B out_dtype=dot_out_dtype allow_tf =allow_tf A_ptr += blocks_stride_P q_ptr += p_ptr = p_offsets_ptr + g _ range nnz q = tl load q_ptr B = tl load B_ptr + q p = tl load p_ptr A = tl load A_ptr + p blocks_stride_P p_ptr += q_ptr += acc_block += tl dot A B out_dtype=dot_out_dtype allow_tf =allow_tf C_ptr = accumulators_ptr + r + pid_b accumulators_stride_B + rm None accumulators_stride_M + rn None accumulators_stride_N tl store C_ptr acc_block accumulators_ptr dtype element_ty _scatter_mm blocks torch Tensor others torch Tensor c_indices torch Tensor r_offsets torch Tensor p_offsets torch Tensor q_offsets torch Tensor meta dict accumulators torch Tensor force_contiguous bool = True SPLIT_N = meta SPLIT_N _P Ms Ks = blocks shape B _K N = others shape B_ _M N_ = accumulators shape assert N_ == N Ns = N SPLIT_N assert B_ == B grid META r_offsets shape B triton cdiv Ms META TILE_M triton cdiv Ns META TILE_N dot_out_dtype = torch float tl float torch bfloat tl float torch float tl float torch float tl float accumulators dtype allow_tf meta meta update allow_tf =dot_out_dtype == tl float assert c_indices stride == assert r_offsets stride == assert p_offsets stride == assert q_offsets stride == Re non-contiguous tensor arguments Sometimes triton kernel launches may fail RuntimeError Triton Error CUDA illegal memory access encountered appears case when size non-contiguous tensor argument larger than certain threshold Could related shared memory L cache size GPU card In anycase ensuring tensor arguments contiguous seems avoid above exception So following we ll always convert tensor arguments C-contiguous tensors force_contiguous blocks = blocks contiguous others = others contiguous accumulators is_contiguous accumulators_ = accumulators contiguous accumulators_ = accumulators accumulators_ = accumulators _scatter_mm _kernel grid B Ms Ks N blocks blocks stride blocks stride blocks stride others others stride others stride others stride accumulators_ accumulators_ stride accumulators_ stride accumulators_ stride c_indices r_offsets p_offsets q_offsets dot_out_dtype=dot_out_dtype meta force_contiguous accumulators is_contiguous accumulators copy_ accumulators_ triton jit _bsr_strided_addmm_kernel values prologue values_ptr values_batch_stride values_nnz_stride values_row_block_stride values_col_block_stride values epilogue crow_indices prologue crow_indices_ptr crow_indices_batch_stride crow_indices_stride crow_indices epilogue col_indices prologue col_indices_ptr col_indices_batch_stride col_indices_stride col_indices epilogue input prologue input_ptr input_batch_stride input_tiled_row_stride input_tiled_col_stride input_row_block_stride input_col_block_stride input epilogue dense prologue dense_ptr dense_batch_stride dense_tiled_row_stride dense_tiled_col_stride dense_row_block_stride dense_col_block_stride dense epilogue left_alpha prologue left_alpha_ptr left_alpha_batch_stride left_alpha_tiled_row_stride left_alpha_tiled_col_stride tl constexpr left_alpha_row_block_stride left_alpha_col_block_stride tl constexpr left_alpha epilogue right_alpha prologue right_alpha_ptr right_alpha_batch_stride right_alpha_tiled_row_stride tl constexpr right_alpha_tiled_col_stride right_alpha_row_block_stride tl constexpr right_alpha_col_block_stride right_alpha epilogue output prologue output_ptr output_batch_stride output_tiled_row_stride output_tiled_col_stride output_row_block_stride output_col_block_stride output epilogue beta alpha beta_is_one tl constexpr beta_is_nonzero tl constexpr alpha_is_one tl constexpr left_alpha_is_one tl constexpr right_alpha_is_one tl constexpr BLOCKSIZE_ROW tl constexpr BLOCKSIZE_COL tl constexpr BLOCKSIZE_INNER tl constexpr acc_dtype tl constexpr allow_tf tl constexpr GROUP_SIZE_ROW tl constexpr SPLIT_N tl constexpr left right_alpha tensors originally + -dimensional assert left_alpha_tiled_col_stride == assert left_alpha_col_block_stride == assert right_alpha_tiled_row_stride == assert right_alpha_row_block_stride == batch_pid = tl program_id axis= row_block_pid = tl program_id axis= col_block_pid = tl program_id axis= n_block_rows = tl num_programs axis= n_block_cols = tl num_programs axis= row_block_pid col_block_pid = tl swizzle d row_block_pid col_block_pid n_block_rows n_block_cols GROUP_SIZE_ROW crow_indices_offset_ptr = crow_indices_ptr + crow_indices_batch_stride batch_pid + crow_indices_stride row_block_pid nnz_offset = tl load crow_indices_offset_ptr nnz_offset_next = tl load crow_indices_offset_ptr + crow_indices_stride Compute nnz row number row_block_pid row_nnz = nnz_offset_next - nnz_offset row_block_arange = tl arange BLOCKSIZE_ROW inner_block_arange = tl arange BLOCKSIZE_INNER col_block_arange = tl arange BLOCKSIZE_COL Pointers set first block current row values_block_ptrs = values_ptr + values_batch_stride batch_pid + values_nnz_stride nnz_offset + values_row_block_stride row_block_arange None + values_col_block_stride inner_block_arange None NOTE dense advanced into all dimensions tiled row one That will advanced loop according values col_indices dense_block_ptrs = dense_ptr + dense_batch_stride batch_pid + dense_tiled_col_stride col_block_pid + dense_row_block_stride inner_block_arange None + dense_col_block_stride col_block_arange None Pointers set exact write-to locations output_ptrs = output_ptr + output_batch_stride batch_pid + output_tiled_row_stride row_block_pid + output_tiled_col_stride col_block_pid + output_row_block_stride row_block_arange None + output_col_block_stride col_block_arange None Set pointer first nonzero element current row col_index_nnz_ptr = col_indices_ptr + col_indices_batch_stride batch_pid + col_indices_stride nnz_offset output_acc_block = tl zeros BLOCKSIZE_ROW BLOCKSIZE_COL dtype=acc_dtype _ range row_nnz values_block = tl load values_block_ptrs find which row dense needs get loaded multiplication values_block dense_row_idx = tl load col_index_nnz_ptr dense_block = tl load dense_block_ptrs + dense_tiled_row_stride dense_row_idx do block mm output_acc_block += tl dot values_block dense_block allow_tf =allow_tf out_dtype=acc_dtype move val col_index ptrs next block row values_block_ptrs += values_nnz_stride col_index_nnz_ptr += col_indices_stride alpha_is_one output_acc_block = alpha left_alpha_is_one left_alpha_ptrs = left_alpha_ptr + left_alpha_batch_stride batch_pid + left_alpha_tiled_row_stride row_block_pid + left_alpha_tiled_col_stride col_block_pid + left_alpha_row_block_stride row_block_arange None + left_alpha_col_block_stride col_block_arange None output_acc_block = tl load left_alpha_ptrs right_alpha_is_one right_alpha_ptrs = right_alpha_ptr + right_alpha_batch_stride batch_pid + right_alpha_tiled_row_stride row_block_pid + right_alpha_tiled_col_stride col_block_pid + right_alpha_row_block_stride row_block_arange None + right_alpha_col_block_stride col_block_arange None output_acc_block = tl load right_alpha_ptrs beta_is_nonzero input_ptrs = input_ptr + input_batch_stride batch_pid + input_tiled_row_stride row_block_pid + input_tiled_col_stride col_block_pid + input_row_block_stride row_block_arange None + input_col_block_stride col_block_arange None beta_is_one output_acc_block += tl load input_ptrs output_acc_block += beta tl load input_ptrs write back result tl store output_ptrs output_acc_block output_ptr dtype element_ty bsr_softmax = None type ignore assignment bsr_dense_mm = None type ignore assignment sampled_addmm = None type ignore assignment _scaled_dot_product_attention = None type ignore assignment _scatter_mm = None type ignore assignment _scatter_mm = None type ignore assignment _bsr_strided_addmm_kernel = None type ignore assignment