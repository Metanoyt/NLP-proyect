Owner s oncall export flake noqa copy unittest re escape typing Any List Optional torch torch _dynamo torchdynamo torch _higher_order_ops torchbind enable_torchbind_tracing torch export export FlatArgsAdapter unflatten torch export unflatten _disable_interpreter torch testing _internal common_utils IS_WINDOWS run_tests skipIfTorchDynamo TestCase torch testing _internal torchbind_impls init_torchbind_implementations torch utils _pytree TreeSpec unittest skipIf torchdynamo is_dynamo_supported dynamo isn t support TestUnflatten TestCase compare_outputs eager unflattened args orig_output = eager args unflattened_output = unflattened args assertTrue torch allclose orig_output unflattened_output test_unflatten_nested NestedChild torch nn Module forward x x x Child torch nn Module __init__ - None super __init__ nested = NestedChild register_parameter child param torch nn Parameter torch ones forward x x = nested x x + child param Child torch nn Module __init__ - None super __init__ child buffer = torch nn Buffer torch ones forward x x - child buffer MyModule torch nn Module __init__ - None super __init__ foo = Child bar = Child register_parameter rootparam torch nn Parameter torch ones forward x x = x rootparam x = foo x x = bar x x orig_eager = MyModule export_module = export orig_eager torch rand strict=True unflattened = unflatten export_module inputs = torch rand Compare root modules all submodules compare_outputs orig_eager unflattened inputs compare_outputs orig_eager foo unflattened foo inputs compare_outputs orig_eager bar unflattened bar inputs compare_outputs orig_eager foo nested unflattened foo nested inputs Check state dicts equal orig_state_dict = orig_eager state_dict exported_state_dict = unflattened state_dict name value orig_state_dict items assertTrue torch allclose value exported_state_dict name test_unflatten_buffer_mutation Child torch nn Module __init__ - None super __init__ child buffer = torch nn Buffer torch ones forward x child buffer add_ x x - child buffer MyModule torch nn Module __init__ - None super __init__ foo = Child register_parameter rootparam torch nn Parameter torch ones forward x x = foo x x rootparam eager_module = MyModule export_module = export eager_module torch rand strict=True unflattened_module = unflatten export_module Buffer should look same before after one run eager_buffer = eager_module foo child buffer unflattened_buffer = unflattened_module foo child buffer assertTrue torch allclose eager_buffer unflattened_buffer inputs = torch rand eager_module inputs unflattened_module inputs assertTrue torch allclose eager_buffer unflattened_buffer test_unflatten_nested_access Child torch nn Module __init__ - None super __init__ child buffer = torch nn Buffer torch ones forward x x - child buffer MyModule torch nn Module __init__ - None super __init__ foo = Child register_parameter rootparam torch nn Parameter torch ones forward x x = x + foo child buffer x = foo x x eager_module = MyModule export_module = export eager_module torch rand strict=True unflattened_module = unflatten export_module inputs = torch rand compare_outputs eager_module unflattened_module inputs test_unflatten_shared_submodule Shared torch nn Module __init__ - None super __init__ layernorm = torch nn LayerNorm sub_net = torch nn Sequential layernorm torch nn ReLU layernorm torch nn ReLU forward x sub_net x eager_module = Shared inps = torch rand export_module = export eager_module inps strict=True unflattened_module = unflatten export_module compare_outputs eager_module unflattened_module inps assertTrue hasattr unflattened_module sub_net i range len eager_module sub_net assertTrue hasattr unflattened_module sub_net str i assertEqual id getattr unflattened_module sub_net id getattr unflattened_module sub_net test_assert_tensor_metadata_stack N torch nn Module __init__ super __init__ = torch randn forward x y x = x dtype=torch int y = y dtype=torch int x = x + x + y M torch nn Module __init__ super __init__ n = N forward x y x = x x y = y y n x y m = M ep = torch export export m torch randn torch randn node ep graph nodes node target == torch ops aten _assert_tensor_metadata default assertEqual len node meta get nn_module_stack uep = torch export unflatten ep inp = torch randn torch randn assertTrue torch allclose uep inp m inp unittest skipIf IS_WINDOWS Windows supported test skipIfTorchDynamo Non strict mode meant run dynamo test_unflatten_preserve_signature NestedChild torch nn Module forward zx y x y key + zx w y key zx Child torch nn Module __init__ - None super __init__ nested = NestedChild forward x y z = torch ones_like x xw = nested z x y= key y xw w + z - xw x Child torch nn Module __init__ - None super __init__ forward x x - MyModule torch nn Module __init__ - None super __init__ foo = Child bar = Child forward x y x = foo x y x = bar x x orig_eager = MyModule inps = torch rand torch rand strict True False export_module = export orig_eager inps preserve_module_call_signature= foo nested strict=strict unflattened = unflatten export_module compare_outputs export_module module unflattened inps unflattened foo nested = NestedChild compare_outputs export_module module unflattened inps Test tree spec mismatched input orig_outs = export_module module inps new_inps = inps torch rand assertRaisesRegex TypeError There no flat args adapter specified Are you sure you calling right arguments unflattened new_inps With flat args adapter KeepTwoFlatArgsAdapter FlatArgsAdapter adapt target_spec TreeSpec input_spec TreeSpec input_args List Any metadata dict str Any obj Optional Any = None - List Any while len input_args input_args pop - input_args unflattened = unflatten export_module KeepTwoFlatArgsAdapter new_outs = unflattened new_inps assertTrue torch allclose orig_outs new_outs test_unflatten_param_list_dict Mod torch nn Module __init__ - None super __init__ param_list = torch nn ParameterList param_dict = torch nn ParameterDict i range param_list append torch nn Parameter torch randn param_dict f key_ i = torch nn Parameter torch randn forward x i range x = x + param_list i x = x + param_dict f key_ i x export_module = torch export export Mod torch randn strict=True unflattened = unflatten export_module compare_outputs export_module module unflattened torch randn unittest skipIf IS_WINDOWS Windows supported test test_unflatten_preserve_with_unused_input M torch nn Module forward x b x + b M torch nn Module __init__ - None super __init__ m = M forward x y b = torch topk y m x b ep = torch export export M torch randn torch randn preserve_module_call_signature= m strict=False ep graph eliminate_dead_code unflattened = unflatten ep compare_outputs ep module unflattened torch randn torch randn test_unflatten_wrong_input Mod torch nn Module __init__ - None super __init__ param_list = torch nn ParameterList param_dict = torch nn ParameterDict i range param_list append torch nn Parameter torch randn param_dict f key_ i = torch nn Parameter torch randn forward x = x sum i range = + param_list i sum = + param_dict f key_ i sum export_module = torch export export Mod torch randn strict=True assertRaisesRegex AssertionError escape Guard failed x size == expected got export_module module torch randn unflattened = unflatten export_module assertRaisesRegex RuntimeError escape Expected input args shape equal got unflattened torch randn unittest skipIf IS_WINDOWS Windows supported test test_unflatten_with_inplace_compile NestedChild torch nn Module forward x x x Child torch nn Module __init__ - None super __init__ nested = NestedChild register_parameter child param torch nn Parameter torch ones forward x x = nested x x + child param Child torch nn Module __init__ - None super __init__ child buffer = torch nn Buffer torch ones forward x x - child buffer MyModule torch nn Module __init__ - None super __init__ foo = Child bar = Child register_parameter rootparam torch nn Parameter torch ones forward x x = x rootparam x = foo x x = bar x x orig_eager = MyModule export_module = torch export export orig_eager torch rand strict=True unflattened = unflatten export_module in-place compilation should work Pass fullgraph ensure no graph breaks torch _dynamo backends debugging ExplainWithBackend eb = ExplainWithBackend inductor unflattened foo compile backend=eb fullgraph=True inputs = torch randn compare_outputs orig_eager unflattened inputs assertEqual len eb graphs unflattened compile compare_outputs orig_eager unflattened inputs test_fx_trace MyModule torch nn Module __init__ - None super __init__ forward x y x = x + x x = x + y foo x orig_eager = MyModule inputs = torch rand torch rand foo torch rand export_module = export orig_eager inputs strict=True unflattened = unflatten export_module torch fx symbolic_trace unflattened concrete_args= torch fx PH torch fx PH torch fx PH test_double_nested_submodule SubSubMod torch nn Module __init__ - None super __init__ forward x x x SubMod torch nn Module __init__ - None super __init__ subsubmod = SubSubMod forward x x - x MyModule torch nn Module __init__ - None super __init__ submod = SubMod forward x x + submod subsubmod x orig_eager = MyModule export_module = torch export export orig_eager torch rand strict=True unflattened = unflatten export_module inputs = torch rand compare_outputs orig_eager unflattened inputs test_unflatten_container_type Leaf torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x Bar torch nn Module __init__ - None super __init__ leaf = Leaf buffer = torch nn Buffer torch randn forward x z buffer sum + leaf x sum + z sum + z sum Foo torch nn Module __init__ - None super __init__ bar = Bar forward x z y = bar buffer + x + z + z bar x z + y sum inp = torch randn torch randn torch randn mod = Foo ep_strict = torch export export mod inp strict=True noqa F ep_non_strict = torch export export mod inp strict=False gm_unflat_non_strict = unflatten ep_non_strict ep = torch export export gm_unflat_non_strict inp strict=False assertTrue torch allclose ep module inp mod inp test_unflattened_module_nodes_has_meta_val SubMod torch nn Module __init__ - None super __init__ forward x x + x x x MyModule torch nn Module __init__ - None super __init__ submod = SubMod forward x x + sum submod x orig_eager = MyModule export_module = torch export export orig_eager torch rand strict=True unflattened = unflatten export_module inputs = torch rand compare_outputs orig_eager unflattened inputs check_meta gm n gm graph nodes n op == output continue assertTrue n meta get val None m unflattened modules check_meta m test_unflatten_requires_grad_param M torch nn Module __init__ - None super __init__ p = torch nn Parameter torch ones requires_grad=False forward x p + x torch device meta mod = M inputs = torch randn device= meta ep = export mod inputs strict=True unflattened = unflatten ep assertTrue unflattened state_dict p requires_grad False assertTrue unflattened p requires_grad False test_placeholder_and_get_attr_ordering_after_unflattened TransposeModule torch nn Module __init__ - None super __init__ conv = torch nn Conv d stride= forward x x = conv x x transpose x = torch randn exported_program = export TransposeModule args= x strict=True unflattened_module = unflatten exported_program Check inputs created call_module node order call_module_input_order = node unflattened_module graph nodes node op == call_module transpose_module = unflattened_module get_submodule node target sub_node transpose_module graph nodes sub_node op == placeholder sub_node op == get_attr call_module_input_order append sub_node op assertEqual call_module_input_order placeholder get_attr get_attr test_unflatten_constant_tensor SubMod torch nn Module __init__ - None super __init__ initializer = forward x x + torch tensor initializer Mod torch nn Module __init__ - None super __init__ submod = SubMod forward x x + submod x export_module = torch export export Mod torch randn strict=True unflattened = unflatten export_module compare_outputs export_module module unflattened torch randn skipIfTorchDynamo custom objects supported dynamo yet test_unflatten_constant_obj init_torchbind_implementations torch _library register_fake_class _TorchScriptTesting _Foo FakeFoo noqa F __init__ x int y int x = x y = y classmethod __obj_unflatten__ cls flat_ctx cls dict flat_ctx add_tensor z x + y z SubMod torch nn Module __init__ - None super __init__ attr = torch classes _TorchScriptTesting _Foo forward x x + attr add_tensor x Mod torch nn Module __init__ - None super __init__ submod = SubMod forward x x + submod x enable_torchbind_tracing export_module = torch export export Mod torch randn strict=False unflattened = unflatten export_module compare_outputs export_module module unflattened torch randn test_unflatten_skipped_call_module C torch nn Module __init__ super __init__ forward x d x cos B torch nn Module __init__ super __init__ c = C forward x c x + x D torch nn Module __init__ super __init__ forward x x sin A torch nn Module __init__ super __init__ b = B d = D forward x b x = A The call chain looks like A - B - C - A d ep = torch export export torch randn strict=False ufm = unflatten ep assertExpectedInline str ufm graph_module code strip \ forward x b = b x x = None b assertExpectedInline str ufm b graph_module code strip \ forward x c = c x add = torch ops aten add Tensor c x c = x = None add assertExpectedInline str ufm b c graph_module code strip \ forward x cos = torch ops aten cos default x x = None sin = torch ops aten sin default cos cos = None sin test_nested_leaf_non_strict Leaf torch nn Module forward x x + Nested torch nn Module __init__ - None super __init__ leaf = Leaf forward x leaf x + TopLevel torch nn Module __init__ - None super __init__ nested = Nested forward x nested x + ep = torch export export TopLevel torch randn strict=False preserve_module_call_signature= nested torch export unflatten ep test_unflatten_submodule_ordering Module torch nn Module __init__ - None super __init__ buffer = torch nn Buffer torch rand register_parameter param torch nn Parameter torch rand forward x x + buffer + param Module torch nn Module __init__ - None super __init__ buffer = torch nn Buffer torch rand register_parameter param torch nn Parameter torch rand forward x x + buffer + param Module torch nn Module __init__ - None super __init__ mod = Module mod = mod mod = Module forward x mod mod mod x mod = Module ep = torch export export mod torch randn strict=True unflattened = torch export unflatten ep fqn_list = x x _ unflattened named_modules remove_duplicate=False assertEqual len fqn_list assertEqual x x _ mod named_modules remove_duplicate=False fqn_list test_duplicate_placeholder N C H W = MyModule torch nn Module __init__ - None super __init__ layer = torch nn LayerNorm C H W norms = torch nn ModuleList layer reuse layer norm layer layer forward input_ i range len norms output = norms i input_ input_ = output output mod = MyModule input_ = torch randn N C H W ep_strict = export copy deepcopy mod input_ strict=True umod = unflatten ep_strict assertTrue torch allclose umod input_ mod input_ ep_non_strict = export copy deepcopy mod input_ strict=False umod = unflatten ep_non_strict assertTrue torch allclose umod input_ mod input_ test_simple_alias handle weight sharing check tensor ids after unflattening Foo torch nn Module __init__ - None super __init__ alias param bias = torch nn Parameter torch randn m = torch nn Linear m bias = bias forward x m x + bias m = Foo inps = torch randn ep = export m inps strict=True unep = unflatten ep assertTrue id unep m bias == id unep bias handle aliasing where one alias unused Foo torch nn Module __init__ - None super __init__ bias = torch nn Parameter torch randn m = torch nn Linear m bias = bias bias unused aliasing should handled forward x m x m = Foo inps = torch randn ep = export m inps strict=True unep = unflatten ep assertTrue torch allclose unep inps m inps test_attr_as_submod_input layer torch nn Module forward x const - torch Tensor x + const M torch nn Module __init__ - None super __init__ const = torch nn Buffer torch ones layers = torch nn ModuleList layer _ range forward x torch Tensor - torch Tensor layer layers x = layer x const x mod = M x = torch randn ep = export mod x strict=True unflattened = unflatten ep torch testing assert_close unflattened x mod x test_dedup_sym_size Here sym_size floor div used subgraphs top-level m m only one copy sym_size created initial export graph For m sym_size floordiv should copied recompute since we preserve call signature m floordiv should passed placeholder Test preserved unflattened module runs correctly M torch nn Module forward x y d = x size y d M torch nn Module forward x y d = x size y d M torch nn Module __init__ - None super __init__ m = M m = M forward x y d = x size m _res = m x y m _res = m x y y d + m _res + m _res inputs = torch ones torch ones d_ = torch export Dim foo max= d = d_ ep = torch export export M inputs dynamic_shapes= d d strict=False preserve_module_call_signature= m unflat = unflatten ep unflat inputs fn_count_sym_size = lambda graph node target node graph nodes count torch ops aten sym_size int assertEqual fn_count_sym_size unflat graph assertEqual fn_count_sym_size unflat m graph assertEqual fn_count_sym_size unflat m graph test_unflatten_eager NestedChild torch nn Module forward x x x Child torch nn Module __init__ - None super __init__ nested = NestedChild register_parameter child param torch nn Parameter torch ones forward x x = nested x x + child param Child torch nn Module __init__ - None super __init__ child buffer = torch nn Buffer torch ones forward x x - child buffer MyModule torch nn Module __init__ - None super __init__ foo = Child bar = Child register_parameter rootparam torch nn Parameter torch ones forward x x = x rootparam x = foo x x = bar x x orig_eager = MyModule export_module = export orig_eager torch rand strict=True _disable_interpreter unflattened = unflatten export_module assertEqual unflattened _run_with_interpreter False assertEqual unflattened foo _run_with_interpreter False inputs = torch rand Compare root modules all submodules compare_outputs orig_eager unflattened inputs compare_outputs orig_eager foo unflattened foo inputs compare_outputs orig_eager bar unflattened bar inputs compare_outputs orig_eager foo nested unflattened foo nested inputs Check state dicts equal orig_state_dict = orig_eager state_dict exported_state_dict = unflattened state_dict name value orig_state_dict items assertTrue torch allclose value exported_state_dict name Check composability symbolic trace torchrec ddp uses symbolic tracer symbolic_traced = torch fx symbolic_trace unflattened concrete_args=inputs assertTrue torch allclose orig_eager inputs symbolic_traced inputs torch compile submodule unflattened foo = torch compile unflattened foo fullgraph=True compare_outputs orig_eager unflattened inputs test_unflatten_none M torch nn Module forward x y x + x None M torch nn Module __init__ - None super __init__ m = M forward x y x = x + x m x y ep = export M torch rand None preserve_module_call_signature= m unflattened = unflatten ep inp = torch randn None assertTrue torch allclose M inp unflattened inp test_unflatten_empty_branch M torch nn Module forward x x None torch ones torch ones x + x x x M torch nn Module __init__ super __init__ m = M forward x y b = m x c d = m y + b + c + d ep = torch export export M torch randn None unf = torch export unflatten ep inp = torch randn None assertTrue torch allclose unf inp M inp ep = torch export export M torch randn None preserve_module_call_signature= m unf = torch export unflatten ep inp = torch randn None assertTrue torch allclose unf inp M inp test_unflatten_root_module_type - None M torch nn Module forward x torch Tensor - torch Tensor x + x M torch nn Module __init__ - None super __init__ m = M forward x torch Tensor - torch Tensor m x inp = torch randn ep = torch export export M inp unf = torch export unflatten ep assertIsNotNone unf type_name assertEqual unf type_name split - M assertEqual unf m type_name split - M assertTrue torch allclose unf inp M inp __name__ == __main__ run_tests