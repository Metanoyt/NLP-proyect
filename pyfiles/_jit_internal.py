mypy allow-untyped-defs The weak_script annotation needs here instead inside torch jit so can used other places torch namely torch nn without running into circular dependency problems ast builtins collections contextlib enum inspect io pickle sys textwrap threading types typing warnings weakref typing noqa UP F Dict List Tuple imported torch jit annotations Any Callable Dict Final ForwardRef get_args get_origin List Optional Tuple TypeVar Union typing_extensions ParamSpec torch This needed ` torch _jit_internal ` imported before ` torch distributed __init__ ` Explicitly ask ` torch distributed __init__ ` first Otherwise AttributeError module torch has no attribute distributed raised torch distributed rpc torch package _mangling package_mangling torch _awaits _Await torch _C _Await CAwait Future CFuture torch _sources fake_range get_source_lines_and_file parse_def torch futures Future _P = ParamSpec _P _R = TypeVar _R BuiltinUnionType Union type tuple type = types UnionType LockType type try _thread LockType = _thread LockType except ImportError _dummy_thread type ignore import-not-found LockType = _dummy_thread LockType Wrapper functions can call either functions depending boolean argument boolean_dispatched weakref WeakKeyDictionary Callable dict str Callable = weakref WeakKeyDictionary noqa T FAKE_FILENAME_PREFIX = __torch_jit_dataclass is_final ann - bool hasattr ann __module__ ann __module__ typing typing_extensions get_origin ann Final isinstance ann type Final allows BroadcastingList instance subscriptable BroadcastingListCls __getitem__ types mypy doesn t support parameters types so we have explicitly type each list size BroadcastingList = BroadcastingListCls i range globals f BroadcastingList i = BroadcastingList is_scripting - bool r Function returns True when compilation False otherwise This useful especially unused decorator leave code your model yet TorchScript compatible testcode torch torch jit unused unsupported_linear_op x x linear x torch jit is_scripting torch linear x unsupported_linear_op x False Retrieves fully-qualified name module hierarchy + classname given obj _qualified_name obj mangle_name=True - str This special case allows us override qualified name type It s currently used conjunction tracing where we create fake module filter only supported attributes However since new type defined local we need mechanism override its qualname so appears correctly TorchScript system This we set _jit_override_qualname original traced module s qualified name which picked up here hasattr obj _jit_override_qualname obj _jit_override_qualname short-circuit cases where object already has known qualified name isinstance obj torch _C ScriptFunction obj qualified_name getattr obj __name__ None name = obj __name__ Enum classes do have ` __name__ ` attr instead they have ` name ` isinstance obj enum Enum name = obj name raise RuntimeError Could get name python object name == lambda name = _lambda make name valid identifier module_name = obj __module__ If module actually torchbind module then we should short circuit module_name == torch _classes obj qualified_name pyrefly ignore missing-attribute The Python docs very clear ` __module__ ` can None I can t figure out when actually would module_name None raise RuntimeError f Could get qualified name name __module__ can t None getattr sys modules module_name name obj raise RuntimeError f Could get qualified name name f attr name module module_name torch package TorchScript have separate mangling schemes avoid name collisions multiple packages To avoid them interfering each other normalize package managing here package_mangling is_mangled module_name module_name = module_name replace _ module_name = module_name replace _ The PythonExceptionValue C++ torch csrc jit python python_sugared_value h does need mangle python name mangle_name __main__ builtin module so rewrite __torch__ module_name == __main__ module_name = __torch__ Everything gets __torch__ prefix avoid name collisions names user values module_name = __torch__ + module_name name raise RuntimeError f Could get qualified name name f name valid identifier module_name + + name SourceLoader __init__ content = cache fn source content fn = source get_source fn content get fn loader = SourceLoader createResolutionCallbackFromEnv lookup_base Creates resolution callback will look up qualified names environment starting ` lookup_base ` base any qualified names then proceeding down lookup chain resolved object You should use directly should only used other createResolutionCallbackFrom functions lookupInModule qualified_name module qualified_name base remaining_pieces = qualified_name split maxsplit= module_value = getattr module base lookupInModule remaining_pieces module_value getattr module qualified_name parseNestedExpr expr module - tuple Any int i = while i len expr expr i i += Special case logic empty Tuple subscript used type annotation ` Tuple ` expr i == i base = lookupInModule expr i strip module assert base None f Unresolvable type expr i i == len expr expr i = base i assert expr i == parts = while expr i = part_len = i += part part_len = parseNestedExpr expr i module parts append part i += part_len len parts base tuple parts i + base parts i + parseExpr expr module try value len_parsed = parseNestedExpr expr module assert len_parsed == len expr whole expression parsed falling back c++ parser value except Exception The python resolver fails several cases known unit tests intended fall back gracefully c++ resolver general For example python style annotations which frequent our unit tests often fail types e g int resolvable calling frame None lambda expr parseExpr expr lookup_base createResolutionCallbackFromFrame frames_up int = Creates function which given string variable name returns value variable scope caller function which called createResolutionCallbackFromFrame default This used enable access in-scope Python variables inside TorchScript fragments frames_up number additional frames go up stack The default value which correspond frame caller createResolutionCallbackFromFrame Also example frames_up set then frame caller s caller createResolutionCallbackFromFrame will taken For example following program prints bar cb = createResolutionCallbackFromFrame print cb foo baz foo = bar baz frame = inspect currentframe i = while i frames_up + assert frame None frame = frame f_back i += assert frame None f_locals = frame f_locals f_globals = frame f_globals env __getattr__ key key f_locals f_locals key key f_globals f_globals key key dir builtins getattr builtins key createResolutionCallbackFromEnv env get_closure fn Get dictionary closed over variables function captures = captures update fn __globals__ index captured_name enumerate fn __code__ co_freevars captures captured_name = fn __closure__ index cell_contents captures local resolution python Depending where variable defined where used we may may able recover its value when recursively compiling script function Remember general case module function first defined then later scripted This means we do have chance capture active frames when function defined Hence any name resolution has happen later created closure The way python captures type annotations restricts what we can recover The follow example illustrates different cases MyGlobalClass my_local_scope torch jit script MyClass torch jit script MyClassUsedAsVar eg x MyClass y MyGlobalClass a_local_capture Foo MyClassUsedAsVar x MyGlobalClass defined __globals__ dictionary function eg so always recoverable my_local_scope introduces new local variable scope function Classes defined here only visible local variables For case MyClassUsedAsVar captured because used variable inside body function we can resolve using captures returned ` get_closure ` However type annotations captured closure In Python -- _value_ MyClass MyGlobalClass will available annotations ` eg ` ` starting Python they will represented strings no longer present Furthermore since body ` eg ` does reference those names they do appear list closed over variables In Python x type annotations comments leading similar situation where their definitions available We anticipate most users will run into issue because their modules functions will defined global scope like MyGlobalClass In cases where they possible work around issues declaring values global function In Python declaring global will make invisible ` inspect getsource ` see https bugs python org issue This could worked around manually adding ` global ` dictionary createResolutionCallbackFromClosure fn Create resolutionCallback introspecting function instead looking up stack enclosing scope closure = get_closure fn closure_lookup This since ` closure ` dict s easier ` env_helper ` everything just works ` getattr ` calls __getattr__ key key closure closure key hasattr typing key getattr typing key hasattr builtins key getattr builtins key None createResolutionCallbackFromEnv closure_lookup can_compile_class cls - bool If any functions type don t have code object type can t compiled probably builtin bound C is_ignored_fn cls False Ignore following list built-in classes ignored_builtin_classes = torch nn Module tuple list Exception issubclass cls ignored_builtin_classes False names = cls __dict__ fns = getattr cls name name names inspect isroutine getattr cls name None has_code = hasattr fn __code__ fn fns all has_code get_callable_argument_names fn - list str Gets names all POSITIONAL_OR_KEYWORD arguments callable ` fn ` Returns empty list when other types arguments present This used ` torch jit trace ` assign meaningful argument names traced functions modules Args fn A callable Returns Argument names List str inspect signature may fail give up case try callable_signature = inspect signature fn except Exception argument_names = name param callable_signature parameters items All four other types arguments do map individual values keyword name param kind = param POSITIONAL_OR_KEYWORD continue argument_names append name argument_names get_annotation_str annotation Convert AST node containing type annotation string present source represents same annotation isinstance annotation ast Name annotation id isinstance annotation ast Attribute join get_annotation_str annotation value annotation attr isinstance annotation ast Subscript In Python + subscript indices wrapped ast Index subscript_slice = annotation slice f get_annotation_str annotation value get_annotation_str subscript_slice isinstance annotation ast Tuple join get_annotation_str elt elt annotation elts isinstance annotation ast Constant f annotation value If AST node handled here s probably handled ScriptTypeParser None get_type_hint_captures fn Get dictionary containing type resolution mappings necessary resolve types literal annotations fn These considered closed-over fn must obtained separately e g using function Args fn A callable Returns A Dict str Any containing mapping literal annotations used fn Python objects they refer First try get source function We ll need parse find actual string names used annotate types since inspect signature will only object annotation refers string name If we can t get source simply empty dict This may happen cases where function synthesized dynamically runtime src = loader get_source fn src None try src = inspect getsource fn except OSError e raise OSError f Failed get source fn using inspect getsource e Gather dictionary parameter name - type skipping any parameters whose annotated types strings These only understood TorchScript context type annotation refers its own definition trying include mapping result function would cause infinite recursion because currently being compiled In addition there logic ScriptTypeParser handle signature = inspect signature fn name_to_type = name parameter annotation name parameter signature parameters items parameter annotation inspect Parameter empty isinstance parameter annotation str Then get literal type annotations function declaration source inspection This accounts case which aliases used annotate arguments e g device_t = torch device then d device_t frontend py cannot used here because includes _jit_internal so use ast instead = ast parse textwrap dedent src len body = isinstance body ast FunctionDef raise RuntimeError f Expected fn function f = body Prepare dictionary source annotation - type which will final result function using parsed AST f reconstruct source annotations strings each parameter mapping them type object corresponding annotation via name_to_type using parameter name annotation_to_type = arg f args args Get source type annotation string argument possible arg_annotation_str = get_annotation_str arg annotation arg annotation None If argument has no annotation get_annotation_str cannot convert string arg_annotation_str will None Skip arg ScriptTypeParser will probably handle latter case arg_annotation_str None continue Insert arg_annotation_str type into annotation_to_type possible One reason arg_name may present name_to_type annotation itself string type object common self-refential annotations classes Once again let ScriptTypeParser handle arg_name = arg arg arg_name name_to_type annotation_to_type arg_annotation_str = name_to_type arg_name If there valid annotation include annotation_to_type As argument annotations literal annotation has convertible string get_annotation_str actual type annotation cannot string literal_return_annotation = get_annotation_str f returns valid_literal_annotation = literal_return_annotation None return_annotation = signature return_annotation valid_return_annotation_type = return_annotation inspect Parameter empty isinstance return_annotation str valid_literal_annotation valid_return_annotation_type annotation_to_type literal_return_annotation = return_annotation annotation_to_type createResolutionCallbackForClassMethods cls This looks all methods defined pulls their closed-over variables into dictionary uses resolve variables cls type here so ` ismethod ` false since methods type aren t bound anything so Python treats them regular functions fns = getattr cls name name cls __dict__ inspect isroutine getattr cls name Skip built-ins they do have global scope nor type hints Needed support ` enum Enum ` derived classes Python- That adds ` _new_member_ ` property which alias ` __new__ ` fns = fn fn fns inspect isbuiltin fn hasattr fn __globals__ captures = fn fns captures update get_closure fn captures update get_type_hint_captures fn lookup_in_class key key captures captures key getattr builtins key None lookup_in_class boolean_dispatch arg_name arg_index default if_true if_false module_name func_name Dispatches either script functions based boolean argument In TorchScript boolean argument must constant so correct function use can determined compile time fn args kwargs dispatch_flag = default arg_name kwargs dispatch_flag = kwargs arg_name arg_index len args dispatch_flag = args arg_index dispatch_flag if_true args kwargs if_false args kwargs if_true __doc__ None if_false __doc__ None doc = if_false __doc__ if_true __doc__ = doc if_false __doc__ None if_true __doc__ None doc = if_true __doc__ if_false __doc__ = doc if_false __doc__ None if_true __doc__ None neither function has docstring doc = None raise RuntimeError only one function can have docstring fn __doc__ = doc module_name None fn __module__ = module_name func_name None fn __name__ = func_name boolean_dispatched fn = if_true if_true if_false if_false index arg_index default default arg_name arg_name fn FunctionModifiers Used denote behavior function TorchScript See export ignore details UNUSED = unused ignored replaced raising exception IGNORE = ignore leave call Python cannot torch jit save d EXPORT = export compile function even nothing calls DEFAULT = default compile called exported function forward COPY_TO_SCRIPT_WRAPPER = method scripted copy python method onto scripted model _DROP = _drop function fully ignored declaration can unscriptable export fn Callable _P _R - Callable _P _R This decorator indicates method ` ` nn Module ` ` used entry point into ` ScriptModule ` should compiled ` ` forward ` ` implicitly assumed entry point so does need decorator Functions methods called ` ` forward ` ` compiled they seen compiler so they do need decorator either Example using ` ` torch jit export ` ` method testcode torch torch nn nn MyModule nn Module implicitly_compiled_method x x + ` forward ` implicitly decorated ` torch jit export ` so adding here would have no effect forward x x + torch jit export another_forward x When compiler sees call will compile ` implicitly_compiled_method ` implicitly_compiled_method x unused_method x x - ` m ` will contain compiled methods ` forward ` ` another_forward ` ` implicitly_compiled_method ` ` unused_method ` will compiled since called any compiled methods wasn t decorated ` torch jit export ` m = torch jit script MyModule fn _torchscript_modifier = FunctionModifiers EXPORT type ignore attr-defined fn unused fn Callable _P _R - Callable _P _R This decorator indicates compiler function method should ignored replaced raising exception This allows you leave code your model yet TorchScript compatible still export your model Example using ` ` torch jit unused ` ` method torch torch nn nn MyModule nn Module __init__ use_memory_efficient super __init__ use_memory_efficient = use_memory_efficient torch jit unused memory_efficient x pdb pdb set_trace x + forward x Use not-yet-scriptable memory efficient mode use_memory_efficient memory_efficient x x + m = torch jit script MyModule use_memory_efficient=False m save m pt m = torch jit script MyModule use_memory_efficient=True exception raised m torch rand isinstance fn property prop = fn setattr noqa B prop fget _torchscript_modifier FunctionModifiers UNUSED prop fset setattr noqa B prop fset _torchscript_modifier FunctionModifiers UNUSED prop pyrefly ignore bad-return fn _torchscript_modifier = FunctionModifiers UNUSED type ignore attr-defined fn No op context manager python side _IgnoreContextManager contextlib AbstractContextManager __init__ kwargs pass __exit__ exc_type Any exc_value Any traceback Any - None pass ignore drop=False kwargs This decorator indicates compiler function method should ignored left Python function This allows you leave code your model yet TorchScript compatible If called TorchScript ignored functions will dispatch call Python interpreter Models ignored functions cannot exported use func ` torch jit unused torch jit unused ` instead Example using ` ` torch jit ignore ` ` method torch torch nn nn MyModule nn Module torch jit ignore debugger x pdb pdb set_trace forward x x += The compiler would normally try compile ` debugger ` since ` ignore ` d will left call Python debugger x x m = torch jit script MyModule Error The call ` debugger ` cannot saved since calls into Python m save m pt Example using ` ` torch jit ignore drop=True ` ` method testcode torch torch nn nn MyModule nn Module torch jit ignore drop=True training_method x pdb pdb set_trace forward x training training_method x x m = torch jit script MyModule This OK since ` training_method ` saved call replaced ` raise ` m save m pt testcleanup os os remove m pt callable drop used without any args so drop actually function torch jit ignore fn fn = drop pyrefly ignore missing-attribute fn _torchscript_modifier = FunctionModifiers IGNORE fn isinstance drop bool raise RuntimeError f Argument torch jit ignore must bool function got drop backwards compat drop_on_export = kwargs pop drop_on_export None drop_on_export warnings warn ignore drop_on_export=True has been deprecated TorchScript will now drop function call compilation Use torch jit unused now stacklevel= category=FutureWarning drop = drop_on_export drop warnings warn ignore True has been deprecated TorchScript will now drop function call compilation Use torch jit unused now stacklevel= category=FutureWarning decorator fn drop fn _torchscript_modifier = FunctionModifiers UNUSED fn _torchscript_modifier = FunctionModifiers IGNORE fn decorator _drop fn Callable _P _R - Callable _P _R fn _torchscript_modifier = FunctionModifiers _DROP type ignore attr-defined fn _copy_to_script_wrapper fn Callable _P _R - Callable _P _R fn _torchscript_modifier = FunctionModifiers COPY_TO_SCRIPT_WRAPPER type ignore attr-defined fn module_has_exports mod name dir mod hasattr mod name item = getattr mod name callable item get_torchscript_modifier item FunctionModifiers EXPORT True False WARNING should_drop currently being used our JIT code coverage plug-in mark JIT d code covered If you rename function please update references tools coverage_plugins_package src coverage_plugins jit_plugin py allow JIT d code still covered should_drop fn - bool attr = get_torchscript_modifier fn attr None False attr FunctionModifiers UNUSED attr FunctionModifiers _DROP is_ignored_fn fn - bool mod = get_torchscript_modifier fn mod FunctionModifiers UNUSED mod FunctionModifiers IGNORE mod FunctionModifiers _DROP _is_drop_fn fn - bool mod = get_torchscript_modifier fn mod FunctionModifiers _DROP is_static_fn cls fn - bool isinstance inspect getattr_static cls fn default=None staticmethod get_static_fn cls fn inspect getattr_static cls fn __func__ get_torchscript_modifier fn callable fn None hasattr fn __func__ fn = fn __func__ getattr fn _torchscript_modifier FunctionModifiers DEFAULT copy_torchscript_modifier orig new - None attr = get_torchscript_modifier orig attr None new _torchscript_modifier = attr overloading registration overloads get registered file compiled torch jit __init__ py so they can imported nn functional py without cycle qualified_name = list overload_functions _overloaded_fns dict str list Callable = noqa T _OVERLOAD_EXAMPLE = Example usage overload function torch jit _overload my_function x type - type decl pass torch jit _overload my_function x type - type decl pass my_function x implementation isinstance x type x isinstance x type x get_overload_no_implementation_error_message kind obj sourcelines file_lineno filename = get_source_lines_and_file obj f Implementation kind _qualified_name obj missing Please make f sure definition provided defined after all overload declarations \n f File filename line file_lineno \n + join sourcelines + \n + _OVERLOAD_EXAMPLE _check_overload_body func try parsed_def = parse_def func except OSError Parsing function definition can raise OSError source unavailable Since just initial check just raise warning case warnings warn f Unable retrieve source torch jit _overload function func stacklevel= body = parsed_def ast body body is_pass x isinstance x ast Pass is_ellipsis x isinstance x ast Expr isinstance x value ast Constant x value value Ellipsis len body = is_pass body is_ellipsis body msg = Only ` pass ` statement ` ` can body overload declaration \n msg += \n join parsed_def source split \n msg += - Expecting ` pass ` ` ` here \n + _OVERLOAD_EXAMPLE raise RuntimeError msg _overload func _check_overload_body func qual_name = _qualified_name func global _overloaded_fns fn_overload_list = _overloaded_fns get qual_name fn_overload_list None fn_overload_list = _overloaded_fns qual_name = fn_overload_list fn_overload_list append func func _get_fn_overloads qual_name _overloaded_fns get qual_name _clear_fn_overloads qual_name - None del _overloaded_fns qual_name get_class_name_lineno method - tuple str int current_frame = inspect currentframe one get_class_name call one _overload_method call _ range assert current_frame None assert current frame Optional FrameType current_frame = current_frame f_back assert current_frame None same here class_name = current_frame f_code co_name line_no = current_frame f_code co_firstlineno class_name line_no At point decorator applied methods method has no reference its owning _qualified_name would include defined so any methods same name same file would have same _qualified_name even they defined different classes This problem only exists python We get around problem looking stack frame identifying name throwing error whenever overloads used when modules same name same file qualified_name = name = list overload_functions _overloaded_methods dict str dict str list Callable = noqa T qualified_name name = class_fileno _overloaded_method_class_fileno dict tuple str str int = _overload_method func _check_overload_body func qual_name = _qualified_name func global _overloaded_methods class_name_map = _overloaded_methods get qual_name class_name_map None class_name_map = _overloaded_methods qual_name = class_name_map class_name line_no = get_class_name_lineno func method_overloads = class_name_map get class_name method_overloads None method_overloads = class_name_map class_name = method_overloads _overloaded_method_class_fileno qual_name class_name = line_no existing_lineno = _overloaded_method_class_fileno qual_name class_name existing_lineno = line_no raise RuntimeError Cannot currently overload same method name two different classes same name same module method_overloads append func func _get_overloaded_methods method mod_class TODO __name__ set submodules recursive script hasattr method __name__ None qual_name = _qualified_name method class_name_map = _overloaded_methods get qual_name class_name_map None None overloads = class_name_map get mod_class __name__ None overloads None None method_line_no = get_source_lines_and_file method mod_class_fileno = get_source_lines_and_file mod_class mod_end_fileno = mod_class_fileno + len get_source_lines_and_file mod_class method_line_no = mod_class_fileno method_line_no = mod_end_fileno raise AssertionError Overloads usable when module redeclared within same file + str method overloads is_tuple ann - bool Check typing Tuple missing args ` tuple ` fine ann typing Tuple noqa UP raise_error_container_parameter_missing Tuple For some reason Python violates Type A B __origin__ == Type rule hasattr ann __module__ False ann_origin = get_origin ann ann __module__ builtins typing ann_origin tuple is_list ann - bool Check typing List missing args ` list ` fine ann typing List noqa UP raise_error_container_parameter_missing List hasattr ann __module__ False ann_origin = get_origin ann ann __module__ builtins typing ann_origin list is_dict ann - bool Check typing Dict missing args ` dict ` fine ann typing Dict noqa UP raise_error_container_parameter_missing Dict hasattr ann __module__ False ann_origin = get_origin ann ann __module__ builtins typing ann_origin dict is_union ann ann Union raise_error_container_parameter_missing Union isinstance ann BuiltinUnionType hasattr ann __module__ ann __module__ == typing get_origin ann Union is_optional ann ann Optional raise_error_container_parameter_missing Optional is_optional_as_optional ann hasattr ann __module__ ann __module__ == typing get_origin ann Optional is_union_as_optional ann ann_args = get_args ann len ann_args == None ann_args type None ann_args is_optional_as_optional ann is_union ann is_union_as_optional ann is_future ann - bool ann Future raise RuntimeError Attempted use Future without contained type Please add contained type e g Future int get_origin ann Future is_await ann - bool ann _Await True get_origin ann _Await torch distributed rpc is_available torch _C _distributed_rpc PyRRef torch distributed rpc RRef is_rref ann - bool ann RRef raise RuntimeError Attempted use RRef without contained type Please add contained type e g RRef int get_origin ann RRef is_rref_instance obj - bool isinstance obj PyRRef is_rref_instance obj - bool If RPC module doesn t exist then RRefs don t exist either False _try_get_dispatched_fn fn callable fn None boolean_dispatched get fn _get_named_tuple_properties obj loc Optional torch _C _jit_tree_views SourceRange = None rcb=None loc None loc = fake_range assert issubclass obj tuple hasattr obj _fields hasattr obj _field_defaults defaults = obj _field_defaults field field obj _fields field obj _field_defaults defaults = obj_annotations = inspect get_annotations obj len obj_annotations == hasattr obj __base__ obj_annotations = inspect get_annotations pyrefly ignore bad-argument-type obj __base__ annotations = field obj _fields field obj_annotations field_type = obj_annotations field Note ForwardRef annotations NamedTuple attributes NamedTuple types slightly different normal types Normally annotations evaluated like during jit script Load strings python code into c++ parse Get annotations strings Use PythonResolver s resolution callback rcb convert string into python object We call into annotations py ann_to_type convert python obj step into type torchscript understands NamedTuples more complicated because has sub-types Normally once we have NamedTuple type object we can just look annotation literal values use ann_to_type directly them But sometimes users will annotate string literals e g x int This also happens PEP __forward__ annotations These annotations appear annotation dict ForwardRef int Then we need convert string into python object This requires having local context custom objects imported types rcb what gives us So we plumb rcb through stack so can used context block below FAQ - Why do we need special handling NamedTuple string annotations work fine normal types Normally we parse string directly then call rcb directly C++ - Why use ForwardRef _evaluate For we need globals locals local context where NamedTuple defined rcb what lets us look up into these So basically rcb does hard work us isinstance field_type ForwardRef rcb None rcb_type = rcb field_type __forward_arg__ rcb returns None can t find anything rcb_type None raise ValueError f Unknown type annotation field_type NamedTuple obj __name__ f Likely due partial support ForwardRef parameters NamedTuples see f Issue occurred loc highlight field_type = rcb_type the_type = torch jit annotations ann_to_type field_type loc rcb annotations append the_type annotations append torch _C TensorType getInferred type obj __name__ obj _fields annotations defaults _create_named_tuple t unqual_name str field_names list str defaults tuple Any TupleType = collections namedtuple unqual_name field_names defaults=defaults type ignore call-arg no-redef misc TupleType t contextlib contextmanager _disable_emit_hooks hooks = torch _C _jit_get_emit_hooks torch _C _jit_set_emit_hooks None None try yield finally torch _C _jit_set_emit_hooks hooks hooks _disable_emit_hooks_decorator _DecoratorContextManager - None noqa F noqa F __enter__ - None hooks = torch _C _jit_get_emit_hooks torch _C _jit_set_emit_hooks None None __exit__ args - None torch _C _jit_set_emit_hooks hooks hooks _is_exception obj - bool inspect isclass obj False issubclass obj Exception raise_error_container_parameter_missing target_type - None target_type endswith ict raise RuntimeError f Attempted use target_type without contained types Please add contained type e g f target_type int int raise RuntimeError f Attempted use target_type without contained type Please add contained type e g f target_type int _RAW_TYPE_NAME_MAPPING = dict dict list list tuple tuple typing Dict Dict noqa UP typing List List noqa UP typing Optional Optional typing Tuple Tuple noqa UP check_args_exist target_type - None name = _RAW_TYPE_NAME_MAPPING get target_type raise_error_container_parameter_missing name check_empty_containers obj - None obj == obj == obj == warnings warn The inner type container lost when calling torch jit isinstance eager mode For example List int would become list therefore falsely True List float List str stacklevel= supports List Dict Tuple Optional types TODO support future container_checker obj target_type - bool origin_type = get_origin target_type check_args_exist target_type origin_type None False origin_type list origin_type typing List noqa UP check_empty_containers obj isinstance obj list False arg_type = get_args target_type arg_origin = get_origin arg_type el obj check nested container ex List List str arg_origin processes nested container ex List List str container_checker el arg_type False isinstance el arg_type False True origin_type typing Dict origin_type dict noqa UP check_empty_containers obj isinstance obj dict False key_type = get_args target_type val_type = get_args target_type key val obj items check keys right type isinstance key key_type False val_origin = get_origin val_type val_origin container_checker val val_type False isinstance val val_type False True origin_type typing Tuple origin_type tuple noqa UP check_empty_containers obj isinstance obj tuple False arg_types = get_args target_type len obj = len arg_types False el el_type zip obj arg_types el_origin = get_origin el_type el_origin container_checker el el_type False isinstance el el_type False True origin_type Union issubclass pyrefly ignore bad-argument-type origin_type BuiltinUnionType also handles Optional obj None check before recursion because None always fine True inner_types = get_args target_type t inner_types t_origin = get_origin t t_origin container_checker obj t isinstance obj t True False _isinstance obj target_type - bool isinstance target_type collections abc Container isinstance target_type tuple raise RuntimeError The second argument ` torch jit isinstance ` must type tuple types t_type target_type _isinstance obj t_type True False origin_type = get_origin target_type origin_type container_checker obj target_type Check handle non-typed optional origin returns none instead optional - check_args_exist target_type handle non-containers isinstance obj target_type _TensorExtractor pickle Pickler __init__ args tensors list torch Tensor kwargs super __init__ args kwargs tensors = tensors persistent_id obj isinstance obj torch Tensor tensors append obj Since we just want extract tensors we don t mind object unpicklable doesn t contain tensors we can just ignore skip To play safe we only do so common objects we re sure don t contain tensors Feel free add new types here Note also even type isn t listed here won t block users since they can just add __getstate__ __reduce__ method their isinstance obj LockType Futures RRefs don t technically contain value they just offer means access value isinstance obj CFuture is_rref_instance obj isinstance obj CAwait isinstance obj torch cuda Event isinstance obj threading Thread None _extract_tensors obj r This function exclusively called C++ See ` ` torch csrc jit python python_ivalue h ` ` It extracts tensors contained given object through pickling tensors list torch Tensor = extractor = _TensorExtractor io BytesIO protocol=- tensors=tensors extractor dump obj tensors _get_model_id obj - Optional str isinstance obj torch jit ScriptModule str obj _c _type isinstance obj torch jit ScriptFunction obj qualified_name None In Python- + typed enums i e IntEnum example retain number base methods subclass previously dropped To preserve behavior explicitly drop them there sys version_info = _drop enum Enum __new__ _drop enum Enum __format__ _drop enum Enum __repr__ _drop enum Enum __str__