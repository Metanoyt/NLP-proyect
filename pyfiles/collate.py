mypy allow-untyped-defs r Contains definitions methods used _BaseDataLoaderIter workers These methods used collate samples fetched dataset into Tensor s These needs global scope since Py doesn t support serializing static methods ` default_collate ` ` default_convert ` exposed users via dataloader py collections contextlib copy re collections abc Callable typing Optional Union torch np_str_obj_array_pattern = re compile r SaUO default_convert data r Convert each NumPy array element into ` torch Tensor ` If input ` Sequence ` ` Collection ` ` Mapping ` tries convert each element inside ` torch Tensor ` If input NumPy array left unchanged This used default function collation when both ` batch_sampler ` ` batch_size ` NOT defined ` ~torch utils data DataLoader ` The general input type output type mapping similar func ` ~torch utils data default_collate ` See description there more details Args data single data point converted Examples xdoctest +SKIP Example ` int ` default_convert Example NumPy array default_convert np array tensor Example NamedTuple Point = namedtuple Point x y default_convert Point Point x= y= default_convert Point np array np array Point x=tensor y=tensor Example List default_convert np array np array tensor tensor elem_type = type data isinstance data torch Tensor data elem_type __module__ == numpy elem_type __name__ = str_ elem_type __name__ = string_ array string classes object elem_type __name__ == ndarray np_str_obj_array_pattern search data dtype str None data torch as_tensor data isinstance data collections abc Mapping try isinstance data collections abc MutableMapping The mapping type may have extra properties so we can t just use ` type data ` create new mapping Create clone update mapping type mutable clone = copy copy data clone update key default_convert data key key data clone elem_type key default_convert data key key data except TypeError The mapping type may support ` copy ` ` update mapping ` ` __init__ iterable ` key default_convert data key key data isinstance data tuple hasattr data _fields namedtuple elem_type default_convert d d data isinstance data tuple default_convert d d data Backwards compatibility isinstance data collections abc Sequence isinstance data str bytes try isinstance data collections abc MutableSequence The sequence type may have extra properties so we can t just use ` type data ` create new sequence Create clone update sequence type mutable clone = copy copy data type ignore arg-type i d enumerate data clone i = default_convert d clone elem_type default_convert d d data except TypeError The sequence type may support ` copy ` ` __setitem__ index item ` ` __init__ iterable ` e g ` range ` default_convert d d data data default_collate_err_msg_format = default_collate batch must contain tensors numpy arrays numbers dicts lists found collate batch collate_fn_map Optional dict Union type tuple type Callable = None r General collate function handles collection type element within each batch The function also opens function registry deal specific element types ` default_collate_fn_map ` provides default collate functions tensors numpy arrays numbers strings Args batch single batch collated collate_fn_map Optional dictionary mapping element type corresponding collate function If element type isn t present dictionary function will go through each key dictionary insertion order invoke corresponding collate function element type subclass key Examples collate_tensor_fn batch collate_fn_map Extend function handle batch tensors torch stack batch custom_collate batch collate_map = torch Tensor collate_tensor_fn collate batch collate_fn_map=collate_map Extend ` default_collate ` in-place modifying ` default_collate_fn_map ` default_collate_fn_map update torch Tensor collate_tensor_fn Note Each collate function requires positional argument batch keyword argument dictionary collate functions ` collate_fn_map ` elem = batch elem_type = type elem collate_fn_map None elem_type collate_fn_map collate_fn_map elem_type batch collate_fn_map=collate_fn_map collate_type collate_fn_map isinstance elem collate_type collate_fn_map collate_type batch collate_fn_map=collate_fn_map isinstance elem collections abc Mapping try isinstance elem collections abc MutableMapping The mapping type may have extra properties so we can t just use ` type data ` create new mapping Create clone update mapping type mutable clone = copy copy elem clone update key collate d key d batch collate_fn_map=collate_fn_map key elem clone elem_type key collate d key d batch collate_fn_map=collate_fn_map key elem except TypeError The mapping type may support ` copy ` ` update mapping ` ` __init__ iterable ` key collate d key d batch collate_fn_map=collate_fn_map key elem isinstance elem tuple hasattr elem _fields namedtuple elem_type collate samples collate_fn_map=collate_fn_map samples zip batch strict=False isinstance elem collections abc Sequence check make sure elements batch have consistent size = iter batch elem_size = len next pyrefly ignore not-iterable all len elem == elem_size elem raise RuntimeError each element list batch should equal size transposed = list zip batch strict=False It may accessed twice so we use list isinstance elem tuple collate samples collate_fn_map=collate_fn_map samples transposed Backwards compatibility try isinstance elem collections abc MutableSequence The sequence type may have extra properties so we can t just use ` type data ` create new sequence Create clone update sequence type mutable clone = copy copy elem type ignore arg-type i samples enumerate transposed clone i = collate samples collate_fn_map=collate_fn_map clone elem_type collate samples collate_fn_map=collate_fn_map samples transposed except TypeError The sequence type may support ` copy ` ` __setitem__ index item ` ` __init__ iterable ` e g ` range ` collate samples collate_fn_map=collate_fn_map samples transposed raise TypeError default_collate_err_msg_format format elem_type collate_tensor_fn batch collate_fn_map Optional dict Union type tuple type Callable = None elem = batch out = None elem is_nested raise RuntimeError Batches nested tensors currently supported default collate_fn please provide custom collate_fn handle them appropriately elem layout torch sparse_coo torch sparse_csr torch sparse_bsr torch sparse_csc torch sparse_bsc raise RuntimeError Batches sparse tensors currently supported default collate_fn please provide custom collate_fn handle them appropriately torch utils data get_worker_info None If we re background process concatenate directly into shared memory tensor avoid extra copy numel = sum x numel x batch storage = elem _typed_storage _new_shared numel device=elem device out = elem new storage resize_ len batch list elem size torch stack batch out=out collate_numpy_array_fn batch collate_fn_map Optional dict Union type tuple type Callable = None elem = batch array string classes object np_str_obj_array_pattern search elem dtype str None raise TypeError default_collate_err_msg_format format elem dtype collate torch as_tensor b b batch collate_fn_map=collate_fn_map collate_numpy_scalar_fn batch collate_fn_map Optional dict Union type tuple type Callable = None torch as_tensor batch collate_float_fn batch collate_fn_map Optional dict Union type tuple type Callable = None torch tensor batch dtype=torch float collate_int_fn batch collate_fn_map Optional dict Union type tuple type Callable = None torch tensor batch collate_str_fn batch collate_fn_map Optional dict Union type tuple type Callable = None batch default_collate_fn_map dict Union type tuple type Callable = torch Tensor collate_tensor_fn contextlib suppress ImportError numpy np For both ndarray memmap subclass ndarray default_collate_fn_map np ndarray = collate_numpy_array_fn See scalars hierarchy https numpy org doc stable reference arrays scalars html Skip string scalars default_collate_fn_map np bool_ np number np object_ = collate_numpy_scalar_fn default_collate_fn_map float = collate_float_fn default_collate_fn_map int = collate_int_fn default_collate_fn_map str = collate_str_fn default_collate_fn_map bytes = collate_str_fn default_collate batch r Take batch data put elements within batch into tensor additional outer dimension - batch size The exact output type can ` torch Tensor ` ` Sequence ` ` torch Tensor ` Collection ` torch Tensor ` left unchanged depending input type This used default function collation when ` batch_size ` ` batch_sampler ` defined ` ~torch utils data DataLoader ` Here general input type based type element within batch output type mapping ` torch Tensor ` - ` torch Tensor ` added outer dimension batch size NumPy Arrays - ` torch Tensor ` ` float ` - ` torch Tensor ` ` int ` - ` torch Tensor ` ` str ` - ` str ` unchanged ` bytes ` - ` bytes ` unchanged ` Mapping K V_i ` - ` Mapping K default_collate V_ V_ ` ` NamedTuple V _i V _i ` - ` NamedTuple default_collate V _ V _ default_collate V _ V _ ` ` Sequence V _i V _i ` - ` Sequence default_collate V _ V _ default_collate V _ V _ ` Args batch single batch collated Examples xdoctest +SKIP Example batch ` int ` s default_collate tensor Example batch ` str ` s default_collate b c b c Example ` Map ` inside batch default_collate A B A B A tensor B tensor Example ` NamedTuple ` inside batch Point = namedtuple Point x y default_collate Point Point Point x=tensor y=tensor Example ` Tuple ` inside batch default_collate tensor tensor Example ` List ` inside batch default_collate tensor tensor Two options extend ` default_collate ` handle specific type Option Write custom collate function invoke ` default_collate ` custom_collate batch elem = batch isinstance elem CustomType Some custom condition Fall back ` default_collate ` default_collate batch Option In-place modify ` default_collate_fn_map ` collate_customtype_fn batch collate_fn_map=None default_collate_fn_map update CustomType collate_customtype_fn default_collate batch Handle ` CustomType ` automatically collate batch collate_fn_map=default_collate_fn_map