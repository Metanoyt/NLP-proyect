mypy allow-untyped-defs Common utilities functions flex attention kernels math collections abc Sequence functools partial pathlib Path typing Any Optional TYPE_CHECKING Union sympy torch torch _inductor virtualized V torch utils _ordered_set OrderedSet torch utils _pytree tree_map tree_map_only TYPE_CHECKING torch _inductor codegen cuda_combined_scheduling _IntLike _IntLike = Union int sympy Expr ir ComputedBuffer ExternKernel FixedLayout FlexibleLayout get_fill_order InputBuffer IRNode MutationLayoutSHOULDREMOVE Scatter ShapeAsConstantBuffer StorageBox Subgraph TensorBox lowering _full check_and_broadcast_indices expand index_output_size_and_inner_fn to_dtype select_algorithm realize_inputs utils load_template SubgraphResults = Union list Optional ComputedBuffer Optional ComputedBuffer zeros_and_scatter_lowering shape list int indices values To support backwards captured buffers we register specific lowering our specific custom up Always accumulate into fp then cast grad = _full values get_device torch float shape assert isinstance grad TensorBox grad realize x_size = grad get_size values = to_dtype values grad get_dtype indices_loaders = i make_loader i None None i indices indices tensor_indices = check_and_broadcast_indices indices grad get_device We can use first one since they all required same size tensor_size = list indices tensor_indices get_size indexed_size = x_size i i range len indices expected_vals_size inner_fn = index_output_size_and_inner_fn x_size indices tensor_indices tensor_size indices_loaders indexed_size None check=True values = expand values expected_vals_size device = grad get_device assert device None scatter = Scatter device=device dtype=grad get_dtype inner_fn=values make_loader ranges=expected_vals_size iter_ranges output_indexer=inner_fn scatter_mode= atomic_add buffer = ComputedBuffer name=grad data data name type ignore attr-defined layout=MutationLayoutSHOULDREMOVE grad data=scatter buffer get_fwd_subgraph_outputs subgraph_buffer SubgraphResults mask_graph_buffer SubgraphResults - list Optional ComputedBuffer subgraph_buffer = pyrefly ignore bad-assignment subgraph_buffer isinstance subgraph_buffer Sequence subgraph_buffer mask_graph_buffer = pyrefly ignore bad-assignment mask_graph_buffer isinstance mask_graph_buffer Sequence mask_graph_buffer pyrefly ignore not-iterable subgraph_buffer mask_graph_buffer build_subgraph_module_buffer args list Union TensorBox ShapeAsConstantBuffer graph_module torch fx GraphModule - SubgraphResults This function s goal take required args produce subgraph buffer The subgraph buffer ComputedBuffer will inlined into triton template Args args The args passed into subgraph Contains both fixed lifted inputs subgraph The Subgraph ir which produce output node This one we gotta keep lazy subgraph_lowering PointwiseSubgraphLowering pw_subgraph = PointwiseSubgraphLowering graph_module root_graph_lowering=V graph allowed_mutations=OrderedSet torch ops flex_lib zeros_and_scatter default additional_lowerings= torch ops flex_lib zeros_and_scatter default zeros_and_scatter_lowering V set_graph_handler pw_subgraph type ignore arg-type pw_subgraph run args convert_output_node_to_buffer output_buffer - Optional ComputedBuffer output_buffer None None isinstance output_buffer ComputedBuffer These nodes coming output zeros_and_scatter output_buffer assert isinstance output_buffer TensorBox The output node flex attention s subgraph must TensorBox got type output_buffer assert isinstance output_buffer data StorageBox The output node flex attention subgraph must StorageBox got type output_buffer device = output_buffer data get_device assert device None subgraph_buffer = ComputedBuffer name=None layout=FlexibleLayout device=device dtype=output_buffer data get_dtype size=output_buffer data get_size data=output_buffer data data type ignore arg-type subgraph_buffer tree_map convert_output_node_to_buffer pw_subgraph graph_outputs build_subgraph_buffer args list Union TensorBox ShapeAsConstantBuffer subgraph Subgraph - SubgraphResults build_subgraph_module_buffer args subgraph graph_module maybe_realize args list Optional IRNode Accepts list optional IRNodes returns list realized IRNodes tree_map lambda x realize_inputs x x None isinstance x sympy Symbol x args freeze_irnodes tree Any - Any Freeze layouts every IRNode contained pytree tree None None _freeze node IRNode - IRNode try node freeze_layout except NotImplementedError pass node tree_map_only IRNode _freeze tree create_placeholder name str dtype torch dtype device torch device size Optional list int = None - Union TensorBox ShapeAsConstantBuffer Creates placeholder input buffers producing subgraph_output input_buffer = InputBuffer name=name layout=FixedLayout device dtype size size FlexibleLayout contiguous_strides size size TensorBox create input_buffer construct_strides sizes Sequence _IntLike fill_order Sequence int - Sequence _IntLike From list sizes fill order construct strides permuted tensor Initialize strides assert len sizes == len fill_order Length sizes must match length fill order strides list _IntLike = len sizes Start stride innermost dimension current_stride _IntLike = Iterate through fill order populating strides dim fill_order strides dim = current_stride current_stride = sizes dim strides infer_dense_strides size Sequence _IntLike orig_strides Sequence _IntLike This mirror same function aten src ATen ExpandUtils cpp Args size The size output tensor orig_strides The strides input tensor Returns List int Dense non-overlapping strides preserve input tensor s layout permutation The returned strides follow same stride propagation rules TensorIterator This matches The behavior empty_like fill_order = get_fill_order orig_strides V graph sizevars shape_env construct_strides size fill_order create_indices_fake x - torch Tensor Create fake indices used autotuning size = V graph sizevars size_hint i i x get_size indices = torch arange size - dtype=x get_dtype device=x get_device indices = indices expand size contiguous indices create_num_blocks_fake_generator sparse_indices Create fake num_blocks used autotuning The idea here we need create real tensor real data s representative benchmarking For example returning all zeros ` kv_num_blocks ` input would mean we computing blocks each row which would provide bogus autotuning results In case we choose use min max_block blocks because I Horace think ll probably result pretty representative performance If s too short then prefetching won t help If s too long then autotuning will take longer no good reason create_num_blocks_fake x - torch Tensor num_blocks_for_autotuning = V graph sizevars size_hint sparse_indices shape - size = V graph sizevars size_hint i i x get_size torch full size num_blocks_for_autotuning dtype=x get_dtype device=x get_device create_num_blocks_fake contiguous_last_dim x Ensure realized IR node has contiguous stride last dimension strides = x maybe_get_stride strides strides - = contiguous_stride_order = list reversed range len x get_size ExternKernel require_stride_order x contiguous_stride_order x set_head_dim_values kernel_options dict str Any qk_head_dim v_head_dim graph_sizevars Mutates kernel options adding head dimension calculations Args kernel_options Dictionary populate options qk_head_dim Query Key head dimension v_head_dim Value head dimension graph_sizevars Graph size variables object guard_int method QK dimensions qk_head_dim_static = graph_sizevars guard_int qk_head_dim kernel_options setdefault QK_HEAD_DIM qk_head_dim_static kernel_options setdefault QK_HEAD_DIM_ROUNDED next_power_of_two qk_head_dim_static V dimensions v_head_dim_static = graph_sizevars guard_int v_head_dim kernel_options setdefault V_HEAD_DIM v_head_dim_static kernel_options setdefault V_HEAD_DIM_ROUNDED next_power_of_two v_head_dim_static Safety flag kernel_options setdefault SAFE_HEAD_DIM is_power_of_ qk_head_dim_static is_power_of_ v_head_dim_static is_power_of_ n n = n n - == next_power_of_two n n = math ceil math log n _FLEX_TEMPLATE_DIR = Path __file__ parent templates load_flex_template = partial load_template template_dir=_FLEX_TEMPLATE_DIR Template strings have been moved templates common py jinja