Owner s oncall distributed contextlib sys copy deepcopy functools partial torch torch distributed dist torch nn nn torch distributed algorithms _checkpoint checkpoint_wrapper checkpoint_wrapper offload_wrapper torch distributed fsdp ShardingStrategy torch distributed fsdp fully_sharded_data_parallel CPUOffload FullyShardedDataParallel FSDP torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp _maybe_wrap_fsdp FSDPTest get_devtype torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests TEST_WITH_DEV_DBG_ASAN torch utils checkpoint checkpoint device_type = torch device get_devtype dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit _save_on_cpu_called = False get_patched_save_on_cpu orig_save_on_cpu = torch distributed algorithms _checkpoint checkpoint_wrapper save_on_cpu patched_save_on_cpu args kwargs global _save_on_cpu_called _save_on_cpu_called = True orig_save_on_cpu args kwargs patched_save_on_cpu contextlib contextmanager patch_save_on_cpu new_save_on_cpu orig_save_on_cpu = torch distributed algorithms _checkpoint checkpoint_wrapper save_on_cpu torch distributed algorithms _checkpoint checkpoint_wrapper save_on_cpu = new_save_on_cpu try yield finally torch distributed algorithms _checkpoint checkpoint_wrapper save_on_cpu = orig_save_on_cpu TestFSDPCheckpoint FSDPTest SequentialModule nn Module __init__ checkpoint_layer=False offload_activations=False wrap_fsdp=False fsdp_args fsdp_kwargs torch manual_seed super __init__ l = nn Linear device_type type l = nn Linear device_type type l = nn Linear device_type type checkpoint_layer offload_activations ckpt_wrapper = offload_wrapper ckpt_wrapper = checkpoint_wrapper l = ckpt_wrapper l l = ckpt_wrapper l l = ckpt_wrapper l fsdp_wrapper = partial _maybe_wrap_fsdp fsdp_args wrap_fsdp=wrap_fsdp fsdp_kwargs ffn = nn Sequential fsdp_wrapper l fsdp_wrapper l fsdp_wrapper l forward x ffn x _verify_parity losses outputs models assert losses assert outputs assert models l o zip losses outputs assertEqual losses l assertEqual outputs o Verify grads ref_model = models ref_grads = p grad p ref_model parameters m models grads = p grad p m parameters ref_g g zip ref_grads grads assertEqual ref_g g skip_if_lt_x_gpu parametrize cpu_offload CPUOffload offload_params=True CPUOffload offload_params=False parametrize offload_activations True False parametrize use_orig_params False True test_checkpoint_fsdp_wrapping cpu_offload CPUOffload offload_activations bool use_orig_params bool Test checkpoint FSDP layer FSDP layer offload_activations wrapper_to_use = offload_wrapper wrapper_to_use = checkpoint_wrapper fsdp_kwargs = cpu_offload cpu_offload use_orig_params use_orig_params ckpt_sequential_wrapped_fsdp = wrapper_to_use TestFSDPCheckpoint SequentialModule wrap_fsdp=True fsdp_kwargs Test FSDP checkpoint layer FSDP checkpoint layer inner_ckpt = TestFSDPCheckpoint SequentialModule checkpoint_layer=True offload_activations=offload_activations wrap_fsdp=True fsdp_kwargs baseline = TestFSDPCheckpoint SequentialModule wrap_fsdp=True fsdp_kwargs note reentrant-based checkpointing requires inputs have grad flag set inp = torch randn device=device_type type requires_grad=True global _save_on_cpu_called models = ckpt_sequential_wrapped_fsdp inner_ckpt baseline patch_save_on_cpu get_patched_save_on_cpu i range losses = outputs = m models check_offload = m = baseline i == offload_activations check_offload assertFalse _save_on_cpu_called out = m inp check_offload assertTrue _save_on_cpu_called _save_on_cpu_called = False loss = out sum loss backward losses append loss outputs append out _verify_parity losses outputs models dist barrier skip_if_lt_x_gpu parametrize cpu_offload CPUOffload offload_params=True CPUOffload offload_params=False parametrize offload_activations True False parametrize use_orig_params False True test_basic_checkpoint_end_to_end cpu_offload CPUOffload offload_activations bool use_orig_params bool fsdp_kwargs = cpu_offload cpu_offload use_orig_params use_orig_params global _save_on_cpu_called patch_save_on_cpu get_patched_save_on_cpu seq = TestFSDPCheckpoint SequentialModule device_type type Runs FSDP no checkpointing fsdp_only_seq = FSDP deepcopy seq fsdp_kwargs Runs checkpoint-wrapped FSDP offload_activations wrapper_to_use = offload_wrapper wrapper_to_use = checkpoint_wrapper checkpointed_fsdp = wrapper_to_use FSDP deepcopy seq fsdp_kwargs Runs FSDP-wrapped checkpointed module fsdp_wrapped_checkpoint = FSDP wrapper_to_use deepcopy seq fsdp_kwargs Runs FSDP manual calls checkpoint fsdp_call_checkpoint = FSDP deepcopy seq fsdp_kwargs note reentrant-based checkpointing requires inputs have grad flag set inp = torch randn device=device_type type requires_grad=True models = fsdp_only_seq checkpointed_fsdp fsdp_wrapped_checkpoint fsdp_call_checkpoint Ensure _save_on_cpu yet called assertFalse _save_on_cpu_called i range losses = outputs = m models check_offload = m = fsdp_only_seq i == offload_activations m == fsdp_call_checkpoint _save_on_cpu should called yet assertFalse _save_on_cpu_called offload_ctx = get_patched_save_on_cpu pin_memory=True offload_activations contextlib nullcontext offload_ctx out = checkpoint m inp use_reentrant=True _save_on_cpu should called yet assertFalse _save_on_cpu_called out = m inp check_offload assertTrue _save_on_cpu_called loss = out sum loss backward losses append loss outputs append out _save_on_cpu_called = False _verify_parity losses outputs models dist barrier instantiate_parametrized_tests TestFSDPCheckpoint CheckpointModule nn Module __init__ checkpoint bool = False use_reentrant bool = True super __init__ seq = nn Sequential nn Linear _ range checkpoint = checkpoint use_reentrant = use_reentrant forward x checkpoint seq x use_reentrant=self use_reentrant checkpoint seq x ModelWithCheckpointSubmodule nn Module __init__ checkpoint bool = False use_reentrant bool = True super __init__ l = nn Linear s = CheckpointModule checkpoint use_reentrant s = CheckpointModule checkpoint use_reentrant relu = nn ReLU l = nn Linear forward x l relu s s l x TestModel nn Module __init__ checkpoint bool = False use_reentrant bool = True super __init__ l = nn Linear relu = nn ReLU checkpoint = ModelWithCheckpointSubmodule checkpoint use_reentrant checkpoint = ModelWithCheckpointSubmodule checkpoint use_reentrant l = nn Linear forward x l relu checkpoint checkpoint l x TestFSDPCheckpointSubmodule FSDPTest TODO grad value checks occasionally fails when use_reentrant = True skip_if_lt_x_gpu parametrize use_reentrant False test_checkpoint_submodule device use_reentrant bool model = TestModel use_reentrant=use_reentrant device_type type model_ac = deepcopy model _ m model_ac named_modules isinstance m CheckpointModule m checkpoint = True assertTrue model_ac checkpoint s checkpoint assertTrue model_ac checkpoint s checkpoint fsdp_kwargs = device_id device_type type sharding_strategy ShardingStrategy NO_SHARD Wrap no checkpointing model submodules FSDP model checkpoint = FSDP module=model checkpoint fsdp_kwargs model checkpoint = FSDP module=model checkpoint fsdp_kwargs Wrap checkpointing model submodules FSDP model_ac checkpoint = FSDP module=model_ac checkpoint fsdp_kwargs model_ac checkpoint = FSDP module=model_ac checkpoint fsdp_kwargs x = torch randn device=self device_type model x sum backward model_ac x sum backward n p n p zip model named_parameters model_ac named_parameters assertEqual n n assertTrue p grad allclose p grad devices = cuda hpu xpu instantiate_device_type_tests TestFSDPCheckpointSubmodule globals only_for=devices allow_xpu=True __name__ == __main__ run_tests