Owner s oncall distributed copy os pickle subprocess sys tempfile threading time unittest contextlib nullcontext dataclasses dataclass datetime timedelta itertools product sys platform typing Optional torch torch distributed dist dist is_available print distributed package available skipping tests file=sys stderr sys exit torch distributed algorithms ddp_comm_hooks powerSGD_hook powerSGD torch distributed distributed_c d c d torch nn functional F torch testing _internal common_utils common torch nn torch nn parallel DistributedDataParallel torch testing _internal common_distributed MultiProcessTestCase skip_if_lt_x_gpu torch testing _internal common_utils instantiate_parametrized_tests IS_FBCODE IS_SANDCASTLE load_tests parametrize retry_on_connect_failures run_tests TEST_WITH_DEV_DBG_ASAN TEST_XPU TestCase torch utils checkpoint checkpoint TEST_WITH_DEV_DBG_ASAN print Multiprocessing spawn compatible dev dbg asan file=sys stderr sys exit load_tests common_utils used automatically filter tests sharding sandcastle This line silences flake warnings load_tests = load_tests noqa PLW platform == darwin LOOPBACK = lo LOOPBACK = lo torch backends cuda matmul allow_tf = False device_type = acc type acc = torch accelerator current_accelerator cpu gpus_for_rank world_size Multigpu tests designed simulate multi nodes multi GPUs each node Nccl backend requires equal #GPUs each process On single node all visible GPUs evenly divided subsets each process only uses subset device_count = torch accelerator device_count visible_devices = list range device_count gpus_per_process = device_count world_size gpus_for_rank = rank range world_size gpus_for_rank append visible_devices rank gpus_per_process rank + gpus_per_process gpus_for_rank AbstractTimeoutTest _test_store_timeout backend init_method c p try dist init_process_group backend=backend init_method=init_method world_size= rank= timeout=timedelta seconds= default_store = c d _get_default_store tik = time time assertRaisesRegex RuntimeError i timeout default_store get nonexistent key tok = time time dist destroy_process_group c p append float tok - tik except RuntimeError e catch Address already use error report main thread c p append e _init_methods f = tempfile NamedTemporaryFile delete=False sys platform == win yield file format f name replace \\ f close yield f file f name f close yield f tcp common find_free_port d _test_default_store_timeout backend init_method _init_methods c p = t = threading Thread target=self _test_store_timeout args= backend init_method c p t daemon = True t start t join assertEqual len c p isinstance c p float waiting time should s use s rule out false alarm assertGreater c p isinstance c p RuntimeError let retry_on_connect_failures handle error raise c p raise RuntimeError f Unexpected type type c p TimeoutTest TestCase retry_on_connect_failures test_store_based_barrier f = tempfile NamedTemporaryFile delete=False port = common find_free_port thread_work timeout init_type world_size rank error_list we need create separate store just store barrier test init_type == file barrier_store = dist FileStore f name init_type == tcp barrier_store = dist TCPStore localhost port world_size is_master=rank == wait_for_workers=False init_type == hash barrier_store = dist HashStore try missing worker will cause timeout rank = world_size - c d _store_based_barrier rank=rank store=barrier_store group_name= _ rendezvous_count=world_size timeout=timeout logging_interval=timeout except torch distributed DistStoreError e assertTrue isinstance e torch distributed DistError error_list append e world_size = error_list = threads = init_type file tcp hash rank range world_size t = threading Thread target=thread_work args= timedelta seconds= init_type world_size rank error_list threads append t t start thread threads thread join we expect world_size- threads have failed assertEqual len error_list world_size - error error_list assertTrue Timed out initializing process group store based barrier error args error_list = threads = Net nn Module __init__ - None super __init__ fc = nn Linear bias=False fc = nn Linear bias=False fc = nn Linear bias=False relu = nn ReLU forward x x = relu fc x x = relu fc x x = fc x F softmax x dim= DoubleGpuNet nn Module __init__ gpus super __init__ fc = nn Linear bias=False gpus fc = nn Linear bias=False gpus fc = nn Linear bias=False gpus relu = nn ReLU no_grad_param = nn Parameter torch tensor long requires_grad=False gpus forward x dev = fc weight device dev = fc weight device x = relu fc x dev x = relu fc x dev x = fc x F softmax x dim= dev QuadraGpuNet nn Module __init__ gpus super __init__ fc = nn Linear bias=False gpus fc = nn Linear bias=False gpus fc = nn Linear bias=False gpus fc = nn Linear bias=False gpus relu = nn ReLU no_grad_param = nn Parameter torch tensor long requires_grad=False gpus forward x dev = fc weight device dev = fc weight device dev = fc weight device dev = fc weight device x = relu fc x dev x = relu fc x dev x = relu fc x dev x = fc x dev F softmax x dim= dev ConvNet nn Module __init__ gpus layouts dtypes super __init__ dtypes = dtypes isinstance gpus list layer_gpus = gpus gpus = gpus conv = torch nn Conv d device=gpus memory_format=layouts dtype=dtypes conv = torch nn Conv d device=gpus memory_format=layouts dtype=dtypes conv = torch nn Conv d device=gpus memory_format=layouts dtype=dtypes conv = torch nn Conv d device=gpus memory_format=layouts dtype=dtypes forward x x = x dtypes Could say x = conv x device=self conv weight device dtype=self dtypes etc But I don t want appeal weights devices directly because part test s purpose verify weights where expected model gets replicated gpus = layer_gpus hasattr layer_gpus x device x = conv x device=gpus dtype=self dtypes x = conv x device=gpus dtype=self dtypes x = conv x device=gpus dtype=self dtypes conv x A model involving FFTs used test DDP complex tensors FFTModel nn Module __init__ hin win n_features super __init__ hin = hin win = win weight = nn Parameter torch ones n_features n_features hin win + dtype=torch cfloat forward x xc = torch fft rfft x s= hin win dim= - - norm= ortho xcw = torch einsum nchw cohw- nohw xc weight x = torch fft irfft xcw dim= - - norm= ortho x Task nn Module __init__ - None super __init__ p = nn Parameter torch ones forward x p + x ModuleForDdpCommHook nn Module __init__ - None super __init__ t = Task forward x rank t x + rank SparseGradientModule nn Module __init__ - None super __init__ embedding = nn EmbeddingBag sparse=True forward x F softmax embedding x dim= CommonDistributedDataParallelTest tearDown DistributedDataParallel test doesn t seem call FileStore destructor TODO investigate test test known have issues Use hack remove files test try os remove file_name except OSError AttributeError pass property world_size _prepare_single_device_module process_group devices device_ids global_batch_size gradient_as_bucket_view=False model = Net device = devices devices torch device f cuda rank d ddp_model = DistributedDataParallel copy deepcopy model device device_ids=device_ids process_group=process_group bucket_cap_mb= gradient_as_bucket_view=gradient_as_bucket_view model device input = torch randn global_batch_size device target = torch randn global_batch_size device model ddp_model input target _prepare_multi_device_module process_group devices device_ids global_batch_size gradient_as_bucket_view=False assertTrue len devices == len devices == f unexpected devices ddp tests devices len devices == model = DoubleGpuNet devices len devices == model = QuadraGpuNet devices ddp_model = DistributedDataParallel copy deepcopy model device_ids=device_ids process_group=process_group bucket_cap_mb= gradient_as_bucket_view=gradient_as_bucket_view input = torch randn global_batch_size devices target = torch randn global_batch_size model ddp_model input target _get_store dist FileStore file_name world_size _get_process_group raise NotImplementedError To implemented child _train_model model input_var target loss run_checkpoint=False use_reentrant=True model train run_checkpoint output = checkpoint model input_var use_reentrant=use_reentrant output = model input_var l = loss output target l backward _test_ddp_checkpointing input_model process_group use_bucket_view find_unused_parameters=False static_graph=False run_checkpoint=False use_reentrant=True allow_none_grads=False reproduce same training results torch accelerator set_device_index rank torch manual_seed model = copy deepcopy input_model device_type ddp_model = copy deepcopy input_model device_type ddp_model = nn parallel DistributedDataParallel ddp_model bucket_cap_mb= gradient_as_bucket_view=use_bucket_view device_ids= rank process_group=process_group find_unused_parameters=find_unused_parameters static_graph=static_graph assertEqual ddp_model _get_ddp_logging_data get static_graph static_graph input ddp_input target ddp_target = _prepare_dummy_data loss = nn MSELoss n_iters = i range n_iters model zero_grad set_to_none=False ddp_model zero_grad set_to_none=False _train_model model input target loss run_checkpoint=run_checkpoint use_reentrant=use_reentrant _train_model ddp_model ddp_input ddp_target loss run_checkpoint=run_checkpoint use_reentrant=use_reentrant i j zip model parameters ddp_model parameters allow_none_grads assertTrue i grad None assertTrue j grad None assertEqual i grad j grad rtol= e- atol= e- A list tests ddp activation checkpointing when gradient_as_bucket_view=True False Most tests referred https github com facebookresearch fairscale blob main tests nn pipe test_checkpoint_ddp py CheckpointOnceModule nn Module Runs checkpoint single layer model __init__ use_reentrant=True super __init__ l = nn Linear l = nn Linear use_reentrant = use_reentrant forward inp x = l inp x = checkpoint l x use_reentrant=self use_reentrant x CheckpointTwiceModule CheckpointOnceModule Runs checkpoint same layer twice model This simulates use cases such pipeline parallel where same layer can checkpointed more than one time __init__ use_reentrant=True super __init__ use_reentrant=use_reentrant forward inp x = l inp x = checkpoint l x use_reentrant=self use_reentrant x = checkpoint l x use_reentrant=self use_reentrant x CheckpointTwiceModuleWeightSharing CheckpointTwiceModule Similar CheckpointTwiceModule weights shared __init__ use_reentrant=True super __init__ use_reentrant=use_reentrant Share weights l weight = l weight forward inp x = l inp x = checkpoint l x use_reentrant=self use_reentrant x = checkpoint l x use_reentrant=self use_reentrant x DynamicCheckpointTwiceModule CheckpointTwiceModule __init__ use_reentrant=True super __init__ use_reentrant=use_reentrant count = forward inp count x = checkpoint l inp use_reentrant=self use_reentrant x = checkpoint l inp use_reentrant=self use_reentrant count += x DynamicCheckpointTwiceModuleWeightSharing DynamicCheckpointTwiceModule __init__ use_reentrant=True super __init__ use_reentrant=use_reentrant Share weights l weight = l weight _prepare_dummy_data ddp_bs = bs = ddp_bs world_size input = torch rand bs device=device_type requires_grad=True target = torch randn bs device=device_type offset = rank ddp_bs ddp_input = input offset offset + ddp_bs ddp_target = target offset offset + ddp_bs input ddp_input target ddp_target skip_if_lt_x_gpu parametrize use_reentrant True False test_ddp_checkpointing_once use_reentrant DDP works expected when layer checkpointed only once process_group = _get_process_group use_bucket_view static_graph product False True False True _test_ddp_checkpointing CheckpointOnceModule use_reentrant=use_reentrant process_group=process_group use_bucket_view=use_bucket_view static_graph=static_graph static_graph find_unused_parameters does make difference since ignored static graph _test_ddp_checkpointing CheckpointOnceModule process_group=process_group use_bucket_view=use_bucket_view static_graph=static_graph find_unused_parameters=True skip_if_lt_x_gpu parametrize use_reentrant True False test_ddp_checkpointing_unused_params use_reentrant With reentrant autograd checkpointing impl DDP will fail when there unused params model no static graph training With non-reentrant checkpointing implementation works expected process_group = _get_process_group use_bucket_view True False err_ctx = nullcontext use_reentrant assertRaisesRegex RuntimeError Expected mark variable ready only once err_ctx _test_ddp_checkpointing CheckpointOnceModule use_reentrant=use_reentrant process_group=process_group use_bucket_view=use_bucket_view find_unused_parameters=True test passes when static_graph true _test_ddp_checkpointing CheckpointOnceModule use_reentrant=use_reentrant process_group=process_group use_bucket_view=use_bucket_view find_unused_parameters=True static_graph=True skip_if_lt_x_gpu parametrize use_reentrant True False test_ddp_checkpointing_twice use_reentrant Checkpointing twice fails non-static graph reentrant checkpoint implementation succeeds non-reentrant checkpoint implementation process_group = _get_process_group use_bucket_view True False err_ctx = nullcontext use_reentrant assertRaisesRegex RuntimeError Expected mark variable ready only once err_ctx _test_ddp_checkpointing CheckpointTwiceModule use_reentrant=use_reentrant process_group=process_group use_bucket_view=use_bucket_view static_graph=False err_ctx _test_ddp_checkpointing CheckpointTwiceModule use_reentrant=use_reentrant process_group=process_group use_bucket_view=use_bucket_view static_graph=False find_unused_parameters=True skip_if_lt_x_gpu parametrize use_reentrant True False test_ddp_checkpointing_twice_static_graph use_reentrant Regardless reentrant non-reentrant checkpointing impl checkpointing twice works static graph enabled process_group = _get_process_group use_bucket_view True False Test passes when static_graph=True _test_ddp_checkpointing CheckpointTwiceModule use_reentrant=use_reentrant process_group=process_group use_bucket_view=use_bucket_view static_graph=True skip_if_lt_x_gpu test_ddp_checkpointing_dynamic_module Dynamic module can checkpointed multiple times non-reentrant checkpointing implementation process_group = _get_process_group use_bucket_view True False _test_ddp_checkpointing DynamicCheckpointTwiceModule use_reentrant=False process_group=process_group use_bucket_view=use_bucket_view static_graph=False find_unused_parameters=True Grads can none sometimes due dynamic module using all params allow_none_grads=True skip_if_lt_x_gpu test_ddp_checkpointing_dynamic_weight_sharing Dynamic module can checkpointed multiple times weight sharing using non-reentrant checkpointing implementation process_group = _get_process_group use_bucket_view True False _test_ddp_checkpointing DynamicCheckpointTwiceModuleWeightSharing use_reentrant=False process_group=process_group use_bucket_view=use_bucket_view static_graph=False find_unused_parameters=True Grads can none sometimes due dynamic module using all params allow_none_grads=True DDP works expected there weight sharing among layers skip_if_lt_x_gpu parametrize use_reentrant True False test_ddp_checkpointing_weight_sharing use_reentrant Test checkpointing weight sharing works process_group = _get_process_group torch accelerator set_device_index rank use_bucket_view static_graph product False True False True torch manual_seed l = nn Linear l = nn Linear l weight = l weight model = nn Sequential l l _test_ddp_checkpointing model process_group=process_group use_bucket_view=use_bucket_view static_graph=static_graph run_checkpoint=True use_reentrant=use_reentrant skip_if_lt_x_gpu test_ddp_checkpointing_twice_weight_sharing Checkpointing should work static graph case checkpointing same layer twice having weights shared across layers process_group = _get_process_group torch accelerator set_device_index rank use_bucket_view True False _test_ddp_checkpointing CheckpointTwiceModuleWeightSharing process_group=process_group use_bucket_view=use_bucket_view static_graph=True test_invalid_powerSGD_state start_powerSGD_iter use_error_feedback warm_start product True False True False use_error_feedback warm_start continue assertRaisesRegex ValueError Expect ` start_powerSGD_iter ` ` use_error_feedback ` ` warm_start ` enabled because PowerSGD can only applied after first two iterations DDP powerSGD PowerSGDState process_group=None matrix_approximation_rank= start_powerSGD_iter=start_powerSGD_iter use_error_feedback=use_error_feedback warm_start=warm_start _test_ddp_with_process_group process_group devices device_ids multi_device=False gradient_as_bucket_view=False Note we pass down ` device_ids ` all way DistributedDataParallel part test Below you find tests either use list integers list ` torch Device ` instances empty list The ` devices ` argument used control placement model must always specified list ` torch Device ` instances local_batch_size = devices None len devices global_batch_size = world_size local_batch_size multi_device model ddp_model input target = _prepare_multi_device_module process_group devices device_ids global_batch_size gradient_as_bucket_view ddp_logging_data = ddp_model _get_ddp_logging_data assertTrue ddp_logging_data get is_multi_device_module model ddp_model input target = _prepare_single_device_module process_group devices device_ids global_batch_size gradient_as_bucket_view ddp_logging_data = ddp_model _get_ddp_logging_data assertFalse ddp_logging_data get is_multi_device_module step_model model input target model train output = model input loss = F mse_loss output target output device loss backward update_parameters model param model parameters torch no_grad param -= param grad param grad = None check two model parameters over iterations iteration range single cpu gpu training step_model model input target DDP training DDP scatters subsets input_cpu nodes GPUs step_model ddp_model input rank local_batch_size rank + local_batch_size target rank local_batch_size rank + local_batch_size Update weights run second iteration shake out errors update_parameters model update_parameters ddp_model assertEqual len list model parameters len list ddp_model parameters i j zip model parameters ddp_model parameters assertEqual i j rtol= e- atol= e- Shuffle input so DDP input different torch manual_seed + iteration input = input torch randperm global_batch_size _gpu_model_with_ddp_comm_hook process_group hook=None gradient_as_bucket_view=False state=None device_id = gpus_for_rank world_size rank gpu_model = DistributedDataParallel ModuleForDdpCommHook device_id device_ids= device_id process_group=process_group gradient_as_bucket_view=gradient_as_bucket_view Register DDP communication hook any hook None gpu_model register_comm_hook state hook gpu_model _gpu_model_with_builtin_ddp_comm_hook process_group hook=None gradient_as_bucket_view=False device_id = gpus_for_rank world_size rank gpu_model = DistributedDataParallel ModuleForDdpCommHook device_id device_ids= device_id process_group=process_group gradient_as_bucket_view=gradient_as_bucket_view Register built-in DDP communication hook defined hook None gpu_model _register_builtin_comm_hook hook gpu_model _run_and_verify_hook model input expected_grad Run forward output = model input rank Run backward output mean backward assertEqual p grad expected_grad p model parameters _simple_hook state object bucket dist GradBucket - torch futures Future torch Tensor fut = torch futures Future fut set_result torch ones_like bucket buffer fut_then fut Add ones fut s result t = fut value t + torch ones_like t fut then fut_then _test_not_nan model x y = model x assertFalse y isnan any item y sum backward p model parameters assertFalse p grad isnan any item skip_if_lt_x_gpu test_sync_batch_norm_only_empty_input pg = _get_process_group model = torch nn Sequential nn BatchNorm d device=self rank model = DistributedDataParallel model device_ids= rank process_group=pg model = nn SyncBatchNorm convert_sync_batchnorm model process_group=pg model train only rank receives empty inputs x = torch zeros rank = dtype=torch float device=self rank input requires grad will trigger collective communication backward pass x requires_grad = True _test_not_nan model x input does requires grad x requires_grad = False _test_not_nan model x all ranks receive empty inputs x = torch zeros dtype=torch float device=self rank input requires grad will trigger collective communication backward pass x requires_grad = True _test_not_nan model x input does requires grad x requires_grad = False _test_not_nan model x skip_if_lt_x_gpu test_sync_batch_norm_empty_input pg = _get_process_group model = torch nn Sequential nn Conv d nn BatchNorm d nn Linear device=self rank model = DistributedDataParallel model device_ids= rank process_group=pg model = nn SyncBatchNorm convert_sync_batchnorm model process_group=pg model train only rank receives empty inputs x = torch zeros rank = dtype=torch float device=self rank _test_not_nan model x all ranks receive empty inputs x = torch zeros dtype=torch float device=self rank _test_not_nan model x dataclass CustomOutput o Optional torch Tensor o dict str torch Tensor DataclassOutputModule nn Module __init__ skip_o super __init__ seq = nn Sequential nn Linear _ range relu = nn ReLU seq = nn Sequential nn Linear _ range skip_o = skip_o forward x o = None skip_o relu seq x o = seq x b relu seq x CommonDistributedDataParallelTest CustomOutput o =o o =o _test_dataclass_output skip_o net_x = torch cat torch ones i i range world_size rank ddp_x = torch ones device=self rank rank use manual_seed make sure local models start same values torch manual_seed net = DataclassOutputModule skip_o =skip_o rank ddp = DistributedDataParallel copy deepcopy net device_ids= rank find_unused_parameters=True static_graph=False process_group=self _get_process_group net_out = net net_x ddp_out = ddp ddp_x net_loss = F mse_loss net_out o + net_out o + net_out o b skip_o net_out o + net_out o b torch ones_like net_out o device=self rank ddp_loss = F mse_loss ddp_out o + ddp_out o + ddp_out o b skip_o ddp_out o + ddp_out o b torch ones_like ddp_out o device=self rank net_loss backward ddp_loss backward p p zip net parameters ddp parameters torch is_tensor p grad assertTrue p grad allclose p grad assertEqual p grad p grad skip_if_lt_x_gpu test_dataclass_output _test_dataclass_output skip_o =False skip_if_lt_x_gpu test_dataclass_output_unused_param _test_dataclass_output skip_o =True ComputeBucketAssignmentTest TestCase test_single_limit_single_dtype tensors = torch empty dtype=torch float torch empty dtype=torch float torch empty dtype=torch float torch empty dtype=torch float result per_bucket_size_limits = dist _compute_bucket_assignment_by_size tensors assertTrue all size_lim == size_lim per_bucket_size_limits assertEqual result test_single_limit_multi_dtype tensors = torch empty dtype=torch float torch empty dtype=torch double torch empty dtype=torch float torch empty dtype=torch double torch empty dtype=torch float torch empty dtype=torch double result per_bucket_size_limits = dist _compute_bucket_assignment_by_size tensors assertTrue all size_lim == size_lim per_bucket_size_limits assertEqual result test_multi_limit_single_dtype tensors = torch empty dtype=torch float torch empty dtype=torch float torch empty dtype=torch float torch empty dtype=torch float result per_bucket_size_limits = dist _compute_bucket_assignment_by_size tensors assertEqual per_bucket_size_limits assertEqual result test_multi_limit_multi_dtype tensors = torch empty dtype=torch float torch empty dtype=torch double torch empty dtype=torch float torch empty dtype=torch double torch empty dtype=torch float torch empty dtype=torch double result per_bucket_size_limits = dist _compute_bucket_assignment_by_size tensors assertEqual result assertEqual per_bucket_size_limits AbstractCommTest property op_timeout_sec property world_size property device fail test subclass didn t override device _verify_sequence_number_across_pg pg verify_pg seq_num = pg _get_sequence_number_for_group obj_list = None _ range dist get_world_size verify_pg We use separate pg verify sequence numbers otherwise these collectives will themselves increment sequence number dist all_gather_object obj_list seq_num group=verify_pg assertEqual len set obj_list obj_list _test_sequence_num_incremented process_group ranks verify initial sequence numbers Use distinct process group verification keep counts expected respect process_group verify_pg = dist new_group ranks=ranks backend= gloo assert dist get_world_size process_group == dist get_world_size verify_pg initial_num = _verify_sequence_number_across_pg pg=process_group verify_pg=verify_pg c d _rank_not_in_group process_group - Verify sequence numbers appropriately incremented i range t = torch ones device=device_type dist all_reduce t group=process_group c d _rank_not_in_group process_group seq_num = _verify_sequence_number_across_pg pg=process_group verify_pg=verify_pg assertEqual initial_num + i + seq_num dist get_world_size process_group Test when certain ranks don t call collectives dist get_rank process_group dist all_reduce t group=process_group async_op=True Now ranks should lagging c d _rank_not_in_group process_group seq_num = process_group _get_sequence_number_for_group rank = dist get_rank process_group obj_list = None _ range dist get_world_size verify_pg dist all_gather_object obj_list rank seq_num group=verify_pg rank_to_seq_num = dict obj_list assertEqual len set rank_to_seq_num values assertEqual rank_to_seq_num rank_to_seq_num expected_same = rank_to_seq_num i i rank_to_seq_num keys i assertEqual len expected_same assertEqual rank_to_seq_num + rank_to_seq_num _test_sequence_num_incremented_default_group backend_name torch accelerator set_device_index rank store = dist FileStore file_name world_size dist init_process_group backend_name world_size=self world_size rank=self rank store=store _test_sequence_num_incremented c d _get_default_group ranks=list range dist get_world_size _test_sequence_num_incremented_subgroup backend_name torch accelerator set_device_index rank store = dist FileStore file_name world_size dist init_process_group backend_name world_size=self world_size rank=self rank store=store subgroup_ranks = subgroup = dist new_group subgroup_ranks _test_sequence_num_incremented subgroup subgroup_ranks _test_sequence_num_set_default_pg backend store = dist FileStore file_name world_size dist init_process_group backend world_size=self world_size rank=self rank store=store default_pg = c d _get_default_group seq_num = default_pg _get_sequence_number_for_group obj_list = None _ range dist get_world_size dist all_gather_object obj_list seq_num assertEqual len set obj_list _test_sequence_num_set_new_group backend store = dist FileStore file_name world_size dist init_process_group backend world_size=self world_size rank=self rank store=store subgroup = dist new_group c d _rank_not_in_group subgroup subgroup_seq = subgroup _get_sequence_number_for_group obj_list = None _ range dist get_world_size subgroup dist all_gather_object obj_list subgroup_seq group=subgroup assertEqual len set obj_list _test_warn_not_in_group backend store = dist FileStore file_name world_size dist init_process_group backend world_size=self world_size rank=self rank store=store in_group_ranks = list filter lambda x x == range world_size group = dist new_group in_group_ranks x = torch zeros rank xs = torch zeros rank _ range len in_group_ranks rank in_group_ranks msg = does belong assertWarnsOnceRegex UserWarning msg format all_gather dist all_gather xs x group=group assertWarnsOnceRegex UserWarning msg format all_reduce dist all_reduce x group=group assertWarnsOnceRegex UserWarning msg format barrier dist barrier group=group assertWarnsOnceRegex UserWarning msg format broadcast dist broadcast x src= group=group dist all_gather xs x group=group dist all_reduce x group=group dist barrier group=group dist broadcast x src= group=group _test_rank_membership backend store = dist FileStore file_name world_size dist init_process_group backend world_size=self world_size rank=self rank store=store assertTrue world_size group = dist new_group ranks= assertEqual dist get_group_rank group assertRaisesRegex ValueError part group dist get_group_rank group assertRaisesRegex ValueError registered dist get_group_rank DummyProcessGroup rank world_size assertEqual dist get_global_rank group assertRaisesRegex ValueError part group dist get_global_rank group assertRaisesRegex ValueError registered dist get_global_rank DummyProcessGroup rank world_size assertEqual dist get_process_group_ranks group _test_tensor_dtype_mismatch backend store = dist FileStore file_name world_size dist init_process_group backend world_size=self world_size rank=self rank store=store tensor = torch ones device=self device tensor_h = tensor half tensor_list = torch zeros device=self device _ range world_size tensor_list_h = list tensor_list tensor_list_h = tensor_list_h half assertRaisesRegex ValueError tensors different dtypes dist all_gather tensor_list_h tensor assertRaisesRegex ValueError tensors different dtypes dist all_gather tensor_list tensor_h assertRaisesRegex ValueError tensors different dtypes dist all_gather_coalesced tensor_list_h tensor_list dist all_gather_coalesced tensor_list tensor_list_h assertRaisesRegex ValueError tensors different dtypes dist all_reduce_coalesced tensor_list_h assertRaisesRegex ValueError tensors different dtypes dist reduce_scatter tensor tensor_list_h assertRaisesRegex ValueError tensors different dtypes dist reduce_scatter tensor_h tensor_list assertRaisesRegex ValueError tensors different dtypes dist all_to_all_single tensor_h tensor assertRaisesRegex ValueError tensors different dtypes dist all_to_all tensor_list_h tensor_list assertRaisesRegex ValueError tensors different dtypes dist all_to_all tensor_list tensor_list_h assertRaisesRegex ValueError tensors different dtypes dist scatter tensor tensor_list_h assertRaisesRegex ValueError tensors different dtypes dist gather tensor_h tensor_list assertRaisesRegex ValueError tensors different dtypes dist gather tensor tensor_list_h assertRaisesRegex ValueError tensors different dtypes dist scatter tensor_h tensor_list _test_tensor_dtype_complex backend store = dist FileStore file_name world_size dist init_process_group backend world_size=self world_size rank=self rank store=store tensor = torch rand device=self device tensor_c = torch view_as_complex tensor tensor_list = torch rand device=self device _ range world_size tensor_list_c = list tensor_list tensor_list_c = torch view_as_complex tensor_list_c dist all_gather tensor_list tensor dist all_gather tensor_list tensor_c dist all_gather tensor_list_c tensor dist all_gather tensor_list_c tensor_c _test_bool_tensors backend store = dist FileStore file_name world_size dist init_process_group backend world_size=self world_size rank=self rank store=store device = cuda backend == nccl xpu backend == xccl cpu test alltoall_base tensor = torch tensor dtype=torch bool device=device zeros = torch tensor dtype=torch bool device=device outensor = zeros rank tensor dist broadcast outensor src= assertEqual outensor tensor Variant AbstractCommTest expects world size AbstractLargeCommTest property op_timeout_sec property world_size property device raise RuntimeError Implement me _test_new_group_local_sync backend store = dist FileStore file_name world_size dist init_process_group backend world_size=self world_size rank=self rank store=store rank = dist get_rank ranks_in = rank rank + world_size ranks_out = i i range world_size i ranks_in assertIn rank ranks_in assertNotIn rank ranks_out assertIsNone dist new_group ranks=ranks_out use_local_synchronization=True new_pg = dist new_group ranks=ranks_in use_local_synchronization=True assertIsInstance new_pg dist ProcessGroup PTD sorts ranks before creating PG so actually gets assigned ranks ranks_in sort assertEqual dist get_group_rank new_pg rank ranks_in index rank assertEqual ranks_in dist get_process_group_ranks new_pg f expecting ranks_in got dist get_process_group_ranks new_pg _test_new_group_local_sync_sanity_check backend store = dist FileStore file_name world_size dist init_process_group backend world_size=self world_size rank=self rank store=store rank = dist get_rank split world PGs rank = dist get_rank pg_idx = rank ranks_in = pg_idx pg_idx + new_pg = dist new_group ranks=ranks_in use_local_synchronization=True input_tensor = torch tensor pg_idx rank device=self device output_tensor_list = torch tensor - - device=self device _ range new_pg size dist all_gather output_tensor_list input_tensor group=new_pg expected = torch tensor pg_idx ranks_in device=self device torch tensor pg_idx ranks_in device=self device assertEqual output_tensor_list expected _test_new_group_local_sync_duplicate_pg backend We should support users create multiple PGs same set members no conflict group name store = dist FileStore file_name world_size dist init_process_group backend world_size=self world_size rank=self rank store=store rank = dist get_rank split world PGs rank = dist get_rank pg_idx = rank ranks_in = pg_idx pg_idx + new_pgs = _ range new_pgs append dist new_group ranks=ranks_in use_local_synchronization=True input_tensor = torch tensor pg_idx rank device=self device new_pg new_pgs output_tensor_list = torch tensor - - device=self device _ range new_pg size dist all_gather output_tensor_list input_tensor group=new_pg expected = torch tensor pg_idx ranks_in device=self device torch tensor pg_idx ranks_in device=self device assertEqual output_tensor_list expected CommTest AbstractCommTest MultiProcessTestCase setUp super setUp _spawn_processes tearDown super tearDown try os remove file_name except OSError pass test_debug_level try del os environ TORCH_DISTRIBUTED_DEBUG except KeyError pass dist set_debug_level_from_env Default should off default_debug_mode = dist get_debug_level assertEqual default_debug_mode dist DebugLevel OFF mapping = OFF dist DebugLevel OFF off dist DebugLevel OFF oFf dist DebugLevel OFF INFO dist DebugLevel INFO info dist DebugLevel INFO INfO dist DebugLevel INFO DETAIL dist DebugLevel DETAIL detail dist DebugLevel DETAIL DeTaIl dist DebugLevel DETAIL invalid_debug_modes = foo - mode mapping keys os environ TORCH_DISTRIBUTED_DEBUG = str mode dist set_debug_level_from_env set_debug_mode = dist get_debug_level assertEqual set_debug_mode mapping mode f Expected mode map mapping mode got set_debug_mode mode invalid_debug_modes os environ TORCH_DISTRIBUTED_DEBUG = str mode assertRaisesRegex ValueError The value TORCH_DISTRIBUTED_DEBUG must dist set_debug_level_from_env DummyWork dist _Work wait timeout= torch accelerator is_available torch accelerator current_stream synchronize True DummyProcessGroup dist ProcessGroup __init__ args kwargs super __init__ args kwargs _bound_device_id = None global_rank = args group_size = args _aborted = False _shutdown = False rank global_rank size group_size property supports_splitting True property bound_device_id _bound_device_id bound_device_id setter bound_device_id device _bound_device_id = device eager_connect_single_device device=None _bound_device_id = device _set_sequence_number_for_group pass _get_backend device comm_split_count perform_nocolor_split device pass getBackendName Dummy allgather output_tensor_lists input_tensor_list opts=None output_tensor_list input_tensor zip output_tensor_lists input_tensor_list output_tensor output_tensor_list output_tensor copy_ input_tensor DummyWork allreduce tensor_list opts=None tensor tensor_list tensor add_ DummyWork barrier opts=None store = c d _get_default_store key = TEST DummyProcessGroup barrier rank == worker_count = By default TCPServer lives rank So rank needs make sure does exit too early before other ranks finish using store Note _store_based_barrier does solve problem all ranks need run least one store add key before exiting there no guarantee rank still alive point while worker_count size - worker_count = store add key store add key DummyWork broadcast tensor_list opts=None tensor tensor_list tensor add_ DummyWork reduce_scatter output_tensor_list input_tensor_lists opts=None output_tensor input_tensor_list zip output_tensor_list input_tensor_lists output_tensor copy_ input_tensor_list rank DummyWork send tensor_list dst tag= tensor tensor_list tensor add_ DummyWork recv tensor_list src tag= tensor tensor_list tensor add_ DummyWork abort - None _aborted = True shutdown - None _shutdown = True PythonProcessGroupExtensionTest MultiProcessTestCase setUp super setUp _spawn_processes tearDown super tearDown try os remove file_name except OSError pass test_get_backend_name dpg = DummyProcessGroup assertEqual Dummy dpg name dist Backend register_backend dummy PythonProcessGroupExtensionTest create_dummy os environ MASTER_ADDR = localhost os environ MASTER_PORT = dist init_process_group cpu dummy rank= world_size= dpg = DummyProcessGroup torch distributed distributed_c d _canonicalize_group_rank assertEqual _canonicalize_group_rank dpg group_rank= return_global=False assertRaises RuntimeError _canonicalize_group_rank dpg group_rank= return_global=True test_canonicalize_helper dist Backend register_backend dummy PythonProcessGroupExtensionTest create_dummy os environ MASTER_ADDR = localhost os environ MASTER_PORT = dist init_process_group dummy rank=self rank world_size=self world_size dpg = DummyProcessGroup torch distributed distributed_c d _canonicalize_group_rank we ensure process group more ranks than default group can still used e g dpg had ranks world had only ranks assertEqual _canonicalize_group_rank dpg group_rank= return_global=False assertEqual _canonicalize_group_rank dpg global_rank= return_global=True assertRaises ValueError TODO whc actually catching wrong error ValueError Group __mp_main__ DummyProcessGroup object x faa registered please create group torch distributed new_group API It should catching different error where rank doesn t exist global mapping But s still testing same part _canonicalize_group_rank helper so maybe fine _canonicalize_group_rank dpg group_rank= return_global=True dist destroy_process_group test_backend_class_attr dist Backend register_backend dummy PythonProcessGroupExtensionTest create_dummy assertEqual dist Backend DUMMY dummy assertEqual dist Backend _plugins DUMMY creator_fn PythonProcessGroupExtensionTest create_dummy test_is_backend_available assertEqual dist is_ucc_available dist is_backend_available ucc assertFalse dist is_backend_available dummy dist Backend register_backend dummy PythonProcessGroupExtensionTest create_dummy assertTrue dist is_backend_available dummy test_backend_config dist Backend register_backend dummy PythonProcessGroupExtensionTest create_dummy Ensure backend config can created following arguments backend_config_strings_and_expected_values = dist Backend GLOO cpu gloo cuda gloo dist Backend NCCL cuda nccl dist Backend MPI cpu mpi cuda mpi dist Backend UCC cpu ucc cuda ucc dist Backend DUMMY cpu dummy cuda dummy DUMMY cpu dummy cuda dummy dummy cpu dummy cuda dummy cpu dummy cuda dummy cpu dummy cuda dummy cpu dummy cuda nccl cpu dummy cuda nccl cpu gloo cuda dummy cpu gloo cuda dummy cpu gloo cuda nccl cpu gloo cuda nccl TEST_XPU Override backend_config_strings_and_expected_values Intel GPU backend_config_strings_and_expected_values = dist Backend DUMMY cpu dummy cuda dummy xpu dummy DUMMY cpu dummy cuda dummy xpu dummy dummy cpu dummy cuda dummy xpu dummy cpu dummy xpu dummy cpu dummy xpu dummy cpu dummy xpu xccl cpu dummy xpu xccl cpu gloo xpu dummy cpu gloo xpu dummy cpu gloo xpu xccl cpu gloo xpu xccl config_str expected_value backend_config_strings_and_expected_values subTest config_str ensures these configs strings valid no ValueError raised config = dist BackendConfig config_str assertEqual str config expected_value Ensure backend config will raise ValueError following arguments invalid_backend_config_strings = cpu gloo cuda nccl trailing comma cpu gloo cuda nccl cpu dummy duplicate device cpu gloo xpu xccl trailing comma cpu gloo xpu xccl cpu dummy duplicate device config_str invalid_backend_config_strings subTest config_str assertRaises ValueError dist BackendConfig config_str test_init_process_group_with_multiple_backends dist Backend register_backend dummy PythonProcessGroupExtensionTest create_dummy os environ MASTER_ADDR = localhost os environ MASTER_PORT = dist init_process_group cpu dummy cuda dummy xpu dummy rank=self rank world_size=self world_size test all_gather input_tensor = torch ones output_tensor_list = torch zeros _ range world_size dist all_gather output_tensor_list input_tensor dist barrier dist destroy_process_group Options group_name = None split_from = None split_color = None global_ranks_in_group = None __init__ - None pass create pass staticmethod create_dummy store group_rank group_size timeout DummyProcessGroup group_rank group_size staticmethod create_dummy_ext dist_opts pg_options=None DummyProcessGroup dist_opts group_rank dist_opts group_size test_collectives dist Backend register_backend dummy PythonProcessGroupExtensionTest create_dummy os environ MASTER_ADDR = localhost os environ MASTER_PORT = dist init_process_group dummy rank=self rank world_size=self world_size test all_gather input_tensor = torch ones output_tensor_list = torch zeros _ range world_size dist all_gather output_tensor_list input_tensor tensor output_tensor_list assertEqual tensor input_tensor test all_reduce input_tensor = torch ones dist all_reduce input_tensor assertEqual input_tensor torch ones + test broadcast input_tensor = torch zeros dist broadcast input_tensor async_op=True wait assertEqual torch ones input_tensor test reduce_scatter output_tensor = torch zeros input_tensor_list = torch ones _ range world_size dist reduce_scatter output_tensor input_tensor_list assertEqual output_tensor torch zeros + dist barrier dist destroy_process_group test_send_recv dist Backend register_backend dummy PythonProcessGroupExtensionTest create_dummy os environ MASTER_ADDR = localhost os environ MASTER_PORT = dist init_process_group dummy rank=self rank world_size=self world_size test send input_tensor = torch zeros dist send input_tensor rank + world_size assertEqual input_tensor torch zeros + assertRaises ValueError dist send input_tensor dist get_rank assertRaises ValueError dist send input_tensor group_dst=dist get_rank assertRaises ValueError dist send input_tensor dist get_rank group_dst=dist get_rank assertRaises ValueError dist send input_tensor test recv input_tensor = torch zeros dist recv input_tensor rank + world_size assertEqual input_tensor torch zeros + assertRaises ValueError dist recv input_tensor src= group_src= dist barrier intentionally calling into ` destroy_process_group ` all user applications would explicitly test_shutdown - None dist Backend register_backend dummy PythonProcessGroupExtensionTest create_dummy os environ MASTER_ADDR = localhost os environ MASTER_PORT = dist init_process_group dummy rank=self rank world_size=self world_size pg = c d _get_default_group dist destroy_process_group assertTrue pg _shutdown test_abort - None dist Backend register_backend dummy PythonProcessGroupExtensionTest create_dummy os environ MASTER_ADDR = localhost os environ MASTER_PORT = dist init_process_group dummy rank=self rank world_size=self world_size pg = c d _get_default_group c d _abort_process_group assertTrue pg _aborted instantiate_parametrized_tests CommonDistributedDataParallelTest ProcessGroupWithDispatchedCollectivesTests MultiProcessTestCase property world_size setUp super setUp _spawn_processes tearDown super tearDown try os remove file_name except OSError pass test_init_process_group_optional_backend store = dist FileStore file_name world_size creates both gloo nccl backend dist is_gloo_available dist is_nccl_available dist init_process_group store=store rank=self rank world_size=self world_size dist destroy_process_group test_init_process_group_for_all_backends backend dist Backend backend_list excepted_backend = backend skip backend available system backend == dist Backend UNDEFINED continue backend == dist Backend MPI dist is_mpi_available continue backend == dist Backend NCCL dist is_nccl_available torch cuda is_available continue backend == dist Backend GLOO dist is_gloo_available continue backend == dist Backend UCC dist is_ucc_available continue backend == dist Backend XCCL dist is_xccl_available continue Multi-threaded PG defined pure python Its pg name does going through Pybind so its backend name still threaded instead custom backend = threaded excepted_backend = custom store = dist FileStore file_name world_size dist init_process_group backend=backend rank=self rank world_size=self world_size store=store pg = c d _get_default_group assertEqual pg rank rank assertEqual pg size world_size assertEqual pg name str excepted_backend dist destroy_process_group unittest skipIf IS_FBCODE IS_SANDCASTLE subprocess test fails fbcode test_default_process_group script = Hide all GPUs os os environ CUDA_VISIBLE_DEVICES = torch torch distributed dist This should initialize CPU even though CUDA-enabled build dist init_process_group rank= world_size= store=dist HashStore try subprocess check_output sys executable -c script stderr=subprocess STDOUT On Windows opening subprocess default CWD makes ` torch ` fail so just set CWD script s directory cwd=os path dirname os path realpath __file__ It ok have extra long timeout here timeout means test failed timeout= except subprocess TimeoutExpired fail msg= Example code timed out See code sample test details except subprocess CalledProcessError e fail f Subprocess failed e output decode utf- _call_collective_with_varying_tensors backend collective args call collective varying tensors ensure tensors correctly dispatched TODO will updated future backend specific device = cuda backend == nccl xpu backend == xccl cpu ensure supported devices cpu cuda succeeds during dispatch call tensor = torch zeros device=torch device device multi tensor collectives collective == dist barrier collective collective dist all_gather dist gather collective tensor tensor args collective == dist scatter collective tensor tensor args collective dist reduce_scatter dist all_to_all gloo does support reduce_scatter all_to_all backend = gloo collective == dist reduce_scatter collective tensor tensor args collective tensor tensor args collective tensor args TODO backend will replaced non specified backend _test_collectives backend store = dist FileStore file_name world_size dist init_process_group backend world_size=self world_size rank=self rank store=store collectives_and_args = dist reduce rank dist broadcast rank dist all_reduce dist all_gather dist reduce_scatter dist barrier dist all_to_all dist scatter collective args collectives_and_args subTest collective=collective args=args _call_collective_with_varying_tensors backend collective args _test_allreduce_coalesced backend store = dist FileStore file_name world_size dist init_process_group backend world_size=self world_size rank=self rank store=store TODO will updated future backend specific device = cuda backend == nccl cpu tensors = torch ones device=torch device device dist all_reduce_coalesced tensors dist ReduceOp SUM tensor tensors assertEqual tensor torch ones world_size _test_all_to_all_single backend store = dist FileStore file_name world_size dist init_process_group backend world_size=self world_size rank=self rank store=store device = cuda backend == nccl xpu backend == xccl cpu test alltoall_base input_tensor = torch ones device=torch device device output_tensor = torch zeros device=torch device device dist all_to_all_single output_tensor input_tensor input_tensor = input_tensor t assertRaisesRegex ValueError Tensors must contiguous dist all_to_all_single output_tensor input_tensor ReduceOpTest TestCase Ref https github com pytorch pytorch issues test_op_isinstance_of_reduceop reduce_op c d ReduceOp SUM c d ReduceOp AVG c d ReduceOp PRODUCT c d ReduceOp MIN c d ReduceOp MAX c d ReduceOp BAND c d ReduceOp BOR c d ReduceOp BXOR assertTrue isinstance reduce_op c d ReduceOp scale torch tensor assertTrue isinstance dist _make_nccl_premul_sum scale c d ReduceOp Ref https github com pytorch pytorch pull #discussion_r test_reduceop_copyable reduce_op c d ReduceOp SUM c d ReduceOp AVG c d ReduceOp PRODUCT c d ReduceOp MIN c d ReduceOp MAX c d ReduceOp BAND c d ReduceOp BOR c d ReduceOp BXOR assertEqual copy copy reduce_op reduce_op assertEqual copy deepcopy reduce_op reduce_op assertEqual copy copy c d ReduceOp reduce_op reduce_op assertEqual copy deepcopy c d ReduceOp reduce_op reduce_op scale torch tensor reduce_op = dist _make_nccl_premul_sum scale assertEqual copy copy reduce_op reduce_op assertEqual copy deepcopy reduce_op reduce_op test_reduceop_pickle reduce_op c d ReduceOp SUM c d ReduceOp AVG c d ReduceOp PRODUCT c d ReduceOp MIN c d ReduceOp MAX c d ReduceOp BAND c d ReduceOp BOR c d ReduceOp BXOR pickle loads pickle dumps reduce_op orig = c d ReduceOp reduce_op assertEqual pickle loads pickle dumps orig orig scale torch tensor reduce_op = dist _make_nccl_premul_sum scale assertEqual pickle loads pickle dumps reduce_op reduce_op Ref https github com pytorch pytorch issues test_reduceop_equal not_reduceop = abc reduce_op c d ReduceOp SUM c d ReduceOp AVG c d ReduceOp PRODUCT c d ReduceOp MIN c d ReduceOp MAX c d ReduceOp BAND c d ReduceOp BOR c d ReduceOp BXOR reduce_op_obj = c d ReduceOp reduce_op calls ` ReduceOp __eq__ other ` assertEqual reduce_op_obj reduce_op_obj assertEqual reduce_op_obj reduce_op assertNotEqual reduce_op_obj not_reduceop assertNotEqual reduce_op not_reduceop TODO crcrpar This needs ` assertEqual ` associativity even though comparison ` RedOpType ` ` ReduceOp ` sounds less likely happen compared ` ReduceOp ` ` RedOptype ` calls ` RedOpType __eq__ other ` assertNotEqual reduce_op reduce_op_obj assertFalse None reduce_op reduce_op_obj assertFalse not_reduceop reduce_op reduce_op_obj LocalRankTest MultiProcessTestCase property world_size setUp super setUp _spawn_processes tearDown super tearDown try os remove file_name except OSError pass testWithoutEnv assertRaisesRegex RuntimeError LOCAL_RANK dist get_node_local_rank testWithoutEnvWithFallback assertEqual dist get_node_local_rank fallback_rank= testNodeLocalRankOverridesFallback os environ LOCAL_RANK = str rank assertEqual dist get_node_local_rank fallback_rank= rank testNodeLocalRank os environ LOCAL_RANK = str rank assertEqual dist get_node_local_rank rank __name__ == __main__ device_type = cpu assert torch get_device_module _initialized f test_distributed must have initialized device_type context main process run_tests