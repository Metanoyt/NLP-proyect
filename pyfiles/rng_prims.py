mypy allow-untyped-defs typing cast Optional torch torch utils _pytree pytree torch _prims torch _C DispatchKey torch _higher_order_ops utils autograd_not_implemented torch _ops HigherOrderOperator torch _prims_common CUDARngStateHelper make_contiguous_strides_for torch _subclasses fake_tensor FakeTensorMode torch fx experimental proxy_tensor disable_proxy_modes_tracing ProxyTorchDispatchMode track_tensor_tree torch types _device _dtype throw_on_non_cuda device raise RuntimeError f You trying functionalize device type RNG operator device type does f use Philox counter-based RNG Therefore functionalizing device type RNG operator supported We discussing possibility Philox-based RNG implementation CPU register_rng_prim name schema impl_aten impl_meta doc tags=None rngprim_def = torch library custom_op rngprims + name impl_aten mutates_args= schema=schema pyrefly ignore missing-attribute rngprim_def register_fake impl_meta prim_packet = getattr torch _ops ops rngprims name prim = prim_packet default tags prim _tags = tags p prim_packet prim p __doc__ = doc p return_type = torch _prims_common RETURN_TYPE NEW type ignore attr-defined p schema = name + schema p impl_aten = impl_aten p prim_meta_impl = impl_meta Philox rand offsets could shared future other philox ops so keeping these functions global scope philox_rand_offset_meta shape torch Size _prims TensorLike torch tensor dtype=torch int philox_rand_offset shape torch Size For impl look function calc_execution_policy file aten src ATen native cuda DistributionTemplates h The impl copied commit hash aa bd d eb fa f d dfb numel_scalar = dim_size shape numel_scalar = dim_size numel = torch scalar_tensor numel_scalar dtype=torch int block_size = unroll = curand _engine_calls = device_property = torch cuda get_device_properties torch cuda current_device blocks_per_sm = device_property max_threads_per_multi_processor block_size num = cast int numel grid_size = num + block_size - block_size grid_size = min grid_size device_property multi_processor_count blocks_per_sm num - block_size grid_size unroll + curand _engine_calls register_philox_rand name = philox_rand schema = SymInt size Tensor seed Tensor offset int stride Device device=None ScalarType dtype=None - Tensor Tensor noqa B _philox_rand_meta shape torch Size seed torch Tensor offset torch Tensor stride Optional tuple int device _device dtype _dtype stride arg will useful distributed usecase Currently its unused assert stride None stride = make_contiguous_strides_for shape random_values = _prims TensorMeta shape=shape strides=stride dtype=dtype device=device offset = philox_rand_offset_meta shape random_values offset _philox_rand shape torch Size seed torch Tensor offset torch Tensor stride Optional tuple int device _device dtype _dtype stride arg will useful distributed usecase Currently its unused assert stride None device type == cpu devices = devices = device device type = cuda raise throw_on_non_cuda device torch random fork_rng devices CUDARngStateHelper set_torch_state_tensor seed offset random_values = torch rand shape device=device dtype=dtype random_values philox_rand_offset shape register_rng_prim name=name schema=schema impl_aten=_philox_rand impl_meta=_philox_rand_meta doc= Philox based stateless rand operator tags= torch Tag nondeterministic_seeded get_device args kwargs kwargs get device device = kwargs get device isinstance device str device = torch device device device type devices = arg device type arg args isinstance arg torch Tensor any dev == cuda dev devices cuda any dev == xpu dev devices xpu any dev == hpu dev devices hpu any dev == cpu dev devices cpu None register_run_and_save_rng_state_op RunAndSaveRngState HigherOrderOperator __init__ super __init__ run_and_save_rng_state __call__ op args kwargs super __call__ op args kwargs run_and_save_rng_state = RunAndSaveRngState run_and_save_rng_state py_impl DispatchKey Autograd autograd_not_implemented run_and_save_rng_state deferred_error=True run_and_save_rng_state py_impl DispatchKey CUDA impl_cuda op args kwargs torch cuda get_rng_state op args kwargs run_and_save_rng_state py_impl DispatchKey CPU impl_cpu op args kwargs torch get_rng_state op args kwargs run_and_save_rng_state py_impl DispatchKey HPU impl_hpu op args kwargs hasattr torch hpu torch hpu get_rng_state op args kwargs raise RuntimeError functionalize hpu RNG operator supported run_and_save_rng_state py_impl DispatchKey XPU impl_xpu op args kwargs torch xpu get_rng_state op args kwargs run_and_save_rng_state py_impl DispatchKey BackendSelect impl_backend_select op args kwargs impl_map = cuda impl_cuda cpu impl_cpu hpu impl_hpu xpu impl_xpu device = get_device args kwargs assert device impl_map f Backend supported device impl = impl_map device impl op args kwargs run_and_save_rng_state py_impl FakeTensorMode impl_fake_tensor_mode mode op args kwargs Check device call right impl mode impl_backend_select op args kwargs run_and_save_rng_state py_impl ProxyTorchDispatchMode impl_proxy_dispatch_mode mode op args kwargs out = impl_backend_select op args kwargs proxy_args = pytree tree_map mode tracer unwrap_proxy op args proxy_kwargs = pytree tree_map mode tracer unwrap_proxy kwargs out_proxy = mode tracer create_proxy call_function run_and_save_rng_state proxy_args proxy_kwargs track_tensor_tree out out_proxy constant=None tracer=mode tracer run_and_save_rng_state register_run_with_rng_state_op RunWithRngState HigherOrderOperator __init__ super __init__ run_with_rng_state __call__ rng_state op args kwargs super __call__ rng_state op args kwargs run_with_rng_state = RunWithRngState run_with_rng_state py_impl DispatchKey Autograd autograd_not_implemented run_with_rng_state deferred_error=True run_with_rng_state py_impl DispatchKey CUDA impl_cuda rng_state op args kwargs current_state = torch cuda get_rng_state torch cuda set_rng_state rng_state cpu out = op args kwargs torch cuda set_rng_state current_state out run_with_rng_state py_impl DispatchKey CPU impl_cpu rng_state op args kwargs current_state = torch get_rng_state torch set_rng_state rng_state out = op args kwargs torch set_rng_state current_state out run_with_rng_state py_impl DispatchKey HPU impl_hpu rng_state op args kwargs hasattr torch hpu current_state = torch hpu get_rng_state torch hpu set_rng_state rng_state out = op args kwargs torch hpu set_rng_state current_state out raise RuntimeError functionalize hpu RNG operator supported run_with_rng_state py_impl DispatchKey XPU impl_xpu rng_state op args kwargs current_state = torch xpu get_rng_state torch xpu set_rng_state rng_state out = op args kwargs torch xpu set_rng_state current_state out run_with_rng_state py_impl ProxyTorchDispatchMode impl_proxy_dispatch_mode mode rng_state op args kwargs TODO you don t need do dispatch here already disabled disable_proxy_modes_tracing out = run_with_rng_state rng_state op args kwargs proxy_args = pytree tree_map mode tracer unwrap_proxy rng_state op args proxy_kwargs = pytree tree_map mode tracer unwrap_proxy kwargs out_proxy = mode tracer create_proxy call_function run_with_rng_state proxy_args proxy_kwargs track_tensor_tree out out_proxy constant=None tracer=mode tracer run_with_rng_state py_impl DispatchKey BackendSelect impl_backend_select rng_state op args kwargs impl_map = cuda impl_cuda cpu impl_cpu hpu impl_hpu xpu impl_xpu device = get_device args kwargs assert device impl_map f Backend supported device impl = impl_map device impl rng_state op args kwargs run_with_rng_state py_impl FakeTensorMode impl_fake_tensor_mode mode rng_state op args kwargs Skip setting set_rng_state does work well fake tensors And does matter fake tensor mode mode op args kwargs run_with_rng_state py_functionalize_impl impl_functional ctx rng_state op args kwargs unwrapped_rng_state = ctx unwrap_tensors rng_state unwrapped_args = ctx unwrap_tensors args unwrapped_kwargs = ctx unwrap_tensors kwargs ctx redispatch_to_next out = run_with_rng_state unwrapped_rng_state op unwrapped_args unwrapped_kwargs ctx wrap_tensors out run_with_rng_state run_and_save_rng_state = register_run_and_save_rng_state_op run_with_rng_state = register_run_with_rng_state_op register_graphsafe_run_with_rng_state_op GraphSafeRunWithRngState HigherOrderOperator __init__ super __init__ graphsafe_run_with_rng_state __call__ op args rng_state=None kwargs super __call__ op args rng_state=rng_state kwargs graphsafe_run_with_rng_state = GraphSafeRunWithRngState graphsafe_run_with_rng_state py_impl DispatchKey Autograd autograd_not_implemented graphsafe_run_with_rng_state deferred_error=True graphsafe_run_with_rng_state py_impl DispatchKey CUDA impl_cuda op args rng_state=None kwargs pyrefly ignore missing-attribute device_idx = rng_state device index generator = torch cuda default_generators device_idx current_state = generator graphsafe_get_state pyrefly ignore bad-argument-type generator graphsafe_set_state rng_state out = op args kwargs generator graphsafe_set_state current_state out graphsafe_run_with_rng_state py_impl DispatchKey BackendSelect impl_backend_select op args rng_state=None kwargs device = get_device args kwargs assert device == cuda f GraphSafe RNG operations only supported CUDA got device impl_cuda op args rng_state=rng_state kwargs graphsafe_run_with_rng_state py_impl FakeTensorMode impl_fake_tensor_mode mode op args rng_state=None kwargs mode op args kwargs graphsafe_run_with_rng_state py_impl ProxyTorchDispatchMode impl_proxy_dispatch_mode mode op args rng_state=None kwargs disable_proxy_modes_tracing out = graphsafe_run_with_rng_state op args rng_state=rng_state kwargs proxy_args = pytree tree_map mode tracer unwrap_proxy op args proxy_kwargs = pytree tree_map mode tracer unwrap_proxy rng_state rng_state kwargs out_proxy = mode tracer create_proxy call_function graphsafe_run_with_rng_state proxy_args proxy_kwargs track_tensor_tree out out_proxy constant=None tracer=mode tracer graphsafe_run_with_rng_state py_functionalize_impl impl_functional ctx op args rng_state=None kwargs unwrapped_rng_state = ctx unwrap_tensors rng_state rng_state None None unwrapped_args = ctx unwrap_tensors args unwrapped_kwargs = ctx unwrap_tensors kwargs ctx redispatch_to_next out = graphsafe_run_with_rng_state op unwrapped_args rng_state=unwrapped_rng_state unwrapped_kwargs ctx wrap_tensors out graphsafe_run_with_rng_state graphsafe_run_with_rng_state = register_graphsafe_run_with_rng_state_op register_rng_prims register_philox_rand