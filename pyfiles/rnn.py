warnings collections abc Callable Iterable typing Any NamedTuple Optional overload TypeVar Union typing_extensions Self torch torch _VF Tensor __all__ = PackedSequence invert_permutation pack_padded_sequence pad_packed_sequence pad_sequence unpad_sequence pack_sequence unpack_sequence _T = TypeVar _T _R = TypeVar _R PackedSequence_ NamedTuple data torch Tensor batch_sizes torch Tensor sorted_indices Optional torch Tensor unsorted_indices Optional torch Tensor bind optional Optional _T fn Callable _T _R - Optional _R optional None None fn optional PackedSequence PackedSequence_ r Holds data list attr ` batch_sizes ` packed sequence All RNN modules accept packed sequences inputs Note Instances should never created manually They meant instantiated functions like func ` pack_padded_sequence ` Batch sizes represent number elements each sequence step batch varying sequence lengths passed func ` pack_padded_sequence ` For instance given data ` ` abc ` ` ` ` x ` ` ` PackedSequence ` would contain data ` ` axbc ` ` ` ` batch_sizes= ` ` Attributes data Tensor Tensor containing packed sequence batch_sizes Tensor Tensor integers holding information about batch size each sequence step sorted_indices Tensor optional Tensor integers holding how ` PackedSequence ` constructed sequences unsorted_indices Tensor optional Tensor integers holding how recover original sequences correct order note attr ` data ` can arbitrary device arbitrary dtype attr ` sorted_indices ` attr ` unsorted_indices ` must ` ` torch int ` ` tensors same device attr ` data ` However attr ` batch_sizes ` should always CPU ` ` torch int ` ` tensor This invariant maintained throughout ` PackedSequence ` all functions construct ` PackedSequence ` PyTorch i e they only pass tensors conforming constraint __new__ cls data Tensor batch_sizes Optional Tensor = None sorted_indices Optional Tensor = None unsorted_indices Optional Tensor = None - Self super __new__ cls _packed_sequence_init_args data batch_sizes sorted_indices unsorted_indices NOTE device dtype PackedSequence See note above doc string starting attr ` data ` can arbitrary device pin_memory - Self Why convert ` batch_sizes ` See NOTE device dtype PackedSequence type data pin_memory batch_sizes bind sorted_indices lambda t t pin_memory bind unsorted_indices lambda t t pin_memory overload dtype torch dtype non_blocking bool = copy bool = - Self overload device Optional Union str torch device int = dtype Optional torch dtype = non_blocking bool = copy bool = - Self overload other Tensor non_blocking bool = copy bool = - Self args Any kwargs Any - Self r Perform dtype device conversion ` data ` It has similar signature meth ` torch Tensor ` except optional arguments like ` non_blocking ` ` copy ` should passed kwargs args they will apply index tensors note If ` ` data ` ` Tensor already has correct ` torch dtype ` ` torch device ` then ` ` ` ` returned Otherwise returns copy desired configuration Why convert ` batch_sizes ` See NOTE device dtype PackedSequence data = data args kwargs data data Does forward device dtype arg kwargs device set data device kwargs = dict filter lambda t t = device t = dtype kwargs items sorted_indices = bind sorted_indices lambda t t data device kwargs unsorted_indices = bind unsorted_indices lambda t t data device kwargs type data batch_sizes sorted_indices unsorted_indices cuda args Any kwargs Any - Self Tests see cuda should added kwargs ex = torch tensor dtype=self data dtype device=self data device args kwargs ex is_cuda args kwargs kwargs device = cuda args kwargs cpu args Any kwargs Any - Self ex = torch tensor dtype=self data dtype device=self data device args kwargs ex device type == cpu args kwargs kwargs device = cpu args kwargs double - Self dtype=torch double float - Self dtype=torch float half - Self dtype=torch half long - Self dtype=torch long int - Self dtype=torch int short - Self dtype=torch short char - Self dtype=torch int byte - Self dtype=torch uint property is_cuda - bool r Return true ` data ` stored gpu data is_cuda is_pinned - bool r Return true ` data ` stored pinned memory data is_pinned TorchScript doesn t support constructors named tuples so we use helper method construct PackedSequence _packed_sequence_init_args data Tensor batch_sizes Optional Tensor = None sorted_indices Optional Tensor = None unsorted_indices Optional Tensor = None - tuple Tensor Tensor Optional Tensor Optional Tensor NB unsorted_indices provided should inverse permutation sorted_indices Don t assert here because PackedSequence ctor should only used internally unsorted_indices None unsorted_indices = invert_permutation sorted_indices support being called ` PackedSequence data batch_sizes sorted_indices ` batch_sizes None TODO Re-enable check type isn t supported TorchScript batch_sizes device type = cpu raise ValueError batch_sizes should always CPU Instances PackedSequence should never created manually They should instantiated functions like pack_sequence pack_padded_sequences nn utils rnn https pytorch org docs stable nn html#torch nn utils rnn pack_sequence data batch_sizes sorted_indices unsorted_indices support being called ` PackedSequence data batch_sizes sorted_indices ` assert isinstance data list tuple len data == data data sorted_indices unsorted_indices _packed_sequence_init data Tensor batch_sizes Optional Tensor = None sorted_indices Optional Tensor = None unsorted_indices Optional Tensor = None - PackedSequence data batch_sizes sorted_indices unsorted_indices = _packed_sequence_init_args data batch_sizes sorted_indices unsorted_indices PackedSequence data batch_sizes sorted_indices unsorted_indices invert_permutation permutation Optional Tensor - Optional Tensor Returns inverse ` ` permutation ` ` This useful converting between sorted unsorted indices ` ~nn utils rnn PackedSequence ` Args permutation Tensor optional -D tensor indices invert permutation None None output = torch empty_like permutation memory_format=torch legacy_contiguous_format output scatter_ permutation torch arange permutation numel device=permutation device output pack_padded_sequence input Tensor lengths Union Tensor list int batch_first bool = False enforce_sorted bool = True - PackedSequence r Packs Tensor containing padded sequences variable length attr ` input ` can size ` ` T x B x ` ` attr ` batch_first ` ` ` False ` ` ` ` B x T x ` ` attr ` batch_first ` ` ` True ` ` where ` ` T ` ` length longest sequence ` ` B ` ` batch size ` ` ` ` any number dimensions including For unsorted sequences use ` enforce_sorted = False ` If attr ` enforce_sorted ` ` ` True ` ` sequences should sorted length decreasing order i e ` ` input ` ` should longest sequence ` ` input B- ` ` shortest one ` enforce_sorted = True ` only necessary ONNX export It inverse operation func ` pad_packed_sequence ` hence func ` pad_packed_sequence ` can used recover underlying tensor packed ` PackedSequence ` Note This function accepts any input has least two dimensions You can apply pack labels use output RNN them compute loss directly A Tensor can retrieved ` PackedSequence ` object accessing its ` ` data ` ` attribute Args input Tensor padded batch variable length sequences lengths Tensor list int list sequence lengths each batch element must CPU provided tensor batch_first bool optional ` ` True ` ` input expected ` ` B x T x ` ` format ` ` T x B x ` ` otherwise Default ` ` False ` ` enforce_sorted bool optional ` ` True ` ` input expected contain sequences sorted length decreasing order If ` ` False ` ` input will get sorted unconditionally Default ` ` True ` ` warning The dim ` ` input ` ` tensor will truncated its length larger than correspond value ` ` length ` ` Returns ` PackedSequence ` object isinstance lengths torch Tensor torch _C _get_tracing_state warnings warn pack_padded_sequence has been called Python list sequence lengths The tracer cannot track data flow Python values will treat them constants likely rendering trace incorrect any other combination lengths stacklevel= lengths = torch as_tensor lengths dtype=torch int device= cpu lengths = lengths dtype=torch int enforce_sorted sorted_indices = None lengths sorted_indices = torch sort lengths descending=True sorted_indices = sorted_indices input device batch_dim = batch_first input = input index_select batch_dim sorted_indices data batch_sizes = _VF _pack_padded_sequence input lengths batch_first _packed_sequence_init data batch_sizes sorted_indices None pad_packed_sequence sequence PackedSequence batch_first bool = False padding_value float = total_length Optional int = None - tuple Tensor Tensor r Pad packed batch variable length sequences It inverse operation func ` pack_padded_sequence ` The returned Tensor s data will size ` ` T x B x ` ` attr ` batch_first ` ` ` False ` ` ` ` B x T x ` ` attr ` batch_first ` ` ` True ` ` where ` ` T ` ` length longest sequence ` ` B ` ` batch size Example torch nn utils rnn pack_padded_sequence pad_packed_sequence seq = torch tensor lens = packed = pack_padded_sequence seq lens batch_first=True enforce_sorted=False packed PackedSequence data=tensor batch_sizes=tensor sorted_indices=tensor unsorted_indices=tensor seq_unpacked lens_unpacked = pad_packed_sequence packed batch_first=True seq_unpacked tensor lens_unpacked tensor note attr ` total_length ` useful implement ` ` pack sequence - recurrent network - unpack sequence ` ` pattern ` ~torch nn Module ` wrapped ` ~torch nn DataParallel ` See ref ` FAQ section pack-rnn-unpack-with-data-parallelism ` details Args sequence PackedSequence batch pad batch_first bool optional ` ` True ` ` output will ` ` B x T x ` ` format ` ` T x B x ` ` otherwise padding_value float optional values padded elements total_length int optional ` ` None ` ` output will padded have length attr ` total_length ` This method will throw ` ValueError ` attr ` total_length ` less than max sequence length attr ` sequence ` Returns Tuple Tensor containing padded sequence Tensor containing list lengths each sequence batch Batch elements will re-ordered they ordered originally when batch passed ` ` pack_padded_sequence ` ` ` ` pack_sequence ` ` max_seq_length = sequence batch_sizes size total_length None total_length max_seq_length raise ValueError Expected total_length least length longest sequence input got f total_length= total_length max sequence length being max_seq_length max_seq_length = total_length padded_output lengths = _VF _pad_packed_sequence sequence data sequence batch_sizes batch_first padding_value max_seq_length unsorted_indices = sequence unsorted_indices unsorted_indices None batch_dim = batch_first padded_output index_select batch_dim unsorted_indices lengths unsorted_indices cpu padded_output lengths NOTE JIT-compatibility we need more restrictive here use specific types instead Iterable pad_sequence sequences Union Tensor list Tensor batch_first bool = False padding_value float = padding_side str = right - Tensor r Pad list variable length Tensors attr ` padding_value ` ` ` pad_sequence ` ` stacks list Tensors along new dimension pads them equal length attr ` sequences ` can list sequences size ` ` L x ` ` where ` L ` length sequence ` ` ` ` any number dimensions including ` ` ` ` If attr ` batch_first ` ` ` False ` ` output size ` ` T x B x ` ` ` ` B x T x ` ` otherwise where ` ` B ` ` batch size number elements attr ` sequences ` ` ` T ` ` length longest sequence Example torch nn utils rnn pad_sequence = torch ones b = torch ones c = torch ones pad_sequence b c size torch Size Note This function returns Tensor size ` ` T x B x ` ` ` ` B x T x ` ` where ` T ` length longest sequence This function assumes trailing dimensions type all Tensors sequences same Args sequences list Tensor list variable length sequences batch_first bool optional ` ` True ` ` output will ` ` B x T x ` ` format ` ` T x B x ` ` otherwise padding_value float optional value padded elements Default ` ` ` ` padding_side str optional side pad sequences Default ` ` right ` ` Returns Tensor size ` ` T x B x ` ` attr ` batch_first ` ` ` False ` ` Tensor size ` ` B x T x ` ` otherwise torch jit is_tracing torch jit is_scripting JIT doesn t support ` Iterable ` isinstance sequences Iterable msg = pad_sequence Expected iterable input sequences got arg type f type sequences raise RuntimeError msg In JIT context leads RuntimeError cannot statically infer expected size list context sequences = tuple sequences type ignore assignment For JIT we only support Union Tensor Tuple Tensor isinstance sequences torch Tensor sequences = sequences unbind type ignore assignment assuming trailing dimensions type all Tensors sequences same fetching those sequences torch _C _nn pad_sequence sequences type ignore arg-type batch_first padding_value padding_side type ignore arg-type unpad_sequence padded_sequences Tensor lengths Tensor batch_first bool = False - list Tensor r Unpad padded Tensor into list variable length Tensors ` ` unpad_sequence ` ` unstacks padded Tensor into list variable length Tensors Example torch nn utils rnn pad_sequence unpad_sequence = torch ones b = torch ones c = torch ones sequences = b c padded_sequences = pad_sequence sequences lengths = torch as_tensor v size v sequences unpadded_sequences = unpad_sequence padded_sequences lengths torch allclose sequences unpadded_sequences True torch allclose sequences unpadded_sequences True torch allclose sequences unpadded_sequences True Args padded_sequences Tensor padded sequences lengths Tensor length original unpadded sequences batch_first bool optional whether batch dimension first Default ` ` False ` ` Returns list ` Tensor ` objects unpadded_sequences = batch_first padded_sequences transpose_ max_length = padded_sequences shape idx = torch arange max_length device=lengths device seq length zip padded_sequences lengths strict=True mask = idx length unpacked_seq = seq mask unpadded_sequences append unpacked_seq unpadded_sequences pack_sequence sequences list Tensor enforce_sorted bool = True - PackedSequence r Packs list variable length Tensors Consecutive call next functions ` ` pad_sequence ` ` ` ` pack_padded_sequence ` ` ` ` sequences ` ` should list Tensors size ` ` L x ` ` where ` L ` length sequence ` ` any number trailing dimensions including ` ` ` ` For unsorted sequences use ` enforce_sorted = False ` If ` ` enforce_sorted ` ` ` ` True ` ` sequences should sorted order decreasing length ` ` enforce_sorted = True ` ` only necessary ONNX export Example torch nn utils rnn pack_sequence = torch tensor b = torch tensor c = torch tensor pack_sequence b c PackedSequence data=tensor batch_sizes=tensor sorted_indices=None unsorted_indices=None Args sequences list Tensor A list sequences decreasing length enforce_sorted bool optional ` ` True ` ` checks input contains sequences sorted length decreasing order If ` ` False ` ` condition checked Default ` ` True ` ` Returns ` PackedSequence ` object lengths = torch as_tensor v size v sequences pack_padded_sequence pad_sequence sequences lengths enforce_sorted=enforce_sorted unpack_sequence packed_sequences PackedSequence - list Tensor r Unpack PackedSequence into list variable length Tensors ` ` packed_sequences ` ` should PackedSequence object Example torch nn utils rnn pack_sequence unpack_sequence = torch tensor b = torch tensor c = torch tensor sequences = b c print sequences tensor tensor tensor packed_sequences = pack_sequence sequences print packed_sequences PackedSequence data=tensor batch_sizes=tensor sorted_indices=None unsorted_indices=None unpacked_sequences = unpack_sequence packed_sequences print unpacked_sequences tensor tensor tensor Args packed_sequences PackedSequence A PackedSequence object Returns list ` Tensor ` objects padded_sequences lengths = pad_packed_sequence packed_sequences batch_first=True unpacked_sequences = unpad_sequence padded_sequences lengths batch_first=True unpacked_sequences