mypy allow-untyped-decorators mypy allow-untyped-defs functools itertools logging operator collections Counter defaultdict typing Any Callable TypeVar typing_extensions ParamSpec torch torch _inductor inductor torch utils _pytree pytree torch fx torch _decomp register_decomposition torch _dynamo utils counters torch _inductor comms torch _inductor virtualized ops torch _logging trace_structured torch _prims_common is_boolean_dtype is_expandable_to is_integer_dtype torch fx experimental symbolic_shapes statically_known_true sym_eq torch utils _ordered_set OrderedSet config ir pattern_matcher codegen common custom_backend_passes comms remove_fsdp _unsharded_param_graph_input_usage fx_utils FakeTensorUpdater get_fake_args_kwargs get_node_storage lowering lowerings L pattern_matcher _return_true Arg CallFunction CallFunctionVarArgs filter_nodes fwd_only get_arg_value get_mutation_region_id Ignored init_once_fakemode KeywordArg ListOf Match MultiOutputPattern MULTIPLE PatternMatcherPass PatternMatcherPassBase register_graph_pattern register_replacement stable_topological_sort utils decode_device get_all_devices get_gpu_type is_gpu is_pointwise_use OPTIMUS_EXCLUDE_POST_GRAD virtualized V b b_gemm B B_GEMM_PASS ddp_fusion fuse_ddp_communication group_batch_fusion group_batch_fusion_passes POST_GRAD_FUSIONS micro_pipeline_tp micro_pipeline_tp_pass pre_grad is_same_dict save_inductor_dict reinplace reinplace_inplaceable_ops split_cat POST_GRAD_PATTERNS _T = TypeVar _T _P = ParamSpec _P PatternMatcherPass = functools partial PatternMatcherPassBase subsystem= post_grad_passes log = logging getLogger __name__ aten = torch ops aten prims = torch ops prims First pass_patterns applied then then pass_patterns = PatternMatcherPass PatternMatcherPass PatternMatcherPass post_grad_passes gm torch fx GraphModule is_inference bool Passes run after grad This called once forwards graph once backwards graph The IR here has been normalized functionalized GraphTransformObserver = functools partial torch fx passes graph_transform_observer GraphTransformObserver subsystem= post_grad_passes torch _dynamo config skip_fsdp_hooks remove_fsdp _unsharded_param_graph_input_usage gm graph config dce has some issues mutation inference mode gm graph eliminate_dead_code is_inference config reorder_for_locality GraphTransformObserver gm reorder_for_locality apply_graph_pass reorder_for_locality fake_tensor_updater = FakeTensorUpdater gm graph post_grad_custom_pre_pass = config post_grad_custom_pre_pass GraphTransformObserver gm post_grad_custom_pre_pass apply_graph_pass post_grad_custom_pre_pass torch _C _has_mkldnn config cpp enable_grouped_gemm_template config max_autotune CPP config max_autotune_gemm_backends mkldnn_fusion grouped_gemm_pass grouped_gemm_pass gm graph config cpp enable_concat_linear quantization concat_linear_woq_int Concat linear optimization WOQ int concat_linear_woq_int gm config pattern_matcher lazy_init GraphTransformObserver gm post_grad_custom_pre_pass apply_graph_pass functools partial group_batch_fusion_passes pre_grad=False GraphTransformObserver gm remove_noop_ops apply_graph_pass remove_noop_ops GraphTransformObserver gm remove_assert_ops apply_graph_pass remove_assert_ops i patterns enumerate pass_patterns GraphTransformObserver gm f pass_pattern_ i apply_graph_pass patterns apply pass_name config post_grad_fusion_options skip all patterns group batch fusions quantization patterns pass_name POST_GRAD_FUSIONS pass_name OPTIMUS_EXCLUDE_POST_GRAD continue pattern_matcher_pass = POST_GRAD_PATTERNS pass_name inductor_before_change = save_inductor_dict pattern_matcher_pass pass_name GraphTransformObserver gm pass_name apply_graph_pass pattern_matcher_pass apply is_same_dict counters inductor inductor_before_change trace_structured artifact metadata_fn=lambda name f pattern_matcher_pass pass_name _post_grad encoding string payload_fn=lambda gm print_readable print_output=False include_stride=True include_device=True config b b_gemm_pass B B_GEMM_PASS apply gm graph type ignore arg-type config _micro_pipeline_tp micro_pipeline_tp_pass gm graph config _fuse_ddp_communication GraphTransformObserver gm fuse_ddp_communication apply_graph_pass lambda graph fuse_ddp_communication graph config _fuse_ddp_communication_passes config _fuse_ddp_bucket_size post_grad_custom_post_pass = config post_grad_custom_post_pass GraphTransformObserver gm post_grad_custom_post_pass apply_graph_pass post_grad_custom_post_pass GraphTransformObserver gm stable_sort apply_graph_pass stable_topological_sort GraphTransformObserver gm move_constructors_to_cuda apply_graph_pass move_constructors_to_gpu fake_tensor_updater incremental_update device custom_backend_pass custom_backend_passes items custom_backend_pass None gm_devices = d type d get_all_devices gm device gm_devices pass_name = custom_backend_passes_ + device GraphTransformObserver gm pass_name apply_gm_pass custom_backend_pass collectives_bucketing bool = False config bucket_reduce_scatters_fx = none torch _inductor fx_passes bucketing bucket_reduce_scatter torch _inductor fx_passes fsdp bucket_fsdp_reduce_scatter p = bucket_fsdp_reduce_scatter fsdp config bucket_reduce_scatters_fx bucket_reduce_scatter GraphTransformObserver gm bucket_reduce_scatters apply_graph_pass lambda graph p graph owning_module config bucket_reduce_scatters_fx_bucket_size_determinator config bucket_reduce_scatters_fx type ignore arg-type collectives_bucketing = True Fx all_gather bucketing introduces mutation op Keeping end keep invariant functional graph previous passes config bucket_all_gathers_fx = none torch _inductor fx_passes bucketing bucket_all_gather torch _inductor fx_passes fsdp bucket_fsdp_all_gather p = bucket_fsdp_all_gather type ignore assignment fsdp config bucket_all_gathers_fx bucket_all_gather GraphTransformObserver gm bucket_all_gathers apply_graph_pass lambda graph p graph owning_module config bucket_all_gathers_fx_bucket_size_determinator config bucket_all_gathers_fx type ignore arg-type collectives_bucketing = True collectives_bucketing Fx collectives bucketing passes require topological sort cases when bucketed collectives have users before last collective bucket AND when inputs bucketed collective have ancestors after first collective bucket In case we can manually pick place bucketed collective insertion But we guaranteed bucketing independent collectives bucket possible reorder nodes satisfy all ordering requirements --- before bucketing --- = wait_ag = ag user wait_ag pre_in = = transform pre_in wait_ag = ag user wait_ag --- after bucketing --- = user wait_ag --- wait_ag defined only after bucketed collective pre_in = = transform pre_in ag_bucket +in wait_bucket wait_ag = wait_bucket wait_ag = wait_bucket user wait_ag stable_topological_sort gm graph Apply overlap scheduling enabled config aten_distributed_optimizations enable_overlap_scheduling torch _inductor config aten_distributed_optimizations dist_opts torch _inductor fx_passes overlap_scheduling schedule_overlap_bucketing default insert overlap deps within inductor kwargs dict str object = insert_overlap_deps True config_keys = collective_bucketing max_compute_pre_fetch custom_runtime_estimation insert_overlap_deps key config_keys val = getattr dist_opts key None kwargs key = val GraphTransformObserver gm overlap_scheduling apply_graph_pass lambda graph schedule_overlap_bucketing graph owning_module kwargs type ignore arg-type Keep these last since they introduce mutation Look fx_passes README md discussion mutation invariants GraphTransformObserver gm reinplace_inplaceable_ops apply_graph_pass functools partial reinplace_inplaceable_ops fake_tensor_updater GraphTransformObserver gm decompose_triton_kernel_wrapper_functional apply_graph_pass decompose_triton_kernel_wrapper_functional GraphTransformObserver gm decompose_auto_functionalized apply_graph_pass decompose_auto_functionalized torch _dynamo config skip_fsdp_hooks GraphTransformObserver gm reinplace_fsdp_all_gather apply_graph_pass comms reinplace_fsdp_all_gather GraphTransformObserver gm decompose_scan_to_while_loop apply_gm_pass decompose_scan_to_while_loop GraphTransformObserver gm decompose_map_to_while_loop apply_gm_pass decompose_map_to_while_loop gm recompile gm graph lint prepare_softmax_pattern x dim xmax = x amax dim=dim keepdim=True xsub = x - xmax xexp = xsub exp xsum = xexp sum dim=dim keepdim=True xmax xsum xsub xexp prepare_softmax_replacement x dim Return xsub since otherwise log-softmax can matched due use intermediate node Same reason xsub exp softmax torch _inductor inductor_prims prepare_softmax_online xmax xsum = prepare_softmax_online x dim xsub = x - xmax xmax xsum xsub xsub exp prepare_softmax_extra_check match We only have triton online softmax kernels currently config online_softmax match kwargs x meta val device type == cuda config cuda_backend == triton decompose_map_to_while_loop gm torch fx GraphModule This similar decompose_scan_to_while_loop graph_pass = PatternMatcherPass register_graph_pattern CallFunctionVarArgs torch ops higher_order map_impl pyrefly ignore bad-argument-type pass_dict=graph_pass _ match Match args kwargs assert len kwargs == kwargs map merged into args before entering decompose_map_to_while_loop_pass subgraph fx_xs fx_additional_inputs = args sub_gm torch fx GraphModule = getattr gm subgraph target cur_node = match nodes mapped_outputs = cur_node meta val lower_to_while_loop args kwargs assert len kwargs == xs additional_inputs = pytree tree_unflatten args tree_spec assert isinstance xs tuple list isinstance additional_inputs tuple list xs additional_inputs map_length = xs size loop_idx = torch zeros dtype=torch int device=torch device cpu Similar NOTE Pre-allocate scan s output buffer bound_symbols = arg node expr arg arg pytree tree_leaves args map_length isinstance arg torch SymInt out_buffers = torch empty_strided resolve_shape_to_proxy out size bound_symbols resolve_shape_to_proxy out stride bound_symbols device=out device dtype=out dtype layout=out layout requires_grad=out requires_grad out mapped_outputs while_loop_operands = loop_idx out_buffers xs while_loop_flat_operands operands_spec = pytree tree_flatten while_loop_operands while_loop_additional_inputs = additional_inputs _ operands_and_additional_inputs_spec = pytree tree_flatten while_loop_operands additional_inputs cond_fn flat_args loop_idx _ _ _ = pytree tree_unflatten flat_args operands_and_additional_inputs_spec loop_idx map_length body_fn flat_args loop_idx out_bufs xs additional_inputs = pytree tree_unflatten flat_args operands_and_additional_inputs_spec idx_int = loop_idx item torch ops aten _assert_scalar default idx_int = torch ops aten _assert_scalar default idx_int map_length sub_xs = torch ops aten select int x idx_int x xs outs = sub_gm sub_xs additional_inputs out buffer zip outs out_bufs buffer_slice = torch ops aten select int buffer idx_int buffer_slice copy_ out loop_idx + out_bufs xs _ final_out _ = pytree tree_unflatten torch ops higher_order while_loop cond_fn body_fn tuple while_loop_flat_operands tuple while_loop_additional_inputs operands_spec final_out lower_to_while_loop_args tree_spec = pytree tree_flatten fx_xs fx_additional_inputs match replace_by_example lower_to_while_loop lower_to_while_loop_args run_functional_passes=False graph_pass apply gm _node gm graph find_nodes op= call_function target=torch ops higher_order map_impl raise AssertionError map lowered while_loop resolve_shape_to_proxy shape list int &#124; torch SymInt bound_symbols dict Any Any Given list symints ints function returns calculated expression bound_symbols values When we trace function we ll get graph call_function nodes describes how shape expr computed bound_symbols values Suppose shape = s s s +s bound_symbols = s arg s arg result will arg arg arg + arg torch utils _sympy interp sympy_interp torch utils _sympy reference PythonReferenceAnalysis ret = s shape isinstance s torch SymInt ret append sympy_interp PythonReferenceAnalysis bound_symbols s node expr assert isinstance s int ret append s ret decompose_scan_to_while_loop gm torch fx GraphModule NOTE decompose scan while_loop This pass decomposes ` scan ` ` while_loop ` replacing scan fx_node while_loop hop Suppose we have function f f init = torch zeros xs = torch arange ys = i range xs size init = xs i + init ys append init Return final carry stack intermediates init torch stack ys We could rewrite scan benefits reducing compilation time binary size reducing memory usage supporting loops over unbacked shapes cudagraph etc g step_fn init torch Tensor x torch Tensor next_init = x + init next_init next_init init = torch zeros xs = torch arange final_carry ys = torch _higher_order scan step_fn init xs final_carry ys This pass will rewrite scan into k init = torch zeros xs = torch arange we create loop_idx loop through xs shape loop_idx = torch zeros ys = torch empty_strided _shape_stride_of_ys cond_fn loop_idx ys init xs loop_idx xs shape we pre-allocate output buffer ys inplace copy y each intermediate into slice NOTE Pre-allocate scan s output buffer body_fn loop_idx ys init xs int_idx = loop_idx item next_init y = step_fn init xs int_idx ys int_idx copy_ y loop_idx + ys next_init xs final_carry _ _ ys = torch _higher_order while_loop cond_fn body_fn loop_idx ys init xs final_carry ys graph_pass = PatternMatcherPass register_graph_pattern CallFunctionVarArgs torch ops higher_order scan pyrefly ignore bad-argument-type pass_dict=graph_pass _ match Match args kwargs torch _higher_order_ops scan _extract_carry_and_out assert len kwargs == kwargs scan merged into args before entering decompose_scan_to_while_loop_pass combine_subgraph fx_init fx_xs fx_additional_inputs = args assert combine_subgraph op == get_attr first arg combine_subgraph sub_gm torch fx GraphModule = getattr gm combine_subgraph target cur_node = match nodes num_init_leaves = len fx_init _ ys_outputs = _extract_carry_and_out cur_node meta val num_init_leaves lower_to_while_loop args kwargs The traced graph function will used replace original scan fx_node assert len kwargs == Step construct necessary inputs while_loop based scan s input init xs additional_inputs = pytree tree_unflatten args tree_spec scan_length = xs size loop_idx = torch zeros dtype=torch int device=torch device cpu NOTE Pre-allocate scan s output buffer In order pre-allocate output buffer ys we rely meta scan s fx_node However meta consists concrete symints we need bind those symints proxies order trace torch empty_strided call correctly Also note basic free symbols tensor s shapes guaranteed lifted subgraph inputs dynamo so we can always re-construct sym expression placeholders See Note Auto lift basic free symbols when create_graph_input how done bound_symbols = arg node expr arg arg pytree tree_leaves args scan_length isinstance arg torch SymInt ys_outs = torch empty_strided resolve_shape_to_proxy ys_out size bound_symbols resolve_shape_to_proxy ys_out stride bound_symbols device=ys_out device dtype=ys_out dtype layout=ys_out layout requires_grad=ys_out requires_grad ys_out ys_outputs while_loop_operands = loop_idx ys_outs init xs flat_operands operands_spec = pytree tree_flatten while_loop_operands _ operands_and_additional_inputs_spec = pytree tree_flatten while_loop_operands additional_inputs Step create cond_fn body_fn while_loop cond_fn flat_args loop_idx _ _ _ _ = pytree tree_unflatten flat_args operands_and_additional_inputs_spec type ignore has-type loop_idx scan_length type ignore has-type body_fn flat_args loop_idx ys_outs carry xs additional_inputs = pytree tree_unflatten flat_args operands_and_additional_inputs_spec type ignore has-type idx_int = loop_idx item torch ops aten _assert_scalar default idx_int = torch ops aten _assert_scalar default idx_int scan_length sub_xs = torch ops aten select int x idx_int x xs next_carry ys = _extract_carry_and_out sub_gm list carry + sub_xs + list additional_inputs num_init_leaves y y_out zip ys ys_outs y_out_slice = torch ops aten select int y_out idx_int y_out_slice copy_ y loop_idx + ys_outs next_carry xs Step call while_loop operator _ ys_outs last_carry _ = pytree tree_unflatten torch ops higher_order while_loop cond_fn body_fn tuple flat_operands tuple additional_inputs operands_spec list last_carry + list ys_outs lower_to_while_loop_args tree_spec = pytree tree_flatten fx_init fx_xs fx_additional_inputs match replace_by_example lower_to_while_loop lower_to_while_loop_args run_functional_passes=False graph_pass apply gm _node gm graph find_nodes op= call_function target=torch ops higher_order scan raise AssertionError scan lowered while_loop init_once_fakemode lazy_init torch _C _has_mkldnn decompose_mem_bound_mm noqa F mkldnn_fusion _mkldnn_fusion_init _mkldnn_fusion_init Put patterns post-grad pass rather than joint-graph pass since otherwise there will perf peak-memory regression https github com pytorch pytorch issues register_replacement pyrefly ignore bad-argument-type prepare_softmax_pattern pyrefly ignore bad-argument-type prepare_softmax_replacement torch empty scalar_workaround=dict dim=- pyrefly ignore bad-argument-type trace_fn=fwd_only pyrefly ignore bad-argument-type pass_dicts=pass_patterns extra_check=prepare_softmax_extra_check reorder_for_locality graph torch fx Graph torch distributed is_available check This wait node ` other_node ` ` some collective node Eager semantics allow waits issued different order than collectives Reordering wait node might reorder collectives which cause hangs Once we have SPMD mode we can safely reorder them However increasing locality between collective its wait node generally worse performance node target = torch ops _c d_functional wait_tensor default check True visit other_node other_node op == call_function other_node target = operator getitem all n seen_nodes n other_node users get_mutation_region_id graph node == get_mutation_region_id graph other_node check move node s producers right before node prepend other_node seen_nodes = OrderedSet torch fx Node only reorder nodes before first copy_ graph copy_ will appear end functionalized graphs when there mutation inputs reordering doesn t work well mutation first_copy = next iter graph find_nodes op= call_function target=torch ops aten copy_ default None past_mutating_epilogue = first_copy None node reversed graph nodes seen_nodes add node past_mutating_epilogue past_mutating_epilogue = node first_copy continue torch fx map_arg node args node kwargs visit register_lowering_pattern pattern extra_check=_return_true pass_number= - Callable Callable _P _T Callable _P _T Register aten inductor IR replacement pattern pattern_matcher register_lowering_pattern pattern extra_check pyrefly ignore bad-argument-type pass_dict=pass_patterns pass_number ################################################################################ Actual patterns below point Priority patterns - later output nodes first - order patterns defined ################################################################################ is_valid_mm_plus_mm match Match config max_autotune config max_autotune_gemm False _b m k = match kwargs mat meta get tensor_meta shape _b k n = match kwargs mat meta get tensor_meta shape k = k False _b m k = match kwargs mat meta get tensor_meta shape _b k n = match kwargs mat meta get tensor_meta shape k = k False m = m n = n False True scatter_upon_const_tensor_extra_check m config optimize_scatter_upon_const_tensor False full_shape = m kwargs shape selector = m kwargs selector dim = m kwargs dim dim dim += len full_shape selector_ft = selector meta val assert selector_ft dim == len full_shape idx select_sz full_sz zip itertools count selector_ft shape full_shape idx == dim continue TODO pattern can updated support case index tensor shorter But will need more complex condition expression especially multi-dimensional tensors Skip now isinstance full_sz fx Node full_sz = full_sz meta val select_sz full_sz False Actually we can support small size larger than It would bit tedius E g we load all index values many compare them position tensor decide what value selector_ft size dim == register_lowering_pattern CallFunction aten scatter value CallFunction aten full KeywordArg shape KeywordArg background_val dtype=KeywordArg dtype KeywordArg dim KeywordArg selector KeywordArg val scalar value extra_check=scatter_upon_const_tensor_extra_check scatter_upon_const_tensor match Match shape background_val dtype dim selector val Match pattern full+scatter into pointwise TODO Right now scatter value must scalar But we could support when tensor well torch _inductor metrics Check inputs tensors instead inductor IR nodes isinstance selector torch Tensor Return fake tensor proper shape operator intended device = selector device hasattr selector device torch device cpu torch empty shape dtype=dtype device=device pyrefly ignore bad-assignment metrics num_matches_for_scatter_upon_const_tensor += selector_loader = selector make_loader inner_fn idx selector_idx = list idx selector_idx dim = selector = selector_loader selector_idx ops where selector == ops index_expr idx dim torch int ops constant val dtype ops constant background_val dtype ir Pointwise create device=selector get_device dtype=dtype inner_fn=inner_fn ranges=shape register_lowering_pattern CallFunction aten add CallFunction aten mm KeywordArg mat KeywordArg mat CallFunction aten mm KeywordArg mat KeywordArg mat extra_check=is_valid_mm_plus_mm mm_plus_mm match Match mat mat mat mat inductor kernel mm_plus_mm tuned_mm_plus_mm mat mat mat mat register_graph_pattern CallFunction aten cumsum default CallFunction torch ops aten full default KeywordArg shape KeywordArg fill_value dtype=KeywordArg dtype layout=Ignored device=KeywordArg device pin_memory=False _users=MULTIPLE KeywordArg dim _users=MULTIPLE pyrefly ignore bad-argument-type pass_dict=pass_patterns pointless_cumsum_replacement match Match shape fill_value device dtype dim Based pattern OPTForCausalLM is_integer_dtype dtype is_boolean_dtype dtype cumsum promotes all integral types int dtype = torch int repl shape dim_size = shape dim idx = torch arange dim_size + device=device dtype=dtype inter_shape = len shape inter_shape dim = dim_size idx fill_value view inter_shape expand shape only replace output node all nodes match nodes = match output_node pyrefly ignore bad-argument-type match replace_by_example repl list shape _cat_ = CallFunction aten cat Arg _users= register_lowering_pattern CallFunction aten cat _cat_ CallFunction aten slice _cat_ KeywordArg size cat_slice_cat match cat_input size dim= This example more complex pattern where cat_ used multiple times inside pattern We fold calls cat into one Matches cat_ f = torch ops aten cat default add_ primals_ slice_ f = torch ops aten slice Tensor cat_ slice_ f = torch ops aten slice Tensor slice_ cat_ f = torch ops aten cat default cat_ slice_ Rewrite slice_ = torch ops aten slice Tensor add_ cat_ = torch ops aten cat default add_ primals_ slice first rest = cat_input Optimization optional because we can just fold cat size should within first get_size dim such optimization valid For negative ` end ` we currently fallback optimizing size = V graph sizevars statically_known_leq size first get_size dim fold cats into cat L aten cat first rest L aten slice first dim size dim don t expect hit case just fall back tmp = L aten cat cat_input dim L aten cat tmp L aten slice tmp dim size dim is_valid_splitwithsizes_cat match split_nodes = filter_nodes match nodes aten split_with_sizes cat_nodes = filter_nodes match nodes aten cat get_item_nodes = filter_nodes match nodes operator getitem len split_nodes = len cat_nodes = False split_node cat_node = split_nodes cat_nodes The dim split cat should match passthrough get_arg_value split_node dim = get_arg_value cat_node dim False get_item_args = OrderedSet get_arg_value get_item_node get_item_node get_item_nodes assert None get_item_args split_sizes = get_arg_value split_node split_sizes All parts split should included cat get_item_args = OrderedSet range len split_sizes False The order get_item_args should same cat_node used For example split_node like split_with_sizes input cat node should like cat get_item get_item get_item cat_items_args_order = get_arg_value item_node item_node get_arg_value cat_node cat_items_args_order = list range len split_sizes False True same_meta node torch fx Node node torch fx Node True two nodes have same metadata val = node meta get val val = node meta get val val None val None statically_known_true sym_eq val size val size val layout == val layout val dtype == val dtype val device == val device val layout = torch strided statically_known_true sym_eq val stride val stride noop_registry dict Any Any = register_noop_decomp targets nop_arg= register_fun cond register_decomposition targets registry=noop_registry unsafe=True cond nop_arg type ignore arg-type cond register_fun register_noop_decomp aten slice slice_noop dim= start=None end=None step= start None end None False slice_dim_size = shape dim statically_known_true sym_eq start statically_known_true end = - statically_known_true end = slice_dim_size statically_known_true sym_eq step True False register_noop_decomp aten slice_scatter slice_scatter_noop src dim= start=None end=None step= start None start = end None end = - slice_scatter_dim_size = shape dim shape == src shape start == statically_known_true end = - statically_known_true end = slice_scatter_dim_size step == True False register_noop_decomp aten repeat repeat_noop repeats all r == r repeats register_noop_decomp aten constant_pad_nd constant_pad_nd x padding fill_value= all p == p padding register_noop_decomp torch ops prims convert_element_type convert_element_type_noop x dtype torch dtype x dtype == dtype register_noop_decomp torch ops prims device_put device_put_noop x device non_blocking=True x device == decode_device device register_noop_decomp aten ceil aten floor aten round aten trunc int_noop x is_integer_dtype x dtype register_noop_decomp aten pow pow_noop b isinstance b int b == register_noop_decomp aten cat lambda args args cat_noop inputs dim= len inputs == register_noop_decomp aten view default view_default_noop arg size statically_known_true sym_eq arg shape tuple size register_noop_decomp aten view dtype view_dtype_noop arg dtype arg dtype == dtype Note we also always have check identical metadata which why these safe register_noop_decomp aten copy nop_arg= register_noop_decomp aten alias aten clone true_noop args kwargs True remove_noop_ops graph torch fx Graph Removes both operations essentially aten clone operations essentially aten alias graph inputs = OrderedSet torch fx Node input_storages = OrderedSet int &#124; None output_storages = OrderedSet int &#124; None node graph find_nodes op= placeholder inputs add node input_storages add get_node_storage node output_node = next iter reversed graph nodes assert output_node op == output outputs = output_node args isinstance outputs list tuple nested subgraphs can have singleton outputs outputs = outputs out outputs isinstance out torch fx Node output_storages add get_node_storage out node graph nodes node target noop_registry cond src_index = noop_registry node target isinstance src_index int src = node args src_index src = src_index node args isinstance src torch fx Node continue Don t introduce new aliasing between inputs outputs See fx_passes README md discussion why necessary node_storage = get_node_storage node src_storage = get_node_storage src node_is_view = node_storage == src_storage node_is_view node_storage output_storages src_storage input_storages src_storage output_storages continue Even input outputs expected alias don t make node src True node_is_view node output_node args src inputs src output_node args continue is_valid args kwargs = get_fake_args_kwargs node is_valid continue same_meta node src cond args kwargs node replace_all_uses_with src graph erase_node node remove_assert_ops graph torch fx Graph Removes aten _assert_tensor_metadata default op because will lowered no-op inductor can block fusion such unfuse_bias_add_to_pointwise fusion This op could come aten functionalization export For example we have graph like below addmm = aten addmm default linear_bias arg _ permute _assert_tensor_metadata = aten _assert_tensor_metadata default addmm None None torch float convert_element_type_ = prims convert_element_type default addmm torch float pow_ = aten pow Tensor_Scalar convert_element_type_ We still want fuse add addmm pow instead fusing add mm according unfuse_bias_add_to_pointwise fusion However aten _assert_tensor_metadata default pointwise op would fail should_prefer_unfused_addmm check We remove op so doesn t block fusion decisions It s safe because op lowered no-op register_lowering node graph find_nodes op= call_function target=torch ops aten _assert_tensor_metadata default graph erase_node node decompose_triton_kernel_wrapper_functional graph Decomposes triton_kernel_wrapper_functional nodes into clones underlying mutation node We assume reinplacing pass runs before reinplacing pass tells us via rewriting arguments meta those nodes which Tensors we should clone which Tensors safe reinplace graph_pass = PatternMatcherPass register_graph_pattern CallFunctionVarArgs torch ops higher_order triton_kernel_wrapper_functional pyrefly ignore bad-argument-type pass_dict=graph_pass _ match Match args kwargs torch _higher_order_ops triton_kernel_wrap triton_kernel_wrapper_functional_dense flat_args spec = pytree tree_flatten args kwargs NB we combine args kwargs into flat args replacing This replace_by_example uses make_fx which does support tracing function kwargs decomp flat_args args kwargs = pytree tree_unflatten flat_args spec triton_kernel_wrapper_functional_dense args kwargs pyrefly ignore bad-argument-type match replace_by_example decomp flat_args run_functional_passes=False graph_pass apply graph _ graph find_nodes op= call_function target=torch ops higher_order triton_kernel_wrapper_functional raise AssertionError triton_kernel_wrapper_functional removed decompose_auto_functionalized graph Decomposes auto_functionalized nodes into clones underlying mutation node We assume reinplacing pass runs before reinplacing pass tells us via rewriting arguments meta those nodes which Tensors we should clone which Tensors safe reinplace graph_pass = PatternMatcherPass register_graph_pattern CallFunctionVarArgs torch ops higher_order auto_functionalized pyrefly ignore bad-argument-type pass_dict=graph_pass _ match Match args kwargs torch _higher_order_ops auto_functionalize auto_functionalized_dense only_clone_these_tensors = tuple match nodes meta get only_clone_these_tensors flat_args spec = pytree tree_flatten args kwargs NB we combine args kwargs into flat args replacing This replace_by_example uses make_fx which does support tracing function kwargs decomp flat_args args kwargs = pytree tree_unflatten flat_args spec assert len args == mode = args auto_functionalized_dense mode only_clone_these_tensors kwargs pyrefly ignore bad-argument-type match replace_by_example decomp flat_args run_functional_passes=False register_graph_pattern CallFunctionVarArgs torch ops higher_order auto_functionalized_v pyrefly ignore bad-argument-type pass_dict=graph_pass _ match Match args kwargs torch _higher_order_ops auto_functionalize auto_functionalized_v _dense only_clone_these_bases = tuple match nodes meta get only_clone_these_tensors flat_args spec = pytree tree_flatten args kwargs _maybe_resolve_constant_get_attr node Resolve getattr node its value because they don t always have meta val isinstance node torch fx Node node op == get_attr val node meta const_attr = getattr graph owning_module node target type ignore arg-type assert isinstance const_attr torch fx GraphModule pytree TreeSpec type const_attr const_attr const_attr node flat_args = _maybe_resolve_constant_get_attr arg arg flat_args NB we combine args kwargs into flat args replacing This replace_by_example uses make_fx which does support tracing function kwargs decomp flat_args args kwargs = pytree tree_unflatten flat_args spec assert len args == mutable_op = args auto_functionalized_v _dense mutable_op only_clone_these_bases kwargs pyrefly ignore bad-argument-type match replace_by_example decomp flat_args run_functional_passes=False graph_pass apply graph Remove unused get_attr nodes their corresponding attributes graph module When auto_functionalizing hop we need clean up get_attr nodes _constant_schema auto_functionalized graph module no longer referenced unused_get_attr_nodes = removable_attrs OrderedSet torch fx node Target = OrderedSet protected_attrs OrderedSet torch fx node Target = OrderedSet First pass identify unused get_attr nodes track attribute usage node graph nodes node op = get_attr continue len node users == Node unused mark removal unused_get_attr_nodes append node Check attribute can removed module hasattr graph owning_module node target isinstance getattr graph owning_module node target torch fx GraphModule node target protected_attrs removable_attrs add node target Node used protect its attribute removal node target removable_attrs removable_attrs remove node target protected_attrs add node target Second pass clean up unused nodes attributes node unused_get_attr_nodes graph erase_node node attr_name removable_attrs assert isinstance attr_name str delattr graph owning_module attr_name graph lint _ graph find_nodes op= call_function target=torch ops higher_order auto_functionalized raise AssertionError auto_functionalized removed _ graph find_nodes op= call_function target=torch ops higher_order auto_functionalized_v raise AssertionError auto_functionalized_v removed register_lowering_pattern CallFunction aten cat ListOf CallFunction operator getitem CallFunction aten split_with_sizes KeywordArg input_ Ignored Ignored _users=MULTIPLE Ignored Ignored pass_number= extra_check=is_valid_splitwithsizes_cat splitwithsizes_cat_replace match input_ input_ is_valid_cat_splitwithsizes match cat_nodes = filter_nodes match nodes aten cat split_nodes = filter_nodes match nodes aten split_with_sizes len split_nodes = len cat_nodes = False split_node cat_node = split_nodes cat_nodes cat node has other users can t eliminate len cat_node users False dim cat split should match dim = get_arg_value split_node dim dim = get_arg_value cat_node dim False cat_inputs = list get_arg_value cat_node split_sizes = get_arg_value split_node split_sizes number input tensors cat length split sizes should match len cat_inputs = len split_sizes False cat_input split_size zip cat_inputs split_sizes each cat input tensor s size along dim should match corresponding split size val cat_input meta False cat_input_size = cat_input meta val size dim cat_input_size = split_size False True register_lowering_pattern CallFunction aten split_with_sizes CallFunction aten cat KeywordArg input_ Ignored _users=MULTIPLE Ignored Ignored pass_number= extra_check=is_valid_cat_splitwithsizes cat_splitwithsizes_replace match input_ input_ view_to_reshape gm Replace view ops GraphModule reshape ops subgraph_names OrderedSet str = OrderedSet x target x gm graph find_nodes op= get_attr child_name child_mod gm named_children child_name subgraph_names isinstance child_mod torch fx GraphModule view_to_reshape child_mod nd gm graph find_nodes op= call_function target=torch ops aten view default nd target = torch ops aten reshape default should_prefer_unfused_addmm match inp = match kwargs inp is_gpu inp meta val device type False output = match output_node all is_pointwise_use use use output users register_graph_pattern CallFunction aten addmm KeywordArg inp Arg Arg pyrefly ignore bad-argument-type pass_dict=pass_patterns extra_check=should_prefer_unfused_addmm unfuse_bias_add_to_pointwise match Match mat mat inp repl inp x x x x + inp pyrefly ignore bad-argument-type match replace_by_example repl inp mat mat is_valid_addmm_fusion match mat mat = match args inp = match kwargs inp isinstance inp torch fx Node isinstance inp meta val torch Tensor False Input number in_shape = inp meta val shape mm_shape = mat meta val shape mat meta val shape matched = is_expandable_to in_shape mm_shape matched False Shape mismatch inp_dtype = inp meta val dtype aten cublas integration assumes equal dtypes inp_dtype = mat meta val dtype inp_dtype = mat meta val dtype False should_prefer_unfused_addmm match register_graph_pattern CallFunction aten add CallFunction aten mm Arg Arg KeywordArg inp pyrefly ignore bad-argument-type pass_dict=pass_patterns extra_check=is_valid_addmm_fusion register_graph_pattern CallFunction aten add KeywordArg inp CallFunction aten mm Arg Arg pyrefly ignore bad-argument-type pass_dict=pass_patterns extra_check=is_valid_addmm_fusion addmm match mat mat inp repl inp mat mat aten addmm inp mat mat match replace_by_example repl inp mat mat register_partial_reduction_pattern Reuse partial reductions complete reductions post grad equivalents equiv_red = aten amax default aten max default aten amin default aten min default TODO support other reductions like sum would need skip lower precision reductions since partial output would need kept fp red_op aten amax default aten amin default inp = KeywordArg input partial_reduc = CallFunction red_op inp KeywordArg reduced_dims KeywordArg keepdim full_reduc = CallFunction red_op equiv_red red_op inp register_graph_pattern MultiOutputPattern partial_reduc full_reduc pyrefly ignore bad-argument-type pass_dict=pass_patterns reuse_partial match input reduced_dims keepdim partial_red full_red = match output_nodes they re small reuse worth statically_known_true input meta val numel = True replacement inp torch Tensor - tuple torch Tensor torch Tensor partial = partial_red target inp reduced_dims keepdim complete = full_red target partial partial complete counters inductor partial_reduction_reuse += match replace_by_example replacement input register_partial_reduction_pattern check_shape_cuda_and_fused_int_mm_mul_enabled match config force_fuse_int_mm_with_mul len getattr match args meta get val shape == getattr match args meta get val is_cuda False is_index_put_and_requires_h d_sync_for_gpu_value node torch fx operator_schemas normalize_function node target torch ops aten index_put default torch ops aten index_put_ default False Inductor falls back aten index_put_ index_put_ will will call nonzero perform H D sync any its indices bool byte tensors However will short-circuit H D sync run mask_fill_ value we putting cpu scalar Therefore when inductor sees index_put_ byte tensor indices should convert cpu scalar value into gpu tensor args_ _kwargs = normalize_function node target node args node kwargs type ignore misc any_byte_bool_indices = False indices = args_ i indices i None i meta val dtype torch bool torch int any_byte_bool_indices = True val = args_ meta val val_is_cpu_scalar = val device type == cpu val numel == If both these conditions hold then converting val gpu tensor will incur H D sync when inductor calls aten index_put_ any_byte_bool_indices val_is_cpu_scalar ConstructorMoverPass __init__ target str allow_outputs bool = False allow_inputs bool = False - None Move constructors cpu target_device Sweeps through module looking constructor nodes can moved target_device A constructor node can moved target_device iff all its users can also moved tested cannot_be_moved Otherwise all dependent constructor nodes won t moved - target target device type - allow_outputs allow outputs moved - allow_inputs allow inputs moved target = target allow_inputs = allow_inputs allow_outputs = allow_outputs assert isinstance target str target should string representing device type f Got type target __name__ allow_cpu_device node fx Node - bool Returns whether node returns tensor target device may have cpu tensors input node target torch ops aten index Tensor torch ops aten index_put default torch ops aten index_put_ default torch ops aten copy default torch ops aten copy_ default torch ops aten slice_scatter default is_on_target_device node fx Node - bool Returns whether node target device node_device = get_node_device node node_device None node_device type == target is_cpu_scalar_tensor node fx Node - bool Returns whether node cpu scalar tensor device = get_node_device node is_cpu = device None device type == cpu ten = node meta get val is_scalar = isinstance ten torch Tensor len ten size == is_cpu is_scalar all_inputs_are_cpu_scalar_or_on_target_device node fx Node - bool Returns whether node s inputs either cpu scalar tensors target device inputs = inp inp itertools chain node args node kwargs values isinstance inp fx Node all is_cpu_scalar_tensor inp is_on_target_device inp inp inputs cannot_be_moved node fx Node - bool Returns whether node can moved target device If function returns False means node all its users won t moved into target device node target == output allow_outputs isinstance node target torch _ops OpOverload node target namespace prims aten True is_index_put_and_requires_h d_sync_for_gpu_value node True False get_node_device node fx Node - torch device &#124; None Get device node ten = node meta get val None isinstance ten torch Tensor ten device get_cpu_indeg_count graph fx Graph - dict fx Node int Get number cpu inputs node cpu_indeg dict fx Node int = Counter node graph nodes cpu_count = add_cpu_inp node nonlocal cpu_count device = get_node_device node cpu_count += device None device type == cpu pytree tree_map_only fx Node add_cpu_inp node args node kwargs pyrefly ignore redundant-condition cpu_count cpu_indeg node = cpu_count cpu_indeg __call__ graph fx Graph - None target_devices = OrderedSet torch device constructors = cpu_placeholders OrderedSet fx Node = OrderedSet node graph nodes device = get_node_device node device device type == target target_devices add device allow_inputs node op == placeholder is_cpu_scalar_tensor node cpu_placeholders add node constructors append node continue isinstance node target torch _ops OpOverload node target namespace prims aten continue torch _subclasses fake_tensor _is_tensor_constructor node target continue node kwargs get device = torch device cpu continue constructors append node handling multiple target devices initially constructors len target_devices = movable_constructors = find_movable_constructors graph constructors target_device = next iter target_devices movable_cpu_placeholders = movable_constructors cpu_placeholders movable_cpu_placeholders node = next iter reversed movable_cpu_placeholders last_node = node unsqueezed_nodes = elem movable_cpu_placeholders graph inserting_after last_node unsqueezed_nodes append graph call_function torch ops aten unsqueeze default elem last_node = unsqueezed_nodes - graph inserting_after last_node cpu_concat = graph call_function torch ops aten cat default unsqueezed_nodes last_node = cpu_concat graph inserting_after last_node gpu_concat = graph call_function torch ops prims device_put default cpu_concat target_device True last_node = gpu_concat graph inserting_after last_node gpu_split = graph call_function torch ops aten unbind int gpu_concat last_node = gpu_split idx node enumerate movable_cpu_placeholders graph inserting_after last_node gpu_node = graph call_function operator getitem gpu_split idx node replace_all_uses_with gpu_node lambda x x cpu_concat gpu_concat gpu_split gpu_node + unsqueezed_nodes x target = torch ops aten copy_ default last_node = gpu_node noop elimination there other device_put gpu_node target device Alternatively we could just move other device_put earlier graph supported fx graph yet noop_device_puts = user user gpu_node users user target torch ops prims device_put default user args == target_device noop noop_device_puts noop replace_all_uses_with gpu_node graph erase_node noop movable_constructors -= movable_cpu_placeholders node movable_constructors kwargs = node kwargs copy kwargs device = target_device node kwargs = kwargs find_movable_constructors graph fx Graph constructors list fx Node - OrderedSet fx Node Starting cpu constructors iterate through graph test all their downstream uses can safely moved cpu cpu_indeg dict fx Node int = get_cpu_indeg_count graph which constructors cannot moved gpu cannot_move_to_gpu = OrderedSet fx Node For any node graph which constructors does have dependency constructor_dependencies dict fx Node OrderedSet fx Node = defaultdict OrderedSet cpu node has dependency two different cpu constructors then either constructor cannot moved gpu other cannot well In case any node dependency one will have dependency other equal_constructor_sets dict fx Node OrderedSet fx Node = c OrderedSet c c constructors make_dependencies_equivalent set OrderedSet fx Node set OrderedSet fx Node - OrderedSet fx Node could use union find worth complexity here set update set obj set equal_constructor_sets obj = set set queue list fx Node = list constructors c queue constructor_dependencies c add c while queue node = queue pop dependencies = constructor_dependencies node user node users cannot_be_moved user cannot_move_to_gpu update dependencies break node used op which takes multiple devices output gpu tensor we can convert its cpu input gpu without making further changes allow_cpu_device user is_on_target_device user del cpu_indeg user allow_inputs all_inputs_are_cpu_scalar_or_on_target_device user node takes only cpu scalar tensors gpu tensors inputs outputs gpu tensor we can convert its cpu scalar inputs gpu without making further changes del cpu_indeg user otherwise we should continue look its downstream uses cpu_indeg user -= cpu_indeg user == del cpu_indeg user queue append user unioned_set = make_dependencies_equivalent dependencies constructor_dependencies user constructor_dependencies user = unioned_set node cpu_indeg constructor_dependencies node cannot_move_to_gpu update constructor_dependencies node all_cannot_move_to_gpu = cannot_move_to_gpu copy constructor cannot_move_to_gpu all_cannot_move_to_gpu update equal_constructor_sets constructor OrderedSet constructors - all_cannot_move_to_gpu move_constructors_to_gpu graph fx Graph - None Moves intermediary tensors which constructed cpu gpu when safe cudagraph does support cpu tensors In pass we update graph explicitly moving cpu scalar tensors gpu when profitable relying graph partition split off data copy cudagraphifying remaining gpu ops allow_inputs_outputs = bool torch _inductor config triton cudagraphs torch _inductor config graph_partition ConstructorMoverPass get_gpu_type allow_inputs=allow_inputs_outputs allow_outputs=allow_inputs_outputs graph