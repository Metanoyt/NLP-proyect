Owner s oncall quantization copy math hypothesis given strategies st torch torch ao nn intrinsic qat nniqat torch ao nn qat nnqat torch ao nn qat dynamic nnqatd torch ao nn quantized nnq torch ao nn quantized dynamic nnqd torch backends mkldnn torch nn nn torch testing _internal hypothesis_utils hu torch ao nn intrinsic qat ConvBn d ConvBnReLU d torch ao quantization convert default_embedding_qat_qconfig default_qat_qconfig default_qconfig default_symmetric_qnnpack_qat_qconfig DeQuantStub FixedQParamsFakeQuantize FusedMovingAvgObsFakeQuantize get_default_qat_qconfig get_embedding_qat_module_mappings get_embedding_static_quant_module_mappings NoopObserver prepare prepare_qat quantize_qat QuantStub torch ao quantization qconfig qconfig_equals torch nn BatchNorm d Conv d init ReLU torch nn modules utils _pair torch testing _internal common_quantization DeFusedEmbeddingBagLinear ManualConvLinearQATModel ManualConvLinearSymmQATModel ManualDropoutQATModel ManualEmbeddingBagLinear ManualLinearDynamicQATModel ManualLinearQATModel QuantizationTestCase QuantStubModel test_only_eval_fn test_only_train_fn TwoLayerLinearModel torch testing _internal common_quantized override_qengines override_quantized_engine supported_qengines torch testing _internal common_utils skipIfNoXNNPACK hu assert_deadline_disabled functools reduce _ReferenceConvBnNd torch nn Conv d torch nn modules conv _ConvNd Conv-BN fusion implemented explicit folding Useful verify numerical equivalency non-folded version __init__ ConvNd args in_channels out_channels kernel_size stride padding dilation transposed output_padding groups bias padding_mode BatchNormNd args num_features out_channels eps= e- momentum= affine True track_running_stats True Args module freeze_bn=False qconfig=None nn modules conv _ConvNd __init__ in_channels out_channels kernel_size stride padding dilation transposed output_padding groups False padding_mode assert qconfig qconfig must provided QAT module qconfig = qconfig eps = eps momentum = momentum freeze_bn = freeze_bn training True num_features = out_channels gamma = nn Parameter torch empty out_channels beta = nn Parameter torch empty out_channels affine = True track_running_stats = True running_mean = nn Buffer torch zeros out_channels running_var = nn Buffer torch ones out_channels num_batches_tracked = nn Buffer torch tensor dtype=torch long activation_post_process = qconfig activation weight_fake_quant = qconfig weight bias bias = nn Parameter torch empty out_channels register_parameter bias None reset_bn_parameters reset_running_stats running_mean zero_ running_var fill_ num_batches_tracked zero_ reset_bn_parameters reset_running_stats init uniform_ gamma init zeros_ beta bias None fan_in _ = init _calculate_fan_in_and_fan_out weight bound = math sqrt fan_in init uniform_ bias -bound bound reset_parameters super reset_parameters A hack avoid resetting undefined parameters hasattr gamma reset_bn_parameters update_bn_stats freeze_bn = False freeze_bn_stats freeze_bn = True _forward input exponential_average_factor momentum set when available only so gets updated ONNX graph when node exported ONNX momentum None exponential_average_factor = exponential_average_factor = momentum training freeze_bn track_running_stats TODO statement only here tell jit skip emitting when None num_batches_tracked None num_batches_tracked += momentum None use cumulative moving average exponential_average_factor = float num_batches_tracked use exponential moving average exponential_average_factor = momentum we use running statistics previous batch so approximation approach mentioned whitepaper we only need do one convolution case instead two running_std = torch sqrt running_var + eps scale_factor = gamma running_std scaled_weight = weight scale_factor reshape - bias None zero_bias = torch zeros_like bias dtype=input dtype zero_bias = torch zeros out_channels device=scaled_weight device dtype=input dtype conv = _conv_forward input weight_fake_quant scaled_weight zero_bias training freeze_bn recovering original conv get original batch_mean batch_var bias None conv_orig = conv scale_factor reshape - + bias reshape - conv_orig = conv scale_factor reshape - batch_mean = torch mean conv_orig dim= batch_var = torch var conv_orig dim= unbiased=False n = float conv_orig numel conv_orig size unbiased_batch_var = batch_var n n - batch_rstd = torch ones_like batch_var memory_format=torch contiguous_format torch sqrt batch_var + eps conv = gamma batch_rstd reshape - conv_orig + beta - gamma batch_rstd batch_mean reshape - running_mean = exponential_average_factor batch_mean detach + - exponential_average_factor running_mean running_var = exponential_average_factor unbiased_batch_var detach + - exponential_average_factor running_var bias None conv = conv + beta - gamma running_mean running_std reshape - conv = conv + gamma bias - running_mean running_std + beta reshape - conv extra_repr TODO jerryzh extend super extra_repr forward input activation_post_process _forward input classmethod from_float cls mod qconfig=None r Create qat module float module qparams_dict Args ` mod ` float module either produced torch ao quantization utilities directly user assert type mod cls _FLOAT_MODULE qat + cls __name__ + from_float only works + cls _FLOAT_MODULE __name__ qconfig assert hasattr mod qconfig Input float module must have qconfig defined assert mod qconfig Input float module must have valid qconfig qconfig = mod qconfig conv bn = mod mod qat_convbn = cls conv in_channels conv out_channels conv kernel_size conv stride conv padding conv dilation conv groups conv bias None conv padding_mode bn eps bn momentum False qconfig qat_convbn weight = conv weight qat_convbn bias = conv bias qat_convbn gamma = bn weight qat_convbn beta = bn bias qat_convbn running_mean = bn running_mean qat_convbn running_var = bn running_var qat_convbn num_batches_tracked = bn num_batches_tracked qat_convbn _ReferenceConvBn d _ReferenceConvBnNd nn Conv d _FLOAT_MODULE = torch ao nn intrinsic ConvBn d __init__ ConvNd args in_channels out_channels kernel_size stride= padding= dilation= groups= bias=None padding_mode= zeros BatchNorm d args num_features out_channels eps= e- momentum= affine True track_running_stats True Args module freeze_bn=False qconfig=None kernel_size = _pair kernel_size stride = _pair stride padding = _pair padding dilation = _pair dilation _ReferenceConvBnNd __init__ in_channels out_channels kernel_size stride padding dilation False _pair groups bias padding_mode eps momentum freeze_bn qconfig TestQuantizeEagerQAT QuantizationTestCase setUp super setUp embed_linear_data_train = torch randint dtype=torch long torch randn dtype=torch float _ range embed_data = torch randint test_manual qengine supported_qengines override_quantized_engine qengine model = ManualLinearQATModel qengine model = prepare_qat model checkObservers model test_only_train_fn model train_data model = convert model checkQuantized model assertEqual type model fc nnq Linear assertEqual type model fc nnq Linear test_only_eval_fn model calib_data checkScriptable model calib_data checkNoQconfig model checkQuantized model model = quantize_qat ManualLinearQATModel qengine test_only_train_fn train_data checkQuantized model test_dropout qengine supported_qengines override_quantized_engine qengine model = ManualDropoutQATModel qengine model = prepare_qat model checkObservers model test_only_train_fn model train_data model = convert model checkQuantized model assertEqual type model fc nnq Linear assertEqual type model dropout nnq Dropout test_only_eval_fn model calib_data checkScriptable model calib_data checkNoQconfig model checkQuantized model model = quantize_qat ManualDropoutQATModel qengine test_only_train_fn train_data checkQuantized model test_eval_only_fake_quant r Using FakeQuant evaluation only mode useful estimating accuracy loss when we quantize network qengine supported_qengines override_quantized_engine qengine model = ManualLinearQATModel qengine model = prepare_qat model checkObservers model model eval test_only_eval_fn model calib_data test_conv_linear qengine supported_qengines override_quantized_engine qengine model = ManualConvLinearQATModel model = prepare_qat model checkObservers model test_only_train_fn model img_data_ d_train model = convert model checkQuantized model assertEqual type model conv nnq Conv d assertEqual type model fc nnq Linear assertEqual type model fc nnq Linear test_only_eval_fn model img_data_ d checkScriptable model img_data_ d checkNoQconfig model checkQuantized model model = ManualConvLinearQATModel model = quantize_qat model test_only_train_fn img_data_ d_train checkQuantized model skipIfNoXNNPACK test_conv_linear_symm r Same test_conv_linear Symmetric quantization Supported only qengine=qnnpack which uses symmetric kernels xnnpack library qengine supported_qengines qengine = qnnpack continue override_quantized_engine qengine model = ManualConvLinearSymmQATModel model = prepare_qat model checkObservers model test_only_train_fn model img_data_ d_train model = convert model checkQuantized model assertEqual type model conv nnq Conv d assertEqual type model fc nnq Linear assertEqual type model fc nnq Linear test_only_eval_fn model img_data_ d checkScriptable model img_data_ d checkNoQconfig model checkQuantized model model = ManualConvLinearSymmQATModel model = quantize_qat model test_only_train_fn img_data_ d_train checkQuantized model test_dynamic_qat_linear qengine supported_qengines override_quantized_engine qengine Dynamic QAT without memoryless observers should fail assertRaisesRegex ValueError Dynamic QAT requires memoryless observer + This means MovingAverage observer averaging constant equal model = ManualLinearDynamicQATModel default_qat_qconfig model = prepare_qat model mapping= torch nn Linear nnqatd Linear model = ManualLinearDynamicQATModel model = prepare_qat model mapping= torch nn Linear nnqatd Linear assertEqual type model fc nnqatd Linear assertEqual type model fc nnqatd Linear checkObservers model test_only_train_fn model train_data model = convert model mapping= nnqatd Linear nnqd Linear assertEqual type model fc nnqd Linear assertEqual type model fc nnqd Linear test_only_eval_fn model calib_data checkScriptable model calib_data checkNoQconfig model test_defused_embedding_bag_linear qengine supported_qengines override_quantized_engine qengine model = DeFusedEmbeddingBagLinear train model = prepare_qat model mapping=get_embedding_qat_module_mappings checkObservers model test_only_train_fn model embed_linear_data_train make sure activation_post_process inserted after Linear assertEqual type model linear activation_post_process FusedMovingAvgObsFakeQuantize make sure Embedding has noop activation assertEqual type model emb activation_post_process NoopObserver make sure FakeQuant zero_points correct dtype assertEqual model emb weight_fake_quant zero_point dtype torch float assertEqual model linear weight_fake_quant zero_point dtype torch int model = convert model mapping=get_embedding_static_quant_module_mappings checkQuantized model make sure Embedding now QuantizedEmbedding assertEqual type model emb nn quantized Embedding make sure Linear now QuantizedLinear assertEqual type model linear nn quantized Linear test_only_eval_fn model embed_data checkScriptable model embed_data checkNoQconfig model checkQuantized model test_embedding_bag_linear qengine supported_qengines override_quantized_engine qengine model = ManualEmbeddingBagLinear train model = prepare_qat model mapping=get_embedding_qat_module_mappings checkObservers model test_only_train_fn model embed_linear_data_train make sure activation_post_process inserted EmbeddingBag assertFalse hasattr model activation_post_process make sure FakeQuant zero_points correct dtype assertEqual model emb weight_fake_quant zero_point dtype torch float assertEqual model linear weight_fake_quant zero_point dtype torch int model = convert model mapping=get_embedding_static_quant_module_mappings checkQuantized model Make sure EmbeddingBag now quantized EmbeddingBag assertTrue type model emb nn quantized EmbeddingBag Also test Linear has been quantized assertTrue type model linear nnq Linear test_only_eval_fn model embed_data checkScriptable model embed_data checkNoQconfig model checkQuantized model model = ManualEmbeddingBagLinear test_train_save_load_eval r Test QAT flow creating model doing QAT saving quantized state_dict During eval we first call prepare_qat conver model then load state_dict compare results against original model qengine supported_qengines override_quantized_engine qengine model = TwoLayerLinearModel model = torch ao quantization QuantWrapper model model qconfig = torch ao quantization get_default_qat_qconfig qengine model = prepare_qat model fq_state_dict = model state_dict test_only_train_fn model train_data model = convert model quant_state_dict = model state_dict x = torch rand dtype=torch float ref = model x Create model again eval Check result using quantized state_dict model = TwoLayerLinearModel model = torch ao quantization QuantWrapper model model qconfig = torch ao quantization get_default_qat_qconfig qengine torch ao quantization prepare_qat model inplace=True new_state_dict = model state_dict Check make sure model after prepare_qat has same state_dict original assertEqual set fq_state_dict keys set new_state_dict keys torch ao quantization convert model inplace=True model eval model load_state_dict quant_state_dict out = model x assertEqual ref out Check model created using prepare has same state dict quantized state_dict model = TwoLayerLinearModel model eval model = torch ao quantization QuantWrapper model model qconfig = torch ao quantization get_default_qconfig qengine torch ao quantization prepare model inplace=True torch ao quantization convert model inplace=True assertEqual set model state_dict keys set quant_state_dict keys model eval model load_state_dict quant_state_dict out = model x assertEqual ref out override_qengines test_forward_hooks_preserved r Test QAT preserving pre forward post forward hooks original model qengine = torch backends quantized engine model = QuantStubModel counter = pre_forwards forwards fw_pre_hook h_module input counter pre_forwards += fw_hook h_module input output counter forwards += model fc register_forward_pre_hook fw_pre_hook model fc register_forward_hook fw_hook model qconfig = torch ao quantization get_default_qat_qconfig qengine model = prepare_qat model checkHooksIsPresent model before_convert=True forward_hooks = before_convert assertEqual len model quant _forward_hooks values Quantization observer hook has disappeared forward_hooks = assertObjectIn fw_pre_hook model fc _forward_pre_hooks values assertObjectIn fw_hook model fc _forward_hooks values assertEqual len model fc _forward_pre_hooks values Extra pre forward hooks have appeared layer assertEqual len model fc _forward_hooks values forward_hooks Extra post forward hooks have appeared layer checkHooksIsPresent model True x = torch rand dtype=torch float model x torch ao quantization convert model inplace=True checkHooksIsPresent model False test_add_scalar_uses_input_qparams M torch nn Module __init__ - None super __init__ quant = torch ao quantization QuantStub ff = torch ao nn quantized FloatFunctional forward x x = quant x x = ff add_scalar x x m = M m qconfig = torch ao quantization default_qconfig mp = torch ao quantization prepare_qat m mp torch randn mq = torch ao quantization convert mp res = mq torch randn eps = e- assertTrue torch abs mq quant scale - res q_scale eps test_mul_scalar_uses_input_qparams M torch nn Module __init__ - None super __init__ quant = torch ao quantization QuantStub ff = torch ao nn quantized FloatFunctional forward x x = quant x x = ff mul_scalar x x m = M m qconfig = torch ao quantization default_qconfig mp = torch ao quantization prepare_qat m mp torch randn mq = torch ao quantization convert mp res = mq torch randn eps = e- assertTrue torch abs mq quant scale - res q_scale eps override_qengines test_qat_embedding_bag_errors default_qat_qconfig = get_default_qat_qconfig torch backends quantized engine Test constructor parameters checks here assertRaisesRegex AssertionError qconfig must provided QAT module nnqat EmbeddingBag qconfig=None assertRaisesRegex AssertionError Embedding Bag weights requires qscheme + torch per_channel_affine_float_qparams nnqat EmbeddingBag qconfig=default_qat_qconfig Test from_float checks here embed = nn Embedding assertRaisesRegex AssertionError qat EmbeddingBag from_float only works EmbeddingBag nnqat EmbeddingBag from_float embed embed_bag = nn EmbeddingBag assertRaisesRegex AssertionError Input float module must have qconfig defined nnqat EmbeddingBag from_float embed_bag embed_bag qconfig = None assertRaisesRegex AssertionError Input float module must have valid qconfig nnqat EmbeddingBag from_float embed_bag embed_bag qconfig = default_qat_qconfig assertRaisesRegex AssertionError Embedding Bag weights requires qscheme + torch per_channel_affine_float_qparams nnqat EmbeddingBag from_float embed_bag test_embedding_qat_qconfig_equal Embedding QAT uses NoopObserver activation FakeQuant weight make sure qconfig comparison functions properly mix partial function qconfig model = ManualEmbeddingBagLinear train model = prepare_qat model assertTrue qconfig_equals model emb qconfig default_embedding_qat_qconfig TestQuantizeEagerQATNumerics QuantizationTestCase _test_activation_convert_numerics_impl Act data M torch nn Module __init__ - None super __init__ act = Act quant = QuantStub dequant = DeQuantStub forward x x = quant x x = act x x = dequant x x m = M train m qconfig = default_qat_qconfig m = prepare_qat m before_convert = m data m = convert m after_convert = m data assertEqual before_convert after_convert test_fixed_qparam_ops M torch nn Module __init__ - None super __init__ sigmoid = torch nn Sigmoid hardsigmoid = torch nn Hardsigmoid tanh = torch nn Tanh quant = QuantStub dequant = DeQuantStub forward x x = quant x x = sigmoid x x = hardsigmoid x x = tanh x x = dequant x x m = M train m qconfig = default_qat_qconfig m = prepare_qat m attr sigmoid hardsigmoid tanh assertEqual type getattr m attr activation_post_process FixedQParamsFakeQuantize data = torch randn before_convert = m data m = convert m after_convert = m data assertEqual before_convert after_convert make sure activation post process removed attr sigmoid hardsigmoid tanh verify fake quant module removd assertFalse hasattr getattr m attr activation_post_process verify hooks removed assertTrue len getattr m attr _forward_hooks items == make sure no fake quantize module inserted eval mode checkNoFQModule m attr sigmoid hardsigmoid tanh assertFalse hasattr getattr m attr activation_post_process assertTrue len getattr m attr _forward_hooks items == m = M eval m qconfig = default_qconfig m = prepare m checkNoFQModule m m = convert m checkNoFQModule m test_leaky_relu data = torch randn _test_activation_convert_numerics_impl nn LeakyReLU data test_relu M torch nn Module __init__ - None super __init__ relu = nn ReLU forward x x = relu x x m = M train m qconfig = default_qconfig m = prepare_qat m make sure no activation_post_process inserted relu assertFalse hasattr m activation_post_process m = convert m make sure ReLU module changed assertTrue type m relu nn ReLU given batch_size=st integers input_channels_per_group=st sampled_from height=st integers width=st integers output_channels_per_group=st sampled_from groups=st integers kernel_h=st integers kernel_w=st integers stride_h=st integers stride_w=st integers pad_h=st integers pad_w=st integers dilation=st integers padding_mode=st sampled_from zeros circular use_relu=st booleans eps=st sampled_from e- e- e- momentum=st sampled_from freeze_bn=st booleans zero_gamma=st booleans has_bias=st booleans use_slow_fusion=st booleans test_conv_bn_relu batch_size input_channels_per_group height width output_channels_per_group groups kernel_h kernel_w stride_h stride_w pad_h pad_w dilation padding_mode use_relu eps momentum freeze_bn zero_gamma has_bias use_slow_fusion input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups dilation_h = dilation_w = dilation conv_op = Conv d input_channels output_channels kernel_h kernel_w stride_h stride_w pad_h pad_w dilation_h dilation_w groups has_bias padding_mode dtype=torch double bn_op = BatchNorm d output_channels eps momentum dtype=torch double relu_op = ReLU cls = ConvBnReLU d use_relu ConvBn d qat_op = cls input_channels output_channels kernel_h kernel_w stride_h stride_w pad_h pad_w dilation_h dilation_w groups has_bias padding_mode eps momentum freeze_bn=True qconfig=default_qat_qconfig dtype=torch double qat_op _enable_slow_path_for_better_numerical_stability = use_slow_fusion approximate fusion will work bn weight has zero_gamma use_slow_fusion torch nn init zeros_ qat_op bn weight qat_op apply torch ao quantization disable_fake_quant freeze_bn qat_op apply torch ao nn intrinsic qat freeze_bn_stats qat_op apply torch ao nn intrinsic qat update_bn_stats align inputs internal parameters input = torch randn batch_size input_channels height width dtype=torch double requires_grad=True conv_op weight = torch nn Parameter qat_op weight detach has_bias conv_op bias = torch nn Parameter qat_op bias detach bn_op running_mean = qat_op bn running_mean clone bn_op running_var = qat_op bn running_var clone bn_op weight = torch nn Parameter qat_op bn weight detach bn_op bias = torch nn Parameter qat_op bn bias detach compose functions functions reversed natural reading order reduce lambda f g lambda x f g x functions - lambda x x use_relu relu_op x noqa F x freeze_bn ref_op x x = conv_op x x = x - bn_op running_mean reshape - bn_op weight torch sqrt bn_op running_var + bn_op eps reshape - + bn_op bias reshape - x = relu_op x x ref_op = compose conv_op bn_op relu_op input_clone = input detach clone requires_grad_ _ range result_ref = ref_op input result_actual = qat_op input_clone assertEqual result_ref result_actual backward dout = torch randn result_ref size dtype=torch double loss = result_ref - dout sum loss backward input_grad_ref = input grad cpu weight_grad_ref = conv_op weight grad cpu gamma_grad_ref = bn_op weight grad cpu beta_grad_ref = bn_op bias grad cpu running_mean_ref = bn_op running_mean running_var_ref = bn_op running_var num_batches_tracked_ref = bn_op num_batches_tracked loss = result_actual - dout sum loss backward input_grad_actual = input_clone grad cpu weight_grad_actual = qat_op weight grad cpu gamma_grad_actual = qat_op bn weight grad cpu beta_grad_actual = qat_op bn bias grad cpu running_mean_actual = qat_op bn running_mean running_var_actual = qat_op bn running_var num_batches_tracked_actual = qat_op bn num_batches_tracked precision = e- assertEqual input_grad_ref input_grad_actual atol=precision rtol= assertEqual weight_grad_ref weight_grad_actual atol=precision rtol= assertEqual gamma_grad_ref gamma_grad_actual atol=precision rtol= assertEqual beta_grad_ref beta_grad_actual atol=precision rtol= assertEqual num_batches_tracked_ref num_batches_tracked_actual atol=precision rtol= assertEqual running_mean_ref running_mean_actual atol=precision rtol= assertEqual running_var_ref running_var_actual atol=precision rtol= given batch_size=st integers input_channels_per_group=st sampled_from height=st integers width=st integers output_channels_per_group=st sampled_from groups=st integers kernel_h=st integers kernel_w=st integers stride_h=st integers stride_w=st integers pad_h=st integers pad_w=st integers dilation=st integers padding_mode=st sampled_from zeros circular eps=st sampled_from e- e- e- momentum=st sampled_from freeze_bn=st booleans bias=st booleans test_conv_bn_folded_vs_unfolded batch_size input_channels_per_group height width output_channels_per_group groups kernel_h kernel_w stride_h stride_w pad_h pad_w dilation padding_mode eps momentum freeze_bn bias input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups dilation_h = dilation_w = dilation qat_op = ConvBn d input_channels output_channels kernel_h kernel_w stride_h stride_w pad_h pad_w dilation_h dilation_w groups bias bias padding_mode eps momentum freeze_bn=freeze_bn qconfig=default_qat_qconfig dtype=torch double qat_ref_op = _ReferenceConvBn d input_channels output_channels kernel_h kernel_w stride_h stride_w pad_h pad_w dilation_h dilation_w groups bias bias padding_mode eps momentum freeze_bn=freeze_bn qconfig=default_qat_qconfig dtype=torch double qat_op apply torch ao quantization disable_fake_quant qat_ref_op apply torch ao quantization disable_fake_quant align inputs internal parameters qat_ref_op weight = torch nn Parameter qat_op weight detach clone qat_ref_op running_mean = qat_op bn running_mean clone qat_ref_op running_var = qat_op bn running_var clone qat_ref_op gamma = torch nn Parameter qat_op bn weight detach clone qat_ref_op beta = torch nn Parameter qat_op bn bias detach clone qat_op bias None qat_ref_op bias = torch nn Parameter qat_op bias detach clone lr = qat_op_optim = torch optim SGD qat_op parameters lr=lr qat_ref_op_optim = torch optim SGD qat_ref_op parameters lr=lr i range make sure calling model train does override bn freeze setting qat_op train qat_ref_op train qat_op_optim zero_grad qat_ref_op_optim zero_grad input = torch randn batch_size input_channels height width dtype=torch double requires_grad=True input_clone = input detach clone requires_grad_ i qat_op apply torch ao nn intrinsic qat freeze_bn_stats qat_ref_op freeze_bn_stats i qat_op apply torch ao quantization disable_observer qat_ref_op apply torch ao quantization disable_observer result_ref = qat_ref_op input result_actual = qat_op input_clone assertEqual result_ref result_actual backward dout = torch randn result_ref size dtype=torch double + loss = result_ref - dout sum loss backward input_grad_ref = input grad cpu weight_grad_ref = qat_ref_op weight grad cpu gamma_grad_ref = qat_ref_op gamma grad cpu beta_grad_ref = qat_ref_op beta grad cpu running_mean_ref = qat_ref_op running_mean running_var_ref = qat_ref_op running_var num_batches_tracked_ref = qat_ref_op num_batches_tracked loss = result_actual - dout sum loss backward input_grad_actual = input_clone grad cpu weight_grad_actual = qat_op weight grad cpu gamma_grad_actual = qat_op bn weight grad cpu beta_grad_actual = qat_op bn bias grad cpu running_mean_actual = qat_op bn running_mean running_var_actual = qat_op bn running_var num_batches_tracked_actual = qat_op bn num_batches_tracked precision = e- assertEqual input_grad_ref input_grad_actual atol=precision rtol= assertEqual weight_grad_ref weight_grad_actual atol=precision rtol= assertEqual gamma_grad_ref gamma_grad_actual atol=precision rtol= assertEqual beta_grad_ref beta_grad_actual atol=precision rtol= assertEqual num_batches_tracked_ref num_batches_tracked_actual atol=precision rtol= assertEqual running_mean_ref running_mean_actual atol=precision rtol= assertEqual running_var_ref running_var_actual atol=precision rtol= qat_op_optim step qat_ref_op_optim step override_qengines test_linear_bn_numerics qengine = torch backends quantized engine m_ref = nn Sequential nn Linear nn BatchNorm d m_ref_copy = copy deepcopy m_ref m_ref_copy = torch ao quantization fuse_modules_qat m_ref_copy qconfig = torch ao quantization get_default_qat_qconfig qengine m_ref_copy qconfig = qconfig m = nniqat LinearBn d from_float m_ref_copy without fake_quants fused QAT module should match fp module m apply torch ao quantization disable_fake_quant data = torch randn r = m_ref data r = m data assertTrue torch allclose r r skipIfNoXNNPACK override_qengines test_linear_bn_symm_numerics qengine = torch backends quantized engine qengine = qnnpack Only qnnpack support symmetric quantization m_ref = nn Sequential nn Linear nn BatchNorm d m_ref_copy = copy deepcopy m_ref m_ref_copy = torch ao quantization fuse_modules_qat m_ref_copy qconfig = default_symmetric_qnnpack_qat_qconfig m_ref_copy qconfig = qconfig m = nniqat LinearBn d from_float m_ref_copy without fake_quants fused QAT module should match fp module m apply torch ao quantization disable_fake_quant data = torch randn r = m_ref data r = m data assertTrue torch allclose r r override_qengines test_linear_bn_workflow qengine = torch backends quantized engine m = nn Sequential QuantStub nn Linear nn BatchNorm d data = torch randn m qconfig = torch ao quantization get_default_qat_qconfig qengine m = torch ao quantization fuse_modules_qat m mp = prepare_qat m mp data mq = convert mp assertTrue type mq nnq Linear assertTrue type mq nn Identity skipIfNoXNNPACK override_qengines test_linear_precomputed_fake_quant qengine = torch backends quantized engine qengine = qnnpack Only qnnpack support symmetric quantization m_ref = nn Linear m_ref_copy = copy deepcopy m_ref qconfig = default_qconfig m_ref_copy qconfig = qconfig weight_post_process = copy deepcopy qconfig weight activation = copy deepcopy qconfig activation activation torch randn m_ref_copy activation_post_process = activation m_ref_copy = nnq Linear from_float m_ref_copy weight_post_process = qconfig weight weight_post_process min_val = torch tensor - weight_post_process max_val = torch tensor m_ref weight_post_process = weight_post_process m_ref activation_post_process = activation m_ref qconfig = qconfig m_ref = nnq Linear from_float m_ref use_precomputed_fake_quant=True assertTrue m_ref _weight_bias q_scale = m_ref_copy _weight_bias q_scale __name__ == __main__ raise RuntimeError This test file meant run directly use \n\n \tpython test test_quantization py TESTNAME\n\n instead