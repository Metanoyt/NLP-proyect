mypy allow-untyped-defs multiprocessing os threading multiprocessing reduction multiprocessing util register_after_fork typing Union torch torch _namedtensor_internals check_serializing_named_tensor try Early load resource_sharer prevent partially initialized instance being inherited forked child process The reduce_storage method requires module indirectly through DupFd The built-in mp Queue pickles arguments background thread which may overlap fork multiprocessing resource_sharer except ImportError pass StorageWeakRef r A weak reference Storage The cdata member Python number containing integer representation Storage pointer __slots__ = cdata _free_weak_ref __init__ storage cdata = storage _weak_ref Save direct reference _free_weak_ref because ` torch ` module might cleared during Python shutdown before module cleared _free_weak_ref = torch Storage _free_weak_ref type ignore attr-defined classmethod from_weakref cls cdata instance = cls __new__ cls instance cdata = cdata instance _free_weak_ref = torch Storage _free_weak_ref type ignore attr-defined instance expired torch Storage _expired cdata type ignore attr-defined __del__ _free_weak_ref cdata __hash__ cdata __eq__ other id == id other True cdata == other cdata SharedCache dict Dictionary multiprocessing handles StorageWeakRef __init__ - None free_dead_references called len exceeds current limit The limit scales number remaining live objects limit = ` fork ` inherits lock state so case we fork when lock held we register function reset lock new object avoid possible deadlocks following python multiprocessing library design _after_fork register_after_fork SharedCache _after_fork _after_fork lock = threading Lock get key type ignore override lock dict get key __setitem__ key storage_ref lock dict __setitem__ key storage_ref len limit free_dead_references free_dead_references live = key storage_ref list items storage_ref expired del key live += limit = max live mapping handles StorageWeakRef objects shared_cache = SharedCache rebuild_event device handle torch cuda Event from_ipc_handle device handle reduce_event event handle = event ipc_handle rebuild_event event device handle rebuild_tensor cls storage metadata storage_offset size stride requires_grad = metadata t = torch _utils _rebuild_tensor storage storage_offset size stride cls == torch nn parameter Parameter we have pass requires_grad into constructor rather than set attribute later because s important check Integer Tensors have requires_grad=False they raise error t = torch nn parameter Parameter t requires_grad=requires_grad t requires_grad = requires_grad t rebuild_meta_tensor tensor_cls tensor_size tensor_stride tensor_offset dtype storage_size_bytes requires_grad untyped_storage = torch UntypedStorage storage_size_bytes device= meta typed_storage = torch TypedStorage wrap_storage=untyped_storage dtype=dtype _internal=True t = torch _utils _rebuild_tensor typed_storage tensor_offset tensor_size tensor_stride tensor_cls == torch nn parameter Parameter It crucial integer tensors receive requires_grad=False argument constructor t = torch nn parameter Parameter t requires_grad=requires_grad t requires_grad = requires_grad t rebuild_cuda_tensor tensor_cls tensor_size tensor_stride tensor_offset storage_cls dtype storage_device storage_handle storage_size_bytes storage_offset_bytes requires_grad ref_counter_handle ref_counter_offset event_handle event_sync_required If storage_handle None storage points nullptr storage_handle None storage_size_bytes == storage = storage_cls dtype=dtype device=storage_device _internal=True storage = storage_from_cache storage_cls storage_handle storage_offset_bytes storage None torch cuda _lazy_init storage = storage_cls _new_shared_cuda storage_device storage_handle storage_size_bytes storage_offset_bytes ref_counter_handle ref_counter_offset event_handle event_sync_required shared_cache storage_handle storage_offset_bytes = StorageWeakRef storage We already ref counting Storage producer needs new ref-counters released storage_cls _release_ipc_counter ref_counter_handle ref_counter_offset device=storage_device _storage = storage isinstance storage torch UntypedStorage storage _untyped_storage t = torch _utils _rebuild_tensor torch storage TypedStorage wrap_storage=_storage dtype=dtype _internal=True tensor_offset tensor_size tensor_stride tensor_cls == torch nn parameter Parameter It crucial integer tensors receive requires_grad=False argument constructor t = torch nn parameter Parameter t requires_grad=requires_grad t requires_grad = requires_grad t reduce_tensor tensor tensor requires_grad tensor is_leaf raise RuntimeError Cowardly refusing serialize non-leaf tensor which requires_grad since autograd does support crossing process boundaries If you just want transfer data call detach tensor before serializing e g putting queue check_serializing_named_tensor tensor torch utils hooks warn_if_has_hooks tensor Note CUDA IPC caching allocator ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ When you send CUDA tensor over IPC you might expect you will get out same storage other end However CUDA caching allocator makes difficult preserve invariant Consider following situation tensor size x points offset x storage xA size x For simplicity all these sizes given bytes HOWEVER caching allocator storage might part larger cudaMalloc allocation xA size x When we want send CUDA tensor over IPC we must send entire cudaMalloc allocation i e xA region just storage xA because what CUDA supports So other end there simply isn t any way say Wait you gave me bigger region xA than one I wanted xA OK so you sent cudaMalloc allocation can you just wrap up one storage itself No because cudaMalloc allocation might contain storages mixed types float bytes double If you make entire allocation single storage type A we ll hit error when constructing tensor type B storage cudaIpcMemHandle identifier access sender cudaMalloc allocation receiver side However cudaIpcMemHandles each device given process may only opened one context per device per other process If we open close memory handle multiples times process CUDA allowed give different address similarly once we close memory we re allowed access storage tensor built top even still live original process As we cannot make cudaMalloc allocation single storage one go requires us cache device pointer each cudaIpcMemHandle C++ side reconstruct types storages while keep old ones alives See https docs nvidia com cuda cuda-runtime-api group__CUDART__DEVICE html This fine because all we need do save our position allocation reconstruct storage tensor xA - ------- CUDA Allocation ------ &#124; &#124; &#124; &#124; &#124; &#124; &#124; &#124; xA - -------- storage begin ------ &#124; &#124; xA - -------- tensor begin ------ &#124; &#124; &#124; &#124; &#124; &#124; &#124; &#124; &#124; &#124; xA - -------- tensor end --------- &#124; &#124; &#124; &#124; &#124; &#124; xA - -------- storage end -------- &#124; &#124; xE - -------- CUDA allocation ----- To send tensor following info required sender receiver storage reconstruction cudaIpcMemHandle xA which can mapped basePtr receiver process basePtr may exactly xA since s different process offset xA storage CUDA allocation size storage x On receiver side Get devPtr MemHandle access memory reconstruct storage same type using basePtr offset size we can reconstruct tensor top reconstructed storage Tensor size= x offset= x storage=Storage data=basePtr+ xA size= x This strategy has few implications When we serialize CUDA tensor IPC we cannot do all one go non-compositionally requires have global map memHandle - devPtr each process We MUST NOT let new IPC tensor resizable Originally resize storage beyond x would merely have caused us do reallocation You don t really want do you did all would happen you would lose IPC sharing But you do new world we will happily let you write out bounds your allocation clobbering unrelated data cached allocator block BAD By way old versions PyTorch we supported situation natively using storage view which permitted multiple storages views each other But only use storage views so we eliminated so we could just use tensor views implement same thing TODO Handle distinguishing between subclass non-subclass versions NT better https github com pytorch pytorch issues torch nested _internal nested_tensor NestedTensor tensor is_nested isinstance tensor NestedTensor reduce_nested_tensor tensor tensor layout torch sparse_coo torch sparse_csr torch sparse_bsr torch sparse_csc torch sparse_bsc reduce_sparse_tensor tensor storage = tensor _typed_storage storage _untyped_storage device type == cuda device handle storage_size_bytes storage_offset_bytes ref_counter_handle ref_counter_offset event_handle event_sync_required = storage _share_cuda_ tensor_offset = tensor storage_offset shared_cache handle = StorageWeakRef storage _backward_hooks purposely omitted here see Note Don t serialize hooks rebuild_cuda_tensor type tensor tensor size tensor stride tensor_offset tensor offset its storage type storage tensor dtype device handle identifier which CUDA allocation storage storage_size_bytes size bytes storage storage_offset_bytes offset bytes storage CUDA allocation tensor requires_grad ref_counter_handle ref_counter_offset event_handle event_sync_required storage _untyped_storage device type == meta rebuild_meta_tensor type tensor tensor size tensor stride tensor storage_offset tensor dtype tensor untyped_storage size tensor requires_grad _backward_hooks purposely omitted here see Note Don t serialize hooks metadata = tensor storage_offset tensor size tensor stride tensor requires_grad rebuild_tensor type tensor storage metadata rebuild_nested_tensor rebuild_buffer_func rebuild_buffer_args rebuild_sizes_func rebuild_sizes_args rebuild_strides_func rebuild_strides_args rebuild_offsets_func rebuild_offsets_args buffer = rebuild_buffer_func rebuild_buffer_args sizes = rebuild_sizes_func rebuild_sizes_args strides = rebuild_strides_func rebuild_strides_args offsets = rebuild_offsets_func rebuild_offsets_args torch _nested_view_from_buffer_copy buffer sizes strides offsets reduce_nested_tensor nt rebuild_buffer_func rebuild_buffer_args = reduce_tensor nt values rebuild_sizes_func rebuild_sizes_args = reduce_tensor nt _nested_tensor_size rebuild_strides_func rebuild_strides_args = reduce_tensor nt _nested_tensor_strides rebuild_offsets_func rebuild_offsets_args = reduce_tensor nt _nested_tensor_storage_offsets rebuild_nested_tensor rebuild_buffer_func rebuild_buffer_args rebuild_sizes_func rebuild_sizes_args rebuild_strides_func rebuild_strides_args rebuild_offsets_func rebuild_offsets_args rebuild_sparse_coo_tensor rebuild_indices_func rebuild_indices_args rebuild_values_func rebuild_values_args shape is_coalesced indices = rebuild_indices_func rebuild_indices_args values = rebuild_values_func rebuild_values_args torch sparse_coo_tensor indices values shape is_coalesced=is_coalesced rebuild_sparse_compressed_tensor rebuild_compressed_indices_func rebuild_compressed_indices_args rebuild_plain_indices_func rebuild_plain_indices_args rebuild_values_func rebuild_values_args shape layout compressed_indices = rebuild_compressed_indices_func rebuild_compressed_indices_args plain_indices = rebuild_plain_indices_func rebuild_plain_indices_args values = rebuild_values_func rebuild_values_args torch sparse_compressed_tensor compressed_indices plain_indices values shape layout=layout reduce_sparse_tensor sparse sparse layout torch sparse_coo rebuild_indices_func rebuild_indices_args = reduce_tensor sparse _indices rebuild_values_func rebuild_values_args = reduce_tensor sparse _values rebuild_sparse_coo_tensor rebuild_indices_func rebuild_indices_args rebuild_values_func rebuild_values_args sparse shape sparse is_coalesced sparse layout torch sparse_csr torch sparse_bsr compressed_indices = sparse crow_indices plain_indices = sparse col_indices sparse layout torch sparse_csc torch sparse_bsc compressed_indices = sparse ccol_indices plain_indices = sparse row_indices raise NotImplementedError sparse layout rebuild_compressed_indices_func rebuild_compressed_indices_args = reduce_tensor compressed_indices rebuild_plain_indices_func rebuild_plain_indices_args = reduce_tensor plain_indices rebuild_values_func rebuild_values_args = reduce_tensor sparse values rebuild_sparse_compressed_tensor rebuild_compressed_indices_func rebuild_compressed_indices_args rebuild_plain_indices_func rebuild_plain_indices_args rebuild_values_func rebuild_values_args sparse shape sparse layout fd_id fd Returns tuple which uniquely identifies file descriptor In Mac OS doesn t work shared memory handles which why we don t support file_descriptor sharing method platform stat = os fstat fd stat st_ino stat st_dev storage_from_cache cls key storage_ref = shared_cache get key storage_ref None None torch UntypedStorage _new_with_weak_ptr storage_ref cdata rebuild_storage_fd cls df size fd = df detach try storage = storage_from_cache cls fd_id fd storage None storage storage = cls _new_shared_fd_cpu fd size shared_cache fd_id fd = StorageWeakRef storage storage finally os close fd rebuild_storage_filename cls manager handle size dtype=None storage Union torch TypedStorage torch UntypedStorage = storage_from_cache cls handle storage None storage _shared_decref dtype None storage = torch UntypedStorage _new_shared_filename_cpu manager handle size byte_size = size torch _utils _element_size dtype untyped_storage torch UntypedStorage = torch UntypedStorage _new_shared_filename_cpu manager handle byte_size storage = torch TypedStorage wrap_storage=untyped_storage dtype=dtype _internal=True shared_cache handle = StorageWeakRef storage storage _shared_decref rebuild_storage_empty cls cls rebuild_typed_storage storage dtype torch storage TypedStorage wrap_storage=storage dtype=dtype _internal=True Use torch storage TypedStorage reduce_typed_storage storage rebuild_typed_storage storage _untyped_storage storage dtype rebuild_typed_storage_child storage storage_type storage_type wrap_storage=storage _internal=True Use child classes torch storage TypedStorage like torch FloatStorage reduce_typed_storage_child storage rebuild_typed_storage_child storage _untyped_storage type storage reduce_storage storage get_sharing_strategy storage is_cuda raise RuntimeError Cannot pickle CUDA storage try pickling CUDA tensor instead storage device type == meta raise RuntimeError Cannot pickle meta storage try pickling meta tensor instead get_sharing_strategy == file_system metadata = storage _share_filename_cpu_ cache_key = metadata rebuild = rebuild_storage_filename isinstance storage torch TypedStorage metadata += storage dtype storage _shared_incref storage size == This special cased because Empty tensors size cannot mmapped rebuild_storage_empty type storage fd size = storage _share_fd_cpu_ df = multiprocessing reduction DupFd fd cache_key = fd_id fd metadata = df size rebuild = rebuild_storage_fd type ignore assignment shared_cache cache_key = StorageWeakRef storage rebuild type storage + metadata init_reductions reduction register torch cuda Event reduce_event t torch _storage_classes t __name__ == UntypedStorage reduction register t reduce_storage reduction register t reduce_typed_storage_child reduction register torch storage TypedStorage reduce_typed_storage t torch _tensor_classes reduction register t reduce_tensor TODO Maybe should tensor_classes reduction register torch Tensor reduce_tensor torch nn parameter Parameter reduction register Parameter reduce_tensor