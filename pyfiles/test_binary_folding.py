Owner s module inductor functools importlib itertools os sys torch torch nn torch _dynamo utils counters torch _inductor config inductor_config torch testing _internal common_cuda TEST_CUDNN Make helper files test importable pytorch_test_dir = os path dirname os path dirname os path realpath __file__ sys path append pytorch_test_dir inductor test_inductor_freezing manual=fbcode caffe test inductor inductor_freezing-library TestCase inductor test_torchinductor manual=fbcode caffe test inductor test_inductor-library check_model check_model_gpu copy_tests torch testing _internal inductor_utils skipCUDAIf importlib import_module functorch importlib import_module filelock torch testing _internal inductor_utils GPU_TYPE HAS_CPU HAS_GPU aten = torch ops aten BinaryFoldingTemplate TestCase skipCUDAIf TEST_CUDNN CUDNN has accuracy issues test test_conv_binary_folding torch no_grad test_conv_fusion use_bias module op scalar add_tensor expect_success rtol=None atol=None ConvOp nn Module __constants__ = use_scalar __init__ in_channels out_channels device kwargs super __init__ conv = module in_channels out_channels bias=use_bias kwargs device use_scalar = scalar tensor_size = _ range conv weight ndim tensor_size = conv weight size tensor = torch nn Parameter add_tensor add_tensor None torch rand tensor_size device op = op forward x x = conv x use_scalar op x op x tensor torch _dynamo reset counters clear mod_eager = ConvOp device kernel_size= stride= eval out_optimized = torch compile mod_eager inps = module nn Conv d inps append inps - module nn Conv d inps append inps - inps append inps - torch manual_seed inp = torch rand inps device out_eager = mod_eager inp out_optimized = out_optimized inp assertEqual out_optimized out_eager rtol=rtol atol=atol expect_success assertEqual counters inductor binary_folding assertEqual counters inductor binary_folding conv_bias = True False modules = nn Conv d nn Conv d nn Conv d use_scalar = True False ops = torch add torch sub torch mul torch div use_bias module pytorch_op scalar itertools product conv_bias modules ops use_scalar test_conv_fusion use_bias module pytorch_op scalar add_tensor=None expect_success=True use_bias pytorch_op itertools product conv_bias ops broadcasting add test_conv_fusion use_bias nn Conv d pytorch_op False add_tensor=torch rand device expect_success=False broadcasting add test_conv_fusion use_bias nn Conv d pytorch_op False add_tensor=torch rand device expect_success=True add different dtype test_conv_fusion use_bias nn Conv d pytorch_op False add_tensor=torch tensor torch float device expect_success=False This test float conv fusion different dtype like float which will fused The tolerance float too tight float conv post fusion float tensor Will relax tolerance case rtol= e- atol= e- inductor_config patch freezing True test_conv_bn_folding torch no_grad test_conv_fusion use_bias module expect_success ConvOp nn Module __init__ in_channels out_channels device kwargs super __init__ conv = module in_channels out_channels bias=use_bias kwargs device bn = module out_channels device forward x x = conv x bn x torch _inductor compile_fx compile_fx compile_fx_inner aten_binary = aten add Tensor aten sub Tensor aten mul Tensor aten div Tensor n_binary_ops = my_inner_compile gm example_inputs args kwargs out = compile_fx_inner gm example_inputs args kwargs nonlocal n_binary_ops binarry_ops = n n gm graph nodes n target aten_binary n_binary_ops += len binarry_ops out torch _dynamo reset mod_eager = ConvOp device kernel_size= stride= eval out_optimized = torch compile mod_eager backend=functools partial compile_fx inner_compile=my_inner_compile inps = module nn Conv d inps append inps - module nn Conv d inps append inps - inps append inps - inp = torch rand inps device out_eager = mod_eager inp out_optimized = out_optimized inp assertEqual out_optimized out_eager atol= e- rtol= e- expect_success assertTrue n_binary_ops == assertTrue n_binary_ops conv_bias = True False modules = nn Conv d nn BatchNorm d nn Conv d nn BatchNorm d nn Conv d nn BatchNorm d use_bias module itertools product conv_bias modules test_conv_fusion use_bias module expect_success=True inductor_config patch enable_linear_binary_folding True test_linear_binary_folding torch no_grad test_linear_fusion use_bias op scalar add_tensor expect_success input_ d=False LinearOp nn Module __constants__ = use_scalar __init__ in_channels out_channels device kwargs super __init__ linear = nn Linear in_channels out_channels bias=use_bias kwargs device use_scalar = scalar tensor_size = linear weight size tensor = torch nn Parameter add_tensor add_tensor None torch rand tensor_size device op = op forward x x = linear x use_scalar op x op x tensor torch _dynamo reset counters clear mod_eager = LinearOp device eval out_optimized = torch compile mod_eager torch manual_seed input_ d inp = torch rand device inp = torch rand device out_eager = mod_eager inp out_optimized = out_optimized inp assertEqual out_optimized out_eager atol= e- rtol= e- expect_success assertEqual counters inductor binary_folding assertEqual counters inductor binary_folding linear_bias = True False use_scalar = True False ops = torch add torch sub torch mul torch div add_tensor_size = use_bias pytorch_op scalar tensor_size itertools product linear_bias ops use_scalar add_tensor_size test_linear_fusion use_bias pytorch_op scalar add_tensor=torch rand tensor_size device expect_success=True add_tensor_size extend use_bias pytorch_op scalar tensor_size itertools product linear_bias ops use_scalar add_tensor_size test_linear_fusion use_bias pytorch_op scalar add_tensor=torch rand tensor_size device expect_success=True input_ d=True In following test shape add_tensor does satisfy requirements binary folding so will folded use_bias pytorch_op itertools product linear_bias ops test_linear_fusion use_bias pytorch_op False add_tensor=torch rand device expect_success=False test_linear_fusion use_bias pytorch_op False add_tensor=torch rand device expect_success=False HAS_CPU torch backends mps is_available FreezingCpuTests TestCase common = check_model device = cpu autocast = torch cpu amp autocast copy_tests BinaryFoldingTemplate FreezingCpuTests cpu HAS_GPU FreezingGpuTests TestCase common = check_model_gpu device = GPU_TYPE autocast = torch amp autocast device_type=GPU_TYPE copy_tests BinaryFoldingTemplate FreezingGpuTests GPU_TYPE del BinaryFoldingTemplate __name__ == __main__ torch _inductor test_case run_tests HAS_CPU HAS_GPU run_tests needs= filelock