mypy allow-untyped-defs typing Any Optional torch nn nn torch distributed tensor parallel _data_parallel_utils _flatten_tensor _unflatten_tensor __all__ = type ignore var-annotated _get_submodule_n_params module nn Module path str Get submodule direct path parameter module path path_list = path split parent_module_path = join path_list - module = module get_submodule parent_module_path path = path_list - module path _update_module_param param_list list tuple nn Module str nn Parameter Update parameters within module item param_list parent_module module_path t = item assert hasattr parent_module module_path delattr parent_module module_path setattr parent_module module_path t _reconstruct_dtensor module nn Module _input Any Reconstruct DTensor parameters local tensors param_list = TODO To add perf optimizations iterations name t module named_parameters hasattr t _st_info dtensor = _unflatten_tensor t t _st_info param_list append _get_submodule_n_params module name dtensor _update_module_param param_list type ignore arg-type _localize_dtensor module nn Module _ Any ignored_params Optional set nn Parameter = None Convert DTensor parameters local tensors ignored_params None ignored_params = set param_list = name param module named_parameters param ignored_params continue t sharding_info = _flatten_tensor param sharding_info None t = nn Parameter t t _st_info = sharding_info type ignore attr-defined param_list append _get_submodule_n_params module name t _update_module_param param_list type ignore arg-type _pre_dp_module_transform module nn Module Enable composability between Tensor Parallelism TP Data Parallelism DP PyTorch when using DDP We need convert Parameters which DTensors local tensors before wrapping data parallelism API We then register two hooks one converting local tensors back DTensor preforward one convert DTensors back tensors after Forward By integrating way we avoid any special handling DTensor parameters DDP get DTensor s gradients propagated back DP e g gradient buckets DDP For now API only works ` ` DistributedDataParallel ` ` It will later support other DP methods such FSDP Args module ` nn Module ` Module which has been applied TP Example xdoctest +SKIP distributed torch distributed tensor parallel parallelize_module PairwiseParallel torch nn parallel DistributedDataParallel DDP torch distributed tensor parallel ddp pre_dp_module_transform Define module m = module parallelize_module m PairwiseParallel m = pre_dp_module_transform m m = DDP m _localize_dtensor module None None TODO To add test cases ensure works nested modules module register_forward_pre_hook _reconstruct_dtensor module register_forward_hook _localize_dtensor