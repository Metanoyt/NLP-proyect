Owner s module tests Fuzzer-discovered eager compile divergence test cases All tests marked xfail since they represent known compilation bugs IF YOU ARE HERE YOU LIKELY DIDN T DO ANYTHING WRONG In fact you probably did something right All these tests associated bugs fuzzer found If one these tests starts failing due your PR actually means your PR fixed bug Feel free delete test close out issue linked test pytest torch torch testing _internal common_utils run_tests TestCase torch testing _internal inductor_utils HAS_CUDA_AND_TRITON TestFuzzerCompileIssues TestCase Test cases fuzzer-discovered eager compile divergence issues setUp Configure common test settings torch _dynamo config capture_scalar_outputs = True torch _dynamo config capture_dynamic_output_shape_ops = True torch _inductor config emulate_precision_casts = True pytest mark xfail reason= Issue test_fuzzer_issue_ torch manual_seed foo arg arg arg arg var_node_ = torch full dtype=torch float var_node_ = torch nn functional relu var_node_ var_node_ = torch full - dtype=torch bfloat var_node_ = arg size= stride= dtype=bfloat var_node_ = torch matmul var_node_ torch bfloat var_node_ torch bfloat var_node_ = torch full dtype=torch bfloat var_node_ = torch reshape var_node_ var_node_ = torch full dtype=torch bfloat var_node_ = torch reshape var_node_ var_node_ = torch cat var_node_ var_node_ var_node_ dim= var_node_ = arg size= stride= dtype=bfloat var_node_ = torch sub var_node_ var_node_ var_node_ = torch add var_node_ var_node_ var_node_ = torch full dtype=torch bfloat var_node_ = torch nn functional layer_norm var_node_ result = torch add var_node_ var_node_ output = result + arg + arg output arg = torch rand dtype=torch bfloat device= cuda requires_grad=True arg = torch rand dtype=torch bfloat device= cuda requires_grad=True arg = torch tensor dtype=torch bfloat device= cuda requires_grad=True arg = torch tensor dtype=torch bfloat device= cuda requires_grad=True out_eager = foo arg arg arg arg out_eager sum backward print Eager Success ✅ compiled_foo = torch compile foo fullgraph=True dynamic=True out_compiled = compiled_foo arg arg arg arg out_compiled sum backward print Compile Success ✅ pytest mark xfail reason= Issue test_fuzzer_issue_ torch manual_seed foo arg t = arg size= stride= dtype=float device=cuda t = t clone t zero_ t = t contiguous view t = t clone t zero_ output = t output arg = torch rand dtype=torch float device= cuda requires_grad=True out_eager = foo arg out_eager sum backward print Eager Success ✅ compiled_foo = torch compile foo fullgraph=True dynamic=True out_compiled = compiled_foo arg out_compiled sum backward print Compile Success ✅ pytest mark xfail reason= Issue test_fuzzer_issue_ torch manual_seed foo arg arg arg t = arg size= stride= dtype=bfloat device=cuda t = t mean dim= size= stride= dtype=bfloat device=cuda t = arg size= stride= dtype=int device=cuda t = arg size= stride= dtype=bfloat device=cuda t = torch nn functional embedding torch clamp t t size - torch long t t = torch pow torch pow torch pow torch pow t t t t t t = t contiguous view output = t output arg = torch rand dtype=torch bfloat device= cuda requires_grad=True arg = torch randint dtype=torch int device= cuda arg = torch rand dtype=torch bfloat device= cuda requires_grad=True out_eager = foo arg arg arg out_eager sum backward print Eager Success ✅ compiled_foo = torch compile foo fullgraph=True dynamic=True out_compiled = compiled_foo arg arg arg out_compiled sum backward print Compile Success ✅ pytest mark xfail reason= Issue test_fuzzer_issue_ torch manual_seed foo arg arg arg arg arg arg t = arg size= stride= dtype=int device=cuda t = torch tanh t size= stride= dtype=int device=cuda t = arg size= stride= dtype=int device=cuda t = arg size= stride= dtype=int device=cuda t = t t size= stride= dtype=int device=cuda t = t clone t fill_ t item t = arg size= stride= dtype=float device=cuda t = arg size= stride= dtype=float device=cuda t = arg size= stride= dtype=float device=cuda t = torch cat t t t t dim= t = t std dim= t = torch nn functional embedding torch clamp t t size - t output = t output arg = torch randint dtype=torch int device= cuda arg = torch randint dtype=torch int device= cuda arg = torch randint dtype=torch int device= cuda arg = torch rand dtype=torch float device= cuda requires_grad=True arg = torch rand dtype=torch float device= cuda requires_grad=True arg = torch rand dtype=torch float device= cuda requires_grad=True out_eager = foo arg arg arg arg arg arg out_eager sum backward print Eager Success ✅ compiled_foo = torch compile foo fullgraph=True dynamic=True out_compiled = compiled_foo arg arg arg arg arg arg out_compiled sum backward print Compile Success ✅ pytest mark xfail reason= Issue test_fuzzer_issue_ _already_exists torch manual_seed foo arg arg arg var_node_ = arg size= stride= dtype=float device=cuda var_node_ = torch full - dtype=torch float var_node_ = torch div var_node_ var_node_ var_node_ = torch flatten var_node_ var_node_ = torch full - dtype=torch float var_node_ = arg size= stride= dtype=float var_node_ = torch matmul var_node_ torch float var_node_ torch float var_node_ = arg size= stride= dtype=float var_node_ = torch sub var_node_ var_node_ var_node_ = torch sub var_node_ var_node_ output = var_node_ output arg = torch rand dtype=torch float device= cuda requires_grad=True arg = torch rand dtype=torch float device= cuda requires_grad=True arg = torch rand dtype=torch float device= cuda requires_grad=True out_eager = foo arg arg arg out_eager sum backward print Eager Success ✅ compiled_foo = torch compile foo fullgraph=True dynamic=True out_compiled = compiled_foo arg arg arg out_compiled sum backward print Compile Success ✅ pytest mark xfail reason= Issue test_fuzzer_issue_ torch manual_seed foo arg arg t = arg size= stride= dtype=float device=cuda t = t clone t zero_ size= stride= dtype=float device=cuda t = t reshape size= stride= dtype=float device=cuda t = arg size= stride= dtype=float device=cuda t = t contiguous size= stride= dtype=float device=cuda t = torch nn functional relu t size= stride= dtype=float device=cuda t = t clone t fill_ t item size= stride= dtype=float device=cuda output = t output arg = torch rand dtype=torch float device= cuda requires_grad=True arg = torch rand dtype=torch float device= cuda requires_grad=True out_eager = foo arg arg out_eager sum backward print Eager Success ✅ compiled_foo = torch compile foo fullgraph=True dynamic=True out_compiled = compiled_foo arg arg out_compiled sum backward print Compile Success ✅ pytest mark xfail reason= Issue test_fuzzer_issue_ torch manual_seed foo arg t = arg size= stride= dtype=bfloat device=cuda t = torch softmax t dim= size= stride= dtype=bfloat device=cuda t = torch nn functional gelu t size= stride= dtype=bfloat device=cuda t = torch softmax t dim= size= stride= dtype=bfloat device=cuda output = t output arg = torch rand dtype=torch bfloat device= cuda requires_grad=True out_eager = foo arg out_eager sum backward print Eager Success ✅ compiled_foo = torch compile foo fullgraph=True dynamic=True out_compiled = compiled_foo arg out_compiled sum backward print Compile Success ✅ pytest mark xfail reason= Issue test_fuzzer_issue_ torch manual_seed foo arg arg arg t = arg size= stride= dtype=float device=cuda t = t clone t zero_ size= stride= dtype=float device=cuda t = t contiguous view size= stride= dtype=float device=cuda t = arg size= stride= dtype=int device=cuda t = arg size= stride= dtype=int device=cuda t = t + t + t size= stride= dtype=int device=cuda t = torch exp noqa F t size= stride= dtype=int device=cuda noqa F t = torch nn functional layer_norm t size= stride= dtype=float device=cuda output = t output arg = torch rand dtype=torch float device= cuda requires_grad=True arg = torch randint dtype=torch int device= cuda arg = torch randint dtype=torch int device= cuda out_eager = foo arg arg arg out_eager sum backward print Eager Success ✅ compiled_foo = torch compile foo fullgraph=True dynamic=True out_compiled = compiled_foo arg arg arg out_compiled sum backward print Compile Success ✅ pytest mark xfail reason= Issue test_fuzzer_issue_ torch manual_seed foo arg arg arg arg arg t = arg size= stride= dtype=bfloat device=cuda t = t clone t zero_ size= stride= dtype=bfloat device=cuda t = t contiguous view size= stride= dtype=bfloat device=cuda t = arg size= stride= dtype=bfloat device=cuda t = t min size= stride= dtype=bfloat device=cuda t = arg size= stride= dtype=bfloat device=cuda t = torch nn functional silu t size= stride= dtype=bfloat device=cuda t = arg size= stride= dtype=bfloat device=cuda t = t min size= stride= dtype=bfloat device=cuda t = arg size= stride= dtype=bfloat device=cuda t = t t t size= stride= dtype=bfloat device=cuda t = t + t + t + t + t size= stride= dtype=bfloat device=cuda t = t clone t fill_ t item size= stride= dtype=bfloat device=cuda output = t output arg = torch rand dtype=torch bfloat device= cuda requires_grad=True arg = torch rand dtype=torch bfloat device= cuda requires_grad=True arg = torch rand dtype=torch bfloat device= cuda requires_grad=True arg = torch rand dtype=torch bfloat device= cuda requires_grad=True arg = torch rand dtype=torch bfloat device= cuda requires_grad=True out_eager = foo arg arg arg arg arg out_eager sum backward print Eager Success ✅ compiled_foo = torch compile foo fullgraph=True dynamic=True out_compiled = compiled_foo arg arg arg arg arg out_compiled sum backward print Compile Success ✅ pytest mark xfail reason= Issue test_fuzzer_issue_ torch manual_seed foo arg var_node_ = arg size= stride= dtype=int device=cuda noqa F var_node_ = torch full - dtype=torch int size= stride= dtype=int device=cuda var_node_ = torch full dtype=torch int size= stride= dtype=int device=cuda var_node_ = torch ops aten add var_node_ var_node_ size= stride= dtype=int device=cuda var_node_ = torch full - dtype=torch int size= stride= dtype=int device=cuda var_node_ = torch ops aten mul var_node_ var_node_ size= stride= dtype=int device=cuda var_node_ = torch full False dtype=torch bool size= stride= dtype=bool device=cuda var_node_ = torch nonzero var_node_ size= stride= dtype=int device=cuda var_node_ numel == var_node_ = torch zeros dtype=torch int device= cuda var_node_ = torch ops aten add var_node_ var_node_ output = var_node_ float output arg = torch randint dtype=torch int device= cuda out_eager = foo arg out_eager sum backward print Eager Success ✅ compiled_foo = torch compile foo fullgraph=True dynamic=True out_compiled = compiled_foo arg out_compiled sum backward print Compile Success ✅ pytest mark xfail reason= Issue test_fuzzer_issue_ torch manual_seed foo arg var_node_ = torch full dtype=torch int size= stride= dtype=int device=cuda var_node_ = arg size= stride= dtype=int device=cuda var_node_ = torch add var_node_ var_node_ size= stride= dtype=int device=cuda var_node_ = torch full dtype=torch int size= stride= dtype=int device=cuda var_node_ = torch squeeze var_node_ size= stride= dtype=int device=cuda var_node_ = torch div var_node_ var_node_ size= stride= dtype=int device=cuda result = var_node_ float result arg = torch randint dtype=torch int device= cuda out_eager = foo arg out_eager sum backward print Eager Success ✅ compiled_foo = torch compile foo fullgraph=True dynamic=True out_compiled = compiled_foo arg out_compiled sum backward print Compile Success ✅ pytest mark xfail reason= Issue test_fuzzer_issue_ torch manual_seed foo arg arg arg t = arg size= stride= dtype=float device=cuda t = t clone t zero_ size= stride= dtype=float device=cuda t = arg size= stride= dtype=float device=cuda t = arg size= stride= dtype=float device=cuda t = t clone t fill_ t item size= stride= dtype=float device=cuda t = torch pow t t size= stride= dtype=float device=cuda t = t reshape size= stride= dtype=float device=cuda output = t output arg = torch rand dtype=torch float device= cuda requires_grad=True arg = torch rand dtype=torch float device= cuda requires_grad=True arg = torch rand dtype=torch float device= cuda requires_grad=True out_eager = foo arg arg arg out_eager sum backward print Eager Success ✅ compiled_foo = torch compile foo fullgraph=True dynamic=True out_compiled = compiled_foo arg arg arg out_compiled sum backward print Compile Success ✅ __name__ == __main__ run_tests