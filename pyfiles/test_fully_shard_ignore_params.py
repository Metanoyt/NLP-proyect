Owner s oncall distributed sys torch torch distributed dist torch nn nn torch distributed _composable fsdp fully_shard torch distributed _composable fsdp fully_shard FSDPModule FSDP torch distributed device_mesh init_device_mesh torch distributed tensor DTensor torch distributed tensor experimental implicit_replication torch nn parallel DistributedDataParallel DDP torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp FSDPTest get_devtype torch testing _internal common_utils instantiate_parametrized_tests run_tests TEST_WITH_DEV_DBG_ASAN device_type = torch device get_devtype dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit C nn Module __init__ dim int - None super __init__ lin_c = nn Linear dim dim forward x torch Tensor - torch Tensor lin_c x B nn Module __init__ dim int subtrahend torch Tensor - None super __init__ lin_b = nn Linear dim dim module_c = C dim subtrahend = nn Parameter subtrahend forward x torch Tensor - torch Tensor c_result = module_c x lin_b c_result - subtrahend A nn Module __init__ dim int addend torch Tensor subtrahend torch Tensor - None super __init__ module_b = B dim subtrahend addend = nn Parameter addend forward x torch Tensor - torch Tensor result = module_b x + addend result sum Y nn Module __init__ - None super __init__ p = torch randn device=device_type p = nn Parameter p X nn Module __init__ - None super __init__ q = torch randn device=device_type q = nn Parameter q y = Y _append_prefix prefix str name str - str prefix = name = prefix + + name prefix + name _generate_model_and_input - nn Module dim = torch manual_seed addend = torch randn dim dim device=device_type torch manual_seed subend = torch randn dim dim device=device_type model = A dim addend subend device_type torch manual_seed inp = torch randn dim dim device=device_type model inp _find_name_param_mappings module torch nn Module prefix str name_to_param_map = param_to_name_map = name param module named_parameters prefix name_to_param_map name = param param_to_name_map param = name name_to_param_map param_to_name_map _discover_ddp_ignored_params module torch nn Module prefix str ddp_ignore_parameters list str = isinstance module FSDP ddp_ignore_parameters = name name _ module named_parameters prefix name child list module named_children post order traversal path = _append_prefix prefix name ignored_params = _discover_ddp_ignored_params child path ddp_ignore_parameters extend ignored_params ddp_ignore_parameters _modify_ddp_ignored_params ddp_ignored_param_names list str fsdp_ignored_params set torch nn Parameter name_to_param_map dict modified_list = name ddp_ignored_param_names assert name name_to_param_map param = name_to_param_map name param fsdp_ignored_params DDP can ignore only ignored FSDP modified_list append name modified_list _get_full_tensor name param isinstance param DTensor param full_tensor param _discover_fsdp_ignored_params module torch nn Module ignored_path path str - set torch nn Parameter total_ignored_params = set ignored_path == path Ignore all parameters inside module name_parameters = dict module named_parameters path total_ignored_params = set name_parameters values _ module buffers recurse=True yet handle ignoring buffers raise AssertionError Yet handle ignoring buffers name sub_module list module named_children child_path = _append_prefix path name child_ignored_params = _discover_fsdp_ignored_params sub_module ignored_path child_path total_ignored_params = total_ignored_params &#124; child_ignored_params total_ignored_params _post_order_wrap_fsdp module torch nn Module mesh path str ignored_path str ignored_params set torch nn Parameter - torch nn Module ignored_path = path name sub_module list module named_children child_path = _append_prefix path name _post_order_wrap_fsdp sub_module mesh child_path ignored_path ignored_params fully_shard module mesh=mesh ignored_params=ignored_params module _find_all_fsdped_modules module torch nn Module path - set str result = set name child list module named_children child_path = _append_prefix path name child_result = _find_all_fsdped_modules child child_path result = result &#124; child_result isinstance module FSDP result add path result TestFullyShardIgnoreParams FSDPTest Tests fully_shard ignore params compare_params name ref_param test_param ref_full_tensor = _get_full_tensor name ref_param test_full_tensor = _get_full_tensor name test_param assertTrue torch allclose ref_full_tensor test_full_tensor compare_ref_test_params ref_name_to_param_map test_name_to_param_map name ref_name_to_param_map assertTrue name test_name_to_param_map name test_name_to_param_map assertTrue name ref_name_to_param_map name ref_param ref_name_to_param_map items test_param = test_name_to_param_map name compare_params name ref_param test_param skip_if_lt_x_gpu test_ddp_A_fsdp_B_ddp_C default_pg = dist distributed_c d _get_default_group mesh = init_device_mesh device_type type mesh_shape= default_pg size ref_model ref_inp = _generate_model_and_input ref_model = DDP ref_model process_group=default_pg ref_optim = torch optim Adam ref_model parameters lr= e- ref_name_to_param_map _ = _find_name_param_mappings ref_model test_model test_inp = _generate_model_and_input Computes mappings before applying FSDP DDP test_name_to_param_map _ = _find_name_param_mappings test_model ignored_path = module_b module_c fsdp_ignored_params = _discover_fsdp_ignored_params test_model ignored_path=ignored_path path= test_model module_b = _post_order_wrap_fsdp test_model module_b mesh=mesh path= module_b ignored_path=ignored_path ignored_params=fsdp_ignored_params fsdped_modules = _find_all_fsdped_modules test_model assertEqual fsdped_modules module_b module_b lin_b ddp_ignored_param_names = _discover_ddp_ignored_params test_model assertEqual set ddp_ignored_param_names module_b subtrahend module_b lin_b weight module_b lin_b bias module_b module_c lin_c weight module_b module_c lin_c bias modified_ddp_ignored_param_names = _modify_ddp_ignored_params ddp_ignored_param_names fsdp_ignored_params test_name_to_param_map assertEqual set modified_ddp_ignored_param_names module_b subtrahend module_b lin_b weight module_b lin_b bias DDP _set_params_and_buffers_to_ignore_for_model module=test_model params_and_buffers_to_ignore=modified_ddp_ignored_param_names test_model = DDP test_model broadcast_buffers=False test_optim = torch optim Adam test_model parameters lr= e- Recomputes mappings after applying FSDP DDP test_name_to_param_map _ = _find_name_param_mappings test_model Compare ref test parameters before iterations compare_ref_test_params ref_name_to_param_map test_name_to_param_map _ range ref_loss = ref_model ref_inp test_loss = test_model test_inp Compare ref test loss each step assertTrue torch allclose ref_loss test_loss ref_loss backward test_loss backward implicit_replication ref_optim step ref_optim zero_grad test_optim step test_optim zero_grad Compare ref test parameters each step compare_ref_test_params ref_name_to_param_map test_name_to_param_map instantiate_parametrized_tests TestFullyShardIgnoreParams __name__ == __main__ run_tests