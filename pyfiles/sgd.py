mypy allow-untyped-defs r Implementation Stochastic Gradient Descent optimizer typing cast Optional Union torch torch Tensor optimizer _default_to_fused_or_foreach _device_dtype_check_for_fused _differentiable_doc _foreach_doc _fused_doc _maximize_doc _params_doc _to_scalar _use_grad_for_differentiable DeviceDict Optimizer ParamsT __all__ = SGD sgd SGD Optimizer noqa D __init__ params ParamsT lr Union float Tensor = e- momentum float = dampening float = weight_decay Union float Tensor = nesterov bool = False maximize bool = False foreach Optional bool = None differentiable bool = False fused Optional bool = None noqa D isinstance lr Tensor lr numel = raise ValueError Tensor lr must -element lr raise ValueError f Invalid learning rate lr momentum raise ValueError f Invalid momentum value momentum weight_decay raise ValueError f Invalid weight_decay value weight_decay defaults = lr lr momentum momentum dampening dampening weight_decay weight_decay nesterov nesterov maximize maximize foreach foreach differentiable differentiable fused fused nesterov momentum = dampening = raise ValueError Nesterov momentum requires momentum zero dampening super __init__ params defaults fused _step_supports_amp_scaling = True _need_device_dtype_check_for_fused = True differentiable raise RuntimeError ` fused ` does support ` differentiable ` foreach raise RuntimeError ` fused ` ` foreach ` cannot ` True ` together __setstate__ state noqa D super __setstate__ state group param_groups group setdefault nesterov False group setdefault maximize False group setdefault foreach None group setdefault differentiable False group setdefault fused False _init_group group params grads momentum_buffer_list has_sparse_grad = False p group params p grad None group fused getattr _need_device_dtype_check_for_fused True _device_dtype_check_for_fused p _need_device_dtype_check_for_fused = False params append p grads append p grad p grad is_sparse has_sparse_grad = True group momentum = state = state p momentum_buffer_list append state get momentum_buffer has_sparse_grad _use_grad_for_differentiable step closure=None Perform single optimization step Args closure Callable optional A closure reevaluates model returns loss loss = None closure None torch enable_grad loss = closure group param_groups params list Tensor = grads list Tensor = momentum_buffer_list list Optional Tensor = has_sparse_grad = _init_group group params grads momentum_buffer_list sgd params grads momentum_buffer_list weight_decay=group weight_decay momentum=group momentum lr=group lr dampening=group dampening nesterov=group nesterov maximize=group maximize has_sparse_grad=has_sparse_grad foreach=group foreach fused=group fused grad_scale=getattr grad_scale None found_inf=getattr found_inf None group momentum = update momentum_buffers state p momentum_buffer zip params momentum_buffer_list strict=True state = state p state momentum_buffer = momentum_buffer loss SGD __doc__ = r Implements stochastic gradient descent optionally momentum math \begin aligned \rule mm pt \\ \textbf input \gamma \text lr \ \theta_ \text params \ f \theta \text objective \ \lambda \text weight decay \\ \hspace mm \ \mu \text momentum \ \tau \text dampening \ \textit nesterov \ \textit maximize \\ - ex \rule mm pt \\ \textbf \ t= \ \textbf \ \ldots \ \textbf do \\ \hspace mm \textbf \ \textit maximize \\ \hspace mm g_t \leftarrow -\nabla_ \theta f_t \theta_ t- \\ \hspace mm \textbf \\ \hspace mm g_t \leftarrow \nabla_ \theta f_t \theta_ t- \\ \hspace mm \textbf \ \lambda \neq \\ \hspace mm g_t \leftarrow g_t + \lambda \theta_ t- \\ \hspace mm \textbf \ \mu \neq \\ \hspace mm \textbf \ t \\ \hspace mm \textbf b _t \leftarrow \mu \textbf b _ t- + -\tau g_t \\ \hspace mm \textbf \\ \hspace mm \textbf b _t \leftarrow g_t \\ \hspace mm \textbf \ \textit nesterov \\ \hspace mm g_t \leftarrow g_ t + \mu \textbf b _t \\ \hspace mm \textbf \\ - ex \hspace mm g_t \leftarrow \textbf b _t \\ \hspace mm \theta_t \leftarrow \theta_ t- - \gamma g_t \\ - ex \rule mm pt \\ - ex \bf \ \theta_t \\ - ex \rule mm pt \\ - ex \end aligned Nesterov momentum based formula ` On importance initialization momentum deep learning ` __ + rf Args _params_doc lr float Tensor optional learning rate default e- momentum float optional momentum factor default dampening float optional dampening momentum default weight_decay float optional weight decay L penalty default nesterov bool optional enables Nesterov momentum Only applicable when momentum non-zero default False _maximize_doc _foreach_doc _differentiable_doc _fused_doc + r Example xdoctest +SKIP optimizer = torch optim SGD model parameters lr= momentum= optimizer zero_grad loss_fn model input target backward optimizer step __ http www cs toronto edu Ehinton absps momentum pdf note The implementation SGD Momentum Nesterov subtly differs Sutskever et al implementations some other frameworks Considering specific case Momentum update can written math \begin aligned v_ t+ = \mu v_ t + g_ t+ \\ p_ t+ = p_ t - \text lr v_ t+ \end aligned where math ` p ` math ` g ` math ` v ` math ` \mu ` denote parameters gradient velocity momentum respectively This contrast Sutskever et al other frameworks which employ update form math \begin aligned v_ t+ = \mu v_ t + \text lr g_ t+ \\ p_ t+ = p_ t - v_ t+ \end aligned The Nesterov version analogously modified Moreover initial value momentum buffer set gradient value first step This contrast some other frameworks initialize all zeros One notable side effect decision first momentum value will scaled dampening Dampening will applied starting second step sgd params list Tensor d_p_list list Tensor momentum_buffer_list list Optional Tensor kwonly args defaults supported functions compiled torchscript issue setting kwarg now functional API compiled torch distributed optim has_sparse_grad bool = False foreach Optional bool = None fused Optional bool = None grad_scale Optional Tensor = None found_inf Optional Tensor = None weight_decay float momentum float lr float dampening float nesterov bool maximize bool r Functional API performs SGD algorithm computation See ` ~torch optim SGD ` details Respect when user inputs False True foreach fused We only want change default when neither have been user-specified Note we default foreach pass False use_fused This mistake -- we want give fused impl bake-in time before making default even typically faster foreach None fused None why must we explicit about statement torch jit is_scripting here because JIT can t handle Optionals nor fancy conditionals when scripting torch jit is_scripting fused foreach = _default_to_fused_or_foreach params differentiable=False use_fused=False foreach = False fused = False foreach None foreach = False fused None fused = False foreach torch jit is_scripting raise RuntimeError torch jit script supported foreach optimizers fused torch jit is_scripting raise RuntimeError torch jit script supported fused optimizers foreach torch jit is_scripting func = _multi_tensor_sgd fused torch jit is_scripting func = _fused_sgd func = _single_tensor_sgd func params d_p_list momentum_buffer_list weight_decay=weight_decay momentum=momentum lr=lr dampening=dampening nesterov=nesterov has_sparse_grad=has_sparse_grad maximize=maximize grad_scale=grad_scale found_inf=found_inf _single_tensor_sgd params list Tensor grads list Tensor momentum_buffer_list list Optional Tensor grad_scale Optional Tensor found_inf Optional Tensor weight_decay float momentum float lr float dampening float nesterov bool maximize bool has_sparse_grad bool grad_scale None found_inf None raise AssertionError Expected grad_scale found_inf None torch jit is_scripting lr = _to_scalar lr i param enumerate params grad = grads i maximize -grads i weight_decay = Nested necessary bypass jitscript rules isinstance weight_decay Tensor weight_decay requires_grad usually differentiable path which why param clone needed grad = grad addcmul_ param clone weight_decay pyrefly ignore bad-argument-type grad = grad add param alpha=weight_decay grad = grad add param alpha=weight_decay momentum = buf = momentum_buffer_list i buf None buf = grad detach clone momentum_buffer_list i = buf buf mul_ momentum add_ grad alpha= - dampening nesterov grad = grad add buf alpha=momentum grad = buf Nested necessary bypass jitscript rules isinstance lr Tensor lr requires_grad param addcmul_ grad lr value=- pyrefly ignore bad-argument-type param add_ grad alpha=-lr param add_ grad alpha=-lr _multi_tensor_sgd params list Tensor grads list Tensor momentum_buffer_list list Optional Tensor grad_scale Optional Tensor found_inf Optional Tensor weight_decay float momentum float lr float dampening float nesterov bool maximize bool has_sparse_grad bool grad_scale None found_inf None raise AssertionError Expected grad_scale found_inf None len params == lr = _to_scalar lr grouped_tensors = Optimizer _group_tensors_by_device_and_dtype params grads momentum_buffer_list type ignore list-item with_indices=True device_params_ device_grads_ device_momentum_buffer_list indices grouped_tensors values device_params list Tensor = cast list Tensor device_params_ device_grads list Tensor = cast list Tensor device_grads_ device_has_sparse_grad = has_sparse_grad any grad is_sparse grad device_grads maximize device_grads = torch _foreach_neg device_grads type ignore assignment weight_decay = Reuse intermediate memory device_grads already allocated maximize maximize torch _foreach_add_ device_grads device_params alpha=weight_decay device_grads = torch _foreach_add type ignore assignment device_grads device_params alpha=weight_decay momentum = bufs list Tensor = all_states_with_momentum_buffer = True i range len device_momentum_buffer_list device_momentum_buffer_list i None all_states_with_momentum_buffer = False break bufs append cast Tensor device_momentum_buffer_list i all_states_with_momentum_buffer torch _foreach_mul_ bufs momentum torch _foreach_add_ bufs device_grads alpha= - dampening bufs = i range len device_momentum_buffer_list device_momentum_buffer_list i None buf = device_momentum_buffer_list i = momentum_buffer_list indices i = device_grads i detach clone buf = cast Tensor device_momentum_buffer_list i buf mul_ momentum add_ device_grads i alpha= - dampening bufs append buf nesterov torch _foreach_add_ device_grads bufs alpha=momentum device_grads = bufs device_has_sparse_grad handle internal item call lr tensor isinstance lr torch Tensor torch compiler is_compiling grads_x_lr = torch _foreach_mul device_grads -lr torch _foreach_add_ device_params grads_x_lr torch _foreach_add_ device_params device_grads alpha=-lr foreach APIs don t support sparse i range len device_params device_params i add_ device_grads i alpha=-lr _fused_sgd params list Tensor grads list Tensor momentum_buffer_list list Optional Tensor grad_scale Optional Tensor found_inf Optional Tensor weight_decay float momentum float lr float dampening float nesterov bool maximize bool has_sparse_grad bool - None params has_sparse_grad raise RuntimeError ` _fused_sgd ` does support sparse gradients grad_scale_dict DeviceDict = grad_scale device grad_scale grad_scale None found_inf_dict DeviceDict = found_inf device found_inf found_inf None no_momentum_buffer = momentum == is_first_step = all t None t momentum_buffer_list no_momentum_buffer is_first_step i g enumerate grads momentum_buffer_list i = torch empty_like g grouped_tensors = Optimizer _group_tensors_by_device_and_dtype params grads momentum_buffer_list type ignore list-item with_indices=False device _ device_params_ device_grads_ device_momentum_buffer_list _ grouped_tensors items device_params list Tensor = cast list Tensor device_params_ device_grads list Tensor = cast list Tensor device_grads_ device_grad_scale device_found_inf = None None grad_scale None device_grad_scale = grad_scale_dict setdefault device grad_scale device found_inf_dict None found_inf None device_found_inf = found_inf_dict setdefault device found_inf device torch _fused_sgd_ device_params device_grads no_momentum_buffer cast list Tensor device_momentum_buffer_list weight_decay=weight_decay momentum=momentum lr=lr dampening=dampening nesterov=nesterov maximize=maximize is_first_step=is_first_step grad_scale=device_grad_scale found_inf=device_found_inf