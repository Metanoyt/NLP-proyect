Generates C++ autograd functions derivatives ATen operations This writes two files Functions h cpp subclasses autograd Node python_functions h cpp Python bindings above classes __future__ annotations typing TYPE_CHECKING torchgen api autograd Derivative DifferentiabilityInfo SavedAttribute uses_retain_variables uses_single_grad torchgen api types ArrayRefCType BaseCppType BaseCType Binding boolT doubleT intArrayRefT iTensorListRefT ListCType longT MutRefCType OptionalCType optionalIntArrayRefT optionalSymIntArrayRefT scalarT stringT symIntArrayRefT SymIntT TENSOR_LIST_LIKE_CTYPES tensorListT tensorT VectorCType torchgen code_template CodeTemplate torchgen model Argument FunctionSchema torchgen utils FileManager gen_inplace_or_view_type VIEW_FUNCTIONS TYPE_CHECKING collections abc Sequence FUNCTION_DECLARATION = CodeTemplate \ #ifdef _WIN struct $ op public $ superclass TORCH_API $ op = default #else struct TORCH_API $ op public $ superclass #endif using $ superclass $ superclass variable_list apply variable_list grads override std string name const override $ op void release_variables override $ thread_lock $ release_variables $ will_release_variables void compiled_args CompiledNodeArgs args const override variable_list apply_with_saved const variable_list inputs SwapSavedVariables saved override $ saved_variables $ saved_list_sizes WILL_RELEASE_VARIABLES = CodeTemplate \ bool retain_variables = true void will_release_variables override retain_variables = false We generate e g MulBackward apply have call into MulBackward _apply_functional The apply_functional pure function does rely global state MulBackward apply responsible querying autograd engine which outputs should computed needs_input_grad applying locks unpacking saved variables pass MulBackward _apply_functional needs_input_grad mapping input index input needs gradients computed For operators take List Tensor List Tensor one element needs_input_grad specifies any List Tensor needs input grad In theory could optimized FUNCTION_DEFINITION = CodeTemplate \ static variable_list $ op _apply_functional variable_list grads std array bool $ num_inputs needs_input_grad$ apply_functional_args_signature IndexRangeGenerator gen $ compute_index_ranges variable_list grad_inputs gen size $ body grad_inputs inline variable_list $ op _apply_functional_ivalue const variable_list grads const ivalue_list args #ifdef C _MOBILE TORCH_INTERNAL_ASSERT false compiled autograd doesn t work mobile #else auto packed_args = PackedArgs args auto needs_input_grad = packed_args unpack std array bool $ num_inputs $ unpack_ivalues $ op _apply_functional variable_list grads needs_input_grad$ apply_functional_args #endif variable_list $ op apply variable_list grads $ thread_lock $ asserts $ unpacks $ compute_needs_input_grad $ op _apply_functional std move grads needs_input_grad$ apply_functional_args void $ op compiled_args CompiledNodeArgs args const $ compiled_args variable_list $ op apply_with_saved const variable_list grads SwapSavedVariables saved #ifdef C _MOBILE TORCH_INTERNAL_ASSERT false compiled autograd doesn t work mobile #else $ apply_with_saved_before static bool called = false called called = true $ compute_schema const auto pyinterface = torch dynamo autograd getPyCompilerInterface pyinterface- bind_function saved get_py_compiler name $ op _apply_functional_ivalue schema variable_list output_result PackedArgs packed_args $ asserts $ unpacks $ compute_needs_input_grad packed_args pack needs_input_grad $ get_packed_args output_result = compiled_autograd_apply_functional packed_args next_edges saved grads name $ apply_with_saved_after output_result #endif GRAD_INPUT_MASK = CodeTemplate \ auto grad_input_mask = std array bool $ n $ masks COMPUTE_NEEDS_INPUT_GRAD = CodeTemplate \ IndexRangeGenerator gen $ compute_index_ranges auto needs_input_grad = std array bool $ n $ masks \ DERIVATIVE_SINGLE = CodeTemplate \ needs_input_grad $ name $ idx auto grad_result = $ derivative copy_range grad_inputs $ name _ix grad_result note crcrpar ` ` argument other optional positional argument foreach functions basically list n ` Tensor ` s thus iterating over ` grads ` order utilize apply existing derivative definitions each ` Tensor ` s ` ` others DERIVATIVE_SINGLE_FOREACH = CodeTemplate \ needs_input_grad $ name $ idx $ name std vector Tensor grad_result grad_result reserve grads size const auto i c irange grads size grads i defined grad_result emplace_back $ derivative grad_result emplace_back Tensor copy_range grad_inputs $ name _ix grad_result DERIVATIVE_MULTI_COPY_RANGE = CodeTemplate \ needs_input_grad $ name $ idx copy_range grad_inputs $ name _ix std get $ i grad_result DERIVATIVE_MULTI = CodeTemplate \ $ needs_input_grad $ grad_input_mask auto grad_result = $ derivative $ copy_ranges Generates python bindings This generates definitions The PyTypeObject each backward grad_fn subclassing Node The entry PyTypeObject s tp_getset slot array PyGetSetDef structs We generate one PyGetSetDef struct each grad_fn s saved inputs outputs Each PyGetSetDef has function ptr getter also defined here Getters each grad_fn s saved inputs outputs PY_FUNCTION_DEFINITION = CodeTemplate \ static PyTypeObject $ op Class addClass $ op module $ op Class $ op $ op _properties PY_FUNCTION_PROPS_AND_GETTERS = CodeTemplate \ $ all_getter_definitions static struct PyGetSetDef $ op _properties = THP_FUNCTION_DEFAULT_PROPERTIES $ all_getsetdef_structs nullptr sentinel PY_GETSETDEF_STRUCT = CodeTemplate \ char _saved_$ name getter THP$ op _$ name _getter nullptr nullptr nullptr PY_RAW_GETSETDEF_STRUCT = CodeTemplate \ char _raw_saved_$ name getter THP$ op _$ name _raw_getter nullptr nullptr nullptr Getter templates GETTER_DEFINITION = CodeTemplate \ static PyObject THP$ op _$ name _getter THPCppFunction void _unused HANDLE_TH_ERRORS auto prop = static_cast $ op self- cdata get - $ name $ body END_HANDLE_TH_ERRORS GETTER_DEFINITION_SAVEDVAR = CodeTemplate \ static PyObject THP$ op _$ name _getter THPCppFunction void _unused HANDLE_TH_ERRORS const auto prop = static_cast $ op self- cdata get - $ name _ $ body END_HANDLE_TH_ERRORS GETTER_DEFINITION_RAW_SAVEDVAR = CodeTemplate \ static PyObject THP$ op _$ name _raw_getter THPCppFunction void _unused HANDLE_TH_ERRORS const auto prop = static_cast $ op self- cdata get - $ name _ $ body END_HANDLE_TH_ERRORS GETTER_DEFINITION_VEC_SAVEDVAR = CodeTemplate \ static PyObject THP$ op _$ name _getter THPCppFunction void _unused HANDLE_TH_ERRORS const auto node = static_cast $ op self- cdata get const auto prop = node- $ name _ node- $ name _released_ PyErr_SetString PyExc_RuntimeError ERR_BACKWARD_TWICE nullptr $ body END_HANDLE_TH_ERRORS GETTER_DEFINITION_RAW_VEC_SAVEDVAR = CodeTemplate \ static PyObject THP$ op _$ name _raw_getter THPCppFunction void _unused HANDLE_TH_ERRORS const auto node = static_cast $ op self- cdata get const auto prop = node- $ name _ node- $ name _released_ PyErr_SetString PyExc_RuntimeError ERR_BACKWARD_TWICE nullptr $ body END_HANDLE_TH_ERRORS GETTER_DEFINITION_OPT = CodeTemplate \ static PyObject THP$ op _$ name _getter THPCppFunction void _unused HANDLE_TH_ERRORS auto opt_prop = static_cast $ op self- cdata get - $ name opt_prop has_value Py_RETURN_NONE auto prop = opt_prop value $ body END_HANDLE_TH_ERRORS GETTER_DEFINITION_OPT_ARRAYREF = CodeTemplate \ static PyObject THP$ op _$ name _getter THPCppFunction void _unused HANDLE_TH_ERRORS auto opt_prop = static_cast $ op self- cdata get - $ name opt_prop list has_value Py_RETURN_NONE auto prop = opt_prop list value $ body END_HANDLE_TH_ERRORS Getter body GETTER_BODY_SAVEDVAR = \ THPVariable_Wrap prop unpack self- cdata GETTER_BODY_RAW_SAVEDVAR = \ pybind object obj = pybind cast prop pybind return_value_policy reference obj release ptr GETTER_BODY_VEC_SAVEDVAR = \ PyObject tup = PyTuple_New Py_ssize_t prop size auto i c irange prop size PyTuple_SetItem tup Py_ssize_t i THPVariable_Wrap prop i unpack self- cdata tup GETTER_BODY_RAW_VEC_SAVEDVAR = \ PyObject tup = PyTuple_New Py_ssize_t prop size auto i c irange prop size pybind object obj = pybind cast prop i pybind return_value_policy reference PyTuple_SetItem tup Py_ssize_t i obj release ptr tup GETTER_BODY_ARRAYREF_LONG = \ PyObject tup = PyTuple_New Py_ssize_t prop size auto i c irange prop size PyTuple_SetItem tup Py_ssize_t i PyLong_FromUnsignedLong uint _t prop i tup GETTER_BODY_ARRAYREF_SYMINT = \ PyObject tup = PyTuple_New Py_ssize_t prop size auto i c irange prop size auto si = prop i auto m = si maybe_as_int PyTuple_SetItem tup Py_ssize_t i PyLong_FromUnsignedLong m auto py_symint = py cast si release ptr PyTuple_SetItem tup Py_ssize_t i py_symint tup GETTER_BODY_ARRAYREF_DOUBLE = \ PyObject tup = PyTuple_New Py_ssize_t prop size auto i c irange prop size PyTuple_SetItem tup Py_ssize_t i PyFloat_FromDouble double prop i tup GETTER_BODY_INT _T = \ PyLong_FromUnsignedLong int _t prop GETTER_BODY_SYMINT = \ auto m = prop maybe_as_int PyLong_FromUnsignedLong m py cast prop release ptr GETTER_BODY_DOUBLE = \ PyFloat_FromDouble double prop GETTER_BODY_BOOL = \ prop Py_RETURN_TRUE Py_RETURN_FALSE GETTER_BODY_STRING = \ PyUnicode_FromStringAndSize prop data prop size GETTER_BODY_SCALAR = \ prop isComplex auto cprop = prop c complex double PyComplex_FromDoubles cprop real cprop imag prop isFloatingPoint PyFloat_FromDouble prop double prop isIntegral includeBool= false PyLong_FromLong prop int _t prop isBoolean prop bool Py_RETURN_TRUE Py_RETURN_FALSE PyErr_SetString PyExc_RuntimeError Unknown scalar type nullptr GETTER_BODY_VEC_SCALAR = \ PyObject tup = PyTuple_New Py_ssize_t prop size auto i c irange prop size prop i isComplex auto cprop = prop i c complex double PyTuple_SetItem tup Py_ssize_t i PyComplex_FromDoubles cprop real cprop imag prop i isFloatingPoint auto double_prop = prop i double PyTuple_SetItem tup Py_ssize_t i PyFloat_FromDouble double_prop prop i isIntegral includeBool= false auto long_prop = prop i int _t PyTuple_SetItem tup Py_ssize_t i PyLong_FromLong long_prop prop i isBoolean prop i bool PyTuple_SetItem tup Py_ssize_t i Py_True PyTuple_SetItem tup Py_ssize_t i Py_False PyErr_SetString PyExc_RuntimeError Unknown scalar type nullptr tup MISC_GETTER_DEFS = OptionalCType BaseCType longT GETTER_DEFINITION_OPT GETTER_BODY_INT _T OptionalCType BaseCType SymIntT GETTER_DEFINITION_OPT GETTER_BODY_SYMINT BaseCType doubleT GETTER_DEFINITION GETTER_BODY_DOUBLE OptionalCType BaseCType doubleT GETTER_DEFINITION_OPT GETTER_BODY_DOUBLE BaseCType boolT GETTER_DEFINITION GETTER_BODY_BOOL BaseCType scalarT GETTER_DEFINITION GETTER_BODY_SCALAR OptionalCType BaseCType scalarT GETTER_DEFINITION_OPT GETTER_BODY_SCALAR These functions have backwards which cannot traced so must have their backward functions traced opaquely VIEW_FUNCTIONS traceable because they use as_strided which has untraceable backwards see https github com pytorch pytorch issues TODO This probably exhaustive s start UNTRACEABLE_FUNCTIONS = VIEW_FUNCTIONS get_infos_with_derivatives_list differentiability_infos dict FunctionSchema dict str DifferentiabilityInfo - list DifferentiabilityInfo diff_info_list = info diffinfo_dict differentiability_infos values info diffinfo_dict values list filter lambda info info args_with_derivatives diff_info_list gen_autograd_functions_lib out str differentiability_infos dict FunctionSchema dict str DifferentiabilityInfo template_path str - None Functions h Functions cpp body These contain auto-generated subclasses torch autograd Node each every differentiable torch function get D list diffinfos we do need them per FunctionSchema DispatchKey here infos diff dispatchkeys same name will still same shard infos = get_infos_with_derivatives_list differentiability_infos declarations = process_function f FUNCTION_DECLARATION f infos definitions = process_function f FUNCTION_DEFINITION f infos file_basename = Functions fm = FileManager install_dir=out template_dir=template_path dry_run=False suffix h cpp fname = file_basename + suffix fm write_with_template fname fname lambda generated_comment + f generated fm template_dir_for_comments fname autograd_function_declarations declarations autograd_function_definitions definitions gen_autograd_functions_python out str differentiability_infos dict FunctionSchema dict str DifferentiabilityInfo template_path str - None fm = FileManager install_dir=out template_dir=template_path dry_run=False num_shards = fm write python_functions h lambda generated_comment + f generated fm template_dir_for_comments python_functions h shard_forward_declare f void initialize_autogenerated_functions_ i PyObject module i range num_shards shard_call f initialize_autogenerated_functions_ i module i range num_shards get D list diffinfos we do need them per FunctionSchema DispatchKey here infos diff dispatchkeys same name will still same shard infos = get_infos_with_derivatives_list differentiability_infos fm write_sharded python_functions cpp infos key_fn=lambda info info name base_env= generated_comment + f generated fm template_dir_for_comments python_functions cpp env_callable=lambda info py_function_initializers process_function info PY_FUNCTION_DEFINITION py_function_props_and_getters process_function info PY_FUNCTION_PROPS_AND_GETTERS num_shards=num_shards sharded_keys= py_function_initializers py_function_props_and_getters process_function info DifferentiabilityInfo template CodeTemplate - str saved_variables list str = release_variables list str = saved_list_sizes list str = unpack list str = asserts list str = compute_index_ranges list str = getter_definitions list str = py_getsetdef_structs list str = compiled_args list str = apply_with_saved_before list str = apply_with_saved_after list str = apply_functional_args list str = apply_functional_args_ref_types list str = Maps name input original forward operator examples other order which they appear operator For example operator foo Tensor int _t k Tensor other mapping other We use mapping populate needs_input_grad some order then grab values input_name_to_idx dict str int = idx arg enumerate info args_with_derivatives arg type TENSOR_LIST_LIKE_CTYPES size = f arg name _size_ saved_list_sizes append f size_t arg name _size_ apply_functional_args append f arg name _size_ apply_functional_args_ref_types append size_t size = compute_index_ranges append f auto arg name _ix = gen range size input_name_to_idx arg name = idx save_var var SavedAttribute is_output bool - None name = var nctype name type = var nctype type should_append_getsetdef = True should_append_raw_getsetdef = False visit_name = name uses_cpp_saved_variable_cls = False unpacked_ref_type = None type == BaseCType tensorT type == OptionalCType BaseCType tensorT type == MutRefCType OptionalCType BaseCType tensorT type == BaseCType scalarT is_output uses_cpp_saved_variable_cls = True saved_variables append f SavedVariable name _ release_variables append f name _ reset_data ptr = shared_from_this is_output unpack append f auto name = name _ unpack ptr getter_definitions append GETTER_DEFINITION_SAVEDVAR substitute op=info op name=name body=GETTER_BODY_SAVEDVAR getter_definitions append GETTER_DEFINITION_RAW_SAVEDVAR substitute op=info op name=name body=GETTER_BODY_RAW_SAVEDVAR should_append_raw_getsetdef = True visit_name = f name _ unpacked_ref_type = Tensor type == BaseCType tensorListT type == BaseCType iTensorListRefT type == VectorCType BaseCType tensorT note crcrpar nuanced type out-of-place foreach functions When out-of-place foreach function whose signature ` Tensor ` spells out its backward definitions ` derivatives yaml ` some them depend ` result ` ` result ` s type interpreted treated ` std vector Tensor ` An out-of-place foreach whose backwards rely their output doesn t suffer difference definitions codegen ed This special case needed ` _foreach_pow List ` ` _foreach_pow ScalarAndTensor ` https github com pytorch pytorch pull type == VectorCType BaseCType tensorT assert info func func name name base startswith _foreach is_output uses_cpp_saved_variable_cls = True saved_variables append f std vector SavedVariable name _ saved_variables append f bool name _released_ = false Just clear sufficient we don t need loop clear each variable Because SavedVariable owns tensor grad_fn removing SavedVariable makes them go away well release_variables append f name _ clear release_variables append f name _released_ = true ptr = shared_from_this is_output nullptr unpack append f auto name = unpack_list name _ ptr asserts append f TORCH_CHECK name _released_ ERR_BACKWARD_TWICE getter_definitions append GETTER_DEFINITION_VEC_SAVEDVAR substitute op=info op name=name body=GETTER_BODY_VEC_SAVEDVAR getter_definitions append GETTER_DEFINITION_RAW_VEC_SAVEDVAR substitute op=info op name=name body=GETTER_BODY_RAW_VEC_SAVEDVAR should_append_raw_getsetdef = True visit_name = f name _ unpacked_ref_type = std vector Tensor type == ListCType OptionalCType BaseCType tensorT uses_cpp_saved_variable_cls = True saved_variables append f std vector SavedVariable name _ saved_variables append f bool name _released_ = false Just clear sufficient we don t need loop clear each variable Because SavedVariable owns tensor grad_fn removing SavedVariable makes them go away well release_variables append f name _ clear release_variables append f name _released_ = true unpack append f auto name = unpack_opt_list name _ asserts append f TORCH_CHECK name _released_ ERR_BACKWARD_TWICE getter_definitions append GETTER_DEFINITION_VEC_SAVEDVAR substitute op=info op name=name body=GETTER_BODY_VEC_SAVEDVAR getter_definitions append GETTER_DEFINITION_RAW_VEC_SAVEDVAR substitute op=info op name=name body=GETTER_BODY_RAW_VEC_SAVEDVAR should_append_raw_getsetdef = True visit_name = f name _ unpacked_ref_type = torch List std optional Tensor type == BaseCType intArrayRefT saved_variables append f std vector int _t name getter_definitions append GETTER_DEFINITION substitute op=info op name=name body=GETTER_BODY_ARRAYREF_LONG type == BaseCType symIntArrayRefT saved_variables append f std vector c SymInt name getter_definitions append GETTER_DEFINITION substitute op=info op name=name body=GETTER_BODY_ARRAYREF_SYMINT type == BaseCType optionalIntArrayRefT saved_variables append f c OptionalArray int _t name getter_definitions append GETTER_DEFINITION_OPT_ARRAYREF substitute op=info op name=name body=GETTER_BODY_ARRAYREF_LONG type == BaseCType optionalSymIntArrayRefT saved_variables append f c OptionalArray c SymInt name getter_definitions append GETTER_DEFINITION_OPT_ARRAYREF substitute op=info op name=name body=GETTER_BODY_ARRAYREF_SYMINT type == OptionalCType BaseCType intArrayRefT saved_variables append f c OptionalArray int _t name getter_definitions append GETTER_DEFINITION_OPT_ARRAYREF substitute op=info op name=name body=GETTER_BODY_ARRAYREF_LONG type == OptionalCType BaseCType symIntArrayRefT saved_variables append f c OptionalArray c SymInt name getter_definitions append GETTER_DEFINITION_OPT_ARRAYREF substitute op=info op name=name body=GETTER_BODY_ARRAYREF_SYMINT type == OptionalCType ArrayRefCType BaseCType doubleT saved_variables append f c OptionalArray double name getter_definitions append GETTER_DEFINITION_OPT_ARRAYREF substitute op=info op name=name body=GETTER_BODY_ARRAYREF_DOUBLE type == BaseCType longT saved_variables append f type cpp_type name = getter_definitions append GETTER_DEFINITION substitute op=info op name=name body=GETTER_BODY_INT _T type == BaseCType SymIntT saved_variables append f c SymInt name getter_definitions append GETTER_DEFINITION substitute op=info op name=name body=GETTER_BODY_SYMINT type == BaseCType stringT saved_variables append f std string name getter_definitions append GETTER_DEFINITION substitute op=info op name=name body=GETTER_BODY_STRING type == OptionalCType BaseCType stringT saved_variables append f std optional std string name getter_definitions append GETTER_DEFINITION_OPT substitute op=info op name=name body=GETTER_BODY_STRING type == ArrayRefCType elem=BaseCType type=BaseCppType ns= name= Scalar saved_variables append f std vector Scalar name unpacked_ref_type = std vector Scalar saved_variables append f bool name _released_ = false Just clear sufficient we don t need loop clear each variable Because SavedVariable owns tensor grad_fn removing SavedVariable makes them go away well release_variables append f name clear release_variables append f name _released_ = true unpack append f auto name = unpack_list name _ asserts append f TORCH_CHECK name _released_ ERR_BACKWARD_TWICE getter_definitions append CodeTemplate \ static PyObject THP$ op _$ name _getter THPCppFunction void _unused HANDLE_TH_ERRORS const auto node = static_cast $ op self- cdata get const auto prop = node- $ name node- $ name _released_ PyErr_SetString PyExc_RuntimeError ERR_BACKWARD_TWICE nullptr $ body END_HANDLE_TH_ERRORS substitute op=info op name=name body=GETTER_BODY_VEC_SCALAR Check indicators you re putting non-owning reference into saved variable field If spuriously firing edit field Otherwise you probably need add case above assert ref type cpp_type lower view type cpp_type lower type cpp_type type cpp_type f type cpp_type looks like contains non-owning reference saved_variables append f type cpp_type name type MISC_GETTER_DEFS pyrefly ignore index-error getter_def body = MISC_GETTER_DEFS type getter_definitions append getter_def substitute op=info op name=name body=body Types we don t expose python bindings yet TypeAndSize ScalarType TensorOptions TensorGeometry std vector std vector int _t std vector ScalarType should_append_getsetdef = False should_append_getsetdef py_getsetdef_structs append PY_GETSETDEF_STRUCT substitute op=info op name=name should_append_raw_getsetdef py_getsetdef_structs append PY_RAW_GETSETDEF_STRUCT substitute op=info op name=name uses_cpp_saved_variable_cls compiled_args append f args collect visit_name true is_output false compiled_args append f args collect visit_name apply_with_saved_before append f saved before visit_name apply_with_saved_after append f saved after visit_name unpacked_ref_type None unpacked_ref_type = f saved_variables - split apply_functional_args append str name apply_functional_args_ref_types append unpacked_ref_type var sorted info all_saved_inputs key=lambda sa str sa nctype name save_var var is_output=False var sorted info all_saved_outputs key=lambda sa str sa nctype name save_var var is_output=True lock mutex when we release variables Node apply protect thread safety see Note Thread Safety Autograd Node len release_variables thread_lock = std lock_guard std mutex lock mutex_ thread_lock = uses_retain_variables info apply_functional_args append retain_variables apply_functional_args_ref_types append bool will_release_variables = WILL_RELEASE_VARIABLES substitute will_release_variables = body list str = uses_single_grad info body append const auto grad = grads Generate aliases gradients named returned values body extend f const auto name = grads info available_named_gradients index name name sorted info used_named_gradients emit_derivative derivative Derivative args_with_derivatives Sequence Binding - tuple bool str formula = derivative formula var_names = derivative var_names len var_names == checks_any_grad_defined = False not_implemented formula matching_args = arg arg args_with_derivatives arg name == var_names len matching_args == We can add undefined grad support input variable Tensor arg = matching_args isinstance arg argument Argument str arg argument type Tensor Tensor formula = any_grad_defined + formula + Tensor checks_any_grad_defined = True info name startswith _foreach_ derivative_template = DERIVATIVE_SINGLE_FOREACH derivative_template = DERIVATIVE_SINGLE checks_any_grad_defined derivative_template substitute name=var_names derivative=formula idx=input_name_to_idx var_names grad_input_mask formula masks = f needs_input_grad input_name_to_idx name name var_names grad_input_mask = GRAD_INPUT_MASK substitute n=len var_names masks=masks grad_input_mask = needs_input_grad = f needs_input_grad input_name_to_idx name name var_names needs_input_grad = &#124; &#124; join needs_input_grad copy_ranges list str = i n enumerate var_names copy_ranges append DERIVATIVE_MULTI_COPY_RANGE substitute name=n i=i idx=input_name_to_idx n False DERIVATIVE_MULTI substitute needs_input_grad=needs_input_grad copy_ranges=copy_ranges derivative=formula grad_input_mask=grad_input_mask masks = need_any_grad_defined_var = False derivative info derivatives checks_any_grad_defined derivative_text = emit_derivative derivative info args_with_derivatives body append derivative_text need_any_grad_defined_var &#124; = checks_any_grad_defined name input_name_to_idx masks append f task_should_compute_output name _ix Since single-output derivative formulas need check grads defined only perform check once before all formulas need_any_grad_defined_var body insert -len info derivatives bool any_grad_defined = any_variable_defined grads info name UNTRACEABLE_FUNCTIONS superclass = Node superclass = TraceableFunction all_getsetdef_structs = \n join py_getsetdef_structs + len py_getsetdef_structs = all_getter_definitions = \n join getter_definitions compute_needs_input_grad = COMPUTE_NEEDS_INPUT_GRAD substitute n=len masks compute_index_ranges=compute_index_ranges masks=masks apply_functional_args_signature = f T x T x zip apply_functional_args_ref_types apply_functional_args get_packed_args = \n join f packed_args pack name name apply_functional_args unpack_ivalues = typ name zip apply_functional_args_ref_types apply_functional_args typ = typ removesuffix pyrefly ignore bad-argument-type unpack_ivalues append f auto name = packed_args unpack typ schema_args = f std array bool len input_name_to_idx typ apply_functional_args_ref_types typ = typ removesuffix typ = typ removeprefix const schema_args append typ strip compute_schema = std vector TypePtr schema = schema_arg schema_args compute_schema append f torch dynamo autograd IValuePacker schema_arg packed_type compute_schema append template substitute unpacks= \n join unpack op=info op compute_schema= \n join compute_schema apply_functional_args=apply_functional_args apply_functional_args_signature=apply_functional_args_signature compute_needs_input_grad=compute_needs_input_grad num_inputs=len input_name_to_idx unpack_ivalues= \n join unpack_ivalues compute_index_ranges=compute_index_ranges saved_variables=saved_variables release_variables=release_variables saved_list_sizes=saved_list_sizes asserts=asserts thread_lock=thread_lock will_release_variables=will_release_variables body=body superclass=superclass all_getter_definitions=all_getter_definitions all_getsetdef_structs=all_getsetdef_structs compiled_args=compiled_args apply_with_saved_before=apply_with_saved_before apply_with_saved_after=apply_with_saved_after get_packed_args=get_packed_args