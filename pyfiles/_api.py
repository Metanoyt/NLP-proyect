mypy allow-untyped-decorators mypy allow-untyped-defs Copyright c Meta Platforms Inc affiliates inspect warnings collections abc Callable Sequence typing Any cast Optional typing_extensions deprecated torch torch distributed tensor _dispatch op_dispatch torch distributed tensor _random random torch nn nn torch _export wrappers mark_subclass_constructor_exportable_experimental torch distributed device_mesh _mesh_resources DeviceMesh torch distributed tensor _collective_utils check_tensor_meta mesh_broadcast torch distributed tensor _dtensor_spec DTensorSpec TensorMeta torch distributed tensor _redistribute Redistribute redistribute_local_tensor torch distributed tensor _utils compute_global_tensor_info compute_local_shape_and_global_offset normalize_to_torch_size torch distributed tensor placement_types _StridedShard Partial Placement Replicate Shard __all__ = DTensor distribute_tensor distribute_module ones empty full rand randn zeros aten = torch ops aten NOTE Autograd interaction between torch Tensor The autograd functions defined below being used public facing APIs i e from_local to_local ensure DTensor work together torch Tensor within autograd engine This allows DTensor only exist part module hierarchy As example we have module consists submodules A B C execution flow would like input torch Tensor - Module A - Module B - Module C - output torch Tensor Suppose I only want make Module B sharded module DTensor params following forward backward should work input torch Tensor - Module A - DTensor input from_local - Sharded Module B - DTensor output - torch Tensor output to_local - Module C So from_local to_local must Autograd functions _ToTorchTensor torch autograd Function staticmethod forward type ignore override ctx input DTensor grad_placements Optional Sequence Placement ctx dtensor_spec = input _spec ctx grad_placements = grad_placements local_tensor = input _local_tensor We need fresh Tensor object there autograd metadata will inplaced into So we don t want pollute Tensor object stored _local_tensor DTensor local_tensor view_as local_tensor staticmethod backward ctx grad_output torch Tensor type ignore override dtensor_spec = ctx dtensor_spec mesh = dtensor_spec mesh grad_placements = ctx grad_placements dtensor_meta = dtensor_spec tensor_meta _ tensor_stride = compute_global_tensor_info grad_output mesh dtensor_spec placements tensor_stride = tuple tensor_stride grad_placements = grad_placements dtensor_spec placements grad_spec = DTensorSpec mesh grad_placements tensor_meta=TensorMeta shape=dtensor_meta shape stride=tensor_stride dtype=dtensor_meta dtype pyrefly ignore bad-argument-type DTensor pyrefly ignore bad-argument-count grad_output grad_spec pyrefly ignore unexpected-keyword requires_grad=grad_output requires_grad None _FromTorchTensor torch autograd Function staticmethod forward type ignore override ctx pyre-ignore Parameter must annotated input torch Tensor device_mesh DeviceMesh placements tuple Placement run_check bool shape Optional torch Size = None stride Optional tuple int = None - DTensor ctx previous_placement = placements ctx previous_device_mesh = device_mesh shape stride tensor_shape tensor_stride = shape stride shape stride s default run_check we assume user certain each rank has same tensor shape we just use calculate global shape global_shape global_stride = compute_global_tensor_info input device_mesh placements tensor_shape tensor_stride = torch Size global_shape tuple global_stride raise RuntimeError f Found shape shape stride stride Please pass both shape stride same time device_mesh get_coordinate None global rank participating device mesh we simply set local tensor empty tensor input = input new_empty requires_grad=input requires_grad run_check TODO support uneven sharding when global shape stride passed building global TensorMeta during check_tensor_meta check_shape_stride = shape stride check_tensor_meta input check_shape_stride=check_shape_stride TODO See we need make run_check logic have corresponding backward idx placement enumerate placements placement is_replicate broadcast rank tensor all ranks only broadcast run_check True input = input contiguous mesh_broadcast input device_mesh mesh_dim=idx dist_spec = DTensorSpec device_mesh placements tensor_meta=TensorMeta tensor_shape tensor_stride input dtype We want fresh Tensor object shares memory input tensor pyrefly ignore bad-argument-type dist_tensor = DTensor pyrefly ignore bad-argument-count input view_as input dist_spec requires_grad dist tensor depends input requires_grad pyrefly ignore unexpected-keyword requires_grad=input requires_grad dist_tensor staticmethod backward ctx grad_output DTensor type ignore override previous_placement = ctx previous_placement previous_device_mesh = ctx previous_device_mesh reshard placement when creating DistributedTensor so gradient layout matches we could local gradients directly grad_output placements = previous_placement current_spec = grad_output _spec target_spec = DTensorSpec previous_device_mesh previous_placement tensor_meta=grad_output _spec tensor_meta local_tensor = grad_output _local_tensor output = redistribute_local_tensor local_tensor current_spec target_spec is_backward=True TODO redistributed local tensor directly without differentiable backward see make sense all cases output None None None None None TODO backward also differentiable now add test test higher level gradients grad_output to_local None None None None None DTensor torch Tensor ` ` DTensor ` ` Distributed Tensor subclass ` ` torch Tensor ` ` provides single-device like abstraction program multi-device ` ` torch Tensor ` ` It describes distributed tensor sharding layout DTensor Layout through ` DeviceMesh ` following types ` Placement ` ` Shard ` Tensor sharded tensor dimension ` ` dim ` ` devices ` ` DeviceMesh ` ` dimension ` Replicate ` Tensor replicated devices ` ` DeviceMesh ` ` dimension ` Partial ` Tensor pending reduction devices ` ` DeviceMesh ` ` dimension When calling PyTorch operators ` ` DTensor ` ` overrides PyTorch operators perform sharded computation issue communications whenever necessary Along operator computation ` ` DTensor ` ` will transform propagate placements DTensor Layout properly based operator semantic itself generate new ` ` DTensor ` ` outputs To ensure numerical correctness ` ` DTensor ` ` sharded computation when calling PyTorch operators ` ` DTensor ` ` requires every Tensor argument operator DTensor note Directly using Tensor subclass constructor here recommended way create ` ` DTensor ` ` i e does handle autograd correctly hence public API Please refer ` create_dtensor ` _ section see how create ` ` DTensor ` ` _local_tensor torch Tensor _spec DTensorSpec __slots__ = _local_tensor _spec _op_dispatcher instance attribute handle runtime dispatching logic _op_dispatcher op_dispatch OpDispatcher = op_dispatch OpDispatcher This implementation just convince mypy _spec _local_tensor initialized immediately overridden below __new__ cls local_tensor torch Tensor spec DTensorSpec requires_grad bool - DTensor r = torch Tensor _dtensor__new__ cls local_tensor spec requires_grad=requires_grad r _spec = spec r _local_tensor = local_tensor r __new__ = torch Tensor _dtensor__new__ type ignore assignment noqa F torch _disable_dynamo mark_subclass_constructor_exportable_experimental __init__ args kwargs Construct DTensor local tensor device mesh placement other tensor properties i e shape requires_grad strides etc note This public API s only supposed used operator implementations internals If you want construct DTensor local tensor consider using ` ` DTensor from_local ` ` you want construct DTensor global tensor where you already have tensor initialized want shard tensor consider using ` ` distribute_tensor ` ` super __init__ pyre-fixme ` __repr__ ` overrides method defined ` DTensor ` inconsistently pyre-fixme Return type must annotated __repr__ type ignore override TODO consider all_gather local tensors better debugging f DTensor local_tensor= _local_tensor device_mesh= _spec mesh placements= _spec placements __tensor_flatten__ protocol inform how flatten DTensor local tensor PT tracing _local_tensor _spec requires_grad staticmethod __tensor_unflatten__ inner_tensors flatten_spec outer_size outer_stride assert flatten_spec None Expecting spec None ` __tensor_flatten__ ` value local_tensor = inner_tensors _local_tensor spec requires_grad = flatten_spec unflatten_tensor_meta = TensorMeta shape=outer_size stride=outer_stride dtype=spec tensor_meta dtype unflatten_spec = DTensorSpec spec mesh spec placements tensor_meta=unflatten_tensor_meta pyrefly ignore bad-argument-type DTensor pyrefly ignore bad-argument-count local_tensor unflatten_spec pyrefly ignore unexpected-keyword requires_grad=requires_grad __coerce_tangent_metadata__ any isinstance p Partial p placements placements = Replicate isinstance p Partial p p placements redistribute device_mesh=self device_mesh placements=placements __coerce_same_metadata_as_tangent__ flatten_spec expected_type=None expected_type None None spec _ = flatten_spec Result tensor_flatten redistribute device_mesh=self device_mesh placements=spec placements classmethod torch _disable_dynamo pyre-fixme Return type must annotated pyre-fixme Parameter must annotated __torch_dispatch__ cls func types args= kwargs=None type ignore override DTensor _op_dispatcher dispatch func args kwargs staticmethod from_local local_tensor torch Tensor device_mesh Optional DeviceMesh = None placements Optional Sequence Placement = None run_check bool = False shape Optional torch Size = None stride Optional tuple int = None - DTensor Create ` DTensor ` local torch Tensor each rank according ` ` device_mesh ` ` ` ` placements ` ` specified Args local_tensor torch Tensor local torch Tensor each rank device_mesh ` DeviceMesh ` optional DeviceMesh place tensor specified must called under DeviceMesh context manager default None placements List ` Placement ` optional placements describes how place local torch Tensor DeviceMesh must have same number elements ` ` device_mesh ndim ` ` Keyword args run_check bool optional cost extra communications perform sanity check across ranks check each local tensor s meta information ensure correctness If have ` Replicate ` ` ` placements ` ` data first rank device mesh dimension will broadcasted other ranks default False shape torch Size optional A List int which specifies size DTensor which build top ` local_tensor ` Note needs provided shape ` ` local_tensor ` ` different across ranks If provided ` ` shape ` ` will computed assuming given distributed tensor evenly sharded across ranks default None stride tuple optional A List int which specifies stride DTensor If provided ` ` stride ` ` will computed assuming given distributed tensor evenly sharded across ranks default None Returns A ` DTensor ` object note When ` ` run_check=False ` ` user s responsibility ensure local tensor passed correct across ranks i e tensor sharded ` ` Shard dim ` ` placement replicated ` ` Replicate ` ` placement If behavior created DTensor undefined note ` ` from_local ` ` differentiable ` requires_grad ` created ` DTensor ` object will depend ` local_tensor ` requires_grad ` local_tensor ` argument cannot DTensor isinstance local_tensor DTensor raise RuntimeError f local_tensor argument only accepts torch Tensor got type local_tensor value same shape dtype no need run_check must allgather metadatas check size dtype across ranks There should no data communication unless there s replication strategy where we broadcast replication first rank mesh dimension device_mesh = device_mesh _mesh_resources get_current_mesh device_type = device_mesh device_type convert local tensor desired device base device mesh s device_type device_type = local_tensor device type local_tensor is_meta local_tensor = local_tensor device_type set default placements replicated specified placements None placements = Replicate _ range device_mesh ndim placements = list placements idx placement enumerate placements normalize shard dim positive placement is_shard placement = cast Shard placement placement dim placements idx = Shard placement dim + local_tensor ndim ` from_local ` differentiable gradient dist tensor function created should flow back gradients local_tensor so we call autograd function construct dist tensor instead _FromTorchTensor apply pyre-ignore autograd func local_tensor device_mesh tuple placements run_check shape stride to_local grad_placements Optional Sequence Placement = None - torch Tensor Get local tensor DTensor its current rank For sharding returns local shard logical tensor view replication returns replica its current rank Keyword args grad_placements List ` Placement ` optional placements describes future layout any gradient layout Tensor returned function ` to_local ` converts DTensor local tensor returned local tensor might used original DTensor layout later code This argument hint user can give autograd case gradient layout returned tensor does match original DTensor layout If specified we will assume gradient layout remains same original DTensor use gradient computation Returns A ` torch Tensor ` ` ` AsyncCollectiveTensor ` ` object represents local tensor its current rank When ` ` AsyncCollectiveTensor ` ` object returned means local tensor ready yet i e communication finished In case user needs call ` ` wait ` ` wait local tensor ready note ` ` to_local ` ` differentiable ` ` requires_grad ` ` local tensor returned will depend ` DTensor ` requires_grad torch is_grad_enabled _local_tensor grad_placements None isinstance grad_placements tuple grad_placements = tuple grad_placements _ToTorchTensor apply grad_placements pyre-ignore autograd func redistribute device_mesh Optional DeviceMesh = None placements Optional Sequence Placement = None async_op bool = False forward_dtype Optional torch dtype = None backward_dtype Optional torch dtype = None - DTensor ` ` redistribute ` ` performs necessary collective operations redistribute current DTensor its current placements new placements its current DeviceMesh new DeviceMesh i e we can turn Sharded DTensor Replicated DTensor specifying Replicate placement each dimension DeviceMesh When redistributing current new placements one device mesh dimension we will perform following operations including communication collective local operation ` ` Shard dim ` ` - ` ` Replicate ` ` ` ` all_gather ` ` ` ` Shard src_dim ` ` - ` ` Shard dst_dim ` ` ` ` all_to_all ` ` ` ` Replicate ` ` - ` ` Shard dim ` ` local chunking i e ` ` torch chunk ` ` ` ` Partial ` ` - ` ` Replicate ` ` ` ` all_reduce ` ` ` ` Partial ` ` - ` ` Shard dim ` ` ` ` reduce_scatter ` ` ` ` redistribute ` ` would correctly figure out necessary redistribute steps DTensors created either -D N-D DeviceMesh Args device_mesh ` DeviceMesh ` optional DeviceMesh place DTensor If specified would use current DTensor s DeviceMesh default None placements List ` Placement ` optional new placements describes how place DTensor into DeviceMesh must have same number elements ` ` device_mesh ndim ` ` default replicate all mesh dimensions Keyword args async_op bool optional whether perform DTensor redistribute operation asynchronously Default False forward_dtype torch dtype optional local tensor datatype can converted ` ` forward_dtype ` ` before redistributing local tensor its forward The result DTensor will ` ` forward_dtype ` ` Default None backward_dtype torch dtype optional local tensor datatype can converted ` ` backward_dtype ` ` before redistributing local tensor its backward The result DTensor gradient would converted back current DTensor dtype Default None Returns A ` DTensor ` object note ` ` redistribute ` ` differentiable which means user do need worry about backward formula redistribute operation note ` ` redistribute ` ` currently only supports redistributing DTensor same DeviceMesh Please file issue you need redistribute DTensor different DeviceMesh NOTE This redistribute API currently only supports out place redistribution i e always create new DTensor object leave original one unchanged device_mesh specified use current device_mesh device_mesh = device_mesh device_mesh raise error new placements specified placements None raise RuntimeError placements needed redistribute placements = list placements i placement enumerate placements placement is_partial placements i = placement raise RuntimeError f Can redistribute placements i placement redistributing Partial internal use only isinstance placement Shard placement dim normalize shard dim positive placements i = Shard placement dim + ndim placements = tuple placements pyre-fixme ` Redistribute ` has no attribute ` apply ` Redistribute apply device_mesh placements async_op forward_dtype backward_dtype full_tensor grad_placements Optional Sequence Placement = None - torch Tensor Return full tensor DTensor It will perform necessary collectives gather local tensors other ranks its DeviceMesh concatenate them together It s syntactic sugar following code ` ` dtensor redistribute placements= Replicate mesh ndim to_local ` ` Keyword args grad_placements List ` Placement ` optional placements describes future layout any gradient layout full Tensor returned function ` full_tensor ` converts DTensor full torch Tensor returned torch tensor might used original replicated DTensor layout later code This argument hint user can give autograd case gradient layout returned tensor does match original replicated DTensor layout If specified we will assume gradient layout full tensor replicated Returns A ` torch Tensor ` object represents full tensor DTensor note ` ` full_tensor ` ` differentiable redist_res = redistribute placements= Replicate device_mesh ndim async_op=False _ToTorchTensor apply redist_res grad_placements property device_mesh - DeviceMesh The ` DeviceMesh ` attribute associates DTensor object note ` ` device_mesh ` ` read-only property can set _spec mesh property placements - tuple Placement The placements attribute DTensor describes layout DTensor its DeviceMesh note ` ` placements ` ` read-only property can set _spec placements _raise_if_contains_partial_placements - None Raise error DTensor contains partial placements placement _spec placements isinstance placement Partial continue raise ValueError Any checkpointing related operations supported DTensor partial placements __create_write_items__ fqn str object Any _raise_if_contains_partial_placements torch distributed checkpoint planner_helpers _create_write_items_for_dtensor hasattr _local_tensor __create_write_items__ _local_tensor __create_write_items__ fqn object type ignore attr-defined isinstance _local_tensor torch Tensor _create_write_items_for_dtensor fqn object raise RuntimeError Unsupported tensor type __create_chunk_list__ Return list ChunkStorageMetadata which dataclass describes size offset local shard replica current rank For DTensor each rank will have single local shard replica so returned list usually only has one element This dunder method primariy used distributed checkpoint purpose Returns A List ` ChunkStorageMetadata ` object represents shard size offset current rank _raise_if_contains_partial_placements torch distributed checkpoint planner_helpers _create_chunk_from_dtensor hasattr _local_tensor __create_chunk_list__ _local_tensor __create_chunk_list__ type ignore attr-defined isinstance _local_tensor torch Tensor _create_chunk_from_dtensor raise RuntimeError Unsupported tensor type __get_tensor_shard__ index _raise_if_contains_partial_placements hasattr _local_tensor __get_tensor_shard__ _local_tensor __get_tensor_shard__ index type ignore attr-defined isinstance _local_tensor torch Tensor to_local raise RuntimeError Unsupported tensor type classmethod __metadata_guard__ cls orig tuple DTensorSpec bool other tuple DTensorSpec bool - bool orig_spec orig_requires_grad = orig other_spec other_requires_grad = other orig_spec _check_equals other_spec skip_shapes=True orig_requires_grad == other_requires_grad distribute_tensor tensor torch Tensor device_mesh Optional DeviceMesh = None placements Optional Sequence Placement = None src_data_rank Optional int = - DTensor Distribute leaf ` ` torch Tensor ` ` i e nn Parameter buffers ` ` device_mesh ` ` according ` ` placements ` ` specified The rank ` ` device_mesh ` ` ` ` placements ` ` must same The ` ` tensor ` ` distribute logical global tensor API would use ` ` tensor ` ` first rank DeviceMesh dimension source truth preserve single-device semantic If you want construct DTensor middle Autograd computation please use meth ` DTensor from_local ` instead Args tensor torch Tensor torch Tensor distributed Note you want shard tensor dimension evenly divisible number devices mesh dimension we use ` ` torch chunk ` ` semantic shard tensor scatter shards The uneven sharding behavior experimental subject change device_mesh ` DeviceMesh ` optional DeviceMesh distribute tensor specified must called under DeviceMesh context manager default None placements List ` Placement ` optional placements describes how place tensor DeviceMesh must have same number elements ` ` device_mesh ndim ` ` If specified we will default replicate tensor across ` ` device_mesh ` ` first rank each dimension ` device_mesh ` Keyword args src_data_rank int optional rank source data logical global tensor used meth ` distribute_tensor ` scatter broadcast shards replicas other ranks By default we use ` ` group_rank= ` ` each DeviceMesh dimension source data preserve single-device semantic If passing ` ` None ` ` explicitly meth ` distribute_tensor ` simply uses its local data instead trying preserve single-device semantic via scatter broadcast Default Returns A ` DTensor ` ` ` XLAShardedTensor ` ` object note When initialize DeviceMesh ` ` xla ` ` device_type ` ` distribute_tensor ` ` ` XLAShardedTensor ` instead see ` issue https github com pytorch pytorch issues ` __ more details The XLA integration experimental subject change torch _C _log_api_usage_once torch dtensor distribute_tensor get default device mesh there s nothing specified device_mesh = device_mesh _mesh_resources get_current_mesh device_type = device_mesh device_type device_type == xla try call PyTorch XLA SPMD ` xla ` backend type device mesh This returns XLAShardedTensor torch_xla distributed spmd type ignore xla_distribute_tensor xla_distribute_tensor tensor device_mesh placements type ignore return-value except ImportError e msg = To use DTensor API xla you must install torch_xla package raise ImportError msg e tensor is_leaf raise RuntimeError ` distribute_tensor ` should used distribute leaf tensors found non-leaf tensor convert tensor corresponding device type s device type device_type = tensor device type tensor is_meta tensor = tensor device_type set default placements replicated specified placements None placements = Replicate _ range device_mesh ndim len placements = device_mesh ndim raise ValueError f ` placements ` must have same length ` device_mesh ndim ` f Found placements length len placements device_mesh ndim device_mesh ndim isinstance tensor DTensor tensor already DTensor we need check we can further shard DTensor two device mesh belong same parenet mesh further sharding possible check device mesh placements same tensor device_mesh = device_mesh raise ValueError f Cannot distribute DTensor device mesh tensor device_mesh f different device mesh device_mesh tensor placements = tuple placements raise ValueError f Cannot distribute DTensor placements tensor placements f different placements placements do you want call f ` redistribute ` instead tensor local_tensor = tensor detach TODO xilun address sharding order distribute tensor according placements placements = list placements idx placement enumerate placements isinstance placement Shard placement_dim = placement dim + tensor ndim placement dim placement dim isinstance placement _StridedShard local_tensor = _StridedShard _make_shard_tensor placement_dim local_tensor device_mesh idx src_data_rank split_factor=placement split_factor placements idx = _StridedShard placement_dim split_factor=placement split_factor local_tensor = Shard _make_shard_tensor placement_dim local_tensor device_mesh idx src_data_rank placements idx = Shard placement_dim isinstance placement Replicate local_tensor = Replicate _make_replicate_tensor local_tensor device_mesh idx src_data_rank raise RuntimeError f Trying distribute tensor unsupported placements placement device mesh dimension idx placements = tuple placements assert local_tensor None distributing tensor should None detach local tensor passed DTensor since after construction DTensor autograd would work top DTensor instead local tensor spec = DTensorSpec mesh=device_mesh placements=placements tensor_meta=TensorMeta shape=tensor size stride=tensor stride dtype=tensor dtype pyrefly ignore bad-argument-type DTensor pyrefly ignore bad-argument-count local_tensor requires_grad_ tensor requires_grad spec pyrefly ignore unexpected-keyword requires_grad=tensor requires_grad deprecated Please use ` distribute_tensor ` ` src_data_rank=None ` instead _shard_tensor full_tensor torch Tensor placements Sequence Shard device_mesh Optional DeviceMesh = None - DTensor Locally shards full tensor based indicated sharding arrangement returns DTensor containing local shard warning This private API subject change It skips communication otherwise required ` distribute_tensor ` It only applicable cases where all ranks have same ` full_tensor ` For example distributed inference all ranks load same checkpoint This API will check data equality between ranks thus user s responsibility ensure ` full_tensor ` same across ranks Args full_tensor torch Tensor full tensor sharded placements Sequence ` Shard ` placements describes how place local tensor DeviceMesh device_mesh ` DeviceMesh ` optional DeviceMesh place DTensor Must have same dimension number placements If specified would retrieve current context Returns A ` DTensor ` object shard its local tensor Examples xdoctest +SKIP need world_size rank device_mesh = dist init_device_mesh cuda world_size full_tensor = torch arange world_size device=f cuda rank dtensor = _shard_tensor full_tensor Shard device_mesh distribute_tensor full_tensor device_mesh placements src_data_rank=None distribute_module module nn Module device_mesh Optional DeviceMesh = None partition_fn Optional Callable str nn Module DeviceMesh None = None input_fn Optional Callable nn Module Any DeviceMesh None = None output_fn Optional Callable nn Module Any DeviceMesh None = None - nn Module This function expose three functions control parameters inputs outputs module To perform sharding module before runtime execution specifying ` ` partition_fn ` ` i e allow user convert Module parameters ` DTensor ` parameters according ` partition_fn ` specified To control inputs outputs module during runtime execution specifying ` ` input_fn ` ` ` ` output_fn ` ` i e convert input ` DTensor ` convert output back ` ` torch Tensor ` ` Args module ` nn Module ` user module partitioned device_mesh ` DeviceMesh ` device mesh place module partition_fn Callable function partition parameters i e shard certain parameters across ` ` device_mesh ` ` If ` ` partition_fn ` ` specified default we replicate all module parameters ` ` module ` ` across mesh input_fn Callable specify input distribution i e could control how input module sharded ` ` input_fn ` ` will installed module ` ` forward_pre_hook ` ` pre forward hook output_fn Callable specify output distribution i e could control how output sharded convert back torch Tensor ` ` output_fn ` ` will installed module ` ` forward_hook ` ` post forward hook Returns A module contains parameters buffers all ` ` DTensor ` ` s note When initialize DeviceMesh ` ` xla ` ` device_type ` ` distribute_module ` ` nn Module PyTorch XLA SPMD annotated parameters See ` issue https github com pytorch pytorch issues ` __ more details The XLA integration experimental subject change torch _C _log_api_usage_once torch dtensor distribute_module already_distributed = getattr module _distribute_module_applied False already_distributed raise RuntimeError distribute_module should only called once module has already been called module device_mesh = device_mesh _mesh_resources get_current_mesh device_type = device_mesh device_type device_type == xla try This function annotates all module parameters auto-partitioning PyTorch XLA SPMD explicitly partition ` XLAShardedTensor ` parameters according ` partition_fn ` specified torch_xla distributed spmd type ignore xla_distribute_module xla_distribute_module module device_mesh partition_fn input_fn output_fn type ignore return-value except ImportError e msg = To use DTensor API xla you must install torch_xla package raise ImportError msg e replicate_module_params_buffers m nn Module mesh DeviceMesh - None This function loop over immediate module parameters buffers replicate all non DTensor params buffers DTensor parameters buffers they have been partitioned partition_fn we can t easily use ` module _apply ` here because we don t know what happened inside partition_fn user could do anything i e install hooks we want preserve those full_replicate = Replicate mesh ndim key param m _parameters items param None isinstance param DTensor m register_parameter key nn Parameter distribute_tensor param data mesh full_replicate key buffer m _buffers items buffer None isinstance buffer DTensor m _buffers key = distribute_tensor buffer mesh full_replicate partition_fn None partition_fn specified we default replicate all module params buffers submod module modules replicate_module_params_buffers submod device_mesh apply partition_fun submodules name submod module named_modules partition_fn name submod device_mesh replicate_module_params_buffers submod device_mesh register input_fn module forward pre hook input_fn None check input_fn signature num_args = len inspect signature input_fn parameters num_args == input_fn only takes inputs device mesh warnings warn Deprecating input_fn takes two arguments inputs device_mesh please use input_fn takes module inputs device_mesh instead FutureWarning stacklevel= module register_forward_pre_hook lambda _ inputs input_fn inputs device_mesh type ignore call-arg num_args == input_fn takes module inputs device mesh module register_forward_pre_hook lambda mod inputs input_fn mod inputs device_mesh raise ValueError f input_fn should take arguments got num_args arguments register output_fn module forward hook output_fn None num_args = len inspect signature output_fn parameters num_args == output_fn only takes outputs device mesh warnings warn Deprecating output_fn takes two arguments inputs device_mesh please use output_fn takes module inputs device_mesh instead FutureWarning stacklevel= module register_forward_hook lambda mod inputs outputs output_fn outputs device_mesh type ignore call-arg num_args == module register_forward_hook lambda mod inputs outputs output_fn mod outputs device_mesh raise ValueError f output_fn should take arguments got num_args arguments module _distribute_module_applied = True type ignore assignment module Below tensor factory function APIs which used create DTensor directly We need make separate factory function APIs because tensor subclass could override tensor factory methods we need user call factory functions user intended device_mesh placements create proper DTensor _dtensor_init_helper type ignore no-untyped-def init_op size torch Size device_mesh Optional DeviceMesh = None placements Optional Sequence Placement = None kwargs - DTensor device_mesh None use one mesh resources device_mesh = device_mesh _mesh_resources get_current_mesh kwargs device = device_mesh device_type set default placements replicated specified placements = placements tuple Replicate _ range device_mesh ndim check device_mesh against placements assert device_mesh ndim == len placements mesh dimension does match length placements assert kwargs layout == torch strided layout value supported torch_stride = torch _prims_common make_contiguous_strides_for size get local tensor shape local_shape _ = compute_local_shape_and_global_offset size device_mesh placements initialize local tensor init_op torch full fill_value = kwargs pop fill_value local_tensor = init_op local_shape fill_value kwargs init_op torch rand init_op torch randn tensor meta used except ` shape ` dtype = kwargs get dtype torch get_default_dtype tensor_meta = TensorMeta size dtype spec = DTensorSpec device_mesh tuple placements tensor_meta=tensor_meta random is_rng_supported_mesh device_mesh random _rng_tracker random _rng_tracker = random OffsetBasedRNGTracker device_mesh assert random _rng_tracker None random _rng_tracker _distribute_region spec local_tensor = init_op local_shape kwargs local_tensor = init_op local_shape kwargs spec = DTensorSpec device_mesh tuple placements tensor_meta=TensorMeta size torch_stride local_tensor dtype pyrefly ignore bad-argument-type DTensor pyrefly ignore bad-argument-count local_tensor spec pyrefly ignore unexpected-keyword requires_grad=kwargs requires_grad ones type ignore no-untyped-def size dtype Optional torch dtype = None layout torch layout = torch strided requires_grad bool = False device_mesh Optional DeviceMesh = None placements Optional Sequence Placement = None - DTensor Returns ` DTensor ` filled scalar value shape defined variable argument ` ` size ` ` Args size int sequence integers defining shape output ` DTensor ` Can variable number arguments collection like list tuple E g ones ones ones Keyword args dtype ` torch dtype ` optional desired data type returned ` DTensor ` Default ` ` None ` ` uses global default see func ` torch set_default_dtype ` layout ` torch layout ` optional desired layout returned DTensor Default ` ` torch strided ` ` requires_grad bool optional If autograd should record operations returned ` DTensor ` Default ` ` False ` ` device_mesh ` DeviceMesh ` type contains mesh info ranks placements sequence ` Placement ` type ` ` Shard ` ` ` ` Replicate ` ` Returns A ` DTensor ` object each rank torch_size = normalize_to_torch_size size _dtensor_init_helper torch ones torch_size dtype=dtype layout=layout requires_grad=requires_grad device_mesh=device_mesh placements=placements empty type ignore no-untyped-def size dtype Optional torch dtype = None layout torch layout = torch strided requires_grad bool = False device_mesh Optional DeviceMesh = None placements Optional Sequence Placement = None - DTensor Returns ` DTensor ` filled uninitialized data The shape ` DTensor ` defined variable argument ` ` size ` ` Args size int sequence integers defining shape output ` DTensor ` Can variable number arguments collection like list tuple E g empty empty empty Keyword args dtype ` torch dtype ` optional desired data type returned ` DTensor ` Default ` ` None ` ` uses global default see func ` torch set_default_dtype ` \ layout ` torch layout ` optional desired layout returned ` DTensor ` Default ` ` torch strided ` ` requires_grad bool optional If autograd should record operations returned ` DTensor ` Default ` ` False ` ` device_mesh ` DeviceMesh ` type contains mesh info ranks placements sequence ` Placement ` type ` ` Shard ` ` ` ` Replicate ` ` Returns A ` DTensor ` object each rank torch_size = normalize_to_torch_size size _dtensor_init_helper torch empty torch_size dtype=dtype layout=layout requires_grad=requires_grad device_mesh=device_mesh placements=placements full type ignore no-untyped-def size fill_value dtype Optional torch dtype = None layout torch layout = torch strided requires_grad bool = False device_mesh Optional DeviceMesh = None placements Optional Sequence Placement = None - DTensor Returns ` DTensor ` filled ` ` fill_value ` ` according ` ` device_mesh ` ` ` ` placements ` ` shape defined argument ` ` size ` ` Args size int sequence integers defining shape output ` DTensor ` Can variable number arguments collection like list tuple E g ones ones ones fill_value Scalar value fill output tensor Keyword args dtype ` torch dtype ` optional desired data type returned ` DTensor ` Default ` ` None ` ` uses global default see func ` torch set_default_dtype ` layout ` torch layout ` optional desired layout returned DTensor Default ` ` torch strided ` ` requires_grad bool optional If autograd should record operations returned ` DTensor ` Default ` ` False ` ` device_mesh ` DeviceMesh ` type contains mesh info ranks placements sequence ` Placement ` type ` ` Shard ` ` ` ` Replicate ` ` Returns A ` DTensor ` object each rank torch_size = normalize_to_torch_size size _dtensor_init_helper torch full torch_size fill_value=fill_value dtype=dtype layout=layout requires_grad=requires_grad device_mesh=device_mesh placements=placements rand type ignore no-untyped-def size requires_grad bool = False dtype Optional torch dtype = None layout torch layout = torch strided device_mesh Optional DeviceMesh = None placements Optional Sequence Placement = None - DTensor Returns ` DTensor ` filled random numbers uniform distribution interval ` ` ` ` The shape tensor defined variable argument ` ` size ` ` Args size int sequence integers defining shape output ` DTensor ` Can variable number arguments collection like list tuple E g ones ones ones Keyword args dtype ` torch dtype ` optional desired data type returned ` DTensor ` Default ` ` None ` ` uses global default see func ` torch set_default_dtype ` layout ` torch layout ` optional desired layout returned DTensor Default ` ` torch strided ` ` requires_grad bool optional If autograd should record operations returned ` DTensor ` Default ` ` False ` ` device_mesh ` DeviceMesh ` type contains mesh info ranks placements sequence ` Placement ` type ` ` Shard ` ` ` ` Replicate ` ` Returns A ` DTensor ` object each rank torch_size = normalize_to_torch_size size _dtensor_init_helper torch rand torch_size dtype=dtype layout=layout requires_grad=requires_grad device_mesh=device_mesh placements=placements randn type ignore no-untyped-def size requires_grad bool = False dtype Optional torch dtype = None layout torch layout = torch strided device_mesh Optional DeviceMesh = None placements Optional Sequence Placement = None - DTensor Returns ` DTensor ` filled random numbers normal distribution mean variance The shape tensor defined variable argument ` ` size ` ` Args size int sequence integers defining shape output ` DTensor ` Can variable number arguments collection like list tuple E g ones ones ones Keyword args dtype ` torch dtype ` optional desired data type returned ` DTensor ` Default ` ` None ` ` uses global default see func ` torch set_default_dtype ` layout ` torch layout ` optional desired layout returned DTensor Default ` ` torch strided ` ` requires_grad bool optional If autograd should record operations returned ` DTensor ` Default ` ` False ` ` device_mesh ` DeviceMesh ` type contains mesh info ranks placements sequence ` Placement ` type ` ` Shard ` ` ` ` Replicate ` ` Returns A ` DTensor ` object each rank torch_size = normalize_to_torch_size size _dtensor_init_helper torch randn torch_size dtype=dtype layout=layout requires_grad=requires_grad device_mesh=device_mesh placements=placements zeros type ignore no-untyped-def size requires_grad bool = False dtype Optional torch dtype = None layout torch layout = torch strided device_mesh Optional DeviceMesh = None placements Optional Sequence Placement = None - DTensor Returns ` DTensor ` filled scalar value Args size int sequence integers defining shape output ` DTensor ` Can variable number arguments collection like list tuple E g zeros zeros zeros Keyword args requires_grad bool optional If autograd should record operations returned ` DTensor ` Default ` ` False ` ` dtype ` torch dtype ` optional desired data type returned ` DTensor ` Default ` ` None ` ` uses global default see func ` torch set_default_dtype ` layout ` torch layout ` optional desired layout returned ` DTensor ` Default ` ` torch strided ` ` device_mesh ` DeviceMesh ` type contains mesh info ranks placements sequence ` Placement ` type ` ` Shard ` ` ` ` Replicate ` ` Returns A ` DTensor ` object each rank torch_size = normalize_to_torch_size size _dtensor_init_helper torch zeros torch_size dtype=dtype layout=layout requires_grad=requires_grad device_mesh=device_mesh placements=placements