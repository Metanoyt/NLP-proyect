unittest collections abc Sequence typing Optional torch common_utils MACOS_VERSION opinfo core DecorateInfo OpInfo torch backends mps is_available mps_ops_modifier ops Sequence OpInfo device_type str = mps xfail_exclusion Optional list str = None sparse bool = False - Sequence OpInfo xfail_exclusion None xfail_exclusion = Supported complex OPS SUPPORTED_COMPLEX_OPS = __radd__ __rmul__ __rsub__ __getitem__ _unsafe_masked_index _unsafe_masked_index_put_accumulate abs add alias_copy argwhere atleast_ d atleast_ d atleast_ d as_strided as_strided_copy as_strided_scatter asin asinh acos atan broadcast_tensors broadcast_to chalf cfloat chunk clone conj conj_physical contiguous cos cosh diag diag_embed diagflat diagonal diagonal_copy diagonal_scatter divno_rounding_mode dsplit empty empty_permuted empty_strided exp expm exp expand expand_as expand_copy flatten fill full full_like H hsplit imag index_add index_copy index_select index_put isfinite isinf isreal item kron linalg diagonal linalg householder_product linalg svd log log p log log logaddexp logaddexp mH mT masked_fill masked_scatter masked_select meshgridlist_of_tensors meshgridvariadic_tensors movedim mul narrow narrow_copy neg new_full new_ones new_zeros nn functional conv d nn functional conv d nn functional conv_transpose d nn functional conv_transpose d nn functional conv_transpose d nn functional feature_alpha_dropoutwithout_train nn functional padcircular nn functional softsign nn functional tanhshrink nn functional unfold nonzero ones ones_like outer permute permute_copy positive randn ravel real repeat_interleave reshape_as reshape resolve_conj resolve_neg rsqrt rsub scalar_tensor select sgn sigmoid sin sinc sinh slice special spherical_bessel_j special entr special xlog py special zeta split split_with_sizes split_with_sizes_copy splitlist_args sqrt squeeze squeeze_copy squeezemultiple sub svd t t_copy tanh tan tensor_split transpose transpose_copy tril triu true_divide T unbind unbind_copy unflatten unfold unfold_copy unsafe_chunk unsafe_split unsqueeze unsqueeze_copy view_as view_as_real view view_copy vsplit zero_ zeros zeros_like __rdiv__ __rmatmul__ _chunk_cat acosh all allclose angle any addcdiv addcmul addmmdecomposed addmv atanh bfloat bmm bool cartesian_prod cat char column_stack combinations corrcoef constant_pad_nd cov count_nonzero diff div dot dstack einsum eq equal eye fft fft fft fft fft fftn fft fftshift fft ifft fft ifft fft ifftn fft ifftshift fft irfftn fft irfft fft irfft fft hfftn fft hfft fft hfft flip fliplr flipud float gradient half hstack inner int isclose isnan ldexp lerp linalg multi_dot linalg pinv linspace linspacetensor_overload logical_and logical_not logical_or logical_xor logsumexp long masked mean masked prod masked std masked sum masked var masked logsumexp matmul mean mm mv ne nn functional padconstant nn functional padreflect nn functional padreplicate nn functional pixel_shuffle nn functional pixel_unshuffle nn functional rms_norm pinverse prod reciprocal roll rot short square stack stft sum sum_to_size tensordot trace trapz trapezoid vstack where byte MACOS_BEFORE_ _ _XFAILLIST = These ops work fine fail x fft hfft torch complex Those ops expected work UNIMPLEMENTED_XFAILLIST dict str Optional list = Failures due lack op implementation MPS backend logspace None logspacetensor_overload None linalg eig None linalg eigvals None put None cauchy_ None cauchy None cholesky_inverse None cholesky_solve None frexp None gcd None geqrf None nn functional grid_sample None Unsupported Border padding mode hash_tensor None heaviside None index_reduceprod None index_reducemean None index_reduceamax None index_reduceamin None kthvalue None lcm None linalg cond None linalg eigh None linalg eigvalsh None linalg ldl_factor None linalg ldl_factor_ex None linalg ldl_solve None linalg lstsq None linalg lstsqgrad_oriented None linalg lu None linalg lu_solve None linalg matrix_norm torch float linalg norm torch float linalg normsubgradients_at_zero torch float linalg qr None linalg svdvals None linalg vecdot None lu_solve None masked median None matrix_exp None mode None normnuc None nn functional fractional_max_pool d None nn functional fractional_max_pool d None nn functional adaptive_avg_pool d None nn functional adaptive_max_pool d None nn functional interpolatearea None nn functional interpolatebicubic torch uint nn functional ctc_loss None nn functional multi_margin_loss None nn functional multilabel_margin_loss None nn functional pdist None nn functional rrelu None nn functional norm None ormqr None pca_lowrank None qr None scatter_reduceamax torch int torch int MACOS_VERSION torch int scatter_reduceamin torch int torch int MACOS_VERSION torch int segment_reduce None _segment reduce None segment reduce None segment_reduce_offsets None _segment_reduce_offsets None _segment_reduce_lengths None _segment_reducelengths None _segment_reduceoffsets None sparse mm None sparse sampled_addmm None sparse mmreduce None special airy_ai None special erfcx None special laguerre_polynomial_l None special legendre_polynomial_p None special log_ndtr None special ndtri None svd_lowrank None symeig None take None None vdot None segment_reduce_ None _upsample_bilinear d_aa torch uint uint CPU only _upsample_bicubic d_aa torch uint uint CPU only geometric None geometric_ None log_normal_ None log_normal None cdouble None double None nn functional softminwith_dtype None log_softmaxwith_dtype None softmaxwith_dtype None float_power None linalg matrix_rankhermitian None linalg pinvhermitian None nonzero_static None MPS input sizes must divisible output sizes nn functional adaptive_avg_pool d None nn functional adaptive_avg_pool d None Convolution integral types supported MPS nn functional conv d torch int nn functional conv d torch int nn functional conv d torch int nn functional conv_transpose d torch int nn functional conv_transpose d torch int torch bfloat nn functional conv_transpose d torch int torch bfloat torch float Unsupported dtypes histc torch float torch bfloat GEMM MPS supported integral types nn functional linear torch int torch int torch int torch uint torch int addbmm torch int torch int torch int torch uint torch int baddbmm torch int torch int torch int torch uint torch int mat torch int torch int torch int torch uint torch int returned output CPU float bincount torch int torch int torch int torch uint torch int UNIMPLEMENTED_XFAILLIST_SPARSE dict str Optional list = logspace None logspacetensor_overload None linalg eig None linalg eigvals None put None MACOS_VERSION UNIMPLEMENTED_XFAILLIST update quantile None nanquantile None sparse UNIMPLEMENTED_XFAILLIST update UNIMPLEMENTED_XFAILLIST_SPARSE UNDEFINED_XFAILLIST dict str Optional list = Top operators topk fails duplicate indices topk torch int torch int torch int torch uint torch int Failures due random output they generate using Philox engine causing mismatch CPU results multinomial torch float torch float torch bfloat random results uniform torch float torch float torch bfloat rand_like torch float torch float torch bfloat randint None randint_like None randn None randn_like None bernoulli torch float torch float torch bfloat exponential torch float torch float torch bfloat nn functional feature_alpha_dropoutwith_train torch float torch float torch bfloat normal torch float torch float torch bfloat normalin_place torch float torch float torch bfloat normalnumber_mean torch float torch float torch bfloat nn functional alpha_dropout torch float torch float torch bfloat nn functional dropout torch float torch float torch bfloat nn functional dropout d torch float torch float torch bfloat nn functional dropout d torch float torch float torch bfloat See https github com pytorch pytorch issues nn functional multi_head_attention_forward torch float torch float torch bfloat zero negative integer powers undefined __rpow__ torch int torch int torch int torch int resize_ torch float torch float torch bfloat resize_as_ torch float torch float torch bfloat CPU Errors addr torch bool torch int torch int torch int torch uint torch int addmv_impl_cpu implemented Half as_stridedpartial_views None cpu result off showing random values random results mps vs cpu Mismatched elements Greatest absolute difference index up e- allowed Greatest relative difference inf index up e- allowed cuda dev +cu vs cpu Mismatched elements Greatest absolute difference index up e- allowed Greatest relative difference inf index up e- allowed nn functional scaled_dot_product_attention torch float torch float torch bfloat ON_MPS_XFAILLIST dict str Optional list = Failures due lack implementation downstream functions MPS backend TODO remove these once downstream function aten _linalg_svd U have been implemented linalg matrix_rank None Exception Caused ` torch arange - - dtype=torch uint device= mps ` arange torch uint before macOS falls back cpu pass forward pass grid_sampler_ d torch float torch float torch bfloat Unsupported Border padding mode Failure due precision issue fp both cpu mps there test cases might produce inf result nn functional pairwise_distance torch float test blow pass macOS falls back cpu Argsort case using duplicate indices undefined behaviour - CPU output tensor device= cpu - MPS output tensor device= mps Elements index both equal Since CPU using argsort stable=True these cases result undefined behaviour argsort torch float torch int torch uint torch bool torch bfloat Same issue ` argsort ` duplicate indices This test checks both sorted values indices The values sorted tensor match CPU case returned indices results undefined behaviour sort torch int torch uint torch bool torch float torch bfloat EMPTY_OPS_SKIPLIST = Fill tensors uninitialized data causing mismatch CPU They occasionally match thus skipping them See https github com pytorch pytorch issues new_empty None new_empty_strided None empty_strided None CPU empty returning all s there mismatch MPS allocation MacOS According https pytorch org docs generated torch empty html empty None empty_like None empty_permuted None SKIPLIST = Unsupported This doesn t work M partially working M exception torch float nn functional conv d None The CPU impl grid_sampler_ d does use opmath_t so has large amount error compared MPS impl half precision types So we have skip these now grid_sampler_ d torch float torch bfloat addDecorator op OpInfo d DecorateInfo - None device_type None d device_type = device_type op decorators = op decorators + d op ops key = op name + op variant_test_name addDecorator op DecorateInfo unittest expectedFailure dtypes= torch double torch cdouble sparse Skipped due test_sparse_zero_dims test test_sparse py which allocates empty tensor which leads unexpected success addDecorator op DecorateInfo unittest skip Skipped due MPS supporting complex tensors dtypes= torch complex key EMPTY_OPS_SKIPLIST addDecorator op DecorateInfo unittest skip Skipping empty ops dtypes=EMPTY_OPS_SKIPLIST key key SKIPLIST addDecorator op DecorateInfo unittest skip Skipped dtypes=SKIPLIST key xfaillist UNIMPLEMENTED_XFAILLIST UNDEFINED_XFAILLIST ON_MPS_XFAILLIST key xfaillist key xfail_exclusion addDecorator op DecorateInfo unittest expectedFailure dtypes=xfaillist key key MACOS_BEFORE_ _ _XFAILLIST key xfail_exclusion MACOS_VERSION addDecorator op DecorateInfo unittest expectedFailure dtypes=MACOS_BEFORE_ _ _XFAILLIST key If ops supported complex types expect fail key SUPPORTED_COMPLEX_OPS addDecorator op DecorateInfo unittest expectedFailure dtypes= torch complex torch complex ops mps_ops_grad_modifier ops Sequence OpInfo - Sequence OpInfo XFAILLIST_GRAD = Unimplemented ops _segment_reduce torch float torch float _chunk_cat torch float torch float _upsample_bilinear d_aa None ` _upsample_bilinear d_aa_backward_out ` implemented MPS _upsample_bicubic d_aa None ` _upsample_bilinear d_aa_backward_out ` implemented MPS sparse mmreduce torch float csr supported linalg householder_product None unique_consecutive torch float torch float scalar_tensor torch float torch float cdist torch float masked scatter torch float torch float grid_sampler_ d None index_fill torch float torch float missing ` aten _unique ` igamma None currently supported any device igammac None currently supported any device linalg solve torch float torch float missing ` aten lu_solve ` linalg solve_ex torch float torch float missing ` aten lu_solve ` linalg tensorsolve torch float torch float missing ` aten lu_solve ` linalg det torch float torch float missing aten lu_solve out linalg slogdet torch float torch float missing aten lu_solve out logdet torch float torch float missing aten lu_solve out aminmax torch float torch float special i torch float i _backward implemented Half special i e torch float i e_backward implemented Half Correctness issues atanh torch float Same issue ` argsort ` ` sort ` duplicate elements undefined behaviour Forward pass passing since ` msort ` doesn t indices just values which match CPU On backward pass ` sort ` both used values indices thus resulting issmatch between CPU MPS Running ` msort ` stable ` sort ` passes msort torch float Random output exponential torch float torch float CPU errors derivative zeta implemented special zeta None derivative aten nextafter implemented CPU nextafter None derivative aten floor_divide implemented CPU floor_divide torch float torch float derivative aten narrow_copy implemented CPU narrow_copy torch float torch float derivative aten _histogramdd_from_bin_cts implemented CPU histogramdd torch float torch float derivative aten histogram implemented histogram torch float torch float bool object iterable allclose torch float torch float equal torch float torch float float object iterable item torch float torch float cpu error grad requires non-empty inputs randn torch float torch float signal windows bartlett torch float signal windows blackman torch float signal windows cosine torch float signal windows exponential torch float signal windows gaussian torch float signal windows general_cosine torch float signal windows general_hamming torch float signal windows hamming torch float signal windows hann torch float signal windows kaiser torch float signal windows nuttall torch float eye torch float torch float topk fails duplicate indices topk torch float Could run aten uniform_ arguments SparseCPU backend to_sparse None Exception derivative _unique implemented unique None SKIPLIST_GRAD = nn functional pairwise_distance torch float failed assertion ` destination datatype must fp nn functional conv d torch float nn functional conv d torch float nn functional conv d torch float nn functional conv_transpose d torch float nn functional conv_transpose d torch float nn functional conv_transpose d torch float ON_MPS_XFAILLIST = Failures due lack implementation downstream functions MPS backend TODO remove these once downstream function aten _linalg_svd U have been implemented linalg matrix_rank None Exception Caused sample input index MPS nn functional conv d torch float addDecorator op OpInfo d DecorateInfo - None op decorators = op decorators + d op ops key = op name + op variant_test_name key XFAILLIST_GRAD addDecorator op DecorateInfo unittest expectedFailure dtypes=XFAILLIST_GRAD key key SKIPLIST_GRAD addDecorator op DecorateInfo unittest skip dtypes=SKIPLIST_GRAD key key ON_MPS_XFAILLIST addDecorator op DecorateInfo unittest expectedFailure dtypes=ON_MPS_XFAILLIST key ops mps_ops_error_inputs_modifier ops Sequence OpInfo - Sequence OpInfo Error input samples do take dtype argument XFAILLIST = Exceptions raised __rmod__ __rsub__ __rpow__ clamp_max clamp_min masked_scatter unsupported float dtype multinomial nn functional conv d nn functional conv d nn functional conv d gather scatter scatter_add MPS does support tensor dimensions amax amin aminmax addDecorator op OpInfo d DecorateInfo - None op decorators = op decorators + d op ops key = op name + op variant_test_name key XFAILLIST addDecorator op DecorateInfo unittest expectedFailure ops mps_ops_modifier ops Sequence OpInfo device_type str = mps xfail_exclusion Optional list str = None sparse bool = False - Sequence OpInfo ops