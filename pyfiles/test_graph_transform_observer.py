Owner s module inductor glob math os shutil tempfile torch torch _dynamo torch _inductor config inductor_config torch _inductor test_case run_tests TestCase torch testing _internal common_cuda PLATFORM_SUPPORTS_FUSED_ATTENTION torch testing _internal common_utils IS_LINUX torch testing _internal inductor_utils HAS_CUDA_AND_TRITON try pydot noqa F HAS_PYDOT = True except ImportError HAS_PYDOT = False HAS_DOT = shutil which dot None TestGraphTransformObserver TestCase test_sdpa_rewriter HAS_CUDA_AND_TRITON PLATFORM_SUPPORTS_FUSED_ATTENTION HAS_PYDOT HAS_DOT dot_prod_attention query torch Tensor key torch Tensor value torch Tensor - torch Tensor Input tensors assumed have shape batch_size n_head seq_len embed_dim torch matmul query key transpose - - div math sqrt key shape - softmax dim=- matmul value log_url = tempfile mkdtemp inductor_config trace log_url_for_graph_xform = log_url inductor_config force_disable_caches = True compiled_fn = torch compile dot_prod_attention fullgraph=True tensor_shape = q = torch randn tensor_shape device= cuda k = torch randn tensor_shape device= cuda v = torch randn tensor_shape device= cuda compiled_fn q k v found_input_svg = False found_output_svg = False filepath_object glob glob log_url + os path isfile filepath_object filepath_object endswith input_graph dot found_input_svg = True filepath_object endswith output_graph dot found_output_svg = True assertTrue found_input_svg assertTrue found_output_svg __name__ == __main__ IS_LINUX run_tests