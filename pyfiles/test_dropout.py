Owner s module nn itertools random unittest itertools product torch torch nn nn torch nn functional F torch testing _internal common_cuda TEST_CUDA torch testing _internal common_device_type dtypes dtypesIfMPS expectedFailureMPS expectedFailureXLA instantiate_device_type_tests torch testing _internal common_nn freeze_rng_state NNTestCase torch testing _internal common_utils instantiate_parametrized_tests run_tests set_default_dtype TEST_PRIVATEUSE TestDropoutNN NNTestCase _do_cuda_memory_leak_check = True _do_cuda_non_default_stream = True _test_alpha_dropout cls input mean = input mean std = input std p module = cls p input_var = input detach clone requires_grad_ output = module input_var output mean should close input mean assertLess abs output data mean - mean output std should close input std assertLess abs output data std - std output backward input test_AlphaDropout generate random tensor zero mean unit std input = torch randn _test_alpha_dropout nn AlphaDropout input test_FeatureAlphaDropout b = random randint w = random randint h = random randint d = random randint num_features = input = torch randn num_features b d w h _test_alpha_dropout nn FeatureAlphaDropout input no batch dims input = torch randn _test_alpha_dropout nn FeatureAlphaDropout input unittest skipIf TEST_CUDA TEST_PRIVATEUSE CUDA PRIVATEUSE unavailable test_native_dropout_corner_case TEST_CUDA device = cuda TEST_PRIVATEUSE device = torch _C _get_privateuse _backend_name train True False p current_device device cpu x = torch randn device=current_device requires_grad_ x_ref = x detach requires_grad_ o = torch native_dropout x p train o_ref = torch dropout x_ref p train o sum backward o_ref sum backward assert o equal o_ref assert x grad equal x_ref grad test_invalid_dropout_p v = torch ones assertRaises ValueError lambda nn Dropout - assertRaises ValueError lambda nn Dropout assertRaises ValueError lambda nn Dropout d - assertRaises ValueError lambda nn Dropout d assertRaises ValueError lambda nn Dropout d - assertRaises ValueError lambda nn Dropout d assertRaises ValueError lambda nn Dropout d - assertRaises ValueError lambda nn Dropout d assertRaises ValueError lambda F dropout v - assertRaises ValueError lambda F dropout v TestDropoutNNDeviceType NNTestCase _test_dropout cls device input memory_format=torch contiguous_format p = input = input device fill_ - p module = cls p input_var = input clone memory_format=memory_format requires_grad_ output = module input_var assertTrue output is_contiguous memory_format=memory_format assertLess abs output data mean - - p output backward input assertTrue input_var grad is_contiguous memory_format=memory_format assertLess abs input_var grad data mean - - p module = cls p True input_var = input clone memory_format=memory_format requires_grad_ output = module input_var + assertTrue output is_contiguous memory_format=memory_format assertLess abs output data mean - - p output backward input assertTrue input_var grad is_contiguous memory_format=memory_format assertLess abs input_var grad data mean - - p check eval mode doesn t change anything inplace True False module = cls p inplace eval assertEqual input module input Check these don t raise errors module __repr__ str module _test_dropout_discontiguous cls device memory_format=torch contiguous_format In test we verify dropout preserves layout data different memory formats We check whether we get same values output dropout when probability dropout very close Reference https github com pytorch pytorch issues close_to_zero_p = e- Should almost zero zero p= different path taken p close_to_zero_p inp = torch ones device=device inp_discontiguous = torch empty device=device memory_format=memory_format inp_discontiguous copy_ inp mod = cls p=p out = mod inp_discontiguous p = Zero will keep strides based input When prob == input stride - output stride When prob = input stride - output stride assertTrue out is_contiguous memory_format=memory_format assertEqual inp_discontiguous out _test_dropout_stride_mean_preserve cls device invert_perm p d = x i i x enumerate p d d d d inp = torch ones device=device shifts = perm itertools permutations r= shift shifts p e- mod = cls p=p permuted_inp = inp permute perm contiguous permute invert_perm perm permuted_inp = permuted_inp shift shift out = mod permuted_inp assertTrue out permute perm is_contiguous assertEqual inp mean out mean rtol= atol= p == e- assertEqual permuted_inp out assertNotEqual permuted_inp out test_Dropout device input = torch empty _test_dropout nn Dropout device input _test_dropout_discontiguous nn Dropout device _test_dropout_discontiguous nn Dropout device memory_format=torch channels_last _test_dropout_stride_mean_preserve nn Dropout device device_type == cuda device_type == cpu input = input bfloat _test_dropout nn Dropout device input _test_dropoutNd_no_batch dropout input input_clone = input clone freeze_rng_state res_no_batch = dropout input freeze_rng_state res_batched = dropout input_clone unsqueeze squeeze assertEqual res_no_batch res_batched _test_dropoutNd_channel_zero dropout input Verify number zeros channel number elements channel fully positive input tensor shape = input shape B = shape C = shape channel_numel = torch tensor shape prod result = dropout input b c product range B range C assertTrue result b c count_nonzero channel_numel expectedFailureXLA seems like freeze_rng_state honoured XLA dtypes torch double dtypesIfMPS torch float expectedFailureMPS test_Dropout d device dtype set_default_dtype dtype N C L = random randint random randint random randint input = torch empty N C L _test_dropout nn Dropout d device input assertRaisesRegex RuntimeError Expected D D input received D input nn Dropout d p= torch rand device=device assertRaisesRegex RuntimeError Expected D D input received D input nn Dropout d p= torch rand device=device no batch dims input = torch rand device=device _test_dropoutNd_no_batch nn Dropout d p= input _test_dropoutNd_no_batch nn Dropout d p= inplace=True input check complete channels dropped input = torch ones device=device _test_dropoutNd_channel_zero nn Dropout d p= input _test_dropoutNd_channel_zero nn Dropout d p= inplace=True input expectedFailureXLA seems like freeze_rng_state honoured XLA test_Dropout d device b = random randint w = random randint h = random randint num_features = input = torch empty num_features b w h _test_dropout nn Dropout d device input _test_dropout nn Dropout d device input memory_format=torch channels_last _test_dropout_discontiguous nn Dropout d device _test_dropout_discontiguous nn Dropout d device memory_format=torch channels_last assertWarnsRegex UserWarning Received -D input dropout d nn Dropout d p= torch rand device=device assertWarnsRegex UserWarning Received -D input dropout d nn Dropout d p= torch rand device=device TODO Uncomment these lines once no-batch-dim inputs supported For now historical dropout d behavior performed D inputs See https github com pytorch pytorch issues input = torch rand device=device _test_dropoutNd_no_batch nn Dropout d p= input _test_dropoutNd_no_batch nn Dropout d p= inplace=True input assertWarnsRegex UserWarning assuming channel-wise D dropout behavior desired nn Dropout d p= torch rand device=device check complete channels dropped input = torch ones device=device _test_dropoutNd_channel_zero nn Dropout d p= input _test_dropoutNd_channel_zero nn Dropout d p= inplace=True input expectedFailureXLA seems like freeze_rng_state honoured XLA expectedFailureMPS Failing current pytorch MPS test_Dropout d device b = random randint w = random randint h = random randint d = random randint num_features = input = torch empty num_features b d w h _test_dropout nn Dropout d device input _test_dropout_discontiguous nn Dropout d device _test_dropout_discontiguous nn Dropout d device memory_format=torch channels_last assertWarnsRegex UserWarning Received -D input dropout d nn Dropout d p= torch rand device=device assertWarnsRegex UserWarning Received -D input dropout d nn Dropout d p= torch rand device=device no batch dims input = torch rand device=device _test_dropoutNd_no_batch nn Dropout d p= input _test_dropoutNd_no_batch nn Dropout d p= inplace=True input check complete channels dropped input = torch ones device=device _test_dropoutNd_channel_zero nn Dropout d p= input _test_dropoutNd_channel_zero nn Dropout d p= inplace=True input test_empty_dropout device x = torch tensor device out = torch nn functional dropout x assertEqual out size x size instantiate_device_type_tests TestDropoutNNDeviceType globals allow_mps=True instantiate_parametrized_tests TestDropoutNN __name__ == __main__ run_tests