typing Any TYPE_CHECKING torch torch _C DispatchKey torch _higher_order_ops utils autograd_not_implemented torch _ops HigherOrderOperator torch _subclasses fake_tensor FakeTensorMode TYPE_CHECKING torch _subclasses functional_tensor BaseFunctionalizeAPI torch fx experimental proxy_tensor ProxyTorchDispatchMode track_tensor_tree torch utils _pytree pytree RunConstGraph HigherOrderOperator __init__ - None super __init__ run_const_graph __call__ graph torch fx GraphModule args tuple object - object super __call__ graph args run_const_graph = RunConstGraph run_const_graph py_impl ProxyTorchDispatchMode run_const_graph_dispatch_mode mode ProxyTorchDispatchMode graph torch fx GraphModule args tuple object - object const_gm weights = graph args p_args = pytree tree_map mode tracer unwrap_proxy graph args type ignore union-attr assert isinstance const_gm torch fx GraphModule assert hasattr mode tracer root _const_graph type ignore union-attr mode tracer root register_module _const_graph const_gm type ignore union-attr proxy = mode tracer create_proxy call_function run_const_graph p_args out = const_gm weights track_tensor_tree out proxy constant=None tracer=mode tracer run_const_graph py_functionalize_impl run_const_graph_functional ctx BaseFunctionalizeAPI graph torch fx GraphModule args tuple Any - Any unwrapped_args = ctx unwrap_tensors args ctx redispatch_to_next out = run_const_graph graph unwrapped_args ctx wrap_tensors out type ignore arg-type run_const_graph py_autograd_impl autograd_not_implemented run_const_graph deferred_error=True run_const_graph py_impl FakeTensorMode run_const_graph_fake_tensor_mode mode FakeTensorMode graph torch fx GraphModule args tuple object - object assert isinstance graph torch fx GraphModule mode graph args run_const_graph py_impl DispatchKey CPU run_const_graph_cpu graph torch fx GraphModule args tuple object - object assert isinstance graph torch fx GraphModule graph args