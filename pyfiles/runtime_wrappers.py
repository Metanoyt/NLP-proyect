mypy allow-untyped-defs This module defines runtime wrappers which based previous analysis attempts process inputs outputs apply mutations handle functionalized randomness deduplicate inputs consolidate views into their bases see input_output_analysis builtins collections contextlib copy functools itertools pprint collections abc Callable contextlib AbstractContextManager nullcontext dataclasses dataclass field functools wraps typing Any Optional TYPE_CHECKING Union TYPE_CHECKING collections abc Sequence torch torch fx fx torch utils dlpack torch Tensor torch _dynamo config dynamo_config torch _dynamo callback callback_handler CallbackTrigger torch _dynamo utils CompileEventLogger dynamo_timed get_metrics_context torch _guards compile_context CompileContext detect_fake_mode DuplicateInputs tracing TracingContext torch _prims_common CUDARngStateHelper torch _subclasses FakeTensor torch fx experimental _backward_state BackwardState torch multiprocessing reductions StorageWeakRef torch utils _python_dispatch is_traceable_wrapper_subclass config collect_metadata_analysis run_functionalized_fw_and_collect_metadata descriptors AOTInput AOTOutput DummyAOTInput MetadataMutationAOTOutput SyntheticBaseAOTInput ViewBaseAOTInput functional_utils gen_alias_from_base graph_capture_wrappers aot_dispatch_subclass input_output_analysis compute_overlapping_inputs create_synthetic_base_metadata remove_dupe_metadata logging_utils describe_input format_guard_bug_msg track_graph_compiling schemas AOTConfig CompilerWrapper FxValue InductorWrapper InputAliasInfo MemoryFormatMeta MutationType OutputType PlainTensorMeta SubclassCreationMeta SubclassMeta TensorAlias TraceFn ViewAndMutationMeta subclass_utils requires_subclass_dispatch runtime_unwrap_tensor_subclasses wrap_tensor_subclasses utils call_and_expect_output_descs call_func_at_runtime_with_args make_boxed_func partial_flatten_asdict simple_wraps strict_zip without_output_descs zip = strict_zip The wrapper created function handles all runtime aliasing mutation epilogue logic needs run after compiled function This function accepts trace_joint flag indicating whether we re generating runtime epilogue forward-only inference graph autograd Function apply function This because there some minor differences how we treat these cases runtime - resize_ currently handled inference case fully handled autograd case - autograd cases inserts TensorAlias wrapper objects outputs alias inputs dataclass RuntimeWrapper CompilerWrapper indices_of_inps_to_detach list int trace_joint bool disable_amp bool post_compile compiled_fn aot_config AOTConfig runtime_metadata ViewAndMutationMeta _create_runtime_wrapper compiled_fn runtime_metadata=runtime_metadata indices_of_inps_to_detach=self indices_of_inps_to_detach trace_joint=self trace_joint keep_input_mutations=aot_config keep_inference_input_mutations disable_amp=self disable_amp NoopAliasHandler __init__ info runtime_metadata trace_joint pass __call__ orig_inputs fw_outs out out _unwrap_tensoralias x assert isinstance x TensorAlias x alias _identity x x AliasOfInputHandler __init__ info runtime_metadata trace_joint base_idx = info base_idx unwrap_out = _unwrap_tensoralias trace_joint _identity requires_grad = info requires_grad view_meta_sequence = info view_meta_sequence replay_views = config view_replay_for_aliased_outputs __call__ orig_inputs fw_outs out aliased_base_tensor = orig_inputs base_idx gen_alias_from_base aliased_base_tensor unwrap_out out requires_grad view_meta_sequence replay_views=self replay_views IsInputHandler __init__ info runtime_metadata trace_joint base_idx = info base_idx unwrap_out = _unwrap_tensoralias trace_joint _identity __call__ orig_inputs fw_outs out aliased_base_tensor = orig_inputs base_idx aliased_base_tensor AliasOfIntermediateHandler __init__ info runtime_metadata trace_joint _unwrap_aliased_base_tensor = _identity info output_type OutputType alias_of_intermediate OutputType alias_of_intermediate_save_as_output num_user_outputs = len runtime_metadata output_info base_idx = info base_idx + num_user_outputs base_idx = info base_idx base_idx runtime_metadata aliased_out_indices _unwrap_aliased_base_tensor = _unwrap_tensoralias unwrap_out = _unwrap_tensoralias trace_joint _identity requires_grad = info requires_grad view_meta_sequence = info view_meta_sequence replay_views = config view_replay_for_aliased_outputs __call__ orig_inputs fw_outs out aliased_base_tensor = fw_outs base_idx gen_alias_from_base _unwrap_aliased_base_tensor aliased_base_tensor unwrap_out out requires_grad view_meta_sequence replay_views=self replay_views _HANDLER_MAP = OutputType non_alias NoopAliasHandler OutputType unsafe_view_alias NoopAliasHandler OutputType custom_function_view NoopAliasHandler OutputType alias_of_input AliasOfInputHandler OutputType is_input IsInputHandler OutputType alias_of_intermediate AliasOfIntermediateHandler OutputType alias_of_intermediate_save_as_output AliasOfIntermediateHandler OutputType alias_of_intermediate_base_is_user_output AliasOfIntermediateHandler make_output_handler info runtime_metadata trace_joint handler_type = _HANDLER_MAP info output_type handler_type info runtime_metadata trace_joint sure why AOTDispatcher needs manually set maybe_mark_dynamic_helper t torch Tensor dims set int hasattr t _dynamo_weak_dynamic_indices pyrefly ignore missing-attribute t _dynamo_weak_dynamic_indices &#124; = dims t _dynamo_weak_dynamic_indices = dims copy type ignore attr-defined _should_disable_saved_tensors_hooks Compiled autograd supported yet added future torch _dynamo compiled_autograd in_compiled_autograd_region False get_hooks = torch _functorch _aot_autograd utils top_saved_tensors_hooks are_inline_hooks = torch _functorch _aot_autograd utils saved_tensors_hooks_are_inlineable hooks = get_hooks are_inline_hooks hooks True False _create_runtime_wrapper compiled_fn runtime_metadata ViewAndMutationMeta indices_of_inps_to_detach list int trace_joint bool keep_input_mutations bool disable_amp bool getattr compiled_fn _boxed_call False compiled_fn = make_boxed_func compiled_fn Note Inputs needed runtime epilogue after list clearing In Python functions you can t free input arguments function within scope function A workaround wrap input arguments list clear list within function Here implemented ` call_func_at_runtime_with_args steal_args=True ` This needed Compiled Autograd since some inputs activations should freed early However we cannot blindly clear entire list because AOTAutograd may need access some graph inputs after compiled function has finished running There two main cases Input mutations If there input mutations we must run outside graph we need access input Output aliasing Outputs aliases graph inputs generally must regenerated outside ` autograd Function ` doing so requires us accessing corresponding input after compiled artifact has run epilogue_args_idx = epilogue_args_idx extend runtime_metadata mutated_inp_runtime_indices info runtime_metadata output_info info output_type == OutputType alias_of_input info output_type == OutputType is_input assert isinstance info base_idx int epilogue_args_idx append info base_idx config unlift_effect_tokens assert len runtime_metadata tokens == runtime_metadata num_outputs_aliased output_handlers = tuple make_output_handler info runtime_metadata trace_joint info runtime_metadata output_info record_runtime_wrapper_prologue_enter - Optional AbstractContextManager None torch autograd profiler _is_profiler_enabled dynamo_config record_runtime_overhead cm = torch _C _profiler _RecordFunctionFast AOTDispatcher Runtime Wrapper Prologue cm __enter__ cm None record_runtime_wrapper_prologue_exit cm Optional AbstractContextManager None - None cm None cm __exit__ None None None simple_wraps compiled_fn runtime_wrapper args list Any Create context manager profiler cm = record_runtime_wrapper_prologue_enter stash ref each input tensor we plan use after compiled function orig_inputs = i args i i epilogue_args_idx keep_input_mutations mutated_args = args i i runtime_metadata mutated_graph_handled_indices_seen_by_autograd torch autograd graph increment_version mutated_args trace_joint args_ = list args See Note Detaching inputs never need gradients idx indices_of_inps_to_detach isinstance args_ idx torch Tensor args_ idx = args_ idx detach It s possible have trace_joint inside user specified no_grad region there nested enable_grad forces some outputs require gradients Therefore we unconditionally turn enable_grad compiled_fn execution torch autograd _force_original_view_tracking True torch enable_grad record_runtime_wrapper_prologue_exit cm all_outs = call_func_at_runtime_with_args compiled_fn args_ disable_amp=disable_amp steal_args=True When we have inference graph we run grad disabled It s possible get inference graph inputs require grad which case we want make sure autograd disabled since e g inductor will generate aten addmm out calls which autograd will complain NOTE We use _set_grad_enabled directly reduce runtime overhead grad_enabled = torch is_grad_enabled try grad_enabled torch _C _set_grad_enabled False record_runtime_wrapper_prologue_exit cm all_outs = call_func_at_runtime_with_args compiled_fn args disable_amp=disable_amp steal_args=True finally grad_enabled torch _C _set_grad_enabled True del args num_mutated_runtime_inps = runtime_metadata num_mutated_inp_runtime_indices num_intermediate_bases = runtime_metadata num_intermediate_bases assert len all_outs == num_mutated_runtime_inps + runtime_metadata num_outputs + num_intermediate_bases Step After running compiled fw apply updates mutated inputs num_mutations_to_apply = runtime_metadata num_mutated_inp_runtime_indices num_mutations_to_apply updated_inputs = all_outs num_mutations_to_apply fw_outs = all_outs num_mutations_to_apply i inpt_idx enumerate runtime_metadata mutated_inp_runtime_indices meta = runtime_metadata input_info inpt_idx meta mutates_data meta mutates_metadata continue original_inpt = orig_inputs inpt_idx updated_inpt = updated_inputs i meta mutates_storage_metadata See Note set_ Input Mutations AOTAutograd mutates_storage_metadata means our input saw x set_ y call What x also saw data metadata mutation If meta data mutation occurred after set_ then there no need copy_ data When we perform x set_ x_updated we guaranteed x_updated already has final version data metadata If data mutation occurred before set_ This case seems very difficult support TODO discuss PR decide we want tr either support detect ban trace_joint assert isinstance updated_inpt TensorAlias updated_inpt = updated_inpt alias torch no_grad original_inpt set_ updated_inpt continue meta mutates_metadata meta mutates_data trace_joint assert isinstance updated_inpt TensorAlias updated_inpt = updated_inpt alias We need grab size stride storage_offset compiled forward use mutate metadata input original_inpt as_strided_ updated_inpt size updated_inpt stride updated_inpt storage_offset meta mutates_data meta mutates_metadata original_inpt as_strided_ updated_inpt size updated_inpt stride updated_inpt storage_offset assert meta mutates_data meta is_leaf original_inpt requires_grad We can hit situation case f x x detach mul_ x + AOTAutograd will see mutation above case try apply copy_ here epilogue But x required gradients leaf then autograd will yell us trying mutate However s only possible end up scenario like above all mutations leaf input non-autograd-tracking mutations aka mutations under no_grad detached views In case we fully want hide mutation autograd so detaching ok original_inpt detach copy_ updated_inpt original_inpt copy_ updated_inpt fw_outs = all_outs Step Manually regenerate any outputs aliased inputs instead compiling them runtime_metadata num_outputs_aliased The compiled forward also returned intermediate bases We don t want them user expect_num_outputs = len output_handlers + runtime_metadata num_intermediate_bases assert len fw_outs == expect_num_outputs ret_outs = handler orig_inputs fw_outs out out handler builtins zip fw_outs output_handlers ret_outs = fw_outs runtime_metadata dynamic_outputs t o zip ret_outs runtime_metadata output_info o dynamic_dims None continue maybe_mark_dynamic_helper t o dynamic_dims runtime_metadata grad_enabled_mutation None torch _C _set_grad_enabled runtime_metadata grad_enabled_mutation ret_outs trace_joint _should_disable_saved_tensors_hooks runtime_wrapper Disabling saved tensors hooks simple_wraps runtime_wrapper _runtime_wrapper args kwargs _disable_saved_tensors_hooks runtime_wrapper args kwargs _runtime_wrapper WARNING does NOT operate TraceFn dataclass FunctionalizedRngRuntimeWrapper InductorWrapper TODO I would love get rid argument s Wrapped pretty tightly around our aot_dispatch_autograd logic Specifically tensors_saved_for_backwards_slice s value both used calculating indices setting placeholder strides which done before runtime before wrapper runs saving tensors backward which done during runtime after wrapper runs So aot_dispatch_autograd wrapper can t edit set outs without making one those two indices incorrect return_new_outs bool = True pre_compile flat_fn torch fx GraphModule flat_args aot_config fw_metadata - None config functionalize_rng_ops Update example inputs fw_compiler fake_mode = detect_fake_mode assert fake_mode None seed offset = CUDARngStateHelper get_torch_state_as_tuple fake_mode flat_args extend seed offset We clearing flat_args here because There check debug compiler end It does matter these fake tensors post_compile compiled_fn aot_config AOTConfig runtime_metadata ViewAndMutationMeta wraps compiled_fn wrapper runtime_args list Any runtime_metadata is_rng_op_functionalized Add seed offset args seed offset = CUDARngStateHelper get_torch_state_as_tuple runtime_args extend seed offset out = compiled_fn runtime_args out = _functionalized_rng_runtime_epilogue runtime_metadata out TODO won t right backward when we convert call_compiled_backward use wrapper runtime_metadata num_forward_returns out compiled_fn runtime_args wrapper Calling convention If we running functionalized RNG then outs consists user_outs rng_offset _functionalized_rng_runtime_epilogue metadata ViewAndMutationMeta outs offset_index metadata is_rng_op_functionalized assert metadata num_outputs_rng_offset == new_rng_offset = outs offset_index CUDARngStateHelper set_new_offset new_rng_offset return_new_outs user_outs = outs offset_index + outs offset_index + user_outs outs outs WARNING does NOT operate TraceFn dataclass FakifiedOutWrapper InductorWrapper out_metas list torch Tensor = field default_factory=list TracingContext fwd_output_strides Generated actually doing compile NB entry None s Tensor fwd_output_strides Optional list Optional list int = None needs_post_compile bool = True pre_compile fw_module fx GraphModule Must fw_module aot_dispatch_ _graph flat_args aot_config fw_metadata - None tracing_context = torch _guards TracingContext try_get tracing_context tracing_context fakify_first_call out_metas = n meta val n list fw_module graph nodes - args needs_post_compile = False _compute_output_meta_with_inductor_strides out = out_metas fwd_output_strides = fwd_output_strides fwd_output_strides out torch fx experimental symbolic_shapes statically_known_true i range len out isinstance out i Tensor continue strides = fwd_output_strides i fwd_output_strides best effort Inductor When output Tensor has unbacked SymInts Inductor may sometimes unable compute what output stride would If Inductor doesn t have any clear direction layout we don t have run as_strided To repro without run python test distributed test_dynamo_distributed py TestFakeDistributedSingleProc test_unbacked_symbol_splitting_no_binding strides None continue all statically_known_true s == s s s zip out i stride strides continue out i = out i as_strided out i shape strides out To called post compile set_fwd_output_strides fwd_output_strides fwd_output_strides = fwd_output_strides post_compile compiled_fn aot_config AOTConfig runtime_metadata ViewAndMutationMeta needs_post_compile assert fwd_output_strides None fakified_out = _compute_output_meta_with_inductor_strides wraps compiled_fn wrapper runtime_args nonlocal fakified_out fakified_out None out = fakified_out fakified_out = None out compiled_fn runtime_args wrapper If we don t need fakify we can just original compiled function compiled_fn This wrapper handles AOTDispatch runtime logic tensor subclasses At runtime we have compiled function knows how operate domain DenseTensor - DenseTensor But user might have passed us some tensor subclass inputs expect some subclass tensor outputs This function handles wrapping unwrapping tensor subclasses runtime dataclass AOTDispatchSubclassWrapper CompilerWrapper trace_joint bool fw_only Optional Callable Not cached only used pre_compile maybe_subclass_meta Optional SubclassMeta num_fw_outs_saved_for_bw Optional int pre_compile flat_fn TraceFn flat_args list FxValue flat_args_descs list AOTInput aot_config AOTConfig fw_metadata ViewAndMutationMeta new_flat_fn new_flat_args new_flat_args_descs subclass_meta = aot_dispatch_subclass flat_fn flat_args flat_args_descs is_joint_structure=self trace_joint meta=fw_metadata fw_only=self fw_only type ignore arg-type maybe_subclass_meta = subclass_meta new_flat_fn new_flat_args new_flat_args_descs fw_metadata post_compile compiled_fn _aot_config AOTConfig runtime_metadata ViewAndMutationMeta maybe_subclass_meta None compiled_fn subclass_metas = runtime_metadata subclass_fw_graph_out_meta wraps compiled_fn inner_fn args list Any unwrapped_args = runtime_unwrap_tensor_subclasses args subclass_metas=runtime_metadata subclass_inp_meta append_symints=True args clear expectation runtime_fn boxed fn unwrapped_outs = compiled_fn unwrapped_args wrapped_outs = wrap_tensor_subclasses unwrapped_outs subclass_metas=subclass_metas num_fw_outs_saved_for_bw=self num_fw_outs_saved_for_bw is_runtime=True included_subclass_symints=True wrapped_outs box inner_fn _boxed_call = True type ignore attr-defined inner_fn dataclass EffectTokensWrapper CompilerWrapper post_compile compiled_fn _aot_config runtime_metadata ViewAndMutationMeta num_tokens = len runtime_metadata tokens wraps compiled_fn inner_fn args list Any num_tokens Pass forward effect tokens See Note Side-Effectful Tokens AOTAutograd old_args = args args = None num_tokens args old_args clear outs = compiled_fn args Inductor cache DummyModule can None outs None None Toss out effect tokens See Note Side-Effectful Tokens AOTAutograd outs num_tokens num_tokens = outs box inner_fn _boxed_call = True type ignore attr-defined inner_fn MOTIVATION When tracing functions future execution one must careful pass same input tensor multiple times e g f x x can result graphs ONLY valid you later pass new tensor exactly same way e g f y y NB we really mean duplicate two distinct tensors alias each other different situation covered aot_dispatch_deduplicated_autograd Here two examples Suppose you have function f x y x + y If you make_fx f x x you will trace out f x y y + y Oops For most tensors x y you can compute f s gradient respect these inputs saying torch autograd grad f x y x y However x y you will trace out program gets incorrect gradients x = torch randn requires_grad=True torch autograd grad x + x x x tensor tensor In other words gradient double-counted Deduplicating arguments gives you appropriate gradient y = torch randn requires_grad=True torch autograd grad x + y x y tensor tensor HOW TO DEDUPLICATE There few strategies order preference For every duplicate argument function detach into separate leaf tensor so no longer duplicated PRO The resulting compiled graph works any configuration duplicated arguments CON It does naively work you mutate metadata inputs f x y x transpose_ y transpose_ x = torch randn f x x The ordering transposes inside f dictates whether you get This means you cannot precompute what metadata mutations should get applied each input you need assume they aren t duplicates what we do today preserve original metadata mutations exactly order so they work any duplicate configuration CON It does naively work you mutate data inputs In particular leaf tensors require grad cannot mutated makes impossible differentiate respect original base For every duplicate argument function remove so no longer part true signature PRO Implemented naively still works metadata data mutation CON The resulting compiled graph duplicate-specialized only works future calls duplicate arguments exactly same way Horribly Dynamo doesn t guard moment But even did you could still end up recompiling bunch each duplicate Our strategy do we can do otherwise erroring Dynamo s guards enough In practice seems cover everything dataclass AOTDedupeWrapper CompilerWrapper keep_arg_mask list bool = field default_factory=list add_dupe_map list int = field default_factory=list old_input_metadata list InputAliasInfo = field default_factory=list needs_post_compile bool = True NB Hot path avoid set lookups here TODO Can avoid zip here too probably remove_dupe_args args t t keep zip args keep_arg_mask keep add_dupe_args args args i i add_dupe_map pre_compile flat_fn TraceFn flat_args list FxValue flat_args_descs list AOTInput aot_config AOTConfig fw_metadata ViewAndMutationMeta - tuple TraceFn list FxValue list AOTInput ViewAndMutationMeta Use information about whether flat_fn mutates its arguments handle dupe args Strategy For any input mutated we can leafify we need remove duplicate leaf_flat_args list FxValue = leaf_flat_args_descs list AOTInput = args_set = set ok = True i a_desc enumerate zip flat_args flat_args_descs isinstance torch Tensor leaf_flat_args append leaf_flat_args_descs append a_desc args_set args_set add leaf_flat_args append leaf_flat_args_descs append a_desc fw_metadata input_info i mutates_data fw_metadata input_info i mutates_metadata leaf_flat_args append detach requires_grad_ requires_grad leaf_flat_args_descs append a_desc ok = False break ok needs_post_compile = False flat_fn leaf_flat_args leaf_flat_args_descs fw_metadata requires_subclass_dispatch leaf_flat_args fw_metadata raise RuntimeError \ Encountered duplicate inputs mutated graph least one input output graph tensor subclass This supported today You can try remove aliasing yourself workaround otherwise file issue github export path ban duplicate inputs now add later requested aot_config is_export raise RuntimeError f \ Encountered duplicated inputs mutated graph you trying export This functionality currently supported If needed please file github issue fw_metadata= str fw_metadata Strategy Duplicate specialization When we have duplicate arguments function call we need handle them specially For example we have function call f b c we need Remove duplicates get deduplicated list b c Compile our function work deduplicated list At runtime convert incoming arguments duplicates deduplicated form Pass deduplicated arguments our compiled function To do we need two helper functions - remove_dupe_args Converts b c - b c - add_dupe_args Converts b c - b c For our example b c we track - seen_args = b c maps each unique arg its first position - add_dupe_map = tells us how reconstruct original list - keep_arg_mask = True True False True tells us which args keep when deduplicating seen_args dict Tensor int = Implicitly map duped arg position list index de-duped arg position keep_arg_mask list bool = add_dupe_map list int = duped_arg_len = len flat_args j = index into deduped_flat_args t flat_args isinstance t torch Tensor t seen_args keep_arg_mask append False add_dupe_map append seen_args t continue seen_args t = j keep_arg_mask append True add_dupe_map append j j += assert len add_dupe_map == duped_arg_len f Expects add_dupe_map have length duped_arg_len got len add_dupe_map keep_arg_mask = keep_arg_mask add_dupe_map = add_dupe_map deduped_flat_args = remove_dupe_args flat_args TODO instead arbitrarily removing args might useful have record these duped perhaps mutable attribute kept arg Do someone needs deduped_flat_args_descs = remove_dupe_args flat_args_descs Update our input metadata remove duped input metadata updated_fw_metadata = remove_dupe_metadata fw_metadata keep_arg_mask add_dupe_map tracing_context = TracingContext try_get aot_config aot_autograd_arg_pos_to_source TODO voz This structure we could consider alternate structure like kept_pos dupe_arg_pos however add_dupe_map so we would need new structure there which feels like needless complexity tiny bit efficiency point dupe_arg_pos kept_pos keep_arg enumerate zip add_dupe_map keep_arg_mask keep_arg dupe_arg_source = aot_config aot_autograd_arg_pos_to_source dupe_arg_pos kept_arg_source = aot_config aot_autograd_arg_pos_to_source kept_pos tracing_context guards_context aotautograd_guards append type ignore attr-defined DuplicateInputs kept_arg_source dupe_arg_source simple_wraps flat_fn wrapped_flat_fn args FxValue - tuple list FxValue list AOTOutput outs out_descs = call_and_expect_output_descs flat_fn add_dupe_args args outs out_descs config debug_assert ref_fw_metadata = run_functionalized_fw_and_collect_metadata without_output_descs wrapped_flat_fn flat_args_descs=deduped_flat_args_descs static_input_indices=aot_config static_input_indices keep_input_mutations=fw_metadata keep_input_mutations is_train=fw_metadata is_train deduped_flat_args assert ref_fw_metadata == updated_fw_metadata f ref_metadata= str ref_fw_metadata actual_metadata= str updated_fw_metadata wrapped_flat_fn deduped_flat_args deduped_flat_args_descs updated_fw_metadata post_compile compiled_fn aot_config AOTConfig runtime_metadata ViewAndMutationMeta needs_post_compile compiled_fn wraps compiled_fn wrapped_compiled_fn args list Any deduped_args = remove_dupe_args args args clear compiled_fn deduped_args wrapped_compiled_fn _boxed_call = True type ignore attr-defined This can uncommented when we properly guard duplicates right now we must do config debug_assert wrapped_compiled_fn wraps wrapped_compiled_fn debugged_compiled_fn args Test computed remove add arg functions inverse new_args = add_dupe_args remove_dupe_args args seen dict Any None = i x y enumerate zip new_args args seen y = None assert x y format_guard_bug_msg aot_config f describe_input i aot_config would duplicate f describe_input add_dupe_map i aot_config This only error there metadata mutation both duped arguments case we need know what order metadata mutation applies You ll get correct result otherwise because graph assumes distinct inputs works you dupe inputs gradient contributions each input will get summed up appropriately TODO work out how setup assert correctly assert len seen == unique_args format_guard_bug_msg aot_config f there would unique_args distinct arguments wrapped_compiled_fn args debugged_compiled_fn _boxed_call = True type ignore attr-defined debugged_compiled_fn This layer handles situation where you have two inputs alias each other one inputs mutated We need take special care ensure mutation applied other aliases graph pre-condition AOTDedupWrapper has already run This function will theory work there duplicate args However synthetic base code path bit sub-optimal running dupe d inputs would cause us hit path more frequently dataclass AOTSyntheticBaseWrapper CompilerWrapper Currently only reason we need plumb bool because synthetic base code prohibits more cases autograd case than inference case trace_joint bool TODO refactor trace_joint needs_post_compile bool = True aliased_arg_idx_with_metadata_mutations list int = field default_factory=list pre_compile flat_fn TraceFn flat_args list FxValue flat_args_descs list AOTInput aot_config AOTConfig fw_metadata ViewAndMutationMeta - tuple Callable list FxValue list AOTInput ViewAndMutationMeta is_inference = trace_joint flat_args_with_synthetic_bases flat_args_descs_with_synthetic_bases synthetic_base_info = merge_view_inputs aot_config flat_args flat_args_descs fw_metadata input_info is_inference=is_inference Happy path we don t need synthetic bases synthetic_base_info None needs_post_compile = False flat_fn flat_args flat_args_descs fw_metadata export path ban synthetic bases now add later requested requires_subclass_dispatch flat_args fw_metadata raise RuntimeError \ Encountered aliased inputs mutated graph least one input output graph tensor subclass This supported today You can try remove aliasing yourself workaround otherwise file issue github aot_config is_export raise RuntimeError f \ Encountered aliased inputs mutated graph you trying export This functionality currently supported If needed please file github issue synthetic_base_info= str synthetic_base_info fw_metadata= str fw_metadata assert len fw_metadata input_info == len synthetic_base_info Update our forward metadata take synthetic bases into account fw_metadata_updated aliased_arg_idx_with_metadata_mutations = create_synthetic_base_metadata fw_metadata synthetic_base_info flat_args flat_args_with_synthetic_bases flat_args_descs_with_synthetic_bases Save old input args post-compile old_input_info = fw_metadata input_info aliased_arg_idx_with_metadata_mutations = aliased_arg_idx_with_metadata_mutations replay_views = config view_replay_for_aliased_outputs _unpack_synthetic_bases primals tuple Any - list Any f_args_inner = pyrefly ignore not-iterable inner_idx_or_tuple synthetic_base_info isinstance inner_idx_or_tuple int f_args_inner append primals inner_idx_or_tuple inner_base_idx view_tensor = inner_idx_or_tuple base = primals inner_base_idx view_arg = gen_alias_from_base base view_tensor view_tensor requires_grad replay_views=replay_views f_args_inner append view_arg f_args_inner simple_wraps flat_fn wrapped_flat_fn args unpacked_args = _unpack_synthetic_bases args This bit subtle The goal entire function aot_dispatch_synthetic_bases relieve downstream logic having reason about mutations inputs alias each other replacing aliased inputs synthetic base One area where breaks down bit however one those aliased inputs experienced metadata mutation We now obligated reapply metadata mutation directly user s input isn t enough apply mutations back synthetic base downstream logic The way we handle pretending those aliased inputs experience metadata mutations additional outputs user s forward function The downstream logic will just treat these user outputs alias inputs However we will manually grab them runtime here use them reapply metadata mutation user inputs them user aliased_args_with_metadata_mutations = x i x enumerate unpacked_args i aliased_arg_idx_with_metadata_mutations out out_descs = call_and_expect_output_descs flat_fn unpacked_args len aliased_args_with_metadata_mutations TODO record more detailed desc information here out aliased_args_with_metadata_mutations out_descs MetadataMutationAOTOutput i i range len aliased_arg_idx_with_metadata_mutations out out_descs config debug_assert ref_fw_metadata = run_functionalized_fw_and_collect_metadata without_output_descs wrapped_flat_fn flat_args_descs=flat_args_descs_with_synthetic_bases static_input_indices=aot_config static_input_indices keep_input_mutations=fw_metadata keep_input_mutations is_train=fw_metadata is_train flat_args_with_synthetic_bases assert ref_fw_metadata == fw_metadata_updated f ref_metadata= pprint pformat partial_flatten_asdict ref_fw_metadata f \nactual_metadata= pprint pformat partial_flatten_asdict fw_metadata_updated wrapped_flat_fn flat_args_with_synthetic_bases flat_args_descs_with_synthetic_bases fw_metadata_updated post_compile compiled_fn aot_config AOTConfig runtime_metadata ViewAndMutationMeta needs_post_compile compiled_fn is_inference = trace_joint wraps compiled_fn wrapped_compiled_fn args TODO sure seems expensive run runtime which post_compile seems imply does args_with_synthetic_bases _ synthetic_base_info = merge_view_inputs aot_config args None old_input_info is_inference=is_inference assert synthetic_base_info None aliased_args_w_metadata_mutations = args i i aliased_arg_idx_with_metadata_mutations num_aliased_args_with_metadata_mutations = len aliased_args_w_metadata_mutations args clear outs = compiled_fn args_with_synthetic_bases num_aliased_args_with_metadata_mutations This code does handle all input metadata mutations Instead only handles metadata mutations inputs converted into synthetic bases which only happens least one aliased input experienced data mutation e g f b mul_ b t_ f x view x view mutated_metadata_inps = outs -num_aliased_args_with_metadata_mutations user_outs = outs -num_aliased_args_with_metadata_mutations inp mutated_inp zip aliased_args_w_metadata_mutations mutated_metadata_inps inp as_strided_ mutated_inp size mutated_inp stride mutated_inp storage_offset user_outs outs wrapped_compiled_fn Note Handling mutations input aliases other inputs The easiest example show-case edge case here f b mul_ out = + b out b = torch ones = b view - f b In situation b happened aliased we need trace something different Suppose we had b = view - In case means ` _base b ` We need ensure aliasing relationship between b preserved We do detecting specific situation above mutate input aliases another input when we do we create synthetic base argument Then inside traced forward we regenerate b off base The complete example transformed function looks like The traced forward takes synthetic base regenerates aliased inputs views We could consider getting view-replay support here minimize as_strided_scatter ops graph traced_forward base = base as_strided b = base as_strided a_updated = mul base_updated = torch as_strided_scatter base a_updated b_updated = base_updated as_strided out = a_updated + b_updated a_updated out compiled_fn b we detect differentiable base here base = In other situations we might do either b both views off some larger differentiable base assert _base b _base _base None base = _base b both don t require gradients Create base storage assert _base None b _base None base = torch Tensor storage a_updated out = traced_forward base copy_ a_updated out This function Merges input views into synthetic base argument when any those input views mutated Returns metadata telling autograd Function how modify their arguments properly respect new calling convention The calling convention follows Any inputs originally views one another get yanked replaced synthetic base The argument list ordering goes base baseN arg argN Where ordering bases determined ordering original view args baseA will come before baseB earliest original argument coming baseA showed up earlier argument list than earliest original argument coming baseB Example given some tensors b c d call site f c view - b view - b c d Modified argument list c_base comes first because first c view came earlier arg list than first b view d still show up modified arg list b c don t- they re regenerated their bases b_base = torch Tensor b storage c_base = torch Tensor c storage f c_base b_base d merge_view_inputs aot_config AOTConfig fwd_inputs list Any This None when called runtime post_compile closure fwd_inputs_descs Optional list AOTInput mutated_input_info list InputAliasInfo The autograd case currently has more restrictions than inference case is_inference bool - tuple list Any list AOTInput Optional list Union int tuple int torch Tensor fwd_inputs_descs None fwd_inputs_descs = DummyAOTInput i i range len fwd_inputs _are_differentiable_views view view view view True view _base None view _base None False view _base view _base view _base view view view _base True False _same_dtype_views view view view dtype = view dtype False view _base None view dtype = view _base dtype False view _base None view dtype = view _base dtype False True assert len fwd_inputs == len mutated_input_info info info mutated_input_info info mutates_data Return early when there no mutations fwd_inputs fwd_inputs_descs None storage_ref_to_idx dict StorageWeakRef list int = collections defaultdict list base_args = other_args = base_args_descs = other_args_descs = i inpt source enumerate zip fwd_inputs fwd_inputs_descs isinstance inpt Tensor storage_ref = StorageWeakRef inpt untyped_storage storage_ref_to_idx storage_ref append i other_args append inpt other_args_descs append source Note Synthetic Base Info Metadata This list contains metadata tells you what i th argument inner calling convention should It s either - another int corresponding index argument list element outer calling convention - idx view_tensor where we can generate new output view_tensor _view_func old_args idx idx corresponds which synthetic base outer calling context view inner_calling_convention_meta dict int Union int tuple int torch Tensor = aliased_input_indices storage_ref_to_idx values len aliased_input_indices = any We only care about mutations affect all aliases so metadata mutations input doesn t require us do synthetic base handling mutated_input_info inpt_idx mutates_data inpt_idx aliased_input_indices other_args extend fwd_inputs curr_idx curr_idx aliased_input_indices other_args_descs extend fwd_inputs_descs curr_idx curr_idx aliased_input_indices continue Here we attempt do more complicated check detect false aliasing e g all tensors have same storage don t actually overlap In theory we could have large group tensors all share storages where only some them have overlapping memory I don t bother case now here we only bail out earlier we detect every pair tensors current group shares storage non-overlapping aliased_input_indices_no_false_sharing = compute_overlapping_inputs aot_config fwd_inputs aliased_input_indices len aliased_input_indices_no_false_sharing = other_args extend fwd_inputs curr_idx curr_idx aliased_input_indices other_args_descs extend fwd_inputs_descs curr_idx curr_idx aliased_input_indices continue We detected input mutated AND aliases another input we need replace set aliased inputs single synthetic base For now I m banning bunch cases We expect dynamo properly detect these cases error out We can fix them later These checks transitive so we don t need check every pair idx idx zip aliased_input_indices aliased_input_indices strict=False view = fwd_inputs idx view = fwd_inputs idx The inputs aliased have different differentiable bases case more complicated hopefully pretty rare Not currently handled is_inference assert _are_differentiable_views view view aot_autograd does yet handle non-differentiable view input mutations Regenerating views when reinterpreting complex real tensors seems non-trivial handling now assert _same_dtype_views view view aot_autograd does yet handle input mutations views different dtypes non_none_bases = i fwd_inputs i _base i aliased_input_indices fwd_inputs i _base None aliases_with_none_bases = fwd_inputs i i aliased_input_indices fwd_inputs i _base None synthetic_base_desc AOTInput len non_none_bases == Case where none aliases have _base we generate synthetic base without gradients generate views off We hit case when we have input tensors graph share storage do have _base field Wondering when we hit case The _base field simply says autograd knows about aliasing relationship sometimes we create tensors which aliased out same storage guaranteed disjoint In these cases we will skip setting up _base relationship performance reasons because fact tensors share same storage unobservable unless you do naughty things resize_ as_strided look storage -- we doing here One particular example optimizer steps LSTM module LSTM parameters packed into contiguous storage efficiency reasons when calling cuDNN kernels so when these parameters get passed optimizer we will find they share same storage do have _base set since they all disjoint NOTE There one case where unsafe torch Tensor storage will ALWAYS create D tensor which necessarily same shape actual base tensor came For most part fine because we always use as_strided generate original aliased inputs again If we use view-replay though could cause aliased views have incorrect sizes example_idx = aliased_input_indices example_alias = fwd_inputs example_idx Note function reused both trace time runtime At trace time we re under FakeMode so synthetic_base becomes FakeTensor synthetic_base = torch empty dtype=example_alias dtype device=example_alias device We don t actually have convenient way going storage - tensor So using set_ here we suffer some minor overhead case rare synthetic_base set_ example_alias untyped_storage synthetic_base_desc = SyntheticBaseAOTInput fwd_inputs_descs example_idx Case where all aliases require gradients have same _base i synthetic_base = non_none_bases synthetic_base_desc = ViewBaseAOTInput fwd_inputs_descs i _ other_base non_none_bases assert other_base synthetic_base aot_autograd does yet handle non-differentiable view input mutations alias aliases_with_none_bases assert alias synthetic_base aot_autograd does yet handle non-differentiable view input mutations base_args append synthetic_base base_args_descs append synthetic_base_desc curr_view_idx aliased_input_indices curr_view = fwd_inputs curr_view_idx base_idx = len base_args - We store just enough info here so we can regenerate view later Regeneration curr_view _view_func args base_idx inner_calling_convention_meta curr_view_idx = base_idx curr_view len base_args == assert len other_args == len fwd_inputs If no synthetic bases necessary just original inputs fwd_inputs fwd_inputs_descs None torch fx experimental symbolic_shapes SymIntEqByExpr make_hashable arg isinstance arg torch SymInt Since only nested SymInt objects can hashed we wrap them SymIntEqByExpr which hashable wrapper SymInts SymIntEqByExpr arg arg Otherwise The new args according updated calling convention synthetic_bases other_args Metadata telling functionalization how generate inner argument list given outer calling convention We post-process into list where meta i tells you info about i th argument inner calling convention args_to_functionalization = base_args + other_args args_to_functionalization_descs = base_args_descs + other_args_descs Map each argument into its old index There may some repeated arguments so we collect their indices list arg_to_old_idx_map = collections defaultdict list i arg enumerate fwd_inputs arg_to_old_idx_map make_hashable arg append i Reverse list each argument so we can easily pop them one-after-the-other order hashable_arg arg_to_old_idx_map arg_to_old_idx_map hashable_arg = list reversed arg_to_old_idx_map hashable_arg i other_arg enumerate other_args new_idx = len base_args + i old_idx = arg_to_old_idx_map make_hashable other_arg pop inner_calling_convention_meta old_idx = new_idx post process into list post_processed_calling_convention_meta list Union int tuple int torch Tensor = - _ range len inner_calling_convention_meta k v inner_calling_convention_meta items post_processed_calling_convention_meta k = v Quick assert every argument inner calling convention should accounted x post_processed_calling_convention_meta assert x = - args_to_functionalization args_to_functionalization_descs post_processed_calling_convention_meta Note Backward graph lazy lowering After AOTDispatch traces backward graphs requiring autograd we will lower graph lazily unless we suspect inductor might specialize insert additional guards When we do lazy lowering we stash AOT backward graph bw_module Lowering passes performed deepcopy bw_module due compatibility compiled autograd See https github com pytorch pytorch pull #discussion_r dataclass AutogradLazyBackwardCompileInfo bw_module Callable placeholder_list list Any saved_context Optional TracingContext saved_compile_context Optional CompileContext On AOT Autograd cache hit we already have lowered backward so there usually no need keep information around new lazy compilation Except compiled autograd which wants retrace backward into larger graph needs graph module do so dataclass CachedAutogradLazyBackwardCompileInfo bw_module_fn Callable _raise_if_functorch_active ideal prevent user seeing nasty traceback - See stack = torch _C _functorch peek_interpreter_stack torch _check stack None lambda It looks like you re trying call compiled backward function within vmap grad vjp which isn t supported Try wrapping vmap inside torch compile skip compiling backward function NOTE function must torch _dynamo allow_in_graph-able Non tensor symnode inputs must constants _backward_prologue_functional ctx_saved_tensors ctx_symints metadata maybe_subclass_metadata flat_args Calling convention we expect grad_out passed backward - every output fw does alias input graph intermediate - every updated_input generated fw does alias input aka only data-mutations - every graph intermediate we need use generate output later The other outputs autograd Function forward do show up backward include - outputs alias inputs graph intermediates - updated inputs due metadata-only mutations We need them forward ensure they all do get gradients backward we filter them out here before passing remaining grad_outputs into compiled backward _raise_if_functorch_active num_intermediate_bases = metadata num_intermediate_bases num_mutated_runtime_inps = metadata num_mutated_inp_runtime_indices expected_grad_outs = metadata num_outputs + num_mutated_runtime_inps + num_intermediate_bases deterministic = metadata deterministic global_deterministic = torch are_deterministic_algorithms_enabled deterministic None torch _check deterministic global_deterministic lambda This compiled backward function being run torch use_deterministic_algorithms True previously generated during forward function while torch use_deterministic_algorithms False set assert len flat_args == expected_grad_outs out_info = metadata output_info inp_tangents out_tangents intermediate_base_tangents = flat_args num_mutated_runtime_inps flat_args num_mutated_runtime_inps num_mutated_runtime_inps + metadata num_outputs flat_args num_mutated_runtime_inps + metadata num_outputs input_info contains info every input But backward we only given grad outputs every mutated input We then need filter out grad outputs correspond metadata-only mutations don t require grad input_info = metadata input_info inp_tangents_filtered = x x info_idx zip inp_tangents metadata mutated_inp_runtime_indices input_info info_idx mutates_data input_info info_idx requires_grad We also need filter out grad outputs correspond outputs aliasing inputs intermediates out_tangents_filtered = x x info zip out_tangents out_info info output_type OutputType non_alias OutputType unsafe_view_alias OutputType custom_function_view issubclass info raw_type torch Tensor info requires_grad intermediate bases always require gradients always participate backward graph flat_bw_args_with_grads = inp_tangents_filtered out_tangents_filtered intermediate_base_tangents num_flat_bw_args_with_grads = len flat_bw_args_with_grads sanity asserts metadata_only_inps = x x info_idx zip inp_tangents mutated_inp_indices input_info info_idx mutates_data aliased_outputs = x x info zip out_tangents out_info info output_type = OutputType non_alias assert all x None x metadata_only_inps assert all x None x aliased_outputs TODO replace FunctionalizedRngRuntimeWrapper rng_args = metadata is_rng_op_functionalized Add seed offset args rng_args = CUDARngStateHelper get_torch_state_as_tuple bw_tokens = None metadata num_backward_tokens - note donated buffer logic requires ctx symints ctx saved_tensors showing up first bw output order Every dereference ctx saved_tensors incurs saved_tensors_hooks calls There tests count these calls saving var num_ctx_saved_tensors = len ctx_saved_tensors all_args = ctx_symints ctx_saved_tensors flat_bw_args_with_grads bw_tokens rng_args del ctx_saved_tensors Note AOTAutograd Backward Guards During AOTDispatch we eagerly create trace out joint fw-bw graph Doing so requires us guess about some metadata our grad_outputs In particular output forward plain tensor subclass its corresponding grad_output backward may may plain tensor subclass The main cases If output plain tensor its grad_out will also plain tensor unless output used some subclass compute later forward graph which will cause its grad_output become subclass If output subclass its grad_out will also subclass unless output forward did actually participate gradient computation which case autograd will insert plain tensor zeros grad_output We could avoid case ` torch autograd Function set_materialize_grads ` although turned today AOTAutgrad would require more work Today we make guess subclass-ness based above examples hard-error backward we guessed wrong In future we should add backward guards would allow us properly handle case instead erroring we would need retrace backward graph since we might produce entirely different trace our grad_outputs subclass del flat_bw_args_with_grads tangents_start_idx = len all_args - num_flat_bw_args_with_grads - len rng_args - len bw_tokens assert tangents_start_idx == len ctx_symints + num_ctx_saved_tensors tangents_end_idx = len all_args - len rng_args - len bw_tokens TODO figure out how refactor backward properly so I can use aot_dispatch_subclass_wrapper here maybe_subclass_metadata None tangents = all_args tangents_start_idx tangents_end_idx len tangents = len metadata subclass_tangent_meta raise RuntimeError The grad inputs should same number forward output tangents flat_processed_tangents = list itertools chain from_iterable AOTDispatchAutograd process_runtime_tangent t m t m zip tangents metadata subclass_tangent_meta all_args = runtime_unwrap_tensor_subclasses all_args tangents_start_idx SymInts inputs backward graph already included all_args list Any symints coming tensor subclasses should always come primals so they will show up extra arguments forward graph they will saved activation backward graph append_symints=False + flat_processed_tangents + runtime_unwrap_tensor_subclasses all_args tangents_end_idx append_symints=False all_args = AOTDispatchAutograd process_runtime_tangent t metadata subclass_tangent_meta i - tangents_start_idx tangents_start_idx = i tangents_end_idx t i t enumerate all_args Backward forward inputs mutations supported double backward torch is_grad_enabled metadata indices_of_inputs_that_requires_grad_with_mutations_in_bw raise RuntimeError aot_autograd does support input mutations requires_grad backward create_graph=True all_args initialize_rng_states num_rng int graphsafe_idx int fwd_rng_states list torch Generator bwd_rng_states list torch Generator Initialize cudagraph safe rng states Initialization rng states should have few properties - initialization each rng state should independent - initialization should deterministic - initialization should based off current rng state so independent graphs do have equal rng behavior We defer initialization rng states until runtime because compilation wrapped preserve_rng_states Seed initialization should advance rng states so consecutive compilations do give equal randomness torch utils _python_dispatch _disable_current_modes seeds = torch randint torch iinfo torch int max num_rng device= cpu fwd_rng_states extend torch cuda default_generators graphsafe_idx clone_state manual_seed int seeds i i range num_rng bwd_rng_states extend torch cuda default_generators graphsafe_idx clone_state manual_seed int seeds i i range num_rng NOTE function must torch _dynamo allow_in_graph-able Non tensor symnode inputs must constants _backward_epilogue_functional metadata maybe_subclass_metadata out make_subclass_override=None Toss out backward output tokens num_bw_tokens = metadata num_backward_tokens num_bw_tokens out = out -num_bw_tokens TODO replace FunctionalizedRngRuntimeWrapper post_compile out = FunctionalizedRngRuntimeWrapper _functionalized_rng_runtime_epilogue metadata out offset_index=len out - out = tuple out TODO figure out how refactor backward properly so I can use aot_dispatch_subclass_wrapper here maybe_subclass_metadata None assert maybe_subclass_metadata grad_input_metas None outs_wrapped = wrap_tensor_subclasses out subclass_metas=maybe_subclass_metadata grad_input_metas included_subclass_symints=True is_runtime=True make_subclass_override=make_subclass_override outs_wrapped out coerce_to_expected_memory_format x torch Tensor memory_format MemoryFormatMeta memory_format memory_format None Coerce torch memory_format x is_contiguous memory_format=memory_format memory_format x = x contiguous memory_format=memory_format memory_format x expected_size = memory_format size assert expected_size None expected_stride = memory_format stride assert expected_stride None Expected size stride static ints ok use == compare runtime tensor strides shapes x shape == expected_size x stride == expected_stride Runtime tangent size stride same expected no need coerce x Empty_strided creates raw Tensor We guaranteed only raw Tensors has expected size stride Subclasses have only expected memory_format restrided = torch empty_strided size=expected_size stride=expected_stride dtype=x dtype device=x device layout=x layout requires_grad=x requires_grad restrided copy_ x restrided contextlib contextmanager _disable_saved_tensors_hooks error_message = Saved tensors hooks specialized GraphModules In case aot_autograd inlines them forward backward graph disables them during runtime aot_autograd compiled region If you see error means there some unexpected push pop manipulation during aot_autograd compiled region runtime Compilation different hooks must result recompilation fail_if_non_empty = False maybe_prev_message = None try maybe_prev_message = torch _C _autograd _saved_tensors_hooks_get_disabled_error_message torch _C _autograd _saved_tensors_hooks_disable error_message fail_if_non_empty yield finally maybe_prev_message None torch _C _autograd _saved_tensors_hooks_enable torch _C _autograd _saved_tensors_hooks_disable maybe_prev_message fail_if_non_empty dataclass SerializableCompiledFunction Represents result AOTDispatch after calling inner compiler can serialized compiled_fn Callable serialize_fn Callable __init__ compiled_fn Callable serialize_fn Callable compiled_fn = compiled_fn serialize_fn = serialize_fn Equivalent functools wraps functools update_wrapper compiled_fn assigned= __doc__ __annotations__ __type_params__ serialize - Any serialize_fn __call__ args kwargs compiled_fn args kwargs This wrapped just namespacing purposes No need make into actual CompilerWrapper because doesn t fit abstract cleanly AOTDispatchAutograd staticmethod process_runtime_tangent x meta Union PlainTensorMeta SubclassCreationMeta isinstance x torch Tensor x x isinstance x FakeTensor assert meta memory_format x = coerce_to_expected_memory_format x meta memory_format x x expected_type Optional type = torch Tensor expected_meta = None isinstance meta SubclassCreationMeta expected_type = meta original_subclass_type expected_meta = meta meta runtime_type = type x When we re inside compiled autograd s AOTDispatcher step regular Tensors look like FunctionalTensors Tensor subclasses still look like Tensor subclasses though isinstance x torch _subclasses functional_tensor FunctionalTensor runtime_type = torch Tensor runtime_meta = None runtime_subclass_keys Sequence str = is_traceable_wrapper_subclass x runtime_subclass_keys runtime_meta = x __tensor_flatten__ maybe_coerce x same_type bool = expected_type == runtime_type same_meta bool = expected_meta == runtime_meta same_type same_meta x hasattr x __coerce_same_metadata_as_tangent__ None same_type Backward Compatibility some Subclass impls can have original -arg function x __coerce_same_metadata_as_tangent__ expected_meta x __coerce_same_metadata_as_tangent__ expected_meta expected_type Coerce expected type metadata orig_x = x x = maybe_coerce x x None raise RuntimeError f During backward we encountered tensor subclass where we guessed its metadata incorrectly Expected metadata str expected_meta expected type str expected_type Runtime metadata str runtime_meta runtime type str runtime_type shape str orig_x shape To fix your tensor subclass must implement dunder method __force_to_same_metadata__ Coerce expected memory format assert meta memory_format x = coerce_to_expected_memory_format x meta memory_format is_traceable_wrapper_subclass x x x assert isinstance meta SubclassCreationMeta orig_x x runtime_subclass_keys = x __tensor_flatten__ assert len meta attrs == len runtime_subclass_keys leaves = attr attr_meta meta attrs items elem = getattr x attr new_elem elem_leaves = AOTDispatchAutograd process_runtime_tangent elem attr_meta new_elem elem setattr x attr new_elem leaves extend elem_leaves x leaves staticmethod post_compile compiled_fw_func fw_module after compilation + wrappers compiled_bw_func bw_module after compilation + wrappers maybe_subclass_meta Optional SubclassMeta num_symints_saved_for_bw_ int backward_state_indices list int disable_amp bool indices_of_inps_to_detach list int lazy_backward_info Optional Union AutogradLazyBackwardCompileInfo CachedAutogradLazyBackwardCompileInfo aot_config AOTConfig fw_metadata ViewAndMutationMeta runtime metadata try_save_cache_entry Optional Callable Serialization function For additional context see Note CUDA Graph Safe RNG Functionalization Each pair forward backward rng states must equal prior its invocation any iteration forward backward Because they initialized equal computing same rng op running forward then backward advances them same amount keeps them equal However user may invoke multiple forwards then backwards such they sync Initially we have fwd_state == bwd_state Lets say we run fwd fwd_state - fwd_state fwd fwd_state - fwd_state fwd fwd_state - fwd_state If we now invoke bwd we need update bwd_state equal rng observed fwd we save rng_state fwd_state forward because we detect current backward state therefore would accessible we do save Similarly we going update backward state new value there pending forwards which needs its current state we will save Within autograd context we keep track curr iteration so backward we know what generator state must before backward run num_rng = fw_metadata num_graphsafe_rng_states graphsafe_idx = fw_metadata graphsafe_rng_state_index fwd_rng_states list torch Generator = bwd_rng_states list torch Generator = curr_fwd_iter = itertools count backward_state_position = pending_forwards set int = set saved_backward_tensor_states dict int list torch Tensor = CompiledFunction torch autograd Function compiled_fw = compiled_fw_func compiled_bw = compiled_bw_func metadata ViewAndMutationMeta = fw_metadata type ignore assignment maybe_subclass_metadata Optional SubclassMeta = maybe_subclass_meta num_symints_saved_for_bw = num_symints_saved_for_bw_ _aot_id = aot_config aot_id _lazy_backward_info = lazy_backward_info staticmethod _compiled_autograd_key ctx ctx _autograd_function_id ctx symints staticmethod pyrefly ignore bad-override forward ctx deduped_flat_tensor_args args = deduped_flat_tensor_args backward_state_indices bw_state = args backward_state_indices assert isinstance bw_state BackwardState ctx _compiled_autograd_backward_state = bw_state num_rng len fwd_rng_states == assert graphsafe_idx None initialize_rng_states num_rng graphsafe_idx fwd_rng_states bwd_rng_states _curr_iter = next curr_fwd_iter ctx _curr_iter = _curr_iter state contained backward we need save when its backward pass happens _curr_iter = backward_state_position saved_backward_tensor_states _curr_iter = rng_state get_state rng_state fwd_rng_states pending_forwards add _curr_iter args = args fwd_rng_states There pretty complicated calling convention around what compiled fw returns The full list outputs their relative order tokens mutated_inputs fw_outs fw_intermediate_bases saved_tensors saved_symints - Note synthetic bases case mutated_inputs will correspond updated version original view synthetic base - Note donated buffer logic requires saved_tensors saved_symints showing up last fw output order fw_outs = call_func_at_runtime_with_args CompiledFunction compiled_fw pyrefly ignore bad-argument-type args disable_amp=disable_amp num_outputs = CompiledFunction metadata num_outputs num_outputs_aliased = CompiledFunction metadata num_outputs_aliased num_mutated_runtime_inps = CompiledFunction metadata num_mutated_inp_runtime_indices num_forward_returns = CompiledFunction metadata num_forward_returns Partitioners must put symint arguments end separate tensor arguments tensors_saved_for_backwards = fw_outs CompiledFunction metadata tensors_saved_for_backwards_slice assert all isinstance x torch Tensor x tensors_saved_for_backwards mark_dynamic_activations activations list torch Tensor idx dims CompiledFunction metadata dynamic_saved_tensors_idxs items maybe_mark_dynamic_helper activations idx dims activations See Note Detaching saved tensors AOTAutograd ctx save_for_backward mark_dynamic_activations x detach x _is_view x x tensors_saved_for_backwards symint_outs = fw_outs CompiledFunction metadata symints_saved_for_backwards_slice assert all isinstance x int float torch SymInt torch SymFloat x symint_outs str type x x symint_outs ctx symints = symint_outs raw_returns = fw_outs num_forward_returns Wrap all autograd Function forward outputs aliases so autograd Function doesn t treat them tensors num_mutated_runtime_inps i idx enumerate CompiledFunction metadata mutated_inp_runtime_indices We could make faster only looping over inputs metadata-only mutations instead looping over inputs either data metadata mutations there shouldn t many info = CompiledFunction metadata input_info idx info mutates_metadata info mutates_data raw_return_idx = i raw_returns raw_return_idx = TensorAlias raw_returns raw_return_idx config debug_assert user_mutated_inputs_raw = raw_returns num_mutated_runtime_inps mut_inp_infos = x x CompiledFunction metadata input_info x mutates_data x mutates_metadata assert len user_mutated_inputs_raw == len mut_inp_infos CompiledFunction metadata num_unsafe_view_outputs idx CompiledFunction metadata unsafe_view_out_indices raw_return_idx = num_mutated_runtime_inps + idx o = raw_returns raw_return_idx raw_returns raw_return_idx = torch ops aten _unsafe_view o o shape num_outputs_aliased idx CompiledFunction metadata aliased_out_indices raw_return_idx = num_mutated_runtime_inps + idx raw_returns raw_return_idx = TensorAlias raw_returns raw_return_idx config debug_assert intermediates_raw = raw_returns num_mutated_runtime_inps + num_outputs assert any isinstance x TensorAlias x intermediates_raw invariant intermediate bases always require gradients so we don t have consider marking them non-differentiable raw_returns_not_including_intermediate_bases = raw_returns num_mutated_runtime_inps + num_outputs raw_returns_meta = x x CompiledFunction metadata input_info x mutation_type == MutationType MUTATED_OUT_GRAPH + CompiledFunction metadata output_info fw_outs_not_requiring_grad = x i x enumerate raw_returns_not_including_intermediate_bases isinstance x torch Tensor raw_returns_meta i requires_grad ctx mark_non_differentiable fw_outs_not_requiring_grad ctx _materialize_non_diff_grads = False tuple raw_returns staticmethod backward ctx flat_args all_args = _backward_prologue_functional ctx saved_tensors ctx symints CompiledFunction metadata CompiledFunction maybe_subclass_metadata flat_args num_rng nonlocal backward_state_position bwd_rng_states curr_backward_iter = ctx _curr_iter retain_graph = torch _C _autograd _get_current_graph_task_keep_graph Save current state we have pending forward needs state state may needed again because retain graph backward_state_position pending_forwards backward_state_position saved_backward_tensor_states backward_state_position = curr_backward_iter retain_graph saved_backward_tensor_states backward_state_position = rng_state get_state rng_state bwd_rng_states Restore saved states needed curr_backward_iter saved_backward_tensor_states backward_state_position = curr_backward_iter bwd_state saved_state zip bwd_rng_states saved_backward_tensor_states curr_backward_iter bwd_state set_state saved_state retain_graph del saved_backward_tensor_states curr_backward_iter assert backward_state_position == curr_backward_iter backward_state_position = curr_backward_iter + retain_graph pending_forwards remove curr_backward_iter all_args extend bwd_rng_states impl_fn double_ctx=None out = CompiledFunction _backward_impl ctx all_args _backward_epilogue_functional CompiledFunction metadata CompiledFunction maybe_subclass_metadata out needs_grad = torch is_grad_enabled any t requires_grad t all_args isinstance t torch Tensor needs_grad double backward CompiledFunction _double_backward ctx impl_fn all_args impl_fn staticmethod _double_backward ctx impl_fn all_args Ensure graph connected error double backward performed See comment why once_differentiable sufficient https github com pytorch pytorch pull files#r CompiledFunctionBackward torch autograd Function CompiledFunctionBackward yet supported dynamo skipfiles _aot_id = aot_config aot_id staticmethod pyrefly ignore bad-override forward double_ctx unused_args impl_fn double_ctx staticmethod backward double_ctx args raise RuntimeError torch compile aot_autograd does currently support double backward CompiledFunctionBackward _compiled_autograd_key = type ignore method-assign CompiledFunction _compiled_autograd_key CompiledFunctionBackward apply all_args staticmethod _backward_impl ctx all_args torch _inductor async_compile async_compile_pool_manager compiled autograd reimplements function proxy_call_aot_backward assert backward_state_indices BackwardState requires CompiledAutograd ctx maybe_clear_saved_tensors saved_tensors_use_once = torch _C _autograd _get_current_graph_task_keep_graph CompiledFunction compiled_bw None assert lazy_backward_info None assert isinstance lazy_backward_info AutogradLazyBackwardCompileInfo hasattr lazy_backward_info saved_context lazy_backward_info saved_context None assert isinstance lazy_backward_info saved_context TracingContext ddp_ctx = lazy_backward_info saved_context ddp_optimizer_ctx ddp_ctx None assert ddp_ctx curr_bucket = f expected same fw bw compiles found bucket ddp_ctx curr_bucket curr_fw_meta = ddp_ctx metadata_per_bucket ddp_ctx curr_bucket Note DDPOptimizer fw_metadata When using DDPOptimizer we have single dynamo graph TracingContext multiple AOTDispatcher graph One consequence there will multiple fw_metadata objects one per AOT graph which we stash fw_metadata TracingContext Normally what happens we compile AOT graphs N we clobber fw_metadata graph i- when we start running AOT graph i Ordinarily fine because inductor no longer needs metadata graph i- However problem lazy compilation backward During backward compilation we compile backward lazily backward runtime meaning we will first compile backward graph N N- We need ensure time inductor compiles bw graph N- can access corresponding fw_metadta graph N- We do stashing DDPOptimizerContext which tracks - metadata all N graphs - graph we currently compiling our DDPOptimizer region ddp_ctx curr_bucket -= lazy_backward_info saved_context fw_metadata = curr_fw_meta saved_tensors_use_once fw_metadata bw_donated_idxs = Update bw_donated_idxs using lazy_backward_info ` aot_dispatch_autograd ` hasattr lazy_backward_info saved_context hasattr lazy_backward_info saved_context fw_metadata hasattr lazy_backward_info saved_context fw_metadata type ignore union-attr bw_donated_idxs lazy_backward_info saved_context fw_metadata bw_donated_idxs = type ignore union-attr bw_module = lazy_backward_info bw_module placeholder_list = lazy_backward_info placeholder_list saved_context = lazy_backward_info saved_context saved_compile_context = lazy_backward_info saved_compile_context context = torch _C _DisableAutocast disable_amp nullcontext metrics_context = get_metrics_context tracing saved_context compile_context saved_compile_context async_compile_pool_manager context track_graph_compiling aot_config backward metrics_context dynamo_timed backward _backward_impl phase_name= entire_backward_compile log_pt _compile_event=True dynamo_compile_column_us= backward_cumulative_compile_time_us log_waitcounter=True waitcounter_name_override= entire_backward_compile callback_handler install_callbacks CallbackTrigger LAZY_BACKWARD str CompileContext current_compile_id CompileEventLogger compilation_metric is_forward=False See Note Backward graph lazy lowering CompiledFunction compiled_bw = aot_config bw_compiler copy deepcopy bw_module placeholder_list Maybe save cache entry try_save_cache_entry None try_save_cache_entry CompiledFunction compiled_bw bw_module fw_metadata aot_config torch _functorch config donated_buffer saved_tensors_use_once fw_metadata bw_donated_idxs = torch _check False lambda This backward function compiled non-empty donated buffers which requires create_graph=False retain_graph=False Please keep backward create_graph=False retain_graph=False across all backward function calls set torch _functorch config donated_buffer=False disable donated buffer out = call_func_at_runtime_with_args CompiledFunction compiled_bw all_args steal_args=True disable_amp=disable_amp out compiled_function = RuntimeWrapper indices_of_inps_to_detach=indices_of_inps_to_detach trace_joint=True disable_amp=disable_amp post_compile CompiledFunction apply aot_config runtime_metadata=fw_metadata compiled_function dataclass DebugAssertWrapper CompilerWrapper flat_requires_grad list Optional bool = field default_factory=list post_compile compiled_fn aot_config AOTConfig runtime_metadata ViewAndMutationMeta wraps compiled_fn debug_compiled_function args list Any TODO Check aliasing relationships TODO Check strides metadata mutation NB ideally logic factored out function you move these debug checks there Check requires grad Bad case when we compiled requires_grad = False input requires_grad = True vice versa OK we compute gradient then throw away when hits input i enumerate args can_require_grad = flat_requires_grad i can_require_grad None assert isinstance Tensor can_require_grad assert requires_grad format_guard_bug_msg aot_config f describe_input i aot_config would require grad compiled_fn args debug_compiled_function pre_compile wrappers list CompilerWrapper flat_fn TraceFn flat_args list FxValue flat_args_descs list AOTInput aot_config AOTConfig fw_metadata ViewAndMutationMeta - tuple TraceFn list FxValue list AOTInput ViewAndMutationMeta Runs sequence wrappers given function arguments Mutates wrappers place wrapper wrappers flat_fn flat_args flat_args_descs fw_metadata = wrapper pre_compile flat_fn flat_args flat_args_descs aot_config fw_metadata=fw_metadata flat_fn flat_args flat_args_descs fw_metadata post_compile wrappers list CompilerWrapper compiled_fn Callable aot_config AOTConfig runtime_metadata ViewAndMutationMeta - tuple Callable ViewAndMutationMeta Runs sequence wrappers given function Should called after pre_compile wrapper reversed wrappers compiled_fn = wrapper post_compile compiled_fn aot_config runtime_metadata=runtime_metadata compiled_fn runtime_metadata make_runtime_safe fw_metadata ViewAndMutationMeta maybe_subclass_meta Optional SubclassMeta Calls make_runtime_safe all ViewAndMutationMetas Modifies both arguments Allows ViewAndMutationMetas safely cached AOTAutogradCache fw_metadata make_runtime_safe maybe_subclass_meta None maybe_subclass_meta fw_metadata make_runtime_safe maybe_subclass_meta grad_input_metas meta maybe_subclass_meta grad_input_metas isinstance meta SubclassCreationMeta meta make_runtime_safe