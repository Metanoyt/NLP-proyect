mypy allow-untyped-defs copy json re weakref collections defaultdict typing Any torch torch nn torch _guards detect_fake_mode torch autograd graph register_multi_grad_hook torch distributed _tools mod_tracker ModTracker torch distributed tensor _api DTensor torch nn modules module register_module_forward_hook register_module_forward_pre_hook register_module_full_backward_pre_hook torch utils _python_dispatch TorchDispatchMode torch utils _pytree tree_flatten __all__ = CommDebugMode funcol_native = torch ops _c d_functional funcol_py = torch ops c d_functional funcol_autograd = torch ops _c d_functional_autograd c d_ops = torch ops c d NATIVE_TO_PY_MAPPING = funcol_native all_gather_into_tensor funcol_py all_gather_into_tensor funcol_native all_gather_into_tensor_coalesced funcol_py all_gather_into_tensor_coalesced funcol_native all_reduce funcol_py all_reduce funcol_native all_reduce_coalesced funcol_py all_reduce_coalesced funcol_native all_to_all_single funcol_py all_to_all_single funcol_native broadcast funcol_py broadcast funcol_native reduce_scatter_tensor funcol_py reduce_scatter_tensor funcol_native reduce_scatter_tensor_coalesced funcol_py reduce_scatter_tensor_coalesced functional ops funcol_autograd all_to_all_single funcol_py all_to_all_single c d_collective_ops = c d_ops _allgather_base_ c d_ops _reduce_scatter_base_ c d_ops allgather_ c d_ops allgather_coalesced_ c d_ops allgather_into_tensor_coalesced_ c d_ops allreduce_ c d_ops allreduce_coalesced_ c d_ops alltoall_ c d_ops alltoall_base_ c d_ops broadcast_ c d_ops gather_ c d_ops scatter_ c d_ops reduce_ c d_ops reduce_scatter_ c d_ops reduce_scatter_tensor_coalesced_ trivial_ops = aten detach default aten t default aten view default aten _to_copy default aten as_strided default aten transpose int _CommModeModuleTracker ModTracker Inherits ModuleTracker expands its functionality track parameters sharding information model module-level __init__ super __init__ module_helper_dict = module_parameters_dict = module_parents_dict = register_forward_hook_handles = parent_dict = parent_list = sharding_dict = activation_checkpointing = False name = _fw_set_module_hook mod input output Updates current module after module finishes running all other hooks resolved is_bw activation_checkpointing = True activation_checkpointing = False activation_checkpointing module no longer parent next modules parent_list pop set current module previous parent module name = parent_list - _fw_pre_hook mod input This function called before forward pass module It collects parameters sharding information module stores dictionary is_bw activation_checkpointing = True activation_checkpointing = False name = super _get_mod_name mod w_mod = weakref ref mod adds current sub-module module tracker parent super _get_append_fn w_mod name False args _ = tree_flatten input tensors = args isinstance torch Tensor requires_grad is_bw tensors register_multi_grad_hook tensors super _get_pop_fn w_mod name True activation_checkpointing contains information about module ordering depth module tree name module_helper_dict module_helper_dict name = module_helper_dict name module_type = str type mod replace replace module_helper_dict name depth = len parents - param_name param mod named_parameters recurse=False name module_parameters_dict module_parameters_dict name = module_parameters_dict name param_name = param data isinstance param data DTensor key_name = name + + param_name sharding_dict key_name = param data placements parameters module_helper_dict name module_helper_dict name parameters = module_helper_dict name parameters param_name = str param data placements used store module s parents ensure correctness backward pass checkpointing name module_parents_dict module_parents_dict name = copy deepcopy parents used create parent-child module associations json dumps parent = parent_list - parent parent_dict parent_dict parent = parent_dict parent append name parent_list append name register_forward_hook_handles name = mod register_forward_hook _fw_set_module_hook _fw_post_hook mod input output This function called when forward pass module called It updates module tracker removes module parent data super _fw_post_hook mod input output _bw_hook mod output This function called when backward pass module called It updates current module backward passes activation_checkpointing = False name = super _get_mod_name mod __enter__ activation_checkpointing = False module_parameters_dict clear sharding_dict clear parent_dict clear parent_list = Global module_helper_dict clear module_helper_dict Global = depth module_parents_dict clear module_parents_dict Global = set _fw_pre_handle = register_module_forward_pre_hook _fw_pre_hook _fw_post_handle = register_module_forward_hook _fw_post_hook register_forward_hook_handles clear _bw_handle = register_module_full_backward_pre_hook _bw_hook name = Global __exit__ args super __exit__ args _bw_handle remove removes all forward_hook handles added pre-hook handle register_forward_hook_handles values handle remove print_paramater_info print module_parameters_dict print_sharding_info key value sharding_dict items print key + + str value CommDebugMode TorchDispatchMode ` CommDebugMode ` context manager counts number functional collectives within its context It does using ` ` TorchDispatchMode ` ` note Not all collectives supported yet Example usage code-block python mod = comm_mode = CommDebugMode comm_mode mod sum backward print comm_mode get_comm_counts __init__ comm_counts dict Any int = defaultdict int comm_module_counts = comm_module_operation_counts = comm_registry = set native_op py_op NATIVE_TO_PY_MAPPING items comm_registry add native_op comm_registry add py_op comm_registry add torch ops _dtensor shard_dim_alltoall advanced_module_tracker = _CommModeModuleTracker generate_json_dump file_name= comm_mode_log json noise_level= Creates json file used build browser visual prints module-level collective counts prints dTensor operations included trivial operations prints operations included trivial operations prints all operations include_DTensor_ops include_module_data include_ops include_trivial_ops = _set_noise_parameters noise_level recursively builds json data add_json_information json_dict fqn json_dict fqn = fqn json_dict module_type = json_dict parameters = json_dict children = json_dict collectives_forward = json_dict collectives_backward = json_dict operations_forward = json_dict operations_backward = adds module layer type parameters their sharding module_type advanced_module_tracker module_helper_dict fqn include_module_data json_dict module_type = advanced_module_tracker module_helper_dict fqn module_type parameters advanced_module_tracker module_helper_dict fqn param_name placement advanced_module_tracker module_helper_dict fqn parameters items json_dict parameters append param_name placement adds module collective information fqn comm_module_counts collective count comm_module_counts fqn forward items json_dict collectives_forward append str collective count collective count comm_module_counts fqn backward items json_dict collectives_backward append str collective count adds module operation information forward_operations = backward_operations = checkpointing_operations = only get operations minimum operation noise level set true include_DTensor_ops fqn comm_module_operation_counts forward_operations backward_operations checkpointing_operations = _get_operations_list comm_module_operation_counts fqn remove all operations who don t have DTensor inputs include_ops forward_operations = op op forward_operations len op input_sharding backward_operations = op op backward_operations len op input_sharding checkpointing_operations = op op checkpointing_operations len op input_sharding remove all operations trivial operations set include_trivial_ops forward_operations = op op forward_operations str op name trivial_ops backward_operations = op op backward_operations str op name trivial_ops checkpointing_operations = op op checkpointing_operations str op name trivial_ops converts operation information into string format json dumps forward_operations = copy deepcopy forward_operations op forward_operations op name = str op name i range len op input_sharding op input_sharding i = str op input_sharding i op input_shape i = str op input_shape i backward_operations = copy deepcopy backward_operations op backward_operations op name = str op name i range len op input_sharding op input_sharding i = str op input_sharding i op input_shape i = str op input_shape i checkpointing_operations = copy deepcopy checkpointing_operations op checkpointing_operations op name = str op name i range len op input_sharding op input_sharding i = str op input_sharding i op input_shape i = str op input_shape i json_dict operations_forward = forward_operations json_dict operations_backward = backward_operations json_dict operations_checkpointing = checkpointing_operations fqn advanced_module_tracker parent_dict json_dict recursively adds module s children ele advanced_module_tracker parent_dict fqn json_dict children append add_json_information ele json_dict json_dict dict str Any = add_json_information json_dict Global converts dictionary into json file open file_name w json_file json dump json_dict json_file indent= generate_comm_debug_tracing_table noise_level= Generates detailed table displaying operations collective tracing information module level Amount information dependent noise_level prints module-level collective counts prints dTensor operations included trivial operations module information prints operations included trivial operations prints all operations include_DTensor_ops include_module_data include_ops include_trivial_ops = _set_noise_parameters noise_level table = fqn advanced_module_tracker module_helper_dict setting up indentations table formatting indent = advanced_module_tracker module_helper_dict fqn depth table += f indent fqn \n include_module_data module_type advanced_module_tracker module_helper_dict fqn module_type = advanced_module_tracker module_helper_dict fqn module_type table += f indent module type module_type \n parameters advanced_module_tracker module_helper_dict fqn table += f indent Parameter List\n param_name placement advanced_module_tracker module_helper_dict fqn parameters items table += f indent param_name placement \n indent += collective_indent = advanced_module_tracker module_helper_dict fqn depth + operation_indent = advanced_module_tracker module_helper_dict fqn depth + separate module s collective operations forward backward forward_collectives = backward_collectives = fqn comm_module_counts forward_collectives = comm_module_counts fqn forward backward_collectives = comm_module_counts fqn backward forward_operations = backward_operations = checkpointing_operations = include_DTensor_ops fqn comm_module_operation_counts forward_operations backward_operations checkpointing_operations = _get_operations_list comm_module_operation_counts fqn add_tracing_information table collectives_dict operation_list adds tracing information module s forward backward collective count collectives_dict items table += f \ m collective_indent collective count \ m\n add_operations table operation collective_indent operation_indent adds operation information table table += f \ m collective_indent operation_name \ m\n len operation input_shape operation_shape = operation input_shape operation_sharding = operation input_sharding operation_device_mesh = operation device_mesh table += f \ m operation_indent shape operation_shape \ m\n table += f \ m operation_indent sharding operation_sharding \ m\n table += f \ m operation_indent device mesh operation_device_mesh \ m\n table operation operation_list operation_name = str operation name include all operations include_trivial_ops table = add_operations table operation collective_indent operation_indent include all operations trivial operations include_ops operation_name trivial_ops table = add_operations table operation collective_indent operation_indent only include dTensor operations trivial set include_DTensor_ops operation_name trivial_ops len operation input_shape table = add_operations table operation collective_indent operation_indent table len forward_collectives len forward_operations table += f indent FORWARD PASS\n table = add_tracing_information table forward_collectives forward_operations len backward_collectives len backward_operations table += f indent BACKWARD PASS\n table = add_tracing_information table backward_collectives backward_operations len checkpointing_operations table += f indent ACTIVATION CHECKPOINTING\n table = add_tracing_information table checkpointing_operations table _get_operations_list module_operation_counts forward_operations = op op module_operation_counts operations_list op is_bw backward_operations = op op module_operation_counts operations_list op is_bw op is_activation_checkpointing checkpointing_operations = op op module_operation_counts operations_list op is_activation_checkpointing forward_operations backward_operations checkpointing_operations get_total_counts - int sum comm_counts values get_comm_counts - dict Any int Returns communication counts dictionary Returns Dict Any int The communication counts dictionary comm_counts get_parameter_info - dict str dict str Any advanced_module_tracker module_parameters_dict get_sharding_info - dict str dict str Any advanced_module_tracker sharding_dict __enter__ comm_counts clear comm_module_counts clear comm_module_counts Global = comm_module_counts Global forward = defaultdict int comm_module_counts Global backward = defaultdict int comm_module_operation_counts clear super __enter__ advanced_module_tracker __enter__ pyrefly ignore bad-override __exit__ args advanced_module_tracker __exit__ super __exit__ args log_comm_debug_tracing_table_to_file file_name= comm_mode_log txt noise_level= Alternative console CommDebugMode output writes file specified user ansi_escape = re compile r \x B\ - - -~ table = ansi_escape sub generate_comm_debug_tracing_table noise_level open file_name w log_file log_file write table _set_noise_parameters noise_level sets variables controlling what information displays based noise level include_DTensor_ops = False include_module_data = False include_ops = False include_trivial_ops = False noise_level include_DTensor_ops = True include_module_data = True noise_level include_ops = True noise_level include_trivial_ops = True include_DTensor_ops include_module_data include_ops include_trivial_ops __torch_dispatch__ func types args= kwargs=None When running mode DTensor ordinarily all modes will run before subclasses get chance run Returning NotImplemented here gives us chance let DTensor run desugar into comms ops before CommDebugMode sees them sets up operation-level collective count advanced_module_tracker name comm_module_operation_counts dictionary should hold module input output shape operations list collective counter comm_module_operation_counts advanced_module_tracker name = operations_list operation_dict = operation_dict name = func operation_dict input_shape = operation_dict input_sharding = operation_dict device_mesh = tracks operation part backward pass operation_dict is_bw = advanced_module_tracker is_bw tracks operation part activation checkpointing operation_dict is_activation_checkpointing = advanced_module_tracker activation_checkpointing any t == DTensor t types ele args isinstance ele DTensor saves shapes placements all DTensor args operation_dict input_shape append ele shape operation_dict input_sharding append ele placements operation_dict device_mesh = str ele device_mesh comm_module_operation_counts advanced_module_tracker name operations_list append operation_dict NotImplemented kwargs = kwargs kwargs out = func args kwargs func_packet = func _overloadpacket We have many tests use CommDebugMode verify occurrence collectives These tests do so querying comm_counts legacy funcol ops key For purpose native funcol migration we need these tests work both legacy native funcol To avoid need modify all tests accommodate two implementations we make CommDebugMode translate native funcol ops into legacy funcol ops until migration finishes func_packet comm_registry func_packet c d_collective_ops func_packet NATIVE_TO_PY_MAPPING func_packet = NATIVE_TO_PY_MAPPING func_packet comm_counts func_packet += key = forward advanced_module_tracker is_bw key = backward adds collective count current module advanced_module_tracker name comm_module_counts comm_module_counts advanced_module_tracker name = comm_module_counts advanced_module_tracker name forward = defaultdict int comm_module_counts advanced_module_tracker name backward = defaultdict int comm_module_counts advanced_module_tracker name key func_packet += adds collective count parent modules par advanced_module_tracker module_parents_dict advanced_module_tracker name makes sure we aren t double counting when current sub-module hasn t been removed parents par = advanced_module_tracker name par comm_module_counts comm_module_counts par = comm_module_counts par forward = defaultdict int comm_module_counts par backward = defaultdict int comm_module_counts par key func_packet += tensor op uses fake tensors detect_fake_mode args out add tensor operation module operation list comm_module_operation_counts advanced_module_tracker name operations_list append operation_dict out __repr__ f CommDebugMode get_total_counts = get_total_counts