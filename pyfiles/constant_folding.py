collections typing Any Callable Optional torch torch utils _pytree pytree torch _inductor freezing_utils maybe_set_is_frozen_param torch utils _ordered_set OrderedSet aten = torch ops aten We would like split modules into two subgraphs runtime weight updates work correctly The use case more information could found https docs google com document d inZC- KarJ gKB G egmYLx V_dKX_apxon w zPC Q edit usp=sharing META_TAG = MODULE_TYPE MODULE_TAG = _MAIN_MODULE CONST_MODULE_TAG = _CONST_MODULE _dont_constant_fold list torch fx node Target = add_dont_constant_fold op torch fx node Target - None global _dont_constant_fold _dont_constant_fold append op clear_dont_constant_fold - None global _dont_constant_fold _dont_constant_fold clear replace_node_with_constant gm torch fx GraphModule node torch fx Node constant Optional torch Tensor = None name Optional str = None - None g = gm graph name qualname = name hasattr gm _frozen_param_count gm _frozen_param_count = type ignore assignment i = gm _frozen_param_count while True qualname = f _frozen_param i hasattr gm qualname break i += type ignore assignment operator gm _frozen_param_count = i + type ignore assignment operator g inserting_before node constant None new_input_node = g create_node get_attr qualname case lifted constants new_input_node = g create_node placeholder qualname node replace_all_uses_with new_input_node new_input_node meta update node meta g erase_node node new_input_node name = node name constant None needed suppress ` does reference nn Module nn Parameter buffer ` warning gm register_buffer qualname constant setattr gm qualname constant mark any constants created during freezing maybe_set_is_frozen_param constant is_const_source node torch fx Node lifted_constant_names Optional list str - bool node op == get_attr node name lifted_constant_names ConstantFolder torch fx Interpreter __init__ gm torch fx GraphModule skip_constructors bool = False lifted_constant_names Optional list str = None skip_folding_node_fn Optional Callable torch fx Node bool = None - None super __init__ gm node_replacements dict torch fx Node Any = replaced_uses dict torch fx Node int = collections Counter unknown_value = object skip_constructors bool = skip_constructors overwrite deallocate env values their only remaining use output user_to_last_uses = node_to_last_non_output_use lifted_constant_names = lifted_constant_names deferred_value = object skip_folding_node_fn = skip_folding_node_fn _support_dynamic_shape - bool ConstantFolder support dynamic shape now False _deduce_value node torch fx Node - Any lifted_constant_names None super run_node node lifted_constant_names passed no concrete value available so we just check all inputs have values skip_folding_node_fn None skip_folding_node_fn node unknown_value flattened_node_inps = pytree arg_tree_leaves node args node kwargs inp flattened_node_inps isinstance inp torch fx Node inp name lifted_constant_names env inp = deferred_value unknown_value deferred_value is_impure node torch fx node Node - bool is_woq_int _pattern node torch fx node Node - bool node target torch ops prims convert_element_type default type ignore return-value isinstance node args torch fx Node val node args meta node args meta val dtype == torch int type ignore union-attr node args == torch bfloat is_woq_int _pattern node node target torch ops aten permute default len node users == is_woq_int _pattern next iter node users is_const_source node args type ignore arg-type lifted_constant_names Case int _weight - dq - bf _weight Case int _weight - permute - dq - bf _weight True quant_registered = getattr torch ops quantized_decomposed dequantize_per_channel None None quant_registered node target torch ops quantized_decomposed dequantize_per_channel default torch ops quantized_decomposed dequantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor tensor torch ops quantized_decomposed convert_element_type no_fuse For pattern fp _weight - q - dq We only folding fp _weight - q int _weight leave dq graph fused True node target _dont_constant_fold True False node_to_last_non_output_use - dict torch fx Node list torch fx Node last_non_output_use = collections defaultdict list seen_uses = OrderedSet torch fx Node output_node = next iter reversed module graph nodes type ignore arg-type union-attr node reversed module graph nodes type ignore arg-type union-attr node target == output continue add_use inp torch fx Node - None inp seen_uses seen_uses add inp last_non_output_use node append inp In-place fine since we don t mutate pytree tree_map_only_ torch fx Node add_use node args node kwargs node only used output we want gc right away len node users == output_node node users last_non_output_use node append node last_non_output_use run_node node torch fx Node - Any node target == output because we remove nodes env last non output use re-define them now we ll get error interpreter set_env arg torch fx Node - None env arg = unknown_value In-place fine since we don t mutate pytree tree_map_only_ torch fx Node set_env node args super run_node node args kwargs = fetch_args_kwargs_from_env node flattened_inputs = pytree arg_tree_leaves args kwargs We need do weird thing because cases where flattened_inputs contains ScriptObject equality checking results type error types different any type unknown_value type input_ unknown_value == input_ input_ flattened_inputs unknown_value TODO - fix errors node op == call_function node target aten _efficientzerotensor default unknown_value TODO - constant folding triton kernel returns inputs -- fix node op == call_function node name == triton_kernel_wrapper_functional_proxy unknown_value skip constructors since inductor generates optimal code them already turning into tensor would result additional global memory read TODO - more complicated strategy skip_constructors is_const_source node lifted_constant_names any isinstance e torch Tensor e flattened_inputs unknown_value All mutations should either removed inputs which we did make constant isinstance node target torch _ops OpOverload torch Tag nondeterministic_seeded node target tags unknown_value node op == call_function isinstance node target torch _ops HigherOrderOperator unknown_value out = _deduce_value node isinstance out torch _C ScriptObject out out == unknown_value unknown_value is_const_source node lifted_constant_names isinstance out torch Tensor out == deferred_value out = deferred_value out device type == meta out insertable_tensor_check out out is_impure node unknown_value add_node_replacement node out flattened_node_inps = pytree arg_tree_leaves node args node kwargs n flattened_node_inps isinstance n torch fx Node continue replaced_uses n += to_delete user_to_last_uses get node replaced_uses to_delete == len to_delete users node_replacements pop to_delete None out insertable_tensor_check tensor torch Tensor - bool True add_node_replacement node torch fx Node tensor torch Tensor - None node_replacements node = tensor run - Any type ignore override env dict torch fx Node Any = insert_placerholder_values env super run initial_env=env insert_placerholder_values env dict torch fx Node Any - None n module graph find_nodes op= placeholder type ignore operator union-attr env n = unknown_value type ignore assignment lifted_constant_names None n module graph nodes type ignore union-attr n name lifted_constant_names env n = deferred_value constant_fold gm torch fx GraphModule constraint_fn Optional Callable torch fx Node bool = None - None torch utils _python_dispatch _disable_current_modes cf = ConstantFolder gm skip_constructors=True cf run node constant cf node_replacements items constraint_fn None constraint_fn node continue replace_node_with_constant gm node constant erased_params = node gm graph find_nodes op= get_attr len node users == hasattr gm node target delattr gm node target erased_params append node node erased_params gm graph erase_node node gm graph eliminate_dead_code gm graph lint gm recompile constant_graph_tag gm torch fx GraphModule skip_constructors bool = True lifted_constant_names Optional list str = None skip_folding_node_fn Optional Callable torch fx Node bool = None - None torch utils _python_dispatch _disable_current_modes cf = ConstantFolder gm skip_constructors=skip_constructors lifted_constant_names=lifted_constant_names skip_folding_node_fn=skip_folding_node_fn cf run node gm graph nodes skip_folding_node_fn None skip_folding_node_fn node node meta META_TAG = MODULE_TAG continue is_const_source node lifted_constant_names node cf node_replacements node cf replaced_uses node meta META_TAG = CONST_MODULE_TAG node meta META_TAG = MODULE_TAG run_and_get_constant_graph gm torch fx GraphModule skip_constructors bool = True lifted_constant_names Optional list str = None skip_folding_node_fn Optional Callable torch fx Node bool = None - torch fx GraphModule Construct GraphModule which corresponds part which could constant folded provided gm constant_graph_tag gm skip_constructors lifted_constant_names skip_folding_node_fn untag node torch fx Node - bool used_to_fold = False u node users u meta META_TAG == CONST_MODULE_TAG used_to_fold = True break used_to_fold node meta META_TAG = MODULE_TAG used_to_fold We rewrite tags s constant being directly consumed without any folding opportunity we keep main gm node gm graph nodes node op == get_attr node name lifted_constant_names untag node new_graph = torch fx Graph node_remapping dict torch fx Node torch fx Node = output_nodes = node gm graph nodes node meta META_TAG == MODULE_TAG continue new_node = new_graph node_copy node lambda x node_remapping x node_remapping node = new_node user node users user meta META_TAG == MODULE_TAG output_nodes append new_node break new_graph output tuple output_nodes new_graph lint new_gm = torch fx GraphModule gm new_graph new_gm