Owner s oncall distributed sys torch torch distributed dist torch nn nn torch distributed _shard shard_module torch distributed _shard sharded_tensor ShardedTensor torch distributed _shard sharding_plan ShardingPlan ShardingPlanner torch distributed _shard sharding_spec ChunkShardingSpec torch testing _internal common_distributed requires_nccl skip_if_lt_x_gpu torch testing _internal common_utils run_tests TEST_WITH_DEV_DBG_ASAN torch testing _internal distributed _shard sharded_tensor ShardedTensorTestBase TEST_GPU_NUM with_comms torch testing _internal distributed _shard sharded_tensor _test_ops_common generate_chunk_sharding_specs_for_test torch testing _internal distributed _shard test_common SimpleMegatronLM TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit Example ShardingPlanner chunks every parameter module all available devices defined ChunkAllShardingPlanner ShardingPlanner dim = devices = __init__ chunk_dim= device_count= dim = chunk_dim devices = f rank i cuda i i range device_count build_plan module nn Module - ShardingPlan named_params = module named_parameters plan = name _ named_params plan name = ChunkShardingSpec dim placements=self devices ShardingPlan plan=plan TestShardingPlan ShardedTensorTestBase with_comms init_rpc=False skip_if_lt_x_gpu TEST_GPU_NUM requires_nccl test_sharding_plan_errors rowwise_sharding_spec = generate_chunk_sharding_specs_for_test sharding_plan_wrong_plan = ShardingPlan plan= fc weight torch randn output_plan= rowwise_sharding_spec megatron_lm = SimpleMegatronLM cuda rank assertRaisesRegex TypeError Only ` ShardingSpec ` ` Sharder ` supported shard shard module provided sharding plan shard_module megatron_lm sharding_plan_wrong_plan sharding_plan_wrong_output_plan = ShardingPlan plan= fc weight rowwise_sharding_spec output_plan= torch randn assertRaisesRegex TypeError Only ` ShardingSpec ` supported output_plan shard module provided sharding plan shard_module megatron_lm sharding_plan_wrong_output_plan sharding_plan_wrong_module_path = ShardingPlan plan= fc weight rowwise_sharding_spec assertRaisesRegex AttributeError has no attribute shard module provided sharding plan shard_module megatron_lm sharding_plan_wrong_module_path sharding_plan_wrong_param_path = ShardingPlan plan= fc biass rowwise_sharding_spec assertRaisesRegex AttributeError has no attribute shard module provided sharding plan shard_module megatron_lm sharding_plan_wrong_param_path with_comms init_rpc=False skip_if_lt_x_gpu TEST_GPU_NUM requires_nccl test_custom_sharding_planner megatron_lm = SimpleMegatronLM rank=self rank cuda rank planner = ChunkAllShardingPlanner device_count=TEST_GPU_NUM sharding_plan = planner build_plan megatron_lm shard_module megatron_lm sharding_plan check make sure module already been sharded assertTrue isinstance megatron_lm fc weight ShardedTensor assertTrue isinstance megatron_lm fc weight ShardedTensor assertTrue isinstance megatron_lm fc bias ShardedTensor assertTrue isinstance megatron_lm fc bias ShardedTensor with_comms init_rpc=False skip_if_lt_x_gpu TEST_GPU_NUM requires_nccl test_shard_module_sub_process_group megatron_lm = SimpleMegatronLM rank=self rank colwise_sharding_spec = ChunkShardingSpec dim= placements= rank cuda rank cuda rowwise_sharding_spec = ChunkShardingSpec dim= placements= rank cuda rank cuda sharding_plan = ShardingPlan plan= fc weight colwise_sharding_spec fc weight rowwise_sharding_spec pg = dist new_group rank = shard_module megatron_lm sharding_plan process_group=pg __name__ == __main__ run_tests