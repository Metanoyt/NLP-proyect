This module implements TorchDynamo s core frame conversion functionality transforming Python frames into FX graphs It handles - Frame analysis bytecode transformation - Guard creation management dynamic behaviors - Cache management recompilation - Error handling fallback mechanisms Key classes - ConvertFrame Main entry point frame conversion error handling - ConvertFrameAssert Implements core frame graph conversion logic - Tracker Tracks input output code objects during conversion - CatchErrorsWrapper Provides error handling suppression logic The conversion process preserves program semantics while enabling optimizations through torch compile related systems NOTE _torchdynamo_orig_backend used convert frame wrappers identify inner wrapped function By going down _torchdynamo_orig_backend chain one can recover original unwrapped backend which checked during Dynamo cache lookup __future__ annotations collections contextlib cProfile dis functools gc inspect itertools logging os pstats random subprocess sys threading time traceback types typing weakref dataclasses dataclass pathlib Path types CellType CodeType FunctionType ModuleType typing Any Optional TypeVar Union typing_extensions ParamSpec weakref ReferenceType torch torch _logging torch _C _dynamo guards GlobalStateGuard torch _dynamo callback CallbackTrigger torch _dynamo distributed get_compile_pg torch _dynamo symbolic_convert TensorifyState torch _guards compile_context CompileContext CompileId tracing torch _logging structured torch _utils_internal compile_time_strobelight_meta justknobs_check maybe_upload_prof_stats_to_manifold signpost_event torch fx _lazy_graph_module _use_lazy_graph_module torch fx experimental symbolic_shapes ConstraintViolationError GuardOnDataDependentSymNode torch fx graph_module _forward_from_src original_forward_from_src torch monitor _WaitCounter torch nn parallel distributed DistributedDataParallel torch utils _python_dispatch _disable_current_modes is_in_any_mode_without_ignore_compile_internals is_in_torch_dispatch_mode torch utils _traceback CapturedTraceback format_traceback_short config decorators exc graph_break_hints trace_rules bytecode_analysis remove_dead_code remove_pointless_jumps bytecode_transformation check_inst_exn_tab_entries_valid Instruction is_generator propagate_inst_exn_table_entries transform_code_object cache_size CacheSizeRelevantForFrame compute_cache_size exceeds_recompile_limit is_recompilation eval_frame always_optimize_code_objects Constraint dynamo_tls skip_code TorchPatcher exc augment_exc_message BackendCompilerFailed FailOnRecompileLimitHit format_error_msg InternalTorchDynamoError PackageError RecompileLimitExceeded ResumePrologueTracingError ShortenTraceback SkipCodeRecursiveException TorchRuntimeError UncapturedHigherOrderOpError unimplemented_v Unsupported graph_bytecode_inputs reset_user_object_tracking guards CheckFunctionManager get_and_maybe_log_recompilation_reasons GuardedCode hooks Hooks output_graph DynamoTracerOutput OutputGraphCommon pgo _log_size_mismatch_recompile log_frame_dynamic_whitelist put_code_state replay_record ExecutionRecord resume_execution TORCH_DYNAMO_RESUME_IN_PREFIX symbolic_convert DistributedState ExceptionStack InstructionTranslator LocalState SpeculationLog trace_rules is_numpy types ConvertFrameReturn FrameAction FrameExecStrategy wrap_guarded_code utils _get_error_on_graph_break chromium_event_timed CleanupManager CompileTimeInstructionCounter counters dynamo_timed format_bytecode gen_record_file_name get_hook_for_recompile_user_context get_metrics_context increment_frame is_namedtuple istype LazyString maybe_disable_inference_mode maybe_disable_inference_mode_for_fake_prop orig_code_map reset_graph_break_dup_checker setup_compile_debug to_int_us troubleshooting_url write_record_to_file variables torch_function torch_function_mode_stack_state_mgr np Optional ModuleType try numpy np except ModuleNotFoundError np = None typing TYPE_CHECKING collections abc Callable torch utils weak WeakIdKeyDictionary backends registry CompilerFn package CompilePackage repro after_dynamo WrapBackendDebug types BytecodeHook CacheEntry DynamoFrameType variables builder FrameStateSizeEntry log = logging getLogger __name__ bytecode_log = torch _logging getArtifactLogger __name__ bytecode graph_break_log = torch _logging getArtifactLogger __name__ graph_breaks compile_lock = threading RLock _T = TypeVar _T _P = ParamSpec _P TODO_UNKNOWN pass Tracker __init__ - None seen list ReferenceType CodeType = seen_ids set int = set add strong_obj CodeType - None idx = id strong_obj idx seen_ids obj = weakref ref strong_obj lambda _ seen_ids remove idx seen append obj seen_ids add idx __contains__ item CodeType - bool id item seen_ids clear - None seen clear seen_ids clear input_codes = Tracker output_codes = Tracker initial_global_state Optional GlobalStateGuard = None functools wraps original_forward_from_src fx_forward_from_src_skip_result src str globals dict str Any co_fields Optional dict str str = None - FunctionType we monkey patch FX prevent infinite loop trying convert our generated code result = original_forward_from_src src globals co_fields skip_code result __code__ result log_dynamo_start code CodeType skip int = - list str convert_frame_intern = structured intern_string __file__ captured_tb = CapturedTraceback extract skip= + skip summary frames_interned = structured from_traceback captured_tb Extract filter stack stack = list itertools takewhile lambda f f filename = convert_frame_intern frames_interned + line code co_firstlineno name code co_name filename structured intern_string code co_filename Initialize ChromiumEventLogger start torch _logging trace_structured dynamo_start lambda stack stack Capture stack separately without using from_traceback get actual filenames stack_strings = f Line frame lineno Name frame name Filename frame filename frame captured_tb frame filename = convert_frame_intern + f Line code co_firstlineno Name code co_name Filename code co_filename stack_strings preserve_global_state fn Callable _P _T - Callable _P _T Context manager Save restore torch is_grad_enabled state Save restore python random state Save restore torch random state Monkey patch torch fx graph_module _forward_from_src functools wraps fn _fn args _P args kwargs _P kwargs - _T guards = GlobalStateGuard prior_grad_mode = torch is_grad_enabled Just case we get left bad dispatch state we want restore This can happen because dispatch bits aren t true stack counter - so we can t just increment decrement them we enter leave torch _C _PreserveDispatchKeyGuard maybe_disable_inference_mode maybe_disable_inference_mode_for_fake_prop prior_inference_mode = torch is_inference_mode_enabled prior_deterministic = torch are_deterministic_algorithms_enabled prior_warn_only = torch is_deterministic_algorithms_warn_only_enabled prior_mobile_allocator_state = torch _C _is_default_mobile_cpu_allocator_set py_rng_state = random getstate prior_dtype = torch get_default_dtype torch_rng_state = torch random get_rng_state cuda_rng_state = None torch cuda is_available torch _C DisableTorchFunction cuda_rng_state = torch cuda get_rng_state cuda_matmul_fp _prec = torch _C _get_fp _precision_getter cuda matmul prior_fwd_from_src = torch fx graph_module _forward_from_src torch fx graph_module _forward_from_src = fx_forward_from_src_skip_result cleanup = setup_compile_debug exit_stack = contextlib ExitStack exit_stack enter_context torch fx _symbolic_trace _maybe_revert_all_patches exit_stack enter_context torch_function_mode_stack_state_mgr reset_user_object_tracking try fn args kwargs finally cleanup close assert torch _C _len_torch_function_stack == Torch function mode stack state changed while dynamo tracing please report bug exit_stack close torch _C _set_grad_enabled prior_grad_mode torch autograd grad_mode _enter_inference_mode prior_inference_mode torch use_deterministic_algorithms prior_deterministic warn_only=prior_warn_only random setstate py_rng_state torch random set_rng_state torch_rng_state torch set_default_dtype prior_dtype curr_mobile_allocator_state = torch _C _is_default_mobile_cpu_allocator_set prior_mobile_allocator_state = curr_mobile_allocator_state torch _C _unset_default_mobile_cpu_allocator cuda_rng_state None torch _C DisableTorchFunction torch cuda set_rng_state cuda_rng_state torch _C _set_fp _precision_setter cuda matmul cuda_matmul_fp _prec torch fx graph_module _forward_from_src = prior_fwd_from_src assert guards check f Global guards reason state changed while dynamo tracing please report bug _fn _torchdynamo_orig_backend = fn type ignore attr-defined _fn TorchPatcher suppress_torch_distributed_warnings has_tensor_in_frame frame DynamoFrameType - bool Check frame has torch related bits Check function decorated using torch _dynamo optimize frame f_code always_optimize_code_objects True Check there global torch co_name frame f_code co_names co_name frame f_globals obj = frame f_globals co_name isinstance obj ModuleType obj __name__ startswith torch obj torch True global numpy np config trace_numpy obj np is_numpy obj True seen_ids dict int bool = has_tensor obj object - bool Recursively check obj has tensor obj_id = id obj obj_id seen_ids seen_ids obj_id seen_ids obj_id = False isinstance obj torch Tensor torch nn Module istype obj type issubclass obj torch nn Module seen_ids obj_id = True seen_ids obj_id config trace_numpy np istype obj np ndarray isinstance obj np generic seen_ids obj_id = True seen_ids obj_id istype obj list tuple seen_ids obj_id = any has_tensor v v obj seen_ids obj_id istype obj dict Some packages like pytest can updated during runtime So make copy values avoid issues like RuntimeError dictionary changed size during iteration values = list obj values seen_ids obj_id = any has_tensor v v values seen_ids obj_id istype obj str int float type None bool seen_ids obj_id = False seen_ids obj_id is_namedtuple obj hasattr obj _fields seen_ids obj_id = any has_tensor getattr obj v v obj _fields seen_ids obj_id config debug print f Assuming object type type obj does have tensor False Check passed arguments type Tensor value frame f_locals values has_tensor value True log debug skipping because no torch s \ s s frame f_code co_name frame f_code co_filename frame f_code co_firstlineno False exception_handler e Exception code CodeType frame Optional DynamoFrameType = None export bool = False - None record_filename = None hasattr e exec_record record_filename = gen_record_file_name e code write_record_to_file record_filename e exec_record e record_filename = record_filename type ignore attr-defined augment_exc_message e export=export FRAME_COUNTER = FRAME_COMPILE_COUNTER typing Counter Union int FrameStateSizeEntry = collections Counter maybe_cprofile func Callable _P _T - Callable _P _T config cprofile cprofile_wrapper func func cprofile_wrapper func Callable _P _T - Callable _P _T functools wraps func profile_wrapper args _P args kwargs _P kwargs - _T trace_id = CompileContext current_trace_id assert trace_id Trace id None profile_path = Path f tmp func __name__ _ str trace_id replace _ profile prof = cProfile Profile try prof enable start_ts = time time pyrefly ignore bad-argument-type retval = prof runcall func args kwargs profile_latency = time time - start_ts prof disable except ValueError log exception failed enable cProfile profile_latency = retval = func args kwargs log warning ### Cprofile s trace id s took f seconds ### func __name__ trace_id profile_latency ps = pstats Stats prof try prof dump_stats profile_path except OSError log exception Cannot write s profile_path log warning Raw profile s profile_path svg_path = profile_path with_suffix svg try gprof dot_process = subprocess Popen gprof dot -f pstats -- node-label=total-time-percentage -- node-label=self-time-percentage -- node-label=total-time str profile_path stdout=subprocess PIPE subprocess check_call dot -Tsvg -o str svg_path stdin=gprof dot_process stdout log warning Generated SVG profile s svg_path except FileNotFoundError log warning Failed generate SVG profile -- dumping stats instead Try installing gprof dot dot better visualization ps sort_stats pstats SortKey TIME print_stats ps sort_stats pstats SortKey CUMULATIVE print_stats manifold_link = maybe_upload_prof_stats_to_manifold str profile_path fb-only torch _logging trace_structured link lambda name cprofile_manifold_url url manifold_link retval profile_wrapper dataclass ConvertFrameBox error_on_graph_break Optional bool = None get_compile_id frame_state dict str Union int FrameStateSizeEntry - CompileId global FRAME_COUNTER _id frame_state frame_state _id = FRAME_COUNTER FRAME_COUNTER += frame_id = frame_state _id assert isinstance frame_id int frame_compile_id = FRAME_COMPILE_COUNTER frame_id FRAME_COMPILE_COUNTER frame_id += compiled_autograd_id = None prior = CompileContext current_compile_id compiled_autograd_id = prior compiled_autograd_id CompileId compiled_autograd_id=compiled_autograd_id frame_id=frame_id frame_compile_id=frame_compile_id ConvertFrameAssert __init__ compiler_fn CompilerFn one_graph bool = True export bool = False export_constraints Optional typing Never = None package Optional CompilePackage = None - None assert export_constraints None reset_graph_break_dup_checker _torchdynamo_orig_backend = compiler_fn _one_graph = one_graph _export = export _export_constraints = export_constraints _package = package _box = ConvertFrameBox property _clone_with_backend - Callable CompilerFn ConvertFrameAssert lambda backend convert_frame_assert backend _one_graph _export _export_constraints __call__ frame DynamoFrameType cache_entry Optional CacheEntry hooks Hooks frame_state dict str Union int FrameStateSizeEntry skip int = - ConvertFrameReturn increment_frame code = frame f_code cache_size = compute_cache_size frame cache_entry input_codes add code code output_codes ConvertFrameReturn os environ get TORCHDYNAMO_DEBUG_FUNCTION os environ get TORCHDYNAMO_DEBUG_FUNCTION = code co_name ConvertFrameReturn code co_name == genexpr code co_filename endswith transformers file_utils py transformers utils generic py diffusers utils outputs py needed cleans up torchbench error stats ConvertFrameReturn code co_name == __setattr__ setattr could tricky handle generally also likely useful compile- skip whole frame ConvertFrameReturn code co_name == __init__ code co_filename startswith os path dirname torch optim __file__ optimizer support still incomplete see test_state_dict test dynamo test_optimizers py ConvertFrameReturn Check frame generated exec builtin call TODO - Running exec generated frame seems propagates f_globals next frames code co_name == module code co_filename == string ConvertFrameReturn code co_name == lambda code co_filename == string bool frame f_builtins namedtuple subclass constructor Empty builtins cause issue len keyword LIST_LEN guard ConvertFrameReturn is_generator code unimplemented_v gb_type= Attempt trace generator context= explanation= Generators cannot compiled directly ` torch compile ` hints= Call generator inside non-generator Python function compile function instead graph_break_hints FUNDAMENTAL has_tensor_in_frame frame ConvertFrameReturn skip tracing non-recursive disabled functions detect previous frame non-convert_frame non-recursive disable wrapper prev_frame = sys _getframe while prev_frame torch _dynamo convert_frame py prev_frame f_code co_filename prev_frame = prev_frame f_back type ignore assignment prev_frame prev_frame f_code decorators _nonrecursive_disable_wrapper_code ConvertFrameReturn apply_to_code=False global initial_global_state initial_global_state = GlobalStateGuard compile_id = get_compile_id frame_state frame_id = compile_id frame_id signpost_event dynamo _convert_frame_assert _compile co_name code co_name frame_id frame_id compile_id str compile_id co_filename code co_filename co_firstlineno code co_firstlineno cache_size cache_size num_cache_entries_with_same_id_matched_objs accumulated_cache_size cache_size num_cache_entries Record traced frames skipping Dynamo generated ones code co_name startswith TORCH_DYNAMO_RESUME_IN_PREFIX info = f code co_name code co_filename code co_firstlineno dynamo_tls traced_frame_infos append info compile_context CompileContext compile_id result = _compile frame f_code frame f_globals frame f_locals frame f_builtins frame closure _torchdynamo_orig_backend _one_graph _export _export_constraints hooks cache_entry cache_size frame frame_state=frame_state compile_id=compile_id skip=skip + package=self _package convert_frame_box=self _box config caching_precompile _package None package DynamoCache Record dynamo package has changed DynamoCache record_package _package result convert_frame_assert compiler_fn CompilerFn one_graph bool = True export bool = False export_constraints Optional typing Never = None package Optional CompilePackage = None - ConvertFrameAssert Fully convert frame into FX graph raising exception we fail ConvertFrameAssert compiler_fn one_graph export export_constraints package collections OrderedDict torch utils hooks RemovableHandle we have use ` OrderedDict ` make ` RemovableHandle ` work _bytecode_hooks dict int BytecodeHook = OrderedDict register_bytecode_hook hook BytecodeHook - RemovableHandle Register hooks bytecode generated Dynamo The hook can do some logging well new code object used Please refer ` BytecodeHook ` hook signature handle = RemovableHandle _bytecode_hooks _bytecode_hooks handle id = hook handle TODO - We want run preserve_node_meta context manager here CI fails its unclear failures flaky torch fx traceback preserve_node_meta preserve_global_state trace_frame code types CodeType globals dict str object locals dict str object builtins dict str object closure tuple CellType compiler_fn CompilerFn tf_mode_stack list torch overrides TorchFunctionMode one_graph bool speculation_log SpeculationLog instructions list Instruction code_options dict str object export bool = False export_constraints Optional typing Never = None frame_state Optional dict str Union int FrameStateSizeEntry = None distributed_state Optional DistributedState = None package Optional CompilePackage = None - DynamoTracerOutput torch fx experimental validator bisect translation_validation_enabled speculation_log restart type ignore has-type exn_vt_stack = ExceptionStack tracer = InstructionTranslator instructions code locals globals builtins closure tf_mode_stack code_options compiler_fn one_graph export export_constraints frame_state=frame_state speculation_log=speculation_log type ignore has-type exn_vt_stack=exn_vt_stack distributed_state=distributed_state type ignore has-type package=package run_tracer - None try tracer output mark_bytecode_tracing_start tracing tracer output tracing_context tracer set_current_tx tracer run except exc UnspecializeRestartAnalysis speculation_log clear type ignore has-type raise except exc SpeculationRestartAnalysis exc TensorifyScalarRestartAnalysis exc SkipFrame raise except Exception translation_validation_enabled bisect tracer output shape_env raise finally tracer output call_cleanup_hooks try run_tracer tracer_output = DynamoTracerOutput tracer output = tracer_output output_graph assert output None assert output output_instructions instructions = output output_instructions code_options update output code_options propagate_inst_exn_table_entries instructions check_inst_exn_tab_entries_valid instructions instructions = remove_pointless_jumps remove_dead_code instructions except Exception e e _torch_dynamo_tracer_output = DynamoTracerOutput tracer error=True type ignore attr-defined raise tracer_output dataclass DynamoOutput Represents core data returned single dynamo run including - Guards wrapped inside tracer_output output_graph guards - Generated bytecode - Other information needed compilation This data structure should capture all interesting information dynamo produces frontend side before enters user backend tracer_output DynamoTracerOutput bytecode types CodeType last_attempt_start_time Optional float build_guards code types CodeType hooks Optional Hooks = None save bool = False cache_entry Optional CacheEntry = None strict_error bool = False - CheckFunctionManager output_graph = tracer_output output_graph assert output_graph None CheckFunctionManager code output_graph cache_entry hooks guard_fail_fn hooks None hooks guard_filter_fn hooks None save_guards=save strict_error=strict_error graph_capture_output argdefs Optional tuple Any = None - GraphCaptureOutput output_graph = tracer_output output_graph assert output_graph None GraphCaptureOutput OutputGraphCommon output_graph dump_guards_state output_graph import_sources output_graph shape_env output_graph export_metadata output_graph tracked_fakes_id_to_source output_graph import_sources output_graph traced_code bytecode tracer_output closure argdefs dataclass BackendInput Represents core data structure dynamo will pass backend including - Graph module - Example inputs - The FakeTensorMode used compiling graph This data structure should capture all information dynamo produces user backend backend_id str graph_module torch fx GraphModule example_inputs Any fake_mode torch _subclasses fake_tensor FakeTensorMode tensor_to_context WeakIdKeyDictionary dataclass GraphCaptureOutput Minimal version DynamoOutput output_graph OutputGraphCommon import_sources dict str str traced_code list CodeType bytecode CodeType closure Optional tuple Any argdefs Optional tuple Any build_guards code types CodeType hooks Optional Hooks = None save bool = False cache_entry Optional CacheEntry = None strict_error bool = False - CheckFunctionManager CheckFunctionManager code output_graph cache_entry hooks guard_fail_fn hooks None hooks guard_filter_fn hooks None save_guards=save strict_error=strict_error dataclass CaptureOutput CaptureOutput should represent all information produced torch compiler single graph capture This intends consumed various compiler frontends so we can share much compiler internals possible avoid great divergence between different stacks This data structure should eventually contain all information compiler produces more refactors happens converge different compiler frontends graph_capture_output GraphCaptureOutput BackendInput can None when dynamo didn t compile any graph no tensor op backend_input Optional BackendInput forward_callable - Callable Any importlib TODO code sharing import_sources = graph_capture_output output_graph import_sources assert backend_input None backend_id = backend_input backend_id import_sources = alias importlib import_module module_name alias module_name import_sources items f_globals = import_sources backend_id backend_input graph_module types FunctionType graph_capture_output bytecode f_globals closure=self graph_capture_output closure argdefs=self graph_capture_output argdefs get_traced_fn mod Any - tuple FunctionType Optional object Utility function get function trace optionally bound object callable nn Module function method inspect isinstance mod torch nn Module mod = mod forward hasattr mod __self__ pyrefly ignore missing-attribute mod __func__ mod __self__ inspect isfunction mod mod None raise RuntimeError f Unsupported model code type mod _get_signature fn Any - inspect Signature inspect signature fn follow_wrapped=False _get_frame mod Any args tuple Any kwargs Optional dict str Any = None - FrameInfo Create frame trace given model args optional kwargs builtins fn self_opt = get_traced_fn mod self_opt None args = self_opt + args kwargs None kwargs = signature = _get_signature fn bound_arguments = signature bind args kwargs bound_arguments apply_defaults f_locals = bound_arguments arguments closure = fn __closure__ freevars = fn __code__ co_freevars freevars closure assert len closure == len freevars f_locals update name cell cell_contents name cell zip freevars closure FrameInfo fn __code__ fn __globals__ f_locals builtins __dict__ closure=fn __closure__ type ignore arg-type argdefs=fn __defaults__ fullgraph_capture mod Any args tuple Any kwargs Optional dict str Any = None constraints Optional list Constraint = None _is_export_deprecated_do_not_use bool = False - CaptureOutput This API captures full graph model given example inputs trace Specifically takes callable nn Module method function args optional kwargs returns Dynamo-captured graph along other important compile-time information This serves common graph-capture mechanism different torch compiler AOT frontends e g AOT precompile export Note API doesn t apply context managers like metrics context expectation caller will apply them depending use case The CaptureOutput separated into two parts Frontend specific information which includes - guards - generated bytecode - other information tracked OutputGraphCommon Backend specific information indexed unique backend id such - fx graph - example inputs frame = _get_frame mod args kwargs compile_context CompileContext get_compile_id _fullgraph_capture_frame frame constraints=constraints _is_export_deprecated_do_not_use=_is_export_deprecated_do_not_use dataclass FrameInfo code types CodeType globals dict str object locals dict str object builtins dict str object closure tuple CellType argdefs Optional tuple Any _fullgraph_capture_frame frame FrameInfo constraints Optional list Constraint = None _is_export_deprecated_do_not_use bool = False - CaptureOutput torch _guards TracingContext backend_input Optional BackendInput = None fullgraph_compiler gm torch fx GraphModule example_inputs list torch Tensor - torch fx GraphModule nonlocal backend_input tracing_context = TracingContext get fake_mode = tracing_context fake_mode tensor_to_context = tracing_context tensor_to_context assert fake_mode None assert isinstance gm meta backend_id str backend_input = BackendInput gm meta backend_id gm example_inputs fake_mode tensor_to_context gm try dynamo_output = compile_frame frame code frame globals frame locals frame builtins frame closure compiler_fn=fullgraph_compiler export=_is_export_deprecated_do_not_use export_constraints=constraints type ignore arg-type one_graph=True restart_reasons=set https github com pytorch pytorch blob main torch _dynamo eval_frame py#L except Unsupported e augment_exc_message e config verbose raise strip internal tracebacks causes cur_exn BaseException = e while cur_exn __cause__ None cur_exn __cause__ with_traceback None cur_exn = cur_exn __cause__ pyrefly ignore invalid-inheritance raise e with_traceback None e __cause__ User compiler error CaptureOutput dynamo_output graph_capture_output frame argdefs backend_input compile_frame type ignore code types CodeType globals dict str object locals dict str object builtins dict str object closure tuple CellType compiler_fn CompilerFn one_graph bool restart_reasons set str export bool = False export_constraints Optional typing Never = None frame_state Optional dict str Union int FrameStateSizeEntry = None distributed_state Optional DistributedState = None package Optional CompilePackage = None pyrefly ignore bad-return - DynamoOutput A helper function taking frame backend then generated bytecode guards common data structure This shared interface multiple compiler frontends e g torch compile torch export needs capture graph out python code This shared across restarts speculation_log = SpeculationLog transform instructions list Instruction code_options dict str object - DynamoTracerOutput tf_mode_stack list torch overrides TorchFunctionMode = torch overrides _get_current_function_mode_stack tracer_output = trace_frame code globals locals builtins closure compiler_fn tf_mode_stack one_graph speculation_log instructions code_options export=export export_constraints=export_constraints frame_state=frame_state distributed_state=distributed_state package=package assert tracer_output None tracer_output last_attempt_start_time = None attempt itertools count CompileContext get attempt = attempt try dynamo_timed f compile_attempt_ attempt log_pt _compile_event=True bytecode tracer_output = transform_code_object code transform assert tracer_output None DynamoOutput tracer_output=tracer_output bytecode=bytecode last_attempt_start_time=last_attempt_start_time except exc RestartAnalysis e isinstance e exc TensorifyScalarRestartAnalysis TensorifyState clear log info Restarting analysis due s LazyString format_traceback_short e __traceback__ If restart reason None just log type exception restart_reasons add e restart_reason str type e We now have new last attempt reset clock last_attempt_start_time = time time attempt unimplemented_v gb_type= Excessive RestartAnalysis calls context= explanation= Dynamo attempted trace same frame + times Giving up compiling compile time tradeoff likely worth performance gain hints= except exc SkipFrame e isinstance e exc TensorifyScalarRestartAnalysis TensorifyState clear log debug noqa G Skipping frame s s \ s s e code co_name code co_filename code co_firstlineno raise _compile code CodeType globals dict str object locals dict str object builtins dict str object closure tuple CellType compiler_fn CompilerFn one_graph bool export bool export_constraints Optional typing Never hooks Hooks cache_entry Optional CacheEntry cache_size CacheSizeRelevantForFrame frame Optional DynamoFrameType = None frame_state Optional dict str Union int FrameStateSizeEntry = None compile_id CompileId skip int = package Optional CompilePackage = None Can used record things caller both case normal exception code paths convert_frame_box Optional ConvertFrameBox = None - ConvertFrameReturn torch _inductor async_compile async_compile_pool_manager torch fx experimental validator BisectValidationException ValidationException Only nonlocal defs here please Time spent compiling frame before restarting failing analysis dynamo_time_before_restart float = compile_time_strobelight_meta phase_name= compile_inner compile_inner code CodeType one_graph bool hooks Hooks - tuple ConvertFrameReturn Optional DynamoTracerOutput contextlib ExitStack stack stack enter_context torch _dynamo callback_handler install_callbacks CallbackTrigger DYNAMO str CompileContext current_compile_id stack enter_context CompileTimeInstructionCounter record _compile_inner code one_graph hooks ConvertFrameReturn None dead see https github com python mypy issues maybe_cprofile _compile_inner code CodeType one_graph bool hooks Hooks - tuple ConvertFrameReturn DynamoTracerOutput nonlocal dynamo_time_before_restart last_attempt_start_time = start_time = time time log_bytecode prefix str name str filename str line_no int code CodeType - None bytecode_log isEnabledFor logging DEBUG bytecode_log debug format_bytecode prefix name filename line_no code log_bytecode ORIGINAL BYTECODE code co_name code co_filename code co_firstlineno code out_code = None try dynamo_output = compile_frame code globals locals builtins closure compiler_fn one_graph restart_reasons export=export export_constraints=export_constraints frame_state=frame_state distributed_state=distributed_state package=package except exc SkipFrame e one_graph log debug No graph captured export fullgraph=True assert e _torch_dynamo_tracer_output None ConvertFrameReturn e _torch_dynamo_tracer_output assert distributed_state None distributed_state all_states None type ignore has-type compiler collective wasn t run before compilation completed out_code = dynamo_output bytecode tracer_output = dynamo_output tracer_output dynamo_output last_attempt_start_time None last_attempt_start_time = dynamo_output last_attempt_start_time assert out_code None log_bytecode MODIFIED BYTECODE code co_name code co_filename code co_firstlineno out_code idx hook enumerate _bytecode_hooks values dynamo_timed f bytecode_hooks_ idx log_pt _compile_event=True hook_output = hook code out_code hook_output None out_code = hook_output orig_code_map out_code = code output_codes add out_code dynamo_time_before_restart = last_attempt_start_time - start_time assert tracer_output output_graph None output = tracer_output output_graph Tests new code objects The rationale these tests can found torch csrc dynamo eval_frame c Only test once code object created They tested during runtime count_args code CodeType - int inspect code co_argcount + code co_kwonlyargcount + bool code co_flags inspect CO_VARARGS + bool code co_flags inspect CO_VARKEYWORDS assert out_code None total_argcount_old = count_args code total_argcount_new = count_args out_code msg = arg mismatch msg += f old code object has args code co_varnames total_argcount_old msg += f new code object has args out_code co_varnames total_argcount_new assert code co_varnames total_argcount_old == out_code co_varnames total_argcount_new msg msg = free var mismatch msg += f old code object has free var code co_freevars msg += f new code object has free var out_code co_freevars assert code co_freevars == out_code co_freevars msg msg = cell var mismatch msg += f old code object has cell var code co_cellvars msg += f new code object has cell var out_code co_cellvars assert code co_cellvars == out_code co_cellvars msg Skipping Dynamo frame without any extracted graph This does affect eager functionality But necessary export cases where Dynamo-reconstructed bytecode can create new function frames confusing export thinking there extra graphs now output export output is_empty_graph ConvertFrameReturn tracer_output assert output guards None CleanupManager instance out_code = output cleanups nonlocal cache_entry dynamo_timed build_guards log_pt _compile_event=True check_fn = dynamo_output build_guards code hooks=hooks save=package None cache_entry=cache_entry package None assert check_fn guards_state None package add_guarded_code check_fn guards_state out_code package add_inlined_source output tracing_context traced_code package update_device_type output current_tracer graph compile_id_str = str compile_id compile_id None Unknown annotation_str = Torch-Compiled Region + compile_id_str guarded_code = GuardedCode out_code check_fn guard_manager type ignore arg-type compile_id annotation_str output is_empty_graph hooks guard_export_fn None We should run guard_export_fn when Dynamo does generate any graph This can happen export when TorchDynamo generated bytecode has some reconstruction logic mutated variables which can trigger TorchDynamo children frames they benign do generate any new graphs hooks guard_export_fn output guards wrap_guarded_code guarded_code tracer_output metrics_context = get_metrics_context code_context = package code_context code package None contextlib nullcontext _use_lazy_graph_module config use_lazy_graph_module compile_context CompileContext compile_id async_compile_pool_manager chromium_event_timed dynamo reset_event_log_on_exit=True log_pt _compile_event=True _WaitCounter pytorch wait_counter entire_forward_compile guard metrics_context dynamo_timed _compile compile_inner phase_name= entire_frame_compile dynamo_compile_column_us= dynamo_cumulative_compile_time_us code_context restart_reasons set str = set compile_pg = get_compile_pg distributed_state = DistributedState compile_pg LocalState distributed_state = None Check recompilations recompile_reason Optional str = None is_recompilation cache_size frame reasons = get_and_maybe_log_recompilation_reasons cache_entry frame recompile_reason = Unable find recompilation reasons reasons reasons Recheck recompilation when inline_inbuilt_nn_modules set False inline_inbuilt_nn_modules_candidate = False config inline_inbuilt_nn_modules frame inbuilt_nn_reasons = get_and_maybe_log_recompilation_reasons cache_entry frame skip_logging=True inbuilt_nn_recompile_reason = None inbuilt_nn_reasons inbuilt_nn_reasons inbuilt_nn_recompile_reason None inline-inbuilt-nn-modules-candidate inbuilt_nn_recompile_reason inline_inbuilt_nn_modules_candidate = True Set recompile candidate inline_inbuilt_nn_modules regardless whether inline_inbuilt_nn_modules set metrics_context update_outer recompile_reason recompile_reason inline_inbuilt_nn_modules_candidate inline_inbuilt_nn_modules_candidate recompile_user_contexts = get_hook_for_recompile_user_context recompile_user_contexts cap each user context N chars data retention purposes N= chosen large enough capture most important info user_contexts_msg = user_context user_context recompile_user_contexts metrics_context set recompile_user_contexts user_contexts_msg exceeded limit_type = exceeds_recompile_limit cache_size compile_id exceeded format_func_info code CodeType - str f code co_name code co_filename code co_firstlineno NS Don t add period end string ll added URL rendering incorrect log warning torch _dynamo hit config s s \n function s\n last reason s\n To log all recompilation reasons use TORCH_LOGS= recompiles \n To diagnose recompilation issues see s limit_type getattr config limit_type format_func_info code recompile_reason troubleshooting_url config fail_on_recompile_limit_hit raise FailOnRecompileLimitHit f limit_type reached because fail_on_recompile_limit_hit = True HARD failure one_graph raise FailOnRecompileLimitHit f limit_type reached fullgraph=True Excessive recompilations can degrade performance due compilation overhead each recompilation To monitor recompilations enable TORCH_LOGS=recompiles If recompilations expected consider increasing torch _dynamo config cache_size_limit appropriate value justknobs_check pytorch compiler skip_code_recursive_on_recompile_limit_hit raise RecompileLimitExceeded f limit_type reached do recursively skip frames unimplemented_v gb_type= Dynamo cache limit exceeded context=f Limit type limit_type explanation= Dynamo attempted recompile code object too many times f exceeding limit_type cache size limit Giving up compiling compile time tradeoff likely worth performance gain hints= log debug torchdynamo start compiling s s s stack elided s frames \n s code co_name code co_filename code co_firstlineno skip + - omit current frame omit contextlib decorator join CapturedTraceback extract skip= + skip format - - above plus trace_structured frames NB frame looks like handled skip argument torch _dynamo convert_frame py catch_errors torch _dynamo convert_frame py _convert_frame torch _dynamo convert_frame py _convert_frame_assert torch _utils_internal py wrapper_function current frame context lib env lib python contextlib py inner torch _dynamo convert_frame py _compile extra here torch _logging _internal py trace_structured torch _dynamo convert_frame py lambda stack_trace = log_dynamo_start code skip start_time_ns = time time_ns fail_type Optional str = None fail_reason Optional str = None exception_stack_trace Optional list str = None fail_user_frame_filename Optional str = None fail_user_frame_lineno Optional int = None torch _dynamo utils ReinplaceCounters clear guarded_code = None tracer_output = None try guarded_code tracer_output = compile_inner code one_graph hooks NB We only put_code_state success case Success case here does include graph breaks specifically graph break still resulted partially compiled graph we WILL here An Unsupported exception will only bubble top level we unable compile frame all In case there s no point uploading code state because we will always fail exactly same way even without update It s useful upload graph break though because can prevent extra graph break compilations put_code_state tracer_output output_graph = tracer_output output_graph output_graph has_outputs log_frame_dynamic_whitelist code recompile_reason size mismatch index recompile_reason _log_size_mismatch_recompile guarded_code except Exception e NB e s msg mutated here add user stack we DON T want stack Scuba logged fail_reason So we grab fail info here add metrics context below fail_type = type e __qualname__ fail_reason = str e exception_stack_trace = traceback format_exc exception_handler e code frame export=export NB post-mutation exception torch _logging trace_structured artifact metadata_fn=lambda name dynamo_error encoding string payload_fn=lambda traceback format_exc fail_user_frame_filename fail_user_frame_lineno = exc get_exc_message e compile_id tracer_output = getattr e _torch_dynamo_tracer_output None isinstance e Unsupported TorchRuntimeError BackendCompilerFailed AssertionError ConstraintViolationError GuardOnDataDependentSymNode ValidationException UncapturedHigherOrderOpError BisectValidationException ShortenTraceback PackageError ResumePrologueTracingError raise Rewrap clarity raise InternalTorchDynamoError f type e __qualname__ str e with_traceback e __traceback__ None finally === WARNING WARNING WARNING === If you commit bug here will suppress writing dynamo_compile table we will have telemetry Be extra careful when making changes here torch _dynamo config run_gc_after_compile dynamo_timed gc dynamo_compile_column_us= gc_time_us log info run_gc_after_compile running gc gc collect output = None tracer_output output = tracer_output output_graph output output local_scope = tracer should already None keep extra check here just case tracer = output root_tx tracer f_locals = utils curr_frame frame_key = str curr_frame fail_reason None output None guard_count = len output guards shape_env_guard_count = len output shape_env guards graph_op_count = output count_calls graph_node_count = len output graph nodes graph_node_shapes = output get_graph_sizes_structured graph_input_count = len output placeholders non_compliant_ops = op __qualname__ op output non_compliant_ops compliant_custom_ops = op __qualname__ op output compliant_custom_ops torch _dynamo utils ReinplaceCounters log guard_count = None shape_env_guard_count = None graph_op_count = None graph_node_count = None graph_node_shapes = graph_input_count = None non_compliant_ops = set compliant_custom_ops = set restart_reasons = set If compilation failed entire time wasted dynamo_time_before_restart = time time_ns - start_time_ns e metrics = frame_key frame_key co_name code co_name co_filename code co_filename co_firstlineno code co_firstlineno cache_size cache_size num_cache_entries_with_same_id_matched_objs accumulated_cache_size cache_size num_cache_entries guard_count guard_count shape_env_guard_count shape_env_guard_count graph_op_count graph_op_count graph_node_count graph_node_count graph_input_count graph_input_count fail_type fail_type fail_reason fail_reason fail_user_frame_filename fail_user_frame_filename fail_user_frame_lineno fail_user_frame_lineno non_compliant_ops non_compliant_ops compliant_custom_ops compliant_custom_ops restart_reasons restart_reasons dynamo_time_before_restart_s dynamo_time_before_restart has_guarded_code guarded_code None specialize_float config specialize_float is_forward True dynamo_compile_time_before_restart_us to_int_us dynamo_time_before_restart stack_trace stack_trace graph_node_shapes str graph_node_shapes exception_stack_trace exception_stack_trace TODO replace CompileEventLogger compilation_metrics There some columns here PT Compile Events so we need slightly change metrics_context update_outer metrics === END WARNING WARNING WARNING === If tracer available then tracer error_on_graph_break reflects value global symbolic_convert error_on_graph_break time graph break - symbolic_convert error_on_graph_break may have been correctly changed during cleanup If tracer unavailable then fallback symbolic_convert error_on_graph_break convert_frame_box convert_frame_box error_on_graph_break = tracer_output error_on_graph_break tracer_output _get_error_on_graph_break ConvertFrame __init__ compiler_fn CompilerFn hooks Hooks package Optional CompilePackage = None - None _torchdynamo_orig_backend = compiler_fn _inner_convert = convert_frame_assert compiler_fn one_graph=False package=package _hooks = hooks property _clone_with_backend - Callable WrapBackendDebug ConvertFrame lambda backend convert_frame backend _hooks __call__ frame DynamoFrameType cache_entry Optional CacheEntry hooks Hooks frame_state dict str Union int FrameStateSizeEntry skip int = - ConvertFrameReturn input_codes add frame f_code counters frames total += try result = _inner_convert frame cache_entry hooks frame_state skip=skip + counters frames ok += result except Exception e Do allow errors suppressed we re tracing resume function prologue isinstance e ResumePrologueTracingError raise error_on_graph_break = _inner_convert _box error_on_graph_break None assert error_on_graph_break None _inner_convert _box error_on_graph_break NOTE we _might_ have wrap current custom exception order correctly bubble up top-level compile wrapper eval_frame py But re-raising seems work now because exceptions tracing nested call results top-level frame compile will handled caller observed exception - we don t expect exception suppressed raise These two exception types soft failure sense we know due something we didn t implement all way scare user less about That being said you trying understand why graph break happened s still important have information so offer NB NotImplementedError used list actually impossible reach here converted into InternalTorchDynamoError This behavior seemed reasonable me ezyang Aug so I kept maybe some point someone wanted these also get suppressed If so you ll need make these exceptions get wrapped We intentionally don t want suppress error here isinstance e UncapturedHigherOrderOpError raise soft_fail = isinstance e Unsupported This soft failure In sense code path reaches here when we do support graph breaks bytecodes like LOAD_ATTR BUILD_SET etc In such case we can fallback eager without scaring users soft_fail graph_break_log isEnabledFor logging DEBUG Log message graph break Also use string skip tell whole frame falling back eager hasattr e compile_id hasattr e real_stack compile_context CompileContext e compile_id type ignore attr-defined user_stack = e real_stack user_stack_formatted = join traceback format_list user_stack user_stack_trace = f Graph break skip user code \n user_stack_formatted torch _logging trace_structured artifact metadata_fn=lambda name dynamo_graph_break_reason encoding string payload_fn=lambda f user_stack_trace \n traceback format_exc graph_break_log debug user_stack_trace exc_info=True config suppress_errors soft_fail raise Suppress error NB It s very important do suppression logging HERE where actual suppression happens Previously somewhere so possible accidentally log all record_filename = getattr e record_filename None code = frame f_code error_msg = format_error_msg e code record_filename frame soft_fail log info error_msg exc_info=True log warning error_msg exc_info=True isinstance e SkipCodeRecursiveException ConvertFrameReturn frame_exec_strategy=FrameExecStrategy FrameAction SKIP FrameAction SKIP isinstance e RecompileLimitExceeded ConvertFrameReturn frame_exec_strategy=FrameExecStrategy FrameAction RUN_ONLY FrameAction RUN_ONLY ConvertFrameReturn convert_frame compiler_fn CompilerFn hooks Hooks package Optional CompilePackage = None - ConvertFrame Try convert frame into FX graph error leave frame unmodified ConvertFrame compiler_fn hooks package=package TODO mlazos add support same args record them replay filename str - None backends debugging eager original_replay_val = config replay_record_enabled config replay_record_enabled = False open filename rb in_file record = ExecutionRecord load in_file record globals = dict itertools chain record globals items globals items decorators error_on_graph_break False try _compile record code record globals record locals record builtins record closure compiler_fn=eager one_graph=False export=False export_constraints=None hooks=Hooks cache_size=CacheSizeRelevantForFrame cache_entry=None frame=None frame_state= compile_id=CompileId frame_id= frame_compile_id= finally config replay_record_enabled = original_replay_val first_real_inst_idx code CodeType - int sys version_info inst dis get_instructions code inst opname == RESUME inst offset raise RuntimeError RESUME instruction found code ConvertFrameProtocol typing Protocol __call__ frame DynamoFrameType cache_entry Optional CacheEntry hooks Hooks frame_state dict str Union int FrameStateSizeEntry skip int = - ConvertFrameReturn should_skip_due_to_torch_dispatch_mode - bool is_in_any_mode_without_ignore_compile_internals CatchErrorsWrapper __init__ callback ConvertFrameProtocol hooks Hooks - None functools wraps callback _torchdynamo_orig_backend = callback hooks = hooks __call__ frame DynamoFrameType cache_entry Optional CacheEntry frame_state dict str Union int FrameStateSizeEntry - ConvertFrameReturn assert frame_state None input_codes add frame f_code is_skipfile = trace_rules check frame f_code sys version_info = has_started_execution = frame f_lasti first_real_inst_idx frame f_code has_started_execution = frame f_lasti = first_real_inst_idx frame f_code TODO first condition covered any test has_started_execution is_skipfile config disable should_skip_due_to_torch_dispatch_mode getattr _torchdynamo_orig_backend _export False log isEnabledFor logging DEBUG has_started_execution skip_reason = traced frame already trace_rules check frame f_code skip_reason = skipfiles is_in_torch_dispatch_mode include_infra_modes=False skip_reason = non-infra torch dispatch mode present supported today torch compile skip_reason = dynamo tracing disabled log debug skipping s reason s file s frame f_code co_name skip_reason frame f_code co_filename ConvertFrameReturn frame f_code co_filename == string frame f_code co_name == __new__ frame f_code co_filename endswith collections __init__ py frame f_code co_name == _make nametuple constructor _make ConvertFrameReturn torch _dynamo utils get_optimize_ddp_mode == ddp_optimizer ddp_module = DistributedDataParallel _get_active_ddp_module ddp_module compile_lock torch _dynamo backends distributed DDPOptimizer ddp_optimizer = DDPOptimizer bucket_bytes_cap=ddp_module bucket_bytes_cap backend_compile_fn=self _torchdynamo_orig_backend _torchdynamo_orig_backend type ignore attr-defined assert hasattr _torchdynamo_orig_backend _clone_with_backend DDPOptimizer only supports callback fns know how clone themselves hijacked_callback = _torchdynamo_orig_backend _clone_with_backend ddp_optimizer compile_fn hijacked_callback frame cache_entry hooks frame_state compile_lock _disable_current_modes skip= skip frame result = _torchdynamo_orig_backend frame cache_entry hooks frame_state skip= result catch_errors_wrapper callback ConvertFrameProtocol hooks Hooks - CatchErrorsWrapper CatchErrorsWrapper callback hooks