mypy allow-untyped-defs dataclasses dataclass typing cast Optional TYPE_CHECKING Union torch torch distributed dist torch distributed _shard sharded_tensor metadata sharded_tensor_meta torch distributed distributed_c d distributed_c d torch distributed _shard _utils narrow_tensor torch distributed _shard metadata ShardMetadata torch distributed _shard sharded_tensor shard Shard torch distributed _shard sharded_tensor utils _parse_and_validate_remote_device _internals get_chunked_dim_size get_split_size api ShardingSpec TYPE_CHECKING Only include ShardedTensor when do type checking exclude run-time resolve circular dependency torch distributed _shard sharded_tensor ShardedTensor dataclass ChunkShardingSpec ShardingSpec This type PlacementSpec defines placement being sharded across multiple devices In particular represents sharding Tensor along single dimension into equal chunks similar meth ` torch chunk ` The semantics how tensor partitioned inline meth ` torch chunk ` where ` ` dim ` ` torch chunk corresponds specified ` ` dim ` ` ` ` chunks ` ` torch chunk number elements placement specified Args dim int str The dimension shard could integer representing dimension string case named tensors where dimensions named Note named tensor support added yet placement List Union _remote_device str Specifies placement each shard Tensor The size list represents number shards created This could list ` torch distributed _remote_device ` s This list could also contain string which represents remote device accepted ` torch distributed _remote_device ` ShardingDim = Union int str dim ShardingDim placements list Union torch distributed _remote_device str __post_init__ _verify_dim dim i remote_device enumerate placements isinstance remote_device torch distributed _remote_device placements i = torch distributed _remote_device remote_device staticmethod _verify_dim dim Validate sharding spec TODO support named dimension isinstance dim str raise NotImplementedError ChunkShardingSpec does support named dimension yet isinstance dim int raise ValueError f Sharding dim needs integer found dim build_metadata tensor_sizes torch Size tensor_properties sharded_tensor_meta TensorProperties - sharded_tensor_meta ShardedTensorMetadata tensor_num_dim = len tensor_sizes _verify_dim dim dim = tensor_num_dim dim -tensor_num_dim type ignore operator raise ValueError f Invalid sharding dim dim shards_metadata = sharding_dim_size = tensor_sizes dim type ignore index chunks = len placements split_size = get_split_size sharding_dim_size chunks idx placement enumerate placements generate ShardMetadata each placement device chunked_dim_size = get_chunked_dim_size sharding_dim_size split_size idx shard_size = list tensor_sizes current_offsets = tensor_num_dim current_offsets dim = split_size idx type ignore index shard_size dim = chunked_dim_size type ignore index shard_metadata = ShardMetadata shard_offsets=current_offsets shard_sizes=shard_size placement=placement shards_metadata append shard_metadata sharded_tensor_meta ShardedTensorMetadata shards_metadata tensor_sizes tensor_properties shard tensor torch Tensor src_rank int = process_group=None - ShardedTensor Args src_rank group rank relative ` ` process_group ` ` N B If ` ` process_group ` ` None ` ` src_rank ` ` global rank relative imports avoid circular dependency torch distributed _shard sharded_tensor ShardedTensor tensor_properties = sharded_tensor_meta TensorProperties dtype=tensor dtype layout=tensor layout requires_grad=tensor requires_grad memory_format=torch contiguous_format pin_memory=tensor is_pinned current_rank = dist get_rank process_group current_global_rank = dist get_rank tensor_meta = build_metadata tensor size tensor_properties local_shards = local_tensor = None local_metadata = None tensors_to_scatter = cast list Optional torch Tensor None dist get_world_size process_group sharding_dim_size = tensor size dim type ignore index chunks = len placements split_size = get_split_size sharding_dim_size chunks scatter_shape = list tensor size scatter_shape dim = split_size type ignore index shard_meta tensor_meta shards_metadata remote_global_rank device = _parse_and_validate_remote_device process_group shard_meta placement current_rank == src_rank Reshape get shard rank we don t want autograd recording here narrow op local_shard should leaf variable autograd graph narrowed_tensor = narrow_tensor tensor shard_meta shard_meta shard_sizes dim split_size type ignore index last shard might smaller other shards resize narrowed tensor same size use scatter collective dist scatter requires same size inputs every rank tensor_to_scatter = narrowed_tensor detach clone resize_ scatter_shape tensor_to_scatter = narrowed_tensor detach clone memory_format=torch contiguous_format tensors_to_scatter pyrefly ignore bad-argument-type dist get_group_rank process_group remote_global_rank = tensor_to_scatter current_global_rank == remote_global_rank local_tensor = torch empty scatter_shape dtype=tensor dtype layout=tensor layout device=device local_metadata = shard_meta each rank should have local_tensor local_metadata initialized we build metadata list correct way assert local_tensor None assert local_metadata None Scatter shards all ranks pg scatter takes global rank ` ` src ` ` src_for_scatter = src_rank process_group None process_group distributed_c d _get_default_group src_for_scatter = distributed_c d get_global_rank process_group src_for_scatter tensors_to_scatter_ Optional list torch Tensor = None current_rank == src_rank tensors_to_scatter_ = t tensors_to_scatter assert isinstance t torch Tensor tensors_to_scatter_ append t dist scatter local_tensor scatter_list=tensors_to_scatter_ src=src_for_scatter group=process_group list local_tensor size = local_metadata shard_sizes detach again after receiving ensure local shards remain leaf node local_tensor = local_tensor resize_ local_metadata shard_sizes detach Sync requires_grad local_shard local_tensor requires_grad = tensor requires_grad local_shards append Shard tensor=local_tensor metadata=local_metadata st = ShardedTensor _init_from_local_shards_and_global_metadata local_shards tensor_meta process_group=process_group Manually set sharding_spec st _sharding_spec = st