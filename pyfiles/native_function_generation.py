__future__ annotations string collections defaultdict typing TYPE_CHECKING torchgen api dispatcher dispatcher torchgen api translate translate torchgen api types Binding DispatcherSignature Expr torchgen context with_native_function torchgen model Annotation Argument BackendIndex BackendMetadata BaseOperatorName BaseTy BaseType DEFAULT_KERNEL_NAMESPACE DeviceCheckType DispatchKey FunctionSchema NativeFunction NativeFunctionsGroup OperatorName Return SchemaKind Variant torchgen utils concatMap TYPE_CHECKING collections abc Sequence See Note Out ops functional variants don t get grouped properly OUT_OPS_THAT_DONT_GET_GROUPED_PROPERLY = This has functional variant s currently marked private This function should marked private well _backward ops aren t exposed python anyway adaptive_avg_pool d_backward grad_input There s functional variant _slow_conv d_backward output_mask isn t grouped properly Maybe we can kill operator favor convolution_backward _slow_conv d_backward grad_input See Note Mutable ops cannot get out variant MUTABLE_OPS_THAT_CANNOT_GET_AN_OUT_VARIANT = should out= _cummax_helper should out= _cummin_helper All these operators don t have any tensor like returns FUNCTIONAL_OPS_THAT_CANNOT_GET_AN_OUT_VARIANT = _assert_async no _assert_async msg no _assert_tensor_metadata no _cslt_sparse_mm_search returns int _assert_scalar no _dimI returns int _dimV returns int _has_same_storage_numel returns boolean _linalg_check_errors no _local_scalar_dense returns Scalar _nested_tensor_from_mask_left_aligned returns boolean _nnz returns int _use_cudnn_ctc_loss returns boolean _use_cudnn_ctc_loss Tensor returns boolean _validate_compressed_sparse_indices no allclose returns boolean dense_dim returns int equal returns boolean is_coalesced returns boolean is_pinned returns boolean is_same_size returns boolean is_set_to returns boolean q_per_channel_axis returns int q_scale returns float q_zero_point returns int qscheme returns QScheme record_stream no sparse_dim returns int sym_constrain_range no sym_constrain_range_for_size no _nested_tensor_storage_offsets returns vector ints _chunk_grad_outputs_efficient_attention returns bool _fused_sdp_choice returns int _print no _sink_tokens no _nested_get_ragged_idx returns int INPLACE_OPS_THAT_DONT_GET_GROUPED_PROPERLY = polygamma polygamma out both exist have pre-self arg while polygamma_ does We should either fix schema so can grouped properly allow codegen generate new functional out= NativeFunctions op which would require changing its overload name prevent overload ambiguity polygamma_ Groups similar NativeFunctions together example add Tensor add_ Tensor add out similar NativeFunctions all expected have identical ` signature ` But have differing SchemaKinds pre_group_native_functions native_functions Sequence NativeFunction - dict FunctionSchema dict SchemaKind NativeFunction pre_grouped_native_functions dict FunctionSchema dict SchemaKind NativeFunction = defaultdict dict f native_functions d = pre_grouped_native_functions f func signature assert f func kind d d f func kind = f pre_grouped_native_functions Returns out variant overload name given base function overload name get_expected_out_variant_overload_name overload_name str &#124; None - str out overload_name f overload_name _out Helper function given inplace FunctionSchema generate its corresponding out= variant Example before _add_relu_ Scalar Tensor Scalar other Scalar alpha= - Tensor Example after _add_relu Scalar_out Tensor Scalar other Scalar alpha= Tensor out self_to_out_signature func FunctionSchema - FunctionSchema Generating out= schema inplace schema assert func kind == SchemaKind inplace assert func arguments self_arg None The new out= schema has - new out argument same type func mutable annotation - The returns any now alias out= argument instead func - out overload name FunctionSchema name=func name remove_inplace with_overload get_expected_out_variant_overload_name func name overload_name arguments=func arguments remove_self_annotation with_out_args Argument name= out type=func arguments self_arg argument type default=None annotation=func arguments self_arg argument annotation returns=func returns Helper function given functional FunctionSchema generate its corresponding out= variant Example before _to_copy Tensor ScalarType dtype=None Layout layout=None Device device=None bool pin_memory=None bool non_blocking=False MemoryFormat memory_format=None - Tensor Example after _to_copy _out Tensor bool non_blocking=False MemoryFormat memory_format=None Tensor out - Tensor functional_to_out_signature func FunctionSchema - FunctionSchema Generating out= schema functional schema assert func kind == SchemaKind functional new_returns new_out_args = generate_out_args_from_schema func The new out= schema has - one more new out argument s same type returns mutable annotation - The returns now alias out= arguments - _out overload name FunctionSchema name=func name with_overload get_expected_out_variant_overload_name func name overload_name arguments=func arguments signature with_out_args new_out_args returns=tuple new_returns Helper function given function schema generate corresponding out arguments also updated annotations generate_out_args_from_schema func FunctionSchema - tuple list Return list Argument More sanity check - our existing restrictions schemas should enforce mutable schema kinds never their mutable arguments assert any r annotation None r annotation is_write r func returns tensorlike_rets = r r func returns r type is_tensor_like assert len tensorlike_rets used_annotations = concatMap lambda annotation None annotation alias_set func arguments flat_all valid_annotations = x x string ascii_lowercase x used_annotations all_rets_are_tensors = all r type == BaseType BaseTy Tensor r func returns new_out_args list Argument = The end result new_returns - If every plain tensor then new returns == old returns out= alias annotations added - Otherwise none out arguments show up returns we re only left non-tensor-like returns any new_returns list Return = i r enumerate func returns r type is_tensor_like new_out = Argument name= out len func returns == f out i type=r type default=None annotation=Annotation parse f valid_annotations i new_out_args append new_out all_rets_are_tensors The convention out= schemas they only their out arguments plain Tensor s tuple plain Tensors new_ret = Return name=None type=new_out type annotation=new_out annotation new_returns append new_ret new_returns append r new_returns new_out_args Helper function given mutable FunctionSchema generate its corresponding out= variant Example before _fused_moving_avg_obs_fq_helper Tensor Tensor observer_on Tensor fake_quant_on Tensor running_min Tensor b running_max Tensor c scale Tensor d zero_point float averaging_const int quant_min int quant_max int ch_axis bool per_row_fake_quant=False bool symmetric_quant=False - Tensor output Tensor mask noqa B Example after _fused_moving_avg_obs_fq_helper _out Tensor Tensor observer_on Tensor fake_quant_on Tensor running_min Tensor b running_max Tensor c scale Tensor d zero_point float averaging_const int quant_min int quant_max int ch_axis bool per_row_fake_quant=False bool symmetric_quant=False Tensor e out Tensor f out - Tensor e Tensor f noqa B mutable_to_out_signature func FunctionSchema - FunctionSchema Generating out= schema mutable schema assert func kind == SchemaKind mutable The new out= schema has - Any non-aliased tensor-like returns converted mutable aliased out= arguments argument tensor then we also method chaining otherwise we nothing - out overload name Note This also means we can only generate out= variant mutable schema mutable schema has least one tensor-like non-aliasing The generated out= variant still has mutable positional arguments necessary we could probably add another out= variant also functionalizes mutable arguments functional_out variant new_returns new_out_args = generate_out_args_from_schema func FunctionSchema name=func name remove_inplace with_overload get_expected_out_variant_overload_name func name overload_name arguments=func arguments with_out_args new_out_args returns=tuple new_returns This function given function one SchemaKind well target SchemaKind generates new NativeFunction same properties using target SchemaKind We only actually generate functions either functional out= SchemaKinds This function returns tuple - The generated NativeFunction - dictionary ` BackendIndex ` objects describing which dispatch keys we will generate kernels new NativeFunction Details function we only generate composite kernels some cases today generate_function f NativeFunction k SchemaKind - tuple NativeFunction dict DispatchKey dict OperatorName BackendMetadata torchgen api cpp k == SchemaKind functional assert f func kind = SchemaKind functional The new functional NativeFunction has - any mutable arguments have been converted into immutable returns mutable argument also gets converted one - _functional appended base name ONLY IF op has mutable variant See Note Overload Ambiguity With Functional Variants The default grouping logic signature actually already does so we can piggy-back off we still want names func = f func signature keep_return_names=True with_name OperatorName name=BaseOperatorName base=f func name name base inplace=False dunder_method=f func name name dunder_method See Note Overload Ambiguity With Functional Variants functional_overload=f func kind == SchemaKind mutable overload_name=f func name overload_name k == SchemaKind out We generate out= ops mostly just so we can pair up NativeFunctions into groups easily least today there no good reason actually use them we ll generate dispatcher entry them won t actually register any kernels them f func kind == SchemaKind inplace func = self_to_out_signature f func f func kind == SchemaKind mutable func = mutable_to_out_signature f func f func kind == SchemaKind functional func = functional_to_out_signature f func raise AssertionError We only bother generating out= functions either inplace mutable functional variants raise AssertionError We currently only generate either functional out= NativeFunctions Generated kernel naming convention out op_name _ overload_name The reason disambiguate operator same name different overload name e g ` randn names_out ` ` randn generator_with_names_out ` kernel_name = func name unambiguous_name func kind == SchemaKind out cpp name func f func has_symint kernel_name += _symint backend_metadata = DispatchKey CompositeExplicitAutograd func name BackendMetadata kernel=kernel_name structured=False cpp_namespace=DEFAULT_KERNEL_NAMESPACE tags = generated &#124; set f tags nondeterministic_seeded view_copy pt _compliant_tag NativeFunction func=func use_const_ref_for_mutable_tensors=f use_const_ref_for_mutable_tensors These generated fn s aren t meant user friendly- don t generate methods variants= Variant function structured=False structured_delegate=None structured_inherits=None precomputed=None autogen= ufunc_inner_loop= manual_kernel_registration=False manual_cpp_binding=False python_module=None category_override=None device_guard=False device_check=DeviceCheckType NoCheck loc=f loc cpp_no_default_args=set is_abstract=f is_abstract has_composite_implicit_autograd_kernel=False has_composite_implicit_autograd_nested_tensor_kernel=False has_composite_explicit_autograd_kernel=True has_composite_explicit_autograd_non_functional_kernel=False Every generated NativeFunction gets generated tag so s easy tell which NativeFunction objects did come directly native_functions yaml tags=tags namespace=f namespace backend_metadata This function responsible adding generated NativeFunctions which don t appear explicitly codegen You can inspect full list NativeFunctions yourself torchgen package running torchgen parse_native_yaml aten src ATen native native_functions yaml aten src ATen native tags yaml Maybe we should make friendly API Note function mutates its two inputs adding new NativeFunctions BackendMetadata them add_generated_native_functions rs list NativeFunction indices dict DispatchKey dict OperatorName BackendMetadata - None The main code generating new NativeFunctions First we group NativeFunctions schema kind then we detect which ones missing generate them pre_grouped_native_functions = pre_group_native_functions rs d pre_grouped_native_functions values has_functional = SchemaKind functional d has_inplace = SchemaKind inplace d has_mutable = SchemaKind mutable d has_out = SchemaKind out d is_core = any core variant tags variant d values We automatically generate few native functions don t exist yaml few reasons If operator has inplace out= variant no functional variant we can generate simple functional variant functionalization pass can consume If operator has inplace functional no out= variant we generate out= variant mostly so we can easily pair up functions into NativeFunctionsGroup while maintaining constraint out= variant required has_mutable has_inplace has_out has_functional Don t bother generating functions trio s native functions bypass dispatcher are_manual = all f manual_cpp_binding f d values Don t bother generating functional + out= variants view operators set_ technically inplace_view now treated normal inplace op codegen has_view_ops = any f is_view_op str f func name name = set_ f d values Don t generate other variants non-core CompositeImplicitAutograd operators We could probably do main benefit generating function triplets transforms need them transforms don t need act directly CompositeImplicitAutograd operators since we let them decompose are_composite_implicit = all f has_composite_implicit_autograd_kernel f d values are_manual has_view_ops are_composite_implicit is_core continue has_out len d values == Note Out ops functional variants don t get grouped properly In theory we could validly have out= operator native_functions yaml has no other variants But today all operators where s case actually do have functional variants we just unable pair up properly I think banning all together probably safer you can always add functional variant yourself you want add new out= operator We should probably fix existing cases check prevent us adding more over time str d SchemaKind out func name OUT_OPS_THAT_DONT_GET_GROUPED_PROPERLY raise AssertionError f Found out= operator we could find any other variants str d SchemaKind out func continue Some inplace ops have problematic schemas we should fix which prevent us generating out= functional variants has_inplace str d SchemaKind inplace func name INPLACE_OPS_THAT_DONT_GET_GROUPED_PROPERLY continue base_fn = d SchemaKind mutable has_mutable d SchemaKind inplace has_inplace d SchemaKind out has_out d SchemaKind functional Note Mutable ops cannot get out variant We can only generate out= variant either - original function has tensor-like returns since we can convert them out kwargs - s inplace since we can convert ` ` out kwarg There only two functions don t fit criteria today though they both look like they should fixed out= variants so feels safer ban schema all-together base_fn_valid = base_fn func kind == SchemaKind inplace any r type is_tensor_like r base_fn func returns Note Loosen assertion all functional should have out variant By design all functional operators should have our variants The needs_out check loosening requirement changing only generate out variant there s ` autogen ` block native function long run should removed FIXME Remove after figuring out CI job failures related min max mean needs_out = any out str op_name op_name base_fn autogen gets_out_variant = has_out base_fn_valid needs_out has_out base_fn_valid str base_fn func name MUTABLE_OPS_THAT_CANNOT_GET_AN_OUT_VARIANT str base_fn func name FUNCTIONAL_OPS_THAT_CANNOT_GET_AN_OUT_VARIANT raise AssertionError f Found operator we could generate out= variant str base_fn func This type operators don t have tensor-like making difficult generate proper out= variant If out= variant needed please add function name into FUNCTIONAL_OPS_THAT_CANNOT_GET_AN_OUT_VARIANT list Generate out= variant gets_out_variant fn metadata = generate_function base_fn SchemaKind out d SchemaKind out = fn BackendIndex grow_index indices metadata rs append fn Generate functional variant only do operator got out= variant Functional variants only useful we can group up variants which we can only do they have out= variant has_functional has_out gets_out_variant fn metadata = generate_function base_fn SchemaKind functional d SchemaKind functional = fn BackendIndex grow_index indices metadata rs append fn return_str rets tuple Return names list str - str assert len rets == len names len rets == len rets == f names f dispatcher returns_type rets cpp_type join names Given function name variable corresponding output function gather up all individual returns aliased gather_nonaliased_inner_rets func FunctionSchema out_var str - list str aliased_rets = func aliased_return_names non_aliased_names = is_out_var_a_tuple = len func returns i r enumerate aliased_rets r None non_aliased_names append f std get i out_var is_out_var_a_tuple out_var non_aliased_names Generates functional kernels terms their inplace mutable counterparts We only do generated NativeFunctions with_native_function gen_composite_functional_kernel g NativeFunctionsGroup - str &#124; None We should only generating these code-generated NativeFunctions generated g functional tags None And we always write kernel generated op terms non-generated op g inplace None generated g inplace tags target_f = g inplace g mutable None generated g mutable tags target_f = g mutable We should guaranteed have valid inplace mutable variant call into See Note Mutable Ops Not Using Functionalization raise AssertionError str g functional func sig = DispatcherSignature g functional func target_sig = DispatcherSignature target_f func context list Binding &#124; Expr = clone_mutable_inputs = cloned_return_names = We can t just directly pass all arguments functional op into mutating op We need check which inputs mutating operator mutable clone those inputs first a_curr a_tgt zip dispatcher jit_arguments g functional func dispatcher jit_arguments target_f func a_tgt annotation None a_tgt annotation is_write clone_mutable_inputs append f auto a_curr name _clone = clone_arg a_curr name context append Expr expr=f a_curr name _clone type=dispatcher argument_type a_curr binds=a_curr name Invariant mutable arguments inner mutable op always returns functional op cloned_return_names append f a_curr name _clone context append dispatcher argument a_curr exprs = join e expr e translate context target_sig arguments out_name = output maybe_assign = f auto out_name = len target_f func returns inner_return_names = gather_nonaliased_inner_rets target_f func out_name ret_str = return_str g functional func returns inner_return_names + cloned_return_names clone_mutable_inputs_str = \n join clone_mutable_inputs f sig defn name=sig name + _symint g out func has_symint clone_mutable_inputs_str maybe_assign _ops target_f func name unambiguous_name call exprs ret_str Generates out= kernels terms their functional counterparts We only do generated NativeFunctions with_native_function gen_composite_out_kernel g NativeFunctionsGroup - str &#124; None We should only generating these code-generated NativeFunctions generated g out tags None And we always write kernel out= op terms functional Note functional op might have also been generated we don t have worry about cycles because generated functional kernels always implemented terms non-generated kernels see gen_composite_functional_kernel sig = DispatcherSignature g out func target_sig = DispatcherSignature g functional func exprs = join e expr e translate sig arguments target_sig arguments copy_outs = out_name = tmp_output i out_arg enumerate g out func arguments out functional_return_name = out_name len g functional func returns == f std get i out_name copy_outs append f \ resize_out_helper out_arg name functional_return_name copy_arg out_arg name functional_return_name rets = For each arg calling out= operator If corresponds aliased input input Otherwise corresponding output calling functional operator i ret_name enumerate g out func aliased_return_names ret_name None rets append ret_name functional_return_name = out_name len g functional func returns == f std get i out_name rets append functional_return_name copy_outs_str = \n join copy_outs Kernel name needs follow naming convention defined ` generate_function ` f sig defn name=g out func name unambiguous_name + _symint g out func has_symint auto out_name = _ops g functional func name unambiguous_name call exprs copy_outs_str return_str g out func returns rets