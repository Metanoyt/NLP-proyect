Owner s oncall distributed logging typing Any torch torch distributed dist torch distributed checkpoint dist_cp torch distributed checkpoint _extension ZStandard torch distributed checkpoint metadata ChunkStorageMetadata MetadataIndex TensorProperties torch distributed checkpoint planner TensorWriteData WriteItem WriteItemType torch distributed device_mesh init_device_mesh torch distributed tensor distribute_tensor DTensor Replicate Shard zeros torch distributed tensor _shards_wrapper LocalShardsWrapper torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests torch testing _internal distributed _tensor common_dtensor DTensorTestBase skip_if_lt_x_gpu with_comms torch testing _internal distributed checkpoint_utils get_test_extension_registry Rot Example with_temp_dir logger = logging getLogger __name__ logger setLevel logging INFO CHECKPOINT_DIR = checkpoint ONE_D_PLACEMENTS = Shard Replicate ONE_D_TO_ONE_D_PLACEMENTS = Replicate Shard Shard Replicate TWO_D_PLACEMENTS = Replicate Replicate Replicate Shard Shard Replicate Shard Shard TWO_D_TO_TWO_D_PLACEMENTS = p TWO_D_PLACEMENTS p TWO_D_PLACEMENTS p = p TWO_D_TO_TWO_D_PLACEMENTS append p p instantiate_parametrized_tests TestDTensorReshardPlacementChange DTensorTestBase Test DCP reshard DTensor placements changes without world_size change mesh_tensor change with_comms skip_if_lt_x_gpu with_temp_dir parametrize extensions None Rot Example ZStandard test_ d_to_ d_reshard_placement_change extensions - None CHECKPOINT_DIR = temp_dir one_d_to_one_d_placements ONE_D_TO_ONE_D_PLACEMENTS original_placement new_placement = one_d_to_one_d_placements global_tensor = torch arange dtype=torch float view mesh_shape = world_size device_mesh = init_device_mesh device_type mesh_shape dtensor = distribute_tensor global_tensor device_mesh placements=original_placement state_dict_to_save = dtensor dtensor dist_cp save state_dict=state_dict_to_save storage_writer=dist_cp FileSystemWriter path=CHECKPOINT_DIR _extensions=extensions planner=dist_cp DefaultSavePlanner zero_dtensor = zeros device_mesh=device_mesh placements=new_placement state_dict_to_load = dtensor zero_dtensor dist_cp load state_dict=state_dict_to_load storage_reader=dist_cp FileSystemReader CHECKPOINT_DIR _extension_registry=get_test_extension_registry planner=dist_cp DefaultLoadPlanner materialzie whole tensor compare original global_tensor state_dict_to_load dtensor = state_dict_to_load dtensor redistribute device_mesh placements= Replicate assertEqual global_tensor state_dict_to_load dtensor to_local redistribute tensor back its original placement comparison state_dict_to_load dtensor = state_dict_to_load dtensor redistribute device_mesh placements=original_placement assertEqual state_dict_to_save dtensor to_local state_dict_to_load dtensor to_local with_comms skip_if_lt_x_gpu with_temp_dir test_ d_to_ d_reshard_placement_change - None CHECKPOINT_DIR = temp_dir two_d_to_two_d_placements TWO_D_TO_TWO_D_PLACEMENTS original_placement new_placement = two_d_to_two_d_placements global_tensor = torch arange dtype=torch float view mesh_shape = world_size mesh_ d = init_device_mesh device_type mesh_shape dtensor = distribute_tensor global_tensor mesh_ d placements=original_placement state_dict_to_save = dtensor dtensor dist_cp save state_dict=state_dict_to_save storage_writer=dist_cp FileSystemWriter path=CHECKPOINT_DIR planner=dist_cp DefaultSavePlanner zero_dtensor = zeros device_mesh=mesh_ d placements=new_placement state_dict_to_load = dtensor zero_dtensor dist_cp load state_dict=state_dict_to_load storage_reader=dist_cp FileSystemReader CHECKPOINT_DIR planner=dist_cp DefaultLoadPlanner state_dict_to_load dtensor = state_dict_to_load dtensor redistribute mesh_ d placements= Replicate Replicate assertEqual global_tensor state_dict_to_load dtensor to_local state_dict_to_load dtensor = state_dict_to_load dtensor redistribute mesh_ d placements=original_placement assertEqual state_dict_to_save dtensor to_local state_dict_to_load dtensor to_local TestDTensorReshardMeshChange DTensorTestBase Test DCP reshard DTensor placements changes mesh_tensor change with_comms with_temp_dir skip_if_lt_x_gpu test_ d_to_ d_reshard_mesh_change - None CHECKPOINT_DIR = temp_dir placements_ d ONE_D_PLACEMENTS global_tensor = torch arange dtype=torch float view mesh_shape = world_size mesh_ d = init_device_mesh device_type mesh_shape dtensor = distribute_tensor global_tensor mesh_ d placements=placements_ d state_dict_to_save = dtensor dtensor dist_cp save state_dict=state_dict_to_save storage_writer=dist_cp FileSystemWriter path=CHECKPOINT_DIR planner=dist_cp DefaultSavePlanner placements_ d TWO_D_PLACEMENTS mesh_shape = world_size mesh_ d = init_device_mesh device_type mesh_shape zero_dtensor = zeros device_mesh=mesh_ d placements=placements_ d state_dict_to_load = dtensor zero_dtensor dist_cp load state_dict=state_dict_to_load storage_reader=dist_cp FileSystemReader CHECKPOINT_DIR planner=dist_cp DefaultLoadPlanner materialzie whole tensor compare original global_tensor state_dict_to_load dtensor = state_dict_to_load dtensor redistribute mesh_ d placements= Replicate Replicate assertEqual global_tensor state_dict_to_load dtensor to_local with_comms with_temp_dir skip_if_lt_x_gpu test_ d_to_ d_reshard_mesh_change - None CHECKPOINT_DIR = temp_dir placements_ d TWO_D_PLACEMENTS global_tensor = torch arange dtype=torch float view mesh_shape = world_size mesh_ d = init_device_mesh device_type mesh_shape dtensor = distribute_tensor global_tensor mesh_ d placements=placements_ d state_dict_to_save = dtensor dtensor dist_cp save state_dict=state_dict_to_save storage_writer=dist_cp FileSystemWriter path=CHECKPOINT_DIR planner=dist_cp DefaultSavePlanner placements_ d ONE_D_PLACEMENTS mesh_shape = world_size mesh_ d = init_device_mesh device_type mesh_shape zero_dtensor = zeros device_mesh=mesh_ d placements=placements_ d state_dict_to_load = dtensor zero_dtensor dist_cp load state_dict=state_dict_to_load storage_reader=dist_cp FileSystemReader CHECKPOINT_DIR planner=dist_cp DefaultLoadPlanner materialzie whole tensor compare original global_tensor state_dict_to_load dtensor = state_dict_to_load dtensor redistribute mesh_ d placements= Replicate assertEqual global_tensor state_dict_to_load dtensor to_local with_comms with_temp_dir skip_if_lt_x_gpu test_dtensor_checkpoint_resharding_with_empty_shard Test dtensor checkpoint resharding dtensor containing empty shards tensor = torch rand device_type mesh = init_device_mesh device_type world_size dtensor = distribute_tensor tensor mesh Shard ref_state_dict = dtensor dtensor dist_cp save state_dict=ref_state_dict storage_writer=dist_cp FileSystemWriter path=self temp_dir tensor = torch rand device_type mesh_ = init_device_mesh device_type world_size dtensor = distribute_tensor tensor mesh_ Shard Shard state_dict = dtensor dtensor dist_cp load state_dict=state_dict storage_reader=dist_cp FileSystemReader temp_dir with_comms with_temp_dir skip_if_lt_x_gpu test_dtensor_checkpoint_with_uneven_shards - None Saving dtensor uneven shards rank - rank - rank - rank - CHECKPOINT_DIR = temp_dir mesh_shape = world_size mesh_ = init_device_mesh device_type mesh_shape my_rank = dist get_rank Make last shard uneven my_rank == world_size - local_tensor = torch arange start=my_rank end= my_rank + dtype=torch float view local_tensor = torch arange start=my_rank end= my_rank + dtype=torch float view dtensor = DTensor from_local local_tensor mesh_ Shard run_check=True shape=torch Size stride=torch Size state_dict_to_save = uneven_sharded_dtensor dtensor dist_cp save state_dict=state_dict_to_save storage_writer=dist_cp FileSystemWriter path=CHECKPOINT_DIR planner=dist_cp DefaultSavePlanner loading_full_tensor = torch rand dtype=torch float device= cpu print f rank my_rank loading_dtensor load \n loading_full_tensor state_dict_to_load = uneven_sharded_dtensor loading_full_tensor re-sharding load dist_cp load state_dict=state_dict_to_load storage_reader=dist_cp FileSystemReader temp_dir CheckpointableDistTensor torch Tensor A distributed checkpointable tensor representation Unlike Dtensor representation cannot used distributed training Supports distributed tensor save loads has uneven shards DTensor cannot support same _local_tensor torch Tensor _shard_offsets torch Size _overall_size torch Size staticmethod __new__ cls fqn str local_tensor torch Tensor shard_offsets list int overall_size list int - CheckpointableDistTensor r = torch Tensor _make_wrapper_subclass cls overall_size dtype=local_tensor dtype device=local_tensor device layout=local_tensor layout r _fqn = fqn r _local_tensor = local_tensor r _shard_offsets = torch Size shard_offsets r _overall_size = torch Size overall_size r __init__ args kwargs super __init__ classmethod __torch_dispatch__ cls func types args= kwargs=None type ignore override raise NotImplementedError f func supported CheckpointableDistTensor __create_chunk_list__ ChunkStorageMetadata offsets=self _shard_offsets sizes=self _local_tensor size __create_write_items__ fqn str object Any - list WriteItem WriteItem index=MetadataIndex fqn=self _fqn offset=self _shard_offsets type=WriteItemType SHARD tensor_data=TensorWriteData chunk=ChunkStorageMetadata offsets=self _shard_offsets sizes=self _local_tensor size properties=TensorProperties create_from_tensor _local_tensor size=self _overall_size __get_tensor_shard__ index MetadataIndex - torch Tensor assert _fqn == index fqn _shard_offsets == index offset _local_tensor __repr__ f CheckpointableDistributedTensor f fqn= _fqn f local_tensor= _local_tensor f shard_offset= _shard_offset f overall_size= _overall_size TestCheckpointableReshard DTensorTestBase Test DCP reshard loads when shard sizes uneven across ranks with_comms with_temp_dir test_uneven_reshard_with_checkpointable_api - None Saves d distributed tensor has shards uneven sizes using Checkpointable API Loads them back different shard plan resharding By default UT runs NUM_DEVICES = saving_ d_shard_plan = offset length tuples loading_ d_shard_plan = CHECKPOINT_DIR = temp_dir my_rank = dist get_rank saving_shard_offset saving_shard_length = saving_ d_shard_plan my_rank saving_local_tensor = torch arange start=saving_shard_offset end=saving_shard_offset + saving_shard_length dtype=torch float view saving_shard_length logger info f my_rank saving_local_tensor saving_local_tensor noqa G saving_cp_dist_tensor = CheckpointableDistTensor fqn= checkpointable_tensor local_tensor=saving_local_tensor shard_offsets= saving_shard_offset overall_size= state_dict_to_save = checkpointable_tensor saving_cp_dist_tensor dist_cp save state_dict=state_dict_to_save storage_writer=dist_cp FileSystemWriter path=CHECKPOINT_DIR planner=dist_cp DefaultSavePlanner loading_shard_offset loading_shard_length = loading_ d_shard_plan my_rank loading_local_tensor = torch rand loading_shard_length dtype=torch float logger info f my_rank loading_local_tensor initialized random vals loading_local_tensor noqa G expected_loaded_local_val_tensor = torch arange start=loading_shard_offset end=loading_shard_offset + loading_shard_length dtype=torch float view loading_shard_length loading_cp_dist_tensor = CheckpointableDistTensor fqn= checkpointable_tensor local_tensor=loading_local_tensor shard_offsets= loading_shard_offset overall_size= state_dict_to_load = checkpointable_tensor loading_cp_dist_tensor dist_cp load state_dict=state_dict_to_load storage_reader=dist_cp FileSystemReader temp_dir assert torch equal loading_local_tensor expected_loaded_local_val_tensor with_comms with_temp_dir test_uneven_reshard_with_dtensor_shards_wrapper_api - None Saves d distributed tensor has shards uneven sizes using Checkpointable API Loads them back different shard plan resharding By default UT runs NUM_DEVICES = NB saving shardin plan loading sharding plans different their shard lengths uneven saving_ d_shard_plan = offset length tuples loading_ d_shard_plan = cp_path = temp_dir my_rank = dist get_rank d device mesh CPU device mesh_shape = world_size device_mesh = init_device_mesh cpu mesh_shape saving_shard_offset saving_shard_length = saving_ d_shard_plan my_rank saving_local_tensor = torch arange start=saving_shard_offset end=saving_shard_offset + saving_shard_length dtype=torch float view saving_shard_length In order support uneven shards we have wrap original shards LocalShardsWrapper saving_local_shard_wrapper = LocalShardsWrapper local_shards= saving_local_tensor local_offsets= saving_shard_offset logger info f my_rank saving_local_shard_warpper saving_local_shard_wrapper noqa G saving_cp_dist_tensor = DTensor from_local local_tensor=saving_local_shard_wrapper device_mesh=device_mesh placements= Shard shape=torch Size stride=torch Size put DTensor state dict call DCP save state_dict_to_save = checkpointable_tensor saving_cp_dist_tensor dist_cp save state_dict=state_dict_to_save storage_writer=dist_cp FileSystemWriter path=cp_path planner=dist_cp DefaultSavePlanner loading_shard_offset loading_shard_length = loading_ d_shard_plan my_rank loading_local_tensor = torch rand loading_shard_length dtype=torch float device= cpu loading_local_shard_wrapper = LocalShardsWrapper local_shards= loading_local_tensor local_offsets= loading_shard_offset expected_loaded_local_val_tensor = torch arange start=loading_shard_offset end=loading_shard_offset + loading_shard_length dtype=torch float view loading_shard_length loading_cp_dist_tensor = DTensor from_local local_tensor=loading_local_shard_wrapper device_mesh=device_mesh placements= Shard shape=torch Size stride=torch Size state_dict_to_load = checkpointable_tensor loading_cp_dist_tensor dist_cp load state_dict=state_dict_to_load storage_reader=dist_cp FileSystemReader path=cp_path logger info f my_rank loaded_shards_wrapper loading_local_shard_wrapper noqa G assert torch equal loading_local_tensor expected_loaded_local_val_tensor dist barrier TODO Add dtensor resharding test when world size changes __name__ == __main__ run_tests