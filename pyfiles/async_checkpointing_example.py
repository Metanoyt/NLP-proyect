mypy allow-untyped-defs Owner s oncall distributed os shutil traceback concurrent futures Future torch torch distributed dist torch distributed checkpoint dcp torch multiprocessing mp torch nn nn torch nn functional F torch distributed checkpoint state_dict _patch_model_state_dict _patch_optimizer_state_dict torch distributed fsdp FullyShardedDataParallel FSDP torch distributed tensor device_mesh init_device_mesh DEVICE = cuda NUM_EPOCHS = SAVE_PERIOD = FAULT_PERIOD = CHECKPOINT_DIR = f ~ os environ get LOGNAME checkpoint InjectedException Exception pass Model torch nn Module __init__ - None super __init__ net = nn Linear net = nn Linear net = nn Linear net = nn Linear net = nn Linear forward x x = F relu net x x = F relu net x x = F relu net x x = F relu net x x = F sigmoid net x x _init_model rank world_size device_mesh = init_device_mesh DEVICE world_size Create dummy model wrap FSDP model = Model cuda device_mesh = init_device_mesh DEVICE world_size model = FSDP model device_mesh=device_mesh use_orig_params=True optim = torch optim Adam model parameters lr= _patch_model_state_dict model pyrefly ignore bad-argument-type _patch_optimizer_state_dict model optimizers=optim model optim _print msg dist get_rank == print msg _input x = torch rand device= cuda y = torch zeros device= cuda y torch sum x dim= = = x y run rank world_size Set up world pg os environ MASTER_ADDR = localhost os environ MASTER_PORT = dist init_process_group cpu gloo cuda nccl rank=rank world_size=world_size torch cuda set_device rank model optim = _init_model rank world_size state_dict = model model optim optim loss_calc = torch nn BCELoss f = None pyrefly ignore bad-assignment epoch range NUM_EPOCHS try torch manual_seed epoch x y = _input loss = loss_calc model x y _print f epoch= loss= loss backward optim step optim zero_grad epoch SAVE_PERIOD == f None isinstance f Future raise AssertionError f should Future instance f result f = dcp state_dict_saver async_save state_dict checkpoint_id=CHECKPOINT_DIR FAULT_PERIOD epoch FAULT_PERIOD == raise InjectedException Fault injection except InjectedException e dist barrier _print Trainer encountered exception traceback print_tb e __traceback__ _print Reloading model last checkpoint f None isinstance f Future raise AssertionError f should Future instance None f result dcp load state_dict __name__ == __main__ world_size = torch cuda device_count print f Running example Async Checkpointing world_size devices shutil rmtree CHECKPOINT_DIR ignore_errors=True mp spawn run args= world_size nprocs=world_size join=True