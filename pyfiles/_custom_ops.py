mypy allow-untyped-defs inspect torch _custom_op impl _custom_op_with_schema _find_custom_op infer_schema parse_qualname validate_namespace torch library get_ctx __all__ = custom_op impl impl_abstract get_ctx impl_save_for_backward impl_backward custom_op qualname func_or_schema=None r Register new custom operator In PyTorch defining op short operator two step-process - we need define op providing operator name schema - we need implement behavior how operator interacts various PyTorch subsystems like CPU CUDA Tensors Autograd etc This entrypoint defines custom operator first step you must then perform second step calling various ` ` impl_ ` ` APIs This API may used decorator see examples For detailed guide custom ops please see https docs google com document d aGWtgxV HppuxQAdddyPrs _aEntpkYt MalnCKnhk Arguments qualname str Should string looks like namespace operator_name Operators PyTorch need namespace avoid name collisions given operator may only created once If you writing Python library we recommend namespace name your top-level module func_or_schema Union Callable str Each PyTorch operator needs schema tells PyTorch types inputs outputs If Callable we will automatically infer schema type annotations function see examples Otherwise you don t want use type annotations you may provide us schema string Example xdoctest +REQUIRES env TORCH_DOCTEST_CUDA torch numpy np torch Tensor Step define custom op We need provide API prototype function function returns NotImplementedError which we will infer types inputs outputs torch _custom_ops custom_op mylibrary numpy_sin numpy_sin x Tensor - Tensor raise NotImplementedError The custom op now accessible via torch ops module torch ops mylibrary numpy_sin Step Register implementation various PyTorch subsystems Register implementation CPU tensors torch _custom_ops impl mylibrary numpy_sin device_types= cpu numpy_sin_impl_cpu x torch from_numpy np sin x numpy Register implementation CUDA tensors torch _custom_ops impl mylibrary numpy_sin device_types= cuda numpy_sin_impl_cuda x torch from_numpy np sin x cpu numpy x device x = torch randn torch ops mylibrary numpy_sin x calls numpy_sin_impl_cpu x_cuda = x cuda torch ops mylibrary numpy_sin x calls numpy_sin_impl_cuda ns name = parse_qualname qualname validate_namespace ns inner func inspect isfunction func raise ValueError f custom_op func Expected ` func ` Python f function got type func func __name__ = name raise ValueError f custom_op qualname= qualname func expected ` func ` f have name name got func __name__ f Please either change name ` func ` qualname f passed ` custom_op ` schema = infer_schema func mutates_args= _custom_op_with_schema qualname schema func func_or_schema None inner isinstance func_or_schema str _custom_op_with_schema qualname func_or_schema inner func_or_schema impl qualname device_types= cpu cuda func=None r Register implementation device type custom op If op passed multiple Tensor inputs different device types will dispatch registered implementation highest priority device type among those present The supported device types order priority cuda cpu This API may used decorator see examples For detailed guide custom ops please see https docs google com document d aGWtgxV HppuxQAdddyPrs _aEntpkYt MalnCKnhk Arguments device_types str Iterable str device type s register function Example xdoctest +REQUIRES env TORCH_DOCTEST_CUDA torch numpy np torch Tensor Step define custom op We need provide API prototype function function returns NotImplementedError which we will infer types inputs outputs torch _custom_ops custom_op mylibrary numpy_cos numpy_cos x Tensor - Tensor raise NotImplementedError The custom op now accessible via torch ops module torch ops mylibrary numpy_cos Step Register implementation various PyTorch subsystems Register implementation CPU tensors torch _custom_ops impl mylibrary numpy_cos device_types= cpu numpy_cos_impl_cpu x torch from_numpy np cos x numpy Register implementation CUDA tensors torch _custom_ops impl mylibrary numpy_cos device_types= cuda numpy_cos_impl_cuda x torch from_numpy np cos x cpu numpy x device x = torch randn torch ops mylibrary numpy_cos x calls numpy_cos_impl_cpu x_cuda = x cuda torch ops mylibrary numpy_cos x calls numpy_cos_impl_cuda inner func custom_op = _find_custom_op qualname also_check_torch_library=True custom_op impl device_types _stacklevel= func func func None inner inner func impl_abstract qualname func=None r Register abstract implementation operator An abstract implementation specifies behavior operator Tensors carry no data Given some input Tensors certain properties sizes strides storage_offset device specifies what properties output Tensors The abstract implementation has same signature operator It run both FakeTensors meta tensors To write abstract implementation assume all Tensor inputs operator regular CPU CUDA Meta tensors they do have storage you trying regular CPU CUDA Meta tensor s output The abstract implementation must consist only PyTorch operations may directly access storage data any input intermediate Tensors This API may used decorator see examples For detailed guide custom ops please see https docs google com document d aGWtgxV HppuxQAdddyPrs _aEntpkYt MalnCKnhk Examples numpy np torch Tensor Example operator without data-dependent output shape torch _custom_ops custom_op mylibrary custom_linear custom_linear x Tensor weight Tensor bias Tensor - Tensor raise NotImplementedError torch _custom_ops impl_abstract mylibrary custom_linear custom_linear_abstract x weight assert x dim == assert weight dim == assert bias dim == assert x shape == weight shape assert weight shape == bias shape assert x device == weight device x weight t + bias Example operator data-dependent output shape torch _custom_ops custom_op mylibrary custom_nonzero custom_nonzero x Tensor - Tensor torch _custom_ops impl_abstract mylibrary custom_nonzero custom_nonzero_abstract x Number nonzero-elements data-dependent Since we cannot peek data abstract impl we use ctx object construct new symint represents data-dependent size ctx = torch _custom_ops get_ctx nnz = ctx create_unbacked_symint shape = x dim nnz result = x new_empty shape dtype=torch long result torch _custom_ops impl mylibrary custom_nonzero custom_nonzero_impl x x_np = to_numpy x res = np stack np nonzero x_np axis= unbacked symbolic ints PyTorch must = so we constrain range least res shape = raise RuntimeError supported torch tensor res device=x device torch library torch library register_fake qualname func _stacklevel= impl_save_for_backward qualname func=None r Register function tells us what save backward Please see func ` impl_backward ` more details inner func custom_op = _find_custom_op qualname also_check_torch_library=True custom_op impl_save_for_backward _stacklevel= func func func None inner inner func impl_backward qualname output_differentiability=None func=None r Registers backward formula operator In order operator work autograd you need register backward formula There two pieces You must give us function specify what save backward Call save backward function You must give us function computes gradients Call backward function Use ` impl_save_for_backward ` define save backward function specifies what gets saved backward The function should accept two arguments ` ` inputs output ` ` quantities saved backward During runtime when you call operator forwards pass PyTorch will invoke save backward function inputs output operator Use ` impl_backward ` define backward function The backward function must accept ` ` ctx saved grads ` ` - ` ` ctx ` ` context object where we may provide information - ` ` saved ` ` exactly what gets returned save backward function - ` ` grads ` ` one more gradients The number gradients matches number outputs operator The backward function must dict maps name input operator its corresponding gradient All inputs declared Tensors operator definition must accounted dict The gradient may Tensor None For detailed guide custom ops please see https docs google com document d aGWtgxV HppuxQAdddyPrs _aEntpkYt MalnCKnhk inner func custom_op = _find_custom_op qualname also_check_torch_library=True custom_op impl_backward output_differentiability _stacklevel= func func func None inner inner func _destroy qualname De-registers custom op For testing purposes only custom_op = _find_custom_op qualname custom_op _destroy