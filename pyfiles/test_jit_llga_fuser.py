Owner s module mkldnn sys torch unittest itertools torch nn nn functools wraps concurrent futures torch nn functional F torch fx experimental optimization optimization torch testing _internal jit_utils JitTestCase torch testing _internal common_utils run_tests TEST_SCIPY IS_WINDOWS IS_MACOS torch testing _internal common_device_type instantiate_device_type_tests onlyCPU dtypes We use wrapper run UTs TorchVision models because memory-leak issue JIT tracing causes traced model objects persist memory Ref https github com pytorch pytorch issues Memory requirement running these UTs thus increasing cumulatively invoked Linux kernel OOM killer linux xlarge PyTorch CI runners which only have GB RAM Cumulatively these UTs had been using more than GB memory per psutils So now we run each TorchVision model UTs separate processes separate_process func wraps func wrapper args kwargs futures ProcessPoolExecutor executor future = executor submit func args kwargs futures wait future wrapper is_avx _supported sys platform = linux False open proc cpuinfo encoding= ascii f lines = f read avx lines IS_AVX _UNSUPPORTED = is_avx _supported LLGA_FUSION_GROUP = prim oneDNNFusionGroup LLGA_NOT_ENABLED = torch backends mkldnn is_available IS_WINDOWS IS_MACOS warmup_forward f args profiling_count= _ range profiling_count results = f args results JitLlgaTestCase JitTestCase setUp PyTorch has divergent op support AMP JIT eager modes so we disable AMP JIT leverage eager-mode AMP Ref https github com pytorch pytorch issues original_autocast_mode = torch _C _jit_set_autocast_mode False torch jit enable_onednn_fusion True tearDown torch jit enable_onednn_fusion False torch _C _jit_set_autocast_mode original_autocast_mode checkTrace m x dtype=torch float args kwargs isinstance m torch nn Module m eval torch no_grad torch _jit_internal _disable_emit_hooks dtype == torch bfloat We rely upon eager-mode AMP support BF torch autocast device_type= cpu cache_enabled=False dtype=torch bfloat traced = torch jit trace m x isinstance m torch nn Module traced = torch jit freeze traced warmup_forward traced x ref_o = m x fwd_graph = traced graph_for x traced = torch jit trace m x isinstance m torch nn Module traced = torch jit freeze traced warmup_forward traced x ref_o = m x fwd_graph = traced graph_for x jit_o = traced x assertEqual jit_o ref_o traced fwd_graph assertFused graph fused_patterns pat fused_patterns assertGraphContainsExactly graph pat findFusionGroups graph result = n graph nodes n kind == LLGA_FUSION_GROUP result append n g Subgraph continue block n blocks result += findFusionGroups block result checkPatterns graph patterns fusion_groups = findFusionGroups graph assert len fusion_groups == len patterns length subgraphs equal length given patterns i range len fusion_groups pattern patterns i assertGraphContains fusion_groups i pattern try torchvision HAS_TORCHVISION = True except ImportError HAS_TORCHVISION = False except RuntimeError HAS_TORCHVISION = False skipIfNoTorchVision = unittest skipIf HAS_TORCHVISION no torchvision get_eltwise_fn name hasattr torch name getattr torch name hasattr F name getattr F name name == hardswish_ torch nn Hardswish inplace=True raise NameError f Eltwise function name found unittest skipIf IS_AVX _UNSUPPORTED This test fails BF machines without AVX unittest skipIf LLGA_NOT_ENABLED MKL-DNN build disabled TestOp JitLlgaTestCase onlyCPU dtypes torch float torch bfloat test_conv d dtype spatial in_channels out_channels kernel padding stride dilation g bias itertools product True False m = nn Conv d in_channels=in_channels g out_channels=out_channels g kernel_size=kernel padding=padding stride=stride dilation=dilation groups=g bias=bias x = torch rand in_channels g spatial spatial _ graph = checkTrace m x dtype assertGraphContainsExactly graph LLGA_FUSION_GROUP onlyCPU dtypes torch float torch bfloat test_bn d dtype m = nn BatchNorm d eval x = torch rand _ graph = checkTrace m x dtype single-op partition shouldn t created softmax assertGraphContainsExactly graph LLGA_FUSION_GROUP onlyCPU dtypes torch float torch bfloat test_eltwise dtype M nn Module __init__ eltwise_fn super __init__ eltwise = eltwise_fn forward x eltwise x eltwise relu gelu eltwise_fn = get_eltwise_fn eltwise m = M eltwise_fn x = torch rand _ graph = checkTrace m x dtype single-op partition shouldn t created assertGraphContainsExactly graph LLGA_FUSION_GROUP onlyCPU dtypes torch float torch bfloat test_max_pool d dtype spatial kernel padding stride dilation ceil_mode itertools product TODO fix issue pad calculation TODO backend support dilation True False m = nn MaxPool d kernel_size=kernel stride=stride padding=padding dilation=dilation ceil_mode=ceil_mode x = torch rand spatial spatial _ graph = checkTrace m x dtype assertGraphContainsExactly graph LLGA_FUSION_GROUP onlyCPU dtypes torch float torch bfloat test_avg_pool d dtype spatial kernel padding stride ceil_mode count_include_pad itertools product False TODO oneDNN Graph does fully support ceil_mode=True True False m = nn AvgPool d kernel_size=kernel stride=stride padding=padding ceil_mode=ceil_mode count_include_pad=count_include_pad x = torch rand spatial spatial _ graph = checkTrace m x dtype assertGraphContainsExactly graph LLGA_FUSION_GROUP onlyCPU dtypes torch float torch bfloat test_variable_kernel_avg_pool d dtype M nn Module forward x x = F avg_pool d x kernel_size= x size x size padding= count_include_pad=False x x = torch randn m = M _ graph = checkTrace m x dtype kernel_size Constant shouldn t have any LLGA_FUSION_GROUP TODO shape specialization should have LLGA_FUSION_GROUP assertGraphContainsExactly graph LLGA_FUSION_GROUP onlyCPU dtypes torch float torch bfloat test_softmax dtype dim - - - - m = nn Softmax dim=dim x = torch rand _ graph = checkTrace m x dtype single-op partition shouldn t created softmax assertGraphContainsExactly graph LLGA_FUSION_GROUP onlyCPU dtypes torch float torch bfloat test_linear dtype bias True False x = torch rand m = torch nn Linear in_features= out_features= bias=bias _ graph = checkTrace m x dtype assertGraphContainsExactly graph LLGA_FUSION_GROUP assertFused graph aten linear _gen_binary_inputs gen_permute=True xshape yshape yield torch rand xshape torch rand yshape gen_permute xshape = yshape yield torch rand yshape torch rand xshape onlyCPU dtypes torch float torch bfloat test_add dtype forward_add x y torch add x y alpha= x y _gen_binary_inputs _ graph = checkTrace forward_add x y dtype assertGraphContainsExactly graph LLGA_FUSION_GROUP onlyCPU dtypes torch float torch bfloat test_add_scalar dtype add_scalar x + x + x = torch rand _ graph = checkTrace add_scalar x dtype assertGraphContainsExactly graph LLGA_FUSION_GROUP onlyCPU dtypes torch float torch bfloat test_addmm dtype Just sidenote - comparison eager-mode oneDNN Graph JIT outputs addmm which entails matmul-bias-add fusion might require higher tolerance bounds BF This subject change near future addmm x y z alpha beta default torch addmm z x y x = torch rand y = torch rand z = torch rand _ graph = checkTrace addmm x y z dtype single-op partition should created matmul bias assertGraphContainsExactly graph LLGA_FUSION_GROUP onlyCPU dtypes torch float torch bfloat test_mul dtype forward_mul x y torch mul x y x y _gen_binary_inputs _ graph = checkTrace forward_mul x y dtype single-op partitions shouldn t created assertGraphContainsExactly graph LLGA_FUSION_GROUP onlyCPU dtypes torch float torch bfloat test_identity_binary dtype forward x x + x = torch rand _ graph = checkTrace forward x dtype assertFused graph aten add aten mul onlyCPU dtypes torch float torch bfloat test_layer_norm dtype TODO support more normalized_shape m = torch nn LayerNorm x = torch randn _ graph = checkTrace m x dtype assertGraphContainsExactly graph LLGA_FUSION_GROUP onlyCPU dtypes torch float torch bfloat test_cat dtype cat_along_dim d forward_cat inputs torch cat inputs d forward_cat xshape d range len xshape x = torch rand xshape _ graph = checkTrace cat_along_dim d x x x dtype assertGraphContainsExactly graph LLGA_FUSION_GROUP onlyCPU dtypes torch float torch bfloat test_typecheck dtype x = torch rand dtype=dtype m = torch nn Linear in_features= out_features= bias=True dtype=dtype traced graph = checkTrace m x dtype assertGraphContainsExactly graph LLGA_FUSION_GROUP assertFused graph aten linear change shape input we should enter fallback graph x = torch rand dtype=dtype assertEqual m x traced x unittest skipIf IS_AVX _UNSUPPORTED This test fails BF machines without AVX unittest skipIf LLGA_NOT_ENABLED MKL-DNN build disabled TestFusionPattern JitLlgaTestCase onlyCPU dtypes torch float torch bfloat test_conv d_eltwise dtype M nn Module __init__ eltwise_fn super __init__ conv = nn Conv d padding= bias=True conv = nn Conv d padding= bias=False eltwise = eltwise_fn forward x x = conv x x = eltwise x x = conv x x = eltwise x x eltwise relu leaky_relu sigmoid square abs exp hardswish tanh hardtanh inplace True False eltwise_fn_name = eltwise + _ inplace eltwise eltwise_fn = get_eltwise_fn eltwise_fn_name m = M eltwise_fn x = torch rand _ graph = checkTrace m x dtype=dtype assertGraphContainsExactly graph LLGA_FUSION_GROUP test relu_ replace relu mutation removal pass assertFused graph aten + eltwise_fn_name test relu fused into fusion group assertFused graph aten + eltwise onlyCPU dtypes torch float torch bfloat test_conv d_silu dtype M nn Module __init__ inplace super __init__ conv = nn Conv d padding= bias=True conv = nn Conv d padding= bias=True eltwise = nn SiLU inplace=inplace forward x x = conv x x = eltwise x x = conv x x inplace False True memory_format torch contiguous_format torch channels_last m = M inplace x = torch rand memory_format=memory_format _ graph = checkTrace m x dtype assertGraphContainsExactly graph LLGA_FUSION_GROUP oneDNN graph does have silu OP The bridge will convert silu sigmoid - mul Inplace op will become outplace op JIT graph patterns = aten _convolution aten sigmoid aten mul aten _convolution silu_op = aten silu_ inplace aten silu assertFused graph aten _convolution silu_op checkPatterns graph patterns onlyCPU dtypes torch float torch bfloat test_ensure_tensor_is_rewrapped dtype M nn Module __init__ eltwise_fn super __init__ conv = nn Conv d padding= bias=True conv = nn Conv d padding= bias=True conv = nn Conv d padding= bias=True conv = nn Conv d padding= bias=True eltwise = eltwise_fn adaptive_avg_pool_ d = nn AdaptiveAvgPool d forward x y x = conv x x = eltwise x x = conv x x = eltwise x y = conv y y = eltwise y y = conv y y = eltwise y x = torch add x y x = adaptive_avg_pool_ d x x eltwise_fn_name = relu eltwise_fn = get_eltwise_fn eltwise_fn_name m = M eltwise_fn m = m memory_format=torch channels_last x = torch rand memory_format=torch channels_last y = torch rand memory_format=torch channels_last Simply test output accurate The output second partition input adaptive_avg_pool d which unsupported LLGA In resnext x d we encountered accuracy issue _ graph = checkTrace m x y dtype assertGraphContainsExactly graph LLGA_FUSION_GROUP onlyCPU dtypes torch float torch bfloat test_conv d_clamp dtype M nn Module __init__ - None super __init__ conv = nn Conv d padding= bias=True conv = nn Conv d padding= bias=True conv = nn Conv d padding= bias=True conv = nn Conv d padding= bias=True conv = nn Conv d padding= bias=True forward x x = conv x x = torch clamp x min=float -inf x = conv x x = torch clamp x min=- x = conv x x = torch clamp x min= max=float inf x = conv x x = torch clamp x min= max= x = conv x x = torch clamp x max= x inplace False True noqa F memory_format torch contiguous_format torch channels_last x = torch rand memory_format=memory_format m = M _ graph = checkTrace m x dtype assertGraphContainsExactly graph LLGA_FUSION_GROUP assertFused graph aten _convolution aten clamp onlyCPU dtypes torch float torch bfloat test_conv d_bn dtype M nn Module __init__ - None super __init__ conv = nn Conv d padding= bias=True bn = nn BatchNorm d forward x x = conv x x = bn x x m = M eval dtype == torch bfloat m = optimization fuse m x = torch rand _ graph = checkTrace m x dtype assertGraphContainsExactly graph LLGA_FUSION_GROUP assertFused graph aten _convolution aten batch_norm onlyCPU dtypes torch float torch bfloat test_conv d_bn_relu dtype M nn Module __init__ - None super __init__ conv = nn Conv d padding= bias=True bn = nn BatchNorm d forward x x = conv x x = bn x x = F relu x x m = M eval dtype == torch bfloat m = optimization fuse m x = torch rand _ graph = checkTrace m x dtype assertGraphContainsExactly graph LLGA_FUSION_GROUP assertFused graph aten _convolution aten batch_norm aten relu onlyCPU dtypes torch float torch bfloat test_bn d_eltwise dtype M nn Module __init__ eltwise_fn super __init__ eltwise = eltwise_fn bn = nn BatchNorm d forward x x = bn x x = eltwise x x eltwise relu eltwise_fn = get_eltwise_fn eltwise m = M eltwise_fn eval x = torch rand _ graph = checkTrace m x dtype assertGraphContainsExactly graph LLGA_FUSION_GROUP assertFused graph aten + eltwise onlyCPU dtypes torch float torch bfloat test_linear_eltwise dtype M nn Module __init__ eltwise_fn bias super __init__ linear = nn Linear bias eltwise = eltwise_fn forward x x = linear x x = eltwise x x has_bias eltwise itertools product True False relu gelu sigmoid hardtanh relu elu eltwise_fn = get_eltwise_fn eltwise m = M eltwise_fn has_bias x = torch rand requires_grad=False _ graph = checkTrace m x dtype assertGraphContainsExactly graph LLGA_FUSION_GROUP assertFused graph aten + eltwise onlyCPU dtypes torch float torch bfloat test_conv d_sum dtype M nn Module __init__ bias=False super __init__ conv = nn Conv d padding= bias=bias bn = nn BatchNorm d conv = nn Conv d padding= bias=bias bn = nn BatchNorm d relu = nn ReLU conv = nn Conv d padding= bias=bias bn = nn BatchNorm d forward x y x = conv x x = bn x y = conv y y = bn y z = relu x + y z = conv z z = bn z z bias True False m = M bias eval dtype == torch bfloat m = optimization fuse m x = torch rand requires_grad=False y = torch rand requires_grad=False _ graph = checkTrace m x y dtype assertGraphContainsExactly graph LLGA_FUSION_GROUP onlyCPU dtypes torch float torch bfloat test_wildcard dtype M nn Module __init__ - None super __init__ conv = nn Conv d padding= bias=True eltwise = nn ReLU forward x x = conv x y = eltwise x x y The pattern following conv &#124; \ eltwise \ &#124; \ ListConstruct The output conv used wildcard op ListConstruct Thus conv-eltwise cannot selected into same Partition m = M x = torch rand _ graph = checkTrace m x dtype conv can exist single-op oneDNN Graph partition relu assertGraphContainsExactly graph LLGA_FUSION_GROUP assertFused graph aten _convolution onlyCPU dtypes torch int test_wildcard_unsupported_dtype dtype M nn Module forward x y = x y In shufflenet_v _x _ channels_per_groups computed channels_per_group = num_channels groups JIT IR converts groups Long dtype which unsupported oneDNN Graph viz Long requires_grad= device=cpu = prim Constant value= This test just ensures bridge code can handle unsupported dtypes inputs ops unsupported oneDNN Graph In particular UT aten floor_divide would added wildcard graph-construction stage m = M x = torch tensor dtype=dtype _ graph = checkTrace m x dtype assertGraphContainsExactly graph LLGA_FUSION_GROUP onlyCPU dtypes torch float torch bfloat test_rewrap_tensor_input_to_pytorch dtype M nn Module __init__ eltwise_fn super __init__ conv = nn Conv d padding= bias=True conv = nn Conv d padding= bias=True eltwise = eltwise_fn adaptive_avg_pool_ d = nn AdaptiveAvgPool d forward x y x = conv x x = eltwise x x = conv x x = eltwise x x = torch add x y x = adaptive_avg_pool_ d x x eltwise_fn_name = relu eltwise_fn = get_eltwise_fn eltwise_fn_name m = M eltwise_fn m = m memory_format=torch channels_last x = torch rand memory_format=torch channels_last y = torch rand memory_format=torch channels_last Simply test output accurate The output second partition input adaptive_avg_pool d which unsupported LLGA so must handled PyTorch which should receive correct strides info channels-last tensor checkTrace m x y dtype unittest skipIf LLGA_NOT_ENABLED MKL-DNN build disabled TestEnableDisableLlgaFuser JitTestCase setUp super setUp is_enabled = torch _C _jit_set_llga_enabled False tearDown torch _C _jit_set_llga_enabled is_enabled super tearDown test_context_manager x = torch randn y = torch randn torch jit fuser fuser torch jit fuser fuser t x y o = x + y o = o + o t_jit = torch jit script t t_jit x y t_jit x y assertGraphContains t_jit graph_for x y LLGA_FUSION_GROUP t x y o = x + y o = o + o t_jit_ = torch jit script t t_jit_ x y t_jit_ x y assertGraphContains t_jit_ graph_for x y LLGA_FUSION_GROUP t x y o = x + y o = o + o t_jit_ = torch jit script t t_jit_ x y t_jit_ x y assertGraphContainsExactly t_jit_ graph_for x y LLGA_FUSION_GROUP unittest skipIf LLGA_NOT_ENABLED MKL-DNN build disabled unittest skip Enable when integration dynamo aot_autograd more stable TestDynamoAOT JitTestCase test_dynamo_aot_ts_onednn Seq nn Module __init__ - None super __init__ layers = nn Sequential nn Linear nn ReLU nn Linear nn ReLU forward x layers x mod = Seq torch _dynamo aot_mod = torch compile mod backend= aot_ts fullgraph=True _ range torch jit fuser fuser loss = aot_mod torch rand sum loss backward torch _dynamo reset unittest skipIf IS_AVX _UNSUPPORTED This test fails BF machines without AVX unittest skipIf LLGA_NOT_ENABLED MKL-DNN build disabled TestModel JitLlgaTestCase skipIfNoTorchVision _test_vision model_name dtype m = getattr torchvision models model_name eval dtype == torch bfloat m = optimization fuse m x = torch rand _ graph = checkTrace m x dtype assertFused graph aten _convolution aten batch_norm aten relu aten linear aten avg_pool d aten max_pool d model_name enabled resnet True resnext _ x d True resnext _ x d True densenet True densenet True densenet True densenet True efficientnet_b True efficientnet_b True efficientnet_b True efficientnet_b True efficientnet_b True efficientnet_b True efficientnet_b True efficientnet_b True regnet_y_ mf True googlenet TEST_SCIPY mobilenet_v True mobilenet_v _large True mnasnet _ True squeezenet _ True vgg True alexnet True shufflenet_v _x _ True wide_resnet _ True _wrapper mname dtype unittest skipIf enabled Disabled separate_process test dtype=dtype _test_vision mname dtype test dtype torch bfloat torch float setattr TestModel test_vision_ _ format model_name str dtype split torch _wrapper model_name dtype instantiate_device_type_tests TestFusionPattern globals instantiate_device_type_tests TestOp globals __name__ == __main__ run_tests