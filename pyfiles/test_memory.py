Owner s module inductor unittest unittest mock torch torch _C FileCheck torch _dynamo utils same torch _inductor config memory torch _inductor test_case TestCase torch _inductor utils run_and_get_triton_code torch testing _internal common_utils serialTest torch testing _internal inductor_utils GPU_TYPE HAS_GPU try triton triton language tl TRITON_AVAILABLE = True except ImportError TRITON_AVAILABLE = False Foo torch nn Module The default compiled graph graph op num_users= = call_function args = primals_ primals_ op num_users= = call_function args = primals_ primals_ op num_users= = call_function args = op primals_ op num_users= = call_function args = op primals_ op num_users= = call_function args = op op num_users= = call_function args = op op _op num_users= = call_function args = op op __init__ super __init__ w = torch nn Parameter torch ones w = torch nn Parameter torch ones w = torch nn Parameter torch ones w = torch nn Parameter torch ones forward x t = torch matmul x w t = torch matmul x w t = torch matmul t w t = torch matmul t w t sum + t sum The tests uses very small tensors The default score_fusion_memory threshold will cause different fusion decisions generate different wrapper Override threshold make these tests happy config patch score_fusion_memory_threshold TestOperatorReorderForPeakMemory TestCase setUp super setUp model = Foo GPU_TYPE inputs = torch ones device=GPU_TYPE orig_reorder_method = memory reorder_for_peak_memory mock patch object config reorder_for_peak_memory True test_reorder_peak_memory outp_corr = model inputs compiled_model = torch compile model code = run_and_get_triton_code compiled_model inputs call_str = call args torch _inductor config graph_partition call args FileCheck check call_str check buf = check buf = check buf = check buf = check buf = check buf = check buf = run code check correctness outp = compiled_model inputs assertTrue same outp outp_corr mock patch object config reorder_for_peak_memory True test_reorder_peak_memory_lpmf outp_corr = model inputs reorder_with_only_lpmf nodes name_to_buf name_to_fused_node graph_inputs graph_outputs methods=None orig_reorder_method nodes name_to_buf name_to_fused_node graph_inputs graph_outputs methods= memory topological_sort_lpmf call_str = call args torch _inductor config graph_partition call args mock patch object memory reorder_for_peak_memory reorder_with_only_lpmf compiled_model = torch compile model code = run_and_get_triton_code compiled_model inputs FileCheck check call_str check buf = check buf = check buf = check buf = check buf = check buf = check buf = run code check correctness outp = compiled_model inputs assertTrue same outp outp_corr mock patch object config reorder_for_peak_memory True test_reorder_peak_memory_bfs outp_corr = model inputs reorder_with_only_bfs nodes name_to_buf name_to_fused_node graph_inputs graph_outputs methods=None orig_reorder_method nodes name_to_buf name_to_fused_node graph_inputs graph_outputs methods= memory topological_sort_bfs call_str = call args torch _inductor config graph_partition call args mock patch object memory reorder_for_peak_memory reorder_with_only_bfs compiled_model = torch compile model code = run_and_get_triton_code compiled_model inputs FileCheck check call_str check buf = check buf = check buf = check buf = check buf = check buf = check buf = run code check correctness outp = compiled_model inputs assertTrue same outp outp_corr mock patch object config reorder_for_peak_memory True test_reorder_peak_memory_dfs outp_corr = model inputs reorder_with_only_dfs nodes name_to_buf name_to_fused_node graph_inputs graph_outputs methods=None orig_reorder_method nodes name_to_buf name_to_fused_node graph_inputs graph_outputs methods= memory topological_sort_dfs call_str = call args torch _inductor config graph_partition call args mock patch object memory reorder_for_peak_memory reorder_with_only_dfs compiled_model = torch compile model code = run_and_get_triton_code compiled_model inputs FileCheck check call_str check buf = check buf = check buf = check buf = check buf = check buf = check buf = run code check correctness outp = compiled_model inputs assertTrue same outp outp_corr mock patch object config allow_buffer_reuse False unittest skipUnless TRITON_AVAILABLE Triton available config patch test_configs track_memory_lifecycle assert test_mutation_size_propogation This tests correct size propogation case mutations In example buf mutation buf we should have buf has size_alloc size_free buf has size_alloc size_free This because - when buf created no additional memory used - bytes memory can only released when buf freed Similar arguments buf buf buf buf etc using triton custom kernel creat small example mutations triton jit convert_to_bf _kernel input_ptr output_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load input_ptr + offsets mask=mask x_bf = x tl bfloat tl store output_ptr + offsets x_bf mask=mask convert_to_bf x output = torch empty_like x dtype=torch bfloat n_elements = x numel BLOCK_SIZE = grid = triton cdiv n_elements BLOCK_SIZE convert_to_bf _kernel grid x flatten output flatten n_elements BLOCK_SIZE output view x shape create custom function record buffer size information buffer_info = og_method = memory assign_memory_planning_info_for_scheduler_buffers assign_memory_planning_info_for_scheduler_buffers_with_records nodes name_to_buf og_method nodes name_to_buf buf_name buf name_to_buf items buffer_info buf_name = buf mpi_buffer size_alloc buf mpi_buffer size_free buf mpi_buffer succ_nodes test example checks f p e e = convert_to_bf e p = p e p = torch randn device=GPU_TYPE _ range p = torch ones size dtype=torch bfloat device=GPU_TYPE mock patch object memory assign_memory_planning_info_for_scheduler_buffers assign_memory_planning_info_for_scheduler_buffers_with_records f_compiled = torch compile f f_compiled p pre_mutation = buf buf buf buf post_mutation = buf buf buf buf pre post zip pre_mutation post_mutation assertEqual buffer_info pre assertEqual buffer_info post succ nodes should forwarded pre mutation buffer assertTrue buffer_info post = buffer_info pre unittest skipIf torch cuda is_available torch cuda get_device_properties total_memory int e Need GB memory safe run test test_fusing_reductions_increase_peak_memory torch compile f b c c sum dim=- + b c sum dim=- = torch randn device=GPU_TYPE b = torch randn device=GPU_TYPE c = torch randn device=GPU_TYPE torch cuda reset_peak_memory_stats f b c peak_mem = torch cuda max_memory_allocated expected_bound = size c size dtype itemsize assertLess peak_mem expected_bound serialTest test_fusion_acc_large_reads f x y z res = torch zeros_like x _ range temp = torch matmul x y + z res = res + temp res N = x = torch rand N N dtype=torch float device=GPU_TYPE y = torch rand N N dtype=torch float device=GPU_TYPE z = torch rand N N dtype=torch float device=GPU_TYPE torch _inductor choices InductorChoices torch _inductor scheduler BaseSchedulerNode Scheduler CustomInductorChoices InductorChoices staticmethod can_fuse scheduler Scheduler node BaseSchedulerNode node BaseSchedulerNode shared_data_score int - bool can_fuse_default = InductorChoices can_fuse scheduler node node shared_data_score can_fuse_default config realize_acc_reads_size_threshold can_fuse_default all_reads = node read_writes reads &#124; node read_writes reads - node read_writes writes &#124; node read_writes writes size_of_reads = scheduler dep_size_hint dep dep all_reads sum size_of_reads config realize_acc_reads_size_threshold torch _inductor virtualized V set_choices_handler CustomInductorChoices CASE no restriction amount accumulation config patch realize_acc_reads_size_threshold float inf f_compiled = torch compile f code = run_and_get_triton_code f_compiled x y z FileCheck check triton_poi_fused_add_ run buf arg _ buf buf buf run code CASE tensors same size x which N bytes most = reads can accumulated during fusion config patch realize_acc_reads_size_threshold N f_compiled = torch compile f code = run_and_get_triton_code f_compiled x y z FileCheck check triton_poi_fused_add_ run buf arg _ buf buf check triton_poi_fused_add_ run buf buf arg _ run code CASE no such fusion allowed config patch realize_acc_reads_size_threshold N f_compiled = torch compile f code = run_and_get_triton_code f_compiled x y z FileCheck check triton_poi_fused_add_ run buf arg _ buf check triton_poi_fused_add_ run buf buf arg _ check triton_poi_fused_add_ run buf buf arg _ run code unittest skipUnless TRITON_AVAILABLE Triton available test_multiple_mutations_of_buf torch compile foo inp inp inp = inp inp inp = inp view - x = inp y = inp x y = torch _foreach_add x y out = x sum out = y sum dim=- out out inp inp inp = torch rand device=GPU_TYPE inp = torch rand device=GPU_TYPE replace_foreach gm nodes = gm find_nodes op= call_function target=torch ops aten _foreach_add Scalar assert len nodes == node = nodes nodes target = torch ops aten _foreach_add_ Scalar inp out zip node args list node users keys out replace_all_uses_with inp gm erase_node out torch _inductor config patch post_grad_custom_post_pass replace_foreach test_configs track_memory_lifecycle assert allow_buffer_reuse False make sure mm end so earlier deallocation last step which doesnt distinguish between returned tensors which tensors deallocated immediately prior reorder_for_peak_memory False code = run_and_get_triton_code foo inp inp FileCheck check allocated= buf run code __name__ == __main__ torch _inductor test_case run_tests HAS_GPU run_tests