Owner s module inductor os subprocess sys torch torch _inductor async_compile noqa F required warm up AsyncCompile pools torch _inductor codecache PyCodeCache torch _inductor test_case run_tests TestCase torch testing _internal inductor_utils GPU_TYPE HAS_GPU TestTritonWrapper TestCase get_compiled_module compiled_module = None v PyCodeCache modules hasattr v benchmark_compiled_module assertTrue compiled_module None Found multiple compiled modules compiled_module = v assertTrue compiled_module None compiled_module test_wrapper_using_gpu_seed Make sure subprocess check_output does throw torch compile f x y dropout will result usage cuda_seed z = torch nn functional dropout x z + y N = x = torch rand N device=GPU_TYPE y = torch rand N device=GPU_TYPE out = f x y noqa F compiled_module = get_compiled_module make sure subprocess runs exact same path parent process we augment PYTHONPATH env var augmented_pp = join sys path os environ get PYTHONPATH augmented_pp = f os environ get PYTHONPATH augmented_pp now run compiled module subprocess check its output bench_out = subprocess check_output f sys executable compiled_module __file__ split stderr=subprocess STDOUT env= os environ PYTHONPATH augmented_pp decode assertTrue len bench_out __name__ == __main__ HAS_GPU run_tests