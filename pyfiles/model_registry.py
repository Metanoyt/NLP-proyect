Copyright c Meta Platforms Inc affiliates Owner s oncall distributed This file model zoo testing torch distributed pipelining torch torch autograd Function torch distributed pipelining pipe_split SplitPoint ExampleCode torch nn Module __init__ d_hid splits= assert splits = super __init__ splits = splits mm_param = torch nn Parameter torch randn d_hid d_hid mm_param = torch nn Parameter torch randn d_hid d_hid cval = torch nn Buffer torch randn d_hid requires_grad=False lin = torch nn Linear d_hid d_hid lin = torch nn Linear d_hid d_hid lin = torch nn Linear d_hid d_hid lin = torch nn Linear d_hid d_hid lin = torch nn Linear d_hid d_hid lin = torch nn Linear d_hid d_hid lin = torch nn Linear d_hid d_hid forward x x = torch mm x mm_param x = torch relu x try passing value doesn t require_grad across skip boundaries a_constant = cval clone x = lin x pipe_split x = torch relu x + a_constant x = torch mm x mm_param splits pipe_split x = lin x x = torch relu x splits pipe_split x = lin x x = torch relu x splits pipe_split x = lin x x = torch relu x splits pipe_split x = lin x x = torch relu x splits pipe_split x = lin x x = torch relu x splits pipe_split x = lin x x = torch relu x x ModelWithKwargs torch nn Module DEFAULT_DHID = DEFAULT_BATCH_SIZE = __init__ d_hid int = DEFAULT_DHID splits= assert splits = super __init__ splits = splits mm_param = torch nn Parameter torch randn d_hid d_hid mm_param = torch nn Parameter torch randn d_hid d_hid lin = torch nn Linear d_hid d_hid lin = torch nn Linear d_hid d_hid lin = torch nn Linear d_hid d_hid lin = torch nn Linear d_hid d_hid lin = torch nn Linear d_hid d_hid lin = torch nn Linear d_hid d_hid lin = torch nn Linear d_hid d_hid lin = torch nn Linear d_hid d_hid forward x y=torch zeros DEFAULT_BATCH_SIZE DEFAULT_DHID x = torch mm x mm_param x = x + y x = lin x x = torch relu x pipe_split x = torch mm x mm_param x = lin x x = torch relu x splits pipe_split x = lin x x = torch relu x splits pipe_split x = lin x x = torch relu x splits pipe_split x = lin x x = torch relu x splits pipe_split x = lin x x = torch relu x splits pipe_split x = lin x x = torch relu x splits pipe_split x = lin x x = torch relu x x ModelWithParamAlias torch nn Module default_dhid = default_batch_size = __init__ d_hid int = default_dhid super __init__ mm_param = mm_param = torch nn Parameter torch randn d_hid d_hid lin = lin = torch nn Linear d_hid d_hid forward x y x = torch mm x mm_param x = x + y x = lin x x = torch relu x pipe_split x = torch mm x mm_param x = lin x x = torch relu x x MLP Layer MLPModule torch nn Module __init__ d_hid int super __init__ net = torch nn Linear d_hid d_hid relu = torch nn ReLU net = torch nn Linear d_hid d_hid forward x x = net x x = relu x x = net x x MLPKWargModule torch nn Module __init__ d_hid int layer_num super __init__ net = torch nn Linear d_hid d_hid relu = torch nn ReLU net = torch nn Linear d_hid d_hid layer_num = layer_num forward x unused_kwarg torch Tensor = torch zeros x = net x x = relu x x = net x Test when only module has extra outputs TODO handle case later layer_num == x unused_kwarg x x Multi-MLP model MultiMLP torch nn Module __init__ d_hid int n_layers int = super __init__ layers = torch nn ModuleList MLPModule d_hid _ range n_layers For testing purpose only should defined user split_spec = f layers i SplitPoint BEGINNING i range n_layers forward x layer layers x = layer x x Multi-MLP kwargs model MultiMLPKwargs torch nn Module __init__ d_hid int n_layers int = super __init__ layers = torch nn ModuleList MLPKWargModule d_hid i i range n_layers For testing purpose only should defined user split_spec = f layers i SplitPoint BEGINNING i range n_layers forward x unused_kwarg torch Tensor = torch zeros layer layers TODO handle case later layer layer_num == x _ = layer x unused_kwarg x = layer x x = layer x x CustomLinearDx Function staticmethod forward ctx input_val weight bias module layer_idx ctx save_for_backward input_val weight bias ctx module = module ctx layer_idx = layer_idx input_val mm weight t + bias staticmethod backward ctx grad_output input_val weight _ = ctx saved_tensors grad_input = grad_output mm weight ctx module cached_context ctx layer_idx append grad_output clone ctx module cached_context str ctx layer_idx + _input append input_val clone grad_input None None None None CustomLinearDxDw Function staticmethod forward ctx input_val weight bias ctx save_for_backward input_val weight bias input_val mm weight t + bias staticmethod backward ctx grad_output input_val weight _ = ctx saved_tensors grad_input = grad_output mm weight grad_weight = grad_output t mm input_val grad_bias = grad_output sum grad_input grad_weight grad_bias MLPModuleWithDw torch nn Module __init__ d_hid int super __init__ fc _weight = torch nn Parameter torch randn d_hid d_hid fc _bias = torch nn Parameter torch randn d_hid fc _weight = torch nn Parameter torch randn d_hid d_hid fc _bias = torch nn Parameter torch randn d_hid torch nn init uniform_ fc _weight - torch nn init uniform_ fc _weight - torch nn init uniform_ fc _bias - torch nn init uniform_ fc _bias - cached_context = cached_context fc = cached_context fc = cached_context fc _input = cached_context fc _input = use_custom_logic = False forward x use_custom_logic hidden = CustomLinearDxDw apply x fc _weight fc _bias hidden = torch nn functional relu hidden output = CustomLinearDxDw apply hidden fc _weight fc _bias output hidden = CustomLinearDx apply x fc _weight fc _bias fc hidden = torch nn functional relu hidden output = CustomLinearDx apply hidden fc _weight fc _bias fc output compute_dW grad_output_fc = cached_context fc pop grad_output_fc = cached_context fc pop cached_input_fc = cached_context fc _input pop cached_input_fc = cached_context fc _input pop dW = grad_output_fc t mm cached_input_fc db = grad_output_fc sum dW = grad_output_fc t mm cached_input_fc db = grad_output_fc sum fc _weight grad None fc _weight grad += dW fc _bias grad += db fc _weight grad += dW fc _bias grad += db fc _weight grad = dW fc _bias grad = db fc _weight grad = dW fc _bias grad = db toggle use_custom_logic = use_custom_logic Multi-MLP model With Dw MultiMLPWithDw torch nn Module __init__ d_hid int n_layers int = super __init__ layers = torch nn ModuleList MLPModuleWithDw d_hid _ range n_layers For testing purpose only should defined user split_spec = f layers i SplitPoint BEGINNING i range n_layers use_custom_logic = False forward x layer layers x = layer x x toggle use_custom_logic = use_custom_logic layer layers layer toggle compute_dW use_custom_logic raise RuntimeError Need call toggle enable custom backward dW i reversed range len layers layers i compute_dW