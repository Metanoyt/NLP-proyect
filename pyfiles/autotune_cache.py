PyTorch Inductor Autotuning Cache System This module implements caching system autotuning configurations PyTorch s Inductor compiler It provides mechanisms store retrieve optimal kernel configurations both locally remotely which significantly speeds up compilation reusing previously discovered optimal parameters The caching system includes - Local filesystem caching individual machine reuse - Remote caching sharing optimizations across machines - Bundled caching efficiently store multiple related configurations - Cache invalidation based PyTorch versions backend changes - Serialization deserialization support worker processes Key components - AutotuneCache Main managing cache access storage - AutotuneCacheBundler Bundles multiple cache entries efficient storage - LocalAutotuneCache Handles filesystem-based caching - _LocalAutotuneCacheBackend Low-level file operations cache storage - AutotuneCacheArtifact Integration PyTorch s artifact system This caching system critical performance eliminates need re-run expensive autotuning operations when same kernels compiled multiple times __future__ annotations dataclasses hashlib logging os os path re typing Any TYPE_CHECKING typing_extensions override torch torch _inductor runtime runtime_utils cache_dir torch compiler _cache CacheArtifact CacheArtifactFactory CacheArtifactManager torch utils _triton has_triton remote_cache create_cache JsonDataTy RemoteCache RemoteCacheBackend RemoteCacheJsonSerde triton_compat Config HAS_WARP_SPEC TYPE_CHECKING remote_cache Sample log = logging getLogger __name__ _InductorMetaTy = dict str object inductor_meta_from_config - _InductorMetaTy torch _inductor config backend_hash = None has_triton try backend_hash = torch utils _triton triton_hash_with_backend except RuntimeError This can get error RuntimeError active drivers There should only one pass is_hip = None torch version hip None is_hip = True autotune_local_cache config autotune_local_cache autotune_remote_cache config autotune_remote_cache backend_hash backend_hash bundled_autotune_remote_cache config bundled_autotune_remote_cache coordinate_descent_tuning config coordinate_descent_tuning is_fbcode config is_fbcode is_hip is_hip CacheArtifactFactory register AutotuneCacheArtifact CacheArtifact override populate_cache - None autotune_cache = _LocalAutotuneCacheBackend key = os path join cache_dir key autotune_cache _put key content override staticmethod type - str autotune override staticmethod encode content JsonDataTy - bytes assert isinstance content bytes serde = RemoteCacheJsonSerde content_bytes = serde encode content assert isinstance content_bytes bytes content_bytes dataclasses dataclass AutotuneCache configs_hash str local_cache tuple RemoteCache JsonDataTy str &#124; None = None remote_cache tuple RemoteCache JsonDataTy str &#124; None = None Create AutotuneCache Returns None none caches can used staticmethod create inductor_meta _InductorMetaTy filename str configs_hash str - AutotuneCache &#124; None cache = AutotuneCache configs_hash key = AutotuneCache _prepare_key filename cache _setup_local_cache inductor_meta os path dirname filename key cache _setup_remote_autotune_cache inductor_meta key cache local_cache cache remote_cache cache None staticmethod _prepare_key filename str - str torch compiler config cconfig base filename already sha hash source contents key = f os path basename filename cconfig cache_key_tag hashlib sha key encode utf- hexdigest Read best config options most local cache _read - dict str JsonDataTy &#124; None local_cache = local_cache cache key = local_cache best_config = cache get key isinstance best_config dict best_config remote_cache = remote_cache cache key = remote_cache best_config = cache get key isinstance best_config dict best_config None Read best config options most local cache figure out which ` configs ` represents option read_best inductor_meta _InductorMetaTy configs list Config - Config &#124; None best = _read _load_cached_autotuning best configs_hash configs inductor_meta None Set up local filesystem caching information _setup_local_cache inductor_meta _InductorMetaTy dirname str cache_key str - None inductor_meta get autotune_local_cache True codecache torch_key Note torch_key autotune cache key Include torch_key cache key so different versions torch result cache invalidation This important case changes best_config format other code changes backward compatible w r t cache hasher = hashlib sha hasher update cache_key encode utf- hasher update torch_key updated_cache_key = hasher hexdigest cache_filename = f dirname updated_cache_key best_config local_cache = LocalAutotuneCache local_cache = local_cache cache_filename Set up remote caching information _setup_remote_autotune_cache inductor_meta _InductorMetaTy cache_key str - None _should_use_remote_autotune_cache inductor_meta backend_hash = inductor_meta get backend_hash None None log debug backend_hash passed inductor_meta unable use autotune remote cache assert isinstance backend_hash str codecache torch_key is_fbcode = bool inductor_meta get is_fbcode False salt = autotune-best-config-v re torch_key - see Note torch_key autotune cache key key = torch_key hex + backend_hash + configs_hash + salt key = hashlib sha key encode utf- hexdigest remote_cache = create_cache key is_fbcode FbRemoteAutotuneCache RemoteAutotuneCache remote_cache Save args passed create_cache case AutotuneCache needs pickled remote_cache_full_key = key is_fbcode = is_fbcode remote_cache = remote_cache cache_key The AutotuneCache may serialized deserialized we re using AsyncCompile worker processes run triton compilation This because AutotuneCache instances created worker process we need run AutotuneCache save parent process when actually doing autotuning __getstate__ - dict str Any The remote cache handles themselves may serializable So clear reconstruct setstate remote_cache = getattr remote_cache None __dict__ Save cache_key portion remote_cache remote_cache remote_cache __setstate__ state dict str Any - None Reconstruct remote cache parent __dict__ update state remote_cache None assert isinstance remote_cache str assert hasattr remote_cache_full_key assert hasattr is_fbcode cache_key = remote_cache remote_cache = create_cache remote_cache_full_key is_fbcode FbRemoteAutotuneCache RemoteAutotuneCache remote_cache None remote_cache = remote_cache cache_key log warning Warning failed recreate remote cache after pickling remote_cache = None Save config caches save config Config time_taken_ns int found_by_coordesc bool = False triton_cache_hash str &#124; None = None - None data = pyrefly ignore missing-attribute config kwargs pyrefly ignore missing-attribute num_warps config num_warps pyrefly ignore missing-attribute num_stages config num_stages configs_hash configs_hash found_by_coordesc found_by_coordesc time_taken_ms time_taken_ns Convert NS MS triton_cache_hash triton_cache_hash HAS_WARP_SPEC data update num_consumer_groups getattr config num_consumer_groups num_buffers_warp_spec getattr config num_buffers_warp_spec local_cache = local_cache cache key = local_cache cache put key data AutotuneCacheBundler put key data autotune_artifact_key = os path join key split os sep - CacheArtifactManager record_artifact AutotuneCacheArtifact type autotune_artifact_key data log isEnabledFor logging DEBUG type_str = coordesc found_by_coordesc heuristic log debug Save s tuning result s type_str key remote_cache = remote_cache cache key = remote_cache cache put key data _AutotuneCacheBundlerImpl Caches set LocalAutotuneCacheBackend entries together single cache _key str _cache RemoteCache JsonDataTy All known entries LocalAutotuneCache put _entries dict str JsonDataTy end_compile - None TODO Do we need compute time_taken_ms encode somehow _entries _cache put _key _entries put basename str data JsonDataTy - None Do we need worry about duplicates We only have single local fs entry - so probably _entries basename = data __init__ key str cache RemoteCache JsonDataTy - None _key = key _cache = cache _entries = sync - None We don t currently use - we could async load starting ` begin_compile ` wait load finished here pass classmethod _should_use_bundled_autotune_remote_cache cls inductor_meta _InductorMetaTy - bool The bundled autotune cache only available you ve also got local caching enabled because we feed bundled data local cache inductor_meta get autotune_local_cache True False Check we re enabled via config bundled_autotune_remote_cache = inductor_meta get bundled_autotune_remote_cache None bool bundled_autotune_remote_cache cls _get_is_fbcode inductor_meta False torch _utils_internal is_fb_unit_test False inductor_meta get is_hip False try torch _inductor fb remote_cache REMOTE_CACHE_VERSION except ModuleNotFoundError False jk = torch _utils_internal justknobs_getval_int pytorch remote_cache bundled_autotune_remote_cache_version REMOTE_CACHE_VERSION = jk _load_cache - bool torch _inductor codecache The single key defined construction cache entries = _cache get _key entries None isinstance entries dict We couldn t load cache - so mark _entries non-None so we store local cache values False Go through entries we got cache save them locally time_saved_ns = basename data entries items Reconstruct final filename see put root ext = _splitext_nodot basename _ _ filename = codecache get_path root ext isinstance data dict tsns = data get time_saved_ns time_saved_ns += int tsns type ignore arg-type local_cache = LocalAutotuneCache local_cache put filename data codecache add_ephemeral_timeout_increase_for_distributed time_saved_ns True staticmethod _get_is_fbcode inductor_meta _InductorMetaTy - bool bool inductor_meta get is_fbcode False staticmethod _get_backend_hash inductor_meta _InductorMetaTy - str backend_hash = inductor_meta backend_hash assert isinstance backend_hash str backend_hash AutotuneCacheBundler _bundler _AutotuneCacheBundlerImpl &#124; None = None __init__ - None pass Call before we start any autotune computation inductor python file On cache hit copies individual results into local autotune caches classmethod begin_compile cls inductor_meta _InductorMetaTy code str &#124; None = None code_hash str &#124; None = None - None assert cls _bundler None code None assert code_hash None Cannot specify both code code_hash code_hash = _comment_stripped_hash code assert code_hash None _AutotuneCacheBundlerImpl _should_use_bundled_autotune_remote_cache inductor_meta cache = create_cache bundled-autotune-v _AutotuneCacheBundlerImpl _get_is_fbcode inductor_meta FbRemoteBundledAutotuneCache RemoteBundledAutotuneCache cache We re starting compilation phase We have cache key code we re compiling We ll get individual autotune bundles later via put For now create AutotuneCacheBundler try load cache salt = bundled-autotune-best-configs-v backend_hash = _AutotuneCacheBundlerImpl _get_backend_hash inductor_meta TODO The autotune cache includes configs_hash key The problem configs_hash includes info individual pointwise calls size_hints example which we can t know yet I think info basically present ` code_hash ` since s parameter pointwise decorator - there other info we need include inductor_meta key = code_hash + backend_hash + salt key = hashlib sha key encode utf- hexdigest bundler = _AutotuneCacheBundlerImpl key cache bundler _load_cache We couldn t load cache - so save data so we can store saved autotunes cls _bundler = bundler If we get cache hit don t bother saving any individual autotune results Call after all individual autotune results finished inductor python file If we gathered any individual results then we bundle those put into cache classmethod end_compile cls - None bundler = cls _bundler cls _bundler = None bundler end_compile classmethod sync cls - None bundler = cls _bundler bundler sync classmethod put cls filename str data JsonDataTy - None bundler = cls _bundler The filename comes something like tmp tmp random aa basename py where aa basename Strip down make sure looks like path we could reconstruct because s possible caller customize path basename = os path basename filename TODO check cache_dir vs filename then strip dirname bundler put basename data Remove comments code which include things like run ids file paths then hash result _comment_stripped_hash code str - str code = re sub r $ code count= flags=re MULTILINE torch _inductor codecache code_hash code _should_use_remote_autotune_cache inductor_meta _InductorMetaTy - bool config = inductor_meta get autotune_remote_cache None bool config inductor_meta get is_fbcode False torch _utils_internal is_fb_unit_test False inductor_meta get is_hip False try torch _inductor fb remote_cache REMOTE_CACHE_VERSION except ModuleNotFoundError False REMOTE_CACHE_VERSION = torch _utils_internal justknobs_getval_int pytorch remote_cache autotune_memcache_version _load_cached_autotuning best_config dict str JsonDataTy configs_hash str configs list Config inductor_meta _InductorMetaTy - Config &#124; None best_config None None best_config pop configs_hash None = configs_hash None Remove time taken comparison best_config pop time_taken_ms None best_config pop triton_cache_hash None inductor_meta get coordinate_descent_tuning best_config pop found_by_coordesc False num_warps = best_config pop num_warps num_stages = best_config pop num_stages Extract common arguments config_args = num_warps num_warps num_stages num_stages HAS_WARP_SPEC config_args update num_consumer_groups best_config pop num_consumer_groups num_buffers_warp_spec best_config pop num_buffers_warp_spec Create triton_config appropriate arguments pyrefly ignore bad-argument-count triton_config = Config best_config config_args pyrefly ignore missing-attribute triton_config found_by_coordesc = True triton_config matching_configs = cfg cfg configs pyrefly ignore missing-attribute all val == best_config get key key val cfg kwargs items pyrefly ignore missing-attribute cfg num_warps == best_config get num_warps pyrefly ignore missing-attribute cfg num_stages == best_config get num_stages len matching_configs = None matching_configs _LocalAutotuneCacheBackend RemoteCacheBackend bytes override _get key str - bytes &#124; None try open key rb fd fd read except FileNotFoundError None override _put key str data bytes - None os makedirs os path dirname key exist_ok=True torch _inductor codecache codecache write_atomic key data LocalAutotuneCache RemoteCache JsonDataTy __init__ - None backend = _LocalAutotuneCacheBackend serde = RemoteCacheJsonSerde super __init__ backend serde override _get key str sample Sample &#124; None - JsonDataTy &#124; None AutotuneCacheBundler sync result = super _get key sample result None assert isinstance result dict What Why we doing put here Imagine we have new model reuses some existing kernels have already been compiled If we didn t do ` put ` here cache hit then new model would only bundle newly compiled kernels existing kernels already compiled cached AutotuneCacheBundler put key result autotune_artifact_key = os path join key split os sep - CacheArtifactManager record_artifact AutotuneCacheArtifact type autotune_artifact_key result result override _put key str value JsonDataTy sample Sample &#124; None - None AutotuneCacheBundler put key value super _put key value sample _splitext_nodot basename str - tuple str str root ext = os path splitext basename ext ext = ext root ext