Owner s module inductor contextlib unittest skipIf torch torch distributed dist torch _inductor config metrics torch _inductor comm_analysis estimate_nccl_collective_runtime torch _inductor compile_fx compile_fx compile_fx_inner torch _inductor test_case TestCase InductorTestCase torch _inductor utils is_collective torch testing _internal common_device_type expectedFailureXPU torch testing _internal inductor_utils GPU_TYPE HAS_GPU aten = torch ops aten c d = torch ops c d_functional _c d = torch ops _c d_functional compile_but_use_eager gm example_inputs inner_compile gm args kwargs compile_fx_inner gm args kwargs gm compile_fx gm example_inputs inner_compile=inner_compile calculate_runtime f args - float Assumes all inputs fp metrics reset torch _logging set_logs inductor_metrics=True torch compile f backend=compile_but_use_eager args print metrics node_runtimes ret = pair metrics node_runtimes ret += pair torch _logging set_logs ret DEVICE = GPU_TYPE T size dtype=torch float device=DEVICE grad=False - torch Tensor torch randn size dtype=dtype device=device requires_grad=grad TestCase InductorTestCase device = DEVICE Helper methods compare runtime estimate against Since estimate hardware dependent stronger comparisons may fail depending host s specs atol rtol must provided explicitly each call since precision rel_tol overrides always utilized setUp super setUp These tests check metrics node_runtimes we don t save restore those FX graph cache _test_snode_stack = contextlib ExitStack _test_snode_stack enter_context config patch fx_graph_remote_cache False tearDown _test_snode_stack close super tearDown assertZero x float assert isinstance x float super assertEqual x atol= rtol= assertNotZero x assert isinstance x float super assertNotEqual x atol= rtol= UnsupportedTests TestCase device = DEVICE test_no_op f inp = T assertZero calculate_runtime f inp test_no_cuda f inp = torch randn device= cpu assertZero calculate_runtime f inp ComputeBoundedTests TestCase device = DEVICE lack profiler XPU expectedFailureXPU test_conv d f x y torch nn functional conv d x y inp = T T assertNotZero calculate_runtime f inp lack profiler XPU expectedFailureXPU test_conv d f x y torch nn functional conv d x y padding= inp = T T assertNotZero calculate_runtime f inp lack profiler XPU expectedFailureXPU test_conv d_transpose f x y torch nn functional conv_transpose d x y padding= inp = T T assertNotZero calculate_runtime f inp lack profiler XPU expectedFailureXPU test_conv d f x y torch nn functional conv d x y inp = T T assertNotZero calculate_runtime f inp lack profiler XPU expectedFailureXPU test_mm f b torch mm b inp = T T assertNotZero calculate_runtime f inp lack profiler XPU expectedFailureXPU test_addmm f b c torch addmm b c inp = T T T assertNotZero calculate_runtime f inp lack profiler XPU expectedFailureXPU test_bmm f b torch bmm b inp = T T assertNotZero calculate_runtime f inp MemoryBoundedTests TestCase device = DEVICE lack profiler XPU expectedFailureXPU test_relu f torch nn functional relu inp = T assertNotZero calculate_runtime f inp lack profiler XPU expectedFailureXPU test_horizontal_reduction_pointwise f b = sum dim= c = cos b c inp = T assertNotZero calculate_runtime f inp lack profiler XPU expectedFailureXPU test_pointwise f x x cos inp = T assertNotZero calculate_runtime f inp lack profiler XPU expectedFailureXPU torch _dynamo config patch assume_static_by_default=False test_dynamic f x x cos inp = T assertNotZero calculate_runtime f inp skipIf dist is_available requires distributed TestCommAnalysis TestCase device = DEVICE WORLD_SIZE int = RANKS = list range _verify_runtime_estimation fn inps torch testing _internal distributed fake_pg FakeStore store = FakeStore dist init_process_group backend= fake rank= world_size=self WORLD_SIZE store=store try metrics reset torch _logging set_logs inductor_metrics=True torch compile fn inps found_collective = False snode runtime metrics node_runtimes is_collective snode node continue found_collective = True Inductor swallows errors snode runtime estimations We call estimate_nccl_collective_runtime white-box fashion here so potential issues can surfaced tests est = estimate_nccl_collective_runtime snode node assertNotZero est Also make sure estimate_nccl_collective_runtime works correctly inductor assertNotZero runtime Make sure collective kernel found graph assertTrue found_collective torch _logging set_logs finally dist destroy_process_group test_legacy_all_reduce fn x r = c d all_reduce x sum RANKS WORLD_SIZE c d wait_tensor r inp = T _verify_runtime_estimation fn inp test_legacy_all_reduce_coalesced fn x rs = c d all_reduce_coalesced x sum RANKS WORLD_SIZE c d wait_tensor r r rs inp = T T _verify_runtime_estimation fn inp test_legacy_all_gather_into_tensor_coalesced fn x rs = c d all_gather_into_tensor_coalesced x RANKS WORLD_SIZE c d wait_tensor r r rs inp = T T _verify_runtime_estimation fn inp test_all_reduce fn x r = _c d all_reduce x sum _c d wait_tensor r inp = T _verify_runtime_estimation fn inp test_all_reduce_coalesced fn x rs = _c d all_reduce_coalesced x sum _c d wait_tensor r r rs inp = T T _verify_runtime_estimation fn inp test_all_gather_into_tensor fn x rs = _c d all_gather_into_tensor x WORLD_SIZE _c d wait_tensor r r rs inp = T _verify_runtime_estimation fn inp test_all_gather_into_tensor_coalesced fn x rs = _c d all_gather_into_tensor_coalesced x WORLD_SIZE _c d wait_tensor r r rs inp = T T _verify_runtime_estimation fn inp test_reduce_scatter_tensor fn x rs = _c d reduce_scatter_tensor x sum WORLD_SIZE _c d wait_tensor r r rs inp = T WORLD_SIZE _verify_runtime_estimation fn inp test_reduce_scatter_tensor_coalesced fn x rs = _c d reduce_scatter_tensor_coalesced x sum WORLD_SIZE _c d wait_tensor r r rs inp = T WORLD_SIZE T WORLD_SIZE _verify_runtime_estimation fn inp __name__ == __main__ torch _inductor test_case run_tests HAS_GPU run_tests needs= filelock