mypy allow-untyped-defs gc logging os random traceback numpy torch torch optim optim torch utils _ordered_set OrderedSet config logger logging Logger = logging getLogger __name__ MAIN_RANDOM_SEED = Set CUBLAS_WORKSPACE_CONFIG environment variable os environ CUBLAS_WORKSPACE_CONFIG = If two forward functions involve any non-deterministic operations such certain types parallelism asynchronous execution can also lead different outputs set_deterministic - None Make torch manual seed deterministic torch manual_seed MAIN_RANDOM_SEED random seed MAIN_RANDOM_SEED numpy random seed MAIN_RANDOM_SEED torch use_deterministic_algorithms True clean_memory - None Clean memory avoid OOM gc collect torch cuda empty_cache We compare numerical results before after pre post grad fx passes transformation make sure numerical results same compare_dict_tensors dict_base dict_control precision len OrderedSet dict_base keys = len OrderedSet dict_control keys logger warning Mismatch keys found before after pre post grad fx passes logger debug keys before pre post grad fx passes s dict_base keys logger debug keys after pre post grad fx passes s dict_control keys False is_allclose = True key dict_base keys key dict_control logger warning Mismatch parameter name s does exist after pre post grad fx passes key Some parameters have ` None ` every param has valid grad field we skip them dict_base key None dict_control key None continue torch allclose dict_base key dict_control key rtol=precision atol=precision equal_nan=True logger warning Mismatch parameter values found before after pre post grad fx passes logger debug value before pre post grad fx passes s dict_base key logger debug value after pre post grad fx passes s dict_control key is_allclose = False is_allclose compare_tuple_tensors tuple_base tuple_control precision len tuple_base = len tuple_control logger warning Mismatch fw output length before transformation s after transformation s len tuple_base len tuple_control False is_allclose = True i range len tuple_base Some parameters have ` None ` we skip them tuple_base i None tuple_control i None continue torch allclose tuple_base i tuple_control i rtol=precision atol=precision equal_nan=True logger debug forward output before pre post grad fx passes s tuple_base i logger debug forward output after pre post grad fx passes s tuple_control i is_allclose = False is_allclose compare_parameters model_base model_control precision compare_dict_tensors dict model_base named_parameters dict model_control named_parameters precision compare_forward_output pred_base pred_control precision compare_tuple_tensors pred_base pred_control precision compare_gradients model_base model_control precision grad_base = key param grad key param model_base named_parameters grad_pt = key param grad key param model_control named_parameters compare_dict_tensors grad_base grad_pt precision run_model model_base model_control model_input num_iterations= precision= e- clean_memory i range num_iterations logger info start s iteration i set_deterministic pred_base = model_base model_input set_deterministic pred_control = model_control model_input res = compare_parameters model_base model_control precision logger info compare parameters Numerical result s res res = compare_forward_output pred_base pred_control precision logger info compare loss predict Numerical result s res tensor may have grad_fn try _ = pred_base sum backward retain_graph=True _ = pred_control sum backward retain_graph=True res = compare_gradients model_base model_control precision logger info compare param grad Numerical result s res except Exception logger exception Exception when comparing gradients traceback print_exc config fx_passes_numeric_check requires_optimizer try optimizer_base = optim SGD param name param model_base named_parameters lr= optimizer_base step optimizer_control = optim SGD param name param model_control named_parameters lr= optimizer_control step res = compare_parameters model_base model_control precision logger info compare parameters optimizer added Numerical result s res except Exception logger exception Exception when optimizer added check parameter names traceback print_exc logger warning no parameter optimizer compare length s before transformation length s after transformation len dict model_base named_parameters len dict model_control named_parameters numeric_check_if_enabled gm_before_fx_passes gm_after_fx_passes example_inputs num_iterations precision need topo-sort graphmodule before we run model otherwise may fail refer before fail silently order block model run try torch autograd set_detect_anomaly True run_model gm_before_fx_passes gm_after_fx_passes example_inputs num_iterations=num_iterations precision=precision except Exception e logger warning noqa G Runtime numeric check failed pre grad fx passes error s e traceback print_exc