Owner s oncall distributed test_c d_spawn _torch_dist_nn_available TestDistributedNNFunctions torch torch distributed c d torch testing _internal common_distributed requires_nccl skip_if_lt_x_gpu torch testing _internal common_utils run_tests skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN NO_NCCL = hasattr c d ProcessGroupNCCL Fails Python- see https github com pytorch pytorch issues Skip dev-asan torch + multiprocessing spawn have known issues TEST_WITH_DEV_DBG_ASAN TestDistributedNNFunctionsNccl TestDistributedNNFunctions Test Common Ops First requires_nccl skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if _torch_dist_nn_available torch distributed nn available test_broadcast _test_broadcast nccl requires_nccl skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if _torch_dist_nn_available torch distributed nn available test_reduce _test_reduce nccl requires_nccl skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if _torch_dist_nn_available torch distributed nn available test_allreduce _test_allreduce nccl requires_nccl skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if _torch_dist_nn_available torch distributed nn available test_all_gather _test_all_gather nccl requires_nccl skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if _torch_dist_nn_available torch distributed nn available test_all_to_all _test_all_to_all nccl requires_nccl skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if _torch_dist_nn_available torch distributed nn available test_all_to_all_single _test_all_to_all_single nccl Test Ops only supported NCCL requires_nccl skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if _torch_dist_nn_available torch distributed nn available test_reduce_scatter store = c d FileStore file_name world_size This required because these functions calls directly dist needs world initialized c d init_process_group store=store rank=self rank world_size=self world_size backend= nccl device = torch device f cuda rank x = torch ones device=device + rank x = torch ones device=device + rank + x requires_grad = True x requires_grad = True y = torch empty_like x expected = + world_size world_size + world_size rank y = torch distributed nn reduce_scatter y x x assertEqual y torch ones device=device expected z = y sin sum z backward expected_ = + world_size world_size expected_ = expected_ + world_size x_s_ = expected_ torch ones device=device cos x_s_ = expected_ torch ones device=device cos assertEqual x grad x_s_ assertEqual x grad x_s_ requires_nccl skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if _torch_dist_nn_available torch distributed nn available test_reduce_scatter_non_contiguous store = c d FileStore file_name world_size This required because these functions calls directly dist needs world initialized c d init_process_group store=store rank=self rank world_size=self world_size backend= nccl device = torch device f cuda rank NonContiguousGrad torch autograd Function staticmethod forward ctx input input staticmethod backward ctx grad_output Make grad non-contiguous grad_output clone transpose x = torch rand device=device requires_grad=True x = torch rand device=device requires_grad=True y = torch empty device=device y = torch distributed nn reduce_scatter y x x NonContiguousGrad apply y sum backward requires_nccl skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if _torch_dist_nn_available torch distributed nn available test_all_reduce_non_contiguous store = c d FileStore file_name world_size This required because these functions calls directly dist needs world initialized c d init_process_group store=store rank=self rank world_size=self world_size backend= nccl device = torch device f cuda rank NonContiguousGrad torch autograd Function staticmethod forward ctx input input staticmethod backward ctx grad_output Make grad non-contiguous grad_output clone transpose x = torch rand device=device requires_grad=True y = torch distributed nn all_reduce x NonContiguousGrad apply y sum backward requires_nccl skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if _torch_dist_nn_available torch distributed nn available test_all_gather_base store = c d FileStore file_name world_size c d init_process_group store=store rank=self rank world_size=self world_size backend= nccl device = torch device f cuda rank x = torch ones device=device + rank x requires_grad = True output = torch empty world_size device=device output = torch distributed nn functional _all_gather_base output x assertEqual output size torch Size world_size idx range world_size assertEqual output idx idx + torch ones device=device + idx y = torch sum output view world_size axis= z = y sin sum z backward x_s = torch ones device=device cos assertEqual x grad x_s __name__ == __main__ run_tests