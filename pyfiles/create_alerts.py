usr bin env python __future__ annotations argparse json os re collections defaultdict difflib SequenceMatcher typing Any requests setuptools distutils type ignore attr-defined ALL_SKIPPED_THRESHOLD = SIMILARITY_THRESHOLD = FAILURE_CHAIN_THRESHOLD = MAX_CONCURRENT_ALERTS = FAILED_JOB_PATTERN = r ^- \ \ \ \ failed consecutively starting commit \ \ \ \ $ PENDING = pending NEUTRAL = neutral SKIPPED = skipped SUCCESS = success FAILURE = failure CANCELED = canceled ISSUES_WITH_LABEL_QUERY = query $ owner String $ name String $ labels String repository owner $ owner name $ name followRenames false issues last labels $ labels states OPEN nodes id title closed number body createdAt comments first nodes bodyText databaseId NUM_ISSUES_QUERY = query $ query String search type ISSUE query $ query issueCount DISABLED_ALERTS = rerun_disabled_tests unstable JobStatus job_name str = jobs list Any = current_status Any = None job_statuses list Any = filtered_statuses list Any = failure_chain list Any = flaky_jobs list Any = __init__ job_name str job_statuses list Any - None job_name = job_name job_statuses = job_statuses filtered_statuses = list filter lambda j is_job_skipped j job_statuses current_status = get_current_status failure_chain = get_most_recent_failure_chain flaky_jobs = get_flaky_jobs get_current_status - Any When getting current status we want latest status which pending success failure status filtered_statuses status conclusion = PENDING status None get_unique_failures jobs list Any - dict str list Any Returns list jobs grouped failureCaptures input list failures = defaultdict list job jobs job conclusion == failure found_similar_failure = False failureCaptures job failures unclassified = job continue This now list returned HUD API string failureCaptures = join job failureCaptures failure failures seq = SequenceMatcher None failureCaptures failure seq ratio SIMILARITY_THRESHOLD failures failure append job found_similar_failure = True break found_similar_failure failures failureCaptures = job failures A flaky job s only job has failureCapture most recent job get_flaky_jobs - list Any unique_failures = get_unique_failures filtered_statuses flaky_jobs = failure unique_failures failure_list = unique_failures failure len failure_list == failure_list sha = current_status sha flaky_jobs append failure_list flaky_jobs The most recent failure chain array jobs have same-ish failures A success middle chain will terminate chain get_most_recent_failure_chain - list Any failures = found_most_recent_failure = False job filtered_statuses is_job_failed job failures append job found_most_recent_failure = True found_most_recent_failure is_job_failed job break failures should_alert - bool Group jobs their failures The length failure chain used raise alert so we can do simple tweak here use length longest unique chain unique_failures = get_unique_failures failure_chain current_status None current_status conclusion = SUCCESS any len failure_chain = FAILURE_CHAIN_THRESHOLD failure_chain unique_failures values all disabled_alert job_name disabled_alert DISABLED_ALERTS __repr__ - str f jobName job_name fetch_hud_data repo str branch str - Any response = requests get f https hud pytorch org api hud repo branch response raise_for_status hud_data = json loads response text hud_data jobNames hud_data shaGrid Creates Dict Job Name - JobData Essentially Column HUD map_job_data jobNames Any shaGrid Any - dict str Any jobData = defaultdict list sha shaGrid ind job enumerate sha jobs jobData jobNames ind append job jobData is_job_failed job Any - bool conclusion = job get conclusion None conclusion None conclusion = SUCCESS conclusion = PENDING is_job_skipped job Any - bool conclusion = job get conclusion None conclusion NEUTRAL SKIPPED conclusion None get_failed_jobs job_data list Any - list Any job job job_data job conclusion == failure classify_jobs all_job_names list str sha_grid Any filtered_jobs_names set str - tuple list JobStatus list Any Creates Job Statuses which has logic need alert there s flaky jobs Classifies jobs into jobs alert flaky jobs param all_job_names list all job names returned HUD param sha_grid list all job data returned HUD parallel index all_job_names param filtered_jobs_names set job names actually consider job_data = map_job_data all_job_names sha_grid job_statuses list JobStatus = job job_data job_statuses append JobStatus job job_data job jobs_to_alert_on = flaky_jobs = job_status job_statuses job_status job_name filtered_jobs_names continue job_status should_alert jobs_to_alert_on append job_status flaky_jobs extend job_status flaky_jobs jobs_to_alert_on flaky_jobs filter job names don t match regex filter_job_names job_names list str job_name_regex str - list str job_name_regex job_name job_name job_names re match job_name_regex job_name job_names get_recurrently_failing_jobs_alerts repo str branch str job_name_regex str - list dict str Any job_names sha_grid = fetch_hud_data repo=repo branch=branch filtered_job_names = set filter_job_names job_names job_name_regex job_name_regex print print f Filtered len filtered_job_names jobs len filtered_job_names == print No jobs matched regex len filtered_job_names == len job_names print All jobs matched regex print \n join filtered_job_names recurrently_failing_jobs flaky_jobs = classify_jobs job_names sha_grid filtered_job_names alerts = job recurrently_failing_jobs entry = AlertType Recurrently Failing Job AlertObject job job_name OncallTeams OncallIndividuals Flags sha job failure_chain - sha branch branch alerts append entry alerts parse_args - argparse Namespace parser = argparse ArgumentParser parser add_argument -- repo help= Repository do checks type=str default=os getenv REPO_TO_CHECK pytorch pytorch parser add_argument -- branch help= Branch do checks type=str default=os getenv BRANCH_TO_CHECK main parser add_argument -- job-name-regex help= Consider only job names matching given regex omitted all jobs matched type=str default=os getenv JOB_NAME_REGEX parser add_argument -- with-flaky-test-alert help= Run script flaky test alerting type=distutils util strtobool default=os getenv WITH_FLAKY_TEST_ALERT YES parser add_argument -- dry-run help= Whether actually post issues type=distutils util strtobool default=os getenv DRY_RUN YES parser parse_args __name__ == __main__ args = parse_args data = json dumps get_recurrently_failing_jobs_alerts args repo args branch args job_name_regex print data