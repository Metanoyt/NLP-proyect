Owner s oncall quantization torch torch nn nn torch ao nn intrinsic nni torch ao nn intrinsic quantized nniq torch ao nn quantized reference nnqr torch ao quantization torch ao nn quantized nnq torch ao nn quantized dynamic nnqd torch ao quantization get_default_static_quant_module_mappings default_float_qparams_observer PerChannelMinMaxObserver torch package PackageExporter PackageImporter torch testing _internal common_quantization QuantizationTestCase prepare_dynamic _make_conv_test_input skipIfNoFBGEMM lengths_to_offsets skipIfNoONEDNN _make_conv_add_extra_input_tensor torch testing _internal common_quantized _calculate_dynamic_qparams override_quantized_engine override_qengines qengine_is_qnnpack qengine_is_onednn torch testing _internal common_utils raise_on_run_directly torch fx hypothesis assume given hypothesis strategies st torch testing _internal hypothesis_utils hu hu assert_deadline_disabled copy io numpy np itertools Note tests file just API test make sure we wrapped quantized operator implementations correctly user facing APIs these correctness test underlying quantized operators For correctness test please see ` test quantization test_quantized_op py ` TestStaticQuantizedModule QuantizationTestCase test_relu relu_module = nn ReLU relu _module = nnq ReLU x = torch arange - dtype=torch float y_ref = torch relu x y _ref = torch nn modules ReLU x qx = torch quantize_per_tensor x dtype=torch qint qy = relu_module qx qy = relu _module qx assertEqual y_ref qy dequantize msg= ReLU module API failed assertEqual y _ref qy dequantize msg= ReLU module API failed override_qengines test_linear test API functionality nn quantized linear options = itertools product True False True False batch_size in_features out_features use_bias per_channel options _test_linear_api_impl nnq Linear QuantizedLinear torch ops quantized linear batch_size in_features out_features use_bias per_channel override_qengines test_linear_relu test API functionality nn intrinsic quantized linear_relu options = itertools product True False True False batch_size in_features out_features use_bias per_channel options _test_linear_api_impl nniq LinearReLU QuantizedLinearReLU torch ops quantized linear_relu batch_size in_features out_features use_bias per_channel _test_linear_api_impl qlinear_module module_name qlinear_op batch_size in_features out_features use_bias per_channel post_ops_kwargs torch backends quantized engine == qnnpack per_channel = False W = torch rand out_features in_features float per_channel scale_tensor = torch ones out_features dtype=torch double zero_point_tensor = torch zeros out_features dtype=torch long i range len scale_tensor scale_tensor i = i + W_q = torch quantize_per_channel W scales=scale_tensor zero_points=zero_point_tensor axis= dtype=torch qint ONEDNN only supports symmetric quantization weight W_zp = qengine_is_onednn W_q = torch quantize_per_tensor W W_zp torch qint X = torch rand batch_size in_features float X_q = torch quantize_per_tensor X torch quint B = torch rand out_features float use_bias None scale = zero_point = qlinear = qlinear_module in_features out_features post_ops_kwargs qlinear_copy = copy deepcopy qlinear set random quantized weight bias before test torch scriptable qlinear_copy set_weight_bias W_q B checkScriptable qlinear_copy X_q check_save_load=True Run module default-initialized parameters This tests constructor correct qlinear X_q qlinear set_weight_bias W_q B Simple round-trip test ensure weight set_weight API assertEqual qlinear weight W_q atol= e- rtol= testing packed param implementation qlinear scale = float scale qlinear zero_point = int zero_point Z_q = qlinear X_q Check module implementation matches calling ops directly W_pack = qlinear _packed_params _packed_params Z_ref = qlinear_op X_q W_pack scale zero_point post_ops_kwargs assertEqual Z_ref Z_q assertTrue module_name str qlinear Test serialization quantized Linear Module using state_dict model_dict = qlinear state_dict b = io BytesIO torch save model_dict b weights_only True False b seek loaded_dict = torch load b weights_only=weights_only key model_dict isinstance model_dict key torch _C ScriptObject assert isinstance loaded_dict key torch _C ScriptObject w_model b_model = torch ops quantized linear_unpack model_dict key w_loaded b_loaded = torch ops quantized linear_unpack loaded_dict key assertEqual w_model w_loaded assertEqual b_model b_loaded assertEqual model_dict key loaded_dict key loaded_qlinear = qlinear_module in_features out_features post_ops_kwargs loaded_qlinear load_state_dict loaded_dict linear_unpack = torch ops quantized linear_unpack assertEqual linear_unpack qlinear _packed_params _packed_params linear_unpack loaded_qlinear _packed_params _packed_params assertEqual qlinear scale loaded_qlinear scale assertEqual qlinear zero_point loaded_qlinear zero_point scripting will add __overloads__ __dict__ which why we script copy able do check next line checkScriptable copy deepcopy loaded_qlinear X_q check_save_load=True assertTrue dir qlinear == dir loaded_qlinear assertEqual qlinear _weight_bias loaded_qlinear _weight_bias assertEqual qlinear _weight_bias torch ops quantized linear_unpack qlinear _packed_params _packed_params Z_q = loaded_qlinear X_q assertEqual Z_q Z_q Test serialization b = io BytesIO torch save qlinear b b seek weights_only=False legacy code saves model loaded = torch load b weights_only=False assertEqual qlinear weight loaded weight assertEqual qlinear scale loaded scale assertEqual qlinear zero_point loaded zero_point Test torch package buffer = io BytesIO PackageExporter buffer pe pe save_pickle module qlinear pkl qlinear buffer seek importer = PackageImporter buffer loaded_from_package = importer load_pickle module qlinear pkl assertEqual qlinear weight loaded_from_package weight assertEqual qlinear scale loaded_from_package scale assertEqual qlinear zero_point loaded_from_package zero_point name _ loaded_from_package named_modules noop just make sure attribute _modules restored correctly during torch package assert name None noqa E Test copy deepcopy copied_linear = copy copy qlinear assertEqual copied_linear bias qlinear bias assertEqual copied_linear scale qlinear scale assertEqual copied_linear zero_point qlinear zero_point Y_copied = copied_linear X_q np testing assert_array_almost_equal Z_q int_repr numpy Y_copied int_repr numpy decimal= deepcopied_linear = copy deepcopy qlinear assertEqual deepcopied_linear bias qlinear bias assertEqual deepcopied_linear scale qlinear scale assertEqual deepcopied_linear zero_point qlinear zero_point Y_deepcopied = copied_linear X_q np testing assert_array_almost_equal Z_q int_repr numpy Y_deepcopied int_repr numpy decimal= Test JIT checkScriptable qlinear X_q check_save_load=True Make sure ` from_float ` works all linear variants modules_under_test = torch nn Linear torch nn modules linear NonDynamicallyQuantizableLinear mut modules_under_test Test from_float float_linear = mut in_features out_features float float_linear qconfig = torch ao quantization default_qconfig torch ao quantization prepare float_linear inplace=True float_linear X float Sequential allows swapping using convert quantized_float_linear = torch nn Sequential float_linear quantized_float_linear = torch ao quantization convert quantized_float_linear inplace=True Smoke test make sure module actually runs quantized_float_linear X_q Smoke test extra_repr assertTrue QuantizedLinear str quantized_float_linear test_quant_dequant_api r = torch tensor - - dtype=torch float scale zero_point dtype = torch qint testing Quantize API qr = torch quantize_per_tensor r scale zero_point dtype quant_m = nnq Quantize scale zero_point dtype qr = quant_m r assertEqual qr qr testing Dequantize API rqr = qr dequantize dequant_m = nnq DeQuantize rqr = dequant_m qr assertEqual rqr rqr _test_conv_api_impl module_name qconv_module conv_module batch_size in_channels_per_group input_feature_map_size out_channels_per_group groups kernel_size stride padding padding_mode dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias post_op use_channelwise X _scale= X _zero_point= i range len kernel_size assume input_feature_map_size i + padding i = dilation i kernel_size i - + in_channels = in_channels_per_group groups out_channels = out_channels_per_group groups X X_q W W_q b = _make_conv_test_input batch_size in_channels_per_group input_feature_map_size out_channels_per_group groups kernel_size X_scale X_zero_point W_scale W_zero_point use_bias use_channelwise example_input = X example_input_q = X_q post_op add add_relu X X _q = _make_conv_add_extra_input_tensor X _scale X _zero_point conv_module X size example_input = X X example_input_q = X_q X _q Make sure weight shape correct assertTrue qconv_module weight shape == W_q shape qconv_module set_weight_bias W_q b qconv_module scale = Y_scale qconv_module zero_point = Y_zero_point raw_conv_module = conv_module post_op relu add add_relu conv_module raw_conv_module weight data = W use_bias raw_conv_module bias data = b Test members assertTrue module_name == qconv_module _get_name module_name + + qconv_module _get_name assertTrue hasattr qconv_module _packed_params assertTrue hasattr qconv_module scale assertTrue hasattr qconv_module zero_point Test properties assertEqual W_q qconv_module weight use_bias assertEqual b qconv_module bias assertEqual Y_scale qconv_module scale assertEqual Y_zero_point qconv_module zero_point Test forward Y_exp = conv_module example_input Y_exp = torch quantize_per_tensor Y_exp scale=Y_scale zero_point=Y_zero_point dtype=torch quint Y_act = qconv_module example_input_q Make sure results match assert_array_almost_equal compares using following formula abs desired-actual -decimal https numpy org doc stable reference generated numpy testing assert_almost_equal html We use decimal = ignore off-by- differences between reference test Off-by- differences arise due order round zero_point addition operation i e addition followed round used reference round followed addition used test results may differ For example result round + while round + assuming rounding mode round-to-nearest ties-to-even skip numerics checking reference module np testing assert_array_almost_equal Y_exp int_repr numpy Y_act int_repr numpy decimal= Test serialization quantized Conv Module using state_dict model_dict = qconv_module state_dict assertEqual model_dict weight W_q use_bias assertEqual model_dict bias b bytes_io = io BytesIO torch save model_dict bytes_io weights_only True False bytes_io seek loaded_dict = torch load bytes_io weights_only=weights_only key loaded_dict assertEqual model_dict key loaded_dict key loaded_qconv_module = type qconv_module in_channels out_channels kernel_size stride padding dilation groups use_bias padding_mode=padding_mode loaded_qconv_module load_state_dict loaded_dict assertTrue dir loaded_qconv_module == dir qconv_module assertTrue module_name == loaded_qconv_module _get_name assertTrue hasattr loaded_qconv_module _packed_params assertTrue hasattr loaded_qconv_module _weight_bias assertEqual qconv_module weight loaded_qconv_module weight use_bias assertEqual qconv_module bias loaded_qconv_module bias assertEqual qconv_module scale loaded_qconv_module scale assertEqual qconv_module zero_point loaded_qconv_module zero_point Y_loaded = loaded_qconv_module example_input_q np testing assert_array_almost_equal Y_exp int_repr numpy Y_loaded int_repr numpy decimal= Test serialization b = io BytesIO torch save qconv_module b b seek weights_only=False legacy code saves model loaded_conv = torch load b weights_only=False assertEqual loaded_conv bias qconv_module bias assertEqual loaded_conv scale qconv_module scale assertEqual loaded_conv zero_point qconv_module zero_point Test copy deepcopy copied_conv = copy copy qconv_module assertEqual copied_conv bias qconv_module bias assertEqual copied_conv scale qconv_module scale assertEqual copied_conv zero_point qconv_module zero_point Y_copied = copied_conv example_input_q np testing assert_array_almost_equal Y_exp int_repr numpy Y_copied int_repr numpy decimal= deepcopied_conv = copy deepcopy qconv_module assertEqual deepcopied_conv bias qconv_module bias assertEqual deepcopied_conv scale qconv_module scale assertEqual deepcopied_conv zero_point qconv_module zero_point Y_deepcopied = deepcopied_conv example_input_q np testing assert_array_almost_equal Y_exp int_repr numpy Y_deepcopied int_repr numpy decimal= JIT testing checkScriptable qconv_module example_input_q check_save_load=True _FusedModule_two_input_args torch ao nn intrinsic _FusedModule Help Module ConvAdd d since torch ao nn intrinsic _FusedModule only support one input arg forward x x input = x x input Test from_float fused_conv_module = _FusedModule_two_input_args conv_module \ post_op add add_relu torch ao nn intrinsic _FusedModule conv_module fused_conv_module qconfig = torch ao quantization default_qconfig torch ao quantization prepare fused_conv_module inplace=True example_input = example_input float fused_conv_module example_input converted_qconv_module = fused_conv_module reference_mapping = get_default_static_quant_module_mappings reference_mapping type conv_module = type qconv_module torch ao quantization convert converted_qconv_module mapping=reference_mapping inplace=True Smoke test make sure module actually runs use_bias assertEqual conv_module bias post_op relu add add_relu conv_module bias converted_qconv_module bias Smoke test extra_repr assertTrue module_name == converted_qconv_module _get_name override_qengines test_conv d_api options = itertools product zeros reflect pad_mode True False use_bias True False use_channelwise pad_mode use_bias use_channelwise options torch backends quantized engine == qnnpack use_channelwise = False batch_size = in_channels_per_group = length = out_channels_per_group = groups = kernel = stride = pad = dilation = Tests correctness conv d module in_channels = in_channels_per_group groups out_channels = out_channels_per_group groups input_feature_map_size = length kernel_size = kernel stride = stride pad = pad dilation = dilation X_scale = X_zero_point = W_scale = W_zero_point = qengine_is_onednn Y_scale = Y_zero_point = torch backends quantized engine == qnnpack use_channelwise = False qconv_cls = nnq Conv d module_name = QuantizedConv d qconv_module = qconv_cls in_channels out_channels kernel stride pad dilation groups use_bias padding_mode=pad_mode conv_module = nn Conv d in_channels out_channels kernel stride pad dilation groups use_bias padding_mode=pad_mode conv_module = conv_module float _test_conv_api_impl module_name qconv_module conv_module batch_size in_channels_per_group input_feature_map_size out_channels_per_group groups kernel_size stride pad pad_mode dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias none use_channelwise override_qengines test_conv d_relu_api options = itertools product zeros reflect pad_mode True False use_bias True False use_channelwise batch_size = in_channels_per_group = length = out_channels_per_group = groups = kernel = stride = pad = dilation = Tests correctness conv d module in_channels = in_channels_per_group groups out_channels = out_channels_per_group groups input_feature_map_size = length kernel_size = kernel stride = stride pad = pad dilation = dilation X_scale = X_zero_point = W_scale = W_zero_point = qengine_is_onednn Y_scale = Y_zero_point = qconv_cls = nniq ConvReLU d module_name = QuantizedConvReLU d pad_mode use_bias use_channelwise options torch backends quantized engine == qnnpack use_channelwise = False qconv_module = qconv_cls in_channels out_channels kernel stride pad dilation groups use_bias padding_mode=pad_mode conv_module = nn Conv d in_channels out_channels kernel stride pad dilation groups use_bias padding_mode=pad_mode relu_module = nn ReLU conv_module = nni ConvReLU d conv_module relu_module conv_module = conv_module float _test_conv_api_impl module_name qconv_module conv_module batch_size in_channels_per_group input_feature_map_size out_channels_per_group groups kernel_size stride pad pad_mode dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias relu use_channelwise override_qengines test_conv d_api options = itertools product zeros reflect pad_mode True False use_bias True False use_channelwise pad_mode use_bias use_channelwise options torch backends quantized engine == qnnpack use_channelwise = False batch_size = in_channels_per_group = H = W = out_channels_per_group = groups = kernel_h = kernel_w = stride_h = stride_w = pad_h = pad_w = dilation = Tests correctness conv d module in_channels = in_channels_per_group groups out_channels = out_channels_per_group groups input_feature_map_size = H W kernel_size = kernel_h kernel_w stride = stride_h stride_w padding = pad_h pad_w dilation = dilation dilation X_scale = X_zero_point = W_scale = W_zero_point = qengine_is_onednn Y_scale = Y_zero_point = qconv_cls = nnq Conv d module_name = QuantizedConv d qconv_module = qconv_cls in_channels out_channels kernel_size stride padding dilation groups use_bias padding_mode=pad_mode conv_module = nn Conv d in_channels out_channels kernel_size stride padding dilation groups use_bias padding_mode=pad_mode conv_module = conv_module float _test_conv_api_impl module_name qconv_module conv_module batch_size in_channels_per_group input_feature_map_size out_channels_per_group groups kernel_size stride padding pad_mode dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias none use_channelwise override_qengines test_conv d_relu_api options = itertools product zeros reflect pad_mode True False use_bias True False use_channelwise batch_size = in_channels_per_group = H = W = out_channels_per_group = groups = kernel_h = kernel_w = stride_h = stride_w = pad_h = pad_w = dilation = Tests correctness conv d module in_channels = in_channels_per_group groups out_channels = out_channels_per_group groups input_feature_map_size = H W kernel_size = kernel_h kernel_w stride = stride_h stride_w padding = pad_h pad_w dilation = dilation dilation X_scale = X_zero_point = W_scale = W_zero_point = qengine_is_onednn Y_scale = Y_zero_point = qconv_cls = nniq ConvReLU d module_name = QuantizedConvReLU d pad_mode use_bias use_channelwise options torch backends quantized engine == qnnpack use_channelwise = False qconv_module = qconv_cls in_channels out_channels kernel_size stride padding dilation groups use_bias padding_mode=pad_mode conv_module = nn Conv d in_channels out_channels kernel_size stride padding dilation groups use_bias padding_mode=pad_mode relu_module = nn ReLU conv_module = nni ConvReLU d conv_module relu_module conv_module = conv_module float _test_conv_api_impl module_name qconv_module conv_module batch_size in_channels_per_group input_feature_map_size out_channels_per_group groups kernel_size stride padding pad_mode dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias relu use_channelwise skipIfNoFBGEMM test_conv d_api options = itertools product True False use_bias True False use_channelwise batch_size = in_channels_per_group = H = W = D = out_channels_per_group = groups = kernel_h = kernel_w = kernel_d = stride_h = stride_w = stride_d = pad_mode = zeros d doesn t support reflect padding pad_h = pad_w = pad_d = dilation = Tests correctness conv d module in_channels = in_channels_per_group groups out_channels = out_channels_per_group groups input_feature_map_size = D H W kernel_size = kernel_d kernel_h kernel_w stride = stride_d stride_h stride_w padding = pad_d pad_h pad_w dilation = dilation dilation dilation X_scale = X_zero_point = W_scale = W_zero_point = qengine_is_onednn Y_scale = Y_zero_point = qconv_cls = nnq Conv d module_name = QuantizedConv d use_bias use_channelwise options torch backends quantized engine == qnnpack use_channelwise = False override_quantized_engine fbgemm qconv_module = qconv_cls in_channels out_channels kernel_size stride padding dilation groups use_bias padding_mode=pad_mode conv_module = nn Conv d in_channels out_channels kernel_size stride padding dilation groups use_bias padding_mode=pad_mode conv_module = conv_module float _test_conv_api_impl module_name qconv_module conv_module batch_size in_channels_per_group input_feature_map_size out_channels_per_group groups kernel_size stride padding pad_mode dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias none use_channelwise skipIfNoFBGEMM test_conv d_relu_api options = itertools product True False use_bias True False use_channelwise batch_size = in_channels_per_group = H = W = D = out_channels_per_group = groups = kernel_h = kernel_w = kernel_d = stride_h = stride_w = stride_d = pad_mode = zeros d doesn t support reflect padding pad_h = pad_w = pad_d = dilation = Tests correctness conv d module in_channels = in_channels_per_group groups out_channels = out_channels_per_group groups input_feature_map_size = D H W kernel_size = kernel_d kernel_h kernel_w stride = stride_d stride_h stride_w padding = pad_d pad_h pad_w dilation = dilation dilation dilation X_scale = X_zero_point = W_scale = W_zero_point = qengine_is_onednn Y_scale = Y_zero_point = qconv_cls = nniq ConvReLU d module_name = QuantizedConvReLU d use_bias use_channelwise options torch backends quantized engine == qnnpack use_channelwise = False override_quantized_engine fbgemm qconv_module = qconv_cls in_channels out_channels kernel_size stride padding dilation groups use_bias padding_mode=pad_mode conv_module = nn Conv d in_channels out_channels kernel_size stride padding dilation groups use_bias padding_mode=pad_mode relu_module = nn ReLU conv_module = nni ConvReLU d conv_module relu_module conv_module = conv_module float _test_conv_api_impl module_name qconv_module conv_module batch_size in_channels_per_group input_feature_map_size out_channels_per_group groups kernel_size stride padding pad_mode dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias relu use_channelwise skipIfNoONEDNN test_conv d_add test API functionality nn intrinsic quantized ConvAdd d override_quantized_engine onednn options = itertools product zeros reflect pad_mode True False use_bias True False use_channelwise batch_size = in_channels_per_group = H = W = out_channels_per_group = groups = kernel_h = kernel_w = stride_h = stride_w = pad_h = pad_w = dilation = Tests correctness conv d module in_channels = in_channels_per_group groups out_channels = out_channels_per_group groups input_feature_map_size = H W kernel_size = kernel_h kernel_w stride = stride_h stride_w padding = pad_h pad_w dilation = dilation dilation X_scale = X_zero_point = X _scale = X _zero_point = W_scale = W_zero_point = qengine_is_onednn Y_scale = Y_zero_point = qconv_cls = nniq ConvAdd d module_name = QuantizedConvAdd d pad_mode use_bias use_channelwise options qconv_module = qconv_cls in_channels out_channels kernel_size stride padding dilation groups use_bias padding_mode=pad_mode conv_module = nn Conv d in_channels out_channels kernel_size stride padding dilation groups use_bias padding_mode=pad_mode conv_module = torch ao nn intrinsic ConvAdd d conv_module torch add conv_module = conv_module float _test_conv_api_impl module_name qconv_module conv_module batch_size in_channels_per_group input_feature_map_size out_channels_per_group groups kernel_size stride padding pad_mode dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias add use_channelwise X _scale X _zero_point skipIfNoONEDNN test_conv d_add_relu test API functionality nn intrinsic quantized ConvAdd d override_quantized_engine onednn options = itertools product zeros reflect pad_mode True False use_bias True False use_channelwise batch_size = in_channels_per_group = H = W = out_channels_per_group = groups = kernel_h = kernel_w = stride_h = stride_w = pad_h = pad_w = dilation = Tests correctness conv d module in_channels = in_channels_per_group groups out_channels = out_channels_per_group groups input_feature_map_size = H W kernel_size = kernel_h kernel_w stride = stride_h stride_w padding = pad_h pad_w dilation = dilation dilation X_scale = X_zero_point = X _scale = X _zero_point = W_scale = W_zero_point = qengine_is_onednn Y_scale = Y_zero_point = qconv_cls = nniq ConvAddReLU d module_name = QuantizedConvAddReLU d pad_mode use_bias use_channelwise options qconv_module = qconv_cls in_channels out_channels kernel_size stride padding dilation groups use_bias padding_mode=pad_mode conv_module = nn Conv d in_channels out_channels kernel_size stride padding dilation groups use_bias padding_mode=pad_mode conv_module = torch ao nn intrinsic ConvAddReLU d conv_module torch add nn ReLU conv_module = conv_module float _test_conv_api_impl module_name qconv_module conv_module batch_size in_channels_per_group input_feature_map_size out_channels_per_group groups kernel_size stride padding pad_mode dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias add_relu use_channelwise X _scale X _zero_point test_pool_api Tests correctness pool module The correctness defined against functional implementation N C H W = kwargs = kernel_size stride None padding dilation scale zero_point = X = torch randn N C H W dtype=torch float qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch quint qX_expect = torch nn functional max_pool d qX kwargs pool_under_test = torch ao nn quantized MaxPool d kwargs qX_hat = pool_under_test qX assertEqual qX_expect qX_hat JIT Testing checkScriptable pool_under_test X test_dropout Tests correctness dropout module The correctness defined against functional implementation x = torch randn dtype=torch float float_mod = torch nn Dropout p= float_mod training = False y_ref = float_mod x quant_ref = torch quantize_per_tensor y_ref dtype=torch quint quant_mod = nnq Dropout p= qx = torch quantize_per_tensor x dtype=torch quint qy = quant_mod qx assertEqual quant_ref int_repr numpy qy int_repr numpy msg= Dropout module API failed _test_dropout_serialization get_model data data m = get_model m qconfig = torch ao quantization default_qconfig mp = torch ao quantization prepare m mp data mq = torch ao quantization convert mp ref = mq data m = get_model m qconfig = torch ao quantization default_qconfig mp = torch ao quantization prepare m mq = torch ao quantization convert mp mq load_state_dict mq state_dict ref = mq data assertTrue torch allclose ref ref test_dropout_serialization data = torch randn data = torch randn _get_model nn Sequential torch ao quantization QuantStub nn Dropout p= torch ao quantization DeQuantStub eval _test_dropout_serialization _get_model data data test_batch_norm d Tests correctness batchnorm d module The correctness defined against functional implementation x = torch randn dtype=torch float float_mod = torch nn BatchNorm d float_mod training = False y_ref = float_mod x quant_ref = torch quantize_per_tensor y_ref dtype=torch quint quant_mod = nnq BatchNorm d qx = torch quantize_per_tensor x dtype=torch quint qy = quant_mod qx assertEqual quant_ref int_repr numpy qy int_repr numpy msg= BatchNorm d module API failed test_batch_norm d Tests correctness batchnorm d module The correctness defined against functional implementation x = torch randn dtype=torch float float_mod = torch nn BatchNorm d float_mod training = False y_ref = float_mod x quant_ref = torch quantize_per_tensor y_ref dtype=torch quint quant_mod = nnq BatchNorm d qx = torch quantize_per_tensor x dtype=torch quint qy = quant_mod qx assertEqual quant_ref int_repr numpy qy int_repr numpy msg= BatchNorm d module API failed _test_batch_norm_serialization get_model data data m = get_model m qconfig = torch ao quantization default_qconfig mp = torch ao quantization prepare m mp data mq = torch ao quantization convert mp ref = mq data m = get_model m qconfig = torch ao quantization default_qconfig mp = torch ao quantization prepare m mq = torch ao quantization convert mp mq load_state_dict mq state_dict ref = mq data assertTrue torch allclose ref ref test_batch_norm d_serialization data = torch randn data = torch randn _get_model nn Sequential torch ao quantization QuantStub nn BatchNorm d torch ao quantization DeQuantStub eval _test_batch_norm_serialization _get_model data data test_batch_norm d_serialization data = torch randn data = torch randn _get_model nn Sequential torch ao quantization QuantStub nn BatchNorm d torch ao quantization DeQuantStub eval _test_batch_norm_serialization _get_model data data test_layer_norm Tests correctness layernorm module The correctness defined against functional implementation x_scale = x_zero_point = y_scale = y_zero_point = dims = X = torch randn dims dtype=torch float - qX = torch quantize_per_tensor X x_scale x_zero_point dtype=torch quint dqX = qX dequantize float_mod = torch nn LayerNorm dqX size float float_mod weight = torch nn Parameter torch rand dims float_mod bias = torch nn Parameter torch rand dims dqY_ref = float_mod dqX qY_ref = torch quantize_per_tensor dqY_ref y_scale y_zero_point dtype=torch quint quant_mod = nnq LayerNorm qX size float_mod weight float_mod bias y_scale y_zero_point qY = quant_mod qX assertEqual qY_ref int_repr numpy qY int_repr numpy msg=f LayerNorm module API failed qY_ref\n qY_ref vs qY\n qY test_group_norm Tests correctness groupnorm module The correctness defined against functional implementation x_scale = x_zero_point = y_scale = y_zero_point = dims = X = torch randn dims dtype=torch float - qX = torch quantize_per_tensor X x_scale x_zero_point dtype=torch quint dqX = qX dequantize float_mod = torch nn GroupNorm float float_mod weight = torch nn Parameter torch rand dims float_mod bias = torch nn Parameter torch rand dims dqY_ref = float_mod dqX qY_ref = torch quantize_per_tensor dqY_ref y_scale y_zero_point dtype=torch quint quant_mod = nnq GroupNorm float_mod weight float_mod bias y_scale y_zero_point qY = quant_mod qX assertEqual qY_ref int_repr numpy qY int_repr numpy msg=f GroupNorm module API failed qY_ref\n qY_ref vs qY\n qY test_instance_norm Tests correctness instancenorm n d modules The correctness defined against functional implementation x_scale = x_zero_point = y_scale = y_zero_point = dims_to_modules = torch nn InstanceNorm d nnq InstanceNorm d torch nn InstanceNorm d nnq InstanceNorm d torch nn InstanceNorm d nnq InstanceNorm d dim_to_modules dims_to_modules dims float_cls q_cls = dim_to_modules X = torch randn dims dtype=torch float - qX = torch quantize_per_tensor X x_scale x_zero_point dtype=torch quint dqX = qX dequantize float_mod = float_cls dims float float_mod weight = torch nn Parameter torch rand dims float_mod bias = torch nn Parameter torch rand dims dqY_ref = float_mod dqX qY_ref = torch quantize_per_tensor dqY_ref y_scale y_zero_point dtype=torch quint quant_mod = q_cls dims float_mod weight float_mod bias y_scale y_zero_point qY = quant_mod qX assertEqual qY_ref int_repr numpy qY int_repr numpy msg=f InstanceNorm module API failed qY_ref\n qY_ref vs qY\n qY _test_activation_module_impl name float_module_class quantized_module_class extra_kwargs Tests correctness ELU module The correctness defined against functional implementation x_scale = x_zero_point = y_scale = y_zero_point = dims = X = torch randn dims dtype=torch float - qX = torch quantize_per_tensor X x_scale x_zero_point dtype=torch quint dqX = qX dequantize float_mod = float_module_class extra_kwargs float dqY_ref = float_mod dqX qY_ref = torch quantize_per_tensor dqY_ref y_scale y_zero_point dtype=torch quint quant_mod = quantized_module_class y_scale y_zero_point extra_kwargs qY = quant_mod qX assertEqual qY_ref int_repr numpy qY int_repr numpy msg=f name module API failed qY_ref\n qY_ref vs qY\n qY _test_leaky_relu_serialization scale_original = zero_point_original = quant_mod_original = nnq LeakyReLU scale_original zero_point_original state_dict = quant_mod_original state_dict scale_new = zero_point_new = quant_mod_new = nnq LeakyReLU scale_new zero_point_new quant_mod_new load_state_dict state_dict assertEqual quant_mod_original scale quant_mod_new scale assertEqual quant_mod_original zero_point quant_mod_new zero_point test_elu Tests correctness ELU module The correctness defined against functional implementation _test_activation_module_impl ELU nn ELU nnq ELU alpha test_leaky_relu _test_activation_module_impl LeakyReLU nn LeakyReLU nnq LeakyReLU negative_slope _test_leaky_relu_serialization test_sigmoid _test_activation_module_impl Sigmoid nn Sigmoid nnq Sigmoid _test_hard_swish_serialization scale_original = zero_point_original = quant_mod_original = nnq Hardswish scale_original zero_point_original state_dict = quant_mod_original state_dict scale_new = zero_point_new = quant_mod_new = nnq Hardswish scale_new zero_point_new quant_mod_new load_state_dict state_dict assertEqual quant_mod_original scale quant_mod_new scale assertEqual quant_mod_original zero_point quant_mod_new zero_point test_hard_swish _test_activation_module_impl Hardswish nn Hardswish nnq Hardswish _test_hard_swish_serialization given num_embeddings=st integers embedding_dim=st integers filter lambda x x == set_qconfig=st booleans skipIfNoFBGEMM test_embedding_api num_embeddings embedding_dim set_qconfig num_lengths = np random randint lengths = np random randint size=num_lengths astype np int num_indices = np sum lengths indices = torch from_numpy np random randint low= high=num_embeddings size=num_indices dtype=np int weights = torch from_numpy np random random_sample num_embeddings embedding_dim + astype np float obs = default_float_qparams_observer obs weights qparams = obs calculate_qparams dtypes = torch quint x torch quint embedding_funcs = torch ops quantized embedding_ bit torch ops quantized embedding_byte dtype embedding_func zip dtypes embedding_funcs Quantize weights qweight = torch quantize_per_channel weights qparams qparams axis= dtype=dtype qemb = nnq Embedding num_embeddings=num_embeddings embedding_dim=embedding_dim dtype=dtype qemb set_weight qweight qemb indices Ensure module has correct weights assertEqual qweight qemb weight w_packed = qemb _packed_params _packed_weight module_out = qemb indices Call bit qembedding operator directly ref = embedding_func w_packed indices pruned_weights=False assertEqual module_out ref checkEmbeddingSerialization qemb num_embeddings embedding_dim indices None set_qconfig=False is_emb_bag=False dtype=dtype given num_embeddings=st integers embedding_dim=st integers filter lambda x x == num_offsets=st integers set_qconfig=st booleans skipIfNoFBGEMM test_embedding_bag_api num_embeddings embedding_dim num_offsets set_qconfig r Test execution serialization dynamic quantized embedding_bag modules int num_lengths = np random randint lengths = np random randint size=num_lengths astype np int num_indices = np sum lengths indices = torch from_numpy np random randint low= high=num_embeddings size=num_indices dtype=np int offsets = lengths_to_offsets lengths include last offset offsets = torch cat offsets torch tensor indices size dtype=torch long weights = torch from_numpy np random random_sample num_embeddings embedding_dim + astype np float qdtype torch quint torch quint x obs = PerChannelMinMaxObserver dtype=qdtype qscheme=torch per_channel_affine_float_qparams ch_axis= obs weights Get scale zero point weight tensor qparams = obs calculate_qparams Quantize weights bits qweight = torch quantize_per_channel weights qparams qparams axis= dtype=qdtype qemb = nnq EmbeddingBag num_embeddings=num_embeddings embedding_dim=embedding_dim include_last_offset=True mode= sum _weight=qweight dtype=qdtype qemb indices offsets Ensure module has correct weights assertEqual qweight qemb weight w_packed = qemb _packed_params _packed_weight module_out = qemb indices offsets Call qembedding_bag operator directly qdtype == torch quint ref = torch ops quantized embedding_bag_byte w_packed indices offsets mode= per_sample_weights=None include_last_offset=True ref = torch ops quantized embedding_bag_ bit w_packed indices offsets mode= per_sample_weights=None include_last_offset=True assertEqual module_out ref checkEmbeddingSerialization qemb num_embeddings embedding_dim indices offsets set_qconfig is_emb_bag=True dtype=qdtype test_prelu num_parameters range x = torch randn num_parameters qx = torch quantize_per_tensor_dynamic x dtype=torch quint reduce_range=False f_prelu = torch nn PReLU num_parameters=num_parameters f_prelu weight = torch nn Parameter torch randn num_parameters abs f_prelu qconfig = torch ao quantization QConfig activation=torch ao quantization default_observer weight=torch ao quantization default_observer f_prelu activation_post_process = f_prelu qconfig activation f_prelu activation_post_process f_prelu x q_prelu = nnq PReLU from_float f_prelu w_obs = f_prelu qconfig weight w_obs f_prelu weight w_scale w_zp = w_obs calculate_qparams q_prelu_weight = torch quantize_per_tensor f_prelu weight dtype=torch quint scale=w_scale zero_point=w_zp dequantize check weight makes sense assertEqual q_prelu weight dequantize q_prelu_weight f_prelu weight = torch nn Parameter q_prelu weight dequantize qy = q_prelu qx qy_ref = torch quantize_per_tensor f_prelu qx dequantize q_prelu scale q_prelu zero_point dtype=torch quint check output makes sense assertEqual qy qy_ref atol= rtol= test_channel_shuffle Tests correctness ChannelShuffle module x_scale = x_zero_point = y_scale = x_scale y_zero_point = x_zero_point dims = groups = X = torch randn dims dtype=torch float - qX = torch quantize_per_tensor X x_scale x_zero_point dtype=torch quint dqX = qX dequantize float_mod = torch nn ChannelShuffle groups float dqY_ref = float_mod dqX qY_ref = torch quantize_per_tensor dqY_ref y_scale y_zero_point dtype=torch quint quant_mod = torch nn ChannelShuffle groups qY = quant_mod qX assertEqual qY_ref int_repr numpy qY int_repr numpy msg=f ChannelShuffle module API failed qY_ref\n qY_ref vs qY\n qY skipIfNoONEDNN test_linear_leaky_relu test API functionality nn intrinsic quantized linear_leaky_relu override_quantized_engine onednn options = itertools product batch size in_features out_features True False use_bias True False per_channel negative slope batch_size in_features out_features use_bias per_channel neg_slope options _test_linear_api_impl nniq LinearLeakyReLU QuantizedLinearLeakyReLU torch ops quantized linear_leaky_relu batch_size in_features out_features use_bias per_channel negative_slope=neg_slope skipIfNoONEDNN test_linear_tanh test API functionality nn intrinsic quantized linear_tanh override_quantized_engine onednn options = itertools product batch size in_features out_features True False use_bias True False negative slope batch_size in_features out_features use_bias per_channel options _test_linear_api_impl nniq LinearTanh QuantizedLinearTanh torch ops quantized linear_tanh batch_size in_features out_features use_bias per_channel TestDynamicQuantizedModule QuantizationTestCase _test_qconv_impl q_mod dq_mod dim dtype bias in_channels = out_channels = kernel_size = stride = padding = dilation = groups = padding_mode = zeros qengine_is_qnnpack reduce_range = False reduce_range = True X_fp = torch randn in_channels dim s z = _calculate_dynamic_qparams X_fp dtype reduce_range X_q = torch quantize_per_tensor X_fp s z dtype X_dq = torch dequantize X_q quantized_module = q_mod in_channels out_channels kernel_size stride=stride padding=padding dilation=dilation groups=groups bias=bias padding_mode=padding_mode dynamic_module = dq_mod in_channels out_channels kernel_size stride=stride padding=padding dilation=dilation groups=groups bias=bias padding_mode=padding_mode quantized_module scale quantized_module zero_point = s z dynamic_module set_weight_bias quantized_module _weight_bias Y_q_ref = quantized_module X_q Y_ref = torch dequantize Y_q_ref Y = dynamic_module X_dq reduce_range assertEqual Y Y_ref Test serialization quantized Conv Module using state_dict W_q b = dynamic_module _weight_bias model_dict = dynamic_module state_dict assertEqual model_dict weight W_q assertEqual model_dict bias b bytes_io = io BytesIO torch save model_dict bytes_io weights_only True False bytes_io seek loaded_dict = torch load bytes_io weights_only=weights_only key loaded_dict assertEqual model_dict key loaded_dict key loaded_qconv_module = type dynamic_module in_channels out_channels kernel_size stride=stride padding=padding dilation=dilation groups=groups bias=bias padding_mode=padding_mode loaded_qconv_module load_state_dict loaded_dict assertTrue dir loaded_qconv_module == dir dynamic_module assertTrue dynamic_module _get_name == loaded_qconv_module _get_name assertTrue hasattr loaded_qconv_module _packed_params assertTrue hasattr loaded_qconv_module _weight_bias assertEqual dynamic_module weight loaded_qconv_module weight bias assertEqual dynamic_module bias loaded_qconv_module bias assertEqual dynamic_module scale loaded_qconv_module scale assertEqual dynamic_module zero_point loaded_qconv_module zero_point Y_loaded = loaded_qconv_module X_fp reduce_range np testing assert_array_almost_equal Y numpy Y_loaded numpy decimal= Test serialization b = io BytesIO torch save dynamic_module b b seek weights_only=False legacy code saves model loaded_conv = torch load b weights_only=False assertEqual loaded_conv bias dynamic_module bias assertEqual loaded_conv scale dynamic_module scale assertEqual loaded_conv zero_point dynamic_module zero_point Test copy deepcopy copied_conv = copy copy dynamic_module assertEqual copied_conv bias dynamic_module bias assertEqual copied_conv scale dynamic_module scale assertEqual copied_conv zero_point dynamic_module zero_point Y_copied = copied_conv X_fp reduce_range np testing assert_array_almost_equal Y numpy Y_copied numpy decimal= deepcopied_conv = copy deepcopy dynamic_module assertEqual deepcopied_conv bias dynamic_module bias assertEqual deepcopied_conv scale dynamic_module scale assertEqual deepcopied_conv zero_point dynamic_module zero_point Y_deepcopied = copied_conv X_fp reduce_range np testing assert_array_almost_equal Y numpy Y_deepcopied numpy decimal= need fix JIT testing checkScriptable dynamic_module X_dq check_save_load=True Test from_float conv_module = dynamic_module _FLOAT_MODULE in_channels out_channels kernel_size conv_module qconfig = torch ao quantization default_dynamic_qconfig type ignore assignment prepare_dynamic conv_module conv_module X_dq quantized_conv_module = dq_mod from_float conv_module Smoke test make sure module actually runs quantized_conv_module X_dq Smoke test extra_repr assertEqual dynamic_module _get_name quantized_conv_module _get_name override_qengines test_dynamic_conv d q_mod = torch ao nn quantized Conv d dq_mod = torch ao nn quantized dynamic Conv d dim = dtype = torch quint bias True False _test_qconv_impl q_mod dq_mod dim dtype bias override_qengines test_dynamic_conv d q_mod = torch ao nn quantized Conv d dq_mod = torch ao nn quantized dynamic Conv d dim = dtype = torch quint bias True False _test_qconv_impl q_mod dq_mod dim dtype bias override_qengines test_dynamic_conv d q_mod = torch ao nn quantized Conv d dq_mod = torch ao nn quantized dynamic Conv d dim = dtype = torch quint qengine_is_qnnpack qnnpack doesn t support unpacking conv d bias True False _test_qconv_impl q_mod dq_mod dim dtype bias override_qengines test_dynamic_convtranspose d q_mod = torch ao nn quantized ConvTranspose d dq_mod = torch ao nn quantized dynamic ConvTranspose d dim = dtype = torch quint bias True False _test_qconv_impl q_mod dq_mod dim dtype bias override_qengines test_dynamic_convtranspose d q_mod = torch ao nn quantized ConvTranspose d dq_mod = torch ao nn quantized dynamic ConvTranspose d dim = dtype = torch quint bias True False _test_qconv_impl q_mod dq_mod dim dtype bias override_qengines test_dynamic_convtranspose d q_mod = torch ao nn quantized ConvTranspose d dq_mod = torch ao nn quantized dynamic ConvTranspose d dim = dtype = torch quint qengine_is_qnnpack qnnpack doesn t support unpacking conv d bias True False _test_qconv_impl q_mod dq_mod dim dtype bias given batch_size=st integers in_features=st integers out_features=st integers use_bias=st booleans use_default_observer=st booleans override_qengines test_linear_api batch_size in_features out_features use_bias use_default_observer test API functionality nn quantized dynamic Linear W = torch rand out_features in_features float qscheme = torch per_tensor_symmetric qengine_is_onednn torch per_tensor_affine W_scale W_zp = _calculate_dynamic_qparams W torch qint qscheme=qscheme W_q = torch quantize_per_tensor W W_scale W_zp torch qint X = torch rand batch_size in_features float B = torch rand out_features float use_bias None qlinear = nnqd Linear in_features out_features Run module default-initialized parameters This tests constructor correct qlinear set_weight_bias W_q B qlinear X Simple round-trip test ensure weight set_weight API assertEqual qlinear weight W_q W_pack = qlinear _packed_params _packed_params Z_dq = qlinear X Check module implementation matches calling ops directly Z_ref = torch ops quantized linear_dynamic X W_pack reduce_range=True assertEqual Z_ref Z_dq Test serialization dynamic quantized Linear Module using state_dict model_dict = qlinear state_dict b = io BytesIO torch save model_dict b weights_only True False b seek loaded_dict = torch load b weights_only=weights_only key model_dict isinstance model_dict key torch _C ScriptObject assert isinstance loaded_dict key torch _C ScriptObject w_model b_model = torch ops quantized linear_unpack model_dict key w_loaded b_loaded = torch ops quantized linear_unpack loaded_dict key assertEqual w_model w_loaded assertEqual b_model b_loaded assertEqual model_dict key loaded_dict key loaded_qlinear = nnqd Linear in_features out_features loaded_qlinear load_state_dict loaded_dict linear_unpack = torch ops quantized linear_unpack assertEqual linear_unpack qlinear _packed_params _packed_params linear_unpack loaded_qlinear _packed_params _packed_params use_bias assertEqual qlinear bias loaded_qlinear bias assertTrue dir qlinear == dir loaded_qlinear assertTrue hasattr qlinear _packed_params assertTrue hasattr loaded_qlinear _packed_params assertTrue hasattr qlinear _weight_bias assertTrue hasattr loaded_qlinear _weight_bias assertEqual qlinear _weight_bias loaded_qlinear _weight_bias assertEqual qlinear _weight_bias torch ops quantized linear_unpack qlinear _packed_params _packed_params Z_dq = qlinear X assertEqual Z_dq Z_dq b = io BytesIO torch save qlinear b b seek weights_only=False legacy code saves model loaded = torch load b weights_only=False assertEqual qlinear weight loaded weight assertEqual qlinear zero_point loaded zero_point Test JIT checkScriptable qlinear X check_save_load=True modules_under_test = torch nn Linear torch nn modules linear NonDynamicallyQuantizableLinear mut modules_under_test Test from_float float_linear = mut in_features out_features float use_default_observer float_linear qconfig = torch ao quantization default_dynamic_qconfig prepare_dynamic float_linear float_linear X float quantized_float_linear = nnqd Linear from_float float_linear Smoke test make sure module actually runs quantized_float_linear X Smoke test extra_repr assertTrue QuantizedLinear str quantized_float_linear given dtype=st sampled_from torch qint torch float bidirectional=st booleans override_qengines test_lstm_api dtype bidirectional r Test execution serialization dynamic quantized lstm modules int fp Check module matches numerics op ensure module can instantiated all engines dtypes seq_len = batch = input_size = hidden_size = num_layers = bias = True weight_keys = bias_keys = num_directions = bidirectional layer range num_layers direction range num_directions suffix = _reverse direction == key_name = f weight_ih_l layer suffix key_name = f weight_hh_l layer suffix weight_keys append key_name weight_keys append key_name key_name = f bias_ih_l layer suffix key_name = f bias_hh_l layer suffix bias_keys append key_name bias_keys append key_name dtype == torch float torch backends quantized engine qnnpack onednn fp dynamic quant supported qnnpack onednn x = torch randn seq_len batch input_size h = torch randn num_layers bidirectional + batch hidden_size c = torch randn num_layers bidirectional + batch hidden_size cell_dq = torch ao nn quantized dynamic LSTM input_size=input_size hidden_size=hidden_size num_layers=num_layers bias=bias batch_first=False dropout= bidirectional=bidirectional dtype=dtype ref_dq = torch ao nn quantized dynamic LSTM input_size=input_size hidden_size=hidden_size num_layers=num_layers bias=bias batch_first=False dropout= bidirectional=bidirectional dtype=dtype _all_params = m param m cell_dq _all_weight_values result = torch quantized_lstm x h c _all_params cell_dq bias cell_dq num_layers float cell_dq dropout False bidirectional False dtype=dtype use_dynamic=True y h c = cell_dq x h c assertEqual result y assertEqual result h assertEqual result c x = torch randn check_eager_serialization cell_dq ref_dq x check_weight_bias_api cell_dq weight_keys bias_keys override_qengines test_gru_api r Test execution serialization dynamic quantized lstm modules int fp Check module matches numerics op ensure module can instantiated all engines dtypes dtype torch qint torch float dtype == torch float torch backends quantized engine qnnpack onednn fp dynamic quant supported qnnpack onednn continue Test default instantiation seq_len = batch = input_size = hidden_size = num_layers = bias = True bidirectional = False x = torch rand seq_len batch input_size h = torch rand num_layers bidirectional + batch hidden_size cell_dq = torch ao nn quantized dynamic GRU input_size=input_size hidden_size=hidden_size num_layers=num_layers bias=bias batch_first=False dropout= bidirectional=bidirectional dtype=dtype _all_params = m param m cell_dq _all_weight_values result = torch quantized_gru x h _all_params cell_dq bias cell_dq num_layers float cell_dq dropout False bidirectional False y h = cell_dq x h assertEqual result y msg= GRU module API failed assertEqual result h msg= GRU module API failed given dtype=st sampled_from torch qint torch float override_qengines test_cell_api dtype r Test execution serialization dynamic quantized lstm modules int fp Check module matches numerics op ensure module can instantiated all engines dtypes batch = input_size = hidden_size = bias = True x = torch rand batch input_size h = torch rand batch hidden_size cell_dict = LSTMCell torch ao nn quantized dynamic LSTMCell GRUCell torch ao nn quantized dynamic GRUCell RNNTanh torch ao nn quantized dynamic RNNCell RNNReLU torch ao nn quantized dynamic RNNCell state = LSTMCell h h GRUCell h RNNTanh h RNNReLU h qfn_dict = LSTMCell torch ops quantized quantized_lstm_cell_dynamic GRUCell torch ops quantized quantized_gru_cell_dynamic RNNTanh torch ops quantized quantized_rnn_tanh_cell_dynamic RNNReLU torch ops quantized quantized_rnn_relu_cell_dynamic rnn_type cell_dict keys dtype == torch float torch backends quantized engine qnnpack onednn fp dynamic quant supported qnnpack onednn kwargs = input_size input_size hidden_size hidden_size bias bias dtype dtype rnn_type == RNNReLU kwargs nonlinearity = relu rnn_type == RNNTanh kwargs nonlinearity = tanh cell_dq = cell_dict rnn_type kwargs result = qfn_dict rnn_type x state rnn_type cell_dq _packed_weight_ih cell_dq _packed_weight_hh cell_dq bias_ih cell_dq bias_hh result_module = cell_dq x state rnn_type assertEqual result result_module msg= RNNCell module API failed assertEqual result result_module msg= RNNCell module API failed weight_keys = weight_ih weight_hh bias_keys = bias_ih bias_hh check_eager_serialization cell_dq cell_dict rnn_type kwargs x check_weight_bias_api cell_dq weight_keys bias_keys TestReferenceQuantizedModule QuantizationTestCase _quant_dequant_weight weight weight_qparams qscheme = weight_qparams qscheme scale = weight_qparams scale zero_point = weight_qparams zero_point dtype = weight_qparams dtype qscheme == torch per_tensor_affine weight = torch quantize_per_tensor weight scale zero_point dtype per channel affine axis = weight_qparams axis weight = torch quantize_per_channel weight scale zero_point axis dtype weight = weight dequantize weight TODO add tests conv linear test_rnn_cell Checks rnn cell reference quantized modules has correct numerics This includes LSTMCell GRUCell RNNCell batch = input_size = hidden_size = bias = True x = torch rand batch input_size h = torch rand batch hidden_size cell_dict = LSTMCell torch nn LSTMCell GRUCell torch nn GRUCell RNNTanh torch nn RNNCell RNNReLU torch nn RNNCell state = LSTMCell h h GRUCell h RNNTanh h RNNReLU h qfn_dict = LSTMCell nnqr LSTMCell GRUCell nnqr GRUCell RNNTanh nnqr RNNCell RNNReLU nnqr RNNCell rnn_type cell_dict keys kwargs = input_size input_size hidden_size hidden_size bias bias rnn_type == RNNReLU kwargs nonlinearity = relu rnn_type == RNNTanh kwargs nonlinearity = tanh fp_cell = cell_dict rnn_type kwargs initialize ref rnn cell module weight_qparams = qscheme torch per_tensor_affine dtype torch quint scale zero_point weight_qparams_dict = weight_ih weight_qparams weight_hh weight_qparams is_decomposed False ref_kwargs = kwargs copy ref_kwargs weight_qparams_dict = weight_qparams_dict ref_cell = qfn_dict rnn_type ref_kwargs reassign weights fp rnn cell modulea ref_cell weight_ih = fp_cell weight_ih ref_cell weight_hh = fp_cell weight_hh ref_cell bias_ih = fp_cell bias_ih ref_cell bias_hh = fp_cell bias_hh ref_res = ref_cell x state rnn_type change weight fp_res we first want run quantie dequantize weight fp_cell weight_ih = torch nn Parameter _quant_dequant_weight fp_cell weight_ih weight_qparams_dict weight_ih fp_cell weight_hh = torch nn Parameter _quant_dequant_weight fp_cell weight_hh weight_qparams_dict weight_hh fp_res = fp_cell x state rnn_type assertEqual ref_res fp_res msg= RNNCell module API failed assertEqual ref_res fp_res msg= RNNCell module API failed test_rnn Checks rnn reference quantized modules has correct numerics This includes LSTM seq_len = batch = input_size = hidden_size = num_layers = bias = True bidirectional True False x = torch randn seq_len batch input_size h = torch randn num_layers bidirectional + batch hidden_size c = torch randn num_layers bidirectional + batch hidden_size fp _rnn = torch nn LSTM input_size=input_size hidden_size=hidden_size num_layers=num_layers bias=bias batch_first=False dropout= bidirectional=bidirectional initialize ref rnn module weight_qparams = qscheme torch per_tensor_affine dtype torch qint scale zero_point weight_qparams_dict = key weight_qparams key fp _rnn _flat_weights_names key startswith weight weight_qparams_dict is_decomposed = False ref_rnn = nnqr LSTM input_size=input_size hidden_size=hidden_size num_layers=num_layers bias=bias batch_first=False dropout= bidirectional=bidirectional weight_qparams_dict=weight_qparams_dict wn fp _rnn _flat_weights_names setattr ref_rnn wn copy deepcopy getattr fp _rnn wn ref_rnn _flat_weights = copy deepcopy fp _rnn _flat_weights quantize dequantize weights fp _rnn module flat_weights = wn fp _rnn _flat_weights_names wn startswith weight weight = _quant_dequant_weight getattr fp _rnn wn weight_qparams weight = getattr fp _rnn wn flat_weights append weight fp _rnn _flat_weights = flat_weights fp _res = fp _rnn x h c ref_res = ref_rnn x h c assertEqual fp _res ref_res test_sparse Embedding EmbeddingBag num_embeddings = embedding_dim = embedding input ex = torch LongTensor embedding bag input ebx = torch tensor dtype=torch long offsets = torch tensor dtype=torch long fp_to_ref = nn Embedding nnqr Embedding ex nn EmbeddingBag nnqr EmbeddingBag ebx offsets per_tensor_weight_qparams = qscheme torch per_tensor_affine dtype torch quint scale zero_point is_decomposed False per_channel_weight_qparams = qscheme torch per_channel_affine dtype torch quint scale torch randn zero_point torch randint axis is_decomposed False per_channel_weight_qparams_quint x = qscheme torch per_channel_affine_float_qparams dtype torch quint x scale torch randn zero_point torch randint axis is_decomposed False weight_qparams_options = per_tensor_weight_qparams per_channel_weight_qparams per_channel_weight_qparams_quint x fp_cls weight_qparams itertools product nn Embedding nn EmbeddingBag weight_qparams_options TODO torch quint x supported quantize_per_channel need add support weight_qparams dtype == torch quint x continue ref_cls args = fp_to_ref fp_cls fp _embedding = fp_cls num_embeddings embedding_dim ref_embedding = ref_cls num_embeddings embedding_dim weight_qparams=weight_qparams ref_embedding weight = fp _embedding weight quantize dequantize weight fp module fp _embedding weight = torch nn Parameter _quant_dequant_weight fp _embedding weight weight_qparams fp _res = fp _embedding args ref_res = ref_embedding args assertEqual fp _res ref_res test_linear_decomposed_weight_custom_qmin_qmax Verify reference Linear respects custom qmin qmax weight linear_fp = torch nn Linear qconfig = torch ao quantization default_symmetric_qnnpack_qconfig w_obs = qconfig weight assertTrue w_obs quant_min == - assertTrue w_obs quant_max == w_obs linear_fp weight weight_qparams = torch ao quantization utils get_qparam_dict w_obs weight_qparams is_decomposed = True linear_ref = nnqr Linear from_float linear_fp weight_qparams linear_ref_traced = torch fx symbolic_trace linear_ref verify qmin qmax arguments weight q dq correctly taken observer found = n linear_ref_traced graph nodes n op = call_function continue n target torch ops quantized_decomposed quantize_per_tensor torch ops quantized_decomposed dequantize_per_tensor _ _ _ qmin qmax _ = n args assertTrue qmin == - assertTrue qmax == found += assertTrue found == __name__ == __main__ raise_on_run_directly test test_quantization py