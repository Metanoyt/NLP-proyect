__future__ annotations collections contextlib dataclasses functools inspect itertools logging math operator os pprint textwrap traceback typing collections Counter defaultdict typing Any Callable Generic Optional TYPE_CHECKING TypeVar Union typing_extensions ParamSpec TypeAlias torch utils _ordered_set OrderedSet ir ComputedBuffer TYPE_CHECKING collections abc Iterator Sequence types ModuleType sympy torch torch _inductor async_compile noqa F required warm up AsyncCompile pools torch utils _pytree pytree torch _dynamo utils counters dynamo_timed torch _inductor codecache LambdaFuture PyCodeCache torch _inductor ir TritonTemplateCallerBase torch _inductor metrics get_metric_table is_metric_table_enabled torch fx experimental symbolic_shapes free_symbols torch utils _sympy symbol free_symbol_is_type symbol_is_type SymT torch utils _triton has_triton comms config config_comms dependencies ir metrics analyze_preserves_zero_mask can_codegen_without_upcasts codegen common BackendFeature get_scheduling_for_device Kernel comm_analysis estimate_nccl_collective_runtime estimate_nccl_collective_runtime_nccl_estimator dependencies Dep MemoryDep StarDep WeakDep exc GPUTooOldForTriton TritonMissing fx_utils count_flops_fx ir assign_origin_node get_device_type GraphPartitionSignature MultiOutput MultiOutputLayout NoneLayout loop_body LoopBody memory MemoryPlanningInfoForBuffer MemoryPlanningInfoForNode runtime hints ReductionHint runtime runtime_utils green_text red_text sizevars SimplifyIndexing utils _unstable_customized_partition_wrapper cache_on_self cmp device_need_guard get_current_backend get_device_tflops get_dtype_size get_gpu_dram_gbps GraphPartitionMap IndentedBuffer is_collective is_cudagraph_unsafe_op is_gpu is_multi_outputs_template is_output_of_multi_outputs_template is_wait maybe_log_cudagraph_partition sympy_product virtualized V log = logging getLogger __name__ fusion_log = torch _logging getArtifactLogger __name__ fusion loop_ordering_log = torch _logging getArtifactLogger __name__ loop_ordering compute_dependencies_log = torch _logging getArtifactLogger __name__ compute_dependencies PartitionType TypeAlias = list BaseSchedulerNode _T = TypeVar _T _P = ParamSpec _P MixOrderReduction This contains utility functions decide we should fuse reductions reducing across different dimensions same input tensor staticmethod is_split_reduction node BaseSchedulerNode - bool node is_reduction all subnode node _split_size None subnode node get_nodes isinstance subnode SchedulerNode subnode is_reduction isinstance subnode node ComputedBuffer classmethod get_numel_rnumel cls node BaseSchedulerNode - tuple sympy Expr sympy Expr cls is_split_reduction node xnumel = None rnumel = None subnode node get_nodes isinstance subnode SchedulerNode subnode is_reduction isinstance subnode node ComputedBuffer continue assert subnode node _original_ranges None curxnumel = V graph sizevars simplify sympy_product subnode node _original_ranges assert subnode node _original_reduction_ranges None currnumel = V graph sizevars simplify sympy_product subnode node _original_reduction_ranges xnumel None xnumel = curxnumel rnumel = currnumel assert V graph sizevars statically_known_equals xnumel curxnumel f xnumel v s curxnumel assert V graph sizevars statically_known_equals rnumel currnumel f rnumel v s currnumel assert xnumel None xnumel rnumel node group type ignore return-value classmethod has_mix_reduction_orders cls node BaseSchedulerNode node BaseSchedulerNode - bool g = cls get_numel_rnumel node g = cls get_numel_rnumel node len g = len g = g == g False tuple g == tuple reversed g classmethod _is_full_access cls buf str node BaseSchedulerNode - bool The access buf broadcast access found_dep = None dep node read_writes reads isinstance dep MemoryDep dep name == buf found_dep = dep break found_dep False index = found_dep index var_ranges = node read_writes var_ranges var_ranges assert isinstance node FusedSchedulerNode f type node var_ranges = node snodes read_writes var_ranges assert var_ranges OrderedSet var_ranges - OrderedSet index free_symbols True cases happen after merging loops MemoryDep arg _ c c var_ranges= d d V graph sizevars statically_known_equals sympy_product found_dep size sympy_product var_ranges values True False classmethod get_common_read cls node BaseSchedulerNode node BaseSchedulerNode - list str out = common_reads = node used_buffer_names node used_buffer_names buf common_reads cls _is_full_access buf node cls _is_full_access buf node out append buf out classmethod has_common_read cls node BaseSchedulerNode node BaseSchedulerNode - bool len cls get_common_read node node TODO add cache classmethod can_fuse cls node BaseSchedulerNode node BaseSchedulerNode - bool config triton mix_order_reduction False node is_gpu node is_gpu False device_type = node get_device type type ignore union-attr device_type cuda xpu get_current_backend device_type = triton False node is_reduction node is_reduction False check mix reduction orders cls has_mix_reduction_orders node node False check common buffer accesses common_reads = MixOrderReduction get_common_read node node len common_reads == False g = cls get_numel_rnumel node nrow = sympy Max g g ncol = sympy Min g g We require more more row than columns since we prefer doing persistent reduction each row we will split reduction across rows V graph sizevars statically_known_geq nrow ncol False When nrow small ncol should also small due check above Thus entire tensor should well cached L Mix order reduction less beneficial V graph sizevars statically_known_geq nrow False contiguous_node other_node = node node g == ncol node node all cls is_contiguous_load buf contiguous_node buf common_reads False Make sure persistent reduction will generated any subnode node data reduction_hint type ignore union-attr ReductionHint INNER ReductionHint DEFAULT subnode contiguous_node get_nodes subnode is_reduction False rnumel so large we will generated persistent reduction V graph sizevars statically_known_leq ncol False Other reduction types like max min supported yet There no real use case well out = all subnode node get_reduction_type type ignore union-attr sum prod subnode other_node get_nodes subnode is_reduction out classmethod are_mix_order_reductions cls node BaseSchedulerNode node BaseSchedulerNode - bool cls can_fuse node node classmethod is_contiguous_load cls buf str parent_node BaseSchedulerNode - bool torch _inductor loop_body MemoryUsageType n_congituous_read = node parent_node get_nodes assert isinstance node SchedulerNode loop_body = node _body entries = loop_body memory_usage MemoryUsageType LOAD index_names = e index_name e entries e buffer_name == buf len index_names == continue there can multiple index_names some times index_name index_names index_expr = loop_body indexing_exprs index_name var_ranges = loop_body var_ranges assumes final symbol reduction var_symbols = list var_ranges keys stride_vars = V graph sizevars stride_vars index_expr var_symbols var_symbols n_congituous_read += stride_vars - == n_congituous_read True False dataclasses dataclass SchedulerBuffer scheduler Scheduler node ir Buffer defining_op Optional BaseSchedulerNode users list NodeUser = dataclasses field default_factory=list mpi_buffer MemoryPlanningInfoForBuffer = dataclasses field default_factory=MemoryPlanningInfoForBuffer defining_op_name - str op = defining_op assert op None op get_name __hash__ - int hash node name debug_str - str result = IndentedBuffer name = get_name result writeline f name type node __name__ result writeline f name layout = node layout get_aliases result writeline f name aliases = pformat get_aliases get_mutations result writeline f name mutations = pformat get_mutations len users = result writeline f name users = users result writeline f name users = result indent user users result writeline f user result writeline result getrawvalue get_name - str node get_name allocate - None assert node None node should_allocate node get_inputs_that_alias_output node get_mutation_names isinstance node get_output_spec ir CommBufferLayout V graph wrapper_code codegen_allocation node hacky check V kernel real kernel NullHandler hasattr V kernel args get_name V kernel inplace_update_buffers input_buffer Union ir DonatedBuffer ir Buffer input_buffer_name = V kernel inplace_update_buffers get_name input_buffer_name scheduler name_to_donated_buffer input_buffer = scheduler name_to_donated_buffer input_buffer_name node input_buffer = scheduler name_to_buf input_buffer_name node V graph wrapper_code codegen_inplace_reuse input_buffer node V graph wrapper_code codegen_allocation node can_free - bool There s no real allocated buffer no need free assert node None isinstance node layout ir NoneLayout is_multi_outputs_template node False use users isinstance use node OutputNode False True set_users users list NodeUser - None deduplicate result dict int NodeUser = use users id use node result result id use node = use merge result id use node result id use node = use users = list result values get_aliases - Sequence str assert node None node get_inputs_that_alias_output get_mutations - Sequence str assert node None node get_mutation_names get_device - Optional torch device node get_output_spec get_device dataclasses dataclass SchedulerDonatedBuffer SchedulerBuffer defining_op Optional BaseSchedulerNode = None BaseSchedulerNode ancestors OrderedSet str debug_device_str Callable BaseSchedulerNode list str group tuple torch device tuple tuple sympy Expr last_usage OrderedSet str min_order max_order only relevant grouped nodes such FusedSchedulerNode e g FusedSchedulerNode includes nodes op_ op_ op_ op_X X-th node ` scheduler nodes ` then FusedSchedulerNode min_order max_order For non- grouped nodes i e regular SchedulerNode min_order = max_order = X node X-th node ` scheduler nodes ` min_order int max_order int mpi_node MemoryPlanningInfoForNode mutation_renames dict str str node Optional ir Operation outputs list SchedulerBuffer outputs_by_name dict str SchedulerBuffer override_estimated_runtime Optional float = None read_writes dependencies ReadWrites unmet_dependencies OrderedSet Dep __init__ scheduler Scheduler - None scheduler = scheduler debug_device_str = lambda args kwargs _init_from_node node ir Operation - None node = node ancestors = OrderedSet last_usage = OrderedSet buffers won t used after kernel written = False outputs = SchedulerBuffer scheduler=self scheduler node=output defining_op=self output node get_outputs outputs_by_name = buf get_name buf buf outputs mutation_renames current node Due potential more mutations happening later can different Scheduler mutation_renames Also dict should small since only mutation information relevant deps node stored here mutation_renames = __repr__ - str f type __name__ name= get_name r debug_str - str Longer form printout trace logs name = get_name buf = IndentedBuffer buf splice f \ name type __name__ type getattr node None __name__ name writes = pformat read_writes writes name unmet_dependencies = pformat unmet_dependencies name met_dependencies = pformat read_writes reads - unmet_dependencies name outputs = buf indent out get_outputs buf splice out debug_str buf writeline try buf splice debug_str_extra except Exception log warning Ignoring error debug_str exc_info=True buf getrawvalue rstrip debug_str_extra - str _debug_str_for_device - list str debug_device_str debug_str_short - str maybe_data = getattr node data None data_str = isinstance maybe_data torch _inductor ir Pointwise data_str = + maybe_data str_helper maybe_data get_size shorten=False multiline=False isinstance maybe_data torch _inductor ir Reduction data_str = + maybe_data str_helper maybe_data get_reduction_size maybe_data get_reduction_type shorten=False multiline=False f data_str log_details - None log info s unmet_dependencies = s writes = s unmet_dependencies read_writes writes reorder_loops_by_dep_pair self_dep MemoryDep other_dep MemoryDep - bool False update_mutated_names renames dict str str - None mutation_renames = name renames name name dep name dep read_writes reads_and_writes name renames set_read_writes read_writes rename mutation_renames add_fake_dep dep Dep - None set_read_writes read_writes with_read dep has_aliasing_or_mutation - bool any buf get_aliases buf get_mutations buf get_outputs set_read_writes rw dependencies ReadWrites - None read_writes = rw unmet_dependencies = read_writes reads prune_deps set_last_usage future_used_buffers OrderedSet str mutation_real_name dict str str - None used_buffers = used_or_aliased_buffer_names used_buffers = OrderedSet mutation_real_name get k k k used_buffers last_usage = used_buffers - future_used_buffers mark_run - None buf outputs buf allocate used_buffer_names - OrderedSet str OrderedSet dep name dep itertools chain read_writes reads read_writes writes used_or_aliased_buffer_names - OrderedSet str used_names OrderedSet str = OrderedSet deps = dep name dep itertools chain read_writes reads read_writes writes while len deps dep = deps pop used_names add dep V graph name_to_buffer get dep deps extend alias alias V graph name_to_buffer dep get_inputs_that_alias_output alias used_names used_names prune_deps - None unmet_dependencies = OrderedSet dep dep unmet_dependencies dep name scheduler available_buffer_names prune_weak_deps - None Prune weak dependencies operations have been removed should_prune dep Dep - bool isinstance dep WeakDep False op_name = scheduler name_to_buf dep name defining_op_name op_name V graph removed_operations to_remove = OrderedSet dep dep read_writes reads should_prune dep set_read_writes read_writes remove_reads to_remove prune_redundant_deps name_to_fused_node dict str BaseSchedulerNode - None _prune_redundant_deps name_to_fused_node scheduler name_to_buf get_name - str assert node None node get_operation_name get_first_name - str get_name cache_on_self get_operation_names - OrderedSet str OrderedSet node get_name node get_nodes cache_on_self get_buffer_names - OrderedSet str OrderedSet out get_name out outputs cache_on_self can_codegen_in_low_precision - bool all isinstance n SchedulerNode can_codegen_without_upcasts n disallow_fp _ops=True n get_nodes cache_on_self can_codegen_without_upcasts - bool all isinstance n SchedulerNode can_codegen_without_upcasts n n get_nodes get_nodes - Sequence BaseSchedulerNode get_outputs - Sequence SchedulerBuffer outputs get_output buf_name str - SchedulerBuffer outputs_by_name buf_name get_device - Optional torch device assert node None node get_device is_cpu - bool device = get_device device None device type == cpu is_gpu - bool device = get_device device None is_gpu device type is_reduction - bool False is_native_matmul - bool False is_split_scan - bool False is_template - bool False is_extern - bool False is_foreach - bool False can_inplace read_dep dependencies Dep - bool False has_side_effects - bool False decide_inplace_update - None Decide there should inplace updates node record decision active kernel codegen wrapper can_match_buffer_size isinstance SchedulerNode config inplace_buffers V graph has_feature get_device BackendFeature INPLACE_BUFFERS isinstance V kernel torch _inductor codegen simd SIMDKernel getattr V kernel mutations None None hacky check V kernel real kernel NullHandler hasattr V kernel args NOTE remove V graph removed_operations once deps issue fixed inconsequential_nodes = ancestors &#124; V graph removed_operations &#124; scheduler completed_operations single_index_in_fused_node buf_to_be_inplaced SchedulerBuffer - bool Inside NodeUser we track read write equivalent before deciding use can inplace But use fused into larger kernel we need check equivalence other accesses fused scheduler node well fused_node = buf_to_be_inplaced scheduler get_fused_node buf_name = buf_to_be_inplaced get_name Dedup read writes equivalent indices TODO - would nice we could just cache accesses ReadWrites enforce variant members functional deps OrderedSet Dep = OrderedSet user buf_to_be_inplaced users user_node = user node isinstance user_node BaseSchedulerNode continue user_node get_first_name buf_to_be_inplaced scheduler name_to_fused_node buf_to_be_inplaced scheduler get_fused_node user_node fused_node continue deps &#124; = o o user_node read_writes reads_and_writes o name == buf_name len deps False True buf get_outputs buf_node = buf node assert buf_node None buf_node should_allocate buf_node get_inputs_that_alias_output buf_node get_mutation_names buf get_name V graph removed_buffers continue read read_writes reads input_buf Optional Union SchedulerBuffer SchedulerDonatedBuffer read name scheduler name_to_donated_buffer input_buf = scheduler name_to_donated_buffer read name input_buf = scheduler name_to_buf get read name input_buf V graph wrapper_code can_reuse input_buf isinstance input_buf defining_op NopKernelSchedulerNode assert input_buf users None remaining_uses = x x input_buf users x node get_name inconsequential_nodes len remaining_uses == remaining_uses can_inplace remaining_uses node input_buf node None isinstance input_buf node get_output_spec ir NoneLayout ir MultiOutputLayout ir MutationLayoutSHOULDREMOVE input_buf defining_op isinstance input_buf defining_op node ir FallbackKernel ir MultiOutput len input_buf node get_inputs_that_alias_output can_match_buffer_size input_buf node buf node single_index_in_fused_node input_buf there isn t triton kernel then we don t need call triton-specific things TODO might convenient place signal Collective kernels inplace can we make kernel less generic name V kernel args make_inplace input_buf get_name buf get_name mutations tracked cpp kernels isinstance V kernel torch _inductor codegen simd SIMDKernel V kernel mutations add input_buf get_name V kernel mutations add buf get_name V kernel inplace_update_buffers buf get_name = input_buf get_name break codegen_originating_info buffer IndentedBuffer only_once bool = True - None config comment_origin only_once written assert node None origins = node get_origins out_lines = o origins o op == output These boring samey continue out_lines append TODO voz Should pragma constant somewhere out_lines append #pragma CMT ORIGIN op_info_str = f #pragma CMT o op o target seq_nr o meta op_info_str = op_info_str + f seq_nr o meta seq_nr out_lines append op_info_str stack_trace o meta stack_trace = f o meta stack_trace stack_trace_last_line = stack_trace rsplit &#124; maxsplit= - out_lines append #pragma CMT + stack_trace_last_line replace replace replace \n \\ replace \\ \\\\ For windows safe path avoid example \x \U out_lines append #pragma CMT END ORIGIN out_lines append len out_lines == TODO voz Ostensibly we should need But there cases where C++ codegen does use BracesBuffer so we have no good indicator C++ buffer atm buffer writelines out_lines written = True cache_on_self get_read_write_buffers_sizes - int get_read_write_buffers_sizes_impl include_reads=True include_writes=True cache_on_self get_read_buffer_sizes - int get_read_write_buffers_sizes_impl include_reads=True include_writes=False cache_on_self get_write_buffer_sizes - int get_read_write_buffers_sizes_impl include_reads=False include_writes=True get_read_write_buffers_sizes_impl include_reads bool include_writes bool - int sum get_read_write_buffer_accesses include_reads=include_reads include_writes=include_writes values start= get_read_write_buffer_accesses include_reads bool include_writes bool - dict str int Counting number bytes accessed kernel surprisingly tricky In particular there differentiation between theoretical memory accesses practical memory accesses For example layernorm kernel may actually access input times theory only needs access its input once may optimized do so through say persistent reductions Another example even though buffer passed we may access entire buffer This may occur we accessing slice buffer Another tricky case indirect indexing where amount bytes accessed depends values input What function aims compute memory accesses worst-case inputs best-case optimization What means each buffer we compute amount potential accesses two ways take minimum Numel ranges multiplied number deps buffer has The buffer size Returns memory accesses per buffer isinstance NopKernelSchedulerNode isinstance ExternKernelSchedulerNode isinstance node MultiOutput todo Calculate - s kinda annoying isinstance ExternKernelSchedulerNode isinstance node ir FallbackKernel node op_overload torch _prims rng_prims graphsafe_run_with_rng_state try_size_hint s sympy Expr - int V graph sizevars size_hint s fallback= isinstance SchedulerNode node_numel = try_size_hint sympy_product get_ranges sympy_product get_ranges node_numel = int e buf_accesses = collections defaultdict list include_reads dep read_writes reads buf_accesses dep name append dep include_writes dep read_writes writes buf_accesses dep name append dep reads = OrderedSet dep name dep read_writes reads include_reads OrderedSet writes = OrderedSet dep name dep read_writes writes include_writes OrderedSet is_materialized buf str snodes Sequence BaseSchedulerNode - bool users = scheduler name_to_buf buf users buf_uses = OrderedSet user node user users len buf_uses - OrderedSet snodes isinstance FusedSchedulerNode removed_buffers = OrderedSet dep dep writes is_materialized dep snodes writes = writes - removed_buffers reads = reads - removed_buffers buf_byte_accesses dict str int = buf_name reads &#124; writes buf_accessed_elems = sum node_numel dep buf_accesses buf_name buf Union ir Buffer ir TensorBox ir TorchBindObject buf_name V graph name_to_buffer buf = V graph name_to_buffer buf_name buf_name V graph graph_inputs buf = V graph graph_inputs buf_name continue get_buf_bytes buf Optional Union ir Buffer ir TensorBox ir TorchBindObject - int buf isinstance buf ir TorchBindObject buf get_buf_bytes isinstance buf layout MultiOutputLayout Kind lazy way get MultiOutput nodes corresponding MultiOutputLayout users = scheduler name_to_buf buf get_name users tot = user users assert isinstance user node BaseSchedulerNode isinstance user node node MultiOutput sched_buf user node get_outputs tot += get_buf_bytes sched_buf node Buf MultiOutputLayout all its users MultiOutputs TODO Figure out what s going tot isinstance buf layout ir NoneLayout sum get_buf_bytes V graph get_buffer mut_name mut_name buf get_mutation_names buf_elems = try_size_hint sympy_product buf get_size get_dtype_size buf get_dtype min buf_accessed_elems buf_elems buf_bytes = get_buf_bytes buf buf_name buf_byte_accesses buf_byte_accesses buf_name = buf_bytes buf_byte_accesses buf_name += buf_bytes buf_byte_accesses cache_on_self estimate_flops - int &#124; None node None None fx_node = node get_origin_node fx_node None None flops = count_flops_fx fx_node flops None None resolved_flops = V graph sizevars size_hint flops fallback= counters inductor flop_count += resolved_flops resolved_flops get_estimated_runtime - float override_estimated_runtime None override_estimated_runtime _get_estimated_runtime cache_on_self _get_estimated_runtime - float Returns estimated op runtime milliseconds ms buf = get_nodes get_outputs layout = buf node get_output_spec is_gpu get_device_type layout default no reordering based runtime Collective kernels is_collective node assert isinstance node ir IRNode try config_comms runtime_estimations_use_nccl_lib_estimations cache_key = get_estimate_runtime_cache_key_from_snode cache = get_estimate_runtime_cache cache_val = cache lookup cache_key cache_val None assert isinstance cache_val float cache_val ms = estimate_nccl_collective_runtime_nccl_estimator ms None NCCL estimations fail fallback in-tree algorithmic estimation ms = estimate_nccl_collective_runtime node cache set_value cache_key value=ms ms estimate_nccl_collective_runtime node except ValueError e We don t know how estimate runtime collective falling back log info e noqa G except TypeError e happens when collective type ir _CollectiveKernel log info e noqa G is_wait node ir Wait only used collective ops The time needed collective op already estimated considered when we processing collective op IR node so ir Wait takes time since doesn t take extra time get result after collective completed ret = maybe_estimate_runtime_benchmark ret None ret dtype = buf node maybe_get_dtype try gpu_memory_bandwidth = get_gpu_dram_gbps gpu_flops = get_device_tflops dtype If cudaGetDeviceProperties returns gpu_memory_bandwidth gpu_flops there chance continue execution successfully Otherwise would fail ZeroDivisionError below gpu_memory_bandwidth = raise AssertionError f gpu_memory_bandwidth cannot = got gpu_memory_bandwidth gpu_flops = raise AssertionError f gpu_flops cannot = got gpu_flops except Exception flops_est = estimate_flops flops_est == flops_est None no flops estimate so fall back memory estimate ns = get_read_write_buffers_sizes gpu_memory_bandwidth ms = ns e ms TODO xmfan find better heuristic model FLOPS latency relationship factor = counted_bytes = get_read_write_buffers_sizes counted_bytes = counted_bytes None counted_bytes compute_time = factor flops_est gpu_flops e transfer_time = counted_bytes gpu_memory_bandwidth Return estimated runtime milliseconds ns = max compute_time transfer_time ms = ns e ms get_template_node - Optional ir TemplateBuffer None get_template_node_or_throw - ir TemplateBuffer template = get_template_node assert template None template staticmethod get_prologue_template_epilogue nodes list BaseSchedulerNode - tuple list BaseSchedulerNode BaseSchedulerNode list BaseSchedulerNode For list nodes get prologue template epilogue template_index = next i i n enumerate nodes n is_template prologue = nodes template_index template_node = nodes template_index epilogue = nodes template_index + prologue template_node epilogue functools cache get_estimate_runtime_cache - torch _inductor codecache LocalCache torch _inductor codecache LocalCache get_estimate_runtime_cache_key_from_snode snode BaseSchedulerNode - str python_kernel_name = getattr snode node python_kernel_name args = snode node inputs type ignore union-attr args = snode node fill_non_provided_args type ignore union-attr args snode node constant_args type ignore union-attr snode node kwargs type ignore union-attr kwargs = snode node kwargs type ignore union-attr flat_args flat_args_pytree_spec = pytree tree_flatten args kwargs _is_tensor_ir x - bool type ignore no-untyped-def isinstance x ir IRNode isinstance x ir GeneratorState cache_key = str python_kernel_name + tuple tuple get_size _is_tensor_ir None flat_args cache_key _get_mm_like_fn snode BaseSchedulerNode - Optional Callable Any Any isinstance snode ExternKernelSchedulerNode None mms_fns = extern_kernels mm torch ops aten mm extern_kernels bmm torch ops aten bmm extern_kernels addmm torch ops aten addmm python_kernel_name = getattr snode node python_kernel_name python_kernel_name mms_fns None isinstance snode node ir ExternKernel None mms_fns python_kernel_name maybe_estimate_runtime_benchmark snode BaseSchedulerNode - Optional float bench_fn = None args_kwargs_fn = None config runtime_estimations_mms_benchmark mm_fn = _get_mm_like_fn snode mm_fn None None bench_fn = mm_fn pyrefly ignore unbound-name args_kwargs_fn = lambda snode_args_kwargs snode noqa E None cache_key = get_estimate_runtime_cache_key_from_snode snode cache = get_estimate_runtime_cache cache_val = cache lookup cache_key cache_val None assert isinstance cache_val float cache_val utils snode_args_kwargs args kwargs = args_kwargs_fn torch _inductor runtime benchmarking benchmarker ms = benchmarker benchmark bench_fn args kwargs type ignore arg-type cache set_value cache_key value=ms ms dataclasses dataclass slots=True WhyNoFuse name str name str reason str args tuple Any __init__ node BaseSchedulerNode node BaseSchedulerNode - None name = node get_name name = node get_name __call__ reason str args Any - None reason = reason args = args fusion_log debug __str__ - str f cannot fuse name name + reason args pformat obj Any - str isinstance obj OrderedSet set noqa set_linter pformat has trouble sets sympy exprs obj = sorted obj key=str result = pprint pformat obj indent= \n result f \n textwrap indent result result OutputNode __init__ dep StarDep - None unmet_dependencies = OrderedSet dep is_reduction - bool False get_inputs_that_alias_output - Sequence str get_name - str OUTPUT __repr__ = get_name _prune_redundant_deps node BaseSchedulerNode name_to_fused_node dict str BaseSchedulerNode name_to_buf dict str SchedulerBuffer - None Prunes weakdeps intended mutation ordering upstream fused node after fusion there another dependency fused upstream node making weakdep redundant In essence enforces ordering fusions As fusions occur weakdeps will incrementally removed enabling other fusions ensuring they fused order name_to_dep_count Counter str = collections Counter dep node unmet_dependencies isinstance dep WeakDep op_name = name_to_buf dep name defining_op_name name_to_dep_count name_to_fused_node op_name get_name += should_prune dep Dep - bool isinstance dep WeakDep op_name = name_to_buf dep name defining_op_name is_redundant = name_to_dep_count name_to_fused_node op_name get_name node scheduler fusable_weak_dep dep name_to_fused_node op_name node These can occur because fused nodes always gather deps their snodes If B has weakdep A B gets fused C then any time BC fused weakdep will reappear is_self_dep = name_to_fused_node op_name == node is_redundant is_self_dep False deps_to_prune = OrderedSet dep dep node unmet_dependencies should_prune dep deps_to_prune node unmet_dependencies = node unmet_dependencies - deps_to_prune node set_read_writes node read_writes remove_reads deps_to_prune ExternKernelSchedulerNode BaseSchedulerNode __init__ scheduler Scheduler node ir Operation - None super __init__ scheduler _init_from_node node set_read_writes node get_read_writes debug_str_extra - str f get_name node kernel = getattr node python_kernel_name None is_extern - bool True has_side_effects - bool assert node None hasattr node has_side_effects node has_side_effects NopKernelSchedulerNode BaseSchedulerNode __init__ scheduler Scheduler node ir Operation - None super __init__ scheduler _init_from_node node set_read_writes node get_read_writes SchedulerNode BaseSchedulerNode A SchedulerNode node scheduling encapsulates either ComputedBuffer TemplateBuffer _sizes tuple Sequence sympy Expr _body LoopBody __init__ scheduler Scheduler node Union ir ComputedBuffer ir TemplateBuffer - None super __init__ scheduler _init_from_node node _compute_attrs _compute_attrs extra_indexing_constraints Optional tuple dict Any Any list Any = None recompute_sizes_body_func Optional Callable _P _T = None - None assert isinstance node ir ComputedBuffer ir TemplateBuffer _sizes body = node simplify_and_reorder extra_indexing_constraints=extra_indexing_constraints recompute_sizes_body_func=recompute_sizes_body_func _body = body type ignore assignment device = node get_device_or_error group_fn = scheduler get_backend device group_fn group = device group_fn _sizes Don t normalize since normalization will merge loops which makes hard decide new loop orders should_normalize = config loop_ordering_after_fusion is_gpu device type isinstance node ir TemplateBuffer set_read_writes node extract_read_writes normalize=should_normalize set_read_writes dependencies extract_read_writes _body _sizes normalize=should_normalize recompute_size_and_body extra_indexing_constraints Optional tuple dict Any Any list Any = None recompute_sizes_body_func Optional Callable Any = None - None _compute_attrs extra_indexing_constraints=extra_indexing_constraints recompute_sizes_body_func=recompute_sizes_body_func refresh_dependencies normalize bool need_clear_tiling_cache bool - None Fake dependencies added manually They can analyzed extract_read_writes Find them out apply manually fake_deps OrderedSet Dep = OrderedSet dep dep read_writes reads isinstance dep WeakDep StarDep don t normalize since loop order may need further changed later set_read_writes dependencies extract_read_writes _body _sizes normalize=normalize with_read fake_deps rename mutation_renames pointwise_read_writes clear_cache need_clear_tiling_cache codegen simd SIMDScheduling TODO shunting cause compilation time increase when enabling LOAF default try just clearing specific cache entry using customized cache implementation rather than lru_cache SIMDScheduling candidate_tilings cache_clear apply_new_loop_order new_order Sequence int - None _body = _body reorder_iter_loops new_order _sizes = _body sizes refresh_dependencies normalize=False need_clear_tiling_cache=True swap_pw_red_dimension - None num_rdims = _body get_original_num_rdims num_pwdims = len _body iter_vars - num_rdims pwdims = tuple range num_pwdims rdims = tuple range num_pwdims num_pwdims + num_rdims apply_new_loop_order rdims + pwdims assert len group == group = group group group extract_pw_from_reduction - BaseSchedulerNode _body = _body extract_pw_from_reduction cancel_reduction_split - None MixOrderReduction is_split_reduction assert isinstance node ir ComputedBuffer node with_original_inner_fn _compute_attrs expand_dimension_for_pointwise_node dimension int new_range int - None assert isinstance node ir ComputedBuffer ir TemplateBuffer _body = _body expand_dimension_for_pointwise_node dimension new_range _sizes = _body sizes device = node get_device_or_error group_fn = scheduler get_backend device group_fn group = device group_fn _sizes Need normalize prefix name facilitate finding common dependencies refresh_dependencies normalize=True need_clear_tiling_cache=True merge_loops - None _body = _body merge_loops _sizes = _body sizes merge_loops called after loop reordering We still need retain fake dependencies since codegen estimated amount memory access rely them Merge loops does affect tiling decision So we don t need clear tiling cache refresh_dependencies normalize=True need_clear_tiling_cache=False reorder_loops_by_dep_pair self_dep MemoryDep other_dep MemoryDep - bool new_order = None self_sizes = _sizes len self_sizes == self_dep num_vars == other_dep num_vars new_order = self_dep decide_loop_order_to_match other_dep new_order pyrefly ignore bad-assignment metrics num_loop_reordering += loop_ordering_log debug Reorder loops s order s get_name new_order apply_new_loop_order new_order True loop_ordering_log debug Don t reordering s because we can decide suitable loop order get_name False debug_str_extra - str name = get_name lines = f name group device = group f name group iteration = group f name sizes = _sizes dep read_writes reads_and_writes isinstance dep WeakDep buf_name = dep name buf = V graph get_buffer buf_name isinstance buf ir TorchBindObject lines append f buf_name _layout = pformat buf layout isinstance _body LoopBody lines append f name _loop_body lines append textwrap indent _body debug_str assert node None lines extend _debug_str_for_device \n join lines get_ranges - Sequence Sequence sympy Expr _sizes is_reduction - bool assert isinstance node ir ComputedBuffer ir TemplateBuffer f type node = _body containing partial accumulate means reduction converted pointwise node Need extra check since we change _body didn t change node IRNode when converting reduction pointwise bool node get_reduction_type _body None _body has_partial_accumulate is_native_matmul - bool assert isinstance node ir ComputedBuffer f type node = node get_reduction_type == dot is_split_scan - bool assert isinstance node ir ComputedBuffer ir TemplateBuffer f type node = isinstance node ir ComputedBuffer isinstance node data ir SplitScan is_template - bool isinstance node ir TemplateBuffer get_template_node - Optional ir TemplateBuffer node isinstance node ir TemplateBuffer None run index_vars Sequence sympy Expr - None decide_inplace_update mark_run codegen index_vars ranges_from_index_vars index_vars Sequence Sequence sympy Expr - dict sympy Expr sympy Expr sizes = _sizes assert sum map len sizes == sum map len index_vars var_ranges = dict zip itertools chain from_iterable index_vars itertools chain from_iterable sizes var_ranges codegen index_vars Sequence Sequence sympy Expr - None Generate code node using provided index variables This method sets up appropriate context code generation including simplifying indexing expressions based variable ranges then calls node s body function index variables Args index_vars A sequence sequences sympy expressions representing index variables each dimension computation var_ranges = ranges_from_index_vars index_vars try V set_ops_handler SimplifyIndexing V get_ops_handler var_ranges V kernel set_current_node _body index_vars except Exception log fatal Error codegen s node raise pointwise_or_reduction_read_writes pointwise bool = True - dependencies ReadWrites Get memory dependencies either pointwise reduction axes keep_sizes ignore_sizes = _sizes pointwise reversed _sizes dependencies extract_read_writes _body keep_sizes hidden_args= sympy S Zero len ignore_sizes cache_on_self pointwise_read_writes - dependencies ReadWrites Get memory dependencies non-reduction axes pointwise_or_reduction_read_writes pointwise=True cache_on_self reduction_read_writes - dependencies ReadWrites Get memory dependencies reduction axes pointwise_or_reduction_read_writes pointwise=False can_inplace read_dep dependencies Dep - bool is_template False any out get_aliases out get_outputs False len read_writes writes == isinstance read_dep dependencies MemoryDep write_dep = next iter read_writes writes assert isinstance write_dep dependencies MemoryDep f type write_dep = read_dep index == write_dep index read_dep size == write_dep size False cache_on_self _get_atomic_add_buffers - OrderedSet str buffers_store_as_atomic_add OrderedSet str = OrderedSet isinstance _body LoopBody node _body get_nodes node op == call_method node target == store mode node kwargs node kwargs mode == atomic_add len node args == node args == atomic_add buffers_store_as_atomic_add add node kwargs name name node kwargs node args len node args = buffers_store_as_atomic_add cache_on_self has_side_effects - bool _body None sometimes s why check added _body None _body has_op device_assert_async True super has_side_effects refresh_group_node_dependencies group_snode Union FusedSchedulerNode GroupedSchedulerNode - None snodes = group_snode snodes group_snode set_read_writes dependencies ReadWrites merge_list x read_writes x snodes group_snode unmet_dependencies = OrderedSet dep dep OrderedSet union x unmet_dependencies x snodes dep name group_snode get_buffer_names - group_snode read_writes writes init_group_node group_snode Union FusedSchedulerNode GroupedSchedulerNode scheduler Scheduler snodes list BaseSchedulerNode - None assert isinstance group_snode FusedSchedulerNode GroupedSchedulerNode group_snode snodes = snodes group_snode scheduler = scheduler group_snode node = None group_snode ancestors = OrderedSet union x ancestors x snodes x ancestors None refresh_group_node_dependencies group_snode group_snode min_order = min x min_order x group_snode snodes group_snode max_order = max x max_order x group_snode snodes group_snode outputs_by_name = buf get_name buf buf group_snode get_outputs FusedSchedulerNode BaseSchedulerNode This fake scheduler node represents group scheduler nodes meant fused together The way does maintaining its unmet dependencies union its constituent nodes snodes list BaseSchedulerNode classmethod fuse cls node BaseSchedulerNode node BaseSchedulerNode - FusedSchedulerNode assert node scheduler node scheduler assert isinstance node SchedulerNode FusedSchedulerNode node is_template isinstance node ExternKernelSchedulerNode Fuse multi outputs template its outputs Node has memorydep MultiOutput reads Node has StarDep MultiOutput writes Rewrite Node StarDep MemoryDep because calculate score_fusion_memory template node its epilogue requires same type dependencies assert isinstance node node MultiOutput assert len node read_writes writes == assert isinstance next iter node read_writes writes StarDep name = next iter node read_writes writes name template_nodes = node node node get_nodes node is_template assert len template_nodes == template_node = template_nodes assert len template_node read_writes writes == write = next iter template_node read_writes writes assert isinstance write MemoryDep node read_writes writes = OrderedSet MemoryDep name write index write var_names write size write mode assert isinstance node SchedulerNode FusedSchedulerNode nodes = list itertools chain node get_nodes node get_nodes cls node scheduler nodes extract_pw_from_reduction - BaseSchedulerNode subnode snodes assert isinstance subnode SchedulerNode assert subnode is_reduction subnode extract_pw_from_reduction swap_pw_red_dimension - None subnode snodes assert isinstance subnode SchedulerNode subnode swap_pw_red_dimension cache_on_self estimate_flops - int &#124; None don t increment counters fused methods so we don t double count fps = list filter None node estimate_flops node get_nodes node is_template node is_extern len fps == None ret = sum fps ret reorder_loops_by_dep_pair self_dep MemoryDep other_dep MemoryDep - bool Return true loop reordering performed is_template We can really reorder loops triton template False self_sizes = None snode snodes assert isinstance snode SchedulerNode self_sizes None tuple self_sizes = tuple snode _sizes loop_ordering_log debug Can reorder fused node due different sizes False self_sizes = snode _sizes new_order = None assert self_sizes None len self_sizes == self_dep num_vars == other_dep num_vars new_order = self_dep decide_loop_order_to_match other_dep new_order loop_ordering_log debug Dont reordering fused node s because we can decide suitable loop order get_name False pyrefly ignore bad-assignment metrics num_loop_reordering += loop_ordering_log debug Reorder loops fused node s order s get_name new_order snode snodes assert isinstance snode SchedulerNode snode apply_new_loop_order new_order refresh_group_node_dependencies True __init__ scheduler Scheduler snodes list BaseSchedulerNode - None super __init__ scheduler init_group_node scheduler snodes users list NodeUser = group = max snodes key=lambda x int x is_reduction group cache_on_self get_name - str _ join x get_name x snodes get_first_name - str snodes get_name cache_on_self get_buffer_names - OrderedSet str OrderedSet union x get_buffer_names x snodes get_outputs - list SchedulerBuffer result list SchedulerBuffer = node snodes result extend node get_outputs result debug_str_extra - str lines = f get_name snodes i =\n node debug_str i node enumerate snodes node = snodes node node None lines extend _debug_str_for_device textwrap indent \n join lines rstrip debug_str_short - str snodes_str = node debug_str_short node snodes f snodes snodes_str set_last_usage future_used_buffers OrderedSet str mutation_real_name dict str str - None Set last_usage using global information This will used inter-kernel optimisations super set_last_usage future_used_buffers mutation_real_name Set last_usage snodes This will used optimisations within kernel future_used_buffers OrderedSet str = OrderedSet node reversed snodes node set_last_usage future_used_buffers mutation_real_name future_used_buffers update node last_usage cache_on_self used_buffer_names - OrderedSet str OrderedSet union x used_buffer_names x snodes cache_on_self used_or_aliased_buffer_names - OrderedSet str OrderedSet union x used_or_aliased_buffer_names x snodes get_nodes - Sequence BaseSchedulerNode snodes __repr__ - str f type __name__ nodes= get_name cache_on_self is_reduction - bool any x is_reduction x snodes cache_on_self is_native_matmul - bool any x is_native_matmul x snodes cache_on_self is_split_scan - bool any x is_split_scan x snodes cache_on_self is_template - bool any x is_template x snodes cache_on_self get_template_node - Optional ir TemplateBuffer node snodes node is_template node get_template_node None get_device - torch device group cache_on_self has_aliasing_or_mutation - bool any x has_aliasing_or_mutation x snodes None these need implemented FusedSchedulerNode just abstraction scheduling purposes update_mutated_names renames dict str str - None raise NotImplementedError add_fake_dep name Dep - None raise NotImplementedError can_inplace read_dep dependencies Dep - bool raise NotImplementedError debug_str - str Longer form printout trace logs name = get_name node_typestr = join type n __name__ n snodes buf = IndentedBuffer buf splice f \ name type __name__ node_typestr name writes = pformat read_writes writes name unmet_dependencies = pformat unmet_dependencies name met_dependencies = pformat read_writes reads - unmet_dependencies name outputs = buf indent out get_outputs buf splice out debug_str buf writeline try buf splice debug_str_extra except Exception log warning Ignoring error debug_str exc_info=True buf getrawvalue rstrip cache_on_self has_side_effects - bool snodes None any node has_side_effects node snodes super has_side_effects FusedMixOrderReductions FusedSchedulerNode __init__ node BaseSchedulerNode node BaseSchedulerNode - None node = node node = node super __init__ node scheduler list node get_nodes + list node get_nodes ForeachKernelSchedulerNode FusedSchedulerNode This schedular node consists set scheduler nodes has no data dependencies among them can executed parallel get_consumer_subnode_for producer BaseSchedulerNode - Optional BaseSchedulerNode buf producer get_outputs buf get_name read_to_node read_to_node buf get_name None get_producer_subnode_for consumer BaseSchedulerNode - Optional BaseSchedulerNode producers = OrderedSet BaseSchedulerNode rd consumer read_writes reads rd name scheduler name_to_buf continue node_name = scheduler name_to_buf rd name defining_op_name node_name name_to_node producers add name_to_node node_name Don t permit fusion there multiple subnodes consumer reads len producers == next iter producers None classmethod can_fuse cls producer BaseSchedulerNode consumer BaseSchedulerNode - bool why = WhyNoFuse producer consumer producer is_foreach consumer is_foreach producer = typing cast ForeachKernelSchedulerNode producer consumer = typing cast ForeachKernelSchedulerNode consumer foreach_match = len producer snodes == len consumer snodes foreach_match why foreach do have same length foreach_match all producer scheduler can_fuse l r l r zip producer snodes consumer snodes consumer is_foreach producer is_reduction why candidate producer reduction foreach ops cannot fused reductions currently False consumer = typing cast ForeachKernelSchedulerNode consumer consumer_subnode = consumer get_consumer_subnode_for producer consumer_subnode None consumer scheduler can_fuse producer consumer_subnode why candidate producer dep any foreach consumer False producer is_foreach consumer is_reduction why candidate consumer reduction foreach ops cannot fused reductions currently False producer = typing cast ForeachKernelSchedulerNode producer producer_subnode = producer get_producer_subnode_for consumer producer_subnode None producer scheduler can_fuse producer_subnode consumer why candidate consumer has no dep any foreach producer False raise AssertionError At least one node passed ForeachKernelSchedulerNode can_fuse should foreach node classmethod fuse cls producer BaseSchedulerNode consumer BaseSchedulerNode - ForeachKernelSchedulerNode assert producer is_foreach consumer is_foreach producer is_foreach producer = typing cast ForeachKernelSchedulerNode producer use_custom_partition_algo = producer use_custom_partition_algo enable_autotune = producer enable_autotune consumer = typing cast ForeachKernelSchedulerNode consumer use_custom_partition_algo = consumer use_custom_partition_algo enable_autotune = consumer enable_autotune prev_node_ = None prev_node_ = None fused_nodes list BaseSchedulerNode producer is_foreach consumer is_foreach producer = typing cast ForeachKernelSchedulerNode producer consumer = typing cast ForeachKernelSchedulerNode consumer fused_nodes = FusedSchedulerNode fuse l r l r zip producer snodes consumer snodes producer is_foreach producer = typing cast ForeachKernelSchedulerNode producer producer_subnode = producer get_producer_subnode_for consumer fused_nodes = prev_node_ = producer prev_node_ = None node producer snodes node producer_subnode new_node = FusedSchedulerNode fuse node consumer prev_node_ = new_node fused_nodes append new_node fused_nodes append node consumer is_foreach consumer = typing cast ForeachKernelSchedulerNode consumer consumer_subnode = consumer get_consumer_subnode_for producer fused_nodes = prev_node_ = consumer prev_node_ = None node consumer snodes node consumer_subnode new_node = FusedSchedulerNode fuse producer node prev_node_ = new_node fused_nodes append new_node fused_nodes append node raise AssertionError At least one node passed ForeachKernelSchedulerNode fuse should foreach node cls producer scheduler fused_nodes use_custom_partition_algo=use_custom_partition_algo prev_node_ =prev_node_ prev_node_ =prev_node_ enable_autotune=enable_autotune __init__ scheduler Scheduler snodes list BaseSchedulerNode use_custom_partition_algo bool prev_node_ Optional BaseSchedulerNode = None prev_node_ Optional BaseSchedulerNode = None enable_autotune bool = False - None read_to_node = name_to_node = prev_node_ None prev_node_ None super __init__ scheduler snodes node snodes read node read_writes reads read_to_node read name = node name node get_operation_names name_to_node name = node scheduler = scheduler snodes = snodes node = None users list NodeUser = set_read_writes dependencies ReadWrites merge_list prev_node_ read_writes prev_node_ read_writes unmet_dependencies = OrderedSet dep dep OrderedSet union prev_node_ unmet_dependencies prev_node_ unmet_dependencies dep name get_buffer_names - read_writes writes min_order = min prev_node_ min_order prev_node_ min_order max_order = max prev_node_ max_order prev_node_ max_order prev_node_ is_foreach assert isinstance prev_node_ ForeachKernelSchedulerNode foreach_node other_node = prev_node_ prev_node_ assert isinstance prev_node_ ForeachKernelSchedulerNode foreach_node other_node = prev_node_ prev_node_ ancestors = foreach_node ancestors ancestors update other_node ancestors name_to_node = foreach_node name_to_node name other_node get_operation_names name_to_node name = other_node outputs_by_name dict str SchedulerBuffer = k v snode snodes k v snode outputs_by_name items use_custom_partition_algo = use_custom_partition_algo device = snodes get_device assert device group = device sympy Expr combo_kernel origins = OrderedSet torch fx Node enable_autotune = enable_autotune classmethod combinable_nodes cls nodes list BaseSchedulerNode - list BaseSchedulerNode extern = x x nodes isinstance x ExternKernelSchedulerNode extern log debug ComboKernels d external nodes filtered s len extern node node get_origins node extern node node None filtered_nodes = x x nodes isinstance x NopKernelSchedulerNode ExternKernelSchedulerNode foreach_nodes = x x filtered_nodes isinstance x ForeachKernelSchedulerNode foreach_nodes log debug ComboKernels d foreach nodes filtered len foreach_nodes filtered_nodes = x x filtered_nodes isinstance x ForeachKernelSchedulerNode template_nodes = x x filtered_nodes x is_template template_nodes log debug ComboKernels d template nodes filtered s len template_nodes template_nodes filtered_nodes = x x filtered_nodes x template_nodes filtered_nodes staticmethod _default_group_nodes_for_combo_kernels scheduler Scheduler - list list BaseSchedulerNode Returns list lists nodes grouped together sorted_nodes = scheduler _topological_sort_nodes grouped_nodes = max_num_nodes = nodes sorted_nodes grouped_nodes extend nodes i i + max_num_nodes i range len nodes max_num_nodes grouped_nodes group_algorithm_for_combo_kernels Callable Scheduler list list BaseSchedulerNode = _default_group_nodes_for_combo_kernels staticmethod set_group_algorithm_for_combo_kernels custom_group_algorithm Callable Scheduler list list BaseSchedulerNode - None ForeachKernelSchedulerNode group_algorithm_for_combo_kernels = custom_group_algorithm staticmethod group_nodes_for_combo_kernels scheduler Scheduler - list list BaseSchedulerNode ForeachKernelSchedulerNode group_algorithm_for_combo_kernels scheduler mark_run - None raise NotImplementedError codegen - None raise NotImplementedError is_foreach - bool True get_subkernel_nodes - list BaseSchedulerNode Returns list nodes which comprise combo kernel These nodes may vertically fused list snodes get_nodes - Sequence BaseSchedulerNode Returns all nodes contained kernel unpacking fused nodes into their constituent scheduler nodes list itertools chain from_iterable x get_nodes x snodes get_first_name - str snodes get_first_name prune_redundant_deps name_to_fused_node dict str BaseSchedulerNode - None _prune_redundant_deps name_to_fused_node scheduler name_to_buf node snodes node prune_redundant_deps name_to_fused_node GroupedSchedulerNode BaseSchedulerNode This fake scheduler node represents group scheduler nodes meant grouped together does allow another node scheduled between its constituent nodes nor does allow another node fuse into any its constituent nodes The way does maintaining its unmet dependencies union its constituent nodes Fusion will still happen among nodes within each GroupedSchedulerNode At codegen time scheduler node will unpacked codegen called each constituent node snodes list BaseSchedulerNode classmethod create cls snodes list BaseSchedulerNode - GroupedSchedulerNode scheduler = snodes scheduler assert all node scheduler scheduler node snodes grouped_snode = cls scheduler snodes snode snodes scheduler name_to_fused_node snode get_name = grouped_snode scheduler name_to_fused_node grouped_snode get_name = grouped_snode grouped_snode __init__ scheduler Scheduler snodes list BaseSchedulerNode temp_grouping bool = False - None super __init__ scheduler init_group_node scheduler snodes This flag introduced temporary grouping during some passes Where nodes grouped moved together After pass those nodes flattened Reusing calculation grouped unmed_dependencies etc No fusion logic case temp_grouping = temp_grouping unpack - list BaseSchedulerNode Do fusion among nodes within GroupedSchedulerNode then unpack GroupedSchedulerNode into regular nodes temp_grouping snodes snode snodes scheduler name_to_fused_node snode get_name = snode del scheduler name_to_fused_node get_name scheduler fuse_nodes snodes add_fake_dep fake_dep Dep - None set_read_writes read_writes with_read fake_dep unmet_dependencies add fake_dep cache_on_self get_name - str _ join x get_name x snodes get_first_name - str snodes get_name cache_on_self get_buffer_names - OrderedSet str OrderedSet union x get_buffer_names x snodes get_outputs - list SchedulerBuffer result list SchedulerBuffer = node snodes result extend node get_outputs result cache_on_self estimate_flops - int &#124; None don t increment counters fused methods so we don t double count fps = list filter None node estimate_flops node get_nodes node is_template node is_extern len fps == None ret = sum fps ret get_nodes - Sequence BaseSchedulerNode snodes classmethod can_fuse cls producer BaseSchedulerNode consumer BaseSchedulerNode - bool GroupedSchedulerNode cannot fused another node False pick_loop_order stride_lengths list list int sizes Sequence sympy Expr priority_idx Sequence int = - list int A heuristic decide loop iteration orders This has been well tuned may something we should autotune functools cmp_to_key index_cmp int b int - int sizes == sizes b == -sizes don t matter just move them end cmp sizes == sizes b == Take abs otherwise flipped dimensions treated smaller strides than contiguous dims stride_len_a = abs sl sl stride_lengths stride_len_b = abs sl b sl stride_lengths equivalent np logical_or stride_lengths b == stride_lengths stride_lengths b all a_first = sum sl_b == sl_a sl_b sl_a sl_b zip stride_len_a stride_len_b b_first = sum sl_a == sl_b sl_a sl_a sl_b zip stride_len_a stride_len_b a_first b_first - b_first a_first otherwise contiguous cmp b order = list reversed range len stride_lengths len priority_idx we have priority node only use node s order stride_lengths = stride_lengths pi pi priority_idx config pick_loop_orders order sort key=index_cmp order _replace_operation_buffer orig_node ir MultiTemplateBuffer new_node ir OperationBuffer - None replaced_buf_name = new_node get_name orig_buf_name = orig_node get_name assert isinstance orig_buf_name str isinstance replaced_buf_name str replaced_op_name = new_node get_operation_name orig_op_name = orig_node get_operation_name assert isinstance orig_op_name str isinstance replaced_op_name str del V graph name_to_buffer replaced_buf_name new_node name = orig_buf_name del V graph name_to_op replaced_op_name new_node operation_name = orig_op_name orig = V graph buffers index orig_node V graph buffers remove new_node V graph buffers orig = new_node V graph name_to_buffer orig_buf_name = new_node orig = V graph operations index orig_node V graph operations remove new_node V graph operations orig = new_node V graph name_to_op orig_op_name = new_node dataclasses dataclass NodeUser node Union BaseSchedulerNode OutputNode can_inplace bool = False A weak user must scheduled after given node doesn t actually use result is_weak bool = False __hash__ - int hash node get_name can_inplace is_weak __eq__ other object - bool isinstance other NodeUser get_name == other get_name can_inplace == other can_inplace is_weak == other is_weak get_name - str node get_name merge other NodeUser - NodeUser assert node other node NodeUser node can_inplace other can_inplace is_weak other is_weak _post_grad_graph_counter = itertools count used_non_deterministic_runtime_estimations - bool config runtime_estimations_mms_benchmark Scheduler A Scheduler graph BaseSchedulerNodes It responsible optimizations such fusion reorder graph partition __init__ nodes list ir Operation - None dynamo_timed Scheduler __init__ _init nodes _init nodes list ir Operation - None super __init__ V graph scheduler = backends dict torch device BaseScheduling = post_grad_graph_id = next _post_grad_graph_counter _graph_partition_counter = itertools count completed_operations OrderedSet str = OrderedSet available_buffer_names = OrderedSet V graph graph_inputs keys V graph constants keys V graph torchbind_constants keys nodes = create_scheduler_node n n nodes current_node Optional BaseSchedulerNode = None update_zero_dim_cpu_tensor some new constants could have been created above available_buffer_names update V graph constants keys node nodes node prune_deps See Note Graph Partition Device Contexts default_device_context Optional torch device = None name_to_donated_buffer dict str SchedulerDonatedBuffer = get_donated_buffers name_to_node dict str BaseSchedulerNode = n get_name n n nodes name_to_buf dict str SchedulerBuffer = buf get_name buf node nodes buf node get_outputs name_to_fused_node dict str BaseSchedulerNode = name_to_node copy mutation_real_name Maps back original name codegen Example If you mutate buf inside buf s kernel then mutation_real_name = buf buf all subsequent uses buf become buf s usage dependency graph mutation_real_name dict str str = We handle mutation renaming modified versions same buffer dependency graph prevent cycles mutation_renames tracks current name given buffer changed once per mutation Example If you mutate buf inside buf s kernel then mutation_renames = buf buf codegen we only use buf never buf mutation_renames dict str str = Must run first correctly set dependencies before all other passes rely reading read_writes reads unmet_dependencies nodes = comms decide_global_ordering_of_comms nodes name_to_buf name_to_fused_node compute_dependencies nodes = topological_sort_schedule nodes dead_node_elimination name_to_fused_node = n get_name n n nodes compute_ancestors pyrefly ignore bad-assignment metrics ir_nodes_pre_fusion += len nodes torch _inductor debug log_ir_post_fusion log_ir_pre_fusion log_ir_pre_fusion nodes num_orig_nodes = len nodes create_foreach_nodes nodes = topological_sort_schedule nodes logged_slow_fusion = OrderedSet tuple str str config _pre_fusion_custom_pass None nodes = config _pre_fusion_custom_pass nodes nodes = fuse_nodes nodes config _post_fusion_custom_pass None nodes = config _post_fusion_custom_pass nodes merge_loops finalize_multi_template_buffers config combo_kernels dynamo_timed Scheduler create_combo_kernel_nodes log_pt _compile_event=True log_waitcounter=True create_combo_kernel_nodes num_ck_nodes=None Peak memory pass overlap pass must run last otherwise other reordering passes could undo their effects config reorder_for_peak_memory memory reorder_for_peak_memory nodes = reorder_for_peak_memory nodes name_to_buf name_to_fused_node OrderedSet V graph graph_inputs keys OrderedSet V graph get_output_names reorder_for_compute_comm_overlap may do benchmarking estimate op runtime Disable now deterministic mode config deterministic config reorder_for_compute_comm_overlap config reorder_for_peak_memory memory assign_memory_planning_info_for_scheduler_buffers assign_memory_planning_info_for_scheduler_buffers nodes name_to_buf used_non_deterministic_runtime_estimations config_comms runtime_estimations_align_across_all_distributed_ranks comms align_runtime_estimations_across_all_distributed_ranks align_runtime_estimations_across_all_distributed_ranks nodes torch _logging trace_structured trace_structured artifact metadata_fn=lambda name scheduler_nodes_before_comm_overlap encoding string payload_fn=lambda \n\n join f snode i + n debug_str + f buffer_names n get_buffer_names i n enumerate nodes nodes = comms reorder_compute_and_comm_for_overlap nodes process_grouped_nodes torch _inductor config graph_partition torch _inductor config triton cudagraphs nodes = maybe_reorder_for_minimizing_partition nodes nodes = reorder_for_partition_with_simple_dependency nodes compute_last_usage torch _inductor config test_configs track_memory_lifecycle insert_memory_check_nodes log_ir_post_fusion nodes V debug graph_diagram nodes debug_draw_graph used during codegen buffer_names_to_free OrderedSet str = OrderedSet fx graph node position appears graph debug attribution origin_to_index dict torch fx Node int = get_metric_table graph_stats add_row lambda graph_id post_grad_graph_id num_nodes_before_fusion num_orig_nodes num_nodes_after_fusion len nodes Unlike V graph removed_buffers op recorded here removed we still need buffer generated alternative ways removed_ops OrderedSet str = OrderedSet get_donated_buffers - dict str SchedulerDonatedBuffer name_to_donated_buf = name V graph graph_inputs_original isinstance V graph graph_inputs_original name ir DonatedBuffer name_to_donated_buf name = SchedulerDonatedBuffer V graph graph_inputs_original name defining_op=None name_to_donated_buf property current_device - Optional torch device V graph current_device current_device setter current_device device Optional torch device - None V graph current_device = device debug_draw_graph - None Generate image graph debugging os environ get INDUCTOR_WRITE_SCHEDULER_GRAPH None == debug draw_buffers draw_buffers nodes print_graph=True debug_print_nodes label str - None log isEnabledFor logging INFO log info s label node nodes node log_details create_scheduler_node node ir Operation - BaseSchedulerNode assert node get_origins None All nodes passed scheduling must have origin node is_no_op NopKernelSchedulerNode node isinstance node ir ComputedBuffer ir TemplateBuffer SchedulerNode node isinstance node ir ExternKernel ExternKernelSchedulerNode node raise NotImplementedError node create_foreach_nodes - None removed_node_names OrderedSet str = OrderedSet fe_nodes = kept_node_names = name_to_fused_node keys names V graph lists values names = name name names name kept_node_names isinstance name_to_node name NopKernelSchedulerNode names All nodes eliminated continue removed_node_names update names snodes = name_to_node name name names enable_autotune = config combo_kernels_autotune fe_node = ForeachKernelSchedulerNode snodes use_custom_partition_algo=False enable_autotune=enable_autotune fe_nodes append fe_node name names name_to_fused_node name = fe_node nodes = node node nodes node get_name removed_node_names + list fe_nodes compute_dependencies - None Create dependency edges between nodes handling aliasing mutation properly DedupList Generic _T This data structure behaves like list except makes sure elements remain unique Normally one could use OrderedSet dict purpose however list question gets elements appended being iterated over which means we need keep list semantics __init__ items Optional list _T = None membership Optional OrderedSet _T = None - None items = items membership = membership OrderedSet append node_user _T - None node_user membership items append node_user membership add node_user __add__ other DedupList _T - DedupList _T new_membership = OrderedSet union membership other membership new_items = items + x x other items x membership DedupList new_items new_membership pyrefly ignore not-a-type name_to_users defaultdict str DedupList NodeUser = collections defaultdict DedupList handle aliasing using python aliasing name_to_users foo aliases bar then we will make name_to_users foo point same python list name_to_users bar node nodes buf node get_outputs buf _name = buf get_name This handling auto functionized ops which None mutate more than inputs we shouldn t let them all point same user list since buffers aliases list might alias each other isinstance buf node layout ir NoneLayout len buf get_aliases continue buf _name buf get_aliases buf _name name_to_users buf _name name_to_users merge two list = name_to_users buf _name list = name_to_users buf _name combined = list + list key name_to_users keys name_to_users key list name_to_users key list name_to_users key = combined buf _name name_to_users name_to_users buf _name = name_to_users buf _name name_to_users buf _name = name_to_users buf _name pyrefly ignore not-a-type rename n str - str n mutation_renames rename mutation_renames n n add_user pyrefly ignore not-a-type used_by_name str user_node Union BaseSchedulerNode OutputNode can_inplace bool = False is_weak bool = False - None name_to_users rename used_by_name append NodeUser user_node can_inplace is_weak pyrefly ignore not-a-type unbacked_symbol_to_origin_node dict sympy Symbol Optional str = NB None means dependency input Don t actually generate dependency because we do Inductor will start trying free unbacked int s pointless val V graph graph_inputs values isinstance val sympy Expr fs val free_symbols unbacked_symbol_to_origin_node fs = None isinstance val ir TensorBox We also need add symbols input size well because AOTI doesn t lift unbacked symints inputs sym_size = s s val get_size isinstance s sympy Expr s sym_size fs s free_symbols unbacked_symbol_to_origin_node fs = None has_non_input_unbacked_defs = False node nodes assert node node None unbacked symbols don t follow ordinary buffer dependencies so we track their uses separately unbacked_symbol_defs = sorted node node get_unbacked_symbol_defs key=lambda x x name s unbacked_symbol_defs assert isinstance s sympy Symbol Pick first definer canonical There may multiple because MultiOutputLayout buffer propagates unbacked symint multiple outputs they will all claim has_non_input_unbacked_defs = True s unbacked_symbol_to_origin_node unbacked_symbol_to_origin_node s = node get_name node nodes log debug scheduling s node node has_non_input_unbacked_defs assert node node None unbacked_symbol_uses = sorted node node get_free_symbol_uses unbacked_only=True key=lambda x x name kernel takes unbacked symints register dependencies s unbacked_symbol_uses assert s unbacked_symbol_to_origin_node f s unbacked_symbol_to_origin_node r = unbacked_symbol_to_origin_node s None buf name_to_node r get_outputs node add_fake_dep StarDep buf get_name len node read_writes writes == dep = next iter node read_writes writes isinstance dep MemoryDep node_mode = dep mode node_mode = None Handle output mutations buf node get_outputs node will mutate either buffers assert len buf get_mutations = alt_name buf get_mutations alt_name = rename alt_name node must run after prior writer add_user alt_name node node add_fake_dep StarDep alt_name mode=node_mode user name_to_users alt_name items user get_name == node get_name continue assert isinstance user node BaseSchedulerNode other_name user node get_buffer_names node must run after all prior readers other_name = rename other_name node add_fake_dep WeakDep other_name mutating_buf=buf get_name add_user other_name node is_weak=True add_dep V graph additional_buffer_deps node get_name add_user add_dep node is_weak=True node add_fake_dep WeakDep add_dep node get_name add normal non-mutation dependencies read node read_writes reads isinstance read WeakDep add_user read name node node can_inplace read node update_mutated_names mutation_renames update our renaming scheme next iteration buf node get_outputs alt_name buf get_mutations mutation_renames rename alt_name = buf get_name mutation_renames alt_name = buf get_name mutation_real_name buf get_name = mutation_real_name get alt_name alt_name make sure outputs aren t dead-code-eliminated buf_name V graph get_output_names log debug scheduling output s buf_name add_user buf_name OutputNode StarDep buf_name make sure unbacked symints aren t dead-code-eliminated has_non_input_unbacked_defs out V graph graph_outputs s out get_free_symbol_uses unbacked_only=True assert s unbacked_symbol_to_origin_node f s unbacked_symbol_to_origin_node keys r = unbacked_symbol_to_origin_node s buf_name name_to_node r get_buffer_names log debug scheduling output s unbacked symint s buf_name s add_user buf_name OutputNode StarDep buf_name make sure input mutation isn t dead-code-eliminated name mutation_renames name V graph graph_inputs add_user name OutputNode StarDep name V graph mutated_inputs add name name V graph constants In AOTI module parameters buffers lifted graph inputs add_user name OutputNode StarDep name inp_names = name index index name enumerate V graph graph_inputs keys V graph mutated_input_idxs = inp_names name name V graph mutated_inputs copy users information onto nodes node nodes buf node get_outputs buf set_users name_to_users buf get_name items name name_to_donated_buffer name_to_donated_buffer name set_users name_to_users name items For debug logging logbuf = IndentedBuffer logbuf splice key value name_to_users items logbuf indent users = v get_name v value items logbuf splice f key users logbuf splice str = logbuf getrawvalue rstrip compute_dependencies_log debug BUFFER USER LIST\n compute_dependencies_log debug ===== AFTER SCHEDULING =====\n s str insert_memory_check_nodes - None memory assign_memory_planning_info_for_scheduler_buffers compute_memory_timeline FreeableInputBuffer get_freeable_input_buf graph_inputs OrderedSet str = OrderedSet V graph graph_inputs keys name_to_freeable_input_buf dict str FreeableInputBuffer = get_freeable_input_buf nodes graph_inputs torch _inductor config reorder_for_peak_memory assign_memory_planning_info_for_scheduler_buffers nodes name_to_buf graph_outputs OrderedSet str = OrderedSet V graph get_output_names buf_info_list _ _ = compute_memory_timeline nodes name_to_freeable_input_buf graph_outputs step_allocs_deallocs list tuple list str list str = _ range len nodes buf_info buf_info_list Skip zero-size buffers buf_info size_alloc == buf_info size_free == continue buf_name = buf_info buffer get_name step_allocs_deallocs buf_info start_step append buf_name step_allocs_deallocs buf_info end_step append buf_name torch _inductor runtime debug_utils register_check_mem_op register_check_mem_op construct_mem_check_node step_idx int is_final_step bool - ExternKernelSchedulerNode expected_newly_alive = step_allocs_deallocs step_idx expected_newly_dead = step_allocs_deallocs step_idx nontensor_args = expected_newly_alive expected_newly_dead is_final_step node = ir MemoryCheckKernel layout=NoneLayout device=torch device cpu kernel=torch ops _inductor_debug check_memory_step default tensor_args= nontensor_args=nontensor_args unflatten_args=lambda tensor_args constant_args tensor_args alive constant_args dead constant_args is_final_step constant_args node operation_name = f mem_check_ nodes step_idx get_name ExternKernelSchedulerNode node new_nodes = i node enumerate nodes new_nodes append node new_nodes append construct_mem_check_node i is_final_step= i == len nodes - nodes = new_nodes dead_node_elimination - None Remove any nodes without users nodes topological order so iterating reverse order we have visited potentially removed all users before visiting given node updated_nodes = node reversed nodes can_eliminate_user user NodeUser - bool user is_weak user get_name V graph removed_operations active_buffers = False buf node get_outputs can_eliminate = all can_eliminate_user u u buf users can_eliminate log debug removed dead buffer s buf get_name V graph removed_buffers add buf get_name active_buffers = True can_eliminate = node has_side_effects active_buffers can_eliminate updated_nodes append node dead code log debug removed dead operation s node get_name V graph removed_operations add node get_name read node read_writes reads read name name_to_buf users = name_to_buf read name users name_to_buf read name users = u u users u node get_name = node get_name nodes = list reversed updated_nodes Prune any WeakDeps no longer needed node nodes node prune_weak_deps topological_sort_schedule nodes list BaseSchedulerNode - list BaseSchedulerNode Ensure nodes topologically sorted order seen = OrderedSet BaseSchedulerNode name_to_node dict str BaseSchedulerNode = dict result list BaseSchedulerNode = visit n BaseSchedulerNode - None n seen seen add n dep sorted n unmet_dependencies key=lambda d d name We only care about doing toposort within ` nodes ` dep name name_to_node continue visit name_to_node dep name result append n node nodes name node get_buffer_names name_to_node name = node node nodes visit node result _get_unmet_dep_nodes snode BaseSchedulerNode - list BaseSchedulerNode unmet_deps OrderedSet str = OrderedSet isinstance snode SchedulerNode ExternKernelSchedulerNode NopKernelSchedulerNode FusedSchedulerNode dep snode unmet_dependencies unmet_deps add dep name raise RuntimeError f get_unmet_dep_nodes implemented type snode unmet_dep_ops = name_to_buf dep defining_op_name dep unmet_deps list OrderedSet name_to_fused_node n n unmet_dep_ops _topological_sort_nodes - list list BaseSchedulerNode Sort nodes their topological order list node lists order = nodes = dict fromkeys nodes children dict Any Any = node nodes deps = _get_unmet_dep_nodes node nodes node = len deps dep deps c = children get dep c append node children dep = c zero_deg_nodes = n n v nodes items v == while zero_deg_nodes order append zero_deg_nodes n zero_deg_nodes user children get n nodes user -= nodes pop n zero_deg_nodes = n n v nodes items v == assert nodes Topological sort failed order compute_ancestors - None Populate each node ancestors note nodes topologically sorted name_to_ancestors dict str OrderedSet str = node nodes ancestors OrderedSet str = OrderedSet dep node unmet_dependencies dep_node_name = name_to_buf dep name defining_op_name ancestors add dep_node_name ancestors &#124; = name_to_ancestors dep_node_name name_to_ancestors node get_name = ancestors node ancestors = ancestors order node enumerate nodes node min_order = order node max_order = order merge_loops - None config loop_ordering_after_fusion node nodes Even CPU we using halide backend we still need merge loops steps below isinstance node SchedulerNode FusedSchedulerNode node is_gpu config cpu_backend = halide continue snode node get_nodes merge loops scheduler node isinstance snode SchedulerNode snode is_template continue snode merge_loops Note CPU backend merging loops will change snode group It s fine Triton backend But we simplify update snode group like group_fn = get_backend snode node get_device group_fn snode group = snode node get_device group_fn snode _sizes There still issue due different snode FusedSchedulerNode having different merged loops Skip CPU backend now fuse_nodes nodes list BaseSchedulerNode - list BaseSchedulerNode Combine eligible nodes into FusedSchedulerNodes dynamo_timed Scheduler fused_nodes log_pt _compile_event=True log_waitcounter=True i range old_len = len nodes fusion_log debug ===== attempting fusion d d nodes ===== i + old_len nodes = fuse_nodes_once nodes is_reorder_round=False new_len = len nodes fusion_log debug completed fusion round d fused d nodes into d nodes\n i + old_len new_len new_len == old_len new_len == fusion_log debug ===== fusion complete d iterations ===== i + break config loop_ordering_after_fusion nodes = fuse_nodes_once nodes is_reorder_round=True nodes process_grouped_nodes - None Unpack GroupedSchedulerNode into regular nodes new_nodes list BaseSchedulerNode = node nodes new_nodes extend node unpack isinstance node GroupedSchedulerNode node nodes = new_nodes benchmark_fused_nodes nodes Sequence BaseSchedulerNode - tuple float str Benchmark fused list nodes execution time milliseconds randomly generated inputs assert len nodes device = nodes get_device current_device = device backend = get_backend device dynamo_timed benchmark_fused_nodes log_pt _compile_event=True dynamo_compile_column_us= compile_time_autotune_time_us backend benchmark_fused_nodes nodes generate_kernel_code_from_nodes nodes Sequence BaseSchedulerNode benchmark_kernel bool hint_override Optional int = None - str Benchmark fused list nodes execution time milliseconds randomly generated inputs assert len nodes device = nodes get_device current_device = device backend = get_backend device dynamo_timed benchmark_fused_nodes backend generate_kernel_code_from_nodes nodes benchmark_kernel hint_override=hint_override benchmark_codegened_module module ModuleType device torch device - tuple float str Benchmark fused list nodes execution time milliseconds randomly generated inputs current_device = device backend = get_backend device dynamo_timed benchmark_fused_nodes backend benchmark_codegened_module module finalize_multi_template_buffers - None Finalize backing choice MultiTemplateBuffers which did already have choice finalized through fusion In case extern choice will result replacing SchedulerNode If MultiTemplateBuffer did have any fusion opportunities finalizing choice will force completion compilation benchmarking i node enumerate nodes isinstance node SchedulerNode isinstance node node ir MultiTemplateBuffer multi_node = node node config test_configs force_extern_kernel_in_multi_template min_node_unfused _ = multi_node get_min_choice min_node_unfused = next timing timing multi_node choice_timings isinstance timing torch _inductor select_algorithm ExternKernelCaller isinstance min_node_unfused torch _inductor ir TritonTemplateCallerBase config multi_kernel_hints callers dict Optional int TritonTemplateCallerBase = callers None = min_node_unfused hint config multi_kernel_hints timings = multi_node choice_timings hint_override=hint triton_timings = k v k v timings items isinstance k TritonTemplateCallerBase choice = min triton_timings items key=lambda x x callers hint = choice node node finalize_as_triton_callers callers node node finalize_as_triton_caller min_node_unfused continue ir IRNode current_origins multi_node origins out_tensorbox = min_node_unfused output_node out_storage = out_tensorbox data type ignore union-attr assert isinstance out_storage ir StorageBox out_buffer = out_storage data assert isinstance out_buffer ir OperationBuffer multi_node origin_node assign_origin_node out_tensorbox multi_node origin_node out_buffer layout = multi_node layout _replace_node out_buffer multi_node i node _replace_node out_buffer ir OperationBuffer multi_node ir MultiTemplateBuffer i int node SchedulerNode - None _replace_operation_buffer multi_node out_buffer new_scheduler_node = create_scheduler_node out_buffer nodes i = new_scheduler_node name_to_node node get_name = new_scheduler_node name_to_fused_node node get_name = new_scheduler_node We need reflect mutation renames recorded original node mutation_renames = dep itertools chain node read_writes reads node unmet_dependencies real_name = mutation_real_name get dep name None mutation_renames real_name = dep name rename_deps deps OrderedSet Dep - OrderedSet Dep OrderedSet dep rename mutation_renames dep deps new_scheduler_node unmet_dependencies = rename_deps new_scheduler_node unmet_dependencies new_scheduler_node read_writes reads = rename_deps new_scheduler_node read_writes reads new_out old_out zip new_scheduler_node get_outputs node get_outputs name_to_buf old_out get_name = new_out new_out users = old_out users new_scheduler_node min_order = node min_order new_scheduler_node max_order = node max_order new_scheduler_node last_usage = node last_usage _any_atomic_add node_list Sequence BaseSchedulerNode - bool any hasattr n node data n node None hasattr n node data scatter_mode n node data scatter_mode == atomic_add n node_list speedup_by_fusion node BaseSchedulerNode node BaseSchedulerNode - Union bool Callable bool If config benchmark_fusion False always True Otherwise True fusion can brings speedup is_multi_template = any n is_template isinstance n get_template_node ir MultiTemplateBuffer n node node config benchmark_fusion is_multi_template True node is_template isinstance node get_template_node ir TritonTemplateBuffer node is_foreach node is_foreach TODO support benchmarking epilogue fusion True node_list_ = node get_nodes device = node_list_ get_device assert device don t support benchmark fusion CPU right now device type == cpu True node_list_ = node get_nodes node_list_fused = list itertools chain node_list_ node_list_ We can accurately benchmark kernel using atomic_add due how we generate random integer inputs Skip benchmarking them allowing fusion _any_atomic_add node_list_fused True triton compiler errors CompilationError why = WhyNoFuse node node device = node_list_fused get_device assert device None log_fusion ms_fused float ms float ms float - None fusion_log isEnabledFor logging DEBUG ms_fused ms + ms fusion_log debug can fuse benchmark fusing s s cause sx speedup node get_buffer_names node get_buffer_names green_text f ms + ms ms_fused f fusion_log debug cannot fuse benchmark fusing s s cause sx slowdown node get_buffer_names node get_buffer_names red_text f ms_fused ms + ms f async_compile = torch _inductor async_compile AsyncCompile compile_kernel nodes Sequence BaseSchedulerNode hint_override Optional int = None - tuple Optional LambdaFuture ModuleType src_code = generate_kernel_code_from_nodes nodes benchmark_kernel=True hint_override=hint_override mod = PyCodeCache load src_code async_compile use_process_pool fut = None fut = async_compile triton kernel_name= triton_ source_code=src_code assert isinstance fut LambdaFuture fut mod is_multi_template any n get_template_node None n node node epilogue_fusion = node get_template_node None multi_node = node get_template_node epilogue_fusion node get_template_node assert isinstance multi_node ir MultiTemplateBuffer hint_override_best_fusion_choice dict Optional int TritonTemplateCallerBase = future_choices list tuple Any Optional LambdaFuture ModuleType = hint_override config multi_kernel_hints choice_timings = multi_node choice_timings hint_override choice _ sorted choice_timings items key=lambda x x isinstance choice torch _inductor select_algorithm TritonTemplateCaller continue multi_node swap_as_triton_caller choice future_choices append choice compile_kernel node_list_fused hint_override=choice hint_override min_ms_fused = float inf ms_fused_choice Optional TritonTemplateCallerBase = None new_timings = choice future mod_fused future_choices try future None future result except Exception e fusion_log isEnabledFor logging DEBUG fusion_log debug noqa G Exception compiling s s prologue epilogue_fusion epilogue str e continue multi_node swap_as_triton_caller choice ms_fused path = benchmark_codegened_module mod_fused device new_timings choice = ms_fused ms_fused min_ms_fused min_ms_fused = ms_fused ms_fused_choice = choice multi_node _choice_timings hint_override = new_timings assert isinstance ms_fused_choice TritonTemplateCallerBase hint_override_best_fusion_choice hint_override = ms_fused_choice Eagerly compile benchmark non-template nodes choice_timings = multi_node choice_timings _ ms = multi_node get_min_choice ms path = benchmark_fused_nodes node_list_ epilogue_fusion benchmark_fused_nodes node_list_ Start compiling choices parallel future_choices list tuple Any Optional LambdaFuture ModuleType = triton_choices = choice unfused_time sorted choice_timings items key=operator itemgetter isinstance choice torch _inductor ir TritonTemplateCallerBase continue For prologue fusion we check underlying template choice supports all allowed prologue inputs If we skip choice fusion benchmark TODO Remove check after all Triton templates support prologue fusion Currently persistent+TMA Triton template does due TMA-based loads epilogue_fusion hasattr choice allowed_prologue_inps choice allowed_prologue_inps = multi_node allowed_prologue_inps continue unfused_time = ms + ms break triton_choices += triton_choices config max_epilogue_benchmarked_choices break multi_node swap_as_triton_caller choice future_choices append choice compile_kernel node_list_fused len future_choices == False benchmark_when_ready - bool min_ms_fused = float inf ms_fused_choice = None new_timings = Benchmark each choice after compilation completes choice future mod_fused future_choices try future None future result Ideally we would more narrowly catch Exceptions here triton will unpredictably error valid prologue fusions except Exception e fusion_log isEnabledFor logging DEBUG fusion_log debug noqa G Exception compiling s s prologue epilogue_fusion epilogue str e continue pyrefly ignore missing-attribute multi_node swap_as_triton_caller choice ms_fused path = benchmark_codegened_module mod_fused pyrefly ignore bad-argument-type device new_timings choice = ms_fused ms_fused min_ms_fused min_ms_fused = ms_fused ms_fused_choice = choice log_fusion min_ms_fused ms ms min_ms_fused ms + ms ms_fused_choice None config multi_kernel_hints hint_override_best_fusion_choice None = ms_fused_choice pyrefly ignore missing-attribute multi_node finalize_as_triton_callers hint_override_best_fusion_choice pyrefly ignore missing-attribute multi_node finalize_as_triton_caller ms_fused_choice pyrefly ignore missing-attribute multi_node _choice_timings None = new_timings True False benchmark_when_ready Start parallel compilation all three kernels future_and_mod_l = compile_kernel node_list_ future_and_mod_l = compile_kernel node_list_ future_and_mod_l _fused = compile_kernel node_list_fused benchmark_when_ready - bool torch _inductor runtime triton_heuristics NoTritonConfigsError try Wait all compilations complete fut future_and_mod_l future_and_mod_l future_and_mod_l _fused fut None fut result ms path = benchmark_codegened_module future_and_mod_l pyrefly ignore bad-argument-type device math isinf ms why register spilling first kernel False ms path = benchmark_codegened_module future_and_mod_l pyrefly ignore bad-argument-type device math isinf ms why register spilling second kernel False ms_fused path_fused = benchmark_codegened_module future_and_mod_l _fused pyrefly ignore bad-argument-type device math isinf ms_fused why register spilling fused kernel False log_fusion ms_fused ms ms is_metric_table_enabled slow_fusion ms_fused = ms + ms path path logged_slow_fusion logged_slow_fusion add path path get_metric_table slow_fusion add_row lambda kernel _path path kernel _latency ms kernel _path path kernel _latency ms fused_kernel_path path_fused fused_kernel_latency ms_fused slow_down_ratio ms_fused ms + ms ms_fused ms + ms except NoTritonConfigsError False except CompilationError e Loop-carried variable str e True raise benchmark_when_ready get_fused_node node BaseSchedulerNode - BaseSchedulerNode Look up node Scheduler name_to_fused_node name_to_fused_node node get_first_name fuse_nodes_once nodes list BaseSchedulerNode is_reorder_round bool - list BaseSchedulerNode Combine eligible nodes into FusedSchedulerNodes This relies two key functions control logic - can_fuse checks fusion legal - score_fusion assigns priority given fusion prune_redundant_deps nodes fused_nodes = OrderedSet nodes fusion_log isEnabledFor logging DEBUG fusion_log debug fuse_nodes_once candidates node fused_nodes fusion_log debug s node debug_str_short These potential fusions which we async compiling which we will benchmark profitability pending_fusions dict BaseSchedulerNode tuple Callable bool BaseSchedulerNode BaseSchedulerNode = fuse_two_nodes node BaseSchedulerNode node BaseSchedulerNode - BaseSchedulerNode fusion_log debug fusing s s node get_name node get_name device = node get_device assert node get_device == device node = get_backend device fuse node node fused_nodes remove node fused_nodes remove node fused_nodes add node name_to_fused_node update n get_name node n node get_nodes node resolve_pending_fusions node BaseSchedulerNode node BaseSchedulerNode - None while get_fused_node node pending_fusions get_fused_node node pending_fusions pending_fusion = pending_fusions get get_fused_node node pending_fusions get get_fused_node node None assert pending_fusion None is_speedup node_key node_key = pending_fusion pending_fusions pop node_key None pending_fusions pop node_key None assert get_fused_node node_key node_key assert get_fused_node node_key node_key is_speedup will_fusion_create_cycle node node continue fuse_two_nodes node_key node_key node node get_possible_fusions nodes is_reorder_round either node pending fusion resolve since we iterate potential fusions based profitability first potential fusion should take precedence resolve_pending_fusions node node node = get_fused_node node node = get_fused_node node can_fuse node node is_reorder_round will_fusion_create_cycle node node speedup = speedup_by_fusion node node callable speedup pending_fusions node = speedup node node pending_fusions node = speedup node node continue speedup continue fuse_two_nodes node node seen_pair_speedup_fn OrderedSet Callable bool = OrderedSet is_speedup_fn node_key node_key pending_fusions values is_speedup_fn seen_pair_speedup_fn continue seen_pair_speedup_fn add is_speedup_fn assert get_fused_node node_key node_key assert get_fused_node node_key node_key is_speedup_fn will_fusion_create_cycle node_key node_key fuse_two_nodes node_key node_key nodes = sorted fused_nodes key=lambda x x min_order nodes = topological_sort_schedule nodes nodes create_combo_kernel_nodes num_ck_nodes Optional int = None - None Groups parallel nodes fused_nodes = OrderedSet nodes count = num_nodes_orig = len nodes log debug ComboKernels Generating num_ck_nodes = s num_ck_nodes num node_list enumerate ForeachKernelSchedulerNode group_nodes_for_combo_kernels node_list = ForeachKernelSchedulerNode combinable_nodes node_list len node_list continue num_ck_nodes None count num_ck_nodes break speedup_by_combo_kernel node_list log debug ComboKernels Not speeding up d-th group num continue count += enable_autotune = config combo_kernels_autotune group_snode = ForeachKernelSchedulerNode node_list scheduler node_list use_custom_partition_algo=True enable_autotune=enable_autotune log info ComboKernels Combining d nodes d-th group len node_list num node node_list fused_nodes remove node fused_nodes add group_snode name_to_fused_node update n get_name group_snode n group_snode get_nodes nodes = sorted fused_nodes key=lambda x x min_order nodes = topological_sort_schedule nodes log info Generated ComboKernel nodes d ComboKernels totally d - d nodes count num_nodes_orig len nodes prune_redundant_deps nodes prune_redundant_deps nodes list BaseSchedulerNode - None node nodes node prune_redundant_deps name_to_fused_node get_possible_fusions nodes list BaseSchedulerNode is_reorder_round bool - list tuple BaseSchedulerNode BaseSchedulerNode Helper find all legal fusion opportunities sorted score_fusion possible_fusions = seen = OrderedSet tuple BaseSchedulerNode BaseSchedulerNode check_all_pairs nodes list BaseSchedulerNode - None node _index node enumerate nodes node nodes node _index + node _index + + config max_fusion_buffer_group_pairwise_attempts key = node node key seen continue seen add key can_fuse node node is_reorder_round possible_fusions append key node is_template node is_foreach can_fuse node node is_reorder_round foreach fusions epilogue fusions order dependent possible_fusions append node node buffer_names_grouping = collections defaultdict list node nodes unfusable_node node continue buf node used_buffer_names buffer_names_grouping buf append node node_grouping buffer_names_grouping values check_all_pairs node_grouping config aggressive_fusion group_grouping = collections defaultdict list node nodes group = getattr node group None group group_grouping group append node node_grouping group_grouping values check_all_pairs node_grouping possible_fusions = get_possible_fusions_with_highest_priority possible_fusions possible_fusions sort key=self score_fusion_key reverse=True fusion_log debug found d possible fusions len possible_fusions possible_fusions will_fusion_create_cycle node BaseSchedulerNode node BaseSchedulerNode - bool Finds whether there s path node node vice-versa caused indirectly other fusions since we just returning boolean here use slightly faster unordered set visited = OrderedSet FusedSchedulerNode found_path node BaseSchedulerNode - bool only fused nodes can introduce new ancestors isinstance node FusedSchedulerNode node visited visited add node node get_operation_names issubset combined_ancestors All fusion outputs ancestors node node thus cannot introduce new path output neither descendent node node output cannot introduce path due can_fuse WLOG output descendent node cannot path node - node hence cannot ancestor node due acyclic WLOG output descendent node cannot ancestor node False continue DFS new ancestors introduced fusion bool combined_names node ancestors any found_path name_to_fused_node n n node ancestors - combined_ancestors False above - use slightly faster unordered set combined_names = node get_operation_names _dict keys &#124; node get_operation_names _dict keys combined_ancestors = node ancestors _dict keys &#124; node ancestors _dict keys - combined_names cycle = any found_path name_to_fused_node n n combined_ancestors cycle WhyNoFuse node node will create cycle cycle can_fusion_increase_peak_memory node BaseSchedulerNode node BaseSchedulerNode - bool Return true fusing two nodes can potentially increasing peak memory The implementation more like heuristic since we don t really know we peak when trying fuse these two nodes The order nodes may change later which makes peak memory estimation hard Here how we decide LOWER BOUND extra memory allocation we fuse these nodes find all buffers read each node single user These buffers supposed reused we don t fuses these nodes find intersection these buffers two node sum total buffer size If we don t fuse these two nodes we can lease avoid much memory allocation Note extra memory allocation necessarily causing peak memory increase This just heuristic We true only saving fusion can trade off extra memory allocation codegen wrapper buffer_reuse_key _find_single_user_inputs node BaseSchedulerNode - list ir Buffer output = rd node read_writes reads buf = name_to_buf get rd name buf len buf users == buf node has_tensor_output output append buf node output Check inputs can potentially reused lhs_dep_nodes = _find_single_user_inputs node rhs_dep_nodes = _find_single_user_inputs node lhs_reuse_keys = OrderedSet buffer_reuse_key buf buf lhs_dep_nodes rhs_reuse_keys = OrderedSet buffer_reuse_key buf buf rhs_dep_nodes common_reuse_keys = lhs_reuse_keys intersection rhs_reuse_keys memory_overhead = key common_reuse_keys try memory_overhead += int key except ValueError integer Fallback fuse False bw_saving = score_fusion_memory node node The factor here quite arbitrary V graph sizevars statically_known_gt memory_overhead bw_saving True False fusion_prevent_too_many_reads_and_writes node BaseSchedulerNode node BaseSchedulerNode threshold int - bool After fusion we need calculate unique I O buffers accounting buffers become internal removed through fusion Get all nodes will fused node fused_node_names = OrderedSet node get_name node node get_nodes + node get_name node node get_nodes Calculate node reads can removed through fusion i e node reads outputs node node _write_names = OrderedSet dep name dep node read_writes writes node _read_names = OrderedSet dep name dep node read_writes reads reads_removed_through_fusion = node _read_names node _write_names Calculate node writes can removed through fusion i e node writes only read node writes_removed_through_fusion OrderedSet str = OrderedSet write_dep node read_writes writes can_buffer_be_removed_through_fusion write_dep name fused_node_names writes_removed_through_fusion add write_dep name Get all unique reads union both nodes reads all_read_names = OrderedSet dep name dep node read_writes reads &#124; OrderedSet dep name dep node read_writes reads Get all unique writes union both nodes writes all_write_names = OrderedSet dep name dep node read_writes writes &#124; OrderedSet dep name dep node read_writes writes Remove reads become internal unique_reads = all_read_names - reads_removed_through_fusion Remove writes become internal unique_writes = all_write_names - writes_removed_through_fusion Get all unique buffer names reads writes combined no double counting unique_io_buffers = unique_reads &#124; unique_writes len unique_io_buffers threshold are_long_distant_nodes node BaseSchedulerNode node BaseSchedulerNode - bool This function prevents fusion nodes can increase memory footprint This problem more common horizontal fusion where nodes far apart original order get fused lengthening live intervals tensors This very evident models activation checkpointing where recomputed nodes different checkpointed regions get fused significantly increase memory footprint The current attempt quick possibly hacky heuristic prevent fusion nodes far away original order A better difficult implement heurisitic would use live intervals buffers find region peak pressure original program prevent fusion crosses peak region We might need special care good approximation implementation fusion node changes live intervals re-computing live intervals peak memory after each fusion can introduce large compilation overhead proximity_score = max abs node min_order - node max_order abs node min_order - node max_order proximity_score decide_fusion_fail_reason node BaseSchedulerNode node BaseSchedulerNode common_buf_names Union tuple str OrderedSet str - str Try decide reasons why fusion fail due no shared memory even though there common buffers reasons = node _name dep = dep name dep dep node read_writes reads_and_writes node _name dep = dep name dep dep node read_writes reads_and_writes buf_name common_buf_names buf = V graph get_buffer buf_name lhs_dep = node _name dep buf_name rhs_dep = node _name dep buf_name isinstance lhs_dep MemoryDep isinstance rhs_dep MemoryDep reasons buf_name = f MemoryDep type lhs_dep v s type rhs_dep continue lhs_dep get_numel = rhs_dep get_numel reasons buf_name = f different numel lhs_dep get_numel v s rhs_dep get_numel continue same numel different MemoryDep size Should broadcasting sympy_product lhs_dep size = sympy_product rhs_dep size reasons buf_name = broadcast continue lhs_off = lhs_dep get_offset rhs_off = rhs_dep get_offset lhs_off = rhs_off One example transformer we use concatenated linear layer project Q K V then split result The splits will point same buffer different offsets reasons buf_name = f different offset lhs_off v s rhs_off continue lhs_dep normalize_with_stride_order == rhs_dep normalize_with_stride_order reasons buf_name = f Mismatch loop orders lhs_dep v s rhs_dep continue Add more rules here layout_str = isinstance buf ir TorchBindObject layout_str = f Layout buf layout reasons buf_name = f Unknown reason lhs_dep v s rhs_dep layout_str str reasons shared_data_after_reordering_loop node BaseSchedulerNode node BaseSchedulerNode - int Right now just greedily reorder loop node compatible node ideally we should have some heuristics reorder loop node compatible node s more efficient Return amount shared data re-computed method If no such recomputation happens - since valid amount shared data TODO Don t do loop reordering CPU now Should debug more why does work CPU codegen config loop_ordering_after_fusion any n is_cpu n node node - some rare case template can passed Check test_interaction_with_multi_template test_loop_ordering py https github com pytorch pytorch issues node is_template node is_template - node _buffer_names = node read_writes buffer_names node _buffer_names = node read_writes buffer_names Fast path no common buffers common_buffer_names = node _buffer_names node _buffer_names common_buffer_names - node _name dep = dep name dep dep node read_writes reads_and_writes node _name dep = dep name dep dep node read_writes reads_and_writes Find commons buffers has different loop orders candidates = buffer_name common_buffer_names lhs_dep = node _name dep buffer_name rhs_dep = node _name dep buffer_name lhs_dep normalize_with_stride_order == rhs_dep normalize_with_stride_order candidates append V graph sizevars size_hint lhs_dep get_numel fallback= lhs_dep rhs_dep len candidates == - Pick largest buffer guide loop reordering _numel lhs_dep rhs_dep = max candidates key=operator itemgetter isinstance lhs_dep MemoryDep isinstance rhs_dep MemoryDep - lhs_dep num_vars = rhs_dep num_vars can happen due we don t merge loops We can do loop reordering case right now Simply returning true two Deps same after normalization merging loops lhs_dep normalize == rhs_dep normalize dep_size_hint lhs_dep - reordered = False Only reorder loops pointwise now node is_reduction reordered = node reorder_loops_by_dep_pair lhs_dep rhs_dep node is_reduction reordered = node reorder_loops_by_dep_pair rhs_dep lhs_dep loop_ordering_log debug Don t reorder loops since both nodes reductions s v s s node get_name node get_name score_fusion_memory node node reordered - unfusable_node node BaseSchedulerNode - bool Is node unfusable under any conditions isinstance node ExternKernelSchedulerNode NopKernelSchedulerNode node is_template is_output_of_multi_outputs_template node node check_prologue_fusion_heuristics_fusable prologue_node BaseSchedulerNode template_node BaseSchedulerNode why WhyNoFuse - bool Heuristics avoid benchmarking predictably slow prologue fusions user opt into more aggressive prologue fusion dont use heuristics prologue_node get_operation_names = V graph invoke_quant_ops True read_bytes = prologue_node get_read_buffer_sizes write_bytes = prologue_node get_write_buffer_sizes Initially only do fusions which will result fewer memory accesses inside template avoid potential bad cache behavior shared memory use we also want avoid benchmarking reliably unprofitable fusions like downcasts fp - fp inside kernel allowing gathers allowing increasing write_bytes small factor TODO - make configurable per input instance bias can fuse fp - fp profitably BYTES_THRESHOLD_MULTIPLIER = read_bytes write_bytes BYTES_THRESHOLD_MULTIPLIER why prologue fusion will increase amount bytes read kernel False we want avoid attempting fuse predictably unprofitable prologues such increasing unaligned reads writes TODO - would nice generalize however we would need more explicit knowledge memory access patterns TritonTemplate order know stride order check alignment origins = tuple e target n prologue_node get_nodes n node None e n node get_origins e op == call_function origins == torch ops aten constant_pad_nd default why prologue fusion will increase attempt fuse padding bc increases unaligned reads False low_prec_fp dtype torch dtype - bool dtype itemsize = dtype is_floating_point low_prec_fp template_node get_template_node_or_throw dtype prologue_node can_codegen_in_low_precision why prologue fusion must upcast fp profitable low precision templates False True get_expand_dim_for_pointwise_nodes node BaseSchedulerNode node BaseSchedulerNode - Optional tuple int SchedulerNode sympy Expr Fusing two small pointwise nodes significantly reduces kernel overhead launch overhead However slightly different sizes would prevent fusion Here we decide expanding sizes one node profitible allowing fusion returns dimension expand node smaller sizes new size after expand only support scheduler node isinstance node SchedulerNode isinstance node SchedulerNode None only support computued buffer isinstance node node ir ComputedBuffer isinstance node node ir ComputedBuffer None does support mutation yet since relying index mod handle out-of-boundary access node has_aliasing_or_mutation node has_aliasing_or_mutation None skip halide which does support mod index config cpu_backend == halide None only support pointwise nodes same reduction size n _sizes n _sizes = node _sizes node _sizes n _iter_sizes n _reduce_sizes = n _sizes n _iter_sizes n _reduce_sizes = n _sizes node is_reduction node is_reduction n _reduce_sizes = n _reduce_sizes len n _iter_sizes = len n _iter_sizes None only support nodes write simplification len node read_writes writes len node read_writes writes None When memory access small reducing gpu kernel overhead profitable over slightly larger memory access node _write_memory = dep_size_hint next iter node read_writes writes node _write_memory = dep_size_hint next iter node read_writes writes max node _write_memory node _write_memory config small_memory_access_threshold None does support reinplace since ` index boundary ` may lead race condition has_reusable_buffer node BaseSchedulerNode - bool read node read_writes reads input_buf Optional Union SchedulerBuffer SchedulerDonatedBuffer read name name_to_donated_buffer input_buf = name_to_donated_buffer read name input_buf = name_to_buf get read name input_buf V graph wrapper_code can_reuse input_buf node isinstance input_buf defining_op NopKernelSchedulerNode True False has_reusable_buffer node has_reusable_buffer node None only support nodes mismatch dimension mismatch_dimensions = idx n _size n _size enumerate zip n _iter_sizes n _iter_sizes n _size = n _size mismatch_dimensions append idx len mismatch_dimensions = None mismatch_dim = mismatch_dimensions mismatch_size mismatch_size = n _iter_sizes mismatch_dim n _iter_sizes mismatch_dim V graph sizevars statically_known_lt mismatch_size mismatch_size mismatch_dim node mismatch_size V graph sizevars statically_known_lt mismatch_size mismatch_size mismatch_dim node mismatch_size None can_fuse node BaseSchedulerNode node BaseSchedulerNode can_reorder bool = False - bool Determine possible combine node node into single fused node node node False We don t further fuse FusedMixOrderReductions now It s big deal since score fusion mix order reduction low When we do kind fusion participants should have already been well fused isinstance node FusedMixOrderReductions isinstance node FusedMixOrderReductions False why = WhyNoFuse node node node is_template get_backend node get_device can_fuse_multi_outputs_template node node True isinstance node GroupedSchedulerNode isinstance node GroupedSchedulerNode why grouped node must fused other nodes False isinstance node ExternKernelSchedulerNode NopKernelSchedulerNode node is_template why node extern nop False isinstance node ExternKernelSchedulerNode NopKernelSchedulerNode node is_template why node extern nop False node get_operation_names node ancestors why node must go before node False node is_template config prologue_fusion why prologue fusion turned off False node is_reduction node is_template why prologue fusion only supported pointwise nodes False template = node get_template_node_or_throw isinstance template ir TritonTemplateBuffer why prologue fusion only supported TritonTemplates False allowed_prologue_inps = template get_allowed_prologue_inps unsupported_prologue_args = OrderedSet inp get_name inp template inputs type ignore union-attr - allowed_prologue_inps node get_buffer_names unsupported_prologue_args why prologue fusion implemented kernel these inputs False node has_aliasing_or_mutation node has_aliasing_or_mutation why template prologue can only fuse functional pointwise nodes False prologue_nodes = node get_nodes node prologue_nodes - node_outs = node get_outputs out node_outs all user node prologue_nodes user out users why template prologue can only fuse nodes single use False template_snodes = node isinstance node FusedSchedulerNode n n node snodes n is_template assert len template_snodes == template_snode = template_snodes len prologue_nodes - outputs == len prologue_nodes - outputs users == prologue_nodes - outputs users node template_snode why template prologue can only fuse nodes single use into template False check_prologue_fusion_heuristics_fusable node node why False node is_template node has_aliasing_or_mutation node is_reduction config epilogue_fusion why template epilogue satisfied False node get_buffer_names V graph no_fuse_buffer_names node get_buffer_names V graph no_fuse_buffer_names why fusion buffer explicit disabled False device = node get_device device = node get_device device = device why device mismatch s vs s device device False del device shared_data_score = score_fusion_memory node node can_reorder shared_data_score config score_fusion_memory_threshold config loop_ordering_after_fusion new_shared_data_score = shared_data_after_reordering_loop node node new_shared_data_score = shared_data_score = new_shared_data_score config expand_dimension_for_pointwise_nodes expand_analysis = get_expand_dim_for_pointwise_nodes node node expand_dim smaller_node expand_size = expand_analysis smaller_node expand_dimension_for_pointwise_node expand_dim expand_size shared_data_score = score_fusion_memory node node loop_ordering_log isEnabledFor logging DEBUG loop_ordering_log debug s s has s shared data node get_name node get_name shared_data_score V choices can_fuse node node shared_data_score False node get_operation_names node ancestors node depends node outputs can_fuse_vertical node node V choices can_fuse_vertical node node shared_data_score get_backend device can_fuse_vertical node node nodes don t depend each other may have common reads V choices can_fuse_horizontal node node shared_data_score get_backend device can_fuse_horizontal node node can_fuse_vertical node BaseSchedulerNode node BaseSchedulerNode - bool Check legal fuse consumer node into producer node We can fuse them all reads node either match corresponding writes node written nodes can scheduled before fusion node node node _buf_names = node get_buffer_names why = WhyNoFuse node node remaining_deps_by_name dict str list Dep = defaultdict list dep node unmet_dependencies name = mutation_renames get dep name dep name isinstance dep WeakDep fusable_weak_dep dep node node continue remaining_deps_by_name name append dep cd node read_writes writes isinstance cd MemoryDep continue remaining = remaining_deps_by_name get mutation_renames get cd name cd name remaining rd remaining fusable_read_and_write rd cd remaining remove rd noqa B remaining_deps = OrderedSet dep name dep itertools chain from_iterable remaining_deps_by_name values remaining_deps node _buf_names MemoryDeps didn t match read different locations same buffer Examples here include - MemoryDep foo x = MemoryDep foo x + - MemoryDep foo x = StarDep foo why memory deps did match False node _op_names = node get_operation_names name remaining_deps op_name = name_to_buf name defining_op_name node _op_names name_to_fused_node op_name ancestors why intermediate nodes between node node False True fusable_weak_dep weak_dep WeakDep node BaseSchedulerNode node BaseSchedulerNode - bool weak_dep name node get_buffer_names False A weak dep can fused only fused operation acts inplace buffer being mutated i e same index being read then mutated mutating_writes = write write node read_writes writes write name == weak_dep mutating_buf len mutating_writes = False write = mutating_writes isinstance write StarDep False assert isinstance write MemoryDep free_symbol_is_type write index SymT TMP False real_name = mutation_real_name weak_dep mutating_buf relevant_reading_nodes = node isinstance node ForeachKernelSchedulerNode relevant_reading_nodes = node snodes num_concurrent_reads = reading_node relevant_reading_nodes relevant_reads = read read reading_node read_writes reads read name == real_name relevant_reads continue num_concurrent_reads += all isinstance read MemoryDep free_symbol_is_type read index SymT TMP read index == write index read size == write size read relevant_reads False num_concurrent_reads = StarDep doesn t match MemoryDep different indices don t match However broadcasting sometimes strips dimensions s case we still can match unmet dep there s indirect indexing don t match fusable_read_and_write read Dep write MemoryDep - bool isinstance read MemoryDep read_name = mutation_renames get read name read name read_name = write name free_symbol_is_type read index SymT TMP free_symbol_is_type write index SymT TMP False config loop_ordering_after_fusion read num_vars = write num_vars Need merge loops we do loop ordering after fusion since we have merged loops yet when creating scheduler nodes read = read normalize write = write normalize read index == write index len read size = len write size read size len write size == write size isinstance read StarDep read_name = mutation_renames get read name read name write_name = mutation_renames get write name write name read mode == write mode write mode None read_name == write_name True False dep_size_hint dep Dep - int V graph get_dep_size_hint dep score_fusion_memory node BaseSchedulerNode node BaseSchedulerNode - int The first term our fusion score estimates number saved memory operations node _dep_len = len node read_writes reads + len node read_writes writes node _dep_len = len node read_writes reads + len node read_writes writes optimization iter over smaller set min node _dep_len node _dep_len max node _dep_len node _dep_len node _dep_len node _dep_len node node = node node deps = dep dep node read_writes reads &#124; node read_writes writes dep node read_writes reads dep node read_writes writes sum dep_size_hint dep dep deps common_memory_deps = node read_writes reads &#124; node read_writes writes node read_writes reads &#124; node read_writes writes sum dep_size_hint dep dep common_memory_deps get_possible_fusions_with_highest_priority possible_fusions list tuple BaseSchedulerNode BaseSchedulerNode - list tuple BaseSchedulerNode BaseSchedulerNode Group possible fusions based their priority backend Only group possible fusions highest priority len possible_fusions == possible_fusions possible_fusions_group_by_priority dict int list tuple BaseSchedulerNode BaseSchedulerNode = node node possible_fusions assert node get_device == node get_device device = node get_device fusion_pair_priority = int get_backend device get_fusion_pair_priority node node fusion_pair_priority possible_fusions_group_by_priority possible_fusions_group_by_priority fusion_pair_priority = node node possible_fusions_group_by_priority fusion_pair_priority append node node possible fusions highest priority possible_fusions_with_highest_priority = min possible_fusions_group_by_priority items key=operator itemgetter assert len possible_fusions_with_highest_priority possible_fusions_with_highest_priority score_fusion_key nodes tuple BaseSchedulerNode BaseSchedulerNode - Any Shim list sort key= V choices score_fusion nodes compute_last_usage - None Populate node last_usage recursively also nodes within FusedSchedulerNode future_used_buffers = OrderedSet V graph get_output_names node reversed nodes node set_last_usage future_used_buffers mutation_real_name future_used_buffers update node last_usage free_buffers - None Free any buffers no longer needed name sorted buffer_names_to_free - V graph removed_buffers - V graph wrapper_code freed type ignore has-type name name_to_buf buf = name_to_buf name buf can_free V graph wrapper_code codegen_free buf node name V graph graph_inputs inp = V graph graph_inputs name isinstance inp ir TorchBindObject V graph wrapper_code codegen_free inp isinstance inp ir GeneratorState continue storage = inp data assert isinstance storage ir StorageBox storage is_input_buffer V graph wrapper_code codegen_free storage data buffer_names_to_free clear flush - None backend backends values backend flush free_buffers codegen_extern_call scheduler_node ExternKernelSchedulerNode - None assert isinstance scheduler_node ExternKernelSchedulerNode decide_inplace_update stores inplace update decisions current kernel where allocate retrieve those decisions We have make sure there non-NULL kernel handler store those inplace update decisions counters inductor extern_calls += V set_kernel_handler Kernel increase_kernel_count=False scheduler_node decide_inplace_update scheduler_node mark_run node = scheduler_node node assert isinstance node ir ExternKernel f type node = node codegen V graph wrapper_code free_buffers create_backend device torch device - BaseScheduling assert is_gpu device type device index None f device should have been normalized lowering V graph add_device_info device device_scheduling = get_scheduling_for_device device type device_scheduling None raise RuntimeError f Unsupported device type device type has_triton device type == cuda device_props = torch cuda get_device_properties device major raise GPUTooOldForTriton device_props inspect currentframe is_gpu device type device type == mps raise TritonMissing inspect currentframe device_scheduling get_backend device Optional torch device - BaseScheduling assert device None device backends backends device = create_backend device backends device enter_context node BaseSchedulerNode - None get_order n torch fx Node - int n origin_to_index origin_to_index update n i i n enumerate n graph nodes origin_to_index n Use dict have ordering origins = get_order e e None n node get_nodes n node None e n node get_origins origins = list origins keys origins _ last = max origins key=operator itemgetter V graph wrapper_code enter_context last can_buffer_be_removed_through_fusion name str fused_node_names OrderedSet str - bool try users = name_to_buf name users except KeyError False all user is_weak user get_name fused_node_names user users name mutation_renames name mutation_real_name should_partition node BaseSchedulerNode should_log bool = False - bool Return True we should partition inductor graph node Allow users manually specify node should partitioned Can only do FallbackKernels ir_node = node node isinstance ir_node torch _inductor ir FallbackKernel op = ir_node op_overload op_overload_packet_name = op name op_overload_name = f op_overload_packet_name op _overloadname isinstance op torch _ops OpOverload op_overload_packet_name op_overload_packet_name config custom_should_partition_ops op_overload_name config custom_should_partition_ops assert isinstance op torch _ops OpOverload True When using cudagraphs keep all kernels ` call ` function instead graph partition functions since graph partition only brings benefit cudagraph torch _inductor config triton cudagraphs _unstable_customized_partition_wrapper wrapper None True avoid duplicating logs when should_partition called multiple times same node noop_log msg str node Optional BaseSchedulerNode - None log_partition_reason = maybe_log_cudagraph_partition should_log noop_log isinstance node FusedSchedulerNode any should_partition snode snode node snodes assert node node None node is_gpu log_partition_reason non gpu ops node=node True isinstance node node ir DeviceCopy log_partition_reason DeviceCopy ops node=node True isinstance node node ir Conditional log_partition_reason Conditional ops node=node True getattr node node unbacked_bindings None log_partition_reason unbacked binding ops node=node True is_cudagraph_unsafe_op node node log_partition_reason CUDAGraph-unsafe custom ops node=node True False get_name_to_nodes - dict str Union ir IRNode ir TorchBindObject sympy Expr Return mapping name strings corresponding graph inputs base scheduler node outputs name_to_node dict str Union ir IRNode ir TorchBindObject sympy Expr = name_to_node update V graph graph_inputs node nodes name scheduler_buffer node outputs_by_name items name_to_node name = scheduler_buffer node name_to_node compute_graph_partition_maps signatures list GraphPartitionSignature - None computes mapping partition input output indices graph input output indices each partition name_to_graph_input_index = name idx idx name enumerate V graph graph_inputs name_to_graph_output_index = name idx idx name enumerate V graph get_output_names V graph partition_maps = partition_id signature enumerate signatures signature skip_cudagraph Note Graph Partition Map CUDAGraph number partition map should same number generated partition functions This assumption will used when cudagraphify each partition function continue input_mapping = name signature input_nodes input_mapping append name_to_graph_input_index get name output_mapping = node signature output_nodes output_mapping append name_to_graph_output_index get node get_name V graph partition_maps append GraphPartitionMap partition_id input_mapping output_mapping signature constant_names get_graph_partition_symbol_inputs partition PartitionType input_nodes dict str Union ir IRNode ir TorchBindObject sympy Expr - OrderedSet sympy Symbol Returns all symbol inputs which required scope successfully perform codegen graph partition including - free symbols used partition nodes - free symbols partition input node shapes strides offsets This needed recording cudagraphs tensors dynamic shapes get_layout_symints node ir IRNode - OrderedSet sympy Symbol free_symbol_uses OrderedSet sympy Symbol = OrderedSet layout = node maybe_get_layout isinstance layout ir Layout free_symbol_uses update free_symbols layout size &#124; free_symbols layout stride &#124; free_symbols layout offset isinstance layout ir MutationLayoutSHOULDREMOVE symint may used index layout target free_symbol_uses update get_layout_symints layout target assert layout None f Expect layout None found layout= layout free_symbol_uses get_scheduler_node_symbol_uses node BaseSchedulerNode - OrderedSet sympy Symbol Gets symbols used node isinstance node FusedSchedulerNode OrderedSet union get_scheduler_node_symbol_uses snode snode node snodes assert node node None free_symbol_uses = node node get_free_symbol_uses free_symbol_uses update get_layout_symints ir_node ir_node node node get_outputs free_symbol_uses get_input_node_symbols node Union ir IRNode sympy Expr ir TorchBindObject - OrderedSet sympy Symbol Gets symbols used input node shapes strides offsets isinstance node ir TorchBindObject TorchBindObject does involve dynamic shapes yet OrderedSet isinstance node ir IRNode get_layout_symints node node cannot sympy Expr since node comes read_writes read_writes does contain sympy Expr raise NotImplementedError f Unsupported input node type type node filter_symbols symbols OrderedSet sympy Symbol - OrderedSet sympy Symbol Filters set symbols required codegen Skip symbols always internal kernels such SymT TMP SymT INDEX SymT R _INDEX OrderedSet s s symbols symbol_is_type s SymT SIZE SymT FLOAT SymT UNBACKED_INT SymT UNBACKED_FLOAT candidate_symbols OrderedSet sympy Symbol = OrderedSet union get_scheduler_node_symbol_uses node node partition candidate_symbols union get_input_node_symbols node _ node input_nodes items candidate_symbols = filter_symbols candidate_symbols res OrderedSet sympy Symbol = OrderedSet s candidate_symbols symplified_s = V graph sizevars simplify s use free_symbols only when s simplified Integer expr res update symplified_s free_symbols OrderedSet sorted res key=operator attrgetter name get_graph_partition_signature partitions list PartitionType skip_cudagraphs list bool - list GraphPartitionSignature Gets signature each graph partition including input nodes output nodes whether deallocating input within graph partition signatures = unmet_output_names = OrderedSet V graph get_output_names name_to_node = get_name_to_nodes is_none_layout buf_name str - bool Checks buf_name NoneLayout Buffers NoneLayout allocated so graph partition should take inputs outputs buf = name_to_buf get buf_name None buf None False isinstance buf node layout NoneLayout isinstance buf node ir MutationOutput real_name = mutation_real_name get buf_name None is_none_layout real_name True False partition skip_cudagraph zip reversed partitions reversed skip_cudagraphs output_names OrderedSet str = OrderedSet node partition output_names update node outputs_by_name keys returned_output_names = output_names intersection unmet_output_names all reads writes partition inputs except those generated within partition tensor constants read_writes = dependencies ReadWrites merge_list node read_writes node partition WeakDep fake dependency unused buffer It should appear partition_input_names inputs actually read written partition_input_names = OrderedSet x name x read_writes reads &#124; read_writes writes is_none_layout x name - output_names partition_input_names = OrderedSet mutation_real_name get name name name partition_input_names buffer_names_to_free OrderedSet str = OrderedSet node partition buffer_names_to_free update node last_usage buffer_names_to_free may contain buffers allocated previous graph partitions These buffers should also partition input extra_input_names = name name buffer_names_to_free - output_names name name_to_node partition_input_names update extra_input_names input_nodes = name name_to_node name name partition_input_names name name_to_node input_deallocation = name name buffer_names_to_free name partition_input_names name name_to_node input tensor freed partition function should also returned output This brings benefits cudagraph since returned output tensor cudagraph managed tensor static tensor address extra_output_names = name name partition_input_names name name_to_node name buffer_names_to_free returned_output_names update extra_output_names returned_output_names = OrderedSet mutation_real_name get name name name returned_output_names output_nodes = name_to_node name name returned_output_names is_none_layout name constant_names = name name partition_input_names name V graph constants symbol_inputs = get_graph_partition_symbol_inputs partition input_nodes partition_signature = GraphPartitionSignature symbol_inputs input_nodes output_nodes input_deallocation skip_cudagraph constant_names signatures append partition_signature unmet_output_names = partition_input_names union pyrefly ignore unsupported-operation unmet_output_names - returned_output_names signatures - clean_removed_buffer_from_partition_signatures signature GraphPartitionSignature - GraphPartitionSignature Updates partition signature removing buffers specified V graph removed_buffers See Note Removed Graph Partition Arguments input_nodes = name buffer name buffer signature input_nodes items name V graph removed_buffers input_deallocation = name val name val signature input_deallocation items name V graph removed_buffers output_nodes = node node signature output_nodes node maybe_get_name V graph removed_buffers constant_names = name name signature constant_names name V graph removed_buffers GraphPartitionSignature signature symbol_inputs input_nodes output_nodes input_deallocation signature skip_cudagraph constant_names reorder_for_minimizing_partition nodes list BaseSchedulerNode - list BaseSchedulerNode Reorder nodes minimize number partitions via bfs topological sort This optimal reordering such number partitions cannot reduced further This may sub-optimal other metrics such peak memory This does change relative orders two cudagraphable nodes nor relative order two non_cudagraphable nodes heapq node_to_indegree dict BaseSchedulerNode int = dict cudagraphable_nodes list tuple int BaseSchedulerNode = non_cudagraphable_nodes list tuple int BaseSchedulerNode = node_to_index = node idx idx node enumerate nodes insert_pending_nodes node BaseSchedulerNode - None node_with_index = node_to_index node node should_partition node heapq heappush non_cudagraphable_nodes node_with_index heapq heappush cudagraphable_nodes node_with_index update_indegree node BaseSchedulerNode - None succ_node node mpi_node succ_nodes assert node_to_indegree succ_node node_to_indegree succ_node -= node_to_indegree succ_node == insert_pending_nodes succ_node node nodes node_to_indegree node = len node mpi_node pred_nodes node_to_indegree node == insert_pending_nodes node schedule list BaseSchedulerNode = num_iters int = while num_iters len nodes non_cudagraphable_nodes cudagraphable_nodes while non_cudagraphable_nodes _ node = heapq heappop non_cudagraphable_nodes schedule append node update_indegree node while cudagraphable_nodes _ node = heapq heappop cudagraphable_nodes schedule append node update_indegree node num_iters += num_iters len nodes raise RuntimeError Failed schedule while loop ran too long when reordering minimizing num partitions schedule maybe_reorder_for_minimizing_partition nodes list BaseSchedulerNode - list BaseSchedulerNode Reorder nodes minimize number partitions only slightly increase peak memory memory estimate_peak_memory prepare_planning_info graph_outputs = OrderedSet V graph get_output_names default_peak_memory name_to_freeable_input_buf = prepare_planning_info nodes name_to_buf name_to_fused_node OrderedSet V graph graph_inputs keys graph_outputs reordered_nodes = reorder_for_minimizing_partition nodes reorder_peak_memory _ = estimate_peak_memory reordered_nodes name_to_freeable_input_buf graph_outputs here means extra peak memory budget which quite arbitrary reorder_peak_memory default_peak_memory reordered_nodes nodes reorder_for_partition_with_simple_dependency nodes list BaseSchedulerNode - list BaseSchedulerNode Reorder node should partitioned has simple dependency move partitioned node front has no dependency move partitioned node back only used OutputNode otherwise do reorder front list BaseSchedulerNode = middle list BaseSchedulerNode = back list BaseSchedulerNode = only_output_user node BaseSchedulerNode - bool buf node get_outputs use buf users isinstance use node OutputNode False True node nodes should_partition = should_partition node should_partition len node unmet_dependencies == front append node should_partition only_output_user node back append node middle append node front + middle + back graph_partition - tuple list PartitionType list GraphPartitionSignature Given list BaseSchedulerNodes split into list graph partitions compute partition input output signatures partitions list PartitionType = skip_cudagraph = True cur_partition PartitionType = skip_cudagraphs = node nodes should_partition = should_partition node should_log=True cur_partition skip_cudagraph = should_partition partitions append cur_partition skip_cudagraphs append skip_cudagraph cur_partition = skip_cudagraph = should_partition cur_partition append node cur_partition partitions append cur_partition skip_cudagraphs append skip_cudagraph signatures = get_graph_partition_signature partitions=partitions skip_cudagraphs=skip_cudagraphs compute_graph_partition_maps signatures partitions signatures codegen - None dynamo_timed Scheduler codegen _codegen_partitions torch _inductor config graph_partition _codegen nodes _codegen_partition_wrapper partition PartitionType signature GraphPartitionSignature - None Codegen partition given its inputs outputs codegen wrapper SubgraphPythonWrapperCodegen parent_wrapper_code = V graph wrapper_code graph_partition_id = next _graph_partition_counter V graph set_current_wrapper_code V graph init_wrapper_code is_subgraph=True subgraph_name=f partition_ graph_partition_id parent_wrapper_code=parent_wrapper_code partition_signatures=signature _codegen partition Note Removed Graph Partition Arguments Graph partition relies node read_writes analyze partition inputs outputs However during codegen we may decide some buffers internal kernel e g triton kernel such these buffers never actually defined This information collected during codegen recorded V graph removed_buffers So we cleanup signature write prefix i e generating call function outputs after we have codegen partition assert isinstance V graph wrapper_code SubgraphPythonWrapperCodegen signature = clean_removed_buffer_from_partition_signatures signature V graph wrapper_code partition_signatures = signature V graph wrapper_code write_prefix graph_name = V graph name partition_code _ = V graph wrapper_code generate V graph is_inference V graph wrapper_code define_subgraph_launcher_fn graph_name partition_code V graph wrapper_code codegen_partition_call graph_partition_id signature V graph wrapper_code allocated update type ignore has-type node get_name node signature output_nodes use_default_device_context partitions list PartitionType signatures list GraphPartitionSignature - contextlib AbstractContextManager None contextlib contextmanager ctx - Iterator None update_graph_partition_default_device partitions signatures default_device_context device_need_guard default_device_context type assert default_device_context index None device should have index V graph wrapper_code codegen_device_guard_enter default_device_context index try yield finally default_device_context device_need_guard default_device_context type V graph wrapper_code codegen_device_guard_exit default_device_context = None ctx update_graph_partition_default_device partitions list PartitionType signatures list GraphPartitionSignature - None Note Graph Partition Device Contexts Entering device context takes microseconds exiting device context takes microseconds If all graph partitions cudagraph-unsafe ops happen same device we can share device context len partitions == signatures skip_cudagraph If there only cudagraph partition device context should happen within cudagraph partition which would removed cudagraph get_cudagraph_partition_device partition PartitionType - torch device partition_device = partition get_device assert partition_device None partition_device all_on_target_device partition PartitionType target_device torch device - bool node partition device = node get_device device = target_device False True cudagraph_partition_device = None partition signature zip partitions signatures signature skip_cudagraph cudagraph_partition_device = get_cudagraph_partition_device partition break all partitions skip cudagraph cudagraph_partition_device None partition signature zip partitions signatures signature skip_cudagraph all_on_target_device partition cudagraph_partition_device default_device_context = cudagraph_partition_device _codegen_partitions - None Split nodes into partitions codegen each partition into separate functions This allows further applying different optimizations e g cudagraph each function partitions signatures = graph_partition len partitions msg = f cudagraph partition into len partitions partitions maybe_log_cudagraph_partition msg=msg prefix= use_default_device_context partitions signatures partition signature zip partitions signatures assert len partition = f Each partition must have least one node found len partition signature skip_cudagraph _codegen partition _codegen_partition_wrapper partition signature num_partitions = next _graph_partition_counter V graph wrapper_code set_all_partition_names num_partitions See Note Graph Partition Map CUDAGraph num_partitions assert V graph partition_maps None assert num_partitions == len V graph partition_maps f Expect num_partitions partition maps got len V graph partition_maps _codegen nodes list BaseSchedulerNode - None config check_stack_no_cycles_TESTING_ONLY torch _dynamo convert_frame stack = traceback extract_stack seen OrderedSet tuple str int &#124; None = OrderedSet frame reversed stack This where maybe_cprofile frame name == _compile_inner frame filename == torch _dynamo convert_frame __file__ break key = frame filename frame lineno assert key seen f Duplicate stack frame frame filename frame lineno did you add decorator one functions stack trace If so try using context manager instead seen add key current_device = default_device_context pyrefly ignore unbound-name default_device_context config triton autotune_at_compile_time V graph wrapper_code write_get_raw_stream_header node nodes log isEnabledFor logging DEBUG try log debug Generating code node s estimated runtime f node get_name node get_estimated_runtime except Exception log debug Generating code node s estimated runtime node get_name enter_context node device = node get_device device = current_device node is_extern node is_template flush device = current_device current_device device_need_guard current_device type V graph wrapper_code codegen_device_guard_exit current_device = device device_need_guard device type assert device index None device should have index V graph wrapper_code codegen_device_guard_enter device index current_node = node buffer_names_to_free update node last_usage node is_template prologue template_node epilogue = node get_prologue_template_epilogue list node get_nodes pyrefly ignore unbound-name get_backend device codegen_template template_node epilogue prologue node is_extern node = typing cast ExternKernelSchedulerNode node codegen_extern_call node node is_foreach node = typing cast ForeachKernelSchedulerNode node pyrefly ignore unbound-name backend_ = get_backend device codegen cuda_combined_scheduling CUDACombinedScheduling codegen simd SIMDScheduling isinstance backend_ SIMDScheduling CUDACombinedScheduling backend = backend_ raise AssertionError f type = backend codegen_combo_kernel node isinstance node FusedMixOrderReductions pyrefly ignore unbound-name get_backend device codegen_mix_order_reduction node isinstance node FusedSchedulerNode SchedulerNode pyrefly ignore unbound-name get_backend device codegen_node node assert isinstance node NopKernelSchedulerNode node mark_run pyrefly ignore unbound-name config triton debug_sync_kernel pyrefly ignore unbound-name get_backend device codegen_sync available_buffer_names update node get_buffer_names completed_operations update node get_operation_names isinstance node NopKernelSchedulerNode device = node get_device device None device type = meta get_backend device ready_to_flush flush current_device = default_device_context when default_device_context None we codegen graph partitions all nodes must same default device assert current_device None device_need_guard current_device type exit outermost CUDA device guard important nested indentation codegen-ing V graph wrapper_code codegen_device_guard_exit flush benchmark_combo_kernel node_list Sequence BaseSchedulerNode - tuple float float list Optional str Benchmark fused list nodes execution time milliseconds randomly generated inputs device = node_list get_device V graph scheduler = current_device = device assert device None backend = get_backend device backend benchmark_combo_kernel node_list speedup_by_combo_kernel nodes list BaseSchedulerNode - bool If config benchmark_fusion False always True Otherwise True fusion can brings speedup config benchmark_combo_kernel True subkernel_nodes = nodes device = subkernel_nodes get_device don t support benchmark fusion CPU right now device None device type == cpu True triton compiler errors CompilationError ms path _list = i snode enumerate subkernel_nodes node_list = snode get_nodes We can accurately benchmark kernel using atomic_add due how we generate random integer inputs _any_atomic_add node_list fusion_log debug ComboKernel benchmarking may accurate due atomic_add try ms path = benchmark_fused_nodes node_list math isinf ms fusion_log debug ComboKernel benchmark register spilling d-th subkernel i False except CompilationError e workaround triton issue https github com triton-lang triton issues Loop-carried variable str e fusion_log debug ComboKernel benchmark True because loop-carried variable True allow fusion raise ms += ms path _list append path try ms ms _clone _path _list = benchmark_combo_kernel subkernel_nodes except CompilationError e workaround triton issue https github com triton-lang triton issues Loop-carried variable str e fusion_log debug ComboKernel benchmark True because loop-carried variable True allow fusion raise small kernels very likely have speedup hard benchmark So we skip benchmarking small_kernel = ms - ms _clone ms fusion_log isEnabledFor logging DEBUG ms ms small_kernel fusion_log debug can fuse benchmark fusing causes sx speedup green_text f ms ms f fusion_log debug cannot fuse benchmark fusing causes sx slowdown red_text f ms ms f ms returned benchmark_fused_nodes discounted clone time ms - ms _clone ms small_kernel get_buffer_layout buf_name str - ir Layout buf = name_to_buf buf_name assert buf node None buf node get_layout update_zero_dim_cpu_tensor - None node nodes node is_gpu read node read_writes reads buffer = V graph name_to_buffer get read name buffer get_device_type buffer == cpu isinstance buffer layout NoneLayout MultiOutputLayout buffer get_size == V graph zero_dim_cpu_tensor_list add read name BaseScheduling noqa docstring_linter __init__ scheduler Optional Scheduler super __init__ scheduler = scheduler free_buffers_in_scheduler - None scheduler scheduler free_buffers get_backend_features device torch device - OrderedSet BackendFeature Return set codegen common BackendFeature OrderedSet can_fuse_vertical node BaseSchedulerNode node BaseSchedulerNode - bool Check whether node node can vertically fused raise NotImplementedError can_fuse_horizontal node BaseSchedulerNode node BaseSchedulerNode - bool Check whether node node can horizontally fused raise NotImplementedError can_fuse_multi_outputs_template node BaseSchedulerNode node BaseSchedulerNode - bool A Multi-Output Template referenced template node MultiOutputLayout its output buffers instances MultiOutput In context we verify whether node represents Multi-Output Template node corresponds one its outputs If so we further check backend supports fusion False fuse node BaseSchedulerNode node BaseSchedulerNode - FusedSchedulerNode Fuse two nodes node is_foreach node is_foreach ForeachKernelSchedulerNode fuse node node MixOrderReduction are_mix_order_reductions node node FusedMixOrderReductions node node FusedSchedulerNode fuse node node group_fn sizes Sequence Sequence sympy Expr - tuple tuple sympy Expr Process iteration sizes case transformation needs applied raise NotImplementedError codegen_template template_node BaseSchedulerNode epilogue_nodes Sequence BaseSchedulerNode prologue_nodes Sequence BaseSchedulerNode - Optional str Given template node generate kernel This function only available triton now If third-party backend behaves sub-class TritonScheduling can override reuse raise NotImplementedError generate_kernel_code_from_nodes nodes Sequence BaseSchedulerNode benchmark_kernel bool hint_override Optional int = None - str Generate kernel given list pre-fused nodes raise NotImplementedError codegen_node node Union FusedSchedulerNode SchedulerNode - None Generate kernel given list pre-fused nodes raise NotImplementedError codegen_mix_order_reduction node FusedMixOrderReductions - None raise NotImplementedError codegen_sync - None Generate synchronization code kernel This method depends hardware characteristics raise NotImplementedError ready_to_flush - bool Check whether backend requesting scheduler flush generated kernel If supported please False False flush - None Flush generated kernel python wrapper code source code file raise NotImplementedError benchmark_fused_nodes nodes Sequence BaseSchedulerNode - tuple float str Benchmark fused list nodes execution time milliseconds randomly generated inputs raise NotImplementedError benchmark_codegened_module module ModuleType - tuple float str Benchmark compiled module execution time milliseconds randomly generated inputs raise NotImplementedError get_fusion_pair_priority node BaseSchedulerNode node BaseSchedulerNode - int Return unsigned integer which represents priority fusion pair The smaller higher priority benchmark_combo_kernel node_list Sequence BaseSchedulerNode - tuple float float list Optional str Benchmark list nodes combine execution time memory copy time milliseconds randomly generated inputs raise NotImplementedError codegen_comment node_schedule Sequence BaseSchedulerNode kernel_name Optional str = None - None kernel_name torch _inductor debug set_kernel_post_grad_provenance_tracing debug_handle = set_kernel_post_grad_provenance_tracing node_schedule type ignore arg-type kernel_name V graph wrapper_code write_provenance_debug_handle kernel_name debug_handle