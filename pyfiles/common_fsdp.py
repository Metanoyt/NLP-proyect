mypy allow-untyped-defs Owner s oncall distributed contextlib os re sys time unittest warnings abc ABC abstractmethod collections abc Callable contextlib nullcontext copy deepcopy enum auto Enum functools wraps typing Any cast no_type_check Optional Union unittest mock torch torch distributed dist torch nn nn torch nn functional F torch distributed _composable checkpoint torch distributed device_mesh DeviceMesh torch distributed fsdp CPUOffload fully_shard FullyShardedDataParallel FSDP torch distributed fsdp _common_utils TrainingState torch distributed fsdp _fully_shard _fsdp_param_group FSDPParamGroup RegisterPostBackwardFunction torch distributed fsdp _init_utils NO_RESHARD_AFTER_FORWARD_STRATEGIES torch distributed fsdp fully_sharded_data_parallel BackwardPrefetch MixedPrecision ShardingStrategy torch distributed fsdp sharded_grad_scaler ShardedGradScaler torch distributed fsdp wrap always_wrap_policy ModuleWrapPolicy wrap torch distributed tensor distribute_tensor DTensor Shard torch distributed tensor parallel ColwiseParallel parallelize_module RowwiseParallel SequenceParallel torch nn TransformerDecoderLayer TransformerEncoderLayer torch nn parallel distributed DistributedDataParallel DDP torch testing _internal common_distributed MultiProcessTestCase MultiThreadedTestCase run_subtests TEST_SKIPS torch testing _internal common_utils FILE_SCHEMA get_cycles_per_ms set_rng_seed TEST_CUDA TEST_HPU TEST_XPU torch utils _triton has_triton DEVICE_COUNT = default TEST_CUDA DEVICE_TYPE = cuda DISTRIBUTED_BACKEND = nccl DEVICE_COUNT = torch cuda device_count TEST_HPU DEVICE_TYPE = hpu DISTRIBUTED_BACKEND = hccl TEST_XPU DEVICE_TYPE = xpu DISTRIBUTED_BACKEND = xccl DEVICE_COUNT = torch xpu device_count DEVICE_TYPE = cpu DISTRIBUTED_BACKEND = gloo DEVICE_COUNT = FSDPInitMode Enum No FSDP wrapping NO_FSDP = auto FSDP recursive wrapping RECURSIVE = auto TODO FSDP non-recursive wrapping NONRECURSIVE = auto DEVICEInitMode Enum Move model DEVICE before passing FSDP constructor DEVICE_BEFORE = auto Move model DEVICE after passing FSDP constructor DEVICE_AFTER = auto Keep CPU DEVICE_NEVER = auto FSDPTestModel nn Module ABC This defines interface expected all models used commonly FSDP unit tests abstractmethod get_input device - tuple torch Tensor Returns input model tuple abstractmethod get_loss input output - torch Tensor Returns loss given input output abstractmethod run_backward loss - None Runs backward pass e g including ` ` loss backward ` ` staticmethod abstractmethod init args Any kwargs Any - nn Module Initializes instance model _assert_module_states model nn Module process_group dist ProcessGroup assert_fn Callable All-gathers module states across ranks calls ` ` assert_fn ` ` each pair corresponding states rank nonzero rank For example ` ` assert_fn ` ` ` ` assertEqual ` ` then checks all module states equal across ranks Include names debugging convenience named_module_states = param_name param detach cpu param_name param model named_parameters named_module_states += buffer_name buffer detach cpu buffer_name buffer model named_buffers world_size = dist get_world_size process_group olist = None _ range world_size dist all_gather_object olist named_module_states group=process_group rank _states = olist assert rank _states None mypy state olist assert state None mypy _ p _ p zip rank _states state strict=True assert_fn p p get_devtype torch device DEVICE_TYPE _zero_model model nn Module zero_buffers bool = False summon_full=True Zeros parameters optionally buffers ` ` model ` ` place ctx = FSDP summon_full_params model summon_full nullcontext ctx param model parameters torch no_grad param zero_ zero_buffers buffer model buffers torch no_grad buffer zero_ _get_state_dict model cpu_offload=False half=False cpu_offload model = model DEVICE_TYPE half model half model state_dict subtest_name test_name_mapping args _ join test_name_mapping str s s None none s args _broadcast_state_dict rank state_dict For non-FSDP roots some parts model state rank may CPU so we move everything CPU avoid issues like https github com pytorch pytorch issues param_name param state_dict items param device = torch device cpu state_dict param_name = param cpu olist = state_dict rank == None dist broadcast_object_list olist state_dict = cast dict str torch Tensor olist Ensure state DEVICE param_name state_dict keys state_dict param_name = state_dict param_name DEVICE_TYPE state_dict get_full_params model nn Module recurse bool = True Returns full unsharded parameters ` ` model ` ` Any FSDP-managed parameters offloaded CPU moved GPU returned list Args recurse bool If ` ` False ` ` only unshards parameters immediate ` ` model ` ` ` ` True ` ` recurses through module hierarchy rooted ` ` model ` ` FSDP summon_full_params model recurse=recurse deepcopy list model parameters _move_to_device model nn Module move_to_device bool model DEVICE_TYPE move_to_device model _maybe_wrap_fsdp model nn Module wrap_fsdp bool args kwargs model wrap_fsdp FSDP model args kwargs DummyProcessGroup __init__ rank int size int _rank = rank _size = size rank - int _rank size - int _size allreduce args kwargs dist_wait = mock Mock get_future future torch futures Future = torch futures Future future set_result future dist_wait get_future = get_future dist_wait TransformerWithSharedParams FSDPTestModel __init__ group dist ProcessGroup device_init_mode DEVICEInitMode add_bn bool deterministic bool super __init__ rank = group rank world_size = group size deterministic torch manual_seed d_vocab = d_model = embed_tokens = nn Embedding d_vocab d_model transformer = nn Transformer d_model=d_model num_encoder_layers= num_decoder_layers= dim_feedforward= dropout= output_proj = nn Linear d_model d_vocab share embedding output projection weights output_proj weight = embed_tokens weight register_buffer vocab_bias embed_tokens weight new_ones d_model register_buffer long_buffer torch zeros_like vocab_bias dtype=torch long type ignore arg-type type ignore arg-type bs = bn = torch nn BatchNorm d bs add_bn torch nn Identity device_init_mode == DEVICEInitMode DEVICE_BEFORE = DEVICE_TYPE deterministic eval get_input device torch manual_seed + rank keep everything deterministic src = torch arange device=device view bs T x B tgt = torch arange bs device=device view bs T x B src tgt forward src_ids tgt_ids src = embed_tokens src_ids src = src + vocab_bias + long_buffer type_as src type ignore operator tgt = embed_tokens tgt_ids tgt = bn tgt x = transformer src tgt output_proj x get_loss input output _ tgt = input nn functional cross_entropy output view - output size - tgt view - reduction= sum run_backward loss loss backward staticmethod init group dist ProcessGroup fsdp_init_mode FSDPInitMode device_init_mode DEVICEInitMode fsdp_kwargs Optional dict str Any = None deterministic bool = False add_bn bool = True - Union nn Module FSDP Initializes ` TransformerWithSharedParams ` instance Args fsdp_init_mode FSDPInitMode If ` ` NO_FSDP ` ` then does wrap any modules FSDP If ` ` RECURSIVE ` ` then wraps top-level FSDP By default top-level FSDP uses ` ` ModuleWrapPolicy ` ` encoder decoder layers different auto wrap policy may specified via ` ` fsdp_kwargs ` ` device_init_mode DEVICEInitMode Determines model movement DEVICE fsdp_kwargs Optional Dict str Any Optional keyword arguments forwarded FSDP constructor deterministic bool Whether make model deterministic across constructions add_bn bool Whether include batch norm model fsdp_kwargs None fsdp_kwargs = fsdp_init_mode == FSDPInitMode NO_FSDP isinstance group tuple pg = group pg = group TransformerWithSharedParams pg device_init_mode add_bn deterministic fsdp_init_mode == FSDPInitMode RECURSIVE Default ` ModuleWrapPolicy ` auto_wrap_policy fsdp_kwargs auto_wrap_policy = ModuleWrapPolicy TransformerEncoderLayer TransformerDecoderLayer auto_wrap_policy = fsdp_kwargs pop auto_wrap_policy sharding_strategy fsdp_kwargs fsdp_kwargs sharding_strategy ShardingStrategy HYBRID_SHARD ShardingStrategy _HYBRID_SHARD_ZERO isinstance group tuple fsdp_pg = None fsdp_pg = group isinstance group tuple tformer_pg = group tformer_pg = group m = TransformerWithSharedParams tformer_pg device_init_mode add_bn deterministic fsdp_model = FSDP m fsdp_pg auto_wrap_policy=auto_wrap_policy fsdp_kwargs device_init_mode == DEVICEInitMode DEVICE_AFTER fsdp_model = fsdp_model DEVICE_TYPE fsdp_model raise ValueError f Unsupported FSDP init mode fsdp_init_mode get_ignored_modules transformer NestedWrappedModule FSDPTestModel __init__ group dist ProcessGroup wrap_fsdp bool device_init_mode DEVICEInitMode deterministic bool fsdp_kwargs super __init__ rank = group rank world_size = group size move_to_device = device_init_mode == DEVICEInitMode DEVICE_BEFORE _maybe_wrap layer wrap_fsdp FSDP layer group fsdp_kwargs layer deterministic torch manual_seed module = nn Sequential _move_to_device nn Linear move_to_device _maybe_wrap nn Sequential _maybe_wrap _move_to_device nn Linear move_to_device _move_to_device nn Linear move_to_device _maybe_wrap _move_to_device nn Linear move_to_device _move_to_device nn Linear move_to_device get_input device torch manual_seed + rank keep everything deterministic torch rand device=device forward x module x get_loss input output loss = output sum loss run_backward loss loss backward staticmethod init group dist ProcessGroup fsdp_init_mode FSDPInitMode device_init_mode DEVICEInitMode fsdp_kwargs Optional dict str Any = None deterministic bool = False - nn Module Initializes ` NestedWrappedModule ` instance Args fsdp_init_mode FSDPInitMode If ` ` NO_FSDP ` ` then does wrap any modules FSDP If ` ` RECURSIVE ` ` then wraps some nested modules FSDP top-level module The model may later wrapped top-level FSDP external method desired device_init_mode DEVICEInitMode Determines model movement DEVICE fsdp_kwargs Optional Dict str Any Optional keyword arguments forwarded FSDP constructor deterministic bool Whether make model deterministic across constructions fsdp_kwargs None fsdp_kwargs = fsdp_init_mode == FSDPInitMode NO_FSDP NestedWrappedModule group wrap_fsdp=False device_init_mode=device_init_mode deterministic=deterministic fsdp_init_mode == FSDPInitMode RECURSIVE Does wrap top-level FSDP fsdp_model = NestedWrappedModule group wrap_fsdp=True device_init_mode=device_init_mode deterministic=deterministic fsdp_kwargs device_init_mode == DEVICEInitMode DEVICE_AFTER fsdp_model = fsdp_model DEVICE_TYPE fsdp_model raise ValueError f Unsupported FSDP init mode fsdp_init_mode AlwaysWrapNestedWrappedModule NestedWrappedModule staticmethod init group dist ProcessGroup fsdp_init_mode FSDPInitMode device_init_mode DEVICEInitMode fsdp_kwargs Optional dict str Any = None deterministic bool = False Initializes ` NestedWrappedModule ` instance unlike meth ` NestedWrappedModule init ` ` ` RECURSIVE ` ` init mode wraps top-level FSDP ` ` always_wrap_policy ` ` auto wrap policy model = super AlwaysWrapNestedWrappedModule AlwaysWrapNestedWrappedModule init group=group fsdp_init_mode=FSDPInitMode NO_FSDP device_init_mode=device_init_mode fsdp_kwargs=fsdp_kwargs deterministic=deterministic fsdp_init_mode == FSDPInitMode NO_FSDP model fsdp_init_mode == FSDPInitMode RECURSIVE fsdp_kwargs = fsdp_kwargs fsdp_model = FSDP model auto_wrap_policy=always_wrap_policy fsdp_kwargs device_init_mode == DEVICEInitMode DEVICE_AFTER fsdp_model = fsdp_model DEVICE_TYPE fsdp_model NonUniformReqGradNWM NestedWrappedModule __init__ group dist ProcessGroup wrap_fsdp bool device_init_mode DEVICEInitMode deterministic bool fsdp_kwargs super NestedWrappedModule __init__ This ` __init__ ` only differs ` NestedWrappedModule __init__ ` last two ` nn Linear ` layers FSDP wrapped ` nn Sequential ` container This arrangement results all elements last two parameters residing single rank Freezing all parameters except those two allows us verify ` ShardedGradScaler ` accommodates situations where some ranks have no non-zero sized parameter shards rank = group rank world_size = group size move_to_device = device_init_mode == DEVICEInitMode DEVICE_BEFORE _maybe_wrap layer wrap_fsdp FSDP layer group fsdp_kwargs layer deterministic torch manual_seed module = nn Sequential _move_to_device nn Linear move_to_device _maybe_wrap nn Sequential _maybe_wrap _move_to_device nn Linear move_to_device _move_to_device nn Linear move_to_device _maybe_wrap nn Sequential _move_to_device nn Linear move_to_device _move_to_device nn Linear move_to_device staticmethod _set_nonuniform_req_grad model req_grad_mask - None n p model named_parameters re match req_grad_mask n p requires_grad_ False staticmethod init group dist ProcessGroup fsdp_init_mode FSDPInitMode device_init_mode DEVICEInitMode fsdp_kwargs Optional dict str Any = None deterministic bool = False Initializes ` NestedWrappedModule ` instance unlike meth ` NestedWrappedModule init ` wraps second ` torch nn Sequential ` container enable desired non-uniform ` ` requires_grad ` ` ` ` use_orig_params=True ` ` tests For both ` ` RECURSIVE ` ` ` ` NO_FSDP ` ` init modes freezes all parameters except last two validate ` ` ShardedGradScaler ` ` support ranks no non-zero sized local shards FSDP ` ` use_orig_params=True ` ` mode The parameters should remain unfrozen ` module ` The regex pattern below matches relevant parameter names both without interstitial FSDP module indicator ` _fsdp_wrapped_module ` present req_grad_pattern = re compile r module\ \ fsdp_init_mode == FSDPInitMode NO_FSDP ddp_model = NonUniformReqGradNWM group wrap_fsdp=False device_init_mode=device_init_mode deterministic=deterministic NonUniformReqGradNWM _set_nonuniform_req_grad ddp_model req_grad_pattern ddp_model fsdp_init_mode == FSDPInitMode RECURSIVE fsdp_kwargs None fsdp_kwargs = fsdp_model = NonUniformReqGradNWM group wrap_fsdp=True device_init_mode=device_init_mode deterministic=deterministic fsdp_kwargs device_init_mode == DEVICEInitMode DEVICE_AFTER fsdp_model = fsdp_model DEVICE_TYPE NonUniformReqGradNWM _set_nonuniform_req_grad fsdp_model req_grad_pattern fsdp_model raise ValueError f Unsupported FSDP init mode fsdp_init_mode ModuleWithDelay FSDPTestModel This wraps ` FSDPTestModel ` optionally add delay after computing loss before gradient reduction __init__ module nn Module delay_after_loss_ms int delay_before_reduction_ms int super __init__ delay_after_loss_ms = delay_after_loss_ms delay_before_reduction_ms = delay_before_reduction_ms module = module get_input device module get_input device type ignore operator forward x module x get_loss input output loss = module get_loss input output type ignore operator delay_after_loss_ms TEST_HPU TEST_XPU time sleep delay_after_loss_ms TEST_CUDA torch cuda _sleep int delay_after_loss_ms get_cycles_per_ms loss run_backward loss orig_reduce_scatter = torch distributed reduce_scatter_tensor _delayed_reduce_scatter args kwargs delay_before_reduction_ms TEST_CUDA torch cuda _sleep int delay_before_reduction_ms get_cycles_per_ms TEST_HPU TEST_XPU time sleep delay_before_reduction_ms orig_reduce_scatter args kwargs mock patch torch distributed reduce_scatter_tensor _delayed_reduce_scatter module run_backward loss type ignore operator staticmethod init module_class type FSDPTestModel model_args Any delay_after_loss_ms int delay_before_reduction_ms int model_kwargs Any Args module_class Type FSDPTestModel Wrapped module which add delays model_args Positional arguments forwarded ` ` module_class ` ` ` ` init ` ` delay_after_loss_ms int Delay after computing loss before optimizer step ms delay_before_reduction_ms int Delay before reduce-scattering gradients ms model_kwargs Keyword arguments forwarded ` ` module_class ` ` ` ` init ` ` ModuleWithDelay module_class init model_args model_kwargs delay_after_loss_ms delay_before_reduction_ms NestedWrappedModuleWithDelay ModuleWithDelay staticmethod init type ignore override group dist ProcessGroup fsdp_init_mode FSDPInitMode device_init_mode DEVICEInitMode = DEVICEInitMode DEVICE_AFTER fsdp_kwargs Optional dict str Any = None deterministic bool = False delay_after_loss_ms int = delay_before_reduction_ms int = ModuleWithDelay init NestedWrappedModule group=group fsdp_init_mode=fsdp_init_mode device_init_mode=device_init_mode fsdp_kwargs=fsdp_kwargs deterministic=deterministic delay_after_loss_ms=delay_after_loss_ms delay_before_reduction_ms=delay_before_reduction_ms DummyDDP nn Module __init__ module super __init__ module = module forward args kwargs module args kwargs MixtureOfExperts NestedWrappedModule __init__ group dist ProcessGroup wrap_fsdp bool device_init_mode DEVICEInitMode delay_before_free_ms int deterministic bool fsdp_kwargs super __init__ group=group wrap_fsdp=wrap_fsdp device_init_mode=device_init_mode deterministic=deterministic group = group delay_before_free_ms = delay_before_free_ms wrap_fsdp = wrap_fsdp move_to_device = device_init_mode == DEVICEInitMode DEVICE_BEFORE deterministic Give each rank different expert parameters torch manual_seed + rank d_expert = d_shared = d_input = expert = _move_to_device nn Linear d_expert d_shared move_to_device num_expert_params = sum p numel p expert parameters p expert parameters p expert = True type ignore attr-defined deterministic Keep all other parameters same across ranks torch manual_seed shared = _move_to_device nn Linear d_shared d_expert move_to_device wrap_fsdp we create process group size expert params expert_group = torch distributed new_group group rank world size means no shard expert = FSDP expert expert_group fsdp_kwargs type ignore assignment shared = FSDP shared group fsdp_kwargs type ignore assignment module = nn Sequential _move_to_device nn Linear d_input d_shared move_to_device shared expert _move_to_device nn Linear d_shared d_input move_to_device forward x delay_before_free_ms expert = module isinstance expert FSDP orig_reshard = torch distributed fsdp _runtime_utils _reshard _delayed_reshard args kwargs TEST_CUDA torch cuda _sleep int delay_before_free_ms get_cycles_per_ms TEST_HPU TEST_XPU time sleep delay_before_free_ms orig_reshard args kwargs This patch covers any ` torch _reshard ` uses mock patch torch distributed fsdp _runtime_utils _reshard _delayed_reshard module x module x run_backward loss loss backward Manually reduce gradients wrapped FullyShardedDataParallel wrap_fsdp torch no_grad p parameters hasattr p expert continue these params don t need grad reduction p grad None p grad div_ world_size torch distributed all_reduce p grad group=self group staticmethod init group dist ProcessGroup fsdp_init_mode FSDPInitMode device_init_mode DEVICEInitMode fsdp_kwargs Optional dict str Any = None deterministic bool = False delay_before_free_ms int = Initializes ` MixtureOfExperts ` instance Args fsdp_init_mode FSDPInitMode If ` ` NO_FSDP ` ` then does wrap any modules FSDP If ` ` RECURSIVE ` ` then wraps some nested modules FSDP including expert shared layers top-level module The model may later wrapped top-level FSDP external method desired device_init_mode DEVICEInitMode Determines model movement DEVICE fsdp_kwargs Optional Dict str Any Optional keyword arguments forwarded FSDP constructor deterministic bool Whether make model deterministic across constructions delay_before_free_ms int Delay before resharding expert parameters forward pass ms fsdp_kwargs None fsdp_kwargs = fsdp_init_mode == FSDPInitMode NO_FSDP MixtureOfExperts group wrap_fsdp=False device_init_mode=device_init_mode delay_before_free_ms=delay_before_free_ms deterministic=deterministic fsdp_init_mode == FSDPInitMode RECURSIVE Does wrap top-level FSDP fsdp_model = MixtureOfExperts group wrap_fsdp=True device_init_mode=device_init_mode delay_before_free_ms=delay_before_free_ms deterministic=deterministic fsdp_kwargs device_init_mode == DEVICEInitMode DEVICE_AFTER fsdp_model = fsdp_model DEVICE_TYPE fsdp_model raise ValueError f Unsupported FSDP init mode fsdp_init_mode MLP nn Module __init__ dim int device Optional torch device = None bias bool = True with_buffer bool = False dim_multiplier int = super __init__ in_proj = nn Linear dim dim_multiplier dim device=device bias=bias out_proj = nn Linear dim_multiplier dim dim device=device bias=bias with_buffer register_buffer buffer torch randn dim device=device buffer = None forward x torch Tensor - torch Tensor z = in_proj x z = F relu z z = out_proj z z = F relu z buffer None z = z + buffer z reset_parameters buffer None torch nn init normal_ buffer MLPStack nn Sequential __init__ mlp_dim int with_seq_parallel bool = False modules list nn Module = Use multiplier exercise uneven case MLP mlp_dim dim_multiplier= MLP mlp_dim MLP mlp_dim dim_multiplier= with_seq_parallel modules append nn LayerNorm mlp_dim bias=False super __init__ modules with_seq_parallel = with_seq_parallel parallelize tp_mesh DeviceMesh dp_mesh DeviceMesh use_activation_checkpointing bool fsdp_kwargs - MLPStack parallelize_plan = Pass ` use_local_output=False ` keep DTensor preserve uneven activation dims in_proj ColwiseParallel use_local_output=False out_proj RowwiseParallel use_local_output=False in_proj ColwiseParallel use_local_output=False out_proj RowwiseParallel use_local_output=False in_proj ColwiseParallel use_local_output=False out_proj RowwiseParallel output_layouts=Shard with_seq_parallel RowwiseParallel with_seq_parallel parallelize_plan = SequenceParallel sequence_dim= parallelize_module device_mesh=tp_mesh parallelize_plan=parallelize_plan module isinstance module nn LayerNorm continue use_activation_checkpointing checkpoint module fully_shard module mesh=dp_mesh fsdp_kwargs fully_shard mesh=dp_mesh fsdp_kwargs DoubleLinear nn Module This can used returning multiple outputs module ` ` use_second_linear=True ` ` having unused module ` ` False ` ` __init__ dim int use_second_linear bool = True super __init__ lin = nn Linear dim dim lin = nn Linear dim dim relu = nn ReLU use_second_linear = use_second_linear forward x torch Tensor - Union tuple torch Tensor torch Tensor torch Tensor use_second_linear relu lin x relu lin x relu lin x NOTE For these patch methods we want safety under multi-threading e g when using multi-threaded process group then we want barrier immediately after reading original value ensure all threads see same original value barrier immediately before restoring original value ensure all threads use patched value inside context contextlib contextmanager patch_all_gather new_all_gather_into_tensor Callable orig_all_gather = dist all_gather_into_tensor dist barrier dist all_gather_into_tensor = new_all_gather_into_tensor try yield finally dist barrier dist all_gather_into_tensor = orig_all_gather contextlib contextmanager patch_foreach_all_gather new_foreach_all_gather Callable orig_foreach_all_gather = torch distributed fsdp _fully_shard _fsdp_param_group foreach_all_gather dist barrier torch distributed fsdp _fully_shard _fsdp_param_group foreach_all_gather = new_foreach_all_gather try yield finally dist barrier torch distributed fsdp _fully_shard _fsdp_param_group foreach_all_gather = orig_foreach_all_gather contextlib contextmanager patch_foreach_reduce new_foreach_reduce Callable orig_foreach_foreach_reduce = torch distributed fsdp _fully_shard _fsdp_param_group foreach_reduce dist barrier torch distributed fsdp _fully_shard _fsdp_param_group foreach_reduce = new_foreach_reduce try yield finally dist barrier torch distributed fsdp _fully_shard _fsdp_param_group foreach_reduce = orig_foreach_foreach_reduce contextlib contextmanager patch_reduce_scatter new_reduce_scatter_tensor Callable orig_reduce_scatter = dist reduce_scatter_tensor dist barrier dist reduce_scatter_tensor = new_reduce_scatter_tensor try yield finally dist barrier dist reduce_scatter_tensor = orig_reduce_scatter contextlib contextmanager patch_all_reduce new_all_reduce Callable orig_all_reduce = dist all_reduce dist barrier dist all_reduce = new_all_reduce try yield finally dist barrier dist all_reduce = orig_all_reduce no_type_check contextlib contextmanager patch_unshard new_unshard Callable orig_unshard = FSDPParamGroup unshard dist barrier FSDPParamGroup unshard = new_unshard try yield finally dist barrier FSDPParamGroup unshard = orig_unshard no_type_check contextlib contextmanager patch_reshard new_reshard Callable orig_reshard = FSDPParamGroup reshard dist barrier FSDPParamGroup reshard = new_reshard try yield finally dist barrier FSDPParamGroup reshard = orig_reshard no_type_check contextlib contextmanager patch_post_backward new_post_backward Callable orig_post_backward = FSDPParamGroup post_backward dist barrier FSDPParamGroup post_backward = new_post_backward try yield finally dist barrier FSDPParamGroup post_backward = orig_post_backward no_type_check contextlib contextmanager patch_register_post_backward_hook_backward new_backward Callable orig_backward = RegisterPostBackwardFunction backward dist barrier RegisterPostBackwardFunction backward = new_backward try yield finally dist barrier RegisterPostBackwardFunction backward = orig_backward reduce_scatter_with_assert cls orig_reduce_scatter Callable assert_fn Callable ` assert_fn output Tensor ` args Any kwargs Any len args output = args output kwargs output = kwargs output raise AssertionError f Cannot get reduce-scatter output from\nargs args \nkwargs kwargs assert_fn output orig_reduce_scatter args kwargs check_sharded_parity cls unit test replicated_module nn Module sharded_module nn Module prefixes_to_ignore tuple str = replicated_name replicated_param sharded_name sharded_param zip replicated_module named_parameters sharded_module named_parameters strict=True clean_sharded_name = sharded_name prefix prefixes_to_ignore clean_sharded_name = clean_sharded_name replace prefix cls assertEqual replicated_name clean_sharded_name cls assertIsInstance sharded_param DTensor assert isinstance sharded_param DTensor mypy mesh placements = sharded_param device_mesh sharded_param placements tuple placements == Shard Shard raise AssertionError FSDP s Shard Shard layout differs distribute_tensor so we cannot check equality using sharded_ref_param = distribute_tensor replicated_param mesh placements cls assertEqual sharded_param to_local sharded_ref_param to_local replicated_param grad None cls assertIsNone sharded_param grad continue cls assertIsNotNone sharded_param grad sharded_ref_grad = distribute_tensor replicated_param grad mesh placements cls assertIsInstance sharded_param grad DTensor assert isinstance sharded_param grad DTensor mypy cls assertEqual sharded_param grad to_local sharded_ref_grad to_local unittest skipIf TEST_XPU not-support-multithread FSDPTestMultiThread MultiThreadedTestCase property world_size DEVICE_COUNT setUp super setUp _spawn_threads run_subtests args kwargs run_subtests args kwargs perThreadSetUp torch _dynamo reset perThreadTearDown torch _dynamo reset FSDPTest MultiProcessTestCase setUp super setUp Set TORCH_NCCL_DESYNC_DEBUG= disable NCCL ` workCleanupLoop ` which can cause unit test flakiness https github com pytorch pytorch issues os environ TORCH_NCCL_DESYNC_DEBUG = _spawn_processes property world_size DEVICE_COUNT property process_group dist distributed_c d _get_default_group property destroy_pg_upon_exit - bool Overriding base test do auto destroy PG upon exit False property init_method f FILE_SCHEMA file_name _check_cpu_offload fsdp_model cpu_offload assertEqual cpu_offload fsdp_model cpu_offload _check_backward_prefetch fsdp_model backward_prefetch assertEqual backward_prefetch fsdp_model backward_prefetch _check_forward_prefetch fsdp_model forward_prefetch assertEqual forward_prefetch fsdp_model forward_prefetch run_subtests args kwargs run_subtests args kwargs classmethod _run cls rank test_name file_name pipe kwargs type ignore override = cls test_name rank = rank file_name = file_name fake_pg = kwargs get fake_pg False print f dist init r= rank world= world_size torch accelerator device_count world_size sys exit TEST_SKIPS f multi-gpu- world_size exit_code Specify gloo backend make init_process_group succeed Actual tests will skipped there no enough GPUs try fake_pg store = torch testing _internal distributed fake_pg FakeStore dist init_process_group backend= fake world_size=self world_size rank=rank store=store dist init_process_group init_method=self init_method backend=DISTRIBUTED_BACKEND world_size=int world_size rank=self rank except RuntimeError e recompile e args sys exit TEST_SKIPS backend_unavailable exit_code raise device_ids = None device_id = rank DEVICE_COUNT TEST_CUDA TEST_XPU torch accelerator set_device_index device_id device_ids = device_id Execute barrier prior running test ensure every process has finished initialization following test immediately exiting due skip doesn t cause flakiness dist barrier device_ids=device_ids torch _dynamo reset set_rng_seed run_test test_name pipe torch _dynamo reset dist barrier device_ids=device_ids dist destroy_process_group _train_for_several_steps model nn Module num_steps int autocast bool lr float = fsdp_cpu_offload Optional CPUOffload = None save_model bool = False mixed_precision Optional MixedPrecision = None enable_sharded_grad_scaler bool = False use_pure_fp bool = False sharded_grad_scaler_kwargs Optional dict str Any = None cpu_offload_params = fsdp_cpu_offload fsdp_cpu_offload offload_params model_device = next model parameters device sharded_grad_scaler_kwargs None sharded_grad_scaler_kwargs = sharded_grad_scaler = ShardedGradScaler enabled=enable_sharded_grad_scaler sharded_grad_scaler_kwargs use SGD momentum instead Adam since Adam scale invariant makes bad tests optim = torch optim SGD model parameters lr=lr momentum= _ range num_steps optim zero_grad torch amp autocast DEVICE_TYPE enabled=autocast Inputs always cuda regardless cpu offloading model device input = model module get_input torch device DEVICE_TYPE type ignore operator union-attr use_pure_fp mixed_precision isinstance model FSDP isinstance input torch Tensor input = input half input = tuple x half x input output = model input Post-forward CPU offloading model param should CPU cpu_offload_params isinstance model FSDP If resharding after forward parameters still exposed unsharded views into GPU flat parameter model sharding_strategy NO_RESHARD_AFTER_FORWARD_STRATEGIES p model parameters Params should always CPU assertEqual p device torch device cpu loss = model module get_loss input output model_device type ignore operator union-attr loss = sharded_grad_scaler scale loss mixed_precision use_pure_fp assert loss dtype == torch float loss data type should float original \ parameter data type float use_pure_fp assertEqual loss dtype torch float FSDP loss fp DDP AMP loss fp isinstance model FSDP assert mixed_precision None mypy assertEqual loss dtype mixed_precision param_dtype assertEqual loss dtype torch float model module run_backward loss type ignore operator union-attr Post-backward CPU offloading model params should CPU cpu_offload_params isinstance model FSDP p model parameters Params should always CPU assertEqual p device torch device cpu Unscale gradients step sharded_grad_scaler step optim Update scale factor sharded_grad_scaler update save_model simulate save + load save_model state_dict = k v clone k v model state_dict items Zero params save load state_dict did work properly would break parity test DDP _zero_model model model load_state_dict state_dict isinstance model FSDP model _assert_state TrainingState IDLE loss detach type ignore possibly-undefined _test_fsdp_parity model_class type FSDPTestModel fsdp_init_mode FSDPInitMode device_init_mode DEVICEInitMode ref_init_fn Optional Callable = None num_iters int = save_model bool = True cpu_offload CPUOffload = CPUOffload backward_prefetch Optional BackwardPrefetch = None sharding_strategy Optional ShardingStrategy = None mixed_precision Optional MixedPrecision = None forward_prefetch bool = False use_orig_params bool = False enable_sharded_grad_scaler bool = False use_pure_fp bool = False init_kwargs Optional dict str Any = None sharded_grad_scaler_kwargs Optional dict str Any = None fsdp_kwargs Tests FSDP training against reference which defaults DDP may customized ` ` ref_init_fn ` ` Args model_class Type FSDPTestModel A model inherits ` ` FSDPTestModel ` ` which defines expected interface fsdp_init_mode FSDPInitMode The mode initialize FSDP-wrapped model This should ` ` NO_FSDP ` ` ref_init_fn Optional Callable A callable invoke wraps non-wrapped model construct reference model where wrapper should provide data parallel semantics If ` ` None ` ` then callable defaults DDP constructor assert fsdp_init_mode = FSDPInitMode NO_FSDP Expects FSDP init mode wraps FSDP init_kwargs None init_kwargs = lr = e- rank = process_group rank Establish reference behavior DDP model = model_class init process_group FSDPInitMode NO_FSDP DEVICEInitMode DEVICE_BEFORE deterministic=True init_kwargs ref_init_fn None TEST_HPU ref_model = DDP model device_ids= DEVICE_TYPE output_device=DEVICE_TYPE ref_model = DDP model device_ids= rank output_device=rank ref_model = ref_init_fn model use_pure_fp ref_model = ref_model half ref_loss = _train_for_several_steps ref_model num_iters autocast=mixed_precision None lr=lr fsdp_cpu_offload=cpu_offload mixed_precision=mixed_precision enable_sharded_grad_scaler=enable_sharded_grad_scaler use_pure_fp =use_pure_fp sharded_grad_scaler_kwargs=sharded_grad_scaler_kwargs ddp_params = list ref_model parameters Check against FSDP behavior fsdp_kwargs update cpu_offload cpu_offload backward_prefetch backward_prefetch sharding_strategy sharding_strategy mixed_precision mixed_precision forward_prefetch forward_prefetch use_orig_params use_orig_params try fsdp_model = model_class init process_group fsdp_init_mode device_init_mode fsdp_kwargs deterministic=True init_kwargs except Exception e raise ValueError f Initializing model_class raised error str e e isinstance fsdp_model FSDP Enforce we wrap top-level FSDP since we comparing assuming data parallel reference some test models may do so their ` init ` method fsdp_model = FSDP fsdp_model process_group fsdp_kwargs use_pure_fp Change model parameter dtype after FSDP initialization fsdp_model = fsdp_model half device_init_mode == DEVICEInitMode DEVICE_AFTER fsdp_model = fsdp_model DEVICE_TYPE offload_params = cpu_offload None cpu_offload offload_params Offloading parameters ` DEVICE_AFTER ` should raise error during lazy initialization due parameter devices being CPU otherwise all parameter devices should CPU expects_device_error = offload_params device_init_mode == DEVICEInitMode DEVICE_AFTER expects_cpu_device = offload_params device_init_mode = DEVICEInitMode DEVICE_AFTER expects_cpu_device cpu_device = torch device cpu param fsdp_model parameters assertEqual param device cpu_device context = assertRaisesRegex RuntimeError An FSDP-managed module parameter CPU offloading enabled f has parameters DEVICE_TYPE expects_device_error nullcontext context fsdp_loss = _train_for_several_steps fsdp_model num_iters autocast=False lr=lr fsdp_cpu_offload=cpu_offload save_model=save_model mixed_precision=mixed_precision enable_sharded_grad_scaler=enable_sharded_grad_scaler use_pure_fp =use_pure_fp sharded_grad_scaler_kwargs=sharded_grad_scaler_kwargs No need check parameter loss parity expecting error expects_device_error Check parameter devices CPU offloading CPU before calling ` get_full_params ` which will cast parameters FP offload_params cpu_device = torch device cpu param fsdp_model parameters assertEqual param device cpu_device fsdp_loss = fsdp_loss DEVICE_TYPE fsdp_unsharded_params = get_full_params fsdp_model Do check dtype since reference DDP loss may same dtype FSDP loss case mixed precision torch testing assert_close ref_loss fsdp_loss check_dtype=False Do check parameter parity using mixed precision since DDP parameters FP ` half ` while FSDP parameters FP ` summon_full_params ` DDP runs optimizer FP while FSDP runs FP TODO Disable checking parameters pure FP due floating point inaccuracy Note means backward pass checked https github com pytorch pytorch issues mixed_precision None use_pure_fp assertEqual ddp_params fsdp_unsharded_params exact_device=True msg= FSDP did match DDP compiled_fsdp_test compile_compute_on_module Optional type = None fully_shard_with_compiled_compute args kwargs torch distributed fsdp fully_shard args kwargs type ignore operator compile_compute_on_module None isinstance args compile_compute_on_module args compile FullyShardMode Enum EAGER = auto COMPILED_COMPUTE = auto decorator func wraps func wrapper args kwargs original_fully_shard Any = torch distributed fsdp fully_shard mode FullyShardMode mode = FullyShardMode EAGER has_triton warnings warn Inductor GPU needs Triton recent GPU arch stacklevel= continue barrier ensure thread reading same value original_skip_fsdp_hooks = torch _dynamo config skip_fsdp_hooks original_compile_threads = torch _inductor config compile_threads torch distributed barrier mode == FullyShardMode EAGER fully_shard_patch = original_fully_shard mode == FullyShardMode COMPILED_COMPUTE torch _dynamo config skip_fsdp_hooks = True torch _inductor config compile_threads = fully_shard_patch = fully_shard_with_compiled_compute type ignore assignment raise NotImplementedError f Need implement FullyShardMode= mode fully_shard imported global through ` fully_shard ` func __globals__ original_fully_shard __name__ = fully_shard_patch func args kwargs other threads use patched func before thread restores torch distributed barrier func __globals__ original_fully_shard __name__ = original_fully_shard torch _dynamo config skip_fsdp_hooks = original_skip_fsdp_hooks torch _inductor config compile_threads = original_compile_threads wrapper decorator SkipModule nn Module __init__ - None super __init__ lin = nn Linear bias=False forward x lin x NestedLinear nn Module __init__ fsdp_wrap super __init__ fsdp_wrap nested_linear = wrap nn Linear bias=False DEVICE_TYPE nested_linear = nn Linear bias=False DEVICE_TYPE forward x nested_linear x SkipModel nn Module __init__ double_nest super __init__ linear = nn Linear bias=False DEVICE_TYPE linear_skip = SkipModule DEVICE_TYPE nested_linear = wrap NestedLinear fsdp_wrap=double_nest device_id=DEVICE_TYPE forward x x = linear x x = linear_skip x x = nested_linear x x