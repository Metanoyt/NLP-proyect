Owner s oncall distributed sys torch torch distributed dist torch distributed fsdp FullyShardedDataParallel FSDP torch nn Linear Module torch nn parallel DistributedDataParallel torch optim SGD torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp FSDPTest get_devtype get_full_params torch testing _internal common_utils run_tests TEST_WITH_DEV_DBG_ASAN device_type = torch device get_devtype dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit Model Module __init__ wrap_fsdp super __init__ keep everything deterministic model initialization torch manual_seed inner = Linear wrap_fsdp inner = FSDP inner outer = Linear forward x Forward twice i = inner x j = inner x outer i + j TestMultiForward FSDPTest _dist_train wrap_fsdp keep everything deterministic input data torch manual_seed model = Model wrap_fsdp device_type type wrap_fsdp model = FSDP model device_id=device_type type model = DistributedDataParallel model device_ids= device_type type optim = SGD model parameters lr= in_data = torch rand device_type type in_data requires_grad = True _ range out = model in_data out sum backward optim step optim zero_grad wrap_fsdp get_full_params model list model parameters skip_if_lt_x_gpu test_multi_forward DDP ddp_state = _dist_train wrap_fsdp=False FSDP fsdp_state = _dist_train wrap_fsdp=True assertEqual ddp_state fsdp_state devices = cpu hpu xpu instantiate_device_type_tests TestMultiForward globals only_for=devices allow_xpu=True __name__ == __main__ run_tests