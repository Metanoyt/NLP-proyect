argparse math torch torch nn nn torch nn functional F torch func functional_call grad_and_value stack_module_state vmap Adapted http willwhitney com parallel-training-jax html which tutorial Model Ensembling JAX Will Whitney The original code comes following citation misc Whitney Parallelizing author = William F Whitney title = Parallelizing neural networks one GPU JAX year = url = http willwhitney com parallel-training-jax html GOAL Demonstrate possible use eager-mode vmap parallelize training over models parser = argparse ArgumentParser description= Functorch Ensembled Models parser add_argument -- device type=str default= cpu help= CPU GPU ID process default cpu args = parser parse_args DEVICE = args device Step Make some spirals make_spirals n_samples noise_std= rotations= ts = torch linspace n_samples device=DEVICE rs = ts thetas = rs rotations math pi signs = torch randint n_samples device=DEVICE - labels = signs torch long DEVICE xs = rs signs torch cos thetas + torch randn n_samples device=DEVICE noise_std ys = rs signs torch sin thetas + torch randn n_samples device=DEVICE noise_std points = torch stack xs ys dim= points labels points labels = make_spirals noise_std= Step Define two-layer MLP loss function MLPClassifier nn Module __init__ hidden_dim= n_classes= super __init__ hidden_dim = hidden_dim n_classes = n_classes fc = nn Linear hidden_dim fc = nn Linear hidden_dim n_classes forward x x = fc x x = F relu x x = fc x x = F log_softmax x - x loss_fn = nn NLLLoss model = MLPClassifier DEVICE train_step_fn weights batch targets lr= compute_loss weights batch targets output = functional_call model weights batch loss = loss_fn output targets loss grad_weights loss = grad_and_value compute_loss weights batch targets NB PyTorch missing functional optimizer API possibly coming soon so we going re-implement SGD here new_weights = torch no_grad key grad_weights new_weights key = weights key - grad_weights key lr loss new_weights Step Let s verify actually trains We should see loss decrease step global weights i range loss weights = train_step_fn dict model named_parameters points labels i == print loss step Step We re ready multiple models Let s define init_fn given number models returns us all weights init_fn num_models models = MLPClassifier DEVICE _ range num_models params _ = stack_module_state models params Step Now can we try multiple models same time The answer yes ` loss ` -tuple we can see value keeps decreasing step parallel_train_step_fn = vmap train_step_fn in_dims= None None batched_weights = init_fn num_models= i range loss batched_weights = parallel_train_step_fn batched_weights points labels i == print loss step Step Now flaw step we training same exact data This can lead all models ensemble overfitting same way The solution http willwhitney com parallel-training-jax html applies randomly subset data way models do receive exactly same data each training step Because goal doc show we can use eager-mode vmap achieve similar things JAX rest left exercise reader In conclusion achieve what http willwhitney com parallel-training-jax html does we used following additional items PyTorch does have NN module functional API turns module into state state_less_fn pair Functional optimizers A functional grad API effectively wraps autograd grad Composability between functional grad API torch vmap