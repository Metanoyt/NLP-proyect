mypy ignore-errors A thin pytorch numpy compat layer Things imported here have numpy-compatible signatures operate pytorch tensors Contents module ends up main namespace via _funcs py where type annotations used conjunction normalizer decorator __future__ annotations builtins itertools operator typing Optional TYPE_CHECKING torch _dtypes_impl _util TYPE_CHECKING collections abc Sequence _normalizations ArrayLike ArrayLikeOrScalar CastingModes DTypeLike NDArray NotImplementedType OutArray copy ArrayLike order NotImplementedType = K subok NotImplementedType = False clone copyto dst NDArray src ArrayLike casting Optional CastingModes = same_kind where NotImplementedType = None src = _util typecast_tensors src dst dtype casting=casting dst copy_ src atleast_ d arys ArrayLike res = torch atleast_ d arys isinstance res tuple list res res atleast_ d arys ArrayLike res = torch atleast_ d arys isinstance res tuple list res res atleast_ d arys ArrayLike res = torch atleast_ d arys isinstance res tuple list res res _concat_check tup dtype out tup == raise ValueError need least one array concatenate Check inputs concatenate et al out None dtype None mimic numpy raise TypeError concatenate only takes ` out ` ` dtype ` argument both provided _concat_cast_helper tensors out=None dtype=None casting= same_kind Figure out dtypes cast necessary out None dtype None figure out type inputs outputs out_dtype = out dtype torch_dtype dtype None dtype out_dtype = _dtypes_impl result_type_impl tensors cast input arrays necessary do broadcast them against ` out ` tensors = _util typecast_tensors tensors out_dtype casting tensors _concatenate tensors axis= out=None dtype=None casting Optional CastingModes = same_kind pure torch implementation used below cov corrcoef below tensors axis = _util axis_none_flatten tensors axis=axis tensors = _concat_cast_helper tensors out dtype casting torch cat tensors axis concatenate ar_tuple Sequence ArrayLike axis= out Optional OutArray = None dtype Optional DTypeLike = None casting Optional CastingModes = same_kind _concat_check ar_tuple dtype out=out result = _concatenate ar_tuple axis=axis out=out dtype=dtype casting=casting result vstack tup Sequence ArrayLike dtype Optional DTypeLike = None casting Optional CastingModes = same_kind _concat_check tup dtype out=None tensors = _concat_cast_helper tup dtype=dtype casting=casting torch vstack tensors row_stack = vstack hstack tup Sequence ArrayLike dtype Optional DTypeLike = None casting Optional CastingModes = same_kind _concat_check tup dtype out=None tensors = _concat_cast_helper tup dtype=dtype casting=casting torch hstack tensors dstack tup Sequence ArrayLike dtype Optional DTypeLike = None casting Optional CastingModes = same_kind XXX numpy dstack does have dtype casting keywords h v stack do Hence add them here consistency _concat_check tup dtype out=None tensors = _concat_cast_helper tup dtype=dtype casting=casting torch dstack tensors column_stack tup Sequence ArrayLike dtype Optional DTypeLike = None casting Optional CastingModes = same_kind XXX numpy column_stack does have dtype casting keywords row_stack does because row_stack alias vstack really Hence add these keywords here consistency _concat_check tup dtype out=None tensors = _concat_cast_helper tup dtype=dtype casting=casting torch column_stack tensors stack arrays Sequence ArrayLike axis= out Optional OutArray = None dtype Optional DTypeLike = None casting Optional CastingModes = same_kind _concat_check arrays dtype out=out tensors = _concat_cast_helper arrays dtype=dtype casting=casting result_ndim = tensors ndim + axis = _util normalize_axis_index axis result_ndim torch stack tensors axis=axis append arr ArrayLike values ArrayLike axis=None axis None arr ndim = arr = arr flatten values = values flatten axis = arr ndim - _concatenate arr values axis=axis ### split ### _split_helper tensor indices_or_sections axis strict=False isinstance indices_or_sections int _split_helper_int tensor indices_or_sections axis strict isinstance indices_or_sections list tuple NB drop split= only applies split_helper_int _split_helper_list tensor list indices_or_sections axis raise TypeError split_helper type indices_or_sections _split_helper_int tensor indices_or_sections axis strict=False isinstance indices_or_sections int raise NotImplementedError split indices_or_sections axis = _util normalize_axis_index axis tensor ndim numpy l n chunks size l n + rest sized l n l n = tensor shape axis indices_or_sections n = raise ValueError l n == num sz = n l n lst = sz num strict raise ValueError array split does result equal division num sz = l n l n + lst = sz num lst += sz - n - num torch split tensor lst axis _split_helper_list tensor indices_or_sections axis isinstance indices_or_sections list raise NotImplementedError split indices_or_sections list numpy expects indices while torch expects lengths sections also numpy appends zero-size arrays indices above shape axis lst = x x indices_or_sections x = tensor shape axis num_extra = len indices_or_sections - len lst lst append tensor shape axis lst = lst + - b b zip lst lst - lst += num_extra torch split tensor lst axis array_split ary ArrayLike indices_or_sections axis= _split_helper ary indices_or_sections axis split ary ArrayLike indices_or_sections axis= _split_helper ary indices_or_sections axis strict=True hsplit ary ArrayLike indices_or_sections ary ndim == raise ValueError hsplit only works arrays more dimensions axis = ary ndim _split_helper ary indices_or_sections axis strict=True vsplit ary ArrayLike indices_or_sections ary ndim raise ValueError vsplit only works arrays more dimensions _split_helper ary indices_or_sections strict=True dsplit ary ArrayLike indices_or_sections ary ndim raise ValueError dsplit only works arrays more dimensions _split_helper ary indices_or_sections strict=True kron ArrayLike b ArrayLike torch kron b vander x ArrayLike N=None increasing=False torch vander x N increasing ### linspace geomspace logspace arange ### linspace start ArrayLike stop ArrayLike num= endpoint=True retstep=False dtype Optional DTypeLike = None axis= axis = retstep endpoint raise NotImplementedError dtype None dtype = _dtypes_impl default_dtypes float_dtype XXX raises TypeError start stop scalars torch linspace start stop num dtype=dtype geomspace start ArrayLike stop ArrayLike num= endpoint=True dtype Optional DTypeLike = None axis= axis = endpoint raise NotImplementedError base = torch pow stop start num - logbase = torch log base torch logspace torch log start logbase torch log stop logbase num base=base logspace start stop num= endpoint=True base= dtype Optional DTypeLike = None axis= axis = endpoint raise NotImplementedError torch logspace start stop num base=base dtype=dtype arange start Optional ArrayLikeOrScalar = None stop Optional ArrayLikeOrScalar = None step Optional ArrayLikeOrScalar = dtype Optional DTypeLike = None like NotImplementedType = None step == raise ZeroDivisionError stop None start None raise TypeError stop None XXX breaks start passed kwarg arange start= should raise no stop doesn t start stop = start start None start = dtype result dtype None dtype = _dtypes_impl default_dtypes float_dtype any _dtypes_impl is_float_or_fp_tensor x x start stop step _dtypes_impl default_dtypes int_dtype work_dtype = torch float dtype is_complex dtype RuntimeError lt_cpu implemented ComplexFloat Fall back eager any _dtypes_impl is_complex_or_complex_tensor x x start stop step raise NotImplementedError step start stop step start stop empty range torch empty dtype=dtype result = torch arange start stop step dtype=work_dtype result = _util cast_if_needed result dtype result ### zeros ones empty full ### empty shape dtype Optional DTypeLike = None order NotImplementedType = C like NotImplementedType = None dtype None dtype = _dtypes_impl default_dtypes float_dtype torch empty shape dtype=dtype NB _like functions deliberately deviate numpy has subok=True default we set subok=False raise anything empty_like prototype ArrayLike dtype Optional DTypeLike = None order NotImplementedType = K subok NotImplementedType = False shape=None result = torch empty_like prototype dtype=dtype shape None result = result reshape shape result full shape fill_value ArrayLike dtype Optional DTypeLike = None order NotImplementedType = C like NotImplementedType = None isinstance shape int shape = shape dtype None dtype = fill_value dtype isinstance shape tuple list shape = shape torch full shape fill_value dtype=dtype full_like ArrayLike fill_value dtype Optional DTypeLike = None order NotImplementedType = K subok NotImplementedType = False shape=None XXX fill_value broadcasts result = torch full_like fill_value dtype=dtype shape None result = result reshape shape result ones shape dtype Optional DTypeLike = None order NotImplementedType = C like NotImplementedType = None dtype None dtype = _dtypes_impl default_dtypes float_dtype torch ones shape dtype=dtype ones_like ArrayLike dtype Optional DTypeLike = None order NotImplementedType = K subok NotImplementedType = False shape=None result = torch ones_like dtype=dtype shape None result = result reshape shape result zeros shape dtype Optional DTypeLike = None order NotImplementedType = C like NotImplementedType = None dtype None dtype = _dtypes_impl default_dtypes float_dtype torch zeros shape dtype=dtype zeros_like ArrayLike dtype Optional DTypeLike = None order NotImplementedType = K subok NotImplementedType = False shape=None result = torch zeros_like dtype=dtype shape None result = result reshape shape result ### cov corrcoef ### _xy_helper_corrcoef x_tensor y_tensor=None rowvar=True Prepare inputs cov corrcoef https github com numpy numpy blob v numpy lib function_base py#L y_tensor None make sure x y least D ndim_extra = - x_tensor ndim ndim_extra x_tensor = x_tensor view ndim_extra + x_tensor shape rowvar x_tensor shape = x_tensor = x_tensor mT x_tensor = x_tensor clone ndim_extra = - y_tensor ndim ndim_extra y_tensor = y_tensor view ndim_extra + y_tensor shape rowvar y_tensor shape = y_tensor = y_tensor mT y_tensor = y_tensor clone x_tensor = _concatenate x_tensor y_tensor axis= x_tensor corrcoef x ArrayLike y Optional ArrayLike = None rowvar=True bias=None ddof=None dtype Optional DTypeLike = None bias None ddof None deprecated NumPy raise NotImplementedError xy_tensor = _xy_helper_corrcoef x y rowvar is_half = xy_tensor dtype == torch float xy_tensor is_cpu is_half work around torch s addmm_impl_cpu_ implemented Half dtype = torch float xy_tensor = _util cast_if_needed xy_tensor dtype result = torch corrcoef xy_tensor is_half result = result torch float result cov m ArrayLike y Optional ArrayLike = None rowvar=True bias=False ddof=None fweights Optional ArrayLike = None aweights Optional ArrayLike = None dtype Optional DTypeLike = None m = _xy_helper_corrcoef m y rowvar ddof None ddof = bias == is_half = m dtype == torch float m is_cpu is_half work around torch s addmm_impl_cpu_ implemented Half dtype = torch float m = _util cast_if_needed m dtype result = torch cov m correction=ddof aweights=aweights fweights=fweights is_half result = result torch float result _conv_corr_impl v mode dt = _dtypes_impl result_type_impl v = _util cast_if_needed dt v = _util cast_if_needed v dt padding = v shape - mode == full mode padding == same v shape == UserWarning Using padding= same even kernel lengths odd dilation may require zero-padded copy input created Triggered internally pytorch aten src ATen native Convolution cpp raise NotImplementedError mode= same even-length weights NumPy only accepts D arrays PyTorch requires D inputs D weights aa = None vv = v None None result = torch nn functional conv d aa vv padding=padding torch returns D result numpy returns D array result convolve ArrayLike v ArrayLike mode= full NumPy v longer than arrays swapped before computation shape v shape v = v flip weights since numpy does torch does v = torch flip v _conv_corr_impl v mode correlate ArrayLike v ArrayLike mode= valid v = torch conj_physical v _conv_corr_impl v mode ### logic element selection ### bincount x ArrayLike weights Optional ArrayLike = None minlength= x numel == edge case allowed numpy x = x new_empty dtype=int int_dtype = _dtypes_impl default_dtypes int_dtype x = _util typecast_tensors x int_dtype casting= safe torch bincount x weights minlength where condition ArrayLike x Optional ArrayLikeOrScalar = None y Optional ArrayLikeOrScalar = None x None = y None raise ValueError either both neither x y should given condition dtype = torch bool condition = condition torch bool x None y None result = torch where condition result = torch where condition x y result ###### module-level queries object properties ndim ArrayLike ndim shape ArrayLike tuple shape size ArrayLike axis=None axis None numel shape axis ###### shape manipulations indexing expand_dims ArrayLike axis shape = _util expand_shape shape axis view shape never copies flip m ArrayLike axis=None XXX semantic difference np flip returns view torch flip copies axis None axis = tuple range m ndim axis = _util normalize_axis_tuple axis m ndim torch flip m axis flipud m ArrayLike torch flipud m fliplr m ArrayLike torch fliplr m rot m ArrayLike k= axes= axes = _util normalize_axis_tuple axes m ndim torch rot m k axes ### broadcasting indices ### broadcast_to array ArrayLike shape subok NotImplementedType = False torch broadcast_to array size=shape This function tuples tuples so we just reuse torch broadcast_shapes broadcast_arrays args ArrayLike subok NotImplementedType = False torch broadcast_tensors args meshgrid xi ArrayLike copy=True sparse=False indexing= xy ndim = len xi indexing xy ij raise ValueError Valid values ` indexing ` xy ij s = ndim output = x reshape s i + - + s i + i x enumerate xi indexing == xy ndim switch first second axis output = output reshape - + s output = output reshape - + s sparse Return full N-D matrix only -D vector output = torch broadcast_tensors output copy output = x clone x output list output match numpy list indices dimensions dtype Optional DTypeLike = int sparse=False https github com numpy numpy blob v numpy core numeric py#L -L dimensions = tuple dimensions N = len dimensions shape = N sparse res = res = torch empty N + dimensions dtype=dtype i dim enumerate dimensions idx = torch arange dim dtype=dtype reshape shape i + dim + shape i + sparse res = res + idx res i = idx res ### tri -something ### tril m ArrayLike k= torch tril m k triu m ArrayLike k= torch triu m k tril_indices n k= m=None m None m = n torch tril_indices n m offset=k triu_indices n k= m=None m None m = n torch triu_indices n m offset=k tril_indices_from arr ArrayLike k= arr ndim = raise ValueError input array must -d Return tensor rather than tuple avoid graphbreak torch tril_indices arr shape arr shape offset=k triu_indices_from arr ArrayLike k= arr ndim = raise ValueError input array must -d Return tensor rather than tuple avoid graphbreak torch triu_indices arr shape arr shape offset=k tri N M=None k= dtype Optional DTypeLike = None like NotImplementedType = None M None M = N tensor = torch ones N M dtype=dtype torch tril tensor diagonal=k ### equality equivalence allclose ### isclose ArrayLike b ArrayLike rtol= e- atol= e- equal_nan=False dtype = _dtypes_impl result_type_impl b = _util cast_if_needed dtype b = _util cast_if_needed b dtype torch isclose b rtol=rtol atol=atol equal_nan=equal_nan allclose ArrayLike b ArrayLike rtol= e- atol= e- equal_nan=False dtype = _dtypes_impl result_type_impl b = _util cast_if_needed dtype b = _util cast_if_needed b dtype torch allclose b rtol=rtol atol=atol equal_nan=equal_nan _tensor_equal equal_nan=False Implementation array_equal array_equiv shape = shape False cond = == equal_nan cond = cond &#124; torch isnan torch isnan cond all item array_equal ArrayLike ArrayLike equal_nan=False _tensor_equal equal_nan=equal_nan array_equiv ArrayLike ArrayLike almost same array_equal _equiv tries broadcast _equal does try _t _t = torch broadcast_tensors except RuntimeError failed broadcast = equivalent False _tensor_equal _t _t nan_to_num x ArrayLike copy NotImplementedType = True nan= posinf=None neginf=None work around RuntimeError nan_to_num implemented ComplexDouble x is_complex re = torch nan_to_num x real nan=nan posinf=posinf neginf=neginf im = torch nan_to_num x imag nan=nan posinf=posinf neginf=neginf re + j im torch nan_to_num x nan=nan posinf=posinf neginf=neginf ### put take_along_axis ### take ArrayLike indices ArrayLike axis=None out Optional OutArray = None mode NotImplementedType = raise axis = _util axis_none_flatten axis=axis axis = _util normalize_axis_index axis ndim idx = slice None axis + indices result = idx result take_along_axis arr ArrayLike indices ArrayLike axis arr axis = _util axis_none_flatten arr axis=axis axis = _util normalize_axis_index axis arr ndim torch take_along_dim arr indices axis put NDArray indices ArrayLike values ArrayLike mode NotImplementedType = raise v = values type dtype If indices larger than v expand v least size indices Any unnecessary trailing elements then trimmed indices numel v numel ratio = indices numel + v numel - v numel v = v unsqueeze expand ratio + v shape Trim unnecessary elements regardless v expanded Note np put trims v match indices default too indices numel v numel v = v flatten v = v indices numel put_ indices v None put_along_axis arr ArrayLike indices ArrayLike values ArrayLike axis arr axis = _util axis_none_flatten arr axis=axis axis = _util normalize_axis_index axis arr ndim indices values = torch broadcast_tensors indices values values = _util cast_if_needed values arr dtype result = torch scatter arr axis indices values arr copy_ result reshape arr shape None choose ArrayLike choices Sequence ArrayLike out Optional OutArray = None mode NotImplementedType = raise First broadcast elements ` choices ` choices = torch stack torch broadcast_tensors choices Use analog ` gather choices ` which broadcasts ` choices ` vs ` ` taken https github com pytorch pytorch issues #issuecomment- idx_list = torch arange dim view i + dim + choices ndim - i - i dim enumerate choices shape idx_list = choices tuple idx_list squeeze ### unique et al ### unique ar ArrayLike return_index NotImplementedType = False return_inverse=False return_counts=False axis=None equal_nan NotImplementedType = True ar axis = _util axis_none_flatten ar axis=axis axis = _util normalize_axis_index axis ar ndim result = torch unique ar return_inverse=return_inverse return_counts=return_counts dim=axis result nonzero ArrayLike torch nonzero as_tuple=True argwhere ArrayLike torch argwhere flatnonzero ArrayLike torch flatten nonzero as_tuple=True clip ArrayLike min Optional ArrayLike = None max Optional ArrayLike = None out Optional OutArray = None torch clamp min max repeat ArrayLike repeats ArrayLikeOrScalar axis=None torch repeat_interleave repeats axis tile A ArrayLike reps isinstance reps int reps = reps torch tile A reps resize ArrayLike new_shape=None implementation vendored https github com numpy numpy blob v numpy core fromnumeric py#L -L new_shape None isinstance new_shape int new_shape = new_shape = flatten new_size = dim_length new_shape new_size = dim_length dim_length raise ValueError all elements ` new_shape ` must non-negative numel == new_size == First case must zero fill The second would have repeats == torch zeros new_shape dtype=a dtype repeats = - -new_size numel ceil division = concatenate repeats new_size reshape new_shape ### diag et al ### diagonal ArrayLike offset= axis = axis = axis = _util normalize_axis_index axis ndim axis = _util normalize_axis_index axis ndim torch diagonal offset axis axis trace ArrayLike offset= axis = axis = dtype Optional DTypeLike = None out Optional OutArray = None result = torch diagonal offset dim =axis dim =axis sum - dtype=dtype result eye N M=None k= dtype Optional DTypeLike = None order NotImplementedType = C like NotImplementedType = None dtype None dtype = _dtypes_impl default_dtypes float_dtype M None M = N z = torch zeros N M dtype=dtype z diagonal k fill_ z identity n dtype Optional DTypeLike = None like NotImplementedType = None torch eye n dtype=dtype diag v ArrayLike k= torch diag v k diagflat v ArrayLike k= torch diagflat v k diag_indices n ndim= idx = torch arange n idx ndim diag_indices_from arr ArrayLike arr ndim = raise ValueError input array must least -d For more than d= strided formula only valid arrays all dimensions equal so we check first s = arr shape s = s - raise ValueError All dimensions input must equal length diag_indices s arr ndim fill_diagonal ArrayLike val ArrayLike wrap=False ndim raise ValueError array must least -d val numel == wrap fill_diagonal_ val val ndim == val = val unsqueeze torch Tensor fill_diagonal_ only accepts scalars If size val too large then val trimmed ndim == tall = shape shape wrap does nothing wide matrices wrap tall Never wraps diag = diagonal diag copy_ val diag numel wraps tall leaving one empty line between diagonals max_ min_ = shape idx = torch arange max_ - max_ min_ + mod = idx min_ div = idx min_ div min_ + + mod mod = val idx numel idx = diag_indices_from shape = n n n idx = val shape vdot ArrayLike b ArrayLike torch only accepts D arrays numpy flattens torch requires matching dtype while numpy casts t_a t_b = torch atleast_ d b t_a ndim t_a = t_a flatten t_b ndim t_b = t_b flatten dtype = _dtypes_impl result_type_impl t_a t_b is_half = dtype == torch float t_a is_cpu t_b is_cpu is_bool = dtype == torch bool work around torch s dot implemented Half Bool is_half dtype = torch float is_bool dtype = torch uint t_a = _util cast_if_needed t_a dtype t_b = _util cast_if_needed t_b dtype result = torch vdot t_a t_b is_half result = result torch float is_bool result = result torch bool result tensordot ArrayLike b ArrayLike axes= isinstance axes list tuple axes = ax isinstance ax int ax ax axes target_dtype = _dtypes_impl result_type_impl b = _util cast_if_needed target_dtype b = _util cast_if_needed b target_dtype torch tensordot b dims=axes dot ArrayLike b ArrayLike out Optional OutArray = None dtype = _dtypes_impl result_type_impl b is_bool = dtype == torch bool is_bool dtype = torch uint = _util cast_if_needed dtype b = _util cast_if_needed b dtype ndim == b ndim == result = b result = torch matmul b is_bool result = result torch bool result inner ArrayLike b ArrayLike dtype = _dtypes_impl result_type_impl b is_half = dtype == torch float is_cpu b is_cpu is_bool = dtype == torch bool is_half work around torch s addmm_impl_cpu_ implemented Half dtype = torch float is_bool dtype = torch uint = _util cast_if_needed dtype b = _util cast_if_needed b dtype result = torch inner b is_half result = result torch float is_bool result = result torch bool result outer ArrayLike b ArrayLike out Optional OutArray = None torch outer b cross ArrayLike b ArrayLike axisa=- axisb=- axisc=- axis=None implementation vendored https github com numpy numpy blob v numpy core numeric py#L -L axis None axisa axisb axisc = axis Check axisa axisb within bounds axisa = _util normalize_axis_index axisa ndim axisb = _util normalize_axis_index axisb b ndim Move working axis end shape = torch moveaxis axisa - b = torch moveaxis b axisb - msg = incompatible dimensions cross product\n dimension must shape - b shape - raise ValueError msg Create output array shape = broadcast_shapes shape b shape shape - == b shape - == shape += Check axisc within bounds axisc = _util normalize_axis_index axisc len shape dtype = _dtypes_impl result_type_impl b cp = torch empty shape dtype=dtype recast arrays dtype = _util cast_if_needed dtype b = _util cast_if_needed b dtype create local aliases readability = = shape - == = b = b b = b b shape - == b = b cp ndim = cp shape - == cp = cp cp = cp cp = cp shape - == b shape - == b - b cp = b - b cp assert b shape - == cp = b - = cp = - b = cp = b - b cp = b cp = -a b cp = b - b assert shape - == b shape - == cp = b - b cp = b - b cp = b - b assert b shape - == cp = -a b cp = b cp = b - b torch moveaxis cp - axisc einsum operands out=None dtype=None order= K casting= safe optimize=False Have manually normalize operands kwargs following NumPy signature We have local avoid polluting global space will then exported funcs py _ndarray ndarray _normalizations maybe_copy_to normalize_array_like normalize_casting normalize_dtype wrap_tensors dtype = normalize_dtype dtype casting = normalize_casting casting out None isinstance out ndarray raise TypeError out must array order = K raise NotImplementedError order parameter supported parse arrays normalize them sublist_format = isinstance operands str sublist_format op str op str sublistout format normalize every other argument - sublistout given length operands even we pick odd-numbered elements which arrays - sublistout given length operands odd we peel off last one pick odd-numbered elements which arrays Without - we would have picked sublistout too array_operands = operands - ij- arrays format subscripts array_operands = operands operands tensors = normalize_array_like op op array_operands target_dtype = _dtypes_impl result_type_impl tensors dtype None dtype work around bmm implemented Half etc is_half = target_dtype == torch float all t is_cpu t tensors is_half target_dtype = torch float is_short_int = target_dtype torch uint torch int torch int torch int is_short_int target_dtype = torch int tensors = _util typecast_tensors tensors target_dtype casting torch backends opt_einsum try set global state handle optimize= argument restore exit opt_einsum is_available old_strategy = torch backends opt_einsum strategy old_enabled = torch backends opt_einsum enabled torch einsum calls opt_einsum contract_path which runs into https github com dgasmith opt_einsum issues strategy= True False optimize True optimize = auto optimize False torch backends opt_einsum enabled = False torch backends opt_einsum strategy = optimize sublist_format recombine operands sublists = operands has_sublistout = len operands == has_sublistout sublistout = operands - operands = list itertools chain from_iterable zip tensors sublists has_sublistout operands append sublistout result = torch einsum operands result = torch einsum subscripts tensors finally opt_einsum is_available torch backends opt_einsum strategy = old_strategy torch backends opt_einsum enabled = old_enabled result = maybe_copy_to out result wrap_tensors result ### sort partition ### _sort_helper tensor axis kind order tensor dtype is_complex raise NotImplementedError f sorting tensor dtype supported tensor axis = _util axis_none_flatten tensor axis=axis axis = _util normalize_axis_index axis tensor ndim stable = kind == stable tensor axis stable sort ArrayLike axis=- kind=None order NotImplementedType = None ` order ` keyword arg only relevant structured dtypes so supported here axis stable = _sort_helper axis kind order result = torch sort dim=axis stable=stable result values argsort ArrayLike axis=- kind=None order NotImplementedType = None axis stable = _sort_helper axis kind order torch argsort dim=axis stable=stable searchsorted ArrayLike v ArrayLike side= left sorter Optional ArrayLike = None dtype is_complex raise NotImplementedError f searchsorted dtype= dtype torch searchsorted v side=side sorter=sorter ### swap move roll axis ### moveaxis ArrayLike source destination source = _util normalize_axis_tuple source ndim source destination = _util normalize_axis_tuple destination ndim destination torch moveaxis source destination swapaxes ArrayLike axis axis axis = _util normalize_axis_index axis ndim axis = _util normalize_axis_index axis ndim torch swapaxes axis axis rollaxis ArrayLike axis start= Straight vendor https github com numpy numpy blob v numpy core numeric py#L Also note function NumPy mostly retained backwards compat https stackoverflow com questions reason-why-numpy-rollaxis-is-so-confusing so let s touch unless hard pressed n = ndim axis = _util normalize_axis_index axis n start start += n msg = s arg requires d = s d d passed = start n + raise _util AxisError msg start -n start n + start axis start s been removed start -= axis == start numpy returns view here we try returning tensor itself tensor axes = list range n axes remove axis axes insert start axis view axes roll ArrayLike shift axis=None axis None axis = _util normalize_axis_tuple axis ndim allow_duplicate=True isinstance shift tuple shift = shift len axis torch roll shift axis ### shape manipulations ### squeeze ArrayLike axis=None axis == result = axis None result = squeeze isinstance axis tuple result = ax axis result = squeeze ax result = squeeze axis result reshape ArrayLike newshape order NotImplementedType = C sh = numpy allows both reshape sh reshape sh newshape = newshape len newshape == newshape reshape newshape NB cannot use torch reshape newshape above because Pdb torch reshape torch as_tensor TypeError reshape argument shape position must tuple SymInts int transpose ArrayLike axes=None numpy allows both transpose sh transpose sh also older code uses axes being list axes None None axes = tuple reversed range ndim len axes == axes = axes permute axes ravel ArrayLike order NotImplementedType = C torch flatten diff ArrayLike n= axis=- prepend Optional ArrayLike = None append Optional ArrayLike = None axis = _util normalize_axis_index axis ndim n raise ValueError f order must non-negative got n n == match numpy input immediately prepend None shape = list shape shape axis = prepend shape axis prepend ndim prepend = torch broadcast_to prepend shape append None shape = list shape shape axis = append shape axis append ndim append = torch broadcast_to append shape torch diff n axis=axis prepend=prepend append=append ### math functions ### angle z ArrayLike deg=False result = torch angle z deg result = result torch pi result sinc x ArrayLike torch sinc x NB have normalize varargs manually gradient f ArrayLike varargs axis=None edge_order= N = f ndim number dimensions varargs = _util ndarrays_to_tensors varargs axis None axes = tuple range N axes = _util normalize_axis_tuple axis N len_axes = len axes n = len varargs n == no spacing argument - use all axes dx = len_axes n == _dtypes_impl is_scalar varargs varargs ndim == single scalar D tensor all axes np ndim varargs == dx = varargs len_axes n == len_axes scalar d array each axis dx = list varargs i distances enumerate dx distances = torch as_tensor distances distances ndim == continue distances ndim = raise ValueError distances must either scalars d len distances = f shape axes i raise ValueError when d distances must match length corresponding dimension distances dtype is_floating_point distances dtype is_complex distances = distances double diffx = torch diff distances distances constant reduce scalar case since brings consistent speedup diffx == diffx all diffx = diffx dx i = diffx raise TypeError invalid number arguments edge_order raise ValueError edge_order greater than supported use central differences interior one-sided differences endpoints This preserves second order-accuracy over full domain outvals = create slice objects --- initially all slice = slice None N slice = slice None N slice = slice None N slice = slice None N otype = f dtype _dtypes_impl python_type_for_torch otype int bool Convert floating point First check f numpy integer type so convert f float avoid modular arithmetic when computing changes f f = f double otype = torch float axis ax_dx zip axes dx f shape axis edge_order + raise ValueError Shape array too small calculate numerical gradient least edge_order + elements required result allocation out = torch empty_like f dtype=otype spacing current axis NB np ndim ax_dx == uniform_spacing = _dtypes_impl is_scalar ax_dx ax_dx ndim == Numerical differentiation nd order interior slice axis = slice - slice axis = slice None - slice axis = slice - slice axis = slice None uniform_spacing out tuple slice = f tuple slice - f tuple slice ax_dx dx = ax_dx - dx = ax_dx = - dx dx dx + dx b = dx - dx dx dx c = dx dx dx + dx fix shape broadcasting shape = N shape axis = - = reshape shape b = b reshape shape c = c reshape shape D equivalent -- out - = f - + b f - + c f out tuple slice = f tuple slice + b f tuple slice + c f tuple slice Numerical differentiation st order edges edge_order == slice axis = slice axis = slice axis = dx_ = ax_dx uniform_spacing ax_dx D equivalent -- out = f - f x - x out tuple slice = f tuple slice - f tuple slice dx_ slice axis = - slice axis = - slice axis = - dx_n = ax_dx uniform_spacing ax_dx - D equivalent -- out - = f - - f - x - - x - out tuple slice = f tuple slice - f tuple slice dx_n Numerical differentiation nd order edges slice axis = slice axis = slice axis = slice axis = uniform_spacing = - ax_dx b = ax_dx c = - ax_dx dx = ax_dx dx = ax_dx = - dx + dx dx dx + dx b = dx + dx dx dx c = -dx dx dx + dx D equivalent -- out = f + b f + c f out tuple slice = f tuple slice + b f tuple slice + c f tuple slice slice axis = - slice axis = - slice axis = - slice axis = - uniform_spacing = ax_dx b = - ax_dx c = ax_dx dx = ax_dx - dx = ax_dx - = dx dx dx + dx b = - dx + dx dx dx c = dx + dx dx dx + dx D equivalent -- out - = f - + b f - + c f - out tuple slice = f tuple slice + b f tuple slice + c f tuple slice outvals append out reset slice object dimension slice axis = slice None slice axis = slice None slice axis = slice None slice axis = slice None len_axes == outvals outvals ### Type shape etc queries ### round ArrayLike decimals= out Optional OutArray = None is_floating_point result = torch round decimals=decimals is_complex RuntimeError round_cpu implemented ComplexFloat result = torch complex torch round real decimals=decimals torch round imag decimals=decimals RuntimeError round_cpu implemented int result = result around = round round_ = round real_if_close ArrayLike tol= torch is_complex tol Undocumented numpy tol s absolute tolerance Otherwise tol relative tolerance units dtype epsilon https github com numpy numpy blob v numpy lib type_check py#L tol = tol torch finfo dtype eps mask = torch abs imag tol real mask all real ArrayLike torch real imag ArrayLike is_complex imag torch zeros_like iscomplex x ArrayLike torch is_complex x x imag = torch zeros_like x dtype=torch bool isreal x ArrayLike torch is_complex x x imag == torch ones_like x dtype=torch bool iscomplexobj x ArrayLike torch is_complex x isrealobj x ArrayLike torch is_complex x isneginf x ArrayLike out Optional OutArray = None torch isneginf x isposinf x ArrayLike out Optional OutArray = None torch isposinf x i x ArrayLike torch special i x isscalar We need use normalize_array_like we don t want export funcs py _normalizations normalize_array_like try t = normalize_array_like t numel == except Exception False ### Filter windows ### hamming M dtype = _dtypes_impl default_dtypes float_dtype torch hamming_window M periodic=False dtype=dtype hanning M dtype = _dtypes_impl default_dtypes float_dtype torch hann_window M periodic=False dtype=dtype kaiser M beta dtype = _dtypes_impl default_dtypes float_dtype torch kaiser_window M beta=beta periodic=False dtype=dtype blackman M dtype = _dtypes_impl default_dtypes float_dtype torch blackman_window M periodic=False dtype=dtype bartlett M dtype = _dtypes_impl default_dtypes float_dtype torch bartlett_window M periodic=False dtype=dtype ### Dtype routines ### vendored https github com numpy numpy blob v numpy lib type_check py#L array_type = torch float torch float torch float None torch complex torch complex array_precision = torch float torch float torch float torch complex torch complex common_type tensors ArrayLike is_complex = False precision = tensors t = dtype iscomplexobj is_complex = True t is_floating_point t is_complex p = array_precision _nx double p = array_precision get t p None raise TypeError can t get common type non-numeric array precision = builtins max precision p is_complex array_type precision array_type precision ### histograms ### histogram ArrayLike bins ArrayLike = range=None normed=None weights Optional ArrayLike = None density=None normed None raise ValueError normed argument deprecated use density= instead weights None weights dtype is_complex raise NotImplementedError complex weights histogram is_a_int = dtype is_floating_point dtype is_complex is_w_int = weights None weights dtype is_floating_point is_a_int = double weights None weights = _util cast_if_needed weights dtype isinstance bins torch Tensor bins ndim == bins single int bins = operator index bins bins = _util cast_if_needed bins dtype range None h b = torch histogram bins weight=weights density=bool density h b = torch histogram bins range=range weight=weights density=bool density density is_w_int h = h long is_a_int b = b long h b histogram d x y bins= range Optional ArrayLike = None normed=None weights Optional ArrayLike = None density=None vendored https github com numpy numpy blob v numpy lib twodim_base py#L -L len x = len y raise ValueError x y must have same length try N = len bins except TypeError N = N = N = bins = bins bins h e = histogramdd x y bins range normed weights density h e e histogramdd sample bins= range Optional ArrayLike = None normed=None weights Optional ArrayLike = None density=None have normalize manually because ` sample ` interpretation differs list lists D array normed None raise ValueError normed argument deprecated use density= instead _normalizations normalize_array_like normalize_seq_array_like isinstance sample list tuple sample = normalize_array_like sample T sample = normalize_array_like sample sample = torch atleast_ d sample sample dtype is_floating_point sample dtype is_complex sample = sample double bins either int sequence ints sequence arrays bins_is_array = isinstance bins int builtins all isinstance b int b bins bins_is_array bins = normalize_seq_array_like bins bins_dtypes = b dtype b bins bins = _util cast_if_needed b sample dtype b bins range None range = range flatten tolist weights None range= required interleave min max values per dimension mm = sample aminmax dim= range = torch cat mm reshape - T flatten range = tuple range tolist weights = _util cast_if_needed weights sample dtype w_kwd = weight weights w_kwd = h b = torch histogramdd sample bins range density=bool density w_kwd bins_is_array b = _util cast_if_needed bb dtyp bb dtyp zip b bins_dtypes h b ### odds ends min_scalar_type ArrayLike https github com numpy numpy blob maintenance x numpy core src multiarray convert_datatype c#L _dtypes DType numel numpy docs For non-scalar array returns vector s dtype unmodified DType dtype dtype == torch bool dtype = torch bool dtype is_complex fi = torch finfo torch float fits_in_single = dtype == torch complex fi min = real = fi max fi min = imag = fi max dtype = torch complex fits_in_single torch complex dtype is_floating_point dt torch float torch float torch float fi = torch finfo dt fi min = = fi max dtype = dt break must integer dt torch uint torch int torch int torch int torch int Prefer unsigned int where possible numpy does ii = torch iinfo dt ii min = = ii max dtype = dt break DType dtype pad array ArrayLike pad_width ArrayLike mode= constant kwargs mode = constant raise NotImplementedError value = kwargs get constant_values ` value ` must python scalar torch nn functional pad typ = _dtypes_impl python_type_for_torch array dtype value = typ value pad_width = torch broadcast_to pad_width array ndim pad_width = torch flip pad_width flatten torch nn functional pad array tuple pad_width value=value