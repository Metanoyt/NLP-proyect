Owner s module unknown ruff noqa F unittest typing Optional numpy np torch torch nn torch testing _internal common_utils TestCase run_tests torch testing _internal static_module StaticModule linear_shim input torch Tensor weight torch Tensor bias Optional torch Tensor = None - torch Tensor output = input matmul weight t bias None output += bias ret = output ret torch nn functional linear = linear_shim MultiHeadAttentionLayer nn Module __init__ hid_dim n_heads dropout device super __init__ assert hid_dim n_heads == hid_dim = hid_dim n_heads = n_heads head_dim = hid_dim n_heads fc_q = nn Linear hid_dim hid_dim fc_k = nn Linear hid_dim hid_dim fc_v = nn Linear hid_dim hid_dim fc_o = nn Linear hid_dim hid_dim dropout = nn Dropout dropout scale = torch sqrt torch FloatTensor head_dim device forward query key value mask batch_size = query shape Q = fc_q query K = fc_k key V = fc_v value Q = Q view batch_size - n_heads head_dim permute K = K view batch_size - n_heads head_dim permute V = V view batch_size - n_heads head_dim permute energy = torch matmul Q K permute scale energy = energy masked_fill mask == - e attention = torch softmax energy dim=- x = torch matmul dropout attention V x = torch matmul attention V x = x permute contiguous x = x view batch_size - hid_dim x = fc_o x x attention Taken https github com facebookresearch dlrm blob master dlrm_s_pytorch py create_mlp ln sigmoid_layer layers = nn ModuleList i range len ln - n = ln i m = ln i + LL = nn Linear int n int m bias=True mean = std_dev = np sqrt variance std_dev = np sqrt m + n np sqrt m np sqrt n W = np random normal mean std_dev size= m n astype np float std_dev = np sqrt m np sqrt m + bt = np random normal mean std_dev size=m astype np float LL weight data = torch tensor W requires_grad=True LL bias data = torch tensor bt requires_grad=True layers append LL i == sigmoid_layer layers append nn Sigmoid layers append nn ReLU torch no_grad s = torch jit script torch nn Sequential layers s eval s trivial_graph b c s = torch tensor + b c + s elementwise_square_addition input input input input + input input fork_wait_graph input input fut = torch jit fork elementwise_square_addition input input torch jit wait fut fork_wait_graph input input fut = torch jit fork loop_graph input input torch jit wait fut graph multiple fork wait operations param input torch tensor input forked subgraph param iters number future wait pairs created fork_wait_graph input iters int futures list torch jit Future torch Tensor = _ range iters futures append torch jit fork torch neg input results = future futures results append torch jit wait future torch sum torch stack results graph multi-level fork wait operations param input torch tensor input forked subgraph param num_forks number top level forks param num_child_forks number child forks per parent fork fork_wait_graph input num_forks int num_child_forks int futures list torch jit Future torch Tensor = _ range num_forks futures append torch jit fork fork_wait_graph input num_child_forks results = future futures results append torch jit wait future torch sum torch stack results add_tensor input input input + input fork_wait_graph_exception input input fut = torch jit fork add_tensor input input torch jit wait fut loop_graph b iters int c = + b _ range iters c = c + b c = c -= c output_graph b c iters int s = torch tensor k = + b c + s d dict int torch Tensor = i range iters d i = k + i d SubModule nn Module __init__ - None super __init__ = b = forward x + b + x SubModule nn Module __init__ - None super __init__ = b = forward x b = + b + x TestModule nn Module __init__ - None super __init__ sub = SubModule sub = SubModule = b = forward x b = sub x + + b + sub x TestStaticModule TestCase Test Case To test simple fork wait operation graph fork called simple addition operation input tensors test_fork_wait_ inp = torch ones inp = torch randn torch_graph = torch jit script fork_wait_graph output_ref = torch_graph inp inp static_runtime_module = StaticModule torch_graph output_test = static_runtime_module inp inp torch testing assert_close output_test output_ref Test Case To test simple fork wait operation StaticRuntime runAsync API returning future test_fork_wait_ _async inp = torch ones inp = torch randn torch_graph = torch jit script fork_wait_graph output_ref = torch_graph inp inp static_runtime_module = StaticModule torch_graph output_test = static_runtime_module runAsync inp inp output_test wait torch testing assert_close output_test value output_ref Test Case To test fork wait operation graph loop subgraph performing mix operations test_fork_wait_ inp = torch randn inp = torch randn torch_graph = torch jit script fork_wait_graph output_ref = torch_graph inp inp static_runtime_module = StaticModule torch_graph output_test = static_runtime_module inp inp torch testing assert_close output_test output_ref Test Case To test fork wait operation loop subgraph StaticRuntime runAsync API returning future test_fork_wait_ _async inp = torch randn inp = torch randn torch_graph = torch jit script fork_wait_graph output_ref = torch_graph inp inp static_runtime_module = StaticModule torch_graph output_test = static_runtime_module runAsync inp inp output_test wait torch testing assert_close output_test value output_ref Test Case To test fork wait operation graph having multiple fork wait operations test_fork_wait_ input = torch ones num_forks = torch_graph = torch jit script fork_wait_graph output_ref = torch_graph input num_forks static_runtime_module = StaticModule torch_graph output_test = static_runtime_module input num_forks torch testing assert_close output_test output_ref Test Case To test fork wait operation graph multiple fork wait operations runAsync API returning future test_fork_wait_ _async input = torch ones num_forks = torch_graph = torch jit script fork_wait_graph output_ref = torch_graph input num_forks static_runtime_module = StaticModule torch_graph output_test = static_runtime_module runAsync input num_forks output_test wait torch testing assert_close output_test value output_ref Test Case To test fork wait operation graph multiple nested fork wait operations unittest skip Broken test https github com pytorch pytorch issues test_fork_wait_ input = torch ones num_forks = num_child_forks = torch_graph = torch jit script fork_wait_graph static_runtime_module = StaticModule torch_graph output_ref = torch_graph input num_forks num_child_forks output_test = static_runtime_module input num_forks num_child_forks torch testing assert_close output_test output_ref Test Case To test fork wait operation graph multiple nested fork wait operations runAsync API returning future unittest skip Broken test https github com pytorch pytorch issues test_fork_wait_ _async input = torch ones num_forks = num_child_forks = torch_graph = torch jit script fork_wait_graph static_runtime_module = StaticModule torch_graph output_ref = torch_graph input num_forks num_child_forks output_test = static_runtime_module runAsync input num_forks num_child_forks output_test wait torch testing assert_close output_test value output_ref Test Case To test exception handling fork wait operation Add Tensor op called tensors non-matching dims forked subgraph exception raised subgraph set future returned prim fork parent graph Returned exception checked substring expected_error_msg declared below test_fork_wait_exception incompatible tensors add due shape mismatch input = torch randn input = torch randn torch_graph = torch jit script fork_wait_graph_exception try static_runtime_module = StaticModule torch_graph output_test = static_runtime_module input input except Exception error expected_error_msg = The size tensor must match size tensor b non-singleton dimension test fails error does contain expected substr str error find expected_error_msg == - raise RuntimeError Tried execution add Tensors incompatible shape Exception raised forked runtime execution does f contain expected substring expected_error_msg error Test Case To test exception handling fork wait operation runAsync API Add Tensor op called tensors non-matching dims forked subgraph exception raised subgraph set future returned prim fork parent graph Returned exception checked substring expected_error_msg declared below test_fork_wait_exception_async incompatible tensors add due shape mismatch input = torch randn input = torch randn torch_graph = torch jit script fork_wait_graph_exception try static_runtime_module = StaticModule torch_graph output_test = static_runtime_module runAsync input input except Exception error expected_error_msg = The size tensor must match size tensor b non-singleton dimension test fails error does contain expected substr str error find expected_error_msg == - raise RuntimeError Tried execution add Tensors incompatible shape Exception raised forked runtime execution does f contain expected substring expected_error_msg error test_multihead_attention_layer HID_DIM = QUERY_LEN = BATCH_SIZE = LAYERS = HEADS = DROPOUT = device = torch device cpu attention = MultiHeadAttentionLayer HID_DIM HEADS DROPOUT device device torch no_grad src = torch randn BATCH_SIZE QUERY_LEN HID_DIM device src_mask = src unsqueeze unsqueeze device attention eval attention = torch jit script attention attention eval o_ref = attention src src src src_mask attention_a = StaticModule attention o_test = attention_a src src src src_mask o_test_kw = attention_a src src value=src mask=src_mask b zip o_ref o_test torch testing assert_close b b zip o_ref o_test_kw torch testing assert_close b test_multihead_attention_layer_benchmark HID_DIM = QUERY_LEN = BATCH_SIZE = LAYERS = HEADS = DROPOUT = device = torch device cpu attention = MultiHeadAttentionLayer HID_DIM HEADS DROPOUT device device torch no_grad src = torch randn BATCH_SIZE QUERY_LEN HID_DIM device src_mask = src unsqueeze unsqueeze device attention eval attention = torch jit script attention attention_a = StaticModule attention attention_a benchmark src src src src_mask metrics = attention_a benchmark_individual_ops src src src src_mask test_mlp Arguments taken benchmark script bench dlrm_s_benchmark sh ln_bot = sigmoid_bot = - ln_top = sigmoid_top = bot_l = create_mlp ln_bot sigmoid_bot bot_l_acc = StaticModule bot_l top_l = create_mlp ln_top sigmoid_top top_l_acc = StaticModule top_l torch no_grad bot_inp = torch randn torch Size top_inp = torch randn torch Size ref_bot = bot_l bot_inp acc_bot = bot_l_acc bot_inp torch testing assert_close acc_bot ref_bot ref_top = top_l top_inp acc_top = top_l_acc top_inp torch testing assert_close acc_top ref_top _ range torch no_grad bot_inp = torch randn torch Size top_inp = torch randn torch Size ref_bot = bot_l bot_inp acc_bot = bot_l_acc bot_inp torch testing assert_close acc_bot ref_bot ref_top = top_l top_inp acc_top = top_l_acc top_inp torch testing assert_close acc_top ref_top test_trivial_graph s = torch full tg = torch jit script trivial_graph o_ref = tg s s s tg_a = StaticModule tg o_test = tg_a s s s torch testing assert_close o_ref o_test test_leaky_relu s = torch randn tg = torch jit script nn LeakyReLU o_ref = tg s tg_a = StaticModule tg o_test = tg_a s torch testing assert_close o_ref o_test test_attr TorchScript IR TestModule after freezing graph __torch__ test_static_runtime ___torch_mangle_ TestModule x Tensor int = prim Constant value= int = prim Constant value= int = prim Constant value= int = prim Constant value= sub int = prim Constant value= int = prim Constant value= = prim SetAttr name= b Tensor = aten add x Tensor = aten add b int = prim GetAttr name= b Tensor = aten add b sub __torch__ test_static_runtime ___torch_mangle_ SubModule = prim GetAttr name= sub = prim SetAttr name= b sub b int = prim GetAttr name= b sub int = aten add sub b Tensor = aten add x Tensor = aten add test prim SetAttr prim GetAttr impl Static Runtime m = TestModule m eval input = torch randn output_s = m forward input ms = torch jit script m sm = StaticModule ms output_sm = sm input torch testing assert_close output_s output_sm sm benchmark input sm benchmark_individual_ops input sm benchmark x input sm benchmark_individual_ops x input unittest skip Temporarily disabled test_fusion_trivial_graph s = torch full tg = torch jit script trivial_graph o_ref = tg s s s torch _C _fuse_to_static_module tg graph assert StaticSubgraph str tg graph o_test = tg s s s torch testing assert_close o_ref o_test unittest skip Temporarily disabled test_fusion_multihead_attention_layer HID_DIM = QUERY_LEN = BATCH_SIZE = LAYERS = HEADS = DROPOUT = device = torch device cpu attention = MultiHeadAttentionLayer HID_DIM HEADS DROPOUT device device torch no_grad src = torch randn BATCH_SIZE QUERY_LEN HID_DIM device src_mask = src unsqueeze unsqueeze device attention eval attention = torch jit script attention attention eval o_ref = attention src src src src_mask torch _C _fuse_to_static_module attention _c o_test = attention src src src src_mask b zip o_ref o_test torch testing assert_close b unittest skip Temporarily disabled test_fusion_loop = torch randn b = torch randn c = lg = torch jit script loop_graph o_ref = lg b c torch _C _fuse_to_static_module lg graph assert StaticSubgraph str lg graph o_test = lg b c torch testing assert_close o_ref o_test unittest skip Temporarily disabled test_fusion_outputs = torch randn b = torch randn c = og = torch jit script output_graph o_ref = og b b c torch _C _fuse_to_static_module og graph assert StaticSubgraph str og graph o_test = og b b c i o_ref keys torch testing assert_close o_ref i o_test i test_create_object Foo noqa B __init__ x torch Tensor - None x = x Mod torch nn Module __init__ - None super __init__ forward y torch Tensor - torch Tensor foo = Foo y y foo x mod = torch jit script Mod eval y = torch randn expected = mod y static_mod = StaticModule torch jit freeze mod actual = static_mod y assertEqual expected actual __name__ == __main__ run_tests