argparse asyncio os path subprocess threading time concurrent futures ThreadPoolExecutor queue Empty numpy np pandas pd torch torch multiprocessing mp FrontendWorker mp Process This worker will send requests backend process measure throughput latency those requests well GPU utilization __init__ metrics_dict request_queue response_queue read_requests_event batch_size num_iters= super __init__ metrics_dict = metrics_dict request_queue = request_queue response_queue = response_queue read_requests_event = read_requests_event warmup_event = mp Event batch_size = batch_size num_iters = num_iters poll_gpu = True start_send_time = None end_recv_time = None _run_metrics metrics_lock This function will poll response queue until has received all responses It records startup latency average max min latency well throughput requests warmup_response_time = None response_times = i range num_iters + response request_time = response_queue get warmup_response_time None warmup_event set warmup_response_time = time time - request_time response_times append time time - request_time end_recv_time = time time poll_gpu = False response_times = np array response_times metrics_lock metrics_dict warmup_latency = warmup_response_time metrics_dict average_latency = response_times mean metrics_dict max_latency = response_times max metrics_dict min_latency = response_times min metrics_dict throughput = num_iters batch_size end_recv_time - start_send_time _run_gpu_utilization metrics_lock This function will poll nvidia-smi GPU utilization every ms record average GPU utilization get_gpu_utilization try nvidia_smi_output = subprocess check_output nvidia-smi -- query-gpu=utilization gpu -- id= -- format=csv noheader nounits gpu_utilization = nvidia_smi_output decode strip gpu_utilization except subprocess CalledProcessError N A gpu_utilizations = while poll_gpu gpu_utilization = get_gpu_utilization gpu_utilization = N A gpu_utilizations append float gpu_utilization metrics_lock metrics_dict gpu_util = torch tensor gpu_utilizations mean item _send_requests This function will send one warmup request then num_iters requests backend process fake_data = torch randn batch_size requires_grad=False other_data = torch randn batch_size requires_grad=False i range num_iters Send one batch warmup data request_queue put fake_data time time Tell backend poll queue warmup request read_requests_event set warmup_event wait Tell backend poll queue rest requests read_requests_event set Send fake data start_send_time = time time i range num_iters request_queue put other_data i time time run Lock writing metrics_dict metrics_lock = threading Lock requests_thread = threading Thread target=self _send_requests metrics_thread = threading Thread target=self _run_metrics args= metrics_lock gpu_utilization_thread = threading Thread target=self _run_gpu_utilization args= metrics_lock requests_thread start metrics_thread start only start polling GPU utilization after warmup request complete warmup_event wait gpu_utilization_thread start requests_thread join metrics_thread join gpu_utilization_thread join BackendWorker This worker will take tensors request queue do some computation then result back response queue __init__ metrics_dict request_queue response_queue read_requests_event batch_size num_workers model_dir= compile_model=True super __init__ device = cuda metrics_dict = metrics_dict request_queue = request_queue response_queue = response_queue read_requests_event = read_requests_event batch_size = batch_size num_workers = num_workers model_dir = model_dir compile_model = compile_model _setup_complete = False h d_stream = torch cuda Stream d h_stream = torch cuda Stream maps thread_id cuda Stream associated worker thread stream_map = _setup time torchvision models resnet BasicBlock ResNet torch Create ResNet meta device torch device meta m = ResNet BasicBlock Load pretrained weights start_load_time = time time state_dict = torch load f model_dir resnet -f fd pth mmap=True map_location=self device metrics_dict torch_load_time = time time - start_load_time m load_state_dict state_dict assign=True m eval compile_model start_compile_time = time time m compile end_compile_time = time time metrics_dict m_compile_time = end_compile_time - start_compile_time m model_predict model input_buffer copy_event compute_event copy_sem compute_sem response_list request_time copy_sem makes sure copy_event has been recorded data copying thread copy_sem acquire stream_map threading get_native_id wait_event copy_event torch cuda stream stream_map threading get_native_id torch no_grad response_list append model input_buffer compute_event record compute_sem release del input_buffer copy_data input_buffer data copy_event copy_sem data = data pin_memory torch cuda stream h d_stream input_buffer copy_ data non_blocking=True copy_event record copy_sem release respond compute_event compute_sem response_list request_time compute_sem makes sure compute_event has been recorded model_predict thread compute_sem acquire d h_stream wait_event compute_event torch cuda stream d h_stream response_queue put response_list cpu request_time async run worker_initializer stream_map threading get_native_id = torch cuda Stream worker_pool = ThreadPoolExecutor max_workers=self num_workers initializer=worker_initializer h d_pool = ThreadPoolExecutor max_workers= d h_pool = ThreadPoolExecutor max_workers= read_requests_event wait Clear we will wait event again before continuing poll request_queue non-warmup requests read_requests_event clear while True try data request_time = request_queue get timeout= except Empty break _setup_complete model = _setup copy_sem = threading Semaphore compute_sem = threading Semaphore copy_event = torch cuda Event compute_event = torch cuda Event response_list = input_buffer = torch empty batch_size dtype=torch float device= cuda asyncio get_running_loop run_in_executor h d_pool copy_data input_buffer data copy_event copy_sem asyncio get_running_loop run_in_executor worker_pool model_predict model input_buffer copy_event compute_event copy_sem compute_sem response_list request_time asyncio get_running_loop run_in_executor d h_pool respond compute_event compute_sem response_list request_time _setup_complete read_requests_event wait _setup_complete = True __name__ == __main__ parser = argparse ArgumentParser parser add_argument -- num_iters type=int default= parser add_argument -- batch_size type=int default= parser add_argument -- model_dir type=str default= parser add_argument -- compile default=True action=argparse BooleanOptionalAction parser add_argument -- output_file type=str default= output csv parser add_argument -- profile default=False action=argparse BooleanOptionalAction parser add_argument -- num_workers type=int default= args = parser parse_args downloaded_checkpoint = False os path isfile f args model_dir resnet -f fd pth p = subprocess run wget https download pytorch org models resnet -f fd pth p returncode == downloaded_checkpoint = True raise RuntimeError Failed download checkpoint try mp set_start_method forkserver request_queue = mp Queue response_queue = mp Queue read_requests_event = mp Event manager = mp Manager metrics_dict = manager dict metrics_dict batch_size = args batch_size metrics_dict compile = args compile frontend = FrontendWorker metrics_dict request_queue response_queue read_requests_event args batch_size num_iters=args num_iters backend = BackendWorker metrics_dict request_queue response_queue read_requests_event args batch_size args num_workers args model_dir args compile frontend start args profile trace_handler prof prof export_chrome_trace trace json torch profiler profile on_trace_ready=trace_handler prof asyncio run backend run asyncio run backend run frontend join metrics_dict = k v k v metrics_dict _getvalue items output = pd DataFrame from_dict metrics_dict orient= columns output_file = results + args output_file is_empty = os path isfile output_file open output_file a+ newline= file output to_csv file header=is_empty index=False finally Cleanup checkpoint file we downloaded downloaded_checkpoint os remove f args model_dir resnet -f fd pth