mypy allow-untyped-defs pyre-strict __future__ annotations heapq importlib itertools logging operator sys time collections defaultdict dataclasses dataclass typing Any Optional TYPE_CHECKING Union torch torch _logging trace_structured torch multiprocessing reductions StorageWeakRef torch utils _ordered_set OrderedSet config ir dependencies WeakDep TYPE_CHECKING ir IRNode Operation scheduler SchedulerBuffer memory estimate_peak_memory estimate_peak_memory_allocfree FreeableInputBuffer get_freeable_input_buf SNodeMemory utils contains_collective contains_wait find_recursive_deps_of_node find_recursive_users_of_node is_collective is_fallback_op is_wait virtualized V log = logging getLogger __name__ overlap_log = torch _logging getArtifactLogger __name__ overlap TYPE_CHECKING torch _inductor scheduler BaseSchedulerNode align_runtime_estimations_across_all_distributed_ranks snodes list BaseSchedulerNode runtime_estimations = snode snodes runtime_estimations snode = snode get_estimated_runtime torch distributed dist torch distributed distributed_c d _get_default_group world_size = dist get_world_size pg = _get_default_group gathered_runtime_estimations list list float = _ range world_size dist all_gather_object gathered_runtime_estimations list runtime_estimations values pg median_runtime_estimations = torch median torch tensor gathered_runtime_estimations dim= values tolist i range len snodes snodes i override_estimated_runtime = median_runtime_estimations i sink_waits snodes list BaseSchedulerNode - list BaseSchedulerNode Greedily schedules waits late possible _schedule_for_comm snodes raise_comms=False sink_waits=True reorder_for_overlap=False raise_comms snodes list BaseSchedulerNode - list BaseSchedulerNode Greedily schedules comms early possible _schedule_for_comm snodes raise_comms=True sink_waits=False reorder_for_overlap=False reorder_compute_for_overlap snodes list BaseSchedulerNode - list BaseSchedulerNode This achieves following overall scheduling procedure Step Given we ve currently scheduled comm N we now schedule all compute nodes required comm N + do depend comm N run same time comm N Step If all those compute nodes sufficient overlap comm N we re done Otherwise we now need look elsewhere find compute overlaps comm N We prioritize compute nodes needed sooner Step We schedule compute nodes dependent comm N required comm N + Step We schedule comm N + Repeat subsequent comm nodes _schedule_for_comm snodes raise_comms=True sink_waits=True reorder_for_overlap=True reorder_communication_preserving_peak_memory snodes list BaseSchedulerNode - list BaseSchedulerNode Reorders communication ops relative computation ops improve communication-compute overlapping hide comm latency Stops moving particular op reaches point would have increased peak memory footprint Currently follows these heuristics subject change tune - never reorders collectives relative one another SPMD safety - has option per-collective prefetch limit does enable default - limits total number reorder steps some factor graph size prevent worst-case quadratic performance Prerequisite sink_comms_and_waits - ensure comm wait nodes scheduled late possible respecting data dependencies That allows reorder_communication_preserving_peak_memory take best case peak-memory snapshot then monotonically improve latency moving collectives backward time Peak memory impact computed iterative fashion First memory use each timestep computed global peak memory computed max over timesteps Then when swapping any two adjacent nodes only curr-memory earlier nodes after swap affected This enables checking step step whether swap peak-memory-safe bailing out Example n C n C + Allocs n - Frees n n C + Allocs n - Frees n + Allocs n - Frees n n C n C + Allocs n - Frees n -- After moving n Time only time memory changes n C + Allocs n - Frees n + Allocs n - Frees n reordered_snodes node_stats = _reorder_communication_preserving_peak_memory_internal snodes reordered_snodes dataclass ReorderInfo Debug info describing how individual snode reordered initial_exposed float = - final_exposed float = - limiting_factor str = None moves int = grouped int = grouped_info str = property improvement initial_exposed - final_exposed is_gemm_like node Optional Union IRNode Operation - bool node None False is_fallback_op node type ignore arg-type torch ops aten _scaled_dot_product_flash_attention default True python_kernel_name = getattr node python_kernel_name None extern_kernels python_kernel_name True False contains_gemm_like snode BaseSchedulerNode - bool torch _inductor scheduler GroupedSchedulerNode isinstance snode GroupedSchedulerNode any contains_gemm_like x x snode snodes is_gemm_like snode node _temp_group_visit_leaves snode fn torch _inductor scheduler GroupedSchedulerNode isinstance snode GroupedSchedulerNode snode temp_grouping _snode snode snodes fn _snode fn snode _group_name snode with_bufs=False - str ret = n snode snodes ret ret += _ ret += n get_name with_bufs ret += f list snode get_buffer_names ret _is_fake_dep d isinstance d WeakDep d is_fake _group_names gns list BaseSchedulerNode - str ~ join gn get_name gn gns _initialize_memory_tracking snodes graph_inputs graph_outputs Initialize memory tracking data structures name_to_freeable_input_buf = get_freeable_input_buf snodes graph_inputs peak_memory snodes_curr_memory snodes_allocfree buf_to_snode_last_use = estimate_peak_memory_allocfree snodes name_to_freeable_input_buf graph_outputs _curr_memory = dict zip snodes snodes_curr_memory _curr_memory None = peak_memory _curr_memory snodes_allocfree buf_to_snode_last_use name_to_freeable_input_buf _initialize_double_linked_list snodes list BaseSchedulerNode - tuple dict BaseSchedulerNode Optional BaseSchedulerNode dict BaseSchedulerNode Optional BaseSchedulerNode BaseSchedulerNode Create double-linked list structure snodes _prev = _next = i snode enumerate snodes _prev snode = snodes i - i None _next snode = snodes i + i len snodes - None _head = snodes _prev _next _head _reorder_communication_preserving_peak_memory_internal snodes list BaseSchedulerNode - tuple list BaseSchedulerNode dict BaseSchedulerNode ReorderInfo Internal testing helper also returns debug info Returns - reordered snodes list - dict snode ReorderInfo has_collectives = False snode snodes contains_collective snode has_collectives = True break has_collectives snodes torch _inductor scheduler GroupedSchedulerNode original_snodes_num = len snodes heuristic avoid degenerating quadratic time graph_inputs OrderedSet str = OrderedSet V graph graph_inputs keys graph_outputs OrderedSet str = OrderedSet V graph get_output_names peak_memory _curr_memory snodes_allocfree buf_to_snode_last_use name_to_freeable_input_buf = _initialize_memory_tracking snodes graph_inputs graph_outputs runtimes dict BaseSchedulerNode float = snode estimate_op_runtime snode snode snodes debug stats stats dict BaseSchedulerNode ReorderInfo = exposed_communication_time collective_snode BaseSchedulerNode remaining_snodes list BaseSchedulerNode - float assumes linear schedule computes overlap collective remaining nodes comm_time = estimate_op_runtime collective_snode compute_time = snode remaining_snodes contains_collective snode continue contains_wait snode TODO - wait collective started before collective another stream we can ignore Otherwise s end road overlap opportunities break accumulate_time _snode BaseSchedulerNode - None nonlocal compute_time compute_time += runtimes _snode _temp_group_visit_leaves snode accumulate_time max comm_time - compute_time total_moves = _prev _next _head = _initialize_double_linked_list snodes _group_nodes head Optional BaseSchedulerNode tail Optional BaseSchedulerNode - list BaseSchedulerNode ret = n = head while True n None ret append n n == tail break n = _next n type ignore index ret _perform_double_linked_list_swap candidate group_head group_tail swap candidate group_head group_tail Before candidate_prev - - candidate - - group_head group_tail - - group_tail_next After candidate_prev - - group_head group_tail - - candidate - - group_tail_next candidate_prev = _prev candidate candidate_prev _next candidate_prev = group_head _prev group_head = candidate_prev group_tail_next = _next group_tail group_tail_next _prev group_tail_next = candidate _next candidate = group_tail_next _prev candidate = group_tail _next group_tail = candidate nonlocal _head _head == candidate _head = group_head _calculate_potential_peak_memory candidate group_ns group_n_to_bufs_after_swap_dealloc_by_candidate Caching calculations memory group nodes candidate apply without recalculation after swap _post_alloc_update dict BaseSchedulerNode int = potential_peak int = group_n_to_bufs_after_swap_dealloc_by_candidate Not accounting buffers last use change potential_peak = max group_peak_memory - candidate_delta_mem _curr_memory group_tail - candidate_delta_mem + candidate_allocfree size_alloc potential_peak _post_alloc_update If candidate will after group starting memory level group nodes changes - candidate size_alloc - candidate size_free mem_after_reorder_delta int = -candidate_delta_mem gn gns gn_post_alloc_mem = _curr_memory gn + mem_after_reorder_delta _post_alloc_update gn = gn_post_alloc_mem potential_peak = max potential_peak gn_post_alloc_mem bufs = group_n_to_bufs_after_swap_dealloc_by_candidate get gn None bufs None buf bufs Candidate will deallocate those buffers mem_after_reorder_delta += buf mpi_buffer size_free candidate_mem_post_alloc = _curr_memory group_tail + mem_after_reorder_delta + candidate_allocfree size_alloc _post_alloc_update candidate = candidate_mem_post_alloc potential_peak = max potential_peak candidate_mem_post_alloc potential_peak _post_alloc_update _update_memory_tracking_after_swap candidate gns group_n_to_bufs_after_swap_dealloc_by_candidate _post_alloc_update group_n_to_bufs_after_swap_dealloc_by_candidate gn gns cm = _curr_memory gn _curr_memory gn = cm - candidate_delta_mem cm - candidate_delta_mem _candidate_post_alloc_mem = _curr_memory group_tail + candidate_allocfree size_alloc _candidate_post_free_mem = _candidate_post_alloc_mem - candidate_allocfree size_free _curr_memory candidate = _candidate_post_alloc_mem _candidate_post_free_mem Candidate becomes last use some bufs bufs group_n_to_bufs_after_swap_dealloc_by_candidate values buf bufs buf_to_snode_last_use buf = candidate size_free_to_move_to_candidate_sum int = n gns _gn_post_alloc_mem int = _post_alloc_update n size_free_to_move_to_candidate int = sum buf mpi_buffer size_free buf group_n_to_bufs_after_swap_dealloc_by_candidate n size_free_to_move_to_candidate_sum += size_free_to_move_to_candidate group node does deallocate after swap snodes_allocfree n size_free -= size_free_to_move_to_candidate gn_post_free_mem int = _gn_post_alloc_mem - snodes_allocfree n size_free _curr_memory n = _gn_post_alloc_mem gn_post_free_mem _candidate_post_alloc_mem = _post_alloc_update candidate snodes_allocfree candidate size_free += size_free_to_move_to_candidate_sum candidate_post_free_mem = _candidate_post_alloc_mem - snodes_allocfree candidate size_free _curr_memory candidate = _candidate_post_alloc_mem candidate_post_free_mem debug_num_collectives_to_reorder Optional int = config reorder_iterative_debug_limit_to_reorder num_processed_collectives int = curr = _head debug_iterative_memory_recompute = config reorder_iterative_debug_memory_recompute iterative_recompute_error = False while _next curr None iterative_recompute_error break pyrefly ignore bad-argument-type contains_collective curr debug_num_collectives_to_reorder None num_processed_collectives = debug_num_collectives_to_reorder break num_processed_collectives += info = stats curr = ReorderInfo info initial_exposed = info final_exposed = exposed_communication_time curr _group_nodes _next curr None candidate = _prev curr group_head = curr group_tail = curr group_peak_memory = _curr_memory curr post_alloc memory while candidate None contains_collective candidate info limiting_factor = collective ordering break gns list BaseSchedulerNode = _group_nodes group_head group_tail group = GroupedSchedulerNode curr scheduler gns temp_grouping=True We can have multiple deps same name As we ignore WeakDep is_fake=True = filter them out first avoid overwriting real dep data_deps = d name d d group unmet_dependencies _is_fake_dep d candidate_outs = candidate get_outputs data_dep = None o candidate_outs d = data_deps get o get_name None data_dep = d break data_dep None is_groupable candidate BaseSchedulerNode - tuple bool Optional str preserve ordering contains_collective candidate False contains_collective contains_gemm_like candidate False contains_gemm_like True None is_groupable_result grouping_reason = is_groupable candidate is_groupable_result group_head = candidate group_peak_memory = max group_peak_memory _curr_memory candidate info grouped += info grouped_info = _group_names gns candidate = _prev candidate continue msg = f data dependency data_dep dep_names list data_deps keys f \n candidate candidate get_name outs candidate get_buffer_names f dep _group_names gns f \n non_group_reason grouping_reason info limiting_factor = msg break candidate_allocfree SNodeMemory = snodes_allocfree candidate candidate_delta_mem int = candidate_allocfree size_alloc - candidate_allocfree size_free candidate one group nodes successors same buffer last use buffer happen group nodes This last use deallocates If we swap candidate group group candidate candidate becomes last use deallocated buffer instead group node we need update size_free accordingly group_node candidate recalculate post_alloc post_free them Buf changes its last use snode after swap will deallocated only candidate while before deallocated group node group_n_to_bufs_after_swap_dealloc_by_candidate dict BaseSchedulerNode list Union FreeableInputBuffer Any = defaultdict list buf snode_last_use buf_to_snode_last_use items succ_nodes = buf mpi_buffer succ_nodes candidate succ_nodes continue any gn == snode_last_use gn gns continue group_n_to_bufs_after_swap_dealloc_by_candidate snode_last_use append buf potential_peak _post_alloc_update = _calculate_potential_peak_memory candidate gns group_n_to_bufs_after_swap_dealloc_by_candidate potential_peak peak_memory info limiting_factor = f peak memory new potential_peak vs base peak_memory break info moves += total_moves += _perform_double_linked_list_swap candidate group_head group_tail info final_exposed = exposed_communication_time curr _group_nodes _next curr None _update_memory_tracking_after_swap candidate gns group_n_to_bufs_after_swap_dealloc_by_candidate _post_alloc_update debug_iterative_memory_recompute Compare iteratively recomputed memory data full run estimate_peak_memory comms_debug _debug_iterative_memory_recompute iterative_recompute_error = _debug_iterative_memory_recompute candidate gns _group_names gns _group_nodes _head None name_to_freeable_input_buf graph_outputs peak_memory _curr_memory snodes_allocfree reorder_communication_preserving_peak_memory group_n_to_bufs_after_swap_dealloc_by_candidate iterative_recompute_error break candidate = _prev group_head curr = _next curr type ignore assignment node_stats = stats improvement = snode node_stats snode improvement snode node_stats total_improvement = sum improvement snode snode improvement total_moves = sum node_stats snode moves snode node_stats reorder_log_str = f reorder_communication_preserving_peak_memory improved overlap total_improvement ns f after total_moves reorders \n headers = Collective node initial exposed final exposed improvement limiting factor moves grouped grouped_info rows = node_summary snode node_info initial_exposed node_info final_exposed node_info improvement node_info limiting_factor node_info moves node_info grouped node_info grouped_info snode node_info node_stats items importlib util find_spec tabulate tabulate tabulate reorder_log_str += tabulate rows headers=headers reorder_log_str += Please ` pip install tabulate ` nicely render overlap stats \n reorder_log_str += str headers + \n reorder_log_str += \n join map str rows new_snodes = _group_nodes _head None assert len new_snodes == original_snodes_num new_peak_memory _ _ _ = estimate_peak_memory_allocfree new_snodes name_to_freeable_input_buf graph_outputs reorder_log_str += f \n peak_memory_before peak_memory reorder_log_str += f \n peak_memory_after new_peak_memory overlap_log info reorder_log_str trace_structured artifact metadata_fn=lambda name reorder_communication_preserving_peak_memory encoding string payload_fn=lambda reorder_log_str new_snodes stats _schedule_for_comm snodes list BaseSchedulerNode raise_comms bool sink_waits bool reorder_for_overlap bool - list BaseSchedulerNode Schedule ` snodes ` various comm optimization objectives Args snodes nodes scheduled raise_comms whether greedily schedule collectives early possible sink_wait whether greedily schedule waits late possible reorder_compute_for_overlap whether reorder compute nodes optimize compute communication overlapping Returns The new schedule order Some notes synergy between different options - ` raise_comms ` provides more overlapping oppurtunies ` reorder_compute_for_overlap ` - When both ` raise_comms ` ` sink_waits ` ` True ` ` raise_comms ` prioritized We assign each node tuple scores score_ score_ score_ decreasing importance lower value indicating higher ranking - score_ lowest comm_idx among comm nodes node blocks If node doesn t block any comm nodes its score_ set sys maxsize This score ensures comm nodes get scheduled early possible - score_ node wait node otherwise This score ensures wait nodes deferred late possible - score_ index node original topological order This score provides stability case ties When only raise_comms True only score_ score_ considered When only sink_waits True only score_ score_ considered When neither True original order yielded buf_name_to_snode = name_to_fused_node = scores_ scores_ scores_ = idx snode enumerate snodes buf_name snode get_buffer_names buf_name_to_snode buf_name = snode op_name snode get_operation_names name_to_fused_node op_name = snode name_to_fused_node snode get_name = snode node_name = snode get_name scores_ node_name = sys maxsize scores_ node_name = scores_ node_name = idx comm_idx = snode snodes raise_comms contains_collective snode scores_ snode get_name = comm_idx ancestor snode ancestors anc_fused_name = name_to_fused_node ancestor get_name scores_ anc_fused_name = min scores_ anc_fused_name comm_idx comm_idx += sink_waits contains_wait snode scores_ snode get_name = Runnable __init__ snode - None snode = snode name = next iter snode get_operation_names fused_name = name_to_fused_node name get_name score = scores_ fused_name scores_ fused_name scores_ fused_name __lt__ other score other score unmet_deps dict BaseSchedulerNode OrderedSet str = snode OrderedSet dep name dep snode unmet_dependencies snode snodes ready list Runnable = buffer_users dict str OrderedSet BaseSchedulerNode = defaultdict OrderedSet snode_to_cost = snode estimate_op_runtime snode snode snodes snode deps unmet_deps items len deps == heapq heappush ready Runnable snode dep deps buffer_users dep add snode scheduled = schedule snode Schedules ` snode ` put all unblocked nodes onto ready queue scheduled append snode buf_name snode get_buffer_names snode buffer_users buf_name unmet_deps snode remove buf_name len unmet_deps snode == heapq heappush ready Runnable snode get_overlapping_candidate Return next node ready queue s neither collective wait candidates = x x ready contains_collective x snode contains_wait x snode len candidates == None min candidates key=lambda x x score schedule_collective_for_overlap snode Schedules collective node ` snode ` along one more compute nodes overlap The strategy described comment ` reorder_compute_for_overlap ` assert contains_collective snode schedule snode collective_cost = snode_to_cost snode while collective_cost candidate = get_overlapping_candidate None ready remove candidate schedule candidate snode collective_cost -= snode_to_cost candidate snode heapq heapify ready while ready snode = heapq heappop ready snode reorder_for_overlap contains_collective snode schedule_collective_for_overlap snode schedule snode deps unmet_deps values assert len deps == f Detected unscheduled nodes Nodes unmet dependencies unmet_deps scheduled decide_global_ordering_of_comms nodes list BaseSchedulerNode name_to_buf name_to_fused_node - list BaseSchedulerNode Decide global ordering comms just enforcing ordering s input graph might same ordering eager mode program TODO Come up better approach torch distributed is_available nodes comm_nodes = n n nodes contains_collective n i range len comm_nodes Enforce ordering making previous comm ` WeakDep ` dependency next comm mutating_buf = next iter comm_nodes i get_buffer_names buf comm_nodes i - get_buffer_names comm_nodes i add_fake_dep WeakDep buf mutating_buf=mutating_buf is_fake=True nodes dataclass SinkWaitInfo grouped int = grouped_info str = moves int = moves_info str = limiting_factor str = None _sink_waits_iterative_internal snodes list BaseSchedulerNode - tuple list BaseSchedulerNode dict BaseSchedulerNode SinkWaitInfo torch _inductor scheduler GroupedSchedulerNode original_snodes_num = len snodes original_snodes_num == snodes graph_inputs OrderedSet str = OrderedSet V graph graph_inputs keys graph_outputs OrderedSet str = OrderedSet V graph get_output_names peak_memory _curr_memory snodes_allocfree buf_to_snode_last_use name_to_freeable_input_buf = _initialize_memory_tracking snodes graph_inputs graph_outputs _prev _next _head = _initialize_double_linked_list snodes stats dict BaseSchedulerNode SinkWaitInfo = _group_nodes head Optional BaseSchedulerNode tail Optional BaseSchedulerNode - list BaseSchedulerNode ret = n = head while True n None ret append n n == tail break n = _next n type ignore index ret _calculate_potential_peak_memory candidate group_ns group_n_to_bufs_after_swap_dealloc_instead_of_candidate pre_group_mem = _curr_memory group_head - snodes_allocfree group_head size_alloc Stash memory tracing updates recompute them after swap _post_alloc_update dict BaseSchedulerNode int = _size_free_delta_update dict BaseSchedulerNode int = potential_peak = group_n_to_bufs_after_swap_dealloc_instead_of_candidate Not accounting buffers liveliness change potential_peak = max group_peak_memory + candidate_delta_mem pre_group_mem + candidate_allocfree size_alloc potential_peak _post_alloc_update _size_free_delta_update candidate_post_alloc = pre_group_mem + candidate_allocfree size_alloc _post_alloc_update candidate = candidate_post_alloc potential_peak = candidate_post_alloc candidate_size_free_to_move = sum buf mpi_buffer size_free type ignore attr-defined buf itertools chain from_iterable group_n_to_bufs_after_swap_dealloc_instead_of_candidate values _size_free_delta_update candidate = -candidate_size_free_to_move delta_mem = candidate_delta_mem + candidate_size_free_to_move gn gns gn_post_alloc = _curr_memory gn + delta_mem _post_alloc_update gn = gn_post_alloc potential_peak = max potential_peak gn_post_alloc gn_size_free_to_add = gn group_n_to_bufs_after_swap_dealloc_instead_of_candidate bufs = group_n_to_bufs_after_swap_dealloc_instead_of_candidate gn buf bufs gn_size_free_to_add += buf mpi_buffer size_free _size_free_delta_update gn = gn_size_free_to_add delta_mem -= gn_size_free_to_add potential_peak _post_alloc_update _size_free_delta_update _perform_double_linked_list_swap candidate group_head group_tail group_head_prev - - candidate - - group_head group_tail - - candidate_next group_head_prev = _prev group_head group_head_prev _next group_head_prev = candidate _prev candidate = group_head_prev candidate_next = _next candidate candidate_next _prev candidate_next = group_tail _next group_tail = candidate_next _prev group_head = candidate _next candidate = group_head nonlocal _head group_head == _head _head = candidate _update_memory_tracking_after_swap candidate gns group_n_to_bufs_after_swap_dealloc_instead_of_candidate _post_alloc_update _size_free_delta_update group_head = gns pre_group_mem = _curr_memory group_head - snodes_allocfree group_head size_alloc group_n_to_bufs_after_swap_dealloc_instead_of_candidate candidate_post_alloc = pre_group_mem + candidate_allocfree size_alloc _curr_memory candidate = candidate_post_alloc candidate_post_alloc - candidate_allocfree size_free gn gns cm = _curr_memory gn _curr_memory gn = cm + candidate_delta_mem cm + candidate_delta_mem n candidate gns post_alloc = _post_alloc_update n snodes_allocfree n size_free += _size_free_delta_update n _curr_memory n = post_alloc post_alloc - snodes_allocfree n size_free curr = snodes - processed_waits = OrderedSet type ignore var-annotated debug_iterative_memory_recompute = config reorder_iterative_debug_memory_recompute debug_num_sink_waits_to_reorder Optional int = config sink_waits_iterative_debug_limit_to_sink iterative_recompute_error = False while _prev curr None iterative_recompute_error break debug_num_sink_waits_to_reorder None len processed_waits = debug_num_sink_waits_to_reorder break pyrefly ignore bad-argument-type contains_wait curr curr processed_waits processed_waits add curr info = stats curr = SinkWaitInfo candidate = _next curr wait_snode = curr group_head = curr group_tail = curr group_peak_memory = _curr_memory curr while candidate None iterative_recompute_error break gns list BaseSchedulerNode = _group_nodes group_head group_tail group = GroupedSchedulerNode wait_snode scheduler gns temp_grouping=True We can have multiple deps same name As we ignore WeakDep is_fake=True = filter them out first avoid overwriting real dep data_deps = d name d d candidate unmet_dependencies _is_fake_dep d group_outs = group get_outputs data_dep = None o group_outs d = data_deps get o get_name None data_dep = d break If we have data_dep - we can swap = trying group If swap candidate current node both contain collectives = trying group data_dep None both_contain_comms = contains_collective group contains_collective candidate is_groupable snode We do want group collectives reorder them forward contains_collective snode False f candidate contains collective snode get_name contains_gemm_like snode False f candidate contains gemm_like snode get_name True None is_grp grp_reason = is_groupable candidate is_grp group_tail = candidate group_peak_memory = max group_peak_memory _curr_memory candidate info grouped += info grouped_info = _group_names gns candidate = _next candidate continue data_dep None both_contain_comms info limiting_factor = f collective ordering _group_names gns f candidate candidate get_name break info limiting_factor = f data dependency data_dep dep_names list data_deps keys f \n candidate candidate get_name os candidate get_buffer_names f dep gns f \n outs o get_name o group_outs f \n non_group_reason grp_reason break candidate_allocfree SNodeMemory = snodes_allocfree candidate candidate_delta_mem = candidate_allocfree size_alloc - candidate_allocfree size_free group candidate - candidate group Check buffers successors group candidate last successor Buf changes its last use snode It deallocated candidate after swap will deallocated group node group_n_to_bufs_after_swap_dealloc_instead_of_candidate dict BaseSchedulerNode list Union FreeableInputBuffer SchedulerBuffer = defaultdict list buf snode_last_use buf_to_snode_last_use items succ_nodes = buf mpi_buffer succ_nodes snode_last_use = candidate noqa E continue candidate last use buf last_succ_gn = None gn gns gn succ_nodes last_succ_gn = gn last_succ_gn None continue gn has successors buf after potential swap will become last use buf start deallocating buf instead candidate group_n_to_bufs_after_swap_dealloc_instead_of_candidate last_succ_gn append buf potential_peak _post_alloc_update _size_free_delta_update = _calculate_potential_peak_memory candidate gns group_n_to_bufs_after_swap_dealloc_instead_of_candidate potential_peak peak_memory info limiting_factor = f peak memory new potential_peak vs base peak_memory break info moves += info moves_info += f + candidate get_name _perform_double_linked_list_swap candidate group_head group_tail _update_memory_tracking_after_swap candidate gns group_n_to_bufs_after_swap_dealloc_instead_of_candidate _post_alloc_update _size_free_delta_update debug_iterative_memory_recompute comms_debug _debug_iterative_memory_recompute iterative_recompute_error = _debug_iterative_memory_recompute candidate gns _group_names gns _group_nodes _head None name_to_freeable_input_buf graph_outputs peak_memory _curr_memory snodes_allocfree sink_waits_iterative group_n_to_bufs_after_swap_dealloc_instead_of_candidate iterative_recompute_error break candidate = _next group_tail curr = _prev curr type ignore assignment headers = Wait node grouped grouped_info moves moves_info limiting factor rows = node_summary snode info grouped info grouped_info info moves info moves_info info limiting_factor snode info stats items log_str = importlib util find_spec tabulate tabulate tabulate log_str += tabulate rows headers=headers log_str += Please ` pip install tabulate ` nicely render overlap stats \n log_str += str headers + \n log_str += \n join map str rows overlap_log info log_str new_snodes = _group_nodes _head None assert len new_snodes == original_snodes_num new_peak_memory _ _ _ = estimate_peak_memory_allocfree new_snodes name_to_freeable_input_buf graph_outputs log_str += f \n sink_waits_iterative peak_memory_before peak_memory log_str += f \n sink_waits_iterative peak_memory_after new_peak_memory trace_structured artifact metadata_fn=lambda name sink_waits_iterative_info encoding string payload_fn=lambda log_str new_snodes stats sink_waits_iterative snodes list BaseSchedulerNode - list BaseSchedulerNode _sink_waits_iterative_internal snodes estimate_op_runtime snode BaseSchedulerNode - float Returns estimated op runtime nanoseconds ns config estimate_op_runtime == default runtime = snode get_estimated_runtime assert callable config estimate_op_runtime runtime = config estimate_op_runtime snode runtime node_summary snode snodes = snode get_nodes len snodes == detail = isinstance snode node ir ExternKernelOut ir _CollectiveKernel outs_str = f outs o get_name o snode get_outputs ins_str = f ins d name d snode unmet_dependencies detail = f snode get_name snode node python_kernel_name \n outs_str \n ins_str layouts = child node get_output_spec child snode get_nodes out_tensor_info = join f size= layout size stride= layout stride isinstance layout ir Layout layout layouts try node_name = snode node maybe_get_name except AttributeError TODO node_summary written without FusedSchedulerNode mind generally needs hardened node_name = f snode node __class__ __name__ detail out_tensor_info node_name snode get_estimated_runtime f ns Flatten summaries Fused Foreach Grouped nodes summaries = child_snode snodes summaries append node_summary child_snode f snode __class__ __name__ join summaries visualize_overlap order TODO - function probably doesn t do very good job estimating runtime because doesn t carefully model streams overlap For now its mostly useful debug visualization total_est_runtime float = cur_comm_node = None step_log step msg overlap_log debug f step msg noqa G step snode enumerate order cur_comm_node None contains_collective snode total_est_runtime += estimate_op_runtime snode cur_comm_node = snode node is_wait snode node raise AssertionError Wait expected when there no collective running pass exposed compute op total_est_runtime += estimate_op_runtime snode step_log step f node_summary snode cur_comm_node None contains_collective snode total_est_runtime += estimate_op_runtime snode cur_comm_node = snode node step_log step f node_summary snode noqa G is_wait snode node end comm op step_log step f node_summary snode cur_comm_node = None overlapped compute op step_log step f &#124; node_summary snode overlap_log debug f Est runtime ms total_est_runtime noqa G reorder_compute_and_comm_for_overlap snodes list BaseSchedulerNode - list BaseSchedulerNode order = snodes graph_inputs OrderedSet str = OrderedSet V graph graph_inputs keys graph_outputs OrderedSet str = OrderedSet V graph get_output_names p config reorder_for_compute_comm_overlap_passes isinstance p str p globals p = globals p builtin pass assert callable p f Invalid reorder_compute_and_comm_for_overlap pass p callable peak_memory _ = estimate_peak_memory snodes get_freeable_input_buf snodes graph_inputs graph_outputs torch distributed get_rank == overlap_log debug f ==== Visualize overlap before reordering pass p peak_memory= ==== noqa G try visualize_overlap order except Exception e overlap_log debug exc_info=e t = time time order = p order type ignore operator t = time time - t torch distributed get_rank == overlap_log debug f ==== Visualize overlap after reordering pass p ran t sec ==== noqa G try visualize_overlap order except Exception e overlap_log debug exc_info=e peak_memory _ = estimate_peak_memory snodes get_freeable_input_buf snodes graph_inputs graph_outputs print f final peak_memory= pyrefly ignore bad-return order remove_fsdp _unsharded_param_graph_input_usage graph torch fx Graph This FX graph pass replaces uses FSDP unsharded params their corresponding graph intermediates fsdp copy_ into unsharded params original graph NOTE Can only apply pass any FSDP unsharded params have pattern repetition ` resize_ full - copy_ - resize_ ` Because partial-graph case where ` resize_ full - copy_ ` one graph ` resize_ ` another graph we can t remove these resize copy ops thus we will have worse performance there In other words do we try remove all resize_ full - copy_ - resize_ nodes unsharded param actually per-unsharded-param decision since each unsharded param we look its resize sequence pattern ` check_resize_pattern ` determine its set resize copy nodes can removed node_list = list graph nodes Find all graph inputs their resize counts graph_input_to_resized_to_full_node_idxes = defaultdict list graph_input_to_resized_to_ _node_idxes = defaultdict list idx node enumerate node_list node op == call_function node target torch ops inductor resize_storage_bytes_ default assert node args op == placeholder f \ Resize can only operate graph inputs got node which resizing non-graph-input node args graph_input = node args new_size = node args new_size graph_input_to_resized_to_full_node_idxes graph_input append idx graph_input_to_resized_to_ _node_idxes graph_input append idx check_resize_pattern graph_input Check number resize-to-full resize-to- nodes equal each resize-to-full resize-to- pair resize-to-full node always happens before resize-to- node This precondition being able remove all resize copy nodes specific unsharded param resized_to_full_idxes = graph_input_to_resized_to_full_node_idxes get graph_input resized_to_ _idxes = graph_input_to_resized_to_ _node_idxes get graph_input len resized_to_full_idxes = len resized_to_ _idxes log warning f Unequal number resize-to-full resize-to- nodes graph input graph_input len resized_to_full_idxes vs len resized_to_ _idxes Skipping ` remove_fsdp _unsharded_param_graph_input_usage ` FX graph pass noqa G False Check sequence resize_to_full - resize_to_ + resize_to_full_idx resize_to_ _idx zip resized_to_full_idxes resized_to_ _idxes resize_to_full_idx = resize_to_ _idx log warning f For graph input graph_input resize-to-full node node_list resize_to_full_idx index resize_to_full_idx happens after resize-to- node node_list resize_to_ _idx index resize_to_ _idx Skipping ` remove_fsdp _unsharded_param_graph_input_usage ` FX graph pass unsharded param noqa G False True Find all eligible unsharded params their corresponding graph intermediates unsharded_param_to_fsdp_copy_node_idxes = defaultdict list idx node enumerate node_list node op == call_function node target torch ops fsdp copy_ default fsdp_copy_node = node unsharded_param = node args assert unsharded_param op == placeholder f Assumed all FSDP ` unsharded_param ` s graph input s true Offending node unsharded_param Graph graph check_resize_pattern unsharded_param unsharded_param_to_fsdp_copy_node_idxes unsharded_param append idx is_allowed_mutation node node target torch ops fsdp copy_ default node target torch ops inductor resize_storage_bytes_ default is_node_mutating_unsharded_param_or_its_alias node unsharded_params Check whether node mutating any unsharded params their aliases mutated_arg_idxes = i i x enumerate node target _schema arguments x alias_info None x alias_info is_write isinstance node target torch _ops OpOverload mutated_node_arg_storages = OrderedSet StorageWeakRef node args i meta val untyped_storage i mutated_arg_idxes storages_of_unsharded_params = OrderedSet StorageWeakRef unsharded_param meta val untyped_storage unsharded_param unsharded_params len mutated_node_arg_storages storages_of_unsharded_params Check no user mutation any unsharded_param node node_list node op == call_function isinstance node target torch _ops OpOverload node target _schema is_mutable is_allowed_mutation node assert is_node_mutating_unsharded_param_or_its_alias node unsharded_param_to_fsdp_copy_node_idxes keys f \ User mutation FSDP unsharded param allowed when Traceable FSDP used Violating node node For each ` fsdp copy_ unsharded_param Y ` replace downstream usage ` unsharded_param ` ` Y ` NOTE Because layer reuse use case there could multiple ` fsdp copy_ ` same ` unsharded_param ` graph input e g ` ` ` fsdp_copy_ = fsdp copy_ unsharded_param_ Y use unsharded_param_ - Subgraph fsdp_copy_ = fsdp copy_ unsharded_param_ Y use unsharded_param_ - Subgraph fsdp_copy_ = fsdp copy_ unsharded_param_ Y use unsharded_param_ - Subgraph ` ` ` We must do replacement only within each subgraph unsharded_param fsdp_copy_node_idxes unsharded_param_to_fsdp_copy_node_idxes items i fsdp_copy_node_idx enumerate fsdp_copy_node_idxes fsdp_copy_node = node_list fsdp_copy_node_idx assert fsdp_copy_node args unsharded_param _ replacement = fsdp_copy_node args subgraph_start_idx exclusive subgraph_start_idx = fsdp_copy_node_idx + subgraph_end_idx exclusive also intentionally don t replace args op subgraph_end_idx = fsdp_copy_node_idxes i + i len fsdp_copy_node_idxes - len node_list - subgraph_nodes = node_list subgraph_start_idx subgraph_end_idx assert any is_node_mutating_unsharded_param_or_its_alias node unsharded_param node subgraph_nodes f \ Assumed no ops mutating unsharded param unsharded_param subgraph subgraph_nodes s true Graph graph node subgraph_nodes node op == call_function unsharded_param node args node target = torch ops inductor resize_storage_bytes_ default TODO yf implement replacement kwargs new_args = tuple replacement arg unsharded_param arg arg node args node args = new_args Delete ` fsdp copy_ unsharded_param Y ` nodes fsdp_copy_node_idxes unsharded_param_to_fsdp_copy_node_idxes values fsdp_copy_node_idx fsdp_copy_node_idxes fsdp_copy_node = node_list fsdp_copy_node_idx graph erase_node fsdp_copy_node Delete ` resize_ unsharded_param ` nodes node node_list node op == call_function node target torch ops inductor resize_storage_bytes_ default node args unsharded_param_to_fsdp_copy_node_idxes graph erase_node node reinplace_fsdp_all_gather graph torch fx Graph - None try torch distributed fsdp _fully_shard _fsdp_collectives assert torch distributed is_available Assert existence these ops assert torch ops _c d_functional all_gather_into_tensor torch ops _c d_functional all_gather_into_tensor_out except ImportError AttributeError AssertionError pattern_matcher CallFunction KeywordArg Match PatternMatcherPass register_graph_pattern all_gather_copy_in = torch ops fsdp all_gather_copy_in default getitem = all_gather_copy_in getitem_ = all_gather_copy_in optional all_gather_into_tensor = torch ops _c d_functional all_gather_into_tensor default getitem - all_gather_copy_in = torch ops fsdp all_gather_copy_in default getitem = all_gather_copy_in getitem_ = all_gather_copy_in all_gather_into_tensor = torch ops _c d_functional all_gather_into_tensor_out default getitem out=getitem_ remove_unused_getitem g Remove ` getitem_X = all_gather_copy_in ` which never used node_list = list g nodes n node_list n target operator getitem n args target torch ops fsdp all_gather_copy_in default n args == g erase_node n graph_pass = PatternMatcherPass register_graph_pattern CallFunction torch ops _c d_functional all_gather_into_tensor default CallFunction operator getitem CallFunction torch ops fsdp all_gather_copy_in default KeywordArg all_gather_inputs KeywordArg all_gather_output KeywordArg inp_split_sizes KeywordArg all_gather_input_numel KeywordArg rank KeywordArg item_idx KeywordArg group_size KeywordArg group_name pyrefly ignore bad-argument-type pass_dict=graph_pass extra_check=lambda match match kwargs item_idx == reinplace_all_gather match Match args kwargs repl args copy_in_args = args - group_size = args - group_name = args - all_gather_copy_in = torch ops fsdp all_gather_copy_in default copy_in_args getitem = all_gather_copy_in getitem_ = all_gather_copy_in all_gather_into_tensor = torch ops _c d_functional all_gather_into_tensor_out default getitem group_size group_name out=getitem_ all_gather_into_tensor match replace_by_example pyrefly ignore bad-argument-type repl kwargs all_gather_inputs kwargs all_gather_output kwargs inp_split_sizes kwargs all_gather_input_numel kwargs rank kwargs group_size kwargs group_name remove_unused_getitem graph graph_pass apply graph type ignore arg-type get_op_idx snode assert isinstance snode torch _inductor scheduler FusedSchedulerNode torch _inductor scheduler GroupedSchedulerNode int snode get_name enforce_comm_ordering_for_fsdp snodes list torch _inductor scheduler BaseSchedulerNode name_to_buf dict str torch _inductor scheduler SchedulerBuffer name_to_fused_node dict str BaseSchedulerNode - list torch _inductor scheduler BaseSchedulerNode scheduler new_order list BaseSchedulerNode = scheduled = OrderedSet Any ag_exists = False rs_exists = False ag_grouped_node_to_wait_grouped_node = rs_grouped_node_to_wait_grouped_node = snode_name_to_final_snode = _create_group_node snodes_to_group group_node = scheduler GroupedSchedulerNode create snodes_to_group snode snodes_to_group snode_name_to_final_snode snode get_name = group_node snode_name_to_final_snode group_node get_name = group_node group_node Create grouped nodes specific sets ops snode snodes Case Handle AllGather is_collective snode node op=torch ops _c d_functional all_gather_into_tensor_out default any is_fallback_op name_to_fused_node x node torch ops fsdp all_gather_copy_in default x snode ancestors ag_exists = True ag_snode = snode ag_related_snode_set OrderedSet scheduler BaseSchedulerNode = OrderedSet Find cast + copy_in + getitem + all_gather code block find_recursive_deps_of_node ag_snode ag_related_snode_set name_to_buf name_to_fused_node Find all_gather + all_gather_wait_tensor + copy_out code block allowed_ops = OrderedSet torch ops _c d_functional all_gather_into_tensor_out default torch ops _c d_functional wait_tensor default torch ops fsdp split_with_sizes_copy default find_recursive_users_of_node ag_snode ag_related_snode_set name_to_buf name_to_fused_node criteria_cb=lambda x isinstance x scheduler NopKernelSchedulerNode isinstance x scheduler ExternKernelSchedulerNode x node op_overload allowed_ops type ignore union-attr sort nodes original operation order ag_related_snodes = sorted ag_related_snode_set key=lambda x get_op_idx x In reuse layer case some ops nd all-gather code block could also depend ops st all-gather code block we don t want group them together end_idx_of_current_ag_block = len ag_related_snodes copy_out_count = i range len ag_related_snodes cur_snode = ag_related_snodes i is_fallback_op cur_snode node torch ops fsdp split_with_sizes_copy default copy_out_count += copy_out_count end_idx_of_current_ag_block = i break ag_related_snodes = ag_related_snodes end_idx_of_current_ag_block Group cast + copy_in + getitem + all_gather into one GroupedSchedulerNode wait_node_idx = None i range len ag_related_snodes - isinstance ag_related_snodes i + node ir _WaitKernel wait_node_idx = i + break assert wait_node_idx None ag_group_node = _create_group_node ag_related_snodes wait_node_idx Group all_gather_wait_tensor + copy_out into one GroupedSchedulerNode ag_wait_group_node = _create_group_node ag_related_snodes wait_node_idx ag_grouped_node_to_wait_grouped_node ag_group_node = ag_wait_group_node Case Handle ReduceScatter is_fallback_op snode node torch ops fsdp chunk_cat default rs_exists = True rs_snode = snode Find reduce_scatter copy-in + reduce_scatter comm + reduce_scatter wait code block rs_related_snode_set OrderedSet scheduler BaseSchedulerNode = OrderedSet find_recursive_users_of_node rs_snode rs_related_snode_set name_to_buf name_to_fused_node sort nodes original operation order rs_related_snodes = sorted rs_related_snode_set key=lambda x get_op_idx x Group reduce_scatter copy-in + reduce_scatter comm into one GroupedSchedulerNode wait_node_idx = None i range len rs_related_snodes - isinstance rs_related_snodes i + node ir _WaitKernel wait_node_idx = i + break assert wait_node_idx None rs_group_node = _create_group_node rs_related_snodes wait_node_idx Group reduce_scatter wait + related output nodes into one GroupedSchedulerNode rs_wait_group_node = _create_group_node rs_related_snodes wait_node_idx rs_grouped_node_to_wait_grouped_node rs_group_node = rs_wait_group_node assert len snode_name_to_final_snode ag_exists assert len ag_grouped_node_to_wait_grouped_node rs_exists assert len rs_grouped_node_to_wait_grouped_node Build new node schedule taking GroupedSchedulerNode into account snode snodes snode get_name snode_name_to_final_snode snode = snode_name_to_final_snode snode get_name snode scheduled continue new_order append snode scheduled add snode Enforce AllGather ordering previous AllGather s wait then copy_out group node must run before next AllGather s copy_in then AG group node prev_ag_wait = None ag_group_node wait_group_node ag_grouped_node_to_wait_grouped_node items prev_ag_wait None mutating_buf = next iter ag_group_node get_buffer_names o prev_ag_wait get_outputs ag_group_node add_fake_dep WeakDep o get_name mutating_buf=mutating_buf is_fake=True prev_ag_wait = wait_group_node Enforce ReduceScatter ordering previous ReduceScatter s wait group node must run before next ReduceScatter s copy_in then RS group node prev_rs_wait = None rs_group_node wait_group_node rs_grouped_node_to_wait_grouped_node items prev_rs_wait None mutating_buf = next iter rs_group_node get_buffer_names o prev_rs_wait get_outputs rs_group_node add_fake_dep WeakDep o get_name mutating_buf=mutating_buf is_fake=True prev_rs_wait = wait_group_node new_order type ignore return-value