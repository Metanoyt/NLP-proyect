os sys os environ TORCH_LOGS = inductor itertools logging time abc abstractmethod collections defaultdict collections abc Callable dataclasses asdict dataclass field typing Any Optional tabulate tabulate tqdm tqdm triton testing do_bench torch torch _inductor config inductor_config torch testing _internal inductor_utils _quantize_rowwise log logging Logger = logging getLogger __name__ inductor_config autotune_num_choices_displayed = None force autotuning reuse compilation artifacts inductor_config autotune_local_cache = False uncomment better debugging inductor_config force_disable_caches = True USE_FAST_ACCUM = True UNITS = name forward_time us teraflops TFLOPS compilation_time s PERF_OVER_ATEN_STR str = perf_over_aten OP_NAMES = mm addmm bmm _scaled_mm SHAPES = M N K BATCH_SIZES = For non-bmm testing still need specify something DTYPES = torch float torch bfloat torch float _e m fn triton knobs ENABLE_PERSISTENT_TMA_MATMULS = False True cutlass knobs CUTLASS_INSTANTIATION_LEVELS = benchmark_torch_function_in_microseconds func Callable args kwargs - float do_bench lambda func args kwargs warmup= rep= e dataclass frozen=True kw_only=True ExperimentConfig max_autotune bool = True coordinate_descent_tuning bool = True max_autotune_gemm_backends str = ATEN abstractmethod name - str pass to_options - dict str Any max_autotune max_autotune coordinate_descent_tuning coordinate_descent_tuning max_autotune_gemm_backends max_autotune_gemm_backends dataclass frozen=True kw_only=True AtenExperimentConfig ExperimentConfig name - str aten dataclass frozen=True kw_only=True CutlassExperimentConfig ExperimentConfig cutlass_instantiation_level str name - str level_name = cutlass_instantiation_level cutlass_instantiation_level = default f cutlass_lvl_ level_name to_options - dict str Any super to_options cuda cutlass_instantiation_level cutlass_instantiation_level dataclass frozen=True kw_only=True TritonExperimentConfig ExperimentConfig enable_persistent_tma_matmul bool = False name - str enable_persistent_tma_matmul triton_persistent_tma triton to_options - dict str Any super to_options triton enable_persistent_tma_matmul enable_persistent_tma_matmul dataclass frozen=True kw_only=True ExperimentGroupConfig op_name str shape tuple int int int dtype torch dtype batch_size int experiments list ExperimentConfig = field default_factory=list name - str M N K = shape B = batch_size sizes = f BS B M x K K x N op_name == bmm f M x K K x N f op_name sizes dtype dataclass frozen=True kw_only=True ExperimentResults name str forward_time float teraflops float compilation_time float asdict asdict dataclass frozen=True kw_only=True ExperimentGroup config ExperimentGroupConfig results list ExperimentResults = field default_factory=list get_inputs config ExperimentGroupConfig - tuple torch Tensor op_name = config op_name M N K = config shape batch_size = config batch_size dtype = config dtype device = torch device cuda op_name == mm A = torch randn M K dtype=dtype device=device B = torch randn N K dtype=dtype device=device t A B op_name == addmm A = torch randn M K dtype=dtype device=device B = torch randn N K dtype=dtype device=device t C = torch randn N dtype=dtype device=device C A B op_name == bmm A = torch randn batch_size M K dtype=dtype device=device B = torch randn batch_size N K dtype=dtype device=device permute A B op_name == _scaled_mm For _scaled_mm we only support fp e m rowwise scaling dtype = torch float _e m fn raise ValueError f _scaled_mm only supports fp e m got dtype Create input tensors bfloat first then quantize fp input_dtype = torch bfloat x = torch randn M K dtype=input_dtype device=device w = torch randn N K dtype=input_dtype device=device Quantize using rowwise scaling w_fp w_inverse_scale = _quantize_rowwise w dtype w_t_fp = w_fp t w_inverse_scale = w_inverse_scale t scale_b should N x_fp x_inverse_scale = _quantize_rowwise x dtype Return inputs _scaled_mm input weight_t scale_a scale_b bias out out_dtype use_fast_accum x_fp w_t_fp x_inverse_scale w_inverse_scale None None torch bfloat USE_FAST_ACCUM raise ValueError f Unknown op op_name run_single_experiment_group group_config ExperimentGroupConfig - list ExperimentResults inputs = get_inputs group_config op = getattr torch group_config op_name results = config group_config experiments torch _dynamo reset torch _inductor utils clear_caches compiled_op = torch compile op options=config to_options start_time = time perf_counter try _ = compiled_op inputs except Exception e traceback log warning f Benchmark config config name failed e noqa G f traceback traceback format_exc results append ExperimentResults name=config name forward_time=float inf teraflops= compilation_time=float inf continue compilation_time = time perf_counter - start_time forward_time = benchmark_torch_function_in_microseconds compiled_op inputs flops = calculate_flops group_config op_name group_config shape group_config batch_size teraflops = flops forward_time e- e results append ExperimentResults name=config name forward_time=forward_time teraflops=teraflops compilation_time=compilation_time results generate_experiment_groups op_names list str shapes list tuple int int int dtypes list torch dtype enable_persistent_tma_matmuls list bool cutlass_instantiation_levels list str batch_sizes list int - list ExperimentGroupConfig groups = op_name shape dtype batch_size itertools product op_names shapes dtypes batch_sizes group = ExperimentGroupConfig op_name=op_name shape=shape dtype=dtype batch_size=batch_size experiments = generate_experiment_configs enable_persistent_tma_matmuls cutlass_instantiation_levels group experiments extend experiments groups append group groups generate_experiment_configs enable_persistent_tma_matmuls list bool cutlass_instantiation_levels list str - list ExperimentConfig configs = add aten configs configs append AtenExperimentConfig max_autotune_gemm_backends= ATEN add triton configs enable_persistent_tma_matmul enable_persistent_tma_matmuls configs append TritonExperimentConfig max_autotune_gemm_backends= TRITON enable_persistent_tma_matmul=enable_persistent_tma_matmul add cutlass configs cutlass_instantiation_level cutlass_instantiation_levels configs append CutlassExperimentConfig max_autotune_gemm_backends= CUTLASS cutlass_instantiation_level=cutlass_instantiation_level configs calculate_table_data results list ExperimentResults - dict table_data = defaultdict list aten_perf Optional float = None experiment_result results key value experiment_result asdict items assert key UNITS f Unknown key key table_data key + UNITS key append value experiment_result name == aten aten_perf = experiment_result forward_time table_data PERF_OVER_ATEN_STR append NA aten_perf None perf_over_aten = experiment_result forward_time - aten_perf aten_perf table_data PERF_OVER_ATEN_STR append perf_over_aten fallback case aten experiment group table_data PERF_OVER_ATEN_STR append NA table_data calculate_flops op_name str shape tuple int int int batch_size int - int Calculate number floating point operations based operation type shape M N K = shape op_name == bmm batch_size M N K op_name == addmm M N K + M N op_name == _scaled_mm M N K M N K get_printable_results experiment_groups list ExperimentGroup - list str edge_over_aten = defaultdict list output = experiment_group experiment_groups group_config_name = experiment_group config name output append f \nExperiment group group_config_name table_data = calculate_table_data experiment_group results name edge zip table_data name table_data PERF_OVER_ATEN_STR edge_over_aten name append edge output append tabulate table_data headers= keys tablefmt= pretty floatfmt= f aten edge_over_aten output append \nAverage edge over aten max -edge higher better name edge_over_aten name = aten values = max -v v edge_over_aten name v = float inf v = NA valid_count = len values average_edge = sum values valid_count values No valid data output append f name average_edge valid_count valid values output append \n \n join output main seed = torch manual_seed seed results = log info Starting benchmarking configs = list generate_experiment_groups OP_NAMES SHAPES DTYPES ENABLE_PERSISTENT_TMA_MATMULS CUTLASS_INSTANTIATION_LEVELS BATCH_SIZES i group_config enumerate tqdm configs group_results = run_single_experiment_group group_config noqa G results append ExperimentGroup config=group_config results=group_results sys stderr write f \nINTERMEDIATE results i + len configs \n + get_printable_results results print \nFINAL results print get_printable_results results __name__ == __main__ main