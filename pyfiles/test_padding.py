Owner s module inductor copy functools os unittest torch torch nn Tensor torch _dynamo convert_frame maybe_cprofile torch _dynamo device_interface get_interface_for_device torch _dynamo testing rand_strided reduce_to_scalar_loss torch _inductor config ir metrics torch _inductor fx_passes pad_mm pad_mm_pass torch _inductor runtime benchmarking benchmarker torch _inductor test_case run_tests TestCase torch _inductor utils ceildiv run_and_get_code torch testing _internal common_utils instantiate_parametrized_tests parametrize serialTest torch testing _internal inductor_utils GPU_TYPE HAS_GPU requires_gpu DO_PERF_TEST = os environ get DO_PERF_TEST == DO_ACC_TEST = os environ get DO_ACC_TEST == WITH_STACK = os environ get WITH_STACK == USE_CUDA_GRAPHS = os environ get USE_CUDA_GRAPHS == try transformers noqa F HAS_TRANSFORMER = True except ImportError HAS_TRANSFORMER = False get_optim m torch optim Adam m parameters lr= capturable=True foreach=True gen_transformer_inputs vocab_size bs seq_length geninp torch randint vocab_size bs seq_length dtype=torch int requires_grad=False input_dict = input_ids geninp labels geninp input_dict get_padded_stride shape alignment_bytes pad_output itemsize align = alignment_bytes itemsize new_strides = _ range len shape new_strides len shape - = i range len shape - - stride = shape i new_strides i pad_output stride align = stride = stride + align - align align new_strides i - = stride tuple new_strides LinearAndSoftmax nn Module It s very common transformer model will do matmul then softmax log_softmax end Creating toy model capture pattern make sure we do proper padding __init__ vocab_size= bias=True The default vocab size BertForMaskedLM We run few test cases good bad vocab_size around Bert s default value super __init__ vocab_size = vocab_size linear = nn Linear vocab_size bias=bias ce = nn CrossEntropyLoss forward x label x = linear x ce x view - vocab_size label view - get_example_inputs batch_size= torch randn batch_size torch randint vocab_size batch_size forward_and_backward_pass m inputs m inputs sum backward config patch benchmark_kernel True triton unique_kernel_names True triton cudagraphs USE_CUDA_GRAPHS requires_gpu TestCaseBase TestCase classmethod setUpClass cls HAS_GPU cls prior_float _matmul_precision = torch get_float _matmul_precision cls prior_default_device = torch get_default_device torch version hip torch set_float _matmul_precision highest torch set_float _matmul_precision high torch set_default_device GPU_TYPE classmethod tearDownClass cls HAS_GPU torch set_float _matmul_precision cls prior_float _matmul_precision torch set_default_device cls prior_default_device cls prior_float _matmul_precision = None cls prior_default_device = None check_close ref act tol= e- type ref __name__ == LongformerMaskedLMOutput ref = ref loss act = act loss type ref __name__ == SequenceClassifierOutput ref = ref logits act = act logits isinstance ref dict loss ref ref = ref loss act = act loss assertTrue torch allclose ref act atol=tol rtol=tol f ref \n ref \nact \n act common_numeric_check f args tol= e- kwargs ref = f args kwargs opt_f = torch compile f act = opt_f args kwargs check_close ref act tol do_profiling f_lhs f_rhs tag_lhs= With padding tag_rhs= Without padding args= kwargs=None kwargs None kwargs = device_interface = get_interface_for_device GPU_TYPE device_interface synchronize torch profiler profile with_stack=WITH_STACK p niter = _ range niter torch profiler record_function tag_lhs f_lhs args kwargs torch profiler record_function tag_rhs f_rhs args kwargs device_interface synchronize profile_path = tmp chrome json p export_chrome_trace profile_path print f Chrome trace written profile_path PerfTestBetweenGoodAndBadShape TestCaseBase unittest skipIf DO_PERF_TEST Perf test enabled test_nobias_LinearAndSoftmax_both_shapes test_LinearAndSoftmax_both_shapes bias=False unittest skipIf DO_PERF_TEST Perf test enabled test_LinearAndSoftmax_both_shapes bias=True Compare perf good bad shape m_bad_shape = LinearAndSoftmax vocab_size= bias=bias inptus_bad_shape = m_bad_shape get_example_inputs m_good_shape = LinearAndSoftmax vocab_size= bias=bias inputs_good_shape = m_good_shape get_example_inputs m_bad_shape_opt = torch compile m_bad_shape m_good_shape_opt = torch compile m_good_shape latency_good_shape = benchmarker benchmark_gpu lambda forward_and_backward_pass m_good_shape_opt inputs_good_shape latency_bad_shape = benchmarker benchmark_gpu lambda forward_and_backward_pass m_bad_shape_opt inptus_bad_shape print f Latency good shape v s bad shape latency_good_shape f ms v s latency_bad_shape f ms unittest skipIf DO_PERF_TEST HAS_TRANSFORMER Perf test enabled test_BertForMaskedLM num_layers= Compare perf between doing padding good shape transformers BertForMaskedLM config_cls = BertForMaskedLM config_class bs = seq_length = create_model vocab_size config = config_cls config num_hidden_layers = num_layers config vocab_size = vocab_size inputs = gen_transformer_inputs config vocab_size bs seq_length model = BertForMaskedLM config optim = get_optim model f inputs optim zero_grad True torch autocast GPU_TYPE pred = model inputs loss = pred loss backward optim step torch compile f inputs f_good_shape inputs_good_shape = create_model f_bad_shape inputs_bad_shape = create_model print benchmark good shape latency_good_shape = benchmarker benchmark_gpu lambda f_good_shape inputs_good_shape print benchmark bad shape latency_bad_shape = benchmarker benchmark_gpu lambda f_bad_shape inputs_bad_shape print f Latency good bad shape latency_good_shape f v s latency_bad_shape f do_profiling lambda f_good_shape inputs_good_shape lambda f_bad_shape inputs_bad_shape tag_lhs= With good shape tag_rhs= With bad shape PerfTestWithAndWithoutPadding TestCaseBase maybe_cprofile run_acc_and_perf_test model inputs perf_inputs=None tol= e- Run accuracy test Also compare perf without comprehensive padding DO_PERF_TEST true perf_inputs None perf_inputs = inputs _process_inputs x args kwargs isinstance x dict x isinstance inputs tuple list x = x x args kwargs = _process_inputs inputs perf_args perf_kwargs = _process_inputs perf_inputs DO_ACC_TEST model eval common_numeric_check model args kwargs tol=tol print Accuracy test skipped model train DO_PERF_TEST print Do performance test get_f m optim f args kwargs optim zero_grad True torch autocast GPU_TYPE pred = m args kwargs loss = reduce_to_scalar_loss pred loss backward optim step f latency_with_padding = None print Benchmark padding config patch comprehensive_padding=True m_copy_with_padding = copy deepcopy model optim_with_padding = get_optim m_copy_with_padding opt_f_with_padding = torch compile get_f m_copy_with_padding optim_with_padding latency_with_padding = benchmarker benchmark_gpu lambda opt_f_with_padding perf_args perf_kwargs latency_without_padding = None print bencmark without padding config patch comprehensive_padding=False m_copy_without_padding = copy deepcopy model optim_without_padding = get_optim m_copy_without_padding opt_f_without_padding = torch compile get_f m_copy_without_padding optim_without_padding latency_without_padding = benchmarker benchmark_gpu lambda opt_f_without_padding perf_args perf_kwargs print f Latency without padding latency_with_padding f v s latency_without_padding f profiling do_profiling opt_f_with_padding opt_f_without_padding args=perf_args kwargs=perf_kwargs test_nvidia_deeprecommender Compared perf without comprehensive padding layer_sizes = x = torch randn layer_sizes Model nn Module __init__ - None super __init__ mod_list = i range len layer_sizes - mod_list append nn Linear layer_sizes i layer_sizes i + mod_list append nn SELU i == mod_list append nn Dropout seq = nn Sequential mod_list forward x seq x m = Model perf_inputs = torch randn layer_sizes run_acc_and_perf_test m x perf_inputs unittest skipIf DO_PERF_TEST HAS_TRANSFORMER Perf test enabled test_longformer bs= transformers AutoConfig AutoModelForMaskedLM config = AutoConfig from_pretrained allenai longformer-base- model = AutoModelForMaskedLM from_config config vocab_size = model config vocab_size seq_length = input_dict = gen_transformer_inputs vocab_size bs seq_length run_acc_and_perf_test model input_dict unittest skipIf DO_PERF_TEST HAS_TRANSFORMER Perf test enabled test_longformer_small_bs The model exists both HF TB In TB uses smaller batch size test_longformer bs= instantiate_parametrized_tests PaddingTest TestCaseBase unittest skipIf DO_PERF_TEST Perf test enabled test_mm_padding_perf naive_mm b b _compute_padding s align s + align - align align - s torch compile pad_mm b align= NOTE function only pad single dimension which good enough testing m_padding = _compute_padding size align k_padding = _compute_padding size align n_padding = _compute_padding b size align pad_mm_pass pad_mm b m_padding k_padding n_padding M K N f naive_mm pad_mm naive_mm naive_mm naive_mm = torch randn M K b = torch randn K N ms = benchmarker benchmark_gpu lambda f b print f MxKxN M x K x N f __name__ ms f ms unittest skipIf DO_PERF_TEST Perf test enabled test_padmm Latency between original matmul padded matmul v s mat _pad = torch randn dtype=torch float mat _pad = torch randn dtype=torch float f mat _pad mat _pad pad_dim x Tensor padded_length int dim int - Tensor pad = x new_zeros x shape dim padded_length x shape dim + torch cat x pad dim=dim torch compile fullgraph=True options= triton cudagraphs False g mat = mat _pad mat = mat _pad mat = pad_dim mat mat = pad_dim mat torch ops aten mm mat mat ori_time = benchmarker benchmark_gpu f pad_time = benchmarker benchmark_gpu g print f Latency between original matmul padded matmul ori_time f v s pad_time f do_profiling f g No MM Padding With mm padding unittest skipIf DO_PERF_TEST Perf test enabled test_matmul Latency good bad shapes v s x_good_shape = torch randn dtype=torch float weight_good_shape = torch randn dtype=torch float out_good_shape = torch randn dtype=torch float Using stride does make difference here x_bad_shape = rand_strided device=GPU_TYPE dtype=torch float weight_bad_shape = torch randn dtype=torch float out_bad_shape = torch randn dtype=torch float f x weight out torch mm x weight out=out out f = torch compile functools partial f x_good_shape weight_good_shape out_good_shape f = torch compile functools partial f x_bad_shape weight_bad_shape out_bad_shape latency_good_shape = benchmarker benchmark_gpu f latency_bad_shape = benchmarker benchmark_gpu f print f Latency good bad shapes latency_good_shape f v s latency_bad_shape f do_profiling f f serialTest test_nobias_LinearAndSoftmax_codegen test_LinearAndSoftmax_codegen bias=False test_LinearAndSoftmax_codegen bias=True m_bad_shape = LinearAndSoftmax vocab_size= bias=bias inputs_bad_shape = m_bad_shape get_example_inputs m_bad_shape_opt = torch compile copy deepcopy m_bad_shape _ wrapper_codes = run_and_get_code forward_and_backward_pass m_bad_shape_opt inputs_bad_shape forward_and_backward_pass m_bad_shape inputs_bad_shape assertEqual m_bad_shape linear weight grad m_bad_shape_opt linear weight grad assertTrue len wrapper_codes == one forward one backward forward_wrapper = wrapper_codes make sure load softmax aligned assertTrue tl load in_ptr + r _ + x forward_wrapper f forward_wrapper forward_wrapper DO_PERF_TEST latency = benchmarker benchmark_gpu lambda forward_and_backward_pass m_bad_shape_opt inputs_bad_shape print f latency latency f ms config patch pattern_matcher=False test_attention batch_size seq_len num_heads hidden_size = inv_scale = num_heads hidden_size Attention nn Module __init__ - None super __init__ query = nn Linear hidden_size hidden_size key = nn Linear hidden_size hidden_size value = nn Linear hidden_size hidden_size staticmethod reshape x x view batch_size seq_len num_heads - permute staticmethod cancel_reshape x x permute view batch_size seq_len hidden_size forward x query key value = query x key x value x weights = torch matmul reshape query reshape key permute inv_scale softmax dim=- cancel_reshape torch matmul weights reshape value attn = Attention x = torch randn batch_size seq_len hidden_size common_numeric_check attn x test_view f x x view x = torch randn common_numeric_check f x test_pad_strides Note dim s stride also padded even though its previous value already multiple The reason we padded dim s stride We have correspondingly increase stride dim sizes = in_strides = out_strides = list ir Layout _pad_strides in_strides sizes torch float expected_strides = assertEqual expected_strides out_strides f expected_strides v s out_strides test_pad_strides_skip The padding skipped avoid too much memory overhead sizes = in_strides = out_strides = list ir Layout _pad_strides in_strides sizes torch float expected_strides = assertEqual expected_strides out_strides f expected_strides v s out_strides test_pad_ d_tensor Constructing test case guided fact we don t pad placeholder user visible output s strides Add matmul beginning end so we can pad strides intermediate tensors f x y x = torch matmul x y x = x + torch matmul x y x = torch randn y = torch randn common_numeric_check f x y tol= e- assertTrue metrics num_comprehensive_padding test_conv Padding input convolution may cause extra copy kernel being called Check example trace https gist github com shunting ce f d ce fc d faddb x_shape = x = torch randn x_shape padded_stride = ir Layout _pad_strides x stride x shape torch float x = rand_strided x_shape padded_stride device=GPU_TYPE x copy_ x weight = torch randn fun x weight torch convolution x weight stride= padding= dilation= transposed=False output_padding= groups= bias=None ref = fun x weight act = fun x weight check_close ref act DO_PERF_TEST latency_with_padding = benchmarker benchmark_gpu lambda fun x weight latency_without_padding = benchmarker benchmark_gpu lambda fun x weight print f Latency without padding latency_with_padding f v s latency_without_padding f do_profiling lambda fun x weight lambda fun x weight unittest skipIf DO_PERF_TEST Perf test enabled test_cat Compare perf between aten cat compiled cat Latency between eager compiled v s Eager cat can x slower than inductor kernel x = torch randn dtype=torch float f x pad = x new_zeros x size torch cat x pad dim= disable cudagraphs since cudagraphs need copy input which distort latency lot double latency here compiled version config patch triton cudagraphs False opt_f = torch compile f opt_f x eager_time = benchmarker benchmark_gpu lambda f x opt_time = benchmarker benchmark_gpu lambda opt_f x print f Latency between eager compiled eager_time f v s opt_time f do_profiling lambda f x lambda opt_f x Eager Cat Compiled Cat test_pad_channels_last t = torch randn in_strides = t stride out_strides = ir Layout _pad_strides in_strides t shape torch float assertTrue in_strides = out_strides t = t memory_format=torch channels_last in_strides = t stride out_strides = ir Layout _pad_strides in_strides t shape torch float assertTrue in_strides == out_strides parametrize alignment_bytes parametrize shape parametrize dtype torch float torch float test_pad_outputs dtype torch dtype shape tuple int alignment_bytes int Tests padding output tensors specific alignment This enabled config flag func = torch add inputs = tuple torch randn shape dtype=dtype input_idx range Compile run config patch comprehensive_padding True padding_alignment_bytes alignment_bytes padding_stride_threshold pad_outputs True compiled_func = torch compile func compiled_out = compiled_func inputs Check numerics eager_out = func inputs check_close eager_out compiled_out Compute expected padding element_size = torch tensor dtype=dtype element_size assertGreater alignment_bytes element_size assertEqual alignment_bytes element_size alignment_elements = alignment_bytes element_size contiguous_stride = inputs stride expected_stride = dim reversed shape slice_size = dim expected_stride new_stride = alignment_elements ceildiv slice_size alignment_elements expected_stride insert new_stride expected_stride = tuple expected_stride assertNotEqual expected_stride contiguous_stride Check strides assertFalse compiled_out is_contiguous assertEqual compiled_out stride expected_stride parametrize shape alignment_bytes pad_output False True False True test_noop_concat_output_padding shape alignment_bytes pad_output When we generate no-op concat kernel alignment inputs outputs should honored based padding_alignment_bytes get_input size tuple int alignment_bytes int - torch Tensor size_padded = list size elem_size = float pad_elems = alignment_bytes elem_size pad_output size_padded - = size_padded - + pad_elems - pad_elems pad_elems full = torch randn size_padded dtype=torch float view = torch as_strided full size full stride view num_inputs = input_tensors = get_input shape alignment_bytes _ range num_inputs config_patches = compile_threads comprehensive_padding pad_output cpu_backend triton disable_padding_cpu False implicit_fallbacks False inplace_buffers False padding_alignment_bytes alignment_bytes pad_channels_last True pad_outputs True padding_stride_threshold triton prefer_nd_tiling True triton use_block_ptr True triton codegen_upcast_to_fp False unroll_reductions_threshold config patch config_patches compiled = torch compile torch cat _ code = run_and_get_code compiled input_tensors output_shape = shape num_inputs shape output_stride = input_tensors stride output_line = f buf = empty_strided_ GPU_TYPE output_shape output_stride torch float assertTrue output_line code parametrize shape alignment_bytes enable_pad False True False True False True False True test_outer_dynamic_shape_padding shape alignment_bytes enable_pad When only outermost dim dynamic shape output can still padded up based padding configuration num_inputs = input_tensors = torch randn shape dtype=torch float _ range num_inputs config_patches = comprehensive_padding enable_pad pad_dynamic_shapes True cpu_backend triton padding_alignment_bytes alignment_bytes pad_outputs True padding_stride_threshold config patch config_patches torch _dynamo mark_dynamic input_tensors torch _dynamo mark_dynamic input_tensors compiled = torch compile torch add result _ = run_and_get_code compiled input_tensors expected_stride = get_padded_stride result shape alignment_bytes enable_pad result dtype itemsize assertEqual result stride expected_stride parametrize shape perm alignment_bytes enable_pad False True True False False True True False test_perm_outer_dynamic_shape_padding shape perm alignment_bytes enable_pad When only outermost dim dynamic shape output can still padded up based padding configuration Test when occurs after permute op permute_contig x torch permute x perm contiguous num_inputs = input_tensors = torch randn shape dtype=torch float _ range num_inputs config_patches = comprehensive_padding enable_pad pad_dynamic_shapes True cpu_backend triton padding_alignment_bytes alignment_bytes pad_outputs True padding_stride_threshold triton use_block_ptr True config patch config_patches torch _dynamo mark_dynamic input_tensors compiled = torch compile permute_contig result _ = run_and_get_code compiled input_tensors expected_stride = get_padded_stride result shape alignment_bytes enable_pad result dtype itemsize assertEqual result stride expected_stride parametrize shape alignment_bytes enable_pad False True False True False True False True test_dynamic_shape_padding shape alignment_bytes enable_pad When only outermost dim dynamic shape output can still padded up based padding configuration num_inputs = input_tensors = torch randn shape dtype=torch float _ range num_inputs config_patches = comprehensive_padding enable_pad pad_dynamic_shapes enable_pad cpu_backend triton padding_alignment_bytes alignment_bytes pad_outputs True padding_stride_threshold config patch config_patches compiled = torch compile torch add dynamic=True result _ = run_and_get_code compiled input_tensors expected_stride = get_padded_stride result shape alignment_bytes enable_pad result dtype itemsize assertEqual result stride expected_stride __name__ == __main__ HAS_GPU run_tests