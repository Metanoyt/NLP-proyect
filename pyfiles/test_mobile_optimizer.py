Owner s oncall mobile unittest torch torch nn nn torch utils bundled_inputs torch testing _internal common_utils TestCase run_tests skipIfNoXNNPACK torch testing _internal jit_utils get_forward get_forward_graph torch utils mobile_optimizer LintCode generate_mobile_module_lints optimize_for_mobile MobileOptimizerType torch nn functional F torch testing _internal common_quantized override_quantized_engine try torchvision HAS_TORCHVISION = True except ImportError HAS_TORCHVISION = False FileCheck = torch _C FileCheck TestOptimizer TestCase skipIfNoXNNPACK test_optimize_for_mobile batch_size = input_channels_per_group = height = width = output_channels_per_group = groups = kernel_h = kernel_w = stride_h = stride_w = pad_h = pad_w = dilation = input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups strides = stride_h stride_w paddings = pad_h pad_w dilations = dilation dilation conv_weight_shape = output_channels input_channels_per_group kernel_h kernel_w conv_bias_shape = output_channels input_data = torch rand batch_size input_channels height width conv_weight = torch rand output_channels input_channels_per_group kernel_h kernel_w conv_bias = torch rand output_channels result = F conv d input_data conv_weight conv_bias strides paddings dilations groups weight_output_dim = linear_input_shape = result shape linear_weight_shape = weight_output_dim linear_input_shape MyTestModule torch nn Module __init__ - None super __init__ conv_weight = torch nn Parameter torch rand conv_weight_shape conv_bias = torch nn Parameter torch rand conv_bias_shape linear_weight = torch nn Parameter torch rand linear_weight_shape linear_bias = torch nn Parameter torch rand weight_output_dim strides = strides paddings = paddings dilations = dilations groups = groups forward x o = F conv d x conv_weight conv_bias strides paddings dilations groups o = F relu o x = o permute o = F linear x linear_weight linear_bias o = o + x F relu o torch jit export foo x o = F conv d x conv_weight conv_bias strides paddings dilations groups o = F relu o x = o permute o = F linear x linear_weight linear_bias o = o + x F relu o BNTestModule torch nn Module __init__ - None super __init__ conv = torch nn Conv d bn = torch nn BatchNorm d num_features= bn eps = forward x x = conv x x = bn x x data_shape = batch_size input_channels height width input_data = torch normal size=data_shape scripted_model = torch jit script MyTestModule scripted_model eval initial_result = scripted_model input_data initial_foo_result = scripted_model foo input_data optimized_scripted_model = optimize_for_mobile scripted_model preserved_methods= foo optimized_result = optimized_scripted_model input_data optimized_foo_result = optimized_scripted_model foo input_data FileCheck check_not Tensor = aten conv d \ check_not Tensor = prim CallFunction \ check_not prepacked conv d_clamp_prepack \ check_count prepacked conv d_clamp_run exactly=True \ check_not prepacked linear_clamp_prepack \ check_count prepacked linear_clamp_run exactly=True \ check_not aten add \ check_not aten relu \ check_count aten _add_relu exactly=True \ run optimized_scripted_model graph torch testing assert_close initial_result optimized_result rtol= e- atol= e- FileCheck check_not Tensor = aten conv d \ check_not Tensor = prim CallFunction \ check_not prepacked conv d_clamp_prepack \ check_count prepacked conv d_clamp_run exactly=True \ check_not prepacked linear_clamp_prepack \ check_count prepacked linear_clamp_run exactly=True \ check_not aten add \ check_not aten relu \ check_count aten _add_relu exactly=True \ run optimized_scripted_model foo graph torch testing assert_close initial_foo_result optimized_foo_result rtol= e- atol= e- optimization_blocklist_no_prepack = MobileOptimizerType INSERT_FOLD_PREPACK_OPS optimized_scripted_model_no_prepack = optimize_for_mobile scripted_model optimization_blocklist_no_prepack optimized_result_no_prepack = optimized_scripted_model_no_prepack input_data FileCheck check_count Tensor = aten conv d exactly=True \ check_not prepacked linear_clamp_run \ check_not prepacked conv d_clamp_run \ run optimized_scripted_model_no_prepack graph torch testing assert_close initial_result optimized_result_no_prepack rtol= e- atol= e- bn_test_module = BNTestModule bn_scripted_module = torch jit script bn_test_module bn_scripted_module eval assertEqual len torch jit export_opnames bn_scripted_module FileCheck check_count prim CallMethod name= forward exactly=True \ run str get_forward bn_scripted_module _c graph optimization_blocklist_no_prepack = MobileOptimizerType INSERT_FOLD_PREPACK_OPS bn_fold_scripted_module = optimize_for_mobile bn_scripted_module optimization_blocklist_no_prepack assertEqual len torch jit export_opnames bn_fold_scripted_module bn_input = torch rand torch testing assert_close bn_scripted_module bn_input bn_fold_scripted_module bn_input rtol= e- atol= e- optimization_blocklist_no_fold_bn = MobileOptimizerType CONV_BN_FUSION no_bn_fold_scripted_module = optimize_for_mobile bn_scripted_module optimization_blocklist_no_fold_bn FileCheck check_count aten batch_norm exactly=True \ run str get_forward_graph no_bn_fold_scripted_module _c bn_input = torch rand torch testing assert_close bn_scripted_module bn_input no_bn_fold_scripted_module bn_input rtol= e- atol= e- MyMobileOptimizedTagTest torch nn Module __init__ - None super __init__ linear_weight = torch nn Parameter torch rand linear_weight_shape linear_bias = torch nn Parameter torch rand weight_output_dim forward x o = F linear x linear_weight linear_bias F relu o mobile_optimized_tag_module = MyMobileOptimizedTagTest m = torch jit script mobile_optimized_tag_module m eval opt_m = optimize_for_mobile m tag = getattr opt_m mobile_optimized None assertTrue tag MyPreserveMethodsTest torch nn Module __init__ - None super __init__ linear_weight = torch nn Parameter torch rand linear_weight_shape linear_bias = torch nn Parameter torch rand weight_output_dim forward x o = F linear x linear_weight linear_bias F relu o torch jit export preserveThis pass preserve_method_module = MyPreserveMethodsTest m = torch jit script preserve_method_module m eval opt_m = optimize_for_mobile m no_preserveThis = getattr opt_m preserveThis None assertEqual no_preserveThis None opt_m = optimize_for_mobile m preserved_methods= preserveThis preserveThis = getattr opt_m preserveThis None assertNotEqual preserveThis None OptimizeNoForwardTest torch nn Module __init__ - None super __init__ l = nn Linear l = nn Linear d = nn Dropout p= torch jit export foo x x = d F relu l x x = l x x = x + torch ones F relu x input_data = torch ones m = torch jit script OptimizeNoForwardTest m eval initial_result = m foo input_data optimized_scripted_model = optimize_for_mobile m preserved_methods= foo optimized_result = optimized_scripted_model foo input_data FileCheck check_not dropout __ \ check_count aten _add_relu exactly=True \ run optimized_scripted_model foo graph torch testing assert_close initial_result optimized_result rtol= e- atol= e- BNTestNoForwardModule torch nn Module __init__ - None super __init__ conv = torch nn Conv d bn = torch nn BatchNorm d num_features= bn eps = torch jit export foo x x = conv x x = bn x x bn_test_no_forward_module = BNTestNoForwardModule bn_no_forward_scripted_module = torch jit script bn_test_no_forward_module bn_no_forward_scripted_module eval assertEqual len torch jit export_opnames bn_no_forward_scripted_module FileCheck check_count prim CallMethod name= forward exactly=True \ run bn_no_forward_scripted_module foo graph bn_fold_no_forward_scripted_module = optimize_for_mobile bn_no_forward_scripted_module preserved_methods= foo assertEqual len torch jit export_opnames bn_fold_no_forward_scripted_module bn_input = torch rand torch testing assert_close bn_no_forward_scripted_module foo bn_input bn_fold_no_forward_scripted_module foo bn_input rtol= e- atol= e- skipIfNoXNNPACK test_quantized_conv_no_asan_failures There ASAN failures when fold_conv_bn run already quantized conv modules Verifying does happen again qnnpack torch backends quantized supported_engines Child nn Module __init__ - None super __init__ conv = nn Conv d forward x x = conv x x Parent nn Module __init__ - None super __init__ quant = torch ao quantization QuantStub conv = nn Conv d child = Child dequant = torch ao quantization DeQuantStub forward x x = quant x x = conv x x = child x x = dequant x x override_quantized_engine qnnpack model = Parent model qconfig = torch ao quantization get_default_qconfig qnnpack torch ao quantization prepare model inplace=True model torch randn torch ao quantization convert model inplace=True model = torch jit script model line should have ASAN failures optimize_for_mobile model test_generate_mobile_module_lints MyTestModule torch nn Module __init__ - None super __init__ fc = torch nn Linear dropout = torch nn Dropout p= forward inputs out = fc inputs out = dropout out out MyBNModule torch nn Module __init__ - None super __init__ bn = torch nn BatchNorm d affine=True forward inputs bn = bn inputs bn MyBundledInputModule torch nn Module forward inputs inputs get_lint_count_by_type lint_type module_lint_List len lint_dict lint_dict module_lint_List lint_dict name == lint_type name test_module = torch jit script MyTestModule test_module_lint_list = generate_mobile_module_lints test_module assertEqual len test_module_lint_list assertEqual get_lint_count_by_type LintCode BUNDLED_INPUT test_module_lint_list assertEqual get_lint_count_by_type LintCode DROPOUT test_module_lint_list assertEqual get_lint_count_by_type LintCode REQUIRES_GRAD test_module_lint_list bn_module = torch jit script MyBNModule bn_module_lint_list = generate_mobile_module_lints bn_module assertEqual len bn_module_lint_list assertEqual get_lint_count_by_type LintCode BUNDLED_INPUT bn_module_lint_list assertEqual get_lint_count_by_type LintCode BATCHNORM bn_module_lint_list assertEqual get_lint_count_by_type LintCode REQUIRES_GRAD bn_module_lint_list bi_module = torch jit script MyBundledInputModule torch utils bundled_inputs augment_model_with_bundled_inputs bi_module torch tensor bi_module_lint_list = generate_mobile_module_lints bi_module assertEqual len bi_module_lint_list skipIfNoXNNPACK test_preserve_bundled_inputs_methods MyBundledInputModule torch nn Module forward inputs inputs MyIncompleteBundledInputModule torch nn Module forward inputs inputs torch jit export get_all_bundled_inputs pass bi_module = torch jit script MyBundledInputModule module_optim_bi_not_preserved = optimize_for_mobile bi_module Expected False since no bundled inputs methods added assertFalse hasattr module_optim_bi_not_preserved get_all_bundled_inputs hasattr module_optim_bi_not_preserved get_num_bundled_inputs Add bundled inputs methods module torch utils bundled_inputs augment_model_with_bundled_inputs bi_module torch tensor Now they should preserved module_optim_bi_preserved = optimize_for_mobile bi_module All bundled inputs methods preserved assertTrue hasattr module_optim_bi_preserved get_all_bundled_inputs hasattr module_optim_bi_preserved get_num_bundled_inputs bundled_input = module_optim_bi_preserved get_all_bundled_inputs module_optim_bi_preserved bundled_input If all bundled inputs methods present module we will try preserve them unless specified user incomplete_bi_module = torch jit script MyIncompleteBundledInputModule incomplete_bi_module_optim = optimize_for_mobile incomplete_bi_module assertFalse hasattr incomplete_bi_module_optim get_all_bundled_inputs Specifically preserve get_all_bundled_inputs even s only one bundled inputs method available incomplete_bi_module_optim = optimize_for_mobile incomplete_bi_module preserved_methods= get_all_bundled_inputs assertTrue hasattr incomplete_bi_module_optim get_all_bundled_inputs skipIfNoXNNPACK test_hoist_conv_packed_params qnnpack torch backends quantized supported_engines Standalone nn Module __init__ - None super __init__ quant = torch ao quantization QuantStub conv = nn Conv d conv = nn Conv d relu = nn ReLU dequant = torch ao quantization DeQuantStub forward x x = quant x x = conv x x = conv x x = relu x x = dequant x x fuse_model torch ao quantization fuse_modules conv relu inplace=True Child nn Module __init__ - None super __init__ conv = nn Conv d forward x x = conv x x Parent nn Module __init__ - None super __init__ quant = torch ao quantization QuantStub conv = nn Conv d child = Child TODO test nn Sequential after fixed dequant = torch ao quantization DeQuantStub forward x x = quant x x = conv x x = child x x = dequant x x fuse_model pass override_quantized_engine qnnpack _quant_script_and_optimize model model qconfig = torch ao quantization get_default_qconfig qnnpack model fuse_model torch ao quantization prepare model inplace=True model torch randn torch ao quantization convert model inplace=True model = torch jit script model model_optim = optimize_for_mobile model model model_optim basic case m m_optim = _quant_script_and_optimize Standalone FileCheck check_not Conv d = prim GetAttr name= conv \ check_count __torch__ torch classes quantized Conv dPackedParamsBase = prim Constant exactly=True \ run m_optim graph assertFalse hasattr m_optim conv assertFalse hasattr m_optim conv data = torch randn m_res = m data m_optim_res = m_optim data torch testing assert_close m_res m_optim_res rtol= e- atol= e- generic case m m_optim = _quant_script_and_optimize Parent FileCheck check_not Conv d = prim GetAttr name= conv \ check_count __torch__ torch classes quantized Conv dPackedParamsBase = prim Constant exactly=True \ run m_optim graph assertFalse hasattr m_optim conv assertFalse hasattr m_optim child data = torch randn m_res = m data m_optim_res = m_optim data torch testing assert_close m_res m_optim_res rtol= e- atol= e- skipIfNoXNNPACK unittest skipUnless HAS_TORCHVISION Needs torchvision test_mobilenet_optimize_for_mobile m = torchvision models mobilenet_v _small m = torch jit script m m = optimize_for_mobile m run forward times until segfault see https github com pytorch pytorch issues x = torch zeros assertEqual m x numel assertEqual m x numel assertEqual m x numel test_clone_module_with_class MyInnerTestModule torch nn Module __init__ - None super __init__ pqr = torch Tensor forward inputs inputs torch jit export dummy_method_not_cloned MyTestModule torch nn Module __init__ - None super __init__ abc = pqr = torch Tensor inner = MyInnerTestModule forward inputs x = dummy_method_cloned The call inner dummy_method_not_cloned should raise error y = inner dummy_method_not_cloned The call inner pqr should raise error z = inner pqr inputs x y z torch jit export dummy_method_not_cloned The call inner dummy_method_not_cloned should raise error y = inner dummy_method_not_cloned The call inner pqr should raise error z = inner pqr pqr dummy_method_not_cloned y z torch jit export dummy_method_not_cloned None torch jit export dummy_method_cloned None torch jit export dummy_method_ref_attr_pqr pqr inner pqr m = torch jit script MyTestModule Check methods exist original model assertEqual hasattr m dummy_method_not_cloned True assertEqual hasattr m dummy_method_cloned True assertEqual hasattr m dummy_method_not_cloned True assertEqual hasattr m pqr True Case- Successfully clone ignoring methods keeping all attributes cloned = torch _C _hack_do_not_use_clone_module_with_class m _c dummy_method_not_cloned dummy_method_not_cloned ignored_methods ignored_attributes Check ignored methods don t exist cloned model assertEqual hasattr cloned dummy_method_not_cloned False assertEqual hasattr cloned dummy_method_cloned True assertEqual hasattr cloned dummy_method_not_cloned False assertEqual hasattr cloned pqr True Check cloned has classname starts __torch__ assertTrue cloned qualified_name startswith __torch__ Expected cloned module s name start string f __torch__ got cloned qualified_name Case- Successfully clone module ignoring attribute pqr method references cloned = torch _C _hack_do_not_use_clone_module_with_class m _c dummy_method_not_cloned dummy_method_not_cloned dummy_method_ref_attr_pqr pqr Check ignored methods don t exist cloned model assertEqual hasattr cloned dummy_method_not_cloned False assertEqual hasattr cloned dummy_method_cloned True assertEqual hasattr cloned dummy_method_not_cloned False assertEqual hasattr cloned dummy_method_ref_attr_pqr False assertEqual hasattr cloned pqr False Case- The statement below will throw since dummy_method_cloned preserved references dummy_method_not_cloned which cloned assertRaises RuntimeError cloned = torch _C _hack_do_not_use_clone_module_with_class m _c dummy_method_not_cloned Case- The statement below will throw since dummy_method_ref_attr_pqr preserved references pqr which cloned assertRaises RuntimeError cloned = torch _C _hack_do_not_use_clone_module_with_class m _c dummy_method_not_cloned dummy_method_not_cloned pqr __name__ == __main__ run_tests