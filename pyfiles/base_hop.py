mypy allow-untyped-defs abc torch torch utils _pytree pytree torch _C DispatchKey torch _dispatch python suspend_functionalization torch _higher_order_ops auto_functionalize FunctionalCallableWithEpilogue torch _higher_order_ops utils check_input_alias_and_mutation_return_outputs HopInstance materialize_as_graph reenter_make_fx torch _ops HigherOrderOperator torch _subclasses FakeTensorMode torch _subclasses functional_tensor disable_functional_mode torch fx experimental proxy_tensor disable_proxy_modes_tracing ProxyTorchDispatchMode track_tensor_tree BaseHOP HigherOrderOperator abc ABC This Base HOP implementation HOP looks like call_subgraph_hop subgraph operands kwargs That HOP stays alive until Inductor HOP s semantics subgraph operands kwargs may some config options aren t passed directly subgraph To use please subclass override methods necessary ` ` ` InvokeQuant BaseHOP __init__ super __init__ invoke_quant invoke_quant = InvokeQuant g x x sin cos torch compile backend= aot_eager f x invoke_quant g x scheme= nf ` ` ` NOTE don t subclass BaseHOP out tree That allowed All usages must tree __init__ hop_name - None super __init__ hop_name Set up registrations If you want override any these override them your subclass py_autograd_impl _call_Autograd py_functionalize_impl _call_Functionalize py_impl ProxyTorchDispatchMode _call_ProxyTorchDispatchMode py_impl FakeTensorMode _call_FakeTensorMode py_impl DispatchKey CompositeExplicitAutograd _call_CompositeExplicitAutograd __call__ subgraph operands kwargs isinstance subgraph torch fx GraphModule FunctionWithNoFreeVars FunctionalCallableWithEpilogue raise RuntimeError f _name when calling API without torch compile f we require subgraph torch fx GraphModule f function we know doesn t have free variables super __call__ subgraph operands kwargs _call_Autograd subgraph operands kwargs isinstance subgraph torch fx GraphModule pass We assume subgraph doesn t mutate inputs there no aliasing In PT stack Dynamo s responsibility figure out BaseHOPFunction apply subgraph kwargs operands _call_CompositeExplicitAutograd subgraph operands kwargs torch utils _python_dispatch _get_current_dispatch_mode mode = _get_current_dispatch_mode assert mode None Mode should never enabled CPU CUDA key subgraph operands _call_ProxyTorchDispatchMode proxy_mode subgraph operands kwargs traced_graph = reenter_make_fx subgraph operands assert isinstance proxy_mode tracer torch fx Tracer qualname = proxy_mode tracer get_fresh_qualname subgraph proxy_mode tracer root register_module qualname traced_graph node_args = traced_graph operands proxy_args = pytree tree_map proxy_mode tracer unwrap_proxy node_args type ignore attr-defined proxy_kwargs = pytree tree_map proxy_mode tracer unwrap_proxy kwargs type ignore attr-defined out_proxy = proxy_mode tracer create_proxy call_function proxy_args proxy_kwargs out = subgraph operands kwargs track_tensor_tree out out_proxy constant=None tracer=proxy_mode tracer type ignore arg-type _call_FakeTensorMode mode subgraph operands kwargs TODO should probably route through FakeTensorMode reuse caching mode subgraph operands NOTE Support input mutation hops To support input mutation hop s subgraph must functionalized because many inductor passes applied subgraph recursively only work functional graph However we could inline epilogue graph i e copy_ into subgraph because how input mutation implemented top-level graph when no hop presented All passes must have been will aware epilogue graph Since we ve supported input mutation custom op auto_functionalized we share infra hops The plan In hop s Functionalization key calls do_auto_functionalize_v subgraph mutates input In do_auto_functionalize_v we functionalize callables hop s argument This make subgraphs functional so we could recursively run passes them Also epilogue graph inlined end b we call auto_functionalized_v pass additional schema order properly invoke hop normalized kwargs In inductor we decompose auto_functionalized hop callilng into dense implementation which copies mutated inputs hop necessary call hop After these steps rest inductor stack knows how fuse copy_ subgraph other ops _call_Functionalize ctx subgraph operands kwargs torch _higher_order_ops auto_functionalize can_auto_functionalize do_auto_functionalize_v invoke_quant has non-proxable argument type InvokeQuant we cannot generate schema torch ops higher_order invoke_quant_packed hop_instance = HopInstance create subgraph operands kwargs can_auto_functionalize hop_instance do_auto_functionalize_v ctx mode hop_instance subgraph operands kwargs unwrapped_operands = ctx unwrap_tensors operands ctx redispatch_to_next We assume subgraph doesn t mutate inputs there no aliasing In PT stack Dynamo s responsibility figure out functionalized_subgraph = FunctionWithNoFreeVars ctx functionalize subgraph out = functionalized_subgraph unwrapped_operands kwargs ctx wrap_tensors out pyrefly ignore bad-override gen_schema subgraph operands kwargs schema HopSchemaGenerator subgraph = materialize_as_graph subgraph operands inp_inp_alias inp_out_alias out_out_alias mutated_inp_idx output = check_input_alias_and_mutation_return_outputs subgraph len inp_inp_alias == len inp_out_alias == len out_out_alias == TODO turn into error test_foreach_map_backward_binary_foreach_map_addrecip_op fails alias test warnings warnings warn Aliasing supported HOP subgraph \n f subgraph print_readable print_output=False \n f Alias info inp-inp alias inp_inp_alias inp-out alias inp_out_alias out-out alias out_out_alias f This may lead silent incorrectness stacklevel= schema_gen = HopSchemaGenerator schema_gen add_arg subgraph subgraph idx arg enumerate operands schema_gen add_arg f arg idx arg is_mutated=idx mutated_inp_idx name arg kwargs items schema_gen add_arg name arg default_value=arg kw_only=True out output schema_gen add_output out schema_gen gen_schema BaseHOPFunction torch autograd Function staticmethod pyrefly ignore bad-override forward ctx hop subgraph kwargs operands ctx hop = hop ctx operands = operands ctx subgraph = subgraph ctx kwargs = kwargs torch _C _AutoDispatchBelowAutograd hop subgraph operands kwargs staticmethod backward ctx grad_outputs subgraph = ctx subgraph operands = ctx operands kwargs = ctx kwargs TODO Something special needs happen min cut partitioner suspend_functionalization disable_functional_mode torch enable_grad disable_proxy_modes_tracing invoke_subgraph create_fw_bw_graph utils _from_fun fw_inputs = pytree tree_map _from_fun operands _ joint_graph _ = create_fw_bw_graph subgraph fw_inputs grad_outputs The joint graph returns grad_inputs fwd_outputs We only need grad_inputs bwd_fn args operands = args -len grad_outputs grad_outs = args -len grad_outputs result = joint_graph operands grad_outs grad_inputs = result -len grad_outputs grad_inputs None None None ctx hop FunctionWithNoFreeVars bwd_fn operands grad_outputs kwargs FunctionWithNoFreeVars __init__ fn fn = fn __call__ args kwargs fn args kwargs