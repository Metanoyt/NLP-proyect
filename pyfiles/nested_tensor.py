mypy allow-untyped-defs typing noqa F torch torch _C DispatchKey DispatchKeySet torch _prims_common is_expandable_to torch nested _internal nested_int NestedIntNode torch utils weak WeakTensorKeyDictionary _tensor_id_counter = _tensor_symint_registry = WeakTensorKeyDictionary get_tensor_symint tensor coeff= torch _subclasses fake_tensor FakeTensor torch _subclasses functional_tensor mb_unwrap_functional_tensor NB Only FakeTensor associated memo tensor = mb_unwrap_functional_tensor tensor isinstance tensor FakeTensor tensor get_nested_int coeff=coeff global _tensor_id_counter tensor_symint = _tensor_symint_registry get tensor tensor_symint None tensor_symint = torch SymInt NestedIntNode _tensor_id_counter coeff _tensor_id_counter += _tensor_symint_registry tensor = tensor_symint tensor_symint SDPA metadata max min seqlens needed e g flash _get_sdpa_extreme_seqlen func tensor int func tensor item _store_val_in_tensor val - torch Tensor hack get dynamic shapes support store val shaped tensor torch zeros val _load_val_from_tensor t torch Tensor t shape serialization function must defined top level _rebuild_njt constructor_kwargs NestedTensor constructor_kwargs NestedTensor torch Tensor _values torch Tensor type ignore assignment _offsets torch Tensor _lengths Optional torch Tensor NOTE Nested ints ragged sizes strides Jagged layout tensors tensors represent n-dim tensor ragged dimension backed n- -dim tensor underneath e g jagged tensor outer shape B x D represented internally tensor shape sum x D where we introduce what we call nested int denoted x here sometimes denoted represent ragged dimension sum x represents dim inner tensor equivalently sum all sizes constituent tensors varying lengths We also use nested ints represent strides tensor For example jagged tensor shape B x D can strided two ways xD D x sum x where xD represents x multiplied D _size tuple int _strides tuple int Indicates nth dimension ragged _ragged_idx int _metadata_cache Dict str Any staticmethod __new__ cls values offsets lengths=None kwargs ks = DispatchKeySet DispatchKey NestedTensor ks = ks add DispatchKey AutogradNestedTensor Only support jagged now assert offsets None assert offsets ndim == assert isinstance values NestedTensor assert values device == offsets device Query cache symint associated offsets lengths create new one needed ragged_source = offsets lengths None lengths ragged_size = get_tensor_symint ragged_source coeff= _ragged_idx = kwargs get _ragged_idx B = offsets shape - lengths None assert B == lengths shape subtract convert values dim space r = _ragged_idx - _size = B values shape r ragged_size values shape r + stride = values stride _strides = ragged_size stride r stride r = torch Tensor _make_wrapper_subclass cls _size _strides torch contiguous_format values dtype torch jagged values device False kwargs get requires_grad False sizes False True dispatch_layout ks don t try calculate storage based non-zero size storage_size=values untyped_storage size r _ragged_idx = _ragged_idx r _size = _size r _strides = _strides r __init__ values offsets lengths=None kwargs super __init__ _values = values _offsets = offsets _lengths = lengths holds properties computed lazily _metadata_cache = kwargs get _metadata_cache collapsed ragged dim must always dynamic torch _dynamo maybe_mark_dynamic _ragged_idx torch _dynamo maybe_mark_dynamic _values _ragged_idx - min max sequence length should dynamic present max_seqlen_tensor = _metadata_cache get max_seqlen None max_seqlen_tensor None torch _dynamo mark_dynamic max_seqlen_tensor min_seqlen_tensor = _metadata_cache get min_seqlen None min_seqlen_tensor None torch _dynamo mark_dynamic min_seqlen_tensor values dispatch get proper view relationship torch _nested_get_values type ignore attr-defined offsets _offsets lengths _lengths Private accessor functions min max sequence length They re purposefully properties because those don t work PT yet These compute cache present TODO Revisit when properties better supported PT I think ideal state would have public properties min max sequence length compile including setters _get_max_seqlen max_seqlen_tensor = _max_seqlen_tensor max_seqlen_tensor None compute cache max_val = _get_sdpa_extreme_seqlen torch max _offsets diff _lengths None _lengths max_seqlen_tensor = _store_val_in_tensor max_val _metadata_cache max_seqlen = max_seqlen_tensor _load_val_from_tensor max_seqlen_tensor _get_min_seqlen min_seqlen_tensor = _min_seqlen_tensor min_seqlen_tensor None compute cache min_val = _get_sdpa_extreme_seqlen torch min _offsets diff _lengths None _lengths min_seqlen_tensor = _store_val_in_tensor min_val _metadata_cache min_seqlen = min_seqlen_tensor _load_val_from_tensor min_seqlen_tensor Private accessors used treating min max seqlen inner tensors flatten unflatten These must properties work traceable wrapper subclass logic These do compute cache present property _max_seqlen_tensor - Optional torch Tensor _metadata_cache get max_seqlen None _max_seqlen_tensor setter _max_seqlen_tensor val Optional torch Tensor - None _metadata_cache max_seqlen = val property _min_seqlen_tensor - Optional torch Tensor _metadata_cache get min_seqlen None _min_seqlen_tensor setter _min_seqlen_tensor val Optional torch Tensor - None _metadata_cache min_seqlen = val These old private property accessors kept around internal BC reasons TODO Remove these property _max_seqlen _get_max_seqlen property _min_seqlen _get_min_seqlen Convenience accessors min max seqlen one present do NOT compute cache them they re property _maybe_max_seqlen - Optional int mt = _max_seqlen_tensor None mt None _load_val_from_tensor mt property _maybe_min_seqlen - Optional int mt = _min_seqlen_tensor None mt None _load_val_from_tensor mt _is_contiguous_or_false lengths None False torch _prims_common is_contiguous_for_memory_format_or_false is_contiguous_for_memory_format_or_false _values memory_format=torch contiguous_format __repr__ type ignore override We should implement torch _tensor_str py instead grad_fn_str = f requires_grad= requires_grad requires_grad grad_fn grad_fn_str = f grad_fn= grad_fn f NestedTensor size= _size offsets= _offsets grad_fn_str contiguous= _is_contiguous_or_false TODO Remove favor default tensor subclass serialization logic We don t do today because https github com pytorch pytorch issues __reduce_ex__ proto state = torch _utils _get_obj_state Cached PyCapsules sizes strides serializable See Note Tensor Subclass custom size stride caching strategy _clear_non_serializable_cached_data SymNodes serializable assert _size state _strides state state = dict state del state _size del state _strides func = _rebuild_njt constructor_kwargs = values _values offsets _offsets lengths _lengths _ragged_idx _ragged_idx _metadata_cache _metadata_cache requires_grad requires_grad args = constructor_kwargs torch _tensor _rebuild_from_type_v func type args state __tensor_flatten__ ctx = requires_grad requires_grad ragged_idx _ragged_idx inner_tensors = _values _offsets _lengths None inner_tensors append _lengths _min_seqlen_tensor None inner_tensors append _min_seqlen_tensor _max_seqlen_tensor None inner_tensors append _max_seqlen_tensor inner_tensors ctx staticmethod __tensor_unflatten__ inner_tensors Dict meta outer_size outer_stride torch _subclasses fake_tensor FakeTensor inner tensors _values _offsets _lengths _min_seqlen _max_seqlen assert len inner_tensors = len inner_tensors = values = inner_tensors _values offsets = inner_tensors _offsets lengths = inner_tensors get _lengths None min_seqlen_tensor = inner_tensors get _min_seqlen_tensor None max_seqlen_tensor = inner_tensors get _max_seqlen_tensor None metadata_cache = min_seqlen_tensor None metadata_cache min_seqlen = min_seqlen_tensor max_seqlen_tensor None metadata_cache max_seqlen = max_seqlen_tensor ragged_idx = meta ragged_idx Alternatively we could make caller s responsibility cache But heuristic seems simple enough ragged_source = offsets lengths None lengths isinstance ragged_source FakeTensor ragged_size = outer_size ragged_idx ragged_source nested_int_memo = ragged_size NestedTensor values offsets=offsets lengths=lengths requires_grad=meta requires_grad _ragged_idx=ragged_idx _metadata_cache=metadata_cache classmethod __torch_dispatch__ cls func types args= kwargs=None type ignore override If you re wondering why there s nested tensor one its size = - see note NJT outer_size AOTDispatcher kwargs = kwargs None kwargs Lazy avoid circular dependency ops lookup_jagged fn = lookup_jagged func args kwargs fn None fn args kwargs Poor man s redispatch composite ops This becomes relevant under inference mode where disabling autograd key dispatch prevents decomposition all_dks = We want handle both cases where NestedTensor overrides composite implicit autograd kernel case where doesn t Prioritize calling into NestedTensor s kernel exists torch _C DispatchKey CompositeImplicitAutogradNestedTensor torch _C DispatchKey CompositeImplicitAutograd dk all_dks torch _C _dispatch_has_kernel_for_dispatch_key func name dk torch overrides enable_reentrant_dispatch func _op_dk dk args kwargs raise NotImplementedError func classmethod __torch_function__ cls func types args= kwargs=None kwargs None kwargs = torch fx experimental proxy_tensor maybe_enable_thunkify ops jagged_torch_function This should removed after https github com pytorch pytorch pull lands maybe_enable_thunkify try jagged_torch_function func args kwargs except NotImplementedError pass torch _C DisableTorchFunctionSubclass func args kwargs NB These fake view autograd Functions superseded real view ops Don t use them TODO Remove ViewBufferFromNested ViewNestedFromBuffer buffer_from_jagged once internal BC period has passed Not actually view ViewBufferFromNested torch autograd Function staticmethod forward ctx x NestedTensor type ignore override ctx save_for_backward x offsets ctx metadata_cache = x _metadata_cache ctx ragged_idx = x _ragged_idx x _values staticmethod backward ctx gO torch Tensor type ignore override offsets = ctx saved_tensors NestedTensor gO offsets=offsets _metadata_cache=ctx metadata_cache _ragged_idx=ctx ragged_idx Not actually view ViewNestedFromBuffer torch autograd Function staticmethod forward pyrefly ignore bad-override ctx values torch Tensor offsets torch Tensor metadata_cache Optional Dict str Any = None type ignore override maintain BC usages where seqlens stuffed directly into metadata cache non-Tensors ints metadata_cache None min_seqlen = metadata_cache get min_seqlen None max_seqlen = metadata_cache get max_seqlen None min_seqlen None isinstance min_seqlen torch Tensor metadata_cache min_seqlen = _store_val_in_tensor min_seqlen max_seqlen None isinstance max_seqlen torch Tensor metadata_cache max_seqlen = _store_val_in_tensor max_seqlen NestedTensor values detach offsets=offsets _metadata_cache=metadata_cache staticmethod backward ctx gO NestedTensor type ignore override gO _values None None buffer_from_jagged jagged ViewBufferFromNested apply jagged Need make obvious users should passing offsets jagged_from_list tensors List torch Tensor offsets Optional torch Tensor dtype=None device=None - tuple NestedTensor torch Tensor Constructs NestedTensor backed jagged layout list tensors len tensors == raise RuntimeError Cannot construct nested tensor empty tensor list len set t dtype t tensors == noqa C raise RuntimeError When constructing nested tensor all tensors list must have same dtype len set t device t tensors == noqa C raise RuntimeError When constructing nested tensor all tensors list must same device len set t dim t tensors == noqa C raise RuntimeError When constructing nested tensor all tensors list must have same dim component_dim = tensors dim component_dim == raise RuntimeError Cannot construct nested tensor list zero-dim tensors Check NT representable jagged layout which allows single ragged dimension after batch dim e g B D_ D_N B D_ D_N etc sizes = t shape t tensors ragged_idx = None d range component_dim dim_is_ragged = any size d = sizes d size sizes dim_is_ragged ragged_idx None add convert outer NJT dim space ragged_idx = d + raise RuntimeError Cannot represent given tensor list nested tensor jagged layout Note jagged layout only allows single ragged dimension For example B D_ D_ D_N ragged dim allow rectangular NJT default ragged dim next batch dim ragged_idx None ragged_idx = Set properties appropriately values = torch cat tensors dim= ragged_idx - to_kwargs = device None to_kwargs device = device dtype None to_kwargs dtype = dtype values = values to_kwargs Calculate jagged offsets provided offsets None Jagged layout specifies offsets stored int same device values TODO An alternative way construct offsets use F pad This avoids creating extra leaf tensor during forward potentially resolving compatibility issues offsets = torch cat torch zeros dtype=torch int device=values device torch tensor s ragged_idx - s sizes device=values device cumsum dim= compute now since s easy min_seqlen = min t shape ragged_idx - t tensors max_seqlen = max t shape ragged_idx - t tensors ret_nt = nested_view_from_values_offsets values offsets min_seqlen=min_seqlen max_seqlen=max_seqlen ragged_idx=ragged_idx ret_nt offsets type ignore return-value jagged_from_tensor_and_lengths tensor torch Tensor starts torch Tensor lengths torch Tensor - tuple NestedTensor torch Tensor Optional torch Tensor Constructs NestedTensor backed jagged layout tensor starts sequences sequence lengths batch_size = tensor shape is_expandable_to starts shape batch_size is_expandable_to lengths shape batch_size start_list = starts expand batch_size length_list = lengths expand batch_size raise RuntimeError When constructing jagged nested tensor using narrow your start length must Tensors broadcast input shape Calculate jagged offsets assert len tensor shape = tensor must least D nested narrow op work max_seq_len = tensor shape offset_lengths = max_seq_len torch arange batch_size dtype=torch int device=tensor device Jagged layout specifies offsets stored int same device values offsets = torch cat start_list + offset_lengths start_list - + offset_lengths - + length_list - unsqueeze Reshape buffer flatten st nd dimension view used enforce non-copy len tensor shape values = tensor view - tensor shape values = tensor view - Check offsets lengths make possibly contiguous regular NT is_contiguous = True orig_dim = tensor shape torch any length_list - ne orig_dim is_contiguous = False torch any offsets - diff ne orig_dim is_contiguous = False offsets + length_list = orig_dim is_contiguous = False actual_max_seqlen = int torch max lengths item min_seqlen = int torch min lengths item is_contiguous ret_nt = nested_view_from_values_offsets values offsets offsets - offsets - offsets min_seqlen=min_seqlen max_seqlen=actual_max_seqlen ret_nt = nested_view_from_values_offsets_lengths values offsets length_list min_seqlen=min_seqlen max_seqlen=actual_max_seqlen ret_nt offsets None is_contiguous length_list NB A dummy arg required so NestedTensor __torch_dispatch__ invoked _nested_view_from_values_offsets Sizes don t matter much they shouldn t because dummy can fake-ified we want avoid specializing This arg otherwise unused _dummy_instance Optional torch Tensor = None _nt_view_dummy - torch Tensor global _dummy_instance _dummy_instance None _dummy_instance = NestedTensor values=torch zeros device= meta offsets=torch zeros device= meta dtype=torch int detach _dummy_instance nested_view_from_values_offsets values offsets ragged_idx= min_seqlen=None max_seqlen=None min_seqlen_tensor = None min_seqlen None min_seqlen_tensor = _store_val_in_tensor min_seqlen max_seqlen_tensor = None max_seqlen None max_seqlen_tensor = _store_val_in_tensor max_seqlen torch _nested_view_from_jagged type ignore attr-defined values offsets _nt_view_dummy None ragged_idx min_seqlen_tensor max_seqlen_tensor type ignore return-value nested_view_from_values_offsets_lengths values offsets lengths ragged_idx= min_seqlen=None max_seqlen=None min_seqlen_tensor = None min_seqlen None min_seqlen_tensor = _store_val_in_tensor min_seqlen max_seqlen_tensor = None max_seqlen None max_seqlen_tensor = _store_val_in_tensor max_seqlen torch _nested_view_from_jagged type ignore attr-defined values offsets _nt_view_dummy lengths ragged_idx min_seqlen_tensor max_seqlen_tensor type ignore return-value nested_from_padded padded offsets ragged_idx= min_seqlen=None max_seqlen=None sum_S=None min_seqlen_tensor = None min_seqlen None min_seqlen_tensor = _store_val_in_tensor min_seqlen max_seqlen_tensor = None max_seqlen None max_seqlen_tensor = _store_val_in_tensor max_seqlen torch _nested_from_padded_tensor padded offsets _nt_view_dummy ragged_idx min_seqlen_tensor max_seqlen_tensor sum_S