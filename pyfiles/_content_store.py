mypy allow-untyped-defs This module provides FAST GPU content addressable store storages tensors top them VERY WEAK portability guarantees e g don t expect CPU CUDA address same hash don t expect portable across devices NOT cryptographically secure In we able hash G tensor data GPU less than second compared running SHA- CPU which would minute so The primary use case efficiently snapshotting intermediate tensor data offline debugging s been put module case you think another use case The hash function could replaced straight reimplementation SHA- which would give us much stronger portability guarantees WARNING THERE IS NO BC FC GUARANTEE FOR THIS FORMAT If you need format shift result consider packing into single torch save object traditional view sharing Because weak portability guarantees you can only write content store single process we don t provide any capability reopening content store add more things But we don t assume you can keep all tensors you want add store memory once because you probably can t Nor do we assume you know priori whether two storages can deduplicated Note only storages content-addressed tensors name addressed Note our padding strategy means int tensors would map same padded storage We think will immaterial most users ctypes functools hashlib os path struct collections defaultdict typing Optional torch torch _prims prims torch _utils torch nn functional F torch multiprocessing reductions StorageWeakRef lazy_compile compile_kwargs Lazily wrap function torch compile first call This avoids eagerly importing dynamo decorate_fn fn functools wraps fn compile_hook args kwargs compiled_fn = torch compile fn compile_kwargs globals fn __name__ = functools wraps fn compiled_fn compiled_fn args kwargs compile_hook decorate_fn Use torch compile mandatory good memory usage xor_sum implementation This our first instance using PT implement kernel PyTorch we get AOT capabilities would good apply here lazy_compile dynamic=True hash_storage_kernel x The randint calls carefully written hit things we have lowerings inductor Lack unsigned -bit integer pain = torch randint - x shape device=x device dtype=torch int abs = - + long b = torch randint - x shape device=x device dtype=torch int abs long This standard shift-multiply universal hash family plus xor sum hash using Philox generate random numbers Our Philox RNG deterministic across devices so don t use stable hashing This assumes fixed length so you re also obligated bucket length tensor well prims xor_sum x + b int Returns hex digest data storage Guaranteed SHA- stable_hash=True otherwise will consistent single process run necessarily across processes hash_storage storage torch UntypedStorage stable_hash bool = False - str torch _dynamo torch _dynamo utils is_compile_supported device_type = storage device type stable_hash is_compile_supported device_type cpu_storage = storage cpu TODO make storage support buffer protocol so isn t necessary buf = ctypes c_byte cpu_storage nbytes from_address cpu_storage data_ptr sha = hashlib sha usedforsecurity=False sha update buf sha hexdigest TODO factor into random utility device_type == cpu generator = torch _C default_generator device_type == cuda generator = torch cuda default_generators storage device index device_type == mps generator = torch mps _get_default_mps_generator device_type == xpu generator = torch xpu default_generators storage device index raise AssertionError f unhandled device type device_type state = generator get_state try generator manual_seed x = torch empty dtype=torch uint device=storage device set_ storage type ignore call-overload The dtype-casting view cannot compiled so padding reshaping also needs done externally even though could profitably fused pad = -x numel pad x = F pad x pad constant x = x view torch int We run -bit hash five times differing parameters reduce chance collision ITER = cs = hash_storage_kernel x item _ range ITER struct pack + i ITER cs hex finally generator set_state state ContentStoreWriter Structure storages tensors name __init__ loc str stable_hash bool = False - None loc str = loc seen_storage_hashes set str = set stable_hash = stable_hash TODO offer some sort non-blocking API speed things up write_storage storage torch UntypedStorage - str h = hash_storage storage stable_hash=self stable_hash h seen_storage_hashes h TODO consider using torch save we don t actually need any metadata storage subfolder = os path join loc storages os makedirs subfolder exist_ok=True target = os path join subfolder h os path exists target h torch save storage target seen_storage_hashes add h h compute_tensor_metadata t torch Tensor h=None h None h = hash_storage t untyped_storage stable_hash=self stable_hash t dtype h t storage_offset tuple t shape t stride torch _utils get_tensor_metadata t write_tensor name str t torch Tensor - None storage = t untyped_storage h = write_storage storage TODO Support more advanced snapshotting requires_grad grad etc d f = os path split name payload = compute_tensor_metadata t h=h subfolder = os path join loc tensors d os makedirs subfolder exist_ok=True torch save payload os path join subfolder f ContentStoreReader __init__ loc str cache=True - None loc = loc storage_cache Optional dict Optional torch device dict str StorageWeakRef = None cache storage_cache = defaultdict dict read_storage h str device=None - torch UntypedStorage device None device = torch device device ws = storage_cache device get h storage_cache None None s Optional torch UntypedStorage ws None s = torch UntypedStorage _new_with_weak_ptr ws cdata s None s s = torch load os path join loc storages h weights_only=True map_location=device _untyped_storage s None raise AssertionError f expected storage hash h os path join loc storages got None storage_cache None storage_cache device h = StorageWeakRef s s read_tensor_metadata name str fn = os path join loc tensors name os path exists fn raise FileNotFoundError fn torch load fn weights_only=True read_tensor name str device=None - torch Tensor dtype h storage_offset size stride metadata = read_tensor_metadata name storage = read_storage h device=device t = torch tensor dtype=dtype device=storage device t set_ storage storage_offset size stride torch _utils set_tensor_metadata t metadata t