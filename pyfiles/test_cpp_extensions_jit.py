Owner s module cpp-extensions glob locale os random re shutil string subprocess sys tempfile unittest warnings torch torch backends cudnn torch multiprocessing mp torch testing _internal common_utils common torch utils cpp_extension torch testing _internal common_cuda TEST_CUDA TEST_CUDNN torch testing _internal common_utils gradcheck TEST_XPU torch utils cpp_extension _get_cuda_arch_flags _TORCH_PATH check_compiler_is_gcc CUDA_HOME get_cxx_compiler remove_extension_h_precompiler_headers ROCM_HOME define TEST_ROCM before changing TEST_CUDA TEST_ROCM = TEST_CUDA torch version hip None ROCM_HOME None TEST_CUDA = TEST_CUDA CUDA_HOME None TEST_MPS = torch backends mps is_available IS_WINDOWS = sys platform == win IS_LINUX = sys platform startswith linux There s only one test runs gradcheck run slow mode manually torch testing _internal common_utils markDynamoStrictTest TestCppExtensionJIT common TestCase Tests just-in-time cpp extensions Don t confuse PyTorch JIT aka TorchScript setUp super setUp cpp extensions use relative paths Those paths relative file so we ll change working directory temporarily old_working_dir = os getcwd os chdir os path dirname os path abspath __file__ tearDown super tearDown working directory see setUp os chdir old_working_dir classmethod setUpClass cls torch testing _internal common_utils remove_cpp_extensions_build_root classmethod tearDownClass cls torch testing _internal common_utils remove_cpp_extensions_build_root test_jit_compile_extension module = torch utils cpp_extension load name= jit_extension sources= cpp_extensions jit_extension cpp cpp_extensions jit_extension cpp extra_include_paths= cpp_extensions path spaces path quote extra_cflags= -g verbose=True x = torch randn y = torch randn z = module tanh_add x y assertEqual z x tanh + y tanh Checking we can call method defined main C++ file z = module exp_add x y assertEqual z x exp + y exp Checking we can use JIT-compiled doubler = module Doubler assertIsNone doubler get grad assertEqual doubler get sum assertEqual doubler forward sum unittest skipIf TEST_CUDA TEST_ROCM CUDA found test_jit_cuda_extension NOTE The name extension must equal name module module = torch utils cpp_extension load name= torch_test_cuda_extension sources= cpp_extensions cuda_extension cpp cpp_extensions cuda_extension cu extra_cuda_cflags= -O verbose=True keep_intermediates=False x = torch zeros device= cuda dtype=torch float y = torch zeros device= cuda dtype=torch float z = module sigmoid_add x y cpu sigmoid = = assertEqual z torch ones_like z _test_jit_xpu_extension extra_sycl_cflags randomizing extension name names extension methods case when we test building few extensions row using function rand = join random sample string ascii_letters name = f torch_test_xpu_extension_ rand temp_dir = tempfile mkdtemp try open cpp_extensions xpu_extension sycl f text = f read fn sigmoid_add SigmoidAddKernel text = text replace fn f fn _ rand sycl_file = f temp_dir xpu_extension sycl open sycl_file w f f write text module = torch utils cpp_extension load name=name sources= sycl_file extra_sycl_cflags=extra_sycl_cflags verbose=True keep_intermediates=True build_directory=temp_dir x = torch zeros device= xpu dtype=torch float y = torch zeros device= xpu dtype=torch float method = f sigmoid_add_ rand assertTrue hasattr module method z = getattr module method x y cpu sigmoid = = assertEqual z torch ones_like z finally shutil rmtree temp_dir unittest skipIf TEST_XPU XPU found test_jit_xpu_extension NOTE test can affected setting TORCH_XPU_ARCH_LIST _test_jit_xpu_extension extra_sycl_cflags= unittest skipIf TEST_XPU XPU found test_jit_xpu_archlists NOTE test we explicitly test few different options TORCH_XPU_ARCH_LIST Setting TORCH_XPU_ARCH_LIST environment before test won t affect cases = Testing JIT compilation archlist extra_sycl_cflags Testing JIT + AOT full torch AOT arch list NOTE default cpp extension AOT arch list might reduced full list archlist join torch xpu get_arch_list extra_sycl_cflags Testing AOT full torch AOT arch list NOTE default cpp extension AOT arch list might reduced full list archlist join torch xpu get_arch_list below excludes spir target responsible JIT extra_sycl_cflags -fsycl-targets=spir _gen old_envvar = os environ get TORCH_XPU_ARCH_LIST None try c cases os environ TORCH_XPU_ARCH_LIST = c archlist _test_jit_xpu_extension extra_sycl_cflags=c extra_sycl_cflags finally old_envvar None os environ pop TORCH_XPU_ARCH_LIST os environ TORCH_XPU_ARCH_LIST = old_envvar unittest skipIf TEST_MPS MPS found test_mps_extension module = torch utils cpp_extension load name= torch_test_mps_extension sources= cpp_extensions mps_extension mm verbose=True keep_intermediates=False tensor_length = x = torch randn tensor_length device= cpu dtype=torch float y = torch randn tensor_length device= cpu dtype=torch float cpu_output = module get_cpu_add_output x y mps_output = module get_mps_add_output x mps y mps assertEqual cpu_output mps_output cpu Regression test https github com pytorch pytorch issues lib = torch mps compile_shader void kernel noop device float x lib noop mps_output module mps_add_one_new_context mps_output assertEqual cpu_output + mps_output cpu _run_jit_cuda_archflags flags expected Compile extension given ` flags ` _check_cuobjdump_output expected_values is_ptx=False elf_or_ptx = -- list-ptx is_ptx -- list-elf lib_ext = pyd IS_WINDOWS so Note extension name may include _v _v so first find exact name ext_filename = glob glob os path join temp_dir cudaext_archflag + lib_ext command = cuobjdump elf_or_ptx ext_filename p = subprocess Popen command stdout=subprocess PIPE stderr=subprocess PIPE output err = p communicate output = output decode ascii err = err decode ascii p returncode == err == raise AssertionError f Flags flags \nReturncode p returncode \nStderr err \n f Output output actual_arches = sorted re findall r sm_\d+ output expected_arches = sorted sm_ + xx replace xx expected_values assertEqual actual_arches expected_arches msg=f Flags flags Actual actual_arches Expected expected_arches \n f Stderr err \nOutput output temp_dir = tempfile mkdtemp old_envvar = os environ get TORCH_CUDA_ARCH_LIST None try os environ TORCH_CUDA_ARCH_LIST = flags params = name cudaext_archflags sources cpp_extensions cuda_extension cpp cpp_extensions cuda_extension cu extra_cuda_cflags -O verbose True build_directory temp_dir IS_WINDOWS p = mp Process target=torch utils cpp_extension load kwargs=params Compile load test CUDA arch different Python process avoid polluting current one causes test_jit_cuda_extension fail Windows There no clear way unload module after has been imported torch utils cpp_extension load builds loads module one go See https github com pytorch pytorch issues more details p start p join torch utils cpp_extension load params Expected output -- list-elf ELF file cudaext_archflags sm_ cubin ELF file cudaext_archflags sm_ cubin _check_cuobjdump_output expected expected None Expected output -- list-ptx PTX file cudaext_archflags sm_ ptx _check_cuobjdump_output expected is_ptx=True finally IS_WINDOWS rmtree returns permission error WinError Access denied Windows word-around subprocess run rm -rf temp_dir stdout=subprocess PIPE shutil rmtree temp_dir old_envvar None os environ pop TORCH_CUDA_ARCH_LIST os environ TORCH_CUDA_ARCH_LIST = old_envvar unittest skipIf TEST_CUDA CUDA found unittest skipIf TEST_ROCM disabled rocm test_jit_cuda_archflags Test number combinations - default machine we re testing - Separators can most common - Architecture names - With without +PTX n = torch cuda device_count capabilities = torch cuda get_device_capability i i range n expected values length- tuple list ELF list PTX note there should more than one PTX value archflags = f capability capability capability capabilities None archflags +PTX = major minor = map int torch version cuda split major major == minor = Compute capability = only supported up CUDA archflags Maxwell+Tegra = None archflags Volta = archflags +PTX = major CUDA drops compute capability archflags Pascal = None flags expected archflags items try _run_jit_cuda_archflags flags expected except RuntimeError e Using device default empty flags may fail device newer than CUDA compiler This raises RuntimeError specific message which we explicitly ignore here flags Error building str e pass raise try torch cuda synchronize except RuntimeError Ignore any error e g unsupported PTX code current device avoid errors here leaking into other tests pass unittest skipIf TEST_CUDA CUDA found test_cuda_arch_flags_non_default_gencode user_arch_flags = -gencode=arch=compute_ code=sm_ result = _get_cuda_arch_flags user_arch_flags assertEqual len result f User arch flags should prevent default generation f Expected Got result unittest skipIf TEST_CUDA CUDA found test_cuda_arch_flags_default_gencode default_flags = _get_cuda_arch_flags assertGreater len default_flags No args should generate default flags non_arch_flags = _get_cuda_arch_flags -O -- use-fast-math assertGreater len non_arch_flags Non-arch flags should still generate defaults empty_flags = _get_cuda_arch_flags assertGreater len empty_flags Empty list should generate default flags unittest skipIf TEST_CUDNN CuDNN found unittest skipIf TEST_ROCM Not supported ROCm test_jit_cudnn_extension implementation CuDNN ReLU IS_WINDOWS extra_ldflags = cudnn lib extra_ldflags = -lcudnn module = torch utils cpp_extension load name= torch_test_cudnn_extension sources= cpp_extensions cudnn_extension cpp extra_ldflags=extra_ldflags verbose=True with_cuda=True x = torch randn device= cuda dtype=torch float y = torch zeros device= cuda dtype=torch float module cudnn_relu x y y=relu x assertEqual torch nn functional relu x y assertRaisesRegex RuntimeError same size y_incorrect = torch zeros device= cuda dtype=torch float module cudnn_relu x y_incorrect test_inline_jit_compile_extension_with_functions_as_list cpp_source = torch Tensor tanh_add torch Tensor x torch Tensor y x tanh + y tanh module = torch utils cpp_extension load_inline name= inline_jit_extension_with_functions_list cpp_sources=cpp_source functions= tanh_add verbose=True assertEqual module tanh_add __doc__ split \n tanh_add x = torch randn y = torch randn z = module tanh_add x y assertEqual z x tanh + y tanh test_inline_jit_compile_extension_with_functions_as_dict cpp_source = torch Tensor tanh_add torch Tensor x torch Tensor y x tanh + y tanh module = torch utils cpp_extension load_inline name= inline_jit_extension_with_functions_dict cpp_sources=cpp_source functions= tanh_add Tanh then sum D verbose=True assertEqual module tanh_add __doc__ split \n Tanh then sum D test_inline_jit_compile_extension_multiple_sources_and_no_functions cpp_source = torch Tensor sin_add torch Tensor x torch Tensor y x sin + y sin cpp_source = #include torch extension h torch Tensor sin_add torch Tensor x torch Tensor y PYBIND _MODULE TORCH_EXTENSION_NAME m m sin_add sin_add sin x + sin y module = torch utils cpp_extension load_inline name= inline_jit_extension cpp_sources= cpp_source cpp_source verbose=True x = torch randn y = torch randn z = module sin_add x y assertEqual z x sin + y sin unittest skip Temporarily disabled unittest skipIf TEST_CUDA TEST_ROCM CUDA found test_inline_jit_compile_extension_cuda cuda_source = __global__ void cos_add_kernel const float __restrict__ x const float __restrict__ y float __restrict__ output const int size const auto index = blockIdx x blockDim x + threadIdx x index size output index = __cosf x index + __cosf y index torch Tensor cos_add torch Tensor x torch Tensor y auto output = torch zeros_like x const int threads = const int blocks = output numel + threads - threads cos_add_kernel blocks threads x data float y data float output data float output numel output Here C++ source need only declare function signature cpp_source = torch Tensor cos_add torch Tensor x torch Tensor y module = torch utils cpp_extension load_inline name= inline_jit_extension_cuda cpp_sources=cpp_source cuda_sources=cuda_source functions= cos_add verbose=True assertEqual module cos_add __doc__ split \n cos_add x = torch randn device= cuda dtype=torch float y = torch randn device= cuda dtype=torch float z = module cos_add x y assertEqual z x cos + y cos unittest skip Temporarily disabled unittest skipIf TEST_CUDA TEST_ROCM CUDA found test_inline_jit_compile_custom_op_cuda cuda_source = __global__ void cos_add_kernel const float __restrict__ x const float __restrict__ y float __restrict__ output const int size const auto index = blockIdx x blockDim x + threadIdx x index size output index = __cosf x index + __cosf y index torch Tensor cos_add torch Tensor x torch Tensor y auto output = torch zeros_like x const int threads = const int blocks = output numel + threads - threads cos_add_kernel blocks threads x data_ptr float y data_ptr float output data_ptr float output numel output Here C++ source need only declare function signature cpp_source = #include torch library h torch Tensor cos_add torch Tensor x torch Tensor y TORCH_LIBRARY inline_jit_extension_custom_op_cuda m m cos_add cos_add torch utils cpp_extension load_inline name= inline_jit_extension_custom_op_cuda cpp_sources=cpp_source cuda_sources=cuda_source verbose=True is_python_module=False x = torch randn device= cuda dtype=torch float y = torch randn device= cuda dtype=torch float z = torch ops inline_jit_extension_custom_op_cuda cos_add x y assertEqual z x cos + y cos unittest skipIf TEST_XPU XPU found test_inline_jit_compile_extension_xpu sycl_source = #include c xpu XPUStream h CosAddKernel public void operator const sycl nd_item item_ct const const int index = item_ct get_group item_ct get_local_range + item_ct get_local_id index size output index = cosf x index + cosf y index CosAddKernel const float _x const float _y float _output int _size x _x y _y output _output size _size private const float x const float y float output int size void cos_add_kernel const float x const float y float output int size CosAddKernel krn x y output size const int threads = const int blocks = size + threads - threads sycl queue queue = c xpu getCurrentXPUStream queue queue submit sycl handler cgh cgh parallel_for CosAddKernel sycl nd_range sycl range blocks sycl range threads sycl range threads krn torch Tensor cos_add torch Tensor x torch Tensor y auto output = torch zeros_like x const int threads = const int blocks = output numel + threads - threads cos_add_kernel x data_ptr float y data_ptr float output data_ptr float output numel output Here C++ source need only declare function signature cpp_source = torch Tensor cos_add torch Tensor x torch Tensor y module = torch utils cpp_extension load_inline name= inline_jit_extension_xpu cpp_sources=cpp_source sycl_sources=sycl_source functions= cos_add verbose=True assertEqual module cos_add __doc__ split \n cos_add x = torch randn device= xpu dtype=torch float y = torch randn device= xpu dtype=torch float z = module cos_add x y assertEqual z x cos + y cos test_inline_jit_compile_extension_throws_when_functions_is_bad assertRaises ValueError torch utils cpp_extension load_inline name= invalid_jit_extension cpp_sources= functions= test_lenient_flag_handling_in_jit_extensions cpp_source = torch Tensor tanh_add torch Tensor x torch Tensor y x tanh + y tanh module = torch utils cpp_extension load_inline name= lenient_flag_handling_extension cpp_sources=cpp_source functions= tanh_add extra_cflags= -g\n\n -O -Wall extra_include_paths= cpp_extensions\n verbose=True x = torch zeros dtype=torch float y = torch zeros dtype=torch float z = module tanh_add x y cpu assertEqual z x tanh + y tanh unittest skip Temporarily disabled unittest skipIf TEST_CUDA TEST_ROCM CUDA found test_half_support Checks issue operator ambiguity half when certain THC headers included See https github com pytorch pytorch pull #issuecomment- corresponding issue cuda_source = template typename T typename U __global__ void half_test_kernel const T input U output input input &#124; &#124; input = input output = torch Tensor half_test torch Tensor input auto output = torch empty input options dtype torch kFloat AT_DISPATCH_FLOATING_TYPES_AND_HALF input scalar_type half_test half_test_kernel scalar_t input data scalar_t output data float output module = torch utils cpp_extension load_inline name= half_test_extension cpp_sources= torch Tensor half_test torch Tensor input cuda_sources=cuda_source functions= half_test verbose=True x = torch randn device= cuda dtype=torch half result = module half_test x assertEqual result test_reload_jit_extension compile code torch utils cpp_extension load_inline name= reloaded_jit_extension cpp_sources=code functions= f verbose=True module = compile int f assertEqual module f module = compile int f assertEqual module f module = compile int f assertEqual module f module = compile int f assertEqual module f unittest skipIf utf locale getlocale lower Only test UTF- locale test_load_with_non_platform_default_encoding Assume code saved UTF- locale set different encoding You might encounter decoding errors ExtensionVersioner But case quite hard cover because CI environments may non-latin locale So following code just test source file gbk locale utf- cpp_source = #include torch extension h Non-latin character test 字符 It will cause utf- decoding error int f PYBIND _MODULE TORCH_EXTENSION_NAME m m f f f build_dir = tempfile mkdtemp src_path = os path join build_dir main cpp open src_path encoding= gbk mode= w f f write cpp_source module = torch utils cpp_extension load name= non_default_encoding sources=src_path verbose=True assertEqual module f test_cpp_frontend_module_has_same_output_as_python dtype=torch double extension = torch utils cpp_extension load name= cpp_frontend_extension sources= cpp_extensions cpp_frontend_extension cpp verbose=True input = torch randn dtype=dtype cpp_linear = extension Net cpp_linear dtype python_linear = torch nn Linear dtype First make sure they have same parameters cpp_parameters = dict cpp_linear named_parameters torch no_grad python_linear weight copy_ cpp_parameters fc weight python_linear bias copy_ cpp_parameters fc bias cpp_output = cpp_linear forward input python_output = python_linear input assertEqual cpp_output python_output cpp_output sum backward python_output sum backward p cpp_linear parameters assertFalse p grad None assertEqual cpp_parameters fc weight grad python_linear weight grad assertEqual cpp_parameters fc bias grad python_linear bias grad test_cpp_frontend_module_python_inter_op extension = torch utils cpp_extension load name= cpp_frontend_extension sources= cpp_extensions cpp_frontend_extension cpp verbose=True Create torch nn Module which uses C++ module submodule M torch nn Module __init__ - None super __init__ x = torch nn Parameter torch tensor net = extension Net forward input net forward input + x net = extension Net net double net torch get_default_dtype assertEqual str net Net Further embed torch nn Module into Sequential also add C++ module element Sequential sequential = torch nn Sequential M torch nn Tanh net torch nn Sigmoid input = torch randn Try calling module output = sequential forward input The call operator bound forward too assertEqual output sequential input assertEqual list output shape Do changes module hierarchy old_dtype = torch get_default_dtype sequential torch float sequential torch float sequential old_dtype assertEqual sequential parameters dtype old_dtype Make sure we can access these methods recursively assertEqual len list sequential parameters len net parameters + assertEqual len list sequential named_parameters len net named_parameters + assertEqual len list sequential buffers len net buffers assertEqual len list sequential modules Test clone net = net clone assertEqual len net parameters len net parameters assertEqual len net buffers len net buffers assertEqual len net modules len net modules Try differentiating through whole module parameter net parameters assertIsNone parameter grad output sum backward parameter net parameters assertFalse parameter grad None assertGreater parameter grad sum Try calling zero_grad net zero_grad p net parameters assert p grad None zero_grad defaults setting grads None Test train eval training property assertTrue net training net eval assertFalse net training net train assertTrue net training net eval Try calling additional methods we registered biased_input = torch randn output_before = net forward biased_input bias = net get_bias clone assertEqual list bias shape net set_bias bias + assertEqual net get_bias bias + output_after = net forward biased_input assertNotEqual output_before output_after Try accessing parameters assertEqual len net parameters np = net named_parameters assertEqual len np assertIn fc weight np assertIn fc bias np assertEqual len net buffers nb = net named_buffers assertEqual len nb assertIn buf nb assertEqual nb torch eye test_cpp_frontend_module_has_up_to_date_attributes extension = torch utils cpp_extension load name= cpp_frontend_extension sources= cpp_extensions cpp_frontend_extension cpp verbose=True net = extension Net assertEqual len net _parameters net add_new_parameter foo torch eye assertEqual len net _parameters assertEqual len net _buffers net add_new_buffer bar torch eye assertEqual len net _buffers assertEqual len net _modules net add_new_submodule fc assertEqual len net _modules unittest skipIf TEST_CUDA TEST_ROCM CUDA found test_cpp_frontend_module_python_inter_op_with_cuda extension = torch utils cpp_extension load name= cpp_frontend_extension sources= cpp_extensions cpp_frontend_extension cpp verbose=True net = extension Net p net parameters assertTrue p device type == cpu cpu_parameters = p clone p net parameters device = torch device cuda net device i p enumerate net parameters assertTrue p device type == cuda assertTrue p device index == assertEqual cpu_parameters i p net cpu net add_new_parameter torch eye net add_new_parameter b torch eye net add_new_buffer c torch eye net add_new_buffer d torch eye net add_new_submodule fc net add_new_submodule fc p net parameters assertTrue p device type == cpu net cuda p net parameters assertTrue p device type == cuda test_returns_shared_library_path_when_is_python_module_is_true source = #include torch script h torch Tensor func torch Tensor x x static torch RegisterOperators r test func func torch utils cpp_extension load_inline name= is_python_module cpp_sources=source functions= func verbose=True is_python_module=False assertEqual torch ops test func torch eye torch eye test_set_default_type_also_changes_aten_default_type module = torch utils cpp_extension load_inline name= test_set_default_type cpp_sources= torch Tensor get torch empty functions= get verbose=True initial_default = torch get_default_dtype try assertEqual module get dtype initial_default torch set_default_dtype torch float assertEqual module get dtype torch float torch set_default_dtype torch float assertEqual module get dtype torch float torch set_default_dtype torch float assertEqual module get dtype torch float finally torch set_default_dtype initial_default test_compilation_error_formatting Test missing-semicolon error message has linebreaks This ll fail message has been munged into single line It s hard write anything more specific every compiler has s own error formatting assertRaises RuntimeError e torch utils cpp_extension load_inline name= test_compilation_error_formatting cpp_sources= int main pattern = r \\n &#124; \\r assertNotRegex str e pattern test_warning Note module created source will include py key_error symbol But because visibility fact lives different compilation unit than pybind trips up ubsan even though fine ubsan supp thus needs contain vptr warn_mod so source = error_type no error torch TypeError python_error py error_already_set Tensor foo Tensor x int error_type std ostringstream err_stream err_stream Error x type TORCH_WARN err_stream str error_type == throw torch TypeError err_stream str c_str error_type == PyObject obj = PyTuple_New - TORCH_CHECK obj Pretend caught different thread restored here auto e = python_error e persist e restore throw e error_type == throw py key_error err_stream str x cos Ensure double type hard-coded c name below t = torch rand double cpp_tensor_name = r CPUDoubleType Without error handling warnings cannot caught warn_mod = torch utils cpp_extension load_inline name= warn_mod cpp_sources= source functions= foo with_pytorch_error_handling=False warnings catch_warnings record=True w warn_mod foo t assertEqual len w assertRaisesRegex TypeError t type warn_mod foo t assertEqual len w assertRaisesRegex SystemError bad argument internal function warn_mod foo t assertEqual len w assertRaisesRegex KeyError cpp_tensor_name warn_mod foo t assertEqual len w warn_mod = torch utils cpp_extension load_inline name= warn_mod cpp_sources= source functions= foo with_pytorch_error_handling=True warnings catch_warnings record=True w Caught no error should detected warn_mod foo t assertEqual len w Caught cpp error should also detected assertRaisesRegex TypeError t type warn_mod foo t assertEqual len w Caught python error should also detected assertRaisesRegex SystemError bad argument internal function warn_mod foo t assertEqual len w Caught pybind error should also detected Note there no type name translation pybind errors assertRaisesRegex KeyError cpp_tensor_name warn_mod foo t assertEqual len w Make sure raising warnings handled properly warnings catch_warnings record=True w warnings simplefilter error No error warning should raise assertRaisesRegex UserWarning t type warn_mod foo t assertEqual len w Another error happened warning ignored assertRaisesRegex TypeError t type warn_mod foo t assertEqual len w test_autograd_from_cpp source = void run_back Tensor x x backward void run_back_no_gil Tensor x pybind gil_scoped_release no_gil x backward MyFn torch autograd Function staticmethod forward ctx x x clone staticmethod backward ctx gx gx test_backward_deadlock = torch utils cpp_extension load_inline name= test_backward_deadlock cpp_sources= source functions= run_back run_back_no_gil This used deadlock inp = torch rand requires_grad=True loss = MyFn apply inp sum assertRaisesRegex RuntimeError The autograd engine called while holding GIL test_backward_deadlock run_back loss inp = torch rand requires_grad=True loss = MyFn apply inp sum test_backward_deadlock run_back_no_gil loss test_custom_compound_op_autograd Test custom compound op i e custom op just calls other aten ops correctly returns gradients those other ops source = #include torch library h torch Tensor my_add torch Tensor x torch Tensor y x + y TORCH_LIBRARY my m m add my_add torch utils cpp_extension load_inline name= is_python_module cpp_sources=source verbose=True is_python_module=False = torch randn requires_grad=True b = torch randn requires_grad=True fast_mode True False gradcheck torch ops my add b eps= e- fast_mode=fast_mode test_custom_functorch_error Test custom C++ Function raises error under functorch transforms identity_m = torch utils cpp_extension load name= identity sources= cpp_extensions identity cpp t = torch randn requires_grad=True msg = r cannot use C\+\+ torch autograd Function functorch assertRaisesRegex RuntimeError msg torch func vmap identity_m identity t assertRaisesRegex RuntimeError msg torch func grad identity_m identity t test_gen_extension_h_pch IS_LINUX source = Tensor sin_add Tensor x Tensor y x sin + y sin head_file_pch = os path join _TORCH_PATH include torch extension h gch head_file_signature = os path join _TORCH_PATH include torch extension h sign remove_extension_h_precompiler_headers pch_exist = os path exists head_file_pch signature_exist = os path exists head_file_signature assertEqual pch_exist False assertEqual signature_exist False torch utils cpp_extension load_inline name= inline_extension_with_pch cpp_sources= source functions= sin_add verbose=True use_pch=True pch_exist = os path exists head_file_pch signature_exist = os path exists head_file_signature compiler = get_cxx_compiler check_compiler_is_gcc compiler assertEqual pch_exist True assertEqual signature_exist True test_aoti_torch_call_dispatcher source = #include torch csrc inductor aoti_runtime utils h #include torch csrc inductor aoti_torch utils h #include torch csrc inductor aoti_torch c shim h #include torch csrc stable stableivalue_conversions h using RAIIATH = torch aot_inductor RAIIAtenTensorHandle Tensor my_abs Tensor x StableIValue stack RAIIATH raii torch aot_inductor new_tensor_handle std move x stack = torch stable detail raii release aoti_torch_call_dispatcher aten abs stack RAIIATH res torch stable detail AtenTensorHandle stack reinterpret_cast Tensor res release Tensor my_floor Tensor x StableIValue stack RAIIATH raii torch aot_inductor new_tensor_handle std move x stack = torch stable detail raii release aoti_torch_call_dispatcher aten floor stack RAIIATH res torch stable detail AtenTensorHandle stack reinterpret_cast Tensor res release module = torch utils cpp_extension load_inline name= inline_extension_using_shim_dispatcher cpp_sources= source functions= my_abs my_floor t = torch rand - floor_t = module my_floor t abs_t = module my_abs t assertEqual abs_t torch abs t assertEqual floor_t torch floor t unittest skipIf TEST_CUDA TEST_ROCM CUDA found test_cuda_pluggable_allocator_include This method creates minimal example replicate apex setup py build nccl_allocator extension cpp source includes CUDAPluggableAllocator has empty exported function cpp_source = #include torch csrc cuda CUDAPluggableAllocator h #include torch extension h int get_nccl_allocator PYBIND _MODULE TORCH_EXTENSION_NAME m m get_nccl_allocator get_nccl_allocator build_dir = tempfile mkdtemp src_path = os path join build_dir NCCLAllocator cpp open src_path mode= w f f write cpp_source initially success false success = False try try build module torch utils cpp_extension load name= nccl_allocator sources=src_path verbose=True with_cuda=True set success true built successfully success = True except Exception e print f Failed load module e test build successful assertEqual success True __name__ == __main__ common run_tests