Owner s module inductor functools partial unittest skipIf torch torch _inductor ir Pointwise torch _inductor lowering make_pointwise register_lowering torch _inductor test_case TestCase InductorTestCase torch _inductor virtualized ops torch testing _internal common_utils skipIfRocm skipIfXpu torch testing _internal inductor_utils GPU_TYPE HAS_CPU HAS_GPU requires_gpu These tests check issues lowerings aren t main pytorch repo TestCustomLowering InductorTestCase classmethod setUpClass cls super setUpClass cls test_inductor_ops = torch library Library noqa TOR test_inductor_ops DEF cls device_list = Meta CUDA XPU device cls device_list setattr cls impl_ + device lower torch library Library noqa TOR test_inductor_ops IMPL device cls _register_jagged_to_padded_dense cls _register_asm_op classmethod tearDown cls super tearDownClass classmethod _register_jagged_to_padded_dense cls Approximation fbgemm jagged_to_padded_dense_forward cls test_inductor_ops define jagged_to_padded_dense Tensor input Tensor offsets SymInt max_seq_len Scalar pad_value - Tensor j pd_meta inp offsets max_seq_len pad_value torch empty offsets shape - max_seq_len inp shape device=inp device dtype=inp dtype j pd_gpu inp offsets max_seq_len pad_value res = torch full offsets shape - max_seq_len inp shape pad_value device=inp device dtype=inp dtype b range offsets shape - r range offsets b + - offsets b res b r = inp offsets b + r res j pd_lowering inp offsets max_seq_len pad_value offsets_loader = offsets make_loader inp_loader = inp make_loader jagged_len = inp get_size offsets_dtype = offsets get_dtype inner_fn index batch_idx seq_idx emb_idx = index begin_idx = ops indirect_indexing offsets_loader batch_idx jagged_len + end_idx = offsets_loader batch_idx + jagged_idx = begin_idx + seq_idx ops masked ops lt ops index_expr jagged_idx offsets_dtype end_idx lambda inp_loader jagged_idx emb_idx pad_value Pointwise create device=inp get_device dtype=inp get_dtype inner_fn=inner_fn ranges= offsets get_size - max_seq_len inp get_size register_lowering torch ops test_inductor_ops jagged_to_padded_dense type_promotion_kind=None j pd_lowering cls impl_meta impl jagged_to_padded_dense j pd_meta cls impl_cuda impl jagged_to_padded_dense j pd_gpu cls impl_xpu impl jagged_to_padded_dense j pd_gpu classmethod _register_asm_op cls Approximation fbgemm jagged_to_padded_dense_forward cls test_inductor_ops define tanh_approx Tensor input - Tensor tanh_approx_meta inp torch tanh inp cls impl_meta impl tanh_approx tanh_approx_meta tanh_approx_lowering inp fn = partial ops inline_asm_elementwise asm= tanh approx f $ $ make_pointwise fn inp register_lowering torch ops test_inductor_ops tanh_approx type_promotion_kind=None tanh_approx_lowering cls test_inductor_ops define add_custom Tensor Tensor b - Tensor add_custom b + b cls impl_meta impl add_custom add_custom add_custom_lowering b fn = partial ops inline_asm_elementwise asm= add f $ $ $ make_pointwise fn b register_lowering torch ops test_inductor_ops add_custom type_promotion_kind=None add_custom_lowering test_register_lowering_custom_dict custom_lowering_dict = torch _inductor lowering register_lowering torch library custom_op helion_test foo mutates_args= foo x torch Tensor - torch Tensor x register_lowering torch ops helion_test foo lowering_dict=custom_lowering_dict foo_lowering x x assert torch ops helion_test foo custom_lowering_dict assert torch ops helion_test foo torch _inductor lowering lowerings requires_gpu skipIf GPU_TYPE == mps Not applicable MPS test_jagged_to_padded_dense_sanity_cuda fn inp offsets max_seq_len torch ops test_inductor_ops jagged_to_padded_dense inp offsets max_seq_len inp = torch rand device=GPU_TYPE offsets = torch tensor dtype=torch int device=GPU_TYPE max_seq_len = res = fn inp offsets max_seq_len assertEqual inp res assertEqual inp res assertEqual inp res assertEqual inp res assertEqual inp res assertEqual inp res fn_opt = torch compile fn assertEqual fn inp offsets max_seq_len fn_opt inp offsets max_seq_len requires_gpu skipIf GPU_TYPE == mps Not applicable MPS test_jagged_to_padded_dense_zero_size Previously masking being completely stripped masked load input value That would lead IMA because cuda trying read index zero-size tensor fn inp offsets max_seq_len inp = torch bmm inp torch ones device=GPU_TYPE view torch ops test_inductor_ops jagged_to_padded_dense inp offsets max_seq_len inp = torch rand device=GPU_TYPE offsets = torch zeros device=GPU_TYPE dtype=torch int max_seq_len = fn_opt = torch compile fn assertEqual fn inp offsets max_seq_len fn_opt inp offsets max_seq_len requires_gpu skipIfRocm skipIfXpu skipIf GPU_TYPE == mps Not applicable MPS test_tanh_approx fn inp torch ops test_inductor_ops tanh_approx inp inp = torch randn device=GPU_TYPE fn_opt = torch compile fn = torch tanh inp b = fn_opt inp assertEqual b requires_gpu skipIfRocm skipIfXpu skipIf GPU_TYPE == mps Not applicable MPS test_multi_inp_asm fn b torch ops test_inductor_ops add_custom b = torch randn device=GPU_TYPE b = torch randn device=GPU_TYPE fn_opt = torch compile fn out = + b out = fn_opt b assertEqual out out __name__ == __main__ torch _inductor test_case run_tests HAS_CPU HAS_GPU run_tests needs= filelock