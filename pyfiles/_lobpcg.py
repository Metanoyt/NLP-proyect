mypy allow-untyped-defs Locally Optimal Block Preconditioned Conjugate Gradient methods Author Pearu Peterson Created February typing Optional torch torch _linalg_utils _utils Tensor torch overrides handle_torch_function has_torch_function __all__ = lobpcg _symeig_backward_complete_eigenspace D_grad U_grad A D U compute F such F_ij = d_j - d_i ^ - i = j F_ii = F = D unsqueeze - - D unsqueeze - F diagonal dim =- dim =- fill_ float inf F pow_ - A grad = U D grad + U^T U grad F U^T Ut = U mT contiguous res = torch matmul U torch matmul torch diag_embed D_grad + torch matmul Ut U_grad F Ut res _polynomial_coefficients_given_roots roots Given ` roots ` polynomial find polynomial s coefficients If roots = r_ r_n then method returns coefficients a_ a_ a_n == so p x = x - r_ x - r_n = x^n + a_ n- x^ n- + a_ x_ + a_ Note better performance requires writing low-level kernel poly_order = roots shape - poly_coeffs_shape = list roots shape we assume p x = x^n + a_ n- x^ n- + + a_ x + a_ so poly_coeffs = a_ a_n a_ n+ == we insert one extra coefficient enable better vectorization below poly_coeffs_shape - += poly_coeffs = roots new_zeros poly_coeffs_shape poly_coeffs = poly_coeffs - = perform Horner s rule i range poly_order + note computationally hard compute backward method because then given coefficients would require finding roots calculating sensitivity based Vieta s theorem So code below tries circumvent explicit root finding series operations memory copies imitating Horner s method The memory copies required construct nodes computational graph exploiting explicit in-place separate node each step recursion Horner s method Needs more memory O k^ only O k^ complexity poly_coeffs_new = poly_coeffs clone roots requires_grad poly_coeffs out = poly_coeffs_new narrow - poly_order - i i + out -= roots narrow - i - poly_coeffs narrow - poly_order - i + i + poly_coeffs = poly_coeffs_new poly_coeffs narrow - poly_order + _polynomial_value poly x zero_power transition A generic method computing poly x using Horner s rule Args poly Tensor possibly batched D Tensor representing polynomial coefficients such poly i = a_ i_ i_n == poly x = poly zero_power + + poly n x^n x Tensor value possible batched evaluate polynomial ` poly ` zero_power Tensor representation ` x^ ` It application-specific transition Callable function accepts some intermediate result ` int_val ` ` x ` specific polynomial coefficient ` poly k ` some iteration ` k ` It basically performs one iteration Horner s rule defined ` x int_val + poly k zero_power ` Note ` zero_power ` parameter because step ` + poly k zero_power ` depends ` x ` whether vector matrix something so functionality delegated user res = zero_power clone k range poly size - - - - res = transition res x poly k res _matrix_polynomial_value poly x zero_power=None Evaluates ` poly x ` batched matrix input ` x ` Check out ` _polynomial_value ` function more details matrix-aware Horner s rule iteration transition curr_poly_val x poly_coeff res = x matmul curr_poly_val res diagonal dim =- dim =- add_ poly_coeff unsqueeze - res zero_power None zero_power = torch eye x size - x size - dtype=x dtype device=x device view len list x shape - x size - x size - _polynomial_value poly x zero_power transition _vector_polynomial_value poly x zero_power=None Evaluates ` poly x ` batched vector input ` x ` Check out ` _polynomial_value ` function more details vector-aware Horner s rule iteration transition curr_poly_val x poly_coeff res = torch addcmul poly_coeff unsqueeze - x curr_poly_val res zero_power None zero_power = x new_ones expand x shape _polynomial_value poly x zero_power transition _symeig_backward_partial_eigenspace D_grad U_grad A D U largest compute projection operator onto orthogonal subspace spanned columns U defined I - UU^T Ut = U mT contiguous proj_U_ortho = -U matmul Ut proj_U_ortho diagonal dim =- dim =- add_ compute U_ortho basis orthogonal complement span U projecting random m m - k matrix onto subspace spanned columns U fix generator determinism gen = torch Generator A device orthogonal complement span U U_ortho = proj_U_ortho matmul torch randn A shape - A size - - D size - dtype=A dtype device=A device generator=gen U_ortho_t = U_ortho mT contiguous compute coefficients characteristic polynomial tensor D Note D diagonal so diagonal elements exactly roots characteristic polynomial chr_poly_D = _polynomial_coefficients_given_roots D code below finds explicit solution Sylvester equation U_ortho^T A U_ortho dX - dX D = -U_ortho^T A U incorporates into whole gradient stored ` res ` variable Equivalent following naive implementation res = A new_zeros A shape p_res = A new_zeros A shape - D size - k range chr_poly_D size - p_res zero_ i range k p_res += A matrix_power k - - i U_grad D pow i unsqueeze - res -= chr_poly_D k U_ortho poly_D_at_A inverse U_ortho_t p_res U t Note dX differential so gradient contribution comes backward sensitivity Tr f U_grad D_grad A U D ^T dX = Tr g U_grad A U D ^T dA some functions f g we need compute g U_grad A U D The naive implementation based paper Hu Qingxi Daizhan Cheng The polynomial solution Sylvester matrix equation Applied mathematics letters - We can modify computation ` p_res ` above more efficient way p_res = U_grad chr_poly_D D pow + + chr_poly_D k D pow k unsqueeze - + A U_grad chr_poly_D D pow + + chr_poly_D k D pow k - unsqueeze - + + A matrix_power k - U_grad chr_poly_D k Note saves us redundant matrix products A elimination matrix_power U_grad_projected = U_grad series_acc = U_grad_projected new_zeros U_grad_projected shape k range chr_poly_D size - poly_D = _vector_polynomial_value chr_poly_D k D series_acc += U_grad_projected poly_D unsqueeze - U_grad_projected = A matmul U_grad_projected compute chr_poly_D A which essentially chr_poly_D_at_A = A new_zeros A shape k range chr_poly_D size - chr_poly_D_at_A += chr_poly_D k A matrix_power k Note however better performance we use Horner s rule chr_poly_D_at_A = _matrix_polynomial_value chr_poly_D A compute action ` chr_poly_D_at_A ` restricted U_ortho_t chr_poly_D_at_A_to_U_ortho = torch matmul U_ortho_t torch matmul chr_poly_D_at_A U_ortho we need invert chr_poly_D_at_A_to_U_ortho ` we compute its Cholesky decomposition then use ` torch cholesky_solve ` better stability Cholesky decomposition requires input positive-definite Note ` chr_poly_D_at_A_to_U_ortho ` positive-definite ` largest ` == False ` largest ` == True ` k ` even under assumption ` A ` has distinct eigenvalues check ` chr_poly_D_at_A_to_U_ortho ` positive-definite negative-definite chr_poly_D_at_A_to_U_ortho_sign = - largest k == + chr_poly_D_at_A_to_U_ortho_L = torch linalg cholesky chr_poly_D_at_A_to_U_ortho_sign chr_poly_D_at_A_to_U_ortho compute gradient part span U res = _symeig_backward_complete_eigenspace D_grad U_grad A D U incorporate Sylvester equation solution into full gradient resides span U_ortho res -= U_ortho matmul chr_poly_D_at_A_to_U_ortho_sign torch cholesky_solve U_ortho_t matmul series_acc chr_poly_D_at_A_to_U_ortho_L matmul Ut res _symeig_backward D_grad U_grad A D U largest ` U ` square then columns ` U ` complete eigenspace U size - == U size - _symeig_backward_complete_eigenspace D_grad U_grad A D U _symeig_backward_partial_eigenspace D_grad U_grad A D U largest LOBPCGAutogradFunction torch autograd Function staticmethod forward type ignore override ctx A Tensor k Optional int = None B Optional Tensor = None X Optional Tensor = None n Optional int = None iK Optional Tensor = None niter Optional int = None tol Optional float = None largest Optional bool = None method Optional str = None tracker None = None ortho_iparams Optional dict str int = None ortho_fparams Optional dict str float = None ortho_bparams Optional dict str bool = None - tuple Tensor Tensor makes sure input contiguous efficiency Note autograd does support dense gradients sparse input yet A = A contiguous A is_sparse A B None B = B contiguous B is_sparse B D U = _lobpcg A k B X n iK niter tol largest method tracker ortho_iparams ortho_fparams ortho_bparams ctx save_for_backward A B D U ctx largest = largest D U staticmethod backward ctx D_grad U_grad pyrefly ignore bad-override A_grad = B_grad = None grads = None A B D U = ctx saved_tensors largest = ctx largest lobpcg backward has some limitations Checks unsupported input A is_sparse B None B is_sparse ctx needs_input_grad raise ValueError lobpcg backward does support sparse input yet Note lobpcg forward does though A dtype torch complex torch complex B None B dtype torch complex torch complex raise ValueError lobpcg backward does support complex input yet Note lobpcg forward does though B None raise ValueError lobpcg backward does support backward B = I yet largest None largest = True symeig backward B None A_grad = _symeig_backward D_grad U_grad A D U largest A has index grads = A_grad B has index grads = B_grad tuple grads lobpcg A Tensor k Optional int = None B Optional Tensor = None X Optional Tensor = None n Optional int = None iK Optional Tensor = None niter Optional int = None tol Optional float = None largest Optional bool = None method Optional str = None tracker None = None ortho_iparams Optional dict str int = None ortho_fparams Optional dict str float = None ortho_bparams Optional dict str bool = None - tuple Tensor Tensor Find k largest smallest eigenvalues corresponding eigenvectors symmetric positive definite generalized eigenvalue problem using matrix-free LOBPCG methods This function front-end following LOBPCG algorithms selectable via ` method ` argument ` method= basic ` - LOBPCG method introduced Andrew Knyazev see Knyazev A less robust method may fail when Cholesky applied singular input ` method= ortho ` - LOBPCG method orthogonal basis selection StathopoulosEtal A robust method Supported inputs dense sparse batches dense matrices note In general basic method spends least time per iteration However robust methods converge much faster more stable So usage basic method generally recommended there exist cases where usage basic method may preferred warning The backward method does support sparse complex inputs It works only when ` B ` provided i e ` B == None ` We actively working extensions details algorithms going published promptly warning While assumed ` A ` symmetric ` A grad ` To make sure ` A grad ` symmetric so ` A - t A grad ` symmetric first-order optimization routines prior running ` lobpcg ` we do following symmetrization map ` A - A + A t ` The map performed only when ` A ` requires gradients warning LOBPCG algorithm applicable when number ` A ` s rows smaller than x number requested eigenpairs ` n ` Args A Tensor input tensor size math ` m m ` k integer optional number requested eigenpairs Default number math ` X ` columns when specified ` ` B Tensor optional input tensor size math ` m m ` When specified ` B ` interpreted identity matrix X tensor optional input tensor size math ` m n ` where ` k = n = m ` When specified used initial approximation eigenvectors X must dense tensor n integer optional math ` X ` specified then ` n ` specifies size generated random approximation eigenvectors Default value ` n ` ` k ` If math ` X ` specified any provided value ` n ` ignored ` n ` automatically set number columns math ` X ` iK tensor optional input tensor size math ` m m ` When specified will used preconditioner niter int optional maximum number iterations When reached iteration process hard-stopped current approximation eigenpairs returned For infinite iteration until convergence criteria met use ` - ` tol float optional residual tolerance stopping criterion Default ` feps ` where ` feps ` smallest non-zero floating-point number given input tensor ` A ` data type largest bool optional when True solve eigenproblem largest eigenvalues Otherwise solve eigenproblem smallest eigenvalues Default ` True ` method str optional select LOBPCG method See description function above Default ortho tracker callable optional function tracing iteration process When specified called each iteration step LOBPCG instance argument The LOBPCG instance holds full state iteration process following attributes ` iparams ` ` fparams ` ` bparams ` - dictionaries integer float boolean valued input parameters respectively ` ivars ` ` fvars ` ` bvars ` ` tvars ` - dictionaries integer float boolean Tensor valued iteration variables respectively ` A ` ` B ` ` iK ` - input Tensor arguments ` E ` ` X ` ` S ` ` R ` - iteration Tensor variables For instance ` ivars istep ` - current iteration step ` X ` - current approximation eigenvectors ` E ` - current approximation eigenvalues ` R ` - current residual ` ivars converged_count ` - current number converged eigenpairs ` tvars rerr ` - current state convergence criteria Note when ` tracker ` stores Tensor objects LOBPCG instance must make copies these If ` tracker ` sets ` bvars force_stop = True ` iteration process will hard-stopped ortho_iparams ortho_fparams ortho_bparams dict optional various parameters LOBPCG algorithm when using ` method= ortho ` Returns E Tensor tensor eigenvalues size math ` k ` X Tensor tensor eigenvectors size math ` m k ` References Knyazev Andrew V Knyazev Toward Optimal Preconditioned Eigensolver Locally Optimal Block Preconditioned Conjugate Gradient Method SIAM J Sci Comput - pages https epubs siam org doi abs S StathopoulosEtal Andreas Stathopoulos Kesheng Wu A Block Orthogonalization Procedure Constant Synchronization Requirements SIAM J Sci Comput - pages https epubs siam org doi S DuerschEtal Jed A Duersch Meiyue Shao Chao Yang Ming Gu A Robust Efficient Implementation LOBPCG SIAM J Sci Comput C -C pages https arxiv org abs torch jit is_scripting tensor_ops = A B X iK set map type tensor_ops issubset torch Tensor type None has_torch_function tensor_ops handle_torch_function lobpcg tensor_ops A k=k B=B X=X n=n iK=iK niter=niter tol=tol largest=largest method=method tracker=tracker ortho_iparams=ortho_iparams ortho_fparams=ortho_fparams ortho_bparams=ortho_bparams torch _jit_internal is_scripting A requires_grad B None B requires_grad While expected ` A ` symmetric ` A_grad ` might Therefore we perform trick below so ` A_grad ` becomes symmetric The symmetrization important first-order optimization methods so A - alpha A_grad still symmetric matrix Same holds ` B ` A_sym = A + A mT B_sym = B + B mT B None None LOBPCGAutogradFunction apply A_sym k B_sym X n iK niter tol largest method tracker ortho_iparams ortho_fparams ortho_bparams A requires_grad B None B requires_grad raise RuntimeError Script require grads supported atm If you just want do forward use detach A B before calling into lobpcg _lobpcg A k B X n iK niter tol largest method tracker ortho_iparams ortho_fparams ortho_bparams _lobpcg A Tensor k Optional int = None B Optional Tensor = None X Optional Tensor = None n Optional int = None iK Optional Tensor = None niter Optional int = None tol Optional float = None largest Optional bool = None method Optional str = None tracker None = None ortho_iparams Optional dict str int = None ortho_fparams Optional dict str float = None ortho_bparams Optional dict str bool = None - tuple Tensor Tensor A must square assert A shape - == A shape - A shape B None A B must have same shapes assert A shape == B shape A shape B shape dtype = _utils get_floating_dtype A device = A device tol None feps = torch float e- torch float e- dtype tol = feps m = A shape - k = X None X shape - k None k n = k n None n X None X shape - m n raise ValueError f LPBPCG algorithm applicable when number A rows = m f smaller than x number requested eigenpairs = n method = ortho method None method iparams = m m n n k k niter niter None niter fparams = tol tol bparams = largest True largest None largest method == ortho ortho_iparams None iparams update ortho_iparams ortho_fparams None fparams update ortho_fparams ortho_bparams None bparams update ortho_bparams iparams ortho_i_max = iparams get ortho_i_max iparams ortho_j_max = iparams get ortho_j_max fparams ortho_tol = fparams get ortho_tol tol fparams ortho_tol_drop = fparams get ortho_tol_drop tol fparams ortho_tol_replace = fparams get ortho_tol_replace tol bparams ortho_use_drop = bparams get ortho_use_drop False torch jit is_scripting LOBPCG call_tracker = LOBPCG_call_tracker type ignore method-assign len A shape N = int torch prod torch tensor A shape - bA = A reshape N + A shape - bB = B reshape N + A shape - B None None bX = X reshape N + X shape - X None None bE = torch empty N k dtype=dtype device=device bXret = torch empty N m k dtype=dtype device=device i range N A_ = bA i B_ = bB i bB None None X_ = torch randn m n dtype=dtype device=device bX None bX i assert len X_ shape == X_ shape == m n X_ shape m n iparams batch_index = i worker = LOBPCG A_ B_ X_ iK iparams fparams bparams method tracker worker run bE i = worker E k bXret i = worker X k torch jit is_scripting LOBPCG call_tracker = LOBPCG_call_tracker_orig type ignore method-assign bE reshape A shape - + k bXret reshape A shape - + m k X = torch randn m n dtype=dtype device=device X None X assert len X shape == X shape == m n X shape m n worker = LOBPCG A B X iK iparams fparams bparams method tracker worker run torch jit is_scripting LOBPCG call_tracker = LOBPCG_call_tracker_orig type ignore method-assign worker E k worker X k LOBPCG Worker LOBPCG methods __init__ A Optional Tensor B Optional Tensor X Tensor iK Optional Tensor iparams dict str int fparams dict str float bparams dict str bool method str tracker None - None constant parameters A = A B = B iK = iK iparams = iparams fparams = fparams bparams = bparams method = method tracker = tracker m = iparams m n = iparams n variable parameters X = X E = torch zeros n dtype=X dtype device=X device R = torch zeros m n dtype=X dtype device=X device S = torch zeros m n dtype=X dtype device=X device tvars dict str Tensor = ivars dict str int = istep fvars dict str float = _ bvars dict str bool = _ False __str__ lines = LOPBCG lines += f iparams= iparams lines += f fparams= fparams lines += f bparams= bparams lines += f ivars= ivars lines += f fvars= fvars lines += f bvars= bvars lines += f tvars= tvars lines += f A= A lines += f B= B lines += f iK= iK lines += f X= X lines += f E= E r = line lines r += line + \n r update Set update iteration variables ivars istep == X_norm = float torch norm X iX_norm = X_norm - A_norm = float torch norm _utils matmul A X iX_norm B_norm = float torch norm _utils matmul B X iX_norm fvars X_norm = X_norm fvars A_norm = A_norm fvars B_norm = B_norm ivars iterations_left = iparams niter ivars converged_count = ivars converged_end = method == ortho _update_ortho _update_basic ivars iterations_left = ivars iterations_left - ivars istep = ivars istep + update_residual Update residual R A B X E mm = _utils matmul R = mm A X - mm B X E update_converged_count Determine number converged eigenpairs using backward stable convergence criterion see discussion Sec DuerschEtal Users may redefine method custom convergence criteria - int prev_count = ivars converged_count tol = fparams tol A_norm = fvars A_norm B_norm = fvars B_norm E X R = E X R rerr = torch norm R torch norm X A_norm + torch abs E X shape - B_norm converged = rerr tol count = b converged b ignore convergence following pairs ensure strict ordering eigenpairs break count += assert count = prev_count f number converged eigenpairs prev_count got count cannot decrease ivars converged_count = count tvars rerr = rerr count stop_iteration Return True stop iterations Note tracker defined can force-stop iterations setting ` ` worker bvars force_stop = True ` ` bvars get force_stop False ivars iterations_left == ivars converged_count = iparams k run Run LOBPCG iterations Use method template implementing LOBPCG iteration scheme custom tracker compatible TorchScript update torch jit is_scripting tracker None call_tracker while stop_iteration update torch jit is_scripting tracker None call_tracker torch jit unused call_tracker Interface tracking iteration process Python mode Tracking iteration process disabled TorchScript mode In fact one should specify tracker=None when JIT compiling functions using lobpcg do nothing when TorchScript mode Internal methods _update_basic Update initialize iteration variables when ` method == basic ` mm = torch matmul ns = ivars converged_end nc = ivars converged_count n = iparams n largest = bparams largest ivars istep == Ri = _get_rayleigh_ritz_transform X M = _utils qform _utils qform A X Ri E Z = _utils symeig M largest X = mm X mm Ri Z E = E np = update_residual nc = update_converged_count S n = X W = _utils matmul iK R ivars converged_end = ns = n + np + W shape - S n + np ns = W S_ = S nc ns Ri = _get_rayleigh_ritz_transform S_ M = _utils qform _utils qform A S_ Ri E_ Z = _utils symeig M largest X nc = mm S_ mm Ri Z n - nc E nc = E_ n - nc P = mm S_ mm Ri Z n n - nc np = P shape - update_residual nc = update_converged_count S n = X S n n + np = P W = _utils matmul iK R nc ivars converged_end = ns = n + np + W shape - S n + np ns = W _update_ortho Update initialize iteration variables when ` method == ortho ` mm = torch matmul ns = ivars converged_end nc = ivars converged_count n = iparams n largest = bparams largest ivars istep == Ri = _get_rayleigh_ritz_transform X M = _utils qform _utils qform A X Ri _E Z = _utils symeig M largest X = mm X mm Ri Z update_residual np = nc = update_converged_count S n = X W = _get_ortho R X ns = ivars converged_end = n + np + W shape - S n + np ns = W S_ = S nc ns Rayleigh-Ritz procedure E_ Z = _utils symeig _utils qform A S_ largest Update E X P X nc = mm S_ Z n - nc E nc = E_ n - nc P = mm S_ mm Z n - nc _utils basis Z n - nc n - nc mT np = P shape - check convergence update_residual nc = update_converged_count update S S n = X S n n + np = P W = _get_ortho R nc S n + np ns = ivars converged_end = n + np + W shape - S n + np ns = W _get_rayleigh_ritz_transform S Return transformation matrix used Rayleigh-Ritz procedure reducing general eigenvalue problem math ` S^TAS C = S^TBS C E ` standard eigenvalue problem math ` Ri^T S^TAS Ri Z = Z E ` where ` C = Ri Z ` note In original Rayleight-Ritz procedure DuerschEtal problem formulated follows SAS = S^T A S SBS = S^T B S D = diagonal matrix SBS - R^T R = Cholesky D SBS D Ri = D R^- solve symeig problem Ri^T SAS Ri Z = Theta Z C = Ri Z To reduce number matrix products denoted empty space between matrices here we introduce element-wise products denoted symbol ` ` so Rayleight-Ritz procedure becomes SAS = S^T A S SBS = S^T B S d = diagonal SBS - -d column vector dd = d d^T -d matrix R^T R = Cholesky dd SBS Ri = R^- d broadcasting solve symeig problem Ri^T SAS Ri Z = Theta Z C = Ri Z where ` dd ` -d matrix replaces matrix products ` D M D ` one element-wise product ` M dd ` ` d ` replaces matrix product ` D M ` element-wise product ` M d ` Also creating diagonal matrix ` D ` avoided Args S Tensor matrix basis search subspace size math ` m n ` Returns Ri tensor upper-triangular transformation matrix size math ` n n ` B = B SBS = _utils qform B S d_row = SBS diagonal - - - d_col = d_row reshape d_row shape TODO use torch linalg cholesky_solve once implemented R = torch linalg cholesky SBS d_row d_col upper=True torch linalg solve_triangular R d_row diag_embed upper=True left=False _get_svqb U Tensor drop bool tau float - Tensor Return B-orthonormal U note When ` drop ` ` False ` then ` svqb ` based Algorithm DuerschPhD slight modification corresponding algorithm introduced StathopolousWu Args U Tensor initial approximation size m n drop bool when True drop columns contribution ` span U ` small tau float positive tolerance Returns U Tensor B-orthonormal columns math ` U^T B U = I ` size m n where ` n = n ` ` drop ` ` False otherwise ` n = n ` torch numel U == U UBU = _utils qform B U d = UBU diagonal - - Detect drop exact zero columns U While test ` abs d == ` unlikely True random data possible construct input data lobpcg where will True leading failure notice ` d - ` operation original algorithm To prevent failure we drop exact zero columns here then continue original algorithm below nz = torch where abs d = assert len nz == nz len nz len d U = U nz torch numel U == U UBU = _utils qform B U d = UBU diagonal - - nz = torch where abs d = assert len nz == len d The original algorithm DuerschPhD d_col = d - reshape d shape DUBUD = UBU d_col d_col mT E Z = _utils symeig DUBUD t = tau abs E max drop keep = torch where E t assert len keep == keep E = E keep Z = Z keep d_col = d_col keep E torch where E t = t torch matmul U d_col mT pyrefly ignore unsupported-operation Z E - _get_ortho U V Return B-orthonormal U columns B-orthogonal V note When ` bparams ortho_use_drop == False ` then ` _get_ortho ` based Algorithm DuerschPhD slight modification corresponding algorithm introduced StathopolousWu Otherwise method implements Algorithm DuerschPhD note If all U columns B-collinear V then returned tensor U will empty Args U Tensor initial approximation size m n V Tensor B-orthogonal external basis size m k Returns U Tensor B-orthonormal columns math ` U^T B U = I ` such math ` V^T B U= ` size m n where ` n = n ` ` drop ` ` False otherwise ` n = n ` mm = torch matmul mm_B = _utils matmul m = iparams m tau_ortho = fparams ortho_tol tau_drop = fparams ortho_tol_drop tau_replace = fparams ortho_tol_replace i_max = iparams ortho_i_max j_max = iparams ortho_j_max when use_drop==True enable dropping U columns have small contribution ` span U V ` use_drop = bparams ortho_use_drop clean up variables previous call vkey list fvars keys vkey startswith ortho_ vkey endswith _rerr fvars pop vkey ivars pop ortho_i ivars pop ortho_j BV_norm = torch norm mm_B B V BU = mm_B B U VBU = mm V mT BU i = j = i range i_max U = U - mm V VBU drop = False tau_svqb = tau_drop j range j_max use_drop U = _get_svqb U drop tau_svqb drop = True tau_svqb = tau_replace U = _get_svqb U False tau_replace torch numel U == all initial U columns B-collinear V ivars ortho_i = i ivars ortho_j = j U BU = mm_B B U UBU = mm U mT BU U_norm = torch norm U BU_norm = torch norm BU R = UBU - torch eye UBU shape - device=UBU device dtype=UBU dtype R_norm = torch norm R https github com pytorch pytorch issues workaround rerr = float R_norm float BU_norm U_norm - vkey = f ortho_UBUmI_rerr i j fvars vkey = rerr rerr tau_ortho break VBU = mm V mT BU VBU_norm = torch norm VBU U_norm = torch norm U rerr = float VBU_norm float BV_norm U_norm - vkey = f ortho_VBU_rerr i fvars vkey = rerr rerr tau_ortho break m U shape - + V shape - TorchScript needs var assigned local do optional type refinement B = B assert B None raise ValueError Overdetermined shape U f #B-cols = B shape - = #U-cols = U shape - + #V-cols = V shape - must hold ivars ortho_i = i ivars ortho_j = j U Calling tracker separated LOBPCG definitions because TorchScript does support user-defined callback arguments LOBPCG_call_tracker_orig = LOBPCG call_tracker LOBPCG_call_tracker tracker