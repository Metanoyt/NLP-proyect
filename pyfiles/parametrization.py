mypy allow-untyped-defs torch torch nn torch nn utils parametrize is_parametrized module_contains_param module parametrization is_parametrized module see any module tensors have parametriztion attached matches one passed any any isinstance param parametrization param param_list key param_list module parametrizations items False Structured Pruning Parameterizations FakeStructuredSparsity nn Module r Parametrization Structured Pruning Like FakeSparsity should attached weight any other parameter requires mask Instead element-wise bool mask parameterization uses row-wise bool mask __init__ mask super __init__ register_buffer mask mask forward x isinstance mask torch Tensor raise AssertionError mask must torch Tensor mask shape = x shape raise AssertionError f mask shape mask shape must match x shape x shape shape = len x shape shape = - mask reshape shape x state_dict args kwargs avoid double saving masks BiasHook __init__ parametrization prune_bias param = parametrization prune_bias = prune_bias __call__ module input output getattr module _bias None None bias = module _bias data prune_bias bias ~self param mask = reshape bias broadcast over output dimensions idx = len output shape idx = - bias = bias reshape idx output += bias output