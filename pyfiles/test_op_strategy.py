Owner s oncall distributed itertools random contextlib contextmanager itertools chain unittest mock patch numpy np torch torch distributed tensor DeviceMesh distribute_tensor DTensor init_device_mesh Partial Replicate Shard torch distributed tensor _collective_utils redistribute_cost torch distributed tensor _dtensor_spec DTensorSpec TensorMeta torch distributed tensor _op_schema OpSchema OpSpec OpStrategy RuntimeSchemaInfo torch distributed tensor _ops _einsum_strategy EinsumDims gen_einsum_strategies torch distributed tensor _ops utils register_op_strategy replicate_op_strategy torch distributed tensor debug CommDebugMode torch testing _internal common_utils run_tests TestCase torch testing _internal distributed _tensor common_dtensor create_local_tensor_test_class DTensorOpTestBase DTensorTestBase with_comms try torch utils _cxx_pytree tree_leaves except ImportError torch utils _pytree tree_leaves type ignore no-redef extract_tensor_meta t - TensorMeta TensorMeta t shape t stride t dtype TestEinsumDims TestCase test_batch_dims equation = abc abc- abc input_dims output_dim = EinsumDims parse_equation equation edims = EinsumDims parse_dims input_dims output_dim assertEqual edims batch_dims b c assertEqual edims contracting_dims assertEqual edims lhs_out_only_dims assertEqual edims rhs_out_only_dims test_mm_dims equation = mk kn- mn input_dims output_dim = EinsumDims parse_equation equation edims = EinsumDims parse_dims input_dims output_dim assertEqual edims batch_dims assertEqual edims contracting_dims k assertEqual edims lhs_out_only_dims m assertEqual edims rhs_out_only_dims n test_bmm_dims equation = bmk bkn- bmn input_dims output_dim = EinsumDims parse_equation equation edims = EinsumDims parse_dims input_dims output_dim assertEqual edims batch_dims b assertEqual edims contracting_dims k assertEqual edims lhs_out_only_dims m assertEqual edims rhs_out_only_dims n equation = bcmk bckn- bcmn input_dims output_dim = EinsumDims parse_equation equation edims = EinsumDims parse_dims input_dims output_dim assertEqual edims batch_dims b c assertEqual edims contracting_dims k assertEqual edims lhs_out_only_dims m assertEqual edims rhs_out_only_dims n test_free_dims equation = abc ab- abc input_dims output_dim = EinsumDims parse_equation equation edims = EinsumDims parse_dims input_dims output_dim assertEqual edims batch_dims b assertEqual edims contracting_dims assertEqual edims lhs_out_only_dims c assertEqual edims rhs_out_only_dims equation = abd bf- abfd codespell ignore input_dims output_dim = EinsumDims parse_equation equation edims = EinsumDims parse_dims input_dims output_dim assertEqual edims batch_dims b assertEqual edims contracting_dims assertEqual edims lhs_out_only_dims d assertEqual edims rhs_out_only_dims f TestEinsumStrategies DTensorOpTestBase property world_size - int test_mm_ d_mesh mesh = build_device_mesh all_strats = gen_einsum_strategies mk kn- mn mesh assertEqual len all_strats strategies test_mm_ d_mesh mesh = DeviceMesh device_type torch arange world_size reshape all_strats = gen_einsum_strategies mk kn- mn mesh assertEqual len all_strats strategies test_bmm_ d_mesh mesh = build_device_mesh all_strats = gen_einsum_strategies bmk bkn- bmn mesh assertEqual len all_strats strategies test_bmm_diffinndim_ d_mesh mesh = DeviceMesh device_type torch arange world_size reshape all_strats = gen_einsum_strategies bmk kn- bmn mesh assertEqual len all_strats strategies test_bmm_diffoutndim_ d_mesh mesh = DeviceMesh device_type torch arange world_size reshape all_strats = gen_einsum_strategies bmk k- bm mesh assertEqual len all_strats strategies test_bmm_ d_mesh mesh = DeviceMesh device_type torch arange world_size reshape all_strats = gen_einsum_strategies bmk bkn- bmn mesh assertEqual len all_strats strategies test_pointwise_ d_mesh mesh = build_device_mesh simple_strats = gen_einsum_strategies abcd abcd- abcd mesh assertEqual len simple_strats strategies broadcast_strats = gen_einsum_strategies bcd abcd- abcd mesh assertEqual len broadcast_strats strategies test_linearity_ d_mesh mesh = build_device_mesh all_strats = gen_einsum_strategies abcd abcd- abcd mesh linearity=True assertEqual len all_strats strategies TestCostModel DTensorOpTestBase property world_size - int test_redistribute_cost_mesh_ d mesh_ d = build_device_mesh shard_placement = Shard replica_placement = Replicate partial_placement = Partial global_tensor = torch randn global_tensor_meta = extract_tensor_meta global_tensor shard spec shard_spec = DTensorSpec mesh_ d shard_placement global_tensor_meta replica spec replica_spec = DTensorSpec mesh_ d replica_placement global_tensor_meta partial spec partial_spec = DTensorSpec mesh_ d partial_placement global_tensor_meta make sure reshard cost same spec redistribute spec shard_spec replica_spec partial_spec cost = redistribute_cost spec spec assertEqual cost shard - replicate allgather_cost = redistribute_cost shard_spec replica_spec partial - shard reduce_scatter_cost = redistribute_cost partial_spec shard_spec partial - replicate allreduce_cost = redistribute_cost partial_spec replica_spec assertEqual allgather_cost reduce_scatter_cost assertTrue allreduce_cost + allgather_cost + reduce_scatter_cost shard partial cost = redistribute_cost shard_spec partial_spec assertEqual cost float inf test_redistribute_cost_latency test cost model addmm op torch distributed tensor _ops _matrix_ops addmm_strategy mesh = build_device_mesh shard _placement = Shard partial_placement = Partial shard _placement = Shard shard _tensor_meta = extract_tensor_meta torch randn partial_tensor_meta = extract_tensor_meta torch randn shard _tensor_meta = extract_tensor_meta torch randn shard spec shard _spec = DTensorSpec mesh shard _placement shard _tensor_meta replica spec partial_spec = DTensorSpec mesh partial_placement partial_tensor_meta partial spec shard _spec = DTensorSpec mesh shard _placement shard _tensor_meta op_schema = OpSchema torch ops aten addmm default OpStrategy OpSpec shard _spec OpStrategy OpSpec partial_spec OpStrategy OpSpec shard _spec output_strategy = addmm_strategy op_schema strategy_costs = strategy output_strategy strategies redistribute_cost = sum chain from_iterable strategy redistribute_cost strategy_costs str strategy = redistribute_cost assert cost model counts collective latency i e multiple comm penalized assertTrue strategy_costs S R S - S strategy_costs R S R - S assert single allreduce best one assertEqual strategy_costs S R S - S min strategy_costs values test_redistribute_cost_mesh_ d mesh_ d = DeviceMesh device_type torch arange world_size reshape shard_placement = Shard Shard replica_placement = Replicate Replicate partial_placement = Partial Partial global_tensor = torch randn global_tensor_meta = extract_tensor_meta global_tensor shard spec shard_spec = DTensorSpec mesh_ d shard_placement global_tensor_meta replica spec replica_spec = DTensorSpec mesh_ d replica_placement global_tensor_meta partial spec partial_spec = DTensorSpec mesh_ d partial_placement global_tensor_meta make sure reshard cost same spec redistribute spec shard_spec replica_spec partial_spec cost = redistribute_cost spec spec assertEqual cost shard - replicate allgather_cost = redistribute_cost shard_spec replica_spec partial - replicate allreduce_cost = redistribute_cost partial_spec replica_spec partial - shard reduce_scatter_cost = redistribute_cost partial_spec shard_spec assertTrue allreduce_cost allgather_cost assertTrue allreduce_cost reduce_scatter_cost test_mm_strategies torch distributed tensor _ops _matrix_ops mm_strategy mesh = build_device_mesh lhs_tensor = torch randn rhs_tensor = torch randn lhs_tensor_meta = extract_tensor_meta lhs_tensor rhs_tensor_meta = extract_tensor_meta rhs_tensor mm_combs = Shard Replicate Replicate Shard Shard Shard Replicate Replicate lhs rhs mm_combs lhs_spec = DTensorSpec mesh lhs lhs_tensor_meta rhs_spec = DTensorSpec mesh rhs rhs_tensor_meta op_schema = OpSchema torch ops aten mm default OpStrategy OpSpec lhs_spec OpStrategy OpSpec rhs_spec test strategy res_strategies = mm_strategy op_schema strtgy res_strategies strategies strtgy input_specs == lhs_spec rhs_spec assertEqual strtgy redistribute_cost break op_schema = OpSchema torch ops aten mm default lhs_spec rhs_spec test sharding prop output_sharding = DTensor _op_dispatcher sharding_propagator propagate_op_sharding_non_cached op_schema assertFalse output_sharding needs_redistribute test_bmm_strategies torch distributed tensor _ops _matrix_ops bmm_strategy mesh = build_device_mesh lhs_tensor = torch randn rhs_tensor = torch randn lhs_tensor_meta = extract_tensor_meta lhs_tensor rhs_tensor_meta = extract_tensor_meta rhs_tensor bmm_combs = Shard Shard Shard Replicate Replicate Shard Shard Shard Replicate Replicate lhs rhs bmm_combs lhs_spec = DTensorSpec mesh lhs lhs_tensor_meta rhs_spec = DTensorSpec mesh rhs rhs_tensor_meta op_schema = OpSchema torch ops aten bmm default OpStrategy OpSpec lhs_spec OpStrategy OpSpec rhs_spec test strategy res_strategies = bmm_strategy op_schema strtgy res_strategies strategies strtgy input_specs == lhs_spec rhs_spec assertEqual strtgy redistribute_cost break op_schema = OpSchema torch ops aten bmm default lhs_spec rhs_spec test sharding prop output_sharding = DTensor _op_dispatcher sharding_propagator propagate_op_sharding_non_cached op_schema assertFalse output_sharding needs_redistribute ------------- Test op strategy registration ------------- custom op without List Tensor input reference https docs pytorch org docs stable library html#torch library register_autograd torch library custom_op mylib numpy_sin mutates_args= numpy_sin x torch Tensor y torch Tensor - torch Tensor x_np = x cpu numpy y_np = y cpu numpy out_np = np sin x_np + np sin y_np torch from_numpy out_np device=x device setup_context ctx inputs output x y = inputs ctx save_for_backward x y backward ctx grad x y = ctx saved_tensors grad x cos grad y cos numpy_sin register_fake _fw x y torch empty_like x torch library register_autograd mylib numpy_sin backward setup_context=setup_context custom op List Tensor input torch library custom_op mylib numpy_tuple_sin mutates_args= numpy_tuple_sin x torch Tensor y list torch Tensor z torch Tensor - torch Tensor x_np = x cpu numpy y_np = i cpu numpy i y z_np = z cpu numpy out_np = np sin x_np + np sin z_np + sum np sin i i y_np torch from_numpy out_np device=x device setup_tuple_context ctx inputs output x y z = inputs ctx save_for_backward x y z tuple_backward ctx grad x y z = ctx saved_tensors grad x cos grad i cos i y grad z cos numpy_tuple_sin register_fake _fw_tuple x y z torch empty_like x torch library register_autograd mylib numpy_tuple_sin tuple_backward setup_context=setup_tuple_context contextmanager op_strategy_context op_overload strategy_func schema_info=None Context manager setting clearing op strategies Args op_overload The operator overload set clear strategy strategy_func The strategy function set operator overload schema_info Optional schema information operator overload Yields None propagator = DTensor _op_dispatcher sharding_propagator _origin_op_strategy_funcs = None _origin_op_strategy_schema = None try register op strategy op_overload propagator op_strategy_funcs _origin_op_strategy_funcs = propagator op_strategy_funcs op_overload del propagator op_strategy_funcs op_overload op_overload propagator op_to_schema_info _origin_op_strategy_schema = propagator op_to_schema_info op_overload del propagator op_to_schema_info op_overload register_op_strategy op_overload schema_info=schema_info strategy_func yield finally clear op strategy cache _origin_op_strategy_funcs None op_overload propagator op_strategy_funcs del propagator op_strategy_funcs op_overload propagator op_strategy_funcs op_overload = _origin_op_strategy_funcs _origin_op_strategy_schema None op_overload propagator op_to_schema_info del propagator op_to_schema_info op_overload propagator op_to_schema_info op_overload = _origin_op_strategy_schema propagator propagate_op_sharding cache cache_clear detect_exists_identical_opspec args op mesh strategy_function - bool Given sample input args detect identical OpSpecs exists under same OpStrategy tree_args = tree_leaves args metadata each argument arg_tensor_metadata = extract_tensor_meta i i args possible combination placements each arg arg_placement_comb = i tree_args isinstance i torch Tensor possible placement choice argument i placement_choices = Replicate Shard i i range i ndim expand placement choice into full Placements argument i arg_placement_comb append list itertools product placement_choices repeat=mesh ndim random shuffle arg_placement_comb - arg_opspec_list = idx arg_placement enumerate arg_placement_comb arg_opspec_list append placement arg_placement arg_opspec_list idx append OpSpec output_specs=DTensorSpec mesh placement tensor_meta=arg_tensor_metadata idx op_schema = OpSchema op args_schema= tuple OpStrategy i i arg_opspec_list kwargs_schema= op_strategy_context op strategy_function output_strategy = strategy_function op_schema OpSpec doesn t have hashing convert str compare output_strategy_str_list = str j i tree_leaves output_strategy j i strategies len output_strategy_str_list == len set output_strategy_str_list DistTensorReplicateStrategyRegistrationTest DTensorTestBase with_comms patch torch distributed tensor _sharding_prop ShardingPropagator _select_strategy test_replicate_strategy_placement mock_select_strategy costs_from__select_strategy = mock_select_func strategy op_schema=None function copied _select_strategy cost capturing nonlocal costs_from__select_strategy len strategy strategies == costs_from__select_strategy = strategy strategies redistribute_cost strategy strategies op_spec_costs list float = op_spec strategy strategies assert op_spec redistribute_cost None must set redistribute cost each OpSpec costs_from__select_strategy append op_spec redistribute_cost redistribute_cost = sum chain from_iterable op_spec redistribute_cost op_spec_costs append redistribute_cost strategy strategies op_spec_costs index min op_spec_costs mock_select_strategy side_effect = mock_select_func mesh = init_device_mesh device_type world_size comm_mode = CommDebugMode test_op = torch ops mylib numpy_sin input_x = torch randn device=self device_type input_y = torch randn device=self device_type output = test_op input_x input_y input_x_dt = distribute_tensor input_x mesh Shard Shard input_y_dt = distribute_tensor input_y mesh Shard Shard x_spec = DTensorSpec mesh input_x_dt placements extract_tensor_meta input_x new_x_spec = DTensorSpec mesh Replicate Replicate extract_tensor_meta input_x y_spec = DTensorSpec mesh input_y_dt placements extract_tensor_meta input_y new_y_spec = DTensorSpec mesh Replicate Replicate extract_tensor_meta input_y comm_mode op_strategy_context test_op default replicate_op_strategy output_dt = test_op input_x_dt input_y_dt assertEqual comm_mode get_comm_counts torch ops c d_functional all_gather_into_tensor expected_cost = redistribute_cost x_spec new_x_spec redistribute_cost y_spec new_y_spec assertEqual expected_cost costs_from__select_strategy assertEqual output_dt full_tensor output assertEqual output_dt placements Replicate Replicate assertTrue detect_exists_identical_opspec input_x input_y op=test_op default mesh=mesh strategy_function=replicate_op_strategy with_comms test_tuple_replicate_strategy_placement mesh = init_device_mesh device_type world_size test_op = torch ops mylib numpy_tuple_sin op_strategy_context test_op default replicate_op_strategy schema_info=RuntimeSchemaInfo needs_pytree=True input_x = torch randn device=self device_type input_y = torch randn device=self device_type _ range input_z = torch randn device=self device_type output = test_op input_x input_y input_z input_x_dt = distribute_tensor input_x mesh Shard Shard input_y_dt = distribute_tensor i mesh Shard Shard i input_y input_z_dt = distribute_tensor input_z mesh Shard Shard output_dt = test_op input_x_dt input_y_dt input_z_dt assertEqual output_dt full_tensor output assertEqual output_dt placements Replicate Replicate TestStrategyHashing DTensorTestBase with_comms test_call_with_different_nontensor_args mesh = build_device_mesh global_tensor = torch tensor shard_spec = Shard sharded_dtensor = distribute_tensor global_tensor mesh shard_spec op_strategy_context torch ops aten sort default replicate_op_strategy intentionally do supply ` schema_info=RuntimeSchemaInfo ` torch sort sharded_dtensor dim= sort each column out _ = torch sort sharded_dtensor dim= sort each row op_strategy_context torch ops aten sort default replicate_op_strategy out _ = torch sort sharded_dtensor dim= assertEqual out full_tensor out full_tensor DistTensorReplicateStrategyRegistrationTestWithLocalTensor = create_local_tensor_test_class DistTensorReplicateStrategyRegistrationTest TestStrategyHashingWithLocalTensor = create_local_tensor_test_class TestStrategyHashing __name__ == __main__ run_tests