mypy allow-untyped-defs typing Optional torch torch nn functional F expanded_weights_impl implements_per_sample_grads expanded_weights_utils forward_helper is_batch_first set_grad_sample_if_exists unpack_expanded_weight_or_tensor implements_per_sample_grads F linear LinearPerSampleGrad torch autograd Function staticmethod pyrefly ignore bad-override forward ctx _ __ expanded_args_and_kwargs len expanded_args_and_kwargs shape = raise RuntimeError Input does have batch dimension Expanded Weights expected input f least rank got rank len expanded_args_and_kwargs shape expanded_kwargs = bias expanded_args_and_kwargs len expanded_args_and_kwargs == None expanded_args = expanded_args_and_kwargs ctx batch_first = is_batch_first expanded_args_and_kwargs output = forward_helper F linear expanded_args expanded_kwargs ctx args = expanded_args ctx kwargs = expanded_kwargs output staticmethod pyrefly ignore bad-override backward ctx grad_output input weight = ctx args bias = ctx kwargs bias results list Optional torch Tensor = results append None kwarg_names results append None op reference input requires_grad results append grad_output matmul unpack_expanded_weight_or_tensor weight results append None results extend None weight bias don t compute batched gradients ctx batch_first grad_output = grad_output transpose input = input transpose weight bias get their grad_sample fields set directly they exist set_grad_sample_if_exists weight lambda _ torch einsum n i n j- nij grad_output input set_grad_sample_if_exists bias lambda _ torch einsum n k- nk grad_output tuple results