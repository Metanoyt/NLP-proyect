Owner s module inductor os re unittest torch torch nn torch _dynamo testing reset_rng_state torch _inductor config test_operators torch _inductor codegen multi_kernel MultiKernelCall torch _inductor test_case TestCase torch _inductor utils run_and_get_code torch nn functional F torch testing make_tensor torch testing _internal common_utils instantiate_parametrized_tests parametrize skipIfRocm skipIfXpu torch testing _internal inductor_utils GPU_TYPE HAS_GPU IS_BIG_GPU requires_triton TransformerSnippet nn Module __init__ - None super __init__ ln = nn LayerNorm ln = nn LayerNorm forward x x x = F dropout x x = F dropout ln x ln x + x example_inputs torch randn GPU_TYPE torch randn GPU_TYPE _contains_multi_kernel_code wrapper_code str re search r multi_kernel_ ^ = async_compile multi_kernel wrapper_code None _contains_size_hint_multi_kernel_code wrapper_code str re search r multi_kernel_ ^ = async_compile size_hint_multi_kernel wrapper_code None make_cpp_wrapper_test orig_test extra_args Wrap existing test into new test cpp-wrapper enabled Make free function rather than staticmethod MultiKernelTest Otherwise we get TypeError staticmethod object callable error py py works config patch cpp_wrapper True skipIfXpu msg= cpp wrapper doesn t currently work XPU stack fn The same kernel may have been compiled previous tests cpp_wrapper disabled Clear cache so we go ahead re-compile kernel cpp_wrapper enabled torch _inductor codecache codecache PyCodeCache cache_clear orig_test extra_args fn config patch triton multi_kernel int os environ get TORCHINDUCTOR_MULTI_KERNEL benchmark_kernel True multi_kernel_hints instantiate_parametrized_tests MultiKernelTest TestCase test_softmax expect_multi_kernel=True x = torch rand GPU_TYPE ref = torch softmax x - compiled_fn = torch compile torch softmax act wrapper_code = run_and_get_code compiled_fn x - wrapper_code will contains entries cpp_wrapper=True One first pass one second pass We mainly care about wrapper final pass here wrapper_code = wrapper_code - assertEqual ref act expect_multi_kernel assertTrue _contains_multi_kernel_code wrapper_code assertFalse _contains_multi_kernel_code wrapper_code requires_triton TODO bobrenjc fix multi-kernel ROCM skipIfRocm skipIfXpu unittest skipIf IS_BIG_GPU templates require big gpu test_triton_gemm fn x y x y compiled_fn = torch compile fn options= max_autotune True max_autotune_gemm_backends TRITON x = torch randn device=GPU_TYPE y = torch randn device=GPU_TYPE torch _dynamo mark_dynamic x act wrapper_code = run_and_get_code compiled_fn x y ref = fn x y wrapper_code will contains entries cpp_wrapper=True One first pass one second pass We mainly care about wrapper final pass here wrapper_code = wrapper_code - assertEqual ref act assertTrue _contains_size_hint_multi_kernel_code wrapper_code requires_triton TODO bobrenjc fix multi-kernel ROCM skipIfRocm skipIfXpu unittest skipIf IS_BIG_GPU templates require big gpu test_triton_relu_fused_gemm fn x y x y relu compiled_fn = torch compile fn options= max_autotune True max_autotune_gemm_backends TRITON x = torch randn device=GPU_TYPE y = torch randn device=GPU_TYPE torch _dynamo mark_dynamic x act wrapper_code = run_and_get_code compiled_fn x y ref = fn x y wrapper_code will contains entries cpp_wrapper=True One first pass one second pass We mainly care about wrapper final pass here wrapper_code = wrapper_code - assertEqual ref act assertTrue _contains_size_hint_multi_kernel_code wrapper_code parametrize force_kernel unittest mock patch dict os environ TORCHINDUCTOR_DISABLE_MULTI_KERNEL_CACHE test_softmax_force_non_persistent_reduction force_kernel Force specific sub-kernel being picked mocking benchmark result x = torch rand GPU_TYPE mock_latency = mock_latency force_kernel = make sure force_kernel will picked f x torch softmax x - + force_kernel orig_run = MultiKernelCall run picked_kernel = None mock_run args kwargs out = orig_run args kwargs nonlocal picked_kernel picked_kernel = picked_kernel out unittest mock patch object MultiKernelCall run mock_run unittest mock patch object MultiKernelCall benchmark_sub_kernels lambda args kwargs mock_latency torch compile f x assertEqual picked_kernel force_kernel config patch warn_mix_layout True test_softmax_warn_mixed_layout test_softmax test_softmax_cpp_wrapper = make_cpp_wrapper_test test_softmax expect_multi_kernel=True test_layernorm ln = nn LayerNorm GPU_TYPE x = torch rand GPU_TYPE ref = ln x act = torch compile ln x assertEqual ref act atol= e- rtol= e- test_inplace_update Inductor generate inplace kernel mul f x y x sum dim=- keepdims=True y y x = torch rand GPU_TYPE y = torch rand GPU_TYPE ref = f x y act = torch compile f x y assertEqual ref act test_transformer_snippet model = TransformerSnippet GPU_TYPE x = model example_inputs f x y = model x y reset_rng_state ref = f x opt_f = torch compile f reset_rng_state act = opt_f x don t compare tensor using inductor random number generator inductor random number implementation different eager We should fallback eager we want test accuracy config fallback_random assertEqual ref act atol= e- rtol= e- test_transformer_snippet_with_fallback_random Same test_transformer_snippet fallback random number generator eager so we can check accuracy config patch fallback_random True test_transformer_snippet test_batchnorm_training For training batchnorm will tracking running mean variance during forward pass The kernel generated inductor currently will pass those tensors twice arguments once input once output They ruled out in-out argument because they considered graph inputs Multi-kernel previously assumes we never pass same argument multi times kernel No matter we change inductor behavior assure s better make multi-kernel being able handle those cases bn = nn BatchNorm d GPU_TYPE torch compile f x bn x sum backward _ wrapper_code _ = run_and_get_code f torch randn device=GPU_TYPE assertTrue _contains_multi_kernel_code wrapper_code test_pass_same_arg_multi_times A super simple example simulate how BatchNorm update running stats Inductor currently pass same tensor multiple times generated kernel once input once output Here paster generated kernel without multi-kernel enabled https gist github com shunting f b b b f e dcd e cf f x y x = x sum dim= keepdim=False y copy_ y + x x = torch randn device=GPU_TYPE y = torch randn device=GPU_TYPE y_ref = y clone ref = f x y_ref noqa F act = torch compile f x y noqa F assertEqual y_ref y test_reduction_scratch_buffer force_multi_kernel= The explicitly realized buffer test function will passed scratch buffer non-persistent reduction kernel can skipped persistent reduction kernel This causes different argument lists non-persistent reduction kernel persistent reduction kernel Check documentation around torch _inductor config triton multi_kernel about how interpret force_multi_kernel argument f x x = x sum dim=- keepdim=True + x x = test_operators realize x x = x sum dim=- keepdim=True + x x x = torch rand device=GPU_TYPE ref = f x config patch triton multi_kernel force_multi_kernel act = torch compile f x assertEqual ref act test_split_scan force_multi_kernel= f x x = x view - torch cumsum x x = make_tensor low= dtype=torch float device=GPU_TYPE expect = f x config patch triton multi_kernel force_multi_kernel actual = torch compile f x assertEqual expect actual test_sort_disables_multi_kernel force_multi_kernel= Sort currently requires persistent kernel so multi-kernel possible Make sure falls back gracefully f x x sort - values x = torch rand device=GPU_TYPE expect = f x config patch triton multi_kernel force_multi_kernel actual = torch compile f x assertEqual expect actual Use benchmarking pick faster kernel test_reduction_scratch_buffer_cpp_wrapper = make_cpp_wrapper_test test_reduction_scratch_buffer force_multi_kernel= force pick persistent reduction This can good test since persistent reduction uses less call arguments than corresponding non-persistent reduction test_reduction_scratch_buffer_cpp_wrapper_persistent_reduction = make_cpp_wrapper_test test_reduction_scratch_buffer force_multi_kernel= force pick non-persistent reduction test_reduction_scratch_buffer_cpp_wrapper_non_persistent_reduction = make_cpp_wrapper_test test_reduction_scratch_buffer force_multi_kernel= __name__ == __main__ torch _inductor test_case run_tests HAS_GPU run_tests