Copyright c Meta Platforms Inc affiliates collections abc Sequence typing cast Optional torch torch distributed tensor _dtensor_spec DTensorSpec torch distributed tensor _op_schema OpSchema OpSpec OpStrategy RuntimeSchemaInfo StrategyType TupleStrategy torch distributed tensor _ops utils generate_redistribute_costs infer_broadcast_dims_map map_placements_after_broadcast normalize_dim register_op_strategy torch distributed tensor placement_types Partial Placement Replicate Shard torch utils _typing_utils not_none aten = torch ops aten leave remaining pointwise_ops list here convenience Below ops some pointwise ops yet supported they might complete list pointwise_ops = fake_quantize_per_channel_affine fake_quantize_per_tensor_affine floor_divide floor_divide deprecated frexp multiple output pointwise op need add support gradient need investigation op imag complex data type only quantized_batch_norm quantized_max_pool d quantized_max_pool d real complex data type only pointwise_ops = please keep entries below alphabetically sorted aten __ilshift__ Scalar aten __ilshift__ Tensor aten __irshift__ Scalar aten __irshift__ Tensor aten __lshift__ Scalar aten __lshift__ Tensor aten __rshift__ Scalar aten __rshift__ Tensor aten _conj default aten abs default aten abs out aten abs_ default aten acos default aten acos out aten acos_ default aten acosh default aten acosh out aten acosh_ default aten add Scalar aten add out aten add_ Scalar aten addcdiv default aten addcdiv out aten addcdiv_ default aten addcmul default aten addcmul out aten addcmul_ default aten angle default aten angle out aten asin default aten asin out aten asin_ default aten asinh default aten asinh out aten asinh_ default aten atan default aten atan out aten atan default aten atan out aten atan _ default aten atan_ default aten atanh default aten atanh out aten atanh_ default aten bitwise_and Scalar aten bitwise_and Scalar_Tensor aten bitwise_and Scalar_out aten bitwise_and Tensor aten bitwise_and Tensor_out aten bitwise_and_ Scalar aten bitwise_and_ Tensor aten bitwise_left_shift Scalar_Tensor aten bitwise_left_shift Tensor aten bitwise_left_shift Tensor_Scalar aten bitwise_left_shift Tensor_Scalar_out aten bitwise_left_shift Tensor_out aten bitwise_left_shift_ Tensor aten bitwise_left_shift_ Tensor_Scalar aten bitwise_not default aten bitwise_not out aten bitwise_not_ default aten bitwise_or Scalar aten bitwise_or Scalar_Tensor aten bitwise_or Scalar_out aten bitwise_or Tensor aten bitwise_or Tensor_out aten bitwise_or_ Scalar aten bitwise_or_ Tensor aten bitwise_right_shift Scalar_Tensor aten bitwise_right_shift Tensor aten bitwise_right_shift Tensor_Scalar aten bitwise_right_shift Tensor_Scalar_out aten bitwise_right_shift Tensor_out aten bitwise_right_shift_ Tensor aten bitwise_right_shift_ Tensor_Scalar aten bitwise_xor Scalar aten bitwise_xor Scalar_Tensor aten bitwise_xor Scalar_out aten bitwise_xor Tensor aten bitwise_xor Tensor_out aten bitwise_xor_ Scalar aten bitwise_xor_ Tensor aten ceil default aten ceil out aten ceil_ default aten clamp default aten clamp Tensor aten clamp out aten clamp_ default aten clamp_ Tensor aten clamp_min default aten clamp_min Tensor aten clamp_max default aten clamp_max Tensor aten clip default aten clip out aten clip_ default aten conj_physical default aten conj_physical out aten conj_physical_ default aten copysign Scalar aten copysign Scalar_out aten copysign Tensor aten copysign out aten copysign_ Scalar aten copysign_ Tensor aten cos default aten cos out aten cos_ default aten cosh default aten cosh out aten cosh_ default aten deg rad default aten deg rad out aten deg rad_ default aten digamma default aten digamma out aten digamma_ default aten div Tensor aten div Tensor_mode aten div out aten div out_mode aten div_ Tensor aten div_ Tensor_mode aten eq Tensor aten eq Tensor_out aten eq Scalar aten eq Scalar_out aten erf default aten erf out aten erf_ default aten erfc default aten erfc out aten erfc_ default aten erfinv default aten erfinv out aten erfinv_ default aten exp default aten exp out aten exp default aten exp out aten exp _ default aten exp_ default aten expm default aten expm out aten expm _ default aten float_power Scalar aten float_power Scalar_out aten float_power Tensor_Scalar aten float_power Tensor_Scalar_out aten float_power Tensor_Tensor aten float_power Tensor_Tensor_out aten float_power_ Scalar aten float_power_ Tensor aten floor default aten floor out aten floor_ default aten fmod Scalar aten fmod Scalar_out aten fmod Tensor aten fmod Tensor_out aten fmod_ Scalar aten fmod_ Tensor aten frac default aten frac out aten frac_ default aten ge Scalar aten ge Tensor aten gelu default aten gt Tensor aten gt Tensor_out aten gt Scalar aten gt Scalar_out aten gt Scalar aten gt Tensor aten hypot default aten hypot out aten hypot_ default aten i default aten i out aten i _ default aten igamma default aten igamma out aten igamma_ default aten igammac default aten igammac out aten igammac_ default aten isinf default aten isnan default aten isneginf default aten isneginf out aten isposinf default aten isposinf out aten ldexp default aten ldexp out aten ldexp_ default aten lt Tensor aten lt Tensor_out aten lt Scalar aten lt Scalar_out aten le Scalar aten le Tensor aten lerp Scalar aten lerp Scalar_out aten lerp Tensor aten lerp Tensor_out aten lerp_ Scalar aten lerp_ Tensor aten lgamma default aten lgamma out aten lgamma_ default aten log default aten log out aten log default aten log out aten log _ default aten log p default aten log p out aten log p_ default aten log default aten log out aten log _ default aten log_ default aten logaddexp default aten logaddexp out aten logaddexp default aten logaddexp out aten logical_and default aten logical_and out aten logical_and_ default aten logical_not default aten logical_not out aten logical_not_ default aten logical_or default aten logical_or out aten logical_or_ default aten logical_xor default aten logical_xor out aten logical_xor_ default aten logit default aten logit out aten logit_ default aten masked_fill Scalar aten maximum default aten maximum out aten minimum default aten minimum out aten mul out aten mvlgamma default aten mvlgamma out aten mvlgamma_ default aten native_dropout_backward default aten native_dropout_backward out aten nan_to_num default aten nan_to_num out aten nan_to_num_ default aten ne Scalar aten neg default aten neg out aten neg_ default aten nextafter default aten nextafter out aten nextafter_ default aten polygamma default aten polygamma out aten polygamma_ default aten positive default aten pow Scalar aten pow Scalar_out aten pow Tensor_Scalar aten pow Tensor_Scalar_out aten pow Tensor_Tensor aten pow Tensor_Tensor_out aten pow_ Scalar aten pow_ Tensor aten reciprocal default aten reciprocal out aten reciprocal_ default aten rad deg default aten rad deg out aten rad deg_ default aten relu default aten relu_ default aten remainder Scalar aten remainder Scalar_Tensor aten remainder Scalar_out aten remainder Tensor aten remainder Tensor_out aten remainder_ Scalar aten remainder_ Tensor aten round decimals aten round decimals_out aten round default aten round out aten round_ decimals aten round_ default aten rsqrt default aten rsqrt out aten rsqrt_ default aten rsub Scalar aten sgn default aten sgn out aten sgn_ default aten sigmoid default aten sigmoid out aten sigmoid_ default aten sign default aten sign out aten sign_ default aten signbit default aten signbit out aten silu default aten silu out aten sin default aten sin out aten sin_ default aten sinc default aten sinc out aten sinc_ default aten sinh default aten sinh out aten sinh_ default aten sqrt default aten sqrt out aten sqrt_ default aten square default aten square out aten square_ default aten sub Scalar aten sub Tensor aten sub out aten sub_ Scalar aten sub_ Tensor aten tan default aten tan out aten tan_ default aten tanh default aten tanh out aten tanh_ default aten true_divide Tensor aten trunc default aten trunc out aten trunc_ default aten where aten where self_out aten xlogy OutScalar_Self aten xlogy OutScalar_Other aten xlogy OutTensor aten xlogy Scalar_Other aten xlogy Scalar_Self aten xlogy Tensor aten xlogy_ Scalar_Other aten xlogy_ Tensor backward point-wise ops please keep entries below alphabetically sorted aten gelu_backward default aten sigmoid_backward default aten silu_backward default aten tanh_backward default aten threshold_backward default linear pointwise ops map key op value type linearity linear_pointwise_ops = aten dtype aten add Tensor aten add_ Tensor aten div Scalar aten div_ Scalar aten mul Scalar aten mul_ Scalar aten mul Tensor aten mul_ Tensor aten copy_ default pointwise_strategy op_schema OpSchema linearity int = - - OpStrategy followed_strategy_index = - max_shards = - max_ndim = - op_schema is_inplace_op inplace op should follow first arg strategy followed_strategy = op_schema args_schema followed_strategy_index = op_schema is_out_variant_op out variant op should follow out kwarg strategy followed_strategy = op_schema kwargs_schema out out variant technically kwarg strategy follow so does have index we set reasonably large number just indicate s valid index followed_strategy_index = normal pointwise op we choose follow arg max shards case operands needs reshard case multiple operands max shard we take one max number dimensions idx arg_strategy enumerate op_schema args_schema isinstance arg_strategy OpStrategy continue arg_max_shards = arg_strategy max_num_shards arg_max_ndim = arg_strategy ndim arg_max_shards max_shards arg_max_shards == max_shards arg_max_ndim max_ndim followed_strategy_index = idx max_shards = arg_max_shards max_ndim = arg_max_ndim followed_strategy = op_schema args_schema followed_strategy_index assert isinstance followed_strategy OpStrategy f no strategy follow op_schema common_pointwise_strategy op_schema args_schema followed_strategy followed_strategy_index linearity linear_pointwise_strategy op_schema OpSchema - StrategyType Linear pointwise operators can propagate pending reductions For example c = add b pending sum then c will pending sum well without any communication overhead Note Only unary binary operations supported out variant ops supported There re multiple types linearity refer doc common_pointwise_strategy more details linearity_type = linear_pointwise_ops get op_schema op - pointwise_strategy op_schema linearity=linearity_type common_pointwise_strategy args_schema Sequence object followed_strategy OpStrategy followed_strategy_index int linearity int = - scalar_tensor_idx Optional int = None - OpStrategy Common strategy pointwise operations Args args_schema Input arguments schema followed_strategy Strategy follow output placement followed_strategy_index Index strategy being followed linearity depending operator we support different types linearity - operation does support linearity unary operation supports linearity output propagates partial binary operation supports add linearity where requires every operand partial output propagates partial binary operation supports multiplicative linearity where requires primary operand partial other operands replicate output propagates partial scalar_tensor_idx Index Replicate scalar tensor which we allow mesh different mesh followed_strategy handle broadcasting common_shape = torch broadcast_shapes arg shape arg args_schema isinstance arg OpStrategy pointwise_strategy = OpStrategy op_spec followed_strategy strategies spec_to_follow = op_spec output_spec out_placements list Placement = placement spec_to_follow placements isinstance placement Shard shard_dim = normalize_dim placement dim len spec_to_follow shape common_ndim = len common_shape new_shard_dim = common_ndim - len spec_to_follow shape + shard_dim out_placements append Shard new_shard_dim isinstance placement Partial note only partial-sum partial-avg supported linearity partial_supports_linearity = placement is_partial sum placement is_partial avg linearity partial_supports_linearity propagate partial placement out_placements append placement clear partial placement op does support linearity default we just replicate partial need see optimal all cases out_placements append Replicate out_placements append placement input_specs list DTensorSpec = redistribute_costs list list float = input_idx input_arg enumerate args_schema isinstance input_arg OpStrategy input_arg_spec = input_arg strategies output_spec sanity check all args follow same strategy same DeviceMesh input_arg mesh = followed_strategy mesh For scalar tensor arg fused ops do follow followed_strategy instead let input mesh Replicate placements propagate through input_idx == scalar_tensor_idx assert all p == Replicate p input_arg_spec placements input_arg_target_spec = DTensorSpec mesh=input_arg mesh placements=input_arg_spec placements tensor_meta=input_arg_spec tensor_meta input_specs append input_arg_target_spec redistribute_costs append generate_redistribute_costs input_arg input_arg_target_spec continue raise ValueError f Could run pointwise computation across different mesh f Found input_arg mesh followed_strategy mesh every arg follow out_placements need handle broadcasting input_arg_dims_map = infer_broadcast_dims_map common_shape input_arg_spec shape Determine input should convert Partial Replicate base linearity should_convert_partial = linearity == input_idx = followed_strategy_index Don t convert followed strategy input_target_placements = map_placements_after_broadcast tuple out_placements common_shape input_arg_dims_map partial_to_replicate=should_convert_partial input_arg_target_spec = DTensorSpec mesh=followed_strategy mesh placements=input_target_placements tensor_meta=input_arg_spec tensor_meta input_specs append input_arg_target_spec redistribute_costs append generate_redistribute_costs input_arg input_arg_target_spec pointwise_strategy strategies append OpSpec output_specs=DTensorSpec mesh=followed_strategy mesh placements=tuple out_placements input_specs=input_specs redistribute_cost=redistribute_costs pointwise_strategy op linear_pointwise_ops keys register_op_strategy op schema_info=RuntimeSchemaInfo static_kwargkey= out linear_pointwise_strategy op pointwise_ops register_op_strategy op schema_info=RuntimeSchemaInfo static_kwargkey= out pointwise_strategy TODO add all for_each ops for_each_ops = aten _foreach_abs default aten _foreach_abs_ default aten _foreach_addcdiv_ Scalar aten _foreach_addcdiv_ ScalarList aten _foreach_addcdiv_ Tensor aten _foreach_addcmul Scalar aten _foreach_addcmul_ Scalar aten _foreach_addcmul_ ScalarList aten _foreach_addcmul_ Tensor aten _foreach_clamp_max_ Scalar aten _foreach_clamp_min_ Scalar aten _foreach_div_ List aten _foreach_div_ Scalar aten _foreach_div_ ScalarList aten _foreach_div_ Tensor aten _foreach_div List aten _foreach_div Scalar aten _foreach_div ScalarList aten _foreach_div Tensor aten _foreach_lerp_ Scalar aten _foreach_maximum_ List aten _foreach_mul Scalar aten _foreach_mul ScalarList aten _foreach_mul Tensor aten _foreach_mul List aten _foreach_mul_ Scalar aten _foreach_mul_ ScalarList aten _foreach_mul_ Tensor aten _foreach_mul_ List aten _foreach_pow List aten _foreach_pow ScalarList aten _foreach_neg default aten _foreach_neg_ default aten _foreach_reciprocal_ default aten _foreach_sub Scalar aten _foreach_sub_ Scalar aten _foreach_sub List aten _foreach_sub_ List aten _foreach_sub ScalarList aten _foreach_sub_ ScalarList aten _foreach_sqrt default aten _foreach_sqrt_ default aten _foreach_zero_ default aten _foreach_exp default aten _foreach_exp_ default aten _foreach_cos default aten _foreach_cos_ default aten _foreach_log default aten _foreach_log_ default aten _amp_foreach_non_finite_check_and_unscale_ default for_each_linearity_ops = aten _foreach_add Scalar aten _foreach_add_ Scalar aten _foreach_add_ ScalarList aten _foreach_add List aten _foreach_add_ List list_pointwise_strategy op_schema OpSchema linearity bool = False - StrategyType Apply pointwise strategy zipped arguments For example we run foreach add two lists l l then we apply pointwise strategy each pair l i l i If first argument list second later one tensor then we broadcast tensor replicating into list length first argument Args mesh DeviceMesh device mesh pointwise ops op_schema OpSchema schema operator generate strategy linearity bool specify whether op + op b = op + b Returns OpStrategy generated strategy args_tuple_strategies args_schema tuple object - list Optional TupleStrategy first_arg = args_schema assert isinstance first_arg TupleStrategy strategy_len = len first_arg children tuple_strategies list Optional TupleStrategy = arg_idx arg enumerate args_schema isinstance arg TupleStrategy every tuple strategy should have same length assert len arg children == strategy_len tuple_strategies append arg isinstance arg OpStrategy arg_idx implicitly broadcast tuple_strategies append TupleStrategy arg _ range strategy_len raise RuntimeError f list op only supports tuple strategy op_schema insert None placeholder so idx arg kept tuple_strategies append None tuple_strategies args_strategies = args_tuple_strategies op_schema args_schema follow_strategy TupleStrategy = not_none args_strategies list_strategy list OpStrategy = child_idx child_strtgy enumerate follow_strategy children assert isinstance child_strtgy OpStrategy args_schema list Optional OpStrategy = cast OpStrategy arg_strategy children child_idx arg_strategy None arg_strategy args_strategies pointwise_strategy OpStrategy = common_pointwise_strategy args_schema child_strtgy linearity scalar_tensor_idx= _FUSED_OP_SCALAR_IDX op_schema op fused_ops None list_strategy append pointwise_strategy TupleStrategy list_strategy list_linear_pointwise_strategy op_schema OpSchema - StrategyType each list op stratgy supports linearity list_pointwise_strategy op_schema linearity=True op for_each_ops register_op_strategy op schema_info=RuntimeSchemaInfo needs_pytree=True list_pointwise_strategy op for_each_linearity_ops register_op_strategy op schema_info=RuntimeSchemaInfo needs_pytree=True list_linear_pointwise_strategy fused_ops = aten _fused_adam_ default aten _fused_adam default aten _fused_adam tensor_lr aten _fused_adam_ tensor_lr aten _fused_adamw_ default aten _fused_adamw default aten _fused_adamw tensor_lr aten _fused_adamw_ tensor_lr The state_steps arg fused adam adamw Replicate scalar tensor which will put compute_mesh op across all parameter groups even when all parameter groups same device mesh This idx will help avoid hitting exceptions unnecessary redistribute during sharding propagation _FUSED_OP_SCALAR_IDX = op fused_ops register_op_strategy op schema_info=RuntimeSchemaInfo needs_pytree=True list_pointwise_strategy