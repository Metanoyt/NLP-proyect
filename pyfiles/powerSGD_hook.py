mypy allow-untyped-defs logging math collections defaultdict torch torch distributed dist torch distributed distributed_c d torch utils _typing_utils not_none default_hooks default __all__ = PowerSGDState powerSGD_hook batched_powerSGD_hook logger = logging getLogger __name__ _orthogonalize matrices epsilon= Decide between Gram-Schmidt QR factorization orthogonalize batch matrices QR factorization doesn t work half-precision usually faster rank assert len matrices shape == matrices shape = matrices shape num_matrices = matrices shape rank = matrices shape dtype = matrices dtype rank = dtype torch float torch bfloat _orthogonalize_gram_schmidt matrices epsilon=epsilon torch linalg qr matrices out= matrices torch empty num_matrices rank rank device=matrices device dtype=dtype _orthogonalize_gram_schmidt matrices epsilon= Apply Gram-Schmidt procedure orthogonalize batch matrices If epsilon equivalent ` torch qr matrices out= matrices _ ` num_cols = matrices shape i range num_cols Normalize i th column col = matrices i i + If no epsilon added here division zero may caused vanishing gradients This epsilon needed input batch matrices covers gradients least one entire layer neural network epsilon == Note col can underflow overflow we use FP May need consider multiplying scaling factor dividing later using bfloat instead try col = torch norm col dim= keepdim=True except ZeroDivisionError logger error The matrices orthogonalized has least column all s Please set small value such e- ` orthogonalization_epsilon ` PowerSGD state Recover values NaNs s col fill_ col = torch norm col dim= keepdim=True + epsilon Project rest remove i + num_cols rest = matrices i + rest -= torch sum col rest dim= keepdim=True col _should_compress num_rows num_cols matrix_approximation_rank min_compression_rate Recommend tensor given worth compressing Returns recommendation whether D tensor described arguments worth compressing including statistics describing expected savings compression We consider tensor worth compressing when ` ` min_compression_rate ` ` uncompressed size compressed size where uncompressed size = ` ` num_rows ` ` ` ` num_cols ` ` compressed size = ` ` num_rows ` ` + ` ` num_cols ` ` ` ` matrix_approximation_rank ` ` The result function tuple form compression_recommendation uncompressed_el_count compressed_el_count where compression_recommendation true tensor worth compressing false otherwise see above uncompressed_el_count uncompressed element count i e ` ` num_rows ` ` ` ` num_cols ` ` compress_el_count element count after compression i e ` ` num_rows ` ` + ` ` num_cols ` ` ` ` matrix_approximation_rank ` ` noqa B uncompressed_size = num_rows num_cols compressed_size = num_rows + num_cols matrix_approximation_rank compressed_size min_compression_rate uncompressed_size uncompressed_size compressed_size _report_compression_stats bucket state Report compression stats frequency ` ` compression_stats_logging_frequency ` ` specified PowerSGD state bucket is_last state iter = state next_stats_report stats = state compression_stats logger info Compression stats iter s total before compression s total after compression s rate s state iter stats stats stats state next_stats_report = state iter + state compression_stats_logging_frequency PowerSGDState r Store both algorithm s hyperparameters internal state all gradients during training Particularly ` ` matrix_approximation_rank ` ` ` ` start_powerSGD_iter ` ` main hyperparameters should tuned user For performance we suggest keep binary hyperparameters ` ` use_error_feedback ` ` ` ` warm_start ` ` ` ` matrix_approximation_rank ` ` controls size compressed low-rank tensors which determines compression rate The lower rank stronger compression If ` ` matrix_approximation_rank ` ` too low full model quality will need more training steps reach will never reach yield loss accuracy The increase ` ` matrix_approximation_rank ` ` can substantially increase computation costs compression accuracy may further improved beyond certain ` ` matrix_approximation_rank ` ` threshold To tune ` ` matrix_approximation_rank ` ` we suggest start increase factors like exponential grid search until satisfactory accuracy reached Typically only small value - used For some NLP tasks shown Appendix D original paper value has been increased ` ` start_powerSGD_iter ` ` defers PowerSGD compression until step ` ` start_powerSGD_iter ` ` vanilla allreduce runs prior step ` ` start_powerSGD_iter ` ` This hybrid scheme vanilla allreduce + PowerSGD can effectively improve accuracy even relatively small ` ` matrix_approximation_rank ` ` used This because beginning training phase usually very sensitive inaccurate gradients compressing gradients too early may make training quickly take suboptimal trajectory which can result irrecoverable impact accuracy To tune ` ` start_powerSGD_iter ` ` we suggest start total training steps increase until satisfactory accuracy reached If there warm-up stage training ` ` start_powerSGD_iter ` ` typically should no less than number warm-up steps ` ` min_compression_rate ` ` minimum compression rate required when layer compressed Due computation overheads incurred compression tensor worth compressing only there can sufficient saving bandwidth where ` ` num_rows + num_cols matrix_approximation_rank min_compression_rate num_rows num_cols ` ` If specified compression rate threshold cannot satisfied tensor will directly allreduced without compression Compression statistics logged every ` ` compression_stats_logging_frequency ` ` iterations once PowerSGD compression starts ` ` orthogonalization_epsilon ` ` can very small value e g e- added every normalized matrix column orthogonalization step prevent div-by-zero error any column has all s If can already prevented e g batch normalization epsilon recommended accuracy ` ` batch_tensors_with_same_shape ` ` controls whether compress decompress tensors same shape batched operation achieve higher parallelism Note you should also increase bucket size i e ` ` bucket_cap_mb ` ` arg DDP constructor make more same-shaped tensors appear same bucket however may reduce overlap between computation communication increase memory footprint due stacking tensors same shape Set ` ` True ` ` compression decompression computation bottleneck warning If error feedback warm-up enabled minimum value ` ` start_powerSGD_iter ` ` allowed DDP This because there another internal optimization rebuilds buckets iteration DDP can conflict any tensor memorized before rebuild process noqa B __slots__ = process_group The fields below hyperparameters often need tuned user matrix_approximation_rank start_powerSGD_iter The fields below hyperparameters seldom need tuned user min_compression_rate orthogonalization_epsilon The fields below binary hyperparameters recommended turned performance accuracy use_error_feedback warm_start batch_tensors_with_same_shape The fields below internal state rng error_dict p_memory_dict q_memory_dict iter The fields below recording compression stats total_numel_before_compression total_numel_after_compression compression_stats_logging_frequency next_stats_report __init__ process_group matrix_approximation_rank= start_powerSGD_iter= _ min_compression_rate= use_error_feedback=True warm_start=True orthogonalization_epsilon= random_seed= compression_stats_logging_frequency= _ batch_tensors_with_same_shape bool = False logger info PowerSGD config matrix_approximation_rank = s start_powerSGD_iter = s min_compression_rate = s orthogonalization_epsilon = s use_error_feedback = s warm_start = s random_seed = s compression_stats_logging_frequency = s batch_tensors_with_same_shape = s matrix_approximation_rank start_powerSGD_iter min_compression_rate orthogonalization_epsilon use_error_feedback warm_start random_seed compression_stats_logging_frequency batch_tensors_with_same_shape process_group = process_group matrix_approximation_rank = matrix_approximation_rank Deferring PowerSGD compression util step start_powerSGD_iter can have two advantages It turns out PowerSGD may lead non-trivial accuracy loss even matrix approximation rank increased large value To mitigate accuracy loss simple yet effective way mixing vanilla allreduce more conservative compression such FP compression PowerSGD There internal optimization rebuilding buckets process DDP order save memory space This step takes place after first iteration However means shape input bucketized tensors subject change which will complicate implementations error feedback warm-up Running vanilla allreduce first few iterations can avoid complexity use_error_feedback warm_start start_powerSGD_iter = raise ValueError Expect ` start_powerSGD_iter ` ` use_error_feedback ` ` warm_start ` enabled because PowerSGD can only applied after first two iterations DDP start_powerSGD_iter = start_powerSGD_iter min_compression_rate = min_compression_rate Error feedback usually crucial both convergence generalization because PowerSGD biased compressor i e compressing decompressing random gradient does yield original expectation This mechanism requires temporary copy input gradients so increases peak memory consumption size gradient tensor However target matrices known exactly low-ranked instead just low stable rank sometimes possible converge optima without error feedback See http proceedings mlr press v yurtsever yurtsever pdf use_error_feedback = use_error_feedback Warm-start reuses P s Q s previous iteration This can improve approximation quality hence improve accuracy Additionally avoiding initialization these low-rank tensors every step can also accelerate training However cost extra memory warm_start = warm_start Can use very small value prevent div-by-zero error caused orthogonalization vanishing gradients orthogonalization_epsilon = orthogonalization_epsilon The purpose RNG generate different random seeds initializing Q across iterations same order all DDP replicas Different random seeds across iterations indicate different projections gradients different SGD steps If same random projection used there will differences between gradients never synchronized numpy np rng = np random RandomState random_seed Since there only single state instance all input buckets need maintain dictionary maps each bucket index local error error_dict dict int torch Tensor = p_memory_dict dict int torch Tensor = q_memory_dict dict int torch Tensor = Iteration step training loop iter = Compression stats accumulators total_numel_before_compression = total_numel_after_compression = We ll report compression stats every compression_stats_logging_frequency iterations Note we always report compression stats least once compression_stats_logging_frequency = max compression_stats_logging_frequency next_stats_report = Batching tensors same shape can increase parallelism compression decompression computation This requires larger bucket size make more same-shaped tensor appear one bucket however may reduce overlap between computation communication increase memory footprint due stacking tensors Turn compression decompression computation bottleneck batch_tensors_with_same_shape = batch_tensors_with_same_shape __getstate__ r Return ` ` Dict str Any ` ` which will pickled saved ` ` process_group ` ` serializable excluded returned state logger warning NOTE Process group serializable excluded saved state slot getattr slot slot __slots__ slot = process_group __setstate__ state r Take provided ` ` state ` ` set ` ` PowerSGDState ` ` instance ` ` process_group ` ` set default process_group = distributed_c d _get_default_group logger warning NOTE Process group will set default group i e world size \ If different group desired please set ` process_group ` after PowerSGD state loaded slot value state items setattr slot value maybe_increase_iter bucket Track iterations trigger log message start local SGD Since bucket last bucket allreduce iteration Only increase ` iter ` when bucket processed bucket is_last iter += iter == start_powerSGD_iter logger info Start apply PowerSGD after s iterations iter compression_stats r Return latest compression statistics tuple Returns tuple form compress_rate numel_before_compression numel_after_compression where compress_rate effective compression rate i e number elements before compression number elements after compression numel_before_compression total number elements before compression applied numel_after_compression total number elements after compression applied noqa B compress_rate = total_numel_before_compression total_numel_after_compression total_numel_after_compression compress_rate total_numel_before_compression total_numel_after_compression powerSGD_hook state PowerSGDState bucket dist GradBucket - torch futures Future torch Tensor r Implement PowerSGD algorithm This DDP communication hook implements PowerSGD gradient compression algorithm described ` paper https arxiv org abs ` _ Once gradient tensors aggregated across all workers hook applies compression follows Views input flattened D gradient tensor list per-parameter tensors divides all tensors into two groups The tensors should compressed before allreduce because compression can give enough saving bandwidth Rest tensors will directly allreduced without compression including all vector tensors biases Handles uncompressed tensors Allocate contiguous memory those uncompressed tensors allreduces all uncompressed tensors batch without compression Copies individual uncompressed tensors contiguous memory back input tensor Handles tensors should compressed PowerSGD compression For each tensor M creates two low-rank tensors P Q decomposing M such M = PQ^T where Q initialized standard normal distribution orthogonalized Computes each P Ps which equal MQ Allreduces Ps batch Orthogonalizes each P Ps Computes each Q Qs which approximately equal M^TP Allreduces Qs batch Computes each M among all compressed tensors which approximately equal PQ^T Note communication hook enforces vanilla allreduce first ` ` state start_powerSGD_iter ` ` iterations This only gives user more control over tradeoff between speedup accuracy also helps abstract away some complexity internal optimization DDP future communication hook developers Args state PowerSGDState State information configure compression rate support error feedback warm start etc To tune compression configs mainly need tune ` ` matrix_approximation_rank ` ` ` ` start_powerSGD_iter ` ` ` ` min_compression_rate ` ` bucket dist GradBucket Bucket stores D flattened gradient tensor batches multiple per-variable tensors Note since DDP comm hook only supports single process single device mode only exactly one tensor stored bucket Returns Future handler communication which updates gradients place Example xdoctest +SKIP state = PowerSGDState process_group=process_group matrix_approximation_rank= start_powerSGD_iter= min_compression_rate= ddp_model register_comm_hook state powerSGD_hook noqa B process_group = state process_group group_to_use = process_group process_group None not_none dist group WORLD world_size = group_to_use size The input tensor flattened D tensor input_tensor = bucket buffer Run vanilla allreduce first ` start_powerSGD_iter ` iterations state iter state start_powerSGD_iter state maybe_increase_iter bucket default _allreduce_fut group_to_use input_tensor Apply PowerSGD after ` start_powerSGD_iter ` iterations device = input_tensor device dtype = input_tensor dtype Incorporate error previous state into gradients bucket_index = bucket index input_tensor_cp = None total_length = input_tensor shape state use_error_feedback bucket_index state error_dict input_tensor add_ state error_dict bucket_index logger info A zero tensor length s represents local error created total_length state error_dict bucket_index = torch zeros total_length device=device dtype=dtype Keep copy input tensor so we can compute local error caused compression later comparing copy input tensor updated after decompression input_tensor_cp = input_tensor detach clone Unflatten input tensor into per-parameter tensors layer-wise compression tensors = bucket gradients Step I Divide all tensors into two groups one will compressed before allreduce other will directly allreduced without compression tensors_to_compress uncompressed_tensors = total_Ps_size = total_Qs_size = tensor tensors matrix = tensor view tensor shape - n m = matrix shape matrix_approximation_rank = min n m state matrix_approximation_rank compress_test = _should_compress n m matrix_approximation_rank state min_compression_rate state total_numel_before_compression += compress_test compress_test tensors_to_compress append matrix total_Ps_size += n matrix_approximation_rank total_Qs_size += m matrix_approximation_rank state total_numel_after_compression += compress_test uncompressed_tensors append tensor state total_numel_after_compression += compress_test _report_compression_stats bucket state Step II Handle uncompressed tensors Allocate contiguous memory these tensors allreduce efficiently uncompressed_tensors_memory = torch cat tensor view - tensor uncompressed_tensors uncompressed_tensors torch tensor device=device dtype=dtype Step III Handle tensors should compressed Allocate contiguous memory Ps Qs allreduce efficiently If warm-start enabled reuse Ps Qs previous iteration possible The memory spaces Ps Qs need allocated first iteration when PowerSGD applied need_randomize_qs = False state warm_start bucket_index state p_memory_dict need_randomize_qs = True If warm-start disabled low-rank tensors will initialized every step Only log warm-start avoid spamming state warm_start logger info Allocating contiguous memory length s Ps length s Qs respectively total_Ps_size total_Qs_size state p_memory_dict bucket_index = torch empty total_Ps_size device=device dtype=dtype state q_memory_dict bucket_index = torch empty total_Qs_size device=device dtype=dtype Batch tensors compress shape shape_to_tensors = defaultdict list tensor tensors_to_compress shape_to_tensors tensor shape append tensor This function decides whether batch tensors same shape according argument so following process could share same code maybe_batched_tensors_to_compress tensors shape_to_tensors values state batch_tensors_with_same_shape batch_size = len tensors batch_size == Use original tensor avoid copy yield tensors unsqueeze yield torch stack tensors tensor tensors yield tensor unsqueeze Create Ps Qs point allocated memory tensors_to_compress = ps = qs = p_idx = q_idx = tensor maybe_batched_tensors_to_compress batch_size n m = tensor shape matrix_approximation_rank = min n m state matrix_approximation_rank tensors_to_compress append tensor ps append state p_memory_dict bucket_index p_idx p_idx + batch_size n matrix_approximation_rank view batch_size n matrix_approximation_rank qs append state q_memory_dict bucket_index q_idx q_idx + batch_size m matrix_approximation_rank view batch_size m matrix_approximation_rank p_idx += batch_size n matrix_approximation_rank q_idx += batch_size m matrix_approximation_rank If warm-start enabled reuse Qs previous iteration possible skip filling random values The exception first iteration when PowerSGD applied need_randomize_qs q qs _orthogonalize q state orthogonalization_epsilon torch random fork_rng devices= Fork RNG avoid changing seed globally affecting random sampling anywhere training The seed makes sure initial random values same across all DDP replicas This seed should differ every step Since very slow fork RNG state across all CUDA devices only fork CPU then move generated tensor CUDA device overwriting q torch manual_seed state rng randint _ _ _ q qs q copy_ torch randn q shape device= cpu dtype=dtype _orthogonalize q state orthogonalization_epsilon Compute Ps tensor q p zip tensors_to_compress qs ps torch bmm tensor q out=p This allreduce only applied uncompressed tensors so should have been kicked off before above computation compressed tensors hide more communication costs However somehow requires separate future chain time allreduce_contiguous_uncompressed_tensors_fut = dist all_reduce uncompressed_tensors_memory group=group_to_use async_op=True get_future unpack_uncompressed_tensors_and_allreduce_ps fut uncompressed_tensors_memory = fut value div_ world_size idx = tensor uncompressed_tensors tensor copy_ uncompressed_tensors_memory idx idx + tensor numel view_as tensor idx += tensor numel Since these Ps will orthogonalized later no need divide them world size dist all_reduce state p_memory_dict bucket_index group=group_to_use async_op=True get_future wait compute_qs fut state p_memory_dict bucket_index = fut value p ps _orthogonalize p state orthogonalization_epsilon Compute Qs tensor p q zip tensors_to_compress ps qs torch bmm tensor transpose p out=q TODO The above procedure does two matmul+allreduce steps per iteration -- one left multiplication one right multiplication For warm-start can take one such step time alternate between them Allreduce Qs dist all_reduce state q_memory_dict bucket_index group=group_to_use async_op=True get_future wait decompress fut state q_memory_dict bucket_index = fut value div_ world_size p q tensor zip ps qs tensors_to_compress torch bmm p q transpose out=tensor Copy batched tensors back original buffer state batch_tensors_with_same_shape tensor tensors_to_compress tensor shape == Skip tensor batch_size == since itself original tensor continue original_tensors = shape_to_tensors tensor shape i original_tensor enumerate original_tensors original_tensor copy_ tensor i torch cuda is_available torch cuda synchronize device state use_error_feedback Memorize local errors assert input_tensor_cp None state error_dict bucket_index = input_tensor_cp - input_tensor state warm_start state p_memory_dict clear state q_memory_dict clear state maybe_increase_iter bucket input_tensor allreduce_contiguous_uncompressed_tensors_fut then unpack_uncompressed_tensors_and_allreduce_ps then compute_qs then decompress batched_powerSGD_hook state PowerSGDState bucket dist GradBucket - torch futures Future torch Tensor r Implement simplified PowerSGD algorithm This DDP communication hook implements simplified PowerSGD gradient compression algorithm described ` paper https arxiv org abs ` _ This variant does compress gradients layer layer instead compresses flattened input tensor batches all gradients Therefore faster than meth ` powerSGD_hook ` usually results much lower accuracy unless ` ` matrix_approximation_rank ` ` warning Increasing ` ` matrix_approximation_rank ` ` here may necessarily increase accuracy because batching per-parameter tensors without column row alignment can destroy low-rank structure Therefore user should always consider meth ` powerSGD_hook ` first only consider variant when satisfactory accuracy can achieved when ` ` matrix_approximation_rank ` ` Once gradient tensors aggregated across all workers hook applies compression follows Views input flattened D gradient tensor square-shaped tensor M paddings Creates two low-rank tensors P Q decomposing M such M = PQ^T where Q initialized standard normal distribution orthogonalized Computes P which equal MQ Allreduces P Orthogonalizes P Computes Q which approximately equal M^TP Allreduces Q Computes M which approximately equal PQ^T Truncates input tensor original length Note communication hook enforces vanilla allreduce first ` ` state start_powerSGD_iter ` ` iterations This only gives user more control over tradeoff between speedup accuracy also helps abstract away some complexity internal optimization DDP future communication hook developers Args state PowerSGDState State information configure compression rate support error feedback warm start etc To tune compression configs mainly need tune ` ` matrix_approximation_rank ` ` ` ` start_powerSGD_iter ` ` bucket dist GradBucket Bucket stores D flattened gradient tensor batches multiple per-variable tensors Note since DDP comm hook only supports single process single device mode only exactly one tensor stored bucket Returns Future handler communication which updates gradients place Example xdoctest +SKIP state = PowerSGDState process_group=process_group matrix_approximation_rank= ddp_model register_comm_hook state batched_powerSGD_hook noqa B process_group = state process_group group_to_use = process_group process_group None not_none dist group WORLD world_size = group_to_use size The input tensor flattened D tensor input_tensor = bucket buffer Run vanilla allreduce first ` start_powerSGD_iter ` iterations state iter state start_powerSGD_iter state maybe_increase_iter bucket default _allreduce_fut group_to_use input_tensor Apply PowerSGD after ` start_powerSGD_iter ` iterations device = input_tensor device total_length = input_tensor shape state total_numel_before_compression += total_length View input tensor D square-shape tensor pad s necessary square_side_length = math ceil math sqrt total_length state total_numel_after_compression += square_side_length state matrix_approximation_rank padded_total_length = square_side_length input_tensor resize_ padded_total_length input_tensor total_length padded_total_length fill_ _report_compression_stats bucket state Incorporate error previous state into gradients bucket_index = bucket index input_tensor_cp = None state use_error_feedback bucket_index state error_dict input_tensor add_ state error_dict bucket_index logger info A zero tensor length s represents local error created padded_total_length state error_dict bucket_index = torch zeros padded_total_length device=device dtype=input_tensor dtype Keep copy input tensor so we can compute local error caused compression later comparing copy input tensor updated after decompression input_tensor_cp = input_tensor detach clone matrix = input_tensor view square_side_length square_side_length Reuse P Q previous iteration possible The memory spaces P Q need allocated first iteration when PowerSGD applied state warm_start bucket_index state p_memory_dict If warm-start disabled low-rank tensors will initialized every step Only log warm-start avoid spamming state warm_start logger info Initializing low-rank tensors P Q each which has shape s x s square_side_length state matrix_approximation_rank create_low_rank_tensor fill_random_values rng Return low-rank D tensor square_side_length matrix_approximation_rank fill_random_values torch random fork_rng devices= Fork RNG avoid changing seed globally affecting random sampling anywhere training The seed makes sure initial random values same across all DDP replicas This seed should differ every step Since very slow fork RNG state across all CUDA devices only fork CPU then move generated tensor CUDA device torch manual_seed rng randint _ _ _ torch randn square_side_length state matrix_approximation_rank device= cpu dtype=input_tensor dtype device torch empty square_side_length state matrix_approximation_rank device=device dtype=input_tensor dtype state p_memory_dict bucket_index = create_low_rank_tensor fill_random_values=False rng=state rng state q_memory_dict bucket_index = create_low_rank_tensor fill_random_values=True rng=state rng _orthogonalize state q_memory_dict bucket_index torch matmul matrix state q_memory_dict bucket_index out=state p_memory_dict bucket_index allreduce_p_fut = dist all_reduce state p_memory_dict bucket_index group=group_to_use async_op=True get_future compute_q fut state p_memory_dict bucket_index = fut value _orthogonalize state p_memory_dict bucket_index torch matmul matrix t state p_memory_dict bucket_index out=state q_memory_dict bucket_index TODO The above procedure does two matmul+allreduce steps per iteration -- one left multiplication one right multiplication For warm-start can take one such step time alternate between them dist all_reduce state q_memory_dict bucket_index group=group_to_use async_op=True get_future wait decompress fut state q_memory_dict bucket_index = fut value div_ world_size torch matmul state p_memory_dict bucket_index state q_memory_dict bucket_index t out=matrix state use_error_feedback Memorize local errors assert input_tensor_cp None state error_dict bucket_index = input_tensor_cp - input_tensor Removing seemingly unnecessary sync somehow may cause failures See https github com pytorch pytorch pull torch cuda is_available torch cuda synchronize device state warm_start state p_memory_dict clear state q_memory_dict clear ret = input_tensor resize_ total_length state maybe_increase_iter bucket ret allreduce_p_fut then compute_q then decompress