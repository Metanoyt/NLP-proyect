Copyright c Meta Platforms Inc affiliates Owner s oncall distributed copy logging dataclasses dataclass model_registry ModelWithKwargs MultiMLP MultiMLPKwargs MultiMLPWithDw schedule_registry ScheduleUnbalanced ScheduleVShaped ScheduleWithReorderedB ScheduleWithW torch torch distributed dist torch distributed pipelining _ScheduleForwardOnly pipeline PipelineStage Schedule F B ScheduleDualPipeV ScheduleGPipe ScheduleInterleaved F B ScheduleInterleavedZeroBubble ScheduleLoopedBFS ScheduleZBVZeroBubble torch distributed pipelining schedules _Action _PipelineContext _PipelineScheduleRuntime _wait_batch_p p FORWARD OVERLAP_F_B torch distributed pipelining stage _PipelineStageBase noqa TC torch nn modules loss MSELoss torch testing _internal common_distributed MultiProcContinuousTest requires_accelerator_dist_backend torch testing _internal common_utils check_leaked_tensors instantiate_parametrized_tests parametrize run_tests skip_but_pass_in_sandcastle_if logger = logging getLogger __name__ d_hid = batch_size = torch manual_seed device_type = acc type acc = torch accelerator current_accelerator cpu backend = dist get_default_backend_for_device device_type TEST_MULTIACCELERATOR = torch accelerator device_count = dataclass PipelineTestConfig world_size int device torch device rank int setup_models_and_data config PipelineTestConfig n_layers=None model_class=MultiMLP Setup models input data target data loss function n_layers None n_layers = config world_size full_mod = model_class d_hid n_layers=n_layers full_mod config device ref_mod = copy deepcopy full_mod x = torch randn batch_size d_hid device=config device torch no_grad y = ref_mod x target = y + torch randn batch_size d_hid device=config device loss_fn = torch nn MSELoss reduction= sum full_mod ref_mod x target loss_fn create_single_stage_pipeline config PipelineTestConfig mod x chunks use_tracer=True Create single-stage pipeline using either tracer manual stage creation use_tracer x_mb = x chunk chunks split_spec = mod split_spec hasattr mod split_spec None pipe = pipeline mod mb_args= x_mb split_spec=split_spec stage = pipe build_stage config rank config device stage_module = pipe get_stage_module config rank stage stage_module stage_module Manual stage creation submod_name = f layers config rank stage_module = mod get_submodule submod_name stage = PipelineStage stage_module config rank config world_size config device stage stage_module stage_module create_multi_stage_pipeline config PipelineTestConfig mod stages_per_rank n_stages stage_indices=None Create multiple pipeline stages interleaved schedules stage_indices None stage_indices = config rank + i config world_size i range stages_per_rank submod_names = f layers i i stage_indices stage_modules = mod get_submodule submod_name submod_name submod_names stages = PipelineStage stage_module stage_idx n_stages config device stage_module stage_idx zip stage_modules stage_indices strict=True stages stage_modules submod_names run_reference_model ref_mod x target loss_fn num_iterations= kwargs Run reference model specified iterations final output loss ref_out = None ref_loss = None _ range num_iterations ref_mod zero_grad ref_out = ref_mod x kwargs ref_loss = loss_fn ref_out target ref_loss backward ref_out ref_loss check_gradients config PipelineTestConfig stage_modules ref_mod submod_names=None rtol= e- atol= e- Check gradients match between pipeline stages reference model using flexible comparison grad_check grad grad param_name rtol atol tolerance= grad None grad None grad None grad None raise AssertionError f One gradient None param_name grad vs grad try torch testing assert_close grad grad rtol=rtol atol=atol except AssertionError print f Numerical issues detected param_name param grad grad vs ref grad grad raise submod_names None Single stage case - need detect tracer vs manual pipeline stage_modules = stage_modules Try detect tracer-based pipeline checking parameter exists ref_mod sample_param_name = next iter stage_modules named_parameters try Try get parameter directly reference model tracer-based ref_mod get_parameter sample_param_name is_tracer_based = True except AttributeError Parameter doesn t exist root level must manual pipeline is_tracer_based = False is_tracer_based Tracer-based pipeline parameter names full paths root model name p stage_modules named_parameters ref_p = ref_mod get_parameter name grad_check p grad ref_p grad name rtol atol Manual pipeline parameter names local submodule submod_name = f layers config rank ref_submod = ref_mod get_submodule submod_name name p stage_modules named_parameters ref_p = ref_submod get_parameter name grad_check p grad ref_p grad f submod_name name rtol atol Multi-stage case - always use submodule approach stage_module submod_name zip stage_modules submod_names ref_submod = ref_mod get_submodule submod_name name p stage_module named_parameters ref_p = ref_submod get_parameter name grad_check p grad ref_p grad f submod_name name rtol atol zero_gradients stage_modules Zero gradients all stage modules isinstance stage_modules list stage_modules = stage_modules stage_module stage_modules stage_module zero_grad ScheduleTest MultiProcContinuousTest world_size = classmethod backend_str cls - str Testing NCCL backend backend property device - torch device torch device device_type rank property config - PipelineTestConfig Lazily create pipeline test configuration PipelineTestConfig world_size=self world_size device=self device rank=self rank requires_accelerator_dist_backend nccl xccl skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR f backend test requires + GPUs parametrize ScheduleClass _ScheduleForwardOnly test_forward_only ScheduleClass mod mod_ref x _ _ = setup_models_and_data config x_clone = x clone num_microbatches = world_size stage _ _ = create_single_stage_pipeline config mod x num_microbatches schedule = ScheduleClass stage num_microbatches scale_grads=False Run forward-only schedule out = None num_iters = _ range num_iters rank == schedule step x dist recv x src=self world_size - rank == world_size - out = schedule step dist send out dst= schedule step Validate pipelined output matches reference model rank == world_size - _ range num_iters x_clone = mod_ref x_clone torch testing assert_close x_clone out requires_accelerator_dist_backend nccl xccl skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR f backend test requires + GPUs parametrize ScheduleClass ScheduleGPipe Schedule F B ScheduleInterleaved F B ScheduleLoopedBFS ScheduleInterleavedZeroBubble test_eval_inference_mode ScheduleClass num_microbatches = ScheduleClass ScheduleInterleaved F B ScheduleLoopedBFS ScheduleInterleavedZeroBubble Multi-stage schedules stages_per_rank = n_stages = stages_per_rank world_size mod _ x target loss_fn = setup_models_and_data config n_layers=n_stages Create multi-stage pipeline stages stage_modules _ = create_multi_stage_pipeline config mod stages_per_rank n_stages schedule = ScheduleClass stages num_microbatches loss_fn=loss_fn scale_grads=False Single-stage schedules mod _ x target loss_fn = setup_models_and_data config Create single-stage pipeline stage stage_module _ = create_single_stage_pipeline config mod x num_microbatches stage_modules = stage_module schedule = ScheduleClass stage num_microbatches loss_fn=loss_fn scale_grads=False Clear gradients run eval zero_gradients stage_modules losses = rank == Support without no_grad torch no_grad schedule eval x rank == world_size - schedule eval target=target losses=losses schedule eval Check gradients NOT computed during eval grad_computed_eval = any param grad None stage_module stage_modules param stage_module parameters Verify gradients computed during eval assertFalse grad_computed_eval Gradients should computed during eval Verify losses still computed during eval rank == world_size - assertTrue len losses Losses should computed during eval requires_accelerator_dist_backend nccl xccl skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR f backend test requires + GPUs parametrize ScheduleClass ScheduleGPipe Schedule F B ScheduleInterleaved F B ScheduleLoopedBFS ScheduleInterleavedZeroBubble test_return_output ScheduleClass num_microbatches = ScheduleClass ScheduleInterleaved F B ScheduleLoopedBFS ScheduleInterleavedZeroBubble Multi-stage schedules stages_per_rank = n_stages = stages_per_rank world_size mod _ x target loss_fn = setup_models_and_data config n_layers=n_stages Create multi-stage pipeline stages stage_modules _ = create_multi_stage_pipeline config mod stages_per_rank n_stages schedule = ScheduleClass stages num_microbatches loss_fn=loss_fn scale_grads=False Single-stage schedules mod _ x target loss_fn = setup_models_and_data config Create single-stage pipeline stage stage_module _ = create_single_stage_pipeline config mod x num_microbatches schedule = ScheduleClass stage num_microbatches loss_fn=loss_fn scale_grads=False losses = rank == world_size - output = schedule step target=target losses=losses return_outputs=False schedule step x Verify output None rank == world_size - assertTrue output None Output should None requires_accelerator_dist_backend nccl xccl skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR f backend test requires + GPUs parametrize ScheduleClass ScheduleGPipe Schedule F B test_multi_iter ScheduleClass mod _ x target loss_fn = setup_models_and_data config chunks = stage _ _ = create_single_stage_pipeline config mod x chunks schedule = ScheduleClass stage chunks loss_fn=loss_fn scale_grads=False Run _ range rank == schedule step x rank == world_size - losses = schedule step target=target losses=losses schedule step dist barrier device_ids= rank requires_accelerator_dist_backend nccl xccl skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR f backend test requires + GPUs parametrize ScheduleClass ScheduleGPipe Schedule F B test_kwargs_with_tracer ScheduleClass mod = ModelWithKwargs d_hid splits=self world_size mod device x = torch randn batch_size d_hid device=self device y = torch randn batch_size d_hid device=self device target = torch randn batch_size d_hid device=self device loss_fn = torch nn MSELoss reduction= sum chunks = x_mb = x chunk chunks y_mb = y chunk chunks pipe = pipeline mod mb_args= x_mb mb_kwargs= y y_mb stage = pipe build_stage rank device Attach schedule schedule = ScheduleClass stage chunks loss_fn=loss_fn scale_grads=False Run out = None losses = rank == schedule step x y=y rank == world_size - out = schedule step target=target losses=losses schedule step dist barrier device_ids= rank Last rank checks result rank == world_size - ref_out = mod x y=y ref_loss = loss_fn ref_out target pipe_loss = sum losses torch testing assert_close out ref_out rtol= e- atol= e- torch testing assert_close pipe_loss ref_loss requires_accelerator_dist_backend nccl xccl skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR f backend test requires + GPUs parametrize ScheduleClass ScheduleGPipe Schedule F B test_grad_with_tracer ScheduleClass mod ref_mod x target loss_fn = setup_models_and_data config Run reference ref_out ref_loss = run_reference_model ref_mod x target loss_fn Create pipeline schedule chunks = world_size stage stage_module stage_modules = create_single_stage_pipeline config mod x chunks schedule = ScheduleClass stage chunks loss_fn=loss_fn scale_grads=False Run pipeline out = None losses = _ range zero_gradients stage_module rank == schedule step x rank == world_size - out = schedule step target=target losses=losses schedule step dist barrier device_ids= rank Last rank checks result rank == world_size - torch testing assert_close out ref_out pipe_loss = sum losses torch testing assert_close pipe_loss ref_loss Check gradients using helper method check_gradients config stage_module ref_mod requires_accelerator_dist_backend nccl xccl skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR f backend test requires + GPUs parametrize ScheduleClass ScheduleGPipe Schedule F B parametrize shape_inference True False test_grad_with_manual ScheduleClass shape_inference mod ref_mod x target loss_fn = setup_models_and_data config Run reference ref_out ref_loss = run_reference_model ref_mod x target loss_fn Create manual pipeline stage chunks = world_size stage stage_module _ = create_single_stage_pipeline config mod x chunks use_tracer=False Handle shape inference shape_inference input_args = x chunk chunks torch no_grad output_args = stage_module input_args stage = PipelineStage stage_module rank world_size device input_args=input_args output_args=output_args schedule = ScheduleClass stage chunks loss_fn=loss_fn scale_grads=False Run pipeline out = None losses = _ range zero_gradients stage_module rank == schedule step x rank == world_size - out = schedule step target=target losses=losses schedule step dist barrier device_ids= rank Last rank checks result rank == world_size - torch testing assert_close out ref_out pipe_loss = sum losses torch testing assert_close pipe_loss ref_loss Check gradients using helper method check_gradients config stage_module ref_mod requires_accelerator_dist_backend nccl xccl skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR f backend test requires + GPUs parametrize ScheduleClass ScheduleInterleaved F B ScheduleLoopedBFS ScheduleInterleavedZeroBubble test_grad_with_manual_interleaved ScheduleClass stages_per_rank = n_stages = stages_per_rank world_size mod ref_mod x target loss_fn = setup_models_and_data config n_layers=n_stages Run reference ref_out ref_loss = run_reference_model ref_mod x target loss_fn Create multi-stage pipeline stages stage_modules submod_names = create_multi_stage_pipeline config mod stages_per_rank n_stages print f Rank rank stages stage stage_index stage stages num_microbatches = ScheduleClass num_microbatches hasattr ScheduleClass num_microbatches world_size Create schedule schedule = ScheduleClass stages num_microbatches loss_fn=loss_fn scale_grads=False Run pipeline tensor leak checking out = None losses = check_leaked_tensors garbage_tensors _ range zero_gradients stage_modules rank == schedule step x rank == world_size - out = schedule step target=target losses=losses schedule step assertEqual len garbage_tensors Found leaked tensors check logs above debug info dist barrier Verify results rank == world_size - torch testing assert_close out ref_out pipe_loss = sum losses torch testing assert_close pipe_loss ref_loss Check gradients - use relaxed tolerances interleaved schedules since gradients small check_gradients config stage_modules ref_mod submod_names rtol= e- atol= e- requires_accelerator_dist_backend nccl xccl skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR f backend test requires + GPUs parametrize ScheduleClass ScheduleInterleavedZeroBubble test_schedule_with_weight_update_mlp_e e ScheduleClass stages_per_rank = n_stages = stages_per_rank world_size full_mod ref_mod x target _ = setup_models_and_data config n_layers=n_stages model_class=MultiMLPWithDw full_mod toggle loss_fn = MSELoss Run reference ref_out ref_loss = run_reference_model ref_mod x target loss_fn Create multi-stage pipeline custom dw_builder stages stage_modules submod_names = create_multi_stage_pipeline config full_mod stages_per_rank n_stages CustomState __init__ stage_module stage_idx rank i = stage_module = stage_module stage_idx = stage_idx rank = rank dw_builder dw_runner i += print f Rank rank dw_count= i stage= stage_idx stage_module compute_dW dw_runner Create custom states rebuild stages dw_builder cs = stage_indices = rank + i world_size i range stages_per_rank stage_module stage_idx zip stage_modules stage_indices cs stage_idx = CustomState stage_module stage_idx rank stages = PipelineStage stage_module stage_idx n_stages device dw_builder=cs stage_idx dw_builder stage_module stage_idx zip stage_modules stage_indices schedule = ScheduleClass stages loss_fn=loss_fn Run pipeline out = None losses = _ range zero_gradients stage_modules rank == schedule step x rank == world_size - out = schedule step target=target losses=losses schedule step dist barrier device_ids= rank Verify results rank == world_size - torch testing assert_close out ref_out pipe_loss = sum losses len losses torch testing assert_close pipe_loss ref_loss Check gradients using helper method check_gradients config stage_modules ref_mod submod_names requires_accelerator_dist_backend nccl xccl skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR f backend test requires + GPUs parametrize schedule_class ScheduleZBVZeroBubble ScheduleDualPipeV test_v_shape_schedules schedule_class n_stages = rank_stages = mod ref_mod x target loss_fn = setup_models_and_data config n_layers=n_stages Run reference ref_out ref_loss = run_reference_model ref_mod x target loss_fn Create multi-stage pipeline custom stage indices num_microbatches = stage_indices = rank_stages rank stages stage_modules submod_names = create_multi_stage_pipeline config mod len stage_indices n_stages stage_indices schedule = schedule_class stages num_microbatches loss_fn=loss_fn scale_grads=False Run pipeline - special case where first last stage rank out = None losses = _ range zero_gradients stage_modules rank == out = schedule step x target=target losses=losses schedule step Verify results rank has both first last stages rank == torch testing assert_close out ref_out pipe_loss = sum losses torch testing assert_close pipe_loss ref_loss Check gradients using helper method check_gradients config stage_modules ref_mod submod_names requires_accelerator_dist_backend nccl xccl skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR f backend test requires + GPUs test_custom_function_callback Test custom function callback functionality _PipelineScheduleRuntime n_stages = rank_stages = mod ref_mod x target loss_fn = setup_models_and_data config n_layers=n_stages Run reference ref_out ref_loss = run_reference_model ref_mod x target loss_fn Create multi-stage pipeline custom stage indices num_microbatches = stage_indices = rank_stages rank stages stage_modules submod_names = create_multi_stage_pipeline config mod len stage_indices n_stages stage_indices Use DualPipeV schedule base schedule base_schedule = ScheduleDualPipeV stages num_microbatches loss_fn=loss_fn scale_grads=False base_schedule _prepare_schedule_with_comms base_schedule pipeline_order Track both types callbacks separately forward_calls = overlap_calls = forward_callback action _Action ctx _PipelineContext Custom callback FORWARD computation mimics original implementation schedule = ctx schedule_ref assert isinstance schedule _PipelineScheduleRuntime stage_index_to_stage dict int _PipelineStageBase = stage stage_index stage stage schedule _stages stage = stage_index_to_stage action stage_index stage_index = stage stage_index mb_index = action microbatch_index assert mb_index None fwd_recv_ops = schedule fwd_recv_ops arg_mbs = ctx arg_mbs kwarg_mbs = ctx kwarg_mbs is_next_stage_on_this_rank = stage_index + stage_index_to_stage is_prev_stage_on_this_rank = stage_index - stage_index_to_stage used verification end forward_calls append stage_index mb_index stage is_first no recv op expected V-schedule special case see Note V-schedule special case is_prev_stage_on_this_rank assert stage_index mb_index fwd_recv_ops f Computing action= before receiving input torch distributed pipelining schedules _wait_batch_p p _wait_batch_p p fwd_recv_ops pop stage_index mb_index output = stage forward_one_chunk mb_index arg_mbs mb_index type ignore index kwarg_mbs mb_index type ignore index schedule _maybe_compute_loss stage output ctx target_mbs mb_index SEND RECV op avoided special case adjacent stages same rank see Note V-schedule special case is_next_stage_on_this_rank stage_index_to_stage stage_index + set_local_fwd_input output mb_index overlap_callback action _Action ctx _PipelineContext Custom callback OVERLAP_F_B computation mimics original implementation schedule = ctx schedule_ref assert isinstance schedule _PipelineScheduleRuntime stage_index_to_stage dict int _PipelineStageBase = stage stage_index stage stage schedule _stages assert action sub_actions None fwd_action = action sub_actions bwd_action = action sub_actions Forward ======================================================== forward_callback fwd_action ctx overlap_calls append fwd_action stage_index fwd_action microbatch_index bwd_action stage_index bwd_action microbatch_index Backward ======================================================== backward_stage_index = bwd_action stage_index backward_stage = stage_index_to_stage backward_stage_index backward_mb_index = bwd_action microbatch_index assert backward_mb_index None bwd_recv_ops = schedule bwd_recv_ops is_next_stage_on_this_rank = backward_stage stage_index + stage_index_to_stage is_prev_stage_on_this_rank = backward_stage stage_index - stage_index_to_stage backward_stage is_last no recv op expected V-schedule special case see Note V-schedule special case is_next_stage_on_this_rank assert backward_stage_index backward_mb_index bwd_recv_ops f Attempted run compute action= before receiving input _wait_batch_p p bwd_recv_ops pop backward_stage_index backward_mb_index loss = schedule _maybe_get_loss backward_stage backward_mb_index schedule backward_counter backward_stage_index += last_backward = schedule backward_counter backward_stage_index == schedule _n_microbatches grad_scale_factor = schedule _n_microbatches schedule scale_grads backward_stage backward_one_chunk backward_mb_index loss=loss full_backward=True last_backward=last_backward last_backward backward_stage scale_grads grad_scale_factor SEND RECV op avoided special case adjacent stages same rank see Note V-schedule special case is_prev_stage_on_this_rank stage_index_to_stage backward_stage_index - set_local_bwd_input backward_stage get_local_bwd_output backward_mb_index backward_mb_index Add callback FORWARD computation type base_schedule register_custom_function FORWARD forward_callback base_schedule register_custom_function OVERLAP_F_B overlap_callback Run pipeline - special case where first last stage rank out = None losses = num_loops = _ range num_loops zero_gradients stage_modules rank == out = base_schedule step x target=target losses=losses base_schedule step dist barrier Verify results rank has both first last stages rank == torch testing assert_close out ref_out pipe_loss = sum losses torch testing assert_close pipe_loss ref_loss Verify overlap callbacks called assertGreater len overlap_calls OVERLAP_F_B callback should have been called In V-schedule microbatches stages per rank rank should have calls microbatches stages loops expected_count = num_microbatches num_loops assertEqual len forward_calls expected_count Verify all callback calls stages rank stage_idx _ forward_calls assertIn stage_idx stage_indices f Callback called stage stage_idx rank rank Check gradients using helper method check_gradients config stage_modules ref_mod submod_names skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR NCCL test requires + GPUs parametrize ScheduleClass ScheduleInterleavedZeroBubble ScheduleInterleaved F B test_zero_bubble_with_model_kwargs ScheduleClass stages_per_rank = n_stages = stages_per_rank world_size mod ref_mod x target loss_fn = setup_models_and_data config n_layers=n_stages model_class=MultiMLPKwargs unused_kwarg = torch tensor device=self device Run reference kwargs ref_out ref_loss = run_reference_model ref_mod x target loss_fn unused_kwarg=unused_kwarg Create multi-stage pipeline stages stage_modules submod_names = create_multi_stage_pipeline config mod stages_per_rank n_stages num_microbatches = ScheduleClass num_microbatches hasattr ScheduleClass num_microbatches world_size schedule = ScheduleClass stages num_microbatches loss_fn=loss_fn scale_grads=False Run pipeline kwargs out = None losses = _ range zero_gradients stage_modules rank == schedule step x unused_kwarg=unused_kwarg clone unsqueeze expand num_microbatches - rank == world_size - out = schedule step target=target losses=losses schedule step dist barrier Verify results rank == world_size - torch testing assert_close out ref_out pipe_loss = sum losses torch testing assert_close pipe_loss ref_loss Check gradients using helper method check_gradients config stage_modules ref_mod submod_names rtol= e- atol= e- instantiate_parametrized_tests ScheduleTest CustomSchedulesTest MultiProcContinuousTest These schedules ScheduleRegistry require world_size == The schedules test weird unconventional schedules edge cases world_size = classmethod backend_str cls - str Testing NCCL backend backend property device - torch device torch device device_type rank property config - PipelineTestConfig Lazily create pipeline test configuration PipelineTestConfig world_size=self world_size device=self device rank=self rank requires_accelerator_dist_backend nccl xccl skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR f backend test requires + GPUs parametrize schedule_class ScheduleVShaped ScheduleUnbalanced test_non_symmetric_stage_ids schedule_class n_stages = schedule_class n_stages rank_stages = schedule_class rank_stages mod ref_mod x target loss_fn = setup_models_and_data config n_layers=n_stages Run reference ref_out ref_loss = run_reference_model ref_mod x target loss_fn Create multi-stage pipeline custom stage indices num_microbatches = stage_indices = rank_stages rank print f Rank rank stages stage_indices stages stage_modules submod_names = create_multi_stage_pipeline config mod len stage_indices n_stages stage_indices schedule = schedule_class stages num_microbatches loss_fn=loss_fn scale_grads=False Run pipeline - special case where first last stage rank out = None losses = _ range zero_gradients stage_modules rank == out = schedule step x target=target losses=losses schedule step dist barrier Verify results rank has both first last stages rank == torch testing assert_close out ref_out pipe_loss = sum losses torch testing assert_close pipe_loss ref_loss Check gradients using helper method check_gradients config stage_modules ref_mod submod_names requires_accelerator_dist_backend nccl xccl skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR f backend test requires + GPUs parametrize ScheduleClass ScheduleWithReorderedB test_pipeline_schedule_runtime_custom_sched ScheduleClass n_stages = stages_per_rank = mod ref_mod x target loss_fn = setup_models_and_data config n_layers=n_stages Run reference ref_out ref_loss = run_reference_model ref_mod x target loss_fn Create pipeline stages stages stage_modules submod_names = create_multi_stage_pipeline config mod stages_per_rank n_stages print f Rank rank stages stage stage_index stage stages num_microbatches = ScheduleClass num_microbatches hasattr ScheduleClass num_microbatches schedule = ScheduleClass stages num_microbatches loss_fn=loss_fn scale_grads=False assert isinstance schedule _PipelineScheduleRuntime Run pipeline tensor leak checking check_leaked_tensors garbage_tensors _ range zero_gradients stage_modules rank == schedule step x rank == world_size - losses = out = schedule step target=target losses=losses schedule step assertEqual len garbage_tensors Found leaked tensors check logs above debug info dist barrier Verify results rank == world_size - torch testing assert_close out ref_out pipe_loss = sum losses torch testing assert_close pipe_loss ref_loss Check gradients using helper method check_gradients config stage_modules ref_mod submod_names requires_accelerator_dist_backend nccl xccl skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR f backend test requires + GPUs parametrize ScheduleClass ScheduleWithW test_schedule_with_native_zero_bubble ScheduleClass n_stages = ScheduleClass n_stages num_microbatches = ScheduleClass num_microbatches rank_stages = ScheduleClass rank_stages num_steps = mod ref_mod x target loss_fn = setup_models_and_data config n_layers=n_stages Create multi-stage pipeline custom stage indices stage_indices = rank_stages rank print f Rank rank stages stage_indices stages stage_modules submod_names = create_multi_stage_pipeline config mod len stage_indices n_stages stage_indices schedule = ScheduleClass stages num_microbatches loss_fn=loss_fn scale_grads=False Run reference model ref_x = x detach clone requires_grad_ x requires_grad torch testing assert_close x ref_x _ range num_steps ref_out = ref_mod ref_x ref_loss = loss_fn ref_out target ref_loss backward Run pipeline tensor leak checking losses = check_leaked_tensors garbage_tensors _ range num_steps rank == schedule step x rank == world_size - schedule step target=target losses=losses schedule step assertEqual len garbage_tensors Found leaked tensors check logs above debug info Check gradients using helper method check_gradients config stage_modules ref_mod submod_names instantiate_parametrized_tests CustomSchedulesTest __name__ == __main__ run_tests