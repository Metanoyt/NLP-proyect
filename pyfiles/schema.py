copy dataclasses dataclass typing Any Optional torch torch utils _pytree pytree torch fx node Target Below implementation generating FunctionSchema example values This helpful generating FunctionSchema HigherOrderOperator where we don t have function inspect each call higher order operator would have different schema dataclass frozen=True HopArgumentInfo Could give name operand default s empty string name str example_value Any Provide default_value default_value Any Whether argument gets mutated hop subgraph For output should always False is_mutated bool kw_only bool HopArgumentInfoGen staticmethod from_example example_value Any name str = default_value Optional Any = None is_mutated bool = False kw_only bool = False - HopArgumentInfo default_value None assert type example_value type default_value f example_value type type example_value doesn t match default_value type type default_value HopArgumentInfo name=name example_value=example_value default_value=default_value is_mutated=is_mutated kw_only=kw_only CTypeGen convert_to_base_ty = int torch _C IntType get float torch _C FloatType get str torch _C StringType get bool torch _C BoolType get should torch _C JitType annotation busted staticmethod from_example obj Any - Any torch isinstance obj torch fx GraphModule torch _C AnyType get isinstance obj torch SymInt torch _C SymIntType get isinstance obj torch SymBool torch _C SymBoolType get torch _C _jit_try_infer_type obj type CArgumentGen staticmethod from_hop_argument_info arg_idx int arg_info HopArgumentInfo is_output bool = False - Any typ = CTypeGen from_example arg_info example_value is_output torch _C Argument typ None None False None alias_set = set f alias arg_idx arg_info is_mutated set alias_info = torch _C _AliasInfo arg_info is_mutated alias_set alias_set type ignore attr-defined torch _C Argument arg_info name typ None arg_info default_value arg_info kw_only alias_info HopSchemaGenerator __init__ hop torch _ops HigherOrderOperator arg_infos list HopArgumentInfo = example_outputs list Any = schema_tree_spec Optional pytree TreeSpec = None hop = hop add_arg name str example_value Any default_value Optional Any = None is_mutated bool = False kw_only bool = False - None callable example_value assert isinstance example_value torch fx GraphModule torch _ops OperatorBase Expect callable GraphModule Please call materialize_as_graph first f turn callable arguments example_value into GraphModule _ flat_spec = pytree tree_flatten example_value flat_spec is_leaf raise RuntimeError f example_value example_value leaf node Please only add flattened inputs hop schema If you need some structure arguments please add_arg flattened args one one then call add_schema_tree_spec register original pytree spec args arg_info = HopArgumentInfoGen from_example example_value=example_value name=name default_value=default_value is_mutated=is_mutated kw_only=kw_only arg_infos append arg_info add_output output Any - None example_outputs append output add_schema_tree_spec args Any kwargs Any - None schema tree spec tree spec flattening all inputs hop pytree tree_flatten Since torch FunctionSchema only have proper mutation alias support flattened inputs we need store tree spec order reconstruct inputs hop schema_tree_spec = pytree tree_flatten args kwargs gen_schema - torch _C FunctionSchema i arg_info enumerate arg_infos arg_spec = pytree tree_flatten arg_info example_value arg_spec is_leaf schema_tree_spec None raise RuntimeError f example_value arg_infos i arg_info example_value which leaf node Please call add_schema_tree_spec add schema tree spec first Or consider changing hop s signature only take flattened arguments CFunctionSchemaGen from_hop_argument_info str hop arg_infos HopArgumentInfoGen from_example tuple example_outputs name= out schema_tree_spec CFunctionSchemaGen Note HigherOrderOperator schema generation Each invocation HigherOrderOperator will have different schema For example schema torch cond varies depending true_fn false_fn So we need way generate schema each invocation HOP We want enforce following invariants HOP s schema Flattened inputs There should no pytree structure Flattened outputs Note even hop returns single value should wrapped tuple No aliasing This includes inp-inp aliasing inp-out aliasing out-out aliasing By enforcing these invariants we could make HOP s schema meets requirement schema parser makes hop easier handle downstream For example suppose we have invoke_quant_test HOP GraphModule torch nn Module forward l_x_ l_y_ subgraph_ = subgraph_ invoke_quant_test = torch ops higher_order invoke_quant_test subgraph_ l_x_ l_y_ scheme = nf subgraph_ torch nn Module forward l_x_ l_y_ add_ = l_x_ add_ matmul = l_x_ l_y_ sin = matmul sin child = sin cos child_ = l_x_ + l_y_ child_ = l_x_ - l_y_ child_ = l_x_ l_y_ child child_ child_ child_ By encoding inputs hop into list HopArgumentInfo output single HopArgumentInfo we would get following schema invoke_quant_test Any arg Tensor arg Tensor arg str scheme= \\ nf \\ - Tensor Tensor Tensor Tensor staticmethod from_hop_argument_info op_name str inp_argument_info list HopArgumentInfo out_argument_info HopArgumentInfo schema_tree_spec Optional pytree TreeSpec - Any args = i arg_info enumerate inp_argument_info args append CArgumentGen from_hop_argument_info i arg_info NOTE we want output always single argument torch _C TupleType assert isinstance out_argument_info example_value tuple f expect out_argument_info s example_value tuple got out_argument_info example_value assert out_argument_info is_mutated out_argument_info is_mutated should always set False rets = None len out_argument_info example_value == rets = CArgumentGen from_hop_argument_info out_argument_info True rets = CArgumentGen from_hop_argument_info i HopArgumentInfoGen from_example name=f out i example_value=val default_value=None is_mutated=False is_output=True i val enumerate out_argument_info example_value HopSchema op_name args rets False False schema_tree_spec HopSchema torch _C FunctionSchema __init__ name str overload_name str arguments list torch _C Argument returns list torch _C Argument is_vararg bool is_varret bool schema_tree_spec Optional pytree TreeSpec tree_spec = schema_tree_spec is_vararg = is_vararg is_varret = is_varret super __init__ name overload_name arguments returns is_vararg is_varret __deepcopy__ memo Any - HopSchema Need additionally copy tree_spec since s member torch _C FunctionSchema HopSchema name overload_name arguments returns is_vararg is_varret copy deepcopy tree_spec find_hop_schema gm torch fx GraphModule target Target - list torch _C FunctionSchema schemas = node gm graph find_nodes op= call_function target=target _get_example_value node torch fx Node - Any node op == get_attr assert isinstance node target str getattr gm node target node meta example_value example_value node meta node meta val fake_args fake_kwargs = pytree tree_map_only torch fx Node _get_example_value node args node kwargs schema = node target gen_schema fake_args fake_kwargs schemas append schema schemas