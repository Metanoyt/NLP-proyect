mypy ignore-errors Function-related variable tracking classes Dynamo s symbolic execution This module contains classes track different types functions during graph compilation including - User-defined functions methods - Built-in functions methods - Wrapped functions e g decorators - Special function types e g functools partial - Triton kernels related function types These classes responsible - Tracking function calls their arguments - Managing function closures cell variables - Handling function attributes special methods - Maintaining guards function identity closure contents - Supporting function inlining specialization - Enabling proper symbolic execution different function types The variable trackers here work together rest Dynamo enable accurate graph capture while handling Python s various function-related behaviors builtins functools inspect itertools logging sys traceback types collections abc Callable Sequence types FunctionType typing Any Optional TYPE_CHECKING TypeVar typing_extensions Never weakref WeakKeyDictionary torch torch _dynamo exc get_stack_above_dynamo config graph_break_hints polyfills variables bytecode_transformation create_call_function create_rot_n is_generator exc get_dynamo_observed_exception handle_observed_exception InfiniteGeneratorError ObservedException ObservedGeneratorExit ObservedUserStopIteration raise_observed_exception SkipFrame StepUnsupported unimplemented_v Unsupported guards GuardBuilder install_guard source AttrSource ClosureSource ConstantSource DefaultsSource GetItemSource SkipGuardSource utils check_constant_args check_unspec_or_constant_args cmp_name_to_op_mapping identity is_function is_wrapper_or_member_descriptor istype make_cell base AsPythonConstantNotImplementedError AttributeMutationNew raise_type_error_exc ValueMutationNew VariableTracker constant ConstantVariable try torch distributed fsdp _fully_shard _fsdp_param_group except ModuleNotFoundError _fsdp_param_group = None TYPE_CHECKING torch _dynamo codegen PyCodegen torch _dynamo symbolic_convert InstructionTranslator torch _higher_order_ops triton_kernel_wrap TritonGridType TritonKernelType _F = TypeVar _F bound=Callable CO_VARARGS = x CO_VARKEYWORDS = x Module-level cache keyed function object _spec_cache = WeakKeyDictionary FunctionSpec __init__ func FunctionType code = func __code__ vn = code co_varnames posonly_count = code co_posonlyargcount arg_count = code co_argcount kwonly_count = code co_kwonlyargcount posonly_names = vn posonly_count pos_or_kw_names = vn posonly_count arg_count all_pos_names = posonly_names + pos_or_kw_names kwonly_names = vn arg_count arg_count + kwonly_count off = arg_count + kwonly_count varargs_name = vn off code co_flags CO_VARARGS None off += varargs_name varkw_name = vn off code co_flags CO_VARKEYWORDS None update_defaults func FunctionType Defaults can change function call function call So re-update them every call defaults = func __defaults__ kwdefaults = func __kwdefaults__ Map positional-default names â†’ their index defaults pos_default_map = dict zip all_pos_names -len defaults range len defaults _get_spec func FunctionType - FunctionSpec spec = _spec_cache get func spec None spec = FunctionSpec func _spec_cache func = spec spec bind_args_cached func tx fn_source args kwargs spec = _get_spec func spec update_defaults func ba = rem_kw = dict kwargs Bind all positional pos-only + pos-or-kw Apply pos-defaults first maybe overridden later name idx spec pos_default_map items default_source = None fn_source ConstantVariable is_literal spec defaults idx config skip_guards_on_constant_func_defaults default_source = DefaultsSource fn_source idx ba name = wrap_bound_arg tx spec defaults idx default_source Fill provided positional args i name enumerate spec all_pos_names i len args Maybe override pos-defaults applied above ba name = wrap_bound_arg tx args i name rem_kw ` kwargs ` can have same key pos-only arg ` name ` If case happens we should consume ` name ` here keep ` kwargs ` fn kwargs kwargs fn a= name spec posonly_names Maybe override pos-defaults applied above ba name = wrap_bound_arg tx rem_kw pop name name ba raise_observed_exception TypeError tx args= ConstantVariable create f Missing required positional argument name args extra = args len spec all_pos_names spec varargs_name ba spec varargs_name = wrap_bound_arg tx tuple extra extra raise_observed_exception TypeError tx args= ConstantVariable create f Too many positional arguments got len args expected len spec all_pos_names Keyword-only name spec kwonly_names name rem_kw ba name = wrap_bound_arg tx rem_kw pop name name spec kwdefaults kwdefault_source = None fn_source kwdefault_source = DefaultsSource fn_source name is_kw=True ba name = wrap_bound_arg tx spec kwdefaults name kwdefault_source raise_observed_exception TypeError tx args= ConstantVariable create f Missing required keyword-only argument name kwargs spec varkw_name ba spec varkw_name = wrap_bound_arg tx rem_kw rem_kw raise_observed_exception TypeError tx args= ConstantVariable create f Unexpected keyword arguments list rem_kw ba wrap_bound_arg tx InstructionTranslator val source=None Source propagation best effort since every object we encounter has source begin isinstance val VariableTracker val source VariableTracker build tx val Create lazy variable avoid guarding __defaults__ unless really needed variables LazyVariableTracker create val source wrap_args_kwargs tx InstructionTranslator result k v list result items isinstance v tuple dict args kwargs result k = wrap_bound_arg tx v init_cellvars parent result dict str VariableTracker code Update ` result ` add mapping local name new cells created directly ` code ` update SideEffects ` parent ` local cell already ` result ` cell argument side_effects = parent output side_effects name code co_cellvars new_cell = side_effects track_cell_new name result This handles when function argument cell e g captured nested func See ` MAKE_CELL ` bytecode more info side_effects store_cell new_cell result pop name result name = new_cell _create_nested_fn code f_globals name defaults closure kwdefaults annotations types FunctionType func = FunctionType code f_globals name defaults closure func __kwdefaults__ = kwdefaults isinstance annotations tuple itertools pairwise annotations = dict pairwise annotations TypeError __annotations__ must set dict object assert annotations None isinstance annotations dict func __annotations__ = annotations func fn_known_dunder_attrs = __annotations__ __defaults__ __kwdefaults__ __code__ __globals__ __closure__ __doc__ fn_var_getattr tx fn source name source = source AttrSource source name source name == __annotations__ We get large number silly guards annotations inspect module Changing annotations rare impacting extracted graph even rarer So skip guards source = SkipGuardSource source try subobj = inspect getattr_static fn name except AttributeError function does have __getattr__ __getattribute__ method so we can safely assume attribute absent raise_observed_exception AttributeError tx Special handling known dunder attributes name fn_known_dunder_attrs subobj = getattr fn name source variables LazyVariableTracker create subobj source VariableTracker build tx subobj BaseUserFunctionVariable VariableTracker get_filename get_code co_filename get_name get_code co_name call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker tx inline_user_function_return self_args args kwargs call_obj_hasattr tx InstructionTranslator name str - VariableTracker result = False try result = hasattr get_function name except NotImplementedError name == __name__ isinstance NestedUserFunctionVariable result = True variables ConstantVariable create result inspect_parameter_names list inspect signature get_function parameters closure_vars tx UserFunctionVariable BaseUserFunctionVariable Some unsupported user-defined global function _nonvar_fields = fn is_constant BaseUserFunctionVariable _nonvar_fields classmethod create_with_source cls value source install_guard source make_guard GuardBuilder CLOSURE_MATCH cls value source=source __init__ fn is_constant=False kwargs - None super __init__ kwargs getattr fn _dynamo_marked_constant False This method should treated constant purposes compilation is_constant = True is_constant = False TODO putting here avoid duplication because we could hit several paths e g SuperVariable ` var_getattr ` s isinstance fn types FunctionType torch jit ScriptFunction unimplemented_v gb_type= can t handle functions implemented python context=f fn explanation= Dynamo can only handle functions defined python hints= Move usage function out ` torch compile ` region graph_break_hints INFERENCE_MODE TODO anijain - Replace directly calling UserFunctionVariable VariableBuilder which handles wrapping _torchdynamo_inline unpack torch _dynamo optimize fn wrapped function fn = inspect getattr_static fn _torchdynamo_inline fn fn types FunctionType = fn as_python_constant istype UserFunctionVariable fn subclasses such methods usually aren t constant super as_python_constant self_args get_function fn get_code fn __code__ python_type types FunctionType has_self getattr fn __self__ None None get_globals fn __globals__ get_source source = source source isinstance variables UserMethodVariable source = source_fn source bind_args parent args kwargs - dict str VariableTracker Assume ` args ` ` kwargs ` VariableTracker arguments call function create new bindings initial locals assert is_constant fn types FunctionType = fn isinstance fn FunctionType raise TypeError Only supports regular Python functions root_tx = parent output root_tx source = get_source result = bind_args_cached fn root_tx source args kwargs init_cellvars parent result fn __code__ closure = fn __closure__ assert len closure == len fn __code__ co_freevars idx name cell zip itertools count fn __code__ co_freevars closure TODO refactor these branches side_effects = parent output side_effects cell side_effects cell_var = side_effects cell source closure_cell = GetItemSource ClosureSource source idx closure_cell_contents = AttrSource closure_cell cell_contents try contents_var = VariableTracker build parent cell cell_contents closure_cell_contents except ValueError Cell has yet been assigned contents_var = variables DeletedVariable cell_var = side_effects track_cell_existing closure_cell cell contents_var TODO figure out why source isn t available here whether we can fix remove branch try contents_var = VariableTracker build parent cell cell_contents except ValueError Cell has yet been assigned contents_var = variables DeletedVariable cell_var = side_effects track_cell_existing None cell contents_var result name = cell_var result var_getattr tx InstructionTranslator name str name cmp_name_to_op_mapping variables GetAttrVariable name source = get_source fn_var_getattr tx fn source name call_obj_hasattr tx InstructionTranslator name str - VariableTracker result = hasattr fn name variables ConstantVariable create result call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker Handle patch_dynamo_config call fn torch _dynamo patch_dynamo_config try args_const = arg as_python_constant arg args kwargs_const = key val as_python_constant key val kwargs items changes = torch _dynamo patch_dynamo_config args_const kwargs_const changes variables DynamoConfigPatchVariable changes except AsPythonConstantNotImplementedError e raise RuntimeError Cannot convert patch_dynamo_config args kwargs constants Please fix your call patch_dynamo_config using simpler inputs f args args kwargs kwargs e fn torch _dynamo error_on_graph_break try bound = inspect signature fn bind args kwargs error_on_graph_break = bound arguments error_on_graph_break as_python_constant assert isinstance error_on_graph_break bool variables ErrorOnGraphBreakVariable error_on_graph_break except Exception e raise RuntimeError Improper error_on_graph_break call Please fix your call error_on_graph_break f args args kwargs kwargs e Handle ` nonstrict_trace fn ` call fn torch _dynamo nonstrict_trace bound = inspect signature fn bind args kwargs fn_var = bound args isinstance fn_var BaseUserFunctionVariable typ = fn_var python_type msg = f ` nonstrict_trace ` expects callable got value type typ __name__ unimplemented_v gb_type= TypeError user code context=f call_function value args kwargs explanation=msg hints= graph_break_hints USER_ERROR isinstance fn_var UserFunctionVariable fn_name = fn_var get_name msg = f Applying ` nonstrict_trace ` function fn_name however ` nonstrict_trace ` currently requires function defined outside ` torch compile ` region noqa B unimplemented_v gb_type= Limitation ` nonstrict_trace context=f explanation=msg hints= f make sure definition fn_name outside ` torch compile ` region fn = fn_var fn variables TorchInGraphFunctionVariable fn nonstrict_traceable=True is_constant invoke_and_store_as_constant tx fn get_name args kwargs tx output current_tracer unsafe_allow_externally_visible_side_effects fn torch _dynamo utils _disable_side_effect_safety_checks_for_current_subtracer torch _dynamo side_effects allow_externally_visible_side_effects_in_subtracer tx super call_function tx args kwargs tx output current_tracer under_activation_checkpoint tx output current_tracer allow_side_effects_under_checkpoint try torch distributed fsdp _fully_shard _fsdp_state FSDPState except Exception FSDPState = None FSDPState None fn FSDPState _pre_forward FSDPState _post_forward torch _dynamo side_effects allow_side_effects_under_checkpoint tx super call_function tx args kwargs super call_function tx args kwargs BuiltinMethodVariable BaseUserFunctionVariable __init__ fn is_constant=False kwargs - None super __init__ kwargs assert isinstance fn types BuiltinMethodType fn = fn staticmethod is_supported_builtin_method obj method_self = obj __self__ method_name = obj __name__ TODO anijain - Add support more builtin methods Supports tuple __new__ frozenset __contains__ method_self tuple method_name == __new__ type method_self frozenset method_name == __contains__ call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker method_self = fn __self__ name = fn __name__ obj_source = source AttrSource source __self__ obj_vt = VariableTracker build tx method_self obj_source obj_vt call_method tx name args kwargs LocalGeneratorObjectVariable VariableTracker __init__ code types CodeType f_globals inline_tracer Optional InstructionTranslator kwargs super __init__ kwargs code = code f_globals = f_globals inline_tracer = inline_tracer get_code code get_filename get_code co_filename get_name get_code co_name get_function raise NotImplementedError has_self False __name__ get_name __str__ f __class__ __name__ get_name __repr__ = __str__ reconstruct codegen PyCodegen torch _dynamo side_effects disallow_side_effects_in_generator torch _dynamo symbolic_convert InstructionTranslator save_and_restart_speculation_log temporarely_allow_writes_to_output_graph tx = InstructionTranslator current_tx save = save_and_restart_speculation_log tx disallow = disallow_side_effects_in_generator tx temp = temporarely_allow_writes_to_output_graph tx save disallow temp tracer = _get_inline_tracer tx tracer generator_exhausted remaining_items = force_unpack_var_sequence tx variables ListIteratorVariable remaining_items reconstruct codegen bind_args tx args kwargs fn bind_args tx args kwargs get_globals f_globals python_type types GeneratorType _get_inline_tracer tx torch _dynamo symbolic_convert InliningInstructionTranslator inline_tracer None inline_tracer = InliningInstructionTranslator build_inline_tracer tx inline_tracer next_variable tx tracer = _get_inline_tracer tx _is_generator_exhausted raise_observed_exception StopIteration tx try Hierarchically tx can seen parent inline tracer created call_function Any exception needs propagated tx Dynamo behave correctly tracer inline_call_ except ObservedException e tracer generator_exhausted = True raise e except InfiniteGeneratorError test dynamo test_misc py test_iterator_limit raise except Unsupported e torch _dynamo eval_frame skip_code get_code raise SkipFrame e call_obj_hasattr tx name name python_type __dict__ ConstantVariable create True ConstantVariable create False has_unpack_var_sequence tx False has_force_unpack_var_sequence tx - builtins bool True force_unpack_var_sequence tx - list VariableTracker result = force_apply_to_var_sequence tx result append result force_apply_to_var_sequence tx fn - None while True try fn next_variable tx except ObservedUserStopIteration handle_observed_exception tx break _setup_exception tx exc tracer = _get_inline_tracer tx try tracer _raise_exception_variable exc except ObservedException e no handler available i e user code doesn t catch exception raised again tracer exception_handler e _is_generator_just_started inline_tracer None inline_tracer instruction_pointer == _is_generator_exhausted getattr inline_tracer generator_exhausted False call_method tx InstructionTranslator name str args list VariableTracker kwargs dict str VariableTracker - VariableTracker name == __next__ next_variable tx name == __iter__ iter gen returns itself name == send Sends value into generator function Returns next value yielded generator raises StopIteration generator exits without yielding another value _is_generator_just_started len args can t send non-None value just-started generator Test GeneratorCPythonTests test_send_non_none_to_new_gen all isinstance arg ConstantVariable arg value None arg args raise_observed_exception TypeError tx tracer = _get_inline_tracer tx tracer push_many args next_variable tx name == close Raises GeneratorExit point where generator function paused If generator function catches exception returns value value returned close - Python + If generator function already closed raises GeneratorExit catching exception close returns None If generator yields value RuntimeError raised If generator raises any other exception propagated caller If generator has already exited due exception normal exit close returns None has no other effect Return None close called just-started generator See test GeneratorCloseCpythonTests test_close_not_started tracer = _get_inline_tracer tx _is_generator_just_started _is_generator_exhausted tracer generator_exhausted = True variables ConstantVariable None Raise GeneratorExit see user code catches Any other exception propagated parent frame try _setup_exception tx variables ExceptionVariable GeneratorExit There s extra block Python + handle StopIteration see https github com python cpython blob f dd f b abad d df c cbd f e Objects genobject c#L -L RETURN_GENERATOR POP_TOP RESUME LOAD_CONST YIELD_VALUE RESUME POP_TOP RETURN_CONST None CALL_INTRINSIC_ INTRINSIC_STOPITERATION_ERROR RERAISE ExceptionTable - lasti sys version_info = tracer next_instruction opname == CALL_INTRINSIC_ tracer generator_exhausted = True variables ConstantVariable None except ObservedGeneratorExit If doesn t catch we just None per text above tracer generator_exhausted = True variables ConstantVariable None try Raise RuntimeError generator yields any other value next_variable tx raise_observed_exception RuntimeError tx except ObservedGeneratorExit tracer generator_exhausted = True variables ConstantVariable None except ObservedUserStopIteration In Python + one can capture GeneratorExit value See test_generator py test_close_capture_GeneratorExit_return https discuss python org t let-generator-close-return-stopiteration-value https github com python cpython pull assert tracer symbolic_result None tracer symbolic_result name == throw Raises exception point where generator paused returns next value yielded generator If generator exits without yielding raise StopIteration If generator function does catch passed-in exception raises different exception then exception propagates caller Setup exception table jump target case try finally tracer = _get_inline_tracer tx try In Python exception represented triple typ val tb In such cases we re-raise exception object given avoid creating new object so IS_OP works See https github com pytorch pytorch pull _setup_exception tx args len args == args except ObservedException noqa TRY propagate exception back parent caller raise retval = next_variable tx The exception raised before still active We need check exception table one more time find next target But why Let s walk through example its generated bytecode https godbolt org z ebdTbMv M z = whoo global z z = try yield except ValueError yield finally z += z += gen = whoo next gen gen throw ValueError print z z - z = PUSH_EXC_INFO LOAD_GLOBAL ValueError CHECK_EXC_MATCH POP_JUMP_IF_FALSE POP_TOP LOAD_CONST YIELD_VALUE ------ ValueError still active here RESUME POP_TOP POP_EXCEPT jump_backward ExceptionTable - lasti - - lasti - lasti ------ move - - lasti - - lasti - lasti In scenario generator can yield after ` throw ` called Even after exception raised few lines above remains active within ` YIELD_VALUE ` instruction When generator resumes after second yield instruction ` RESUME ` we cannot simply control flow next instruction Instead one must check exception table equivalent find next target In case says instruction pointer must moved Without step we let trace proceed next instruction would follow control flow where exception raised ` throw ` handled swallowed potentially leading incorrect behavior exc_type = type __InternalThrowException Exception try _setup_exception tx variables ExceptionVariable exc_type next_variable tx except get_dynamo_observed_exception exc_type We should get back exception raised before pass raise_observed_exception RuntimeError tracer retval super call_method tx name args kwargs ContextlibContextManagerLocalGeneratorObjectVariable LocalGeneratorObjectVariable note This only used when function annotated contextlib contextmanager It special case generator function we do allow context manager torch compile function LocalGeneratorFunctionVariable BaseUserFunctionVariable functions behaves like iterators note This wrapper around Nested UserFunctionVariable __init__ vt VariableTracker generator_cls=LocalGeneratorObjectVariable kwargs super __init__ kwargs vt = vt generator_cls = generator_cls __getattr__ name name __class__ __dict__ keys getattr name getattr vt name _build_inline_tracer tx args kwargs torch _dynamo symbolic_convert InliningInstructionTranslator InliningInstructionTranslator build_inline_tracer tx args kwargs call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker is_generator vt get_code unimplemented_v gb_type= non-generator contextlib contextmanager context=str vt get_code explanation= Cannot compile function decorated ` contextlib contextmanager ` generator i e does use ` yield ` hints= Use ` yield ` function body instead ` ` Remove ` contextlib contextmanager ` decorator inline_tracer = _build_inline_tracer tx args kwargs code = vt get_code f_globals = vt get_globals calling generator returns generator object generator_cls code f_globals inline_tracer source=self source FunctionDecoratedByContextlibContextManagerVariable LocalGeneratorFunctionVariable note This only used when function annotated contextlib contextmanager __init__ vt kwargs super __init__ vt generator_cls=ContextlibContextManagerLocalGeneratorObjectVariable kwargs _build_inline_tracer tx args kwargs NOTE This only exists break support context manager when config enable_faithful_generator_behavior = False config enable_trace_contextlib = True In case former false Dynamo should still able trace through contextmanager functions tracer = super _build_inline_tracer tx args kwargs assert isinstance tracer torch _dynamo symbolic_convert InliningGeneratorInstructionTranslator tracer is_generator_from_ctx_manager = True tracer UserMethodVariable UserFunctionVariable Some unsupported user-defined method __init__ fn obj source_fn=None kwargs - None super __init__ fn=fn kwargs obj = obj source_fn = source_fn Note source source_fn Be careful ` source ` when delegating UserFunctionVariable base-class methods In __init__ ` source ` bound method object base expects underlying function object One way simplly use ` __func__ ` unwrap For recursive dict-tag optimizations can faster fetch function directly ` cls __dict__ ` s why we pass ` source_fn ` Whenever possible access function cls __dict__ we pass ` source_fn ` Because bind_args operates unbound function most guards should target ` source_fn ` rather than original ` source ` source_fn None kwargs get source None source_fn = AttrSource kwargs get source __func__ __repr__ - str f __class__ __name__ fn obj self_args obj python_type types MethodType call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker NOTE handle methods annotated ` nonstrict_trace ` Usually ` nonstrict_trace ` -ed function will wrapped ` VariableTracker build ` route ` TorchInGraphFunctionVariable ` case method we manually wrap ` UserMethodVariable ` inside ` UserDefinedObjectVariable var_getattr ` We might able simplify away canonicalizing function method wrapping code paths trace_rules is_nonstrict_trace_callable is_nonstrict_trace_callable fn call_args = self_args args var = variables TorchInGraphFunctionVariable fn nonstrict_traceable=True var call_function tx call_args kwargs For nn Module methods redirecting NNModuleVariable call_method optimized solution rather than simple inlining E g putting ` call_method ` op FX graph ` forward ` method since we ensure ` forward ` allowed modules can traced AOT safely Note only allowed modules user customized modules can extend allowed modules using parent s ` forward ` method which also covered branch If we tracing higher order op we want Dynamo step inside module call so Dynamo can see underlying parameters buffers raise them inputs graph The is_root_tracer check bypasses condition non-root tracers directly calls super call_function end which basically equivalent inlining method tx output is_root_tracer isinstance obj variables NNModuleVariable module_attr = getattr fn __module__ inline torch nn utils parametrize module_attr None module_attr startswith torch nn module_attr = torch nn utils parametrize is_constant obj call_method tx fn __name__ args kwargs constant=self is_constant _fsdp_param_group None fn _fsdp_param_group FSDPParamGroup use_training_state variables TorchCtxManagerClassVariable fn call_function tx obj args kwargs is_constant fn = getattr obj value fn __name__ invoke_and_store_as_constant tx fn get_name args kwargs super call_function tx args kwargs inspect_parameter_names super inspect_parameter_names var_getattr tx InstructionTranslator name str name == __self__ obj name == __func__ We might have better way access function object information stored source_fn use construct variable tracker VariableTracker build tx fn source_fn super var_getattr tx name WrappedUserMethodVariable UserMethodVariable __init__ wrapped context kwargs - None kwargs pop fn None kwargs pop obj None super __init__ wrapped fn wrapped obj kwargs wrapped = wrapped context = context call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker context enter tx result = super call_function tx args kwargs context exit tx result reconstruct codegen codegen add_push_null lambda codegen context codegen wrapped codegen extend_output create_call_function False WrappedUserFunctionVariable UserFunctionVariable __init__ wrapped context kwargs - None kwargs pop fn None super __init__ wrapped fn kwargs wrapped = wrapped context = context call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker context enter tx result = super call_function tx args kwargs context exit tx result reconstruct codegen codegen add_push_null lambda codegen context codegen wrapped codegen extend_output create_call_function False invoke_and_store_as_constant tx InstructionTranslator fn name args kwargs convert x isinstance x variables TensorVariable x get_real_value x as_python_constant args = convert x x args kwargs = k convert v k v kwargs items res = fn args kwargs tx output register_attr_or_module res name source=ConstantSource name NestedUserFunctionVariable BaseUserFunctionVariable _nonvar_fields = f_globals BaseUserFunctionVariable _nonvar_fields __init__ fn_name code f_globals defaults kwdefaults annotations closure This present when function created ` functools wrap wrapped_fn this_fn ` wrapped_fn=None kwargs - None kwargs get mutation_type None kwargs update mutation_type=AttributeMutationNew super __init__ kwargs assert isinstance fn_name as_python_constant str assert isinstance code as_python_constant types CodeType assert isinstance f_globals dict fn_name = fn_name code = code f_globals = f_globals defaults = defaults kwdefaults = kwdefaults annotations = annotations closure = closure wrapped_fn Optional VariableTracker = wrapped_fn self_args get_code code as_python_constant python_type types FunctionType get_function closure raise NotImplementedError func = types FunctionType code as_python_constant f_globals fn_name as_python_constant defaults func __defaults__ = defaults as_python_constant kwdefaults func __kwdefaults__ = kwdefaults as_python_constant annotations annotations = annotations as_python_constant isinstance annotations tuple itertools pairwise annotations = dict pairwise annotations TypeError __annotations__ must set dict object assert isinstance annotations dict func __annotations__ = annotations func call_setattr tx InstructionTranslator name_var VariableTracker val VariableTracker tx output side_effects store_attr name_var value val ConstantVariable None call_method tx name args kwargs name == __setattr__ call_setattr tx args super call_method tx name args kwargs has_closure closure None const_getattr tx name name == __name__ get_name name == __code__ get_code name == __defaults__ d = getattr defaults None d as_python_constant d None super const_getattr tx name call_obj_hasattr tx InstructionTranslator name name == __code__ variables ConstantVariable create hasattr code name == __defaults__ variables ConstantVariable create hasattr defaults super call_obj_hasattr tx name has_self False get_globals f_globals bind_args parent args kwargs code = get_code func = types FunctionType code f_globals fn_name as_python_constant tuple defaults items defaults None tuple make_cell None _ range len get_code co_freevars kwdefaults func __kwdefaults__ = kwdefaults keys_as_python_constant bound = inspect signature func bind args kwargs bound apply_defaults result = dict bound arguments items wrap_args_kwargs parent output root_tx result init_cellvars parent result code idx name enumerate code co_freevars assert name result cell = closure items idx result name = cell result reconstruct codegen PyCodegen codegen add_push_null lambda codegen load_import_from __name__ _create_nested_fn codegen code codegen extend_output codegen create_load_const_unchecked f_globals codegen ConstantVariable create code value co_name defaults codegen defaults codegen extend_output codegen create_load_const None closure codegen closure codegen extend_output codegen create_load_const None kwdefaults codegen kwdefaults codegen extend_output codegen create_load_const None annotations try annotations = annotations as_python_constant codegen extend_output codegen create_load_const_unchecked annotations except NotImplementedError codegen annotations codegen extend_output codegen create_load_const None codegen extend_output create_call_function False wrapped_fn codegen add_push_null lambda codegen load_import_from functools wraps codegen wrapped_fn codegen extend_output create_call_function False codegen extend_output create_rot_n codegen extend_output create_call_function True codegen attributes torch _dynamo symbolic_convert InstructionTranslator tx = InstructionTranslator current_tx tx output side_effects has_pending_mutation name value tx output side_effects store_attr_mutations items codegen dup_top codegen value codegen extend_output create_rot_n codegen store_attr name WrappedNestedUserFunctionVariable NestedUserFunctionVariable __init__ wrapped context kwargs - None kwargs pop fn_name None kwargs pop code None kwargs pop f_globals None kwargs pop defaults None kwargs pop kwdefaults None kwargs pop annotations None kwargs pop closure None kwargs pop wrapped_fn None super __init__ wrapped fn_name wrapped code wrapped f_globals wrapped defaults wrapped kwdefaults wrapped annotations wrapped closure wrapped wrapped_fn wrapped = wrapped context = context call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker context enter tx result = super call_function tx args kwargs context exit tx result reconstruct codegen codegen add_push_null lambda codegen context codegen wrapped codegen extend_output create_call_function False SkipFunctionVariable VariableTracker _nonvar_fields = value reason VariableTracker _nonvar_fields __init__ value reason=None kwargs - None super __init__ kwargs value = value reason = reason as_python_constant value classmethod create_with_source cls value source Use closure match guard i e guard __code__ object instead function id avoid guarding nested functions inspect getattr_static value _torchdynamo_disable False For torch _dynamo disable function ensure original function guarded Otherwise branch will guard _dynamo disable __code__ guard_on_source = source guard_on_value = value while getattr guard_on_value _torchdynamo_orig_callable False guard_on_value = guard_on_value _torchdynamo_orig_callable guard_on_source = AttrSource guard_on_source _torchdynamo_orig_callable guard_on_source make_guard GuardBuilder CLOSURE_MATCH inspect isbuiltin value install_guard source make_guard GuardBuilder BUILTIN_MATCH is_wrapper_or_member_descriptor value These descriptors guaranteed same object attribute lookup They unlikely changed so we can skip guarding them install_guard source make_guard GuardBuilder CLOSURE_MATCH cls value source=source call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker inspect getattr_static value _torchdynamo_disable False msg = inspect getattr_static value _torchdynamo_disable_msg None unimplemented_v gb_type= Skip calling ` torch compiler disable ` d function context=str value explanation=f Skip calling function ` value ` since wrapped f ` torch compiler disable ` reason msg hints= Remove ` torch compiler disable ` call value torch _dynamo graph_break graph_break_msg = kwargs get msg None graph_break_msg graph_break_msg = graph_break_msg as_python_constant unimplemented_v gb_type= Call ` torch _dynamo graph_break ` context=f Called ` torch _dynamo graph_break ` args ` args ` kwargs ` kwargs ` explanation=f User-inserted graph break Message graph_break_msg hints= Remove ` torch _dynamo graph_break ` call value torch _dynamo skip_frame skip_frame_msg = kwargs get msg None skip_frame_msg skip_frame_msg = skip_frame_msg as_python_constant raise SkipFrame f Skip frame due ` torch _dynamo skip_frame ` Message skip_frame_msg value torch _dynamo step_unsupported raise StepUnsupported config dont_skip_tracing builder SourcelessBuilder re-build function attempting skip rebuilt_fn = SourcelessBuilder create tx value we still get SkipFunctionVariable then we really should skip function isinstance rebuilt_fn SkipFunctionVariable rebuilt_fn call_function tx args kwargs qualname = getattr value __qualname__ unknown qualname module_or = getattr value __module__ None module_name = unknown module module_or None str module_or try path = inspect getfile value explanation = f Dynamo developers have intentionally marked function ` qualname ` f file ` path ` should traced hints = f Avoid calling function ` qualname ` TODO improve trace_rules reasoning provide better hints How do we tell function file should NOT removed skip files Do very basic check now _dynamo path hints += f Apply ` torch _dynamo dont_skip_tracing ` function ` qualname ` force tracing into function More graph breaks may occur result attempting trace into function Please file issue PyTorch except TypeError known_python_builtin_modules = _abc _warnings module_or known_python_builtin_modules explanation = f Dynamo does know how trace Python builtin f ` module_name qualname ` hints = If you attempting call logging function e g ` _warnings warn ` you can try adding ` torch _dynamo config reorderable_logging_functions ` Please file issue GitHub so PyTorch team can add support module_or None module_or startswith optree explanation = f Dynamo cannot trace optree C C++ function module_name qualname hints = Consider using torch utils _pytree - https github com pytorch pytorch blob main torch utils _pytree py also warn because most users won t see graph break message torch _dynamo utils warn_once explanation + \n + \n join hints explanation = f Dynamo does know how trace builtin ` module_name qualname ` f This function either Python builtin e g _warnings warn f third-party C C++ Python extension perhaps created pybind hints = If Python builtin please file issue GitHub so PyTorch team can add support see next case workaround If third-party C C++ Python extension please either wrap into PyTorch-understood custom operator see https pytorch org tutorials advanced custom_ops_landing_page html more details traceable use ` torch compiler allow_in_graph ` also warn because most users won t see graph break message torch _dynamo utils warn_once explanation + \n + \n join hints qualname == allow_in_graph explanation = Found allow_in_graph decorator function which created inside parent function getting compiled This supported now hints = reason = reason reason missing reason unimplemented_v gb_type= Attempted call function marked skipped context=f module module_name qualname qualname skip reason reason explanation=explanation hints=hints call_obj_hasattr tx InstructionTranslator name variables ConstantVariable create hasattr value name var_getattr tx InstructionTranslator name str name cmp_name_to_op_mapping variables GetAttrVariable name fn_var_getattr tx value source name WrappedSkipFunctionVariable SkipFunctionVariable __init__ wrapped context kwargs - None kwargs pop value None kwargs pop reason None super __init__ wrapped value reason=wrapped reason kwargs wrapped = wrapped context = context call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker context enter tx result = super call_function tx args kwargs context exit tx result reconstruct codegen codegen add_push_null lambda codegen context codegen wrapped codegen extend_output create_call_function False WrapperUserFunctionVariable VariableTracker Used represent wrapper object contains actual callable attribute For example torch jit script trace have original function their _torchdynamo_inline attribute Similarly functions __script_if_tracing_wrapper have original attr __original_fn __init__ wrapper_obj attr_to_trace kwargs - None super __init__ kwargs wrapper_obj = wrapper_obj attr_to_trace = attr_to_trace var_getattr tx InstructionTranslator name name == attr_to_trace val = getattr wrapper_obj attr_to_trace source = source AttrSource source name VariableTracker build tx val source super var_getattr tx name self_args call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker hasattr wrapper_obj cache_info target_fn = getattr wrapper_obj attr_to_trace None module_name = getattr target_fn __module__ module_name split maxsplit= = torch msg = Dynamo detected call ` functools lru_cache ` -wrapped function Dynamo ignores cache wrapper directly traces wrapped function Silent incorrectness only potential risk something we have observed Enable TORCH_LOGS= +dynamo DEBUG stack trace torch _dynamo utils warn_once msg dynamo_logger = torch _dynamo utils logging getLogger torch _dynamo dynamo_logger isEnabledFor logging DEBUG user_stack = torch _guards TracingContext extract_stack user_stack = get_stack_above_dynamo + user_stack frame_loc = user_stack - filename user_stack - lineno user_stack_formatted = join traceback format_list user_stack user_stack_trace = f call lru_cache wrapped function frame_loc frame_loc \n user_stack_trace += str user_stack_formatted dynamo_logger debug user_stack_trace all_args = self_args + args variables UserFunctionVariable polyfills getattr_and_trace call_function tx variables ConstantVariable attr_to_trace all_args kwargs WrapperUserMethodVariable WrapperUserFunctionVariable Similar WrapperUserFunctionVariable methods The only delta saving vt ` ` object method which then used WrapperUserFunctionVariable ` call_function ` method __init__ wrapper_obj attr_to_trace self_obj kwargs - None super __init__ wrapper_obj attr_to_trace kwargs obj = self_obj self_args obj _traceable_collective_remaps We can t rely importing distributed since s always built torch distributed is_available torch distributed _functional_collectives traceable_collective_remaps traceable_collective_remaps _traceable_collectives_source tx InstructionTranslator fn assert torch distributed is_available Illegal invocation assert fn _traceable_collective_remaps values inner_name = fn __name__ path_source = tx import_source torch distributed _functional_collectives AttrSource path_source inner_name CollectiveFunctionRewriteVariable UserFunctionVariable Some torch distributed collective APIs possible rewrite traceable collectives This provides both way check function remappable perform remapping In case function remappable only some combinations call-time arguments we check args ` call_function ` time fall back graph-breaking needed This no worse than status-quo we currently graph-break all distributed collectives __init__ fn replacement_var kwargs - None super __init__ fn kwargs assert isinstance replacement_var UserFunctionVariable replacement_var = replacement_var staticmethod create tx InstructionTranslator old_fn source options new_fn new_source = CollectiveFunctionRewriteVariable rewrite tx old_fn CollectiveFunctionRewriteVariable old_fn replacement_var=UserFunctionVariable new_fn source=new_source options source=source options staticmethod can_rewrite variable inspect isfunction variable variable _traceable_collective_remaps staticmethod rewrite tx InstructionTranslator fn new_fn = _traceable_collective_remaps fn new_fn _traceable_collectives_source tx new_fn call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker call_function must check any unsupported arguments graph-break It s safe assume args kwargs orig_fn map args kwargs remapped_fn since s contract putting mapping ` traceable_collective_remaps ` torch distributed dist torch distributed _functional_collectives REDUCE_OP_TO_STR Merge args into kwargs so positional keyword args can processed same way signature = inspect signature fn kwargs = dict signature bind args kwargs arguments args = async_op kwargs kwargs async_op as_python_constant unimplemented_v gb_type= async_op=True distributed collectives context=f fn args= kwargs= explanation=f ` torch compile ` doesn t support ` async_op=True fn hints= graph_break_hints SUPPORTABLE fn dist all_reduce dist reduce_scatter_tensor dist _reduce_scatter_base reduce_op_var = kwargs get op reduce_op = reduce_op_var value reduce_op_var None signature parameters op default reduce_op REDUCE_OP_TO_STR raise ValueError f Unsupported all_reduce op reduce_op kwargs op = variables ConstantVariable create REDUCE_OP_TO_STR reduce_op replacement_var call_function tx args kwargs FunctoolsWrapsVariable UserFunctionVariable call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker kwargs len args == wraps fn isinstance fn variables NestedUserFunctionVariable fn clone wrapped_fn=args unimplemented_v gb_type= functools wraps context=f fn explanation= ` torch compile ` can t trace ` functools wraps ` functions defined outside compile region hints= graph_break_hints SUPPORTABLE variables LambdaVariable wraps super call_function tx args kwargs CollectionsNamedTupleFunction UserFunctionVariable as_python_constant fn call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker constant_args = check_constant_args args kwargs constant_args try value = fn x as_python_constant x args k v as_python_constant k v kwargs items except TypeError exc raise_observed_exception type exc tx args=list map ConstantVariable create exc args variables UserDefinedClassVariable value mutation_type=ValueMutationNew unimplemented_v gb_type= namedtuple construction context=f args= kwargs= explanation= ` torch compile ` only support certain input types namedtuple hints= graph_break_hints SUPPORTABLE FunctoolsPartialVariable VariableTracker __init__ func VariableTracker args keywords kwargs - None super __init__ kwargs func = func assert isinstance args list args = args assert isinstance keywords dict keywords = keywords fake_value used id calculation Creating value id ng sufficient tracing purposes fake_value = functools partial identity python_type functools partial reconstruct codegen PyCodegen codegen add_push_null lambda codegen load_import_from functools partial codegen func args codegen foreach args keywords codegen extend_output create_call_function len args + False codegen foreach keywords values keys = tuple keywords keys codegen extend_output codegen create_call_function_kw len keys + len args + keys False get_function as_python_constant call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker merged_args = args + args merged_kwargs = keywords kwargs func call_function tx merged_args merged_kwargs call_obj_hasattr tx InstructionTranslator name str - VariableTracker functools partial uses slots so attributes constant variables ConstantVariable create hasattr functools partial identity name var_getattr tx InstructionTranslator name str source = source AttrSource source name Handle __slots__ name == func func name == args variables ListVariable args source=source name == keywords items = ConstantVariable create k v k v keywords items variables ConstDictVariable items source=source name cmp_name_to_op_mapping variables GetAttrVariable name raise_observed_exception AttributeError tx as_python_constant functools partial func as_python_constant arg as_python_constant arg args k v as_python_constant k v keywords items guard_as_python_constant Similar as_python_constant add ID_MATCH guards try force things become constants functools partial func guard_as_python_constant v guard_as_python_constant v args k v guard_as_python_constant k v keywords items PolyfilledFunctionVariable VariableTracker _nonvar_fields = fn wrapped_fn traceable_fn VariableTracker _nonvar_fields classmethod functools cache _get_polyfill_handlers cls - dict Callable Any types FunctionType classmethod create_with_source cls value source install_guard source make_guard GuardBuilder CLOSURE_MATCH cls value source=source __init__ fn _F kwargs - None super __init__ kwargs fn _F = fn handler = _get_polyfill_handlers get fn fn assert callable handler f Polyfill handler handler callable fn candidate_attr __torch_dynamo_polyfill__ registered polyfill __python_implementation__ handler third-party libraries candidate = getattr handler candidate_attr None candidate assert callable candidate traceable_fn = candidate break raise RuntimeError f Polyfill handler handler does have traceable function wrapped_fn _F = handler traceable_fn _F = traceable_fn property polyfill_fn - _F traceable_fn can_constant_fold_through getattr wrapped_fn __torch_dynamo_can_constant_fold_through__ False get_function as_python_constant call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker can_constant_fold_through check_unspec_or_constant_args args kwargs result = fn use original function which faster than polyfill x as_python_constant x args k v as_python_constant k v kwargs items VariableTracker build tx result Special case sum tuple list ints fn builtins sum len args == kwargs isinstance args variables ListVariable variables TupleVariable all isinstance x variables ConstantVariable isinstance x value int isinstance x variables SymNodeVariable x python_type int x args items variables SymNodeVariable create tx tx output create_proxy call_function torch sym_sum tuple as_proxy args items sym_num=torch sym_sum x value isinstance x variables ConstantVariable x sym_num x args items traceable_function_variable = VariableTracker build tx traceable_fn traceable_function_variable call_function tx args kwargs call_method tx name args list VariableTracker kwargs dict str VariableTracker - VariableTracker name == __call__ call_function tx args kwargs method = getattr fn name None method is_function method raise_type_error_exc tx f Cannot find callable name fn options = source options source = AttrSource source name polyfilled_method_variable = PolyfilledFunctionVariable method options polyfilled_method_variable call_function tx args kwargs as_python_constant fn TracebackVariable VariableTracker We don t track traceback A call any function module no-op call_function tx args kwargs SysFunctionVariable VariableTracker __init__ value kwargs super __init__ kwargs value = value exc_info tx len tx exn_vt_stack exn = tx exn_vt_stack - typ = exn exc_type tb = None items = VariableTracker build tx typ exn VariableTracker build tx tb items = variables ConstantVariable None variables ConstantVariable None variables ConstantVariable None variables TupleVariable items exception tx exc_info tx items call_function tx args kwargs value sys exc_info exc_info tx assert value sys exception exception tx torch _higher_order_ops triton_kernel_wrap create_tma_experimental_metadata create_tma_stable_metadata TMADescriptorMetadata TritonHOPifier DynamoTritonHOPifier TritonHOPifier raise_unsupported msg str - Never raise Unsupported msg is_callable maybe_callable Any - bool isinstance maybe_callable NestedUserFunctionVariable UserFunctionVariable get_value val Any - Any val value check_grid grid - tuple torch fx proxy Proxy lists BaseListVariable isinstance grid BaseListVariable grid as_proxy unimplemented_v gb_type= unsupported grid type triton hop check_grid context=f grid type = type grid explanation= ` torch compile ` only supports list-like grid check_grid hints= graph_break_hints SUPPORTABLE call_grid grid meta tx meta = variables ConstantVariable create k v k v meta items grid = grid call_function tx meta grid We use function wrap call_prune_configs call_user_defined_fn user_fn args kwargs tx variable builder SourcelessBuilder wrapped_user_function = SourcelessBuilder create tx user_fn result = wrapped_user_function call_function tx args kwargs result wrap_user_defined_obj user_obj tx variable name builder VariableBuilder wrapped_user_obj = VariableBuilder tx AttrSource variable kernel_source f name _wrap user_obj wrapped_user_obj maybe_unpack_configs configs tx unpack list configs configs = configs unpack_var_sequence tx guard_as_python_constant inserts guards Dynamo check configs object changed configs = config guard_as_python_constant config configs configs maybe_unpack_heuristic_result result Any - Any result is_python_constant raise_unsupported triton heuristics must constant values because configs can only contain constant values result guard_as_python_constant We need override call_getitem here so we can add source case where we call triton kernel grid call_getitem variable TritonKernelVariable args Sequence Any - TritonKernelVariable __getitem__ should only called we don t already have grid Only grid needs passed variable grid None len args = raise_unsupported Triton kernels should called only single grid type variable kernel=variable kernel kernel_idx=variable kernel_idx grid=args kernel_source=variable source call_HOP variable grids combined_args_raw tx - ConstantVariable constant ConstantVariable dicts ConstDictVariable we can only pass tensors non-const args fx graph here we replace TMA descriptors TMADescriptorExperimentalVariable TMADescriptorStableVariable instances underlying tensors while moving TMA descriptor-related metadata separate argument so we can reconstruct TMA descriptors downstream tma_descriptor_metadata TMADescriptorMetadata = k list combined_args_raw keys v = combined_args_raw k isinstance v TMADescriptorExperimentalVariable TMADescriptorStableVariable tma_descriptor_metadata k = v to_metadata combined_args_raw k = v get_tensor combined_args = variables ConstantVariable create k v k v combined_args_raw items torch _higher_order_ops triton_kernel_wrap kernel_side_table triton_kernel_wrapper_mutation Combine args kwargs pass dict so user defined triton kernel uses variables grid kernel does conflict parameters wrapper function constant_args = k v as_python_constant k v combined_args_raw items isinstance v ConstantVariable non_constant_args = k v k v combined_args items isinstance v ConstantVariable v non_constant_args values v = v realize isinstance v variables TensorVariable variables SymNodeVariable raise_unsupported f Unexpected argument type Triton kernel repr v constant_args_idx = kernel_side_table add_constant_args constant_args meta = ConstDictVariable non_constant_args dict tx output create_proxy call_function triton_kernel_wrapper_mutation kernel_idx variable kernel_idx constant_args_idx constant_args_idx grid grids tma_descriptor_metadata tma_descriptor_metadata kwargs meta as_proxy variables ConstantVariable None dynamo_triton_hopifier_singleton = DynamoTritonHOPifier TritonKernelVariable VariableTracker grid TritonGridType kernel TritonKernelType kernel_idx Optional int kernel_source AttrSource __init__ kernel kernel_idx grid kwargs - None kernel_source = kwargs pop kernel_source None super __init__ kwargs dynamo_triton_hopifier_singleton init_variable kernel kernel_idx grid call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker dynamo_triton_hopifier_singleton call_triton_kernel args kwargs tx call_method tx name args list VariableTracker kwargs dict str VariableTracker - VariableTracker name == __getitem__ dynamo_triton_hopifier_singleton call_getitem args name == run dynamo_triton_hopifier_singleton call_run args kwargs tx Bail out parent s implementation super call_method tx name args kwargs specialize_symbolic arg Any - Any constant ConstantVariable tensor SymNodeVariable See Note Specialize tl constexpr args user-defined triton kernels isinstance arg SymNodeVariable ConstantVariable create arg evaluate_expr arg TMADescriptorExperimentalVariable VariableTracker __init__ data_ptr variables DataPtrVariable dims list ConstantVariable block_dims list ConstantVariable element_size ConstantVariable kwargs assert isinstance data_ptr variables DataPtrVariable super __init__ kwargs data_ptr = data_ptr dims = dims block_dims = block_dims element_size = element_size to_metadata create_tma_experimental_metadata dim as_proxy dim dims dim as_proxy dim block_dims element_size as_proxy reconstruct codegen PyCodegen codegen add_push_null lambda codegen load_import_from triton tools experimental_descriptor f create_ len dims d_tma_descriptor data_ptr reconstruct codegen args = dims block_dims element_size codegen foreach args codegen call_function len args + False get_tensor data_ptr from_tensor TMADescriptorStableVariable VariableTracker __init__ tensor variables TensorVariable block_shape variables ListVariable kwargs assert isinstance tensor variables TensorVariable super __init__ kwargs tensor = tensor block_shape = block_shape to_metadata create_tma_stable_metadata block_shape as_proxy reconstruct codegen PyCodegen codegen add_push_null lambda codegen load_import_from triton tools tensor_descriptor TensorDescriptor codegen load_method from_tensor tensor reconstruct codegen codegen block_shape codegen call_method get_tensor - variables TensorVariable tensor CreateTMADescriptorExperimentalVariable VariableTracker __init__ rank int kwargs - None assert rank super __init__ kwargs rank = rank call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker ptr = kwargs ptr ptr kwargs args isinstance ptr variables DataPtrVariable raise Unsupported Please ensure there no graph breaks between f create_ rank d_tma_descriptor upstream data_ptr call rank == len args + len kwargs = raise_type_error_exc tx f TMA metadata rank= requires exactly arguments got len args + len kwargs dims = kwargs dim dim kwargs args block_dims = kwargs block_dim block_dim kwargs args len args + len kwargs = raise_type_error_exc tx f TMA metadata rank= requires exactly arguments got len args + len kwargs dims = kwargs dim dim kwargs args kwargs dim dim kwargs args block_dims = kwargs block_dim block_dim kwargs args kwargs block_dim block_dim kwargs args element_size = kwargs element_size element_size kwargs args - TMADescriptorExperimentalVariable data_ptr=ptr dims=dims block_dims=block_dims element_size=element_size CreateTMADescriptorStableVariable VariableTracker call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker tensor = kwargs tensor tensor kwargs args block_shape = kwargs block_shape block_shape kwargs args TMADescriptorStableVariable tensor=tensor block_shape=block_shape