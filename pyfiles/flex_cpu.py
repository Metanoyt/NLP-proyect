mypy allow-untyped-defs CPU-specific implementations flex attention copy os sys typing Any sympy torch torch _inductor virtualized V torch utils _ordered_set OrderedSet torch utils _sympy numbers int_oo torch utils _sympy value_ranges ValueRanges codegen cpp_flex_attention_template CppFlexAttentionTemplate ir Buffer FixedLayout TensorBox select_algorithm autotune_select_algorithm common build_subgraph_buffer build_subgraph_module_buffer contiguous_last_dim create_placeholder get_fwd_subgraph_outputs infer_dense_strides maybe_realize check_cpu_supported requires_avx _on_cpu = torch cpu _is_avx _supported os getenv ATEN_CPU_CAPABILITY = default supported = requires_avx _on_cpu torch xpu is_available sys platform = darwin supported lower_cpu query key value subgraph block_mask scale kernel_options score_mod_other_buffers mask_mod_other_buffers CPP based template flex attention x CPUs _ q_length _ kv_length kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices q_num_blocks q_indices full_q_num_blocks full_q_indices SPARSE_Q_BLOCK_SIZE SPARSE_KV_BLOCK_SIZE mask_graph = block_mask kernel_options OUTPUT_LOGSUMEXP raise NotImplementedError torch compile CPU only supports inference ` return_lse ` supported yet check_cpu_supported raise NotImplementedError torch compile current platform supported CPU fake_buffers list Buffer = noqa F Note Handle case where split sizes statically known The value cur_qSplitSize cur_kvSplitSize decided during runtime We use symbols represent them during compilation here They ll replaced string cur_qSplitSize cur_kvSplitSize modification function CppFlexAttentionTemplate cur_qSplitSize = V graph sizevars shape_env create_unbacked_symint node expr cur_kvSplitSize = V graph sizevars shape_env create_unbacked_symint node expr shape_env = V graph sizevars shape_env We don t know concrete value cur_qSplitSize cur_kvSplitSize during compilation Mark symbols ensure broadcasting always applied This avoids treating them equal when ` eq var ` evaluated ` broadcast_symbolic_shapes ` shape_env var_to_range cur_qSplitSize = ValueRanges int_oo shape_env var_to_range cur_kvSplitSize = ValueRanges int_oo score_dtype = torch float placeholder_inps = create_placeholder name dtype query get_device size name dtype size score score_dtype cur_qSplitSize cur_kvSplitSize b torch int h torch int q_idx torch int cur_qSplitSize kv_idx torch int cur_kvSplitSize subgraph_buffer = build_subgraph_buffer placeholder_inps + list score_mod_other_buffers subgraph subgraph_buffer None isinstance subgraph_buffer list _buf subgraph_buffer _buf None _buf freeze_layout subgraph_buffer freeze_layout mask_graph_placeholder_inps = create_placeholder name dtype query get_device size name dtype size score score_dtype cur_qSplitSize cur_kvSplitSize b torch int h torch int q_idx torch int cur_qSplitSize kv_idx torch int cur_kvSplitSize The original mask_graph works scalar only includes logic calculating mask value We need add logic applying mark qk_data tensor into graph later codegen part Example mask_graph mask_fn b h q_idx kv_idx mask = q_idx = kv_idx mask The converted_mask_graph should converted_mask_fn qk_data b h q_idx kv_idx mask = q_idx = kv_idx qk_data = torch where mask qk_data torch full_like qk_data -float inf qk_data convert_mask_graph_module mask_graph gm = copy deepcopy mask_graph graph_module graph = gm graph Add qk_data first input graph inserting_before next iter graph nodes qk_data_node = graph placeholder qk_data Find node returns mask output_node = None node graph nodes node op == output output_node = node break Get mask node assert output_node None mask_node = output_node args size_node = cur_qSplitSize cur_kvSplitSize Create new node torch full graph inserting_after mask_node full_node = graph call_function torch full args= size_node -float inf kwargs= dtype score_dtype Create new node torch where graph inserting_after full_node where_node = graph call_function torch ops aten where args= mask_node qk_data_node full_node Update output node result torch where output_node args = where_node graph lint converted = torch fx GraphModule gm graph converted converted_mask_graph_module = convert_mask_graph_module mask_graph mask_graph_buffer = build_subgraph_module_buffer mask_graph_placeholder_inps + list mask_mod_other_buffers converted_mask_graph_module Clear pending fresh unbacked symbols created cur_qSplitSize cur_kvSplitSize current kernel pending = V graph sizevars shape_env pending_fresh_unbacked_symbols V graph sizevars shape_env pending_fresh_unbacked_symbols = x x pending x cur_qSplitSize cur_kvSplitSize buffer_list = placeholder_inps + list score_mod_other_buffers + mask_graph_placeholder_inps + list mask_mod_other_buffers item buffer_list isinstance item TensorBox fake_buffers append item data data type ignore attr-defined CPU kernel requires last dim contiguous query key value = map contiguous_last_dim query key value query key value kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices q_num_blocks q_indices full_q_num_blocks full_q_indices = maybe_realize query key value kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices q_num_blocks q_indices full_q_num_blocks full_q_indices len OrderedSet query get_name key get_name value get_name = raise NotImplementedError Unsupported now query key value same buffer query get_dtype torch float torch bfloat torch float raise NotImplementedError ` torch float ` ` torch float ` ` torch bfloat ` supported FlexAttention CPU device f Found input tensors ` query get_dtype ` score_mod_other_buffers = maybe_realize score_mod_other_buffers mask_mod_other_buffers = maybe_realize mask_mod_other_buffers Bq Hq seq_len_q qk_head_dim = query get_size Bkv Hkv seq_len_kv v_head_dim = value get_size B = Bq Construct output layout strides matching query out_size = B Hq seq_len_q v_head_dim out_strides = infer_dense_strides out_size query get_stride layout = FixedLayout query get_device query get_dtype B Hq seq_len_q v_head_dim stride= sympy sympify s s out_strides _choices list Any = input_nodes = query key value kv_num_blocks kv_indices full_kv_num_blocks no_full_kv_block = True no_full_kv_block = False input_nodes += full_kv_num_blocks input_nodes += full_kv_indices has_other_buffer = False kernel_input_name_to_buffer = score_mod_other_buffers mask_mod_other_buffers has_other_buffer = True prefix buffers score_others score_mod_other_buffers mask_others mask_mod_other_buffers kernel_input_name_to_buffer update f prefix _ i buf i buf enumerate buffers input_nodes += value value kernel_input_name_to_buffer values isinstance value sympy Symbol skip_mask_score = kernel_options get SKIP_MASK_SCORE False Mark SPARSE_KV_BLOCK_SIZE SPARSE_Q_BLOCK_SIZE static shapes add guards SPARSE_KV_BLOCK_SIZE = V graph sizevars guard_int SPARSE_KV_BLOCK_SIZE SPARSE_Q_BLOCK_SIZE = V graph sizevars guard_int SPARSE_Q_BLOCK_SIZE assert V graph sizevars evaluate_expr sympy Le seq_len_q sympy Mul kv_indices get_size - SPARSE_Q_BLOCK_SIZE Q seqlen must smaller than block_mask size Q dimension considering pass larger block_mask assert V graph sizevars evaluate_expr sympy Le seq_len_kv sympy Mul kv_indices get_size - SPARSE_KV_BLOCK_SIZE KV seqlen must smaller than block_mask size KV dimension considering pass larger block_mask CppFlexAttentionTemplate add_choices choices=_choices input_nodes=input_nodes layout=layout scale=scale score_mod=None skip_mask_score subgraph_buffer mask_mod=None skip_mask_score mask_graph_buffer kv_block_size=SPARSE_KV_BLOCK_SIZE q_block_size=SPARSE_Q_BLOCK_SIZE has_other_buffer=has_other_buffer no_full_kv_block=no_full_kv_block fake_buffers=fake_buffers len_score_other=len score_mod_other_buffers len_mask_other=len mask_mod_other_buffers kernel_input_name_to_buffer=kernel_input_name_to_buffer block_vars= cur_qSplitSize cur_kvSplitSize inputs_for_autotuning = query key value res = autotune_select_algorithm flex_attention _choices inputs_for_autotuning layout need subgraph inputs outputs analyze all symints used flex attention res data data subgraph_inps = list score_mod_other_buffers + list mask_mod_other_buffers res data data subgraph_outs = get_fwd_subgraph_outputs subgraph_buffer mask_graph_buffer res