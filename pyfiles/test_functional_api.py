Owner s oncall distributed sys unittest functools partial wraps torch torch distributed dist torch distributed _functional_collectives ft_c torch distributed distributed_c d c d torch distributed tensor dt functorch make_fx torch _inductor utils run_and_get_code torch testing FileCheck torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal inductor_utils HAS_GPU dist is_available print Distributed available skipping tests file=sys stderr sys exit torch testing _internal common_distributed DistributedTestBase MultiThreadedTestCase requires_accelerator_dist_backend TEST_SKIPS torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests skipIfHpu TEST_CUDA TEST_HPU TEST_XPU TestCase NOTE Instructions adding new device types test file This test file contains two types tests Tests run both CPUs accelerators Tests run only accelerators We use two variables manage device types - ` devices ` A list containing device types both CPU accelerator tests - ` DEVICE ` A string containing only accelerator type accelerator-only tests To add new device type Add new ` ` statement if-else ladder below Check presence your device e g TEST_NEW_DEVICE Append your device type ` devices ` list Assign your device type string ` DEVICE ` Example TEST_NEW_DEVICE devices append new_device DEVICE = new_device DEVICE = cuda devices = cpu TEST_HPU devices append hpu DEVICE = hpu TEST_XPU devices append xpu DEVICE = xpu TEST_CUDA devices append cuda new_subgroups group_size int pg_tag=None world_size = dist get_world_size subgroups = cur_subgroup = None subgroup_id range world_size group_size start_rank = subgroup_id group_size end_rank = start_rank + group_size ranks_in_subgroup = list range start_rank end_rank subgroup = c d _new_group_with_tag ranks=ranks_in_subgroup pg_tag=pg_tag subgroups append subgroup rank = dist get_rank rank ranks_in_subgroup cur_subgroup = subgroup cur_subgroup subgroups skipIfHpu TestExpand MultiThreadedTestCase property world_size setUp super setUp _spawn_threads test_expand_ d_rank_list tag rankset group_size = ft_c _expand_group assertEqual tag assertEqual rankset assertEqual group_size tag rankset group_size = ft_c _expand_group bla assertEqual bla tag test_expand_ d_rank_list tag rankset group_size = ft_c _expand_group assertEqual tag assertEqual rankset assertEqual group_size tag rankset group_size = ft_c _expand_group blu assertEqual blu tag assertRaisesRegex ValueError group sizes must identical ft_c _expand_group test_expand_process_group tag rankset group_size = ft_c _expand_group dist group WORLD assertEqual c d _get_group_tag dist group WORLD tag assertEqual rankset assertEqual group_size tag rankset group_size = ft_c _expand_group dist group WORLD bla assertEqual bla tag my_pg _ = new_subgroups group_size= tag rankset group_size = ft_c _expand_group my_pg assertEqual c d _get_group_tag my_pg tag assertEqual dist get_process_group_ranks my_pg rankset assertEqual group_size my_pg = None i range dist get_world_size group = c d _new_group_with_tag i pg_tag= my_pg i == dist get_rank my_pg = group tag rankset group_size = ft_c _expand_group my_pg assertEqual my_pg tag assertEqual dist get_rank rankset assertEqual group_size tag rankset group_size = ft_c _expand_group my_pg bla assertEqual bla tag test_expand_device_mesh mesh = dt DeviceMesh cpu torch arange tag rankset group_size = ft_c _expand_group mesh assertEqual c d _get_group_tag mesh get_group mesh_dim= tag assertEqual rankset assertEqual group_size mesh = dt DeviceMesh cpu torch arange tag rankset group_size = ft_c _expand_group mesh assertEqual c d _get_group_tag mesh get_group mesh_dim= tag assertEqual rankset assertEqual group_size test_expand_device_mesh_tuple mesh = dt DeviceMesh cpu torch arange view assertRaisesRegex AssertionError Only D mesh tag rankset group_size = ft_c _expand_group mesh tag rankset group_size = ft_c _expand_group mesh assertEqual c d _get_group_tag mesh get_group mesh_dim= tag expected_rankset = dist get_rank assertEqual expected_rankset rankset assertEqual group_size tag rankset group_size = ft_c _expand_group mesh expected_rankset = dist get_rank assertEqual c d _get_group_tag mesh get_group mesh_dim= tag assertEqual expected_rankset rankset assertEqual group_size skipIfHpu TestPgTag MultiThreadedTestCase property world_size setUp super setUp _spawn_threads The behavior we want follow - rankset+tag will always result same PG Do we enforce failing creation new PGs returning existing ones Return existing one - default tag gives existing behavior This means we should create duplicates - _expand_group _default-tagged pg should always resolve This mean we can t depend empty tag + rankset test_pg_creation_with_tag my_group _ = new_subgroups group_size= pg_tag= blu my_group _ = new_subgroups group_size= pg_tag= blu assertEqual my_group my_group my_group _ = new_subgroups group_size= pg_tag= blu assertNotEqual my_group my_group my_group _ = new_subgroups group_size= assertNotEqual my_group my_group my_group _ = new_subgroups group_size= assertNotEqual my_group my_group test_pg_lookup_roundtrip pg_tag _ = new_subgroups group_size= pg_tag= blu pg_tag _ = new_subgroups group_size= pg_tag= blu pg_notag _ = new_subgroups group_size= pg_notag _ = new_subgroups group_size= roundtrip pg tag rankset _ = ft_c _expand_group pg c d _find_pg_by_ranks_and_tag tag rankset assertEqual pg_tag roundtrip pg_tag assertEqual pg_tag roundtrip pg_tag assertEqual pg_notag roundtrip pg_notag assertEqual pg_notag roundtrip pg_notag test_pg_lookup_with_tag pg_tag _ = new_subgroups group_size= pg_tag= blu pg_tag _ = new_subgroups group_size= pg_tag= bla pg_notag _ = new_subgroups group_size= roundtrip pg pg_tag tag rankset _ = ft_c _expand_group pg pg_tag c d _find_pg_by_ranks_and_tag tag rankset assertEqual pg_tag roundtrip pg_tag blu assertEqual pg_tag roundtrip pg_notag blu Cannot erase tag PG assertEqual pg_tag roundtrip pg_tag test_find_or_create_pg pg = c d _find_or_create_pg_by_ranks_and_tag blu pg_tag _ = new_subgroups group_size= pg_tag= blu assertEqual pg pg_tag test_find_root_pg pg = c d _find_pg_by_ranks_and_tag assertEqual dist group WORLD pg instantiate_parametrized_tests skipIfHpu TestTraceableCollectives MultiThreadedTestCase property world_size setUp super setUp _spawn_threads parametrize device devices test_broadcast device device = cpu torch accelerator device_count world_size skipTest Not enough accelerator devices torch accelerator set_device_index dist get_rank dist get_rank == tensor = torch ones device=device tensor = torch zeros device=device mesh = dt DeviceMesh device torch arange res = ft_c broadcast tensor mesh assertEqual res torch ones device=device parametrize device devices test_all_reduce_eager device device = cpu torch accelerator device_count world_size skipTest Not enough accelerator devices torch accelerator set_device_index dist get_rank tensor = torch ones device=device mesh = dt DeviceMesh device torch arange res = ft_c all_reduce tensor sum mesh assertEqual res torch tensor dtype=torch float mesh = dt DeviceMesh device torch arange view res = ft_c all_reduce tensor sum mesh assertEqual res torch tensor dtype=torch float parametrize device devices test_all_reduce_coalesced_eager device device = cpu torch accelerator device_count world_size skipTest Not enough accelerator devices torch accelerator set_device_index dist get_rank t = torch ones device=device t = torch ones device=device + mesh = dt DeviceMesh device torch arange res = ft_c all_reduce_coalesced t t sum mesh assertEqual res t assertEqual res t parametrize device devices test_all_gather_tensor device device = cpu torch accelerator device_count world_size skipTest Not enough accelerator devices torch accelerator set_device_index dist get_rank testing d d mesh mesh_ d = dt DeviceMesh device torch arange world_size mesh_ d = dt DeviceMesh device torch arange world_size view mesh mesh_ d mesh_ d dims_to_gather = dim dims_to_gather output_size = output_size dim = mesh size each rank have its own tensor all_gather gives bigger tensor local_tensor = torch ones device=device gathered_tensor = ft_c all_gather_tensor local_tensor gather_dim=dim group= mesh assertEqual gathered_tensor torch ones output_size parametrize device devices test_all_gather_into_tensor_coalesced device device = cpu torch accelerator device_count world_size skipTest Not enough accelerator devices torch accelerator set_device_index dist get_rank tensors = torch ones device=device torch ones device=device + mesh = dt DeviceMesh device torch arange res = ft_c all_gather_into_tensor_coalesced tensors mesh assertEqual len res assertEqual torch ones dist get_world_size device=device res assertEqual torch ones dist get_world_size device=device + res parametrize device devices test_reduce_scatter_tensor device device = cpu torch accelerator device_count world_size skipTest Not enough accelerator devices torch accelerator set_device_index dist get_rank testing d d mesh mesh_ d = dt DeviceMesh device torch arange world_size mesh_ d = dt DeviceMesh device torch arange world_size view mesh mesh_ d mesh_ d dims_to_scatter = dim dims_to_scatter group_size = mesh size input_size = output_size = output_size dim = group_size input_tensor = torch ones output_size device=device res_num = group_size rs_tensor = ft_c reduce_scatter_tensor input_tensor sum scatter_dim=dim group= mesh assertEqual rs_tensor torch ones input_size res_num parametrize device devices test_reduce_scatter_into_tensor_coalesced device device = cpu torch accelerator device_count world_size skipTest Not enough accelerator devices torch accelerator set_device_index dist get_rank tensors = torch ones dtype=torch int device=device torch ones dtype=torch int device=device + mesh = dt DeviceMesh device torch arange res = ft_c reduce_scatter_tensor_coalesced tensors sum mesh assertEqual len res assertEqual torch tensor device=device res assertEqual torch tensor device=device res TestMetaCollectives TestCase test_all_reduce x = torch rand device= meta out = ft_c all_reduce x sum assertEqual x size out size skipIfHpu TestGradCollectives MultiThreadedTestCase property world_size setUp super setUp _spawn_threads test_all_reduce x = torch rand requires_grad=True y = torch rand requires_grad=True out = ft_c all_reduce x sum dist group WORLD out + y sum backward assertIsNone x grad TestMakeFx TestCase setUp make_fx thread-safe due patching nd mutating global states so create fake_pg rank = world_size = dist init_process_group backend= fake world_size=self world_size rank=self rank tearDown super tearDown assertFalse torch fx _symbolic_trace is_fx_tracing test_all_reduce_tracing allred input ft_c all_reduce input sum group=dist group WORLD + graph = make_fx allred torch rand FileCheck check all_reduce check wait_tensor run str graph graph mesh = dt DeviceMesh cpu torch arange world_size allred_mesh input ft_c all_reduce input sum mesh + mesh_graph = make_fx allred_mesh torch rand FileCheck check_not get_attr check wait_tensor run str mesh_graph graph allred_mesh_dim input ft_c all_reduce input sum mesh + mesh_dim_graph = make_fx allred_mesh_dim torch rand FileCheck check_not get_attr check wait_tensor run str mesh_dim_graph graph BACKEND = dist Backend NCCL torch cuda is_available dist Backend GLOO Adding support HCCL backend To add different backend add same chain conditional checking device type along lines TEST_HPU TEST_CUDA And then set BACKEND variable appropriately TEST_HPU BACKEND = dist Backend HCCL TEST_XPU BACKEND = dist Backend XCCL allows you check multiple accelerator irrespective device type add new device types check simply follow same format append conditional appropriate device count function your new device exit_if_lt_x_accelerators x torch accelerator is_available torch accelerator device_count x sys exit TEST_SKIPS f multi-accelerator- x exit_code with_comms func=None func None partial with_comms wraps func wrapper args kwargs BACKEND == dist Backend NCCL BACKEND == dist Backend XCCL torch accelerator device_count world_size sys exit TEST_SKIPS f multi-gpu- world_size exit_code kwargs device = DEVICE pg = create_pg device=DEVICE try func args kwargs finally torch distributed destroy_process_group wrapper TestCollectivesWithDistributedBackend DistributedTestBase with_comms test_all_gather_into_tensor_coalesced device exit_if_lt_x_accelerators world_size tensors = torch ones device=device torch ones device=device + mesh = dt DeviceMesh device torch arange world_size res = ft_c all_gather_into_tensor_coalesced tensors mesh assertEqual len res assertEqual torch ones dist get_world_size res assertEqual torch ones dist get_world_size + res with_comms test_all_to_all_single device mesh = dt DeviceMesh device torch arange world_size rank = dist get_rank row = world_size rank + world_size + x = torch ones int row device=device rank + split_sizes = i + rank + i range world_size y = ft_c all_to_all_single x output_split_sizes=split_sizes input_split_sizes=split_sizes group=mesh expected = idx tensor enumerate torch split x split_sizes expected append torch full_like tensor idx + expected = torch cat expected assertEqual y expected with_comms test_all_to_all_single_ d_input device mesh = dt DeviceMesh device torch arange world_size rank = dist get_rank row = world_size rank + world_size + x = torch ones int row device=device rank + split_sizes = i + rank + i range world_size y = ft_c all_to_all_single x output_split_sizes=split_sizes input_split_sizes=split_sizes group=mesh expected = idx tensor enumerate torch split x split_sizes expected append torch full_like tensor idx + expected = torch cat expected assertEqual y expected with_comms test_all_to_all_single_split_sizes_none device mesh = dt DeviceMesh device torch arange world_size rank = dist get_rank x = torch ones world_size world_size device=device rank + y = ft_c all_to_all_single x output_split_sizes=None input_split_sizes=None group=mesh expected = idx tensor enumerate torch chunk x world_size expected append torch full_like tensor idx + expected = torch cat expected assertEqual y expected unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch requires_accelerator_dist_backend nccl xccl with_comms test_tracing device allreduce t pg ft_c all_reduce t sum pg compiled_allreduce = torch compile allreduce fullgraph=True compiled_allreduce torch randn device=device pg unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_tracing_with_fakepg device=DEVICE exit_if_lt_x_accelerators world_size allreduce t pg ft_c all_reduce t sum pg compiled_allreduce = torch compile allreduce fullgraph=True noqa F dist init_process_group backend= fake rank= world_size= allreduce torch randn device=device pg=dist group WORLD dist destroy_process_group unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch requires_accelerator_dist_backend nccl xccl with_comms test_tracing_with_dce_code device world_size func batch group rank ret = ft_c permute_tensor batch group hasattr ret wait ret = ret wait rank == ret batch compiled_func = torch compile func compiled_func torch ones device=device process_group rank dist barrier TestDistributedBackendCollectivesWithWorldSize TestCollectivesWithDistributedBackend property world_size with_comms test_permute_tensor_with_sub_group device exit_if_lt_x_accelerators world_size mesh_dim_names = dp tp mesh_ d = dt init_device_mesh device world_size mesh_dim_names=mesh_dim_names mesh_name mesh_dim_names mesh = mesh_ d mesh_name rank = mesh get_local_rank rank rank send_tensor = torch arange dtype=torch float device=device + rank recvd_tensor = ft_c permute_tensor send_tensor group=mesh rank rank expected = torch arange dtype=torch float device=device + rank - + assertEqual recvd_tensor expected msg=f Expected expected rank= local_rank= rank f received recvd_tensor instead instantiate_parametrized_tests skipIfHpu TestFunctionalAutograd MultiThreadedTestCase setUp super setUp _spawn_threads property world_size parametrize compile True False test_all_to_all_single compile bool = True - None group = dist group WORLD group_name t = torch ones world_size requires_grad=True my_func t torch Tensor world_size int - torch Tensor sizes = world_size t = t assert t requires_grad out = ft_c all_to_all_single_autograd t sizes sizes group out = out + out compile compiled = torch compile my_func fullgraph=True backend= aot_eager compiled = my_func out = compiled t world_size assertEqual out shape t shape assertEqual out torch full_like t assertIsNotNone out grad_fn assertTrue out requires_grad loss = out sum loss backward assertEqual t grad torch full_like t test_all_to_all_single_inductor - None group = dist group WORLD group_name t = torch rand world_size requires_grad=True my_func t torch Tensor world_size int - torch Tensor sizes = world_size t = t assert t requires_grad out = ft_c all_to_all_single_autograd t sizes sizes group out = out + out sum compiled = torch compile my_func fullgraph=True run_with_backward out = compiled t world_size out backward _ codes = run_and_get_code run_with_backward code codes assert_keywords = assert_size_stride assert_alignment filtered_lines = line line code splitlines any assert_key line assert_key assert_keywords code = \n join filtered_lines FileCheck check_count _c d_functional all_to_all_single default exactly=True check_count _c d_functional wait_tensor default exactly=True run code assertIsNotNone t grad parametrize compile True False test_all_gather_tensor compile bool - None group = dist group WORLD group_name my_func t torch Tensor dim int - torch Tensor assert t requires_grad out = ft_c all_gather_tensor_autograd t gather_dim=dim group=group out = out out compile compiled = torch compile my_func fullgraph=True backend= aot_eager compiled = my_func dims_to_gather = dim dims_to_gather output_size = output_size dim = world_size each rank have its own tensor all_gather gives bigger tensor local_tensor = torch ones requires_grad=True gathered_tensor = compiled local_tensor dim assertEqual gathered_tensor torch ones output_size gathered_tensor sum backward assertEqual local_tensor grad torch full fill_value=float world_size parametrize compile True False test_reduce_scatter_tensor compile bool - None group = dist group WORLD group_name my_func t torch Tensor dim int - torch Tensor assert t requires_grad rs_tensor = ft_c reduce_scatter_tensor_autograd input_tensor sum scatter_dim=dim group=group rs_tensor compile compiled = torch compile my_func fullgraph=True backend= aot_eager compiled = my_func dims_to_scatter = dim dims_to_scatter group_size = world_size input_size = output_size = output_size dim = group_size input_tensor = torch ones output_size requires_grad=True rs_tensor = compiled input_tensor dim res_num = group_size assertEqual rs_tensor torch ones input_size res_num rs_tensor sum backward assertEqual input_tensor grad torch full output_size fill_value= TestFunctionalAutogradWithDistributedBackend DistributedTestBase with_comms test_all_to_all_single device - None group = pg t = torch ones world_size requires_grad=True device=device sizes = world_size assert t requires_grad out = ft_c all_to_all_single_autograd t sizes sizes group + assertEqual out shape t shape assertEqual out torch full_like t assertIsNotNone out grad_fn assertTrue out requires_grad loss = out sum loss backward assertEqual t grad torch full_like t Update supported devices DEVICE instantiate_device_type_tests TestCollectivesWithDistributedBackend globals only_for=DEVICE allow_xpu=True instantiate_device_type_tests TestDistributedBackendCollectivesWithWorldSize globals only_for=DEVICE allow_xpu=True instantiate_device_type_tests TestFunctionalAutogradWithDistributedBackend globals only_for=DEVICE allow_xpu=True __name__ == __main__ run_tests