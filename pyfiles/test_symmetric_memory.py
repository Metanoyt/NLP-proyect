Owner s module c d itertools os random contextlib nullcontext unittest skip skipIf skipUnless torch torch distributed dist torch distributed _symmetric_memory symm_mem torch _C _autograd DeviceType torch _C _distributed_c d _SymmetricMemory torch _inductor utils fresh_cache run_and_get_triton_code torch distributed _functional_collectives all_gather_tensor torch distributed _symmetric_memory _fused_all_gather_matmul_fallback _fused_all_gather_scaled_matmul_fallback _fused_matmul_reduce_scatter_fallback _test_mode enable_symm_mem_for_group restride_A_for_fused_matmul_reduce_scatter restride_A_shard_for_fused_all_gather_matmul torch testing _internal common_cuda _get_torch_cuda_version SM OrLater SM OrLater SM OrLater xfailIfSM OrLater torch testing _internal common_device_type e m _type torch testing _internal common_distributed MultiProcContinuousTest MultiProcessTestCase PLATFORM_SUPPORTS_SYMM_MEM requires_multicast_support skip_if_lt_x_gpu skip_if_rocm_multiprocess skip_if_rocm_ver_lessthan_multiprocess torch testing _internal common_utils instantiate_parametrized_tests parametrize requires_cuda requires_cuda_p p_access run_tests TEST_WITH_ROCM TestCase test_contexts = nullcontext _test_mode Set environment variable disable multicast all tests module os environ TORCH_SYMM_MEM_DISABLE_MULTICAST = So tests written device-agnostic way device_type = cuda device_module = torch get_device_module device_type instantiate_parametrized_tests requires_cuda_p p_access SymmetricMemoryTest MultiProcContinuousTest property device - torch device torch device device_type rank _init_process torch cuda set_device device torch manual_seed + rank test_has_multicast_support - None validate has_multicast_support returns false instead throwing assertFalse _SymmetricMemory has_multicast_support DeviceType CPU NOTE DeviceType CUDA implicitly tested through requires_multicast_support skipIf PLATFORM_SUPPORTS_SYMM_MEM SymmMem supported ROCm arch skip_if_lt_x_gpu test_get_backend - None backend = symm_mem get_backend torch device cuda assertIsNotNone backend backend = symm_mem get_backend cuda assertIsNotNone backend skip_if_rocm_multiprocess skip_if_lt_x_gpu test_cuda_nvlink_connectivity_detection - None torch _C _distributed_c d _detect_dma_connectivity connectivity = _detect_dma_connectivity DeviceType CUDA nvlink assertEqual connectivity device_type DeviceType CUDA assertEqual connectivity connection_type nvlink assertEqual len connectivity matrix torch cuda device_count row connectivity matrix assertEqual len row torch cuda device_count skipIf PLATFORM_SUPPORTS_SYMM_MEM SymmMem supported ROCm arch test_large_alloc - None t = symm_mem empty dtype=torch uint device= cuda assertEqual t numel t element_size skipIf PLATFORM_SUPPORTS_SYMM_MEM SymmMem supported ROCm arch skip_if_lt_x_gpu test_get_signal_pad - None _init_process t = symm_mem empty device= cuda symm_mem_hdl = symm_mem rendezvous t group=dist group WORLD peer_rank = rank + world_size signal_pad = symm_mem_hdl get_signal_pad rank assertEqual signal_pad data_ptr symm_mem_hdl signal_pad_ptrs symm_mem_hdl rank signal_pad = symm_mem_hdl get_signal_pad peer_rank assertEqual signal_pad dtype torch uint assertEqual signal_pad numel symm_mem_hdl signal_pad_size Only specify sizes signal_pad = symm_mem_hdl get_signal_pad peer_rank assertEqual signal_pad dtype torch uint assertEqual signal_pad numel Only specify dtype signal_pad = symm_mem_hdl get_signal_pad peer_rank dtype=torch uint assertEqual signal_pad dtype torch uint assertEqual signal_pad numel symm_mem_hdl signal_pad_size Specify both sizes dtype signal_pad = symm_mem_hdl get_signal_pad peer_rank dtype=torch uint assertEqual signal_pad dtype torch uint assertEqual signal_pad numel Sanity check writes buffer doesn t corrupt signal_pad t = symm_mem empty device= cuda symm_mem_hdl = symm_mem rendezvous t group=dist group WORLD signal_pad = symm_mem_hdl get_signal_pad rank signal_pad fill_ t fill_ assertTrue signal_pad eq all skipIf PLATFORM_SUPPORTS_SYMM_MEM SymmMem supported ROCm arch requires_cuda test_allow_overlapping_devices - None os environ TORCH_SYMM_MEM_ALLOW_OVERLAPPING_DEVICES = t = symm_mem empty device= cuda symm_mem_hdl = symm_mem rendezvous t group=dist group WORLD assertEqual symm_mem_hdl rank rank assertEqual symm_mem_hdl world_size world_size rank range world_size buf = symm_mem_hdl get_buffer rank torch float rank == rank assertEqual buf data_ptr t data_ptr assertEqual buf device t device os environ TORCH_SYMM_MEM_ALLOW_OVERLAPPING_DEVICES = skipIf PLATFORM_SUPPORTS_SYMM_MEM SymmMem supported ROCm arch skip_if_lt_x_gpu parametrize symm_mem_input True False test_low_contention_all_gather symm_mem_input bool - None _init_process symm_mem_input t = _SymmetricMemory empty_strided_p p size= stride= dtype=torch float device=self device group_name= fill_ rank t = torch full rank dtype=torch float device=self device res = torch ops symm_mem _low_contention_all_gather t res = torch ops _c d_functional wait_tensor res assertEqual res shape world_size chunks = res chunk world_size r range world_size assertTrue chunks r eq r all skipIf PLATFORM_SUPPORTS_SYMM_MEM SymmMem supported ROCm arch skip_if_lt_x_gpu parametrize reduce_op sum avg parametrize symm_mem_input True False test_low_contention_reduce_scatter reduce_op str symm_mem_input bool - None _init_process symm_mem_input t = _SymmetricMemory empty_strided_p p size= stride= dtype=torch float device=self device group_name= t = torch empty dtype=torch float device=self device chunks = t chunk world_size r range world_size chunks r fill_ r res = torch ops symm_mem _low_contention_reduce_scatter t reduce_op res = torch ops _c d_functional wait_tensor res assertEqual res shape world_size reduce_op == sum expect = rank world_size reduce_op == avg expect = rank raise AssertionError f Unexpected reduce_op reduce_op assertTrue res eq expect all skipIf PLATFORM_SUPPORTS_SYMM_MEM SymmMem supported ROCm arch skip_if_lt_x_gpu test_subgroup - None _init_process ranks = list range world_size subgroup_ = dist new_group ranks len ranks subgroup_ = dist new_group ranks len ranks world = dist group WORLD subgroup = subgroup_ world rank world size subgroup_ t = symm_mem empty device= cuda symm_mem_world = symm_mem rendezvous t group=world symm_mem_subgroup = symm_mem rendezvous t group=subgroup assertEqual symm_mem_world world_size world size assertEqual symm_mem_world rank world rank assertEqual symm_mem_subgroup world_size world size assertEqual symm_mem_subgroup rank world rank subgroup size t fill_ world rank symm_mem_world barrier Observe peer buffer via world group peer_rank = world rank + world size buf = symm_mem_world get_buffer peer_rank torch float assertTrue buf eq peer_rank all Observe peer buffer via subgroup peer_rank = subgroup rank + subgroup size buf = symm_mem_subgroup get_buffer peer_rank torch float world rank world size assertTrue buf eq peer_rank all assertTrue buf eq peer_rank + world size all We move AsyncTP tests separate test suite because Async TP ops core symmetric memory APIs they more like applications MultiProcContinuousTest will skip all following tests test fails we should fix too We still want get test signals core symmetric memory APIs when Async TP ops fail skip_if_rocm_multiprocess AsyncTP yet supported ROCm instantiate_parametrized_tests requires_cuda_p p_access AsyncTPTest MultiProcContinuousTest property device - torch device torch device device_type rank _init_process torch cuda set_device device torch manual_seed + rank torch use_deterministic_algorithms True torch set_deterministic_debug_mode warn torch utils deterministic fill_uninitialized_memory = True skipIf PLATFORM_SUPPORTS_SYMM_MEM SymmMem supported ROCm arch skip_if_lt_x_gpu parametrize gather_dim test_fused_all_gather_matmul gather_dim int - None _init_process BATCH = M = N = K = group = dist group WORLD rank = rank torch manual_seed + rank A_shard_shape = BATCH M K A_shard_shape gather_dim = world_size A_shard = torch rand A_shard_shape device= cuda Bs = torch rand K N device= cuda _ range ag_output_ mm_outputs_ = _fused_all_gather_matmul_fallback A_shard Bs gather_dim=gather_dim group_name=group group_name ag_output_ mm_outputs_ = torch ops symm_mem fused_all_gather_matmul A_shard Bs gather_dim=gather_dim group_name=group group_name assert torch allclose ag_output_ ag_output_ assert ag_output_ stride == ag_output_ stride mm_output_ mm_output_ zip mm_outputs_ mm_outputs_ assert torch allclose mm_output_ mm_output_ assert mm_output_ stride mm_output_ stride skip_if_rocm_multiprocess requires async_input_mm support skipIf SM OrLater _fused_all_gather_matmul_native currently only supports sm = skip_if_lt_x_gpu parametrize symm_mem_input True False parametrize is_b_row_major True False skipIf SM OrLater https github com pytorch pytorch issues test_fused_all_gather_matmul_native symm_mem_input bool is_b_row_major bool - None os environ TORCH_SYMM_MEM_ENABLE_NATIVE_ASYNC_TP = _init_process See _should_use_fused_all_gather_matmul_native algo selection criteria _fused_all_gather_matmul_native M = N = K = group_name = dist group WORLD group_name torch manual_seed + rank symm_mem_input A_shard = symm_mem empty M world_size K dtype=torch bfloat device=self device normal_ A_shard = torch rand M world_size K dtype=torch bfloat device= cuda is_b_row_major B = torch rand K N dtype=torch bfloat device= cuda B = torch rand N K dtype=torch bfloat device= cuda t ag_baseline mm_baseline = _fused_all_gather_matmul_fallback A_shard B gather_dim= group_name=group_name torch profiler profile activities= torch profiler ProfilerActivity CUDA prof ag_target mm_target = torch ops symm_mem fused_all_gather_matmul A_shard B gather_dim= group_name=group_name assertTrue any PersistentAsyncInputScheduler event key event prof events torch testing assert_close ag_target ag_baseline torch testing assert_close mm_target mm_baseline os environ TORCH_SYMM_MEM_ENABLE_NATIVE_ASYNC_TP = skip_if_lt_x_gpu requires_multicast_support test_multimem_all_gather_matmul - None _init_process See _should_use_multimem_all_gather_matmul algo selection criteria _multimem_gather_matmul M = N = K = group_name = dist group WORLD group_name torch manual_seed + rank A_shard = torch rand M world_size K dtype=torch bfloat device= cuda B = torch rand K N dtype=torch bfloat device= cuda ag_baseline mm_baseline = _fused_all_gather_matmul_fallback A_shard B gather_dim= group_name=group_name return_A=False torch profiler profile activities= torch profiler ProfilerActivity CUDA prof ag_target mm_target = torch ops symm_mem fused_all_gather_matmul A_shard B gather_dim= group_name=group_name return_A=False assertTrue any multimem_all_gather_kernel event key event prof events torch testing assert_close ag_target ag_baseline torch testing assert_close mm_target mm_baseline skipIf PLATFORM_SUPPORTS_SYMM_MEM SymmMem supported ROCm arch skip_if_lt_x_gpu skipUnless SM OrLater Requires compute capability = parametrize gather_dim parametrize scale_mode tensor-wise row-wise-replicated row-wise-sharded test_fused_all_gather_scaled_matmul gather_dim int scale_mode str - None _init_process BATCH = M = N = K = group = dist group WORLD rank = rank gather_dim == leading_dims = BATCH world_size M gather_dim == leading_dims = BATCH M world_size raise AssertionError f Invalid scale_mode scale_mode torch manual_seed + rank A_shard = torch rand leading_dims K device= cuda e m _type Bs = torch rand N K device= cuda e m _type T _ range scale_mode == tensor-wise A_scale = torch tensor device= cuda B_scales = torch tensor device= cuda _ range out_dtypes = None torch bfloat torch float scale_mode == row-wise-sharded A_scale = torch full leading_dims device= cuda B_scales = torch full N device= cuda _ range out_dtypes = torch bfloat scale_mode == row-wise-replicated A_scale = torch full BATCH M device= cuda B_scales = torch full N device= cuda _ range out_dtypes = torch bfloat raise AssertionError f Invalid scale_mode scale_mode ag_output_ mm_outputs_ = _fused_all_gather_scaled_matmul_fallback A_shard Bs A_scale B_scales gather_dim=gather_dim group_name=group group_name biases= None len Bs result_scales= None len Bs out_dtypes=out_dtypes use_fast_accum= None len Bs ag_output_ mm_outputs_ = torch ops symm_mem fused_all_gather_scaled_matmul A_shard Bs A_scale B_scales gather_dim=gather_dim group_name=group group_name biases= None len Bs result_scales= None len Bs out_dtypes=out_dtypes use_fast_accum= None len Bs assertTrue torch allclose ag_output_ torch float ag_output_ torch float assertEqual ag_output_ stride ag_output_ stride mm_output_ mm_output_ zip mm_outputs_ mm_outputs_ assertTrue torch allclose mm_output_ torch float mm_output_ torch float assertEqual mm_output_ stride mm_output_ stride assertEqual mm_output_ dtype mm_output_ dtype skipIf PLATFORM_SUPPORTS_SYMM_MEM SymmMem supported ROCm arch skip_if_lt_x_gpu parametrize scatter_dim test_fused_matmul_reduce_scatter scatter_dim int - None _init_process BATCH = M = N = K = group = dist group WORLD rank = rank torch manual_seed + rank A = torch rand BATCH M K device= cuda B = torch rand K N device= cuda output_ = _fused_matmul_reduce_scatter_fallback A B avg scatter_dim=scatter_dim group_name=group group_name output_ = torch ops symm_mem fused_matmul_reduce_scatter A B avg scatter_dim=scatter_dim group_name=group group_name assert torch allclose output_ output_ assert output_ stride == output_ stride skip_if_rocm_multiprocess AsyncTP support changed _fused_scaled_matmul_reduce_scatter_fallback API need more changes skip_if_lt_x_gpu skipUnless SM OrLater Requires compute capability = parametrize scatter_dim parametrize rowwise True False skipIf SM OrLater https github com pytorch pytorch issues test_fused_scaled_matmul_reduce_scatter scatter_dim int rowwise bool - None _init_process BATCH = M = N = K = group = dist group WORLD rank = rank torch manual_seed + rank A = torch rand BATCH M K device= cuda e m _type B = torch rand N K device= cuda e m _type T rowwise A_scale = torch full BATCH M device= cuda B_scale = torch full N device= cuda A_scale = torch tensor device= cuda B_scale = torch tensor device= cuda output_shape = A shape - B shape outputs = context test_contexts context outputs append torch ops symm_mem fused_scaled_matmul_reduce_scatter A B A_scale B_scale avg scatter_dim scatter_dim group group_name output_shape out_dtype=torch bfloat assert outputs stride == outputs stride assertEqual outputs outputs skipIf PLATFORM_SUPPORTS_SYMM_MEM SymmMem supported ROCm arch parametrize dim test_optimal_layout dim int - None t = torch rand x = restride_A_shard_for_fused_all_gather_matmul t dim assertTrue x movedim dim is_contiguous assertTrue torch allclose x t x = restride_A_for_fused_matmul_reduce_scatter t dim assertTrue x movedim dim is_contiguous assertTrue torch allclose x t READ ME FIRST The ` SymmMemEmptySetDeviceTest ` suite parameterizes whether user sets device before calling symm_mem empty Either way should work However since ` set_device ` persistent we cannot use ` MultiProcContinuousTest ` template because next function will contaminated leading flaky tests e g hang Therefore we use ` MultiProcessTestCase ` which spawns new processes each test function Please limit number tests you want add under test suite respawning processes ` init_process_group ` expensive instantiate_parametrized_tests requires_cuda_p p_access SymmMemEmptySetDeviceTest MultiProcessTestCase setUp - None super setUp _spawn_processes property world_size - int device_module device_count property device - torch device torch device device_type rank _init_process set_device bool set_device torch cuda set_device device store = dist FileStore file_name world_size dist init_process_group backend= nccl world_size=self world_size rank=self rank store=store torch manual_seed + rank _get_test_alloc_args shape = stride = dtype = torch float device = device shape stride dtype device _verify_symmetric_memory symm_mem_hdl assertEqual symm_mem_hdl world_size world_size buf = symm_mem_hdl get_buffer symm_mem_hdl buffer_size torch float assertEqual buf storage_offset assertEqual buf untyped_storage size symm_mem_hdl buffer_size symm_mem_hdl rank == symm_mem_hdl wait_signal src_rank= assertTrue buf eq all buf fill_ symm_mem_hdl put_signal dst_rank= symm_mem_hdl barrier symm_mem_hdl rank == symm_mem_hdl barrier assertTrue buf eq all buf fill_ symm_mem_hdl barrier symm_mem_hdl barrier skipIf PLATFORM_SUPPORTS_SYMM_MEM SymmMem supported ROCm arch skip_if_lt_x_gpu parametrize set_device True False test_empty_strided_p p set_device bool - None _init_process set_device group_name = dist group WORLD group_name enable_symm_mem_for_group group_name alloc_args = _get_test_alloc_args t = torch empty device=self device assertIsNone _SymmetricMemory rendezvous t t = _SymmetricMemory empty_strided_p p alloc_args group_name=group_name symm_mem_hdl = _SymmetricMemory rendezvous t del t _verify_symmetric_memory symm_mem_hdl skipIf PLATFORM_SUPPORTS_SYMM_MEM SymmMem supported ROCm arch skip_if_rocm_ver_lessthan_multiprocess skip_if_lt_x_gpu parametrize set_device True False test_empty_strided_p p_persistent set_device bool - None _init_process set_device group_name = dist group WORLD group_name enable_symm_mem_for_group group_name alloc_args = _get_test_alloc_args alloc_id = + random randint t = _SymmetricMemory empty_strided_p p alloc_args group_name=group_name alloc_id=alloc_id data_ptr = t data_ptr Verify persistent allocation would fail there s active allocation same alloc_id assertRaises RuntimeError _SymmetricMemory empty_strided_p p alloc_args group_name=group_name alloc_id=alloc_id Verify persistent allocation would succeed lieu activate allocations same alloc_id returned tensor would have same data pointer del t t = _SymmetricMemory empty_strided_p p alloc_args group_name=group_name alloc_id=alloc_id assertEqual t data_ptr data_ptr symm_mem_hdl = _SymmetricMemory rendezvous t _verify_symmetric_memory symm_mem_hdl This Test used test error handling SymmetricMemory APIs Since process restart often needed after each test we use MultiProcessTestCase instead MultiProcContinuousTest requires_cuda_p p_access SymmMemNegativeTest MultiProcessTestCase setUp - None super setUp _spawn_processes property world_size - int device_module device_count property device - torch device torch device device_type rank _init_process torch cuda set_device device store = dist FileStore file_name world_size dist init_process_group backend= nccl world_size=self world_size rank=self rank store=store torch manual_seed + rank These timeout tests skipped ROCm because timeout calls trap which handled differently inside hip runtime It collects gpu coredump causes linux kernel create core dump host application The functionality there meaning timeout happening correctly However there isn t nice way test current executing thread will coredump exit skip_if_rocm_multiprocess skip_if_lt_x_gpu test_barrier_timeout - None _init_process t = symm_mem empty device= cuda symm_mem_hdl = symm_mem rendezvous t group=dist group WORLD rank == assertRaises RuntimeError symm_mem_hdl barrier timeout_ms= torch cuda synchronize torch cuda synchronize The device-side timeout triggers __trap causes all subsequent host device interactions result unspecified launch failure Using os _exit abort test s impossible terminate process state os _exit These timeout tests skipped ROCm because timeout calls trap which handled differently inside hip runtime It collects gpu coredump causes linux kernel create core dump host application The functionality there meaning timeout happening correctly However there isn t nice way test current executing thread will coredump exit skip_if_rocm_multiprocess skip_if_lt_x_gpu test_put_signal_timeout - None _init_process t = symm_mem empty device= cuda symm_mem_hdl = symm_mem rendezvous t group=dist group WORLD rank == assertRaises RuntimeError First put signal into rank s signal pad Since rank doesn t wait signal subsequent put will timeout symm_mem_hdl put_signal dst_rank= symm_mem_hdl put_signal dst_rank= timeout_ms= torch cuda synchronize torch cuda synchronize The device-side timeout triggers __trap causes all subsequent host device interactions result unspecified launch failure Using os _exit abort test s impossible terminate process state os _exit These timeout tests skipped ROCm because timeout calls trap which handled differently inside hip runtime It collects gpu coredump causes linux kernel create core dump host application The functionality there meaning timeout happening correctly However there isn t nice way test current executing thread will coredump exit skip_if_rocm_multiprocess skip_if_lt_x_gpu test_wait_signal_timeout - None _init_process t = symm_mem empty device= cuda symm_mem_hdl = symm_mem rendezvous t group=dist group WORLD rank == assertRaises RuntimeError symm_mem_hdl wait_signal src_rank= timeout_ms= torch cuda synchronize torch cuda synchronize The device-side timeout triggers __trap causes all subsequent host device interactions result unspecified launch failure Using os _exit abort test s impossible terminate process state os _exit instantiate_parametrized_tests requires_cuda_p p_access SymmMemCollectiveTest MultiProcContinuousTest property device - torch device torch device device_type rank _init_process torch cuda set_device device torch manual_seed + rank skip_if_lt_x_gpu requires_multicast_support parametrize dtype torch float torch bfloat parametrize align_bytes parametrize size_bytes test_multimem_all_reduce dtype torch dtype size_bytes int align_bytes int - None _init_process group_name = dist group WORLD group_name t = symm_mem empty dtype=dtype device=self device symm_mem rendezvous t group=dist group WORLD assertTrue t data_ptr == assertTrue align_bytes t element_size == assertTrue size_bytes t element_size == shift = align_bytes t element_size numel = size_bytes t element_size res = t shift shift + numel res normal_ inp = res clone torch ops symm_mem multimem_all_reduce_ res sum group_name Head tail should written assertTrue t shift eq all item assertTrue t shift + numel eq all item _verify_all_reduce_result inp res skip_if_lt_x_gpu requires_multicast_support parametrize dtype torch float torch bfloat parametrize align_bytes parametrize size_bytes https github com pytorch pytorch issues xfailIfSM OrLater test_multimem_one_shot_all_reduce dtype torch dtype size_bytes int align_bytes int - None _init_process group_name = dist group WORLD group_name inp = symm_mem empty size_bytes dtype itemsize dtype=dtype device=self device normal_ symm_mem rendezvous inp group=group_name res = torch ops symm_mem multimem_one_shot_all_reduce inp sum group_name gathered_inps = all_gather_tensor inp view world_size - Only verify results close sum inputs across ranks see Note multimem_one_shot_all_reduce torch testing assert_close gathered_inps sum dim= res rtol= e- atol= e- skip_if_lt_x_gpu requires_multicast_support parametrize dtype torch float torch bfloat parametrize size_bytes https github com pytorch pytorch issues xfailIfSM OrLater test_multimem_one_shot_reduce_out dtype torch dtype size_bytes int - None _init_process group_name = dist group WORLD group_name inp = symm_mem empty size_bytes dtype itemsize dtype=dtype device=self device normal_ out = torch empty_like inp symm_mem rendezvous inp group=group_name root = torch ops symm_mem multimem_one_shot_reduce_out inp sum root group_name out gathered_inps = all_gather_tensor inp view world_size - Only verify results close sum inputs across ranks see Note multimem_one_shot_all_reduce rank == root torch testing assert_close gathered_inps sum dim= out rtol= e- atol= e- skipIf PLATFORM_SUPPORTS_SYMM_MEM SymmMem supported ROCm arch skip_if_lt_x_gpu test_one_shot_all_reduce - None _init_process group_name = dist group WORLD group_name dtype size_bytes align_bytes copy offset itertools product torch float torch bfloat TODO add back currently OOM when looping over all combinations True False inp = symm_mem empty size_bytes dtype itemsize + offset dtype=dtype device=self device symm_mem rendezvous inp group=group_name copy inp normal_ res = torch ops symm_mem one_shot_all_reduce inp offset sum group_name copy local_inp = torch randn_like inp offset res = torch ops symm_mem one_shot_all_reduce_copy inp offset local_inp sum group_name _verify_all_reduce_result local_inp copy inp offset res skipIf PLATFORM_SUPPORTS_SYMM_MEM SymmMem supported ROCm arch skip_if_lt_x_gpu test_two_shot_all_reduce - None _init_process group_name = dist group WORLD group_name dtype size_bytes align_bytes inplace itertools product torch float torch bfloat TODO add back currently OOM when looping over all combinations True False t = symm_mem empty dtype=dtype device=self device fill_ symm_mem rendezvous t group=group_name assertTrue t data_ptr == assertTrue align_bytes t element_size == assertTrue size_bytes t element_size == shift = align_bytes t element_size numel = size_bytes t element_size res = t shift shift + numel res normal_ inp = res clone inplace out = torch empty_like inp torch ops symm_mem two_shot_all_reduce_out res sum group_name out torch ops symm_mem two_shot_all_reduce_ res sum group_name Head tail should written assertTrue t shift eq all item assertTrue t shift + numel eq all item _verify_all_reduce_result inp res inplace out _verify_all_reduce_result inp res gathered_res = all_gather_tensor res view world_size - Verify results across ranks identical assertEqual gathered_res == gathered_res all dim= sum inp numel Verify result close sum inputs across ranks gathered_inps = all_gather_tensor inp view world_size - torch testing assert_close gathered_inps sum dim= res rtol= e- atol= e- skipIf PLATFORM_SUPPORTS_SYMM_MEM SymmMem supported ROCm arch skip_if_lt_x_gpu test_reduce_scatter - None _init_process group_name = dist group WORLD group_name dtype size_bytes align_bytes split_last_dim itertools product torch float torch bfloat TODO add back currently OOM when looping over all combinations True False t = symm_mem empty dtype=dtype device=self device fill_ symm_mem rendezvous t group=group_name assertTrue t data_ptr == assertTrue align_bytes t element_size == assertTrue size_bytes t element_size == shift = align_bytes t element_size numel = size_bytes t element_size res = t shift shift + numel normal_ split_last_dim res = res view - t element_size inp = res clone out_size = list inp shape out_size - = inp shape - world_size out = torch empty out_size dtype=dtype device=self device torch ops symm_mem reduce_scatter_out res group_name split_last_dim out Head tail should written assertTrue t shift eq all item assertTrue t shift + numel eq all item _verify_reduce_scatter_result inp out skipIf PLATFORM_SUPPORTS_SYMM_MEM SymmMem supported ROCm arch skip_if_lt_x_gpu test_reduce_scatter_corner_cases - None _init_process dtype = torch bfloat group_name = dist group WORLD group_name t = symm_mem empty dtype=dtype device=self device fill_ symm_mem rendezvous t group=group_name res = t out_size = res shape world_size out = torch empty out_size dtype=dtype device=self device torch ops symm_mem reduce_scatter_out res group_name False out res = t out_size = res shape world_size out = torch empty out_size dtype=dtype device=self device assertRaisesRegex RuntimeError divisible torch ops symm_mem reduce_scatter_out res group_name False out res = t view out = torch empty world_size dtype=dtype device=self device assertRaisesRegex RuntimeError divisible torch ops symm_mem reduce_scatter_out res group_name True out _verify_reduce_scatter_result inp res gathered_res = all_gather_tensor res view world_size res shape gathered_inps = all_gather_tensor inp view world_size inp shape sum_inps = gathered_inps sum slice_width = sum_inps shape - world_size i range world_size torch testing assert_close gathered_res i sum_inps i slice_width i + slice_width rtol= e- atol= e- skip_if_lt_x_gpu requires_multicast_support parametrize align_bytes test_multimem_all_gather align_bytes int - None _init_process group_name = dist group WORLD group_name input_numel = shift = align_bytes input = torch zeros shift + input_numel device=self device shift fill_ rank out = symm_mem empty shift + input_numel world_size device=self device zero_ shift symm_mem rendezvous out group=group_name torch ops symm_mem multimem_all_gather_out input group_name out ref = torch ops _c d_functional all_gather_into_tensor input world_size group_name ref = torch ops _c d_functional wait_tensor ref assertTrue out eq ref all instantiate_parametrized_tests requires_cuda_p p_access LoweringTest MultiProcContinuousTest _init_process - None torch cuda set_device device enable_symm_mem_for_group dist group WORLD group_name torch manual_seed + rank torch _inductor config _collective auto_select = True property device - torch device torch device device_type rank skip Fails one_shot_all_reduce found AOT graph TODO fix skip_if_rocm_multiprocess requires registered-buffer support skip_if_lt_x_gpu fresh_cache test_lowering_one_shot_all_reduce _init_process arg = torch rand device=self device func_ x x = x + x = torch ops _c d_functional all_reduce x sum torch ops _c d_functional wait_tensor x compiled_ = torch compile func_ fullgraph=True code_ = run_and_get_triton_code compiled_ arg assertIn one_shot_all_reduce code_ assertNotIn buf code_ All-reduce slice view func_ x x = x + x = x x = torch ops _c d_functional all_reduce x sum torch ops _c d_functional wait_tensor x compiled_ = torch compile func_ fullgraph=True code_ = run_and_get_triton_code compiled_ arg assertIn one_shot_all_reduce code_ assertNotIn buf code_ All-reduce input func_ x x = torch ops _c d_functional all_reduce x sum torch ops _c d_functional wait_tensor x compiled_ = torch compile func_ fullgraph=True code_ = run_and_get_triton_code compiled_ arg assertNotIn one_shot_all_reduce code_ All-reduce matmul output func_ x x = x x x = torch ops _c d_functional all_reduce x sum torch ops _c d_functional wait_tensor x compiled_ = torch compile func_ fullgraph=True code_ = run_and_get_triton_code compiled_ arg assertIn one_shot_all_reduce code_ assertNotIn buf code_ SymmMemSingleProcTest TestCase requires_cuda skipIf TEST_WITH_ROCM _get_torch_cuda_version stream_write_value currently only supports cuda version = skipIf PLATFORM_SUPPORTS_SYMM_MEM SymmMem supported ROCm arch test_stream_write_value tensor = torch zeros dtype=torch uint device= cuda expect = torch tril torch ones device= cuda torch uint i range _SymmetricMemory stream_write_value tensor i torch testing assert_close tensor expect i assertRaises RuntimeError _SymmetricMemory stream_write_value tensor offset=- val= assertRaises RuntimeError _SymmetricMemory stream_write_value tensor offset= val= skipIf PLATFORM_SUPPORTS_SYMM_MEM SymmMem supported ROCm arch requires_cuda test_memset t = _SymmetricMemory empty_strided_p p dtype=torch uint device=torch device cuda group_name= fill_ _SymmetricMemory memset t offset= val= count= assertTrue t eq all assertTrue t eq all assertTrue t eq all assertRaisesRegex RuntimeError input must flat contiguous uint tensor _SymmetricMemory memset t view offset= val= count= assertRaisesRegex RuntimeError input must flat contiguous uint tensor _SymmetricMemory memset t view torch float offset= val= count= assertRaisesRegex RuntimeError offset must greater than equal _SymmetricMemory memset t offset=- val= count= assertRaisesRegex RuntimeError r val must range \ uint _t\ _SymmetricMemory memset t offset= val= count= assertRaisesRegex RuntimeError count must positive integer _SymmetricMemory memset t offset= val= count=- assertRaisesRegex RuntimeError count must positive integer _SymmetricMemory memset t offset= val= count= assertRaisesRegex RuntimeError r offset \+ count exceeded numel input _SymmetricMemory memset t offset= val= count= assertRaisesRegex RuntimeError r offset \+ count exceeded numel input _SymmetricMemory memset t offset= val= count= _SymmetricMemory memset t offset= val= count= _SymmetricMemory memset t offset= val= count= __name__ == __main__ run_tests