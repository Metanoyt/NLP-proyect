functools collections abc Sequence typing Callable Optional Protocol Union sympy torch virtualized OpsValue V BlockShapeType = Optional Sequence Union int str ShapeVar Protocol property shape - BlockShapeType ShapeArg = Union ShapeVar torch types Number str OpsValue torch dtype Inputs need cacheable e g CSEVar order cache effective So first decompose CSEVars - tuple before calling functools lru_cache None get_broadcasted_shape BlockShapeType b BlockShapeType - BlockShapeType assert isinstance Sequence assert isinstance b Sequence len len b get_broadcasted_shape len - len b b len len b b = b get_broadcasted_shape len - len b b _get_broadcasted_dim d Union int str d Union int str - Union int str str d == d str d == d assert str d == str d d tuple _get_broadcasted_dim d d d d zip b broadcast_shapes_for_args args Sequence ShapeArg - BlockShapeType result_shape BlockShapeType = None arg args hasattr arg shape shape = arg shape shape None None result_shape None result_shape = tuple shape result_shape = get_broadcasted_shape result_shape tuple shape isinstance arg int float result_shape None result_shape = isinstance arg torch dtype continue torch _inductor loop_body LoopBody LoopBodyBlock isinstance arg LoopBodyBlock LoopBody OpsValue TODO fix me None raise TypeError f Unknown type type arg result_shape ShapePropagationOpsHandler Propagate shape args output staticmethod constant value torch types Number dtype torch dtype - BlockShapeType See implementation constant triton reason torch _inductor codegen triton triton_compute_type TritonKernel triton_type = triton_compute_type dtype isinstance V kernel TritonKernel triton_type = tl float ndim = V kernel triton_tensor_ndim tuple ndim staticmethod store_reduction name str index int value ShapeArg - None None staticmethod reduction dtype torch dtype src_dtype torch dtype reduction_type str value Union ShapeArg tuple ShapeArg - Union BlockShapeType tuple BlockShapeType raise NotImplementedError staticmethod store name str index int value ShapeArg mode Optional str = None - None None staticmethod to_dtype value ShapeVar dtype torch dtype src_dtype Optional torch dtype = None use_compute_types bool = True - BlockShapeType value shape staticmethod dot sympy Expr b sympy Expr - BlockShapeType torch _inductor codegen triton TritonKernel assert isinstance V kernel TritonKernel dot supports Triton only YBLOCK XBLOCK staticmethod index_expr expr sympy Expr dtype torch dtype - BlockShapeType shape implicitly embedded expr None staticmethod load_seed name str offset int - BlockShapeType staticmethod indirect_indexing var ShapeArg size Union sympy Expr int check bool = True wrap_neg bool = True - None None __getattr__ name str - Callable BlockShapeType lambda args kwargs broadcast_shapes_for_args args staticmethod device_assert_async cond ShapeArg msg str - None None