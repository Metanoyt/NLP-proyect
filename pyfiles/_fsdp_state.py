mypy allow-untyped-decorators mypy allow-untyped-defs functools logging collections abc Callable Sequence typing Any Optional TYPE_CHECKING torch torch nn nn torch _logging warning_once torch autograd Variable torch autograd graph _MultiHandle torch distributed _composable_state _get_module_state _insert_module_state _State torch distributed device_mesh _get_device_handle torch distributed utils _apply_to_tensors _to_kwargs torch utils _pytree tree_flatten _fsdp_api MixedPrecisionPolicy _fsdp_common _cast_fp_tensor compiled_autograd_enabled detect_compiled_autograd TrainingState _fsdp_param_group FSDPCommContext FSDPParamGroup TYPE_CHECKING _fsdp_param FSDPParam logger = logging getLogger torch distributed fsdp fully_shard FSDPStateContext This has state shared across FSDP states __init__ - None All FSDP states root state s module tree all_states list FSDPState = Iteration s forward root runs once-per-forward logic root may overall root set lazy initialization cases where only submodule runs forward e g encoder-only eval iter_forward_root Optional FSDPState = None Final callback should only queued once per backward post_backward_final_callback_queued bool = False Whether finalize backward backward s final callback is_last_backward bool = True Optional user-provided event recorded after optimizer all-gather streams wait root pre-forward post_optim_event Optional torch Event = None disable_if_config_true func functools wraps func fsdp_hook_wrapper args kwargs torch _dynamo config skip_fsdp_hooks torch _dynamo disable func recursive=True reason= skipping FSDP hooks since torch _dynamo config skip_fsdp_hooks set args kwargs func args kwargs fsdp_hook_wrapper FSDPState _State __init__ - None super __init__ _fsdp_param_group Optional FSDPParamGroup = None _is_root Optional bool = None root set during lazy init _state_ctx = FSDPStateContext _comm_ctx = FSDPCommContext _training_state TrainingState = TrainingState IDLE _states_to_forward_prefetch list FSDPState = _states_to_backward_prefetch list FSDPState = _modules_to_run_forward set nn Module = set ` ` False ` ` when user set reshard_after_forward through ` ` fully_shard ` ` ` ` set_reshard_after_forward ` ` _auto_reshard_after_forward Optional bool = True Define separate init since ` __init__ ` called contract init modules tuple nn Module device torch device mp_policy MixedPrecisionPolicy auto_reshard_after_forward bool - None module modules _insert_module_state module _modules = modules pyrefly ignore read-only _device = device _device_handle = _get_device_handle device type _mp_policy = mp_policy _auto_reshard_after_forward = auto_reshard_after_forward len modules == _pre_forward_hook_handle = modules register_forward_pre_hook _pre_forward prepend=True with_kwargs=True _post_forward_hook_handle = modules register_forward_hook _post_forward prepend=False hook_handle = _register_group_forward_hooks modules _pre_forward _post_forward _modules_to_run_forward _pre_forward_hook_handle = hook_handle _post_forward_hook_handle = hook_handle _root_pre_forward module nn Module args tuple Any kwargs dict str Any - tuple tuple Any dict str Any _lazy_init _state_ctx iter_forward_root None args kwargs compiled_autograd_enabled logger debug FSDP root_pre_forward _state_ctx iter_forward_root = torch profiler record_function FSDP root_pre_forward Wait optimizer before implicitly prefetched all-gathers event = _state_ctx post_optim_event None _comm_ctx all_gather_copy_in_stream wait_event event _comm_ctx all_gather_stream wait_event event _state_ctx post_optim_event = None current_stream = _device_handle current_stream _comm_ctx all_gather_copy_in_stream wait_stream current_stream _comm_ctx all_gather_stream wait_stream current_stream _device type cuda hpu xpu mtia torch _C _get_privateuse _backend_name torch profiler record_function FSDP inputs_to_device args_tuple kwargs_tuple = _to_kwargs args kwargs _device False same DDP args kwargs = args_tuple kwargs_tuple args kwargs _lazy_init - None Lazy initialization represents when all modules parallelisms have finalized e g FSDP has been applied all desired modules This means we can determine which state root we do so st state run forward _is_root None no-op already initialized _is_root = True len _modules raise RuntimeError f FSDP requires single root module got _modules detect_compiled_autograd root_module = _modules visited_states set FSDPState = set module_name module root_module named_modules state = _get_module_fsdp_state module None continue module root_module state visited_states state _is_root None raise RuntimeError FSDP state has already been lazily initialized f module_name \nFSDP requires running forward through root module first state _is_root = False _state_ctx all_states append state visited_states add state _fsdp_param_group _auto_reshard_after_forward For root do reshard after forward since training parameters would freed all-gathered immediately _fsdp_param_group post_forward_mesh_info = None _init_fqns _init_shared_state Run parameter group lazy inits after initializing FQNs improved error messages state _state_ctx all_states state _fsdp_param_group state _fsdp_param_group lazy_init _init_shared_state - None _comm_ctx lazy_init _device state _state_ctx all_states state _state_ctx = _state_ctx state _comm_ctx = _comm_ctx fsdp_param_group = state _fsdp_param_group fsdp_param_group comm_ctx = _comm_ctx _init_fqns - None Sets module parameter FQN attributes debugging _is_root raise AssertionError Expected _is_root True root_module = _modules param_to_fsdp_param dict nn Parameter FSDPParam = module_to_fsdp_param_group dict nn Module FSDPParamGroup = state _state_ctx all_states fsdp_param_group = state _fsdp_param_group fsdp_param fsdp_param_group fsdp_params param_to_fsdp_param fsdp_param sharded_param = fsdp_param module fsdp_param_group modules module_to_fsdp_param_group module = fsdp_param_group param_name param root_module named_parameters param param_to_fsdp_param param_to_fsdp_param param _param_fqn = param_name module_name module root_module named_modules module module_to_fsdp_param_group module_fqn = module_to_fsdp_param_group module _module_fqn module_fqn None module_to_fsdp_param_group module _module_fqn = module_name isinstance module_fqn str raise AssertionError f Expected module_fqn str got type module_fqn module_fqn module_fqn += f module_name module_to_fsdp_param_group module _module_fqn = module_fqn disable_if_config_true _pre_forward module nn Module args tuple Any kwargs dict str Any - tuple tuple Any dict str Any When composing module-hook-based activation checkpointing pre-backward hook responsible unshard _training_state == TrainingState PRE_BACKWARD args kwargs _training_state = TrainingState FORWARD args kwargs = _root_pre_forward module args kwargs _mp_policy cast_forward_inputs _mp_policy param_dtype torch profiler record_function FSDP cast_forward_inputs cast_fn = functools partial _cast_fp_tensor _mp_policy param_dtype args kwargs = _apply_to_tensors cast_fn args _apply_to_tensors cast_fn kwargs _fsdp_param_group args kwargs = _fsdp_param_group pre_forward module args kwargs fsdp_state _states_to_forward_prefetch target_param_group = fsdp_state _fsdp_param_group None FSDPParamGroup _prefetch_unshard target_param_group forward args kwargs disable_if_config_true _post_forward module nn Module input Any output Any - Any When composing module-hook-based activation checkpointing post-backward hook responsible reshard _training_state == TrainingState PRE_BACKWARD output _fsdp_param_group output = _fsdp_param_group post_forward module input output output = _register_pre_backward_hook output _training_state = TrainingState IDLE _state_ctx iter_forward_root all_gather_state = _comm_ctx all_gather_state Free last all-gather result needed refer Note Overlapping all-gather copy-in all-gather _comm_ctx all_gather_copy_in_stream wait_event all_gather_state event _comm_ctx all_gather_stream wait_event all_gather_state event _comm_ctx all_gather_state = None free all-gather result _state_ctx iter_forward_root = None _mp_policy output_dtype None torch profiler record_function FSDP cast_forward_outputs output = _apply_to_tensors functools partial _cast_fp_tensor _mp_policy output_dtype output output _pre_backward grad torch Tensor - torch Tensor _training_state = TrainingState PRE_BACKWARD _register_root_post_backward_final_callback _fsdp_param_group default_prefetch = len _states_to_backward_prefetch == _fsdp_param_group pre_backward default_prefetch fsdp_state _states_to_backward_prefetch target_param_group = fsdp_state _fsdp_param_group None FSDPParamGroup _prefetch_unshard target_param_group backward grad _root_post_backward_final_callback - None compiled_autograd_enabled logger debug FSDP root_post_backward torch profiler record_function FSDP root_post_backward_callback state _state_ctx all_states fsdp_param_group = state _fsdp_param_group fsdp_param_group fsdp_param_group _training_state = TrainingState POST_BACKWARD Run post-backward case forward inputs did require gradient so autograd backward did run fsdp_param_group post_backward state _training_state = TrainingState IDLE fsdp_param_group fsdp_param_group _training_state = TrainingState IDLE _state_ctx is_last_backward state _finalize_backward _state_ctx is_last_backward _comm_ctx post_forward_order clear _comm_ctx reduce_scatter_state None _device_handle current_stream wait_event _comm_ctx reduce_scatter_state event _comm_ctx reduce_scatter_state = None _state_ctx post_backward_final_callback_queued = False _finalize_backward - None _modules_to_run_forward msg = f len _modules_to_run_forward len _modules f modules passed fully_shard did run forward before backward which error-prone since FSDP post-forward pre-backward logic will run these modules We recommend passing only modules run forward together Modules did run forward f list _modules_to_run_forward warning_once logger msg stacklevel= Clear since we want next forward run _modules_to_run_forward clear _fsdp_param_group _fsdp_param_group finalize_backward _register_pre_backward_hook output Any - Any torch is_grad_enabled output flat_outputs _ = tree_flatten output t flat_outputs torch is_tensor t t requires_grad t register_hook _pre_backward output _register_root_post_backward_final_callback _state_ctx post_backward_final_callback_queued _state_ctx post_backward_final_callback_queued = True Variable _execution_engine queue_callback _root_post_backward_final_callback _get_module_fsdp_state module nn Module - Optional FSDPState state = _get_module_state module isinstance state FSDPState state None _register_group_forward_hooks modules Sequence nn Module pre_hook Callable post_hook Callable modules_to_run set nn Module Registers group forward pre post-hooks The pre-hook runs upon first module pre-forward post-hook runs upon last If least one module does run forward then post-hook does run modules_set = set modules disable_if_config_true functools wraps pre_hook wrapped_pre_hook args Any kwargs Any len modules_to_run == first run modules_to_run update modules_set pre_hook args kwargs disable_if_config_true get_wrapped_post_hook module nn Module functools wraps post_hook wrapped_post_hook args Any kwargs Any modules_to_run discard module len modules_to_run == post_hook args kwargs wrapped_post_hook pre_handles = module register_forward_pre_hook wrapped_pre_hook prepend=True with_kwargs=True module modules post_handles = module register_forward_hook get_wrapped_post_hook module prepend=False always_call=True module modules _MultiHandle tuple pre_handles + post_handles